1
00:00:00,000 --> 00:00:05,000
Hi, everyone. It's Tristan, and this is your Undivided Attention.

2
00:00:05,000 --> 00:00:09,000
Up next, we have our unedited conversation with Daniel Schmacktenberger.

3
00:00:09,000 --> 00:00:13,000
And because it's unedited, it's longer and not corrected for fact-checking purposes,

4
00:00:13,000 --> 00:00:17,000
but you can find our shorter, edited version wherever you found this one.

5
00:00:17,000 --> 00:00:21,000
Listen to both versions, and then come to our podcast club with Daniel and me,

6
00:00:21,000 --> 00:00:25,000
and hopefully you, on July 9th. Details are in the show notes.

7
00:00:25,000 --> 00:00:27,000
And with that, here we go.

8
00:00:31,000 --> 00:00:33,000
Welcome to your Undivided Attention.

9
00:00:33,000 --> 00:00:38,000
Today, I am so honored and happy to have my friend, Daniel Schmacktenberger,

10
00:00:38,000 --> 00:00:42,000
as our guest, who works on the topics of existential risk

11
00:00:42,000 --> 00:00:46,000
and what are the underlying drivers of all of the major problems,

12
00:00:46,000 --> 00:00:50,000
or many of the major problems, that are really facing us today as a civilization,

13
00:00:50,000 --> 00:00:54,000
be it climate change, breakdown of truth, social media, our information systems.

14
00:00:54,000 --> 00:00:58,000
Those of you who've been following your Undivided Attention will hear this

15
00:00:58,000 --> 00:01:00,000
as a very different kind of episode.

16
00:01:00,000 --> 00:01:05,000
We almost think of it as a meta-episode about the underlying drivers

17
00:01:05,000 --> 00:01:10,000
of many of the topics that we have covered on your Undivided Attention thus far.

18
00:01:10,000 --> 00:01:13,000
So if you think about the topics that we've covered,

19
00:01:13,000 --> 00:01:17,000
whether you've seen the social dilemma or you followed our interviews previously

20
00:01:17,000 --> 00:01:23,000
on topics like attention span shortening or addiction or information overwhelm and distraction,

21
00:01:23,000 --> 00:01:27,000
the fall of trust in society, more polarization, breakdown of truth,

22
00:01:27,000 --> 00:01:30,000
our inability to solve problems like climate change,

23
00:01:30,000 --> 00:01:35,000
well, this is really about an interconnected set of problems

24
00:01:35,000 --> 00:01:40,000
and the kind of core generator functions that are leading to all of these things to happen at once.

25
00:01:40,000 --> 00:01:44,000
And so I really encourage you to listen to this all the way through,

26
00:01:44,000 --> 00:01:48,000
and I think that we're going to get into some very deep and important knowledge

27
00:01:48,000 --> 00:01:51,000
that will hopefully be orienting for all of us.

28
00:01:52,000 --> 00:01:55,000
One of my favorite quotes is by Charles Kettering,

29
00:01:55,000 --> 00:02:00,000
who said that a problem not fully understood is unsolvable

30
00:02:00,000 --> 00:02:03,000
and a problem that is fully understood is half-solved.

31
00:02:03,000 --> 00:02:08,000
And what I hope we talk about with Daniel is what about the framework that we are using

32
00:02:08,000 --> 00:02:13,000
to address or try to meet the various problems that we have has been inadequate

33
00:02:13,000 --> 00:02:16,000
and what is the problem-solving framework that we're going to need

34
00:02:16,000 --> 00:02:19,000
to deal with the existential crises that face us.

35
00:02:19,000 --> 00:02:22,000
So Daniel, welcome to Your Undivided Detention.

36
00:02:22,000 --> 00:02:24,000
Thank you, Tristan.

37
00:02:24,000 --> 00:02:28,000
I've been looking forward to us dialoguing about these things publicly for a while.

38
00:02:28,000 --> 00:02:30,000
Well, you and me both.

39
00:02:30,000 --> 00:02:33,000
And for those who don't know, Daniel and I have been friends for a very long time,

40
00:02:33,000 --> 00:02:38,000
and his work has been highly influential to me and many people in my circles.

41
00:02:38,000 --> 00:02:43,000
So Daniel, maybe we should just start with what is the metacrisis

42
00:02:43,000 --> 00:02:47,000
and why are these problems seemingly not getting solved,

43
00:02:47,000 --> 00:02:51,000
whether it's the SDGs, climate change, or anything that we really care about right now?

44
00:02:53,000 --> 00:02:58,000
I think a lot of people have the general sense that there is an increasing number

45
00:02:58,000 --> 00:03:06,000
of possibly catastrophic issues and that as new categories of tech,

46
00:03:06,000 --> 00:03:09,000
tech that allows major cyber attacks on infrastructure,

47
00:03:09,000 --> 00:03:12,000
tech that allows weaponized drone attacks on infrastructure,

48
00:03:12,000 --> 00:03:17,000
biotechnologies, artificial intelligence and moving towards AGI,

49
00:03:17,000 --> 00:03:21,000
that there are new catastrophic risks with all of those categories of tech

50
00:03:21,000 --> 00:03:28,000
and that those tech are creating larger jumps in power faster than any types of jumps of tech,

51
00:03:28,000 --> 00:03:32,000
including the development of the nuclear bomb in the past by many orders of magnitude.

52
00:03:32,000 --> 00:03:40,000
So there's a general sense that whether we're talking about future pandemic related issues

53
00:03:40,000 --> 00:03:45,000
or whether we're talking about climate change or climate change as a forcing function

54
00:03:45,000 --> 00:03:50,000
for human migration that then causes resource wars and political instability

55
00:03:50,000 --> 00:03:55,000
or the fragility of the highly interconnected globalized world

56
00:03:55,000 --> 00:03:58,000
where a problem in one part of the world can create supply chain issues

57
00:03:58,000 --> 00:04:00,000
that create problems all around the world,

58
00:04:00,000 --> 00:04:04,000
there's a sense that there's an increasing number of catastrophic risks

59
00:04:04,000 --> 00:04:07,000
and that they're increasing faster than we are solving them.

60
00:04:07,000 --> 00:04:10,000
And that when you mention like with the UN,

61
00:04:10,000 --> 00:04:16,000
while progress has been made in certain defined areas of the sustainable development goals

62
00:04:16,000 --> 00:04:19,000
and progress was made back when they were called the Millennium Development Goals,

63
00:04:19,000 --> 00:04:23,000
we're very far from anything like a comprehensive solution to any of them.

64
00:04:23,000 --> 00:04:28,000
We're not even on track for something that is converging towards a comprehensive solution.

65
00:04:28,000 --> 00:04:35,000
And if we look at kind of the core initial mandate of the United Nations

66
00:04:35,000 --> 00:04:39,000
in terms of thinking about how to recognizing after World War II

67
00:04:39,000 --> 00:04:43,000
that nations take government alone wouldn't prevent World War

68
00:04:43,000 --> 00:04:45,000
and now that World War was no longer viable

69
00:04:45,000 --> 00:04:49,000
because the amount of technology we had made it a war that no one could win,

70
00:04:49,000 --> 00:04:52,000
we still haven't succeeded at nuclear disarmament.

71
00:04:52,000 --> 00:04:57,000
We did some very limited nuclear disarmament success while doing nuclear arms races at the same time

72
00:04:57,000 --> 00:05:01,000
and we went from two countries with nukes to more countries with better nukes.

73
00:05:01,000 --> 00:05:03,000
And that's simultaneous to that.

74
00:05:03,000 --> 00:05:06,000
Every new type of tech that has emerged has created an arms race.

75
00:05:06,000 --> 00:05:08,000
We haven't been able to prevent any of those.

76
00:05:08,000 --> 00:05:12,000
And the major tragedy of the commons issues like climate change and overfishing

77
00:05:12,000 --> 00:05:17,000
and dead zones in the oceans and microplastics in the oceans and biodiversity loss,

78
00:05:17,000 --> 00:05:19,000
we haven't been able to solve those either.

79
00:05:19,000 --> 00:05:25,000
And so rather than just think about this as like an overwhelming number of totally separate issues,

80
00:05:25,000 --> 00:05:34,000
the question of why are the patterns of human behavior

81
00:05:34,000 --> 00:05:36,000
as we increase our total technological capacity,

82
00:05:36,000 --> 00:05:40,000
why are they increasing catastrophic risk and why are we not solving them well?

83
00:05:40,000 --> 00:05:43,000
Are there underlying patterns that we could think of as,

84
00:05:43,000 --> 00:05:47,000
as you mentioned, generator functions of the catastrophic risk,

85
00:05:47,000 --> 00:05:49,000
generator functions of our inability to solve them,

86
00:05:49,000 --> 00:05:52,000
that if we were to identify those and work at that level,

87
00:05:52,000 --> 00:05:55,000
we could solve all of the expressions or symptoms.

88
00:05:55,000 --> 00:05:58,000
And if we don't work at that level, we might not be able to solve any of them.

89
00:05:58,000 --> 00:06:01,000
And again, people have been thinking about this for a long time,

90
00:06:01,000 --> 00:06:03,000
kind of notice these issues.

91
00:06:03,000 --> 00:06:08,000
They notice that you try to solve a,

92
00:06:08,000 --> 00:06:10,000
like the first one I noticed when I was a kid

93
00:06:10,000 --> 00:06:14,000
was trying to solve an elephant poaching issue in one particular region of Africa

94
00:06:14,000 --> 00:06:20,000
that didn't address the poverty of the people that had no mechanism other than black market on poaching,

95
00:06:20,000 --> 00:06:22,000
didn't address people's mindset towards animals,

96
00:06:22,000 --> 00:06:25,000
didn't address a macroeconomy that created poverty at scale.

97
00:06:25,000 --> 00:06:29,000
So when the laws were put in place and the fences were put in place

98
00:06:29,000 --> 00:06:31,000
to protect those elephants in that area better,

99
00:06:31,000 --> 00:06:34,000
the poachers moved to poaching other animals,

100
00:06:34,000 --> 00:06:38,000
particularly in that situation rhinos and gorillas

101
00:06:38,000 --> 00:06:40,000
that were both more endangered than the elephants had been.

102
00:06:40,000 --> 00:06:44,000
So you moved a problem from one area to another and actually a more sensitive area.

103
00:06:44,000 --> 00:06:47,000
And we see this with, well, can we solve hunger

104
00:06:47,000 --> 00:06:51,000
by bringing commercial agriculture to parts of the world that don't have it

105
00:06:51,000 --> 00:06:54,000
so that the people don't either not have food or we have to ship them food,

106
00:06:54,000 --> 00:06:58,000
but if it's commercial agriculture based on the kind of unsustainable,

107
00:06:58,000 --> 00:07:01,000
environmentally unsustainable agricultural processes

108
00:07:01,000 --> 00:07:04,000
that lead to huge amounts of nitrogen runoff going into river deltas

109
00:07:04,000 --> 00:07:06,000
that are causing dead zones in the ocean

110
00:07:06,000 --> 00:07:10,000
that can actually collapse the biosphere's capacity to support life faster

111
00:07:10,000 --> 00:07:13,000
than we're solving for a short-term issue that's important

112
00:07:13,000 --> 00:07:15,000
and driving even worse long-term issues.

113
00:07:15,000 --> 00:07:21,000
We see that many of the reasons people who oppose climate change solutions

114
00:07:21,000 --> 00:07:24,000
in the West oppose them is because,

115
00:07:24,000 --> 00:07:28,000
not because they have even really deeply engaged in the underlying science

116
00:07:28,000 --> 00:07:31,000
and say the climate change isn't real,

117
00:07:31,000 --> 00:07:33,000
that will oftentimes be what's said,

118
00:07:33,000 --> 00:07:36,000
but because the solution itself seems like it'll cause problems

119
00:07:36,000 --> 00:07:39,000
to other areas that they're paying attention to that seem even more critical to them.

120
00:07:39,000 --> 00:07:43,000
So if the solution involves some kind of carbon tax

121
00:07:43,000 --> 00:07:48,000
or something that would decrease GDP for the countries that agree to it

122
00:07:48,000 --> 00:07:51,000
and some other countries don't agree to it,

123
00:07:51,000 --> 00:07:53,000
and let's say in this particular case,

124
00:07:53,000 --> 00:07:56,000
the model that many people have is western countries agree to it,

125
00:07:56,000 --> 00:07:59,000
their GDP growth decreases, China doesn't agree to it,

126
00:07:59,000 --> 00:08:03,000
and there's already a very, very close neck-in-neck fight

127
00:08:03,000 --> 00:08:06,000
for who controls power in the 21st century.

128
00:08:06,000 --> 00:08:09,000
Are we seeding the world to Chinese control

129
00:08:09,000 --> 00:08:12,000
that many people would think it has less civil liberties

130
00:08:12,000 --> 00:08:15,000
and is more authoritarian in its nature?

131
00:08:15,000 --> 00:08:21,000
Or some people's answer to climate change is what we just have to use less energy,

132
00:08:21,000 --> 00:08:24,000
but when you understand that energy correlates directly to GDP

133
00:08:24,000 --> 00:08:27,000
and when GDP goes down, it affects poverty,

134
00:08:27,000 --> 00:08:29,000
people in extreme poverty first and worst,

135
00:08:29,000 --> 00:08:33,000
and wars increase because people who have desire to get more

136
00:08:33,000 --> 00:08:35,000
end up going zero sum on each other,

137
00:08:35,000 --> 00:08:38,000
and only when it's very positive sum does that not happen.

138
00:08:38,000 --> 00:08:40,000
You see all these intricate theory of trade-off,

139
00:08:40,000 --> 00:08:43,000
so we can't see that the problem is climate change.

140
00:08:43,000 --> 00:08:47,000
Everybody knows the problem of climate change seems like a big thing,

141
00:08:47,000 --> 00:08:52,000
but you've got to look at climate change plus the macroeconomic issues

142
00:08:52,000 --> 00:08:54,000
that would affect the poorest people

143
00:08:54,000 --> 00:08:56,000
and that would increase the chance of war

144
00:08:56,000 --> 00:09:01,000
and the geopolitical dynamics between the West and China,

145
00:09:01,000 --> 00:09:04,000
and the enforcement dynamics of international agreement.

146
00:09:04,000 --> 00:09:08,000
When you start to recognize that the problem is that suite of things together,

147
00:09:08,000 --> 00:09:12,000
in a way it seems, well, that's too hard, we can't even begin to focus on it.

148
00:09:12,000 --> 00:09:14,000
I would say that that's actually easier

149
00:09:14,000 --> 00:09:17,000
because trying to solve climate change on its own is actually impossible,

150
00:09:17,000 --> 00:09:21,000
because if you're trying to solve something

151
00:09:21,000 --> 00:09:24,000
that is going to externalize harm to some other thing,

152
00:09:24,000 --> 00:09:28,000
maybe you solve that thing,

153
00:09:28,000 --> 00:09:30,000
but you find out that you're in a worse position,

154
00:09:30,000 --> 00:09:33,000
so I would say that it's impossible to actually improve the world that way,

155
00:09:33,000 --> 00:09:37,000
or half the world that is paying attention to that other thing disagrees with you

156
00:09:37,000 --> 00:09:40,000
so vehemently that all the energy goes into infighting

157
00:09:40,000 --> 00:09:42,000
and whatever some part of the world is trying to organize to do,

158
00:09:42,000 --> 00:09:45,000
the other part of the world is doing everything they can to resist from happening,

159
00:09:45,000 --> 00:09:48,000
then all the creative energy just burns up as heat

160
00:09:48,000 --> 00:09:50,000
and we don't actually accomplish anything.

161
00:09:50,000 --> 00:09:55,000
So I would say that the way we're trying to solve the problems is actually mostly impossible.

162
00:09:55,000 --> 00:09:58,000
It either solves it in a very narrow way

163
00:09:58,000 --> 00:10:01,000
while externalizing harm and causing worse problems,

164
00:10:01,000 --> 00:10:05,000
or it makes it impossible to solve it all because it drives polarization.

165
00:10:05,000 --> 00:10:08,000
And so going to the level at which the problems interconnect

166
00:10:08,000 --> 00:10:11,000
where that which everybody cares about is being factored

167
00:10:11,000 --> 00:10:13,000
and where you're not externalizing other problems

168
00:10:13,000 --> 00:10:16,000
while it seems more complex is actually possible.

169
00:10:16,000 --> 00:10:18,000
Impossible is easier than impossible.

170
00:10:18,000 --> 00:10:23,000
And so it's not just that there's a lot of issues, right?

171
00:10:23,000 --> 00:10:25,000
There are a lot of issues,

172
00:10:25,000 --> 00:10:29,000
and just that the issues are both more consequential at greater scope

173
00:10:29,000 --> 00:10:31,000
and moving faster than previous issues

174
00:10:31,000 --> 00:10:34,000
because of the nature of exponentiating technology.

175
00:10:34,000 --> 00:10:35,000
That's part of it.

176
00:10:35,000 --> 00:10:37,000
It's not just that the problems are all interconnected.

177
00:10:37,000 --> 00:10:41,000
It's also that they do have underlying drivers that have to be addressed,

178
00:10:41,000 --> 00:10:44,000
otherwise a symptomatic only approach doesn't work.

179
00:10:44,000 --> 00:10:48,000
The first underlying driver that when people look at it they generally see

180
00:10:48,000 --> 00:10:55,000
is they see things like structural perverse incentive built into macroeconomics,

181
00:10:55,000 --> 00:11:00,000
that the elephant dead is worth more than the elephant alive is,

182
00:11:00,000 --> 00:11:02,000
and so is the rhino, and so is the...

183
00:11:02,000 --> 00:11:07,000
And so how do you have a situation where that's the nature of incentive,

184
00:11:07,000 --> 00:11:10,000
where you're incentivizing an activity and then trying to bind it,

185
00:11:10,000 --> 00:11:12,000
or keep it from happening?

186
00:11:12,000 --> 00:11:14,000
And the same would be true with overfishing,

187
00:11:14,000 --> 00:11:17,000
as long as live fish are worth nothing and dead fish are worth more.

188
00:11:17,000 --> 00:11:22,000
There's something fundamentally perverse about the nature of the economic incentive.

189
00:11:22,000 --> 00:11:29,000
And the same is true that when we have war and there's more military manufacturing, GDP goes up.

190
00:11:29,000 --> 00:11:33,000
And when there's more addiction and people are buying the supply of their addiction, GDP goes up.

191
00:11:33,000 --> 00:11:36,000
And when there are more sick people paying for health care, cost GDP goes up.

192
00:11:36,000 --> 00:11:40,000
So it's obviously a perverse kind of metric.

193
00:11:40,000 --> 00:11:46,000
So anytime someone can fiscally advantage themselves or a corporation can

194
00:11:46,000 --> 00:11:53,000
in a way that either directly causes harm or indirectly externalizes harm,

195
00:11:53,000 --> 00:11:55,000
we have to fundamentally solve that.

196
00:11:55,000 --> 00:11:59,000
If there's something like 70 trillion dollars a day of activity happening

197
00:11:59,000 --> 00:12:02,000
that is a decentralized system of incentive,

198
00:12:02,000 --> 00:12:06,000
that is incenting people to do things that are directly or indirectly causing harm,

199
00:12:06,000 --> 00:12:12,000
there's really nothing we can do with some billions of dollars of nonprofit or state

200
00:12:12,000 --> 00:12:14,000
or whatever money that is going to solve that thing.

201
00:12:14,000 --> 00:12:18,000
So we have to say, well, what changes at the level of macroeconomics need to happen

202
00:12:18,000 --> 00:12:23,000
where the incentive of individuals and the incentive of corporations and the incentive of nations

203
00:12:23,000 --> 00:12:26,000
is more well aligned with the well-being and the incentive of others.

204
00:12:26,000 --> 00:12:30,000
And so we're less fundamentally rivalrous in the nature of our incentive.

205
00:12:30,000 --> 00:12:37,000
So we can see that underneath heaps of the problems, structures of macroeconomic incentive are there.

206
00:12:37,000 --> 00:12:40,000
That's kind of maybe the first one that most people see.

207
00:12:40,000 --> 00:12:44,000
We can go deeper to seeing that even as an expression,

208
00:12:44,000 --> 00:12:49,000
because whether it's a economic incentive for a corporation or whether it's a power incentive,

209
00:12:49,000 --> 00:12:52,000
a political power incentive or a political party or for a country,

210
00:12:52,000 --> 00:12:57,000
they're both instantiations of rivalrous-type dynamics that end up driving arms races,

211
00:12:57,000 --> 00:13:01,000
because if you win at a rivalrous dynamic, the other side reverse-engineers your tech,

212
00:13:01,000 --> 00:13:05,000
figures out how to make better versions, comes back, which creates an exponentiation in warfare

213
00:13:05,000 --> 00:13:11,000
and eventually exponential warfare becomes self-terminating on a finite planet.

214
00:13:11,000 --> 00:13:14,000
Exponential externalities also become self-terminating.

215
00:13:14,000 --> 00:13:19,000
So if we want to say, what are the underlying generator functions of catastrophic risk?

216
00:13:19,000 --> 00:13:24,000
First, maybe just to make clear, the catastrophic risk landscape.

217
00:13:24,000 --> 00:13:29,000
Is this all right if we do a brief aside on that?

218
00:13:29,000 --> 00:13:30,000
Yeah, let's do it.

219
00:13:30,000 --> 00:13:34,000
I think what we should do, let's do that, and then let's recap just what these structures are.

220
00:13:34,000 --> 00:13:39,000
People are tracking each of these components, because you've already mentioned a few different things.

221
00:13:39,000 --> 00:13:46,000
The first thing is just many listeners might hear what you're sharing as an overwhelming set of problems,

222
00:13:46,000 --> 00:13:48,000
and I think it's just to recap.

223
00:13:48,000 --> 00:13:53,000
It's important people understand that it's overwhelming if you're not using a problem-solving framework

224
00:13:53,000 --> 00:13:56,000
that allows you to see the interconnected nature of those problems,

225
00:13:56,000 --> 00:13:59,000
because if you solve them with the limited tools we have now,

226
00:13:59,000 --> 00:14:03,000
let's just solve the social media problem by pulling one lever and changing one business model of one company,

227
00:14:03,000 --> 00:14:09,000
or banning TikTok, but then you get 20 other TikToks that come and sit in its place with the same perverse incentive of addiction,

228
00:14:09,000 --> 00:14:13,000
the same rival risk dynamic competing for human attention.

229
00:14:13,000 --> 00:14:15,000
We're going to end up perpetuating those problems.

230
00:14:15,000 --> 00:14:18,000
And so just to sort of maybe recap some of that for listeners,

231
00:14:18,000 --> 00:14:21,000
and I think maybe let you continue with the other generator function.

232
00:14:21,000 --> 00:14:24,000
Let's just make sure that people really get those frameworks.

233
00:14:24,000 --> 00:14:27,000
I think it's really important.

234
00:14:27,000 --> 00:14:33,000
Yeah, I mean, in the case that you in Center for Humane Technology have brought so much attention to,

235
00:14:33,000 --> 00:14:40,000
with regard to the attention harvesting and directing economy,

236
00:14:40,000 --> 00:14:50,000
it's fair to say that it probably was not Facebook or Google's goal to create the type of effects that they had.

237
00:14:50,000 --> 00:14:52,000
Those were unintended externalities.

238
00:14:52,000 --> 00:14:53,000
They were second order effects.

239
00:14:53,000 --> 00:14:55,000
But they were trying to solve problems, right?

240
00:14:55,000 --> 00:14:59,000
Like, let's solve the problem if we're Google of organizing the world's information and making better search.

241
00:14:59,000 --> 00:15:01,000
That seems like a pretty good thing to do.

242
00:15:01,000 --> 00:15:04,000
And let's solve the problem of making it freely available to everybody.

243
00:15:04,000 --> 00:15:06,000
That seems like a pretty good thing to do.

244
00:15:06,000 --> 00:15:09,000
And with the AdModel, we can make it freely available to everyone.

245
00:15:09,000 --> 00:15:14,000
And let's recognize that only if we get a lot of data will our machine learning get better.

246
00:15:14,000 --> 00:15:17,000
And so we need to actually get everybody on this thing.

247
00:15:17,000 --> 00:15:19,000
So we definitely have to make it free.

248
00:15:19,000 --> 00:15:24,000
And then we get this kind of recursive process.

249
00:15:24,000 --> 00:15:32,000
Well, then the nature of the AdModel, doing time on site optimization and stuff I'm not going to get into because you've addressed it so well,

250
00:15:32,000 --> 00:15:39,000
ends up appealing to people's existing biases rather than correcting their bias,

251
00:15:39,000 --> 00:15:42,000
appealing to their tribal in-group identities rather than correcting them,

252
00:15:42,000 --> 00:15:45,000
and appealing to limbic hijacks rather than helping people transcend them.

253
00:15:45,000 --> 00:15:54,000
And as a result, you end up actually breaking the social solidarity and epistemic capacity necessary for democracy.

254
00:15:54,000 --> 00:15:57,000
So it's like, oh, let's solve the search problem.

255
00:15:57,000 --> 00:15:58,000
That seems like a nice thing.

256
00:15:58,000 --> 00:16:03,000
The side effect is we're going to destroy democracy and open societies in the process and all those other things.

257
00:16:03,000 --> 00:16:08,000
Like, those are examples of solving a problem in a way that is externalizing harm,

258
00:16:08,000 --> 00:16:10,000
causing other problems that are oftentimes worse.

259
00:16:10,000 --> 00:16:15,000
And so let's just focus on the opportunity.

260
00:16:15,000 --> 00:16:21,000
And just to say, typically, this will get accounted for as, oh, this is just an unintended consequence.

261
00:16:21,000 --> 00:16:23,000
But there's some other generator functions I think we should outline.

262
00:16:23,000 --> 00:16:29,000
I mean, if YouTube and Google didn't personalize search results and what video to show you next,

263
00:16:29,000 --> 00:16:36,000
and the other guy did on TikTok starts personalizing, they're caught in a race to the bottom of whoever personalizes more for the best limbic hijack.

264
00:16:36,000 --> 00:16:40,000
And so just to sort of connect some of those things together for listeners.

265
00:16:40,000 --> 00:16:44,000
So you mentioned race to the bottom, and obviously CHT has discussed this before,

266
00:16:44,000 --> 00:16:51,000
and this is a key piece of the game theoretic challenge and global coordination.

267
00:16:51,000 --> 00:16:57,000
And the two primary ways it expresses itself is arms races and tragedy of the commons.

268
00:16:57,000 --> 00:17:07,000
And the tragedy of the commons scenario is if we don't overfish that area of virgin ocean,

269
00:17:07,000 --> 00:17:14,000
but we can't control that someone else doesn't, because how do we do enforcement if they're also a nuclear country?

270
00:17:14,000 --> 00:17:19,000
That's a tricky thing, right? How do you do enforcement on nuclear countries, equipped countries?

271
00:17:19,000 --> 00:17:23,000
So us not doing it doesn't mean that the fish don't all get taken.

272
00:17:23,000 --> 00:17:28,000
It just means that they grow their populations and their GDP faster, which they will use rivalrously.

273
00:17:28,000 --> 00:17:33,000
So we might as well do it. In fact, we might as well race to do it faster than they do.

274
00:17:33,000 --> 00:17:35,000
Those are the tragedy of the commons type issues.

275
00:17:35,000 --> 00:17:41,000
The arms race version is if we can't ensure that they don't build AI weapons or they don't build surveillance tech

276
00:17:41,000 --> 00:17:47,000
and they get increased near-term power from doing so, we just have to race to get there before them.

277
00:17:47,000 --> 00:17:49,000
That's the arms race type thing.

278
00:17:49,000 --> 00:17:54,000
It just happens to be that while that makes sense for each agent on their own in the short term,

279
00:17:54,000 --> 00:17:59,000
it creates global dynamics for the whole and the long term that self-terminate,

280
00:17:59,000 --> 00:18:04,000
because you can't run exponential externality on a finite planet.

281
00:18:04,000 --> 00:18:09,000
That's the tragedy of the commons one, and you can't run exponential arms races and exponential conflict on a finite planet.

282
00:18:09,000 --> 00:18:15,000
So the thing that has always made sense, which is just keep winning at the arms races,

283
00:18:15,000 --> 00:18:22,000
has had a world where we've had lots of wars increasing in their scale and lots of environmental damage.

284
00:18:22,000 --> 00:18:24,000
We started desertification thousands of years ago.

285
00:18:24,000 --> 00:18:30,000
It's just has been a long, slow, exponential curve that really started to pick up with the industrial revolution

286
00:18:30,000 --> 00:18:36,000
and is now really verticalizing with the digital revolution and the cumulative harm of that kind of thing becomes impossible now.

287
00:18:36,000 --> 00:18:45,000
So basically, with the environmental destruction, with the wars and with the class subjugation things that civilization has had in the past,

288
00:18:45,000 --> 00:18:52,000
pretty much anyone would say we have not been the best stewards of power, and technology is increasing our power.

289
00:18:52,000 --> 00:18:57,000
Exponential tech means tech that makes better versions of itself, so you get an exponent on the curve.

290
00:18:57,000 --> 00:19:00,000
We're now in a process where that's a very, very rapid.

291
00:19:00,000 --> 00:19:04,000
Computation gives the ability to design better systems of computation.

292
00:19:04,000 --> 00:19:13,000
Computation and AI applied to biological big data and protein folding gives the ability to do that on biotech and on and on, right?

293
00:19:13,000 --> 00:19:22,000
So we could say the central question of our time is if we've been poor stewards of power for a long time and that's always caused problems,

294
00:19:22,000 --> 00:19:26,000
but the problems now become existential, they become catastrophic, we can't keep doing that.

295
00:19:26,000 --> 00:19:35,000
How do we become adequately good stewards of exponential power in time, right?

296
00:19:35,000 --> 00:19:43,000
How do we develop the good decision-making processes, the wisdom necessary to be able to be stewards of that much power?

297
00:19:43,000 --> 00:19:47,000
I think that's a fair way to talk about the central thing.

298
00:19:47,000 --> 00:19:55,000
If it's okay, the thread we were about to get to, I think, is a good one, which was the history of catastrophic risk coming up to now,

299
00:19:55,000 --> 00:20:05,000
is that before World War II, catastrophic risk was actually a real part of people's experience.

300
00:20:05,000 --> 00:20:14,000
It was just always local, but an individual kingdom might face existential risk in a war where they would lose.

301
00:20:14,000 --> 00:20:22,000
And so the people faced those kinds of reality, and in fact, one thing that we can see when you read books like The Collapse of Complex Societies by Joseph Tainter

302
00:20:22,000 --> 00:20:34,000
and any study of history is that all the great civilizations don't still exist, which means that one of the first things we can say about civilizations is that they die.

303
00:20:34,000 --> 00:20:36,000
They have a finite lifespan on them.

304
00:20:36,000 --> 00:20:41,000
One of the interesting things we can find is that they usually die from self-induced causes.

305
00:20:41,000 --> 00:20:49,000
They either over-consume the resources and then stop being able to meet the needs of the people through unrenewable environmental dynamics, and that's old.

306
00:20:49,000 --> 00:21:01,000
Or they have increasing border conflicts that lead to enmity, that has more arms race activity coming back at them,

307
00:21:01,000 --> 00:21:11,000
or they have increasing institutional decay of their internal coordination processes that leads to inability to operate quickly in those types of things.

308
00:21:11,000 --> 00:21:22,000
So we can say that it's the fundamentally most all civilizations collapse in a way that is based on generally self-terminating dynamics.

309
00:21:22,000 --> 00:21:32,000
And we see that even when they were overtaken by armies, oftentimes they were armies that were smaller than ones they had defended against successfully at earlier peaks in their adaptive capacity.

310
00:21:32,000 --> 00:21:37,000
Okay, so catastrophic risk has been a real thing. It's just been local.

311
00:21:37,000 --> 00:21:48,000
And it wasn't until World War II that we had enough technological power that catastrophic risk became a global possibility for the first time ever.

312
00:21:48,000 --> 00:21:55,500
And this is a really important thing to get because the world before World War II and the world after was different and kind so fundamentally.

313
00:21:55,500 --> 00:22:04,000
And this is why when you study history, so much of what you're studying is history of warfare, of neighboring kingdoms and neighboring empires fighting.

314
00:22:04,000 --> 00:22:13,500
And because the wars were fundamentally winnable, at least for some, right? They weren't winnable for all the people who died, but at least for some.

315
00:22:13,500 --> 00:22:20,500
And with World War II and the development of the bomb became the beginning of wars that were no longer winnable.

316
00:22:20,500 --> 00:22:33,500
And that if we employed our full tech and continued the arms race even beyond the existing tech, it's a war where when lose becomes omni-lose-lose at that particular level of power.

317
00:22:33,500 --> 00:22:41,500
And so that created the need to do something that humanity had never done, which was that the major superpowers didn't war.

318
00:22:41,500 --> 00:22:46,500
The whole history of the world, the history of the thing we call civilization, they always did.

319
00:22:46,500 --> 00:22:54,500
And so we made an entire world system, a globalized world system that was with the aim of preventing World War III.

320
00:22:54,500 --> 00:23:03,500
So we could have non-kinetic wars, and we did, right? Increasingly you can see from World War II to now a movement to unconventional warfare,

321
00:23:03,500 --> 00:23:09,500
narrative and information warfare, economic, diplomatic warfare, those types of things, resource warfare.

322
00:23:09,500 --> 00:23:12,500
And you could, if you were going to have a physical kinetic war, it had to be a proxy war.

323
00:23:12,500 --> 00:23:19,500
But to have a proxy war, that also required narrative warfare to be able to create a justification for it.

324
00:23:19,500 --> 00:23:29,500
But also to be able to prevent the war, so the post-World War II Bretton Woods Mutually Assured Destruction United Nations World

325
00:23:29,500 --> 00:23:35,500
was a solution to be able to steward that level of tech without destroying ourselves.

326
00:23:35,500 --> 00:23:42,500
And it really was a reorganization of the world. It was a whole new advent of social technologies or social systems,

327
00:23:42,500 --> 00:23:46,500
just like the U.S. was new social technologies or social systems coming out of the Industrial Revolution.

328
00:23:46,500 --> 00:23:50,500
The Industrial Revolution ended up giving rise to kind of nation-state democracies.

329
00:23:50,500 --> 00:23:56,500
The nuclear revolution in this way kind of gave rise to this I.G.O. intergovernmental world.

330
00:23:56,500 --> 00:24:01,500
And it was predicated on a few things. Mutually Assured Destruction was critical.

331
00:24:01,500 --> 00:24:09,500
Globalization and economic trade was critical that we, if the computer that we're talking on and the phone that we talk on

332
00:24:09,500 --> 00:24:14,500
is made over six continents and no countries can make them on our own, we don't want to blow them up and ruin their infrastructure

333
00:24:14,500 --> 00:24:20,500
because we depend upon it. So let's create radical economic interdependence so we have more economic incentive to cooperate.

334
00:24:20,500 --> 00:24:29,500
Makes sense. And let's grow the materials economy so fast through this globalization

335
00:24:29,500 --> 00:24:34,500
that the world gets to be very positive GDP and gets to be very positive sum

336
00:24:34,500 --> 00:24:37,500
so that everybody can have more without having to take each other's stuff.

337
00:24:37,500 --> 00:24:41,500
That was kind of like the basis of that whole world system.

338
00:24:41,500 --> 00:24:45,500
And we can see that we've had wars, but they've been proxy wars and cold wars.

339
00:24:45,500 --> 00:24:49,500
They haven't been major superpower wars and they've been unconventional ones.

340
00:24:49,500 --> 00:24:56,500
But we haven't had a kinetic World War III. We have had increase of prosperity of certain kinds.

341
00:24:56,500 --> 00:25:03,500
75 years give or take. Now we're at a point where that radically positive sum economy

342
00:25:03,500 --> 00:25:09,500
that required an exponential growth of the economy, which means of the materials economy,

343
00:25:09,500 --> 00:25:15,500
and it's a linear materials economy that unrenewably takes resources from the earth faster than they can reproduce themselves

344
00:25:15,500 --> 00:25:20,500
and turns them into waste faster than they can process themselves, has led to the planetary boundaries issue

345
00:25:20,500 --> 00:25:28,500
where it's not just climate change or overfishing or dead zones in the ocean or microplastics or species extinction

346
00:25:28,500 --> 00:25:31,500
or peak phosphorus. It's a hundred things, right?

347
00:25:31,500 --> 00:25:38,500
There's all these planetary boundaries so we can't keep doing exponential linear materials economy.

348
00:25:38,500 --> 00:25:43,500
That thing has come to an end because now that drives its own set of catastrophic risks.

349
00:25:43,500 --> 00:25:48,500
We see that the radical interconnection of the world was good in terms of will not bomb each other

350
00:25:48,500 --> 00:25:55,500
but it also created very high fragility because what it meant is a failure anywhere could cascade to failures everywhere

351
00:25:55,500 --> 00:26:03,500
because of that much dependence. So we can see with COVID we had what was a local issue to an area of China

352
00:26:03,500 --> 00:26:08,500
but because of how interconnected the world is with travel it became a global issue at the pandemic level

353
00:26:08,500 --> 00:26:16,500
and it also became an issue where to shut down the transmission of the virus we shut down travel

354
00:26:16,500 --> 00:26:21,500
which also meant shut down supply chains which meant so many things, right?

355
00:26:21,500 --> 00:26:29,500
And very fundamental things that weren't obvious to people at first like that countries agriculture depends upon the shipment of pesticides that they don't have stored

356
00:26:29,500 --> 00:26:34,500
and so we got these swarms of locusts because of not having the pesticides which damaged the food supply

357
00:26:34,500 --> 00:26:43,500
and shipments of fertilizer and shipments of seed so we end up seeing a drive of food insecurity of extreme poverty

358
00:26:43,500 --> 00:26:47,500
at a scale of death threat that is larger than the COVID death threat was.

359
00:26:47,500 --> 00:26:52,500
As a second order effect of our problem we were trying to solve the problem of don't spread COVID

360
00:26:52,500 --> 00:26:57,500
and the solution had these massive second third order effects that are still playing out, right?

361
00:26:57,500 --> 00:27:04,500
And that was a relatively benign pandemic a relatively benign catastrophe compared to a lot of scenarios we can model out

362
00:27:04,500 --> 00:27:10,500
so we can say okay well we like the benefit of interconnectivity so we're not invested in bombing each other

363
00:27:10,500 --> 00:27:13,500
but we need more anti-fragility in the system.

364
00:27:13,500 --> 00:27:20,500
And then the mutually assured destruction thing doesn't work anymore because we don't have two countries with one catastrophe weapon

365
00:27:20,500 --> 00:27:24,500
that's really really hard to make and easy to monitor because there's not that many places that have uranium

366
00:27:24,500 --> 00:27:27,500
it's hard to enrich it you can monitor by satellites.

367
00:27:27,500 --> 00:27:33,500
We have lots of countries with nukes but we also have lots of new catastrophe weapons that are not hard to make

368
00:27:33,500 --> 00:27:37,500
that are not easy to monitor that don't even take nation states to make them.

369
00:27:37,500 --> 00:27:44,500
So if you have many many actors of different kinds with many different types of catastrophe weapons

370
00:27:44,500 --> 00:27:46,500
how do you do mutually assured destruction?

371
00:27:46,500 --> 00:27:48,500
You can't do it the same way.

372
00:27:48,500 --> 00:27:56,500
And so what we find is that the set of solutions post World War II that kept us from blowing ourselves up with our new power

373
00:27:56,500 --> 00:28:04,500
lasted for a while but those set of solutions have ended and they have now created their own set of new problems.

374
00:28:04,500 --> 00:28:10,500
So there's kind of the catastrophic risk world before World War II

375
00:28:10,500 --> 00:28:14,500
the catastrophic risk world from World War II till now and then the new thing.

376
00:28:14,500 --> 00:28:19,500
So the new thing says we have to have solutions that deal with the planetary boundary issues

377
00:28:19,500 --> 00:28:24,500
that deal with global fragility issues and that deal with the exponential tech issues

378
00:28:24,500 --> 00:28:28,500
both in terms of the way exponential tech can be intentionally used to cause harm

379
00:28:28,500 --> 00:28:31,500
i.e. exponential tech empowered warfare and unintentionally

380
00:28:31,500 --> 00:28:37,500
i.e. exponential tech empowered externalities and even just totally unanticipated types of mistakes

381
00:28:37,500 --> 00:28:42,500
the Facebook Google type problem multiplied by AGI and things like that.

382
00:28:42,500 --> 00:28:49,500
And so when we talk about what the catastrophic risk landscape is like that's the landscape

383
00:28:49,500 --> 00:28:55,500
the metacrisis is how do we solve all of that and recognizing that our problem solving mechanisms

384
00:28:55,500 --> 00:29:01,500
haven't even been able to solve the problems we've had for the last many years let alone prevent these things

385
00:29:01,500 --> 00:29:07,500
and so the central orienting question it's like the UNS-17 Sustainable Development Goals

386
00:29:07,500 --> 00:29:13,500
there's really one that must supersede them all which is develop the capacity for global coordination

387
00:29:13,500 --> 00:29:18,500
that can solve global problems. If you get that one you get all the other ones

388
00:29:18,500 --> 00:29:26,500
if you don't get that one you don't get any of the other ones and so we can talk about how do we do that

389
00:29:26,500 --> 00:29:32,500
but that becomes the central imperative for the world at this time.

390
00:29:32,500 --> 00:29:40,500
So you're saying a whole bunch of things and one thing that comes to mind here

391
00:29:40,500 --> 00:29:44,500
if I'm just reading back some of the things you've shared

392
00:29:44,500 --> 00:29:48,500
the development of the let's call it one of the first exponential technologies which is the nuclear bomb

393
00:29:48,500 --> 00:29:55,500
led to a new social system which was sort of the post Bretton Woods world of trying to stabilize

394
00:29:55,500 --> 00:29:59,500
that one exponential technology in the world in a way that would not be catastrophic

395
00:29:59,500 --> 00:30:02,500
and even there we weren't able to sort of make it all work

396
00:30:02,500 --> 00:30:07,500
and I think people should have maybe a list of some of the other exponential technologies

397
00:30:07,500 --> 00:30:12,500
because I want to make sure that phrase is defined for listeners and there's a lot of different ways

398
00:30:12,500 --> 00:30:17,500
that we've now not just created more exponential technologies but more decentralized exponential technologies

399
00:30:17,500 --> 00:30:23,500
and I think people should see Facebook and Google as exponential attention mapping

400
00:30:23,500 --> 00:30:28,500
or information driving technologies that are shaping the global information flows

401
00:30:28,500 --> 00:30:33,500
or the wiring diagram of the sort of global societal brain at scales that are exponential

402
00:30:33,500 --> 00:30:39,500
it's sort of a nuclear scale rewiring of the human civilization

403
00:30:39,500 --> 00:30:42,500
we couldn't do that with newspapers we couldn't do that with a printing press

404
00:30:42,500 --> 00:30:44,500
not at the scale speed et cetera that we have now

405
00:30:44,500 --> 00:30:47,500
so do you want to give maybe some more examples of exponential technologies

406
00:30:47,500 --> 00:30:51,500
because I think that's going to lead to we're going to need a new kinds of social systems

407
00:30:51,500 --> 00:30:57,500
to manage this different landscape of not just one exponential nuclear bomb but a landscape

408
00:30:57,500 --> 00:31:04,500
indulge me as I tell a story first that leads into it because it'll be a relevant framework

409
00:31:04,500 --> 00:31:09,500
obviously the bomb was central to World War II and the world system that came afterwards

410
00:31:09,500 --> 00:31:14,500
and what motivated our activity getting into it but it was not the only tech

411
00:31:14,500 --> 00:31:20,500
it was one new technology that was part of a suite of new technologies that could all be developed

412
00:31:20,500 --> 00:31:23,500
because of kind of the level science had gotten to

413
00:31:23,500 --> 00:31:30,500
and basically like physics and chemistry had gotten to the point that we could work on a nuclear bomb

414
00:31:30,500 --> 00:31:33,500
we could start to work on computation

415
00:31:33,500 --> 00:31:41,500
we could get things like the V2 rocket and rockets and a whole host of applied chemistry

416
00:31:41,500 --> 00:31:46,500
and one way of thinking about what World War II was

417
00:31:46,500 --> 00:31:51,500
one way of thinking about it but it's useful frame and I think it's a fair frame

418
00:31:51,500 --> 00:31:56,500
is that there were a few competing social ideologies at the time

419
00:31:56,500 --> 00:32:04,500
primarily kind of German fascism fascism socialism whatever you want to call it

420
00:32:04,500 --> 00:32:10,500
Soviet communism and Western liberalism something like that

421
00:32:10,500 --> 00:32:17,500
and that this new suite of technologies whoever kind of developed it and was able to implement it at scale first would win

422
00:32:17,500 --> 00:32:21,500
that social ideology would win because it's just so much more powerful

423
00:32:21,500 --> 00:32:24,500
if you have nukes and they have guns you're going to win right

424
00:32:24,500 --> 00:32:28,500
and Germans were actually ahead of both the US and the Soviets

425
00:32:28,500 --> 00:32:33,500
because of some things that they did to invest in education and tech development

426
00:32:33,500 --> 00:32:39,500
but that led both the Soviets and the US to really working to catch up as fast as they can

427
00:32:39,500 --> 00:32:44,500
and when the US finally figured it out which we were actually a little bit slow to right

428
00:32:44,500 --> 00:32:48,500
Einstein actually wrote a letter the Einstein solar letter that went to the US government

429
00:32:48,500 --> 00:32:52,500
saying now the science really does say that this thing could happen and the Germans could get it

430
00:32:52,500 --> 00:32:55,500
and you should focus on it and at first they didn't take them up on it

431
00:32:55,500 --> 00:33:00,500
it wasn't until the private sector actually nonprofit supported advanced it further

432
00:33:00,500 --> 00:33:05,500
that then the Manhattan Project was engaged in but then it was engaged in when they recognized the seriousness

433
00:33:05,500 --> 00:33:13,500
that there was an actual eminent existential risk to the nation and the whole Western ideology and whatever

434
00:33:13,500 --> 00:33:17,500
then it was an unlimited budget right it was a let's find all the smartest people in the world

435
00:33:17,500 --> 00:33:21,500
and let's bring them here and let's organize however we need to to make this thing happen

436
00:33:21,500 --> 00:33:26,500
and let's do it for all of the new areas of tech we're going to get the enigma machine and crack the enigma code

437
00:33:26,500 --> 00:33:30,500
we're going to get a v2 rocket we're going to figure out how to reverse engineer that in advanced rocketry

438
00:33:30,500 --> 00:33:34,500
we're going to do everything needed to make a nuclear bomb and then more advanced ones

439
00:33:34,500 --> 00:33:40,500
it was the biggest jump in technology ever in the history of the world in record history as we know it

440
00:33:40,500 --> 00:33:45,500
and it wasn't actually done by the market right it was done by the state that's a very important thing

441
00:33:45,500 --> 00:33:49,500
this idea that markets innovate and states don't innovate is just historically not true here

442
00:33:49,500 --> 00:33:55,500
this was state funds and state controlled operation in the same way that the Apollo project coming out of it was

443
00:33:55,500 --> 00:34:04,500
and a technological jump of that kind hasn't happened since

444
00:34:04,500 --> 00:34:11,500
so it's an important thing to understand but we can say though this is not a totally fair thing to say

445
00:34:11,500 --> 00:34:16,500
we can say that the US came out dominant in that technological race

446
00:34:16,500 --> 00:34:22,500
the US and the USSR both had a lot of capacity so that was the Cold War and then finally the US came out

447
00:34:22,500 --> 00:34:26,500
and so the post-World War II system was a US led system

448
00:34:26,500 --> 00:34:31,500
the UN was in the US the Bretton Wood system was pegged to the US dollar

449
00:34:31,500 --> 00:34:38,500
what I would say is that so it wasn't one type of tech

450
00:34:38,500 --> 00:34:43,500
it was the recognition that science had got to a place where there's going to be a whole suite of new tech

451
00:34:43,500 --> 00:34:49,500
and the new tech meant more power and whoever had the power would determine the next phase of the world

452
00:34:49,500 --> 00:34:54,500
and if we didn't like the social ideologies that were going to be guiding it

453
00:34:54,500 --> 00:34:59,500
of course we can also think of it as just who wanted to win at the game of power

454
00:34:59,500 --> 00:35:05,500
but from the philosophical argument if we didn't like the social ideologies then we have another social ideology

455
00:35:05,500 --> 00:35:06,500
get it

456
00:35:06,500 --> 00:35:11,500
what I would say is that there is an emerging suite of technologies now

457
00:35:11,500 --> 00:35:20,500
that is much more powerful in the total level of jump

458
00:35:20,500 --> 00:35:23,500
technological jump than the World War II suite was

459
00:35:23,500 --> 00:35:25,500
in fact orders of magnitude more

460
00:35:25,500 --> 00:35:32,500
and only those who are developing and employing exponential tech

461
00:35:32,500 --> 00:35:34,500
will have much of a say in the direction of the future

462
00:35:34,500 --> 00:35:38,500
because just from a real politic point of view that's where the power is

463
00:35:38,500 --> 00:35:42,500
and if you don't have the power you won't be able to oppose it

464
00:35:42,500 --> 00:35:45,500
and so what do we mean by exponential tech?

465
00:35:45,500 --> 00:35:47,500
there's a couple different ways of thinking about it

466
00:35:47,500 --> 00:35:50,500
just exponentially more powerful is a very simplistic way

467
00:35:50,500 --> 00:35:53,500
and in that definition nuclear is exponential tech

468
00:35:53,500 --> 00:36:00,500
but what we typically mean with exponential tech is tech that makes it possible to make better versions of itself

469
00:36:00,500 --> 00:36:03,500
so that there is like a compounding interest kind of curve

470
00:36:03,500 --> 00:36:07,500
the tech makes it easier to make a better version which makes it easier to make a better version

471
00:36:07,500 --> 00:36:12,500
and so we see that starting with computation really in a fundamental way

472
00:36:12,500 --> 00:36:17,500
because computation allows us to advance models of computation

473
00:36:17,500 --> 00:36:19,500
how do we make better computational substrates

474
00:36:19,500 --> 00:36:22,500
how do we get more transistors in a chip

475
00:36:22,500 --> 00:36:27,500
how do we make better arrangements of chip so we get GPUs and those types of things

476
00:36:27,500 --> 00:36:34,500
and so in this new suite of technology the center of it is computation

477
00:36:34,500 --> 00:36:39,500
the very very center of that is AI

478
00:36:39,500 --> 00:36:45,500
is kind of self-learning computation on large fields of data

479
00:36:45,500 --> 00:36:54,500
the other kind of software advances like advances in various meaningful advances in cryptography

480
00:36:54,500 --> 00:37:02,500
and big data and the ability to get data from sensors and you know sensor processing image recognition

481
00:37:02,500 --> 00:37:05,500
that is a part of that central suite

482
00:37:05,500 --> 00:37:12,500
and the application of that to the directing of attention and the directing of behavior by directing attention

483
00:37:12,500 --> 00:37:15,500
which you focused on very centrally

484
00:37:15,500 --> 00:37:19,500
then the next phase is the application of the tech

485
00:37:19,500 --> 00:37:24,500
the application of computation to increasing computational substrate

486
00:37:24,500 --> 00:37:29,500
so this is now the software advancing the hardware that can advance the total level of software

487
00:37:29,500 --> 00:37:33,500
so that's not just continuously better chips

488
00:37:33,500 --> 00:37:37,500
it's also quantum computing, photo computing, DNA computing, those other types of things

489
00:37:37,500 --> 00:37:41,500
and the other types of hardware that need to be part of that thing

490
00:37:41,500 --> 00:37:44,500
i.e. sensor tech in particular

491
00:37:44,500 --> 00:37:50,500
so that you can keep getting more data going into that system that can do big data machine learning on it

492
00:37:50,500 --> 00:37:55,500
then it's the application of that computation in AI specifically to physical tech

493
00:37:55,500 --> 00:38:01,500
so to nanotech, material sciences, biotech and even things like modeling how to do better nuclear

494
00:38:01,500 --> 00:38:05,500
and you know robotics and automation

495
00:38:05,500 --> 00:38:10,500
and so when you start thinking about better computational substrates running better software

496
00:38:10,500 --> 00:38:13,500
with more total data going in with better sensors in better robots

497
00:38:13,500 --> 00:38:17,500
you start getting the sense of what that whole suite of things looks like

498
00:38:18,500 --> 00:38:26,500
so that's the suite of things that I would say is what we would kind of call exponential tech

499
00:38:26,500 --> 00:38:33,500
and the reason why the term exponential is important is we don't think exponentially well

500
00:38:33,500 --> 00:38:38,500
our intuitions are bad for it because we think about how much progress was made over the last five years

501
00:38:38,500 --> 00:38:40,500
and we imagine there will be a similar amount over the next five years

502
00:38:40,500 --> 00:38:43,500
and that's not the way exponential curves work, right?

503
00:38:43,500 --> 00:38:48,500
and so it's very hard for us, our intuition was calibrated on the past

504
00:38:48,500 --> 00:38:55,500
and it's going to be miscalibrated for forecasting the total rate of change and the magnitude of change

505
00:38:58,500 --> 00:39:04,500
so to link this for one much more narrow aspect for our listeners who are familiar with social media

506
00:39:04,500 --> 00:39:10,500
and social dilemma and you're talking about sort of self-compounding systems that improve recursively like that

507
00:39:11,500 --> 00:39:18,500
if I'm TikTok or if I'm Facebook and I use data to figure out what's the thing to show you

508
00:39:18,500 --> 00:39:22,500
and that's going to keep you here for long since going to bypass your prefrontal cortex

509
00:39:22,500 --> 00:39:25,500
and go straight to your limbic system, your lizard brain

510
00:39:25,500 --> 00:39:28,500
well the better it gets at doing that and succeeding at that

511
00:39:28,500 --> 00:39:30,500
the more data it has to make a better prediction the next time

512
00:39:30,500 --> 00:39:32,500
but then a new user comes along who it's never seen before

513
00:39:32,500 --> 00:39:36,500
but hey they're clicking on exactly the same pattern of anorexia videos

514
00:39:36,500 --> 00:39:39,500
that we've seen these other 2 million users have that turn out to be teenage girls

515
00:39:39,500 --> 00:39:43,500
and it just happens to know that this other set of videos that are more anorexia videos

516
00:39:43,500 --> 00:39:45,500
are also going to work really really well

517
00:39:45,500 --> 00:39:48,500
so there's sort of a self-compounding loop that's learning not just from one person

518
00:39:48,500 --> 00:39:52,500
and getting a better version of hijacking your nervous system

519
00:39:52,500 --> 00:39:54,500
but learning across individuals

520
00:39:54,500 --> 00:39:57,500
and so now you get a new person coming in from developing countries

521
00:39:57,500 --> 00:40:02,500
never used TikTok before and they're just barely walking in for the front door the very first time

522
00:40:02,500 --> 00:40:05,500
it's sort of like when Coca-Cola goes to Southeast Asia for the first time

523
00:40:05,500 --> 00:40:10,500
and you get diabetes 10 years later because you refined all the techniques of marketing so effectively

524
00:40:10,500 --> 00:40:13,500
but now happening at scales that are automated with computation

525
00:40:13,500 --> 00:40:16,500
so what you're talking about is the impact of computation

526
00:40:16,500 --> 00:40:18,500
and learning on top of learning data on top of data

527
00:40:18,500 --> 00:40:21,500
and then cross-referencing look-alike models and all of this kind of thing

528
00:40:21,500 --> 00:40:24,500
you could apply to the domain at least that social dilemma

529
00:40:24,500 --> 00:40:28,500
watchers and people who are familiar with our work might be able to tie into

530
00:40:28,500 --> 00:40:34,500
Yeah, the more people you have in the system and the more data per person that you're able to harvest

531
00:40:34,500 --> 00:40:38,500
the more stuff you have for the machine learning to figure out patterns on

532
00:40:38,500 --> 00:40:42,500
which also means that the machine learning can provide things that the users want more

533
00:40:42,500 --> 00:40:46,500
even if it's manufactured want, right, even if it's manufactured demand

534
00:40:46,500 --> 00:40:49,500
which means that then more users will come and put more data in

535
00:40:49,500 --> 00:40:54,500
and it can specifically figure out how to manufacture the types of behavior that increase data collection

536
00:40:54,500 --> 00:41:00,500
and so you do get this recursive process on how many people, how much data, how good are the machine learning algorithms

537
00:41:00,500 --> 00:41:05,500
you know, that kind of thing and this is one of the reasons that we see these natural monopoly formations

538
00:41:05,500 --> 00:41:08,500
within these categories of tech

539
00:41:08,500 --> 00:41:12,500
and this is another reason that's important to understand like

540
00:41:12,500 --> 00:41:18,500
these types of self-reinforcing dynamics and things like network effects like Metcalf's law

541
00:41:18,500 --> 00:41:23,500
didn't exist when the Scottish Enlightenment was coming up with its ideas of capitalism

542
00:41:23,500 --> 00:41:27,500
and market and the healthy competition and markets and why that creates checks and balances on power

543
00:41:27,500 --> 00:41:31,500
they didn't exist, Adam Smith did not get to think about those things

544
00:41:31,500 --> 00:41:38,500
and so when you have a situation where the value of the network is proportional to the square of the

545
00:41:38,500 --> 00:41:45,500
people coming into the network then you're incented to keep it free up front, maximize addiction, drive behavior into the system

546
00:41:45,500 --> 00:41:54,500
and then once you get to the kind of breakaway point on the return of that thing

547
00:41:54,500 --> 00:41:58,500
it becomes nearly impossible for anyone else to come in and overtake that thing

548
00:41:58,500 --> 00:42:01,500
so you get a power law distribution in each vertical

549
00:42:01,500 --> 00:42:05,500
you get one online market that is bigger than all the other online markets

550
00:42:05,500 --> 00:42:09,500
one video player that's bigger than all the other video players, one search, one social network one

551
00:42:09,500 --> 00:42:15,500
and that's not because of a government monopoly, that's because of this kind of natural tech monopoly

552
00:42:15,500 --> 00:42:20,500
this also means that when we created the laws around monopolies they don't apply to this thing

553
00:42:20,500 --> 00:42:26,500
and yet this thing still has the same spirit of power concentration and unchecked power

554
00:42:26,500 --> 00:42:32,500
that our ideas of monopoly had but it's able to grow much faster than law is able to figure out how to deal with it

555
00:42:32,500 --> 00:42:36,500
or faster than economic theory can change itself, right?

556
00:42:36,500 --> 00:42:41,500
and so one of the things that we see is that our social technologies like law, like governance, like economics

557
00:42:41,500 --> 00:42:46,500
are actually being obsolete by the development of totally new types of behavior and mechanics

558
00:42:46,500 --> 00:42:52,500
that weren't part of the world they were trying to solve problems for, right?

559
00:42:52,500 --> 00:42:58,500
and so the Scottish Enlightenment was the development of new ideas of how to problem solve, the problems of its time

560
00:42:58,500 --> 00:43:03,500
the constitution was trying to figure out how to solve the problems of its time

561
00:43:03,500 --> 00:43:06,500
I would say they were good thinking, right? they were good work

562
00:43:06,500 --> 00:43:10,500
the Bretton Woods world was, none of them are adequate to solve these problems

563
00:43:10,500 --> 00:43:13,500
because these problems are different in kind

564
00:43:13,500 --> 00:43:18,500
and even where they're just an extension of magnitude, when you get enough change in magnitude

565
00:43:18,500 --> 00:43:22,500
sometimes it becomes a difference in kind, like as you're getting more and more information to process

566
00:43:22,500 --> 00:43:26,500
once you get past what humans can process, infosingularity type issues

567
00:43:26,500 --> 00:43:30,500
okay, well now it's a difference in magnitude that becomes a difference in kind

568
00:43:30,500 --> 00:43:33,500
which means you need a fundamentally different approach

569
00:43:33,500 --> 00:43:38,500
so I would say this is where it's important to recognize that those social technologies that we loved so much

570
00:43:38,500 --> 00:43:41,500
because they seemed so much better than all the other options we had at the time

571
00:43:41,500 --> 00:43:44,500
like markets and like democracy

572
00:43:44,500 --> 00:43:47,500
these are not terminal goods in and of themselves

573
00:43:47,500 --> 00:43:50,500
the terminal goods were things like human liberty

574
00:43:50,500 --> 00:43:54,500
and justice and checks and balances on power

575
00:43:54,500 --> 00:44:00,500
and opportunity and distribution of opportunity and things like that

576
00:44:00,500 --> 00:44:04,500
these were the best social technologies possible at the time

577
00:44:04,500 --> 00:44:07,500
the new technologies both kill those things

578
00:44:07,500 --> 00:44:08,500
they don't work anymore, right?

579
00:44:08,500 --> 00:44:12,500
you can't have the social technology of the fourth estate

580
00:44:12,500 --> 00:44:14,500
that was necessary for democracy

581
00:44:14,500 --> 00:44:16,500
which is why founding fathers said things like

582
00:44:16,500 --> 00:44:18,500
if I could have perfect newspapers and a broken government

583
00:44:18,500 --> 00:44:21,500
or perfect government and broken newspapers, I'd take the newspapers

584
00:44:21,500 --> 00:44:24,500
because if you have an educated populace that all understands what's going on

585
00:44:24,500 --> 00:44:26,500
they can make a new form of government

586
00:44:26,500 --> 00:44:28,500
if you have people that have no idea what's going on

587
00:44:28,500 --> 00:44:32,500
how could they possibly make good choices if their sense making is totally broken

588
00:44:32,500 --> 00:44:36,500
so we had this idea that the fourth estate was a prerequisite

589
00:44:36,500 --> 00:44:38,500
to a participatory governance

590
00:44:38,500 --> 00:44:44,500
but that was based on a very narrow limited capacity for print

591
00:44:44,500 --> 00:44:47,500
and again it was the technology of the Gutenberg Press

592
00:44:47,500 --> 00:44:50,500
that was one of the things that actually ended feudalism

593
00:44:50,500 --> 00:44:54,500
and so the founding fathers were employing that new tech

594
00:44:54,500 --> 00:44:57,500
both because it upended the previous tech and it made this new thing possible

595
00:44:57,500 --> 00:44:59,500
same with guns

596
00:44:59,500 --> 00:45:03,500
they needed guns and second amendments to make this new thing possible

597
00:45:03,500 --> 00:45:07,500
but once we get to a internet world

598
00:45:07,500 --> 00:45:09,500
where you don't have centralized broadcasts

599
00:45:09,500 --> 00:45:11,500
you have decentralized and then there's so much stuff

600
00:45:11,500 --> 00:45:13,500
that you can never possibly find at all in search

601
00:45:13,500 --> 00:45:15,500
whoever coordinates the search

602
00:45:15,500 --> 00:45:18,500
the content aggregators, which is the Facebook, the YouTube, whatever

603
00:45:18,500 --> 00:45:21,500
are doing it with the types of business models we have

604
00:45:21,500 --> 00:45:23,500
the fourth estate is just dead forever

605
00:45:23,500 --> 00:45:26,500
that old version, there's no way to recreate that version

606
00:45:26,500 --> 00:45:28,500
that either means democracy is dead forever

607
00:45:28,500 --> 00:45:31,500
or anything like a well-informed citizenry

608
00:45:31,500 --> 00:45:33,500
that could participate in its governance in any form

609
00:45:33,500 --> 00:45:39,500
or you have to say what is a post-internet, post-social media, post-infosingularity

610
00:45:39,500 --> 00:45:43,500
fourth estate that creates an adequately educated citizenry

611
00:45:43,500 --> 00:45:46,500
that's thinking about the way that our social technologies

612
00:45:46,500 --> 00:45:49,500
our social systems have to upgrade themselves

613
00:45:49,500 --> 00:45:52,500
in the presence of the tech that obsoleted the way they did work

614
00:45:52,500 --> 00:45:54,500
but we can also see and we can give examples of this

615
00:45:54,500 --> 00:45:58,500
the new tech also makes possible new things that weren't possible before

616
00:45:58,500 --> 00:46:00,500
so we can do something better than industrial-era democracy

617
00:46:00,500 --> 00:46:02,500
or industrial-era markets

618
00:46:02,500 --> 00:46:04,500
which is why I say they aren't a terminal good

619
00:46:04,500 --> 00:46:07,500
they're a way to deliver certain human values that really matter

620
00:46:07,500 --> 00:46:10,500
and the new technology that obsoletes those

621
00:46:10,500 --> 00:46:15,500
can actually also be facilitative in designing systems

622
00:46:15,500 --> 00:46:17,500
that also serve those values

623
00:46:17,500 --> 00:46:19,500
but it's not a given that it does

624
00:46:19,500 --> 00:46:22,500
that has to become the kind of central orienting mission

625
00:46:22,500 --> 00:46:26,500
so Dan just to make sure we're linking this back to the start of this conversation

626
00:46:26,500 --> 00:46:28,500
we started this conversation by saying

627
00:46:28,500 --> 00:46:31,500
the way that we are going about solving problems

628
00:46:31,500 --> 00:46:36,500
let's say using the legacy systems of lawmaking in a congress

629
00:46:36,500 --> 00:46:40,500
or using the legacy systems of a town hall to vote on a proposition

630
00:46:40,500 --> 00:46:45,500
or trying to pass laws as fast as social media as rewiring society

631
00:46:45,500 --> 00:46:47,500
the lines don't match

632
00:46:47,500 --> 00:46:49,500
and so what you're saying is that

633
00:46:49,500 --> 00:46:52,500
and just for listeners because I know that you use the phrase social technology

634
00:46:52,500 --> 00:46:55,500
but I think you're really talking about it

635
00:46:55,500 --> 00:46:58,500
social systems, ways of organizing democracy

636
00:46:58,500 --> 00:47:01,500
technology in the most fundamental sense of the word

637
00:47:01,500 --> 00:47:07,500
of something humans design to facilitate certain kinds of activity or outcomes

638
00:47:07,500 --> 00:47:09,500
like language is a technology

639
00:47:09,500 --> 00:47:11,500
or democracy is a technology

640
00:47:11,500 --> 00:47:13,500
social systems

641
00:47:13,500 --> 00:47:17,500
and so if the kind of old world approach of

642
00:47:17,500 --> 00:47:19,500
some people might be hearing this and say to themselves

643
00:47:19,500 --> 00:47:20,500
now hold on a second

644
00:47:20,500 --> 00:47:22,500
so we have all these institutions

645
00:47:22,500 --> 00:47:24,500
we have all these structures

646
00:47:24,500 --> 00:47:25,500
we live in a democracy

647
00:47:25,500 --> 00:47:28,500
and we live in a system that is working the way it does

648
00:47:28,500 --> 00:47:30,500
it has its courts, it has its attorney generals

649
00:47:30,500 --> 00:47:32,500
it has its litigation procedures

650
00:47:32,500 --> 00:47:34,500
it has its lawmaking bodies

651
00:47:34,500 --> 00:47:37,500
if you're saying that we can't use those things

652
00:47:37,500 --> 00:47:39,500
because they're not adequate

653
00:47:39,500 --> 00:47:41,500
or they won't help us solve those problems

654
00:47:41,500 --> 00:47:43,500
we need to have new social systems

655
00:47:43,500 --> 00:47:46,500
maybe you could give us some hope about why that might be feasible

656
00:47:46,500 --> 00:47:48,500
instead of feeling impossible

657
00:47:48,500 --> 00:47:51,500
because this is actually precedented in our history

658
00:47:51,500 --> 00:47:53,500
when new technologies show up

659
00:47:53,500 --> 00:47:55,500
and then new social systems emerge

660
00:47:55,500 --> 00:47:58,500
to make room for those technologies functioning well

661
00:47:58,500 --> 00:47:59,500
you briefly touched on that

662
00:47:59,500 --> 00:48:03,500
but I think it's important to give listeners a few concrete examples

663
00:48:03,500 --> 00:48:08,500
there's a number of good academics and disciplines of academics

664
00:48:08,500 --> 00:48:12,500
that look at the history of evolutions and physical technology

665
00:48:12,500 --> 00:48:15,500
and the corresponding evolutions in

666
00:48:15,500 --> 00:48:18,500
thought and culture and social systems

667
00:48:18,500 --> 00:48:21,500
Marvin Harris, the cultural materialism

668
00:48:21,500 --> 00:48:23,500
did a kind of major opus work here

669
00:48:23,500 --> 00:48:26,500
where he specifically looked at how changes in social systems and cultures

670
00:48:26,500 --> 00:48:28,500
followed changes in technology

671
00:48:28,500 --> 00:48:31,500
there are other bodies of work that will

672
00:48:31,500 --> 00:48:33,500
look at the social systems as primary

673
00:48:33,500 --> 00:48:35,500
or the cultures as primary

674
00:48:35,500 --> 00:48:37,500
and we can say they're inter-affecting

675
00:48:37,500 --> 00:48:40,500
but for instance

676
00:48:40,500 --> 00:48:43,500
the vast majority of human history was tribal

677
00:48:43,500 --> 00:48:46,500
however much 200,000 years of humans

678
00:48:46,500 --> 00:48:50,500
in the small Dunbar number villages

679
00:48:50,500 --> 00:48:52,500
there was a social technology

680
00:48:52,500 --> 00:48:53,500
social systems that mediated that

681
00:48:53,500 --> 00:48:55,500
that had to do with how the tribal circles worked

682
00:48:55,500 --> 00:48:59,500
and the nature of how resources were shared

683
00:48:59,500 --> 00:49:01,500
it was a very different kind of economic system

684
00:49:01,500 --> 00:49:03,500
a very different kind of judicial system

685
00:49:03,500 --> 00:49:05,500
a different educational system

686
00:49:05,500 --> 00:49:06,500
it had all those things

687
00:49:06,500 --> 00:49:07,500
it had a way of education

688
00:49:07,500 --> 00:49:09,500
meaning intergenerational knowledge transfer

689
00:49:09,500 --> 00:49:11,500
of the entire knowledge set that was needed

690
00:49:11,500 --> 00:49:15,500
for the tribe to continue operating

691
00:49:15,500 --> 00:49:18,500
the development of certain technologies

692
00:49:18,500 --> 00:49:20,500
particularly the plow

693
00:49:20,500 --> 00:49:23,500
but baskets and a few other things

694
00:49:23,500 --> 00:49:24,500
obsolete that thing

695
00:49:24,500 --> 00:49:27,500
because all of a sudden it made possible

696
00:49:27,500 --> 00:49:28,500
big amounts of surplus

697
00:49:28,500 --> 00:49:31,500
that made reason for much larger populations

698
00:49:31,500 --> 00:49:32,500
to emerge

699
00:49:32,500 --> 00:49:34,500
those larger populations

700
00:49:34,500 --> 00:49:36,500
were going to win in conflict against

701
00:49:36,500 --> 00:49:38,500
the smaller populations

702
00:49:38,500 --> 00:49:40,500
and so you can see that then the emergence

703
00:49:40,500 --> 00:49:41,500
of new social technology

704
00:49:41,500 --> 00:49:43,500
to facilitate large groups of people

705
00:49:43,500 --> 00:49:45,500
empire types

706
00:49:45,500 --> 00:49:47,500
civilization technology emerged

707
00:49:47,500 --> 00:49:49,500
you can see

708
00:49:49,500 --> 00:49:52,500
and that there were a few other shifts in technology

709
00:49:52,500 --> 00:49:55,500
that evolved the types of empires that were there

710
00:49:55,500 --> 00:49:57,500
and then the next one that people talk about a lot

711
00:49:57,500 --> 00:49:59,500
is the industrial revolution

712
00:49:59,500 --> 00:50:01,500
from the printing press specifically

713
00:50:01,500 --> 00:50:02,500
and then steam engine

714
00:50:02,500 --> 00:50:04,500
the gunpowder revolution was part of it

715
00:50:04,500 --> 00:50:06,500
that kind of ended feudalism

716
00:50:06,500 --> 00:50:08,500
and began the nation state world

717
00:50:08,500 --> 00:50:10,500
and so you can see like

718
00:50:10,500 --> 00:50:13,500
what is the thing that the founding fathers

719
00:50:13,500 --> 00:50:14,500
in the US were doing

720
00:50:14,500 --> 00:50:17,500
well they weren't trying to keep winning at feudalism

721
00:50:17,500 --> 00:50:19,500
there was a game that had been happening

722
00:50:19,500 --> 00:50:20,500
for a long time

723
00:50:20,500 --> 00:50:21,500
and they were saying

724
00:50:21,500 --> 00:50:23,500
like no we're all people who are in

725
00:50:23,500 --> 00:50:25,500
of the type of people who could do well

726
00:50:25,500 --> 00:50:26,500
at that system

727
00:50:26,500 --> 00:50:28,500
and rather than do that

728
00:50:28,500 --> 00:50:29,500
we recognize that there are

729
00:50:29,500 --> 00:50:31,500
fundamentally things wrong with this system

730
00:50:31,500 --> 00:50:33,500
and fundamentally new possibilities

731
00:50:33,500 --> 00:50:35,500
that hadn't been previously recognized

732
00:50:35,500 --> 00:50:37,500
so we're going to actually try to design

733
00:50:37,500 --> 00:50:39,500
a fundamentally different system

734
00:50:39,500 --> 00:50:40,500
that we think

735
00:50:40,500 --> 00:50:42,500
a more perfect union

736
00:50:42,500 --> 00:50:44,500
that makes life liberty in the pursuit of happiness

737
00:50:44,500 --> 00:50:45,500
better for everybody

738
00:50:45,500 --> 00:50:47,500
and increases productive capacity

739
00:50:47,500 --> 00:50:48,500
and things like that

740
00:50:48,500 --> 00:50:50,500
so that was fundamentally in advance

741
00:50:50,500 --> 00:50:52,500
in social technology or social systems

742
00:50:52,500 --> 00:50:53,500
that both utilized

743
00:50:53,500 --> 00:50:55,500
new physical technology

744
00:50:55,500 --> 00:50:57,500
and was enabled by it

745
00:51:00,500 --> 00:51:02,500
in the current situation

746
00:51:03,500 --> 00:51:05,500
there are groups that are advancing

747
00:51:05,500 --> 00:51:07,500
the exponential technologies

748
00:51:07,500 --> 00:51:09,500
and that means whatever social systems

749
00:51:09,500 --> 00:51:10,500
that they're employing

750
00:51:10,500 --> 00:51:12,500
are the social systems of the future

751
00:51:12,500 --> 00:51:13,500
if we don't change it

752
00:51:13,500 --> 00:51:15,500
and that's what I want to get to in a moment

753
00:51:15,500 --> 00:51:18,500
but like who is working to

754
00:51:18,500 --> 00:51:21,500
implement any of the new emerging

755
00:51:21,500 --> 00:51:23,500
tech for better social systems

756
00:51:23,500 --> 00:51:25,500
that are aligned with social systems we want

757
00:51:25,500 --> 00:51:27,500
you've had Audrey Tang

758
00:51:27,500 --> 00:51:28,500
on the show

759
00:51:28,500 --> 00:51:30,500
do you want to just briefly describe

760
00:51:30,500 --> 00:51:32,500
an example of what she

761
00:51:32,500 --> 00:51:34,500
and what they have done there

762
00:51:34,500 --> 00:51:36,500
because if people aren't aware of it

763
00:51:36,500 --> 00:51:38,500
that's a pretty prime example

764
00:51:38,500 --> 00:51:40,500
of for this particular iteration

765
00:51:40,500 --> 00:51:42,500
sure well

766
00:51:42,500 --> 00:51:45,500
and maybe just to go back briefly

767
00:51:45,500 --> 00:51:47,500
because you gave this example

768
00:51:47,500 --> 00:51:49,500
in one of our earlier conversations

769
00:51:49,500 --> 00:51:51,500
that the printing press

770
00:51:51,500 --> 00:51:53,500
could have been used by the feudal lords

771
00:51:53,500 --> 00:51:55,500
for consciously reinforcing feudalism

772
00:51:55,500 --> 00:51:57,500
but instead this new technology

773
00:51:57,500 --> 00:51:59,500
the printing press gives way

774
00:51:59,500 --> 00:52:01,500
to new ways of organizing society

775
00:52:01,500 --> 00:52:03,500
so we can actually have things like

776
00:52:03,500 --> 00:52:05,500
a fourth estate or newspapers

777
00:52:05,500 --> 00:52:07,500
or things like that

778
00:52:07,500 --> 00:52:09,500
both happen

779
00:52:09,500 --> 00:52:11,500
but then the new thing theoretically

780
00:52:11,500 --> 00:52:13,500
has to win out over the old thing

781
00:52:13,500 --> 00:52:15,500
at least the one that we want

782
00:52:15,500 --> 00:52:17,500
that holds the values that the society wants

783
00:52:17,500 --> 00:52:19,500
so

784
00:52:19,500 --> 00:52:21,500
I think a lot of people can hear our conversation

785
00:52:21,500 --> 00:52:23,500
we've had this riff before

786
00:52:23,500 --> 00:52:25,500
actually following our last episode

787
00:52:25,500 --> 00:52:27,500
after my senate testimony

788
00:52:27,500 --> 00:52:29,500
speaking about a frame

789
00:52:29,500 --> 00:52:31,500
that you have offered and know well

790
00:52:31,500 --> 00:52:33,500
which is that we can notice

791
00:52:33,500 --> 00:52:35,500
that digital authoritarian societies

792
00:52:35,500 --> 00:52:37,500
right now like China

793
00:52:37,500 --> 00:52:39,500
are consciously using exponential technologies

794
00:52:39,500 --> 00:52:41,500
to make stronger more effective digital

795
00:52:41,500 --> 00:52:43,500
closed and authoritarian societies

796
00:52:43,500 --> 00:52:45,500
and in contrast digital open societies

797
00:52:45,500 --> 00:52:47,500
democracies like the United States

798
00:52:47,500 --> 00:52:49,500
are not consciously using technology

799
00:52:49,500 --> 00:52:51,500
to make stronger healthier open societies

800
00:52:51,500 --> 00:52:53,500
instead they've sort of surrendered

801
00:52:53,500 --> 00:52:55,500
what they are to

802
00:52:55,500 --> 00:52:57,500
private technology

803
00:52:57,500 --> 00:52:59,500
corporations pursuing self-interest

804
00:52:59,500 --> 00:53:01,500
to shareholders and are

805
00:53:01,500 --> 00:53:03,500
profiting from the degradation

806
00:53:03,500 --> 00:53:05,500
and dysfunction of democracies

807
00:53:05,500 --> 00:53:07,500
and so

808
00:53:07,500 --> 00:53:09,500
when we say all this and we talk about

809
00:53:09,500 --> 00:53:11,500
how do we build the kind of next social system

810
00:53:11,500 --> 00:53:13,500
and Audrey Tang and her work

811
00:53:13,500 --> 00:53:15,500
I think people

812
00:53:15,500 --> 00:53:17,500
get tripped up in thinking that what we really mean

813
00:53:17,500 --> 00:53:19,500
is we have to make some kind of 21st century

814
00:53:19,500 --> 00:53:21,500
digital democracy in fact I probably said those words

815
00:53:21,500 --> 00:53:23,500
but what we're really talking about here

816
00:53:23,500 --> 00:53:25,500
is some new concept that preserves

817
00:53:25,500 --> 00:53:27,500
the principles of what we meant by

818
00:53:27,500 --> 00:53:29,500
a democracy

819
00:53:29,500 --> 00:53:31,500
but instantiated with the new technologies

820
00:53:31,500 --> 00:53:33,500
our version of the new printing press

821
00:53:33,500 --> 00:53:35,500
which is networked

822
00:53:35,500 --> 00:53:37,500
information environments and

823
00:53:37,500 --> 00:53:39,500
all of the new capacities that we have in the 21st century

824
00:53:39,500 --> 00:53:41,500
with

825
00:53:41,500 --> 00:53:43,500
mobility where everyone's connected to everywhere

826
00:53:43,500 --> 00:53:45,500
and everything all at once

827
00:53:45,500 --> 00:53:47,500
so what is that system

828
00:53:47,500 --> 00:53:49,500
that leverages the current technology

829
00:53:49,500 --> 00:53:51,500
and makes a stronger healthier open society

830
00:53:51,500 --> 00:53:53,500
and I think Audrey Tang's work

831
00:53:53,500 --> 00:53:55,500
I mean I would probably send listeners back

832
00:53:55,500 --> 00:53:57,500
to listen to that episode I think it's one of our

833
00:53:57,500 --> 00:53:59,500
most listened to and most popular episodes for a reason

834
00:53:59,500 --> 00:54:01,500
because in Taiwan

835
00:54:01,500 --> 00:54:03,500
she's essentially built an entire

836
00:54:03,500 --> 00:54:05,500
civic technology ecosystem

837
00:54:05,500 --> 00:54:07,500
in which people are really participating

838
00:54:07,500 --> 00:54:09,500
in the governance of their society

839
00:54:09,500 --> 00:54:11,500
we need masks, we need

840
00:54:11,500 --> 00:54:13,500
better air quality sensors, we need to fix these

841
00:54:13,500 --> 00:54:15,500
potholes, there are processes

842
00:54:15,500 --> 00:54:17,500
by which every time you're frustrated by something

843
00:54:17,500 --> 00:54:19,500
you actually get invited into a civic design process

844
00:54:19,500 --> 00:54:21,500
where whether it's the potholes or the masks

845
00:54:21,500 --> 00:54:23,500
you can actually participate in having a better

846
00:54:23,500 --> 00:54:25,500
system

847
00:54:25,500 --> 00:54:27,500
complaining about the tax system and filing your taxes

848
00:54:27,500 --> 00:54:29,500
and maybe it's an inefficient form

849
00:54:29,500 --> 00:54:31,500
or something like that you get brought into a design process

850
00:54:31,500 --> 00:54:33,500
of what would make it better

851
00:54:33,500 --> 00:54:35,500
and so the system is participatory but not in that

852
00:54:35,500 --> 00:54:37,500
kind of 18th century way of hey there's a

853
00:54:37,500 --> 00:54:39,500
physical wooden townhouse and we're going to walk into it

854
00:54:39,500 --> 00:54:41,500
and we're going to hang out there for three hours

855
00:54:41,500 --> 00:54:43,500
and we're going to yell and scream about issues that are more local

856
00:54:43,500 --> 00:54:45,500
within 10, 15 miles

857
00:54:45,500 --> 00:54:47,500
of where we are because we were existing in a world before automobiles

858
00:54:47,500 --> 00:54:49,500
we're now talking about how do you do

859
00:54:49,500 --> 00:54:51,500
an open society social system

860
00:54:51,500 --> 00:54:53,500
but in a world with all of the new technologies

861
00:54:53,500 --> 00:54:55,500
that are not just here today but emerging

862
00:54:55,500 --> 00:54:57,500
and so do you want to talk a little bit

863
00:54:57,500 --> 00:54:59,500
about what

864
00:54:59,500 --> 00:55:01,500
how we even navigate that challenge

865
00:55:01,500 --> 00:55:03,500
and why is some new social system like that

866
00:55:03,500 --> 00:55:05,500
necessary

867
00:55:05,500 --> 00:55:07,500
for dealing with these problems that you've

868
00:55:07,500 --> 00:55:09,500
sort of laid out at the beginning

869
00:55:09,500 --> 00:55:11,500
because I'm sure people would like to feel

870
00:55:11,500 --> 00:55:13,500
less anxiety about those things

871
00:55:13,500 --> 00:55:15,500
hanging around for longer

872
00:55:15,500 --> 00:55:17,500
yeah I think

873
00:55:17,500 --> 00:55:19,500
I think what

874
00:55:19,500 --> 00:55:21,500
Taiwan has been doing

875
00:55:21,500 --> 00:55:23,500
and what Audrey Tang

876
00:55:23,500 --> 00:55:25,500
in the digital ministry position

877
00:55:25,500 --> 00:55:27,500
in particular has been leading

878
00:55:27,500 --> 00:55:29,500
is probably the best example

879
00:55:29,500 --> 00:55:31,500
certainly one of the best examples in the world of this kind of

880
00:55:31,500 --> 00:55:33,500
process in thinking

881
00:55:33,500 --> 00:55:35,500
and

882
00:55:35,500 --> 00:55:37,500
does it apply in the

883
00:55:37,500 --> 00:55:39,500
or could it apply in the exact same way to the US

884
00:55:39,500 --> 00:55:41,500
no of course not like we know that

885
00:55:41,500 --> 00:55:43,500
because

886
00:55:43,500 --> 00:55:45,500
of the relatively small geography

887
00:55:45,500 --> 00:55:47,500
and

888
00:55:47,500 --> 00:55:49,500
high-speed train transportation

889
00:55:49,500 --> 00:55:51,500
you can get across Taiwan in an hour and a half

890
00:55:51,500 --> 00:55:53,500
and so when you're mentioning the small scale

891
00:55:53,500 --> 00:55:55,500
of local government at the beginning of the US

892
00:55:55,500 --> 00:55:57,500
where you come to the town hall in a way they have

893
00:55:57,500 --> 00:55:59,500
that right like it's 23 million people

894
00:55:59,500 --> 00:56:01,500
but there is

895
00:56:01,500 --> 00:56:03,500
an older shared culture

896
00:56:03,500 --> 00:56:05,500
there is all there also happens

897
00:56:05,500 --> 00:56:07,500
to be an existential threat just right

898
00:56:07,500 --> 00:56:09,500
off their border that is

899
00:56:09,500 --> 00:56:11,500
you know big enough that they can't

900
00:56:11,500 --> 00:56:13,500
just chill and not focus on it

901
00:56:13,500 --> 00:56:15,500
everyone has to be civically engaged

902
00:56:15,500 --> 00:56:17,500
with some civic identity and like that

903
00:56:17,500 --> 00:56:19,500
they didn't start making

904
00:56:19,500 --> 00:56:21,500
their culture in the industrial era and then have to

905
00:56:21,500 --> 00:56:23,500
upgrade it right like they started

906
00:56:23,500 --> 00:56:25,500
later

907
00:56:25,500 --> 00:56:27,500
where they're we're able to start at a higher

908
00:56:27,500 --> 00:56:29,500
level of the tech stack

909
00:56:29,500 --> 00:56:31,500
so there's a number of reasons why it's different

910
00:56:31,500 --> 00:56:33,500
so we're not going to naively say what you do

911
00:56:33,500 --> 00:56:35,500
in a tiny country that is culturally

912
00:56:35,500 --> 00:56:37,500
and ethnically homogeneous

913
00:56:39,500 --> 00:56:41,500
and has a higher GDP

914
00:56:41,500 --> 00:56:43,500
education per capita and whatever is the same thing

915
00:56:43,500 --> 00:56:45,500
you would do but we can certainly take a lot

916
00:56:45,500 --> 00:56:47,500
of the examples and say how would

917
00:56:47,500 --> 00:56:49,500
they apply differently in different

918
00:56:49,500 --> 00:56:51,500
contexts

919
00:56:53,500 --> 00:56:55,500
so

920
00:56:55,500 --> 00:56:57,500
the thing we said earlier

921
00:56:57,500 --> 00:56:59,500
that this suite of

922
00:56:59,500 --> 00:57:01,500
exponential technologies is so much more powerful

923
00:57:01,500 --> 00:57:03,500
than all of the previous types

924
00:57:03,500 --> 00:57:05,500
of power that only

925
00:57:05,500 --> 00:57:07,500
those who are

926
00:57:07,500 --> 00:57:09,500
developing and deploying

927
00:57:09,500 --> 00:57:11,500
them will be

928
00:57:11,500 --> 00:57:13,500
really steering the direction of the future

929
00:57:13,500 --> 00:57:15,500
and

930
00:57:15,500 --> 00:57:17,500
that there are ways of employing them

931
00:57:17,500 --> 00:57:19,500
that do cause

932
00:57:19,500 --> 00:57:21,500
catastrophic risk

933
00:57:21,500 --> 00:57:23,500
and

934
00:57:23,500 --> 00:57:25,500
the catastrophic risk is of two primary

935
00:57:25,500 --> 00:57:27,500
kinds right conflict theory mediated

936
00:57:27,500 --> 00:57:29,500
and

937
00:57:29,500 --> 00:57:31,500
you can't

938
00:57:31,500 --> 00:57:33,500
you just can't do warfare

939
00:57:33,500 --> 00:57:35,500
with this level of technology

940
00:57:35,500 --> 00:57:37,500
and this interconnected a world

941
00:57:37,500 --> 00:57:39,500
and make it through well

942
00:57:39,500 --> 00:57:41,500
not all catastrophic risk means existential

943
00:57:41,500 --> 00:57:43,500
doesn't all mean nuclear

944
00:57:43,500 --> 00:57:45,500
war and nuclear winter and we've killed

945
00:57:45,500 --> 00:57:47,500
all the mammals on earth it might

946
00:57:47,500 --> 00:57:49,500
just mean we break global supply chains

947
00:57:49,500 --> 00:57:51,500
kill lots of people and regress

948
00:57:51,500 --> 00:57:53,500
humanity and the quality of the biosphere

949
00:57:53,500 --> 00:57:55,500
pretty significantly so

950
00:57:55,500 --> 00:57:57,500
I'm not

951
00:57:57,500 --> 00:57:59,500
just focused on existential risk I'm interested

952
00:57:59,500 --> 00:58:01,500
in kind of catastrophic risk

953
00:58:01,500 --> 00:58:03,500
at scale in general

954
00:58:03,500 --> 00:58:05,500
and we can see that exponential tech

955
00:58:05,500 --> 00:58:07,500
applied

956
00:58:07,500 --> 00:58:09,500
as

957
00:58:09,500 --> 00:58:11,500
in conflict theory and in

958
00:58:11,500 --> 00:58:13,500
mistake

959
00:58:13,500 --> 00:58:15,500
as externalities and the cumulative effects

960
00:58:15,500 --> 00:58:17,500
could you define

961
00:58:17,500 --> 00:58:19,500
conflict theory and mistake theory for people

962
00:58:19,500 --> 00:58:21,500
who are not familiar with those terms

963
00:58:21,500 --> 00:58:23,500
yeah there's

964
00:58:23,500 --> 00:58:25,500
a very nice

965
00:58:25,500 --> 00:58:27,500
discussion on the less wrong forum

966
00:58:27,500 --> 00:58:29,500
if people are interested to go deeper

967
00:58:29,500 --> 00:58:31,500
and it's this question of

968
00:58:31,500 --> 00:58:33,500
how much of the problems in the world

969
00:58:33,500 --> 00:58:35,500
are the result of

970
00:58:35,500 --> 00:58:37,500
conflict theory versus mistake theory

971
00:58:37,500 --> 00:58:39,500
meaning conflict theory is

972
00:58:39,500 --> 00:58:41,500
we knew we either wanted to cause

973
00:58:41,500 --> 00:58:43,500
that problem that harm to whomever

974
00:58:43,500 --> 00:58:45,500
as in a knowingly wanted to win

975
00:58:45,500 --> 00:58:47,500
at a war and

976
00:58:47,500 --> 00:58:49,500
or at least we knew we were

977
00:58:49,500 --> 00:58:51,500
going to cause that problem and didn't care because

978
00:58:51,500 --> 00:58:53,500
it was attached to something we wanted

979
00:58:53,500 --> 00:58:55,500
right conflict theory or mistake

980
00:58:55,500 --> 00:58:57,500
theory we didn't know we didn't want to cause

981
00:58:57,500 --> 00:58:59,500
it and we really didn't know and it was just

982
00:58:59,500 --> 00:59:01,500
unintended unanticipatable consequence

983
00:59:01,500 --> 00:59:03,500
and it's fair to say that there's

984
00:59:03,500 --> 00:59:05,500
both right there's plenty of both

985
00:59:05,500 --> 00:59:07,500
one

986
00:59:07,500 --> 00:59:09,500
thing that is worth knowing is that

987
00:59:09,500 --> 00:59:11,500
if I

988
00:59:11,500 --> 00:59:13,500
if I'm trying to do something that is

989
00:59:13,500 --> 00:59:15,500
actually motivated by conflict theory

990
00:59:15,500 --> 00:59:17,500
it benefits me to pretend that it was mistake

991
00:59:17,500 --> 00:59:19,500
theory benefits me to pretend that I had no idea

992
00:59:19,500 --> 00:59:21,500
and then afterwards say oh it was

993
00:59:21,500 --> 00:59:23,500
an unintended unanticipatable consequence

994
00:59:23,500 --> 00:59:25,500
it was too complex people can't predict stuff

995
00:59:25,500 --> 00:59:27,500
like that and so

996
00:59:27,500 --> 00:59:29,500
the reality

997
00:59:29,500 --> 00:59:31,500
of mistake theory

998
00:59:31,500 --> 00:59:33,500
ends up being a source of plausible

999
00:59:33,500 --> 00:59:35,500
deniability for conflict theory

1000
00:59:35,500 --> 00:59:37,500
and

1001
00:59:37,500 --> 00:59:39,500
but they're both things and we have to

1002
00:59:39,500 --> 00:59:41,500
overcome both meaning we have to

1003
00:59:41,500 --> 00:59:43,500
have choice making processes in our new system

1004
00:59:43,500 --> 00:59:45,500
of coordination and like this sounds

1005
00:59:45,500 --> 00:59:47,500
like maybe hippy stuff

1006
00:59:47,500 --> 00:59:49,500
until you take

1007
00:59:49,500 --> 00:59:51,500
seriously the change of context

1008
00:59:51,500 --> 00:59:53,500
oh we have to have problems of choice making that

1009
00:59:53,500 --> 00:59:55,500
consider the whole that sounds like

1010
00:59:55,500 --> 00:59:57,500
unrealizable hippy stuff

1011
00:59:57,500 --> 00:59:59,500
until you realize

1012
00:59:59,500 --> 01:00:01,500
but we're making choices that affect

1013
01:00:01,500 --> 01:00:03,500
the whole

1014
01:00:03,500 --> 01:00:05,500
at a level that

1015
01:00:05,500 --> 01:00:07,500
can even individually be catastrophic

1016
01:00:07,500 --> 01:00:09,500
and is definitely catastrophic cumulatively

1017
01:00:09,500 --> 01:00:11,500
so if we aren't factoring

1018
01:00:11,500 --> 01:00:13,500
it

1019
01:00:13,500 --> 01:00:15,500
then the human experiment self terminates and maybe that's

1020
01:00:15,500 --> 01:00:17,500
the answer to the great filter hypothesis

1021
01:00:17,500 --> 01:00:19,500
right and so

1022
01:00:19,500 --> 01:00:21,500
our

1023
01:00:21,500 --> 01:00:23,500
well yeah

1024
01:00:23,500 --> 01:00:25,500
I think people don't have an intuitive grasp of

1025
01:00:25,500 --> 01:00:27,500
what it means that each of us

1026
01:00:27,500 --> 01:00:29,500
are walking around with the power of

1027
01:00:29,500 --> 01:00:31,500
gods to influence huge enormous

1028
01:00:31,500 --> 01:00:33,500
consequences I mean

1029
01:00:33,500 --> 01:00:35,500
I could give a few examples every time you

1030
01:00:35,500 --> 01:00:37,500
enact with the global supply chain and hit buy on Amazon

1031
01:00:37,500 --> 01:00:39,500
you invisibly enacted

1032
01:00:39,500 --> 01:00:41,500
shipping and planes

1033
01:00:41,500 --> 01:00:43,500
and petroleum and wars in the middle east

1034
01:00:43,500 --> 01:00:45,500
there's a whole bunch of things that we're

1035
01:00:45,500 --> 01:00:47,500
sort of tied into when you are

1036
01:00:47,500 --> 01:00:49,500
posting something on social media and have

1037
01:00:49,500 --> 01:00:51,500
more than a million followers you're

1038
01:00:51,500 --> 01:00:53,500
influencing the global information ecology

1039
01:00:53,500 --> 01:00:55,500
and if you're angry and biased about one

1040
01:00:55,500 --> 01:00:57,500
side of the other of the pandemic is real

1041
01:00:57,500 --> 01:00:59,500
or it's not real or something like that you're

1042
01:00:59,500 --> 01:01:01,500
externalizing more bias into the commons

1043
01:01:01,500 --> 01:01:03,500
of how keep the rest of the world understands

1044
01:01:03,500 --> 01:01:05,500
things so we're walking around with increasing

1045
01:01:05,500 --> 01:01:07,500
power but I don't think the increasing power

1046
01:01:07,500 --> 01:01:09,500
that we've granted is intuitive

1047
01:01:09,500 --> 01:01:11,500
for some folks did you explain

1048
01:01:11,500 --> 01:01:13,500
some more examples of that there's both

1049
01:01:13,500 --> 01:01:15,500
cumulative effect and

1050
01:01:15,500 --> 01:01:17,500
like cumulative long-term

1051
01:01:17,500 --> 01:01:19,500
and fairly singular short-term

1052
01:01:19,500 --> 01:01:21,500
and cumulative long-term

1053
01:01:21,500 --> 01:01:23,500
I mean you go back to

1054
01:01:25,500 --> 01:01:27,500
early

1055
01:01:27,500 --> 01:01:29,500
US settlers coming into the

1056
01:01:29,500 --> 01:01:31,500
US moving west

1057
01:01:31,500 --> 01:01:33,500
and they're being buffalo everywhere and there had

1058
01:01:33,500 --> 01:01:35,500
been buffalo everywhere for a very long time

1059
01:01:35,500 --> 01:01:37,500
and then there's no buffalo in whole areas that

1060
01:01:37,500 --> 01:01:39,500
were forested with old growth forest became

1061
01:01:39,500 --> 01:01:41,500
deforested and it was

1062
01:01:41,500 --> 01:01:43,500
like no it's impossible we could never get rid of all the

1063
01:01:43,500 --> 01:01:45,500
buffalo like I we could never

1064
01:01:45,500 --> 01:01:47,500
cut down all the trees but the cumulative effect

1065
01:01:47,500 --> 01:01:49,500
of lots of people thinking that way we're individually

1066
01:01:49,500 --> 01:01:51,500
I have no incentive to leave the buffalo alive

1067
01:01:51,500 --> 01:01:53,500
and I do have an incentive for my family individually

1068
01:01:53,500 --> 01:01:55,500
to kill it but everybody thinking

1069
01:01:55,500 --> 01:01:57,500
that way and

1070
01:01:57,500 --> 01:01:59,500
increasing our

1071
01:01:59,500 --> 01:02:01,500
desire for how much we consume per capita

1072
01:02:01,500 --> 01:02:03,500
our technology that allows us

1073
01:02:03,500 --> 01:02:05,500
to consume more per capita

1074
01:02:05,500 --> 01:02:07,500
and developing more capital

1075
01:02:07,500 --> 01:02:09,500
more total people well

1076
01:02:09,500 --> 01:02:11,500
then you start getting

1077
01:02:11,500 --> 01:02:13,500
environmental destruction and species extinction at scale

1078
01:02:13,500 --> 01:02:15,500
and that's a long time ago

1079
01:02:15,500 --> 01:02:17,500
right like that's much lower tech and much less

1080
01:02:17,500 --> 01:02:19,500
people and it's

1081
01:02:19,500 --> 01:02:21,500
distributed action it's a

1082
01:02:21,500 --> 01:02:23,500
cumulative effect issue

1083
01:02:23,500 --> 01:02:25,500
and obviously we see that with

1084
01:02:27,500 --> 01:02:29,500
nobody's intending to fill the

1085
01:02:29,500 --> 01:02:31,500
ocean with microplastics

1086
01:02:31,500 --> 01:02:33,500
but everybody's buying shit that is filling the

1087
01:02:33,500 --> 01:02:35,500
oceans with microplastics and

1088
01:02:35,500 --> 01:02:37,500
so everyone is participating with systems where

1089
01:02:37,500 --> 01:02:39,500
the system as a whole

1090
01:02:39,500 --> 01:02:41,500
is sociopathic the system is self-terminating

1091
01:02:41,500 --> 01:02:43,500
the system doesn't exist without all the agents

1092
01:02:43,500 --> 01:02:45,500
interacting with it all the agents feel like

1093
01:02:45,500 --> 01:02:47,500
their behavior is so small

1094
01:02:47,500 --> 01:02:49,500
that that justifies everybody doing

1095
01:02:49,500 --> 01:02:51,500
that thing right so that's what we

1096
01:02:51,500 --> 01:02:53,500
mean by cumulative kind of catastrophic risk

1097
01:02:53,500 --> 01:02:55,500
but it's

1098
01:02:55,500 --> 01:02:57,500
also true that

1099
01:02:57,500 --> 01:02:59,500
whoever made

1100
01:02:59,500 --> 01:03:01,500
that

1101
01:03:01,500 --> 01:03:03,500
thermite bomb and hooked it to a drone and hit

1102
01:03:03,500 --> 01:03:05,500
the Ukrainian munitions factory a couple years

1103
01:03:05,500 --> 01:03:07,500
ago that caused a billion dollars

1104
01:03:07,500 --> 01:03:09,500
in damage exploded munitions factory

1105
01:03:09,500 --> 01:03:11,500
the effect of a bomb as big

1106
01:03:11,500 --> 01:03:13,500
as the largest

1107
01:03:13,500 --> 01:03:15,500
nuclear bomb the U.S. arsenal has an incendiary bomb

1108
01:03:15,500 --> 01:03:17,500
this is a home

1109
01:03:17,500 --> 01:03:19,500
that was a homemade little bomb in a

1110
01:03:19,500 --> 01:03:21,500
drone right and

1111
01:03:21,500 --> 01:03:23,500
so and crisper gene

1112
01:03:23,500 --> 01:03:25,500
drives are

1113
01:03:25,500 --> 01:03:27,500
cheap and easy and it doesn't take

1114
01:03:27,500 --> 01:03:29,500
that much advanced

1115
01:03:29,500 --> 01:03:31,500
knowledge to start working with them and

1116
01:03:31,500 --> 01:03:33,500
so that that starts to look

1117
01:03:33,500 --> 01:03:35,500
like

1118
01:03:35,500 --> 01:03:37,500
individuals and small

1119
01:03:37,500 --> 01:03:39,500
groups with real catastrophic ability

1120
01:03:39,500 --> 01:03:41,500
not long-term and cumulatively

1121
01:03:41,500 --> 01:03:43,500
the increase in our tech

1122
01:03:43,500 --> 01:03:45,500
gives us both issues via

1123
01:03:45,500 --> 01:03:47,500
globalization and the overall system you

1124
01:03:47,500 --> 01:03:49,500
get these cumulative long-term effects

1125
01:03:49,500 --> 01:03:51,500
and with the exponential

1126
01:03:51,500 --> 01:03:53,500
tech creating decentralized catastrophic

1127
01:03:53,500 --> 01:03:55,500
capabilities one

1128
01:03:55,500 --> 01:03:57,500
of the core questions we have to answer is how do we

1129
01:03:57,500 --> 01:03:59,500
make a world that is anti fragile

1130
01:03:59,500 --> 01:04:01,500
in the presence of

1131
01:04:01,500 --> 01:04:03,500
those kind of catastrophic capabilities that are

1132
01:04:03,500 --> 01:04:05,500
easy to produce and thus

1133
01:04:05,500 --> 01:04:07,500
decentralizable

1134
01:04:07,500 --> 01:04:09,500
and so

1135
01:04:09,500 --> 01:04:11,500
how do we do that

1136
01:04:11,500 --> 01:04:13,500
what are the social systems that we need

1137
01:04:13,500 --> 01:04:15,500
to employ to bind some of these

1138
01:04:15,500 --> 01:04:17,500
bad

1139
01:04:17,500 --> 01:04:19,500
facts and ways that the natural

1140
01:04:19,500 --> 01:04:21,500
inclinations of self-interested actors

1141
01:04:21,500 --> 01:04:23,500
will drive things in that direction just

1142
01:04:23,500 --> 01:04:25,500
to link this to the social media space for people

1143
01:04:25,500 --> 01:04:27,500
if I know that I can get

1144
01:04:27,500 --> 01:04:29,500
a little bit more attention and a little bit

1145
01:04:29,500 --> 01:04:31,500
more likes and clicks and follows and shares

1146
01:04:31,500 --> 01:04:33,500
and so on if I exaggerate the truth

1147
01:04:33,500 --> 01:04:35,500
by five percent just to use a little bit

1148
01:04:35,500 --> 01:04:37,500
more of an extreme adjective

1149
01:04:37,500 --> 01:04:39,500
you know I know that that in the long

1150
01:04:39,500 --> 01:04:41,500
run would be bad if everybody did that

1151
01:04:41,500 --> 01:04:43,500
but for me right now I can win a few hits

1152
01:04:43,500 --> 01:04:45,500
and I can get more influence and I'm an Instagram influencer

1153
01:04:45,500 --> 01:04:47,500
and I'm making ten thousand dollars a month and if I don't do it

1154
01:04:47,500 --> 01:04:49,500
I'm noticing everyone else's do it and if I don't use

1155
01:04:49,500 --> 01:04:51,500
the filter everyone else is using the filter

1156
01:04:51,500 --> 01:04:53,500
and so everyone ends up in this sort of another race

1157
01:04:53,500 --> 01:04:55,500
at the bottom sort of situation

1158
01:04:55,500 --> 01:04:57,500
that has that kind of cumulative degradation

1159
01:04:57,500 --> 01:04:59,500
or cumulative

1160
01:04:59,500 --> 01:05:01,500
derangement where there's increasing distance

1161
01:05:01,500 --> 01:05:03,500
between what is true and what people believe

1162
01:05:03,500 --> 01:05:05,500
because we've all been subtly exaggerating it

1163
01:05:05,500 --> 01:05:07,500
to make our point and gain influence and so on

1164
01:05:07,500 --> 01:05:09,500
and so

1165
01:05:09,500 --> 01:05:11,500
just to give another example

1166
01:05:11,500 --> 01:05:13,500
maybe for listeners in kind of the space that they're

1167
01:05:13,500 --> 01:05:15,500
more familiar with

1168
01:05:15,500 --> 01:05:17,500
but going back I mean the whole premise

1169
01:05:17,500 --> 01:05:19,500
of this is as we gain more

1170
01:05:19,500 --> 01:05:21,500
exponential technologies that have

1171
01:05:21,500 --> 01:05:23,500
more capacity and more hands

1172
01:05:23,500 --> 01:05:25,500
so instead of having just the US and Russia having this

1173
01:05:25,500 --> 01:05:27,500
you have

1174
01:05:27,500 --> 01:05:29,500
whether as you mentioned CRISPR gene drives or

1175
01:05:29,500 --> 01:05:31,500
some of the drone things that are out there

1176
01:05:31,500 --> 01:05:33,500
more and more people have access to these things

1177
01:05:33,500 --> 01:05:35,500
how can we bind

1178
01:05:35,500 --> 01:05:37,500
those kinds of forces and what are the

1179
01:05:37,500 --> 01:05:39,500
social systems that we need to make that happen

1180
01:05:39,500 --> 01:05:41,500
yeah I want to go back

1181
01:05:41,500 --> 01:05:43,500
as you were describing this

1182
01:05:43,500 --> 01:05:45,500
I was thinking about how many people

1183
01:05:45,500 --> 01:05:47,500
who

1184
01:05:47,500 --> 01:05:49,500
listen to your show

1185
01:05:49,500 --> 01:05:51,500
who maybe work in technology who might have

1186
01:05:51,500 --> 01:05:53,500
they work in technology

1187
01:05:53,500 --> 01:05:55,500
because they see the positive things technology can do

1188
01:05:55,500 --> 01:05:57,500
and have more of a kind of techno-optimist

1189
01:05:57,500 --> 01:05:59,500
point of view and this overall conversation

1190
01:05:59,500 --> 01:06:01,500
might sound very techno-pessimist and

1191
01:06:01,500 --> 01:06:03,500
like did we not

1192
01:06:03,500 --> 01:06:05,500
read Pinker and watch Hans Rosling and

1193
01:06:05,500 --> 01:06:07,500
you know those types of things

1194
01:06:07,500 --> 01:06:09,500
and

1195
01:06:09,500 --> 01:06:11,500
so I want to speak to that briefly

1196
01:06:15,500 --> 01:06:17,500
first this is a meta-point

1197
01:06:17,500 --> 01:06:19,500
but it's worth saying right now

1198
01:06:19,500 --> 01:06:21,500
particularly in on this podcast

1199
01:06:21,500 --> 01:06:23,500
and in the kind of post-truth

1200
01:06:23,500 --> 01:06:25,500
post or fake fact

1201
01:06:25,500 --> 01:06:27,500
world where then so much

1202
01:06:27,500 --> 01:06:29,500
of the emphasis has gone into we need fact

1203
01:06:29,500 --> 01:06:31,500
checkers and we need real

1204
01:06:31,500 --> 01:06:33,500
facts

1205
01:06:33,500 --> 01:06:35,500
obviously it's possible

1206
01:06:35,500 --> 01:06:37,500
to have an epistemic

1207
01:06:37,500 --> 01:06:39,500
error or even intentional error in the

1208
01:06:39,500 --> 01:06:41,500
process of generating a fact

1209
01:06:41,500 --> 01:06:43,500
is there corruption in the institutions

1210
01:06:43,500 --> 01:06:45,500
and that kind of thing but let's even say that wasn't

1211
01:06:45,500 --> 01:06:47,500
an issue and the things that go through the right epistemic

1212
01:06:47,500 --> 01:06:49,500
process as facts are facts

1213
01:06:49,500 --> 01:06:51,500
can you lie with facts

1214
01:06:51,500 --> 01:06:53,500
totally can you can you mislead

1215
01:06:53,500 --> 01:06:55,500
with facts yeah because nobody

1216
01:06:55,500 --> 01:06:57,500
is going to make their choice on one fact

1217
01:06:57,500 --> 01:06:59,500
they make their choice based on a situational assessment

1218
01:06:59,500 --> 01:07:01,500
based on a narrative based on a gestalt

1219
01:07:01,500 --> 01:07:03,500
of a whole thing that's lots of different facts

1220
01:07:03,500 --> 01:07:05,500
well which facts do I include and which facts do I not

1221
01:07:05,500 --> 01:07:07,500
include and

1222
01:07:07,500 --> 01:07:09,500
am I de-contextualizing the fact

1223
01:07:09,500 --> 01:07:11,500
so

1224
01:07:11,500 --> 01:07:13,500
the quality of life has gone up

1225
01:07:13,500 --> 01:07:15,500
so much because we average person lived

1226
01:07:15,500 --> 01:07:17,500
on less than a dollar a day in the US in 1815

1227
01:07:17,500 --> 01:07:19,500
and now they live on this many dollars

1228
01:07:19,500 --> 01:07:21,500
a day which inflation adjusted means higher quality

1229
01:07:21,500 --> 01:07:23,500
of life yeah but in 1815 most

1230
01:07:23,500 --> 01:07:25,500
of their needs didn't come through dollars

1231
01:07:25,500 --> 01:07:27,500
they grew their own vegetables they hunted

1232
01:07:27,500 --> 01:07:29,500
so I'm de-contextualizing

1233
01:07:29,500 --> 01:07:31,500
the facts to compare something that's really apples

1234
01:07:31,500 --> 01:07:33,500
and oranges so even if the fact is quote unquote

1235
01:07:33,500 --> 01:07:35,500
true the de-contextualization and

1236
01:07:35,500 --> 01:07:37,500
recontextualization makes it seem like it means

1237
01:07:37,500 --> 01:07:39,500
something different than it means

1238
01:07:39,500 --> 01:07:41,500
and the same with the cherry picking

1239
01:07:41,500 --> 01:07:43,500
of facts and I can very easily

1240
01:07:43,500 --> 01:07:45,500
say oh there's a lower percentage of people in extreme

1241
01:07:45,500 --> 01:07:47,500
poverty but I might also be changing the definitions

1242
01:07:47,500 --> 01:07:49,500
of extreme poverty I can also

1243
01:07:49,500 --> 01:07:51,500
rather than focus on percentage say well there's more

1244
01:07:51,500 --> 01:07:53,500
total people in poverty than there were total people

1245
01:07:53,500 --> 01:07:55,500
in the world before the industrial revolution so like

1246
01:07:55,500 --> 01:07:57,500
so there's the

1247
01:07:57,500 --> 01:07:59,500
abilities to de-contextualize

1248
01:07:59,500 --> 01:08:01,500
and recontextualize facts there's the ability to

1249
01:08:01,500 --> 01:08:03,500
cherry pick facts and there's

1250
01:08:03,500 --> 01:08:05,500
the ability to lake off frame facts

1251
01:08:05,500 --> 01:08:07,500
and put particular kinds

1252
01:08:07,500 --> 01:08:09,500
of sentiment and moral valence

1253
01:08:09,500 --> 01:08:11,500
on it and so am I talking about them as

1254
01:08:11,500 --> 01:08:13,500
illegal aliens or undocumented

1255
01:08:13,500 --> 01:08:15,500
workers and I get a very different kind

1256
01:08:15,500 --> 01:08:17,500
of sentiment so talking about it

1257
01:08:17,500 --> 01:08:19,500
as a pre-owned car or a used car

1258
01:08:19,500 --> 01:08:21,500
everyone loves a pre-owned car no one wants

1259
01:08:21,500 --> 01:08:23,500
a used car and so these

1260
01:08:23,500 --> 01:08:25,500
very simple semantic frames

1261
01:08:25,500 --> 01:08:27,500
contextual frames cherry picking

1262
01:08:27,500 --> 01:08:29,500
of the things means that I can

1263
01:08:29,500 --> 01:08:31,500
make a narrative

1264
01:08:31,500 --> 01:08:33,500
where all the facts went through the most

1265
01:08:33,500 --> 01:08:35,500
rigorous fact checker and yet the narrative

1266
01:08:35,500 --> 01:08:37,500
as a whole is misleading

1267
01:08:37,500 --> 01:08:39,500
and so fact checking is necessary

1268
01:08:39,500 --> 01:08:41,500
but it is not sufficient for a good

1269
01:08:41,500 --> 01:08:43,500
epistemic and good sense making and not

1270
01:08:43,500 --> 01:08:45,500
only is it not sufficient it's even weaponizable

1271
01:08:45,500 --> 01:08:47,500
this is a very important thing

1272
01:08:47,500 --> 01:08:49,500
to understand because if you are not

1273
01:08:49,500 --> 01:08:51,500
pursuing that

1274
01:08:51,500 --> 01:08:53,500
you're if you're not recognizing

1275
01:08:53,500 --> 01:08:55,500
that you might be believing

1276
01:08:55,500 --> 01:08:57,500
nonsense thinking that you're using

1277
01:08:57,500 --> 01:08:59,500
epistemic rigor

1278
01:09:01,500 --> 01:09:03,500
okay so the techno pessimists

1279
01:09:03,500 --> 01:09:05,500
and the techno optimists both cherry pick

1280
01:09:05,500 --> 01:09:07,500
and they both lake off

1281
01:09:07,500 --> 01:09:09,500
frame and

1282
01:09:09,500 --> 01:09:11,500
this is true with

1283
01:09:11,500 --> 01:09:13,500
all the difference in almost every

1284
01:09:13,500 --> 01:09:15,500
political ideology the woke and the

1285
01:09:15,500 --> 01:09:17,500
woke the

1286
01:09:17,500 --> 01:09:19,500
the pro socialist

1287
01:09:19,500 --> 01:09:21,500
pro capitalist you'll notice that the way

1288
01:09:21,500 --> 01:09:23,500
they do their arguments the

1289
01:09:23,500 --> 01:09:25,500
systemic racism is really really terrible

1290
01:09:25,500 --> 01:09:27,500
no there's not that bad the systemic racism

1291
01:09:27,500 --> 01:09:29,500
they both have stats

1292
01:09:29,500 --> 01:09:31,500
but this is actually you can almost think

1293
01:09:31,500 --> 01:09:33,500
of it as statistical warfare as a tool

1294
01:09:33,500 --> 01:09:35,500
of narrative warfare and

1295
01:09:37,500 --> 01:09:39,500
so this is where

1296
01:09:39,500 --> 01:09:41,500
a higher level of earnestness

1297
01:09:41,500 --> 01:09:43,500
rather than a particular

1298
01:09:43,500 --> 01:09:45,500
bested interest or bias a higher willingness

1299
01:09:45,500 --> 01:09:47,500
to look at biases a higher level of rigor

1300
01:09:47,500 --> 01:09:49,500
you know ends up being critical to

1301
01:09:49,500 --> 01:09:51,500
actually overcome any of these things

1302
01:09:51,500 --> 01:09:53,500
so

1303
01:09:53,500 --> 01:09:55,500
can I cherry pick stats that make it look like everything is getting

1304
01:09:55,500 --> 01:09:57,500
better totally those things are true

1305
01:09:57,500 --> 01:09:59,500
and nobody wants to go back to a world

1306
01:09:59,500 --> 01:10:01,500
before novocaine when you have to do dentistry

1307
01:10:01,500 --> 01:10:03,500
and

1308
01:10:03,500 --> 01:10:05,500
nobody wants to go back to a world before

1309
01:10:05,500 --> 01:10:07,500
penicillin when basic bacterial infections go around

1310
01:10:07,500 --> 01:10:09,500
and like there's totally good stuff that has

1311
01:10:09,500 --> 01:10:11,500
emerged

1312
01:10:11,500 --> 01:10:13,500
and are there all

1313
01:10:13,500 --> 01:10:15,500
kinds of

1314
01:10:15,500 --> 01:10:17,500
ubiquitous

1315
01:10:17,500 --> 01:10:19,500
mental illnesses and

1316
01:10:19,500 --> 01:10:21,500
chronic complex disease

1317
01:10:21,500 --> 01:10:23,500
that didn't exist before and

1318
01:10:23,500 --> 01:10:25,500
increase in the total

1319
01:10:25,500 --> 01:10:27,500
number of addictive type behaviors within populations

1320
01:10:27,500 --> 01:10:29,500
and

1321
01:10:29,500 --> 01:10:31,500
and a radical

1322
01:10:31,500 --> 01:10:33,500
increase in the catastrophic risk landscape

1323
01:10:33,500 --> 01:10:35,500
and negative effect to environmental metrics

1324
01:10:35,500 --> 01:10:37,500
so things are getting better and things are getting worse at the same time

1325
01:10:37,500 --> 01:10:39,500
it's important to understand that depending upon what you pick

1326
01:10:39,500 --> 01:10:41,500
it's just that the things that are getting worse are heading

1327
01:10:41,500 --> 01:10:43,500
towards

1328
01:10:43,500 --> 01:10:45,500
tipping points that make the whole thing no longer viable

1329
01:10:45,500 --> 01:10:47,500
and so that we're not

1330
01:10:47,500 --> 01:10:49,500
denying that there are things that are getting better we're

1331
01:10:49,500 --> 01:10:51,500
saying that for the game to continue

1332
01:10:51,500 --> 01:10:53,500
at all right to have it

1333
01:10:53,500 --> 01:10:55,500
be an infinite game that gets to keep continuing

1334
01:10:55,500 --> 01:10:57,500
there are certain things that have to not happen

1335
01:10:57,500 --> 01:10:59,500
and you can't have the things that are getting worse keep getting

1336
01:10:59,500 --> 01:11:01,500
worse at the curve that they are

1337
01:11:01,500 --> 01:11:03,500
and have the things that are getting better be able to

1338
01:11:03,500 --> 01:11:05,500
continue at all so I just want to

1339
01:11:05,500 --> 01:11:07,500
say that so

1340
01:11:07,500 --> 01:11:09,500
naive techno optimism

1341
01:11:09,500 --> 01:11:11,500
can actually

1342
01:11:11,500 --> 01:11:13,500
make you a part of the problem because

1343
01:11:13,500 --> 01:11:15,500
then you do things like develop

1344
01:11:15,500 --> 01:11:17,500
a solution to a narrowly

1345
01:11:17,500 --> 01:11:19,500
defined problem and externalize harm to other areas

1346
01:11:19,500 --> 01:11:21,500
because you weren't taking seriously enough

1347
01:11:21,500 --> 01:11:23,500
not doing that

1348
01:11:23,500 --> 01:11:25,500
but techno pessimism also

1349
01:11:25,500 --> 01:11:27,500
makes you a part of the problem

1350
01:11:27,500 --> 01:11:29,500
or at least not a part of the solution because

1351
01:11:29,500 --> 01:11:31,500
because the world is the future

1352
01:11:31,500 --> 01:11:33,500
is not going to be determined by Luddites

1353
01:11:33,500 --> 01:11:35,500
it's not going to be determined by people who aren't developing the tools of power

1354
01:11:35,500 --> 01:11:37,500
so if you aren't actually looking at

1355
01:11:37,500 --> 01:11:39,500
how do we develop a high

1356
01:11:39,500 --> 01:11:41,500
tech world that is also a fundamentally desirable

1357
01:11:41,500 --> 01:11:43,500
in terms of a high nature and high touch

1358
01:11:43,500 --> 01:11:45,500
world then you really

1359
01:11:45,500 --> 01:11:47,500
aren't thinking about it in a way that ends up

1360
01:11:47,500 --> 01:11:49,500
mattering and so we are

1361
01:11:49,500 --> 01:11:51,500
techno optimist but

1362
01:11:51,500 --> 01:11:53,500
not naive techno optimist we go through the

1363
01:11:53,500 --> 01:11:55,500
totally cynical phase of man tech

1364
01:11:55,500 --> 01:11:57,500
is a serious issue and then you go to a post

1365
01:11:57,500 --> 01:11:59,500
cynical phase of

1366
01:11:59,500 --> 01:12:01,500
if I want to be techno optimist

1367
01:12:01,500 --> 01:12:03,500
and not be silly

1368
01:12:03,500 --> 01:12:05,500
what does it take to imagine a world

1369
01:12:05,500 --> 01:12:07,500
where humans have that much power

1370
01:12:07,500 --> 01:12:09,500
and we are good stewards of it

1371
01:12:09,500 --> 01:12:11,500
meaning that we actually tend to each other

1372
01:12:11,500 --> 01:12:13,500
well and we don't create a

1373
01:12:13,500 --> 01:12:15,500
dystopic world that has

1374
01:12:15,500 --> 01:12:17,500
exponential wealth and equality

1375
01:12:17,500 --> 01:12:19,500
and an underclass that nobody in the upper class would want to trade places with

1376
01:12:19,500 --> 01:12:21,500
and that doesn't cause catastrophic risk

1377
01:12:21,500 --> 01:12:23,500
right now the amount

1378
01:12:23,500 --> 01:12:25,500
of power of exponential tech

1379
01:12:25,500 --> 01:12:27,500
makes two attractors most likely

1380
01:12:27,500 --> 01:12:29,500
catastrophic risk

1381
01:12:29,500 --> 01:12:31,500
of some kind

1382
01:12:31,500 --> 01:12:33,500
or social systems

1383
01:12:33,500 --> 01:12:35,500
that do not preserve

1384
01:12:35,500 --> 01:12:37,500
the values that we care most about

1385
01:12:37,500 --> 01:12:39,500
that are the ones that are currently most

1386
01:12:39,500 --> 01:12:41,500
working to develop and deploy that technology

1387
01:12:41,500 --> 01:12:43,500
and to just give a very brief

1388
01:12:43,500 --> 01:12:45,500
recap of the frame that Tristan

1389
01:12:45,500 --> 01:12:47,500
you gave on earlier as you mentioned

1390
01:12:49,500 --> 01:12:51,500
China is not leaving 100%

1391
01:12:51,500 --> 01:12:53,500
of its technology

1392
01:12:53,500 --> 01:12:55,500
development to the market to develop however

1393
01:12:55,500 --> 01:12:57,500
it wants even if it harms the nation state

1394
01:12:57,500 --> 01:12:59,500
they are happy to bind technology

1395
01:12:59,500 --> 01:13:01,500
companies that are getting too large

1396
01:13:01,500 --> 01:13:03,500
and in ways that would damage the nation state

1397
01:13:03,500 --> 01:13:05,500
as we saw with Ant Corporation

1398
01:13:05,500 --> 01:13:07,500
and they are doing a lot of

1399
01:13:07,500 --> 01:13:09,500
very centralized innovation

1400
01:13:09,500 --> 01:13:11,500
as well

1401
01:13:11,500 --> 01:13:13,500
associated with long term planning

1402
01:13:13,500 --> 01:13:15,500
long term planning is a key thing

1403
01:13:15,500 --> 01:13:17,500
in the US

1404
01:13:17,500 --> 01:13:19,500
term limits make long term planning very hard

1405
01:13:19,500 --> 01:13:21,500
as does a highly rival risk

1406
01:13:21,500 --> 01:13:23,500
two party system that is

1407
01:13:23,500 --> 01:13:25,500
willing to damage the

1408
01:13:25,500 --> 01:13:27,500
nation as a whole to drive party wins

1409
01:13:27,500 --> 01:13:29,500
so in that system

1410
01:13:29,500 --> 01:13:31,500
almost all the energy

1411
01:13:31,500 --> 01:13:33,500
just goes into trying to win

1412
01:13:33,500 --> 01:13:35,500
you spend at least a couple years

1413
01:13:35,500 --> 01:13:37,500
but even the years before that are fundraising

1414
01:13:37,500 --> 01:13:39,500
creating alliances to just try to win

1415
01:13:39,500 --> 01:13:41,500
then you are not going to invest in anything

1416
01:13:41,500 --> 01:13:43,500
heavily that has return times

1417
01:13:43,500 --> 01:13:45,500
longer than four years because it won't get you re-elected

1418
01:13:45,500 --> 01:13:47,500
so no real long term planning

1419
01:13:47,500 --> 01:13:49,500
and then

1420
01:13:49,500 --> 01:13:51,500
whatever you do do in those four years

1421
01:13:51,500 --> 01:13:53,500
will get undone systematically in the next four years

1422
01:13:53,500 --> 01:13:55,500
for the most part

1423
01:13:55,500 --> 01:13:57,500
that system of governance

1424
01:13:57,500 --> 01:13:59,500
will just fail comprehensively

1425
01:14:01,500 --> 01:14:03,500
in relationship to a more

1426
01:14:05,500 --> 01:14:07,500
to a system that

1427
01:14:07,500 --> 01:14:09,500
doesn't have that much internal infighting

1428
01:14:09,500 --> 01:14:11,500
and that has

1429
01:14:11,500 --> 01:14:13,500
the capacity to do long term planning

1430
01:14:13,500 --> 01:14:15,500
and there is a million examples we can look at

1431
01:14:15,500 --> 01:14:17,500
but just when did high speed

1432
01:14:17,500 --> 01:14:19,500
trains start? they started

1433
01:14:19,500 --> 01:14:21,500
we saw them emerge in Europe

1434
01:14:21,500 --> 01:14:23,500
we saw them emerge in Japan and in China

1435
01:14:23,500 --> 01:14:25,500
we started to export them all around the world

1436
01:14:25,500 --> 01:14:27,500
and the US still doesn't have any high speed trains

1437
01:14:27,500 --> 01:14:29,500
and it's like what happened?

1438
01:14:29,500 --> 01:14:31,500
and we can see

1439
01:14:31,500 --> 01:14:33,500
that the US innovated in fundamental

1440
01:14:33,500 --> 01:14:35,500
tech in the Manhattan project

1441
01:14:35,500 --> 01:14:37,500
kind of through the Apollo project

1442
01:14:37,500 --> 01:14:39,500
but then it started to privatize almost everything to the market

1443
01:14:39,500 --> 01:14:41,500
the market started to develop in ways that really

1444
01:14:41,500 --> 01:14:43,500
were not advancing the technology

1445
01:14:43,500 --> 01:14:45,500
in a way that increased the coherence

1446
01:14:45,500 --> 01:14:47,500
of the nation

1447
01:14:47,500 --> 01:14:49,500
and the fundamental civic values and ideas of the nation

1448
01:14:49,500 --> 01:14:51,500
even the World War II thing

1449
01:14:51,500 --> 01:14:53,500
increased our military capacities radically

1450
01:14:53,500 --> 01:14:55,500
but that didn't mean we actually really advanced the ideas

1451
01:14:55,500 --> 01:14:57,500
of democracy or those values of

1452
01:14:57,500 --> 01:14:59,500
do we make a better system

1453
01:14:59,500 --> 01:15:01,500
to educate the people and inform them

1454
01:15:01,500 --> 01:15:03,500
and help them participate in their governance

1455
01:15:03,500 --> 01:15:05,500
do we make better governance?

1456
01:15:05,500 --> 01:15:07,500
this is why the US military is so powerful

1457
01:15:07,500 --> 01:15:09,500
but the US government is so kind of inept

1458
01:15:09,500 --> 01:15:11,500
which is why nobody wants to fight a war

1459
01:15:11,500 --> 01:15:13,500
with the US, a kinetic war

1460
01:15:13,500 --> 01:15:15,500
but it's very easy

1461
01:15:15,500 --> 01:15:17,500
right now to

1462
01:15:17,500 --> 01:15:19,500
engage in

1463
01:15:19,500 --> 01:15:21,500
supporting narrative warfare

1464
01:15:21,500 --> 01:15:23,500
where you turn the left and the right against each other

1465
01:15:23,500 --> 01:15:25,500
increasingly

1466
01:15:25,500 --> 01:15:27,500
and where you do long-term planning

1467
01:15:27,500 --> 01:15:29,500
where the US can't do long-term planning of those kinds

1468
01:15:29,500 --> 01:15:31,500
so we can see

1469
01:15:31,500 --> 01:15:33,500
that the

1470
01:15:33,500 --> 01:15:35,500
government of the US

1471
01:15:35,500 --> 01:15:37,500
and not just the US but like we can see that

1472
01:15:37,500 --> 01:15:39,500
open societies are not innovating

1473
01:15:39,500 --> 01:15:41,500
in how to be better open societies

1474
01:15:41,500 --> 01:15:43,500
for the most part, more effective ones

1475
01:15:43,500 --> 01:15:45,500
where they're using the new tech to make better open societies

1476
01:15:45,500 --> 01:15:47,500
that's happening in the market sector

1477
01:15:47,500 --> 01:15:49,500
the market is making exponentially

1478
01:15:49,500 --> 01:15:51,500
more powerful companies

1479
01:15:51,500 --> 01:15:53,500
a company is not a democracy

1480
01:15:53,500 --> 01:15:55,500
it's not a participatory governance structure

1481
01:15:55,500 --> 01:15:57,500
in general, it's a kind of very top-down

1482
01:15:57,500 --> 01:15:59,500
autocratic type system

1483
01:15:59,500 --> 01:16:01,500
and so we see that

1484
01:16:01,500 --> 01:16:03,500
there's more authoritarian

1485
01:16:03,500 --> 01:16:05,500
nation states that are

1486
01:16:05,500 --> 01:16:07,500
intentionally doing long-term

1487
01:16:07,500 --> 01:16:09,500
planning of the development and deployment of

1488
01:16:09,500 --> 01:16:11,500
exponential tech to make better

1489
01:16:11,500 --> 01:16:13,500
nation states of that kind

1490
01:16:13,500 --> 01:16:15,500
and we can't even blame them

1491
01:16:15,500 --> 01:16:17,500
they look at

1492
01:16:17,500 --> 01:16:19,500
trying to have the benefit of getting to see both where the USA

1493
01:16:19,500 --> 01:16:21,500
failed and where the USSR failed

1494
01:16:21,500 --> 01:16:23,500
and try to make something they didn't fail in either of those ways

1495
01:16:23,500 --> 01:16:25,500
and there's some things that are very smart about

1496
01:16:25,500 --> 01:16:27,500
those approaches

1497
01:16:27,500 --> 01:16:29,500
so we see though exponentially empowered

1498
01:16:31,500 --> 01:16:33,500
more autocratic type structures

1499
01:16:33,500 --> 01:16:35,500
and the emergence of

1500
01:16:35,500 --> 01:16:37,500
one natural

1501
01:16:37,500 --> 01:16:39,500
monopoly per tech sector

1502
01:16:39,500 --> 01:16:41,500
and then the interaction of those that kind of

1503
01:16:41,500 --> 01:16:43,500
becomes like oligarchic feudalism

1504
01:16:43,500 --> 01:16:45,500
tech feudalism

1505
01:16:45,500 --> 01:16:47,500
neither of those

1506
01:16:47,500 --> 01:16:49,500
have the types of jurisprudence

1507
01:16:49,500 --> 01:16:51,500
or public accountability or whatever

1508
01:16:51,500 --> 01:16:53,500
that we're really interested in

1509
01:16:53,500 --> 01:16:55,500
so the two attractors right now

1510
01:16:55,500 --> 01:16:57,500
is

1511
01:16:57,500 --> 01:16:59,500
the emergence of social systems

1512
01:16:59,500 --> 01:17:01,500
that are deploying

1513
01:17:01,500 --> 01:17:03,500
the exponential tech

1514
01:17:03,500 --> 01:17:05,500
that will probably not preserve

1515
01:17:05,500 --> 01:17:07,500
the social values that we're interested in

1516
01:17:07,500 --> 01:17:09,500
and not be maximally desirable civilizations

1517
01:17:09,500 --> 01:17:11,500
probably pretty dystopic ones

1518
01:17:11,500 --> 01:17:13,500
or not even guiding it well enough

1519
01:17:13,500 --> 01:17:15,500
to prevent catastrophic risk

1520
01:17:15,500 --> 01:17:17,500
those are the two major types of attractors

1521
01:17:17,500 --> 01:17:19,500
we want a new attractor which is

1522
01:17:19,500 --> 01:17:21,500
how do we

1523
01:17:21,500 --> 01:17:23,500
utilize the new exponential technologies

1524
01:17:23,500 --> 01:17:25,500
the whole suite of them

1525
01:17:25,500 --> 01:17:27,500
to build new systems of collective intelligence

1526
01:17:27,500 --> 01:17:29,500
new better systems of social technology

1527
01:17:29,500 --> 01:17:31,500
how do you make a fourth estate that can really adequately

1528
01:17:31,500 --> 01:17:33,500
educate everyone

1529
01:17:33,500 --> 01:17:35,500
in a post

1530
01:17:35,500 --> 01:17:37,500
Facebook world

1531
01:17:37,500 --> 01:17:39,500
well the same way that we're trying to

1532
01:17:39,500 --> 01:17:41,500
optimize control patterns

1533
01:17:41,500 --> 01:17:43,500
of human behavior for market purposes

1534
01:17:43,500 --> 01:17:45,500
to get them to buy certain things

1535
01:17:45,500 --> 01:17:47,500
and to direct their attention

1536
01:17:47,500 --> 01:17:49,500
could that be used educationally

1537
01:17:49,500 --> 01:17:51,500
of course it could if it was being

1538
01:17:51,500 --> 01:17:53,500
developed for that purpose

1539
01:17:53,500 --> 01:17:55,500
and the AI tech

1540
01:17:55,500 --> 01:17:57,500
that can take a bunch of faces

1541
01:17:57,500 --> 01:17:59,500
and make a new face that is merged out of those

1542
01:17:59,500 --> 01:18:01,500
could it take semantic fields of people's propositions

1543
01:18:01,500 --> 01:18:03,500
and values and create a proposition

1544
01:18:03,500 --> 01:18:05,500
that is kind of the semantic

1545
01:18:05,500 --> 01:18:07,500
center of the space and then could we use

1546
01:18:07,500 --> 01:18:09,500
we can't all fit into a town hall

1547
01:18:09,500 --> 01:18:11,500
but can we engage in digital

1548
01:18:11,500 --> 01:18:13,500
spaces where we can have

1549
01:18:13,500 --> 01:18:15,500
better processes of proposing

1550
01:18:15,500 --> 01:18:17,500
refinements to the propositions

1551
01:18:17,500 --> 01:18:19,500
of course we can, could we use blockchain

1552
01:18:19,500 --> 01:18:21,500
and other types of uncorruptible ledgers

1553
01:18:21,500 --> 01:18:23,500
to solve corruption which is something that universally

1554
01:18:23,500 --> 01:18:25,500
everybody thinks is a good idea

1555
01:18:25,500 --> 01:18:27,500
should all government money

1556
01:18:27,500 --> 01:18:29,500
be on a blockchain

1557
01:18:29,500 --> 01:18:31,500
the movement of it so you have provenance so you can see

1558
01:18:31,500 --> 01:18:33,500
where the money is actually going and if someone wants to be

1559
01:18:33,500 --> 01:18:35,500
a private contractor

1560
01:18:35,500 --> 01:18:37,500
they have to agree that the accounting system

1561
01:18:37,500 --> 01:18:39,500
if they want government money

1562
01:18:39,500 --> 01:18:41,500
goes on the blockchain so we can see the entire provenance

1563
01:18:41,500 --> 01:18:43,500
of the taxpayer money so that

1564
01:18:43,500 --> 01:18:45,500
you can't have representation if there isn't

1565
01:18:45,500 --> 01:18:47,500
transparency of how it happens

1566
01:18:47,500 --> 01:18:49,500
so there's a whole

1567
01:18:49,500 --> 01:18:51,500
bunch of when you start to think about

1568
01:18:51,500 --> 01:18:53,500
attention directing technology

1569
01:18:53,500 --> 01:18:55,500
and what its pedagogical applications could be

1570
01:18:55,500 --> 01:18:57,500
when you start to think about

1571
01:18:57,500 --> 01:18:59,500
AI and how it could actually help

1572
01:18:59,500 --> 01:19:01,500
proposition development

1573
01:19:01,500 --> 01:19:03,500
and parsing huge amounts of information to make a better

1574
01:19:03,500 --> 01:19:05,500
epistemic commons when you start to think about

1575
01:19:05,500 --> 01:19:07,500
blockchain and could we actually resolve corruption

1576
01:19:07,500 --> 01:19:09,500
using uncorruptible ledgers and making

1577
01:19:09,500 --> 01:19:11,500
the provenance of physical supply chains

1578
01:19:11,500 --> 01:19:13,500
and information and money all flow

1579
01:19:13,500 --> 01:19:15,500
across those, totally new possibilities

1580
01:19:15,500 --> 01:19:17,500
start to emerge

1581
01:19:17,500 --> 01:19:19,500
that never emerged before, that were never possible

1582
01:19:19,500 --> 01:19:21,500
before

1583
01:19:21,500 --> 01:19:23,500
but if it doesn't become our central design imperative

1584
01:19:23,500 --> 01:19:25,500
to develop those, those are not

1585
01:19:25,500 --> 01:19:27,500
the highest marketed

1586
01:19:27,500 --> 01:19:29,500
opportunities for those right now

1587
01:19:29,500 --> 01:19:31,500
the highest market opportunity for blockchain is speculative

1588
01:19:31,500 --> 01:19:33,500
tokens that have no real utility

1589
01:19:33,500 --> 01:19:35,500
and for AI is things that actually

1590
01:19:35,500 --> 01:19:37,500
drive ads and purchasing

1591
01:19:37,500 --> 01:19:39,500
and you know, on and on

1592
01:19:39,500 --> 01:19:41,500
and for attention tech it is the same thing

1593
01:19:43,500 --> 01:19:45,500
So you've sold me on the idea that we

1594
01:19:45,500 --> 01:19:47,500
have two dystopian attractors

1595
01:19:47,500 --> 01:19:49,500
that we don't want and the third

1596
01:19:49,500 --> 01:19:51,500
attractor that we're trying to develop

1597
01:19:51,500 --> 01:19:53,500
here is some kind of open society

1598
01:19:53,500 --> 01:19:55,500
that is consciously using

1599
01:19:55,500 --> 01:19:57,500
all the modern technologies

1600
01:19:57,500 --> 01:19:59,500
towards the values that we care about

1601
01:19:59,500 --> 01:20:01,500
can you give some concrete examples

1602
01:20:01,500 --> 01:20:03,500
of what it would look like to

1603
01:20:03,500 --> 01:20:05,500
use you know, AI and attention

1604
01:20:05,500 --> 01:20:07,500
driving tech and click driving tech

1605
01:20:07,500 --> 01:20:09,500
and block chains and all these things

1606
01:20:09,500 --> 01:20:11,500
but in a way that would make a stronger, healthier open society

1607
01:20:11,500 --> 01:20:13,500
Yeah, totally

1608
01:20:17,500 --> 01:20:19,500
So let's say we take

1609
01:20:19,500 --> 01:20:21,500
the attention

1610
01:20:21,500 --> 01:20:23,500
tech that you've looked at so much

1611
01:20:23,500 --> 01:20:25,500
that when it is applied

1612
01:20:25,500 --> 01:20:27,500
for a commercial application

1613
01:20:27,500 --> 01:20:29,500
it is seeking to gather data

1614
01:20:29,500 --> 01:20:31,500
to both maximize time on site

1615
01:20:31,500 --> 01:20:33,500
and maximize engagement with certain kinds of ads

1616
01:20:33,500 --> 01:20:35,500
and whatever

1617
01:20:35,500 --> 01:20:37,500
That's obviously

1618
01:20:37,500 --> 01:20:39,500
the ability to

1619
01:20:39,500 --> 01:20:41,500
direct human behavior

1620
01:20:41,500 --> 01:20:43,500
and direct human feeling

1621
01:20:43,500 --> 01:20:45,500
and thought

1622
01:20:45,500 --> 01:20:47,500
In a way

1623
01:20:47,500 --> 01:20:49,500
that has both emerged out of capitalism

1624
01:20:49,500 --> 01:20:51,500
and has become

1625
01:20:51,500 --> 01:20:53,500
almost a new macroeconomic structure

1626
01:20:53,500 --> 01:20:55,500
more powerful than capitalism

1627
01:20:55,500 --> 01:20:57,500
more powerful than being able to

1628
01:20:57,500 --> 01:20:59,500
incent people's behavior with money

1629
01:20:59,500 --> 01:21:01,500
as being able to direct what they think and feel

1630
01:21:01,500 --> 01:21:03,500
to where the thing that they think of

1631
01:21:03,500 --> 01:21:05,500
as their own intrinsic motive

1632
01:21:05,500 --> 01:21:07,500
has actually been influenced or captured

1633
01:21:07,500 --> 01:21:09,500
So

1634
01:21:09,500 --> 01:21:11,500
if we

1635
01:21:11,500 --> 01:21:13,500
if we wanted to

1636
01:21:13,500 --> 01:21:15,500
apply that type of technology

1637
01:21:15,500 --> 01:21:17,500
and we figured out how to make

1638
01:21:17,500 --> 01:21:19,500
the kind of transparency

1639
01:21:19,500 --> 01:21:21,500
that made institutions

1640
01:21:21,500 --> 01:21:23,500
that were trustworthy enough

1641
01:21:23,500 --> 01:21:25,500
and already we have institutions that have it

1642
01:21:25,500 --> 01:21:27,500
that we have no basis to trust with it

1643
01:21:27,500 --> 01:21:29,500
could that same

1644
01:21:29,500 --> 01:21:31,500
tech be used educationally

1645
01:21:31,500 --> 01:21:33,500
to be able to personalize education

1646
01:21:33,500 --> 01:21:35,500
to the learning style

1647
01:21:35,500 --> 01:21:37,500
of a kid or to an adult to their particular

1648
01:21:37,500 --> 01:21:39,500
areas of interest

1649
01:21:39,500 --> 01:21:41,500
and to be able to

1650
01:21:41,500 --> 01:21:43,500
not

1651
01:21:43,500 --> 01:21:45,500
use

1652
01:21:45,500 --> 01:21:47,500
the ability to control them

1653
01:21:47,500 --> 01:21:49,500
for game-theoretic purposes

1654
01:21:49,500 --> 01:21:51,500
but use the ability to influence them

1655
01:21:51,500 --> 01:21:53,500
to even help them learn

1656
01:21:53,500 --> 01:21:55,500
what makes their own

1657
01:21:55,500 --> 01:21:57,500
center

1658
01:21:57,500 --> 01:21:59,500
their locus of action more internalized

1659
01:21:59,500 --> 01:22:01,500
we could teach people

1660
01:22:01,500 --> 01:22:03,500
with that kind of tech

1661
01:22:03,500 --> 01:22:05,500
how to notice their own bias

1662
01:22:05,500 --> 01:22:07,500
how to notice their own emotional behaviors

1663
01:22:07,500 --> 01:22:09,500
how to notice groupthink type dynamics

1664
01:22:09,500 --> 01:22:11,500
how to understand propaganda and media literacy

1665
01:22:11,500 --> 01:22:13,500
so could we actually use those tools

1666
01:22:13,500 --> 01:22:15,500
to increase people's immune system

1667
01:22:15,500 --> 01:22:17,500
against bad actors' use of those tools

1668
01:22:17,500 --> 01:22:19,500
totally

1669
01:22:19,500 --> 01:22:21,500
could we use them pedagogically

1670
01:22:21,500 --> 01:22:23,500
in general to be able to identify

1671
01:22:23,500 --> 01:22:25,500
rather than manufacturing

1672
01:22:25,500 --> 01:22:27,500
desires in people

1673
01:22:27,500 --> 01:22:29,500
or appealing to the lowest angels of their nature

1674
01:22:29,500 --> 01:22:31,500
because addiction is profitable

1675
01:22:31,500 --> 01:22:33,500
can you appeal to the highest

1676
01:22:33,500 --> 01:22:35,500
angels in people's nature

1677
01:22:35,500 --> 01:22:37,500
but that are aligned with intrinsic incentives

1678
01:22:37,500 --> 01:22:39,500
and be able to

1679
01:22:39,500 --> 01:22:41,500
create customized educational programs

1680
01:22:41,500 --> 01:22:43,500
that are based on what each person is actually

1681
01:22:43,500 --> 01:22:45,500
innately intrinsically motivated by

1682
01:22:45,500 --> 01:22:47,500
but that are their higher innate motivators

1683
01:22:47,500 --> 01:22:49,500
everybody can have a reward circuit

1684
01:22:49,500 --> 01:22:51,500
that is based on

1685
01:22:51,500 --> 01:22:53,500
you know chocolate cake and sloth

1686
01:22:53,500 --> 01:22:55,500
but the immediate spike

1687
01:22:55,500 --> 01:22:57,500
that comes from the chocolate cake

1688
01:22:57,500 --> 01:22:59,500
ends up then having a crash

1689
01:22:59,500 --> 01:23:01,500
and increased weight and inflammation

1690
01:23:01,500 --> 01:23:03,500
and whatever where the baseline of their happiness

1691
01:23:03,500 --> 01:23:05,500
goes down over time

1692
01:23:05,500 --> 01:23:07,500
even though every time they eat the chocolate cake they get a spike

1693
01:23:07,500 --> 01:23:09,500
the exercise reward circuit is maybe

1694
01:23:09,500 --> 01:23:11,500
not that fun maybe even kind of painful

1695
01:23:11,500 --> 01:23:13,500
and dreadful in the moment

1696
01:23:13,500 --> 01:23:15,500
but then creates a higher baseline

1697
01:23:15,500 --> 01:23:17,500
of energy and capacity

1698
01:23:17,500 --> 01:23:19,500
and endurance and self-esteem

1699
01:23:19,500 --> 01:23:21,500
and you start to actually have the process become

1700
01:23:21,500 --> 01:23:23,500
more fun you get a new reward circuit

1701
01:23:23,500 --> 01:23:25,500
and the baseline goes up

1702
01:23:25,500 --> 01:23:27,500
so of course I can appeal to the lower reward

1703
01:23:27,500 --> 01:23:29,500
circuit and say hey I'm just giving people

1704
01:23:29,500 --> 01:23:31,500
what they want

1705
01:23:31,500 --> 01:23:33,500
but if you have a billion dollar

1706
01:23:33,500 --> 01:23:35,500
or a trillion dollar organization

1707
01:23:35,500 --> 01:23:37,500
that is preying upon them

1708
01:23:37,500 --> 01:23:39,500
and you discuss this very well all the time

1709
01:23:39,500 --> 01:23:41,500
the vulnerabilities that make people's life

1710
01:23:41,500 --> 01:23:43,500
worse to then have the plausible deniability

1711
01:23:43,500 --> 01:23:45,500
to say yeah but they wanted it

1712
01:23:45,500 --> 01:23:47,500
but it was a manufactured demand

1713
01:23:47,500 --> 01:23:49,500
and a vulnerability where's the no bless

1714
01:23:49,500 --> 01:23:51,500
oblige where's the obligation

1715
01:23:51,500 --> 01:23:53,500
of having that much power to actually be

1716
01:23:53,500 --> 01:23:55,500
a good steward of power a steward of that

1717
01:23:55,500 --> 01:23:57,500
for other people where if there are

1718
01:23:57,500 --> 01:23:59,500
reward circuits that decrease the quality

1719
01:23:59,500 --> 01:24:01,500
of their life reward circuits that increase

1720
01:24:01,500 --> 01:24:03,500
that we're trying to appeal to one rather than the other

1721
01:24:03,500 --> 01:24:05,500
could we do that yeah totally we could

1722
01:24:05,500 --> 01:24:07,500
could we have an education system as a result

1723
01:24:07,500 --> 01:24:09,500
that was identifying innate

1724
01:24:09,500 --> 01:24:11,500
aptitudes innate interests

1725
01:24:11,500 --> 01:24:13,500
of everyone and facilitating

1726
01:24:13,500 --> 01:24:15,500
their development so not only did they

1727
01:24:15,500 --> 01:24:17,500
become

1728
01:24:17,500 --> 01:24:19,500
good at something but they became increasingly

1729
01:24:19,500 --> 01:24:21,500
more intrinsically motivated fascinated

1730
01:24:21,500 --> 01:24:23,500
and passionate by life

1731
01:24:23,500 --> 01:24:25,500
which also meant continuously better at the thing

1732
01:24:25,500 --> 01:24:27,500
well in a world of

1733
01:24:27,500 --> 01:24:29,500
increasing technological automation

1734
01:24:29,500 --> 01:24:31,500
coming up both robotic and AI automation

1735
01:24:31,500 --> 01:24:33,500
where so many of the jobs are about to be

1736
01:24:33,500 --> 01:24:35,500
obsolete

1737
01:24:35,500 --> 01:24:37,500
our economy

1738
01:24:37,500 --> 01:24:39,500
and our education system

1739
01:24:39,500 --> 01:24:41,500
have to radically change to deal with that

1740
01:24:41,500 --> 01:24:43,500
because

1741
01:24:43,500 --> 01:24:45,500
the core of like one of the core

1742
01:24:45,500 --> 01:24:47,500
things an economy has been trying to do forever

1743
01:24:47,500 --> 01:24:49,500
was

1744
01:24:49,500 --> 01:24:51,500
deal with the need that a society had

1745
01:24:51,500 --> 01:24:53,500
for a labor force

1746
01:24:53,500 --> 01:24:55,500
and that there were these jobs that society

1747
01:24:55,500 --> 01:24:57,500
needed to get done that nobody would really want to do

1748
01:24:57,500 --> 01:24:59,500
so either the state has to force them to do it

1749
01:24:59,500 --> 01:25:01,500
or

1750
01:25:01,500 --> 01:25:03,500
you have to make it to where the people also need the job

1751
01:25:03,500 --> 01:25:05,500
so there's a cemetery and so kind of the market

1752
01:25:05,500 --> 01:25:07,500
forces them to do it well when you technologically

1753
01:25:07,500 --> 01:25:09,500
automate those jobs and it happens

1754
01:25:09,500 --> 01:25:11,500
to be that the things that are the most

1755
01:25:11,500 --> 01:25:13,500
wrote are the least fun for people

1756
01:25:13,500 --> 01:25:15,500
and the easiest to program machines to do

1757
01:25:15,500 --> 01:25:17,500
and

1758
01:25:17,500 --> 01:25:19,500
so

1759
01:25:19,500 --> 01:25:21,500
if you keep the same economy

1760
01:25:21,500 --> 01:25:23,500
where if people don't produce

1761
01:25:23,500 --> 01:25:25,500
they don't have any basic needs met

1762
01:25:25,500 --> 01:25:27,500
then people want

1763
01:25:27,500 --> 01:25:29,500
those crappy jobs right

1764
01:25:29,500 --> 01:25:31,500
but if you make it to where they have other

1765
01:25:31,500 --> 01:25:33,500
opportunities then of course

1766
01:25:33,500 --> 01:25:35,500
having those jobs be automated is fine

1767
01:25:35,500 --> 01:25:37,500
but what does it mean to really be able to have

1768
01:25:37,500 --> 01:25:39,500
other better opportunities

1769
01:25:39,500 --> 01:25:41,500
so

1770
01:25:41,500 --> 01:25:43,500
if one of the fundamental like

1771
01:25:43,500 --> 01:25:45,500
axioms

1772
01:25:45,500 --> 01:25:47,500
of all of our economic theories

1773
01:25:47,500 --> 01:25:49,500
is that we need to figure out how to

1774
01:25:49,500 --> 01:25:51,500
incent a labor force to do things that nobody

1775
01:25:51,500 --> 01:25:53,500
wants to do

1776
01:25:53,500 --> 01:25:55,500
and emerging technological automation starts to

1777
01:25:55,500 --> 01:25:57,500
debase that

1778
01:25:57,500 --> 01:25:59,500
that means we have to rethink economics from scratch

1779
01:25:59,500 --> 01:26:01,500
because we don't have to do that thing anymore

1780
01:26:01,500 --> 01:26:03,500
so maybe if now the jobs don't need the people

1781
01:26:03,500 --> 01:26:05,500
that need the jobs can we start to create

1782
01:26:05,500 --> 01:26:07,500
commonwealth resources that everyone has access to

1783
01:26:07,500 --> 01:26:09,500
where people's access

1784
01:26:09,500 --> 01:26:11,500
isn't based on possession

1785
01:26:11,500 --> 01:26:13,500
that automatically limits everyone else's access

1786
01:26:13,500 --> 01:26:15,500
if you get around

1787
01:26:15,500 --> 01:26:17,500
transportation wise with a car

1788
01:26:17,500 --> 01:26:19,500
based on owning that car

1789
01:26:19,500 --> 01:26:21,500
where the vast majority of the life of the car

1790
01:26:21,500 --> 01:26:23,500
it's just sitting not being used

1791
01:26:23,500 --> 01:26:25,500
for you to have access to the car you have to have

1792
01:26:25,500 --> 01:26:27,500
possession of it which means that it's a

1793
01:26:27,500 --> 01:26:29,500
mostly underutilized asset I don't have

1794
01:26:29,500 --> 01:26:31,500
access to the thing that you possess

1795
01:26:31,500 --> 01:26:33,500
now what we see with Uber of course is a situation

1796
01:26:33,500 --> 01:26:35,500
where your access

1797
01:26:35,500 --> 01:26:37,500
is not mediated by your possession

1798
01:26:37,500 --> 01:26:39,500
so now turn that into electric

1799
01:26:39,500 --> 01:26:41,500
self-driving cars and now make the entire

1800
01:26:41,500 --> 01:26:43,500
thing on a blockchain so you disintermediate

1801
01:26:43,500 --> 01:26:45,500
even the central business make it a commonwealth

1802
01:26:45,500 --> 01:26:47,500
resource and everyone has access to

1803
01:26:47,500 --> 01:26:49,500
transportation as a commonwealth resource

1804
01:26:49,500 --> 01:26:51,500
it'll take a 20th of the number of cars

1805
01:26:51,500 --> 01:26:53,500
to meet the same level

1806
01:26:53,500 --> 01:26:55,500
of convenience during peak demand time

1807
01:26:55,500 --> 01:26:57,500
so much less environmental harm

1808
01:26:57,500 --> 01:26:59,500
it'll actually be more convenient because I don't

1809
01:26:59,500 --> 01:27:01,500
have to be engaged in driving the thing

1810
01:27:01,500 --> 01:27:03,500
and there's less traffic because of the coordination

1811
01:27:03,500 --> 01:27:05,500
and better maintenance and there isn't a desire

1812
01:27:05,500 --> 01:27:07,500
for an incentive for designed

1813
01:27:07,500 --> 01:27:09,500
an obsolescence in that system

1814
01:27:09,500 --> 01:27:11,500
you can see a situation where

1815
01:27:11,500 --> 01:27:13,500
can we make it to where

1816
01:27:13,500 --> 01:27:15,500
the wealth augmenting capacity

1817
01:27:15,500 --> 01:27:17,500
of that technologic automation goes back

1818
01:27:17,500 --> 01:27:19,500
into a commonwealth because we don't have to

1819
01:27:19,500 --> 01:27:21,500
have the same axioms of needing to incend the people

1820
01:27:21,500 --> 01:27:23,500
oh yeah but if you don't incend the people

1821
01:27:23,500 --> 01:27:25,500
they'll all be lazy welfare people

1822
01:27:25,500 --> 01:27:27,500
nonsense

1823
01:27:27,500 --> 01:27:29,500
Einstein didn't do what he did based on

1824
01:27:29,500 --> 01:27:31,500
economic incentive and

1825
01:27:31,500 --> 01:27:33,500
neither did Mozart and neither did Gandhi

1826
01:27:33,500 --> 01:27:35,500
and none of the people that we

1827
01:27:35,500 --> 01:27:37,500
are most inspired by through history

1828
01:27:37,500 --> 01:27:39,500
were doing that

1829
01:27:39,500 --> 01:27:41,500
and what kids will

1830
01:27:41,500 --> 01:27:43,500
who will spend so much

1831
01:27:43,500 --> 01:27:45,500
time doing where they ask questions

1832
01:27:45,500 --> 01:27:47,500
about why this why this why this

1833
01:27:47,500 --> 01:27:49,500
and building forts and whatever is intrinsic motive

1834
01:27:49,500 --> 01:27:51,500
it's just we don't facilitate

1835
01:27:51,500 --> 01:27:53,500
the things that they're interested in

1836
01:27:53,500 --> 01:27:55,500
we try to force them to be interested in things

1837
01:27:55,500 --> 01:27:57,500
that's what ends up breaking their interest in life

1838
01:27:57,500 --> 01:27:59,500
and then they just want a hypernormal stimuli

1839
01:27:59,500 --> 01:28:01,500
and play video games whatever

1840
01:28:01,500 --> 01:28:03,500
what if you had a system that was facilitating

1841
01:28:03,500 --> 01:28:05,500
their interest the entire time

1842
01:28:05,500 --> 01:28:07,500
now you have a situation where you can start to

1843
01:28:07,500 --> 01:28:09,500
decrease the total amount of extrinsic

1844
01:28:09,500 --> 01:28:11,500
incentive in the system as a whole

1845
01:28:11,500 --> 01:28:13,500
use the technology

1846
01:28:13,500 --> 01:28:15,500
to the automation to decrease

1847
01:28:15,500 --> 01:28:17,500
the need for extrinsic incentive

1848
01:28:17,500 --> 01:28:19,500
and make an educational system

1849
01:28:19,500 --> 01:28:21,500
and culture that's about optimizing intrinsic

1850
01:28:21,500 --> 01:28:23,500
incentive because if my needs are already

1851
01:28:23,500 --> 01:28:25,500
met getting stuff there's no

1852
01:28:25,500 --> 01:28:27,500
and everybody's needs are met through access

1853
01:28:27,500 --> 01:28:29,500
to commonwealth resources there's no real status

1854
01:28:29,500 --> 01:28:31,500
conferred that there's only status

1855
01:28:31,500 --> 01:28:33,500
conferred by what I create so now there is a

1856
01:28:33,500 --> 01:28:35,500
any status is bound to a kind of creative

1857
01:28:35,500 --> 01:28:37,500
imperative that's an example

1858
01:28:37,500 --> 01:28:39,500
we can look at blockchain

1859
01:28:39,500 --> 01:28:41,500
tech even more near term and say

1860
01:28:41,500 --> 01:28:43,500
but

1861
01:28:43,500 --> 01:28:45,500
but just to come back to this technological

1862
01:28:45,500 --> 01:28:47,500
automation thing so obviously it makes possible

1863
01:28:47,500 --> 01:28:49,500
changing economics and changing education

1864
01:28:49,500 --> 01:28:51,500
but also

1865
01:28:51,500 --> 01:28:53,500
what is the role of humans

1866
01:28:53,500 --> 01:28:55,500
in a

1867
01:28:55,500 --> 01:28:57,500
post AI robotic automation world

1868
01:28:57,500 --> 01:28:59,500
because that is coming very very soon

1869
01:28:59,500 --> 01:29:01,500
and what is the future of

1870
01:29:01,500 --> 01:29:03,500
education where you don't have to prepare

1871
01:29:03,500 --> 01:29:05,500
people to be

1872
01:29:05,500 --> 01:29:07,500
things that you can just program computers

1873
01:29:07,500 --> 01:29:09,500
to be

1874
01:29:09,500 --> 01:29:11,500
well the role of education has to be based on

1875
01:29:11,500 --> 01:29:13,500
what is the role of people in that world

1876
01:29:13,500 --> 01:29:15,500
that is such a deep redesign of civilization

1877
01:29:15,500 --> 01:29:17,500
because the tech is changing

1878
01:29:17,500 --> 01:29:19,500
the possibility set that deeply

1879
01:29:19,500 --> 01:29:21,500
so at the heart of this are kind of

1880
01:29:21,500 --> 01:29:23,500
deep existential questions of what is a meaningful

1881
01:29:23,500 --> 01:29:25,500
human life and then what is a good civilization

1882
01:29:25,500 --> 01:29:27,500
that increases the possibility

1883
01:29:27,500 --> 01:29:29,500
space of that for everybody and how do we design

1884
01:29:29,500 --> 01:29:31,500
that thing we come back to

1885
01:29:31,500 --> 01:29:33,500
blockchain and we say

1886
01:29:33,500 --> 01:29:35,500
well blockchain is an uncorruptible ledger

1887
01:29:35,500 --> 01:29:37,500
well

1888
01:29:37,500 --> 01:29:39,500
one thing that the left and right and everybody

1889
01:29:39,500 --> 01:29:41,500
agrees on is that we did corruption

1890
01:29:41,500 --> 01:29:43,500
happens and it's bad for the societies

1891
01:29:43,500 --> 01:29:45,500
at home we don't like it we just disagree

1892
01:29:45,500 --> 01:29:47,500
on who does it

1893
01:29:47,500 --> 01:29:49,500
is it possible

1894
01:29:49,500 --> 01:29:51,500
that that tech could

1895
01:29:51,500 --> 01:29:53,500
make possible decreasing

1896
01:29:53,500 --> 01:29:55,500
corruption as a whole

1897
01:29:55,500 --> 01:29:57,500
it actually decreases the possibility set for corruption

1898
01:29:57,500 --> 01:29:59,500
yeah in order to do corruption

1899
01:29:59,500 --> 01:30:01,500
I have to be able to hide that I did it

1900
01:30:01,500 --> 01:30:03,500
right I either have to

1901
01:30:03,500 --> 01:30:05,500
to break enforcement or break accounting and mostly

1902
01:30:05,500 --> 01:30:07,500
it's break accounting

1903
01:30:07,500 --> 01:30:09,500
and so what if

1904
01:30:09,500 --> 01:30:11,500
all government spending was on a blockchain

1905
01:30:11,500 --> 01:30:13,500
and doesn't have to be a blockchain

1906
01:30:13,500 --> 01:30:15,500
it has to be an uncorruptible ledger of some kind

1907
01:30:15,500 --> 01:30:17,500
all a chain is a good example

1908
01:30:17,500 --> 01:30:19,500
that is pioneering another way of doing it

1909
01:30:19,500 --> 01:30:21,500
but uncorruptible ledger of some kind

1910
01:30:21,500 --> 01:30:23,500
where you actually see

1911
01:30:23,500 --> 01:30:25,500
where all taxpayer money goes and you see how

1912
01:30:25,500 --> 01:30:27,500
it's utilized the entire thing and have independent

1913
01:30:27,500 --> 01:30:29,500
auditing agencies and the public can transparently

1914
01:30:29,500 --> 01:30:31,500
be engaged in the auditing of it

1915
01:30:31,500 --> 01:30:33,500
and if the government

1916
01:30:33,500 --> 01:30:35,500
is going to privately contract a corporation

1917
01:30:35,500 --> 01:30:37,500
the corporation agrees

1918
01:30:37,500 --> 01:30:39,500
that if they want that government money

1919
01:30:39,500 --> 01:30:41,500
the blockchain accounting has to extend

1920
01:30:41,500 --> 01:30:43,500
into the corporation so there can't be

1921
01:30:43,500 --> 01:30:45,500
very very

1922
01:30:45,500 --> 01:30:47,500
bloated corruption everybody got to see

1923
01:30:47,500 --> 01:30:49,500
that when Elon made

1924
01:30:49,500 --> 01:30:51,500
SpaceX all of a sudden he was making

1925
01:30:51,500 --> 01:30:53,500
rockets for like a hundreds to a thousands

1926
01:30:53,500 --> 01:30:55,500
of the price that Lockheed or Boeing were

1927
01:30:55,500 --> 01:30:57,500
who had just had these almost monopolistic government

1928
01:30:57,500 --> 01:30:59,500
contracts for a long time

1929
01:30:59,500 --> 01:31:01,500
well if the taxpayer money

1930
01:31:01,500 --> 01:31:03,500
is going to the government

1931
01:31:03,500 --> 01:31:05,500
is going to an external private contractor

1932
01:31:05,500 --> 01:31:07,500
who's making the things for a hundred to a thousand

1933
01:31:07,500 --> 01:31:09,500
times more than it costs

1934
01:31:09,500 --> 01:31:11,500
we get this false dichotomy sold to us that

1935
01:31:11,500 --> 01:31:13,500
either

1936
01:31:13,500 --> 01:31:15,500
we have to pay more taxes

1937
01:31:15,500 --> 01:31:17,500
to have better national security

1938
01:31:17,500 --> 01:31:19,500
or if we want to cut taxes

1939
01:31:19,500 --> 01:31:21,500
we're going to have less national security

1940
01:31:21,500 --> 01:31:23,500
what about just having less gruesome

1941
01:31:23,500 --> 01:31:25,500
bloat because you have better accounting

1942
01:31:25,500 --> 01:31:27,500
and we make the rockets for a hundredth

1943
01:31:27,500 --> 01:31:29,500
of the price and we have better national

1944
01:31:29,500 --> 01:31:31,500
security and better social services and less

1945
01:31:31,500 --> 01:31:33,500
taxes well that's

1946
01:31:33,500 --> 01:31:35,500
everyone would vote for that right

1947
01:31:35,500 --> 01:31:37,500
who wouldn't vote for that thing well that wasn't

1948
01:31:37,500 --> 01:31:39,500
possible before uncorruptible ledgers

1949
01:31:39,500 --> 01:31:41,500
also means you can have provenance

1950
01:31:41,500 --> 01:31:43,500
on supply chains to make the supply

1951
01:31:43,500 --> 01:31:45,500
chains closed loop so that you can see

1952
01:31:45,500 --> 01:31:47,500
that all the new stuff is being made from old stuff

1953
01:31:47,500 --> 01:31:49,500
and you can see where all the pollution is going and you can see

1954
01:31:49,500 --> 01:31:51,500
who did it which means you can now internalize

1955
01:31:51,500 --> 01:31:53,500
the externalities rigorously

1956
01:31:53,500 --> 01:31:55,500
and nobody can destroy those

1957
01:31:55,500 --> 01:31:57,500
emails or burn those files right

1958
01:31:57,500 --> 01:31:59,500
what if

1959
01:31:59,500 --> 01:32:01,500
the changes

1960
01:32:01,500 --> 01:32:03,500
in law and

1961
01:32:03,500 --> 01:32:05,500
the

1962
01:32:05,500 --> 01:32:07,500
decision-making processes

1963
01:32:07,500 --> 01:32:09,500
also followed a

1964
01:32:09,500 --> 01:32:11,500
blockchain process where there was a provenance

1965
01:32:11,500 --> 01:32:13,500
on the input of information well that would also

1966
01:32:13,500 --> 01:32:15,500
be a very meaningful thing to be able to

1967
01:32:15,500 --> 01:32:17,500
follow so

1968
01:32:17,500 --> 01:32:19,500
this is an example of like can we actually

1969
01:32:19,500 --> 01:32:21,500
structurally remove

1970
01:32:21,500 --> 01:32:23,500
the capacity for corruption

1971
01:32:23,500 --> 01:32:25,500
by technology that makes

1972
01:32:25,500 --> 01:32:27,500
corruption much much much harder that forces

1973
01:32:27,500 --> 01:32:29,500
types of transparency on auditability

1974
01:32:29,500 --> 01:32:31,500
what if also

1975
01:32:31,500 --> 01:32:33,500
you're able to record history

1976
01:32:33,500 --> 01:32:35,500
you're able to record the events that are occurring

1977
01:32:35,500 --> 01:32:37,500
in a blockchain that's uncruptible where you can't change history later

1978
01:32:37,500 --> 01:32:39,500
so you actually get

1979
01:32:39,500 --> 01:32:41,500
the possibility of real justice and real history

1980
01:32:41,500 --> 01:32:43,500
and multiple different simultaneous timelines

1981
01:32:43,500 --> 01:32:45,500
that are happening that's humongous

1982
01:32:45,500 --> 01:32:47,500
in terms of what it does

1983
01:32:47,500 --> 01:32:49,500
what if you can

1984
01:32:49,500 --> 01:32:51,500
have an open data platform

1985
01:32:51,500 --> 01:32:53,500
and an open science platform where

1986
01:32:53,500 --> 01:32:55,500
someone doesn't get to cherry pick which data

1987
01:32:55,500 --> 01:32:57,500
they include in their peer reviewed paper later

1988
01:32:57,500 --> 01:32:59,500
we get to see all of the data that was happening

1989
01:32:59,500 --> 01:33:01,500
we solve the oracle issues that are associated

1990
01:33:01,500 --> 01:33:03,500
and then if we find out that a particular

1991
01:33:03,500 --> 01:33:05,500
piece of science was wrong later we can see

1992
01:33:05,500 --> 01:33:07,500
downstream everything that used

1993
01:33:07,500 --> 01:33:09,500
that output as an input and automatically

1994
01:33:09,500 --> 01:33:11,500
flag what things need to change

1995
01:33:11,500 --> 01:33:13,500
that's so

1996
01:33:13,500 --> 01:33:15,500
powerful like the least

1997
01:33:15,500 --> 01:33:17,500
interesting example

1998
01:33:17,500 --> 01:33:19,500
of blockchain is currency creation

1999
01:33:19,500 --> 01:33:21,500
these are actually

2000
01:33:21,500 --> 01:33:23,500
like

2001
01:33:23,500 --> 01:33:25,500
the capacity for the right types of

2002
01:33:25,500 --> 01:33:27,500
accounting means the right type of choice making

2003
01:33:27,500 --> 01:33:29,500
right let's take AI

2004
01:33:29,500 --> 01:33:31,500
with AI we can make

2005
01:33:31,500 --> 01:33:33,500
super terrible deep fakes and destroy

2006
01:33:33,500 --> 01:33:35,500
the epistemic commons you know using that

2007
01:33:35,500 --> 01:33:37,500
and other things like that

2008
01:33:37,500 --> 01:33:39,500
but we can see

2009
01:33:39,500 --> 01:33:41,500
the way that the AI makes the deep fake by

2010
01:33:41,500 --> 01:33:43,500
being able to take enough different images of

2011
01:33:43,500 --> 01:33:45,500
the person's face and movements that it can generate

2012
01:33:45,500 --> 01:33:47,500
new ones we can see where it can generate totally

2013
01:33:47,500 --> 01:33:49,500
new faces averaging faces together somebody

2014
01:33:49,500 --> 01:33:51,500
sent me some new work that they were just doing on this

2015
01:33:51,500 --> 01:33:53,500
the other day I found very interesting they said

2016
01:33:53,500 --> 01:33:55,500
we're going to take a very similar type of tech and apply

2017
01:33:55,500 --> 01:33:57,500
it to semantic fields where

2018
01:33:57,500 --> 01:33:59,500
we can take everybody's sentiment on a topic

2019
01:33:59,500 --> 01:34:01,500
and actually generate

2020
01:34:01,500 --> 01:34:03,500
a proposition that is at the semantic

2021
01:34:03,500 --> 01:34:05,500
center or take everybody's

2022
01:34:07,500 --> 01:34:09,500
sentiment and abstract from it the

2023
01:34:09,500 --> 01:34:11,500
values that they care about and create

2024
01:34:11,500 --> 01:34:13,500
values taxonomies and say

2025
01:34:13,500 --> 01:34:15,500
we should come up with a proposition that meets

2026
01:34:15,500 --> 01:34:17,500
all these values then

2027
01:34:17,500 --> 01:34:19,500
can you have digital processes where you can't fit

2028
01:34:19,500 --> 01:34:21,500
everybody into

2029
01:34:21,500 --> 01:34:23,500
into a

2030
01:34:23,500 --> 01:34:25,500
town hall but everybody who wants

2031
01:34:25,500 --> 01:34:27,500
to can participate in a digital space

2032
01:34:27,500 --> 01:34:29,500
that rather than vote

2033
01:34:29,500 --> 01:34:31,500
yes or no on a proposition that was made by a

2034
01:34:31,500 --> 01:34:33,500
special interest group where we didn't have a say

2035
01:34:33,500 --> 01:34:35,500
in the proposition or even the values it was seeking

2036
01:34:35,500 --> 01:34:37,500
to serve so it was made in a very

2037
01:34:37,500 --> 01:34:39,500
narrow way that like we mentioned earlier

2038
01:34:39,500 --> 01:34:41,500
benefits one thing and harms something else which is

2039
01:34:41,500 --> 01:34:43,500
why

2040
01:34:43,500 --> 01:34:45,500
almost every proposition gets about half of the

2041
01:34:45,500 --> 01:34:47,500
vote and inherently polarizes the

2042
01:34:47,500 --> 01:34:49,500
population well people are so

2043
01:34:49,500 --> 01:34:51,500
dumb and so rival risk the process of voting

2044
01:34:51,500 --> 01:34:53,500
with

2045
01:34:53,500 --> 01:34:55,500
bad propositions and

2046
01:34:55,500 --> 01:34:57,500
and bad representation

2047
01:34:57,500 --> 01:34:59,500
process is inherently polarizing

2048
01:34:59,500 --> 01:35:01,500
and downgrading to people so

2049
01:35:01,500 --> 01:35:03,500
what if there's a process by which there's

2050
01:35:03,500 --> 01:35:05,500
a decision that wants to be made you start

2051
01:35:05,500 --> 01:35:07,500
by identifying what are the values everybody cares

2052
01:35:07,500 --> 01:35:09,500
about and then we say the

2053
01:35:09,500 --> 01:35:11,500
first proposition that meets all these

2054
01:35:11,500 --> 01:35:13,500
values well becomes the

2055
01:35:13,500 --> 01:35:15,500
thing that we vote on and then instead of just

2056
01:35:15,500 --> 01:35:17,500
a direct vote do we engage

2057
01:35:17,500 --> 01:35:19,500
types of qualified and liquid democracy

2058
01:35:19,500 --> 01:35:21,500
together where you have to show that you understand

2059
01:35:21,500 --> 01:35:23,500
the basics of

2060
01:35:23,500 --> 01:35:25,500
that topic to be able to vote on it

2061
01:35:25,500 --> 01:35:27,500
but the education is free and you can keep retesting

2062
01:35:27,500 --> 01:35:29,500
and the basics don't show leaning one way or the

2063
01:35:29,500 --> 01:35:31,500
other just shows you understand the stated pros and

2064
01:35:31,500 --> 01:35:33,500
cons so that massive populism

2065
01:35:33,500 --> 01:35:35,500
doesn't happen but if

2066
01:35:35,500 --> 01:35:37,500
you don't want to come to understand it you can seed

2067
01:35:37,500 --> 01:35:39,500
your vote to someone else who has passed that thing

2068
01:35:39,500 --> 01:35:41,500
these are that type of liquid

2069
01:35:41,500 --> 01:35:43,500
democracy that type of qualified

2070
01:35:43,500 --> 01:35:45,500
educated democracy where it doesn't have to

2071
01:35:45,500 --> 01:35:47,500
be educated across everything it can be per

2072
01:35:47,500 --> 01:35:49,500
issue and where you're not just voting

2073
01:35:49,500 --> 01:35:51,500
on a thing you're helping craft the propositions

2074
01:35:51,500 --> 01:35:53,500
these completely change

2075
01:35:53,500 --> 01:35:55,500
the possibility space of social technology

2076
01:35:55,500 --> 01:35:57,500
and we can go on and on in terms of

2077
01:35:57,500 --> 01:35:59,500
examples but these are ways

2078
01:35:59,500 --> 01:36:01,500
that the same type

2079
01:36:01,500 --> 01:36:03,500
of new emergent physical tech

2080
01:36:03,500 --> 01:36:05,500
that can destroy the epistemic

2081
01:36:05,500 --> 01:36:07,500
commons and create autocracies and create catastrophic

2082
01:36:07,500 --> 01:36:09,500
risks could also be used

2083
01:36:09,500 --> 01:36:11,500
to realize a much

2084
01:36:11,500 --> 01:36:13,500
more

2085
01:36:13,500 --> 01:36:15,500
pro-topic world

2086
01:36:15,500 --> 01:36:17,500
so I love so many of those

2087
01:36:17,500 --> 01:36:19,500
examples and I especially on

2088
01:36:19,500 --> 01:36:21,500
the blockchain and corruption one because

2089
01:36:21,500 --> 01:36:23,500
I think as you said something

2090
01:36:23,500 --> 01:36:25,500
the left and the right can both agree on is that

2091
01:36:25,500 --> 01:36:27,500
our systems are not really functional and there's

2092
01:36:27,500 --> 01:36:29,500
definitely corruption and defection going on

2093
01:36:29,500 --> 01:36:31,500
and just to add to your example

2094
01:36:31,500 --> 01:36:33,500
imagine if citizens could even earn money

2095
01:36:33,500 --> 01:36:35,500
by spotting inefficiencies or corruption

2096
01:36:35,500 --> 01:36:37,500
in that transparent ledger

2097
01:36:37,500 --> 01:36:39,500
so that we actually have a system that is

2098
01:36:39,500 --> 01:36:41,500
actually profiting

2099
01:36:41,500 --> 01:36:43,500
by getting more and more efficient over time

2100
01:36:43,500 --> 01:36:45,500
and actually better serving the needs of the people

2101
01:36:45,500 --> 01:36:47,500
and having less and less corruption and so there's

2102
01:36:47,500 --> 01:36:49,500
definitely more trust and faith and that's actually

2103
01:36:49,500 --> 01:36:51,500
a kind of digital society

2104
01:36:51,500 --> 01:36:53,500
that when you look at let's say the closed

2105
01:36:53,500 --> 01:36:55,500
China's digital authoritarian society

2106
01:36:55,500 --> 01:36:57,500
and you look at this open one that's actually operating

2107
01:36:57,500 --> 01:36:59,500
more for the people with more transparency

2108
01:36:59,500 --> 01:37:01,500
with more efficiencies you get more

2109
01:37:01,500 --> 01:37:03,500
SpaceX Elon Musk type cheap

2110
01:37:03,500 --> 01:37:05,500
ways of sending rockets to the moon and becoming a multi-planetary

2111
01:37:05,500 --> 01:37:07,500
civilization as opposed to

2112
01:37:07,500 --> 01:37:09,500
more bloat and more mega monopolies

2113
01:37:09,500 --> 01:37:11,500
defense contractors that are not taking us

2114
01:37:11,500 --> 01:37:13,500
to where we need to go

2115
01:37:13,500 --> 01:37:15,500
that's just an inspiring vision and I just hope

2116
01:37:15,500 --> 01:37:17,500
you share it and kind of go back because there's a lot

2117
01:37:17,500 --> 01:37:19,500
of different aspects there

2118
01:37:19,500 --> 01:37:21,500
I think the question on many people's mind right now

2119
01:37:21,500 --> 01:37:23,500
is going to be

2120
01:37:23,500 --> 01:37:25,500
how do we get from where we are

2121
01:37:25,500 --> 01:37:27,500
to the world that you are talking about

2122
01:37:27,500 --> 01:37:29,500
what are the steps that are in between

2123
01:37:29,500 --> 01:37:31,500
obviously I don't know

2124
01:37:31,500 --> 01:37:33,500
nobody knows

2125
01:37:33,500 --> 01:37:35,500
there's gonna like

2126
01:37:35,500 --> 01:37:37,500
which projects emerge

2127
01:37:37,500 --> 01:37:39,500
and

2128
01:37:39,500 --> 01:37:41,500
first and start really making success

2129
01:37:41,500 --> 01:37:43,500
that there's a lot of different

2130
01:37:43,500 --> 01:37:45,500
possible paths

2131
01:37:45,500 --> 01:37:47,500
I can say some of the things

2132
01:37:47,500 --> 01:37:49,500
that could happen and some of the things

2133
01:37:49,500 --> 01:37:51,500
that I think need to happen

2134
01:37:53,500 --> 01:37:55,500
so we take all the catastrophic

2135
01:37:55,500 --> 01:37:57,500
risks that exponential tech

2136
01:37:57,500 --> 01:37:59,500
makes possible and the dystopic attractors

2137
01:37:59,500 --> 01:38:01,500
and we say okay so we need to solve all those

2138
01:38:01,500 --> 01:38:03,500
problems but we're not doing

2139
01:38:03,500 --> 01:38:05,500
really good at solving those problems right now

2140
01:38:05,500 --> 01:38:07,500
so our problem solving processes need upgraded

2141
01:38:07,500 --> 01:38:09,500
and that means new

2142
01:38:09,500 --> 01:38:11,500
institutions

2143
01:38:11,500 --> 01:38:13,500
and when we say

2144
01:38:13,500 --> 01:38:15,500
institution we usually think of a pretty centralized

2145
01:38:15,500 --> 01:38:17,500
thing and with things like decentralized

2146
01:38:17,500 --> 01:38:19,500
governance emerging

2147
01:38:19,500 --> 01:38:21,500
the institution might be a decentralized one

2148
01:38:21,500 --> 01:38:23,500
but it's individual

2149
01:38:23,500 --> 01:38:25,500
people aren't going to solve all of that right

2150
01:38:25,500 --> 01:38:27,500
so it's new institution

2151
01:38:27,500 --> 01:38:29,500
centralized and decentralized that have the right

2152
01:38:29,500 --> 01:38:31,500
capacities to solve these types of problems

2153
01:38:31,500 --> 01:38:33,500
need to come about

2154
01:38:33,500 --> 01:38:35,500
alright well who develops those

2155
01:38:35,500 --> 01:38:37,500
institutions and who empowers them

2156
01:38:37,500 --> 01:38:39,500
and this is where

2157
01:38:39,500 --> 01:38:41,500
the democratic idea of

2158
01:38:41,500 --> 01:38:43,500
the

2159
01:38:43,500 --> 01:38:45,500
power of government coming from the

2160
01:38:45,500 --> 01:38:47,500
consent of the governed

2161
01:38:47,500 --> 01:38:49,500
is one of the key ideas to what

2162
01:38:49,500 --> 01:38:51,500
we would think of as the values of an open

2163
01:38:51,500 --> 01:38:53,500
society let's say that there's a small

2164
01:38:53,500 --> 01:38:55,500
number of people who think we understand these

2165
01:38:55,500 --> 01:38:57,500
problems we understand the solutions that must happen

2166
01:38:57,500 --> 01:38:59,500
everybody else doesn't get it so we're going to make this

2167
01:38:59,500 --> 01:39:01,500
thing happen and because we have the power we can

2168
01:39:01,500 --> 01:39:03,500
just kind of implement it by force and so

2169
01:39:03,500 --> 01:39:05,500
that becomes

2170
01:39:05,500 --> 01:39:07,500
its own dystopia right

2171
01:39:07,500 --> 01:39:09,500
and implement it by force might be well the people think

2172
01:39:09,500 --> 01:39:11,500
they need to be free so we'll implement it

2173
01:39:11,500 --> 01:39:13,500
by attention hijacking them so that they

2174
01:39:13,500 --> 01:39:15,500
participate with it or

2175
01:39:15,500 --> 01:39:17,500
don't even realize that it's happening and they just

2176
01:39:17,500 --> 01:39:19,500
keep doing whatever is next

2177
01:39:21,500 --> 01:39:23,500
the cultural element why we talk

2178
01:39:23,500 --> 01:39:25,500
about the need for a new cultural enlightenment

2179
01:39:25,500 --> 01:39:27,500
is

2180
01:39:27,500 --> 01:39:29,500
of course when we look at like the founding

2181
01:39:29,500 --> 01:39:31,500
of the US we can see all that was

2182
01:39:33,500 --> 01:39:35,500
super wrong with it right

2183
01:39:35,500 --> 01:39:37,500
just to mention like how when Churchill said

2184
01:39:37,500 --> 01:39:39,500
democracy is the worst form of government ever

2185
01:39:39,500 --> 01:39:41,500
saved for all the other forms

2186
01:39:41,500 --> 01:39:43,500
there's when when Socrates

2187
01:39:43,500 --> 01:39:45,500
talked about in

2188
01:39:45,500 --> 01:39:47,500
in the republic when Plato was

2189
01:39:47,500 --> 01:39:49,500
discussing it why

2190
01:39:49,500 --> 01:39:51,500
democracy was a dreadful idea

2191
01:39:51,500 --> 01:39:53,500
the arguments are good arguments right like

2192
01:39:53,500 --> 01:39:55,500
do you want

2193
01:39:55,500 --> 01:39:57,500
people who understand

2194
01:39:57,500 --> 01:39:59,500
seafaring to man the boat or just a general

2195
01:39:59,500 --> 01:40:01,500
population who knows nothing about it to man the boat

2196
01:40:01,500 --> 01:40:03,500
well that's not a very good idea do you want the general

2197
01:40:03,500 --> 01:40:05,500
population that knows nothing about it to build the NASA

2198
01:40:05,500 --> 01:40:07,500
rocket or do you want people that know what they're doing

2199
01:40:07,500 --> 01:40:09,500
well why would we think people who have no idea what

2200
01:40:09,500 --> 01:40:11,500
they're doing are going to be good at figuring out

2201
01:40:11,500 --> 01:40:13,500
how a civilization should be run what should

2202
01:40:13,500 --> 01:40:15,500
our nuclear first strike policy should be how should

2203
01:40:15,500 --> 01:40:17,500
we deal with the stability of the energy grid

2204
01:40:17,500 --> 01:40:19,500
against Carrington events

2205
01:40:19,500 --> 01:40:21,500
and so what does it take

2206
01:40:21,500 --> 01:40:23,500
to have a population educated enough

2207
01:40:23,500 --> 01:40:25,500
and yet then if we say okay

2208
01:40:25,500 --> 01:40:27,500
but then the other

2209
01:40:27,500 --> 01:40:29,500
problem is if we say the people are

2210
01:40:29,500 --> 01:40:31,500
too uneducated and maybe

2211
01:40:31,500 --> 01:40:33,500
too irrational and rival risk to be able

2212
01:40:33,500 --> 01:40:35,500
to hold that power so it needs to be held by

2213
01:40:35,500 --> 01:40:37,500
some how do we ensure non-corruption and

2214
01:40:37,500 --> 01:40:39,500
who is a trust

2215
01:40:39,500 --> 01:40:41,500
worthy authority to be

2216
01:40:41,500 --> 01:40:43,500
able to hold that power and not have vested

2217
01:40:43,500 --> 01:40:45,500
interest mess it up and so this is why I

2218
01:40:45,500 --> 01:40:47,500
think it was a Jefferson quote of

2219
01:40:47,500 --> 01:40:49,500
the ultimate

2220
01:40:49,500 --> 01:40:51,500
depository of the power must be the people and if

2221
01:40:51,500 --> 01:40:53,500
we think the people too uneducated not enlightened

2222
01:40:53,500 --> 01:40:55,500
to be able to hold that power

2223
01:40:55,500 --> 01:40:57,500
we must do everything we can to seek to

2224
01:40:57,500 --> 01:40:59,500
educate and lighten them not think that there

2225
01:40:59,500 --> 01:41:01,500
is any other safe depository

2226
01:41:01,500 --> 01:41:03,500
and so

2227
01:41:03,500 --> 01:41:05,500
even with that

2228
01:41:05,500 --> 01:41:07,500
we take the

2229
01:41:07,500 --> 01:41:09,500
US formation and

2230
01:41:09,500 --> 01:41:11,500
you've got some founders

2231
01:41:11,500 --> 01:41:13,500
who

2232
01:41:13,500 --> 01:41:15,500
had read most of the books of the time

2233
01:41:15,500 --> 01:41:17,500
read most of the books of philosophy knew the history

2234
01:41:17,500 --> 01:41:19,500
of the Magna Carta and the Treaty of the Forest

2235
01:41:19,500 --> 01:41:21,500
and all these kinds of things thought and talked deeply

2236
01:41:21,500 --> 01:41:23,500
spent many years were willing to die

2237
01:41:23,500 --> 01:41:25,500
fighting a revolutionary war were not

2238
01:41:25,500 --> 01:41:27,500
going along with winning at the current system

2239
01:41:27,500 --> 01:41:29,500
but really trying to do a fundamentally different thing

2240
01:41:29,500 --> 01:41:31,500
to develop a new system

2241
01:41:31,500 --> 01:41:33,500
not everybody who was participating in the US

2242
01:41:33,500 --> 01:41:35,500
was doing that thing they weren't all doing systems

2243
01:41:35,500 --> 01:41:37,500
architecture right

2244
01:41:37,500 --> 01:41:39,500
but they were all basically

2245
01:41:39,500 --> 01:41:41,500
saying

2246
01:41:41,500 --> 01:41:43,500
we agree to this kind of systems

2247
01:41:43,500 --> 01:41:45,500
architecture and we want to

2248
01:41:45,500 --> 01:41:47,500
learn how to participate with it adequately

2249
01:41:47,500 --> 01:41:49,500
will read a newspaper we will do a

2250
01:41:49,500 --> 01:41:51,500
jury duty will come to the town hall that kind of

2251
01:41:51,500 --> 01:41:53,500
thing so

2252
01:41:53,500 --> 01:41:55,500
um

2253
01:41:55,500 --> 01:41:57,500
in Taiwan's example I think their population

2254
01:41:57,500 --> 01:41:59,500
is 23 million people in their

2255
01:41:59,500 --> 01:42:01,500
online citizen engagement platform has something

2256
01:42:01,500 --> 01:42:03,500
like 5 million people engaging

2257
01:42:03,500 --> 01:42:05,500
that's pretty awesome right that's not everybody

2258
01:42:05,500 --> 01:42:07,500
and no one should be forced to be engaging

2259
01:42:09,500 --> 01:42:11,500
and one of the critical things

2260
01:42:11,500 --> 01:42:13,500
when we think deeper about is it a democracy

2261
01:42:13,500 --> 01:42:15,500
is it a republic

2262
01:42:15,500 --> 01:42:17,500
is it a epistocracy is it a

2263
01:42:17,500 --> 01:42:19,500
um we want to think about

2264
01:42:19,500 --> 01:42:21,500
the values not the previous

2265
01:42:21,500 --> 01:42:23,500
frames for them and the values exist

2266
01:42:23,500 --> 01:42:25,500
in dialectics and we need to be able to hold those

2267
01:42:25,500 --> 01:42:27,500
together of course we want individual liberty

2268
01:42:27,500 --> 01:42:29,500
but we don't want individual liberty that gets to harm

2269
01:42:29,500 --> 01:42:31,500
other people and other things so we want

2270
01:42:31,500 --> 01:42:33,500
also you know law justice collective

2271
01:42:33,500 --> 01:42:35,500
integrity how do you relate those things

2272
01:42:35,500 --> 01:42:37,500
one of the core things is the relationship between

2273
01:42:37,500 --> 01:42:39,500
rights and responsibilities so

2274
01:42:39,500 --> 01:42:41,500
there

2275
01:42:41,500 --> 01:42:43,500
if I have rights and I don't have

2276
01:42:43,500 --> 01:42:45,500
responsibilities there ends up

2277
01:42:45,500 --> 01:42:47,500
being like tyranny and entitlement

2278
01:42:47,500 --> 01:42:49,500
if I and we can see

2279
01:42:49,500 --> 01:42:51,500
that that's kind of rampant the entitlement

2280
01:42:51,500 --> 01:42:53,500
thing if I have responsibilities

2281
01:42:53,500 --> 01:42:55,500
and I don't have any attendant rights

2282
01:42:55,500 --> 01:42:57,500
at servitude neither of those

2283
01:42:57,500 --> 01:42:59,500
involve a healthy just society so

2284
01:42:59,500 --> 01:43:01,500
if I want the right

2285
01:43:01,500 --> 01:43:03,500
to drive a car the responsibility

2286
01:43:03,500 --> 01:43:05,500
to do the driver's education and actually

2287
01:43:05,500 --> 01:43:07,500
learn how to drive a car safely is important and we

2288
01:43:07,500 --> 01:43:09,500
can see that some countries have less car

2289
01:43:09,500 --> 01:43:11,500
accidents than others associated with better

2290
01:43:11,500 --> 01:43:13,500
drivers education um and so

2291
01:43:13,500 --> 01:43:15,500
increasing the responsibilities a good

2292
01:43:15,500 --> 01:43:17,500
thing we can see that some countries

2293
01:43:17,500 --> 01:43:19,500
have way less gun violence than others

2294
01:43:19,500 --> 01:43:21,500
even factoring a similar per capita

2295
01:43:21,500 --> 01:43:23,500
amount of guns based on more training

2296
01:43:23,500 --> 01:43:25,500
associated with guns and mental health

2297
01:43:25,500 --> 01:43:27,500
and things like that

2298
01:43:27,500 --> 01:43:29,500
so if I have a right to bear arms

2299
01:43:29,500 --> 01:43:31,500
do I also have a responsibility

2300
01:43:31,500 --> 01:43:33,500
to be part of a well organized militia

2301
01:43:33,500 --> 01:43:35,500
train with them and be willing to actually sacrifice

2302
01:43:35,500 --> 01:43:37,500
myself to protect the whole or

2303
01:43:37,500 --> 01:43:39,500
sign up for a thing to do that to have to be a

2304
01:43:39,500 --> 01:43:41,500
reservist of some kind those are the right

2305
01:43:41,500 --> 01:43:43,500
responsibility if I want the right to vote

2306
01:43:43,500 --> 01:43:45,500
is there a responsibility to be educated

2307
01:43:45,500 --> 01:43:47,500
about the issue

2308
01:43:47,500 --> 01:43:49,500
yes yes now

2309
01:43:49,500 --> 01:43:51,500
does that make it very unequal no

2310
01:43:51,500 --> 01:43:53,500
because the capacity

2311
01:43:53,500 --> 01:43:55,500
to get educated has to be something that the

2312
01:43:55,500 --> 01:43:57,500
society invests in making possible for

2313
01:43:57,500 --> 01:43:59,500
everyone and of course we would all be

2314
01:43:59,500 --> 01:44:01,500
silly to not

2315
01:44:01,500 --> 01:44:03,500
be dubious factoring the previous

2316
01:44:03,500 --> 01:44:05,500
history of these things but this is what we then

2317
01:44:05,500 --> 01:44:07,500
have to insist upon because do we want

2318
01:44:07,500 --> 01:44:09,500
people who really

2319
01:44:09,500 --> 01:44:11,500
don't understand the issues but think they do

2320
01:44:11,500 --> 01:44:13,500
voting now that's a dreadful system

2321
01:44:13,500 --> 01:44:15,500
but do we want people who know

2322
01:44:15,500 --> 01:44:17,500
something to have no avenue or who care

2323
01:44:17,500 --> 01:44:19,500
do we want people who know something to have

2324
01:44:19,500 --> 01:44:21,500
no avenue to input that into the system or

2325
01:44:21,500 --> 01:44:23,500
people who care to have no opportunity

2326
01:44:23,500 --> 01:44:25,500
to learn no that's also dreadful

2327
01:44:25,500 --> 01:44:27,500
so how do we make the on-ramps to learning

2328
01:44:27,500 --> 01:44:29,500
available for everyone not enforced

2329
01:44:29,500 --> 01:44:31,500
but we're actually incentivizing

2330
01:44:31,500 --> 01:44:33,500
can we use those same kind of social media

2331
01:44:33,500 --> 01:44:35,500
behavior and sending technologies to increase

2332
01:44:35,500 --> 01:44:37,500
everyone's desire for more rights

2333
01:44:37,500 --> 01:44:39,500
and attendant responsibilities

2334
01:44:39,500 --> 01:44:41,500
so that there's actually a gradient

2335
01:44:41,500 --> 01:44:43,500
of civic virtue and civic engagement

2336
01:44:43,500 --> 01:44:45,500
yeah we could totally do that

2337
01:44:47,500 --> 01:44:49,500
so this is where the cultural enlightenment

2338
01:44:49,500 --> 01:44:51,500
layer is of course not everyone is going to

2339
01:44:51,500 --> 01:44:53,500
be working on how do we develop AI

2340
01:44:53,500 --> 01:44:55,500
and blockchain for these purposes

2341
01:44:55,500 --> 01:44:57,500
but they can certainly be saying I am going

2342
01:44:57,500 --> 01:44:59,500
to make sure that my representatives are talking

2343
01:44:59,500 --> 01:45:01,500
about these issues I want all the

2344
01:45:01,500 --> 01:45:03,500
presidential candidates to be talking about these issues

2345
01:45:03,500 --> 01:45:05,500
I'm going to pay attention to and support

2346
01:45:05,500 --> 01:45:07,500
candidates who really do in earnest ways

2347
01:45:07,500 --> 01:45:09,500
I'm going to invest in

2348
01:45:09,500 --> 01:45:11,500
companies that are doing those things I'm going to

2349
01:45:11,500 --> 01:45:13,500
invest from companies that are doing the other things

2350
01:45:13,500 --> 01:45:15,500
there is a cultural

2351
01:45:15,500 --> 01:45:17,500
enlightenment that is needed

2352
01:45:17,500 --> 01:45:19,500
to be able to create the

2353
01:45:19,500 --> 01:45:21,500
demand and the support for

2354
01:45:21,500 --> 01:45:23,500
where those projects

2355
01:45:23,500 --> 01:45:25,500
that are earnestly working on and have

2356
01:45:25,500 --> 01:45:27,500
the capability start to emerge

2357
01:45:27,500 --> 01:45:29,500
so you've painted

2358
01:45:29,500 --> 01:45:31,500
a compelling vision of

2359
01:45:31,500 --> 01:45:33,500
some of the ways that a open

2360
01:45:33,500 --> 01:45:35,500
society could consciously employ

2361
01:45:35,500 --> 01:45:37,500
some of these technologies to

2362
01:45:37,500 --> 01:45:39,500
revisit and

2363
01:45:39,500 --> 01:45:41,500
re-fulfill some of the original

2364
01:45:41,500 --> 01:45:43,500
values for which they were intended

2365
01:45:43,500 --> 01:45:45,500
how much of this

2366
01:45:45,500 --> 01:45:47,500
how does this work with the

2367
01:45:47,500 --> 01:45:49,500
existing institutions that we have how much is

2368
01:45:49,500 --> 01:45:51,500
this going to rely on

2369
01:45:51,500 --> 01:45:53,500
transforming the existing

2370
01:45:53,500 --> 01:45:55,500
digital Leviathans into something new

2371
01:45:55,500 --> 01:45:57,500
how much is going to depend on blockchain projects

2372
01:45:57,500 --> 01:45:59,500
how much is going to depend on

2373
01:45:59,500 --> 01:46:01,500
existing institutions would be the Brookings

2374
01:46:01,500 --> 01:46:03,500
institution or

2375
01:46:03,500 --> 01:46:05,500
the New York Times can you speak

2376
01:46:05,500 --> 01:46:07,500
to the role of new

2377
01:46:07,500 --> 01:46:09,500
and future institutions in making this transition

2378
01:46:09,500 --> 01:46:11,500
possible

2379
01:46:13,500 --> 01:46:15,500
yeah

2380
01:46:15,500 --> 01:46:17,500
it's interesting

2381
01:46:17,500 --> 01:46:19,500
when we look at institutions that emerged

2382
01:46:19,500 --> 01:46:21,500
to try to solve some social

2383
01:46:21,500 --> 01:46:23,500
or environmental problems or nonprofits

2384
01:46:23,500 --> 01:46:25,500
in particular and some government branches

2385
01:46:25,500 --> 01:46:27,500
that are associated

2386
01:46:27,500 --> 01:46:29,500
with that there's this kind of structural

2387
01:46:29,500 --> 01:46:31,500
perverse incentive

2388
01:46:31,500 --> 01:46:33,500
that if I

2389
01:46:33,500 --> 01:46:35,500
am an organization which means

2390
01:46:35,500 --> 01:46:37,500
I'm people in an organization

2391
01:46:37,500 --> 01:46:39,500
that

2392
01:46:39,500 --> 01:46:41,500
have some that have job security

2393
01:46:41,500 --> 01:46:43,500
and some actual power and access and whatever

2394
01:46:43,500 --> 01:46:45,500
because of this position

2395
01:46:45,500 --> 01:46:47,500
and my job is to solve a problem

2396
01:46:47,500 --> 01:46:49,500
if I fully solve the problem I would obsolete my job

2397
01:46:49,500 --> 01:46:51,500
and obsolete myself so then there's this kind of

2398
01:46:51,500 --> 01:46:53,500
perverse incentive to continue managing

2399
01:46:53,500 --> 01:46:55,500
the problem continue manufacturing the narrative

2400
01:46:55,500 --> 01:46:57,500
that we're needed to manage the problem continue

2401
01:46:57,500 --> 01:46:59,500
manufacturing the narrative that the problem

2402
01:46:59,500 --> 01:47:01,500
is really hard and is hard to solve and so we got

2403
01:47:01,500 --> 01:47:03,500
to keep doing this thing

2404
01:47:03,500 --> 01:47:05,500
and so one of the fundamental

2405
01:47:05,500 --> 01:47:07,500
dispositions of systems is that

2406
01:47:07,500 --> 01:47:09,500
they want to keep existing and

2407
01:47:09,500 --> 01:47:11,500
so and yet they might

2408
01:47:11,500 --> 01:47:13,500
no longer be fit for purpose they might even be

2409
01:47:13,500 --> 01:47:15,500
antithetical to the purpose we have to be very

2410
01:47:15,500 --> 01:47:17,500
careful about this

2411
01:47:17,500 --> 01:47:19,500
with regard to

2412
01:47:19,500 --> 01:47:21,500
the new institutions we need to what degree could

2413
01:47:21,500 --> 01:47:23,500
existing institutions reform

2414
01:47:23,500 --> 01:47:25,500
themselves to what degree does it need to be new ones

2415
01:47:25,500 --> 01:47:27,500
it's kind of up to them like it's kind of

2416
01:47:27,500 --> 01:47:29,500
up to the the depth of

2417
01:47:29,500 --> 01:47:31,500
realization of the need and the

2418
01:47:31,500 --> 01:47:33,500
sincerity and then the coordination capacity of

2419
01:47:33,500 --> 01:47:35,500
people in current institutions

2420
01:47:35,500 --> 01:47:37,500
how much role they could play we can

2421
01:47:37,500 --> 01:47:39,500
see the way that going into World

2422
01:47:39,500 --> 01:47:41,500
War II coming out of the depression the U.S.

2423
01:47:41,500 --> 01:47:43,500
up-regulated its

2424
01:47:43,500 --> 01:47:45,500
coordination capacity so profoundly

2425
01:47:45,500 --> 01:47:47,500
so could we

2426
01:47:47,500 --> 01:47:49,500
have a Manhattan project like level

2427
01:47:49,500 --> 01:47:51,500
organization

2428
01:47:51,500 --> 01:47:53,500
by organization

2429
01:47:53,500 --> 01:47:55,500
I mean

2430
01:47:55,500 --> 01:47:57,500
the

2431
01:47:57,500 --> 01:47:59,500
capacity to organize not a

2432
01:47:59,500 --> 01:48:01,500
singular thing

2433
01:48:01,500 --> 01:48:03,500
that was oriented

2434
01:48:03,500 --> 01:48:05,500
to how do we

2435
01:48:05,500 --> 01:48:07,500
instantiate the next model

2436
01:48:07,500 --> 01:48:09,500
of civilization how do we instantiate the next model

2437
01:48:09,500 --> 01:48:11,500
of social systems and social technologies

2438
01:48:11,500 --> 01:48:13,500
what is the future of education what's the future of economics

2439
01:48:13,500 --> 01:48:15,500
what's the future of

2440
01:48:15,500 --> 01:48:17,500
the fourth estate of law etc

2441
01:48:17,500 --> 01:48:19,500
that are

2442
01:48:19,500 --> 01:48:21,500
that fulfill the values

2443
01:48:21,500 --> 01:48:23,500
that are meaningful and are antifragile

2444
01:48:23,500 --> 01:48:25,500
in the presence of the current technologies

2445
01:48:25,500 --> 01:48:27,500
and they can actually compete with the other applications

2446
01:48:27,500 --> 01:48:29,500
of those technologies towards

2447
01:48:29,500 --> 01:48:31,500
things that serve different values

2448
01:48:31,500 --> 01:48:33,500
and or aren't antifragile

2449
01:48:33,500 --> 01:48:35,500
I would

2450
01:48:35,500 --> 01:48:37,500
love to see the U.S.

2451
01:48:37,500 --> 01:48:39,500
make that a

2452
01:48:39,500 --> 01:48:41,500
central imperative Manhattan project level

2453
01:48:41,500 --> 01:48:43,500
to be able to do that not just how do we create a more

2454
01:48:43,500 --> 01:48:45,500
powerful military but how do we create

2455
01:48:45,500 --> 01:48:47,500
a more powerful a

2456
01:48:47,500 --> 01:48:49,500
healthier fundamentally a healthier

2457
01:48:49,500 --> 01:48:51,500
society that up regulates

2458
01:48:51,500 --> 01:48:53,500
and engages collective intelligence

2459
01:48:53,500 --> 01:48:55,500
in its own problem solving and innovation better

2460
01:48:55,500 --> 01:48:57,500
I would like to see lots

2461
01:48:57,500 --> 01:48:59,500
of countries do that I think there are countries

2462
01:48:59,500 --> 01:49:01,500
that did

2463
01:49:01,500 --> 01:49:03,500
not yet transition to democracy

2464
01:49:03,500 --> 01:49:05,500
are interested in it and completely bypass

2465
01:49:05,500 --> 01:49:07,500
the industrial era democracies

2466
01:49:07,500 --> 01:49:09,500
and go directly to better systems

2467
01:49:09,500 --> 01:49:11,500
I think networks

2468
01:49:11,500 --> 01:49:13,500
of small countries you see what Taiwan

2469
01:49:13,500 --> 01:49:15,500
is doing Estonia is trying to do some interesting

2470
01:49:15,500 --> 01:49:17,500
things I think networks of small countries could start

2471
01:49:17,500 --> 01:49:19,500
sharing best practices and sharing

2472
01:49:19,500 --> 01:49:21,500
resources so they don't all have to develop the stuff from

2473
01:49:21,500 --> 01:49:23,500
scratch which could start to lead

2474
01:49:23,500 --> 01:49:25,500
to coalitions of countries like the EU

2475
01:49:25,500 --> 01:49:27,500
saying let's do some fundamentally better

2476
01:49:27,500 --> 01:49:29,500
things I think

2477
01:49:29,500 --> 01:49:31,500
it will happen also not at the level of nation

2478
01:49:31,500 --> 01:49:33,500
states were like

2479
01:49:33,500 --> 01:49:35,500
decentralized groups blockchain

2480
01:49:35,500 --> 01:49:37,500
type groups say all right let's

2481
01:49:37,500 --> 01:49:39,500
really earnestly take on what these primary

2482
01:49:39,500 --> 01:49:41,500
problems are and work on developing

2483
01:49:41,500 --> 01:49:43,500
these solutions these capacities

2484
01:49:43,500 --> 01:49:45,500
for the tech companies

2485
01:49:45,500 --> 01:49:47,500
to do so would be very hard because

2486
01:49:47,500 --> 01:49:49,500
while

2487
01:49:49,500 --> 01:49:51,500
it could be

2488
01:49:51,500 --> 01:49:53,500
still

2489
01:49:53,500 --> 01:49:55,500
profitable

2490
01:49:55,500 --> 01:49:57,500
long term it would not be profit

2491
01:49:57,500 --> 01:49:59,500
maximizing short term relative

2492
01:49:59,500 --> 01:50:01,500
to the current thing they're doing as we said

2493
01:50:01,500 --> 01:50:03,500
winning at the current game

2494
01:50:03,500 --> 01:50:05,500
and building a new game are different things

2495
01:50:05,500 --> 01:50:07,500
and winning at a current game that self

2496
01:50:07,500 --> 01:50:09,500
terminating is a very short sighted thing

2497
01:50:09,500 --> 01:50:11,500
to want to keep doing so

2498
01:50:11,500 --> 01:50:13,500
if Facebook or Google or whatever

2499
01:50:13,500 --> 01:50:15,500
were to cut its ad model

2500
01:50:15,500 --> 01:50:17,500
it would have a hard time being able to meet

2501
01:50:17,500 --> 01:50:19,500
its fiduciary responsibility to shareholders a different

2502
01:50:19,500 --> 01:50:21,500
way but could it

2503
01:50:21,500 --> 01:50:23,500
in conjunction with

2504
01:50:23,500 --> 01:50:25,500
a

2505
01:50:25,500 --> 01:50:27,500
participatory

2506
01:50:27,500 --> 01:50:29,500
government regulatory process that wanted

2507
01:50:29,500 --> 01:50:31,500
to help change its fiduciary

2508
01:50:31,500 --> 01:50:33,500
responsibility

2509
01:50:33,500 --> 01:50:35,500
where it became more of a social utility

2510
01:50:35,500 --> 01:50:37,500
start to actually redirect its technology

2511
01:50:37,500 --> 01:50:39,500
and redirect its decision making process

2512
01:50:39,500 --> 01:50:41,500
yeah it could and that would be super interesting

2513
01:50:41,500 --> 01:50:43,500
so

2514
01:50:43,500 --> 01:50:45,500
I would like to see

2515
01:50:45,500 --> 01:50:47,500
as we mentioned earlier I'd like to see the UN

2516
01:50:47,500 --> 01:50:49,500
recognize that the

2517
01:50:49,500 --> 01:50:51,500
level of progress that it has made

2518
01:50:51,500 --> 01:50:53,500
at

2519
01:50:53,500 --> 01:50:55,500
the sustainable development goals, nuclear

2520
01:50:55,500 --> 01:50:57,500
deproliferation and other types of

2521
01:50:57,500 --> 01:50:59,500
international things like

2522
01:50:59,500 --> 01:51:01,500
economic equality

2523
01:51:01,500 --> 01:51:03,500
globally writ large

2524
01:51:03,500 --> 01:51:05,500
and

2525
01:51:05,500 --> 01:51:07,500
preventing arms races and tragedy of the commons that

2526
01:51:07,500 --> 01:51:09,500
well it hasn't done nothing

2527
01:51:09,500 --> 01:51:11,500
what it's doing is not converging

2528
01:51:11,500 --> 01:51:13,500
it's not adequate, it's not converging on

2529
01:51:13,500 --> 01:51:15,500
eventually solving the problem set

2530
01:51:15,500 --> 01:51:17,500
just more of that approach, it needs a different approach

2531
01:51:17,500 --> 01:51:19,500
and so to say

2532
01:51:19,500 --> 01:51:21,500
clearly we don't know how to facilitate

2533
01:51:21,500 --> 01:51:23,500
coordination of global problems well enough

2534
01:51:23,500 --> 01:51:25,500
so let's have the superseding focus

2535
01:51:25,500 --> 01:51:27,500
be innovation towards better methods of global

2536
01:51:27,500 --> 01:51:29,500
coordination, that becomes our new number one

2537
01:51:29,500 --> 01:51:31,500
goal

2538
01:51:31,500 --> 01:51:33,500
because we know we only get all the other goals if we get that

2539
01:51:33,500 --> 01:51:35,500
and you can see that during World War II

2540
01:51:35,500 --> 01:51:37,500
when we had to crack the enigma machine

2541
01:51:37,500 --> 01:51:39,500
and figure out

2542
01:51:39,500 --> 01:51:41,500
computation and whatever

2543
01:51:41,500 --> 01:51:43,500
we got touring, we got von Neumann

2544
01:51:43,500 --> 01:51:45,500
one of the smartest people from

2545
01:51:45,500 --> 01:51:47,500
countries all around the world

2546
01:51:47,500 --> 01:51:49,500
engaged in solving those problems

2547
01:51:49,500 --> 01:51:51,500
I would like to see the US, the UN

2548
01:51:51,500 --> 01:51:53,500
I would like to see other countries and I would like to see private sector

2549
01:51:53,500 --> 01:51:55,500
taking seriously the actual

2550
01:51:55,500 --> 01:51:57,500
problemscape we have

2551
01:51:57,500 --> 01:51:59,500
and innovating not for

2552
01:51:59,500 --> 01:52:01,500
just short-term advantage or narrow

2553
01:52:01,500 --> 01:52:03,500
in-group advantage

2554
01:52:03,500 --> 01:52:05,500
but for long-term

2555
01:52:07,500 --> 01:52:09,500
advantage of the whole, how do we

2556
01:52:09,500 --> 01:52:11,500
since we have global effect

2557
01:52:11,500 --> 01:52:13,500
global coordination adequate to what is needed

2558
01:52:13,500 --> 01:52:15,500
to me that has to become

2559
01:52:15,500 --> 01:52:17,500
the central zeitgeist

2560
01:52:17,500 --> 01:52:19,500
and whatever groups figure out how to do it

2561
01:52:19,500 --> 01:52:21,500
effectively will be the groups

2562
01:52:21,500 --> 01:52:23,500
that can direct the future

2563
01:52:23,500 --> 01:52:25,500
and I know that this is the work

2564
01:52:25,500 --> 01:52:27,500
that you are working towards with the conciliance project

2565
01:52:27,500 --> 01:52:29,500
do you want to talk

2566
01:52:29,500 --> 01:52:31,500
just about how you are working towards that

2567
01:52:31,500 --> 01:52:33,500
with your work and how we are collaborating?

2568
01:52:33,500 --> 01:52:35,500
yeah, I mean we are

2569
01:52:35,500 --> 01:52:37,500
we are at the very very beginning

2570
01:52:37,500 --> 01:52:39,500
the conciliance project

2571
01:52:39,500 --> 01:52:41,500
has a site up that is not even

2572
01:52:41,500 --> 01:52:43,500
a beta yet

2573
01:52:43,500 --> 01:52:45,500
because we

2574
01:52:45,500 --> 01:52:47,500
in just starting wanted to

2575
01:52:49,500 --> 01:52:51,500
work on building stuff

2576
01:52:51,500 --> 01:52:53,500
in association with thinking

2577
01:52:53,500 --> 01:52:55,500
but this talk is very

2578
01:52:55,500 --> 01:52:57,500
central, this conversation you and I are having is very central to the aims of the conciliance project

2579
01:52:57,500 --> 01:52:59,500
which is

2580
01:52:59,500 --> 01:53:01,500
we are wanting to

2581
01:53:01,500 --> 01:53:03,500
inspire, inform and help

2582
01:53:03,500 --> 01:53:05,500
direct a innovation zeitgeist

2583
01:53:05,500 --> 01:53:07,500
where

2584
01:53:07,500 --> 01:53:09,500
the many different problems of the world

2585
01:53:09,500 --> 01:53:11,500
start to get seen in terms of

2586
01:53:11,500 --> 01:53:13,500
having interconnectivity

2587
01:53:13,500 --> 01:53:15,500
and underlying drivers

2588
01:53:15,500 --> 01:53:17,500
and the forcing function

2589
01:53:17,500 --> 01:53:19,500
of the power of exponential tech

2590
01:53:19,500 --> 01:53:21,500
is taken seriously that says in order to become

2591
01:53:21,500 --> 01:53:23,500
good stewards of that requires

2592
01:53:23,500 --> 01:53:25,500
evolutions of both our social

2593
01:53:25,500 --> 01:53:27,500
systems and our culture

2594
01:53:27,500 --> 01:53:29,500
the wisdom to be able to guide

2595
01:53:29,500 --> 01:53:31,500
that power, a recoupling

2596
01:53:31,500 --> 01:53:33,500
of wisdom and power in that

2597
01:53:33,500 --> 01:53:35,500
adequate to what is needed

2598
01:53:35,500 --> 01:53:37,500
so how do we innovate in culture

2599
01:53:37,500 --> 01:53:39,500
the development of people

2600
01:53:39,500 --> 01:53:41,500
and how do we innovate in the social systems

2601
01:53:41,500 --> 01:53:43,500
the advancement of our coordination

2602
01:53:43,500 --> 01:53:45,500
both employing the exponential tech

2603
01:53:45,500 --> 01:53:47,500
and being able to rightly guide it

2604
01:53:47,500 --> 01:53:49,500
and so

2605
01:53:49,500 --> 01:53:51,500
we have

2606
01:53:51,500 --> 01:53:53,500
a really great team of people that are

2607
01:53:53,500 --> 01:53:55,500
doing research

2608
01:53:55,500 --> 01:53:57,500
and writing basically the types

2609
01:53:57,500 --> 01:53:59,500
of things we are talking about here in more depth explaining

2610
01:53:59,500 --> 01:54:01,500
what is the role of

2611
01:54:01,500 --> 01:54:03,500
the various social systems like what is the role of education

2612
01:54:03,500 --> 01:54:05,500
to any society help understand

2613
01:54:05,500 --> 01:54:07,500
fundamentally what that is

2614
01:54:07,500 --> 01:54:09,500
understand why there is

2615
01:54:09,500 --> 01:54:11,500
a particularly higher educational

2616
01:54:11,500 --> 01:54:13,500
threshold for open societies where people

2617
01:54:13,500 --> 01:54:15,500
need to participate not just in the

2618
01:54:15,500 --> 01:54:17,500
market but in governance

2619
01:54:17,500 --> 01:54:19,500
understand how that has been disrupted

2620
01:54:19,500 --> 01:54:21,500
by the emerging tech and will be disrupted further

2621
01:54:21,500 --> 01:54:23,500
by things like technological automation

2622
01:54:23,500 --> 01:54:25,500
and then envision what is the future

2623
01:54:25,500 --> 01:54:27,500
of education adequate to an open

2624
01:54:27,500 --> 01:54:29,500
society in a world that has the

2625
01:54:29,500 --> 01:54:31,500
technology that's emerging

2626
01:54:31,500 --> 01:54:33,500
and we don't necessarily know what the answer is

2627
01:54:33,500 --> 01:54:35,500
but we know examples and we know criteria

2628
01:54:35,500 --> 01:54:37,500
so then it's like innovate in this area

2629
01:54:37,500 --> 01:54:39,500
and make sure you factor these criteria

2630
01:54:39,500 --> 01:54:41,500
and the same thing with the fourth estate the same thing with law

2631
01:54:41,500 --> 01:54:43,500
the same thing with economics

2632
01:54:43,500 --> 01:54:45,500
and so the goal is not how do we take

2633
01:54:45,500 --> 01:54:47,500
some small group of people to build the future

2634
01:54:47,500 --> 01:54:49,500
it's how do we help get

2635
01:54:49,500 --> 01:54:51,500
what the criteria of a viable future

2636
01:54:51,500 --> 01:54:53,500
must be and if people disagree awesome

2637
01:54:53,500 --> 01:54:55,500
publicly disagree and have the conversation now

2638
01:54:55,500 --> 01:54:57,500
but if we get to put out those design constraints

2639
01:54:57,500 --> 01:54:59,500
someone says no we think it's other ones

2640
01:54:59,500 --> 01:55:01,500
the center of culture starts to be thinking about

2641
01:55:01,500 --> 01:55:03,500
the most pressing issues

2642
01:55:03,500 --> 01:55:05,500
in fundamental ways and how to think about them

2643
01:55:05,500 --> 01:55:07,500
appropriately and how to approach them appropriately

2644
01:55:07,500 --> 01:55:09,500
so fundamentally our goal

2645
01:55:09,500 --> 01:55:11,500
is

2646
01:55:11,500 --> 01:55:13,500
supporting an increased

2647
01:55:13,500 --> 01:55:15,500
cultural understanding of the nature of the

2648
01:55:15,500 --> 01:55:17,500
problems that we face a clearer understanding

2649
01:55:17,500 --> 01:55:19,500
rather than just there's lots of problems and it's overwhelming

2650
01:55:19,500 --> 01:55:21,500
and it's a bummer and so

2651
01:55:21,500 --> 01:55:23,500
either some very narrow

2652
01:55:23,500 --> 01:55:25,500
action on some very narrow part

2653
01:55:25,500 --> 01:55:27,500
of it makes sense just most of activism

2654
01:55:27,500 --> 01:55:29,500
or just nihilism

2655
01:55:29,500 --> 01:55:31,500
we want to be able to say actually

2656
01:55:31,500 --> 01:55:33,500
because there are underlying drivers

2657
01:55:33,500 --> 01:55:35,500
there is actually a possibility

2658
01:55:35,500 --> 01:55:37,500
to resolve these things

2659
01:55:37,500 --> 01:55:39,500
it does require the fullness of our capacity

2660
01:55:39,500 --> 01:55:41,500
applied to it

2661
01:55:41,500 --> 01:55:43,500
and with the fullness of our capacity so it's not a

2662
01:55:43,500 --> 01:55:45,500
given but with the fullness of our capacity applied to it

2663
01:55:45,500 --> 01:55:47,500
there is actually a path forward

2664
01:55:47,500 --> 01:55:49,500
and

2665
01:55:49,500 --> 01:55:51,500
so we're writing these papers

2666
01:55:51,500 --> 01:55:53,500
that basically would be kind of like

2667
01:55:53,500 --> 01:55:55,500
a meta-curriculum

2668
01:55:55,500 --> 01:55:57,500
for people who want to be engaged in designing the future

2669
01:55:57,500 --> 01:55:59,500
and

2670
01:55:59,500 --> 01:56:01,500
some of them have to do with current

2671
01:56:01,500 --> 01:56:03,500
public culture and how to be able to

2672
01:56:03,500 --> 01:56:05,500
change patterns of public culture that lead to

2673
01:56:05,500 --> 01:56:07,500
better conversation, better sense

2674
01:56:07,500 --> 01:56:09,500
making, better meaning making and choice making

2675
01:56:09,500 --> 01:56:11,500
so that there's an on-ramp into higher quality

2676
01:56:11,500 --> 01:56:13,500
conversations meaning higher quality process of

2677
01:56:13,500 --> 01:56:15,500
conversation and then some of them are things

2678
01:56:15,500 --> 01:56:17,500
like what are the design criteria

2679
01:56:17,500 --> 01:56:19,500
of the future social systems and how could

2680
01:56:19,500 --> 01:56:21,500
we build those things then

2681
01:56:21,500 --> 01:56:23,500
not everybody will read those

2682
01:56:23,500 --> 01:56:25,500
some people who

2683
01:56:25,500 --> 01:56:27,500
have the ability to help start building them

2684
01:56:27,500 --> 01:56:29,500
will but

2685
01:56:29,500 --> 01:56:31,500
we hope that other people will

2686
01:56:31,500 --> 01:56:33,500
take that and translate it on podcasts

2687
01:56:33,500 --> 01:56:35,500
and into animations and in

2688
01:56:35,500 --> 01:56:37,500
whatever other forms of media

2689
01:56:37,500 --> 01:56:39,500
so that those topics start

2690
01:56:39,500 --> 01:56:41,500
to become increasingly

2691
01:56:41,500 --> 01:56:43,500
present in people's awareness

2692
01:56:43,500 --> 01:56:45,500
then of course

2693
01:56:45,500 --> 01:56:47,500
the next part is what groups start

2694
01:56:47,500 --> 01:56:49,500
emerging wanting to address those

2695
01:56:49,500 --> 01:56:51,500
and what can we do to help facilitate

2696
01:56:51,500 --> 01:56:53,500
good solutions in those groups

2697
01:56:57,500 --> 01:56:59,500
and this is where you and I

2698
01:56:59,500 --> 01:57:01,500
I've learned a lot from you about

2699
01:57:01,500 --> 01:57:03,500
the

2700
01:57:03,500 --> 01:57:05,500
social media issues in particular and how

2701
01:57:05,500 --> 01:57:07,500
central they are to the breakdown of sense making

2702
01:57:07,500 --> 01:57:09,500
because obviously without good shared sense making

2703
01:57:09,500 --> 01:57:11,500
there is no possibility for emergent order

2704
01:57:11,500 --> 01:57:13,500
you either just get chaos or you have to

2705
01:57:13,500 --> 01:57:15,500
have imposed order if you want emergent

2706
01:57:15,500 --> 01:57:17,500
order that means emergent good choice making

2707
01:57:17,500 --> 01:57:19,500
that means emergent good sense making

2708
01:57:19,500 --> 01:57:21,500
and so

2709
01:57:21,500 --> 01:57:23,500
we've learned a lot and discussed these things for a long time

2710
01:57:23,500 --> 01:57:25,500
and obviously also not just you and I

2711
01:57:25,500 --> 01:57:27,500
there's a whole network of people that we're connected to

2712
01:57:27,500 --> 01:57:29,500
that have been thinking deeply about these things

2713
01:57:29,500 --> 01:57:31,500
that we continue to

2714
01:57:31,500 --> 01:57:33,500
try to think about what

2715
01:57:33,500 --> 01:57:35,500
adequate solutions could look like

2716
01:57:35,500 --> 01:57:37,500
and

2717
01:57:37,500 --> 01:57:39,500
I think what

2718
01:57:39,500 --> 01:57:41,500
CHT did with the social dilemma

2719
01:57:41,500 --> 01:57:43,500
took

2720
01:57:43,500 --> 01:57:45,500
one really critical part of this metacrisis

2721
01:57:45,500 --> 01:57:47,500
into popular attention

2722
01:57:47,500 --> 01:57:49,500
in a more powerful way than I have seen done

2723
01:57:49,500 --> 01:57:51,500
otherwise because as big a deal

2724
01:57:51,500 --> 01:57:53,500
is getting climate change and public attention is

2725
01:57:53,500 --> 01:57:55,500
it's not clear that

2726
01:57:55,500 --> 01:57:57,500
climate change is something that is making

2727
01:57:57,500 --> 01:57:59,500
that is driving

2728
01:57:59,500 --> 01:58:01,500
the underlying basis of all the problems

2729
01:58:01,500 --> 01:58:03,500
but a breakdown in sense making and a control

2730
01:58:03,500 --> 01:58:05,500
of patterns of human behavior that kind of downgrade

2731
01:58:05,500 --> 01:58:07,500
people like oh wow that really does make all these

2732
01:58:07,500 --> 01:58:09,500
other things worse

2733
01:58:09,500 --> 01:58:11,500
so I see that as a

2734
01:58:11,500 --> 01:58:13,500
as a very powerful and personal

2735
01:58:13,500 --> 01:58:15,500
on-ramp for those who are interested

2736
01:58:15,500 --> 01:58:17,500
to be able to come into

2737
01:58:17,500 --> 01:58:19,500
this deeper conversation

2738
01:58:19,500 --> 01:58:21,500
and some

2739
01:58:21,500 --> 01:58:23,500
of them it'll simply help them

2740
01:58:23,500 --> 01:58:25,500
be like okay now I know

2741
01:58:25,500 --> 01:58:27,500
what I was intuitively feeling somebody's put it

2742
01:58:27,500 --> 01:58:29,500
into words and I at least feel more oriented

2743
01:58:29,500 --> 01:58:31,500
and that's the extent because they don't necessarily

2744
01:58:31,500 --> 01:58:33,500
have the ability to build new blockchain systems

2745
01:58:33,500 --> 01:58:35,500
or whatever it is and they should be

2746
01:58:35,500 --> 01:58:37,500
doing the nursing or education or whatever really

2747
01:58:37,500 --> 01:58:39,500
other important social value they're doing

2748
01:58:39,500 --> 01:58:41,500
some people

2749
01:58:41,500 --> 01:58:43,500
will be able to say this actually really resonates

2750
01:58:43,500 --> 01:58:45,500
I can translate this to other audiences

2751
01:58:47,500 --> 01:58:49,500
and get more people engaged and some people say

2752
01:58:49,500 --> 01:58:51,500
I can actually start innovating and working with this stuff

2753
01:58:51,500 --> 01:58:53,500
and all of those are good

2754
01:58:55,500 --> 01:58:57,500
yeah I

2755
01:58:57,500 --> 01:58:59,500
I agree and I think

2756
01:58:59,500 --> 01:59:01,500
what we've essentially been

2757
01:59:01,500 --> 01:59:03,500
outlining here and you sort of hit it at the end

2758
01:59:05,500 --> 01:59:07,500
is going back to the Charles Kettering

2759
01:59:07,500 --> 01:59:09,500
quote which I learned from you

2760
01:59:09,500 --> 01:59:11,500
and I've learned so many things from you over the years

2761
01:59:11,500 --> 01:59:13,500
which is

2762
01:59:13,500 --> 01:59:15,500
that a problem not fully

2763
01:59:15,500 --> 01:59:17,500
understood is unsolvable

2764
01:59:17,500 --> 01:59:19,500
and a problem that is fully understood is half

2765
01:59:19,500 --> 01:59:21,500
solved and I just want to maybe

2766
01:59:21,500 --> 01:59:23,500
leave our listeners with that which is

2767
01:59:23,500 --> 01:59:25,500
I think people can look at the

2768
01:59:25,500 --> 01:59:27,500
long litany of problems and

2769
01:59:27,500 --> 01:59:29,500
feel overwhelmed or get to despair

2770
01:59:29,500 --> 01:59:31,500
in a hurry I think is your phrase for it

2771
01:59:31,500 --> 01:59:33,500
and I think that

2772
01:59:33,500 --> 01:59:35,500
when you understand the core generator functions

2773
01:59:35,500 --> 01:59:37,500
for what is driving

2774
01:59:37,500 --> 01:59:39,500
so many of these problems to happen simultaneously

2775
01:59:39,500 --> 01:59:41,500
there's a different

2776
01:59:41,500 --> 01:59:43,500
and more empowering relationship to that

2777
01:59:43,500 --> 01:59:45,500
and you've actually offered a vision

2778
01:59:45,500 --> 01:59:47,500
for how technology can be consciously employed

2779
01:59:47,500 --> 01:59:49,500
these new technologies can be consciously employed

2780
01:59:49,500 --> 01:59:51,500
in ways that should feel inspiring and exciting

2781
01:59:51,500 --> 01:59:53,500
I mean I want that transparent blockchain

2782
01:59:53,500 --> 01:59:55,500
on a budget for every country in the world

2783
01:59:55,500 --> 01:59:57,500
and we can see examples like Estonia and Taiwan

2784
01:59:57,500 --> 01:59:59,500
moving in this direction already

2785
01:59:59,500 --> 02:00:01,500
and we can see Taiwan building some of the technologies

2786
02:00:01,500 --> 02:00:03,500
you mentioned to identify

2787
02:00:03,500 --> 02:00:05,500
propositions of shared values between citizens

2788
02:00:05,500 --> 02:00:07,500
who want to vote collectively on something that

2789
02:00:07,500 --> 02:00:09,500
obviously would have driven up more polarization

2790
02:00:09,500 --> 02:00:11,500
so we're seeing this thing emerging

2791
02:00:11,500 --> 02:00:13,500
and I think what we need is to

2792
02:00:13,500 --> 02:00:15,500
sort of have this be seen as

2793
02:00:15,500 --> 02:00:17,500
a necessary upgrade to

2794
02:00:17,500 --> 02:00:19,500
let me do that again

2795
02:00:19,500 --> 02:00:21,500
I think we need to see this as

2796
02:00:21,500 --> 02:00:23,500
not just an upgrade but

2797
02:00:23,500 --> 02:00:25,500
the kind of cultural enlightenment that you speak of

2798
02:00:25,500 --> 02:00:27,500
that so many different actors are in a sense

2799
02:00:27,500 --> 02:00:29,500
already working on you know we used to

2800
02:00:29,500 --> 02:00:31,500
have this phrase that everyone is on the same team

2801
02:00:31,500 --> 02:00:33,500
they just don't know it yet

2802
02:00:33,500 --> 02:00:35,500
and once you understand the

2803
02:00:35,500 --> 02:00:37,500
degree to which we are in trouble

2804
02:00:37,500 --> 02:00:39,500
if we do not get our heads around

2805
02:00:39,500 --> 02:00:41,500
this and identify the kind of

2806
02:00:41,500 --> 02:00:43,500
core generator functions that we need to be addressing

2807
02:00:43,500 --> 02:00:45,500
once we all see that

2808
02:00:45,500 --> 02:00:47,500
I'll just speak to my own

2809
02:00:47,500 --> 02:00:49,500
experience when I first encountered your work

2810
02:00:49,500 --> 02:00:51,500
and I encountered

2811
02:00:51,500 --> 02:00:53,500
the kind of core drivers

2812
02:00:53,500 --> 02:00:55,500
that drive so much of the danger

2813
02:00:55,500 --> 02:00:57,500
that we are headed towards

2814
02:00:57,500 --> 02:00:59,500
I

2815
02:00:59,500 --> 02:01:01,500
immediately

2816
02:01:01,500 --> 02:01:03,500
I was kind of already in this direction already

2817
02:01:03,500 --> 02:01:05,500
reoriented my whole life to say

2818
02:01:05,500 --> 02:01:07,500
how do we be in service if this is not happening

2819
02:01:07,500 --> 02:01:09,500
and of creating a better world

2820
02:01:09,500 --> 02:01:11,500
that actually meets and addresses these problems

2821
02:01:11,500 --> 02:01:13,500
and I know so many other people

2822
02:01:13,500 --> 02:01:15,500
whose work

2823
02:01:15,500 --> 02:01:17,500
and whose lives and whose daily missions

2824
02:01:17,500 --> 02:01:19,500
and purpose have been redirected

2825
02:01:19,500 --> 02:01:21,500
by I think hearing some of the core

2826
02:01:21,500 --> 02:01:23,500
frames that you offer

2827
02:01:23,500 --> 02:01:25,500
and who I hope

2828
02:01:25,500 --> 02:01:27,500
and who are many of whom are already working on

2829
02:01:27,500 --> 02:01:29,500
active projects to deal with this and those who are not

2830
02:01:29,500 --> 02:01:31,500
are supporting in other ways

2831
02:01:31,500 --> 02:01:33,500
and I hope that our audience takes this as

2832
02:01:33,500 --> 02:01:35,500
an inspiration

2833
02:01:35,500 --> 02:01:37,500
for how can we in the face of

2834
02:01:37,500 --> 02:01:39,500
stark and difficult realities

2835
02:01:39,500 --> 02:01:41,500
as part of this process

2836
02:01:41,500 --> 02:01:43,500
gain the kind of cultural

2837
02:01:43,500 --> 02:01:45,500
strength to face these things head on

2838
02:01:45,500 --> 02:01:47,500
and to orient our lives accordingly

2839
02:01:47,500 --> 02:01:49,500
because I have

2840
02:01:49,500 --> 02:01:51,500
while bearing periods of time

2841
02:01:51,500 --> 02:01:53,500
hit probably low grade despair

2842
02:01:53,500 --> 02:01:55,500
myself

2843
02:01:55,500 --> 02:01:57,500
I actually feel more inspired than ever

2844
02:01:57,500 --> 02:01:59,500
the amount of things and the number of people

2845
02:01:59,500 --> 02:02:01,500
who face these challenges

2846
02:02:01,500 --> 02:02:03,500
and I'll just say that

2847
02:02:03,500 --> 02:02:05,500
I think when you face these challenges alone

2848
02:02:05,500 --> 02:02:07,500
and you feel like you're the only one seeing them

2849
02:02:07,500 --> 02:02:09,500
or you have a weird feeling in your stomach

2850
02:02:09,500 --> 02:02:11,500
it can feel debilitating

2851
02:02:11,500 --> 02:02:13,500
and when you realize the number of people

2852
02:02:13,500 --> 02:02:15,500
who are also putting their heads up to say

2853
02:02:15,500 --> 02:02:17,500
how can we change this

2854
02:02:17,500 --> 02:02:19,500
that's what feels hopeful

2855
02:02:19,500 --> 02:02:21,500
and that's where I derive my optimism

2856
02:02:21,500 --> 02:02:23,500
so Daniel thank you so much for coming on

2857
02:02:23,500 --> 02:02:25,500
it's an honor to have you

2858
02:02:25,500 --> 02:02:27,500
your work has touched the lives

2859
02:02:27,500 --> 02:02:29,500
who may not always say so publicly

2860
02:02:29,500 --> 02:02:31,500
but I know that you

2861
02:02:31,500 --> 02:02:33,500
had also a huge hand in

2862
02:02:33,500 --> 02:02:35,500
inspiring some of the themes that emerged

2863
02:02:35,500 --> 02:02:37,500
in the social dilemma which has impacted so many people

2864
02:02:37,500 --> 02:02:39,500
as well so thank you so much

2865
02:02:39,500 --> 02:02:41,500
really wonderful that we get to have this conversation

2866
02:02:41,500 --> 02:02:43,500
thanks just on

2867
02:02:43,500 --> 02:02:45,500
absolutely

