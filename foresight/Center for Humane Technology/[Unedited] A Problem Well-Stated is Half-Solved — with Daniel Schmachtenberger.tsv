start	end	text
0	5000	Hi, everyone. It's Tristan, and this is your Undivided Attention.
5000	9000	Up next, we have our unedited conversation with Daniel Schmacktenberger.
9000	13000	And because it's unedited, it's longer and not corrected for fact-checking purposes,
13000	17000	but you can find our shorter, edited version wherever you found this one.
17000	21000	Listen to both versions, and then come to our podcast club with Daniel and me,
21000	25000	and hopefully you, on July 9th. Details are in the show notes.
25000	27000	And with that, here we go.
31000	33000	Welcome to your Undivided Attention.
33000	38000	Today, I am so honored and happy to have my friend, Daniel Schmacktenberger,
38000	42000	as our guest, who works on the topics of existential risk
42000	46000	and what are the underlying drivers of all of the major problems,
46000	50000	or many of the major problems, that are really facing us today as a civilization,
50000	54000	be it climate change, breakdown of truth, social media, our information systems.
54000	58000	Those of you who've been following your Undivided Attention will hear this
58000	60000	as a very different kind of episode.
60000	65000	We almost think of it as a meta-episode about the underlying drivers
65000	70000	of many of the topics that we have covered on your Undivided Attention thus far.
70000	73000	So if you think about the topics that we've covered,
73000	77000	whether you've seen the social dilemma or you followed our interviews previously
77000	83000	on topics like attention span shortening or addiction or information overwhelm and distraction,
83000	87000	the fall of trust in society, more polarization, breakdown of truth,
87000	90000	our inability to solve problems like climate change,
90000	95000	well, this is really about an interconnected set of problems
95000	100000	and the kind of core generator functions that are leading to all of these things to happen at once.
100000	104000	And so I really encourage you to listen to this all the way through,
104000	108000	and I think that we're going to get into some very deep and important knowledge
108000	111000	that will hopefully be orienting for all of us.
112000	115000	One of my favorite quotes is by Charles Kettering,
115000	120000	who said that a problem not fully understood is unsolvable
120000	123000	and a problem that is fully understood is half-solved.
123000	128000	And what I hope we talk about with Daniel is what about the framework that we are using
128000	133000	to address or try to meet the various problems that we have has been inadequate
133000	136000	and what is the problem-solving framework that we're going to need
136000	139000	to deal with the existential crises that face us.
139000	142000	So Daniel, welcome to Your Undivided Detention.
142000	144000	Thank you, Tristan.
144000	148000	I've been looking forward to us dialoguing about these things publicly for a while.
148000	150000	Well, you and me both.
150000	153000	And for those who don't know, Daniel and I have been friends for a very long time,
153000	158000	and his work has been highly influential to me and many people in my circles.
158000	163000	So Daniel, maybe we should just start with what is the metacrisis
163000	167000	and why are these problems seemingly not getting solved,
167000	171000	whether it's the SDGs, climate change, or anything that we really care about right now?
173000	178000	I think a lot of people have the general sense that there is an increasing number
178000	186000	of possibly catastrophic issues and that as new categories of tech,
186000	189000	tech that allows major cyber attacks on infrastructure,
189000	192000	tech that allows weaponized drone attacks on infrastructure,
192000	197000	biotechnologies, artificial intelligence and moving towards AGI,
197000	201000	that there are new catastrophic risks with all of those categories of tech
201000	208000	and that those tech are creating larger jumps in power faster than any types of jumps of tech,
208000	212000	including the development of the nuclear bomb in the past by many orders of magnitude.
212000	220000	So there's a general sense that whether we're talking about future pandemic related issues
220000	225000	or whether we're talking about climate change or climate change as a forcing function
225000	230000	for human migration that then causes resource wars and political instability
230000	235000	or the fragility of the highly interconnected globalized world
235000	238000	where a problem in one part of the world can create supply chain issues
238000	240000	that create problems all around the world,
240000	244000	there's a sense that there's an increasing number of catastrophic risks
244000	247000	and that they're increasing faster than we are solving them.
247000	250000	And that when you mention like with the UN,
250000	256000	while progress has been made in certain defined areas of the sustainable development goals
256000	259000	and progress was made back when they were called the Millennium Development Goals,
259000	263000	we're very far from anything like a comprehensive solution to any of them.
263000	268000	We're not even on track for something that is converging towards a comprehensive solution.
268000	275000	And if we look at kind of the core initial mandate of the United Nations
275000	279000	in terms of thinking about how to recognizing after World War II
279000	283000	that nations take government alone wouldn't prevent World War
283000	285000	and now that World War was no longer viable
285000	289000	because the amount of technology we had made it a war that no one could win,
289000	292000	we still haven't succeeded at nuclear disarmament.
292000	297000	We did some very limited nuclear disarmament success while doing nuclear arms races at the same time
297000	301000	and we went from two countries with nukes to more countries with better nukes.
301000	303000	And that's simultaneous to that.
303000	306000	Every new type of tech that has emerged has created an arms race.
306000	308000	We haven't been able to prevent any of those.
308000	312000	And the major tragedy of the commons issues like climate change and overfishing
312000	317000	and dead zones in the oceans and microplastics in the oceans and biodiversity loss,
317000	319000	we haven't been able to solve those either.
319000	325000	And so rather than just think about this as like an overwhelming number of totally separate issues,
325000	334000	the question of why are the patterns of human behavior
334000	336000	as we increase our total technological capacity,
336000	340000	why are they increasing catastrophic risk and why are we not solving them well?
340000	343000	Are there underlying patterns that we could think of as,
343000	347000	as you mentioned, generator functions of the catastrophic risk,
347000	349000	generator functions of our inability to solve them,
349000	352000	that if we were to identify those and work at that level,
352000	355000	we could solve all of the expressions or symptoms.
355000	358000	And if we don't work at that level, we might not be able to solve any of them.
358000	361000	And again, people have been thinking about this for a long time,
361000	363000	kind of notice these issues.
363000	368000	They notice that you try to solve a,
368000	370000	like the first one I noticed when I was a kid
370000	374000	was trying to solve an elephant poaching issue in one particular region of Africa
374000	380000	that didn't address the poverty of the people that had no mechanism other than black market on poaching,
380000	382000	didn't address people's mindset towards animals,
382000	385000	didn't address a macroeconomy that created poverty at scale.
385000	389000	So when the laws were put in place and the fences were put in place
389000	391000	to protect those elephants in that area better,
391000	394000	the poachers moved to poaching other animals,
394000	398000	particularly in that situation rhinos and gorillas
398000	400000	that were both more endangered than the elephants had been.
400000	404000	So you moved a problem from one area to another and actually a more sensitive area.
404000	407000	And we see this with, well, can we solve hunger
407000	411000	by bringing commercial agriculture to parts of the world that don't have it
411000	414000	so that the people don't either not have food or we have to ship them food,
414000	418000	but if it's commercial agriculture based on the kind of unsustainable,
418000	421000	environmentally unsustainable agricultural processes
421000	424000	that lead to huge amounts of nitrogen runoff going into river deltas
424000	426000	that are causing dead zones in the ocean
426000	430000	that can actually collapse the biosphere's capacity to support life faster
430000	433000	than we're solving for a short-term issue that's important
433000	435000	and driving even worse long-term issues.
435000	441000	We see that many of the reasons people who oppose climate change solutions
441000	444000	in the West oppose them is because,
444000	448000	not because they have even really deeply engaged in the underlying science
448000	451000	and say the climate change isn't real,
451000	453000	that will oftentimes be what's said,
453000	456000	but because the solution itself seems like it'll cause problems
456000	459000	to other areas that they're paying attention to that seem even more critical to them.
459000	463000	So if the solution involves some kind of carbon tax
463000	468000	or something that would decrease GDP for the countries that agree to it
468000	471000	and some other countries don't agree to it,
471000	473000	and let's say in this particular case,
473000	476000	the model that many people have is western countries agree to it,
476000	479000	their GDP growth decreases, China doesn't agree to it,
479000	483000	and there's already a very, very close neck-in-neck fight
483000	486000	for who controls power in the 21st century.
486000	489000	Are we seeding the world to Chinese control
489000	492000	that many people would think it has less civil liberties
492000	495000	and is more authoritarian in its nature?
495000	501000	Or some people's answer to climate change is what we just have to use less energy,
501000	504000	but when you understand that energy correlates directly to GDP
504000	507000	and when GDP goes down, it affects poverty,
507000	509000	people in extreme poverty first and worst,
509000	513000	and wars increase because people who have desire to get more
513000	515000	end up going zero sum on each other,
515000	518000	and only when it's very positive sum does that not happen.
518000	520000	You see all these intricate theory of trade-off,
520000	523000	so we can't see that the problem is climate change.
523000	527000	Everybody knows the problem of climate change seems like a big thing,
527000	532000	but you've got to look at climate change plus the macroeconomic issues
532000	534000	that would affect the poorest people
534000	536000	and that would increase the chance of war
536000	541000	and the geopolitical dynamics between the West and China,
541000	544000	and the enforcement dynamics of international agreement.
544000	548000	When you start to recognize that the problem is that suite of things together,
548000	552000	in a way it seems, well, that's too hard, we can't even begin to focus on it.
552000	554000	I would say that that's actually easier
554000	557000	because trying to solve climate change on its own is actually impossible,
557000	561000	because if you're trying to solve something
561000	564000	that is going to externalize harm to some other thing,
564000	568000	maybe you solve that thing,
568000	570000	but you find out that you're in a worse position,
570000	573000	so I would say that it's impossible to actually improve the world that way,
573000	577000	or half the world that is paying attention to that other thing disagrees with you
577000	580000	so vehemently that all the energy goes into infighting
580000	582000	and whatever some part of the world is trying to organize to do,
582000	585000	the other part of the world is doing everything they can to resist from happening,
585000	588000	then all the creative energy just burns up as heat
588000	590000	and we don't actually accomplish anything.
590000	595000	So I would say that the way we're trying to solve the problems is actually mostly impossible.
595000	598000	It either solves it in a very narrow way
598000	601000	while externalizing harm and causing worse problems,
601000	605000	or it makes it impossible to solve it all because it drives polarization.
605000	608000	And so going to the level at which the problems interconnect
608000	611000	where that which everybody cares about is being factored
611000	613000	and where you're not externalizing other problems
613000	616000	while it seems more complex is actually possible.
616000	618000	Impossible is easier than impossible.
618000	623000	And so it's not just that there's a lot of issues, right?
623000	625000	There are a lot of issues,
625000	629000	and just that the issues are both more consequential at greater scope
629000	631000	and moving faster than previous issues
631000	634000	because of the nature of exponentiating technology.
634000	635000	That's part of it.
635000	637000	It's not just that the problems are all interconnected.
637000	641000	It's also that they do have underlying drivers that have to be addressed,
641000	644000	otherwise a symptomatic only approach doesn't work.
644000	648000	The first underlying driver that when people look at it they generally see
648000	655000	is they see things like structural perverse incentive built into macroeconomics,
655000	660000	that the elephant dead is worth more than the elephant alive is,
660000	662000	and so is the rhino, and so is the...
662000	667000	And so how do you have a situation where that's the nature of incentive,
667000	670000	where you're incentivizing an activity and then trying to bind it,
670000	672000	or keep it from happening?
672000	674000	And the same would be true with overfishing,
674000	677000	as long as live fish are worth nothing and dead fish are worth more.
677000	682000	There's something fundamentally perverse about the nature of the economic incentive.
682000	689000	And the same is true that when we have war and there's more military manufacturing, GDP goes up.
689000	693000	And when there's more addiction and people are buying the supply of their addiction, GDP goes up.
693000	696000	And when there are more sick people paying for health care, cost GDP goes up.
696000	700000	So it's obviously a perverse kind of metric.
700000	706000	So anytime someone can fiscally advantage themselves or a corporation can
706000	713000	in a way that either directly causes harm or indirectly externalizes harm,
713000	715000	we have to fundamentally solve that.
715000	719000	If there's something like 70 trillion dollars a day of activity happening
719000	722000	that is a decentralized system of incentive,
722000	726000	that is incenting people to do things that are directly or indirectly causing harm,
726000	732000	there's really nothing we can do with some billions of dollars of nonprofit or state
732000	734000	or whatever money that is going to solve that thing.
734000	738000	So we have to say, well, what changes at the level of macroeconomics need to happen
738000	743000	where the incentive of individuals and the incentive of corporations and the incentive of nations
743000	746000	is more well aligned with the well-being and the incentive of others.
746000	750000	And so we're less fundamentally rivalrous in the nature of our incentive.
750000	757000	So we can see that underneath heaps of the problems, structures of macroeconomic incentive are there.
757000	760000	That's kind of maybe the first one that most people see.
760000	764000	We can go deeper to seeing that even as an expression,
764000	769000	because whether it's a economic incentive for a corporation or whether it's a power incentive,
769000	772000	a political power incentive or a political party or for a country,
772000	777000	they're both instantiations of rivalrous-type dynamics that end up driving arms races,
777000	781000	because if you win at a rivalrous dynamic, the other side reverse-engineers your tech,
781000	785000	figures out how to make better versions, comes back, which creates an exponentiation in warfare
785000	791000	and eventually exponential warfare becomes self-terminating on a finite planet.
791000	794000	Exponential externalities also become self-terminating.
794000	799000	So if we want to say, what are the underlying generator functions of catastrophic risk?
799000	804000	First, maybe just to make clear, the catastrophic risk landscape.
804000	809000	Is this all right if we do a brief aside on that?
809000	810000	Yeah, let's do it.
810000	814000	I think what we should do, let's do that, and then let's recap just what these structures are.
814000	819000	People are tracking each of these components, because you've already mentioned a few different things.
819000	826000	The first thing is just many listeners might hear what you're sharing as an overwhelming set of problems,
826000	828000	and I think it's just to recap.
828000	833000	It's important people understand that it's overwhelming if you're not using a problem-solving framework
833000	836000	that allows you to see the interconnected nature of those problems,
836000	839000	because if you solve them with the limited tools we have now,
839000	843000	let's just solve the social media problem by pulling one lever and changing one business model of one company,
843000	849000	or banning TikTok, but then you get 20 other TikToks that come and sit in its place with the same perverse incentive of addiction,
849000	853000	the same rival risk dynamic competing for human attention.
853000	855000	We're going to end up perpetuating those problems.
855000	858000	And so just to sort of maybe recap some of that for listeners,
858000	861000	and I think maybe let you continue with the other generator function.
861000	864000	Let's just make sure that people really get those frameworks.
864000	867000	I think it's really important.
867000	873000	Yeah, I mean, in the case that you in Center for Humane Technology have brought so much attention to,
873000	880000	with regard to the attention harvesting and directing economy,
880000	890000	it's fair to say that it probably was not Facebook or Google's goal to create the type of effects that they had.
890000	892000	Those were unintended externalities.
892000	893000	They were second order effects.
893000	895000	But they were trying to solve problems, right?
895000	899000	Like, let's solve the problem if we're Google of organizing the world's information and making better search.
899000	901000	That seems like a pretty good thing to do.
901000	904000	And let's solve the problem of making it freely available to everybody.
904000	906000	That seems like a pretty good thing to do.
906000	909000	And with the AdModel, we can make it freely available to everyone.
909000	914000	And let's recognize that only if we get a lot of data will our machine learning get better.
914000	917000	And so we need to actually get everybody on this thing.
917000	919000	So we definitely have to make it free.
919000	924000	And then we get this kind of recursive process.
924000	932000	Well, then the nature of the AdModel, doing time on site optimization and stuff I'm not going to get into because you've addressed it so well,
932000	939000	ends up appealing to people's existing biases rather than correcting their bias,
939000	942000	appealing to their tribal in-group identities rather than correcting them,
942000	945000	and appealing to limbic hijacks rather than helping people transcend them.
945000	954000	And as a result, you end up actually breaking the social solidarity and epistemic capacity necessary for democracy.
954000	957000	So it's like, oh, let's solve the search problem.
957000	958000	That seems like a nice thing.
958000	963000	The side effect is we're going to destroy democracy and open societies in the process and all those other things.
963000	968000	Like, those are examples of solving a problem in a way that is externalizing harm,
968000	970000	causing other problems that are oftentimes worse.
970000	975000	And so let's just focus on the opportunity.
975000	981000	And just to say, typically, this will get accounted for as, oh, this is just an unintended consequence.
981000	983000	But there's some other generator functions I think we should outline.
983000	989000	I mean, if YouTube and Google didn't personalize search results and what video to show you next,
989000	996000	and the other guy did on TikTok starts personalizing, they're caught in a race to the bottom of whoever personalizes more for the best limbic hijack.
996000	1000000	And so just to sort of connect some of those things together for listeners.
1000000	1004000	So you mentioned race to the bottom, and obviously CHT has discussed this before,
1004000	1011000	and this is a key piece of the game theoretic challenge and global coordination.
1011000	1017000	And the two primary ways it expresses itself is arms races and tragedy of the commons.
1017000	1027000	And the tragedy of the commons scenario is if we don't overfish that area of virgin ocean,
1027000	1034000	but we can't control that someone else doesn't, because how do we do enforcement if they're also a nuclear country?
1034000	1039000	That's a tricky thing, right? How do you do enforcement on nuclear countries, equipped countries?
1039000	1043000	So us not doing it doesn't mean that the fish don't all get taken.
1043000	1048000	It just means that they grow their populations and their GDP faster, which they will use rivalrously.
1048000	1053000	So we might as well do it. In fact, we might as well race to do it faster than they do.
1053000	1055000	Those are the tragedy of the commons type issues.
1055000	1061000	The arms race version is if we can't ensure that they don't build AI weapons or they don't build surveillance tech
1061000	1067000	and they get increased near-term power from doing so, we just have to race to get there before them.
1067000	1069000	That's the arms race type thing.
1069000	1074000	It just happens to be that while that makes sense for each agent on their own in the short term,
1074000	1079000	it creates global dynamics for the whole and the long term that self-terminate,
1079000	1084000	because you can't run exponential externality on a finite planet.
1084000	1089000	That's the tragedy of the commons one, and you can't run exponential arms races and exponential conflict on a finite planet.
1089000	1095000	So the thing that has always made sense, which is just keep winning at the arms races,
1095000	1102000	has had a world where we've had lots of wars increasing in their scale and lots of environmental damage.
1102000	1104000	We started desertification thousands of years ago.
1104000	1110000	It's just has been a long, slow, exponential curve that really started to pick up with the industrial revolution
1110000	1116000	and is now really verticalizing with the digital revolution and the cumulative harm of that kind of thing becomes impossible now.
1116000	1125000	So basically, with the environmental destruction, with the wars and with the class subjugation things that civilization has had in the past,
1125000	1132000	pretty much anyone would say we have not been the best stewards of power, and technology is increasing our power.
1132000	1137000	Exponential tech means tech that makes better versions of itself, so you get an exponent on the curve.
1137000	1140000	We're now in a process where that's a very, very rapid.
1140000	1144000	Computation gives the ability to design better systems of computation.
1144000	1153000	Computation and AI applied to biological big data and protein folding gives the ability to do that on biotech and on and on, right?
1153000	1162000	So we could say the central question of our time is if we've been poor stewards of power for a long time and that's always caused problems,
1162000	1166000	but the problems now become existential, they become catastrophic, we can't keep doing that.
1166000	1175000	How do we become adequately good stewards of exponential power in time, right?
1175000	1183000	How do we develop the good decision-making processes, the wisdom necessary to be able to be stewards of that much power?
1183000	1187000	I think that's a fair way to talk about the central thing.
1187000	1195000	If it's okay, the thread we were about to get to, I think, is a good one, which was the history of catastrophic risk coming up to now,
1195000	1205000	is that before World War II, catastrophic risk was actually a real part of people's experience.
1205000	1214000	It was just always local, but an individual kingdom might face existential risk in a war where they would lose.
1214000	1222000	And so the people faced those kinds of reality, and in fact, one thing that we can see when you read books like The Collapse of Complex Societies by Joseph Tainter
1222000	1234000	and any study of history is that all the great civilizations don't still exist, which means that one of the first things we can say about civilizations is that they die.
1234000	1236000	They have a finite lifespan on them.
1236000	1241000	One of the interesting things we can find is that they usually die from self-induced causes.
1241000	1249000	They either over-consume the resources and then stop being able to meet the needs of the people through unrenewable environmental dynamics, and that's old.
1249000	1261000	Or they have increasing border conflicts that lead to enmity, that has more arms race activity coming back at them,
1261000	1271000	or they have increasing institutional decay of their internal coordination processes that leads to inability to operate quickly in those types of things.
1271000	1282000	So we can say that it's the fundamentally most all civilizations collapse in a way that is based on generally self-terminating dynamics.
1282000	1292000	And we see that even when they were overtaken by armies, oftentimes they were armies that were smaller than ones they had defended against successfully at earlier peaks in their adaptive capacity.
1292000	1297000	Okay, so catastrophic risk has been a real thing. It's just been local.
1297000	1308000	And it wasn't until World War II that we had enough technological power that catastrophic risk became a global possibility for the first time ever.
1308000	1315500	And this is a really important thing to get because the world before World War II and the world after was different and kind so fundamentally.
1315500	1324000	And this is why when you study history, so much of what you're studying is history of warfare, of neighboring kingdoms and neighboring empires fighting.
1324000	1333500	And because the wars were fundamentally winnable, at least for some, right? They weren't winnable for all the people who died, but at least for some.
1333500	1340500	And with World War II and the development of the bomb became the beginning of wars that were no longer winnable.
1340500	1353500	And that if we employed our full tech and continued the arms race even beyond the existing tech, it's a war where when lose becomes omni-lose-lose at that particular level of power.
1353500	1361500	And so that created the need to do something that humanity had never done, which was that the major superpowers didn't war.
1361500	1366500	The whole history of the world, the history of the thing we call civilization, they always did.
1366500	1374500	And so we made an entire world system, a globalized world system that was with the aim of preventing World War III.
1374500	1383500	So we could have non-kinetic wars, and we did, right? Increasingly you can see from World War II to now a movement to unconventional warfare,
1383500	1389500	narrative and information warfare, economic, diplomatic warfare, those types of things, resource warfare.
1389500	1392500	And you could, if you were going to have a physical kinetic war, it had to be a proxy war.
1392500	1399500	But to have a proxy war, that also required narrative warfare to be able to create a justification for it.
1399500	1409500	But also to be able to prevent the war, so the post-World War II Bretton Woods Mutually Assured Destruction United Nations World
1409500	1415500	was a solution to be able to steward that level of tech without destroying ourselves.
1415500	1422500	And it really was a reorganization of the world. It was a whole new advent of social technologies or social systems,
1422500	1426500	just like the U.S. was new social technologies or social systems coming out of the Industrial Revolution.
1426500	1430500	The Industrial Revolution ended up giving rise to kind of nation-state democracies.
1430500	1436500	The nuclear revolution in this way kind of gave rise to this I.G.O. intergovernmental world.
1436500	1441500	And it was predicated on a few things. Mutually Assured Destruction was critical.
1441500	1449500	Globalization and economic trade was critical that we, if the computer that we're talking on and the phone that we talk on
1449500	1454500	is made over six continents and no countries can make them on our own, we don't want to blow them up and ruin their infrastructure
1454500	1460500	because we depend upon it. So let's create radical economic interdependence so we have more economic incentive to cooperate.
1460500	1469500	Makes sense. And let's grow the materials economy so fast through this globalization
1469500	1474500	that the world gets to be very positive GDP and gets to be very positive sum
1474500	1477500	so that everybody can have more without having to take each other's stuff.
1477500	1481500	That was kind of like the basis of that whole world system.
1481500	1485500	And we can see that we've had wars, but they've been proxy wars and cold wars.
1485500	1489500	They haven't been major superpower wars and they've been unconventional ones.
1489500	1496500	But we haven't had a kinetic World War III. We have had increase of prosperity of certain kinds.
1496500	1503500	75 years give or take. Now we're at a point where that radically positive sum economy
1503500	1509500	that required an exponential growth of the economy, which means of the materials economy,
1509500	1515500	and it's a linear materials economy that unrenewably takes resources from the earth faster than they can reproduce themselves
1515500	1520500	and turns them into waste faster than they can process themselves, has led to the planetary boundaries issue
1520500	1528500	where it's not just climate change or overfishing or dead zones in the ocean or microplastics or species extinction
1528500	1531500	or peak phosphorus. It's a hundred things, right?
1531500	1538500	There's all these planetary boundaries so we can't keep doing exponential linear materials economy.
1538500	1543500	That thing has come to an end because now that drives its own set of catastrophic risks.
1543500	1548500	We see that the radical interconnection of the world was good in terms of will not bomb each other
1548500	1555500	but it also created very high fragility because what it meant is a failure anywhere could cascade to failures everywhere
1555500	1563500	because of that much dependence. So we can see with COVID we had what was a local issue to an area of China
1563500	1568500	but because of how interconnected the world is with travel it became a global issue at the pandemic level
1568500	1576500	and it also became an issue where to shut down the transmission of the virus we shut down travel
1576500	1581500	which also meant shut down supply chains which meant so many things, right?
1581500	1589500	And very fundamental things that weren't obvious to people at first like that countries agriculture depends upon the shipment of pesticides that they don't have stored
1589500	1594500	and so we got these swarms of locusts because of not having the pesticides which damaged the food supply
1594500	1603500	and shipments of fertilizer and shipments of seed so we end up seeing a drive of food insecurity of extreme poverty
1603500	1607500	at a scale of death threat that is larger than the COVID death threat was.
1607500	1612500	As a second order effect of our problem we were trying to solve the problem of don't spread COVID
1612500	1617500	and the solution had these massive second third order effects that are still playing out, right?
1617500	1624500	And that was a relatively benign pandemic a relatively benign catastrophe compared to a lot of scenarios we can model out
1624500	1630500	so we can say okay well we like the benefit of interconnectivity so we're not invested in bombing each other
1630500	1633500	but we need more anti-fragility in the system.
1633500	1640500	And then the mutually assured destruction thing doesn't work anymore because we don't have two countries with one catastrophe weapon
1640500	1644500	that's really really hard to make and easy to monitor because there's not that many places that have uranium
1644500	1647500	it's hard to enrich it you can monitor by satellites.
1647500	1653500	We have lots of countries with nukes but we also have lots of new catastrophe weapons that are not hard to make
1653500	1657500	that are not easy to monitor that don't even take nation states to make them.
1657500	1664500	So if you have many many actors of different kinds with many different types of catastrophe weapons
1664500	1666500	how do you do mutually assured destruction?
1666500	1668500	You can't do it the same way.
1668500	1676500	And so what we find is that the set of solutions post World War II that kept us from blowing ourselves up with our new power
1676500	1684500	lasted for a while but those set of solutions have ended and they have now created their own set of new problems.
1684500	1690500	So there's kind of the catastrophic risk world before World War II
1690500	1694500	the catastrophic risk world from World War II till now and then the new thing.
1694500	1699500	So the new thing says we have to have solutions that deal with the planetary boundary issues
1699500	1704500	that deal with global fragility issues and that deal with the exponential tech issues
1704500	1708500	both in terms of the way exponential tech can be intentionally used to cause harm
1708500	1711500	i.e. exponential tech empowered warfare and unintentionally
1711500	1717500	i.e. exponential tech empowered externalities and even just totally unanticipated types of mistakes
1717500	1722500	the Facebook Google type problem multiplied by AGI and things like that.
1722500	1729500	And so when we talk about what the catastrophic risk landscape is like that's the landscape
1729500	1735500	the metacrisis is how do we solve all of that and recognizing that our problem solving mechanisms
1735500	1741500	haven't even been able to solve the problems we've had for the last many years let alone prevent these things
1741500	1747500	and so the central orienting question it's like the UNS-17 Sustainable Development Goals
1747500	1753500	there's really one that must supersede them all which is develop the capacity for global coordination
1753500	1758500	that can solve global problems. If you get that one you get all the other ones
1758500	1766500	if you don't get that one you don't get any of the other ones and so we can talk about how do we do that
1766500	1772500	but that becomes the central imperative for the world at this time.
1772500	1780500	So you're saying a whole bunch of things and one thing that comes to mind here
1780500	1784500	if I'm just reading back some of the things you've shared
1784500	1788500	the development of the let's call it one of the first exponential technologies which is the nuclear bomb
1788500	1795500	led to a new social system which was sort of the post Bretton Woods world of trying to stabilize
1795500	1799500	that one exponential technology in the world in a way that would not be catastrophic
1799500	1802500	and even there we weren't able to sort of make it all work
1802500	1807500	and I think people should have maybe a list of some of the other exponential technologies
1807500	1812500	because I want to make sure that phrase is defined for listeners and there's a lot of different ways
1812500	1817500	that we've now not just created more exponential technologies but more decentralized exponential technologies
1817500	1823500	and I think people should see Facebook and Google as exponential attention mapping
1823500	1828500	or information driving technologies that are shaping the global information flows
1828500	1833500	or the wiring diagram of the sort of global societal brain at scales that are exponential
1833500	1839500	it's sort of a nuclear scale rewiring of the human civilization
1839500	1842500	we couldn't do that with newspapers we couldn't do that with a printing press
1842500	1844500	not at the scale speed et cetera that we have now
1844500	1847500	so do you want to give maybe some more examples of exponential technologies
1847500	1851500	because I think that's going to lead to we're going to need a new kinds of social systems
1851500	1857500	to manage this different landscape of not just one exponential nuclear bomb but a landscape
1857500	1864500	indulge me as I tell a story first that leads into it because it'll be a relevant framework
1864500	1869500	obviously the bomb was central to World War II and the world system that came afterwards
1869500	1874500	and what motivated our activity getting into it but it was not the only tech
1874500	1880500	it was one new technology that was part of a suite of new technologies that could all be developed
1880500	1883500	because of kind of the level science had gotten to
1883500	1890500	and basically like physics and chemistry had gotten to the point that we could work on a nuclear bomb
1890500	1893500	we could start to work on computation
1893500	1901500	we could get things like the V2 rocket and rockets and a whole host of applied chemistry
1901500	1906500	and one way of thinking about what World War II was
1906500	1911500	one way of thinking about it but it's useful frame and I think it's a fair frame
1911500	1916500	is that there were a few competing social ideologies at the time
1916500	1924500	primarily kind of German fascism fascism socialism whatever you want to call it
1924500	1930500	Soviet communism and Western liberalism something like that
1930500	1937500	and that this new suite of technologies whoever kind of developed it and was able to implement it at scale first would win
1937500	1941500	that social ideology would win because it's just so much more powerful
1941500	1944500	if you have nukes and they have guns you're going to win right
1944500	1948500	and Germans were actually ahead of both the US and the Soviets
1948500	1953500	because of some things that they did to invest in education and tech development
1953500	1959500	but that led both the Soviets and the US to really working to catch up as fast as they can
1959500	1964500	and when the US finally figured it out which we were actually a little bit slow to right
1964500	1968500	Einstein actually wrote a letter the Einstein solar letter that went to the US government
1968500	1972500	saying now the science really does say that this thing could happen and the Germans could get it
1972500	1975500	and you should focus on it and at first they didn't take them up on it
1975500	1980500	it wasn't until the private sector actually nonprofit supported advanced it further
1980500	1985500	that then the Manhattan Project was engaged in but then it was engaged in when they recognized the seriousness
1985500	1993500	that there was an actual eminent existential risk to the nation and the whole Western ideology and whatever
1993500	1997500	then it was an unlimited budget right it was a let's find all the smartest people in the world
1997500	2001500	and let's bring them here and let's organize however we need to to make this thing happen
2001500	2006500	and let's do it for all of the new areas of tech we're going to get the enigma machine and crack the enigma code
2006500	2010500	we're going to get a v2 rocket we're going to figure out how to reverse engineer that in advanced rocketry
2010500	2014500	we're going to do everything needed to make a nuclear bomb and then more advanced ones
2014500	2020500	it was the biggest jump in technology ever in the history of the world in record history as we know it
2020500	2025500	and it wasn't actually done by the market right it was done by the state that's a very important thing
2025500	2029500	this idea that markets innovate and states don't innovate is just historically not true here
2029500	2035500	this was state funds and state controlled operation in the same way that the Apollo project coming out of it was
2035500	2044500	and a technological jump of that kind hasn't happened since
2044500	2051500	so it's an important thing to understand but we can say though this is not a totally fair thing to say
2051500	2056500	we can say that the US came out dominant in that technological race
2056500	2062500	the US and the USSR both had a lot of capacity so that was the Cold War and then finally the US came out
2062500	2066500	and so the post-World War II system was a US led system
2066500	2071500	the UN was in the US the Bretton Wood system was pegged to the US dollar
2071500	2078500	what I would say is that so it wasn't one type of tech
2078500	2083500	it was the recognition that science had got to a place where there's going to be a whole suite of new tech
2083500	2089500	and the new tech meant more power and whoever had the power would determine the next phase of the world
2089500	2094500	and if we didn't like the social ideologies that were going to be guiding it
2094500	2099500	of course we can also think of it as just who wanted to win at the game of power
2099500	2105500	but from the philosophical argument if we didn't like the social ideologies then we have another social ideology
2105500	2106500	get it
2106500	2111500	what I would say is that there is an emerging suite of technologies now
2111500	2120500	that is much more powerful in the total level of jump
2120500	2123500	technological jump than the World War II suite was
2123500	2125500	in fact orders of magnitude more
2125500	2132500	and only those who are developing and employing exponential tech
2132500	2134500	will have much of a say in the direction of the future
2134500	2138500	because just from a real politic point of view that's where the power is
2138500	2142500	and if you don't have the power you won't be able to oppose it
2142500	2145500	and so what do we mean by exponential tech?
2145500	2147500	there's a couple different ways of thinking about it
2147500	2150500	just exponentially more powerful is a very simplistic way
2150500	2153500	and in that definition nuclear is exponential tech
2153500	2160500	but what we typically mean with exponential tech is tech that makes it possible to make better versions of itself
2160500	2163500	so that there is like a compounding interest kind of curve
2163500	2167500	the tech makes it easier to make a better version which makes it easier to make a better version
2167500	2172500	and so we see that starting with computation really in a fundamental way
2172500	2177500	because computation allows us to advance models of computation
2177500	2179500	how do we make better computational substrates
2179500	2182500	how do we get more transistors in a chip
2182500	2187500	how do we make better arrangements of chip so we get GPUs and those types of things
2187500	2194500	and so in this new suite of technology the center of it is computation
2194500	2199500	the very very center of that is AI
2199500	2205500	is kind of self-learning computation on large fields of data
2205500	2214500	the other kind of software advances like advances in various meaningful advances in cryptography
2214500	2222500	and big data and the ability to get data from sensors and you know sensor processing image recognition
2222500	2225500	that is a part of that central suite
2225500	2232500	and the application of that to the directing of attention and the directing of behavior by directing attention
2232500	2235500	which you focused on very centrally
2235500	2239500	then the next phase is the application of the tech
2239500	2244500	the application of computation to increasing computational substrate
2244500	2249500	so this is now the software advancing the hardware that can advance the total level of software
2249500	2253500	so that's not just continuously better chips
2253500	2257500	it's also quantum computing, photo computing, DNA computing, those other types of things
2257500	2261500	and the other types of hardware that need to be part of that thing
2261500	2264500	i.e. sensor tech in particular
2264500	2270500	so that you can keep getting more data going into that system that can do big data machine learning on it
2270500	2275500	then it's the application of that computation in AI specifically to physical tech
2275500	2281500	so to nanotech, material sciences, biotech and even things like modeling how to do better nuclear
2281500	2285500	and you know robotics and automation
2285500	2290500	and so when you start thinking about better computational substrates running better software
2290500	2293500	with more total data going in with better sensors in better robots
2293500	2297500	you start getting the sense of what that whole suite of things looks like
2298500	2306500	so that's the suite of things that I would say is what we would kind of call exponential tech
2306500	2313500	and the reason why the term exponential is important is we don't think exponentially well
2313500	2318500	our intuitions are bad for it because we think about how much progress was made over the last five years
2318500	2320500	and we imagine there will be a similar amount over the next five years
2320500	2323500	and that's not the way exponential curves work, right?
2323500	2328500	and so it's very hard for us, our intuition was calibrated on the past
2328500	2335500	and it's going to be miscalibrated for forecasting the total rate of change and the magnitude of change
2338500	2344500	so to link this for one much more narrow aspect for our listeners who are familiar with social media
2344500	2350500	and social dilemma and you're talking about sort of self-compounding systems that improve recursively like that
2351500	2358500	if I'm TikTok or if I'm Facebook and I use data to figure out what's the thing to show you
2358500	2362500	and that's going to keep you here for long since going to bypass your prefrontal cortex
2362500	2365500	and go straight to your limbic system, your lizard brain
2365500	2368500	well the better it gets at doing that and succeeding at that
2368500	2370500	the more data it has to make a better prediction the next time
2370500	2372500	but then a new user comes along who it's never seen before
2372500	2376500	but hey they're clicking on exactly the same pattern of anorexia videos
2376500	2379500	that we've seen these other 2 million users have that turn out to be teenage girls
2379500	2383500	and it just happens to know that this other set of videos that are more anorexia videos
2383500	2385500	are also going to work really really well
2385500	2388500	so there's sort of a self-compounding loop that's learning not just from one person
2388500	2392500	and getting a better version of hijacking your nervous system
2392500	2394500	but learning across individuals
2394500	2397500	and so now you get a new person coming in from developing countries
2397500	2402500	never used TikTok before and they're just barely walking in for the front door the very first time
2402500	2405500	it's sort of like when Coca-Cola goes to Southeast Asia for the first time
2405500	2410500	and you get diabetes 10 years later because you refined all the techniques of marketing so effectively
2410500	2413500	but now happening at scales that are automated with computation
2413500	2416500	so what you're talking about is the impact of computation
2416500	2418500	and learning on top of learning data on top of data
2418500	2421500	and then cross-referencing look-alike models and all of this kind of thing
2421500	2424500	you could apply to the domain at least that social dilemma
2424500	2428500	watchers and people who are familiar with our work might be able to tie into
2428500	2434500	Yeah, the more people you have in the system and the more data per person that you're able to harvest
2434500	2438500	the more stuff you have for the machine learning to figure out patterns on
2438500	2442500	which also means that the machine learning can provide things that the users want more
2442500	2446500	even if it's manufactured want, right, even if it's manufactured demand
2446500	2449500	which means that then more users will come and put more data in
2449500	2454500	and it can specifically figure out how to manufacture the types of behavior that increase data collection
2454500	2460500	and so you do get this recursive process on how many people, how much data, how good are the machine learning algorithms
2460500	2465500	you know, that kind of thing and this is one of the reasons that we see these natural monopoly formations
2465500	2468500	within these categories of tech
2468500	2472500	and this is another reason that's important to understand like
2472500	2478500	these types of self-reinforcing dynamics and things like network effects like Metcalf's law
2478500	2483500	didn't exist when the Scottish Enlightenment was coming up with its ideas of capitalism
2483500	2487500	and market and the healthy competition and markets and why that creates checks and balances on power
2487500	2491500	they didn't exist, Adam Smith did not get to think about those things
2491500	2498500	and so when you have a situation where the value of the network is proportional to the square of the
2498500	2505500	people coming into the network then you're incented to keep it free up front, maximize addiction, drive behavior into the system
2505500	2514500	and then once you get to the kind of breakaway point on the return of that thing
2514500	2518500	it becomes nearly impossible for anyone else to come in and overtake that thing
2518500	2521500	so you get a power law distribution in each vertical
2521500	2525500	you get one online market that is bigger than all the other online markets
2525500	2529500	one video player that's bigger than all the other video players, one search, one social network one
2529500	2535500	and that's not because of a government monopoly, that's because of this kind of natural tech monopoly
2535500	2540500	this also means that when we created the laws around monopolies they don't apply to this thing
2540500	2546500	and yet this thing still has the same spirit of power concentration and unchecked power
2546500	2552500	that our ideas of monopoly had but it's able to grow much faster than law is able to figure out how to deal with it
2552500	2556500	or faster than economic theory can change itself, right?
2556500	2561500	and so one of the things that we see is that our social technologies like law, like governance, like economics
2561500	2566500	are actually being obsolete by the development of totally new types of behavior and mechanics
2566500	2572500	that weren't part of the world they were trying to solve problems for, right?
2572500	2578500	and so the Scottish Enlightenment was the development of new ideas of how to problem solve, the problems of its time
2578500	2583500	the constitution was trying to figure out how to solve the problems of its time
2583500	2586500	I would say they were good thinking, right? they were good work
2586500	2590500	the Bretton Woods world was, none of them are adequate to solve these problems
2590500	2593500	because these problems are different in kind
2593500	2598500	and even where they're just an extension of magnitude, when you get enough change in magnitude
2598500	2602500	sometimes it becomes a difference in kind, like as you're getting more and more information to process
2602500	2606500	once you get past what humans can process, infosingularity type issues
2606500	2610500	okay, well now it's a difference in magnitude that becomes a difference in kind
2610500	2613500	which means you need a fundamentally different approach
2613500	2618500	so I would say this is where it's important to recognize that those social technologies that we loved so much
2618500	2621500	because they seemed so much better than all the other options we had at the time
2621500	2624500	like markets and like democracy
2624500	2627500	these are not terminal goods in and of themselves
2627500	2630500	the terminal goods were things like human liberty
2630500	2634500	and justice and checks and balances on power
2634500	2640500	and opportunity and distribution of opportunity and things like that
2640500	2644500	these were the best social technologies possible at the time
2644500	2647500	the new technologies both kill those things
2647500	2648500	they don't work anymore, right?
2648500	2652500	you can't have the social technology of the fourth estate
2652500	2654500	that was necessary for democracy
2654500	2656500	which is why founding fathers said things like
2656500	2658500	if I could have perfect newspapers and a broken government
2658500	2661500	or perfect government and broken newspapers, I'd take the newspapers
2661500	2664500	because if you have an educated populace that all understands what's going on
2664500	2666500	they can make a new form of government
2666500	2668500	if you have people that have no idea what's going on
2668500	2672500	how could they possibly make good choices if their sense making is totally broken
2672500	2676500	so we had this idea that the fourth estate was a prerequisite
2676500	2678500	to a participatory governance
2678500	2684500	but that was based on a very narrow limited capacity for print
2684500	2687500	and again it was the technology of the Gutenberg Press
2687500	2690500	that was one of the things that actually ended feudalism
2690500	2694500	and so the founding fathers were employing that new tech
2694500	2697500	both because it upended the previous tech and it made this new thing possible
2697500	2699500	same with guns
2699500	2703500	they needed guns and second amendments to make this new thing possible
2703500	2707500	but once we get to a internet world
2707500	2709500	where you don't have centralized broadcasts
2709500	2711500	you have decentralized and then there's so much stuff
2711500	2713500	that you can never possibly find at all in search
2713500	2715500	whoever coordinates the search
2715500	2718500	the content aggregators, which is the Facebook, the YouTube, whatever
2718500	2721500	are doing it with the types of business models we have
2721500	2723500	the fourth estate is just dead forever
2723500	2726500	that old version, there's no way to recreate that version
2726500	2728500	that either means democracy is dead forever
2728500	2731500	or anything like a well-informed citizenry
2731500	2733500	that could participate in its governance in any form
2733500	2739500	or you have to say what is a post-internet, post-social media, post-infosingularity
2739500	2743500	fourth estate that creates an adequately educated citizenry
2743500	2746500	that's thinking about the way that our social technologies
2746500	2749500	our social systems have to upgrade themselves
2749500	2752500	in the presence of the tech that obsoleted the way they did work
2752500	2754500	but we can also see and we can give examples of this
2754500	2758500	the new tech also makes possible new things that weren't possible before
2758500	2760500	so we can do something better than industrial-era democracy
2760500	2762500	or industrial-era markets
2762500	2764500	which is why I say they aren't a terminal good
2764500	2767500	they're a way to deliver certain human values that really matter
2767500	2770500	and the new technology that obsoletes those
2770500	2775500	can actually also be facilitative in designing systems
2775500	2777500	that also serve those values
2777500	2779500	but it's not a given that it does
2779500	2782500	that has to become the kind of central orienting mission
2782500	2786500	so Dan just to make sure we're linking this back to the start of this conversation
2786500	2788500	we started this conversation by saying
2788500	2791500	the way that we are going about solving problems
2791500	2796500	let's say using the legacy systems of lawmaking in a congress
2796500	2800500	or using the legacy systems of a town hall to vote on a proposition
2800500	2805500	or trying to pass laws as fast as social media as rewiring society
2805500	2807500	the lines don't match
2807500	2809500	and so what you're saying is that
2809500	2812500	and just for listeners because I know that you use the phrase social technology
2812500	2815500	but I think you're really talking about it
2815500	2818500	social systems, ways of organizing democracy
2818500	2821500	technology in the most fundamental sense of the word
2821500	2827500	of something humans design to facilitate certain kinds of activity or outcomes
2827500	2829500	like language is a technology
2829500	2831500	or democracy is a technology
2831500	2833500	social systems
2833500	2837500	and so if the kind of old world approach of
2837500	2839500	some people might be hearing this and say to themselves
2839500	2840500	now hold on a second
2840500	2842500	so we have all these institutions
2842500	2844500	we have all these structures
2844500	2845500	we live in a democracy
2845500	2848500	and we live in a system that is working the way it does
2848500	2850500	it has its courts, it has its attorney generals
2850500	2852500	it has its litigation procedures
2852500	2854500	it has its lawmaking bodies
2854500	2857500	if you're saying that we can't use those things
2857500	2859500	because they're not adequate
2859500	2861500	or they won't help us solve those problems
2861500	2863500	we need to have new social systems
2863500	2866500	maybe you could give us some hope about why that might be feasible
2866500	2868500	instead of feeling impossible
2868500	2871500	because this is actually precedented in our history
2871500	2873500	when new technologies show up
2873500	2875500	and then new social systems emerge
2875500	2878500	to make room for those technologies functioning well
2878500	2879500	you briefly touched on that
2879500	2883500	but I think it's important to give listeners a few concrete examples
2883500	2888500	there's a number of good academics and disciplines of academics
2888500	2892500	that look at the history of evolutions and physical technology
2892500	2895500	and the corresponding evolutions in
2895500	2898500	thought and culture and social systems
2898500	2901500	Marvin Harris, the cultural materialism
2901500	2903500	did a kind of major opus work here
2903500	2906500	where he specifically looked at how changes in social systems and cultures
2906500	2908500	followed changes in technology
2908500	2911500	there are other bodies of work that will
2911500	2913500	look at the social systems as primary
2913500	2915500	or the cultures as primary
2915500	2917500	and we can say they're inter-affecting
2917500	2920500	but for instance
2920500	2923500	the vast majority of human history was tribal
2923500	2926500	however much 200,000 years of humans
2926500	2930500	in the small Dunbar number villages
2930500	2932500	there was a social technology
2932500	2933500	social systems that mediated that
2933500	2935500	that had to do with how the tribal circles worked
2935500	2939500	and the nature of how resources were shared
2939500	2941500	it was a very different kind of economic system
2941500	2943500	a very different kind of judicial system
2943500	2945500	a different educational system
2945500	2946500	it had all those things
2946500	2947500	it had a way of education
2947500	2949500	meaning intergenerational knowledge transfer
2949500	2951500	of the entire knowledge set that was needed
2951500	2955500	for the tribe to continue operating
2955500	2958500	the development of certain technologies
2958500	2960500	particularly the plow
2960500	2963500	but baskets and a few other things
2963500	2964500	obsolete that thing
2964500	2967500	because all of a sudden it made possible
2967500	2968500	big amounts of surplus
2968500	2971500	that made reason for much larger populations
2971500	2972500	to emerge
2972500	2974500	those larger populations
2974500	2976500	were going to win in conflict against
2976500	2978500	the smaller populations
2978500	2980500	and so you can see that then the emergence
2980500	2981500	of new social technology
2981500	2983500	to facilitate large groups of people
2983500	2985500	empire types
2985500	2987500	civilization technology emerged
2987500	2989500	you can see
2989500	2992500	and that there were a few other shifts in technology
2992500	2995500	that evolved the types of empires that were there
2995500	2997500	and then the next one that people talk about a lot
2997500	2999500	is the industrial revolution
2999500	3001500	from the printing press specifically
3001500	3002500	and then steam engine
3002500	3004500	the gunpowder revolution was part of it
3004500	3006500	that kind of ended feudalism
3006500	3008500	and began the nation state world
3008500	3010500	and so you can see like
3010500	3013500	what is the thing that the founding fathers
3013500	3014500	in the US were doing
3014500	3017500	well they weren't trying to keep winning at feudalism
3017500	3019500	there was a game that had been happening
3019500	3020500	for a long time
3020500	3021500	and they were saying
3021500	3023500	like no we're all people who are in
3023500	3025500	of the type of people who could do well
3025500	3026500	at that system
3026500	3028500	and rather than do that
3028500	3029500	we recognize that there are
3029500	3031500	fundamentally things wrong with this system
3031500	3033500	and fundamentally new possibilities
3033500	3035500	that hadn't been previously recognized
3035500	3037500	so we're going to actually try to design
3037500	3039500	a fundamentally different system
3039500	3040500	that we think
3040500	3042500	a more perfect union
3042500	3044500	that makes life liberty in the pursuit of happiness
3044500	3045500	better for everybody
3045500	3047500	and increases productive capacity
3047500	3048500	and things like that
3048500	3050500	so that was fundamentally in advance
3050500	3052500	in social technology or social systems
3052500	3053500	that both utilized
3053500	3055500	new physical technology
3055500	3057500	and was enabled by it
3060500	3062500	in the current situation
3063500	3065500	there are groups that are advancing
3065500	3067500	the exponential technologies
3067500	3069500	and that means whatever social systems
3069500	3070500	that they're employing
3070500	3072500	are the social systems of the future
3072500	3073500	if we don't change it
3073500	3075500	and that's what I want to get to in a moment
3075500	3078500	but like who is working to
3078500	3081500	implement any of the new emerging
3081500	3083500	tech for better social systems
3083500	3085500	that are aligned with social systems we want
3085500	3087500	you've had Audrey Tang
3087500	3088500	on the show
3088500	3090500	do you want to just briefly describe
3090500	3092500	an example of what she
3092500	3094500	and what they have done there
3094500	3096500	because if people aren't aware of it
3096500	3098500	that's a pretty prime example
3098500	3100500	of for this particular iteration
3100500	3102500	sure well
3102500	3105500	and maybe just to go back briefly
3105500	3107500	because you gave this example
3107500	3109500	in one of our earlier conversations
3109500	3111500	that the printing press
3111500	3113500	could have been used by the feudal lords
3113500	3115500	for consciously reinforcing feudalism
3115500	3117500	but instead this new technology
3117500	3119500	the printing press gives way
3119500	3121500	to new ways of organizing society
3121500	3123500	so we can actually have things like
3123500	3125500	a fourth estate or newspapers
3125500	3127500	or things like that
3127500	3129500	both happen
3129500	3131500	but then the new thing theoretically
3131500	3133500	has to win out over the old thing
3133500	3135500	at least the one that we want
3135500	3137500	that holds the values that the society wants
3137500	3139500	so
3139500	3141500	I think a lot of people can hear our conversation
3141500	3143500	we've had this riff before
3143500	3145500	actually following our last episode
3145500	3147500	after my senate testimony
3147500	3149500	speaking about a frame
3149500	3151500	that you have offered and know well
3151500	3153500	which is that we can notice
3153500	3155500	that digital authoritarian societies
3155500	3157500	right now like China
3157500	3159500	are consciously using exponential technologies
3159500	3161500	to make stronger more effective digital
3161500	3163500	closed and authoritarian societies
3163500	3165500	and in contrast digital open societies
3165500	3167500	democracies like the United States
3167500	3169500	are not consciously using technology
3169500	3171500	to make stronger healthier open societies
3171500	3173500	instead they've sort of surrendered
3173500	3175500	what they are to
3175500	3177500	private technology
3177500	3179500	corporations pursuing self-interest
3179500	3181500	to shareholders and are
3181500	3183500	profiting from the degradation
3183500	3185500	and dysfunction of democracies
3185500	3187500	and so
3187500	3189500	when we say all this and we talk about
3189500	3191500	how do we build the kind of next social system
3191500	3193500	and Audrey Tang and her work
3193500	3195500	I think people
3195500	3197500	get tripped up in thinking that what we really mean
3197500	3199500	is we have to make some kind of 21st century
3199500	3201500	digital democracy in fact I probably said those words
3201500	3203500	but what we're really talking about here
3203500	3205500	is some new concept that preserves
3205500	3207500	the principles of what we meant by
3207500	3209500	a democracy
3209500	3211500	but instantiated with the new technologies
3211500	3213500	our version of the new printing press
3213500	3215500	which is networked
3215500	3217500	information environments and
3217500	3219500	all of the new capacities that we have in the 21st century
3219500	3221500	with
3221500	3223500	mobility where everyone's connected to everywhere
3223500	3225500	and everything all at once
3225500	3227500	so what is that system
3227500	3229500	that leverages the current technology
3229500	3231500	and makes a stronger healthier open society
3231500	3233500	and I think Audrey Tang's work
3233500	3235500	I mean I would probably send listeners back
3235500	3237500	to listen to that episode I think it's one of our
3237500	3239500	most listened to and most popular episodes for a reason
3239500	3241500	because in Taiwan
3241500	3243500	she's essentially built an entire
3243500	3245500	civic technology ecosystem
3245500	3247500	in which people are really participating
3247500	3249500	in the governance of their society
3249500	3251500	we need masks, we need
3251500	3253500	better air quality sensors, we need to fix these
3253500	3255500	potholes, there are processes
3255500	3257500	by which every time you're frustrated by something
3257500	3259500	you actually get invited into a civic design process
3259500	3261500	where whether it's the potholes or the masks
3261500	3263500	you can actually participate in having a better
3263500	3265500	system
3265500	3267500	complaining about the tax system and filing your taxes
3267500	3269500	and maybe it's an inefficient form
3269500	3271500	or something like that you get brought into a design process
3271500	3273500	of what would make it better
3273500	3275500	and so the system is participatory but not in that
3275500	3277500	kind of 18th century way of hey there's a
3277500	3279500	physical wooden townhouse and we're going to walk into it
3279500	3281500	and we're going to hang out there for three hours
3281500	3283500	and we're going to yell and scream about issues that are more local
3283500	3285500	within 10, 15 miles
3285500	3287500	of where we are because we were existing in a world before automobiles
3287500	3289500	we're now talking about how do you do
3289500	3291500	an open society social system
3291500	3293500	but in a world with all of the new technologies
3293500	3295500	that are not just here today but emerging
3295500	3297500	and so do you want to talk a little bit
3297500	3299500	about what
3299500	3301500	how we even navigate that challenge
3301500	3303500	and why is some new social system like that
3303500	3305500	necessary
3305500	3307500	for dealing with these problems that you've
3307500	3309500	sort of laid out at the beginning
3309500	3311500	because I'm sure people would like to feel
3311500	3313500	less anxiety about those things
3313500	3315500	hanging around for longer
3315500	3317500	yeah I think
3317500	3319500	I think what
3319500	3321500	Taiwan has been doing
3321500	3323500	and what Audrey Tang
3323500	3325500	in the digital ministry position
3325500	3327500	in particular has been leading
3327500	3329500	is probably the best example
3329500	3331500	certainly one of the best examples in the world of this kind of
3331500	3333500	process in thinking
3333500	3335500	and
3335500	3337500	does it apply in the
3337500	3339500	or could it apply in the exact same way to the US
3339500	3341500	no of course not like we know that
3341500	3343500	because
3343500	3345500	of the relatively small geography
3345500	3347500	and
3347500	3349500	high-speed train transportation
3349500	3351500	you can get across Taiwan in an hour and a half
3351500	3353500	and so when you're mentioning the small scale
3353500	3355500	of local government at the beginning of the US
3355500	3357500	where you come to the town hall in a way they have
3357500	3359500	that right like it's 23 million people
3359500	3361500	but there is
3361500	3363500	an older shared culture
3363500	3365500	there is all there also happens
3365500	3367500	to be an existential threat just right
3367500	3369500	off their border that is
3369500	3371500	you know big enough that they can't
3371500	3373500	just chill and not focus on it
3373500	3375500	everyone has to be civically engaged
3375500	3377500	with some civic identity and like that
3377500	3379500	they didn't start making
3379500	3381500	their culture in the industrial era and then have to
3381500	3383500	upgrade it right like they started
3383500	3385500	later
3385500	3387500	where they're we're able to start at a higher
3387500	3389500	level of the tech stack
3389500	3391500	so there's a number of reasons why it's different
3391500	3393500	so we're not going to naively say what you do
3393500	3395500	in a tiny country that is culturally
3395500	3397500	and ethnically homogeneous
3399500	3401500	and has a higher GDP
3401500	3403500	education per capita and whatever is the same thing
3403500	3405500	you would do but we can certainly take a lot
3405500	3407500	of the examples and say how would
3407500	3409500	they apply differently in different
3409500	3411500	contexts
3413500	3415500	so
3415500	3417500	the thing we said earlier
3417500	3419500	that this suite of
3419500	3421500	exponential technologies is so much more powerful
3421500	3423500	than all of the previous types
3423500	3425500	of power that only
3425500	3427500	those who are
3427500	3429500	developing and deploying
3429500	3431500	them will be
3431500	3433500	really steering the direction of the future
3433500	3435500	and
3435500	3437500	that there are ways of employing them
3437500	3439500	that do cause
3439500	3441500	catastrophic risk
3441500	3443500	and
3443500	3445500	the catastrophic risk is of two primary
3445500	3447500	kinds right conflict theory mediated
3447500	3449500	and
3449500	3451500	you can't
3451500	3453500	you just can't do warfare
3453500	3455500	with this level of technology
3455500	3457500	and this interconnected a world
3457500	3459500	and make it through well
3459500	3461500	not all catastrophic risk means existential
3461500	3463500	doesn't all mean nuclear
3463500	3465500	war and nuclear winter and we've killed
3465500	3467500	all the mammals on earth it might
3467500	3469500	just mean we break global supply chains
3469500	3471500	kill lots of people and regress
3471500	3473500	humanity and the quality of the biosphere
3473500	3475500	pretty significantly so
3475500	3477500	I'm not
3477500	3479500	just focused on existential risk I'm interested
3479500	3481500	in kind of catastrophic risk
3481500	3483500	at scale in general
3483500	3485500	and we can see that exponential tech
3485500	3487500	applied
3487500	3489500	as
3489500	3491500	in conflict theory and in
3491500	3493500	mistake
3493500	3495500	as externalities and the cumulative effects
3495500	3497500	could you define
3497500	3499500	conflict theory and mistake theory for people
3499500	3501500	who are not familiar with those terms
3501500	3503500	yeah there's
3503500	3505500	a very nice
3505500	3507500	discussion on the less wrong forum
3507500	3509500	if people are interested to go deeper
3509500	3511500	and it's this question of
3511500	3513500	how much of the problems in the world
3513500	3515500	are the result of
3515500	3517500	conflict theory versus mistake theory
3517500	3519500	meaning conflict theory is
3519500	3521500	we knew we either wanted to cause
3521500	3523500	that problem that harm to whomever
3523500	3525500	as in a knowingly wanted to win
3525500	3527500	at a war and
3527500	3529500	or at least we knew we were
3529500	3531500	going to cause that problem and didn't care because
3531500	3533500	it was attached to something we wanted
3533500	3535500	right conflict theory or mistake
3535500	3537500	theory we didn't know we didn't want to cause
3537500	3539500	it and we really didn't know and it was just
3539500	3541500	unintended unanticipatable consequence
3541500	3543500	and it's fair to say that there's
3543500	3545500	both right there's plenty of both
3545500	3547500	one
3547500	3549500	thing that is worth knowing is that
3549500	3551500	if I
3551500	3553500	if I'm trying to do something that is
3553500	3555500	actually motivated by conflict theory
3555500	3557500	it benefits me to pretend that it was mistake
3557500	3559500	theory benefits me to pretend that I had no idea
3559500	3561500	and then afterwards say oh it was
3561500	3563500	an unintended unanticipatable consequence
3563500	3565500	it was too complex people can't predict stuff
3565500	3567500	like that and so
3567500	3569500	the reality
3569500	3571500	of mistake theory
3571500	3573500	ends up being a source of plausible
3573500	3575500	deniability for conflict theory
3575500	3577500	and
3577500	3579500	but they're both things and we have to
3579500	3581500	overcome both meaning we have to
3581500	3583500	have choice making processes in our new system
3583500	3585500	of coordination and like this sounds
3585500	3587500	like maybe hippy stuff
3587500	3589500	until you take
3589500	3591500	seriously the change of context
3591500	3593500	oh we have to have problems of choice making that
3593500	3595500	consider the whole that sounds like
3595500	3597500	unrealizable hippy stuff
3597500	3599500	until you realize
3599500	3601500	but we're making choices that affect
3601500	3603500	the whole
3603500	3605500	at a level that
3605500	3607500	can even individually be catastrophic
3607500	3609500	and is definitely catastrophic cumulatively
3609500	3611500	so if we aren't factoring
3611500	3613500	it
3613500	3615500	then the human experiment self terminates and maybe that's
3615500	3617500	the answer to the great filter hypothesis
3617500	3619500	right and so
3619500	3621500	our
3621500	3623500	well yeah
3623500	3625500	I think people don't have an intuitive grasp of
3625500	3627500	what it means that each of us
3627500	3629500	are walking around with the power of
3629500	3631500	gods to influence huge enormous
3631500	3633500	consequences I mean
3633500	3635500	I could give a few examples every time you
3635500	3637500	enact with the global supply chain and hit buy on Amazon
3637500	3639500	you invisibly enacted
3639500	3641500	shipping and planes
3641500	3643500	and petroleum and wars in the middle east
3643500	3645500	there's a whole bunch of things that we're
3645500	3647500	sort of tied into when you are
3647500	3649500	posting something on social media and have
3649500	3651500	more than a million followers you're
3651500	3653500	influencing the global information ecology
3653500	3655500	and if you're angry and biased about one
3655500	3657500	side of the other of the pandemic is real
3657500	3659500	or it's not real or something like that you're
3659500	3661500	externalizing more bias into the commons
3661500	3663500	of how keep the rest of the world understands
3663500	3665500	things so we're walking around with increasing
3665500	3667500	power but I don't think the increasing power
3667500	3669500	that we've granted is intuitive
3669500	3671500	for some folks did you explain
3671500	3673500	some more examples of that there's both
3673500	3675500	cumulative effect and
3675500	3677500	like cumulative long-term
3677500	3679500	and fairly singular short-term
3679500	3681500	and cumulative long-term
3681500	3683500	I mean you go back to
3685500	3687500	early
3687500	3689500	US settlers coming into the
3689500	3691500	US moving west
3691500	3693500	and they're being buffalo everywhere and there had
3693500	3695500	been buffalo everywhere for a very long time
3695500	3697500	and then there's no buffalo in whole areas that
3697500	3699500	were forested with old growth forest became
3699500	3701500	deforested and it was
3701500	3703500	like no it's impossible we could never get rid of all the
3703500	3705500	buffalo like I we could never
3705500	3707500	cut down all the trees but the cumulative effect
3707500	3709500	of lots of people thinking that way we're individually
3709500	3711500	I have no incentive to leave the buffalo alive
3711500	3713500	and I do have an incentive for my family individually
3713500	3715500	to kill it but everybody thinking
3715500	3717500	that way and
3717500	3719500	increasing our
3719500	3721500	desire for how much we consume per capita
3721500	3723500	our technology that allows us
3723500	3725500	to consume more per capita
3725500	3727500	and developing more capital
3727500	3729500	more total people well
3729500	3731500	then you start getting
3731500	3733500	environmental destruction and species extinction at scale
3733500	3735500	and that's a long time ago
3735500	3737500	right like that's much lower tech and much less
3737500	3739500	people and it's
3739500	3741500	distributed action it's a
3741500	3743500	cumulative effect issue
3743500	3745500	and obviously we see that with
3747500	3749500	nobody's intending to fill the
3749500	3751500	ocean with microplastics
3751500	3753500	but everybody's buying shit that is filling the
3753500	3755500	oceans with microplastics and
3755500	3757500	so everyone is participating with systems where
3757500	3759500	the system as a whole
3759500	3761500	is sociopathic the system is self-terminating
3761500	3763500	the system doesn't exist without all the agents
3763500	3765500	interacting with it all the agents feel like
3765500	3767500	their behavior is so small
3767500	3769500	that that justifies everybody doing
3769500	3771500	that thing right so that's what we
3771500	3773500	mean by cumulative kind of catastrophic risk
3773500	3775500	but it's
3775500	3777500	also true that
3777500	3779500	whoever made
3779500	3781500	that
3781500	3783500	thermite bomb and hooked it to a drone and hit
3783500	3785500	the Ukrainian munitions factory a couple years
3785500	3787500	ago that caused a billion dollars
3787500	3789500	in damage exploded munitions factory
3789500	3791500	the effect of a bomb as big
3791500	3793500	as the largest
3793500	3795500	nuclear bomb the U.S. arsenal has an incendiary bomb
3795500	3797500	this is a home
3797500	3799500	that was a homemade little bomb in a
3799500	3801500	drone right and
3801500	3803500	so and crisper gene
3803500	3805500	drives are
3805500	3807500	cheap and easy and it doesn't take
3807500	3809500	that much advanced
3809500	3811500	knowledge to start working with them and
3811500	3813500	so that that starts to look
3813500	3815500	like
3815500	3817500	individuals and small
3817500	3819500	groups with real catastrophic ability
3819500	3821500	not long-term and cumulatively
3821500	3823500	the increase in our tech
3823500	3825500	gives us both issues via
3825500	3827500	globalization and the overall system you
3827500	3829500	get these cumulative long-term effects
3829500	3831500	and with the exponential
3831500	3833500	tech creating decentralized catastrophic
3833500	3835500	capabilities one
3835500	3837500	of the core questions we have to answer is how do we
3837500	3839500	make a world that is anti fragile
3839500	3841500	in the presence of
3841500	3843500	those kind of catastrophic capabilities that are
3843500	3845500	easy to produce and thus
3845500	3847500	decentralizable
3847500	3849500	and so
3849500	3851500	how do we do that
3851500	3853500	what are the social systems that we need
3853500	3855500	to employ to bind some of these
3855500	3857500	bad
3857500	3859500	facts and ways that the natural
3859500	3861500	inclinations of self-interested actors
3861500	3863500	will drive things in that direction just
3863500	3865500	to link this to the social media space for people
3865500	3867500	if I know that I can get
3867500	3869500	a little bit more attention and a little bit
3869500	3871500	more likes and clicks and follows and shares
3871500	3873500	and so on if I exaggerate the truth
3873500	3875500	by five percent just to use a little bit
3875500	3877500	more of an extreme adjective
3877500	3879500	you know I know that that in the long
3879500	3881500	run would be bad if everybody did that
3881500	3883500	but for me right now I can win a few hits
3883500	3885500	and I can get more influence and I'm an Instagram influencer
3885500	3887500	and I'm making ten thousand dollars a month and if I don't do it
3887500	3889500	I'm noticing everyone else's do it and if I don't use
3889500	3891500	the filter everyone else is using the filter
3891500	3893500	and so everyone ends up in this sort of another race
3893500	3895500	at the bottom sort of situation
3895500	3897500	that has that kind of cumulative degradation
3897500	3899500	or cumulative
3899500	3901500	derangement where there's increasing distance
3901500	3903500	between what is true and what people believe
3903500	3905500	because we've all been subtly exaggerating it
3905500	3907500	to make our point and gain influence and so on
3907500	3909500	and so
3909500	3911500	just to give another example
3911500	3913500	maybe for listeners in kind of the space that they're
3913500	3915500	more familiar with
3915500	3917500	but going back I mean the whole premise
3917500	3919500	of this is as we gain more
3919500	3921500	exponential technologies that have
3921500	3923500	more capacity and more hands
3923500	3925500	so instead of having just the US and Russia having this
3925500	3927500	you have
3927500	3929500	whether as you mentioned CRISPR gene drives or
3929500	3931500	some of the drone things that are out there
3931500	3933500	more and more people have access to these things
3933500	3935500	how can we bind
3935500	3937500	those kinds of forces and what are the
3937500	3939500	social systems that we need to make that happen
3939500	3941500	yeah I want to go back
3941500	3943500	as you were describing this
3943500	3945500	I was thinking about how many people
3945500	3947500	who
3947500	3949500	listen to your show
3949500	3951500	who maybe work in technology who might have
3951500	3953500	they work in technology
3953500	3955500	because they see the positive things technology can do
3955500	3957500	and have more of a kind of techno-optimist
3957500	3959500	point of view and this overall conversation
3959500	3961500	might sound very techno-pessimist and
3961500	3963500	like did we not
3963500	3965500	read Pinker and watch Hans Rosling and
3965500	3967500	you know those types of things
3967500	3969500	and
3969500	3971500	so I want to speak to that briefly
3975500	3977500	first this is a meta-point
3977500	3979500	but it's worth saying right now
3979500	3981500	particularly in on this podcast
3981500	3983500	and in the kind of post-truth
3983500	3985500	post or fake fact
3985500	3987500	world where then so much
3987500	3989500	of the emphasis has gone into we need fact
3989500	3991500	checkers and we need real
3991500	3993500	facts
3993500	3995500	obviously it's possible
3995500	3997500	to have an epistemic
3997500	3999500	error or even intentional error in the
3999500	4001500	process of generating a fact
4001500	4003500	is there corruption in the institutions
4003500	4005500	and that kind of thing but let's even say that wasn't
4005500	4007500	an issue and the things that go through the right epistemic
4007500	4009500	process as facts are facts
4009500	4011500	can you lie with facts
4011500	4013500	totally can you can you mislead
4013500	4015500	with facts yeah because nobody
4015500	4017500	is going to make their choice on one fact
4017500	4019500	they make their choice based on a situational assessment
4019500	4021500	based on a narrative based on a gestalt
4021500	4023500	of a whole thing that's lots of different facts
4023500	4025500	well which facts do I include and which facts do I not
4025500	4027500	include and
4027500	4029500	am I de-contextualizing the fact
4029500	4031500	so
4031500	4033500	the quality of life has gone up
4033500	4035500	so much because we average person lived
4035500	4037500	on less than a dollar a day in the US in 1815
4037500	4039500	and now they live on this many dollars
4039500	4041500	a day which inflation adjusted means higher quality
4041500	4043500	of life yeah but in 1815 most
4043500	4045500	of their needs didn't come through dollars
4045500	4047500	they grew their own vegetables they hunted
4047500	4049500	so I'm de-contextualizing
4049500	4051500	the facts to compare something that's really apples
4051500	4053500	and oranges so even if the fact is quote unquote
4053500	4055500	true the de-contextualization and
4055500	4057500	recontextualization makes it seem like it means
4057500	4059500	something different than it means
4059500	4061500	and the same with the cherry picking
4061500	4063500	of facts and I can very easily
4063500	4065500	say oh there's a lower percentage of people in extreme
4065500	4067500	poverty but I might also be changing the definitions
4067500	4069500	of extreme poverty I can also
4069500	4071500	rather than focus on percentage say well there's more
4071500	4073500	total people in poverty than there were total people
4073500	4075500	in the world before the industrial revolution so like
4075500	4077500	so there's the
4077500	4079500	abilities to de-contextualize
4079500	4081500	and recontextualize facts there's the ability to
4081500	4083500	cherry pick facts and there's
4083500	4085500	the ability to lake off frame facts
4085500	4087500	and put particular kinds
4087500	4089500	of sentiment and moral valence
4089500	4091500	on it and so am I talking about them as
4091500	4093500	illegal aliens or undocumented
4093500	4095500	workers and I get a very different kind
4095500	4097500	of sentiment so talking about it
4097500	4099500	as a pre-owned car or a used car
4099500	4101500	everyone loves a pre-owned car no one wants
4101500	4103500	a used car and so these
4103500	4105500	very simple semantic frames
4105500	4107500	contextual frames cherry picking
4107500	4109500	of the things means that I can
4109500	4111500	make a narrative
4111500	4113500	where all the facts went through the most
4113500	4115500	rigorous fact checker and yet the narrative
4115500	4117500	as a whole is misleading
4117500	4119500	and so fact checking is necessary
4119500	4121500	but it is not sufficient for a good
4121500	4123500	epistemic and good sense making and not
4123500	4125500	only is it not sufficient it's even weaponizable
4125500	4127500	this is a very important thing
4127500	4129500	to understand because if you are not
4129500	4131500	pursuing that
4131500	4133500	you're if you're not recognizing
4133500	4135500	that you might be believing
4135500	4137500	nonsense thinking that you're using
4137500	4139500	epistemic rigor
4141500	4143500	okay so the techno pessimists
4143500	4145500	and the techno optimists both cherry pick
4145500	4147500	and they both lake off
4147500	4149500	frame and
4149500	4151500	this is true with
4151500	4153500	all the difference in almost every
4153500	4155500	political ideology the woke and the
4155500	4157500	woke the
4157500	4159500	the pro socialist
4159500	4161500	pro capitalist you'll notice that the way
4161500	4163500	they do their arguments the
4163500	4165500	systemic racism is really really terrible
4165500	4167500	no there's not that bad the systemic racism
4167500	4169500	they both have stats
4169500	4171500	but this is actually you can almost think
4171500	4173500	of it as statistical warfare as a tool
4173500	4175500	of narrative warfare and
4177500	4179500	so this is where
4179500	4181500	a higher level of earnestness
4181500	4183500	rather than a particular
4183500	4185500	bested interest or bias a higher willingness
4185500	4187500	to look at biases a higher level of rigor
4187500	4189500	you know ends up being critical to
4189500	4191500	actually overcome any of these things
4191500	4193500	so
4193500	4195500	can I cherry pick stats that make it look like everything is getting
4195500	4197500	better totally those things are true
4197500	4199500	and nobody wants to go back to a world
4199500	4201500	before novocaine when you have to do dentistry
4201500	4203500	and
4203500	4205500	nobody wants to go back to a world before
4205500	4207500	penicillin when basic bacterial infections go around
4207500	4209500	and like there's totally good stuff that has
4209500	4211500	emerged
4211500	4213500	and are there all
4213500	4215500	kinds of
4215500	4217500	ubiquitous
4217500	4219500	mental illnesses and
4219500	4221500	chronic complex disease
4221500	4223500	that didn't exist before and
4223500	4225500	increase in the total
4225500	4227500	number of addictive type behaviors within populations
4227500	4229500	and
4229500	4231500	and a radical
4231500	4233500	increase in the catastrophic risk landscape
4233500	4235500	and negative effect to environmental metrics
4235500	4237500	so things are getting better and things are getting worse at the same time
4237500	4239500	it's important to understand that depending upon what you pick
4239500	4241500	it's just that the things that are getting worse are heading
4241500	4243500	towards
4243500	4245500	tipping points that make the whole thing no longer viable
4245500	4247500	and so that we're not
4247500	4249500	denying that there are things that are getting better we're
4249500	4251500	saying that for the game to continue
4251500	4253500	at all right to have it
4253500	4255500	be an infinite game that gets to keep continuing
4255500	4257500	there are certain things that have to not happen
4257500	4259500	and you can't have the things that are getting worse keep getting
4259500	4261500	worse at the curve that they are
4261500	4263500	and have the things that are getting better be able to
4263500	4265500	continue at all so I just want to
4265500	4267500	say that so
4267500	4269500	naive techno optimism
4269500	4271500	can actually
4271500	4273500	make you a part of the problem because
4273500	4275500	then you do things like develop
4275500	4277500	a solution to a narrowly
4277500	4279500	defined problem and externalize harm to other areas
4279500	4281500	because you weren't taking seriously enough
4281500	4283500	not doing that
4283500	4285500	but techno pessimism also
4285500	4287500	makes you a part of the problem
4287500	4289500	or at least not a part of the solution because
4289500	4291500	because the world is the future
4291500	4293500	is not going to be determined by Luddites
4293500	4295500	it's not going to be determined by people who aren't developing the tools of power
4295500	4297500	so if you aren't actually looking at
4297500	4299500	how do we develop a high
4299500	4301500	tech world that is also a fundamentally desirable
4301500	4303500	in terms of a high nature and high touch
4303500	4305500	world then you really
4305500	4307500	aren't thinking about it in a way that ends up
4307500	4309500	mattering and so we are
4309500	4311500	techno optimist but
4311500	4313500	not naive techno optimist we go through the
4313500	4315500	totally cynical phase of man tech
4315500	4317500	is a serious issue and then you go to a post
4317500	4319500	cynical phase of
4319500	4321500	if I want to be techno optimist
4321500	4323500	and not be silly
4323500	4325500	what does it take to imagine a world
4325500	4327500	where humans have that much power
4327500	4329500	and we are good stewards of it
4329500	4331500	meaning that we actually tend to each other
4331500	4333500	well and we don't create a
4333500	4335500	dystopic world that has
4335500	4337500	exponential wealth and equality
4337500	4339500	and an underclass that nobody in the upper class would want to trade places with
4339500	4341500	and that doesn't cause catastrophic risk
4341500	4343500	right now the amount
4343500	4345500	of power of exponential tech
4345500	4347500	makes two attractors most likely
4347500	4349500	catastrophic risk
4349500	4351500	of some kind
4351500	4353500	or social systems
4353500	4355500	that do not preserve
4355500	4357500	the values that we care most about
4357500	4359500	that are the ones that are currently most
4359500	4361500	working to develop and deploy that technology
4361500	4363500	and to just give a very brief
4363500	4365500	recap of the frame that Tristan
4365500	4367500	you gave on earlier as you mentioned
4369500	4371500	China is not leaving 100%
4371500	4373500	of its technology
4373500	4375500	development to the market to develop however
4375500	4377500	it wants even if it harms the nation state
4377500	4379500	they are happy to bind technology
4379500	4381500	companies that are getting too large
4381500	4383500	and in ways that would damage the nation state
4383500	4385500	as we saw with Ant Corporation
4385500	4387500	and they are doing a lot of
4387500	4389500	very centralized innovation
4389500	4391500	as well
4391500	4393500	associated with long term planning
4393500	4395500	long term planning is a key thing
4395500	4397500	in the US
4397500	4399500	term limits make long term planning very hard
4399500	4401500	as does a highly rival risk
4401500	4403500	two party system that is
4403500	4405500	willing to damage the
4405500	4407500	nation as a whole to drive party wins
4407500	4409500	so in that system
4409500	4411500	almost all the energy
4411500	4413500	just goes into trying to win
4413500	4415500	you spend at least a couple years
4415500	4417500	but even the years before that are fundraising
4417500	4419500	creating alliances to just try to win
4419500	4421500	then you are not going to invest in anything
4421500	4423500	heavily that has return times
4423500	4425500	longer than four years because it won't get you re-elected
4425500	4427500	so no real long term planning
4427500	4429500	and then
4429500	4431500	whatever you do do in those four years
4431500	4433500	will get undone systematically in the next four years
4433500	4435500	for the most part
4435500	4437500	that system of governance
4437500	4439500	will just fail comprehensively
4441500	4443500	in relationship to a more
4445500	4447500	to a system that
4447500	4449500	doesn't have that much internal infighting
4449500	4451500	and that has
4451500	4453500	the capacity to do long term planning
4453500	4455500	and there is a million examples we can look at
4455500	4457500	but just when did high speed
4457500	4459500	trains start? they started
4459500	4461500	we saw them emerge in Europe
4461500	4463500	we saw them emerge in Japan and in China
4463500	4465500	we started to export them all around the world
4465500	4467500	and the US still doesn't have any high speed trains
4467500	4469500	and it's like what happened?
4469500	4471500	and we can see
4471500	4473500	that the US innovated in fundamental
4473500	4475500	tech in the Manhattan project
4475500	4477500	kind of through the Apollo project
4477500	4479500	but then it started to privatize almost everything to the market
4479500	4481500	the market started to develop in ways that really
4481500	4483500	were not advancing the technology
4483500	4485500	in a way that increased the coherence
4485500	4487500	of the nation
4487500	4489500	and the fundamental civic values and ideas of the nation
4489500	4491500	even the World War II thing
4491500	4493500	increased our military capacities radically
4493500	4495500	but that didn't mean we actually really advanced the ideas
4495500	4497500	of democracy or those values of
4497500	4499500	do we make a better system
4499500	4501500	to educate the people and inform them
4501500	4503500	and help them participate in their governance
4503500	4505500	do we make better governance?
4505500	4507500	this is why the US military is so powerful
4507500	4509500	but the US government is so kind of inept
4509500	4511500	which is why nobody wants to fight a war
4511500	4513500	with the US, a kinetic war
4513500	4515500	but it's very easy
4515500	4517500	right now to
4517500	4519500	engage in
4519500	4521500	supporting narrative warfare
4521500	4523500	where you turn the left and the right against each other
4523500	4525500	increasingly
4525500	4527500	and where you do long-term planning
4527500	4529500	where the US can't do long-term planning of those kinds
4529500	4531500	so we can see
4531500	4533500	that the
4533500	4535500	government of the US
4535500	4537500	and not just the US but like we can see that
4537500	4539500	open societies are not innovating
4539500	4541500	in how to be better open societies
4541500	4543500	for the most part, more effective ones
4543500	4545500	where they're using the new tech to make better open societies
4545500	4547500	that's happening in the market sector
4547500	4549500	the market is making exponentially
4549500	4551500	more powerful companies
4551500	4553500	a company is not a democracy
4553500	4555500	it's not a participatory governance structure
4555500	4557500	in general, it's a kind of very top-down
4557500	4559500	autocratic type system
4559500	4561500	and so we see that
4561500	4563500	there's more authoritarian
4563500	4565500	nation states that are
4565500	4567500	intentionally doing long-term
4567500	4569500	planning of the development and deployment of
4569500	4571500	exponential tech to make better
4571500	4573500	nation states of that kind
4573500	4575500	and we can't even blame them
4575500	4577500	they look at
4577500	4579500	trying to have the benefit of getting to see both where the USA
4579500	4581500	failed and where the USSR failed
4581500	4583500	and try to make something they didn't fail in either of those ways
4583500	4585500	and there's some things that are very smart about
4585500	4587500	those approaches
4587500	4589500	so we see though exponentially empowered
4591500	4593500	more autocratic type structures
4593500	4595500	and the emergence of
4595500	4597500	one natural
4597500	4599500	monopoly per tech sector
4599500	4601500	and then the interaction of those that kind of
4601500	4603500	becomes like oligarchic feudalism
4603500	4605500	tech feudalism
4605500	4607500	neither of those
4607500	4609500	have the types of jurisprudence
4609500	4611500	or public accountability or whatever
4611500	4613500	that we're really interested in
4613500	4615500	so the two attractors right now
4615500	4617500	is
4617500	4619500	the emergence of social systems
4619500	4621500	that are deploying
4621500	4623500	the exponential tech
4623500	4625500	that will probably not preserve
4625500	4627500	the social values that we're interested in
4627500	4629500	and not be maximally desirable civilizations
4629500	4631500	probably pretty dystopic ones
4631500	4633500	or not even guiding it well enough
4633500	4635500	to prevent catastrophic risk
4635500	4637500	those are the two major types of attractors
4637500	4639500	we want a new attractor which is
4639500	4641500	how do we
4641500	4643500	utilize the new exponential technologies
4643500	4645500	the whole suite of them
4645500	4647500	to build new systems of collective intelligence
4647500	4649500	new better systems of social technology
4649500	4651500	how do you make a fourth estate that can really adequately
4651500	4653500	educate everyone
4653500	4655500	in a post
4655500	4657500	Facebook world
4657500	4659500	well the same way that we're trying to
4659500	4661500	optimize control patterns
4661500	4663500	of human behavior for market purposes
4663500	4665500	to get them to buy certain things
4665500	4667500	and to direct their attention
4667500	4669500	could that be used educationally
4669500	4671500	of course it could if it was being
4671500	4673500	developed for that purpose
4673500	4675500	and the AI tech
4675500	4677500	that can take a bunch of faces
4677500	4679500	and make a new face that is merged out of those
4679500	4681500	could it take semantic fields of people's propositions
4681500	4683500	and values and create a proposition
4683500	4685500	that is kind of the semantic
4685500	4687500	center of the space and then could we use
4687500	4689500	we can't all fit into a town hall
4689500	4691500	but can we engage in digital
4691500	4693500	spaces where we can have
4693500	4695500	better processes of proposing
4695500	4697500	refinements to the propositions
4697500	4699500	of course we can, could we use blockchain
4699500	4701500	and other types of uncorruptible ledgers
4701500	4703500	to solve corruption which is something that universally
4703500	4705500	everybody thinks is a good idea
4705500	4707500	should all government money
4707500	4709500	be on a blockchain
4709500	4711500	the movement of it so you have provenance so you can see
4711500	4713500	where the money is actually going and if someone wants to be
4713500	4715500	a private contractor
4715500	4717500	they have to agree that the accounting system
4717500	4719500	if they want government money
4719500	4721500	goes on the blockchain so we can see the entire provenance
4721500	4723500	of the taxpayer money so that
4723500	4725500	you can't have representation if there isn't
4725500	4727500	transparency of how it happens
4727500	4729500	so there's a whole
4729500	4731500	bunch of when you start to think about
4731500	4733500	attention directing technology
4733500	4735500	and what its pedagogical applications could be
4735500	4737500	when you start to think about
4737500	4739500	AI and how it could actually help
4739500	4741500	proposition development
4741500	4743500	and parsing huge amounts of information to make a better
4743500	4745500	epistemic commons when you start to think about
4745500	4747500	blockchain and could we actually resolve corruption
4747500	4749500	using uncorruptible ledgers and making
4749500	4751500	the provenance of physical supply chains
4751500	4753500	and information and money all flow
4753500	4755500	across those, totally new possibilities
4755500	4757500	start to emerge
4757500	4759500	that never emerged before, that were never possible
4759500	4761500	before
4761500	4763500	but if it doesn't become our central design imperative
4763500	4765500	to develop those, those are not
4765500	4767500	the highest marketed
4767500	4769500	opportunities for those right now
4769500	4771500	the highest market opportunity for blockchain is speculative
4771500	4773500	tokens that have no real utility
4773500	4775500	and for AI is things that actually
4775500	4777500	drive ads and purchasing
4777500	4779500	and you know, on and on
4779500	4781500	and for attention tech it is the same thing
4783500	4785500	So you've sold me on the idea that we
4785500	4787500	have two dystopian attractors
4787500	4789500	that we don't want and the third
4789500	4791500	attractor that we're trying to develop
4791500	4793500	here is some kind of open society
4793500	4795500	that is consciously using
4795500	4797500	all the modern technologies
4797500	4799500	towards the values that we care about
4799500	4801500	can you give some concrete examples
4801500	4803500	of what it would look like to
4803500	4805500	use you know, AI and attention
4805500	4807500	driving tech and click driving tech
4807500	4809500	and block chains and all these things
4809500	4811500	but in a way that would make a stronger, healthier open society
4811500	4813500	Yeah, totally
4817500	4819500	So let's say we take
4819500	4821500	the attention
4821500	4823500	tech that you've looked at so much
4823500	4825500	that when it is applied
4825500	4827500	for a commercial application
4827500	4829500	it is seeking to gather data
4829500	4831500	to both maximize time on site
4831500	4833500	and maximize engagement with certain kinds of ads
4833500	4835500	and whatever
4835500	4837500	That's obviously
4837500	4839500	the ability to
4839500	4841500	direct human behavior
4841500	4843500	and direct human feeling
4843500	4845500	and thought
4845500	4847500	In a way
4847500	4849500	that has both emerged out of capitalism
4849500	4851500	and has become
4851500	4853500	almost a new macroeconomic structure
4853500	4855500	more powerful than capitalism
4855500	4857500	more powerful than being able to
4857500	4859500	incent people's behavior with money
4859500	4861500	as being able to direct what they think and feel
4861500	4863500	to where the thing that they think of
4863500	4865500	as their own intrinsic motive
4865500	4867500	has actually been influenced or captured
4867500	4869500	So
4869500	4871500	if we
4871500	4873500	if we wanted to
4873500	4875500	apply that type of technology
4875500	4877500	and we figured out how to make
4877500	4879500	the kind of transparency
4879500	4881500	that made institutions
4881500	4883500	that were trustworthy enough
4883500	4885500	and already we have institutions that have it
4885500	4887500	that we have no basis to trust with it
4887500	4889500	could that same
4889500	4891500	tech be used educationally
4891500	4893500	to be able to personalize education
4893500	4895500	to the learning style
4895500	4897500	of a kid or to an adult to their particular
4897500	4899500	areas of interest
4899500	4901500	and to be able to
4901500	4903500	not
4903500	4905500	use
4905500	4907500	the ability to control them
4907500	4909500	for game-theoretic purposes
4909500	4911500	but use the ability to influence them
4911500	4913500	to even help them learn
4913500	4915500	what makes their own
4915500	4917500	center
4917500	4919500	their locus of action more internalized
4919500	4921500	we could teach people
4921500	4923500	with that kind of tech
4923500	4925500	how to notice their own bias
4925500	4927500	how to notice their own emotional behaviors
4927500	4929500	how to notice groupthink type dynamics
4929500	4931500	how to understand propaganda and media literacy
4931500	4933500	so could we actually use those tools
4933500	4935500	to increase people's immune system
4935500	4937500	against bad actors' use of those tools
4937500	4939500	totally
4939500	4941500	could we use them pedagogically
4941500	4943500	in general to be able to identify
4943500	4945500	rather than manufacturing
4945500	4947500	desires in people
4947500	4949500	or appealing to the lowest angels of their nature
4949500	4951500	because addiction is profitable
4951500	4953500	can you appeal to the highest
4953500	4955500	angels in people's nature
4955500	4957500	but that are aligned with intrinsic incentives
4957500	4959500	and be able to
4959500	4961500	create customized educational programs
4961500	4963500	that are based on what each person is actually
4963500	4965500	innately intrinsically motivated by
4965500	4967500	but that are their higher innate motivators
4967500	4969500	everybody can have a reward circuit
4969500	4971500	that is based on
4971500	4973500	you know chocolate cake and sloth
4973500	4975500	but the immediate spike
4975500	4977500	that comes from the chocolate cake
4977500	4979500	ends up then having a crash
4979500	4981500	and increased weight and inflammation
4981500	4983500	and whatever where the baseline of their happiness
4983500	4985500	goes down over time
4985500	4987500	even though every time they eat the chocolate cake they get a spike
4987500	4989500	the exercise reward circuit is maybe
4989500	4991500	not that fun maybe even kind of painful
4991500	4993500	and dreadful in the moment
4993500	4995500	but then creates a higher baseline
4995500	4997500	of energy and capacity
4997500	4999500	and endurance and self-esteem
4999500	5001500	and you start to actually have the process become
5001500	5003500	more fun you get a new reward circuit
5003500	5005500	and the baseline goes up
5005500	5007500	so of course I can appeal to the lower reward
5007500	5009500	circuit and say hey I'm just giving people
5009500	5011500	what they want
5011500	5013500	but if you have a billion dollar
5013500	5015500	or a trillion dollar organization
5015500	5017500	that is preying upon them
5017500	5019500	and you discuss this very well all the time
5019500	5021500	the vulnerabilities that make people's life
5021500	5023500	worse to then have the plausible deniability
5023500	5025500	to say yeah but they wanted it
5025500	5027500	but it was a manufactured demand
5027500	5029500	and a vulnerability where's the no bless
5029500	5031500	oblige where's the obligation
5031500	5033500	of having that much power to actually be
5033500	5035500	a good steward of power a steward of that
5035500	5037500	for other people where if there are
5037500	5039500	reward circuits that decrease the quality
5039500	5041500	of their life reward circuits that increase
5041500	5043500	that we're trying to appeal to one rather than the other
5043500	5045500	could we do that yeah totally we could
5045500	5047500	could we have an education system as a result
5047500	5049500	that was identifying innate
5049500	5051500	aptitudes innate interests
5051500	5053500	of everyone and facilitating
5053500	5055500	their development so not only did they
5055500	5057500	become
5057500	5059500	good at something but they became increasingly
5059500	5061500	more intrinsically motivated fascinated
5061500	5063500	and passionate by life
5063500	5065500	which also meant continuously better at the thing
5065500	5067500	well in a world of
5067500	5069500	increasing technological automation
5069500	5071500	coming up both robotic and AI automation
5071500	5073500	where so many of the jobs are about to be
5073500	5075500	obsolete
5075500	5077500	our economy
5077500	5079500	and our education system
5079500	5081500	have to radically change to deal with that
5081500	5083500	because
5083500	5085500	the core of like one of the core
5085500	5087500	things an economy has been trying to do forever
5087500	5089500	was
5089500	5091500	deal with the need that a society had
5091500	5093500	for a labor force
5093500	5095500	and that there were these jobs that society
5095500	5097500	needed to get done that nobody would really want to do
5097500	5099500	so either the state has to force them to do it
5099500	5101500	or
5101500	5103500	you have to make it to where the people also need the job
5103500	5105500	so there's a cemetery and so kind of the market
5105500	5107500	forces them to do it well when you technologically
5107500	5109500	automate those jobs and it happens
5109500	5111500	to be that the things that are the most
5111500	5113500	wrote are the least fun for people
5113500	5115500	and the easiest to program machines to do
5115500	5117500	and
5117500	5119500	so
5119500	5121500	if you keep the same economy
5121500	5123500	where if people don't produce
5123500	5125500	they don't have any basic needs met
5125500	5127500	then people want
5127500	5129500	those crappy jobs right
5129500	5131500	but if you make it to where they have other
5131500	5133500	opportunities then of course
5133500	5135500	having those jobs be automated is fine
5135500	5137500	but what does it mean to really be able to have
5137500	5139500	other better opportunities
5139500	5141500	so
5141500	5143500	if one of the fundamental like
5143500	5145500	axioms
5145500	5147500	of all of our economic theories
5147500	5149500	is that we need to figure out how to
5149500	5151500	incent a labor force to do things that nobody
5151500	5153500	wants to do
5153500	5155500	and emerging technological automation starts to
5155500	5157500	debase that
5157500	5159500	that means we have to rethink economics from scratch
5159500	5161500	because we don't have to do that thing anymore
5161500	5163500	so maybe if now the jobs don't need the people
5163500	5165500	that need the jobs can we start to create
5165500	5167500	commonwealth resources that everyone has access to
5167500	5169500	where people's access
5169500	5171500	isn't based on possession
5171500	5173500	that automatically limits everyone else's access
5173500	5175500	if you get around
5175500	5177500	transportation wise with a car
5177500	5179500	based on owning that car
5179500	5181500	where the vast majority of the life of the car
5181500	5183500	it's just sitting not being used
5183500	5185500	for you to have access to the car you have to have
5185500	5187500	possession of it which means that it's a
5187500	5189500	mostly underutilized asset I don't have
5189500	5191500	access to the thing that you possess
5191500	5193500	now what we see with Uber of course is a situation
5193500	5195500	where your access
5195500	5197500	is not mediated by your possession
5197500	5199500	so now turn that into electric
5199500	5201500	self-driving cars and now make the entire
5201500	5203500	thing on a blockchain so you disintermediate
5203500	5205500	even the central business make it a commonwealth
5205500	5207500	resource and everyone has access to
5207500	5209500	transportation as a commonwealth resource
5209500	5211500	it'll take a 20th of the number of cars
5211500	5213500	to meet the same level
5213500	5215500	of convenience during peak demand time
5215500	5217500	so much less environmental harm
5217500	5219500	it'll actually be more convenient because I don't
5219500	5221500	have to be engaged in driving the thing
5221500	5223500	and there's less traffic because of the coordination
5223500	5225500	and better maintenance and there isn't a desire
5225500	5227500	for an incentive for designed
5227500	5229500	an obsolescence in that system
5229500	5231500	you can see a situation where
5231500	5233500	can we make it to where
5233500	5235500	the wealth augmenting capacity
5235500	5237500	of that technologic automation goes back
5237500	5239500	into a commonwealth because we don't have to
5239500	5241500	have the same axioms of needing to incend the people
5241500	5243500	oh yeah but if you don't incend the people
5243500	5245500	they'll all be lazy welfare people
5245500	5247500	nonsense
5247500	5249500	Einstein didn't do what he did based on
5249500	5251500	economic incentive and
5251500	5253500	neither did Mozart and neither did Gandhi
5253500	5255500	and none of the people that we
5255500	5257500	are most inspired by through history
5257500	5259500	were doing that
5259500	5261500	and what kids will
5261500	5263500	who will spend so much
5263500	5265500	time doing where they ask questions
5265500	5267500	about why this why this why this
5267500	5269500	and building forts and whatever is intrinsic motive
5269500	5271500	it's just we don't facilitate
5271500	5273500	the things that they're interested in
5273500	5275500	we try to force them to be interested in things
5275500	5277500	that's what ends up breaking their interest in life
5277500	5279500	and then they just want a hypernormal stimuli
5279500	5281500	and play video games whatever
5281500	5283500	what if you had a system that was facilitating
5283500	5285500	their interest the entire time
5285500	5287500	now you have a situation where you can start to
5287500	5289500	decrease the total amount of extrinsic
5289500	5291500	incentive in the system as a whole
5291500	5293500	use the technology
5293500	5295500	to the automation to decrease
5295500	5297500	the need for extrinsic incentive
5297500	5299500	and make an educational system
5299500	5301500	and culture that's about optimizing intrinsic
5301500	5303500	incentive because if my needs are already
5303500	5305500	met getting stuff there's no
5305500	5307500	and everybody's needs are met through access
5307500	5309500	to commonwealth resources there's no real status
5309500	5311500	conferred that there's only status
5311500	5313500	conferred by what I create so now there is a
5313500	5315500	any status is bound to a kind of creative
5315500	5317500	imperative that's an example
5317500	5319500	we can look at blockchain
5319500	5321500	tech even more near term and say
5321500	5323500	but
5323500	5325500	but just to come back to this technological
5325500	5327500	automation thing so obviously it makes possible
5327500	5329500	changing economics and changing education
5329500	5331500	but also
5331500	5333500	what is the role of humans
5333500	5335500	in a
5335500	5337500	post AI robotic automation world
5337500	5339500	because that is coming very very soon
5339500	5341500	and what is the future of
5341500	5343500	education where you don't have to prepare
5343500	5345500	people to be
5345500	5347500	things that you can just program computers
5347500	5349500	to be
5349500	5351500	well the role of education has to be based on
5351500	5353500	what is the role of people in that world
5353500	5355500	that is such a deep redesign of civilization
5355500	5357500	because the tech is changing
5357500	5359500	the possibility set that deeply
5359500	5361500	so at the heart of this are kind of
5361500	5363500	deep existential questions of what is a meaningful
5363500	5365500	human life and then what is a good civilization
5365500	5367500	that increases the possibility
5367500	5369500	space of that for everybody and how do we design
5369500	5371500	that thing we come back to
5371500	5373500	blockchain and we say
5373500	5375500	well blockchain is an uncorruptible ledger
5375500	5377500	well
5377500	5379500	one thing that the left and right and everybody
5379500	5381500	agrees on is that we did corruption
5381500	5383500	happens and it's bad for the societies
5383500	5385500	at home we don't like it we just disagree
5385500	5387500	on who does it
5387500	5389500	is it possible
5389500	5391500	that that tech could
5391500	5393500	make possible decreasing
5393500	5395500	corruption as a whole
5395500	5397500	it actually decreases the possibility set for corruption
5397500	5399500	yeah in order to do corruption
5399500	5401500	I have to be able to hide that I did it
5401500	5403500	right I either have to
5403500	5405500	to break enforcement or break accounting and mostly
5405500	5407500	it's break accounting
5407500	5409500	and so what if
5409500	5411500	all government spending was on a blockchain
5411500	5413500	and doesn't have to be a blockchain
5413500	5415500	it has to be an uncorruptible ledger of some kind
5415500	5417500	all a chain is a good example
5417500	5419500	that is pioneering another way of doing it
5419500	5421500	but uncorruptible ledger of some kind
5421500	5423500	where you actually see
5423500	5425500	where all taxpayer money goes and you see how
5425500	5427500	it's utilized the entire thing and have independent
5427500	5429500	auditing agencies and the public can transparently
5429500	5431500	be engaged in the auditing of it
5431500	5433500	and if the government
5433500	5435500	is going to privately contract a corporation
5435500	5437500	the corporation agrees
5437500	5439500	that if they want that government money
5439500	5441500	the blockchain accounting has to extend
5441500	5443500	into the corporation so there can't be
5443500	5445500	very very
5445500	5447500	bloated corruption everybody got to see
5447500	5449500	that when Elon made
5449500	5451500	SpaceX all of a sudden he was making
5451500	5453500	rockets for like a hundreds to a thousands
5453500	5455500	of the price that Lockheed or Boeing were
5455500	5457500	who had just had these almost monopolistic government
5457500	5459500	contracts for a long time
5459500	5461500	well if the taxpayer money
5461500	5463500	is going to the government
5463500	5465500	is going to an external private contractor
5465500	5467500	who's making the things for a hundred to a thousand
5467500	5469500	times more than it costs
5469500	5471500	we get this false dichotomy sold to us that
5471500	5473500	either
5473500	5475500	we have to pay more taxes
5475500	5477500	to have better national security
5477500	5479500	or if we want to cut taxes
5479500	5481500	we're going to have less national security
5481500	5483500	what about just having less gruesome
5483500	5485500	bloat because you have better accounting
5485500	5487500	and we make the rockets for a hundredth
5487500	5489500	of the price and we have better national
5489500	5491500	security and better social services and less
5491500	5493500	taxes well that's
5493500	5495500	everyone would vote for that right
5495500	5497500	who wouldn't vote for that thing well that wasn't
5497500	5499500	possible before uncorruptible ledgers
5499500	5501500	also means you can have provenance
5501500	5503500	on supply chains to make the supply
5503500	5505500	chains closed loop so that you can see
5505500	5507500	that all the new stuff is being made from old stuff
5507500	5509500	and you can see where all the pollution is going and you can see
5509500	5511500	who did it which means you can now internalize
5511500	5513500	the externalities rigorously
5513500	5515500	and nobody can destroy those
5515500	5517500	emails or burn those files right
5517500	5519500	what if
5519500	5521500	the changes
5521500	5523500	in law and
5523500	5525500	the
5525500	5527500	decision-making processes
5527500	5529500	also followed a
5529500	5531500	blockchain process where there was a provenance
5531500	5533500	on the input of information well that would also
5533500	5535500	be a very meaningful thing to be able to
5535500	5537500	follow so
5537500	5539500	this is an example of like can we actually
5539500	5541500	structurally remove
5541500	5543500	the capacity for corruption
5543500	5545500	by technology that makes
5545500	5547500	corruption much much much harder that forces
5547500	5549500	types of transparency on auditability
5549500	5551500	what if also
5551500	5553500	you're able to record history
5553500	5555500	you're able to record the events that are occurring
5555500	5557500	in a blockchain that's uncruptible where you can't change history later
5557500	5559500	so you actually get
5559500	5561500	the possibility of real justice and real history
5561500	5563500	and multiple different simultaneous timelines
5563500	5565500	that are happening that's humongous
5565500	5567500	in terms of what it does
5567500	5569500	what if you can
5569500	5571500	have an open data platform
5571500	5573500	and an open science platform where
5573500	5575500	someone doesn't get to cherry pick which data
5575500	5577500	they include in their peer reviewed paper later
5577500	5579500	we get to see all of the data that was happening
5579500	5581500	we solve the oracle issues that are associated
5581500	5583500	and then if we find out that a particular
5583500	5585500	piece of science was wrong later we can see
5585500	5587500	downstream everything that used
5587500	5589500	that output as an input and automatically
5589500	5591500	flag what things need to change
5591500	5593500	that's so
5593500	5595500	powerful like the least
5595500	5597500	interesting example
5597500	5599500	of blockchain is currency creation
5599500	5601500	these are actually
5601500	5603500	like
5603500	5605500	the capacity for the right types of
5605500	5607500	accounting means the right type of choice making
5607500	5609500	right let's take AI
5609500	5611500	with AI we can make
5611500	5613500	super terrible deep fakes and destroy
5613500	5615500	the epistemic commons you know using that
5615500	5617500	and other things like that
5617500	5619500	but we can see
5619500	5621500	the way that the AI makes the deep fake by
5621500	5623500	being able to take enough different images of
5623500	5625500	the person's face and movements that it can generate
5625500	5627500	new ones we can see where it can generate totally
5627500	5629500	new faces averaging faces together somebody
5629500	5631500	sent me some new work that they were just doing on this
5631500	5633500	the other day I found very interesting they said
5633500	5635500	we're going to take a very similar type of tech and apply
5635500	5637500	it to semantic fields where
5637500	5639500	we can take everybody's sentiment on a topic
5639500	5641500	and actually generate
5641500	5643500	a proposition that is at the semantic
5643500	5645500	center or take everybody's
5647500	5649500	sentiment and abstract from it the
5649500	5651500	values that they care about and create
5651500	5653500	values taxonomies and say
5653500	5655500	we should come up with a proposition that meets
5655500	5657500	all these values then
5657500	5659500	can you have digital processes where you can't fit
5659500	5661500	everybody into
5661500	5663500	into a
5663500	5665500	town hall but everybody who wants
5665500	5667500	to can participate in a digital space
5667500	5669500	that rather than vote
5669500	5671500	yes or no on a proposition that was made by a
5671500	5673500	special interest group where we didn't have a say
5673500	5675500	in the proposition or even the values it was seeking
5675500	5677500	to serve so it was made in a very
5677500	5679500	narrow way that like we mentioned earlier
5679500	5681500	benefits one thing and harms something else which is
5681500	5683500	why
5683500	5685500	almost every proposition gets about half of the
5685500	5687500	vote and inherently polarizes the
5687500	5689500	population well people are so
5689500	5691500	dumb and so rival risk the process of voting
5691500	5693500	with
5693500	5695500	bad propositions and
5695500	5697500	and bad representation
5697500	5699500	process is inherently polarizing
5699500	5701500	and downgrading to people so
5701500	5703500	what if there's a process by which there's
5703500	5705500	a decision that wants to be made you start
5705500	5707500	by identifying what are the values everybody cares
5707500	5709500	about and then we say the
5709500	5711500	first proposition that meets all these
5711500	5713500	values well becomes the
5713500	5715500	thing that we vote on and then instead of just
5715500	5717500	a direct vote do we engage
5717500	5719500	types of qualified and liquid democracy
5719500	5721500	together where you have to show that you understand
5721500	5723500	the basics of
5723500	5725500	that topic to be able to vote on it
5725500	5727500	but the education is free and you can keep retesting
5727500	5729500	and the basics don't show leaning one way or the
5729500	5731500	other just shows you understand the stated pros and
5731500	5733500	cons so that massive populism
5733500	5735500	doesn't happen but if
5735500	5737500	you don't want to come to understand it you can seed
5737500	5739500	your vote to someone else who has passed that thing
5739500	5741500	these are that type of liquid
5741500	5743500	democracy that type of qualified
5743500	5745500	educated democracy where it doesn't have to
5745500	5747500	be educated across everything it can be per
5747500	5749500	issue and where you're not just voting
5749500	5751500	on a thing you're helping craft the propositions
5751500	5753500	these completely change
5753500	5755500	the possibility space of social technology
5755500	5757500	and we can go on and on in terms of
5757500	5759500	examples but these are ways
5759500	5761500	that the same type
5761500	5763500	of new emergent physical tech
5763500	5765500	that can destroy the epistemic
5765500	5767500	commons and create autocracies and create catastrophic
5767500	5769500	risks could also be used
5769500	5771500	to realize a much
5771500	5773500	more
5773500	5775500	pro-topic world
5775500	5777500	so I love so many of those
5777500	5779500	examples and I especially on
5779500	5781500	the blockchain and corruption one because
5781500	5783500	I think as you said something
5783500	5785500	the left and the right can both agree on is that
5785500	5787500	our systems are not really functional and there's
5787500	5789500	definitely corruption and defection going on
5789500	5791500	and just to add to your example
5791500	5793500	imagine if citizens could even earn money
5793500	5795500	by spotting inefficiencies or corruption
5795500	5797500	in that transparent ledger
5797500	5799500	so that we actually have a system that is
5799500	5801500	actually profiting
5801500	5803500	by getting more and more efficient over time
5803500	5805500	and actually better serving the needs of the people
5805500	5807500	and having less and less corruption and so there's
5807500	5809500	definitely more trust and faith and that's actually
5809500	5811500	a kind of digital society
5811500	5813500	that when you look at let's say the closed
5813500	5815500	China's digital authoritarian society
5815500	5817500	and you look at this open one that's actually operating
5817500	5819500	more for the people with more transparency
5819500	5821500	with more efficiencies you get more
5821500	5823500	SpaceX Elon Musk type cheap
5823500	5825500	ways of sending rockets to the moon and becoming a multi-planetary
5825500	5827500	civilization as opposed to
5827500	5829500	more bloat and more mega monopolies
5829500	5831500	defense contractors that are not taking us
5831500	5833500	to where we need to go
5833500	5835500	that's just an inspiring vision and I just hope
5835500	5837500	you share it and kind of go back because there's a lot
5837500	5839500	of different aspects there
5839500	5841500	I think the question on many people's mind right now
5841500	5843500	is going to be
5843500	5845500	how do we get from where we are
5845500	5847500	to the world that you are talking about
5847500	5849500	what are the steps that are in between
5849500	5851500	obviously I don't know
5851500	5853500	nobody knows
5853500	5855500	there's gonna like
5855500	5857500	which projects emerge
5857500	5859500	and
5859500	5861500	first and start really making success
5861500	5863500	that there's a lot of different
5863500	5865500	possible paths
5865500	5867500	I can say some of the things
5867500	5869500	that could happen and some of the things
5869500	5871500	that I think need to happen
5873500	5875500	so we take all the catastrophic
5875500	5877500	risks that exponential tech
5877500	5879500	makes possible and the dystopic attractors
5879500	5881500	and we say okay so we need to solve all those
5881500	5883500	problems but we're not doing
5883500	5885500	really good at solving those problems right now
5885500	5887500	so our problem solving processes need upgraded
5887500	5889500	and that means new
5889500	5891500	institutions
5891500	5893500	and when we say
5893500	5895500	institution we usually think of a pretty centralized
5895500	5897500	thing and with things like decentralized
5897500	5899500	governance emerging
5899500	5901500	the institution might be a decentralized one
5901500	5903500	but it's individual
5903500	5905500	people aren't going to solve all of that right
5905500	5907500	so it's new institution
5907500	5909500	centralized and decentralized that have the right
5909500	5911500	capacities to solve these types of problems
5911500	5913500	need to come about
5913500	5915500	alright well who develops those
5915500	5917500	institutions and who empowers them
5917500	5919500	and this is where
5919500	5921500	the democratic idea of
5921500	5923500	the
5923500	5925500	power of government coming from the
5925500	5927500	consent of the governed
5927500	5929500	is one of the key ideas to what
5929500	5931500	we would think of as the values of an open
5931500	5933500	society let's say that there's a small
5933500	5935500	number of people who think we understand these
5935500	5937500	problems we understand the solutions that must happen
5937500	5939500	everybody else doesn't get it so we're going to make this
5939500	5941500	thing happen and because we have the power we can
5941500	5943500	just kind of implement it by force and so
5943500	5945500	that becomes
5945500	5947500	its own dystopia right
5947500	5949500	and implement it by force might be well the people think
5949500	5951500	they need to be free so we'll implement it
5951500	5953500	by attention hijacking them so that they
5953500	5955500	participate with it or
5955500	5957500	don't even realize that it's happening and they just
5957500	5959500	keep doing whatever is next
5961500	5963500	the cultural element why we talk
5963500	5965500	about the need for a new cultural enlightenment
5965500	5967500	is
5967500	5969500	of course when we look at like the founding
5969500	5971500	of the US we can see all that was
5973500	5975500	super wrong with it right
5975500	5977500	just to mention like how when Churchill said
5977500	5979500	democracy is the worst form of government ever
5979500	5981500	saved for all the other forms
5981500	5983500	there's when when Socrates
5983500	5985500	talked about in
5985500	5987500	in the republic when Plato was
5987500	5989500	discussing it why
5989500	5991500	democracy was a dreadful idea
5991500	5993500	the arguments are good arguments right like
5993500	5995500	do you want
5995500	5997500	people who understand
5997500	5999500	seafaring to man the boat or just a general
5999500	6001500	population who knows nothing about it to man the boat
6001500	6003500	well that's not a very good idea do you want the general
6003500	6005500	population that knows nothing about it to build the NASA
6005500	6007500	rocket or do you want people that know what they're doing
6007500	6009500	well why would we think people who have no idea what
6009500	6011500	they're doing are going to be good at figuring out
6011500	6013500	how a civilization should be run what should
6013500	6015500	our nuclear first strike policy should be how should
6015500	6017500	we deal with the stability of the energy grid
6017500	6019500	against Carrington events
6019500	6021500	and so what does it take
6021500	6023500	to have a population educated enough
6023500	6025500	and yet then if we say okay
6025500	6027500	but then the other
6027500	6029500	problem is if we say the people are
6029500	6031500	too uneducated and maybe
6031500	6033500	too irrational and rival risk to be able
6033500	6035500	to hold that power so it needs to be held by
6035500	6037500	some how do we ensure non-corruption and
6037500	6039500	who is a trust
6039500	6041500	worthy authority to be
6041500	6043500	able to hold that power and not have vested
6043500	6045500	interest mess it up and so this is why I
6045500	6047500	think it was a Jefferson quote of
6047500	6049500	the ultimate
6049500	6051500	depository of the power must be the people and if
6051500	6053500	we think the people too uneducated not enlightened
6053500	6055500	to be able to hold that power
6055500	6057500	we must do everything we can to seek to
6057500	6059500	educate and lighten them not think that there
6059500	6061500	is any other safe depository
6061500	6063500	and so
6063500	6065500	even with that
6065500	6067500	we take the
6067500	6069500	US formation and
6069500	6071500	you've got some founders
6071500	6073500	who
6073500	6075500	had read most of the books of the time
6075500	6077500	read most of the books of philosophy knew the history
6077500	6079500	of the Magna Carta and the Treaty of the Forest
6079500	6081500	and all these kinds of things thought and talked deeply
6081500	6083500	spent many years were willing to die
6083500	6085500	fighting a revolutionary war were not
6085500	6087500	going along with winning at the current system
6087500	6089500	but really trying to do a fundamentally different thing
6089500	6091500	to develop a new system
6091500	6093500	not everybody who was participating in the US
6093500	6095500	was doing that thing they weren't all doing systems
6095500	6097500	architecture right
6097500	6099500	but they were all basically
6099500	6101500	saying
6101500	6103500	we agree to this kind of systems
6103500	6105500	architecture and we want to
6105500	6107500	learn how to participate with it adequately
6107500	6109500	will read a newspaper we will do a
6109500	6111500	jury duty will come to the town hall that kind of
6111500	6113500	thing so
6113500	6115500	um
6115500	6117500	in Taiwan's example I think their population
6117500	6119500	is 23 million people in their
6119500	6121500	online citizen engagement platform has something
6121500	6123500	like 5 million people engaging
6123500	6125500	that's pretty awesome right that's not everybody
6125500	6127500	and no one should be forced to be engaging
6129500	6131500	and one of the critical things
6131500	6133500	when we think deeper about is it a democracy
6133500	6135500	is it a republic
6135500	6137500	is it a epistocracy is it a
6137500	6139500	um we want to think about
6139500	6141500	the values not the previous
6141500	6143500	frames for them and the values exist
6143500	6145500	in dialectics and we need to be able to hold those
6145500	6147500	together of course we want individual liberty
6147500	6149500	but we don't want individual liberty that gets to harm
6149500	6151500	other people and other things so we want
6151500	6153500	also you know law justice collective
6153500	6155500	integrity how do you relate those things
6155500	6157500	one of the core things is the relationship between
6157500	6159500	rights and responsibilities so
6159500	6161500	there
6161500	6163500	if I have rights and I don't have
6163500	6165500	responsibilities there ends up
6165500	6167500	being like tyranny and entitlement
6167500	6169500	if I and we can see
6169500	6171500	that that's kind of rampant the entitlement
6171500	6173500	thing if I have responsibilities
6173500	6175500	and I don't have any attendant rights
6175500	6177500	at servitude neither of those
6177500	6179500	involve a healthy just society so
6179500	6181500	if I want the right
6181500	6183500	to drive a car the responsibility
6183500	6185500	to do the driver's education and actually
6185500	6187500	learn how to drive a car safely is important and we
6187500	6189500	can see that some countries have less car
6189500	6191500	accidents than others associated with better
6191500	6193500	drivers education um and so
6193500	6195500	increasing the responsibilities a good
6195500	6197500	thing we can see that some countries
6197500	6199500	have way less gun violence than others
6199500	6201500	even factoring a similar per capita
6201500	6203500	amount of guns based on more training
6203500	6205500	associated with guns and mental health
6205500	6207500	and things like that
6207500	6209500	so if I have a right to bear arms
6209500	6211500	do I also have a responsibility
6211500	6213500	to be part of a well organized militia
6213500	6215500	train with them and be willing to actually sacrifice
6215500	6217500	myself to protect the whole or
6217500	6219500	sign up for a thing to do that to have to be a
6219500	6221500	reservist of some kind those are the right
6221500	6223500	responsibility if I want the right to vote
6223500	6225500	is there a responsibility to be educated
6225500	6227500	about the issue
6227500	6229500	yes yes now
6229500	6231500	does that make it very unequal no
6231500	6233500	because the capacity
6233500	6235500	to get educated has to be something that the
6235500	6237500	society invests in making possible for
6237500	6239500	everyone and of course we would all be
6239500	6241500	silly to not
6241500	6243500	be dubious factoring the previous
6243500	6245500	history of these things but this is what we then
6245500	6247500	have to insist upon because do we want
6247500	6249500	people who really
6249500	6251500	don't understand the issues but think they do
6251500	6253500	voting now that's a dreadful system
6253500	6255500	but do we want people who know
6255500	6257500	something to have no avenue or who care
6257500	6259500	do we want people who know something to have
6259500	6261500	no avenue to input that into the system or
6261500	6263500	people who care to have no opportunity
6263500	6265500	to learn no that's also dreadful
6265500	6267500	so how do we make the on-ramps to learning
6267500	6269500	available for everyone not enforced
6269500	6271500	but we're actually incentivizing
6271500	6273500	can we use those same kind of social media
6273500	6275500	behavior and sending technologies to increase
6275500	6277500	everyone's desire for more rights
6277500	6279500	and attendant responsibilities
6279500	6281500	so that there's actually a gradient
6281500	6283500	of civic virtue and civic engagement
6283500	6285500	yeah we could totally do that
6287500	6289500	so this is where the cultural enlightenment
6289500	6291500	layer is of course not everyone is going to
6291500	6293500	be working on how do we develop AI
6293500	6295500	and blockchain for these purposes
6295500	6297500	but they can certainly be saying I am going
6297500	6299500	to make sure that my representatives are talking
6299500	6301500	about these issues I want all the
6301500	6303500	presidential candidates to be talking about these issues
6303500	6305500	I'm going to pay attention to and support
6305500	6307500	candidates who really do in earnest ways
6307500	6309500	I'm going to invest in
6309500	6311500	companies that are doing those things I'm going to
6311500	6313500	invest from companies that are doing the other things
6313500	6315500	there is a cultural
6315500	6317500	enlightenment that is needed
6317500	6319500	to be able to create the
6319500	6321500	demand and the support for
6321500	6323500	where those projects
6323500	6325500	that are earnestly working on and have
6325500	6327500	the capability start to emerge
6327500	6329500	so you've painted
6329500	6331500	a compelling vision of
6331500	6333500	some of the ways that a open
6333500	6335500	society could consciously employ
6335500	6337500	some of these technologies to
6337500	6339500	revisit and
6339500	6341500	re-fulfill some of the original
6341500	6343500	values for which they were intended
6343500	6345500	how much of this
6345500	6347500	how does this work with the
6347500	6349500	existing institutions that we have how much is
6349500	6351500	this going to rely on
6351500	6353500	transforming the existing
6353500	6355500	digital Leviathans into something new
6355500	6357500	how much is going to depend on blockchain projects
6357500	6359500	how much is going to depend on
6359500	6361500	existing institutions would be the Brookings
6361500	6363500	institution or
6363500	6365500	the New York Times can you speak
6365500	6367500	to the role of new
6367500	6369500	and future institutions in making this transition
6369500	6371500	possible
6373500	6375500	yeah
6375500	6377500	it's interesting
6377500	6379500	when we look at institutions that emerged
6379500	6381500	to try to solve some social
6381500	6383500	or environmental problems or nonprofits
6383500	6385500	in particular and some government branches
6385500	6387500	that are associated
6387500	6389500	with that there's this kind of structural
6389500	6391500	perverse incentive
6391500	6393500	that if I
6393500	6395500	am an organization which means
6395500	6397500	I'm people in an organization
6397500	6399500	that
6399500	6401500	have some that have job security
6401500	6403500	and some actual power and access and whatever
6403500	6405500	because of this position
6405500	6407500	and my job is to solve a problem
6407500	6409500	if I fully solve the problem I would obsolete my job
6409500	6411500	and obsolete myself so then there's this kind of
6411500	6413500	perverse incentive to continue managing
6413500	6415500	the problem continue manufacturing the narrative
6415500	6417500	that we're needed to manage the problem continue
6417500	6419500	manufacturing the narrative that the problem
6419500	6421500	is really hard and is hard to solve and so we got
6421500	6423500	to keep doing this thing
6423500	6425500	and so one of the fundamental
6425500	6427500	dispositions of systems is that
6427500	6429500	they want to keep existing and
6429500	6431500	so and yet they might
6431500	6433500	no longer be fit for purpose they might even be
6433500	6435500	antithetical to the purpose we have to be very
6435500	6437500	careful about this
6437500	6439500	with regard to
6439500	6441500	the new institutions we need to what degree could
6441500	6443500	existing institutions reform
6443500	6445500	themselves to what degree does it need to be new ones
6445500	6447500	it's kind of up to them like it's kind of
6447500	6449500	up to the the depth of
6449500	6451500	realization of the need and the
6451500	6453500	sincerity and then the coordination capacity of
6453500	6455500	people in current institutions
6455500	6457500	how much role they could play we can
6457500	6459500	see the way that going into World
6459500	6461500	War II coming out of the depression the U.S.
6461500	6463500	up-regulated its
6463500	6465500	coordination capacity so profoundly
6465500	6467500	so could we
6467500	6469500	have a Manhattan project like level
6469500	6471500	organization
6471500	6473500	by organization
6473500	6475500	I mean
6475500	6477500	the
6477500	6479500	capacity to organize not a
6479500	6481500	singular thing
6481500	6483500	that was oriented
6483500	6485500	to how do we
6485500	6487500	instantiate the next model
6487500	6489500	of civilization how do we instantiate the next model
6489500	6491500	of social systems and social technologies
6491500	6493500	what is the future of education what's the future of economics
6493500	6495500	what's the future of
6495500	6497500	the fourth estate of law etc
6497500	6499500	that are
6499500	6501500	that fulfill the values
6501500	6503500	that are meaningful and are antifragile
6503500	6505500	in the presence of the current technologies
6505500	6507500	and they can actually compete with the other applications
6507500	6509500	of those technologies towards
6509500	6511500	things that serve different values
6511500	6513500	and or aren't antifragile
6513500	6515500	I would
6515500	6517500	love to see the U.S.
6517500	6519500	make that a
6519500	6521500	central imperative Manhattan project level
6521500	6523500	to be able to do that not just how do we create a more
6523500	6525500	powerful military but how do we create
6525500	6527500	a more powerful a
6527500	6529500	healthier fundamentally a healthier
6529500	6531500	society that up regulates
6531500	6533500	and engages collective intelligence
6533500	6535500	in its own problem solving and innovation better
6535500	6537500	I would like to see lots
6537500	6539500	of countries do that I think there are countries
6539500	6541500	that did
6541500	6543500	not yet transition to democracy
6543500	6545500	are interested in it and completely bypass
6545500	6547500	the industrial era democracies
6547500	6549500	and go directly to better systems
6549500	6551500	I think networks
6551500	6553500	of small countries you see what Taiwan
6553500	6555500	is doing Estonia is trying to do some interesting
6555500	6557500	things I think networks of small countries could start
6557500	6559500	sharing best practices and sharing
6559500	6561500	resources so they don't all have to develop the stuff from
6561500	6563500	scratch which could start to lead
6563500	6565500	to coalitions of countries like the EU
6565500	6567500	saying let's do some fundamentally better
6567500	6569500	things I think
6569500	6571500	it will happen also not at the level of nation
6571500	6573500	states were like
6573500	6575500	decentralized groups blockchain
6575500	6577500	type groups say all right let's
6577500	6579500	really earnestly take on what these primary
6579500	6581500	problems are and work on developing
6581500	6583500	these solutions these capacities
6583500	6585500	for the tech companies
6585500	6587500	to do so would be very hard because
6587500	6589500	while
6589500	6591500	it could be
6591500	6593500	still
6593500	6595500	profitable
6595500	6597500	long term it would not be profit
6597500	6599500	maximizing short term relative
6599500	6601500	to the current thing they're doing as we said
6601500	6603500	winning at the current game
6603500	6605500	and building a new game are different things
6605500	6607500	and winning at a current game that self
6607500	6609500	terminating is a very short sighted thing
6609500	6611500	to want to keep doing so
6611500	6613500	if Facebook or Google or whatever
6613500	6615500	were to cut its ad model
6615500	6617500	it would have a hard time being able to meet
6617500	6619500	its fiduciary responsibility to shareholders a different
6619500	6621500	way but could it
6621500	6623500	in conjunction with
6623500	6625500	a
6625500	6627500	participatory
6627500	6629500	government regulatory process that wanted
6629500	6631500	to help change its fiduciary
6631500	6633500	responsibility
6633500	6635500	where it became more of a social utility
6635500	6637500	start to actually redirect its technology
6637500	6639500	and redirect its decision making process
6639500	6641500	yeah it could and that would be super interesting
6641500	6643500	so
6643500	6645500	I would like to see
6645500	6647500	as we mentioned earlier I'd like to see the UN
6647500	6649500	recognize that the
6649500	6651500	level of progress that it has made
6651500	6653500	at
6653500	6655500	the sustainable development goals, nuclear
6655500	6657500	deproliferation and other types of
6657500	6659500	international things like
6659500	6661500	economic equality
6661500	6663500	globally writ large
6663500	6665500	and
6665500	6667500	preventing arms races and tragedy of the commons that
6667500	6669500	well it hasn't done nothing
6669500	6671500	what it's doing is not converging
6671500	6673500	it's not adequate, it's not converging on
6673500	6675500	eventually solving the problem set
6675500	6677500	just more of that approach, it needs a different approach
6677500	6679500	and so to say
6679500	6681500	clearly we don't know how to facilitate
6681500	6683500	coordination of global problems well enough
6683500	6685500	so let's have the superseding focus
6685500	6687500	be innovation towards better methods of global
6687500	6689500	coordination, that becomes our new number one
6689500	6691500	goal
6691500	6693500	because we know we only get all the other goals if we get that
6693500	6695500	and you can see that during World War II
6695500	6697500	when we had to crack the enigma machine
6697500	6699500	and figure out
6699500	6701500	computation and whatever
6701500	6703500	we got touring, we got von Neumann
6703500	6705500	one of the smartest people from
6705500	6707500	countries all around the world
6707500	6709500	engaged in solving those problems
6709500	6711500	I would like to see the US, the UN
6711500	6713500	I would like to see other countries and I would like to see private sector
6713500	6715500	taking seriously the actual
6715500	6717500	problemscape we have
6717500	6719500	and innovating not for
6719500	6721500	just short-term advantage or narrow
6721500	6723500	in-group advantage
6723500	6725500	but for long-term
6727500	6729500	advantage of the whole, how do we
6729500	6731500	since we have global effect
6731500	6733500	global coordination adequate to what is needed
6733500	6735500	to me that has to become
6735500	6737500	the central zeitgeist
6737500	6739500	and whatever groups figure out how to do it
6739500	6741500	effectively will be the groups
6741500	6743500	that can direct the future
6743500	6745500	and I know that this is the work
6745500	6747500	that you are working towards with the conciliance project
6747500	6749500	do you want to talk
6749500	6751500	just about how you are working towards that
6751500	6753500	with your work and how we are collaborating?
6753500	6755500	yeah, I mean we are
6755500	6757500	we are at the very very beginning
6757500	6759500	the conciliance project
6759500	6761500	has a site up that is not even
6761500	6763500	a beta yet
6763500	6765500	because we
6765500	6767500	in just starting wanted to
6769500	6771500	work on building stuff
6771500	6773500	in association with thinking
6773500	6775500	but this talk is very
6775500	6777500	central, this conversation you and I are having is very central to the aims of the conciliance project
6777500	6779500	which is
6779500	6781500	we are wanting to
6781500	6783500	inspire, inform and help
6783500	6785500	direct a innovation zeitgeist
6785500	6787500	where
6787500	6789500	the many different problems of the world
6789500	6791500	start to get seen in terms of
6791500	6793500	having interconnectivity
6793500	6795500	and underlying drivers
6795500	6797500	and the forcing function
6797500	6799500	of the power of exponential tech
6799500	6801500	is taken seriously that says in order to become
6801500	6803500	good stewards of that requires
6803500	6805500	evolutions of both our social
6805500	6807500	systems and our culture
6807500	6809500	the wisdom to be able to guide
6809500	6811500	that power, a recoupling
6811500	6813500	of wisdom and power in that
6813500	6815500	adequate to what is needed
6815500	6817500	so how do we innovate in culture
6817500	6819500	the development of people
6819500	6821500	and how do we innovate in the social systems
6821500	6823500	the advancement of our coordination
6823500	6825500	both employing the exponential tech
6825500	6827500	and being able to rightly guide it
6827500	6829500	and so
6829500	6831500	we have
6831500	6833500	a really great team of people that are
6833500	6835500	doing research
6835500	6837500	and writing basically the types
6837500	6839500	of things we are talking about here in more depth explaining
6839500	6841500	what is the role of
6841500	6843500	the various social systems like what is the role of education
6843500	6845500	to any society help understand
6845500	6847500	fundamentally what that is
6847500	6849500	understand why there is
6849500	6851500	a particularly higher educational
6851500	6853500	threshold for open societies where people
6853500	6855500	need to participate not just in the
6855500	6857500	market but in governance
6857500	6859500	understand how that has been disrupted
6859500	6861500	by the emerging tech and will be disrupted further
6861500	6863500	by things like technological automation
6863500	6865500	and then envision what is the future
6865500	6867500	of education adequate to an open
6867500	6869500	society in a world that has the
6869500	6871500	technology that's emerging
6871500	6873500	and we don't necessarily know what the answer is
6873500	6875500	but we know examples and we know criteria
6875500	6877500	so then it's like innovate in this area
6877500	6879500	and make sure you factor these criteria
6879500	6881500	and the same thing with the fourth estate the same thing with law
6881500	6883500	the same thing with economics
6883500	6885500	and so the goal is not how do we take
6885500	6887500	some small group of people to build the future
6887500	6889500	it's how do we help get
6889500	6891500	what the criteria of a viable future
6891500	6893500	must be and if people disagree awesome
6893500	6895500	publicly disagree and have the conversation now
6895500	6897500	but if we get to put out those design constraints
6897500	6899500	someone says no we think it's other ones
6899500	6901500	the center of culture starts to be thinking about
6901500	6903500	the most pressing issues
6903500	6905500	in fundamental ways and how to think about them
6905500	6907500	appropriately and how to approach them appropriately
6907500	6909500	so fundamentally our goal
6909500	6911500	is
6911500	6913500	supporting an increased
6913500	6915500	cultural understanding of the nature of the
6915500	6917500	problems that we face a clearer understanding
6917500	6919500	rather than just there's lots of problems and it's overwhelming
6919500	6921500	and it's a bummer and so
6921500	6923500	either some very narrow
6923500	6925500	action on some very narrow part
6925500	6927500	of it makes sense just most of activism
6927500	6929500	or just nihilism
6929500	6931500	we want to be able to say actually
6931500	6933500	because there are underlying drivers
6933500	6935500	there is actually a possibility
6935500	6937500	to resolve these things
6937500	6939500	it does require the fullness of our capacity
6939500	6941500	applied to it
6941500	6943500	and with the fullness of our capacity so it's not a
6943500	6945500	given but with the fullness of our capacity applied to it
6945500	6947500	there is actually a path forward
6947500	6949500	and
6949500	6951500	so we're writing these papers
6951500	6953500	that basically would be kind of like
6953500	6955500	a meta-curriculum
6955500	6957500	for people who want to be engaged in designing the future
6957500	6959500	and
6959500	6961500	some of them have to do with current
6961500	6963500	public culture and how to be able to
6963500	6965500	change patterns of public culture that lead to
6965500	6967500	better conversation, better sense
6967500	6969500	making, better meaning making and choice making
6969500	6971500	so that there's an on-ramp into higher quality
6971500	6973500	conversations meaning higher quality process of
6973500	6975500	conversation and then some of them are things
6975500	6977500	like what are the design criteria
6977500	6979500	of the future social systems and how could
6979500	6981500	we build those things then
6981500	6983500	not everybody will read those
6983500	6985500	some people who
6985500	6987500	have the ability to help start building them
6987500	6989500	will but
6989500	6991500	we hope that other people will
6991500	6993500	take that and translate it on podcasts
6993500	6995500	and into animations and in
6995500	6997500	whatever other forms of media
6997500	6999500	so that those topics start
6999500	7001500	to become increasingly
7001500	7003500	present in people's awareness
7003500	7005500	then of course
7005500	7007500	the next part is what groups start
7007500	7009500	emerging wanting to address those
7009500	7011500	and what can we do to help facilitate
7011500	7013500	good solutions in those groups
7017500	7019500	and this is where you and I
7019500	7021500	I've learned a lot from you about
7021500	7023500	the
7023500	7025500	social media issues in particular and how
7025500	7027500	central they are to the breakdown of sense making
7027500	7029500	because obviously without good shared sense making
7029500	7031500	there is no possibility for emergent order
7031500	7033500	you either just get chaos or you have to
7033500	7035500	have imposed order if you want emergent
7035500	7037500	order that means emergent good choice making
7037500	7039500	that means emergent good sense making
7039500	7041500	and so
7041500	7043500	we've learned a lot and discussed these things for a long time
7043500	7045500	and obviously also not just you and I
7045500	7047500	there's a whole network of people that we're connected to
7047500	7049500	that have been thinking deeply about these things
7049500	7051500	that we continue to
7051500	7053500	try to think about what
7053500	7055500	adequate solutions could look like
7055500	7057500	and
7057500	7059500	I think what
7059500	7061500	CHT did with the social dilemma
7061500	7063500	took
7063500	7065500	one really critical part of this metacrisis
7065500	7067500	into popular attention
7067500	7069500	in a more powerful way than I have seen done
7069500	7071500	otherwise because as big a deal
7071500	7073500	is getting climate change and public attention is
7073500	7075500	it's not clear that
7075500	7077500	climate change is something that is making
7077500	7079500	that is driving
7079500	7081500	the underlying basis of all the problems
7081500	7083500	but a breakdown in sense making and a control
7083500	7085500	of patterns of human behavior that kind of downgrade
7085500	7087500	people like oh wow that really does make all these
7087500	7089500	other things worse
7089500	7091500	so I see that as a
7091500	7093500	as a very powerful and personal
7093500	7095500	on-ramp for those who are interested
7095500	7097500	to be able to come into
7097500	7099500	this deeper conversation
7099500	7101500	and some
7101500	7103500	of them it'll simply help them
7103500	7105500	be like okay now I know
7105500	7107500	what I was intuitively feeling somebody's put it
7107500	7109500	into words and I at least feel more oriented
7109500	7111500	and that's the extent because they don't necessarily
7111500	7113500	have the ability to build new blockchain systems
7113500	7115500	or whatever it is and they should be
7115500	7117500	doing the nursing or education or whatever really
7117500	7119500	other important social value they're doing
7119500	7121500	some people
7121500	7123500	will be able to say this actually really resonates
7123500	7125500	I can translate this to other audiences
7127500	7129500	and get more people engaged and some people say
7129500	7131500	I can actually start innovating and working with this stuff
7131500	7133500	and all of those are good
7135500	7137500	yeah I
7137500	7139500	I agree and I think
7139500	7141500	what we've essentially been
7141500	7143500	outlining here and you sort of hit it at the end
7145500	7147500	is going back to the Charles Kettering
7147500	7149500	quote which I learned from you
7149500	7151500	and I've learned so many things from you over the years
7151500	7153500	which is
7153500	7155500	that a problem not fully
7155500	7157500	understood is unsolvable
7157500	7159500	and a problem that is fully understood is half
7159500	7161500	solved and I just want to maybe
7161500	7163500	leave our listeners with that which is
7163500	7165500	I think people can look at the
7165500	7167500	long litany of problems and
7167500	7169500	feel overwhelmed or get to despair
7169500	7171500	in a hurry I think is your phrase for it
7171500	7173500	and I think that
7173500	7175500	when you understand the core generator functions
7175500	7177500	for what is driving
7177500	7179500	so many of these problems to happen simultaneously
7179500	7181500	there's a different
7181500	7183500	and more empowering relationship to that
7183500	7185500	and you've actually offered a vision
7185500	7187500	for how technology can be consciously employed
7187500	7189500	these new technologies can be consciously employed
7189500	7191500	in ways that should feel inspiring and exciting
7191500	7193500	I mean I want that transparent blockchain
7193500	7195500	on a budget for every country in the world
7195500	7197500	and we can see examples like Estonia and Taiwan
7197500	7199500	moving in this direction already
7199500	7201500	and we can see Taiwan building some of the technologies
7201500	7203500	you mentioned to identify
7203500	7205500	propositions of shared values between citizens
7205500	7207500	who want to vote collectively on something that
7207500	7209500	obviously would have driven up more polarization
7209500	7211500	so we're seeing this thing emerging
7211500	7213500	and I think what we need is to
7213500	7215500	sort of have this be seen as
7215500	7217500	a necessary upgrade to
7217500	7219500	let me do that again
7219500	7221500	I think we need to see this as
7221500	7223500	not just an upgrade but
7223500	7225500	the kind of cultural enlightenment that you speak of
7225500	7227500	that so many different actors are in a sense
7227500	7229500	already working on you know we used to
7229500	7231500	have this phrase that everyone is on the same team
7231500	7233500	they just don't know it yet
7233500	7235500	and once you understand the
7235500	7237500	degree to which we are in trouble
7237500	7239500	if we do not get our heads around
7239500	7241500	this and identify the kind of
7241500	7243500	core generator functions that we need to be addressing
7243500	7245500	once we all see that
7245500	7247500	I'll just speak to my own
7247500	7249500	experience when I first encountered your work
7249500	7251500	and I encountered
7251500	7253500	the kind of core drivers
7253500	7255500	that drive so much of the danger
7255500	7257500	that we are headed towards
7257500	7259500	I
7259500	7261500	immediately
7261500	7263500	I was kind of already in this direction already
7263500	7265500	reoriented my whole life to say
7265500	7267500	how do we be in service if this is not happening
7267500	7269500	and of creating a better world
7269500	7271500	that actually meets and addresses these problems
7271500	7273500	and I know so many other people
7273500	7275500	whose work
7275500	7277500	and whose lives and whose daily missions
7277500	7279500	and purpose have been redirected
7279500	7281500	by I think hearing some of the core
7281500	7283500	frames that you offer
7283500	7285500	and who I hope
7285500	7287500	and who are many of whom are already working on
7287500	7289500	active projects to deal with this and those who are not
7289500	7291500	are supporting in other ways
7291500	7293500	and I hope that our audience takes this as
7293500	7295500	an inspiration
7295500	7297500	for how can we in the face of
7297500	7299500	stark and difficult realities
7299500	7301500	as part of this process
7301500	7303500	gain the kind of cultural
7303500	7305500	strength to face these things head on
7305500	7307500	and to orient our lives accordingly
7307500	7309500	because I have
7309500	7311500	while bearing periods of time
7311500	7313500	hit probably low grade despair
7313500	7315500	myself
7315500	7317500	I actually feel more inspired than ever
7317500	7319500	the amount of things and the number of people
7319500	7321500	who face these challenges
7321500	7323500	and I'll just say that
7323500	7325500	I think when you face these challenges alone
7325500	7327500	and you feel like you're the only one seeing them
7327500	7329500	or you have a weird feeling in your stomach
7329500	7331500	it can feel debilitating
7331500	7333500	and when you realize the number of people
7333500	7335500	who are also putting their heads up to say
7335500	7337500	how can we change this
7337500	7339500	that's what feels hopeful
7339500	7341500	and that's where I derive my optimism
7341500	7343500	so Daniel thank you so much for coming on
7343500	7345500	it's an honor to have you
7345500	7347500	your work has touched the lives
7347500	7349500	who may not always say so publicly
7349500	7351500	but I know that you
7351500	7353500	had also a huge hand in
7353500	7355500	inspiring some of the themes that emerged
7355500	7357500	in the social dilemma which has impacted so many people
7357500	7359500	as well so thank you so much
7359500	7361500	really wonderful that we get to have this conversation
7361500	7363500	thanks just on
7363500	7365500	absolutely
