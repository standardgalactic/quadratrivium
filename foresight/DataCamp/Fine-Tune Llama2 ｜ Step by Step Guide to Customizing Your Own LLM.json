{"text": " In this session we are going to cover two notebooks, one this is the first one dedicated to the creation of the dataset and a second one dedicated to the fine-tuning of the model. Here we're going to have a very big and friendly approach to this topic and then at the end of this session I will present some resources to go beyond that and to be able to also like use better tools and perform other things but in this session we're going to stick to like the basics and try to really learn the theory behind it so we're also like more aware of what's going on when we use automated tools. In this first notebook we're going to talk about datasets and how to create a high quality dataset so I want to mention the fact that there are basically two datasets several datasets we're interested in. The first one is instruction datasets where inputs are instructions this is basically how you use chart gpt like write me something and the model is supposed to write you something and not complete what you just said because this is real completion and this is part of the pre-training of these models they pre-train on a lot of data and the task is next token prediction so they're just here to predict what the next word is going to be and this is nice but not really what we want to create assistance so instead we're going to use supervised fine-tuning to be able to turn a base model into a useful chat model here. There are also preference datasets I want to mention it briefly this is reinforcement learning from human feedback this is often used after the supervised fine-tuning process and in these datasets you will find different answers and some kind of ranking of these answers to say hey this one is more useful than the other one we're going to mention it during the fine-tuning process in the second notebook and finally there are other types of dataset it can be sentence classification it can be code dataset where you have a fill in the middle objective where you want to fill in code that has some context so before your cursor and after your cursor and this is where you want the code to be but we're not going to talk about this it was just a general overview of the kind of dataset you can expect in this space so as mentioned here we're just going to use supervised fine-tuning so with an instruction dataset and we're going to be a lot owned by filtering an existing dataset so the one that we're going to use today is the open platypus dataset it's a collection of different datasets actually this is already a dataset that has been filtered using all the dataset but we're going to do it even more we can check what it looks like on the hugging face page here so here you have the instructions for example the board game speeder and divider into three parts blah blah blah and the output that is expected by the model so this is basically what your model is going to learn during the supervised fine-tuning process it's going to learn to output this answer when it has this instruction and we repeat it a lot of times and at some point the model is pretty good at understanding what it needs to do so it needs to provide a useful answer for this instruction you have more information about the platypus dataset and there's also a really nice paper you can find here about how they did it and how they trained their models called platypus so an interesting read if you want to know more about it so first of all let's start with the code we're going to need to install some libraries so here we're going to keep install a bunch of libraries we're going to install datasets the library from hugging face you handle this also transformers very useful sentence transformers too because we are going to use sentence transformers to create embeddings of our data we're going to see why in a few minutes and finally a vector database from facebook face gpu here you can see the runtime so make sure you have a t4 gpu or more if you can afford it but the entire code should run using a free tier google collab so with a t4 gpu for the sake of this exercise i'm using a v100 with higher ram because it's just going to be a bit faster and we won't have to stay at the screen for 30 minutes the second step that is really useful so this is the first time i tried to use it please bear with me this is quite a new feature in google collab but now we have a secret tab here on the left and as you can see here you can add a new secret and you can give it a name hugging face and a value so you can retrieve the value using this link if you have a hugging face account so here i collab i can copy it here i can write the hugging face here and copy paste the value here so and give access to the notebook um so this should be a really clean way of managing your secrets because now they share across all your notebooks and they won't going to be shared with people um other than in your account um to use it to jump in um we just got a question from the audience that i'm not sure what the answer is i don't know how widespread this problem is but do you know what an insure file accessible error message might be oh uh sorry i thought that we tested that uh let me let me just uh try out the links that are used okay uh anyone with the link should be able to access them actually uh some can some people confirm that you can access it which you can you access these notebooks uh i can access the notebooks one thing might be the case like you do need to be logged into google i think in order to access colab so if you're not logged into google then you probably need to do that all right we'll continue for now um if anyone else has any problems please do let us know in the chat of the comments uh yes please uh oh uh james just might be a company blocking it for security reasons yeah if you can't access any google products from uh your corporate uh laptop then yeah you probably have to find a different network unfortunately or just watch for now follow along uh when you get the recording yeah exactly uh sorry about that google colab can be a bit difficult to use in some in some context um here we're going to uh import our secret using uh google colab use the data use the data get so now the hf token um has the correct value and this is what we're going to use if we need it in the when we need it in the rest of this collab this is optional uh we are only going to need it when we want to upload uh the data set so if you don't have a hugging face account you don't need to create one i recommend it because it's nicer and you'll be able to upload your own data set in your own account but if not it's okay i will upload it in my account and you'll be able to reuse it from this account so now we have the hugging face token and the first thing that we want to do is to load the data set that we want to use so in this case it's the data set that was mentioned here so open open platypus we can copy paste it from here and it should load the data set download it and load it and we're going to see all the different columns inside of the data set so input output instruction and data source uh that should be correct here and we have the number of rows so almost to 25k rows um so we can see a bit more about it we're going to read it as a bandit's data set and we can even with google collab convert it into an interactive table it's a better way of looking at it unfortunately it can take some time for google collab to convert it so we're going to see but yeah it's nice to see that we have the instruction and we have the output and this is all that matters here we're going to explore a bit more about instruction the output if it was a real data set real raw data set what we want to do at this point is really read not every line because it's too too too much but a lot of these lines and be able to get a good understanding of what's in this data set and what we expect and also like just clean it like if there are samples that are not high quality that have like bad english that are just plain wrong we want to remove them this is very important we want to create the best data set possible so it means filtering out a lot of samples okay so now we have this data set we can read everything here this is a lot of data so we won't do it in this case it could take too much time but now something that we can do let me show you once again the the code here so load data set and then the name of the data and what we are going to do here is that we're going to use the transformers library to import the tokenizer this will convert the row text into tokens then we will import the matplotlib library to plot the results and also the cborn library for the same reasons first we want to import the tokenizer so in our case we want the tokenizer not from but as suggested here but from lamatu because this is the model that we want to use so we're going to use a new research version of lamatu and not the official one from meta why that it's because if you don't have a hugging face account you will not be able to access the official version of it so news research just re-uploaded the entire thing so now we have the tokenizer we can test it here it's going to download it once again from hugging face and and then we're going to print it once it's done here you can sit with all the special tokens unknown etc when it's done we want to use it to tokenize our instructions here and also outputs here so the way that we're going to do it we're going to create a table an area called instruction token counts it's a bit of a ghost but and we're going to use the tokenizer to tokenize every sample in our data set this is not correct actually this is why you should not always trust coder lambs for example in our data set train and close it so here in this table we should get like the token counts for every instruction and we're going to repeat this process with the output so pretty much the same thing here and we want the output and finally we want to combine the instruction in the output because this is like the entire data set and so instruction plus output and in this case we want to call it combine token counts and here we're going to merge these two tables for instruction outputs in zip etc and we should get like the the sum of all the tokens here in both instruction outputs now that is done we are going to do a little function to plot the distribution of our token so we can have an easy visualization of it so here I'm going to use sns set style white grid and slowly but surely get everything here actually I'm sorry but I'm going to copy paste it so it's a bit faster to do it and now we can plot the distribution for every instruction output and combine token count so this is a very simple plot and we can call it using plot distribution and here we're going to first use instruction token counts and we're going to see the distribution of token counts for instruction on A this part takes a bit of time unfortunately because yeah it needs to tokenize the entire data set but once you've done it you can basically comment it it's okay and we're going to repeat this process with the output token counts so this time it's for output only and finally the combine token counts so here it's for instruction plus output here I'm going to comment it so we get faster results and now we have a distribution for all of our data so here you can see that we have approximately like the the minis is around here so around 500 tokens there's a long tail distribution goes up to like 5000 tokens why is it important why does it matter it's because these models they have a certain context window and if it's because beyond this context window it's not going to be very helpful so it's important to know like the the number of tokens in our data set we also maybe want to sample more from samples that have more tokens because they're going to be more informative than others but we'll see about that basically here we can see okay I'm going to put a certain threshold for this data set at 2k tokens the max context size for lambda 2 is actually 4k but this is just an example to show how we can just set a threshold so here we want to filter out rows with more than 2000 samples so one way of doing it is to say if I count in numeric combined token counts if count okay so here we're going to retrieve the index of every sample where the count in combined token counts is lower than 2k and now we can print how many of this we have okay and if we print length of the data set in train and we are going to remove the length of the valid indices so we see how many we remove okay we only remove 31 of them but that's fine you can have a more aggressive threshold if you want in this case yeah we're going to remove just a few of them as you can see here then we're going to extract the valid rows based on the indices so here we need to take the data set train and we're going to use the select method to only get the valid indices and then we're going to get the token counts so for the valid rows so we can also plot this distribution here and we plot the distribution of the token counts after filtering exactly like that okay there's an issue here because I executed the code twice I shouldn't have but that's fine if you execute it once it should be okay now it's already filtered which is why it's grid in there and here you see that we have a very different plot because all this right part has been filtered out another thing that we can do is near the duplication using embeddings which is why we install the sentient transformers library and what we want to do here is we want to embed every row every sample from our data set so here we want to translate that into a vector we call an embedding and using an embedding model how to choose the best embedding model it's often it's a popular question one way of answering it is looking at the MTEB the board on a hugging face this is where you can see like a competition with all the embedding models on various tasks it's funny because since I've made this screenshot there are new ones on top of it the one that we're going to use here is the GTE base embedding model it's a it's a really good model it's not the best model but it's going to be faster than other options which is why we're going to use it here and we're going to use these embeddings to then calculate the similarity between them and when they're too similar we're just going to filter them out so how are we going to do it we're going to use the sentient transformer library and we import the sentient transformer class we're also going to import face the vector database from facebook it's not the best vector database but it's very simple it's very minimalistic which is why I used it in this example from data set we are going to use data set and data set dict to be a bit fancy we're also going to use tqdm to have a nice loading bar and finally number for some operations so here we're going to really create the code in one function to do everything so we are going to pass it a data set we are going to pass it the name of the embedding model we want to use and we're going to pass it threshold for example 95 percent it means that when it's 95 percent as similar as another embedding it's fishy and we probably do not want to to use it we probably want to filter out this model so as a sentence transformer we're going to use the model that we we pass this argument for the output we are going to use all the example in the data set so what we want to filter filter out here are just the outputs are not the instructions we're fine if we have similar instructions we just do not want similar outputs because this is what the model is going to be trained on then we are going to say that we are converting text to embeddings we are going to use the sentence model and encode the outputs that we have and we can even show a progress bar to be fancy bar then we're going to get the dimension of our embeddings so you can see in the little boards some of them they have like 1024 some of them they have like 768 basically yeah they have different dimensions take that into account we are going to create our index using the vector database so it's going to be a flat ip index in this case and we need to normalize our embeddings the face already has a function to do it but i do not trust it that much so we're going to do it on our own because i had a bad experience with it and i think it's going to be better that way so here we're using numpy to normalize it using the norm to just take the norm to normalize the entire embedding space and then we're going to add it to our index as normalized embeddings then we're going to say we are filtering out let's say near duplicates and in this part of the code we want to use the index search so we have the normalized embedding we just put and here we are going to say k equal 2 so we are going to return at most two vectors we don't need more in this case and we're going to create a list of the samples we want to keep call it to keep and this is the main loop finally range length yeah it's fine and we're going to do something nice for the QDM so we have a nice loading bar and what we want here is if the dimension is so this if this is we're going to return here the similarity between these embeddings and if this is this if the cosine similarity is below the threshold we are going to keep it so we are going to add it to the to keep happened and then we have i yes an index index and then we can create data set trains and we are going to use the select method from the data set object to only keep these indexes then we are going to return it as a data set dict this is not the most elegant way of doing it but it's going to be fine for the purpose of this exercise and then we can call our function so the duplicate data set and we are going to pass the data set and we're going to pass the embedding model we want to use so in this case as I mentioned it's going to be the gte large and we can just copy paste it here and as a threshold I'm going to use 0.95 be careful if you switch the embedding model you won't have the same distribution of cosine similarity so some models to get the same results you're going to need like 85 others you can might need 99.9 it really depends on the embedding model that you use it should be fine so now we can convert the entire data set into embeddings here we are downloading the embedding model it's not a big model which is why it's it's pretty fast the long part is actually comparing the embeddings I should mention why we're doing it using a vector database instead of a for loop I've tried to do a very minimalistic version using two for loop so we would compare every embedding to all the other embeddings but it took a very long time so this is why we're using a vector database here to be more efficient to be used by the computations and and get the results basically just faster because otherwise it would take like two hours it was really really too long unfortunately so here we still downloading the models and then with a v100 with high ram it should take about three to four minutes to get all the embeddings and to filter out the data set in the meantime we can continue I'm just going to show the code here because I don't know if you can really see it if you had time to see the code this part might be a bit confusing but I don't want to delve too deep to the details of the face vector database okay so now you see that it's converting the text to embeddings oh it's going to take much longer than last time I've tried unfortunately but it's okay like we we can stop it or come back later to finish it what we want to see when we have this dedupe data set is the number of samples that were filtered out so we can print the length of the original data set we can print the length of the dedupe data set and we can even print the number of samples that were removed so in this case the length of the original data set minus the length of the dedupe data set and this will tell us that how many rows we we removed and last time you can see it later on the solution notebook it's about 8 000 samples one thing that we can do when that part is over is topk sampling so in this case we still have too many samples because if we remove 8 000 samples we're still going to have 20 no we're still going to have 16 k samples maybe this is too much for what we want to do so we can randomly sample some some rows in order to do that we're going to create a new function called topk rows we are going to use a data set token counts and k to know how many we want to have we're going to sort the indices because we can sort it by descending token count and get the topk indices in this case so it's going to be sorted range length token counts and here we are going to use going to use lambda i token counts reverse is equal to true so here we should get everything that we need so the token counts and get all the data sets with most samples first and then topk indices we are going to just keep those those topk yeah and i just talked about randomly doing it but it's not true we're just getting like the the samples with the most tokens sorry about that and then we can create topk data and here we have instruction where we want data sets that are in the topk indices and we're going to do the same thing with the output you could do something similar with the select method but yeah this time this time i want to to be clear about what's going on so we have a full loop to select all the samples that were in the sorted indices here and we are not going to keep like 1000 of them and we are going to do the same thing with output and finally we can return our data sets from the dictionary that we did topk data so this is our function but in order to call it we are going to need to have the new token counts because here we filtered out a lot of samples so we can just copy paste what we did at the beginning here get the instruction token counts the output token counts the combined token counts and this is what we will use when we want to call this function so let's have a k of 1000 and the topk data set will be get topk rows data set we're going to use it with the combined counts and the k of 1000 so this is how we're going to call the function and finally we are going to save it as a dictionary like yeah data set dict as they call it to make sure that we still have like a train split but not not very important after we've done that we can once again re-compute all of the token counts and the plot actually like also plot the distribution so this is just to see what's the new distribution after after filtering after topk sampling now how it looks like and yeah we'll see we'll see in a few minutes when this is done and we can see the the distribution we can just also see the samples themselves to see like with pandas how it looks like like we've done at the very beginning of this notebook here and we'll see like how many samples remain and finally I want to mention chat templates so there's a need to define a chat template if you want to use your large language model as a chat bot there are different ways of doing it here's a way of doing it we have a raw user content either raw assistant so this is more like the raw data this is a format that you can use just user two points and then the message then assistant two point nice nice to meet you in the case of flamato you have this particular template so you have this token s then you have the instruction you have space not not not the instruction it's it's another token for instruction then you have a sys you have the system prompt you have here the user prompt and finally the model answer it's quite a difficult template you we don't need to use it to function our lamato model because we're functioning the base model and this chat template is only used in the chat version of lamato this is not the one that we use here I wanted to mention the chat ml template from open ai it looks like this it's the most popular and standardized one you can see it in a lot of state-of-the-art open source models we're not going to use this one because it requires adding tokens it's more difficult so the one that we're going to use is going to be quite simple we're going to create a function called chat template about it with an example and in this example we're going to format we format the instruction and here we're going to use this instruction then break line then we can finally put the original instruction and break line break line and here we can put like output or response and go for response and another break line why this one honestly there's no good reason you could imagine a lot of different prompt templates but it's going to be nice to see that the function model will follow this prompt template finally we can return example and we map that using the map method from the data set object and this will yeah this will change all of our instructions so they can follow this template we're going to visualize it when okay it's done let's go back a bit earlier so we managed to remove sorry a bit earlier so yeah we filtered everything that we wanted to filter we filtered out like 8k samples like I mentioned previously then there's a top case sampling where we said we only want to keep the top 1000 samples in terms of token counts so the one with the most tokens and you can see here the distribution of token counts for instruction only for token counts and finally the distribution of token counts for instruction for output you can see we don't have samples with less than 1000 samples thanks to the top case sampling yeah it should make sense hopefully so here we have 1000 samples with a lot of tokens and they should be high quality because they're not close to each other we need to duplicate them so it means that they should be pretty far away from each other here you can see all the 1000 rows once again you can click it here if you want to have a good overview of the samples that we that were selected and now they should follow our chat template so just let's check if if this is correct and here you can see the instruction let's click it here we really have like the instruction and the response as mentioned here so this is working as intended instruction response and here is the response that we want the model to follow all right so this is done and the final thing that we can do here this is optional this is if you have a hugging phase hub count if you put the value here of your secrets then you can just push the data set to the hugging phase hub like this and we specify the token here i'm calling it mini platpus you can call it however you want um and this is going to upload it and you can even check if it's correctly uploaded if I go to my hugging phase account and I check my data set this one was uploaded dated less than one minute ago and here you can see our entire data set uh cool so we have everything now uh and we're ready to go to fine tuning uh I hope it was it was clear uh and if not I hope that the solution notebook will help you uh to to to create your own data sets to go further beyond that you can create synthetic data using um 24 it's something that is used quite a lot and it creates uh really good uh data sets so it is something that you can play with and otherwise it's really like a manual reviewing you can import this data set in google sheets and really manually review every row possible maybe create some regex to automate the process a little bit but this is a very time consuming process but it's also like very nice because then people can reuse your data sets if you share them but now let's go to the fine tuning uh notebook you should also have it uh let me uh check the the chat uh okay you have the solution notebook cool um so here you have um lama 2 and we're going to delve deep into the fine tuning process so as mentioned previously there are two ways of fine tuning these models this supervised fine tuning this is what we're gonna do uh so we're gonna tune it on a set of instructions and responses it's going to help the model focus where we want um so to be helpful to follow the chat template too and there's also the reinforcement learning from human feedback where we want the model to maximize your word signal i'm not going to delve into that uh there are a lot of good articles about it uh it's able to capture more complex uh preferences but it's also like more difficult to implement and in practice uh most um if not all this this year but except this year nearly all the state-of-the-art open source LLMs just use supervised fine tuning so yeah something to keep in mind um and once again there's an example uh from a few months ago now the NEMA paper that shows that only 1000 high quality samples can really make the difference and and get very far um in this case when you have a 65 billion models um and i want to mention the open LM leaderboard you might be familiar with it um but this is quite useful to see uh what are the best models so currently you have like this non-Lama model um i wanted to uh yeah show this Godzilla 2 7b model because um it's using a Lama 2 70b and um uh i saw that it was using my data set so when i was telling you like yeah it's nice to also share your data set you see like sometimes it can be reused by other people without uh you knowing anything about it but i'm glad this one was useful um so what we're going to do here is um as previously we are going to start by installing all the libraries that we want so in this case we're going to go pip install queue and we're going to update because uh Google collab already has some of these libraries but we want to use really like the latest version available in this case bits and bytes and 1db transformers and the hugging face oops i'm going to disconnect this session okay uh once again you can use a t4 gpu for the entire uh notebook here i'm just going to use the v100 because it's going to be faster transformers for the transformers data set for the data set accelerating it's it's to make things uh faster pft it's going to be for the fine tuning process that we're going to use i will mention it later tl is a wrapper it can be used for supervised fine tuning or for reinforcement learning from human feedbacks we have bits and bytes for quantization because we are not going to use the model in full precision and 1db for reporting so we can have a nice dashboard where we can track the progress of our model um once again we're going to use google collab i'm just going to copy paste it uh from the previous um notebook so we have our secret token current access okay um it's optional if you don't have a hugging face account once again and here we're going to import a lot of libraries we can import os we're going to import torch we're going to import the data set from data sets and from the transformers library we need to import a lot of classes uh so auto model for causal lm auto tokenizer uh bits and bytes config auto auto tokenizer uh training arguments and pipeline when we want to run it um when the model is trained and then from pft we also need to import a few of them so lower config pft model and something called prepare model for kbit training and the last one is the wrapper for supervised training from the tira library called sft trainer um i'm going to let it here for for a second and um we're going to talk about uh the different ways we can find during this model so we have three ways there's the full fine tuning there's laura and there is uh q laura with full fine tuning uh we're going to use um the the entire model so we're going to uh train all the weights in the model which is very costly then we have laura which instead of training all the weights we're just going to add some adapters in in some layers and we're going to only train these uh added weights uh so this really reduces the cost of training the model because we are just going to train like one percent two percent of the entire weights and finally we have q laura which is using laura but with a model that has been quantized so uh in this case we're not going to use the model in six-bit precision so with every weight in the model uh occupying 16 bits on the disc but instead they're just going to be quantized into four bits so we can lose a bit of precision here but in the end there are mechanisms to make it less impactful and we'll be able to get a really strong model using q laura a bit of calculation here we have 16 gigabytes of VRAM with our GPU here you can see it's 16 gigabytes and um lama 2 7B weights so we have seven billion parameters if they take up two bytes it means that we're going to use 14 gigabytes so we are almost like using the entire VRAM and in addition there are like other things there's an overhead due to optimizer stays gradients for all activations so it's going to be challenging but we we can manage to fit it into only 16 gigabytes of memory okay so now we're going to really delve into the code to function it um we are going to reuse the news research model here like previously and we are going to give a name to our new model so in this case i'm going to call it lama 2 7B and mini platypus and we're going to reuse the dataset that we just created so it should be called mini platypus and we're just going to use the train splits finally we have the tokenizer so here we are going to use the tokenizer from the lama 2 model and we're going to use the fast version of it we are going to do something that is um some people hate it we don't have a padding token for lama and this is a really big problem because we we have a dataset with different number of tokens for each row so we need to pad it so they all have the same length right and there are different ways of doing it here i'm using a token called the end of sentence token and this will have an impact on the generation of our model this is what we're going to use here there are different ways of doing it this is definitely not the best version of it if you want to learn more about it i linked an article from benjamin mary about two other ways of doing it and this is what we should do but for the sake of simplicity here i'm telling you about this problem but i'm still using the end of sentence token for this fine-tuning then we are going to talk about the configuration of the culor so here bnb config it's the bits and byte configuration the first thing that we want to do here is to load the model using four bits otherwise it will not fit into the VRAM in order to do that we can specify the quant type that we want to use in our case we want to use the nf4 format this is the format that was introduced in the culor paper and we are going to use a compute type so this is how the weights are stored using four bits and when we want to compute it's only it's going to use 16 bits so we have more accuracy and we are also going to use something called double quantization so even the quantization parameters are quantized it's it's to like really take even less space than without using it then we have the lower configuration so on top of culor we also have the lower configuration and in this case we have a bunch of diameters one of them is the alpha the alpha is basically the strength of the adapter the impact it has on the the model because you can merge these adapters in a very weak way so using very little weight or using a big weight 32 is a pretty big weight but this is a quite standard value for this parameter we also have like the dropout because when we add these adapters they have a little dropout so we have a 5 probability of skipping these connections and finally we have a rank which is like the dimension of the the matrix that I use here if you want to know more about law and like a minimalistic implementation of it I made this notebook called nano law and this goes into in depth into like the theory behind it and we will help you understand these parameters a bit better we do not want to take care of the bias so we have the weights and the biases here we do not care about the biases and the task type in this case is causal lm because we are auto aggressive and the target modules here we have a very long list of target modules the target modules it's something that you can see here actually and the attention lma attention thing it's basically like okay what which which modules do you want to not train but add a law adapter to it and we are going to use a lot of them because it's been shown to really improve performance so the more modules we have the more parameters we are going to train but this is fine like we we can afford it with our limited budget it's going to help us in the long run then we're going to load the model from pre-trained and we're going to use the base model that we typed earlier the quantization config so we have the bnb config here this is what we're going to use and finally the device map so here it's we could also use auto but I'm going to use that it's going to automatically detect the device the hardware that you have so in our case we'll detect the gpu and make sure that we're using the gpu for training otherwise it's not going to work and finally we are going to call a function called model for kb training this will cast the layer norm in fp32 so in more precision it will make the output embedding layer require grads and add the upcasting to the mlhead to fp32 so what it means is that it's going to take some layers some modules and make sure that we are using them with the highest precision possible because it's been showed to really improve the performance too so yeah there are some modules that we will not that do not really matter some of them matter quite a lot and we want to be quite proactive on that and in the end this will help us build the best model possible so if I oops I forgot to execute that um and then if I forgot to execute that too oh just while you're running those in the bits um the data set sorry I just wanted to refer to just like just because we're coming up to time um just we are going to over in a bit before we get to before you all jump off for those of you who have to dash as I was going to say we've got three webinars coming up next week so on Tuesday we've got an introduction to snowflake code along coming along uh on Wednesday we've got a session on using ai in robotics so if you're interested in agreeing on ai then that's uh something uh definitely worth attending and then on Thursday we've got a session for best practices on putting lmms into production so three great sessions please do go to datacamp.com slash webinars sign up for those um we have got some great questions from the audience as well so I'm hoping to get to those afterwards if you do have to jump that fine uh please do catch up on the recording for everyone else um I hope you're okay hanging around for a minute and with that I will dash off and let you uh get back to it Maxine yes sorry about that we should be over in like 10 minutes I underestimated the time uh it takes you to write the code um but um yeah here we have lower configuration loading the model preparing the model for training uh we are currently downloading the model here you can see the different modules in the lma attention class and this those are the one that we target also in the lma mlp you can see the hugging face implementation of it to to have more details about how it's actually implemented um once it's done we have more uh boilerplate code uh to to type uh this one it's training arguments uh so we have the training arguments here and what we want to do is to give like a bunch of parameters to it so like where do I output the results we're going to specify uh directory for that uh how many epochs we want to run the model on um here I'm going to put one but let's put four or five uh basically between three and five it is pretty good for an amateur model at this size there is the per device uh train batch size uh this will tell us like how many um yeah the number of batches that we're going to take for every every every step um we have the gradient accumulation uh steps we're not going to use it here it's basically a for loop inside of the um training uh so we don't have to add more um use more VRAM but in this case it's going to be fine we won't need it we have the evaluation strategy it's not going to be very useful here because we are not going to um evaluate this model um we just want to train it and we're going to mention what evaluation looks like with uh uh these models a bit later logging steps we want to log every step uh the optimizer that we're going to use is the adam optimizer uh but a version that is paged in eight bits so it's going to lose test memory uh the learning rate uh we are going to use uh this one there are different learning rates that that what we can use um refer to the qloa because qloa really impacts the learning rate the the model also really impact the learning rate that you want to use we have the scheduler uh in this case we're going to use linear and warm up steps like it won't be really useful here but we can say uh 10 uh to warm up the the optimizer we want to report it to weights and biases and finally uh something that i'm going to put here but uh remove this line uh for like for real training we're just going to stop after two steps otherwise it will take like an hour to train the model but yeah you can just feel free to remove this step if you want a real training so those are the training arguments uh then we need also to use the fft trainer so the wrap i mentioned earlier and in this case we just specify the model the training data set uh we don't have an eval data set so i'm just going to reuse the same one pft config um we specified it um it wants to know the text field here so the instruction field in our data set in our data set was called instruction um the max sequence length um so we're gonna go for uh 512 uh you could say like yeah but we we put a threshold at 2k but we don't have enough VRAM unfortunately uh it it will take like a lot of VRAM to to to put everything into memory so we we're just gonna stop at uh 500 in this example and finally we're going to give it the training arguments so this is what we have and when it's done we can start the training and when the training is done uh we can even save the model uh using that so we should have yeah the model has been downloaded here and now we are training the model so this is a loss a training loss and evaluation loss from weights and biases and as you can see it's a very nice way to tracking the the progress of the model we can see here the warm-up steps where it's it's pretty bad and then it goes better and better uh you can see the training uh loss is in blue so it's quite spiky it's a bit noisy the eval loss is in orange it's a lot less noisy because it's less frequent and something that you can observe uh if you train it for like five epochs is that the eval loss will go up instead of down uh normally uh traditionally in machine learning this is a bad thing but with large average model it's been proven uh time and time again that it's actually desirable and the best models um actually like overfit really a lot on the training data and this is not a problem actually this makes them better um here as you can see your model has already been trained for two steps so if you want you can add more steps you can remove it if you want to train it on the entire data set it will take a while however uh we can check um weights and biases here we won't see any the lot because we only have two steps uh but just to show you uh here's our run and you can see here the global step uh train run time train loss of 1.2 so yeah this is what you would use if you run it on the entire data set finally we can um use our model now that it's been it's been trained uh we can prompt it and say what is a large language model uh we have to wrap it using the right uh chat template so with instruction prompt and and then response and we can use a a pipeline here uh from hugging face it's going to be pretty nice so model tokenizer equal to tokenizer and we're going to restrict the generation in 128 and finally we can get the result here and print it so i'm going to print the generated text it's an object that returns um and we can do something fancy and remove the instruction part this is a question that a lot of people ask me like why do i see the instruction uh in the generation in the generated text you can just trim it basically this is how the hugging face object works but you can remove it manually like this so now the train model is going to answer the question what is a large language model and it will print um the answer here then we want to delete it okay we have the answer so what is a neural network no the answer is a large language model is a type of artificial intelligence model that is trained large amount of text data to generate human like text which is pretty good actually um not thanks to our fine tuning because it was not intense enough but it's a pretty good answer and then you can see that it keeps repeating like instruction response instruction and this is because of our padding actually it's because we're using the end of sentence token as a padding token so now it just doesn't stop and keeps talking so if you don't want to have this behavior please use a different padding technique as mentioned previously finally we want to remove a lot of things so this is specific to google collab we want to collect all the model and the objects in the in the memory in the VRAM so we can merge it with the so we can merge the base model with the adapter that we trained this is a piece of code that is difficult to understand why do we need to call it twice honestly I do not know I just know that it works when I do it and actually and sometimes it doesn't work so well worst case scenario you can just like restart the the collab and and just execute this part of the code but here for the sake of time I am going to copy paste the code so we are going to re reload the base model here and we are going to also load the adapter so the cooler adapter that we we created you can see it here this is our cooler adapter with adapter config adapter model and this is what we want to to load hopefully it will work and and then we can push our model to the hugging face hub when we do that we are going to push the model and the tokenizer and we're using the hf token here this is optional of course and this will upload it to our our hugging face account so here we are merging and unloading the model we are going to reload the tokenizer not just in order to save it too and it's going to be uploaded okay so this is the end of this session sorry it's a bit late but if you want to go further just know that you should be able to reuse use this entire collab notebook with mistral 7b instead of flama 7b mistral 7b is a better model but the name of this talk was fine-tuning llama 2 so I stuck to llama 2 but I would yeah I encourage you to try it out if you want a better fine-tuning tool I recommend actual tool because these Google collab they're really nice to understand the theory behind everything and be able to implement this fine-tuning process on your own but if you want to really fine-tune state-of-the-art open source LLMs I recommend actual tool it's really a great tool I've been using it a lot of people have been using it and this is quite easy to use so yeah this is a good recommendation and then what you can do with this model is you can evaluate it using a evaluation harness you can even get on the open LM leaderboard if you have a good model or you can quantize it so you would make it easier to execute on consumer grade hardware and you could use your fine-tune model on your own GPU so that's it for me it will take a while for the model to be pushed to the hub because it's quite it's quite big but as you can see it's been merged and everything is working correctly so I hope that you found it useful and if you have any questions maybe now it's the time to ask the question to answer the questions all right thank you Maxim that was fantastic a lot to unpack there I actually have like a lot of questions for you but I think if I start asking questions we're going to go on longer than an Ed Sheeran concert so I'm going to stick to audience questions instead let's go with this one first from Prudine so Prudine is saying like maybe don't need to show this but how do you go about querying tabular data so I mean this is very much focused on we've just got a lot of text if it's tabular data what's the difference for tabular data I would not recommend using LLMs because they've really made for for text there are some yeah actually I'm wondering like I don't know maybe maybe we can't get into too much detail but is the standard like if you've got just a text file or once if you've got like a pandas data frame of text yeah this is a good question actually you could see it when we uploaded our data set to the hiking phase hub this is kind of a data frame but it only has a text so if we go back to the data set that we've dealt here you can see like it's basically data frame with instruction output columns but it's not it's not tabular right it's it's really text yeah okay interesting all right next question comes from Kiran saying okay so we've been doing this fine tuning on a GPU do we also need a GPU at the point where we're hosting these things is it just the training bit that's computationally intensive or is it also inference no the inference is also like very intensive unfortunately and you definitely need well you need a GPU in general but in particular if you use lama.cpp I can show it on my screen if you use lama.cpp you can use it on a CPU you can see it sorry I'm gonna you can see it here it's me like running it and it runs on on a CPU you'll have to compromise a little bit because like you need to lower the precision of the model so it's smaller and faster to execute but this is something that you can do on the CPU all right very good so um for anyone who's interested in lama.cpp I know I've got a tutorial on that perhaps Reece can post a link to that in the moment next question comes from Minfem so it looks like you you got some fancy co-pilot action or some sort of autocomplete thing going on there in co-lab where does it what's that tool yeah it's a tool called codium and yeah it works really well with Google collab as you could see I don't know if it learned from my own code but it was like really accurate this time I spoke well when you're practicing rehearsing the the tutorial you probably have the same thing a few times so yeah yeah good way to train it yeah I mean they're training dataset now nice okay so codium's the thing uh next thing comes from from Wei saying uh how important is parameter tuning during function I guess that's like hyper parameter tuning yeah it's a really good question for some of the hyper parameters going to be very important um and for some of them it's it's more like one percent two percent gains for example everything here honestly if you stick to like traditional values uh it's it's gonna make like not super meaningful improvements of course they're important because one percent two percent it's good to have but they're not yeah that that important uh and there are other yeah parameters that are a bit more important the learning rate is a really important one and for this one yeah recommend checking the model that you want to use if you use mistral instead of lambda two it's going to impact it if you use q lower lower or full fine tuning it's also going to impact it all right um excellent next one is from bed can you show again how you show how you load save models maybe we'll skip the using for text generation but if you just cover loading save models I think that's useful okay so the you save the model by calling the the trainer object and with the model and save pre-trained new model uh so this is uh yeah just just some code you need to you know and and then it was the text generation right uh yes uh and in this case I use the pipeline there are a lot of ways of like using them um it's not super pretty but uh it doesn't take a lot of lines of code and yeah this is an object from hugging face from their library which allows you to nicely um use the the text generation for instance okay oh I think the question is about like once you've saved it how do you load that back okay uh basically just reusing that and if you check actually um I've uh the model is uploaded on hugging face already and here you have a usage um section where I describe how you all the code you need to use it so okay see you're getting the whatever model type is from pre-trained uh pulling back off the hugging face page all right nice yeah okay so uh next question from Arun saying can you see what percentage of trainable parameters um we are reduced we're going to have to queue Laura so this is like how many different I remember you saying queue Laura just changes some of the weights in the model so I guess you want to know what percentage that is um it's an excellent question I uh I cannot show you because I deleted the model type in trainer um before but yeah there's a command to do that I recommend checking on google but yeah definitely you can see exactly like the percentage of parameters the number of parameters that you're training using either lower or queue lower all right excellent um and we've got so many more questions uh all right let's just do a couple more um so this next one all right okay it's always fine okay so Alexander asks how do you find tune an LLM so it can extract JSON from differently format that's actually maybe a little bit um specific but can you talk about how you apply it to like um a sort of you've got a CSV file or an excel file of text how do you how I guess how do you standardize that data um yeah I I don't know if it's really a task for an LLM um because there um um if it's just extracting I would say like why do you want to use an LLM and not something else um other than that um there are different frameworks like uh JSON former or LMQM even better LMQM let me show you if it's really about the generation generating a popularly formatted JSON this is a really good uh framework to do it um there are a lot of them but this one is currently the most popular one uh it's quite easy to use I'm not sure if we'd answer the questions but I wouldn't use an LLM to extract this information I would use it to generate a JSON uh and to generate this JSON this is the library I would use okay uh so LMQL was that all right LMQL yeah LMQL all right that's worth looking into then all right one very one very last question then since we're well over time anyway we're we're past limits all right so um how can you improve the forms of LLMs you think basically what's the what a llama index and lang chain and what's the difference between them um yeah so this is um you have fine tuning and um lang chain and llama index they are more about like creating this retrieval augmented generations so um fine tuning is is one way of customizing an LLM for your use case and the RAG pipeline is another way of doing it so with lang chain and llama index you're going to retrieve more context using some vector database or regular databases that you have the difference between them I'm not going to delve into the details LLM index is is there's less stuff but maybe more in depth than lang chain and I would recommend actually implementing both approaches so fine tuning your LLM and using this fine tune LLM with the RAG pipeline and this is where you'll get the best performance possible all right fantastic we're gonna have to call it a day there I think I know there's more questions so sorry to everyone in the audience if you didn't get to your question I just want to say thank you again Maxim that was like incredibly informative and lots of new things that I think we need to explore so brilliant thank you again thank you to recent moderating oh sorry gone Maxim thank you Richie and thanks everyone for your patience I know it's been a lot but I hope that you found it informative so yeah all right brilliant and yeah so thank you to everyone in the audience who asked the question thank you to everyone who showed up today hope to see you all again soon lots of exciting webinars coming up so goodbye have a great weekend", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.12, "text": " In this session we are going to cover two notebooks, one this is the first one dedicated to the", "tokens": [50364, 682, 341, 5481, 321, 366, 516, 281, 2060, 732, 43782, 11, 472, 341, 307, 264, 700, 472, 8374, 281, 264, 50720], "temperature": 0.0, "avg_logprob": -0.1407689941063356, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.008368588984012604}, {"id": 1, "seek": 0, "start": 7.12, "end": 13.52, "text": " creation of the dataset and a second one dedicated to the fine-tuning of the model.", "tokens": [50720, 8016, 295, 264, 28872, 293, 257, 1150, 472, 8374, 281, 264, 2489, 12, 83, 37726, 295, 264, 2316, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1407689941063356, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.008368588984012604}, {"id": 2, "seek": 0, "start": 15.120000000000001, "end": 22.080000000000002, "text": " Here we're going to have a very big and friendly approach to this topic and then at the end of", "tokens": [51120, 1692, 321, 434, 516, 281, 362, 257, 588, 955, 293, 9208, 3109, 281, 341, 4829, 293, 550, 412, 264, 917, 295, 51468], "temperature": 0.0, "avg_logprob": -0.1407689941063356, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.008368588984012604}, {"id": 3, "seek": 0, "start": 22.080000000000002, "end": 29.84, "text": " this session I will present some resources to go beyond that and to be able to also like use", "tokens": [51468, 341, 5481, 286, 486, 1974, 512, 3593, 281, 352, 4399, 300, 293, 281, 312, 1075, 281, 611, 411, 764, 51856], "temperature": 0.0, "avg_logprob": -0.1407689941063356, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.008368588984012604}, {"id": 4, "seek": 2984, "start": 30.08, "end": 37.28, "text": " better tools and perform other things but in this session we're going to stick to like the basics", "tokens": [50376, 1101, 3873, 293, 2042, 661, 721, 457, 294, 341, 5481, 321, 434, 516, 281, 2897, 281, 411, 264, 14688, 50736], "temperature": 0.0, "avg_logprob": -0.09818374151471017, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.004030617419630289}, {"id": 5, "seek": 2984, "start": 37.28, "end": 43.6, "text": " and try to really learn the theory behind it so we're also like more aware of what's going on when", "tokens": [50736, 293, 853, 281, 534, 1466, 264, 5261, 2261, 309, 370, 321, 434, 611, 411, 544, 3650, 295, 437, 311, 516, 322, 562, 51052], "temperature": 0.0, "avg_logprob": -0.09818374151471017, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.004030617419630289}, {"id": 6, "seek": 2984, "start": 43.6, "end": 53.760000000000005, "text": " we use automated tools. In this first notebook we're going to talk about datasets and how to", "tokens": [51052, 321, 764, 18473, 3873, 13, 682, 341, 700, 21060, 321, 434, 516, 281, 751, 466, 42856, 293, 577, 281, 51560], "temperature": 0.0, "avg_logprob": -0.09818374151471017, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.004030617419630289}, {"id": 7, "seek": 2984, "start": 53.760000000000005, "end": 59.760000000000005, "text": " create a high quality dataset so I want to mention the fact that there are basically two datasets", "tokens": [51560, 1884, 257, 1090, 3125, 28872, 370, 286, 528, 281, 2152, 264, 1186, 300, 456, 366, 1936, 732, 42856, 51860], "temperature": 0.0, "avg_logprob": -0.09818374151471017, "compression_ratio": 1.759090909090909, "no_speech_prob": 0.004030617419630289}, {"id": 8, "seek": 5984, "start": 60.720000000000006, "end": 66.16, "text": " several datasets we're interested in. The first one is instruction datasets where", "tokens": [50408, 2940, 42856, 321, 434, 3102, 294, 13, 440, 700, 472, 307, 10951, 42856, 689, 50680], "temperature": 0.0, "avg_logprob": -0.13288019952319918, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0028598906937986612}, {"id": 9, "seek": 5984, "start": 66.16, "end": 71.36, "text": " inputs are instructions this is basically how you use chart gpt like write me something and", "tokens": [50680, 15743, 366, 9415, 341, 307, 1936, 577, 291, 764, 6927, 290, 662, 411, 2464, 385, 746, 293, 50940], "temperature": 0.0, "avg_logprob": -0.13288019952319918, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0028598906937986612}, {"id": 10, "seek": 5984, "start": 71.36, "end": 77.92, "text": " the model is supposed to write you something and not complete what you just said because this is", "tokens": [50940, 264, 2316, 307, 3442, 281, 2464, 291, 746, 293, 406, 3566, 437, 291, 445, 848, 570, 341, 307, 51268], "temperature": 0.0, "avg_logprob": -0.13288019952319918, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0028598906937986612}, {"id": 11, "seek": 5984, "start": 77.92, "end": 84.08000000000001, "text": " real completion and this is part of the pre-training of these models they pre-train on a lot of data", "tokens": [51268, 957, 19372, 293, 341, 307, 644, 295, 264, 659, 12, 17227, 1760, 295, 613, 5245, 436, 659, 12, 83, 7146, 322, 257, 688, 295, 1412, 51576], "temperature": 0.0, "avg_logprob": -0.13288019952319918, "compression_ratio": 1.7922705314009661, "no_speech_prob": 0.0028598906937986612}, {"id": 12, "seek": 8408, "start": 84.16, "end": 90.96, "text": " and the task is next token prediction so they're just here to predict what the next word is going", "tokens": [50368, 293, 264, 5633, 307, 958, 14862, 17630, 370, 436, 434, 445, 510, 281, 6069, 437, 264, 958, 1349, 307, 516, 50708], "temperature": 0.0, "avg_logprob": -0.08092676207076671, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.003021403681486845}, {"id": 13, "seek": 8408, "start": 90.96, "end": 98.32, "text": " to be and this is nice but not really what we want to create assistance so instead we're going to", "tokens": [50708, 281, 312, 293, 341, 307, 1481, 457, 406, 534, 437, 321, 528, 281, 1884, 9683, 370, 2602, 321, 434, 516, 281, 51076], "temperature": 0.0, "avg_logprob": -0.08092676207076671, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.003021403681486845}, {"id": 14, "seek": 8408, "start": 98.32, "end": 104.64, "text": " use supervised fine-tuning to be able to turn a base model into a useful chat model here.", "tokens": [51076, 764, 46533, 2489, 12, 83, 37726, 281, 312, 1075, 281, 1261, 257, 3096, 2316, 666, 257, 4420, 5081, 2316, 510, 13, 51392], "temperature": 0.0, "avg_logprob": -0.08092676207076671, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.003021403681486845}, {"id": 15, "seek": 8408, "start": 105.75999999999999, "end": 111.52, "text": " There are also preference datasets I want to mention it briefly this is reinforcement", "tokens": [51448, 821, 366, 611, 17502, 42856, 286, 528, 281, 2152, 309, 10515, 341, 307, 29280, 51736], "temperature": 0.0, "avg_logprob": -0.08092676207076671, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.003021403681486845}, {"id": 16, "seek": 11152, "start": 111.52, "end": 118.56, "text": " learning from human feedback this is often used after the supervised fine-tuning process and in", "tokens": [50364, 2539, 490, 1952, 5824, 341, 307, 2049, 1143, 934, 264, 46533, 2489, 12, 83, 37726, 1399, 293, 294, 50716], "temperature": 0.0, "avg_logprob": -0.047491195656004404, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.0018939091823995113}, {"id": 17, "seek": 11152, "start": 118.56, "end": 125.28, "text": " these datasets you will find different answers and some kind of ranking of these answers to say", "tokens": [50716, 613, 42856, 291, 486, 915, 819, 6338, 293, 512, 733, 295, 17833, 295, 613, 6338, 281, 584, 51052], "temperature": 0.0, "avg_logprob": -0.047491195656004404, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.0018939091823995113}, {"id": 18, "seek": 11152, "start": 125.28, "end": 130.24, "text": " hey this one is more useful than the other one we're going to mention it during the fine-tuning", "tokens": [51052, 4177, 341, 472, 307, 544, 4420, 813, 264, 661, 472, 321, 434, 516, 281, 2152, 309, 1830, 264, 2489, 12, 83, 37726, 51300], "temperature": 0.0, "avg_logprob": -0.047491195656004404, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.0018939091823995113}, {"id": 19, "seek": 11152, "start": 130.24, "end": 135.92, "text": " process in the second notebook and finally there are other types of dataset it can be sentence", "tokens": [51300, 1399, 294, 264, 1150, 21060, 293, 2721, 456, 366, 661, 3467, 295, 28872, 309, 393, 312, 8174, 51584], "temperature": 0.0, "avg_logprob": -0.047491195656004404, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.0018939091823995113}, {"id": 20, "seek": 13592, "start": 135.92, "end": 141.67999999999998, "text": " classification it can be code dataset where you have a fill in the middle objective where you", "tokens": [50364, 21538, 309, 393, 312, 3089, 28872, 689, 291, 362, 257, 2836, 294, 264, 2808, 10024, 689, 291, 50652], "temperature": 0.0, "avg_logprob": -0.0644215616312894, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.004599886946380138}, {"id": 21, "seek": 13592, "start": 141.67999999999998, "end": 148.88, "text": " want to fill in code that has some context so before your cursor and after your cursor and", "tokens": [50652, 528, 281, 2836, 294, 3089, 300, 575, 512, 4319, 370, 949, 428, 28169, 293, 934, 428, 28169, 293, 51012], "temperature": 0.0, "avg_logprob": -0.0644215616312894, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.004599886946380138}, {"id": 22, "seek": 13592, "start": 148.88, "end": 154.16, "text": " this is where you want the code to be but we're not going to talk about this it was just a general", "tokens": [51012, 341, 307, 689, 291, 528, 264, 3089, 281, 312, 457, 321, 434, 406, 516, 281, 751, 466, 341, 309, 390, 445, 257, 2674, 51276], "temperature": 0.0, "avg_logprob": -0.0644215616312894, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.004599886946380138}, {"id": 23, "seek": 13592, "start": 154.16, "end": 160.07999999999998, "text": " overview of the kind of dataset you can expect in this space so as mentioned here we're just going", "tokens": [51276, 12492, 295, 264, 733, 295, 28872, 291, 393, 2066, 294, 341, 1901, 370, 382, 2835, 510, 321, 434, 445, 516, 51572], "temperature": 0.0, "avg_logprob": -0.0644215616312894, "compression_ratio": 1.8018867924528301, "no_speech_prob": 0.004599886946380138}, {"id": 24, "seek": 16008, "start": 160.16000000000003, "end": 164.8, "text": " to use supervised fine-tuning so with an instruction dataset and we're going to be a lot", "tokens": [50368, 281, 764, 46533, 2489, 12, 83, 37726, 370, 365, 364, 10951, 28872, 293, 321, 434, 516, 281, 312, 257, 688, 50600], "temperature": 0.0, "avg_logprob": -0.08486049094896637, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.01241728663444519}, {"id": 25, "seek": 16008, "start": 164.8, "end": 172.4, "text": " owned by filtering an existing dataset so the one that we're going to use today is the open", "tokens": [50600, 11684, 538, 30822, 364, 6741, 28872, 370, 264, 472, 300, 321, 434, 516, 281, 764, 965, 307, 264, 1269, 50980], "temperature": 0.0, "avg_logprob": -0.08486049094896637, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.01241728663444519}, {"id": 26, "seek": 16008, "start": 172.4, "end": 178.16000000000003, "text": " platypus dataset it's a collection of different datasets actually this is already a dataset that", "tokens": [50980, 3403, 88, 31624, 28872, 309, 311, 257, 5765, 295, 819, 42856, 767, 341, 307, 1217, 257, 28872, 300, 51268], "temperature": 0.0, "avg_logprob": -0.08486049094896637, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.01241728663444519}, {"id": 27, "seek": 16008, "start": 178.16000000000003, "end": 184.16000000000003, "text": " has been filtered using all the dataset but we're going to do it even more we can check what it looks", "tokens": [51268, 575, 668, 37111, 1228, 439, 264, 28872, 457, 321, 434, 516, 281, 360, 309, 754, 544, 321, 393, 1520, 437, 309, 1542, 51568], "temperature": 0.0, "avg_logprob": -0.08486049094896637, "compression_ratio": 1.7877358490566038, "no_speech_prob": 0.01241728663444519}, {"id": 28, "seek": 18416, "start": 184.16, "end": 192.16, "text": " like on the hugging face page here so here you have the instructions for example the board game", "tokens": [50364, 411, 322, 264, 41706, 1851, 3028, 510, 370, 510, 291, 362, 264, 9415, 337, 1365, 264, 3150, 1216, 50764], "temperature": 0.0, "avg_logprob": -0.08844496987082741, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00999284628778696}, {"id": 29, "seek": 18416, "start": 192.16, "end": 197.76, "text": " speeder and divider into three parts blah blah blah and the output that is expected by the model", "tokens": [50764, 3073, 260, 293, 3414, 1438, 666, 1045, 3166, 12288, 12288, 12288, 293, 264, 5598, 300, 307, 5176, 538, 264, 2316, 51044], "temperature": 0.0, "avg_logprob": -0.08844496987082741, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00999284628778696}, {"id": 30, "seek": 18416, "start": 197.76, "end": 202.72, "text": " so this is basically what your model is going to learn during the supervised fine-tuning process", "tokens": [51044, 370, 341, 307, 1936, 437, 428, 2316, 307, 516, 281, 1466, 1830, 264, 46533, 2489, 12, 83, 37726, 1399, 51292], "temperature": 0.0, "avg_logprob": -0.08844496987082741, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00999284628778696}, {"id": 31, "seek": 18416, "start": 202.72, "end": 209.92, "text": " it's going to learn to output this answer when it has this instruction and we repeat it a lot of", "tokens": [51292, 309, 311, 516, 281, 1466, 281, 5598, 341, 1867, 562, 309, 575, 341, 10951, 293, 321, 7149, 309, 257, 688, 295, 51652], "temperature": 0.0, "avg_logprob": -0.08844496987082741, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.00999284628778696}, {"id": 32, "seek": 20992, "start": 209.92, "end": 215.11999999999998, "text": " times and at some point the model is pretty good at understanding what it needs to do so it needs", "tokens": [50364, 1413, 293, 412, 512, 935, 264, 2316, 307, 1238, 665, 412, 3701, 437, 309, 2203, 281, 360, 370, 309, 2203, 50624], "temperature": 0.0, "avg_logprob": -0.037622309279167786, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.005806477274745703}, {"id": 33, "seek": 20992, "start": 215.11999999999998, "end": 223.27999999999997, "text": " to provide a useful answer for this instruction you have more information about the platypus dataset", "tokens": [50624, 281, 2893, 257, 4420, 1867, 337, 341, 10951, 291, 362, 544, 1589, 466, 264, 3403, 88, 31624, 28872, 51032], "temperature": 0.0, "avg_logprob": -0.037622309279167786, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.005806477274745703}, {"id": 34, "seek": 20992, "start": 223.27999999999997, "end": 230.16, "text": " and there's also a really nice paper you can find here about how they did it and how they trained", "tokens": [51032, 293, 456, 311, 611, 257, 534, 1481, 3035, 291, 393, 915, 510, 466, 577, 436, 630, 309, 293, 577, 436, 8895, 51376], "temperature": 0.0, "avg_logprob": -0.037622309279167786, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.005806477274745703}, {"id": 35, "seek": 20992, "start": 230.16, "end": 235.76, "text": " their models called platypus so an interesting read if you want to know more about it", "tokens": [51376, 641, 5245, 1219, 3403, 88, 31624, 370, 364, 1880, 1401, 498, 291, 528, 281, 458, 544, 466, 309, 51656], "temperature": 0.0, "avg_logprob": -0.037622309279167786, "compression_ratio": 1.7685185185185186, "no_speech_prob": 0.005806477274745703}, {"id": 36, "seek": 23576, "start": 236.56, "end": 243.12, "text": " so first of all let's start with the code we're going to need to install some libraries", "tokens": [50404, 370, 700, 295, 439, 718, 311, 722, 365, 264, 3089, 321, 434, 516, 281, 643, 281, 3625, 512, 15148, 50732], "temperature": 0.0, "avg_logprob": -0.1542109817755027, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.0018939953297376633}, {"id": 37, "seek": 23576, "start": 243.76, "end": 251.6, "text": " so here we're going to keep install a bunch of libraries we're going to install datasets the", "tokens": [50764, 370, 510, 321, 434, 516, 281, 1066, 3625, 257, 3840, 295, 15148, 321, 434, 516, 281, 3625, 42856, 264, 51156], "temperature": 0.0, "avg_logprob": -0.1542109817755027, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.0018939953297376633}, {"id": 38, "seek": 23576, "start": 251.6, "end": 258.48, "text": " library from hugging face you handle this also transformers very useful sentence transformers", "tokens": [51156, 6405, 490, 41706, 1851, 291, 4813, 341, 611, 4088, 433, 588, 4420, 8174, 4088, 433, 51500], "temperature": 0.0, "avg_logprob": -0.1542109817755027, "compression_ratio": 1.7564102564102564, "no_speech_prob": 0.0018939953297376633}, {"id": 39, "seek": 25848, "start": 258.48, "end": 266.8, "text": " too because we are going to use sentence transformers to create embeddings of our data we're going to", "tokens": [50364, 886, 570, 321, 366, 516, 281, 764, 8174, 4088, 433, 281, 1884, 12240, 29432, 295, 527, 1412, 321, 434, 516, 281, 50780], "temperature": 0.0, "avg_logprob": -0.09501928172699392, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.00869448110461235}, {"id": 40, "seek": 25848, "start": 266.8, "end": 275.68, "text": " see why in a few minutes and finally a vector database from facebook face gpu here you can see", "tokens": [50780, 536, 983, 294, 257, 1326, 2077, 293, 2721, 257, 8062, 8149, 490, 23372, 1851, 290, 34859, 510, 291, 393, 536, 51224], "temperature": 0.0, "avg_logprob": -0.09501928172699392, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.00869448110461235}, {"id": 41, "seek": 25848, "start": 275.68, "end": 283.6, "text": " the runtime so make sure you have a t4 gpu or more if you can afford it but the entire code should", "tokens": [51224, 264, 34474, 370, 652, 988, 291, 362, 257, 256, 19, 290, 34859, 420, 544, 498, 291, 393, 6157, 309, 457, 264, 2302, 3089, 820, 51620], "temperature": 0.0, "avg_logprob": -0.09501928172699392, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.00869448110461235}, {"id": 42, "seek": 28360, "start": 283.6, "end": 291.52000000000004, "text": " run using a free tier google collab so with a t4 gpu for the sake of this exercise i'm using a v100", "tokens": [50364, 1190, 1228, 257, 1737, 12362, 20742, 44228, 370, 365, 257, 256, 19, 290, 34859, 337, 264, 9717, 295, 341, 5380, 741, 478, 1228, 257, 371, 6879, 50760], "temperature": 0.0, "avg_logprob": -0.10039756321670985, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.011839158833026886}, {"id": 43, "seek": 28360, "start": 291.52000000000004, "end": 296.24, "text": " with higher ram because it's just going to be a bit faster and we won't have to stay at the screen", "tokens": [50760, 365, 2946, 10211, 570, 309, 311, 445, 516, 281, 312, 257, 857, 4663, 293, 321, 1582, 380, 362, 281, 1754, 412, 264, 2568, 50996], "temperature": 0.0, "avg_logprob": -0.10039756321670985, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.011839158833026886}, {"id": 44, "seek": 28360, "start": 296.24, "end": 303.12, "text": " for 30 minutes the second step that is really useful so this is the first time i tried to use it", "tokens": [50996, 337, 2217, 2077, 264, 1150, 1823, 300, 307, 534, 4420, 370, 341, 307, 264, 700, 565, 741, 3031, 281, 764, 309, 51340], "temperature": 0.0, "avg_logprob": -0.10039756321670985, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.011839158833026886}, {"id": 45, "seek": 28360, "start": 303.12, "end": 308.32000000000005, "text": " please bear with me this is quite a new feature in google collab but now we have a secret tab here", "tokens": [51340, 1767, 6155, 365, 385, 341, 307, 1596, 257, 777, 4111, 294, 20742, 44228, 457, 586, 321, 362, 257, 4054, 4421, 510, 51600], "temperature": 0.0, "avg_logprob": -0.10039756321670985, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.011839158833026886}, {"id": 46, "seek": 30832, "start": 308.32, "end": 314.71999999999997, "text": " on the left and as you can see here you can add a new secret and you can give it a name", "tokens": [50364, 322, 264, 1411, 293, 382, 291, 393, 536, 510, 291, 393, 909, 257, 777, 4054, 293, 291, 393, 976, 309, 257, 1315, 50684], "temperature": 0.0, "avg_logprob": -0.09014128967070244, "compression_ratio": 1.8926174496644295, "no_speech_prob": 0.019698765128850937}, {"id": 47, "seek": 30832, "start": 315.28, "end": 321.68, "text": " hugging face and a value so you can retrieve the value using this link if you have a hugging face", "tokens": [50712, 41706, 1851, 293, 257, 2158, 370, 291, 393, 30254, 264, 2158, 1228, 341, 2113, 498, 291, 362, 257, 41706, 1851, 51032], "temperature": 0.0, "avg_logprob": -0.09014128967070244, "compression_ratio": 1.8926174496644295, "no_speech_prob": 0.019698765128850937}, {"id": 48, "seek": 30832, "start": 321.68, "end": 329.28, "text": " account so here i collab i can copy it here i can write the hugging face here and copy paste the", "tokens": [51032, 2696, 370, 510, 741, 44228, 741, 393, 5055, 309, 510, 741, 393, 2464, 264, 41706, 1851, 510, 293, 5055, 9163, 264, 51412], "temperature": 0.0, "avg_logprob": -0.09014128967070244, "compression_ratio": 1.8926174496644295, "no_speech_prob": 0.019698765128850937}, {"id": 49, "seek": 32928, "start": 329.28, "end": 338.0, "text": " value here so and give access to the notebook um so this should be a really clean way of", "tokens": [50364, 2158, 510, 370, 293, 976, 2105, 281, 264, 21060, 1105, 370, 341, 820, 312, 257, 534, 2541, 636, 295, 50800], "temperature": 0.0, "avg_logprob": -0.12637147903442383, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.06670178472995758}, {"id": 50, "seek": 32928, "start": 338.0, "end": 342.47999999999996, "text": " managing your secrets because now they share across all your notebooks and they won't going to be", "tokens": [50800, 11642, 428, 14093, 570, 586, 436, 2073, 2108, 439, 428, 43782, 293, 436, 1582, 380, 516, 281, 312, 51024], "temperature": 0.0, "avg_logprob": -0.12637147903442383, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.06670178472995758}, {"id": 51, "seek": 32928, "start": 343.2, "end": 352.96, "text": " shared with people um other than in your account um to use it to jump in um we just got a question", "tokens": [51060, 5507, 365, 561, 1105, 661, 813, 294, 428, 2696, 1105, 281, 764, 309, 281, 3012, 294, 1105, 321, 445, 658, 257, 1168, 51548], "temperature": 0.0, "avg_logprob": -0.12637147903442383, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.06670178472995758}, {"id": 52, "seek": 32928, "start": 352.96, "end": 357.44, "text": " from the audience that i'm not sure what the answer is i don't know how widespread this problem is", "tokens": [51548, 490, 264, 4034, 300, 741, 478, 406, 988, 437, 264, 1867, 307, 741, 500, 380, 458, 577, 22679, 341, 1154, 307, 51772], "temperature": 0.0, "avg_logprob": -0.12637147903442383, "compression_ratio": 1.6768558951965065, "no_speech_prob": 0.06670178472995758}, {"id": 53, "seek": 35744, "start": 358.4, "end": 367.04, "text": " but do you know what an insure file accessible error message might be oh uh sorry i thought", "tokens": [50412, 457, 360, 291, 458, 437, 364, 1028, 540, 3991, 9515, 6713, 3636, 1062, 312, 1954, 2232, 2597, 741, 1194, 50844], "temperature": 0.0, "avg_logprob": -0.18664776195179333, "compression_ratio": 1.4661016949152543, "no_speech_prob": 0.004015865735709667}, {"id": 54, "seek": 35744, "start": 367.04, "end": 376.15999999999997, "text": " that we tested that uh let me let me just uh try out the links that are used okay", "tokens": [50844, 300, 321, 8246, 300, 2232, 718, 385, 718, 385, 445, 2232, 853, 484, 264, 6123, 300, 366, 1143, 1392, 51300], "temperature": 0.0, "avg_logprob": -0.18664776195179333, "compression_ratio": 1.4661016949152543, "no_speech_prob": 0.004015865735709667}, {"id": 55, "seek": 37616, "start": 377.12, "end": 388.0, "text": " uh anyone with the link should be able to access them actually uh some can some people", "tokens": [50412, 2232, 2878, 365, 264, 2113, 820, 312, 1075, 281, 2105, 552, 767, 2232, 512, 393, 512, 561, 50956], "temperature": 0.0, "avg_logprob": -0.09945659410385858, "compression_ratio": 1.8704663212435233, "no_speech_prob": 0.013708837330341339}, {"id": 56, "seek": 37616, "start": 388.0, "end": 392.16, "text": " confirm that you can access it which you can you access these notebooks uh i can access", "tokens": [50956, 9064, 300, 291, 393, 2105, 309, 597, 291, 393, 291, 2105, 613, 43782, 2232, 741, 393, 2105, 51164], "temperature": 0.0, "avg_logprob": -0.09945659410385858, "compression_ratio": 1.8704663212435233, "no_speech_prob": 0.013708837330341339}, {"id": 57, "seek": 37616, "start": 392.16, "end": 396.72, "text": " the notebooks one thing might be the case like you do need to be logged into google i think", "tokens": [51164, 264, 43782, 472, 551, 1062, 312, 264, 1389, 411, 291, 360, 643, 281, 312, 27231, 666, 20742, 741, 519, 51392], "temperature": 0.0, "avg_logprob": -0.09945659410385858, "compression_ratio": 1.8704663212435233, "no_speech_prob": 0.013708837330341339}, {"id": 58, "seek": 37616, "start": 396.72, "end": 401.92, "text": " in order to access colab so if you're not logged into google then you probably need to do that", "tokens": [51392, 294, 1668, 281, 2105, 1173, 455, 370, 498, 291, 434, 406, 27231, 666, 20742, 550, 291, 1391, 643, 281, 360, 300, 51652], "temperature": 0.0, "avg_logprob": -0.09945659410385858, "compression_ratio": 1.8704663212435233, "no_speech_prob": 0.013708837330341339}, {"id": 59, "seek": 40192, "start": 402.88, "end": 406.72, "text": " all right we'll continue for now um if anyone else has any problems please do let us know in", "tokens": [50412, 439, 558, 321, 603, 2354, 337, 586, 1105, 498, 2878, 1646, 575, 604, 2740, 1767, 360, 718, 505, 458, 294, 50604], "temperature": 0.0, "avg_logprob": -0.12167937021989089, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.01289096288383007}, {"id": 60, "seek": 40192, "start": 406.72, "end": 413.52000000000004, "text": " the chat of the comments uh yes please uh oh uh james just might be a company blocking it for", "tokens": [50604, 264, 5081, 295, 264, 3053, 2232, 2086, 1767, 2232, 1954, 2232, 361, 1632, 445, 1062, 312, 257, 2237, 17776, 309, 337, 50944], "temperature": 0.0, "avg_logprob": -0.12167937021989089, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.01289096288383007}, {"id": 61, "seek": 40192, "start": 413.52000000000004, "end": 419.84000000000003, "text": " security reasons yeah if you can't access any google products from uh your corporate uh laptop", "tokens": [50944, 3825, 4112, 1338, 498, 291, 393, 380, 2105, 604, 20742, 3383, 490, 2232, 428, 10896, 2232, 10732, 51260], "temperature": 0.0, "avg_logprob": -0.12167937021989089, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.01289096288383007}, {"id": 62, "seek": 40192, "start": 419.84000000000003, "end": 425.12, "text": " then yeah you probably have to find a different network unfortunately or just watch for now", "tokens": [51260, 550, 1338, 291, 1391, 362, 281, 915, 257, 819, 3209, 7015, 420, 445, 1159, 337, 586, 51524], "temperature": 0.0, "avg_logprob": -0.12167937021989089, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.01289096288383007}, {"id": 63, "seek": 40192, "start": 425.12, "end": 431.44, "text": " follow along uh when you get the recording yeah exactly uh sorry about that google colab can be", "tokens": [51524, 1524, 2051, 2232, 562, 291, 483, 264, 6613, 1338, 2293, 2232, 2597, 466, 300, 20742, 1173, 455, 393, 312, 51840], "temperature": 0.0, "avg_logprob": -0.12167937021989089, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.01289096288383007}, {"id": 64, "seek": 43192, "start": 431.92, "end": 440.0, "text": " a bit difficult to use in some in some context um here we're going to uh import our secret", "tokens": [50364, 257, 857, 2252, 281, 764, 294, 512, 294, 512, 4319, 1105, 510, 321, 434, 516, 281, 2232, 974, 527, 4054, 50768], "temperature": 0.0, "avg_logprob": -0.10651920279678033, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0038079835940152407}, {"id": 65, "seek": 43192, "start": 440.0, "end": 447.92, "text": " using uh google colab use the data use the data get so now the hf token um has the correct value", "tokens": [50768, 1228, 2232, 20742, 1173, 455, 764, 264, 1412, 764, 264, 1412, 483, 370, 586, 264, 276, 69, 14862, 1105, 575, 264, 3006, 2158, 51164], "temperature": 0.0, "avg_logprob": -0.10651920279678033, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0038079835940152407}, {"id": 66, "seek": 43192, "start": 447.92, "end": 452.40000000000003, "text": " and this is what we're going to use if we need it in the when we need it in the rest of this", "tokens": [51164, 293, 341, 307, 437, 321, 434, 516, 281, 764, 498, 321, 643, 309, 294, 264, 562, 321, 643, 309, 294, 264, 1472, 295, 341, 51388], "temperature": 0.0, "avg_logprob": -0.10651920279678033, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0038079835940152407}, {"id": 67, "seek": 43192, "start": 452.40000000000003, "end": 460.16, "text": " collab this is optional uh we are only going to need it when we want to upload uh the data set", "tokens": [51388, 44228, 341, 307, 17312, 2232, 321, 366, 787, 516, 281, 643, 309, 562, 321, 528, 281, 6580, 2232, 264, 1412, 992, 51776], "temperature": 0.0, "avg_logprob": -0.10651920279678033, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.0038079835940152407}, {"id": 68, "seek": 46016, "start": 460.16, "end": 465.92, "text": " so if you don't have a hugging face account you don't need to create one i recommend it because", "tokens": [50364, 370, 498, 291, 500, 380, 362, 257, 41706, 1851, 2696, 291, 500, 380, 643, 281, 1884, 472, 741, 2748, 309, 570, 50652], "temperature": 0.0, "avg_logprob": -0.05960703414419423, "compression_ratio": 1.859375, "no_speech_prob": 0.0009537800797261298}, {"id": 69, "seek": 46016, "start": 465.92, "end": 471.36, "text": " it's nicer and you'll be able to upload your own data set in your own account but if not it's okay", "tokens": [50652, 309, 311, 22842, 293, 291, 603, 312, 1075, 281, 6580, 428, 1065, 1412, 992, 294, 428, 1065, 2696, 457, 498, 406, 309, 311, 1392, 50924], "temperature": 0.0, "avg_logprob": -0.05960703414419423, "compression_ratio": 1.859375, "no_speech_prob": 0.0009537800797261298}, {"id": 70, "seek": 46016, "start": 472.08000000000004, "end": 475.76000000000005, "text": " i will upload it in my account and you'll be able to reuse it from this account", "tokens": [50960, 741, 486, 6580, 309, 294, 452, 2696, 293, 291, 603, 312, 1075, 281, 26225, 309, 490, 341, 2696, 51144], "temperature": 0.0, "avg_logprob": -0.05960703414419423, "compression_ratio": 1.859375, "no_speech_prob": 0.0009537800797261298}, {"id": 71, "seek": 46016, "start": 476.88, "end": 482.16, "text": " so now we have the hugging face token and the first thing that we want to do is to", "tokens": [51200, 370, 586, 321, 362, 264, 41706, 1851, 14862, 293, 264, 700, 551, 300, 321, 528, 281, 360, 307, 281, 51464], "temperature": 0.0, "avg_logprob": -0.05960703414419423, "compression_ratio": 1.859375, "no_speech_prob": 0.0009537800797261298}, {"id": 72, "seek": 48216, "start": 482.88000000000005, "end": 492.24, "text": " load the data set that we want to use so in this case it's the data set that was mentioned here so", "tokens": [50400, 3677, 264, 1412, 992, 300, 321, 528, 281, 764, 370, 294, 341, 1389, 309, 311, 264, 1412, 992, 300, 390, 2835, 510, 370, 50868], "temperature": 0.0, "avg_logprob": -0.07214937075762681, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.01824689656496048}, {"id": 73, "seek": 48216, "start": 492.24, "end": 501.76000000000005, "text": " open open platypus we can copy paste it from here and it should load the data set download it", "tokens": [50868, 1269, 1269, 3403, 88, 31624, 321, 393, 5055, 9163, 309, 490, 510, 293, 309, 820, 3677, 264, 1412, 992, 5484, 309, 51344], "temperature": 0.0, "avg_logprob": -0.07214937075762681, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.01824689656496048}, {"id": 74, "seek": 48216, "start": 501.76000000000005, "end": 507.68, "text": " and load it and we're going to see all the different columns inside of the data set so", "tokens": [51344, 293, 3677, 309, 293, 321, 434, 516, 281, 536, 439, 264, 819, 13766, 1854, 295, 264, 1412, 992, 370, 51640], "temperature": 0.0, "avg_logprob": -0.07214937075762681, "compression_ratio": 1.8235294117647058, "no_speech_prob": 0.01824689656496048}, {"id": 75, "seek": 50768, "start": 508.64, "end": 516.5600000000001, "text": " input output instruction and data source uh that should be correct here and we have the", "tokens": [50412, 4846, 5598, 10951, 293, 1412, 4009, 2232, 300, 820, 312, 3006, 510, 293, 321, 362, 264, 50808], "temperature": 0.0, "avg_logprob": -0.13956090983222513, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.0004875694285146892}, {"id": 76, "seek": 50768, "start": 516.5600000000001, "end": 527.52, "text": " number of rows so almost to 25k rows um so we can see a bit more about it we're going to", "tokens": [50808, 1230, 295, 13241, 370, 1920, 281, 3552, 74, 13241, 1105, 370, 321, 393, 536, 257, 857, 544, 466, 309, 321, 434, 516, 281, 51356], "temperature": 0.0, "avg_logprob": -0.13956090983222513, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.0004875694285146892}, {"id": 77, "seek": 50768, "start": 528.48, "end": 535.12, "text": " read it as a bandit's data set and we can even with google collab convert it into an interactive", "tokens": [51404, 1401, 309, 382, 257, 4116, 270, 311, 1412, 992, 293, 321, 393, 754, 365, 20742, 44228, 7620, 309, 666, 364, 15141, 51736], "temperature": 0.0, "avg_logprob": -0.13956090983222513, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.0004875694285146892}, {"id": 78, "seek": 53512, "start": 535.12, "end": 540.96, "text": " table it's a better way of looking at it unfortunately it can take some time for google", "tokens": [50364, 3199, 309, 311, 257, 1101, 636, 295, 1237, 412, 309, 7015, 309, 393, 747, 512, 565, 337, 20742, 50656], "temperature": 0.0, "avg_logprob": -0.09140723684559697, "compression_ratio": 1.8341463414634147, "no_speech_prob": 0.0015699145151302218}, {"id": 79, "seek": 53512, "start": 540.96, "end": 547.76, "text": " collab to convert it so we're going to see but yeah it's nice to see that we have the instruction", "tokens": [50656, 44228, 281, 7620, 309, 370, 321, 434, 516, 281, 536, 457, 1338, 309, 311, 1481, 281, 536, 300, 321, 362, 264, 10951, 50996], "temperature": 0.0, "avg_logprob": -0.09140723684559697, "compression_ratio": 1.8341463414634147, "no_speech_prob": 0.0015699145151302218}, {"id": 80, "seek": 53512, "start": 547.76, "end": 552.72, "text": " and we have the output and this is all that matters here we're going to explore a bit more", "tokens": [50996, 293, 321, 362, 264, 5598, 293, 341, 307, 439, 300, 7001, 510, 321, 434, 516, 281, 6839, 257, 857, 544, 51244], "temperature": 0.0, "avg_logprob": -0.09140723684559697, "compression_ratio": 1.8341463414634147, "no_speech_prob": 0.0015699145151302218}, {"id": 81, "seek": 53512, "start": 552.72, "end": 559.84, "text": " about instruction the output if it was a real data set real raw data set what we want to do at this", "tokens": [51244, 466, 10951, 264, 5598, 498, 309, 390, 257, 957, 1412, 992, 957, 8936, 1412, 992, 437, 321, 528, 281, 360, 412, 341, 51600], "temperature": 0.0, "avg_logprob": -0.09140723684559697, "compression_ratio": 1.8341463414634147, "no_speech_prob": 0.0015699145151302218}, {"id": 82, "seek": 55984, "start": 559.84, "end": 567.12, "text": " point is really read not every line because it's too too too much but a lot of these lines", "tokens": [50364, 935, 307, 534, 1401, 406, 633, 1622, 570, 309, 311, 886, 886, 886, 709, 457, 257, 688, 295, 613, 3876, 50728], "temperature": 0.0, "avg_logprob": -0.09221377902560764, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.011131824925541878}, {"id": 83, "seek": 55984, "start": 567.12, "end": 573.2800000000001, "text": " and be able to get a good understanding of what's in this data set and what we expect and also like", "tokens": [50728, 293, 312, 1075, 281, 483, 257, 665, 3701, 295, 437, 311, 294, 341, 1412, 992, 293, 437, 321, 2066, 293, 611, 411, 51036], "temperature": 0.0, "avg_logprob": -0.09221377902560764, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.011131824925541878}, {"id": 84, "seek": 55984, "start": 573.2800000000001, "end": 580.4, "text": " just clean it like if there are samples that are not high quality that have like bad english that are", "tokens": [51036, 445, 2541, 309, 411, 498, 456, 366, 10938, 300, 366, 406, 1090, 3125, 300, 362, 411, 1578, 32169, 300, 366, 51392], "temperature": 0.0, "avg_logprob": -0.09221377902560764, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.011131824925541878}, {"id": 85, "seek": 55984, "start": 580.4, "end": 586.72, "text": " just plain wrong we want to remove them this is very important we want to create the best", "tokens": [51392, 445, 11121, 2085, 321, 528, 281, 4159, 552, 341, 307, 588, 1021, 321, 528, 281, 1884, 264, 1151, 51708], "temperature": 0.0, "avg_logprob": -0.09221377902560764, "compression_ratio": 1.7442922374429224, "no_speech_prob": 0.011131824925541878}, {"id": 86, "seek": 58672, "start": 587.28, "end": 594.32, "text": " data set possible so it means filtering out a lot of samples okay so now we have this data set we", "tokens": [50392, 1412, 992, 1944, 370, 309, 1355, 30822, 484, 257, 688, 295, 10938, 1392, 370, 586, 321, 362, 341, 1412, 992, 321, 50744], "temperature": 0.0, "avg_logprob": -0.09795471740095582, "compression_ratio": 1.6900584795321638, "no_speech_prob": 0.0022160254884511232}, {"id": 87, "seek": 58672, "start": 594.32, "end": 603.44, "text": " can read everything here this is a lot of data so we won't do it in this case it could take too", "tokens": [50744, 393, 1401, 1203, 510, 341, 307, 257, 688, 295, 1412, 370, 321, 1582, 380, 360, 309, 294, 341, 1389, 309, 727, 747, 886, 51200], "temperature": 0.0, "avg_logprob": -0.09795471740095582, "compression_ratio": 1.6900584795321638, "no_speech_prob": 0.0022160254884511232}, {"id": 88, "seek": 58672, "start": 603.44, "end": 613.76, "text": " much time but now something that we can do let me show you once again the the code here so load", "tokens": [51200, 709, 565, 457, 586, 746, 300, 321, 393, 360, 718, 385, 855, 291, 1564, 797, 264, 264, 3089, 510, 370, 3677, 51716], "temperature": 0.0, "avg_logprob": -0.09795471740095582, "compression_ratio": 1.6900584795321638, "no_speech_prob": 0.0022160254884511232}, {"id": 89, "seek": 61376, "start": 613.76, "end": 620.56, "text": " data set and then the name of the data and what we are going to do here is that we're going to", "tokens": [50364, 1412, 992, 293, 550, 264, 1315, 295, 264, 1412, 293, 437, 321, 366, 516, 281, 360, 510, 307, 300, 321, 434, 516, 281, 50704], "temperature": 0.0, "avg_logprob": -0.11839449232903079, "compression_ratio": 1.78343949044586, "no_speech_prob": 0.00467461533844471}, {"id": 90, "seek": 61376, "start": 620.56, "end": 630.64, "text": " use the transformers library to import the tokenizer this will convert the row text into tokens", "tokens": [50704, 764, 264, 4088, 433, 6405, 281, 974, 264, 14862, 6545, 341, 486, 7620, 264, 5386, 2487, 666, 22667, 51208], "temperature": 0.0, "avg_logprob": -0.11839449232903079, "compression_ratio": 1.78343949044586, "no_speech_prob": 0.00467461533844471}, {"id": 91, "seek": 61376, "start": 631.2, "end": 640.0, "text": " then we will import the matplotlib library to plot the results and also the cborn library", "tokens": [51236, 550, 321, 486, 974, 264, 3803, 564, 310, 38270, 6405, 281, 7542, 264, 3542, 293, 611, 264, 269, 10324, 6405, 51676], "temperature": 0.0, "avg_logprob": -0.11839449232903079, "compression_ratio": 1.78343949044586, "no_speech_prob": 0.00467461533844471}, {"id": 92, "seek": 64000, "start": 640.0, "end": 650.08, "text": " for the same reasons first we want to import the tokenizer so in our case we want the tokenizer not", "tokens": [50364, 337, 264, 912, 4112, 700, 321, 528, 281, 974, 264, 14862, 6545, 370, 294, 527, 1389, 321, 528, 264, 14862, 6545, 406, 50868], "temperature": 0.0, "avg_logprob": -0.11138687963071077, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.00939931720495224}, {"id": 93, "seek": 64000, "start": 650.08, "end": 655.28, "text": " from but as suggested here but from lamatu because this is the model that we want to use", "tokens": [50868, 490, 457, 382, 10945, 510, 457, 490, 24688, 20546, 570, 341, 307, 264, 2316, 300, 321, 528, 281, 764, 51128], "temperature": 0.0, "avg_logprob": -0.11138687963071077, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.00939931720495224}, {"id": 94, "seek": 64000, "start": 656.4, "end": 665.84, "text": " so we're going to use a new research version of lamatu and not the official one from meta", "tokens": [51184, 370, 321, 434, 516, 281, 764, 257, 777, 2132, 3037, 295, 24688, 20546, 293, 406, 264, 4783, 472, 490, 19616, 51656], "temperature": 0.0, "avg_logprob": -0.11138687963071077, "compression_ratio": 1.705521472392638, "no_speech_prob": 0.00939931720495224}, {"id": 95, "seek": 66584, "start": 665.84, "end": 671.44, "text": " why that it's because if you don't have a hugging face account you will not be able to access the", "tokens": [50364, 983, 300, 309, 311, 570, 498, 291, 500, 380, 362, 257, 41706, 1851, 2696, 291, 486, 406, 312, 1075, 281, 2105, 264, 50644], "temperature": 0.0, "avg_logprob": -0.0933280247513966, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013876070734113455}, {"id": 96, "seek": 66584, "start": 672.08, "end": 678.24, "text": " official version of it so news research just re-uploaded the entire thing so now we have", "tokens": [50676, 4783, 3037, 295, 309, 370, 2583, 2132, 445, 319, 12, 84, 21132, 12777, 264, 2302, 551, 370, 586, 321, 362, 50984], "temperature": 0.0, "avg_logprob": -0.0933280247513966, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013876070734113455}, {"id": 97, "seek": 66584, "start": 678.24, "end": 685.6800000000001, "text": " the tokenizer we can test it here it's going to download it once again from hugging face and", "tokens": [50984, 264, 14862, 6545, 321, 393, 1500, 309, 510, 309, 311, 516, 281, 5484, 309, 1564, 797, 490, 41706, 1851, 293, 51356], "temperature": 0.0, "avg_logprob": -0.0933280247513966, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013876070734113455}, {"id": 98, "seek": 66584, "start": 685.6800000000001, "end": 694.32, "text": " and then we're going to print it once it's done here you can sit with all the special tokens", "tokens": [51356, 293, 550, 321, 434, 516, 281, 4482, 309, 1564, 309, 311, 1096, 510, 291, 393, 1394, 365, 439, 264, 2121, 22667, 51788], "temperature": 0.0, "avg_logprob": -0.0933280247513966, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0013876070734113455}, {"id": 99, "seek": 69432, "start": 694.32, "end": 703.2, "text": " unknown etc when it's done we want to use it to tokenize our instructions here and also outputs", "tokens": [50364, 9841, 5183, 562, 309, 311, 1096, 321, 528, 281, 764, 309, 281, 14862, 1125, 527, 9415, 510, 293, 611, 23930, 50808], "temperature": 0.0, "avg_logprob": -0.14616351326306662, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.0052148401737213135}, {"id": 100, "seek": 69432, "start": 703.2, "end": 713.0400000000001, "text": " here so the way that we're going to do it we're going to create a table an area called instruction", "tokens": [50808, 510, 370, 264, 636, 300, 321, 434, 516, 281, 360, 309, 321, 434, 516, 281, 1884, 257, 3199, 364, 1859, 1219, 10951, 51300], "temperature": 0.0, "avg_logprob": -0.14616351326306662, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.0052148401737213135}, {"id": 101, "seek": 71304, "start": 713.12, "end": 723.92, "text": " token counts it's a bit of a ghost but and we're going to use the tokenizer to tokenize", "tokens": [50368, 14862, 14893, 309, 311, 257, 857, 295, 257, 8359, 457, 293, 321, 434, 516, 281, 764, 264, 14862, 6545, 281, 14862, 1125, 50908], "temperature": 0.0, "avg_logprob": -0.15990156823016227, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.015420311130583286}, {"id": 102, "seek": 71304, "start": 724.56, "end": 734.88, "text": " every sample in our data set this is not correct actually this is why you should not always trust", "tokens": [50940, 633, 6889, 294, 527, 1412, 992, 341, 307, 406, 3006, 767, 341, 307, 983, 291, 820, 406, 1009, 3361, 51456], "temperature": 0.0, "avg_logprob": -0.15990156823016227, "compression_ratio": 1.434108527131783, "no_speech_prob": 0.015420311130583286}, {"id": 103, "seek": 73488, "start": 735.4399999999999, "end": 751.6, "text": " coder lambs for example in our data set train and close it so here in this table we should get", "tokens": [50392, 17656, 260, 24688, 929, 337, 1365, 294, 527, 1412, 992, 3847, 293, 1998, 309, 370, 510, 294, 341, 3199, 321, 820, 483, 51200], "temperature": 0.0, "avg_logprob": -0.15608144336276583, "compression_ratio": 1.441860465116279, "no_speech_prob": 0.007935570552945137}, {"id": 104, "seek": 73488, "start": 751.6, "end": 757.4399999999999, "text": " like the token counts for every instruction and we're going to repeat this process with the", "tokens": [51200, 411, 264, 14862, 14893, 337, 633, 10951, 293, 321, 434, 516, 281, 7149, 341, 1399, 365, 264, 51492], "temperature": 0.0, "avg_logprob": -0.15608144336276583, "compression_ratio": 1.441860465116279, "no_speech_prob": 0.007935570552945137}, {"id": 105, "seek": 75744, "start": 758.24, "end": 766.32, "text": " output so pretty much the same thing here and we want the output and finally we want to", "tokens": [50404, 5598, 370, 1238, 709, 264, 912, 551, 510, 293, 321, 528, 264, 5598, 293, 2721, 321, 528, 281, 50808], "temperature": 0.0, "avg_logprob": -0.11017042253075576, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.008059420622885227}, {"id": 106, "seek": 75744, "start": 766.32, "end": 775.6, "text": " combine the instruction in the output because this is like the entire data set and so instruction", "tokens": [50808, 10432, 264, 10951, 294, 264, 5598, 570, 341, 307, 411, 264, 2302, 1412, 992, 293, 370, 10951, 51272], "temperature": 0.0, "avg_logprob": -0.11017042253075576, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.008059420622885227}, {"id": 107, "seek": 77560, "start": 775.6800000000001, "end": 788.08, "text": " plus output and in this case we want to call it combine token counts and here we're going to merge", "tokens": [50368, 1804, 5598, 293, 294, 341, 1389, 321, 528, 281, 818, 309, 10432, 14862, 14893, 293, 510, 321, 434, 516, 281, 22183, 50988], "temperature": 0.0, "avg_logprob": -0.12178389569546313, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.03787223994731903}, {"id": 108, "seek": 77560, "start": 788.08, "end": 803.52, "text": " these two tables for instruction outputs in zip etc and we should get like the the sum of all the", "tokens": [50988, 613, 732, 8020, 337, 10951, 23930, 294, 20730, 5183, 293, 321, 820, 483, 411, 264, 264, 2408, 295, 439, 264, 51760], "temperature": 0.0, "avg_logprob": -0.12178389569546313, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.03787223994731903}, {"id": 109, "seek": 80352, "start": 804.16, "end": 812.96, "text": " tokens here in both instruction outputs now that is done we are going to do a little function to", "tokens": [50396, 22667, 510, 294, 1293, 10951, 23930, 586, 300, 307, 1096, 321, 366, 516, 281, 360, 257, 707, 2445, 281, 50836], "temperature": 0.0, "avg_logprob": -0.09563979895218559, "compression_ratio": 1.4885496183206106, "no_speech_prob": 0.000910581205971539}, {"id": 110, "seek": 80352, "start": 812.96, "end": 820.64, "text": " plot the distribution of our token so we can have an easy visualization of it so here I'm going to", "tokens": [50836, 7542, 264, 7316, 295, 527, 14862, 370, 321, 393, 362, 364, 1858, 25801, 295, 309, 370, 510, 286, 478, 516, 281, 51220], "temperature": 0.0, "avg_logprob": -0.09563979895218559, "compression_ratio": 1.4885496183206106, "no_speech_prob": 0.000910581205971539}, {"id": 111, "seek": 82064, "start": 820.72, "end": 832.8, "text": " use sns set style white grid and slowly but surely get everything here", "tokens": [50368, 764, 262, 3695, 992, 3758, 2418, 10748, 293, 5692, 457, 11468, 483, 1203, 510, 50972], "temperature": 0.0, "avg_logprob": -0.17049987141678974, "compression_ratio": 1.3513513513513513, "no_speech_prob": 0.01770791783928871}, {"id": 112, "seek": 82064, "start": 838.0, "end": 844.64, "text": " actually I'm sorry but I'm going to copy paste it so it's a bit faster to do it", "tokens": [51232, 767, 286, 478, 2597, 457, 286, 478, 516, 281, 5055, 9163, 309, 370, 309, 311, 257, 857, 4663, 281, 360, 309, 51564], "temperature": 0.0, "avg_logprob": -0.17049987141678974, "compression_ratio": 1.3513513513513513, "no_speech_prob": 0.01770791783928871}, {"id": 113, "seek": 84464, "start": 845.28, "end": 852.64, "text": " and now we can plot the distribution for every instruction output and combine token count", "tokens": [50396, 293, 586, 321, 393, 7542, 264, 7316, 337, 633, 10951, 5598, 293, 10432, 14862, 1207, 50764], "temperature": 0.0, "avg_logprob": -0.11062070528666178, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0021814091596752405}, {"id": 114, "seek": 84464, "start": 853.52, "end": 862.48, "text": " so this is a very simple plot and we can call it using plot distribution and here we're going to", "tokens": [50808, 370, 341, 307, 257, 588, 2199, 7542, 293, 321, 393, 818, 309, 1228, 7542, 7316, 293, 510, 321, 434, 516, 281, 51256], "temperature": 0.0, "avg_logprob": -0.11062070528666178, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0021814091596752405}, {"id": 115, "seek": 84464, "start": 862.48, "end": 868.64, "text": " first use instruction token counts and we're going to see the distribution of token counts", "tokens": [51256, 700, 764, 10951, 14862, 14893, 293, 321, 434, 516, 281, 536, 264, 7316, 295, 14862, 14893, 51564], "temperature": 0.0, "avg_logprob": -0.11062070528666178, "compression_ratio": 1.910344827586207, "no_speech_prob": 0.0021814091596752405}, {"id": 116, "seek": 86864, "start": 869.28, "end": 872.0, "text": " for instruction on A", "tokens": [50396, 337, 10951, 322, 316, 50532], "temperature": 0.0, "avg_logprob": -0.1807397512289194, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0014767153188586235}, {"id": 117, "seek": 86864, "start": 880.88, "end": 887.04, "text": " this part takes a bit of time unfortunately because yeah it needs to tokenize the entire data set", "tokens": [50976, 341, 644, 2516, 257, 857, 295, 565, 7015, 570, 1338, 309, 2203, 281, 14862, 1125, 264, 2302, 1412, 992, 51284], "temperature": 0.0, "avg_logprob": -0.1807397512289194, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0014767153188586235}, {"id": 118, "seek": 86864, "start": 890.3199999999999, "end": 897.2, "text": " but once you've done it you can basically comment it it's okay and we're going to repeat this process", "tokens": [51448, 457, 1564, 291, 600, 1096, 309, 291, 393, 1936, 2871, 309, 309, 311, 1392, 293, 321, 434, 516, 281, 7149, 341, 1399, 51792], "temperature": 0.0, "avg_logprob": -0.1807397512289194, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0014767153188586235}, {"id": 119, "seek": 89864, "start": 898.72, "end": 905.6, "text": " with the output token counts so this time it's for output only and finally the", "tokens": [50368, 365, 264, 5598, 14862, 14893, 370, 341, 565, 309, 311, 337, 5598, 787, 293, 2721, 264, 50712], "temperature": 0.0, "avg_logprob": -0.07899499761647191, "compression_ratio": 1.6225165562913908, "no_speech_prob": 0.0010482966899871826}, {"id": 120, "seek": 89864, "start": 907.04, "end": 916.4, "text": " combine token counts so here it's for instruction plus output here I'm going to", "tokens": [50784, 10432, 14862, 14893, 370, 510, 309, 311, 337, 10951, 1804, 5598, 510, 286, 478, 516, 281, 51252], "temperature": 0.0, "avg_logprob": -0.07899499761647191, "compression_ratio": 1.6225165562913908, "no_speech_prob": 0.0010482966899871826}, {"id": 121, "seek": 89864, "start": 916.4, "end": 926.24, "text": " comment it so we get faster results and now we have a distribution for all of our data", "tokens": [51252, 2871, 309, 370, 321, 483, 4663, 3542, 293, 586, 321, 362, 257, 7316, 337, 439, 295, 527, 1412, 51744], "temperature": 0.0, "avg_logprob": -0.07899499761647191, "compression_ratio": 1.6225165562913908, "no_speech_prob": 0.0010482966899871826}, {"id": 122, "seek": 92624, "start": 926.88, "end": 934.4, "text": " so here you can see that we have approximately like the the minis is around here so around", "tokens": [50396, 370, 510, 291, 393, 536, 300, 321, 362, 10447, 411, 264, 264, 923, 271, 307, 926, 510, 370, 926, 50772], "temperature": 0.0, "avg_logprob": -0.08892481826072515, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0023939816746860743}, {"id": 123, "seek": 92624, "start": 934.96, "end": 942.08, "text": " 500 tokens there's a long tail distribution goes up to like 5000 tokens why is it important", "tokens": [50800, 5923, 22667, 456, 311, 257, 938, 6838, 7316, 1709, 493, 281, 411, 23777, 22667, 983, 307, 309, 1021, 51156], "temperature": 0.0, "avg_logprob": -0.08892481826072515, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0023939816746860743}, {"id": 124, "seek": 92624, "start": 942.08, "end": 948.88, "text": " why does it matter it's because these models they have a certain context window and if it's", "tokens": [51156, 983, 775, 309, 1871, 309, 311, 570, 613, 5245, 436, 362, 257, 1629, 4319, 4910, 293, 498, 309, 311, 51496], "temperature": 0.0, "avg_logprob": -0.08892481826072515, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0023939816746860743}, {"id": 125, "seek": 92624, "start": 948.88, "end": 955.44, "text": " because beyond this context window it's not going to be very helpful so it's important to know like", "tokens": [51496, 570, 4399, 341, 4319, 4910, 309, 311, 406, 516, 281, 312, 588, 4961, 370, 309, 311, 1021, 281, 458, 411, 51824], "temperature": 0.0, "avg_logprob": -0.08892481826072515, "compression_ratio": 1.7476635514018692, "no_speech_prob": 0.0023939816746860743}, {"id": 126, "seek": 95544, "start": 955.44, "end": 961.6800000000001, "text": " the the number of tokens in our data set we also maybe want to sample more from", "tokens": [50364, 264, 264, 1230, 295, 22667, 294, 527, 1412, 992, 321, 611, 1310, 528, 281, 6889, 544, 490, 50676], "temperature": 0.0, "avg_logprob": -0.12735295877224062, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0018629557453095913}, {"id": 127, "seek": 95544, "start": 963.36, "end": 967.44, "text": " samples that have more tokens because they're going to be more informative than others", "tokens": [50760, 10938, 300, 362, 544, 22667, 570, 436, 434, 516, 281, 312, 544, 27759, 813, 2357, 50964], "temperature": 0.0, "avg_logprob": -0.12735295877224062, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0018629557453095913}, {"id": 128, "seek": 95544, "start": 968.1600000000001, "end": 975.84, "text": " but we'll see about that basically here we can see okay I'm going to put a certain threshold", "tokens": [51000, 457, 321, 603, 536, 466, 300, 1936, 510, 321, 393, 536, 1392, 286, 478, 516, 281, 829, 257, 1629, 14678, 51384], "temperature": 0.0, "avg_logprob": -0.12735295877224062, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0018629557453095913}, {"id": 129, "seek": 95544, "start": 975.84, "end": 983.6, "text": " for this data set at 2k tokens the max context size for lambda 2 is actually 4k but", "tokens": [51384, 337, 341, 1412, 992, 412, 568, 74, 22667, 264, 11469, 4319, 2744, 337, 13607, 568, 307, 767, 1017, 74, 457, 51772], "temperature": 0.0, "avg_logprob": -0.12735295877224062, "compression_ratio": 1.6490384615384615, "no_speech_prob": 0.0018629557453095913}, {"id": 130, "seek": 98360, "start": 983.6, "end": 990.8000000000001, "text": " this is just an example to show how we can just set a threshold so here we want to filter out", "tokens": [50364, 341, 307, 445, 364, 1365, 281, 855, 577, 321, 393, 445, 992, 257, 14678, 370, 510, 321, 528, 281, 6608, 484, 50724], "temperature": 0.0, "avg_logprob": -0.14475991774578484, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.005549038294702768}, {"id": 131, "seek": 98360, "start": 990.8000000000001, "end": 1004.72, "text": " rows with more than 2000 samples so one way of doing it is to say if I count in numeric combined", "tokens": [50724, 13241, 365, 544, 813, 8132, 10938, 370, 472, 636, 295, 884, 309, 307, 281, 584, 498, 286, 1207, 294, 7866, 299, 9354, 51420], "temperature": 0.0, "avg_logprob": -0.14475991774578484, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.005549038294702768}, {"id": 132, "seek": 100472, "start": 1005.6800000000001, "end": 1020.4, "text": " token counts if count okay so here we're going to retrieve the index of every sample where the", "tokens": [50412, 14862, 14893, 498, 1207, 1392, 370, 510, 321, 434, 516, 281, 30254, 264, 8186, 295, 633, 6889, 689, 264, 51148], "temperature": 0.0, "avg_logprob": -0.15110032823350694, "compression_ratio": 1.46875, "no_speech_prob": 0.005539607722312212}, {"id": 133, "seek": 100472, "start": 1020.4, "end": 1030.88, "text": " count in combined token counts is lower than 2k and now we can print how many of this we have", "tokens": [51148, 1207, 294, 9354, 14862, 14893, 307, 3126, 813, 568, 74, 293, 586, 321, 393, 4482, 577, 867, 295, 341, 321, 362, 51672], "temperature": 0.0, "avg_logprob": -0.15110032823350694, "compression_ratio": 1.46875, "no_speech_prob": 0.005539607722312212}, {"id": 134, "seek": 103472, "start": 1034.96, "end": 1037.52, "text": " okay and if we print", "tokens": [50376, 1392, 293, 498, 321, 4482, 50504], "temperature": 0.0, "avg_logprob": -0.08059385495308118, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.001031720545142889}, {"id": 135, "seek": 103472, "start": 1040.88, "end": 1048.96, "text": " length of the data set in train and we are going to remove the length of the valid indices", "tokens": [50672, 4641, 295, 264, 1412, 992, 294, 3847, 293, 321, 366, 516, 281, 4159, 264, 4641, 295, 264, 7363, 43840, 51076], "temperature": 0.0, "avg_logprob": -0.08059385495308118, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.001031720545142889}, {"id": 136, "seek": 103472, "start": 1048.96, "end": 1055.3600000000001, "text": " so we see how many we remove okay we only remove 31 of them but that's fine you can have a more", "tokens": [51076, 370, 321, 536, 577, 867, 321, 4159, 1392, 321, 787, 4159, 10353, 295, 552, 457, 300, 311, 2489, 291, 393, 362, 257, 544, 51396], "temperature": 0.0, "avg_logprob": -0.08059385495308118, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.001031720545142889}, {"id": 137, "seek": 103472, "start": 1055.3600000000001, "end": 1061.1200000000001, "text": " aggressive threshold if you want in this case yeah we're going to remove just a few of them as you", "tokens": [51396, 10762, 14678, 498, 291, 528, 294, 341, 1389, 1338, 321, 434, 516, 281, 4159, 445, 257, 1326, 295, 552, 382, 291, 51684], "temperature": 0.0, "avg_logprob": -0.08059385495308118, "compression_ratio": 1.728813559322034, "no_speech_prob": 0.001031720545142889}, {"id": 138, "seek": 106112, "start": 1061.12, "end": 1068.32, "text": " can see here then we're going to extract the valid rows based on the indices so here we need to", "tokens": [50364, 393, 536, 510, 550, 321, 434, 516, 281, 8947, 264, 7363, 13241, 2361, 322, 264, 43840, 370, 510, 321, 643, 281, 50724], "temperature": 0.0, "avg_logprob": -0.09189939498901367, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0014099371619522572}, {"id": 139, "seek": 106112, "start": 1069.04, "end": 1076.8799999999999, "text": " take the data set train and we're going to use the select method to only get the valid indices", "tokens": [50760, 747, 264, 1412, 992, 3847, 293, 321, 434, 516, 281, 764, 264, 3048, 3170, 281, 787, 483, 264, 7363, 43840, 51152], "temperature": 0.0, "avg_logprob": -0.09189939498901367, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0014099371619522572}, {"id": 140, "seek": 106112, "start": 1078.4799999999998, "end": 1088.8, "text": " and then we're going to get the token counts so for the valid rows so we can also plot this", "tokens": [51232, 293, 550, 321, 434, 516, 281, 483, 264, 14862, 14893, 370, 337, 264, 7363, 13241, 370, 321, 393, 611, 7542, 341, 51748], "temperature": 0.0, "avg_logprob": -0.09189939498901367, "compression_ratio": 1.8675496688741722, "no_speech_prob": 0.0014099371619522572}, {"id": 141, "seek": 108880, "start": 1088.8, "end": 1096.56, "text": " distribution here and we plot the distribution of the token counts after filtering exactly like that", "tokens": [50364, 7316, 510, 293, 321, 7542, 264, 7316, 295, 264, 14862, 14893, 934, 30822, 2293, 411, 300, 50752], "temperature": 0.0, "avg_logprob": -0.10239255623739274, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.000969604414422065}, {"id": 142, "seek": 108880, "start": 1101.44, "end": 1108.32, "text": " okay there's an issue here because I executed the code twice I shouldn't have", "tokens": [50996, 1392, 456, 311, 364, 2734, 510, 570, 286, 17577, 264, 3089, 6091, 286, 4659, 380, 362, 51340], "temperature": 0.0, "avg_logprob": -0.10239255623739274, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.000969604414422065}, {"id": 143, "seek": 108880, "start": 1109.04, "end": 1114.1599999999999, "text": " but that's fine if you execute it once it should be okay now it's already filtered which is why", "tokens": [51376, 457, 300, 311, 2489, 498, 291, 14483, 309, 1564, 309, 820, 312, 1392, 586, 309, 311, 1217, 37111, 597, 307, 983, 51632], "temperature": 0.0, "avg_logprob": -0.10239255623739274, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.000969604414422065}, {"id": 144, "seek": 111416, "start": 1114.16, "end": 1120.5600000000002, "text": " it's grid in there and here you see that we have a very different plot because all this", "tokens": [50364, 309, 311, 10748, 294, 456, 293, 510, 291, 536, 300, 321, 362, 257, 588, 819, 7542, 570, 439, 341, 50684], "temperature": 0.0, "avg_logprob": -0.15809041536771334, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0020171727519482374}, {"id": 145, "seek": 111416, "start": 1120.5600000000002, "end": 1129.1200000000001, "text": " right part has been filtered out another thing that we can do is near the duplication using", "tokens": [50684, 558, 644, 575, 668, 37111, 484, 1071, 551, 300, 321, 393, 360, 307, 2651, 264, 17154, 399, 1228, 51112], "temperature": 0.0, "avg_logprob": -0.15809041536771334, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0020171727519482374}, {"id": 146, "seek": 111416, "start": 1129.1200000000001, "end": 1136.5600000000002, "text": " embeddings which is why we install the sentient transformers library and what we want to do here", "tokens": [51112, 12240, 29432, 597, 307, 983, 321, 3625, 264, 2279, 1196, 4088, 433, 6405, 293, 437, 321, 528, 281, 360, 510, 51484], "temperature": 0.0, "avg_logprob": -0.15809041536771334, "compression_ratio": 1.6046511627906976, "no_speech_prob": 0.0020171727519482374}, {"id": 147, "seek": 113656, "start": 1136.56, "end": 1147.12, "text": " is we want to embed every row every sample from our data set so here we want to translate that into", "tokens": [50364, 307, 321, 528, 281, 12240, 633, 5386, 633, 6889, 490, 527, 1412, 992, 370, 510, 321, 528, 281, 13799, 300, 666, 50892], "temperature": 0.0, "avg_logprob": -0.10251114103529188, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.004822989925742149}, {"id": 148, "seek": 113656, "start": 1147.12, "end": 1154.08, "text": " a vector we call an embedding and using an embedding model how to choose the best embedding model", "tokens": [50892, 257, 8062, 321, 818, 364, 12240, 3584, 293, 1228, 364, 12240, 3584, 2316, 577, 281, 2826, 264, 1151, 12240, 3584, 2316, 51240], "temperature": 0.0, "avg_logprob": -0.10251114103529188, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.004822989925742149}, {"id": 149, "seek": 113656, "start": 1154.08, "end": 1161.6799999999998, "text": " it's often it's a popular question one way of answering it is looking at the MTEB the board", "tokens": [51240, 309, 311, 2049, 309, 311, 257, 3743, 1168, 472, 636, 295, 13430, 309, 307, 1237, 412, 264, 376, 13639, 33, 264, 3150, 51620], "temperature": 0.0, "avg_logprob": -0.10251114103529188, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.004822989925742149}, {"id": 150, "seek": 116168, "start": 1161.76, "end": 1168.3200000000002, "text": " on a hugging face this is where you can see like a competition with all the embedding models on", "tokens": [50368, 322, 257, 41706, 1851, 341, 307, 689, 291, 393, 536, 411, 257, 6211, 365, 439, 264, 12240, 3584, 5245, 322, 50696], "temperature": 0.0, "avg_logprob": -0.09984052424528161, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.006093037314713001}, {"id": 151, "seek": 116168, "start": 1169.44, "end": 1176.0800000000002, "text": " various tasks it's funny because since I've made this screenshot there are new ones on top of it", "tokens": [50752, 3683, 9608, 309, 311, 4074, 570, 1670, 286, 600, 1027, 341, 27712, 456, 366, 777, 2306, 322, 1192, 295, 309, 51084], "temperature": 0.0, "avg_logprob": -0.09984052424528161, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.006093037314713001}, {"id": 152, "seek": 116168, "start": 1176.96, "end": 1184.72, "text": " the one that we're going to use here is the GTE base embedding model it's a it's a really good", "tokens": [51128, 264, 472, 300, 321, 434, 516, 281, 764, 510, 307, 264, 460, 13639, 3096, 12240, 3584, 2316, 309, 311, 257, 309, 311, 257, 534, 665, 51516], "temperature": 0.0, "avg_logprob": -0.09984052424528161, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.006093037314713001}, {"id": 153, "seek": 116168, "start": 1184.72, "end": 1190.0, "text": " model it's not the best model but it's going to be faster than other options which is why we're going", "tokens": [51516, 2316, 309, 311, 406, 264, 1151, 2316, 457, 309, 311, 516, 281, 312, 4663, 813, 661, 3956, 597, 307, 983, 321, 434, 516, 51780], "temperature": 0.0, "avg_logprob": -0.09984052424528161, "compression_ratio": 1.7522522522522523, "no_speech_prob": 0.006093037314713001}, {"id": 154, "seek": 119000, "start": 1190.0, "end": 1197.6, "text": " to use it here and we're going to use these embeddings to then calculate the similarity between", "tokens": [50364, 281, 764, 309, 510, 293, 321, 434, 516, 281, 764, 613, 12240, 29432, 281, 550, 8873, 264, 32194, 1296, 50744], "temperature": 0.0, "avg_logprob": -0.08974864266135475, "compression_ratio": 2.0, "no_speech_prob": 0.002432022476568818}, {"id": 155, "seek": 119000, "start": 1197.6, "end": 1203.36, "text": " them and when they're too similar we're just going to filter them out so how are we going to do it", "tokens": [50744, 552, 293, 562, 436, 434, 886, 2531, 321, 434, 445, 516, 281, 6608, 552, 484, 370, 577, 366, 321, 516, 281, 360, 309, 51032], "temperature": 0.0, "avg_logprob": -0.08974864266135475, "compression_ratio": 2.0, "no_speech_prob": 0.002432022476568818}, {"id": 156, "seek": 119000, "start": 1203.36, "end": 1210.32, "text": " we're going to use the sentient transformer library and we import the sentient transformer", "tokens": [51032, 321, 434, 516, 281, 764, 264, 2279, 1196, 31782, 6405, 293, 321, 974, 264, 2279, 1196, 31782, 51380], "temperature": 0.0, "avg_logprob": -0.08974864266135475, "compression_ratio": 2.0, "no_speech_prob": 0.002432022476568818}, {"id": 157, "seek": 119000, "start": 1211.04, "end": 1218.08, "text": " class we're also going to import face the vector database from facebook it's not the best vector", "tokens": [51416, 1508, 321, 434, 611, 516, 281, 974, 1851, 264, 8062, 8149, 490, 23372, 309, 311, 406, 264, 1151, 8062, 51768], "temperature": 0.0, "avg_logprob": -0.08974864266135475, "compression_ratio": 2.0, "no_speech_prob": 0.002432022476568818}, {"id": 158, "seek": 121808, "start": 1218.08, "end": 1223.76, "text": " database but it's very simple it's very minimalistic which is why I used it in this example from", "tokens": [50364, 8149, 457, 309, 311, 588, 2199, 309, 311, 588, 13206, 3142, 597, 307, 983, 286, 1143, 309, 294, 341, 1365, 490, 50648], "temperature": 0.0, "avg_logprob": -0.11660933176676433, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002321857726201415}, {"id": 159, "seek": 121808, "start": 1223.76, "end": 1232.96, "text": " data set we are going to use data set and data set dict to be a bit fancy we're also going to use", "tokens": [50648, 1412, 992, 321, 366, 516, 281, 764, 1412, 992, 293, 1412, 992, 12569, 281, 312, 257, 857, 10247, 321, 434, 611, 516, 281, 764, 51108], "temperature": 0.0, "avg_logprob": -0.11660933176676433, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002321857726201415}, {"id": 160, "seek": 121808, "start": 1232.96, "end": 1242.56, "text": " tqdm to have a nice loading bar and finally number for some operations so here we're going to", "tokens": [51108, 256, 80, 67, 76, 281, 362, 257, 1481, 15114, 2159, 293, 2721, 1230, 337, 512, 7705, 370, 510, 321, 434, 516, 281, 51588], "temperature": 0.0, "avg_logprob": -0.11660933176676433, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.002321857726201415}, {"id": 161, "seek": 124256, "start": 1242.56, "end": 1249.6799999999998, "text": " really create the code in one function to do everything so we are going to pass it a data set", "tokens": [50364, 534, 1884, 264, 3089, 294, 472, 2445, 281, 360, 1203, 370, 321, 366, 516, 281, 1320, 309, 257, 1412, 992, 50720], "temperature": 0.0, "avg_logprob": -0.07530298540669103, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.007221932988613844}, {"id": 162, "seek": 124256, "start": 1250.32, "end": 1255.84, "text": " we are going to pass it the name of the embedding model we want to use and we're going to pass it", "tokens": [50752, 321, 366, 516, 281, 1320, 309, 264, 1315, 295, 264, 12240, 3584, 2316, 321, 528, 281, 764, 293, 321, 434, 516, 281, 1320, 309, 51028], "temperature": 0.0, "avg_logprob": -0.07530298540669103, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.007221932988613844}, {"id": 163, "seek": 124256, "start": 1255.84, "end": 1263.9199999999998, "text": " threshold for example 95 percent it means that when it's 95 percent as similar as another", "tokens": [51028, 14678, 337, 1365, 13420, 3043, 309, 1355, 300, 562, 309, 311, 13420, 3043, 382, 2531, 382, 1071, 51432], "temperature": 0.0, "avg_logprob": -0.07530298540669103, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.007221932988613844}, {"id": 164, "seek": 124256, "start": 1263.9199999999998, "end": 1270.96, "text": " embedding it's fishy and we probably do not want to to use it we probably want to filter out this", "tokens": [51432, 12240, 3584, 309, 311, 41991, 293, 321, 1391, 360, 406, 528, 281, 281, 764, 309, 321, 1391, 528, 281, 6608, 484, 341, 51784], "temperature": 0.0, "avg_logprob": -0.07530298540669103, "compression_ratio": 1.8855721393034826, "no_speech_prob": 0.007221932988613844}, {"id": 165, "seek": 127096, "start": 1270.96, "end": 1279.1200000000001, "text": " model so as a sentence transformer we're going to use the model that we we pass this argument", "tokens": [50364, 2316, 370, 382, 257, 8174, 31782, 321, 434, 516, 281, 764, 264, 2316, 300, 321, 321, 1320, 341, 6770, 50772], "temperature": 0.0, "avg_logprob": -0.15479063146254596, "compression_ratio": 1.9840425531914894, "no_speech_prob": 0.00101466232445091}, {"id": 166, "seek": 127096, "start": 1280.4, "end": 1290.56, "text": " for the output we are going to use all the example in the data set so what we want to filter", "tokens": [50836, 337, 264, 5598, 321, 366, 516, 281, 764, 439, 264, 1365, 294, 264, 1412, 992, 370, 437, 321, 528, 281, 6608, 51344], "temperature": 0.0, "avg_logprob": -0.15479063146254596, "compression_ratio": 1.9840425531914894, "no_speech_prob": 0.00101466232445091}, {"id": 167, "seek": 127096, "start": 1291.3600000000001, "end": 1296.4, "text": " filter out here are just the outputs are not the instructions we're fine if we have similar", "tokens": [51384, 6608, 484, 510, 366, 445, 264, 23930, 366, 406, 264, 9415, 321, 434, 2489, 498, 321, 362, 2531, 51636], "temperature": 0.0, "avg_logprob": -0.15479063146254596, "compression_ratio": 1.9840425531914894, "no_speech_prob": 0.00101466232445091}, {"id": 168, "seek": 127096, "start": 1296.4, "end": 1300.88, "text": " instructions we just do not want similar outputs because this is what the model is going to be", "tokens": [51636, 9415, 321, 445, 360, 406, 528, 2531, 23930, 570, 341, 307, 437, 264, 2316, 307, 516, 281, 312, 51860], "temperature": 0.0, "avg_logprob": -0.15479063146254596, "compression_ratio": 1.9840425531914894, "no_speech_prob": 0.00101466232445091}, {"id": 169, "seek": 130096, "start": 1301.04, "end": 1311.3600000000001, "text": " trained on then we are going to say that we are converting text to embeddings we are going to", "tokens": [50368, 8895, 322, 550, 321, 366, 516, 281, 584, 300, 321, 366, 29942, 2487, 281, 12240, 29432, 321, 366, 516, 281, 50884], "temperature": 0.0, "avg_logprob": -0.09270496990369714, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0012639638734981418}, {"id": 170, "seek": 130096, "start": 1312.72, "end": 1321.3600000000001, "text": " use the sentence model and encode the outputs that we have and we can even show a progress bar", "tokens": [50952, 764, 264, 8174, 2316, 293, 2058, 1429, 264, 23930, 300, 321, 362, 293, 321, 393, 754, 855, 257, 4205, 2159, 51384], "temperature": 0.0, "avg_logprob": -0.09270496990369714, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.0012639638734981418}, {"id": 171, "seek": 132136, "start": 1322.32, "end": 1325.04, "text": " to be fancy bar", "tokens": [50412, 281, 312, 10247, 2159, 50548], "temperature": 0.0, "avg_logprob": -0.1609635353088379, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00955610815435648}, {"id": 172, "seek": 132136, "start": 1327.76, "end": 1334.7199999999998, "text": " then we're going to get the dimension of our embeddings so you can see in the", "tokens": [50684, 550, 321, 434, 516, 281, 483, 264, 10139, 295, 527, 12240, 29432, 370, 291, 393, 536, 294, 264, 51032], "temperature": 0.0, "avg_logprob": -0.1609635353088379, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00955610815435648}, {"id": 173, "seek": 132136, "start": 1335.6799999999998, "end": 1343.4399999999998, "text": " little boards some of them they have like 1024 some of them they have like 768 basically yeah", "tokens": [51080, 707, 13293, 512, 295, 552, 436, 362, 411, 1266, 7911, 512, 295, 552, 436, 362, 411, 24733, 23, 1936, 1338, 51468], "temperature": 0.0, "avg_logprob": -0.1609635353088379, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00955610815435648}, {"id": 174, "seek": 132136, "start": 1343.4399999999998, "end": 1349.52, "text": " they have different dimensions take that into account we are going to create our index using", "tokens": [51468, 436, 362, 819, 12819, 747, 300, 666, 2696, 321, 366, 516, 281, 1884, 527, 8186, 1228, 51772], "temperature": 0.0, "avg_logprob": -0.1609635353088379, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00955610815435648}, {"id": 175, "seek": 134952, "start": 1350.08, "end": 1358.08, "text": " the vector database so it's going to be a flat ip index in this case", "tokens": [50392, 264, 8062, 8149, 370, 309, 311, 516, 281, 312, 257, 4962, 28501, 8186, 294, 341, 1389, 50792], "temperature": 0.0, "avg_logprob": -0.11200814387377571, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0017532906495034695}, {"id": 176, "seek": 134952, "start": 1359.68, "end": 1369.84, "text": " and we need to normalize our embeddings the face already has a function to do it but i do not trust", "tokens": [50872, 293, 321, 643, 281, 2710, 1125, 527, 12240, 29432, 264, 1851, 1217, 575, 257, 2445, 281, 360, 309, 457, 741, 360, 406, 3361, 51380], "temperature": 0.0, "avg_logprob": -0.11200814387377571, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0017532906495034695}, {"id": 177, "seek": 134952, "start": 1369.84, "end": 1374.72, "text": " it that much so we're going to do it on our own because i had a bad experience with it", "tokens": [51380, 309, 300, 709, 370, 321, 434, 516, 281, 360, 309, 322, 527, 1065, 570, 741, 632, 257, 1578, 1752, 365, 309, 51624], "temperature": 0.0, "avg_logprob": -0.11200814387377571, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.0017532906495034695}, {"id": 178, "seek": 137472, "start": 1375.68, "end": 1382.16, "text": " and i think it's going to be better that way so here we're using numpy to normalize it", "tokens": [50412, 293, 741, 519, 309, 311, 516, 281, 312, 1101, 300, 636, 370, 510, 321, 434, 1228, 1031, 8200, 281, 2710, 1125, 309, 50736], "temperature": 0.0, "avg_logprob": -0.13835658302790002, "compression_ratio": 1.8363636363636364, "no_speech_prob": 0.0030725018586963415}, {"id": 179, "seek": 137472, "start": 1383.2, "end": 1390.08, "text": " using the norm to just take the norm to normalize the entire embedding space", "tokens": [50788, 1228, 264, 2026, 281, 445, 747, 264, 2026, 281, 2710, 1125, 264, 2302, 12240, 3584, 1901, 51132], "temperature": 0.0, "avg_logprob": -0.13835658302790002, "compression_ratio": 1.8363636363636364, "no_speech_prob": 0.0030725018586963415}, {"id": 180, "seek": 137472, "start": 1390.64, "end": 1395.28, "text": " and then we're going to add it to our index as normalized embeddings", "tokens": [51160, 293, 550, 321, 434, 516, 281, 909, 309, 281, 527, 8186, 382, 48704, 12240, 29432, 51392], "temperature": 0.0, "avg_logprob": -0.13835658302790002, "compression_ratio": 1.8363636363636364, "no_speech_prob": 0.0030725018586963415}, {"id": 181, "seek": 137472, "start": 1396.64, "end": 1402.32, "text": " then we're going to say we are filtering out let's say near duplicates", "tokens": [51460, 550, 321, 434, 516, 281, 584, 321, 366, 30822, 484, 718, 311, 584, 2651, 17154, 1024, 51744], "temperature": 0.0, "avg_logprob": -0.13835658302790002, "compression_ratio": 1.8363636363636364, "no_speech_prob": 0.0030725018586963415}, {"id": 182, "seek": 140232, "start": 1402.6399999999999, "end": 1411.4399999999998, "text": " and in this part of the code we want to use the index search so we have", "tokens": [50380, 293, 294, 341, 644, 295, 264, 3089, 321, 528, 281, 764, 264, 8186, 3164, 370, 321, 362, 50820], "temperature": 0.0, "avg_logprob": -0.14608904520670574, "compression_ratio": 1.5410958904109588, "no_speech_prob": 0.0011153158266097307}, {"id": 183, "seek": 140232, "start": 1413.36, "end": 1419.52, "text": " the normalized embedding we just put and here we are going to say k equal", "tokens": [50916, 264, 48704, 12240, 3584, 321, 445, 829, 293, 510, 321, 366, 516, 281, 584, 350, 2681, 51224], "temperature": 0.0, "avg_logprob": -0.14608904520670574, "compression_ratio": 1.5410958904109588, "no_speech_prob": 0.0011153158266097307}, {"id": 184, "seek": 140232, "start": 1420.3999999999999, "end": 1426.32, "text": " 2 so we are going to return at most two vectors we don't need more in this case", "tokens": [51268, 568, 370, 321, 366, 516, 281, 2736, 412, 881, 732, 18875, 321, 500, 380, 643, 544, 294, 341, 1389, 51564], "temperature": 0.0, "avg_logprob": -0.14608904520670574, "compression_ratio": 1.5410958904109588, "no_speech_prob": 0.0011153158266097307}, {"id": 185, "seek": 142632, "start": 1427.28, "end": 1434.6399999999999, "text": " and we're going to create a list of the samples we want to keep call it to keep", "tokens": [50412, 293, 321, 434, 516, 281, 1884, 257, 1329, 295, 264, 10938, 321, 528, 281, 1066, 818, 309, 281, 1066, 50780], "temperature": 0.0, "avg_logprob": -0.09666228294372559, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.005467107053846121}, {"id": 186, "seek": 142632, "start": 1435.36, "end": 1442.72, "text": " and this is the main loop finally range length yeah it's fine", "tokens": [50816, 293, 341, 307, 264, 2135, 6367, 2721, 3613, 4641, 1338, 309, 311, 2489, 51184], "temperature": 0.0, "avg_logprob": -0.09666228294372559, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.005467107053846121}, {"id": 187, "seek": 142632, "start": 1445.12, "end": 1450.32, "text": " and we're going to do something nice for the QDM so we have a nice loading bar", "tokens": [51304, 293, 321, 434, 516, 281, 360, 746, 1481, 337, 264, 1249, 35, 44, 370, 321, 362, 257, 1481, 15114, 2159, 51564], "temperature": 0.0, "avg_logprob": -0.09666228294372559, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.005467107053846121}, {"id": 188, "seek": 145032, "start": 1451.2, "end": 1465.12, "text": " and what we want here is if the dimension is so this if this is we're going to return here the", "tokens": [50408, 293, 437, 321, 528, 510, 307, 498, 264, 10139, 307, 370, 341, 498, 341, 307, 321, 434, 516, 281, 2736, 510, 264, 51104], "temperature": 0.0, "avg_logprob": -0.1262858610886794, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.002888487186282873}, {"id": 189, "seek": 145032, "start": 1465.12, "end": 1472.48, "text": " similarity between these embeddings and if this is this if the cosine similarity is", "tokens": [51104, 32194, 1296, 613, 12240, 29432, 293, 498, 341, 307, 341, 498, 264, 23565, 32194, 307, 51472], "temperature": 0.0, "avg_logprob": -0.1262858610886794, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.002888487186282873}, {"id": 190, "seek": 145032, "start": 1472.48, "end": 1478.3999999999999, "text": " below the threshold we are going to keep it so we are going to add it to the to keep", "tokens": [51472, 2507, 264, 14678, 321, 366, 516, 281, 1066, 309, 370, 321, 366, 516, 281, 909, 309, 281, 264, 281, 1066, 51768], "temperature": 0.0, "avg_logprob": -0.1262858610886794, "compression_ratio": 1.852112676056338, "no_speech_prob": 0.002888487186282873}, {"id": 191, "seek": 147840, "start": 1479.3600000000001, "end": 1492.3200000000002, "text": " happened and then we have i yes an index index and then we can create data set trains and we", "tokens": [50412, 2011, 293, 550, 321, 362, 741, 2086, 364, 8186, 8186, 293, 550, 321, 393, 1884, 1412, 992, 16329, 293, 321, 51060], "temperature": 0.0, "avg_logprob": -0.11829637444537619, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.0031201737001538277}, {"id": 192, "seek": 147840, "start": 1492.3200000000002, "end": 1498.96, "text": " are going to use the select method from the data set object to only keep these indexes", "tokens": [51060, 366, 516, 281, 764, 264, 3048, 3170, 490, 264, 1412, 992, 2657, 281, 787, 1066, 613, 8186, 279, 51392], "temperature": 0.0, "avg_logprob": -0.11829637444537619, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.0031201737001538277}, {"id": 193, "seek": 147840, "start": 1500.64, "end": 1507.52, "text": " then we are going to return it as a data set dict this is not the most elegant way of doing it but", "tokens": [51476, 550, 321, 366, 516, 281, 2736, 309, 382, 257, 1412, 992, 12569, 341, 307, 406, 264, 881, 21117, 636, 295, 884, 309, 457, 51820], "temperature": 0.0, "avg_logprob": -0.11829637444537619, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.0031201737001538277}, {"id": 194, "seek": 150752, "start": 1508.24, "end": 1514.72, "text": " it's going to be fine for the purpose of this exercise and then we can call our function so", "tokens": [50400, 309, 311, 516, 281, 312, 2489, 337, 264, 4334, 295, 341, 5380, 293, 550, 321, 393, 818, 527, 2445, 370, 50724], "temperature": 0.0, "avg_logprob": -0.09870958999848702, "compression_ratio": 1.761290322580645, "no_speech_prob": 0.00046548599493689835}, {"id": 195, "seek": 150752, "start": 1515.68, "end": 1522.6399999999999, "text": " the duplicate data set and we are going to pass the data set and we're going to pass", "tokens": [50772, 264, 23976, 1412, 992, 293, 321, 366, 516, 281, 1320, 264, 1412, 992, 293, 321, 434, 516, 281, 1320, 51120], "temperature": 0.0, "avg_logprob": -0.09870958999848702, "compression_ratio": 1.761290322580645, "no_speech_prob": 0.00046548599493689835}, {"id": 196, "seek": 150752, "start": 1523.84, "end": 1530.4, "text": " the embedding model we want to use so in this case as I mentioned it's going to be the gte large", "tokens": [51180, 264, 12240, 3584, 2316, 321, 528, 281, 764, 370, 294, 341, 1389, 382, 286, 2835, 309, 311, 516, 281, 312, 264, 290, 975, 2416, 51508], "temperature": 0.0, "avg_logprob": -0.09870958999848702, "compression_ratio": 1.761290322580645, "no_speech_prob": 0.00046548599493689835}, {"id": 197, "seek": 153040, "start": 1531.2, "end": 1540.0, "text": " and we can just copy paste it here and as a threshold I'm going to use 0.95 be careful if", "tokens": [50404, 293, 321, 393, 445, 5055, 9163, 309, 510, 293, 382, 257, 14678, 286, 478, 516, 281, 764, 1958, 13, 15718, 312, 5026, 498, 50844], "temperature": 0.0, "avg_logprob": -0.1464531762259347, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.0028883563354611397}, {"id": 198, "seek": 153040, "start": 1540.0, "end": 1547.1200000000001, "text": " you switch the embedding model you won't have the same distribution of cosine similarity so some", "tokens": [50844, 291, 3679, 264, 12240, 3584, 2316, 291, 1582, 380, 362, 264, 912, 7316, 295, 23565, 32194, 370, 512, 51200], "temperature": 0.0, "avg_logprob": -0.1464531762259347, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.0028883563354611397}, {"id": 199, "seek": 153040, "start": 1547.1200000000001, "end": 1553.8400000000001, "text": " models to get the same results you're going to need like 85 others you can might need 99.9", "tokens": [51200, 5245, 281, 483, 264, 912, 3542, 291, 434, 516, 281, 643, 411, 14695, 2357, 291, 393, 1062, 643, 11803, 13, 24, 51536], "temperature": 0.0, "avg_logprob": -0.1464531762259347, "compression_ratio": 1.4972972972972973, "no_speech_prob": 0.0028883563354611397}, {"id": 200, "seek": 155384, "start": 1554.6399999999999, "end": 1562.56, "text": " it really depends on the embedding model that you use it should be fine so now we can convert", "tokens": [50404, 309, 534, 5946, 322, 264, 12240, 3584, 2316, 300, 291, 764, 309, 820, 312, 2489, 370, 586, 321, 393, 7620, 50800], "temperature": 0.0, "avg_logprob": -0.08032048638187238, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.003169785253703594}, {"id": 201, "seek": 155384, "start": 1562.56, "end": 1570.56, "text": " the entire data set into embeddings here we are downloading the embedding model it's not a big", "tokens": [50800, 264, 2302, 1412, 992, 666, 12240, 29432, 510, 321, 366, 32529, 264, 12240, 3584, 2316, 309, 311, 406, 257, 955, 51200], "temperature": 0.0, "avg_logprob": -0.08032048638187238, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.003169785253703594}, {"id": 202, "seek": 155384, "start": 1570.56, "end": 1577.84, "text": " model which is why it's it's pretty fast the long part is actually comparing the embeddings", "tokens": [51200, 2316, 597, 307, 983, 309, 311, 309, 311, 1238, 2370, 264, 938, 644, 307, 767, 15763, 264, 12240, 29432, 51564], "temperature": 0.0, "avg_logprob": -0.08032048638187238, "compression_ratio": 1.7721518987341771, "no_speech_prob": 0.003169785253703594}, {"id": 203, "seek": 157784, "start": 1578.72, "end": 1585.52, "text": " I should mention why we're doing it using a vector database instead of a for loop I've tried to do a", "tokens": [50408, 286, 820, 2152, 983, 321, 434, 884, 309, 1228, 257, 8062, 8149, 2602, 295, 257, 337, 6367, 286, 600, 3031, 281, 360, 257, 50748], "temperature": 0.0, "avg_logprob": -0.07245431775632112, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.003117017215117812}, {"id": 204, "seek": 157784, "start": 1585.52, "end": 1591.12, "text": " very minimalistic version using two for loop so we would compare every embedding to all the other", "tokens": [50748, 588, 13206, 3142, 3037, 1228, 732, 337, 6367, 370, 321, 576, 6794, 633, 12240, 3584, 281, 439, 264, 661, 51028], "temperature": 0.0, "avg_logprob": -0.07245431775632112, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.003117017215117812}, {"id": 205, "seek": 157784, "start": 1591.12, "end": 1598.32, "text": " embeddings but it took a very long time so this is why we're using a vector database here to be", "tokens": [51028, 12240, 29432, 457, 309, 1890, 257, 588, 938, 565, 370, 341, 307, 983, 321, 434, 1228, 257, 8062, 8149, 510, 281, 312, 51388], "temperature": 0.0, "avg_logprob": -0.07245431775632112, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.003117017215117812}, {"id": 206, "seek": 157784, "start": 1598.32, "end": 1605.12, "text": " more efficient to be used by the computations and and get the results basically just faster because", "tokens": [51388, 544, 7148, 281, 312, 1143, 538, 264, 2807, 763, 293, 293, 483, 264, 3542, 1936, 445, 4663, 570, 51728], "temperature": 0.0, "avg_logprob": -0.07245431775632112, "compression_ratio": 1.790909090909091, "no_speech_prob": 0.003117017215117812}, {"id": 207, "seek": 160512, "start": 1605.12, "end": 1611.28, "text": " otherwise it would take like two hours it was really really too long unfortunately so here we", "tokens": [50364, 5911, 309, 576, 747, 411, 732, 2496, 309, 390, 534, 534, 886, 938, 7015, 370, 510, 321, 50672], "temperature": 0.0, "avg_logprob": -0.07586313080001664, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0010810252279043198}, {"id": 208, "seek": 160512, "start": 1611.28, "end": 1618.8799999999999, "text": " still downloading the models and then with a v100 with high ram it should take about three to four", "tokens": [50672, 920, 32529, 264, 5245, 293, 550, 365, 257, 371, 6879, 365, 1090, 10211, 309, 820, 747, 466, 1045, 281, 1451, 51052], "temperature": 0.0, "avg_logprob": -0.07586313080001664, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0010810252279043198}, {"id": 209, "seek": 160512, "start": 1618.8799999999999, "end": 1626.7199999999998, "text": " minutes to get all the embeddings and to filter out the data set in the meantime we can continue", "tokens": [51052, 2077, 281, 483, 439, 264, 12240, 29432, 293, 281, 6608, 484, 264, 1412, 992, 294, 264, 14991, 321, 393, 2354, 51444], "temperature": 0.0, "avg_logprob": -0.07586313080001664, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0010810252279043198}, {"id": 210, "seek": 160512, "start": 1626.7199999999998, "end": 1634.6399999999999, "text": " I'm just going to show the code here because I don't know if you can really see it if you had time", "tokens": [51444, 286, 478, 445, 516, 281, 855, 264, 3089, 510, 570, 286, 500, 380, 458, 498, 291, 393, 534, 536, 309, 498, 291, 632, 565, 51840], "temperature": 0.0, "avg_logprob": -0.07586313080001664, "compression_ratio": 1.6943231441048034, "no_speech_prob": 0.0010810252279043198}, {"id": 211, "seek": 163464, "start": 1635.2800000000002, "end": 1641.3600000000001, "text": " to see the code this part might be a bit confusing but I don't want to delve too deep", "tokens": [50396, 281, 536, 264, 3089, 341, 644, 1062, 312, 257, 857, 13181, 457, 286, 500, 380, 528, 281, 43098, 886, 2452, 50700], "temperature": 0.0, "avg_logprob": -0.09696410358815953, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00101409328635782}, {"id": 212, "seek": 163464, "start": 1641.3600000000001, "end": 1649.92, "text": " to the details of the face vector database okay so now you see that it's converting the text to", "tokens": [50700, 281, 264, 4365, 295, 264, 1851, 8062, 8149, 1392, 370, 586, 291, 536, 300, 309, 311, 29942, 264, 2487, 281, 51128], "temperature": 0.0, "avg_logprob": -0.09696410358815953, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00101409328635782}, {"id": 213, "seek": 163464, "start": 1649.92, "end": 1656.0, "text": " embeddings oh it's going to take much longer than last time I've tried unfortunately but it's okay", "tokens": [51128, 12240, 29432, 1954, 309, 311, 516, 281, 747, 709, 2854, 813, 1036, 565, 286, 600, 3031, 7015, 457, 309, 311, 1392, 51432], "temperature": 0.0, "avg_logprob": -0.09696410358815953, "compression_ratio": 1.5819209039548023, "no_speech_prob": 0.00101409328635782}, {"id": 214, "seek": 165600, "start": 1656.0, "end": 1666.16, "text": " like we we can stop it or come back later to finish it what we want to see when we have this", "tokens": [50364, 411, 321, 321, 393, 1590, 309, 420, 808, 646, 1780, 281, 2413, 309, 437, 321, 528, 281, 536, 562, 321, 362, 341, 50872], "temperature": 0.0, "avg_logprob": -0.14050642013549805, "compression_ratio": 1.52, "no_speech_prob": 0.004259251058101654}, {"id": 215, "seek": 165600, "start": 1666.16, "end": 1675.68, "text": " dedupe data set is the number of samples that were filtered out so we can print the length of the", "tokens": [50872, 4172, 84, 494, 1412, 992, 307, 264, 1230, 295, 10938, 300, 645, 37111, 484, 370, 321, 393, 4482, 264, 4641, 295, 264, 51348], "temperature": 0.0, "avg_logprob": -0.14050642013549805, "compression_ratio": 1.52, "no_speech_prob": 0.004259251058101654}, {"id": 216, "seek": 167568, "start": 1676.64, "end": 1689.04, "text": " original data set we can print the length of the dedupe data set and we can even print the number", "tokens": [50412, 3380, 1412, 992, 321, 393, 4482, 264, 4641, 295, 264, 4172, 84, 494, 1412, 992, 293, 321, 393, 754, 4482, 264, 1230, 51032], "temperature": 0.0, "avg_logprob": -0.1129671016209562, "compression_ratio": 2.05, "no_speech_prob": 0.0032202955335378647}, {"id": 217, "seek": 167568, "start": 1689.04, "end": 1695.44, "text": " of samples that were removed so in this case the length of the original data set minus the", "tokens": [51032, 295, 10938, 300, 645, 7261, 370, 294, 341, 1389, 264, 4641, 295, 264, 3380, 1412, 992, 3175, 264, 51352], "temperature": 0.0, "avg_logprob": -0.1129671016209562, "compression_ratio": 2.05, "no_speech_prob": 0.0032202955335378647}, {"id": 218, "seek": 167568, "start": 1695.44, "end": 1703.92, "text": " length of the dedupe data set and this will tell us that how many rows we we removed and last time", "tokens": [51352, 4641, 295, 264, 4172, 84, 494, 1412, 992, 293, 341, 486, 980, 505, 300, 577, 867, 13241, 321, 321, 7261, 293, 1036, 565, 51776], "temperature": 0.0, "avg_logprob": -0.1129671016209562, "compression_ratio": 2.05, "no_speech_prob": 0.0032202955335378647}, {"id": 219, "seek": 170392, "start": 1703.92, "end": 1715.44, "text": " you can see it later on the solution notebook it's about 8 000 samples one thing that we can do", "tokens": [50364, 291, 393, 536, 309, 1780, 322, 264, 3827, 21060, 309, 311, 466, 1649, 13711, 10938, 472, 551, 300, 321, 393, 360, 50940], "temperature": 0.0, "avg_logprob": -0.14845450719197592, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.002179777016863227}, {"id": 220, "seek": 170392, "start": 1716.0, "end": 1723.2, "text": " when that part is over is topk sampling so in this case we still have too many samples because if", "tokens": [50968, 562, 300, 644, 307, 670, 307, 1192, 74, 21179, 370, 294, 341, 1389, 321, 920, 362, 886, 867, 10938, 570, 498, 51328], "temperature": 0.0, "avg_logprob": -0.14845450719197592, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.002179777016863227}, {"id": 221, "seek": 170392, "start": 1723.2, "end": 1731.1200000000001, "text": " we remove 8 000 samples we're still going to have 20 no we're still going to have 16 k samples", "tokens": [51328, 321, 4159, 1649, 13711, 10938, 321, 434, 920, 516, 281, 362, 945, 572, 321, 434, 920, 516, 281, 362, 3165, 350, 10938, 51724], "temperature": 0.0, "avg_logprob": -0.14845450719197592, "compression_ratio": 1.7349397590361446, "no_speech_prob": 0.002179777016863227}, {"id": 222, "seek": 173112, "start": 1731.1999999999998, "end": 1736.1599999999999, "text": " maybe this is too much for what we want to do so we can randomly sample", "tokens": [50368, 1310, 341, 307, 886, 709, 337, 437, 321, 528, 281, 360, 370, 321, 393, 16979, 6889, 50616], "temperature": 0.0, "avg_logprob": -0.08658488094806671, "compression_ratio": 1.6418918918918919, "no_speech_prob": 0.0036469348706305027}, {"id": 223, "seek": 173112, "start": 1739.1999999999998, "end": 1746.3999999999999, "text": " some some rows in order to do that we're going to create a new function called topk", "tokens": [50768, 512, 512, 13241, 294, 1668, 281, 360, 300, 321, 434, 516, 281, 1884, 257, 777, 2445, 1219, 1192, 74, 51128], "temperature": 0.0, "avg_logprob": -0.08658488094806671, "compression_ratio": 1.6418918918918919, "no_speech_prob": 0.0036469348706305027}, {"id": 224, "seek": 173112, "start": 1747.36, "end": 1755.84, "text": " rows we are going to use a data set token counts and k to know how many we want to have", "tokens": [51176, 13241, 321, 366, 516, 281, 764, 257, 1412, 992, 14862, 14893, 293, 350, 281, 458, 577, 867, 321, 528, 281, 362, 51600], "temperature": 0.0, "avg_logprob": -0.08658488094806671, "compression_ratio": 1.6418918918918919, "no_speech_prob": 0.0036469348706305027}, {"id": 225, "seek": 175584, "start": 1756.48, "end": 1764.0, "text": " we're going to sort the indices because we can sort it by descending token count", "tokens": [50396, 321, 434, 516, 281, 1333, 264, 43840, 570, 321, 393, 1333, 309, 538, 40182, 14862, 1207, 50772], "temperature": 0.0, "avg_logprob": -0.11023786972308981, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.005903769750148058}, {"id": 226, "seek": 175584, "start": 1764.0, "end": 1770.0, "text": " and get the topk indices in this case so it's going to be sorted range", "tokens": [50772, 293, 483, 264, 1192, 74, 43840, 294, 341, 1389, 370, 309, 311, 516, 281, 312, 25462, 3613, 51072], "temperature": 0.0, "avg_logprob": -0.11023786972308981, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.005903769750148058}, {"id": 227, "seek": 175584, "start": 1772.72, "end": 1784.48, "text": " length token counts and here we are going to use going to use lambda i token counts", "tokens": [51208, 4641, 14862, 14893, 293, 510, 321, 366, 516, 281, 764, 516, 281, 764, 13607, 741, 14862, 14893, 51796], "temperature": 0.0, "avg_logprob": -0.11023786972308981, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.005903769750148058}, {"id": 228, "seek": 178448, "start": 1784.48, "end": 1788.16, "text": " reverse is equal to true so here we should get", "tokens": [50364, 9943, 307, 2681, 281, 2074, 370, 510, 321, 820, 483, 50548], "temperature": 0.0, "avg_logprob": -0.11345321056889553, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.0015001898864284158}, {"id": 229, "seek": 178448, "start": 1791.44, "end": 1799.1200000000001, "text": " everything that we need so the token counts and get all the data sets with most samples first", "tokens": [50712, 1203, 300, 321, 643, 370, 264, 14862, 14893, 293, 483, 439, 264, 1412, 6352, 365, 881, 10938, 700, 51096], "temperature": 0.0, "avg_logprob": -0.11345321056889553, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.0015001898864284158}, {"id": 230, "seek": 178448, "start": 1799.92, "end": 1807.44, "text": " and then topk indices we are going to just keep those those topk", "tokens": [51136, 293, 550, 1192, 74, 43840, 321, 366, 516, 281, 445, 1066, 729, 729, 1192, 74, 51512], "temperature": 0.0, "avg_logprob": -0.11345321056889553, "compression_ratio": 1.5185185185185186, "no_speech_prob": 0.0015001898864284158}, {"id": 231, "seek": 180744, "start": 1808.4, "end": 1814.0, "text": " yeah and i just talked about randomly doing it but it's not true we're just getting", "tokens": [50412, 1338, 293, 741, 445, 2825, 466, 16979, 884, 309, 457, 309, 311, 406, 2074, 321, 434, 445, 1242, 50692], "temperature": 0.0, "avg_logprob": -0.1615589396158854, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0028000622987747192}, {"id": 232, "seek": 180744, "start": 1814.0, "end": 1821.76, "text": " like the the samples with the most tokens sorry about that and then we can create topk data", "tokens": [50692, 411, 264, 264, 10938, 365, 264, 881, 22667, 2597, 466, 300, 293, 550, 321, 393, 1884, 1192, 74, 1412, 51080], "temperature": 0.0, "avg_logprob": -0.1615589396158854, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0028000622987747192}, {"id": 233, "seek": 180744, "start": 1821.76, "end": 1830.8, "text": " and here we have instruction where we want data sets that are in the topk indices", "tokens": [51080, 293, 510, 321, 362, 10951, 689, 321, 528, 1412, 6352, 300, 366, 294, 264, 1192, 74, 43840, 51532], "temperature": 0.0, "avg_logprob": -0.1615589396158854, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0028000622987747192}, {"id": 234, "seek": 180744, "start": 1833.52, "end": 1836.0800000000002, "text": " and we're going to do the same thing with the output", "tokens": [51668, 293, 321, 434, 516, 281, 360, 264, 912, 551, 365, 264, 5598, 51796], "temperature": 0.0, "avg_logprob": -0.1615589396158854, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0028000622987747192}, {"id": 235, "seek": 183744, "start": 1837.76, "end": 1844.0800000000002, "text": " you could do something similar with the select method but yeah this time this time i want to", "tokens": [50380, 291, 727, 360, 746, 2531, 365, 264, 3048, 3170, 457, 1338, 341, 565, 341, 565, 741, 528, 281, 50696], "temperature": 0.0, "avg_logprob": -0.10702885752138884, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0005790507420897484}, {"id": 236, "seek": 183744, "start": 1844.64, "end": 1851.1200000000001, "text": " to be clear about what's going on so we have a full loop to select all the samples that were", "tokens": [50724, 281, 312, 1850, 466, 437, 311, 516, 322, 370, 321, 362, 257, 1577, 6367, 281, 3048, 439, 264, 10938, 300, 645, 51048], "temperature": 0.0, "avg_logprob": -0.10702885752138884, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0005790507420897484}, {"id": 237, "seek": 183744, "start": 1851.1200000000001, "end": 1858.16, "text": " in the sorted indices here and we are not going to keep like 1000 of them and we are going to do", "tokens": [51048, 294, 264, 25462, 43840, 510, 293, 321, 366, 406, 516, 281, 1066, 411, 9714, 295, 552, 293, 321, 366, 516, 281, 360, 51400], "temperature": 0.0, "avg_logprob": -0.10702885752138884, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.0005790507420897484}, {"id": 238, "seek": 185816, "start": 1858.96, "end": 1868.5600000000002, "text": " the same thing with output and finally we can return our data sets from the dictionary that we", "tokens": [50404, 264, 912, 551, 365, 5598, 293, 2721, 321, 393, 2736, 527, 1412, 6352, 490, 264, 25890, 300, 321, 50884], "temperature": 0.0, "avg_logprob": -0.06991264713344289, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0022858111187815666}, {"id": 239, "seek": 185816, "start": 1868.5600000000002, "end": 1880.72, "text": " did topk data so this is our function but in order to call it we are going to need to have", "tokens": [50884, 630, 1192, 74, 1412, 370, 341, 307, 527, 2445, 457, 294, 1668, 281, 818, 309, 321, 366, 516, 281, 643, 281, 362, 51492], "temperature": 0.0, "avg_logprob": -0.06991264713344289, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0022858111187815666}, {"id": 240, "seek": 185816, "start": 1880.72, "end": 1887.28, "text": " the new token counts because here we filtered out a lot of samples so we can just copy paste", "tokens": [51492, 264, 777, 14862, 14893, 570, 510, 321, 37111, 484, 257, 688, 295, 10938, 370, 321, 393, 445, 5055, 9163, 51820], "temperature": 0.0, "avg_logprob": -0.06991264713344289, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0022858111187815666}, {"id": 241, "seek": 188728, "start": 1887.28, "end": 1893.44, "text": " what we did at the beginning here get the instruction token counts the output token", "tokens": [50364, 437, 321, 630, 412, 264, 2863, 510, 483, 264, 10951, 14862, 14893, 264, 5598, 14862, 50672], "temperature": 0.0, "avg_logprob": -0.07868440712199491, "compression_ratio": 1.7452229299363058, "no_speech_prob": 0.0011876797070726752}, {"id": 242, "seek": 188728, "start": 1893.44, "end": 1900.6399999999999, "text": " counts the combined token counts and this is what we will use when we want to call this function", "tokens": [50672, 14893, 264, 9354, 14862, 14893, 293, 341, 307, 437, 321, 486, 764, 562, 321, 528, 281, 818, 341, 2445, 51032], "temperature": 0.0, "avg_logprob": -0.07868440712199491, "compression_ratio": 1.7452229299363058, "no_speech_prob": 0.0011876797070726752}, {"id": 243, "seek": 188728, "start": 1901.2, "end": 1911.6, "text": " so let's have a k of 1000 and the topk data set will be get topk rows data set we're going to", "tokens": [51060, 370, 718, 311, 362, 257, 350, 295, 9714, 293, 264, 1192, 74, 1412, 992, 486, 312, 483, 1192, 74, 13241, 1412, 992, 321, 434, 516, 281, 51580], "temperature": 0.0, "avg_logprob": -0.07868440712199491, "compression_ratio": 1.7452229299363058, "no_speech_prob": 0.0011876797070726752}, {"id": 244, "seek": 191160, "start": 1911.6, "end": 1917.28, "text": " use it with the combined counts and the k of 1000 so this is how we're going to call the function", "tokens": [50364, 764, 309, 365, 264, 9354, 14893, 293, 264, 350, 295, 9714, 370, 341, 307, 577, 321, 434, 516, 281, 818, 264, 2445, 50648], "temperature": 0.0, "avg_logprob": -0.08336531032215465, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.0037063551135361195}, {"id": 245, "seek": 191160, "start": 1918.32, "end": 1929.52, "text": " and finally we are going to save it as a dictionary like yeah data set dict as they call it", "tokens": [50700, 293, 2721, 321, 366, 516, 281, 3155, 309, 382, 257, 25890, 411, 1338, 1412, 992, 12569, 382, 436, 818, 309, 51260], "temperature": 0.0, "avg_logprob": -0.08336531032215465, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.0037063551135361195}, {"id": 246, "seek": 191160, "start": 1931.04, "end": 1935.84, "text": " to make sure that we still have like a train split but not not very important", "tokens": [51336, 281, 652, 988, 300, 321, 920, 362, 411, 257, 3847, 7472, 457, 406, 406, 588, 1021, 51576], "temperature": 0.0, "avg_logprob": -0.08336531032215465, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.0037063551135361195}, {"id": 247, "seek": 193584, "start": 1936.24, "end": 1944.1599999999999, "text": " after we've done that we can once again re-compute all of the token counts", "tokens": [50384, 934, 321, 600, 1096, 300, 321, 393, 1564, 797, 319, 12, 21541, 1169, 439, 295, 264, 14862, 14893, 50780], "temperature": 0.0, "avg_logprob": -0.16774159557414506, "compression_ratio": 1.556338028169014, "no_speech_prob": 0.0019257419044151902}, {"id": 248, "seek": 193584, "start": 1946.24, "end": 1949.6, "text": " and the plot actually like also plot the distribution", "tokens": [50884, 293, 264, 7542, 767, 411, 611, 7542, 264, 7316, 51052], "temperature": 0.0, "avg_logprob": -0.16774159557414506, "compression_ratio": 1.556338028169014, "no_speech_prob": 0.0019257419044151902}, {"id": 249, "seek": 193584, "start": 1952.0, "end": 1958.48, "text": " so this is just to see what's the new distribution after after filtering after topk sampling", "tokens": [51172, 370, 341, 307, 445, 281, 536, 437, 311, 264, 777, 7316, 934, 934, 30822, 934, 1192, 74, 21179, 51496], "temperature": 0.0, "avg_logprob": -0.16774159557414506, "compression_ratio": 1.556338028169014, "no_speech_prob": 0.0019257419044151902}, {"id": 250, "seek": 195848, "start": 1959.44, "end": 1966.8, "text": " now how it looks like and yeah we'll see we'll see in a few minutes", "tokens": [50412, 586, 577, 309, 1542, 411, 293, 1338, 321, 603, 536, 321, 603, 536, 294, 257, 1326, 2077, 50780], "temperature": 0.0, "avg_logprob": -0.11072258949279785, "compression_ratio": 1.575, "no_speech_prob": 0.0056387451477348804}, {"id": 251, "seek": 195848, "start": 1968.64, "end": 1971.52, "text": " when this is done and we can see the", "tokens": [50872, 562, 341, 307, 1096, 293, 321, 393, 536, 264, 51016], "temperature": 0.0, "avg_logprob": -0.11072258949279785, "compression_ratio": 1.575, "no_speech_prob": 0.0056387451477348804}, {"id": 252, "seek": 195848, "start": 1976.16, "end": 1985.92, "text": " the distribution we can just also see the samples themselves to see like with pandas", "tokens": [51248, 264, 7316, 321, 393, 445, 611, 536, 264, 10938, 2969, 281, 536, 411, 365, 4565, 296, 51736], "temperature": 0.0, "avg_logprob": -0.11072258949279785, "compression_ratio": 1.575, "no_speech_prob": 0.0056387451477348804}, {"id": 253, "seek": 198592, "start": 1986.64, "end": 1993.52, "text": " how it looks like like we've done at the very beginning of this notebook here and we'll see", "tokens": [50400, 577, 309, 1542, 411, 411, 321, 600, 1096, 412, 264, 588, 2863, 295, 341, 21060, 510, 293, 321, 603, 536, 50744], "temperature": 0.0, "avg_logprob": -0.1083034975775357, "compression_ratio": 1.75, "no_speech_prob": 0.0044641499407589436}, {"id": 254, "seek": 198592, "start": 1993.52, "end": 2000.64, "text": " like how many samples remain and finally I want to mention chat templates so there's a need to", "tokens": [50744, 411, 577, 867, 10938, 6222, 293, 2721, 286, 528, 281, 2152, 5081, 21165, 370, 456, 311, 257, 643, 281, 51100], "temperature": 0.0, "avg_logprob": -0.1083034975775357, "compression_ratio": 1.75, "no_speech_prob": 0.0044641499407589436}, {"id": 255, "seek": 198592, "start": 2000.64, "end": 2005.68, "text": " define a chat template if you want to use your large language model as a chat bot there are", "tokens": [51100, 6964, 257, 5081, 12379, 498, 291, 528, 281, 764, 428, 2416, 2856, 2316, 382, 257, 5081, 10592, 456, 366, 51352], "temperature": 0.0, "avg_logprob": -0.1083034975775357, "compression_ratio": 1.75, "no_speech_prob": 0.0044641499407589436}, {"id": 256, "seek": 198592, "start": 2005.68, "end": 2012.0, "text": " different ways of doing it here's a way of doing it we have a raw user content either", "tokens": [51352, 819, 2098, 295, 884, 309, 510, 311, 257, 636, 295, 884, 309, 321, 362, 257, 8936, 4195, 2701, 2139, 51668], "temperature": 0.0, "avg_logprob": -0.1083034975775357, "compression_ratio": 1.75, "no_speech_prob": 0.0044641499407589436}, {"id": 257, "seek": 201200, "start": 2012.96, "end": 2015.84, "text": " raw assistant so this is more like the raw data", "tokens": [50412, 8936, 10994, 370, 341, 307, 544, 411, 264, 8936, 1412, 50556], "temperature": 0.0, "avg_logprob": -0.16403433981906163, "compression_ratio": 2.0939226519337018, "no_speech_prob": 0.003425338538363576}, {"id": 258, "seek": 201200, "start": 2017.92, "end": 2024.0, "text": " this is a format that you can use just user two points and then the message then assistant", "tokens": [50660, 341, 307, 257, 7877, 300, 291, 393, 764, 445, 4195, 732, 2793, 293, 550, 264, 3636, 550, 10994, 50964], "temperature": 0.0, "avg_logprob": -0.16403433981906163, "compression_ratio": 2.0939226519337018, "no_speech_prob": 0.003425338538363576}, {"id": 259, "seek": 201200, "start": 2024.0, "end": 2030.4, "text": " two point nice nice to meet you in the case of flamato you have this particular template so", "tokens": [50964, 732, 935, 1481, 1481, 281, 1677, 291, 294, 264, 1389, 295, 932, 335, 2513, 291, 362, 341, 1729, 12379, 370, 51284], "temperature": 0.0, "avg_logprob": -0.16403433981906163, "compression_ratio": 2.0939226519337018, "no_speech_prob": 0.003425338538363576}, {"id": 260, "seek": 201200, "start": 2030.4, "end": 2033.92, "text": " you have this token s then you have the instruction you have space", "tokens": [51284, 291, 362, 341, 14862, 262, 550, 291, 362, 264, 10951, 291, 362, 1901, 51460], "temperature": 0.0, "avg_logprob": -0.16403433981906163, "compression_ratio": 2.0939226519337018, "no_speech_prob": 0.003425338538363576}, {"id": 261, "seek": 201200, "start": 2036.24, "end": 2040.16, "text": " not not not the instruction it's it's another token for instruction then you have", "tokens": [51576, 406, 406, 406, 264, 10951, 309, 311, 309, 311, 1071, 14862, 337, 10951, 550, 291, 362, 51772], "temperature": 0.0, "avg_logprob": -0.16403433981906163, "compression_ratio": 2.0939226519337018, "no_speech_prob": 0.003425338538363576}, {"id": 262, "seek": 204016, "start": 2041.1200000000001, "end": 2046.0, "text": " a sys you have the system prompt you have here the user prompt and finally the model answer", "tokens": [50412, 257, 262, 749, 291, 362, 264, 1185, 12391, 291, 362, 510, 264, 4195, 12391, 293, 2721, 264, 2316, 1867, 50656], "temperature": 0.0, "avg_logprob": -0.15035354957151947, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0013035944430157542}, {"id": 263, "seek": 204016, "start": 2046.0, "end": 2051.6, "text": " it's quite a difficult template you we don't need to use it to function our lamato model", "tokens": [50656, 309, 311, 1596, 257, 2252, 12379, 291, 321, 500, 380, 643, 281, 764, 309, 281, 2445, 527, 24688, 2513, 2316, 50936], "temperature": 0.0, "avg_logprob": -0.15035354957151947, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0013035944430157542}, {"id": 264, "seek": 204016, "start": 2051.6, "end": 2056.8, "text": " because we're functioning the base model and this chat template is only used in the chat version", "tokens": [50936, 570, 321, 434, 18483, 264, 3096, 2316, 293, 341, 5081, 12379, 307, 787, 1143, 294, 264, 5081, 3037, 51196], "temperature": 0.0, "avg_logprob": -0.15035354957151947, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0013035944430157542}, {"id": 265, "seek": 204016, "start": 2056.8, "end": 2064.16, "text": " of lamato this is not the one that we use here I wanted to mention the chat ml template from open", "tokens": [51196, 295, 24688, 2513, 341, 307, 406, 264, 472, 300, 321, 764, 510, 286, 1415, 281, 2152, 264, 5081, 23271, 12379, 490, 1269, 51564], "temperature": 0.0, "avg_logprob": -0.15035354957151947, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.0013035944430157542}, {"id": 266, "seek": 206416, "start": 2064.24, "end": 2070.48, "text": " ai it looks like this it's the most popular and standardized one you can see it in a lot of", "tokens": [50368, 9783, 309, 1542, 411, 341, 309, 311, 264, 881, 3743, 293, 31677, 472, 291, 393, 536, 309, 294, 257, 688, 295, 50680], "temperature": 0.0, "avg_logprob": -0.06123715258659201, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.01361297257244587}, {"id": 267, "seek": 206416, "start": 2070.48, "end": 2075.2, "text": " state-of-the-art open source models we're not going to use this one because it requires adding", "tokens": [50680, 1785, 12, 2670, 12, 3322, 12, 446, 1269, 4009, 5245, 321, 434, 406, 516, 281, 764, 341, 472, 570, 309, 7029, 5127, 50916], "temperature": 0.0, "avg_logprob": -0.06123715258659201, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.01361297257244587}, {"id": 268, "seek": 206416, "start": 2075.2, "end": 2081.3599999999997, "text": " tokens it's more difficult so the one that we're going to use is going to be quite simple we're", "tokens": [50916, 22667, 309, 311, 544, 2252, 370, 264, 472, 300, 321, 434, 516, 281, 764, 307, 516, 281, 312, 1596, 2199, 321, 434, 51224], "temperature": 0.0, "avg_logprob": -0.06123715258659201, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.01361297257244587}, {"id": 269, "seek": 206416, "start": 2081.3599999999997, "end": 2091.04, "text": " going to create a function called chat template about it with an example and in this example we're", "tokens": [51224, 516, 281, 1884, 257, 2445, 1219, 5081, 12379, 466, 309, 365, 364, 1365, 293, 294, 341, 1365, 321, 434, 51708], "temperature": 0.0, "avg_logprob": -0.06123715258659201, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.01361297257244587}, {"id": 270, "seek": 209104, "start": 2091.04, "end": 2102.64, "text": " going to format we format the instruction and here we're going to use this instruction then", "tokens": [50364, 516, 281, 7877, 321, 7877, 264, 10951, 293, 510, 321, 434, 516, 281, 764, 341, 10951, 550, 50944], "temperature": 0.0, "avg_logprob": -0.07645531296730042, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0013881584163755178}, {"id": 271, "seek": 209104, "start": 2102.64, "end": 2114.08, "text": " break line then we can finally put the original instruction and break line break line and here", "tokens": [50944, 1821, 1622, 550, 321, 393, 2721, 829, 264, 3380, 10951, 293, 1821, 1622, 1821, 1622, 293, 510, 51516], "temperature": 0.0, "avg_logprob": -0.07645531296730042, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.0013881584163755178}, {"id": 272, "seek": 211408, "start": 2114.08, "end": 2121.44, "text": " we can put like output or response and go for response and another break line why this one", "tokens": [50364, 321, 393, 829, 411, 5598, 420, 4134, 293, 352, 337, 4134, 293, 1071, 1821, 1622, 983, 341, 472, 50732], "temperature": 0.0, "avg_logprob": -0.10034150925893633, "compression_ratio": 1.6647398843930636, "no_speech_prob": 0.004977457225322723}, {"id": 273, "seek": 211408, "start": 2121.44, "end": 2126.72, "text": " honestly there's no good reason you could imagine a lot of different prompt templates but it's going", "tokens": [50732, 6095, 456, 311, 572, 665, 1778, 291, 727, 3811, 257, 688, 295, 819, 12391, 21165, 457, 309, 311, 516, 50996], "temperature": 0.0, "avg_logprob": -0.10034150925893633, "compression_ratio": 1.6647398843930636, "no_speech_prob": 0.004977457225322723}, {"id": 274, "seek": 211408, "start": 2126.72, "end": 2133.92, "text": " to be nice to see that the function model will follow this prompt template finally we can return", "tokens": [50996, 281, 312, 1481, 281, 536, 300, 264, 2445, 2316, 486, 1524, 341, 12391, 12379, 2721, 321, 393, 2736, 51356], "temperature": 0.0, "avg_logprob": -0.10034150925893633, "compression_ratio": 1.6647398843930636, "no_speech_prob": 0.004977457225322723}, {"id": 275, "seek": 213392, "start": 2134.0, "end": 2141.28, "text": " example and we map that using the map method from the data set object and this will", "tokens": [50368, 1365, 293, 321, 4471, 300, 1228, 264, 4471, 3170, 490, 264, 1412, 992, 2657, 293, 341, 486, 50732], "temperature": 0.0, "avg_logprob": -0.13777259088331653, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.03111102804541588}, {"id": 276, "seek": 213392, "start": 2143.84, "end": 2148.64, "text": " yeah this will change all of our instructions so they can follow this template we're going to", "tokens": [50860, 1338, 341, 486, 1319, 439, 295, 527, 9415, 370, 436, 393, 1524, 341, 12379, 321, 434, 516, 281, 51100], "temperature": 0.0, "avg_logprob": -0.13777259088331653, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.03111102804541588}, {"id": 277, "seek": 213392, "start": 2148.64, "end": 2158.88, "text": " visualize it when okay it's done let's go back a bit earlier so we managed to remove", "tokens": [51100, 23273, 309, 562, 1392, 309, 311, 1096, 718, 311, 352, 646, 257, 857, 3071, 370, 321, 6453, 281, 4159, 51612], "temperature": 0.0, "avg_logprob": -0.13777259088331653, "compression_ratio": 1.5688622754491017, "no_speech_prob": 0.03111102804541588}, {"id": 278, "seek": 215888, "start": 2158.88, "end": 2167.84, "text": " sorry a bit earlier so yeah we filtered everything that we wanted to filter we filtered out like", "tokens": [50364, 2597, 257, 857, 3071, 370, 1338, 321, 37111, 1203, 300, 321, 1415, 281, 6608, 321, 37111, 484, 411, 50812], "temperature": 0.0, "avg_logprob": -0.1553640646093032, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.0014317580498754978}, {"id": 279, "seek": 215888, "start": 2168.4, "end": 2175.84, "text": " 8k samples like I mentioned previously then there's a top case sampling where we said we only want to", "tokens": [50840, 1649, 74, 10938, 411, 286, 2835, 8046, 550, 456, 311, 257, 1192, 1389, 21179, 689, 321, 848, 321, 787, 528, 281, 51212], "temperature": 0.0, "avg_logprob": -0.1553640646093032, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.0014317580498754978}, {"id": 280, "seek": 215888, "start": 2176.48, "end": 2186.2400000000002, "text": " keep the top 1000 samples in terms of token counts so the one with the most tokens and you can see", "tokens": [51244, 1066, 264, 1192, 9714, 10938, 294, 2115, 295, 14862, 14893, 370, 264, 472, 365, 264, 881, 22667, 293, 291, 393, 536, 51732], "temperature": 0.0, "avg_logprob": -0.1553640646093032, "compression_ratio": 1.6054054054054054, "no_speech_prob": 0.0014317580498754978}, {"id": 281, "seek": 218624, "start": 2186.24, "end": 2193.3599999999997, "text": " here the distribution of token counts for instruction only for token counts and finally", "tokens": [50364, 510, 264, 7316, 295, 14862, 14893, 337, 10951, 787, 337, 14862, 14893, 293, 2721, 50720], "temperature": 0.0, "avg_logprob": -0.07804022337260999, "compression_ratio": 1.931578947368421, "no_speech_prob": 0.0020487315487116575}, {"id": 282, "seek": 218624, "start": 2194.16, "end": 2200.0, "text": " the distribution of token counts for instruction for output you can see we don't have samples with", "tokens": [50760, 264, 7316, 295, 14862, 14893, 337, 10951, 337, 5598, 291, 393, 536, 321, 500, 380, 362, 10938, 365, 51052], "temperature": 0.0, "avg_logprob": -0.07804022337260999, "compression_ratio": 1.931578947368421, "no_speech_prob": 0.0020487315487116575}, {"id": 283, "seek": 218624, "start": 2200.0, "end": 2206.8799999999997, "text": " less than 1000 samples thanks to the top case sampling yeah it should make sense hopefully", "tokens": [51052, 1570, 813, 9714, 10938, 3231, 281, 264, 1192, 1389, 21179, 1338, 309, 820, 652, 2020, 4696, 51396], "temperature": 0.0, "avg_logprob": -0.07804022337260999, "compression_ratio": 1.931578947368421, "no_speech_prob": 0.0020487315487116575}, {"id": 284, "seek": 218624, "start": 2206.8799999999997, "end": 2212.3199999999997, "text": " so here we have 1000 samples with a lot of tokens and they should be high quality because", "tokens": [51396, 370, 510, 321, 362, 9714, 10938, 365, 257, 688, 295, 22667, 293, 436, 820, 312, 1090, 3125, 570, 51668], "temperature": 0.0, "avg_logprob": -0.07804022337260999, "compression_ratio": 1.931578947368421, "no_speech_prob": 0.0020487315487116575}, {"id": 285, "seek": 221232, "start": 2212.32, "end": 2217.52, "text": " they're not close to each other we need to duplicate them so it means that", "tokens": [50364, 436, 434, 406, 1998, 281, 1184, 661, 321, 643, 281, 23976, 552, 370, 309, 1355, 300, 50624], "temperature": 0.0, "avg_logprob": -0.07212401571727935, "compression_ratio": 1.69377990430622, "no_speech_prob": 0.0015006428584456444}, {"id": 286, "seek": 221232, "start": 2217.52, "end": 2223.6800000000003, "text": " they should be pretty far away from each other here you can see all the 1000 rows once again", "tokens": [50624, 436, 820, 312, 1238, 1400, 1314, 490, 1184, 661, 510, 291, 393, 536, 439, 264, 9714, 13241, 1564, 797, 50932], "temperature": 0.0, "avg_logprob": -0.07212401571727935, "compression_ratio": 1.69377990430622, "no_speech_prob": 0.0015006428584456444}, {"id": 287, "seek": 221232, "start": 2223.6800000000003, "end": 2229.04, "text": " you can click it here if you want to have a good overview of the samples that we that were selected", "tokens": [50932, 291, 393, 2052, 309, 510, 498, 291, 528, 281, 362, 257, 665, 12492, 295, 264, 10938, 300, 321, 300, 645, 8209, 51200], "temperature": 0.0, "avg_logprob": -0.07212401571727935, "compression_ratio": 1.69377990430622, "no_speech_prob": 0.0015006428584456444}, {"id": 288, "seek": 221232, "start": 2229.76, "end": 2236.4, "text": " and now they should follow our chat template so just let's check if if this is correct", "tokens": [51236, 293, 586, 436, 820, 1524, 527, 5081, 12379, 370, 445, 718, 311, 1520, 498, 498, 341, 307, 3006, 51568], "temperature": 0.0, "avg_logprob": -0.07212401571727935, "compression_ratio": 1.69377990430622, "no_speech_prob": 0.0015006428584456444}, {"id": 289, "seek": 223640, "start": 2237.04, "end": 2244.88, "text": " and here you can see the instruction let's click it here we really have like the instruction", "tokens": [50396, 293, 510, 291, 393, 536, 264, 10951, 718, 311, 2052, 309, 510, 321, 534, 362, 411, 264, 10951, 50788], "temperature": 0.0, "avg_logprob": -0.10607521651221104, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.010645764879882336}, {"id": 290, "seek": 223640, "start": 2244.88, "end": 2253.12, "text": " and the response as mentioned here so this is working as intended instruction response and", "tokens": [50788, 293, 264, 4134, 382, 2835, 510, 370, 341, 307, 1364, 382, 10226, 10951, 4134, 293, 51200], "temperature": 0.0, "avg_logprob": -0.10607521651221104, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.010645764879882336}, {"id": 291, "seek": 223640, "start": 2253.12, "end": 2262.96, "text": " here is the response that we want the model to follow all right so this is done and the final", "tokens": [51200, 510, 307, 264, 4134, 300, 321, 528, 264, 2316, 281, 1524, 439, 558, 370, 341, 307, 1096, 293, 264, 2572, 51692], "temperature": 0.0, "avg_logprob": -0.10607521651221104, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.010645764879882336}, {"id": 292, "seek": 226296, "start": 2262.96, "end": 2268.96, "text": " thing that we can do here this is optional this is if you have a hugging phase hub count if you", "tokens": [50364, 551, 300, 321, 393, 360, 510, 341, 307, 17312, 341, 307, 498, 291, 362, 257, 41706, 5574, 11838, 1207, 498, 291, 50664], "temperature": 0.0, "avg_logprob": -0.11501862251595275, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.008308850228786469}, {"id": 293, "seek": 226296, "start": 2270.56, "end": 2277.04, "text": " put the value here of your secrets then you can just push the data set to the hugging phase hub", "tokens": [50744, 829, 264, 2158, 510, 295, 428, 14093, 550, 291, 393, 445, 2944, 264, 1412, 992, 281, 264, 41706, 5574, 11838, 51068], "temperature": 0.0, "avg_logprob": -0.11501862251595275, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.008308850228786469}, {"id": 294, "seek": 226296, "start": 2277.92, "end": 2278.4, "text": " like this", "tokens": [51112, 411, 341, 51136], "temperature": 0.0, "avg_logprob": -0.11501862251595275, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.008308850228786469}, {"id": 295, "seek": 226296, "start": 2282.2400000000002, "end": 2290.48, "text": " and we specify the token here i'm calling it mini platpus you can call it however you want", "tokens": [51328, 293, 321, 16500, 264, 14862, 510, 741, 478, 5141, 309, 8382, 3403, 31624, 291, 393, 818, 309, 4461, 291, 528, 51740], "temperature": 0.0, "avg_logprob": -0.11501862251595275, "compression_ratio": 1.7804878048780488, "no_speech_prob": 0.008308850228786469}, {"id": 296, "seek": 229048, "start": 2291.04, "end": 2300.32, "text": " um and this is going to upload it and you can even check if it's correctly uploaded if I go", "tokens": [50392, 1105, 293, 341, 307, 516, 281, 6580, 309, 293, 291, 393, 754, 1520, 498, 309, 311, 8944, 17135, 498, 286, 352, 50856], "temperature": 0.0, "avg_logprob": -0.12869096902700572, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.005277368240058422}, {"id": 297, "seek": 229048, "start": 2300.32, "end": 2306.2400000000002, "text": " to my hugging phase account and I check my data set this one was uploaded dated less than one", "tokens": [50856, 281, 452, 41706, 5574, 2696, 293, 286, 1520, 452, 1412, 992, 341, 472, 390, 17135, 23804, 1570, 813, 472, 51152], "temperature": 0.0, "avg_logprob": -0.12869096902700572, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.005277368240058422}, {"id": 298, "seek": 229048, "start": 2306.2400000000002, "end": 2313.84, "text": " minute ago and here you can see our entire data set uh cool so we have everything", "tokens": [51152, 3456, 2057, 293, 510, 291, 393, 536, 527, 2302, 1412, 992, 2232, 1627, 370, 321, 362, 1203, 51532], "temperature": 0.0, "avg_logprob": -0.12869096902700572, "compression_ratio": 1.638036809815951, "no_speech_prob": 0.005277368240058422}, {"id": 299, "seek": 231384, "start": 2314.1600000000003, "end": 2322.8, "text": " now uh and we're ready to go to fine tuning uh I hope it was it was clear uh and if not I hope", "tokens": [50380, 586, 2232, 293, 321, 434, 1919, 281, 352, 281, 2489, 15164, 2232, 286, 1454, 309, 390, 309, 390, 1850, 2232, 293, 498, 406, 286, 1454, 50812], "temperature": 0.0, "avg_logprob": -0.15459957943167738, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.005636279471218586}, {"id": 300, "seek": 231384, "start": 2322.8, "end": 2329.84, "text": " that the solution notebook will help you uh to to to create your own data sets to go further beyond", "tokens": [50812, 300, 264, 3827, 21060, 486, 854, 291, 2232, 281, 281, 281, 1884, 428, 1065, 1412, 6352, 281, 352, 3052, 4399, 51164], "temperature": 0.0, "avg_logprob": -0.15459957943167738, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.005636279471218586}, {"id": 301, "seek": 231384, "start": 2329.84, "end": 2336.1600000000003, "text": " that you can create synthetic data using um 24 it's something that is used quite a lot and it", "tokens": [51164, 300, 291, 393, 1884, 23420, 1412, 1228, 1105, 4022, 309, 311, 746, 300, 307, 1143, 1596, 257, 688, 293, 309, 51480], "temperature": 0.0, "avg_logprob": -0.15459957943167738, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.005636279471218586}, {"id": 302, "seek": 231384, "start": 2336.1600000000003, "end": 2341.76, "text": " creates uh really good uh data sets so it is something that you can play with and otherwise", "tokens": [51480, 7829, 2232, 534, 665, 2232, 1412, 6352, 370, 309, 307, 746, 300, 291, 393, 862, 365, 293, 5911, 51760], "temperature": 0.0, "avg_logprob": -0.15459957943167738, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.005636279471218586}, {"id": 303, "seek": 234176, "start": 2341.76, "end": 2348.0800000000004, "text": " it's really like a manual reviewing you can import this data set in google sheets and really", "tokens": [50364, 309, 311, 534, 411, 257, 9688, 19576, 291, 393, 974, 341, 1412, 992, 294, 20742, 15421, 293, 534, 50680], "temperature": 0.0, "avg_logprob": -0.1119620232355027, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0014998316764831543}, {"id": 304, "seek": 234176, "start": 2348.0800000000004, "end": 2355.44, "text": " manually review every row possible maybe create some regex to automate the process a little bit", "tokens": [50680, 16945, 3131, 633, 5386, 1944, 1310, 1884, 512, 319, 432, 87, 281, 31605, 264, 1399, 257, 707, 857, 51048], "temperature": 0.0, "avg_logprob": -0.1119620232355027, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0014998316764831543}, {"id": 305, "seek": 234176, "start": 2355.44, "end": 2362.2400000000002, "text": " but this is a very time consuming process but it's also like very nice because then people", "tokens": [51048, 457, 341, 307, 257, 588, 565, 19867, 1399, 457, 309, 311, 611, 411, 588, 1481, 570, 550, 561, 51388], "temperature": 0.0, "avg_logprob": -0.1119620232355027, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0014998316764831543}, {"id": 306, "seek": 234176, "start": 2362.2400000000002, "end": 2369.76, "text": " can reuse your data sets if you share them but now let's go to the fine tuning uh notebook", "tokens": [51388, 393, 26225, 428, 1412, 6352, 498, 291, 2073, 552, 457, 586, 718, 311, 352, 281, 264, 2489, 15164, 2232, 21060, 51764], "temperature": 0.0, "avg_logprob": -0.1119620232355027, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.0014998316764831543}, {"id": 307, "seek": 236976, "start": 2370.6400000000003, "end": 2377.6800000000003, "text": " you should also have it uh let me uh check the the chat uh okay you have the solution notebook", "tokens": [50408, 291, 820, 611, 362, 309, 2232, 718, 385, 2232, 1520, 264, 264, 5081, 2232, 1392, 291, 362, 264, 3827, 21060, 50760], "temperature": 0.0, "avg_logprob": -0.1331653380661868, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.002211665501818061}, {"id": 308, "seek": 236976, "start": 2378.5600000000004, "end": 2386.8, "text": " cool um so here you have um lama 2 and we're going to delve deep into the fine tuning process so", "tokens": [50804, 1627, 1105, 370, 510, 291, 362, 1105, 45423, 568, 293, 321, 434, 516, 281, 43098, 2452, 666, 264, 2489, 15164, 1399, 370, 51216], "temperature": 0.0, "avg_logprob": -0.1331653380661868, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.002211665501818061}, {"id": 309, "seek": 236976, "start": 2386.8, "end": 2390.88, "text": " as mentioned previously there are two ways of fine tuning these models this supervised fine", "tokens": [51216, 382, 2835, 8046, 456, 366, 732, 2098, 295, 2489, 15164, 613, 5245, 341, 46533, 2489, 51420], "temperature": 0.0, "avg_logprob": -0.1331653380661868, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.002211665501818061}, {"id": 310, "seek": 236976, "start": 2390.88, "end": 2397.2000000000003, "text": " tuning this is what we're gonna do uh so we're gonna tune it on a set of instructions and responses", "tokens": [51420, 15164, 341, 307, 437, 321, 434, 799, 360, 2232, 370, 321, 434, 799, 10864, 309, 322, 257, 992, 295, 9415, 293, 13019, 51736], "temperature": 0.0, "avg_logprob": -0.1331653380661868, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.002211665501818061}, {"id": 311, "seek": 239720, "start": 2397.2, "end": 2404.7999999999997, "text": " it's going to help the model focus where we want um so to be helpful to follow the chat template too", "tokens": [50364, 309, 311, 516, 281, 854, 264, 2316, 1879, 689, 321, 528, 1105, 370, 281, 312, 4961, 281, 1524, 264, 5081, 12379, 886, 50744], "temperature": 0.0, "avg_logprob": -0.09174131263386119, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00393798528239131}, {"id": 312, "seek": 239720, "start": 2405.3599999999997, "end": 2411.04, "text": " and there's also the reinforcement learning from human feedback where we want the model to", "tokens": [50772, 293, 456, 311, 611, 264, 29280, 2539, 490, 1952, 5824, 689, 321, 528, 264, 2316, 281, 51056], "temperature": 0.0, "avg_logprob": -0.09174131263386119, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00393798528239131}, {"id": 313, "seek": 239720, "start": 2411.04, "end": 2415.9199999999996, "text": " maximize your word signal i'm not going to delve into that uh there are a lot of good articles", "tokens": [51056, 19874, 428, 1349, 6358, 741, 478, 406, 516, 281, 43098, 666, 300, 2232, 456, 366, 257, 688, 295, 665, 11290, 51300], "temperature": 0.0, "avg_logprob": -0.09174131263386119, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00393798528239131}, {"id": 314, "seek": 239720, "start": 2415.9199999999996, "end": 2422.56, "text": " about it uh it's able to capture more complex uh preferences but it's also like more difficult to", "tokens": [51300, 466, 309, 2232, 309, 311, 1075, 281, 7983, 544, 3997, 2232, 21910, 457, 309, 311, 611, 411, 544, 2252, 281, 51632], "temperature": 0.0, "avg_logprob": -0.09174131263386119, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00393798528239131}, {"id": 315, "seek": 242256, "start": 2422.56, "end": 2432.72, "text": " implement and in practice uh most um if not all this this year but except this year nearly all", "tokens": [50364, 4445, 293, 294, 3124, 2232, 881, 1105, 498, 406, 439, 341, 341, 1064, 457, 3993, 341, 1064, 6217, 439, 50872], "temperature": 0.0, "avg_logprob": -0.14872396909273589, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006572902202606201}, {"id": 316, "seek": 242256, "start": 2432.72, "end": 2438.16, "text": " the state-of-the-art open source LLMs just use supervised fine tuning so yeah something to keep", "tokens": [50872, 264, 1785, 12, 2670, 12, 3322, 12, 446, 1269, 4009, 441, 43, 26386, 445, 764, 46533, 2489, 15164, 370, 1338, 746, 281, 1066, 51144], "temperature": 0.0, "avg_logprob": -0.14872396909273589, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006572902202606201}, {"id": 317, "seek": 242256, "start": 2438.16, "end": 2445.44, "text": " in mind um and once again there's an example uh from a few months ago now the NEMA paper that", "tokens": [51144, 294, 1575, 1105, 293, 1564, 797, 456, 311, 364, 1365, 2232, 490, 257, 1326, 2493, 2057, 586, 264, 426, 24891, 3035, 300, 51508], "temperature": 0.0, "avg_logprob": -0.14872396909273589, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006572902202606201}, {"id": 318, "seek": 242256, "start": 2445.44, "end": 2451.12, "text": " shows that only 1000 high quality samples can really make the difference and and get very far", "tokens": [51508, 3110, 300, 787, 9714, 1090, 3125, 10938, 393, 534, 652, 264, 2649, 293, 293, 483, 588, 1400, 51792], "temperature": 0.0, "avg_logprob": -0.14872396909273589, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.006572902202606201}, {"id": 319, "seek": 245112, "start": 2451.12, "end": 2457.8399999999997, "text": " um in this case when you have a 65 billion models um and i want to mention the open LM", "tokens": [50364, 1105, 294, 341, 1389, 562, 291, 362, 257, 11624, 5218, 5245, 1105, 293, 741, 528, 281, 2152, 264, 1269, 46529, 50700], "temperature": 0.0, "avg_logprob": -0.1717130252293178, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.0028371866792440414}, {"id": 320, "seek": 245112, "start": 2457.8399999999997, "end": 2464.3199999999997, "text": " leaderboard you might be familiar with it um but this is quite useful to see uh what are the best", "tokens": [50700, 5263, 3787, 291, 1062, 312, 4963, 365, 309, 1105, 457, 341, 307, 1596, 4420, 281, 536, 2232, 437, 366, 264, 1151, 51024], "temperature": 0.0, "avg_logprob": -0.1717130252293178, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.0028371866792440414}, {"id": 321, "seek": 245112, "start": 2464.3199999999997, "end": 2471.52, "text": " models so currently you have like this non-Lama model um i wanted to uh yeah show this Godzilla", "tokens": [51024, 5245, 370, 4362, 291, 362, 411, 341, 2107, 12, 43, 2404, 2316, 1105, 741, 1415, 281, 2232, 1338, 855, 341, 38046, 51384], "temperature": 0.0, "avg_logprob": -0.1717130252293178, "compression_ratio": 1.5469613259668509, "no_speech_prob": 0.0028371866792440414}, {"id": 322, "seek": 247152, "start": 2471.6, "end": 2481.12, "text": " 2 7b model because um it's using a Lama 2 70b and um uh i saw that it was using my data set so", "tokens": [50368, 568, 1614, 65, 2316, 570, 1105, 309, 311, 1228, 257, 441, 2404, 568, 5285, 65, 293, 1105, 2232, 741, 1866, 300, 309, 390, 1228, 452, 1412, 992, 370, 50844], "temperature": 0.0, "avg_logprob": -0.14487207388576073, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.026703940704464912}, {"id": 323, "seek": 247152, "start": 2481.12, "end": 2486.32, "text": " when i was telling you like yeah it's nice to also share your data set you see like sometimes it can", "tokens": [50844, 562, 741, 390, 3585, 291, 411, 1338, 309, 311, 1481, 281, 611, 2073, 428, 1412, 992, 291, 536, 411, 2171, 309, 393, 51104], "temperature": 0.0, "avg_logprob": -0.14487207388576073, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.026703940704464912}, {"id": 324, "seek": 247152, "start": 2486.32, "end": 2492.4, "text": " be reused by other people without uh you knowing anything about it but i'm glad this one was useful", "tokens": [51104, 312, 319, 4717, 538, 661, 561, 1553, 2232, 291, 5276, 1340, 466, 309, 457, 741, 478, 5404, 341, 472, 390, 4420, 51408], "temperature": 0.0, "avg_logprob": -0.14487207388576073, "compression_ratio": 1.5775401069518717, "no_speech_prob": 0.026703940704464912}, {"id": 325, "seek": 249240, "start": 2492.96, "end": 2500.8, "text": " um so what we're going to do here is um as previously we are going to start by installing", "tokens": [50392, 1105, 370, 437, 321, 434, 516, 281, 360, 510, 307, 1105, 382, 8046, 321, 366, 516, 281, 722, 538, 20762, 50784], "temperature": 0.0, "avg_logprob": -0.09820221210348196, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.020634543150663376}, {"id": 326, "seek": 249240, "start": 2500.8, "end": 2508.0, "text": " all the libraries that we want so in this case we're going to go pip install queue and we're", "tokens": [50784, 439, 264, 15148, 300, 321, 528, 370, 294, 341, 1389, 321, 434, 516, 281, 352, 8489, 3625, 18639, 293, 321, 434, 51144], "temperature": 0.0, "avg_logprob": -0.09820221210348196, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.020634543150663376}, {"id": 327, "seek": 249240, "start": 2508.0, "end": 2513.92, "text": " going to update because uh Google collab already has some of these libraries but we want to use", "tokens": [51144, 516, 281, 5623, 570, 2232, 3329, 44228, 1217, 575, 512, 295, 613, 15148, 457, 321, 528, 281, 764, 51440], "temperature": 0.0, "avg_logprob": -0.09820221210348196, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.020634543150663376}, {"id": 328, "seek": 249240, "start": 2513.92, "end": 2521.76, "text": " really like the latest version available in this case bits and bytes and 1db transformers", "tokens": [51440, 534, 411, 264, 6792, 3037, 2435, 294, 341, 1389, 9239, 293, 36088, 293, 502, 67, 65, 4088, 433, 51832], "temperature": 0.0, "avg_logprob": -0.09820221210348196, "compression_ratio": 1.803921568627451, "no_speech_prob": 0.020634543150663376}, {"id": 329, "seek": 252176, "start": 2521.76, "end": 2531.1200000000003, "text": " and the hugging face oops i'm going to disconnect this session okay uh once again you can use a", "tokens": [50364, 293, 264, 41706, 1851, 34166, 741, 478, 516, 281, 14299, 341, 5481, 1392, 2232, 1564, 797, 291, 393, 764, 257, 50832], "temperature": 0.0, "avg_logprob": -0.10555267333984375, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012050746008753777}, {"id": 330, "seek": 252176, "start": 2531.1200000000003, "end": 2536.88, "text": " t4 gpu for the entire uh notebook here i'm just going to use the v100 because it's going to be", "tokens": [50832, 256, 19, 290, 34859, 337, 264, 2302, 2232, 21060, 510, 741, 478, 445, 516, 281, 764, 264, 371, 6879, 570, 309, 311, 516, 281, 312, 51120], "temperature": 0.0, "avg_logprob": -0.10555267333984375, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012050746008753777}, {"id": 331, "seek": 252176, "start": 2536.88, "end": 2543.28, "text": " faster transformers for the transformers data set for the data set accelerating it's it's to make", "tokens": [51120, 4663, 4088, 433, 337, 264, 4088, 433, 1412, 992, 337, 264, 1412, 992, 34391, 309, 311, 309, 311, 281, 652, 51440], "temperature": 0.0, "avg_logprob": -0.10555267333984375, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012050746008753777}, {"id": 332, "seek": 252176, "start": 2543.28, "end": 2549.28, "text": " things uh faster pft it's going to be for the fine tuning process that we're going to use i will", "tokens": [51440, 721, 2232, 4663, 280, 844, 309, 311, 516, 281, 312, 337, 264, 2489, 15164, 1399, 300, 321, 434, 516, 281, 764, 741, 486, 51740], "temperature": 0.0, "avg_logprob": -0.10555267333984375, "compression_ratio": 1.8599033816425121, "no_speech_prob": 0.0012050746008753777}, {"id": 333, "seek": 254928, "start": 2549.28, "end": 2556.96, "text": " mention it later tl is a wrapper it can be used for supervised fine tuning or for reinforcement", "tokens": [50364, 2152, 309, 1780, 256, 75, 307, 257, 46906, 309, 393, 312, 1143, 337, 46533, 2489, 15164, 420, 337, 29280, 50748], "temperature": 0.0, "avg_logprob": -0.09037070327930236, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0021443304140120745}, {"id": 334, "seek": 254928, "start": 2556.96, "end": 2561.84, "text": " learning from human feedbacks we have bits and bytes for quantization because we are not going", "tokens": [50748, 2539, 490, 1952, 5824, 82, 321, 362, 9239, 293, 36088, 337, 4426, 2144, 570, 321, 366, 406, 516, 50992], "temperature": 0.0, "avg_logprob": -0.09037070327930236, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0021443304140120745}, {"id": 335, "seek": 254928, "start": 2561.84, "end": 2567.6000000000004, "text": " to use the model in full precision and 1db for reporting so we can have a nice dashboard where", "tokens": [50992, 281, 764, 264, 2316, 294, 1577, 18356, 293, 502, 67, 65, 337, 10031, 370, 321, 393, 362, 257, 1481, 18342, 689, 51280], "temperature": 0.0, "avg_logprob": -0.09037070327930236, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0021443304140120745}, {"id": 336, "seek": 254928, "start": 2567.6000000000004, "end": 2575.1200000000003, "text": " we can track the progress of our model um once again we're going to use google collab i'm just", "tokens": [51280, 321, 393, 2837, 264, 4205, 295, 527, 2316, 1105, 1564, 797, 321, 434, 516, 281, 764, 20742, 44228, 741, 478, 445, 51656], "temperature": 0.0, "avg_logprob": -0.09037070327930236, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0021443304140120745}, {"id": 337, "seek": 257512, "start": 2575.12, "end": 2583.8399999999997, "text": " going to copy paste it uh from the previous um notebook so we have our secret token", "tokens": [50364, 516, 281, 5055, 9163, 309, 2232, 490, 264, 3894, 1105, 21060, 370, 321, 362, 527, 4054, 14862, 50800], "temperature": 0.0, "avg_logprob": -0.11443651083743933, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.002082023536786437}, {"id": 338, "seek": 257512, "start": 2586.24, "end": 2593.04, "text": " current access okay um it's optional if you don't have a hugging face account once again", "tokens": [50920, 2190, 2105, 1392, 1105, 309, 311, 17312, 498, 291, 500, 380, 362, 257, 41706, 1851, 2696, 1564, 797, 51260], "temperature": 0.0, "avg_logprob": -0.11443651083743933, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.002082023536786437}, {"id": 339, "seek": 257512, "start": 2593.04, "end": 2598.56, "text": " and here we're going to import a lot of libraries we can import os we're going to import torch we're", "tokens": [51260, 293, 510, 321, 434, 516, 281, 974, 257, 688, 295, 15148, 321, 393, 974, 3003, 321, 434, 516, 281, 974, 27822, 321, 434, 51536], "temperature": 0.0, "avg_logprob": -0.11443651083743933, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.002082023536786437}, {"id": 340, "seek": 259856, "start": 2598.56, "end": 2606.0, "text": " going to import the data set from data sets and from the transformers library we need to import", "tokens": [50364, 516, 281, 974, 264, 1412, 992, 490, 1412, 6352, 293, 490, 264, 4088, 433, 6405, 321, 643, 281, 974, 50736], "temperature": 0.0, "avg_logprob": -0.1450002504431683, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.007009670604020357}, {"id": 341, "seek": 259856, "start": 2606.0, "end": 2621.2799999999997, "text": " a lot of classes uh so auto model for causal lm auto tokenizer uh bits and bytes config auto", "tokens": [50736, 257, 688, 295, 5359, 2232, 370, 8399, 2316, 337, 38755, 287, 76, 8399, 14862, 6545, 2232, 9239, 293, 36088, 6662, 8399, 51500], "temperature": 0.0, "avg_logprob": -0.1450002504431683, "compression_ratio": 1.540983606557377, "no_speech_prob": 0.007009670604020357}, {"id": 342, "seek": 262128, "start": 2621.92, "end": 2633.28, "text": " auto tokenizer uh training arguments and pipeline when we want to run it um when the model is", "tokens": [50396, 8399, 14862, 6545, 2232, 3097, 12869, 293, 15517, 562, 321, 528, 281, 1190, 309, 1105, 562, 264, 2316, 307, 50964], "temperature": 0.0, "avg_logprob": -0.14220987757047018, "compression_ratio": 1.5193798449612403, "no_speech_prob": 0.0027572421822696924}, {"id": 343, "seek": 262128, "start": 2633.28, "end": 2644.48, "text": " trained and then from pft we also need to import a few of them so lower config pft model and something", "tokens": [50964, 8895, 293, 550, 490, 280, 844, 321, 611, 643, 281, 974, 257, 1326, 295, 552, 370, 3126, 6662, 280, 844, 2316, 293, 746, 51524], "temperature": 0.0, "avg_logprob": -0.14220987757047018, "compression_ratio": 1.5193798449612403, "no_speech_prob": 0.0027572421822696924}, {"id": 344, "seek": 264448, "start": 2644.48, "end": 2657.28, "text": " called prepare model for kbit training and the last one is the wrapper for supervised training", "tokens": [50364, 1219, 5940, 2316, 337, 350, 5260, 3097, 293, 264, 1036, 472, 307, 264, 46906, 337, 46533, 3097, 51004], "temperature": 0.0, "avg_logprob": -0.21160786187470848, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.0038835930172353983}, {"id": 345, "seek": 264448, "start": 2658.16, "end": 2667.6, "text": " from the tira library called sft trainer um i'm going to let it here for for a second", "tokens": [51048, 490, 264, 256, 4271, 6405, 1219, 47095, 83, 21110, 1105, 741, 478, 516, 281, 718, 309, 510, 337, 337, 257, 1150, 51520], "temperature": 0.0, "avg_logprob": -0.21160786187470848, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.0038835930172353983}, {"id": 346, "seek": 264448, "start": 2668.2400000000002, "end": 2673.28, "text": " and um we're going to talk about uh the different ways we can find during this model so we have", "tokens": [51552, 293, 1105, 321, 434, 516, 281, 751, 466, 2232, 264, 819, 2098, 321, 393, 915, 1830, 341, 2316, 370, 321, 362, 51804], "temperature": 0.0, "avg_logprob": -0.21160786187470848, "compression_ratio": 1.6331360946745561, "no_speech_prob": 0.0038835930172353983}, {"id": 347, "seek": 267328, "start": 2673.28, "end": 2679.44, "text": " three ways there's the full fine tuning there's laura and there is uh q laura with full fine", "tokens": [50364, 1045, 2098, 456, 311, 264, 1577, 2489, 15164, 456, 311, 635, 2991, 293, 456, 307, 2232, 9505, 635, 2991, 365, 1577, 2489, 50672], "temperature": 0.0, "avg_logprob": -0.11164332429567973, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.002978332107886672}, {"id": 348, "seek": 267328, "start": 2679.44, "end": 2689.6800000000003, "text": " tuning uh we're going to use um the the entire model so we're going to uh train all the weights", "tokens": [50672, 15164, 2232, 321, 434, 516, 281, 764, 1105, 264, 264, 2302, 2316, 370, 321, 434, 516, 281, 2232, 3847, 439, 264, 17443, 51184], "temperature": 0.0, "avg_logprob": -0.11164332429567973, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.002978332107886672}, {"id": 349, "seek": 267328, "start": 2689.6800000000003, "end": 2696.0800000000004, "text": " in the model which is very costly then we have laura which instead of training all the weights", "tokens": [51184, 294, 264, 2316, 597, 307, 588, 28328, 550, 321, 362, 635, 2991, 597, 2602, 295, 3097, 439, 264, 17443, 51504], "temperature": 0.0, "avg_logprob": -0.11164332429567973, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.002978332107886672}, {"id": 350, "seek": 267328, "start": 2696.0800000000004, "end": 2702.4, "text": " we're just going to add some adapters in in some layers and we're going to only train these uh added", "tokens": [51504, 321, 434, 445, 516, 281, 909, 512, 23169, 1559, 294, 294, 512, 7914, 293, 321, 434, 516, 281, 787, 3847, 613, 2232, 3869, 51820], "temperature": 0.0, "avg_logprob": -0.11164332429567973, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.002978332107886672}, {"id": 351, "seek": 270240, "start": 2702.4, "end": 2709.2000000000003, "text": " weights uh so this really reduces the cost of training the model because we are just going to", "tokens": [50364, 17443, 2232, 370, 341, 534, 18081, 264, 2063, 295, 3097, 264, 2316, 570, 321, 366, 445, 516, 281, 50704], "temperature": 0.0, "avg_logprob": -0.08520044407374422, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0015715996269136667}, {"id": 352, "seek": 270240, "start": 2709.2000000000003, "end": 2716.2400000000002, "text": " train like one percent two percent of the entire weights and finally we have q laura which is using", "tokens": [50704, 3847, 411, 472, 3043, 732, 3043, 295, 264, 2302, 17443, 293, 2721, 321, 362, 9505, 635, 2991, 597, 307, 1228, 51056], "temperature": 0.0, "avg_logprob": -0.08520044407374422, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0015715996269136667}, {"id": 353, "seek": 270240, "start": 2716.2400000000002, "end": 2724.48, "text": " laura but with a model that has been quantized so uh in this case we're not going to use the model in", "tokens": [51056, 635, 2991, 457, 365, 257, 2316, 300, 575, 668, 4426, 1602, 370, 2232, 294, 341, 1389, 321, 434, 406, 516, 281, 764, 264, 2316, 294, 51468], "temperature": 0.0, "avg_logprob": -0.08520044407374422, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0015715996269136667}, {"id": 354, "seek": 272448, "start": 2724.48, "end": 2734.48, "text": " six-bit precision so with every weight in the model uh occupying 16 bits on the disc but instead", "tokens": [50364, 2309, 12, 5260, 18356, 370, 365, 633, 3364, 294, 264, 2316, 2232, 8073, 1840, 3165, 9239, 322, 264, 2983, 457, 2602, 50864], "temperature": 0.0, "avg_logprob": -0.08431464025419053, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.003703112481161952}, {"id": 355, "seek": 272448, "start": 2734.48, "end": 2741.68, "text": " they're just going to be quantized into four bits so we can lose a bit of precision here but in the", "tokens": [50864, 436, 434, 445, 516, 281, 312, 4426, 1602, 666, 1451, 9239, 370, 321, 393, 3624, 257, 857, 295, 18356, 510, 457, 294, 264, 51224], "temperature": 0.0, "avg_logprob": -0.08431464025419053, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.003703112481161952}, {"id": 356, "seek": 272448, "start": 2741.68, "end": 2748.56, "text": " end there are mechanisms to make it less impactful and we'll be able to get a really strong model using", "tokens": [51224, 917, 456, 366, 15902, 281, 652, 309, 1570, 30842, 293, 321, 603, 312, 1075, 281, 483, 257, 534, 2068, 2316, 1228, 51568], "temperature": 0.0, "avg_logprob": -0.08431464025419053, "compression_ratio": 1.5706806282722514, "no_speech_prob": 0.003703112481161952}, {"id": 357, "seek": 274856, "start": 2748.56, "end": 2758.4, "text": " q laura a bit of calculation here we have 16 gigabytes of VRAM with our GPU here you can see", "tokens": [50364, 9505, 635, 2991, 257, 857, 295, 17108, 510, 321, 362, 3165, 42741, 295, 13722, 2865, 365, 527, 18407, 510, 291, 393, 536, 50856], "temperature": 0.0, "avg_logprob": -0.190005738394601, "compression_ratio": 1.5080213903743316, "no_speech_prob": 0.0013879066100344062}, {"id": 358, "seek": 274856, "start": 2758.4, "end": 2769.84, "text": " it's 16 gigabytes and um lama 2 7B weights so we have seven billion parameters if they take up", "tokens": [50856, 309, 311, 3165, 42741, 293, 1105, 45423, 568, 1614, 33, 17443, 370, 321, 362, 3407, 5218, 9834, 498, 436, 747, 493, 51428], "temperature": 0.0, "avg_logprob": -0.190005738394601, "compression_ratio": 1.5080213903743316, "no_speech_prob": 0.0013879066100344062}, {"id": 359, "seek": 274856, "start": 2770.48, "end": 2778.4, "text": " two bytes it means that we're going to use 14 gigabytes so we are almost like using the entire", "tokens": [51460, 732, 36088, 309, 1355, 300, 321, 434, 516, 281, 764, 3499, 42741, 370, 321, 366, 1920, 411, 1228, 264, 2302, 51856], "temperature": 0.0, "avg_logprob": -0.190005738394601, "compression_ratio": 1.5080213903743316, "no_speech_prob": 0.0013879066100344062}, {"id": 360, "seek": 277856, "start": 2779.12, "end": 2784.4, "text": " VRAM and in addition there are like other things there's an overhead due to optimizer stays gradients", "tokens": [50392, 13722, 2865, 293, 294, 4500, 456, 366, 411, 661, 721, 456, 311, 364, 19922, 3462, 281, 5028, 6545, 10834, 2771, 2448, 50656], "temperature": 0.0, "avg_logprob": -0.1489282427607356, "compression_ratio": 1.5404040404040404, "no_speech_prob": 0.0012062708847224712}, {"id": 361, "seek": 277856, "start": 2784.4, "end": 2792.56, "text": " for all activations so it's going to be challenging but we we can manage to fit it into only 16 gigabytes", "tokens": [50656, 337, 439, 2430, 763, 370, 309, 311, 516, 281, 312, 7595, 457, 321, 321, 393, 3067, 281, 3318, 309, 666, 787, 3165, 42741, 51064], "temperature": 0.0, "avg_logprob": -0.1489282427607356, "compression_ratio": 1.5404040404040404, "no_speech_prob": 0.0012062708847224712}, {"id": 362, "seek": 277856, "start": 2792.56, "end": 2803.52, "text": " of memory okay so now we're going to really delve into the code to function it um we are going to", "tokens": [51064, 295, 4675, 1392, 370, 586, 321, 434, 516, 281, 534, 43098, 666, 264, 3089, 281, 2445, 309, 1105, 321, 366, 516, 281, 51612], "temperature": 0.0, "avg_logprob": -0.1489282427607356, "compression_ratio": 1.5404040404040404, "no_speech_prob": 0.0012062708847224712}, {"id": 363, "seek": 280352, "start": 2803.52, "end": 2813.92, "text": " reuse the news research model here like previously and we are going to give a name to our new model", "tokens": [50364, 26225, 264, 2583, 2132, 2316, 510, 411, 8046, 293, 321, 366, 516, 281, 976, 257, 1315, 281, 527, 777, 2316, 50884], "temperature": 0.0, "avg_logprob": -0.12333219191607307, "compression_ratio": 1.6496815286624205, "no_speech_prob": 0.0046080853790044785}, {"id": 364, "seek": 280352, "start": 2815.04, "end": 2821.68, "text": " so in this case i'm going to call it lama 2 7B and mini platypus", "tokens": [50940, 370, 294, 341, 1389, 741, 478, 516, 281, 818, 309, 45423, 568, 1614, 33, 293, 8382, 3403, 88, 31624, 51272], "temperature": 0.0, "avg_logprob": -0.12333219191607307, "compression_ratio": 1.6496815286624205, "no_speech_prob": 0.0046080853790044785}, {"id": 365, "seek": 280352, "start": 2824.16, "end": 2832.08, "text": " and we're going to reuse the dataset that we just created so it should be called mini platypus", "tokens": [51396, 293, 321, 434, 516, 281, 26225, 264, 28872, 300, 321, 445, 2942, 370, 309, 820, 312, 1219, 8382, 3403, 88, 31624, 51792], "temperature": 0.0, "avg_logprob": -0.12333219191607307, "compression_ratio": 1.6496815286624205, "no_speech_prob": 0.0046080853790044785}, {"id": 366, "seek": 283208, "start": 2833.04, "end": 2838.72, "text": " and we're just going to use the train splits finally we have the tokenizer", "tokens": [50412, 293, 321, 434, 445, 516, 281, 764, 264, 3847, 37741, 2721, 321, 362, 264, 14862, 6545, 50696], "temperature": 0.0, "avg_logprob": -0.10303047607684958, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.0027553890831768513}, {"id": 367, "seek": 283208, "start": 2839.52, "end": 2846.3199999999997, "text": " so here we are going to use the tokenizer from the lama 2 model", "tokens": [50736, 370, 510, 321, 366, 516, 281, 764, 264, 14862, 6545, 490, 264, 45423, 568, 2316, 51076], "temperature": 0.0, "avg_logprob": -0.10303047607684958, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.0027553890831768513}, {"id": 368, "seek": 283208, "start": 2847.84, "end": 2854.88, "text": " and we're going to use the fast version of it we are going to do something that is", "tokens": [51152, 293, 321, 434, 516, 281, 764, 264, 2370, 3037, 295, 309, 321, 366, 516, 281, 360, 746, 300, 307, 51504], "temperature": 0.0, "avg_logprob": -0.10303047607684958, "compression_ratio": 1.753968253968254, "no_speech_prob": 0.0027553890831768513}, {"id": 369, "seek": 285488, "start": 2855.28, "end": 2865.04, "text": " um some people hate it we don't have a padding token for lama and this is a really big problem", "tokens": [50384, 1105, 512, 561, 4700, 309, 321, 500, 380, 362, 257, 39562, 14862, 337, 45423, 293, 341, 307, 257, 534, 955, 1154, 50872], "temperature": 0.0, "avg_logprob": -0.12462078315624292, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.0034808192867785692}, {"id": 370, "seek": 285488, "start": 2865.04, "end": 2874.88, "text": " because we we have a dataset with different number of tokens for each row so we need to", "tokens": [50872, 570, 321, 321, 362, 257, 28872, 365, 819, 1230, 295, 22667, 337, 1184, 5386, 370, 321, 643, 281, 51364], "temperature": 0.0, "avg_logprob": -0.12462078315624292, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.0034808192867785692}, {"id": 371, "seek": 285488, "start": 2874.88, "end": 2881.6, "text": " pad it so they all have the same length right and there are different ways of doing it here i'm using", "tokens": [51364, 6887, 309, 370, 436, 439, 362, 264, 912, 4641, 558, 293, 456, 366, 819, 2098, 295, 884, 309, 510, 741, 478, 1228, 51700], "temperature": 0.0, "avg_logprob": -0.12462078315624292, "compression_ratio": 1.6228571428571428, "no_speech_prob": 0.0034808192867785692}, {"id": 372, "seek": 288160, "start": 2881.6, "end": 2887.2, "text": " a token called the end of sentence token and this will have an impact on the generation of our model", "tokens": [50364, 257, 14862, 1219, 264, 917, 295, 8174, 14862, 293, 341, 486, 362, 364, 2712, 322, 264, 5125, 295, 527, 2316, 50644], "temperature": 0.0, "avg_logprob": -0.06130875193554422, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0034801571164280176}, {"id": 373, "seek": 288160, "start": 2889.04, "end": 2893.8399999999997, "text": " this is what we're going to use here there are different ways of doing it this is definitely", "tokens": [50736, 341, 307, 437, 321, 434, 516, 281, 764, 510, 456, 366, 819, 2098, 295, 884, 309, 341, 307, 2138, 50976], "temperature": 0.0, "avg_logprob": -0.06130875193554422, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0034801571164280176}, {"id": 374, "seek": 288160, "start": 2893.8399999999997, "end": 2900.88, "text": " not the best version of it if you want to learn more about it i linked an article from benjamin", "tokens": [50976, 406, 264, 1151, 3037, 295, 309, 498, 291, 528, 281, 1466, 544, 466, 309, 741, 9408, 364, 7222, 490, 3271, 21241, 51328], "temperature": 0.0, "avg_logprob": -0.06130875193554422, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0034801571164280176}, {"id": 375, "seek": 288160, "start": 2900.88, "end": 2907.52, "text": " mary about two other ways of doing it and this is what we should do but for the sake of simplicity", "tokens": [51328, 275, 822, 466, 732, 661, 2098, 295, 884, 309, 293, 341, 307, 437, 321, 820, 360, 457, 337, 264, 9717, 295, 25632, 51660], "temperature": 0.0, "avg_logprob": -0.06130875193554422, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.0034801571164280176}, {"id": 376, "seek": 290752, "start": 2907.52, "end": 2913.28, "text": " here i'm telling you about this problem but i'm still using the end of sentence token for this", "tokens": [50364, 510, 741, 478, 3585, 291, 466, 341, 1154, 457, 741, 478, 920, 1228, 264, 917, 295, 8174, 14862, 337, 341, 50652], "temperature": 0.0, "avg_logprob": -0.14920319451226127, "compression_ratio": 1.7125748502994012, "no_speech_prob": 0.0019863746128976345}, {"id": 377, "seek": 290752, "start": 2914.08, "end": 2925.36, "text": " fine-tuning then we are going to talk about the configuration of the culor so here bnb config", "tokens": [50692, 2489, 12, 83, 37726, 550, 321, 366, 516, 281, 751, 466, 264, 11694, 295, 264, 11021, 284, 370, 510, 272, 77, 65, 6662, 51256], "temperature": 0.0, "avg_logprob": -0.14920319451226127, "compression_ratio": 1.7125748502994012, "no_speech_prob": 0.0019863746128976345}, {"id": 378, "seek": 290752, "start": 2925.92, "end": 2932.4, "text": " it's the bits and byte configuration the first thing that we want to do here is to load the model", "tokens": [51284, 309, 311, 264, 9239, 293, 40846, 11694, 264, 700, 551, 300, 321, 528, 281, 360, 510, 307, 281, 3677, 264, 2316, 51608], "temperature": 0.0, "avg_logprob": -0.14920319451226127, "compression_ratio": 1.7125748502994012, "no_speech_prob": 0.0019863746128976345}, {"id": 379, "seek": 293240, "start": 2932.4, "end": 2941.52, "text": " using four bits otherwise it will not fit into the VRAM in order to do that we can specify the", "tokens": [50364, 1228, 1451, 9239, 5911, 309, 486, 406, 3318, 666, 264, 13722, 2865, 294, 1668, 281, 360, 300, 321, 393, 16500, 264, 50820], "temperature": 0.0, "avg_logprob": -0.14880865732828777, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.01638968288898468}, {"id": 380, "seek": 293240, "start": 2941.52, "end": 2947.76, "text": " quant type that we want to use in our case we want to use the nf4 format this is the format that was", "tokens": [50820, 4426, 2010, 300, 321, 528, 281, 764, 294, 527, 1389, 321, 528, 281, 764, 264, 297, 69, 19, 7877, 341, 307, 264, 7877, 300, 390, 51132], "temperature": 0.0, "avg_logprob": -0.14880865732828777, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.01638968288898468}, {"id": 381, "seek": 293240, "start": 2949.12, "end": 2961.36, "text": " introduced in the culor paper and we are going to use a compute type so this is how the weights", "tokens": [51200, 7268, 294, 264, 11021, 284, 3035, 293, 321, 366, 516, 281, 764, 257, 14722, 2010, 370, 341, 307, 577, 264, 17443, 51812], "temperature": 0.0, "avg_logprob": -0.14880865732828777, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.01638968288898468}, {"id": 382, "seek": 296136, "start": 2961.36, "end": 2967.2000000000003, "text": " are stored using four bits and when we want to compute it's only it's going to use 16 bits so we", "tokens": [50364, 366, 12187, 1228, 1451, 9239, 293, 562, 321, 528, 281, 14722, 309, 311, 787, 309, 311, 516, 281, 764, 3165, 9239, 370, 321, 50656], "temperature": 0.0, "avg_logprob": -0.07266466056599337, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.001753585529513657}, {"id": 383, "seek": 296136, "start": 2967.2000000000003, "end": 2977.92, "text": " have more accuracy and we are also going to use something called double quantization so even the", "tokens": [50656, 362, 544, 14170, 293, 321, 366, 611, 516, 281, 764, 746, 1219, 3834, 4426, 2144, 370, 754, 264, 51192], "temperature": 0.0, "avg_logprob": -0.07266466056599337, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.001753585529513657}, {"id": 384, "seek": 296136, "start": 2977.92, "end": 2986.8, "text": " quantization parameters are quantized it's it's to like really take even less space than without", "tokens": [51192, 4426, 2144, 9834, 366, 4426, 1602, 309, 311, 309, 311, 281, 411, 534, 747, 754, 1570, 1901, 813, 1553, 51636], "temperature": 0.0, "avg_logprob": -0.07266466056599337, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.001753585529513657}, {"id": 385, "seek": 298680, "start": 2986.88, "end": 2993.2000000000003, "text": " using it then we have the lower configuration so on top of culor we also have the lower", "tokens": [50368, 1228, 309, 550, 321, 362, 264, 3126, 11694, 370, 322, 1192, 295, 11021, 284, 321, 611, 362, 264, 3126, 50684], "temperature": 0.0, "avg_logprob": -0.06002494267054966, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.003647943027317524}, {"id": 386, "seek": 298680, "start": 2993.2000000000003, "end": 3000.48, "text": " configuration and in this case we have a bunch of diameters one of them is the alpha the alpha is", "tokens": [50684, 11694, 293, 294, 341, 1389, 321, 362, 257, 3840, 295, 7484, 6202, 472, 295, 552, 307, 264, 8961, 264, 8961, 307, 51048], "temperature": 0.0, "avg_logprob": -0.06002494267054966, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.003647943027317524}, {"id": 387, "seek": 298680, "start": 3000.48, "end": 3008.8, "text": " basically the strength of the adapter the impact it has on the the model because you can merge these", "tokens": [51048, 1936, 264, 3800, 295, 264, 22860, 264, 2712, 309, 575, 322, 264, 264, 2316, 570, 291, 393, 22183, 613, 51464], "temperature": 0.0, "avg_logprob": -0.06002494267054966, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.003647943027317524}, {"id": 388, "seek": 298680, "start": 3008.8, "end": 3016.0800000000004, "text": " adapters in a very weak way so using very little weight or using a big weight 32 is a pretty big", "tokens": [51464, 23169, 1559, 294, 257, 588, 5336, 636, 370, 1228, 588, 707, 3364, 420, 1228, 257, 955, 3364, 8858, 307, 257, 1238, 955, 51828], "temperature": 0.0, "avg_logprob": -0.06002494267054966, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.003647943027317524}, {"id": 389, "seek": 301608, "start": 3016.08, "end": 3022.64, "text": " weight but this is a quite standard value for this parameter we also have like the dropout because", "tokens": [50364, 3364, 457, 341, 307, 257, 1596, 3832, 2158, 337, 341, 13075, 321, 611, 362, 411, 264, 3270, 346, 570, 50692], "temperature": 0.0, "avg_logprob": -0.09975095813194018, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0013431544648483396}, {"id": 390, "seek": 301608, "start": 3022.64, "end": 3028.64, "text": " when we add these adapters they have a little dropout so we have a 5 probability of skipping", "tokens": [50692, 562, 321, 909, 613, 23169, 1559, 436, 362, 257, 707, 3270, 346, 370, 321, 362, 257, 1025, 8482, 295, 31533, 50992], "temperature": 0.0, "avg_logprob": -0.09975095813194018, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0013431544648483396}, {"id": 391, "seek": 301608, "start": 3028.64, "end": 3036.56, "text": " these connections and finally we have a rank which is like the dimension of the the matrix that I use", "tokens": [50992, 613, 9271, 293, 2721, 321, 362, 257, 6181, 597, 307, 411, 264, 10139, 295, 264, 264, 8141, 300, 286, 764, 51388], "temperature": 0.0, "avg_logprob": -0.09975095813194018, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0013431544648483396}, {"id": 392, "seek": 301608, "start": 3036.56, "end": 3045.12, "text": " here if you want to know more about law and like a minimalistic implementation of it I made this", "tokens": [51388, 510, 498, 291, 528, 281, 458, 544, 466, 2101, 293, 411, 257, 13206, 3142, 11420, 295, 309, 286, 1027, 341, 51816], "temperature": 0.0, "avg_logprob": -0.09975095813194018, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.0013431544648483396}, {"id": 393, "seek": 304512, "start": 3045.12, "end": 3053.12, "text": " notebook called nano law and this goes into in depth into like the theory behind it and we", "tokens": [50364, 21060, 1219, 30129, 2101, 293, 341, 1709, 666, 294, 7161, 666, 411, 264, 5261, 2261, 309, 293, 321, 50764], "temperature": 0.0, "avg_logprob": -0.0903329849243164, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0018381071276962757}, {"id": 394, "seek": 304512, "start": 3053.12, "end": 3059.44, "text": " will help you understand these parameters a bit better we do not want to take care of the bias so", "tokens": [50764, 486, 854, 291, 1223, 613, 9834, 257, 857, 1101, 321, 360, 406, 528, 281, 747, 1127, 295, 264, 12577, 370, 51080], "temperature": 0.0, "avg_logprob": -0.0903329849243164, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0018381071276962757}, {"id": 395, "seek": 304512, "start": 3059.44, "end": 3064.88, "text": " we have the weights and the biases here we do not care about the biases and the task type in this", "tokens": [51080, 321, 362, 264, 17443, 293, 264, 32152, 510, 321, 360, 406, 1127, 466, 264, 32152, 293, 264, 5633, 2010, 294, 341, 51352], "temperature": 0.0, "avg_logprob": -0.0903329849243164, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0018381071276962757}, {"id": 396, "seek": 304512, "start": 3064.88, "end": 3073.52, "text": " case is causal lm because we are auto aggressive and the target modules here we have a very long", "tokens": [51352, 1389, 307, 38755, 287, 76, 570, 321, 366, 8399, 10762, 293, 264, 3779, 16679, 510, 321, 362, 257, 588, 938, 51784], "temperature": 0.0, "avg_logprob": -0.0903329849243164, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0018381071276962757}, {"id": 397, "seek": 307352, "start": 3073.52, "end": 3080.72, "text": " list of target modules the target modules it's something that you can see here actually", "tokens": [50364, 1329, 295, 3779, 16679, 264, 3779, 16679, 309, 311, 746, 300, 291, 393, 536, 510, 767, 50724], "temperature": 0.0, "avg_logprob": -0.13073578476905823, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.004461673554033041}, {"id": 398, "seek": 307352, "start": 3080.72, "end": 3090.56, "text": " and the attention lma attention thing it's basically like okay what which which modules do you want", "tokens": [50724, 293, 264, 3202, 287, 1696, 3202, 551, 309, 311, 1936, 411, 1392, 437, 597, 597, 16679, 360, 291, 528, 51216], "temperature": 0.0, "avg_logprob": -0.13073578476905823, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.004461673554033041}, {"id": 399, "seek": 307352, "start": 3092.08, "end": 3103.04, "text": " to not train but add a law adapter to it and we are going to use a lot of them because", "tokens": [51292, 281, 406, 3847, 457, 909, 257, 2101, 22860, 281, 309, 293, 321, 366, 516, 281, 764, 257, 688, 295, 552, 570, 51840], "temperature": 0.0, "avg_logprob": -0.13073578476905823, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.004461673554033041}, {"id": 400, "seek": 310304, "start": 3103.04, "end": 3109.6, "text": " it's been shown to really improve performance so the more modules we have the more", "tokens": [50364, 309, 311, 668, 4898, 281, 534, 3470, 3389, 370, 264, 544, 16679, 321, 362, 264, 544, 50692], "temperature": 0.0, "avg_logprob": -0.07441797541148627, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.004392039496451616}, {"id": 401, "seek": 310304, "start": 3111.04, "end": 3118.24, "text": " parameters we are going to train but this is fine like we we can afford it with our limited", "tokens": [50764, 9834, 321, 366, 516, 281, 3847, 457, 341, 307, 2489, 411, 321, 321, 393, 6157, 309, 365, 527, 5567, 51124], "temperature": 0.0, "avg_logprob": -0.07441797541148627, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.004392039496451616}, {"id": 402, "seek": 310304, "start": 3118.24, "end": 3128.88, "text": " budget it's going to help us in the long run then we're going to load the model from pre-trained", "tokens": [51124, 4706, 309, 311, 516, 281, 854, 505, 294, 264, 938, 1190, 550, 321, 434, 516, 281, 3677, 264, 2316, 490, 659, 12, 17227, 2001, 51656], "temperature": 0.0, "avg_logprob": -0.07441797541148627, "compression_ratio": 1.6130952380952381, "no_speech_prob": 0.004392039496451616}, {"id": 403, "seek": 312888, "start": 3129.76, "end": 3140.48, "text": " and we're going to use the base model that we typed earlier the quantization config so we have", "tokens": [50408, 293, 321, 434, 516, 281, 764, 264, 3096, 2316, 300, 321, 33941, 3071, 264, 4426, 2144, 6662, 370, 321, 362, 50944], "temperature": 0.0, "avg_logprob": -0.13905980851915148, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.005907184444367886}, {"id": 404, "seek": 312888, "start": 3140.48, "end": 3146.7200000000003, "text": " the bnb config here this is what we're going to use and finally the device map so here it's", "tokens": [50944, 264, 272, 77, 65, 6662, 510, 341, 307, 437, 321, 434, 516, 281, 764, 293, 2721, 264, 4302, 4471, 370, 510, 309, 311, 51256], "temperature": 0.0, "avg_logprob": -0.13905980851915148, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.005907184444367886}, {"id": 405, "seek": 312888, "start": 3147.6800000000003, "end": 3156.0, "text": " we could also use auto but I'm going to use that it's going to automatically detect the device the", "tokens": [51304, 321, 727, 611, 764, 8399, 457, 286, 478, 516, 281, 764, 300, 309, 311, 516, 281, 6772, 5531, 264, 4302, 264, 51720], "temperature": 0.0, "avg_logprob": -0.13905980851915148, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.005907184444367886}, {"id": 406, "seek": 315600, "start": 3156.0, "end": 3160.96, "text": " hardware that you have so in our case we'll detect the gpu and make sure that we're using the gpu for", "tokens": [50364, 8837, 300, 291, 362, 370, 294, 527, 1389, 321, 603, 5531, 264, 290, 34859, 293, 652, 988, 300, 321, 434, 1228, 264, 290, 34859, 337, 50612], "temperature": 0.0, "avg_logprob": -0.10779883644797585, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.0016719299601390958}, {"id": 407, "seek": 315600, "start": 3160.96, "end": 3167.44, "text": " training otherwise it's not going to work and finally we are going to call a function called", "tokens": [50612, 3097, 5911, 309, 311, 406, 516, 281, 589, 293, 2721, 321, 366, 516, 281, 818, 257, 2445, 1219, 50936], "temperature": 0.0, "avg_logprob": -0.10779883644797585, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.0016719299601390958}, {"id": 408, "seek": 315600, "start": 3168.08, "end": 3177.12, "text": " model for kb training this will cast the layer norm in fp32 so in more precision it will make", "tokens": [50968, 2316, 337, 350, 65, 3097, 341, 486, 4193, 264, 4583, 2026, 294, 283, 79, 11440, 370, 294, 544, 18356, 309, 486, 652, 51420], "temperature": 0.0, "avg_logprob": -0.10779883644797585, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.0016719299601390958}, {"id": 409, "seek": 315600, "start": 3177.12, "end": 3185.52, "text": " the output embedding layer require grads and add the upcasting to the mlhead to fp32 so what it", "tokens": [51420, 264, 5598, 12240, 3584, 4583, 3651, 2771, 82, 293, 909, 264, 493, 48860, 281, 264, 23271, 1934, 281, 283, 79, 11440, 370, 437, 309, 51840], "temperature": 0.0, "avg_logprob": -0.10779883644797585, "compression_ratio": 1.786046511627907, "no_speech_prob": 0.0016719299601390958}, {"id": 410, "seek": 318552, "start": 3185.52, "end": 3191.7599999999998, "text": " means is that it's going to take some layers some modules and make sure that we are using them with", "tokens": [50364, 1355, 307, 300, 309, 311, 516, 281, 747, 512, 7914, 512, 16679, 293, 652, 988, 300, 321, 366, 1228, 552, 365, 50676], "temperature": 0.0, "avg_logprob": -0.054567913676417154, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.0010474020382389426}, {"id": 411, "seek": 318552, "start": 3191.7599999999998, "end": 3197.6, "text": " the highest precision possible because it's been showed to really improve the performance too so", "tokens": [50676, 264, 6343, 18356, 1944, 570, 309, 311, 668, 4712, 281, 534, 3470, 264, 3389, 886, 370, 50968], "temperature": 0.0, "avg_logprob": -0.054567913676417154, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.0010474020382389426}, {"id": 412, "seek": 318552, "start": 3197.6, "end": 3203.84, "text": " yeah there are some modules that we will not that do not really matter some of them matter", "tokens": [50968, 1338, 456, 366, 512, 16679, 300, 321, 486, 406, 300, 360, 406, 534, 1871, 512, 295, 552, 1871, 51280], "temperature": 0.0, "avg_logprob": -0.054567913676417154, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.0010474020382389426}, {"id": 413, "seek": 318552, "start": 3203.84, "end": 3211.52, "text": " quite a lot and we want to be quite proactive on that and in the end this will help us build", "tokens": [51280, 1596, 257, 688, 293, 321, 528, 281, 312, 1596, 28028, 322, 300, 293, 294, 264, 917, 341, 486, 854, 505, 1322, 51664], "temperature": 0.0, "avg_logprob": -0.054567913676417154, "compression_ratio": 1.8009478672985781, "no_speech_prob": 0.0010474020382389426}, {"id": 414, "seek": 321152, "start": 3211.52, "end": 3222.08, "text": " the best model possible so if I oops I forgot to execute that um and then if I forgot to execute", "tokens": [50364, 264, 1151, 2316, 1944, 370, 498, 286, 34166, 286, 5298, 281, 14483, 300, 1105, 293, 550, 498, 286, 5298, 281, 14483, 50892], "temperature": 0.0, "avg_logprob": -0.20276778084891184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.010684328153729439}, {"id": 415, "seek": 321152, "start": 3222.08, "end": 3228.56, "text": " that too oh just while you're running those in the bits um the data set sorry I just wanted to", "tokens": [50892, 300, 886, 1954, 445, 1339, 291, 434, 2614, 729, 294, 264, 9239, 1105, 264, 1412, 992, 2597, 286, 445, 1415, 281, 51216], "temperature": 0.0, "avg_logprob": -0.20276778084891184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.010684328153729439}, {"id": 416, "seek": 321152, "start": 3228.56, "end": 3233.12, "text": " refer to just like just because we're coming up to time um just we are going to over in a bit", "tokens": [51216, 2864, 281, 445, 411, 445, 570, 321, 434, 1348, 493, 281, 565, 1105, 445, 321, 366, 516, 281, 670, 294, 257, 857, 51444], "temperature": 0.0, "avg_logprob": -0.20276778084891184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.010684328153729439}, {"id": 417, "seek": 321152, "start": 3233.92, "end": 3237.36, "text": " before we get to before you all jump off for those of you who have to dash", "tokens": [51484, 949, 321, 483, 281, 949, 291, 439, 3012, 766, 337, 729, 295, 291, 567, 362, 281, 8240, 51656], "temperature": 0.0, "avg_logprob": -0.20276778084891184, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.010684328153729439}, {"id": 418, "seek": 323736, "start": 3237.36, "end": 3243.04, "text": " as I was going to say we've got three webinars coming up next week so on Tuesday we've got", "tokens": [50364, 382, 286, 390, 516, 281, 584, 321, 600, 658, 1045, 26065, 1348, 493, 958, 1243, 370, 322, 10017, 321, 600, 658, 50648], "temperature": 0.0, "avg_logprob": -0.13376693190815292, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.027646927163004875}, {"id": 419, "seek": 323736, "start": 3243.04, "end": 3249.52, "text": " an introduction to snowflake code along coming along uh on Wednesday we've got a session on", "tokens": [50648, 364, 9339, 281, 44124, 619, 3089, 2051, 1348, 2051, 2232, 322, 10579, 321, 600, 658, 257, 5481, 322, 50972], "temperature": 0.0, "avg_logprob": -0.13376693190815292, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.027646927163004875}, {"id": 420, "seek": 323736, "start": 3249.52, "end": 3254.56, "text": " using ai in robotics so if you're interested in agreeing on ai then that's uh something uh", "tokens": [50972, 1228, 9783, 294, 34145, 370, 498, 291, 434, 3102, 294, 36900, 322, 9783, 550, 300, 311, 2232, 746, 2232, 51224], "temperature": 0.0, "avg_logprob": -0.13376693190815292, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.027646927163004875}, {"id": 421, "seek": 323736, "start": 3254.56, "end": 3259.04, "text": " definitely worth attending and then on Thursday we've got a session for best practices on putting", "tokens": [51224, 2138, 3163, 15862, 293, 550, 322, 10383, 321, 600, 658, 257, 5481, 337, 1151, 7525, 322, 3372, 51448], "temperature": 0.0, "avg_logprob": -0.13376693190815292, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.027646927163004875}, {"id": 422, "seek": 323736, "start": 3259.04, "end": 3266.1600000000003, "text": " lmms into production so three great sessions please do go to datacamp.com slash webinars", "tokens": [51448, 287, 76, 2592, 666, 4265, 370, 1045, 869, 11081, 1767, 360, 352, 281, 1137, 326, 1215, 13, 1112, 17330, 26065, 51804], "temperature": 0.0, "avg_logprob": -0.13376693190815292, "compression_ratio": 1.8253968253968254, "no_speech_prob": 0.027646927163004875}, {"id": 423, "seek": 326616, "start": 3266.16, "end": 3270.7999999999997, "text": " sign up for those um we have got some great questions from the audience as well so I'm", "tokens": [50364, 1465, 493, 337, 729, 1105, 321, 362, 658, 512, 869, 1651, 490, 264, 4034, 382, 731, 370, 286, 478, 50596], "temperature": 0.0, "avg_logprob": -0.11656483288468986, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.004518186207860708}, {"id": 424, "seek": 326616, "start": 3270.7999999999997, "end": 3275.3599999999997, "text": " hoping to get to those afterwards if you do have to jump that fine uh please do catch up on the", "tokens": [50596, 7159, 281, 483, 281, 729, 10543, 498, 291, 360, 362, 281, 3012, 300, 2489, 2232, 1767, 360, 3745, 493, 322, 264, 50824], "temperature": 0.0, "avg_logprob": -0.11656483288468986, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.004518186207860708}, {"id": 425, "seek": 326616, "start": 3275.3599999999997, "end": 3280.3199999999997, "text": " recording for everyone else um I hope you're okay hanging around for a minute and with that I will", "tokens": [50824, 6613, 337, 1518, 1646, 1105, 286, 1454, 291, 434, 1392, 8345, 926, 337, 257, 3456, 293, 365, 300, 286, 486, 51072], "temperature": 0.0, "avg_logprob": -0.11656483288468986, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.004518186207860708}, {"id": 426, "seek": 326616, "start": 3280.3199999999997, "end": 3285.3599999999997, "text": " dash off and let you uh get back to it Maxine yes sorry about that we should be over in like 10", "tokens": [51072, 8240, 766, 293, 718, 291, 2232, 483, 646, 281, 309, 7402, 533, 2086, 2597, 466, 300, 321, 820, 312, 670, 294, 411, 1266, 51324], "temperature": 0.0, "avg_logprob": -0.11656483288468986, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.004518186207860708}, {"id": 427, "seek": 326616, "start": 3285.3599999999997, "end": 3293.68, "text": " minutes I underestimated the time uh it takes you to write the code um but um yeah here we have", "tokens": [51324, 2077, 286, 24612, 33008, 264, 565, 2232, 309, 2516, 291, 281, 2464, 264, 3089, 1105, 457, 1105, 1338, 510, 321, 362, 51740], "temperature": 0.0, "avg_logprob": -0.11656483288468986, "compression_ratio": 1.6832740213523132, "no_speech_prob": 0.004518186207860708}, {"id": 428, "seek": 329368, "start": 3294.24, "end": 3298.96, "text": " lower configuration loading the model preparing the model for training uh we are currently", "tokens": [50392, 3126, 11694, 15114, 264, 2316, 10075, 264, 2316, 337, 3097, 2232, 321, 366, 4362, 50628], "temperature": 0.0, "avg_logprob": -0.17095459134955154, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.005427249241620302}, {"id": 429, "seek": 329368, "start": 3300.24, "end": 3304.8799999999997, "text": " downloading the model here you can see the different modules in the lma attention", "tokens": [50692, 32529, 264, 2316, 510, 291, 393, 536, 264, 819, 16679, 294, 264, 287, 1696, 3202, 50924], "temperature": 0.0, "avg_logprob": -0.17095459134955154, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.005427249241620302}, {"id": 430, "seek": 329368, "start": 3306.08, "end": 3312.3999999999996, "text": " class and this those are the one that we target also in the lma mlp you can see the", "tokens": [50984, 1508, 293, 341, 729, 366, 264, 472, 300, 321, 3779, 611, 294, 264, 287, 1696, 23271, 79, 291, 393, 536, 264, 51300], "temperature": 0.0, "avg_logprob": -0.17095459134955154, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.005427249241620302}, {"id": 431, "seek": 329368, "start": 3312.3999999999996, "end": 3318.08, "text": " hugging face implementation of it to to have more details about how it's actually implemented", "tokens": [51300, 41706, 1851, 11420, 295, 309, 281, 281, 362, 544, 4365, 466, 577, 309, 311, 767, 12270, 51584], "temperature": 0.0, "avg_logprob": -0.17095459134955154, "compression_ratio": 1.8229166666666667, "no_speech_prob": 0.005427249241620302}, {"id": 432, "seek": 331808, "start": 3318.88, "end": 3326.24, "text": " um once it's done we have more uh boilerplate code uh to to type uh this one it's training", "tokens": [50404, 1105, 1564, 309, 311, 1096, 321, 362, 544, 2232, 39228, 37008, 3089, 2232, 281, 281, 2010, 2232, 341, 472, 309, 311, 3097, 50772], "temperature": 0.0, "avg_logprob": -0.14503884651291538, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.0029773712158203125}, {"id": 433, "seek": 331808, "start": 3326.24, "end": 3333.6, "text": " arguments uh so we have the training arguments here and what we want to do is to give like a bunch of", "tokens": [50772, 12869, 2232, 370, 321, 362, 264, 3097, 12869, 510, 293, 437, 321, 528, 281, 360, 307, 281, 976, 411, 257, 3840, 295, 51140], "temperature": 0.0, "avg_logprob": -0.14503884651291538, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.0029773712158203125}, {"id": 434, "seek": 331808, "start": 3336.3199999999997, "end": 3343.12, "text": " parameters to it so like where do I output the results we're going to specify uh directory for", "tokens": [51276, 9834, 281, 309, 370, 411, 689, 360, 286, 5598, 264, 3542, 321, 434, 516, 281, 16500, 2232, 21120, 337, 51616], "temperature": 0.0, "avg_logprob": -0.14503884651291538, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.0029773712158203125}, {"id": 435, "seek": 334312, "start": 3343.12, "end": 3350.64, "text": " that uh how many epochs we want to run the model on um here I'm going to put one but let's put four", "tokens": [50364, 300, 2232, 577, 867, 30992, 28346, 321, 528, 281, 1190, 264, 2316, 322, 1105, 510, 286, 478, 516, 281, 829, 472, 457, 718, 311, 829, 1451, 50740], "temperature": 0.0, "avg_logprob": -0.10621617862156459, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.001621578587219119}, {"id": 436, "seek": 334312, "start": 3350.64, "end": 3357.04, "text": " or five uh basically between three and five it is pretty good for an amateur model at this size", "tokens": [50740, 420, 1732, 2232, 1936, 1296, 1045, 293, 1732, 309, 307, 1238, 665, 337, 364, 29339, 2316, 412, 341, 2744, 51060], "temperature": 0.0, "avg_logprob": -0.10621617862156459, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.001621578587219119}, {"id": 437, "seek": 334312, "start": 3358.0, "end": 3364.24, "text": " there is the per device uh train batch size uh this will tell us like how many", "tokens": [51108, 456, 307, 264, 680, 4302, 2232, 3847, 15245, 2744, 2232, 341, 486, 980, 505, 411, 577, 867, 51420], "temperature": 0.0, "avg_logprob": -0.10621617862156459, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.001621578587219119}, {"id": 438, "seek": 336424, "start": 3364.64, "end": 3365.8399999999997, "text": " um", "tokens": [50384, 1105, 50444], "temperature": 0.0, "avg_logprob": -0.17993934840372164, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.0027130835223942995}, {"id": 439, "seek": 336424, "start": 3368.16, "end": 3373.68, "text": " yeah the number of batches that we're going to take for every every every step um", "tokens": [50560, 1338, 264, 1230, 295, 15245, 279, 300, 321, 434, 516, 281, 747, 337, 633, 633, 633, 1823, 1105, 50836], "temperature": 0.0, "avg_logprob": -0.17993934840372164, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.0027130835223942995}, {"id": 440, "seek": 336424, "start": 3374.3999999999996, "end": 3379.6, "text": " we have the gradient accumulation uh steps we're not going to use it here it's basically a", "tokens": [50872, 321, 362, 264, 16235, 35647, 2232, 4439, 321, 434, 406, 516, 281, 764, 309, 510, 309, 311, 1936, 257, 51132], "temperature": 0.0, "avg_logprob": -0.17993934840372164, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.0027130835223942995}, {"id": 441, "seek": 336424, "start": 3379.6, "end": 3388.24, "text": " for loop inside of the um training uh so we don't have to add more um use more VRAM but in this case", "tokens": [51132, 337, 6367, 1854, 295, 264, 1105, 3097, 2232, 370, 321, 500, 380, 362, 281, 909, 544, 1105, 764, 544, 13722, 2865, 457, 294, 341, 1389, 51564], "temperature": 0.0, "avg_logprob": -0.17993934840372164, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.0027130835223942995}, {"id": 442, "seek": 338824, "start": 3388.3199999999997, "end": 3394.3199999999997, "text": " it's going to be fine we won't need it we have the evaluation strategy it's not going to be very", "tokens": [50368, 309, 311, 516, 281, 312, 2489, 321, 1582, 380, 643, 309, 321, 362, 264, 13344, 5206, 309, 311, 406, 516, 281, 312, 588, 50668], "temperature": 0.0, "avg_logprob": -0.08853450351291232, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.01261849980801344}, {"id": 443, "seek": 338824, "start": 3394.3199999999997, "end": 3400.9599999999996, "text": " useful here because we are not going to um evaluate this model um we just want to train it", "tokens": [50668, 4420, 510, 570, 321, 366, 406, 516, 281, 1105, 13059, 341, 2316, 1105, 321, 445, 528, 281, 3847, 309, 51000], "temperature": 0.0, "avg_logprob": -0.08853450351291232, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.01261849980801344}, {"id": 444, "seek": 338824, "start": 3401.6, "end": 3406.9599999999996, "text": " and we're going to mention what evaluation looks like with uh uh these models a bit later", "tokens": [51032, 293, 321, 434, 516, 281, 2152, 437, 13344, 1542, 411, 365, 2232, 2232, 613, 5245, 257, 857, 1780, 51300], "temperature": 0.0, "avg_logprob": -0.08853450351291232, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.01261849980801344}, {"id": 445, "seek": 338824, "start": 3407.8399999999997, "end": 3413.6, "text": " logging steps we want to log every step uh the optimizer that we're going to use is the", "tokens": [51344, 27991, 4439, 321, 528, 281, 3565, 633, 1823, 2232, 264, 5028, 6545, 300, 321, 434, 516, 281, 764, 307, 264, 51632], "temperature": 0.0, "avg_logprob": -0.08853450351291232, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.01261849980801344}, {"id": 446, "seek": 341360, "start": 3414.3199999999997, "end": 3420.72, "text": " adam optimizer uh but a version that is paged in eight bits so it's going to lose", "tokens": [50400, 16368, 5028, 6545, 2232, 457, 257, 3037, 300, 307, 280, 2980, 294, 3180, 9239, 370, 309, 311, 516, 281, 3624, 50720], "temperature": 0.0, "avg_logprob": -0.14908308206602586, "compression_ratio": 1.907103825136612, "no_speech_prob": 0.0006360356346704066}, {"id": 447, "seek": 341360, "start": 3420.72, "end": 3428.24, "text": " test memory uh the learning rate uh we are going to use uh this one there are different", "tokens": [50720, 1500, 4675, 2232, 264, 2539, 3314, 2232, 321, 366, 516, 281, 764, 2232, 341, 472, 456, 366, 819, 51096], "temperature": 0.0, "avg_logprob": -0.14908308206602586, "compression_ratio": 1.907103825136612, "no_speech_prob": 0.0006360356346704066}, {"id": 448, "seek": 341360, "start": 3428.24, "end": 3435.2, "text": " learning rates that that what we can use um refer to the qloa because qloa really impacts", "tokens": [51096, 2539, 6846, 300, 300, 437, 321, 393, 764, 1105, 2864, 281, 264, 9505, 752, 64, 570, 9505, 752, 64, 534, 11606, 51444], "temperature": 0.0, "avg_logprob": -0.14908308206602586, "compression_ratio": 1.907103825136612, "no_speech_prob": 0.0006360356346704066}, {"id": 449, "seek": 341360, "start": 3435.2, "end": 3439.7599999999998, "text": " the learning rate the the model also really impact the learning rate that you want to use", "tokens": [51444, 264, 2539, 3314, 264, 264, 2316, 611, 534, 2712, 264, 2539, 3314, 300, 291, 528, 281, 764, 51672], "temperature": 0.0, "avg_logprob": -0.14908308206602586, "compression_ratio": 1.907103825136612, "no_speech_prob": 0.0006360356346704066}, {"id": 450, "seek": 343976, "start": 3440.48, "end": 3447.28, "text": " we have the scheduler uh in this case we're going to use linear and warm up steps", "tokens": [50400, 321, 362, 264, 12000, 260, 2232, 294, 341, 1389, 321, 434, 516, 281, 764, 8213, 293, 4561, 493, 4439, 50740], "temperature": 0.0, "avg_logprob": -0.12221949441092354, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.004979317542165518}, {"id": 451, "seek": 343976, "start": 3449.44, "end": 3456.96, "text": " like it won't be really useful here but we can say uh 10 uh to warm up the the optimizer", "tokens": [50848, 411, 309, 1582, 380, 312, 534, 4420, 510, 457, 321, 393, 584, 2232, 1266, 2232, 281, 4561, 493, 264, 264, 5028, 6545, 51224], "temperature": 0.0, "avg_logprob": -0.12221949441092354, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.004979317542165518}, {"id": 452, "seek": 343976, "start": 3456.96, "end": 3463.84, "text": " we want to report it to weights and biases and finally uh something that i'm going to put here but", "tokens": [51224, 321, 528, 281, 2275, 309, 281, 17443, 293, 32152, 293, 2721, 2232, 746, 300, 741, 478, 516, 281, 829, 510, 457, 51568], "temperature": 0.0, "avg_logprob": -0.12221949441092354, "compression_ratio": 1.5730994152046784, "no_speech_prob": 0.004979317542165518}, {"id": 453, "seek": 346384, "start": 3464.32, "end": 3474.7200000000003, "text": " uh remove this line uh for like for real training we're just going to stop after two steps otherwise", "tokens": [50388, 2232, 4159, 341, 1622, 2232, 337, 411, 337, 957, 3097, 321, 434, 445, 516, 281, 1590, 934, 732, 4439, 5911, 50908], "temperature": 0.0, "avg_logprob": -0.11898350185818142, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.002510620513930917}, {"id": 454, "seek": 346384, "start": 3474.7200000000003, "end": 3479.52, "text": " it will take like an hour to train the model but yeah you can just feel free to remove this step", "tokens": [50908, 309, 486, 747, 411, 364, 1773, 281, 3847, 264, 2316, 457, 1338, 291, 393, 445, 841, 1737, 281, 4159, 341, 1823, 51148], "temperature": 0.0, "avg_logprob": -0.11898350185818142, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.002510620513930917}, {"id": 455, "seek": 346384, "start": 3479.52, "end": 3485.52, "text": " if you want a real training so those are the training arguments uh then we need also to use", "tokens": [51148, 498, 291, 528, 257, 957, 3097, 370, 729, 366, 264, 3097, 12869, 2232, 550, 321, 643, 611, 281, 764, 51448], "temperature": 0.0, "avg_logprob": -0.11898350185818142, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.002510620513930917}, {"id": 456, "seek": 346384, "start": 3485.52, "end": 3491.84, "text": " the fft trainer so the wrap i mentioned earlier and in this case we just specify the model the", "tokens": [51448, 264, 283, 844, 21110, 370, 264, 7019, 741, 2835, 3071, 293, 294, 341, 1389, 321, 445, 16500, 264, 2316, 264, 51764], "temperature": 0.0, "avg_logprob": -0.11898350185818142, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.002510620513930917}, {"id": 457, "seek": 349184, "start": 3491.84, "end": 3497.6800000000003, "text": " training data set uh we don't have an eval data set so i'm just going to reuse the same one", "tokens": [50364, 3097, 1412, 992, 2232, 321, 500, 380, 362, 364, 1073, 304, 1412, 992, 370, 741, 478, 445, 516, 281, 26225, 264, 912, 472, 50656], "temperature": 0.0, "avg_logprob": -0.11965075661154355, "compression_ratio": 1.6832298136645962, "no_speech_prob": 0.0011506666196510196}, {"id": 458, "seek": 349184, "start": 3498.2400000000002, "end": 3508.6400000000003, "text": " pft config um we specified it um it wants to know the text field here so the instruction field", "tokens": [50684, 280, 844, 6662, 1105, 321, 22206, 309, 1105, 309, 2738, 281, 458, 264, 2487, 2519, 510, 370, 264, 10951, 2519, 51204], "temperature": 0.0, "avg_logprob": -0.11965075661154355, "compression_ratio": 1.6832298136645962, "no_speech_prob": 0.0011506666196510196}, {"id": 459, "seek": 349184, "start": 3508.6400000000003, "end": 3516.6400000000003, "text": " in our data set in our data set was called instruction um the max sequence length um", "tokens": [51204, 294, 527, 1412, 992, 294, 527, 1412, 992, 390, 1219, 10951, 1105, 264, 11469, 8310, 4641, 1105, 51604], "temperature": 0.0, "avg_logprob": -0.11965075661154355, "compression_ratio": 1.6832298136645962, "no_speech_prob": 0.0011506666196510196}, {"id": 460, "seek": 351664, "start": 3517.44, "end": 3525.52, "text": " so we're gonna go for uh 512 uh you could say like yeah but we we put a threshold at 2k but", "tokens": [50404, 370, 321, 434, 799, 352, 337, 2232, 1025, 4762, 2232, 291, 727, 584, 411, 1338, 457, 321, 321, 829, 257, 14678, 412, 568, 74, 457, 50808], "temperature": 0.0, "avg_logprob": -0.14472380199947874, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.00035137240774929523}, {"id": 461, "seek": 351664, "start": 3525.52, "end": 3532.08, "text": " we don't have enough VRAM unfortunately uh it it will take like a lot of VRAM to to", "tokens": [50808, 321, 500, 380, 362, 1547, 13722, 2865, 7015, 2232, 309, 309, 486, 747, 411, 257, 688, 295, 13722, 2865, 281, 281, 51136], "temperature": 0.0, "avg_logprob": -0.14472380199947874, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.00035137240774929523}, {"id": 462, "seek": 351664, "start": 3532.08, "end": 3539.6, "text": " to put everything into memory so we we're just gonna stop at uh 500 in this example and finally", "tokens": [51136, 281, 829, 1203, 666, 4675, 370, 321, 321, 434, 445, 799, 1590, 412, 2232, 5923, 294, 341, 1365, 293, 2721, 51512], "temperature": 0.0, "avg_logprob": -0.14472380199947874, "compression_ratio": 1.4728260869565217, "no_speech_prob": 0.00035137240774929523}, {"id": 463, "seek": 353960, "start": 3539.6, "end": 3547.12, "text": " we're going to give it the training arguments so this is what we have and when it's done we can", "tokens": [50364, 321, 434, 516, 281, 976, 309, 264, 3097, 12869, 370, 341, 307, 437, 321, 362, 293, 562, 309, 311, 1096, 321, 393, 50740], "temperature": 0.0, "avg_logprob": -0.0749374717029173, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.0031234705820679665}, {"id": 464, "seek": 353960, "start": 3547.12, "end": 3555.2, "text": " start the training and when the training is done uh we can even save the model uh using that", "tokens": [50740, 722, 264, 3097, 293, 562, 264, 3097, 307, 1096, 2232, 321, 393, 754, 3155, 264, 2316, 2232, 1228, 300, 51144], "temperature": 0.0, "avg_logprob": -0.0749374717029173, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.0031234705820679665}, {"id": 465, "seek": 353960, "start": 3557.12, "end": 3563.7599999999998, "text": " so we should have yeah the model has been downloaded here and now we are training the model", "tokens": [51240, 370, 321, 820, 362, 1338, 264, 2316, 575, 668, 21748, 510, 293, 586, 321, 366, 3097, 264, 2316, 51572], "temperature": 0.0, "avg_logprob": -0.0749374717029173, "compression_ratio": 1.8791946308724832, "no_speech_prob": 0.0031234705820679665}, {"id": 466, "seek": 356376, "start": 3564.5600000000004, "end": 3571.1200000000003, "text": " so this is a loss a training loss and evaluation loss from weights and biases and as you can see", "tokens": [50404, 370, 341, 307, 257, 4470, 257, 3097, 4470, 293, 13344, 4470, 490, 17443, 293, 32152, 293, 382, 291, 393, 536, 50732], "temperature": 0.0, "avg_logprob": -0.09285132398883116, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0021148209925740957}, {"id": 467, "seek": 356376, "start": 3571.1200000000003, "end": 3576.4, "text": " it's a very nice way to tracking the the progress of the model we can see here the warm-up steps", "tokens": [50732, 309, 311, 257, 588, 1481, 636, 281, 11603, 264, 264, 4205, 295, 264, 2316, 321, 393, 536, 510, 264, 4561, 12, 1010, 4439, 50996], "temperature": 0.0, "avg_logprob": -0.09285132398883116, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0021148209925740957}, {"id": 468, "seek": 356376, "start": 3576.4, "end": 3583.36, "text": " where it's it's pretty bad and then it goes better and better uh you can see the training uh loss is", "tokens": [50996, 689, 309, 311, 309, 311, 1238, 1578, 293, 550, 309, 1709, 1101, 293, 1101, 2232, 291, 393, 536, 264, 3097, 2232, 4470, 307, 51344], "temperature": 0.0, "avg_logprob": -0.09285132398883116, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0021148209925740957}, {"id": 469, "seek": 356376, "start": 3583.36, "end": 3590.0, "text": " in blue so it's quite spiky it's a bit noisy the eval loss is in orange it's a lot less noisy", "tokens": [51344, 294, 3344, 370, 309, 311, 1596, 637, 1035, 88, 309, 311, 257, 857, 24518, 264, 1073, 304, 4470, 307, 294, 7671, 309, 311, 257, 688, 1570, 24518, 51676], "temperature": 0.0, "avg_logprob": -0.09285132398883116, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0021148209925740957}, {"id": 470, "seek": 359000, "start": 3590.0, "end": 3596.8, "text": " because it's less frequent and something that you can observe uh if you train it for like five epochs", "tokens": [50364, 570, 309, 311, 1570, 18004, 293, 746, 300, 291, 393, 11441, 2232, 498, 291, 3847, 309, 337, 411, 1732, 30992, 28346, 50704], "temperature": 0.0, "avg_logprob": -0.08693018489413791, "compression_ratio": 1.752212389380531, "no_speech_prob": 0.0009840946877375245}, {"id": 471, "seek": 359000, "start": 3596.8, "end": 3604.08, "text": " is that the eval loss will go up instead of down uh normally uh traditionally in machine learning", "tokens": [50704, 307, 300, 264, 1073, 304, 4470, 486, 352, 493, 2602, 295, 760, 2232, 5646, 2232, 19067, 294, 3479, 2539, 51068], "temperature": 0.0, "avg_logprob": -0.08693018489413791, "compression_ratio": 1.752212389380531, "no_speech_prob": 0.0009840946877375245}, {"id": 472, "seek": 359000, "start": 3604.08, "end": 3609.36, "text": " this is a bad thing but with large average model it's been proven uh time and time again that it's", "tokens": [51068, 341, 307, 257, 1578, 551, 457, 365, 2416, 4274, 2316, 309, 311, 668, 12785, 2232, 565, 293, 565, 797, 300, 309, 311, 51332], "temperature": 0.0, "avg_logprob": -0.08693018489413791, "compression_ratio": 1.752212389380531, "no_speech_prob": 0.0009840946877375245}, {"id": 473, "seek": 359000, "start": 3609.36, "end": 3616.32, "text": " actually desirable and the best models um actually like overfit really a lot on the training data", "tokens": [51332, 767, 30533, 293, 264, 1151, 5245, 1105, 767, 411, 670, 6845, 534, 257, 688, 322, 264, 3097, 1412, 51680], "temperature": 0.0, "avg_logprob": -0.08693018489413791, "compression_ratio": 1.752212389380531, "no_speech_prob": 0.0009840946877375245}, {"id": 474, "seek": 361632, "start": 3616.32, "end": 3622.7200000000003, "text": " and this is not a problem actually this makes them better um here as you can see your model", "tokens": [50364, 293, 341, 307, 406, 257, 1154, 767, 341, 1669, 552, 1101, 1105, 510, 382, 291, 393, 536, 428, 2316, 50684], "temperature": 0.0, "avg_logprob": -0.0841220219930013, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00047259623534046113}, {"id": 475, "seek": 361632, "start": 3622.7200000000003, "end": 3628.1600000000003, "text": " has already been trained for two steps so if you want you can add more steps you can remove it if", "tokens": [50684, 575, 1217, 668, 8895, 337, 732, 4439, 370, 498, 291, 528, 291, 393, 909, 544, 4439, 291, 393, 4159, 309, 498, 50956], "temperature": 0.0, "avg_logprob": -0.0841220219930013, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00047259623534046113}, {"id": 476, "seek": 361632, "start": 3628.1600000000003, "end": 3633.52, "text": " you want to train it on the entire data set it will take a while however uh we can check um", "tokens": [50956, 291, 528, 281, 3847, 309, 322, 264, 2302, 1412, 992, 309, 486, 747, 257, 1339, 4461, 2232, 321, 393, 1520, 1105, 51224], "temperature": 0.0, "avg_logprob": -0.0841220219930013, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00047259623534046113}, {"id": 477, "seek": 361632, "start": 3633.52, "end": 3639.6000000000004, "text": " weights and biases here we won't see any the lot because we only have two steps uh but just to show", "tokens": [51224, 17443, 293, 32152, 510, 321, 1582, 380, 536, 604, 264, 688, 570, 321, 787, 362, 732, 4439, 2232, 457, 445, 281, 855, 51528], "temperature": 0.0, "avg_logprob": -0.0841220219930013, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.00047259623534046113}, {"id": 478, "seek": 363960, "start": 3639.6, "end": 3647.7599999999998, "text": " you uh here's our run and you can see here the global step uh train run time train loss of 1.2", "tokens": [50364, 291, 2232, 510, 311, 527, 1190, 293, 291, 393, 536, 510, 264, 4338, 1823, 2232, 3847, 1190, 565, 3847, 4470, 295, 502, 13, 17, 50772], "temperature": 0.0, "avg_logprob": -0.10333207580778334, "compression_ratio": 1.603658536585366, "no_speech_prob": 0.0043951296247541904}, {"id": 479, "seek": 363960, "start": 3648.96, "end": 3652.88, "text": " so yeah this is what you would use if you run it on the entire data set", "tokens": [50832, 370, 1338, 341, 307, 437, 291, 576, 764, 498, 291, 1190, 309, 322, 264, 2302, 1412, 992, 51028], "temperature": 0.0, "avg_logprob": -0.10333207580778334, "compression_ratio": 1.603658536585366, "no_speech_prob": 0.0043951296247541904}, {"id": 480, "seek": 363960, "start": 3654.64, "end": 3663.2799999999997, "text": " finally we can um use our model now that it's been it's been trained uh we can prompt it and say", "tokens": [51116, 2721, 321, 393, 1105, 764, 527, 2316, 586, 300, 309, 311, 668, 309, 311, 668, 8895, 2232, 321, 393, 12391, 309, 293, 584, 51548], "temperature": 0.0, "avg_logprob": -0.10333207580778334, "compression_ratio": 1.603658536585366, "no_speech_prob": 0.0043951296247541904}, {"id": 481, "seek": 366328, "start": 3664.0, "end": 3674.5600000000004, "text": " what is a large language model uh we have to wrap it using the right uh chat template so with", "tokens": [50400, 437, 307, 257, 2416, 2856, 2316, 2232, 321, 362, 281, 7019, 309, 1228, 264, 558, 2232, 5081, 12379, 370, 365, 50928], "temperature": 0.0, "avg_logprob": -0.11949493487675984, "compression_ratio": 1.2077922077922079, "no_speech_prob": 0.0073348223231732845}, {"id": 482, "seek": 367456, "start": 3674.56, "end": 3684.24, "text": " instruction prompt and and then response", "tokens": [50364, 10951, 12391, 293, 293, 550, 4134, 50848], "temperature": 0.0, "avg_logprob": -0.1612521084872159, "compression_ratio": 1.32, "no_speech_prob": 0.0088444072753191}, {"id": 483, "seek": 367456, "start": 3688.48, "end": 3698.48, "text": " and we can use a a pipeline here uh from hugging face it's going to be pretty nice so model", "tokens": [51060, 293, 321, 393, 764, 257, 257, 15517, 510, 2232, 490, 41706, 1851, 309, 311, 516, 281, 312, 1238, 1481, 370, 2316, 51560], "temperature": 0.0, "avg_logprob": -0.1612521084872159, "compression_ratio": 1.32, "no_speech_prob": 0.0088444072753191}, {"id": 484, "seek": 369848, "start": 3699.12, "end": 3709.12, "text": " tokenizer equal to tokenizer and we're going to restrict the generation in 128 and finally", "tokens": [50396, 14862, 6545, 2681, 281, 14862, 6545, 293, 321, 434, 516, 281, 7694, 264, 5125, 294, 29810, 293, 2721, 50896], "temperature": 0.0, "avg_logprob": -0.0974743077249238, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.010005556046962738}, {"id": 485, "seek": 369848, "start": 3709.12, "end": 3719.28, "text": " we can get the result here and print it so i'm going to print the generated text it's an object", "tokens": [50896, 321, 393, 483, 264, 1874, 510, 293, 4482, 309, 370, 741, 478, 516, 281, 4482, 264, 10833, 2487, 309, 311, 364, 2657, 51404], "temperature": 0.0, "avg_logprob": -0.0974743077249238, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.010005556046962738}, {"id": 486, "seek": 369848, "start": 3719.28, "end": 3727.44, "text": " that returns um and we can do something fancy and remove the instruction part this is a question", "tokens": [51404, 300, 11247, 1105, 293, 321, 393, 360, 746, 10247, 293, 4159, 264, 10951, 644, 341, 307, 257, 1168, 51812], "temperature": 0.0, "avg_logprob": -0.0974743077249238, "compression_ratio": 1.6453488372093024, "no_speech_prob": 0.010005556046962738}, {"id": 487, "seek": 372744, "start": 3727.44, "end": 3733.44, "text": " that a lot of people ask me like why do i see the instruction uh in the generation in the generated", "tokens": [50364, 300, 257, 688, 295, 561, 1029, 385, 411, 983, 360, 741, 536, 264, 10951, 2232, 294, 264, 5125, 294, 264, 10833, 50664], "temperature": 0.0, "avg_logprob": -0.06513707021648964, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0038208081386983395}, {"id": 488, "seek": 372744, "start": 3733.44, "end": 3740.88, "text": " text you can just trim it basically this is how the hugging face object works but you can remove", "tokens": [50664, 2487, 291, 393, 445, 10445, 309, 1936, 341, 307, 577, 264, 41706, 1851, 2657, 1985, 457, 291, 393, 4159, 51036], "temperature": 0.0, "avg_logprob": -0.06513707021648964, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0038208081386983395}, {"id": 489, "seek": 372744, "start": 3740.88, "end": 3747.2000000000003, "text": " it manually like this so now the train model is going to answer the question what is a large", "tokens": [51036, 309, 16945, 411, 341, 370, 586, 264, 3847, 2316, 307, 516, 281, 1867, 264, 1168, 437, 307, 257, 2416, 51352], "temperature": 0.0, "avg_logprob": -0.06513707021648964, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0038208081386983395}, {"id": 490, "seek": 372744, "start": 3747.2000000000003, "end": 3756.96, "text": " language model and it will print um the answer here then we want to delete it okay we have the", "tokens": [51352, 2856, 2316, 293, 309, 486, 4482, 1105, 264, 1867, 510, 550, 321, 528, 281, 12097, 309, 1392, 321, 362, 264, 51840], "temperature": 0.0, "avg_logprob": -0.06513707021648964, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0038208081386983395}, {"id": 491, "seek": 375696, "start": 3757.04, "end": 3763.28, "text": " answer so what is a neural network no the answer is a large language model is a type of artificial", "tokens": [50368, 1867, 370, 437, 307, 257, 18161, 3209, 572, 264, 1867, 307, 257, 2416, 2856, 2316, 307, 257, 2010, 295, 11677, 50680], "temperature": 0.0, "avg_logprob": -0.08239579668232039, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.0020480770617723465}, {"id": 492, "seek": 375696, "start": 3763.28, "end": 3768.16, "text": " intelligence model that is trained large amount of text data to generate human like text which is", "tokens": [50680, 7599, 2316, 300, 307, 8895, 2416, 2372, 295, 2487, 1412, 281, 8460, 1952, 411, 2487, 597, 307, 50924], "temperature": 0.0, "avg_logprob": -0.08239579668232039, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.0020480770617723465}, {"id": 493, "seek": 375696, "start": 3768.16, "end": 3773.04, "text": " pretty good actually um not thanks to our fine tuning because it was not intense enough but", "tokens": [50924, 1238, 665, 767, 1105, 406, 3231, 281, 527, 2489, 15164, 570, 309, 390, 406, 9447, 1547, 457, 51168], "temperature": 0.0, "avg_logprob": -0.08239579668232039, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.0020480770617723465}, {"id": 494, "seek": 375696, "start": 3773.04, "end": 3777.84, "text": " it's a pretty good answer and then you can see that it keeps repeating like instruction response", "tokens": [51168, 309, 311, 257, 1238, 665, 1867, 293, 550, 291, 393, 536, 300, 309, 5965, 18617, 411, 10951, 4134, 51408], "temperature": 0.0, "avg_logprob": -0.08239579668232039, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.0020480770617723465}, {"id": 495, "seek": 375696, "start": 3777.84, "end": 3782.64, "text": " instruction and this is because of our padding actually it's because we're using the end of", "tokens": [51408, 10951, 293, 341, 307, 570, 295, 527, 39562, 767, 309, 311, 570, 321, 434, 1228, 264, 917, 295, 51648], "temperature": 0.0, "avg_logprob": -0.08239579668232039, "compression_ratio": 1.8416988416988418, "no_speech_prob": 0.0020480770617723465}, {"id": 496, "seek": 378264, "start": 3782.64, "end": 3792.08, "text": " sentence token as a padding token so now it just doesn't stop and keeps talking so if you", "tokens": [50364, 8174, 14862, 382, 257, 39562, 14862, 370, 586, 309, 445, 1177, 380, 1590, 293, 5965, 1417, 370, 498, 291, 50836], "temperature": 0.0, "avg_logprob": -0.10973336619715537, "compression_ratio": 1.6011560693641618, "no_speech_prob": 0.0018368014134466648}, {"id": 497, "seek": 378264, "start": 3792.08, "end": 3797.12, "text": " don't want to have this behavior please use a different padding technique as mentioned previously", "tokens": [50836, 500, 380, 528, 281, 362, 341, 5223, 1767, 764, 257, 819, 39562, 6532, 382, 2835, 8046, 51088], "temperature": 0.0, "avg_logprob": -0.10973336619715537, "compression_ratio": 1.6011560693641618, "no_speech_prob": 0.0018368014134466648}, {"id": 498, "seek": 378264, "start": 3798.96, "end": 3807.04, "text": " finally we want to remove a lot of things so this is specific to google collab we want to", "tokens": [51180, 2721, 321, 528, 281, 4159, 257, 688, 295, 721, 370, 341, 307, 2685, 281, 20742, 44228, 321, 528, 281, 51584], "temperature": 0.0, "avg_logprob": -0.10973336619715537, "compression_ratio": 1.6011560693641618, "no_speech_prob": 0.0018368014134466648}, {"id": 499, "seek": 380704, "start": 3807.84, "end": 3814.96, "text": " collect all the model and the objects in the in the memory in the VRAM so we can merge it with the", "tokens": [50404, 2500, 439, 264, 2316, 293, 264, 6565, 294, 264, 294, 264, 4675, 294, 264, 13722, 2865, 370, 321, 393, 22183, 309, 365, 264, 50760], "temperature": 0.0, "avg_logprob": -0.10859245796726175, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.005135600920766592}, {"id": 500, "seek": 380704, "start": 3815.92, "end": 3823.36, "text": " so we can merge the base model with the adapter that we trained this is a piece of code that is", "tokens": [50808, 370, 321, 393, 22183, 264, 3096, 2316, 365, 264, 22860, 300, 321, 8895, 341, 307, 257, 2522, 295, 3089, 300, 307, 51180], "temperature": 0.0, "avg_logprob": -0.10859245796726175, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.005135600920766592}, {"id": 501, "seek": 380704, "start": 3823.36, "end": 3829.12, "text": " difficult to understand why do we need to call it twice honestly I do not know I just know that it", "tokens": [51180, 2252, 281, 1223, 983, 360, 321, 643, 281, 818, 309, 6091, 6095, 286, 360, 406, 458, 286, 445, 458, 300, 309, 51468], "temperature": 0.0, "avg_logprob": -0.10859245796726175, "compression_ratio": 1.6839080459770115, "no_speech_prob": 0.005135600920766592}, {"id": 502, "seek": 382912, "start": 3829.12, "end": 3838.48, "text": " works when I do it and actually and sometimes it doesn't work so well worst case scenario you can", "tokens": [50364, 1985, 562, 286, 360, 309, 293, 767, 293, 2171, 309, 1177, 380, 589, 370, 731, 5855, 1389, 9005, 291, 393, 50832], "temperature": 0.0, "avg_logprob": -0.09103068183450137, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.01494905911386013}, {"id": 503, "seek": 382912, "start": 3838.48, "end": 3846.7999999999997, "text": " just like restart the the collab and and just execute this part of the code but here for the", "tokens": [50832, 445, 411, 21022, 264, 264, 44228, 293, 293, 445, 14483, 341, 644, 295, 264, 3089, 457, 510, 337, 264, 51248], "temperature": 0.0, "avg_logprob": -0.09103068183450137, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.01494905911386013}, {"id": 504, "seek": 382912, "start": 3846.7999999999997, "end": 3858.72, "text": " sake of time I am going to copy paste the code so we are going to re reload the base model here", "tokens": [51248, 9717, 295, 565, 286, 669, 516, 281, 5055, 9163, 264, 3089, 370, 321, 366, 516, 281, 319, 25628, 264, 3096, 2316, 510, 51844], "temperature": 0.0, "avg_logprob": -0.09103068183450137, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.01494905911386013}, {"id": 505, "seek": 385912, "start": 3859.2, "end": 3865.52, "text": " and we are going to also load the adapter so the cooler adapter that we", "tokens": [50368, 293, 321, 366, 516, 281, 611, 3677, 264, 22860, 370, 264, 15566, 22860, 300, 321, 50684], "temperature": 0.0, "avg_logprob": -0.13990849354228035, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.0011323877843096852}, {"id": 506, "seek": 385912, "start": 3867.04, "end": 3874.88, "text": " we created you can see it here this is our cooler adapter with adapter config adapter model", "tokens": [50760, 321, 2942, 291, 393, 536, 309, 510, 341, 307, 527, 15566, 22860, 365, 22860, 6662, 22860, 2316, 51152], "temperature": 0.0, "avg_logprob": -0.13990849354228035, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.0011323877843096852}, {"id": 507, "seek": 385912, "start": 3874.88, "end": 3886.24, "text": " and this is what we want to to load hopefully it will work and and then we can push our model", "tokens": [51152, 293, 341, 307, 437, 321, 528, 281, 281, 3677, 4696, 309, 486, 589, 293, 293, 550, 321, 393, 2944, 527, 2316, 51720], "temperature": 0.0, "avg_logprob": -0.13990849354228035, "compression_ratio": 1.7972027972027973, "no_speech_prob": 0.0011323877843096852}, {"id": 508, "seek": 388624, "start": 3886.24, "end": 3893.2799999999997, "text": " to the hugging face hub when we do that we are going to push the model and the tokenizer and", "tokens": [50364, 281, 264, 41706, 1851, 11838, 562, 321, 360, 300, 321, 366, 516, 281, 2944, 264, 2316, 293, 264, 14862, 6545, 293, 50716], "temperature": 0.0, "avg_logprob": -0.11424286926493925, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0036472880747169256}, {"id": 509, "seek": 388624, "start": 3893.9199999999996, "end": 3901.2, "text": " we're using the hf token here this is optional of course and this will upload it to our", "tokens": [50748, 321, 434, 1228, 264, 276, 69, 14862, 510, 341, 307, 17312, 295, 1164, 293, 341, 486, 6580, 309, 281, 527, 51112], "temperature": 0.0, "avg_logprob": -0.11424286926493925, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0036472880747169256}, {"id": 510, "seek": 388624, "start": 3903.68, "end": 3911.3599999999997, "text": " our hugging face account so here we are merging and unloading the model we are going to reload", "tokens": [51236, 527, 41706, 1851, 2696, 370, 510, 321, 366, 44559, 293, 32165, 278, 264, 2316, 321, 366, 516, 281, 25628, 51620], "temperature": 0.0, "avg_logprob": -0.11424286926493925, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.0036472880747169256}, {"id": 511, "seek": 391136, "start": 3911.36, "end": 3919.36, "text": " the tokenizer not just in order to save it too and it's going to be uploaded okay so this is the", "tokens": [50364, 264, 14862, 6545, 406, 445, 294, 1668, 281, 3155, 309, 886, 293, 309, 311, 516, 281, 312, 17135, 1392, 370, 341, 307, 264, 50764], "temperature": 0.0, "avg_logprob": -0.163335403838715, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.002657794626429677}, {"id": 512, "seek": 391136, "start": 3919.36, "end": 3928.4, "text": " end of this session sorry it's a bit late but if you want to go further just know that you should", "tokens": [50764, 917, 295, 341, 5481, 2597, 309, 311, 257, 857, 3469, 457, 498, 291, 528, 281, 352, 3052, 445, 458, 300, 291, 820, 51216], "temperature": 0.0, "avg_logprob": -0.163335403838715, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.002657794626429677}, {"id": 513, "seek": 391136, "start": 3928.4, "end": 3936.1600000000003, "text": " be able to reuse use this entire collab notebook with mistral 7b instead of flama 7b mistral 7b", "tokens": [51216, 312, 1075, 281, 26225, 764, 341, 2302, 44228, 21060, 365, 3544, 2155, 1614, 65, 2602, 295, 932, 2404, 1614, 65, 3544, 2155, 1614, 65, 51604], "temperature": 0.0, "avg_logprob": -0.163335403838715, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.002657794626429677}, {"id": 514, "seek": 393616, "start": 3936.16, "end": 3942.48, "text": " is a better model but the name of this talk was fine-tuning llama 2 so I stuck to llama 2", "tokens": [50364, 307, 257, 1101, 2316, 457, 264, 1315, 295, 341, 751, 390, 2489, 12, 83, 37726, 23272, 568, 370, 286, 5541, 281, 23272, 568, 50680], "temperature": 0.0, "avg_logprob": -0.14213897870934528, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.03819390758872032}, {"id": 515, "seek": 393616, "start": 3943.12, "end": 3950.96, "text": " but I would yeah I encourage you to try it out if you want a better fine-tuning tool I recommend", "tokens": [50712, 457, 286, 576, 1338, 286, 5373, 291, 281, 853, 309, 484, 498, 291, 528, 257, 1101, 2489, 12, 83, 37726, 2290, 286, 2748, 51104], "temperature": 0.0, "avg_logprob": -0.14213897870934528, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.03819390758872032}, {"id": 516, "seek": 393616, "start": 3950.96, "end": 3955.7599999999998, "text": " actual tool because these Google collab they're really nice to understand the theory behind everything", "tokens": [51104, 3539, 2290, 570, 613, 3329, 44228, 436, 434, 534, 1481, 281, 1223, 264, 5261, 2261, 1203, 51344], "temperature": 0.0, "avg_logprob": -0.14213897870934528, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.03819390758872032}, {"id": 517, "seek": 393616, "start": 3955.7599999999998, "end": 3964.56, "text": " and be able to implement this fine-tuning process on your own but if you want to really", "tokens": [51344, 293, 312, 1075, 281, 4445, 341, 2489, 12, 83, 37726, 1399, 322, 428, 1065, 457, 498, 291, 528, 281, 534, 51784], "temperature": 0.0, "avg_logprob": -0.14213897870934528, "compression_ratio": 1.7136363636363636, "no_speech_prob": 0.03819390758872032}, {"id": 518, "seek": 396456, "start": 3965.36, "end": 3972.0, "text": " fine-tune state-of-the-art open source LLMs I recommend actual tool it's really a great tool", "tokens": [50404, 2489, 12, 83, 2613, 1785, 12, 2670, 12, 3322, 12, 446, 1269, 4009, 441, 43, 26386, 286, 2748, 3539, 2290, 309, 311, 534, 257, 869, 2290, 50736], "temperature": 0.0, "avg_logprob": -0.10933235994319326, "compression_ratio": 1.7395348837209301, "no_speech_prob": 0.005043880548328161}, {"id": 519, "seek": 396456, "start": 3972.0, "end": 3978.32, "text": " I've been using it a lot of people have been using it and this is quite easy to use so yeah", "tokens": [50736, 286, 600, 668, 1228, 309, 257, 688, 295, 561, 362, 668, 1228, 309, 293, 341, 307, 1596, 1858, 281, 764, 370, 1338, 51052], "temperature": 0.0, "avg_logprob": -0.10933235994319326, "compression_ratio": 1.7395348837209301, "no_speech_prob": 0.005043880548328161}, {"id": 520, "seek": 396456, "start": 3978.32, "end": 3985.2, "text": " this is a good recommendation and then what you can do with this model is you can evaluate it", "tokens": [51052, 341, 307, 257, 665, 11879, 293, 550, 437, 291, 393, 360, 365, 341, 2316, 307, 291, 393, 13059, 309, 51396], "temperature": 0.0, "avg_logprob": -0.10933235994319326, "compression_ratio": 1.7395348837209301, "no_speech_prob": 0.005043880548328161}, {"id": 521, "seek": 396456, "start": 3985.2, "end": 3990.56, "text": " using a evaluation harness you can even get on the open LM leaderboard if you have a good model", "tokens": [51396, 1228, 257, 13344, 19700, 291, 393, 754, 483, 322, 264, 1269, 46529, 5263, 3787, 498, 291, 362, 257, 665, 2316, 51664], "temperature": 0.0, "avg_logprob": -0.10933235994319326, "compression_ratio": 1.7395348837209301, "no_speech_prob": 0.005043880548328161}, {"id": 522, "seek": 399056, "start": 3991.52, "end": 3998.24, "text": " or you can quantize it so you would make it easier to execute on consumer grade hardware", "tokens": [50412, 420, 291, 393, 4426, 1125, 309, 370, 291, 576, 652, 309, 3571, 281, 14483, 322, 9711, 7204, 8837, 50748], "temperature": 0.0, "avg_logprob": -0.053806154351485405, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0027115491684526205}, {"id": 523, "seek": 399056, "start": 3998.24, "end": 4006.88, "text": " and you could use your fine-tune model on your own GPU so that's it for me it will take a while", "tokens": [50748, 293, 291, 727, 764, 428, 2489, 12, 83, 2613, 2316, 322, 428, 1065, 18407, 370, 300, 311, 309, 337, 385, 309, 486, 747, 257, 1339, 51180], "temperature": 0.0, "avg_logprob": -0.053806154351485405, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0027115491684526205}, {"id": 524, "seek": 399056, "start": 4006.88, "end": 4011.36, "text": " for the model to be pushed to the hub because it's quite it's quite big but as you can see it's", "tokens": [51180, 337, 264, 2316, 281, 312, 9152, 281, 264, 11838, 570, 309, 311, 1596, 309, 311, 1596, 955, 457, 382, 291, 393, 536, 309, 311, 51404], "temperature": 0.0, "avg_logprob": -0.053806154351485405, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0027115491684526205}, {"id": 525, "seek": 399056, "start": 4011.36, "end": 4019.36, "text": " been merged and everything is working correctly so I hope that you found it useful and if you", "tokens": [51404, 668, 36427, 293, 1203, 307, 1364, 8944, 370, 286, 1454, 300, 291, 1352, 309, 4420, 293, 498, 291, 51804], "temperature": 0.0, "avg_logprob": -0.053806154351485405, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.0027115491684526205}, {"id": 526, "seek": 401936, "start": 4019.36, "end": 4024.88, "text": " have any questions maybe now it's the time to ask the question to answer the questions", "tokens": [50364, 362, 604, 1651, 1310, 586, 309, 311, 264, 565, 281, 1029, 264, 1168, 281, 1867, 264, 1651, 50640], "temperature": 0.0, "avg_logprob": -0.13402328258607446, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.010649017989635468}, {"id": 527, "seek": 401936, "start": 4026.08, "end": 4031.92, "text": " all right thank you Maxim that was fantastic a lot to unpack there I actually have like", "tokens": [50700, 439, 558, 1309, 291, 29076, 300, 390, 5456, 257, 688, 281, 26699, 456, 286, 767, 362, 411, 50992], "temperature": 0.0, "avg_logprob": -0.13402328258607446, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.010649017989635468}, {"id": 528, "seek": 401936, "start": 4031.92, "end": 4036.4, "text": " a lot of questions for you but I think if I start asking questions we're going to go on", "tokens": [50992, 257, 688, 295, 1651, 337, 291, 457, 286, 519, 498, 286, 722, 3365, 1651, 321, 434, 516, 281, 352, 322, 51216], "temperature": 0.0, "avg_logprob": -0.13402328258607446, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.010649017989635468}, {"id": 529, "seek": 401936, "start": 4036.4, "end": 4041.04, "text": " longer than an Ed Sheeran concert so I'm going to stick to audience questions instead", "tokens": [51216, 2854, 813, 364, 3977, 1240, 260, 282, 8543, 370, 286, 478, 516, 281, 2897, 281, 4034, 1651, 2602, 51448], "temperature": 0.0, "avg_logprob": -0.13402328258607446, "compression_ratio": 1.7227722772277227, "no_speech_prob": 0.010649017989635468}, {"id": 530, "seek": 404104, "start": 4041.52, "end": 4050.48, "text": " let's go with this one first from Prudine so Prudine is saying like maybe don't need to show", "tokens": [50388, 718, 311, 352, 365, 341, 472, 700, 490, 2114, 532, 533, 370, 2114, 532, 533, 307, 1566, 411, 1310, 500, 380, 643, 281, 855, 50836], "temperature": 0.0, "avg_logprob": -0.18550209045410157, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0678197368979454}, {"id": 531, "seek": 404104, "start": 4050.48, "end": 4055.68, "text": " this but how do you go about querying tabular data so I mean this is very much focused on we've just", "tokens": [50836, 341, 457, 577, 360, 291, 352, 466, 7083, 1840, 4421, 1040, 1412, 370, 286, 914, 341, 307, 588, 709, 5178, 322, 321, 600, 445, 51096], "temperature": 0.0, "avg_logprob": -0.18550209045410157, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0678197368979454}, {"id": 532, "seek": 404104, "start": 4055.68, "end": 4063.12, "text": " got a lot of text if it's tabular data what's the difference for tabular data I would not", "tokens": [51096, 658, 257, 688, 295, 2487, 498, 309, 311, 4421, 1040, 1412, 437, 311, 264, 2649, 337, 4421, 1040, 1412, 286, 576, 406, 51468], "temperature": 0.0, "avg_logprob": -0.18550209045410157, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0678197368979454}, {"id": 533, "seek": 404104, "start": 4063.12, "end": 4069.68, "text": " recommend using LLMs because they've really made for for text there are some yeah actually I'm", "tokens": [51468, 2748, 1228, 441, 43, 26386, 570, 436, 600, 534, 1027, 337, 337, 2487, 456, 366, 512, 1338, 767, 286, 478, 51796], "temperature": 0.0, "avg_logprob": -0.18550209045410157, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.0678197368979454}, {"id": 534, "seek": 406968, "start": 4069.68, "end": 4074.96, "text": " wondering like I don't know maybe maybe we can't get into too much detail but is the standard like", "tokens": [50364, 6359, 411, 286, 500, 380, 458, 1310, 1310, 321, 393, 380, 483, 666, 886, 709, 2607, 457, 307, 264, 3832, 411, 50628], "temperature": 0.0, "avg_logprob": -0.13410315262643915, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009737636428326368}, {"id": 535, "seek": 406968, "start": 4076.24, "end": 4080.96, "text": " if you've got just a text file or once if you've got like a pandas data frame of text", "tokens": [50692, 498, 291, 600, 658, 445, 257, 2487, 3991, 420, 1564, 498, 291, 600, 658, 411, 257, 4565, 296, 1412, 3920, 295, 2487, 50928], "temperature": 0.0, "avg_logprob": -0.13410315262643915, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009737636428326368}, {"id": 536, "seek": 406968, "start": 4082.3199999999997, "end": 4088.7999999999997, "text": " yeah this is a good question actually you could see it when we uploaded our data set to the", "tokens": [50996, 1338, 341, 307, 257, 665, 1168, 767, 291, 727, 536, 309, 562, 321, 17135, 527, 1412, 992, 281, 264, 51320], "temperature": 0.0, "avg_logprob": -0.13410315262643915, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009737636428326368}, {"id": 537, "seek": 406968, "start": 4088.7999999999997, "end": 4095.7599999999998, "text": " hiking phase hub this is kind of a data frame but it only has a text so if we go back to the", "tokens": [51320, 23784, 5574, 11838, 341, 307, 733, 295, 257, 1412, 3920, 457, 309, 787, 575, 257, 2487, 370, 498, 321, 352, 646, 281, 264, 51668], "temperature": 0.0, "avg_logprob": -0.13410315262643915, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009737636428326368}, {"id": 538, "seek": 409576, "start": 4095.76, "end": 4100.08, "text": " data set that we've dealt here you can see like it's basically data frame with instruction", "tokens": [50364, 1412, 992, 300, 321, 600, 15991, 510, 291, 393, 536, 411, 309, 311, 1936, 1412, 3920, 365, 10951, 50580], "temperature": 0.0, "avg_logprob": -0.13212645184862745, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.005982582923024893}, {"id": 539, "seek": 409576, "start": 4100.08, "end": 4108.400000000001, "text": " output columns but it's not it's not tabular right it's it's really text yeah okay interesting", "tokens": [50580, 5598, 13766, 457, 309, 311, 406, 309, 311, 406, 4421, 1040, 558, 309, 311, 309, 311, 534, 2487, 1338, 1392, 1880, 50996], "temperature": 0.0, "avg_logprob": -0.13212645184862745, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.005982582923024893}, {"id": 540, "seek": 409576, "start": 4109.04, "end": 4116.0, "text": " all right next question comes from Kiran saying okay so we've been doing this fine tuning on a GPU", "tokens": [51028, 439, 558, 958, 1168, 1487, 490, 11305, 282, 1566, 1392, 370, 321, 600, 668, 884, 341, 2489, 15164, 322, 257, 18407, 51376], "temperature": 0.0, "avg_logprob": -0.13212645184862745, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.005982582923024893}, {"id": 541, "seek": 409576, "start": 4116.0, "end": 4120.24, "text": " do we also need a GPU at the point where we're hosting these things is it just the training bit", "tokens": [51376, 360, 321, 611, 643, 257, 18407, 412, 264, 935, 689, 321, 434, 16058, 613, 721, 307, 309, 445, 264, 3097, 857, 51588], "temperature": 0.0, "avg_logprob": -0.13212645184862745, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.005982582923024893}, {"id": 542, "seek": 412024, "start": 4120.32, "end": 4126.4, "text": " that's computationally intensive or is it also inference no the inference is also like very", "tokens": [50368, 300, 311, 24903, 379, 18957, 420, 307, 309, 611, 38253, 572, 264, 38253, 307, 611, 411, 588, 50672], "temperature": 0.0, "avg_logprob": -0.09891644768092943, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.013895773328840733}, {"id": 543, "seek": 412024, "start": 4126.4, "end": 4133.36, "text": " intensive unfortunately and you definitely need well you need a GPU in general but in particular", "tokens": [50672, 18957, 7015, 293, 291, 2138, 643, 731, 291, 643, 257, 18407, 294, 2674, 457, 294, 1729, 51020], "temperature": 0.0, "avg_logprob": -0.09891644768092943, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.013895773328840733}, {"id": 544, "seek": 412024, "start": 4133.36, "end": 4146.719999999999, "text": " if you use lama.cpp I can show it on my screen if you use lama.cpp you can use it on a CPU", "tokens": [51020, 498, 291, 764, 45423, 13, 66, 427, 286, 393, 855, 309, 322, 452, 2568, 498, 291, 764, 45423, 13, 66, 427, 291, 393, 764, 309, 322, 257, 13199, 51688], "temperature": 0.0, "avg_logprob": -0.09891644768092943, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.013895773328840733}, {"id": 545, "seek": 414672, "start": 4146.72, "end": 4156.8, "text": " you can see it sorry I'm gonna you can see it here it's me like running it and it runs on on a CPU", "tokens": [50364, 291, 393, 536, 309, 2597, 286, 478, 799, 291, 393, 536, 309, 510, 309, 311, 385, 411, 2614, 309, 293, 309, 6676, 322, 322, 257, 13199, 50868], "temperature": 0.0, "avg_logprob": -0.1494770050048828, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.005218040198087692}, {"id": 546, "seek": 414672, "start": 4157.84, "end": 4163.68, "text": " you'll have to compromise a little bit because like you need to lower the precision of the model so", "tokens": [50920, 291, 603, 362, 281, 18577, 257, 707, 857, 570, 411, 291, 643, 281, 3126, 264, 18356, 295, 264, 2316, 370, 51212], "temperature": 0.0, "avg_logprob": -0.1494770050048828, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.005218040198087692}, {"id": 547, "seek": 414672, "start": 4163.68, "end": 4172.320000000001, "text": " it's smaller and faster to execute but this is something that you can do on the CPU", "tokens": [51212, 309, 311, 4356, 293, 4663, 281, 14483, 457, 341, 307, 746, 300, 291, 393, 360, 322, 264, 13199, 51644], "temperature": 0.0, "avg_logprob": -0.1494770050048828, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.005218040198087692}, {"id": 548, "seek": 417232, "start": 4173.2, "end": 4177.679999999999, "text": " all right very good so um for anyone who's interested in lama.cpp I know I've got a", "tokens": [50408, 439, 558, 588, 665, 370, 1105, 337, 2878, 567, 311, 3102, 294, 45423, 13, 66, 427, 286, 458, 286, 600, 658, 257, 50632], "temperature": 0.0, "avg_logprob": -0.16337252656618753, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.003491004230454564}, {"id": 549, "seek": 417232, "start": 4177.679999999999, "end": 4184.0, "text": " tutorial on that perhaps Reece can post a link to that in the moment next question", "tokens": [50632, 7073, 322, 300, 4317, 38231, 384, 393, 2183, 257, 2113, 281, 300, 294, 264, 1623, 958, 1168, 50948], "temperature": 0.0, "avg_logprob": -0.16337252656618753, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.003491004230454564}, {"id": 550, "seek": 417232, "start": 4184.799999999999, "end": 4193.28, "text": " comes from Minfem so it looks like you you got some fancy co-pilot action or some sort of", "tokens": [50988, 1487, 490, 2829, 69, 443, 370, 309, 1542, 411, 291, 291, 658, 512, 10247, 598, 12, 79, 31516, 3069, 420, 512, 1333, 295, 51412], "temperature": 0.0, "avg_logprob": -0.16337252656618753, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.003491004230454564}, {"id": 551, "seek": 417232, "start": 4193.28, "end": 4199.28, "text": " autocomplete thing going on there in co-lab where does it what's that tool yeah it's a tool called", "tokens": [51412, 45833, 298, 17220, 551, 516, 322, 456, 294, 598, 12, 44990, 689, 775, 309, 437, 311, 300, 2290, 1338, 309, 311, 257, 2290, 1219, 51712], "temperature": 0.0, "avg_logprob": -0.16337252656618753, "compression_ratio": 1.5638766519823788, "no_speech_prob": 0.003491004230454564}, {"id": 552, "seek": 419928, "start": 4199.28, "end": 4204.32, "text": " codium and yeah it works really well with Google collab as you could see I don't know if it learned", "tokens": [50364, 17656, 2197, 293, 1338, 309, 1985, 534, 731, 365, 3329, 44228, 382, 291, 727, 536, 286, 500, 380, 458, 498, 309, 3264, 50616], "temperature": 0.0, "avg_logprob": -0.1556077418120011, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.008318854495882988}, {"id": 553, "seek": 419928, "start": 4204.32, "end": 4211.36, "text": " from my own code but it was like really accurate this time I spoke well when you're practicing", "tokens": [50616, 490, 452, 1065, 3089, 457, 309, 390, 411, 534, 8559, 341, 565, 286, 7179, 731, 562, 291, 434, 11350, 50968], "temperature": 0.0, "avg_logprob": -0.1556077418120011, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.008318854495882988}, {"id": 554, "seek": 419928, "start": 4211.36, "end": 4217.04, "text": " rehearsing the the tutorial you probably have the same thing a few times so yeah yeah good way to", "tokens": [50968, 17052, 278, 264, 264, 7073, 291, 1391, 362, 264, 912, 551, 257, 1326, 1413, 370, 1338, 1338, 665, 636, 281, 51252], "temperature": 0.0, "avg_logprob": -0.1556077418120011, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.008318854495882988}, {"id": 555, "seek": 419928, "start": 4217.04, "end": 4225.599999999999, "text": " train it yeah I mean they're training dataset now nice okay so codium's the thing uh next thing", "tokens": [51252, 3847, 309, 1338, 286, 914, 436, 434, 3097, 28872, 586, 1481, 1392, 370, 17656, 2197, 311, 264, 551, 2232, 958, 551, 51680], "temperature": 0.0, "avg_logprob": -0.1556077418120011, "compression_ratio": 1.6869565217391305, "no_speech_prob": 0.008318854495882988}, {"id": 556, "seek": 422560, "start": 4225.68, "end": 4231.92, "text": " comes from from Wei saying uh how important is parameter tuning during function I guess that's", "tokens": [50368, 1487, 490, 490, 21174, 1566, 2232, 577, 1021, 307, 13075, 15164, 1830, 2445, 286, 2041, 300, 311, 50680], "temperature": 0.0, "avg_logprob": -0.15574890734201455, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.016346968710422516}, {"id": 557, "seek": 422560, "start": 4231.92, "end": 4237.280000000001, "text": " like hyper parameter tuning yeah it's a really good question for some of the hyper parameters", "tokens": [50680, 411, 9848, 13075, 15164, 1338, 309, 311, 257, 534, 665, 1168, 337, 512, 295, 264, 9848, 9834, 50948], "temperature": 0.0, "avg_logprob": -0.15574890734201455, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.016346968710422516}, {"id": 558, "seek": 422560, "start": 4237.280000000001, "end": 4243.68, "text": " going to be very important um and for some of them it's it's more like one percent two percent", "tokens": [50948, 516, 281, 312, 588, 1021, 1105, 293, 337, 512, 295, 552, 309, 311, 309, 311, 544, 411, 472, 3043, 732, 3043, 51268], "temperature": 0.0, "avg_logprob": -0.15574890734201455, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.016346968710422516}, {"id": 559, "seek": 422560, "start": 4243.68, "end": 4250.56, "text": " gains for example everything here honestly if you stick to like traditional values uh it's it's", "tokens": [51268, 16823, 337, 1365, 1203, 510, 6095, 498, 291, 2897, 281, 411, 5164, 4190, 2232, 309, 311, 309, 311, 51612], "temperature": 0.0, "avg_logprob": -0.15574890734201455, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.016346968710422516}, {"id": 560, "seek": 425056, "start": 4250.56, "end": 4255.68, "text": " gonna make like not super meaningful improvements of course they're important because one percent", "tokens": [50364, 799, 652, 411, 406, 1687, 10995, 13797, 295, 1164, 436, 434, 1021, 570, 472, 3043, 50620], "temperature": 0.0, "avg_logprob": -0.13220319515321313, "compression_ratio": 1.8066037735849056, "no_speech_prob": 0.010465587489306927}, {"id": 561, "seek": 425056, "start": 4255.68, "end": 4262.160000000001, "text": " two percent it's good to have but they're not yeah that that important uh and there are other", "tokens": [50620, 732, 3043, 309, 311, 665, 281, 362, 457, 436, 434, 406, 1338, 300, 300, 1021, 2232, 293, 456, 366, 661, 50944], "temperature": 0.0, "avg_logprob": -0.13220319515321313, "compression_ratio": 1.8066037735849056, "no_speech_prob": 0.010465587489306927}, {"id": 562, "seek": 425056, "start": 4262.160000000001, "end": 4268.56, "text": " yeah parameters that are a bit more important the learning rate is a really important one", "tokens": [50944, 1338, 9834, 300, 366, 257, 857, 544, 1021, 264, 2539, 3314, 307, 257, 534, 1021, 472, 51264], "temperature": 0.0, "avg_logprob": -0.13220319515321313, "compression_ratio": 1.8066037735849056, "no_speech_prob": 0.010465587489306927}, {"id": 563, "seek": 425056, "start": 4269.68, "end": 4274.64, "text": " and for this one yeah recommend checking the model that you want to use if you use mistral instead of", "tokens": [51320, 293, 337, 341, 472, 1338, 2748, 8568, 264, 2316, 300, 291, 528, 281, 764, 498, 291, 764, 3544, 2155, 2602, 295, 51568], "temperature": 0.0, "avg_logprob": -0.13220319515321313, "compression_ratio": 1.8066037735849056, "no_speech_prob": 0.010465587489306927}, {"id": 564, "seek": 427464, "start": 4275.6, "end": 4280.96, "text": " lambda two it's going to impact it if you use q lower lower or full fine tuning it's also going to", "tokens": [50412, 13607, 732, 309, 311, 516, 281, 2712, 309, 498, 291, 764, 9505, 3126, 3126, 420, 1577, 2489, 15164, 309, 311, 611, 516, 281, 50680], "temperature": 0.0, "avg_logprob": -0.17845702701144747, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.011702117510139942}, {"id": 565, "seek": 427464, "start": 4280.96, "end": 4290.88, "text": " impact it all right um excellent next one is from bed can you show again how you show how you", "tokens": [50680, 2712, 309, 439, 558, 1105, 7103, 958, 472, 307, 490, 2901, 393, 291, 855, 797, 577, 291, 855, 577, 291, 51176], "temperature": 0.0, "avg_logprob": -0.17845702701144747, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.011702117510139942}, {"id": 566, "seek": 427464, "start": 4290.88, "end": 4295.76, "text": " load save models maybe we'll skip the using for text generation but if you just cover", "tokens": [51176, 3677, 3155, 5245, 1310, 321, 603, 10023, 264, 1228, 337, 2487, 5125, 457, 498, 291, 445, 2060, 51420], "temperature": 0.0, "avg_logprob": -0.17845702701144747, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.011702117510139942}, {"id": 567, "seek": 427464, "start": 4295.76, "end": 4301.52, "text": " loading save models I think that's useful okay so the you save the model by calling the the trainer", "tokens": [51420, 15114, 3155, 5245, 286, 519, 300, 311, 4420, 1392, 370, 264, 291, 3155, 264, 2316, 538, 5141, 264, 264, 21110, 51708], "temperature": 0.0, "avg_logprob": -0.17845702701144747, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.011702117510139942}, {"id": 568, "seek": 430152, "start": 4302.4800000000005, "end": 4310.080000000001, "text": " object and with the model and save pre-trained new model uh so this is uh yeah just just some", "tokens": [50412, 2657, 293, 365, 264, 2316, 293, 3155, 659, 12, 17227, 2001, 777, 2316, 2232, 370, 341, 307, 2232, 1338, 445, 445, 512, 50792], "temperature": 0.0, "avg_logprob": -0.13481513821348853, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.002547937911003828}, {"id": 569, "seek": 430152, "start": 4310.080000000001, "end": 4317.360000000001, "text": " code you need to you know and and then it was the text generation right uh yes uh and in this case", "tokens": [50792, 3089, 291, 643, 281, 291, 458, 293, 293, 550, 309, 390, 264, 2487, 5125, 558, 2232, 2086, 2232, 293, 294, 341, 1389, 51156], "temperature": 0.0, "avg_logprob": -0.13481513821348853, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.002547937911003828}, {"id": 570, "seek": 430152, "start": 4317.360000000001, "end": 4324.0, "text": " I use the pipeline there are a lot of ways of like using them um it's not super pretty but uh it", "tokens": [51156, 286, 764, 264, 15517, 456, 366, 257, 688, 295, 2098, 295, 411, 1228, 552, 1105, 309, 311, 406, 1687, 1238, 457, 2232, 309, 51488], "temperature": 0.0, "avg_logprob": -0.13481513821348853, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.002547937911003828}, {"id": 571, "seek": 430152, "start": 4324.0, "end": 4329.040000000001, "text": " doesn't take a lot of lines of code and yeah this is an object from hugging face from their library", "tokens": [51488, 1177, 380, 747, 257, 688, 295, 3876, 295, 3089, 293, 1338, 341, 307, 364, 2657, 490, 41706, 1851, 490, 641, 6405, 51740], "temperature": 0.0, "avg_logprob": -0.13481513821348853, "compression_ratio": 1.728888888888889, "no_speech_prob": 0.002547937911003828}, {"id": 572, "seek": 432904, "start": 4329.6, "end": 4338.0, "text": " which allows you to nicely um use the the text generation for instance okay oh I think the", "tokens": [50392, 597, 4045, 291, 281, 9594, 1105, 764, 264, 264, 2487, 5125, 337, 5197, 1392, 1954, 286, 519, 264, 50812], "temperature": 0.0, "avg_logprob": -0.1515006592024618, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004525345750153065}, {"id": 573, "seek": 432904, "start": 4338.0, "end": 4344.16, "text": " question is about like once you've saved it how do you load that back okay uh basically just reusing", "tokens": [50812, 1168, 307, 466, 411, 1564, 291, 600, 6624, 309, 577, 360, 291, 3677, 300, 646, 1392, 2232, 1936, 445, 319, 7981, 51120], "temperature": 0.0, "avg_logprob": -0.1515006592024618, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004525345750153065}, {"id": 574, "seek": 432904, "start": 4344.16, "end": 4354.08, "text": " that and if you check actually um I've uh the model is uploaded on hugging face already and here", "tokens": [51120, 300, 293, 498, 291, 1520, 767, 1105, 286, 600, 2232, 264, 2316, 307, 17135, 322, 41706, 1851, 1217, 293, 510, 51616], "temperature": 0.0, "avg_logprob": -0.1515006592024618, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.004525345750153065}, {"id": 575, "seek": 435408, "start": 4354.16, "end": 4364.64, "text": " you have a usage um section where I describe how you all the code you need to use it so okay", "tokens": [50368, 291, 362, 257, 14924, 1105, 3541, 689, 286, 6786, 577, 291, 439, 264, 3089, 291, 643, 281, 764, 309, 370, 1392, 50892], "temperature": 0.0, "avg_logprob": -0.1371023758597996, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.004447749350219965}, {"id": 576, "seek": 435408, "start": 4364.64, "end": 4370.16, "text": " see you're getting the whatever model type is from pre-trained uh pulling back off the hugging", "tokens": [50892, 536, 291, 434, 1242, 264, 2035, 2316, 2010, 307, 490, 659, 12, 17227, 2001, 2232, 8407, 646, 766, 264, 41706, 51168], "temperature": 0.0, "avg_logprob": -0.1371023758597996, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.004447749350219965}, {"id": 577, "seek": 435408, "start": 4370.16, "end": 4378.88, "text": " face page all right nice yeah okay so uh next question from Arun saying can you see what percentage", "tokens": [51168, 1851, 3028, 439, 558, 1481, 1338, 1392, 370, 2232, 958, 1168, 490, 1587, 409, 1566, 393, 291, 536, 437, 9668, 51604], "temperature": 0.0, "avg_logprob": -0.1371023758597996, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.004447749350219965}, {"id": 578, "seek": 437888, "start": 4378.88, "end": 4386.400000000001, "text": " of trainable parameters um we are reduced we're going to have to queue Laura so this is like how", "tokens": [50364, 295, 3847, 712, 9834, 1105, 321, 366, 9212, 321, 434, 516, 281, 362, 281, 18639, 13220, 370, 341, 307, 411, 577, 50740], "temperature": 0.0, "avg_logprob": -0.17365193896823458, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.03220430389046669}, {"id": 579, "seek": 437888, "start": 4386.400000000001, "end": 4390.96, "text": " many different I remember you saying queue Laura just changes some of the weights in the model so I", "tokens": [50740, 867, 819, 286, 1604, 291, 1566, 18639, 13220, 445, 2962, 512, 295, 264, 17443, 294, 264, 2316, 370, 286, 50968], "temperature": 0.0, "avg_logprob": -0.17365193896823458, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.03220430389046669}, {"id": 580, "seek": 437888, "start": 4390.96, "end": 4398.24, "text": " guess you want to know what percentage that is um it's an excellent question I uh I cannot show you", "tokens": [50968, 2041, 291, 528, 281, 458, 437, 9668, 300, 307, 1105, 309, 311, 364, 7103, 1168, 286, 2232, 286, 2644, 855, 291, 51332], "temperature": 0.0, "avg_logprob": -0.17365193896823458, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.03220430389046669}, {"id": 581, "seek": 437888, "start": 4398.24, "end": 4407.2, "text": " because I deleted the model type in trainer um before but yeah there's a command to do that", "tokens": [51332, 570, 286, 22981, 264, 2316, 2010, 294, 21110, 1105, 949, 457, 1338, 456, 311, 257, 5622, 281, 360, 300, 51780], "temperature": 0.0, "avg_logprob": -0.17365193896823458, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.03220430389046669}, {"id": 582, "seek": 440720, "start": 4407.2, "end": 4411.84, "text": " I recommend checking on google but yeah definitely you can see exactly like the percentage of", "tokens": [50364, 286, 2748, 8568, 322, 20742, 457, 1338, 2138, 291, 393, 536, 2293, 411, 264, 9668, 295, 50596], "temperature": 0.0, "avg_logprob": -0.15922075803162622, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.008013295009732246}, {"id": 583, "seek": 440720, "start": 4411.84, "end": 4418.72, "text": " parameters the number of parameters that you're training using either lower or queue lower all", "tokens": [50596, 9834, 264, 1230, 295, 9834, 300, 291, 434, 3097, 1228, 2139, 3126, 420, 18639, 3126, 439, 50940], "temperature": 0.0, "avg_logprob": -0.15922075803162622, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.008013295009732246}, {"id": 584, "seek": 440720, "start": 4418.72, "end": 4426.0, "text": " right excellent um and we've got so many more questions uh all right let's just do a couple more", "tokens": [50940, 558, 7103, 1105, 293, 321, 600, 658, 370, 867, 544, 1651, 2232, 439, 558, 718, 311, 445, 360, 257, 1916, 544, 51304], "temperature": 0.0, "avg_logprob": -0.15922075803162622, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.008013295009732246}, {"id": 585, "seek": 442600, "start": 4426.56, "end": 4427.92, "text": " um so", "tokens": [50392, 1105, 370, 50460], "temperature": 0.0, "avg_logprob": -0.2850429018338521, "compression_ratio": 1.3430656934306568, "no_speech_prob": 0.022778278216719627}, {"id": 586, "seek": 442600, "start": 4434.08, "end": 4443.6, "text": " this next one all right okay it's always fine okay so Alexander asks how do you find", "tokens": [50768, 341, 958, 472, 439, 558, 1392, 309, 311, 1009, 2489, 1392, 370, 14845, 8962, 577, 360, 291, 915, 51244], "temperature": 0.0, "avg_logprob": -0.2850429018338521, "compression_ratio": 1.3430656934306568, "no_speech_prob": 0.022778278216719627}, {"id": 587, "seek": 442600, "start": 4443.6, "end": 4450.0, "text": " tune an LLM so it can extract JSON from differently format that's actually maybe a little bit", "tokens": [51244, 10864, 364, 441, 43, 44, 370, 309, 393, 8947, 31828, 490, 7614, 7877, 300, 311, 767, 1310, 257, 707, 857, 51564], "temperature": 0.0, "avg_logprob": -0.2850429018338521, "compression_ratio": 1.3430656934306568, "no_speech_prob": 0.022778278216719627}, {"id": 588, "seek": 445000, "start": 4450.16, "end": 4457.04, "text": " um specific but can you talk about how you apply it to like um a sort of you've got a CSV file or", "tokens": [50372, 1105, 2685, 457, 393, 291, 751, 466, 577, 291, 3079, 309, 281, 411, 1105, 257, 1333, 295, 291, 600, 658, 257, 48814, 3991, 420, 50716], "temperature": 0.0, "avg_logprob": -0.13757734413606576, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.012990505434572697}, {"id": 589, "seek": 445000, "start": 4457.04, "end": 4466.8, "text": " an excel file of text how do you how I guess how do you standardize that data um yeah I I don't know", "tokens": [50716, 364, 24015, 3991, 295, 2487, 577, 360, 291, 577, 286, 2041, 577, 360, 291, 3832, 1125, 300, 1412, 1105, 1338, 286, 286, 500, 380, 458, 51204], "temperature": 0.0, "avg_logprob": -0.13757734413606576, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.012990505434572697}, {"id": 590, "seek": 445000, "start": 4466.8, "end": 4474.32, "text": " if it's really a task for an LLM um because there um um if it's just extracting I would say like why", "tokens": [51204, 498, 309, 311, 534, 257, 5633, 337, 364, 441, 43, 44, 1105, 570, 456, 1105, 1105, 498, 309, 311, 445, 49844, 286, 576, 584, 411, 983, 51580], "temperature": 0.0, "avg_logprob": -0.13757734413606576, "compression_ratio": 1.5255102040816326, "no_speech_prob": 0.012990505434572697}, {"id": 591, "seek": 447432, "start": 4474.32, "end": 4481.679999999999, "text": " do you want to use an LLM and not something else um other than that um there are different", "tokens": [50364, 360, 291, 528, 281, 764, 364, 441, 43, 44, 293, 406, 746, 1646, 1105, 661, 813, 300, 1105, 456, 366, 819, 50732], "temperature": 0.0, "avg_logprob": -0.08988694349924724, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00817673560231924}, {"id": 592, "seek": 447432, "start": 4481.679999999999, "end": 4490.48, "text": " frameworks like uh JSON former or LMQM even better LMQM let me show you if it's really about the", "tokens": [50732, 29834, 411, 2232, 31828, 5819, 420, 46529, 48, 44, 754, 1101, 46529, 48, 44, 718, 385, 855, 291, 498, 309, 311, 534, 466, 264, 51172], "temperature": 0.0, "avg_logprob": -0.08988694349924724, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00817673560231924}, {"id": 593, "seek": 447432, "start": 4490.48, "end": 4497.759999999999, "text": " generation generating a popularly formatted JSON this is a really good uh framework to do it um", "tokens": [51172, 5125, 17746, 257, 3743, 356, 1254, 32509, 31828, 341, 307, 257, 534, 665, 2232, 8388, 281, 360, 309, 1105, 51536], "temperature": 0.0, "avg_logprob": -0.08988694349924724, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00817673560231924}, {"id": 594, "seek": 447432, "start": 4497.759999999999, "end": 4503.28, "text": " there are a lot of them but this one is currently the most popular one uh it's quite easy to use", "tokens": [51536, 456, 366, 257, 688, 295, 552, 457, 341, 472, 307, 4362, 264, 881, 3743, 472, 2232, 309, 311, 1596, 1858, 281, 764, 51812], "temperature": 0.0, "avg_logprob": -0.08988694349924724, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.00817673560231924}, {"id": 595, "seek": 450328, "start": 4503.92, "end": 4509.2, "text": " I'm not sure if we'd answer the questions but I wouldn't use an LLM to extract this information", "tokens": [50396, 286, 478, 406, 988, 498, 321, 1116, 1867, 264, 1651, 457, 286, 2759, 380, 764, 364, 441, 43, 44, 281, 8947, 341, 1589, 50660], "temperature": 0.0, "avg_logprob": -0.10257308999287713, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0034397144336253405}, {"id": 596, "seek": 450328, "start": 4509.2, "end": 4514.4, "text": " I would use it to generate a JSON uh and to generate this JSON this is the library I would use", "tokens": [50660, 286, 576, 764, 309, 281, 8460, 257, 31828, 2232, 293, 281, 8460, 341, 31828, 341, 307, 264, 6405, 286, 576, 764, 50920], "temperature": 0.0, "avg_logprob": -0.10257308999287713, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0034397144336253405}, {"id": 597, "seek": 450328, "start": 4515.759999999999, "end": 4523.84, "text": " okay uh so LMQL was that all right LMQL yeah LMQL all right that's worth looking into then", "tokens": [50988, 1392, 2232, 370, 46529, 48, 43, 390, 300, 439, 558, 46529, 48, 43, 1338, 46529, 48, 43, 439, 558, 300, 311, 3163, 1237, 666, 550, 51392], "temperature": 0.0, "avg_logprob": -0.10257308999287713, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0034397144336253405}, {"id": 598, "seek": 450328, "start": 4523.84, "end": 4528.32, "text": " all right one very one very last question then since we're well over time anyway we're", "tokens": [51392, 439, 558, 472, 588, 472, 588, 1036, 1168, 550, 1670, 321, 434, 731, 670, 565, 4033, 321, 434, 51616], "temperature": 0.0, "avg_logprob": -0.10257308999287713, "compression_ratio": 1.7440758293838863, "no_speech_prob": 0.0034397144336253405}, {"id": 599, "seek": 452832, "start": 4528.32, "end": 4535.2, "text": " we're past limits all right so um how can you improve the forms of LLMs you think basically", "tokens": [50364, 321, 434, 1791, 10406, 439, 558, 370, 1105, 577, 393, 291, 3470, 264, 6422, 295, 441, 43, 26386, 291, 519, 1936, 50708], "temperature": 0.0, "avg_logprob": -0.2194771462298454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.00509084016084671}, {"id": 600, "seek": 452832, "start": 4535.2, "end": 4541.28, "text": " what's the what a llama index and lang chain and what's the difference between them um yeah so this", "tokens": [50708, 437, 311, 264, 437, 257, 23272, 8186, 293, 2265, 5021, 293, 437, 311, 264, 2649, 1296, 552, 1105, 1338, 370, 341, 51012], "temperature": 0.0, "avg_logprob": -0.2194771462298454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.00509084016084671}, {"id": 601, "seek": 452832, "start": 4541.28, "end": 4548.08, "text": " is um you have fine tuning and um lang chain and llama index they are more about like creating this", "tokens": [51012, 307, 1105, 291, 362, 2489, 15164, 293, 1105, 2265, 5021, 293, 23272, 8186, 436, 366, 544, 466, 411, 4084, 341, 51352], "temperature": 0.0, "avg_logprob": -0.2194771462298454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.00509084016084671}, {"id": 602, "seek": 452832, "start": 4548.08, "end": 4556.88, "text": " retrieval augmented generations so um fine tuning is is one way of customizing an LLM for your use", "tokens": [51352, 19817, 3337, 36155, 10593, 370, 1105, 2489, 15164, 307, 307, 472, 636, 295, 2375, 3319, 364, 441, 43, 44, 337, 428, 764, 51792], "temperature": 0.0, "avg_logprob": -0.2194771462298454, "compression_ratio": 1.7180616740088106, "no_speech_prob": 0.00509084016084671}, {"id": 603, "seek": 455688, "start": 4556.88, "end": 4563.84, "text": " case and the RAG pipeline is another way of doing it so with lang chain and llama index you're going", "tokens": [50364, 1389, 293, 264, 14626, 38, 15517, 307, 1071, 636, 295, 884, 309, 370, 365, 2265, 5021, 293, 23272, 8186, 291, 434, 516, 50712], "temperature": 0.0, "avg_logprob": -0.14836385515001085, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0016469142865389585}, {"id": 604, "seek": 455688, "start": 4563.84, "end": 4570.4800000000005, "text": " to retrieve more context using some vector database or regular databases that you have", "tokens": [50712, 281, 30254, 544, 4319, 1228, 512, 8062, 8149, 420, 3890, 22380, 300, 291, 362, 51044], "temperature": 0.0, "avg_logprob": -0.14836385515001085, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0016469142865389585}, {"id": 605, "seek": 455688, "start": 4572.24, "end": 4577.12, "text": " the difference between them I'm not going to delve into the details LLM index is is", "tokens": [51132, 264, 2649, 1296, 552, 286, 478, 406, 516, 281, 43098, 666, 264, 4365, 441, 43, 44, 8186, 307, 307, 51376], "temperature": 0.0, "avg_logprob": -0.14836385515001085, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0016469142865389585}, {"id": 606, "seek": 455688, "start": 4578.56, "end": 4584.24, "text": " there's less stuff but maybe more in depth than lang chain and I would recommend", "tokens": [51448, 456, 311, 1570, 1507, 457, 1310, 544, 294, 7161, 813, 2265, 5021, 293, 286, 576, 2748, 51732], "temperature": 0.0, "avg_logprob": -0.14836385515001085, "compression_ratio": 1.6073059360730593, "no_speech_prob": 0.0016469142865389585}, {"id": 607, "seek": 458424, "start": 4585.2, "end": 4589.84, "text": " actually implementing both approaches so fine tuning your LLM and using this fine", "tokens": [50412, 767, 18114, 1293, 11587, 370, 2489, 15164, 428, 441, 43, 44, 293, 1228, 341, 2489, 50644], "temperature": 0.0, "avg_logprob": -0.12234655133000126, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.001561777200549841}, {"id": 608, "seek": 458424, "start": 4589.84, "end": 4594.88, "text": " tune LLM with the RAG pipeline and this is where you'll get the best performance possible", "tokens": [50644, 10864, 441, 43, 44, 365, 264, 14626, 38, 15517, 293, 341, 307, 689, 291, 603, 483, 264, 1151, 3389, 1944, 50896], "temperature": 0.0, "avg_logprob": -0.12234655133000126, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.001561777200549841}, {"id": 609, "seek": 458424, "start": 4596.48, "end": 4601.76, "text": " all right fantastic we're gonna have to call it a day there I think I know there's more questions", "tokens": [50976, 439, 558, 5456, 321, 434, 799, 362, 281, 818, 309, 257, 786, 456, 286, 519, 286, 458, 456, 311, 544, 1651, 51240], "temperature": 0.0, "avg_logprob": -0.12234655133000126, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.001561777200549841}, {"id": 610, "seek": 458424, "start": 4601.76, "end": 4605.599999999999, "text": " so sorry to everyone in the audience if you didn't get to your question I just want to say thank you", "tokens": [51240, 370, 2597, 281, 1518, 294, 264, 4034, 498, 291, 994, 380, 483, 281, 428, 1168, 286, 445, 528, 281, 584, 1309, 291, 51432], "temperature": 0.0, "avg_logprob": -0.12234655133000126, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.001561777200549841}, {"id": 611, "seek": 458424, "start": 4605.599999999999, "end": 4610.48, "text": " again Maxim that was like incredibly informative and lots of new things that I think we need to", "tokens": [51432, 797, 29076, 300, 390, 411, 6252, 27759, 293, 3195, 295, 777, 721, 300, 286, 519, 321, 643, 281, 51676], "temperature": 0.0, "avg_logprob": -0.12234655133000126, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.001561777200549841}, {"id": 612, "seek": 461048, "start": 4610.48, "end": 4617.36, "text": " explore so brilliant thank you again thank you to recent moderating oh sorry gone Maxim", "tokens": [50364, 6839, 370, 10248, 1309, 291, 797, 1309, 291, 281, 5162, 10494, 990, 1954, 2597, 2780, 29076, 50708], "temperature": 0.0, "avg_logprob": -0.13924323296060367, "compression_ratio": 1.8211382113821137, "no_speech_prob": 0.008634760044515133}, {"id": 613, "seek": 461048, "start": 4618.08, "end": 4623.28, "text": " thank you Richie and thanks everyone for your patience I know it's been a lot but I hope that", "tokens": [50744, 1309, 291, 6781, 414, 293, 3231, 1518, 337, 428, 14826, 286, 458, 309, 311, 668, 257, 688, 457, 286, 1454, 300, 51004], "temperature": 0.0, "avg_logprob": -0.13924323296060367, "compression_ratio": 1.8211382113821137, "no_speech_prob": 0.008634760044515133}, {"id": 614, "seek": 461048, "start": 4623.28, "end": 4628.639999999999, "text": " you found it informative so yeah all right brilliant and yeah so thank you to everyone in", "tokens": [51004, 291, 1352, 309, 27759, 370, 1338, 439, 558, 10248, 293, 1338, 370, 1309, 291, 281, 1518, 294, 51272], "temperature": 0.0, "avg_logprob": -0.13924323296060367, "compression_ratio": 1.8211382113821137, "no_speech_prob": 0.008634760044515133}, {"id": 615, "seek": 461048, "start": 4628.639999999999, "end": 4632.32, "text": " the audience who asked the question thank you to everyone who showed up today hope to see you all", "tokens": [51272, 264, 4034, 567, 2351, 264, 1168, 1309, 291, 281, 1518, 567, 4712, 493, 965, 1454, 281, 536, 291, 439, 51456], "temperature": 0.0, "avg_logprob": -0.13924323296060367, "compression_ratio": 1.8211382113821137, "no_speech_prob": 0.008634760044515133}, {"id": 616, "seek": 461048, "start": 4632.32, "end": 4636.879999999999, "text": " again soon lots of exciting webinars coming up so goodbye have a great weekend", "tokens": [51456, 797, 2321, 3195, 295, 4670, 26065, 1348, 493, 370, 12084, 362, 257, 869, 6711, 51684], "temperature": 0.0, "avg_logprob": -0.13924323296060367, "compression_ratio": 1.8211382113821137, "no_speech_prob": 0.008634760044515133}], "language": "en"}