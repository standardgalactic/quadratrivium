In this session we are going to cover two notebooks, one this is the first one dedicated to the
creation of the dataset and a second one dedicated to the fine-tuning of the model.
Here we're going to have a very big and friendly approach to this topic and then at the end of
this session I will present some resources to go beyond that and to be able to also like use
better tools and perform other things but in this session we're going to stick to like the basics
and try to really learn the theory behind it so we're also like more aware of what's going on when
we use automated tools. In this first notebook we're going to talk about datasets and how to
create a high quality dataset so I want to mention the fact that there are basically two datasets
several datasets we're interested in. The first one is instruction datasets where
inputs are instructions this is basically how you use chart gpt like write me something and
the model is supposed to write you something and not complete what you just said because this is
real completion and this is part of the pre-training of these models they pre-train on a lot of data
and the task is next token prediction so they're just here to predict what the next word is going
to be and this is nice but not really what we want to create assistance so instead we're going to
use supervised fine-tuning to be able to turn a base model into a useful chat model here.
There are also preference datasets I want to mention it briefly this is reinforcement
learning from human feedback this is often used after the supervised fine-tuning process and in
these datasets you will find different answers and some kind of ranking of these answers to say
hey this one is more useful than the other one we're going to mention it during the fine-tuning
process in the second notebook and finally there are other types of dataset it can be sentence
classification it can be code dataset where you have a fill in the middle objective where you
want to fill in code that has some context so before your cursor and after your cursor and
this is where you want the code to be but we're not going to talk about this it was just a general
overview of the kind of dataset you can expect in this space so as mentioned here we're just going
to use supervised fine-tuning so with an instruction dataset and we're going to be a lot
owned by filtering an existing dataset so the one that we're going to use today is the open
platypus dataset it's a collection of different datasets actually this is already a dataset that
has been filtered using all the dataset but we're going to do it even more we can check what it looks
like on the hugging face page here so here you have the instructions for example the board game
speeder and divider into three parts blah blah blah and the output that is expected by the model
so this is basically what your model is going to learn during the supervised fine-tuning process
it's going to learn to output this answer when it has this instruction and we repeat it a lot of
times and at some point the model is pretty good at understanding what it needs to do so it needs
to provide a useful answer for this instruction you have more information about the platypus dataset
and there's also a really nice paper you can find here about how they did it and how they trained
their models called platypus so an interesting read if you want to know more about it
so first of all let's start with the code we're going to need to install some libraries
so here we're going to keep install a bunch of libraries we're going to install datasets the
library from hugging face you handle this also transformers very useful sentence transformers
too because we are going to use sentence transformers to create embeddings of our data we're going to
see why in a few minutes and finally a vector database from facebook face gpu here you can see
the runtime so make sure you have a t4 gpu or more if you can afford it but the entire code should
run using a free tier google collab so with a t4 gpu for the sake of this exercise i'm using a v100
with higher ram because it's just going to be a bit faster and we won't have to stay at the screen
for 30 minutes the second step that is really useful so this is the first time i tried to use it
please bear with me this is quite a new feature in google collab but now we have a secret tab here
on the left and as you can see here you can add a new secret and you can give it a name
hugging face and a value so you can retrieve the value using this link if you have a hugging face
account so here i collab i can copy it here i can write the hugging face here and copy paste the
value here so and give access to the notebook um so this should be a really clean way of
managing your secrets because now they share across all your notebooks and they won't going to be
shared with people um other than in your account um to use it to jump in um we just got a question
from the audience that i'm not sure what the answer is i don't know how widespread this problem is
but do you know what an insure file accessible error message might be oh uh sorry i thought
that we tested that uh let me let me just uh try out the links that are used okay
uh anyone with the link should be able to access them actually uh some can some people
confirm that you can access it which you can you access these notebooks uh i can access
the notebooks one thing might be the case like you do need to be logged into google i think
in order to access colab so if you're not logged into google then you probably need to do that
all right we'll continue for now um if anyone else has any problems please do let us know in
the chat of the comments uh yes please uh oh uh james just might be a company blocking it for
security reasons yeah if you can't access any google products from uh your corporate uh laptop
then yeah you probably have to find a different network unfortunately or just watch for now
follow along uh when you get the recording yeah exactly uh sorry about that google colab can be
a bit difficult to use in some in some context um here we're going to uh import our secret
using uh google colab use the data use the data get so now the hf token um has the correct value
and this is what we're going to use if we need it in the when we need it in the rest of this
collab this is optional uh we are only going to need it when we want to upload uh the data set
so if you don't have a hugging face account you don't need to create one i recommend it because
it's nicer and you'll be able to upload your own data set in your own account but if not it's okay
i will upload it in my account and you'll be able to reuse it from this account
so now we have the hugging face token and the first thing that we want to do is to
load the data set that we want to use so in this case it's the data set that was mentioned here so
open open platypus we can copy paste it from here and it should load the data set download it
and load it and we're going to see all the different columns inside of the data set so
input output instruction and data source uh that should be correct here and we have the
number of rows so almost to 25k rows um so we can see a bit more about it we're going to
read it as a bandit's data set and we can even with google collab convert it into an interactive
table it's a better way of looking at it unfortunately it can take some time for google
collab to convert it so we're going to see but yeah it's nice to see that we have the instruction
and we have the output and this is all that matters here we're going to explore a bit more
about instruction the output if it was a real data set real raw data set what we want to do at this
point is really read not every line because it's too too too much but a lot of these lines
and be able to get a good understanding of what's in this data set and what we expect and also like
just clean it like if there are samples that are not high quality that have like bad english that are
just plain wrong we want to remove them this is very important we want to create the best
data set possible so it means filtering out a lot of samples okay so now we have this data set we
can read everything here this is a lot of data so we won't do it in this case it could take too
much time but now something that we can do let me show you once again the the code here so load
data set and then the name of the data and what we are going to do here is that we're going to
use the transformers library to import the tokenizer this will convert the row text into tokens
then we will import the matplotlib library to plot the results and also the cborn library
for the same reasons first we want to import the tokenizer so in our case we want the tokenizer not
from but as suggested here but from lamatu because this is the model that we want to use
so we're going to use a new research version of lamatu and not the official one from meta
why that it's because if you don't have a hugging face account you will not be able to access the
official version of it so news research just re-uploaded the entire thing so now we have
the tokenizer we can test it here it's going to download it once again from hugging face and
and then we're going to print it once it's done here you can sit with all the special tokens
unknown etc when it's done we want to use it to tokenize our instructions here and also outputs
here so the way that we're going to do it we're going to create a table an area called instruction
token counts it's a bit of a ghost but and we're going to use the tokenizer to tokenize
every sample in our data set this is not correct actually this is why you should not always trust
coder lambs for example in our data set train and close it so here in this table we should get
like the token counts for every instruction and we're going to repeat this process with the
output so pretty much the same thing here and we want the output and finally we want to
combine the instruction in the output because this is like the entire data set and so instruction
plus output and in this case we want to call it combine token counts and here we're going to merge
these two tables for instruction outputs in zip etc and we should get like the the sum of all the
tokens here in both instruction outputs now that is done we are going to do a little function to
plot the distribution of our token so we can have an easy visualization of it so here I'm going to
use sns set style white grid and slowly but surely get everything here
actually I'm sorry but I'm going to copy paste it so it's a bit faster to do it
and now we can plot the distribution for every instruction output and combine token count
so this is a very simple plot and we can call it using plot distribution and here we're going to
first use instruction token counts and we're going to see the distribution of token counts
for instruction on A
this part takes a bit of time unfortunately because yeah it needs to tokenize the entire data set
but once you've done it you can basically comment it it's okay and we're going to repeat this process
with the output token counts so this time it's for output only and finally the
combine token counts so here it's for instruction plus output here I'm going to
comment it so we get faster results and now we have a distribution for all of our data
so here you can see that we have approximately like the the minis is around here so around
500 tokens there's a long tail distribution goes up to like 5000 tokens why is it important
why does it matter it's because these models they have a certain context window and if it's
because beyond this context window it's not going to be very helpful so it's important to know like
the the number of tokens in our data set we also maybe want to sample more from
samples that have more tokens because they're going to be more informative than others
but we'll see about that basically here we can see okay I'm going to put a certain threshold
for this data set at 2k tokens the max context size for lambda 2 is actually 4k but
this is just an example to show how we can just set a threshold so here we want to filter out
rows with more than 2000 samples so one way of doing it is to say if I count in numeric combined
token counts if count okay so here we're going to retrieve the index of every sample where the
count in combined token counts is lower than 2k and now we can print how many of this we have
okay and if we print
length of the data set in train and we are going to remove the length of the valid indices
so we see how many we remove okay we only remove 31 of them but that's fine you can have a more
aggressive threshold if you want in this case yeah we're going to remove just a few of them as you
can see here then we're going to extract the valid rows based on the indices so here we need to
take the data set train and we're going to use the select method to only get the valid indices
and then we're going to get the token counts so for the valid rows so we can also plot this
distribution here and we plot the distribution of the token counts after filtering exactly like that
okay there's an issue here because I executed the code twice I shouldn't have
but that's fine if you execute it once it should be okay now it's already filtered which is why
it's grid in there and here you see that we have a very different plot because all this
right part has been filtered out another thing that we can do is near the duplication using
embeddings which is why we install the sentient transformers library and what we want to do here
is we want to embed every row every sample from our data set so here we want to translate that into
a vector we call an embedding and using an embedding model how to choose the best embedding model
it's often it's a popular question one way of answering it is looking at the MTEB the board
on a hugging face this is where you can see like a competition with all the embedding models on
various tasks it's funny because since I've made this screenshot there are new ones on top of it
the one that we're going to use here is the GTE base embedding model it's a it's a really good
model it's not the best model but it's going to be faster than other options which is why we're going
to use it here and we're going to use these embeddings to then calculate the similarity between
them and when they're too similar we're just going to filter them out so how are we going to do it
we're going to use the sentient transformer library and we import the sentient transformer
class we're also going to import face the vector database from facebook it's not the best vector
database but it's very simple it's very minimalistic which is why I used it in this example from
data set we are going to use data set and data set dict to be a bit fancy we're also going to use
tqdm to have a nice loading bar and finally number for some operations so here we're going to
really create the code in one function to do everything so we are going to pass it a data set
we are going to pass it the name of the embedding model we want to use and we're going to pass it
threshold for example 95 percent it means that when it's 95 percent as similar as another
embedding it's fishy and we probably do not want to to use it we probably want to filter out this
model so as a sentence transformer we're going to use the model that we we pass this argument
for the output we are going to use all the example in the data set so what we want to filter
filter out here are just the outputs are not the instructions we're fine if we have similar
instructions we just do not want similar outputs because this is what the model is going to be
trained on then we are going to say that we are converting text to embeddings we are going to
use the sentence model and encode the outputs that we have and we can even show a progress bar
to be fancy bar
then we're going to get the dimension of our embeddings so you can see in the
little boards some of them they have like 1024 some of them they have like 768 basically yeah
they have different dimensions take that into account we are going to create our index using
the vector database so it's going to be a flat ip index in this case
and we need to normalize our embeddings the face already has a function to do it but i do not trust
it that much so we're going to do it on our own because i had a bad experience with it
and i think it's going to be better that way so here we're using numpy to normalize it
using the norm to just take the norm to normalize the entire embedding space
and then we're going to add it to our index as normalized embeddings
then we're going to say we are filtering out let's say near duplicates
and in this part of the code we want to use the index search so we have
the normalized embedding we just put and here we are going to say k equal
2 so we are going to return at most two vectors we don't need more in this case
and we're going to create a list of the samples we want to keep call it to keep
and this is the main loop finally range length yeah it's fine
and we're going to do something nice for the QDM so we have a nice loading bar
and what we want here is if the dimension is so this if this is we're going to return here the
similarity between these embeddings and if this is this if the cosine similarity is
below the threshold we are going to keep it so we are going to add it to the to keep
happened and then we have i yes an index index and then we can create data set trains and we
are going to use the select method from the data set object to only keep these indexes
then we are going to return it as a data set dict this is not the most elegant way of doing it but
it's going to be fine for the purpose of this exercise and then we can call our function so
the duplicate data set and we are going to pass the data set and we're going to pass
the embedding model we want to use so in this case as I mentioned it's going to be the gte large
and we can just copy paste it here and as a threshold I'm going to use 0.95 be careful if
you switch the embedding model you won't have the same distribution of cosine similarity so some
models to get the same results you're going to need like 85 others you can might need 99.9
it really depends on the embedding model that you use it should be fine so now we can convert
the entire data set into embeddings here we are downloading the embedding model it's not a big
model which is why it's it's pretty fast the long part is actually comparing the embeddings
I should mention why we're doing it using a vector database instead of a for loop I've tried to do a
very minimalistic version using two for loop so we would compare every embedding to all the other
embeddings but it took a very long time so this is why we're using a vector database here to be
more efficient to be used by the computations and and get the results basically just faster because
otherwise it would take like two hours it was really really too long unfortunately so here we
still downloading the models and then with a v100 with high ram it should take about three to four
minutes to get all the embeddings and to filter out the data set in the meantime we can continue
I'm just going to show the code here because I don't know if you can really see it if you had time
to see the code this part might be a bit confusing but I don't want to delve too deep
to the details of the face vector database okay so now you see that it's converting the text to
embeddings oh it's going to take much longer than last time I've tried unfortunately but it's okay
like we we can stop it or come back later to finish it what we want to see when we have this
dedupe data set is the number of samples that were filtered out so we can print the length of the
original data set we can print the length of the dedupe data set and we can even print the number
of samples that were removed so in this case the length of the original data set minus the
length of the dedupe data set and this will tell us that how many rows we we removed and last time
you can see it later on the solution notebook it's about 8 000 samples one thing that we can do
when that part is over is topk sampling so in this case we still have too many samples because if
we remove 8 000 samples we're still going to have 20 no we're still going to have 16 k samples
maybe this is too much for what we want to do so we can randomly sample
some some rows in order to do that we're going to create a new function called topk
rows we are going to use a data set token counts and k to know how many we want to have
we're going to sort the indices because we can sort it by descending token count
and get the topk indices in this case so it's going to be sorted range
length token counts and here we are going to use going to use lambda i token counts
reverse is equal to true so here we should get
everything that we need so the token counts and get all the data sets with most samples first
and then topk indices we are going to just keep those those topk
yeah and i just talked about randomly doing it but it's not true we're just getting
like the the samples with the most tokens sorry about that and then we can create topk data
and here we have instruction where we want data sets that are in the topk indices
and we're going to do the same thing with the output
you could do something similar with the select method but yeah this time this time i want to
to be clear about what's going on so we have a full loop to select all the samples that were
in the sorted indices here and we are not going to keep like 1000 of them and we are going to do
the same thing with output and finally we can return our data sets from the dictionary that we
did topk data so this is our function but in order to call it we are going to need to have
the new token counts because here we filtered out a lot of samples so we can just copy paste
what we did at the beginning here get the instruction token counts the output token
counts the combined token counts and this is what we will use when we want to call this function
so let's have a k of 1000 and the topk data set will be get topk rows data set we're going to
use it with the combined counts and the k of 1000 so this is how we're going to call the function
and finally we are going to save it as a dictionary like yeah data set dict as they call it
to make sure that we still have like a train split but not not very important
after we've done that we can once again re-compute all of the token counts
and the plot actually like also plot the distribution
so this is just to see what's the new distribution after after filtering after topk sampling
now how it looks like and yeah we'll see we'll see in a few minutes
when this is done and we can see the
the distribution we can just also see the samples themselves to see like with pandas
how it looks like like we've done at the very beginning of this notebook here and we'll see
like how many samples remain and finally I want to mention chat templates so there's a need to
define a chat template if you want to use your large language model as a chat bot there are
different ways of doing it here's a way of doing it we have a raw user content either
raw assistant so this is more like the raw data
this is a format that you can use just user two points and then the message then assistant
two point nice nice to meet you in the case of flamato you have this particular template so
you have this token s then you have the instruction you have space
not not not the instruction it's it's another token for instruction then you have
a sys you have the system prompt you have here the user prompt and finally the model answer
it's quite a difficult template you we don't need to use it to function our lamato model
because we're functioning the base model and this chat template is only used in the chat version
of lamato this is not the one that we use here I wanted to mention the chat ml template from open
ai it looks like this it's the most popular and standardized one you can see it in a lot of
state-of-the-art open source models we're not going to use this one because it requires adding
tokens it's more difficult so the one that we're going to use is going to be quite simple we're
going to create a function called chat template about it with an example and in this example we're
going to format we format the instruction and here we're going to use this instruction then
break line then we can finally put the original instruction and break line break line and here
we can put like output or response and go for response and another break line why this one
honestly there's no good reason you could imagine a lot of different prompt templates but it's going
to be nice to see that the function model will follow this prompt template finally we can return
example and we map that using the map method from the data set object and this will
yeah this will change all of our instructions so they can follow this template we're going to
visualize it when okay it's done let's go back a bit earlier so we managed to remove
sorry a bit earlier so yeah we filtered everything that we wanted to filter we filtered out like
8k samples like I mentioned previously then there's a top case sampling where we said we only want to
keep the top 1000 samples in terms of token counts so the one with the most tokens and you can see
here the distribution of token counts for instruction only for token counts and finally
the distribution of token counts for instruction for output you can see we don't have samples with
less than 1000 samples thanks to the top case sampling yeah it should make sense hopefully
so here we have 1000 samples with a lot of tokens and they should be high quality because
they're not close to each other we need to duplicate them so it means that
they should be pretty far away from each other here you can see all the 1000 rows once again
you can click it here if you want to have a good overview of the samples that we that were selected
and now they should follow our chat template so just let's check if if this is correct
and here you can see the instruction let's click it here we really have like the instruction
and the response as mentioned here so this is working as intended instruction response and
here is the response that we want the model to follow all right so this is done and the final
thing that we can do here this is optional this is if you have a hugging phase hub count if you
put the value here of your secrets then you can just push the data set to the hugging phase hub
like this
and we specify the token here i'm calling it mini platpus you can call it however you want
um and this is going to upload it and you can even check if it's correctly uploaded if I go
to my hugging phase account and I check my data set this one was uploaded dated less than one
minute ago and here you can see our entire data set uh cool so we have everything
now uh and we're ready to go to fine tuning uh I hope it was it was clear uh and if not I hope
that the solution notebook will help you uh to to to create your own data sets to go further beyond
that you can create synthetic data using um 24 it's something that is used quite a lot and it
creates uh really good uh data sets so it is something that you can play with and otherwise
it's really like a manual reviewing you can import this data set in google sheets and really
manually review every row possible maybe create some regex to automate the process a little bit
but this is a very time consuming process but it's also like very nice because then people
can reuse your data sets if you share them but now let's go to the fine tuning uh notebook
you should also have it uh let me uh check the the chat uh okay you have the solution notebook
cool um so here you have um lama 2 and we're going to delve deep into the fine tuning process so
as mentioned previously there are two ways of fine tuning these models this supervised fine
tuning this is what we're gonna do uh so we're gonna tune it on a set of instructions and responses
it's going to help the model focus where we want um so to be helpful to follow the chat template too
and there's also the reinforcement learning from human feedback where we want the model to
maximize your word signal i'm not going to delve into that uh there are a lot of good articles
about it uh it's able to capture more complex uh preferences but it's also like more difficult to
implement and in practice uh most um if not all this this year but except this year nearly all
the state-of-the-art open source LLMs just use supervised fine tuning so yeah something to keep
in mind um and once again there's an example uh from a few months ago now the NEMA paper that
shows that only 1000 high quality samples can really make the difference and and get very far
um in this case when you have a 65 billion models um and i want to mention the open LM
leaderboard you might be familiar with it um but this is quite useful to see uh what are the best
models so currently you have like this non-Lama model um i wanted to uh yeah show this Godzilla
2 7b model because um it's using a Lama 2 70b and um uh i saw that it was using my data set so
when i was telling you like yeah it's nice to also share your data set you see like sometimes it can
be reused by other people without uh you knowing anything about it but i'm glad this one was useful
um so what we're going to do here is um as previously we are going to start by installing
all the libraries that we want so in this case we're going to go pip install queue and we're
going to update because uh Google collab already has some of these libraries but we want to use
really like the latest version available in this case bits and bytes and 1db transformers
and the hugging face oops i'm going to disconnect this session okay uh once again you can use a
t4 gpu for the entire uh notebook here i'm just going to use the v100 because it's going to be
faster transformers for the transformers data set for the data set accelerating it's it's to make
things uh faster pft it's going to be for the fine tuning process that we're going to use i will
mention it later tl is a wrapper it can be used for supervised fine tuning or for reinforcement
learning from human feedbacks we have bits and bytes for quantization because we are not going
to use the model in full precision and 1db for reporting so we can have a nice dashboard where
we can track the progress of our model um once again we're going to use google collab i'm just
going to copy paste it uh from the previous um notebook so we have our secret token
current access okay um it's optional if you don't have a hugging face account once again
and here we're going to import a lot of libraries we can import os we're going to import torch we're
going to import the data set from data sets and from the transformers library we need to import
a lot of classes uh so auto model for causal lm auto tokenizer uh bits and bytes config auto
auto tokenizer uh training arguments and pipeline when we want to run it um when the model is
trained and then from pft we also need to import a few of them so lower config pft model and something
called prepare model for kbit training and the last one is the wrapper for supervised training
from the tira library called sft trainer um i'm going to let it here for for a second
and um we're going to talk about uh the different ways we can find during this model so we have
three ways there's the full fine tuning there's laura and there is uh q laura with full fine
tuning uh we're going to use um the the entire model so we're going to uh train all the weights
in the model which is very costly then we have laura which instead of training all the weights
we're just going to add some adapters in in some layers and we're going to only train these uh added
weights uh so this really reduces the cost of training the model because we are just going to
train like one percent two percent of the entire weights and finally we have q laura which is using
laura but with a model that has been quantized so uh in this case we're not going to use the model in
six-bit precision so with every weight in the model uh occupying 16 bits on the disc but instead
they're just going to be quantized into four bits so we can lose a bit of precision here but in the
end there are mechanisms to make it less impactful and we'll be able to get a really strong model using
q laura a bit of calculation here we have 16 gigabytes of VRAM with our GPU here you can see
it's 16 gigabytes and um lama 2 7B weights so we have seven billion parameters if they take up
two bytes it means that we're going to use 14 gigabytes so we are almost like using the entire
VRAM and in addition there are like other things there's an overhead due to optimizer stays gradients
for all activations so it's going to be challenging but we we can manage to fit it into only 16 gigabytes
of memory okay so now we're going to really delve into the code to function it um we are going to
reuse the news research model here like previously and we are going to give a name to our new model
so in this case i'm going to call it lama 2 7B and mini platypus
and we're going to reuse the dataset that we just created so it should be called mini platypus
and we're just going to use the train splits finally we have the tokenizer
so here we are going to use the tokenizer from the lama 2 model
and we're going to use the fast version of it we are going to do something that is
um some people hate it we don't have a padding token for lama and this is a really big problem
because we we have a dataset with different number of tokens for each row so we need to
pad it so they all have the same length right and there are different ways of doing it here i'm using
a token called the end of sentence token and this will have an impact on the generation of our model
this is what we're going to use here there are different ways of doing it this is definitely
not the best version of it if you want to learn more about it i linked an article from benjamin
mary about two other ways of doing it and this is what we should do but for the sake of simplicity
here i'm telling you about this problem but i'm still using the end of sentence token for this
fine-tuning then we are going to talk about the configuration of the culor so here bnb config
it's the bits and byte configuration the first thing that we want to do here is to load the model
using four bits otherwise it will not fit into the VRAM in order to do that we can specify the
quant type that we want to use in our case we want to use the nf4 format this is the format that was
introduced in the culor paper and we are going to use a compute type so this is how the weights
are stored using four bits and when we want to compute it's only it's going to use 16 bits so we
have more accuracy and we are also going to use something called double quantization so even the
quantization parameters are quantized it's it's to like really take even less space than without
using it then we have the lower configuration so on top of culor we also have the lower
configuration and in this case we have a bunch of diameters one of them is the alpha the alpha is
basically the strength of the adapter the impact it has on the the model because you can merge these
adapters in a very weak way so using very little weight or using a big weight 32 is a pretty big
weight but this is a quite standard value for this parameter we also have like the dropout because
when we add these adapters they have a little dropout so we have a 5 probability of skipping
these connections and finally we have a rank which is like the dimension of the the matrix that I use
here if you want to know more about law and like a minimalistic implementation of it I made this
notebook called nano law and this goes into in depth into like the theory behind it and we
will help you understand these parameters a bit better we do not want to take care of the bias so
we have the weights and the biases here we do not care about the biases and the task type in this
case is causal lm because we are auto aggressive and the target modules here we have a very long
list of target modules the target modules it's something that you can see here actually
and the attention lma attention thing it's basically like okay what which which modules do you want
to not train but add a law adapter to it and we are going to use a lot of them because
it's been shown to really improve performance so the more modules we have the more
parameters we are going to train but this is fine like we we can afford it with our limited
budget it's going to help us in the long run then we're going to load the model from pre-trained
and we're going to use the base model that we typed earlier the quantization config so we have
the bnb config here this is what we're going to use and finally the device map so here it's
we could also use auto but I'm going to use that it's going to automatically detect the device the
hardware that you have so in our case we'll detect the gpu and make sure that we're using the gpu for
training otherwise it's not going to work and finally we are going to call a function called
model for kb training this will cast the layer norm in fp32 so in more precision it will make
the output embedding layer require grads and add the upcasting to the mlhead to fp32 so what it
means is that it's going to take some layers some modules and make sure that we are using them with
the highest precision possible because it's been showed to really improve the performance too so
yeah there are some modules that we will not that do not really matter some of them matter
quite a lot and we want to be quite proactive on that and in the end this will help us build
the best model possible so if I oops I forgot to execute that um and then if I forgot to execute
that too oh just while you're running those in the bits um the data set sorry I just wanted to
refer to just like just because we're coming up to time um just we are going to over in a bit
before we get to before you all jump off for those of you who have to dash
as I was going to say we've got three webinars coming up next week so on Tuesday we've got
an introduction to snowflake code along coming along uh on Wednesday we've got a session on
using ai in robotics so if you're interested in agreeing on ai then that's uh something uh
definitely worth attending and then on Thursday we've got a session for best practices on putting
lmms into production so three great sessions please do go to datacamp.com slash webinars
sign up for those um we have got some great questions from the audience as well so I'm
hoping to get to those afterwards if you do have to jump that fine uh please do catch up on the
recording for everyone else um I hope you're okay hanging around for a minute and with that I will
dash off and let you uh get back to it Maxine yes sorry about that we should be over in like 10
minutes I underestimated the time uh it takes you to write the code um but um yeah here we have
lower configuration loading the model preparing the model for training uh we are currently
downloading the model here you can see the different modules in the lma attention
class and this those are the one that we target also in the lma mlp you can see the
hugging face implementation of it to to have more details about how it's actually implemented
um once it's done we have more uh boilerplate code uh to to type uh this one it's training
arguments uh so we have the training arguments here and what we want to do is to give like a bunch of
parameters to it so like where do I output the results we're going to specify uh directory for
that uh how many epochs we want to run the model on um here I'm going to put one but let's put four
or five uh basically between three and five it is pretty good for an amateur model at this size
there is the per device uh train batch size uh this will tell us like how many
um
yeah the number of batches that we're going to take for every every every step um
we have the gradient accumulation uh steps we're not going to use it here it's basically a
for loop inside of the um training uh so we don't have to add more um use more VRAM but in this case
it's going to be fine we won't need it we have the evaluation strategy it's not going to be very
useful here because we are not going to um evaluate this model um we just want to train it
and we're going to mention what evaluation looks like with uh uh these models a bit later
logging steps we want to log every step uh the optimizer that we're going to use is the
adam optimizer uh but a version that is paged in eight bits so it's going to lose
test memory uh the learning rate uh we are going to use uh this one there are different
learning rates that that what we can use um refer to the qloa because qloa really impacts
the learning rate the the model also really impact the learning rate that you want to use
we have the scheduler uh in this case we're going to use linear and warm up steps
like it won't be really useful here but we can say uh 10 uh to warm up the the optimizer
we want to report it to weights and biases and finally uh something that i'm going to put here but
uh remove this line uh for like for real training we're just going to stop after two steps otherwise
it will take like an hour to train the model but yeah you can just feel free to remove this step
if you want a real training so those are the training arguments uh then we need also to use
the fft trainer so the wrap i mentioned earlier and in this case we just specify the model the
training data set uh we don't have an eval data set so i'm just going to reuse the same one
pft config um we specified it um it wants to know the text field here so the instruction field
in our data set in our data set was called instruction um the max sequence length um
so we're gonna go for uh 512 uh you could say like yeah but we we put a threshold at 2k but
we don't have enough VRAM unfortunately uh it it will take like a lot of VRAM to to
to put everything into memory so we we're just gonna stop at uh 500 in this example and finally
we're going to give it the training arguments so this is what we have and when it's done we can
start the training and when the training is done uh we can even save the model uh using that
so we should have yeah the model has been downloaded here and now we are training the model
so this is a loss a training loss and evaluation loss from weights and biases and as you can see
it's a very nice way to tracking the the progress of the model we can see here the warm-up steps
where it's it's pretty bad and then it goes better and better uh you can see the training uh loss is
in blue so it's quite spiky it's a bit noisy the eval loss is in orange it's a lot less noisy
because it's less frequent and something that you can observe uh if you train it for like five epochs
is that the eval loss will go up instead of down uh normally uh traditionally in machine learning
this is a bad thing but with large average model it's been proven uh time and time again that it's
actually desirable and the best models um actually like overfit really a lot on the training data
and this is not a problem actually this makes them better um here as you can see your model
has already been trained for two steps so if you want you can add more steps you can remove it if
you want to train it on the entire data set it will take a while however uh we can check um
weights and biases here we won't see any the lot because we only have two steps uh but just to show
you uh here's our run and you can see here the global step uh train run time train loss of 1.2
so yeah this is what you would use if you run it on the entire data set
finally we can um use our model now that it's been it's been trained uh we can prompt it and say
what is a large language model uh we have to wrap it using the right uh chat template so with
instruction prompt and and then response
and we can use a a pipeline here uh from hugging face it's going to be pretty nice so model
tokenizer equal to tokenizer and we're going to restrict the generation in 128 and finally
we can get the result here and print it so i'm going to print the generated text it's an object
that returns um and we can do something fancy and remove the instruction part this is a question
that a lot of people ask me like why do i see the instruction uh in the generation in the generated
text you can just trim it basically this is how the hugging face object works but you can remove
it manually like this so now the train model is going to answer the question what is a large
language model and it will print um the answer here then we want to delete it okay we have the
answer so what is a neural network no the answer is a large language model is a type of artificial
intelligence model that is trained large amount of text data to generate human like text which is
pretty good actually um not thanks to our fine tuning because it was not intense enough but
it's a pretty good answer and then you can see that it keeps repeating like instruction response
instruction and this is because of our padding actually it's because we're using the end of
sentence token as a padding token so now it just doesn't stop and keeps talking so if you
don't want to have this behavior please use a different padding technique as mentioned previously
finally we want to remove a lot of things so this is specific to google collab we want to
collect all the model and the objects in the in the memory in the VRAM so we can merge it with the
so we can merge the base model with the adapter that we trained this is a piece of code that is
difficult to understand why do we need to call it twice honestly I do not know I just know that it
works when I do it and actually and sometimes it doesn't work so well worst case scenario you can
just like restart the the collab and and just execute this part of the code but here for the
sake of time I am going to copy paste the code so we are going to re reload the base model here
and we are going to also load the adapter so the cooler adapter that we
we created you can see it here this is our cooler adapter with adapter config adapter model
and this is what we want to to load hopefully it will work and and then we can push our model
to the hugging face hub when we do that we are going to push the model and the tokenizer and
we're using the hf token here this is optional of course and this will upload it to our
our hugging face account so here we are merging and unloading the model we are going to reload
the tokenizer not just in order to save it too and it's going to be uploaded okay so this is the
end of this session sorry it's a bit late but if you want to go further just know that you should
be able to reuse use this entire collab notebook with mistral 7b instead of flama 7b mistral 7b
is a better model but the name of this talk was fine-tuning llama 2 so I stuck to llama 2
but I would yeah I encourage you to try it out if you want a better fine-tuning tool I recommend
actual tool because these Google collab they're really nice to understand the theory behind everything
and be able to implement this fine-tuning process on your own but if you want to really
fine-tune state-of-the-art open source LLMs I recommend actual tool it's really a great tool
I've been using it a lot of people have been using it and this is quite easy to use so yeah
this is a good recommendation and then what you can do with this model is you can evaluate it
using a evaluation harness you can even get on the open LM leaderboard if you have a good model
or you can quantize it so you would make it easier to execute on consumer grade hardware
and you could use your fine-tune model on your own GPU so that's it for me it will take a while
for the model to be pushed to the hub because it's quite it's quite big but as you can see it's
been merged and everything is working correctly so I hope that you found it useful and if you
have any questions maybe now it's the time to ask the question to answer the questions
all right thank you Maxim that was fantastic a lot to unpack there I actually have like
a lot of questions for you but I think if I start asking questions we're going to go on
longer than an Ed Sheeran concert so I'm going to stick to audience questions instead
let's go with this one first from Prudine so Prudine is saying like maybe don't need to show
this but how do you go about querying tabular data so I mean this is very much focused on we've just
got a lot of text if it's tabular data what's the difference for tabular data I would not
recommend using LLMs because they've really made for for text there are some yeah actually I'm
wondering like I don't know maybe maybe we can't get into too much detail but is the standard like
if you've got just a text file or once if you've got like a pandas data frame of text
yeah this is a good question actually you could see it when we uploaded our data set to the
hiking phase hub this is kind of a data frame but it only has a text so if we go back to the
data set that we've dealt here you can see like it's basically data frame with instruction
output columns but it's not it's not tabular right it's it's really text yeah okay interesting
all right next question comes from Kiran saying okay so we've been doing this fine tuning on a GPU
do we also need a GPU at the point where we're hosting these things is it just the training bit
that's computationally intensive or is it also inference no the inference is also like very
intensive unfortunately and you definitely need well you need a GPU in general but in particular
if you use lama.cpp I can show it on my screen if you use lama.cpp you can use it on a CPU
you can see it sorry I'm gonna you can see it here it's me like running it and it runs on on a CPU
you'll have to compromise a little bit because like you need to lower the precision of the model so
it's smaller and faster to execute but this is something that you can do on the CPU
all right very good so um for anyone who's interested in lama.cpp I know I've got a
tutorial on that perhaps Reece can post a link to that in the moment next question
comes from Minfem so it looks like you you got some fancy co-pilot action or some sort of
autocomplete thing going on there in co-lab where does it what's that tool yeah it's a tool called
codium and yeah it works really well with Google collab as you could see I don't know if it learned
from my own code but it was like really accurate this time I spoke well when you're practicing
rehearsing the the tutorial you probably have the same thing a few times so yeah yeah good way to
train it yeah I mean they're training dataset now nice okay so codium's the thing uh next thing
comes from from Wei saying uh how important is parameter tuning during function I guess that's
like hyper parameter tuning yeah it's a really good question for some of the hyper parameters
going to be very important um and for some of them it's it's more like one percent two percent
gains for example everything here honestly if you stick to like traditional values uh it's it's
gonna make like not super meaningful improvements of course they're important because one percent
two percent it's good to have but they're not yeah that that important uh and there are other
yeah parameters that are a bit more important the learning rate is a really important one
and for this one yeah recommend checking the model that you want to use if you use mistral instead of
lambda two it's going to impact it if you use q lower lower or full fine tuning it's also going to
impact it all right um excellent next one is from bed can you show again how you show how you
load save models maybe we'll skip the using for text generation but if you just cover
loading save models I think that's useful okay so the you save the model by calling the the trainer
object and with the model and save pre-trained new model uh so this is uh yeah just just some
code you need to you know and and then it was the text generation right uh yes uh and in this case
I use the pipeline there are a lot of ways of like using them um it's not super pretty but uh it
doesn't take a lot of lines of code and yeah this is an object from hugging face from their library
which allows you to nicely um use the the text generation for instance okay oh I think the
question is about like once you've saved it how do you load that back okay uh basically just reusing
that and if you check actually um I've uh the model is uploaded on hugging face already and here
you have a usage um section where I describe how you all the code you need to use it so okay
see you're getting the whatever model type is from pre-trained uh pulling back off the hugging
face page all right nice yeah okay so uh next question from Arun saying can you see what percentage
of trainable parameters um we are reduced we're going to have to queue Laura so this is like how
many different I remember you saying queue Laura just changes some of the weights in the model so I
guess you want to know what percentage that is um it's an excellent question I uh I cannot show you
because I deleted the model type in trainer um before but yeah there's a command to do that
I recommend checking on google but yeah definitely you can see exactly like the percentage of
parameters the number of parameters that you're training using either lower or queue lower all
right excellent um and we've got so many more questions uh all right let's just do a couple more
um so
this next one all right okay it's always fine okay so Alexander asks how do you find
tune an LLM so it can extract JSON from differently format that's actually maybe a little bit
um specific but can you talk about how you apply it to like um a sort of you've got a CSV file or
an excel file of text how do you how I guess how do you standardize that data um yeah I I don't know
if it's really a task for an LLM um because there um um if it's just extracting I would say like why
do you want to use an LLM and not something else um other than that um there are different
frameworks like uh JSON former or LMQM even better LMQM let me show you if it's really about the
generation generating a popularly formatted JSON this is a really good uh framework to do it um
there are a lot of them but this one is currently the most popular one uh it's quite easy to use
I'm not sure if we'd answer the questions but I wouldn't use an LLM to extract this information
I would use it to generate a JSON uh and to generate this JSON this is the library I would use
okay uh so LMQL was that all right LMQL yeah LMQL all right that's worth looking into then
all right one very one very last question then since we're well over time anyway we're
we're past limits all right so um how can you improve the forms of LLMs you think basically
what's the what a llama index and lang chain and what's the difference between them um yeah so this
is um you have fine tuning and um lang chain and llama index they are more about like creating this
retrieval augmented generations so um fine tuning is is one way of customizing an LLM for your use
case and the RAG pipeline is another way of doing it so with lang chain and llama index you're going
to retrieve more context using some vector database or regular databases that you have
the difference between them I'm not going to delve into the details LLM index is is
there's less stuff but maybe more in depth than lang chain and I would recommend
actually implementing both approaches so fine tuning your LLM and using this fine
tune LLM with the RAG pipeline and this is where you'll get the best performance possible
all right fantastic we're gonna have to call it a day there I think I know there's more questions
so sorry to everyone in the audience if you didn't get to your question I just want to say thank you
again Maxim that was like incredibly informative and lots of new things that I think we need to
explore so brilliant thank you again thank you to recent moderating oh sorry gone Maxim
thank you Richie and thanks everyone for your patience I know it's been a lot but I hope that
you found it informative so yeah all right brilliant and yeah so thank you to everyone in
the audience who asked the question thank you to everyone who showed up today hope to see you all
again soon lots of exciting webinars coming up so goodbye have a great weekend
