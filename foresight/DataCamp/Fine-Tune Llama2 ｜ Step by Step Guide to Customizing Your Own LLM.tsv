start	end	text
0	7120	In this session we are going to cover two notebooks, one this is the first one dedicated to the
7120	13520	creation of the dataset and a second one dedicated to the fine-tuning of the model.
15120	22080	Here we're going to have a very big and friendly approach to this topic and then at the end of
22080	29840	this session I will present some resources to go beyond that and to be able to also like use
30080	37280	better tools and perform other things but in this session we're going to stick to like the basics
37280	43600	and try to really learn the theory behind it so we're also like more aware of what's going on when
43600	53760	we use automated tools. In this first notebook we're going to talk about datasets and how to
53760	59760	create a high quality dataset so I want to mention the fact that there are basically two datasets
60720	66160	several datasets we're interested in. The first one is instruction datasets where
66160	71360	inputs are instructions this is basically how you use chart gpt like write me something and
71360	77920	the model is supposed to write you something and not complete what you just said because this is
77920	84080	real completion and this is part of the pre-training of these models they pre-train on a lot of data
84160	90960	and the task is next token prediction so they're just here to predict what the next word is going
90960	98320	to be and this is nice but not really what we want to create assistance so instead we're going to
98320	104640	use supervised fine-tuning to be able to turn a base model into a useful chat model here.
105760	111520	There are also preference datasets I want to mention it briefly this is reinforcement
111520	118560	learning from human feedback this is often used after the supervised fine-tuning process and in
118560	125280	these datasets you will find different answers and some kind of ranking of these answers to say
125280	130240	hey this one is more useful than the other one we're going to mention it during the fine-tuning
130240	135920	process in the second notebook and finally there are other types of dataset it can be sentence
135920	141680	classification it can be code dataset where you have a fill in the middle objective where you
141680	148880	want to fill in code that has some context so before your cursor and after your cursor and
148880	154160	this is where you want the code to be but we're not going to talk about this it was just a general
154160	160080	overview of the kind of dataset you can expect in this space so as mentioned here we're just going
160160	164800	to use supervised fine-tuning so with an instruction dataset and we're going to be a lot
164800	172400	owned by filtering an existing dataset so the one that we're going to use today is the open
172400	178160	platypus dataset it's a collection of different datasets actually this is already a dataset that
178160	184160	has been filtered using all the dataset but we're going to do it even more we can check what it looks
184160	192160	like on the hugging face page here so here you have the instructions for example the board game
192160	197760	speeder and divider into three parts blah blah blah and the output that is expected by the model
197760	202720	so this is basically what your model is going to learn during the supervised fine-tuning process
202720	209920	it's going to learn to output this answer when it has this instruction and we repeat it a lot of
209920	215120	times and at some point the model is pretty good at understanding what it needs to do so it needs
215120	223280	to provide a useful answer for this instruction you have more information about the platypus dataset
223280	230160	and there's also a really nice paper you can find here about how they did it and how they trained
230160	235760	their models called platypus so an interesting read if you want to know more about it
236560	243120	so first of all let's start with the code we're going to need to install some libraries
243760	251600	so here we're going to keep install a bunch of libraries we're going to install datasets the
251600	258480	library from hugging face you handle this also transformers very useful sentence transformers
258480	266800	too because we are going to use sentence transformers to create embeddings of our data we're going to
266800	275680	see why in a few minutes and finally a vector database from facebook face gpu here you can see
275680	283600	the runtime so make sure you have a t4 gpu or more if you can afford it but the entire code should
283600	291520	run using a free tier google collab so with a t4 gpu for the sake of this exercise i'm using a v100
291520	296240	with higher ram because it's just going to be a bit faster and we won't have to stay at the screen
296240	303120	for 30 minutes the second step that is really useful so this is the first time i tried to use it
303120	308320	please bear with me this is quite a new feature in google collab but now we have a secret tab here
308320	314720	on the left and as you can see here you can add a new secret and you can give it a name
315280	321680	hugging face and a value so you can retrieve the value using this link if you have a hugging face
321680	329280	account so here i collab i can copy it here i can write the hugging face here and copy paste the
329280	338000	value here so and give access to the notebook um so this should be a really clean way of
338000	342480	managing your secrets because now they share across all your notebooks and they won't going to be
343200	352960	shared with people um other than in your account um to use it to jump in um we just got a question
352960	357440	from the audience that i'm not sure what the answer is i don't know how widespread this problem is
358400	367040	but do you know what an insure file accessible error message might be oh uh sorry i thought
367040	376160	that we tested that uh let me let me just uh try out the links that are used okay
377120	388000	uh anyone with the link should be able to access them actually uh some can some people
388000	392160	confirm that you can access it which you can you access these notebooks uh i can access
392160	396720	the notebooks one thing might be the case like you do need to be logged into google i think
396720	401920	in order to access colab so if you're not logged into google then you probably need to do that
402880	406720	all right we'll continue for now um if anyone else has any problems please do let us know in
406720	413520	the chat of the comments uh yes please uh oh uh james just might be a company blocking it for
413520	419840	security reasons yeah if you can't access any google products from uh your corporate uh laptop
419840	425120	then yeah you probably have to find a different network unfortunately or just watch for now
425120	431440	follow along uh when you get the recording yeah exactly uh sorry about that google colab can be
431920	440000	a bit difficult to use in some in some context um here we're going to uh import our secret
440000	447920	using uh google colab use the data use the data get so now the hf token um has the correct value
447920	452400	and this is what we're going to use if we need it in the when we need it in the rest of this
452400	460160	collab this is optional uh we are only going to need it when we want to upload uh the data set
460160	465920	so if you don't have a hugging face account you don't need to create one i recommend it because
465920	471360	it's nicer and you'll be able to upload your own data set in your own account but if not it's okay
472080	475760	i will upload it in my account and you'll be able to reuse it from this account
476880	482160	so now we have the hugging face token and the first thing that we want to do is to
482880	492240	load the data set that we want to use so in this case it's the data set that was mentioned here so
492240	501760	open open platypus we can copy paste it from here and it should load the data set download it
501760	507680	and load it and we're going to see all the different columns inside of the data set so
508640	516560	input output instruction and data source uh that should be correct here and we have the
516560	527520	number of rows so almost to 25k rows um so we can see a bit more about it we're going to
528480	535120	read it as a bandit's data set and we can even with google collab convert it into an interactive
535120	540960	table it's a better way of looking at it unfortunately it can take some time for google
540960	547760	collab to convert it so we're going to see but yeah it's nice to see that we have the instruction
547760	552720	and we have the output and this is all that matters here we're going to explore a bit more
552720	559840	about instruction the output if it was a real data set real raw data set what we want to do at this
559840	567120	point is really read not every line because it's too too too much but a lot of these lines
567120	573280	and be able to get a good understanding of what's in this data set and what we expect and also like
573280	580400	just clean it like if there are samples that are not high quality that have like bad english that are
580400	586720	just plain wrong we want to remove them this is very important we want to create the best
587280	594320	data set possible so it means filtering out a lot of samples okay so now we have this data set we
594320	603440	can read everything here this is a lot of data so we won't do it in this case it could take too
603440	613760	much time but now something that we can do let me show you once again the the code here so load
613760	620560	data set and then the name of the data and what we are going to do here is that we're going to
620560	630640	use the transformers library to import the tokenizer this will convert the row text into tokens
631200	640000	then we will import the matplotlib library to plot the results and also the cborn library
640000	650080	for the same reasons first we want to import the tokenizer so in our case we want the tokenizer not
650080	655280	from but as suggested here but from lamatu because this is the model that we want to use
656400	665840	so we're going to use a new research version of lamatu and not the official one from meta
665840	671440	why that it's because if you don't have a hugging face account you will not be able to access the
672080	678240	official version of it so news research just re-uploaded the entire thing so now we have
678240	685680	the tokenizer we can test it here it's going to download it once again from hugging face and
685680	694320	and then we're going to print it once it's done here you can sit with all the special tokens
694320	703200	unknown etc when it's done we want to use it to tokenize our instructions here and also outputs
703200	713040	here so the way that we're going to do it we're going to create a table an area called instruction
713120	723920	token counts it's a bit of a ghost but and we're going to use the tokenizer to tokenize
724560	734880	every sample in our data set this is not correct actually this is why you should not always trust
735440	751600	coder lambs for example in our data set train and close it so here in this table we should get
751600	757440	like the token counts for every instruction and we're going to repeat this process with the
758240	766320	output so pretty much the same thing here and we want the output and finally we want to
766320	775600	combine the instruction in the output because this is like the entire data set and so instruction
775680	788080	plus output and in this case we want to call it combine token counts and here we're going to merge
788080	803520	these two tables for instruction outputs in zip etc and we should get like the the sum of all the
804160	812960	tokens here in both instruction outputs now that is done we are going to do a little function to
812960	820640	plot the distribution of our token so we can have an easy visualization of it so here I'm going to
820720	832800	use sns set style white grid and slowly but surely get everything here
838000	844640	actually I'm sorry but I'm going to copy paste it so it's a bit faster to do it
845280	852640	and now we can plot the distribution for every instruction output and combine token count
853520	862480	so this is a very simple plot and we can call it using plot distribution and here we're going to
862480	868640	first use instruction token counts and we're going to see the distribution of token counts
869280	872000	for instruction on A
880880	887040	this part takes a bit of time unfortunately because yeah it needs to tokenize the entire data set
890320	897200	but once you've done it you can basically comment it it's okay and we're going to repeat this process
898720	905600	with the output token counts so this time it's for output only and finally the
907040	916400	combine token counts so here it's for instruction plus output here I'm going to
916400	926240	comment it so we get faster results and now we have a distribution for all of our data
926880	934400	so here you can see that we have approximately like the the minis is around here so around
934960	942080	500 tokens there's a long tail distribution goes up to like 5000 tokens why is it important
942080	948880	why does it matter it's because these models they have a certain context window and if it's
948880	955440	because beyond this context window it's not going to be very helpful so it's important to know like
955440	961680	the the number of tokens in our data set we also maybe want to sample more from
963360	967440	samples that have more tokens because they're going to be more informative than others
968160	975840	but we'll see about that basically here we can see okay I'm going to put a certain threshold
975840	983600	for this data set at 2k tokens the max context size for lambda 2 is actually 4k but
983600	990800	this is just an example to show how we can just set a threshold so here we want to filter out
990800	1004720	rows with more than 2000 samples so one way of doing it is to say if I count in numeric combined
1005680	1020400	token counts if count okay so here we're going to retrieve the index of every sample where the
1020400	1030880	count in combined token counts is lower than 2k and now we can print how many of this we have
1034960	1037520	okay and if we print
1040880	1048960	length of the data set in train and we are going to remove the length of the valid indices
1048960	1055360	so we see how many we remove okay we only remove 31 of them but that's fine you can have a more
1055360	1061120	aggressive threshold if you want in this case yeah we're going to remove just a few of them as you
1061120	1068320	can see here then we're going to extract the valid rows based on the indices so here we need to
1069040	1076880	take the data set train and we're going to use the select method to only get the valid indices
1078480	1088800	and then we're going to get the token counts so for the valid rows so we can also plot this
1088800	1096560	distribution here and we plot the distribution of the token counts after filtering exactly like that
1101440	1108320	okay there's an issue here because I executed the code twice I shouldn't have
1109040	1114160	but that's fine if you execute it once it should be okay now it's already filtered which is why
1114160	1120560	it's grid in there and here you see that we have a very different plot because all this
1120560	1129120	right part has been filtered out another thing that we can do is near the duplication using
1129120	1136560	embeddings which is why we install the sentient transformers library and what we want to do here
1136560	1147120	is we want to embed every row every sample from our data set so here we want to translate that into
1147120	1154080	a vector we call an embedding and using an embedding model how to choose the best embedding model
1154080	1161680	it's often it's a popular question one way of answering it is looking at the MTEB the board
1161760	1168320	on a hugging face this is where you can see like a competition with all the embedding models on
1169440	1176080	various tasks it's funny because since I've made this screenshot there are new ones on top of it
1176960	1184720	the one that we're going to use here is the GTE base embedding model it's a it's a really good
1184720	1190000	model it's not the best model but it's going to be faster than other options which is why we're going
1190000	1197600	to use it here and we're going to use these embeddings to then calculate the similarity between
1197600	1203360	them and when they're too similar we're just going to filter them out so how are we going to do it
1203360	1210320	we're going to use the sentient transformer library and we import the sentient transformer
1211040	1218080	class we're also going to import face the vector database from facebook it's not the best vector
1218080	1223760	database but it's very simple it's very minimalistic which is why I used it in this example from
1223760	1232960	data set we are going to use data set and data set dict to be a bit fancy we're also going to use
1232960	1242560	tqdm to have a nice loading bar and finally number for some operations so here we're going to
1242560	1249680	really create the code in one function to do everything so we are going to pass it a data set
1250320	1255840	we are going to pass it the name of the embedding model we want to use and we're going to pass it
1255840	1263920	threshold for example 95 percent it means that when it's 95 percent as similar as another
1263920	1270960	embedding it's fishy and we probably do not want to to use it we probably want to filter out this
1270960	1279120	model so as a sentence transformer we're going to use the model that we we pass this argument
1280400	1290560	for the output we are going to use all the example in the data set so what we want to filter
1291360	1296400	filter out here are just the outputs are not the instructions we're fine if we have similar
1296400	1300880	instructions we just do not want similar outputs because this is what the model is going to be
1301040	1311360	trained on then we are going to say that we are converting text to embeddings we are going to
1312720	1321360	use the sentence model and encode the outputs that we have and we can even show a progress bar
1322320	1325040	to be fancy bar
1327760	1334720	then we're going to get the dimension of our embeddings so you can see in the
1335680	1343440	little boards some of them they have like 1024 some of them they have like 768 basically yeah
1343440	1349520	they have different dimensions take that into account we are going to create our index using
1350080	1358080	the vector database so it's going to be a flat ip index in this case
1359680	1369840	and we need to normalize our embeddings the face already has a function to do it but i do not trust
1369840	1374720	it that much so we're going to do it on our own because i had a bad experience with it
1375680	1382160	and i think it's going to be better that way so here we're using numpy to normalize it
1383200	1390080	using the norm to just take the norm to normalize the entire embedding space
1390640	1395280	and then we're going to add it to our index as normalized embeddings
1396640	1402320	then we're going to say we are filtering out let's say near duplicates
1402640	1411440	and in this part of the code we want to use the index search so we have
1413360	1419520	the normalized embedding we just put and here we are going to say k equal
1420400	1426320	2 so we are going to return at most two vectors we don't need more in this case
1427280	1434640	and we're going to create a list of the samples we want to keep call it to keep
1435360	1442720	and this is the main loop finally range length yeah it's fine
1445120	1450320	and we're going to do something nice for the QDM so we have a nice loading bar
1451200	1465120	and what we want here is if the dimension is so this if this is we're going to return here the
1465120	1472480	similarity between these embeddings and if this is this if the cosine similarity is
1472480	1478400	below the threshold we are going to keep it so we are going to add it to the to keep
1479360	1492320	happened and then we have i yes an index index and then we can create data set trains and we
1492320	1498960	are going to use the select method from the data set object to only keep these indexes
1500640	1507520	then we are going to return it as a data set dict this is not the most elegant way of doing it but
1508240	1514720	it's going to be fine for the purpose of this exercise and then we can call our function so
1515680	1522640	the duplicate data set and we are going to pass the data set and we're going to pass
1523840	1530400	the embedding model we want to use so in this case as I mentioned it's going to be the gte large
1531200	1540000	and we can just copy paste it here and as a threshold I'm going to use 0.95 be careful if
1540000	1547120	you switch the embedding model you won't have the same distribution of cosine similarity so some
1547120	1553840	models to get the same results you're going to need like 85 others you can might need 99.9
1554640	1562560	it really depends on the embedding model that you use it should be fine so now we can convert
1562560	1570560	the entire data set into embeddings here we are downloading the embedding model it's not a big
1570560	1577840	model which is why it's it's pretty fast the long part is actually comparing the embeddings
1578720	1585520	I should mention why we're doing it using a vector database instead of a for loop I've tried to do a
1585520	1591120	very minimalistic version using two for loop so we would compare every embedding to all the other
1591120	1598320	embeddings but it took a very long time so this is why we're using a vector database here to be
1598320	1605120	more efficient to be used by the computations and and get the results basically just faster because
1605120	1611280	otherwise it would take like two hours it was really really too long unfortunately so here we
1611280	1618880	still downloading the models and then with a v100 with high ram it should take about three to four
1618880	1626720	minutes to get all the embeddings and to filter out the data set in the meantime we can continue
1626720	1634640	I'm just going to show the code here because I don't know if you can really see it if you had time
1635280	1641360	to see the code this part might be a bit confusing but I don't want to delve too deep
1641360	1649920	to the details of the face vector database okay so now you see that it's converting the text to
1649920	1656000	embeddings oh it's going to take much longer than last time I've tried unfortunately but it's okay
1656000	1666160	like we we can stop it or come back later to finish it what we want to see when we have this
1666160	1675680	dedupe data set is the number of samples that were filtered out so we can print the length of the
1676640	1689040	original data set we can print the length of the dedupe data set and we can even print the number
1689040	1695440	of samples that were removed so in this case the length of the original data set minus the
1695440	1703920	length of the dedupe data set and this will tell us that how many rows we we removed and last time
1703920	1715440	you can see it later on the solution notebook it's about 8 000 samples one thing that we can do
1716000	1723200	when that part is over is topk sampling so in this case we still have too many samples because if
1723200	1731120	we remove 8 000 samples we're still going to have 20 no we're still going to have 16 k samples
1731200	1736160	maybe this is too much for what we want to do so we can randomly sample
1739200	1746400	some some rows in order to do that we're going to create a new function called topk
1747360	1755840	rows we are going to use a data set token counts and k to know how many we want to have
1756480	1764000	we're going to sort the indices because we can sort it by descending token count
1764000	1770000	and get the topk indices in this case so it's going to be sorted range
1772720	1784480	length token counts and here we are going to use going to use lambda i token counts
1784480	1788160	reverse is equal to true so here we should get
1791440	1799120	everything that we need so the token counts and get all the data sets with most samples first
1799920	1807440	and then topk indices we are going to just keep those those topk
1808400	1814000	yeah and i just talked about randomly doing it but it's not true we're just getting
1814000	1821760	like the the samples with the most tokens sorry about that and then we can create topk data
1821760	1830800	and here we have instruction where we want data sets that are in the topk indices
1833520	1836080	and we're going to do the same thing with the output
1837760	1844080	you could do something similar with the select method but yeah this time this time i want to
1844640	1851120	to be clear about what's going on so we have a full loop to select all the samples that were
1851120	1858160	in the sorted indices here and we are not going to keep like 1000 of them and we are going to do
1858960	1868560	the same thing with output and finally we can return our data sets from the dictionary that we
1868560	1880720	did topk data so this is our function but in order to call it we are going to need to have
1880720	1887280	the new token counts because here we filtered out a lot of samples so we can just copy paste
1887280	1893440	what we did at the beginning here get the instruction token counts the output token
1893440	1900640	counts the combined token counts and this is what we will use when we want to call this function
1901200	1911600	so let's have a k of 1000 and the topk data set will be get topk rows data set we're going to
1911600	1917280	use it with the combined counts and the k of 1000 so this is how we're going to call the function
1918320	1929520	and finally we are going to save it as a dictionary like yeah data set dict as they call it
1931040	1935840	to make sure that we still have like a train split but not not very important
1936240	1944160	after we've done that we can once again re-compute all of the token counts
1946240	1949600	and the plot actually like also plot the distribution
1952000	1958480	so this is just to see what's the new distribution after after filtering after topk sampling
1959440	1966800	now how it looks like and yeah we'll see we'll see in a few minutes
1968640	1971520	when this is done and we can see the
1976160	1985920	the distribution we can just also see the samples themselves to see like with pandas
1986640	1993520	how it looks like like we've done at the very beginning of this notebook here and we'll see
1993520	2000640	like how many samples remain and finally I want to mention chat templates so there's a need to
2000640	2005680	define a chat template if you want to use your large language model as a chat bot there are
2005680	2012000	different ways of doing it here's a way of doing it we have a raw user content either
2012960	2015840	raw assistant so this is more like the raw data
2017920	2024000	this is a format that you can use just user two points and then the message then assistant
2024000	2030400	two point nice nice to meet you in the case of flamato you have this particular template so
2030400	2033920	you have this token s then you have the instruction you have space
2036240	2040160	not not not the instruction it's it's another token for instruction then you have
2041120	2046000	a sys you have the system prompt you have here the user prompt and finally the model answer
2046000	2051600	it's quite a difficult template you we don't need to use it to function our lamato model
2051600	2056800	because we're functioning the base model and this chat template is only used in the chat version
2056800	2064160	of lamato this is not the one that we use here I wanted to mention the chat ml template from open
2064240	2070480	ai it looks like this it's the most popular and standardized one you can see it in a lot of
2070480	2075200	state-of-the-art open source models we're not going to use this one because it requires adding
2075200	2081360	tokens it's more difficult so the one that we're going to use is going to be quite simple we're
2081360	2091040	going to create a function called chat template about it with an example and in this example we're
2091040	2102640	going to format we format the instruction and here we're going to use this instruction then
2102640	2114080	break line then we can finally put the original instruction and break line break line and here
2114080	2121440	we can put like output or response and go for response and another break line why this one
2121440	2126720	honestly there's no good reason you could imagine a lot of different prompt templates but it's going
2126720	2133920	to be nice to see that the function model will follow this prompt template finally we can return
2134000	2141280	example and we map that using the map method from the data set object and this will
2143840	2148640	yeah this will change all of our instructions so they can follow this template we're going to
2148640	2158880	visualize it when okay it's done let's go back a bit earlier so we managed to remove
2158880	2167840	sorry a bit earlier so yeah we filtered everything that we wanted to filter we filtered out like
2168400	2175840	8k samples like I mentioned previously then there's a top case sampling where we said we only want to
2176480	2186240	keep the top 1000 samples in terms of token counts so the one with the most tokens and you can see
2186240	2193360	here the distribution of token counts for instruction only for token counts and finally
2194160	2200000	the distribution of token counts for instruction for output you can see we don't have samples with
2200000	2206880	less than 1000 samples thanks to the top case sampling yeah it should make sense hopefully
2206880	2212320	so here we have 1000 samples with a lot of tokens and they should be high quality because
2212320	2217520	they're not close to each other we need to duplicate them so it means that
2217520	2223680	they should be pretty far away from each other here you can see all the 1000 rows once again
2223680	2229040	you can click it here if you want to have a good overview of the samples that we that were selected
2229760	2236400	and now they should follow our chat template so just let's check if if this is correct
2237040	2244880	and here you can see the instruction let's click it here we really have like the instruction
2244880	2253120	and the response as mentioned here so this is working as intended instruction response and
2253120	2262960	here is the response that we want the model to follow all right so this is done and the final
2262960	2268960	thing that we can do here this is optional this is if you have a hugging phase hub count if you
2270560	2277040	put the value here of your secrets then you can just push the data set to the hugging phase hub
2277920	2278400	like this
2282240	2290480	and we specify the token here i'm calling it mini platpus you can call it however you want
2291040	2300320	um and this is going to upload it and you can even check if it's correctly uploaded if I go
2300320	2306240	to my hugging phase account and I check my data set this one was uploaded dated less than one
2306240	2313840	minute ago and here you can see our entire data set uh cool so we have everything
2314160	2322800	now uh and we're ready to go to fine tuning uh I hope it was it was clear uh and if not I hope
2322800	2329840	that the solution notebook will help you uh to to to create your own data sets to go further beyond
2329840	2336160	that you can create synthetic data using um 24 it's something that is used quite a lot and it
2336160	2341760	creates uh really good uh data sets so it is something that you can play with and otherwise
2341760	2348080	it's really like a manual reviewing you can import this data set in google sheets and really
2348080	2355440	manually review every row possible maybe create some regex to automate the process a little bit
2355440	2362240	but this is a very time consuming process but it's also like very nice because then people
2362240	2369760	can reuse your data sets if you share them but now let's go to the fine tuning uh notebook
2370640	2377680	you should also have it uh let me uh check the the chat uh okay you have the solution notebook
2378560	2386800	cool um so here you have um lama 2 and we're going to delve deep into the fine tuning process so
2386800	2390880	as mentioned previously there are two ways of fine tuning these models this supervised fine
2390880	2397200	tuning this is what we're gonna do uh so we're gonna tune it on a set of instructions and responses
2397200	2404800	it's going to help the model focus where we want um so to be helpful to follow the chat template too
2405360	2411040	and there's also the reinforcement learning from human feedback where we want the model to
2411040	2415920	maximize your word signal i'm not going to delve into that uh there are a lot of good articles
2415920	2422560	about it uh it's able to capture more complex uh preferences but it's also like more difficult to
2422560	2432720	implement and in practice uh most um if not all this this year but except this year nearly all
2432720	2438160	the state-of-the-art open source LLMs just use supervised fine tuning so yeah something to keep
2438160	2445440	in mind um and once again there's an example uh from a few months ago now the NEMA paper that
2445440	2451120	shows that only 1000 high quality samples can really make the difference and and get very far
2451120	2457840	um in this case when you have a 65 billion models um and i want to mention the open LM
2457840	2464320	leaderboard you might be familiar with it um but this is quite useful to see uh what are the best
2464320	2471520	models so currently you have like this non-Lama model um i wanted to uh yeah show this Godzilla
2471600	2481120	2 7b model because um it's using a Lama 2 70b and um uh i saw that it was using my data set so
2481120	2486320	when i was telling you like yeah it's nice to also share your data set you see like sometimes it can
2486320	2492400	be reused by other people without uh you knowing anything about it but i'm glad this one was useful
2492960	2500800	um so what we're going to do here is um as previously we are going to start by installing
2500800	2508000	all the libraries that we want so in this case we're going to go pip install queue and we're
2508000	2513920	going to update because uh Google collab already has some of these libraries but we want to use
2513920	2521760	really like the latest version available in this case bits and bytes and 1db transformers
2521760	2531120	and the hugging face oops i'm going to disconnect this session okay uh once again you can use a
2531120	2536880	t4 gpu for the entire uh notebook here i'm just going to use the v100 because it's going to be
2536880	2543280	faster transformers for the transformers data set for the data set accelerating it's it's to make
2543280	2549280	things uh faster pft it's going to be for the fine tuning process that we're going to use i will
2549280	2556960	mention it later tl is a wrapper it can be used for supervised fine tuning or for reinforcement
2556960	2561840	learning from human feedbacks we have bits and bytes for quantization because we are not going
2561840	2567600	to use the model in full precision and 1db for reporting so we can have a nice dashboard where
2567600	2575120	we can track the progress of our model um once again we're going to use google collab i'm just
2575120	2583840	going to copy paste it uh from the previous um notebook so we have our secret token
2586240	2593040	current access okay um it's optional if you don't have a hugging face account once again
2593040	2598560	and here we're going to import a lot of libraries we can import os we're going to import torch we're
2598560	2606000	going to import the data set from data sets and from the transformers library we need to import
2606000	2621280	a lot of classes uh so auto model for causal lm auto tokenizer uh bits and bytes config auto
2621920	2633280	auto tokenizer uh training arguments and pipeline when we want to run it um when the model is
2633280	2644480	trained and then from pft we also need to import a few of them so lower config pft model and something
2644480	2657280	called prepare model for kbit training and the last one is the wrapper for supervised training
2658160	2667600	from the tira library called sft trainer um i'm going to let it here for for a second
2668240	2673280	and um we're going to talk about uh the different ways we can find during this model so we have
2673280	2679440	three ways there's the full fine tuning there's laura and there is uh q laura with full fine
2679440	2689680	tuning uh we're going to use um the the entire model so we're going to uh train all the weights
2689680	2696080	in the model which is very costly then we have laura which instead of training all the weights
2696080	2702400	we're just going to add some adapters in in some layers and we're going to only train these uh added
2702400	2709200	weights uh so this really reduces the cost of training the model because we are just going to
2709200	2716240	train like one percent two percent of the entire weights and finally we have q laura which is using
2716240	2724480	laura but with a model that has been quantized so uh in this case we're not going to use the model in
2724480	2734480	six-bit precision so with every weight in the model uh occupying 16 bits on the disc but instead
2734480	2741680	they're just going to be quantized into four bits so we can lose a bit of precision here but in the
2741680	2748560	end there are mechanisms to make it less impactful and we'll be able to get a really strong model using
2748560	2758400	q laura a bit of calculation here we have 16 gigabytes of VRAM with our GPU here you can see
2758400	2769840	it's 16 gigabytes and um lama 2 7B weights so we have seven billion parameters if they take up
2770480	2778400	two bytes it means that we're going to use 14 gigabytes so we are almost like using the entire
2779120	2784400	VRAM and in addition there are like other things there's an overhead due to optimizer stays gradients
2784400	2792560	for all activations so it's going to be challenging but we we can manage to fit it into only 16 gigabytes
2792560	2803520	of memory okay so now we're going to really delve into the code to function it um we are going to
2803520	2813920	reuse the news research model here like previously and we are going to give a name to our new model
2815040	2821680	so in this case i'm going to call it lama 2 7B and mini platypus
2824160	2832080	and we're going to reuse the dataset that we just created so it should be called mini platypus
2833040	2838720	and we're just going to use the train splits finally we have the tokenizer
2839520	2846320	so here we are going to use the tokenizer from the lama 2 model
2847840	2854880	and we're going to use the fast version of it we are going to do something that is
2855280	2865040	um some people hate it we don't have a padding token for lama and this is a really big problem
2865040	2874880	because we we have a dataset with different number of tokens for each row so we need to
2874880	2881600	pad it so they all have the same length right and there are different ways of doing it here i'm using
2881600	2887200	a token called the end of sentence token and this will have an impact on the generation of our model
2889040	2893840	this is what we're going to use here there are different ways of doing it this is definitely
2893840	2900880	not the best version of it if you want to learn more about it i linked an article from benjamin
2900880	2907520	mary about two other ways of doing it and this is what we should do but for the sake of simplicity
2907520	2913280	here i'm telling you about this problem but i'm still using the end of sentence token for this
2914080	2925360	fine-tuning then we are going to talk about the configuration of the culor so here bnb config
2925920	2932400	it's the bits and byte configuration the first thing that we want to do here is to load the model
2932400	2941520	using four bits otherwise it will not fit into the VRAM in order to do that we can specify the
2941520	2947760	quant type that we want to use in our case we want to use the nf4 format this is the format that was
2949120	2961360	introduced in the culor paper and we are going to use a compute type so this is how the weights
2961360	2967200	are stored using four bits and when we want to compute it's only it's going to use 16 bits so we
2967200	2977920	have more accuracy and we are also going to use something called double quantization so even the
2977920	2986800	quantization parameters are quantized it's it's to like really take even less space than without
2986880	2993200	using it then we have the lower configuration so on top of culor we also have the lower
2993200	3000480	configuration and in this case we have a bunch of diameters one of them is the alpha the alpha is
3000480	3008800	basically the strength of the adapter the impact it has on the the model because you can merge these
3008800	3016080	adapters in a very weak way so using very little weight or using a big weight 32 is a pretty big
3016080	3022640	weight but this is a quite standard value for this parameter we also have like the dropout because
3022640	3028640	when we add these adapters they have a little dropout so we have a 5 probability of skipping
3028640	3036560	these connections and finally we have a rank which is like the dimension of the the matrix that I use
3036560	3045120	here if you want to know more about law and like a minimalistic implementation of it I made this
3045120	3053120	notebook called nano law and this goes into in depth into like the theory behind it and we
3053120	3059440	will help you understand these parameters a bit better we do not want to take care of the bias so
3059440	3064880	we have the weights and the biases here we do not care about the biases and the task type in this
3064880	3073520	case is causal lm because we are auto aggressive and the target modules here we have a very long
3073520	3080720	list of target modules the target modules it's something that you can see here actually
3080720	3090560	and the attention lma attention thing it's basically like okay what which which modules do you want
3092080	3103040	to not train but add a law adapter to it and we are going to use a lot of them because
3103040	3109600	it's been shown to really improve performance so the more modules we have the more
3111040	3118240	parameters we are going to train but this is fine like we we can afford it with our limited
3118240	3128880	budget it's going to help us in the long run then we're going to load the model from pre-trained
3129760	3140480	and we're going to use the base model that we typed earlier the quantization config so we have
3140480	3146720	the bnb config here this is what we're going to use and finally the device map so here it's
3147680	3156000	we could also use auto but I'm going to use that it's going to automatically detect the device the
3156000	3160960	hardware that you have so in our case we'll detect the gpu and make sure that we're using the gpu for
3160960	3167440	training otherwise it's not going to work and finally we are going to call a function called
3168080	3177120	model for kb training this will cast the layer norm in fp32 so in more precision it will make
3177120	3185520	the output embedding layer require grads and add the upcasting to the mlhead to fp32 so what it
3185520	3191760	means is that it's going to take some layers some modules and make sure that we are using them with
3191760	3197600	the highest precision possible because it's been showed to really improve the performance too so
3197600	3203840	yeah there are some modules that we will not that do not really matter some of them matter
3203840	3211520	quite a lot and we want to be quite proactive on that and in the end this will help us build
3211520	3222080	the best model possible so if I oops I forgot to execute that um and then if I forgot to execute
3222080	3228560	that too oh just while you're running those in the bits um the data set sorry I just wanted to
3228560	3233120	refer to just like just because we're coming up to time um just we are going to over in a bit
3233920	3237360	before we get to before you all jump off for those of you who have to dash
3237360	3243040	as I was going to say we've got three webinars coming up next week so on Tuesday we've got
3243040	3249520	an introduction to snowflake code along coming along uh on Wednesday we've got a session on
3249520	3254560	using ai in robotics so if you're interested in agreeing on ai then that's uh something uh
3254560	3259040	definitely worth attending and then on Thursday we've got a session for best practices on putting
3259040	3266160	lmms into production so three great sessions please do go to datacamp.com slash webinars
3266160	3270800	sign up for those um we have got some great questions from the audience as well so I'm
3270800	3275360	hoping to get to those afterwards if you do have to jump that fine uh please do catch up on the
3275360	3280320	recording for everyone else um I hope you're okay hanging around for a minute and with that I will
3280320	3285360	dash off and let you uh get back to it Maxine yes sorry about that we should be over in like 10
3285360	3293680	minutes I underestimated the time uh it takes you to write the code um but um yeah here we have
3294240	3298960	lower configuration loading the model preparing the model for training uh we are currently
3300240	3304880	downloading the model here you can see the different modules in the lma attention
3306080	3312400	class and this those are the one that we target also in the lma mlp you can see the
3312400	3318080	hugging face implementation of it to to have more details about how it's actually implemented
3318880	3326240	um once it's done we have more uh boilerplate code uh to to type uh this one it's training
3326240	3333600	arguments uh so we have the training arguments here and what we want to do is to give like a bunch of
3336320	3343120	parameters to it so like where do I output the results we're going to specify uh directory for
3343120	3350640	that uh how many epochs we want to run the model on um here I'm going to put one but let's put four
3350640	3357040	or five uh basically between three and five it is pretty good for an amateur model at this size
3358000	3364240	there is the per device uh train batch size uh this will tell us like how many
3364640	3365840	um
3368160	3373680	yeah the number of batches that we're going to take for every every every step um
3374400	3379600	we have the gradient accumulation uh steps we're not going to use it here it's basically a
3379600	3388240	for loop inside of the um training uh so we don't have to add more um use more VRAM but in this case
3388320	3394320	it's going to be fine we won't need it we have the evaluation strategy it's not going to be very
3394320	3400960	useful here because we are not going to um evaluate this model um we just want to train it
3401600	3406960	and we're going to mention what evaluation looks like with uh uh these models a bit later
3407840	3413600	logging steps we want to log every step uh the optimizer that we're going to use is the
3414320	3420720	adam optimizer uh but a version that is paged in eight bits so it's going to lose
3420720	3428240	test memory uh the learning rate uh we are going to use uh this one there are different
3428240	3435200	learning rates that that what we can use um refer to the qloa because qloa really impacts
3435200	3439760	the learning rate the the model also really impact the learning rate that you want to use
3440480	3447280	we have the scheduler uh in this case we're going to use linear and warm up steps
3449440	3456960	like it won't be really useful here but we can say uh 10 uh to warm up the the optimizer
3456960	3463840	we want to report it to weights and biases and finally uh something that i'm going to put here but
3464320	3474720	uh remove this line uh for like for real training we're just going to stop after two steps otherwise
3474720	3479520	it will take like an hour to train the model but yeah you can just feel free to remove this step
3479520	3485520	if you want a real training so those are the training arguments uh then we need also to use
3485520	3491840	the fft trainer so the wrap i mentioned earlier and in this case we just specify the model the
3491840	3497680	training data set uh we don't have an eval data set so i'm just going to reuse the same one
3498240	3508640	pft config um we specified it um it wants to know the text field here so the instruction field
3508640	3516640	in our data set in our data set was called instruction um the max sequence length um
3517440	3525520	so we're gonna go for uh 512 uh you could say like yeah but we we put a threshold at 2k but
3525520	3532080	we don't have enough VRAM unfortunately uh it it will take like a lot of VRAM to to
3532080	3539600	to put everything into memory so we we're just gonna stop at uh 500 in this example and finally
3539600	3547120	we're going to give it the training arguments so this is what we have and when it's done we can
3547120	3555200	start the training and when the training is done uh we can even save the model uh using that
3557120	3563760	so we should have yeah the model has been downloaded here and now we are training the model
3564560	3571120	so this is a loss a training loss and evaluation loss from weights and biases and as you can see
3571120	3576400	it's a very nice way to tracking the the progress of the model we can see here the warm-up steps
3576400	3583360	where it's it's pretty bad and then it goes better and better uh you can see the training uh loss is
3583360	3590000	in blue so it's quite spiky it's a bit noisy the eval loss is in orange it's a lot less noisy
3590000	3596800	because it's less frequent and something that you can observe uh if you train it for like five epochs
3596800	3604080	is that the eval loss will go up instead of down uh normally uh traditionally in machine learning
3604080	3609360	this is a bad thing but with large average model it's been proven uh time and time again that it's
3609360	3616320	actually desirable and the best models um actually like overfit really a lot on the training data
3616320	3622720	and this is not a problem actually this makes them better um here as you can see your model
3622720	3628160	has already been trained for two steps so if you want you can add more steps you can remove it if
3628160	3633520	you want to train it on the entire data set it will take a while however uh we can check um
3633520	3639600	weights and biases here we won't see any the lot because we only have two steps uh but just to show
3639600	3647760	you uh here's our run and you can see here the global step uh train run time train loss of 1.2
3648960	3652880	so yeah this is what you would use if you run it on the entire data set
3654640	3663280	finally we can um use our model now that it's been it's been trained uh we can prompt it and say
3664000	3674560	what is a large language model uh we have to wrap it using the right uh chat template so with
3674560	3684240	instruction prompt and and then response
3688480	3698480	and we can use a a pipeline here uh from hugging face it's going to be pretty nice so model
3699120	3709120	tokenizer equal to tokenizer and we're going to restrict the generation in 128 and finally
3709120	3719280	we can get the result here and print it so i'm going to print the generated text it's an object
3719280	3727440	that returns um and we can do something fancy and remove the instruction part this is a question
3727440	3733440	that a lot of people ask me like why do i see the instruction uh in the generation in the generated
3733440	3740880	text you can just trim it basically this is how the hugging face object works but you can remove
3740880	3747200	it manually like this so now the train model is going to answer the question what is a large
3747200	3756960	language model and it will print um the answer here then we want to delete it okay we have the
3757040	3763280	answer so what is a neural network no the answer is a large language model is a type of artificial
3763280	3768160	intelligence model that is trained large amount of text data to generate human like text which is
3768160	3773040	pretty good actually um not thanks to our fine tuning because it was not intense enough but
3773040	3777840	it's a pretty good answer and then you can see that it keeps repeating like instruction response
3777840	3782640	instruction and this is because of our padding actually it's because we're using the end of
3782640	3792080	sentence token as a padding token so now it just doesn't stop and keeps talking so if you
3792080	3797120	don't want to have this behavior please use a different padding technique as mentioned previously
3798960	3807040	finally we want to remove a lot of things so this is specific to google collab we want to
3807840	3814960	collect all the model and the objects in the in the memory in the VRAM so we can merge it with the
3815920	3823360	so we can merge the base model with the adapter that we trained this is a piece of code that is
3823360	3829120	difficult to understand why do we need to call it twice honestly I do not know I just know that it
3829120	3838480	works when I do it and actually and sometimes it doesn't work so well worst case scenario you can
3838480	3846800	just like restart the the collab and and just execute this part of the code but here for the
3846800	3858720	sake of time I am going to copy paste the code so we are going to re reload the base model here
3859200	3865520	and we are going to also load the adapter so the cooler adapter that we
3867040	3874880	we created you can see it here this is our cooler adapter with adapter config adapter model
3874880	3886240	and this is what we want to to load hopefully it will work and and then we can push our model
3886240	3893280	to the hugging face hub when we do that we are going to push the model and the tokenizer and
3893920	3901200	we're using the hf token here this is optional of course and this will upload it to our
3903680	3911360	our hugging face account so here we are merging and unloading the model we are going to reload
3911360	3919360	the tokenizer not just in order to save it too and it's going to be uploaded okay so this is the
3919360	3928400	end of this session sorry it's a bit late but if you want to go further just know that you should
3928400	3936160	be able to reuse use this entire collab notebook with mistral 7b instead of flama 7b mistral 7b
3936160	3942480	is a better model but the name of this talk was fine-tuning llama 2 so I stuck to llama 2
3943120	3950960	but I would yeah I encourage you to try it out if you want a better fine-tuning tool I recommend
3950960	3955760	actual tool because these Google collab they're really nice to understand the theory behind everything
3955760	3964560	and be able to implement this fine-tuning process on your own but if you want to really
3965360	3972000	fine-tune state-of-the-art open source LLMs I recommend actual tool it's really a great tool
3972000	3978320	I've been using it a lot of people have been using it and this is quite easy to use so yeah
3978320	3985200	this is a good recommendation and then what you can do with this model is you can evaluate it
3985200	3990560	using a evaluation harness you can even get on the open LM leaderboard if you have a good model
3991520	3998240	or you can quantize it so you would make it easier to execute on consumer grade hardware
3998240	4006880	and you could use your fine-tune model on your own GPU so that's it for me it will take a while
4006880	4011360	for the model to be pushed to the hub because it's quite it's quite big but as you can see it's
4011360	4019360	been merged and everything is working correctly so I hope that you found it useful and if you
4019360	4024880	have any questions maybe now it's the time to ask the question to answer the questions
4026080	4031920	all right thank you Maxim that was fantastic a lot to unpack there I actually have like
4031920	4036400	a lot of questions for you but I think if I start asking questions we're going to go on
4036400	4041040	longer than an Ed Sheeran concert so I'm going to stick to audience questions instead
4041520	4050480	let's go with this one first from Prudine so Prudine is saying like maybe don't need to show
4050480	4055680	this but how do you go about querying tabular data so I mean this is very much focused on we've just
4055680	4063120	got a lot of text if it's tabular data what's the difference for tabular data I would not
4063120	4069680	recommend using LLMs because they've really made for for text there are some yeah actually I'm
4069680	4074960	wondering like I don't know maybe maybe we can't get into too much detail but is the standard like
4076240	4080960	if you've got just a text file or once if you've got like a pandas data frame of text
4082320	4088800	yeah this is a good question actually you could see it when we uploaded our data set to the
4088800	4095760	hiking phase hub this is kind of a data frame but it only has a text so if we go back to the
4095760	4100080	data set that we've dealt here you can see like it's basically data frame with instruction
4100080	4108400	output columns but it's not it's not tabular right it's it's really text yeah okay interesting
4109040	4116000	all right next question comes from Kiran saying okay so we've been doing this fine tuning on a GPU
4116000	4120240	do we also need a GPU at the point where we're hosting these things is it just the training bit
4120320	4126400	that's computationally intensive or is it also inference no the inference is also like very
4126400	4133360	intensive unfortunately and you definitely need well you need a GPU in general but in particular
4133360	4146720	if you use lama.cpp I can show it on my screen if you use lama.cpp you can use it on a CPU
4146720	4156800	you can see it sorry I'm gonna you can see it here it's me like running it and it runs on on a CPU
4157840	4163680	you'll have to compromise a little bit because like you need to lower the precision of the model so
4163680	4172320	it's smaller and faster to execute but this is something that you can do on the CPU
4173200	4177680	all right very good so um for anyone who's interested in lama.cpp I know I've got a
4177680	4184000	tutorial on that perhaps Reece can post a link to that in the moment next question
4184800	4193280	comes from Minfem so it looks like you you got some fancy co-pilot action or some sort of
4193280	4199280	autocomplete thing going on there in co-lab where does it what's that tool yeah it's a tool called
4199280	4204320	codium and yeah it works really well with Google collab as you could see I don't know if it learned
4204320	4211360	from my own code but it was like really accurate this time I spoke well when you're practicing
4211360	4217040	rehearsing the the tutorial you probably have the same thing a few times so yeah yeah good way to
4217040	4225600	train it yeah I mean they're training dataset now nice okay so codium's the thing uh next thing
4225680	4231920	comes from from Wei saying uh how important is parameter tuning during function I guess that's
4231920	4237280	like hyper parameter tuning yeah it's a really good question for some of the hyper parameters
4237280	4243680	going to be very important um and for some of them it's it's more like one percent two percent
4243680	4250560	gains for example everything here honestly if you stick to like traditional values uh it's it's
4250560	4255680	gonna make like not super meaningful improvements of course they're important because one percent
4255680	4262160	two percent it's good to have but they're not yeah that that important uh and there are other
4262160	4268560	yeah parameters that are a bit more important the learning rate is a really important one
4269680	4274640	and for this one yeah recommend checking the model that you want to use if you use mistral instead of
4275600	4280960	lambda two it's going to impact it if you use q lower lower or full fine tuning it's also going to
4280960	4290880	impact it all right um excellent next one is from bed can you show again how you show how you
4290880	4295760	load save models maybe we'll skip the using for text generation but if you just cover
4295760	4301520	loading save models I think that's useful okay so the you save the model by calling the the trainer
4302480	4310080	object and with the model and save pre-trained new model uh so this is uh yeah just just some
4310080	4317360	code you need to you know and and then it was the text generation right uh yes uh and in this case
4317360	4324000	I use the pipeline there are a lot of ways of like using them um it's not super pretty but uh it
4324000	4329040	doesn't take a lot of lines of code and yeah this is an object from hugging face from their library
4329600	4338000	which allows you to nicely um use the the text generation for instance okay oh I think the
4338000	4344160	question is about like once you've saved it how do you load that back okay uh basically just reusing
4344160	4354080	that and if you check actually um I've uh the model is uploaded on hugging face already and here
4354160	4364640	you have a usage um section where I describe how you all the code you need to use it so okay
4364640	4370160	see you're getting the whatever model type is from pre-trained uh pulling back off the hugging
4370160	4378880	face page all right nice yeah okay so uh next question from Arun saying can you see what percentage
4378880	4386400	of trainable parameters um we are reduced we're going to have to queue Laura so this is like how
4386400	4390960	many different I remember you saying queue Laura just changes some of the weights in the model so I
4390960	4398240	guess you want to know what percentage that is um it's an excellent question I uh I cannot show you
4398240	4407200	because I deleted the model type in trainer um before but yeah there's a command to do that
4407200	4411840	I recommend checking on google but yeah definitely you can see exactly like the percentage of
4411840	4418720	parameters the number of parameters that you're training using either lower or queue lower all
4418720	4426000	right excellent um and we've got so many more questions uh all right let's just do a couple more
4426560	4427920	um so
4434080	4443600	this next one all right okay it's always fine okay so Alexander asks how do you find
4443600	4450000	tune an LLM so it can extract JSON from differently format that's actually maybe a little bit
4450160	4457040	um specific but can you talk about how you apply it to like um a sort of you've got a CSV file or
4457040	4466800	an excel file of text how do you how I guess how do you standardize that data um yeah I I don't know
4466800	4474320	if it's really a task for an LLM um because there um um if it's just extracting I would say like why
4474320	4481680	do you want to use an LLM and not something else um other than that um there are different
4481680	4490480	frameworks like uh JSON former or LMQM even better LMQM let me show you if it's really about the
4490480	4497760	generation generating a popularly formatted JSON this is a really good uh framework to do it um
4497760	4503280	there are a lot of them but this one is currently the most popular one uh it's quite easy to use
4503920	4509200	I'm not sure if we'd answer the questions but I wouldn't use an LLM to extract this information
4509200	4514400	I would use it to generate a JSON uh and to generate this JSON this is the library I would use
4515760	4523840	okay uh so LMQL was that all right LMQL yeah LMQL all right that's worth looking into then
4523840	4528320	all right one very one very last question then since we're well over time anyway we're
4528320	4535200	we're past limits all right so um how can you improve the forms of LLMs you think basically
4535200	4541280	what's the what a llama index and lang chain and what's the difference between them um yeah so this
4541280	4548080	is um you have fine tuning and um lang chain and llama index they are more about like creating this
4548080	4556880	retrieval augmented generations so um fine tuning is is one way of customizing an LLM for your use
4556880	4563840	case and the RAG pipeline is another way of doing it so with lang chain and llama index you're going
4563840	4570480	to retrieve more context using some vector database or regular databases that you have
4572240	4577120	the difference between them I'm not going to delve into the details LLM index is is
4578560	4584240	there's less stuff but maybe more in depth than lang chain and I would recommend
4585200	4589840	actually implementing both approaches so fine tuning your LLM and using this fine
4589840	4594880	tune LLM with the RAG pipeline and this is where you'll get the best performance possible
4596480	4601760	all right fantastic we're gonna have to call it a day there I think I know there's more questions
4601760	4605600	so sorry to everyone in the audience if you didn't get to your question I just want to say thank you
4605600	4610480	again Maxim that was like incredibly informative and lots of new things that I think we need to
4610480	4617360	explore so brilliant thank you again thank you to recent moderating oh sorry gone Maxim
4618080	4623280	thank you Richie and thanks everyone for your patience I know it's been a lot but I hope that
4623280	4628640	you found it informative so yeah all right brilliant and yeah so thank you to everyone in
4628640	4632320	the audience who asked the question thank you to everyone who showed up today hope to see you all
4632320	4636880	again soon lots of exciting webinars coming up so goodbye have a great weekend
