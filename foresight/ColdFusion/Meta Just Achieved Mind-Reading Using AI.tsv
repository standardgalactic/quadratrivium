start	end	text
0	3000	This video is brought to you by Brilliant.
3000	6000	Welcome to another episode of Cold Fusion.
6000	11000	It's the year 2054 and the United States has just federally launched a new police unit.
11000	17000	It's special, precisely because the police will arrest people who commit crimes in the future,
17000	19000	a concept known as pre-crime.
19000	24000	This is, of course, the plot to the 2002 movie Minority Report.
24000	28000	Such a world where crimes are known before they're committed is disturbing.
28000	31000	It sits in between the comforts of security and low crime,
31000	34000	but also complete government knowledge and control.
34000	39000	It's scary, but at the end of the day, it's just a concept in a movie.
39000	41000	Or is it?
41000	46000	This next breakthrough is a lot deeper, all the way down to the very tissue of our brains,
46000	51000	where researchers have used AI to translate brain scans into text.
51000	56000	UT researchers have essentially created a device called a semantic decoder
56000	60000	that can read a person's mind by converting brain activity into a string of text.
60000	64000	In May of 2023, at the University of Texas at Austin,
64000	68000	a device was created that can turn a person's brain activity and thoughts
68000	71000	into a conscious, understandable stream of text.
71000	74000	The company Meta also recently did the same.
74000	76000	But that's not all.
76000	81000	Just last month, Meta also unveiled an AI system that can analyze a person's brain waves
81000	83000	and predict what that person is looking at.
83000	86000	If that's not enough, they were able to do this in real time.
86000	89000	Basically, the system was mind-reading.
89000	94000	Has a multi-billion dollar social media conglomerate just created a telepathy device
94000	96000	with the help of generative AI?
96000	100000	On the one hand, this is a potential disaster for privacy,
100000	105000	but on the other hand, such technologies could help many who can't speak due to illness or injury.
105000	111000	It appears that these days, every major technological innovation raises more questions than answers.
111000	115000	We're not quite yet at the capabilities of Minority Report,
115000	120000	but reading brain waves and interpreting what people are thinking is a step in that direction.
120000	124000	In this episode, we'll take a look at all of this, so get ready for a wild ride.
124000	128000	Let's start by diving deep into the University Mind-Reading AI,
128000	130000	then we'll get onto what Meta is doing.
130000	135000	You're going to want to hang around for that one, because it's world-changing stuff.
135000	138000	You are watching TelFusion TV.
146000	151000	UT's newest artificial intelligence tool could be a game-changer for some folks.
151000	157000	Our real hope is that this could help people who have lost the ability to communicate for some reason.
157000	160000	Jerry Tang, a doctoral student in computer science,
160000	165000	and Alexander Hooth, an assistant professor of neuroscience and computer science,
165000	170000	led the research at the University of Texas at Austin, the team that developed the Mind-Reading device.
170000	174000	They published their study in the Journal of Nature Neuroscience.
174000	178000	The Mind-Reading device is called a non-invasive language decoder.
178000	184000	While language decoders are not new, using them with the recent breakthroughs in generative AI certainly is.
184000	190000	Older versions could recognize only a few words here or there, or at best, a couple of phrases.
190000	193000	Jerry Tang and his team have managed to blow this out of the water.
193000	200000	Their device can reconstruct continuous language from perceived speech, imagined speech, and silent videos.
200000	204000	A technology like Neuralink, on the other hand, would be an invasive method.
204000	208000	It would provide clearer data but has higher risks associated.
209000	216000	The UT Austin team created a new system called a semantic decoder.
216000	224000	It works by analyzing non-invasive brain recordings through functional magnetic resonance imaging, or FMRI.
224000	230000	The decoder reconstructs what a person perceives, or imagines, into continuous natural language.
230000	236000	It sounds incredible, but before we get too excited, let's understand the limitations of a non-invasive system
236000	239000	and the challenges that the researchers had to overcome.
239000	246000	While invasive methods such as electrodes implanted into the brain have demonstrated notable achievements in the past,
246000	252000	non-invasive systems are still in their early stages of development, trailing behind in vital areas.
252000	258000	The team chose the non-invasive FMRI because it has excellent spatial specificity,
258000	262000	meaning that it can pinpoint neuroactivity in the brain with great accuracy.
262000	270000	However, its major limitation is its temporal resolution, or in other words, how quickly it captures changes in brain activity.
270000	277000	The blood oxygen level dependent, or bold, signal that the FMRI measures is notoriously slow,
277000	281000	taking about 10 seconds to rise and fall in response to neural activity.
281000	287000	Now, imagine trying to decode natural language where we speak at a rate of 1-2 words per second.
287000	289000	So how did they tackle this problem?
289000	295000	To address this, researchers employed an encoding model which predicts how the brain responds to natural language.
295000	302000	To train the model, they recorded brain responses while subjects listened to 16 hours of spoken narrative stories.
302000	309000	By extracting the semantic features and using linear regression, they built an accurate model of the brain's responses to different word sequences.
309000	320000	It's basically a model that has been trained with over 15 hours of data per person to take in their brain activity when they're sitting in an MRI scanner
320000	323000	and to spit out the story that they're listening to.
323000	325000	Here is where it becomes a little more interesting.
325000	329000	To ensure the words that were decoded had proper English grammar and structure,
329000	334000	the researchers incorporated a Generative Neural Network Language Model, or Generative AI.
334000	340000	Interestingly, they used GPT-1, an earlier version of the now famous chat GPT models.
340000	345000	This model can accurately predict the most likely words to follow in a given sequence.
345000	352000	But here's the catch. With countless possible word sequences, it would be impractical to generate and score each one individually.
352000	355000	This is where a clever algorithm called Beam Search comes into play.
355000	357000	Let me briefly explain how this works.
357000	363000	First, it analyses brain activity and then generates word sequences, one word at a time.
363000	366000	It starts with a group of the most probable word sentences.
366000	374000	As it analyses brain activity further and identifies new words, it generates more possible continuations for each sequence in the group.
374000	380000	This approach gradually narrows down possibilities and the most promising options are kept for each step.
380000	387000	By keeping the most credible sequences, the decoder continuously refines its predictions and determines the most likely words over time.
387000	389000	An example would be necessary here.
389000	390000	Consider the scenario.
390000	396000	If a person hears or thinks the word Apple, the decoder learns the corresponding brain activity.
396000	405000	When the person encounters a new word or sentence, the algorithm utilizes learned associations to generate a word sequence matching the language stimulus's meaning.
405000	414000	For example, if a person hears or thinks, I ate an apple, the algorithm may generate I consumed a fruit, or an apple was in my mouth.
414000	420000	The generated word sequence may not be an exact replica, but it captures the semantic representation.
420000	435000	So with the help of an ingenious encoding system, generative AI and a clever search algorithm, the scientists were able to circumvent the limitations of invasive fMRI scans.
435000	441000	So with the challenges tackled, it was time for the researchers to put the system to the test and see what it could do.
441000	450000	The team trained decoders for three individuals and evaluated each person's decoder by analyzing the brain responses while listening to new stories.
450000	454000	The goal was to determine if the decoder could understand the meaning of the information.
454000	457000	And the findings? Absolutely remarkable.
457000	464000	The decoded word sequences not only captured the meaning of the information, but often replicated the exact words and phrases.
464000	476000	This shows that semantic information or in-depth meaning can indeed be extracted from brain signals measured by fMRI.
476000	483000	The researchers also wanted to find out whether different parts of the brain have similar or different information about language.
483000	488000	Essentially, they were looking for how the decoding process works across cortical regions.
488000	494000	Cortical regions are specific areas in the outer layer of the brain known as the cerebral cortex.
494000	499000	Each region corresponds to distinct sensory, motor or cognitive processes.
499000	506000	The researchers compared predictions from decoding word sequences in these regions and they discovered something extraordinary.
506000	512000	They found out that diverse brain regions redundantly encode word-level language representations.
512000	517000	Basically, our brains have backup systems for language understanding and usage.
517000	524000	It's like having several copies of the same book. If you lose one copy or it gets damaged, you can still read the book with another copy.
524000	531000	So it suggests that even if one brain region is damaged, others can process the same information preserving our language abilities.
531000	534000	And that's a pretty astonishing find.
539000	546000	The researchers went even further. They tested their system on imagined speech, which is crucial for interpreting silent speech in the brain.
546000	555000	Participants were told to imagine stories during fMRI scans and the decoder successfully identified the content and meaning of their stories.
555000	564000	The researchers were ecstatic about their new discoveries and intrigued by the prospect of decoding other types of information as well, so they explored cross-modal decoding.
564000	571000	Researchers say they were surprised the decoder still worked even when the participants weren't hearing spoken language.
571000	579000	Even when somebody is seeing a movie, the model can kind of predict word descriptions of what they're seeing.
579000	585000	This study is a significant advancement for brain-computer interfaces that don't require invasive methods.
585000	592000	However, the study highlighted the need to include motor features and participant feedback to improve the decoding process.
592000	599000	But this story isn't over. As promised, Meta has come into the picture and they've come to raise the bar of what we thought was possible.
599000	608000	Although in the movie Minority Report, it was PsyKix that helped reveal the intentions of an individual, it's AI that's looking to do the same here in real life.
608000	610000	But how does AI work anyway?
610000	616000	Well, now there's a fun and easy way to learn about that and many other STEM subjects with today's sponsor, Brilliant.
616000	620000	Brilliant's course on artificial neural networks is perfect for that.
620000	625000	I've used it to brush up on some background context when I was making AI episodes.
625000	631000	Brilliant has interactive STEM courses in anything from maths and computer science to general science and statistics.
631000	637000	Whether you just want to brush up on learning or need a refresher for your career, Brilliant has you covered.
637000	644000	You can get started free for 30 days and the first 200 people to sign up get 20% off an annual plan.
644000	648000	Visit the URL brilliant.org slash coldfusion to get started.
648000	651000	Thanks and now back to the episode.
652000	658000	On October 18th, 2023, Meta pushed this field of research even further.
658000	666000	Instead of using fMRI, which has a resolution range of one data point every few seconds, Meta used MEG technology,
666000	672000	another non-invasive neuroimaging technique which can take thousands of brain activity measurements per second.
672000	678000	With this, they created an AI system capable of decoding visual representations in the brain.
678000	686000	The system can be deployed in real time to reconstruct from brain activity the images perceived and processed by the brain at each instant.
686000	689000	I told you this was going to get wild.
689000	695000	The system consists of three parts, an image encoder, a brain encoder and an image decoder.
695000	701000	The AI was trained on a public dataset of MEG recordings acquired from healthy volunteers.
701000	709000	In Meta's research, it was found that, quote, brain signals best align with modern computer vision AI systems like Dinov2.
709000	714000	This AI learned imagery by itself and wasn't supervised by human labels.
714000	720000	Meta states that, quote, self-supervised learning leads AI systems to learn brain-like representations.
720000	728000	The artificial neurons in the algorithm tend to be activated similarly to physical neurons in the human brain in response to the same image.
728000	737000	This functional alignment between such AI systems and the brain can be used to guide the generation of images similar to what the participants see in the scanner.
737000	742000	Using this technique, Meta managed to read the minds of people and interpret what they were looking at.
742000	746000	On the left is the original image shown to the participant.
746000	749000	On the right is what the AI thinks the person is looking at.
749000	754000	And remember, this resulting image was only determined by the brain waves.
754000	757000	The AI had no idea what the person was actually looking at.
757000	764000	The resulting images could nail broad object categories, but were unstable in the finer details.
764000	771000	Meta claims that their research is ultimately to guide the development of AI systems designed to learn and reason like humans.
771000	774000	We'll see about that.
774000	779000	Envisioning the future of brain reading technology is challenging, but the potential could be vast.
779000	785000	For instance, it could help those who are mentally conscious but are unable to speak or communicate.
785000	793000	Thinking further afield, imagine if a smartphone's volume, notifications, and music selection would adapt to a user's mood or brain activity.
793000	798000	And smartphones and other devices could be commanded or queried just by thoughts alone.
798000	802000	It would be like speaking to Google Assistant, but with your brain.
802000	809000	Next sense, which originated from Google's parent company, Alphabet's X division, has been working on something interesting since 2020.
809000	815000	The startup is currently developing earbuds with the remarkable ability to capture human brain signals.
815000	822000	These earbuds have the potential to monitor and diagnose brain conditions like epilepsy, depression, and sleep disorders.
822000	831000	They can also detect seizures, assess medication effectiveness, improve deep sleep, and provide valuable insights for comprehensive brain assessment.
831000	834000	Now, there's obviously a long way to go with brain scanning technology,
834000	841000	but it's clear that we're at the very start of a new era of interpreting the human brain thanks to generative AI.
841000	846000	On the negative side, however, the sum of all of this technology could be a privacy catastrophe.
846000	853000	Imagine Meta or Google being able to know what you're looking at or what you're even thinking in order to better target their advertising
853000	856000	or to sway public opinion in one way or the other.
856000	863000	Who knows what this technology will evolve into in the next 30 years in the very same era that Minority Report is set in?
863000	866000	With Meta getting involved, there are natural concerns for me.
866000	877000	Mind decoding technology in research is one thing, but having popular wearable technology such as the MetaQuest VR headset or video camera glasses could be a future temptation for the firm.
877000	881000	Meta were never known for their care of user privacy and consent.
881000	889000	Though still, to achieve true mind reading or telepathy, the decoding technology would need to overcome a lot of challenges and limitations.
889000	897000	Accuracy and resolution must increase. The system would need to understand tone, emotion and contextual nuances of thoughts.
897000	904000	Second, it would need to work in both directions, allowing a person to both send and receive thoughts from one person to another.
904000	909000	So true telepathy remains firmly rooted in fiction and perhaps that's for the best.
909000	913000	So back to the university researchers for a second.
914000	920000	The scientific world has been stunned. The findings of the semantic decoder are undeniably remarkable.
920000	929000	Tonight we turn to news in one specific way that artificial intelligence technology is changing our world and it is absolutely remarkable.
929000	934000	These neuroscientists at the University of Texas in Austin say they've made a major breakthrough.
934000	938000	They've figured out how to translate brain activity into words using artificial intelligence.
938000	943000	Dr. Alexander Hooth, the lead researcher, expressed his surprise to the Guardian saying,
943000	947000	quote, we were kind of shocked that it works as well as it does.
947000	954000	I've been working on this for 15 years, so it was shocking and exciting when it finally did work, end quote.
954000	958000	The breakthrough captured widespread attention from the media and scientists alike.
958000	963000	Professor Tim Behrens from the University of Oxford praised the study's technical prowess,
963000	969000	highlighting its potential to decipher thoughts and dreams and reveal new ideas from subconscious brain activity.
969000	975000	He said to the Guardian, quote, these generative models are letting you see what's in the brain at a new level.
975000	980000	It means that you can really read out something deep from the fMRI, end quote.
980000	987000	When you stop and think about it, an artificial brain interpreting the internal happenings of a human brain makes total sense,
987000	990000	but it's just something that I thought would be a long way off.
990000	997000	It's just my opinion, but I think that these AI technologies truly shine when they're applied to scientific research and studies like this.
997000	1002000	That's when we get to harness the full power of artificial intelligence.
1002000	1010000	So there you have it, brain decoders that utilize the power of AI to interpret human thoughts and generate coherent language and imagery.
1010000	1013000	Well, at least in a controlled environment for now.
1013000	1018000	The scanners are still bulky and expensive, and the training process is long and tiresome.
1018000	1022000	However, as with all technology, this is only going to get better.
1022000	1027000	Of course, there are concerns companies could misuse this for their own selfish gains,
1027000	1031000	but for the physically impaired, the possibilities are almost endless.
1031000	1033000	Personally, I found this quite fascinating.
1033000	1035000	But what are your thoughts on this?
1035000	1037000	How do you guys think that this is going to shape the future?
1037000	1042000	Do you think that it's good or bad to be able to read or at least interpret someone's thoughts?
1042000	1046000	Or do you just think that this technology should just never see the light of day?
1046000	1051000	It's an interesting discussion, so feel free to discuss in the comments section below.
1051000	1053000	Alright, so that's about it from me.
1053000	1054000	Thanks for watching.
1054000	1059000	If you want to see anything else on science, technology, business or history, feel free to subscribe to Cold Fusion.
1059000	1060000	It's free.
1060000	1065000	My name is DeGogo, and you've been watching Cold Fusion, and I'll catch you again soon for the next episode.
1065000	1066000	Cheers, guys.
1066000	1068000	Have a good one.
1072000	1074000	Cold Fusion, it's me thinking.
