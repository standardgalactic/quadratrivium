WEBVTT

00:00.000 --> 00:03.000
This video is brought to you by Brilliant.

00:03.000 --> 00:06.000
Welcome to another episode of Cold Fusion.

00:06.000 --> 00:11.000
It's the year 2054 and the United States has just federally launched a new police unit.

00:11.000 --> 00:17.000
It's special, precisely because the police will arrest people who commit crimes in the future,

00:17.000 --> 00:19.000
a concept known as pre-crime.

00:19.000 --> 00:24.000
This is, of course, the plot to the 2002 movie Minority Report.

00:24.000 --> 00:28.000
Such a world where crimes are known before they're committed is disturbing.

00:28.000 --> 00:31.000
It sits in between the comforts of security and low crime,

00:31.000 --> 00:34.000
but also complete government knowledge and control.

00:34.000 --> 00:39.000
It's scary, but at the end of the day, it's just a concept in a movie.

00:39.000 --> 00:41.000
Or is it?

00:41.000 --> 00:46.000
This next breakthrough is a lot deeper, all the way down to the very tissue of our brains,

00:46.000 --> 00:51.000
where researchers have used AI to translate brain scans into text.

00:51.000 --> 00:56.000
UT researchers have essentially created a device called a semantic decoder

00:56.000 --> 01:00.000
that can read a person's mind by converting brain activity into a string of text.

01:00.000 --> 01:04.000
In May of 2023, at the University of Texas at Austin,

01:04.000 --> 01:08.000
a device was created that can turn a person's brain activity and thoughts

01:08.000 --> 01:11.000
into a conscious, understandable stream of text.

01:11.000 --> 01:14.000
The company Meta also recently did the same.

01:14.000 --> 01:16.000
But that's not all.

01:16.000 --> 01:21.000
Just last month, Meta also unveiled an AI system that can analyze a person's brain waves

01:21.000 --> 01:23.000
and predict what that person is looking at.

01:23.000 --> 01:26.000
If that's not enough, they were able to do this in real time.

01:26.000 --> 01:29.000
Basically, the system was mind-reading.

01:29.000 --> 01:34.000
Has a multi-billion dollar social media conglomerate just created a telepathy device

01:34.000 --> 01:36.000
with the help of generative AI?

01:36.000 --> 01:40.000
On the one hand, this is a potential disaster for privacy,

01:40.000 --> 01:45.000
but on the other hand, such technologies could help many who can't speak due to illness or injury.

01:45.000 --> 01:51.000
It appears that these days, every major technological innovation raises more questions than answers.

01:51.000 --> 01:55.000
We're not quite yet at the capabilities of Minority Report,

01:55.000 --> 02:00.000
but reading brain waves and interpreting what people are thinking is a step in that direction.

02:00.000 --> 02:04.000
In this episode, we'll take a look at all of this, so get ready for a wild ride.

02:04.000 --> 02:08.000
Let's start by diving deep into the University Mind-Reading AI,

02:08.000 --> 02:10.000
then we'll get onto what Meta is doing.

02:10.000 --> 02:15.000
You're going to want to hang around for that one, because it's world-changing stuff.

02:15.000 --> 02:18.000
You are watching TelFusion TV.

02:26.000 --> 02:31.000
UT's newest artificial intelligence tool could be a game-changer for some folks.

02:31.000 --> 02:37.000
Our real hope is that this could help people who have lost the ability to communicate for some reason.

02:37.000 --> 02:40.000
Jerry Tang, a doctoral student in computer science,

02:40.000 --> 02:45.000
and Alexander Hooth, an assistant professor of neuroscience and computer science,

02:45.000 --> 02:50.000
led the research at the University of Texas at Austin, the team that developed the Mind-Reading device.

02:50.000 --> 02:54.000
They published their study in the Journal of Nature Neuroscience.

02:54.000 --> 02:58.000
The Mind-Reading device is called a non-invasive language decoder.

02:58.000 --> 03:04.000
While language decoders are not new, using them with the recent breakthroughs in generative AI certainly is.

03:04.000 --> 03:10.000
Older versions could recognize only a few words here or there, or at best, a couple of phrases.

03:10.000 --> 03:13.000
Jerry Tang and his team have managed to blow this out of the water.

03:13.000 --> 03:20.000
Their device can reconstruct continuous language from perceived speech, imagined speech, and silent videos.

03:20.000 --> 03:24.000
A technology like Neuralink, on the other hand, would be an invasive method.

03:24.000 --> 03:28.000
It would provide clearer data but has higher risks associated.

03:29.000 --> 03:36.000
The UT Austin team created a new system called a semantic decoder.

03:36.000 --> 03:44.000
It works by analyzing non-invasive brain recordings through functional magnetic resonance imaging, or FMRI.

03:44.000 --> 03:50.000
The decoder reconstructs what a person perceives, or imagines, into continuous natural language.

03:50.000 --> 03:56.000
It sounds incredible, but before we get too excited, let's understand the limitations of a non-invasive system

03:56.000 --> 03:59.000
and the challenges that the researchers had to overcome.

03:59.000 --> 04:06.000
While invasive methods such as electrodes implanted into the brain have demonstrated notable achievements in the past,

04:06.000 --> 04:12.000
non-invasive systems are still in their early stages of development, trailing behind in vital areas.

04:12.000 --> 04:18.000
The team chose the non-invasive FMRI because it has excellent spatial specificity,

04:18.000 --> 04:22.000
meaning that it can pinpoint neuroactivity in the brain with great accuracy.

04:22.000 --> 04:30.000
However, its major limitation is its temporal resolution, or in other words, how quickly it captures changes in brain activity.

04:30.000 --> 04:37.000
The blood oxygen level dependent, or bold, signal that the FMRI measures is notoriously slow,

04:37.000 --> 04:41.000
taking about 10 seconds to rise and fall in response to neural activity.

04:41.000 --> 04:47.000
Now, imagine trying to decode natural language where we speak at a rate of 1-2 words per second.

04:47.000 --> 04:49.000
So how did they tackle this problem?

04:49.000 --> 04:55.000
To address this, researchers employed an encoding model which predicts how the brain responds to natural language.

04:55.000 --> 05:02.000
To train the model, they recorded brain responses while subjects listened to 16 hours of spoken narrative stories.

05:02.000 --> 05:09.000
By extracting the semantic features and using linear regression, they built an accurate model of the brain's responses to different word sequences.

05:09.000 --> 05:20.000
It's basically a model that has been trained with over 15 hours of data per person to take in their brain activity when they're sitting in an MRI scanner

05:20.000 --> 05:23.000
and to spit out the story that they're listening to.

05:23.000 --> 05:25.000
Here is where it becomes a little more interesting.

05:25.000 --> 05:29.000
To ensure the words that were decoded had proper English grammar and structure,

05:29.000 --> 05:34.000
the researchers incorporated a Generative Neural Network Language Model, or Generative AI.

05:34.000 --> 05:40.000
Interestingly, they used GPT-1, an earlier version of the now famous chat GPT models.

05:40.000 --> 05:45.000
This model can accurately predict the most likely words to follow in a given sequence.

05:45.000 --> 05:52.000
But here's the catch. With countless possible word sequences, it would be impractical to generate and score each one individually.

05:52.000 --> 05:55.000
This is where a clever algorithm called Beam Search comes into play.

05:55.000 --> 05:57.000
Let me briefly explain how this works.

05:57.000 --> 06:03.000
First, it analyses brain activity and then generates word sequences, one word at a time.

06:03.000 --> 06:06.000
It starts with a group of the most probable word sentences.

06:06.000 --> 06:14.000
As it analyses brain activity further and identifies new words, it generates more possible continuations for each sequence in the group.

06:14.000 --> 06:20.000
This approach gradually narrows down possibilities and the most promising options are kept for each step.

06:20.000 --> 06:27.000
By keeping the most credible sequences, the decoder continuously refines its predictions and determines the most likely words over time.

06:27.000 --> 06:29.000
An example would be necessary here.

06:29.000 --> 06:30.000
Consider the scenario.

06:30.000 --> 06:36.000
If a person hears or thinks the word Apple, the decoder learns the corresponding brain activity.

06:36.000 --> 06:45.000
When the person encounters a new word or sentence, the algorithm utilizes learned associations to generate a word sequence matching the language stimulus's meaning.

06:45.000 --> 06:54.000
For example, if a person hears or thinks, I ate an apple, the algorithm may generate I consumed a fruit, or an apple was in my mouth.

06:54.000 --> 07:00.000
The generated word sequence may not be an exact replica, but it captures the semantic representation.

07:00.000 --> 07:15.000
So with the help of an ingenious encoding system, generative AI and a clever search algorithm, the scientists were able to circumvent the limitations of invasive fMRI scans.

07:15.000 --> 07:21.000
So with the challenges tackled, it was time for the researchers to put the system to the test and see what it could do.

07:21.000 --> 07:30.000
The team trained decoders for three individuals and evaluated each person's decoder by analyzing the brain responses while listening to new stories.

07:30.000 --> 07:34.000
The goal was to determine if the decoder could understand the meaning of the information.

07:34.000 --> 07:37.000
And the findings? Absolutely remarkable.

07:37.000 --> 07:44.000
The decoded word sequences not only captured the meaning of the information, but often replicated the exact words and phrases.

07:44.000 --> 07:56.000
This shows that semantic information or in-depth meaning can indeed be extracted from brain signals measured by fMRI.

07:56.000 --> 08:03.000
The researchers also wanted to find out whether different parts of the brain have similar or different information about language.

08:03.000 --> 08:08.000
Essentially, they were looking for how the decoding process works across cortical regions.

08:08.000 --> 08:14.000
Cortical regions are specific areas in the outer layer of the brain known as the cerebral cortex.

08:14.000 --> 08:19.000
Each region corresponds to distinct sensory, motor or cognitive processes.

08:19.000 --> 08:26.000
The researchers compared predictions from decoding word sequences in these regions and they discovered something extraordinary.

08:26.000 --> 08:32.000
They found out that diverse brain regions redundantly encode word-level language representations.

08:32.000 --> 08:37.000
Basically, our brains have backup systems for language understanding and usage.

08:37.000 --> 08:44.000
It's like having several copies of the same book. If you lose one copy or it gets damaged, you can still read the book with another copy.

08:44.000 --> 08:51.000
So it suggests that even if one brain region is damaged, others can process the same information preserving our language abilities.

08:51.000 --> 08:54.000
And that's a pretty astonishing find.

08:59.000 --> 09:06.000
The researchers went even further. They tested their system on imagined speech, which is crucial for interpreting silent speech in the brain.

09:06.000 --> 09:15.000
Participants were told to imagine stories during fMRI scans and the decoder successfully identified the content and meaning of their stories.

09:15.000 --> 09:24.000
The researchers were ecstatic about their new discoveries and intrigued by the prospect of decoding other types of information as well, so they explored cross-modal decoding.

09:24.000 --> 09:31.000
Researchers say they were surprised the decoder still worked even when the participants weren't hearing spoken language.

09:31.000 --> 09:39.000
Even when somebody is seeing a movie, the model can kind of predict word descriptions of what they're seeing.

09:39.000 --> 09:45.000
This study is a significant advancement for brain-computer interfaces that don't require invasive methods.

09:45.000 --> 09:52.000
However, the study highlighted the need to include motor features and participant feedback to improve the decoding process.

09:52.000 --> 09:59.000
But this story isn't over. As promised, Meta has come into the picture and they've come to raise the bar of what we thought was possible.

09:59.000 --> 10:08.000
Although in the movie Minority Report, it was PsyKix that helped reveal the intentions of an individual, it's AI that's looking to do the same here in real life.

10:08.000 --> 10:10.000
But how does AI work anyway?

10:10.000 --> 10:16.000
Well, now there's a fun and easy way to learn about that and many other STEM subjects with today's sponsor, Brilliant.

10:16.000 --> 10:20.000
Brilliant's course on artificial neural networks is perfect for that.

10:20.000 --> 10:25.000
I've used it to brush up on some background context when I was making AI episodes.

10:25.000 --> 10:31.000
Brilliant has interactive STEM courses in anything from maths and computer science to general science and statistics.

10:31.000 --> 10:37.000
Whether you just want to brush up on learning or need a refresher for your career, Brilliant has you covered.

10:37.000 --> 10:44.000
You can get started free for 30 days and the first 200 people to sign up get 20% off an annual plan.

10:44.000 --> 10:48.000
Visit the URL brilliant.org slash coldfusion to get started.

10:48.000 --> 10:51.000
Thanks and now back to the episode.

10:52.000 --> 10:58.000
On October 18th, 2023, Meta pushed this field of research even further.

10:58.000 --> 11:06.000
Instead of using fMRI, which has a resolution range of one data point every few seconds, Meta used MEG technology,

11:06.000 --> 11:12.000
another non-invasive neuroimaging technique which can take thousands of brain activity measurements per second.

11:12.000 --> 11:18.000
With this, they created an AI system capable of decoding visual representations in the brain.

11:18.000 --> 11:26.000
The system can be deployed in real time to reconstruct from brain activity the images perceived and processed by the brain at each instant.

11:26.000 --> 11:29.000
I told you this was going to get wild.

11:29.000 --> 11:35.000
The system consists of three parts, an image encoder, a brain encoder and an image decoder.

11:35.000 --> 11:41.000
The AI was trained on a public dataset of MEG recordings acquired from healthy volunteers.

11:41.000 --> 11:49.000
In Meta's research, it was found that, quote, brain signals best align with modern computer vision AI systems like Dinov2.

11:49.000 --> 11:54.000
This AI learned imagery by itself and wasn't supervised by human labels.

11:54.000 --> 12:00.000
Meta states that, quote, self-supervised learning leads AI systems to learn brain-like representations.

12:00.000 --> 12:08.000
The artificial neurons in the algorithm tend to be activated similarly to physical neurons in the human brain in response to the same image.

12:08.000 --> 12:17.000
This functional alignment between such AI systems and the brain can be used to guide the generation of images similar to what the participants see in the scanner.

12:17.000 --> 12:22.000
Using this technique, Meta managed to read the minds of people and interpret what they were looking at.

12:22.000 --> 12:26.000
On the left is the original image shown to the participant.

12:26.000 --> 12:29.000
On the right is what the AI thinks the person is looking at.

12:29.000 --> 12:34.000
And remember, this resulting image was only determined by the brain waves.

12:34.000 --> 12:37.000
The AI had no idea what the person was actually looking at.

12:37.000 --> 12:44.000
The resulting images could nail broad object categories, but were unstable in the finer details.

12:44.000 --> 12:51.000
Meta claims that their research is ultimately to guide the development of AI systems designed to learn and reason like humans.

12:51.000 --> 12:54.000
We'll see about that.

12:54.000 --> 12:59.000
Envisioning the future of brain reading technology is challenging, but the potential could be vast.

12:59.000 --> 13:05.000
For instance, it could help those who are mentally conscious but are unable to speak or communicate.

13:05.000 --> 13:13.000
Thinking further afield, imagine if a smartphone's volume, notifications, and music selection would adapt to a user's mood or brain activity.

13:13.000 --> 13:18.000
And smartphones and other devices could be commanded or queried just by thoughts alone.

13:18.000 --> 13:22.000
It would be like speaking to Google Assistant, but with your brain.

13:22.000 --> 13:29.000
Next sense, which originated from Google's parent company, Alphabet's X division, has been working on something interesting since 2020.

13:29.000 --> 13:35.000
The startup is currently developing earbuds with the remarkable ability to capture human brain signals.

13:35.000 --> 13:42.000
These earbuds have the potential to monitor and diagnose brain conditions like epilepsy, depression, and sleep disorders.

13:42.000 --> 13:51.000
They can also detect seizures, assess medication effectiveness, improve deep sleep, and provide valuable insights for comprehensive brain assessment.

13:51.000 --> 13:54.000
Now, there's obviously a long way to go with brain scanning technology,

13:54.000 --> 14:01.000
but it's clear that we're at the very start of a new era of interpreting the human brain thanks to generative AI.

14:01.000 --> 14:06.000
On the negative side, however, the sum of all of this technology could be a privacy catastrophe.

14:06.000 --> 14:13.000
Imagine Meta or Google being able to know what you're looking at or what you're even thinking in order to better target their advertising

14:13.000 --> 14:16.000
or to sway public opinion in one way or the other.

14:16.000 --> 14:23.000
Who knows what this technology will evolve into in the next 30 years in the very same era that Minority Report is set in?

14:23.000 --> 14:26.000
With Meta getting involved, there are natural concerns for me.

14:26.000 --> 14:37.000
Mind decoding technology in research is one thing, but having popular wearable technology such as the MetaQuest VR headset or video camera glasses could be a future temptation for the firm.

14:37.000 --> 14:41.000
Meta were never known for their care of user privacy and consent.

14:41.000 --> 14:49.000
Though still, to achieve true mind reading or telepathy, the decoding technology would need to overcome a lot of challenges and limitations.

14:49.000 --> 14:57.000
Accuracy and resolution must increase. The system would need to understand tone, emotion and contextual nuances of thoughts.

14:57.000 --> 15:04.000
Second, it would need to work in both directions, allowing a person to both send and receive thoughts from one person to another.

15:04.000 --> 15:09.000
So true telepathy remains firmly rooted in fiction and perhaps that's for the best.

15:09.000 --> 15:13.000
So back to the university researchers for a second.

15:14.000 --> 15:20.000
The scientific world has been stunned. The findings of the semantic decoder are undeniably remarkable.

15:20.000 --> 15:29.000
Tonight we turn to news in one specific way that artificial intelligence technology is changing our world and it is absolutely remarkable.

15:29.000 --> 15:34.000
These neuroscientists at the University of Texas in Austin say they've made a major breakthrough.

15:34.000 --> 15:38.000
They've figured out how to translate brain activity into words using artificial intelligence.

15:38.000 --> 15:43.000
Dr. Alexander Hooth, the lead researcher, expressed his surprise to the Guardian saying,

15:43.000 --> 15:47.000
quote, we were kind of shocked that it works as well as it does.

15:47.000 --> 15:54.000
I've been working on this for 15 years, so it was shocking and exciting when it finally did work, end quote.

15:54.000 --> 15:58.000
The breakthrough captured widespread attention from the media and scientists alike.

15:58.000 --> 16:03.000
Professor Tim Behrens from the University of Oxford praised the study's technical prowess,

16:03.000 --> 16:09.000
highlighting its potential to decipher thoughts and dreams and reveal new ideas from subconscious brain activity.

16:09.000 --> 16:15.000
He said to the Guardian, quote, these generative models are letting you see what's in the brain at a new level.

16:15.000 --> 16:20.000
It means that you can really read out something deep from the fMRI, end quote.

16:20.000 --> 16:27.000
When you stop and think about it, an artificial brain interpreting the internal happenings of a human brain makes total sense,

16:27.000 --> 16:30.000
but it's just something that I thought would be a long way off.

16:30.000 --> 16:37.000
It's just my opinion, but I think that these AI technologies truly shine when they're applied to scientific research and studies like this.

16:37.000 --> 16:42.000
That's when we get to harness the full power of artificial intelligence.

16:42.000 --> 16:50.000
So there you have it, brain decoders that utilize the power of AI to interpret human thoughts and generate coherent language and imagery.

16:50.000 --> 16:53.000
Well, at least in a controlled environment for now.

16:53.000 --> 16:58.000
The scanners are still bulky and expensive, and the training process is long and tiresome.

16:58.000 --> 17:02.000
However, as with all technology, this is only going to get better.

17:02.000 --> 17:07.000
Of course, there are concerns companies could misuse this for their own selfish gains,

17:07.000 --> 17:11.000
but for the physically impaired, the possibilities are almost endless.

17:11.000 --> 17:13.000
Personally, I found this quite fascinating.

17:13.000 --> 17:15.000
But what are your thoughts on this?

17:15.000 --> 17:17.000
How do you guys think that this is going to shape the future?

17:17.000 --> 17:22.000
Do you think that it's good or bad to be able to read or at least interpret someone's thoughts?

17:22.000 --> 17:26.000
Or do you just think that this technology should just never see the light of day?

17:26.000 --> 17:31.000
It's an interesting discussion, so feel free to discuss in the comments section below.

17:31.000 --> 17:33.000
Alright, so that's about it from me.

17:33.000 --> 17:34.000
Thanks for watching.

17:34.000 --> 17:39.000
If you want to see anything else on science, technology, business or history, feel free to subscribe to Cold Fusion.

17:39.000 --> 17:40.000
It's free.

17:40.000 --> 17:45.000
My name is DeGogo, and you've been watching Cold Fusion, and I'll catch you again soon for the next episode.

17:45.000 --> 17:46.000
Cheers, guys.

17:46.000 --> 17:48.000
Have a good one.

17:52.000 --> 17:54.000
Cold Fusion, it's me thinking.

