{"text": " Great. Yeah, so hi, I'm Hyunjin Chung. I'm a PhD student at bio-imaging and signal processing and learning lab at KAIST Korea. I'm honored to give a talk in such a prestigious group in Netherlands. And so thank you very much for having me here. So today I'll give a talk mostly about diffusion models and how to use them to solve inverse problems in imaging, focusing on a few recent papers that I wrote. So specifically it will be focused on base and inference in the context of inverse problem solving. So I believe it will be of quite a relevance to you. And I'm pretty sure that most of you are already familiar with diffusion models, but some of you may not be familiar with inverse problems. So and since we have a rather small group, I feel free to speak up with your microphone. I'm very sure that I'm going to miss some chats. So I hope the talk will be as interactive as possible. Yeah, so the talk will be structured in a bit of an odd way because we're going to first talk about two closely related work in a reverse order. So the first part of the talk will be about a paper named DPS, accepted to iClear recently. Then we will move backwards and talk about MCG, which was supposed to be the main content of the talk today. And finally, if time allows, I would also like to briefly touch the most recent work that applies these methods to more advanced problems called blind inverse problems. So I'll just briefly cover the basic concepts of diffusion models. I don't want to bore you too much, but I'll just speak in a briefly in a score perspective. So what we do in diffusion models is we try to estimate the gradient of the lock density with the neural network S of theta, which points to the high density most of the distribution. In order to circumvent the intractable explicit score matching, you train your network with denoising score matching, which boils down to essentially training a residual denoiser. Once that is done, you can use the train score functions to sample from the distribution p of x with, for example, Langevin MCMC. And another interesting view is that one can view the data noising process as some linear forward SE, and the data generating process as the corresponding reverse stochastic differential equation, where the drift function is governed by the score function. Hence, when we want to sample data, you can discretize the reverse SDE and numerically solve it using the pre-trained score function. On the other hand, here we are interested in solving inverse problems. In the inverse problem setting, our aim is to recover the ground truth x from the noisy measurements y obtained through some integral imaging system A. And there will be some pollution of noise, and here, A, sorry, and the problem is naturally opposed, which means that there exist infinitely many solutions to this problem, meaning that given this noisy image, noisy blurred image of the leopard, there could exist a lot of different feasible answers to the actual clean image. So in order to correctly specify which one of the solutions is the one that we want, we need to specify the prior of the data distribution, and in other words, how the images would actually look like in real world. Examples of such inverse problems include in-painting and deep learning in the context of low-level vision. They're also called image restoration problems. And in the context of biomedical imaging, there could be compressing MRI, sparsive CT, and so on. So we'll go over how we can solve such inverse problems with diffusion models. So let's first consider the following measurements model. Now, given y, what we want is to sample from the posterior distribution p of x given y. And using Bayes' rule, it's straightforward to see that the gradient of the log posterior is the sum of gradient of the log likelihood plus the score function. Here, gradient of the log likelihood in red gives us the information about the measurement process. And the score function is what we've already estimated with the neural network. So one should note that the likelihood function p of y given x is in most cases known. And in 95% of the cases, it can be modeled relatively well with the Gaussian. And among the rest of the 5%, 4% is Poisson, meaning that the likelihood function is often known a priority. So hence, in the perspective of Bayesian reconstruction, all we have to do when posterior sampling is desirable is to specify the correct prior distribution. Then the measurement process is hand wavy, it's known so we can sample from the posterior. So let's consider the case where we modeled the data distribution with the diffusion model. From the score or the SDE perspective, recall that the generative process is defined by this general form of reverse SDE. So in order to do unconditional sampling from the prior distribution, we can use the following discretization steps of the reverse SDE. So for simplicity, I denoted Euler-Maruyama discretization here, but other forms of sampling can of course be used, such as ancestral sampling of DDPMs or hybrid methods that incorporate MCMC chains within the numerical integration solvers. They're called predictor-corrector solvers in score SDE paper. Moreover, when we want to do posterior sampling, we can use what we've derived from the Bayes rule and incorporate the gradient of the walk likelihood. So here comes the problem. Can I ask a question? I'm sorry, I'm decently new to the diffusion model, so maybe there is something I didn't understand. I don't understand the sign, why there is a minus sign in front of the even the original paper, why there is a minus sign in front of the gradient of the score. Oh, so you're asking, how did the reverse SDE come along here, right? Well, I'm saying that with that minus sign, it will do the opposite of going to the high probability regions. It will go away from it. So am I missing something? Oh, yeah. So intuitively, if you want to sample from the high density modes of the distribution, you would kind of do gradient ascent with it. And yeah, there's a caveat here, because the differential dt here is a time running backwards. So you have to show the minus sign. Yeah. But then the sign of f is wrong, because the sign of f should have the same sign of g, doesn't it? Actually, no. So the derivation process of this reverse SDE comes from the thing called Anderson's theorem. So if you plug in this general form of forward SDE, then there's a theorem that states that this reverse SDE is the correct form. I understand. But so if there isn't, let's say that there is no score, so there is no goal, then if you have a, let's say, a Oldston-Ulenbach process, the forward and reverse dynamics are the same. So you're saying that f dt is running backwards, then f should be negative. If you have a stationary process like an Oldston-Ulenbach, regardless whether you run it forward or backward, you have the same dynamics. Therefore, you are not going to flip the sign of f if you change the time. Let's say, in an Oldston-Ulenbach process, f induces the mean reversion. So if you actually flip the sign, you will have the process going away from the mean, which is not the reverse time process. The reverse time process wants to go back to the mean exactly like the forward process. Right, right. I'm also a bit confused myself. Because everything works, I think, I don't know. So again, I could be misinterpreting something. Because I do see this in papers, but I just, it's, again, if it's an Oldston-Ulenbach process, the forward and reverse dynamics, assuming score zero, should be the same. So you cannot flip the sign. Otherwise, you get the opposite, you get the diverging process. And if you assume the generative direction to be forward, then the sign of the score should be plus, not minus. But of course, it would become minus if you are actually considering that going backward. But okay, but maybe we can discuss this offline. And also, I mean, again, it could be that I'm missing some details of the notation that for which things will match up. I'm not sure if, especially in the case of Ornstein-Ulenbach process, is there, is the score function zero there? Well, so the, well, so the, if the forward process is Oldston-Ulenbach, then f of x of t is just minus alpha x. Right. And that would be true both in the forward and in the backward, because again, it's time invariant. If you flip, you don't change anything. The score happened in order to enforce the terminal condition that the process has to fit the data. So it, in this sense, it's nothing to do with the Ornstein-Ulenbach process. It's just a starting point of the process, which if then, if you want to reverse, it will actually go back to there. Yeah, I think we should come back to this later. I think it's better to discuss So coming back to where we were, I think it was right here. So I was talking about how we could switch from prior sampling to posterior sampling by, by just plugging, plugging in this base rule. And here I said that care must be taken here because the gradient of the log likelihood is in fact intractable. And note that this is different from my claim earlier that the likelihood function is in most cases known. And this is, this arises from the fact that there are noisy x i's here rather than x zero. So let us dive into closely examine what I mean by this. To see why this is the case, I'm sorry that I'm switching notations with i and t. For i's, I'm denoting discretized things and t. I'm just pointing at some general continuous time. and instantiations. So here, consider the following probabilistic graph in the context of diffusion models. So we know two conditional distributions p of y given x zero and p of x t given x zero. And for now, let's assume that the first one is the measurement distribution is given as typically Gaussian. So, and the second word, the four distribution of the diffusion is also Gaussian. However, the reverse distribution p of x zero given x t shown with blue dotted line is intractable in general. So hence p of y given x t is intractable because we have no information about this blue dotted line. So in our work, we aim to approximate the intractable distribution. The first key comes from the factorization. Since x zero is conditionally independent on y and x t, we can factor the integrand as follows. Where the former term is what we know and the latter term is what we partially know. And by partially known, I mean that we know how to obtain the posterior mean of the distribution, which is given by the 3ds formula. Note that 3ds formula is used widely in diffusion model context, as it states that the posterior mean, or the denoised estimate, can always be achieved when we know the so-called blurred score function. That is, the score function of the intermediate noisy variables x t. Here we see that we can plug in 3d denoising to achieve some posterior mean of the distribution in the context of the dpms. And to fully enjoy the effectiveness of 3ds formula, and because leaving the expectation outside is intractable, let us push the expectation inside for now. When we do that, by the 3ds formula proposition, we have a fully tractable distribution where the condition is now given by x zero hat, the denoised estimate from x t. Now, since we proposed an approximation here, it is important that we quantify the approximation bound. And for that, we have a theorem that states the approximation error, or the so-called Jensen gap, and used by pushing the expectation inside the function. And we show that this approximation error is upper bounded by this constant with 3 constituents. So, the latter two parts are usually bounded in most practical situation. And the interesting part here is the first one, where we see that when sigma goes to zero, the entire constant goes to zero. This is useful because it states that in cases where we have negligible measurement noise, the approximation will be tight. However, in practice, even when we have high measurement noise, the method also works very well. So, just summing everything up, thanks to theorem one, we can achieve what we call diffusion posterior sampling, or DPS in short. We start with standard Bayes rule in the context of diffusion models. We can apply theorem one to get the approximation. I lost him. I don't hear anything anymore. Do you hear me now? Yeah. Yeah. Yeah. Sorry. Where did the... I don't have any sound anymore. Yeah, now we're here. Yeah, now it's back. But we cannot see the screen. Can you try to share the screen again? I can see a screen. I can see the screen, the Gaussian and Poisson. Yeah, me too. Yeah. Can you try to just stop sharing again? We'll share it again. Yeah, sure. Thanks. So, is this where the connection was disconnected? Yeah, you were just starting about the Gaussian and Poisson. Yeah. So, I was saying that these gradients can be analytically computed because these functional forms are already known. So, we see that for Gaussian, we're essentially performing gradient descent that minimizes the squared L2 norm of the residual. I cannot see the slides, but I cannot see the screen. Yeah, neither. No. They just dropped away in the halfway through your sentence. Okay. So, let's try to reshare... Yeah, now I can see them. Yeah, me too. Thanks. So, does this work? Yes. It seems like... Yeah. So, yeah, I was saying that for Gaussian measurement models, we're trying to do gradient descent that minimizes the squared L2 norm of the residual, and for Poisson measurements, we're minimizing the squared weighted norm of the residual. So, incorporating ancestral sampling for DDPMs, we have our algorithm of DPS where we can derive separate algorithms according to the measurement model in hand. Note that line seven is where DPS takes place. If we were to remove line seven, we would simply be sampling from the prior distribution, the usual diffusion models, what they do. And hence, our algorithm is just very simple modification of the usual DDPM sampling. When we do that, we achieve these results. So, since our method is not dependent on the measurement model, we can apply the same score function to various problems. For example, this is the result of applying our method to super resolution that are contaminated with Gaussian noise. We can also apply our method to noisy in painting, and this is the case where 92% of the pixels are blocked out in a 256x256 image. This is noisy Gaussian deep blurring. We can do noisy motion deep blurring. And what's even more, since we're not restricted to the choice of the forward operator A, for the first time in the context of inverse problem solving with diffusion models, we show that we can also solve nonlinear inverse problems, such as phase retrieval presented here. And for those of you who do not know what phase retrieval is, it's a problem that tries to estimate the phase and the Fourier domain, and this is a notoriously hard problem because most of the information of the image is actually concentrated on the Fourier phase rather than the Fourier magnitude of an image. And we can also solve problems like non-uniform deep blurring, which is another nonlinear inverse problem, given that the forward measurement model is actually differentiable. This is one application that I want to highlight because we actually used a complex neural network that emulates the nonlinear blurring here. And neural network is one of the most nonlinear forward models that you can imagine. So even when the forward operator is highly nonlinear, we can see that DPS is capable of recovering the image with high fidelity. So that was it for DPS. I think we can briefly stop to see if any of you have any questions. I have a question. So basically, did you also show, because you can have, due to the stochasticity, you can have posterior distribution of the denoid samples, right? So do you notice a difference compared for the one you, the blur, for example, this image, do you get some deviations that are meaningful in one sense? Yeah, so I'm sorry that I don't have examples here, but yeah, you do see quite a bit of stochasticity, especially when the degradation is heavy as, for example, the example here, blurring is heavy. So the posterior samples have high standard deviation in terms of reconstructions. Yeah, that's another question. I don't know if you're going to explain it and maybe let me study that, but you also mentioned that reconstruction is possible if you have a prior, right? Right. But how do you choose the prior? Do you learn it or is it something that you'd have to? Oh yeah. So the thing that we're proposing here is that we're using the diffusion prior, right? So we're using this generative prior, and the prior is learned through the usual training process of diffusion models. So whether it be score matching or epsilon matching or whatever, we only, what we only need in the sampling process is the pre-trained score function s theta here. So all the examples that I show you here are generated by pre-trained models available online. So specifically, we use the open AI model, and we don't need any fine-tuning for any of these problems. We can just plug it into the solver and that will act as the prior of the distribution. Okay, so the prior is trained on the data that you use it on. So you use the simulation of the model pre-trained on different data. Yeah. Thanks. So how this method would differ from the paper also from song et al for inverse problems in medical imaging? So is there a likely difference? They are different and that will be covered more in detail in the second section where I explain the nearest paper. Yeah. So if you could summarize, for example, in a few words, because the main paper from song et al, you can also do inverse problems straightforward. I mean, you can do the painting. What would be like the main contribution is this non-linear approach to inverse problems, or what would be the main difference? So it's two things. I guess I'll try to explain it later. So you're mentioning the original paper of song et al at iClear 2021 and there was a paper that you mentioned that tackled medical imaging in the next year of the same author. They solved inverse problems by using a so-called projection approach, which means that you're directly replacing what you have as the measurements and you're keeping only the rest of it. So this is the visualization of such projection process. And this projection process works for certain inverse problems, for example, in painting. And for in painting, when you have a small degradation, for example, the mask is small or the blocked out pixels are not dominant. In those cases, projection approaches work fine. But for example, when you have a large mask, for example, a 128 by 128 mask cure, when you apply song's method, projection type approaches tend to fail dramatically. This is also relevant to the case of medical imaging, where you have high degradations. These projection-based approaches will typically fail. Whereas DPS that I proposed in the last section and the MCG that I will explain soon after does not have this property. And the methodological main difference is that you song and all use projection type approaches, whereas MCG or DPS uses gradient type approaches. So it's a smoother transition towards the that adapts to the measurement process. Yeah, that's pretty nice. Yeah, that's pretty clear. Thanks. Yeah. Okay. Should I move on to the next section? Yeah, thanks. Yeah, so the next paper that I'm going to talk about is actually a paper that came before DPS. But I explained DPS first because it really uses the same gradient, I would say similar, but almost the same gradient method with DPS. But at the time of development of MCG paper, we did not really understand how mathematically the solvers would be derived. So here in this paper, we focused more on the explanation in the geometric context of diffusion models. So I hope that this also helps the understanding of the underlying intuitive things about these gradients based methods. So in order to understand the paper, let us review some of the important properties of high dimensional Gaussian random variables. And specifically for very high dimensional Gaussian random variables, while the mode of the PDF may be near the main of the distribution, the measure is actually concentrated in the annulus that is distant from the center. This is the reason why when we add fixed variance Gaussian noise to an image, you never really see a clean image, even when the highest probability of a Gaussian noise would be zero noise. We always get images with very similar noise levels. Now extending that, let us think of a random variable Y that is corrupted with Gaussian noise by adding some noise. Sorry, can you maybe rephrase that? I mean, I think that's interesting, the slides before. So the concentration of Gaussian measure, right? Yeah, so this is a Gaussian annulus theorem, right? And then the effect that we see, what you said is that we don't see like, so we see the same effect, right? Or what do you mean? Definitely. So because of the Gaussian annulus theorem, what we see when we add Gaussian noise to images with, let's say, fixed variance Gaussian noise to an image, I guess the highest probability instantiation of this Gaussian distribution would be no noise, right? Zeroes. But that never happens. So if you add some random Gaussian noise to an image, you will always see some very similar noise level images that are corrupted in a very similar way. So that was what I was talking about. Yeah, okay. Yeah. So it's also due to the convolution, right? You can observe this as a convolution. Yeah. So this is kind of related to what I was talking about before. So let's say you have like a linear space of X. And what you would do is you add some Gaussian noise to that random variable X. Because this is a convolution, we can show that the marginal distribution P of Y is actually a, the measure actually concentrates on the hypercylinder that is distant from the center of PX. So this is kind of intuitive reason why the noisy images with the fixed variance have similar looking structure. So in this work, we propose to interpret diffusion models using some assumptions about the data manifold. So the usual manifold hypothesis states that the data lives in a low dimensional manifold with much smaller dimensionality than the ambient space. And in this work, we add an additional strong assumption that the manifold, the central data manifold is locally linear. And in order to leverage, and this is because we want to leverage the results from the concentration of Gaussian measure. And specifically, when we assume that our data manifold is locally linear, what we can show is that the geometry of diffusion model is given by the successive manifolds where there exists a clean data manifold in the center. And there are continuously many noisy manifolds where the noisy samples reside. So these are the manifolds of the noisy images. Extending that to a continuous limit, we propose that diffusion model is an onion because where when we push T to infinity, the distribution actually becomes a pure Gaussian. And since the measure of a high dimensional Gaussian resides in a Gaussian hypersphere, the intermediate manifolds are covers of the central manifolds that interpolates between these two. Therefore, this diffusion process would correspond to these red arrows, and the reverse diffusion would correspond to the blue arrows. And furthermore, when we are considering the case where we are trying to solve inverse problems with diffusion, we are left with the figure on the right. Here, we are not only trying to find points in the central manifold M, but we're trying to find the intersection between the manifold M and the line Y equals HX. So I've already said this earlier, but let's examine some of the representative methods that solve inverse problems using diffusion. These methods, Song at L and Choi at L, are a type of projection-based approach, since they directly replace the part that is known and keep only the unknown part. Another way to impose data consistency in the Bayesian framework is to incorporate likelihood. In other words, try to minimize the residual by gradient descent. However, with diffusion models, if we were to use naive gradient descent with a noisy residual, we will acquire no meaningful gradient, as the residual between HXI and Y will only be focused on the Gaussian noise part of the residual. Now, as we saw in diffusion posterior sampling, one can actually approximate the correct likelihood by switching to 2D denoised estimate X0 hat here. Here, visually and intuitively, we see why this may help. This is because after denoising, the residual is more likely to focus on the structural differences between HX0 hats and Y. In practice, using this gradient step over the projections work much, much better. And you can see the kind of the resulting differences when we use projection type approach and the gradient approaches. So in the paper, we properly analyze why this would be the case, leveraging the geometric understanding of diffusion models. So first, under our assumptions, we show that 2D denoising corresponds to an orthogonal projection to the data manifold. However, note that we have to consider two components of the data manifold, the orthogonal one that is related to the noise components, and the tangential one that is related to the data fidelity. So score function alone cannot handle the data fidelity. And here comes our theorem, which states that the proposed manifold constraint gradient term points to a direction that is tangential to the central data manifold. And therefore, our solution will move closer to Y equals HX every time we apply this MCG step. Another important aspect of this is that since MCG lets the sample move on the data manifold, whereas projections move the sample off the data manifold because projections are hard constraints that try to make Y minus HX equals 0. Note that MCG will not do that. It will move on the data manifolds in the direction tangential to the manifold. So those falling off the manifold will not happen. So here, note that the score functions that are pre-trained were not trained with samples that are off the data manifold. So they were trained with the samples that were on the noisy data manifolds. So it will not be able to properly denoise samples that are off the data manifolds. So by using projections, the intermediate error will probably accumulate, and we conjecture that this is the reason why projection-based approaches often fail to produce a good sample. But however, as I said, in the time we wrote MCG, there was a gap between theory and practice because even with the MCG steps, we had a hard time trying to solve inverse problems. So we had to resort to some projections within the denoising and the gradient steps. But in DPS, we managed to achieve a general solver that only uses these gradient steps. Now, for the Gaussian case, the only difference between MCG and DPS is that we do not use projections for DPS. Leveraging the geometric viewpoint of diffusion models, we can think that DPS is superior to MCG specifically because we avoid such projections. As projections here can potentially throw samples off the data manifold. Hence, DPS will typically be more stable by avoiding such throw-offs. And by the way, this is just an illustration. It doesn't really guarantee anything that our solver will always stay on the noisy data manifolds. Yeah, so that was about it for MCG. Any questions up until now? I just have a question, but maybe a very simple question. At the beginning of this slide, when you were explaining the, I think it's, no, the next one, two after this one. But yeah, this one, the previous one. How do you define this k? So n, what is n and what is k? I would have to look for the definition of n and k here. From my memory, n is the dimension of the ambient space, in case the dimension of the locally linear, low-dimensional dimensionality of the data manifold. I guess you're assuming that this, so the brown, the mean is basically used, the mean of each intermediate distribution. So it's the mean of the diffuse or denoise distribution. And then this, I'm just wondering, shouldn't you start going from a big standard deviation to a small standard deviation? Because basically when you do the denoising, you start with a high variance and then you start denoising and the variance slowly decreases. That's just something I was thinking that if that's not the case, so I'm missing out something. Can you repeat the question again? I'm not sure if I understood it correctly. Yeah, so the point of this is to interpret what's going on, like how the denoising samples behave in the data manifold through the trajectory. So when you do the denoising, so you start with the isotropic Gaussian, which is the highest entropy you can have. So you start with the maximum variance that you have and then slowly you start removing the noise and by doing that interactively the variance at each intermediate step goes down. So what I was expecting is to have this behavior when you have the mean and then the variance is large at the beginning and then starts decreasing. Because ideally you would reduce the variance, but I don't know if I'm missing something there. Oh, but I think it's what you're saying is kind of different from what this slide is saying. So I'm not really stating anything about denoising anything because the flow here is that we're first assuming that there exists a central data manifold M here. And then we're trying to visualize how the manifold of the noisy images would look like if there exists the central data manifold M. And I'm claiming that these noisy data manifolds will be covers of the central data manifold, where the distance between from the central data manifold will be defined by this constant value. So it's really not about starting from high variance Gaussian noise and reducing it to anything. We're starting from a clean image and then we're defining the manifolds of these of these noisy images. Okay, so I will have to look at that for I guess also more too. Yeah, thanks. Yeah. So I guess we just tell me if if I'm it's gonna get it's a good cliffhanger. It's first again. Um, do you hear me okay? Yeah, yes. Yeah, so I think it's about 48 minutes. So I don't think we have to go over the final section. So yeah. So I'll just try to we just we cannot see you or the slides anymore if you can do the same thing as before to stop sharing and start again. Yeah. Well, the talk is basically done. So okay, that was the last slide if we if we don't cover the last section. I'm not sure if this is the correct way of saying thank you very much in in Dutch. Maybe you should put it in Italian. Yeah, so yeah, that is the right way to say thank you. Thank you too.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.5200000000000005, "text": " Great. Yeah, so hi, I'm Hyunjin Chung. I'm a PhD student at bio-imaging and signal", "tokens": [50364, 3769, 13, 865, 11, 370, 4879, 11, 286, 478, 18398, 13970, 38314, 13, 286, 478, 257, 14476, 3107, 412, 12198, 12, 332, 3568, 293, 6358, 50740], "temperature": 0.0, "avg_logprob": -0.19007955571656587, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.088312067091465}, {"id": 1, "seek": 0, "start": 7.5200000000000005, "end": 13.200000000000001, "text": " processing and learning lab at KAIST Korea. I'm honored to give a talk in such a prestigious", "tokens": [50740, 9007, 293, 2539, 2715, 412, 31233, 19756, 6307, 13, 286, 478, 14556, 281, 976, 257, 751, 294, 1270, 257, 33510, 51024], "temperature": 0.0, "avg_logprob": -0.19007955571656587, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.088312067091465}, {"id": 2, "seek": 0, "start": 13.200000000000001, "end": 19.04, "text": " group in Netherlands. And so thank you very much for having me here. So today I'll give a talk", "tokens": [51024, 1594, 294, 20873, 13, 400, 370, 1309, 291, 588, 709, 337, 1419, 385, 510, 13, 407, 965, 286, 603, 976, 257, 751, 51316], "temperature": 0.0, "avg_logprob": -0.19007955571656587, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.088312067091465}, {"id": 3, "seek": 0, "start": 19.04, "end": 23.52, "text": " mostly about diffusion models and how to use them to solve inverse problems in imaging,", "tokens": [51316, 5240, 466, 25242, 5245, 293, 577, 281, 764, 552, 281, 5039, 17340, 2740, 294, 25036, 11, 51540], "temperature": 0.0, "avg_logprob": -0.19007955571656587, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.088312067091465}, {"id": 4, "seek": 2352, "start": 24.08, "end": 30.08, "text": " focusing on a few recent papers that I wrote. So specifically it will be focused on base and", "tokens": [50392, 8416, 322, 257, 1326, 5162, 10577, 300, 286, 4114, 13, 407, 4682, 309, 486, 312, 5178, 322, 3096, 293, 50692], "temperature": 0.0, "avg_logprob": -0.06760474457137886, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.256070613861084}, {"id": 5, "seek": 2352, "start": 30.08, "end": 35.44, "text": " inference in the context of inverse problem solving. So I believe it will be of quite a", "tokens": [50692, 38253, 294, 264, 4319, 295, 17340, 1154, 12606, 13, 407, 286, 1697, 309, 486, 312, 295, 1596, 257, 50960], "temperature": 0.0, "avg_logprob": -0.06760474457137886, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.256070613861084}, {"id": 6, "seek": 2352, "start": 35.44, "end": 40.8, "text": " relevance to you. And I'm pretty sure that most of you are already familiar with diffusion models,", "tokens": [50960, 32684, 281, 291, 13, 400, 286, 478, 1238, 988, 300, 881, 295, 291, 366, 1217, 4963, 365, 25242, 5245, 11, 51228], "temperature": 0.0, "avg_logprob": -0.06760474457137886, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.256070613861084}, {"id": 7, "seek": 2352, "start": 40.8, "end": 47.36, "text": " but some of you may not be familiar with inverse problems. So and since we have a rather small", "tokens": [51228, 457, 512, 295, 291, 815, 406, 312, 4963, 365, 17340, 2740, 13, 407, 293, 1670, 321, 362, 257, 2831, 1359, 51556], "temperature": 0.0, "avg_logprob": -0.06760474457137886, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.256070613861084}, {"id": 8, "seek": 4736, "start": 47.44, "end": 54.88, "text": " group, I feel free to speak up with your microphone. I'm very sure that I'm going to miss", "tokens": [50368, 1594, 11, 286, 841, 1737, 281, 1710, 493, 365, 428, 10952, 13, 286, 478, 588, 988, 300, 286, 478, 516, 281, 1713, 50740], "temperature": 0.0, "avg_logprob": -0.08861587859772063, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.039570197463035583}, {"id": 9, "seek": 4736, "start": 54.88, "end": 59.519999999999996, "text": " some chats. So I hope the talk will be as interactive as possible.", "tokens": [50740, 512, 38057, 13, 407, 286, 1454, 264, 751, 486, 312, 382, 15141, 382, 1944, 13, 50972], "temperature": 0.0, "avg_logprob": -0.08861587859772063, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.039570197463035583}, {"id": 10, "seek": 4736, "start": 62.72, "end": 68.88, "text": " Yeah, so the talk will be structured in a bit of an odd way because we're going to first talk about", "tokens": [51132, 865, 11, 370, 264, 751, 486, 312, 18519, 294, 257, 857, 295, 364, 7401, 636, 570, 321, 434, 516, 281, 700, 751, 466, 51440], "temperature": 0.0, "avg_logprob": -0.08861587859772063, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.039570197463035583}, {"id": 11, "seek": 4736, "start": 68.88, "end": 74.96000000000001, "text": " two closely related work in a reverse order. So the first part of the talk will be about a paper", "tokens": [51440, 732, 8185, 4077, 589, 294, 257, 9943, 1668, 13, 407, 264, 700, 644, 295, 264, 751, 486, 312, 466, 257, 3035, 51744], "temperature": 0.0, "avg_logprob": -0.08861587859772063, "compression_ratio": 1.6650943396226414, "no_speech_prob": 0.039570197463035583}, {"id": 12, "seek": 7496, "start": 74.96, "end": 81.19999999999999, "text": " named DPS, accepted to iClear recently. Then we will move backwards and talk about MCG,", "tokens": [50364, 4926, 413, 6273, 11, 9035, 281, 741, 34, 5797, 3938, 13, 1396, 321, 486, 1286, 12204, 293, 751, 466, 8797, 38, 11, 50676], "temperature": 0.0, "avg_logprob": -0.07841677946202895, "compression_ratio": 1.5062240663900415, "no_speech_prob": 0.0028437618166208267}, {"id": 13, "seek": 7496, "start": 81.19999999999999, "end": 85.91999999999999, "text": " which was supposed to be the main content of the talk today. And finally, if time allows,", "tokens": [50676, 597, 390, 3442, 281, 312, 264, 2135, 2701, 295, 264, 751, 965, 13, 400, 2721, 11, 498, 565, 4045, 11, 50912], "temperature": 0.0, "avg_logprob": -0.07841677946202895, "compression_ratio": 1.5062240663900415, "no_speech_prob": 0.0028437618166208267}, {"id": 14, "seek": 7496, "start": 85.91999999999999, "end": 91.44, "text": " I would also like to briefly touch the most recent work that applies these methods to more", "tokens": [50912, 286, 576, 611, 411, 281, 10515, 2557, 264, 881, 5162, 589, 300, 13165, 613, 7150, 281, 544, 51188], "temperature": 0.0, "avg_logprob": -0.07841677946202895, "compression_ratio": 1.5062240663900415, "no_speech_prob": 0.0028437618166208267}, {"id": 15, "seek": 7496, "start": 91.44, "end": 99.44, "text": " advanced problems called blind inverse problems. So I'll just briefly cover the basic concepts", "tokens": [51188, 7339, 2740, 1219, 6865, 17340, 2740, 13, 407, 286, 603, 445, 10515, 2060, 264, 3875, 10392, 51588], "temperature": 0.0, "avg_logprob": -0.07841677946202895, "compression_ratio": 1.5062240663900415, "no_speech_prob": 0.0028437618166208267}, {"id": 16, "seek": 9944, "start": 99.44, "end": 104.96, "text": " of diffusion models. I don't want to bore you too much, but I'll just speak in a briefly in", "tokens": [50364, 295, 25242, 5245, 13, 286, 500, 380, 528, 281, 26002, 291, 886, 709, 11, 457, 286, 603, 445, 1710, 294, 257, 10515, 294, 50640], "temperature": 0.0, "avg_logprob": -0.08277417088414098, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.003648721845820546}, {"id": 17, "seek": 9944, "start": 104.96, "end": 110.88, "text": " a score perspective. So what we do in diffusion models is we try to estimate the gradient of", "tokens": [50640, 257, 6175, 4585, 13, 407, 437, 321, 360, 294, 25242, 5245, 307, 321, 853, 281, 12539, 264, 16235, 295, 50936], "temperature": 0.0, "avg_logprob": -0.08277417088414098, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.003648721845820546}, {"id": 18, "seek": 9944, "start": 110.88, "end": 115.44, "text": " the lock density with the neural network S of theta, which points to the high density most of the", "tokens": [50936, 264, 4017, 10305, 365, 264, 18161, 3209, 318, 295, 9725, 11, 597, 2793, 281, 264, 1090, 10305, 881, 295, 264, 51164], "temperature": 0.0, "avg_logprob": -0.08277417088414098, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.003648721845820546}, {"id": 19, "seek": 9944, "start": 115.44, "end": 123.12, "text": " distribution. In order to circumvent the intractable explicit score matching, you train your network", "tokens": [51164, 7316, 13, 682, 1668, 281, 7125, 2475, 264, 560, 1897, 712, 13691, 6175, 14324, 11, 291, 3847, 428, 3209, 51548], "temperature": 0.0, "avg_logprob": -0.08277417088414098, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.003648721845820546}, {"id": 20, "seek": 9944, "start": 123.12, "end": 128.8, "text": " with denoising score matching, which boils down to essentially training a residual denoiser.", "tokens": [51548, 365, 1441, 78, 3436, 6175, 14324, 11, 597, 35049, 760, 281, 4476, 3097, 257, 27980, 1441, 78, 6694, 13, 51832], "temperature": 0.0, "avg_logprob": -0.08277417088414098, "compression_ratio": 1.7695167286245352, "no_speech_prob": 0.003648721845820546}, {"id": 21, "seek": 12944, "start": 130.4, "end": 136.72, "text": " Once that is done, you can use the train score functions to sample from the distribution p of x", "tokens": [50412, 3443, 300, 307, 1096, 11, 291, 393, 764, 264, 3847, 6175, 6828, 281, 6889, 490, 264, 7316, 280, 295, 2031, 50728], "temperature": 0.0, "avg_logprob": -0.14906119204115595, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.0001334048283752054}, {"id": 22, "seek": 12944, "start": 136.72, "end": 144.24, "text": " with, for example, Langevin MCMC. And another interesting view is that one can view the data", "tokens": [50728, 365, 11, 337, 1365, 11, 441, 933, 4796, 8797, 39261, 13, 400, 1071, 1880, 1910, 307, 300, 472, 393, 1910, 264, 1412, 51104], "temperature": 0.0, "avg_logprob": -0.14906119204115595, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.0001334048283752054}, {"id": 23, "seek": 12944, "start": 144.24, "end": 150.24, "text": " noising process as some linear forward SE, and the data generating process as the corresponding", "tokens": [51104, 572, 3436, 1399, 382, 512, 8213, 2128, 10269, 11, 293, 264, 1412, 17746, 1399, 382, 264, 11760, 51404], "temperature": 0.0, "avg_logprob": -0.14906119204115595, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.0001334048283752054}, {"id": 24, "seek": 12944, "start": 150.24, "end": 156.16, "text": " reverse stochastic differential equation, where the drift function is governed by the score function.", "tokens": [51404, 9943, 342, 8997, 2750, 15756, 5367, 11, 689, 264, 19699, 2445, 307, 35529, 538, 264, 6175, 2445, 13, 51700], "temperature": 0.0, "avg_logprob": -0.14906119204115595, "compression_ratio": 1.6355932203389831, "no_speech_prob": 0.0001334048283752054}, {"id": 25, "seek": 15616, "start": 156.72, "end": 162.07999999999998, "text": " Hence, when we want to sample data, you can discretize the reverse SDE and numerically", "tokens": [50392, 22229, 11, 562, 321, 528, 281, 6889, 1412, 11, 291, 393, 25656, 1125, 264, 9943, 14638, 36, 293, 7866, 984, 50660], "temperature": 0.0, "avg_logprob": -0.10435452518692936, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0007095810724422336}, {"id": 26, "seek": 15616, "start": 162.07999999999998, "end": 169.51999999999998, "text": " solve it using the pre-trained score function. On the other hand, here we are interested in", "tokens": [50660, 5039, 309, 1228, 264, 659, 12, 17227, 2001, 6175, 2445, 13, 1282, 264, 661, 1011, 11, 510, 321, 366, 3102, 294, 51032], "temperature": 0.0, "avg_logprob": -0.10435452518692936, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0007095810724422336}, {"id": 27, "seek": 15616, "start": 169.51999999999998, "end": 174.56, "text": " solving inverse problems. In the inverse problem setting, our aim is to recover the ground truth", "tokens": [51032, 12606, 17340, 2740, 13, 682, 264, 17340, 1154, 3287, 11, 527, 5939, 307, 281, 8114, 264, 2727, 3494, 51284], "temperature": 0.0, "avg_logprob": -0.10435452518692936, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0007095810724422336}, {"id": 28, "seek": 15616, "start": 174.56, "end": 179.04, "text": " x from the noisy measurements y obtained through some integral imaging system A.", "tokens": [51284, 2031, 490, 264, 24518, 15383, 288, 14879, 807, 512, 11573, 25036, 1185, 316, 13, 51508], "temperature": 0.0, "avg_logprob": -0.10435452518692936, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.0007095810724422336}, {"id": 29, "seek": 17904, "start": 180.0, "end": 186.56, "text": " And there will be some pollution of noise, and here, A, sorry, and the problem is naturally", "tokens": [50412, 400, 456, 486, 312, 512, 16727, 295, 5658, 11, 293, 510, 11, 316, 11, 2597, 11, 293, 264, 1154, 307, 8195, 50740], "temperature": 0.0, "avg_logprob": -0.18689630031585694, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.06555379182100296}, {"id": 30, "seek": 17904, "start": 186.56, "end": 190.88, "text": " opposed, which means that there exist infinitely many solutions to this problem,", "tokens": [50740, 8851, 11, 597, 1355, 300, 456, 2514, 36227, 867, 6547, 281, 341, 1154, 11, 50956], "temperature": 0.0, "avg_logprob": -0.18689630031585694, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.06555379182100296}, {"id": 31, "seek": 17904, "start": 192.07999999999998, "end": 199.28, "text": " meaning that given this noisy image, noisy blurred image of the leopard, there could exist", "tokens": [51016, 3620, 300, 2212, 341, 24518, 3256, 11, 24518, 43525, 3256, 295, 264, 47161, 11, 456, 727, 2514, 51376], "temperature": 0.0, "avg_logprob": -0.18689630031585694, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.06555379182100296}, {"id": 32, "seek": 17904, "start": 199.28, "end": 207.68, "text": " a lot of different feasible answers to the actual clean image. So in order to correctly specify", "tokens": [51376, 257, 688, 295, 819, 26648, 6338, 281, 264, 3539, 2541, 3256, 13, 407, 294, 1668, 281, 8944, 16500, 51796], "temperature": 0.0, "avg_logprob": -0.18689630031585694, "compression_ratio": 1.6775700934579438, "no_speech_prob": 0.06555379182100296}, {"id": 33, "seek": 20768, "start": 207.68, "end": 213.84, "text": " which one of the solutions is the one that we want, we need to specify the prior of the data", "tokens": [50364, 597, 472, 295, 264, 6547, 307, 264, 472, 300, 321, 528, 11, 321, 643, 281, 16500, 264, 4059, 295, 264, 1412, 50672], "temperature": 0.0, "avg_logprob": -0.0694383789511288, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.00020984013099223375}, {"id": 34, "seek": 20768, "start": 213.84, "end": 219.6, "text": " distribution, and in other words, how the images would actually look like in real world.", "tokens": [50672, 7316, 11, 293, 294, 661, 2283, 11, 577, 264, 5267, 576, 767, 574, 411, 294, 957, 1002, 13, 50960], "temperature": 0.0, "avg_logprob": -0.0694383789511288, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.00020984013099223375}, {"id": 35, "seek": 20768, "start": 222.16, "end": 228.16, "text": " Examples of such inverse problems include in-painting and deep learning in the context", "tokens": [51088, 48591, 295, 1270, 17340, 2740, 4090, 294, 12, 79, 491, 783, 293, 2452, 2539, 294, 264, 4319, 51388], "temperature": 0.0, "avg_logprob": -0.0694383789511288, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.00020984013099223375}, {"id": 36, "seek": 20768, "start": 228.16, "end": 232.64000000000001, "text": " of low-level vision. They're also called image restoration problems. And in the context of", "tokens": [51388, 295, 2295, 12, 12418, 5201, 13, 814, 434, 611, 1219, 3256, 23722, 2740, 13, 400, 294, 264, 4319, 295, 51612], "temperature": 0.0, "avg_logprob": -0.0694383789511288, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.00020984013099223375}, {"id": 37, "seek": 23264, "start": 232.64, "end": 237.76, "text": " biomedical imaging, there could be compressing MRI, sparsive CT, and so on.", "tokens": [50364, 49775, 25036, 11, 456, 727, 312, 14778, 278, 32812, 11, 637, 685, 488, 19529, 11, 293, 370, 322, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12822385267777878, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.00030531451920978725}, {"id": 38, "seek": 23264, "start": 240.39999999999998, "end": 244.64, "text": " So we'll go over how we can solve such inverse problems with diffusion models.", "tokens": [50752, 407, 321, 603, 352, 670, 577, 321, 393, 5039, 1270, 17340, 2740, 365, 25242, 5245, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12822385267777878, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.00030531451920978725}, {"id": 39, "seek": 23264, "start": 246.95999999999998, "end": 253.83999999999997, "text": " So let's first consider the following measurements model. Now, given y, what we want is to sample", "tokens": [51080, 407, 718, 311, 700, 1949, 264, 3480, 15383, 2316, 13, 823, 11, 2212, 288, 11, 437, 321, 528, 307, 281, 6889, 51424], "temperature": 0.0, "avg_logprob": -0.12822385267777878, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.00030531451920978725}, {"id": 40, "seek": 23264, "start": 253.83999999999997, "end": 261.36, "text": " from the posterior distribution p of x given y. And using Bayes' rule, it's straightforward to see", "tokens": [51424, 490, 264, 33529, 7316, 280, 295, 2031, 2212, 288, 13, 400, 1228, 7840, 279, 6, 4978, 11, 309, 311, 15325, 281, 536, 51800], "temperature": 0.0, "avg_logprob": -0.12822385267777878, "compression_ratio": 1.481012658227848, "no_speech_prob": 0.00030531451920978725}, {"id": 41, "seek": 26136, "start": 261.36, "end": 267.52000000000004, "text": " that the gradient of the log posterior is the sum of gradient of the log likelihood", "tokens": [50364, 300, 264, 16235, 295, 264, 3565, 33529, 307, 264, 2408, 295, 16235, 295, 264, 3565, 22119, 50672], "temperature": 0.0, "avg_logprob": -0.07767614906216845, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0012444788590073586}, {"id": 42, "seek": 26136, "start": 267.52000000000004, "end": 274.08000000000004, "text": " plus the score function. Here, gradient of the log likelihood in red gives us the information", "tokens": [50672, 1804, 264, 6175, 2445, 13, 1692, 11, 16235, 295, 264, 3565, 22119, 294, 2182, 2709, 505, 264, 1589, 51000], "temperature": 0.0, "avg_logprob": -0.07767614906216845, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0012444788590073586}, {"id": 43, "seek": 26136, "start": 274.08000000000004, "end": 279.68, "text": " about the measurement process. And the score function is what we've already estimated with", "tokens": [51000, 466, 264, 13160, 1399, 13, 400, 264, 6175, 2445, 307, 437, 321, 600, 1217, 14109, 365, 51280], "temperature": 0.0, "avg_logprob": -0.07767614906216845, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0012444788590073586}, {"id": 44, "seek": 26136, "start": 279.68, "end": 286.24, "text": " the neural network. So one should note that the likelihood function p of y given x is in most cases", "tokens": [51280, 264, 18161, 3209, 13, 407, 472, 820, 3637, 300, 264, 22119, 2445, 280, 295, 288, 2212, 2031, 307, 294, 881, 3331, 51608], "temperature": 0.0, "avg_logprob": -0.07767614906216845, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0012444788590073586}, {"id": 45, "seek": 28624, "start": 286.32, "end": 291.6, "text": " known. And in 95% of the cases, it can be modeled relatively well with the Gaussian.", "tokens": [50368, 2570, 13, 400, 294, 13420, 4, 295, 264, 3331, 11, 309, 393, 312, 37140, 7226, 731, 365, 264, 39148, 13, 50632], "temperature": 0.0, "avg_logprob": -0.08492893460153163, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.003427601419389248}, {"id": 46, "seek": 28624, "start": 292.24, "end": 298.72, "text": " And among the rest of the 5%, 4% is Poisson, meaning that the likelihood function is often known", "tokens": [50664, 400, 3654, 264, 1472, 295, 264, 1025, 8923, 1017, 4, 307, 6165, 30472, 11, 3620, 300, 264, 22119, 2445, 307, 2049, 2570, 50988], "temperature": 0.0, "avg_logprob": -0.08492893460153163, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.003427601419389248}, {"id": 47, "seek": 28624, "start": 298.72, "end": 304.08, "text": " a priority. So hence, in the perspective of Bayesian reconstruction, all we have to do when", "tokens": [50988, 257, 9365, 13, 407, 16678, 11, 294, 264, 4585, 295, 7840, 42434, 31565, 11, 439, 321, 362, 281, 360, 562, 51256], "temperature": 0.0, "avg_logprob": -0.08492893460153163, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.003427601419389248}, {"id": 48, "seek": 28624, "start": 304.08, "end": 309.92, "text": " posterior sampling is desirable is to specify the correct prior distribution. Then the measurement", "tokens": [51256, 33529, 21179, 307, 30533, 307, 281, 16500, 264, 3006, 4059, 7316, 13, 1396, 264, 13160, 51548], "temperature": 0.0, "avg_logprob": -0.08492893460153163, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.003427601419389248}, {"id": 49, "seek": 30992, "start": 309.92, "end": 318.56, "text": " process is hand wavy, it's known so we can sample from the posterior. So let's consider the case", "tokens": [50364, 1399, 307, 1011, 261, 15498, 11, 309, 311, 2570, 370, 321, 393, 6889, 490, 264, 33529, 13, 407, 718, 311, 1949, 264, 1389, 50796], "temperature": 0.0, "avg_logprob": -0.09640736526317811, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.000646124011836946}, {"id": 50, "seek": 30992, "start": 318.56, "end": 325.04, "text": " where we modeled the data distribution with the diffusion model. From the score or the SDE perspective,", "tokens": [50796, 689, 321, 37140, 264, 1412, 7316, 365, 264, 25242, 2316, 13, 3358, 264, 6175, 420, 264, 14638, 36, 4585, 11, 51120], "temperature": 0.0, "avg_logprob": -0.09640736526317811, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.000646124011836946}, {"id": 51, "seek": 30992, "start": 325.04, "end": 330.88, "text": " recall that the generative process is defined by this general form of reverse SDE. So in order", "tokens": [51120, 9901, 300, 264, 1337, 1166, 1399, 307, 7642, 538, 341, 2674, 1254, 295, 9943, 14638, 36, 13, 407, 294, 1668, 51412], "temperature": 0.0, "avg_logprob": -0.09640736526317811, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.000646124011836946}, {"id": 52, "seek": 30992, "start": 330.88, "end": 336.24, "text": " to do unconditional sampling from the prior distribution, we can use the following discretization", "tokens": [51412, 281, 360, 47916, 21179, 490, 264, 4059, 7316, 11, 321, 393, 764, 264, 3480, 25656, 2144, 51680], "temperature": 0.0, "avg_logprob": -0.09640736526317811, "compression_ratio": 1.7012987012987013, "no_speech_prob": 0.000646124011836946}, {"id": 53, "seek": 33624, "start": 336.32, "end": 342.96000000000004, "text": " steps of the reverse SDE. So for simplicity, I denoted Euler-Maruyama discretization here,", "tokens": [50368, 4439, 295, 264, 9943, 14638, 36, 13, 407, 337, 25632, 11, 286, 1441, 23325, 462, 26318, 12, 16639, 7493, 2404, 25656, 2144, 510, 11, 50700], "temperature": 0.0, "avg_logprob": -0.12649549596449908, "compression_ratio": 1.4622222222222223, "no_speech_prob": 0.0050584664568305016}, {"id": 54, "seek": 33624, "start": 342.96000000000004, "end": 348.0, "text": " but other forms of sampling can of course be used, such as ancestral sampling of DDPMs", "tokens": [50700, 457, 661, 6422, 295, 21179, 393, 295, 1164, 312, 1143, 11, 1270, 382, 40049, 21179, 295, 413, 11373, 26386, 50952], "temperature": 0.0, "avg_logprob": -0.12649549596449908, "compression_ratio": 1.4622222222222223, "no_speech_prob": 0.0050584664568305016}, {"id": 55, "seek": 33624, "start": 348.56, "end": 353.6, "text": " or hybrid methods that incorporate MCMC chains within the numerical integration solvers.", "tokens": [50980, 420, 13051, 7150, 300, 16091, 8797, 39261, 12626, 1951, 264, 29054, 10980, 1404, 840, 13, 51232], "temperature": 0.0, "avg_logprob": -0.12649549596449908, "compression_ratio": 1.4622222222222223, "no_speech_prob": 0.0050584664568305016}, {"id": 56, "seek": 33624, "start": 354.48, "end": 359.52, "text": " They're called predictor-corrector solvers in score SDE paper.", "tokens": [51276, 814, 434, 1219, 6069, 284, 12, 19558, 265, 1672, 1404, 840, 294, 6175, 14638, 36, 3035, 13, 51528], "temperature": 0.0, "avg_logprob": -0.12649549596449908, "compression_ratio": 1.4622222222222223, "no_speech_prob": 0.0050584664568305016}, {"id": 57, "seek": 35952, "start": 360.15999999999997, "end": 368.0, "text": " Moreover, when we want to do posterior sampling, we can use what we've derived from the Bayes rule", "tokens": [50396, 19838, 11, 562, 321, 528, 281, 360, 33529, 21179, 11, 321, 393, 764, 437, 321, 600, 18949, 490, 264, 7840, 279, 4978, 50788], "temperature": 0.0, "avg_logprob": -0.15984728003060947, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.002526573371142149}, {"id": 58, "seek": 35952, "start": 368.96, "end": 375.76, "text": " and incorporate the gradient of the walk likelihood. So here comes the problem.", "tokens": [50836, 293, 16091, 264, 16235, 295, 264, 1792, 22119, 13, 407, 510, 1487, 264, 1154, 13, 51176], "temperature": 0.0, "avg_logprob": -0.15984728003060947, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.002526573371142149}, {"id": 59, "seek": 35952, "start": 375.76, "end": 381.84, "text": " Can I ask a question? I'm sorry, I'm decently new to the diffusion model, so maybe there is", "tokens": [51176, 1664, 286, 1029, 257, 1168, 30, 286, 478, 2597, 11, 286, 478, 979, 2276, 777, 281, 264, 25242, 2316, 11, 370, 1310, 456, 307, 51480], "temperature": 0.0, "avg_logprob": -0.15984728003060947, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.002526573371142149}, {"id": 60, "seek": 35952, "start": 381.84, "end": 386.15999999999997, "text": " something I didn't understand. I don't understand the sign, why there is a minus sign in front of", "tokens": [51480, 746, 286, 994, 380, 1223, 13, 286, 500, 380, 1223, 264, 1465, 11, 983, 456, 307, 257, 3175, 1465, 294, 1868, 295, 51696], "temperature": 0.0, "avg_logprob": -0.15984728003060947, "compression_ratio": 1.5593220338983051, "no_speech_prob": 0.002526573371142149}, {"id": 61, "seek": 38616, "start": 386.24, "end": 389.68, "text": " the even the original paper, why there is a minus sign in front of the gradient of the", "tokens": [50368, 264, 754, 264, 3380, 3035, 11, 983, 456, 307, 257, 3175, 1465, 294, 1868, 295, 264, 16235, 295, 264, 50540], "temperature": 0.0, "avg_logprob": -0.17728212510032215, "compression_ratio": 1.5402843601895735, "no_speech_prob": 0.0017465361161157489}, {"id": 62, "seek": 38616, "start": 391.6, "end": 399.44000000000005, "text": " score. Oh, so you're asking, how did the reverse SDE come along here, right?", "tokens": [50636, 6175, 13, 876, 11, 370, 291, 434, 3365, 11, 577, 630, 264, 9943, 14638, 36, 808, 2051, 510, 11, 558, 30, 51028], "temperature": 0.0, "avg_logprob": -0.17728212510032215, "compression_ratio": 1.5402843601895735, "no_speech_prob": 0.0017465361161157489}, {"id": 63, "seek": 38616, "start": 400.0, "end": 408.32000000000005, "text": " Well, I'm saying that with that minus sign, it will do the opposite of going to the high", "tokens": [51056, 1042, 11, 286, 478, 1566, 300, 365, 300, 3175, 1465, 11, 309, 486, 360, 264, 6182, 295, 516, 281, 264, 1090, 51472], "temperature": 0.0, "avg_logprob": -0.17728212510032215, "compression_ratio": 1.5402843601895735, "no_speech_prob": 0.0017465361161157489}, {"id": 64, "seek": 38616, "start": 408.32000000000005, "end": 412.88, "text": " probability regions. It will go away from it. So am I missing something?", "tokens": [51472, 8482, 10682, 13, 467, 486, 352, 1314, 490, 309, 13, 407, 669, 286, 5361, 746, 30, 51700], "temperature": 0.0, "avg_logprob": -0.17728212510032215, "compression_ratio": 1.5402843601895735, "no_speech_prob": 0.0017465361161157489}, {"id": 65, "seek": 41288, "start": 413.84, "end": 420.08, "text": " Oh, yeah. So intuitively, if you want to sample from the high density modes of the", "tokens": [50412, 876, 11, 1338, 13, 407, 46506, 11, 498, 291, 528, 281, 6889, 490, 264, 1090, 10305, 14068, 295, 264, 50724], "temperature": 0.0, "avg_logprob": -0.18480932539787845, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0008955770754255354}, {"id": 66, "seek": 41288, "start": 420.08, "end": 427.68, "text": " distribution, you would kind of do gradient ascent with it. And yeah, there's a caveat here,", "tokens": [50724, 7316, 11, 291, 576, 733, 295, 360, 16235, 382, 2207, 365, 309, 13, 400, 1338, 11, 456, 311, 257, 43012, 510, 11, 51104], "temperature": 0.0, "avg_logprob": -0.18480932539787845, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0008955770754255354}, {"id": 67, "seek": 41288, "start": 427.68, "end": 435.6, "text": " because the differential dt here is a time running backwards. So you have to show the minus sign.", "tokens": [51104, 570, 264, 15756, 36423, 510, 307, 257, 565, 2614, 12204, 13, 407, 291, 362, 281, 855, 264, 3175, 1465, 13, 51500], "temperature": 0.0, "avg_logprob": -0.18480932539787845, "compression_ratio": 1.4756756756756757, "no_speech_prob": 0.0008955770754255354}, {"id": 68, "seek": 43560, "start": 435.68, "end": 442.96000000000004, "text": " Yeah. But then the sign of f is wrong, because the sign of f should have the same", "tokens": [50368, 865, 13, 583, 550, 264, 1465, 295, 283, 307, 2085, 11, 570, 264, 1465, 295, 283, 820, 362, 264, 912, 50732], "temperature": 0.0, "avg_logprob": -0.12543885786454756, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.006791544612497091}, {"id": 69, "seek": 43560, "start": 442.96000000000004, "end": 453.12, "text": " sign of g, doesn't it? Actually, no. So the derivation process of this reverse SDE comes from", "tokens": [50732, 1465, 295, 290, 11, 1177, 380, 309, 30, 5135, 11, 572, 13, 407, 264, 10151, 399, 1399, 295, 341, 9943, 14638, 36, 1487, 490, 51240], "temperature": 0.0, "avg_logprob": -0.12543885786454756, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.006791544612497091}, {"id": 70, "seek": 43560, "start": 454.16, "end": 460.48, "text": " the thing called Anderson's theorem. So if you plug in this general form of forward SDE,", "tokens": [51292, 264, 551, 1219, 18768, 311, 20904, 13, 407, 498, 291, 5452, 294, 341, 2674, 1254, 295, 2128, 14638, 36, 11, 51608], "temperature": 0.0, "avg_logprob": -0.12543885786454756, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.006791544612497091}, {"id": 71, "seek": 43560, "start": 460.48, "end": 464.96000000000004, "text": " then there's a theorem that states that this reverse SDE is the correct form.", "tokens": [51608, 550, 456, 311, 257, 20904, 300, 4368, 300, 341, 9943, 14638, 36, 307, 264, 3006, 1254, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12543885786454756, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.006791544612497091}, {"id": 72, "seek": 46496, "start": 464.96, "end": 469.84, "text": " I understand. But so if there isn't, let's say that there is no score, so there is no goal,", "tokens": [50364, 286, 1223, 13, 583, 370, 498, 456, 1943, 380, 11, 718, 311, 584, 300, 456, 307, 572, 6175, 11, 370, 456, 307, 572, 3387, 11, 50608], "temperature": 0.0, "avg_logprob": -0.1768187915577608, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0010226857848465443}, {"id": 73, "seek": 46496, "start": 469.84, "end": 475.2, "text": " then if you have a, let's say, a Oldston-Ulenbach process, the forward and reverse dynamics are the", "tokens": [50608, 550, 498, 291, 362, 257, 11, 718, 311, 584, 11, 257, 8633, 8805, 12, 52, 6698, 32096, 1399, 11, 264, 2128, 293, 9943, 15679, 366, 264, 50876], "temperature": 0.0, "avg_logprob": -0.1768187915577608, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0010226857848465443}, {"id": 74, "seek": 46496, "start": 475.2, "end": 490.0, "text": " same. So you're saying that f dt is running backwards, then f should be negative. If you", "tokens": [50876, 912, 13, 407, 291, 434, 1566, 300, 283, 36423, 307, 2614, 12204, 11, 550, 283, 820, 312, 3671, 13, 759, 291, 51616], "temperature": 0.0, "avg_logprob": -0.1768187915577608, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0010226857848465443}, {"id": 75, "seek": 46496, "start": 490.0, "end": 494.32, "text": " have a stationary process like an Oldston-Ulenbach, regardless whether you run it forward or", "tokens": [51616, 362, 257, 30452, 1399, 411, 364, 8633, 8805, 12, 52, 6698, 32096, 11, 10060, 1968, 291, 1190, 309, 2128, 420, 51832], "temperature": 0.0, "avg_logprob": -0.1768187915577608, "compression_ratio": 1.7110091743119267, "no_speech_prob": 0.0010226857848465443}, {"id": 76, "seek": 49432, "start": 494.32, "end": 499.44, "text": " backward, you have the same dynamics. Therefore, you are not going to flip the sign of f if you", "tokens": [50364, 23897, 11, 291, 362, 264, 912, 15679, 13, 7504, 11, 291, 366, 406, 516, 281, 7929, 264, 1465, 295, 283, 498, 291, 50620], "temperature": 0.0, "avg_logprob": -0.09992137322059044, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.0037912530824542046}, {"id": 77, "seek": 49432, "start": 499.44, "end": 515.4399999999999, "text": " change the time. Let's say, in an Oldston-Ulenbach process, f induces the mean reversion. So if", "tokens": [50620, 1319, 264, 565, 13, 961, 311, 584, 11, 294, 364, 8633, 8805, 12, 52, 6698, 32096, 1399, 11, 283, 13716, 887, 264, 914, 14582, 313, 13, 407, 498, 51420], "temperature": 0.0, "avg_logprob": -0.09992137322059044, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.0037912530824542046}, {"id": 78, "seek": 49432, "start": 515.4399999999999, "end": 519.52, "text": " you actually flip the sign, you will have the process going away from the mean, which is not", "tokens": [51420, 291, 767, 7929, 264, 1465, 11, 291, 486, 362, 264, 1399, 516, 1314, 490, 264, 914, 11, 597, 307, 406, 51624], "temperature": 0.0, "avg_logprob": -0.09992137322059044, "compression_ratio": 1.5865921787709498, "no_speech_prob": 0.0037912530824542046}, {"id": 79, "seek": 51952, "start": 519.52, "end": 524.88, "text": " the reverse time process. The reverse time process wants to go back to the mean exactly like the", "tokens": [50364, 264, 9943, 565, 1399, 13, 440, 9943, 565, 1399, 2738, 281, 352, 646, 281, 264, 914, 2293, 411, 264, 50632], "temperature": 0.0, "avg_logprob": -0.11272047505234227, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.015229936689138412}, {"id": 80, "seek": 51952, "start": 524.88, "end": 530.64, "text": " forward process. Right, right. I'm also a bit confused myself.", "tokens": [50632, 2128, 1399, 13, 1779, 11, 558, 13, 286, 478, 611, 257, 857, 9019, 2059, 13, 50920], "temperature": 0.0, "avg_logprob": -0.11272047505234227, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.015229936689138412}, {"id": 81, "seek": 51952, "start": 534.0799999999999, "end": 540.48, "text": " Because everything works, I think, I don't know. So again, I could be misinterpreting something.", "tokens": [51092, 1436, 1203, 1985, 11, 286, 519, 11, 286, 500, 380, 458, 13, 407, 797, 11, 286, 727, 312, 3346, 5106, 3712, 783, 746, 13, 51412], "temperature": 0.0, "avg_logprob": -0.11272047505234227, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.015229936689138412}, {"id": 82, "seek": 54048, "start": 541.44, "end": 548.64, "text": " Because I do see this in papers, but I just,", "tokens": [50412, 1436, 286, 360, 536, 341, 294, 10577, 11, 457, 286, 445, 11, 50772], "temperature": 0.0, "avg_logprob": -0.22805278027643922, "compression_ratio": 1.3312101910828025, "no_speech_prob": 0.005918008740991354}, {"id": 83, "seek": 54048, "start": 553.6800000000001, "end": 558.8000000000001, "text": " it's, again, if it's an Oldston-Ulenbach process, the forward and reverse dynamics,", "tokens": [51024, 309, 311, 11, 797, 11, 498, 309, 311, 364, 8633, 8805, 12, 52, 6698, 32096, 1399, 11, 264, 2128, 293, 9943, 15679, 11, 51280], "temperature": 0.0, "avg_logprob": -0.22805278027643922, "compression_ratio": 1.3312101910828025, "no_speech_prob": 0.005918008740991354}, {"id": 84, "seek": 54048, "start": 559.84, "end": 563.36, "text": " assuming score zero, should be the same. So you cannot flip the sign. Otherwise,", "tokens": [51332, 11926, 6175, 4018, 11, 820, 312, 264, 912, 13, 407, 291, 2644, 7929, 264, 1465, 13, 10328, 11, 51508], "temperature": 0.0, "avg_logprob": -0.22805278027643922, "compression_ratio": 1.3312101910828025, "no_speech_prob": 0.005918008740991354}, {"id": 85, "seek": 56336, "start": 563.44, "end": 571.84, "text": " you get the opposite, you get the diverging process. And if you assume the generative", "tokens": [50368, 291, 483, 264, 6182, 11, 291, 483, 264, 18558, 3249, 1399, 13, 400, 498, 291, 6552, 264, 1337, 1166, 50788], "temperature": 0.0, "avg_logprob": -0.1325227571570355, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.01677705906331539}, {"id": 86, "seek": 56336, "start": 571.84, "end": 575.92, "text": " direction to be forward, then the sign of the score should be plus, not minus. But of course,", "tokens": [50788, 3513, 281, 312, 2128, 11, 550, 264, 1465, 295, 264, 6175, 820, 312, 1804, 11, 406, 3175, 13, 583, 295, 1164, 11, 50992], "temperature": 0.0, "avg_logprob": -0.1325227571570355, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.01677705906331539}, {"id": 87, "seek": 56336, "start": 575.92, "end": 580.72, "text": " it would become minus if you are actually considering that going backward. But okay,", "tokens": [50992, 309, 576, 1813, 3175, 498, 291, 366, 767, 8079, 300, 516, 23897, 13, 583, 1392, 11, 51232], "temperature": 0.0, "avg_logprob": -0.1325227571570355, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.01677705906331539}, {"id": 88, "seek": 56336, "start": 580.72, "end": 585.6800000000001, "text": " but maybe we can discuss this offline. And also, I mean, again, it could be that I'm missing some", "tokens": [51232, 457, 1310, 321, 393, 2248, 341, 21857, 13, 400, 611, 11, 286, 914, 11, 797, 11, 309, 727, 312, 300, 286, 478, 5361, 512, 51480], "temperature": 0.0, "avg_logprob": -0.1325227571570355, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.01677705906331539}, {"id": 89, "seek": 56336, "start": 585.6800000000001, "end": 592.4, "text": " details of the notation that for which things will match up. I'm not sure if, especially in the case", "tokens": [51480, 4365, 295, 264, 24657, 300, 337, 597, 721, 486, 2995, 493, 13, 286, 478, 406, 988, 498, 11, 2318, 294, 264, 1389, 51816], "temperature": 0.0, "avg_logprob": -0.1325227571570355, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.01677705906331539}, {"id": 90, "seek": 59240, "start": 592.4, "end": 599.68, "text": " of Ornstein-Ulenbach process, is there, is the score function zero there? Well, so the,", "tokens": [50364, 295, 1610, 77, 9089, 12, 52, 6698, 32096, 1399, 11, 307, 456, 11, 307, 264, 6175, 2445, 4018, 456, 30, 1042, 11, 370, 264, 11, 50728], "temperature": 0.0, "avg_logprob": -0.2123011386755741, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.002333646407350898}, {"id": 91, "seek": 59240, "start": 601.1999999999999, "end": 608.56, "text": " well, so the, if the forward process is Oldston-Ulenbach, then f of x of t is just minus alpha x.", "tokens": [50804, 731, 11, 370, 264, 11, 498, 264, 2128, 1399, 307, 8633, 8805, 12, 52, 6698, 32096, 11, 550, 283, 295, 2031, 295, 256, 307, 445, 3175, 8961, 2031, 13, 51172], "temperature": 0.0, "avg_logprob": -0.2123011386755741, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.002333646407350898}, {"id": 92, "seek": 59240, "start": 609.1999999999999, "end": 613.92, "text": " Right. And that would be true both in the forward and in the backward,", "tokens": [51204, 1779, 13, 400, 300, 576, 312, 2074, 1293, 294, 264, 2128, 293, 294, 264, 23897, 11, 51440], "temperature": 0.0, "avg_logprob": -0.2123011386755741, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.002333646407350898}, {"id": 93, "seek": 59240, "start": 613.92, "end": 617.28, "text": " because again, it's time invariant. If you flip, you don't change anything.", "tokens": [51440, 570, 797, 11, 309, 311, 565, 33270, 394, 13, 759, 291, 7929, 11, 291, 500, 380, 1319, 1340, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2123011386755741, "compression_ratio": 1.5660377358490567, "no_speech_prob": 0.002333646407350898}, {"id": 94, "seek": 61728, "start": 618.24, "end": 623.28, "text": " The score happened in order to enforce the terminal condition that the process has to fit the data.", "tokens": [50412, 440, 6175, 2011, 294, 1668, 281, 24825, 264, 14709, 4188, 300, 264, 1399, 575, 281, 3318, 264, 1412, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17159642978590361, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011759570334106684}, {"id": 95, "seek": 61728, "start": 626.16, "end": 630.48, "text": " So it, in this sense, it's nothing to do with the Ornstein-Ulenbach process. It's just a", "tokens": [50808, 407, 309, 11, 294, 341, 2020, 11, 309, 311, 1825, 281, 360, 365, 264, 1610, 77, 9089, 12, 52, 6698, 32096, 1399, 13, 467, 311, 445, 257, 51024], "temperature": 0.0, "avg_logprob": -0.17159642978590361, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011759570334106684}, {"id": 96, "seek": 61728, "start": 632.0799999999999, "end": 637.52, "text": " starting point of the process, which if then, if you want to reverse, it will actually go back to", "tokens": [51104, 2891, 935, 295, 264, 1399, 11, 597, 498, 550, 11, 498, 291, 528, 281, 9943, 11, 309, 486, 767, 352, 646, 281, 51376], "temperature": 0.0, "avg_logprob": -0.17159642978590361, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011759570334106684}, {"id": 97, "seek": 61728, "start": 637.52, "end": 643.1999999999999, "text": " there. Yeah, I think we should come back to this later. I think it's better to discuss", "tokens": [51376, 456, 13, 865, 11, 286, 519, 321, 820, 808, 646, 281, 341, 1780, 13, 286, 519, 309, 311, 1101, 281, 2248, 51660], "temperature": 0.0, "avg_logprob": -0.17159642978590361, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.0011759570334106684}, {"id": 98, "seek": 64728, "start": 648.16, "end": 656.48, "text": " So coming back to where we were, I think it was right here. So I was talking about how we could", "tokens": [50408, 407, 1348, 646, 281, 689, 321, 645, 11, 286, 519, 309, 390, 558, 510, 13, 407, 286, 390, 1417, 466, 577, 321, 727, 50824], "temperature": 0.0, "avg_logprob": -0.12524264509027655, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00032500529778189957}, {"id": 99, "seek": 64728, "start": 657.04, "end": 662.64, "text": " switch from prior sampling to posterior sampling by, by just plugging, plugging in this base rule.", "tokens": [50852, 3679, 490, 4059, 21179, 281, 33529, 21179, 538, 11, 538, 445, 42975, 11, 42975, 294, 341, 3096, 4978, 13, 51132], "temperature": 0.0, "avg_logprob": -0.12524264509027655, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00032500529778189957}, {"id": 100, "seek": 64728, "start": 663.68, "end": 670.16, "text": " And here I said that care must be taken here because the gradient of the log likelihood", "tokens": [51184, 400, 510, 286, 848, 300, 1127, 1633, 312, 2726, 510, 570, 264, 16235, 295, 264, 3565, 22119, 51508], "temperature": 0.0, "avg_logprob": -0.12524264509027655, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00032500529778189957}, {"id": 101, "seek": 64728, "start": 670.16, "end": 676.64, "text": " is in fact intractable. And note that this is different from my claim earlier that the", "tokens": [51508, 307, 294, 1186, 560, 1897, 712, 13, 400, 3637, 300, 341, 307, 819, 490, 452, 3932, 3071, 300, 264, 51832], "temperature": 0.0, "avg_logprob": -0.12524264509027655, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00032500529778189957}, {"id": 102, "seek": 67664, "start": 676.64, "end": 682.3199999999999, "text": " likelihood function is in most cases known. And this is, this arises from the fact that", "tokens": [50364, 22119, 2445, 307, 294, 881, 3331, 2570, 13, 400, 341, 307, 11, 341, 27388, 490, 264, 1186, 300, 50648], "temperature": 0.0, "avg_logprob": -0.11424966734282825, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00012729557056445628}, {"id": 103, "seek": 67664, "start": 682.3199999999999, "end": 689.1999999999999, "text": " there are noisy x i's here rather than x zero. So let us dive into closely examine what I mean by this.", "tokens": [50648, 456, 366, 24518, 2031, 741, 311, 510, 2831, 813, 2031, 4018, 13, 407, 718, 505, 9192, 666, 8185, 17496, 437, 286, 914, 538, 341, 13, 50992], "temperature": 0.0, "avg_logprob": -0.11424966734282825, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00012729557056445628}, {"id": 104, "seek": 67664, "start": 690.4, "end": 697.12, "text": " To see why this is the case, I'm sorry that I'm switching notations with i and t. For i's,", "tokens": [51052, 1407, 536, 983, 341, 307, 264, 1389, 11, 286, 478, 2597, 300, 286, 478, 16493, 406, 763, 365, 741, 293, 256, 13, 1171, 741, 311, 11, 51388], "temperature": 0.0, "avg_logprob": -0.11424966734282825, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00012729557056445628}, {"id": 105, "seek": 67664, "start": 697.12, "end": 706.48, "text": " I'm denoting discretized things and t. I'm just pointing at some general continuous time.", "tokens": [51388, 286, 478, 1441, 17001, 25656, 1602, 721, 293, 256, 13, 286, 478, 445, 12166, 412, 512, 2674, 10957, 565, 13, 51856], "temperature": 0.0, "avg_logprob": -0.11424966734282825, "compression_ratio": 1.6103896103896105, "no_speech_prob": 0.00012729557056445628}, {"id": 106, "seek": 70664, "start": 706.88, "end": 713.84, "text": " and instantiations. So here, consider the following probabilistic graph in the context of diffusion", "tokens": [50376, 293, 9836, 72, 763, 13, 407, 510, 11, 1949, 264, 3480, 31959, 3142, 4295, 294, 264, 4319, 295, 25242, 50724], "temperature": 0.0, "avg_logprob": -0.1560437626308865, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0010985431727021933}, {"id": 107, "seek": 70664, "start": 713.84, "end": 720.48, "text": " models. So we know two conditional distributions p of y given x zero and p of x t given x zero.", "tokens": [50724, 5245, 13, 407, 321, 458, 732, 27708, 37870, 280, 295, 288, 2212, 2031, 4018, 293, 280, 295, 2031, 256, 2212, 2031, 4018, 13, 51056], "temperature": 0.0, "avg_logprob": -0.1560437626308865, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0010985431727021933}, {"id": 108, "seek": 70664, "start": 721.1999999999999, "end": 726.0, "text": " And for now, let's assume that the first one is the measurement distribution is given as typically", "tokens": [51092, 400, 337, 586, 11, 718, 311, 6552, 300, 264, 700, 472, 307, 264, 13160, 7316, 307, 2212, 382, 5850, 51332], "temperature": 0.0, "avg_logprob": -0.1560437626308865, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0010985431727021933}, {"id": 109, "seek": 70664, "start": 726.0, "end": 732.48, "text": " Gaussian. So, and the second word, the four distribution of the diffusion is also Gaussian.", "tokens": [51332, 39148, 13, 407, 11, 293, 264, 1150, 1349, 11, 264, 1451, 7316, 295, 264, 25242, 307, 611, 39148, 13, 51656], "temperature": 0.0, "avg_logprob": -0.1560437626308865, "compression_ratio": 1.7309417040358743, "no_speech_prob": 0.0010985431727021933}, {"id": 110, "seek": 73248, "start": 733.44, "end": 739.36, "text": " However, the reverse distribution p of x zero given x t shown with blue dotted line is intractable", "tokens": [50412, 2908, 11, 264, 9943, 7316, 280, 295, 2031, 4018, 2212, 2031, 256, 4898, 365, 3344, 37459, 1622, 307, 560, 1897, 712, 50708], "temperature": 0.0, "avg_logprob": -0.08193498187594944, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.0008829272701404989}, {"id": 111, "seek": 73248, "start": 739.36, "end": 746.0, "text": " in general. So hence p of y given x t is intractable because we have no information about this blue", "tokens": [50708, 294, 2674, 13, 407, 16678, 280, 295, 288, 2212, 2031, 256, 307, 560, 1897, 712, 570, 321, 362, 572, 1589, 466, 341, 3344, 51040], "temperature": 0.0, "avg_logprob": -0.08193498187594944, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.0008829272701404989}, {"id": 112, "seek": 73248, "start": 746.0, "end": 753.6, "text": " dotted line. So in our work, we aim to approximate the intractable distribution.", "tokens": [51040, 37459, 1622, 13, 407, 294, 527, 589, 11, 321, 5939, 281, 30874, 264, 560, 1897, 712, 7316, 13, 51420], "temperature": 0.0, "avg_logprob": -0.08193498187594944, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.0008829272701404989}, {"id": 113, "seek": 73248, "start": 755.2, "end": 761.76, "text": " The first key comes from the factorization. Since x zero is conditionally independent on y", "tokens": [51500, 440, 700, 2141, 1487, 490, 264, 5952, 2144, 13, 4162, 2031, 4018, 307, 4188, 379, 6695, 322, 288, 51828], "temperature": 0.0, "avg_logprob": -0.08193498187594944, "compression_ratio": 1.7370892018779343, "no_speech_prob": 0.0008829272701404989}, {"id": 114, "seek": 76176, "start": 761.76, "end": 768.4, "text": " and x t, we can factor the integrand as follows. Where the former term is what we know and the", "tokens": [50364, 293, 2031, 256, 11, 321, 393, 5952, 264, 16200, 3699, 382, 10002, 13, 2305, 264, 5819, 1433, 307, 437, 321, 458, 293, 264, 50696], "temperature": 0.0, "avg_logprob": -0.10172210137049358, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0010814957786351442}, {"id": 115, "seek": 76176, "start": 768.4, "end": 774.56, "text": " latter term is what we partially know. And by partially known, I mean that we know how to obtain", "tokens": [50696, 18481, 1433, 307, 437, 321, 18886, 458, 13, 400, 538, 18886, 2570, 11, 286, 914, 300, 321, 458, 577, 281, 12701, 51004], "temperature": 0.0, "avg_logprob": -0.10172210137049358, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0010814957786351442}, {"id": 116, "seek": 76176, "start": 774.56, "end": 781.36, "text": " the posterior mean of the distribution, which is given by the 3ds formula. Note that 3ds formula", "tokens": [51004, 264, 33529, 914, 295, 264, 7316, 11, 597, 307, 2212, 538, 264, 805, 16063, 8513, 13, 11633, 300, 805, 16063, 8513, 51344], "temperature": 0.0, "avg_logprob": -0.10172210137049358, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0010814957786351442}, {"id": 117, "seek": 76176, "start": 781.36, "end": 787.4399999999999, "text": " is used widely in diffusion model context, as it states that the posterior mean, or the denoised", "tokens": [51344, 307, 1143, 13371, 294, 25242, 2316, 4319, 11, 382, 309, 4368, 300, 264, 33529, 914, 11, 420, 264, 1441, 78, 2640, 51648], "temperature": 0.0, "avg_logprob": -0.10172210137049358, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.0010814957786351442}, {"id": 118, "seek": 78744, "start": 787.44, "end": 793.2, "text": " estimate, can always be achieved when we know the so-called blurred score function. That is,", "tokens": [50364, 12539, 11, 393, 1009, 312, 11042, 562, 321, 458, 264, 370, 12, 11880, 43525, 6175, 2445, 13, 663, 307, 11, 50652], "temperature": 0.0, "avg_logprob": -0.15745137101512843, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.0005356835317797959}, {"id": 119, "seek": 78744, "start": 793.9200000000001, "end": 797.5200000000001, "text": " the score function of the intermediate noisy variables x t.", "tokens": [50688, 264, 6175, 2445, 295, 264, 19376, 24518, 9102, 2031, 256, 13, 50868], "temperature": 0.0, "avg_logprob": -0.15745137101512843, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.0005356835317797959}, {"id": 120, "seek": 78744, "start": 801.84, "end": 809.5200000000001, "text": " Here we see that we can plug in 3d denoising to achieve some posterior mean of the", "tokens": [51084, 1692, 321, 536, 300, 321, 393, 5452, 294, 805, 67, 1441, 78, 3436, 281, 4584, 512, 33529, 914, 295, 264, 51468], "temperature": 0.0, "avg_logprob": -0.15745137101512843, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.0005356835317797959}, {"id": 121, "seek": 80952, "start": 810.16, "end": 818.3199999999999, "text": " distribution in the context of the dpms. And to fully enjoy the effectiveness of 3ds formula,", "tokens": [50396, 7316, 294, 264, 4319, 295, 264, 274, 79, 2592, 13, 400, 281, 4498, 2103, 264, 21208, 295, 805, 16063, 8513, 11, 50804], "temperature": 0.0, "avg_logprob": -0.09018353665812631, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0028889942914247513}, {"id": 122, "seek": 80952, "start": 818.3199999999999, "end": 824.24, "text": " and because leaving the expectation outside is intractable, let us push the expectation inside", "tokens": [50804, 293, 570, 5012, 264, 14334, 2380, 307, 560, 1897, 712, 11, 718, 505, 2944, 264, 14334, 1854, 51100], "temperature": 0.0, "avg_logprob": -0.09018353665812631, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0028889942914247513}, {"id": 123, "seek": 80952, "start": 824.24, "end": 832.8, "text": " for now. When we do that, by the 3ds formula proposition, we have a fully tractable distribution", "tokens": [51100, 337, 586, 13, 1133, 321, 360, 300, 11, 538, 264, 805, 16063, 8513, 24830, 11, 321, 362, 257, 4498, 24207, 712, 7316, 51528], "temperature": 0.0, "avg_logprob": -0.09018353665812631, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0028889942914247513}, {"id": 124, "seek": 80952, "start": 832.8, "end": 837.84, "text": " where the condition is now given by x zero hat, the denoised estimate from x t.", "tokens": [51528, 689, 264, 4188, 307, 586, 2212, 538, 2031, 4018, 2385, 11, 264, 1441, 78, 2640, 12539, 490, 2031, 256, 13, 51780], "temperature": 0.0, "avg_logprob": -0.09018353665812631, "compression_ratio": 1.7136150234741785, "no_speech_prob": 0.0028889942914247513}, {"id": 125, "seek": 83952, "start": 840.48, "end": 845.92, "text": " Now, since we proposed an approximation here, it is important that we quantify the approximation", "tokens": [50412, 823, 11, 1670, 321, 10348, 364, 28023, 510, 11, 309, 307, 1021, 300, 321, 40421, 264, 28023, 50684], "temperature": 0.0, "avg_logprob": -0.13116901735716227, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0001313348620897159}, {"id": 126, "seek": 83952, "start": 845.92, "end": 850.48, "text": " bound. And for that, we have a theorem that states the approximation error,", "tokens": [50684, 5472, 13, 400, 337, 300, 11, 321, 362, 257, 20904, 300, 4368, 264, 28023, 6713, 11, 50912], "temperature": 0.0, "avg_logprob": -0.13116901735716227, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0001313348620897159}, {"id": 127, "seek": 83952, "start": 850.48, "end": 855.1999999999999, "text": " or the so-called Jensen gap, and used by pushing the expectation inside the function.", "tokens": [50912, 420, 264, 370, 12, 11880, 508, 32934, 7417, 11, 293, 1143, 538, 7380, 264, 14334, 1854, 264, 2445, 13, 51148], "temperature": 0.0, "avg_logprob": -0.13116901735716227, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0001313348620897159}, {"id": 128, "seek": 83952, "start": 856.72, "end": 863.52, "text": " And we show that this approximation error is upper bounded by this constant with 3 constituents.", "tokens": [51224, 400, 321, 855, 300, 341, 28023, 6713, 307, 6597, 37498, 538, 341, 5754, 365, 805, 30847, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13116901735716227, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0001313348620897159}, {"id": 129, "seek": 86352, "start": 864.48, "end": 868.88, "text": " So, the latter two parts are usually bounded in most practical situation.", "tokens": [50412, 407, 11, 264, 18481, 732, 3166, 366, 2673, 37498, 294, 881, 8496, 2590, 13, 50632], "temperature": 0.0, "avg_logprob": -0.1035753768167378, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0022867689840495586}, {"id": 130, "seek": 86352, "start": 869.68, "end": 877.1999999999999, "text": " And the interesting part here is the first one, where we see that when sigma goes to zero,", "tokens": [50672, 400, 264, 1880, 644, 510, 307, 264, 700, 472, 11, 689, 321, 536, 300, 562, 12771, 1709, 281, 4018, 11, 51048], "temperature": 0.0, "avg_logprob": -0.1035753768167378, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0022867689840495586}, {"id": 131, "seek": 86352, "start": 877.1999999999999, "end": 882.96, "text": " the entire constant goes to zero. This is useful because it states that in cases where we have", "tokens": [51048, 264, 2302, 5754, 1709, 281, 4018, 13, 639, 307, 4420, 570, 309, 4368, 300, 294, 3331, 689, 321, 362, 51336], "temperature": 0.0, "avg_logprob": -0.1035753768167378, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0022867689840495586}, {"id": 132, "seek": 86352, "start": 882.96, "end": 888.48, "text": " negligible measurement noise, the approximation will be tight. However, in practice, even when", "tokens": [51336, 32570, 964, 13160, 5658, 11, 264, 28023, 486, 312, 4524, 13, 2908, 11, 294, 3124, 11, 754, 562, 51612], "temperature": 0.0, "avg_logprob": -0.1035753768167378, "compression_ratio": 1.6238532110091743, "no_speech_prob": 0.0022867689840495586}, {"id": 133, "seek": 88848, "start": 888.48, "end": 897.76, "text": " we have high measurement noise, the method also works very well. So, just summing everything up,", "tokens": [50364, 321, 362, 1090, 13160, 5658, 11, 264, 3170, 611, 1985, 588, 731, 13, 407, 11, 445, 2408, 2810, 1203, 493, 11, 50828], "temperature": 0.0, "avg_logprob": -0.0885773213704427, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.0003459026920609176}, {"id": 134, "seek": 88848, "start": 897.76, "end": 902.0, "text": " thanks to theorem one, we can achieve what we call diffusion posterior sampling,", "tokens": [50828, 3231, 281, 20904, 472, 11, 321, 393, 4584, 437, 321, 818, 25242, 33529, 21179, 11, 51040], "temperature": 0.0, "avg_logprob": -0.0885773213704427, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.0003459026920609176}, {"id": 135, "seek": 88848, "start": 902.0, "end": 907.52, "text": " or DPS in short. We start with standard Bayes rule in the context of diffusion models.", "tokens": [51040, 420, 413, 6273, 294, 2099, 13, 492, 722, 365, 3832, 7840, 279, 4978, 294, 264, 4319, 295, 25242, 5245, 13, 51316], "temperature": 0.0, "avg_logprob": -0.0885773213704427, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.0003459026920609176}, {"id": 136, "seek": 88848, "start": 908.8000000000001, "end": 911.2, "text": " We can apply theorem one to get the approximation.", "tokens": [51380, 492, 393, 3079, 20904, 472, 281, 483, 264, 28023, 13, 51500], "temperature": 0.0, "avg_logprob": -0.0885773213704427, "compression_ratio": 1.5441176470588236, "no_speech_prob": 0.0003459026920609176}, {"id": 137, "seek": 91848, "start": 918.48, "end": 938.8000000000001, "text": " I lost him. I don't hear anything anymore. Do you hear me now?", "tokens": [50364, 286, 2731, 796, 13, 286, 500, 380, 1568, 1340, 3602, 13, 1144, 291, 1568, 385, 586, 30, 51380], "temperature": 0.0, "avg_logprob": -0.4332247688656762, "compression_ratio": 1.0, "no_speech_prob": 0.029911156743764877}, {"id": 138, "seek": 93880, "start": 938.9599999999999, "end": 942.9599999999999, "text": " Yeah. Yeah. Yeah. Sorry. Where did the...", "tokens": [50372, 865, 13, 865, 13, 865, 13, 4919, 13, 2305, 630, 264, 485, 50572], "temperature": 0.0, "avg_logprob": -0.397983473174426, "compression_ratio": 1.3153153153153154, "no_speech_prob": 0.07190677523612976}, {"id": 139, "seek": 93880, "start": 950.9599999999999, "end": 953.12, "text": " I don't have any sound anymore.", "tokens": [50972, 286, 500, 380, 362, 604, 1626, 3602, 13, 51080], "temperature": 0.0, "avg_logprob": -0.397983473174426, "compression_ratio": 1.3153153153153154, "no_speech_prob": 0.07190677523612976}, {"id": 140, "seek": 93880, "start": 957.1999999999999, "end": 961.52, "text": " Yeah, now we're here. Yeah, now it's back. But we cannot see the screen.", "tokens": [51284, 865, 11, 586, 321, 434, 510, 13, 865, 11, 586, 309, 311, 646, 13, 583, 321, 2644, 536, 264, 2568, 13, 51500], "temperature": 0.0, "avg_logprob": -0.397983473174426, "compression_ratio": 1.3153153153153154, "no_speech_prob": 0.07190677523612976}, {"id": 141, "seek": 96152, "start": 961.52, "end": 970.88, "text": " Can you try to share the screen again? I can see a screen. I can see the screen,", "tokens": [50364, 1664, 291, 853, 281, 2073, 264, 2568, 797, 30, 286, 393, 536, 257, 2568, 13, 286, 393, 536, 264, 2568, 11, 50832], "temperature": 0.0, "avg_logprob": -0.31114004096206355, "compression_ratio": 1.4690265486725664, "no_speech_prob": 0.027318978682160378}, {"id": 142, "seek": 96152, "start": 970.88, "end": 981.52, "text": " the Gaussian and Poisson. Yeah, me too. Yeah. Can you try to just stop sharing again?", "tokens": [50832, 264, 39148, 293, 6165, 30472, 13, 865, 11, 385, 886, 13, 865, 13, 1664, 291, 853, 281, 445, 1590, 5414, 797, 30, 51364], "temperature": 0.0, "avg_logprob": -0.31114004096206355, "compression_ratio": 1.4690265486725664, "no_speech_prob": 0.027318978682160378}, {"id": 143, "seek": 98152, "start": 981.52, "end": 984.48, "text": " We'll share it again. Yeah, sure. Thanks.", "tokens": [50364, 492, 603, 2073, 309, 797, 13, 865, 11, 988, 13, 2561, 13, 50512], "temperature": 0.0, "avg_logprob": -0.18263857705252512, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.004065090790390968}, {"id": 144, "seek": 98152, "start": 990.24, "end": 994.4, "text": " So, is this where the connection was disconnected?", "tokens": [50800, 407, 11, 307, 341, 689, 264, 4984, 390, 29426, 30, 51008], "temperature": 0.0, "avg_logprob": -0.18263857705252512, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.004065090790390968}, {"id": 145, "seek": 98152, "start": 996.0799999999999, "end": 999.6, "text": " Yeah, you were just starting about the Gaussian and Poisson. Yeah.", "tokens": [51092, 865, 11, 291, 645, 445, 2891, 466, 264, 39148, 293, 6165, 30472, 13, 865, 13, 51268], "temperature": 0.0, "avg_logprob": -0.18263857705252512, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.004065090790390968}, {"id": 146, "seek": 98152, "start": 1001.1999999999999, "end": 1006.0, "text": " So, I was saying that these gradients can be analytically computed because these", "tokens": [51348, 407, 11, 286, 390, 1566, 300, 613, 2771, 2448, 393, 312, 10783, 984, 40610, 570, 613, 51588], "temperature": 0.0, "avg_logprob": -0.18263857705252512, "compression_ratio": 1.437125748502994, "no_speech_prob": 0.004065090790390968}, {"id": 147, "seek": 100600, "start": 1006.0, "end": 1011.36, "text": " functional forms are already known. So, we see that for Gaussian, we're essentially", "tokens": [50364, 11745, 6422, 366, 1217, 2570, 13, 407, 11, 321, 536, 300, 337, 39148, 11, 321, 434, 4476, 50632], "temperature": 0.0, "avg_logprob": -0.24934477276272243, "compression_ratio": 1.5, "no_speech_prob": 0.0032696661073714495}, {"id": 148, "seek": 100600, "start": 1011.36, "end": 1016.48, "text": " performing gradient descent that minimizes the squared L2 norm of the residual.", "tokens": [50632, 10205, 16235, 23475, 300, 4464, 5660, 264, 8889, 441, 17, 2026, 295, 264, 27980, 13, 50888], "temperature": 0.0, "avg_logprob": -0.24934477276272243, "compression_ratio": 1.5, "no_speech_prob": 0.0032696661073714495}, {"id": 149, "seek": 100600, "start": 1018.32, "end": 1023.76, "text": " I cannot see the slides, but I cannot see the screen. Yeah, neither. No.", "tokens": [50980, 286, 2644, 536, 264, 9788, 11, 457, 286, 2644, 536, 264, 2568, 13, 865, 11, 9662, 13, 883, 13, 51252], "temperature": 0.0, "avg_logprob": -0.24934477276272243, "compression_ratio": 1.5, "no_speech_prob": 0.0032696661073714495}, {"id": 150, "seek": 100600, "start": 1025.04, "end": 1030.56, "text": " They just dropped away in the halfway through your sentence.", "tokens": [51316, 814, 445, 8119, 1314, 294, 264, 15461, 807, 428, 8174, 13, 51592], "temperature": 0.0, "avg_logprob": -0.24934477276272243, "compression_ratio": 1.5, "no_speech_prob": 0.0032696661073714495}, {"id": 151, "seek": 103056, "start": 1031.36, "end": 1034.32, "text": " Okay. So, let's try to reshare...", "tokens": [50404, 1033, 13, 407, 11, 718, 311, 853, 281, 725, 31932, 485, 50552], "temperature": 0.0, "avg_logprob": -0.26426562324899144, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.0007776310085318983}, {"id": 152, "seek": 103056, "start": 1041.36, "end": 1046.32, "text": " Yeah, now I can see them. Yeah, me too. Thanks. So, does this work?", "tokens": [50904, 865, 11, 586, 286, 393, 536, 552, 13, 865, 11, 385, 886, 13, 2561, 13, 407, 11, 775, 341, 589, 30, 51152], "temperature": 0.0, "avg_logprob": -0.26426562324899144, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.0007776310085318983}, {"id": 153, "seek": 103056, "start": 1048.56, "end": 1057.76, "text": " Yes. It seems like... Yeah. So, yeah, I was saying that for Gaussian measurement models,", "tokens": [51264, 1079, 13, 467, 2544, 411, 485, 865, 13, 407, 11, 1338, 11, 286, 390, 1566, 300, 337, 39148, 13160, 5245, 11, 51724], "temperature": 0.0, "avg_logprob": -0.26426562324899144, "compression_ratio": 1.3571428571428572, "no_speech_prob": 0.0007776310085318983}, {"id": 154, "seek": 105776, "start": 1057.76, "end": 1064.16, "text": " we're trying to do gradient descent that minimizes the squared L2 norm of the residual,", "tokens": [50364, 321, 434, 1382, 281, 360, 16235, 23475, 300, 4464, 5660, 264, 8889, 441, 17, 2026, 295, 264, 27980, 11, 50684], "temperature": 0.0, "avg_logprob": -0.10824210223029641, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008163555758073926}, {"id": 155, "seek": 105776, "start": 1064.16, "end": 1068.56, "text": " and for Poisson measurements, we're minimizing the squared weighted norm of the residual.", "tokens": [50684, 293, 337, 6165, 30472, 15383, 11, 321, 434, 46608, 264, 8889, 32807, 2026, 295, 264, 27980, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10824210223029641, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008163555758073926}, {"id": 156, "seek": 105776, "start": 1071.2, "end": 1077.76, "text": " So, incorporating ancestral sampling for DDPMs, we have our algorithm of DPS where we can derive", "tokens": [51036, 407, 11, 33613, 40049, 21179, 337, 413, 11373, 26386, 11, 321, 362, 527, 9284, 295, 413, 6273, 689, 321, 393, 28446, 51364], "temperature": 0.0, "avg_logprob": -0.10824210223029641, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008163555758073926}, {"id": 157, "seek": 105776, "start": 1077.76, "end": 1084.8799999999999, "text": " separate algorithms according to the measurement model in hand. Note that line seven is where DPS", "tokens": [51364, 4994, 14642, 4650, 281, 264, 13160, 2316, 294, 1011, 13, 11633, 300, 1622, 3407, 307, 689, 413, 6273, 51720], "temperature": 0.0, "avg_logprob": -0.10824210223029641, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0008163555758073926}, {"id": 158, "seek": 108488, "start": 1084.88, "end": 1090.8000000000002, "text": " takes place. If we were to remove line seven, we would simply be sampling from the prior distribution,", "tokens": [50364, 2516, 1081, 13, 759, 321, 645, 281, 4159, 1622, 3407, 11, 321, 576, 2935, 312, 21179, 490, 264, 4059, 7316, 11, 50660], "temperature": 0.0, "avg_logprob": -0.07334832487435176, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0023590584751218557}, {"id": 159, "seek": 108488, "start": 1091.8400000000001, "end": 1096.96, "text": " the usual diffusion models, what they do. And hence, our algorithm is just very simple", "tokens": [50712, 264, 7713, 25242, 5245, 11, 437, 436, 360, 13, 400, 16678, 11, 527, 9284, 307, 445, 588, 2199, 50968], "temperature": 0.0, "avg_logprob": -0.07334832487435176, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0023590584751218557}, {"id": 160, "seek": 108488, "start": 1096.96, "end": 1104.0, "text": " modification of the usual DDPM sampling. When we do that, we achieve these results.", "tokens": [50968, 26747, 295, 264, 7713, 413, 11373, 44, 21179, 13, 1133, 321, 360, 300, 11, 321, 4584, 613, 3542, 13, 51320], "temperature": 0.0, "avg_logprob": -0.07334832487435176, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0023590584751218557}, {"id": 161, "seek": 108488, "start": 1104.88, "end": 1109.8400000000001, "text": " So, since our method is not dependent on the measurement model, we can apply the same score", "tokens": [51364, 407, 11, 1670, 527, 3170, 307, 406, 12334, 322, 264, 13160, 2316, 11, 321, 393, 3079, 264, 912, 6175, 51612], "temperature": 0.0, "avg_logprob": -0.07334832487435176, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.0023590584751218557}, {"id": 162, "seek": 110984, "start": 1109.84, "end": 1114.8799999999999, "text": " function to various problems. For example, this is the result of applying our method to super", "tokens": [50364, 2445, 281, 3683, 2740, 13, 1171, 1365, 11, 341, 307, 264, 1874, 295, 9275, 527, 3170, 281, 1687, 50616], "temperature": 0.0, "avg_logprob": -0.10136521139810252, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0022166140843182802}, {"id": 163, "seek": 110984, "start": 1114.8799999999999, "end": 1122.08, "text": " resolution that are contaminated with Gaussian noise. We can also apply our method to noisy in", "tokens": [50616, 8669, 300, 366, 34492, 365, 39148, 5658, 13, 492, 393, 611, 3079, 527, 3170, 281, 24518, 294, 50976], "temperature": 0.0, "avg_logprob": -0.10136521139810252, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0022166140843182802}, {"id": 164, "seek": 110984, "start": 1122.08, "end": 1129.9199999999998, "text": " painting, and this is the case where 92% of the pixels are blocked out in a 256x256 image.", "tokens": [50976, 5370, 11, 293, 341, 307, 264, 1389, 689, 28225, 4, 295, 264, 18668, 366, 15470, 484, 294, 257, 38882, 87, 6074, 21, 3256, 13, 51368], "temperature": 0.0, "avg_logprob": -0.10136521139810252, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0022166140843182802}, {"id": 165, "seek": 110984, "start": 1132.0, "end": 1136.72, "text": " This is noisy Gaussian deep blurring. We can do noisy motion deep blurring.", "tokens": [51472, 639, 307, 24518, 39148, 2452, 14257, 2937, 13, 492, 393, 360, 24518, 5394, 2452, 14257, 2937, 13, 51708], "temperature": 0.0, "avg_logprob": -0.10136521139810252, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.0022166140843182802}, {"id": 166, "seek": 113672, "start": 1137.1200000000001, "end": 1142.88, "text": " And what's even more, since we're not restricted to the choice of the forward operator A,", "tokens": [50384, 400, 437, 311, 754, 544, 11, 1670, 321, 434, 406, 20608, 281, 264, 3922, 295, 264, 2128, 12973, 316, 11, 50672], "temperature": 0.0, "avg_logprob": -0.1147564866326072, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0006070163217373192}, {"id": 167, "seek": 113672, "start": 1143.92, "end": 1147.76, "text": " for the first time in the context of inverse problem solving with diffusion models,", "tokens": [50724, 337, 264, 700, 565, 294, 264, 4319, 295, 17340, 1154, 12606, 365, 25242, 5245, 11, 50916], "temperature": 0.0, "avg_logprob": -0.1147564866326072, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0006070163217373192}, {"id": 168, "seek": 113672, "start": 1147.76, "end": 1154.24, "text": " we show that we can also solve nonlinear inverse problems, such as phase retrieval presented here.", "tokens": [50916, 321, 855, 300, 321, 393, 611, 5039, 2107, 28263, 17340, 2740, 11, 1270, 382, 5574, 19817, 3337, 8212, 510, 13, 51240], "temperature": 0.0, "avg_logprob": -0.1147564866326072, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0006070163217373192}, {"id": 169, "seek": 113672, "start": 1154.96, "end": 1161.76, "text": " And for those of you who do not know what phase retrieval is, it's a problem that tries to estimate", "tokens": [51276, 400, 337, 729, 295, 291, 567, 360, 406, 458, 437, 5574, 19817, 3337, 307, 11, 309, 311, 257, 1154, 300, 9898, 281, 12539, 51616], "temperature": 0.0, "avg_logprob": -0.1147564866326072, "compression_ratio": 1.7064220183486238, "no_speech_prob": 0.0006070163217373192}, {"id": 170, "seek": 116176, "start": 1162.4, "end": 1168.0, "text": " the phase and the Fourier domain, and this is a notoriously hard problem because most of the", "tokens": [50396, 264, 5574, 293, 264, 36810, 9274, 11, 293, 341, 307, 257, 46772, 8994, 1152, 1154, 570, 881, 295, 264, 50676], "temperature": 0.0, "avg_logprob": -0.1053853988647461, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0017004813998937607}, {"id": 171, "seek": 116176, "start": 1168.0, "end": 1173.84, "text": " information of the image is actually concentrated on the Fourier phase rather than the Fourier", "tokens": [50676, 1589, 295, 264, 3256, 307, 767, 21321, 322, 264, 36810, 5574, 2831, 813, 264, 36810, 50968], "temperature": 0.0, "avg_logprob": -0.1053853988647461, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0017004813998937607}, {"id": 172, "seek": 116176, "start": 1173.84, "end": 1182.0, "text": " magnitude of an image. And we can also solve problems like non-uniform deep blurring, which", "tokens": [50968, 15668, 295, 364, 3256, 13, 400, 321, 393, 611, 5039, 2740, 411, 2107, 12, 409, 8629, 2452, 14257, 2937, 11, 597, 51376], "temperature": 0.0, "avg_logprob": -0.1053853988647461, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0017004813998937607}, {"id": 173, "seek": 116176, "start": 1182.0, "end": 1187.28, "text": " is another nonlinear inverse problem, given that the forward measurement model is actually", "tokens": [51376, 307, 1071, 2107, 28263, 17340, 1154, 11, 2212, 300, 264, 2128, 13160, 2316, 307, 767, 51640], "temperature": 0.0, "avg_logprob": -0.1053853988647461, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0017004813998937607}, {"id": 174, "seek": 118728, "start": 1187.28, "end": 1193.68, "text": " differentiable. This is one application that I want to highlight because we actually used", "tokens": [50364, 819, 9364, 13, 639, 307, 472, 3861, 300, 286, 528, 281, 5078, 570, 321, 767, 1143, 50684], "temperature": 0.0, "avg_logprob": -0.06439849308558873, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.002359204925596714}, {"id": 175, "seek": 118728, "start": 1193.68, "end": 1200.16, "text": " a complex neural network that emulates the nonlinear blurring here. And neural network is one", "tokens": [50684, 257, 3997, 18161, 3209, 300, 846, 26192, 264, 2107, 28263, 14257, 2937, 510, 13, 400, 18161, 3209, 307, 472, 51008], "temperature": 0.0, "avg_logprob": -0.06439849308558873, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.002359204925596714}, {"id": 176, "seek": 118728, "start": 1200.16, "end": 1205.76, "text": " of the most nonlinear forward models that you can imagine. So even when the forward operator is", "tokens": [51008, 295, 264, 881, 2107, 28263, 2128, 5245, 300, 291, 393, 3811, 13, 407, 754, 562, 264, 2128, 12973, 307, 51288], "temperature": 0.0, "avg_logprob": -0.06439849308558873, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.002359204925596714}, {"id": 177, "seek": 118728, "start": 1205.76, "end": 1210.6399999999999, "text": " highly nonlinear, we can see that DPS is capable of recovering the image with high fidelity.", "tokens": [51288, 5405, 2107, 28263, 11, 321, 393, 536, 300, 413, 6273, 307, 8189, 295, 29180, 264, 3256, 365, 1090, 46404, 13, 51532], "temperature": 0.0, "avg_logprob": -0.06439849308558873, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.002359204925596714}, {"id": 178, "seek": 121064, "start": 1211.44, "end": 1224.72, "text": " So that was it for DPS. I think we can briefly stop to see if any of you have any questions.", "tokens": [50404, 407, 300, 390, 309, 337, 413, 6273, 13, 286, 519, 321, 393, 10515, 1590, 281, 536, 498, 604, 295, 291, 362, 604, 1651, 13, 51068], "temperature": 0.0, "avg_logprob": -0.21078459008947595, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0034783093724399805}, {"id": 179, "seek": 121064, "start": 1224.72, "end": 1233.2, "text": " I have a question. So basically, did you also show, because you can have, due to the stochasticity,", "tokens": [51068, 286, 362, 257, 1168, 13, 407, 1936, 11, 630, 291, 611, 855, 11, 570, 291, 393, 362, 11, 3462, 281, 264, 342, 8997, 2750, 507, 11, 51492], "temperature": 0.0, "avg_logprob": -0.21078459008947595, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0034783093724399805}, {"id": 180, "seek": 121064, "start": 1233.2, "end": 1240.0, "text": " you can have posterior distribution of the denoid samples, right? So do you notice a difference", "tokens": [51492, 291, 393, 362, 33529, 7316, 295, 264, 1441, 17079, 10938, 11, 558, 30, 407, 360, 291, 3449, 257, 2649, 51832], "temperature": 0.0, "avg_logprob": -0.21078459008947595, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.0034783093724399805}, {"id": 181, "seek": 124000, "start": 1240.08, "end": 1246.08, "text": " compared for the one you, the blur, for example, this image, do you get some deviations that are", "tokens": [50368, 5347, 337, 264, 472, 291, 11, 264, 14257, 11, 337, 1365, 11, 341, 3256, 11, 360, 291, 483, 512, 31219, 763, 300, 366, 50668], "temperature": 0.0, "avg_logprob": -0.1308192899150233, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0010802149772644043}, {"id": 182, "seek": 124000, "start": 1246.72, "end": 1252.96, "text": " meaningful in one sense? Yeah, so I'm sorry that I don't have examples here, but", "tokens": [50700, 10995, 294, 472, 2020, 30, 865, 11, 370, 286, 478, 2597, 300, 286, 500, 380, 362, 5110, 510, 11, 457, 51012], "temperature": 0.0, "avg_logprob": -0.1308192899150233, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0010802149772644043}, {"id": 183, "seek": 124000, "start": 1254.24, "end": 1261.12, "text": " yeah, you do see quite a bit of stochasticity, especially when the degradation is heavy as,", "tokens": [51076, 1338, 11, 291, 360, 536, 1596, 257, 857, 295, 342, 8997, 2750, 507, 11, 2318, 562, 264, 40519, 307, 4676, 382, 11, 51420], "temperature": 0.0, "avg_logprob": -0.1308192899150233, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0010802149772644043}, {"id": 184, "seek": 124000, "start": 1262.16, "end": 1268.32, "text": " for example, the example here, blurring is heavy. So the posterior samples have high", "tokens": [51472, 337, 1365, 11, 264, 1365, 510, 11, 14257, 2937, 307, 4676, 13, 407, 264, 33529, 10938, 362, 1090, 51780], "temperature": 0.0, "avg_logprob": -0.1308192899150233, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0010802149772644043}, {"id": 185, "seek": 126832, "start": 1268.32, "end": 1278.1599999999999, "text": " standard deviation in terms of reconstructions. Yeah, that's another question. I don't know if", "tokens": [50364, 3832, 25163, 294, 2115, 295, 31499, 626, 13, 865, 11, 300, 311, 1071, 1168, 13, 286, 500, 380, 458, 498, 50856], "temperature": 0.0, "avg_logprob": -0.19902809460957846, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0014857604401186109}, {"id": 186, "seek": 126832, "start": 1278.1599999999999, "end": 1282.72, "text": " you're going to explain it and maybe let me study that, but you also mentioned that reconstruction", "tokens": [50856, 291, 434, 516, 281, 2903, 309, 293, 1310, 718, 385, 2979, 300, 11, 457, 291, 611, 2835, 300, 31565, 51084], "temperature": 0.0, "avg_logprob": -0.19902809460957846, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0014857604401186109}, {"id": 187, "seek": 126832, "start": 1282.72, "end": 1288.3999999999999, "text": " is possible if you have a prior, right? Right. But how do you choose the prior? Do you learn", "tokens": [51084, 307, 1944, 498, 291, 362, 257, 4059, 11, 558, 30, 1779, 13, 583, 577, 360, 291, 2826, 264, 4059, 30, 1144, 291, 1466, 51368], "temperature": 0.0, "avg_logprob": -0.19902809460957846, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0014857604401186109}, {"id": 188, "seek": 126832, "start": 1288.3999999999999, "end": 1296.0, "text": " it or is it something that you'd have to? Oh yeah. So the thing that we're proposing here is that", "tokens": [51368, 309, 420, 307, 309, 746, 300, 291, 1116, 362, 281, 30, 876, 1338, 13, 407, 264, 551, 300, 321, 434, 29939, 510, 307, 300, 51748], "temperature": 0.0, "avg_logprob": -0.19902809460957846, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.0014857604401186109}, {"id": 189, "seek": 129600, "start": 1296.08, "end": 1300.24, "text": " we're using the diffusion prior, right? So we're using this generative prior,", "tokens": [50368, 321, 434, 1228, 264, 25242, 4059, 11, 558, 30, 407, 321, 434, 1228, 341, 1337, 1166, 4059, 11, 50576], "temperature": 0.0, "avg_logprob": -0.11518669128417969, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.0003919806913472712}, {"id": 190, "seek": 129600, "start": 1300.24, "end": 1306.4, "text": " and the prior is learned through the usual training process of diffusion models. So", "tokens": [50576, 293, 264, 4059, 307, 3264, 807, 264, 7713, 3097, 1399, 295, 25242, 5245, 13, 407, 50884], "temperature": 0.0, "avg_logprob": -0.11518669128417969, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.0003919806913472712}, {"id": 191, "seek": 129600, "start": 1306.4, "end": 1311.44, "text": " whether it be score matching or epsilon matching or whatever, we only, what we only need in the", "tokens": [50884, 1968, 309, 312, 6175, 14324, 420, 17889, 14324, 420, 2035, 11, 321, 787, 11, 437, 321, 787, 643, 294, 264, 51136], "temperature": 0.0, "avg_logprob": -0.11518669128417969, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.0003919806913472712}, {"id": 192, "seek": 129600, "start": 1311.44, "end": 1319.92, "text": " sampling process is the pre-trained score function s theta here. So all the examples that I show you", "tokens": [51136, 21179, 1399, 307, 264, 659, 12, 17227, 2001, 6175, 2445, 262, 9725, 510, 13, 407, 439, 264, 5110, 300, 286, 855, 291, 51560], "temperature": 0.0, "avg_logprob": -0.11518669128417969, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.0003919806913472712}, {"id": 193, "seek": 131992, "start": 1319.92, "end": 1328.64, "text": " here are generated by pre-trained models available online. So specifically, we use the", "tokens": [50364, 510, 366, 10833, 538, 659, 12, 17227, 2001, 5245, 2435, 2950, 13, 407, 4682, 11, 321, 764, 264, 50800], "temperature": 0.0, "avg_logprob": -0.11414813022224271, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0005028991145081818}, {"id": 194, "seek": 131992, "start": 1328.64, "end": 1333.8400000000001, "text": " open AI model, and we don't need any fine-tuning for any of these problems. We can just plug it", "tokens": [50800, 1269, 7318, 2316, 11, 293, 321, 500, 380, 643, 604, 2489, 12, 83, 37726, 337, 604, 295, 613, 2740, 13, 492, 393, 445, 5452, 309, 51060], "temperature": 0.0, "avg_logprob": -0.11414813022224271, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0005028991145081818}, {"id": 195, "seek": 131992, "start": 1333.8400000000001, "end": 1342.24, "text": " into the solver and that will act as the prior of the distribution. Okay, so the prior is trained", "tokens": [51060, 666, 264, 1404, 331, 293, 300, 486, 605, 382, 264, 4059, 295, 264, 7316, 13, 1033, 11, 370, 264, 4059, 307, 8895, 51480], "temperature": 0.0, "avg_logprob": -0.11414813022224271, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0005028991145081818}, {"id": 196, "seek": 131992, "start": 1342.24, "end": 1347.2, "text": " on the data that you use it on. So you use the simulation of the model pre-trained on different", "tokens": [51480, 322, 264, 1412, 300, 291, 764, 309, 322, 13, 407, 291, 764, 264, 16575, 295, 264, 2316, 659, 12, 17227, 2001, 322, 819, 51728], "temperature": 0.0, "avg_logprob": -0.11414813022224271, "compression_ratio": 1.6491228070175439, "no_speech_prob": 0.0005028991145081818}, {"id": 197, "seek": 134720, "start": 1347.2, "end": 1357.68, "text": " data. Yeah. Thanks. So how this method would differ from the paper also from song et al for", "tokens": [50364, 1412, 13, 865, 13, 2561, 13, 407, 577, 341, 3170, 576, 743, 490, 264, 3035, 611, 490, 2153, 1030, 419, 337, 50888], "temperature": 0.0, "avg_logprob": -0.1774577796459198, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0026195868849754333}, {"id": 198, "seek": 134720, "start": 1357.68, "end": 1365.52, "text": " inverse problems in medical imaging? So is there a likely difference? They are different and that", "tokens": [50888, 17340, 2740, 294, 4625, 25036, 30, 407, 307, 456, 257, 3700, 2649, 30, 814, 366, 819, 293, 300, 51280], "temperature": 0.0, "avg_logprob": -0.1774577796459198, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0026195868849754333}, {"id": 199, "seek": 134720, "start": 1365.52, "end": 1372.8, "text": " will be covered more in detail in the second section where I explain the nearest paper.", "tokens": [51280, 486, 312, 5343, 544, 294, 2607, 294, 264, 1150, 3541, 689, 286, 2903, 264, 23831, 3035, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1774577796459198, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.0026195868849754333}, {"id": 200, "seek": 137280, "start": 1373.04, "end": 1380.8, "text": " Yeah. So if you could summarize, for example, in a few words, because the main paper from song et al,", "tokens": [50376, 865, 13, 407, 498, 291, 727, 20858, 11, 337, 1365, 11, 294, 257, 1326, 2283, 11, 570, 264, 2135, 3035, 490, 2153, 1030, 419, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1445969839890798, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.002244794275611639}, {"id": 201, "seek": 137280, "start": 1380.8, "end": 1385.76, "text": " you can also do inverse problems straightforward. I mean, you can do the painting. What would be", "tokens": [50764, 291, 393, 611, 360, 17340, 2740, 15325, 13, 286, 914, 11, 291, 393, 360, 264, 5370, 13, 708, 576, 312, 51012], "temperature": 0.0, "avg_logprob": -0.1445969839890798, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.002244794275611639}, {"id": 202, "seek": 137280, "start": 1385.76, "end": 1394.3999999999999, "text": " like the main contribution is this non-linear approach to inverse problems, or what would", "tokens": [51012, 411, 264, 2135, 13150, 307, 341, 2107, 12, 28263, 3109, 281, 17340, 2740, 11, 420, 437, 576, 51444], "temperature": 0.0, "avg_logprob": -0.1445969839890798, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.002244794275611639}, {"id": 203, "seek": 137280, "start": 1394.3999999999999, "end": 1401.04, "text": " be the main difference? So it's two things. I guess I'll try to explain it later. So you're", "tokens": [51444, 312, 264, 2135, 2649, 30, 407, 309, 311, 732, 721, 13, 286, 2041, 286, 603, 853, 281, 2903, 309, 1780, 13, 407, 291, 434, 51776], "temperature": 0.0, "avg_logprob": -0.1445969839890798, "compression_ratio": 1.6170212765957446, "no_speech_prob": 0.002244794275611639}, {"id": 204, "seek": 140104, "start": 1401.04, "end": 1407.84, "text": " mentioning the original paper of song et al at iClear 2021 and there was a paper that you mentioned", "tokens": [50364, 18315, 264, 3380, 3035, 295, 2153, 1030, 419, 412, 741, 34, 5797, 7201, 293, 456, 390, 257, 3035, 300, 291, 2835, 50704], "temperature": 0.0, "avg_logprob": -0.08525551449168813, "compression_ratio": 1.625, "no_speech_prob": 0.00043043491314165294}, {"id": 205, "seek": 140104, "start": 1407.84, "end": 1416.8799999999999, "text": " that tackled medical imaging in the next year of the same author. They solved inverse problems by", "tokens": [50704, 300, 9426, 1493, 4625, 25036, 294, 264, 958, 1064, 295, 264, 912, 3793, 13, 814, 13041, 17340, 2740, 538, 51156], "temperature": 0.0, "avg_logprob": -0.08525551449168813, "compression_ratio": 1.625, "no_speech_prob": 0.00043043491314165294}, {"id": 206, "seek": 140104, "start": 1417.52, "end": 1423.36, "text": " using a so-called projection approach, which means that you're directly replacing what you have as", "tokens": [51188, 1228, 257, 370, 12, 11880, 22743, 3109, 11, 597, 1355, 300, 291, 434, 3838, 19139, 437, 291, 362, 382, 51480], "temperature": 0.0, "avg_logprob": -0.08525551449168813, "compression_ratio": 1.625, "no_speech_prob": 0.00043043491314165294}, {"id": 207, "seek": 140104, "start": 1423.36, "end": 1428.48, "text": " the measurements and you're keeping only the rest of it. So this is the visualization of such", "tokens": [51480, 264, 15383, 293, 291, 434, 5145, 787, 264, 1472, 295, 309, 13, 407, 341, 307, 264, 25801, 295, 1270, 51736], "temperature": 0.0, "avg_logprob": -0.08525551449168813, "compression_ratio": 1.625, "no_speech_prob": 0.00043043491314165294}, {"id": 208, "seek": 142848, "start": 1428.48, "end": 1436.0, "text": " projection process. And this projection process works for certain inverse problems, for example,", "tokens": [50364, 22743, 1399, 13, 400, 341, 22743, 1399, 1985, 337, 1629, 17340, 2740, 11, 337, 1365, 11, 50740], "temperature": 0.0, "avg_logprob": -0.12199076265096664, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.003822987899184227}, {"id": 209, "seek": 142848, "start": 1436.0, "end": 1444.0, "text": " in painting. And for in painting, when you have a small degradation, for example, the mask is small", "tokens": [50740, 294, 5370, 13, 400, 337, 294, 5370, 11, 562, 291, 362, 257, 1359, 40519, 11, 337, 1365, 11, 264, 6094, 307, 1359, 51140], "temperature": 0.0, "avg_logprob": -0.12199076265096664, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.003822987899184227}, {"id": 210, "seek": 142848, "start": 1444.0, "end": 1450.96, "text": " or the blocked out pixels are not dominant. In those cases, projection approaches work fine.", "tokens": [51140, 420, 264, 15470, 484, 18668, 366, 406, 15657, 13, 682, 729, 3331, 11, 22743, 11587, 589, 2489, 13, 51488], "temperature": 0.0, "avg_logprob": -0.12199076265096664, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.003822987899184227}, {"id": 211, "seek": 145096, "start": 1451.68, "end": 1459.8400000000001, "text": " But for example, when you have a large mask, for example, a 128 by 128 mask cure, when you", "tokens": [50400, 583, 337, 1365, 11, 562, 291, 362, 257, 2416, 6094, 11, 337, 1365, 11, 257, 29810, 538, 29810, 6094, 13698, 11, 562, 291, 50808], "temperature": 0.0, "avg_logprob": -0.10822634198772374, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004752557724714279}, {"id": 212, "seek": 145096, "start": 1459.8400000000001, "end": 1467.2, "text": " apply song's method, projection type approaches tend to fail dramatically. This is also relevant", "tokens": [50808, 3079, 2153, 311, 3170, 11, 22743, 2010, 11587, 3928, 281, 3061, 17548, 13, 639, 307, 611, 7340, 51176], "temperature": 0.0, "avg_logprob": -0.10822634198772374, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004752557724714279}, {"id": 213, "seek": 145096, "start": 1467.2, "end": 1473.52, "text": " to the case of medical imaging, where you have high degradations. These projection-based approaches", "tokens": [51176, 281, 264, 1389, 295, 4625, 25036, 11, 689, 291, 362, 1090, 24740, 763, 13, 1981, 22743, 12, 6032, 11587, 51492], "temperature": 0.0, "avg_logprob": -0.10822634198772374, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.004752557724714279}, {"id": 214, "seek": 147352, "start": 1473.52, "end": 1486.24, "text": " will typically fail. Whereas DPS that I proposed in the last section and the MCG that I will explain", "tokens": [50364, 486, 5850, 3061, 13, 13813, 413, 6273, 300, 286, 10348, 294, 264, 1036, 3541, 293, 264, 8797, 38, 300, 286, 486, 2903, 51000], "temperature": 0.0, "avg_logprob": -0.13424022631211716, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.0618191696703434}, {"id": 215, "seek": 147352, "start": 1486.24, "end": 1495.76, "text": " soon after does not have this property. And the methodological main difference is that", "tokens": [51000, 2321, 934, 775, 406, 362, 341, 4707, 13, 400, 264, 3170, 4383, 2135, 2649, 307, 300, 51476], "temperature": 0.0, "avg_logprob": -0.13424022631211716, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.0618191696703434}, {"id": 216, "seek": 149576, "start": 1496.64, "end": 1503.68, "text": " you song and all use projection type approaches, whereas MCG or DPS uses gradient type approaches.", "tokens": [50408, 291, 2153, 293, 439, 764, 22743, 2010, 11587, 11, 9735, 8797, 38, 420, 413, 6273, 4960, 16235, 2010, 11587, 13, 50760], "temperature": 0.0, "avg_logprob": -0.1797612190246582, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.0025100288912653923}, {"id": 217, "seek": 149576, "start": 1503.68, "end": 1510.8799999999999, "text": " So it's a smoother transition towards the that adapts to the measurement process.", "tokens": [50760, 407, 309, 311, 257, 28640, 6034, 3030, 264, 300, 23169, 1373, 281, 264, 13160, 1399, 13, 51120], "temperature": 0.0, "avg_logprob": -0.1797612190246582, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.0025100288912653923}, {"id": 218, "seek": 149576, "start": 1515.76, "end": 1518.8, "text": " Yeah, that's pretty nice. Yeah, that's pretty clear. Thanks.", "tokens": [51364, 865, 11, 300, 311, 1238, 1481, 13, 865, 11, 300, 311, 1238, 1850, 13, 2561, 13, 51516], "temperature": 0.0, "avg_logprob": -0.1797612190246582, "compression_ratio": 1.5157232704402517, "no_speech_prob": 0.0025100288912653923}, {"id": 219, "seek": 151880, "start": 1519.52, "end": 1528.08, "text": " Yeah. Okay. Should I move on to the next section?", "tokens": [50400, 865, 13, 1033, 13, 6454, 286, 1286, 322, 281, 264, 958, 3541, 30, 50828], "temperature": 0.0, "avg_logprob": -0.2592681021917434, "compression_ratio": 1.3035714285714286, "no_speech_prob": 0.009261940605938435}, {"id": 220, "seek": 151880, "start": 1531.04, "end": 1540.8, "text": " Yeah, thanks. Yeah, so the next paper that I'm going to talk about is actually a paper that came", "tokens": [50976, 865, 11, 3231, 13, 865, 11, 370, 264, 958, 3035, 300, 286, 478, 516, 281, 751, 466, 307, 767, 257, 3035, 300, 1361, 51464], "temperature": 0.0, "avg_logprob": -0.2592681021917434, "compression_ratio": 1.3035714285714286, "no_speech_prob": 0.009261940605938435}, {"id": 221, "seek": 154080, "start": 1540.8, "end": 1548.48, "text": " before DPS. But I explained DPS first because it really uses the same gradient, I would say", "tokens": [50364, 949, 413, 6273, 13, 583, 286, 8825, 413, 6273, 700, 570, 309, 534, 4960, 264, 912, 16235, 11, 286, 576, 584, 50748], "temperature": 0.0, "avg_logprob": -0.08577387149517353, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.060892507433891296}, {"id": 222, "seek": 154080, "start": 1548.48, "end": 1554.3999999999999, "text": " similar, but almost the same gradient method with DPS. But at the time of development of MCG paper,", "tokens": [50748, 2531, 11, 457, 1920, 264, 912, 16235, 3170, 365, 413, 6273, 13, 583, 412, 264, 565, 295, 3250, 295, 8797, 38, 3035, 11, 51044], "temperature": 0.0, "avg_logprob": -0.08577387149517353, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.060892507433891296}, {"id": 223, "seek": 154080, "start": 1554.3999999999999, "end": 1564.0, "text": " we did not really understand how mathematically the solvers would be derived. So here in this", "tokens": [51044, 321, 630, 406, 534, 1223, 577, 44003, 264, 1404, 840, 576, 312, 18949, 13, 407, 510, 294, 341, 51524], "temperature": 0.0, "avg_logprob": -0.08577387149517353, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.060892507433891296}, {"id": 224, "seek": 154080, "start": 1564.0, "end": 1569.6, "text": " paper, we focused more on the explanation in the geometric context of diffusion models. So I hope", "tokens": [51524, 3035, 11, 321, 5178, 544, 322, 264, 10835, 294, 264, 33246, 4319, 295, 25242, 5245, 13, 407, 286, 1454, 51804], "temperature": 0.0, "avg_logprob": -0.08577387149517353, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.060892507433891296}, {"id": 225, "seek": 156960, "start": 1569.6, "end": 1574.6399999999999, "text": " that this also helps the understanding of the underlying intuitive", "tokens": [50364, 300, 341, 611, 3665, 264, 3701, 295, 264, 14217, 21769, 50616], "temperature": 0.0, "avg_logprob": -0.1288324013734475, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0004726963525172323}, {"id": 226, "seek": 156960, "start": 1576.32, "end": 1578.48, "text": " things about these gradients based methods.", "tokens": [50700, 721, 466, 613, 2771, 2448, 2361, 7150, 13, 50808], "temperature": 0.0, "avg_logprob": -0.1288324013734475, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0004726963525172323}, {"id": 227, "seek": 156960, "start": 1580.8799999999999, "end": 1586.48, "text": " So in order to understand the paper, let us review some of the important properties of", "tokens": [50928, 407, 294, 1668, 281, 1223, 264, 3035, 11, 718, 505, 3131, 512, 295, 264, 1021, 7221, 295, 51208], "temperature": 0.0, "avg_logprob": -0.1288324013734475, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0004726963525172323}, {"id": 228, "seek": 156960, "start": 1586.48, "end": 1591.76, "text": " high dimensional Gaussian random variables. And specifically for very high dimensional Gaussian", "tokens": [51208, 1090, 18795, 39148, 4974, 9102, 13, 400, 4682, 337, 588, 1090, 18795, 39148, 51472], "temperature": 0.0, "avg_logprob": -0.1288324013734475, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0004726963525172323}, {"id": 229, "seek": 156960, "start": 1591.76, "end": 1596.8, "text": " random variables, while the mode of the PDF may be near the main of the distribution,", "tokens": [51472, 4974, 9102, 11, 1339, 264, 4391, 295, 264, 17752, 815, 312, 2651, 264, 2135, 295, 264, 7316, 11, 51724], "temperature": 0.0, "avg_logprob": -0.1288324013734475, "compression_ratio": 1.7465437788018434, "no_speech_prob": 0.0004726963525172323}, {"id": 230, "seek": 159680, "start": 1597.44, "end": 1602.8799999999999, "text": " the measure is actually concentrated in the annulus that is distant from the center.", "tokens": [50396, 264, 3481, 307, 767, 21321, 294, 264, 2324, 26107, 300, 307, 17275, 490, 264, 3056, 13, 50668], "temperature": 0.0, "avg_logprob": -0.07954182380285019, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.002471981104463339}, {"id": 231, "seek": 159680, "start": 1604.24, "end": 1608.24, "text": " This is the reason why when we add fixed variance Gaussian noise to an image,", "tokens": [50736, 639, 307, 264, 1778, 983, 562, 321, 909, 6806, 21977, 39148, 5658, 281, 364, 3256, 11, 50936], "temperature": 0.0, "avg_logprob": -0.07954182380285019, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.002471981104463339}, {"id": 232, "seek": 159680, "start": 1609.84, "end": 1614.96, "text": " you never really see a clean image, even when the highest probability of a Gaussian noise would be", "tokens": [51016, 291, 1128, 534, 536, 257, 2541, 3256, 11, 754, 562, 264, 6343, 8482, 295, 257, 39148, 5658, 576, 312, 51272], "temperature": 0.0, "avg_logprob": -0.07954182380285019, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.002471981104463339}, {"id": 233, "seek": 159680, "start": 1614.96, "end": 1622.72, "text": " zero noise. We always get images with very similar noise levels. Now extending that,", "tokens": [51272, 4018, 5658, 13, 492, 1009, 483, 5267, 365, 588, 2531, 5658, 4358, 13, 823, 24360, 300, 11, 51660], "temperature": 0.0, "avg_logprob": -0.07954182380285019, "compression_ratio": 1.6018518518518519, "no_speech_prob": 0.002471981104463339}, {"id": 234, "seek": 162272, "start": 1622.8, "end": 1626.88, "text": " let us think of a random variable Y that is corrupted with Gaussian noise", "tokens": [50368, 718, 505, 519, 295, 257, 4974, 7006, 398, 300, 307, 39480, 365, 39148, 5658, 50572], "temperature": 0.0, "avg_logprob": -0.16583451127584, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.0014301328919827938}, {"id": 235, "seek": 162272, "start": 1626.88, "end": 1632.56, "text": " by adding some noise. Sorry, can you maybe rephrase that? I mean, I think that's interesting,", "tokens": [50572, 538, 5127, 512, 5658, 13, 4919, 11, 393, 291, 1310, 319, 44598, 651, 300, 30, 286, 914, 11, 286, 519, 300, 311, 1880, 11, 50856], "temperature": 0.0, "avg_logprob": -0.16583451127584, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.0014301328919827938}, {"id": 236, "seek": 162272, "start": 1632.56, "end": 1638.96, "text": " the slides before. So the concentration of Gaussian measure, right? Yeah, so this is a", "tokens": [50856, 264, 9788, 949, 13, 407, 264, 9856, 295, 39148, 3481, 11, 558, 30, 865, 11, 370, 341, 307, 257, 51176], "temperature": 0.0, "avg_logprob": -0.16583451127584, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.0014301328919827938}, {"id": 237, "seek": 162272, "start": 1638.96, "end": 1644.24, "text": " Gaussian annulus theorem, right? And then the effect that we see, what you said is that we", "tokens": [51176, 39148, 2324, 26107, 20904, 11, 558, 30, 400, 550, 264, 1802, 300, 321, 536, 11, 437, 291, 848, 307, 300, 321, 51440], "temperature": 0.0, "avg_logprob": -0.16583451127584, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.0014301328919827938}, {"id": 238, "seek": 162272, "start": 1645.1200000000001, "end": 1651.68, "text": " don't see like, so we see the same effect, right? Or what do you mean? Definitely. So", "tokens": [51484, 500, 380, 536, 411, 11, 370, 321, 536, 264, 912, 1802, 11, 558, 30, 1610, 437, 360, 291, 914, 30, 12151, 13, 407, 51812], "temperature": 0.0, "avg_logprob": -0.16583451127584, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.0014301328919827938}, {"id": 239, "seek": 165168, "start": 1652.24, "end": 1658.48, "text": " because of the Gaussian annulus theorem, what we see when we add Gaussian noise to images with,", "tokens": [50392, 570, 295, 264, 39148, 2324, 26107, 20904, 11, 437, 321, 536, 562, 321, 909, 39148, 5658, 281, 5267, 365, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11516654852664832, "compression_ratio": 1.554945054945055, "no_speech_prob": 6.814255903009325e-05}, {"id": 240, "seek": 165168, "start": 1659.04, "end": 1666.96, "text": " let's say, fixed variance Gaussian noise to an image, I guess the highest probability", "tokens": [50732, 718, 311, 584, 11, 6806, 21977, 39148, 5658, 281, 364, 3256, 11, 286, 2041, 264, 6343, 8482, 51128], "temperature": 0.0, "avg_logprob": -0.11516654852664832, "compression_ratio": 1.554945054945055, "no_speech_prob": 6.814255903009325e-05}, {"id": 241, "seek": 165168, "start": 1666.96, "end": 1677.92, "text": " instantiation of this Gaussian distribution would be no noise, right? Zeroes. But that never happens.", "tokens": [51128, 9836, 6642, 295, 341, 39148, 7316, 576, 312, 572, 5658, 11, 558, 30, 17182, 279, 13, 583, 300, 1128, 2314, 13, 51676], "temperature": 0.0, "avg_logprob": -0.11516654852664832, "compression_ratio": 1.554945054945055, "no_speech_prob": 6.814255903009325e-05}, {"id": 242, "seek": 167792, "start": 1677.92, "end": 1684.24, "text": " So if you add some random Gaussian noise to an image, you will always see some", "tokens": [50364, 407, 498, 291, 909, 512, 4974, 39148, 5658, 281, 364, 3256, 11, 291, 486, 1009, 536, 512, 50680], "temperature": 0.0, "avg_logprob": -0.08453981427178867, "compression_ratio": 1.5337078651685394, "no_speech_prob": 6.919320003362373e-05}, {"id": 243, "seek": 167792, "start": 1685.28, "end": 1691.28, "text": " very similar noise level images that are corrupted in a very similar way. So that was what I was", "tokens": [50732, 588, 2531, 5658, 1496, 5267, 300, 366, 39480, 294, 257, 588, 2531, 636, 13, 407, 300, 390, 437, 286, 390, 51032], "temperature": 0.0, "avg_logprob": -0.08453981427178867, "compression_ratio": 1.5337078651685394, "no_speech_prob": 6.919320003362373e-05}, {"id": 244, "seek": 167792, "start": 1691.28, "end": 1702.48, "text": " talking about. Yeah, okay. Yeah. So it's also due to the convolution, right? You can observe this", "tokens": [51032, 1417, 466, 13, 865, 11, 1392, 13, 865, 13, 407, 309, 311, 611, 3462, 281, 264, 45216, 11, 558, 30, 509, 393, 11441, 341, 51592], "temperature": 0.0, "avg_logprob": -0.08453981427178867, "compression_ratio": 1.5337078651685394, "no_speech_prob": 6.919320003362373e-05}, {"id": 245, "seek": 170248, "start": 1702.56, "end": 1710.64, "text": " as a convolution. Yeah. So this is kind of related to what I was talking about before.", "tokens": [50368, 382, 257, 45216, 13, 865, 13, 407, 341, 307, 733, 295, 4077, 281, 437, 286, 390, 1417, 466, 949, 13, 50772], "temperature": 0.0, "avg_logprob": -0.11644699914114816, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0007431517587974668}, {"id": 246, "seek": 170248, "start": 1710.64, "end": 1718.96, "text": " So let's say you have like a linear space of X. And what you would do is you add some Gaussian", "tokens": [50772, 407, 718, 311, 584, 291, 362, 411, 257, 8213, 1901, 295, 1783, 13, 400, 437, 291, 576, 360, 307, 291, 909, 512, 39148, 51188], "temperature": 0.0, "avg_logprob": -0.11644699914114816, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0007431517587974668}, {"id": 247, "seek": 170248, "start": 1718.96, "end": 1727.04, "text": " noise to that random variable X. Because this is a convolution, we can show that the marginal", "tokens": [51188, 5658, 281, 300, 4974, 7006, 1783, 13, 1436, 341, 307, 257, 45216, 11, 321, 393, 855, 300, 264, 16885, 51592], "temperature": 0.0, "avg_logprob": -0.11644699914114816, "compression_ratio": 1.478494623655914, "no_speech_prob": 0.0007431517587974668}, {"id": 248, "seek": 172704, "start": 1727.04, "end": 1735.68, "text": " distribution P of Y is actually a, the measure actually concentrates on the hypercylinder", "tokens": [50364, 7316, 430, 295, 398, 307, 767, 257, 11, 264, 3481, 767, 5512, 12507, 322, 264, 9848, 1344, 75, 5669, 50796], "temperature": 0.0, "avg_logprob": -0.1220131422344007, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.0013247954193502665}, {"id": 249, "seek": 172704, "start": 1736.24, "end": 1744.0, "text": " that is distant from the center of PX. So this is kind of intuitive reason why", "tokens": [50824, 300, 307, 17275, 490, 264, 3056, 295, 430, 55, 13, 407, 341, 307, 733, 295, 21769, 1778, 983, 51212], "temperature": 0.0, "avg_logprob": -0.1220131422344007, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.0013247954193502665}, {"id": 250, "seek": 172704, "start": 1745.76, "end": 1749.68, "text": " the noisy images with the fixed variance have similar looking structure.", "tokens": [51300, 264, 24518, 5267, 365, 264, 6806, 21977, 362, 2531, 1237, 3877, 13, 51496], "temperature": 0.0, "avg_logprob": -0.1220131422344007, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.0013247954193502665}, {"id": 251, "seek": 174968, "start": 1750.5600000000002, "end": 1760.16, "text": " So in this work, we propose to interpret diffusion models using some assumptions about the data", "tokens": [50408, 407, 294, 341, 589, 11, 321, 17421, 281, 7302, 25242, 5245, 1228, 512, 17695, 466, 264, 1412, 50888], "temperature": 0.0, "avg_logprob": -0.17460918426513672, "compression_ratio": 1.8106796116504855, "no_speech_prob": 0.00031502233468927443}, {"id": 252, "seek": 174968, "start": 1760.16, "end": 1767.6000000000001, "text": " manifold. So the usual manifold hypothesis states that the data lives in a low dimensional manifold", "tokens": [50888, 47138, 13, 407, 264, 7713, 47138, 17291, 4368, 300, 264, 1412, 2909, 294, 257, 2295, 18795, 47138, 51260], "temperature": 0.0, "avg_logprob": -0.17460918426513672, "compression_ratio": 1.8106796116504855, "no_speech_prob": 0.00031502233468927443}, {"id": 253, "seek": 174968, "start": 1767.6000000000001, "end": 1772.88, "text": " with much smaller dimensionality than the ambient space. And in this work, we add an", "tokens": [51260, 365, 709, 4356, 10139, 1860, 813, 264, 22997, 1901, 13, 400, 294, 341, 589, 11, 321, 909, 364, 51524], "temperature": 0.0, "avg_logprob": -0.17460918426513672, "compression_ratio": 1.8106796116504855, "no_speech_prob": 0.00031502233468927443}, {"id": 254, "seek": 174968, "start": 1772.88, "end": 1778.0800000000002, "text": " additional strong assumption that the manifold, the central data manifold is locally linear.", "tokens": [51524, 4497, 2068, 15302, 300, 264, 47138, 11, 264, 5777, 1412, 47138, 307, 16143, 8213, 13, 51784], "temperature": 0.0, "avg_logprob": -0.17460918426513672, "compression_ratio": 1.8106796116504855, "no_speech_prob": 0.00031502233468927443}, {"id": 255, "seek": 177808, "start": 1778.56, "end": 1783.36, "text": " And in order to leverage, and this is because we want to leverage the results from the concentration", "tokens": [50388, 400, 294, 1668, 281, 13982, 11, 293, 341, 307, 570, 321, 528, 281, 13982, 264, 3542, 490, 264, 9856, 50628], "temperature": 0.0, "avg_logprob": -0.10298268588972681, "compression_ratio": 1.6875, "no_speech_prob": 0.00013981174561195076}, {"id": 256, "seek": 177808, "start": 1783.36, "end": 1791.04, "text": " of Gaussian measure. And specifically, when we assume that our data manifold is locally linear,", "tokens": [50628, 295, 39148, 3481, 13, 400, 4682, 11, 562, 321, 6552, 300, 527, 1412, 47138, 307, 16143, 8213, 11, 51012], "temperature": 0.0, "avg_logprob": -0.10298268588972681, "compression_ratio": 1.6875, "no_speech_prob": 0.00013981174561195076}, {"id": 257, "seek": 177808, "start": 1791.6, "end": 1796.8799999999999, "text": " what we can show is that the geometry of diffusion model is given by the successive manifolds", "tokens": [51040, 437, 321, 393, 855, 307, 300, 264, 18426, 295, 25242, 2316, 307, 2212, 538, 264, 48043, 8173, 31518, 51304], "temperature": 0.0, "avg_logprob": -0.10298268588972681, "compression_ratio": 1.6875, "no_speech_prob": 0.00013981174561195076}, {"id": 258, "seek": 177808, "start": 1796.8799999999999, "end": 1802.3999999999999, "text": " where there exists a clean data manifold in the center. And there are continuously many", "tokens": [51304, 689, 456, 8198, 257, 2541, 1412, 47138, 294, 264, 3056, 13, 400, 456, 366, 15684, 867, 51580], "temperature": 0.0, "avg_logprob": -0.10298268588972681, "compression_ratio": 1.6875, "no_speech_prob": 0.00013981174561195076}, {"id": 259, "seek": 180240, "start": 1802.48, "end": 1810.8000000000002, "text": " noisy manifolds where the noisy samples reside. So these are the manifolds of the noisy images.", "tokens": [50368, 24518, 8173, 31518, 689, 264, 24518, 10938, 40134, 13, 407, 613, 366, 264, 8173, 31518, 295, 264, 24518, 5267, 13, 50784], "temperature": 0.0, "avg_logprob": -0.09890241622924804, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0008039750391617417}, {"id": 260, "seek": 180240, "start": 1813.3600000000001, "end": 1819.3600000000001, "text": " Extending that to a continuous limit, we propose that diffusion model is an onion because", "tokens": [50912, 9881, 2029, 300, 281, 257, 10957, 4948, 11, 321, 17421, 300, 25242, 2316, 307, 364, 10916, 570, 51212], "temperature": 0.0, "avg_logprob": -0.09890241622924804, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0008039750391617417}, {"id": 261, "seek": 180240, "start": 1819.3600000000001, "end": 1824.5600000000002, "text": " where when we push T to infinity, the distribution actually becomes a pure Gaussian.", "tokens": [51212, 689, 562, 321, 2944, 314, 281, 13202, 11, 264, 7316, 767, 3643, 257, 6075, 39148, 13, 51472], "temperature": 0.0, "avg_logprob": -0.09890241622924804, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0008039750391617417}, {"id": 262, "seek": 180240, "start": 1825.1200000000001, "end": 1829.76, "text": " And since the measure of a high dimensional Gaussian resides in a Gaussian hypersphere,", "tokens": [51500, 400, 1670, 264, 3481, 295, 257, 1090, 18795, 39148, 47157, 294, 257, 39148, 7420, 433, 6605, 11, 51732], "temperature": 0.0, "avg_logprob": -0.09890241622924804, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.0008039750391617417}, {"id": 263, "seek": 182976, "start": 1830.48, "end": 1835.6, "text": " the intermediate manifolds are covers of the central manifolds that interpolates between these", "tokens": [50400, 264, 19376, 8173, 31518, 366, 10538, 295, 264, 5777, 8173, 31518, 300, 44902, 1024, 1296, 613, 50656], "temperature": 0.0, "avg_logprob": -0.07941909983188292, "compression_ratio": 1.8125, "no_speech_prob": 0.0013247457100078464}, {"id": 264, "seek": 182976, "start": 1835.6, "end": 1843.44, "text": " two. Therefore, this diffusion process would correspond to these red arrows, and the reverse", "tokens": [50656, 732, 13, 7504, 11, 341, 25242, 1399, 576, 6805, 281, 613, 2182, 19669, 11, 293, 264, 9943, 51048], "temperature": 0.0, "avg_logprob": -0.07941909983188292, "compression_ratio": 1.8125, "no_speech_prob": 0.0013247457100078464}, {"id": 265, "seek": 182976, "start": 1843.44, "end": 1851.6, "text": " diffusion would correspond to the blue arrows. And furthermore, when we are considering the case", "tokens": [51048, 25242, 576, 6805, 281, 264, 3344, 19669, 13, 400, 3052, 3138, 11, 562, 321, 366, 8079, 264, 1389, 51456], "temperature": 0.0, "avg_logprob": -0.07941909983188292, "compression_ratio": 1.8125, "no_speech_prob": 0.0013247457100078464}, {"id": 266, "seek": 182976, "start": 1851.6, "end": 1856.4, "text": " where we are trying to solve inverse problems with diffusion, we are left with the figure on", "tokens": [51456, 689, 321, 366, 1382, 281, 5039, 17340, 2740, 365, 25242, 11, 321, 366, 1411, 365, 264, 2573, 322, 51696], "temperature": 0.0, "avg_logprob": -0.07941909983188292, "compression_ratio": 1.8125, "no_speech_prob": 0.0013247457100078464}, {"id": 267, "seek": 185640, "start": 1856.4, "end": 1861.6000000000001, "text": " the right. Here, we are not only trying to find points in the central manifold M,", "tokens": [50364, 264, 558, 13, 1692, 11, 321, 366, 406, 787, 1382, 281, 915, 2793, 294, 264, 5777, 47138, 376, 11, 50624], "temperature": 0.0, "avg_logprob": -0.09438276290893555, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.003822882194072008}, {"id": 268, "seek": 185640, "start": 1862.48, "end": 1868.16, "text": " but we're trying to find the intersection between the manifold M and the line Y equals HX.", "tokens": [50668, 457, 321, 434, 1382, 281, 915, 264, 15236, 1296, 264, 47138, 376, 293, 264, 1622, 398, 6915, 389, 55, 13, 50952], "temperature": 0.0, "avg_logprob": -0.09438276290893555, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.003822882194072008}, {"id": 269, "seek": 185640, "start": 1873.68, "end": 1880.5600000000002, "text": " So I've already said this earlier, but let's examine some of the representative methods that", "tokens": [51228, 407, 286, 600, 1217, 848, 341, 3071, 11, 457, 718, 311, 17496, 512, 295, 264, 12424, 7150, 300, 51572], "temperature": 0.0, "avg_logprob": -0.09438276290893555, "compression_ratio": 1.5142857142857142, "no_speech_prob": 0.003822882194072008}, {"id": 270, "seek": 188056, "start": 1880.56, "end": 1886.56, "text": " solve inverse problems using diffusion. These methods, Song at L and Choi at L,", "tokens": [50364, 5039, 17340, 2740, 1228, 25242, 13, 1981, 7150, 11, 11862, 412, 441, 293, 33479, 412, 441, 11, 50664], "temperature": 0.0, "avg_logprob": -0.13077784173282575, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0019873243290930986}, {"id": 271, "seek": 188056, "start": 1887.52, "end": 1893.2, "text": " are a type of projection-based approach, since they directly replace the part that is known", "tokens": [50712, 366, 257, 2010, 295, 22743, 12, 6032, 3109, 11, 1670, 436, 3838, 7406, 264, 644, 300, 307, 2570, 50996], "temperature": 0.0, "avg_logprob": -0.13077784173282575, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0019873243290930986}, {"id": 272, "seek": 188056, "start": 1893.76, "end": 1902.8, "text": " and keep only the unknown part. Another way to impose data consistency in the Bayesian framework", "tokens": [51024, 293, 1066, 787, 264, 9841, 644, 13, 3996, 636, 281, 26952, 1412, 14416, 294, 264, 7840, 42434, 8388, 51476], "temperature": 0.0, "avg_logprob": -0.13077784173282575, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0019873243290930986}, {"id": 273, "seek": 188056, "start": 1902.8, "end": 1908.32, "text": " is to incorporate likelihood. In other words, try to minimize the residual by gradient descent.", "tokens": [51476, 307, 281, 16091, 22119, 13, 682, 661, 2283, 11, 853, 281, 17522, 264, 27980, 538, 16235, 23475, 13, 51752], "temperature": 0.0, "avg_logprob": -0.13077784173282575, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0019873243290930986}, {"id": 274, "seek": 190832, "start": 1909.12, "end": 1914.56, "text": " However, with diffusion models, if we were to use naive gradient descent with a noisy residual,", "tokens": [50404, 2908, 11, 365, 25242, 5245, 11, 498, 321, 645, 281, 764, 29052, 16235, 23475, 365, 257, 24518, 27980, 11, 50676], "temperature": 0.0, "avg_logprob": -0.11024106409131866, "compression_ratio": 1.6124031007751938, "no_speech_prob": 0.0005111819482408464}, {"id": 275, "seek": 190832, "start": 1914.56, "end": 1919.6, "text": " we will acquire no meaningful gradient, as the residual between HXI and Y", "tokens": [50676, 321, 486, 20001, 572, 10995, 16235, 11, 382, 264, 27980, 1296, 389, 55, 40, 293, 398, 50928], "temperature": 0.0, "avg_logprob": -0.11024106409131866, "compression_ratio": 1.6124031007751938, "no_speech_prob": 0.0005111819482408464}, {"id": 276, "seek": 190832, "start": 1920.56, "end": 1923.76, "text": " will only be focused on the Gaussian noise part of the residual.", "tokens": [50976, 486, 787, 312, 5178, 322, 264, 39148, 5658, 644, 295, 264, 27980, 13, 51136], "temperature": 0.0, "avg_logprob": -0.11024106409131866, "compression_ratio": 1.6124031007751938, "no_speech_prob": 0.0005111819482408464}, {"id": 277, "seek": 190832, "start": 1925.84, "end": 1931.2, "text": " Now, as we saw in diffusion posterior sampling, one can actually approximate the correct likelihood", "tokens": [51240, 823, 11, 382, 321, 1866, 294, 25242, 33529, 21179, 11, 472, 393, 767, 30874, 264, 3006, 22119, 51508], "temperature": 0.0, "avg_logprob": -0.11024106409131866, "compression_ratio": 1.6124031007751938, "no_speech_prob": 0.0005111819482408464}, {"id": 278, "seek": 190832, "start": 1931.2, "end": 1937.6, "text": " by switching to 2D denoised estimate X0 hat here. Here, visually and intuitively,", "tokens": [51508, 538, 16493, 281, 568, 35, 1441, 78, 2640, 12539, 1783, 15, 2385, 510, 13, 1692, 11, 19622, 293, 46506, 11, 51828], "temperature": 0.0, "avg_logprob": -0.11024106409131866, "compression_ratio": 1.6124031007751938, "no_speech_prob": 0.0005111819482408464}, {"id": 279, "seek": 193760, "start": 1937.6, "end": 1944.3999999999999, "text": " we see why this may help. This is because after denoising, the residual is more likely to focus", "tokens": [50364, 321, 536, 983, 341, 815, 854, 13, 639, 307, 570, 934, 1441, 78, 3436, 11, 264, 27980, 307, 544, 3700, 281, 1879, 50704], "temperature": 0.0, "avg_logprob": -0.09126776106217328, "compression_ratio": 1.4742268041237114, "no_speech_prob": 0.000616322795394808}, {"id": 280, "seek": 193760, "start": 1944.3999999999999, "end": 1951.84, "text": " on the structural differences between HX0 hats and Y. In practice, using this gradient step over", "tokens": [50704, 322, 264, 15067, 7300, 1296, 389, 55, 15, 20549, 293, 398, 13, 682, 3124, 11, 1228, 341, 16235, 1823, 670, 51076], "temperature": 0.0, "avg_logprob": -0.09126776106217328, "compression_ratio": 1.4742268041237114, "no_speech_prob": 0.000616322795394808}, {"id": 281, "seek": 193760, "start": 1951.84, "end": 1959.6799999999998, "text": " the projections work much, much better. And you can see the kind of the resulting differences", "tokens": [51076, 264, 32371, 589, 709, 11, 709, 1101, 13, 400, 291, 393, 536, 264, 733, 295, 264, 16505, 7300, 51468], "temperature": 0.0, "avg_logprob": -0.09126776106217328, "compression_ratio": 1.4742268041237114, "no_speech_prob": 0.000616322795394808}, {"id": 282, "seek": 195968, "start": 1959.68, "end": 1963.52, "text": " when we use projection type approach and the gradient approaches.", "tokens": [50364, 562, 321, 764, 22743, 2010, 3109, 293, 264, 16235, 11587, 13, 50556], "temperature": 0.0, "avg_logprob": -0.09220380287665825, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0007792765973135829}, {"id": 283, "seek": 195968, "start": 1967.52, "end": 1973.76, "text": " So in the paper, we properly analyze why this would be the case, leveraging the geometric", "tokens": [50756, 407, 294, 264, 3035, 11, 321, 6108, 12477, 983, 341, 576, 312, 264, 1389, 11, 32666, 264, 33246, 51068], "temperature": 0.0, "avg_logprob": -0.09220380287665825, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0007792765973135829}, {"id": 284, "seek": 195968, "start": 1973.76, "end": 1980.24, "text": " understanding of diffusion models. So first, under our assumptions, we show that 2D denoising", "tokens": [51068, 3701, 295, 25242, 5245, 13, 407, 700, 11, 833, 527, 17695, 11, 321, 855, 300, 568, 35, 1441, 78, 3436, 51392], "temperature": 0.0, "avg_logprob": -0.09220380287665825, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0007792765973135829}, {"id": 285, "seek": 195968, "start": 1980.24, "end": 1986.3200000000002, "text": " corresponds to an orthogonal projection to the data manifold. However, note that we have to consider", "tokens": [51392, 23249, 281, 364, 41488, 22743, 281, 264, 1412, 47138, 13, 2908, 11, 3637, 300, 321, 362, 281, 1949, 51696], "temperature": 0.0, "avg_logprob": -0.09220380287665825, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0007792765973135829}, {"id": 286, "seek": 198632, "start": 1986.32, "end": 1992.1599999999999, "text": " two components of the data manifold, the orthogonal one that is related to the noise components,", "tokens": [50364, 732, 6677, 295, 264, 1412, 47138, 11, 264, 41488, 472, 300, 307, 4077, 281, 264, 5658, 6677, 11, 50656], "temperature": 0.0, "avg_logprob": -0.10331143503603728, "compression_ratio": 1.9107142857142858, "no_speech_prob": 0.0013667898019775748}, {"id": 287, "seek": 198632, "start": 1992.1599999999999, "end": 1995.12, "text": " and the tangential one that is related to the data fidelity.", "tokens": [50656, 293, 264, 10266, 2549, 472, 300, 307, 4077, 281, 264, 1412, 46404, 13, 50804], "temperature": 0.0, "avg_logprob": -0.10331143503603728, "compression_ratio": 1.9107142857142858, "no_speech_prob": 0.0013667898019775748}, {"id": 288, "seek": 198632, "start": 1996.56, "end": 2002.08, "text": " So score function alone cannot handle the data fidelity. And here comes our theorem,", "tokens": [50876, 407, 6175, 2445, 3312, 2644, 4813, 264, 1412, 46404, 13, 400, 510, 1487, 527, 20904, 11, 51152], "temperature": 0.0, "avg_logprob": -0.10331143503603728, "compression_ratio": 1.9107142857142858, "no_speech_prob": 0.0013667898019775748}, {"id": 289, "seek": 198632, "start": 2002.08, "end": 2007.4399999999998, "text": " which states that the proposed manifold constraint gradient term points to a direction that is", "tokens": [51152, 597, 4368, 300, 264, 10348, 47138, 25534, 16235, 1433, 2793, 281, 257, 3513, 300, 307, 51420], "temperature": 0.0, "avg_logprob": -0.10331143503603728, "compression_ratio": 1.9107142857142858, "no_speech_prob": 0.0013667898019775748}, {"id": 290, "seek": 198632, "start": 2007.4399999999998, "end": 2013.2, "text": " tangential to the central data manifold. And therefore, our solution will move closer to Y", "tokens": [51420, 10266, 2549, 281, 264, 5777, 1412, 47138, 13, 400, 4412, 11, 527, 3827, 486, 1286, 4966, 281, 398, 51708], "temperature": 0.0, "avg_logprob": -0.10331143503603728, "compression_ratio": 1.9107142857142858, "no_speech_prob": 0.0013667898019775748}, {"id": 291, "seek": 201320, "start": 2013.2, "end": 2021.44, "text": " equals HX every time we apply this MCG step. Another important aspect of this is that since MCG", "tokens": [50364, 6915, 389, 55, 633, 565, 321, 3079, 341, 8797, 38, 1823, 13, 3996, 1021, 4171, 295, 341, 307, 300, 1670, 8797, 38, 50776], "temperature": 0.0, "avg_logprob": -0.09916246682405472, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.003123115049675107}, {"id": 292, "seek": 201320, "start": 2021.44, "end": 2029.2, "text": " lets the sample move on the data manifold, whereas projections move the sample off the data manifold", "tokens": [50776, 6653, 264, 6889, 1286, 322, 264, 1412, 47138, 11, 9735, 32371, 1286, 264, 6889, 766, 264, 1412, 47138, 51164], "temperature": 0.0, "avg_logprob": -0.09916246682405472, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.003123115049675107}, {"id": 293, "seek": 201320, "start": 2031.04, "end": 2035.52, "text": " because projections are hard constraints that try to make Y minus HX equals 0.", "tokens": [51256, 570, 32371, 366, 1152, 18491, 300, 853, 281, 652, 398, 3175, 389, 55, 6915, 1958, 13, 51480], "temperature": 0.0, "avg_logprob": -0.09916246682405472, "compression_ratio": 1.5988372093023255, "no_speech_prob": 0.003123115049675107}, {"id": 294, "seek": 203552, "start": 2036.08, "end": 2044.6399999999999, "text": " Note that MCG will not do that. It will move on the data manifolds in the direction tangential", "tokens": [50392, 11633, 300, 8797, 38, 486, 406, 360, 300, 13, 467, 486, 1286, 322, 264, 1412, 8173, 31518, 294, 264, 3513, 10266, 2549, 50820], "temperature": 0.0, "avg_logprob": -0.14796218564433436, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.00130442064255476}, {"id": 295, "seek": 203552, "start": 2044.6399999999999, "end": 2049.36, "text": " to the manifold. So those falling off the manifold will not happen.", "tokens": [50820, 281, 264, 47138, 13, 407, 729, 7440, 766, 264, 47138, 486, 406, 1051, 13, 51056], "temperature": 0.0, "avg_logprob": -0.14796218564433436, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.00130442064255476}, {"id": 296, "seek": 203552, "start": 2051.68, "end": 2058.32, "text": " So here, note that the score functions that are pre-trained were not trained with", "tokens": [51172, 407, 510, 11, 3637, 300, 264, 6175, 6828, 300, 366, 659, 12, 17227, 2001, 645, 406, 8895, 365, 51504], "temperature": 0.0, "avg_logprob": -0.14796218564433436, "compression_ratio": 1.6712328767123288, "no_speech_prob": 0.00130442064255476}, {"id": 297, "seek": 205832, "start": 2059.04, "end": 2066.8, "text": " samples that are off the data manifold. So they were trained with the samples that were on the", "tokens": [50400, 10938, 300, 366, 766, 264, 1412, 47138, 13, 407, 436, 645, 8895, 365, 264, 10938, 300, 645, 322, 264, 50788], "temperature": 0.0, "avg_logprob": -0.09788860593523298, "compression_ratio": 1.85, "no_speech_prob": 0.0032727180514484644}, {"id": 298, "seek": 205832, "start": 2066.8, "end": 2073.04, "text": " noisy data manifolds. So it will not be able to properly denoise samples that are off the data", "tokens": [50788, 24518, 1412, 8173, 31518, 13, 407, 309, 486, 406, 312, 1075, 281, 6108, 1441, 38800, 10938, 300, 366, 766, 264, 1412, 51100], "temperature": 0.0, "avg_logprob": -0.09788860593523298, "compression_ratio": 1.85, "no_speech_prob": 0.0032727180514484644}, {"id": 299, "seek": 205832, "start": 2073.04, "end": 2079.04, "text": " manifolds. So by using projections, the intermediate error will probably accumulate,", "tokens": [51100, 8173, 31518, 13, 407, 538, 1228, 32371, 11, 264, 19376, 6713, 486, 1391, 33384, 11, 51400], "temperature": 0.0, "avg_logprob": -0.09788860593523298, "compression_ratio": 1.85, "no_speech_prob": 0.0032727180514484644}, {"id": 300, "seek": 205832, "start": 2079.6800000000003, "end": 2084.96, "text": " and we conjecture that this is the reason why projection-based approaches often fail to produce", "tokens": [51432, 293, 321, 416, 1020, 540, 300, 341, 307, 264, 1778, 983, 22743, 12, 6032, 11587, 2049, 3061, 281, 5258, 51696], "temperature": 0.0, "avg_logprob": -0.09788860593523298, "compression_ratio": 1.85, "no_speech_prob": 0.0032727180514484644}, {"id": 301, "seek": 208496, "start": 2084.96, "end": 2094.2400000000002, "text": " a good sample. But however, as I said, in the time we wrote MCG, there was a gap between", "tokens": [50364, 257, 665, 6889, 13, 583, 4461, 11, 382, 286, 848, 11, 294, 264, 565, 321, 4114, 8797, 38, 11, 456, 390, 257, 7417, 1296, 50828], "temperature": 0.0, "avg_logprob": -0.1054731011390686, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.002216329565271735}, {"id": 302, "seek": 208496, "start": 2094.2400000000002, "end": 2102.0, "text": " theory and practice because even with the MCG steps, we had a hard time trying to solve inverse", "tokens": [50828, 5261, 293, 3124, 570, 754, 365, 264, 8797, 38, 4439, 11, 321, 632, 257, 1152, 565, 1382, 281, 5039, 17340, 51216], "temperature": 0.0, "avg_logprob": -0.1054731011390686, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.002216329565271735}, {"id": 303, "seek": 208496, "start": 2102.0, "end": 2108.2400000000002, "text": " problems. So we had to resort to some projections within the denoising and the gradient steps.", "tokens": [51216, 2740, 13, 407, 321, 632, 281, 19606, 281, 512, 32371, 1951, 264, 1441, 78, 3436, 293, 264, 16235, 4439, 13, 51528], "temperature": 0.0, "avg_logprob": -0.1054731011390686, "compression_ratio": 1.5245901639344261, "no_speech_prob": 0.002216329565271735}, {"id": 304, "seek": 210824, "start": 2109.12, "end": 2114.7999999999997, "text": " But in DPS, we managed to achieve a general solver that only uses these gradient steps.", "tokens": [50408, 583, 294, 413, 6273, 11, 321, 6453, 281, 4584, 257, 2674, 1404, 331, 300, 787, 4960, 613, 16235, 4439, 13, 50692], "temperature": 0.0, "avg_logprob": -0.0814314948187934, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0011334531009197235}, {"id": 305, "seek": 210824, "start": 2119.6, "end": 2126.72, "text": " Now, for the Gaussian case, the only difference between MCG and DPS is that we do not use projections", "tokens": [50932, 823, 11, 337, 264, 39148, 1389, 11, 264, 787, 2649, 1296, 8797, 38, 293, 413, 6273, 307, 300, 321, 360, 406, 764, 32371, 51288], "temperature": 0.0, "avg_logprob": -0.0814314948187934, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0011334531009197235}, {"id": 306, "seek": 210824, "start": 2126.72, "end": 2133.8399999999997, "text": " for DPS. Leveraging the geometric viewpoint of diffusion models, we can think that DPS is", "tokens": [51288, 337, 413, 6273, 13, 441, 1054, 3568, 264, 33246, 35248, 295, 25242, 5245, 11, 321, 393, 519, 300, 413, 6273, 307, 51644], "temperature": 0.0, "avg_logprob": -0.0814314948187934, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0011334531009197235}, {"id": 307, "seek": 213384, "start": 2133.84, "end": 2141.1200000000003, "text": " superior to MCG specifically because we avoid such projections. As projections here can potentially", "tokens": [50364, 13028, 281, 8797, 38, 4682, 570, 321, 5042, 1270, 32371, 13, 1018, 32371, 510, 393, 7263, 50728], "temperature": 0.0, "avg_logprob": -0.07659195363521576, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.0002453346096444875}, {"id": 308, "seek": 213384, "start": 2141.1200000000003, "end": 2150.1600000000003, "text": " throw samples off the data manifold. Hence, DPS will typically be more stable by avoiding such", "tokens": [50728, 3507, 10938, 766, 264, 1412, 47138, 13, 22229, 11, 413, 6273, 486, 5850, 312, 544, 8351, 538, 20220, 1270, 51180], "temperature": 0.0, "avg_logprob": -0.07659195363521576, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.0002453346096444875}, {"id": 309, "seek": 213384, "start": 2150.1600000000003, "end": 2157.2000000000003, "text": " throw-offs. And by the way, this is just an illustration. It doesn't really guarantee anything", "tokens": [51180, 3507, 12, 19231, 13, 400, 538, 264, 636, 11, 341, 307, 445, 364, 22645, 13, 467, 1177, 380, 534, 10815, 1340, 51532], "temperature": 0.0, "avg_logprob": -0.07659195363521576, "compression_ratio": 1.4522613065326633, "no_speech_prob": 0.0002453346096444875}, {"id": 310, "seek": 215720, "start": 2157.2, "end": 2162.72, "text": " that our solver will always stay on the noisy data manifolds.", "tokens": [50364, 300, 527, 1404, 331, 486, 1009, 1754, 322, 264, 24518, 1412, 8173, 31518, 13, 50640], "temperature": 0.0, "avg_logprob": -0.14074013596874171, "compression_ratio": 1.3782051282051282, "no_speech_prob": 0.0014757061144337058}, {"id": 311, "seek": 215720, "start": 2165.52, "end": 2173.4399999999996, "text": " Yeah, so that was about it for MCG. Any questions up until now?", "tokens": [50780, 865, 11, 370, 300, 390, 466, 309, 337, 8797, 38, 13, 2639, 1651, 493, 1826, 586, 30, 51176], "temperature": 0.0, "avg_logprob": -0.14074013596874171, "compression_ratio": 1.3782051282051282, "no_speech_prob": 0.0014757061144337058}, {"id": 312, "seek": 215720, "start": 2179.4399999999996, "end": 2185.6, "text": " I just have a question, but maybe a very simple question. At the beginning of this slide,", "tokens": [51476, 286, 445, 362, 257, 1168, 11, 457, 1310, 257, 588, 2199, 1168, 13, 1711, 264, 2863, 295, 341, 4137, 11, 51784], "temperature": 0.0, "avg_logprob": -0.14074013596874171, "compression_ratio": 1.3782051282051282, "no_speech_prob": 0.0014757061144337058}, {"id": 313, "seek": 218560, "start": 2185.6, "end": 2194.48, "text": " when you were explaining the, I think it's, no, the next one, two after this one.", "tokens": [50364, 562, 291, 645, 13468, 264, 11, 286, 519, 309, 311, 11, 572, 11, 264, 958, 472, 11, 732, 934, 341, 472, 13, 50808], "temperature": 0.0, "avg_logprob": -0.3088665349142892, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.026994217187166214}, {"id": 314, "seek": 218560, "start": 2197.04, "end": 2208.08, "text": " But yeah, this one, the previous one. How do you define this k? So n, what is n and what is k?", "tokens": [50936, 583, 1338, 11, 341, 472, 11, 264, 3894, 472, 13, 1012, 360, 291, 6964, 341, 350, 30, 407, 297, 11, 437, 307, 297, 293, 437, 307, 350, 30, 51488], "temperature": 0.0, "avg_logprob": -0.3088665349142892, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.026994217187166214}, {"id": 315, "seek": 220808, "start": 2208.08, "end": 2219.44, "text": " I would have to look for the definition of n and k here.", "tokens": [50364, 286, 576, 362, 281, 574, 337, 264, 7123, 295, 297, 293, 350, 510, 13, 50932], "temperature": 0.0, "avg_logprob": -0.22665253663674378, "compression_ratio": 1.4059405940594059, "no_speech_prob": 0.004329819232225418}, {"id": 316, "seek": 220808, "start": 2230.3199999999997, "end": 2236.08, "text": " From my memory, n is the dimension of the ambient space, in case the dimension of the", "tokens": [51476, 3358, 452, 4675, 11, 297, 307, 264, 10139, 295, 264, 22997, 1901, 11, 294, 1389, 264, 10139, 295, 264, 51764], "temperature": 0.0, "avg_logprob": -0.22665253663674378, "compression_ratio": 1.4059405940594059, "no_speech_prob": 0.004329819232225418}, {"id": 317, "seek": 223608, "start": 2236.08, "end": 2243.84, "text": " locally linear, low-dimensional dimensionality of the data manifold.", "tokens": [50364, 16143, 8213, 11, 2295, 12, 18759, 10139, 1860, 295, 264, 1412, 47138, 13, 50752], "temperature": 0.0, "avg_logprob": -0.2767022019725735, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0027414443902671337}, {"id": 318, "seek": 223608, "start": 2245.92, "end": 2253.36, "text": " I guess you're assuming that this, so the brown, the mean is basically used, the mean of each", "tokens": [50856, 286, 2041, 291, 434, 11926, 300, 341, 11, 370, 264, 6292, 11, 264, 914, 307, 1936, 1143, 11, 264, 914, 295, 1184, 51228], "temperature": 0.0, "avg_logprob": -0.2767022019725735, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0027414443902671337}, {"id": 319, "seek": 223608, "start": 2253.36, "end": 2260.4, "text": " intermediate distribution. So it's the mean of the diffuse or denoise distribution.", "tokens": [51228, 19376, 7316, 13, 407, 309, 311, 264, 914, 295, 264, 42165, 420, 1441, 38800, 7316, 13, 51580], "temperature": 0.0, "avg_logprob": -0.2767022019725735, "compression_ratio": 1.607843137254902, "no_speech_prob": 0.0027414443902671337}, {"id": 320, "seek": 226040, "start": 2261.36, "end": 2270.88, "text": " And then this, I'm just wondering, shouldn't you start going from a big", "tokens": [50412, 400, 550, 341, 11, 286, 478, 445, 6359, 11, 4659, 380, 291, 722, 516, 490, 257, 955, 50888], "temperature": 0.0, "avg_logprob": -0.1586982951444738, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.015387832187116146}, {"id": 321, "seek": 226040, "start": 2272.56, "end": 2277.44, "text": " standard deviation to a small standard deviation? Because basically when you do the denoising,", "tokens": [50972, 3832, 25163, 281, 257, 1359, 3832, 25163, 30, 1436, 1936, 562, 291, 360, 264, 1441, 78, 3436, 11, 51216], "temperature": 0.0, "avg_logprob": -0.1586982951444738, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.015387832187116146}, {"id": 322, "seek": 226040, "start": 2277.44, "end": 2284.88, "text": " you start with a high variance and then you start denoising and the variance slowly decreases.", "tokens": [51216, 291, 722, 365, 257, 1090, 21977, 293, 550, 291, 722, 1441, 78, 3436, 293, 264, 21977, 5692, 24108, 13, 51588], "temperature": 0.0, "avg_logprob": -0.1586982951444738, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.015387832187116146}, {"id": 323, "seek": 226040, "start": 2285.6, "end": 2290.1600000000003, "text": " That's just something I was thinking that if that's not the case, so I'm missing out something.", "tokens": [51624, 663, 311, 445, 746, 286, 390, 1953, 300, 498, 300, 311, 406, 264, 1389, 11, 370, 286, 478, 5361, 484, 746, 13, 51852], "temperature": 0.0, "avg_logprob": -0.1586982951444738, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.015387832187116146}, {"id": 324, "seek": 229040, "start": 2291.12, "end": 2295.84, "text": " Can you repeat the question again? I'm not sure if I understood it correctly.", "tokens": [50400, 1664, 291, 7149, 264, 1168, 797, 30, 286, 478, 406, 988, 498, 286, 7320, 309, 8944, 13, 50636], "temperature": 0.0, "avg_logprob": -0.10782263495705345, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.000654957490041852}, {"id": 325, "seek": 229040, "start": 2295.84, "end": 2303.76, "text": " Yeah, so the point of this is to interpret what's going on, like how the denoising samples", "tokens": [50636, 865, 11, 370, 264, 935, 295, 341, 307, 281, 7302, 437, 311, 516, 322, 11, 411, 577, 264, 1441, 78, 3436, 10938, 51032], "temperature": 0.0, "avg_logprob": -0.10782263495705345, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.000654957490041852}, {"id": 326, "seek": 229040, "start": 2304.48, "end": 2313.6, "text": " behave in the data manifold through the trajectory. So when you do the denoising,", "tokens": [51068, 15158, 294, 264, 1412, 47138, 807, 264, 21512, 13, 407, 562, 291, 360, 264, 1441, 78, 3436, 11, 51524], "temperature": 0.0, "avg_logprob": -0.10782263495705345, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.000654957490041852}, {"id": 327, "seek": 229040, "start": 2313.6, "end": 2319.2000000000003, "text": " so you start with the isotropic Gaussian, which is the highest entropy you can have. So you start", "tokens": [51524, 370, 291, 722, 365, 264, 38018, 39173, 39148, 11, 597, 307, 264, 6343, 30867, 291, 393, 362, 13, 407, 291, 722, 51804], "temperature": 0.0, "avg_logprob": -0.10782263495705345, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.000654957490041852}, {"id": 328, "seek": 231920, "start": 2319.2, "end": 2326.8799999999997, "text": " with the maximum variance that you have and then slowly you start removing the noise and by doing", "tokens": [50364, 365, 264, 6674, 21977, 300, 291, 362, 293, 550, 5692, 291, 722, 12720, 264, 5658, 293, 538, 884, 50748], "temperature": 0.0, "avg_logprob": -0.13414274260055187, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0031089771073311567}, {"id": 329, "seek": 231920, "start": 2326.8799999999997, "end": 2336.64, "text": " that interactively the variance at each intermediate step goes down. So what I was expecting is to have", "tokens": [50748, 300, 4648, 3413, 264, 21977, 412, 1184, 19376, 1823, 1709, 760, 13, 407, 437, 286, 390, 9650, 307, 281, 362, 51236], "temperature": 0.0, "avg_logprob": -0.13414274260055187, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0031089771073311567}, {"id": 330, "seek": 231920, "start": 2337.2, "end": 2342.48, "text": " this behavior when you have the mean and then the variance is large at the beginning and then", "tokens": [51264, 341, 5223, 562, 291, 362, 264, 914, 293, 550, 264, 21977, 307, 2416, 412, 264, 2863, 293, 550, 51528], "temperature": 0.0, "avg_logprob": -0.13414274260055187, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0031089771073311567}, {"id": 331, "seek": 231920, "start": 2342.48, "end": 2348.72, "text": " starts decreasing. Because ideally you would reduce the variance, but I don't know if I'm missing", "tokens": [51528, 3719, 23223, 13, 1436, 22915, 291, 576, 5407, 264, 21977, 11, 457, 286, 500, 380, 458, 498, 286, 478, 5361, 51840], "temperature": 0.0, "avg_logprob": -0.13414274260055187, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.0031089771073311567}, {"id": 332, "seek": 234872, "start": 2348.72, "end": 2356.3999999999996, "text": " something there. Oh, but I think it's what you're saying is kind of different from what", "tokens": [50364, 746, 456, 13, 876, 11, 457, 286, 519, 309, 311, 437, 291, 434, 1566, 307, 733, 295, 819, 490, 437, 50748], "temperature": 0.0, "avg_logprob": -0.10982913237351638, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0010000779293477535}, {"id": 333, "seek": 234872, "start": 2356.3999999999996, "end": 2363.6, "text": " this slide is saying. So I'm not really stating anything about denoising anything because the", "tokens": [50748, 341, 4137, 307, 1566, 13, 407, 286, 478, 406, 534, 26688, 1340, 466, 1441, 78, 3436, 1340, 570, 264, 51108], "temperature": 0.0, "avg_logprob": -0.10982913237351638, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0010000779293477535}, {"id": 334, "seek": 234872, "start": 2364.64, "end": 2370.3999999999996, "text": " flow here is that we're first assuming that there exists a central data manifold M here.", "tokens": [51160, 3095, 510, 307, 300, 321, 434, 700, 11926, 300, 456, 8198, 257, 5777, 1412, 47138, 376, 510, 13, 51448], "temperature": 0.0, "avg_logprob": -0.10982913237351638, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0010000779293477535}, {"id": 335, "seek": 237040, "start": 2371.28, "end": 2380.2400000000002, "text": " And then we're trying to visualize how the manifold of the noisy images would look like", "tokens": [50408, 400, 550, 321, 434, 1382, 281, 23273, 577, 264, 47138, 295, 264, 24518, 5267, 576, 574, 411, 50856], "temperature": 0.0, "avg_logprob": -0.07627753288515153, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.0023593467194586992}, {"id": 336, "seek": 237040, "start": 2380.2400000000002, "end": 2388.56, "text": " if there exists the central data manifold M. And I'm claiming that these noisy data manifolds will", "tokens": [50856, 498, 456, 8198, 264, 5777, 1412, 47138, 376, 13, 400, 286, 478, 19232, 300, 613, 24518, 1412, 8173, 31518, 486, 51272], "temperature": 0.0, "avg_logprob": -0.07627753288515153, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.0023593467194586992}, {"id": 337, "seek": 237040, "start": 2388.56, "end": 2396.56, "text": " be covers of the central data manifold, where the distance between from the central data manifold", "tokens": [51272, 312, 10538, 295, 264, 5777, 1412, 47138, 11, 689, 264, 4560, 1296, 490, 264, 5777, 1412, 47138, 51672], "temperature": 0.0, "avg_logprob": -0.07627753288515153, "compression_ratio": 1.7423312883435582, "no_speech_prob": 0.0023593467194586992}, {"id": 338, "seek": 239656, "start": 2396.56, "end": 2406.96, "text": " will be defined by this constant value. So it's really not about starting from high variance", "tokens": [50364, 486, 312, 7642, 538, 341, 5754, 2158, 13, 407, 309, 311, 534, 406, 466, 2891, 490, 1090, 21977, 50884], "temperature": 0.0, "avg_logprob": -0.09695436737754128, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.0021099725272506475}, {"id": 339, "seek": 239656, "start": 2406.96, "end": 2412.7999999999997, "text": " Gaussian noise and reducing it to anything. We're starting from a clean image and then we're defining", "tokens": [50884, 39148, 5658, 293, 12245, 309, 281, 1340, 13, 492, 434, 2891, 490, 257, 2541, 3256, 293, 550, 321, 434, 17827, 51176], "temperature": 0.0, "avg_logprob": -0.09695436737754128, "compression_ratio": 1.416058394160584, "no_speech_prob": 0.0021099725272506475}, {"id": 340, "seek": 241280, "start": 2412.8, "end": 2420.1600000000003, "text": " the manifolds of these of these noisy images. Okay, so I will have to look at that for I guess also", "tokens": [50364, 264, 8173, 31518, 295, 613, 295, 613, 24518, 5267, 13, 1033, 11, 370, 286, 486, 362, 281, 574, 412, 300, 337, 286, 2041, 611, 50732], "temperature": 0.0, "avg_logprob": -0.34279466711956524, "compression_ratio": 1.3361344537815125, "no_speech_prob": 0.03199028968811035}, {"id": 341, "seek": 241280, "start": 2420.1600000000003, "end": 2442.6400000000003, "text": " more too. Yeah, thanks. Yeah. So I guess we just tell me if", "tokens": [50732, 544, 886, 13, 865, 11, 3231, 13, 865, 13, 407, 286, 2041, 321, 445, 980, 385, 498, 51856], "temperature": 0.0, "avg_logprob": -0.34279466711956524, "compression_ratio": 1.3361344537815125, "no_speech_prob": 0.03199028968811035}, {"id": 342, "seek": 244280, "start": 2442.8, "end": 2453.52, "text": " if I'm", "tokens": [50364, 498, 286, 478, 50900], "temperature": 0.0, "avg_logprob": -0.6809949193681989, "compression_ratio": 1.0, "no_speech_prob": 0.006359846796840429}, {"id": 343, "seek": 244280, "start": 2453.52, "end": 2455.52, "text": " it's gonna get it's a good cliffhanger.", "tokens": [50900, 309, 311, 799, 483, 309, 311, 257, 665, 22316, 71, 3176, 13, 51000], "temperature": 0.0, "avg_logprob": -0.6809949193681989, "compression_ratio": 1.0, "no_speech_prob": 0.006359846796840429}, {"id": 344, "seek": 244280, "start": 2458.48, "end": 2459.2000000000003, "text": " It's first again.", "tokens": [51148, 467, 311, 700, 797, 13, 51184], "temperature": 0.0, "avg_logprob": -0.6809949193681989, "compression_ratio": 1.0, "no_speech_prob": 0.006359846796840429}, {"id": 345, "seek": 245920, "start": 2459.2, "end": 2467.4399999999996, "text": " Um, do you hear me okay?", "tokens": [50364, 3301, 11, 360, 291, 1568, 385, 1392, 30, 50776], "temperature": 0.0, "avg_logprob": -0.3007112363489663, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0043052551336586475}, {"id": 346, "seek": 245920, "start": 2468.96, "end": 2476.96, "text": " Yeah, yes. Yeah, so I think it's about 48 minutes. So I don't think we have to go over the final", "tokens": [50852, 865, 11, 2086, 13, 865, 11, 370, 286, 519, 309, 311, 466, 11174, 2077, 13, 407, 286, 500, 380, 519, 321, 362, 281, 352, 670, 264, 2572, 51252], "temperature": 0.0, "avg_logprob": -0.3007112363489663, "compression_ratio": 1.1523809523809523, "no_speech_prob": 0.0043052551336586475}, {"id": 347, "seek": 247696, "start": 2476.96, "end": 2490.0, "text": " section. So yeah. So I'll just try to we just we cannot see you or the slides anymore if you", "tokens": [50364, 3541, 13, 407, 1338, 13, 407, 286, 603, 445, 853, 281, 321, 445, 321, 2644, 536, 291, 420, 264, 9788, 3602, 498, 291, 51016], "temperature": 0.0, "avg_logprob": -0.28406425550872205, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.01658531092107296}, {"id": 348, "seek": 247696, "start": 2490.0, "end": 2499.76, "text": " can do the same thing as before to stop sharing and start again. Yeah. Well, the talk is basically", "tokens": [51016, 393, 360, 264, 912, 551, 382, 949, 281, 1590, 5414, 293, 722, 797, 13, 865, 13, 1042, 11, 264, 751, 307, 1936, 51504], "temperature": 0.0, "avg_logprob": -0.28406425550872205, "compression_ratio": 1.4044117647058822, "no_speech_prob": 0.01658531092107296}, {"id": 349, "seek": 249976, "start": 2500.7200000000003, "end": 2507.28, "text": " done. So okay, that was the last slide if we if we don't cover the last section.", "tokens": [50412, 1096, 13, 407, 1392, 11, 300, 390, 264, 1036, 4137, 498, 321, 498, 321, 500, 380, 2060, 264, 1036, 3541, 13, 50740], "temperature": 0.0, "avg_logprob": -0.25294638299322747, "compression_ratio": 1.5549132947976878, "no_speech_prob": 0.011282917112112045}, {"id": 350, "seek": 249976, "start": 2510.2400000000002, "end": 2514.1600000000003, "text": " I'm not sure if this is the correct way of saying thank you very much in in Dutch.", "tokens": [50888, 286, 478, 406, 988, 498, 341, 307, 264, 3006, 636, 295, 1566, 1309, 291, 588, 709, 294, 294, 15719, 13, 51084], "temperature": 0.0, "avg_logprob": -0.25294638299322747, "compression_ratio": 1.5549132947976878, "no_speech_prob": 0.011282917112112045}, {"id": 351, "seek": 249976, "start": 2515.1200000000003, "end": 2516.5600000000004, "text": " Maybe you should put it in Italian.", "tokens": [51132, 2704, 291, 820, 829, 309, 294, 10003, 13, 51204], "temperature": 0.0, "avg_logprob": -0.25294638299322747, "compression_ratio": 1.5549132947976878, "no_speech_prob": 0.011282917112112045}, {"id": 352, "seek": 249976, "start": 2522.96, "end": 2526.6400000000003, "text": " Yeah, so yeah, that is the right way to say thank you. Thank you too.", "tokens": [51524, 865, 11, 370, 1338, 11, 300, 307, 264, 558, 636, 281, 584, 1309, 291, 13, 1044, 291, 886, 13, 51708], "temperature": 0.0, "avg_logprob": -0.25294638299322747, "compression_ratio": 1.5549132947976878, "no_speech_prob": 0.011282917112112045}], "language": "en"}