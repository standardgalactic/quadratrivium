1
00:00:00,000 --> 00:00:07,520
Great. Yeah, so hi, I'm Hyunjin Chung. I'm a PhD student at bio-imaging and signal

2
00:00:07,520 --> 00:00:13,200
processing and learning lab at KAIST Korea. I'm honored to give a talk in such a prestigious

3
00:00:13,200 --> 00:00:19,040
group in Netherlands. And so thank you very much for having me here. So today I'll give a talk

4
00:00:19,040 --> 00:00:23,520
mostly about diffusion models and how to use them to solve inverse problems in imaging,

5
00:00:24,080 --> 00:00:30,080
focusing on a few recent papers that I wrote. So specifically it will be focused on base and

6
00:00:30,080 --> 00:00:35,440
inference in the context of inverse problem solving. So I believe it will be of quite a

7
00:00:35,440 --> 00:00:40,800
relevance to you. And I'm pretty sure that most of you are already familiar with diffusion models,

8
00:00:40,800 --> 00:00:47,360
but some of you may not be familiar with inverse problems. So and since we have a rather small

9
00:00:47,440 --> 00:00:54,880
group, I feel free to speak up with your microphone. I'm very sure that I'm going to miss

10
00:00:54,880 --> 00:00:59,520
some chats. So I hope the talk will be as interactive as possible.

11
00:01:02,720 --> 00:01:08,880
Yeah, so the talk will be structured in a bit of an odd way because we're going to first talk about

12
00:01:08,880 --> 00:01:14,960
two closely related work in a reverse order. So the first part of the talk will be about a paper

13
00:01:14,960 --> 00:01:21,200
named DPS, accepted to iClear recently. Then we will move backwards and talk about MCG,

14
00:01:21,200 --> 00:01:25,920
which was supposed to be the main content of the talk today. And finally, if time allows,

15
00:01:25,920 --> 00:01:31,440
I would also like to briefly touch the most recent work that applies these methods to more

16
00:01:31,440 --> 00:01:39,440
advanced problems called blind inverse problems. So I'll just briefly cover the basic concepts

17
00:01:39,440 --> 00:01:44,960
of diffusion models. I don't want to bore you too much, but I'll just speak in a briefly in

18
00:01:44,960 --> 00:01:50,880
a score perspective. So what we do in diffusion models is we try to estimate the gradient of

19
00:01:50,880 --> 00:01:55,440
the lock density with the neural network S of theta, which points to the high density most of the

20
00:01:55,440 --> 00:02:03,120
distribution. In order to circumvent the intractable explicit score matching, you train your network

21
00:02:03,120 --> 00:02:08,800
with denoising score matching, which boils down to essentially training a residual denoiser.

22
00:02:10,400 --> 00:02:16,720
Once that is done, you can use the train score functions to sample from the distribution p of x

23
00:02:16,720 --> 00:02:24,240
with, for example, Langevin MCMC. And another interesting view is that one can view the data

24
00:02:24,240 --> 00:02:30,240
noising process as some linear forward SE, and the data generating process as the corresponding

25
00:02:30,240 --> 00:02:36,160
reverse stochastic differential equation, where the drift function is governed by the score function.

26
00:02:36,720 --> 00:02:42,080
Hence, when we want to sample data, you can discretize the reverse SDE and numerically

27
00:02:42,080 --> 00:02:49,520
solve it using the pre-trained score function. On the other hand, here we are interested in

28
00:02:49,520 --> 00:02:54,560
solving inverse problems. In the inverse problem setting, our aim is to recover the ground truth

29
00:02:54,560 --> 00:02:59,040
x from the noisy measurements y obtained through some integral imaging system A.

30
00:03:00,000 --> 00:03:06,560
And there will be some pollution of noise, and here, A, sorry, and the problem is naturally

31
00:03:06,560 --> 00:03:10,880
opposed, which means that there exist infinitely many solutions to this problem,

32
00:03:12,080 --> 00:03:19,280
meaning that given this noisy image, noisy blurred image of the leopard, there could exist

33
00:03:19,280 --> 00:03:27,680
a lot of different feasible answers to the actual clean image. So in order to correctly specify

34
00:03:27,680 --> 00:03:33,840
which one of the solutions is the one that we want, we need to specify the prior of the data

35
00:03:33,840 --> 00:03:39,600
distribution, and in other words, how the images would actually look like in real world.

36
00:03:42,160 --> 00:03:48,160
Examples of such inverse problems include in-painting and deep learning in the context

37
00:03:48,160 --> 00:03:52,640
of low-level vision. They're also called image restoration problems. And in the context of

38
00:03:52,640 --> 00:03:57,760
biomedical imaging, there could be compressing MRI, sparsive CT, and so on.

39
00:04:00,400 --> 00:04:04,640
So we'll go over how we can solve such inverse problems with diffusion models.

40
00:04:06,960 --> 00:04:13,840
So let's first consider the following measurements model. Now, given y, what we want is to sample

41
00:04:13,840 --> 00:04:21,360
from the posterior distribution p of x given y. And using Bayes' rule, it's straightforward to see

42
00:04:21,360 --> 00:04:27,520
that the gradient of the log posterior is the sum of gradient of the log likelihood

43
00:04:27,520 --> 00:04:34,080
plus the score function. Here, gradient of the log likelihood in red gives us the information

44
00:04:34,080 --> 00:04:39,680
about the measurement process. And the score function is what we've already estimated with

45
00:04:39,680 --> 00:04:46,240
the neural network. So one should note that the likelihood function p of y given x is in most cases

46
00:04:46,320 --> 00:04:51,600
known. And in 95% of the cases, it can be modeled relatively well with the Gaussian.

47
00:04:52,240 --> 00:04:58,720
And among the rest of the 5%, 4% is Poisson, meaning that the likelihood function is often known

48
00:04:58,720 --> 00:05:04,080
a priority. So hence, in the perspective of Bayesian reconstruction, all we have to do when

49
00:05:04,080 --> 00:05:09,920
posterior sampling is desirable is to specify the correct prior distribution. Then the measurement

50
00:05:09,920 --> 00:05:18,560
process is hand wavy, it's known so we can sample from the posterior. So let's consider the case

51
00:05:18,560 --> 00:05:25,040
where we modeled the data distribution with the diffusion model. From the score or the SDE perspective,

52
00:05:25,040 --> 00:05:30,880
recall that the generative process is defined by this general form of reverse SDE. So in order

53
00:05:30,880 --> 00:05:36,240
to do unconditional sampling from the prior distribution, we can use the following discretization

54
00:05:36,320 --> 00:05:42,960
steps of the reverse SDE. So for simplicity, I denoted Euler-Maruyama discretization here,

55
00:05:42,960 --> 00:05:48,000
but other forms of sampling can of course be used, such as ancestral sampling of DDPMs

56
00:05:48,560 --> 00:05:53,600
or hybrid methods that incorporate MCMC chains within the numerical integration solvers.

57
00:05:54,480 --> 00:05:59,520
They're called predictor-corrector solvers in score SDE paper.

58
00:06:00,160 --> 00:06:08,000
Moreover, when we want to do posterior sampling, we can use what we've derived from the Bayes rule

59
00:06:08,960 --> 00:06:15,760
and incorporate the gradient of the walk likelihood. So here comes the problem.

60
00:06:15,760 --> 00:06:21,840
Can I ask a question? I'm sorry, I'm decently new to the diffusion model, so maybe there is

61
00:06:21,840 --> 00:06:26,160
something I didn't understand. I don't understand the sign, why there is a minus sign in front of

62
00:06:26,240 --> 00:06:29,680
the even the original paper, why there is a minus sign in front of the gradient of the

63
00:06:31,600 --> 00:06:39,440
score. Oh, so you're asking, how did the reverse SDE come along here, right?

64
00:06:40,000 --> 00:06:48,320
Well, I'm saying that with that minus sign, it will do the opposite of going to the high

65
00:06:48,320 --> 00:06:52,880
probability regions. It will go away from it. So am I missing something?

66
00:06:53,840 --> 00:07:00,080
Oh, yeah. So intuitively, if you want to sample from the high density modes of the

67
00:07:00,080 --> 00:07:07,680
distribution, you would kind of do gradient ascent with it. And yeah, there's a caveat here,

68
00:07:07,680 --> 00:07:15,600
because the differential dt here is a time running backwards. So you have to show the minus sign.

69
00:07:15,680 --> 00:07:22,960
Yeah. But then the sign of f is wrong, because the sign of f should have the same

70
00:07:22,960 --> 00:07:33,120
sign of g, doesn't it? Actually, no. So the derivation process of this reverse SDE comes from

71
00:07:34,160 --> 00:07:40,480
the thing called Anderson's theorem. So if you plug in this general form of forward SDE,

72
00:07:40,480 --> 00:07:44,960
then there's a theorem that states that this reverse SDE is the correct form.

73
00:07:44,960 --> 00:07:49,840
I understand. But so if there isn't, let's say that there is no score, so there is no goal,

74
00:07:49,840 --> 00:07:55,200
then if you have a, let's say, a Oldston-Ulenbach process, the forward and reverse dynamics are the

75
00:07:55,200 --> 00:08:10,000
same. So you're saying that f dt is running backwards, then f should be negative. If you

76
00:08:10,000 --> 00:08:14,320
have a stationary process like an Oldston-Ulenbach, regardless whether you run it forward or

77
00:08:14,320 --> 00:08:19,440
backward, you have the same dynamics. Therefore, you are not going to flip the sign of f if you

78
00:08:19,440 --> 00:08:35,440
change the time. Let's say, in an Oldston-Ulenbach process, f induces the mean reversion. So if

79
00:08:35,440 --> 00:08:39,520
you actually flip the sign, you will have the process going away from the mean, which is not

80
00:08:39,520 --> 00:08:44,880
the reverse time process. The reverse time process wants to go back to the mean exactly like the

81
00:08:44,880 --> 00:08:50,640
forward process. Right, right. I'm also a bit confused myself.

82
00:08:54,080 --> 00:09:00,480
Because everything works, I think, I don't know. So again, I could be misinterpreting something.

83
00:09:01,440 --> 00:09:08,640
Because I do see this in papers, but I just,

84
00:09:13,680 --> 00:09:18,800
it's, again, if it's an Oldston-Ulenbach process, the forward and reverse dynamics,

85
00:09:19,840 --> 00:09:23,360
assuming score zero, should be the same. So you cannot flip the sign. Otherwise,

86
00:09:23,440 --> 00:09:31,840
you get the opposite, you get the diverging process. And if you assume the generative

87
00:09:31,840 --> 00:09:35,920
direction to be forward, then the sign of the score should be plus, not minus. But of course,

88
00:09:35,920 --> 00:09:40,720
it would become minus if you are actually considering that going backward. But okay,

89
00:09:40,720 --> 00:09:45,680
but maybe we can discuss this offline. And also, I mean, again, it could be that I'm missing some

90
00:09:45,680 --> 00:09:52,400
details of the notation that for which things will match up. I'm not sure if, especially in the case

91
00:09:52,400 --> 00:09:59,680
of Ornstein-Ulenbach process, is there, is the score function zero there? Well, so the,

92
00:10:01,200 --> 00:10:08,560
well, so the, if the forward process is Oldston-Ulenbach, then f of x of t is just minus alpha x.

93
00:10:09,200 --> 00:10:13,920
Right. And that would be true both in the forward and in the backward,

94
00:10:13,920 --> 00:10:17,280
because again, it's time invariant. If you flip, you don't change anything.

95
00:10:18,240 --> 00:10:23,280
The score happened in order to enforce the terminal condition that the process has to fit the data.

96
00:10:26,160 --> 00:10:30,480
So it, in this sense, it's nothing to do with the Ornstein-Ulenbach process. It's just a

97
00:10:32,080 --> 00:10:37,520
starting point of the process, which if then, if you want to reverse, it will actually go back to

98
00:10:37,520 --> 00:10:43,200
there. Yeah, I think we should come back to this later. I think it's better to discuss

99
00:10:48,160 --> 00:10:56,480
So coming back to where we were, I think it was right here. So I was talking about how we could

100
00:10:57,040 --> 00:11:02,640
switch from prior sampling to posterior sampling by, by just plugging, plugging in this base rule.

101
00:11:03,680 --> 00:11:10,160
And here I said that care must be taken here because the gradient of the log likelihood

102
00:11:10,160 --> 00:11:16,640
is in fact intractable. And note that this is different from my claim earlier that the

103
00:11:16,640 --> 00:11:22,320
likelihood function is in most cases known. And this is, this arises from the fact that

104
00:11:22,320 --> 00:11:29,200
there are noisy x i's here rather than x zero. So let us dive into closely examine what I mean by this.

105
00:11:30,400 --> 00:11:37,120
To see why this is the case, I'm sorry that I'm switching notations with i and t. For i's,

106
00:11:37,120 --> 00:11:46,480
I'm denoting discretized things and t. I'm just pointing at some general continuous time.

107
00:11:46,880 --> 00:11:53,840
and instantiations. So here, consider the following probabilistic graph in the context of diffusion

108
00:11:53,840 --> 00:12:00,480
models. So we know two conditional distributions p of y given x zero and p of x t given x zero.

109
00:12:01,200 --> 00:12:06,000
And for now, let's assume that the first one is the measurement distribution is given as typically

110
00:12:06,000 --> 00:12:12,480
Gaussian. So, and the second word, the four distribution of the diffusion is also Gaussian.

111
00:12:13,440 --> 00:12:19,360
However, the reverse distribution p of x zero given x t shown with blue dotted line is intractable

112
00:12:19,360 --> 00:12:26,000
in general. So hence p of y given x t is intractable because we have no information about this blue

113
00:12:26,000 --> 00:12:33,600
dotted line. So in our work, we aim to approximate the intractable distribution.

114
00:12:35,200 --> 00:12:41,760
The first key comes from the factorization. Since x zero is conditionally independent on y

115
00:12:41,760 --> 00:12:48,400
and x t, we can factor the integrand as follows. Where the former term is what we know and the

116
00:12:48,400 --> 00:12:54,560
latter term is what we partially know. And by partially known, I mean that we know how to obtain

117
00:12:54,560 --> 00:13:01,360
the posterior mean of the distribution, which is given by the 3ds formula. Note that 3ds formula

118
00:13:01,360 --> 00:13:07,440
is used widely in diffusion model context, as it states that the posterior mean, or the denoised

119
00:13:07,440 --> 00:13:13,200
estimate, can always be achieved when we know the so-called blurred score function. That is,

120
00:13:13,920 --> 00:13:17,520
the score function of the intermediate noisy variables x t.

121
00:13:21,840 --> 00:13:29,520
Here we see that we can plug in 3d denoising to achieve some posterior mean of the

122
00:13:30,160 --> 00:13:38,320
distribution in the context of the dpms. And to fully enjoy the effectiveness of 3ds formula,

123
00:13:38,320 --> 00:13:44,240
and because leaving the expectation outside is intractable, let us push the expectation inside

124
00:13:44,240 --> 00:13:52,800
for now. When we do that, by the 3ds formula proposition, we have a fully tractable distribution

125
00:13:52,800 --> 00:13:57,840
where the condition is now given by x zero hat, the denoised estimate from x t.

126
00:14:00,480 --> 00:14:05,920
Now, since we proposed an approximation here, it is important that we quantify the approximation

127
00:14:05,920 --> 00:14:10,480
bound. And for that, we have a theorem that states the approximation error,

128
00:14:10,480 --> 00:14:15,200
or the so-called Jensen gap, and used by pushing the expectation inside the function.

129
00:14:16,720 --> 00:14:23,520
And we show that this approximation error is upper bounded by this constant with 3 constituents.

130
00:14:24,480 --> 00:14:28,880
So, the latter two parts are usually bounded in most practical situation.

131
00:14:29,680 --> 00:14:37,200
And the interesting part here is the first one, where we see that when sigma goes to zero,

132
00:14:37,200 --> 00:14:42,960
the entire constant goes to zero. This is useful because it states that in cases where we have

133
00:14:42,960 --> 00:14:48,480
negligible measurement noise, the approximation will be tight. However, in practice, even when

134
00:14:48,480 --> 00:14:57,760
we have high measurement noise, the method also works very well. So, just summing everything up,

135
00:14:57,760 --> 00:15:02,000
thanks to theorem one, we can achieve what we call diffusion posterior sampling,

136
00:15:02,000 --> 00:15:07,520
or DPS in short. We start with standard Bayes rule in the context of diffusion models.

137
00:15:08,800 --> 00:15:11,200
We can apply theorem one to get the approximation.

138
00:15:18,480 --> 00:15:38,800
I lost him. I don't hear anything anymore. Do you hear me now?

139
00:15:38,960 --> 00:15:42,960
Yeah. Yeah. Yeah. Sorry. Where did the...

140
00:15:50,960 --> 00:15:53,120
I don't have any sound anymore.

141
00:15:57,200 --> 00:16:01,520
Yeah, now we're here. Yeah, now it's back. But we cannot see the screen.

142
00:16:01,520 --> 00:16:10,880
Can you try to share the screen again? I can see a screen. I can see the screen,

143
00:16:10,880 --> 00:16:21,520
the Gaussian and Poisson. Yeah, me too. Yeah. Can you try to just stop sharing again?

144
00:16:21,520 --> 00:16:24,480
We'll share it again. Yeah, sure. Thanks.

145
00:16:30,240 --> 00:16:34,400
So, is this where the connection was disconnected?

146
00:16:36,080 --> 00:16:39,600
Yeah, you were just starting about the Gaussian and Poisson. Yeah.

147
00:16:41,200 --> 00:16:46,000
So, I was saying that these gradients can be analytically computed because these

148
00:16:46,000 --> 00:16:51,360
functional forms are already known. So, we see that for Gaussian, we're essentially

149
00:16:51,360 --> 00:16:56,480
performing gradient descent that minimizes the squared L2 norm of the residual.

150
00:16:58,320 --> 00:17:03,760
I cannot see the slides, but I cannot see the screen. Yeah, neither. No.

151
00:17:05,040 --> 00:17:10,560
They just dropped away in the halfway through your sentence.

152
00:17:11,360 --> 00:17:14,320
Okay. So, let's try to reshare...

153
00:17:21,360 --> 00:17:26,320
Yeah, now I can see them. Yeah, me too. Thanks. So, does this work?

154
00:17:28,560 --> 00:17:37,760
Yes. It seems like... Yeah. So, yeah, I was saying that for Gaussian measurement models,

155
00:17:37,760 --> 00:17:44,160
we're trying to do gradient descent that minimizes the squared L2 norm of the residual,

156
00:17:44,160 --> 00:17:48,560
and for Poisson measurements, we're minimizing the squared weighted norm of the residual.

157
00:17:51,200 --> 00:17:57,760
So, incorporating ancestral sampling for DDPMs, we have our algorithm of DPS where we can derive

158
00:17:57,760 --> 00:18:04,880
separate algorithms according to the measurement model in hand. Note that line seven is where DPS

159
00:18:04,880 --> 00:18:10,800
takes place. If we were to remove line seven, we would simply be sampling from the prior distribution,

160
00:18:11,840 --> 00:18:16,960
the usual diffusion models, what they do. And hence, our algorithm is just very simple

161
00:18:16,960 --> 00:18:24,000
modification of the usual DDPM sampling. When we do that, we achieve these results.

162
00:18:24,880 --> 00:18:29,840
So, since our method is not dependent on the measurement model, we can apply the same score

163
00:18:29,840 --> 00:18:34,880
function to various problems. For example, this is the result of applying our method to super

164
00:18:34,880 --> 00:18:42,080
resolution that are contaminated with Gaussian noise. We can also apply our method to noisy in

165
00:18:42,080 --> 00:18:49,920
painting, and this is the case where 92% of the pixels are blocked out in a 256x256 image.

166
00:18:52,000 --> 00:18:56,720
This is noisy Gaussian deep blurring. We can do noisy motion deep blurring.

167
00:18:57,120 --> 00:19:02,880
And what's even more, since we're not restricted to the choice of the forward operator A,

168
00:19:03,920 --> 00:19:07,760
for the first time in the context of inverse problem solving with diffusion models,

169
00:19:07,760 --> 00:19:14,240
we show that we can also solve nonlinear inverse problems, such as phase retrieval presented here.

170
00:19:14,960 --> 00:19:21,760
And for those of you who do not know what phase retrieval is, it's a problem that tries to estimate

171
00:19:22,400 --> 00:19:28,000
the phase and the Fourier domain, and this is a notoriously hard problem because most of the

172
00:19:28,000 --> 00:19:33,840
information of the image is actually concentrated on the Fourier phase rather than the Fourier

173
00:19:33,840 --> 00:19:42,000
magnitude of an image. And we can also solve problems like non-uniform deep blurring, which

174
00:19:42,000 --> 00:19:47,280
is another nonlinear inverse problem, given that the forward measurement model is actually

175
00:19:47,280 --> 00:19:53,680
differentiable. This is one application that I want to highlight because we actually used

176
00:19:53,680 --> 00:20:00,160
a complex neural network that emulates the nonlinear blurring here. And neural network is one

177
00:20:00,160 --> 00:20:05,760
of the most nonlinear forward models that you can imagine. So even when the forward operator is

178
00:20:05,760 --> 00:20:10,640
highly nonlinear, we can see that DPS is capable of recovering the image with high fidelity.

179
00:20:11,440 --> 00:20:24,720
So that was it for DPS. I think we can briefly stop to see if any of you have any questions.

180
00:20:24,720 --> 00:20:33,200
I have a question. So basically, did you also show, because you can have, due to the stochasticity,

181
00:20:33,200 --> 00:20:40,000
you can have posterior distribution of the denoid samples, right? So do you notice a difference

182
00:20:40,080 --> 00:20:46,080
compared for the one you, the blur, for example, this image, do you get some deviations that are

183
00:20:46,720 --> 00:20:52,960
meaningful in one sense? Yeah, so I'm sorry that I don't have examples here, but

184
00:20:54,240 --> 00:21:01,120
yeah, you do see quite a bit of stochasticity, especially when the degradation is heavy as,

185
00:21:02,160 --> 00:21:08,320
for example, the example here, blurring is heavy. So the posterior samples have high

186
00:21:08,320 --> 00:21:18,160
standard deviation in terms of reconstructions. Yeah, that's another question. I don't know if

187
00:21:18,160 --> 00:21:22,720
you're going to explain it and maybe let me study that, but you also mentioned that reconstruction

188
00:21:22,720 --> 00:21:28,400
is possible if you have a prior, right? Right. But how do you choose the prior? Do you learn

189
00:21:28,400 --> 00:21:36,000
it or is it something that you'd have to? Oh yeah. So the thing that we're proposing here is that

190
00:21:36,080 --> 00:21:40,240
we're using the diffusion prior, right? So we're using this generative prior,

191
00:21:40,240 --> 00:21:46,400
and the prior is learned through the usual training process of diffusion models. So

192
00:21:46,400 --> 00:21:51,440
whether it be score matching or epsilon matching or whatever, we only, what we only need in the

193
00:21:51,440 --> 00:21:59,920
sampling process is the pre-trained score function s theta here. So all the examples that I show you

194
00:21:59,920 --> 00:22:08,640
here are generated by pre-trained models available online. So specifically, we use the

195
00:22:08,640 --> 00:22:13,840
open AI model, and we don't need any fine-tuning for any of these problems. We can just plug it

196
00:22:13,840 --> 00:22:22,240
into the solver and that will act as the prior of the distribution. Okay, so the prior is trained

197
00:22:22,240 --> 00:22:27,200
on the data that you use it on. So you use the simulation of the model pre-trained on different

198
00:22:27,200 --> 00:22:37,680
data. Yeah. Thanks. So how this method would differ from the paper also from song et al for

199
00:22:37,680 --> 00:22:45,520
inverse problems in medical imaging? So is there a likely difference? They are different and that

200
00:22:45,520 --> 00:22:52,800
will be covered more in detail in the second section where I explain the nearest paper.

201
00:22:53,040 --> 00:23:00,800
Yeah. So if you could summarize, for example, in a few words, because the main paper from song et al,

202
00:23:00,800 --> 00:23:05,760
you can also do inverse problems straightforward. I mean, you can do the painting. What would be

203
00:23:05,760 --> 00:23:14,400
like the main contribution is this non-linear approach to inverse problems, or what would

204
00:23:14,400 --> 00:23:21,040
be the main difference? So it's two things. I guess I'll try to explain it later. So you're

205
00:23:21,040 --> 00:23:27,840
mentioning the original paper of song et al at iClear 2021 and there was a paper that you mentioned

206
00:23:27,840 --> 00:23:36,880
that tackled medical imaging in the next year of the same author. They solved inverse problems by

207
00:23:37,520 --> 00:23:43,360
using a so-called projection approach, which means that you're directly replacing what you have as

208
00:23:43,360 --> 00:23:48,480
the measurements and you're keeping only the rest of it. So this is the visualization of such

209
00:23:48,480 --> 00:23:56,000
projection process. And this projection process works for certain inverse problems, for example,

210
00:23:56,000 --> 00:24:04,000
in painting. And for in painting, when you have a small degradation, for example, the mask is small

211
00:24:04,000 --> 00:24:10,960
or the blocked out pixels are not dominant. In those cases, projection approaches work fine.

212
00:24:11,680 --> 00:24:19,840
But for example, when you have a large mask, for example, a 128 by 128 mask cure, when you

213
00:24:19,840 --> 00:24:27,200
apply song's method, projection type approaches tend to fail dramatically. This is also relevant

214
00:24:27,200 --> 00:24:33,520
to the case of medical imaging, where you have high degradations. These projection-based approaches

215
00:24:33,520 --> 00:24:46,240
will typically fail. Whereas DPS that I proposed in the last section and the MCG that I will explain

216
00:24:46,240 --> 00:24:55,760
soon after does not have this property. And the methodological main difference is that

217
00:24:56,640 --> 00:25:03,680
you song and all use projection type approaches, whereas MCG or DPS uses gradient type approaches.

218
00:25:03,680 --> 00:25:10,880
So it's a smoother transition towards the that adapts to the measurement process.

219
00:25:15,760 --> 00:25:18,800
Yeah, that's pretty nice. Yeah, that's pretty clear. Thanks.

220
00:25:19,520 --> 00:25:28,080
Yeah. Okay. Should I move on to the next section?

221
00:25:31,040 --> 00:25:40,800
Yeah, thanks. Yeah, so the next paper that I'm going to talk about is actually a paper that came

222
00:25:40,800 --> 00:25:48,480
before DPS. But I explained DPS first because it really uses the same gradient, I would say

223
00:25:48,480 --> 00:25:54,400
similar, but almost the same gradient method with DPS. But at the time of development of MCG paper,

224
00:25:54,400 --> 00:26:04,000
we did not really understand how mathematically the solvers would be derived. So here in this

225
00:26:04,000 --> 00:26:09,600
paper, we focused more on the explanation in the geometric context of diffusion models. So I hope

226
00:26:09,600 --> 00:26:14,640
that this also helps the understanding of the underlying intuitive

227
00:26:16,320 --> 00:26:18,480
things about these gradients based methods.

228
00:26:20,880 --> 00:26:26,480
So in order to understand the paper, let us review some of the important properties of

229
00:26:26,480 --> 00:26:31,760
high dimensional Gaussian random variables. And specifically for very high dimensional Gaussian

230
00:26:31,760 --> 00:26:36,800
random variables, while the mode of the PDF may be near the main of the distribution,

231
00:26:37,440 --> 00:26:42,880
the measure is actually concentrated in the annulus that is distant from the center.

232
00:26:44,240 --> 00:26:48,240
This is the reason why when we add fixed variance Gaussian noise to an image,

233
00:26:49,840 --> 00:26:54,960
you never really see a clean image, even when the highest probability of a Gaussian noise would be

234
00:26:54,960 --> 00:27:02,720
zero noise. We always get images with very similar noise levels. Now extending that,

235
00:27:02,800 --> 00:27:06,880
let us think of a random variable Y that is corrupted with Gaussian noise

236
00:27:06,880 --> 00:27:12,560
by adding some noise. Sorry, can you maybe rephrase that? I mean, I think that's interesting,

237
00:27:12,560 --> 00:27:18,960
the slides before. So the concentration of Gaussian measure, right? Yeah, so this is a

238
00:27:18,960 --> 00:27:24,240
Gaussian annulus theorem, right? And then the effect that we see, what you said is that we

239
00:27:25,120 --> 00:27:31,680
don't see like, so we see the same effect, right? Or what do you mean? Definitely. So

240
00:27:32,240 --> 00:27:38,480
because of the Gaussian annulus theorem, what we see when we add Gaussian noise to images with,

241
00:27:39,040 --> 00:27:46,960
let's say, fixed variance Gaussian noise to an image, I guess the highest probability

242
00:27:46,960 --> 00:27:57,920
instantiation of this Gaussian distribution would be no noise, right? Zeroes. But that never happens.

243
00:27:57,920 --> 00:28:04,240
So if you add some random Gaussian noise to an image, you will always see some

244
00:28:05,280 --> 00:28:11,280
very similar noise level images that are corrupted in a very similar way. So that was what I was

245
00:28:11,280 --> 00:28:22,480
talking about. Yeah, okay. Yeah. So it's also due to the convolution, right? You can observe this

246
00:28:22,560 --> 00:28:30,640
as a convolution. Yeah. So this is kind of related to what I was talking about before.

247
00:28:30,640 --> 00:28:38,960
So let's say you have like a linear space of X. And what you would do is you add some Gaussian

248
00:28:38,960 --> 00:28:47,040
noise to that random variable X. Because this is a convolution, we can show that the marginal

249
00:28:47,040 --> 00:28:55,680
distribution P of Y is actually a, the measure actually concentrates on the hypercylinder

250
00:28:56,240 --> 00:29:04,000
that is distant from the center of PX. So this is kind of intuitive reason why

251
00:29:05,760 --> 00:29:09,680
the noisy images with the fixed variance have similar looking structure.

252
00:29:10,560 --> 00:29:20,160
So in this work, we propose to interpret diffusion models using some assumptions about the data

253
00:29:20,160 --> 00:29:27,600
manifold. So the usual manifold hypothesis states that the data lives in a low dimensional manifold

254
00:29:27,600 --> 00:29:32,880
with much smaller dimensionality than the ambient space. And in this work, we add an

255
00:29:32,880 --> 00:29:38,080
additional strong assumption that the manifold, the central data manifold is locally linear.

256
00:29:38,560 --> 00:29:43,360
And in order to leverage, and this is because we want to leverage the results from the concentration

257
00:29:43,360 --> 00:29:51,040
of Gaussian measure. And specifically, when we assume that our data manifold is locally linear,

258
00:29:51,600 --> 00:29:56,880
what we can show is that the geometry of diffusion model is given by the successive manifolds

259
00:29:56,880 --> 00:30:02,400
where there exists a clean data manifold in the center. And there are continuously many

260
00:30:02,480 --> 00:30:10,800
noisy manifolds where the noisy samples reside. So these are the manifolds of the noisy images.

261
00:30:13,360 --> 00:30:19,360
Extending that to a continuous limit, we propose that diffusion model is an onion because

262
00:30:19,360 --> 00:30:24,560
where when we push T to infinity, the distribution actually becomes a pure Gaussian.

263
00:30:25,120 --> 00:30:29,760
And since the measure of a high dimensional Gaussian resides in a Gaussian hypersphere,

264
00:30:30,480 --> 00:30:35,600
the intermediate manifolds are covers of the central manifolds that interpolates between these

265
00:30:35,600 --> 00:30:43,440
two. Therefore, this diffusion process would correspond to these red arrows, and the reverse

266
00:30:43,440 --> 00:30:51,600
diffusion would correspond to the blue arrows. And furthermore, when we are considering the case

267
00:30:51,600 --> 00:30:56,400
where we are trying to solve inverse problems with diffusion, we are left with the figure on

268
00:30:56,400 --> 00:31:01,600
the right. Here, we are not only trying to find points in the central manifold M,

269
00:31:02,480 --> 00:31:08,160
but we're trying to find the intersection between the manifold M and the line Y equals HX.

270
00:31:13,680 --> 00:31:20,560
So I've already said this earlier, but let's examine some of the representative methods that

271
00:31:20,560 --> 00:31:26,560
solve inverse problems using diffusion. These methods, Song at L and Choi at L,

272
00:31:27,520 --> 00:31:33,200
are a type of projection-based approach, since they directly replace the part that is known

273
00:31:33,760 --> 00:31:42,800
and keep only the unknown part. Another way to impose data consistency in the Bayesian framework

274
00:31:42,800 --> 00:31:48,320
is to incorporate likelihood. In other words, try to minimize the residual by gradient descent.

275
00:31:49,120 --> 00:31:54,560
However, with diffusion models, if we were to use naive gradient descent with a noisy residual,

276
00:31:54,560 --> 00:31:59,600
we will acquire no meaningful gradient, as the residual between HXI and Y

277
00:32:00,560 --> 00:32:03,760
will only be focused on the Gaussian noise part of the residual.

278
00:32:05,840 --> 00:32:11,200
Now, as we saw in diffusion posterior sampling, one can actually approximate the correct likelihood

279
00:32:11,200 --> 00:32:17,600
by switching to 2D denoised estimate X0 hat here. Here, visually and intuitively,

280
00:32:17,600 --> 00:32:24,400
we see why this may help. This is because after denoising, the residual is more likely to focus

281
00:32:24,400 --> 00:32:31,840
on the structural differences between HX0 hats and Y. In practice, using this gradient step over

282
00:32:31,840 --> 00:32:39,680
the projections work much, much better. And you can see the kind of the resulting differences

283
00:32:39,680 --> 00:32:43,520
when we use projection type approach and the gradient approaches.

284
00:32:47,520 --> 00:32:53,760
So in the paper, we properly analyze why this would be the case, leveraging the geometric

285
00:32:53,760 --> 00:33:00,240
understanding of diffusion models. So first, under our assumptions, we show that 2D denoising

286
00:33:00,240 --> 00:33:06,320
corresponds to an orthogonal projection to the data manifold. However, note that we have to consider

287
00:33:06,320 --> 00:33:12,160
two components of the data manifold, the orthogonal one that is related to the noise components,

288
00:33:12,160 --> 00:33:15,120
and the tangential one that is related to the data fidelity.

289
00:33:16,560 --> 00:33:22,080
So score function alone cannot handle the data fidelity. And here comes our theorem,

290
00:33:22,080 --> 00:33:27,440
which states that the proposed manifold constraint gradient term points to a direction that is

291
00:33:27,440 --> 00:33:33,200
tangential to the central data manifold. And therefore, our solution will move closer to Y

292
00:33:33,200 --> 00:33:41,440
equals HX every time we apply this MCG step. Another important aspect of this is that since MCG

293
00:33:41,440 --> 00:33:49,200
lets the sample move on the data manifold, whereas projections move the sample off the data manifold

294
00:33:51,040 --> 00:33:55,520
because projections are hard constraints that try to make Y minus HX equals 0.

295
00:33:56,080 --> 00:34:04,640
Note that MCG will not do that. It will move on the data manifolds in the direction tangential

296
00:34:04,640 --> 00:34:09,360
to the manifold. So those falling off the manifold will not happen.

297
00:34:11,680 --> 00:34:18,320
So here, note that the score functions that are pre-trained were not trained with

298
00:34:19,040 --> 00:34:26,800
samples that are off the data manifold. So they were trained with the samples that were on the

299
00:34:26,800 --> 00:34:33,040
noisy data manifolds. So it will not be able to properly denoise samples that are off the data

300
00:34:33,040 --> 00:34:39,040
manifolds. So by using projections, the intermediate error will probably accumulate,

301
00:34:39,680 --> 00:34:44,960
and we conjecture that this is the reason why projection-based approaches often fail to produce

302
00:34:44,960 --> 00:34:54,240
a good sample. But however, as I said, in the time we wrote MCG, there was a gap between

303
00:34:54,240 --> 00:35:02,000
theory and practice because even with the MCG steps, we had a hard time trying to solve inverse

304
00:35:02,000 --> 00:35:08,240
problems. So we had to resort to some projections within the denoising and the gradient steps.

305
00:35:09,120 --> 00:35:14,800
But in DPS, we managed to achieve a general solver that only uses these gradient steps.

306
00:35:19,600 --> 00:35:26,720
Now, for the Gaussian case, the only difference between MCG and DPS is that we do not use projections

307
00:35:26,720 --> 00:35:33,840
for DPS. Leveraging the geometric viewpoint of diffusion models, we can think that DPS is

308
00:35:33,840 --> 00:35:41,120
superior to MCG specifically because we avoid such projections. As projections here can potentially

309
00:35:41,120 --> 00:35:50,160
throw samples off the data manifold. Hence, DPS will typically be more stable by avoiding such

310
00:35:50,160 --> 00:35:57,200
throw-offs. And by the way, this is just an illustration. It doesn't really guarantee anything

311
00:35:57,200 --> 00:36:02,720
that our solver will always stay on the noisy data manifolds.

312
00:36:05,520 --> 00:36:13,440
Yeah, so that was about it for MCG. Any questions up until now?

313
00:36:19,440 --> 00:36:25,600
I just have a question, but maybe a very simple question. At the beginning of this slide,

314
00:36:25,600 --> 00:36:34,480
when you were explaining the, I think it's, no, the next one, two after this one.

315
00:36:37,040 --> 00:36:48,080
But yeah, this one, the previous one. How do you define this k? So n, what is n and what is k?

316
00:36:48,080 --> 00:36:59,440
I would have to look for the definition of n and k here.

317
00:37:10,320 --> 00:37:16,080
From my memory, n is the dimension of the ambient space, in case the dimension of the

318
00:37:16,080 --> 00:37:23,840
locally linear, low-dimensional dimensionality of the data manifold.

319
00:37:25,920 --> 00:37:33,360
I guess you're assuming that this, so the brown, the mean is basically used, the mean of each

320
00:37:33,360 --> 00:37:40,400
intermediate distribution. So it's the mean of the diffuse or denoise distribution.

321
00:37:41,360 --> 00:37:50,880
And then this, I'm just wondering, shouldn't you start going from a big

322
00:37:52,560 --> 00:37:57,440
standard deviation to a small standard deviation? Because basically when you do the denoising,

323
00:37:57,440 --> 00:38:04,880
you start with a high variance and then you start denoising and the variance slowly decreases.

324
00:38:05,600 --> 00:38:10,160
That's just something I was thinking that if that's not the case, so I'm missing out something.

325
00:38:11,120 --> 00:38:15,840
Can you repeat the question again? I'm not sure if I understood it correctly.

326
00:38:15,840 --> 00:38:23,760
Yeah, so the point of this is to interpret what's going on, like how the denoising samples

327
00:38:24,480 --> 00:38:33,600
behave in the data manifold through the trajectory. So when you do the denoising,

328
00:38:33,600 --> 00:38:39,200
so you start with the isotropic Gaussian, which is the highest entropy you can have. So you start

329
00:38:39,200 --> 00:38:46,880
with the maximum variance that you have and then slowly you start removing the noise and by doing

330
00:38:46,880 --> 00:38:56,640
that interactively the variance at each intermediate step goes down. So what I was expecting is to have

331
00:38:57,200 --> 00:39:02,480
this behavior when you have the mean and then the variance is large at the beginning and then

332
00:39:02,480 --> 00:39:08,720
starts decreasing. Because ideally you would reduce the variance, but I don't know if I'm missing

333
00:39:08,720 --> 00:39:16,400
something there. Oh, but I think it's what you're saying is kind of different from what

334
00:39:16,400 --> 00:39:23,600
this slide is saying. So I'm not really stating anything about denoising anything because the

335
00:39:24,640 --> 00:39:30,400
flow here is that we're first assuming that there exists a central data manifold M here.

336
00:39:31,280 --> 00:39:40,240
And then we're trying to visualize how the manifold of the noisy images would look like

337
00:39:40,240 --> 00:39:48,560
if there exists the central data manifold M. And I'm claiming that these noisy data manifolds will

338
00:39:48,560 --> 00:39:56,560
be covers of the central data manifold, where the distance between from the central data manifold

339
00:39:56,560 --> 00:40:06,960
will be defined by this constant value. So it's really not about starting from high variance

340
00:40:06,960 --> 00:40:12,800
Gaussian noise and reducing it to anything. We're starting from a clean image and then we're defining

341
00:40:12,800 --> 00:40:20,160
the manifolds of these of these noisy images. Okay, so I will have to look at that for I guess also

342
00:40:20,160 --> 00:40:42,640
more too. Yeah, thanks. Yeah. So I guess we just tell me if

343
00:40:42,800 --> 00:40:53,520
if I'm

344
00:40:53,520 --> 00:40:55,520
it's gonna get it's a good cliffhanger.

345
00:40:58,480 --> 00:40:59,200
It's first again.

346
00:40:59,200 --> 00:41:07,440
Um, do you hear me okay?

347
00:41:08,960 --> 00:41:16,960
Yeah, yes. Yeah, so I think it's about 48 minutes. So I don't think we have to go over the final

348
00:41:16,960 --> 00:41:30,000
section. So yeah. So I'll just try to we just we cannot see you or the slides anymore if you

349
00:41:30,000 --> 00:41:39,760
can do the same thing as before to stop sharing and start again. Yeah. Well, the talk is basically

350
00:41:40,720 --> 00:41:47,280
done. So okay, that was the last slide if we if we don't cover the last section.

351
00:41:50,240 --> 00:41:54,160
I'm not sure if this is the correct way of saying thank you very much in in Dutch.

352
00:41:55,120 --> 00:41:56,560
Maybe you should put it in Italian.

353
00:42:02,960 --> 00:42:06,640
Yeah, so yeah, that is the right way to say thank you. Thank you too.

