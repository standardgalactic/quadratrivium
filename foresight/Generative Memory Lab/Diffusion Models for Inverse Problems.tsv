start	end	text
0	7520	Great. Yeah, so hi, I'm Hyunjin Chung. I'm a PhD student at bio-imaging and signal
7520	13200	processing and learning lab at KAIST Korea. I'm honored to give a talk in such a prestigious
13200	19040	group in Netherlands. And so thank you very much for having me here. So today I'll give a talk
19040	23520	mostly about diffusion models and how to use them to solve inverse problems in imaging,
24080	30080	focusing on a few recent papers that I wrote. So specifically it will be focused on base and
30080	35440	inference in the context of inverse problem solving. So I believe it will be of quite a
35440	40800	relevance to you. And I'm pretty sure that most of you are already familiar with diffusion models,
40800	47360	but some of you may not be familiar with inverse problems. So and since we have a rather small
47440	54880	group, I feel free to speak up with your microphone. I'm very sure that I'm going to miss
54880	59520	some chats. So I hope the talk will be as interactive as possible.
62720	68880	Yeah, so the talk will be structured in a bit of an odd way because we're going to first talk about
68880	74960	two closely related work in a reverse order. So the first part of the talk will be about a paper
74960	81200	named DPS, accepted to iClear recently. Then we will move backwards and talk about MCG,
81200	85920	which was supposed to be the main content of the talk today. And finally, if time allows,
85920	91440	I would also like to briefly touch the most recent work that applies these methods to more
91440	99440	advanced problems called blind inverse problems. So I'll just briefly cover the basic concepts
99440	104960	of diffusion models. I don't want to bore you too much, but I'll just speak in a briefly in
104960	110880	a score perspective. So what we do in diffusion models is we try to estimate the gradient of
110880	115440	the lock density with the neural network S of theta, which points to the high density most of the
115440	123120	distribution. In order to circumvent the intractable explicit score matching, you train your network
123120	128800	with denoising score matching, which boils down to essentially training a residual denoiser.
130400	136720	Once that is done, you can use the train score functions to sample from the distribution p of x
136720	144240	with, for example, Langevin MCMC. And another interesting view is that one can view the data
144240	150240	noising process as some linear forward SE, and the data generating process as the corresponding
150240	156160	reverse stochastic differential equation, where the drift function is governed by the score function.
156720	162080	Hence, when we want to sample data, you can discretize the reverse SDE and numerically
162080	169520	solve it using the pre-trained score function. On the other hand, here we are interested in
169520	174560	solving inverse problems. In the inverse problem setting, our aim is to recover the ground truth
174560	179040	x from the noisy measurements y obtained through some integral imaging system A.
180000	186560	And there will be some pollution of noise, and here, A, sorry, and the problem is naturally
186560	190880	opposed, which means that there exist infinitely many solutions to this problem,
192080	199280	meaning that given this noisy image, noisy blurred image of the leopard, there could exist
199280	207680	a lot of different feasible answers to the actual clean image. So in order to correctly specify
207680	213840	which one of the solutions is the one that we want, we need to specify the prior of the data
213840	219600	distribution, and in other words, how the images would actually look like in real world.
222160	228160	Examples of such inverse problems include in-painting and deep learning in the context
228160	232640	of low-level vision. They're also called image restoration problems. And in the context of
232640	237760	biomedical imaging, there could be compressing MRI, sparsive CT, and so on.
240400	244640	So we'll go over how we can solve such inverse problems with diffusion models.
246960	253840	So let's first consider the following measurements model. Now, given y, what we want is to sample
253840	261360	from the posterior distribution p of x given y. And using Bayes' rule, it's straightforward to see
261360	267520	that the gradient of the log posterior is the sum of gradient of the log likelihood
267520	274080	plus the score function. Here, gradient of the log likelihood in red gives us the information
274080	279680	about the measurement process. And the score function is what we've already estimated with
279680	286240	the neural network. So one should note that the likelihood function p of y given x is in most cases
286320	291600	known. And in 95% of the cases, it can be modeled relatively well with the Gaussian.
292240	298720	And among the rest of the 5%, 4% is Poisson, meaning that the likelihood function is often known
298720	304080	a priority. So hence, in the perspective of Bayesian reconstruction, all we have to do when
304080	309920	posterior sampling is desirable is to specify the correct prior distribution. Then the measurement
309920	318560	process is hand wavy, it's known so we can sample from the posterior. So let's consider the case
318560	325040	where we modeled the data distribution with the diffusion model. From the score or the SDE perspective,
325040	330880	recall that the generative process is defined by this general form of reverse SDE. So in order
330880	336240	to do unconditional sampling from the prior distribution, we can use the following discretization
336320	342960	steps of the reverse SDE. So for simplicity, I denoted Euler-Maruyama discretization here,
342960	348000	but other forms of sampling can of course be used, such as ancestral sampling of DDPMs
348560	353600	or hybrid methods that incorporate MCMC chains within the numerical integration solvers.
354480	359520	They're called predictor-corrector solvers in score SDE paper.
360160	368000	Moreover, when we want to do posterior sampling, we can use what we've derived from the Bayes rule
368960	375760	and incorporate the gradient of the walk likelihood. So here comes the problem.
375760	381840	Can I ask a question? I'm sorry, I'm decently new to the diffusion model, so maybe there is
381840	386160	something I didn't understand. I don't understand the sign, why there is a minus sign in front of
386240	389680	the even the original paper, why there is a minus sign in front of the gradient of the
391600	399440	score. Oh, so you're asking, how did the reverse SDE come along here, right?
400000	408320	Well, I'm saying that with that minus sign, it will do the opposite of going to the high
408320	412880	probability regions. It will go away from it. So am I missing something?
413840	420080	Oh, yeah. So intuitively, if you want to sample from the high density modes of the
420080	427680	distribution, you would kind of do gradient ascent with it. And yeah, there's a caveat here,
427680	435600	because the differential dt here is a time running backwards. So you have to show the minus sign.
435680	442960	Yeah. But then the sign of f is wrong, because the sign of f should have the same
442960	453120	sign of g, doesn't it? Actually, no. So the derivation process of this reverse SDE comes from
454160	460480	the thing called Anderson's theorem. So if you plug in this general form of forward SDE,
460480	464960	then there's a theorem that states that this reverse SDE is the correct form.
464960	469840	I understand. But so if there isn't, let's say that there is no score, so there is no goal,
469840	475200	then if you have a, let's say, a Oldston-Ulenbach process, the forward and reverse dynamics are the
475200	490000	same. So you're saying that f dt is running backwards, then f should be negative. If you
490000	494320	have a stationary process like an Oldston-Ulenbach, regardless whether you run it forward or
494320	499440	backward, you have the same dynamics. Therefore, you are not going to flip the sign of f if you
499440	515440	change the time. Let's say, in an Oldston-Ulenbach process, f induces the mean reversion. So if
515440	519520	you actually flip the sign, you will have the process going away from the mean, which is not
519520	524880	the reverse time process. The reverse time process wants to go back to the mean exactly like the
524880	530640	forward process. Right, right. I'm also a bit confused myself.
534080	540480	Because everything works, I think, I don't know. So again, I could be misinterpreting something.
541440	548640	Because I do see this in papers, but I just,
553680	558800	it's, again, if it's an Oldston-Ulenbach process, the forward and reverse dynamics,
559840	563360	assuming score zero, should be the same. So you cannot flip the sign. Otherwise,
563440	571840	you get the opposite, you get the diverging process. And if you assume the generative
571840	575920	direction to be forward, then the sign of the score should be plus, not minus. But of course,
575920	580720	it would become minus if you are actually considering that going backward. But okay,
580720	585680	but maybe we can discuss this offline. And also, I mean, again, it could be that I'm missing some
585680	592400	details of the notation that for which things will match up. I'm not sure if, especially in the case
592400	599680	of Ornstein-Ulenbach process, is there, is the score function zero there? Well, so the,
601200	608560	well, so the, if the forward process is Oldston-Ulenbach, then f of x of t is just minus alpha x.
609200	613920	Right. And that would be true both in the forward and in the backward,
613920	617280	because again, it's time invariant. If you flip, you don't change anything.
618240	623280	The score happened in order to enforce the terminal condition that the process has to fit the data.
626160	630480	So it, in this sense, it's nothing to do with the Ornstein-Ulenbach process. It's just a
632080	637520	starting point of the process, which if then, if you want to reverse, it will actually go back to
637520	643200	there. Yeah, I think we should come back to this later. I think it's better to discuss
648160	656480	So coming back to where we were, I think it was right here. So I was talking about how we could
657040	662640	switch from prior sampling to posterior sampling by, by just plugging, plugging in this base rule.
663680	670160	And here I said that care must be taken here because the gradient of the log likelihood
670160	676640	is in fact intractable. And note that this is different from my claim earlier that the
676640	682320	likelihood function is in most cases known. And this is, this arises from the fact that
682320	689200	there are noisy x i's here rather than x zero. So let us dive into closely examine what I mean by this.
690400	697120	To see why this is the case, I'm sorry that I'm switching notations with i and t. For i's,
697120	706480	I'm denoting discretized things and t. I'm just pointing at some general continuous time.
706880	713840	and instantiations. So here, consider the following probabilistic graph in the context of diffusion
713840	720480	models. So we know two conditional distributions p of y given x zero and p of x t given x zero.
721200	726000	And for now, let's assume that the first one is the measurement distribution is given as typically
726000	732480	Gaussian. So, and the second word, the four distribution of the diffusion is also Gaussian.
733440	739360	However, the reverse distribution p of x zero given x t shown with blue dotted line is intractable
739360	746000	in general. So hence p of y given x t is intractable because we have no information about this blue
746000	753600	dotted line. So in our work, we aim to approximate the intractable distribution.
755200	761760	The first key comes from the factorization. Since x zero is conditionally independent on y
761760	768400	and x t, we can factor the integrand as follows. Where the former term is what we know and the
768400	774560	latter term is what we partially know. And by partially known, I mean that we know how to obtain
774560	781360	the posterior mean of the distribution, which is given by the 3ds formula. Note that 3ds formula
781360	787440	is used widely in diffusion model context, as it states that the posterior mean, or the denoised
787440	793200	estimate, can always be achieved when we know the so-called blurred score function. That is,
793920	797520	the score function of the intermediate noisy variables x t.
801840	809520	Here we see that we can plug in 3d denoising to achieve some posterior mean of the
810160	818320	distribution in the context of the dpms. And to fully enjoy the effectiveness of 3ds formula,
818320	824240	and because leaving the expectation outside is intractable, let us push the expectation inside
824240	832800	for now. When we do that, by the 3ds formula proposition, we have a fully tractable distribution
832800	837840	where the condition is now given by x zero hat, the denoised estimate from x t.
840480	845920	Now, since we proposed an approximation here, it is important that we quantify the approximation
845920	850480	bound. And for that, we have a theorem that states the approximation error,
850480	855200	or the so-called Jensen gap, and used by pushing the expectation inside the function.
856720	863520	And we show that this approximation error is upper bounded by this constant with 3 constituents.
864480	868880	So, the latter two parts are usually bounded in most practical situation.
869680	877200	And the interesting part here is the first one, where we see that when sigma goes to zero,
877200	882960	the entire constant goes to zero. This is useful because it states that in cases where we have
882960	888480	negligible measurement noise, the approximation will be tight. However, in practice, even when
888480	897760	we have high measurement noise, the method also works very well. So, just summing everything up,
897760	902000	thanks to theorem one, we can achieve what we call diffusion posterior sampling,
902000	907520	or DPS in short. We start with standard Bayes rule in the context of diffusion models.
908800	911200	We can apply theorem one to get the approximation.
918480	938800	I lost him. I don't hear anything anymore. Do you hear me now?
938960	942960	Yeah. Yeah. Yeah. Sorry. Where did the...
950960	953120	I don't have any sound anymore.
957200	961520	Yeah, now we're here. Yeah, now it's back. But we cannot see the screen.
961520	970880	Can you try to share the screen again? I can see a screen. I can see the screen,
970880	981520	the Gaussian and Poisson. Yeah, me too. Yeah. Can you try to just stop sharing again?
981520	984480	We'll share it again. Yeah, sure. Thanks.
990240	994400	So, is this where the connection was disconnected?
996080	999600	Yeah, you were just starting about the Gaussian and Poisson. Yeah.
1001200	1006000	So, I was saying that these gradients can be analytically computed because these
1006000	1011360	functional forms are already known. So, we see that for Gaussian, we're essentially
1011360	1016480	performing gradient descent that minimizes the squared L2 norm of the residual.
1018320	1023760	I cannot see the slides, but I cannot see the screen. Yeah, neither. No.
1025040	1030560	They just dropped away in the halfway through your sentence.
1031360	1034320	Okay. So, let's try to reshare...
1041360	1046320	Yeah, now I can see them. Yeah, me too. Thanks. So, does this work?
1048560	1057760	Yes. It seems like... Yeah. So, yeah, I was saying that for Gaussian measurement models,
1057760	1064160	we're trying to do gradient descent that minimizes the squared L2 norm of the residual,
1064160	1068560	and for Poisson measurements, we're minimizing the squared weighted norm of the residual.
1071200	1077760	So, incorporating ancestral sampling for DDPMs, we have our algorithm of DPS where we can derive
1077760	1084880	separate algorithms according to the measurement model in hand. Note that line seven is where DPS
1084880	1090800	takes place. If we were to remove line seven, we would simply be sampling from the prior distribution,
1091840	1096960	the usual diffusion models, what they do. And hence, our algorithm is just very simple
1096960	1104000	modification of the usual DDPM sampling. When we do that, we achieve these results.
1104880	1109840	So, since our method is not dependent on the measurement model, we can apply the same score
1109840	1114880	function to various problems. For example, this is the result of applying our method to super
1114880	1122080	resolution that are contaminated with Gaussian noise. We can also apply our method to noisy in
1122080	1129920	painting, and this is the case where 92% of the pixels are blocked out in a 256x256 image.
1132000	1136720	This is noisy Gaussian deep blurring. We can do noisy motion deep blurring.
1137120	1142880	And what's even more, since we're not restricted to the choice of the forward operator A,
1143920	1147760	for the first time in the context of inverse problem solving with diffusion models,
1147760	1154240	we show that we can also solve nonlinear inverse problems, such as phase retrieval presented here.
1154960	1161760	And for those of you who do not know what phase retrieval is, it's a problem that tries to estimate
1162400	1168000	the phase and the Fourier domain, and this is a notoriously hard problem because most of the
1168000	1173840	information of the image is actually concentrated on the Fourier phase rather than the Fourier
1173840	1182000	magnitude of an image. And we can also solve problems like non-uniform deep blurring, which
1182000	1187280	is another nonlinear inverse problem, given that the forward measurement model is actually
1187280	1193680	differentiable. This is one application that I want to highlight because we actually used
1193680	1200160	a complex neural network that emulates the nonlinear blurring here. And neural network is one
1200160	1205760	of the most nonlinear forward models that you can imagine. So even when the forward operator is
1205760	1210640	highly nonlinear, we can see that DPS is capable of recovering the image with high fidelity.
1211440	1224720	So that was it for DPS. I think we can briefly stop to see if any of you have any questions.
1224720	1233200	I have a question. So basically, did you also show, because you can have, due to the stochasticity,
1233200	1240000	you can have posterior distribution of the denoid samples, right? So do you notice a difference
1240080	1246080	compared for the one you, the blur, for example, this image, do you get some deviations that are
1246720	1252960	meaningful in one sense? Yeah, so I'm sorry that I don't have examples here, but
1254240	1261120	yeah, you do see quite a bit of stochasticity, especially when the degradation is heavy as,
1262160	1268320	for example, the example here, blurring is heavy. So the posterior samples have high
1268320	1278160	standard deviation in terms of reconstructions. Yeah, that's another question. I don't know if
1278160	1282720	you're going to explain it and maybe let me study that, but you also mentioned that reconstruction
1282720	1288400	is possible if you have a prior, right? Right. But how do you choose the prior? Do you learn
1288400	1296000	it or is it something that you'd have to? Oh yeah. So the thing that we're proposing here is that
1296080	1300240	we're using the diffusion prior, right? So we're using this generative prior,
1300240	1306400	and the prior is learned through the usual training process of diffusion models. So
1306400	1311440	whether it be score matching or epsilon matching or whatever, we only, what we only need in the
1311440	1319920	sampling process is the pre-trained score function s theta here. So all the examples that I show you
1319920	1328640	here are generated by pre-trained models available online. So specifically, we use the
1328640	1333840	open AI model, and we don't need any fine-tuning for any of these problems. We can just plug it
1333840	1342240	into the solver and that will act as the prior of the distribution. Okay, so the prior is trained
1342240	1347200	on the data that you use it on. So you use the simulation of the model pre-trained on different
1347200	1357680	data. Yeah. Thanks. So how this method would differ from the paper also from song et al for
1357680	1365520	inverse problems in medical imaging? So is there a likely difference? They are different and that
1365520	1372800	will be covered more in detail in the second section where I explain the nearest paper.
1373040	1380800	Yeah. So if you could summarize, for example, in a few words, because the main paper from song et al,
1380800	1385760	you can also do inverse problems straightforward. I mean, you can do the painting. What would be
1385760	1394400	like the main contribution is this non-linear approach to inverse problems, or what would
1394400	1401040	be the main difference? So it's two things. I guess I'll try to explain it later. So you're
1401040	1407840	mentioning the original paper of song et al at iClear 2021 and there was a paper that you mentioned
1407840	1416880	that tackled medical imaging in the next year of the same author. They solved inverse problems by
1417520	1423360	using a so-called projection approach, which means that you're directly replacing what you have as
1423360	1428480	the measurements and you're keeping only the rest of it. So this is the visualization of such
1428480	1436000	projection process. And this projection process works for certain inverse problems, for example,
1436000	1444000	in painting. And for in painting, when you have a small degradation, for example, the mask is small
1444000	1450960	or the blocked out pixels are not dominant. In those cases, projection approaches work fine.
1451680	1459840	But for example, when you have a large mask, for example, a 128 by 128 mask cure, when you
1459840	1467200	apply song's method, projection type approaches tend to fail dramatically. This is also relevant
1467200	1473520	to the case of medical imaging, where you have high degradations. These projection-based approaches
1473520	1486240	will typically fail. Whereas DPS that I proposed in the last section and the MCG that I will explain
1486240	1495760	soon after does not have this property. And the methodological main difference is that
1496640	1503680	you song and all use projection type approaches, whereas MCG or DPS uses gradient type approaches.
1503680	1510880	So it's a smoother transition towards the that adapts to the measurement process.
1515760	1518800	Yeah, that's pretty nice. Yeah, that's pretty clear. Thanks.
1519520	1528080	Yeah. Okay. Should I move on to the next section?
1531040	1540800	Yeah, thanks. Yeah, so the next paper that I'm going to talk about is actually a paper that came
1540800	1548480	before DPS. But I explained DPS first because it really uses the same gradient, I would say
1548480	1554400	similar, but almost the same gradient method with DPS. But at the time of development of MCG paper,
1554400	1564000	we did not really understand how mathematically the solvers would be derived. So here in this
1564000	1569600	paper, we focused more on the explanation in the geometric context of diffusion models. So I hope
1569600	1574640	that this also helps the understanding of the underlying intuitive
1576320	1578480	things about these gradients based methods.
1580880	1586480	So in order to understand the paper, let us review some of the important properties of
1586480	1591760	high dimensional Gaussian random variables. And specifically for very high dimensional Gaussian
1591760	1596800	random variables, while the mode of the PDF may be near the main of the distribution,
1597440	1602880	the measure is actually concentrated in the annulus that is distant from the center.
1604240	1608240	This is the reason why when we add fixed variance Gaussian noise to an image,
1609840	1614960	you never really see a clean image, even when the highest probability of a Gaussian noise would be
1614960	1622720	zero noise. We always get images with very similar noise levels. Now extending that,
1622800	1626880	let us think of a random variable Y that is corrupted with Gaussian noise
1626880	1632560	by adding some noise. Sorry, can you maybe rephrase that? I mean, I think that's interesting,
1632560	1638960	the slides before. So the concentration of Gaussian measure, right? Yeah, so this is a
1638960	1644240	Gaussian annulus theorem, right? And then the effect that we see, what you said is that we
1645120	1651680	don't see like, so we see the same effect, right? Or what do you mean? Definitely. So
1652240	1658480	because of the Gaussian annulus theorem, what we see when we add Gaussian noise to images with,
1659040	1666960	let's say, fixed variance Gaussian noise to an image, I guess the highest probability
1666960	1677920	instantiation of this Gaussian distribution would be no noise, right? Zeroes. But that never happens.
1677920	1684240	So if you add some random Gaussian noise to an image, you will always see some
1685280	1691280	very similar noise level images that are corrupted in a very similar way. So that was what I was
1691280	1702480	talking about. Yeah, okay. Yeah. So it's also due to the convolution, right? You can observe this
1702560	1710640	as a convolution. Yeah. So this is kind of related to what I was talking about before.
1710640	1718960	So let's say you have like a linear space of X. And what you would do is you add some Gaussian
1718960	1727040	noise to that random variable X. Because this is a convolution, we can show that the marginal
1727040	1735680	distribution P of Y is actually a, the measure actually concentrates on the hypercylinder
1736240	1744000	that is distant from the center of PX. So this is kind of intuitive reason why
1745760	1749680	the noisy images with the fixed variance have similar looking structure.
1750560	1760160	So in this work, we propose to interpret diffusion models using some assumptions about the data
1760160	1767600	manifold. So the usual manifold hypothesis states that the data lives in a low dimensional manifold
1767600	1772880	with much smaller dimensionality than the ambient space. And in this work, we add an
1772880	1778080	additional strong assumption that the manifold, the central data manifold is locally linear.
1778560	1783360	And in order to leverage, and this is because we want to leverage the results from the concentration
1783360	1791040	of Gaussian measure. And specifically, when we assume that our data manifold is locally linear,
1791600	1796880	what we can show is that the geometry of diffusion model is given by the successive manifolds
1796880	1802400	where there exists a clean data manifold in the center. And there are continuously many
1802480	1810800	noisy manifolds where the noisy samples reside. So these are the manifolds of the noisy images.
1813360	1819360	Extending that to a continuous limit, we propose that diffusion model is an onion because
1819360	1824560	where when we push T to infinity, the distribution actually becomes a pure Gaussian.
1825120	1829760	And since the measure of a high dimensional Gaussian resides in a Gaussian hypersphere,
1830480	1835600	the intermediate manifolds are covers of the central manifolds that interpolates between these
1835600	1843440	two. Therefore, this diffusion process would correspond to these red arrows, and the reverse
1843440	1851600	diffusion would correspond to the blue arrows. And furthermore, when we are considering the case
1851600	1856400	where we are trying to solve inverse problems with diffusion, we are left with the figure on
1856400	1861600	the right. Here, we are not only trying to find points in the central manifold M,
1862480	1868160	but we're trying to find the intersection between the manifold M and the line Y equals HX.
1873680	1880560	So I've already said this earlier, but let's examine some of the representative methods that
1880560	1886560	solve inverse problems using diffusion. These methods, Song at L and Choi at L,
1887520	1893200	are a type of projection-based approach, since they directly replace the part that is known
1893760	1902800	and keep only the unknown part. Another way to impose data consistency in the Bayesian framework
1902800	1908320	is to incorporate likelihood. In other words, try to minimize the residual by gradient descent.
1909120	1914560	However, with diffusion models, if we were to use naive gradient descent with a noisy residual,
1914560	1919600	we will acquire no meaningful gradient, as the residual between HXI and Y
1920560	1923760	will only be focused on the Gaussian noise part of the residual.
1925840	1931200	Now, as we saw in diffusion posterior sampling, one can actually approximate the correct likelihood
1931200	1937600	by switching to 2D denoised estimate X0 hat here. Here, visually and intuitively,
1937600	1944400	we see why this may help. This is because after denoising, the residual is more likely to focus
1944400	1951840	on the structural differences between HX0 hats and Y. In practice, using this gradient step over
1951840	1959680	the projections work much, much better. And you can see the kind of the resulting differences
1959680	1963520	when we use projection type approach and the gradient approaches.
1967520	1973760	So in the paper, we properly analyze why this would be the case, leveraging the geometric
1973760	1980240	understanding of diffusion models. So first, under our assumptions, we show that 2D denoising
1980240	1986320	corresponds to an orthogonal projection to the data manifold. However, note that we have to consider
1986320	1992160	two components of the data manifold, the orthogonal one that is related to the noise components,
1992160	1995120	and the tangential one that is related to the data fidelity.
1996560	2002080	So score function alone cannot handle the data fidelity. And here comes our theorem,
2002080	2007440	which states that the proposed manifold constraint gradient term points to a direction that is
2007440	2013200	tangential to the central data manifold. And therefore, our solution will move closer to Y
2013200	2021440	equals HX every time we apply this MCG step. Another important aspect of this is that since MCG
2021440	2029200	lets the sample move on the data manifold, whereas projections move the sample off the data manifold
2031040	2035520	because projections are hard constraints that try to make Y minus HX equals 0.
2036080	2044640	Note that MCG will not do that. It will move on the data manifolds in the direction tangential
2044640	2049360	to the manifold. So those falling off the manifold will not happen.
2051680	2058320	So here, note that the score functions that are pre-trained were not trained with
2059040	2066800	samples that are off the data manifold. So they were trained with the samples that were on the
2066800	2073040	noisy data manifolds. So it will not be able to properly denoise samples that are off the data
2073040	2079040	manifolds. So by using projections, the intermediate error will probably accumulate,
2079680	2084960	and we conjecture that this is the reason why projection-based approaches often fail to produce
2084960	2094240	a good sample. But however, as I said, in the time we wrote MCG, there was a gap between
2094240	2102000	theory and practice because even with the MCG steps, we had a hard time trying to solve inverse
2102000	2108240	problems. So we had to resort to some projections within the denoising and the gradient steps.
2109120	2114800	But in DPS, we managed to achieve a general solver that only uses these gradient steps.
2119600	2126720	Now, for the Gaussian case, the only difference between MCG and DPS is that we do not use projections
2126720	2133840	for DPS. Leveraging the geometric viewpoint of diffusion models, we can think that DPS is
2133840	2141120	superior to MCG specifically because we avoid such projections. As projections here can potentially
2141120	2150160	throw samples off the data manifold. Hence, DPS will typically be more stable by avoiding such
2150160	2157200	throw-offs. And by the way, this is just an illustration. It doesn't really guarantee anything
2157200	2162720	that our solver will always stay on the noisy data manifolds.
2165520	2173440	Yeah, so that was about it for MCG. Any questions up until now?
2179440	2185600	I just have a question, but maybe a very simple question. At the beginning of this slide,
2185600	2194480	when you were explaining the, I think it's, no, the next one, two after this one.
2197040	2208080	But yeah, this one, the previous one. How do you define this k? So n, what is n and what is k?
2208080	2219440	I would have to look for the definition of n and k here.
2230320	2236080	From my memory, n is the dimension of the ambient space, in case the dimension of the
2236080	2243840	locally linear, low-dimensional dimensionality of the data manifold.
2245920	2253360	I guess you're assuming that this, so the brown, the mean is basically used, the mean of each
2253360	2260400	intermediate distribution. So it's the mean of the diffuse or denoise distribution.
2261360	2270880	And then this, I'm just wondering, shouldn't you start going from a big
2272560	2277440	standard deviation to a small standard deviation? Because basically when you do the denoising,
2277440	2284880	you start with a high variance and then you start denoising and the variance slowly decreases.
2285600	2290160	That's just something I was thinking that if that's not the case, so I'm missing out something.
2291120	2295840	Can you repeat the question again? I'm not sure if I understood it correctly.
2295840	2303760	Yeah, so the point of this is to interpret what's going on, like how the denoising samples
2304480	2313600	behave in the data manifold through the trajectory. So when you do the denoising,
2313600	2319200	so you start with the isotropic Gaussian, which is the highest entropy you can have. So you start
2319200	2326880	with the maximum variance that you have and then slowly you start removing the noise and by doing
2326880	2336640	that interactively the variance at each intermediate step goes down. So what I was expecting is to have
2337200	2342480	this behavior when you have the mean and then the variance is large at the beginning and then
2342480	2348720	starts decreasing. Because ideally you would reduce the variance, but I don't know if I'm missing
2348720	2356400	something there. Oh, but I think it's what you're saying is kind of different from what
2356400	2363600	this slide is saying. So I'm not really stating anything about denoising anything because the
2364640	2370400	flow here is that we're first assuming that there exists a central data manifold M here.
2371280	2380240	And then we're trying to visualize how the manifold of the noisy images would look like
2380240	2388560	if there exists the central data manifold M. And I'm claiming that these noisy data manifolds will
2388560	2396560	be covers of the central data manifold, where the distance between from the central data manifold
2396560	2406960	will be defined by this constant value. So it's really not about starting from high variance
2406960	2412800	Gaussian noise and reducing it to anything. We're starting from a clean image and then we're defining
2412800	2420160	the manifolds of these of these noisy images. Okay, so I will have to look at that for I guess also
2420160	2442640	more too. Yeah, thanks. Yeah. So I guess we just tell me if
2442800	2453520	if I'm
2453520	2455520	it's gonna get it's a good cliffhanger.
2458480	2459200	It's first again.
2459200	2467440	Um, do you hear me okay?
2468960	2476960	Yeah, yes. Yeah, so I think it's about 48 minutes. So I don't think we have to go over the final
2476960	2490000	section. So yeah. So I'll just try to we just we cannot see you or the slides anymore if you
2490000	2499760	can do the same thing as before to stop sharing and start again. Yeah. Well, the talk is basically
2500720	2507280	done. So okay, that was the last slide if we if we don't cover the last section.
2510240	2514160	I'm not sure if this is the correct way of saying thank you very much in in Dutch.
2515120	2516560	Maybe you should put it in Italian.
2522960	2526640	Yeah, so yeah, that is the right way to say thank you. Thank you too.
