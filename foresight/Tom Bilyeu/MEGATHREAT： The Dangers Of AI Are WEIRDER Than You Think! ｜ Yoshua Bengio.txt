I want to start with a quote from Ilya Setskiver.
He said, for people that don't know,
he's a co-founder of OpenAI.
He said, it may be that today's large neural networks
are slightly conscious.
So I want to pose that question to you.
Are computers becoming conscious right now?
I think it's a question that doesn't make much sense,
because we don't even have a clear,
scientific understanding of what conscious means.
So based on that, I would say, no.
There are lots of properties of our consciousness
that are missing.
What it means to be conscious, in other words,
what sort of computations are going on in our brain
when we become conscious of something,
and how that is related to notions, for example, of self,
or relations to others, how thoughts emerge,
and how they're related to each other,
all kinds of clues we have about consciousness,
including how it's implemented in neural circuits that
are completely missing in large language models.
All right, as we think about consciousness
from an evolutionary standpoint,
we think about its utility.
And for people that haven't heard consciousness
defined before, I think the easiest way to explain it
is it feels like something to be a human.
And so the question is, does it feel like something
to be a machine?
And the most important question, I think,
as we think about the dangers of AI and what's coming,
is does it matter?
Is it additional utility for it to feel like something
to be a human or to be a machine?
Do you agree that that's going to matter
in terms of goal orientation, in terms of, quote unquote,
wanting to do something?
As we think about our AI, is it going to take over?
Are we going to be dealing with killer robots?
Or am I totally off base with that?
My group put out a paper just in the last couple of months.
And we have proposed a theory that
is anchored in how brains compute.
So the theory has to do with the dynamical nature
of the brain.
In other words, you have 80 billion neurons,
and their activity is changing over time.
The trajectory that your brain goes through
as all these neurons change their activity
tends to converge towards some configuration
when you're becoming conscious.
That convergence has mathematical implications
that would suggest that what we store in our short-term memory
are these thoughts that are discreet but compositional.
In other words, like a short sentence.
And it's also something ineffable,
which means it's very hard to translate in words,
and there are good reasons for that.
It's just that it would take a huge number of words
to be able to translate the trajectory that's
state of your brain, which is a very, very high-dimensional
object, into words.
It's just impossible, essentially.
So even though we may communicate with language,
we may have a different interpretation of what this means.
And in particular, a different subjective experience
because of our life has been different.
So we've learned different ways of interpreting the world.
OK, if consciousness is a byproduct of the feeling
I get when my particular brain is honing in on a thought,
that there is a neural pattern that becomes recognizable,
the thing I think that becomes important,
and the reason that I think this is important
as we think about artificial intelligence
potentially becoming killer robots,
is my big thing with AI has always been,
AI has to want something.
It has to want an outcome.
Not necessarily.
Interesting.
Let me finish that sentence, and then we'll pick that apart.
But if I'm right, and AI has to want something,
and that's certainly how humans behave,
then I understand the utility of this ineffable feeling
that you're talking about, the we call consciousness.
Because for humans to make a decision
and know what direction to go in, we must have emotion.
If you selectively damage the region of the brain
that controls emotion, people cannot make decisions.
They can tell you all the rational reasons
why they should eat fish instead of beef
or beef instead of fish,
but they can't then actually decide and do it.
So we need that feeling
that where this thing is more desirable than that thing.
And so my thinking has always been, as it relates to AI,
that if AI doesn't want something,
it will never be from an emotional standpoint.
If it doesn't feel like anything to be a robot,
they will never have the final decision-making capability
to care enough to take over the world.
And so that's where it's like, if it becomes conscious
and it suddenly feels like something to be a robot,
then they're gonna be motivated in a direction.
That direction could be bad, it could be good, whatever.
But they're gonna be motivated in a direction now
if they are like humans.
But if they never become conscious
or it never feels like anything,
I would think they would be much like they are now
where it's like, well, it could be this, it could be that,
if you've ever talked to a chat GPT,
which of course you have,
but that feels like it would sort of be
a perpetual state of affairs.
What might I be getting wrong?
My belief is that you're talking about two things
that are actually quite separate as if they were one.
So wanting something, having goals,
and getting some kind of internal or external reward
for achieving those goals
is something that we already do in machine learning.
Reinforcement learning is all based on this.
And you don't need subjective experience for that.
So these are like really distinct capabilities.
Subjective experience is related to thoughts
that we discussed earlier.
We could have machines that have something like thoughts.
And potentially, if we implement it similarly
to how it is in our brain,
they might have subjective experience.
It doesn't mean that they need to have goals.
I think we can build machines
that have these capabilities.
In other words, they can help us solve problems
by telling us how, what is the problem?
What is the good scientific understanding
of what is going on?
And what might be better solutions?
But they're not trying to achieve anything except
be as truthful to the data
what they know, what they have observed.
What then is the disaster scenario
of something that can pass the touring test
that you're worried enough that you're saying,
we need to treat this the way
that we would treat anything else dangerous,
whether that's the environment,
whether that's, or sorry, climate change,
or whether that's nuclear weapons.
Like to put it on that level,
just at the touring test level,
give me the disaster scenario.
We already have trolls, right,
that are trying to influence people
on the internet, social media.
But they're humans,
and you can't scale the number of trolls very easily.
This would be too expensive,
and maybe people would not want to do it,
even if you paid them.
But you can scale AI with just more compute power.
So you could have AI trolls that,
I mean, I think there already exist AI trolls,
but they're stupid.
It's easy to interact with them a little bit,
and you see they're not human.
I mean, they're repetitive and so on.
And so now we get to the point
where you could have AI trolls
that essentially invade our social media,
invade, or even our email.
And in fact, you could do better than that.
It could be personalized.
So right now, it's a little bit difficult
for a human troll to have a good personal understanding
of every person that they hit on,
to know their history.
I mean, it would just take too much time for them
to study you and multiply by a billion people.
But an AI system that could just have access
to all of the interactions that you've had,
but in the videos where you spoke,
the texts that's available on the internet,
they could know you a lot better, right?
So how could that be used?
Well, it could be used to hit on the right buttons
for you to change your political opinion on something.
It could be used to even fool you into thinking you're
in a conversation with someone you know,
because they can know you and they can know your friend
and they can impersonate your friend,
at least at a text level.
So I don't think we have these things,
but just we're just like one small step away
from having these capabilities.
As I was thinking through the same problem,
I was thinking, here's a terrifying example.
Dear parents, AI is gonna reach out to you,
mimicking your child, asking for money.
And so it's not a Nigerian prince anymore, it's mom.
Something happened at school, whatever.
They talk in their language,
they reference things that you don't think
that they could have possibly put out there.
But of course, if the AI is good at image recognition
and it knows that you guys were on a beach seven years ago,
it could replicate things in the form of a memory
that you would never believe
that anybody else could possibly know,
but we leak, especially kids,
leak so much data out into social media
that to your point, the AI would be able
to have so much context.
So at my last company, we got socially engineered
and they convinced us to wire 50 grand.
And when we went back and looked at the emails
back and forth between our finance department and the COO,
it was so believable.
It wrote, like it was obviously a person,
but it was writing like they would write to each other
and I was really flabbergasted.
And so to think that a human could do that to your point,
it's very hard for them to get the amount of context
as it takes so much time,
but when AI is doing it and it can churn through everything
that those two people had ever said to each other,
ever online, that gets really scary, really fast.
Okay, so if we did this pause,
the letter that you guys wrote and we paused for six months
and we were gonna hold a convention in that time
and all governments were there, Yoshua,
and you're up on stage and your job isn't to tell us
what to do, but it's to open the conversation
in the right place.
Where would you open that conversation?
What do you want us focused on in turn?
I'm guessing it's like we need to limit this
or something along those lines.
Where do you begin?
I don't know for sure exactly
how these technologies could be used.
You and I can like make up things.
Maybe some are gonna be easier than we thought,
some are gonna be harder,
but there's so much uncertainty about how bad it can turn
that we need to be prudent.
So prudence here is something that we need to bring in
our decision-making individually
because we're gonna be facing potentially these attacks
as nations at the planet level.
Yeah, that would be my main message
that the technology has reached a point
where it can be very damaging
and there's too much unknown of how this can happen,
when it will happen.
And even the strongest expert,
even the people who built the latest systems
can't tell you.
It means that we have to get our act together
and most of these gonna come from governments.
So we need those people to get quickly educated
and we need to also have scholars, experts,
not just AI experts, but like social scientists,
legal scholars, psychologists,
because this is the psychology of how these could be used,
how to exploit people's weaknesses.
In order to do the work, the research,
also like what sort of precautions do we need?
So there are very simple things
that we can do very quickly.
For example, watermarks and content origin display.
So watermarks just means that one company,
say like OpenAI puts up their software,
they could easily put out another software
that anybody could run that can test
with 99.99% confidence,
whether a text came from their system or not.
So humans wouldn't see the difference,
but for a machine that has the right code,
it's very easy if their system is instrumented properly.
In other words, the kind of sneaky
and some bits of information that are not,
you can't notice, statistically, there is no difference,
but the chances of having this particular sequence of words
would be very, very unlikely and would go to zero quickly
as the length of the message increases.
So watermarks are easy to put in, technically speaking,
and they would say this text comes from this company,
this version, whatever.
So a piece of software running on your computer
would be able to say, oh, by the way,
the text that you gave me to read
is coming from this company, blah, blah, blah.
And then we need that information to be displayed,
because of course, being able to detect
that it's coming from an AI system is one thing,
but when you have a user interface,
it should also be mandatory.
Like if I'm on a social media in particular,
and I'm getting,
I'm interacting with some character out there online,
I need to know that that character is not a human.
And so that must be displayed.
If I get a picture or a video or text in an email,
I need my email software to tell me,
warning, this is coming from OpenAI GPT 5.6.
Okay, so I'm gonna push back with the obvious thing,
and I think I won't even have to play devil's advocate here.
I, maybe I'm not more pessimistic than you,
but I am in the toothpaste is out of the tube,
and there's no getting it back in.
So I, as a way to move all this forward,
lets you and I actually debate the reality of all this.
So I'm at the governmental meeting,
you start saying that my immediate reaction is,
Yoshua, China is going to develop this, if we don't,
if we put the brakes on this, they're not going to.
And this is a winner-take-all scenario.
We cannot allow ourselves to get behind, what say you?
It's a good, it's a good concern.
And that's why we have to get China
around the table as well, and Russia and all the countries
that may have the capability to do this.
But Russia right now feels hemmed into a corner.
They are, Putin is literally intimating
that he's gonna use nuclear weapons.
There's no universe, like we've already tried
financial sanctions, that's caused them to, you know,
start trading in non-dollar denominations.
They're grouping up with China, Brazil, South Africa,
they, India, they don't care.
Like they're going to use that to their advantage.
In fact, even bluffing would be a way smarter play for him
to say, no, no, no, we're gonna keep doing it,
even if he wasn't, even if they're like backwaters,
it would be wise of him to say, no.
In fact, if you don't, NATO, if you don't immediately
back off, we're going to unleash a troll farm,
the likes of which you've never seen,
we're gonna completely destroy democracy
in the Western world.
Yeah, so first of all,
we can protect ourselves without necessarily
hampering the research.
So I think people misunderstood the letter,
it never said stop the eye research.
It's mostly about these very large systems
that can be deployed in the public
and then used potentially in the various ways
that we have to be careful with.
So tiny, tiny sliver of the whole thing that we're doing.
Second, and second, in the short term,
we do have to protect the public in our societies
with things like trolls and cyber attacks
and that can exploit the eye.
Third, I don't know, I'm not a, no, either.
And I don't mind comfort zone here
in terms of diplomacy and, you know.
You and me both, but it's fun.
But my guess is that the authoritarian governments
are probably as scared of this technology
but for different reasons.
So why are they scared?
Because the same AI systems that could perturb our democracies
could also challenge their power.
In other words, imagine AI trolls, you know,
being able to defeat the protections of the Chinese firewall
and interacting with people and, you know,
putting democratic ideas in their heads in China.
Well, that would not be something
that this government probably would like to see.
And in fact, I think China has been
the fastest moving on regulation,
not for the same reasons as we are.
So they are afraid of this.
So I think they will come to the table,
but again, like it's not my specialty.
But at least we, there's a chance
that they might be willing to talk.
And remember the nuclear treaties were worked on
and signed right in the middle of the Cold War.
So long as each party recognizes
that they might have something worse to lose
by not entering those discussions,
I think there's a chance we can have a global coordination.
And we have to work, even if it's hard,
we have to work on it.
Yeah, I'm not so worried about the hard part as I am.
What is the natural reaction
when you have a very difficult, dangerous thing?
And history tells me that we don't come to the table
to sign the non-proliferation agreement
until we have proliferated so far
and we have so many missiles pointed at each other
that we finally go, okay, let's not let this go beyond anymore
and let's not let it go out to other countries.
Like we're perfectly fine
being in a stalemate with each other.
And I worry that a similar kind of reaction
will be had here.
But I take your point that this is not an area
where either of us are an expert,
as much as I find it utterly fascinating
to pursue that line of thought.
But I wanna now go back to what would we do
to actually begin to limit this stuff?
So we need to get people thinking,
hey, this is dangerous, that's clear.
But then the watermark thing to me works only for people
that agree that they're going to do it.
But is there a way, so taking the,
instead of trying to get people to not do things,
how do we build defensive things
that even when somebody's trying to hack the system,
so I doubt you know this about me,
but we're building a video game.
And so one of the things you have to think about
is this game, people will attempt to hack it.
Like that is just, it goes without saying.
So rather than me trying to ask everybody,
hey, please don't hack video games,
like literally, it's the dumbest thing ever
for the gamers to hack the games, it's stupid.
You end up ruining the fun, that game will die out,
and then people will try to invent a whole new game.
Far better for everybody to just let's all agree
that we're not going to hack it.
But human nature is what it is,
and that's never gonna work.
So what they do is they create an adversarial approach
where it's like, I find the best hackers in the world
to come in to try to hack this game,
and then I figure out what I would have to do
to defeat that.
So what would an adversarial setup look like in AI
when someone's trying not to watermark,
but I can still figure out who that came from,
or is there a signature or something like that
that we could identify?
You can reboot your life, your health,
even your career, anything you want.
All you need is discipline.
I can teach you the tactics that I learned
while growing a billion dollar business
that will allow you to see your goals through.
Whether you want better health, stronger relationships,
a more successful career, any of that is possible
with the mindset and business programs
in Impact Theory University.
Join the thousands of students
who have already accomplished amazing things.
Tap now for a free trial and get started today.
Watermarks are the easy thing,
and I agree they will only be done by the legit actors.
People have already been working on machine learning,
trained to detect text or images
that come from other machine on existing systems,
but these systems are not nearly as good.
But yes, this is already being developed,
and presumably there's going to be a lot more efforts
in that direction.
We need that as plan B, right?
But plan A is already to reduce the,
right now it's just too easy to,
you can have an API and just write on top of chat GPT.
So yeah, we should do all these things.
By the way, the kind of adversarial approach
that you're talking about is from what I hear and read
is also what OpenEI is being doing,
and companies like Google have been doing.
They hire people to try to break their system
as much as they can.
That's exactly what they're doing, like red teams.
And that's good, we need to continue doing that.
But maybe we need to make sure the guidelines
for doing that are shared across the board,
and people can, we ensure all companies
have that sort of retesting before it's released
to the public, for example.
Yeah, I want to add one thing about,
because you asked like, what can we do
in the short term at the beginning of your question?
So Canada has a bill that is going to pass
into law probably in the spring,
that maybe the first one around the world on AI,
and it has a nice feature,
which hopefully other countries will imitate,
which is that the law itself is fairly,
simple, it states a number of principles,
and then it leaves the details
of what exactly needs to be enforced through regulation.
And the reason this is good
is because it's much easier for governments
to change regulation to be changed like this.
You don't need to go back to the parliament.
And so you can have much more adaptive legislative system,
including the law and the regulation.
And that's going to be super important
because the nefarious users that we didn't think about
they're going to come up and we need to react quickly.
If we have to go back to parliament
and it's going to take two years,
now this is not going to work, right?
We need to have a system that's very adaptive
in terms of legislation.
Yeah, that is inevitable.
That brings me back to, we're in this situation
because I think people are surprised
at how rapidly AI is advancing.
How did we get caught off guard?
Someone like you has been in this for so long.
You knew the rate of change.
What happened?
Is it just, we just could not anticipate
as we scale the data up how fast the machine would learn
or is there, what is the X?
We were surprised that the machine did X quickly.
What was X?
Ask the terrain tests.
In other words, manipulate language well enough
that it can fool us.
The experience I had of...
Sorry, what I'm asking is what allowed it to do that
in a way that caught us off guard?
Well, that's interesting, right?
It didn't require any new science.
It's essentially scale that did it.
Do you think consciousness is a function of scale?
No, right?
No, I don't think so.
I mean, some people think so,
but there are theories around that.
I think scale is probably useful,
but that there are some very specific qualitative features
of how we become conscious that would work
even at smaller scales.
So yeah, scale is important simply because the job
that we're asking these computers to do
when they answer questions is competitionally very demanding.
And this comes from...
So I have these...
I have a blog post where I talk about the large language models
and some of their limitations.
The issue here is that if you take almost any problem
in computer science that you can write down,
probably like try to optimize this or that
or to find the answer to this and that question,
almost all of these questions,
the optimal solution is intractable,
meaning it would take an exponential amount of computation
compared with how big the question is.
And so the...
It's like, if you want the optimal neural net
that can answer questions about that they can reason properly
and so on is exponentially big,
which means we can't have it,
but the bigger our neural net, the better it approximates this.
So there's a sense in which bigger is better
because of that, even with problems that look simple.
So as an example, to illustrate what I mean,
consider the problem of playing the game of Go.
The rules of the game are fairly simple.
You can write a few lines of code that check the rules
and tell you how many points you get and so on.
The neural net that can play Go
and like really win,
like, in other words, go by the rules and exploit them
in order to figure out what is the optimal move and so on.
That neural net, the neural nets we have now
that play really better than humans, they are huge also.
It's just a property of many computer science problems
that are like that.
The knowledge needed to describe the problem maybe
even when the knowledge is small,
the size of the machine that's necessary to answer questions,
take decisions that are optimal, is very big.
So I think that's the reason why we need big neural nets.
That's why we have a big brain,
even if the amount of knowledge that's involved is small.
Now, in addition, the amount of knowledge that's necessary
to understand the world around us is also big.
But I think the biggest part of what our brain does
is inference, this is the technical term to mean,
given knowledge, how do you answer questions properly,
like how to optimize or take decisions
that are good given that knowledge.
Okay, is inference the ability to apply a pattern
that I saw in the past to a new novel problem?
That's, yes, that's part of inference.
In classical AI, things were very clear
between knowledge and inference.
So knowledge was people having typed
a bunch of rules and facts.
And so the knowledge was not large, it was handcrafted.
And inference was, well, you have some search procedure
that looks how to combine these pieces of knowledge,
these facts and rules in order to answer a question.
And we know that's NP-hard, that's like exponentially hard.
And so we use approximations, it's never perfect and so on.
But people didn't use neural nets in those days,
they use like classical computer science algorithms
that try to approximate this, like A star.
Now we have neural nets.
And neural nets can do this approximated,
it can be trained to do a really good job
at searching for good answers to questions
given that piece of knowledge.
How does it define good is,
I always assumed that what AI was doing
was trying to guess effectively
the next letter or the next word.
So based on all the patterns that it had seen.
So it's like, I've seen questions like this before
and here are the answers that have been rewarded
in that a human has told me that it likes this answer
better than this answer.
And that the pattern recognition of the machine
combined with the human ranking those responses
from the machine gives us the way that the AI approaches
that question to this answer.
Am I missing something?
Yeah, I think, I mean, what you're saying makes sense,
but there's also a lot of knowledge we have
that can be distilled.
For example, through how we do it for education,
we do it through books and psychopedia.
So it's not all the knowledge we have,
but you can see that.
So let me try to put it in this way.
Wikipedia is way smaller than your brain.
Smaller than my brain.
Yeah, smaller is a number of bits
that are needed to encode it.
Whereas a number of bits that are needed to encode
all the synaptic weights in your brain.
Got it, yep, yep.
Huge orders of magnitude greater.
So if we were just talking about these kinds of knowledge,
which is not everything, obviously,
like physical intuitions and so on,
is another kind that we can't put in Wikipedia.
But if we just talk about that kind of knowledge,
you would want a very big brain,
just to be able to answer questions
that are consistent with that knowledge.
That's what I meant.
And right now, that's not the way we train
our large language models, by the way.
The way we train them is,
we look at texts that presumably is more or less consistent
without because that's not even the case.
There is like, people are not truthful
and they see all kinds of things, but even if it were,
and then by imitating that text,
like predicting the next word and so on,
we implicitly encapsulate then the line knowledge,
let's say is Wikipedia.
But yeah, so again, the argument is scale is important
because many problems require doing computation
that is intractable,
if you want to really get the right answer.
And so we need these really large neural nets
to do a good job of approximating how to compute the answer.
Okay, so now I'm gonna have to get
into the nitty-gritty a little bit.
This will be really one-on-one for you,
but it certainly will be instructive for me
and hopefully many others.
To say that a neural network is large, what do we mean?
Are we just daisy-chaining GPUs, CPUs, are they?
So when I think about the brain,
the brain is broken into these hyper-specialized regions.
So for instance, vision is comprised
of this part of vision tracks motion.
And I can selectively damage the motion center of your brain
and now you see everything in a snapshot.
There's things to deal with corners.
And so you can selectively damage the part of your brain
that detects corners.
There's things that detect straight lines, curved lines.
It's all these like hyper-specific little bits and pieces.
And I don't, my understanding of a neural network
is it isn't that hyper-specialized.
It's a lot of the same thing
over and over and over and over and over and over.
Help me understand what it means
to be a large neural network.
Okay, so you write that the brain
seems to have very specialized and modular structure
as in different parts of cortex, especially
when we look at what neurons do in different parts,
we see that they're rather specialized.
It's not perfectly easy to identify what this neuron does,
but we get a sense of what it's about.
And it's also true of our large neural nets,
but to a lesser extent.
So people have been trying to give a name
to what each particular unit in a large neural net is doing.
And we can do that by checking when does it turn on
what kind of input was present.
So if we look a lot of the things
that make this particular unit on,
and we ask humans, what's the category
that this belongs to, then we are often able to give a name.
And at least that has been done a lot
for image processing neural nets,
because it's easy sometimes you can say,
well, it's this part of the image and this kind of object.
For text, I know there's some papers doing that.
Now, I do think that our brain is more modular,
with more specialization than what we're currently seeing.
By the way, cortex is a uniform architecture.
The part of your brain that is cortex,
which is thought to be the practice
more modern in evolution and really essential
for like advanced capabilities,
is all the same texture,
it's all the same kind of units repeated all over the place.
And depending on your experience
or the kinds of brain accidents that you may have,
a different part of cortex will latch on a different job.
So these are more or less replaceable pieces of hardware,
like our neural nets.
There are other pieces in the brain that are not cortex
that seem to be much more specialized,
like hippocampus and hippodalinus and so on.
I'm at the edges now.
That was certainly useful information,
but I wanna push a little bit farther.
So what I'm trying to wrap my head around
is I have a vague understanding
of how the brain works, very specialized.
I do not understand how we scale a neural network,
unless you're saying that each,
okay, let me, I was gonna say each node,
and then I realized to me a node is either a GPU or a CPU,
but I actually don't know if that's true.
So first is I would need to understand
what is a node inside of a neural net,
and then how are the different parts
of the neural net program to do a specialized thing?
We'll start there.
Okay, okay.
All right, I'm gonna start with the end.
They're not programmed to do a specialized thing.
That emerges through learning.
Whoa, whoa, whoa, whoa.
That's true of the brain and that's true of neural nets.
You don't tell this part of the neural net,
you'd be responsible for vision,
and this part you'll be responsible for language.
But that happens?
Yes, you get specialization that happens.
Whoa.
Because they collaborate to solve the problem.
They're different pieces.
That's how learning does.
Like even like a simple neural net from 1990 does that.
How complex is that underlying code?
Is that really basic, but somehow has these
incredibly complex emergent properties,
or is that incredibly sophisticated?
The first, it's very simple.
What the complexity emerges because you have
all of these degrees of freedom,
and you have a powerful way to train
each of these degrees of freedom, these synaptic weights,
so that collectively they optimize what you want,
which is like predicting the piece of text
that comes next properly.
But let me go back to the hardware question.
The hardware we use currently to train our
artificial neural nets is very different from the brain.
They're very, very, very different.
We don't know how to build hardware
that would be as efficient as the brain
in terms of energy and all compute
that we can squeeze into a few watts, right?
And we wish we would.
So lots of people are trying to figure out
how to build circuits that would be
as efficient computationally as the brain.
Another difference is that the brain has
highly decentralized at the level of neurons
that we got like 80 billions of them,
decentralized memory and computation.
The traditional CPU has memory completely separated
from compute, and you have bus that transfers
information from one to the other
to do the computation in the little CPU.
That's very different from how the brain is organized,
where every neuron has a bit of memory
and a bit of compute.
Now, people doing hardware have been working
to build chips that would have something
that's more decentralized and more like the brain.
And there are several companies doing these sort of things.
They haven't yet reached a point where it can be a GPU.
So a GPU is a kind of hybrid thing
where it's really the same CPU pattern,
but instead of having one CPU, you've got 5,000.
And they each have their little memory,
but there's also some shared memory.
And it was designed initially for graphics,
a bit of graphics, but it turned out that
for many of the kinds of neural nets that we wanted to do,
it was a pretty good computational architecture,
but it has its own limitation.
It's energy-wise, it's like a huge waste
compared to the brain, as I said earlier.
And a large part of that waste is
because you have all that traffic still between memory,
places that contain memory and places that do compute.
So it's much more parallel than the good old CPU,
but much less parallel than the brain.
You're so deep in this, it probably doesn't freak you out
as much as it freaks me out.
But this is like as I really start to try
to wrap my head around what is happening,
this feels deeply mysterious.
Now, I've heard people say
that one of the things is freaking them out,
and this is people deep, deep, deep in AI.
One of the things that they find unnerving
is that they don't understand
what the neural network is doing.
They don't understand how it came up with a given answer.
Is, how is that possible?
It's just a dynamical property of systems that learn.
And that learn not like a set of simple recipes,
like you would learn how to do a recipe in your kitchen,
but learn something very complicated
that cannot be reduced to a few formulas,
like how to walk or how to speak
or how to translate
or how to go from speech acoustics to sequence of words.
These tasks cannot be easily
done by traditional programming,
but if you put a machine that has,
that can like approximate any function
to some degree of precision, so a big neural net,
and you tweak each of the parameters of that machine
billions of times, it can learn to do what you want.
It can change its, but then you don't really understand
how it does it.
You understand why it, you understand the code
that specifies how this machine computes,
but the actual computation it does
depends on what it has learned,
which is based on lots and lots of experience.
So maybe a good analogy is like our own intuition.
These machines are like intuition machines.
So what I mean is this, you know how to act
in different contexts, like for example,
how to climb stairs, but you can't explain it to a machine.
You can't write a program.
People have tried, roboticists have tried.
You can't write a program that does that.
One reason is, it's all happening in the unconscious, right?
But there's a more, the reason it's all happening
in the unconscious, it's just too big.
It's a very, very complicated program
that's running in your brain.
And the only way that you can acquire that skill
that's reasonable is by trial and error and practice,
and maybe some evolutionary pressure
that initializes your weights close
to something that's needed to learn to walk.
So things that we do intuitively
that need a lot of practice are exactly
like what those machines are learning.
They can't explain it.
We can't explain our own intuition.
We just know this is how we should do it.
And it's knowledge that's so complex
that we can't put it in form.
We cannot put it in a few formalized or a few sentences.
It's just, that's a nature of things
that there are very complicated things
that can't be easily put into verbalizable form,
but they can still be discovered, acquired,
through learning, through practice,
through repetition of doing the exercise again and again.
I have a grandson who's been learning
to walk him the last few months.
He was stumbling a lot and going again and again and again,
and after a few months, now he's pretty good.
He's not like us yet, but it's months and months of practice
and getting better gradually through lots
and lots of practice.
That's how we train those neural nets,
and that's why we can't explain
why they give this particular answer.
They're just like, well, I know this is the answer,
but I can't explain to you because it's too complicated.
I have like 500 billion weights
that really are the explanation.
Do you want those 500 billion weights?
What are you gonna do with that?
Okay, let's start teasing this apart.
So one of the more interesting things
in what you just said is going to highlight
the difference between what humans do
and what machines do and why until there is a breakthrough.
And I always love saying this stuff in front of experts.
You can strike me down if you think I'm crazy,
but I think one of the reasons
that a breakthrough is gonna be required
and that we're not just gonna be able to scale our way
to artificial general intelligence,
and I've completely heard you,
that AI passing a Turing test opens up a Pandora's box
that is utterly terrifying in terms of its ability
to dysregulate the human's ability
to function well as a hive herd.
But now the reason I think there's gonna need
to be a breakthrough is that the reason
that your grandson is able to get better over time
isn't just the calculus of balance.
It's that by doing it, he's building stabilizing muscles.
And so his muscles are getting stronger in areas
that they didn't need to be strong in when he was crawling.
So you get this biological feedback loop of,
oh, I see what I'm gonna have to do.
Part of the repetition isn't just locking it into my brain,
part of the repetition is that I'm gonna need
to develop the muscle fibers and the strength.
Now how much of that is mediated by the brain
and a part of the brain that's subconscious
is a huge question and certainly gets to the complexity
in your 50 billion parameters and all that.
The other part is that his brain is reconfiguring
neuronal connections and it's making some of those connections
more efficient through a process called myelination.
So it's wrapping the fatty tissue
to sheath different connections
just like an electrician would do.
And now it's got this incredible biological feedback loop
of I have a desire, I'm goal oriented.
I want to do this thing, this thing is walk.
Now how the interplay of I want to walk
because I see my parents walk, I see grandpa walking,
I want to do that thing or I have something in me
tells me being over there is better than being here.
And so I actually want to locomote to get there
and I would figure this out
even if I never saw anybody move,
which is probably more likely
given the baby start crawling
and they don't see people crawl.
They just have a desire to locomote somewhere.
Again, going back to my initial thing about
I think machines are going to need to have desire.
They have a reason that they want to cross the road
if we want to get to human level intelligence.
But let's just let me not fractal too much here.
So okay, we have this biological feedback loop.
You're not going to get that with a neural network.
No matter how much you scale it up,
it doesn't have a biological,
it doesn't have the ability to change itself yet.
Now maybe it will and maybe it could architect a new chip
or something once it has the ability
to manipulate 3D printers or what have you.
But for now it's stuck with a physical configuration
of chips unlike a human,
which can morph from muscles to brain matter.
It's stuck with a configuration,
but and this feels like the very interesting thing
that we've gotten right so far,
which is I have figured out the pieces that I need.
So whether that's GPUs or the code or both,
but I figured out the pieces that I need
for that configuration to learn in a very emergent way.
So I set up the pieces and then I give it a thing
I want it to learn and a quote unquote reward for doing so.
And then a massive amount of emergent behavior
comes out of that.
But it's always gonna be limited
in a way that human intelligence is not
because of the biological feedback loop.
Okay, now that I've set that stage,
do you agree that machines will need something
that imitates that biological feedback loop?
Meaning I need efficiency here
that I did not have a moment ago
for me to continue to get good at this thing.
And that without that,
we're sort of stuck at the highly potentially destructive
ability to manipulate language and images, but that's it.
So actually, current neural nets already do what you say.
I mean, they don't have the biological framework,
but they do learn from practice and mistakes.
But can they reconfigure their architecture
to get better at it?
You don't need to change the chips,
you just need to change the content of the memory
in those chips that contains, that says...
So why is the biological loop different?
Why is it different?
It's different because it has been designed by evolution,
whereas we are designing these things using our beans.
But fundamentally, let me step back here a little bit.
To state something important as a kind of starting point.
Bodies are machines.
They are biological machines.
Cells are machines, they are biological machines.
We don't fully understand them.
We know it's full of feedback loops, we know a lot.
I mean, we know a lot of biology,
but we don't understand the full thing.
But we know it's just matter interacting
and exchanging information.
So, yeah, it's just a different kind of machine.
Now, the question...
Some people think that, in particular,
when people were discussing consciousness,
because consciousness looks mysterious,
some people think that, well,
it's got to be something that's based on biology.
Otherwise, how could it ever be in machines?
Well, I disagree completely with that,
because it's just information processing.
Now, the kind of information processing going on
in our bodies and our brains and so on,
may have some particular attributes
that we still don't have in our current machines.
But the specific hardware just needs to have enough power.
So, you know, one of the great starting points
of computer science by people like Turing and von Neumann
in the early days of computing
is the realization with, for example, the Turing machine
that you can decouple the hardware from the software
that the same outward-facing behavior
can be achieved by just changing the software part.
So long as the hardware is sufficiently complex.
And Turing showed that you need very, very simple hardware,
and then you can do any computation.
That's like computer science 101.
So, that would suggest that there is no reason
why we couldn't, in the future, build machines
that have the same capabilities as we do.
Now, we are still, the current systems
are missing a bunch of things.
You talked, you know, we talked about walking,
and why is it that we don't have robots that can walk?
I mean, that can walk as well as humans.
Have you seen Boston Dynamics?
That stuff's freakish.
It can parkour.
They're not as good as humans by, you know, big gap.
But yeah, I've seen that.
But I think the issue is simply that we have
tons more data available to train language models
than we have for training robots.
It's hard to create the training data for a robot
because it's in the physical world.
You can't just replicate a million robots,
but eventually people will do it.
Or be able to do a good enough job with simulation.
There's a lot of work going in that direction.
But yeah, so I kind of disagree with your conclusion.
So go back to the reason that we don't have robots
that can walk is because it's just not,
it's not able to use some sort of model to see enough.
Okay, but there's, you're saying the point of that
is there's nothing fundamentally missing
from the architecture that the AI is running on.
It's just a modeling problem.
Yes, the software part we're still far off.
For example, you know, one of the clues I mentioned earlier
is that the amount of training data
that a large language model needs like, you know, GPTX
compared to what a human needs in terms of amount of text
to kind of understand language is hugely different.
So that tells me we are missing something important.
But I don't think it's because we're missing something
in the low level hardware of biology.
Although, you know, I'm a big fan of listening to biology
and understanding what brains are doing and so on
so they can serve as inspiration.
But I don't think it's a hardware problem.
Now hardware is important for efficiency.
So current GPUs are not efficient compared to our brains.
But it doesn't mean that in the next few years
we will not be able to build a specialized hardware
that will be a thousand times more efficient than current ones.
And now there's a much bigger incentive for companies
to actually invest in this because these AI systems
are going to be more and more everywhere
and it's going to become much more profitable
to do these investments.
Yeah, and proliferation of AI is crazy.
Before we derail on that though, I want to ask you,
so we're comparing the way that machines are evolving,
the way the AI is evolving to human evolution.
I've always thought of evolution as to use Richard Dawkins
quote, the blind watchmaker.
It's not trying to make a watch,
but the watch emerges out of what we could probably refer to
as a few simple lines of code.
It's like replication and the way that it replicates
plus a desire to survive on a long enough time scale.
There's not even a need for a desire to survive.
It's simply the selection of those who survive.
Yeah, interesting.
Is that an important distinction?
Because I worry, actually I don't worry,
this would then maybe what you're trying to get me to understand
about why machines don't need a desire.
There needs to be a selection criteria
for the one that does the thing better
and that will be enough to boom,
to have the exponential movement.
And that's the way we train those systems.
So the way we train them is that if you want,
we throw away all the configurations of parameters
that don't work and we focus more and more
on ones that do.
That's how trading proceeds.
It changes things in small steps, just like evolution does.
Evolution does it in parallel with billions of individuals
searching the space of genetic configurations
that can be useful,
whereas we're doing it the learning way.
So we have one individual big neural net
and we're making one small change at a time.
But it's both our processes of search
in a very high-dimensional space of computations.
Okay, so let me...
This was something that I heard you say in an interview at one point.
I wasn't sure if I was going to ask it,
but it's now, as you were saying that,
I realize that the entire universe
is born of a simple set of physical laws,
for lack of a better word.
And everything that we see from...
Because I was trying to think,
what is the origin of evolution?
Because you said that you don't need it to desire.
It just needs to get selected.
And then I was like, well, what's selecting it?
The laws of physics.
Just dictate that certain things will continue
to hold their form and function
and others will disintegrate.
Okay, so then everything is born out of these laws of physics,
which we don't fully understand yet.
But do you think there will be similar laws of intelligence
that we realize, oh, here are the very simple subset
and all of the struggle that we have right now
is because much like we don't yet fully understand
the laws of physics,
but yet we can still build a nuclear bomb,
nuclear power, GPS, all of that.
We know enough to do amazing things,
but we don't know everything.
Do you think we have the same thing happening in intelligence?
That's what drove me into the field.
The hope that there may be some principles
that we can understand as humans.
We verbally write about them, explain them to each other,
and so on.
Maybe write math that formalizes them
that are sufficient to explain our intelligence.
Now, obviously for this to work,
it has to be that it explains how we learn,
because the content of what we learn,
the knowledge that has been acquired by evolution
and then by our individual life,
is too big to be put in a few lines of math.
So whether this is true or not,
obviously we don't know,
but everything we have seen with the progress of neural nets
in the last few decades suggests that yes,
because if you look inside these systems,
like what are the mathematical principles
behind those large language models?
Very few.
It's something you can describe,
and you can explain, you know, when I teach,
we explain these to students and so on.
It's not that complicated.
It's just like physics is not that complicated.
What is complicated is the consequence.
So I think there's a good analogy here
to also understand the story about intuition
and very complicated things
that are difficult to put in formula.
The laws of physics are very simple.
You can write them down.
But what's complicated is,
well, if you put a huge number of atoms together,
that obey these laws,
and you get something very complicated,
like an ice storm,
it's very difficult to predict
because we don't have the computational power
to emulate that.
It's out of very simple things,
like simple laws of physics,
you get something extremely complicated
that comes out, that emerges.
And it is similar with nearness.
A few simple lines of code,
a few simple mathematical equations,
plus, you know, basically that at scale
and with enough data in this case,
and you get something that emerges
that's very powerful and very complicated
and not easy to reduce to those initial principles.
Okay, so now I want to bring back in
the idea of alignment of desire.
So if physics runs off the back of a set of simple rules
that does not need to want any outcome,
but humans manifest desire,
and so we rapidly become the most complicated thing
that we know of,
is there, do you think about the problem of alignment?
Are AI researchers trying to give
the intelligence a level of desire
because that would make it more profound,
or is that, am I just barking up the wrong tree?
I keep coming back to
AI without desire, mildly potent,
AI with desire,
dangerous beyond all measure and reason.
Yes and no.
So, yes, with desires
and a lot of, and the right, you know, computational
and the right algorithms
could be very potent and very dangerous
and potentially very difficult to align to our needs,
our values and so on.
And lots of people are working on this,
like how do we design the algorithms
so that even though we give goals to the machines,
they will not end up doing things that are against
what we want.
So that's the alignment problem.
But where I disagree with you
is that I think we're going to have AI systems
that have no goals, no wants,
but are just trained to do good inference,
to learn as well as possible about the world
from the data they have
and to recapitulate to us
what are good answers to the questions we're asking.
So let me explain why this would be very useful.
In science, typically we do experiments
and then we try to make sense of that data.
We come up with theories
and there could be multiple theories
that are consistent with the data
and so different people may have different opinions on them
or they recognize that all of these theories are possible
and at this point we can't disambiguate between those theories.
Then what they do is based on the fact
that we have these competing theories,
they will design another batch of experiments
to try to eliminate some of those theories.
And then the cycle goes back
because more experiments, more data, more analysis,
more theories, and eventually we hopefully zoom in
on fewer and fewer theories.
So this is the experimental process of science.
We come up with an understanding of the world,
but it's not one understanding, there's always some ambiguity.
In some cases we are very sure,
but yeah, a scientist who's honest is never, never sure,
except maybe for math, right?
So why am I telling you all this?
Because that whole process is at the heart
of all the progress we've seen in humanity,
which would be needed to cure disease,
to fight climate change,
even to understand how society works
and people interact with each other better.
So all of the things that scientists do
to make sense of the world
and come up with proposals of things we could do
to achieve goals.
All of that process can be done by very powerful AI systems
that don't have any goals.
Their only job is to make sense of the data,
represent all the theories that are compatible with it,
and suggest the best choice of experiments we should do next
in order to get the answers to the questions we want.
And that can all be done without any wants,
just by obeying some laws of probability
that we know are known,
and we just need the computational scale to implement that
and algorithms that people will discover,
but I think we only have the basis for that.
So what I'm trying to say is we could have machines
that are extremely powerful, more powerful
even than a human brain.
We have scientists doing that job right now,
but I'm looking, for example, in biology
because of the progress of biotech,
we are now generating data sets
that no human brain can visualize, can absorb.
We have robots that do experiments, again, in biotechnology,
where the number of experiments is in the millions.
A human cannot specify a million different things to try by hand.
A machine can.
A machine with the right codes,
and that machine doesn't need to have any wants.
It just needs to do Bayesian inference,
if you want the technical term,
it just needs to do Bayesian inference.
So, yeah, bottom line is we can have hugely useful machines
that are incredibly smart, but have no wants whatsoever.
Okay, so it's becoming clear to me now
what our base assumptions are.
So your base assumption is that, I think,
AI already does all the amazing things you want it to do,
is as dangerous as you could hope it to be
as a tool for humans to use.
And the thing that I'm focused on is,
in your scenario, I can just tell it to stop and it will stop.
The paperclip problem, in my estimation, isn't a real problem.
If I can just tell it, stop, stop making paperclips,
and then it shuts down.
Where it becomes a problem is when it's like,
no, I want to make paperclips,
and I'm going to keep making paperclips,
and there's nothing you can do to stop me,
and I'm going to go around you this way and that way.
And I'm not nearly as concerned.
Humans have so many weapons at their disposal.
I already know what the world looks like
when people have just unbelievably powerful weapons
at their disposal.
It's manageable.
But when the weapon gets to be a million times smarter than I am
and decide what it wants to aim at
and decide when it wants to go off
and nobody gets to tell it otherwise,
that's a world that freaks me out.
And so, when you think about the alignment problem,
do you think it's a problem?
Like, because in your world where the AI doesn't have
its own wants and desires coming from an emotional place
where one thing feels better than the other,
and so it has the same type of human desire
to go in a given direction that we have
and we know what that's like.
People will kill for their fucking kids, man.
They will do crazy things when the thing feels good enough.
So, in your world, can't we just tell it to stop?
Okay, so there are two kinds of machines we could build
with the current state of our technology today.
This is a choice.
One kind of machine is more like us
and has wants and goals
and it could decide to do something we did not anticipate.
And that could be very dangerous.
And people are trying to see how we could program them
in a way that would be safer.
That's the AI alignment problem.
But we have a choice.
We don't have to go that route.
We could build machines that are not like us.
We don't try to make them like humans.
We don't give them feelings.
We don't give them wants.
See, the thing is, once we understand those principles
of intelligence, we can choose how we fly them.
If we're wise, we're going to choose a safe path.
It doesn't do anything.
It doesn't want anything.
It's just, its training objective is truth.
Okay, so might I suggest,
when we've gathered all the nations together,
you're about to go on stage.
What I'm going to try to then get you to convince people is
that that becomes the most important thing.
Do not give AI desire.
Period.
Like inference only, truth only.
That's it.
That's it.
That's it.
Actually, I think that's the safest route.
The problem now, the problem is we need to have all these people
around the table and to agree.
And honestly, I'm not sure it's going to work.
There might be some crazy guy somewhere who says yes,
but then goes a different route because he wants to have fun
with those machines that look like humans.
Andy's a moron and doesn't realize how dangerous it is?
People are crazy.
People have emotions.
People are unconscious of the consequences.
They think, oh, it's going to be fine,
but I'm going to make a lot more money than the other guy
because I'm going to use this thing that is more like humans.
There is going to be a temptation to build systems that are like us.
Would it be more powerful if it was more like us?
I don't know if they would be, well,
they would be more powerful in the sense of being able to act in the world,
but that's also more dangerous.
Act in the world based on their goals.
That's the place which is a slippery slope.
Or maybe we can make progress,
but even if we make progress with AI and techniques,
we're trying to design rules and algorithms such that
even if they have goals, it's going to be safe.
But even that is not a sufficient protection
because somebody could just decide to not use those guidelines.
So having algorithms that make AI alignment work is not enough.
We have a social problem.
We have a problem of collective wisdom.
How do we change our society
so that we avoid somebody doing a catastrophic thing
with a very powerful tool that can potentially destroy us all?
It's not something for tomorrow.
It's not going to happen tomorrow.
It's not going to happen next year.
It's not going to happen in five years.
But we are on that path
and it's going to take a lot of time for society to adapt
and probably reinvent itself deeply
for us to find a way to be happy and safe.
When was the last time we had to reinvent ourselves like that?
We reinvented ourselves many times over,
but not like that.
Of course, this challenge is completely new.
But we did.
So think about major cultural changes
that have happened in the history of humankind.
Think about like religions,
the invention of nation-state,
the invention of central banks and money.
I'm always quoting Harari here.
So we've created all kinds of fictions, as he calls them,
that drive our society and people
in ways that kind of work,
but are not adequate for the next challenge.
By the way, dealing with this challenge
also helps us deal with things like climate change
and nuclear dangers and so on.
Because it's all about how do we coordinate
the billions of people on Earth
so that we all behave in a way that's not dangerous to the rest.
I don't know how to do that,
but we need our best minds to start thinking about it.
You're really starting to pull together
some very interesting threads here.
You've all know a Harari's idea of a collective fiction.
I've heard other people refer to it as a useful fiction.
That's very interesting.
Now, my concern is that that works when people don't understand.
So I'll go to the most recent one, central banking.
So people don't understand it.
And you've got the whole idea of what's it called,
the beast from Jekyll Island or something from Jekyll Island.
Where they go, and to your point, it was very much a decision.
A cabal of people went and decided we're going to do it like this
and we're going to present it to the world like that.
And they did it.
And hey, it just quote unquote works.
There's very few things, though, more unnerving
than peeling back that and realizing what it actually is.
And so I wonder how we present a useful fiction to the world
about AI that will get us all unified in a way that will be useful
but isn't manipulative.
I think that's the essence of what democracy should be.
That we rationally accept the collective rules
for our individual and collective well-being.
So that actually has worked quite well in many countries.
But we need to go like one step further in that direction.
It can absolutely be truthful and not manipulative.
So long as principles of justice and fairness
and equity and so on are respected, people will go with that.
But here, I think we need to go even beyond the kind of democratic system.
So in a democratic system, if it works well,
we don't need to lie to people to get them to accept to go in a particular direction,
to vote for a referendum or something for a particular decision.
They should be, in fact, as conscious and well-understanding
of the decisions that they're actively taking.
Yeah, getting everybody on the same page, that is the tricky part.
That's why when you first said it in the context of religion,
it immediately felt like, ooh, if we could pull that off,
if we had a collective narrative about what this meant, it might work.
It's not my preferred way of solving the problem, obviously.
I'd much rather go with an Uber democracy that really goes to these principles even further.
Yeah, that's where I think I begin to, I get it, regulation works.
And on the countries that come up to the table, regulation is amazing.
We should regulate this.
I think people, we have to.
You have to do something to your point just because it's hard
doesn't mean you should stand still.
But at the same time, that's the one where I'm like, yeah,
well, all the countries that regulate it does not account for the person.
Like you were talking about that's like, oh, I'm going to go build this thing.
They don't recognize second, third order consequences or more terrifyingly,
they do recognize the second and third order consequences and they do it anyway.
Or even like, because that gets into the crazy man hypothesis.
But having read about Robert Oppenheimer when they were building the bomb
and how you just become convinced that, look, the Nazis are building a bomb.
We need to do it.
We have to do it faster.
We'll sort of worry about the bigger problems later down the road.
And I very much worry that that's where we are with AI.
Okay.
I'm going to set that aside for a second because it's terrifying.
And I, so as I said at the beginning, I worry too.
Yeah.
I rightly so how we solve the problem is a completely different thing.
Let me, let me ask you, do you think as AI continues to come on board
and let's say that we were thoughtful about it, we've got good regulation in place.
Will it be like dealing with a hyper intelligent human?
Or will it feel completely alien to us?
It depends how we choose to design it.
So if we build systems that have wants that have a personality that,
that, that have emotions, we could because the more we understand these things from humans,
the more we'll be able to do it.
Personally, I don't think that's the wise choice.
And so if we go the other route of systems that are useful to us,
but not necessarily acting like humans, I think it will be much easier,
cognitively, because we won't be expecting those things to interact with us like,
like humans do.
They, they will be just assistants basically that help us sort out problems and find solutions.
Yeah.
The alien idea.
I, as you were answering that question, I had a wave of, I don't,
I don't see how we're going to loneliness unto itself is going to lead people to play with making it emotional.
Even, even as I think about the way that we want to use AI in our, in my company,
it's to generate very realistic characters in a video game.
And I can just see that to make them more and more realistic, you're going to want them,
you can mimic emotion for sure.
And if we pass regulations, that's probably where we stop is you create things that mimic emotion,
but don't actually have them.
But to create something that is, that is more realistic, we will.
So I want to go back to go for a second.
So in go, they said that the, it was like playing an alien.
It just, it made moves that were so different.
So given that now already, you have people saying that it comes at something so counterintuitive that it feels completely foreign.
You don't think that even without wants and desires that it's going to feel just just completely different.
The reason that it, it looks foreign to current players might be the same as the current we are playing go at the master level.
I might look foreign to somebody a hundred years ago, because we've made progress in our understanding of how to play well.
And, and the strategies we use now may look very surprising to somebody a hundred years ago.
So it's just that these machines have now trained on someone so many games that they're like, you know, a hundred years into the future,
if you want, in the evolution of go, if we had let things go.
So they discovered, basically it's like, like science, right?
Looks like magic until you understand it.
So if, if, if, if somebody from a hundred years ago comes today and looks at our cell phones, it's going to look like magic.
It's going to look very unintuitive. What? How could that possibly be, right?
And we are just used to that.
And so I, yeah, I don't think it's because there's something fundamentally different.
I mean, there are financial differences, but, but that is just being systems that are more competent because they've been trained on more data and train longer and focusing on this particular problem in case of go.
Evolution is one of the big themes that has come out today.
If people want to keep up, Joshua, with you and the ever-evolving science that is AI, where should they go?
Well, I have a website. It's easy to find and a blog where I write some of my ideas.
And of course, I also write a lot of scientific papers.
My group does.
The Institute that I founded with all of the universities here in Montreal has about a thousand AI researchers working on many of these problems, but also thinking hard about the responsible AI aspect and these questions.
And there are many people around the world who are thinking hard about this and as it should.
The truth is, hitting your career goals is not easy.
You have to be willing to go the extra mile to stand out and do hard things better than anybody else.
But there are 10 steps I want to take you through that will 100x your efficiency so you can crush your goals and get back more time into your day.
You'll not only get control of your time, you'll learn how to use that momentum to take on your next big goal.
To help you do this, I've created a list of the 10 most impactful things that any high achiever needs to dominate.
And you can download it for free by clicking the link in today's description.
All right, my friend, back to today's episode.
This is literally a direct quote from the book towards the end.
But, and I quote, we are headed for collapse.
Civilization is becoming incoherent around us.
I'd love to know what you guys mean by that.
And if that to you is a big part of the thread through the book because it was for me.
Well, the first thing to say is that you skipped the warning in the front of the book that it should only be read while sitting down so fall over and injure themselves.
Yeah, well, we are headed for collapse. That's really not even an extraordinary claim if you just simply extrapolate out from where we are we are outstripping the planet's capacity to house us and we don't appear to have a plan for shifting gears so it's really a factual statement.
Now the question really is why, and the, the bitter pill is that the very thing that made us so successful as a species is now setting us up for disaster, that is to say, our evolutionary capacity to solve problems has outstripped our capacity to adapt to the
new world that we've created for ourselves and so we've become psychologically and socially and physiologically and politically unhealthy and our civilization isn't any better.
That said, if any species could get us out of this mess, it's us. Like, you know, it's exactly as Brett said, we are the most labile the most adaptable the most generalist species on the planet, and born with the most potential to become anything else previously
so I do feel like in the end the message of the book which is explicitly and consistently evolutionary and all of its different instantiations is hopeful.
And yes that that quote that you read is ominous, and I think as Brett said you know factual statement, but we can do this, we have, we have to do it, and we need to try.
And in fact, in evolutionary biology, we recognize something we call adaptive peaks and adaptive valleys, and it would have to be true that to shift gears to something much better something that that gave humans more of what it is that we all value.
We would have to go through an adaptive valley and it would look frightening and in fact they are dangerous places to be, but it's part and parcel of shifting from one mode of existence to another.
All right, I think an idea that's going to be really important to get across and this is something as a guy that only ever thought I would talk about business.
And then in trying to explain how to get good at business I kept having to come back to mindset and then trying to explain mindset I keep having to go to evolution.
It's like that we're having a biological experience that your brain is an organ it comes.
You guys said that we are not a blank slate, but we are the blankest of slates, which I think is a phenomenal way to put this idea.
And I want to tie that to the title and get your guys this take.
So it's a hunter gatherers guide to the 21st century.
And so the way that I take that is that notion you have to understand that you're a product of evolution, that your brain is a product of evolution.
And then once you understand the forces of evolution and how we got here, then maybe just maybe we can find our way out of it.
So what are the key elements to being a product of evolution that you think people miss that we must understand if we're going to navigate our way well out of this valley of evolution.
Let me say first that the title hunter gatherers guide to the 21st century evokes that sort of romanticized hunter gatherer on the African savannah of the Paleolithic, which of course is a part of our human history, and does have many lessons in it to teach
us about who we are now and who we can become.
But as we say in the book, we are all parts of our history, like we are not just hunter gatherers, we are also right now post industrialists, and there are evolutionary implications of that.
Go a little farther back a lot farther back depending on your framing, and we are agriculturalists go farther back we're hunter gatherers go farther back with primates were mammals were fish, all of these moments of our evolutionary history, have left their mark in
us and have something to teach us about both what our capacities are, and what our weaknesses are and what we can do going forward.
And I would add the lessons from evolution are both good and bad here.
One thing that we realized that our students over the course of many years of teaching this material realized was that everything about our experience as human beings is shaped by our evolutionary nature.
And that has a very disturbing upshot, because we are fantastic creatures with an utterly mundane mission, the very same mission that every other evolved creature has to lodge its genes in the future.
And that this actually explains the nature, not only of our physical beings, but of our culture and our perception of the world.
So understanding that all of that marvelous architecture is built for an utterly mind numbing purpose is an important first step in seeing where to go.
Another thing to realize and you referenced our, our assertion that we are the blankest slate that has ever existed, or has ever been produced by evolution.
And what this means is that we actually have an arbitrary map of what we can change that to the extent that our genomes have offloaded much of the evolutionary adaptive work to the software layer.
That means we are actually capable of changing that layer, because that layer is built for change, but not everything exists in that layer.
So some things about what we are are very difficult to change some things are actually trivial, easily changed, and knowing which is which is a matter of sorting out where the information is housed but it's all there for the same reason.
It's exactly it's all there for the same reason it's all evolutionary be it genetic or cultural or anything else.
Can you guys give us an example of, and I found this very provocative in the book and it certainly rings true to me, but that to say that we are, in some ways, fish from an evolutionary standpoint that we are, you know, in some ways primates from an evolutionary standpoint.
What does that mean exactly.
Again, it's a factual claim one that once you've seen the picture standing from the right place is uncontrovertial.
We say, you know, is a platypus warm blooded, we are not asking a question about its phylogeny. Right, we're asking about how it works. Right.
When we ask is a whale, a mammal, we are asking a question about phylogeny. So, when we ask the question, are humans fish. If we're asking a functional question, then maybe not.
But if we're actually asking a question akin to is a mouse a mammal. Right.
Then we are asking a question about the evolutionary relatedness of that creature to everything else. And the key thing you need to understand is that a group, a good evolutionary group like mammal or primate or ape is a group that if you imagine the tree of life falls from the tree of life with a
little clip, right, if you clip the tree of life at a particular place, all of the apes fall together if you clip it lower down all of the primates fall together.
And the claim that we are fish is a simple matter of if we agree that a shark is a fish and we agree that a guppy is a fish.
If you clip the tree of life, such that you capture those two species, you will inherently capture all the tetrapods, which is to say creatures like us.
And so we are fish as a factual matter if the question is one of evolutionary relatedness. So let me just say, say that in in slightly different words.
There are at least two main ways to be similar. Right, you can be similar because you have shared history, and you can be similar because you've converged on some solution.
So dragonflies and swans both fly, not because the most recent common ancestor of dragonflies and swans flew, but because in each of their environments flight was an adaptive response.
And that means that flying flyingness is not a phylogenetic it's not a historical representation of what those two things are.
Because if you say well, both map both whales and humans lactate in order to feed their babies. That is a description of of something that they both inherited from a shared ancestor.
Right, so the earliest mammal lactated to feed its young.
If any, any organism on the planet today that is descendant of that first mammal that lactated to feed its young is a mammal, even if some future mammal went a different way and lost the ability to lactate it would still be a mammal.
You know Brett mentioned tetrapods tetrapods came you know with the fish that came out on to land with your four feet and started moving around as amphibians and the reptiles and the birds and the mammals.
But snakes are tetrapods, not because they still have four feet because they don't but because they're a member of those that group.
So it's a it's a historical description of group membership as opposed to like an ecological description of what we're doing. So we're not aquatic like most fish are, but we're fish because we belong to a group that includes all the fish.
I'm going to say why I think that matters and why I think you guys put that at the beginning of a book that sort of has this punchline of like hey we're really headed towards disaster and we have to be very thoughtful and here are some solutions.
So the reason why in business you end up having to talk about evolution is because I need a business owner to understand you cannot trust your impulses because your impulses may not have the growth of your business in mind.
It may not reflect an understanding of consumer behavior. It may simply be something from our evolutionary past that was like akin to it's better to jump away from the garden hose thinking that it might be a snake than it is to think that it might be a garden hose and it really is a snake.
Once you understand okay my mind is structured in a certain way it has these insane biases it tends me towards certain things like the one that bothers me the absolute most is that when people have a feeling.
It feels so real it and you never translated into logic so you're like that thing makes me angry therefore it is bad and it must be attacked to sailed whatever.
And if you run a business like that if you cannot divorce yourself from I have an impulse stop that insert conscious control and then figure out sort of what the first principles logical build up is you can't solve a novel problem.
And until you can solve a novel problem in an environment that changes as rapidly as our current world you guys call it hyper novelty if I remember correctly.
You you get into these crazy making scenarios and so while.
It seems almost absurd to say that in some way we are fish the key point that I take away from your book and that just seems so powerful to recognize to me is that.
You have to understand that you it wasn't a perfect construction at least not towards modern goals does that make sense to you guys.
Absolutely. Absolutely. Now there are there are really two upshots to this claim that you are a fish right it's very hard for people to wrap their minds around at the first time but once you realize that this is what we mean when we say.
You know a whale is a mammal that we are making a claim about the tree of life.
And then you can actually teach yourself how adaptive evolution works just by simply recognizing that snakes are the most species clade of legless lizards snakes are lizards right you don't think of it that way.
But they are seals are bears that have returned to the sea right so once you understand that all you have to do is say actually this is a that it's unambiguous and that means.
That adaptive evolution is the kind of process that can turn a bear into an aquatic creature like a seal right so that's one or a lizard into a snake right or a lizard into a snake.
The other thing that you mentioned and you're right on the money, which is that if you use your intuitive honed instincts in order to sort through novel problems you will constantly up and yourself because those instincts aren't built with those problems in mind.
Now the thing that's special for us humans is that we have an alternative and the alternative we argue in the second to last chapter of the book is consciousness that the correct tool for approaching novel problems is to promote whatever the underlying
issue is to consciousness to share it between individuals who likely have different experience will see different components of it clearly, and to come to an emergent understanding of what the meaning of the problem is and what the most likely useful solution may be so in some
sense really what you're saying is in this context you're trying to get people to get into their conscious mind and process this as a team activity, rather than go with their gut which is very likely wrong.
Absolutely. And, you know, our capacity as humans but that includes as a modern human who is, you know, trying to engage in business with people to oscillate between this conscious state and a cultural state, which is one in which
maybe change isn't happening so rapidly, maybe the rules that we've got are good for the current situation. Let's just do this let's do a set and forget on this set of things over here and not not constantly renegotiate.
Whereas, in this other part of the landscape, we actually do need to stay in our conscious minds and yes we need to tamp down the emotion and tamp down the, you know, the quick gut response, but engage with one another and recognize that, you know, it's not Satan on the
interaction it's another human being with all the same kinds of strengths and weaknesses as each of us has.
Yeah, there's a really interesting thing that happens when you have a team around you whether they're employees or otherwise where the just literally just the other day I said something to my team and several of them misconstrued it and I could see they were having a big emotional
response and I said okay, tell me your objection in a single sentence with no commas, no run-ons, no parentheticals. And what you find is that old Einstein quote of if you can't explain it simply you probably don't understand it very well.
And so people have this emotional reaction, but they, and they then enact out in the world that emotional reaction, but they don't actually stop to take the time to be able to say it in a single sentence.
And so you end up in what my my wife and business partner and I call you end up having to chase them because you'll solve the they'll say here's my problem you'll solve it and say cool so if I do something that addresses that and they'll be like well it's not quite that it's it's this and then you solve
that and they're like well it's not and it's like when you force people to say something really simply it forces them to interpret that emotion to bring it into the conscious mind and then to actually deal with it.
I find utterly fascinating. Do you guys have a method by which you do that in your own lives or that you've taught other people to do it.
Yeah, I would say there's a first go to move, which is let's figure out what we actually disagree about. And very frequently, you can cover half the distance or more, just simply by separating an issue into two different ones.
For example, if I talk to a conservative audience. I know we're going to disagree about climate change. But I also know from experience that I can get a conservative audience to agree that if they believed that human activity was causing substantial change to the climate and that
we need to destabilize systems on which we were dependent that they would be enthusiastic about doing something about it. And so what we really disagree about is whether or not we are causing something sufficient that we need to take that action.
Right, that's half the distance covered in a matter of just simply dividing it into two puzzles and you'd be amazed, almost everything that we have fierce disagreements about look like this where you just sort of assume the other side has every defect rather than realizing
that we disagree to a point at which point we differ. Yeah, no, and this is, this is different from what we were just talking about right with regard to you know you're having an emotional or analytical response.
This is a question of, Okay, we think we're talking about the same thing, but probably we are using the same words for different categories. Yes.
And can we can we figure out how many subcategories there are, and you know say I've got five and my thing you've got five and maybe there's only two that overlap.
So maybe we focus on those two but maybe there's also maybe that you know the devil in the details is in one of those, one of those other six that is only in one of the people's brains.
And when it's revealed to me like actually, you think I believe that thing and I don't like that's not something we share between us. So, yeah, having the capability to go in and like zoom in and out on problems and say, actually the problem can be smaller than
you think and and also it is larger than you think, and then I think and let's constantly reevaluate the framing and the scale at which we're doing analysis.
There's a lot of talk in the book about theory of mind and Heather I know you've either started writing have written or have threatened to write a science fiction novel which you know I desperately want you to do and publish.
But I've started doing a game when I find myself in that situation where and I learned this in my previous company where both of my partners are really smart guys but every now and then we get an argument and I'd be like I think they're an idiot but I know they're not an idiot and they think
they're an idiot but I'm not an idiot. And so I started approaching it as a writer and saying, okay, if I were writing this character in this scene, what would have to be true for them to be acting this way what would they have to believe be thinking
And in my marriage, this has become an extraordinary tool of saying, for you to be reacting this way you would have to think that I believe X, Y, Z. Is that the issue. And then by getting to that what I call base assumptions, you can really begin to facilitate that you guys must have
encountered this a bazillion times with students. How do you unearth that like what's the process of of uncovering that especially. In fact, it is so weird to me that you two have become like the most attacked people on planet Earth.
I will never quite understand how this has happened but how do you guys tease out and not just go out there evil how do you find those underlying issues.
Well, first of all, I think we're, we're attacked because we, we look like villains.
So much so.
Exactly.
Well, you, you hinted an important issue here that I think is actually quite modern. So if you lived any sort of normal existence from an ancestor, you know, even in just a couple hundred years before the present, you would find that they pretty much grew up around the people that
they ended up interacting with as adults they didn't stray very far from home, everything would be incredibly familiar and the language that they used to interact with everybody they were encountering would have been shared because it would have been picked up from an immediate group of
people that they both knew. Right. When we use English to talk to someone else. We have an incredibly blunt tool, because the ancestor from which we picked up that shared language is quite distant.
And what this does, you know, you really have two kinds of people in the world you've got people who more or less use the tool like English as it was handed to them.
And you have people who are trying to break new ground. And what is true for everybody who breaks new ground is that they end up building a personal toolkit they will redefine words so that they become sharper and more refined and more useful.
And then when you put two such people together, they will talk right past each other because they don't remember that they redefined things.
So one thing that is essential if you're going to team up with someone else who is generative and done their own work and arrived at some interesting conclusion, you need time, it's weeks of talking to each other before you even understand how they use language.
Once you do that, you can have an incredible conversation but if you think you're going to sit down with them, and immediately pool what you know and get somewhere, you got another thing coming because at first they're going to sound like they don't know what they're
talking about, right, you've got to find those definitions and figure out what they mean. And it's actually, if it once you realize that this is the job, it's very pleasurable and it's, it's really an honor when somebody lets you look through their eyes.
You say, Oh, that's how you see the world and now I get a chance to see it that way. And then let me show you what I'm seeing. And you really can get somewhere but there's no shortcut about the time necessary to learn each other's language.
Right. And that, that really is a parallel for what we were doing in the classroom as well. You know, we didn't, you know, if we were teaching 18 year olds we were teaching freshmen, we didn't assume that they all came in as experts, obviously.
And yet, the same logic applies that everyone has, you know, I wouldn't say actually I don't think I really agree that you know, regardless of what language you're speaking, you either, you know, take it on on faith as as you have received it, or you act decisively to change it
I think teenagers tend to be modifying language pretty actively and so especially when you, you know, find yourself in a room full of, you know, relatively young people in a college classroom, you have a lot of people who are using language differently than you the professor does and then you're also in the
business of introducing to them.
Some of the tools, some of which has specialized language associated with it, associated with, you know, whatever it is that you're teaching, and finding the common ground between these like okay, actually all of us modify language some.
And let's figure out how to use language that we can all agree on and understand. And, you know, for the purposes of communication, as opposed to for the purposes of displaying group membership.
Because that's because that's what jargon is often is about group membership displays, and that's what you know memes and especially.
Well, a lot of them, a lot of the very rapidly changing language that doesn't happen in technical space is really about demonstrating that you're on the inside of some, some joke.
Well, actually, this is a perfect case of a personal definition that must be shared otherwise you can't talk right because I at least distinguish between terms of art and jargon.
Most people will use the term jargon for both things but the point is terms of art are unnecessary evil.
You have to add some special term because the language that your hand at the general language doesn't cover it and so you need a special term to describe some thing. And that means that somebody walking into the conversation isn't necessarily aware of what's being said until they've learned that term.
Jargon is the pathological version of this jargon is the use of these specially defined terms to exclude people from a conversation that they probably could understand and that they might even realize you didn't know what you were talking about.
If they could understand the words that you were using so you use those words to protect yourself. And until somebody gets that when you say jargon, you're not talking about specialist language you're talking about a competitive strategy.
They won't know what you're saying so.
And you know the difference as Heather points out is in a room full of 18 year olds, especially when you're the professor at some level you can say look here are the terms that we need in order to have this conversation and more or less people will adopt them because that's the natural
state of things rather than two peers getting together where you have to, you know, my, my rule is, I don't care whether the definition ends up being the one that I came up with, or your set of definitions.
It doesn't matter to me what I need is a term for everything that needs to be distinguished and we both need to know what those terms are in order to have the conversation but whose terms they are doesn't matter.
Well, and yet, you know, as, as I think we say in the book, our undergraduate advisor Bob trivers extraordinary evolutionary biologist.
When we were leaving college and applying to grad school, he gave us a piece of advice about what kinds of jobs we might ultimately want if we were to stay in academia, and he said, do not accept a job in which you are not exposed to undergraduates,
because teaching undergraduates means exposing yourself and the thinking that you are presenting to, to naive minds who will throw curveballs at you.
And some of those curveballs are going to be nuisances and maybe they'll waste your time but some of them are likely to reveal to you the frailty in your own thinking or in the thinking of the field, and that is the way that progress is made.
And so, you know, whom we call peers is up for discussion and recognizing that we can, we can all learn from almost every person that we interact with is a remarkable way forward.
Yeah, and the corollary to that is there's a lot of pressure not to reveal what you don't know by asking questions that will establish the boundaries of your knowledge and being courageous about actually acknowledging what you don't know often leads to the best conversations.
Right.
Yeah, you guys do talk about that in the book and I think that this is such an important idea I'd love to tie it to something else you talk about which is, what is science like you guys have a pretty unique take on what science is that it could be done with a machete and a pair of boots out in the jungle it can be done in a laboratory.
Yeah, what is science.
Science is a method for correcting for bias.
And that method is pretty well known it has had a few updates along the way, but the, the basic idea is it is a slightly cumbersome mechanism for correcting for human bias and the result is that it produces a set of models and a
scope of knowledge that improves over time and what improves means is it explains more while assuming less and and fits with all of the other things that we think are true maximum.
Right, ultimately, all true narratives must reconcile and that includes the scientific narratives that we tell at different scales right the nano scale has to fit with the macroscopic scale, even if we don't understand how they fit together
yet. So ultimately we're sort of filling in from both sides what we understand. And what we expect is that they will meet in the middle like a bridge and if they don't it means we got something wrong somewhere.
Yeah, so science is not the methods of science.
It's not the glassware and the expensive instrumentation. And it's not the indicators that your scientists because you're wearing these things you know it's not the lab coat.
And it's not the conclusions of science. It's not the things that we think we know many of which things are actually true and some of which aren't.
Science is the process. And all those other things are sort of hallmarks that may or may not be accurate proxies. When you're trying to figure out is that person doing science is the science over here.
But what science is is actually process.
And it's worth saying that you don't need it for realms that are not counterintuitive.
Right, you don't need to do science in order to figure out where the desk chair is before you sit down right it is apparent to you where the desk chair is because you're built to perceive it directly.
And every so often, we all have the experience of looking at something and not being able to figure out what we're seeing there's some optical illusion the way we are sitting where we are in relation to the object we're looking at.
And then you will go through a scientific process, you know, if that is a so and so that also suggests this and I can see that that's not true so what could it be.
Right that that process is scientific but by and large the direct perception of objects around you because it is intuitive because it's built to be intuitive your system is built to understand it in a way that makes it intuitive doesn't require this.
So we need science where things are sufficiently difficult to observe or counterintuitive. So you need a process to correct for your expectations.
And what drives all this to me and that gets missed even though it's sitting in plain sight is to make progress you must hunger to know where you are wrong.
And if you can derive and again I come at everything from a business lens in business if you can derive tremendous pleasure and quite frankly self esteem from your willingness to seek out the imperfections and you're thinking you'll actually make it.
If you don't and it's an ego protective game for you and your ego is built around being right then you're you're going under and to your point about exposing yourself to undergrads some of the most phenomenal like incisive questions challenging my leadership have come from like interns who just they've never had a job before.
So they're like oh why are we doing XYZ and you're like why are we doing that. And if in that moment you're like I must you know present myself and have a reason for why we're doing that you actually talk yourself into something.
And because the market much like evolution or reality which is something I want definitely want to talk about how there's a weirdness that we're living through now where people feel like if they can convince you through language of something that it actually somehow affects the underlying truth.
But in business the market does not care like you can convince your team that you're right but if the market doesn't embrace it you're going to fail and there's something wonderful about that.
Well I want to I want to push back slightly admittedly this is not an area of expertise but it seems to me that there are two things that business needs to be divided into two things in order to really understand what you're getting at the business where the market is actually.
In a position to test your understanding of what is true and what will work and what people want and things like that is one thing that's real business and then there's a kind of rent seeking in which it may be about.
You know, a company that does not have a functional product that is selling the idea that it will have a product that no one else will have, and its stock price rises.
As a matter of speculation that may well be a realm in which is, it is deception. In fact, this is beyond the scope of the book, but wherever perception is the mediator of success, you have deception as an important evolutionary force where physics dictates whether you've succeeded
You can't fool physics so I don't know what the two words for the two kinds of business are but the rent seeking part of business and the actual production of superior goods or the same goods at a cheaper price.
That's a different kind of a business structure.
Well here's what's interesting. I really fast on that point I think that they do fall under the same category so when I say that the market decides so if your pitches hey boys and girls we have to deceive the market and we have to.
You know game it and here's how we game it and so everything is a function of your goal so if your goal is to deceive and to you know create a pump in your stock price there is a way to do that that will work and there is a way to do that that won't work.
And now getting into honorable goals versus you know dishonorable goals that that is really fascinating.
But I think that they do fall into the same category of either the thing you do moves you towards your goals or it does not.
Yep. I mean I still think there's room for division, because there is, you know, the mythology of the market is that it pays for value and rent seeking violates that rent seeking effectively is a failure of the market.
And so I don't know I don't know where the definitional split needs to be but it does seem to me that although you're right the you know whether whether what you are doing is assessing what you believe the psychology of the market to be or whether you are assessing what might be
physically possible in terms of a product. Those are both real systems that you are either correct about or not, but there does seem to me to be a distinction between rent seeking and and the production of actual value.
And there's a perfect analogy to be made to academic science, of course, and so in academia, if you are a scientist, you are supposed to be seeking an understanding of reality.
The way that modern science is done involves a lot of requesting of grants from most of the federal government and just as I imagine in business although definitely not my area of expertise, the bigger you are the harder it is to change course.
And in academia in part that means the later in your career you are the harder it is to change course and therefore the harder it's going to be to do something like embrace that you were wrong.
And, you know, actual honorable good scientists will always will always fess up and talk publicly about when they were wrong.
But if your entire lab is contingent on a model of the universe that is turning out to look ever less likely. It's going to be much more difficult for you to do that for you to embrace the wrongness of you know what might be the livelihoods of not just you but many of the people who are working under you.
How would you handle it.
Well, you have to restructure things so that what actually matters is being right in the long term. What we have is an epidemic of corruption inside of science which has more or less been spotted first with respect to psychology psychology is difficult to do because you're inherently
looking into the mind and you don't have a direct ability to measure most of what's there, but the P hacking crisis. Basically, the abuse of statistics to create the impression of discovery, which then resulted in the inability to reproduce a large fraction of the results in
psychology is actually the tip of a much larger iceberg that basically science as a process is excellent, but science as a social environment is defective, and especially defective, where we have plugged it very directly into market incentives
put scientists at an unnatural level of competition for a tiny number of jobs we produce huge numbers of applicants, which means that the incentive to cheat is tremendous and those who stick to the rules probably don't succeed very well so basically what we have is a
a race to discover who is best at appearing scientific and delivering those things that that the field wants to believe rather than those things that the field needs to know.
So, the short answer to your question which isn't especially operationalizable is you need to put a firewall between market forces and the scientific endeavor, because although science is an incredibly powerful process.
It is also a fragile process that needs insulation from market forces or it cannot work.
So I would say just in brief, again, not particularly operationalizable but reward public error correction.
You know, no matter no matter at what stage you are, and what the nature of the air was, unless there was intentional fraud, which of course does exist.
Public error correction should be rewarded without shaming without, you know, loss of priority and other things and the ability to do science because not only do we need people to be able to see that they've made mistakes and and actually course correct.
We need people to be taking enough risks early on that they are likely to sometimes make errors. And so in the current environment where any error, it can be considered like the death nail for a career.
We have ever more timid scientists, and that is making us less good at science as a society. And in fact, it almost seems implausible that people would go around acknowledging their errors but it wasn't so long ago that this was fairly common.
I used to study bats, and there's a famous example of this not so long ago, a guy named pedigree had advanced a radical hypothesis that suggested that the old world fruit bats the so called flying foxes were in fact not part of the same evolutionary history as the bats that we see here in the
world. For example, the micro bats.
He argued that they were in fact flying primates, which was a fascinating argument.
It was based on their neuro biology looking more like monkey neuro biology than it does like bat neuro biology which turned out to be the result of the fact that they use their eyes rather than echolocation.
And what he said at the point that it was revealed by the genes that he had been wrong was, if it is a wrong hypothesis, it has been a most fruitful wrong hypothesis, which was absolutely right the work that was done to sort that out was tremendously valuable.
And so anyway, nobody who has had to course correct and admit an error finds it pleasant, but we have to restore the rules of the game where the longer you wait, the worse it is so that the incentive is as soon as you know you're wrong owning up to it so that you are on the right side of the
puzzle as quickly as possible.
That has to be the objective.
As you guys look at society and where we're at now so one problem you've obviously just very eloquently laid out you've got incentives around admitting that you're wrong is could be the death knell of your career.
What else is going on that makes you guys have that quote that we started the episode with around, you know, sort of the you didn't use the word disintegrating but that there's to put my own words to it there's a crazy making that's happening at the societal level.
What has led to that like what are three or four factors that are causing that breakdown.
Well, you know, in part, you know the bias that we have as evolutionary biologists is that we see a failure to understand what we are as producing short term reductionist metric heavy pseudo quantitative answers to questions that warrant a much more holistic and
broader approach. And so what are some of the, the things that modern humans have embraced or have been told to embrace and some of us have and some of us haven't that have helped produce problems for for modern people.
This is not, this is not new with us, but the ubiquity of screens, the change in parenting styles to protect children from risk and unstructured play, and the drugging of children legally with anti anxiety and anti depression meds, more likely if
the speed of their boys. Those three things in combination, all of which were sort of on the rise in the 90s and hit fever pitch and the in the odds and early teens helped produce a generation that became in body adults, but with minds that had not had a chance
to actually learn what it is to be human. And some of that is reversible. And you know really we just by by chance, we were college professors.
We were college professors for basically the entire period of time, during which millennials were in college so we taught millennials from from beginning to end, and almost to a person our students were amazing and receptive and creative and and capable.
And if you, you know, when when we talk about the generation of millennials, it's those people who were drugged and screened and helicopter and snow plow parented right so with individual attention, people can be pulled out of the tailspin but as beside a level.
That's exactly what ran as a tailspin.
And is the tailspin exactly that what is it about those things that what does it create in people.
I want to address that as part of a slight reorientation of the question.
So, one of the things that is causing the dysfunction is, you know, it's not just the fact of the screens but it's what they imply that virtually everything that people know is coming through a social channel.
It is all open to manipulation, augmentation distortion.
And what people generally do not pick up in the normal course of an education, even what we consider to be a high quality education is interaction with systems that allow you to check whether or not that which sounds right actually comports with logic.
So, for example, if you interact with an engine, you can't fool an engine into starting you either figure out why it isn't starting or you don't.
And so we advocated for students that they dedicate some large fraction of their education to systems that are not socially mediated in which success or failure is dictated by a physical system that tells you whether or not you've understood or failed to understand.
And this can be mechanics or carpentry but it also can be baking frankly or learning to play the guitar.
Right or parkour anything where success or failure is non arbitrary what you don't want is an education built entirely of I succeeded when the person at the front of the room told me I got it because the person at the front of the room is a dope which unfortunately happens too often you may pick up wrong ideas and feel
rewarded for believing in them, and that can result in tremendous confusion.
I would just finally say that the book really is about what we have informally called an evolutionary toolkit, and that evolutionary toolkit, the beauty of it, what we saw, and what students reported to us in their picking it up.
The toolkit allows with a very small set of assumptions, the understanding of a large fraction of the phenomena that we care about almost everything we care about as humans is evolutionarily impacted, and the ability to go through what you are told about your psychology
and say, does that make sense, given the highest quality Darwinism that we've got. Does it make sense to be told that our genomes suddenly went haywire and that's why an ever increasing fraction of young people need orthodontia.
Does it make sense that we have a piece of our intestine called the appendix that is no longer of any value, and yet a huge number of people have this thing become inflamed and burst so that their lives are placed in jeopardy.
Nope, it does not. The ability to check what you're being told against a set of law, a toolkit for logic that is so robust that you can instantly spot nonsense is a very powerful enhancement, and it does not involve knowing more.
It involves knowing less and having that little bit that you know, be really robust.
That's terrific, I would just say it doesn't necessarily involve you knowing less, but being certain less.
It requires that you rest what you know on less. The foundation is more robust and less elaborate.
I was just about to ask what it means to know less, so thank you for that.
Yeah, that is very interesting when I think about, I forget the exact quote, but as the island of your knowledge grows, the shore of your ignorance grows too, you know, whatever the famous quote, but it's a really interesting dichotomy.
So, alright, we've got this generation that's growing up, they're looking at screens, you guys make a pretty interesting assertion in the book about what screens do in terms of you're getting emotional cues from a non-human entity and that it may play a part in the rise in autism.
I found that incredibly interesting.
What I want to better understand is what's going on in our brains that, so helicopter parenting or snowplow parenting, for instance, like why does that trap us in a perpetual childhood?
You guys talk about rites of passage in the book, I'd be very curious to hear, like, how do we begin to deal with some of these things, whether it's screens, whether it's snowplow parenting, you know, if I find myself a 19-year-old,
and I realize I've been done dirty, I've been on drugs for ages, I was raised essentially by a screen, I'm, you know, having trouble connecting, having trouble relating, and my parents have taken care of everything for me.
What are the symptoms I need to look out for, and then how do I push forward?
Well, in terms of symptoms, this is more or less a, it's a self-diagnosing problem.
You either, none of us feel perfectly at home in modernity, because in fact we are not at home, we can't be even, you know, the world that we live in is not the world of our grandparents, it's not even the world that we were born into.
We live as adults in a world that just literally didn't exist when we were born.
It's not the world even that our children were born into unless they were literally born yesterday.
Right, exactly. It's changing so fast. It can't be. But that said, you either are feeling constantly confused about what you're seeing and hearing and you don't know what to think, or you've found something that allows you to move forward.
And even if you can't fully manage what it is you're confronting, it should surprise you less and less.
And so we provide a couple of tools in the book. We talk about the precautionary principle, and we talk about Chesterton's fence, which are really two sides of the same coin.
And if your life has been built around the idea that whatever the newest thing is the, you know, the latest wisdom is what you were brought up on, then in all likelihood, you are, you know, taking various drugs to correct for various things, which may very well be the symptoms of the last drugs you took.
You, you know, you may be engaging in all kinds of behaviors to fix mysterious problems, maybe you can't sleep and, you know, so you're taking some aggressive mechanism to deal with that.
The basic point is back away from that which is novel and untested, and in the direction of that which is time tested, and it will result in a decrease in anxiety and increase in your control over your own life.
And the way you'll tell is that you will feel less confused more of the time.
Can you guys define Chesterton's fence? I thought that was a really great part of the book.
Yeah. So GK Chesterton was a 20th century political philosopher, maybe I'm not sure exactly how he would have defined himself, but of the many contributions that he made to, you know, I think he was a conservative, but one of the many
contributions that he made was imagining two people on a walk together and coming across a fence that appeared to be on their way. And person A says, let's get rid of the fence.
And person B says, well, what's it here for? Person A says, I don't care, it doesn't matter, I just want it gone.
And person B, Chesterton, I suppose, in my telling here says, there's no way that I should let you get rid of the fence until and unless you can tell me what its function is.
If you can tell me what its function is or was originally here for, then maybe we can talk about whether or not it's time for it to go.
But until you can explain to me what the function is or was, then there's no way that I should allow you to get rid of it simply because you see it as an inconvenience.
So, you know, the appendix that Brett already mentioned is a perfect example of this. And we talk about in the book things like, you know, Chesterton's breast milk, you know, we should be, you know, we should, we should be abandoning breastfeeding.
We are abandoning breastfeeding to the degree that we're doing so at our peril.
Chesterton's play, not letting children have long periods of unstructured play in which adults are not monitoring them and are not telling them not to bully each other, even though bullying is bad, yes.
But allowing children to figure out for themselves in mixed age groups, how it is to navigate risk themselves.
That is how those children will grow into competent young people. And, you know, if you do arrive at 19, having been drugged into submission and having had your parents clear all of the hazards out of the way for you.
The thing you can do is start exposing yourself to risk. And risk is risky. You know, this is, you know, this is both a tautology and also shocking to people because, you know, wait, you're telling me I need to expose my children to risk.
Well, if you want to guarantee that your child will make it to their 18th birthday alive, then sure, put them in a cocoon. Right. That's the way to make sure that their body will get to 18 is to reduce all risk from their lives and protect them from everything.
But will they have the mind of an 18 year old at that point. No, they will not. So you trade a little bit of security that your child will survive. And, you know, every time I say anything like that I get chills, you know, we have children.
They're teenagers now and the idea that one of them would die, and that they would die taking a risk that we had implicitly or explicitly encouraged.
I don't know how you go on right and your parents do but I don't know how you go on.
But the bigger risk is that they get to 18 and they're incompetent. They can't think and they don't know how to navigate the world, especially now where the world in the future will look nothing like it did in the past.
So we need to be able to problem solve and the way to do that is to be exposed to as many situations in which they are navigating on their own as early as possible.
Selection has really given parents a, the job of both managing risk and not fully managing risk. In other words, it's not that you don't protect your children, but you want to protect them at a level where they do make mistakes, and those mistakes do come back to haunt them
and it causes them to be wise adults who are capable of managing risk when the risks when the stakes are much higher.
And that's really the question. It's not do you want your child to be safe. Of course you do, but you want them to be safe across their entire life and if you protect them too much when they are young, they will not be able to do it when they are older, and the risks are frankly much larger.
One of the things that I find most intoxicating about you guys, your book, your podcast is nuance complexity, like recognizing that by being reductionist by boiling things down to, you know, make them simple, but note, no further, whatever the quote is, that there is a point at which you can reduce something so far that you lose what's really going on.
And finding our way through all of this complexity, though, is incredibly difficult. So as it comes to your own parenting style, how have you guys employed this? The idea that I'm most interested of yours is, is the idea that the magic happens in the friction.
So whether it's male female, whether it's right left, it's understanding or safety and risk, it's understanding that it's either side is problematic. How have you guys navigated that complexity?
Well, we gambled, neither of us knew particularly much about rearing children at the point we ended up with them, and we more or less gambled on the idea.
It wasn't that much of a surprise to me at the point that we ended up with them.
No, no, that carried them for months.
No, no, certainly, we knew they were coming for many months, but, but from the point of view of what one does to raise children well.
We hadn't had a lot of experience with young kids, they just hadn't been in our lives. And we gambled on an idea that I still think it's not entirely obvious why it works at all.
But if you treat your children more or less, at least cognitively, you just shoot way over their heads, right? You talk to them like, like adults from very early on.
And they cannot respond in kind, but they get much more than you would think based on what they can say in response.
And so we have been extremely open with our children about the hazards in the world that they face and the hazards in our family have been, frankly, greater than than most children would be confronted with.
At least in the weird world.
We have been honest with them. We, you know, we have an explicit rule in our household, and the children could recite it without thought, right? You are allowed to break your arm or your leg.
You are not allowed to damage your eyes. You're not allowed to damage your skull. You're not allowed to damage your neck or your back, right?
Now, when you say that to a kid, and they realize, actually, it's not that I'm being told no, no, no, no, no, I'm being told I am actually allowed to break my arm and nobody is going to necessarily, you know, be concerned.
You know, yes, we'll take care of you no matter what. And if you damage your eyes, we'll take care of you then too.
But the basic point is there's just a fundamental distinction between damaging things which repair pretty well and damaging things which don't. And that ought to exist in your mind.
You know, every time you leave the house, understanding that there are certain things, you know, that it's not that you want to avoid bad things and go towards good things, it's that there's a whole spectrum of bad.
And you may need in an instant to navigate, you know, if you're driving down the highway.
Yes, the first job is don't crash. Right. Don't crash is a good rule, but you can't always not crash. And sometimes you've got a choice about what you crash into or how you crash.
And, you know, if you've just got everything filed as a binary, then you're, you're in much more danger. So being clear with kids about the subtleties and the nuance and frankly about the bind that you're in.
Our children know that we have made a conscious decision that in order that they can manage risk as adults, they have to face risks as children that could potentially cost them their lives.
You know, we took our kids into the Amazon, for example, that's not a safe place to be, but they're also the kind of kids who can handle it now.
So one of the things that was very important to us was that our children literally learn how to fall.
That when they were climbing up on things on trees or in jungle gyms, that they would launch themselves intentionally so they would learn how to fall safely.
And metaphorically learning how to fall is the other thing that you learn once you are engaged in literally learning how to fall. And maybe, maybe that is the kind of risk that we are in fact trying to prepare our children for and that we are arguing that parents everywhere should be preparing their
children to learn how to fall safely so that you get up and can live to maybe not fall again, but if you do fall again, live to get up again another day.
Yeah, actually, it occurs to me right now that engineers know this backwards and forwards, right? Failsafe, that's what you want a system that fails safely and building that into your kids is an essential skill.
So one of the things you talked about in the book that I was like, whoa, was when your son broke his arm, I think he was older, because I know when the ones broke it going down the stairs, that one required immediate medical attention.
But there was a time where he broke his arm and it was like a couple days, I think, before you actually went and had it looked at.
And there's like an actual principle behind strengthening the bone that you guys go through and I was very impressed.
Talk about that, including the notion that as this is like, you guys have overcome one of the reasons that I didn't become a father was it seemed so self evident to me that you had to do things like let your kids take risks,
you know, within confines that you had to make things hard for them within confines.
And I wasn't sure that I would enjoy that process so it was obvious I would have to do it, and not obvious that I would enjoy it.
And when you guys talked about like, how we sort of over coddle things, which I could immediately empathize with I get why people do it why you want to wrap broken arm in the thickest cast you can possibly find, but that even that isn't always the right answer.
Yeah, no it's really not that we are brains are anti fragile and our bones are anti fragile, and they, they become stronger with stressors.
And society seems to be imagining that what we all are is fragile, and by imagining that we're fragile by creating conditions that imagine that we're fragile.
We become more and more fragile and less anti fragile so in the case of our older son, or it was our younger son but when he was older, who broke his arm in the last day of camp, we did get him to, to an emergency room that day it was several hours but it was that
And they told us that at the point that we got back home to Portland which was several, several hour drive and it was going to be many days before we got there that we should go see an orthopedist and to have a cast put on to have a cast put on so we spent several days, splinted.
He spent, he spent several days splinted and with some, with some pain medication before we ended up going home to Portland and where we did not get a cast but the important thing is, this is not an experiment we were ran on our child before I had to learn it on myself.
Right, so, evolutionarily speaking, there is a logic to what one does with broken bones and it's a very different logic.
Lots of creatures don't heal so well, horses famously don't heal very well and the reason for this is fairly obvious.
A horse, a wild ancestral horse that had a broken limb wasn't going to recover, that is to say once an animal was hobbled by a broken limb it was going to be picked off by a predator so the selection that creates the capacity to repair wasn't there.
On the other hand, sloths, which fall out of trees fairly regularly but don't depend on their ability to get away from predators through speed, actually survive very frequently and when we look at sloth carcasses, they very regularly have breaks that have healed so creatures that can heal have that capacity.
And humans are such creatures.
We are such creatures so one wants to be very careful right if a bone is misaligned, you then want to utilize medicine in order to get the healing process to work correctly so it doesn't heal in a misaligned way.
But if you've got a fracture and you haven't misaligned something, there's a whole other logic that takes over.
Immobilizing the arm isn't what we're built for, in fact what you're built for is to have pain and inflammation do the job and the result when I broke my arm and I decided you know what I've thought for my whole life why is it that we rush to get a doctor to to immobilize this and then we atrophy and have it removed and we have to rebuild our strength
maybe that's not how it's supposed to be logically evolution has prepared us for this let's see what happens when I broke my arm and I was certain that it was not misaligned and I let it go.
What I found out was a one has to be very careful for the first day or two until you learn what it is that you're capable of doing, but your capacity begins to return very, very quickly and the degree to which I was better off the time that I broke my arm that I fractured
my arm and did nothing medical.
Then the time that I fractured my arm and did the standard medical thing and had the cast was night and day different and the fact is, we talked to Toby our younger son when he broke his arm and we told him what we were thinking and he had watched me go through the experiment, and he elected to go through it
himself, and lo and behold the same logic applied in his case.
Yeah, that's really interesting and feels like there's a lot that we can extrapolate from that in terms of our real lives.
One idea that I find really enticing and I'm sad that I didn't go through it when I was a kid our rights of passage. Do you guys think about that.
We have dispensed so this is a classic Chesterton's fence issue where it used to be that there were these, you know, hallmarks of having passed through a certain developmental state and at some point, I think people started to feel that these things were primitive,
and they dispensed with all of them and much to our peril because it, you know what you are is a creature that starts out utterly helpless and ends up incredibly capable.
But there are moments at which you take on new responsibility, right now it's arbitrary is an 18 year old really an adult.
The ceremony is yes, in some ways. No, it's not really a moment at which you become an adult, but you do need a moment at which we say actually at this point, these responsibilities are ones we believe you can handle and going forward.
That's what they are and the ceremony itself instantiates it. The ceremony helps make it real. And maybe it's at 18 maybe it's at 15 maybe it's at 13, depending on the tradition, maybe it's counting in a different way, in cases where perhaps adherence to the calendar is not
the thing, but you know the moment now you are a man, now you are a woman is has got to be an empowering one and it's one of the things that is almost universally lost for us, us weird people.
And, you know, it may well be the case just as is the case with something like follow through in sport, what you do after you hit the ball actually does not matter but it is very important that you intend to follow through.
And in that same way, going through your life knowing that at this moment I'm going to be expected to do this thing, whether it's a vision quest or whatever it may be, knowing that that moment is coming and that on the far side of it you will be a different person is a developmental process in and of itself.
So it's very likely the thing that happens as you anticipate this right of passage that is really the important developmental thing but we've just dispensed with them all.
So we've talked a lot about evolution and all the different things and ways that it manifests in our life and I want to bring now people back to where we started in the book that, you know, we're at, if instead of a nuclear clock coming towards 12, you guys would say that from just a societal standpoint we're edging up somewhere in there.
Talk to us about the fourth frontier, but to understand the fourth frontier I think we have to understand the first three frontiers.
So if you can walk us through that it was a really interesting idea it was the part of your book that I had to read twice, because I was like, whoa, there's really something fascinating here and it hints at a very complex answer to a very complex problem.
It was entirely novel for me I've never heard this idea explored before, and I think that it'll be really helpful for people to see that you've thought not just through the problem but through potential solutions.
Well, the first thing to realize is that all evolved creatures are effectively in a search for opportunity, and that opportunity looks like for an average creature under average circumstances.
If it's a sexually reproducing creature, the average number of offspring that it will produce that reaches reproductive maturity themselves will be to doesn't matter if they produce 100 babies or three.
The average that will reach that number is to and the reason is because the population isn't growing or contracting so two parents will end up replacing themselves and no better at least on average.
When you have succeeded evolutionarily, you find some opportunity that allows that rule to be broken, right.
A creature that passes over a mountain pass and ends up in a valley in which it has no competitors may leave 100 times as many offspring as it would have if it had remained in its initial habitat and so these places where creatures
are never an unexploited or underexploited opportunity and their population can grow our frontiers and the feeling of growth is the feeling of evolutionary success.
The problem is all of these things are limited, right, no matter what opportunity you've found the population will grow until that opportunity is no longer underexploited at which point the zero sum dynamics will be restored.
So let's talk about the first three types of frontier before perhaps you, you expand on what the fourth frontier is so the first type of frontier being the one that most people think of when you hear the word when they hear the word frontier which is a geographic frontier.
So we begin the book by talking about the Beringians the first Americans who came over from through Beringia across what is now the Bering Strait from Asia into the new world something between 10 and 25,000 years ago.
Coming into two continents that had never before been inhabited by humans and that was a vast geographic frontier.
The second type of frontier might be called a technological frontier in which you innovate something that allows you to make use of resource that you here to for had not had access to.
The terracing of hillsides to allow water to be held and agricultural systems to be to be done where previously all the water would have run off taking the nutrients and the water with it.
That would be an example of a technological frontier.
And then the third type of frontier which is ubiquitous throughout human history is a transfer of resource frontier, and this is really not a frontier it's it's just it's theft, right.
The Beringians coming into the new world for the first time, again, 10 to 25,000 years ago, we're experiencing a geographic frontier.
Thousands of years later, when Europeans came to the new world from the other direction from from the East, they landed in a space that already had 10s of millions of people in it, and basically took over and that was a transfer of resource moment and a transfer
of frontier, basically theft so geographic frontiers and technological frontiers are not inherently theft transfer resource is. And so we are proposing a fourth frontier.
So I just say transfer of resource is the explanation for almost all wars and genocide, from the point of view of some population, the resources of some other population that cannot be defended are as if a frontier.
But the idea of the overarching idea is that all creatures are seeking these non zero sum opportunities that they are experienced as growth, that they are inherently self destabilizing that they cause the growth of populations that
then restores the zero sum dynamics restores the austerity which doesn't feel so good, and the population is then in the search for the next non zero sum growth frontier.
And the problem is, we can't keep doing that. Right, that process made us what we are and we've been tremendously successful at it, but there are no more tech geographic frontiers on earth.
We've found it all technologically we've done an excellent job of figuring out how to exploit the world in fact over exploit the world transfer resource is a world destabilizing not only is it a despicable process.
It's a lethal process from the point of view of the danger it puts us at we simply have weaponry that is too powerful. We are too interconnected. And so, in a sense, our fates are all now linked and we have to agree to put that competition aside and
then the question is well what do we do. Do we face. Do we accept the zero sum dynamics and live with austerity that doesn't sound like a very good sales pitch, even even if it was what we had to do.
What we propose in the book is that there's actually an alternative to this that one can produce a steady state that feels like growth to the people who experience it without having to discover new resource.
And that may sound preposterous it may sound utopian, we are not utopians, we regard utopia as the worst idea human beings ever had or at least very close to the top of that list.
There's nothing undoable about a system that feels like perpetual growth in the same way.
There is nothing utopian about the idea that it's always springtime inside your house. Right. It's always pleasant inside your house. That's not a violation of any physical law.
It's just a simple matter of the fact that we can use energy to modulate the temperature with a negative feedback system, and we can keep it very pleasant in your house all the time.
And the point is, can that be done in our larger environment, such that human beings are liberated to do the things that we are uniquely positioned to do to generate
energy to experience love to feel compassion to enhance our understanding of the world all of those things are the kinds of things that are worthy of us as an objective.
And what we need in order for more people to spend their time pursuing those things is a system in which we are freed from competing one lineage against the others for a limited amount of resources and and the
you know we are condemned to violence against each other in order to pursue these things. So, in essence, the fourth frontier is a steady state designed to liberate people.
We should say it is not something we believe we can blueprint from here. We know enough to navigate our way in that direction, but we cannot blueprint it it is something we will have to prototype and navigate to.
And what the good news is, although we here probably would not live to see the final product, things would start getting better immediately upon our recognition that pursuing the fourth frontier was the right thing to do that suddenly there
would be a tremendous amount of useful work to be done in discovering what the various mechanisms of that new way of being are.
You guys are going to have to give me a little more than that in the book you talk about you give an example and it was the thing that really allowed me to begin to understand how we could achieve a steady state that gave us those things.
I don't know if you remember the example that you gave in the book I do so if you don't let me know and I will refresh your memory but talking about the Mayans is that right in the book you specifically talk about craftsmanship.
But if you've got something for me on the Mayans I'll take it.
Well, I mean, we think we do we do both right.
And let's see.
Well maybe maybe remind us of exactly what we say about craftsmanship I remember that we talk about it but I'm not sure exactly what the context is here.
The idea was basically that so we have this inherent desire for growth but it isn't necessarily growth itself it's sort of the now I'm using my own words it's the neurochemical state of feeling this deep sense of satisfaction at having something of import is probably
the easiest way to think of it and that gave me something to grasp on to because I so I often get asked the question I've had financial success in my life and the irony of my life is that I'm constantly going around trying to convince people that money is not going to do for you what
you think it will it's very powerful but it isn't what most people think they think it will make them feel better about themselves and money is just absolutely incapable of doing that.
And so when you realize the only thing that matters is how you feel about yourself, you start playing a game of neurochemistry. And so this idea of craftsmanship felt like that to me.
Yeah, so you're recognizing the long term hormonal glow that you get from producing something of lasting value and beauty and meaning in the world.
As opposed to only being exposed to short term stuff, you know the difference between buying something at IKEA and putting it together with Allen wrenches on your floor, and of either making yourself or coming to know a craftsman who really builds things with care and
to engage with the intention that you will be able to pass this on to your children or your friends or you know whomever later on this is a piece with with lasting beauty lasting function that was built with someone who knew something about the wood or the metals or whatever the materials are.
That's a way into finding the kinds of meaning that a fourth frontier mentality can provide. Yeah, I think the distinction is one between the satisfaction of life coming from consuming, which is inherently empty versus producing and producing doesn't necessarily
have to mean stuff it can be meaning or insider and any one of a number of other things. But what we say about the Maya in the book, what we argue is that they very conspicuous this is an extremely long lived civilization.
They have lived thousands of years of remarkable success. And they had, as one of the things that they produced in all of their city states, they produced these incredible monuments, which are actually not what they appear to be.
We have spent a lot of time in in my territory. And these things look like pyramids in the sense that the Egyptians produced them but they are not they are in fact growing structures.
These things got bigger and bigger over time the longer a city state existed in the same place. And then there's the hidden version of this, which are an incredible network of roads stone roads that exist between the city states called sock base.
In any case, the point is, the Maya were producing things that stood in for population growth, they were taking some fraction of their productivity, and they were dedicating it to these massive public works projects.
And the thing about a massive public works project is that it brings a kind of reality and cohesion to the people involved. I mean, imagine yourself living in one of these amazing cities, and the public monuments made of stone that speak to the power and the durability of your
you know, part of this, this public space. These things allow the following process if it's not just a pyramid that you, you know, you, it's a line item on a budget you build the pyramid it's done but in fact what you do is you augment it.
Well, then in good years, you will have that to augment, and you will take some fraction of the productivity that might be turned into more people which would then result in more austerity.
You can invest it in these public works projects and then in a lean year, instead of having not enough to go and feed all of the mouths that have been created, you can just simply not augment the public works project.
That is a natural damper for the kind of ebb and flow the boom and bust that we have suffered so mightily under under modern economic systems.
So, the production of meaning, the production of shared space that actually augments the ability of people to interact with each other.
These things are models of what we should probably be seeking as a society, a system that tamps down the fluctuations that provides liberty to people that's really the key thing.
Tonight we want realized liberty for individuals that they can pursue what is meaningful rather than satisfying themselves with consumption. That's sort of a rough outline of what a fourth frontier would look like.
Nice, I love it.
In the book I can't remember if it was liberty that you were talking about specifically but you talk about it's an emergent property I assume you mean the same with liberty.
How do we create the bed from which liberty will emerge.
Well, what we argue in the book is that liberty is a special value. And the reason that is a special value is the really two ways to to delineate it.
You can be technically free, but not really free. Right. If you're concerned about being wiped out by a health care crisis or you're concerned that you may lose your job and have to find another in a different industry you're not really free even if
technically you could go out and start an oil company it's not going to happen so what we argue is that real liberty realized liberty is liberty you can act on.
And in order for a person to be liberated their more mundane concerns their safety, their sustenance all those things have to be taken care of and therefore we can know that we have succeeded when somebody has real liberty that they are capable of acting on it's
liberty. And what we argue is that the objective ought to be to provide real liberty for as many people as possible hopefully ultimately everyone would be liberated to do something truly remarkable rather than only elites
having that freedom.
As, as high a fraction of the population as possible.
If we say, as many people as possible it might sound like we're also interested in maximizing population growth and of course, you know, of course, we're not, you know, I think we will we will peak hopefully at some point soon, and then population may start going down through
that at every moment in in human history going forward.
The fast number of the greatest number of people possible who have maximal liberty will be a success.
And they just refine that slightly.
The objective is the maximum number of liberated people, but not living simultaneously.
Ultimately, the way to grant the marvelous liberated life to the maximum number of people is to get sustainable at the level that humans can live indefinitely on the planet rather than having a clock ticking where we just simply don't have the resources to continue doing what we're doing.
With so much disruption in tech and finances, you have to get educated.
Check out this episode with Raoul Paul to learn about how to protect yourself financially.
