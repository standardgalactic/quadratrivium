1
00:00:00,000 --> 00:00:07,840
Welcome to our tutorial on denoising diffusion-based gender modeling, foundations and applications.

2
00:00:07,840 --> 00:00:13,000
My name is Arash Vathlet, I'm a Principal Research Scientist with NVIDIA Research, and

3
00:00:13,000 --> 00:00:18,560
today I'm very excited to share this tutorial with you along with my dear friends and collaborators

4
00:00:18,560 --> 00:00:23,440
including Karsten Kreis, who is a Senior Research Scientist with NVIDIA, as well as

5
00:00:23,440 --> 00:00:27,800
Ruchi Gau, who is a Research Scientist with Google Brain.

6
00:00:27,800 --> 00:00:32,040
Before starting the tutorial, I'd like to mention that the earlier version of this tutorial

7
00:00:32,040 --> 00:00:36,640
was originally presented at CVPR 2022 in New Orleans.

8
00:00:36,640 --> 00:00:40,080
This tutorial received a lot of interest from the research community, and given this

9
00:00:40,080 --> 00:00:44,920
interest we decided to record our presentation again after the conference, and we'd like

10
00:00:44,920 --> 00:00:48,720
to share this broadly with the research community through YouTube.

11
00:00:48,720 --> 00:00:52,760
If you happen to watch this video, and you enjoy this video, we would like to encourage

12
00:00:52,760 --> 00:00:58,880
you to share this with your friends and collaborators, and hopefully together we can create more

13
00:00:58,880 --> 00:01:07,640
interest around denoising diffusion-based gender models.

14
00:01:07,640 --> 00:01:11,600
If you're watching this video, most likely you'll find me with deep gender learning.

15
00:01:11,600 --> 00:01:15,920
In deep gender learning, we assume that we have access to cliques of samples drawn from

16
00:01:15,920 --> 00:01:16,920
unknown distribution.

17
00:01:16,920 --> 00:01:22,120
We use this cliques of training data to train a deep neural network.

18
00:01:22,120 --> 00:01:27,160
If everything goes smoothly, at the test time, we can use this deep neural network to

19
00:01:27,160 --> 00:01:32,760
draw new samples that would hopefully mimic the training data distribution.

20
00:01:32,760 --> 00:01:39,960
For example, if we use these images of cats to train our deep neural network at the test

21
00:01:39,960 --> 00:01:45,720
time, we do hope that we can also generate images of cute handsome cats, as you can see

22
00:01:45,720 --> 00:01:47,880
in the bottom.

23
00:01:48,840 --> 00:01:52,720
Deep gender learning has many applications.

24
00:01:52,720 --> 00:01:57,760
Mostly the main applications are content generation that have use cases in different

25
00:01:57,760 --> 00:02:06,360
industries, including, for example, entertainment industry.

26
00:02:06,360 --> 00:02:09,920
Deep gender learning can be used for representation learning as well.

27
00:02:09,920 --> 00:02:16,680
If we have a deep gender model that generates really high quality images, mostly the internal

28
00:02:16,680 --> 00:02:23,800
representation in that model can be used also for downstream discriminative applications,

29
00:02:23,800 --> 00:02:29,280
such as semantic image segmentation, as you can see in this slide.

30
00:02:29,280 --> 00:02:33,520
Deep gender models can be also used for building artistic tools.

31
00:02:33,520 --> 00:02:40,400
In this example that you can see on this slide, we have a tool that can be used by an artist

32
00:02:40,400 --> 00:02:46,480
who happens to be just a six-year-old kid to draw a semantic layout of a scene.

33
00:02:46,480 --> 00:02:51,960
This tool can take the semantic layout and generate a corresponding high-quality image

34
00:02:51,960 --> 00:02:56,840
that has the same semantic layout.

35
00:02:56,840 --> 00:03:01,360
If you're a researcher and you watch the landscape of deep gender learning, you're going to see

36
00:03:01,360 --> 00:03:07,160
that this landscape is filled with various frameworks ranging from generative adversarial

37
00:03:07,160 --> 00:03:12,720
networks to virtual autoencoders, energy-based models, and autoregressive models and normalizing

38
00:03:12,720 --> 00:03:13,720
tools.

39
00:03:14,480 --> 00:03:19,280
Historically, the Computer Vision Committee has been using generative adversarial networks

40
00:03:19,280 --> 00:03:23,320
as one of their main tools for training generative models.

41
00:03:23,320 --> 00:03:28,240
In this talk, we would like to argue that there is a new and powerful generative framework

42
00:03:28,240 --> 00:03:30,720
called the Noise and Diffusion Models.

43
00:03:30,720 --> 00:03:36,720
These models obtain very strong results in generation, and we hopefully want to convince

44
00:03:36,720 --> 00:03:40,720
you that these models are very strong and can be used for various applications.

45
00:03:40,720 --> 00:03:45,800
Hopefully, in this talk, we're going to provide you with some foundational knowledge that requires

46
00:03:45,800 --> 00:03:50,720
for using these models in practice, and we're going to even talk about how these models are

47
00:03:50,720 --> 00:03:56,040
currently used for tackling some of the interesting applications that exist in the Computer Vision

48
00:03:56,040 --> 00:03:57,040
Committee.

49
00:03:57,040 --> 00:04:03,640
As I mentioned earlier, the Noise and Diffusion Models have recently emerged as a powerful

50
00:04:03,640 --> 00:04:07,080
generative model of performing cancer.

51
00:04:07,080 --> 00:04:12,720
In these two papers shown on this slide, one from OpenAI on the left side and one from

52
00:04:12,720 --> 00:04:18,120
Google on the right side, researchers observe that you can use the Noise and Diffusion

53
00:04:18,120 --> 00:04:23,360
Models to train generative models on challenging datasets such as ImageNet, and the results

54
00:04:23,360 --> 00:04:28,880
generated by these models is often very high-quality and very diverse, something that we haven't

55
00:04:28,880 --> 00:04:33,040
seen previously with other generative models such as GANs.

56
00:04:34,040 --> 00:04:39,600
The Noise and Diffusion-based models have already been applied to interesting problems

57
00:04:39,600 --> 00:04:41,600
such as super resolution.

58
00:04:41,600 --> 00:04:48,200
In this slide, you can see a super resolution framework that takes a low-resolution Image64x64

59
00:04:48,200 --> 00:04:56,120
dimension and generates high-resolution image in 1024x1024 pixels.

60
00:04:56,120 --> 00:05:01,320
This results show that actually, the Super Resolution Models built on top of the Noise

61
00:05:01,320 --> 00:05:07,320
and Diffusion Models can generate very high-quality, diverse models.

62
00:05:07,320 --> 00:05:12,400
If you're on social media, you've been probably having a hard time not noticing a lot of excitement

63
00:05:12,400 --> 00:05:17,040
that was created around Dolly 2 and Imagine.

64
00:05:17,040 --> 00:05:23,000
These two frameworks are examples of text-to-image generative models that take text as input,

65
00:05:23,000 --> 00:05:28,720
and they generate a corresponding image that can be described using that text.

66
00:05:28,720 --> 00:05:34,680
Using their core, these models use the Noise and Diffusion generative models, and on the

67
00:05:34,680 --> 00:05:40,440
left side, you can see for example Dolly 2 built by OpenAI can actually create this

68
00:05:40,440 --> 00:05:45,400
image of teddy bears skateboarding in Times Square, and on the right side, you can see

69
00:05:45,400 --> 00:05:52,600
Imagine can generate images of multiple teddy bears celebrating their colleague's birthday

70
00:05:52,600 --> 00:05:55,480
while sitting behind a cake that looks like pizza.

71
00:05:55,480 --> 00:05:56,640
This is impressive.

72
00:05:56,640 --> 00:06:01,040
These models can generate very high-quality, diverse images, and they only take text as

73
00:06:01,040 --> 00:06:02,040
input.

74
00:06:02,040 --> 00:06:05,520
Today, not only are we going to talk about diffusion models, we're going to even talk

75
00:06:05,520 --> 00:06:08,320
about how you can use diffusion models to create such models.

76
00:06:08,320 --> 00:06:17,120
Towards the end of the video, Rucci will talk about these applications.

77
00:06:17,120 --> 00:06:24,120
Today's program consists of six main parts, besides introduction and conclusion.

78
00:06:24,120 --> 00:06:30,640
The first three parts that are shown in green are mostly the technical components of the

79
00:06:30,640 --> 00:06:31,640
program.

80
00:06:31,640 --> 00:06:33,520
I'm going to start with part one.

81
00:06:33,520 --> 00:06:38,040
I will talk about the Noise and Diffusion probabilistic models, and after me, Carson

82
00:06:38,040 --> 00:06:42,400
will talk about the score-based generative modeling with differential equations, and

83
00:06:42,400 --> 00:06:47,000
after us, Rucci will talk about advanced techniques, mostly around accelerated sampling

84
00:06:47,000 --> 00:06:49,000
and condition generation.

85
00:06:49,000 --> 00:06:53,920
These parts, each one of them would be roughly around 65 minutes to 45 minutes.

86
00:06:53,920 --> 00:06:59,000
After these parts, we're going to have three short parts around applications, and mostly

87
00:06:59,000 --> 00:07:05,320
computer vision applications that have been recently used diffusion models in their core

88
00:07:05,320 --> 00:07:08,800
to tackle various deep generative learning-related applications.

89
00:07:08,800 --> 00:07:13,440
Finally, we're going to have a very short segment where I will talk about conclusions

90
00:07:13,440 --> 00:07:16,200
of open problems and final remarks.

91
00:07:16,840 --> 00:07:22,320
Before starting, I'd like to mention that our slides, videos, and some content will be

92
00:07:22,320 --> 00:07:26,800
available on this website, so I'd like to encourage you to bookmark this website.

93
00:07:26,800 --> 00:07:31,640
We're hoping to add more content in the future to this website.

94
00:07:33,640 --> 00:07:38,920
Before starting the presentation, I'd like to just mention that we did our best to include

95
00:07:38,920 --> 00:07:46,160
as many papers that we could, and we thought that it could be interesting to the community.

96
00:07:46,160 --> 00:07:51,800
However, due to limited time and capacity, of course, we could not include every paper.

97
00:07:51,800 --> 00:07:55,800
So if there's a particular paper that you are passionate about, you're very excited

98
00:07:55,800 --> 00:08:01,800
about it, and you would like to be included in this tutorial, we apologize that we couldn't

99
00:08:01,800 --> 00:08:06,000
do that, and what we encourage you to send us an email, let us know that there was a

100
00:08:06,000 --> 00:08:10,640
paper that would be interesting to have in our program, and hopefully, in the future

101
00:08:10,680 --> 00:08:15,640
versions of this tutorial, we will try to include those papers as well.

102
00:08:17,640 --> 00:08:23,640
With that in mind, I will start the first segment, the noise and diffusion probabilistic models.

103
00:08:25,640 --> 00:08:28,240
So part one, the noise and diffusion probabilistic model.

104
00:08:28,240 --> 00:08:33,360
Here you can see an image of three cute dogs who are trying to understand the noise and

105
00:08:33,360 --> 00:08:37,280
diffusion probabilistic models, and as you can see, they're a little bit lost.

106
00:08:37,280 --> 00:08:42,440
So let's go over these models and discuss how these models can be generated.

107
00:08:47,360 --> 00:08:54,320
So using diffusion models, officially consists of two processes, a forward diffusion process

108
00:08:54,920 --> 00:08:56,800
that gradually adds noise to input.

109
00:08:57,440 --> 00:09:00,480
This process is shown on this figure from left to right.

110
00:09:01,280 --> 00:09:03,680
It starts from image of this cute cat.

111
00:09:04,040 --> 00:09:06,040
His name is Peanut, he's my cat.

112
00:09:06,400 --> 00:09:11,960
We can start from him, and we're going to add noise to this image one step at a time.

113
00:09:12,680 --> 00:09:17,360
The forward diffusion process does this in so many steps such that eventually on the

114
00:09:17,360 --> 00:09:20,360
right side, we converge to white noise distribution.

115
00:09:21,280 --> 00:09:27,560
The second process is the reverse denoising process that learns to generate data by denoising.

116
00:09:28,120 --> 00:09:33,680
This process is shown on this slide from right to left, and it starts from white noise and

117
00:09:33,680 --> 00:09:37,360
it learns to generate data on the left side by denoising.

118
00:09:37,800 --> 00:09:43,880
So this process will take a noisy image, and it will generate a less noisy version of it,

119
00:09:43,880 --> 00:09:48,920
and it will repeat this process such that it can convert noise to data.

120
00:09:49,760 --> 00:09:54,320
So we're going to dig deeper into these two processes, and we will see how we can define

121
00:09:54,320 --> 00:09:55,800
these processes formally.

122
00:09:56,240 --> 00:10:02,280
Then the forward diffusion process, as I said, starts from data and generates these intermediate

123
00:10:02,280 --> 00:10:06,440
noisy images, by just simply adding noise one step at a time.

124
00:10:07,320 --> 00:10:13,400
At every step, we're going to assume that we're going to use a normal distribution to generate

125
00:10:13,400 --> 00:10:17,000
this noisy image condition on the previous image.

126
00:10:17,800 --> 00:10:21,520
This normal distribution will have a very simple form.

127
00:10:21,600 --> 00:10:27,440
We're going to represent this normal distribution using q that takes this x at the previous

128
00:10:27,440 --> 00:10:30,440
step and generate x at the current step.

129
00:10:30,440 --> 00:10:33,440
So it takes, for example, x1, and it generates x2.

130
00:10:34,320 --> 00:10:40,840
As I said, it's a normal distribution over the current step, xt, where the mean is denoted

131
00:10:40,840 --> 00:10:46,440
by this square root of one minus beta t times the image at the previous step, and this beta

132
00:10:46,440 --> 00:10:48,440
t representing the variance.

133
00:10:49,360 --> 00:10:56,360
For the moment, assume that this beta t is just simply a very small positive scalar value.

134
00:10:56,360 --> 00:11:00,360
It can be like 0.001, some selectors.

135
00:11:02,360 --> 00:11:07,360
Here, this normal distribution basically takes the image at the previous steps.

136
00:11:07,360 --> 00:11:14,360
It rescale this image, the pixel values on this image, by the square root of one minus

137
00:11:15,280 --> 00:11:22,280
beta t, and it adds a tiny bit of noise where the variance of noise is beta t.

138
00:11:22,280 --> 00:11:28,280
So this is just a diffusion kind of we can call per time step, because we had this very

139
00:11:28,280 --> 00:11:35,280
simple form per time step, like per step, in order to generate xt given xt minus one.

140
00:11:35,280 --> 00:11:41,280
Now, we can also define the joint distribution for all the samples that will be generated

141
00:11:42,200 --> 00:11:47,200
in this trajectory, starting from x1, all the way going to x capital T.

142
00:11:47,200 --> 00:11:52,200
Capital T represents the number of steps in our model, and the joint distribution of all

143
00:11:52,200 --> 00:11:58,200
these samples, condition of this input image of peanut, will be the product of conditionals

144
00:11:58,200 --> 00:12:01,200
that are formed at each step.

145
00:12:01,200 --> 00:12:06,200
So this just represents the joint distribution of all the samples that will be generated

146
00:12:06,200 --> 00:12:09,200
on this trajectory using this Markov process.

147
00:12:10,120 --> 00:12:15,120
This is a Markov process that generates one step, that generates examples one step at

148
00:12:15,120 --> 00:12:18,120
the time, given the previous examples.

149
00:12:18,120 --> 00:12:24,120
Now that we know how we can generate samples one step at a time, you may ask me, how can

150
00:12:24,120 --> 00:12:29,120
I now just take this input image and jump to particular time the step?

151
00:12:29,120 --> 00:12:34,120
Do I need to sample, generate samples one step at a time, or can I just take this x0

152
00:12:35,040 --> 00:12:40,040
and generate xt, or x4, for example, here, just directly?

153
00:12:40,040 --> 00:12:46,040
Because in the forward process, we're using a very simple Gaussian kernel to diffuse the

154
00:12:46,040 --> 00:12:52,040
data, we can actually show that because of this simple form, we can first define this

155
00:12:52,040 --> 00:12:54,040
scale.

156
00:12:54,040 --> 00:13:01,040
This scalar alpha bar T is the product of one minus betas from time to step one all the

157
00:13:01,040 --> 00:13:03,040
way up to current step T.

158
00:13:03,960 --> 00:13:08,960
This is just defined based off the parameters of the diffusion kernel, and having defined

159
00:13:08,960 --> 00:13:15,960
this alpha bar T, now we can define a Gaussian kernel or the diffusion kernel that will generate

160
00:13:15,960 --> 00:13:18,960
xt, even x0.

161
00:13:18,960 --> 00:13:22,960
So, for example, we can now generate using this Gaussian kernel, we can sample from x4

162
00:13:22,960 --> 00:13:29,960
given x0, this would be again a normal distribution where mean is same as the input image, really

163
00:13:30,880 --> 00:13:35,880
this square root of alpha bar T defined here alpha bar, and then the variance is also one

164
00:13:35,880 --> 00:13:37,880
minus alpha bar T.

165
00:13:37,880 --> 00:13:44,880
So, just remember that these betas are just parameters of the diffusion process, we can

166
00:13:44,880 --> 00:13:50,880
compute this alpha bar T very easily, and then we can sample from xt given x0 using

167
00:13:50,880 --> 00:13:55,880
this normal distribution, and this we're going to call this diffusion kernel that diffuses

168
00:13:56,800 --> 00:14:01,800
input at time step zero to time step xt.

169
00:14:01,800 --> 00:14:06,800
Recall that if you want to sample from just a simple normal distribution, you can use

170
00:14:06,800 --> 00:14:08,800
the reparameterization trick.

171
00:14:08,800 --> 00:14:13,800
So, if you want to draw samples from xt, you can just set xt to mean plus some white

172
00:14:13,800 --> 00:14:19,800
nose epsilon that is drawn from standard normal distribution, rescaled with this square

173
00:14:19,800 --> 00:14:24,800
root of one minus alpha bar T, which represents just a standard deviation of this normal distribution.

174
00:14:24,800 --> 00:14:31,800
So, using this we can just simply generate samples at time step T given samples at time

175
00:14:31,800 --> 00:14:36,800
step zero, so given x0 we can just diffuse it easily to time step T.

176
00:14:36,800 --> 00:14:43,800
Beta T values are important, these are basically, we're going to call beta T values as the noise

177
00:14:43,800 --> 00:14:49,800
schedule that defines how we're diffusing the data, and this noise schedule, this noise

178
00:14:49,800 --> 00:14:55,800
schedule, designs such that alpha bar T, this alpha bar at the very last step, would

179
00:14:55,800 --> 00:15:01,800
converge to zero, and when this converges to zero, if you just set alpha bar T to zero

180
00:15:01,800 --> 00:15:06,800
here, you're going to see that because the way that the forward diffusion process is

181
00:15:06,800 --> 00:15:13,800
defined, this diffusion kernel at last step given the x capital T given x0 would be just

182
00:15:13,800 --> 00:15:18,800
can be approximately using normal distribution, standard normal distribution.

183
00:15:18,800 --> 00:15:23,800
This basically means that at the end of this process, diffuse data will have just a standard

184
00:15:23,800 --> 00:15:28,800
normal distribution, this is something that we will need later when we want to define a

185
00:15:28,800 --> 00:15:31,800
generative point.

186
00:15:31,800 --> 00:15:37,800
Now that I've talked about this forward process, like how we can diffuse the diffusion

187
00:15:37,800 --> 00:15:43,800
kernel that diffuses data, let's talk about the marginal diffuse data distribution, let's

188
00:15:43,800 --> 00:15:48,800
talk about what happens to data distribution as we go forward in the diffusion process.

189
00:15:48,800 --> 00:15:53,800
So, have in mind that diffusion kernel that generates xt given x0 is different than the

190
00:15:53,800 --> 00:15:59,800
diffuse data distribution, so we're going to use qxt to represent diffuse data distribution,

191
00:15:59,800 --> 00:16:04,800
and in order to obtain this diffuse data distribution, we first need to form the joint

192
00:16:04,800 --> 00:16:12,800
over clean data input data x0 and diffuse data xt, this joint simply can be defined as

193
00:16:12,800 --> 00:16:18,800
product of input data distribution qx0 times this diffusion kernel, which is just a simple

194
00:16:18,800 --> 00:16:24,800
normal distribution, and now we can marginalize at x0, we can just integrate at x0, and this

195
00:16:24,800 --> 00:16:31,800
will give us marginal data diffuse data distribution at times the T.

196
00:16:31,800 --> 00:16:39,800
If we just consider a very simple one dimensional data, and we hear on visualizing on visualizing

197
00:16:39,800 --> 00:16:44,800
the diffuse data distribution at different time steps, on the left side we have data distribution

198
00:16:44,800 --> 00:16:52,800
at times zero, why access represents this just one dimensional random variable and this x axis

199
00:16:52,800 --> 00:16:58,800
represents the PDF probability density function of this random variable at time step zero, this

200
00:16:58,800 --> 00:17:05,800
is just the data distribution visualized for one toy example, one dimensional toy example.

201
00:17:05,800 --> 00:17:11,800
Here you can see the visualization of diffuse data distributions, and as you can see, as we

202
00:17:11,800 --> 00:17:16,800
go in the forward process, we just take this data distribution, we're making kind of this

203
00:17:16,800 --> 00:17:21,800
distribution smoother and smoother as we go forward in time, and eventually it becomes so

204
00:17:21,800 --> 00:17:27,800
smooth that we can just represent this distribution using standard normal distribution, zero mean

205
00:17:27,800 --> 00:17:34,800
unit variance normal distribution. As you see this smoothing process, we can actually show

206
00:17:34,800 --> 00:17:40,800
mathematically this, the diffusion kernel in the forward process, this diffusion kernel here

207
00:17:40,800 --> 00:17:46,800
is kind of applying a Gaussian convolution to the data distribution, so this smoothing

208
00:17:46,800 --> 00:17:50,800
process can be just represented as Gaussian convolution, a convolution in the sense of

209
00:17:50,800 --> 00:17:56,800
like signal processing convolution that takes input data distribution makes it smoother and

210
00:17:56,800 --> 00:18:02,800
smoother. However, we should have in mind that we actually don't have access to these, to input

211
00:18:02,800 --> 00:18:08,800
data distribution and intermediate diffuse data distribution practice, usually we only have

212
00:18:08,800 --> 00:18:14,800
samples from data distribution, we don't have access to the explicit form of the probability density

213
00:18:14,800 --> 00:18:22,800
function of data distribution, right? So even though we don't have access to this distribution

214
00:18:22,800 --> 00:18:29,800
or all the intermediate diffuse distributions, we know how to draw samples from diffuse data

215
00:18:29,800 --> 00:18:33,800
distribution, so in order to generate data from diffuse data distribution, we can just simply

216
00:18:33,800 --> 00:18:39,800
sample from training data x0, and then we can just follow the forward diffusion process,

217
00:18:39,800 --> 00:18:46,800
sample xt to an x0, in order to draw samples from xt, and this is basically the principle

218
00:18:46,800 --> 00:18:51,800
of ancestral sampling that you can just basically use in order to generate data, for example, at times

219
00:18:51,800 --> 00:18:58,800
that t, you can just use the training data, sample from training data, diffuse it and sample

220
00:18:58,800 --> 00:19:07,800
at xt using diffusion kernel, and that will give you samples from marginal data distribution.

221
00:19:07,800 --> 00:19:16,800
Okay, so so far we talked about forward process and how this forward process just smoothens

222
00:19:16,800 --> 00:19:22,800
the data distribution, now let's talk about how we can define a generative model out of this.

223
00:19:22,800 --> 00:19:29,800
In order to generate data in diffusion models, what we can do, we can start from this base

224
00:19:29,800 --> 00:19:35,800
distribution at the end, we know that the diffuse data distribution would converge to

225
00:19:35,800 --> 00:19:42,800
0 in unit variance, standard normal distribution, so we can sample from standard normal distribution

226
00:19:42,800 --> 00:19:50,800
and we can follow the reverse process, where at every step we can now sample from the denoising model

227
00:19:50,800 --> 00:19:56,800
that generates the less noisy version of data given current step, so this is the reverse model

228
00:19:56,800 --> 00:20:02,800
where we now use the true denoising distribution to sample from xt minus 1 given xt.

229
00:20:03,800 --> 00:20:10,800
So we just need to start from xt and just use this true denoising model to generate samples at time step 0.

230
00:20:10,800 --> 00:20:16,800
So the algorithm will be something like this, we sample x capital t from standard normal distribution

231
00:20:16,800 --> 00:20:23,800
and we sample iteratively from xc minus 1 using this true denoising distribution.

232
00:20:24,800 --> 00:20:32,800
The problem here is that in general denoising distribution xt minus 1 given xt is intractable,

233
00:20:32,800 --> 00:20:38,800
we don't have access to this distribution, we can use Bayes' rule to show that this distribution

234
00:20:38,800 --> 00:20:44,800
is proportional to the product of the marginal data distribution at xc minus 1 times this diffusion

235
00:20:44,800 --> 00:20:51,800
at t, this kernel is simple, it's just Gaussian distribution, however this distribution marginal

236
00:20:51,800 --> 00:20:57,800
data distribution is intractable and in general because of this, this product is also intractable

237
00:20:57,800 --> 00:21:04,800
so we don't have it in closed form. Now that we don't have it in closed form, you may say

238
00:21:04,800 --> 00:21:11,800
can I approximate this denoising distribution from data and if yes, what is the best form

239
00:21:11,800 --> 00:21:18,800
I can use to represent that denoising model. So the good news is, yes, we can try to train

240
00:21:18,800 --> 00:21:26,800
a model to mimic this true denoising distribution xt minus 1 given xt and if you want to model

241
00:21:26,800 --> 00:21:31,800
that, the best model that you can use, the statistical model you can use to represent this denoising model

242
00:21:31,800 --> 00:21:40,800
is actually a normal distribution if in the forward process we use very small noise or the variance

243
00:21:40,800 --> 00:21:45,800
of the noise that we're adding at each step is very small. If we know the forward process

244
00:21:45,800 --> 00:21:53,800
at very small amount of noise, we know theoretically that actually the reverse process can be also approximated

245
00:21:53,800 --> 00:21:59,800
using a normal distribution, so this denoising model can be represented using denoising model.

246
00:21:59,800 --> 00:22:09,800
So now that we know this, we can actually define a parametric model to mimic or to train this true denoising model.

247
00:22:10,800 --> 00:22:17,800
So this formally defined our parametric reverse denoising process. Remember that reverse denoising process starts

248
00:22:17,800 --> 00:22:24,800
from noise and generous data by denoising once at a time. We're going to assume that the distribution of data

249
00:22:24,800 --> 00:22:30,800
at time is the capital T at the end of the forward process, so it's the beginning of the reverse process.

250
00:22:30,800 --> 00:22:36,800
It will be standard normal distribution here as soon as data has a standard normal distribution.

251
00:22:36,800 --> 00:22:43,800
We define this denoising distribution. This is a parametric denoising distribution as samples from XT minus from given XT.

252
00:22:43,800 --> 00:22:51,800
We're going to assume that it also can be represented using normal distribution where now mean here is parametric.

253
00:22:51,800 --> 00:23:02,800
It's a deep neural network that takes noisy image XT and predicts the mean of the less noisy version, less noisy image.

254
00:23:03,800 --> 00:23:08,800
So this neural network is the trainable component and then we have also the variance per time step.

255
00:23:08,800 --> 00:23:15,800
This is just think of the sigma T squared as just some scalar value that represents the variance per time step.

256
00:23:15,800 --> 00:23:21,800
And for now just assume that it's just some parameter or we have access to it. We're going to talk about it later.

257
00:23:21,800 --> 00:23:25,800
But the most important part is this mean parameter.

258
00:23:25,800 --> 00:23:34,800
Remember, this is the trainable network and it takes a noisy image at XT and predicts the mean of less noisy image at XT minus 1.

259
00:23:34,800 --> 00:23:42,800
Because it takes an image and predicts an image, we can actually model this using a unit that has the same input and output shape.

260
00:23:42,800 --> 00:23:50,800
Or you can think of it as just a denoising autoencoder that denoises the input image to less noisy level basically.

261
00:23:50,800 --> 00:23:58,800
And we're going to talk about the architecture of this denoising model, this new filter.

262
00:23:58,800 --> 00:24:04,800
Now that we have these definitions, we can define the joint distribution on the full trajectory.

263
00:24:04,800 --> 00:24:16,800
This joint distribution is the product of the base distribution at step XT and the product of conditionals at all these steps at T steps where the condition comes from our denoising model.

264
00:24:16,800 --> 00:24:31,800
This is again just a Markov process and we can define the joint distribution on the full trajectory by the product of base distribution and the product of these individual conditionals on each denoising step.

265
00:24:31,800 --> 00:24:43,800
Okay, so now so far we talked about the forward process, the reverse process. Now let's talk about how we can train these models, how we can train the denoising model.

266
00:24:43,800 --> 00:24:52,800
For training the denoising model, we're going to use variation upper bond that is mostly or commonly used for training by autoencoders.

267
00:24:52,800 --> 00:25:02,800
In the variation upper bond, ideally we want to optimize marginal likelihood of the data under our parametric model.

268
00:25:02,800 --> 00:25:13,800
Here we have this expectation over training data distribution, where we are computing the expected value of the low to likelihood that our trainable model gives to data distribution.

269
00:25:13,800 --> 00:25:27,800
Unfortunately, this is interactive, we don't have access to this quantity here, but we can define variation upper bond, where now we have this expectation over training data, we have expectation over samples drawn from the forward process.

270
00:25:27,800 --> 00:25:38,800
This is forward process, you can think of it as encoder in VAE, that samples from latent space, so you can think of these as latent variable x1 to capital T.

271
00:25:38,800 --> 00:25:54,800
And we have this log ratio here where the nominator is the joint distribution of the samples on the full trajectory that is given by our denoising model, P theta, and in the denominator we have the likelihood of the samples generated by encoder.

272
00:25:54,800 --> 00:26:03,800
This is basically the same objective or same training object, we would see variation of bond in variation autoencoders.

273
00:26:03,800 --> 00:26:04,800
This is exactly the same.

274
00:26:04,800 --> 00:26:14,800
The assumption is that the forward process is kind of like an encoder and the reverse process is the genitive model in VAE.

275
00:26:14,800 --> 00:26:27,800
These two papers, by the way, our papers, the links on our slides are clickable, so if you want to check these papers just find our slides and click on these references and you're going to find the paper.

276
00:26:27,800 --> 00:26:35,800
So these two papers showed that this variation bond can be decomposed to several times that we're going to go one by one.

277
00:26:35,800 --> 00:26:48,800
The first term is just simply the KL divergence from the diffusion kernel in the last step, x capital T given x0 to this base distribution xT.

278
00:26:48,800 --> 00:26:57,800
Remember by definition, the diffusion kernel for x capital T converges to standard normal distribution, which is same distribution we assume for xT.

279
00:26:57,800 --> 00:27:07,800
So we can completely ignore this term because by definition, forward process defines such that at the end of process, samples converge to 0 million units of mass distribution.

280
00:27:07,800 --> 00:27:09,800
So we can ignore this term.

281
00:27:09,800 --> 00:27:18,800
We have this KL term that I'm going to go into details, and then we have this term that can be considered as the reconstruction term in VAEs.

282
00:27:18,800 --> 00:27:27,800
This just measures the likelihood of input clean image, even the noise image at first step under our denoising model.

283
00:27:27,800 --> 00:27:32,800
This term is also very simple, and it has actually structure very similar to this other term.

284
00:27:32,800 --> 00:27:38,800
So for moment, just assume that we can compute this very easily, and we just ignore for a moment.

285
00:27:38,800 --> 00:27:43,800
And mostly we just need to focus on this term, which is the most kind of important term here.

286
00:27:43,800 --> 00:27:53,800
This is the KL divergence between this Q, xT minus 1 given xT and x0 to our denoising model.

287
00:27:53,800 --> 00:27:57,800
This is our parametric denoising model, which is xT minus 1 given xT.

288
00:27:57,800 --> 00:28:02,800
This Q distribution is a pretty well not new distribution.

289
00:28:02,800 --> 00:28:05,800
We're going to call it the tractable posterior distribution.

290
00:28:05,800 --> 00:28:16,800
These samples from xT minus 1, less noisy image, condition on noisy image, xT, and clean image at time step 0.

291
00:28:16,800 --> 00:28:29,800
So it's kind of like you have clean image, you have noisy image, and you want to predict what would be the less noisy variation of this image if you knew what was the starting point, what was the x0 for generating this noisy image.

292
00:28:29,800 --> 00:28:37,800
It turns out that this distribution is also very simple because the forward process is just Gaussian distribution.

293
00:28:37,800 --> 00:28:42,800
The posterior distribution here is also very simple.

294
00:28:42,800 --> 00:28:47,800
This is a pretty good posterior distribution, it's condition on x0. We know what is the starting point.

295
00:28:47,800 --> 00:28:54,800
So this distribution is also normal distribution with mean expressed here.

296
00:28:54,800 --> 00:29:00,800
This mean is just simply weighted average of the clean image and the noisy image at xT.

297
00:29:00,800 --> 00:29:04,800
So it's actually very interesting if you have clean image and you have a noisy image.

298
00:29:04,800 --> 00:29:14,800
If you want to predict what is the distribution of xT minus 1, the mean that this will be normal, this expression is just normal.

299
00:29:14,800 --> 00:29:23,800
And the mean of that normal is just the weighted average of these two images where the weights come from our diffusion process, parametric diffusion process.

300
00:29:23,800 --> 00:29:32,800
Very simple expression and the variance of this distribution is also defined based on the parameters of the forward process.

301
00:29:32,800 --> 00:29:41,800
So you can think of it like this beta tilde t can be computed very easily from the parameters of the diffusion process.

302
00:29:41,800 --> 00:29:44,800
So that we know none of this here, it's interesting.

303
00:29:44,800 --> 00:29:50,800
It's interesting, so we have the scale divergence from this distribution trackable posterior distribution, which is normal.

304
00:29:50,800 --> 00:29:54,800
And then this denoising model, which we assume that is normal as well.

305
00:29:54,800 --> 00:29:59,800
So, now that we have two normal distributions, the scale divergence there.

306
00:29:59,800 --> 00:30:03,800
So I'm just writing down the same scale divergence again.

307
00:30:03,800 --> 00:30:09,800
The scale divergence can be computed analytically for two normal distributions.

308
00:30:09,800 --> 00:30:23,800
We can show that the scale divergence simply just boils down to the squared L2 distance between the mean of this trackable posterior and the mean of the denoising model.

309
00:30:23,800 --> 00:30:30,800
Right, plus some constant terms that we can ignore these constant terms do not depend on any trainable power.

310
00:30:30,800 --> 00:30:34,800
So this just basically scale divergence is very interesting.

311
00:30:34,800 --> 00:30:45,800
It just boils down to the difference between the mean of this denoising, sorry, this mean of the trackable posterior and our denoising model, parametric denoising model, which is represented by mean theta.

312
00:30:45,800 --> 00:30:48,800
And this weight is one over two sigma t squared.

313
00:30:48,800 --> 00:30:52,800
It's just the variance used in the denoising distribution.

314
00:30:52,800 --> 00:30:54,800
So you can ignore for a moment this coefficient.

315
00:30:54,800 --> 00:30:58,800
So we're going to focus on these two terms.

316
00:30:58,800 --> 00:31:08,800
Recall that if you want to generate xt, if you want to generate a sample at times the t, you can use this parameterization trick that we discussed earlier.

317
00:31:08,800 --> 00:31:27,800
And in this paper hall et al. in New York 2020, they observed that you can also express the mean of the trackable posterior distribution I discussed as xt, the input noise image minus the noise that was used to generate that data.

318
00:31:28,800 --> 00:31:39,800
To get this expression, it's very simple, you just need to do some arithmetic operations on this equation and just plug the definition of xt from the parameterization trick.

319
00:31:39,800 --> 00:31:50,800
With some arithmetic operation, you will see that if you basically have a noisy image and you want to predict the less noisy version, right?

320
00:31:50,800 --> 00:31:57,800
If you knew the noise that was used to generate that noisy image, you can just subtract some re-scaled version.

321
00:31:57,800 --> 00:31:59,800
So this is the scaled version of those.

322
00:31:59,800 --> 00:32:08,800
So you can take noise, subtract just some scaled version of that noise from xt to get mean of xt minus one.

323
00:32:08,800 --> 00:32:10,800
This is kind of very interesting information.

324
00:32:10,800 --> 00:32:20,800
So you basically can represent this mean in a very simple form, expression of xt and epsilon, the noise that was used to generate xt.

325
00:32:20,800 --> 00:32:24,800
So this actually is the same noise that was used to generate xt.

326
00:32:24,800 --> 00:32:33,800
Knowing that knowledge, it means that now if we want to parameterize this network, we can use this knowledge in the parameterization of this model.

327
00:32:33,800 --> 00:32:48,800
So we can say that in order to predict this mean of less noisy images, we're going to just take xt and subtract it from a neural network that predicts the noise that was used to generate this xt.

328
00:32:48,800 --> 00:32:57,800
So basically we train a neural network to predict the noise that was used in order to generate xt in order to represent this noisy model.

329
00:32:57,800 --> 00:33:00,800
This is just a parameterization trick.

330
00:33:00,800 --> 00:33:10,800
Instead of just representing the mean of the noisy model directly, what we can do is that we can just represent the noise that was used to generate xt.

331
00:33:10,800 --> 00:33:16,800
And if you have this, you can just subtract this from xt in order to get the mean of the denoising model.

332
00:33:16,800 --> 00:33:22,800
So if you assume this parameterization for the denoising model.

333
00:33:22,800 --> 00:33:26,800
And now we also know that this is true for mu theta t.

334
00:33:26,800 --> 00:33:33,800
If you plot these two expressions into this, you're going to actually get a very simple expression here.

335
00:33:33,800 --> 00:33:46,800
So we have this lt minus one, this is the same term, lt minus one becomes just you need to draw samples from training data distribution, you draw some noise vector from standard normal distribution.

336
00:33:46,800 --> 00:33:53,800
And using this noise vector, you just generate this xt using the few samples using the parameterization trick.

337
00:33:53,800 --> 00:34:05,800
You can now pass this if you sample to your epsilon prediction network, the network that is trained to predict that the noise that was used in order to generate xt.

338
00:34:05,800 --> 00:34:09,800
So it's basically a very simple algorithm.

339
00:34:09,800 --> 00:34:16,800
You draw samples from data distribution, you draw noise, you generate xt from that noise and input data.

340
00:34:16,800 --> 00:34:23,800
And you train a network to predict the noise that was used in order to generate that xt.

341
00:34:23,800 --> 00:34:29,800
And these weights here are just some scalar parameters that comes from this basically one over two sigma t here.

342
00:34:29,800 --> 00:34:41,800
And this is like one over square root of beta t. And these terms that we know, we can compute very easily based on the parameters of the diffusion process.

343
00:34:41,800 --> 00:34:48,800
So you can ignore them for a moment. Think of them as a scalar parameters that we can compute from the diffusion parameters.

344
00:34:48,800 --> 00:34:53,800
So xt minus one can be represented as this weighted objective.

345
00:34:53,800 --> 00:34:58,800
So we're going to just summarize these weights as lambda t. We're going to define a new scalar lambda t.

346
00:34:58,800 --> 00:35:04,800
This lambda t ensures that your training objective is weighted properly for maximum data likelihood training.

347
00:35:04,800 --> 00:35:10,800
So by using this lambda t weights, you're actually maximizing data likelihood.

348
00:35:10,800 --> 00:35:18,800
But however what happens is that this lambda t ends up being very large for small t's and it is small for large t's.

349
00:35:18,800 --> 00:35:25,800
So it kind of monotically decreases for small t's is very high and for large t's is very small.

350
00:35:25,800 --> 00:35:30,800
And this is basically how the maximum data likelihood training is formulated.

351
00:35:30,800 --> 00:35:45,800
However, in this paper by Hoh et al. in Europe's 2020, they observed that if you simply drop these weights or equally just if you simply set these weights to one and train the model using this on the weighted version.

352
00:35:45,800 --> 00:35:51,800
Like if you drop this way, you're going to get very high quality sample generation using diffusion models.

353
00:35:51,800 --> 00:35:57,800
So they introduced this very simple objective that does not have this weighting anymore.

354
00:35:57,800 --> 00:35:59,800
This weighting is one.

355
00:35:59,800 --> 00:36:09,800
So again, this objective simply draws samples from data distribution, draws white noise and randomly samples from one of the time steps from one to capital t.

356
00:36:09,800 --> 00:36:17,800
It generates a diffused sample using the parameterization trick and it trains a model to predict a noise injected.

357
00:36:17,800 --> 00:36:23,800
So it's exactly the same objective without any weighting and it can be done very easily.

358
00:36:23,800 --> 00:36:32,800
And the answer is true that actually with this weighting, you will get very high quality sample generation using diffusion models.

359
00:36:32,800 --> 00:36:40,800
This objective weighting actually plays a key role in getting high quality sample generation in diffusion models.

360
00:36:40,800 --> 00:36:57,800
So if you're interested in checking this area, I would encourage you to check this paper by Hoh et al. published at CUQ 2022 that discuss how you can potentially change this weighting over time to get even better high quality images from diffusion models.

361
00:36:57,800 --> 00:37:04,800
So let's summarize the training and sampling from diffusion models and what we've learned so far.

362
00:37:04,800 --> 00:37:08,800
In order to train diffusion models, the algorithm is extremely simple.

363
00:37:08,800 --> 00:37:11,800
You draw a batch of samples from your training data distribution.

364
00:37:11,800 --> 00:37:15,800
You uniformly sample from these time steps from one to capital t.

365
00:37:15,800 --> 00:37:20,800
You draw some random noise that has same dimensionality as your input data.

366
00:37:20,800 --> 00:37:26,800
And you use the parameterization trick to generate sample at time step t.

367
00:37:26,800 --> 00:37:36,800
You give the sample to your noise prediction network and you train this noise prediction network to predict the noise that was used to generate that diffuse sample.

368
00:37:36,800 --> 00:37:44,800
And to train this you just simply use squared L2 loss to train this noise prediction network.

369
00:37:44,800 --> 00:37:52,800
After training, if you like to draw samples from your diffusion model, you can use the reverse diffusion process to generate data.

370
00:37:52,800 --> 00:37:55,800
So we're going to start from last step x capital t.

371
00:37:55,800 --> 00:37:59,800
We're going to draw samples from standard normal distribution.

372
00:37:59,800 --> 00:38:07,800
And then here we have a full look that walks back in diffusion process starting on capital t all the way to t equals to 1.

373
00:38:07,800 --> 00:38:12,800
At every step we just draw white noise z from standard normal distribution.

374
00:38:12,800 --> 00:38:16,800
Here we're forming the mean of the nosing model.

375
00:38:16,800 --> 00:38:20,800
Remember this is the parameterization we use for the nosing model.

376
00:38:20,800 --> 00:38:29,800
And then we add noise rescaled with the standard deviation of the nosing minus sigma t to generate xt minus 1.

377
00:38:29,800 --> 00:38:34,800
And we can repeat this t times in order to generate x0.

378
00:38:34,800 --> 00:38:40,800
So very simple training and very simple generation process.

379
00:38:41,800 --> 00:38:44,800
So far we talked about training and sampling.

380
00:38:44,800 --> 00:38:50,800
So let's talk about the implementation details of how to form neural networks for the nosing model.

381
00:38:50,800 --> 00:38:57,800
In practice, most diffusion models use unit architecture to represent the nosing model.

382
00:38:57,800 --> 00:39:00,800
This unit architecture often has residual blocks.

383
00:39:01,800 --> 00:39:06,800
So here different rectangles represent residual blocks at different scales.

384
00:39:06,800 --> 00:39:10,800
And these residual blocks often have also self-attention layers in them.

385
00:39:10,800 --> 00:39:15,800
In some layers usually produce self-attention layers.

386
00:39:15,800 --> 00:39:27,800
Remember this unit takes this diffused image, diffused peanut, xt, and it predicts the noise that was used in order to generate this diffuse image.

387
00:39:27,800 --> 00:39:34,800
So this epsilon prediction will be trained to produce the predicted noise that was used to generate this xt.

388
00:39:34,800 --> 00:39:36,800
This network is also conditional time.

389
00:39:36,800 --> 00:39:38,800
It's shared across different time steps.

390
00:39:38,800 --> 00:39:42,800
So it also takes time using some time embedding.

391
00:39:42,800 --> 00:39:54,800
This time embedding can be done using, for example, sinusoidal positional embeddings that are often used in transformers or random free features to represent this time embedding.

392
00:39:54,800 --> 00:40:01,800
This time embedding will be a vector that will be fed to a small fully connected network.

393
00:40:01,800 --> 00:40:06,800
A network consists of a few fully connected layers to access some time representation.

394
00:40:06,800 --> 00:40:10,800
And this time representation is usually fed to all the residual blocks.

395
00:40:10,800 --> 00:40:15,800
In order to combine this time embedding with all the residual blocks, you have a few options.

396
00:40:15,800 --> 00:40:23,800
For example, you can just take this time embedding and do arithmetic sum with all the spatial features in the residual blocks.

397
00:40:23,800 --> 00:40:30,800
Or you can use, for example, adaptive group normalization in order to do this, like to add time embedding into residual blocks.

398
00:40:30,800 --> 00:40:40,800
So I would encourage you to check this paper that discuss fruits and trades between adaptive group normalization and spatial recognition.

399
00:40:40,800 --> 00:40:48,800
So, so far we talked about forward process, reverse process, training, as well as the network's design for diffusion models.

400
00:40:48,800 --> 00:41:03,800
Let's also talk about some of these hyperparameters that we have in diffusion models, mostly beta T schedule, the variance of the forward process and the variance used in the reverse process, sigma T square.

401
00:41:03,800 --> 00:41:20,800
So, one common assumption is that we can, most papers follow Jonathan Hobot of Newripp's 2020 paper, where they use just simply beta T's that are defined using just a linear function.

402
00:41:20,800 --> 00:41:28,800
Just these beta T's are small values and they gradually go to some larger value through some linear schedule.

403
00:41:28,800 --> 00:41:41,800
And it's also common to assume that sigma T square is just equal, set equal to beta T. This works also really well in practice, especially when the number of diffusions steps is large.

404
00:41:41,800 --> 00:41:48,800
But you may ask me, how can I train these palms? Is there any way I can also train beta T and sigma T square?

405
00:41:48,800 --> 00:42:01,800
So, there are a couple of papers that discuss this. One of them is this paper by Kimba Tao at Notives 2022. This paper introduces a new parameterization diffusion model using a concept called signal to noise ratio.

406
00:42:01,800 --> 00:42:10,800
And they show how you can actually train this noise schedule using some training objective. So, they actually propose a method for training these beta T values.

407
00:42:10,800 --> 00:42:18,800
There are also a couple of papers that discuss how you can train sigma T square, the variance of the reverse process.

408
00:42:18,800 --> 00:42:28,800
This first paper here shows how you can use a variational bond that we use for training diffusion models to also train sigma T, the variance of the denuzin models.

409
00:42:28,800 --> 00:42:38,800
And there's only paper here, analytically P.M. by Bobo et al. in ICELER 2022. This paper actually got outstanding paper award this year at ICELER.

410
00:42:38,800 --> 00:42:55,800
And they showed how you can actually post-training, after training your diffusion models, how you can use this to compute the variance of the denuzin model analytically post-training.

411
00:42:55,800 --> 00:43:13,800
So, so far, we talked about how we can train and how we can also pick up these hyperparameters and diffusion models. Let's look at the diffusion process and look into what happens to, for example, images in the forward process.

412
00:43:13,800 --> 00:43:24,800
We call it, in order to sample from time step T, we can use this diffusion channel, and we can use this parameterization trick to generate XT from input image X0.

413
00:43:24,800 --> 00:43:37,800
Here, XT, we know this diffuse sample, in order to analyze what happens to the image, what we're going to do, we're going to use Fourier transform to convert XT to the frequency domain.

414
00:43:37,800 --> 00:43:46,800
So this FXT, you can think of just Fourier transform applied to XT, it's just a representation of the image in the signal, in the frequency domain.

415
00:43:46,800 --> 00:44:03,800
And we know from Fourier transform that this XT can be just represented as Fourier transform of input image, plus Fourier transform of the noise, some together with some weights corresponds to the ways that we use actually in this parameterization trick.

416
00:44:03,800 --> 00:44:08,800
This is just simple rules in Fourier transform.

417
00:44:08,800 --> 00:44:19,800
You should have in mind that most images actually in the frequency domain have a very high response for low frequency, and they have very low response for high frequency content.

418
00:44:19,800 --> 00:44:22,800
And this is because most images are very smooth.

419
00:44:23,800 --> 00:44:36,800
In general, even if they're not super smooth, when you apply Fourier transform to them, you see usually that most images have very high concentration in low frequency, and their high frequency response is very low.

420
00:44:36,800 --> 00:44:39,800
This is very common in most images.

421
00:44:39,800 --> 00:44:53,800
One thing you should also know that if you have a white noise or Gaussian noise and you apply Fourier transform on top of it, in the frequency domain, actually this Gaussian noise can be also represented as just Gaussian noise in the frequency domain as well.

422
00:44:53,800 --> 00:45:00,800
So Fourier transform of a Gaussian noise is itself Gaussian noise, which we can use now here for analysis.

423
00:45:00,800 --> 00:45:11,800
So remember for the small t's, alpha bar t is almost one. So as a result, we actually did the perturbation we apply is very small in the, in the frequency domain as well.

424
00:45:11,800 --> 00:45:26,800
So in frequency domain, because most of our input signal for input image is concentrated at the small t's, and because alpha bar is almost one, we actually don't perturb the low frequency content of the image that most.

425
00:45:26,800 --> 00:45:34,800
And we mostly perturb when we kind of wash out this high frequency content of the image for small t's.

426
00:45:34,800 --> 00:45:54,800
And then for large t's, because alpha bar t, this coefficient here is almost zero. So what happens is that you now push down all the frequency content, so you also push down the low frequency response of the image, and you wash away all the kind of frequency content of the image.

427
00:45:54,800 --> 00:46:08,800
This basically shows that there's kind of a trade off in the forward process. In the forward process what happens is that the high frequency content is perturbed faster than the low frequency content.

428
00:46:08,800 --> 00:46:15,800
So at small t's, most of the low frequency content is not perturbed, it's mostly the high frequency content that is being perturbed.

429
00:46:15,800 --> 00:46:23,800
But eventually at the end of process is a time when we also completely get rid of the low frequency content of the image.

430
00:46:24,800 --> 00:46:42,800
This is very important to also understand what happens in the reverse process, right, so because there's kind of trade off between content and detail, you can think of low frequency response is the main content of image and the high frequency response is just detail in that generation.

431
00:46:43,800 --> 00:47:01,800
These diffuser model kind of trades off between these in different steps, so you can think of when you're training a generative model, the reverse denoising model, for in the large teams, your denoising models becoming specialized at generating low frequency content of the image.

432
00:47:01,800 --> 00:47:06,800
So it's mostly the course content of the image is being generated large t's.

433
00:47:06,800 --> 00:47:14,800
In a small t's, then your denoising model is becoming specialized in generating the high frequency content of the image.

434
00:47:14,800 --> 00:47:21,800
So most of the low level details are generated in the low, low t's, the small t's.

435
00:47:21,800 --> 00:47:36,800
This is also why the weighting of training objective becomes important, right, so because you have a model that is shared in different time steps and this model is responsible for generating course content versus low level details.

436
00:47:36,800 --> 00:47:52,800
By reweighting this training objective, now we can kind of keep balance between how much we want to generate this course content that is visually very appealing usually, versus how much we want to generate the high frequency because that usually we ignore, we cannot necessarily observe them.

437
00:47:52,800 --> 00:48:00,800
And the weighting plays a key role in keeping this trade off, you're balancing this trade off.

438
00:48:01,800 --> 00:48:13,800
So, so far I talked about the fusion was in general, now let's talk about the connections between the fusion was and the VAEs, especially hierarchical VAEs.

439
00:48:13,800 --> 00:48:21,800
In hierarchical VAEs, which one of the examples can be like any VAE work I did a few years ago.

440
00:48:21,800 --> 00:48:31,800
In hierarchical VAEs, we have this deterministic path, you can think of just a resonant that generates data at x at the end, this is just a generative model.

441
00:48:31,800 --> 00:48:50,800
In hierarchical VAEs we usually sample from noise and we inject to this deterministic path and then we go to second group sample from second group condition on the first group, and the after generating this noise we feed it to the, to the deterministic path and we keep doing this, we're just walking down in the hierarchical model.

442
00:48:50,800 --> 00:49:02,800
So the fusion models can be considered as hierarchical models as well, where these diffuse steps are just latent variables in this hierarchical model, the condition dependencies of course different.

443
00:49:02,800 --> 00:49:08,800
And here we're going to discuss like what are the main differences between the fusion was and hierarchical VAEs.

444
00:49:08,800 --> 00:49:27,800
One major difference is that the encoder in VAEs is often trained, whereas encoder, which is the forward diffusion in diffusion models is fixed we're not training the forward diffusion which is using a fixed diffusion process as encode.

445
00:49:27,800 --> 00:49:37,800
That's one major difference. The second difference is that latent variables in hierarchical VAEs can have a different shape and different dimensionality compared to input images.

446
00:49:37,800 --> 00:49:46,800
Whereas in diffusion models we assume that all the intermediate variables have the same dimension as input data.

447
00:49:47,800 --> 00:49:57,800
The third major difference is that in diffusion models, if you think of the noisy model as a generative model, this is actually shared across different steps.

448
00:49:57,800 --> 00:50:06,800
Whereas in hierarchical VAEs, we actually don't make an assumption, we don't share any component in this hierarchical structure, usually.

449
00:50:06,800 --> 00:50:21,800
In hierarchical VAEs, we usually train these models using variational bond, whereas when we're training diffusion models, we're using some different rebating of variational bond in order to drive the training objective of diffusion.

450
00:50:21,800 --> 00:50:32,800
So even though these two are related, they're not exactly the same. There are some trade-offs that occur when we are rebating the variational bond.

451
00:50:32,800 --> 00:50:39,800
So this brings me to the last slide. So in this part, I reviewed the noise and diffusion probabilistic models.

452
00:50:39,800 --> 00:50:53,800
I showed how these models are just simply trained by sampling from forward diffusion process and training a noisy model that simply predicts the noise that was used in order to generate diffuse samples.

453
00:50:53,800 --> 00:51:04,800
We discussed these models from different perspectives. We saw what happens to data as you go in the, what happens to images as you go forward in the diffusion process.

454
00:51:04,800 --> 00:51:14,800
We also discussed what happens to data distribution in the forward process. We saw how data distribution becomes smoother and smoother in forward diffusion.

455
00:51:14,800 --> 00:51:31,800
But of course, like any other deep learning framework, the devil is in the details, the network architecture, objective rebating, or even diffusion parameters play a key role in getting good high-quality results with diffusion models.

456
00:51:31,800 --> 00:51:41,800
So if you're interested in knowing more about important design decisions that actually play a role in getting good diffusion models, I would encourage you to check this paper by other colleagues,

457
00:51:41,800 --> 00:52:01,800
called Elucidating the Design Space of Diffusion-Based Genetic Models by Karas Etal. And this paper discusses important design decisions and how they play a role in getting good diffusion models.

458
00:52:01,800 --> 00:52:17,800
So with that in mind, I'd like to pass the mic and with you to my dear friend and colleague Karsten to talk about score-based genetic modeling with differential courses.

459
00:52:17,800 --> 00:52:27,800
Hello everyone, I'm Karsten, and I will now talk about score-based genetic modeling with differential equations.

460
00:52:27,800 --> 00:52:34,800
In order to get started, let us actually consider the diffusion process that Arash already introduced in his part.

461
00:52:34,800 --> 00:52:41,800
This diffusion process is defined through this Gaussian transition kernel of this form.

462
00:52:41,800 --> 00:52:48,800
But now let us consider the limit of many, many small steps, and each step being very, very tiny.

463
00:52:48,800 --> 00:52:54,800
So how does sampling from this diffusion process and in practice look like?

464
00:52:54,800 --> 00:53:11,800
So from this Gaussian transition kernel, we can just do essentially the parametrization trick, and we take the xt minus one, we scale it down by this one minus beta t square width term, and we add a little bit of noise from the standard normal distribution,

465
00:53:11,800 --> 00:53:15,800
scaled by this square width beta t term.

466
00:53:15,800 --> 00:53:28,800
With beta t, we can actually interpret it as a step size essentially, so if beta t is zero, nothing happens, this term drops out, and also no rescaling of xt minus one happens.

467
00:53:28,800 --> 00:53:37,800
So let's make this a little bit more explicit and write beta t as this delta t times this function beta of t.

468
00:53:37,800 --> 00:53:50,800
So beta t is explicitly our step size, and beta of t is now this time dependent function that allows us to have different step sizes along the diffusion process t.

469
00:53:50,800 --> 00:53:57,800
And now in this limit of many, many small tiny steps, it is delta t that goes to zero.

470
00:53:57,800 --> 00:54:11,800
If delta t goes towards zero or tiny, we can actually tailor expand this square width expression here and obtain this equation at the bottom.

471
00:54:11,800 --> 00:54:13,800
I just copied that over here.

472
00:54:13,800 --> 00:54:17,800
And it turns out that this equation has a particular form.

473
00:54:17,800 --> 00:54:29,800
We can interpret this as some iterative update, like the new xt is given by the old xt plus some term that depends on xt itself.

474
00:54:29,800 --> 00:54:33,800
So this is just a small correction and some noise added.

475
00:54:33,800 --> 00:54:48,800
It turns out that this iterative update will correspond to a certain solution or a certain discretization of a stochastic differential equation, and in particular this stochastic differential equation.

476
00:54:48,800 --> 00:55:03,800
If I wanted to iteratively numerically solve this stochastic differential equation, for instance with an Euler-Maruama solver, then this is exactly the iterative scheme I would end up with.

477
00:55:03,800 --> 00:55:05,800
Let us not get ahead of ourselves.

478
00:55:05,800 --> 00:55:11,800
I'm not sure if everybody who's listening here is an expert in differential equations.

479
00:55:11,800 --> 00:55:22,800
So let us do a one-slide crash course in differential equations, and let us start with ordinary differential equations, which are a little bit simpler than stochastic ones.

480
00:55:22,800 --> 00:55:27,800
Here is an ordinary differential equation that can be written in that form.

481
00:55:27,800 --> 00:55:34,800
So this is now the state that we're interested in. This code, for example, the value of a pixel in an image.

482
00:55:34,800 --> 00:55:42,800
And t is some continuous time variable that captures the time along which this state changes or evolves.

483
00:55:42,800 --> 00:55:51,800
And ultimately one is often interested in the evolution of this state x or this pixel value x of t.

484
00:55:51,800 --> 00:56:04,800
And that is not what we're given an ordinary differential equation. What we've given is an expression for the time derivative of dx to dt in the form of this function f.

485
00:56:04,800 --> 00:56:08,800
So this code, for instance, will be a neural network.

486
00:56:08,800 --> 00:56:10,800
So what does this mean?

487
00:56:10,800 --> 00:56:15,800
This f essentially describes not x itself, but the change of x.

488
00:56:15,800 --> 00:56:19,800
So if you now look here in this graph at the bottom.

489
00:56:19,800 --> 00:56:28,800
So for point x, for a given time t, this f of x now describes the change. So in other words, we could look.

490
00:56:28,800 --> 00:56:32,800
So this f essentially corresponds to an arrow in this graph.

491
00:56:32,800 --> 00:56:39,800
And if I now wanted to get x of t, I would just follow the arrows in this thing here.

492
00:56:39,800 --> 00:56:48,800
So basically you just have to integrate up this differential equation following the arrows to get my final expression x of t.

493
00:56:48,800 --> 00:56:50,800
And that's what I would have to do.

494
00:56:50,800 --> 00:57:09,800
However, in practice, this f is often like highly complex nonlinear function, for instance, like a neural network, and solving this integral here analytically and following these field lines exactly is often not possible.

495
00:57:09,800 --> 00:57:16,800
In fact, one can solve this whole thing iteratively numerically in a stepwise fashion.

496
00:57:16,800 --> 00:57:28,800
So in that case, when we add some point x, we evaluate our neural network or our, well, our nonlinear function f, or function f, yeah, at this x and time t.

497
00:57:28,800 --> 00:57:35,800
And then we do like a small linear step in this direction and access to our old state x.

498
00:57:35,800 --> 00:57:41,800
Continue doing that again evaluate f and again update and so on and so forth.

499
00:57:41,800 --> 00:57:48,800
So we have an approximation essentially to this analytical solution.

500
00:57:48,800 --> 00:57:54,800
So, yeah, and now there are also stochastic differential equations.

501
00:57:54,800 --> 00:58:06,800
And once a little bit more complex, these stochastic differential equations now have an additional term system sigma of x and t times this term omega.

502
00:58:06,800 --> 00:58:08,800
So what is all this.

503
00:58:08,800 --> 00:58:12,800
First of all, Omega, this is called a Vina process.

504
00:58:12,800 --> 00:58:16,800
And what this is in practice is really just Gaussian white noise.

505
00:58:16,800 --> 00:58:26,800
What this means is that our DX of t or ODE equation now has this additional term, which is corresponds to noise injection.

506
00:58:26,800 --> 00:58:33,800
And this noise is scaled by this standard deviation term essentially sigma of x t.

507
00:58:33,800 --> 00:58:36,800
And this has a name, this is a diffusion coefficient.

508
00:58:36,800 --> 00:58:43,800
And in that context also the other term system here is called the drift coefficient.

509
00:58:43,800 --> 00:58:50,800
And we can see that these equations are sometimes written like this explicit form with derivative here.

510
00:58:50,800 --> 00:58:59,800
And sometimes also like this is written on the other side and it becomes a bit of a special expression for the Vina process.

511
00:58:59,800 --> 00:59:09,800
Sometimes this differential equations or stochastic differential equations here also written like this, but this essentially means the same thing for the sake of this talk.

512
00:59:09,800 --> 00:59:21,800
And importantly, keep in mind this omega of t is essentially a Gaussian random variables and strong independent for each team looks like this.

513
00:59:21,800 --> 00:59:27,800
So how does this now look like if I want to solve this stochastic differential equation.

514
00:59:27,800 --> 00:59:34,800
So for example, numerically, so it's a little bit similar to the iterative solution we had here.

515
00:59:34,800 --> 00:59:43,800
So if I'm given a stage, I, I first updated corresponding to my updated corresponding to the drift coefficient, I evaluate that.

516
00:59:43,800 --> 00:59:46,800
So update a little bit with that direction.

517
00:59:46,800 --> 01:00:01,800
But then I also evaluate the diffusion coefficient and add a little bit of noise that is proportional the strength of the noise is proportional to the diffusion coefficient, and additionally also to the to the time stamp.

518
01:00:01,800 --> 01:00:05,800
So if I do this, so each time I do this I made more different noise variables.

519
01:00:05,800 --> 01:00:13,800
So this means there is not a unique solution like there was in the ordinary differential equation case, but there is a lot of noise injected.

520
01:00:13,800 --> 01:00:18,800
So if I do this multiple times, I may get slightly different trajectories.

521
01:00:18,800 --> 01:00:26,800
So over all these trajectories still approximately follow this deterministic F function, but we have additional noise injection.

522
01:00:26,800 --> 01:00:29,800
So, yeah, this is how it may look like.

523
01:00:29,800 --> 01:00:39,800
So I may ask, is there now also like an analytical framework to instead, you know, describe this analytically like it was for the ordinary differential equation case.

524
01:00:39,800 --> 01:00:43,800
So there is, but this is a little bit more involved.

525
01:00:43,800 --> 01:00:56,800
Because now we are not talking about one deterministic solution that we need to describe rather given one state in each at the beginning, we now have a probability distribution over possible states where we could land.

526
01:00:56,800 --> 01:01:06,800
So the definition of these probability distribution that is described by the Fokker-Planck equation, but that is beyond the scope of this tutorial here.

527
01:01:06,800 --> 01:01:16,800
Anyway, I think now you should also have some intuitions for not only ordinary differential equations, but also stochastic differential equations.

528
01:01:16,800 --> 01:01:27,800
Now we can go back to our stochastic differential equation that we had here, and that actually describes the forward diffusion process of diffusion models.

529
01:01:27,800 --> 01:01:38,800
So this is again just copy over the equation from the last slide here at the bottom. And this is now a visualization of how this whole thing looks like in practice more or less.

530
01:01:38,800 --> 01:01:52,800
So let's go through that one by one. On the left hand side, we have some distribution here of x zero, this might be defined through an empirical data distribution or here we have this one dimensional toy distribution.

531
01:01:52,800 --> 01:02:00,800
In a more realistic setting this may represent a distribution of images like images of these cats here.

532
01:02:00,800 --> 01:02:11,800
So now if we simulate this stochastic differential equation forward, we get this green trajectories and they evolve towards this standard normal prior distribution.

533
01:02:11,800 --> 01:02:17,800
And yeah, the images come progressively noisier and noisier as we do this.

534
01:02:17,800 --> 01:02:27,800
So let's also look at the form of this equation and it's kind of intuitive. We see that this is updates or in the DX.

535
01:02:27,800 --> 01:02:37,800
This update direction, it's actually proportional to the negative of the state x we're in. So this means if we have a large pixel value for instance.

536
01:02:37,800 --> 01:02:49,800
It will pull us to go back towards zero. So direction is always is a negative direction corresponding to my, our x.

537
01:02:49,800 --> 01:02:59,800
On the other hand, while all our states are being pulled towards zero as I've just explained, and at the same time we're also injecting noise.

538
01:02:59,800 --> 01:03:07,800
So this makes it intuitive that after a while of simulating this whole process, we end up with this distribution.

539
01:03:07,800 --> 01:03:23,800
It means that every single point basically completely converts itself to just plain noise where, yeah, with mean zero and certain variance.

540
01:03:24,800 --> 01:03:29,800
Here's another animation of that.

541
01:03:29,800 --> 01:03:44,800
Note that throughout this talk, we will make a lot of use of this image of this cat washes cat peanut. We do hope that it will become a little bit famous after this talk, but let's see how that goes.

542
01:03:44,800 --> 01:03:56,800
Right. So, yeah, we have this forward diffusion sd with a drift term and diffusion term, one of them pulls towards the mode of the distribution the other one injects noise.

543
01:03:56,800 --> 01:04:09,800
It may be worth mentioning that in the diffusion model it which are also other differential equations have been used to define other types of diffusion processes, often just take a more general form like this.

544
01:04:09,800 --> 01:04:22,800
Yeah, I don't want to go into too much detail and in this talk we will stick to this equation for simplicity, but all the concepts that we tried in this tutorial also hold for other types of SDS.

545
01:04:22,800 --> 01:04:33,800
The only thing that is important is that these drift kernels and fusion terms here these are basically linear function of X.

546
01:04:33,800 --> 01:04:43,800
Otherwise, we couldn't solve for the probability distributions here, which reminds me the background this where this background in these.

547
01:04:43,800 --> 01:04:57,800
In these pictures here these animations, this actually defines the marginal different probability distribution of the few data, which is your multimodal, and then he becomes union model.

548
01:04:57,800 --> 01:05:06,800
So great, we have not talked about the forward diffusion process, but what about the reverse direction. So, which is necessary for generation.

549
01:05:06,800 --> 01:05:12,800
So, can we also have like some differential equation that is quite sad.

550
01:05:12,800 --> 01:05:14,800
It turns out yes.

551
01:05:14,800 --> 01:05:25,800
There is this result described by an Anderson 1982 and then used in young songs like your paper last year.

552
01:05:25,800 --> 01:05:42,800
So if there is a forward differential equation of this form for STD, then there is a corresponding reverse stochastic differential equation that once in the other direction, but tracks the exact same probability distribution.

553
01:05:42,800 --> 01:05:49,800
So, really like the reverse direction, which generates data from noise, not noise from data.

554
01:05:49,800 --> 01:05:55,800
So, and since we first generate the fusion STD, it looks like this.

555
01:05:55,800 --> 01:05:58,800
And again has drift term and a diffusion term.

556
01:05:58,800 --> 01:06:08,800
It's a diffusion term and the drift term look over all somewhat similar. So the diffusion term is the same thing. So this will be a process that again injects noise.

557
01:06:08,800 --> 01:06:12,800
This drift term also has this same.

558
01:06:13,800 --> 01:06:18,800
X minus one half p to t of X term, but then there was this additional term.

559
01:06:18,800 --> 01:06:21,800
This is this red term and that is very important.

560
01:06:21,800 --> 01:06:31,800
So this is the gradient of the logarithm of the marginal used density probability density of the data.

561
01:06:31,800 --> 01:06:34,800
And this is known as a score function.

562
01:06:35,800 --> 01:06:49,800
So this means, if you now had access to this object here like this score function, we could do data generation from random noise by just sampling or simulating this reverse diffusion STD.

563
01:06:49,800 --> 01:06:57,800
So this in practice and what look like this, like I said, this reverse diffusion STD.

564
01:06:57,800 --> 01:07:09,800
It's really a diffusion process running in the other direction. And so, yeah, simulating this is generative modeling essentially.

565
01:07:09,800 --> 01:07:15,800
So, are we done. So, we can simulate this and generate data.

566
01:07:15,800 --> 01:07:18,800
Well, not quite.

567
01:07:18,800 --> 01:07:25,800
The question is, how do we actually get this score function.

568
01:07:25,800 --> 01:07:35,800
We may have the naive idea, why not learn a neural network for the score function to approximate it, and then we can take this new network, we call this new network as data.

569
01:07:36,800 --> 01:07:41,800
And then we can take this network inside it and our reverse diffusion STD.

570
01:07:41,800 --> 01:07:45,800
And, yeah, so we can simulate it and generate data.

571
01:07:45,800 --> 01:07:52,800
Something we could do, for instance, is draw a diffusion time along this diffusion process.

572
01:07:52,800 --> 01:08:04,800
Now take like data, refused data from this point in the diffusion process samples is QFT of XT.

573
01:08:04,800 --> 01:08:16,800
So we take a neural network that takes us as input, maybe we also additionally give up the time and train this, maybe with some simple square two term, which we minimize.

574
01:08:16,800 --> 01:08:25,800
We train it to predict this score of this diffuse data, this marginal score to diffuse data.

575
01:08:25,800 --> 01:08:40,800
Well, great idea, but unfortunately that doesn't work, because the score of this marginal diffuse data is not tractable or more explicitly this diffuse data density itself QT of XT is not tractable.

576
01:08:40,800 --> 01:08:54,800
We don't have an analytic expression for that, which makes it to a different sense because, well, if we had we could just put NT for zero and get our data distribution that this is what we're interested in modeling in the first place.

577
01:08:55,800 --> 01:08:57,800
So, too bad.

578
01:08:57,800 --> 01:09:07,800
But what we can do is something different, which is now known as denoising score matching. So what we had on the previous slide was just general score matching.

579
01:09:07,800 --> 01:09:17,800
So what we can do is, instead of considering this marginal distribution QFT given X, which corresponds to the full diffuse density.

580
01:09:17,800 --> 01:09:34,800
Let us consider like individual data points X zero and diffuse those. So instead, we consider the conditional density QFT of XT given X zero that distribution actually is tractable.

581
01:09:34,800 --> 01:09:42,800
So that's why it's preserving stochastic differential equation, which is precisely this SDE that we find our forward diffusion process.

582
01:09:42,800 --> 01:10:03,800
For that case, this conditional density QFT of XT given one particular data point X zero, it has this expression, this form. So it is this normal distribution, where the mean is given by the initial point X zero that is now simply scaled down by some gamma of T, which

583
01:10:03,800 --> 01:10:17,800
is a function that starts at one and be cased towards zero. And then we also have some variance, which is the other way around, which starts at zero and grows towards one.

584
01:10:17,800 --> 01:10:27,800
So we can define all denoising score matching objective. So, again, we have, we draw a diffusion time T, we have an expectation over those.

585
01:10:27,800 --> 01:10:40,800
Then we draw a data sample X zero, one particular sample. Overall, we again have an expectation. Then we diffuse that particular data sample.

586
01:10:40,800 --> 01:11:07,800
And now we train it to predict the score of this one particular diffused data sample X zero. And yeah, now this is tractable, you know this is just a normal distribution so we can take the logarithm of this density of this expression for the density and also calculate the gradient.

587
01:11:07,800 --> 01:11:27,800
Great. So, and now there's one very beautiful result. So after this expectation over the data, when we consider that, it turns out that this neural network then will still learn to approximate the score of the marginal data of the marginal diffuse

588
01:11:27,800 --> 01:11:41,800
distribution, which is exactly what we need. And this sort of makes intuitively sense, because this x t that we're feeding to the neural network, this could corresponds to this is noisy right noisy data.

589
01:11:41,800 --> 01:11:57,800
So this could corresponds to many possible different x zeros and many different possible like grant true scores that we we regressed was on practice neural network has to kind of average over those.

590
01:11:57,800 --> 01:12:13,800
And it turns out that, yeah, after this averaging considering the expectation, this neural network will still model the marginal as the score of the marginal diffuse data distribution. And this is exactly what we need.

591
01:12:13,800 --> 01:12:20,800
What we need in our decision model, or more specifically in the reverse genitive SDE for generation.

592
01:12:20,800 --> 01:12:24,800
So, this is great.

593
01:12:24,800 --> 01:12:32,800
So, yeah, in practice diffusion modeling basically boils down to learning this score function.

594
01:12:32,800 --> 01:12:40,800
Let us now talk about a few implementation details here and what people do in practice because that's also crucial.

595
01:12:40,800 --> 01:12:44,800
This is again just copied the denoising score matching formula.

596
01:12:44,800 --> 01:12:58,800
So, how do we actually sample these diffused data points. And so this is really just with time and just sampling again from this normal distribution here so we have gamma times our input x zero.

597
01:12:58,800 --> 01:13:05,800
And then we add noise to us that is scaled by the standard deviation sigma t.

598
01:13:05,800 --> 01:13:17,800
We explicitly write down the star function, how it now looks like. So it's a gradient of the logarithm of this conditional distribution is the given x zero.

599
01:13:17,800 --> 01:13:24,800
Oh, this is a normal distribution. So this is basically some exponential times some stuff.

600
01:13:24,800 --> 01:13:36,800
We have the logarithm and exponential drop out. There's also a, by the way, a normalization constant, but this does not depend on x so it's not important for the derivative.

601
01:13:36,800 --> 01:13:43,800
Anyway, so you advise at this time. So now we can take the gradient of system here.

602
01:13:44,800 --> 01:13:54,800
Now here for x t. This is like the, the diffuse data point that we sampled, we can actually insert this expression here.

603
01:13:54,800 --> 01:14:06,800
Then we get that, but now it turns out all these terms they cancel out this gamma t x zeroes, and also one of those signals, and what we left with a step.

604
01:14:06,800 --> 01:14:25,800
It's interesting, because this means that the score function. It's basically just the, the noise values that we introduced doing the we permit rest sampling of our diffused data, like, minus that noise value and scale with the inverse standard deviation from the fusion.

605
01:14:25,800 --> 01:14:28,800
But still, this is cool.

606
01:14:28,800 --> 01:14:36,800
So, this maybe also suggests us how we should choose our neural network and how we should parameterize this.

607
01:14:36,800 --> 01:14:53,800
More specifically, for instance, we can take this new network and define it by like some, yeah, some other neural networks times minus one and divided by the standard deviation, which is inspired by this salt here.

608
01:14:53,800 --> 01:15:06,800
So if we insert both of the expression for the ground to a score, which is really just this noise value, and also this neural network parameterization, what we left with is this objective here at the bottom.

609
01:15:06,800 --> 01:15:25,800
No, this is interesting. So this means, if I choose this parameterization, our neural network epsilon that amount is basically tasked with predicting noise values epsilon, which are really just the noise values that were used to perturb our data.

610
01:15:25,800 --> 01:15:30,800
This also makes it kind of intuitive for this is called denoising score matching.

611
01:15:30,800 --> 01:15:47,800
Because if our neural network can be nice, can predict those noise values that were used for perturbation, then yeah, we can be nice and reconstruct the original data point x zero.

612
01:15:47,800 --> 01:15:51,800
So there's another implementation detail here.

613
01:15:51,800 --> 01:16:03,800
So, I have kind of arbitrarily motivated that we can use this squared to turn to perform, you know, I think score matching and to regress this function.

614
01:16:03,800 --> 01:16:11,800
And to give different weights to this L2 term to this L2 loss for different points along the diffusion process.

615
01:16:11,800 --> 01:16:21,800
Keep in mind that this is one neural network that just as input gets a noisy state and tea, it's the same neural network for all teeth along the diffusion process.

616
01:16:21,800 --> 01:16:37,800
So, maybe we want to specialize in that with a little bit more for large times along the diffusion process of small times or something like this, and give like different weights to this objective for different times along the diffusion process.

617
01:16:37,800 --> 01:16:39,800
And this is a loss weight in a lot of tea.

618
01:16:39,800 --> 01:16:44,800
So we introduce this loss rating lambda t that the busses.

619
01:16:44,800 --> 01:16:56,800
And it turns out that different loss ratings trade off between like models is different good perceptual quality, like the images and look pretty that we can generate the set and sharp.

620
01:16:56,800 --> 01:17:11,800
And this is no high log likelihood. For instance, if we choose a number of teeth to be exactly this variance of the forward diffusion process here to cancel out the variants in the denominator.

621
01:17:11,800 --> 01:17:19,800
Then this is just an objective that is actually that leads to good high quality perceptual quality outputs.

622
01:17:19,800 --> 01:17:34,800
And if we choose for lambda, for instance, beta of T, which is hyper parameter of the forward diffusion process, then this whole objective that we have corresponds to training our model towards maximum log likelihood.

623
01:17:34,800 --> 01:17:40,800
More specifically, it's kind of a negative elbow.

624
01:17:40,800 --> 01:17:52,800
This is interesting. And it turns out that, yeah, this is exactly the same objectives that we derived with the variational approach and part one presented by our.

625
01:17:52,800 --> 01:18:04,800
And yeah, so this means that there are some deep connections between this variation derivation and actually like score matching and noising score matching in particular.

626
01:18:04,800 --> 01:18:11,800
I would also like to point out that there are like much more sophisticated model parameterizations and loss breaking possible.

627
01:18:11,800 --> 01:18:21,800
I would in particular refer you to this recent paper by Tero Carlos at all, who discusses in quite some detail.

628
01:18:21,800 --> 01:18:26,800
There's another implementation detail I would like to talk about.

629
01:18:26,800 --> 01:18:40,800
So, we know that this variance sigma T squared and of this forward diffusion process for like using individual data points that actually goes to zero as T goes to zero.

630
01:18:40,800 --> 01:18:50,800
But this means that if I sample the T here close to zero, then this loss might be heavily amplified when sampling. Yeah, T close to zero.

631
01:18:50,800 --> 01:18:58,800
And that's for the case when we do not choose lambda already in function to cancel out the sigma spread.

632
01:18:58,800 --> 01:19:11,800
So for some of these for these reasons we sometimes see some tricks and implementations where we train the small time cut off so we prevent sampling teeth that are extremely close to zero.

633
01:19:11,800 --> 01:19:26,800
So in a mental way how this can be fixed, like I said, this is especially relevant training models towards high block likelihood versus lambda T function maybe something like beta of T and the sigma squared is not cancelled out.

634
01:19:26,800 --> 01:19:34,800
In that case, we can perform important sampling with respect to this, yeah, weight of this loss.

635
01:19:34,800 --> 01:19:40,800
So we have an oversampled teeth close to zero and yeah.

636
01:19:40,800 --> 01:19:49,800
So the objective then looks like this. So we oversample small teeth according to the important sample distribution that has most of its weight or small T.

637
01:19:49,800 --> 01:19:58,800
And then we weigh down the confusion of those to the overall loss and with one over our teeth and constant distribution.

638
01:19:58,800 --> 01:20:04,800
And I don't want to go into too much detail but this is a technique you see in several papers.

639
01:20:04,800 --> 01:20:07,800
So here's a visualization of what happens.

640
01:20:07,800 --> 01:20:19,800
So this is not a loss value. The wet is the loss value without an important sampling here. So, yeah, if I sample T close to zero then I have this heavily amplified loss values.

641
01:20:19,800 --> 01:20:27,800
But with important sampling the blue line, the variance is significantly reduced.

642
01:20:27,800 --> 01:20:33,800
Before moving on, it makes sense to briefly recapitulate what we have been doing so far.

643
01:20:33,800 --> 01:20:53,800
So we have been introducing this diffusion modeling framework based on continuous times now, in contrast to the first part presented by Arash, where each diffusion step had a finite size and we overall had a finite number of discrete forward fixed diffusion process steps and also denoising steps.

644
01:20:53,800 --> 01:21:04,800
In this section we have considered a continuous time. This allowed us to introduce this differential equation framework and also to make these connections to score matching.

645
01:21:04,800 --> 01:21:14,800
But it is important to keep in mind that we are still describing the same types of diffusion models in this section. We are just using different tools at a different framework.

646
01:21:14,800 --> 01:21:22,800
So this is important to realise after all we obtained the same objectives as we have seen on the last few slides.

647
01:21:22,800 --> 01:21:30,800
But bear in mind, let us now move on and we will talk now about the probability flow ordinary differential equation.

648
01:21:30,800 --> 01:21:39,800
However, let us first consider the reverse generative diffusion SDE again, that one, so object you have already seen so far.

649
01:21:39,800 --> 01:22:01,800
With this generative reverse diffusion SDE, we can, when sampling random noise from this standard normal prior distribution, we can generate data and more specifically we basically can sample data all along the diffused data distribution, the reddish curves here, the reddish contours here.

650
01:22:02,800 --> 01:22:14,800
It turns out there is an ordinary differential equation called the probability flow ODE that is in distribution equivalent to this reverse generative diffusion SDE.

651
01:22:14,800 --> 01:22:20,800
It will become clear in a minute what exactly I mean with in distribution equivalent.

652
01:22:20,800 --> 01:22:24,800
Let us first have a look at this ODE itself. It is written down here.

653
01:22:24,800 --> 01:22:37,800
In contrast to the generative diffusion SDE, it doesn't have the noise term and also the score function term, which we will then later learn with the neural network, it doesn't have this factor of two in front anymore.

654
01:22:37,800 --> 01:22:41,800
So what do I mean with in distribution equivalent?

655
01:22:41,800 --> 01:23:00,800
When I sample many initial noise values from this standard normal distribution at initialization when I want to generate the data, then simulating all these samples on backwards versus probability flow ODE towards the data.

656
01:23:00,800 --> 01:23:12,800
By doing that, we will sample from the exact same probability distribution, like with the generative diffusion SDE, with the only difference that we don't have this noise anymore.

657
01:23:12,800 --> 01:23:17,800
So how does it look like more specifically? We can see this on that slide.

658
01:23:17,800 --> 01:23:27,800
Here again, we have the probability flow ODE, just that we now have inserted the learned score function as a pattern for the score function.

659
01:23:27,800 --> 01:23:32,800
And now these trajectories defined by this ODE, they look like this.

660
01:23:32,800 --> 01:23:43,800
So we see that when we sample from this standard normal prior distribution on the right, these trajectories they will all flow into the modes of the data distribution.

661
01:23:43,800 --> 01:23:48,800
We see this also at these bluish lines here at the background.

662
01:23:48,800 --> 01:23:54,800
Yeah, so the probability quite literally flows into the modes of the data distribution.

663
01:23:54,800 --> 01:23:59,800
And that's called the probability flow ODE.

664
01:23:59,800 --> 01:24:02,800
Here we have an animation how it looks like.

665
01:24:02,800 --> 01:24:11,800
And I think at this point it should really become clear what I meant with they are the same in distribution and they sample the same distribution.

666
01:24:11,800 --> 01:24:18,800
On the left hand side, we have the SDE that we have that I have introduced earlier already SDE framework.

667
01:24:18,800 --> 01:24:27,800
And we see that these trajectories are zigzagging, I have this noise injection, but I'm still landing at the modes of the data distribution.

668
01:24:27,800 --> 01:24:45,800
Why for the ODE formulation, I now have these pretty like, not exactly straight but more deterministic trajectories that still land in the modes of the data distribution when I initialize some randomly from this prior distribution.

669
01:24:45,800 --> 01:24:54,800
And the visual trajectory on the right is deterministic while this is stochastic.

670
01:24:54,800 --> 01:25:05,800
So this probability flow ordinary differential equation, this is actually an instance of a neural ordinary differential equations which a while ago generated a lot of attention in the literature.

671
01:25:05,800 --> 01:25:12,800
More specifically, we can even see this as a continuous normalizing flow.

672
01:25:12,800 --> 01:25:18,800
So, why should we care, why should we use this probability flow ODE framework.

673
01:25:18,800 --> 01:25:27,800
It turns out that this ordinary differential equation framework that allows the use of advanced ordinary differential equation solvers.

674
01:25:27,800 --> 01:25:34,800
It is somewhat easier to work with ordinary differential equations than with stochastic differential equation.

675
01:25:34,800 --> 01:25:41,800
And there really is a broad literature on how to quickly and very efficiently solve ordinary differential equation.

676
01:25:41,800 --> 01:25:45,800
So we can build on top of this literature here.

677
01:25:45,800 --> 01:25:47,800
But there are more advantages.

678
01:25:47,800 --> 01:26:02,800
This ordinary differential equation, I can run this in both directions, I can run it as generation, where I go from the right to the left, where I sample the noise from a prior distribution and then go to the left to generate data.

679
01:26:02,800 --> 01:26:16,800
But similarly, given a data point, I can also run the probability flow ODE in the other direction and encode this data point in the latent space of this diffusion model, this prior space.

680
01:26:16,800 --> 01:26:17,800
So this is interesting.

681
01:26:17,800 --> 01:26:24,800
And yeah, this allows for interesting applications, for instance, or semantic image interpolation.

682
01:26:24,800 --> 01:26:31,800
And to make clear what I mean with that, let's look at this slide here.

683
01:26:31,800 --> 01:26:39,800
What I'm doing here is I have drawn two noise values, or let's look first at the lower left.

684
01:26:39,800 --> 01:26:45,800
So here we are drawing two noise values in the latent space of the diffusion model and this noise space.

685
01:26:45,800 --> 01:26:50,800
And now I can linear interpolate these noise values in this space.

686
01:26:50,800 --> 01:27:06,800
However, the model was trained in such a way that every sample under this noise distribution, so also every sample along this linear interpolation path between those noise values decodes to a coherent realistic image.

687
01:27:06,800 --> 01:27:16,800
So when I then interpolate, it means that this results in continuous semantically meaningful changes in the data space, right?

688
01:27:16,800 --> 01:27:23,800
And keep in mind, we could not just interpolate directly linearly in pixel space, this would not be meaningful.

689
01:27:23,800 --> 01:27:32,800
But we can do that in noise space and then obtain semantically meaningful interpolations in the pixel space like this.

690
01:27:32,800 --> 01:27:40,800
But because this ODE is so complex, right, under the hood, this means that we will sometimes have some jumps between nodes and such like this.

691
01:27:40,800 --> 01:27:43,800
And we also see this in this animation here.

692
01:27:43,800 --> 01:27:51,800
So in this animation at the top, yeah, we have been doing many of such interpolations one after another.

693
01:27:51,800 --> 01:27:58,800
And yeah, sometimes you see like little jumps, this basically corresponds to that.

694
01:27:58,800 --> 01:28:07,800
So all this is only possible due to this deterministic encoding and decoding path with the probability flow ODE.

695
01:28:07,800 --> 01:28:15,800
I think it's clear that you couldn't do this so easily with a stochastic trajectory.

696
01:28:15,800 --> 01:28:17,800
All right.

697
01:28:17,800 --> 01:28:23,800
So there is another advantage of the probability flow ordinary differential equation.

698
01:28:23,800 --> 01:28:30,800
We can also use it for block likelihood computation as in continuous normalizing flows.

699
01:28:30,800 --> 01:28:39,800
More specifically, we can take a given image or a given data sample, for instance, this image of Arash's cat peanut.

700
01:28:39,800 --> 01:28:46,800
Now we can take peanut and encode peanut in the latent space of our diffusion model.

701
01:28:47,800 --> 01:28:55,800
Now we can calculate the probability of peanuts and coding under the prior distribution of our diffusion model.

702
01:28:55,800 --> 01:29:08,800
And additionally, we take into account this using this instantaneous change of variables formula kind of the distortion of the ODE the volume change along the ODE trajectory.

703
01:29:08,800 --> 01:29:16,800
So the probability of our data sample, in our case, the image of peanut is then given by that expression.

704
01:29:16,800 --> 01:29:24,800
So we're really just using the tricks from the continuous normalizing flow literature here.

705
01:29:24,800 --> 01:29:35,800
What all this means is actually that in their probability flow ODE formulation, diffusion models can also be considered as continuous normalizing flows.

706
01:29:35,800 --> 01:29:42,800
However, in contrast to continuous normalizing flows, we train diffusion models with score matching.

707
01:29:42,800 --> 01:29:51,800
Continuous normalizing flows themselves are usually trained directly with this objective to maximize the likelihood of the data.

708
01:29:51,800 --> 01:30:04,800
However, training with such this objective directly is actually a hard task because for each training iteration, I have to simulate the whole trajectory here and back propagate it through it.

709
01:30:04,800 --> 01:30:09,800
On the other hand, this diffusion model training relies on score matching.

710
01:30:09,800 --> 01:30:14,800
And as we have seen earlier, score matching works quite differently in score matching.

711
01:30:14,800 --> 01:30:20,800
We basically have like we can train for all these different times along the diffusion process separately.

712
01:30:20,800 --> 01:30:24,800
This leads to a much more scalable and robust learning objective.

713
01:30:24,800 --> 01:30:33,800
And yeah, this makes diffusion models very scalable in contrast to these normalizing flows, I would argue.

714
01:30:34,800 --> 01:30:41,800
So I have not talked a lot about these differential equations, derive these, derive them and so on and so forth.

715
01:30:41,800 --> 01:30:46,800
However, how should we actually solve these SDS and ODE in practice?

716
01:30:46,800 --> 01:31:00,800
We have already seen that we can probably not solve this analytically because these SDS and ODE are defined with very complex nonlinear functions, namely these neural networks that approximate the score function.

717
01:31:00,800 --> 01:31:03,800
So let us look at that.

718
01:31:03,800 --> 01:31:07,800
So let's start with the generative diffusion SDE.

719
01:31:07,800 --> 01:31:12,800
So the most naive way to do this is to use Euler-Mariouama sampling.

720
01:31:12,800 --> 01:31:19,800
We have already briefly talked about Euler-Mariouama sampling in this earlier one slide crash course on differential equations.

721
01:31:19,800 --> 01:31:32,800
What we do in that case is we simply evaluate our function here for different for our state t and x, then we propagate for like a small time step delta t.

722
01:31:32,800 --> 01:31:38,800
And we additionally add a little bit of noise, which is also scaled by the time step.

723
01:31:38,800 --> 01:31:48,800
And yeah, so we do this then iteratively one after another given a new step we evaluate again and then add a little bit of noise and so on and so forth.

724
01:31:48,800 --> 01:32:05,800
By the way, as a small comment here, and you may wonder about the sign flip from here to here, this is because our dt is actually negative because we're running from like large time values to small time values and this delta t here is not supposed to be like an absolute step size.

725
01:32:05,800 --> 01:32:08,800
So it's positive.

726
01:32:08,800 --> 01:32:24,800
So this runs out also this enchant to a sampling that I wash talked about in the first part of our tutorial, and most specifically the way he showed us how we can, how we can sample from these discrete time diffusion models.

727
01:32:24,800 --> 01:32:33,800
And this, this can actually be also considered a generative SDE sampler with this particular discretization used in that part.

728
01:32:33,800 --> 01:32:40,800
So let's look at the probability flow ODE. How can we generate that, or using that.

729
01:32:40,800 --> 01:32:49,800
Again, we could basically use Euler's method, which is analogous to the Euler-Maiorama approach and just now without this noise injection.

730
01:32:49,800 --> 01:32:59,800
We would just intuitively evaluate our network and, yeah, the ODE function essentially do a small step, linear step for small time delta t.

731
01:32:59,800 --> 01:33:03,800
We evaluate and continue doing that.

732
01:33:03,800 --> 01:33:08,800
However, this is usually, I think nobody really does this in practice.

733
01:33:08,800 --> 01:33:21,800
In practice, we can hear, as I mentioned earlier, really build on the advanced ordinary differential equation literature and use much better solvers and much better methods and higher order methods in particular.

734
01:33:21,800 --> 01:33:30,800
So what we see for instance is the use of one cutter methods of linear multi stepping methods, exponential integrators.

735
01:33:30,800 --> 01:33:34,800
Yes, there was a lot of literature in that direction.

736
01:33:34,800 --> 01:33:47,800
Yeah, like I just said, adaptive step size when you put a method that's been used, also called the stochastic differential equation actually adaptive step size higher order methods have been used.

737
01:33:47,800 --> 01:33:53,800
We parameterize the ODE has been proposed that also accelerates sampling.

738
01:33:53,800 --> 01:33:58,800
So, yeah, there's a lot of literature in that direction.

739
01:33:58,800 --> 01:34:12,800
And the main reason is that one drawback of fusion models is that sampling from them can be slow and contrast to like sampling from a generative adversarial network or variation auto encoder and such methods for instance, sampling from a differential

740
01:34:12,800 --> 01:34:31,800
model requires many, many function calls on what are specifically neural network evaluations in our case, because during each step of this iterative denoising, we have to call this neural network again so we often have to call it many, many times.

741
01:34:31,800 --> 01:34:43,800
And yeah, so this is why we want to use efficient solvers so that we can reduce this number of neural network evaluations that we have to use.

742
01:34:44,800 --> 01:35:04,800
So now I have talked about how you can use how you can solve the SDS and the ODE and practice, but what should you use, should you actually rather build on the SD or the ODE framework when you want to sample from the model.

743
01:35:05,800 --> 01:35:12,800
So to shine some light into that, let us look at the generative diffusion SD a little bit closer.

744
01:35:12,800 --> 01:35:14,800
So it's like that.

745
01:35:14,800 --> 01:35:18,800
But now we can actually decompose this into two terms.

746
01:35:18,800 --> 01:35:21,800
Right, so this is just from here to here.

747
01:35:21,800 --> 01:35:32,800
And it turns out the first term is really just the probability flow ODE that we have seen already, which itself can be used for deterministic data generation like you're on the right.

748
01:35:32,800 --> 01:35:35,800
But then there is this additional term.

749
01:35:36,800 --> 01:35:40,800
So this code basically corresponds to the noise injection.

750
01:35:40,800 --> 01:35:43,800
And yeah, it has the noise injection here.

751
01:35:43,800 --> 01:35:46,800
So what what do these terms do.

752
01:35:46,800 --> 01:35:54,800
So this probability flow ODE term is essentially responsible for your pushing us from the right to the left here.

753
01:35:54,800 --> 01:36:05,800
And this logical diffusion SD term, what it basically does is for each individual team, it actively pushes us towards correct diffuse data distribution.

754
01:36:05,800 --> 01:36:15,800
But because of this, so when I do ours during my soul during my simulation going from the right to left you're going from noise to data.

755
01:36:15,800 --> 01:36:34,800
If I have ever said, then this logical diffusion SD can help us to correct these errors and actively bring us back to the right data manifold back to the fused data distribution.

756
01:36:34,800 --> 01:36:39,800
So this, yeah, this is an advantage can do some sort of error correction.

757
01:36:39,800 --> 01:36:49,800
On the other hand, it's, it's often slower because this term itself requires a somewhat fine discretization during the solve.

758
01:36:49,800 --> 01:36:51,800
Yeah.

759
01:36:51,800 --> 01:36:54,800
So now let's look at the probability flow ODE.

760
01:36:54,800 --> 01:36:58,800
So in that case, we do not have this SD term.

761
01:36:59,800 --> 01:37:05,800
However, we can now leverage these really fast ODE solvers.

762
01:37:05,800 --> 01:37:09,800
And so this is good when we target very fast sampling.

763
01:37:09,800 --> 01:37:14,800
On the other hand, there is no stochastic error correction going on here.

764
01:37:14,800 --> 01:37:21,800
And because of this, what we see in practice is that this is sometimes slightly lower performing than the stochastic sampling.

765
01:37:21,800 --> 01:37:24,800
When we just look at the quality of the samples.

766
01:37:24,800 --> 01:37:42,800
So to summarize what we see is, if we're not concerned about our budget of like neural network evaluations and we're willing to do like very a lot of steps, then this SDE framework can be very useful.

767
01:37:42,800 --> 01:37:51,800
But if we want to go as fast as possible, then probably the ODE framework is better where we then can leverage these really fast solvers.

768
01:37:51,800 --> 01:37:58,800
It is also worth mentioning that we can do things in between where we have only like this logic in diffusion SDE.

769
01:37:58,800 --> 01:38:00,800
Active a little bit.

770
01:38:00,800 --> 01:38:11,800
We can also, you know, like kind of solve for the first half using stochastic sampling and then afterwards switch to the ODE advanced methods are possible.

771
01:38:11,800 --> 01:38:20,800
I would like to refer you to this paper here with which discusses some of these things in quite some detail.

772
01:38:20,800 --> 01:38:26,800
Next, I would like to talk about a connection between diffusion models and energy based models.

773
01:38:26,800 --> 01:38:30,800
So what are energy based models?

774
01:38:30,800 --> 01:38:34,800
Energy based models are defined like this in an energy based model.

775
01:38:34,800 --> 01:38:47,800
The probability distribution that we want to model the setter of x defined through an exponential to the power of minus a scalar energy function, which is now the function of the data.

776
01:38:48,800 --> 01:38:55,800
And then this thing is normalized by a normalization constant to make sure it's a well defined probability distribution.

777
01:38:55,800 --> 01:39:00,800
This normalization constant is also called sometimes called the partition function.

778
01:39:00,800 --> 01:39:13,800
Furthermore, in this case, I have added a time variable t because this energy based model is now supposed to represent the diffuse data distributions for different t's in our case.

779
01:39:14,800 --> 01:39:24,800
When we want to sample an energy based model, we usually do that by larger than dynamics, which is if we have seen larger than dynamics already.

780
01:39:24,800 --> 01:39:31,800
Basically, this is very closely connected to these stochastic differential equations we have already discussed.

781
01:39:31,800 --> 01:39:40,800
So to do this larger than dynamics sampling, we basically require the gradient of this scalar energy function.

782
01:39:40,800 --> 01:39:44,800
And then we also iteratively update our sample with that.

783
01:39:44,800 --> 01:39:53,800
And we also have like an additional noise term atter and some step size of learning weight atter here.

784
01:39:53,800 --> 01:40:04,800
The important part to realize is when we do when we use these energy based models is that in practice, even though we are learning the scalar energy function.

785
01:40:04,800 --> 01:40:14,800
We only require the gradient of this energy function or more specifically the negative gradient of this energy function for sampling the model at the end.

786
01:40:14,800 --> 01:40:21,800
We do not require the energy function itself, nor do we require the partition function depth setter.

787
01:40:21,800 --> 01:40:29,800
By the way, this atter is implicitly defined through this energy itself that we're learning.

788
01:40:29,800 --> 01:40:39,800
So now it turns out that in diffusion models, what we're basically learning is the energy gradients for all these diffuse data distributions directly.

789
01:40:39,800 --> 01:40:43,800
We are not learning energies, but basically energy gradients.

790
01:40:43,800 --> 01:40:58,800
And one thing I want to add is that because in this EDM spirit, we're directly learning these energies and we have the probability, an expression for the probability distribution while also taking into account this partition function.

791
01:40:58,800 --> 01:41:03,800
Because of this training energy based models can be actually really complex.

792
01:41:03,800 --> 01:41:09,800
This often requires advanced Markov chain Monte Carlo methods, which can be very difficult to deal with.

793
01:41:10,800 --> 01:41:13,800
One of that is available, I would add.

794
01:41:13,800 --> 01:41:20,800
But yeah, diffusion models, we kind of circumvent that and we only directly learn these energy gradients.

795
01:41:20,800 --> 01:41:24,800
Tell me somehow show that and derive that maybe.

796
01:41:24,800 --> 01:41:43,800
So to this end, let's recall again that in diffusion models, our neural network basically we're trying to approximate our model more generally, we're trying to approximate this score function of the diffuse data distributions qt of x.

797
01:41:43,800 --> 01:41:54,800
Now let us suppose that our model is parametrized such that the diffuse data distributions qt are given by this energy based model here.

798
01:41:54,800 --> 01:41:55,800
Right.

799
01:41:55,800 --> 01:42:01,800
So now let us insert this p theta here and yeah through the map.

800
01:42:01,800 --> 01:42:09,800
We apply the logarithm both here, both on e to the minus the scalar energy function and also the denominator.

801
01:42:09,800 --> 01:42:15,800
However, this term drops out because the partition function does not depend on the state x.

802
01:42:15,800 --> 01:42:20,800
And what we are left with is just a negative gradient of the energy.

803
01:42:20,800 --> 01:42:21,800
What does this mean?

804
01:42:21,800 --> 01:42:28,800
So this means that this neural network s that we usually have in diffusion models to model the score function.

805
01:42:28,800 --> 01:42:42,800
It means that it essentially learns the negative energy gradients of the energy model based model that would describe the diffuse data distribution.

806
01:42:42,800 --> 01:43:00,800
So yeah, once again, fusion models kind of circumvent these complications and directly model the energy gradients and say avoid modeling this partition function explicitly for instance which leads to some of these difficulties that we have in classical energy based model training.

807
01:43:00,800 --> 01:43:12,800
Also, these different noise levels that we have in diffusion models. This is actually analogous to a mere sampling and energy based models.

808
01:43:12,800 --> 01:43:19,800
I would like to talk about one more thing about diffusion models, which is unique identity.

809
01:43:19,800 --> 01:43:33,800
It turns out that the denoising model that we're learning in these diffusion models that is supposed to approximate the score function of the diffused data to t of x p.

810
01:43:33,800 --> 01:43:42,800
This denoising model is in principle uniquely determined by the data that we're given and the forward diffusion process.

811
01:43:43,800 --> 01:43:55,800
And not only the score model, so and by learning the score model also these data encodings that we obtain by using the probability flow of the e to deterministically encode data in the latency space.

812
01:43:55,800 --> 01:44:00,800
All this is uniquely determined by the data and the forward diffusion.

813
01:44:00,800 --> 01:44:25,800
What this means is that even if we use different neural network architectures for us and different network initialization, we should at the end recover identical model outputs like identical score function outputs and data encodings in the latency space assuming we have sufficient training data model capacity and optimization accuracy.

814
01:44:26,800 --> 01:44:34,800
This is in contrast, for instance, to generate adversarial networks or variational auto encoders which do not have this property.

815
01:44:34,800 --> 01:44:46,800
Because these models, depending on what kind of architectures we use and what kind of initializations we use, we will always obtain like somewhat different models and yet different data encodings and so on.

816
01:44:46,800 --> 01:44:51,800
This is the unique property about these diffusion models.

817
01:44:51,800 --> 01:45:07,800
Here's an example. What we are seeing here is the first 100 dimensions of the latent code obtained from a random cyber tent image that was encoded in the latency space of a diffusion model using this probability flow or the e approach.

818
01:45:07,800 --> 01:45:17,800
We did this, most specifically Song and I did this with two different models and model architectures that were separately trained.

819
01:45:17,800 --> 01:45:32,800
However, both of these encodings distributions here as we see, they are almost the same, they are almost identical, even though these were different architectures.

820
01:45:32,800 --> 01:45:37,800
With that, I would like to come to a conclusion and briefly summarize.

821
01:45:37,800 --> 01:45:46,800
So in this part of this talk, I have introduced you to this continuous time diffusion framework in contrast to what Arash talked about in step one.

822
01:45:46,800 --> 01:45:53,800
We do not have finite size denoising and diffusion steps anymore and only a finite number of cells.

823
01:45:53,800 --> 01:46:04,800
Rather, we consider continuous perturbations, a continuous forward diffusion process and then also a continuous generative process based on differential equations.

824
01:46:04,800 --> 01:46:11,800
And to train these models, we make connections to score matching, most specifically denoising score matching.

825
01:46:11,800 --> 01:46:16,800
Now, maybe this appeared somewhat complex and mathematically involved.

826
01:46:16,800 --> 01:46:23,800
However, why should he use this differential equation and continuous time framework?

827
01:46:23,800 --> 01:46:31,800
It really has unique advantages as I have shown hopefully and hopefully I can convince you during this part of the talk.

828
01:46:31,800 --> 01:46:41,800
Most importantly, this allows us to leverage this broad existing literature on advanced and fast SCE and ODE solvers when sampling from the model.

829
01:46:41,800 --> 01:46:49,800
This can help us to really accelerate sampling from diffusion models, which is very crucial because they can be slow.

830
01:46:49,800 --> 01:47:06,800
Furthermore, in particular, this probability flow ODE is very useful because it allows us to also perform like these deterministic data encodings and it also allows us to do like local ideal estimation like in continuous normalizing flows and so on.

831
01:47:06,800 --> 01:47:14,800
Additionally, this is overall a fairly clean mathematical framework based on diffusion processes, score matching and so on.

832
01:47:14,800 --> 01:47:29,800
And this allowed us to use these connections to neural ordinary differential equations to continuous normalizing flows and to energy based models, which I think provides a lot of insights into diffusion modeling.

833
01:47:29,800 --> 01:47:42,800
With that, I would like to conclude my part and take the mic to Wicci who will now talk about advanced techniques, accelerated sampling, conditional generation and beyond.

834
01:47:42,800 --> 01:47:45,800
Thank you very much.

835
01:47:45,800 --> 01:47:51,800
Hi everyone. I'm Richie from Google Brain Team. So let's continue our study on diffusion models.

836
01:47:51,800 --> 01:48:02,800
So in the third part, we're going to discuss several advanced techniques of diffusion models which corresponds to accelerating sampling, conditional generation and beyond.

837
01:48:02,800 --> 01:48:11,800
So here's an outline of what we're going to cover in this part. Basically want to address two important questions of diffusion models with advanced techniques.

838
01:48:11,800 --> 01:48:28,800
The first one is how to accelerate the sampling process of diffusion models. We're going to tackle this question from three aspects, advanced forward process, advanced reverse process and advanced modeling, including hybrid models and model distillation.

839
01:48:28,800 --> 01:48:34,800
The second question is how to do a high resolution, optionally conditional generation.

840
01:48:34,800 --> 01:48:50,800
And I will talk about several important techniques to make this happen, especially the general conditional diffusion modeling framework, the classifier classifier free guidance, as well as cascade generation pipeline.

841
01:48:50,800 --> 01:48:56,800
Let's start from the first question, so how to accelerate the sampling process of diffusion models.

842
01:48:56,800 --> 01:49:07,800
To see why this question is important, let's consider what makes a good generative model. So in principle, we want a good generative model to enjoy the pulling three properties.

843
01:49:07,800 --> 01:49:22,800
First, it should be fast to sample from this generative model. Second, we would expect the general model capture most of the major modes of the data distribution, or in other words, they should have adequate sample diversity.

844
01:49:22,800 --> 01:49:28,800
And third, of course, we want the general model to give us high quality or high fidelity samples.

845
01:49:28,800 --> 01:49:44,800
However, there is a generative learning dilemma for existing generative model frameworks. For example, for generating several networks, they are usually fast to sample from, and they can give us high quality samples.

846
01:49:44,800 --> 01:49:55,800
However, because of this discriminative learning framework, there is a decent chance that GANS may miss certain modes of the data distribution.

847
01:49:55,800 --> 01:50:04,800
And the other type of generative models are these likelihood based models, like variational autoencoders or normalizing flows.

848
01:50:04,800 --> 01:50:14,800
So those models are usually optimized by maximizing likelihood or maximizing a variant of likelihood, for example, the evidence lower bound.

849
01:50:14,800 --> 01:50:26,800
So usually this type of models are good at fast sampling, and they are able to capture certain modes of the data distribution because of this maximum likelihood learning framework.

850
01:50:26,800 --> 01:50:31,800
However, usually they lead to subpar sample quality.

851
01:50:31,800 --> 01:50:44,800
On the other hand, the diffusion models are good at both coverage because they are also optimizing the evidence lower bound of log likelihoods, and they are able to generate high quality samples.

852
01:50:44,800 --> 01:50:54,800
However, the sampling of diffusion models is pretty slow, which usually requires thousands of functional calls before getting a simple batch of samples.

853
01:50:54,800 --> 01:51:06,800
So if we can find techniques to accelerate the sampling process of diffusion models, we will get a generative model framework, which enjoys all those three great properties.

854
01:51:06,800 --> 01:51:12,800
That is, we can tackle the dilemma of this generative learning framework.

855
01:51:12,800 --> 01:51:20,800
Before I do that, before diving into details, I would like to recap the general formulation of diffusion models.

856
01:51:20,800 --> 01:51:32,800
So for diffusion models, they usually define a simple forward process which slowly maps data to noise by repeatedly adding noise to the images.

857
01:51:32,800 --> 01:51:43,800
The forward process is defined to map data, or to map noise back to data, and this is where the diffusion model is defined and trained.

858
01:51:43,800 --> 01:52:01,800
In terms of the diffusion model, it is usually parameterized by a unit architecture, which takes the noisy inputs at certain time step, and then it tries to predict the clean samples, or it tries to predict the noise added to this noisy inputs.

859
01:52:02,800 --> 01:52:14,800
So if we think about accelerating sampling, there are some naive methods that immediately come to our mind. For example, in training, we can reduce the number of diffusion time steps.

860
01:52:14,800 --> 01:52:34,800
In sampling, we sample every K time step instead of going over the whole reverse process. However, those naive acceleration methods will lead to immediate voice performance of diffusion models in terms of both the sample quality as well as the likelihood estimations.

861
01:52:34,800 --> 01:52:41,800
So we really need something clever, cleverer than those naive methods.

862
01:52:41,800 --> 01:52:54,800
And more precisely, we want to ask, given a limited number of functional calls, which are usually much less than thousands, so how to improve the performance of diffusion models.

863
01:52:54,800 --> 01:53:10,800
And as a side note, although the following techniques I'm going to discuss take this accelerated sampling as the main motivation, they definitely provide more insights and contributions to diffusion models as we will see soon.

864
01:53:10,800 --> 01:53:26,800
So we will answer this question from three aspects. The first one is advanced forward process, and second is advanced reverse process, and then lastly the advanced diffusion models.

865
01:53:26,800 --> 01:53:48,800
So first let's take a look at some advanced forward process. So recall that the original whole process defines a Markov process, where we start from this x zero, it is the clean sample, and we gradually add noise until it goes to this x big T corresponding to white noise signal.

866
01:53:48,800 --> 01:54:02,800
So QXT, QXT minus one is simply a Gaussian distribution, and this beta T defines the noise schedule, and they are hyper parameters that are predefined before training the models.

867
01:54:02,800 --> 01:54:14,800
So we are interested in the following questions. So first, does this forward process or noise schedule have to be predefined? And does it have to be a Markovian process?

868
01:54:14,800 --> 01:54:28,800
And lastly, does it, is there any faster mixing diffusion process? The faster mixing is an important concept in Markov chain Monte Carlo, as we will discuss later.

869
01:54:28,800 --> 01:54:42,800
For the first work I would like to discuss here is this variational diffusion models. It basically enables us to learn the parameters in this forward process together with the rest of parameters in the diffusion models.

870
01:54:42,800 --> 01:55:02,800
In this case, we can really learn the forward process. So more specifically, given the forward process defined by QXT given x zero, it follows a Gaussian distribution with square root alpha T bar x zero as the mean and one minus alpha T bar as the variance.

871
01:55:02,800 --> 01:55:06,800
So this is the formulation we have learned in part one.

872
01:55:06,800 --> 01:55:23,800
And this work proposed to directly parametrize the variance one minus alpha T bar through a learnable function gamma eta. And this function is definitely by a similar function to ensure that the variance is within the range from zero to one.

873
01:55:23,800 --> 01:55:36,800
Gamma eta T is further parametrized by a monotonic multilayer perceptron by using strictly positive ways and monotonic activations, for example, sigmoid activations.

874
01:55:36,800 --> 01:55:54,800
And recall that in part one, we have learned that these diffusion models are directly connected to hierarchical variational auto encoders in the sense that diffusion models can be considered as a hierarchical variational encoders but with fixed encoder right.

875
01:55:54,800 --> 01:56:11,800
And this model is named as variational diffusion models because it is even more similar to hierarchical variational encoders because we are optimizing the parameters in the encoder together with the parameters in the decoder.

876
01:56:11,800 --> 01:56:33,800
And to optimize the parameters of the forward process, this paper further derive new parametrization of the training objectives in a boring sense. So basically they have shown that optimizing the variational upper bound of the diffusion models can be simplified to the following training objectives.

877
01:56:33,800 --> 01:56:45,800
Note that this gamma eta participates in this weighting of like different L2 norm of different time steps.

878
01:56:45,800 --> 01:57:05,800
This is the training objectives they derive for this great time session. And they have shown that by learning this noise schedule, it actually improves the likelihood estimation of diffusion models a lot, especially when we assume that there are fewer diffusion time steps.

879
01:57:05,800 --> 01:57:19,800
Note that in the second part, we learned that the diffusion models can be interpreted from the perspective of stochastic differential equation and we learned the connection between diffusion models and denoting score matching.

880
01:57:19,800 --> 01:57:38,800
This gives us a hint or a knowledge that the diffusion models can also be defined in the continuous time setting. And in this paper, they explicitly derive this variational upper bound in the continuous time setting with this gamma eta notation.

881
01:57:38,800 --> 01:57:55,800
Basically, they show that when we let this big T goes to infinity, meaning like we have infinity amount of diffusion time steps, this corresponds to a continuous time setting, and then the variational upper bound can be derived in the following formulation.

882
01:57:55,800 --> 01:58:11,800
And here the only difference is that the weighting term of different L2 terms, L2 distance at different time steps equals to the derivative of this gamma eta t function over time t.

883
01:58:11,800 --> 01:58:24,800
And more interestingly, this paper shows that if we define the signal to noise ratio equals to alpha t bar minus one divided by one minus alpha t bar.

884
01:58:24,800 --> 01:58:33,800
And then this L infinity is only related to the signal to noise ratio at the endpoints of the whole forward process.

885
01:58:33,800 --> 01:58:51,800
And it is invariant to the noise schedule in between the endpoints. So if we want to optimize the forward process in the continuous time setting, we only need to optimize the signal to noise ratio at the beginning and the end of the forward process.

886
01:58:51,800 --> 01:59:06,800
And they further show that the in-between process can be learned to minimize the variance of the training objective. And this enables the faster training of diffusion models besides faster sampling.

887
01:59:06,800 --> 01:59:16,800
And another contribution of this work is that they show it is possible to use diffusion models to get state of the art likelihood estimation results.

888
01:59:16,800 --> 01:59:34,800
So before this work, the benchmark of likelihood estimation have been dominated by autoregressive types of models for many years as shown in this figure, but this model shows like actually we can use diffusion models to get a big improvement out of the autoregressive model process.

889
01:59:34,800 --> 01:59:48,800
And one key factor to make this happen is to add further features to the input of the unit. And this, those four features can range from low frequency to very high frequencies.

890
01:59:48,800 --> 02:00:04,800
And the hypothesis for the assumption here is that to get good likelihood estimation, the model you really need to model all the bits or all the details in the input signal, either they are in perceptual or perceptual.

891
02:00:04,800 --> 02:00:20,800
However, new networks are usually bad at modeling small changes to the inputs. So adding those four features, especially those high frequency components can potentially help the network to identify those small details.

892
02:00:20,800 --> 02:00:36,800
And the paper found that this trick doesn't bring like much significant improvements to the autoregressive baselines. However, it leads to significant improvements in likelihood estimation for diffusion model class.

893
02:00:36,800 --> 02:00:55,800
Okay, so next, like paper or method I'm going to discuss is this denoting diffusion implicit models. So in this work, the main idea is like they try to define a family of non markovian diffusion processes and the corresponding reverse processes.

894
02:00:55,800 --> 02:01:05,800
And those processes are designed such that the model can still be optimized by the same surrogate objective as original diffusion models.

895
02:01:05,800 --> 02:01:22,800
We call that this is the surrogate objective right so this L simple where we remove the weighting of each L2 loss term and we just simply take the average of the L2 loss at different time steps.

896
02:01:22,800 --> 02:01:38,800
And then, because they can optimize by the same surrogate objective. So one can simply take a pre train diffusion model and treat it as the model of those non markovian diffusion processes, so that they are able.

897
02:01:38,800 --> 02:01:50,800
We are able to use the corresponding reverse processes to reverse the model, which means like we will have more choices of our sampling procedure.

898
02:01:51,800 --> 02:02:02,800
Okay, so to see how we can define those non markovian forward processes, let's recap the duration of the KL divergence in the variational lower bound.

899
02:02:02,800 --> 02:02:23,800
So we have this LT minus one is defined as the KL divergence between this posterior distribution QXT minus one given XT and X zero, and this denoting distribution P theta XT minus one given XT so this P theta is parameterized by the diffusion model.

900
02:02:23,800 --> 02:02:42,800
And because these two distribution, like both of them are Gaussian distributions with the same variance sigma T square, and this can be written as the outer distance between the the mean of these two distributions times a constant.

901
02:02:42,800 --> 02:03:02,800
Note that these two mean function have been parameterized by like simple linear combination of XT and if so, or the combination of XT and the predicted if so, so if some listen noise added to the queen sample to get XT.

902
02:03:02,800 --> 02:03:18,800
We can further write this KL divergence in the form of lambda T times the outer distance between the true noise if some and the predicted noise if some theta by our diffusion models.

903
02:03:18,800 --> 02:03:36,800
And if we really think about the duration of this LT minus one, we found that if we assume like this lambda T can be after values, because in a surrogate objective we simply set it to one right so it doesn't matter what value this alpha T is originally.

904
02:03:36,800 --> 02:03:48,800
So then we can find that about formulation holds as long as first we have this QXT give X zero, it follows this normal distribution.

905
02:03:48,800 --> 02:04:07,800
And we need to make sure that this XT still equals to this formulation. And we have those two assumptions. First, the follow process, like the posterior distribution QXT minus one, give XT and X zero follows a Gaussian distribution.

906
02:04:07,800 --> 02:04:13,800
And the meaning of this distribution is a linear combination of XT and the epsilon.

907
02:04:13,800 --> 02:04:28,800
And our reverse process to be similar to the posterior distribution, which means it is also a Gaussian, and this new theta is the same linear combination of XT and the predicted noise.

908
02:04:28,800 --> 02:04:43,800
So we can further rewrite, because we know XT equals to a linear combination of X zero and epsilon. So we can replace this epsilon, epsilon by the linear combination of XT and X zero.

909
02:04:43,800 --> 02:05:00,800
And for the reverse process, we can rewrite it as a linear combination of XT and predicted X zero hat. So this X zero hat is defined as the predicted clean sample, given the predicting noise.

910
02:05:00,800 --> 02:05:19,800
So if we make those three assumptions, then we can see that the about duration of our T minus one still holds, which means that we don't really need to specify this QXT, even XT minus one, and we don't need to require to be a common process.

911
02:05:19,800 --> 02:05:26,800
All we need to assume is the posterior distribution of XT minus one, even XT and X zero.

912
02:05:26,800 --> 02:05:42,800
And that's the basic, the, the insights of this, this, like how we can define the non-marcovian forward process, which leads to the same chain objecting as the original diffusion model.

913
02:05:42,800 --> 02:05:58,800
Specifically, so this is the original diffusion process. And now the diffusion process change to the right diagram, where for each XT, it depends on both the XT minus one and the X zero.

914
02:05:58,800 --> 02:06:17,800
And another remaining question is how we specify the linear combination parameters A and B. So note that here we need to specify this A and B such that this QXT given X zero still follows this Gaussian distributions.

915
02:06:17,800 --> 02:06:35,800
At this end, this work defines a family of forward processes that meets the above requirements, which corresponds to specifying the posterior QXT minus one given XT and X zero in this formulation.

916
02:06:35,800 --> 02:06:46,800
So we can similarly define the corresponding reverse process by just replacing this X zero to a predicted X zero hat.

917
02:06:46,800 --> 02:07:03,800
And note that this specifying specification of formulation doesn't require like a specific value of this sigma t-tutor, which means like this sigma t-tutor can be literally arbitrary values.

918
02:07:03,800 --> 02:07:14,800
So that's why it depends, it actually defines a family of forward processes with different values of sigma t-tutor.

919
02:07:14,800 --> 02:07:35,800
And more importantly, if we specify like sigma t-tutor to be zero for all the time steps, this leads to this DDI and sampler where we wish is a deterministic reverse process because this variance is zero here.

920
02:07:35,800 --> 02:07:46,800
And the only randomness comes from the like starting point of the reverse process, which is the starting white noise signal.

921
02:07:46,800 --> 02:08:00,800
And recall that in the second part, we also build connection between the like stochastic reverse process with a probability flow ODE, which is corresponds to a deterministic generative process.

922
02:08:00,800 --> 02:08:13,800
And we can also interpret the DDI and sampler in a similar way. Specifically, this DDI and sampler can be considered as an integration role of the following ordinary differential equation.

923
02:08:13,800 --> 02:08:26,800
And note that here we do a bit of change of a variable where we define this x bar equals to x divided by the scaling factor square root of alpha bar.

924
02:08:26,800 --> 02:08:34,800
And we define this eta to be basically the square root of the inverse signal to noise ratio.

925
02:08:34,800 --> 02:08:51,800
And if we assume this sigma, this epsilon theta to be the optimal model, and then this ODE is equivalent to a probability flow ODE of a variance is floating SDE, which is in the following formulation.

926
02:08:51,800 --> 02:09:06,800
Note that although these two are equivalent, the sampling procedure can still be different, because for the above ODE, we are taking like the sampling process over this eta t.

927
02:09:06,800 --> 02:09:19,800
Well for the second formulation we are taking the, for example, for both of the two equations we use the standard Euler's method, then the second one is taking the Euler step over dt.

928
02:09:19,800 --> 02:09:37,800
And basically, in practice, people find that the first one works better than the second one, because it depends less on the value of t, but it depends directly on the signal to noise ratio of the current time steps.

929
02:09:37,800 --> 02:09:48,800
We also found that with this DDM sampler, we are able to use less time sampling steps, but reach better like performance.

930
02:09:48,800 --> 02:10:06,800
And in terms of why this is true, this, this paper by Carols et al, argues that the ODE of the DDM is favored, because I saw in those two in those three illustrations, especially the third illustration.

931
02:10:06,800 --> 02:10:24,800
The definition of the solution trajectories of DDM always points towards the denoider outputs, while for the first two ODE formulation, the variance preserving ODE and the variance is pulling ODE, which are two very commonly used ODE formulation

932
02:10:24,800 --> 02:10:26,800
diffusion models.

933
02:10:26,800 --> 02:10:32,800
They basically have more like high coverage regions along the trajectories.

934
02:10:32,800 --> 02:10:42,800
So for the DDM, we can see like for the solution trajectories, most of the trajectories are linear and with low coverage.

935
02:10:42,800 --> 02:10:58,800
And it is known that low coverage really means less truncation errors accumulating over the trajectories. So if we use this kind of trajectories, like we will have a smaller chance to accumulate more errors across the trajectories.

936
02:10:58,800 --> 02:11:10,800
Thus, enable us to use like fewer number of diffusion time step, fewer number of sampling steps in inference.

937
02:11:10,800 --> 02:11:20,800
Okay, so the third word I'm going to discuss for advanced, the forward process is just critically down to long-distance diffusion model.

938
02:11:20,800 --> 02:11:31,800
Basically, they are trying to find a faster mixing diffusion process by using certain like background knowledge from Markov Chen and Monte Carlo.

939
02:11:31,800 --> 02:11:43,800
And how this forward process is related to MCMC, let's see, like this is a regular forward diffusion process, which is a stochastic differential equation.

940
02:11:43,800 --> 02:11:58,800
And it is actually a special case of overdone long-distance dynamics. If we assume the target distribution or the target density of this MCMC is this standard Gaussian distribution.

941
02:11:58,800 --> 02:12:09,800
Given this connection, we can actually design more like efficient forward process in terms of MCMC.

942
02:12:09,800 --> 02:12:22,800
Specifically, this word proposed to introduce an auxiliary variable, this velocity V, and the diffusion process is defined on the joint space of this velocity and the input.

943
02:12:22,800 --> 02:12:38,800
And during the forward process, the noise is only added in this velocity space. And this image space or input space is only erupted by the coupling between this data and the velocity.

944
02:12:38,800 --> 02:12:53,800
And the resulting process as showing this figure, we can see that the forward process in the V space is still exact. However, the process in the image space or the data space are much more smoother.

945
02:12:53,800 --> 02:13:18,800
This V components is analogous to the Hamiltonian components in HMC or analogous to the momentum in momentum based optimizers. So by defining this joint space or defining the diffusion in this V space, it actually enables faster mixing and faster traverse of the joint space,

946
02:13:18,800 --> 02:13:28,800
which enables fast, like more smoothly and efficient, more smooth and efficient forward process.

947
02:13:28,800 --> 02:13:41,800
And the second, let's see some advanced reverse process. So before that, we would like to ask a question. So remember that we use this normal approximation of the reverse process, right.

948
02:13:41,800 --> 02:13:56,800
It seems like the denoting distributions are always Gaussian distributions. But if we want to use less diffusion time steps, is this normal approximation of the reverse process still true or accurate?

949
02:13:56,800 --> 02:14:08,800
Unfortunately, the answer is no. So this assumption, normal assumption in this denoting distribution holds only when the adjacent

950
02:14:08,800 --> 02:14:23,800
the noise added between these adjacent steps are small. If we want to use less diffusion time steps in training, then we can see that this denoting distribution is not a unimodal normal distribution anymore.

951
02:14:23,800 --> 02:14:36,800
So they are tend to be like multimodal and more complicated distributions. So in that case, it means like we really need more complicated functional approximators.

952
02:14:36,800 --> 02:14:45,800
So we will talk about two examples of how we can include more complicated functional approximators here.

953
02:14:45,800 --> 02:14:55,800
The first word is this denoting diffusion gains. So in this word, they propose to model the denoting distribution by conditional gain model.

954
02:14:55,800 --> 02:15:10,800
Specifically, the model is training this other several learning framework. And we first get the samples xt minus one and xt by running this forward diffusion process.

955
02:15:10,800 --> 02:15:30,800
And then this generator takes xt as input, as well as the time step as input. And it's actually trying to model the xt minus one. But instead of just directly outputting xt minus one from the network, it's first trying to predict this

956
02:15:30,800 --> 02:15:49,800
screen sample x0. And then it tries to sample xt minus one from this tractable posterior distribution. And then the discriminator takes the real xt minus one and the fake xt minus one prime as input and try to discriminate these two samples.

957
02:15:49,800 --> 02:15:57,800
And again, this discriminator is also conditional xt and the corresponding time step t.

958
02:15:57,800 --> 02:16:05,800
One may ask, what is the benefit of this denoting diffusion gains compared to a one shot gain model.

959
02:16:05,800 --> 02:16:17,800
But this paper shows that because like right now the conditional gains only need to model like the conditional distribution of xt minus one given xt.

960
02:16:17,800 --> 02:16:27,800
This turns out to be a much simpler problem for both the generator and the discriminator compared to directly model the model distribution of the clean sample.

961
02:16:27,800 --> 02:16:40,800
And this simple training objective for the two models leads to stronger mode coverage properties of gains and also leads to better training stability.

962
02:16:40,800 --> 02:16:56,800
And recall that in the second part we learned that there's a close connection between energy based models and diffusion models. So, therefore, a natural idea is like we try to approximate the reverse process by conditional energy based models.

963
02:16:56,800 --> 02:17:10,800
Recall that an energy based model is in the form of P beta x. It is solely dependent on the normalized log density function which is f theta x.

964
02:17:10,800 --> 02:17:25,800
And we can further prime trend is f theta x by minus e theta x. So usually, people call this e theta x as the energy function. And this d theta is the partition function which is usually analytical and tractable.

965
02:17:26,800 --> 02:17:36,800
And we can parameterize this f theta by a neural network which takes the signal as input and output as scalar to represent the value of this f.

966
02:17:36,800 --> 02:17:45,800
And the learning of energy based models can be illustrated as follows. So suppose this is the energy landscape defined by the e theta function.

967
02:17:46,800 --> 02:18:00,800
And after learning, we would like to put the observation data points into the regions of low energy and put all the other inputs to the regions of the high energy.

968
02:18:00,800 --> 02:18:18,800
And optimizing the energy based models require, you really require the MSMC sampling from the current model p theta x as showing this formulation, which is really highly computational expensive, especially for high dimensional data.

969
02:18:19,800 --> 02:18:35,800
So if we want to parameterize the denoising distribution by conditional energy based model, we can start by assuming like at each diffusion time step marginally the data follows a energy based model in the standard formulation.

970
02:18:35,800 --> 02:18:43,800
So here I removed the, the script like the time step script for similar simplicity.

971
02:18:43,800 --> 02:18:47,800
And let x theta be the data at a higher noise level.

972
02:18:47,800 --> 02:18:57,800
So we can derive the conditional energy based models by Bayes and Rowe, but specifically this p x, the x theta is in this formulation.

973
02:18:57,800 --> 02:19:08,800
And if we compare this conditional energy based models with with the original marginal energy based models, we see that the only difference is that there's an extra projected term here.

974
02:19:08,800 --> 02:19:30,800
And this actual projected term actually has the effect of localize this highly multimodal energy landscape to a like more single mode model or uni model landscape, and with the model mode focus around the higher noise level signal x theta.

975
02:19:31,800 --> 02:19:48,800
Therefore, compared to training a single energy based model the sampling here is more friendly and easier to converge because the energy landscape compared to the original marginal energy landscape is more uni model and more and simpler.

976
02:19:48,800 --> 02:19:57,800
So that the training could be more efficient and the conversion and CMC can give us well formed energy potential after training.

977
02:19:57,800 --> 02:20:15,800
And compared to diffusion models, this energy based to using this energy based model to a parameterize the denoting distribution can give enables us to define like much less diffusion steps up to six steps.

978
02:20:15,800 --> 02:20:24,800
And more specifically to learn those models we simply maximize the conditional log likelihoods at each time step.

979
02:20:24,800 --> 02:20:36,800
And then after training we just get samples by progressive sampling from the energy based models from high noise levels to low noise levels.

980
02:20:36,800 --> 02:20:49,800
So the last part comes to the advanced diffusion models, basically want to ask two questions. For first, can we do model distillation so that the distilled model can do faster sampling.

981
02:20:49,800 --> 02:20:57,800
And second, can we lift the diffusion model to a latent space that is faster to diffuse.

982
02:20:57,800 --> 02:21:10,800
So the first idea comes to the distillation so here I want to discuss one representative work in this domain, which is this progressive distillation of diffusion models.

983
02:21:10,800 --> 02:21:19,800
So essentially this work proposed to distill a deterministic DDIM sampler to the same model architecture of the original model.

984
02:21:19,800 --> 02:21:38,800
And it's, it is, it went into this progressive pipeline, in a sense that at each distillation stage, we will have a teacher model, and, and we will learn a student model and this student model is learned to distill.

985
02:21:38,800 --> 02:21:50,800
By each two adjacent sampling steps of the teacher model to one sampling step of the student model. And after learning this student model at next distillation stage.

986
02:21:50,800 --> 02:22:00,800
The student model as a previous stage will serve as the teacher model at this new stage, and then we learn another student model at the new stage.

987
02:22:00,800 --> 02:22:10,800
So we need this process until we can distill the original thousands of sampling steps to a single sampling step.

988
02:22:10,800 --> 02:22:18,800
And implementation wise, the learning of the student model is quite similar to the original diffusion model training pipeline.

989
02:22:18,800 --> 02:22:28,800
The difference is how we define this training target of the diffusion model, specifically, given the teacher model.

990
02:22:28,800 --> 02:22:36,800
And we randomly sample a time step T, and then we draw, we run the sampler for two steps.

991
02:22:36,800 --> 02:22:47,800
And then the target is the, is computed to make sure that the student model can reproduce the two sampling step within one sampling step.

992
02:22:47,800 --> 02:23:04,800
And then the loss is defined as Euro where we minimize the out to distance between this target and the predicted, like X hat from this diffusion model or from the student model.

993
02:23:04,800 --> 02:23:15,800
And after that, we have in the number of sampling steps and repeat this process until we reach one sampling step.

994
02:23:15,800 --> 02:23:24,800
Another idea is like whether we can leave the diffusion models to a latent space, which is more friendly to this diffusion process.

995
02:23:24,800 --> 02:23:36,800
And here's an example of this kind of idea where we can try to leave the diffusion models to a latent space of a pre trained variation of encoder.

996
02:23:36,800 --> 02:23:53,800
In the latent space, the distribution of the data in this latent space is already quite close to the Gaussian distributions, which means like we can definitely use less diffusion time steps to diffuse the data in this latent space.

997
02:23:53,800 --> 02:24:07,800
The advantages are pretty straightforward. So first, because this latent space already close to normal distribution, we are able to use less diffusion time steps to enable faster sampling.

998
02:24:07,800 --> 02:24:19,800
And then compared to the original variational auto encoders, which assume that the prior distribution of the Z follows a single and simple Gaussian distribution.

999
02:24:19,800 --> 02:24:35,800
This kind of hybrid model assume that the PFC is modeled by a diffusion models, which means that it has a diffusion prior. So it definitely will be much more expressive compared to the original variational auto encoder model.

1000
02:24:35,800 --> 02:24:50,800
So we can actually record that for the current stage that diffusion models only defining a continuous data space. However, there are more domains which may have more complicated data structure.

1001
02:24:50,800 --> 02:25:06,800
So this data type, as long as we can find an auto encoder model which are tailored to that data type and can map the data input to a continuous latent space, we will be able to apply the diffusion models to that latent space.

1002
02:25:06,800 --> 02:25:16,800
So these give us more possibilities to apply diffusion models to different modalities and different data types.

1003
02:25:16,800 --> 02:25:31,800
And a bit of the detailed formulation. So in this work, again, we optimize the model in terms of by minimizing the variational upper bound of the negative log likelihood.

1004
02:25:31,800 --> 02:25:45,800
The objective contains three terms. The first two terms are similar to the variational auto encoder objecting. And the third term corresponds to the training objective of the diffusion models.

1005
02:25:45,800 --> 02:25:57,800
And it actually corresponds to, we treat the encoding latents from this QZ0 given X as the observed data of the diffusion models.

1006
02:25:57,800 --> 02:26:08,800
And in that way, we can derive the similar training objective of the diffusion models as the original one. So we first do this random sampling of time step.

1007
02:26:08,800 --> 02:26:22,800
And then we draw samples from this forward diffusion. And then we have this diffusion kernel. This is the forward, this is from the forward process. And then we learn this score function for DT.

1008
02:26:22,800 --> 02:26:29,800
And of course we have some constant that is irrelevant of the model parameters.

1009
02:26:29,800 --> 02:26:37,800
Okay, so the second question we want to answer is how to do high resolution optionally conditional generation using diffusion models.

1010
02:26:37,800 --> 02:26:53,800
In the past two years, we have seen many impressive conditional generation results using diffusion models. For example, this style II and imagine recruits diffusion models to do high resolution text to image generation.

1011
02:26:53,800 --> 02:27:02,800
And another examples includes the using conditional diffusion models for super resolution or colorization.

1012
02:27:02,800 --> 02:27:10,800
Panorama generation is another example where we take a small size input but generate this panorama.

1013
02:27:10,800 --> 02:27:23,800
So how can we do that? Let's first take a look at the general formulation of conditional diffusion models, which is pretty straightforward. So the only modification we need to make is in this reverse process.

1014
02:27:23,800 --> 02:27:39,800
We can let this denoting distribution to incorporate an additional input, which is this condition C. And this corresponds to modify the mean of this Gaussian distribution to take an additional input C.

1015
02:27:39,800 --> 02:27:46,800
And optionally, we can also let this variance to be learned and it takes an input C.

1016
02:27:46,800 --> 02:28:06,800
But in practice, like most mostly we still just use this C in this mean and the variation of upper bounds only includes a small change, which lies in this KL divergence where we plug in this new formulation of the denoting distribution.

1017
02:28:06,800 --> 02:28:17,800
But it is a design arc in terms of how to incorporate different types of conditions into the unit, which is used to parameterize this new data.

1018
02:28:17,800 --> 02:28:32,800
Specifically, like these are the things people use in practice for scalar conditioning. For example, class label, we can do something similar to what we did for time step conditioning.

1019
02:28:32,800 --> 02:28:47,800
Specifically, we can encode the certain scalars to a vector embedding, and then we simply add the embedding to the intermediate layers of the unit, or we do this adaptive professionalization layers.

1020
02:28:47,800 --> 02:28:59,800
And if it is an image conditioning, we can do channel wise concatenation of the conditional image and the input image. And if it is for text conditioning,

1021
02:29:00,800 --> 02:29:12,800
this contains two cases. First, if the text, we embed the text to a single vector, then we can do something similar to the vector derived from the scalar conditioning.

1022
02:29:12,800 --> 02:29:24,800
And if we embed this text to a sequence of vectors, then we can consider using cross attention with the intermediate layers of the unit.

1023
02:29:24,800 --> 02:29:32,800
And for a high resolution conditional generation, another important component is this classifier guidance.

1024
02:29:32,800 --> 02:29:44,800
The main idea is like recall the diffusion model correspond to learning the score of a probability, for example, the score of log px.

1025
02:29:44,800 --> 02:29:59,800
And right now, because we incorporate the class condition, which means like the diffusion model actually gives us a score of a class conditional model p of xt given c.

1026
02:29:59,800 --> 02:30:13,800
And given this, we can train an additional classifier, which gives us the probability of c given x, and then we mix the gradients of these two models during sampling.

1027
02:30:13,800 --> 02:30:25,800
And this corresponds to, we sample from actually a modified score, which corresponds to the gradient of log px given c plus omega times log pc given x.

1028
02:30:25,800 --> 02:30:42,800
And this omega controls the strength of the guidance. And it actually corresponds to approximate sampling from the distribution, which is proportional to p of x given c times p of c given x to the omega power.

1029
02:30:42,800 --> 02:31:05,800
And in practice, it corresponds to we modify the normal distribution where we sample from and the mean of this normal distribution corresponds to the mean predicted by the score model or predicted by the diffusion model, plus the gradients from the classifier.

1030
02:31:06,800 --> 02:31:23,800
And if we use larger omega, then the samples will be more concentrated around the modes of this classifier, which really leads to better individual sample quality, but if we use too large omega, it will reduce the sample diversity.

1031
02:31:23,800 --> 02:31:34,800
So one really needs to find a sweet point for this omega to best balance the individual sample quality and sample diversity.

1032
02:31:34,800 --> 02:31:45,800
And one downside for this classifier guidance is that we need to train an additional classifier to do that, right, so that adds additional model complexity.

1033
02:31:45,800 --> 02:32:04,800
So inspired by that work. This works tries to introduce a classifier free guidance, meaning that we can actually get an implicit classifier by joining training a conditional and unconditional diffusion model in a pulling sense.

1034
02:32:04,800 --> 02:32:26,800
So suppose we have this PX given C, where the the score can be derived by a conditional diffusion model, and we also have the score of a unconditional model p of x, and then we can derive an implicit classifier PC given x which should be proportional to p of x given

1035
02:32:26,800 --> 02:32:41,800
divided by p of x. And in practice, the gradient or the score of these two probability are estimated by randomly dropping the condition in the diffusion models at a certain chance for each iteration.

1036
02:32:41,800 --> 02:32:56,800
And similarly, we can derive the modified score with this implicit classifier. We call this is the original modified score. And now we replace the log PC given x by log PX given C minus log PX.

1037
02:32:57,800 --> 02:33:12,800
And this is the resulting modified score we will use with this classifier free guidance where this log PX given C and this log PX are both estimated by the single diffusion model.

1038
02:33:13,800 --> 02:33:36,800
And in this three panels, we can see the trade off between sample quality and sample diversity more clearly. So from left to right correspond to we gradually increase the strength of the guidance, and we can see clearly individually speaking, the sample quality of each image increases.

1039
02:33:37,800 --> 02:33:48,800
However, the samples that look like more similar to each other if we use a large classifier or classifier free guidance.

1040
02:33:48,800 --> 02:34:06,800
And the last thing I want to talk about is this cascade generation pipeline, which are important to high resolution generation. And this kind of idea have already been explored by other type of generating models for, for example, game model.

1041
02:34:06,800 --> 02:34:23,800
And it's pretty straightforward. So we've started by learning an unconditional diffusion model at the lowest resolution. And then we've learned several super resolution models, taking the down sampled training images at lower resolution as the condition.

1042
02:34:23,800 --> 02:34:39,800
And during sampling, we just run the progressive sampling pipeline starting from the smallest unconditional model and going, going through all those super resolution models until we reach the highest resolution.

1043
02:34:39,800 --> 02:35:00,800
But one notorious problem of this kind of cascaded training pipeline is this compounding error problem. It's more specifically record that during training, the conditions we fit into the super resolution model is the down sampled version of the training images from the data set.

1044
02:35:00,800 --> 02:35:21,800
However, during sampling, the conditions we fit to those super resolution models are actually generative samples of from the low resolution models. So if there are certain artifacts in the samples from the low resolution models, those artifacts or inaccurate samples will affect the

1045
02:35:21,800 --> 02:35:32,800
the sample quality of the super resolution models as well, because of this mismatch issue between the conditions in training and condition in inference.

1046
02:35:32,800 --> 02:35:54,800
To activate this problem, this noise conditioning augmentation is proposed in reference works. Basically, during training, we try to degrade the conditioning low resolution images by adding variance amount of Gaussian noise, or just blur those images by Gaussian kernel.

1047
02:35:55,800 --> 02:36:05,800
And during inference, we sweep over the optimal amount of noise added to the low resolution images, which are the conditions to the super resolution models.

1048
02:36:05,800 --> 02:36:27,800
So the idea or the hypothesis is like if we try to reduce certain amount of information from the condition, then the super resolution model will be trained to be more robust to different type of artifacts when we send like the conditions as the samples.

1049
02:36:27,800 --> 02:36:48,800
And later on, more complicated degradation process have been proposed. For example, we can add a sequence of different types of degradation operations to the image, the low resolution image before sending as a condition to the super resolution models.

1050
02:36:49,800 --> 02:37:11,800
Okay, so here's a summary of these parts. So in this advanced techniques session, we learn to answer two questions. The first one is how we can accelerate the sampling process, and we introduce several important techniques from the aspects of advanced forward process, reverse process and modeling itself.

1051
02:37:11,800 --> 02:37:29,800
And the second question is how we can do high resolution conditional generation using diffusion models, and we discuss the general framework of conditional diffusion models, classifier and classifier free guidance, as well as cascade generation pipeline.

1052
02:37:29,800 --> 02:37:38,800
So in the application section, we will see how all those techniques will benefit in terms of various tasks.

1053
02:37:38,800 --> 02:37:53,800
Hi, everyone. Welcome to the first section of applications of diffusion models. So in this section, we're going to study applications of diffusion models in terms of image synthesized control generation as well as text to image generation.

1054
02:37:53,800 --> 02:38:05,800
So let's run text to image generation. So in the past two years, this task has been shown to be extremely suitable for diffusion models to work on.

1055
02:38:05,800 --> 02:38:17,800
So basically, this task is the inverse of the image captioning tasks, where we are given a text prompt C and we are trying to generate high resolution images X, as shown in this video.

1056
02:38:17,800 --> 02:38:26,800
So this video shows the generative image images by a text to image generation model called imagine as we will show later.

1057
02:38:27,800 --> 02:38:46,800
And let's start from this glide model by opening in last year. So this is essentially a cascading generation diffusion models, which contains 64 by 64 base model, and a 64 by 64 to 56 by 256 super resolution model.

1058
02:38:47,800 --> 02:39:01,800
And they have tried to use classifier free guidance and clip guidance. So I will talk about the clip guidance in details later, and they generally found that classifier free guidance works better than the clip guidance.

1059
02:39:01,800 --> 02:39:16,800
And those figures shows the generative samples from this model. And as we can see, the model is capable of generate fun, normal conversations of concepts that have never been seen from the data set.

1060
02:39:16,800 --> 02:39:25,800
For example, a hedge dog using a calculator and robots meditating in a vipassana retreat or etc.

1061
02:39:25,800 --> 02:39:33,800
So a bit introduction of the clip guidance, it can be treated as a special form of the classifier guidance.

1062
02:39:33,800 --> 02:39:48,800
And in terms of a clean model contains two components, a text encoder, G, and image encoder F, and during training batches of the image and caption pairs assembled from a large datasets.

1063
02:39:48,800 --> 02:40:09,800
And the model optimize a contrasting cross entropy loss, which encourages high dog product between this F and G, if the image X and C comes from the same image caption pair, and it encourages low product if X and C comes from different image caption pairs.

1064
02:40:09,800 --> 02:40:28,800
And it can be proved that the optimal value of this FX times GC is given by log PXC divided by P of X times P of C, which equals to log P, C given X minus log PC. So given this conclusion,

1065
02:40:29,800 --> 02:40:45,800
we will be able to use this clip model as the classifier in the classifier in the classifier guidance in the following sense. So recall that for the classifier guidance, we want to modify the score in this formulation.

1066
02:40:45,800 --> 02:41:03,800
And we can consider augment the augmenting the second term by a minus log PC term, because when we take gradient over X, then this part just disappeared. And then we can see that these two terms together can be modeled by a clip model.

1067
02:41:04,800 --> 02:41:19,800
So basically replace this part by the dog product between FX and GC. So that is the clip guidance. However, in Glide they show that the clip guidance is less favored compared to the classifier free guidance.

1068
02:41:20,800 --> 02:41:41,800
And besides pure text-to-image generation, the Glide has shown that it is possible to fine-tune the model for text-guided impending tasks. Basically, they try to fine-tune the trained text-to-image Glide model by fitting randomly occluded images with an additional mask channel as the input.

1069
02:41:42,800 --> 02:42:03,800
So using this fine-tune model, they are able to change or do image editing by changing the prompt. For example, given an old car in a green forest, they will be able to edit the background to a snowy forest. Similarly, they can add a white hat to a man's hat.

1070
02:42:04,800 --> 02:42:24,800
And later on, this DAO-E2 further scale up the Glide model to support 1K by 1K text-to-image generation. And this DAO-E2 has been shown to outperform the first version of text-to-image 1K by 1K generation by OpenEye, which is this DAO-E, which is an unregressive transformer-based model.

1071
02:42:25,800 --> 02:42:47,800
And in terms of the model components of DAO-E2, it's built up on a pre-trained clip model. More specifically, a clip model is first pre-trained, and the image embedding and text embedding are grabbed from this pre-trained clip embedding and frozen.

1072
02:42:48,800 --> 02:43:09,800
And after that, this pipeline has been built to generate images from text. Basically, this generation model contains two parts. The first is a prior model. This prior model tries to produce clip image embeddings conditioned on the input caption.

1073
02:43:10,800 --> 02:43:19,800
And then the second part is a decoder part, which produces the images conditioned on the clip image embedding as well as the text.

1074
02:43:20,800 --> 02:43:32,800
So one natural question is why we want to condition the decoder on the clip image embeddings, right? So why not we just directly condition this decoder on text only?

1075
02:43:32,800 --> 02:43:55,800
So the hypothesis here is that for the total amount of entropy of an input signal, for example, images, so there's certain part that captures the high-level semantic meanings while there's still a large proportion of the entropy, which corresponds to just low-level details, either perceptual-visible or even perceptual-invisible.

1076
02:43:56,800 --> 02:44:10,800
So the hypothesis is that clip image embeddings tend to have a higher chance to capture the high-level semantic meaning of an input signal, especially those related to the caption information.

1077
02:44:11,800 --> 02:44:20,800
And by conditioning on this high-level semantic meaning, the decoder is able to capture or catch up those low-level details of the images more quickly.

1078
02:44:21,800 --> 02:44:36,800
And later on, this paper also shows that this by-part later repetitions of the clip image embedding as well as the latency in the decoder model enables several text-guided image manipulation tasks, as we will show later.

1079
02:44:37,800 --> 02:44:43,800
And a bit more details of the model architecture of the prior and the decoder models.

1080
02:44:44,800 --> 02:44:56,800
For the prior, the paper tries two options. The first one is the auto-regressive pair, where they quantize the image embedding to a sequence of discrete codes and predict them auto-regressively.

1081
02:44:57,800 --> 02:45:11,800
And the second option is to model the prior by diffusion models, where they directly train diffusion models based on the continuous image embedding as well as the caption input.

1082
02:45:12,800 --> 02:45:16,800
And the paper shows that the second option gives better performance.

1083
02:45:17,800 --> 02:45:27,800
And in terms of the decoder, it's again a cascaded diffusion models, which contains a one-base model and two super resolution models.

1084
02:45:27,800 --> 02:45:39,800
And to save the compute and make the training more efficient, the largest super resolution model is trained on image patches of one quarter size.

1085
02:45:39,800 --> 02:45:46,800
But during inference, the model will take the full resolution inputs and directly do the inference on the full resolution.

1086
02:45:47,800 --> 02:45:56,800
And this paper also shows that the classifier-free guidance and noise conditioning augmentation are super important to make the decoder work well.

1087
02:45:57,800 --> 02:46:09,800
And a little bit more detail about the bipartisan latent representations. So given an input image, we can get the bipartisan latent representations in the following sense.

1088
02:46:09,800 --> 02:46:11,800
So it contains two parts.

1089
02:46:11,800 --> 02:46:20,800
First is this latent variable Z, which is the clip image embeddings, and it can be derived by running the clip image encoder.

1090
02:46:21,800 --> 02:46:25,800
And the second part is this xt, which is the latency from the decoder.

1091
02:46:25,800 --> 02:46:33,800
And this part can be derived by running the inversion of an ddim sampler for the decoder.

1092
02:46:33,800 --> 02:46:45,800
And after getting these two latent representations, the paper shows that it is possible to run this decoder and get near-perfect reconstruction of the original input image.

1093
02:46:45,800 --> 02:46:54,800
And given these bipartisan representations, the paper shows that it is possible to do several image manipulation tasks.

1094
02:46:54,800 --> 02:47:10,800
For example, this image variation tasks target at getting multiple variants of an input image, while with the hope of preserving the high-level semantic meanings of the input image.

1095
02:47:10,800 --> 02:47:21,800
And this is achieved by fixing the clip embedding Z, while changing to different latent xt in the decoder.

1096
02:47:21,800 --> 02:47:30,800
And as shown in this image panel, the first one is the input image, and the rest are the image variants generated by .e2.

1097
02:47:30,800 --> 02:47:38,800
And we can see that certain high-level semantic meanings are preserved, for example, the artist's style.

1098
02:47:38,800 --> 02:47:48,800
And this clock is preserved in all the image variants, but with different details in the image variants.

1099
02:47:48,800 --> 02:47:52,800
And the second task is this image interpolation task.

1100
02:47:52,800 --> 02:48:02,800
So given two input images, it's possible to use .e2 to do interpolation by interpolating the image clip embeddings of these two input images.

1101
02:48:02,800 --> 02:48:11,800
And we can get different interpolation trajectories by using different xt along these trajectories, as shown in those three rows.

1102
02:48:12,800 --> 02:48:26,800
And as we can see, although the trajectories are different, but the high-level semantic meanings are kept well for the two input images for all those three interpolation trajectories.

1103
02:48:26,800 --> 02:48:43,800
And the last task, the most interesting task that they show that can be done by .e2 is this text div task, which means like given an input image and the corresponding description, we would like to add this image towards a different prompt.

1104
02:48:44,800 --> 02:48:49,800
And this corresponds to an arithmetic operation in the latent space.

1105
02:48:49,800 --> 02:48:59,800
More specifically, they first try to compute the difference between the text clip embeddings of the original prompt and the target prompt.

1106
02:48:59,800 --> 02:49:07,800
And then they try to change the image clip embedding of the given input towards the difference between the text prompts.

1107
02:49:07,800 --> 02:49:17,800
And using this approach, they show that it's possible to do this text-guided image editing by changing the prompt.

1108
02:49:17,800 --> 02:49:25,800
And the last task to image diffusion model I want to talk about is this imagined model by Google Brain Team.

1109
02:49:25,800 --> 02:49:28,800
So again, the task is the same as .e2.

1110
02:49:28,800 --> 02:49:38,800
So we are given some text prompts as input, and we are trying to output 1k by 1k images aligned with the input text.

1111
02:49:38,800 --> 02:49:41,800
And the highlight of imagined model is as follows.

1112
02:49:41,800 --> 02:49:56,800
First, it provides an unprecedented degree of photorealisticism in terms of state-of-the-art automatic scores, such as FID scores, as well as state-of-the-art human ratings.

1113
02:49:56,800 --> 02:50:03,800
And it provides a deep level of language understanding, as can be told by the generated samples.

1114
02:50:03,800 --> 02:50:09,800
And it is extremely simple, so there is no latent space and no compensation.

1115
02:50:09,800 --> 02:50:15,800
And as we will see later, it's just like a pure cascaded diffusion model.

1116
02:50:15,800 --> 02:50:26,800
So I will first present several examples of imagined.

1117
02:50:26,800 --> 02:50:34,800
Yeah, this is my favorite one because I just created it to make it related to CVPR.

1118
02:50:34,800 --> 02:50:48,800
And in terms of the key modeling components of imagined, like I mentioned, it is a pure cascaded diffusion model containing one base model and two super resolution models.

1119
02:50:48,800 --> 02:50:56,800
And it uses classifier-free guidance and the dynamic thresholding, as I will talk about later.

1120
02:50:56,800 --> 02:51:17,800
And unlike DAI2, which uses clip text embedding as the, using this clip text encoder, this imagined used frozen large pre-train language models as the text encoders, more specifically this variant of T5 model.

1121
02:51:17,800 --> 02:51:21,800
And there are several key observations from imagined.

1122
02:51:21,800 --> 02:51:27,800
First, it is beneficial to use text conditioning for all the super resolution models.

1123
02:51:27,800 --> 02:51:38,800
The explanation is as follows. So remember, like for cascaded diffusion models, we need to use this noise conditioning augmentation technique to reduce the compounding error.

1124
02:51:38,800 --> 02:51:45,800
But however, this technique has a chance to weaken the information from the low resolution models.

1125
02:51:45,800 --> 02:51:53,800
Thus, we really need the text conditioning as extra information input to support the super resolution models.

1126
02:51:53,800 --> 02:52:01,800
And second observation is that scaling the text encoder is extremely efficient in terms of improving the performance of imagined.

1127
02:52:02,800 --> 02:52:09,800
And it has been shown that this is even more important than scaling the diffusion model side.

1128
02:52:09,800 --> 02:52:27,800
And lastly, comparing using the pre-trained large language model as the encoder versus the clip encoder, human readers actually prefer the large language model over the clip encoder on certain data sets.

1129
02:52:27,800 --> 02:52:33,800
And this dynamic thresholding is a new technique introduced by imagined.

1130
02:52:33,800 --> 02:52:44,800
This is mainly to solve the trade-off problem of using large classifier-free guidance weights, more specifically as we also discussed in the previous part.

1131
02:52:44,800 --> 02:52:55,800
So when we use large classifier guidance weights, there is a chance that it gives us better text alignment but worse image quality.

1132
02:52:56,800 --> 02:53:04,800
So as we use large free guidance weights, the clip score, which corresponds to a better text alignment, increases.

1133
02:53:04,800 --> 02:53:11,800
However, the IFID score also increases, which corresponds to worse sample quality.

1134
02:53:11,800 --> 02:53:21,800
So to elevate this issue, because we really want both the sample quality, like the good sample quality as well as good text elements, right?

1135
02:53:21,800 --> 02:53:36,800
So to elevate this trade-off issue, the hypothesis this paper made is that the reason why the sample quality decreases at large guidance weights is that at large guidance weights,

1136
02:53:36,800 --> 02:53:51,800
usually it corresponds to very large sample gradients in inference, and then the generated samples have a chance to be saturated because of the very large gradient updates.

1137
02:53:51,800 --> 02:54:09,800
So the solution they propose is this dynamic thresholding, meaning that at each sampling step, we adjust the pixel values of the samples to be within a dynamic range, and this dynamic range is computed over the statistics of the current samples.

1138
02:54:09,800 --> 02:54:18,800
And these two panels shows the qualitative comparisons between static thresholding and dynamic thresholding.

1139
02:54:18,800 --> 02:54:30,800
And you can see if we use this static thresholding, the images look kind of saturated, while the dynamic thresholding, the samples look more realistic.

1140
02:54:30,800 --> 02:54:38,800
And another contribution of Imagine is that they introduce a new benchmark, especially for this text-to-image evaluations.

1141
02:54:38,800 --> 02:54:49,800
So the motivation of introducing new benchmarks is that for existing datasets, for example, COCO, the text problem is kind of limited and is kind of easy.

1142
02:54:49,800 --> 02:54:57,800
So this benchmark introduced more challenging prompts to evaluate text-to-image models across multiple dimensions.

1143
02:54:57,800 --> 02:55:10,800
For example, it tries to evaluate the ability of the model to facefully render different colors, numbers of objects, spatial relations, text in the scene, unusual interactions between objects.

1144
02:55:10,800 --> 02:55:24,800
And it also contains some complex prompts, for example, long and intricate descriptions, wire words, and even misspelled prompts to test the robustness of your model.

1145
02:55:24,800 --> 02:55:37,800
And this figure shows several examples of the text prompts in this benchmark, and the corresponding generated images from Imagine using these text prompts.

1146
02:55:37,800 --> 02:55:42,800
And a bit more of the quantitative evaluations of Imagine.

1147
02:55:42,800 --> 02:55:57,800
Imagine got state-of-the-art automatic evaluation scores on the COCO dataset, and it's also preferred over reasoned work by human readers in both sample quality and image text alignment on the drawbench dataset.

1148
02:55:57,800 --> 02:56:08,800
And the reasoned work compared by Imagine includes this DAO-E2 slide, VQGAM plus clip, as well as the latent diffusion models.

1149
02:56:08,800 --> 02:56:15,800
Okay, so besides text-to-image generation, I also want to talk about the controllable generation using diffusion models.

1150
02:56:15,800 --> 02:56:26,800
And a representative work is this diffusion autoencoders, which propose to incorporate a semantic meaningful latent variables to diffusion models.

1151
02:56:26,800 --> 02:56:35,800
So more precisely, so given an input image, semantic encoders learn in this framework to generate this Z-SIM.

1152
02:56:35,800 --> 02:56:43,800
And this Z-SIM is fit into a conditional diffusion models to further predict the clean samples.

1153
02:56:43,800 --> 02:56:58,800
And this leads to some like the bipod latent representation similar to the DAO-E2 model, where we have this Z-SIM with the hope that it can capture high-level semantics.

1154
02:56:58,800 --> 02:57:04,800
And we also have this X-Big-T, which is the inversion of this conditional DTIM sampler.

1155
02:57:04,800 --> 02:57:10,800
And the hope is that it captures the low-level stochastic variations of the images.

1156
02:57:10,800 --> 02:57:19,800
And if we want to do unconditional sampling from this model, optionally, we can learn another diffusion model in the latent space of the Z-SIM.

1157
02:57:19,800 --> 02:57:28,800
Very similar to the latent diffusion models we talked about in the last part, to support this unconditional generation test.

1158
02:57:28,800 --> 02:57:41,800
Interestingly, they found that by assuming a low-dimensional semantic vector Z, they are able to learn different semantic meanings for different dimensions of this latent vector Z.

1159
02:57:41,800 --> 02:57:55,800
For example, by changing the certain dimension of this latent Z, they are trying to identify different semantic meanings such as the higher style, the expression, the age,

1160
02:57:55,800 --> 02:58:01,800
and also the color of the hair.

1161
02:58:01,800 --> 02:58:17,800
And they also are assuming that if we fix the Z-SIM for each row, and we change the latent X-Big-T in the conditional diffusion model, we can see it only corresponds to very tiny details in this image,

1162
02:58:17,800 --> 02:58:23,800
and perhaps other like perceptually invisible features in the images.

1163
02:58:23,800 --> 02:58:30,800
So that ends the first part of the application. Thanks for listening.

1164
02:58:30,800 --> 02:58:38,800
Awesome. Thanks Richie for the nice introduction of the first group of applications.

1165
02:58:38,800 --> 02:58:51,800
Here I'm going to start with the second group of applications. And in this part, I will mostly focus on image editing, image to image translation, super resolution, and semantic segmentation.

1166
02:58:51,800 --> 02:59:05,800
This is a super resolution. I'll start with talking about this work called super resolution, but we are repeated refinements, or SR3, which was proposed by Sahari et al at Google.

1167
02:59:05,800 --> 02:59:21,800
In image super resolution, we can consider this problem as training a conditional model P of X given Y, where Y is the low resolution image and X is the corresponding high resolution image.

1168
02:59:21,800 --> 02:59:28,800
So we want to be able to change the high resolution images given some input low resolution image.

1169
02:59:28,800 --> 02:59:37,800
In order to tackle this problem, the authors proposed to train a diffusion model, a conditional diffusion model using this objective.

1170
02:59:37,800 --> 02:59:46,800
Here in this objective, we have expectation over pairs of high resolution image X and low resolution image Y.

1171
02:59:46,800 --> 03:00:00,800
We have expectation over epsilon, which is drawn from standard normal distribution, and we have expectation over time, where time varies from, for example, zero to capital T corresponding to the diffusion process.

1172
03:00:00,800 --> 03:00:16,800
We have this excellent setup. This is a noise prediction network that takes diffused high resolution image, XT, the time, as well as this Y, this is the low resolution image that is provided as conditioning into epsilon prediction

1173
03:00:16,800 --> 03:00:25,800
network, and we train this epsilon prediction network to predict the noise that was used in order to generate diffused high resolution image.

1174
03:00:25,800 --> 03:00:43,800
The authors in this paper proposed to use different norms for minimizing this objective. They introduced L1, L2 norm, and they observed that one can trade quality for diversity by using L1 norm is L2 norm.

1175
03:00:43,800 --> 03:00:51,800
Since we are training a conditional model now you have in mind that we need to modify the unit that is used for epsilon prediction.

1176
03:00:51,800 --> 03:01:02,800
For the input of the unit, we will have access to diffused high resolution image, as well as this low resolution conditioning input.

1177
03:01:02,800 --> 03:01:20,800
Since these two images don't have the same special dimensions, the author proposed to use just simple image resizing algorithms to up sample the input low resolution image and concatenate it with diffused high resolution image and the channel dimension, and they provide

1178
03:01:20,800 --> 03:01:33,800
to the unit diffusion model or the epsilon prediction model and the network is trained to predict the noise that was used when generating the high resolution image.

1179
03:01:33,800 --> 03:01:50,800
This method achieves very high quality results for super resolution. For example, here you can see super resolution results with 64 by 64 pixel input when the output is 256 by 256.

1180
03:01:50,800 --> 03:02:08,800
And you can see that this method here shown in this column achieves a really high quality result compared to, for example, regression models or just simple image resizing algorithms or using by cubic interpolation.

1181
03:02:08,800 --> 03:02:15,800
And you can see that this actually does a good job of generating low level details.

1182
03:02:15,800 --> 03:02:28,800
Another work that I like to mention is called palette, image to mesh diffusion models. This method is also this paper is also proposed by same authors as a previous method.

1183
03:02:28,800 --> 03:02:32,800
So the author's area at home.

1184
03:02:32,800 --> 03:02:41,800
Similar to super resolution, many image to image translation applications can be considered as training a conditional model X given why.

1185
03:02:41,800 --> 03:02:53,800
This is the input image. For example, if you consider colorization problem X is the output color image and why is the grade level input.

1186
03:02:53,800 --> 03:03:04,800
So similar to the previous part, or previous slide we're going to again use, we're going to again trade and condition diffusion model using this objective.

1187
03:03:04,800 --> 03:03:13,800
Similarly, we have again expectation over pairs of input conditioning why, for example, why is again great image X is the output color image.

1188
03:03:13,800 --> 03:03:31,800
We have expectation over epsilon drawn from standard normal distribution expecting over time, we're training a conditional diffusion model that takes input grade level image for some input conditioning time as well as the diffuse output image that we want to generate

1189
03:03:31,800 --> 03:03:37,800
and then the model is trained to predict the noise that was used to generate diffuse samples.

1190
03:03:37,800 --> 03:03:58,800
Similar to previous part, again, we need to give a pair input to the unit model. And here, because for example, if we're attacking the color problem, we're going to have this grade level image and diffuse color image as input to unit and similarly it's trained to predict the noise that was trained for

1191
03:03:58,800 --> 03:04:02,800
generating diffuse sample.

1192
03:04:02,800 --> 03:04:25,800
The others tried their image to mist translation image to mist diffusion model on four different tasks, including colorization in painting jpeg restoration and uncropping, which is basically given this image, they wanted to extend the image and provide the copper part of this, what they called uncropping.

1193
03:04:25,800 --> 03:04:31,800
This paper shows that actually diffusion walls can achieve very good results on these four tasks.

1194
03:04:31,800 --> 03:04:48,800
We should have in mind that this problem this particular paper assumes that you have access to pairs of input and output data so they're they're training a conditional model assuming that they have input image and the output image that we want to generate.

1195
03:04:48,800 --> 03:05:00,800
If we don't make that assumption, what we can do we can potentially take an unconditional model that is trained on that for example natural images and we can modify it for a particular task.

1196
03:05:00,800 --> 03:05:15,800
So, as example of that approach, I want to mention I'd like to mention this paper called iterative latent variable refinement or by LBR for short, that was proposed by Joey et al at ICCB 2021.

1197
03:05:15,800 --> 03:05:31,800
This paper proposes an approach where, given a reference image, the authors like to modify the generative process of the fusion model, such that the output of the diffusion model can be similar to the reference image.

1198
03:05:31,800 --> 03:05:46,800
Again, we have a reference image and we want to modify the reverse generative SDEs or the reverse generative diffusion process, such that we can generate images that correspond to a reference input image.

1199
03:05:46,800 --> 03:06:02,800
So, and the authors and the speaker proposes to do this through using some unconditional model that is not trained for this specific task, it's just the unconditional model is trained to generate realistic for example faces on this slide.

1200
03:06:02,800 --> 03:06:13,800
So, so the basic idea is to modify the reverse denoting process, such that we can pull the samples towards the reference image.

1201
03:06:13,800 --> 03:06:33,800
So here you have the algorithm proposed in this paper, the algorithm starts from capital T goes to one so this is the reverse denoting process at every step we draw a random nodes vector vector from standard normal distribution, we sample from the reverse denoting distribution

1202
03:06:33,800 --> 03:06:43,800
to generate this proposal of this color x prime t minus one is the proposed denotes sample to run from the denoting model.

1203
03:06:43,800 --> 03:06:59,800
Why here represent the reference image so we're going to use the forward diffusion kernel to generate the diffused version of reference image so we're going to go forward in time for this reference image.

1204
03:07:00,800 --> 03:07:10,800
So I did what we want to do we want to make this proposed denotes image x prime t minus one to be more similar to whitey minus one.

1205
03:07:10,800 --> 03:07:21,800
So to do so that this paper proposes this simple operation here fine and represents some low pass filter.

1206
03:07:21,800 --> 03:07:40,800
So this operation is very simple, we have this proposed denotes image x prime t minus one, we subtract the low pass filter applied to this x t minus one, and we add the back low pass filter output of whitey minus one.

1207
03:07:40,800 --> 03:07:53,800
So you can think of this operation as operation that takes x t minus one, the x prime t minus one, this is the proposed denotes image, it removes its low pass filter low frequency content.

1208
03:07:53,800 --> 03:08:02,800
So, here we are removing the low frequency content of x prime t minus one, and the adding back the low frequency content of whitey minus one.

1209
03:08:02,800 --> 03:08:14,800
So basically we're putting the low frequency content of whitey minus one, this is the reference image into x prime, into x prime denotes the proposal sample.

1210
03:08:14,800 --> 03:08:27,800
What we are doing we're basically making sure that in the reverse process, we're generating a sample where the low frequency content is similar to the reference image so the most of the structure is very similar to the reference image.

1211
03:08:27,800 --> 03:08:45,800
So, as I mentioned, fine and it's just a low pass filter. And in order to implement this the artist proposed to use simply done sampling up sampling operation where n represents the, the, the, the down sampling ratio used for in this operation for something

1212
03:08:45,800 --> 03:08:56,800
n is equal to two, it means we're going to just take reference image or take this input down sample by factor of two and then up sample again by factor of two.

1213
03:08:56,800 --> 03:09:02,800
So, which is which corresponds to to a low pass filter operation.

1214
03:09:02,800 --> 03:09:19,800
Here you can see this reference, these two reference images, and how we can generate images with different values for n. So when n equals to four, it means that we actually take this during the generation we don't sample that for low pass filter we don't sample the

1215
03:09:19,800 --> 03:09:30,800
sample images by factor of four, by factor of four. Since this factor is a small, this means that most of the structure in the generation will be similar to reference image.

1216
03:09:30,800 --> 03:09:45,800
So you can see that in fact we're generating an image that is very similar to reference image. And as we increase this and we can see that now different levels of details can, can be generated through the diffusion model and the more global characteristics

1217
03:09:45,800 --> 03:09:55,800
like more global arrangements or low pass, low frequency content of the image is still remains the same as the reference image.

1218
03:09:55,800 --> 03:10:04,800
And now to show that actually you can do this for different tasks image translation given a portrait image they can generate a realistic image that corresponded to the portrait image.

1219
03:10:04,800 --> 03:10:19,800
So they can do paint to image so they can take all painting and generate realistic image, and they can do some simple editing, and for something can add back this watermark into.

1220
03:10:19,800 --> 03:10:31,800
In this part I'd like to talk about how we can take representation learned through diffusion models and use them for sometimes through applications such as semantic segmentation.

1221
03:10:31,800 --> 03:10:43,800
I'm talking about a specific paper called label efficient semantic segmentation with diffusion models that was proposed by one joke at all at ICLAR 2022.

1222
03:10:43,800 --> 03:11:02,800
So it's pretty a paper, propose a simple approach for using the representation trained in diffusion model for semantic segmentation. The others proposed to take input image and diffuse it by adding by following the forward diffusion process, and they only go to small steps

1223
03:11:02,800 --> 03:11:15,800
by following the forward diffusion step which corresponds to adding just a bit of noise into input image, then they pass this diffuse image to the denoising diffusion model the unit model the epsilon prediction model.

1224
03:11:15,800 --> 03:11:27,800
And they, they extract representation form, the internal representation form in this unit at different resolutions of the unit decoder.

1225
03:11:27,800 --> 03:11:41,800
So given these representations the up sample, all these intermediate representation, so that they have the same dimensional spatial dimensionality as input in so we have these up sampling layers.

1226
03:11:41,800 --> 03:11:56,800
We have these feature maps that have the same dimensionality as the input image, and now they simply concatenate all these intermediate feature maps, and they pass them to one by one convolutions that would do semantic segmentation per pixel.

1227
03:11:56,800 --> 03:12:07,800
So you can think of these as a pixel classifiers that just classify each pixel for each semantic object goals or semantic goals.

1228
03:12:07,800 --> 03:12:15,800
So, in order to train this model, that was supposed to use a pre-trained diffusion model and they're only training this component here.

1229
03:12:15,800 --> 03:12:28,800
Up sampling component doesn't have usually any training parameters, but most of the parameters, the additional parameters are basically here in these one by one convolutional networks.

1230
03:12:28,800 --> 03:12:40,800
This paper particularly shows that this approach is labeled efficient using very few labeled instances they can train diffusion models on several datasets as you can see on this slide.

1231
03:12:40,800 --> 03:12:54,800
And the other show that actually diffusion based segmentation models cannot perform mass encoders, can or VAE based models on this test, which is very interesting.

1232
03:12:54,800 --> 03:13:03,800
It shows that actually representation learning diffusion models can be used for downstream applications such as segmentation.

1233
03:13:03,800 --> 03:13:11,800
In this part, I like to talk about the particular paper parameter that is proposed for image editing. It's called SDE edit.

1234
03:13:11,800 --> 03:13:22,800
This paper was proposed by Ming Itala at Stanford University, and it was proposed, it was presented at ICLR 2022.

1235
03:13:22,800 --> 03:13:34,800
What this paper is trying to tackle is that, given this stroke painting, the authors propose a simple approach to generate realistic images that corresponds to that stroke painting.

1236
03:13:34,800 --> 03:13:44,800
The main intuition or the main idea in this paper is that the distribution of real images and stroke painting images, these are two different distribution.

1237
03:13:44,800 --> 03:13:55,800
If you have some smashes, they're not the same and in the, in the data space, they, they are not completely overlapping each other because of these differences.

1238
03:13:55,800 --> 03:14:10,800
But if you follow the forward diffusion process, if we have this distribution realistic images distribution stroke painted, if we follow the forward diffusion process these two distribution will stop having overlaps with each other because of the definition of forward

1239
03:14:10,800 --> 03:14:19,800
distribution, because we know that actually, if you have two distribution and you diffuse the samples in those two distribution, the distribution will start having overlap.

1240
03:14:19,800 --> 03:14:26,800
This forward diffusion simply corresponds to adding noise into input stroke painting.

1241
03:14:26,800 --> 03:14:37,800
Now that we know these two distribution are overlapping, we can use just a generative model train of real images to solve the reverse SDE, reverse denoising SDE.

1242
03:14:37,800 --> 03:14:46,800
That will start from this diffused stroke painting and try to generate a realistic image that corresponds to this noisy input.

1243
03:14:46,800 --> 03:15:01,800
And the authors show that they're actually using a generative model. This is an unconditional model again, train unrealistic images, they can come back to realistic images that where the colors here are very similar to this stroke painting colors.

1244
03:15:01,800 --> 03:15:11,800
So, this is, I think, a very clever idea to take stroke paintings and generate realistic images that correspond to those stroke paintings.

1245
03:15:11,800 --> 03:15:27,800
However, actually, we should have in mind that this train in a conditional setting, however, we should have in mind that this approach mostly relies on the color information in order to take this stroke painting and generate the corresponding image.

1246
03:15:27,800 --> 03:15:36,800
And this is a bit different than, for example, methods that would use the semantic layout, semantic mask of objects in order to tackle this problem.

1247
03:15:36,800 --> 03:15:41,800
So, it has some advantage and disadvantages that we should have in mind.

1248
03:15:41,800 --> 03:16:02,800
Delta shows very interesting results on different data sets. Here you can see stroke paintings for models train on this on bedroom, this on church and set up a and you can see here in these two row, how a generative model using SDE not can be used to generate realistic images that correspond to stroke.

1249
03:16:03,800 --> 03:16:18,800
Lastly, in this part, I'd like to talk about particular work that we did at NVIDIA for adversarial robustness and in this particular work we introduced diffusion models for adversarial clarification.

1250
03:16:19,800 --> 03:16:38,800
So the basic problem we want to add this is that given an adversially perturbed image, we want to see if we can use diffusion models to remove adversarial perturbations and clean this image such that maybe apply classifier on these adversially perturbed image images, we can actually get robust classification.

1251
03:16:39,800 --> 03:16:50,800
So this, the proposed idea here is similar to SDE edit mostly applied for adversarial clarification, given this adversarial perturbed image.

1252
03:16:50,800 --> 03:17:03,800
So what we propose to do is we propose to follow the forward SDE, which correspond to basically adding noise and diffusing the input adversarial perturbed image, using just a forward diffusion cannon.

1253
03:17:03,800 --> 03:17:11,800
So we go to particular times that T star we call, and we, and we simply diffuse the input adversarial perturbed image.

1254
03:17:11,800 --> 03:17:19,800
We know that by adding noise, we can now wash out all these adversarial perturbations that are applied into image.

1255
03:17:19,800 --> 03:17:30,800
Now that we have this noisy image, we use the reverse genitive SDE or reverse noise SDE to start from this noisy input and generate clean image that corresponds to this noisy image.

1256
03:17:30,800 --> 03:17:40,800
And we know that through this process, we can remove all the noise that was injected as well as all the adversarial perturbations presented here in this image.

1257
03:17:40,800 --> 03:17:49,800
So if we have this clean image or purified image, we can just pass it to classifier and hopefully make a robust classification prediction.

1258
03:17:49,800 --> 03:18:00,800
Like any adversarial difference mechanism, we need to be able to attack this model, evaluate our performance, we need to be able to attack this model.

1259
03:18:00,800 --> 03:18:10,800
And we also show how we can attack this model by backfogating end-to-end through classifier as well as our purification algorithm.

1260
03:18:10,800 --> 03:18:24,800
And this involves basically backfogating through this reverse SDE. So we showed in this paper how we can do this and how we can attack this mechanism end-to-end.

1261
03:18:24,800 --> 03:18:33,800
On the left side in this slide, you can see an example of adversarial perturb images and first column on a set of A data set.

1262
03:18:33,800 --> 03:18:40,800
Here, we intentionally increase the magnitude of adversarial perturbations so that they are visible to us.

1263
03:18:40,800 --> 03:18:52,800
These two groups are representing, these two images are representing adversarial perturbation for a smining class and these two represent adversarial perturbation for eyeglasses.

1264
03:18:52,800 --> 03:19:01,800
Here you can see diffuse samples that are generated by following the forward diffusion. This is simply corresponds to sampling from diffusion kernel.

1265
03:19:01,800 --> 03:19:09,800
And then here in these two columns, you can see samples that are generated when we're solving the reverse genetic SDE.

1266
03:19:09,800 --> 03:19:19,800
And as you can see, at time equals to zero, we can remove not only the adversarial perturbations as well as all the nodes that was injected through forward diffusion process.

1267
03:19:19,800 --> 03:19:26,800
And you can see that our generated energy equals to zero are very similar to input clean original images.

1268
03:19:26,800 --> 03:19:37,800
And we can see that the semantic attributes of these images are very similar to semantic attributes of the original images.

1269
03:19:37,800 --> 03:19:47,800
The nice thing about using a generative model for adversarial purification is that these modes are not trained for specific attacks and specific classifiers.

1270
03:19:47,800 --> 03:19:53,800
So at the test time, we can just apply them for unseen adversarial attacks.

1271
03:19:53,800 --> 03:20:05,800
In comparison to the state of dark methods that are designed for similar situation for unseen threats, we actually see that our proposed diffusion prefabrication method outperform these methods by large margin.

1272
03:20:05,800 --> 03:20:15,800
And we believe that, in fact, the fusion models can can be very strong models for designing adversarial purification techniques.

1273
03:20:15,800 --> 03:20:29,800
And this is probably because the fusion models can generate very high quality images and potential can use for removing all the artifacts that are generated by adversarial perturbations.

1274
03:20:29,800 --> 03:20:44,800
This brings me to the end of the second part of applications. Next person will will continue with the third part of applications.

1275
03:20:44,800 --> 03:20:53,800
All right, I will not talk about video synthesis medical imaging 3d generation and discrete state diffusion models.

1276
03:20:53,800 --> 03:20:56,800
Let's get started with video generation.

1277
03:20:56,800 --> 03:21:05,800
There are samples from a text conditioned video diffusion model like Jonathan how at all, where we condition on the string fireworks.

1278
03:21:05,800 --> 03:21:10,800
So I think these samples look pretty convincing.

1279
03:21:10,800 --> 03:21:23,800
So they're actually in general different video generation tasks. For instance, so it's unconditional generation where we want to generate all frames of the video on scratch without conditioning on anything.

1280
03:21:23,800 --> 03:21:33,800
There was a future prediction where we want to generate future frames conditioning on one or more past frames. We can also do past prediction the other way around.

1281
03:21:33,800 --> 03:21:44,800
We can also do interpolation, when we have some frames and we want to generate in between frames, just for instance useful to increase the frame rate of the video.

1282
03:21:44,800 --> 03:21:59,800
All these generation tasks can basically fall under one modeling framework. In all cases, we basically want to learn a model of form setter of xt one to xtk given x, how one to x, how I am.

1283
03:21:59,800 --> 03:22:07,800
So for the t's and tiles, you know the times for frames that you want to generate and for the frames that we condition on.

1284
03:22:07,800 --> 03:22:17,800
So for future predictions, these tiles will already smaller than the t's unconditional generation and we wouldn't have any tiles and so on and so forth.

1285
03:22:17,800 --> 03:22:25,800
Something we see in multiple of these recent works is that they try to learn one diffusion model for everything.

1286
03:22:25,800 --> 03:22:34,800
What they do is they concatenate and combine both the frames to be predicted and the conditioning frames together.

1287
03:22:34,800 --> 03:22:38,800
And then some of these frames are masked out, so once to be predicted.

1288
03:22:38,800 --> 03:22:51,800
And yeah, based on the conditioning frames, those are then generated and varying the masking and conditioning combinations during training, we can train one model for these different tasks.

1289
03:22:51,800 --> 03:23:02,800
In training, we would also tell the model which frames are masked out and we would feed the model time position encodings to encode the times for the different frames.

1290
03:23:02,800 --> 03:23:05,800
That's visualized here, for instance.

1291
03:23:05,800 --> 03:23:13,800
In terms of architecture, these models are still using these units, which we already know from the image based diffusion models.

1292
03:23:13,800 --> 03:23:24,800
We know like small detail. So, of course, now our data is higher dimensional, because in addition to the image and height and width dimensions, as well as the channel dimensions.

1293
03:23:24,800 --> 03:23:32,800
We now also have the time dimensions with the number of frames, so the data is essentially four dimensional.

1294
03:23:33,800 --> 03:23:45,800
One way is to use now 3D convolution instead of 2D convolutions to run convolutions over height width and the frames. This can be computationally expensive.

1295
03:23:45,800 --> 03:23:53,800
Another option is, for instance, to keep special 2D convolutions and use attention layers along the frame axis.

1296
03:23:54,800 --> 03:24:04,800
This has the additional advantage that ignoring those attention layers, a model can be trained additionally on pure image data, which is kind of nice.

1297
03:24:04,800 --> 03:24:17,800
So, let's see some results. It turns out that these video generation diffusion models, they can actually generate really long-term video in a hierarchical manner, which is quite impressive.

1298
03:24:17,800 --> 03:24:26,800
And there, we precisely leverage these masking schemes that we just had, and these generalized video diffusion frameworks.

1299
03:24:26,800 --> 03:24:36,800
So, one thing you can do, for instance, is we generate future frames in a sparse manner by conditioning on frames far back. This gives us long-term consistency.

1300
03:24:36,800 --> 03:24:45,800
And then we interpolate the in-between frames afterwards. So, we kind of generate the video in a stage-wise hierarchical manner.

1301
03:24:45,800 --> 03:24:57,800
And with that, it's possible to actually generate really long-length, one-hour coherent videos, which is quite impressive. So, here are some samples from this recent Harvey et al. work.

1302
03:24:57,800 --> 03:25:08,800
All right. Let us now talk about another application of diffusion models, which is solving inverse problems in medical imaging, another very relevant application.

1303
03:25:08,800 --> 03:25:16,800
So, medical imaging may refer to computer tomography or magnetic resonance imaging.

1304
03:25:16,800 --> 03:25:27,800
In those cases, we're basically interested in an image X, but that is not what we're actually measuring from the CT scanner or MI scanner.

1305
03:25:27,800 --> 03:25:36,800
So, let's consider the measurement process. For instance, in computer tomography, the forward measurement process can be modeled in the following form.

1306
03:25:36,800 --> 03:25:45,800
In the image, we are basically performing a radon transform, which gives us a sinogram, and then maybe this is sparsely sampled.

1307
03:25:45,800 --> 03:25:58,800
So, we end up, this is sparsely sampled sinogram Y. And now, the task is that needs to be solved to reconstruct the image given this measurement Y. So, this is an inverse problem.

1308
03:25:58,800 --> 03:26:10,800
This is a similar and magnetic resonance imaging, just that the forward process is now basically modeled with a Fourier transform, which is then sparsely sampled.

1309
03:26:10,800 --> 03:26:25,800
So, and this is where diffusion models now come in. So, they can actually be really used in this task. And the highly ideal is here to learn a generative diffusion model as a prior over the images we want to reconstruct.

1310
03:26:25,800 --> 03:26:36,800
And while sampling from the diffusion model, we guide synthesis condition while conditioning on the sparse observations that we have. This is the idea.

1311
03:26:36,800 --> 03:26:45,800
And it turns out that doing this actually performs really, really well. And even this outperforms even fully supervised methods sometimes.

1312
03:26:45,800 --> 03:27:00,800
Specifically, the thing is, when we train this fusion model over these CT or MRI images, we really just need the images, we do not need paired image measurement data to train this.

1313
03:27:00,800 --> 03:27:14,800
So yeah, there is actually a lot of work in that direction because it's a really high impact application, of course, and there are some citations that you are interested in learning more about this.

1314
03:27:14,800 --> 03:27:25,800
So let's move on to the next application topic, which is 3D shaped generation. Also, 3D shaped generation has recently been tackled with diffusion models.

1315
03:27:25,800 --> 03:27:33,800
So let us consider this work by zoo at our, for instance, here, 3D shapes are represented as point clouds.

1316
03:27:33,800 --> 03:27:54,800
And this has the advantage that they can be diffused really easily and intuitively, we see this here at the bottom right so where the diffusion actually goes from the right to the left, we have a bunch of points and they are perturbed and 3D towards this, yeah, Gaussian noise ball kind of, and the generation goes into the other direction.

1317
03:27:54,800 --> 03:28:12,800
So in those cases, the architectures that we use to implement the denoiser network are like typical point state of the art modern point cloud processing networks like no point net advanced versions of point and point boxes and so on and so forth.

1318
03:28:12,800 --> 03:28:14,800
How does this look like then.

1319
03:28:14,800 --> 03:28:19,800
Another animation.

1320
03:28:19,800 --> 03:28:26,800
I think this is quite nice. So, yeah, we can generate these pretty good shapes.

1321
03:28:26,800 --> 03:28:39,800
We can also train conditional shape completion diffusion models very condition, for instance, unlike that or like some sparse points like this, and then complete those shapes.

1322
03:28:39,800 --> 03:28:52,800
So in the multimodal fashion, for instance, in this example we have some legs of the chair given. And now we have like different plausible completions of the chair here.

1323
03:28:52,800 --> 03:29:10,800
Another thing that is also quite cool is that he works on real data. I think here the model was trained only on synthetic shape net data, and yet we can see the model images that we take these images and generate plausible 3D objects.

1324
03:29:10,800 --> 03:29:15,800
Very nice.

1325
03:29:15,800 --> 03:29:31,800
Finally, I would like to talk about discrete states to fusion models. This is less of an application but slightly different type of diffusion model, but I think it is worth mentioning as part of this tutorial.

1326
03:29:31,800 --> 03:29:44,800
So, so far, we have only been considered continuous diffusion entity noising processes, which I mean with that is, we basically kind of assume our data is of a continuous nature.

1327
03:29:44,800 --> 03:30:00,800
And we could add a little bit of Gaussian noise to it in a meaningful way. So both our fixed forward diffusion process and also our reverse generative process are usually were usually implemented as Gaussian distributions like here.

1328
03:30:00,800 --> 03:30:18,800
But what if our data is discrete, categorical, then continuous perturbations are not meaningful, they are not possible. Imagine, for instance, our data as text data, you know, pixel wise segmentation labels, or discrete image encodings.

1329
03:30:18,800 --> 03:30:33,800
Yeah, if our data is discrete, adding Gaussian continuous noise to it doesn't really make much sense. So, can we also generalize this diffusion concept to like discrete state situations.

1330
03:30:33,800 --> 03:30:47,800
In fact, there are categorical diffusion models. And in those cases, the forward diffusion process or like the perturbation now is defined using categorical distributions.

1331
03:30:47,800 --> 03:31:04,800
So consider perturbation corner q of xt given xt minus one, that is supposed to put up the discrete data. So this can now be a categorical distribution, where the probability to sample one of the teachers is now given by some transition

1332
03:31:04,800 --> 03:31:15,800
matrix q multiplied together with the state we are in xt minus one. So the probability to sample like the new state xt.

1333
03:31:16,800 --> 03:31:28,800
So this xt is usually a one-hot state factor describing the state we're in. And yeah, this transition matrix multiplied with will then give us probabilities to sample the next state.

1334
03:31:28,800 --> 03:31:38,800
So with that we can put up complex distributions categorical distributions towards like very random discrete distributions.

1335
03:31:38,800 --> 03:31:48,800
We choose this transition matrix accordingly. So for instance, in this example, if we look at the right, this may now be a complex data distribution.

1336
03:31:48,800 --> 03:31:56,800
We can perturb this towards a uniform discrete distribution over these three different states 13123.

1337
03:31:56,800 --> 03:32:08,800
So again, the reverse process for generation, and which is then implemented through a neural network, we can also parameterize as a categorical distribution.

1338
03:32:08,800 --> 03:32:14,800
In fact, there are different options for this perturbation process, this forward perturbation process.

1339
03:32:14,800 --> 03:32:25,800
We can use uniform categorical diffusion where we pull everything towards a uniform distribution over the different categories like I've just shown.

1340
03:32:25,800 --> 03:32:35,800
We can also progressively kind of mask out the data where we pull everything into one particular state. We can also analytically sample from such a distribution.

1341
03:32:35,800 --> 03:32:39,800
So it's also well suited for diffusion model.

1342
03:32:39,800 --> 03:32:50,800
We can also tailor our diffusion processes to ordinal data and use something like discretized Gaussian diffusion process that's also possible.

1343
03:32:50,800 --> 03:32:53,800
How does this look like for instance.

1344
03:32:53,800 --> 03:33:10,800
So here now I have the data distribution. It's a bit complex, but so this is basically each pixel of this image represents one categorical variable, and now the color of this pixel represents which teacher we are in.

1345
03:33:10,800 --> 03:33:22,800
So now if I would do like this uniform categorical diffusion, I would kind of, you know, yeah, would look like this, where I would transition into different states everywhere in the image.

1346
03:33:22,800 --> 03:33:31,800
I could also do something like Gaussian diffusion where it's more like this ordinal thing that's more based transition to neighboring states.

1347
03:33:31,800 --> 03:33:41,800
And then there was also this absorbing diffusion where I kind of progressively mask out or absorb my state sort of different ways to do this.

1348
03:33:41,800 --> 03:33:52,800
And then in the reverse process may for instance look like this. So here on the far right, this is a stationary distribution of this categorical distribution from which I can sample analytically.

1349
03:33:52,800 --> 03:34:02,800
And then denoising kind of progressively noises is bad towards the data distribution.

1350
03:34:02,800 --> 03:34:16,800
So yeah, one can use this and some papers have explored such discrete state diffusion models. For instance, we can also apply this on images by modeling the pixel values of images as discrete states to be in.

1351
03:34:16,800 --> 03:34:29,800
So this is a start from uniform uniformly distributed pixel values right here from this all gray kind of state or mask out state.

1352
03:34:29,800 --> 03:34:49,800
Another application is to use this in a discrete latin space. So in this work, for instance, images are encoded using a vector quantization techniques into visual tokens in a discrete latin space and then we can use something like discrete diffusion models and similar techniques to model the distribution

1353
03:34:49,800 --> 03:34:52,800
over the visual tokens.

1354
03:34:52,800 --> 03:34:56,800
This is also something one can do.

1355
03:34:56,800 --> 03:35:07,800
We can also use discrete state diffusion models to generate segmentation maps, which are also categorical distributions in pixel space.

1356
03:35:07,800 --> 03:35:23,800
And yeah, that concludes my part. And with that, I would like to pass a mic back to ours, we will now conclude our tutorial. Thank you very much.

1357
03:35:23,800 --> 03:35:33,800
Thank you for being with us. This basically brings us to the last part conclusions, open problems, and final remarks.

1358
03:35:33,800 --> 03:35:46,800
So today was a big day, we learned about diffusion models. At the beginning of this video, I started talking about the noise and diffusion prophecy models, which is a part, which is a type of discrete time diffusion models.

1359
03:35:46,800 --> 03:36:05,800
I showed you how these discrete time diffusion models can be described using two processes, a forward diffusion process that starts from data and generates those by adding those into the input, and then reverse the noise and process that learns to generate data by starting

1360
03:36:05,800 --> 03:36:24,800
from noise and denoising the input image one step at a time. I also talked about how we can train these diffusion models by simply generating diffuse samples and training network to predict that to predict the noise that was used to generate diffuse input images.

1361
03:36:24,800 --> 03:36:33,800
In the second part, Carson talked about the score-based generative modeling with differential equation, which corresponds to continuous time diffusion models.

1362
03:36:33,800 --> 03:36:51,800
Specifically, Carson talked about how we can consider diffusion models in the limit of infinite number of steps and how we can define or how we can describe these forward and reverse processes using stochastic differential equations or STEs.

1363
03:36:51,800 --> 03:37:07,800
I also talked about probability flow ordinary differential equations or ODEs, which describe a deterministic mapping between noise distribution and data distribution.

1364
03:37:07,800 --> 03:37:23,800
Another thing about working with stochastic differential equations or ODEs or ordinary differential equations is that we can actually use the same training that was used for training different discrete time diffusion models in the previous slide.

1365
03:37:23,800 --> 03:37:35,800
However, at the test time, we are free to choose different discretization or different OD or ST solvers that have been studied widely in different areas of science.

1366
03:37:35,800 --> 03:37:46,800
And those are basically to change the sampling time by using, for example, OD or ST solvers that don't require a lot of functional evaluations.

1367
03:37:46,800 --> 03:37:52,800
In the third part, Ruchi talked about advanced topics in diffusion models.

1368
03:37:52,800 --> 03:38:05,800
She mostly focused on accelerating sampling from diffusion models and she studied this from three different perspectives, including how we can define forward processes that accelerate sampling from diffusion models,

1369
03:38:05,800 --> 03:38:14,800
how we can come up with better reverse processes, or how we can come with better denoising models that allows us to access sampling from diffusion models.

1370
03:38:14,800 --> 03:38:24,800
Beyond that, she also talked about how we can scale up diffusion models to generate high resolution images in conditional and conditional setting.

1371
03:38:24,800 --> 03:38:37,800
Actually, especially talk about cascaded models and guided diffusion models that are heavily used in the current state-of-the-art image to text diffusion models, just imagine.

1372
03:38:37,800 --> 03:38:55,800
After talk about fundamental topics, all three of us talked about various computer vision applications that have been recently proposed and mostly applications that rely on diffusion models at their core recently.

1373
03:38:55,800 --> 03:39:05,800
So, now that we know about diffusion models and we know how we can use these models in practical applications, let's talk about some open problems.

1374
03:39:05,800 --> 03:39:20,800
I do hope that now we could make you interested in this topic and now that you know the some fundamental in this area, maybe you can think about open problems that exist in this space and together we can tackle some of these.

1375
03:39:20,800 --> 03:39:31,800
The first problem I want to, first of problems I want to mention are more on the technical side and later I will talk about more applied questions that arises in practice.

1376
03:39:31,800 --> 03:39:44,800
So, if you remember at the beginning of the talk, Mouskarsen and I talked about how diffusion models can be constructed as a special form of MIEs or continuous time normalising flows.

1377
03:39:44,800 --> 03:40:02,800
We exactly don't know why diffusion models do much better than VAEs and continuous time normalising flows. If we can understand this, maybe we can take the lessons learned from diffusion models and why they do so much better than VAEs and continuous time normalising flows in order to improve these frameworks,

1378
03:40:02,800 --> 03:40:10,800
meaning we can maybe use the lessons learned from diffusion models to improve VAEs or normalising flows.

1379
03:40:10,800 --> 03:40:27,800
Even though there has been a tremendous progress in the community for accelerating sampling from diffusion models, we can still do, in the best case scenario, we can still do, actually, the sampling using four to 10 steps on the small matrices such as Cypher 10.

1380
03:40:27,800 --> 03:40:43,800
However, the main question that remains on how we can get one step samples for diffusion models. And this can be very crucial for interactive applications where a user interacts with the diffusion model and this

1381
03:40:43,800 --> 03:40:52,800
we can reduce the latency that usually users observe when they are using generative models.

1382
03:40:52,800 --> 03:41:07,800
Some of the existing problems have to define one step samplers and part of the solution might be to come up with a better diffusion process that are intrinsically faster to generate sample from.

1383
03:41:07,800 --> 03:41:26,800
Diffusal models, very similar to VAEs or GANS, can be considered as latent variable models, but the latency space is very different. For example, GANS, we know in the latent space often have semantic meaning and using latent space manipulations, we can actually come up with image editing or image

1384
03:41:27,800 --> 03:41:43,800
manipulation frameworks. But in diffusion models, the latent space does not have semantics, and it's very tricky to come up with latent space semantic manipulation diffusion models. So part of problem here is how can we define

1385
03:41:43,800 --> 03:41:52,800
semantically meaningful latent space for diffusion models that allow us to do semantic manipulations.

1386
03:41:52,800 --> 03:42:12,800
In this talk, we mostly focus on generative applications, but one open problem is how we can use diffusion models for discriminative applications. For example, one way of using diffusion models might be for representation learning, and we might be able to tackle high level tasks such as image classification

1387
03:42:12,800 --> 03:42:24,800
versus low level tasks such as semantic image segmentation. And these two may require different traits of when we're trying to use diffusion models to address these.

1388
03:42:24,800 --> 03:42:38,800
Another group of applications that may benefit from diffusion models is uncertainty estimation. One question is how can we use on diffusion models to do uncertainty estimation in downstream discriminative applications.

1389
03:42:38,800 --> 03:42:59,800
And finally, one question that remains open is how we can define joint discriminator generator, sorry, joint discriminator generator models that not only classify images or input, they also can generate similar inputs.

1390
03:42:59,800 --> 03:43:14,800
So the committee mostly have been using unit architectures for modeling the score model in the score function in diffusion models. But one question is whether we can go beyond units and come up with better architectures for diffusion models.

1391
03:43:14,800 --> 03:43:32,800
One specific open area is how we can feed time input or other conditioning into diffusion models, and how we can potentially improve the sampling efficiency or how we can reduce the latency of sampling from diffusion models using better network design.

1392
03:43:32,800 --> 03:43:48,800
So far in this talk, we mostly focused on image generation, but we may be interested in generating other types of data. For example, 3D data that has different forms of representation, for example, it can be represented by stance function,

1393
03:43:48,800 --> 03:44:07,800
meshes, voxels, or volumetric representation. Or we might be interested in generating video text graph, which have their own characteristics. And given these characteristics, we actually may need to come up with specific diffusion models for these particular modalities.

1394
03:44:07,800 --> 03:44:22,800
One area of research is to do composition and controllable generation, and this will allow us to go beyond images and be able to generate larger scenes that are composed of multiple, for example, objects.

1395
03:44:22,800 --> 03:44:37,800
And also this will, as a technical example, allow us to have fine grain control in generation. So one interesting open problem is how we can achieve composition and controllable generation using diffusion models.

1396
03:44:37,800 --> 03:44:54,800
Finally, I think if we look back, look back to the vision community and the problems that we solved in the past few years, we see that most of the applications try to solve, most of the applications that rely on generative models, they try to solve,

1397
03:44:54,800 --> 03:45:07,800
and some solve with generative adversarial networks. So maybe it's a time for us to start revisiting those applications and see whether they can benefit from the, the nice properties that diffusion models have.

1398
03:45:07,800 --> 03:45:17,800
So one open question is which applications will benefit most from diffusion models, given that we have such amazing strong tool.

1399
03:45:17,800 --> 03:45:37,800
Let's go to the final slide. I want to say thank you for being with us today. This was, this is a very long video, and I do hope that we could provide some useful and fundamental background on diffusion models and how often are used in practice.

1400
03:45:37,800 --> 03:45:47,800
All of us are active on Twitter, if you are interested in knowing about follow up works that we, we build on diffusion models, please make sure that you follow us.

1401
03:45:47,800 --> 03:45:56,800
And lastly, I want to mention that all the content on this week and this video, including slides will be available on this video on this website.

1402
03:45:56,800 --> 03:46:13,800
If you happen to enjoy this video, I would like to ask you to share this video with your colleagues and collaborators, and hopefully together, we can come with more people to start looking into diffusion models and applying them to various interest applications.

1403
03:46:13,800 --> 03:46:14,800
Thanks a lot.

