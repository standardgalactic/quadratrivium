{"text": " Welcome to our tutorial on denoising diffusion-based gender modeling, foundations and applications. My name is Arash Vathlet, I'm a Principal Research Scientist with NVIDIA Research, and today I'm very excited to share this tutorial with you along with my dear friends and collaborators including Karsten Kreis, who is a Senior Research Scientist with NVIDIA, as well as Ruchi Gau, who is a Research Scientist with Google Brain. Before starting the tutorial, I'd like to mention that the earlier version of this tutorial was originally presented at CVPR 2022 in New Orleans. This tutorial received a lot of interest from the research community, and given this interest we decided to record our presentation again after the conference, and we'd like to share this broadly with the research community through YouTube. If you happen to watch this video, and you enjoy this video, we would like to encourage you to share this with your friends and collaborators, and hopefully together we can create more interest around denoising diffusion-based gender models. If you're watching this video, most likely you'll find me with deep gender learning. In deep gender learning, we assume that we have access to cliques of samples drawn from unknown distribution. We use this cliques of training data to train a deep neural network. If everything goes smoothly, at the test time, we can use this deep neural network to draw new samples that would hopefully mimic the training data distribution. For example, if we use these images of cats to train our deep neural network at the test time, we do hope that we can also generate images of cute handsome cats, as you can see in the bottom. Deep gender learning has many applications. Mostly the main applications are content generation that have use cases in different industries, including, for example, entertainment industry. Deep gender learning can be used for representation learning as well. If we have a deep gender model that generates really high quality images, mostly the internal representation in that model can be used also for downstream discriminative applications, such as semantic image segmentation, as you can see in this slide. Deep gender models can be also used for building artistic tools. In this example that you can see on this slide, we have a tool that can be used by an artist who happens to be just a six-year-old kid to draw a semantic layout of a scene. This tool can take the semantic layout and generate a corresponding high-quality image that has the same semantic layout. If you're a researcher and you watch the landscape of deep gender learning, you're going to see that this landscape is filled with various frameworks ranging from generative adversarial networks to virtual autoencoders, energy-based models, and autoregressive models and normalizing tools. Historically, the Computer Vision Committee has been using generative adversarial networks as one of their main tools for training generative models. In this talk, we would like to argue that there is a new and powerful generative framework called the Noise and Diffusion Models. These models obtain very strong results in generation, and we hopefully want to convince you that these models are very strong and can be used for various applications. Hopefully, in this talk, we're going to provide you with some foundational knowledge that requires for using these models in practice, and we're going to even talk about how these models are currently used for tackling some of the interesting applications that exist in the Computer Vision Committee. As I mentioned earlier, the Noise and Diffusion Models have recently emerged as a powerful generative model of performing cancer. In these two papers shown on this slide, one from OpenAI on the left side and one from Google on the right side, researchers observe that you can use the Noise and Diffusion Models to train generative models on challenging datasets such as ImageNet, and the results generated by these models is often very high-quality and very diverse, something that we haven't seen previously with other generative models such as GANs. The Noise and Diffusion-based models have already been applied to interesting problems such as super resolution. In this slide, you can see a super resolution framework that takes a low-resolution Image64x64 dimension and generates high-resolution image in 1024x1024 pixels. This results show that actually, the Super Resolution Models built on top of the Noise and Diffusion Models can generate very high-quality, diverse models. If you're on social media, you've been probably having a hard time not noticing a lot of excitement that was created around Dolly 2 and Imagine. These two frameworks are examples of text-to-image generative models that take text as input, and they generate a corresponding image that can be described using that text. Using their core, these models use the Noise and Diffusion generative models, and on the left side, you can see for example Dolly 2 built by OpenAI can actually create this image of teddy bears skateboarding in Times Square, and on the right side, you can see Imagine can generate images of multiple teddy bears celebrating their colleague's birthday while sitting behind a cake that looks like pizza. This is impressive. These models can generate very high-quality, diverse images, and they only take text as input. Today, not only are we going to talk about diffusion models, we're going to even talk about how you can use diffusion models to create such models. Towards the end of the video, Rucci will talk about these applications. Today's program consists of six main parts, besides introduction and conclusion. The first three parts that are shown in green are mostly the technical components of the program. I'm going to start with part one. I will talk about the Noise and Diffusion probabilistic models, and after me, Carson will talk about the score-based generative modeling with differential equations, and after us, Rucci will talk about advanced techniques, mostly around accelerated sampling and condition generation. These parts, each one of them would be roughly around 65 minutes to 45 minutes. After these parts, we're going to have three short parts around applications, and mostly computer vision applications that have been recently used diffusion models in their core to tackle various deep generative learning-related applications. Finally, we're going to have a very short segment where I will talk about conclusions of open problems and final remarks. Before starting, I'd like to mention that our slides, videos, and some content will be available on this website, so I'd like to encourage you to bookmark this website. We're hoping to add more content in the future to this website. Before starting the presentation, I'd like to just mention that we did our best to include as many papers that we could, and we thought that it could be interesting to the community. However, due to limited time and capacity, of course, we could not include every paper. So if there's a particular paper that you are passionate about, you're very excited about it, and you would like to be included in this tutorial, we apologize that we couldn't do that, and what we encourage you to send us an email, let us know that there was a paper that would be interesting to have in our program, and hopefully, in the future versions of this tutorial, we will try to include those papers as well. With that in mind, I will start the first segment, the noise and diffusion probabilistic models. So part one, the noise and diffusion probabilistic model. Here you can see an image of three cute dogs who are trying to understand the noise and diffusion probabilistic models, and as you can see, they're a little bit lost. So let's go over these models and discuss how these models can be generated. So using diffusion models, officially consists of two processes, a forward diffusion process that gradually adds noise to input. This process is shown on this figure from left to right. It starts from image of this cute cat. His name is Peanut, he's my cat. We can start from him, and we're going to add noise to this image one step at a time. The forward diffusion process does this in so many steps such that eventually on the right side, we converge to white noise distribution. The second process is the reverse denoising process that learns to generate data by denoising. This process is shown on this slide from right to left, and it starts from white noise and it learns to generate data on the left side by denoising. So this process will take a noisy image, and it will generate a less noisy version of it, and it will repeat this process such that it can convert noise to data. So we're going to dig deeper into these two processes, and we will see how we can define these processes formally. Then the forward diffusion process, as I said, starts from data and generates these intermediate noisy images, by just simply adding noise one step at a time. At every step, we're going to assume that we're going to use a normal distribution to generate this noisy image condition on the previous image. This normal distribution will have a very simple form. We're going to represent this normal distribution using q that takes this x at the previous step and generate x at the current step. So it takes, for example, x1, and it generates x2. As I said, it's a normal distribution over the current step, xt, where the mean is denoted by this square root of one minus beta t times the image at the previous step, and this beta t representing the variance. For the moment, assume that this beta t is just simply a very small positive scalar value. It can be like 0.001, some selectors. Here, this normal distribution basically takes the image at the previous steps. It rescale this image, the pixel values on this image, by the square root of one minus beta t, and it adds a tiny bit of noise where the variance of noise is beta t. So this is just a diffusion kind of we can call per time step, because we had this very simple form per time step, like per step, in order to generate xt given xt minus one. Now, we can also define the joint distribution for all the samples that will be generated in this trajectory, starting from x1, all the way going to x capital T. Capital T represents the number of steps in our model, and the joint distribution of all these samples, condition of this input image of peanut, will be the product of conditionals that are formed at each step. So this just represents the joint distribution of all the samples that will be generated on this trajectory using this Markov process. This is a Markov process that generates one step, that generates examples one step at the time, given the previous examples. Now that we know how we can generate samples one step at a time, you may ask me, how can I now just take this input image and jump to particular time the step? Do I need to sample, generate samples one step at a time, or can I just take this x0 and generate xt, or x4, for example, here, just directly? Because in the forward process, we're using a very simple Gaussian kernel to diffuse the data, we can actually show that because of this simple form, we can first define this scale. This scalar alpha bar T is the product of one minus betas from time to step one all the way up to current step T. This is just defined based off the parameters of the diffusion kernel, and having defined this alpha bar T, now we can define a Gaussian kernel or the diffusion kernel that will generate xt, even x0. So, for example, we can now generate using this Gaussian kernel, we can sample from x4 given x0, this would be again a normal distribution where mean is same as the input image, really this square root of alpha bar T defined here alpha bar, and then the variance is also one minus alpha bar T. So, just remember that these betas are just parameters of the diffusion process, we can compute this alpha bar T very easily, and then we can sample from xt given x0 using this normal distribution, and this we're going to call this diffusion kernel that diffuses input at time step zero to time step xt. Recall that if you want to sample from just a simple normal distribution, you can use the reparameterization trick. So, if you want to draw samples from xt, you can just set xt to mean plus some white nose epsilon that is drawn from standard normal distribution, rescaled with this square root of one minus alpha bar T, which represents just a standard deviation of this normal distribution. So, using this we can just simply generate samples at time step T given samples at time step zero, so given x0 we can just diffuse it easily to time step T. Beta T values are important, these are basically, we're going to call beta T values as the noise schedule that defines how we're diffusing the data, and this noise schedule, this noise schedule, designs such that alpha bar T, this alpha bar at the very last step, would converge to zero, and when this converges to zero, if you just set alpha bar T to zero here, you're going to see that because the way that the forward diffusion process is defined, this diffusion kernel at last step given the x capital T given x0 would be just can be approximately using normal distribution, standard normal distribution. This basically means that at the end of this process, diffuse data will have just a standard normal distribution, this is something that we will need later when we want to define a generative point. Now that I've talked about this forward process, like how we can diffuse the diffusion kernel that diffuses data, let's talk about the marginal diffuse data distribution, let's talk about what happens to data distribution as we go forward in the diffusion process. So, have in mind that diffusion kernel that generates xt given x0 is different than the diffuse data distribution, so we're going to use qxt to represent diffuse data distribution, and in order to obtain this diffuse data distribution, we first need to form the joint over clean data input data x0 and diffuse data xt, this joint simply can be defined as product of input data distribution qx0 times this diffusion kernel, which is just a simple normal distribution, and now we can marginalize at x0, we can just integrate at x0, and this will give us marginal data diffuse data distribution at times the T. If we just consider a very simple one dimensional data, and we hear on visualizing on visualizing the diffuse data distribution at different time steps, on the left side we have data distribution at times zero, why access represents this just one dimensional random variable and this x axis represents the PDF probability density function of this random variable at time step zero, this is just the data distribution visualized for one toy example, one dimensional toy example. Here you can see the visualization of diffuse data distributions, and as you can see, as we go in the forward process, we just take this data distribution, we're making kind of this distribution smoother and smoother as we go forward in time, and eventually it becomes so smooth that we can just represent this distribution using standard normal distribution, zero mean unit variance normal distribution. As you see this smoothing process, we can actually show mathematically this, the diffusion kernel in the forward process, this diffusion kernel here is kind of applying a Gaussian convolution to the data distribution, so this smoothing process can be just represented as Gaussian convolution, a convolution in the sense of like signal processing convolution that takes input data distribution makes it smoother and smoother. However, we should have in mind that we actually don't have access to these, to input data distribution and intermediate diffuse data distribution practice, usually we only have samples from data distribution, we don't have access to the explicit form of the probability density function of data distribution, right? So even though we don't have access to this distribution or all the intermediate diffuse distributions, we know how to draw samples from diffuse data distribution, so in order to generate data from diffuse data distribution, we can just simply sample from training data x0, and then we can just follow the forward diffusion process, sample xt to an x0, in order to draw samples from xt, and this is basically the principle of ancestral sampling that you can just basically use in order to generate data, for example, at times that t, you can just use the training data, sample from training data, diffuse it and sample at xt using diffusion kernel, and that will give you samples from marginal data distribution. Okay, so so far we talked about forward process and how this forward process just smoothens the data distribution, now let's talk about how we can define a generative model out of this. In order to generate data in diffusion models, what we can do, we can start from this base distribution at the end, we know that the diffuse data distribution would converge to 0 in unit variance, standard normal distribution, so we can sample from standard normal distribution and we can follow the reverse process, where at every step we can now sample from the denoising model that generates the less noisy version of data given current step, so this is the reverse model where we now use the true denoising distribution to sample from xt minus 1 given xt. So we just need to start from xt and just use this true denoising model to generate samples at time step 0. So the algorithm will be something like this, we sample x capital t from standard normal distribution and we sample iteratively from xc minus 1 using this true denoising distribution. The problem here is that in general denoising distribution xt minus 1 given xt is intractable, we don't have access to this distribution, we can use Bayes' rule to show that this distribution is proportional to the product of the marginal data distribution at xc minus 1 times this diffusion at t, this kernel is simple, it's just Gaussian distribution, however this distribution marginal data distribution is intractable and in general because of this, this product is also intractable so we don't have it in closed form. Now that we don't have it in closed form, you may say can I approximate this denoising distribution from data and if yes, what is the best form I can use to represent that denoising model. So the good news is, yes, we can try to train a model to mimic this true denoising distribution xt minus 1 given xt and if you want to model that, the best model that you can use, the statistical model you can use to represent this denoising model is actually a normal distribution if in the forward process we use very small noise or the variance of the noise that we're adding at each step is very small. If we know the forward process at very small amount of noise, we know theoretically that actually the reverse process can be also approximated using a normal distribution, so this denoising model can be represented using denoising model. So now that we know this, we can actually define a parametric model to mimic or to train this true denoising model. So this formally defined our parametric reverse denoising process. Remember that reverse denoising process starts from noise and generous data by denoising once at a time. We're going to assume that the distribution of data at time is the capital T at the end of the forward process, so it's the beginning of the reverse process. It will be standard normal distribution here as soon as data has a standard normal distribution. We define this denoising distribution. This is a parametric denoising distribution as samples from XT minus from given XT. We're going to assume that it also can be represented using normal distribution where now mean here is parametric. It's a deep neural network that takes noisy image XT and predicts the mean of the less noisy version, less noisy image. So this neural network is the trainable component and then we have also the variance per time step. This is just think of the sigma T squared as just some scalar value that represents the variance per time step. And for now just assume that it's just some parameter or we have access to it. We're going to talk about it later. But the most important part is this mean parameter. Remember, this is the trainable network and it takes a noisy image at XT and predicts the mean of less noisy image at XT minus 1. Because it takes an image and predicts an image, we can actually model this using a unit that has the same input and output shape. Or you can think of it as just a denoising autoencoder that denoises the input image to less noisy level basically. And we're going to talk about the architecture of this denoising model, this new filter. Now that we have these definitions, we can define the joint distribution on the full trajectory. This joint distribution is the product of the base distribution at step XT and the product of conditionals at all these steps at T steps where the condition comes from our denoising model. This is again just a Markov process and we can define the joint distribution on the full trajectory by the product of base distribution and the product of these individual conditionals on each denoising step. Okay, so now so far we talked about the forward process, the reverse process. Now let's talk about how we can train these models, how we can train the denoising model. For training the denoising model, we're going to use variation upper bond that is mostly or commonly used for training by autoencoders. In the variation upper bond, ideally we want to optimize marginal likelihood of the data under our parametric model. Here we have this expectation over training data distribution, where we are computing the expected value of the low to likelihood that our trainable model gives to data distribution. Unfortunately, this is interactive, we don't have access to this quantity here, but we can define variation upper bond, where now we have this expectation over training data, we have expectation over samples drawn from the forward process. This is forward process, you can think of it as encoder in VAE, that samples from latent space, so you can think of these as latent variable x1 to capital T. And we have this log ratio here where the nominator is the joint distribution of the samples on the full trajectory that is given by our denoising model, P theta, and in the denominator we have the likelihood of the samples generated by encoder. This is basically the same objective or same training object, we would see variation of bond in variation autoencoders. This is exactly the same. The assumption is that the forward process is kind of like an encoder and the reverse process is the genitive model in VAE. These two papers, by the way, our papers, the links on our slides are clickable, so if you want to check these papers just find our slides and click on these references and you're going to find the paper. So these two papers showed that this variation bond can be decomposed to several times that we're going to go one by one. The first term is just simply the KL divergence from the diffusion kernel in the last step, x capital T given x0 to this base distribution xT. Remember by definition, the diffusion kernel for x capital T converges to standard normal distribution, which is same distribution we assume for xT. So we can completely ignore this term because by definition, forward process defines such that at the end of process, samples converge to 0 million units of mass distribution. So we can ignore this term. We have this KL term that I'm going to go into details, and then we have this term that can be considered as the reconstruction term in VAEs. This just measures the likelihood of input clean image, even the noise image at first step under our denoising model. This term is also very simple, and it has actually structure very similar to this other term. So for moment, just assume that we can compute this very easily, and we just ignore for a moment. And mostly we just need to focus on this term, which is the most kind of important term here. This is the KL divergence between this Q, xT minus 1 given xT and x0 to our denoising model. This is our parametric denoising model, which is xT minus 1 given xT. This Q distribution is a pretty well not new distribution. We're going to call it the tractable posterior distribution. These samples from xT minus 1, less noisy image, condition on noisy image, xT, and clean image at time step 0. So it's kind of like you have clean image, you have noisy image, and you want to predict what would be the less noisy variation of this image if you knew what was the starting point, what was the x0 for generating this noisy image. It turns out that this distribution is also very simple because the forward process is just Gaussian distribution. The posterior distribution here is also very simple. This is a pretty good posterior distribution, it's condition on x0. We know what is the starting point. So this distribution is also normal distribution with mean expressed here. This mean is just simply weighted average of the clean image and the noisy image at xT. So it's actually very interesting if you have clean image and you have a noisy image. If you want to predict what is the distribution of xT minus 1, the mean that this will be normal, this expression is just normal. And the mean of that normal is just the weighted average of these two images where the weights come from our diffusion process, parametric diffusion process. Very simple expression and the variance of this distribution is also defined based on the parameters of the forward process. So you can think of it like this beta tilde t can be computed very easily from the parameters of the diffusion process. So that we know none of this here, it's interesting. It's interesting, so we have the scale divergence from this distribution trackable posterior distribution, which is normal. And then this denoising model, which we assume that is normal as well. So, now that we have two normal distributions, the scale divergence there. So I'm just writing down the same scale divergence again. The scale divergence can be computed analytically for two normal distributions. We can show that the scale divergence simply just boils down to the squared L2 distance between the mean of this trackable posterior and the mean of the denoising model. Right, plus some constant terms that we can ignore these constant terms do not depend on any trainable power. So this just basically scale divergence is very interesting. It just boils down to the difference between the mean of this denoising, sorry, this mean of the trackable posterior and our denoising model, parametric denoising model, which is represented by mean theta. And this weight is one over two sigma t squared. It's just the variance used in the denoising distribution. So you can ignore for a moment this coefficient. So we're going to focus on these two terms. Recall that if you want to generate xt, if you want to generate a sample at times the t, you can use this parameterization trick that we discussed earlier. And in this paper hall et al. in New York 2020, they observed that you can also express the mean of the trackable posterior distribution I discussed as xt, the input noise image minus the noise that was used to generate that data. To get this expression, it's very simple, you just need to do some arithmetic operations on this equation and just plug the definition of xt from the parameterization trick. With some arithmetic operation, you will see that if you basically have a noisy image and you want to predict the less noisy version, right? If you knew the noise that was used to generate that noisy image, you can just subtract some re-scaled version. So this is the scaled version of those. So you can take noise, subtract just some scaled version of that noise from xt to get mean of xt minus one. This is kind of very interesting information. So you basically can represent this mean in a very simple form, expression of xt and epsilon, the noise that was used to generate xt. So this actually is the same noise that was used to generate xt. Knowing that knowledge, it means that now if we want to parameterize this network, we can use this knowledge in the parameterization of this model. So we can say that in order to predict this mean of less noisy images, we're going to just take xt and subtract it from a neural network that predicts the noise that was used to generate this xt. So basically we train a neural network to predict the noise that was used in order to generate xt in order to represent this noisy model. This is just a parameterization trick. Instead of just representing the mean of the noisy model directly, what we can do is that we can just represent the noise that was used to generate xt. And if you have this, you can just subtract this from xt in order to get the mean of the denoising model. So if you assume this parameterization for the denoising model. And now we also know that this is true for mu theta t. If you plot these two expressions into this, you're going to actually get a very simple expression here. So we have this lt minus one, this is the same term, lt minus one becomes just you need to draw samples from training data distribution, you draw some noise vector from standard normal distribution. And using this noise vector, you just generate this xt using the few samples using the parameterization trick. You can now pass this if you sample to your epsilon prediction network, the network that is trained to predict that the noise that was used in order to generate xt. So it's basically a very simple algorithm. You draw samples from data distribution, you draw noise, you generate xt from that noise and input data. And you train a network to predict the noise that was used in order to generate that xt. And these weights here are just some scalar parameters that comes from this basically one over two sigma t here. And this is like one over square root of beta t. And these terms that we know, we can compute very easily based on the parameters of the diffusion process. So you can ignore them for a moment. Think of them as a scalar parameters that we can compute from the diffusion parameters. So xt minus one can be represented as this weighted objective. So we're going to just summarize these weights as lambda t. We're going to define a new scalar lambda t. This lambda t ensures that your training objective is weighted properly for maximum data likelihood training. So by using this lambda t weights, you're actually maximizing data likelihood. But however what happens is that this lambda t ends up being very large for small t's and it is small for large t's. So it kind of monotically decreases for small t's is very high and for large t's is very small. And this is basically how the maximum data likelihood training is formulated. However, in this paper by Hoh et al. in Europe's 2020, they observed that if you simply drop these weights or equally just if you simply set these weights to one and train the model using this on the weighted version. Like if you drop this way, you're going to get very high quality sample generation using diffusion models. So they introduced this very simple objective that does not have this weighting anymore. This weighting is one. So again, this objective simply draws samples from data distribution, draws white noise and randomly samples from one of the time steps from one to capital t. It generates a diffused sample using the parameterization trick and it trains a model to predict a noise injected. So it's exactly the same objective without any weighting and it can be done very easily. And the answer is true that actually with this weighting, you will get very high quality sample generation using diffusion models. This objective weighting actually plays a key role in getting high quality sample generation in diffusion models. So if you're interested in checking this area, I would encourage you to check this paper by Hoh et al. published at CUQ 2022 that discuss how you can potentially change this weighting over time to get even better high quality images from diffusion models. So let's summarize the training and sampling from diffusion models and what we've learned so far. In order to train diffusion models, the algorithm is extremely simple. You draw a batch of samples from your training data distribution. You uniformly sample from these time steps from one to capital t. You draw some random noise that has same dimensionality as your input data. And you use the parameterization trick to generate sample at time step t. You give the sample to your noise prediction network and you train this noise prediction network to predict the noise that was used to generate that diffuse sample. And to train this you just simply use squared L2 loss to train this noise prediction network. After training, if you like to draw samples from your diffusion model, you can use the reverse diffusion process to generate data. So we're going to start from last step x capital t. We're going to draw samples from standard normal distribution. And then here we have a full look that walks back in diffusion process starting on capital t all the way to t equals to 1. At every step we just draw white noise z from standard normal distribution. Here we're forming the mean of the nosing model. Remember this is the parameterization we use for the nosing model. And then we add noise rescaled with the standard deviation of the nosing minus sigma t to generate xt minus 1. And we can repeat this t times in order to generate x0. So very simple training and very simple generation process. So far we talked about training and sampling. So let's talk about the implementation details of how to form neural networks for the nosing model. In practice, most diffusion models use unit architecture to represent the nosing model. This unit architecture often has residual blocks. So here different rectangles represent residual blocks at different scales. And these residual blocks often have also self-attention layers in them. In some layers usually produce self-attention layers. Remember this unit takes this diffused image, diffused peanut, xt, and it predicts the noise that was used in order to generate this diffuse image. So this epsilon prediction will be trained to produce the predicted noise that was used to generate this xt. This network is also conditional time. It's shared across different time steps. So it also takes time using some time embedding. This time embedding can be done using, for example, sinusoidal positional embeddings that are often used in transformers or random free features to represent this time embedding. This time embedding will be a vector that will be fed to a small fully connected network. A network consists of a few fully connected layers to access some time representation. And this time representation is usually fed to all the residual blocks. In order to combine this time embedding with all the residual blocks, you have a few options. For example, you can just take this time embedding and do arithmetic sum with all the spatial features in the residual blocks. Or you can use, for example, adaptive group normalization in order to do this, like to add time embedding into residual blocks. So I would encourage you to check this paper that discuss fruits and trades between adaptive group normalization and spatial recognition. So, so far we talked about forward process, reverse process, training, as well as the network's design for diffusion models. Let's also talk about some of these hyperparameters that we have in diffusion models, mostly beta T schedule, the variance of the forward process and the variance used in the reverse process, sigma T square. So, one common assumption is that we can, most papers follow Jonathan Hobot of Newripp's 2020 paper, where they use just simply beta T's that are defined using just a linear function. Just these beta T's are small values and they gradually go to some larger value through some linear schedule. And it's also common to assume that sigma T square is just equal, set equal to beta T. This works also really well in practice, especially when the number of diffusions steps is large. But you may ask me, how can I train these palms? Is there any way I can also train beta T and sigma T square? So, there are a couple of papers that discuss this. One of them is this paper by Kimba Tao at Notives 2022. This paper introduces a new parameterization diffusion model using a concept called signal to noise ratio. And they show how you can actually train this noise schedule using some training objective. So, they actually propose a method for training these beta T values. There are also a couple of papers that discuss how you can train sigma T square, the variance of the reverse process. This first paper here shows how you can use a variational bond that we use for training diffusion models to also train sigma T, the variance of the denuzin models. And there's only paper here, analytically P.M. by Bobo et al. in ICELER 2022. This paper actually got outstanding paper award this year at ICELER. And they showed how you can actually post-training, after training your diffusion models, how you can use this to compute the variance of the denuzin model analytically post-training. So, so far, we talked about how we can train and how we can also pick up these hyperparameters and diffusion models. Let's look at the diffusion process and look into what happens to, for example, images in the forward process. We call it, in order to sample from time step T, we can use this diffusion channel, and we can use this parameterization trick to generate XT from input image X0. Here, XT, we know this diffuse sample, in order to analyze what happens to the image, what we're going to do, we're going to use Fourier transform to convert XT to the frequency domain. So this FXT, you can think of just Fourier transform applied to XT, it's just a representation of the image in the signal, in the frequency domain. And we know from Fourier transform that this XT can be just represented as Fourier transform of input image, plus Fourier transform of the noise, some together with some weights corresponds to the ways that we use actually in this parameterization trick. This is just simple rules in Fourier transform. You should have in mind that most images actually in the frequency domain have a very high response for low frequency, and they have very low response for high frequency content. And this is because most images are very smooth. In general, even if they're not super smooth, when you apply Fourier transform to them, you see usually that most images have very high concentration in low frequency, and their high frequency response is very low. This is very common in most images. One thing you should also know that if you have a white noise or Gaussian noise and you apply Fourier transform on top of it, in the frequency domain, actually this Gaussian noise can be also represented as just Gaussian noise in the frequency domain as well. So Fourier transform of a Gaussian noise is itself Gaussian noise, which we can use now here for analysis. So remember for the small t's, alpha bar t is almost one. So as a result, we actually did the perturbation we apply is very small in the, in the frequency domain as well. So in frequency domain, because most of our input signal for input image is concentrated at the small t's, and because alpha bar is almost one, we actually don't perturb the low frequency content of the image that most. And we mostly perturb when we kind of wash out this high frequency content of the image for small t's. And then for large t's, because alpha bar t, this coefficient here is almost zero. So what happens is that you now push down all the frequency content, so you also push down the low frequency response of the image, and you wash away all the kind of frequency content of the image. This basically shows that there's kind of a trade off in the forward process. In the forward process what happens is that the high frequency content is perturbed faster than the low frequency content. So at small t's, most of the low frequency content is not perturbed, it's mostly the high frequency content that is being perturbed. But eventually at the end of process is a time when we also completely get rid of the low frequency content of the image. This is very important to also understand what happens in the reverse process, right, so because there's kind of trade off between content and detail, you can think of low frequency response is the main content of image and the high frequency response is just detail in that generation. These diffuser model kind of trades off between these in different steps, so you can think of when you're training a generative model, the reverse denoising model, for in the large teams, your denoising models becoming specialized at generating low frequency content of the image. So it's mostly the course content of the image is being generated large t's. In a small t's, then your denoising model is becoming specialized in generating the high frequency content of the image. So most of the low level details are generated in the low, low t's, the small t's. This is also why the weighting of training objective becomes important, right, so because you have a model that is shared in different time steps and this model is responsible for generating course content versus low level details. By reweighting this training objective, now we can kind of keep balance between how much we want to generate this course content that is visually very appealing usually, versus how much we want to generate the high frequency because that usually we ignore, we cannot necessarily observe them. And the weighting plays a key role in keeping this trade off, you're balancing this trade off. So, so far I talked about the fusion was in general, now let's talk about the connections between the fusion was and the VAEs, especially hierarchical VAEs. In hierarchical VAEs, which one of the examples can be like any VAE work I did a few years ago. In hierarchical VAEs, we have this deterministic path, you can think of just a resonant that generates data at x at the end, this is just a generative model. In hierarchical VAEs we usually sample from noise and we inject to this deterministic path and then we go to second group sample from second group condition on the first group, and the after generating this noise we feed it to the, to the deterministic path and we keep doing this, we're just walking down in the hierarchical model. So the fusion models can be considered as hierarchical models as well, where these diffuse steps are just latent variables in this hierarchical model, the condition dependencies of course different. And here we're going to discuss like what are the main differences between the fusion was and hierarchical VAEs. One major difference is that the encoder in VAEs is often trained, whereas encoder, which is the forward diffusion in diffusion models is fixed we're not training the forward diffusion which is using a fixed diffusion process as encode. That's one major difference. The second difference is that latent variables in hierarchical VAEs can have a different shape and different dimensionality compared to input images. Whereas in diffusion models we assume that all the intermediate variables have the same dimension as input data. The third major difference is that in diffusion models, if you think of the noisy model as a generative model, this is actually shared across different steps. Whereas in hierarchical VAEs, we actually don't make an assumption, we don't share any component in this hierarchical structure, usually. In hierarchical VAEs, we usually train these models using variational bond, whereas when we're training diffusion models, we're using some different rebating of variational bond in order to drive the training objective of diffusion. So even though these two are related, they're not exactly the same. There are some trade-offs that occur when we are rebating the variational bond. So this brings me to the last slide. So in this part, I reviewed the noise and diffusion probabilistic models. I showed how these models are just simply trained by sampling from forward diffusion process and training a noisy model that simply predicts the noise that was used in order to generate diffuse samples. We discussed these models from different perspectives. We saw what happens to data as you go in the, what happens to images as you go forward in the diffusion process. We also discussed what happens to data distribution in the forward process. We saw how data distribution becomes smoother and smoother in forward diffusion. But of course, like any other deep learning framework, the devil is in the details, the network architecture, objective rebating, or even diffusion parameters play a key role in getting good high-quality results with diffusion models. So if you're interested in knowing more about important design decisions that actually play a role in getting good diffusion models, I would encourage you to check this paper by other colleagues, called Elucidating the Design Space of Diffusion-Based Genetic Models by Karas Etal. And this paper discusses important design decisions and how they play a role in getting good diffusion models. So with that in mind, I'd like to pass the mic and with you to my dear friend and colleague Karsten to talk about score-based genetic modeling with differential courses. Hello everyone, I'm Karsten, and I will now talk about score-based genetic modeling with differential equations. In order to get started, let us actually consider the diffusion process that Arash already introduced in his part. This diffusion process is defined through this Gaussian transition kernel of this form. But now let us consider the limit of many, many small steps, and each step being very, very tiny. So how does sampling from this diffusion process and in practice look like? So from this Gaussian transition kernel, we can just do essentially the parametrization trick, and we take the xt minus one, we scale it down by this one minus beta t square width term, and we add a little bit of noise from the standard normal distribution, scaled by this square width beta t term. With beta t, we can actually interpret it as a step size essentially, so if beta t is zero, nothing happens, this term drops out, and also no rescaling of xt minus one happens. So let's make this a little bit more explicit and write beta t as this delta t times this function beta of t. So beta t is explicitly our step size, and beta of t is now this time dependent function that allows us to have different step sizes along the diffusion process t. And now in this limit of many, many small tiny steps, it is delta t that goes to zero. If delta t goes towards zero or tiny, we can actually tailor expand this square width expression here and obtain this equation at the bottom. I just copied that over here. And it turns out that this equation has a particular form. We can interpret this as some iterative update, like the new xt is given by the old xt plus some term that depends on xt itself. So this is just a small correction and some noise added. It turns out that this iterative update will correspond to a certain solution or a certain discretization of a stochastic differential equation, and in particular this stochastic differential equation. If I wanted to iteratively numerically solve this stochastic differential equation, for instance with an Euler-Maruama solver, then this is exactly the iterative scheme I would end up with. Let us not get ahead of ourselves. I'm not sure if everybody who's listening here is an expert in differential equations. So let us do a one-slide crash course in differential equations, and let us start with ordinary differential equations, which are a little bit simpler than stochastic ones. Here is an ordinary differential equation that can be written in that form. So this is now the state that we're interested in. This code, for example, the value of a pixel in an image. And t is some continuous time variable that captures the time along which this state changes or evolves. And ultimately one is often interested in the evolution of this state x or this pixel value x of t. And that is not what we're given an ordinary differential equation. What we've given is an expression for the time derivative of dx to dt in the form of this function f. So this code, for instance, will be a neural network. So what does this mean? This f essentially describes not x itself, but the change of x. So if you now look here in this graph at the bottom. So for point x, for a given time t, this f of x now describes the change. So in other words, we could look. So this f essentially corresponds to an arrow in this graph. And if I now wanted to get x of t, I would just follow the arrows in this thing here. So basically you just have to integrate up this differential equation following the arrows to get my final expression x of t. And that's what I would have to do. However, in practice, this f is often like highly complex nonlinear function, for instance, like a neural network, and solving this integral here analytically and following these field lines exactly is often not possible. In fact, one can solve this whole thing iteratively numerically in a stepwise fashion. So in that case, when we add some point x, we evaluate our neural network or our, well, our nonlinear function f, or function f, yeah, at this x and time t. And then we do like a small linear step in this direction and access to our old state x. Continue doing that again evaluate f and again update and so on and so forth. So we have an approximation essentially to this analytical solution. So, yeah, and now there are also stochastic differential equations. And once a little bit more complex, these stochastic differential equations now have an additional term system sigma of x and t times this term omega. So what is all this. First of all, Omega, this is called a Vina process. And what this is in practice is really just Gaussian white noise. What this means is that our DX of t or ODE equation now has this additional term, which is corresponds to noise injection. And this noise is scaled by this standard deviation term essentially sigma of x t. And this has a name, this is a diffusion coefficient. And in that context also the other term system here is called the drift coefficient. And we can see that these equations are sometimes written like this explicit form with derivative here. And sometimes also like this is written on the other side and it becomes a bit of a special expression for the Vina process. Sometimes this differential equations or stochastic differential equations here also written like this, but this essentially means the same thing for the sake of this talk. And importantly, keep in mind this omega of t is essentially a Gaussian random variables and strong independent for each team looks like this. So how does this now look like if I want to solve this stochastic differential equation. So for example, numerically, so it's a little bit similar to the iterative solution we had here. So if I'm given a stage, I, I first updated corresponding to my updated corresponding to the drift coefficient, I evaluate that. So update a little bit with that direction. But then I also evaluate the diffusion coefficient and add a little bit of noise that is proportional the strength of the noise is proportional to the diffusion coefficient, and additionally also to the to the time stamp. So if I do this, so each time I do this I made more different noise variables. So this means there is not a unique solution like there was in the ordinary differential equation case, but there is a lot of noise injected. So if I do this multiple times, I may get slightly different trajectories. So over all these trajectories still approximately follow this deterministic F function, but we have additional noise injection. So, yeah, this is how it may look like. So I may ask, is there now also like an analytical framework to instead, you know, describe this analytically like it was for the ordinary differential equation case. So there is, but this is a little bit more involved. Because now we are not talking about one deterministic solution that we need to describe rather given one state in each at the beginning, we now have a probability distribution over possible states where we could land. So the definition of these probability distribution that is described by the Fokker-Planck equation, but that is beyond the scope of this tutorial here. Anyway, I think now you should also have some intuitions for not only ordinary differential equations, but also stochastic differential equations. Now we can go back to our stochastic differential equation that we had here, and that actually describes the forward diffusion process of diffusion models. So this is again just copy over the equation from the last slide here at the bottom. And this is now a visualization of how this whole thing looks like in practice more or less. So let's go through that one by one. On the left hand side, we have some distribution here of x zero, this might be defined through an empirical data distribution or here we have this one dimensional toy distribution. In a more realistic setting this may represent a distribution of images like images of these cats here. So now if we simulate this stochastic differential equation forward, we get this green trajectories and they evolve towards this standard normal prior distribution. And yeah, the images come progressively noisier and noisier as we do this. So let's also look at the form of this equation and it's kind of intuitive. We see that this is updates or in the DX. This update direction, it's actually proportional to the negative of the state x we're in. So this means if we have a large pixel value for instance. It will pull us to go back towards zero. So direction is always is a negative direction corresponding to my, our x. On the other hand, while all our states are being pulled towards zero as I've just explained, and at the same time we're also injecting noise. So this makes it intuitive that after a while of simulating this whole process, we end up with this distribution. It means that every single point basically completely converts itself to just plain noise where, yeah, with mean zero and certain variance. Here's another animation of that. Note that throughout this talk, we will make a lot of use of this image of this cat washes cat peanut. We do hope that it will become a little bit famous after this talk, but let's see how that goes. Right. So, yeah, we have this forward diffusion sd with a drift term and diffusion term, one of them pulls towards the mode of the distribution the other one injects noise. It may be worth mentioning that in the diffusion model it which are also other differential equations have been used to define other types of diffusion processes, often just take a more general form like this. Yeah, I don't want to go into too much detail and in this talk we will stick to this equation for simplicity, but all the concepts that we tried in this tutorial also hold for other types of SDS. The only thing that is important is that these drift kernels and fusion terms here these are basically linear function of X. Otherwise, we couldn't solve for the probability distributions here, which reminds me the background this where this background in these. In these pictures here these animations, this actually defines the marginal different probability distribution of the few data, which is your multimodal, and then he becomes union model. So great, we have not talked about the forward diffusion process, but what about the reverse direction. So, which is necessary for generation. So, can we also have like some differential equation that is quite sad. It turns out yes. There is this result described by an Anderson 1982 and then used in young songs like your paper last year. So if there is a forward differential equation of this form for STD, then there is a corresponding reverse stochastic differential equation that once in the other direction, but tracks the exact same probability distribution. So, really like the reverse direction, which generates data from noise, not noise from data. So, and since we first generate the fusion STD, it looks like this. And again has drift term and a diffusion term. It's a diffusion term and the drift term look over all somewhat similar. So the diffusion term is the same thing. So this will be a process that again injects noise. This drift term also has this same. X minus one half p to t of X term, but then there was this additional term. This is this red term and that is very important. So this is the gradient of the logarithm of the marginal used density probability density of the data. And this is known as a score function. So this means, if you now had access to this object here like this score function, we could do data generation from random noise by just sampling or simulating this reverse diffusion STD. So this in practice and what look like this, like I said, this reverse diffusion STD. It's really a diffusion process running in the other direction. And so, yeah, simulating this is generative modeling essentially. So, are we done. So, we can simulate this and generate data. Well, not quite. The question is, how do we actually get this score function. We may have the naive idea, why not learn a neural network for the score function to approximate it, and then we can take this new network, we call this new network as data. And then we can take this network inside it and our reverse diffusion STD. And, yeah, so we can simulate it and generate data. Something we could do, for instance, is draw a diffusion time along this diffusion process. Now take like data, refused data from this point in the diffusion process samples is QFT of XT. So we take a neural network that takes us as input, maybe we also additionally give up the time and train this, maybe with some simple square two term, which we minimize. We train it to predict this score of this diffuse data, this marginal score to diffuse data. Well, great idea, but unfortunately that doesn't work, because the score of this marginal diffuse data is not tractable or more explicitly this diffuse data density itself QT of XT is not tractable. We don't have an analytic expression for that, which makes it to a different sense because, well, if we had we could just put NT for zero and get our data distribution that this is what we're interested in modeling in the first place. So, too bad. But what we can do is something different, which is now known as denoising score matching. So what we had on the previous slide was just general score matching. So what we can do is, instead of considering this marginal distribution QFT given X, which corresponds to the full diffuse density. Let us consider like individual data points X zero and diffuse those. So instead, we consider the conditional density QFT of XT given X zero that distribution actually is tractable. So that's why it's preserving stochastic differential equation, which is precisely this SDE that we find our forward diffusion process. For that case, this conditional density QFT of XT given one particular data point X zero, it has this expression, this form. So it is this normal distribution, where the mean is given by the initial point X zero that is now simply scaled down by some gamma of T, which is a function that starts at one and be cased towards zero. And then we also have some variance, which is the other way around, which starts at zero and grows towards one. So we can define all denoising score matching objective. So, again, we have, we draw a diffusion time T, we have an expectation over those. Then we draw a data sample X zero, one particular sample. Overall, we again have an expectation. Then we diffuse that particular data sample. And now we train it to predict the score of this one particular diffused data sample X zero. And yeah, now this is tractable, you know this is just a normal distribution so we can take the logarithm of this density of this expression for the density and also calculate the gradient. Great. So, and now there's one very beautiful result. So after this expectation over the data, when we consider that, it turns out that this neural network then will still learn to approximate the score of the marginal data of the marginal diffuse distribution, which is exactly what we need. And this sort of makes intuitively sense, because this x t that we're feeding to the neural network, this could corresponds to this is noisy right noisy data. So this could corresponds to many possible different x zeros and many different possible like grant true scores that we we regressed was on practice neural network has to kind of average over those. And it turns out that, yeah, after this averaging considering the expectation, this neural network will still model the marginal as the score of the marginal diffuse data distribution. And this is exactly what we need. What we need in our decision model, or more specifically in the reverse genitive SDE for generation. So, this is great. So, yeah, in practice diffusion modeling basically boils down to learning this score function. Let us now talk about a few implementation details here and what people do in practice because that's also crucial. This is again just copied the denoising score matching formula. So, how do we actually sample these diffused data points. And so this is really just with time and just sampling again from this normal distribution here so we have gamma times our input x zero. And then we add noise to us that is scaled by the standard deviation sigma t. We explicitly write down the star function, how it now looks like. So it's a gradient of the logarithm of this conditional distribution is the given x zero. Oh, this is a normal distribution. So this is basically some exponential times some stuff. We have the logarithm and exponential drop out. There's also a, by the way, a normalization constant, but this does not depend on x so it's not important for the derivative. Anyway, so you advise at this time. So now we can take the gradient of system here. Now here for x t. This is like the, the diffuse data point that we sampled, we can actually insert this expression here. Then we get that, but now it turns out all these terms they cancel out this gamma t x zeroes, and also one of those signals, and what we left with a step. It's interesting, because this means that the score function. It's basically just the, the noise values that we introduced doing the we permit rest sampling of our diffused data, like, minus that noise value and scale with the inverse standard deviation from the fusion. But still, this is cool. So, this maybe also suggests us how we should choose our neural network and how we should parameterize this. More specifically, for instance, we can take this new network and define it by like some, yeah, some other neural networks times minus one and divided by the standard deviation, which is inspired by this salt here. So if we insert both of the expression for the ground to a score, which is really just this noise value, and also this neural network parameterization, what we left with is this objective here at the bottom. No, this is interesting. So this means, if I choose this parameterization, our neural network epsilon that amount is basically tasked with predicting noise values epsilon, which are really just the noise values that were used to perturb our data. This also makes it kind of intuitive for this is called denoising score matching. Because if our neural network can be nice, can predict those noise values that were used for perturbation, then yeah, we can be nice and reconstruct the original data point x zero. So there's another implementation detail here. So, I have kind of arbitrarily motivated that we can use this squared to turn to perform, you know, I think score matching and to regress this function. And to give different weights to this L2 term to this L2 loss for different points along the diffusion process. Keep in mind that this is one neural network that just as input gets a noisy state and tea, it's the same neural network for all teeth along the diffusion process. So, maybe we want to specialize in that with a little bit more for large times along the diffusion process of small times or something like this, and give like different weights to this objective for different times along the diffusion process. And this is a loss weight in a lot of tea. So we introduce this loss rating lambda t that the busses. And it turns out that different loss ratings trade off between like models is different good perceptual quality, like the images and look pretty that we can generate the set and sharp. And this is no high log likelihood. For instance, if we choose a number of teeth to be exactly this variance of the forward diffusion process here to cancel out the variants in the denominator. Then this is just an objective that is actually that leads to good high quality perceptual quality outputs. And if we choose for lambda, for instance, beta of T, which is hyper parameter of the forward diffusion process, then this whole objective that we have corresponds to training our model towards maximum log likelihood. More specifically, it's kind of a negative elbow. This is interesting. And it turns out that, yeah, this is exactly the same objectives that we derived with the variational approach and part one presented by our. And yeah, so this means that there are some deep connections between this variation derivation and actually like score matching and noising score matching in particular. I would also like to point out that there are like much more sophisticated model parameterizations and loss breaking possible. I would in particular refer you to this recent paper by Tero Carlos at all, who discusses in quite some detail. There's another implementation detail I would like to talk about. So, we know that this variance sigma T squared and of this forward diffusion process for like using individual data points that actually goes to zero as T goes to zero. But this means that if I sample the T here close to zero, then this loss might be heavily amplified when sampling. Yeah, T close to zero. And that's for the case when we do not choose lambda already in function to cancel out the sigma spread. So for some of these for these reasons we sometimes see some tricks and implementations where we train the small time cut off so we prevent sampling teeth that are extremely close to zero. So in a mental way how this can be fixed, like I said, this is especially relevant training models towards high block likelihood versus lambda T function maybe something like beta of T and the sigma squared is not cancelled out. In that case, we can perform important sampling with respect to this, yeah, weight of this loss. So we have an oversampled teeth close to zero and yeah. So the objective then looks like this. So we oversample small teeth according to the important sample distribution that has most of its weight or small T. And then we weigh down the confusion of those to the overall loss and with one over our teeth and constant distribution. And I don't want to go into too much detail but this is a technique you see in several papers. So here's a visualization of what happens. So this is not a loss value. The wet is the loss value without an important sampling here. So, yeah, if I sample T close to zero then I have this heavily amplified loss values. But with important sampling the blue line, the variance is significantly reduced. Before moving on, it makes sense to briefly recapitulate what we have been doing so far. So we have been introducing this diffusion modeling framework based on continuous times now, in contrast to the first part presented by Arash, where each diffusion step had a finite size and we overall had a finite number of discrete forward fixed diffusion process steps and also denoising steps. In this section we have considered a continuous time. This allowed us to introduce this differential equation framework and also to make these connections to score matching. But it is important to keep in mind that we are still describing the same types of diffusion models in this section. We are just using different tools at a different framework. So this is important to realise after all we obtained the same objectives as we have seen on the last few slides. But bear in mind, let us now move on and we will talk now about the probability flow ordinary differential equation. However, let us first consider the reverse generative diffusion SDE again, that one, so object you have already seen so far. With this generative reverse diffusion SDE, we can, when sampling random noise from this standard normal prior distribution, we can generate data and more specifically we basically can sample data all along the diffused data distribution, the reddish curves here, the reddish contours here. It turns out there is an ordinary differential equation called the probability flow ODE that is in distribution equivalent to this reverse generative diffusion SDE. It will become clear in a minute what exactly I mean with in distribution equivalent. Let us first have a look at this ODE itself. It is written down here. In contrast to the generative diffusion SDE, it doesn't have the noise term and also the score function term, which we will then later learn with the neural network, it doesn't have this factor of two in front anymore. So what do I mean with in distribution equivalent? When I sample many initial noise values from this standard normal distribution at initialization when I want to generate the data, then simulating all these samples on backwards versus probability flow ODE towards the data. By doing that, we will sample from the exact same probability distribution, like with the generative diffusion SDE, with the only difference that we don't have this noise anymore. So how does it look like more specifically? We can see this on that slide. Here again, we have the probability flow ODE, just that we now have inserted the learned score function as a pattern for the score function. And now these trajectories defined by this ODE, they look like this. So we see that when we sample from this standard normal prior distribution on the right, these trajectories they will all flow into the modes of the data distribution. We see this also at these bluish lines here at the background. Yeah, so the probability quite literally flows into the modes of the data distribution. And that's called the probability flow ODE. Here we have an animation how it looks like. And I think at this point it should really become clear what I meant with they are the same in distribution and they sample the same distribution. On the left hand side, we have the SDE that we have that I have introduced earlier already SDE framework. And we see that these trajectories are zigzagging, I have this noise injection, but I'm still landing at the modes of the data distribution. Why for the ODE formulation, I now have these pretty like, not exactly straight but more deterministic trajectories that still land in the modes of the data distribution when I initialize some randomly from this prior distribution. And the visual trajectory on the right is deterministic while this is stochastic. So this probability flow ordinary differential equation, this is actually an instance of a neural ordinary differential equations which a while ago generated a lot of attention in the literature. More specifically, we can even see this as a continuous normalizing flow. So, why should we care, why should we use this probability flow ODE framework. It turns out that this ordinary differential equation framework that allows the use of advanced ordinary differential equation solvers. It is somewhat easier to work with ordinary differential equations than with stochastic differential equation. And there really is a broad literature on how to quickly and very efficiently solve ordinary differential equation. So we can build on top of this literature here. But there are more advantages. This ordinary differential equation, I can run this in both directions, I can run it as generation, where I go from the right to the left, where I sample the noise from a prior distribution and then go to the left to generate data. But similarly, given a data point, I can also run the probability flow ODE in the other direction and encode this data point in the latent space of this diffusion model, this prior space. So this is interesting. And yeah, this allows for interesting applications, for instance, or semantic image interpolation. And to make clear what I mean with that, let's look at this slide here. What I'm doing here is I have drawn two noise values, or let's look first at the lower left. So here we are drawing two noise values in the latent space of the diffusion model and this noise space. And now I can linear interpolate these noise values in this space. However, the model was trained in such a way that every sample under this noise distribution, so also every sample along this linear interpolation path between those noise values decodes to a coherent realistic image. So when I then interpolate, it means that this results in continuous semantically meaningful changes in the data space, right? And keep in mind, we could not just interpolate directly linearly in pixel space, this would not be meaningful. But we can do that in noise space and then obtain semantically meaningful interpolations in the pixel space like this. But because this ODE is so complex, right, under the hood, this means that we will sometimes have some jumps between nodes and such like this. And we also see this in this animation here. So in this animation at the top, yeah, we have been doing many of such interpolations one after another. And yeah, sometimes you see like little jumps, this basically corresponds to that. So all this is only possible due to this deterministic encoding and decoding path with the probability flow ODE. I think it's clear that you couldn't do this so easily with a stochastic trajectory. All right. So there is another advantage of the probability flow ordinary differential equation. We can also use it for block likelihood computation as in continuous normalizing flows. More specifically, we can take a given image or a given data sample, for instance, this image of Arash's cat peanut. Now we can take peanut and encode peanut in the latent space of our diffusion model. Now we can calculate the probability of peanuts and coding under the prior distribution of our diffusion model. And additionally, we take into account this using this instantaneous change of variables formula kind of the distortion of the ODE the volume change along the ODE trajectory. So the probability of our data sample, in our case, the image of peanut is then given by that expression. So we're really just using the tricks from the continuous normalizing flow literature here. What all this means is actually that in their probability flow ODE formulation, diffusion models can also be considered as continuous normalizing flows. However, in contrast to continuous normalizing flows, we train diffusion models with score matching. Continuous normalizing flows themselves are usually trained directly with this objective to maximize the likelihood of the data. However, training with such this objective directly is actually a hard task because for each training iteration, I have to simulate the whole trajectory here and back propagate it through it. On the other hand, this diffusion model training relies on score matching. And as we have seen earlier, score matching works quite differently in score matching. We basically have like we can train for all these different times along the diffusion process separately. This leads to a much more scalable and robust learning objective. And yeah, this makes diffusion models very scalable in contrast to these normalizing flows, I would argue. So I have not talked a lot about these differential equations, derive these, derive them and so on and so forth. However, how should we actually solve these SDS and ODE in practice? We have already seen that we can probably not solve this analytically because these SDS and ODE are defined with very complex nonlinear functions, namely these neural networks that approximate the score function. So let us look at that. So let's start with the generative diffusion SDE. So the most naive way to do this is to use Euler-Mariouama sampling. We have already briefly talked about Euler-Mariouama sampling in this earlier one slide crash course on differential equations. What we do in that case is we simply evaluate our function here for different for our state t and x, then we propagate for like a small time step delta t. And we additionally add a little bit of noise, which is also scaled by the time step. And yeah, so we do this then iteratively one after another given a new step we evaluate again and then add a little bit of noise and so on and so forth. By the way, as a small comment here, and you may wonder about the sign flip from here to here, this is because our dt is actually negative because we're running from like large time values to small time values and this delta t here is not supposed to be like an absolute step size. So it's positive. So this runs out also this enchant to a sampling that I wash talked about in the first part of our tutorial, and most specifically the way he showed us how we can, how we can sample from these discrete time diffusion models. And this, this can actually be also considered a generative SDE sampler with this particular discretization used in that part. So let's look at the probability flow ODE. How can we generate that, or using that. Again, we could basically use Euler's method, which is analogous to the Euler-Maiorama approach and just now without this noise injection. We would just intuitively evaluate our network and, yeah, the ODE function essentially do a small step, linear step for small time delta t. We evaluate and continue doing that. However, this is usually, I think nobody really does this in practice. In practice, we can hear, as I mentioned earlier, really build on the advanced ordinary differential equation literature and use much better solvers and much better methods and higher order methods in particular. So what we see for instance is the use of one cutter methods of linear multi stepping methods, exponential integrators. Yes, there was a lot of literature in that direction. Yeah, like I just said, adaptive step size when you put a method that's been used, also called the stochastic differential equation actually adaptive step size higher order methods have been used. We parameterize the ODE has been proposed that also accelerates sampling. So, yeah, there's a lot of literature in that direction. And the main reason is that one drawback of fusion models is that sampling from them can be slow and contrast to like sampling from a generative adversarial network or variation auto encoder and such methods for instance, sampling from a differential model requires many, many function calls on what are specifically neural network evaluations in our case, because during each step of this iterative denoising, we have to call this neural network again so we often have to call it many, many times. And yeah, so this is why we want to use efficient solvers so that we can reduce this number of neural network evaluations that we have to use. So now I have talked about how you can use how you can solve the SDS and the ODE and practice, but what should you use, should you actually rather build on the SD or the ODE framework when you want to sample from the model. So to shine some light into that, let us look at the generative diffusion SD a little bit closer. So it's like that. But now we can actually decompose this into two terms. Right, so this is just from here to here. And it turns out the first term is really just the probability flow ODE that we have seen already, which itself can be used for deterministic data generation like you're on the right. But then there is this additional term. So this code basically corresponds to the noise injection. And yeah, it has the noise injection here. So what what do these terms do. So this probability flow ODE term is essentially responsible for your pushing us from the right to the left here. And this logical diffusion SD term, what it basically does is for each individual team, it actively pushes us towards correct diffuse data distribution. But because of this, so when I do ours during my soul during my simulation going from the right to left you're going from noise to data. If I have ever said, then this logical diffusion SD can help us to correct these errors and actively bring us back to the right data manifold back to the fused data distribution. So this, yeah, this is an advantage can do some sort of error correction. On the other hand, it's, it's often slower because this term itself requires a somewhat fine discretization during the solve. Yeah. So now let's look at the probability flow ODE. So in that case, we do not have this SD term. However, we can now leverage these really fast ODE solvers. And so this is good when we target very fast sampling. On the other hand, there is no stochastic error correction going on here. And because of this, what we see in practice is that this is sometimes slightly lower performing than the stochastic sampling. When we just look at the quality of the samples. So to summarize what we see is, if we're not concerned about our budget of like neural network evaluations and we're willing to do like very a lot of steps, then this SDE framework can be very useful. But if we want to go as fast as possible, then probably the ODE framework is better where we then can leverage these really fast solvers. It is also worth mentioning that we can do things in between where we have only like this logic in diffusion SDE. Active a little bit. We can also, you know, like kind of solve for the first half using stochastic sampling and then afterwards switch to the ODE advanced methods are possible. I would like to refer you to this paper here with which discusses some of these things in quite some detail. Next, I would like to talk about a connection between diffusion models and energy based models. So what are energy based models? Energy based models are defined like this in an energy based model. The probability distribution that we want to model the setter of x defined through an exponential to the power of minus a scalar energy function, which is now the function of the data. And then this thing is normalized by a normalization constant to make sure it's a well defined probability distribution. This normalization constant is also called sometimes called the partition function. Furthermore, in this case, I have added a time variable t because this energy based model is now supposed to represent the diffuse data distributions for different t's in our case. When we want to sample an energy based model, we usually do that by larger than dynamics, which is if we have seen larger than dynamics already. Basically, this is very closely connected to these stochastic differential equations we have already discussed. So to do this larger than dynamics sampling, we basically require the gradient of this scalar energy function. And then we also iteratively update our sample with that. And we also have like an additional noise term atter and some step size of learning weight atter here. The important part to realize is when we do when we use these energy based models is that in practice, even though we are learning the scalar energy function. We only require the gradient of this energy function or more specifically the negative gradient of this energy function for sampling the model at the end. We do not require the energy function itself, nor do we require the partition function depth setter. By the way, this atter is implicitly defined through this energy itself that we're learning. So now it turns out that in diffusion models, what we're basically learning is the energy gradients for all these diffuse data distributions directly. We are not learning energies, but basically energy gradients. And one thing I want to add is that because in this EDM spirit, we're directly learning these energies and we have the probability, an expression for the probability distribution while also taking into account this partition function. Because of this training energy based models can be actually really complex. This often requires advanced Markov chain Monte Carlo methods, which can be very difficult to deal with. One of that is available, I would add. But yeah, diffusion models, we kind of circumvent that and we only directly learn these energy gradients. Tell me somehow show that and derive that maybe. So to this end, let's recall again that in diffusion models, our neural network basically we're trying to approximate our model more generally, we're trying to approximate this score function of the diffuse data distributions qt of x. Now let us suppose that our model is parametrized such that the diffuse data distributions qt are given by this energy based model here. Right. So now let us insert this p theta here and yeah through the map. We apply the logarithm both here, both on e to the minus the scalar energy function and also the denominator. However, this term drops out because the partition function does not depend on the state x. And what we are left with is just a negative gradient of the energy. What does this mean? So this means that this neural network s that we usually have in diffusion models to model the score function. It means that it essentially learns the negative energy gradients of the energy model based model that would describe the diffuse data distribution. So yeah, once again, fusion models kind of circumvent these complications and directly model the energy gradients and say avoid modeling this partition function explicitly for instance which leads to some of these difficulties that we have in classical energy based model training. Also, these different noise levels that we have in diffusion models. This is actually analogous to a mere sampling and energy based models. I would like to talk about one more thing about diffusion models, which is unique identity. It turns out that the denoising model that we're learning in these diffusion models that is supposed to approximate the score function of the diffused data to t of x p. This denoising model is in principle uniquely determined by the data that we're given and the forward diffusion process. And not only the score model, so and by learning the score model also these data encodings that we obtain by using the probability flow of the e to deterministically encode data in the latency space. All this is uniquely determined by the data and the forward diffusion. What this means is that even if we use different neural network architectures for us and different network initialization, we should at the end recover identical model outputs like identical score function outputs and data encodings in the latency space assuming we have sufficient training data model capacity and optimization accuracy. This is in contrast, for instance, to generate adversarial networks or variational auto encoders which do not have this property. Because these models, depending on what kind of architectures we use and what kind of initializations we use, we will always obtain like somewhat different models and yet different data encodings and so on. This is the unique property about these diffusion models. Here's an example. What we are seeing here is the first 100 dimensions of the latent code obtained from a random cyber tent image that was encoded in the latency space of a diffusion model using this probability flow or the e approach. We did this, most specifically Song and I did this with two different models and model architectures that were separately trained. However, both of these encodings distributions here as we see, they are almost the same, they are almost identical, even though these were different architectures. With that, I would like to come to a conclusion and briefly summarize. So in this part of this talk, I have introduced you to this continuous time diffusion framework in contrast to what Arash talked about in step one. We do not have finite size denoising and diffusion steps anymore and only a finite number of cells. Rather, we consider continuous perturbations, a continuous forward diffusion process and then also a continuous generative process based on differential equations. And to train these models, we make connections to score matching, most specifically denoising score matching. Now, maybe this appeared somewhat complex and mathematically involved. However, why should he use this differential equation and continuous time framework? It really has unique advantages as I have shown hopefully and hopefully I can convince you during this part of the talk. Most importantly, this allows us to leverage this broad existing literature on advanced and fast SCE and ODE solvers when sampling from the model. This can help us to really accelerate sampling from diffusion models, which is very crucial because they can be slow. Furthermore, in particular, this probability flow ODE is very useful because it allows us to also perform like these deterministic data encodings and it also allows us to do like local ideal estimation like in continuous normalizing flows and so on. Additionally, this is overall a fairly clean mathematical framework based on diffusion processes, score matching and so on. And this allowed us to use these connections to neural ordinary differential equations to continuous normalizing flows and to energy based models, which I think provides a lot of insights into diffusion modeling. With that, I would like to conclude my part and take the mic to Wicci who will now talk about advanced techniques, accelerated sampling, conditional generation and beyond. Thank you very much. Hi everyone. I'm Richie from Google Brain Team. So let's continue our study on diffusion models. So in the third part, we're going to discuss several advanced techniques of diffusion models which corresponds to accelerating sampling, conditional generation and beyond. So here's an outline of what we're going to cover in this part. Basically want to address two important questions of diffusion models with advanced techniques. The first one is how to accelerate the sampling process of diffusion models. We're going to tackle this question from three aspects, advanced forward process, advanced reverse process and advanced modeling, including hybrid models and model distillation. The second question is how to do a high resolution, optionally conditional generation. And I will talk about several important techniques to make this happen, especially the general conditional diffusion modeling framework, the classifier classifier free guidance, as well as cascade generation pipeline. Let's start from the first question, so how to accelerate the sampling process of diffusion models. To see why this question is important, let's consider what makes a good generative model. So in principle, we want a good generative model to enjoy the pulling three properties. First, it should be fast to sample from this generative model. Second, we would expect the general model capture most of the major modes of the data distribution, or in other words, they should have adequate sample diversity. And third, of course, we want the general model to give us high quality or high fidelity samples. However, there is a generative learning dilemma for existing generative model frameworks. For example, for generating several networks, they are usually fast to sample from, and they can give us high quality samples. However, because of this discriminative learning framework, there is a decent chance that GANS may miss certain modes of the data distribution. And the other type of generative models are these likelihood based models, like variational autoencoders or normalizing flows. So those models are usually optimized by maximizing likelihood or maximizing a variant of likelihood, for example, the evidence lower bound. So usually this type of models are good at fast sampling, and they are able to capture certain modes of the data distribution because of this maximum likelihood learning framework. However, usually they lead to subpar sample quality. On the other hand, the diffusion models are good at both coverage because they are also optimizing the evidence lower bound of log likelihoods, and they are able to generate high quality samples. However, the sampling of diffusion models is pretty slow, which usually requires thousands of functional calls before getting a simple batch of samples. So if we can find techniques to accelerate the sampling process of diffusion models, we will get a generative model framework, which enjoys all those three great properties. That is, we can tackle the dilemma of this generative learning framework. Before I do that, before diving into details, I would like to recap the general formulation of diffusion models. So for diffusion models, they usually define a simple forward process which slowly maps data to noise by repeatedly adding noise to the images. The forward process is defined to map data, or to map noise back to data, and this is where the diffusion model is defined and trained. In terms of the diffusion model, it is usually parameterized by a unit architecture, which takes the noisy inputs at certain time step, and then it tries to predict the clean samples, or it tries to predict the noise added to this noisy inputs. So if we think about accelerating sampling, there are some naive methods that immediately come to our mind. For example, in training, we can reduce the number of diffusion time steps. In sampling, we sample every K time step instead of going over the whole reverse process. However, those naive acceleration methods will lead to immediate voice performance of diffusion models in terms of both the sample quality as well as the likelihood estimations. So we really need something clever, cleverer than those naive methods. And more precisely, we want to ask, given a limited number of functional calls, which are usually much less than thousands, so how to improve the performance of diffusion models. And as a side note, although the following techniques I'm going to discuss take this accelerated sampling as the main motivation, they definitely provide more insights and contributions to diffusion models as we will see soon. So we will answer this question from three aspects. The first one is advanced forward process, and second is advanced reverse process, and then lastly the advanced diffusion models. So first let's take a look at some advanced forward process. So recall that the original whole process defines a Markov process, where we start from this x zero, it is the clean sample, and we gradually add noise until it goes to this x big T corresponding to white noise signal. So QXT, QXT minus one is simply a Gaussian distribution, and this beta T defines the noise schedule, and they are hyper parameters that are predefined before training the models. So we are interested in the following questions. So first, does this forward process or noise schedule have to be predefined? And does it have to be a Markovian process? And lastly, does it, is there any faster mixing diffusion process? The faster mixing is an important concept in Markov chain Monte Carlo, as we will discuss later. For the first work I would like to discuss here is this variational diffusion models. It basically enables us to learn the parameters in this forward process together with the rest of parameters in the diffusion models. In this case, we can really learn the forward process. So more specifically, given the forward process defined by QXT given x zero, it follows a Gaussian distribution with square root alpha T bar x zero as the mean and one minus alpha T bar as the variance. So this is the formulation we have learned in part one. And this work proposed to directly parametrize the variance one minus alpha T bar through a learnable function gamma eta. And this function is definitely by a similar function to ensure that the variance is within the range from zero to one. Gamma eta T is further parametrized by a monotonic multilayer perceptron by using strictly positive ways and monotonic activations, for example, sigmoid activations. And recall that in part one, we have learned that these diffusion models are directly connected to hierarchical variational auto encoders in the sense that diffusion models can be considered as a hierarchical variational encoders but with fixed encoder right. And this model is named as variational diffusion models because it is even more similar to hierarchical variational encoders because we are optimizing the parameters in the encoder together with the parameters in the decoder. And to optimize the parameters of the forward process, this paper further derive new parametrization of the training objectives in a boring sense. So basically they have shown that optimizing the variational upper bound of the diffusion models can be simplified to the following training objectives. Note that this gamma eta participates in this weighting of like different L2 norm of different time steps. This is the training objectives they derive for this great time session. And they have shown that by learning this noise schedule, it actually improves the likelihood estimation of diffusion models a lot, especially when we assume that there are fewer diffusion time steps. Note that in the second part, we learned that the diffusion models can be interpreted from the perspective of stochastic differential equation and we learned the connection between diffusion models and denoting score matching. This gives us a hint or a knowledge that the diffusion models can also be defined in the continuous time setting. And in this paper, they explicitly derive this variational upper bound in the continuous time setting with this gamma eta notation. Basically, they show that when we let this big T goes to infinity, meaning like we have infinity amount of diffusion time steps, this corresponds to a continuous time setting, and then the variational upper bound can be derived in the following formulation. And here the only difference is that the weighting term of different L2 terms, L2 distance at different time steps equals to the derivative of this gamma eta t function over time t. And more interestingly, this paper shows that if we define the signal to noise ratio equals to alpha t bar minus one divided by one minus alpha t bar. And then this L infinity is only related to the signal to noise ratio at the endpoints of the whole forward process. And it is invariant to the noise schedule in between the endpoints. So if we want to optimize the forward process in the continuous time setting, we only need to optimize the signal to noise ratio at the beginning and the end of the forward process. And they further show that the in-between process can be learned to minimize the variance of the training objective. And this enables the faster training of diffusion models besides faster sampling. And another contribution of this work is that they show it is possible to use diffusion models to get state of the art likelihood estimation results. So before this work, the benchmark of likelihood estimation have been dominated by autoregressive types of models for many years as shown in this figure, but this model shows like actually we can use diffusion models to get a big improvement out of the autoregressive model process. And one key factor to make this happen is to add further features to the input of the unit. And this, those four features can range from low frequency to very high frequencies. And the hypothesis for the assumption here is that to get good likelihood estimation, the model you really need to model all the bits or all the details in the input signal, either they are in perceptual or perceptual. However, new networks are usually bad at modeling small changes to the inputs. So adding those four features, especially those high frequency components can potentially help the network to identify those small details. And the paper found that this trick doesn't bring like much significant improvements to the autoregressive baselines. However, it leads to significant improvements in likelihood estimation for diffusion model class. Okay, so next, like paper or method I'm going to discuss is this denoting diffusion implicit models. So in this work, the main idea is like they try to define a family of non markovian diffusion processes and the corresponding reverse processes. And those processes are designed such that the model can still be optimized by the same surrogate objective as original diffusion models. We call that this is the surrogate objective right so this L simple where we remove the weighting of each L2 loss term and we just simply take the average of the L2 loss at different time steps. And then, because they can optimize by the same surrogate objective. So one can simply take a pre train diffusion model and treat it as the model of those non markovian diffusion processes, so that they are able. We are able to use the corresponding reverse processes to reverse the model, which means like we will have more choices of our sampling procedure. Okay, so to see how we can define those non markovian forward processes, let's recap the duration of the KL divergence in the variational lower bound. So we have this LT minus one is defined as the KL divergence between this posterior distribution QXT minus one given XT and X zero, and this denoting distribution P theta XT minus one given XT so this P theta is parameterized by the diffusion model. And because these two distribution, like both of them are Gaussian distributions with the same variance sigma T square, and this can be written as the outer distance between the the mean of these two distributions times a constant. Note that these two mean function have been parameterized by like simple linear combination of XT and if so, or the combination of XT and the predicted if so, so if some listen noise added to the queen sample to get XT. We can further write this KL divergence in the form of lambda T times the outer distance between the true noise if some and the predicted noise if some theta by our diffusion models. And if we really think about the duration of this LT minus one, we found that if we assume like this lambda T can be after values, because in a surrogate objective we simply set it to one right so it doesn't matter what value this alpha T is originally. So then we can find that about formulation holds as long as first we have this QXT give X zero, it follows this normal distribution. And we need to make sure that this XT still equals to this formulation. And we have those two assumptions. First, the follow process, like the posterior distribution QXT minus one, give XT and X zero follows a Gaussian distribution. And the meaning of this distribution is a linear combination of XT and the epsilon. And our reverse process to be similar to the posterior distribution, which means it is also a Gaussian, and this new theta is the same linear combination of XT and the predicted noise. So we can further rewrite, because we know XT equals to a linear combination of X zero and epsilon. So we can replace this epsilon, epsilon by the linear combination of XT and X zero. And for the reverse process, we can rewrite it as a linear combination of XT and predicted X zero hat. So this X zero hat is defined as the predicted clean sample, given the predicting noise. So if we make those three assumptions, then we can see that the about duration of our T minus one still holds, which means that we don't really need to specify this QXT, even XT minus one, and we don't need to require to be a common process. All we need to assume is the posterior distribution of XT minus one, even XT and X zero. And that's the basic, the, the insights of this, this, like how we can define the non-marcovian forward process, which leads to the same chain objecting as the original diffusion model. Specifically, so this is the original diffusion process. And now the diffusion process change to the right diagram, where for each XT, it depends on both the XT minus one and the X zero. And another remaining question is how we specify the linear combination parameters A and B. So note that here we need to specify this A and B such that this QXT given X zero still follows this Gaussian distributions. At this end, this work defines a family of forward processes that meets the above requirements, which corresponds to specifying the posterior QXT minus one given XT and X zero in this formulation. So we can similarly define the corresponding reverse process by just replacing this X zero to a predicted X zero hat. And note that this specifying specification of formulation doesn't require like a specific value of this sigma t-tutor, which means like this sigma t-tutor can be literally arbitrary values. So that's why it depends, it actually defines a family of forward processes with different values of sigma t-tutor. And more importantly, if we specify like sigma t-tutor to be zero for all the time steps, this leads to this DDI and sampler where we wish is a deterministic reverse process because this variance is zero here. And the only randomness comes from the like starting point of the reverse process, which is the starting white noise signal. And recall that in the second part, we also build connection between the like stochastic reverse process with a probability flow ODE, which is corresponds to a deterministic generative process. And we can also interpret the DDI and sampler in a similar way. Specifically, this DDI and sampler can be considered as an integration role of the following ordinary differential equation. And note that here we do a bit of change of a variable where we define this x bar equals to x divided by the scaling factor square root of alpha bar. And we define this eta to be basically the square root of the inverse signal to noise ratio. And if we assume this sigma, this epsilon theta to be the optimal model, and then this ODE is equivalent to a probability flow ODE of a variance is floating SDE, which is in the following formulation. Note that although these two are equivalent, the sampling procedure can still be different, because for the above ODE, we are taking like the sampling process over this eta t. Well for the second formulation we are taking the, for example, for both of the two equations we use the standard Euler's method, then the second one is taking the Euler step over dt. And basically, in practice, people find that the first one works better than the second one, because it depends less on the value of t, but it depends directly on the signal to noise ratio of the current time steps. We also found that with this DDM sampler, we are able to use less time sampling steps, but reach better like performance. And in terms of why this is true, this, this paper by Carols et al, argues that the ODE of the DDM is favored, because I saw in those two in those three illustrations, especially the third illustration. The definition of the solution trajectories of DDM always points towards the denoider outputs, while for the first two ODE formulation, the variance preserving ODE and the variance is pulling ODE, which are two very commonly used ODE formulation diffusion models. They basically have more like high coverage regions along the trajectories. So for the DDM, we can see like for the solution trajectories, most of the trajectories are linear and with low coverage. And it is known that low coverage really means less truncation errors accumulating over the trajectories. So if we use this kind of trajectories, like we will have a smaller chance to accumulate more errors across the trajectories. Thus, enable us to use like fewer number of diffusion time step, fewer number of sampling steps in inference. Okay, so the third word I'm going to discuss for advanced, the forward process is just critically down to long-distance diffusion model. Basically, they are trying to find a faster mixing diffusion process by using certain like background knowledge from Markov Chen and Monte Carlo. And how this forward process is related to MCMC, let's see, like this is a regular forward diffusion process, which is a stochastic differential equation. And it is actually a special case of overdone long-distance dynamics. If we assume the target distribution or the target density of this MCMC is this standard Gaussian distribution. Given this connection, we can actually design more like efficient forward process in terms of MCMC. Specifically, this word proposed to introduce an auxiliary variable, this velocity V, and the diffusion process is defined on the joint space of this velocity and the input. And during the forward process, the noise is only added in this velocity space. And this image space or input space is only erupted by the coupling between this data and the velocity. And the resulting process as showing this figure, we can see that the forward process in the V space is still exact. However, the process in the image space or the data space are much more smoother. This V components is analogous to the Hamiltonian components in HMC or analogous to the momentum in momentum based optimizers. So by defining this joint space or defining the diffusion in this V space, it actually enables faster mixing and faster traverse of the joint space, which enables fast, like more smoothly and efficient, more smooth and efficient forward process. And the second, let's see some advanced reverse process. So before that, we would like to ask a question. So remember that we use this normal approximation of the reverse process, right. It seems like the denoting distributions are always Gaussian distributions. But if we want to use less diffusion time steps, is this normal approximation of the reverse process still true or accurate? Unfortunately, the answer is no. So this assumption, normal assumption in this denoting distribution holds only when the adjacent the noise added between these adjacent steps are small. If we want to use less diffusion time steps in training, then we can see that this denoting distribution is not a unimodal normal distribution anymore. So they are tend to be like multimodal and more complicated distributions. So in that case, it means like we really need more complicated functional approximators. So we will talk about two examples of how we can include more complicated functional approximators here. The first word is this denoting diffusion gains. So in this word, they propose to model the denoting distribution by conditional gain model. Specifically, the model is training this other several learning framework. And we first get the samples xt minus one and xt by running this forward diffusion process. And then this generator takes xt as input, as well as the time step as input. And it's actually trying to model the xt minus one. But instead of just directly outputting xt minus one from the network, it's first trying to predict this screen sample x0. And then it tries to sample xt minus one from this tractable posterior distribution. And then the discriminator takes the real xt minus one and the fake xt minus one prime as input and try to discriminate these two samples. And again, this discriminator is also conditional xt and the corresponding time step t. One may ask, what is the benefit of this denoting diffusion gains compared to a one shot gain model. But this paper shows that because like right now the conditional gains only need to model like the conditional distribution of xt minus one given xt. This turns out to be a much simpler problem for both the generator and the discriminator compared to directly model the model distribution of the clean sample. And this simple training objective for the two models leads to stronger mode coverage properties of gains and also leads to better training stability. And recall that in the second part we learned that there's a close connection between energy based models and diffusion models. So, therefore, a natural idea is like we try to approximate the reverse process by conditional energy based models. Recall that an energy based model is in the form of P beta x. It is solely dependent on the normalized log density function which is f theta x. And we can further prime trend is f theta x by minus e theta x. So usually, people call this e theta x as the energy function. And this d theta is the partition function which is usually analytical and tractable. And we can parameterize this f theta by a neural network which takes the signal as input and output as scalar to represent the value of this f. And the learning of energy based models can be illustrated as follows. So suppose this is the energy landscape defined by the e theta function. And after learning, we would like to put the observation data points into the regions of low energy and put all the other inputs to the regions of the high energy. And optimizing the energy based models require, you really require the MSMC sampling from the current model p theta x as showing this formulation, which is really highly computational expensive, especially for high dimensional data. So if we want to parameterize the denoising distribution by conditional energy based model, we can start by assuming like at each diffusion time step marginally the data follows a energy based model in the standard formulation. So here I removed the, the script like the time step script for similar simplicity. And let x theta be the data at a higher noise level. So we can derive the conditional energy based models by Bayes and Rowe, but specifically this p x, the x theta is in this formulation. And if we compare this conditional energy based models with with the original marginal energy based models, we see that the only difference is that there's an extra projected term here. And this actual projected term actually has the effect of localize this highly multimodal energy landscape to a like more single mode model or uni model landscape, and with the model mode focus around the higher noise level signal x theta. Therefore, compared to training a single energy based model the sampling here is more friendly and easier to converge because the energy landscape compared to the original marginal energy landscape is more uni model and more and simpler. So that the training could be more efficient and the conversion and CMC can give us well formed energy potential after training. And compared to diffusion models, this energy based to using this energy based model to a parameterize the denoting distribution can give enables us to define like much less diffusion steps up to six steps. And more specifically to learn those models we simply maximize the conditional log likelihoods at each time step. And then after training we just get samples by progressive sampling from the energy based models from high noise levels to low noise levels. So the last part comes to the advanced diffusion models, basically want to ask two questions. For first, can we do model distillation so that the distilled model can do faster sampling. And second, can we lift the diffusion model to a latent space that is faster to diffuse. So the first idea comes to the distillation so here I want to discuss one representative work in this domain, which is this progressive distillation of diffusion models. So essentially this work proposed to distill a deterministic DDIM sampler to the same model architecture of the original model. And it's, it is, it went into this progressive pipeline, in a sense that at each distillation stage, we will have a teacher model, and, and we will learn a student model and this student model is learned to distill. By each two adjacent sampling steps of the teacher model to one sampling step of the student model. And after learning this student model at next distillation stage. The student model as a previous stage will serve as the teacher model at this new stage, and then we learn another student model at the new stage. So we need this process until we can distill the original thousands of sampling steps to a single sampling step. And implementation wise, the learning of the student model is quite similar to the original diffusion model training pipeline. The difference is how we define this training target of the diffusion model, specifically, given the teacher model. And we randomly sample a time step T, and then we draw, we run the sampler for two steps. And then the target is the, is computed to make sure that the student model can reproduce the two sampling step within one sampling step. And then the loss is defined as Euro where we minimize the out to distance between this target and the predicted, like X hat from this diffusion model or from the student model. And after that, we have in the number of sampling steps and repeat this process until we reach one sampling step. Another idea is like whether we can leave the diffusion models to a latent space, which is more friendly to this diffusion process. And here's an example of this kind of idea where we can try to leave the diffusion models to a latent space of a pre trained variation of encoder. In the latent space, the distribution of the data in this latent space is already quite close to the Gaussian distributions, which means like we can definitely use less diffusion time steps to diffuse the data in this latent space. The advantages are pretty straightforward. So first, because this latent space already close to normal distribution, we are able to use less diffusion time steps to enable faster sampling. And then compared to the original variational auto encoders, which assume that the prior distribution of the Z follows a single and simple Gaussian distribution. This kind of hybrid model assume that the PFC is modeled by a diffusion models, which means that it has a diffusion prior. So it definitely will be much more expressive compared to the original variational auto encoder model. So we can actually record that for the current stage that diffusion models only defining a continuous data space. However, there are more domains which may have more complicated data structure. So this data type, as long as we can find an auto encoder model which are tailored to that data type and can map the data input to a continuous latent space, we will be able to apply the diffusion models to that latent space. So these give us more possibilities to apply diffusion models to different modalities and different data types. And a bit of the detailed formulation. So in this work, again, we optimize the model in terms of by minimizing the variational upper bound of the negative log likelihood. The objective contains three terms. The first two terms are similar to the variational auto encoder objecting. And the third term corresponds to the training objective of the diffusion models. And it actually corresponds to, we treat the encoding latents from this QZ0 given X as the observed data of the diffusion models. And in that way, we can derive the similar training objective of the diffusion models as the original one. So we first do this random sampling of time step. And then we draw samples from this forward diffusion. And then we have this diffusion kernel. This is the forward, this is from the forward process. And then we learn this score function for DT. And of course we have some constant that is irrelevant of the model parameters. Okay, so the second question we want to answer is how to do high resolution optionally conditional generation using diffusion models. In the past two years, we have seen many impressive conditional generation results using diffusion models. For example, this style II and imagine recruits diffusion models to do high resolution text to image generation. And another examples includes the using conditional diffusion models for super resolution or colorization. Panorama generation is another example where we take a small size input but generate this panorama. So how can we do that? Let's first take a look at the general formulation of conditional diffusion models, which is pretty straightforward. So the only modification we need to make is in this reverse process. We can let this denoting distribution to incorporate an additional input, which is this condition C. And this corresponds to modify the mean of this Gaussian distribution to take an additional input C. And optionally, we can also let this variance to be learned and it takes an input C. But in practice, like most mostly we still just use this C in this mean and the variation of upper bounds only includes a small change, which lies in this KL divergence where we plug in this new formulation of the denoting distribution. But it is a design arc in terms of how to incorporate different types of conditions into the unit, which is used to parameterize this new data. Specifically, like these are the things people use in practice for scalar conditioning. For example, class label, we can do something similar to what we did for time step conditioning. Specifically, we can encode the certain scalars to a vector embedding, and then we simply add the embedding to the intermediate layers of the unit, or we do this adaptive professionalization layers. And if it is an image conditioning, we can do channel wise concatenation of the conditional image and the input image. And if it is for text conditioning, this contains two cases. First, if the text, we embed the text to a single vector, then we can do something similar to the vector derived from the scalar conditioning. And if we embed this text to a sequence of vectors, then we can consider using cross attention with the intermediate layers of the unit. And for a high resolution conditional generation, another important component is this classifier guidance. The main idea is like recall the diffusion model correspond to learning the score of a probability, for example, the score of log px. And right now, because we incorporate the class condition, which means like the diffusion model actually gives us a score of a class conditional model p of xt given c. And given this, we can train an additional classifier, which gives us the probability of c given x, and then we mix the gradients of these two models during sampling. And this corresponds to, we sample from actually a modified score, which corresponds to the gradient of log px given c plus omega times log pc given x. And this omega controls the strength of the guidance. And it actually corresponds to approximate sampling from the distribution, which is proportional to p of x given c times p of c given x to the omega power. And in practice, it corresponds to we modify the normal distribution where we sample from and the mean of this normal distribution corresponds to the mean predicted by the score model or predicted by the diffusion model, plus the gradients from the classifier. And if we use larger omega, then the samples will be more concentrated around the modes of this classifier, which really leads to better individual sample quality, but if we use too large omega, it will reduce the sample diversity. So one really needs to find a sweet point for this omega to best balance the individual sample quality and sample diversity. And one downside for this classifier guidance is that we need to train an additional classifier to do that, right, so that adds additional model complexity. So inspired by that work. This works tries to introduce a classifier free guidance, meaning that we can actually get an implicit classifier by joining training a conditional and unconditional diffusion model in a pulling sense. So suppose we have this PX given C, where the the score can be derived by a conditional diffusion model, and we also have the score of a unconditional model p of x, and then we can derive an implicit classifier PC given x which should be proportional to p of x given divided by p of x. And in practice, the gradient or the score of these two probability are estimated by randomly dropping the condition in the diffusion models at a certain chance for each iteration. And similarly, we can derive the modified score with this implicit classifier. We call this is the original modified score. And now we replace the log PC given x by log PX given C minus log PX. And this is the resulting modified score we will use with this classifier free guidance where this log PX given C and this log PX are both estimated by the single diffusion model. And in this three panels, we can see the trade off between sample quality and sample diversity more clearly. So from left to right correspond to we gradually increase the strength of the guidance, and we can see clearly individually speaking, the sample quality of each image increases. However, the samples that look like more similar to each other if we use a large classifier or classifier free guidance. And the last thing I want to talk about is this cascade generation pipeline, which are important to high resolution generation. And this kind of idea have already been explored by other type of generating models for, for example, game model. And it's pretty straightforward. So we've started by learning an unconditional diffusion model at the lowest resolution. And then we've learned several super resolution models, taking the down sampled training images at lower resolution as the condition. And during sampling, we just run the progressive sampling pipeline starting from the smallest unconditional model and going, going through all those super resolution models until we reach the highest resolution. But one notorious problem of this kind of cascaded training pipeline is this compounding error problem. It's more specifically record that during training, the conditions we fit into the super resolution model is the down sampled version of the training images from the data set. However, during sampling, the conditions we fit to those super resolution models are actually generative samples of from the low resolution models. So if there are certain artifacts in the samples from the low resolution models, those artifacts or inaccurate samples will affect the the sample quality of the super resolution models as well, because of this mismatch issue between the conditions in training and condition in inference. To activate this problem, this noise conditioning augmentation is proposed in reference works. Basically, during training, we try to degrade the conditioning low resolution images by adding variance amount of Gaussian noise, or just blur those images by Gaussian kernel. And during inference, we sweep over the optimal amount of noise added to the low resolution images, which are the conditions to the super resolution models. So the idea or the hypothesis is like if we try to reduce certain amount of information from the condition, then the super resolution model will be trained to be more robust to different type of artifacts when we send like the conditions as the samples. And later on, more complicated degradation process have been proposed. For example, we can add a sequence of different types of degradation operations to the image, the low resolution image before sending as a condition to the super resolution models. Okay, so here's a summary of these parts. So in this advanced techniques session, we learn to answer two questions. The first one is how we can accelerate the sampling process, and we introduce several important techniques from the aspects of advanced forward process, reverse process and modeling itself. And the second question is how we can do high resolution conditional generation using diffusion models, and we discuss the general framework of conditional diffusion models, classifier and classifier free guidance, as well as cascade generation pipeline. So in the application section, we will see how all those techniques will benefit in terms of various tasks. Hi, everyone. Welcome to the first section of applications of diffusion models. So in this section, we're going to study applications of diffusion models in terms of image synthesized control generation as well as text to image generation. So let's run text to image generation. So in the past two years, this task has been shown to be extremely suitable for diffusion models to work on. So basically, this task is the inverse of the image captioning tasks, where we are given a text prompt C and we are trying to generate high resolution images X, as shown in this video. So this video shows the generative image images by a text to image generation model called imagine as we will show later. And let's start from this glide model by opening in last year. So this is essentially a cascading generation diffusion models, which contains 64 by 64 base model, and a 64 by 64 to 56 by 256 super resolution model. And they have tried to use classifier free guidance and clip guidance. So I will talk about the clip guidance in details later, and they generally found that classifier free guidance works better than the clip guidance. And those figures shows the generative samples from this model. And as we can see, the model is capable of generate fun, normal conversations of concepts that have never been seen from the data set. For example, a hedge dog using a calculator and robots meditating in a vipassana retreat or etc. So a bit introduction of the clip guidance, it can be treated as a special form of the classifier guidance. And in terms of a clean model contains two components, a text encoder, G, and image encoder F, and during training batches of the image and caption pairs assembled from a large datasets. And the model optimize a contrasting cross entropy loss, which encourages high dog product between this F and G, if the image X and C comes from the same image caption pair, and it encourages low product if X and C comes from different image caption pairs. And it can be proved that the optimal value of this FX times GC is given by log PXC divided by P of X times P of C, which equals to log P, C given X minus log PC. So given this conclusion, we will be able to use this clip model as the classifier in the classifier in the classifier guidance in the following sense. So recall that for the classifier guidance, we want to modify the score in this formulation. And we can consider augment the augmenting the second term by a minus log PC term, because when we take gradient over X, then this part just disappeared. And then we can see that these two terms together can be modeled by a clip model. So basically replace this part by the dog product between FX and GC. So that is the clip guidance. However, in Glide they show that the clip guidance is less favored compared to the classifier free guidance. And besides pure text-to-image generation, the Glide has shown that it is possible to fine-tune the model for text-guided impending tasks. Basically, they try to fine-tune the trained text-to-image Glide model by fitting randomly occluded images with an additional mask channel as the input. So using this fine-tune model, they are able to change or do image editing by changing the prompt. For example, given an old car in a green forest, they will be able to edit the background to a snowy forest. Similarly, they can add a white hat to a man's hat. And later on, this DAO-E2 further scale up the Glide model to support 1K by 1K text-to-image generation. And this DAO-E2 has been shown to outperform the first version of text-to-image 1K by 1K generation by OpenEye, which is this DAO-E, which is an unregressive transformer-based model. And in terms of the model components of DAO-E2, it's built up on a pre-trained clip model. More specifically, a clip model is first pre-trained, and the image embedding and text embedding are grabbed from this pre-trained clip embedding and frozen. And after that, this pipeline has been built to generate images from text. Basically, this generation model contains two parts. The first is a prior model. This prior model tries to produce clip image embeddings conditioned on the input caption. And then the second part is a decoder part, which produces the images conditioned on the clip image embedding as well as the text. So one natural question is why we want to condition the decoder on the clip image embeddings, right? So why not we just directly condition this decoder on text only? So the hypothesis here is that for the total amount of entropy of an input signal, for example, images, so there's certain part that captures the high-level semantic meanings while there's still a large proportion of the entropy, which corresponds to just low-level details, either perceptual-visible or even perceptual-invisible. So the hypothesis is that clip image embeddings tend to have a higher chance to capture the high-level semantic meaning of an input signal, especially those related to the caption information. And by conditioning on this high-level semantic meaning, the decoder is able to capture or catch up those low-level details of the images more quickly. And later on, this paper also shows that this by-part later repetitions of the clip image embedding as well as the latency in the decoder model enables several text-guided image manipulation tasks, as we will show later. And a bit more details of the model architecture of the prior and the decoder models. For the prior, the paper tries two options. The first one is the auto-regressive pair, where they quantize the image embedding to a sequence of discrete codes and predict them auto-regressively. And the second option is to model the prior by diffusion models, where they directly train diffusion models based on the continuous image embedding as well as the caption input. And the paper shows that the second option gives better performance. And in terms of the decoder, it's again a cascaded diffusion models, which contains a one-base model and two super resolution models. And to save the compute and make the training more efficient, the largest super resolution model is trained on image patches of one quarter size. But during inference, the model will take the full resolution inputs and directly do the inference on the full resolution. And this paper also shows that the classifier-free guidance and noise conditioning augmentation are super important to make the decoder work well. And a little bit more detail about the bipartisan latent representations. So given an input image, we can get the bipartisan latent representations in the following sense. So it contains two parts. First is this latent variable Z, which is the clip image embeddings, and it can be derived by running the clip image encoder. And the second part is this xt, which is the latency from the decoder. And this part can be derived by running the inversion of an ddim sampler for the decoder. And after getting these two latent representations, the paper shows that it is possible to run this decoder and get near-perfect reconstruction of the original input image. And given these bipartisan representations, the paper shows that it is possible to do several image manipulation tasks. For example, this image variation tasks target at getting multiple variants of an input image, while with the hope of preserving the high-level semantic meanings of the input image. And this is achieved by fixing the clip embedding Z, while changing to different latent xt in the decoder. And as shown in this image panel, the first one is the input image, and the rest are the image variants generated by .e2. And we can see that certain high-level semantic meanings are preserved, for example, the artist's style. And this clock is preserved in all the image variants, but with different details in the image variants. And the second task is this image interpolation task. So given two input images, it's possible to use .e2 to do interpolation by interpolating the image clip embeddings of these two input images. And we can get different interpolation trajectories by using different xt along these trajectories, as shown in those three rows. And as we can see, although the trajectories are different, but the high-level semantic meanings are kept well for the two input images for all those three interpolation trajectories. And the last task, the most interesting task that they show that can be done by .e2 is this text div task, which means like given an input image and the corresponding description, we would like to add this image towards a different prompt. And this corresponds to an arithmetic operation in the latent space. More specifically, they first try to compute the difference between the text clip embeddings of the original prompt and the target prompt. And then they try to change the image clip embedding of the given input towards the difference between the text prompts. And using this approach, they show that it's possible to do this text-guided image editing by changing the prompt. And the last task to image diffusion model I want to talk about is this imagined model by Google Brain Team. So again, the task is the same as .e2. So we are given some text prompts as input, and we are trying to output 1k by 1k images aligned with the input text. And the highlight of imagined model is as follows. First, it provides an unprecedented degree of photorealisticism in terms of state-of-the-art automatic scores, such as FID scores, as well as state-of-the-art human ratings. And it provides a deep level of language understanding, as can be told by the generated samples. And it is extremely simple, so there is no latent space and no compensation. And as we will see later, it's just like a pure cascaded diffusion model. So I will first present several examples of imagined. Yeah, this is my favorite one because I just created it to make it related to CVPR. And in terms of the key modeling components of imagined, like I mentioned, it is a pure cascaded diffusion model containing one base model and two super resolution models. And it uses classifier-free guidance and the dynamic thresholding, as I will talk about later. And unlike DAI2, which uses clip text embedding as the, using this clip text encoder, this imagined used frozen large pre-train language models as the text encoders, more specifically this variant of T5 model. And there are several key observations from imagined. First, it is beneficial to use text conditioning for all the super resolution models. The explanation is as follows. So remember, like for cascaded diffusion models, we need to use this noise conditioning augmentation technique to reduce the compounding error. But however, this technique has a chance to weaken the information from the low resolution models. Thus, we really need the text conditioning as extra information input to support the super resolution models. And second observation is that scaling the text encoder is extremely efficient in terms of improving the performance of imagined. And it has been shown that this is even more important than scaling the diffusion model side. And lastly, comparing using the pre-trained large language model as the encoder versus the clip encoder, human readers actually prefer the large language model over the clip encoder on certain data sets. And this dynamic thresholding is a new technique introduced by imagined. This is mainly to solve the trade-off problem of using large classifier-free guidance weights, more specifically as we also discussed in the previous part. So when we use large classifier guidance weights, there is a chance that it gives us better text alignment but worse image quality. So as we use large free guidance weights, the clip score, which corresponds to a better text alignment, increases. However, the IFID score also increases, which corresponds to worse sample quality. So to elevate this issue, because we really want both the sample quality, like the good sample quality as well as good text elements, right? So to elevate this trade-off issue, the hypothesis this paper made is that the reason why the sample quality decreases at large guidance weights is that at large guidance weights, usually it corresponds to very large sample gradients in inference, and then the generated samples have a chance to be saturated because of the very large gradient updates. So the solution they propose is this dynamic thresholding, meaning that at each sampling step, we adjust the pixel values of the samples to be within a dynamic range, and this dynamic range is computed over the statistics of the current samples. And these two panels shows the qualitative comparisons between static thresholding and dynamic thresholding. And you can see if we use this static thresholding, the images look kind of saturated, while the dynamic thresholding, the samples look more realistic. And another contribution of Imagine is that they introduce a new benchmark, especially for this text-to-image evaluations. So the motivation of introducing new benchmarks is that for existing datasets, for example, COCO, the text problem is kind of limited and is kind of easy. So this benchmark introduced more challenging prompts to evaluate text-to-image models across multiple dimensions. For example, it tries to evaluate the ability of the model to facefully render different colors, numbers of objects, spatial relations, text in the scene, unusual interactions between objects. And it also contains some complex prompts, for example, long and intricate descriptions, wire words, and even misspelled prompts to test the robustness of your model. And this figure shows several examples of the text prompts in this benchmark, and the corresponding generated images from Imagine using these text prompts. And a bit more of the quantitative evaluations of Imagine. Imagine got state-of-the-art automatic evaluation scores on the COCO dataset, and it's also preferred over reasoned work by human readers in both sample quality and image text alignment on the drawbench dataset. And the reasoned work compared by Imagine includes this DAO-E2 slide, VQGAM plus clip, as well as the latent diffusion models. Okay, so besides text-to-image generation, I also want to talk about the controllable generation using diffusion models. And a representative work is this diffusion autoencoders, which propose to incorporate a semantic meaningful latent variables to diffusion models. So more precisely, so given an input image, semantic encoders learn in this framework to generate this Z-SIM. And this Z-SIM is fit into a conditional diffusion models to further predict the clean samples. And this leads to some like the bipod latent representation similar to the DAO-E2 model, where we have this Z-SIM with the hope that it can capture high-level semantics. And we also have this X-Big-T, which is the inversion of this conditional DTIM sampler. And the hope is that it captures the low-level stochastic variations of the images. And if we want to do unconditional sampling from this model, optionally, we can learn another diffusion model in the latent space of the Z-SIM. Very similar to the latent diffusion models we talked about in the last part, to support this unconditional generation test. Interestingly, they found that by assuming a low-dimensional semantic vector Z, they are able to learn different semantic meanings for different dimensions of this latent vector Z. For example, by changing the certain dimension of this latent Z, they are trying to identify different semantic meanings such as the higher style, the expression, the age, and also the color of the hair. And they also are assuming that if we fix the Z-SIM for each row, and we change the latent X-Big-T in the conditional diffusion model, we can see it only corresponds to very tiny details in this image, and perhaps other like perceptually invisible features in the images. So that ends the first part of the application. Thanks for listening. Awesome. Thanks Richie for the nice introduction of the first group of applications. Here I'm going to start with the second group of applications. And in this part, I will mostly focus on image editing, image to image translation, super resolution, and semantic segmentation. This is a super resolution. I'll start with talking about this work called super resolution, but we are repeated refinements, or SR3, which was proposed by Sahari et al at Google. In image super resolution, we can consider this problem as training a conditional model P of X given Y, where Y is the low resolution image and X is the corresponding high resolution image. So we want to be able to change the high resolution images given some input low resolution image. In order to tackle this problem, the authors proposed to train a diffusion model, a conditional diffusion model using this objective. Here in this objective, we have expectation over pairs of high resolution image X and low resolution image Y. We have expectation over epsilon, which is drawn from standard normal distribution, and we have expectation over time, where time varies from, for example, zero to capital T corresponding to the diffusion process. We have this excellent setup. This is a noise prediction network that takes diffused high resolution image, XT, the time, as well as this Y, this is the low resolution image that is provided as conditioning into epsilon prediction network, and we train this epsilon prediction network to predict the noise that was used in order to generate diffused high resolution image. The authors in this paper proposed to use different norms for minimizing this objective. They introduced L1, L2 norm, and they observed that one can trade quality for diversity by using L1 norm is L2 norm. Since we are training a conditional model now you have in mind that we need to modify the unit that is used for epsilon prediction. For the input of the unit, we will have access to diffused high resolution image, as well as this low resolution conditioning input. Since these two images don't have the same special dimensions, the author proposed to use just simple image resizing algorithms to up sample the input low resolution image and concatenate it with diffused high resolution image and the channel dimension, and they provide to the unit diffusion model or the epsilon prediction model and the network is trained to predict the noise that was used when generating the high resolution image. This method achieves very high quality results for super resolution. For example, here you can see super resolution results with 64 by 64 pixel input when the output is 256 by 256. And you can see that this method here shown in this column achieves a really high quality result compared to, for example, regression models or just simple image resizing algorithms or using by cubic interpolation. And you can see that this actually does a good job of generating low level details. Another work that I like to mention is called palette, image to mesh diffusion models. This method is also this paper is also proposed by same authors as a previous method. So the author's area at home. Similar to super resolution, many image to image translation applications can be considered as training a conditional model X given why. This is the input image. For example, if you consider colorization problem X is the output color image and why is the grade level input. So similar to the previous part, or previous slide we're going to again use, we're going to again trade and condition diffusion model using this objective. Similarly, we have again expectation over pairs of input conditioning why, for example, why is again great image X is the output color image. We have expectation over epsilon drawn from standard normal distribution expecting over time, we're training a conditional diffusion model that takes input grade level image for some input conditioning time as well as the diffuse output image that we want to generate and then the model is trained to predict the noise that was used to generate diffuse samples. Similar to previous part, again, we need to give a pair input to the unit model. And here, because for example, if we're attacking the color problem, we're going to have this grade level image and diffuse color image as input to unit and similarly it's trained to predict the noise that was trained for generating diffuse sample. The others tried their image to mist translation image to mist diffusion model on four different tasks, including colorization in painting jpeg restoration and uncropping, which is basically given this image, they wanted to extend the image and provide the copper part of this, what they called uncropping. This paper shows that actually diffusion walls can achieve very good results on these four tasks. We should have in mind that this problem this particular paper assumes that you have access to pairs of input and output data so they're they're training a conditional model assuming that they have input image and the output image that we want to generate. If we don't make that assumption, what we can do we can potentially take an unconditional model that is trained on that for example natural images and we can modify it for a particular task. So, as example of that approach, I want to mention I'd like to mention this paper called iterative latent variable refinement or by LBR for short, that was proposed by Joey et al at ICCB 2021. This paper proposes an approach where, given a reference image, the authors like to modify the generative process of the fusion model, such that the output of the diffusion model can be similar to the reference image. Again, we have a reference image and we want to modify the reverse generative SDEs or the reverse generative diffusion process, such that we can generate images that correspond to a reference input image. So, and the authors and the speaker proposes to do this through using some unconditional model that is not trained for this specific task, it's just the unconditional model is trained to generate realistic for example faces on this slide. So, so the basic idea is to modify the reverse denoting process, such that we can pull the samples towards the reference image. So here you have the algorithm proposed in this paper, the algorithm starts from capital T goes to one so this is the reverse denoting process at every step we draw a random nodes vector vector from standard normal distribution, we sample from the reverse denoting distribution to generate this proposal of this color x prime t minus one is the proposed denotes sample to run from the denoting model. Why here represent the reference image so we're going to use the forward diffusion kernel to generate the diffused version of reference image so we're going to go forward in time for this reference image. So I did what we want to do we want to make this proposed denotes image x prime t minus one to be more similar to whitey minus one. So to do so that this paper proposes this simple operation here fine and represents some low pass filter. So this operation is very simple, we have this proposed denotes image x prime t minus one, we subtract the low pass filter applied to this x t minus one, and we add the back low pass filter output of whitey minus one. So you can think of this operation as operation that takes x t minus one, the x prime t minus one, this is the proposed denotes image, it removes its low pass filter low frequency content. So, here we are removing the low frequency content of x prime t minus one, and the adding back the low frequency content of whitey minus one. So basically we're putting the low frequency content of whitey minus one, this is the reference image into x prime, into x prime denotes the proposal sample. What we are doing we're basically making sure that in the reverse process, we're generating a sample where the low frequency content is similar to the reference image so the most of the structure is very similar to the reference image. So, as I mentioned, fine and it's just a low pass filter. And in order to implement this the artist proposed to use simply done sampling up sampling operation where n represents the, the, the, the down sampling ratio used for in this operation for something n is equal to two, it means we're going to just take reference image or take this input down sample by factor of two and then up sample again by factor of two. So, which is which corresponds to to a low pass filter operation. Here you can see this reference, these two reference images, and how we can generate images with different values for n. So when n equals to four, it means that we actually take this during the generation we don't sample that for low pass filter we don't sample the sample images by factor of four, by factor of four. Since this factor is a small, this means that most of the structure in the generation will be similar to reference image. So you can see that in fact we're generating an image that is very similar to reference image. And as we increase this and we can see that now different levels of details can, can be generated through the diffusion model and the more global characteristics like more global arrangements or low pass, low frequency content of the image is still remains the same as the reference image. And now to show that actually you can do this for different tasks image translation given a portrait image they can generate a realistic image that corresponded to the portrait image. So they can do paint to image so they can take all painting and generate realistic image, and they can do some simple editing, and for something can add back this watermark into. In this part I'd like to talk about how we can take representation learned through diffusion models and use them for sometimes through applications such as semantic segmentation. I'm talking about a specific paper called label efficient semantic segmentation with diffusion models that was proposed by one joke at all at ICLAR 2022. So it's pretty a paper, propose a simple approach for using the representation trained in diffusion model for semantic segmentation. The others proposed to take input image and diffuse it by adding by following the forward diffusion process, and they only go to small steps by following the forward diffusion step which corresponds to adding just a bit of noise into input image, then they pass this diffuse image to the denoising diffusion model the unit model the epsilon prediction model. And they, they extract representation form, the internal representation form in this unit at different resolutions of the unit decoder. So given these representations the up sample, all these intermediate representation, so that they have the same dimensional spatial dimensionality as input in so we have these up sampling layers. We have these feature maps that have the same dimensionality as the input image, and now they simply concatenate all these intermediate feature maps, and they pass them to one by one convolutions that would do semantic segmentation per pixel. So you can think of these as a pixel classifiers that just classify each pixel for each semantic object goals or semantic goals. So, in order to train this model, that was supposed to use a pre-trained diffusion model and they're only training this component here. Up sampling component doesn't have usually any training parameters, but most of the parameters, the additional parameters are basically here in these one by one convolutional networks. This paper particularly shows that this approach is labeled efficient using very few labeled instances they can train diffusion models on several datasets as you can see on this slide. And the other show that actually diffusion based segmentation models cannot perform mass encoders, can or VAE based models on this test, which is very interesting. It shows that actually representation learning diffusion models can be used for downstream applications such as segmentation. In this part, I like to talk about the particular paper parameter that is proposed for image editing. It's called SDE edit. This paper was proposed by Ming Itala at Stanford University, and it was proposed, it was presented at ICLR 2022. What this paper is trying to tackle is that, given this stroke painting, the authors propose a simple approach to generate realistic images that corresponds to that stroke painting. The main intuition or the main idea in this paper is that the distribution of real images and stroke painting images, these are two different distribution. If you have some smashes, they're not the same and in the, in the data space, they, they are not completely overlapping each other because of these differences. But if you follow the forward diffusion process, if we have this distribution realistic images distribution stroke painted, if we follow the forward diffusion process these two distribution will stop having overlaps with each other because of the definition of forward distribution, because we know that actually, if you have two distribution and you diffuse the samples in those two distribution, the distribution will start having overlap. This forward diffusion simply corresponds to adding noise into input stroke painting. Now that we know these two distribution are overlapping, we can use just a generative model train of real images to solve the reverse SDE, reverse denoising SDE. That will start from this diffused stroke painting and try to generate a realistic image that corresponds to this noisy input. And the authors show that they're actually using a generative model. This is an unconditional model again, train unrealistic images, they can come back to realistic images that where the colors here are very similar to this stroke painting colors. So, this is, I think, a very clever idea to take stroke paintings and generate realistic images that correspond to those stroke paintings. However, actually, we should have in mind that this train in a conditional setting, however, we should have in mind that this approach mostly relies on the color information in order to take this stroke painting and generate the corresponding image. And this is a bit different than, for example, methods that would use the semantic layout, semantic mask of objects in order to tackle this problem. So, it has some advantage and disadvantages that we should have in mind. Delta shows very interesting results on different data sets. Here you can see stroke paintings for models train on this on bedroom, this on church and set up a and you can see here in these two row, how a generative model using SDE not can be used to generate realistic images that correspond to stroke. Lastly, in this part, I'd like to talk about particular work that we did at NVIDIA for adversarial robustness and in this particular work we introduced diffusion models for adversarial clarification. So the basic problem we want to add this is that given an adversially perturbed image, we want to see if we can use diffusion models to remove adversarial perturbations and clean this image such that maybe apply classifier on these adversially perturbed image images, we can actually get robust classification. So this, the proposed idea here is similar to SDE edit mostly applied for adversarial clarification, given this adversarial perturbed image. So what we propose to do is we propose to follow the forward SDE, which correspond to basically adding noise and diffusing the input adversarial perturbed image, using just a forward diffusion cannon. So we go to particular times that T star we call, and we, and we simply diffuse the input adversarial perturbed image. We know that by adding noise, we can now wash out all these adversarial perturbations that are applied into image. Now that we have this noisy image, we use the reverse genitive SDE or reverse noise SDE to start from this noisy input and generate clean image that corresponds to this noisy image. And we know that through this process, we can remove all the noise that was injected as well as all the adversarial perturbations presented here in this image. So if we have this clean image or purified image, we can just pass it to classifier and hopefully make a robust classification prediction. Like any adversarial difference mechanism, we need to be able to attack this model, evaluate our performance, we need to be able to attack this model. And we also show how we can attack this model by backfogating end-to-end through classifier as well as our purification algorithm. And this involves basically backfogating through this reverse SDE. So we showed in this paper how we can do this and how we can attack this mechanism end-to-end. On the left side in this slide, you can see an example of adversarial perturb images and first column on a set of A data set. Here, we intentionally increase the magnitude of adversarial perturbations so that they are visible to us. These two groups are representing, these two images are representing adversarial perturbation for a smining class and these two represent adversarial perturbation for eyeglasses. Here you can see diffuse samples that are generated by following the forward diffusion. This is simply corresponds to sampling from diffusion kernel. And then here in these two columns, you can see samples that are generated when we're solving the reverse genetic SDE. And as you can see, at time equals to zero, we can remove not only the adversarial perturbations as well as all the nodes that was injected through forward diffusion process. And you can see that our generated energy equals to zero are very similar to input clean original images. And we can see that the semantic attributes of these images are very similar to semantic attributes of the original images. The nice thing about using a generative model for adversarial purification is that these modes are not trained for specific attacks and specific classifiers. So at the test time, we can just apply them for unseen adversarial attacks. In comparison to the state of dark methods that are designed for similar situation for unseen threats, we actually see that our proposed diffusion prefabrication method outperform these methods by large margin. And we believe that, in fact, the fusion models can can be very strong models for designing adversarial purification techniques. And this is probably because the fusion models can generate very high quality images and potential can use for removing all the artifacts that are generated by adversarial perturbations. This brings me to the end of the second part of applications. Next person will will continue with the third part of applications. All right, I will not talk about video synthesis medical imaging 3d generation and discrete state diffusion models. Let's get started with video generation. There are samples from a text conditioned video diffusion model like Jonathan how at all, where we condition on the string fireworks. So I think these samples look pretty convincing. So they're actually in general different video generation tasks. For instance, so it's unconditional generation where we want to generate all frames of the video on scratch without conditioning on anything. There was a future prediction where we want to generate future frames conditioning on one or more past frames. We can also do past prediction the other way around. We can also do interpolation, when we have some frames and we want to generate in between frames, just for instance useful to increase the frame rate of the video. All these generation tasks can basically fall under one modeling framework. In all cases, we basically want to learn a model of form setter of xt one to xtk given x, how one to x, how I am. So for the t's and tiles, you know the times for frames that you want to generate and for the frames that we condition on. So for future predictions, these tiles will already smaller than the t's unconditional generation and we wouldn't have any tiles and so on and so forth. Something we see in multiple of these recent works is that they try to learn one diffusion model for everything. What they do is they concatenate and combine both the frames to be predicted and the conditioning frames together. And then some of these frames are masked out, so once to be predicted. And yeah, based on the conditioning frames, those are then generated and varying the masking and conditioning combinations during training, we can train one model for these different tasks. In training, we would also tell the model which frames are masked out and we would feed the model time position encodings to encode the times for the different frames. That's visualized here, for instance. In terms of architecture, these models are still using these units, which we already know from the image based diffusion models. We know like small detail. So, of course, now our data is higher dimensional, because in addition to the image and height and width dimensions, as well as the channel dimensions. We now also have the time dimensions with the number of frames, so the data is essentially four dimensional. One way is to use now 3D convolution instead of 2D convolutions to run convolutions over height width and the frames. This can be computationally expensive. Another option is, for instance, to keep special 2D convolutions and use attention layers along the frame axis. This has the additional advantage that ignoring those attention layers, a model can be trained additionally on pure image data, which is kind of nice. So, let's see some results. It turns out that these video generation diffusion models, they can actually generate really long-term video in a hierarchical manner, which is quite impressive. And there, we precisely leverage these masking schemes that we just had, and these generalized video diffusion frameworks. So, one thing you can do, for instance, is we generate future frames in a sparse manner by conditioning on frames far back. This gives us long-term consistency. And then we interpolate the in-between frames afterwards. So, we kind of generate the video in a stage-wise hierarchical manner. And with that, it's possible to actually generate really long-length, one-hour coherent videos, which is quite impressive. So, here are some samples from this recent Harvey et al. work. All right. Let us now talk about another application of diffusion models, which is solving inverse problems in medical imaging, another very relevant application. So, medical imaging may refer to computer tomography or magnetic resonance imaging. In those cases, we're basically interested in an image X, but that is not what we're actually measuring from the CT scanner or MI scanner. So, let's consider the measurement process. For instance, in computer tomography, the forward measurement process can be modeled in the following form. In the image, we are basically performing a radon transform, which gives us a sinogram, and then maybe this is sparsely sampled. So, we end up, this is sparsely sampled sinogram Y. And now, the task is that needs to be solved to reconstruct the image given this measurement Y. So, this is an inverse problem. This is a similar and magnetic resonance imaging, just that the forward process is now basically modeled with a Fourier transform, which is then sparsely sampled. So, and this is where diffusion models now come in. So, they can actually be really used in this task. And the highly ideal is here to learn a generative diffusion model as a prior over the images we want to reconstruct. And while sampling from the diffusion model, we guide synthesis condition while conditioning on the sparse observations that we have. This is the idea. And it turns out that doing this actually performs really, really well. And even this outperforms even fully supervised methods sometimes. Specifically, the thing is, when we train this fusion model over these CT or MRI images, we really just need the images, we do not need paired image measurement data to train this. So yeah, there is actually a lot of work in that direction because it's a really high impact application, of course, and there are some citations that you are interested in learning more about this. So let's move on to the next application topic, which is 3D shaped generation. Also, 3D shaped generation has recently been tackled with diffusion models. So let us consider this work by zoo at our, for instance, here, 3D shapes are represented as point clouds. And this has the advantage that they can be diffused really easily and intuitively, we see this here at the bottom right so where the diffusion actually goes from the right to the left, we have a bunch of points and they are perturbed and 3D towards this, yeah, Gaussian noise ball kind of, and the generation goes into the other direction. So in those cases, the architectures that we use to implement the denoiser network are like typical point state of the art modern point cloud processing networks like no point net advanced versions of point and point boxes and so on and so forth. How does this look like then. Another animation. I think this is quite nice. So, yeah, we can generate these pretty good shapes. We can also train conditional shape completion diffusion models very condition, for instance, unlike that or like some sparse points like this, and then complete those shapes. So in the multimodal fashion, for instance, in this example we have some legs of the chair given. And now we have like different plausible completions of the chair here. Another thing that is also quite cool is that he works on real data. I think here the model was trained only on synthetic shape net data, and yet we can see the model images that we take these images and generate plausible 3D objects. Very nice. Finally, I would like to talk about discrete states to fusion models. This is less of an application but slightly different type of diffusion model, but I think it is worth mentioning as part of this tutorial. So, so far, we have only been considered continuous diffusion entity noising processes, which I mean with that is, we basically kind of assume our data is of a continuous nature. And we could add a little bit of Gaussian noise to it in a meaningful way. So both our fixed forward diffusion process and also our reverse generative process are usually were usually implemented as Gaussian distributions like here. But what if our data is discrete, categorical, then continuous perturbations are not meaningful, they are not possible. Imagine, for instance, our data as text data, you know, pixel wise segmentation labels, or discrete image encodings. Yeah, if our data is discrete, adding Gaussian continuous noise to it doesn't really make much sense. So, can we also generalize this diffusion concept to like discrete state situations. In fact, there are categorical diffusion models. And in those cases, the forward diffusion process or like the perturbation now is defined using categorical distributions. So consider perturbation corner q of xt given xt minus one, that is supposed to put up the discrete data. So this can now be a categorical distribution, where the probability to sample one of the teachers is now given by some transition matrix q multiplied together with the state we are in xt minus one. So the probability to sample like the new state xt. So this xt is usually a one-hot state factor describing the state we're in. And yeah, this transition matrix multiplied with will then give us probabilities to sample the next state. So with that we can put up complex distributions categorical distributions towards like very random discrete distributions. We choose this transition matrix accordingly. So for instance, in this example, if we look at the right, this may now be a complex data distribution. We can perturb this towards a uniform discrete distribution over these three different states 13123. So again, the reverse process for generation, and which is then implemented through a neural network, we can also parameterize as a categorical distribution. In fact, there are different options for this perturbation process, this forward perturbation process. We can use uniform categorical diffusion where we pull everything towards a uniform distribution over the different categories like I've just shown. We can also progressively kind of mask out the data where we pull everything into one particular state. We can also analytically sample from such a distribution. So it's also well suited for diffusion model. We can also tailor our diffusion processes to ordinal data and use something like discretized Gaussian diffusion process that's also possible. How does this look like for instance. So here now I have the data distribution. It's a bit complex, but so this is basically each pixel of this image represents one categorical variable, and now the color of this pixel represents which teacher we are in. So now if I would do like this uniform categorical diffusion, I would kind of, you know, yeah, would look like this, where I would transition into different states everywhere in the image. I could also do something like Gaussian diffusion where it's more like this ordinal thing that's more based transition to neighboring states. And then there was also this absorbing diffusion where I kind of progressively mask out or absorb my state sort of different ways to do this. And then in the reverse process may for instance look like this. So here on the far right, this is a stationary distribution of this categorical distribution from which I can sample analytically. And then denoising kind of progressively noises is bad towards the data distribution. So yeah, one can use this and some papers have explored such discrete state diffusion models. For instance, we can also apply this on images by modeling the pixel values of images as discrete states to be in. So this is a start from uniform uniformly distributed pixel values right here from this all gray kind of state or mask out state. Another application is to use this in a discrete latin space. So in this work, for instance, images are encoded using a vector quantization techniques into visual tokens in a discrete latin space and then we can use something like discrete diffusion models and similar techniques to model the distribution over the visual tokens. This is also something one can do. We can also use discrete state diffusion models to generate segmentation maps, which are also categorical distributions in pixel space. And yeah, that concludes my part. And with that, I would like to pass a mic back to ours, we will now conclude our tutorial. Thank you very much. Thank you for being with us. This basically brings us to the last part conclusions, open problems, and final remarks. So today was a big day, we learned about diffusion models. At the beginning of this video, I started talking about the noise and diffusion prophecy models, which is a part, which is a type of discrete time diffusion models. I showed you how these discrete time diffusion models can be described using two processes, a forward diffusion process that starts from data and generates those by adding those into the input, and then reverse the noise and process that learns to generate data by starting from noise and denoising the input image one step at a time. I also talked about how we can train these diffusion models by simply generating diffuse samples and training network to predict that to predict the noise that was used to generate diffuse input images. In the second part, Carson talked about the score-based generative modeling with differential equation, which corresponds to continuous time diffusion models. Specifically, Carson talked about how we can consider diffusion models in the limit of infinite number of steps and how we can define or how we can describe these forward and reverse processes using stochastic differential equations or STEs. I also talked about probability flow ordinary differential equations or ODEs, which describe a deterministic mapping between noise distribution and data distribution. Another thing about working with stochastic differential equations or ODEs or ordinary differential equations is that we can actually use the same training that was used for training different discrete time diffusion models in the previous slide. However, at the test time, we are free to choose different discretization or different OD or ST solvers that have been studied widely in different areas of science. And those are basically to change the sampling time by using, for example, OD or ST solvers that don't require a lot of functional evaluations. In the third part, Ruchi talked about advanced topics in diffusion models. She mostly focused on accelerating sampling from diffusion models and she studied this from three different perspectives, including how we can define forward processes that accelerate sampling from diffusion models, how we can come up with better reverse processes, or how we can come with better denoising models that allows us to access sampling from diffusion models. Beyond that, she also talked about how we can scale up diffusion models to generate high resolution images in conditional and conditional setting. Actually, especially talk about cascaded models and guided diffusion models that are heavily used in the current state-of-the-art image to text diffusion models, just imagine. After talk about fundamental topics, all three of us talked about various computer vision applications that have been recently proposed and mostly applications that rely on diffusion models at their core recently. So, now that we know about diffusion models and we know how we can use these models in practical applications, let's talk about some open problems. I do hope that now we could make you interested in this topic and now that you know the some fundamental in this area, maybe you can think about open problems that exist in this space and together we can tackle some of these. The first problem I want to, first of problems I want to mention are more on the technical side and later I will talk about more applied questions that arises in practice. So, if you remember at the beginning of the talk, Mouskarsen and I talked about how diffusion models can be constructed as a special form of MIEs or continuous time normalising flows. We exactly don't know why diffusion models do much better than VAEs and continuous time normalising flows. If we can understand this, maybe we can take the lessons learned from diffusion models and why they do so much better than VAEs and continuous time normalising flows in order to improve these frameworks, meaning we can maybe use the lessons learned from diffusion models to improve VAEs or normalising flows. Even though there has been a tremendous progress in the community for accelerating sampling from diffusion models, we can still do, in the best case scenario, we can still do, actually, the sampling using four to 10 steps on the small matrices such as Cypher 10. However, the main question that remains on how we can get one step samples for diffusion models. And this can be very crucial for interactive applications where a user interacts with the diffusion model and this we can reduce the latency that usually users observe when they are using generative models. Some of the existing problems have to define one step samplers and part of the solution might be to come up with a better diffusion process that are intrinsically faster to generate sample from. Diffusal models, very similar to VAEs or GANS, can be considered as latent variable models, but the latency space is very different. For example, GANS, we know in the latent space often have semantic meaning and using latent space manipulations, we can actually come up with image editing or image manipulation frameworks. But in diffusion models, the latent space does not have semantics, and it's very tricky to come up with latent space semantic manipulation diffusion models. So part of problem here is how can we define semantically meaningful latent space for diffusion models that allow us to do semantic manipulations. In this talk, we mostly focus on generative applications, but one open problem is how we can use diffusion models for discriminative applications. For example, one way of using diffusion models might be for representation learning, and we might be able to tackle high level tasks such as image classification versus low level tasks such as semantic image segmentation. And these two may require different traits of when we're trying to use diffusion models to address these. Another group of applications that may benefit from diffusion models is uncertainty estimation. One question is how can we use on diffusion models to do uncertainty estimation in downstream discriminative applications. And finally, one question that remains open is how we can define joint discriminator generator, sorry, joint discriminator generator models that not only classify images or input, they also can generate similar inputs. So the committee mostly have been using unit architectures for modeling the score model in the score function in diffusion models. But one question is whether we can go beyond units and come up with better architectures for diffusion models. One specific open area is how we can feed time input or other conditioning into diffusion models, and how we can potentially improve the sampling efficiency or how we can reduce the latency of sampling from diffusion models using better network design. So far in this talk, we mostly focused on image generation, but we may be interested in generating other types of data. For example, 3D data that has different forms of representation, for example, it can be represented by stance function, meshes, voxels, or volumetric representation. Or we might be interested in generating video text graph, which have their own characteristics. And given these characteristics, we actually may need to come up with specific diffusion models for these particular modalities. One area of research is to do composition and controllable generation, and this will allow us to go beyond images and be able to generate larger scenes that are composed of multiple, for example, objects. And also this will, as a technical example, allow us to have fine grain control in generation. So one interesting open problem is how we can achieve composition and controllable generation using diffusion models. Finally, I think if we look back, look back to the vision community and the problems that we solved in the past few years, we see that most of the applications try to solve, most of the applications that rely on generative models, they try to solve, and some solve with generative adversarial networks. So maybe it's a time for us to start revisiting those applications and see whether they can benefit from the, the nice properties that diffusion models have. So one open question is which applications will benefit most from diffusion models, given that we have such amazing strong tool. Let's go to the final slide. I want to say thank you for being with us today. This was, this is a very long video, and I do hope that we could provide some useful and fundamental background on diffusion models and how often are used in practice. All of us are active on Twitter, if you are interested in knowing about follow up works that we, we build on diffusion models, please make sure that you follow us. And lastly, I want to mention that all the content on this week and this video, including slides will be available on this video on this website. If you happen to enjoy this video, I would like to ask you to share this video with your colleagues and collaborators, and hopefully together, we can come with more people to start looking into diffusion models and applying them to various interest applications. Thanks a lot.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 7.84, "text": " Welcome to our tutorial on denoising diffusion-based gender modeling, foundations and applications.", "tokens": [50364, 4027, 281, 527, 7073, 322, 1441, 78, 3436, 25242, 12, 6032, 7898, 15983, 11, 22467, 293, 5821, 13, 50756], "temperature": 0.0, "avg_logprob": -0.32321904975677207, "compression_ratio": 1.671875, "no_speech_prob": 0.04595249891281128}, {"id": 1, "seek": 0, "start": 7.84, "end": 13.0, "text": " My name is Arash Vathlet, I'm a Principal Research Scientist with NVIDIA Research, and", "tokens": [50756, 1222, 1315, 307, 1587, 1299, 691, 998, 2631, 11, 286, 478, 257, 38575, 10303, 18944, 468, 365, 426, 3958, 6914, 10303, 11, 293, 51014], "temperature": 0.0, "avg_logprob": -0.32321904975677207, "compression_ratio": 1.671875, "no_speech_prob": 0.04595249891281128}, {"id": 2, "seek": 0, "start": 13.0, "end": 18.56, "text": " today I'm very excited to share this tutorial with you along with my dear friends and collaborators", "tokens": [51014, 965, 286, 478, 588, 2919, 281, 2073, 341, 7073, 365, 291, 2051, 365, 452, 6875, 1855, 293, 39789, 51292], "temperature": 0.0, "avg_logprob": -0.32321904975677207, "compression_ratio": 1.671875, "no_speech_prob": 0.04595249891281128}, {"id": 3, "seek": 0, "start": 18.56, "end": 23.44, "text": " including Karsten Kreis, who is a Senior Research Scientist with NVIDIA, as well as", "tokens": [51292, 3009, 8009, 6266, 23625, 271, 11, 567, 307, 257, 18370, 10303, 18944, 468, 365, 426, 3958, 6914, 11, 382, 731, 382, 51536], "temperature": 0.0, "avg_logprob": -0.32321904975677207, "compression_ratio": 1.671875, "no_speech_prob": 0.04595249891281128}, {"id": 4, "seek": 0, "start": 23.44, "end": 27.8, "text": " Ruchi Gau, who is a Research Scientist with Google Brain.", "tokens": [51536, 497, 30026, 460, 1459, 11, 567, 307, 257, 10303, 18944, 468, 365, 3329, 29783, 13, 51754], "temperature": 0.0, "avg_logprob": -0.32321904975677207, "compression_ratio": 1.671875, "no_speech_prob": 0.04595249891281128}, {"id": 5, "seek": 2780, "start": 27.8, "end": 32.04, "text": " Before starting the tutorial, I'd like to mention that the earlier version of this tutorial", "tokens": [50364, 4546, 2891, 264, 7073, 11, 286, 1116, 411, 281, 2152, 300, 264, 3071, 3037, 295, 341, 7073, 50576], "temperature": 0.0, "avg_logprob": -0.18849122871472998, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.05326440930366516}, {"id": 6, "seek": 2780, "start": 32.04, "end": 36.64, "text": " was originally presented at CVPR 2022 in New Orleans.", "tokens": [50576, 390, 7993, 8212, 412, 22995, 15958, 20229, 294, 1873, 24715, 13, 50806], "temperature": 0.0, "avg_logprob": -0.18849122871472998, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.05326440930366516}, {"id": 7, "seek": 2780, "start": 36.64, "end": 40.08, "text": " This tutorial received a lot of interest from the research community, and given this", "tokens": [50806, 639, 7073, 4613, 257, 688, 295, 1179, 490, 264, 2132, 1768, 11, 293, 2212, 341, 50978], "temperature": 0.0, "avg_logprob": -0.18849122871472998, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.05326440930366516}, {"id": 8, "seek": 2780, "start": 40.08, "end": 44.92, "text": " interest we decided to record our presentation again after the conference, and we'd like", "tokens": [50978, 1179, 321, 3047, 281, 2136, 527, 5860, 797, 934, 264, 7586, 11, 293, 321, 1116, 411, 51220], "temperature": 0.0, "avg_logprob": -0.18849122871472998, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.05326440930366516}, {"id": 9, "seek": 2780, "start": 44.92, "end": 48.72, "text": " to share this broadly with the research community through YouTube.", "tokens": [51220, 281, 2073, 341, 19511, 365, 264, 2132, 1768, 807, 3088, 13, 51410], "temperature": 0.0, "avg_logprob": -0.18849122871472998, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.05326440930366516}, {"id": 10, "seek": 2780, "start": 48.72, "end": 52.760000000000005, "text": " If you happen to watch this video, and you enjoy this video, we would like to encourage", "tokens": [51410, 759, 291, 1051, 281, 1159, 341, 960, 11, 293, 291, 2103, 341, 960, 11, 321, 576, 411, 281, 5373, 51612], "temperature": 0.0, "avg_logprob": -0.18849122871472998, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.05326440930366516}, {"id": 11, "seek": 5276, "start": 52.76, "end": 58.879999999999995, "text": " you to share this with your friends and collaborators, and hopefully together we can create more", "tokens": [50364, 291, 281, 2073, 341, 365, 428, 1855, 293, 39789, 11, 293, 4696, 1214, 321, 393, 1884, 544, 50670], "temperature": 0.0, "avg_logprob": -0.20187620321909586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.004878244362771511}, {"id": 12, "seek": 5276, "start": 58.879999999999995, "end": 67.64, "text": " interest around denoising diffusion-based gender models.", "tokens": [50670, 1179, 926, 1441, 78, 3436, 25242, 12, 6032, 7898, 5245, 13, 51108], "temperature": 0.0, "avg_logprob": -0.20187620321909586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.004878244362771511}, {"id": 13, "seek": 5276, "start": 67.64, "end": 71.6, "text": " If you're watching this video, most likely you'll find me with deep gender learning.", "tokens": [51108, 759, 291, 434, 1976, 341, 960, 11, 881, 3700, 291, 603, 915, 385, 365, 2452, 7898, 2539, 13, 51306], "temperature": 0.0, "avg_logprob": -0.20187620321909586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.004878244362771511}, {"id": 14, "seek": 5276, "start": 71.6, "end": 75.92, "text": " In deep gender learning, we assume that we have access to cliques of samples drawn from", "tokens": [51306, 682, 2452, 7898, 2539, 11, 321, 6552, 300, 321, 362, 2105, 281, 596, 4911, 295, 10938, 10117, 490, 51522], "temperature": 0.0, "avg_logprob": -0.20187620321909586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.004878244362771511}, {"id": 15, "seek": 5276, "start": 75.92, "end": 76.92, "text": " unknown distribution.", "tokens": [51522, 9841, 7316, 13, 51572], "temperature": 0.0, "avg_logprob": -0.20187620321909586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.004878244362771511}, {"id": 16, "seek": 5276, "start": 76.92, "end": 82.12, "text": " We use this cliques of training data to train a deep neural network.", "tokens": [51572, 492, 764, 341, 596, 4911, 295, 3097, 1412, 281, 3847, 257, 2452, 18161, 3209, 13, 51832], "temperature": 0.0, "avg_logprob": -0.20187620321909586, "compression_ratio": 1.6613545816733069, "no_speech_prob": 0.004878244362771511}, {"id": 17, "seek": 8212, "start": 82.12, "end": 87.16000000000001, "text": " If everything goes smoothly, at the test time, we can use this deep neural network to", "tokens": [50364, 759, 1203, 1709, 19565, 11, 412, 264, 1500, 565, 11, 321, 393, 764, 341, 2452, 18161, 3209, 281, 50616], "temperature": 0.0, "avg_logprob": -0.2259546392104205, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.009416164830327034}, {"id": 18, "seek": 8212, "start": 87.16000000000001, "end": 92.76, "text": " draw new samples that would hopefully mimic the training data distribution.", "tokens": [50616, 2642, 777, 10938, 300, 576, 4696, 31075, 264, 3097, 1412, 7316, 13, 50896], "temperature": 0.0, "avg_logprob": -0.2259546392104205, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.009416164830327034}, {"id": 19, "seek": 8212, "start": 92.76, "end": 99.96000000000001, "text": " For example, if we use these images of cats to train our deep neural network at the test", "tokens": [50896, 1171, 1365, 11, 498, 321, 764, 613, 5267, 295, 11111, 281, 3847, 527, 2452, 18161, 3209, 412, 264, 1500, 51256], "temperature": 0.0, "avg_logprob": -0.2259546392104205, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.009416164830327034}, {"id": 20, "seek": 8212, "start": 99.96000000000001, "end": 105.72, "text": " time, we do hope that we can also generate images of cute handsome cats, as you can see", "tokens": [51256, 565, 11, 321, 360, 1454, 300, 321, 393, 611, 8460, 5267, 295, 4052, 13421, 11111, 11, 382, 291, 393, 536, 51544], "temperature": 0.0, "avg_logprob": -0.2259546392104205, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.009416164830327034}, {"id": 21, "seek": 8212, "start": 105.72, "end": 107.88000000000001, "text": " in the bottom.", "tokens": [51544, 294, 264, 2767, 13, 51652], "temperature": 0.0, "avg_logprob": -0.2259546392104205, "compression_ratio": 1.7389162561576355, "no_speech_prob": 0.009416164830327034}, {"id": 22, "seek": 10788, "start": 108.83999999999999, "end": 112.72, "text": " Deep gender learning has many applications.", "tokens": [50412, 14895, 7898, 2539, 575, 867, 5821, 13, 50606], "temperature": 0.0, "avg_logprob": -0.2909146698427872, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0005759499617852271}, {"id": 23, "seek": 10788, "start": 112.72, "end": 117.75999999999999, "text": " Mostly the main applications are content generation that have use cases in different", "tokens": [50606, 29035, 264, 2135, 5821, 366, 2701, 5125, 300, 362, 764, 3331, 294, 819, 50858], "temperature": 0.0, "avg_logprob": -0.2909146698427872, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0005759499617852271}, {"id": 24, "seek": 10788, "start": 117.75999999999999, "end": 126.36, "text": " industries, including, for example, entertainment industry.", "tokens": [50858, 13284, 11, 3009, 11, 337, 1365, 11, 12393, 3518, 13, 51288], "temperature": 0.0, "avg_logprob": -0.2909146698427872, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0005759499617852271}, {"id": 25, "seek": 10788, "start": 126.36, "end": 129.92, "text": " Deep gender learning can be used for representation learning as well.", "tokens": [51288, 14895, 7898, 2539, 393, 312, 1143, 337, 10290, 2539, 382, 731, 13, 51466], "temperature": 0.0, "avg_logprob": -0.2909146698427872, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0005759499617852271}, {"id": 26, "seek": 10788, "start": 129.92, "end": 136.68, "text": " If we have a deep gender model that generates really high quality images, mostly the internal", "tokens": [51466, 759, 321, 362, 257, 2452, 7898, 2316, 300, 23815, 534, 1090, 3125, 5267, 11, 5240, 264, 6920, 51804], "temperature": 0.0, "avg_logprob": -0.2909146698427872, "compression_ratio": 1.7339901477832513, "no_speech_prob": 0.0005759499617852271}, {"id": 27, "seek": 13668, "start": 136.68, "end": 143.8, "text": " representation in that model can be used also for downstream discriminative applications,", "tokens": [50364, 10290, 294, 300, 2316, 393, 312, 1143, 611, 337, 30621, 20828, 1166, 5821, 11, 50720], "temperature": 0.0, "avg_logprob": -0.17733047405878702, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.001232916722074151}, {"id": 28, "seek": 13668, "start": 143.8, "end": 149.28, "text": " such as semantic image segmentation, as you can see in this slide.", "tokens": [50720, 1270, 382, 47982, 3256, 9469, 399, 11, 382, 291, 393, 536, 294, 341, 4137, 13, 50994], "temperature": 0.0, "avg_logprob": -0.17733047405878702, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.001232916722074151}, {"id": 29, "seek": 13668, "start": 149.28, "end": 153.52, "text": " Deep gender models can be also used for building artistic tools.", "tokens": [50994, 14895, 7898, 5245, 393, 312, 611, 1143, 337, 2390, 17090, 3873, 13, 51206], "temperature": 0.0, "avg_logprob": -0.17733047405878702, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.001232916722074151}, {"id": 30, "seek": 13668, "start": 153.52, "end": 160.4, "text": " In this example that you can see on this slide, we have a tool that can be used by an artist", "tokens": [51206, 682, 341, 1365, 300, 291, 393, 536, 322, 341, 4137, 11, 321, 362, 257, 2290, 300, 393, 312, 1143, 538, 364, 5748, 51550], "temperature": 0.0, "avg_logprob": -0.17733047405878702, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.001232916722074151}, {"id": 31, "seek": 13668, "start": 160.4, "end": 166.48000000000002, "text": " who happens to be just a six-year-old kid to draw a semantic layout of a scene.", "tokens": [51550, 567, 2314, 281, 312, 445, 257, 2309, 12, 5294, 12, 2641, 1636, 281, 2642, 257, 47982, 13333, 295, 257, 4145, 13, 51854], "temperature": 0.0, "avg_logprob": -0.17733047405878702, "compression_ratio": 1.7589285714285714, "no_speech_prob": 0.001232916722074151}, {"id": 32, "seek": 16648, "start": 166.48, "end": 171.95999999999998, "text": " This tool can take the semantic layout and generate a corresponding high-quality image", "tokens": [50364, 639, 2290, 393, 747, 264, 47982, 13333, 293, 8460, 257, 11760, 1090, 12, 11286, 3256, 50638], "temperature": 0.0, "avg_logprob": -0.2652755493813373, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.0002818242646753788}, {"id": 33, "seek": 16648, "start": 171.95999999999998, "end": 176.83999999999997, "text": " that has the same semantic layout.", "tokens": [50638, 300, 575, 264, 912, 47982, 13333, 13, 50882], "temperature": 0.0, "avg_logprob": -0.2652755493813373, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.0002818242646753788}, {"id": 34, "seek": 16648, "start": 176.83999999999997, "end": 181.35999999999999, "text": " If you're a researcher and you watch the landscape of deep gender learning, you're going to see", "tokens": [50882, 759, 291, 434, 257, 21751, 293, 291, 1159, 264, 9661, 295, 2452, 7898, 2539, 11, 291, 434, 516, 281, 536, 51108], "temperature": 0.0, "avg_logprob": -0.2652755493813373, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.0002818242646753788}, {"id": 35, "seek": 16648, "start": 181.35999999999999, "end": 187.16, "text": " that this landscape is filled with various frameworks ranging from generative adversarial", "tokens": [51108, 300, 341, 9661, 307, 6412, 365, 3683, 29834, 25532, 490, 1337, 1166, 17641, 44745, 51398], "temperature": 0.0, "avg_logprob": -0.2652755493813373, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.0002818242646753788}, {"id": 36, "seek": 16648, "start": 187.16, "end": 192.72, "text": " networks to virtual autoencoders, energy-based models, and autoregressive models and normalizing", "tokens": [51398, 9590, 281, 6374, 8399, 22660, 378, 433, 11, 2281, 12, 6032, 5245, 11, 293, 1476, 418, 3091, 488, 5245, 293, 2710, 3319, 51676], "temperature": 0.0, "avg_logprob": -0.2652755493813373, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.0002818242646753788}, {"id": 37, "seek": 16648, "start": 192.72, "end": 193.72, "text": " tools.", "tokens": [51676, 3873, 13, 51726], "temperature": 0.0, "avg_logprob": -0.2652755493813373, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.0002818242646753788}, {"id": 38, "seek": 19372, "start": 194.48, "end": 199.28, "text": " Historically, the Computer Vision Committee has been using generative adversarial networks", "tokens": [50402, 25108, 984, 11, 264, 22289, 25170, 11556, 575, 668, 1228, 1337, 1166, 17641, 44745, 9590, 50642], "temperature": 0.0, "avg_logprob": -0.17687450408935546, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0006962294573895633}, {"id": 39, "seek": 19372, "start": 199.28, "end": 203.32, "text": " as one of their main tools for training generative models.", "tokens": [50642, 382, 472, 295, 641, 2135, 3873, 337, 3097, 1337, 1166, 5245, 13, 50844], "temperature": 0.0, "avg_logprob": -0.17687450408935546, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0006962294573895633}, {"id": 40, "seek": 19372, "start": 203.32, "end": 208.24, "text": " In this talk, we would like to argue that there is a new and powerful generative framework", "tokens": [50844, 682, 341, 751, 11, 321, 576, 411, 281, 9695, 300, 456, 307, 257, 777, 293, 4005, 1337, 1166, 8388, 51090], "temperature": 0.0, "avg_logprob": -0.17687450408935546, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0006962294573895633}, {"id": 41, "seek": 19372, "start": 208.24, "end": 210.72, "text": " called the Noise and Diffusion Models.", "tokens": [51090, 1219, 264, 44821, 293, 413, 3661, 5704, 6583, 1625, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17687450408935546, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0006962294573895633}, {"id": 42, "seek": 19372, "start": 210.72, "end": 216.72, "text": " These models obtain very strong results in generation, and we hopefully want to convince", "tokens": [51214, 1981, 5245, 12701, 588, 2068, 3542, 294, 5125, 11, 293, 321, 4696, 528, 281, 13447, 51514], "temperature": 0.0, "avg_logprob": -0.17687450408935546, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0006962294573895633}, {"id": 43, "seek": 19372, "start": 216.72, "end": 220.72, "text": " you that these models are very strong and can be used for various applications.", "tokens": [51514, 291, 300, 613, 5245, 366, 588, 2068, 293, 393, 312, 1143, 337, 3683, 5821, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17687450408935546, "compression_ratio": 1.6905660377358491, "no_speech_prob": 0.0006962294573895633}, {"id": 44, "seek": 22072, "start": 220.72, "end": 225.8, "text": " Hopefully, in this talk, we're going to provide you with some foundational knowledge that requires", "tokens": [50364, 10429, 11, 294, 341, 751, 11, 321, 434, 516, 281, 2893, 291, 365, 512, 32195, 3601, 300, 7029, 50618], "temperature": 0.0, "avg_logprob": -0.21047206635170795, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0017500167014077306}, {"id": 45, "seek": 22072, "start": 225.8, "end": 230.72, "text": " for using these models in practice, and we're going to even talk about how these models are", "tokens": [50618, 337, 1228, 613, 5245, 294, 3124, 11, 293, 321, 434, 516, 281, 754, 751, 466, 577, 613, 5245, 366, 50864], "temperature": 0.0, "avg_logprob": -0.21047206635170795, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0017500167014077306}, {"id": 46, "seek": 22072, "start": 230.72, "end": 236.04, "text": " currently used for tackling some of the interesting applications that exist in the Computer Vision", "tokens": [50864, 4362, 1143, 337, 34415, 512, 295, 264, 1880, 5821, 300, 2514, 294, 264, 22289, 25170, 51130], "temperature": 0.0, "avg_logprob": -0.21047206635170795, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0017500167014077306}, {"id": 47, "seek": 22072, "start": 236.04, "end": 237.04, "text": " Committee.", "tokens": [51130, 11556, 13, 51180], "temperature": 0.0, "avg_logprob": -0.21047206635170795, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0017500167014077306}, {"id": 48, "seek": 22072, "start": 237.04, "end": 243.64, "text": " As I mentioned earlier, the Noise and Diffusion Models have recently emerged as a powerful", "tokens": [51180, 1018, 286, 2835, 3071, 11, 264, 44821, 293, 413, 3661, 5704, 6583, 1625, 362, 3938, 20178, 382, 257, 4005, 51510], "temperature": 0.0, "avg_logprob": -0.21047206635170795, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0017500167014077306}, {"id": 49, "seek": 22072, "start": 243.64, "end": 247.07999999999998, "text": " generative model of performing cancer.", "tokens": [51510, 1337, 1166, 2316, 295, 10205, 5592, 13, 51682], "temperature": 0.0, "avg_logprob": -0.21047206635170795, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.0017500167014077306}, {"id": 50, "seek": 24708, "start": 247.08, "end": 252.72, "text": " In these two papers shown on this slide, one from OpenAI on the left side and one from", "tokens": [50364, 682, 613, 732, 10577, 4898, 322, 341, 4137, 11, 472, 490, 7238, 48698, 322, 264, 1411, 1252, 293, 472, 490, 50646], "temperature": 0.0, "avg_logprob": -0.2322612190246582, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0031627838034182787}, {"id": 51, "seek": 24708, "start": 252.72, "end": 258.12, "text": " Google on the right side, researchers observe that you can use the Noise and Diffusion", "tokens": [50646, 3329, 322, 264, 558, 1252, 11, 10309, 11441, 300, 291, 393, 764, 264, 44821, 293, 413, 3661, 5704, 50916], "temperature": 0.0, "avg_logprob": -0.2322612190246582, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0031627838034182787}, {"id": 52, "seek": 24708, "start": 258.12, "end": 263.36, "text": " Models to train generative models on challenging datasets such as ImageNet, and the results", "tokens": [50916, 6583, 1625, 281, 3847, 1337, 1166, 5245, 322, 7595, 42856, 1270, 382, 29903, 31890, 11, 293, 264, 3542, 51178], "temperature": 0.0, "avg_logprob": -0.2322612190246582, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0031627838034182787}, {"id": 53, "seek": 24708, "start": 263.36, "end": 268.88, "text": " generated by these models is often very high-quality and very diverse, something that we haven't", "tokens": [51178, 10833, 538, 613, 5245, 307, 2049, 588, 1090, 12, 11286, 293, 588, 9521, 11, 746, 300, 321, 2378, 380, 51454], "temperature": 0.0, "avg_logprob": -0.2322612190246582, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0031627838034182787}, {"id": 54, "seek": 24708, "start": 268.88, "end": 273.04, "text": " seen previously with other generative models such as GANs.", "tokens": [51454, 1612, 8046, 365, 661, 1337, 1166, 5245, 1270, 382, 460, 1770, 82, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2322612190246582, "compression_ratio": 1.6574803149606299, "no_speech_prob": 0.0031627838034182787}, {"id": 55, "seek": 27304, "start": 274.04, "end": 279.6, "text": " The Noise and Diffusion-based models have already been applied to interesting problems", "tokens": [50414, 440, 44821, 293, 413, 3661, 5704, 12, 6032, 5245, 362, 1217, 668, 6456, 281, 1880, 2740, 50692], "temperature": 0.0, "avg_logprob": -0.3345331127723951, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.0004725082835648209}, {"id": 56, "seek": 27304, "start": 279.6, "end": 281.6, "text": " such as super resolution.", "tokens": [50692, 1270, 382, 1687, 8669, 13, 50792], "temperature": 0.0, "avg_logprob": -0.3345331127723951, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.0004725082835648209}, {"id": 57, "seek": 27304, "start": 281.6, "end": 288.20000000000005, "text": " In this slide, you can see a super resolution framework that takes a low-resolution Image64x64", "tokens": [50792, 682, 341, 4137, 11, 291, 393, 536, 257, 1687, 8669, 8388, 300, 2516, 257, 2295, 12, 495, 3386, 29903, 19395, 87, 19395, 51122], "temperature": 0.0, "avg_logprob": -0.3345331127723951, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.0004725082835648209}, {"id": 58, "seek": 27304, "start": 288.20000000000005, "end": 296.12, "text": " dimension and generates high-resolution image in 1024x1024 pixels.", "tokens": [51122, 10139, 293, 23815, 1090, 12, 495, 3386, 3256, 294, 1266, 7911, 87, 3279, 7911, 18668, 13, 51518], "temperature": 0.0, "avg_logprob": -0.3345331127723951, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.0004725082835648209}, {"id": 59, "seek": 27304, "start": 296.12, "end": 301.32000000000005, "text": " This results show that actually, the Super Resolution Models built on top of the Noise", "tokens": [51518, 639, 3542, 855, 300, 767, 11, 264, 4548, 5015, 3386, 6583, 1625, 3094, 322, 1192, 295, 264, 44821, 51778], "temperature": 0.0, "avg_logprob": -0.3345331127723951, "compression_ratio": 1.5493562231759657, "no_speech_prob": 0.0004725082835648209}, {"id": 60, "seek": 30132, "start": 301.32, "end": 307.32, "text": " and Diffusion Models can generate very high-quality, diverse models.", "tokens": [50364, 293, 413, 3661, 5704, 6583, 1625, 393, 8460, 588, 1090, 12, 11286, 11, 9521, 5245, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19780345629620297, "compression_ratio": 1.562753036437247, "no_speech_prob": 0.002213536063209176}, {"id": 61, "seek": 30132, "start": 307.32, "end": 312.4, "text": " If you're on social media, you've been probably having a hard time not noticing a lot of excitement", "tokens": [50664, 759, 291, 434, 322, 2093, 3021, 11, 291, 600, 668, 1391, 1419, 257, 1152, 565, 406, 21814, 257, 688, 295, 14755, 50918], "temperature": 0.0, "avg_logprob": -0.19780345629620297, "compression_ratio": 1.562753036437247, "no_speech_prob": 0.002213536063209176}, {"id": 62, "seek": 30132, "start": 312.4, "end": 317.04, "text": " that was created around Dolly 2 and Imagine.", "tokens": [50918, 300, 390, 2942, 926, 1144, 13020, 568, 293, 11739, 13, 51150], "temperature": 0.0, "avg_logprob": -0.19780345629620297, "compression_ratio": 1.562753036437247, "no_speech_prob": 0.002213536063209176}, {"id": 63, "seek": 30132, "start": 317.04, "end": 323.0, "text": " These two frameworks are examples of text-to-image generative models that take text as input,", "tokens": [51150, 1981, 732, 29834, 366, 5110, 295, 2487, 12, 1353, 12, 26624, 1337, 1166, 5245, 300, 747, 2487, 382, 4846, 11, 51448], "temperature": 0.0, "avg_logprob": -0.19780345629620297, "compression_ratio": 1.562753036437247, "no_speech_prob": 0.002213536063209176}, {"id": 64, "seek": 30132, "start": 323.0, "end": 328.71999999999997, "text": " and they generate a corresponding image that can be described using that text.", "tokens": [51448, 293, 436, 8460, 257, 11760, 3256, 300, 393, 312, 7619, 1228, 300, 2487, 13, 51734], "temperature": 0.0, "avg_logprob": -0.19780345629620297, "compression_ratio": 1.562753036437247, "no_speech_prob": 0.002213536063209176}, {"id": 65, "seek": 32872, "start": 328.72, "end": 334.68, "text": " Using their core, these models use the Noise and Diffusion generative models, and on the", "tokens": [50364, 11142, 641, 4965, 11, 613, 5245, 764, 264, 44821, 293, 413, 3661, 5704, 1337, 1166, 5245, 11, 293, 322, 264, 50662], "temperature": 0.0, "avg_logprob": -0.2653300545432351, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.004899673629552126}, {"id": 66, "seek": 32872, "start": 334.68, "end": 340.44000000000005, "text": " left side, you can see for example Dolly 2 built by OpenAI can actually create this", "tokens": [50662, 1411, 1252, 11, 291, 393, 536, 337, 1365, 1144, 13020, 568, 3094, 538, 7238, 48698, 393, 767, 1884, 341, 50950], "temperature": 0.0, "avg_logprob": -0.2653300545432351, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.004899673629552126}, {"id": 67, "seek": 32872, "start": 340.44000000000005, "end": 345.40000000000003, "text": " image of teddy bears skateboarding in Times Square, and on the right side, you can see", "tokens": [50950, 3256, 295, 45116, 17276, 32204, 278, 294, 11366, 16463, 11, 293, 322, 264, 558, 1252, 11, 291, 393, 536, 51198], "temperature": 0.0, "avg_logprob": -0.2653300545432351, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.004899673629552126}, {"id": 68, "seek": 32872, "start": 345.40000000000003, "end": 352.6, "text": " Imagine can generate images of multiple teddy bears celebrating their colleague's birthday", "tokens": [51198, 11739, 393, 8460, 5267, 295, 3866, 45116, 17276, 15252, 641, 13532, 311, 6154, 51558], "temperature": 0.0, "avg_logprob": -0.2653300545432351, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.004899673629552126}, {"id": 69, "seek": 32872, "start": 352.6, "end": 355.48, "text": " while sitting behind a cake that looks like pizza.", "tokens": [51558, 1339, 3798, 2261, 257, 5908, 300, 1542, 411, 8298, 13, 51702], "temperature": 0.0, "avg_logprob": -0.2653300545432351, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.004899673629552126}, {"id": 70, "seek": 32872, "start": 355.48, "end": 356.64000000000004, "text": " This is impressive.", "tokens": [51702, 639, 307, 8992, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2653300545432351, "compression_ratio": 1.6509803921568627, "no_speech_prob": 0.004899673629552126}, {"id": 71, "seek": 35664, "start": 356.64, "end": 361.03999999999996, "text": " These models can generate very high-quality, diverse images, and they only take text as", "tokens": [50364, 1981, 5245, 393, 8460, 588, 1090, 12, 11286, 11, 9521, 5267, 11, 293, 436, 787, 747, 2487, 382, 50584], "temperature": 0.0, "avg_logprob": -0.21059237135217546, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.0008029603050090373}, {"id": 72, "seek": 35664, "start": 361.03999999999996, "end": 362.03999999999996, "text": " input.", "tokens": [50584, 4846, 13, 50634], "temperature": 0.0, "avg_logprob": -0.21059237135217546, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.0008029603050090373}, {"id": 73, "seek": 35664, "start": 362.03999999999996, "end": 365.52, "text": " Today, not only are we going to talk about diffusion models, we're going to even talk", "tokens": [50634, 2692, 11, 406, 787, 366, 321, 516, 281, 751, 466, 25242, 5245, 11, 321, 434, 516, 281, 754, 751, 50808], "temperature": 0.0, "avg_logprob": -0.21059237135217546, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.0008029603050090373}, {"id": 74, "seek": 35664, "start": 365.52, "end": 368.32, "text": " about how you can use diffusion models to create such models.", "tokens": [50808, 466, 577, 291, 393, 764, 25242, 5245, 281, 1884, 1270, 5245, 13, 50948], "temperature": 0.0, "avg_logprob": -0.21059237135217546, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.0008029603050090373}, {"id": 75, "seek": 35664, "start": 368.32, "end": 377.12, "text": " Towards the end of the video, Rucci will talk about these applications.", "tokens": [50948, 48938, 264, 917, 295, 264, 960, 11, 497, 30670, 486, 751, 466, 613, 5821, 13, 51388], "temperature": 0.0, "avg_logprob": -0.21059237135217546, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.0008029603050090373}, {"id": 76, "seek": 35664, "start": 377.12, "end": 384.12, "text": " Today's program consists of six main parts, besides introduction and conclusion.", "tokens": [51388, 2692, 311, 1461, 14689, 295, 2309, 2135, 3166, 11, 11868, 9339, 293, 10063, 13, 51738], "temperature": 0.0, "avg_logprob": -0.21059237135217546, "compression_ratio": 1.7025862068965518, "no_speech_prob": 0.0008029603050090373}, {"id": 77, "seek": 38412, "start": 384.12, "end": 390.64, "text": " The first three parts that are shown in green are mostly the technical components of the", "tokens": [50364, 440, 700, 1045, 3166, 300, 366, 4898, 294, 3092, 366, 5240, 264, 6191, 6677, 295, 264, 50690], "temperature": 0.0, "avg_logprob": -0.2403659224510193, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.04482399299740791}, {"id": 78, "seek": 38412, "start": 390.64, "end": 391.64, "text": " program.", "tokens": [50690, 1461, 13, 50740], "temperature": 0.0, "avg_logprob": -0.2403659224510193, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.04482399299740791}, {"id": 79, "seek": 38412, "start": 391.64, "end": 393.52, "text": " I'm going to start with part one.", "tokens": [50740, 286, 478, 516, 281, 722, 365, 644, 472, 13, 50834], "temperature": 0.0, "avg_logprob": -0.2403659224510193, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.04482399299740791}, {"id": 80, "seek": 38412, "start": 393.52, "end": 398.04, "text": " I will talk about the Noise and Diffusion probabilistic models, and after me, Carson", "tokens": [50834, 286, 486, 751, 466, 264, 44821, 293, 413, 3661, 5704, 31959, 3142, 5245, 11, 293, 934, 385, 11, 38731, 51060], "temperature": 0.0, "avg_logprob": -0.2403659224510193, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.04482399299740791}, {"id": 81, "seek": 38412, "start": 398.04, "end": 402.4, "text": " will talk about the score-based generative modeling with differential equations, and", "tokens": [51060, 486, 751, 466, 264, 6175, 12, 6032, 1337, 1166, 15983, 365, 15756, 11787, 11, 293, 51278], "temperature": 0.0, "avg_logprob": -0.2403659224510193, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.04482399299740791}, {"id": 82, "seek": 38412, "start": 402.4, "end": 407.0, "text": " after us, Rucci will talk about advanced techniques, mostly around accelerated sampling", "tokens": [51278, 934, 505, 11, 497, 30670, 486, 751, 466, 7339, 7512, 11, 5240, 926, 29763, 21179, 51508], "temperature": 0.0, "avg_logprob": -0.2403659224510193, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.04482399299740791}, {"id": 83, "seek": 38412, "start": 407.0, "end": 409.0, "text": " and condition generation.", "tokens": [51508, 293, 4188, 5125, 13, 51608], "temperature": 0.0, "avg_logprob": -0.2403659224510193, "compression_ratio": 1.7078189300411524, "no_speech_prob": 0.04482399299740791}, {"id": 84, "seek": 40900, "start": 409.0, "end": 413.92, "text": " These parts, each one of them would be roughly around 65 minutes to 45 minutes.", "tokens": [50364, 1981, 3166, 11, 1184, 472, 295, 552, 576, 312, 9810, 926, 11624, 2077, 281, 6905, 2077, 13, 50610], "temperature": 0.0, "avg_logprob": -0.31272387504577637, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.01353837363421917}, {"id": 85, "seek": 40900, "start": 413.92, "end": 419.0, "text": " After these parts, we're going to have three short parts around applications, and mostly", "tokens": [50610, 2381, 613, 3166, 11, 321, 434, 516, 281, 362, 1045, 2099, 3166, 926, 5821, 11, 293, 5240, 50864], "temperature": 0.0, "avg_logprob": -0.31272387504577637, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.01353837363421917}, {"id": 86, "seek": 40900, "start": 419.0, "end": 425.32, "text": " computer vision applications that have been recently used diffusion models in their core", "tokens": [50864, 3820, 5201, 5821, 300, 362, 668, 3938, 1143, 25242, 5245, 294, 641, 4965, 51180], "temperature": 0.0, "avg_logprob": -0.31272387504577637, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.01353837363421917}, {"id": 87, "seek": 40900, "start": 425.32, "end": 428.8, "text": " to tackle various deep generative learning-related applications.", "tokens": [51180, 281, 14896, 3683, 2452, 1337, 1166, 2539, 12, 12004, 5821, 13, 51354], "temperature": 0.0, "avg_logprob": -0.31272387504577637, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.01353837363421917}, {"id": 88, "seek": 40900, "start": 428.8, "end": 433.44, "text": " Finally, we're going to have a very short segment where I will talk about conclusions", "tokens": [51354, 6288, 11, 321, 434, 516, 281, 362, 257, 588, 2099, 9469, 689, 286, 486, 751, 466, 22865, 51586], "temperature": 0.0, "avg_logprob": -0.31272387504577637, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.01353837363421917}, {"id": 89, "seek": 40900, "start": 433.44, "end": 436.2, "text": " of open problems and final remarks.", "tokens": [51586, 295, 1269, 2740, 293, 2572, 19151, 13, 51724], "temperature": 0.0, "avg_logprob": -0.31272387504577637, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.01353837363421917}, {"id": 90, "seek": 43620, "start": 436.84, "end": 442.32, "text": " Before starting, I'd like to mention that our slides, videos, and some content will be", "tokens": [50396, 4546, 2891, 11, 286, 1116, 411, 281, 2152, 300, 527, 9788, 11, 2145, 11, 293, 512, 2701, 486, 312, 50670], "temperature": 0.0, "avg_logprob": -0.22773895263671876, "compression_ratio": 1.828193832599119, "no_speech_prob": 0.020441394299268723}, {"id": 91, "seek": 43620, "start": 442.32, "end": 446.8, "text": " available on this website, so I'd like to encourage you to bookmark this website.", "tokens": [50670, 2435, 322, 341, 3144, 11, 370, 286, 1116, 411, 281, 5373, 291, 281, 1446, 5638, 341, 3144, 13, 50894], "temperature": 0.0, "avg_logprob": -0.22773895263671876, "compression_ratio": 1.828193832599119, "no_speech_prob": 0.020441394299268723}, {"id": 92, "seek": 43620, "start": 446.8, "end": 451.64, "text": " We're hoping to add more content in the future to this website.", "tokens": [50894, 492, 434, 7159, 281, 909, 544, 2701, 294, 264, 2027, 281, 341, 3144, 13, 51136], "temperature": 0.0, "avg_logprob": -0.22773895263671876, "compression_ratio": 1.828193832599119, "no_speech_prob": 0.020441394299268723}, {"id": 93, "seek": 43620, "start": 453.64, "end": 458.91999999999996, "text": " Before starting the presentation, I'd like to just mention that we did our best to include", "tokens": [51236, 4546, 2891, 264, 5860, 11, 286, 1116, 411, 281, 445, 2152, 300, 321, 630, 527, 1151, 281, 4090, 51500], "temperature": 0.0, "avg_logprob": -0.22773895263671876, "compression_ratio": 1.828193832599119, "no_speech_prob": 0.020441394299268723}, {"id": 94, "seek": 43620, "start": 458.91999999999996, "end": 466.15999999999997, "text": " as many papers that we could, and we thought that it could be interesting to the community.", "tokens": [51500, 382, 867, 10577, 300, 321, 727, 11, 293, 321, 1194, 300, 309, 727, 312, 1880, 281, 264, 1768, 13, 51862], "temperature": 0.0, "avg_logprob": -0.22773895263671876, "compression_ratio": 1.828193832599119, "no_speech_prob": 0.020441394299268723}, {"id": 95, "seek": 46616, "start": 466.16, "end": 471.8, "text": " However, due to limited time and capacity, of course, we could not include every paper.", "tokens": [50364, 2908, 11, 3462, 281, 5567, 565, 293, 6042, 11, 295, 1164, 11, 321, 727, 406, 4090, 633, 3035, 13, 50646], "temperature": 0.0, "avg_logprob": -0.19564714971578345, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0006852475344203413}, {"id": 96, "seek": 46616, "start": 471.8, "end": 475.8, "text": " So if there's a particular paper that you are passionate about, you're very excited", "tokens": [50646, 407, 498, 456, 311, 257, 1729, 3035, 300, 291, 366, 11410, 466, 11, 291, 434, 588, 2919, 50846], "temperature": 0.0, "avg_logprob": -0.19564714971578345, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0006852475344203413}, {"id": 97, "seek": 46616, "start": 475.8, "end": 481.8, "text": " about it, and you would like to be included in this tutorial, we apologize that we couldn't", "tokens": [50846, 466, 309, 11, 293, 291, 576, 411, 281, 312, 5556, 294, 341, 7073, 11, 321, 12328, 300, 321, 2809, 380, 51146], "temperature": 0.0, "avg_logprob": -0.19564714971578345, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0006852475344203413}, {"id": 98, "seek": 46616, "start": 481.8, "end": 486.0, "text": " do that, and what we encourage you to send us an email, let us know that there was a", "tokens": [51146, 360, 300, 11, 293, 437, 321, 5373, 291, 281, 2845, 505, 364, 3796, 11, 718, 505, 458, 300, 456, 390, 257, 51356], "temperature": 0.0, "avg_logprob": -0.19564714971578345, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0006852475344203413}, {"id": 99, "seek": 46616, "start": 486.0, "end": 490.64000000000004, "text": " paper that would be interesting to have in our program, and hopefully, in the future", "tokens": [51356, 3035, 300, 576, 312, 1880, 281, 362, 294, 527, 1461, 11, 293, 4696, 11, 294, 264, 2027, 51588], "temperature": 0.0, "avg_logprob": -0.19564714971578345, "compression_ratio": 1.7530364372469636, "no_speech_prob": 0.0006852475344203413}, {"id": 100, "seek": 49064, "start": 490.68, "end": 495.64, "text": " versions of this tutorial, we will try to include those papers as well.", "tokens": [50366, 9606, 295, 341, 7073, 11, 321, 486, 853, 281, 4090, 729, 10577, 382, 731, 13, 50614], "temperature": 0.0, "avg_logprob": -0.20612744783100329, "compression_ratio": 1.9455445544554455, "no_speech_prob": 0.0032049089204519987}, {"id": 101, "seek": 49064, "start": 497.64, "end": 503.64, "text": " With that in mind, I will start the first segment, the noise and diffusion probabilistic models.", "tokens": [50714, 2022, 300, 294, 1575, 11, 286, 486, 722, 264, 700, 9469, 11, 264, 5658, 293, 25242, 31959, 3142, 5245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20612744783100329, "compression_ratio": 1.9455445544554455, "no_speech_prob": 0.0032049089204519987}, {"id": 102, "seek": 49064, "start": 505.64, "end": 508.24, "text": " So part one, the noise and diffusion probabilistic model.", "tokens": [51114, 407, 644, 472, 11, 264, 5658, 293, 25242, 31959, 3142, 2316, 13, 51244], "temperature": 0.0, "avg_logprob": -0.20612744783100329, "compression_ratio": 1.9455445544554455, "no_speech_prob": 0.0032049089204519987}, {"id": 103, "seek": 49064, "start": 508.24, "end": 513.36, "text": " Here you can see an image of three cute dogs who are trying to understand the noise and", "tokens": [51244, 1692, 291, 393, 536, 364, 3256, 295, 1045, 4052, 7197, 567, 366, 1382, 281, 1223, 264, 5658, 293, 51500], "temperature": 0.0, "avg_logprob": -0.20612744783100329, "compression_ratio": 1.9455445544554455, "no_speech_prob": 0.0032049089204519987}, {"id": 104, "seek": 49064, "start": 513.36, "end": 517.28, "text": " diffusion probabilistic models, and as you can see, they're a little bit lost.", "tokens": [51500, 25242, 31959, 3142, 5245, 11, 293, 382, 291, 393, 536, 11, 436, 434, 257, 707, 857, 2731, 13, 51696], "temperature": 0.0, "avg_logprob": -0.20612744783100329, "compression_ratio": 1.9455445544554455, "no_speech_prob": 0.0032049089204519987}, {"id": 105, "seek": 51728, "start": 517.28, "end": 522.4399999999999, "text": " So let's go over these models and discuss how these models can be generated.", "tokens": [50364, 407, 718, 311, 352, 670, 613, 5245, 293, 2248, 577, 613, 5245, 393, 312, 10833, 13, 50622], "temperature": 0.0, "avg_logprob": -0.3032332041177405, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.086095527280122e-05}, {"id": 106, "seek": 51728, "start": 527.36, "end": 534.3199999999999, "text": " So using diffusion models, officially consists of two processes, a forward diffusion process", "tokens": [50868, 407, 1228, 25242, 5245, 11, 12053, 14689, 295, 732, 7555, 11, 257, 2128, 25242, 1399, 51216], "temperature": 0.0, "avg_logprob": -0.3032332041177405, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.086095527280122e-05}, {"id": 107, "seek": 51728, "start": 534.92, "end": 536.8, "text": " that gradually adds noise to input.", "tokens": [51246, 300, 13145, 10860, 5658, 281, 4846, 13, 51340], "temperature": 0.0, "avg_logprob": -0.3032332041177405, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.086095527280122e-05}, {"id": 108, "seek": 51728, "start": 537.4399999999999, "end": 540.48, "text": " This process is shown on this figure from left to right.", "tokens": [51372, 639, 1399, 307, 4898, 322, 341, 2573, 490, 1411, 281, 558, 13, 51524], "temperature": 0.0, "avg_logprob": -0.3032332041177405, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.086095527280122e-05}, {"id": 109, "seek": 51728, "start": 541.28, "end": 543.68, "text": " It starts from image of this cute cat.", "tokens": [51564, 467, 3719, 490, 3256, 295, 341, 4052, 3857, 13, 51684], "temperature": 0.0, "avg_logprob": -0.3032332041177405, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.086095527280122e-05}, {"id": 110, "seek": 51728, "start": 544.04, "end": 546.04, "text": " His name is Peanut, he's my cat.", "tokens": [51702, 2812, 1315, 307, 48069, 11, 415, 311, 452, 3857, 13, 51802], "temperature": 0.0, "avg_logprob": -0.3032332041177405, "compression_ratio": 1.5980861244019138, "no_speech_prob": 6.086095527280122e-05}, {"id": 111, "seek": 54604, "start": 546.4, "end": 551.9599999999999, "text": " We can start from him, and we're going to add noise to this image one step at a time.", "tokens": [50382, 492, 393, 722, 490, 796, 11, 293, 321, 434, 516, 281, 909, 5658, 281, 341, 3256, 472, 1823, 412, 257, 565, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1544371527068469, "compression_ratio": 1.793859649122807, "no_speech_prob": 0.0014453313779085875}, {"id": 112, "seek": 54604, "start": 552.68, "end": 557.36, "text": " The forward diffusion process does this in so many steps such that eventually on the", "tokens": [50696, 440, 2128, 25242, 1399, 775, 341, 294, 370, 867, 4439, 1270, 300, 4728, 322, 264, 50930], "temperature": 0.0, "avg_logprob": -0.1544371527068469, "compression_ratio": 1.793859649122807, "no_speech_prob": 0.0014453313779085875}, {"id": 113, "seek": 54604, "start": 557.36, "end": 560.36, "text": " right side, we converge to white noise distribution.", "tokens": [50930, 558, 1252, 11, 321, 41881, 281, 2418, 5658, 7316, 13, 51080], "temperature": 0.0, "avg_logprob": -0.1544371527068469, "compression_ratio": 1.793859649122807, "no_speech_prob": 0.0014453313779085875}, {"id": 114, "seek": 54604, "start": 561.28, "end": 567.56, "text": " The second process is the reverse denoising process that learns to generate data by denoising.", "tokens": [51126, 440, 1150, 1399, 307, 264, 9943, 1441, 78, 3436, 1399, 300, 27152, 281, 8460, 1412, 538, 1441, 78, 3436, 13, 51440], "temperature": 0.0, "avg_logprob": -0.1544371527068469, "compression_ratio": 1.793859649122807, "no_speech_prob": 0.0014453313779085875}, {"id": 115, "seek": 54604, "start": 568.12, "end": 573.68, "text": " This process is shown on this slide from right to left, and it starts from white noise and", "tokens": [51468, 639, 1399, 307, 4898, 322, 341, 4137, 490, 558, 281, 1411, 11, 293, 309, 3719, 490, 2418, 5658, 293, 51746], "temperature": 0.0, "avg_logprob": -0.1544371527068469, "compression_ratio": 1.793859649122807, "no_speech_prob": 0.0014453313779085875}, {"id": 116, "seek": 57368, "start": 573.68, "end": 577.3599999999999, "text": " it learns to generate data on the left side by denoising.", "tokens": [50364, 309, 27152, 281, 8460, 1412, 322, 264, 1411, 1252, 538, 1441, 78, 3436, 13, 50548], "temperature": 0.0, "avg_logprob": -0.2056706148035386, "compression_ratio": 1.786096256684492, "no_speech_prob": 0.00020326195226516575}, {"id": 117, "seek": 57368, "start": 577.8, "end": 583.88, "text": " So this process will take a noisy image, and it will generate a less noisy version of it,", "tokens": [50570, 407, 341, 1399, 486, 747, 257, 24518, 3256, 11, 293, 309, 486, 8460, 257, 1570, 24518, 3037, 295, 309, 11, 50874], "temperature": 0.0, "avg_logprob": -0.2056706148035386, "compression_ratio": 1.786096256684492, "no_speech_prob": 0.00020326195226516575}, {"id": 118, "seek": 57368, "start": 583.88, "end": 588.92, "text": " and it will repeat this process such that it can convert noise to data.", "tokens": [50874, 293, 309, 486, 7149, 341, 1399, 1270, 300, 309, 393, 7620, 5658, 281, 1412, 13, 51126], "temperature": 0.0, "avg_logprob": -0.2056706148035386, "compression_ratio": 1.786096256684492, "no_speech_prob": 0.00020326195226516575}, {"id": 119, "seek": 57368, "start": 589.76, "end": 594.3199999999999, "text": " So we're going to dig deeper into these two processes, and we will see how we can define", "tokens": [51168, 407, 321, 434, 516, 281, 2528, 7731, 666, 613, 732, 7555, 11, 293, 321, 486, 536, 577, 321, 393, 6964, 51396], "temperature": 0.0, "avg_logprob": -0.2056706148035386, "compression_ratio": 1.786096256684492, "no_speech_prob": 0.00020326195226516575}, {"id": 120, "seek": 57368, "start": 594.3199999999999, "end": 595.8, "text": " these processes formally.", "tokens": [51396, 613, 7555, 25983, 13, 51470], "temperature": 0.0, "avg_logprob": -0.2056706148035386, "compression_ratio": 1.786096256684492, "no_speech_prob": 0.00020326195226516575}, {"id": 121, "seek": 59580, "start": 596.24, "end": 602.28, "text": " Then the forward diffusion process, as I said, starts from data and generates these intermediate", "tokens": [50386, 1396, 264, 2128, 25242, 1399, 11, 382, 286, 848, 11, 3719, 490, 1412, 293, 23815, 613, 19376, 50688], "temperature": 0.0, "avg_logprob": -0.3498690070175543, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.0005433214246295393}, {"id": 122, "seek": 59580, "start": 602.28, "end": 606.4399999999999, "text": " noisy images, by just simply adding noise one step at a time.", "tokens": [50688, 24518, 5267, 11, 538, 445, 2935, 5127, 5658, 472, 1823, 412, 257, 565, 13, 50896], "temperature": 0.0, "avg_logprob": -0.3498690070175543, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.0005433214246295393}, {"id": 123, "seek": 59580, "start": 607.3199999999999, "end": 613.4, "text": " At every step, we're going to assume that we're going to use a normal distribution to generate", "tokens": [50940, 1711, 633, 1823, 11, 321, 434, 516, 281, 6552, 300, 321, 434, 516, 281, 764, 257, 2710, 7316, 281, 8460, 51244], "temperature": 0.0, "avg_logprob": -0.3498690070175543, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.0005433214246295393}, {"id": 124, "seek": 59580, "start": 613.4, "end": 617.0, "text": " this noisy image condition on the previous image.", "tokens": [51244, 341, 24518, 3256, 4188, 322, 264, 3894, 3256, 13, 51424], "temperature": 0.0, "avg_logprob": -0.3498690070175543, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.0005433214246295393}, {"id": 125, "seek": 59580, "start": 617.8, "end": 621.52, "text": " This normal distribution will have a very simple form.", "tokens": [51464, 639, 2710, 7316, 486, 362, 257, 588, 2199, 1254, 13, 51650], "temperature": 0.0, "avg_logprob": -0.3498690070175543, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.0005433214246295393}, {"id": 126, "seek": 62152, "start": 621.6, "end": 627.4399999999999, "text": " We're going to represent this normal distribution using q that takes this x at the previous", "tokens": [50368, 492, 434, 516, 281, 2906, 341, 2710, 7316, 1228, 9505, 300, 2516, 341, 2031, 412, 264, 3894, 50660], "temperature": 0.0, "avg_logprob": -0.32329185192401594, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.001380880712531507}, {"id": 127, "seek": 62152, "start": 627.4399999999999, "end": 630.4399999999999, "text": " step and generate x at the current step.", "tokens": [50660, 1823, 293, 8460, 2031, 412, 264, 2190, 1823, 13, 50810], "temperature": 0.0, "avg_logprob": -0.32329185192401594, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.001380880712531507}, {"id": 128, "seek": 62152, "start": 630.4399999999999, "end": 633.4399999999999, "text": " So it takes, for example, x1, and it generates x2.", "tokens": [50810, 407, 309, 2516, 11, 337, 1365, 11, 2031, 16, 11, 293, 309, 23815, 2031, 17, 13, 50960], "temperature": 0.0, "avg_logprob": -0.32329185192401594, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.001380880712531507}, {"id": 129, "seek": 62152, "start": 634.3199999999999, "end": 640.84, "text": " As I said, it's a normal distribution over the current step, xt, where the mean is denoted", "tokens": [51004, 1018, 286, 848, 11, 309, 311, 257, 2710, 7316, 670, 264, 2190, 1823, 11, 2031, 83, 11, 689, 264, 914, 307, 1441, 23325, 51330], "temperature": 0.0, "avg_logprob": -0.32329185192401594, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.001380880712531507}, {"id": 130, "seek": 62152, "start": 640.84, "end": 646.4399999999999, "text": " by this square root of one minus beta t times the image at the previous step, and this beta", "tokens": [51330, 538, 341, 3732, 5593, 295, 472, 3175, 9861, 256, 1413, 264, 3256, 412, 264, 3894, 1823, 11, 293, 341, 9861, 51610], "temperature": 0.0, "avg_logprob": -0.32329185192401594, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.001380880712531507}, {"id": 131, "seek": 62152, "start": 646.4399999999999, "end": 648.4399999999999, "text": " t representing the variance.", "tokens": [51610, 256, 13460, 264, 21977, 13, 51710], "temperature": 0.0, "avg_logprob": -0.32329185192401594, "compression_ratio": 1.8036529680365296, "no_speech_prob": 0.001380880712531507}, {"id": 132, "seek": 64844, "start": 649.36, "end": 656.36, "text": " For the moment, assume that this beta t is just simply a very small positive scalar value.", "tokens": [50410, 1171, 264, 1623, 11, 6552, 300, 341, 9861, 256, 307, 445, 2935, 257, 588, 1359, 3353, 39684, 2158, 13, 50760], "temperature": 0.0, "avg_logprob": -0.3698051854183799, "compression_ratio": 1.560846560846561, "no_speech_prob": 0.00022995179460849613}, {"id": 133, "seek": 64844, "start": 656.36, "end": 660.36, "text": " It can be like 0.001, some selectors.", "tokens": [50760, 467, 393, 312, 411, 1958, 13, 628, 16, 11, 512, 3048, 830, 13, 50960], "temperature": 0.0, "avg_logprob": -0.3698051854183799, "compression_ratio": 1.560846560846561, "no_speech_prob": 0.00022995179460849613}, {"id": 134, "seek": 64844, "start": 662.36, "end": 667.36, "text": " Here, this normal distribution basically takes the image at the previous steps.", "tokens": [51060, 1692, 11, 341, 2710, 7316, 1936, 2516, 264, 3256, 412, 264, 3894, 4439, 13, 51310], "temperature": 0.0, "avg_logprob": -0.3698051854183799, "compression_ratio": 1.560846560846561, "no_speech_prob": 0.00022995179460849613}, {"id": 135, "seek": 64844, "start": 667.36, "end": 674.36, "text": " It rescale this image, the pixel values on this image, by the square root of one minus", "tokens": [51310, 467, 9610, 1220, 341, 3256, 11, 264, 19261, 4190, 322, 341, 3256, 11, 538, 264, 3732, 5593, 295, 472, 3175, 51660], "temperature": 0.0, "avg_logprob": -0.3698051854183799, "compression_ratio": 1.560846560846561, "no_speech_prob": 0.00022995179460849613}, {"id": 136, "seek": 67436, "start": 675.28, "end": 682.28, "text": " beta t, and it adds a tiny bit of noise where the variance of noise is beta t.", "tokens": [50410, 9861, 256, 11, 293, 309, 10860, 257, 5870, 857, 295, 5658, 689, 264, 21977, 295, 5658, 307, 9861, 256, 13, 50760], "temperature": 0.0, "avg_logprob": -0.18488091029477924, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.0001684065064182505}, {"id": 137, "seek": 67436, "start": 682.28, "end": 688.28, "text": " So this is just a diffusion kind of we can call per time step, because we had this very", "tokens": [50760, 407, 341, 307, 445, 257, 25242, 733, 295, 321, 393, 818, 680, 565, 1823, 11, 570, 321, 632, 341, 588, 51060], "temperature": 0.0, "avg_logprob": -0.18488091029477924, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.0001684065064182505}, {"id": 138, "seek": 67436, "start": 688.28, "end": 695.28, "text": " simple form per time step, like per step, in order to generate xt given xt minus one.", "tokens": [51060, 2199, 1254, 680, 565, 1823, 11, 411, 680, 1823, 11, 294, 1668, 281, 8460, 2031, 83, 2212, 2031, 83, 3175, 472, 13, 51410], "temperature": 0.0, "avg_logprob": -0.18488091029477924, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.0001684065064182505}, {"id": 139, "seek": 67436, "start": 695.28, "end": 701.28, "text": " Now, we can also define the joint distribution for all the samples that will be generated", "tokens": [51410, 823, 11, 321, 393, 611, 6964, 264, 7225, 7316, 337, 439, 264, 10938, 300, 486, 312, 10833, 51710], "temperature": 0.0, "avg_logprob": -0.18488091029477924, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.0001684065064182505}, {"id": 140, "seek": 70128, "start": 702.1999999999999, "end": 707.1999999999999, "text": " in this trajectory, starting from x1, all the way going to x capital T.", "tokens": [50410, 294, 341, 21512, 11, 2891, 490, 2031, 16, 11, 439, 264, 636, 516, 281, 2031, 4238, 314, 13, 50660], "temperature": 0.0, "avg_logprob": -0.2179420043011101, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0003680785303004086}, {"id": 141, "seek": 70128, "start": 707.1999999999999, "end": 712.1999999999999, "text": " Capital T represents the number of steps in our model, and the joint distribution of all", "tokens": [50660, 21502, 314, 8855, 264, 1230, 295, 4439, 294, 527, 2316, 11, 293, 264, 7225, 7316, 295, 439, 50910], "temperature": 0.0, "avg_logprob": -0.2179420043011101, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0003680785303004086}, {"id": 142, "seek": 70128, "start": 712.1999999999999, "end": 718.1999999999999, "text": " these samples, condition of this input image of peanut, will be the product of conditionals", "tokens": [50910, 613, 10938, 11, 4188, 295, 341, 4846, 3256, 295, 19209, 11, 486, 312, 264, 1674, 295, 4188, 1124, 51210], "temperature": 0.0, "avg_logprob": -0.2179420043011101, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0003680785303004086}, {"id": 143, "seek": 70128, "start": 718.1999999999999, "end": 721.1999999999999, "text": " that are formed at each step.", "tokens": [51210, 300, 366, 8693, 412, 1184, 1823, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2179420043011101, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0003680785303004086}, {"id": 144, "seek": 70128, "start": 721.1999999999999, "end": 726.1999999999999, "text": " So this just represents the joint distribution of all the samples that will be generated", "tokens": [51360, 407, 341, 445, 8855, 264, 7225, 7316, 295, 439, 264, 10938, 300, 486, 312, 10833, 51610], "temperature": 0.0, "avg_logprob": -0.2179420043011101, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0003680785303004086}, {"id": 145, "seek": 70128, "start": 726.1999999999999, "end": 729.1999999999999, "text": " on this trajectory using this Markov process.", "tokens": [51610, 322, 341, 21512, 1228, 341, 3934, 5179, 1399, 13, 51760], "temperature": 0.0, "avg_logprob": -0.2179420043011101, "compression_ratio": 1.8289473684210527, "no_speech_prob": 0.0003680785303004086}, {"id": 146, "seek": 72920, "start": 730.12, "end": 735.12, "text": " This is a Markov process that generates one step, that generates examples one step at", "tokens": [50410, 639, 307, 257, 3934, 5179, 1399, 300, 23815, 472, 1823, 11, 300, 23815, 5110, 472, 1823, 412, 50660], "temperature": 0.0, "avg_logprob": -0.1858010689417521, "compression_ratio": 1.931937172774869, "no_speech_prob": 0.000866019050590694}, {"id": 147, "seek": 72920, "start": 735.12, "end": 738.12, "text": " the time, given the previous examples.", "tokens": [50660, 264, 565, 11, 2212, 264, 3894, 5110, 13, 50810], "temperature": 0.0, "avg_logprob": -0.1858010689417521, "compression_ratio": 1.931937172774869, "no_speech_prob": 0.000866019050590694}, {"id": 148, "seek": 72920, "start": 738.12, "end": 744.12, "text": " Now that we know how we can generate samples one step at a time, you may ask me, how can", "tokens": [50810, 823, 300, 321, 458, 577, 321, 393, 8460, 10938, 472, 1823, 412, 257, 565, 11, 291, 815, 1029, 385, 11, 577, 393, 51110], "temperature": 0.0, "avg_logprob": -0.1858010689417521, "compression_ratio": 1.931937172774869, "no_speech_prob": 0.000866019050590694}, {"id": 149, "seek": 72920, "start": 744.12, "end": 749.12, "text": " I now just take this input image and jump to particular time the step?", "tokens": [51110, 286, 586, 445, 747, 341, 4846, 3256, 293, 3012, 281, 1729, 565, 264, 1823, 30, 51360], "temperature": 0.0, "avg_logprob": -0.1858010689417521, "compression_ratio": 1.931937172774869, "no_speech_prob": 0.000866019050590694}, {"id": 150, "seek": 72920, "start": 749.12, "end": 754.12, "text": " Do I need to sample, generate samples one step at a time, or can I just take this x0", "tokens": [51360, 1144, 286, 643, 281, 6889, 11, 8460, 10938, 472, 1823, 412, 257, 565, 11, 420, 393, 286, 445, 747, 341, 2031, 15, 51610], "temperature": 0.0, "avg_logprob": -0.1858010689417521, "compression_ratio": 1.931937172774869, "no_speech_prob": 0.000866019050590694}, {"id": 151, "seek": 75412, "start": 755.04, "end": 760.04, "text": " and generate xt, or x4, for example, here, just directly?", "tokens": [50410, 293, 8460, 2031, 83, 11, 420, 2031, 19, 11, 337, 1365, 11, 510, 11, 445, 3838, 30, 50660], "temperature": 0.0, "avg_logprob": -0.2172195635343853, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.000984058016911149}, {"id": 152, "seek": 75412, "start": 760.04, "end": 766.04, "text": " Because in the forward process, we're using a very simple Gaussian kernel to diffuse the", "tokens": [50660, 1436, 294, 264, 2128, 1399, 11, 321, 434, 1228, 257, 588, 2199, 39148, 28256, 281, 42165, 264, 50960], "temperature": 0.0, "avg_logprob": -0.2172195635343853, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.000984058016911149}, {"id": 153, "seek": 75412, "start": 766.04, "end": 772.04, "text": " data, we can actually show that because of this simple form, we can first define this", "tokens": [50960, 1412, 11, 321, 393, 767, 855, 300, 570, 295, 341, 2199, 1254, 11, 321, 393, 700, 6964, 341, 51260], "temperature": 0.0, "avg_logprob": -0.2172195635343853, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.000984058016911149}, {"id": 154, "seek": 75412, "start": 772.04, "end": 774.04, "text": " scale.", "tokens": [51260, 4373, 13, 51360], "temperature": 0.0, "avg_logprob": -0.2172195635343853, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.000984058016911149}, {"id": 155, "seek": 75412, "start": 774.04, "end": 781.04, "text": " This scalar alpha bar T is the product of one minus betas from time to step one all the", "tokens": [51360, 639, 39684, 8961, 2159, 314, 307, 264, 1674, 295, 472, 3175, 778, 296, 490, 565, 281, 1823, 472, 439, 264, 51710], "temperature": 0.0, "avg_logprob": -0.2172195635343853, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.000984058016911149}, {"id": 156, "seek": 75412, "start": 781.04, "end": 783.04, "text": " way up to current step T.", "tokens": [51710, 636, 493, 281, 2190, 1823, 314, 13, 51810], "temperature": 0.0, "avg_logprob": -0.2172195635343853, "compression_ratio": 1.5829596412556053, "no_speech_prob": 0.000984058016911149}, {"id": 157, "seek": 78304, "start": 783.9599999999999, "end": 788.9599999999999, "text": " This is just defined based off the parameters of the diffusion kernel, and having defined", "tokens": [50410, 639, 307, 445, 7642, 2361, 766, 264, 9834, 295, 264, 25242, 28256, 11, 293, 1419, 7642, 50660], "temperature": 0.0, "avg_logprob": -0.22821835969623766, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00016854488058015704}, {"id": 158, "seek": 78304, "start": 788.9599999999999, "end": 795.9599999999999, "text": " this alpha bar T, now we can define a Gaussian kernel or the diffusion kernel that will generate", "tokens": [50660, 341, 8961, 2159, 314, 11, 586, 321, 393, 6964, 257, 39148, 28256, 420, 264, 25242, 28256, 300, 486, 8460, 51010], "temperature": 0.0, "avg_logprob": -0.22821835969623766, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00016854488058015704}, {"id": 159, "seek": 78304, "start": 795.9599999999999, "end": 798.9599999999999, "text": " xt, even x0.", "tokens": [51010, 2031, 83, 11, 754, 2031, 15, 13, 51160], "temperature": 0.0, "avg_logprob": -0.22821835969623766, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00016854488058015704}, {"id": 160, "seek": 78304, "start": 798.9599999999999, "end": 802.9599999999999, "text": " So, for example, we can now generate using this Gaussian kernel, we can sample from x4", "tokens": [51160, 407, 11, 337, 1365, 11, 321, 393, 586, 8460, 1228, 341, 39148, 28256, 11, 321, 393, 6889, 490, 2031, 19, 51360], "temperature": 0.0, "avg_logprob": -0.22821835969623766, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00016854488058015704}, {"id": 161, "seek": 78304, "start": 802.9599999999999, "end": 809.9599999999999, "text": " given x0, this would be again a normal distribution where mean is same as the input image, really", "tokens": [51360, 2212, 2031, 15, 11, 341, 576, 312, 797, 257, 2710, 7316, 689, 914, 307, 912, 382, 264, 4846, 3256, 11, 534, 51710], "temperature": 0.0, "avg_logprob": -0.22821835969623766, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.00016854488058015704}, {"id": 162, "seek": 80996, "start": 810.88, "end": 815.88, "text": " this square root of alpha bar T defined here alpha bar, and then the variance is also one", "tokens": [50410, 341, 3732, 5593, 295, 8961, 2159, 314, 7642, 510, 8961, 2159, 11, 293, 550, 264, 21977, 307, 611, 472, 50660], "temperature": 0.0, "avg_logprob": -0.180592044369205, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004952700692228973}, {"id": 163, "seek": 80996, "start": 815.88, "end": 817.88, "text": " minus alpha bar T.", "tokens": [50660, 3175, 8961, 2159, 314, 13, 50760], "temperature": 0.0, "avg_logprob": -0.180592044369205, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004952700692228973}, {"id": 164, "seek": 80996, "start": 817.88, "end": 824.88, "text": " So, just remember that these betas are just parameters of the diffusion process, we can", "tokens": [50760, 407, 11, 445, 1604, 300, 613, 778, 296, 366, 445, 9834, 295, 264, 25242, 1399, 11, 321, 393, 51110], "temperature": 0.0, "avg_logprob": -0.180592044369205, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004952700692228973}, {"id": 165, "seek": 80996, "start": 824.88, "end": 830.88, "text": " compute this alpha bar T very easily, and then we can sample from xt given x0 using", "tokens": [51110, 14722, 341, 8961, 2159, 314, 588, 3612, 11, 293, 550, 321, 393, 6889, 490, 2031, 83, 2212, 2031, 15, 1228, 51410], "temperature": 0.0, "avg_logprob": -0.180592044369205, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004952700692228973}, {"id": 166, "seek": 80996, "start": 830.88, "end": 835.88, "text": " this normal distribution, and this we're going to call this diffusion kernel that diffuses", "tokens": [51410, 341, 2710, 7316, 11, 293, 341, 321, 434, 516, 281, 818, 341, 25242, 28256, 300, 7593, 8355, 51660], "temperature": 0.0, "avg_logprob": -0.180592044369205, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004952700692228973}, {"id": 167, "seek": 83588, "start": 836.8, "end": 841.8, "text": " input at time step zero to time step xt.", "tokens": [50410, 4846, 412, 565, 1823, 4018, 281, 565, 1823, 2031, 83, 13, 50660], "temperature": 0.0, "avg_logprob": -0.20493922723787966, "compression_ratio": 1.8782608695652174, "no_speech_prob": 0.0003050514787901193}, {"id": 168, "seek": 83588, "start": 841.8, "end": 846.8, "text": " Recall that if you want to sample from just a simple normal distribution, you can use", "tokens": [50660, 9647, 336, 300, 498, 291, 528, 281, 6889, 490, 445, 257, 2199, 2710, 7316, 11, 291, 393, 764, 50910], "temperature": 0.0, "avg_logprob": -0.20493922723787966, "compression_ratio": 1.8782608695652174, "no_speech_prob": 0.0003050514787901193}, {"id": 169, "seek": 83588, "start": 846.8, "end": 848.8, "text": " the reparameterization trick.", "tokens": [50910, 264, 1085, 12835, 2398, 2144, 4282, 13, 51010], "temperature": 0.0, "avg_logprob": -0.20493922723787966, "compression_ratio": 1.8782608695652174, "no_speech_prob": 0.0003050514787901193}, {"id": 170, "seek": 83588, "start": 848.8, "end": 853.8, "text": " So, if you want to draw samples from xt, you can just set xt to mean plus some white", "tokens": [51010, 407, 11, 498, 291, 528, 281, 2642, 10938, 490, 2031, 83, 11, 291, 393, 445, 992, 2031, 83, 281, 914, 1804, 512, 2418, 51260], "temperature": 0.0, "avg_logprob": -0.20493922723787966, "compression_ratio": 1.8782608695652174, "no_speech_prob": 0.0003050514787901193}, {"id": 171, "seek": 83588, "start": 853.8, "end": 859.8, "text": " nose epsilon that is drawn from standard normal distribution, rescaled with this square", "tokens": [51260, 6690, 17889, 300, 307, 10117, 490, 3832, 2710, 7316, 11, 9610, 5573, 365, 341, 3732, 51560], "temperature": 0.0, "avg_logprob": -0.20493922723787966, "compression_ratio": 1.8782608695652174, "no_speech_prob": 0.0003050514787901193}, {"id": 172, "seek": 83588, "start": 859.8, "end": 864.8, "text": " root of one minus alpha bar T, which represents just a standard deviation of this normal distribution.", "tokens": [51560, 5593, 295, 472, 3175, 8961, 2159, 314, 11, 597, 8855, 445, 257, 3832, 25163, 295, 341, 2710, 7316, 13, 51810], "temperature": 0.0, "avg_logprob": -0.20493922723787966, "compression_ratio": 1.8782608695652174, "no_speech_prob": 0.0003050514787901193}, {"id": 173, "seek": 86480, "start": 864.8, "end": 871.8, "text": " So, using this we can just simply generate samples at time step T given samples at time", "tokens": [50364, 407, 11, 1228, 341, 321, 393, 445, 2935, 8460, 10938, 412, 565, 1823, 314, 2212, 10938, 412, 565, 50714], "temperature": 0.0, "avg_logprob": -0.20804753023035386, "compression_ratio": 1.8138297872340425, "no_speech_prob": 0.00017113424837589264}, {"id": 174, "seek": 86480, "start": 871.8, "end": 876.8, "text": " step zero, so given x0 we can just diffuse it easily to time step T.", "tokens": [50714, 1823, 4018, 11, 370, 2212, 2031, 15, 321, 393, 445, 42165, 309, 3612, 281, 565, 1823, 314, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20804753023035386, "compression_ratio": 1.8138297872340425, "no_speech_prob": 0.00017113424837589264}, {"id": 175, "seek": 86480, "start": 876.8, "end": 883.8, "text": " Beta T values are important, these are basically, we're going to call beta T values as the noise", "tokens": [50964, 33286, 314, 4190, 366, 1021, 11, 613, 366, 1936, 11, 321, 434, 516, 281, 818, 9861, 314, 4190, 382, 264, 5658, 51314], "temperature": 0.0, "avg_logprob": -0.20804753023035386, "compression_ratio": 1.8138297872340425, "no_speech_prob": 0.00017113424837589264}, {"id": 176, "seek": 86480, "start": 883.8, "end": 889.8, "text": " schedule that defines how we're diffusing the data, and this noise schedule, this noise", "tokens": [51314, 7567, 300, 23122, 577, 321, 434, 7593, 7981, 264, 1412, 11, 293, 341, 5658, 7567, 11, 341, 5658, 51614], "temperature": 0.0, "avg_logprob": -0.20804753023035386, "compression_ratio": 1.8138297872340425, "no_speech_prob": 0.00017113424837589264}, {"id": 177, "seek": 88980, "start": 889.8, "end": 895.8, "text": " schedule, designs such that alpha bar T, this alpha bar at the very last step, would", "tokens": [50364, 7567, 11, 11347, 1270, 300, 8961, 2159, 314, 11, 341, 8961, 2159, 412, 264, 588, 1036, 1823, 11, 576, 50664], "temperature": 0.0, "avg_logprob": -0.1533082363217376, "compression_ratio": 1.760204081632653, "no_speech_prob": 0.0005699238972738385}, {"id": 178, "seek": 88980, "start": 895.8, "end": 901.8, "text": " converge to zero, and when this converges to zero, if you just set alpha bar T to zero", "tokens": [50664, 41881, 281, 4018, 11, 293, 562, 341, 9652, 2880, 281, 4018, 11, 498, 291, 445, 992, 8961, 2159, 314, 281, 4018, 50964], "temperature": 0.0, "avg_logprob": -0.1533082363217376, "compression_ratio": 1.760204081632653, "no_speech_prob": 0.0005699238972738385}, {"id": 179, "seek": 88980, "start": 901.8, "end": 906.8, "text": " here, you're going to see that because the way that the forward diffusion process is", "tokens": [50964, 510, 11, 291, 434, 516, 281, 536, 300, 570, 264, 636, 300, 264, 2128, 25242, 1399, 307, 51214], "temperature": 0.0, "avg_logprob": -0.1533082363217376, "compression_ratio": 1.760204081632653, "no_speech_prob": 0.0005699238972738385}, {"id": 180, "seek": 88980, "start": 906.8, "end": 913.8, "text": " defined, this diffusion kernel at last step given the x capital T given x0 would be just", "tokens": [51214, 7642, 11, 341, 25242, 28256, 412, 1036, 1823, 2212, 264, 2031, 4238, 314, 2212, 2031, 15, 576, 312, 445, 51564], "temperature": 0.0, "avg_logprob": -0.1533082363217376, "compression_ratio": 1.760204081632653, "no_speech_prob": 0.0005699238972738385}, {"id": 181, "seek": 91380, "start": 913.8, "end": 918.8, "text": " can be approximately using normal distribution, standard normal distribution.", "tokens": [50364, 393, 312, 10447, 1228, 2710, 7316, 11, 3832, 2710, 7316, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17178450958638253, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0004954320611432195}, {"id": 182, "seek": 91380, "start": 918.8, "end": 923.8, "text": " This basically means that at the end of this process, diffuse data will have just a standard", "tokens": [50614, 639, 1936, 1355, 300, 412, 264, 917, 295, 341, 1399, 11, 42165, 1412, 486, 362, 445, 257, 3832, 50864], "temperature": 0.0, "avg_logprob": -0.17178450958638253, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0004954320611432195}, {"id": 183, "seek": 91380, "start": 923.8, "end": 928.8, "text": " normal distribution, this is something that we will need later when we want to define a", "tokens": [50864, 2710, 7316, 11, 341, 307, 746, 300, 321, 486, 643, 1780, 562, 321, 528, 281, 6964, 257, 51114], "temperature": 0.0, "avg_logprob": -0.17178450958638253, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0004954320611432195}, {"id": 184, "seek": 91380, "start": 928.8, "end": 931.8, "text": " generative point.", "tokens": [51114, 1337, 1166, 935, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17178450958638253, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0004954320611432195}, {"id": 185, "seek": 91380, "start": 931.8, "end": 937.8, "text": " Now that I've talked about this forward process, like how we can diffuse the diffusion", "tokens": [51264, 823, 300, 286, 600, 2825, 466, 341, 2128, 1399, 11, 411, 577, 321, 393, 42165, 264, 25242, 51564], "temperature": 0.0, "avg_logprob": -0.17178450958638253, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0004954320611432195}, {"id": 186, "seek": 93780, "start": 937.8, "end": 943.8, "text": " kernel that diffuses data, let's talk about the marginal diffuse data distribution, let's", "tokens": [50364, 28256, 300, 7593, 8355, 1412, 11, 718, 311, 751, 466, 264, 16885, 42165, 1412, 7316, 11, 718, 311, 50664], "temperature": 0.0, "avg_logprob": -0.10610121726989746, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0002569984644651413}, {"id": 187, "seek": 93780, "start": 943.8, "end": 948.8, "text": " talk about what happens to data distribution as we go forward in the diffusion process.", "tokens": [50664, 751, 466, 437, 2314, 281, 1412, 7316, 382, 321, 352, 2128, 294, 264, 25242, 1399, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10610121726989746, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0002569984644651413}, {"id": 188, "seek": 93780, "start": 948.8, "end": 953.8, "text": " So, have in mind that diffusion kernel that generates xt given x0 is different than the", "tokens": [50914, 407, 11, 362, 294, 1575, 300, 25242, 28256, 300, 23815, 2031, 83, 2212, 2031, 15, 307, 819, 813, 264, 51164], "temperature": 0.0, "avg_logprob": -0.10610121726989746, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0002569984644651413}, {"id": 189, "seek": 93780, "start": 953.8, "end": 959.8, "text": " diffuse data distribution, so we're going to use qxt to represent diffuse data distribution,", "tokens": [51164, 42165, 1412, 7316, 11, 370, 321, 434, 516, 281, 764, 9505, 734, 281, 2906, 42165, 1412, 7316, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10610121726989746, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0002569984644651413}, {"id": 190, "seek": 93780, "start": 959.8, "end": 964.8, "text": " and in order to obtain this diffuse data distribution, we first need to form the joint", "tokens": [51464, 293, 294, 1668, 281, 12701, 341, 42165, 1412, 7316, 11, 321, 700, 643, 281, 1254, 264, 7225, 51714], "temperature": 0.0, "avg_logprob": -0.10610121726989746, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0002569984644651413}, {"id": 191, "seek": 96480, "start": 964.8, "end": 972.8, "text": " over clean data input data x0 and diffuse data xt, this joint simply can be defined as", "tokens": [50364, 670, 2541, 1412, 4846, 1412, 2031, 15, 293, 42165, 1412, 2031, 83, 11, 341, 7225, 2935, 393, 312, 7642, 382, 50764], "temperature": 0.0, "avg_logprob": -0.1418794790903727, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0022477905731648207}, {"id": 192, "seek": 96480, "start": 972.8, "end": 978.8, "text": " product of input data distribution qx0 times this diffusion kernel, which is just a simple", "tokens": [50764, 1674, 295, 4846, 1412, 7316, 9505, 87, 15, 1413, 341, 25242, 28256, 11, 597, 307, 445, 257, 2199, 51064], "temperature": 0.0, "avg_logprob": -0.1418794790903727, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0022477905731648207}, {"id": 193, "seek": 96480, "start": 978.8, "end": 984.8, "text": " normal distribution, and now we can marginalize at x0, we can just integrate at x0, and this", "tokens": [51064, 2710, 7316, 11, 293, 586, 321, 393, 16885, 1125, 412, 2031, 15, 11, 321, 393, 445, 13365, 412, 2031, 15, 11, 293, 341, 51364], "temperature": 0.0, "avg_logprob": -0.1418794790903727, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0022477905731648207}, {"id": 194, "seek": 96480, "start": 984.8, "end": 991.8, "text": " will give us marginal data diffuse data distribution at times the T.", "tokens": [51364, 486, 976, 505, 16885, 1412, 42165, 1412, 7316, 412, 1413, 264, 314, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1418794790903727, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.0022477905731648207}, {"id": 195, "seek": 99180, "start": 991.8, "end": 999.8, "text": " If we just consider a very simple one dimensional data, and we hear on visualizing on visualizing", "tokens": [50364, 759, 321, 445, 1949, 257, 588, 2199, 472, 18795, 1412, 11, 293, 321, 1568, 322, 5056, 3319, 322, 5056, 3319, 50764], "temperature": 0.0, "avg_logprob": -0.21319239139556884, "compression_ratio": 1.8829268292682926, "no_speech_prob": 0.0007188963936641812}, {"id": 196, "seek": 99180, "start": 999.8, "end": 1004.8, "text": " the diffuse data distribution at different time steps, on the left side we have data distribution", "tokens": [50764, 264, 42165, 1412, 7316, 412, 819, 565, 4439, 11, 322, 264, 1411, 1252, 321, 362, 1412, 7316, 51014], "temperature": 0.0, "avg_logprob": -0.21319239139556884, "compression_ratio": 1.8829268292682926, "no_speech_prob": 0.0007188963936641812}, {"id": 197, "seek": 99180, "start": 1004.8, "end": 1012.8, "text": " at times zero, why access represents this just one dimensional random variable and this x axis", "tokens": [51014, 412, 1413, 4018, 11, 983, 2105, 8855, 341, 445, 472, 18795, 4974, 7006, 293, 341, 2031, 10298, 51414], "temperature": 0.0, "avg_logprob": -0.21319239139556884, "compression_ratio": 1.8829268292682926, "no_speech_prob": 0.0007188963936641812}, {"id": 198, "seek": 99180, "start": 1012.8, "end": 1018.8, "text": " represents the PDF probability density function of this random variable at time step zero, this", "tokens": [51414, 8855, 264, 17752, 8482, 10305, 2445, 295, 341, 4974, 7006, 412, 565, 1823, 4018, 11, 341, 51714], "temperature": 0.0, "avg_logprob": -0.21319239139556884, "compression_ratio": 1.8829268292682926, "no_speech_prob": 0.0007188963936641812}, {"id": 199, "seek": 101880, "start": 1018.8, "end": 1025.8, "text": " is just the data distribution visualized for one toy example, one dimensional toy example.", "tokens": [50364, 307, 445, 264, 1412, 7316, 5056, 1602, 337, 472, 12058, 1365, 11, 472, 18795, 12058, 1365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12617805857717254, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0005876953946426511}, {"id": 200, "seek": 101880, "start": 1025.8, "end": 1031.8, "text": " Here you can see the visualization of diffuse data distributions, and as you can see, as we", "tokens": [50714, 1692, 291, 393, 536, 264, 25801, 295, 42165, 1412, 37870, 11, 293, 382, 291, 393, 536, 11, 382, 321, 51014], "temperature": 0.0, "avg_logprob": -0.12617805857717254, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0005876953946426511}, {"id": 201, "seek": 101880, "start": 1031.8, "end": 1036.8, "text": " go in the forward process, we just take this data distribution, we're making kind of this", "tokens": [51014, 352, 294, 264, 2128, 1399, 11, 321, 445, 747, 341, 1412, 7316, 11, 321, 434, 1455, 733, 295, 341, 51264], "temperature": 0.0, "avg_logprob": -0.12617805857717254, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0005876953946426511}, {"id": 202, "seek": 101880, "start": 1036.8, "end": 1041.8, "text": " distribution smoother and smoother as we go forward in time, and eventually it becomes so", "tokens": [51264, 7316, 28640, 293, 28640, 382, 321, 352, 2128, 294, 565, 11, 293, 4728, 309, 3643, 370, 51514], "temperature": 0.0, "avg_logprob": -0.12617805857717254, "compression_ratio": 1.865979381443299, "no_speech_prob": 0.0005876953946426511}, {"id": 203, "seek": 104180, "start": 1041.8, "end": 1047.8, "text": " smooth that we can just represent this distribution using standard normal distribution, zero mean", "tokens": [50364, 5508, 300, 321, 393, 445, 2906, 341, 7316, 1228, 3832, 2710, 7316, 11, 4018, 914, 50664], "temperature": 0.0, "avg_logprob": -0.15201810201009114, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0009107437217608094}, {"id": 204, "seek": 104180, "start": 1047.8, "end": 1054.8, "text": " unit variance normal distribution. As you see this smoothing process, we can actually show", "tokens": [50664, 4985, 21977, 2710, 7316, 13, 1018, 291, 536, 341, 899, 6259, 571, 1399, 11, 321, 393, 767, 855, 51014], "temperature": 0.0, "avg_logprob": -0.15201810201009114, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0009107437217608094}, {"id": 205, "seek": 104180, "start": 1054.8, "end": 1060.8, "text": " mathematically this, the diffusion kernel in the forward process, this diffusion kernel here", "tokens": [51014, 44003, 341, 11, 264, 25242, 28256, 294, 264, 2128, 1399, 11, 341, 25242, 28256, 510, 51314], "temperature": 0.0, "avg_logprob": -0.15201810201009114, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0009107437217608094}, {"id": 206, "seek": 104180, "start": 1060.8, "end": 1066.8, "text": " is kind of applying a Gaussian convolution to the data distribution, so this smoothing", "tokens": [51314, 307, 733, 295, 9275, 257, 39148, 45216, 281, 264, 1412, 7316, 11, 370, 341, 899, 6259, 571, 51614], "temperature": 0.0, "avg_logprob": -0.15201810201009114, "compression_ratio": 1.8308457711442787, "no_speech_prob": 0.0009107437217608094}, {"id": 207, "seek": 106680, "start": 1066.8, "end": 1070.8, "text": " process can be just represented as Gaussian convolution, a convolution in the sense of", "tokens": [50364, 1399, 393, 312, 445, 10379, 382, 39148, 45216, 11, 257, 45216, 294, 264, 2020, 295, 50564], "temperature": 0.0, "avg_logprob": -0.1595574254574983, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00016594016051385552}, {"id": 208, "seek": 106680, "start": 1070.8, "end": 1076.8, "text": " like signal processing convolution that takes input data distribution makes it smoother and", "tokens": [50564, 411, 6358, 9007, 45216, 300, 2516, 4846, 1412, 7316, 1669, 309, 28640, 293, 50864], "temperature": 0.0, "avg_logprob": -0.1595574254574983, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00016594016051385552}, {"id": 209, "seek": 106680, "start": 1076.8, "end": 1082.8, "text": " smoother. However, we should have in mind that we actually don't have access to these, to input", "tokens": [50864, 28640, 13, 2908, 11, 321, 820, 362, 294, 1575, 300, 321, 767, 500, 380, 362, 2105, 281, 613, 11, 281, 4846, 51164], "temperature": 0.0, "avg_logprob": -0.1595574254574983, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00016594016051385552}, {"id": 210, "seek": 106680, "start": 1082.8, "end": 1088.8, "text": " data distribution and intermediate diffuse data distribution practice, usually we only have", "tokens": [51164, 1412, 7316, 293, 19376, 42165, 1412, 7316, 3124, 11, 2673, 321, 787, 362, 51464], "temperature": 0.0, "avg_logprob": -0.1595574254574983, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00016594016051385552}, {"id": 211, "seek": 106680, "start": 1088.8, "end": 1094.8, "text": " samples from data distribution, we don't have access to the explicit form of the probability density", "tokens": [51464, 10938, 490, 1412, 7316, 11, 321, 500, 380, 362, 2105, 281, 264, 13691, 1254, 295, 264, 8482, 10305, 51764], "temperature": 0.0, "avg_logprob": -0.1595574254574983, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.00016594016051385552}, {"id": 212, "seek": 109480, "start": 1094.8, "end": 1102.8, "text": " function of data distribution, right? So even though we don't have access to this distribution", "tokens": [50364, 2445, 295, 1412, 7316, 11, 558, 30, 407, 754, 1673, 321, 500, 380, 362, 2105, 281, 341, 7316, 50764], "temperature": 0.0, "avg_logprob": -0.14747130719921256, "compression_ratio": 1.8592964824120604, "no_speech_prob": 0.00025669802562333643}, {"id": 213, "seek": 109480, "start": 1102.8, "end": 1109.8, "text": " or all the intermediate diffuse distributions, we know how to draw samples from diffuse data", "tokens": [50764, 420, 439, 264, 19376, 42165, 37870, 11, 321, 458, 577, 281, 2642, 10938, 490, 42165, 1412, 51114], "temperature": 0.0, "avg_logprob": -0.14747130719921256, "compression_ratio": 1.8592964824120604, "no_speech_prob": 0.00025669802562333643}, {"id": 214, "seek": 109480, "start": 1109.8, "end": 1113.8, "text": " distribution, so in order to generate data from diffuse data distribution, we can just simply", "tokens": [51114, 7316, 11, 370, 294, 1668, 281, 8460, 1412, 490, 42165, 1412, 7316, 11, 321, 393, 445, 2935, 51314], "temperature": 0.0, "avg_logprob": -0.14747130719921256, "compression_ratio": 1.8592964824120604, "no_speech_prob": 0.00025669802562333643}, {"id": 215, "seek": 109480, "start": 1113.8, "end": 1119.8, "text": " sample from training data x0, and then we can just follow the forward diffusion process,", "tokens": [51314, 6889, 490, 3097, 1412, 2031, 15, 11, 293, 550, 321, 393, 445, 1524, 264, 2128, 25242, 1399, 11, 51614], "temperature": 0.0, "avg_logprob": -0.14747130719921256, "compression_ratio": 1.8592964824120604, "no_speech_prob": 0.00025669802562333643}, {"id": 216, "seek": 111980, "start": 1119.8, "end": 1126.8, "text": " sample xt to an x0, in order to draw samples from xt, and this is basically the principle", "tokens": [50364, 6889, 2031, 83, 281, 364, 2031, 15, 11, 294, 1668, 281, 2642, 10938, 490, 2031, 83, 11, 293, 341, 307, 1936, 264, 8665, 50714], "temperature": 0.0, "avg_logprob": -0.20051163893479568, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.002604719949886203}, {"id": 217, "seek": 111980, "start": 1126.8, "end": 1131.8, "text": " of ancestral sampling that you can just basically use in order to generate data, for example, at times", "tokens": [50714, 295, 40049, 21179, 300, 291, 393, 445, 1936, 764, 294, 1668, 281, 8460, 1412, 11, 337, 1365, 11, 412, 1413, 50964], "temperature": 0.0, "avg_logprob": -0.20051163893479568, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.002604719949886203}, {"id": 218, "seek": 111980, "start": 1131.8, "end": 1138.8, "text": " that t, you can just use the training data, sample from training data, diffuse it and sample", "tokens": [50964, 300, 256, 11, 291, 393, 445, 764, 264, 3097, 1412, 11, 6889, 490, 3097, 1412, 11, 42165, 309, 293, 6889, 51314], "temperature": 0.0, "avg_logprob": -0.20051163893479568, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.002604719949886203}, {"id": 219, "seek": 111980, "start": 1138.8, "end": 1147.8, "text": " at xt using diffusion kernel, and that will give you samples from marginal data distribution.", "tokens": [51314, 412, 2031, 83, 1228, 25242, 28256, 11, 293, 300, 486, 976, 291, 10938, 490, 16885, 1412, 7316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.20051163893479568, "compression_ratio": 1.857843137254902, "no_speech_prob": 0.002604719949886203}, {"id": 220, "seek": 114780, "start": 1147.8, "end": 1156.8, "text": " Okay, so so far we talked about forward process and how this forward process just smoothens", "tokens": [50364, 1033, 11, 370, 370, 1400, 321, 2825, 466, 2128, 1399, 293, 577, 341, 2128, 1399, 445, 5508, 694, 50814], "temperature": 0.0, "avg_logprob": -0.12186539740789504, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.0005100028938613832}, {"id": 221, "seek": 114780, "start": 1156.8, "end": 1162.8, "text": " the data distribution, now let's talk about how we can define a generative model out of this.", "tokens": [50814, 264, 1412, 7316, 11, 586, 718, 311, 751, 466, 577, 321, 393, 6964, 257, 1337, 1166, 2316, 484, 295, 341, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12186539740789504, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.0005100028938613832}, {"id": 222, "seek": 114780, "start": 1162.8, "end": 1169.8, "text": " In order to generate data in diffusion models, what we can do, we can start from this base", "tokens": [51114, 682, 1668, 281, 8460, 1412, 294, 25242, 5245, 11, 437, 321, 393, 360, 11, 321, 393, 722, 490, 341, 3096, 51464], "temperature": 0.0, "avg_logprob": -0.12186539740789504, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.0005100028938613832}, {"id": 223, "seek": 114780, "start": 1169.8, "end": 1175.8, "text": " distribution at the end, we know that the diffuse data distribution would converge to", "tokens": [51464, 7316, 412, 264, 917, 11, 321, 458, 300, 264, 42165, 1412, 7316, 576, 41881, 281, 51764], "temperature": 0.0, "avg_logprob": -0.12186539740789504, "compression_ratio": 1.7920792079207921, "no_speech_prob": 0.0005100028938613832}, {"id": 224, "seek": 117580, "start": 1175.8, "end": 1182.8, "text": " 0 in unit variance, standard normal distribution, so we can sample from standard normal distribution", "tokens": [50364, 1958, 294, 4985, 21977, 11, 3832, 2710, 7316, 11, 370, 321, 393, 6889, 490, 3832, 2710, 7316, 50714], "temperature": 0.0, "avg_logprob": -0.17076424035159024, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.0026696568820625544}, {"id": 225, "seek": 117580, "start": 1182.8, "end": 1190.8, "text": " and we can follow the reverse process, where at every step we can now sample from the denoising model", "tokens": [50714, 293, 321, 393, 1524, 264, 9943, 1399, 11, 689, 412, 633, 1823, 321, 393, 586, 6889, 490, 264, 1441, 78, 3436, 2316, 51114], "temperature": 0.0, "avg_logprob": -0.17076424035159024, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.0026696568820625544}, {"id": 226, "seek": 117580, "start": 1190.8, "end": 1196.8, "text": " that generates the less noisy version of data given current step, so this is the reverse model", "tokens": [51114, 300, 23815, 264, 1570, 24518, 3037, 295, 1412, 2212, 2190, 1823, 11, 370, 341, 307, 264, 9943, 2316, 51414], "temperature": 0.0, "avg_logprob": -0.17076424035159024, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.0026696568820625544}, {"id": 227, "seek": 117580, "start": 1196.8, "end": 1202.8, "text": " where we now use the true denoising distribution to sample from xt minus 1 given xt.", "tokens": [51414, 689, 321, 586, 764, 264, 2074, 1441, 78, 3436, 7316, 281, 6889, 490, 2031, 83, 3175, 502, 2212, 2031, 83, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17076424035159024, "compression_ratio": 1.9489795918367347, "no_speech_prob": 0.0026696568820625544}, {"id": 228, "seek": 120280, "start": 1203.8, "end": 1210.8, "text": " So we just need to start from xt and just use this true denoising model to generate samples at time step 0.", "tokens": [50414, 407, 321, 445, 643, 281, 722, 490, 2031, 83, 293, 445, 764, 341, 2074, 1441, 78, 3436, 2316, 281, 8460, 10938, 412, 565, 1823, 1958, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14661598205566406, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.000543502508662641}, {"id": 229, "seek": 120280, "start": 1210.8, "end": 1216.8, "text": " So the algorithm will be something like this, we sample x capital t from standard normal distribution", "tokens": [50764, 407, 264, 9284, 486, 312, 746, 411, 341, 11, 321, 6889, 2031, 4238, 256, 490, 3832, 2710, 7316, 51064], "temperature": 0.0, "avg_logprob": -0.14661598205566406, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.000543502508662641}, {"id": 230, "seek": 120280, "start": 1216.8, "end": 1223.8, "text": " and we sample iteratively from xc minus 1 using this true denoising distribution.", "tokens": [51064, 293, 321, 6889, 17138, 19020, 490, 2031, 66, 3175, 502, 1228, 341, 2074, 1441, 78, 3436, 7316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14661598205566406, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.000543502508662641}, {"id": 231, "seek": 122380, "start": 1224.8, "end": 1232.8, "text": " The problem here is that in general denoising distribution xt minus 1 given xt is intractable,", "tokens": [50414, 440, 1154, 510, 307, 300, 294, 2674, 1441, 78, 3436, 7316, 2031, 83, 3175, 502, 2212, 2031, 83, 307, 560, 1897, 712, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1714362825666155, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0006768847233615816}, {"id": 232, "seek": 122380, "start": 1232.8, "end": 1238.8, "text": " we don't have access to this distribution, we can use Bayes' rule to show that this distribution", "tokens": [50814, 321, 500, 380, 362, 2105, 281, 341, 7316, 11, 321, 393, 764, 7840, 279, 6, 4978, 281, 855, 300, 341, 7316, 51114], "temperature": 0.0, "avg_logprob": -0.1714362825666155, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0006768847233615816}, {"id": 233, "seek": 122380, "start": 1238.8, "end": 1244.8, "text": " is proportional to the product of the marginal data distribution at xc minus 1 times this diffusion", "tokens": [51114, 307, 24969, 281, 264, 1674, 295, 264, 16885, 1412, 7316, 412, 2031, 66, 3175, 502, 1413, 341, 25242, 51414], "temperature": 0.0, "avg_logprob": -0.1714362825666155, "compression_ratio": 1.6724137931034482, "no_speech_prob": 0.0006768847233615816}, {"id": 234, "seek": 124480, "start": 1244.8, "end": 1251.8, "text": " at t, this kernel is simple, it's just Gaussian distribution, however this distribution marginal", "tokens": [50364, 412, 256, 11, 341, 28256, 307, 2199, 11, 309, 311, 445, 39148, 7316, 11, 4461, 341, 7316, 16885, 50714], "temperature": 0.0, "avg_logprob": -0.16799261281778524, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.005546141881495714}, {"id": 235, "seek": 124480, "start": 1251.8, "end": 1257.8, "text": " data distribution is intractable and in general because of this, this product is also intractable", "tokens": [50714, 1412, 7316, 307, 560, 1897, 712, 293, 294, 2674, 570, 295, 341, 11, 341, 1674, 307, 611, 560, 1897, 712, 51014], "temperature": 0.0, "avg_logprob": -0.16799261281778524, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.005546141881495714}, {"id": 236, "seek": 124480, "start": 1257.8, "end": 1264.8, "text": " so we don't have it in closed form. Now that we don't have it in closed form, you may say", "tokens": [51014, 370, 321, 500, 380, 362, 309, 294, 5395, 1254, 13, 823, 300, 321, 500, 380, 362, 309, 294, 5395, 1254, 11, 291, 815, 584, 51364], "temperature": 0.0, "avg_logprob": -0.16799261281778524, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.005546141881495714}, {"id": 237, "seek": 124480, "start": 1264.8, "end": 1271.8, "text": " can I approximate this denoising distribution from data and if yes, what is the best form", "tokens": [51364, 393, 286, 30874, 341, 1441, 78, 3436, 7316, 490, 1412, 293, 498, 2086, 11, 437, 307, 264, 1151, 1254, 51714], "temperature": 0.0, "avg_logprob": -0.16799261281778524, "compression_ratio": 1.7980769230769231, "no_speech_prob": 0.005546141881495714}, {"id": 238, "seek": 127180, "start": 1271.8, "end": 1278.8, "text": " I can use to represent that denoising model. So the good news is, yes, we can try to train", "tokens": [50364, 286, 393, 764, 281, 2906, 300, 1441, 78, 3436, 2316, 13, 407, 264, 665, 2583, 307, 11, 2086, 11, 321, 393, 853, 281, 3847, 50714], "temperature": 0.0, "avg_logprob": -0.1270392962864467, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.001205037347972393}, {"id": 239, "seek": 127180, "start": 1278.8, "end": 1286.8, "text": " a model to mimic this true denoising distribution xt minus 1 given xt and if you want to model", "tokens": [50714, 257, 2316, 281, 31075, 341, 2074, 1441, 78, 3436, 7316, 2031, 83, 3175, 502, 2212, 2031, 83, 293, 498, 291, 528, 281, 2316, 51114], "temperature": 0.0, "avg_logprob": -0.1270392962864467, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.001205037347972393}, {"id": 240, "seek": 127180, "start": 1286.8, "end": 1291.8, "text": " that, the best model that you can use, the statistical model you can use to represent this denoising model", "tokens": [51114, 300, 11, 264, 1151, 2316, 300, 291, 393, 764, 11, 264, 22820, 2316, 291, 393, 764, 281, 2906, 341, 1441, 78, 3436, 2316, 51364], "temperature": 0.0, "avg_logprob": -0.1270392962864467, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.001205037347972393}, {"id": 241, "seek": 127180, "start": 1291.8, "end": 1300.8, "text": " is actually a normal distribution if in the forward process we use very small noise or the variance", "tokens": [51364, 307, 767, 257, 2710, 7316, 498, 294, 264, 2128, 1399, 321, 764, 588, 1359, 5658, 420, 264, 21977, 51814], "temperature": 0.0, "avg_logprob": -0.1270392962864467, "compression_ratio": 1.84037558685446, "no_speech_prob": 0.001205037347972393}, {"id": 242, "seek": 130080, "start": 1300.8, "end": 1305.8, "text": " of the noise that we're adding at each step is very small. If we know the forward process", "tokens": [50364, 295, 264, 5658, 300, 321, 434, 5127, 412, 1184, 1823, 307, 588, 1359, 13, 759, 321, 458, 264, 2128, 1399, 50614], "temperature": 0.0, "avg_logprob": -0.10143910240881222, "compression_ratio": 1.8727272727272728, "no_speech_prob": 0.0004645715525839478}, {"id": 243, "seek": 130080, "start": 1305.8, "end": 1313.8, "text": " at very small amount of noise, we know theoretically that actually the reverse process can be also approximated", "tokens": [50614, 412, 588, 1359, 2372, 295, 5658, 11, 321, 458, 29400, 300, 767, 264, 9943, 1399, 393, 312, 611, 8542, 770, 51014], "temperature": 0.0, "avg_logprob": -0.10143910240881222, "compression_ratio": 1.8727272727272728, "no_speech_prob": 0.0004645715525839478}, {"id": 244, "seek": 130080, "start": 1313.8, "end": 1319.8, "text": " using a normal distribution, so this denoising model can be represented using denoising model.", "tokens": [51014, 1228, 257, 2710, 7316, 11, 370, 341, 1441, 78, 3436, 2316, 393, 312, 10379, 1228, 1441, 78, 3436, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10143910240881222, "compression_ratio": 1.8727272727272728, "no_speech_prob": 0.0004645715525839478}, {"id": 245, "seek": 130080, "start": 1319.8, "end": 1329.8, "text": " So now that we know this, we can actually define a parametric model to mimic or to train this true denoising model.", "tokens": [51314, 407, 586, 300, 321, 458, 341, 11, 321, 393, 767, 6964, 257, 6220, 17475, 2316, 281, 31075, 420, 281, 3847, 341, 2074, 1441, 78, 3436, 2316, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10143910240881222, "compression_ratio": 1.8727272727272728, "no_speech_prob": 0.0004645715525839478}, {"id": 246, "seek": 132980, "start": 1330.8, "end": 1337.8, "text": " So this formally defined our parametric reverse denoising process. Remember that reverse denoising process starts", "tokens": [50414, 407, 341, 25983, 7642, 527, 6220, 17475, 9943, 1441, 78, 3436, 1399, 13, 5459, 300, 9943, 1441, 78, 3436, 1399, 3719, 50764], "temperature": 0.0, "avg_logprob": -0.20649792750676474, "compression_ratio": 1.9017857142857142, "no_speech_prob": 0.0009384062723256648}, {"id": 247, "seek": 132980, "start": 1337.8, "end": 1344.8, "text": " from noise and generous data by denoising once at a time. We're going to assume that the distribution of data", "tokens": [50764, 490, 5658, 293, 14537, 1412, 538, 1441, 78, 3436, 1564, 412, 257, 565, 13, 492, 434, 516, 281, 6552, 300, 264, 7316, 295, 1412, 51114], "temperature": 0.0, "avg_logprob": -0.20649792750676474, "compression_ratio": 1.9017857142857142, "no_speech_prob": 0.0009384062723256648}, {"id": 248, "seek": 132980, "start": 1344.8, "end": 1350.8, "text": " at time is the capital T at the end of the forward process, so it's the beginning of the reverse process.", "tokens": [51114, 412, 565, 307, 264, 4238, 314, 412, 264, 917, 295, 264, 2128, 1399, 11, 370, 309, 311, 264, 2863, 295, 264, 9943, 1399, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20649792750676474, "compression_ratio": 1.9017857142857142, "no_speech_prob": 0.0009384062723256648}, {"id": 249, "seek": 132980, "start": 1350.8, "end": 1356.8, "text": " It will be standard normal distribution here as soon as data has a standard normal distribution.", "tokens": [51414, 467, 486, 312, 3832, 2710, 7316, 510, 382, 2321, 382, 1412, 575, 257, 3832, 2710, 7316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20649792750676474, "compression_ratio": 1.9017857142857142, "no_speech_prob": 0.0009384062723256648}, {"id": 250, "seek": 135680, "start": 1356.8, "end": 1363.8, "text": " We define this denoising distribution. This is a parametric denoising distribution as samples from XT minus from given XT.", "tokens": [50364, 492, 6964, 341, 1441, 78, 3436, 7316, 13, 639, 307, 257, 6220, 17475, 1441, 78, 3436, 7316, 382, 10938, 490, 1783, 51, 3175, 490, 2212, 1783, 51, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17431748334099265, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.0015181064372882247}, {"id": 251, "seek": 135680, "start": 1363.8, "end": 1371.8, "text": " We're going to assume that it also can be represented using normal distribution where now mean here is parametric.", "tokens": [50714, 492, 434, 516, 281, 6552, 300, 309, 611, 393, 312, 10379, 1228, 2710, 7316, 689, 586, 914, 510, 307, 6220, 17475, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17431748334099265, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.0015181064372882247}, {"id": 252, "seek": 135680, "start": 1371.8, "end": 1382.8, "text": " It's a deep neural network that takes noisy image XT and predicts the mean of the less noisy version, less noisy image.", "tokens": [51114, 467, 311, 257, 2452, 18161, 3209, 300, 2516, 24518, 3256, 1783, 51, 293, 6069, 82, 264, 914, 295, 264, 1570, 24518, 3037, 11, 1570, 24518, 3256, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17431748334099265, "compression_ratio": 1.7414634146341463, "no_speech_prob": 0.0015181064372882247}, {"id": 253, "seek": 138280, "start": 1383.8, "end": 1388.8, "text": " So this neural network is the trainable component and then we have also the variance per time step.", "tokens": [50414, 407, 341, 18161, 3209, 307, 264, 3847, 712, 6542, 293, 550, 321, 362, 611, 264, 21977, 680, 565, 1823, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20252041334516546, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0008956230012699962}, {"id": 254, "seek": 138280, "start": 1388.8, "end": 1395.8, "text": " This is just think of the sigma T squared as just some scalar value that represents the variance per time step.", "tokens": [50664, 639, 307, 445, 519, 295, 264, 12771, 314, 8889, 382, 445, 512, 39684, 2158, 300, 8855, 264, 21977, 680, 565, 1823, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20252041334516546, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0008956230012699962}, {"id": 255, "seek": 138280, "start": 1395.8, "end": 1401.8, "text": " And for now just assume that it's just some parameter or we have access to it. We're going to talk about it later.", "tokens": [51014, 400, 337, 586, 445, 6552, 300, 309, 311, 445, 512, 13075, 420, 321, 362, 2105, 281, 309, 13, 492, 434, 516, 281, 751, 466, 309, 1780, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20252041334516546, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0008956230012699962}, {"id": 256, "seek": 138280, "start": 1401.8, "end": 1405.8, "text": " But the most important part is this mean parameter.", "tokens": [51314, 583, 264, 881, 1021, 644, 307, 341, 914, 13075, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20252041334516546, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0008956230012699962}, {"id": 257, "seek": 140580, "start": 1405.8, "end": 1414.8, "text": " Remember, this is the trainable network and it takes a noisy image at XT and predicts the mean of less noisy image at XT minus 1.", "tokens": [50364, 5459, 11, 341, 307, 264, 3847, 712, 3209, 293, 309, 2516, 257, 24518, 3256, 412, 1783, 51, 293, 6069, 82, 264, 914, 295, 1570, 24518, 3256, 412, 1783, 51, 3175, 502, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10939028859138489, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.000551000761333853}, {"id": 258, "seek": 140580, "start": 1414.8, "end": 1422.8, "text": " Because it takes an image and predicts an image, we can actually model this using a unit that has the same input and output shape.", "tokens": [50814, 1436, 309, 2516, 364, 3256, 293, 6069, 82, 364, 3256, 11, 321, 393, 767, 2316, 341, 1228, 257, 4985, 300, 575, 264, 912, 4846, 293, 5598, 3909, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10939028859138489, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.000551000761333853}, {"id": 259, "seek": 140580, "start": 1422.8, "end": 1430.8, "text": " Or you can think of it as just a denoising autoencoder that denoises the input image to less noisy level basically.", "tokens": [51214, 1610, 291, 393, 519, 295, 309, 382, 445, 257, 1441, 78, 3436, 8399, 22660, 19866, 300, 1441, 78, 3598, 264, 4846, 3256, 281, 1570, 24518, 1496, 1936, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10939028859138489, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.000551000761333853}, {"id": 260, "seek": 143080, "start": 1430.8, "end": 1438.8, "text": " And we're going to talk about the architecture of this denoising model, this new filter.", "tokens": [50364, 400, 321, 434, 516, 281, 751, 466, 264, 9482, 295, 341, 1441, 78, 3436, 2316, 11, 341, 777, 6608, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1584787822905041, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.0001633994106668979}, {"id": 261, "seek": 143080, "start": 1438.8, "end": 1444.8, "text": " Now that we have these definitions, we can define the joint distribution on the full trajectory.", "tokens": [50764, 823, 300, 321, 362, 613, 21988, 11, 321, 393, 6964, 264, 7225, 7316, 322, 264, 1577, 21512, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1584787822905041, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.0001633994106668979}, {"id": 262, "seek": 143080, "start": 1444.8, "end": 1456.8, "text": " This joint distribution is the product of the base distribution at step XT and the product of conditionals at all these steps at T steps where the condition comes from our denoising model.", "tokens": [51064, 639, 7225, 7316, 307, 264, 1674, 295, 264, 3096, 7316, 412, 1823, 1783, 51, 293, 264, 1674, 295, 4188, 1124, 412, 439, 613, 4439, 412, 314, 4439, 689, 264, 4188, 1487, 490, 527, 1441, 78, 3436, 2316, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1584787822905041, "compression_ratio": 1.7725118483412323, "no_speech_prob": 0.0001633994106668979}, {"id": 263, "seek": 145680, "start": 1456.8, "end": 1471.8, "text": " This is again just a Markov process and we can define the joint distribution on the full trajectory by the product of base distribution and the product of these individual conditionals on each denoising step.", "tokens": [50364, 639, 307, 797, 445, 257, 3934, 5179, 1399, 293, 321, 393, 6964, 264, 7225, 7316, 322, 264, 1577, 21512, 538, 264, 1674, 295, 3096, 7316, 293, 264, 1674, 295, 613, 2609, 4188, 1124, 322, 1184, 1441, 78, 3436, 1823, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14633149539723117, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.0010138772195205092}, {"id": 264, "seek": 145680, "start": 1471.8, "end": 1483.8, "text": " Okay, so now so far we talked about the forward process, the reverse process. Now let's talk about how we can train these models, how we can train the denoising model.", "tokens": [51114, 1033, 11, 370, 586, 370, 1400, 321, 2825, 466, 264, 2128, 1399, 11, 264, 9943, 1399, 13, 823, 718, 311, 751, 466, 577, 321, 393, 3847, 613, 5245, 11, 577, 321, 393, 3847, 264, 1441, 78, 3436, 2316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14633149539723117, "compression_ratio": 1.7652582159624413, "no_speech_prob": 0.0010138772195205092}, {"id": 265, "seek": 148380, "start": 1483.8, "end": 1492.8, "text": " For training the denoising model, we're going to use variation upper bond that is mostly or commonly used for training by autoencoders.", "tokens": [50364, 1171, 3097, 264, 1441, 78, 3436, 2316, 11, 321, 434, 516, 281, 764, 12990, 6597, 6086, 300, 307, 5240, 420, 12719, 1143, 337, 3097, 538, 8399, 22660, 378, 433, 13, 50814], "temperature": 0.0, "avg_logprob": -0.25512529241627663, "compression_ratio": 1.5460122699386503, "no_speech_prob": 0.003067798912525177}, {"id": 266, "seek": 148380, "start": 1492.8, "end": 1502.8, "text": " In the variation upper bond, ideally we want to optimize marginal likelihood of the data under our parametric model.", "tokens": [50814, 682, 264, 12990, 6597, 6086, 11, 22915, 321, 528, 281, 19719, 16885, 22119, 295, 264, 1412, 833, 527, 6220, 17475, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.25512529241627663, "compression_ratio": 1.5460122699386503, "no_speech_prob": 0.003067798912525177}, {"id": 267, "seek": 150280, "start": 1502.8, "end": 1513.8, "text": " Here we have this expectation over training data distribution, where we are computing the expected value of the low to likelihood that our trainable model gives to data distribution.", "tokens": [50364, 1692, 321, 362, 341, 14334, 670, 3097, 1412, 7316, 11, 689, 321, 366, 15866, 264, 5176, 2158, 295, 264, 2295, 281, 22119, 300, 527, 3847, 712, 2316, 2709, 281, 1412, 7316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19653407636895237, "compression_ratio": 1.8672566371681416, "no_speech_prob": 0.008957071229815483}, {"id": 268, "seek": 150280, "start": 1513.8, "end": 1527.8, "text": " Unfortunately, this is interactive, we don't have access to this quantity here, but we can define variation upper bond, where now we have this expectation over training data, we have expectation over samples drawn from the forward process.", "tokens": [50914, 8590, 11, 341, 307, 15141, 11, 321, 500, 380, 362, 2105, 281, 341, 11275, 510, 11, 457, 321, 393, 6964, 12990, 6597, 6086, 11, 689, 586, 321, 362, 341, 14334, 670, 3097, 1412, 11, 321, 362, 14334, 670, 10938, 10117, 490, 264, 2128, 1399, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19653407636895237, "compression_ratio": 1.8672566371681416, "no_speech_prob": 0.008957071229815483}, {"id": 269, "seek": 152780, "start": 1527.8, "end": 1538.8, "text": " This is forward process, you can think of it as encoder in VAE, that samples from latent space, so you can think of these as latent variable x1 to capital T.", "tokens": [50364, 639, 307, 2128, 1399, 11, 291, 393, 519, 295, 309, 382, 2058, 19866, 294, 18527, 36, 11, 300, 10938, 490, 48994, 1901, 11, 370, 291, 393, 519, 295, 613, 382, 48994, 7006, 2031, 16, 281, 4238, 314, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16647125043367084, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.023617401719093323}, {"id": 270, "seek": 152780, "start": 1538.8, "end": 1554.8, "text": " And we have this log ratio here where the nominator is the joint distribution of the samples on the full trajectory that is given by our denoising model, P theta, and in the denominator we have the likelihood of the samples generated by encoder.", "tokens": [50914, 400, 321, 362, 341, 3565, 8509, 510, 689, 264, 5369, 31927, 307, 264, 7225, 7316, 295, 264, 10938, 322, 264, 1577, 21512, 300, 307, 2212, 538, 527, 1441, 78, 3436, 2316, 11, 430, 9725, 11, 293, 294, 264, 20687, 321, 362, 264, 22119, 295, 264, 10938, 10833, 538, 2058, 19866, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16647125043367084, "compression_ratio": 1.6861924686192469, "no_speech_prob": 0.023617401719093323}, {"id": 271, "seek": 155480, "start": 1554.8, "end": 1563.8, "text": " This is basically the same objective or same training object, we would see variation of bond in variation autoencoders.", "tokens": [50364, 639, 307, 1936, 264, 912, 10024, 420, 912, 3097, 2657, 11, 321, 576, 536, 12990, 295, 6086, 294, 12990, 8399, 22660, 378, 433, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2997805962195763, "compression_ratio": 1.6107784431137724, "no_speech_prob": 0.013406675308942795}, {"id": 272, "seek": 155480, "start": 1563.8, "end": 1564.8, "text": " This is exactly the same.", "tokens": [50814, 639, 307, 2293, 264, 912, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2997805962195763, "compression_ratio": 1.6107784431137724, "no_speech_prob": 0.013406675308942795}, {"id": 273, "seek": 155480, "start": 1564.8, "end": 1574.8, "text": " The assumption is that the forward process is kind of like an encoder and the reverse process is the genitive model in VAE.", "tokens": [50864, 440, 15302, 307, 300, 264, 2128, 1399, 307, 733, 295, 411, 364, 2058, 19866, 293, 264, 9943, 1399, 307, 264, 1049, 2187, 2316, 294, 18527, 36, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2997805962195763, "compression_ratio": 1.6107784431137724, "no_speech_prob": 0.013406675308942795}, {"id": 274, "seek": 157480, "start": 1574.8, "end": 1587.8, "text": " These two papers, by the way, our papers, the links on our slides are clickable, so if you want to check these papers just find our slides and click on these references and you're going to find the paper.", "tokens": [50364, 1981, 732, 10577, 11, 538, 264, 636, 11, 527, 10577, 11, 264, 6123, 322, 527, 9788, 366, 2052, 712, 11, 370, 498, 291, 528, 281, 1520, 613, 10577, 445, 915, 527, 9788, 293, 2052, 322, 613, 15400, 293, 291, 434, 516, 281, 915, 264, 3035, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12453331091465095, "compression_ratio": 1.706806282722513, "no_speech_prob": 0.01824146881699562}, {"id": 275, "seek": 157480, "start": 1587.8, "end": 1595.8, "text": " So these two papers showed that this variation bond can be decomposed to several times that we're going to go one by one.", "tokens": [51014, 407, 613, 732, 10577, 4712, 300, 341, 12990, 6086, 393, 312, 22867, 1744, 281, 2940, 1413, 300, 321, 434, 516, 281, 352, 472, 538, 472, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12453331091465095, "compression_ratio": 1.706806282722513, "no_speech_prob": 0.01824146881699562}, {"id": 276, "seek": 159580, "start": 1595.8, "end": 1608.8, "text": " The first term is just simply the KL divergence from the diffusion kernel in the last step, x capital T given x0 to this base distribution xT.", "tokens": [50364, 440, 700, 1433, 307, 445, 2935, 264, 47991, 47387, 490, 264, 25242, 28256, 294, 264, 1036, 1823, 11, 2031, 4238, 314, 2212, 2031, 15, 281, 341, 3096, 7316, 2031, 51, 13, 51014], "temperature": 0.0, "avg_logprob": -0.179571166405311, "compression_ratio": 1.6628571428571428, "no_speech_prob": 0.00711454451084137}, {"id": 277, "seek": 159580, "start": 1608.8, "end": 1617.8, "text": " Remember by definition, the diffusion kernel for x capital T converges to standard normal distribution, which is same distribution we assume for xT.", "tokens": [51014, 5459, 538, 7123, 11, 264, 25242, 28256, 337, 2031, 4238, 314, 9652, 2880, 281, 3832, 2710, 7316, 11, 597, 307, 912, 7316, 321, 6552, 337, 2031, 51, 13, 51464], "temperature": 0.0, "avg_logprob": -0.179571166405311, "compression_ratio": 1.6628571428571428, "no_speech_prob": 0.00711454451084137}, {"id": 278, "seek": 161780, "start": 1617.8, "end": 1627.8, "text": " So we can completely ignore this term because by definition, forward process defines such that at the end of process, samples converge to 0 million units of mass distribution.", "tokens": [50364, 407, 321, 393, 2584, 11200, 341, 1433, 570, 538, 7123, 11, 2128, 1399, 23122, 1270, 300, 412, 264, 917, 295, 1399, 11, 10938, 41881, 281, 1958, 2459, 6815, 295, 2758, 7316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1943181556991384, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.001244213548488915}, {"id": 279, "seek": 161780, "start": 1627.8, "end": 1629.8, "text": " So we can ignore this term.", "tokens": [50864, 407, 321, 393, 11200, 341, 1433, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1943181556991384, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.001244213548488915}, {"id": 280, "seek": 161780, "start": 1629.8, "end": 1638.8, "text": " We have this KL term that I'm going to go into details, and then we have this term that can be considered as the reconstruction term in VAEs.", "tokens": [50964, 492, 362, 341, 47991, 1433, 300, 286, 478, 516, 281, 352, 666, 4365, 11, 293, 550, 321, 362, 341, 1433, 300, 393, 312, 4888, 382, 264, 31565, 1433, 294, 18527, 20442, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1943181556991384, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.001244213548488915}, {"id": 281, "seek": 163880, "start": 1638.8, "end": 1647.8, "text": " This just measures the likelihood of input clean image, even the noise image at first step under our denoising model.", "tokens": [50364, 639, 445, 8000, 264, 22119, 295, 4846, 2541, 3256, 11, 754, 264, 5658, 3256, 412, 700, 1823, 833, 527, 1441, 78, 3436, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1661649743715922, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.017680209130048752}, {"id": 282, "seek": 163880, "start": 1647.8, "end": 1652.8, "text": " This term is also very simple, and it has actually structure very similar to this other term.", "tokens": [50814, 639, 1433, 307, 611, 588, 2199, 11, 293, 309, 575, 767, 3877, 588, 2531, 281, 341, 661, 1433, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1661649743715922, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.017680209130048752}, {"id": 283, "seek": 163880, "start": 1652.8, "end": 1658.8, "text": " So for moment, just assume that we can compute this very easily, and we just ignore for a moment.", "tokens": [51064, 407, 337, 1623, 11, 445, 6552, 300, 321, 393, 14722, 341, 588, 3612, 11, 293, 321, 445, 11200, 337, 257, 1623, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1661649743715922, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.017680209130048752}, {"id": 284, "seek": 163880, "start": 1658.8, "end": 1663.8, "text": " And mostly we just need to focus on this term, which is the most kind of important term here.", "tokens": [51364, 400, 5240, 321, 445, 643, 281, 1879, 322, 341, 1433, 11, 597, 307, 264, 881, 733, 295, 1021, 1433, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1661649743715922, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.017680209130048752}, {"id": 285, "seek": 166380, "start": 1663.8, "end": 1673.8, "text": " This is the KL divergence between this Q, xT minus 1 given xT and x0 to our denoising model.", "tokens": [50364, 639, 307, 264, 47991, 47387, 1296, 341, 1249, 11, 2031, 51, 3175, 502, 2212, 2031, 51, 293, 2031, 15, 281, 527, 1441, 78, 3436, 2316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1684851573063777, "compression_ratio": 1.5673758865248226, "no_speech_prob": 0.008567535318434238}, {"id": 286, "seek": 166380, "start": 1673.8, "end": 1677.8, "text": " This is our parametric denoising model, which is xT minus 1 given xT.", "tokens": [50864, 639, 307, 527, 6220, 17475, 1441, 78, 3436, 2316, 11, 597, 307, 2031, 51, 3175, 502, 2212, 2031, 51, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1684851573063777, "compression_ratio": 1.5673758865248226, "no_speech_prob": 0.008567535318434238}, {"id": 287, "seek": 166380, "start": 1677.8, "end": 1682.8, "text": " This Q distribution is a pretty well not new distribution.", "tokens": [51064, 639, 1249, 7316, 307, 257, 1238, 731, 406, 777, 7316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1684851573063777, "compression_ratio": 1.5673758865248226, "no_speech_prob": 0.008567535318434238}, {"id": 288, "seek": 168280, "start": 1682.8, "end": 1685.8, "text": " We're going to call it the tractable posterior distribution.", "tokens": [50364, 492, 434, 516, 281, 818, 309, 264, 24207, 712, 33529, 7316, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11531453180794764, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.0019563818350434303}, {"id": 289, "seek": 168280, "start": 1685.8, "end": 1696.8, "text": " These samples from xT minus 1, less noisy image, condition on noisy image, xT, and clean image at time step 0.", "tokens": [50514, 1981, 10938, 490, 2031, 51, 3175, 502, 11, 1570, 24518, 3256, 11, 4188, 322, 24518, 3256, 11, 2031, 51, 11, 293, 2541, 3256, 412, 565, 1823, 1958, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11531453180794764, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.0019563818350434303}, {"id": 290, "seek": 168280, "start": 1696.8, "end": 1709.8, "text": " So it's kind of like you have clean image, you have noisy image, and you want to predict what would be the less noisy variation of this image if you knew what was the starting point, what was the x0 for generating this noisy image.", "tokens": [51064, 407, 309, 311, 733, 295, 411, 291, 362, 2541, 3256, 11, 291, 362, 24518, 3256, 11, 293, 291, 528, 281, 6069, 437, 576, 312, 264, 1570, 24518, 12990, 295, 341, 3256, 498, 291, 2586, 437, 390, 264, 2891, 935, 11, 437, 390, 264, 2031, 15, 337, 17746, 341, 24518, 3256, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11531453180794764, "compression_ratio": 1.7831858407079646, "no_speech_prob": 0.0019563818350434303}, {"id": 291, "seek": 170980, "start": 1709.8, "end": 1717.8, "text": " It turns out that this distribution is also very simple because the forward process is just Gaussian distribution.", "tokens": [50364, 467, 4523, 484, 300, 341, 7316, 307, 611, 588, 2199, 570, 264, 2128, 1399, 307, 445, 39148, 7316, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2263344542620933, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.0017533553764224052}, {"id": 292, "seek": 170980, "start": 1717.8, "end": 1722.8, "text": " The posterior distribution here is also very simple.", "tokens": [50764, 440, 33529, 7316, 510, 307, 611, 588, 2199, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2263344542620933, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.0017533553764224052}, {"id": 293, "seek": 170980, "start": 1722.8, "end": 1727.8, "text": " This is a pretty good posterior distribution, it's condition on x0. We know what is the starting point.", "tokens": [51014, 639, 307, 257, 1238, 665, 33529, 7316, 11, 309, 311, 4188, 322, 2031, 15, 13, 492, 458, 437, 307, 264, 2891, 935, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2263344542620933, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.0017533553764224052}, {"id": 294, "seek": 170980, "start": 1727.8, "end": 1734.8, "text": " So this distribution is also normal distribution with mean expressed here.", "tokens": [51264, 407, 341, 7316, 307, 611, 2710, 7316, 365, 914, 12675, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2263344542620933, "compression_ratio": 1.8404255319148937, "no_speech_prob": 0.0017533553764224052}, {"id": 295, "seek": 173480, "start": 1734.8, "end": 1740.8, "text": " This mean is just simply weighted average of the clean image and the noisy image at xT.", "tokens": [50364, 639, 914, 307, 445, 2935, 32807, 4274, 295, 264, 2541, 3256, 293, 264, 24518, 3256, 412, 2031, 51, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16687587556384859, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.00831008143723011}, {"id": 296, "seek": 173480, "start": 1740.8, "end": 1744.8, "text": " So it's actually very interesting if you have clean image and you have a noisy image.", "tokens": [50664, 407, 309, 311, 767, 588, 1880, 498, 291, 362, 2541, 3256, 293, 291, 362, 257, 24518, 3256, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16687587556384859, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.00831008143723011}, {"id": 297, "seek": 173480, "start": 1744.8, "end": 1754.8, "text": " If you want to predict what is the distribution of xT minus 1, the mean that this will be normal, this expression is just normal.", "tokens": [50864, 759, 291, 528, 281, 6069, 437, 307, 264, 7316, 295, 2031, 51, 3175, 502, 11, 264, 914, 300, 341, 486, 312, 2710, 11, 341, 6114, 307, 445, 2710, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16687587556384859, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.00831008143723011}, {"id": 298, "seek": 173480, "start": 1754.8, "end": 1763.8, "text": " And the mean of that normal is just the weighted average of these two images where the weights come from our diffusion process, parametric diffusion process.", "tokens": [51364, 400, 264, 914, 295, 300, 2710, 307, 445, 264, 32807, 4274, 295, 613, 732, 5267, 689, 264, 17443, 808, 490, 527, 25242, 1399, 11, 6220, 17475, 25242, 1399, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16687587556384859, "compression_ratio": 1.9049586776859504, "no_speech_prob": 0.00831008143723011}, {"id": 299, "seek": 176380, "start": 1763.8, "end": 1772.8, "text": " Very simple expression and the variance of this distribution is also defined based on the parameters of the forward process.", "tokens": [50364, 4372, 2199, 6114, 293, 264, 21977, 295, 341, 7316, 307, 611, 7642, 2361, 322, 264, 9834, 295, 264, 2128, 1399, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15368474613536487, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0016972527373582125}, {"id": 300, "seek": 176380, "start": 1772.8, "end": 1781.8, "text": " So you can think of it like this beta tilde t can be computed very easily from the parameters of the diffusion process.", "tokens": [50814, 407, 291, 393, 519, 295, 309, 411, 341, 9861, 45046, 256, 393, 312, 40610, 588, 3612, 490, 264, 9834, 295, 264, 25242, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15368474613536487, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0016972527373582125}, {"id": 301, "seek": 176380, "start": 1781.8, "end": 1784.8, "text": " So that we know none of this here, it's interesting.", "tokens": [51264, 407, 300, 321, 458, 6022, 295, 341, 510, 11, 309, 311, 1880, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15368474613536487, "compression_ratio": 1.6141304347826086, "no_speech_prob": 0.0016972527373582125}, {"id": 302, "seek": 178480, "start": 1784.8, "end": 1790.8, "text": " It's interesting, so we have the scale divergence from this distribution trackable posterior distribution, which is normal.", "tokens": [50364, 467, 311, 1880, 11, 370, 321, 362, 264, 4373, 47387, 490, 341, 7316, 2837, 712, 33529, 7316, 11, 597, 307, 2710, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2319910866873605, "compression_ratio": 1.938095238095238, "no_speech_prob": 0.002321144798770547}, {"id": 303, "seek": 178480, "start": 1790.8, "end": 1794.8, "text": " And then this denoising model, which we assume that is normal as well.", "tokens": [50664, 400, 550, 341, 1441, 78, 3436, 2316, 11, 597, 321, 6552, 300, 307, 2710, 382, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2319910866873605, "compression_ratio": 1.938095238095238, "no_speech_prob": 0.002321144798770547}, {"id": 304, "seek": 178480, "start": 1794.8, "end": 1799.8, "text": " So, now that we have two normal distributions, the scale divergence there.", "tokens": [50864, 407, 11, 586, 300, 321, 362, 732, 2710, 37870, 11, 264, 4373, 47387, 456, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2319910866873605, "compression_ratio": 1.938095238095238, "no_speech_prob": 0.002321144798770547}, {"id": 305, "seek": 178480, "start": 1799.8, "end": 1803.8, "text": " So I'm just writing down the same scale divergence again.", "tokens": [51114, 407, 286, 478, 445, 3579, 760, 264, 912, 4373, 47387, 797, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2319910866873605, "compression_ratio": 1.938095238095238, "no_speech_prob": 0.002321144798770547}, {"id": 306, "seek": 178480, "start": 1803.8, "end": 1809.8, "text": " The scale divergence can be computed analytically for two normal distributions.", "tokens": [51314, 440, 4373, 47387, 393, 312, 40610, 10783, 984, 337, 732, 2710, 37870, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2319910866873605, "compression_ratio": 1.938095238095238, "no_speech_prob": 0.002321144798770547}, {"id": 307, "seek": 180980, "start": 1809.8, "end": 1823.8, "text": " We can show that the scale divergence simply just boils down to the squared L2 distance between the mean of this trackable posterior and the mean of the denoising model.", "tokens": [50364, 492, 393, 855, 300, 264, 4373, 47387, 2935, 445, 35049, 760, 281, 264, 8889, 441, 17, 4560, 1296, 264, 914, 295, 341, 2837, 712, 33529, 293, 264, 914, 295, 264, 1441, 78, 3436, 2316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.227052612910195, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.002707583596929908}, {"id": 308, "seek": 180980, "start": 1823.8, "end": 1830.8, "text": " Right, plus some constant terms that we can ignore these constant terms do not depend on any trainable power.", "tokens": [51064, 1779, 11, 1804, 512, 5754, 2115, 300, 321, 393, 11200, 613, 5754, 2115, 360, 406, 5672, 322, 604, 3847, 712, 1347, 13, 51414], "temperature": 0.0, "avg_logprob": -0.227052612910195, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.002707583596929908}, {"id": 309, "seek": 183080, "start": 1830.8, "end": 1834.8, "text": " So this just basically scale divergence is very interesting.", "tokens": [50364, 407, 341, 445, 1936, 4373, 47387, 307, 588, 1880, 13, 50564], "temperature": 0.0, "avg_logprob": -0.22059346074643343, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.0023945842403918505}, {"id": 310, "seek": 183080, "start": 1834.8, "end": 1845.8, "text": " It just boils down to the difference between the mean of this denoising, sorry, this mean of the trackable posterior and our denoising model, parametric denoising model, which is represented by mean theta.", "tokens": [50564, 467, 445, 35049, 760, 281, 264, 2649, 1296, 264, 914, 295, 341, 1441, 78, 3436, 11, 2597, 11, 341, 914, 295, 264, 2837, 712, 33529, 293, 527, 1441, 78, 3436, 2316, 11, 6220, 17475, 1441, 78, 3436, 2316, 11, 597, 307, 10379, 538, 914, 9725, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22059346074643343, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.0023945842403918505}, {"id": 311, "seek": 183080, "start": 1845.8, "end": 1848.8, "text": " And this weight is one over two sigma t squared.", "tokens": [51114, 400, 341, 3364, 307, 472, 670, 732, 12771, 256, 8889, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22059346074643343, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.0023945842403918505}, {"id": 312, "seek": 183080, "start": 1848.8, "end": 1852.8, "text": " It's just the variance used in the denoising distribution.", "tokens": [51264, 467, 311, 445, 264, 21977, 1143, 294, 264, 1441, 78, 3436, 7316, 13, 51464], "temperature": 0.0, "avg_logprob": -0.22059346074643343, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.0023945842403918505}, {"id": 313, "seek": 183080, "start": 1852.8, "end": 1854.8, "text": " So you can ignore for a moment this coefficient.", "tokens": [51464, 407, 291, 393, 11200, 337, 257, 1623, 341, 17619, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22059346074643343, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.0023945842403918505}, {"id": 314, "seek": 183080, "start": 1854.8, "end": 1858.8, "text": " So we're going to focus on these two terms.", "tokens": [51564, 407, 321, 434, 516, 281, 1879, 322, 613, 732, 2115, 13, 51764], "temperature": 0.0, "avg_logprob": -0.22059346074643343, "compression_ratio": 1.7296296296296296, "no_speech_prob": 0.0023945842403918505}, {"id": 315, "seek": 185880, "start": 1858.8, "end": 1868.8, "text": " Recall that if you want to generate xt, if you want to generate a sample at times the t, you can use this parameterization trick that we discussed earlier.", "tokens": [50364, 9647, 336, 300, 498, 291, 528, 281, 8460, 220, 734, 11, 498, 291, 528, 281, 8460, 257, 6889, 412, 1413, 264, 256, 11, 291, 393, 764, 341, 13075, 2144, 4282, 300, 321, 7152, 3071, 13, 50864], "temperature": 0.0, "avg_logprob": -0.29307943979899087, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0014083411078900099}, {"id": 316, "seek": 185880, "start": 1868.8, "end": 1887.8, "text": " And in this paper hall et al. in New York 2020, they observed that you can also express the mean of the trackable posterior distribution I discussed as xt, the input noise image minus the noise that was used to generate that data.", "tokens": [50864, 400, 294, 341, 3035, 6500, 1030, 419, 13, 294, 1873, 3609, 4808, 11, 436, 13095, 300, 291, 393, 611, 5109, 264, 914, 295, 264, 2837, 712, 33529, 7316, 286, 7152, 382, 220, 734, 11, 264, 4846, 5658, 3256, 3175, 264, 5658, 300, 390, 1143, 281, 8460, 300, 1412, 13, 51814], "temperature": 0.0, "avg_logprob": -0.29307943979899087, "compression_ratio": 1.7079646017699115, "no_speech_prob": 0.0014083411078900099}, {"id": 317, "seek": 188780, "start": 1888.8, "end": 1899.8, "text": " To get this expression, it's very simple, you just need to do some arithmetic operations on this equation and just plug the definition of xt from the parameterization trick.", "tokens": [50414, 1407, 483, 341, 6114, 11, 309, 311, 588, 2199, 11, 291, 445, 643, 281, 360, 512, 42973, 7705, 322, 341, 5367, 293, 445, 5452, 264, 7123, 295, 220, 734, 490, 264, 13075, 2144, 4282, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1434674884961999, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.001983851660043001}, {"id": 318, "seek": 188780, "start": 1899.8, "end": 1910.8, "text": " With some arithmetic operation, you will see that if you basically have a noisy image and you want to predict the less noisy version, right?", "tokens": [50964, 2022, 512, 42973, 6916, 11, 291, 486, 536, 300, 498, 291, 1936, 362, 257, 24518, 3256, 293, 291, 528, 281, 6069, 264, 1570, 24518, 3037, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.1434674884961999, "compression_ratio": 1.6526315789473685, "no_speech_prob": 0.001983851660043001}, {"id": 319, "seek": 191080, "start": 1910.8, "end": 1917.8, "text": " If you knew the noise that was used to generate that noisy image, you can just subtract some re-scaled version.", "tokens": [50364, 759, 291, 2586, 264, 5658, 300, 390, 1143, 281, 8460, 300, 24518, 3256, 11, 291, 393, 445, 16390, 512, 319, 12, 4417, 5573, 3037, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17406550431862855, "compression_ratio": 1.7630057803468209, "no_speech_prob": 0.0032547269947826862}, {"id": 320, "seek": 191080, "start": 1917.8, "end": 1919.8, "text": " So this is the scaled version of those.", "tokens": [50714, 407, 341, 307, 264, 36039, 3037, 295, 729, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17406550431862855, "compression_ratio": 1.7630057803468209, "no_speech_prob": 0.0032547269947826862}, {"id": 321, "seek": 191080, "start": 1919.8, "end": 1928.8, "text": " So you can take noise, subtract just some scaled version of that noise from xt to get mean of xt minus one.", "tokens": [50814, 407, 291, 393, 747, 5658, 11, 16390, 445, 512, 36039, 3037, 295, 300, 5658, 490, 220, 734, 281, 483, 914, 295, 220, 734, 3175, 472, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17406550431862855, "compression_ratio": 1.7630057803468209, "no_speech_prob": 0.0032547269947826862}, {"id": 322, "seek": 191080, "start": 1928.8, "end": 1930.8, "text": " This is kind of very interesting information.", "tokens": [51264, 639, 307, 733, 295, 588, 1880, 1589, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17406550431862855, "compression_ratio": 1.7630057803468209, "no_speech_prob": 0.0032547269947826862}, {"id": 323, "seek": 193080, "start": 1930.8, "end": 1940.8, "text": " So you basically can represent this mean in a very simple form, expression of xt and epsilon, the noise that was used to generate xt.", "tokens": [50364, 407, 291, 1936, 393, 2906, 341, 914, 294, 257, 588, 2199, 1254, 11, 6114, 295, 220, 734, 293, 17889, 11, 264, 5658, 300, 390, 1143, 281, 8460, 220, 734, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1264763108219009, "compression_ratio": 1.7927461139896372, "no_speech_prob": 0.006476949900388718}, {"id": 324, "seek": 193080, "start": 1940.8, "end": 1944.8, "text": " So this actually is the same noise that was used to generate xt.", "tokens": [50864, 407, 341, 767, 307, 264, 912, 5658, 300, 390, 1143, 281, 8460, 220, 734, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1264763108219009, "compression_ratio": 1.7927461139896372, "no_speech_prob": 0.006476949900388718}, {"id": 325, "seek": 193080, "start": 1944.8, "end": 1953.8, "text": " Knowing that knowledge, it means that now if we want to parameterize this network, we can use this knowledge in the parameterization of this model.", "tokens": [51064, 25499, 300, 3601, 11, 309, 1355, 300, 586, 498, 321, 528, 281, 13075, 1125, 341, 3209, 11, 321, 393, 764, 341, 3601, 294, 264, 13075, 2144, 295, 341, 2316, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1264763108219009, "compression_ratio": 1.7927461139896372, "no_speech_prob": 0.006476949900388718}, {"id": 326, "seek": 195380, "start": 1953.8, "end": 1968.8, "text": " So we can say that in order to predict this mean of less noisy images, we're going to just take xt and subtract it from a neural network that predicts the noise that was used to generate this xt.", "tokens": [50364, 407, 321, 393, 584, 300, 294, 1668, 281, 6069, 341, 914, 295, 1570, 24518, 5267, 11, 321, 434, 516, 281, 445, 747, 220, 734, 293, 16390, 309, 490, 257, 18161, 3209, 300, 6069, 82, 264, 5658, 300, 390, 1143, 281, 8460, 341, 220, 734, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08584914629972434, "compression_ratio": 1.8296703296703296, "no_speech_prob": 0.0032607968896627426}, {"id": 327, "seek": 195380, "start": 1968.8, "end": 1977.8, "text": " So basically we train a neural network to predict the noise that was used in order to generate xt in order to represent this noisy model.", "tokens": [51114, 407, 1936, 321, 3847, 257, 18161, 3209, 281, 6069, 264, 5658, 300, 390, 1143, 294, 1668, 281, 8460, 220, 734, 294, 1668, 281, 2906, 341, 24518, 2316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08584914629972434, "compression_ratio": 1.8296703296703296, "no_speech_prob": 0.0032607968896627426}, {"id": 328, "seek": 197780, "start": 1977.8, "end": 1980.8, "text": " This is just a parameterization trick.", "tokens": [50364, 639, 307, 445, 257, 13075, 2144, 4282, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10098379784887963, "compression_ratio": 1.875, "no_speech_prob": 0.010642530396580696}, {"id": 329, "seek": 197780, "start": 1980.8, "end": 1990.8, "text": " Instead of just representing the mean of the noisy model directly, what we can do is that we can just represent the noise that was used to generate xt.", "tokens": [50514, 7156, 295, 445, 13460, 264, 914, 295, 264, 24518, 2316, 3838, 11, 437, 321, 393, 360, 307, 300, 321, 393, 445, 2906, 264, 5658, 300, 390, 1143, 281, 8460, 220, 734, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10098379784887963, "compression_ratio": 1.875, "no_speech_prob": 0.010642530396580696}, {"id": 330, "seek": 197780, "start": 1990.8, "end": 1996.8, "text": " And if you have this, you can just subtract this from xt in order to get the mean of the denoising model.", "tokens": [51014, 400, 498, 291, 362, 341, 11, 291, 393, 445, 16390, 341, 490, 220, 734, 294, 1668, 281, 483, 264, 914, 295, 264, 1441, 78, 3436, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10098379784887963, "compression_ratio": 1.875, "no_speech_prob": 0.010642530396580696}, {"id": 331, "seek": 197780, "start": 1996.8, "end": 2002.8, "text": " So if you assume this parameterization for the denoising model.", "tokens": [51314, 407, 498, 291, 6552, 341, 13075, 2144, 337, 264, 1441, 78, 3436, 2316, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10098379784887963, "compression_ratio": 1.875, "no_speech_prob": 0.010642530396580696}, {"id": 332, "seek": 200280, "start": 2002.8, "end": 2006.8, "text": " And now we also know that this is true for mu theta t.", "tokens": [50364, 400, 586, 321, 611, 458, 300, 341, 307, 2074, 337, 2992, 9725, 256, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1937496094476609, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0024320317897945642}, {"id": 333, "seek": 200280, "start": 2006.8, "end": 2013.8, "text": " If you plot these two expressions into this, you're going to actually get a very simple expression here.", "tokens": [50564, 759, 291, 7542, 613, 732, 15277, 666, 341, 11, 291, 434, 516, 281, 767, 483, 257, 588, 2199, 6114, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1937496094476609, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0024320317897945642}, {"id": 334, "seek": 200280, "start": 2013.8, "end": 2026.8, "text": " So we have this lt minus one, this is the same term, lt minus one becomes just you need to draw samples from training data distribution, you draw some noise vector from standard normal distribution.", "tokens": [50914, 407, 321, 362, 341, 287, 83, 3175, 472, 11, 341, 307, 264, 912, 1433, 11, 287, 83, 3175, 472, 3643, 445, 291, 643, 281, 2642, 10938, 490, 3097, 1412, 7316, 11, 291, 2642, 512, 5658, 8062, 490, 3832, 2710, 7316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1937496094476609, "compression_ratio": 1.6422018348623852, "no_speech_prob": 0.0024320317897945642}, {"id": 335, "seek": 202680, "start": 2026.8, "end": 2033.8, "text": " And using this noise vector, you just generate this xt using the few samples using the parameterization trick.", "tokens": [50364, 400, 1228, 341, 5658, 8062, 11, 291, 445, 8460, 341, 220, 734, 1228, 264, 1326, 10938, 1228, 264, 13075, 2144, 4282, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16524696350097656, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00022278654796537012}, {"id": 336, "seek": 202680, "start": 2033.8, "end": 2045.8, "text": " You can now pass this if you sample to your epsilon prediction network, the network that is trained to predict that the noise that was used in order to generate xt.", "tokens": [50714, 509, 393, 586, 1320, 341, 498, 291, 6889, 281, 428, 17889, 17630, 3209, 11, 264, 3209, 300, 307, 8895, 281, 6069, 300, 264, 5658, 300, 390, 1143, 294, 1668, 281, 8460, 220, 734, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16524696350097656, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.00022278654796537012}, {"id": 337, "seek": 204580, "start": 2045.8, "end": 2049.8, "text": " So it's basically a very simple algorithm.", "tokens": [50364, 407, 309, 311, 1936, 257, 588, 2199, 9284, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13989584620405987, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0013448243262246251}, {"id": 338, "seek": 204580, "start": 2049.8, "end": 2056.8, "text": " You draw samples from data distribution, you draw noise, you generate xt from that noise and input data.", "tokens": [50564, 509, 2642, 10938, 490, 1412, 7316, 11, 291, 2642, 5658, 11, 291, 8460, 220, 734, 490, 300, 5658, 293, 4846, 1412, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13989584620405987, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0013448243262246251}, {"id": 339, "seek": 204580, "start": 2056.8, "end": 2063.8, "text": " And you train a network to predict the noise that was used in order to generate that xt.", "tokens": [50914, 400, 291, 3847, 257, 3209, 281, 6069, 264, 5658, 300, 390, 1143, 294, 1668, 281, 8460, 300, 220, 734, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13989584620405987, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0013448243262246251}, {"id": 340, "seek": 204580, "start": 2063.8, "end": 2069.8, "text": " And these weights here are just some scalar parameters that comes from this basically one over two sigma t here.", "tokens": [51264, 400, 613, 17443, 510, 366, 445, 512, 39684, 9834, 300, 1487, 490, 341, 1936, 472, 670, 732, 12771, 256, 510, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13989584620405987, "compression_ratio": 1.661904761904762, "no_speech_prob": 0.0013448243262246251}, {"id": 341, "seek": 206980, "start": 2069.8, "end": 2081.8, "text": " And this is like one over square root of beta t. And these terms that we know, we can compute very easily based on the parameters of the diffusion process.", "tokens": [50364, 400, 341, 307, 411, 472, 670, 3732, 5593, 295, 9861, 256, 13, 400, 613, 2115, 300, 321, 458, 11, 321, 393, 14722, 588, 3612, 2361, 322, 264, 9834, 295, 264, 25242, 1399, 13, 50964], "temperature": 0.0, "avg_logprob": -0.13229120345342726, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0003458384599070996}, {"id": 342, "seek": 206980, "start": 2081.8, "end": 2088.8, "text": " So you can ignore them for a moment. Think of them as a scalar parameters that we can compute from the diffusion parameters.", "tokens": [50964, 407, 291, 393, 11200, 552, 337, 257, 1623, 13, 6557, 295, 552, 382, 257, 39684, 9834, 300, 321, 393, 14722, 490, 264, 25242, 9834, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13229120345342726, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0003458384599070996}, {"id": 343, "seek": 206980, "start": 2088.8, "end": 2093.8, "text": " So xt minus one can be represented as this weighted objective.", "tokens": [51314, 407, 220, 734, 3175, 472, 393, 312, 10379, 382, 341, 32807, 10024, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13229120345342726, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0003458384599070996}, {"id": 344, "seek": 206980, "start": 2093.8, "end": 2098.8, "text": " So we're going to just summarize these weights as lambda t. We're going to define a new scalar lambda t.", "tokens": [51564, 407, 321, 434, 516, 281, 445, 20858, 613, 17443, 382, 13607, 256, 13, 492, 434, 516, 281, 6964, 257, 777, 39684, 13607, 256, 13, 51814], "temperature": 0.0, "avg_logprob": -0.13229120345342726, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.0003458384599070996}, {"id": 345, "seek": 209880, "start": 2098.8, "end": 2104.8, "text": " This lambda t ensures that your training objective is weighted properly for maximum data likelihood training.", "tokens": [50364, 639, 13607, 256, 28111, 300, 428, 3097, 10024, 307, 32807, 6108, 337, 6674, 1412, 22119, 3097, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16424669103419526, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0008957699174061418}, {"id": 346, "seek": 209880, "start": 2104.8, "end": 2110.8, "text": " So by using this lambda t weights, you're actually maximizing data likelihood.", "tokens": [50664, 407, 538, 1228, 341, 13607, 256, 17443, 11, 291, 434, 767, 5138, 3319, 1412, 22119, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16424669103419526, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0008957699174061418}, {"id": 347, "seek": 209880, "start": 2110.8, "end": 2118.8, "text": " But however what happens is that this lambda t ends up being very large for small t's and it is small for large t's.", "tokens": [50964, 583, 4461, 437, 2314, 307, 300, 341, 13607, 256, 5314, 493, 885, 588, 2416, 337, 1359, 256, 311, 293, 309, 307, 1359, 337, 2416, 256, 311, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16424669103419526, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0008957699174061418}, {"id": 348, "seek": 209880, "start": 2118.8, "end": 2125.8, "text": " So it kind of monotically decreases for small t's is very high and for large t's is very small.", "tokens": [51364, 407, 309, 733, 295, 1108, 310, 984, 24108, 337, 1359, 256, 311, 307, 588, 1090, 293, 337, 2416, 256, 311, 307, 588, 1359, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16424669103419526, "compression_ratio": 1.8310502283105023, "no_speech_prob": 0.0008957699174061418}, {"id": 349, "seek": 212580, "start": 2125.8, "end": 2130.8, "text": " And this is basically how the maximum data likelihood training is formulated.", "tokens": [50364, 400, 341, 307, 1936, 577, 264, 6674, 1412, 22119, 3097, 307, 48936, 13, 50614], "temperature": 0.0, "avg_logprob": -0.21953535615728142, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.0013241178821772337}, {"id": 350, "seek": 212580, "start": 2130.8, "end": 2145.8, "text": " However, in this paper by Hoh et al. in Europe's 2020, they observed that if you simply drop these weights or equally just if you simply set these weights to one and train the model using this on the weighted version.", "tokens": [50614, 2908, 11, 294, 341, 3035, 538, 389, 1445, 1030, 419, 13, 294, 3315, 311, 4808, 11, 436, 13095, 300, 498, 291, 2935, 3270, 613, 17443, 420, 12309, 445, 498, 291, 2935, 992, 613, 17443, 281, 472, 293, 3847, 264, 2316, 1228, 341, 322, 264, 32807, 3037, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21953535615728142, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.0013241178821772337}, {"id": 351, "seek": 212580, "start": 2145.8, "end": 2151.8, "text": " Like if you drop this way, you're going to get very high quality sample generation using diffusion models.", "tokens": [51364, 1743, 498, 291, 3270, 341, 636, 11, 291, 434, 516, 281, 483, 588, 1090, 3125, 6889, 5125, 1228, 25242, 5245, 13, 51664], "temperature": 0.0, "avg_logprob": -0.21953535615728142, "compression_ratio": 1.6680497925311204, "no_speech_prob": 0.0013241178821772337}, {"id": 352, "seek": 215180, "start": 2151.8, "end": 2157.8, "text": " So they introduced this very simple objective that does not have this weighting anymore.", "tokens": [50364, 407, 436, 7268, 341, 588, 2199, 10024, 300, 775, 406, 362, 341, 3364, 278, 3602, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16552341552007765, "compression_ratio": 1.71875, "no_speech_prob": 0.0009836667450144887}, {"id": 353, "seek": 215180, "start": 2157.8, "end": 2159.8, "text": " This weighting is one.", "tokens": [50664, 639, 3364, 278, 307, 472, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16552341552007765, "compression_ratio": 1.71875, "no_speech_prob": 0.0009836667450144887}, {"id": 354, "seek": 215180, "start": 2159.8, "end": 2169.8, "text": " So again, this objective simply draws samples from data distribution, draws white noise and randomly samples from one of the time steps from one to capital t.", "tokens": [50764, 407, 797, 11, 341, 10024, 2935, 20045, 10938, 490, 1412, 7316, 11, 20045, 2418, 5658, 293, 16979, 10938, 490, 472, 295, 264, 565, 4439, 490, 472, 281, 4238, 256, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16552341552007765, "compression_ratio": 1.71875, "no_speech_prob": 0.0009836667450144887}, {"id": 355, "seek": 215180, "start": 2169.8, "end": 2177.8, "text": " It generates a diffused sample using the parameterization trick and it trains a model to predict a noise injected.", "tokens": [51264, 467, 23815, 257, 7593, 4717, 6889, 1228, 264, 13075, 2144, 4282, 293, 309, 16329, 257, 2316, 281, 6069, 257, 5658, 36967, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16552341552007765, "compression_ratio": 1.71875, "no_speech_prob": 0.0009836667450144887}, {"id": 356, "seek": 217780, "start": 2177.8, "end": 2183.8, "text": " So it's exactly the same objective without any weighting and it can be done very easily.", "tokens": [50364, 407, 309, 311, 2293, 264, 912, 10024, 1553, 604, 3364, 278, 293, 309, 393, 312, 1096, 588, 3612, 13, 50664], "temperature": 0.0, "avg_logprob": -0.18326598576137, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0041828639805316925}, {"id": 357, "seek": 217780, "start": 2183.8, "end": 2192.8, "text": " And the answer is true that actually with this weighting, you will get very high quality sample generation using diffusion models.", "tokens": [50664, 400, 264, 1867, 307, 2074, 300, 767, 365, 341, 3364, 278, 11, 291, 486, 483, 588, 1090, 3125, 6889, 5125, 1228, 25242, 5245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18326598576137, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0041828639805316925}, {"id": 358, "seek": 217780, "start": 2192.8, "end": 2200.8, "text": " This objective weighting actually plays a key role in getting high quality sample generation in diffusion models.", "tokens": [51114, 639, 10024, 3364, 278, 767, 5749, 257, 2141, 3090, 294, 1242, 1090, 3125, 6889, 5125, 294, 25242, 5245, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18326598576137, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0041828639805316925}, {"id": 359, "seek": 220080, "start": 2200.8, "end": 2217.8, "text": " So if you're interested in checking this area, I would encourage you to check this paper by Hoh et al. published at CUQ 2022 that discuss how you can potentially change this weighting over time to get even better high quality images from diffusion models.", "tokens": [50364, 407, 498, 291, 434, 3102, 294, 8568, 341, 1859, 11, 286, 576, 5373, 291, 281, 1520, 341, 3035, 538, 389, 1445, 1030, 419, 13, 6572, 412, 29777, 48, 20229, 300, 2248, 577, 291, 393, 7263, 1319, 341, 3364, 278, 670, 565, 281, 483, 754, 1101, 1090, 3125, 5267, 490, 25242, 5245, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13521039640748655, "compression_ratio": 1.521551724137931, "no_speech_prob": 0.009379143826663494}, {"id": 360, "seek": 220080, "start": 2217.8, "end": 2224.8, "text": " So let's summarize the training and sampling from diffusion models and what we've learned so far.", "tokens": [51214, 407, 718, 311, 20858, 264, 3097, 293, 21179, 490, 25242, 5245, 293, 437, 321, 600, 3264, 370, 1400, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13521039640748655, "compression_ratio": 1.521551724137931, "no_speech_prob": 0.009379143826663494}, {"id": 361, "seek": 222480, "start": 2224.8, "end": 2228.8, "text": " In order to train diffusion models, the algorithm is extremely simple.", "tokens": [50364, 682, 1668, 281, 3847, 25242, 5245, 11, 264, 9284, 307, 4664, 2199, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10740009546279908, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.006778600625693798}, {"id": 362, "seek": 222480, "start": 2228.8, "end": 2231.8, "text": " You draw a batch of samples from your training data distribution.", "tokens": [50564, 509, 2642, 257, 15245, 295, 10938, 490, 428, 3097, 1412, 7316, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10740009546279908, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.006778600625693798}, {"id": 363, "seek": 222480, "start": 2231.8, "end": 2235.8, "text": " You uniformly sample from these time steps from one to capital t.", "tokens": [50714, 509, 48806, 6889, 490, 613, 565, 4439, 490, 472, 281, 4238, 256, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10740009546279908, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.006778600625693798}, {"id": 364, "seek": 222480, "start": 2235.8, "end": 2240.8, "text": " You draw some random noise that has same dimensionality as your input data.", "tokens": [50914, 509, 2642, 512, 4974, 5658, 300, 575, 912, 10139, 1860, 382, 428, 4846, 1412, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10740009546279908, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.006778600625693798}, {"id": 365, "seek": 222480, "start": 2240.8, "end": 2246.8, "text": " And you use the parameterization trick to generate sample at time step t.", "tokens": [51164, 400, 291, 764, 264, 13075, 2144, 4282, 281, 8460, 6889, 412, 565, 1823, 256, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10740009546279908, "compression_ratio": 1.6525821596244132, "no_speech_prob": 0.006778600625693798}, {"id": 366, "seek": 224680, "start": 2246.8, "end": 2256.8, "text": " You give the sample to your noise prediction network and you train this noise prediction network to predict the noise that was used to generate that diffuse sample.", "tokens": [50364, 509, 976, 264, 6889, 281, 428, 5658, 17630, 3209, 293, 291, 3847, 341, 5658, 17630, 3209, 281, 6069, 264, 5658, 300, 390, 1143, 281, 8460, 300, 42165, 6889, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13392439006287374, "compression_ratio": 1.9547738693467336, "no_speech_prob": 0.002212567487731576}, {"id": 367, "seek": 224680, "start": 2256.8, "end": 2264.8, "text": " And to train this you just simply use squared L2 loss to train this noise prediction network.", "tokens": [50864, 400, 281, 3847, 341, 291, 445, 2935, 764, 8889, 441, 17, 4470, 281, 3847, 341, 5658, 17630, 3209, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13392439006287374, "compression_ratio": 1.9547738693467336, "no_speech_prob": 0.002212567487731576}, {"id": 368, "seek": 224680, "start": 2264.8, "end": 2272.8, "text": " After training, if you like to draw samples from your diffusion model, you can use the reverse diffusion process to generate data.", "tokens": [51264, 2381, 3097, 11, 498, 291, 411, 281, 2642, 10938, 490, 428, 25242, 2316, 11, 291, 393, 764, 264, 9943, 25242, 1399, 281, 8460, 1412, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13392439006287374, "compression_ratio": 1.9547738693467336, "no_speech_prob": 0.002212567487731576}, {"id": 369, "seek": 227280, "start": 2272.8, "end": 2275.8, "text": " So we're going to start from last step x capital t.", "tokens": [50364, 407, 321, 434, 516, 281, 722, 490, 1036, 1823, 2031, 4238, 256, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1700303940545945, "compression_ratio": 1.7581967213114753, "no_speech_prob": 0.004184841178357601}, {"id": 370, "seek": 227280, "start": 2275.8, "end": 2279.8, "text": " We're going to draw samples from standard normal distribution.", "tokens": [50514, 492, 434, 516, 281, 2642, 10938, 490, 3832, 2710, 7316, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1700303940545945, "compression_ratio": 1.7581967213114753, "no_speech_prob": 0.004184841178357601}, {"id": 371, "seek": 227280, "start": 2279.8, "end": 2287.8, "text": " And then here we have a full look that walks back in diffusion process starting on capital t all the way to t equals to 1.", "tokens": [50714, 400, 550, 510, 321, 362, 257, 1577, 574, 300, 12896, 646, 294, 25242, 1399, 2891, 322, 4238, 256, 439, 264, 636, 281, 256, 6915, 281, 502, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1700303940545945, "compression_ratio": 1.7581967213114753, "no_speech_prob": 0.004184841178357601}, {"id": 372, "seek": 227280, "start": 2287.8, "end": 2292.8, "text": " At every step we just draw white noise z from standard normal distribution.", "tokens": [51114, 1711, 633, 1823, 321, 445, 2642, 2418, 5658, 710, 490, 3832, 2710, 7316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1700303940545945, "compression_ratio": 1.7581967213114753, "no_speech_prob": 0.004184841178357601}, {"id": 373, "seek": 227280, "start": 2292.8, "end": 2296.8, "text": " Here we're forming the mean of the nosing model.", "tokens": [51364, 1692, 321, 434, 15745, 264, 914, 295, 264, 3269, 278, 2316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1700303940545945, "compression_ratio": 1.7581967213114753, "no_speech_prob": 0.004184841178357601}, {"id": 374, "seek": 227280, "start": 2296.8, "end": 2300.8, "text": " Remember this is the parameterization we use for the nosing model.", "tokens": [51564, 5459, 341, 307, 264, 13075, 2144, 321, 764, 337, 264, 3269, 278, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1700303940545945, "compression_ratio": 1.7581967213114753, "no_speech_prob": 0.004184841178357601}, {"id": 375, "seek": 230080, "start": 2300.8, "end": 2309.8, "text": " And then we add noise rescaled with the standard deviation of the nosing minus sigma t to generate xt minus 1.", "tokens": [50364, 400, 550, 321, 909, 5658, 9610, 5573, 365, 264, 3832, 25163, 295, 264, 3269, 278, 3175, 12771, 256, 281, 8460, 2031, 83, 3175, 502, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19527942255923622, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.0003390397469047457}, {"id": 376, "seek": 230080, "start": 2309.8, "end": 2314.8, "text": " And we can repeat this t times in order to generate x0.", "tokens": [50814, 400, 321, 393, 7149, 341, 256, 1413, 294, 1668, 281, 8460, 2031, 15, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19527942255923622, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.0003390397469047457}, {"id": 377, "seek": 230080, "start": 2314.8, "end": 2320.8, "text": " So very simple training and very simple generation process.", "tokens": [51064, 407, 588, 2199, 3097, 293, 588, 2199, 5125, 1399, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19527942255923622, "compression_ratio": 1.5586206896551724, "no_speech_prob": 0.0003390397469047457}, {"id": 378, "seek": 232080, "start": 2321.8, "end": 2324.8, "text": " So far we talked about training and sampling.", "tokens": [50414, 407, 1400, 321, 2825, 466, 3097, 293, 21179, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12223402659098308, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0004796121793333441}, {"id": 379, "seek": 232080, "start": 2324.8, "end": 2330.8, "text": " So let's talk about the implementation details of how to form neural networks for the nosing model.", "tokens": [50564, 407, 718, 311, 751, 466, 264, 11420, 4365, 295, 577, 281, 1254, 18161, 9590, 337, 264, 3269, 278, 2316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12223402659098308, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0004796121793333441}, {"id": 380, "seek": 232080, "start": 2330.8, "end": 2337.8, "text": " In practice, most diffusion models use unit architecture to represent the nosing model.", "tokens": [50864, 682, 3124, 11, 881, 25242, 5245, 764, 4985, 9482, 281, 2906, 264, 3269, 278, 2316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12223402659098308, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0004796121793333441}, {"id": 381, "seek": 232080, "start": 2337.8, "end": 2340.8, "text": " This unit architecture often has residual blocks.", "tokens": [51214, 639, 4985, 9482, 2049, 575, 27980, 8474, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12223402659098308, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0004796121793333441}, {"id": 382, "seek": 234080, "start": 2341.8, "end": 2346.8, "text": " So here different rectangles represent residual blocks at different scales.", "tokens": [50414, 407, 510, 819, 24077, 904, 2906, 27980, 8474, 412, 819, 17408, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17217819779007523, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.003691902616992593}, {"id": 383, "seek": 234080, "start": 2346.8, "end": 2350.8, "text": " And these residual blocks often have also self-attention layers in them.", "tokens": [50664, 400, 613, 27980, 8474, 2049, 362, 611, 2698, 12, 1591, 1251, 7914, 294, 552, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17217819779007523, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.003691902616992593}, {"id": 384, "seek": 234080, "start": 2350.8, "end": 2355.8, "text": " In some layers usually produce self-attention layers.", "tokens": [50864, 682, 512, 7914, 2673, 5258, 2698, 12, 1591, 1251, 7914, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17217819779007523, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.003691902616992593}, {"id": 385, "seek": 234080, "start": 2355.8, "end": 2367.8, "text": " Remember this unit takes this diffused image, diffused peanut, xt, and it predicts the noise that was used in order to generate this diffuse image.", "tokens": [51114, 5459, 341, 4985, 2516, 341, 7593, 4717, 3256, 11, 7593, 4717, 19209, 11, 2031, 83, 11, 293, 309, 6069, 82, 264, 5658, 300, 390, 1143, 294, 1668, 281, 8460, 341, 42165, 3256, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17217819779007523, "compression_ratio": 1.7326732673267327, "no_speech_prob": 0.003691902616992593}, {"id": 386, "seek": 236780, "start": 2367.8, "end": 2374.8, "text": " So this epsilon prediction will be trained to produce the predicted noise that was used to generate this xt.", "tokens": [50364, 407, 341, 17889, 17630, 486, 312, 8895, 281, 5258, 264, 19147, 5658, 300, 390, 1143, 281, 8460, 341, 2031, 83, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13757567604382834, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.004716318100690842}, {"id": 387, "seek": 236780, "start": 2374.8, "end": 2376.8, "text": " This network is also conditional time.", "tokens": [50714, 639, 3209, 307, 611, 27708, 565, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13757567604382834, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.004716318100690842}, {"id": 388, "seek": 236780, "start": 2376.8, "end": 2378.8, "text": " It's shared across different time steps.", "tokens": [50814, 467, 311, 5507, 2108, 819, 565, 4439, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13757567604382834, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.004716318100690842}, {"id": 389, "seek": 236780, "start": 2378.8, "end": 2382.8, "text": " So it also takes time using some time embedding.", "tokens": [50914, 407, 309, 611, 2516, 565, 1228, 512, 565, 12240, 3584, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13757567604382834, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.004716318100690842}, {"id": 390, "seek": 236780, "start": 2382.8, "end": 2394.8, "text": " This time embedding can be done using, for example, sinusoidal positional embeddings that are often used in transformers or random free features to represent this time embedding.", "tokens": [51114, 639, 565, 12240, 3584, 393, 312, 1096, 1228, 11, 337, 1365, 11, 41503, 17079, 304, 2535, 304, 12240, 29432, 300, 366, 2049, 1143, 294, 4088, 433, 420, 4974, 1737, 4122, 281, 2906, 341, 565, 12240, 3584, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13757567604382834, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.004716318100690842}, {"id": 391, "seek": 239480, "start": 2394.8, "end": 2401.8, "text": " This time embedding will be a vector that will be fed to a small fully connected network.", "tokens": [50364, 639, 565, 12240, 3584, 486, 312, 257, 8062, 300, 486, 312, 4636, 281, 257, 1359, 4498, 4582, 3209, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1174852389555711, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0015895675169304013}, {"id": 392, "seek": 239480, "start": 2401.8, "end": 2406.8, "text": " A network consists of a few fully connected layers to access some time representation.", "tokens": [50714, 316, 3209, 14689, 295, 257, 1326, 4498, 4582, 7914, 281, 2105, 512, 565, 10290, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1174852389555711, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0015895675169304013}, {"id": 393, "seek": 239480, "start": 2406.8, "end": 2410.8, "text": " And this time representation is usually fed to all the residual blocks.", "tokens": [50964, 400, 341, 565, 10290, 307, 2673, 4636, 281, 439, 264, 27980, 8474, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1174852389555711, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0015895675169304013}, {"id": 394, "seek": 239480, "start": 2410.8, "end": 2415.8, "text": " In order to combine this time embedding with all the residual blocks, you have a few options.", "tokens": [51164, 682, 1668, 281, 10432, 341, 565, 12240, 3584, 365, 439, 264, 27980, 8474, 11, 291, 362, 257, 1326, 3956, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1174852389555711, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0015895675169304013}, {"id": 395, "seek": 239480, "start": 2415.8, "end": 2423.8, "text": " For example, you can just take this time embedding and do arithmetic sum with all the spatial features in the residual blocks.", "tokens": [51414, 1171, 1365, 11, 291, 393, 445, 747, 341, 565, 12240, 3584, 293, 360, 42973, 2408, 365, 439, 264, 23598, 4122, 294, 264, 27980, 8474, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1174852389555711, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0015895675169304013}, {"id": 396, "seek": 242380, "start": 2423.8, "end": 2430.8, "text": " Or you can use, for example, adaptive group normalization in order to do this, like to add time embedding into residual blocks.", "tokens": [50364, 1610, 291, 393, 764, 11, 337, 1365, 11, 27912, 1594, 2710, 2144, 294, 1668, 281, 360, 341, 11, 411, 281, 909, 565, 12240, 3584, 666, 27980, 8474, 13, 50714], "temperature": 0.0, "avg_logprob": -0.23453196357278264, "compression_ratio": 1.625, "no_speech_prob": 0.0021340087987482548}, {"id": 397, "seek": 242380, "start": 2430.8, "end": 2440.8, "text": " So I would encourage you to check this paper that discuss fruits and trades between adaptive group normalization and spatial recognition.", "tokens": [50714, 407, 286, 576, 5373, 291, 281, 1520, 341, 3035, 300, 2248, 12148, 293, 21287, 1296, 27912, 1594, 2710, 2144, 293, 23598, 11150, 13, 51214], "temperature": 0.0, "avg_logprob": -0.23453196357278264, "compression_ratio": 1.625, "no_speech_prob": 0.0021340087987482548}, {"id": 398, "seek": 242380, "start": 2440.8, "end": 2448.8, "text": " So, so far we talked about forward process, reverse process, training, as well as the network's design for diffusion models.", "tokens": [51214, 407, 11, 370, 1400, 321, 2825, 466, 2128, 1399, 11, 9943, 1399, 11, 3097, 11, 382, 731, 382, 264, 3209, 311, 1715, 337, 25242, 5245, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23453196357278264, "compression_ratio": 1.625, "no_speech_prob": 0.0021340087987482548}, {"id": 399, "seek": 244880, "start": 2448.8, "end": 2463.8, "text": " Let's also talk about some of these hyperparameters that we have in diffusion models, mostly beta T schedule, the variance of the forward process and the variance used in the reverse process, sigma T square.", "tokens": [50364, 961, 311, 611, 751, 466, 512, 295, 613, 9848, 2181, 335, 6202, 300, 321, 362, 294, 25242, 5245, 11, 5240, 9861, 314, 7567, 11, 264, 21977, 295, 264, 2128, 1399, 293, 264, 21977, 1143, 294, 264, 9943, 1399, 11, 12771, 314, 3732, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19943908934897564, "compression_ratio": 1.4375, "no_speech_prob": 0.007444394752383232}, {"id": 400, "seek": 246380, "start": 2463.8, "end": 2480.8, "text": " So, one common assumption is that we can, most papers follow Jonathan Hobot of Newripp's 2020 paper, where they use just simply beta T's that are defined using just a linear function.", "tokens": [50364, 407, 11, 472, 2689, 15302, 307, 300, 321, 393, 11, 881, 10577, 1524, 15471, 22966, 310, 295, 1873, 470, 427, 311, 4808, 3035, 11, 689, 436, 764, 445, 2935, 9861, 314, 311, 300, 366, 7642, 1228, 445, 257, 8213, 2445, 13, 51214], "temperature": 0.0, "avg_logprob": -0.28870638679055605, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.006576375104486942}, {"id": 401, "seek": 246380, "start": 2480.8, "end": 2488.8, "text": " Just these beta T's are small values and they gradually go to some larger value through some linear schedule.", "tokens": [51214, 1449, 613, 9861, 314, 311, 366, 1359, 4190, 293, 436, 13145, 352, 281, 512, 4833, 2158, 807, 512, 8213, 7567, 13, 51614], "temperature": 0.0, "avg_logprob": -0.28870638679055605, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.006576375104486942}, {"id": 402, "seek": 248880, "start": 2488.8, "end": 2501.8, "text": " And it's also common to assume that sigma T square is just equal, set equal to beta T. This works also really well in practice, especially when the number of diffusions steps is large.", "tokens": [50364, 400, 309, 311, 611, 2689, 281, 6552, 300, 12771, 314, 3732, 307, 445, 2681, 11, 992, 2681, 281, 9861, 314, 13, 639, 1985, 611, 534, 731, 294, 3124, 11, 2318, 562, 264, 1230, 295, 7593, 27255, 4439, 307, 2416, 13, 51014], "temperature": 0.0, "avg_logprob": -0.168641167718011, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003022401360794902}, {"id": 403, "seek": 248880, "start": 2501.8, "end": 2508.8, "text": " But you may ask me, how can I train these palms? Is there any way I can also train beta T and sigma T square?", "tokens": [51014, 583, 291, 815, 1029, 385, 11, 577, 393, 286, 3847, 613, 30819, 30, 1119, 456, 604, 636, 286, 393, 611, 3847, 9861, 314, 293, 12771, 314, 3732, 30, 51364], "temperature": 0.0, "avg_logprob": -0.168641167718011, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003022401360794902}, {"id": 404, "seek": 250880, "start": 2508.8, "end": 2521.8, "text": " So, there are a couple of papers that discuss this. One of them is this paper by Kimba Tao at Notives 2022. This paper introduces a new parameterization diffusion model using a concept called signal to noise ratio.", "tokens": [50364, 407, 11, 456, 366, 257, 1916, 295, 10577, 300, 2248, 341, 13, 1485, 295, 552, 307, 341, 3035, 538, 5652, 4231, 26580, 412, 1726, 1539, 20229, 13, 639, 3035, 31472, 257, 777, 13075, 2144, 25242, 2316, 1228, 257, 3410, 1219, 6358, 281, 5658, 8509, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2017713240635248, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.007557741366326809}, {"id": 405, "seek": 250880, "start": 2521.8, "end": 2530.8, "text": " And they show how you can actually train this noise schedule using some training objective. So, they actually propose a method for training these beta T values.", "tokens": [51014, 400, 436, 855, 577, 291, 393, 767, 3847, 341, 5658, 7567, 1228, 512, 3097, 10024, 13, 407, 11, 436, 767, 17421, 257, 3170, 337, 3097, 613, 9861, 314, 4190, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2017713240635248, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.007557741366326809}, {"id": 406, "seek": 253080, "start": 2530.8, "end": 2538.8, "text": " There are also a couple of papers that discuss how you can train sigma T square, the variance of the reverse process.", "tokens": [50364, 821, 366, 611, 257, 1916, 295, 10577, 300, 2248, 577, 291, 393, 3847, 12771, 314, 3732, 11, 264, 21977, 295, 264, 9943, 1399, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20262430631197414, "compression_ratio": 1.7239263803680982, "no_speech_prob": 0.017957504838705063}, {"id": 407, "seek": 253080, "start": 2538.8, "end": 2548.8, "text": " This first paper here shows how you can use a variational bond that we use for training diffusion models to also train sigma T, the variance of the denuzin models.", "tokens": [50764, 639, 700, 3035, 510, 3110, 577, 291, 393, 764, 257, 3034, 1478, 6086, 300, 321, 764, 337, 3097, 25242, 5245, 281, 611, 3847, 12771, 314, 11, 264, 21977, 295, 264, 1441, 3334, 259, 5245, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20262430631197414, "compression_ratio": 1.7239263803680982, "no_speech_prob": 0.017957504838705063}, {"id": 408, "seek": 254880, "start": 2548.8, "end": 2558.8, "text": " And there's only paper here, analytically P.M. by Bobo et al. in ICELER 2022. This paper actually got outstanding paper award this year at ICELER.", "tokens": [50364, 400, 456, 311, 787, 3035, 510, 11, 10783, 984, 430, 13, 44, 13, 538, 6085, 78, 1030, 419, 13, 294, 14360, 3158, 1598, 20229, 13, 639, 3035, 767, 658, 14485, 3035, 7130, 341, 1064, 412, 14360, 3158, 1598, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2500582751105813, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.05256006121635437}, {"id": 409, "seek": 254880, "start": 2558.8, "end": 2575.8, "text": " And they showed how you can actually post-training, after training your diffusion models, how you can use this to compute the variance of the denuzin model analytically post-training.", "tokens": [50864, 400, 436, 4712, 577, 291, 393, 767, 2183, 12, 17227, 1760, 11, 934, 3097, 428, 25242, 5245, 11, 577, 291, 393, 764, 341, 281, 14722, 264, 21977, 295, 264, 1441, 3334, 259, 2316, 10783, 984, 2183, 12, 17227, 1760, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2500582751105813, "compression_ratio": 1.6176470588235294, "no_speech_prob": 0.05256006121635437}, {"id": 410, "seek": 257580, "start": 2575.8, "end": 2593.8, "text": " So, so far, we talked about how we can train and how we can also pick up these hyperparameters and diffusion models. Let's look at the diffusion process and look into what happens to, for example, images in the forward process.", "tokens": [50364, 407, 11, 370, 1400, 11, 321, 2825, 466, 577, 321, 393, 3847, 293, 577, 321, 393, 611, 1888, 493, 613, 9848, 2181, 335, 6202, 293, 25242, 5245, 13, 961, 311, 574, 412, 264, 25242, 1399, 293, 574, 666, 437, 2314, 281, 11, 337, 1365, 11, 5267, 294, 264, 2128, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13134000951593572, "compression_ratio": 1.5133333333333334, "no_speech_prob": 0.003426122246310115}, {"id": 411, "seek": 259380, "start": 2593.8, "end": 2604.8, "text": " We call it, in order to sample from time step T, we can use this diffusion channel, and we can use this parameterization trick to generate XT from input image X0.", "tokens": [50364, 492, 818, 309, 11, 294, 1668, 281, 6889, 490, 565, 1823, 314, 11, 321, 393, 764, 341, 25242, 2269, 11, 293, 321, 393, 764, 341, 13075, 2144, 4282, 281, 8460, 1783, 51, 490, 4846, 3256, 1783, 15, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20387203043157404, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.020291917026042938}, {"id": 412, "seek": 259380, "start": 2604.8, "end": 2617.8, "text": " Here, XT, we know this diffuse sample, in order to analyze what happens to the image, what we're going to do, we're going to use Fourier transform to convert XT to the frequency domain.", "tokens": [50914, 1692, 11, 1783, 51, 11, 321, 458, 341, 42165, 6889, 11, 294, 1668, 281, 12477, 437, 2314, 281, 264, 3256, 11, 437, 321, 434, 516, 281, 360, 11, 321, 434, 516, 281, 764, 36810, 4088, 281, 7620, 1783, 51, 281, 264, 7893, 9274, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20387203043157404, "compression_ratio": 1.697560975609756, "no_speech_prob": 0.020291917026042938}, {"id": 413, "seek": 261780, "start": 2617.8, "end": 2626.8, "text": " So this FXT, you can think of just Fourier transform applied to XT, it's just a representation of the image in the signal, in the frequency domain.", "tokens": [50364, 407, 341, 479, 20542, 11, 291, 393, 519, 295, 445, 36810, 4088, 6456, 281, 1783, 51, 11, 309, 311, 445, 257, 10290, 295, 264, 3256, 294, 264, 6358, 11, 294, 264, 7893, 9274, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14882618257369118, "compression_ratio": 1.7709251101321586, "no_speech_prob": 0.011649384163320065}, {"id": 414, "seek": 261780, "start": 2626.8, "end": 2643.8, "text": " And we know from Fourier transform that this XT can be just represented as Fourier transform of input image, plus Fourier transform of the noise, some together with some weights corresponds to the ways that we use actually in this parameterization trick.", "tokens": [50814, 400, 321, 458, 490, 36810, 4088, 300, 341, 1783, 51, 393, 312, 445, 10379, 382, 36810, 4088, 295, 4846, 3256, 11, 1804, 36810, 4088, 295, 264, 5658, 11, 512, 1214, 365, 512, 17443, 23249, 281, 264, 2098, 300, 321, 764, 767, 294, 341, 13075, 2144, 4282, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14882618257369118, "compression_ratio": 1.7709251101321586, "no_speech_prob": 0.011649384163320065}, {"id": 415, "seek": 264380, "start": 2643.8, "end": 2648.8, "text": " This is just simple rules in Fourier transform.", "tokens": [50364, 639, 307, 445, 2199, 4474, 294, 36810, 4088, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13798001607259114, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.016566546633839607}, {"id": 416, "seek": 264380, "start": 2648.8, "end": 2659.8, "text": " You should have in mind that most images actually in the frequency domain have a very high response for low frequency, and they have very low response for high frequency content.", "tokens": [50614, 509, 820, 362, 294, 1575, 300, 881, 5267, 767, 294, 264, 7893, 9274, 362, 257, 588, 1090, 4134, 337, 2295, 7893, 11, 293, 436, 362, 588, 2295, 4134, 337, 1090, 7893, 2701, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13798001607259114, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.016566546633839607}, {"id": 417, "seek": 264380, "start": 2659.8, "end": 2662.8, "text": " And this is because most images are very smooth.", "tokens": [51164, 400, 341, 307, 570, 881, 5267, 366, 588, 5508, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13798001607259114, "compression_ratio": 1.6369047619047619, "no_speech_prob": 0.016566546633839607}, {"id": 418, "seek": 266280, "start": 2663.8, "end": 2676.8, "text": " In general, even if they're not super smooth, when you apply Fourier transform to them, you see usually that most images have very high concentration in low frequency, and their high frequency response is very low.", "tokens": [50414, 682, 2674, 11, 754, 498, 436, 434, 406, 1687, 5508, 11, 562, 291, 3079, 36810, 4088, 281, 552, 11, 291, 536, 2673, 300, 881, 5267, 362, 588, 1090, 9856, 294, 2295, 7893, 11, 293, 641, 1090, 7893, 4134, 307, 588, 2295, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12396159342357091, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.009829157032072544}, {"id": 419, "seek": 266280, "start": 2676.8, "end": 2679.8, "text": " This is very common in most images.", "tokens": [51064, 639, 307, 588, 2689, 294, 881, 5267, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12396159342357091, "compression_ratio": 1.5060240963855422, "no_speech_prob": 0.009829157032072544}, {"id": 420, "seek": 267980, "start": 2679.8, "end": 2693.8, "text": " One thing you should also know that if you have a white noise or Gaussian noise and you apply Fourier transform on top of it, in the frequency domain, actually this Gaussian noise can be also represented as just Gaussian noise in the frequency domain as well.", "tokens": [50364, 1485, 551, 291, 820, 611, 458, 300, 498, 291, 362, 257, 2418, 5658, 420, 39148, 5658, 293, 291, 3079, 36810, 4088, 322, 1192, 295, 309, 11, 294, 264, 7893, 9274, 11, 767, 341, 39148, 5658, 393, 312, 611, 10379, 382, 445, 39148, 5658, 294, 264, 7893, 9274, 382, 731, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14075010473077948, "compression_ratio": 1.8673469387755102, "no_speech_prob": 0.005460858345031738}, {"id": 421, "seek": 267980, "start": 2693.8, "end": 2700.8, "text": " So Fourier transform of a Gaussian noise is itself Gaussian noise, which we can use now here for analysis.", "tokens": [51064, 407, 36810, 4088, 295, 257, 39148, 5658, 307, 2564, 39148, 5658, 11, 597, 321, 393, 764, 586, 510, 337, 5215, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14075010473077948, "compression_ratio": 1.8673469387755102, "no_speech_prob": 0.005460858345031738}, {"id": 422, "seek": 270080, "start": 2700.8, "end": 2711.8, "text": " So remember for the small t's, alpha bar t is almost one. So as a result, we actually did the perturbation we apply is very small in the, in the frequency domain as well.", "tokens": [50364, 407, 1604, 337, 264, 1359, 256, 311, 11, 8961, 2159, 256, 307, 1920, 472, 13, 407, 382, 257, 1874, 11, 321, 767, 630, 264, 40468, 399, 321, 3079, 307, 588, 1359, 294, 264, 11, 294, 264, 7893, 9274, 382, 731, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18664322728696076, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.0015963178593665361}, {"id": 423, "seek": 270080, "start": 2711.8, "end": 2726.8, "text": " So in frequency domain, because most of our input signal for input image is concentrated at the small t's, and because alpha bar is almost one, we actually don't perturb the low frequency content of the image that most.", "tokens": [50914, 407, 294, 7893, 9274, 11, 570, 881, 295, 527, 4846, 6358, 337, 4846, 3256, 307, 21321, 412, 264, 1359, 256, 311, 11, 293, 570, 8961, 2159, 307, 1920, 472, 11, 321, 767, 500, 380, 40468, 264, 2295, 7893, 2701, 295, 264, 3256, 300, 881, 13, 51664], "temperature": 0.0, "avg_logprob": -0.18664322728696076, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.0015963178593665361}, {"id": 424, "seek": 272680, "start": 2726.8, "end": 2734.8, "text": " And we mostly perturb when we kind of wash out this high frequency content of the image for small t's.", "tokens": [50364, 400, 321, 5240, 40468, 562, 321, 733, 295, 5675, 484, 341, 1090, 7893, 2701, 295, 264, 3256, 337, 1359, 256, 311, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16379679333079944, "compression_ratio": 1.7981220657276995, "no_speech_prob": 0.0014316434971988201}, {"id": 425, "seek": 272680, "start": 2734.8, "end": 2754.8, "text": " And then for large t's, because alpha bar t, this coefficient here is almost zero. So what happens is that you now push down all the frequency content, so you also push down the low frequency response of the image, and you wash away all the kind of frequency content of the image.", "tokens": [50764, 400, 550, 337, 2416, 256, 311, 11, 570, 8961, 2159, 256, 11, 341, 17619, 510, 307, 1920, 4018, 13, 407, 437, 2314, 307, 300, 291, 586, 2944, 760, 439, 264, 7893, 2701, 11, 370, 291, 611, 2944, 760, 264, 2295, 7893, 4134, 295, 264, 3256, 11, 293, 291, 5675, 1314, 439, 264, 733, 295, 7893, 2701, 295, 264, 3256, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16379679333079944, "compression_ratio": 1.7981220657276995, "no_speech_prob": 0.0014316434971988201}, {"id": 426, "seek": 275480, "start": 2754.8, "end": 2768.8, "text": " This basically shows that there's kind of a trade off in the forward process. In the forward process what happens is that the high frequency content is perturbed faster than the low frequency content.", "tokens": [50364, 639, 1936, 3110, 300, 456, 311, 733, 295, 257, 4923, 766, 294, 264, 2128, 1399, 13, 682, 264, 2128, 1399, 437, 2314, 307, 300, 264, 1090, 7893, 2701, 307, 13269, 374, 2883, 4663, 813, 264, 2295, 7893, 2701, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08570771033947285, "compression_ratio": 2.0495495495495497, "no_speech_prob": 0.002509585814550519}, {"id": 427, "seek": 275480, "start": 2768.8, "end": 2775.8, "text": " So at small t's, most of the low frequency content is not perturbed, it's mostly the high frequency content that is being perturbed.", "tokens": [51064, 407, 412, 1359, 256, 311, 11, 881, 295, 264, 2295, 7893, 2701, 307, 406, 13269, 374, 2883, 11, 309, 311, 5240, 264, 1090, 7893, 2701, 300, 307, 885, 13269, 374, 2883, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08570771033947285, "compression_ratio": 2.0495495495495497, "no_speech_prob": 0.002509585814550519}, {"id": 428, "seek": 275480, "start": 2775.8, "end": 2783.8, "text": " But eventually at the end of process is a time when we also completely get rid of the low frequency content of the image.", "tokens": [51414, 583, 4728, 412, 264, 917, 295, 1399, 307, 257, 565, 562, 321, 611, 2584, 483, 3973, 295, 264, 2295, 7893, 2701, 295, 264, 3256, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08570771033947285, "compression_ratio": 2.0495495495495497, "no_speech_prob": 0.002509585814550519}, {"id": 429, "seek": 278380, "start": 2784.8, "end": 2802.8, "text": " This is very important to also understand what happens in the reverse process, right, so because there's kind of trade off between content and detail, you can think of low frequency response is the main content of image and the high frequency response is just detail in that generation.", "tokens": [50414, 639, 307, 588, 1021, 281, 611, 1223, 437, 2314, 294, 264, 9943, 1399, 11, 558, 11, 370, 570, 456, 311, 733, 295, 4923, 766, 1296, 2701, 293, 2607, 11, 291, 393, 519, 295, 2295, 7893, 4134, 307, 264, 2135, 2701, 295, 3256, 293, 264, 1090, 7893, 4134, 307, 445, 2607, 294, 300, 5125, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16263914108276367, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.002280879532918334}, {"id": 430, "seek": 280280, "start": 2803.8, "end": 2821.8, "text": " These diffuser model kind of trades off between these in different steps, so you can think of when you're training a generative model, the reverse denoising model, for in the large teams, your denoising models becoming specialized at generating low frequency content of the image.", "tokens": [50414, 1981, 7593, 18088, 2316, 733, 295, 21287, 766, 1296, 613, 294, 819, 4439, 11, 370, 291, 393, 519, 295, 562, 291, 434, 3097, 257, 1337, 1166, 2316, 11, 264, 9943, 1441, 78, 3436, 2316, 11, 337, 294, 264, 2416, 5491, 11, 428, 1441, 78, 3436, 5245, 5617, 19813, 412, 17746, 2295, 7893, 2701, 295, 264, 3256, 13, 51314], "temperature": 0.0, "avg_logprob": -0.255616406925389, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.005725586786866188}, {"id": 431, "seek": 282180, "start": 2821.8, "end": 2826.8, "text": " So it's mostly the course content of the image is being generated large t's.", "tokens": [50364, 407, 309, 311, 5240, 264, 1164, 2701, 295, 264, 3256, 307, 885, 10833, 2416, 256, 311, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19710629516177708, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00016331003280356526}, {"id": 432, "seek": 282180, "start": 2826.8, "end": 2834.8, "text": " In a small t's, then your denoising model is becoming specialized in generating the high frequency content of the image.", "tokens": [50614, 682, 257, 1359, 256, 311, 11, 550, 428, 1441, 78, 3436, 2316, 307, 5617, 19813, 294, 17746, 264, 1090, 7893, 2701, 295, 264, 3256, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19710629516177708, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00016331003280356526}, {"id": 433, "seek": 282180, "start": 2834.8, "end": 2841.8, "text": " So most of the low level details are generated in the low, low t's, the small t's.", "tokens": [51014, 407, 881, 295, 264, 2295, 1496, 4365, 366, 10833, 294, 264, 2295, 11, 2295, 256, 311, 11, 264, 1359, 256, 311, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19710629516177708, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.00016331003280356526}, {"id": 434, "seek": 284180, "start": 2841.8, "end": 2856.8, "text": " This is also why the weighting of training objective becomes important, right, so because you have a model that is shared in different time steps and this model is responsible for generating course content versus low level details.", "tokens": [50364, 639, 307, 611, 983, 264, 3364, 278, 295, 3097, 10024, 3643, 1021, 11, 558, 11, 370, 570, 291, 362, 257, 2316, 300, 307, 5507, 294, 819, 565, 4439, 293, 341, 2316, 307, 6250, 337, 17746, 1164, 2701, 5717, 2295, 1496, 4365, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18361649305924124, "compression_ratio": 1.44375, "no_speech_prob": 0.003118241438642144}, {"id": 435, "seek": 285680, "start": 2856.8, "end": 2872.8, "text": " By reweighting this training objective, now we can kind of keep balance between how much we want to generate this course content that is visually very appealing usually, versus how much we want to generate the high frequency because that usually we ignore, we cannot necessarily observe them.", "tokens": [50364, 3146, 319, 12329, 278, 341, 3097, 10024, 11, 586, 321, 393, 733, 295, 1066, 4772, 1296, 577, 709, 321, 528, 281, 8460, 341, 1164, 2701, 300, 307, 19622, 588, 23842, 2673, 11, 5717, 577, 709, 321, 528, 281, 8460, 264, 1090, 7893, 570, 300, 2673, 321, 11200, 11, 321, 2644, 4725, 11441, 552, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2188012393904321, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.007803493645042181}, {"id": 436, "seek": 285680, "start": 2872.8, "end": 2880.8, "text": " And the weighting plays a key role in keeping this trade off, you're balancing this trade off.", "tokens": [51164, 400, 264, 3364, 278, 5749, 257, 2141, 3090, 294, 5145, 341, 4923, 766, 11, 291, 434, 22495, 341, 4923, 766, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2188012393904321, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.007803493645042181}, {"id": 437, "seek": 288080, "start": 2881.8, "end": 2893.8, "text": " So, so far I talked about the fusion was in general, now let's talk about the connections between the fusion was and the VAEs, especially hierarchical VAEs.", "tokens": [50414, 407, 11, 370, 1400, 286, 2825, 466, 264, 23100, 390, 294, 2674, 11, 586, 718, 311, 751, 466, 264, 9271, 1296, 264, 23100, 390, 293, 264, 18527, 20442, 11, 2318, 35250, 804, 18527, 20442, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24253010027336352, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0012785253347828984}, {"id": 438, "seek": 288080, "start": 2893.8, "end": 2901.8, "text": " In hierarchical VAEs, which one of the examples can be like any VAE work I did a few years ago.", "tokens": [51014, 682, 35250, 804, 18527, 20442, 11, 597, 472, 295, 264, 5110, 393, 312, 411, 604, 18527, 36, 589, 286, 630, 257, 1326, 924, 2057, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24253010027336352, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.0012785253347828984}, {"id": 439, "seek": 290180, "start": 2901.8, "end": 2911.8, "text": " In hierarchical VAEs, we have this deterministic path, you can think of just a resonant that generates data at x at the end, this is just a generative model.", "tokens": [50364, 682, 35250, 804, 18527, 20442, 11, 321, 362, 341, 15957, 3142, 3100, 11, 291, 393, 519, 295, 445, 257, 12544, 394, 300, 23815, 1412, 412, 2031, 412, 264, 917, 11, 341, 307, 445, 257, 1337, 1166, 2316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21995637269146676, "compression_ratio": 1.991869918699187, "no_speech_prob": 0.010452382266521454}, {"id": 440, "seek": 290180, "start": 2911.8, "end": 2930.8, "text": " In hierarchical VAEs we usually sample from noise and we inject to this deterministic path and then we go to second group sample from second group condition on the first group, and the after generating this noise we feed it to the, to the deterministic path and we keep doing this, we're just walking down in the hierarchical model.", "tokens": [50864, 682, 35250, 804, 18527, 20442, 321, 2673, 6889, 490, 5658, 293, 321, 10711, 281, 341, 15957, 3142, 3100, 293, 550, 321, 352, 281, 1150, 1594, 6889, 490, 1150, 1594, 4188, 322, 264, 700, 1594, 11, 293, 264, 934, 17746, 341, 5658, 321, 3154, 309, 281, 264, 11, 281, 264, 15957, 3142, 3100, 293, 321, 1066, 884, 341, 11, 321, 434, 445, 4494, 760, 294, 264, 35250, 804, 2316, 13, 51814], "temperature": 0.0, "avg_logprob": -0.21995637269146676, "compression_ratio": 1.991869918699187, "no_speech_prob": 0.010452382266521454}, {"id": 441, "seek": 293080, "start": 2930.8, "end": 2942.8, "text": " So the fusion models can be considered as hierarchical models as well, where these diffuse steps are just latent variables in this hierarchical model, the condition dependencies of course different.", "tokens": [50364, 407, 264, 23100, 5245, 393, 312, 4888, 382, 35250, 804, 5245, 382, 731, 11, 689, 613, 42165, 4439, 366, 445, 48994, 9102, 294, 341, 35250, 804, 2316, 11, 264, 4188, 36606, 295, 1164, 819, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1906718611717224, "compression_ratio": 1.6542553191489362, "no_speech_prob": 0.0007089350838214159}, {"id": 442, "seek": 293080, "start": 2942.8, "end": 2948.8, "text": " And here we're going to discuss like what are the main differences between the fusion was and hierarchical VAEs.", "tokens": [50964, 400, 510, 321, 434, 516, 281, 2248, 411, 437, 366, 264, 2135, 7300, 1296, 264, 23100, 390, 293, 35250, 804, 18527, 20442, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1906718611717224, "compression_ratio": 1.6542553191489362, "no_speech_prob": 0.0007089350838214159}, {"id": 443, "seek": 294880, "start": 2948.8, "end": 2967.8, "text": " One major difference is that the encoder in VAEs is often trained, whereas encoder, which is the forward diffusion in diffusion models is fixed we're not training the forward diffusion which is using a fixed diffusion process as encode.", "tokens": [50364, 1485, 2563, 2649, 307, 300, 264, 2058, 19866, 294, 18527, 20442, 307, 2049, 8895, 11, 9735, 2058, 19866, 11, 597, 307, 264, 2128, 25242, 294, 25242, 5245, 307, 6806, 321, 434, 406, 3097, 264, 2128, 25242, 597, 307, 1228, 257, 6806, 25242, 1399, 382, 2058, 1429, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1771087833479339, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0031216770876199007}, {"id": 444, "seek": 296780, "start": 2967.8, "end": 2977.8, "text": " That's one major difference. The second difference is that latent variables in hierarchical VAEs can have a different shape and different dimensionality compared to input images.", "tokens": [50364, 663, 311, 472, 2563, 2649, 13, 440, 1150, 2649, 307, 300, 48994, 9102, 294, 35250, 804, 18527, 20442, 393, 362, 257, 819, 3909, 293, 819, 10139, 1860, 5347, 281, 4846, 5267, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15847042150664747, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.006186170037835836}, {"id": 445, "seek": 296780, "start": 2977.8, "end": 2986.8, "text": " Whereas in diffusion models we assume that all the intermediate variables have the same dimension as input data.", "tokens": [50864, 13813, 294, 25242, 5245, 321, 6552, 300, 439, 264, 19376, 9102, 362, 264, 912, 10139, 382, 4846, 1412, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15847042150664747, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.006186170037835836}, {"id": 446, "seek": 298680, "start": 2987.8, "end": 2997.8, "text": " The third major difference is that in diffusion models, if you think of the noisy model as a generative model, this is actually shared across different steps.", "tokens": [50414, 440, 2636, 2563, 2649, 307, 300, 294, 25242, 5245, 11, 498, 291, 519, 295, 264, 24518, 2316, 382, 257, 1337, 1166, 2316, 11, 341, 307, 767, 5507, 2108, 819, 4439, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1579004634510387, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.001168356859125197}, {"id": 447, "seek": 298680, "start": 2997.8, "end": 3006.8, "text": " Whereas in hierarchical VAEs, we actually don't make an assumption, we don't share any component in this hierarchical structure, usually.", "tokens": [50914, 13813, 294, 35250, 804, 18527, 20442, 11, 321, 767, 500, 380, 652, 364, 15302, 11, 321, 500, 380, 2073, 604, 6542, 294, 341, 35250, 804, 3877, 11, 2673, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1579004634510387, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.001168356859125197}, {"id": 448, "seek": 300680, "start": 3006.8, "end": 3021.8, "text": " In hierarchical VAEs, we usually train these models using variational bond, whereas when we're training diffusion models, we're using some different rebating of variational bond in order to drive the training objective of diffusion.", "tokens": [50364, 682, 35250, 804, 18527, 20442, 11, 321, 2673, 3847, 613, 5245, 1228, 3034, 1478, 6086, 11, 9735, 562, 321, 434, 3097, 25242, 5245, 11, 321, 434, 1228, 512, 819, 12970, 990, 295, 3034, 1478, 6086, 294, 1668, 281, 3332, 264, 3097, 10024, 295, 25242, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1671401546114967, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0012836790410801768}, {"id": 449, "seek": 300680, "start": 3021.8, "end": 3032.8, "text": " So even though these two are related, they're not exactly the same. There are some trade-offs that occur when we are rebating the variational bond.", "tokens": [51114, 407, 754, 1673, 613, 732, 366, 4077, 11, 436, 434, 406, 2293, 264, 912, 13, 821, 366, 512, 4923, 12, 19231, 300, 5160, 562, 321, 366, 12970, 990, 264, 3034, 1478, 6086, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1671401546114967, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.0012836790410801768}, {"id": 450, "seek": 303280, "start": 3032.8, "end": 3039.8, "text": " So this brings me to the last slide. So in this part, I reviewed the noise and diffusion probabilistic models.", "tokens": [50364, 407, 341, 5607, 385, 281, 264, 1036, 4137, 13, 407, 294, 341, 644, 11, 286, 18429, 264, 5658, 293, 25242, 31959, 3142, 5245, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09219910159255519, "compression_ratio": 1.6827956989247312, "no_speech_prob": 0.00039175900747068226}, {"id": 451, "seek": 303280, "start": 3039.8, "end": 3053.8, "text": " I showed how these models are just simply trained by sampling from forward diffusion process and training a noisy model that simply predicts the noise that was used in order to generate diffuse samples.", "tokens": [50714, 286, 4712, 577, 613, 5245, 366, 445, 2935, 8895, 538, 21179, 490, 2128, 25242, 1399, 293, 3097, 257, 24518, 2316, 300, 2935, 6069, 82, 264, 5658, 300, 390, 1143, 294, 1668, 281, 8460, 42165, 10938, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09219910159255519, "compression_ratio": 1.6827956989247312, "no_speech_prob": 0.00039175900747068226}, {"id": 452, "seek": 305380, "start": 3053.8, "end": 3064.8, "text": " We discussed these models from different perspectives. We saw what happens to data as you go in the, what happens to images as you go forward in the diffusion process.", "tokens": [50364, 492, 7152, 613, 5245, 490, 819, 16766, 13, 492, 1866, 437, 2314, 281, 1412, 382, 291, 352, 294, 264, 11, 437, 2314, 281, 5267, 382, 291, 352, 2128, 294, 264, 25242, 1399, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1268514633178711, "compression_ratio": 2.012422360248447, "no_speech_prob": 0.002249592449516058}, {"id": 453, "seek": 305380, "start": 3064.8, "end": 3074.8, "text": " We also discussed what happens to data distribution in the forward process. We saw how data distribution becomes smoother and smoother in forward diffusion.", "tokens": [50914, 492, 611, 7152, 437, 2314, 281, 1412, 7316, 294, 264, 2128, 1399, 13, 492, 1866, 577, 1412, 7316, 3643, 28640, 293, 28640, 294, 2128, 25242, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1268514633178711, "compression_ratio": 2.012422360248447, "no_speech_prob": 0.002249592449516058}, {"id": 454, "seek": 307480, "start": 3074.8, "end": 3091.8, "text": " But of course, like any other deep learning framework, the devil is in the details, the network architecture, objective rebating, or even diffusion parameters play a key role in getting good high-quality results with diffusion models.", "tokens": [50364, 583, 295, 1164, 11, 411, 604, 661, 2452, 2539, 8388, 11, 264, 13297, 307, 294, 264, 4365, 11, 264, 3209, 9482, 11, 10024, 12970, 990, 11, 420, 754, 25242, 9834, 862, 257, 2141, 3090, 294, 1242, 665, 1090, 12, 11286, 3542, 365, 25242, 5245, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09834947391432158, "compression_ratio": 1.4625, "no_speech_prob": 0.008973758667707443}, {"id": 455, "seek": 309180, "start": 3091.8, "end": 3101.8, "text": " So if you're interested in knowing more about important design decisions that actually play a role in getting good diffusion models, I would encourage you to check this paper by other colleagues,", "tokens": [50364, 407, 498, 291, 434, 3102, 294, 5276, 544, 466, 1021, 1715, 5327, 300, 767, 862, 257, 3090, 294, 1242, 665, 25242, 5245, 11, 286, 576, 5373, 291, 281, 1520, 341, 3035, 538, 661, 7734, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1332393059363732, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.012008188292384148}, {"id": 456, "seek": 310180, "start": 3101.8, "end": 3121.8, "text": " called Elucidating the Design Space of Diffusion-Based Genetic Models by Karas Etal. And this paper discusses important design decisions and how they play a role in getting good diffusion models.", "tokens": [50364, 1219, 2699, 1311, 327, 990, 264, 12748, 8705, 295, 413, 3661, 5704, 12, 33, 1937, 3632, 3532, 6583, 1625, 538, 8009, 296, 3790, 304, 13, 400, 341, 3035, 2248, 279, 1021, 1715, 5327, 293, 577, 436, 862, 257, 3090, 294, 1242, 665, 25242, 5245, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2541057625595404, "compression_ratio": 1.3356164383561644, "no_speech_prob": 0.011708111502230167}, {"id": 457, "seek": 312180, "start": 3121.8, "end": 3137.8, "text": " So with that in mind, I'd like to pass the mic and with you to my dear friend and colleague Karsten to talk about score-based genetic modeling with differential courses.", "tokens": [50364, 407, 365, 300, 294, 1575, 11, 286, 1116, 411, 281, 1320, 264, 3123, 293, 365, 291, 281, 452, 6875, 1277, 293, 13532, 8009, 6266, 281, 751, 466, 6175, 12, 6032, 12462, 15983, 365, 15756, 7712, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14634031149057242, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.006280565168708563}, {"id": 458, "seek": 312180, "start": 3137.8, "end": 3147.8, "text": " Hello everyone, I'm Karsten, and I will now talk about score-based genetic modeling with differential equations.", "tokens": [51164, 2425, 1518, 11, 286, 478, 8009, 6266, 11, 293, 286, 486, 586, 751, 466, 6175, 12, 6032, 12462, 15983, 365, 15756, 11787, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14634031149057242, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.006280565168708563}, {"id": 459, "seek": 314780, "start": 3147.8, "end": 3154.8, "text": " In order to get started, let us actually consider the diffusion process that Arash already introduced in his part.", "tokens": [50364, 682, 1668, 281, 483, 1409, 11, 718, 505, 767, 1949, 264, 25242, 1399, 300, 1587, 1299, 1217, 7268, 294, 702, 644, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20187653690935617, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.026649078354239464}, {"id": 460, "seek": 314780, "start": 3154.8, "end": 3161.8, "text": " This diffusion process is defined through this Gaussian transition kernel of this form.", "tokens": [50714, 639, 25242, 1399, 307, 7642, 807, 341, 39148, 6034, 28256, 295, 341, 1254, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20187653690935617, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.026649078354239464}, {"id": 461, "seek": 314780, "start": 3161.8, "end": 3168.8, "text": " But now let us consider the limit of many, many small steps, and each step being very, very tiny.", "tokens": [51064, 583, 586, 718, 505, 1949, 264, 4948, 295, 867, 11, 867, 1359, 4439, 11, 293, 1184, 1823, 885, 588, 11, 588, 5870, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20187653690935617, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.026649078354239464}, {"id": 462, "seek": 314780, "start": 3168.8, "end": 3174.8, "text": " So how does sampling from this diffusion process and in practice look like?", "tokens": [51414, 407, 577, 775, 21179, 490, 341, 25242, 1399, 293, 294, 3124, 574, 411, 30, 51714], "temperature": 0.0, "avg_logprob": -0.20187653690935617, "compression_ratio": 1.6785714285714286, "no_speech_prob": 0.026649078354239464}, {"id": 463, "seek": 317480, "start": 3174.8, "end": 3191.8, "text": " So from this Gaussian transition kernel, we can just do essentially the parametrization trick, and we take the xt minus one, we scale it down by this one minus beta t square width term, and we add a little bit of noise from the standard normal distribution,", "tokens": [50364, 407, 490, 341, 39148, 6034, 28256, 11, 321, 393, 445, 360, 4476, 264, 6220, 302, 24959, 399, 4282, 11, 293, 321, 747, 264, 220, 734, 3175, 472, 11, 321, 4373, 309, 760, 538, 341, 472, 3175, 9861, 256, 3732, 11402, 1433, 11, 293, 321, 909, 257, 707, 857, 295, 5658, 490, 264, 3832, 2710, 7316, 11, 51214], "temperature": 0.0, "avg_logprob": -0.22049772235709178, "compression_ratio": 1.6284153005464481, "no_speech_prob": 0.024392394348978996}, {"id": 464, "seek": 317480, "start": 3191.8, "end": 3195.8, "text": " scaled by this square width beta t term.", "tokens": [51214, 36039, 538, 341, 3732, 11402, 9861, 256, 1433, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22049772235709178, "compression_ratio": 1.6284153005464481, "no_speech_prob": 0.024392394348978996}, {"id": 465, "seek": 319580, "start": 3195.8, "end": 3208.8, "text": " With beta t, we can actually interpret it as a step size essentially, so if beta t is zero, nothing happens, this term drops out, and also no rescaling of xt minus one happens.", "tokens": [50364, 2022, 9861, 256, 11, 321, 393, 767, 7302, 309, 382, 257, 1823, 2744, 4476, 11, 370, 498, 9861, 256, 307, 4018, 11, 1825, 2314, 11, 341, 1433, 11438, 484, 11, 293, 611, 572, 9610, 4270, 295, 220, 734, 3175, 472, 2314, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15863626297206096, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.013013937510550022}, {"id": 466, "seek": 319580, "start": 3208.8, "end": 3217.8, "text": " So let's make this a little bit more explicit and write beta t as this delta t times this function beta of t.", "tokens": [51014, 407, 718, 311, 652, 341, 257, 707, 857, 544, 13691, 293, 2464, 9861, 256, 382, 341, 8289, 256, 1413, 341, 2445, 9861, 295, 256, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15863626297206096, "compression_ratio": 1.580110497237569, "no_speech_prob": 0.013013937510550022}, {"id": 467, "seek": 321780, "start": 3217.8, "end": 3230.8, "text": " So beta t is explicitly our step size, and beta of t is now this time dependent function that allows us to have different step sizes along the diffusion process t.", "tokens": [50364, 407, 9861, 256, 307, 20803, 527, 1823, 2744, 11, 293, 9861, 295, 256, 307, 586, 341, 565, 12334, 2445, 300, 4045, 505, 281, 362, 819, 1823, 11602, 2051, 264, 25242, 1399, 256, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15656949653000127, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.004680763930082321}, {"id": 468, "seek": 321780, "start": 3230.8, "end": 3237.8, "text": " And now in this limit of many, many small tiny steps, it is delta t that goes to zero.", "tokens": [51014, 400, 586, 294, 341, 4948, 295, 867, 11, 867, 1359, 5870, 4439, 11, 309, 307, 8289, 256, 300, 1709, 281, 4018, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15656949653000127, "compression_ratio": 1.5337423312883436, "no_speech_prob": 0.004680763930082321}, {"id": 469, "seek": 323780, "start": 3237.8, "end": 3251.8, "text": " If delta t goes towards zero or tiny, we can actually tailor expand this square width expression here and obtain this equation at the bottom.", "tokens": [50364, 759, 8289, 256, 1709, 3030, 4018, 420, 5870, 11, 321, 393, 767, 33068, 5268, 341, 3732, 11402, 6114, 510, 293, 12701, 341, 5367, 412, 264, 2767, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19166204664442274, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.003323799464851618}, {"id": 470, "seek": 323780, "start": 3251.8, "end": 3253.8, "text": " I just copied that over here.", "tokens": [51064, 286, 445, 25365, 300, 670, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19166204664442274, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.003323799464851618}, {"id": 471, "seek": 323780, "start": 3253.8, "end": 3257.8, "text": " And it turns out that this equation has a particular form.", "tokens": [51164, 400, 309, 4523, 484, 300, 341, 5367, 575, 257, 1729, 1254, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19166204664442274, "compression_ratio": 1.513157894736842, "no_speech_prob": 0.003323799464851618}, {"id": 472, "seek": 325780, "start": 3257.8, "end": 3269.8, "text": " We can interpret this as some iterative update, like the new xt is given by the old xt plus some term that depends on xt itself.", "tokens": [50364, 492, 393, 7302, 341, 382, 512, 17138, 1166, 5623, 11, 411, 264, 777, 220, 734, 307, 2212, 538, 264, 1331, 220, 734, 1804, 512, 1433, 300, 5946, 322, 220, 734, 2564, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15094075202941895, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.005137718748301268}, {"id": 473, "seek": 325780, "start": 3269.8, "end": 3273.8, "text": " So this is just a small correction and some noise added.", "tokens": [50964, 407, 341, 307, 445, 257, 1359, 19984, 293, 512, 5658, 3869, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15094075202941895, "compression_ratio": 1.4122137404580153, "no_speech_prob": 0.005137718748301268}, {"id": 474, "seek": 327380, "start": 3273.8, "end": 3288.8, "text": " It turns out that this iterative update will correspond to a certain solution or a certain discretization of a stochastic differential equation, and in particular this stochastic differential equation.", "tokens": [50364, 467, 4523, 484, 300, 341, 17138, 1166, 5623, 486, 6805, 281, 257, 1629, 3827, 420, 257, 1629, 25656, 2144, 295, 257, 342, 8997, 2750, 15756, 5367, 11, 293, 294, 1729, 341, 342, 8997, 2750, 15756, 5367, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13531472043293277, "compression_ratio": 1.6209677419354838, "no_speech_prob": 0.003272285917773843}, {"id": 475, "seek": 328880, "start": 3288.8, "end": 3303.8, "text": " If I wanted to iteratively numerically solve this stochastic differential equation, for instance with an Euler-Maruama solver, then this is exactly the iterative scheme I would end up with.", "tokens": [50364, 759, 286, 1415, 281, 17138, 19020, 7866, 984, 5039, 341, 342, 8997, 2750, 15756, 5367, 11, 337, 5197, 365, 364, 462, 26318, 12, 44, 16870, 2404, 1404, 331, 11, 550, 341, 307, 2293, 264, 17138, 1166, 12232, 286, 576, 917, 493, 365, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12167611875032124, "compression_ratio": 1.5396039603960396, "no_speech_prob": 0.01321603637188673}, {"id": 476, "seek": 328880, "start": 3303.8, "end": 3305.8, "text": " Let us not get ahead of ourselves.", "tokens": [51114, 961, 505, 406, 483, 2286, 295, 4175, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12167611875032124, "compression_ratio": 1.5396039603960396, "no_speech_prob": 0.01321603637188673}, {"id": 477, "seek": 328880, "start": 3305.8, "end": 3311.8, "text": " I'm not sure if everybody who's listening here is an expert in differential equations.", "tokens": [51214, 286, 478, 406, 988, 498, 2201, 567, 311, 4764, 510, 307, 364, 5844, 294, 15756, 11787, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12167611875032124, "compression_ratio": 1.5396039603960396, "no_speech_prob": 0.01321603637188673}, {"id": 478, "seek": 331180, "start": 3311.8, "end": 3322.8, "text": " So let us do a one-slide crash course in differential equations, and let us start with ordinary differential equations, which are a little bit simpler than stochastic ones.", "tokens": [50364, 407, 718, 505, 360, 257, 472, 12, 10418, 482, 8252, 1164, 294, 15756, 11787, 11, 293, 718, 505, 722, 365, 10547, 15756, 11787, 11, 597, 366, 257, 707, 857, 18587, 813, 342, 8997, 2750, 2306, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11223363876342773, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.015653032809495926}, {"id": 479, "seek": 331180, "start": 3322.8, "end": 3327.8, "text": " Here is an ordinary differential equation that can be written in that form.", "tokens": [50914, 1692, 307, 364, 10547, 15756, 5367, 300, 393, 312, 3720, 294, 300, 1254, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11223363876342773, "compression_ratio": 1.710344827586207, "no_speech_prob": 0.015653032809495926}, {"id": 480, "seek": 332780, "start": 3327.8, "end": 3334.8, "text": " So this is now the state that we're interested in. This code, for example, the value of a pixel in an image.", "tokens": [50364, 407, 341, 307, 586, 264, 1785, 300, 321, 434, 3102, 294, 13, 639, 3089, 11, 337, 1365, 11, 264, 2158, 295, 257, 19261, 294, 364, 3256, 13, 50714], "temperature": 0.0, "avg_logprob": -0.16490969340006512, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.02841217629611492}, {"id": 481, "seek": 332780, "start": 3334.8, "end": 3342.8, "text": " And t is some continuous time variable that captures the time along which this state changes or evolves.", "tokens": [50714, 400, 256, 307, 512, 10957, 565, 7006, 300, 27986, 264, 565, 2051, 597, 341, 1785, 2962, 420, 43737, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16490969340006512, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.02841217629611492}, {"id": 482, "seek": 332780, "start": 3342.8, "end": 3351.8, "text": " And ultimately one is often interested in the evolution of this state x or this pixel value x of t.", "tokens": [51114, 400, 6284, 472, 307, 2049, 3102, 294, 264, 9303, 295, 341, 1785, 2031, 420, 341, 19261, 2158, 2031, 295, 256, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16490969340006512, "compression_ratio": 1.6648936170212767, "no_speech_prob": 0.02841217629611492}, {"id": 483, "seek": 335180, "start": 3351.8, "end": 3364.8, "text": " And that is not what we're given an ordinary differential equation. What we've given is an expression for the time derivative of dx to dt in the form of this function f.", "tokens": [50364, 400, 300, 307, 406, 437, 321, 434, 2212, 364, 10547, 15756, 5367, 13, 708, 321, 600, 2212, 307, 364, 6114, 337, 264, 565, 13760, 295, 30017, 281, 36423, 294, 264, 1254, 295, 341, 2445, 283, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16339170171859416, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0006360381958074868}, {"id": 484, "seek": 335180, "start": 3364.8, "end": 3368.8, "text": " So this code, for instance, will be a neural network.", "tokens": [51014, 407, 341, 3089, 11, 337, 5197, 11, 486, 312, 257, 18161, 3209, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16339170171859416, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0006360381958074868}, {"id": 485, "seek": 335180, "start": 3368.8, "end": 3370.8, "text": " So what does this mean?", "tokens": [51214, 407, 437, 775, 341, 914, 30, 51314], "temperature": 0.0, "avg_logprob": -0.16339170171859416, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0006360381958074868}, {"id": 486, "seek": 335180, "start": 3370.8, "end": 3375.8, "text": " This f essentially describes not x itself, but the change of x.", "tokens": [51314, 639, 283, 4476, 15626, 406, 2031, 2564, 11, 457, 264, 1319, 295, 2031, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16339170171859416, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0006360381958074868}, {"id": 487, "seek": 335180, "start": 3375.8, "end": 3379.8, "text": " So if you now look here in this graph at the bottom.", "tokens": [51564, 407, 498, 291, 586, 574, 510, 294, 341, 4295, 412, 264, 2767, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16339170171859416, "compression_ratio": 1.6106194690265487, "no_speech_prob": 0.0006360381958074868}, {"id": 488, "seek": 337980, "start": 3379.8, "end": 3388.8, "text": " So for point x, for a given time t, this f of x now describes the change. So in other words, we could look.", "tokens": [50364, 407, 337, 935, 2031, 11, 337, 257, 2212, 565, 256, 11, 341, 283, 295, 2031, 586, 15626, 264, 1319, 13, 407, 294, 661, 2283, 11, 321, 727, 574, 13, 50814], "temperature": 0.0, "avg_logprob": -0.179433544476827, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0015725470148026943}, {"id": 489, "seek": 337980, "start": 3388.8, "end": 3392.8, "text": " So this f essentially corresponds to an arrow in this graph.", "tokens": [50814, 407, 341, 283, 4476, 23249, 281, 364, 11610, 294, 341, 4295, 13, 51014], "temperature": 0.0, "avg_logprob": -0.179433544476827, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0015725470148026943}, {"id": 490, "seek": 337980, "start": 3392.8, "end": 3399.8, "text": " And if I now wanted to get x of t, I would just follow the arrows in this thing here.", "tokens": [51014, 400, 498, 286, 586, 1415, 281, 483, 2031, 295, 256, 11, 286, 576, 445, 1524, 264, 19669, 294, 341, 551, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.179433544476827, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0015725470148026943}, {"id": 491, "seek": 337980, "start": 3399.8, "end": 3408.8, "text": " So basically you just have to integrate up this differential equation following the arrows to get my final expression x of t.", "tokens": [51364, 407, 1936, 291, 445, 362, 281, 13365, 493, 341, 15756, 5367, 3480, 264, 19669, 281, 483, 452, 2572, 6114, 2031, 295, 256, 13, 51814], "temperature": 0.0, "avg_logprob": -0.179433544476827, "compression_ratio": 1.6740088105726871, "no_speech_prob": 0.0015725470148026943}, {"id": 492, "seek": 340880, "start": 3408.8, "end": 3410.8, "text": " And that's what I would have to do.", "tokens": [50364, 400, 300, 311, 437, 286, 576, 362, 281, 360, 13, 50464], "temperature": 0.0, "avg_logprob": -0.13597913446097537, "compression_ratio": 1.4602272727272727, "no_speech_prob": 0.001115648541599512}, {"id": 493, "seek": 340880, "start": 3410.8, "end": 3429.8, "text": " However, in practice, this f is often like highly complex nonlinear function, for instance, like a neural network, and solving this integral here analytically and following these field lines exactly is often not possible.", "tokens": [50464, 2908, 11, 294, 3124, 11, 341, 283, 307, 2049, 411, 5405, 3997, 2107, 28263, 2445, 11, 337, 5197, 11, 411, 257, 18161, 3209, 11, 293, 12606, 341, 11573, 510, 10783, 984, 293, 3480, 613, 2519, 3876, 2293, 307, 2049, 406, 1944, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13597913446097537, "compression_ratio": 1.4602272727272727, "no_speech_prob": 0.001115648541599512}, {"id": 494, "seek": 342980, "start": 3429.8, "end": 3436.8, "text": " In fact, one can solve this whole thing iteratively numerically in a stepwise fashion.", "tokens": [50364, 682, 1186, 11, 472, 393, 5039, 341, 1379, 551, 17138, 19020, 7866, 984, 294, 257, 1823, 3711, 6700, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2173633792183616, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.027987953275442123}, {"id": 495, "seek": 342980, "start": 3436.8, "end": 3448.8, "text": " So in that case, when we add some point x, we evaluate our neural network or our, well, our nonlinear function f, or function f, yeah, at this x and time t.", "tokens": [50714, 407, 294, 300, 1389, 11, 562, 321, 909, 512, 935, 2031, 11, 321, 13059, 527, 18161, 3209, 420, 527, 11, 731, 11, 527, 2107, 28263, 2445, 283, 11, 420, 2445, 283, 11, 1338, 11, 412, 341, 2031, 293, 565, 256, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2173633792183616, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.027987953275442123}, {"id": 496, "seek": 342980, "start": 3448.8, "end": 3455.8, "text": " And then we do like a small linear step in this direction and access to our old state x.", "tokens": [51314, 400, 550, 321, 360, 411, 257, 1359, 8213, 1823, 294, 341, 3513, 293, 2105, 281, 527, 1331, 1785, 2031, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2173633792183616, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.027987953275442123}, {"id": 497, "seek": 345580, "start": 3455.8, "end": 3461.8, "text": " Continue doing that again evaluate f and again update and so on and so forth.", "tokens": [50364, 24472, 884, 300, 797, 13059, 283, 293, 797, 5623, 293, 370, 322, 293, 370, 5220, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1975636100769043, "compression_ratio": 1.4657534246575343, "no_speech_prob": 0.0013666960876435041}, {"id": 498, "seek": 345580, "start": 3461.8, "end": 3468.8, "text": " So we have an approximation essentially to this analytical solution.", "tokens": [50664, 407, 321, 362, 364, 28023, 4476, 281, 341, 29579, 3827, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1975636100769043, "compression_ratio": 1.4657534246575343, "no_speech_prob": 0.0013666960876435041}, {"id": 499, "seek": 345580, "start": 3468.8, "end": 3474.8, "text": " So, yeah, and now there are also stochastic differential equations.", "tokens": [51014, 407, 11, 1338, 11, 293, 586, 456, 366, 611, 342, 8997, 2750, 15756, 11787, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1975636100769043, "compression_ratio": 1.4657534246575343, "no_speech_prob": 0.0013666960876435041}, {"id": 500, "seek": 347480, "start": 3474.8, "end": 3486.8, "text": " And once a little bit more complex, these stochastic differential equations now have an additional term system sigma of x and t times this term omega.", "tokens": [50364, 400, 1564, 257, 707, 857, 544, 3997, 11, 613, 342, 8997, 2750, 15756, 11787, 586, 362, 364, 4497, 1433, 1185, 12771, 295, 2031, 293, 256, 1413, 341, 1433, 10498, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2974763635086687, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.05743308365345001}, {"id": 501, "seek": 347480, "start": 3486.8, "end": 3488.8, "text": " So what is all this.", "tokens": [50964, 407, 437, 307, 439, 341, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2974763635086687, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.05743308365345001}, {"id": 502, "seek": 347480, "start": 3488.8, "end": 3492.8, "text": " First of all, Omega, this is called a Vina process.", "tokens": [51064, 2386, 295, 439, 11, 27645, 11, 341, 307, 1219, 257, 691, 1426, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2974763635086687, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.05743308365345001}, {"id": 503, "seek": 347480, "start": 3492.8, "end": 3496.8, "text": " And what this is in practice is really just Gaussian white noise.", "tokens": [51264, 400, 437, 341, 307, 294, 3124, 307, 534, 445, 39148, 2418, 5658, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2974763635086687, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.05743308365345001}, {"id": 504, "seek": 349680, "start": 3496.8, "end": 3506.8, "text": " What this means is that our DX of t or ODE equation now has this additional term, which is corresponds to noise injection.", "tokens": [50364, 708, 341, 1355, 307, 300, 527, 48817, 295, 256, 420, 422, 22296, 5367, 586, 575, 341, 4497, 1433, 11, 597, 307, 23249, 281, 5658, 22873, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2654665470123291, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.010813074186444283}, {"id": 505, "seek": 349680, "start": 3506.8, "end": 3513.8, "text": " And this noise is scaled by this standard deviation term essentially sigma of x t.", "tokens": [50864, 400, 341, 5658, 307, 36039, 538, 341, 3832, 25163, 1433, 4476, 12771, 295, 2031, 256, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2654665470123291, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.010813074186444283}, {"id": 506, "seek": 349680, "start": 3513.8, "end": 3516.8, "text": " And this has a name, this is a diffusion coefficient.", "tokens": [51214, 400, 341, 575, 257, 1315, 11, 341, 307, 257, 25242, 17619, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2654665470123291, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.010813074186444283}, {"id": 507, "seek": 349680, "start": 3516.8, "end": 3523.8, "text": " And in that context also the other term system here is called the drift coefficient.", "tokens": [51364, 400, 294, 300, 4319, 611, 264, 661, 1433, 1185, 510, 307, 1219, 264, 19699, 17619, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2654665470123291, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.010813074186444283}, {"id": 508, "seek": 352380, "start": 3523.8, "end": 3530.8, "text": " And we can see that these equations are sometimes written like this explicit form with derivative here.", "tokens": [50364, 400, 321, 393, 536, 300, 613, 11787, 366, 2171, 3720, 411, 341, 13691, 1254, 365, 13760, 510, 13, 50714], "temperature": 0.0, "avg_logprob": -0.30975123149592704, "compression_ratio": 1.8651162790697675, "no_speech_prob": 0.0053804004564881325}, {"id": 509, "seek": 352380, "start": 3530.8, "end": 3539.8, "text": " And sometimes also like this is written on the other side and it becomes a bit of a special expression for the Vina process.", "tokens": [50714, 400, 2171, 611, 411, 341, 307, 3720, 322, 264, 661, 1252, 293, 309, 3643, 257, 857, 295, 257, 2121, 6114, 337, 264, 691, 1426, 1399, 13, 51164], "temperature": 0.0, "avg_logprob": -0.30975123149592704, "compression_ratio": 1.8651162790697675, "no_speech_prob": 0.0053804004564881325}, {"id": 510, "seek": 352380, "start": 3539.8, "end": 3549.8, "text": " Sometimes this differential equations or stochastic differential equations here also written like this, but this essentially means the same thing for the sake of this talk.", "tokens": [51164, 4803, 341, 15756, 11787, 420, 342, 8997, 2750, 15756, 11787, 510, 611, 3720, 411, 341, 11, 457, 341, 4476, 1355, 264, 912, 551, 337, 264, 9717, 295, 341, 751, 13, 51664], "temperature": 0.0, "avg_logprob": -0.30975123149592704, "compression_ratio": 1.8651162790697675, "no_speech_prob": 0.0053804004564881325}, {"id": 511, "seek": 354980, "start": 3549.8, "end": 3561.8, "text": " And importantly, keep in mind this omega of t is essentially a Gaussian random variables and strong independent for each team looks like this.", "tokens": [50364, 400, 8906, 11, 1066, 294, 1575, 341, 10498, 295, 256, 307, 4476, 257, 39148, 4974, 9102, 293, 2068, 6695, 337, 1184, 1469, 1542, 411, 341, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19864140058818616, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0025888988748192787}, {"id": 512, "seek": 354980, "start": 3561.8, "end": 3567.8, "text": " So how does this now look like if I want to solve this stochastic differential equation.", "tokens": [50964, 407, 577, 775, 341, 586, 574, 411, 498, 286, 528, 281, 5039, 341, 342, 8997, 2750, 15756, 5367, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19864140058818616, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0025888988748192787}, {"id": 513, "seek": 354980, "start": 3567.8, "end": 3574.8, "text": " So for example, numerically, so it's a little bit similar to the iterative solution we had here.", "tokens": [51264, 407, 337, 1365, 11, 7866, 984, 11, 370, 309, 311, 257, 707, 857, 2531, 281, 264, 17138, 1166, 3827, 321, 632, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19864140058818616, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.0025888988748192787}, {"id": 514, "seek": 357480, "start": 3574.8, "end": 3583.8, "text": " So if I'm given a stage, I, I first updated corresponding to my updated corresponding to the drift coefficient, I evaluate that.", "tokens": [50364, 407, 498, 286, 478, 2212, 257, 3233, 11, 286, 11, 286, 700, 10588, 11760, 281, 452, 10588, 11760, 281, 264, 19699, 17619, 11, 286, 13059, 300, 13, 50814], "temperature": 0.0, "avg_logprob": -0.278343881879534, "compression_ratio": 1.9898989898989898, "no_speech_prob": 0.018816951662302017}, {"id": 515, "seek": 357480, "start": 3583.8, "end": 3586.8, "text": " So update a little bit with that direction.", "tokens": [50814, 407, 5623, 257, 707, 857, 365, 300, 3513, 13, 50964], "temperature": 0.0, "avg_logprob": -0.278343881879534, "compression_ratio": 1.9898989898989898, "no_speech_prob": 0.018816951662302017}, {"id": 516, "seek": 357480, "start": 3586.8, "end": 3601.8, "text": " But then I also evaluate the diffusion coefficient and add a little bit of noise that is proportional the strength of the noise is proportional to the diffusion coefficient, and additionally also to the to the time stamp.", "tokens": [50964, 583, 550, 286, 611, 13059, 264, 25242, 17619, 293, 909, 257, 707, 857, 295, 5658, 300, 307, 24969, 264, 3800, 295, 264, 5658, 307, 24969, 281, 264, 25242, 17619, 11, 293, 43181, 611, 281, 264, 281, 264, 565, 9921, 13, 51714], "temperature": 0.0, "avg_logprob": -0.278343881879534, "compression_ratio": 1.9898989898989898, "no_speech_prob": 0.018816951662302017}, {"id": 517, "seek": 360180, "start": 3601.8, "end": 3605.8, "text": " So if I do this, so each time I do this I made more different noise variables.", "tokens": [50364, 407, 498, 286, 360, 341, 11, 370, 1184, 565, 286, 360, 341, 286, 1027, 544, 819, 5658, 9102, 13, 50564], "temperature": 0.0, "avg_logprob": -0.19625644508851778, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.032548047602176666}, {"id": 518, "seek": 360180, "start": 3605.8, "end": 3613.8, "text": " So this means there is not a unique solution like there was in the ordinary differential equation case, but there is a lot of noise injected.", "tokens": [50564, 407, 341, 1355, 456, 307, 406, 257, 3845, 3827, 411, 456, 390, 294, 264, 10547, 15756, 5367, 1389, 11, 457, 456, 307, 257, 688, 295, 5658, 36967, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19625644508851778, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.032548047602176666}, {"id": 519, "seek": 360180, "start": 3613.8, "end": 3618.8, "text": " So if I do this multiple times, I may get slightly different trajectories.", "tokens": [50964, 407, 498, 286, 360, 341, 3866, 1413, 11, 286, 815, 483, 4748, 819, 18257, 2083, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19625644508851778, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.032548047602176666}, {"id": 520, "seek": 360180, "start": 3618.8, "end": 3626.8, "text": " So over all these trajectories still approximately follow this deterministic F function, but we have additional noise injection.", "tokens": [51214, 407, 670, 439, 613, 18257, 2083, 920, 10447, 1524, 341, 15957, 3142, 479, 2445, 11, 457, 321, 362, 4497, 5658, 22873, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19625644508851778, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.032548047602176666}, {"id": 521, "seek": 360180, "start": 3626.8, "end": 3629.8, "text": " So, yeah, this is how it may look like.", "tokens": [51614, 407, 11, 1338, 11, 341, 307, 577, 309, 815, 574, 411, 13, 51764], "temperature": 0.0, "avg_logprob": -0.19625644508851778, "compression_ratio": 1.8267716535433072, "no_speech_prob": 0.032548047602176666}, {"id": 522, "seek": 362980, "start": 3629.8, "end": 3639.8, "text": " So I may ask, is there now also like an analytical framework to instead, you know, describe this analytically like it was for the ordinary differential equation case.", "tokens": [50364, 407, 286, 815, 1029, 11, 307, 456, 586, 611, 411, 364, 29579, 8388, 281, 2602, 11, 291, 458, 11, 6786, 341, 10783, 984, 411, 309, 390, 337, 264, 10547, 15756, 5367, 1389, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18592584894058553, "compression_ratio": 1.6846153846153846, "no_speech_prob": 0.005549049004912376}, {"id": 523, "seek": 362980, "start": 3639.8, "end": 3643.8, "text": " So there is, but this is a little bit more involved.", "tokens": [50864, 407, 456, 307, 11, 457, 341, 307, 257, 707, 857, 544, 3288, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18592584894058553, "compression_ratio": 1.6846153846153846, "no_speech_prob": 0.005549049004912376}, {"id": 524, "seek": 362980, "start": 3643.8, "end": 3656.8, "text": " Because now we are not talking about one deterministic solution that we need to describe rather given one state in each at the beginning, we now have a probability distribution over possible states where we could land.", "tokens": [51064, 1436, 586, 321, 366, 406, 1417, 466, 472, 15957, 3142, 3827, 300, 321, 643, 281, 6786, 2831, 2212, 472, 1785, 294, 1184, 412, 264, 2863, 11, 321, 586, 362, 257, 8482, 7316, 670, 1944, 4368, 689, 321, 727, 2117, 13, 51714], "temperature": 0.0, "avg_logprob": -0.18592584894058553, "compression_ratio": 1.6846153846153846, "no_speech_prob": 0.005549049004912376}, {"id": 525, "seek": 365680, "start": 3656.8, "end": 3666.8, "text": " So the definition of these probability distribution that is described by the Fokker-Planck equation, but that is beyond the scope of this tutorial here.", "tokens": [50364, 407, 264, 7123, 295, 613, 8482, 7316, 300, 307, 7619, 538, 264, 479, 453, 5767, 12, 47, 8658, 547, 5367, 11, 457, 300, 307, 4399, 264, 11923, 295, 341, 7073, 510, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2552832970252404, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.0011332615977153182}, {"id": 526, "seek": 365680, "start": 3666.8, "end": 3676.8, "text": " Anyway, I think now you should also have some intuitions for not only ordinary differential equations, but also stochastic differential equations.", "tokens": [50864, 5684, 11, 286, 519, 586, 291, 820, 611, 362, 512, 16224, 626, 337, 406, 787, 10547, 15756, 11787, 11, 457, 611, 342, 8997, 2750, 15756, 11787, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2552832970252404, "compression_ratio": 1.5989304812834224, "no_speech_prob": 0.0011332615977153182}, {"id": 527, "seek": 367680, "start": 3676.8, "end": 3687.8, "text": " Now we can go back to our stochastic differential equation that we had here, and that actually describes the forward diffusion process of diffusion models.", "tokens": [50364, 823, 321, 393, 352, 646, 281, 527, 342, 8997, 2750, 15756, 5367, 300, 321, 632, 510, 11, 293, 300, 767, 15626, 264, 2128, 25242, 1399, 295, 25242, 5245, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09870534472995335, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.002182055963203311}, {"id": 528, "seek": 367680, "start": 3687.8, "end": 3698.8, "text": " So this is again just copy over the equation from the last slide here at the bottom. And this is now a visualization of how this whole thing looks like in practice more or less.", "tokens": [50914, 407, 341, 307, 797, 445, 5055, 670, 264, 5367, 490, 264, 1036, 4137, 510, 412, 264, 2767, 13, 400, 341, 307, 586, 257, 25801, 295, 577, 341, 1379, 551, 1542, 411, 294, 3124, 544, 420, 1570, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09870534472995335, "compression_ratio": 1.5857142857142856, "no_speech_prob": 0.002182055963203311}, {"id": 529, "seek": 369880, "start": 3698.8, "end": 3712.8, "text": " So let's go through that one by one. On the left hand side, we have some distribution here of x zero, this might be defined through an empirical data distribution or here we have this one dimensional toy distribution.", "tokens": [50364, 407, 718, 311, 352, 807, 300, 472, 538, 472, 13, 1282, 264, 1411, 1011, 1252, 11, 321, 362, 512, 7316, 510, 295, 2031, 4018, 11, 341, 1062, 312, 7642, 807, 364, 31886, 1412, 7316, 420, 510, 321, 362, 341, 472, 18795, 12058, 7316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.25179072393887286, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.019703209400177002}, {"id": 530, "seek": 369880, "start": 3712.8, "end": 3720.8, "text": " In a more realistic setting this may represent a distribution of images like images of these cats here.", "tokens": [51064, 682, 257, 544, 12465, 3287, 341, 815, 2906, 257, 7316, 295, 5267, 411, 5267, 295, 613, 11111, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.25179072393887286, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.019703209400177002}, {"id": 531, "seek": 372080, "start": 3720.8, "end": 3731.8, "text": " So now if we simulate this stochastic differential equation forward, we get this green trajectories and they evolve towards this standard normal prior distribution.", "tokens": [50364, 407, 586, 498, 321, 27817, 341, 342, 8997, 2750, 15756, 5367, 2128, 11, 321, 483, 341, 3092, 18257, 2083, 293, 436, 16693, 3030, 341, 3832, 2710, 4059, 7316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15309582816229927, "compression_ratio": 1.49375, "no_speech_prob": 0.0014772390713915229}, {"id": 532, "seek": 372080, "start": 3731.8, "end": 3737.8, "text": " And yeah, the images come progressively noisier and noisier as we do this.", "tokens": [50914, 400, 1338, 11, 264, 5267, 808, 46667, 572, 271, 811, 293, 572, 271, 811, 382, 321, 360, 341, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15309582816229927, "compression_ratio": 1.49375, "no_speech_prob": 0.0014772390713915229}, {"id": 533, "seek": 373780, "start": 3737.8, "end": 3747.8, "text": " So let's also look at the form of this equation and it's kind of intuitive. We see that this is updates or in the DX.", "tokens": [50364, 407, 718, 311, 611, 574, 412, 264, 1254, 295, 341, 5367, 293, 309, 311, 733, 295, 21769, 13, 492, 536, 300, 341, 307, 9205, 420, 294, 264, 48817, 13, 50864], "temperature": 0.0, "avg_logprob": -0.25271839882010844, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0899040624499321}, {"id": 534, "seek": 373780, "start": 3747.8, "end": 3757.8, "text": " This update direction, it's actually proportional to the negative of the state x we're in. So this means if we have a large pixel value for instance.", "tokens": [50864, 639, 5623, 3513, 11, 309, 311, 767, 24969, 281, 264, 3671, 295, 264, 1785, 2031, 321, 434, 294, 13, 407, 341, 1355, 498, 321, 362, 257, 2416, 19261, 2158, 337, 5197, 13, 51364], "temperature": 0.0, "avg_logprob": -0.25271839882010844, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.0899040624499321}, {"id": 535, "seek": 375780, "start": 3757.8, "end": 3769.8, "text": " It will pull us to go back towards zero. So direction is always is a negative direction corresponding to my, our x.", "tokens": [50364, 467, 486, 2235, 505, 281, 352, 646, 3030, 4018, 13, 407, 3513, 307, 1009, 307, 257, 3671, 3513, 11760, 281, 452, 11, 527, 2031, 13, 50964], "temperature": 0.0, "avg_logprob": -0.3212910909501333, "compression_ratio": 1.5, "no_speech_prob": 0.007336347363889217}, {"id": 536, "seek": 375780, "start": 3769.8, "end": 3779.8, "text": " On the other hand, while all our states are being pulled towards zero as I've just explained, and at the same time we're also injecting noise.", "tokens": [50964, 1282, 264, 661, 1011, 11, 1339, 439, 527, 4368, 366, 885, 7373, 3030, 4018, 382, 286, 600, 445, 8825, 11, 293, 412, 264, 912, 565, 321, 434, 611, 10711, 278, 5658, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3212910909501333, "compression_ratio": 1.5, "no_speech_prob": 0.007336347363889217}, {"id": 537, "seek": 377980, "start": 3779.8, "end": 3787.8, "text": " So this makes it intuitive that after a while of simulating this whole process, we end up with this distribution.", "tokens": [50364, 407, 341, 1669, 309, 21769, 300, 934, 257, 1339, 295, 1034, 12162, 341, 1379, 1399, 11, 321, 917, 493, 365, 341, 7316, 13, 50764], "temperature": 0.0, "avg_logprob": -0.24308372426916053, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.021595878526568413}, {"id": 538, "seek": 377980, "start": 3787.8, "end": 3803.8, "text": " It means that every single point basically completely converts itself to just plain noise where, yeah, with mean zero and certain variance.", "tokens": [50764, 467, 1355, 300, 633, 2167, 935, 1936, 2584, 38874, 2564, 281, 445, 11121, 5658, 689, 11, 1338, 11, 365, 914, 4018, 293, 1629, 21977, 13, 51564], "temperature": 0.0, "avg_logprob": -0.24308372426916053, "compression_ratio": 1.4709302325581395, "no_speech_prob": 0.021595878526568413}, {"id": 539, "seek": 380380, "start": 3804.8, "end": 3809.8, "text": " Here's another animation of that.", "tokens": [50414, 1692, 311, 1071, 9603, 295, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.20844819990255065, "compression_ratio": 1.5430463576158941, "no_speech_prob": 0.02884945459663868}, {"id": 540, "seek": 380380, "start": 3809.8, "end": 3824.8, "text": " Note that throughout this talk, we will make a lot of use of this image of this cat washes cat peanut. We do hope that it will become a little bit famous after this talk, but let's see how that goes.", "tokens": [50664, 11633, 300, 3710, 341, 751, 11, 321, 486, 652, 257, 688, 295, 764, 295, 341, 3256, 295, 341, 3857, 48616, 3857, 19209, 13, 492, 360, 1454, 300, 309, 486, 1813, 257, 707, 857, 4618, 934, 341, 751, 11, 457, 718, 311, 536, 577, 300, 1709, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20844819990255065, "compression_ratio": 1.5430463576158941, "no_speech_prob": 0.02884945459663868}, {"id": 541, "seek": 382480, "start": 3824.8, "end": 3836.8, "text": " Right. So, yeah, we have this forward diffusion sd with a drift term and diffusion term, one of them pulls towards the mode of the distribution the other one injects noise.", "tokens": [50364, 1779, 13, 407, 11, 1338, 11, 321, 362, 341, 2128, 25242, 262, 67, 365, 257, 19699, 1433, 293, 25242, 1433, 11, 472, 295, 552, 16982, 3030, 264, 4391, 295, 264, 7316, 264, 661, 472, 10711, 82, 5658, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2980634177603373, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.018526984378695488}, {"id": 542, "seek": 382480, "start": 3836.8, "end": 3849.8, "text": " It may be worth mentioning that in the diffusion model it which are also other differential equations have been used to define other types of diffusion processes, often just take a more general form like this.", "tokens": [50964, 467, 815, 312, 3163, 18315, 300, 294, 264, 25242, 2316, 309, 597, 366, 611, 661, 15756, 11787, 362, 668, 1143, 281, 6964, 661, 3467, 295, 25242, 7555, 11, 2049, 445, 747, 257, 544, 2674, 1254, 411, 341, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2980634177603373, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.018526984378695488}, {"id": 543, "seek": 384980, "start": 3849.8, "end": 3862.8, "text": " Yeah, I don't want to go into too much detail and in this talk we will stick to this equation for simplicity, but all the concepts that we tried in this tutorial also hold for other types of SDS.", "tokens": [50364, 865, 11, 286, 500, 380, 528, 281, 352, 666, 886, 709, 2607, 293, 294, 341, 751, 321, 486, 2897, 281, 341, 5367, 337, 25632, 11, 457, 439, 264, 10392, 300, 321, 3031, 294, 341, 7073, 611, 1797, 337, 661, 3467, 295, 318, 11844, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1819506287574768, "compression_ratio": 1.3732394366197183, "no_speech_prob": 0.002589594107121229}, {"id": 544, "seek": 386280, "start": 3862.8, "end": 3873.8, "text": " The only thing that is important is that these drift kernels and fusion terms here these are basically linear function of X.", "tokens": [50364, 440, 787, 551, 300, 307, 1021, 307, 300, 613, 19699, 23434, 1625, 293, 23100, 2115, 510, 613, 366, 1936, 8213, 2445, 295, 1783, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2684176233079698, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.006689724512398243}, {"id": 545, "seek": 386280, "start": 3873.8, "end": 3883.8, "text": " Otherwise, we couldn't solve for the probability distributions here, which reminds me the background this where this background in these.", "tokens": [50914, 10328, 11, 321, 2809, 380, 5039, 337, 264, 8482, 37870, 510, 11, 597, 12025, 385, 264, 3678, 341, 689, 341, 3678, 294, 613, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2684176233079698, "compression_ratio": 1.5057471264367817, "no_speech_prob": 0.006689724512398243}, {"id": 546, "seek": 388380, "start": 3883.8, "end": 3897.8, "text": " In these pictures here these animations, this actually defines the marginal different probability distribution of the few data, which is your multimodal, and then he becomes union model.", "tokens": [50364, 682, 613, 5242, 510, 613, 22868, 11, 341, 767, 23122, 264, 16885, 819, 8482, 7316, 295, 264, 1326, 1412, 11, 597, 307, 428, 32972, 378, 304, 11, 293, 550, 415, 3643, 11671, 2316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3354446467231302, "compression_ratio": 1.5970873786407767, "no_speech_prob": 0.0004726794722955674}, {"id": 547, "seek": 388380, "start": 3897.8, "end": 3906.8, "text": " So great, we have not talked about the forward diffusion process, but what about the reverse direction. So, which is necessary for generation.", "tokens": [51064, 407, 869, 11, 321, 362, 406, 2825, 466, 264, 2128, 25242, 1399, 11, 457, 437, 466, 264, 9943, 3513, 13, 407, 11, 597, 307, 4818, 337, 5125, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3354446467231302, "compression_ratio": 1.5970873786407767, "no_speech_prob": 0.0004726794722955674}, {"id": 548, "seek": 390680, "start": 3906.8, "end": 3912.8, "text": " So, can we also have like some differential equation that is quite sad.", "tokens": [50364, 407, 11, 393, 321, 611, 362, 411, 512, 15756, 5367, 300, 307, 1596, 4227, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3347554498789262, "compression_ratio": 1.3066666666666666, "no_speech_prob": 0.012211109511554241}, {"id": 549, "seek": 390680, "start": 3912.8, "end": 3914.8, "text": " It turns out yes.", "tokens": [50664, 467, 4523, 484, 2086, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3347554498789262, "compression_ratio": 1.3066666666666666, "no_speech_prob": 0.012211109511554241}, {"id": 550, "seek": 390680, "start": 3914.8, "end": 3925.8, "text": " There is this result described by an Anderson 1982 and then used in young songs like your paper last year.", "tokens": [50764, 821, 307, 341, 1874, 7619, 538, 364, 18768, 31352, 293, 550, 1143, 294, 2037, 5781, 411, 428, 3035, 1036, 1064, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3347554498789262, "compression_ratio": 1.3066666666666666, "no_speech_prob": 0.012211109511554241}, {"id": 551, "seek": 392580, "start": 3925.8, "end": 3942.8, "text": " So if there is a forward differential equation of this form for STD, then there is a corresponding reverse stochastic differential equation that once in the other direction, but tracks the exact same probability distribution.", "tokens": [50364, 407, 498, 456, 307, 257, 2128, 15756, 5367, 295, 341, 1254, 337, 4904, 35, 11, 550, 456, 307, 257, 11760, 9943, 342, 8997, 2750, 15756, 5367, 300, 1564, 294, 264, 661, 3513, 11, 457, 10218, 264, 1900, 912, 8482, 7316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2420191764831543, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.004328623414039612}, {"id": 552, "seek": 392580, "start": 3942.8, "end": 3949.8, "text": " So, really like the reverse direction, which generates data from noise, not noise from data.", "tokens": [51214, 407, 11, 534, 411, 264, 9943, 3513, 11, 597, 23815, 1412, 490, 5658, 11, 406, 5658, 490, 1412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2420191764831543, "compression_ratio": 1.6736842105263159, "no_speech_prob": 0.004328623414039612}, {"id": 553, "seek": 394980, "start": 3949.8, "end": 3955.8, "text": " So, and since we first generate the fusion STD, it looks like this.", "tokens": [50364, 407, 11, 293, 1670, 321, 700, 8460, 264, 23100, 4904, 35, 11, 309, 1542, 411, 341, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3290509117974175, "compression_ratio": 1.7458563535911602, "no_speech_prob": 0.0011155096581205726}, {"id": 554, "seek": 394980, "start": 3955.8, "end": 3958.8, "text": " And again has drift term and a diffusion term.", "tokens": [50664, 400, 797, 575, 19699, 1433, 293, 257, 25242, 1433, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3290509117974175, "compression_ratio": 1.7458563535911602, "no_speech_prob": 0.0011155096581205726}, {"id": 555, "seek": 394980, "start": 3958.8, "end": 3968.8, "text": " It's a diffusion term and the drift term look over all somewhat similar. So the diffusion term is the same thing. So this will be a process that again injects noise.", "tokens": [50814, 467, 311, 257, 25242, 1433, 293, 264, 19699, 1433, 574, 670, 439, 8344, 2531, 13, 407, 264, 25242, 1433, 307, 264, 912, 551, 13, 407, 341, 486, 312, 257, 1399, 300, 797, 10711, 82, 5658, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3290509117974175, "compression_ratio": 1.7458563535911602, "no_speech_prob": 0.0011155096581205726}, {"id": 556, "seek": 394980, "start": 3968.8, "end": 3972.8, "text": " This drift term also has this same.", "tokens": [51314, 639, 19699, 1433, 611, 575, 341, 912, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3290509117974175, "compression_ratio": 1.7458563535911602, "no_speech_prob": 0.0011155096581205726}, {"id": 557, "seek": 397280, "start": 3973.8, "end": 3978.8, "text": " X minus one half p to t of X term, but then there was this additional term.", "tokens": [50414, 1783, 3175, 472, 1922, 280, 281, 256, 295, 1783, 1433, 11, 457, 550, 456, 390, 341, 4497, 1433, 13, 50664], "temperature": 0.0, "avg_logprob": -0.30950853098993714, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.002980082994326949}, {"id": 558, "seek": 397280, "start": 3978.8, "end": 3981.8, "text": " This is this red term and that is very important.", "tokens": [50664, 639, 307, 341, 2182, 1433, 293, 300, 307, 588, 1021, 13, 50814], "temperature": 0.0, "avg_logprob": -0.30950853098993714, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.002980082994326949}, {"id": 559, "seek": 397280, "start": 3981.8, "end": 3991.8, "text": " So this is the gradient of the logarithm of the marginal used density probability density of the data.", "tokens": [50814, 407, 341, 307, 264, 16235, 295, 264, 41473, 32674, 295, 264, 16885, 1143, 10305, 8482, 10305, 295, 264, 1412, 13, 51314], "temperature": 0.0, "avg_logprob": -0.30950853098993714, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.002980082994326949}, {"id": 560, "seek": 397280, "start": 3991.8, "end": 3994.8, "text": " And this is known as a score function.", "tokens": [51314, 400, 341, 307, 2570, 382, 257, 6175, 2445, 13, 51464], "temperature": 0.0, "avg_logprob": -0.30950853098993714, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.002980082994326949}, {"id": 561, "seek": 399480, "start": 3995.8, "end": 4009.8, "text": " So this means, if you now had access to this object here like this score function, we could do data generation from random noise by just sampling or simulating this reverse diffusion STD.", "tokens": [50414, 407, 341, 1355, 11, 498, 291, 586, 632, 2105, 281, 341, 2657, 510, 411, 341, 6175, 2445, 11, 321, 727, 360, 1412, 5125, 490, 4974, 5658, 538, 445, 21179, 420, 1034, 12162, 341, 9943, 25242, 4904, 35, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20144728251865932, "compression_ratio": 1.375, "no_speech_prob": 0.00043724870192818344}, {"id": 562, "seek": 400980, "start": 4009.8, "end": 4017.8, "text": " So this in practice and what look like this, like I said, this reverse diffusion STD.", "tokens": [50364, 407, 341, 294, 3124, 293, 437, 574, 411, 341, 11, 411, 286, 848, 11, 341, 9943, 25242, 4904, 35, 13, 50764], "temperature": 0.0, "avg_logprob": -0.24183883666992187, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003798989928327501}, {"id": 563, "seek": 400980, "start": 4017.8, "end": 4029.8, "text": " It's really a diffusion process running in the other direction. And so, yeah, simulating this is generative modeling essentially.", "tokens": [50764, 467, 311, 534, 257, 25242, 1399, 2614, 294, 264, 661, 3513, 13, 400, 370, 11, 1338, 11, 1034, 12162, 341, 307, 1337, 1166, 15983, 4476, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24183883666992187, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003798989928327501}, {"id": 564, "seek": 400980, "start": 4029.8, "end": 4035.8, "text": " So, are we done. So, we can simulate this and generate data.", "tokens": [51364, 407, 11, 366, 321, 1096, 13, 407, 11, 321, 393, 27817, 341, 293, 8460, 1412, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24183883666992187, "compression_ratio": 1.550561797752809, "no_speech_prob": 0.0003798989928327501}, {"id": 565, "seek": 403580, "start": 4035.8, "end": 4038.8, "text": " Well, not quite.", "tokens": [50364, 1042, 11, 406, 1596, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2457384467124939, "compression_ratio": 1.5987261146496816, "no_speech_prob": 0.0011157143162563443}, {"id": 566, "seek": 403580, "start": 4038.8, "end": 4045.8, "text": " The question is, how do we actually get this score function.", "tokens": [50514, 440, 1168, 307, 11, 577, 360, 321, 767, 483, 341, 6175, 2445, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2457384467124939, "compression_ratio": 1.5987261146496816, "no_speech_prob": 0.0011157143162563443}, {"id": 567, "seek": 403580, "start": 4045.8, "end": 4055.8, "text": " We may have the naive idea, why not learn a neural network for the score function to approximate it, and then we can take this new network, we call this new network as data.", "tokens": [50864, 492, 815, 362, 264, 29052, 1558, 11, 983, 406, 1466, 257, 18161, 3209, 337, 264, 6175, 2445, 281, 30874, 309, 11, 293, 550, 321, 393, 747, 341, 777, 3209, 11, 321, 818, 341, 777, 3209, 382, 1412, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2457384467124939, "compression_ratio": 1.5987261146496816, "no_speech_prob": 0.0011157143162563443}, {"id": 568, "seek": 405580, "start": 4056.8, "end": 4061.8, "text": " And then we can take this network inside it and our reverse diffusion STD.", "tokens": [50414, 400, 550, 321, 393, 747, 341, 3209, 1854, 309, 293, 527, 9943, 25242, 4904, 35, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3053742420824268, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.006190051324665546}, {"id": 569, "seek": 405580, "start": 4061.8, "end": 4065.8, "text": " And, yeah, so we can simulate it and generate data.", "tokens": [50664, 400, 11, 1338, 11, 370, 321, 393, 27817, 309, 293, 8460, 1412, 13, 50864], "temperature": 0.0, "avg_logprob": -0.3053742420824268, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.006190051324665546}, {"id": 570, "seek": 405580, "start": 4065.8, "end": 4072.8, "text": " Something we could do, for instance, is draw a diffusion time along this diffusion process.", "tokens": [50864, 6595, 321, 727, 360, 11, 337, 5197, 11, 307, 2642, 257, 25242, 565, 2051, 341, 25242, 1399, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3053742420824268, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.006190051324665546}, {"id": 571, "seek": 405580, "start": 4072.8, "end": 4084.8, "text": " Now take like data, refused data from this point in the diffusion process samples is QFT of XT.", "tokens": [51214, 823, 747, 411, 1412, 11, 14654, 1412, 490, 341, 935, 294, 264, 25242, 1399, 10938, 307, 1249, 25469, 295, 1783, 51, 13, 51814], "temperature": 0.0, "avg_logprob": -0.3053742420824268, "compression_ratio": 1.5939086294416243, "no_speech_prob": 0.006190051324665546}, {"id": 572, "seek": 408480, "start": 4084.8, "end": 4096.8, "text": " So we take a neural network that takes us as input, maybe we also additionally give up the time and train this, maybe with some simple square two term, which we minimize.", "tokens": [50364, 407, 321, 747, 257, 18161, 3209, 300, 2516, 505, 382, 4846, 11, 1310, 321, 611, 43181, 976, 493, 264, 565, 293, 3847, 341, 11, 1310, 365, 512, 2199, 3732, 732, 1433, 11, 597, 321, 17522, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2775267616647189, "compression_ratio": 1.5654761904761905, "no_speech_prob": 0.0004441099299583584}, {"id": 573, "seek": 408480, "start": 4096.8, "end": 4105.8, "text": " We train it to predict this score of this diffuse data, this marginal score to diffuse data.", "tokens": [50964, 492, 3847, 309, 281, 6069, 341, 6175, 295, 341, 42165, 1412, 11, 341, 16885, 6175, 281, 42165, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2775267616647189, "compression_ratio": 1.5654761904761905, "no_speech_prob": 0.0004441099299583584}, {"id": 574, "seek": 410580, "start": 4105.8, "end": 4120.8, "text": " Well, great idea, but unfortunately that doesn't work, because the score of this marginal diffuse data is not tractable or more explicitly this diffuse data density itself QT of XT is not tractable.", "tokens": [50364, 1042, 11, 869, 1558, 11, 457, 7015, 300, 1177, 380, 589, 11, 570, 264, 6175, 295, 341, 16885, 42165, 1412, 307, 406, 24207, 712, 420, 544, 20803, 341, 42165, 1412, 10305, 2564, 1249, 51, 295, 1783, 51, 307, 406, 24207, 712, 13, 51114], "temperature": 0.0, "avg_logprob": -0.24308854706433355, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0019557808991521597}, {"id": 575, "seek": 410580, "start": 4120.8, "end": 4134.8, "text": " We don't have an analytic expression for that, which makes it to a different sense because, well, if we had we could just put NT for zero and get our data distribution that this is what we're interested in modeling in the first place.", "tokens": [51114, 492, 500, 380, 362, 364, 40358, 6114, 337, 300, 11, 597, 1669, 309, 281, 257, 819, 2020, 570, 11, 731, 11, 498, 321, 632, 321, 727, 445, 829, 43452, 337, 4018, 293, 483, 527, 1412, 7316, 300, 341, 307, 437, 321, 434, 3102, 294, 15983, 294, 264, 700, 1081, 13, 51814], "temperature": 0.0, "avg_logprob": -0.24308854706433355, "compression_ratio": 1.6590038314176245, "no_speech_prob": 0.0019557808991521597}, {"id": 576, "seek": 413480, "start": 4135.8, "end": 4137.8, "text": " So, too bad.", "tokens": [50414, 407, 11, 886, 1578, 13, 50514], "temperature": 0.0, "avg_logprob": -0.15925040774875218, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.0005111253121867776}, {"id": 577, "seek": 413480, "start": 4137.8, "end": 4147.8, "text": " But what we can do is something different, which is now known as denoising score matching. So what we had on the previous slide was just general score matching.", "tokens": [50514, 583, 437, 321, 393, 360, 307, 746, 819, 11, 597, 307, 586, 2570, 382, 1441, 78, 3436, 6175, 14324, 13, 407, 437, 321, 632, 322, 264, 3894, 4137, 390, 445, 2674, 6175, 14324, 13, 51014], "temperature": 0.0, "avg_logprob": -0.15925040774875218, "compression_ratio": 1.4065040650406504, "no_speech_prob": 0.0005111253121867776}, {"id": 578, "seek": 414780, "start": 4147.8, "end": 4157.8, "text": " So what we can do is, instead of considering this marginal distribution QFT given X, which corresponds to the full diffuse density.", "tokens": [50364, 407, 437, 321, 393, 360, 307, 11, 2602, 295, 8079, 341, 16885, 7316, 1249, 25469, 2212, 1783, 11, 597, 23249, 281, 264, 1577, 42165, 10305, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15072267195757696, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.025554271414875984}, {"id": 579, "seek": 414780, "start": 4157.8, "end": 4174.8, "text": " Let us consider like individual data points X zero and diffuse those. So instead, we consider the conditional density QFT of XT given X zero that distribution actually is tractable.", "tokens": [50864, 961, 505, 1949, 411, 2609, 1412, 2793, 1783, 4018, 293, 42165, 729, 13, 407, 2602, 11, 321, 1949, 264, 27708, 10305, 1249, 25469, 295, 1783, 51, 2212, 1783, 4018, 300, 7316, 767, 307, 24207, 712, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15072267195757696, "compression_ratio": 1.6051282051282052, "no_speech_prob": 0.025554271414875984}, {"id": 580, "seek": 417480, "start": 4174.8, "end": 4182.8, "text": " So that's why it's preserving stochastic differential equation, which is precisely this SDE that we find our forward diffusion process.", "tokens": [50364, 407, 300, 311, 983, 309, 311, 33173, 342, 8997, 2750, 15756, 5367, 11, 597, 307, 13402, 341, 14638, 36, 300, 321, 915, 527, 2128, 25242, 1399, 13, 50764], "temperature": 0.0, "avg_logprob": -0.29108785546344257, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0038235317915678024}, {"id": 581, "seek": 417480, "start": 4182.8, "end": 4203.8, "text": " For that case, this conditional density QFT of XT given one particular data point X zero, it has this expression, this form. So it is this normal distribution, where the mean is given by the initial point X zero that is now simply scaled down by some gamma of T, which", "tokens": [50764, 1171, 300, 1389, 11, 341, 27708, 10305, 1249, 25469, 295, 1783, 51, 2212, 472, 1729, 1412, 935, 1783, 4018, 11, 309, 575, 341, 6114, 11, 341, 1254, 13, 407, 309, 307, 341, 2710, 7316, 11, 689, 264, 914, 307, 2212, 538, 264, 5883, 935, 1783, 4018, 300, 307, 586, 2935, 36039, 760, 538, 512, 15546, 295, 314, 11, 597, 51814], "temperature": 0.0, "avg_logprob": -0.29108785546344257, "compression_ratio": 1.6422764227642277, "no_speech_prob": 0.0038235317915678024}, {"id": 582, "seek": 420380, "start": 4203.8, "end": 4217.8, "text": " is a function that starts at one and be cased towards zero. And then we also have some variance, which is the other way around, which starts at zero and grows towards one.", "tokens": [50364, 307, 257, 2445, 300, 3719, 412, 472, 293, 312, 269, 1937, 3030, 4018, 13, 400, 550, 321, 611, 362, 512, 21977, 11, 597, 307, 264, 661, 636, 926, 11, 597, 3719, 412, 4018, 293, 13156, 3030, 472, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22378481002081008, "compression_ratio": 1.425, "no_speech_prob": 0.00021651864517480135}, {"id": 583, "seek": 421780, "start": 4217.8, "end": 4227.8, "text": " So we can define all denoising score matching objective. So, again, we have, we draw a diffusion time T, we have an expectation over those.", "tokens": [50364, 407, 321, 393, 6964, 439, 1441, 78, 3436, 6175, 14324, 10024, 13, 407, 11, 797, 11, 321, 362, 11, 321, 2642, 257, 25242, 565, 314, 11, 321, 362, 364, 14334, 670, 729, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2792772124795353, "compression_ratio": 1.7239263803680982, "no_speech_prob": 0.039029479026794434}, {"id": 584, "seek": 421780, "start": 4227.8, "end": 4240.8, "text": " Then we draw a data sample X zero, one particular sample. Overall, we again have an expectation. Then we diffuse that particular data sample.", "tokens": [50864, 1396, 321, 2642, 257, 1412, 6889, 1783, 4018, 11, 472, 1729, 6889, 13, 18420, 11, 321, 797, 362, 364, 14334, 13, 1396, 321, 42165, 300, 1729, 1412, 6889, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2792772124795353, "compression_ratio": 1.7239263803680982, "no_speech_prob": 0.039029479026794434}, {"id": 585, "seek": 424080, "start": 4240.8, "end": 4267.8, "text": " And now we train it to predict the score of this one particular diffused data sample X zero. And yeah, now this is tractable, you know this is just a normal distribution so we can take the logarithm of this density of this expression for the density and also calculate the gradient.", "tokens": [50364, 400, 586, 321, 3847, 309, 281, 6069, 264, 6175, 295, 341, 472, 1729, 7593, 4717, 1412, 6889, 1783, 4018, 13, 400, 1338, 11, 586, 341, 307, 24207, 712, 11, 291, 458, 341, 307, 445, 257, 2710, 7316, 370, 321, 393, 747, 264, 41473, 32674, 295, 341, 10305, 295, 341, 6114, 337, 264, 10305, 293, 611, 8873, 264, 16235, 13, 51714], "temperature": 0.0, "avg_logprob": -0.32788518875364275, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.007457644212990999}, {"id": 586, "seek": 426780, "start": 4267.8, "end": 4287.8, "text": " Great. So, and now there's one very beautiful result. So after this expectation over the data, when we consider that, it turns out that this neural network then will still learn to approximate the score of the marginal data of the marginal diffuse", "tokens": [50364, 3769, 13, 407, 11, 293, 586, 456, 311, 472, 588, 2238, 1874, 13, 407, 934, 341, 14334, 670, 264, 1412, 11, 562, 321, 1949, 300, 11, 309, 4523, 484, 300, 341, 18161, 3209, 550, 486, 920, 1466, 281, 30874, 264, 6175, 295, 264, 16885, 1412, 295, 264, 16885, 42165, 51364], "temperature": 0.0, "avg_logprob": -0.18496002341216466, "compression_ratio": 1.5246913580246915, "no_speech_prob": 0.002215901156887412}, {"id": 587, "seek": 428780, "start": 4287.8, "end": 4301.8, "text": " distribution, which is exactly what we need. And this sort of makes intuitively sense, because this x t that we're feeding to the neural network, this could corresponds to this is noisy right noisy data.", "tokens": [50364, 7316, 11, 597, 307, 2293, 437, 321, 643, 13, 400, 341, 1333, 295, 1669, 46506, 2020, 11, 570, 341, 2031, 256, 300, 321, 434, 12919, 281, 264, 18161, 3209, 11, 341, 727, 23249, 281, 341, 307, 24518, 558, 24518, 1412, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21207586924235025, "compression_ratio": 1.45, "no_speech_prob": 0.02260666899383068}, {"id": 588, "seek": 430180, "start": 4301.8, "end": 4317.8, "text": " So this could corresponds to many possible different x zeros and many different possible like grant true scores that we we regressed was on practice neural network has to kind of average over those.", "tokens": [50364, 407, 341, 727, 23249, 281, 867, 1944, 819, 2031, 35193, 293, 867, 819, 1944, 411, 6386, 2074, 13444, 300, 321, 321, 1121, 3805, 390, 322, 3124, 18161, 3209, 575, 281, 733, 295, 4274, 670, 729, 13, 51164], "temperature": 0.0, "avg_logprob": -0.28242316246032717, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.0007094896282069385}, {"id": 589, "seek": 431780, "start": 4317.8, "end": 4333.8, "text": " And it turns out that, yeah, after this averaging considering the expectation, this neural network will still model the marginal as the score of the marginal diffuse data distribution. And this is exactly what we need.", "tokens": [50364, 400, 309, 4523, 484, 300, 11, 1338, 11, 934, 341, 47308, 8079, 264, 14334, 11, 341, 18161, 3209, 486, 920, 2316, 264, 16885, 382, 264, 6175, 295, 264, 16885, 42165, 1412, 7316, 13, 400, 341, 307, 2293, 437, 321, 643, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22623004411396228, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.004130836576223373}, {"id": 590, "seek": 431780, "start": 4333.8, "end": 4340.8, "text": " What we need in our decision model, or more specifically in the reverse genitive SDE for generation.", "tokens": [51164, 708, 321, 643, 294, 527, 3537, 2316, 11, 420, 544, 4682, 294, 264, 9943, 1049, 2187, 14638, 36, 337, 5125, 13, 51514], "temperature": 0.0, "avg_logprob": -0.22623004411396228, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.004130836576223373}, {"id": 591, "seek": 431780, "start": 4340.8, "end": 4344.8, "text": " So, this is great.", "tokens": [51514, 407, 11, 341, 307, 869, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22623004411396228, "compression_ratio": 1.5943396226415094, "no_speech_prob": 0.004130836576223373}, {"id": 592, "seek": 434480, "start": 4344.8, "end": 4352.8, "text": " So, yeah, in practice diffusion modeling basically boils down to learning this score function.", "tokens": [50364, 407, 11, 1338, 11, 294, 3124, 25242, 15983, 1936, 35049, 760, 281, 2539, 341, 6175, 2445, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20038730303446453, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0004801577015314251}, {"id": 593, "seek": 434480, "start": 4352.8, "end": 4360.8, "text": " Let us now talk about a few implementation details here and what people do in practice because that's also crucial.", "tokens": [50764, 961, 505, 586, 751, 466, 257, 1326, 11420, 4365, 510, 293, 437, 561, 360, 294, 3124, 570, 300, 311, 611, 11462, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20038730303446453, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0004801577015314251}, {"id": 594, "seek": 434480, "start": 4360.8, "end": 4364.8, "text": " This is again just copied the denoising score matching formula.", "tokens": [51164, 639, 307, 797, 445, 25365, 264, 1441, 78, 3436, 6175, 14324, 8513, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20038730303446453, "compression_ratio": 1.481081081081081, "no_speech_prob": 0.0004801577015314251}, {"id": 595, "seek": 436480, "start": 4364.8, "end": 4378.8, "text": " So, how do we actually sample these diffused data points. And so this is really just with time and just sampling again from this normal distribution here so we have gamma times our input x zero.", "tokens": [50364, 407, 11, 577, 360, 321, 767, 6889, 613, 7593, 4717, 1412, 2793, 13, 400, 370, 341, 307, 534, 445, 365, 565, 293, 445, 21179, 797, 490, 341, 2710, 7316, 510, 370, 321, 362, 15546, 1413, 527, 4846, 2031, 4018, 13, 51064], "temperature": 0.0, "avg_logprob": -0.24546490018329922, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.005639465991407633}, {"id": 596, "seek": 436480, "start": 4378.8, "end": 4385.8, "text": " And then we add noise to us that is scaled by the standard deviation sigma t.", "tokens": [51064, 400, 550, 321, 909, 5658, 281, 505, 300, 307, 36039, 538, 264, 3832, 25163, 12771, 256, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24546490018329922, "compression_ratio": 1.5195530726256983, "no_speech_prob": 0.005639465991407633}, {"id": 597, "seek": 438580, "start": 4385.8, "end": 4397.8, "text": " We explicitly write down the star function, how it now looks like. So it's a gradient of the logarithm of this conditional distribution is the given x zero.", "tokens": [50364, 492, 20803, 2464, 760, 264, 3543, 2445, 11, 577, 309, 586, 1542, 411, 13, 407, 309, 311, 257, 16235, 295, 264, 41473, 32674, 295, 341, 27708, 7316, 307, 264, 2212, 2031, 4018, 13, 50964], "temperature": 0.0, "avg_logprob": -0.35238222490277205, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.00926307961344719}, {"id": 598, "seek": 438580, "start": 4397.8, "end": 4404.8, "text": " Oh, this is a normal distribution. So this is basically some exponential times some stuff.", "tokens": [50964, 876, 11, 341, 307, 257, 2710, 7316, 13, 407, 341, 307, 1936, 512, 21510, 1413, 512, 1507, 13, 51314], "temperature": 0.0, "avg_logprob": -0.35238222490277205, "compression_ratio": 1.5060975609756098, "no_speech_prob": 0.00926307961344719}, {"id": 599, "seek": 440480, "start": 4404.8, "end": 4416.8, "text": " We have the logarithm and exponential drop out. There's also a, by the way, a normalization constant, but this does not depend on x so it's not important for the derivative.", "tokens": [50364, 492, 362, 264, 41473, 32674, 293, 21510, 3270, 484, 13, 821, 311, 611, 257, 11, 538, 264, 636, 11, 257, 2710, 2144, 5754, 11, 457, 341, 775, 406, 5672, 322, 2031, 370, 309, 311, 406, 1021, 337, 264, 13760, 13, 50964], "temperature": 0.0, "avg_logprob": -0.26829378532640863, "compression_ratio": 1.4602272727272727, "no_speech_prob": 0.018536675721406937}, {"id": 600, "seek": 440480, "start": 4416.8, "end": 4423.8, "text": " Anyway, so you advise at this time. So now we can take the gradient of system here.", "tokens": [50964, 5684, 11, 370, 291, 18312, 412, 341, 565, 13, 407, 586, 321, 393, 747, 264, 16235, 295, 1185, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.26829378532640863, "compression_ratio": 1.4602272727272727, "no_speech_prob": 0.018536675721406937}, {"id": 601, "seek": 442380, "start": 4424.8, "end": 4434.8, "text": " Now here for x t. This is like the, the diffuse data point that we sampled, we can actually insert this expression here.", "tokens": [50414, 823, 510, 337, 2031, 256, 13, 639, 307, 411, 264, 11, 264, 42165, 1412, 935, 300, 321, 3247, 15551, 11, 321, 393, 767, 8969, 341, 6114, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.28675058152940536, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.004005537834018469}, {"id": 602, "seek": 442380, "start": 4434.8, "end": 4446.8, "text": " Then we get that, but now it turns out all these terms they cancel out this gamma t x zeroes, and also one of those signals, and what we left with a step.", "tokens": [50914, 1396, 321, 483, 300, 11, 457, 586, 309, 4523, 484, 439, 613, 2115, 436, 10373, 484, 341, 15546, 256, 2031, 4018, 279, 11, 293, 611, 472, 295, 729, 12354, 11, 293, 437, 321, 1411, 365, 257, 1823, 13, 51514], "temperature": 0.0, "avg_logprob": -0.28675058152940536, "compression_ratio": 1.5277777777777777, "no_speech_prob": 0.004005537834018469}, {"id": 603, "seek": 444680, "start": 4446.8, "end": 4465.8, "text": " It's interesting, because this means that the score function. It's basically just the, the noise values that we introduced doing the we permit rest sampling of our diffused data, like, minus that noise value and scale with the inverse standard deviation from the fusion.", "tokens": [50364, 467, 311, 1880, 11, 570, 341, 1355, 300, 264, 6175, 2445, 13, 467, 311, 1936, 445, 264, 11, 264, 5658, 4190, 300, 321, 7268, 884, 264, 321, 13423, 1472, 21179, 295, 527, 7593, 4717, 1412, 11, 411, 11, 3175, 300, 5658, 2158, 293, 4373, 365, 264, 17340, 3832, 25163, 490, 264, 23100, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3905530987363873, "compression_ratio": 1.560846560846561, "no_speech_prob": 0.015410896390676498}, {"id": 604, "seek": 444680, "start": 4465.8, "end": 4468.8, "text": " But still, this is cool.", "tokens": [51314, 583, 920, 11, 341, 307, 1627, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3905530987363873, "compression_ratio": 1.560846560846561, "no_speech_prob": 0.015410896390676498}, {"id": 605, "seek": 446880, "start": 4468.8, "end": 4476.8, "text": " So, this maybe also suggests us how we should choose our neural network and how we should parameterize this.", "tokens": [50364, 407, 11, 341, 1310, 611, 13409, 505, 577, 321, 820, 2826, 527, 18161, 3209, 293, 577, 321, 820, 13075, 1125, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16438935508190747, "compression_ratio": 1.6479591836734695, "no_speech_prob": 0.0019558004569262266}, {"id": 606, "seek": 446880, "start": 4476.8, "end": 4493.8, "text": " More specifically, for instance, we can take this new network and define it by like some, yeah, some other neural networks times minus one and divided by the standard deviation, which is inspired by this salt here.", "tokens": [50764, 5048, 4682, 11, 337, 5197, 11, 321, 393, 747, 341, 777, 3209, 293, 6964, 309, 538, 411, 512, 11, 1338, 11, 512, 661, 18161, 9590, 1413, 3175, 472, 293, 6666, 538, 264, 3832, 25163, 11, 597, 307, 7547, 538, 341, 5139, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16438935508190747, "compression_ratio": 1.6479591836734695, "no_speech_prob": 0.0019558004569262266}, {"id": 607, "seek": 449380, "start": 4493.8, "end": 4506.8, "text": " So if we insert both of the expression for the ground to a score, which is really just this noise value, and also this neural network parameterization, what we left with is this objective here at the bottom.", "tokens": [50364, 407, 498, 321, 8969, 1293, 295, 264, 6114, 337, 264, 2727, 281, 257, 6175, 11, 597, 307, 534, 445, 341, 5658, 2158, 11, 293, 611, 341, 18161, 3209, 13075, 2144, 11, 437, 321, 1411, 365, 307, 341, 10024, 510, 412, 264, 2767, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24202990024647814, "compression_ratio": 1.4375, "no_speech_prob": 0.0025096177123486996}, {"id": 608, "seek": 450680, "start": 4506.8, "end": 4525.8, "text": " No, this is interesting. So this means, if I choose this parameterization, our neural network epsilon that amount is basically tasked with predicting noise values epsilon, which are really just the noise values that were used to perturb our data.", "tokens": [50364, 883, 11, 341, 307, 1880, 13, 407, 341, 1355, 11, 498, 286, 2826, 341, 13075, 2144, 11, 527, 18161, 3209, 17889, 300, 2372, 307, 1936, 38621, 365, 32884, 5658, 4190, 17889, 11, 597, 366, 534, 445, 264, 5658, 4190, 300, 645, 1143, 281, 40468, 527, 1412, 13, 51314], "temperature": 0.0, "avg_logprob": -0.24244239283543007, "compression_ratio": 1.4819277108433735, "no_speech_prob": 0.0024332895409315825}, {"id": 609, "seek": 452580, "start": 4525.8, "end": 4530.8, "text": " This also makes it kind of intuitive for this is called denoising score matching.", "tokens": [50364, 639, 611, 1669, 309, 733, 295, 21769, 337, 341, 307, 1219, 1441, 78, 3436, 6175, 14324, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2509895324707031, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.0014100157422944903}, {"id": 610, "seek": 452580, "start": 4530.8, "end": 4547.8, "text": " Because if our neural network can be nice, can predict those noise values that were used for perturbation, then yeah, we can be nice and reconstruct the original data point x zero.", "tokens": [50614, 1436, 498, 527, 18161, 3209, 393, 312, 1481, 11, 393, 6069, 729, 5658, 4190, 300, 645, 1143, 337, 40468, 399, 11, 550, 1338, 11, 321, 393, 312, 1481, 293, 31499, 264, 3380, 1412, 935, 2031, 4018, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2509895324707031, "compression_ratio": 1.4802259887005649, "no_speech_prob": 0.0014100157422944903}, {"id": 611, "seek": 454780, "start": 4547.8, "end": 4551.8, "text": " So there's another implementation detail here.", "tokens": [50364, 407, 456, 311, 1071, 11420, 2607, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2845189571380615, "compression_ratio": 1.3724137931034484, "no_speech_prob": 0.0009693076717667282}, {"id": 612, "seek": 454780, "start": 4551.8, "end": 4563.8, "text": " So, I have kind of arbitrarily motivated that we can use this squared to turn to perform, you know, I think score matching and to regress this function.", "tokens": [50564, 407, 11, 286, 362, 733, 295, 19071, 3289, 14515, 300, 321, 393, 764, 341, 8889, 281, 1261, 281, 2042, 11, 291, 458, 11, 286, 519, 6175, 14324, 293, 281, 1121, 735, 341, 2445, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2845189571380615, "compression_ratio": 1.3724137931034484, "no_speech_prob": 0.0009693076717667282}, {"id": 613, "seek": 456380, "start": 4563.8, "end": 4571.8, "text": " And to give different weights to this L2 term to this L2 loss for different points along the diffusion process.", "tokens": [50364, 400, 281, 976, 819, 17443, 281, 341, 441, 17, 1433, 281, 341, 441, 17, 4470, 337, 819, 2793, 2051, 264, 25242, 1399, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22842711494082496, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.020941436290740967}, {"id": 614, "seek": 456380, "start": 4571.8, "end": 4581.8, "text": " Keep in mind that this is one neural network that just as input gets a noisy state and tea, it's the same neural network for all teeth along the diffusion process.", "tokens": [50764, 5527, 294, 1575, 300, 341, 307, 472, 18161, 3209, 300, 445, 382, 4846, 2170, 257, 24518, 1785, 293, 5817, 11, 309, 311, 264, 912, 18161, 3209, 337, 439, 7798, 2051, 264, 25242, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22842711494082496, "compression_ratio": 1.729559748427673, "no_speech_prob": 0.020941436290740967}, {"id": 615, "seek": 458180, "start": 4581.8, "end": 4597.8, "text": " So, maybe we want to specialize in that with a little bit more for large times along the diffusion process of small times or something like this, and give like different weights to this objective for different times along the diffusion process.", "tokens": [50364, 407, 11, 1310, 321, 528, 281, 37938, 294, 300, 365, 257, 707, 857, 544, 337, 2416, 1413, 2051, 264, 25242, 1399, 295, 1359, 1413, 420, 746, 411, 341, 11, 293, 976, 411, 819, 17443, 281, 341, 10024, 337, 819, 1413, 2051, 264, 25242, 1399, 13, 51164], "temperature": 0.0, "avg_logprob": -0.27413798135424416, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.0002652259136084467}, {"id": 616, "seek": 458180, "start": 4597.8, "end": 4599.8, "text": " And this is a loss weight in a lot of tea.", "tokens": [51164, 400, 341, 307, 257, 4470, 3364, 294, 257, 688, 295, 5817, 13, 51264], "temperature": 0.0, "avg_logprob": -0.27413798135424416, "compression_ratio": 1.688235294117647, "no_speech_prob": 0.0002652259136084467}, {"id": 617, "seek": 459980, "start": 4599.8, "end": 4604.8, "text": " So we introduce this loss rating lambda t that the busses.", "tokens": [50364, 407, 321, 5366, 341, 4470, 10990, 13607, 256, 300, 264, 1255, 6196, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2874766808969003, "compression_ratio": 1.51875, "no_speech_prob": 0.0014773834263905883}, {"id": 618, "seek": 459980, "start": 4604.8, "end": 4616.8, "text": " And it turns out that different loss ratings trade off between like models is different good perceptual quality, like the images and look pretty that we can generate the set and sharp.", "tokens": [50614, 400, 309, 4523, 484, 300, 819, 4470, 24603, 4923, 766, 1296, 411, 5245, 307, 819, 665, 43276, 901, 3125, 11, 411, 264, 5267, 293, 574, 1238, 300, 321, 393, 8460, 264, 992, 293, 8199, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2874766808969003, "compression_ratio": 1.51875, "no_speech_prob": 0.0014773834263905883}, {"id": 619, "seek": 461680, "start": 4616.8, "end": 4631.8, "text": " And this is no high log likelihood. For instance, if we choose a number of teeth to be exactly this variance of the forward diffusion process here to cancel out the variants in the denominator.", "tokens": [50364, 400, 341, 307, 572, 1090, 3565, 22119, 13, 1171, 5197, 11, 498, 321, 2826, 257, 1230, 295, 7798, 281, 312, 2293, 341, 21977, 295, 264, 2128, 25242, 1399, 510, 281, 10373, 484, 264, 21669, 294, 264, 20687, 13, 51114], "temperature": 0.0, "avg_logprob": -0.26548343896865845, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.011854263953864574}, {"id": 620, "seek": 461680, "start": 4631.8, "end": 4639.8, "text": " Then this is just an objective that is actually that leads to good high quality perceptual quality outputs.", "tokens": [51114, 1396, 341, 307, 445, 364, 10024, 300, 307, 767, 300, 6689, 281, 665, 1090, 3125, 43276, 901, 3125, 23930, 13, 51514], "temperature": 0.0, "avg_logprob": -0.26548343896865845, "compression_ratio": 1.5515463917525774, "no_speech_prob": 0.011854263953864574}, {"id": 621, "seek": 463980, "start": 4639.8, "end": 4654.8, "text": " And if we choose for lambda, for instance, beta of T, which is hyper parameter of the forward diffusion process, then this whole objective that we have corresponds to training our model towards maximum log likelihood.", "tokens": [50364, 400, 498, 321, 2826, 337, 13607, 11, 337, 5197, 11, 9861, 295, 314, 11, 597, 307, 9848, 13075, 295, 264, 2128, 25242, 1399, 11, 550, 341, 1379, 10024, 300, 321, 362, 23249, 281, 3097, 527, 2316, 3030, 6674, 3565, 22119, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2693203564347892, "compression_ratio": 1.4202127659574468, "no_speech_prob": 0.0013878634199500084}, {"id": 622, "seek": 463980, "start": 4654.8, "end": 4660.8, "text": " More specifically, it's kind of a negative elbow.", "tokens": [51114, 5048, 4682, 11, 309, 311, 733, 295, 257, 3671, 18507, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2693203564347892, "compression_ratio": 1.4202127659574468, "no_speech_prob": 0.0013878634199500084}, {"id": 623, "seek": 466080, "start": 4660.8, "end": 4672.8, "text": " This is interesting. And it turns out that, yeah, this is exactly the same objectives that we derived with the variational approach and part one presented by our.", "tokens": [50364, 639, 307, 1880, 13, 400, 309, 4523, 484, 300, 11, 1338, 11, 341, 307, 2293, 264, 912, 15961, 300, 321, 18949, 365, 264, 3034, 1478, 3109, 293, 644, 472, 8212, 538, 527, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2553978242735932, "compression_ratio": 1.6683417085427135, "no_speech_prob": 0.004260528367012739}, {"id": 624, "seek": 466080, "start": 4672.8, "end": 4684.8, "text": " And yeah, so this means that there are some deep connections between this variation derivation and actually like score matching and noising score matching in particular.", "tokens": [50964, 400, 1338, 11, 370, 341, 1355, 300, 456, 366, 512, 2452, 9271, 1296, 341, 12990, 10151, 399, 293, 767, 411, 6175, 14324, 293, 572, 3436, 6175, 14324, 294, 1729, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2553978242735932, "compression_ratio": 1.6683417085427135, "no_speech_prob": 0.004260528367012739}, {"id": 625, "seek": 468480, "start": 4684.8, "end": 4691.8, "text": " I would also like to point out that there are like much more sophisticated model parameterizations and loss breaking possible.", "tokens": [50364, 286, 576, 611, 411, 281, 935, 484, 300, 456, 366, 411, 709, 544, 16950, 2316, 13075, 14455, 293, 4470, 7697, 1944, 13, 50714], "temperature": 0.0, "avg_logprob": -0.25981313790848004, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.0013453554129227996}, {"id": 626, "seek": 468480, "start": 4691.8, "end": 4701.8, "text": " I would in particular refer you to this recent paper by Tero Carlos at all, who discusses in quite some detail.", "tokens": [50714, 286, 576, 294, 1729, 2864, 291, 281, 341, 5162, 3035, 538, 314, 2032, 19646, 412, 439, 11, 567, 2248, 279, 294, 1596, 512, 2607, 13, 51214], "temperature": 0.0, "avg_logprob": -0.25981313790848004, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.0013453554129227996}, {"id": 627, "seek": 468480, "start": 4701.8, "end": 4706.8, "text": " There's another implementation detail I would like to talk about.", "tokens": [51214, 821, 311, 1071, 11420, 2607, 286, 576, 411, 281, 751, 466, 13, 51464], "temperature": 0.0, "avg_logprob": -0.25981313790848004, "compression_ratio": 1.5276381909547738, "no_speech_prob": 0.0013453554129227996}, {"id": 628, "seek": 470680, "start": 4706.8, "end": 4720.8, "text": " So, we know that this variance sigma T squared and of this forward diffusion process for like using individual data points that actually goes to zero as T goes to zero.", "tokens": [50364, 407, 11, 321, 458, 300, 341, 21977, 12771, 314, 8889, 293, 295, 341, 2128, 25242, 1399, 337, 411, 1228, 2609, 1412, 2793, 300, 767, 1709, 281, 4018, 382, 314, 1709, 281, 4018, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2279174395969936, "compression_ratio": 1.59375, "no_speech_prob": 0.0005273614078760147}, {"id": 629, "seek": 470680, "start": 4720.8, "end": 4730.8, "text": " But this means that if I sample the T here close to zero, then this loss might be heavily amplified when sampling. Yeah, T close to zero.", "tokens": [51064, 583, 341, 1355, 300, 498, 286, 6889, 264, 314, 510, 1998, 281, 4018, 11, 550, 341, 4470, 1062, 312, 10950, 49237, 562, 21179, 13, 865, 11, 314, 1998, 281, 4018, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2279174395969936, "compression_ratio": 1.59375, "no_speech_prob": 0.0005273614078760147}, {"id": 630, "seek": 473080, "start": 4730.8, "end": 4738.8, "text": " And that's for the case when we do not choose lambda already in function to cancel out the sigma spread.", "tokens": [50364, 400, 300, 311, 337, 264, 1389, 562, 321, 360, 406, 2826, 13607, 1217, 294, 2445, 281, 10373, 484, 264, 12771, 3974, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20734351873397827, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.0010002743219956756}, {"id": 631, "seek": 473080, "start": 4738.8, "end": 4751.8, "text": " So for some of these for these reasons we sometimes see some tricks and implementations where we train the small time cut off so we prevent sampling teeth that are extremely close to zero.", "tokens": [50764, 407, 337, 512, 295, 613, 337, 613, 4112, 321, 2171, 536, 512, 11733, 293, 4445, 763, 689, 321, 3847, 264, 1359, 565, 1723, 766, 370, 321, 4871, 21179, 7798, 300, 366, 4664, 1998, 281, 4018, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20734351873397827, "compression_ratio": 1.575268817204301, "no_speech_prob": 0.0010002743219956756}, {"id": 632, "seek": 475180, "start": 4751.8, "end": 4766.8, "text": " So in a mental way how this can be fixed, like I said, this is especially relevant training models towards high block likelihood versus lambda T function maybe something like beta of T and the sigma squared is not cancelled out.", "tokens": [50364, 407, 294, 257, 4973, 636, 577, 341, 393, 312, 6806, 11, 411, 286, 848, 11, 341, 307, 2318, 7340, 3097, 5245, 3030, 1090, 3461, 22119, 5717, 13607, 314, 2445, 1310, 746, 411, 9861, 295, 314, 293, 264, 12771, 8889, 307, 406, 25103, 484, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2987635169230716, "compression_ratio": 1.5186915887850467, "no_speech_prob": 0.012235213071107864}, {"id": 633, "seek": 475180, "start": 4766.8, "end": 4774.8, "text": " In that case, we can perform important sampling with respect to this, yeah, weight of this loss.", "tokens": [51114, 682, 300, 1389, 11, 321, 393, 2042, 1021, 21179, 365, 3104, 281, 341, 11, 1338, 11, 3364, 295, 341, 4470, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2987635169230716, "compression_ratio": 1.5186915887850467, "no_speech_prob": 0.012235213071107864}, {"id": 634, "seek": 477480, "start": 4774.8, "end": 4780.8, "text": " So we have an oversampled teeth close to zero and yeah.", "tokens": [50364, 407, 321, 362, 364, 15488, 335, 15551, 7798, 1998, 281, 4018, 293, 1338, 13, 50664], "temperature": 0.0, "avg_logprob": -0.3207298329001979, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.002049744827672839}, {"id": 635, "seek": 477480, "start": 4780.8, "end": 4789.8, "text": " So the objective then looks like this. So we oversample small teeth according to the important sample distribution that has most of its weight or small T.", "tokens": [50664, 407, 264, 10024, 550, 1542, 411, 341, 13, 407, 321, 15488, 335, 781, 1359, 7798, 4650, 281, 264, 1021, 6889, 7316, 300, 575, 881, 295, 1080, 3364, 420, 1359, 314, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3207298329001979, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.002049744827672839}, {"id": 636, "seek": 477480, "start": 4789.8, "end": 4798.8, "text": " And then we weigh down the confusion of those to the overall loss and with one over our teeth and constant distribution.", "tokens": [51114, 400, 550, 321, 13843, 760, 264, 15075, 295, 729, 281, 264, 4787, 4470, 293, 365, 472, 670, 527, 7798, 293, 5754, 7316, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3207298329001979, "compression_ratio": 1.6974358974358974, "no_speech_prob": 0.002049744827672839}, {"id": 637, "seek": 479880, "start": 4798.8, "end": 4804.8, "text": " And I don't want to go into too much detail but this is a technique you see in several papers.", "tokens": [50364, 400, 286, 500, 380, 528, 281, 352, 666, 886, 709, 2607, 457, 341, 307, 257, 6532, 291, 536, 294, 2940, 10577, 13, 50664], "temperature": 0.0, "avg_logprob": -0.24562970211631374, "compression_ratio": 1.6638655462184875, "no_speech_prob": 0.0017812506994232535}, {"id": 638, "seek": 479880, "start": 4804.8, "end": 4807.8, "text": " So here's a visualization of what happens.", "tokens": [50664, 407, 510, 311, 257, 25801, 295, 437, 2314, 13, 50814], "temperature": 0.0, "avg_logprob": -0.24562970211631374, "compression_ratio": 1.6638655462184875, "no_speech_prob": 0.0017812506994232535}, {"id": 639, "seek": 479880, "start": 4807.8, "end": 4819.8, "text": " So this is not a loss value. The wet is the loss value without an important sampling here. So, yeah, if I sample T close to zero then I have this heavily amplified loss values.", "tokens": [50814, 407, 341, 307, 406, 257, 4470, 2158, 13, 440, 6630, 307, 264, 4470, 2158, 1553, 364, 1021, 21179, 510, 13, 407, 11, 1338, 11, 498, 286, 6889, 314, 1998, 281, 4018, 550, 286, 362, 341, 10950, 49237, 4470, 4190, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24562970211631374, "compression_ratio": 1.6638655462184875, "no_speech_prob": 0.0017812506994232535}, {"id": 640, "seek": 479880, "start": 4819.8, "end": 4827.8, "text": " But with important sampling the blue line, the variance is significantly reduced.", "tokens": [51414, 583, 365, 1021, 21179, 264, 3344, 1622, 11, 264, 21977, 307, 10591, 9212, 13, 51814], "temperature": 0.0, "avg_logprob": -0.24562970211631374, "compression_ratio": 1.6638655462184875, "no_speech_prob": 0.0017812506994232535}, {"id": 641, "seek": 482780, "start": 4827.8, "end": 4833.8, "text": " Before moving on, it makes sense to briefly recapitulate what we have been doing so far.", "tokens": [50364, 4546, 2684, 322, 11, 309, 1669, 2020, 281, 10515, 20928, 270, 5256, 437, 321, 362, 668, 884, 370, 1400, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15293681768723477, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.0014100370462983847}, {"id": 642, "seek": 482780, "start": 4833.8, "end": 4853.8, "text": " So we have been introducing this diffusion modeling framework based on continuous times now, in contrast to the first part presented by Arash, where each diffusion step had a finite size and we overall had a finite number of discrete forward fixed diffusion process steps and also denoising steps.", "tokens": [50664, 407, 321, 362, 668, 15424, 341, 25242, 15983, 8388, 2361, 322, 10957, 1413, 586, 11, 294, 8712, 281, 264, 700, 644, 8212, 538, 1587, 1299, 11, 689, 1184, 25242, 1823, 632, 257, 19362, 2744, 293, 321, 4787, 632, 257, 19362, 1230, 295, 27706, 2128, 6806, 25242, 1399, 4439, 293, 611, 1441, 78, 3436, 4439, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15293681768723477, "compression_ratio": 1.6495726495726495, "no_speech_prob": 0.0014100370462983847}, {"id": 643, "seek": 485380, "start": 4853.8, "end": 4864.8, "text": " In this section we have considered a continuous time. This allowed us to introduce this differential equation framework and also to make these connections to score matching.", "tokens": [50364, 682, 341, 3541, 321, 362, 4888, 257, 10957, 565, 13, 639, 4350, 505, 281, 5366, 341, 15756, 5367, 8388, 293, 611, 281, 652, 613, 9271, 281, 6175, 14324, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12313030121174265, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.006000936962664127}, {"id": 644, "seek": 485380, "start": 4864.8, "end": 4874.8, "text": " But it is important to keep in mind that we are still describing the same types of diffusion models in this section. We are just using different tools at a different framework.", "tokens": [50914, 583, 309, 307, 1021, 281, 1066, 294, 1575, 300, 321, 366, 920, 16141, 264, 912, 3467, 295, 25242, 5245, 294, 341, 3541, 13, 492, 366, 445, 1228, 819, 3873, 412, 257, 819, 8388, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12313030121174265, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.006000936962664127}, {"id": 645, "seek": 485380, "start": 4874.8, "end": 4882.8, "text": " So this is important to realise after all we obtained the same objectives as we have seen on the last few slides.", "tokens": [51414, 407, 341, 307, 1021, 281, 18809, 934, 439, 321, 14879, 264, 912, 15961, 382, 321, 362, 1612, 322, 264, 1036, 1326, 9788, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12313030121174265, "compression_ratio": 1.7846153846153847, "no_speech_prob": 0.006000936962664127}, {"id": 646, "seek": 488280, "start": 4882.8, "end": 4890.8, "text": " But bear in mind, let us now move on and we will talk now about the probability flow ordinary differential equation.", "tokens": [50364, 583, 6155, 294, 1575, 11, 718, 505, 586, 1286, 322, 293, 321, 486, 751, 586, 466, 264, 8482, 3095, 10547, 15756, 5367, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19070959091186523, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.011859579011797905}, {"id": 647, "seek": 488280, "start": 4890.8, "end": 4899.8, "text": " However, let us first consider the reverse generative diffusion SDE again, that one, so object you have already seen so far.", "tokens": [50764, 2908, 11, 718, 505, 700, 1949, 264, 9943, 1337, 1166, 25242, 14638, 36, 797, 11, 300, 472, 11, 370, 2657, 291, 362, 1217, 1612, 370, 1400, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19070959091186523, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.011859579011797905}, {"id": 648, "seek": 489980, "start": 4899.8, "end": 4921.8, "text": " With this generative reverse diffusion SDE, we can, when sampling random noise from this standard normal prior distribution, we can generate data and more specifically we basically can sample data all along the diffused data distribution, the reddish curves here, the reddish contours here.", "tokens": [50364, 2022, 341, 1337, 1166, 9943, 25242, 14638, 36, 11, 321, 393, 11, 562, 21179, 4974, 5658, 490, 341, 3832, 2710, 4059, 7316, 11, 321, 393, 8460, 1412, 293, 544, 4682, 321, 1936, 393, 6889, 1412, 439, 2051, 264, 7593, 4717, 1412, 7316, 11, 264, 2182, 40974, 19490, 510, 11, 264, 2182, 40974, 660, 5067, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16622891426086425, "compression_ratio": 1.686046511627907, "no_speech_prob": 0.0018095817649737}, {"id": 649, "seek": 492180, "start": 4922.8, "end": 4934.8, "text": " It turns out there is an ordinary differential equation called the probability flow ODE that is in distribution equivalent to this reverse generative diffusion SDE.", "tokens": [50414, 467, 4523, 484, 456, 307, 364, 10547, 15756, 5367, 1219, 264, 8482, 3095, 422, 22296, 300, 307, 294, 7316, 10344, 281, 341, 9943, 1337, 1166, 25242, 14638, 36, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12964585801245462, "compression_ratio": 1.5763546798029557, "no_speech_prob": 0.010006391443312168}, {"id": 650, "seek": 492180, "start": 4934.8, "end": 4940.8, "text": " It will become clear in a minute what exactly I mean with in distribution equivalent.", "tokens": [51014, 467, 486, 1813, 1850, 294, 257, 3456, 437, 2293, 286, 914, 365, 294, 7316, 10344, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12964585801245462, "compression_ratio": 1.5763546798029557, "no_speech_prob": 0.010006391443312168}, {"id": 651, "seek": 492180, "start": 4940.8, "end": 4944.8, "text": " Let us first have a look at this ODE itself. It is written down here.", "tokens": [51314, 961, 505, 700, 362, 257, 574, 412, 341, 422, 22296, 2564, 13, 467, 307, 3720, 760, 510, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12964585801245462, "compression_ratio": 1.5763546798029557, "no_speech_prob": 0.010006391443312168}, {"id": 652, "seek": 494480, "start": 4944.8, "end": 4957.8, "text": " In contrast to the generative diffusion SDE, it doesn't have the noise term and also the score function term, which we will then later learn with the neural network, it doesn't have this factor of two in front anymore.", "tokens": [50364, 682, 8712, 281, 264, 1337, 1166, 25242, 14638, 36, 11, 309, 1177, 380, 362, 264, 5658, 1433, 293, 611, 264, 6175, 2445, 1433, 11, 597, 321, 486, 550, 1780, 1466, 365, 264, 18161, 3209, 11, 309, 1177, 380, 362, 341, 5952, 295, 732, 294, 1868, 3602, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18611950344509548, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.0021480892319232225}, {"id": 653, "seek": 494480, "start": 4957.8, "end": 4961.8, "text": " So what do I mean with in distribution equivalent?", "tokens": [51014, 407, 437, 360, 286, 914, 365, 294, 7316, 10344, 30, 51214], "temperature": 0.0, "avg_logprob": -0.18611950344509548, "compression_ratio": 1.5197740112994351, "no_speech_prob": 0.0021480892319232225}, {"id": 654, "seek": 496180, "start": 4961.8, "end": 4980.8, "text": " When I sample many initial noise values from this standard normal distribution at initialization when I want to generate the data, then simulating all these samples on backwards versus probability flow ODE towards the data.", "tokens": [50364, 1133, 286, 6889, 867, 5883, 5658, 4190, 490, 341, 3832, 2710, 7316, 412, 5883, 2144, 562, 286, 528, 281, 8460, 264, 1412, 11, 550, 1034, 12162, 439, 613, 10938, 322, 12204, 5717, 8482, 3095, 422, 22296, 3030, 264, 1412, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1529771848158403, "compression_ratio": 1.4387096774193548, "no_speech_prob": 0.0019265187438577414}, {"id": 655, "seek": 498080, "start": 4980.8, "end": 4992.8, "text": " By doing that, we will sample from the exact same probability distribution, like with the generative diffusion SDE, with the only difference that we don't have this noise anymore.", "tokens": [50364, 3146, 884, 300, 11, 321, 486, 6889, 490, 264, 1900, 912, 8482, 7316, 11, 411, 365, 264, 1337, 1166, 25242, 14638, 36, 11, 365, 264, 787, 2649, 300, 321, 500, 380, 362, 341, 5658, 3602, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15133485794067383, "compression_ratio": 1.6596638655462186, "no_speech_prob": 0.001501032616943121}, {"id": 656, "seek": 498080, "start": 4992.8, "end": 4997.8, "text": " So how does it look like more specifically? We can see this on that slide.", "tokens": [50964, 407, 577, 775, 309, 574, 411, 544, 4682, 30, 492, 393, 536, 341, 322, 300, 4137, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15133485794067383, "compression_ratio": 1.6596638655462186, "no_speech_prob": 0.001501032616943121}, {"id": 657, "seek": 498080, "start": 4997.8, "end": 5007.8, "text": " Here again, we have the probability flow ODE, just that we now have inserted the learned score function as a pattern for the score function.", "tokens": [51214, 1692, 797, 11, 321, 362, 264, 8482, 3095, 422, 22296, 11, 445, 300, 321, 586, 362, 27992, 264, 3264, 6175, 2445, 382, 257, 5102, 337, 264, 6175, 2445, 13, 51714], "temperature": 0.0, "avg_logprob": -0.15133485794067383, "compression_ratio": 1.6596638655462186, "no_speech_prob": 0.001501032616943121}, {"id": 658, "seek": 500780, "start": 5007.8, "end": 5012.8, "text": " And now these trajectories defined by this ODE, they look like this.", "tokens": [50364, 400, 586, 613, 18257, 2083, 7642, 538, 341, 422, 22296, 11, 436, 574, 411, 341, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10901526971296831, "compression_ratio": 1.775229357798165, "no_speech_prob": 0.002286652335897088}, {"id": 659, "seek": 500780, "start": 5012.8, "end": 5023.8, "text": " So we see that when we sample from this standard normal prior distribution on the right, these trajectories they will all flow into the modes of the data distribution.", "tokens": [50614, 407, 321, 536, 300, 562, 321, 6889, 490, 341, 3832, 2710, 4059, 7316, 322, 264, 558, 11, 613, 18257, 2083, 436, 486, 439, 3095, 666, 264, 14068, 295, 264, 1412, 7316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10901526971296831, "compression_ratio": 1.775229357798165, "no_speech_prob": 0.002286652335897088}, {"id": 660, "seek": 500780, "start": 5023.8, "end": 5028.8, "text": " We see this also at these bluish lines here at the background.", "tokens": [51164, 492, 536, 341, 611, 412, 613, 888, 33786, 3876, 510, 412, 264, 3678, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10901526971296831, "compression_ratio": 1.775229357798165, "no_speech_prob": 0.002286652335897088}, {"id": 661, "seek": 500780, "start": 5028.8, "end": 5034.8, "text": " Yeah, so the probability quite literally flows into the modes of the data distribution.", "tokens": [51414, 865, 11, 370, 264, 8482, 1596, 3736, 12867, 666, 264, 14068, 295, 264, 1412, 7316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10901526971296831, "compression_ratio": 1.775229357798165, "no_speech_prob": 0.002286652335897088}, {"id": 662, "seek": 503480, "start": 5034.8, "end": 5039.8, "text": " And that's called the probability flow ODE.", "tokens": [50364, 400, 300, 311, 1219, 264, 8482, 3095, 422, 22296, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13877438335883907, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.0019565257243812084}, {"id": 663, "seek": 503480, "start": 5039.8, "end": 5042.8, "text": " Here we have an animation how it looks like.", "tokens": [50614, 1692, 321, 362, 364, 9603, 577, 309, 1542, 411, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13877438335883907, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.0019565257243812084}, {"id": 664, "seek": 503480, "start": 5042.8, "end": 5051.8, "text": " And I think at this point it should really become clear what I meant with they are the same in distribution and they sample the same distribution.", "tokens": [50764, 400, 286, 519, 412, 341, 935, 309, 820, 534, 1813, 1850, 437, 286, 4140, 365, 436, 366, 264, 912, 294, 7316, 293, 436, 6889, 264, 912, 7316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13877438335883907, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.0019565257243812084}, {"id": 665, "seek": 503480, "start": 5051.8, "end": 5058.8, "text": " On the left hand side, we have the SDE that we have that I have introduced earlier already SDE framework.", "tokens": [51214, 1282, 264, 1411, 1011, 1252, 11, 321, 362, 264, 14638, 36, 300, 321, 362, 300, 286, 362, 7268, 3071, 1217, 14638, 36, 8388, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13877438335883907, "compression_ratio": 1.586046511627907, "no_speech_prob": 0.0019565257243812084}, {"id": 666, "seek": 505880, "start": 5058.8, "end": 5067.8, "text": " And we see that these trajectories are zigzagging, I have this noise injection, but I'm still landing at the modes of the data distribution.", "tokens": [50364, 400, 321, 536, 300, 613, 18257, 2083, 366, 38290, 43886, 3249, 11, 286, 362, 341, 5658, 22873, 11, 457, 286, 478, 920, 11202, 412, 264, 14068, 295, 264, 1412, 7316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16560726401246625, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.003222073195502162}, {"id": 667, "seek": 505880, "start": 5067.8, "end": 5085.8, "text": " Why for the ODE formulation, I now have these pretty like, not exactly straight but more deterministic trajectories that still land in the modes of the data distribution when I initialize some randomly from this prior distribution.", "tokens": [50814, 1545, 337, 264, 422, 22296, 37642, 11, 286, 586, 362, 613, 1238, 411, 11, 406, 2293, 2997, 457, 544, 15957, 3142, 18257, 2083, 300, 920, 2117, 294, 264, 14068, 295, 264, 1412, 7316, 562, 286, 5883, 1125, 512, 16979, 490, 341, 4059, 7316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16560726401246625, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.003222073195502162}, {"id": 668, "seek": 508580, "start": 5085.8, "end": 5094.8, "text": " And the visual trajectory on the right is deterministic while this is stochastic.", "tokens": [50364, 400, 264, 5056, 21512, 322, 264, 558, 307, 15957, 3142, 1339, 341, 307, 342, 8997, 2750, 13, 50814], "temperature": 0.0, "avg_logprob": -0.14704754617479113, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.010485600680112839}, {"id": 669, "seek": 508580, "start": 5094.8, "end": 5105.8, "text": " So this probability flow ordinary differential equation, this is actually an instance of a neural ordinary differential equations which a while ago generated a lot of attention in the literature.", "tokens": [50814, 407, 341, 8482, 3095, 10547, 15756, 5367, 11, 341, 307, 767, 364, 5197, 295, 257, 18161, 10547, 15756, 11787, 597, 257, 1339, 2057, 10833, 257, 688, 295, 3202, 294, 264, 10394, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14704754617479113, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.010485600680112839}, {"id": 670, "seek": 508580, "start": 5105.8, "end": 5112.8, "text": " More specifically, we can even see this as a continuous normalizing flow.", "tokens": [51364, 5048, 4682, 11, 321, 393, 754, 536, 341, 382, 257, 10957, 2710, 3319, 3095, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14704754617479113, "compression_ratio": 1.6635071090047393, "no_speech_prob": 0.010485600680112839}, {"id": 671, "seek": 511280, "start": 5112.8, "end": 5118.8, "text": " So, why should we care, why should we use this probability flow ODE framework.", "tokens": [50364, 407, 11, 983, 820, 321, 1127, 11, 983, 820, 321, 764, 341, 8482, 3095, 422, 22296, 8388, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11245135373847429, "compression_ratio": 2.0045454545454544, "no_speech_prob": 0.001926281489431858}, {"id": 672, "seek": 511280, "start": 5118.8, "end": 5127.8, "text": " It turns out that this ordinary differential equation framework that allows the use of advanced ordinary differential equation solvers.", "tokens": [50664, 467, 4523, 484, 300, 341, 10547, 15756, 5367, 8388, 300, 4045, 264, 764, 295, 7339, 10547, 15756, 5367, 1404, 840, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11245135373847429, "compression_ratio": 2.0045454545454544, "no_speech_prob": 0.001926281489431858}, {"id": 673, "seek": 511280, "start": 5127.8, "end": 5134.8, "text": " It is somewhat easier to work with ordinary differential equations than with stochastic differential equation.", "tokens": [51114, 467, 307, 8344, 3571, 281, 589, 365, 10547, 15756, 11787, 813, 365, 342, 8997, 2750, 15756, 5367, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11245135373847429, "compression_ratio": 2.0045454545454544, "no_speech_prob": 0.001926281489431858}, {"id": 674, "seek": 511280, "start": 5134.8, "end": 5141.8, "text": " And there really is a broad literature on how to quickly and very efficiently solve ordinary differential equation.", "tokens": [51464, 400, 456, 534, 307, 257, 4152, 10394, 322, 577, 281, 2661, 293, 588, 19621, 5039, 10547, 15756, 5367, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11245135373847429, "compression_ratio": 2.0045454545454544, "no_speech_prob": 0.001926281489431858}, {"id": 675, "seek": 514180, "start": 5141.8, "end": 5145.8, "text": " So we can build on top of this literature here.", "tokens": [50364, 407, 321, 393, 1322, 322, 1192, 295, 341, 10394, 510, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13600577534856023, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.0012839102419093251}, {"id": 676, "seek": 514180, "start": 5145.8, "end": 5147.8, "text": " But there are more advantages.", "tokens": [50564, 583, 456, 366, 544, 14906, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13600577534856023, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.0012839102419093251}, {"id": 677, "seek": 514180, "start": 5147.8, "end": 5162.8, "text": " This ordinary differential equation, I can run this in both directions, I can run it as generation, where I go from the right to the left, where I sample the noise from a prior distribution and then go to the left to generate data.", "tokens": [50664, 639, 10547, 15756, 5367, 11, 286, 393, 1190, 341, 294, 1293, 11095, 11, 286, 393, 1190, 309, 382, 5125, 11, 689, 286, 352, 490, 264, 558, 281, 264, 1411, 11, 689, 286, 6889, 264, 5658, 490, 257, 4059, 7316, 293, 550, 352, 281, 264, 1411, 281, 8460, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13600577534856023, "compression_ratio": 1.6577540106951871, "no_speech_prob": 0.0012839102419093251}, {"id": 678, "seek": 516280, "start": 5162.8, "end": 5176.8, "text": " But similarly, given a data point, I can also run the probability flow ODE in the other direction and encode this data point in the latent space of this diffusion model, this prior space.", "tokens": [50364, 583, 14138, 11, 2212, 257, 1412, 935, 11, 286, 393, 611, 1190, 264, 8482, 3095, 422, 22296, 294, 264, 661, 3513, 293, 2058, 1429, 341, 1412, 935, 294, 264, 48994, 1901, 295, 341, 25242, 2316, 11, 341, 4059, 1901, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12548862708793893, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005818090867251158}, {"id": 679, "seek": 516280, "start": 5176.8, "end": 5177.8, "text": " So this is interesting.", "tokens": [51064, 407, 341, 307, 1880, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12548862708793893, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005818090867251158}, {"id": 680, "seek": 516280, "start": 5177.8, "end": 5184.8, "text": " And yeah, this allows for interesting applications, for instance, or semantic image interpolation.", "tokens": [51114, 400, 1338, 11, 341, 4045, 337, 1880, 5821, 11, 337, 5197, 11, 420, 47982, 3256, 44902, 399, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12548862708793893, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005818090867251158}, {"id": 681, "seek": 516280, "start": 5184.8, "end": 5191.8, "text": " And to make clear what I mean with that, let's look at this slide here.", "tokens": [51464, 400, 281, 652, 1850, 437, 286, 914, 365, 300, 11, 718, 311, 574, 412, 341, 4137, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12548862708793893, "compression_ratio": 1.6394849785407726, "no_speech_prob": 0.005818090867251158}, {"id": 682, "seek": 519180, "start": 5191.8, "end": 5199.8, "text": " What I'm doing here is I have drawn two noise values, or let's look first at the lower left.", "tokens": [50364, 708, 286, 478, 884, 510, 307, 286, 362, 10117, 732, 5658, 4190, 11, 420, 718, 311, 574, 700, 412, 264, 3126, 1411, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1126481547500148, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.0046049924567341805}, {"id": 683, "seek": 519180, "start": 5199.8, "end": 5205.8, "text": " So here we are drawing two noise values in the latent space of the diffusion model and this noise space.", "tokens": [50764, 407, 510, 321, 366, 6316, 732, 5658, 4190, 294, 264, 48994, 1901, 295, 264, 25242, 2316, 293, 341, 5658, 1901, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1126481547500148, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.0046049924567341805}, {"id": 684, "seek": 519180, "start": 5205.8, "end": 5210.8, "text": " And now I can linear interpolate these noise values in this space.", "tokens": [51064, 400, 586, 286, 393, 8213, 44902, 473, 613, 5658, 4190, 294, 341, 1901, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1126481547500148, "compression_ratio": 1.6603773584905661, "no_speech_prob": 0.0046049924567341805}, {"id": 685, "seek": 521080, "start": 5210.8, "end": 5226.8, "text": " However, the model was trained in such a way that every sample under this noise distribution, so also every sample along this linear interpolation path between those noise values decodes to a coherent realistic image.", "tokens": [50364, 2908, 11, 264, 2316, 390, 8895, 294, 1270, 257, 636, 300, 633, 6889, 833, 341, 5658, 7316, 11, 370, 611, 633, 6889, 2051, 341, 8213, 44902, 399, 3100, 1296, 729, 5658, 4190, 979, 4789, 281, 257, 36239, 12465, 3256, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09163076227361505, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.003221719991415739}, {"id": 686, "seek": 522680, "start": 5226.8, "end": 5236.8, "text": " So when I then interpolate, it means that this results in continuous semantically meaningful changes in the data space, right?", "tokens": [50364, 407, 562, 286, 550, 44902, 473, 11, 309, 1355, 300, 341, 3542, 294, 10957, 4361, 49505, 10995, 2962, 294, 264, 1412, 1901, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.13062151172493078, "compression_ratio": 1.785, "no_speech_prob": 0.008977255783975124}, {"id": 687, "seek": 522680, "start": 5236.8, "end": 5243.8, "text": " And keep in mind, we could not just interpolate directly linearly in pixel space, this would not be meaningful.", "tokens": [50864, 400, 1066, 294, 1575, 11, 321, 727, 406, 445, 44902, 473, 3838, 43586, 294, 19261, 1901, 11, 341, 576, 406, 312, 10995, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13062151172493078, "compression_ratio": 1.785, "no_speech_prob": 0.008977255783975124}, {"id": 688, "seek": 522680, "start": 5243.8, "end": 5252.8, "text": " But we can do that in noise space and then obtain semantically meaningful interpolations in the pixel space like this.", "tokens": [51214, 583, 321, 393, 360, 300, 294, 5658, 1901, 293, 550, 12701, 4361, 49505, 10995, 44902, 763, 294, 264, 19261, 1901, 411, 341, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13062151172493078, "compression_ratio": 1.785, "no_speech_prob": 0.008977255783975124}, {"id": 689, "seek": 525280, "start": 5252.8, "end": 5260.8, "text": " But because this ODE is so complex, right, under the hood, this means that we will sometimes have some jumps between nodes and such like this.", "tokens": [50364, 583, 570, 341, 422, 22296, 307, 370, 3997, 11, 558, 11, 833, 264, 13376, 11, 341, 1355, 300, 321, 486, 2171, 362, 512, 16704, 1296, 13891, 293, 1270, 411, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15861121283637153, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0251146387308836}, {"id": 690, "seek": 525280, "start": 5260.8, "end": 5263.8, "text": " And we also see this in this animation here.", "tokens": [50764, 400, 321, 611, 536, 341, 294, 341, 9603, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15861121283637153, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0251146387308836}, {"id": 691, "seek": 525280, "start": 5263.8, "end": 5271.8, "text": " So in this animation at the top, yeah, we have been doing many of such interpolations one after another.", "tokens": [50914, 407, 294, 341, 9603, 412, 264, 1192, 11, 1338, 11, 321, 362, 668, 884, 867, 295, 1270, 44902, 763, 472, 934, 1071, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15861121283637153, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0251146387308836}, {"id": 692, "seek": 525280, "start": 5271.8, "end": 5278.8, "text": " And yeah, sometimes you see like little jumps, this basically corresponds to that.", "tokens": [51314, 400, 1338, 11, 2171, 291, 536, 411, 707, 16704, 11, 341, 1936, 23249, 281, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15861121283637153, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0251146387308836}, {"id": 693, "seek": 527880, "start": 5278.8, "end": 5287.8, "text": " So all this is only possible due to this deterministic encoding and decoding path with the probability flow ODE.", "tokens": [50364, 407, 439, 341, 307, 787, 1944, 3462, 281, 341, 15957, 3142, 43430, 293, 979, 8616, 3100, 365, 264, 8482, 3095, 422, 22296, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1775665006775787, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0004304858448449522}, {"id": 694, "seek": 527880, "start": 5287.8, "end": 5295.8, "text": " I think it's clear that you couldn't do this so easily with a stochastic trajectory.", "tokens": [50814, 286, 519, 309, 311, 1850, 300, 291, 2809, 380, 360, 341, 370, 3612, 365, 257, 342, 8997, 2750, 21512, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1775665006775787, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0004304858448449522}, {"id": 695, "seek": 527880, "start": 5295.8, "end": 5297.8, "text": " All right.", "tokens": [51214, 1057, 558, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1775665006775787, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0004304858448449522}, {"id": 696, "seek": 527880, "start": 5297.8, "end": 5303.8, "text": " So there is another advantage of the probability flow ordinary differential equation.", "tokens": [51314, 407, 456, 307, 1071, 5002, 295, 264, 8482, 3095, 10547, 15756, 5367, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1775665006775787, "compression_ratio": 1.5233160621761659, "no_speech_prob": 0.0004304858448449522}, {"id": 697, "seek": 530380, "start": 5303.8, "end": 5310.8, "text": " We can also use it for block likelihood computation as in continuous normalizing flows.", "tokens": [50364, 492, 393, 611, 764, 309, 337, 3461, 22119, 24903, 382, 294, 10957, 2710, 3319, 12867, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17672219483748727, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.04667963460087776}, {"id": 698, "seek": 530380, "start": 5310.8, "end": 5319.8, "text": " More specifically, we can take a given image or a given data sample, for instance, this image of Arash's cat peanut.", "tokens": [50714, 5048, 4682, 11, 321, 393, 747, 257, 2212, 3256, 420, 257, 2212, 1412, 6889, 11, 337, 5197, 11, 341, 3256, 295, 1587, 1299, 311, 3857, 19209, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17672219483748727, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.04667963460087776}, {"id": 699, "seek": 530380, "start": 5319.8, "end": 5326.8, "text": " Now we can take peanut and encode peanut in the latent space of our diffusion model.", "tokens": [51164, 823, 321, 393, 747, 19209, 293, 2058, 1429, 19209, 294, 264, 48994, 1901, 295, 527, 25242, 2316, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17672219483748727, "compression_ratio": 1.5372340425531914, "no_speech_prob": 0.04667963460087776}, {"id": 700, "seek": 532680, "start": 5327.8, "end": 5335.8, "text": " Now we can calculate the probability of peanuts and coding under the prior distribution of our diffusion model.", "tokens": [50414, 823, 321, 393, 8873, 264, 8482, 295, 32895, 293, 17720, 833, 264, 4059, 7316, 295, 527, 25242, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17902002836528577, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0021825511939823627}, {"id": 701, "seek": 532680, "start": 5335.8, "end": 5348.8, "text": " And additionally, we take into account this using this instantaneous change of variables formula kind of the distortion of the ODE the volume change along the ODE trajectory.", "tokens": [50814, 400, 43181, 11, 321, 747, 666, 2696, 341, 1228, 341, 45596, 1319, 295, 9102, 8513, 733, 295, 264, 28426, 295, 264, 422, 22296, 264, 5523, 1319, 2051, 264, 422, 22296, 21512, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17902002836528577, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.0021825511939823627}, {"id": 702, "seek": 534880, "start": 5348.8, "end": 5356.8, "text": " So the probability of our data sample, in our case, the image of peanut is then given by that expression.", "tokens": [50364, 407, 264, 8482, 295, 527, 1412, 6889, 11, 294, 527, 1389, 11, 264, 3256, 295, 19209, 307, 550, 2212, 538, 300, 6114, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12981887817382812, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0006563475471921265}, {"id": 703, "seek": 534880, "start": 5356.8, "end": 5364.8, "text": " So we're really just using the tricks from the continuous normalizing flow literature here.", "tokens": [50764, 407, 321, 434, 534, 445, 1228, 264, 11733, 490, 264, 10957, 2710, 3319, 3095, 10394, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12981887817382812, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0006563475471921265}, {"id": 704, "seek": 534880, "start": 5364.8, "end": 5375.8, "text": " What all this means is actually that in their probability flow ODE formulation, diffusion models can also be considered as continuous normalizing flows.", "tokens": [51164, 708, 439, 341, 1355, 307, 767, 300, 294, 641, 8482, 3095, 422, 22296, 37642, 11, 25242, 5245, 393, 611, 312, 4888, 382, 10957, 2710, 3319, 12867, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12981887817382812, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.0006563475471921265}, {"id": 705, "seek": 537580, "start": 5375.8, "end": 5382.8, "text": " However, in contrast to continuous normalizing flows, we train diffusion models with score matching.", "tokens": [50364, 2908, 11, 294, 8712, 281, 10957, 2710, 3319, 12867, 11, 321, 3847, 25242, 5245, 365, 6175, 14324, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09984583913544078, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.0007205865113064647}, {"id": 706, "seek": 537580, "start": 5382.8, "end": 5391.8, "text": " Continuous normalizing flows themselves are usually trained directly with this objective to maximize the likelihood of the data.", "tokens": [50714, 14674, 12549, 2710, 3319, 12867, 2969, 366, 2673, 8895, 3838, 365, 341, 10024, 281, 19874, 264, 22119, 295, 264, 1412, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09984583913544078, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.0007205865113064647}, {"id": 707, "seek": 537580, "start": 5391.8, "end": 5404.8, "text": " However, training with such this objective directly is actually a hard task because for each training iteration, I have to simulate the whole trajectory here and back propagate it through it.", "tokens": [51164, 2908, 11, 3097, 365, 1270, 341, 10024, 3838, 307, 767, 257, 1152, 5633, 570, 337, 1184, 3097, 24784, 11, 286, 362, 281, 27817, 264, 1379, 21512, 510, 293, 646, 48256, 309, 807, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09984583913544078, "compression_ratio": 1.7468879668049793, "no_speech_prob": 0.0007205865113064647}, {"id": 708, "seek": 540480, "start": 5404.8, "end": 5409.8, "text": " On the other hand, this diffusion model training relies on score matching.", "tokens": [50364, 1282, 264, 661, 1011, 11, 341, 25242, 2316, 3097, 30910, 322, 6175, 14324, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11082197272259256, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.0022506790701299906}, {"id": 709, "seek": 540480, "start": 5409.8, "end": 5414.8, "text": " And as we have seen earlier, score matching works quite differently in score matching.", "tokens": [50614, 400, 382, 321, 362, 1612, 3071, 11, 6175, 14324, 1985, 1596, 7614, 294, 6175, 14324, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11082197272259256, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.0022506790701299906}, {"id": 710, "seek": 540480, "start": 5414.8, "end": 5420.8, "text": " We basically have like we can train for all these different times along the diffusion process separately.", "tokens": [50864, 492, 1936, 362, 411, 321, 393, 3847, 337, 439, 613, 819, 1413, 2051, 264, 25242, 1399, 14759, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11082197272259256, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.0022506790701299906}, {"id": 711, "seek": 540480, "start": 5420.8, "end": 5424.8, "text": " This leads to a much more scalable and robust learning objective.", "tokens": [51164, 639, 6689, 281, 257, 709, 544, 38481, 293, 13956, 2539, 10024, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11082197272259256, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.0022506790701299906}, {"id": 712, "seek": 540480, "start": 5424.8, "end": 5433.8, "text": " And yeah, this makes diffusion models very scalable in contrast to these normalizing flows, I would argue.", "tokens": [51364, 400, 1338, 11, 341, 1669, 25242, 5245, 588, 38481, 294, 8712, 281, 613, 2710, 3319, 12867, 11, 286, 576, 9695, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11082197272259256, "compression_ratio": 1.752988047808765, "no_speech_prob": 0.0022506790701299906}, {"id": 713, "seek": 543480, "start": 5434.8, "end": 5441.8, "text": " So I have not talked a lot about these differential equations, derive these, derive them and so on and so forth.", "tokens": [50364, 407, 286, 362, 406, 2825, 257, 688, 466, 613, 15756, 11787, 11, 28446, 613, 11, 28446, 552, 293, 370, 322, 293, 370, 5220, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1268462042013804, "compression_ratio": 1.672, "no_speech_prob": 0.0015244267415255308}, {"id": 714, "seek": 543480, "start": 5441.8, "end": 5446.8, "text": " However, how should we actually solve these SDS and ODE in practice?", "tokens": [50714, 2908, 11, 577, 820, 321, 767, 5039, 613, 318, 11844, 293, 422, 22296, 294, 3124, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1268462042013804, "compression_ratio": 1.672, "no_speech_prob": 0.0015244267415255308}, {"id": 715, "seek": 543480, "start": 5446.8, "end": 5460.8, "text": " We have already seen that we can probably not solve this analytically because these SDS and ODE are defined with very complex nonlinear functions, namely these neural networks that approximate the score function.", "tokens": [50964, 492, 362, 1217, 1612, 300, 321, 393, 1391, 406, 5039, 341, 10783, 984, 570, 613, 318, 11844, 293, 422, 22296, 366, 7642, 365, 588, 3997, 2107, 28263, 6828, 11, 20926, 613, 18161, 9590, 300, 30874, 264, 6175, 2445, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1268462042013804, "compression_ratio": 1.672, "no_speech_prob": 0.0015244267415255308}, {"id": 716, "seek": 543480, "start": 5460.8, "end": 5463.8, "text": " So let us look at that.", "tokens": [51664, 407, 718, 505, 574, 412, 300, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1268462042013804, "compression_ratio": 1.672, "no_speech_prob": 0.0015244267415255308}, {"id": 717, "seek": 546380, "start": 5463.8, "end": 5467.8, "text": " So let's start with the generative diffusion SDE.", "tokens": [50364, 407, 718, 311, 722, 365, 264, 1337, 1166, 25242, 318, 22296, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11730065712561974, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0003352804924361408}, {"id": 718, "seek": 546380, "start": 5467.8, "end": 5472.8, "text": " So the most naive way to do this is to use Euler-Mariouama sampling.", "tokens": [50564, 407, 264, 881, 29052, 636, 281, 360, 341, 307, 281, 764, 462, 26318, 12, 44, 3504, 263, 2404, 21179, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11730065712561974, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0003352804924361408}, {"id": 719, "seek": 546380, "start": 5472.8, "end": 5479.8, "text": " We have already briefly talked about Euler-Mariouama sampling in this earlier one slide crash course on differential equations.", "tokens": [50814, 492, 362, 1217, 10515, 2825, 466, 462, 26318, 12, 44, 3504, 263, 2404, 21179, 294, 341, 3071, 472, 4137, 8252, 1164, 322, 15756, 11787, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11730065712561974, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.0003352804924361408}, {"id": 720, "seek": 547980, "start": 5479.8, "end": 5492.8, "text": " What we do in that case is we simply evaluate our function here for different for our state t and x, then we propagate for like a small time step delta t.", "tokens": [50364, 708, 321, 360, 294, 300, 1389, 307, 321, 2935, 13059, 527, 2445, 510, 337, 819, 337, 527, 1785, 256, 293, 2031, 11, 550, 321, 48256, 337, 411, 257, 1359, 565, 1823, 8289, 256, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14212409655253092, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.1007888987660408}, {"id": 721, "seek": 547980, "start": 5492.8, "end": 5498.8, "text": " And we additionally add a little bit of noise, which is also scaled by the time step.", "tokens": [51014, 400, 321, 43181, 909, 257, 707, 857, 295, 5658, 11, 597, 307, 611, 36039, 538, 264, 565, 1823, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14212409655253092, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.1007888987660408}, {"id": 722, "seek": 547980, "start": 5498.8, "end": 5508.8, "text": " And yeah, so we do this then iteratively one after another given a new step we evaluate again and then add a little bit of noise and so on and so forth.", "tokens": [51314, 400, 1338, 11, 370, 321, 360, 341, 550, 17138, 19020, 472, 934, 1071, 2212, 257, 777, 1823, 321, 13059, 797, 293, 550, 909, 257, 707, 857, 295, 5658, 293, 370, 322, 293, 370, 5220, 13, 51814], "temperature": 0.0, "avg_logprob": -0.14212409655253092, "compression_ratio": 1.7702702702702702, "no_speech_prob": 0.1007888987660408}, {"id": 723, "seek": 550880, "start": 5508.8, "end": 5525.8, "text": " By the way, as a small comment here, and you may wonder about the sign flip from here to here, this is because our dt is actually negative because we're running from like large time values to small time values and this delta t here is not supposed to be like an absolute step size.", "tokens": [50364, 3146, 264, 636, 11, 382, 257, 1359, 2871, 510, 11, 293, 291, 815, 2441, 466, 264, 1465, 7929, 490, 510, 281, 510, 11, 341, 307, 570, 527, 36423, 307, 767, 3671, 570, 321, 434, 2614, 490, 411, 2416, 565, 4190, 281, 1359, 565, 4190, 293, 341, 8289, 256, 510, 307, 406, 3442, 281, 312, 411, 364, 8236, 1823, 2744, 13, 51214], "temperature": 0.0, "avg_logprob": -0.15036773681640625, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.004535548388957977}, {"id": 724, "seek": 550880, "start": 5525.8, "end": 5528.8, "text": " So it's positive.", "tokens": [51214, 407, 309, 311, 3353, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15036773681640625, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.004535548388957977}, {"id": 725, "seek": 552880, "start": 5528.8, "end": 5544.8, "text": " So this runs out also this enchant to a sampling that I wash talked about in the first part of our tutorial, and most specifically the way he showed us how we can, how we can sample from these discrete time diffusion models.", "tokens": [50364, 407, 341, 6676, 484, 611, 341, 465, 24628, 281, 257, 21179, 300, 286, 390, 71, 2825, 466, 294, 264, 700, 644, 295, 527, 7073, 11, 293, 881, 4682, 264, 636, 415, 4712, 505, 577, 321, 393, 11, 577, 321, 393, 6889, 490, 613, 27706, 565, 25242, 5245, 13, 51164], "temperature": 0.0, "avg_logprob": -0.25540149211883545, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.05918180197477341}, {"id": 726, "seek": 552880, "start": 5544.8, "end": 5553.8, "text": " And this, this can actually be also considered a generative SDE sampler with this particular discretization used in that part.", "tokens": [51164, 400, 341, 11, 341, 393, 767, 312, 611, 4888, 257, 1337, 1166, 318, 22296, 3247, 22732, 365, 341, 1729, 25656, 2144, 1143, 294, 300, 644, 13, 51614], "temperature": 0.0, "avg_logprob": -0.25540149211883545, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.05918180197477341}, {"id": 727, "seek": 555380, "start": 5553.8, "end": 5560.8, "text": " So let's look at the probability flow ODE. How can we generate that, or using that.", "tokens": [50364, 407, 718, 311, 574, 412, 264, 8482, 3095, 422, 22296, 13, 1012, 393, 321, 8460, 300, 11, 420, 1228, 300, 13, 50714], "temperature": 0.0, "avg_logprob": -0.24750180971824517, "compression_ratio": 1.353658536585366, "no_speech_prob": 0.011865529231727123}, {"id": 728, "seek": 555380, "start": 5560.8, "end": 5569.8, "text": " Again, we could basically use Euler's method, which is analogous to the Euler-Maiorama approach and just now without this noise injection.", "tokens": [50714, 3764, 11, 321, 727, 1936, 764, 462, 26318, 311, 3170, 11, 597, 307, 16660, 563, 281, 264, 462, 26318, 12, 44, 1301, 32988, 3109, 293, 445, 586, 1553, 341, 5658, 22873, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24750180971824517, "compression_ratio": 1.353658536585366, "no_speech_prob": 0.011865529231727123}, {"id": 729, "seek": 556980, "start": 5569.8, "end": 5579.8, "text": " We would just intuitively evaluate our network and, yeah, the ODE function essentially do a small step, linear step for small time delta t.", "tokens": [50364, 492, 576, 445, 46506, 13059, 527, 3209, 293, 11, 1338, 11, 264, 422, 22296, 2445, 4476, 360, 257, 1359, 1823, 11, 8213, 1823, 337, 1359, 565, 8289, 256, 13, 50864], "temperature": 0.0, "avg_logprob": -0.21657457190044857, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0390288345515728}, {"id": 730, "seek": 556980, "start": 5579.8, "end": 5583.8, "text": " We evaluate and continue doing that.", "tokens": [50864, 492, 13059, 293, 2354, 884, 300, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21657457190044857, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0390288345515728}, {"id": 731, "seek": 556980, "start": 5583.8, "end": 5588.8, "text": " However, this is usually, I think nobody really does this in practice.", "tokens": [51064, 2908, 11, 341, 307, 2673, 11, 286, 519, 5079, 534, 775, 341, 294, 3124, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21657457190044857, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.0390288345515728}, {"id": 732, "seek": 558880, "start": 5588.8, "end": 5601.8, "text": " In practice, we can hear, as I mentioned earlier, really build on the advanced ordinary differential equation literature and use much better solvers and much better methods and higher order methods in particular.", "tokens": [50364, 682, 3124, 11, 321, 393, 1568, 11, 382, 286, 2835, 3071, 11, 534, 1322, 322, 264, 7339, 10547, 15756, 5367, 10394, 293, 764, 709, 1101, 1404, 840, 293, 709, 1101, 7150, 293, 2946, 1668, 7150, 294, 1729, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2016451109701128, "compression_ratio": 1.6517412935323383, "no_speech_prob": 0.022956470027565956}, {"id": 733, "seek": 558880, "start": 5601.8, "end": 5610.8, "text": " So what we see for instance is the use of one cutter methods of linear multi stepping methods, exponential integrators.", "tokens": [51014, 407, 437, 321, 536, 337, 5197, 307, 264, 764, 295, 472, 25531, 7150, 295, 8213, 4825, 16821, 7150, 11, 21510, 3572, 3391, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2016451109701128, "compression_ratio": 1.6517412935323383, "no_speech_prob": 0.022956470027565956}, {"id": 734, "seek": 561080, "start": 5610.8, "end": 5614.8, "text": " Yes, there was a lot of literature in that direction.", "tokens": [50364, 1079, 11, 456, 390, 257, 688, 295, 10394, 294, 300, 3513, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3583974322757205, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.010980112478137016}, {"id": 735, "seek": 561080, "start": 5614.8, "end": 5627.8, "text": " Yeah, like I just said, adaptive step size when you put a method that's been used, also called the stochastic differential equation actually adaptive step size higher order methods have been used.", "tokens": [50564, 865, 11, 411, 286, 445, 848, 11, 27912, 1823, 2744, 562, 291, 829, 257, 3170, 300, 311, 668, 1143, 11, 611, 1219, 264, 342, 8997, 2750, 15756, 5367, 767, 27912, 1823, 2744, 2946, 1668, 7150, 362, 668, 1143, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3583974322757205, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.010980112478137016}, {"id": 736, "seek": 561080, "start": 5627.8, "end": 5633.8, "text": " We parameterize the ODE has been proposed that also accelerates sampling.", "tokens": [51214, 492, 13075, 1125, 264, 422, 22296, 575, 668, 10348, 300, 611, 10172, 1024, 21179, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3583974322757205, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.010980112478137016}, {"id": 737, "seek": 563380, "start": 5633.8, "end": 5638.8, "text": " So, yeah, there's a lot of literature in that direction.", "tokens": [50364, 407, 11, 1338, 11, 456, 311, 257, 688, 295, 10394, 294, 300, 3513, 13, 50614], "temperature": 0.0, "avg_logprob": -0.4060257299622493, "compression_ratio": 1.6073298429319371, "no_speech_prob": 0.041431527584791183}, {"id": 738, "seek": 563380, "start": 5638.8, "end": 5652.8, "text": " And the main reason is that one drawback of fusion models is that sampling from them can be slow and contrast to like sampling from a generative adversarial network or variation auto encoder and such methods for instance, sampling from a differential", "tokens": [50614, 400, 264, 2135, 1778, 307, 300, 472, 2642, 3207, 295, 23100, 5245, 307, 300, 21179, 490, 552, 393, 312, 2964, 293, 8712, 281, 411, 21179, 490, 257, 1337, 1166, 17641, 44745, 3209, 420, 12990, 8399, 2058, 19866, 293, 1270, 7150, 337, 5197, 11, 21179, 490, 257, 15756, 51314], "temperature": 0.0, "avg_logprob": -0.4060257299622493, "compression_ratio": 1.6073298429319371, "no_speech_prob": 0.041431527584791183}, {"id": 739, "seek": 565280, "start": 5652.8, "end": 5671.8, "text": " model requires many, many function calls on what are specifically neural network evaluations in our case, because during each step of this iterative denoising, we have to call this neural network again so we often have to call it many, many times.", "tokens": [50364, 2316, 7029, 867, 11, 867, 2445, 5498, 322, 437, 366, 4682, 18161, 3209, 43085, 294, 527, 1389, 11, 570, 1830, 1184, 1823, 295, 341, 17138, 1166, 1441, 78, 3436, 11, 321, 362, 281, 818, 341, 18161, 3209, 797, 370, 321, 2049, 362, 281, 818, 309, 867, 11, 867, 1413, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1888095361215097, "compression_ratio": 1.625, "no_speech_prob": 0.05178660526871681}, {"id": 740, "seek": 567180, "start": 5671.8, "end": 5683.8, "text": " And yeah, so this is why we want to use efficient solvers so that we can reduce this number of neural network evaluations that we have to use.", "tokens": [50364, 400, 1338, 11, 370, 341, 307, 983, 321, 528, 281, 764, 7148, 1404, 840, 370, 300, 321, 393, 5407, 341, 1230, 295, 18161, 3209, 43085, 300, 321, 362, 281, 764, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14324625560215543, "compression_ratio": 1.3271028037383177, "no_speech_prob": 0.013843398541212082}, {"id": 741, "seek": 568380, "start": 5684.8, "end": 5704.8, "text": " So now I have talked about how you can use how you can solve the SDS and the ODE and practice, but what should you use, should you actually rather build on the SD or the ODE framework when you want to sample from the model.", "tokens": [50414, 407, 586, 286, 362, 2825, 466, 577, 291, 393, 764, 577, 291, 393, 5039, 264, 318, 11844, 293, 264, 422, 22296, 293, 3124, 11, 457, 437, 820, 291, 764, 11, 820, 291, 767, 2831, 1322, 322, 264, 14638, 420, 264, 422, 22296, 8388, 562, 291, 528, 281, 6889, 490, 264, 2316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1989589078085763, "compression_ratio": 1.5379310344827586, "no_speech_prob": 0.0023593022488057613}, {"id": 742, "seek": 570480, "start": 5705.8, "end": 5712.8, "text": " So to shine some light into that, let us look at the generative diffusion SD a little bit closer.", "tokens": [50414, 407, 281, 12207, 512, 1442, 666, 300, 11, 718, 505, 574, 412, 264, 1337, 1166, 25242, 14638, 257, 707, 857, 4966, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1683367901161069, "compression_ratio": 1.3741935483870968, "no_speech_prob": 0.01168442890048027}, {"id": 743, "seek": 570480, "start": 5712.8, "end": 5714.8, "text": " So it's like that.", "tokens": [50764, 407, 309, 311, 411, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1683367901161069, "compression_ratio": 1.3741935483870968, "no_speech_prob": 0.01168442890048027}, {"id": 744, "seek": 570480, "start": 5714.8, "end": 5718.8, "text": " But now we can actually decompose this into two terms.", "tokens": [50864, 583, 586, 321, 393, 767, 22867, 541, 341, 666, 732, 2115, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1683367901161069, "compression_ratio": 1.3741935483870968, "no_speech_prob": 0.01168442890048027}, {"id": 745, "seek": 570480, "start": 5718.8, "end": 5721.8, "text": " Right, so this is just from here to here.", "tokens": [51064, 1779, 11, 370, 341, 307, 445, 490, 510, 281, 510, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1683367901161069, "compression_ratio": 1.3741935483870968, "no_speech_prob": 0.01168442890048027}, {"id": 746, "seek": 572180, "start": 5721.8, "end": 5732.8, "text": " And it turns out the first term is really just the probability flow ODE that we have seen already, which itself can be used for deterministic data generation like you're on the right.", "tokens": [50364, 400, 309, 4523, 484, 264, 700, 1433, 307, 534, 445, 264, 8482, 3095, 422, 22296, 300, 321, 362, 1612, 1217, 11, 597, 2564, 393, 312, 1143, 337, 15957, 3142, 1412, 5125, 411, 291, 434, 322, 264, 558, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19310408372145432, "compression_ratio": 1.4025157232704402, "no_speech_prob": 0.008845143020153046}, {"id": 747, "seek": 572180, "start": 5732.8, "end": 5735.8, "text": " But then there is this additional term.", "tokens": [50914, 583, 550, 456, 307, 341, 4497, 1433, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19310408372145432, "compression_ratio": 1.4025157232704402, "no_speech_prob": 0.008845143020153046}, {"id": 748, "seek": 573580, "start": 5736.8, "end": 5740.8, "text": " So this code basically corresponds to the noise injection.", "tokens": [50414, 407, 341, 3089, 1936, 23249, 281, 264, 5658, 22873, 13, 50614], "temperature": 0.0, "avg_logprob": -0.24767090479532877, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.005729124415665865}, {"id": 749, "seek": 573580, "start": 5740.8, "end": 5743.8, "text": " And yeah, it has the noise injection here.", "tokens": [50614, 400, 1338, 11, 309, 575, 264, 5658, 22873, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.24767090479532877, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.005729124415665865}, {"id": 750, "seek": 573580, "start": 5743.8, "end": 5746.8, "text": " So what what do these terms do.", "tokens": [50764, 407, 437, 437, 360, 613, 2115, 360, 13, 50914], "temperature": 0.0, "avg_logprob": -0.24767090479532877, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.005729124415665865}, {"id": 751, "seek": 573580, "start": 5746.8, "end": 5754.8, "text": " So this probability flow ODE term is essentially responsible for your pushing us from the right to the left here.", "tokens": [50914, 407, 341, 8482, 3095, 422, 22296, 1433, 307, 4476, 6250, 337, 428, 7380, 505, 490, 264, 558, 281, 264, 1411, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.24767090479532877, "compression_ratio": 1.5534591194968554, "no_speech_prob": 0.005729124415665865}, {"id": 752, "seek": 575480, "start": 5754.8, "end": 5765.8, "text": " And this logical diffusion SD term, what it basically does is for each individual team, it actively pushes us towards correct diffuse data distribution.", "tokens": [50364, 400, 341, 14978, 25242, 14638, 1433, 11, 437, 309, 1936, 775, 307, 337, 1184, 2609, 1469, 11, 309, 13022, 21020, 505, 3030, 3006, 42165, 1412, 7316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3113235897488064, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0012840945273637772}, {"id": 753, "seek": 575480, "start": 5765.8, "end": 5775.8, "text": " But because of this, so when I do ours during my soul during my simulation going from the right to left you're going from noise to data.", "tokens": [50914, 583, 570, 295, 341, 11, 370, 562, 286, 360, 11896, 1830, 452, 5133, 1830, 452, 16575, 516, 490, 264, 558, 281, 1411, 291, 434, 516, 490, 5658, 281, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3113235897488064, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0012840945273637772}, {"id": 754, "seek": 577580, "start": 5775.8, "end": 5794.8, "text": " If I have ever said, then this logical diffusion SD can help us to correct these errors and actively bring us back to the right data manifold back to the fused data distribution.", "tokens": [50364, 759, 286, 362, 1562, 848, 11, 550, 341, 14978, 25242, 14638, 393, 854, 505, 281, 3006, 613, 13603, 293, 13022, 1565, 505, 646, 281, 264, 558, 1412, 47138, 646, 281, 264, 283, 4717, 1412, 7316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3111929655075073, "compression_ratio": 1.3484848484848484, "no_speech_prob": 0.04466458782553673}, {"id": 755, "seek": 579480, "start": 5794.8, "end": 5799.8, "text": " So this, yeah, this is an advantage can do some sort of error correction.", "tokens": [50364, 407, 341, 11, 1338, 11, 341, 307, 364, 5002, 393, 360, 512, 1333, 295, 6713, 19984, 13, 50614], "temperature": 0.0, "avg_logprob": -0.18758724956977657, "compression_ratio": 1.49, "no_speech_prob": 0.01639929600059986}, {"id": 756, "seek": 579480, "start": 5799.8, "end": 5809.8, "text": " On the other hand, it's, it's often slower because this term itself requires a somewhat fine discretization during the solve.", "tokens": [50614, 1282, 264, 661, 1011, 11, 309, 311, 11, 309, 311, 2049, 14009, 570, 341, 1433, 2564, 7029, 257, 8344, 2489, 25656, 2144, 1830, 264, 5039, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18758724956977657, "compression_ratio": 1.49, "no_speech_prob": 0.01639929600059986}, {"id": 757, "seek": 579480, "start": 5809.8, "end": 5811.8, "text": " Yeah.", "tokens": [51114, 865, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18758724956977657, "compression_ratio": 1.49, "no_speech_prob": 0.01639929600059986}, {"id": 758, "seek": 579480, "start": 5811.8, "end": 5814.8, "text": " So now let's look at the probability flow ODE.", "tokens": [51214, 407, 586, 718, 311, 574, 412, 264, 8482, 3095, 422, 22296, 13, 51364], "temperature": 0.0, "avg_logprob": -0.18758724956977657, "compression_ratio": 1.49, "no_speech_prob": 0.01639929600059986}, {"id": 759, "seek": 579480, "start": 5814.8, "end": 5818.8, "text": " So in that case, we do not have this SD term.", "tokens": [51364, 407, 294, 300, 1389, 11, 321, 360, 406, 362, 341, 14638, 1433, 13, 51564], "temperature": 0.0, "avg_logprob": -0.18758724956977657, "compression_ratio": 1.49, "no_speech_prob": 0.01639929600059986}, {"id": 760, "seek": 581880, "start": 5819.8, "end": 5825.8, "text": " However, we can now leverage these really fast ODE solvers.", "tokens": [50414, 2908, 11, 321, 393, 586, 13982, 613, 534, 2370, 422, 22296, 1404, 840, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15900078797951722, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.011681783013045788}, {"id": 761, "seek": 581880, "start": 5825.8, "end": 5829.8, "text": " And so this is good when we target very fast sampling.", "tokens": [50714, 400, 370, 341, 307, 665, 562, 321, 3779, 588, 2370, 21179, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15900078797951722, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.011681783013045788}, {"id": 762, "seek": 581880, "start": 5829.8, "end": 5834.8, "text": " On the other hand, there is no stochastic error correction going on here.", "tokens": [50914, 1282, 264, 661, 1011, 11, 456, 307, 572, 342, 8997, 2750, 6713, 19984, 516, 322, 510, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15900078797951722, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.011681783013045788}, {"id": 763, "seek": 581880, "start": 5834.8, "end": 5841.8, "text": " And because of this, what we see in practice is that this is sometimes slightly lower performing than the stochastic sampling.", "tokens": [51164, 400, 570, 295, 341, 11, 437, 321, 536, 294, 3124, 307, 300, 341, 307, 2171, 4748, 3126, 10205, 813, 264, 342, 8997, 2750, 21179, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15900078797951722, "compression_ratio": 1.5829145728643217, "no_speech_prob": 0.011681783013045788}, {"id": 764, "seek": 584180, "start": 5841.8, "end": 5844.8, "text": " When we just look at the quality of the samples.", "tokens": [50364, 1133, 321, 445, 574, 412, 264, 3125, 295, 264, 10938, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1640280113845575, "compression_ratio": 1.4228571428571428, "no_speech_prob": 0.15186554193496704}, {"id": 765, "seek": 584180, "start": 5844.8, "end": 5862.8, "text": " So to summarize what we see is, if we're not concerned about our budget of like neural network evaluations and we're willing to do like very a lot of steps, then this SDE framework can be very useful.", "tokens": [50514, 407, 281, 20858, 437, 321, 536, 307, 11, 498, 321, 434, 406, 5922, 466, 527, 4706, 295, 411, 18161, 3209, 43085, 293, 321, 434, 4950, 281, 360, 411, 588, 257, 688, 295, 4439, 11, 550, 341, 14638, 36, 8388, 393, 312, 588, 4420, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1640280113845575, "compression_ratio": 1.4228571428571428, "no_speech_prob": 0.15186554193496704}, {"id": 766, "seek": 586280, "start": 5862.8, "end": 5871.8, "text": " But if we want to go as fast as possible, then probably the ODE framework is better where we then can leverage these really fast solvers.", "tokens": [50364, 583, 498, 321, 528, 281, 352, 382, 2370, 382, 1944, 11, 550, 1391, 264, 422, 22296, 8388, 307, 1101, 689, 321, 550, 393, 13982, 613, 534, 2370, 1404, 840, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23062227733099638, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.06949584931135178}, {"id": 767, "seek": 586280, "start": 5871.8, "end": 5878.8, "text": " It is also worth mentioning that we can do things in between where we have only like this logic in diffusion SDE.", "tokens": [50814, 467, 307, 611, 3163, 18315, 300, 321, 393, 360, 721, 294, 1296, 689, 321, 362, 787, 411, 341, 9952, 294, 25242, 14638, 36, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23062227733099638, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.06949584931135178}, {"id": 768, "seek": 586280, "start": 5878.8, "end": 5880.8, "text": " Active a little bit.", "tokens": [51164, 26635, 257, 707, 857, 13, 51264], "temperature": 0.0, "avg_logprob": -0.23062227733099638, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.06949584931135178}, {"id": 769, "seek": 588080, "start": 5880.8, "end": 5891.8, "text": " We can also, you know, like kind of solve for the first half using stochastic sampling and then afterwards switch to the ODE advanced methods are possible.", "tokens": [50364, 492, 393, 611, 11, 291, 458, 11, 411, 733, 295, 5039, 337, 264, 700, 1922, 1228, 342, 8997, 2750, 21179, 293, 550, 10543, 3679, 281, 264, 422, 22296, 7339, 7150, 366, 1944, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15423347891830816, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.005138494540005922}, {"id": 770, "seek": 588080, "start": 5891.8, "end": 5900.8, "text": " I would like to refer you to this paper here with which discusses some of these things in quite some detail.", "tokens": [50914, 286, 576, 411, 281, 2864, 291, 281, 341, 3035, 510, 365, 597, 2248, 279, 512, 295, 613, 721, 294, 1596, 512, 2607, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15423347891830816, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.005138494540005922}, {"id": 771, "seek": 588080, "start": 5900.8, "end": 5906.8, "text": " Next, I would like to talk about a connection between diffusion models and energy based models.", "tokens": [51364, 3087, 11, 286, 576, 411, 281, 751, 466, 257, 4984, 1296, 25242, 5245, 293, 2281, 2361, 5245, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15423347891830816, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.005138494540005922}, {"id": 772, "seek": 590680, "start": 5906.8, "end": 5910.8, "text": " So what are energy based models?", "tokens": [50364, 407, 437, 366, 2281, 2361, 5245, 30, 50564], "temperature": 0.0, "avg_logprob": -0.22157755494117737, "compression_ratio": 1.6863905325443787, "no_speech_prob": 0.017434926703572273}, {"id": 773, "seek": 590680, "start": 5910.8, "end": 5914.8, "text": " Energy based models are defined like this in an energy based model.", "tokens": [50564, 14939, 2361, 5245, 366, 7642, 411, 341, 294, 364, 2281, 2361, 2316, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22157755494117737, "compression_ratio": 1.6863905325443787, "no_speech_prob": 0.017434926703572273}, {"id": 774, "seek": 590680, "start": 5914.8, "end": 5927.8, "text": " The probability distribution that we want to model the setter of x defined through an exponential to the power of minus a scalar energy function, which is now the function of the data.", "tokens": [50764, 440, 8482, 7316, 300, 321, 528, 281, 2316, 264, 992, 391, 295, 2031, 7642, 807, 364, 21510, 281, 264, 1347, 295, 3175, 257, 39684, 2281, 2445, 11, 597, 307, 586, 264, 2445, 295, 264, 1412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.22157755494117737, "compression_ratio": 1.6863905325443787, "no_speech_prob": 0.017434926703572273}, {"id": 775, "seek": 592780, "start": 5928.8, "end": 5935.8, "text": " And then this thing is normalized by a normalization constant to make sure it's a well defined probability distribution.", "tokens": [50414, 400, 550, 341, 551, 307, 48704, 538, 257, 2710, 2144, 5754, 281, 652, 988, 309, 311, 257, 731, 7642, 8482, 7316, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16956815964136368, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.018259577453136444}, {"id": 776, "seek": 592780, "start": 5935.8, "end": 5940.8, "text": " This normalization constant is also called sometimes called the partition function.", "tokens": [50764, 639, 2710, 2144, 5754, 307, 611, 1219, 2171, 1219, 264, 24808, 2445, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16956815964136368, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.018259577453136444}, {"id": 777, "seek": 592780, "start": 5940.8, "end": 5953.8, "text": " Furthermore, in this case, I have added a time variable t because this energy based model is now supposed to represent the diffuse data distributions for different t's in our case.", "tokens": [51014, 23999, 11, 294, 341, 1389, 11, 286, 362, 3869, 257, 565, 7006, 256, 570, 341, 2281, 2361, 2316, 307, 586, 3442, 281, 2906, 264, 42165, 1412, 37870, 337, 819, 256, 311, 294, 527, 1389, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16956815964136368, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.018259577453136444}, {"id": 778, "seek": 595380, "start": 5954.8, "end": 5964.8, "text": " When we want to sample an energy based model, we usually do that by larger than dynamics, which is if we have seen larger than dynamics already.", "tokens": [50414, 1133, 321, 528, 281, 6889, 364, 2281, 2361, 2316, 11, 321, 2673, 360, 300, 538, 4833, 813, 15679, 11, 597, 307, 498, 321, 362, 1612, 4833, 813, 15679, 1217, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18405838012695314, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0006461161538027227}, {"id": 779, "seek": 595380, "start": 5964.8, "end": 5971.8, "text": " Basically, this is very closely connected to these stochastic differential equations we have already discussed.", "tokens": [50914, 8537, 11, 341, 307, 588, 8185, 4582, 281, 613, 342, 8997, 2750, 15756, 11787, 321, 362, 1217, 7152, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18405838012695314, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0006461161538027227}, {"id": 780, "seek": 597180, "start": 5971.8, "end": 5980.8, "text": " So to do this larger than dynamics sampling, we basically require the gradient of this scalar energy function.", "tokens": [50364, 407, 281, 360, 341, 4833, 813, 15679, 21179, 11, 321, 1936, 3651, 264, 16235, 295, 341, 39684, 2281, 2445, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2478605854895807, "compression_ratio": 1.5055555555555555, "no_speech_prob": 0.0023593709338456392}, {"id": 781, "seek": 597180, "start": 5980.8, "end": 5984.8, "text": " And then we also iteratively update our sample with that.", "tokens": [50814, 400, 550, 321, 611, 17138, 19020, 5623, 527, 6889, 365, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2478605854895807, "compression_ratio": 1.5055555555555555, "no_speech_prob": 0.0023593709338456392}, {"id": 782, "seek": 597180, "start": 5984.8, "end": 5993.8, "text": " And we also have like an additional noise term atter and some step size of learning weight atter here.", "tokens": [51014, 400, 321, 611, 362, 411, 364, 4497, 5658, 1433, 412, 391, 293, 512, 1823, 2744, 295, 2539, 3364, 412, 391, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2478605854895807, "compression_ratio": 1.5055555555555555, "no_speech_prob": 0.0023593709338456392}, {"id": 783, "seek": 599380, "start": 5993.8, "end": 6004.8, "text": " The important part to realize is when we do when we use these energy based models is that in practice, even though we are learning the scalar energy function.", "tokens": [50364, 440, 1021, 644, 281, 4325, 307, 562, 321, 360, 562, 321, 764, 613, 2281, 2361, 5245, 307, 300, 294, 3124, 11, 754, 1673, 321, 366, 2539, 264, 39684, 2281, 2445, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12515514950419582, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0008829458965919912}, {"id": 784, "seek": 599380, "start": 6004.8, "end": 6014.8, "text": " We only require the gradient of this energy function or more specifically the negative gradient of this energy function for sampling the model at the end.", "tokens": [50914, 492, 787, 3651, 264, 16235, 295, 341, 2281, 2445, 420, 544, 4682, 264, 3671, 16235, 295, 341, 2281, 2445, 337, 21179, 264, 2316, 412, 264, 917, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12515514950419582, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0008829458965919912}, {"id": 785, "seek": 599380, "start": 6014.8, "end": 6021.8, "text": " We do not require the energy function itself, nor do we require the partition function depth setter.", "tokens": [51414, 492, 360, 406, 3651, 264, 2281, 2445, 2564, 11, 6051, 360, 321, 3651, 264, 24808, 2445, 7161, 992, 391, 13, 51764], "temperature": 0.0, "avg_logprob": -0.12515514950419582, "compression_ratio": 1.9255813953488372, "no_speech_prob": 0.0008829458965919912}, {"id": 786, "seek": 602180, "start": 6021.8, "end": 6029.8, "text": " By the way, this atter is implicitly defined through this energy itself that we're learning.", "tokens": [50364, 3146, 264, 636, 11, 341, 412, 391, 307, 26947, 356, 7642, 807, 341, 2281, 2564, 300, 321, 434, 2539, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1259618787204518, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.479543612338603e-05}, {"id": 787, "seek": 602180, "start": 6029.8, "end": 6039.8, "text": " So now it turns out that in diffusion models, what we're basically learning is the energy gradients for all these diffuse data distributions directly.", "tokens": [50764, 407, 586, 309, 4523, 484, 300, 294, 25242, 5245, 11, 437, 321, 434, 1936, 2539, 307, 264, 2281, 2771, 2448, 337, 439, 613, 42165, 1412, 37870, 3838, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1259618787204518, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.479543612338603e-05}, {"id": 788, "seek": 602180, "start": 6039.8, "end": 6043.8, "text": " We are not learning energies, but basically energy gradients.", "tokens": [51264, 492, 366, 406, 2539, 25737, 11, 457, 1936, 2281, 2771, 2448, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1259618787204518, "compression_ratio": 1.6666666666666667, "no_speech_prob": 8.479543612338603e-05}, {"id": 789, "seek": 604380, "start": 6043.8, "end": 6058.8, "text": " And one thing I want to add is that because in this EDM spirit, we're directly learning these energies and we have the probability, an expression for the probability distribution while also taking into account this partition function.", "tokens": [50364, 400, 472, 551, 286, 528, 281, 909, 307, 300, 570, 294, 341, 18050, 44, 3797, 11, 321, 434, 3838, 2539, 613, 25737, 293, 321, 362, 264, 8482, 11, 364, 6114, 337, 264, 8482, 7316, 1339, 611, 1940, 666, 2696, 341, 24808, 2445, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2567207245599656, "compression_ratio": 1.5817490494296578, "no_speech_prob": 0.006795149762183428}, {"id": 790, "seek": 604380, "start": 6058.8, "end": 6063.8, "text": " Because of this training energy based models can be actually really complex.", "tokens": [51114, 1436, 295, 341, 3097, 2281, 2361, 5245, 393, 312, 767, 534, 3997, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2567207245599656, "compression_ratio": 1.5817490494296578, "no_speech_prob": 0.006795149762183428}, {"id": 791, "seek": 604380, "start": 6063.8, "end": 6069.8, "text": " This often requires advanced Markov chain Monte Carlo methods, which can be very difficult to deal with.", "tokens": [51364, 639, 2049, 7029, 7339, 3934, 5179, 5021, 38105, 45112, 7150, 11, 597, 393, 312, 588, 2252, 281, 2028, 365, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2567207245599656, "compression_ratio": 1.5817490494296578, "no_speech_prob": 0.006795149762183428}, {"id": 792, "seek": 606980, "start": 6070.8, "end": 6073.8, "text": " One of that is available, I would add.", "tokens": [50414, 1485, 295, 300, 307, 2435, 11, 286, 576, 909, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3165107154846191, "compression_ratio": 1.3591549295774648, "no_speech_prob": 0.001648169243708253}, {"id": 793, "seek": 606980, "start": 6073.8, "end": 6080.8, "text": " But yeah, diffusion models, we kind of circumvent that and we only directly learn these energy gradients.", "tokens": [50564, 583, 1338, 11, 25242, 5245, 11, 321, 733, 295, 7125, 2475, 300, 293, 321, 787, 3838, 1466, 613, 2281, 2771, 2448, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3165107154846191, "compression_ratio": 1.3591549295774648, "no_speech_prob": 0.001648169243708253}, {"id": 794, "seek": 606980, "start": 6080.8, "end": 6084.8, "text": " Tell me somehow show that and derive that maybe.", "tokens": [50914, 5115, 385, 6063, 855, 300, 293, 28446, 300, 1310, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3165107154846191, "compression_ratio": 1.3591549295774648, "no_speech_prob": 0.001648169243708253}, {"id": 795, "seek": 608480, "start": 6084.8, "end": 6103.8, "text": " So to this end, let's recall again that in diffusion models, our neural network basically we're trying to approximate our model more generally, we're trying to approximate this score function of the diffuse data distributions qt of x.", "tokens": [50364, 407, 281, 341, 917, 11, 718, 311, 9901, 797, 300, 294, 25242, 5245, 11, 527, 18161, 3209, 1936, 321, 434, 1382, 281, 30874, 527, 2316, 544, 5101, 11, 321, 434, 1382, 281, 30874, 341, 6175, 2445, 295, 264, 42165, 1412, 37870, 9505, 83, 295, 2031, 13, 51314], "temperature": 0.0, "avg_logprob": -0.25383031845092774, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.001187793561257422}, {"id": 796, "seek": 610380, "start": 6103.8, "end": 6114.8, "text": " Now let us suppose that our model is parametrized such that the diffuse data distributions qt are given by this energy based model here.", "tokens": [50364, 823, 718, 505, 7297, 300, 527, 2316, 307, 6220, 302, 470, 11312, 1270, 300, 264, 42165, 1412, 37870, 9505, 83, 366, 2212, 538, 341, 2281, 2361, 2316, 510, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2507119722004178, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0034826097544282675}, {"id": 797, "seek": 610380, "start": 6114.8, "end": 6115.8, "text": " Right.", "tokens": [50914, 1779, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2507119722004178, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0034826097544282675}, {"id": 798, "seek": 610380, "start": 6115.8, "end": 6121.8, "text": " So now let us insert this p theta here and yeah through the map.", "tokens": [50964, 407, 586, 718, 505, 8969, 341, 280, 9725, 510, 293, 1338, 807, 264, 4471, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2507119722004178, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0034826097544282675}, {"id": 799, "seek": 610380, "start": 6121.8, "end": 6129.8, "text": " We apply the logarithm both here, both on e to the minus the scalar energy function and also the denominator.", "tokens": [51264, 492, 3079, 264, 41473, 32674, 1293, 510, 11, 1293, 322, 308, 281, 264, 3175, 264, 39684, 2281, 2445, 293, 611, 264, 20687, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2507119722004178, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0034826097544282675}, {"id": 800, "seek": 612980, "start": 6129.8, "end": 6135.8, "text": " However, this term drops out because the partition function does not depend on the state x.", "tokens": [50364, 2908, 11, 341, 1433, 11438, 484, 570, 264, 24808, 2445, 775, 406, 5672, 322, 264, 1785, 2031, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1801801349805749, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.004398766439408064}, {"id": 801, "seek": 612980, "start": 6135.8, "end": 6140.8, "text": " And what we are left with is just a negative gradient of the energy.", "tokens": [50664, 400, 437, 321, 366, 1411, 365, 307, 445, 257, 3671, 16235, 295, 264, 2281, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1801801349805749, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.004398766439408064}, {"id": 802, "seek": 612980, "start": 6140.8, "end": 6141.8, "text": " What does this mean?", "tokens": [50914, 708, 775, 341, 914, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1801801349805749, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.004398766439408064}, {"id": 803, "seek": 612980, "start": 6141.8, "end": 6148.8, "text": " So this means that this neural network s that we usually have in diffusion models to model the score function.", "tokens": [50964, 407, 341, 1355, 300, 341, 18161, 3209, 262, 300, 321, 2673, 362, 294, 25242, 5245, 281, 2316, 264, 6175, 2445, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1801801349805749, "compression_ratio": 1.553191489361702, "no_speech_prob": 0.004398766439408064}, {"id": 804, "seek": 614880, "start": 6148.8, "end": 6162.8, "text": " It means that it essentially learns the negative energy gradients of the energy model based model that would describe the diffuse data distribution.", "tokens": [50364, 467, 1355, 300, 309, 4476, 27152, 264, 3671, 2281, 2771, 2448, 295, 264, 2281, 2316, 2361, 2316, 300, 576, 6786, 264, 42165, 1412, 7316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2015452713801943, "compression_ratio": 1.4230769230769231, "no_speech_prob": 0.02886192873120308}, {"id": 805, "seek": 616280, "start": 6162.8, "end": 6180.8, "text": " So yeah, once again, fusion models kind of circumvent these complications and directly model the energy gradients and say avoid modeling this partition function explicitly for instance which leads to some of these difficulties that we have in classical energy based model training.", "tokens": [50364, 407, 1338, 11, 1564, 797, 11, 23100, 5245, 733, 295, 7125, 2475, 613, 26566, 293, 3838, 2316, 264, 2281, 2771, 2448, 293, 584, 5042, 15983, 341, 24808, 2445, 20803, 337, 5197, 597, 6689, 281, 512, 295, 613, 14399, 300, 321, 362, 294, 13735, 2281, 2361, 2316, 3097, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1763631563920241, "compression_ratio": 1.5611111111111111, "no_speech_prob": 0.0018099891021847725}, {"id": 806, "seek": 618080, "start": 6180.8, "end": 6192.8, "text": " Also, these different noise levels that we have in diffusion models. This is actually analogous to a mere sampling and energy based models.", "tokens": [50364, 2743, 11, 613, 819, 5658, 4358, 300, 321, 362, 294, 25242, 5245, 13, 639, 307, 767, 16660, 563, 281, 257, 8401, 21179, 293, 2281, 2361, 5245, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17156264361213236, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.044649649411439896}, {"id": 807, "seek": 618080, "start": 6192.8, "end": 6199.8, "text": " I would like to talk about one more thing about diffusion models, which is unique identity.", "tokens": [50964, 286, 576, 411, 281, 751, 466, 472, 544, 551, 466, 25242, 5245, 11, 597, 307, 3845, 6575, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17156264361213236, "compression_ratio": 1.4713375796178343, "no_speech_prob": 0.044649649411439896}, {"id": 808, "seek": 619980, "start": 6199.8, "end": 6213.8, "text": " It turns out that the denoising model that we're learning in these diffusion models that is supposed to approximate the score function of the diffused data to t of x p.", "tokens": [50364, 467, 4523, 484, 300, 264, 1441, 78, 3436, 2316, 300, 321, 434, 2539, 294, 613, 25242, 5245, 300, 307, 3442, 281, 30874, 264, 6175, 2445, 295, 264, 7593, 4717, 1412, 281, 256, 295, 2031, 280, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12529844137338492, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.09001343697309494}, {"id": 809, "seek": 619980, "start": 6213.8, "end": 6222.8, "text": " This denoising model is in principle uniquely determined by the data that we're given and the forward diffusion process.", "tokens": [51064, 639, 1441, 78, 3436, 2316, 307, 294, 8665, 31474, 9540, 538, 264, 1412, 300, 321, 434, 2212, 293, 264, 2128, 25242, 1399, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12529844137338492, "compression_ratio": 1.660919540229885, "no_speech_prob": 0.09001343697309494}, {"id": 810, "seek": 622280, "start": 6223.8, "end": 6235.8, "text": " And not only the score model, so and by learning the score model also these data encodings that we obtain by using the probability flow of the e to deterministically encode data in the latency space.", "tokens": [50414, 400, 406, 787, 264, 6175, 2316, 11, 370, 293, 538, 2539, 264, 6175, 2316, 611, 613, 1412, 2058, 378, 1109, 300, 321, 12701, 538, 1228, 264, 8482, 3095, 295, 264, 308, 281, 15957, 20458, 2058, 1429, 1412, 294, 264, 27043, 1901, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22164913865386462, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.020953388884663582}, {"id": 811, "seek": 622280, "start": 6235.8, "end": 6240.8, "text": " All this is uniquely determined by the data and the forward diffusion.", "tokens": [51014, 1057, 341, 307, 31474, 9540, 538, 264, 1412, 293, 264, 2128, 25242, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22164913865386462, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.020953388884663582}, {"id": 812, "seek": 624080, "start": 6240.8, "end": 6265.8, "text": " What this means is that even if we use different neural network architectures for us and different network initialization, we should at the end recover identical model outputs like identical score function outputs and data encodings in the latency space assuming we have sufficient training data model capacity and optimization accuracy.", "tokens": [50364, 708, 341, 1355, 307, 300, 754, 498, 321, 764, 819, 18161, 3209, 6331, 1303, 337, 505, 293, 819, 3209, 5883, 2144, 11, 321, 820, 412, 264, 917, 8114, 14800, 2316, 23930, 411, 14800, 6175, 2445, 23930, 293, 1412, 2058, 378, 1109, 294, 264, 27043, 1901, 11926, 321, 362, 11563, 3097, 1412, 2316, 6042, 293, 19618, 14170, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18513063524590165, "compression_ratio": 1.6600985221674878, "no_speech_prob": 0.046699609607458115}, {"id": 813, "seek": 626580, "start": 6266.8, "end": 6274.8, "text": " This is in contrast, for instance, to generate adversarial networks or variational auto encoders which do not have this property.", "tokens": [50414, 639, 307, 294, 8712, 11, 337, 5197, 11, 281, 8460, 17641, 44745, 9590, 420, 3034, 1478, 8399, 2058, 378, 433, 597, 360, 406, 362, 341, 4707, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15930530636809592, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.005907855462282896}, {"id": 814, "seek": 626580, "start": 6274.8, "end": 6286.8, "text": " Because these models, depending on what kind of architectures we use and what kind of initializations we use, we will always obtain like somewhat different models and yet different data encodings and so on.", "tokens": [50814, 1436, 613, 5245, 11, 5413, 322, 437, 733, 295, 6331, 1303, 321, 764, 293, 437, 733, 295, 5883, 14455, 321, 764, 11, 321, 486, 1009, 12701, 411, 8344, 819, 5245, 293, 1939, 819, 1412, 2058, 378, 1109, 293, 370, 322, 13, 51414], "temperature": 0.0, "avg_logprob": -0.15930530636809592, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.005907855462282896}, {"id": 815, "seek": 626580, "start": 6286.8, "end": 6291.8, "text": " This is the unique property about these diffusion models.", "tokens": [51414, 639, 307, 264, 3845, 4707, 466, 613, 25242, 5245, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15930530636809592, "compression_ratio": 1.6982758620689655, "no_speech_prob": 0.005907855462282896}, {"id": 816, "seek": 629180, "start": 6291.8, "end": 6307.8, "text": " Here's an example. What we are seeing here is the first 100 dimensions of the latent code obtained from a random cyber tent image that was encoded in the latency space of a diffusion model using this probability flow or the e approach.", "tokens": [50364, 1692, 311, 364, 1365, 13, 708, 321, 366, 2577, 510, 307, 264, 700, 2319, 12819, 295, 264, 48994, 3089, 14879, 490, 257, 4974, 13411, 7054, 3256, 300, 390, 2058, 12340, 294, 264, 27043, 1901, 295, 257, 25242, 2316, 1228, 341, 8482, 3095, 420, 264, 308, 3109, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24601594139547908, "compression_ratio": 1.4506172839506173, "no_speech_prob": 0.006382064428180456}, {"id": 817, "seek": 630780, "start": 6307.8, "end": 6317.8, "text": " We did this, most specifically Song and I did this with two different models and model architectures that were separately trained.", "tokens": [50364, 492, 630, 341, 11, 881, 4682, 11862, 293, 286, 630, 341, 365, 732, 819, 5245, 293, 2316, 6331, 1303, 300, 645, 14759, 8895, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1795382575383262, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.02259061671793461}, {"id": 818, "seek": 630780, "start": 6317.8, "end": 6332.8, "text": " However, both of these encodings distributions here as we see, they are almost the same, they are almost identical, even though these were different architectures.", "tokens": [50864, 2908, 11, 1293, 295, 613, 2058, 378, 1109, 37870, 510, 382, 321, 536, 11, 436, 366, 1920, 264, 912, 11, 436, 366, 1920, 14800, 11, 754, 1673, 613, 645, 819, 6331, 1303, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1795382575383262, "compression_ratio": 1.6424581005586592, "no_speech_prob": 0.02259061671793461}, {"id": 819, "seek": 633280, "start": 6332.8, "end": 6337.8, "text": " With that, I would like to come to a conclusion and briefly summarize.", "tokens": [50364, 2022, 300, 11, 286, 576, 411, 281, 808, 281, 257, 10063, 293, 10515, 20858, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11986381943161423, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.008845389820635319}, {"id": 820, "seek": 633280, "start": 6337.8, "end": 6346.8, "text": " So in this part of this talk, I have introduced you to this continuous time diffusion framework in contrast to what Arash talked about in step one.", "tokens": [50614, 407, 294, 341, 644, 295, 341, 751, 11, 286, 362, 7268, 291, 281, 341, 10957, 565, 25242, 8388, 294, 8712, 281, 437, 1587, 1299, 2825, 466, 294, 1823, 472, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11986381943161423, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.008845389820635319}, {"id": 821, "seek": 633280, "start": 6346.8, "end": 6353.8, "text": " We do not have finite size denoising and diffusion steps anymore and only a finite number of cells.", "tokens": [51064, 492, 360, 406, 362, 19362, 2744, 1441, 78, 3436, 293, 25242, 4439, 3602, 293, 787, 257, 19362, 1230, 295, 5438, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11986381943161423, "compression_ratio": 1.5665024630541873, "no_speech_prob": 0.008845389820635319}, {"id": 822, "seek": 635380, "start": 6353.8, "end": 6364.8, "text": " Rather, we consider continuous perturbations, a continuous forward diffusion process and then also a continuous generative process based on differential equations.", "tokens": [50364, 16571, 11, 321, 1949, 10957, 40468, 763, 11, 257, 10957, 2128, 25242, 1399, 293, 550, 611, 257, 10957, 1337, 1166, 1399, 2361, 322, 15756, 11787, 13, 50914], "temperature": 0.0, "avg_logprob": -0.17857538570057263, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.012817921116948128}, {"id": 823, "seek": 635380, "start": 6364.8, "end": 6371.8, "text": " And to train these models, we make connections to score matching, most specifically denoising score matching.", "tokens": [50914, 400, 281, 3847, 613, 5245, 11, 321, 652, 9271, 281, 6175, 14324, 11, 881, 4682, 1441, 78, 3436, 6175, 14324, 13, 51264], "temperature": 0.0, "avg_logprob": -0.17857538570057263, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.012817921116948128}, {"id": 824, "seek": 635380, "start": 6371.8, "end": 6376.8, "text": " Now, maybe this appeared somewhat complex and mathematically involved.", "tokens": [51264, 823, 11, 1310, 341, 8516, 8344, 3997, 293, 44003, 3288, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17857538570057263, "compression_ratio": 1.6538461538461537, "no_speech_prob": 0.012817921116948128}, {"id": 825, "seek": 637680, "start": 6376.8, "end": 6383.8, "text": " However, why should he use this differential equation and continuous time framework?", "tokens": [50364, 2908, 11, 983, 820, 415, 764, 341, 15756, 5367, 293, 10957, 565, 8388, 30, 50714], "temperature": 0.0, "avg_logprob": -0.1774629257820748, "compression_ratio": 1.5042735042735043, "no_speech_prob": 0.016389168798923492}, {"id": 826, "seek": 637680, "start": 6383.8, "end": 6391.8, "text": " It really has unique advantages as I have shown hopefully and hopefully I can convince you during this part of the talk.", "tokens": [50714, 467, 534, 575, 3845, 14906, 382, 286, 362, 4898, 4696, 293, 4696, 286, 393, 13447, 291, 1830, 341, 644, 295, 264, 751, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1774629257820748, "compression_ratio": 1.5042735042735043, "no_speech_prob": 0.016389168798923492}, {"id": 827, "seek": 637680, "start": 6391.8, "end": 6401.8, "text": " Most importantly, this allows us to leverage this broad existing literature on advanced and fast SCE and ODE solvers when sampling from the model.", "tokens": [51114, 4534, 8906, 11, 341, 4045, 505, 281, 13982, 341, 4152, 6741, 10394, 322, 7339, 293, 2370, 318, 4969, 293, 422, 22296, 1404, 840, 562, 21179, 490, 264, 2316, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1774629257820748, "compression_ratio": 1.5042735042735043, "no_speech_prob": 0.016389168798923492}, {"id": 828, "seek": 640180, "start": 6401.8, "end": 6409.8, "text": " This can help us to really accelerate sampling from diffusion models, which is very crucial because they can be slow.", "tokens": [50364, 639, 393, 854, 505, 281, 534, 21341, 21179, 490, 25242, 5245, 11, 597, 307, 588, 11462, 570, 436, 393, 312, 2964, 13, 50764], "temperature": 0.0, "avg_logprob": -0.15132229049484452, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.0029334924183785915}, {"id": 829, "seek": 640180, "start": 6409.8, "end": 6426.8, "text": " Furthermore, in particular, this probability flow ODE is very useful because it allows us to also perform like these deterministic data encodings and it also allows us to do like local ideal estimation like in continuous normalizing flows and so on.", "tokens": [50764, 23999, 11, 294, 1729, 11, 341, 8482, 3095, 422, 22296, 307, 588, 4420, 570, 309, 4045, 505, 281, 611, 2042, 411, 613, 15957, 3142, 1412, 2058, 378, 1109, 293, 309, 611, 4045, 505, 281, 360, 411, 2654, 7157, 35701, 411, 294, 10957, 2710, 3319, 12867, 293, 370, 322, 13, 51614], "temperature": 0.0, "avg_logprob": -0.15132229049484452, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.0029334924183785915}, {"id": 830, "seek": 642680, "start": 6426.8, "end": 6434.8, "text": " Additionally, this is overall a fairly clean mathematical framework based on diffusion processes, score matching and so on.", "tokens": [50364, 19927, 11, 341, 307, 4787, 257, 6457, 2541, 18894, 8388, 2361, 322, 25242, 7555, 11, 6175, 14324, 293, 370, 322, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10396541111052983, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.006586034782230854}, {"id": 831, "seek": 642680, "start": 6434.8, "end": 6449.8, "text": " And this allowed us to use these connections to neural ordinary differential equations to continuous normalizing flows and to energy based models, which I think provides a lot of insights into diffusion modeling.", "tokens": [50764, 400, 341, 4350, 505, 281, 764, 613, 9271, 281, 18161, 10547, 15756, 11787, 281, 10957, 2710, 3319, 12867, 293, 281, 2281, 2361, 5245, 11, 597, 286, 519, 6417, 257, 688, 295, 14310, 666, 25242, 15983, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10396541111052983, "compression_ratio": 1.5849056603773586, "no_speech_prob": 0.006586034782230854}, {"id": 832, "seek": 644980, "start": 6449.8, "end": 6462.8, "text": " With that, I would like to conclude my part and take the mic to Wicci who will now talk about advanced techniques, accelerated sampling, conditional generation and beyond.", "tokens": [50364, 2022, 300, 11, 286, 576, 411, 281, 16886, 452, 644, 293, 747, 264, 3123, 281, 343, 299, 537, 567, 486, 586, 751, 466, 7339, 7512, 11, 29763, 21179, 11, 27708, 5125, 293, 4399, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20118242899576824, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.01382998563349247}, {"id": 833, "seek": 644980, "start": 6462.8, "end": 6465.8, "text": " Thank you very much.", "tokens": [51014, 1044, 291, 588, 709, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20118242899576824, "compression_ratio": 1.352112676056338, "no_speech_prob": 0.01382998563349247}, {"id": 834, "seek": 646580, "start": 6465.8, "end": 6471.8, "text": " Hi everyone. I'm Richie from Google Brain Team. So let's continue our study on diffusion models.", "tokens": [50364, 2421, 1518, 13, 286, 478, 6781, 414, 490, 3329, 29783, 7606, 13, 407, 718, 311, 2354, 527, 2979, 322, 25242, 5245, 13, 50664], "temperature": 0.0, "avg_logprob": -0.188328674861363, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.012423248961567879}, {"id": 835, "seek": 646580, "start": 6471.8, "end": 6482.8, "text": " So in the third part, we're going to discuss several advanced techniques of diffusion models which corresponds to accelerating sampling, conditional generation and beyond.", "tokens": [50664, 407, 294, 264, 2636, 644, 11, 321, 434, 516, 281, 2248, 2940, 7339, 7512, 295, 25242, 5245, 597, 23249, 281, 34391, 21179, 11, 27708, 5125, 293, 4399, 13, 51214], "temperature": 0.0, "avg_logprob": -0.188328674861363, "compression_ratio": 1.4486486486486487, "no_speech_prob": 0.012423248961567879}, {"id": 836, "seek": 648280, "start": 6482.8, "end": 6491.8, "text": " So here's an outline of what we're going to cover in this part. Basically want to address two important questions of diffusion models with advanced techniques.", "tokens": [50364, 407, 510, 311, 364, 16387, 295, 437, 321, 434, 516, 281, 2060, 294, 341, 644, 13, 8537, 528, 281, 2985, 732, 1021, 1651, 295, 25242, 5245, 365, 7339, 7512, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09709924459457397, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.0033759453799575567}, {"id": 837, "seek": 648280, "start": 6491.8, "end": 6508.8, "text": " The first one is how to accelerate the sampling process of diffusion models. We're going to tackle this question from three aspects, advanced forward process, advanced reverse process and advanced modeling, including hybrid models and model distillation.", "tokens": [50814, 440, 700, 472, 307, 577, 281, 21341, 264, 21179, 1399, 295, 25242, 5245, 13, 492, 434, 516, 281, 14896, 341, 1168, 490, 1045, 7270, 11, 7339, 2128, 1399, 11, 7339, 9943, 1399, 293, 7339, 15983, 11, 3009, 13051, 5245, 293, 2316, 42923, 399, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09709924459457397, "compression_ratio": 1.7617021276595746, "no_speech_prob": 0.0033759453799575567}, {"id": 838, "seek": 650880, "start": 6508.8, "end": 6514.8, "text": " The second question is how to do a high resolution, optionally conditional generation.", "tokens": [50364, 440, 1150, 1168, 307, 577, 281, 360, 257, 1090, 8669, 11, 3614, 379, 27708, 5125, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13103513882077975, "compression_ratio": 1.6, "no_speech_prob": 0.0026306144427508116}, {"id": 839, "seek": 650880, "start": 6514.8, "end": 6530.8, "text": " And I will talk about several important techniques to make this happen, especially the general conditional diffusion modeling framework, the classifier classifier free guidance, as well as cascade generation pipeline.", "tokens": [50664, 400, 286, 486, 751, 466, 2940, 1021, 7512, 281, 652, 341, 1051, 11, 2318, 264, 2674, 27708, 25242, 15983, 8388, 11, 264, 1508, 9902, 1508, 9902, 1737, 10056, 11, 382, 731, 382, 50080, 5125, 15517, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13103513882077975, "compression_ratio": 1.6, "no_speech_prob": 0.0026306144427508116}, {"id": 840, "seek": 653080, "start": 6530.8, "end": 6536.8, "text": " Let's start from the first question, so how to accelerate the sampling process of diffusion models.", "tokens": [50364, 961, 311, 722, 490, 264, 700, 1168, 11, 370, 577, 281, 21341, 264, 21179, 1399, 295, 25242, 5245, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1489964146767893, "compression_ratio": 1.5649717514124293, "no_speech_prob": 0.013628998771309853}, {"id": 841, "seek": 653080, "start": 6536.8, "end": 6547.8, "text": " To see why this question is important, let's consider what makes a good generative model. So in principle, we want a good generative model to enjoy the pulling three properties.", "tokens": [50664, 1407, 536, 983, 341, 1168, 307, 1021, 11, 718, 311, 1949, 437, 1669, 257, 665, 1337, 1166, 2316, 13, 407, 294, 8665, 11, 321, 528, 257, 665, 1337, 1166, 2316, 281, 2103, 264, 8407, 1045, 7221, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1489964146767893, "compression_ratio": 1.5649717514124293, "no_speech_prob": 0.013628998771309853}, {"id": 842, "seek": 654780, "start": 6547.8, "end": 6562.8, "text": " First, it should be fast to sample from this generative model. Second, we would expect the general model capture most of the major modes of the data distribution, or in other words, they should have adequate sample diversity.", "tokens": [50364, 2386, 11, 309, 820, 312, 2370, 281, 6889, 490, 341, 1337, 1166, 2316, 13, 5736, 11, 321, 576, 2066, 264, 2674, 2316, 7983, 881, 295, 264, 2563, 14068, 295, 264, 1412, 7316, 11, 420, 294, 661, 2283, 11, 436, 820, 362, 20927, 6889, 8811, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10461371474795872, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.0862402468919754}, {"id": 843, "seek": 654780, "start": 6562.8, "end": 6568.8, "text": " And third, of course, we want the general model to give us high quality or high fidelity samples.", "tokens": [51114, 400, 2636, 11, 295, 1164, 11, 321, 528, 264, 2674, 2316, 281, 976, 505, 1090, 3125, 420, 1090, 46404, 10938, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10461371474795872, "compression_ratio": 1.6735751295336787, "no_speech_prob": 0.0862402468919754}, {"id": 844, "seek": 656880, "start": 6568.8, "end": 6584.8, "text": " However, there is a generative learning dilemma for existing generative model frameworks. For example, for generating several networks, they are usually fast to sample from, and they can give us high quality samples.", "tokens": [50364, 2908, 11, 456, 307, 257, 1337, 1166, 2539, 34312, 337, 6741, 1337, 1166, 2316, 29834, 13, 1171, 1365, 11, 337, 17746, 2940, 9590, 11, 436, 366, 2673, 2370, 281, 6889, 490, 11, 293, 436, 393, 976, 505, 1090, 3125, 10938, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13047842077306798, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.006587611045688391}, {"id": 845, "seek": 656880, "start": 6584.8, "end": 6595.8, "text": " However, because of this discriminative learning framework, there is a decent chance that GANS may miss certain modes of the data distribution.", "tokens": [51164, 2908, 11, 570, 295, 341, 20828, 1166, 2539, 8388, 11, 456, 307, 257, 8681, 2931, 300, 460, 25711, 815, 1713, 1629, 14068, 295, 264, 1412, 7316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13047842077306798, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.006587611045688391}, {"id": 846, "seek": 659580, "start": 6595.8, "end": 6604.8, "text": " And the other type of generative models are these likelihood based models, like variational autoencoders or normalizing flows.", "tokens": [50364, 400, 264, 661, 2010, 295, 1337, 1166, 5245, 366, 613, 22119, 2361, 5245, 11, 411, 3034, 1478, 8399, 22660, 378, 433, 420, 2710, 3319, 12867, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18475047473249764, "compression_ratio": 1.598802395209581, "no_speech_prob": 0.001244380953721702}, {"id": 847, "seek": 659580, "start": 6604.8, "end": 6614.8, "text": " So those models are usually optimized by maximizing likelihood or maximizing a variant of likelihood, for example, the evidence lower bound.", "tokens": [50814, 407, 729, 5245, 366, 2673, 26941, 538, 5138, 3319, 22119, 420, 5138, 3319, 257, 17501, 295, 22119, 11, 337, 1365, 11, 264, 4467, 3126, 5472, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18475047473249764, "compression_ratio": 1.598802395209581, "no_speech_prob": 0.001244380953721702}, {"id": 848, "seek": 661480, "start": 6614.8, "end": 6626.8, "text": " So usually this type of models are good at fast sampling, and they are able to capture certain modes of the data distribution because of this maximum likelihood learning framework.", "tokens": [50364, 407, 2673, 341, 2010, 295, 5245, 366, 665, 412, 2370, 21179, 11, 293, 436, 366, 1075, 281, 7983, 1629, 14068, 295, 264, 1412, 7316, 570, 295, 341, 6674, 22119, 2539, 8388, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09854418890816825, "compression_ratio": 1.45625, "no_speech_prob": 0.00134561478625983}, {"id": 849, "seek": 661480, "start": 6626.8, "end": 6631.8, "text": " However, usually they lead to subpar sample quality.", "tokens": [50964, 2908, 11, 2673, 436, 1477, 281, 1422, 2181, 6889, 3125, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09854418890816825, "compression_ratio": 1.45625, "no_speech_prob": 0.00134561478625983}, {"id": 850, "seek": 663180, "start": 6631.8, "end": 6644.8, "text": " On the other hand, the diffusion models are good at both coverage because they are also optimizing the evidence lower bound of log likelihoods, and they are able to generate high quality samples.", "tokens": [50364, 1282, 264, 661, 1011, 11, 264, 25242, 5245, 366, 665, 412, 1293, 9645, 570, 436, 366, 611, 40425, 264, 4467, 3126, 5472, 295, 3565, 22119, 82, 11, 293, 436, 366, 1075, 281, 8460, 1090, 3125, 10938, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11868713213049847, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0017269814852625132}, {"id": 851, "seek": 663180, "start": 6644.8, "end": 6654.8, "text": " However, the sampling of diffusion models is pretty slow, which usually requires thousands of functional calls before getting a simple batch of samples.", "tokens": [51014, 2908, 11, 264, 21179, 295, 25242, 5245, 307, 1238, 2964, 11, 597, 2673, 7029, 5383, 295, 11745, 5498, 949, 1242, 257, 2199, 15245, 295, 10938, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11868713213049847, "compression_ratio": 1.6261682242990654, "no_speech_prob": 0.0017269814852625132}, {"id": 852, "seek": 665480, "start": 6654.8, "end": 6666.8, "text": " So if we can find techniques to accelerate the sampling process of diffusion models, we will get a generative model framework, which enjoys all those three great properties.", "tokens": [50364, 407, 498, 321, 393, 915, 7512, 281, 21341, 264, 21179, 1399, 295, 25242, 5245, 11, 321, 486, 483, 257, 1337, 1166, 2316, 8388, 11, 597, 29750, 439, 729, 1045, 869, 7221, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10161475415499706, "compression_ratio": 1.5153374233128833, "no_speech_prob": 0.003537494456395507}, {"id": 853, "seek": 665480, "start": 6666.8, "end": 6672.8, "text": " That is, we can tackle the dilemma of this generative learning framework.", "tokens": [50964, 663, 307, 11, 321, 393, 14896, 264, 34312, 295, 341, 1337, 1166, 2539, 8388, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10161475415499706, "compression_ratio": 1.5153374233128833, "no_speech_prob": 0.003537494456395507}, {"id": 854, "seek": 667280, "start": 6672.8, "end": 6680.8, "text": " Before I do that, before diving into details, I would like to recap the general formulation of diffusion models.", "tokens": [50364, 4546, 286, 360, 300, 11, 949, 20241, 666, 4365, 11, 286, 576, 411, 281, 20928, 264, 2674, 37642, 295, 25242, 5245, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16635882412945782, "compression_ratio": 1.4970760233918128, "no_speech_prob": 0.008442842401564121}, {"id": 855, "seek": 667280, "start": 6680.8, "end": 6692.8, "text": " So for diffusion models, they usually define a simple forward process which slowly maps data to noise by repeatedly adding noise to the images.", "tokens": [50764, 407, 337, 25242, 5245, 11, 436, 2673, 6964, 257, 2199, 2128, 1399, 597, 5692, 11317, 1412, 281, 5658, 538, 18227, 5127, 5658, 281, 264, 5267, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16635882412945782, "compression_ratio": 1.4970760233918128, "no_speech_prob": 0.008442842401564121}, {"id": 856, "seek": 669280, "start": 6692.8, "end": 6703.8, "text": " The forward process is defined to map data, or to map noise back to data, and this is where the diffusion model is defined and trained.", "tokens": [50364, 440, 2128, 1399, 307, 7642, 281, 4471, 1412, 11, 420, 281, 4471, 5658, 646, 281, 1412, 11, 293, 341, 307, 689, 264, 25242, 2316, 307, 7642, 293, 8895, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1729249393238741, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.023679198697209358}, {"id": 857, "seek": 669280, "start": 6703.8, "end": 6721.8, "text": " In terms of the diffusion model, it is usually parameterized by a unit architecture, which takes the noisy inputs at certain time step, and then it tries to predict the clean samples, or it tries to predict the noise added to this noisy inputs.", "tokens": [50914, 682, 2115, 295, 264, 25242, 2316, 11, 309, 307, 2673, 13075, 1602, 538, 257, 4985, 9482, 11, 597, 2516, 264, 24518, 15743, 412, 1629, 565, 1823, 11, 293, 550, 309, 9898, 281, 6069, 264, 2541, 10938, 11, 420, 309, 9898, 281, 6069, 264, 5658, 3869, 281, 341, 24518, 15743, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1729249393238741, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.023679198697209358}, {"id": 858, "seek": 672180, "start": 6722.8, "end": 6734.8, "text": " So if we think about accelerating sampling, there are some naive methods that immediately come to our mind. For example, in training, we can reduce the number of diffusion time steps.", "tokens": [50414, 407, 498, 321, 519, 466, 34391, 21179, 11, 456, 366, 512, 29052, 7150, 300, 4258, 808, 281, 527, 1575, 13, 1171, 1365, 11, 294, 3097, 11, 321, 393, 5407, 264, 1230, 295, 25242, 565, 4439, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10687539577484131, "compression_ratio": 1.3357664233576643, "no_speech_prob": 0.0018092793179675937}, {"id": 859, "seek": 673480, "start": 6734.8, "end": 6754.8, "text": " In sampling, we sample every K time step instead of going over the whole reverse process. However, those naive acceleration methods will lead to immediate voice performance of diffusion models in terms of both the sample quality as well as the likelihood estimations.", "tokens": [50364, 682, 21179, 11, 321, 6889, 633, 591, 565, 1823, 2602, 295, 516, 670, 264, 1379, 9943, 1399, 13, 2908, 11, 729, 29052, 17162, 7150, 486, 1477, 281, 11629, 3177, 3389, 295, 25242, 5245, 294, 2115, 295, 1293, 264, 6889, 3125, 382, 731, 382, 264, 22119, 8017, 763, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17541857326731963, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.010647243820130825}, {"id": 860, "seek": 673480, "start": 6754.8, "end": 6761.8, "text": " So we really need something clever, cleverer than those naive methods.", "tokens": [51364, 407, 321, 534, 643, 746, 13494, 11, 13494, 260, 813, 729, 29052, 7150, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17541857326731963, "compression_ratio": 1.6172248803827751, "no_speech_prob": 0.010647243820130825}, {"id": 861, "seek": 676180, "start": 6761.8, "end": 6774.8, "text": " And more precisely, we want to ask, given a limited number of functional calls, which are usually much less than thousands, so how to improve the performance of diffusion models.", "tokens": [50364, 400, 544, 13402, 11, 321, 528, 281, 1029, 11, 2212, 257, 5567, 1230, 295, 11745, 5498, 11, 597, 366, 2673, 709, 1570, 813, 5383, 11, 370, 577, 281, 3470, 264, 3389, 295, 25242, 5245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12651025513072073, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0021809476893395185}, {"id": 862, "seek": 676180, "start": 6774.8, "end": 6790.8, "text": " And as a side note, although the following techniques I'm going to discuss take this accelerated sampling as the main motivation, they definitely provide more insights and contributions to diffusion models as we will see soon.", "tokens": [51014, 400, 382, 257, 1252, 3637, 11, 4878, 264, 3480, 7512, 286, 478, 516, 281, 2248, 747, 341, 29763, 21179, 382, 264, 2135, 12335, 11, 436, 2138, 2893, 544, 14310, 293, 15725, 281, 25242, 5245, 382, 321, 486, 536, 2321, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12651025513072073, "compression_ratio": 1.6135458167330676, "no_speech_prob": 0.0021809476893395185}, {"id": 863, "seek": 679080, "start": 6790.8, "end": 6806.8, "text": " So we will answer this question from three aspects. The first one is advanced forward process, and second is advanced reverse process, and then lastly the advanced diffusion models.", "tokens": [50364, 407, 321, 486, 1867, 341, 1168, 490, 1045, 7270, 13, 440, 700, 472, 307, 7339, 2128, 1399, 11, 293, 1150, 307, 7339, 9943, 1399, 11, 293, 550, 16386, 264, 7339, 25242, 5245, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15761015866253827, "compression_ratio": 1.4715447154471544, "no_speech_prob": 0.000230461431783624}, {"id": 864, "seek": 680680, "start": 6806.8, "end": 6828.8, "text": " So first let's take a look at some advanced forward process. So recall that the original whole process defines a Markov process, where we start from this x zero, it is the clean sample, and we gradually add noise until it goes to this x big T corresponding to white noise signal.", "tokens": [50364, 407, 700, 718, 311, 747, 257, 574, 412, 512, 7339, 2128, 1399, 13, 407, 9901, 300, 264, 3380, 1379, 1399, 23122, 257, 3934, 5179, 1399, 11, 689, 321, 722, 490, 341, 2031, 4018, 11, 309, 307, 264, 2541, 6889, 11, 293, 321, 13145, 909, 5658, 1826, 309, 1709, 281, 341, 2031, 955, 314, 11760, 281, 2418, 5658, 6358, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2144636275276305, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.025548912584781647}, {"id": 865, "seek": 682880, "start": 6828.8, "end": 6842.8, "text": " So QXT, QXT minus one is simply a Gaussian distribution, and this beta T defines the noise schedule, and they are hyper parameters that are predefined before training the models.", "tokens": [50364, 407, 1249, 20542, 11, 1249, 20542, 3175, 472, 307, 2935, 257, 39148, 7316, 11, 293, 341, 9861, 314, 23122, 264, 5658, 7567, 11, 293, 436, 366, 9848, 9834, 300, 366, 659, 37716, 949, 3097, 264, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3122324129430259, "compression_ratio": 1.3587786259541985, "no_speech_prob": 0.017978571355342865}, {"id": 866, "seek": 684280, "start": 6842.8, "end": 6854.8, "text": " So we are interested in the following questions. So first, does this forward process or noise schedule have to be predefined? And does it have to be a Markovian process?", "tokens": [50364, 407, 321, 366, 3102, 294, 264, 3480, 1651, 13, 407, 700, 11, 775, 341, 2128, 1399, 420, 5658, 7567, 362, 281, 312, 659, 37716, 30, 400, 775, 309, 362, 281, 312, 257, 3934, 5179, 952, 1399, 30, 50964], "temperature": 0.0, "avg_logprob": -0.14601141446596616, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0032717313151806593}, {"id": 867, "seek": 684280, "start": 6854.8, "end": 6868.8, "text": " And lastly, does it, is there any faster mixing diffusion process? The faster mixing is an important concept in Markov chain Monte Carlo, as we will discuss later.", "tokens": [50964, 400, 16386, 11, 775, 309, 11, 307, 456, 604, 4663, 11983, 25242, 1399, 30, 440, 4663, 11983, 307, 364, 1021, 3410, 294, 3934, 5179, 5021, 38105, 45112, 11, 382, 321, 486, 2248, 1780, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14601141446596616, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.0032717313151806593}, {"id": 868, "seek": 686880, "start": 6868.8, "end": 6882.8, "text": " For the first work I would like to discuss here is this variational diffusion models. It basically enables us to learn the parameters in this forward process together with the rest of parameters in the diffusion models.", "tokens": [50364, 1171, 264, 700, 589, 286, 576, 411, 281, 2248, 510, 307, 341, 3034, 1478, 25242, 5245, 13, 467, 1936, 17077, 505, 281, 1466, 264, 9834, 294, 341, 2128, 1399, 1214, 365, 264, 1472, 295, 9834, 294, 264, 25242, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.14532678777521307, "compression_ratio": 1.5755395683453237, "no_speech_prob": 0.0025106666143983603}, {"id": 869, "seek": 688280, "start": 6882.8, "end": 6902.8, "text": " In this case, we can really learn the forward process. So more specifically, given the forward process defined by QXT given x zero, it follows a Gaussian distribution with square root alpha T bar x zero as the mean and one minus alpha T bar as the variance.", "tokens": [50364, 682, 341, 1389, 11, 321, 393, 534, 1466, 264, 2128, 1399, 13, 407, 544, 4682, 11, 2212, 264, 2128, 1399, 7642, 538, 1249, 20542, 2212, 2031, 4018, 11, 309, 10002, 257, 39148, 7316, 365, 3732, 5593, 8961, 314, 2159, 2031, 4018, 382, 264, 914, 293, 472, 3175, 8961, 314, 2159, 382, 264, 21977, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20493704742855495, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.13099564611911774}, {"id": 870, "seek": 688280, "start": 6902.8, "end": 6906.8, "text": " So this is the formulation we have learned in part one.", "tokens": [51364, 407, 341, 307, 264, 37642, 321, 362, 3264, 294, 644, 472, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20493704742855495, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.13099564611911774}, {"id": 871, "seek": 690680, "start": 6906.8, "end": 6923.8, "text": " And this work proposed to directly parametrize the variance one minus alpha T bar through a learnable function gamma eta. And this function is definitely by a similar function to ensure that the variance is within the range from zero to one.", "tokens": [50364, 400, 341, 589, 10348, 281, 3838, 6220, 302, 470, 1381, 264, 21977, 472, 3175, 8961, 314, 2159, 807, 257, 1466, 712, 2445, 15546, 32415, 13, 400, 341, 2445, 307, 2138, 538, 257, 2531, 2445, 281, 5586, 300, 264, 21977, 307, 1951, 264, 3613, 490, 4018, 281, 472, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19367274871239296, "compression_ratio": 1.535031847133758, "no_speech_prob": 0.003705171402543783}, {"id": 872, "seek": 692380, "start": 6923.8, "end": 6936.8, "text": " Gamma eta T is further parametrized by a monotonic multilayer perceptron by using strictly positive ways and monotonic activations, for example, sigmoid activations.", "tokens": [50364, 24723, 1696, 32415, 314, 307, 3052, 6220, 302, 470, 11312, 538, 257, 1108, 310, 11630, 2120, 388, 11167, 43276, 2044, 538, 1228, 20792, 3353, 2098, 293, 1108, 310, 11630, 2430, 763, 11, 337, 1365, 11, 4556, 3280, 327, 2430, 763, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16583739386664498, "compression_ratio": 1.375, "no_speech_prob": 0.005818088073283434}, {"id": 873, "seek": 693680, "start": 6936.8, "end": 6954.8, "text": " And recall that in part one, we have learned that these diffusion models are directly connected to hierarchical variational auto encoders in the sense that diffusion models can be considered as a hierarchical variational encoders but with fixed encoder right.", "tokens": [50364, 400, 9901, 300, 294, 644, 472, 11, 321, 362, 3264, 300, 613, 25242, 5245, 366, 3838, 4582, 281, 35250, 804, 3034, 1478, 8399, 2058, 378, 433, 294, 264, 2020, 300, 25242, 5245, 393, 312, 4888, 382, 257, 35250, 804, 3034, 1478, 2058, 378, 433, 457, 365, 6806, 2058, 19866, 558, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1520842812278054, "compression_ratio": 1.738255033557047, "no_speech_prob": 0.0029805703088641167}, {"id": 874, "seek": 695480, "start": 6954.8, "end": 6971.8, "text": " And this model is named as variational diffusion models because it is even more similar to hierarchical variational encoders because we are optimizing the parameters in the encoder together with the parameters in the decoder.", "tokens": [50364, 400, 341, 2316, 307, 4926, 382, 3034, 1478, 25242, 5245, 570, 309, 307, 754, 544, 2531, 281, 35250, 804, 3034, 1478, 2058, 378, 433, 570, 321, 366, 40425, 264, 9834, 294, 264, 2058, 19866, 1214, 365, 264, 9834, 294, 264, 979, 19866, 13, 51214], "temperature": 0.0, "avg_logprob": -0.05385826496367759, "compression_ratio": 1.6544117647058822, "no_speech_prob": 0.002148407744243741}, {"id": 875, "seek": 697180, "start": 6971.8, "end": 6993.8, "text": " And to optimize the parameters of the forward process, this paper further derive new parametrization of the training objectives in a boring sense. So basically they have shown that optimizing the variational upper bound of the diffusion models can be simplified to the following training objectives.", "tokens": [50364, 400, 281, 19719, 264, 9834, 295, 264, 2128, 1399, 11, 341, 3035, 3052, 28446, 777, 6220, 302, 24959, 399, 295, 264, 3097, 15961, 294, 257, 9989, 2020, 13, 407, 1936, 436, 362, 4898, 300, 40425, 264, 3034, 1478, 6597, 5472, 295, 264, 25242, 5245, 393, 312, 26335, 281, 264, 3480, 3097, 15961, 13, 51464], "temperature": 0.0, "avg_logprob": -0.17650549035323293, "compression_ratio": 1.6162162162162161, "no_speech_prob": 0.004466281738132238}, {"id": 876, "seek": 699380, "start": 6993.8, "end": 7005.8, "text": " Note that this gamma eta participates in this weighting of like different L2 norm of different time steps.", "tokens": [50364, 11633, 300, 341, 15546, 32415, 3421, 1024, 294, 341, 3364, 278, 295, 411, 819, 441, 17, 2026, 295, 819, 565, 4439, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19231156202463004, "compression_ratio": 1.1910112359550562, "no_speech_prob": 0.02673213928937912}, {"id": 877, "seek": 700580, "start": 7005.8, "end": 7025.8, "text": " This is the training objectives they derive for this great time session. And they have shown that by learning this noise schedule, it actually improves the likelihood estimation of diffusion models a lot, especially when we assume that there are fewer diffusion time steps.", "tokens": [50364, 639, 307, 264, 3097, 15961, 436, 28446, 337, 341, 869, 565, 5481, 13, 400, 436, 362, 4898, 300, 538, 2539, 341, 5658, 7567, 11, 309, 767, 24771, 264, 22119, 35701, 295, 25242, 5245, 257, 688, 11, 2318, 562, 321, 6552, 300, 456, 366, 13366, 25242, 565, 4439, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1918342113494873, "compression_ratio": 1.5337078651685394, "no_speech_prob": 0.009265839122235775}, {"id": 878, "seek": 702580, "start": 7025.8, "end": 7039.8, "text": " Note that in the second part, we learned that the diffusion models can be interpreted from the perspective of stochastic differential equation and we learned the connection between diffusion models and denoting score matching.", "tokens": [50364, 11633, 300, 294, 264, 1150, 644, 11, 321, 3264, 300, 264, 25242, 5245, 393, 312, 26749, 490, 264, 4585, 295, 342, 8997, 2750, 15756, 5367, 293, 321, 3264, 264, 4984, 1296, 25242, 5245, 293, 1441, 17001, 6175, 14324, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12383058459259742, "compression_ratio": 1.591549295774648, "no_speech_prob": 0.0073430659249424934}, {"id": 879, "seek": 703980, "start": 7039.8, "end": 7058.8, "text": " This gives us a hint or a knowledge that the diffusion models can also be defined in the continuous time setting. And in this paper, they explicitly derive this variational upper bound in the continuous time setting with this gamma eta notation.", "tokens": [50364, 639, 2709, 505, 257, 12075, 420, 257, 3601, 300, 264, 25242, 5245, 393, 611, 312, 7642, 294, 264, 10957, 565, 3287, 13, 400, 294, 341, 3035, 11, 436, 20803, 28446, 341, 3034, 1478, 6597, 5472, 294, 264, 10957, 565, 3287, 365, 341, 15546, 32415, 24657, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16869350433349609, "compression_ratio": 1.6013071895424837, "no_speech_prob": 0.025166191160678864}, {"id": 880, "seek": 705880, "start": 7058.8, "end": 7075.8, "text": " Basically, they show that when we let this big T goes to infinity, meaning like we have infinity amount of diffusion time steps, this corresponds to a continuous time setting, and then the variational upper bound can be derived in the following formulation.", "tokens": [50364, 8537, 11, 436, 855, 300, 562, 321, 718, 341, 955, 314, 1709, 281, 13202, 11, 3620, 411, 321, 362, 13202, 2372, 295, 25242, 565, 4439, 11, 341, 23249, 281, 257, 10957, 565, 3287, 11, 293, 550, 264, 3034, 1478, 6597, 5472, 393, 312, 18949, 294, 264, 3480, 37642, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1614282895933907, "compression_ratio": 1.511764705882353, "no_speech_prob": 0.004465875681489706}, {"id": 881, "seek": 707580, "start": 7075.8, "end": 7091.8, "text": " And here the only difference is that the weighting term of different L2 terms, L2 distance at different time steps equals to the derivative of this gamma eta t function over time t.", "tokens": [50364, 400, 510, 264, 787, 2649, 307, 300, 264, 3364, 278, 1433, 295, 819, 441, 17, 2115, 11, 441, 17, 4560, 412, 819, 565, 4439, 6915, 281, 264, 13760, 295, 341, 15546, 32415, 256, 2445, 670, 565, 256, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16560283161344982, "compression_ratio": 1.4251968503937007, "no_speech_prob": 0.03843158110976219}, {"id": 882, "seek": 709180, "start": 7091.8, "end": 7104.8, "text": " And more interestingly, this paper shows that if we define the signal to noise ratio equals to alpha t bar minus one divided by one minus alpha t bar.", "tokens": [50364, 400, 544, 25873, 11, 341, 3035, 3110, 300, 498, 321, 6964, 264, 6358, 281, 5658, 8509, 6915, 281, 8961, 256, 2159, 3175, 472, 6666, 538, 472, 3175, 8961, 256, 2159, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09034194320928855, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.010009336285293102}, {"id": 883, "seek": 709180, "start": 7104.8, "end": 7113.8, "text": " And then this L infinity is only related to the signal to noise ratio at the endpoints of the whole forward process.", "tokens": [51014, 400, 550, 341, 441, 13202, 307, 787, 4077, 281, 264, 6358, 281, 5658, 8509, 412, 264, 917, 20552, 295, 264, 1379, 2128, 1399, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09034194320928855, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.010009336285293102}, {"id": 884, "seek": 711380, "start": 7113.8, "end": 7131.8, "text": " And it is invariant to the noise schedule in between the endpoints. So if we want to optimize the forward process in the continuous time setting, we only need to optimize the signal to noise ratio at the beginning and the end of the forward process.", "tokens": [50364, 400, 309, 307, 33270, 394, 281, 264, 5658, 7567, 294, 1296, 264, 917, 20552, 13, 407, 498, 321, 528, 281, 19719, 264, 2128, 1399, 294, 264, 10957, 565, 3287, 11, 321, 787, 643, 281, 19719, 264, 6358, 281, 5658, 8509, 412, 264, 2863, 293, 264, 917, 295, 264, 2128, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07628586509011008, "compression_ratio": 1.66, "no_speech_prob": 0.001867008744738996}, {"id": 885, "seek": 713180, "start": 7131.8, "end": 7146.8, "text": " And they further show that the in-between process can be learned to minimize the variance of the training objective. And this enables the faster training of diffusion models besides faster sampling.", "tokens": [50364, 400, 436, 3052, 855, 300, 264, 294, 12, 32387, 1399, 393, 312, 3264, 281, 17522, 264, 21977, 295, 264, 3097, 10024, 13, 400, 341, 17077, 264, 4663, 3097, 295, 25242, 5245, 11868, 4663, 21179, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12260927298130134, "compression_ratio": 1.4452554744525548, "no_speech_prob": 0.0021152393892407417}, {"id": 886, "seek": 714680, "start": 7146.8, "end": 7156.8, "text": " And another contribution of this work is that they show it is possible to use diffusion models to get state of the art likelihood estimation results.", "tokens": [50364, 400, 1071, 13150, 295, 341, 589, 307, 300, 436, 855, 309, 307, 1944, 281, 764, 25242, 5245, 281, 483, 1785, 295, 264, 1523, 22119, 35701, 3542, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10173466500271572, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.004006219562143087}, {"id": 887, "seek": 714680, "start": 7156.8, "end": 7174.8, "text": " So before this work, the benchmark of likelihood estimation have been dominated by autoregressive types of models for many years as shown in this figure, but this model shows like actually we can use diffusion models to get a big improvement out of the autoregressive model process.", "tokens": [50864, 407, 949, 341, 589, 11, 264, 18927, 295, 22119, 35701, 362, 668, 23755, 538, 1476, 418, 3091, 488, 3467, 295, 5245, 337, 867, 924, 382, 4898, 294, 341, 2573, 11, 457, 341, 2316, 3110, 411, 767, 321, 393, 764, 25242, 5245, 281, 483, 257, 955, 10444, 484, 295, 264, 1476, 418, 3091, 488, 2316, 1399, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10173466500271572, "compression_ratio": 1.854077253218884, "no_speech_prob": 0.004006219562143087}, {"id": 888, "seek": 717480, "start": 7174.8, "end": 7188.8, "text": " And one key factor to make this happen is to add further features to the input of the unit. And this, those four features can range from low frequency to very high frequencies.", "tokens": [50364, 400, 472, 2141, 5952, 281, 652, 341, 1051, 307, 281, 909, 3052, 4122, 281, 264, 4846, 295, 264, 4985, 13, 400, 341, 11, 729, 1451, 4122, 393, 3613, 490, 2295, 7893, 281, 588, 1090, 20250, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1337999224662781, "compression_ratio": 1.4193548387096775, "no_speech_prob": 0.004131399095058441}, {"id": 889, "seek": 718880, "start": 7188.8, "end": 7204.8, "text": " And the hypothesis for the assumption here is that to get good likelihood estimation, the model you really need to model all the bits or all the details in the input signal, either they are in perceptual or perceptual.", "tokens": [50364, 400, 264, 17291, 337, 264, 15302, 510, 307, 300, 281, 483, 665, 22119, 35701, 11, 264, 2316, 291, 534, 643, 281, 2316, 439, 264, 9239, 420, 439, 264, 4365, 294, 264, 4846, 6358, 11, 2139, 436, 366, 294, 43276, 901, 420, 43276, 901, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12978907426198324, "compression_ratio": 1.5683453237410072, "no_speech_prob": 0.004398304969072342}, {"id": 890, "seek": 720480, "start": 7204.8, "end": 7220.8, "text": " However, new networks are usually bad at modeling small changes to the inputs. So adding those four features, especially those high frequency components can potentially help the network to identify those small details.", "tokens": [50364, 2908, 11, 777, 9590, 366, 2673, 1578, 412, 15983, 1359, 2962, 281, 264, 15743, 13, 407, 5127, 729, 1451, 4122, 11, 2318, 729, 1090, 7893, 6677, 393, 7263, 854, 264, 3209, 281, 5876, 729, 1359, 4365, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09234818016610495, "compression_ratio": 1.472972972972973, "no_speech_prob": 0.0016741203144192696}, {"id": 891, "seek": 722080, "start": 7220.8, "end": 7236.8, "text": " And the paper found that this trick doesn't bring like much significant improvements to the autoregressive baselines. However, it leads to significant improvements in likelihood estimation for diffusion model class.", "tokens": [50364, 400, 264, 3035, 1352, 300, 341, 4282, 1177, 380, 1565, 411, 709, 4776, 13797, 281, 264, 1476, 418, 3091, 488, 987, 9173, 13, 2908, 11, 309, 6689, 281, 4776, 13797, 294, 22119, 35701, 337, 25242, 2316, 1508, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11521472249712263, "compression_ratio": 1.5034965034965035, "no_speech_prob": 0.002018509665504098}, {"id": 892, "seek": 723680, "start": 7236.8, "end": 7255.8, "text": " Okay, so next, like paper or method I'm going to discuss is this denoting diffusion implicit models. So in this work, the main idea is like they try to define a family of non markovian diffusion processes and the corresponding reverse processes.", "tokens": [50364, 1033, 11, 370, 958, 11, 411, 3035, 420, 3170, 286, 478, 516, 281, 2248, 307, 341, 1441, 17001, 25242, 26947, 5245, 13, 407, 294, 341, 589, 11, 264, 2135, 1558, 307, 411, 436, 853, 281, 6964, 257, 1605, 295, 2107, 1491, 5179, 952, 25242, 7555, 293, 264, 11760, 9943, 7555, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20047865780917082, "compression_ratio": 1.4759036144578312, "no_speech_prob": 0.002471931278705597}, {"id": 893, "seek": 725580, "start": 7255.8, "end": 7265.8, "text": " And those processes are designed such that the model can still be optimized by the same surrogate objective as original diffusion models.", "tokens": [50364, 400, 729, 7555, 366, 4761, 1270, 300, 264, 2316, 393, 920, 312, 26941, 538, 264, 912, 1022, 6675, 473, 10024, 382, 3380, 25242, 5245, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1829077402750651, "compression_ratio": 1.6195121951219513, "no_speech_prob": 0.0014323712093755603}, {"id": 894, "seek": 725580, "start": 7265.8, "end": 7282.8, "text": " We call that this is the surrogate objective right so this L simple where we remove the weighting of each L2 loss term and we just simply take the average of the L2 loss at different time steps.", "tokens": [50864, 492, 818, 300, 341, 307, 264, 1022, 6675, 473, 10024, 558, 370, 341, 441, 2199, 689, 321, 4159, 264, 3364, 278, 295, 1184, 441, 17, 4470, 1433, 293, 321, 445, 2935, 747, 264, 4274, 295, 264, 441, 17, 4470, 412, 819, 565, 4439, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1829077402750651, "compression_ratio": 1.6195121951219513, "no_speech_prob": 0.0014323712093755603}, {"id": 895, "seek": 728280, "start": 7282.8, "end": 7298.8, "text": " And then, because they can optimize by the same surrogate objective. So one can simply take a pre train diffusion model and treat it as the model of those non markovian diffusion processes, so that they are able.", "tokens": [50364, 400, 550, 11, 570, 436, 393, 19719, 538, 264, 912, 1022, 6675, 473, 10024, 13, 407, 472, 393, 2935, 747, 257, 659, 3847, 25242, 2316, 293, 2387, 309, 382, 264, 2316, 295, 729, 2107, 1491, 5179, 952, 25242, 7555, 11, 370, 300, 436, 366, 1075, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14171424093125742, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.001048231846652925}, {"id": 896, "seek": 728280, "start": 7298.8, "end": 7310.8, "text": " We are able to use the corresponding reverse processes to reverse the model, which means like we will have more choices of our sampling procedure.", "tokens": [51164, 492, 366, 1075, 281, 764, 264, 11760, 9943, 7555, 281, 9943, 264, 2316, 11, 597, 1355, 411, 321, 486, 362, 544, 7994, 295, 527, 21179, 10747, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14171424093125742, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.001048231846652925}, {"id": 897, "seek": 731080, "start": 7311.8, "end": 7322.8, "text": " Okay, so to see how we can define those non markovian forward processes, let's recap the duration of the KL divergence in the variational lower bound.", "tokens": [50414, 1033, 11, 370, 281, 536, 577, 321, 393, 6964, 729, 2107, 1491, 5179, 952, 2128, 7555, 11, 718, 311, 20928, 264, 16365, 295, 264, 47991, 47387, 294, 264, 3034, 1478, 3126, 5472, 13, 50964], "temperature": 0.0, "avg_logprob": -0.192731135600322, "compression_ratio": 1.2820512820512822, "no_speech_prob": 0.0030742946546524763}, {"id": 898, "seek": 732280, "start": 7322.8, "end": 7343.8, "text": " So we have this LT minus one is defined as the KL divergence between this posterior distribution QXT minus one given XT and X zero, and this denoting distribution P theta XT minus one given XT so this P theta is parameterized by the diffusion model.", "tokens": [50364, 407, 321, 362, 341, 42671, 3175, 472, 307, 7642, 382, 264, 47991, 47387, 1296, 341, 33529, 7316, 1249, 20542, 3175, 472, 2212, 1783, 51, 293, 1783, 4018, 11, 293, 341, 1441, 17001, 7316, 430, 9725, 1783, 51, 3175, 472, 2212, 1783, 51, 370, 341, 430, 9725, 307, 13075, 1602, 538, 264, 25242, 2316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.329234057459338, "compression_ratio": 1.6274509803921569, "no_speech_prob": 0.02440977841615677}, {"id": 899, "seek": 734380, "start": 7343.8, "end": 7362.8, "text": " And because these two distribution, like both of them are Gaussian distributions with the same variance sigma T square, and this can be written as the outer distance between the the mean of these two distributions times a constant.", "tokens": [50364, 400, 570, 613, 732, 7316, 11, 411, 1293, 295, 552, 366, 39148, 37870, 365, 264, 912, 21977, 12771, 314, 3732, 11, 293, 341, 393, 312, 3720, 382, 264, 10847, 4560, 1296, 264, 264, 914, 295, 613, 732, 37870, 1413, 257, 5754, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1849388661591903, "compression_ratio": 1.582191780821918, "no_speech_prob": 0.002713927999138832}, {"id": 900, "seek": 736280, "start": 7362.8, "end": 7382.8, "text": " Note that these two mean function have been parameterized by like simple linear combination of XT and if so, or the combination of XT and the predicted if so, so if some listen noise added to the queen sample to get XT.", "tokens": [50364, 11633, 300, 613, 732, 914, 2445, 362, 668, 13075, 1602, 538, 411, 2199, 8213, 6562, 295, 1783, 51, 293, 498, 370, 11, 420, 264, 6562, 295, 1783, 51, 293, 264, 19147, 498, 370, 11, 370, 498, 512, 2140, 5658, 3869, 281, 264, 12206, 6889, 281, 483, 1783, 51, 13, 51364], "temperature": 0.0, "avg_logprob": -0.28217069158014263, "compression_ratio": 1.5314685314685315, "no_speech_prob": 0.0029797162860631943}, {"id": 901, "seek": 738280, "start": 7382.8, "end": 7398.8, "text": " We can further write this KL divergence in the form of lambda T times the outer distance between the true noise if some and the predicted noise if some theta by our diffusion models.", "tokens": [50364, 492, 393, 3052, 2464, 341, 47991, 47387, 294, 264, 1254, 295, 13607, 314, 1413, 264, 10847, 4560, 1296, 264, 2074, 5658, 498, 512, 293, 264, 19147, 5658, 498, 512, 9725, 538, 527, 25242, 5245, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21149063110351562, "compression_ratio": 1.4330708661417322, "no_speech_prob": 0.0004877234168816358}, {"id": 902, "seek": 739880, "start": 7398.8, "end": 7416.8, "text": " And if we really think about the duration of this LT minus one, we found that if we assume like this lambda T can be after values, because in a surrogate objective we simply set it to one right so it doesn't matter what value this alpha T is originally.", "tokens": [50364, 400, 498, 321, 534, 519, 466, 264, 16365, 295, 341, 42671, 3175, 472, 11, 321, 1352, 300, 498, 321, 6552, 411, 341, 13607, 314, 393, 312, 934, 4190, 11, 570, 294, 257, 1022, 6675, 473, 10024, 321, 2935, 992, 309, 281, 472, 558, 370, 309, 1177, 380, 1871, 437, 2158, 341, 8961, 314, 307, 7993, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19155038197835286, "compression_ratio": 1.4624277456647399, "no_speech_prob": 0.006001999601721764}, {"id": 903, "seek": 741680, "start": 7416.8, "end": 7428.8, "text": " So then we can find that about formulation holds as long as first we have this QXT give X zero, it follows this normal distribution.", "tokens": [50364, 407, 550, 321, 393, 915, 300, 466, 37642, 9190, 382, 938, 382, 700, 321, 362, 341, 1249, 20542, 976, 1783, 4018, 11, 309, 10002, 341, 2710, 7316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2504507601261139, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.008844200521707535}, {"id": 904, "seek": 742880, "start": 7428.8, "end": 7447.8, "text": " And we need to make sure that this XT still equals to this formulation. And we have those two assumptions. First, the follow process, like the posterior distribution QXT minus one, give XT and X zero follows a Gaussian distribution.", "tokens": [50364, 400, 321, 643, 281, 652, 988, 300, 341, 1783, 51, 920, 6915, 281, 341, 37642, 13, 400, 321, 362, 729, 732, 17695, 13, 2386, 11, 264, 1524, 1399, 11, 411, 264, 33529, 7316, 1249, 20542, 3175, 472, 11, 976, 1783, 51, 293, 1783, 4018, 10002, 257, 39148, 7316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2819240093231201, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.013422936201095581}, {"id": 905, "seek": 742880, "start": 7447.8, "end": 7453.8, "text": " And the meaning of this distribution is a linear combination of XT and the epsilon.", "tokens": [51314, 400, 264, 3620, 295, 341, 7316, 307, 257, 8213, 6562, 295, 1783, 51, 293, 264, 17889, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2819240093231201, "compression_ratio": 1.6040609137055837, "no_speech_prob": 0.013422936201095581}, {"id": 906, "seek": 745380, "start": 7453.8, "end": 7468.8, "text": " And our reverse process to be similar to the posterior distribution, which means it is also a Gaussian, and this new theta is the same linear combination of XT and the predicted noise.", "tokens": [50364, 400, 527, 9943, 1399, 281, 312, 2531, 281, 264, 33529, 7316, 11, 597, 1355, 309, 307, 611, 257, 39148, 11, 293, 341, 777, 9725, 307, 264, 912, 8213, 6562, 295, 1783, 51, 293, 264, 19147, 5658, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21399472399455746, "compression_ratio": 1.373134328358209, "no_speech_prob": 0.02259705774486065}, {"id": 907, "seek": 746880, "start": 7468.8, "end": 7483.8, "text": " So we can further rewrite, because we know XT equals to a linear combination of X zero and epsilon. So we can replace this epsilon, epsilon by the linear combination of XT and X zero.", "tokens": [50364, 407, 321, 393, 3052, 28132, 11, 570, 321, 458, 1783, 51, 6915, 281, 257, 8213, 6562, 295, 1783, 4018, 293, 17889, 13, 407, 321, 393, 7406, 341, 17889, 11, 17889, 538, 264, 8213, 6562, 295, 1783, 51, 293, 1783, 4018, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22028225792778863, "compression_ratio": 1.525, "no_speech_prob": 0.03674576058983803}, {"id": 908, "seek": 748380, "start": 7483.8, "end": 7500.8, "text": " And for the reverse process, we can rewrite it as a linear combination of XT and predicted X zero hat. So this X zero hat is defined as the predicted clean sample, given the predicting noise.", "tokens": [50364, 400, 337, 264, 9943, 1399, 11, 321, 393, 28132, 309, 382, 257, 8213, 6562, 295, 1783, 51, 293, 19147, 1783, 4018, 2385, 13, 407, 341, 1783, 4018, 2385, 307, 7642, 382, 264, 19147, 2541, 6889, 11, 2212, 264, 32884, 5658, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14288882149590387, "compression_ratio": 1.4580152671755726, "no_speech_prob": 0.009122729301452637}, {"id": 909, "seek": 750080, "start": 7500.8, "end": 7519.8, "text": " So if we make those three assumptions, then we can see that the about duration of our T minus one still holds, which means that we don't really need to specify this QXT, even XT minus one, and we don't need to require to be a common process.", "tokens": [50364, 407, 498, 321, 652, 729, 1045, 17695, 11, 550, 321, 393, 536, 300, 264, 466, 16365, 295, 527, 314, 3175, 472, 920, 9190, 11, 597, 1355, 300, 321, 500, 380, 534, 643, 281, 16500, 341, 1249, 20542, 11, 754, 1783, 51, 3175, 472, 11, 293, 321, 500, 380, 643, 281, 3651, 281, 312, 257, 2689, 1399, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13616195005529066, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.05104053393006325}, {"id": 910, "seek": 750080, "start": 7519.8, "end": 7526.8, "text": " All we need to assume is the posterior distribution of XT minus one, even XT and X zero.", "tokens": [51314, 1057, 321, 643, 281, 6552, 307, 264, 33529, 7316, 295, 1783, 51, 3175, 472, 11, 754, 1783, 51, 293, 1783, 4018, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13616195005529066, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.05104053393006325}, {"id": 911, "seek": 752680, "start": 7526.8, "end": 7542.8, "text": " And that's the basic, the, the insights of this, this, like how we can define the non-marcovian forward process, which leads to the same chain objecting as the original diffusion model.", "tokens": [50364, 400, 300, 311, 264, 3875, 11, 264, 11, 264, 14310, 295, 341, 11, 341, 11, 411, 577, 321, 393, 6964, 264, 2107, 12, 6209, 1291, 85, 952, 2128, 1399, 11, 597, 6689, 281, 264, 912, 5021, 2657, 278, 382, 264, 3380, 25242, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.32019275426864624, "compression_ratio": 1.3909774436090225, "no_speech_prob": 0.0022863149642944336}, {"id": 912, "seek": 754280, "start": 7542.8, "end": 7558.8, "text": " Specifically, so this is the original diffusion process. And now the diffusion process change to the right diagram, where for each XT, it depends on both the XT minus one and the X zero.", "tokens": [50364, 26058, 11, 370, 341, 307, 264, 3380, 25242, 1399, 13, 400, 586, 264, 25242, 1399, 1319, 281, 264, 558, 10686, 11, 689, 337, 1184, 1783, 51, 11, 309, 5946, 322, 1293, 264, 1783, 51, 3175, 472, 293, 264, 1783, 4018, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09306165907118055, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.005640353076159954}, {"id": 913, "seek": 755880, "start": 7558.8, "end": 7577.8, "text": " And another remaining question is how we specify the linear combination parameters A and B. So note that here we need to specify this A and B such that this QXT given X zero still follows this Gaussian distributions.", "tokens": [50364, 400, 1071, 8877, 1168, 307, 577, 321, 16500, 264, 8213, 6562, 9834, 316, 293, 363, 13, 407, 3637, 300, 510, 321, 643, 281, 16500, 341, 316, 293, 363, 1270, 300, 341, 1249, 20542, 2212, 1783, 4018, 920, 10002, 341, 39148, 37870, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1391901451608409, "compression_ratio": 1.44, "no_speech_prob": 0.002216565189883113}, {"id": 914, "seek": 757780, "start": 7577.8, "end": 7595.8, "text": " At this end, this work defines a family of forward processes that meets the above requirements, which corresponds to specifying the posterior QXT minus one given XT and X zero in this formulation.", "tokens": [50364, 1711, 341, 917, 11, 341, 589, 23122, 257, 1605, 295, 2128, 7555, 300, 13961, 264, 3673, 7728, 11, 597, 23249, 281, 1608, 5489, 264, 33529, 1249, 20542, 3175, 472, 2212, 1783, 51, 293, 1783, 4018, 294, 341, 37642, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11761642056842182, "compression_ratio": 1.3424657534246576, "no_speech_prob": 0.009408474899828434}, {"id": 915, "seek": 759580, "start": 7595.8, "end": 7606.8, "text": " So we can similarly define the corresponding reverse process by just replacing this X zero to a predicted X zero hat.", "tokens": [50364, 407, 321, 393, 14138, 6964, 264, 11760, 9943, 1399, 538, 445, 19139, 341, 1783, 4018, 281, 257, 19147, 1783, 4018, 2385, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16378406856371008, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.005553541239351034}, {"id": 916, "seek": 759580, "start": 7606.8, "end": 7623.8, "text": " And note that this specifying specification of formulation doesn't require like a specific value of this sigma t-tutor, which means like this sigma t-tutor can be literally arbitrary values.", "tokens": [50914, 400, 3637, 300, 341, 1608, 5489, 31256, 295, 37642, 1177, 380, 3651, 411, 257, 2685, 2158, 295, 341, 12771, 256, 12, 83, 325, 284, 11, 597, 1355, 411, 341, 12771, 256, 12, 83, 325, 284, 393, 312, 3736, 23211, 4190, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16378406856371008, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.005553541239351034}, {"id": 917, "seek": 762380, "start": 7623.8, "end": 7634.8, "text": " So that's why it depends, it actually defines a family of forward processes with different values of sigma t-tutor.", "tokens": [50364, 407, 300, 311, 983, 309, 5946, 11, 309, 767, 23122, 257, 1605, 295, 2128, 7555, 365, 819, 4190, 295, 12771, 256, 12, 83, 325, 284, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07426042556762695, "compression_ratio": 1.15, "no_speech_prob": 0.0009695703629404306}, {"id": 918, "seek": 763480, "start": 7634.8, "end": 7655.8, "text": " And more importantly, if we specify like sigma t-tutor to be zero for all the time steps, this leads to this DDI and sampler where we wish is a deterministic reverse process because this variance is zero here.", "tokens": [50364, 400, 544, 8906, 11, 498, 321, 16500, 411, 12771, 256, 12, 83, 325, 284, 281, 312, 4018, 337, 439, 264, 565, 4439, 11, 341, 6689, 281, 341, 413, 3085, 293, 3247, 22732, 689, 321, 3172, 307, 257, 15957, 3142, 9943, 1399, 570, 341, 21977, 307, 4018, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.13161994860722467, "compression_ratio": 1.412162162162162, "no_speech_prob": 0.004068933427333832}, {"id": 919, "seek": 765580, "start": 7655.8, "end": 7666.8, "text": " And the only randomness comes from the like starting point of the reverse process, which is the starting white noise signal.", "tokens": [50364, 400, 264, 787, 4974, 1287, 1487, 490, 264, 411, 2891, 935, 295, 264, 9943, 1399, 11, 597, 307, 264, 2891, 2418, 5658, 6358, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12613717247458064, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.007693176623433828}, {"id": 920, "seek": 765580, "start": 7666.8, "end": 7680.8, "text": " And recall that in the second part, we also build connection between the like stochastic reverse process with a probability flow ODE, which is corresponds to a deterministic generative process.", "tokens": [50914, 400, 9901, 300, 294, 264, 1150, 644, 11, 321, 611, 1322, 4984, 1296, 264, 411, 342, 8997, 2750, 9943, 1399, 365, 257, 8482, 3095, 422, 22296, 11, 597, 307, 23249, 281, 257, 15957, 3142, 1337, 1166, 1399, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12613717247458064, "compression_ratio": 1.6649214659685865, "no_speech_prob": 0.007693176623433828}, {"id": 921, "seek": 768080, "start": 7680.8, "end": 7693.8, "text": " And we can also interpret the DDI and sampler in a similar way. Specifically, this DDI and sampler can be considered as an integration role of the following ordinary differential equation.", "tokens": [50364, 400, 321, 393, 611, 7302, 264, 413, 3085, 293, 3247, 22732, 294, 257, 2531, 636, 13, 26058, 11, 341, 413, 3085, 293, 3247, 22732, 393, 312, 4888, 382, 364, 10980, 3090, 295, 264, 3480, 10547, 15756, 5367, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13591368993123373, "compression_ratio": 1.413533834586466, "no_speech_prob": 0.00793431419879198}, {"id": 922, "seek": 769380, "start": 7693.8, "end": 7706.8, "text": " And note that here we do a bit of change of a variable where we define this x bar equals to x divided by the scaling factor square root of alpha bar.", "tokens": [50364, 400, 3637, 300, 510, 321, 360, 257, 857, 295, 1319, 295, 257, 7006, 689, 321, 6964, 341, 2031, 2159, 6915, 281, 2031, 6666, 538, 264, 21589, 5952, 3732, 5593, 295, 8961, 2159, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1349552746476798, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.004981262143701315}, {"id": 923, "seek": 769380, "start": 7706.8, "end": 7714.8, "text": " And we define this eta to be basically the square root of the inverse signal to noise ratio.", "tokens": [51014, 400, 321, 6964, 341, 32415, 281, 312, 1936, 264, 3732, 5593, 295, 264, 17340, 6358, 281, 5658, 8509, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1349552746476798, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.004981262143701315}, {"id": 924, "seek": 771480, "start": 7714.8, "end": 7731.8, "text": " And if we assume this sigma, this epsilon theta to be the optimal model, and then this ODE is equivalent to a probability flow ODE of a variance is floating SDE, which is in the following formulation.", "tokens": [50364, 400, 498, 321, 6552, 341, 12771, 11, 341, 17889, 9725, 281, 312, 264, 16252, 2316, 11, 293, 550, 341, 422, 22296, 307, 10344, 281, 257, 8482, 3095, 422, 22296, 295, 257, 21977, 307, 12607, 14638, 36, 11, 597, 307, 294, 264, 3480, 37642, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17682059605916342, "compression_ratio": 1.3888888888888888, "no_speech_prob": 0.008442931808531284}, {"id": 925, "seek": 773180, "start": 7731.8, "end": 7746.8, "text": " Note that although these two are equivalent, the sampling procedure can still be different, because for the above ODE, we are taking like the sampling process over this eta t.", "tokens": [50364, 11633, 300, 4878, 613, 732, 366, 10344, 11, 264, 21179, 10747, 393, 920, 312, 819, 11, 570, 337, 264, 3673, 422, 22296, 11, 321, 366, 1940, 411, 264, 21179, 1399, 670, 341, 32415, 256, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17159921366993974, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.064595066010952}, {"id": 926, "seek": 773180, "start": 7746.8, "end": 7759.8, "text": " Well for the second formulation we are taking the, for example, for both of the two equations we use the standard Euler's method, then the second one is taking the Euler step over dt.", "tokens": [51114, 1042, 337, 264, 1150, 37642, 321, 366, 1940, 264, 11, 337, 1365, 11, 337, 1293, 295, 264, 732, 11787, 321, 764, 264, 3832, 462, 26318, 311, 3170, 11, 550, 264, 1150, 472, 307, 1940, 264, 462, 26318, 1823, 670, 36423, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17159921366993974, "compression_ratio": 1.751219512195122, "no_speech_prob": 0.064595066010952}, {"id": 927, "seek": 775980, "start": 7759.8, "end": 7777.8, "text": " And basically, in practice, people find that the first one works better than the second one, because it depends less on the value of t, but it depends directly on the signal to noise ratio of the current time steps.", "tokens": [50364, 400, 1936, 11, 294, 3124, 11, 561, 915, 300, 264, 700, 472, 1985, 1101, 813, 264, 1150, 472, 11, 570, 309, 5946, 1570, 322, 264, 2158, 295, 256, 11, 457, 309, 5946, 3838, 322, 264, 6358, 281, 5658, 8509, 295, 264, 2190, 565, 4439, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13825815551135004, "compression_ratio": 1.5034965034965035, "no_speech_prob": 0.009850654751062393}, {"id": 928, "seek": 777780, "start": 7777.8, "end": 7788.8, "text": " We also found that with this DDM sampler, we are able to use less time sampling steps, but reach better like performance.", "tokens": [50364, 492, 611, 1352, 300, 365, 341, 30778, 44, 3247, 22732, 11, 321, 366, 1075, 281, 764, 1570, 565, 21179, 4439, 11, 457, 2524, 1101, 411, 3389, 13, 50914], "temperature": 0.0, "avg_logprob": -0.24646271428754252, "compression_ratio": 1.174757281553398, "no_speech_prob": 0.07579163461923599}, {"id": 929, "seek": 778880, "start": 7788.8, "end": 7806.8, "text": " And in terms of why this is true, this, this paper by Carols et al, argues that the ODE of the DDM is favored, because I saw in those two in those three illustrations, especially the third illustration.", "tokens": [50364, 400, 294, 2115, 295, 983, 341, 307, 2074, 11, 341, 11, 341, 3035, 538, 7925, 82, 1030, 419, 11, 38218, 300, 264, 422, 22296, 295, 264, 30778, 44, 307, 44420, 11, 570, 286, 1866, 294, 729, 732, 294, 729, 1045, 34540, 11, 2318, 264, 2636, 22645, 13, 51264], "temperature": 0.0, "avg_logprob": -0.30538512211219937, "compression_ratio": 1.4225352112676057, "no_speech_prob": 0.012048384174704552}, {"id": 930, "seek": 780680, "start": 7806.8, "end": 7824.8, "text": " The definition of the solution trajectories of DDM always points towards the denoider outputs, while for the first two ODE formulation, the variance preserving ODE and the variance is pulling ODE, which are two very commonly used ODE formulation", "tokens": [50364, 440, 7123, 295, 264, 3827, 18257, 2083, 295, 30778, 44, 1009, 2793, 3030, 264, 1441, 78, 1438, 23930, 11, 1339, 337, 264, 700, 732, 422, 22296, 37642, 11, 264, 21977, 33173, 422, 22296, 293, 264, 21977, 307, 8407, 422, 22296, 11, 597, 366, 732, 588, 12719, 1143, 422, 22296, 37642, 51264], "temperature": 0.0, "avg_logprob": -0.21177471006238782, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.030199119821190834}, {"id": 931, "seek": 780680, "start": 7824.8, "end": 7826.8, "text": " diffusion models.", "tokens": [51264, 25242, 5245, 13, 51364], "temperature": 0.0, "avg_logprob": -0.21177471006238782, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.030199119821190834}, {"id": 932, "seek": 780680, "start": 7826.8, "end": 7832.8, "text": " They basically have more like high coverage regions along the trajectories.", "tokens": [51364, 814, 1936, 362, 544, 411, 1090, 9645, 10682, 2051, 264, 18257, 2083, 13, 51664], "temperature": 0.0, "avg_logprob": -0.21177471006238782, "compression_ratio": 1.6220095693779903, "no_speech_prob": 0.030199119821190834}, {"id": 933, "seek": 783280, "start": 7832.8, "end": 7842.8, "text": " So for the DDM, we can see like for the solution trajectories, most of the trajectories are linear and with low coverage.", "tokens": [50364, 407, 337, 264, 30778, 44, 11, 321, 393, 536, 411, 337, 264, 3827, 18257, 2083, 11, 881, 295, 264, 18257, 2083, 366, 8213, 293, 365, 2295, 9645, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16007862562014732, "compression_ratio": 1.8102564102564103, "no_speech_prob": 0.009120884351432323}, {"id": 934, "seek": 783280, "start": 7842.8, "end": 7858.8, "text": " And it is known that low coverage really means less truncation errors accumulating over the trajectories. So if we use this kind of trajectories, like we will have a smaller chance to accumulate more errors across the trajectories.", "tokens": [50864, 400, 309, 307, 2570, 300, 2295, 9645, 534, 1355, 1570, 504, 409, 46252, 13603, 12989, 12162, 670, 264, 18257, 2083, 13, 407, 498, 321, 764, 341, 733, 295, 18257, 2083, 11, 411, 321, 486, 362, 257, 4356, 2931, 281, 33384, 544, 13603, 2108, 264, 18257, 2083, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16007862562014732, "compression_ratio": 1.8102564102564103, "no_speech_prob": 0.009120884351432323}, {"id": 935, "seek": 785880, "start": 7858.8, "end": 7870.8, "text": " Thus, enable us to use like fewer number of diffusion time step, fewer number of sampling steps in inference.", "tokens": [50364, 13827, 11, 9528, 505, 281, 764, 411, 13366, 1230, 295, 25242, 565, 1823, 11, 13366, 1230, 295, 21179, 4439, 294, 38253, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19394775537344125, "compression_ratio": 1.2823529411764707, "no_speech_prob": 0.002713990630581975}, {"id": 936, "seek": 787080, "start": 7870.8, "end": 7880.8, "text": " Okay, so the third word I'm going to discuss for advanced, the forward process is just critically down to long-distance diffusion model.", "tokens": [50364, 1033, 11, 370, 264, 2636, 1349, 286, 478, 516, 281, 2248, 337, 7339, 11, 264, 2128, 1399, 307, 445, 22797, 760, 281, 938, 12, 67, 20829, 25242, 2316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.24311351776123047, "compression_ratio": 1.46875, "no_speech_prob": 0.0149474386125803}, {"id": 937, "seek": 787080, "start": 7880.8, "end": 7891.8, "text": " Basically, they are trying to find a faster mixing diffusion process by using certain like background knowledge from Markov Chen and Monte Carlo.", "tokens": [50864, 8537, 11, 436, 366, 1382, 281, 915, 257, 4663, 11983, 25242, 1399, 538, 1228, 1629, 411, 3678, 3601, 490, 3934, 5179, 13682, 293, 38105, 45112, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24311351776123047, "compression_ratio": 1.46875, "no_speech_prob": 0.0149474386125803}, {"id": 938, "seek": 789180, "start": 7891.8, "end": 7903.8, "text": " And how this forward process is related to MCMC, let's see, like this is a regular forward diffusion process, which is a stochastic differential equation.", "tokens": [50364, 400, 577, 341, 2128, 1399, 307, 4077, 281, 8797, 39261, 11, 718, 311, 536, 11, 411, 341, 307, 257, 3890, 2128, 25242, 1399, 11, 597, 307, 257, 342, 8997, 2750, 15756, 5367, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17033194224039713, "compression_ratio": 1.6, "no_speech_prob": 0.03306691348552704}, {"id": 939, "seek": 789180, "start": 7903.8, "end": 7918.8, "text": " And it is actually a special case of overdone long-distance dynamics. If we assume the target distribution or the target density of this MCMC is this standard Gaussian distribution.", "tokens": [50964, 400, 309, 307, 767, 257, 2121, 1389, 295, 19853, 546, 938, 12, 67, 20829, 15679, 13, 759, 321, 6552, 264, 3779, 7316, 420, 264, 3779, 10305, 295, 341, 8797, 39261, 307, 341, 3832, 39148, 7316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17033194224039713, "compression_ratio": 1.6, "no_speech_prob": 0.03306691348552704}, {"id": 940, "seek": 791880, "start": 7918.8, "end": 7929.8, "text": " Given this connection, we can actually design more like efficient forward process in terms of MCMC.", "tokens": [50364, 18600, 341, 4984, 11, 321, 393, 767, 1715, 544, 411, 7148, 2128, 1399, 294, 2115, 295, 8797, 39261, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1098573835272538, "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.0026309897657483816}, {"id": 941, "seek": 791880, "start": 7929.8, "end": 7942.8, "text": " Specifically, this word proposed to introduce an auxiliary variable, this velocity V, and the diffusion process is defined on the joint space of this velocity and the input.", "tokens": [50914, 26058, 11, 341, 1349, 10348, 281, 5366, 364, 43741, 7006, 11, 341, 9269, 691, 11, 293, 264, 25242, 1399, 307, 7642, 322, 264, 7225, 1901, 295, 341, 9269, 293, 264, 4846, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1098573835272538, "compression_ratio": 1.5166666666666666, "no_speech_prob": 0.0026309897657483816}, {"id": 942, "seek": 794280, "start": 7942.8, "end": 7958.8, "text": " And during the forward process, the noise is only added in this velocity space. And this image space or input space is only erupted by the coupling between this data and the velocity.", "tokens": [50364, 400, 1830, 264, 2128, 1399, 11, 264, 5658, 307, 787, 3869, 294, 341, 9269, 1901, 13, 400, 341, 3256, 1901, 420, 4846, 1901, 307, 787, 20999, 25383, 538, 264, 37447, 1296, 341, 1412, 293, 264, 9269, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07885941644994224, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.002550364937633276}, {"id": 943, "seek": 795880, "start": 7958.8, "end": 7973.8, "text": " And the resulting process as showing this figure, we can see that the forward process in the V space is still exact. However, the process in the image space or the data space are much more smoother.", "tokens": [50364, 400, 264, 16505, 1399, 382, 4099, 341, 2573, 11, 321, 393, 536, 300, 264, 2128, 1399, 294, 264, 691, 1901, 307, 920, 1900, 13, 2908, 11, 264, 1399, 294, 264, 3256, 1901, 420, 264, 1412, 1901, 366, 709, 544, 28640, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12124469545152453, "compression_ratio": 1.5114503816793894, "no_speech_prob": 0.0006877645500935614}, {"id": 944, "seek": 797380, "start": 7973.8, "end": 7998.8, "text": " This V components is analogous to the Hamiltonian components in HMC or analogous to the momentum in momentum based optimizers. So by defining this joint space or defining the diffusion in this V space, it actually enables faster mixing and faster traverse of the joint space,", "tokens": [50364, 639, 691, 6677, 307, 16660, 563, 281, 264, 18484, 952, 6677, 294, 389, 39261, 420, 16660, 563, 281, 264, 11244, 294, 11244, 2361, 5028, 22525, 13, 407, 538, 17827, 341, 7225, 1901, 420, 17827, 264, 25242, 294, 341, 691, 1901, 11, 309, 767, 17077, 4663, 11983, 293, 4663, 45674, 295, 264, 7225, 1901, 11, 51614], "temperature": 0.0, "avg_logprob": -0.17107427531275257, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.011863790452480316}, {"id": 945, "seek": 799880, "start": 7998.8, "end": 8008.8, "text": " which enables fast, like more smoothly and efficient, more smooth and efficient forward process.", "tokens": [50364, 597, 17077, 2370, 11, 411, 544, 19565, 293, 7148, 11, 544, 5508, 293, 7148, 2128, 1399, 13, 50864], "temperature": 0.0, "avg_logprob": -0.15226370288479713, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.020624659955501556}, {"id": 946, "seek": 799880, "start": 8008.8, "end": 8021.8, "text": " And the second, let's see some advanced reverse process. So before that, we would like to ask a question. So remember that we use this normal approximation of the reverse process, right.", "tokens": [50864, 400, 264, 1150, 11, 718, 311, 536, 512, 7339, 9943, 1399, 13, 407, 949, 300, 11, 321, 576, 411, 281, 1029, 257, 1168, 13, 407, 1604, 300, 321, 764, 341, 2710, 28023, 295, 264, 9943, 1399, 11, 558, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15226370288479713, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.020624659955501556}, {"id": 947, "seek": 802180, "start": 8021.8, "end": 8036.8, "text": " It seems like the denoting distributions are always Gaussian distributions. But if we want to use less diffusion time steps, is this normal approximation of the reverse process still true or accurate?", "tokens": [50364, 467, 2544, 411, 264, 1441, 17001, 37870, 366, 1009, 39148, 37870, 13, 583, 498, 321, 528, 281, 764, 1570, 25242, 565, 4439, 11, 307, 341, 2710, 28023, 295, 264, 9943, 1399, 920, 2074, 420, 8559, 30, 51114], "temperature": 0.0, "avg_logprob": -0.16122762239896335, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.013628088869154453}, {"id": 948, "seek": 802180, "start": 8036.8, "end": 8048.8, "text": " Unfortunately, the answer is no. So this assumption, normal assumption in this denoting distribution holds only when the adjacent", "tokens": [51114, 8590, 11, 264, 1867, 307, 572, 13, 407, 341, 15302, 11, 2710, 15302, 294, 341, 1441, 17001, 7316, 9190, 787, 562, 264, 24441, 51714], "temperature": 0.0, "avg_logprob": -0.16122762239896335, "compression_ratio": 1.6336633663366336, "no_speech_prob": 0.013628088869154453}, {"id": 949, "seek": 804880, "start": 8048.8, "end": 8063.8, "text": " the noise added between these adjacent steps are small. If we want to use less diffusion time steps in training, then we can see that this denoting distribution is not a unimodal normal distribution anymore.", "tokens": [50364, 264, 5658, 3869, 1296, 613, 24441, 4439, 366, 1359, 13, 759, 321, 528, 281, 764, 1570, 25242, 565, 4439, 294, 3097, 11, 550, 321, 393, 536, 300, 341, 1441, 17001, 7316, 307, 406, 257, 517, 332, 378, 304, 2710, 7316, 3602, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15081286430358887, "compression_ratio": 1.7255813953488373, "no_speech_prob": 0.001000159652903676}, {"id": 950, "seek": 804880, "start": 8063.8, "end": 8076.8, "text": " So they are tend to be like multimodal and more complicated distributions. So in that case, it means like we really need more complicated functional approximators.", "tokens": [51114, 407, 436, 366, 3928, 281, 312, 411, 32972, 378, 304, 293, 544, 6179, 37870, 13, 407, 294, 300, 1389, 11, 309, 1355, 411, 321, 534, 643, 544, 6179, 11745, 8542, 3391, 13, 51764], "temperature": 0.0, "avg_logprob": -0.15081286430358887, "compression_ratio": 1.7255813953488373, "no_speech_prob": 0.001000159652903676}, {"id": 951, "seek": 807680, "start": 8076.8, "end": 8085.8, "text": " So we will talk about two examples of how we can include more complicated functional approximators here.", "tokens": [50364, 407, 321, 486, 751, 466, 732, 5110, 295, 577, 321, 393, 4090, 544, 6179, 11745, 8542, 3391, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16630246504297796, "compression_ratio": 1.4939024390243902, "no_speech_prob": 0.0038826959207654}, {"id": 952, "seek": 807680, "start": 8085.8, "end": 8095.8, "text": " The first word is this denoting diffusion gains. So in this word, they propose to model the denoting distribution by conditional gain model.", "tokens": [50814, 440, 700, 1349, 307, 341, 1441, 17001, 25242, 16823, 13, 407, 294, 341, 1349, 11, 436, 17421, 281, 2316, 264, 1441, 17001, 7316, 538, 27708, 6052, 2316, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16630246504297796, "compression_ratio": 1.4939024390243902, "no_speech_prob": 0.0038826959207654}, {"id": 953, "seek": 809580, "start": 8095.8, "end": 8110.8, "text": " Specifically, the model is training this other several learning framework. And we first get the samples xt minus one and xt by running this forward diffusion process.", "tokens": [50364, 26058, 11, 264, 2316, 307, 3097, 341, 661, 2940, 2539, 8388, 13, 400, 321, 700, 483, 264, 10938, 220, 734, 3175, 472, 293, 220, 734, 538, 2614, 341, 2128, 25242, 1399, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1962723599539863, "compression_ratio": 1.3174603174603174, "no_speech_prob": 0.007457963656634092}, {"id": 954, "seek": 811080, "start": 8110.8, "end": 8130.8, "text": " And then this generator takes xt as input, as well as the time step as input. And it's actually trying to model the xt minus one. But instead of just directly outputting xt minus one from the network, it's first trying to predict this", "tokens": [50364, 400, 550, 341, 19265, 2516, 220, 734, 382, 4846, 11, 382, 731, 382, 264, 565, 1823, 382, 4846, 13, 400, 309, 311, 767, 1382, 281, 2316, 264, 220, 734, 3175, 472, 13, 583, 2602, 295, 445, 3838, 5598, 783, 220, 734, 3175, 472, 490, 264, 3209, 11, 309, 311, 700, 1382, 281, 6069, 341, 51364], "temperature": 0.0, "avg_logprob": -0.14484558434321962, "compression_ratio": 1.5394736842105263, "no_speech_prob": 0.024783965200185776}, {"id": 955, "seek": 813080, "start": 8130.8, "end": 8149.8, "text": " screen sample x0. And then it tries to sample xt minus one from this tractable posterior distribution. And then the discriminator takes the real xt minus one and the fake xt minus one prime as input and try to discriminate these two samples.", "tokens": [50364, 2568, 6889, 2031, 15, 13, 400, 550, 309, 9898, 281, 6889, 220, 734, 3175, 472, 490, 341, 24207, 712, 33529, 7316, 13, 400, 550, 264, 20828, 1639, 2516, 264, 957, 220, 734, 3175, 472, 293, 264, 7592, 220, 734, 3175, 472, 5835, 382, 4846, 293, 853, 281, 47833, 613, 732, 10938, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1480004160027755, "compression_ratio": 1.8908045977011494, "no_speech_prob": 0.04398781806230545}, {"id": 956, "seek": 813080, "start": 8149.8, "end": 8157.8, "text": " And again, this discriminator is also conditional xt and the corresponding time step t.", "tokens": [51314, 400, 797, 11, 341, 20828, 1639, 307, 611, 27708, 220, 734, 293, 264, 11760, 565, 1823, 256, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1480004160027755, "compression_ratio": 1.8908045977011494, "no_speech_prob": 0.04398781806230545}, {"id": 957, "seek": 815780, "start": 8157.8, "end": 8165.8, "text": " One may ask, what is the benefit of this denoting diffusion gains compared to a one shot gain model.", "tokens": [50364, 1485, 815, 1029, 11, 437, 307, 264, 5121, 295, 341, 1441, 17001, 25242, 16823, 5347, 281, 257, 472, 3347, 6052, 2316, 13, 50764], "temperature": 0.0, "avg_logprob": -0.16663315421656558, "compression_ratio": 1.5625, "no_speech_prob": 0.006190130487084389}, {"id": 958, "seek": 815780, "start": 8165.8, "end": 8177.8, "text": " But this paper shows that because like right now the conditional gains only need to model like the conditional distribution of xt minus one given xt.", "tokens": [50764, 583, 341, 3035, 3110, 300, 570, 411, 558, 586, 264, 27708, 16823, 787, 643, 281, 2316, 411, 264, 27708, 7316, 295, 220, 734, 3175, 472, 2212, 220, 734, 13, 51364], "temperature": 0.0, "avg_logprob": -0.16663315421656558, "compression_ratio": 1.5625, "no_speech_prob": 0.006190130487084389}, {"id": 959, "seek": 817780, "start": 8177.8, "end": 8187.8, "text": " This turns out to be a much simpler problem for both the generator and the discriminator compared to directly model the model distribution of the clean sample.", "tokens": [50364, 639, 4523, 484, 281, 312, 257, 709, 18587, 1154, 337, 1293, 264, 19265, 293, 264, 20828, 1639, 5347, 281, 3838, 2316, 264, 2316, 7316, 295, 264, 2541, 6889, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09498724142710367, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.012814640998840332}, {"id": 960, "seek": 817780, "start": 8187.8, "end": 8200.8, "text": " And this simple training objective for the two models leads to stronger mode coverage properties of gains and also leads to better training stability.", "tokens": [50864, 400, 341, 2199, 3097, 10024, 337, 264, 732, 5245, 6689, 281, 7249, 4391, 9645, 7221, 295, 16823, 293, 611, 6689, 281, 1101, 3097, 11826, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09498724142710367, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.012814640998840332}, {"id": 961, "seek": 820080, "start": 8200.8, "end": 8216.8, "text": " And recall that in the second part we learned that there's a close connection between energy based models and diffusion models. So, therefore, a natural idea is like we try to approximate the reverse process by conditional energy based models.", "tokens": [50364, 400, 9901, 300, 294, 264, 1150, 644, 321, 3264, 300, 456, 311, 257, 1998, 4984, 1296, 2281, 2361, 5245, 293, 25242, 5245, 13, 407, 11, 4412, 11, 257, 3303, 1558, 307, 411, 321, 853, 281, 30874, 264, 9943, 1399, 538, 27708, 2281, 2361, 5245, 13, 51164], "temperature": 0.0, "avg_logprob": -0.15567718233380998, "compression_ratio": 1.5379746835443038, "no_speech_prob": 0.00280030001886189}, {"id": 962, "seek": 821680, "start": 8216.8, "end": 8230.8, "text": " Recall that an energy based model is in the form of P beta x. It is solely dependent on the normalized log density function which is f theta x.", "tokens": [50364, 9647, 336, 300, 364, 2281, 2361, 2316, 307, 294, 264, 1254, 295, 430, 9861, 2031, 13, 467, 307, 23309, 12334, 322, 264, 48704, 3565, 10305, 2445, 597, 307, 283, 9725, 2031, 13, 51064], "temperature": 0.0, "avg_logprob": -0.29231016976492746, "compression_ratio": 1.7536945812807883, "no_speech_prob": 0.0012063878821209073}, {"id": 963, "seek": 821680, "start": 8230.8, "end": 8245.8, "text": " And we can further prime trend is f theta x by minus e theta x. So usually, people call this e theta x as the energy function. And this d theta is the partition function which is usually analytical and tractable.", "tokens": [51064, 400, 321, 393, 3052, 5835, 6028, 307, 283, 9725, 2031, 538, 3175, 308, 9725, 2031, 13, 407, 2673, 11, 561, 818, 341, 308, 9725, 2031, 382, 264, 2281, 2445, 13, 400, 341, 274, 9725, 307, 264, 24808, 2445, 597, 307, 2673, 29579, 293, 24207, 712, 13, 51814], "temperature": 0.0, "avg_logprob": -0.29231016976492746, "compression_ratio": 1.7536945812807883, "no_speech_prob": 0.0012063878821209073}, {"id": 964, "seek": 824580, "start": 8246.8, "end": 8256.8, "text": " And we can parameterize this f theta by a neural network which takes the signal as input and output as scalar to represent the value of this f.", "tokens": [50414, 400, 321, 393, 13075, 1125, 341, 283, 9725, 538, 257, 18161, 3209, 597, 2516, 264, 6358, 382, 4846, 293, 5598, 382, 39684, 281, 2906, 264, 2158, 295, 341, 283, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13796727619473897, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0016736661782488227}, {"id": 965, "seek": 824580, "start": 8256.8, "end": 8265.8, "text": " And the learning of energy based models can be illustrated as follows. So suppose this is the energy landscape defined by the e theta function.", "tokens": [50914, 400, 264, 2539, 295, 2281, 2361, 5245, 393, 312, 33875, 382, 10002, 13, 407, 7297, 341, 307, 264, 2281, 9661, 7642, 538, 264, 308, 9725, 2445, 13, 51364], "temperature": 0.0, "avg_logprob": -0.13796727619473897, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0016736661782488227}, {"id": 966, "seek": 826580, "start": 8266.8, "end": 8280.8, "text": " And after learning, we would like to put the observation data points into the regions of low energy and put all the other inputs to the regions of the high energy.", "tokens": [50414, 400, 934, 2539, 11, 321, 576, 411, 281, 829, 264, 14816, 1412, 2793, 666, 264, 10682, 295, 2295, 2281, 293, 829, 439, 264, 661, 15743, 281, 264, 10682, 295, 264, 1090, 2281, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1702723503112793, "compression_ratio": 1.4424778761061947, "no_speech_prob": 0.01000957004725933}, {"id": 967, "seek": 828080, "start": 8280.8, "end": 8298.8, "text": " And optimizing the energy based models require, you really require the MSMC sampling from the current model p theta x as showing this formulation, which is really highly computational expensive, especially for high dimensional data.", "tokens": [50364, 400, 40425, 264, 2281, 2361, 5245, 3651, 11, 291, 534, 3651, 264, 7395, 39261, 21179, 490, 264, 2190, 2316, 280, 9725, 2031, 382, 4099, 341, 37642, 11, 597, 307, 534, 5405, 28270, 5124, 11, 2318, 337, 1090, 18795, 1412, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2379375154321844, "compression_ratio": 1.4683544303797469, "no_speech_prob": 0.0006985726649872959}, {"id": 968, "seek": 829880, "start": 8299.8, "end": 8315.8, "text": " So if we want to parameterize the denoising distribution by conditional energy based model, we can start by assuming like at each diffusion time step marginally the data follows a energy based model in the standard formulation.", "tokens": [50414, 407, 498, 321, 528, 281, 13075, 1125, 264, 1441, 78, 3436, 7316, 538, 27708, 2281, 2361, 2316, 11, 321, 393, 722, 538, 11926, 411, 412, 1184, 25242, 565, 1823, 10270, 379, 264, 1412, 10002, 257, 2281, 2361, 2316, 294, 264, 3832, 37642, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17112531053259017, "compression_ratio": 1.5133333333333334, "no_speech_prob": 0.00036824544076807797}, {"id": 969, "seek": 831580, "start": 8315.8, "end": 8323.8, "text": " So here I removed the, the script like the time step script for similar simplicity.", "tokens": [50364, 407, 510, 286, 7261, 264, 11, 264, 5755, 411, 264, 565, 1823, 5755, 337, 2531, 25632, 13, 50764], "temperature": 0.0, "avg_logprob": -0.334054602517022, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.00044415268348529935}, {"id": 970, "seek": 831580, "start": 8323.8, "end": 8327.8, "text": " And let x theta be the data at a higher noise level.", "tokens": [50764, 400, 718, 2031, 9725, 312, 264, 1412, 412, 257, 2946, 5658, 1496, 13, 50964], "temperature": 0.0, "avg_logprob": -0.334054602517022, "compression_ratio": 1.2952380952380953, "no_speech_prob": 0.00044415268348529935}, {"id": 971, "seek": 832780, "start": 8327.8, "end": 8337.8, "text": " So we can derive the conditional energy based models by Bayes and Rowe, but specifically this p x, the x theta is in this formulation.", "tokens": [50364, 407, 321, 393, 28446, 264, 27708, 2281, 2361, 5245, 538, 7840, 279, 293, 497, 6880, 11, 457, 4682, 341, 280, 2031, 11, 264, 2031, 9725, 307, 294, 341, 37642, 13, 50864], "temperature": 0.0, "avg_logprob": -0.24937547956194198, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0013042795471847057}, {"id": 972, "seek": 832780, "start": 8337.8, "end": 8348.8, "text": " And if we compare this conditional energy based models with with the original marginal energy based models, we see that the only difference is that there's an extra projected term here.", "tokens": [50864, 400, 498, 321, 6794, 341, 27708, 2281, 2361, 5245, 365, 365, 264, 3380, 16885, 2281, 2361, 5245, 11, 321, 536, 300, 264, 787, 2649, 307, 300, 456, 311, 364, 2857, 26231, 1433, 510, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24937547956194198, "compression_ratio": 1.7297297297297298, "no_speech_prob": 0.0013042795471847057}, {"id": 973, "seek": 834880, "start": 8348.8, "end": 8370.8, "text": " And this actual projected term actually has the effect of localize this highly multimodal energy landscape to a like more single mode model or uni model landscape, and with the model mode focus around the higher noise level signal x theta.", "tokens": [50364, 400, 341, 3539, 26231, 1433, 767, 575, 264, 1802, 295, 2654, 1125, 341, 5405, 32972, 378, 304, 2281, 9661, 281, 257, 411, 544, 2167, 4391, 2316, 420, 36435, 2316, 9661, 11, 293, 365, 264, 2316, 4391, 1879, 926, 264, 2946, 5658, 1496, 6358, 2031, 9725, 13, 51464], "temperature": 0.0, "avg_logprob": -0.23642915725708008, "compression_ratio": 1.5620915032679739, "no_speech_prob": 0.0021821539849042892}, {"id": 974, "seek": 837080, "start": 8371.8, "end": 8388.8, "text": " Therefore, compared to training a single energy based model the sampling here is more friendly and easier to converge because the energy landscape compared to the original marginal energy landscape is more uni model and more and simpler.", "tokens": [50414, 7504, 11, 5347, 281, 3097, 257, 2167, 2281, 2361, 2316, 264, 21179, 510, 307, 544, 9208, 293, 3571, 281, 41881, 570, 264, 2281, 9661, 5347, 281, 264, 3380, 16885, 2281, 9661, 307, 544, 36435, 2316, 293, 544, 293, 18587, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22160898555408826, "compression_ratio": 1.6573426573426573, "no_speech_prob": 0.000314964447170496}, {"id": 975, "seek": 838880, "start": 8388.8, "end": 8397.8, "text": " So that the training could be more efficient and the conversion and CMC can give us well formed energy potential after training.", "tokens": [50364, 407, 300, 264, 3097, 727, 312, 544, 7148, 293, 264, 14298, 293, 20424, 34, 393, 976, 505, 731, 8693, 2281, 3995, 934, 3097, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23010014796602554, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.0007670908235013485}, {"id": 976, "seek": 838880, "start": 8397.8, "end": 8415.8, "text": " And compared to diffusion models, this energy based to using this energy based model to a parameterize the denoting distribution can give enables us to define like much less diffusion steps up to six steps.", "tokens": [50814, 400, 5347, 281, 25242, 5245, 11, 341, 2281, 2361, 281, 1228, 341, 2281, 2361, 2316, 281, 257, 13075, 1125, 264, 1441, 17001, 7316, 393, 976, 17077, 505, 281, 6964, 411, 709, 1570, 25242, 4439, 493, 281, 2309, 4439, 13, 51714], "temperature": 0.0, "avg_logprob": -0.23010014796602554, "compression_ratio": 1.6502463054187193, "no_speech_prob": 0.0007670908235013485}, {"id": 977, "seek": 841580, "start": 8415.8, "end": 8424.8, "text": " And more specifically to learn those models we simply maximize the conditional log likelihoods at each time step.", "tokens": [50364, 400, 544, 4682, 281, 1466, 729, 5245, 321, 2935, 19874, 264, 27708, 3565, 22119, 82, 412, 1184, 565, 1823, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16726991242053463, "compression_ratio": 1.5209580838323353, "no_speech_prob": 0.00028230276075191796}, {"id": 978, "seek": 841580, "start": 8424.8, "end": 8436.8, "text": " And then after training we just get samples by progressive sampling from the energy based models from high noise levels to low noise levels.", "tokens": [50814, 400, 550, 934, 3097, 321, 445, 483, 10938, 538, 16131, 21179, 490, 264, 2281, 2361, 5245, 490, 1090, 5658, 4358, 281, 2295, 5658, 4358, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16726991242053463, "compression_ratio": 1.5209580838323353, "no_speech_prob": 0.00028230276075191796}, {"id": 979, "seek": 843680, "start": 8436.8, "end": 8449.8, "text": " So the last part comes to the advanced diffusion models, basically want to ask two questions. For first, can we do model distillation so that the distilled model can do faster sampling.", "tokens": [50364, 407, 264, 1036, 644, 1487, 281, 264, 7339, 25242, 5245, 11, 1936, 528, 281, 1029, 732, 1651, 13, 1171, 700, 11, 393, 321, 360, 2316, 42923, 399, 370, 300, 264, 1483, 6261, 2316, 393, 360, 4663, 21179, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19087210155668713, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.000791476049926132}, {"id": 980, "seek": 843680, "start": 8449.8, "end": 8457.8, "text": " And second, can we lift the diffusion model to a latent space that is faster to diffuse.", "tokens": [51014, 400, 1150, 11, 393, 321, 5533, 264, 25242, 2316, 281, 257, 48994, 1901, 300, 307, 4663, 281, 42165, 13, 51414], "temperature": 0.0, "avg_logprob": -0.19087210155668713, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.000791476049926132}, {"id": 981, "seek": 845780, "start": 8457.8, "end": 8470.8, "text": " So the first idea comes to the distillation so here I want to discuss one representative work in this domain, which is this progressive distillation of diffusion models.", "tokens": [50364, 407, 264, 700, 1558, 1487, 281, 264, 42923, 399, 370, 510, 286, 528, 281, 2248, 472, 12424, 589, 294, 341, 9274, 11, 597, 307, 341, 16131, 42923, 399, 295, 25242, 5245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07626962661743164, "compression_ratio": 1.4083333333333334, "no_speech_prob": 0.001925994292832911}, {"id": 982, "seek": 847080, "start": 8470.8, "end": 8479.8, "text": " So essentially this work proposed to distill a deterministic DDIM sampler to the same model architecture of the original model.", "tokens": [50364, 407, 4476, 341, 589, 10348, 281, 42923, 257, 15957, 3142, 413, 3085, 44, 3247, 22732, 281, 264, 912, 2316, 9482, 295, 264, 3380, 2316, 13, 50814], "temperature": 0.0, "avg_logprob": -0.230161452293396, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0446295402944088}, {"id": 983, "seek": 847080, "start": 8479.8, "end": 8498.8, "text": " And it's, it is, it went into this progressive pipeline, in a sense that at each distillation stage, we will have a teacher model, and, and we will learn a student model and this student model is learned to distill.", "tokens": [50814, 400, 309, 311, 11, 309, 307, 11, 309, 1437, 666, 341, 16131, 15517, 11, 294, 257, 2020, 300, 412, 1184, 42923, 399, 3233, 11, 321, 486, 362, 257, 5027, 2316, 11, 293, 11, 293, 321, 486, 1466, 257, 3107, 2316, 293, 341, 3107, 2316, 307, 3264, 281, 42923, 13, 51764], "temperature": 0.0, "avg_logprob": -0.230161452293396, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0446295402944088}, {"id": 984, "seek": 849880, "start": 8498.8, "end": 8510.8, "text": " By each two adjacent sampling steps of the teacher model to one sampling step of the student model. And after learning this student model at next distillation stage.", "tokens": [50364, 3146, 1184, 732, 24441, 21179, 4439, 295, 264, 5027, 2316, 281, 472, 21179, 1823, 295, 264, 3107, 2316, 13, 400, 934, 2539, 341, 3107, 2316, 412, 958, 42923, 399, 3233, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15148568509229973, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.011860553175210953}, {"id": 985, "seek": 849880, "start": 8510.8, "end": 8520.8, "text": " The student model as a previous stage will serve as the teacher model at this new stage, and then we learn another student model at the new stage.", "tokens": [50964, 440, 3107, 2316, 382, 257, 3894, 3233, 486, 4596, 382, 264, 5027, 2316, 412, 341, 777, 3233, 11, 293, 550, 321, 1466, 1071, 3107, 2316, 412, 264, 777, 3233, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15148568509229973, "compression_ratio": 1.9259259259259258, "no_speech_prob": 0.011860553175210953}, {"id": 986, "seek": 852080, "start": 8520.8, "end": 8530.8, "text": " So we need this process until we can distill the original thousands of sampling steps to a single sampling step.", "tokens": [50364, 407, 321, 643, 341, 1399, 1826, 321, 393, 42923, 264, 3380, 5383, 295, 21179, 4439, 281, 257, 2167, 21179, 1823, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2314274210325429, "compression_ratio": 1.748768472906404, "no_speech_prob": 0.0053831711411476135}, {"id": 987, "seek": 852080, "start": 8530.8, "end": 8538.8, "text": " And implementation wise, the learning of the student model is quite similar to the original diffusion model training pipeline.", "tokens": [50864, 400, 11420, 10829, 11, 264, 2539, 295, 264, 3107, 2316, 307, 1596, 2531, 281, 264, 3380, 25242, 2316, 3097, 15517, 13, 51264], "temperature": 0.0, "avg_logprob": -0.2314274210325429, "compression_ratio": 1.748768472906404, "no_speech_prob": 0.0053831711411476135}, {"id": 988, "seek": 852080, "start": 8538.8, "end": 8548.8, "text": " The difference is how we define this training target of the diffusion model, specifically, given the teacher model.", "tokens": [51264, 440, 2649, 307, 577, 321, 6964, 341, 3097, 3779, 295, 264, 25242, 2316, 11, 4682, 11, 2212, 264, 5027, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2314274210325429, "compression_ratio": 1.748768472906404, "no_speech_prob": 0.0053831711411476135}, {"id": 989, "seek": 854880, "start": 8548.8, "end": 8556.8, "text": " And we randomly sample a time step T, and then we draw, we run the sampler for two steps.", "tokens": [50364, 400, 321, 16979, 6889, 257, 565, 1823, 314, 11, 293, 550, 321, 2642, 11, 321, 1190, 264, 3247, 22732, 337, 732, 4439, 13, 50764], "temperature": 0.0, "avg_logprob": -0.21204754284449986, "compression_ratio": 1.5655172413793104, "no_speech_prob": 0.0040049124509096146}, {"id": 990, "seek": 854880, "start": 8556.8, "end": 8567.8, "text": " And then the target is the, is computed to make sure that the student model can reproduce the two sampling step within one sampling step.", "tokens": [50764, 400, 550, 264, 3779, 307, 264, 11, 307, 40610, 281, 652, 988, 300, 264, 3107, 2316, 393, 29501, 264, 732, 21179, 1823, 1951, 472, 21179, 1823, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21204754284449986, "compression_ratio": 1.5655172413793104, "no_speech_prob": 0.0040049124509096146}, {"id": 991, "seek": 856780, "start": 8567.8, "end": 8584.8, "text": " And then the loss is defined as Euro where we minimize the out to distance between this target and the predicted, like X hat from this diffusion model or from the student model.", "tokens": [50364, 400, 550, 264, 4470, 307, 7642, 382, 3010, 689, 321, 17522, 264, 484, 281, 4560, 1296, 341, 3779, 293, 264, 19147, 11, 411, 1783, 2385, 490, 341, 25242, 2316, 420, 490, 264, 3107, 2316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2096300572156906, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.001548359403386712}, {"id": 992, "seek": 856780, "start": 8584.8, "end": 8595.8, "text": " And after that, we have in the number of sampling steps and repeat this process until we reach one sampling step.", "tokens": [51214, 400, 934, 300, 11, 321, 362, 294, 264, 1230, 295, 21179, 4439, 293, 7149, 341, 1399, 1826, 321, 2524, 472, 21179, 1823, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2096300572156906, "compression_ratio": 1.572972972972973, "no_speech_prob": 0.001548359403386712}, {"id": 993, "seek": 859580, "start": 8595.8, "end": 8604.8, "text": " Another idea is like whether we can leave the diffusion models to a latent space, which is more friendly to this diffusion process.", "tokens": [50364, 3996, 1558, 307, 411, 1968, 321, 393, 1856, 264, 25242, 5245, 281, 257, 48994, 1901, 11, 597, 307, 544, 9208, 281, 341, 25242, 1399, 13, 50814], "temperature": 0.0, "avg_logprob": -0.20269121442522323, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.005466127302497625}, {"id": 994, "seek": 859580, "start": 8604.8, "end": 8616.8, "text": " And here's an example of this kind of idea where we can try to leave the diffusion models to a latent space of a pre trained variation of encoder.", "tokens": [50814, 400, 510, 311, 364, 1365, 295, 341, 733, 295, 1558, 689, 321, 393, 853, 281, 1856, 264, 25242, 5245, 281, 257, 48994, 1901, 295, 257, 659, 8895, 12990, 295, 2058, 19866, 13, 51414], "temperature": 0.0, "avg_logprob": -0.20269121442522323, "compression_ratio": 1.759493670886076, "no_speech_prob": 0.005466127302497625}, {"id": 995, "seek": 861680, "start": 8616.8, "end": 8633.8, "text": " In the latent space, the distribution of the data in this latent space is already quite close to the Gaussian distributions, which means like we can definitely use less diffusion time steps to diffuse the data in this latent space.", "tokens": [50364, 682, 264, 48994, 1901, 11, 264, 7316, 295, 264, 1412, 294, 341, 48994, 1901, 307, 1217, 1596, 1998, 281, 264, 39148, 37870, 11, 597, 1355, 411, 321, 393, 2138, 764, 1570, 25242, 565, 4439, 281, 42165, 264, 1412, 294, 341, 48994, 1901, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12372437943803503, "compression_ratio": 1.7238805970149254, "no_speech_prob": 0.028418056666851044}, {"id": 996, "seek": 863380, "start": 8633.8, "end": 8647.8, "text": " The advantages are pretty straightforward. So first, because this latent space already close to normal distribution, we are able to use less diffusion time steps to enable faster sampling.", "tokens": [50364, 440, 14906, 366, 1238, 15325, 13, 407, 700, 11, 570, 341, 48994, 1901, 1217, 1998, 281, 2710, 7316, 11, 321, 366, 1075, 281, 764, 1570, 25242, 565, 4439, 281, 9528, 4663, 21179, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13246858442151868, "compression_ratio": 1.3823529411764706, "no_speech_prob": 0.010323899798095226}, {"id": 997, "seek": 864780, "start": 8647.8, "end": 8659.8, "text": " And then compared to the original variational auto encoders, which assume that the prior distribution of the Z follows a single and simple Gaussian distribution.", "tokens": [50364, 400, 550, 5347, 281, 264, 3380, 3034, 1478, 8399, 2058, 378, 433, 11, 597, 6552, 300, 264, 4059, 7316, 295, 264, 1176, 10002, 257, 2167, 293, 2199, 39148, 7316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.21177305998625578, "compression_ratio": 1.8428571428571427, "no_speech_prob": 0.012051273137331009}, {"id": 998, "seek": 864780, "start": 8659.8, "end": 8675.8, "text": " This kind of hybrid model assume that the PFC is modeled by a diffusion models, which means that it has a diffusion prior. So it definitely will be much more expressive compared to the original variational auto encoder model.", "tokens": [50964, 639, 733, 295, 13051, 2316, 6552, 300, 264, 430, 18671, 307, 37140, 538, 257, 25242, 5245, 11, 597, 1355, 300, 309, 575, 257, 25242, 4059, 13, 407, 309, 2138, 486, 312, 709, 544, 40189, 5347, 281, 264, 3380, 3034, 1478, 8399, 2058, 19866, 2316, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21177305998625578, "compression_ratio": 1.8428571428571427, "no_speech_prob": 0.012051273137331009}, {"id": 999, "seek": 867580, "start": 8675.8, "end": 8690.8, "text": " So we can actually record that for the current stage that diffusion models only defining a continuous data space. However, there are more domains which may have more complicated data structure.", "tokens": [50364, 407, 321, 393, 767, 2136, 300, 337, 264, 2190, 3233, 300, 25242, 5245, 787, 17827, 257, 10957, 1412, 1901, 13, 2908, 11, 456, 366, 544, 25514, 597, 815, 362, 544, 6179, 1412, 3877, 13, 51114], "temperature": 0.0, "avg_logprob": -0.26384273328279195, "compression_ratio": 1.3985507246376812, "no_speech_prob": 0.010984259657561779}, {"id": 1000, "seek": 869080, "start": 8690.8, "end": 8706.8, "text": " So this data type, as long as we can find an auto encoder model which are tailored to that data type and can map the data input to a continuous latent space, we will be able to apply the diffusion models to that latent space.", "tokens": [50364, 407, 341, 1412, 2010, 11, 382, 938, 382, 321, 393, 915, 364, 8399, 2058, 19866, 2316, 597, 366, 34858, 281, 300, 1412, 2010, 293, 393, 4471, 264, 1412, 4846, 281, 257, 10957, 48994, 1901, 11, 321, 486, 312, 1075, 281, 3079, 264, 25242, 5245, 281, 300, 48994, 1901, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14283761462649783, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.006487476639449596}, {"id": 1001, "seek": 869080, "start": 8706.8, "end": 8716.8, "text": " So these give us more possibilities to apply diffusion models to different modalities and different data types.", "tokens": [51164, 407, 613, 976, 505, 544, 12178, 281, 3079, 25242, 5245, 281, 819, 1072, 16110, 293, 819, 1412, 3467, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14283761462649783, "compression_ratio": 1.8415300546448088, "no_speech_prob": 0.006487476639449596}, {"id": 1002, "seek": 871680, "start": 8716.8, "end": 8731.8, "text": " And a bit of the detailed formulation. So in this work, again, we optimize the model in terms of by minimizing the variational upper bound of the negative log likelihood.", "tokens": [50364, 400, 257, 857, 295, 264, 9942, 37642, 13, 407, 294, 341, 589, 11, 797, 11, 321, 19719, 264, 2316, 294, 2115, 295, 538, 46608, 264, 3034, 1478, 6597, 5472, 295, 264, 3671, 3565, 22119, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16037394450261042, "compression_ratio": 1.3492063492063493, "no_speech_prob": 0.006586587987840176}, {"id": 1003, "seek": 873180, "start": 8731.8, "end": 8745.8, "text": " The objective contains three terms. The first two terms are similar to the variational auto encoder objecting. And the third term corresponds to the training objective of the diffusion models.", "tokens": [50364, 440, 10024, 8306, 1045, 2115, 13, 440, 700, 732, 2115, 366, 2531, 281, 264, 3034, 1478, 8399, 2058, 19866, 2657, 278, 13, 400, 264, 2636, 1433, 23249, 281, 264, 3097, 10024, 295, 264, 25242, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13445737361907958, "compression_ratio": 1.4883720930232558, "no_speech_prob": 0.003482901956886053}, {"id": 1004, "seek": 874580, "start": 8745.8, "end": 8757.8, "text": " And it actually corresponds to, we treat the encoding latents from this QZ0 given X as the observed data of the diffusion models.", "tokens": [50364, 400, 309, 767, 23249, 281, 11, 321, 2387, 264, 43430, 4465, 791, 490, 341, 1249, 57, 15, 2212, 1783, 382, 264, 13095, 1412, 295, 264, 25242, 5245, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16763648119839755, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.0033761574886739254}, {"id": 1005, "seek": 874580, "start": 8757.8, "end": 8768.8, "text": " And in that way, we can derive the similar training objective of the diffusion models as the original one. So we first do this random sampling of time step.", "tokens": [50964, 400, 294, 300, 636, 11, 321, 393, 28446, 264, 2531, 3097, 10024, 295, 264, 25242, 5245, 382, 264, 3380, 472, 13, 407, 321, 700, 360, 341, 4974, 21179, 295, 565, 1823, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16763648119839755, "compression_ratio": 1.5628415300546448, "no_speech_prob": 0.0033761574886739254}, {"id": 1006, "seek": 876880, "start": 8768.8, "end": 8782.8, "text": " And then we draw samples from this forward diffusion. And then we have this diffusion kernel. This is the forward, this is from the forward process. And then we learn this score function for DT.", "tokens": [50364, 400, 550, 321, 2642, 10938, 490, 341, 2128, 25242, 13, 400, 550, 321, 362, 341, 25242, 28256, 13, 639, 307, 264, 2128, 11, 341, 307, 490, 264, 2128, 1399, 13, 400, 550, 321, 1466, 341, 6175, 2445, 337, 413, 51, 13, 51064], "temperature": 0.0, "avg_logprob": -0.150007117878307, "compression_ratio": 1.8053097345132743, "no_speech_prob": 0.0009109245147556067}, {"id": 1007, "seek": 876880, "start": 8782.8, "end": 8789.8, "text": " And of course we have some constant that is irrelevant of the model parameters.", "tokens": [51064, 400, 295, 1164, 321, 362, 512, 5754, 300, 307, 28682, 295, 264, 2316, 9834, 13, 51414], "temperature": 0.0, "avg_logprob": -0.150007117878307, "compression_ratio": 1.8053097345132743, "no_speech_prob": 0.0009109245147556067}, {"id": 1008, "seek": 876880, "start": 8789.8, "end": 8797.8, "text": " Okay, so the second question we want to answer is how to do high resolution optionally conditional generation using diffusion models.", "tokens": [51414, 1033, 11, 370, 264, 1150, 1168, 321, 528, 281, 1867, 307, 577, 281, 360, 1090, 8669, 3614, 379, 27708, 5125, 1228, 25242, 5245, 13, 51814], "temperature": 0.0, "avg_logprob": -0.150007117878307, "compression_ratio": 1.8053097345132743, "no_speech_prob": 0.0009109245147556067}, {"id": 1009, "seek": 879780, "start": 8797.8, "end": 8813.8, "text": " In the past two years, we have seen many impressive conditional generation results using diffusion models. For example, this style II and imagine recruits diffusion models to do high resolution text to image generation.", "tokens": [50364, 682, 264, 1791, 732, 924, 11, 321, 362, 1612, 867, 8992, 27708, 5125, 3542, 1228, 25242, 5245, 13, 1171, 1365, 11, 341, 3758, 6351, 293, 3811, 9372, 1208, 25242, 5245, 281, 360, 1090, 8669, 2487, 281, 3256, 5125, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17197777795009925, "compression_ratio": 1.7248677248677249, "no_speech_prob": 0.00043048293446190655}, {"id": 1010, "seek": 879780, "start": 8813.8, "end": 8822.8, "text": " And another examples includes the using conditional diffusion models for super resolution or colorization.", "tokens": [51164, 400, 1071, 5110, 5974, 264, 1228, 27708, 25242, 5245, 337, 1687, 8669, 420, 2017, 2144, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17197777795009925, "compression_ratio": 1.7248677248677249, "no_speech_prob": 0.00043048293446190655}, {"id": 1011, "seek": 882280, "start": 8822.8, "end": 8830.8, "text": " Panorama generation is another example where we take a small size input but generate this panorama.", "tokens": [50364, 7557, 32988, 5125, 307, 1071, 1365, 689, 321, 747, 257, 1359, 2744, 4846, 457, 8460, 341, 2462, 32988, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14593401822176846, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.0021823483984917402}, {"id": 1012, "seek": 882280, "start": 8830.8, "end": 8843.8, "text": " So how can we do that? Let's first take a look at the general formulation of conditional diffusion models, which is pretty straightforward. So the only modification we need to make is in this reverse process.", "tokens": [50764, 407, 577, 393, 321, 360, 300, 30, 961, 311, 700, 747, 257, 574, 412, 264, 2674, 37642, 295, 27708, 25242, 5245, 11, 597, 307, 1238, 15325, 13, 407, 264, 787, 26747, 321, 643, 281, 652, 307, 294, 341, 9943, 1399, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14593401822176846, "compression_ratio": 1.5477386934673367, "no_speech_prob": 0.0021823483984917402}, {"id": 1013, "seek": 884380, "start": 8843.8, "end": 8859.8, "text": " We can let this denoting distribution to incorporate an additional input, which is this condition C. And this corresponds to modify the mean of this Gaussian distribution to take an additional input C.", "tokens": [50364, 492, 393, 718, 341, 1441, 17001, 7316, 281, 16091, 364, 4497, 4846, 11, 597, 307, 341, 4188, 383, 13, 400, 341, 23249, 281, 16927, 264, 914, 295, 341, 39148, 7316, 281, 747, 364, 4497, 4846, 383, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1655698503766741, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.03019667975604534}, {"id": 1014, "seek": 884380, "start": 8859.8, "end": 8866.8, "text": " And optionally, we can also let this variance to be learned and it takes an input C.", "tokens": [51164, 400, 3614, 379, 11, 321, 393, 611, 718, 341, 21977, 281, 312, 3264, 293, 309, 2516, 364, 4846, 383, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1655698503766741, "compression_ratio": 1.7439024390243902, "no_speech_prob": 0.03019667975604534}, {"id": 1015, "seek": 886680, "start": 8866.8, "end": 8886.8, "text": " But in practice, like most mostly we still just use this C in this mean and the variation of upper bounds only includes a small change, which lies in this KL divergence where we plug in this new formulation of the denoting distribution.", "tokens": [50364, 583, 294, 3124, 11, 411, 881, 5240, 321, 920, 445, 764, 341, 383, 294, 341, 914, 293, 264, 12990, 295, 6597, 29905, 787, 5974, 257, 1359, 1319, 11, 597, 9134, 294, 341, 47991, 47387, 689, 321, 5452, 294, 341, 777, 37642, 295, 264, 1441, 17001, 7316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22200201071944892, "compression_ratio": 1.4658385093167703, "no_speech_prob": 0.006287957541644573}, {"id": 1016, "seek": 888680, "start": 8886.8, "end": 8897.8, "text": " But it is a design arc in terms of how to incorporate different types of conditions into the unit, which is used to parameterize this new data.", "tokens": [50364, 583, 309, 307, 257, 1715, 10346, 294, 2115, 295, 577, 281, 16091, 819, 3467, 295, 4487, 666, 264, 4985, 11, 597, 307, 1143, 281, 13075, 1125, 341, 777, 1412, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11405598277777014, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.0033752876333892345}, {"id": 1017, "seek": 888680, "start": 8897.8, "end": 8912.8, "text": " Specifically, like these are the things people use in practice for scalar conditioning. For example, class label, we can do something similar to what we did for time step conditioning.", "tokens": [50914, 26058, 11, 411, 613, 366, 264, 721, 561, 764, 294, 3124, 337, 39684, 21901, 13, 1171, 1365, 11, 1508, 7645, 11, 321, 393, 360, 746, 2531, 281, 437, 321, 630, 337, 565, 1823, 21901, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11405598277777014, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.0033752876333892345}, {"id": 1018, "seek": 891280, "start": 8912.8, "end": 8927.8, "text": " Specifically, we can encode the certain scalars to a vector embedding, and then we simply add the embedding to the intermediate layers of the unit, or we do this adaptive professionalization layers.", "tokens": [50364, 26058, 11, 321, 393, 2058, 1429, 264, 1629, 15664, 685, 281, 257, 8062, 12240, 3584, 11, 293, 550, 321, 2935, 909, 264, 12240, 3584, 281, 264, 19376, 7914, 295, 264, 4985, 11, 420, 321, 360, 341, 27912, 4843, 2144, 7914, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17784970998764038, "compression_ratio": 1.7738693467336684, "no_speech_prob": 0.002251413417980075}, {"id": 1019, "seek": 891280, "start": 8927.8, "end": 8939.8, "text": " And if it is an image conditioning, we can do channel wise concatenation of the conditional image and the input image. And if it is for text conditioning,", "tokens": [51114, 400, 498, 309, 307, 364, 3256, 21901, 11, 321, 393, 360, 2269, 10829, 1588, 7186, 399, 295, 264, 27708, 3256, 293, 264, 4846, 3256, 13, 400, 498, 309, 307, 337, 2487, 21901, 11, 51714], "temperature": 0.0, "avg_logprob": -0.17784970998764038, "compression_ratio": 1.7738693467336684, "no_speech_prob": 0.002251413417980075}, {"id": 1020, "seek": 893980, "start": 8940.8, "end": 8952.8, "text": " this contains two cases. First, if the text, we embed the text to a single vector, then we can do something similar to the vector derived from the scalar conditioning.", "tokens": [50414, 341, 8306, 732, 3331, 13, 2386, 11, 498, 264, 2487, 11, 321, 12240, 264, 2487, 281, 257, 2167, 8062, 11, 550, 321, 393, 360, 746, 2531, 281, 264, 8062, 18949, 490, 264, 39684, 21901, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08042545879588407, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0011877423385158181}, {"id": 1021, "seek": 893980, "start": 8952.8, "end": 8964.8, "text": " And if we embed this text to a sequence of vectors, then we can consider using cross attention with the intermediate layers of the unit.", "tokens": [51014, 400, 498, 321, 12240, 341, 2487, 281, 257, 8310, 295, 18875, 11, 550, 321, 393, 1949, 1228, 3278, 3202, 365, 264, 19376, 7914, 295, 264, 4985, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08042545879588407, "compression_ratio": 1.6888888888888889, "no_speech_prob": 0.0011877423385158181}, {"id": 1022, "seek": 896480, "start": 8964.8, "end": 8972.8, "text": " And for a high resolution conditional generation, another important component is this classifier guidance.", "tokens": [50364, 400, 337, 257, 1090, 8669, 27708, 5125, 11, 1071, 1021, 6542, 307, 341, 1508, 9902, 10056, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20734246571858725, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0013455292209982872}, {"id": 1023, "seek": 896480, "start": 8972.8, "end": 8984.8, "text": " The main idea is like recall the diffusion model correspond to learning the score of a probability, for example, the score of log px.", "tokens": [50764, 440, 2135, 1558, 307, 411, 9901, 264, 25242, 2316, 6805, 281, 2539, 264, 6175, 295, 257, 8482, 11, 337, 1365, 11, 264, 6175, 295, 3565, 280, 87, 13, 51364], "temperature": 0.0, "avg_logprob": -0.20734246571858725, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0013455292209982872}, {"id": 1024, "seek": 898480, "start": 8984.8, "end": 8999.8, "text": " And right now, because we incorporate the class condition, which means like the diffusion model actually gives us a score of a class conditional model p of xt given c.", "tokens": [50364, 400, 558, 586, 11, 570, 321, 16091, 264, 1508, 4188, 11, 597, 1355, 411, 264, 25242, 2316, 767, 2709, 505, 257, 6175, 295, 257, 1508, 27708, 2316, 280, 295, 220, 734, 2212, 269, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12251274209273488, "compression_ratio": 1.678391959798995, "no_speech_prob": 0.00038593169301748276}, {"id": 1025, "seek": 898480, "start": 8999.8, "end": 9013.8, "text": " And given this, we can train an additional classifier, which gives us the probability of c given x, and then we mix the gradients of these two models during sampling.", "tokens": [51114, 400, 2212, 341, 11, 321, 393, 3847, 364, 4497, 1508, 9902, 11, 597, 2709, 505, 264, 8482, 295, 269, 2212, 2031, 11, 293, 550, 321, 2890, 264, 2771, 2448, 295, 613, 732, 5245, 1830, 21179, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12251274209273488, "compression_ratio": 1.678391959798995, "no_speech_prob": 0.00038593169301748276}, {"id": 1026, "seek": 901380, "start": 9013.8, "end": 9025.8, "text": " And this corresponds to, we sample from actually a modified score, which corresponds to the gradient of log px given c plus omega times log pc given x.", "tokens": [50364, 400, 341, 23249, 281, 11, 321, 6889, 490, 767, 257, 15873, 6175, 11, 597, 23249, 281, 264, 16235, 295, 3565, 280, 87, 2212, 269, 1804, 10498, 1413, 3565, 280, 66, 2212, 2031, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12052061557769775, "compression_ratio": 1.8704663212435233, "no_speech_prob": 0.0005440986715257168}, {"id": 1027, "seek": 901380, "start": 9025.8, "end": 9042.8, "text": " And this omega controls the strength of the guidance. And it actually corresponds to approximate sampling from the distribution, which is proportional to p of x given c times p of c given x to the omega power.", "tokens": [50964, 400, 341, 10498, 9003, 264, 3800, 295, 264, 10056, 13, 400, 309, 767, 23249, 281, 30874, 21179, 490, 264, 7316, 11, 597, 307, 24969, 281, 280, 295, 2031, 2212, 269, 1413, 280, 295, 269, 2212, 2031, 281, 264, 10498, 1347, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12052061557769775, "compression_ratio": 1.8704663212435233, "no_speech_prob": 0.0005440986715257168}, {"id": 1028, "seek": 904280, "start": 9042.8, "end": 9065.8, "text": " And in practice, it corresponds to we modify the normal distribution where we sample from and the mean of this normal distribution corresponds to the mean predicted by the score model or predicted by the diffusion model, plus the gradients from the classifier.", "tokens": [50364, 400, 294, 3124, 11, 309, 23249, 281, 321, 16927, 264, 2710, 7316, 689, 321, 6889, 490, 293, 264, 914, 295, 341, 2710, 7316, 23249, 281, 264, 914, 19147, 538, 264, 6175, 2316, 420, 19147, 538, 264, 25242, 2316, 11, 1804, 264, 2771, 2448, 490, 264, 1508, 9902, 13, 51514], "temperature": 0.0, "avg_logprob": -0.15839629906874436, "compression_ratio": 1.74496644295302, "no_speech_prob": 0.000527369964402169}, {"id": 1029, "seek": 906580, "start": 9066.8, "end": 9083.8, "text": " And if we use larger omega, then the samples will be more concentrated around the modes of this classifier, which really leads to better individual sample quality, but if we use too large omega, it will reduce the sample diversity.", "tokens": [50414, 400, 498, 321, 764, 4833, 10498, 11, 550, 264, 10938, 486, 312, 544, 21321, 926, 264, 14068, 295, 341, 1508, 9902, 11, 597, 534, 6689, 281, 1101, 2609, 6889, 3125, 11, 457, 498, 321, 764, 886, 2416, 10498, 11, 309, 486, 5407, 264, 6889, 8811, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08606708526611329, "compression_ratio": 1.8350515463917525, "no_speech_prob": 0.010007630102336407}, {"id": 1030, "seek": 906580, "start": 9083.8, "end": 9094.8, "text": " So one really needs to find a sweet point for this omega to best balance the individual sample quality and sample diversity.", "tokens": [51264, 407, 472, 534, 2203, 281, 915, 257, 3844, 935, 337, 341, 10498, 281, 1151, 4772, 264, 2609, 6889, 3125, 293, 6889, 8811, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08606708526611329, "compression_ratio": 1.8350515463917525, "no_speech_prob": 0.010007630102336407}, {"id": 1031, "seek": 909480, "start": 9094.8, "end": 9105.8, "text": " And one downside for this classifier guidance is that we need to train an additional classifier to do that, right, so that adds additional model complexity.", "tokens": [50364, 400, 472, 25060, 337, 341, 1508, 9902, 10056, 307, 300, 321, 643, 281, 3847, 364, 4497, 1508, 9902, 281, 360, 300, 11, 558, 11, 370, 300, 10860, 4497, 2316, 14024, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13273229598999023, "compression_ratio": 1.4311926605504588, "no_speech_prob": 0.00017129615298472345}, {"id": 1032, "seek": 910580, "start": 9105.8, "end": 9124.8, "text": " So inspired by that work. This works tries to introduce a classifier free guidance, meaning that we can actually get an implicit classifier by joining training a conditional and unconditional diffusion model in a pulling sense.", "tokens": [50364, 407, 7547, 538, 300, 589, 13, 639, 1985, 9898, 281, 5366, 257, 1508, 9902, 1737, 10056, 11, 3620, 300, 321, 393, 767, 483, 364, 26947, 1508, 9902, 538, 5549, 3097, 257, 27708, 293, 47916, 25242, 2316, 294, 257, 8407, 2020, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23750409020317925, "compression_ratio": 1.523489932885906, "no_speech_prob": 0.00026118048117496073}, {"id": 1033, "seek": 912480, "start": 9124.8, "end": 9146.8, "text": " So suppose we have this PX given C, where the the score can be derived by a conditional diffusion model, and we also have the score of a unconditional model p of x, and then we can derive an implicit classifier PC given x which should be proportional to p of x given", "tokens": [50364, 407, 7297, 321, 362, 341, 430, 55, 2212, 383, 11, 689, 264, 264, 6175, 393, 312, 18949, 538, 257, 27708, 25242, 2316, 11, 293, 321, 611, 362, 264, 6175, 295, 257, 47916, 2316, 280, 295, 2031, 11, 293, 550, 321, 393, 28446, 364, 26947, 1508, 9902, 6465, 2212, 2031, 597, 820, 312, 24969, 281, 280, 295, 2031, 2212, 51464], "temperature": 0.0, "avg_logprob": -0.2328700711650233, "compression_ratio": 1.612121212121212, "no_speech_prob": 0.005641191732138395}, {"id": 1034, "seek": 914680, "start": 9146.8, "end": 9161.8, "text": " divided by p of x. And in practice, the gradient or the score of these two probability are estimated by randomly dropping the condition in the diffusion models at a certain chance for each iteration.", "tokens": [50364, 6666, 538, 280, 295, 2031, 13, 400, 294, 3124, 11, 264, 16235, 420, 264, 6175, 295, 613, 732, 8482, 366, 14109, 538, 16979, 13601, 264, 4188, 294, 264, 25242, 5245, 412, 257, 1629, 2931, 337, 1184, 24784, 13, 51114], "temperature": 0.0, "avg_logprob": -0.15511308397565568, "compression_ratio": 1.4316546762589928, "no_speech_prob": 0.001622673706151545}, {"id": 1035, "seek": 916180, "start": 9161.8, "end": 9176.8, "text": " And similarly, we can derive the modified score with this implicit classifier. We call this is the original modified score. And now we replace the log PC given x by log PX given C minus log PX.", "tokens": [50364, 400, 14138, 11, 321, 393, 28446, 264, 15873, 6175, 365, 341, 26947, 1508, 9902, 13, 492, 818, 341, 307, 264, 3380, 15873, 6175, 13, 400, 586, 321, 7406, 264, 3565, 6465, 2212, 2031, 538, 3565, 430, 55, 2212, 383, 3175, 3565, 430, 55, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19340050220489502, "compression_ratio": 1.4621212121212122, "no_speech_prob": 0.011154397390782833}, {"id": 1036, "seek": 917680, "start": 9177.8, "end": 9192.8, "text": " And this is the resulting modified score we will use with this classifier free guidance where this log PX given C and this log PX are both estimated by the single diffusion model.", "tokens": [50414, 400, 341, 307, 264, 16505, 15873, 6175, 321, 486, 764, 365, 341, 1508, 9902, 1737, 10056, 689, 341, 3565, 430, 55, 2212, 383, 293, 341, 3565, 430, 55, 366, 1293, 14109, 538, 264, 2167, 25242, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13562671149649272, "compression_ratio": 1.376923076923077, "no_speech_prob": 0.0012841454008594155}, {"id": 1037, "seek": 919280, "start": 9193.8, "end": 9216.8, "text": " And in this three panels, we can see the trade off between sample quality and sample diversity more clearly. So from left to right correspond to we gradually increase the strength of the guidance, and we can see clearly individually speaking, the sample quality of each image increases.", "tokens": [50414, 400, 294, 341, 1045, 13419, 11, 321, 393, 536, 264, 4923, 766, 1296, 6889, 3125, 293, 6889, 8811, 544, 4448, 13, 407, 490, 1411, 281, 558, 6805, 281, 321, 13145, 3488, 264, 3800, 295, 264, 10056, 11, 293, 321, 393, 536, 4448, 16652, 4124, 11, 264, 6889, 3125, 295, 1184, 3256, 8637, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20512075591505619, "compression_ratio": 1.6342857142857143, "no_speech_prob": 0.0013667764142155647}, {"id": 1038, "seek": 921680, "start": 9217.8, "end": 9228.8, "text": " However, the samples that look like more similar to each other if we use a large classifier or classifier free guidance.", "tokens": [50414, 2908, 11, 264, 10938, 300, 574, 411, 544, 2531, 281, 1184, 661, 498, 321, 764, 257, 2416, 1508, 9902, 420, 1508, 9902, 1737, 10056, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1431058357501852, "compression_ratio": 1.2765957446808511, "no_speech_prob": 0.0010648546740412712}, {"id": 1039, "seek": 922880, "start": 9228.8, "end": 9246.8, "text": " And the last thing I want to talk about is this cascade generation pipeline, which are important to high resolution generation. And this kind of idea have already been explored by other type of generating models for, for example, game model.", "tokens": [50364, 400, 264, 1036, 551, 286, 528, 281, 751, 466, 307, 341, 50080, 5125, 15517, 11, 597, 366, 1021, 281, 1090, 8669, 5125, 13, 400, 341, 733, 295, 1558, 362, 1217, 668, 24016, 538, 661, 2010, 295, 17746, 5245, 337, 11, 337, 1365, 11, 1216, 2316, 13, 51264], "temperature": 0.0, "avg_logprob": -0.21708974838256836, "compression_ratio": 1.4968944099378882, "no_speech_prob": 0.0012446899199858308}, {"id": 1040, "seek": 924680, "start": 9246.8, "end": 9263.8, "text": " And it's pretty straightforward. So we've started by learning an unconditional diffusion model at the lowest resolution. And then we've learned several super resolution models, taking the down sampled training images at lower resolution as the condition.", "tokens": [50364, 400, 309, 311, 1238, 15325, 13, 407, 321, 600, 1409, 538, 2539, 364, 47916, 25242, 2316, 412, 264, 12437, 8669, 13, 400, 550, 321, 600, 3264, 2940, 1687, 8669, 5245, 11, 1940, 264, 760, 3247, 15551, 3097, 5267, 412, 3126, 8669, 382, 264, 4188, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1654312172714545, "compression_ratio": 1.5679012345679013, "no_speech_prob": 0.030664030462503433}, {"id": 1041, "seek": 926380, "start": 9263.8, "end": 9279.8, "text": " And during sampling, we just run the progressive sampling pipeline starting from the smallest unconditional model and going, going through all those super resolution models until we reach the highest resolution.", "tokens": [50364, 400, 1830, 21179, 11, 321, 445, 1190, 264, 16131, 21179, 15517, 2891, 490, 264, 16998, 47916, 2316, 293, 516, 11, 516, 807, 439, 729, 1687, 8669, 5245, 1826, 321, 2524, 264, 6343, 8669, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17305541038513184, "compression_ratio": 1.5514705882352942, "no_speech_prob": 0.0013043575454503298}, {"id": 1042, "seek": 927980, "start": 9279.8, "end": 9300.8, "text": " But one notorious problem of this kind of cascaded training pipeline is this compounding error problem. It's more specifically record that during training, the conditions we fit into the super resolution model is the down sampled version of the training images from the data set.", "tokens": [50364, 583, 472, 38045, 1154, 295, 341, 733, 295, 3058, 66, 12777, 3097, 15517, 307, 341, 14154, 278, 6713, 1154, 13, 467, 311, 544, 4682, 2136, 300, 1830, 3097, 11, 264, 4487, 321, 3318, 666, 264, 1687, 8669, 2316, 307, 264, 760, 3247, 15551, 3037, 295, 264, 3097, 5267, 490, 264, 1412, 992, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1730110101532518, "compression_ratio": 1.558659217877095, "no_speech_prob": 0.0007320993463508785}, {"id": 1043, "seek": 930080, "start": 9300.8, "end": 9321.8, "text": " However, during sampling, the conditions we fit to those super resolution models are actually generative samples of from the low resolution models. So if there are certain artifacts in the samples from the low resolution models, those artifacts or inaccurate samples will affect the", "tokens": [50364, 2908, 11, 1830, 21179, 11, 264, 4487, 321, 3318, 281, 729, 1687, 8669, 5245, 366, 767, 1337, 1166, 10938, 295, 490, 264, 2295, 8669, 5245, 13, 407, 498, 456, 366, 1629, 24617, 294, 264, 10938, 490, 264, 2295, 8669, 5245, 11, 729, 24617, 420, 46443, 10938, 486, 3345, 264, 51414], "temperature": 0.0, "avg_logprob": -0.1418319378259047, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.0006165799568407238}, {"id": 1044, "seek": 932180, "start": 9321.8, "end": 9332.8, "text": " the sample quality of the super resolution models as well, because of this mismatch issue between the conditions in training and condition in inference.", "tokens": [50364, 264, 6889, 3125, 295, 264, 1687, 8669, 5245, 382, 731, 11, 570, 295, 341, 23220, 852, 2734, 1296, 264, 4487, 294, 3097, 293, 4188, 294, 38253, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15889444658833166, "compression_ratio": 1.3944954128440368, "no_speech_prob": 0.0010648660827428102}, {"id": 1045, "seek": 933280, "start": 9332.8, "end": 9354.8, "text": " To activate this problem, this noise conditioning augmentation is proposed in reference works. Basically, during training, we try to degrade the conditioning low resolution images by adding variance amount of Gaussian noise, or just blur those images by Gaussian kernel.", "tokens": [50364, 1407, 13615, 341, 1154, 11, 341, 5658, 21901, 14501, 19631, 307, 10348, 294, 6408, 1985, 13, 8537, 11, 1830, 3097, 11, 321, 853, 281, 368, 8692, 264, 21901, 2295, 8669, 5267, 538, 5127, 21977, 2372, 295, 39148, 5658, 11, 420, 445, 14257, 729, 5267, 538, 39148, 28256, 13, 51464], "temperature": 0.0, "avg_logprob": -0.16852670449476975, "compression_ratio": 1.560693641618497, "no_speech_prob": 0.035661499947309494}, {"id": 1046, "seek": 935480, "start": 9355.8, "end": 9365.8, "text": " And during inference, we sweep over the optimal amount of noise added to the low resolution images, which are the conditions to the super resolution models.", "tokens": [50414, 400, 1830, 38253, 11, 321, 22169, 670, 264, 16252, 2372, 295, 5658, 3869, 281, 264, 2295, 8669, 5267, 11, 597, 366, 264, 4487, 281, 264, 1687, 8669, 5245, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09227630586335153, "compression_ratio": 1.3928571428571428, "no_speech_prob": 0.00205038208514452}, {"id": 1047, "seek": 936580, "start": 9365.8, "end": 9387.8, "text": " So the idea or the hypothesis is like if we try to reduce certain amount of information from the condition, then the super resolution model will be trained to be more robust to different type of artifacts when we send like the conditions as the samples.", "tokens": [50364, 407, 264, 1558, 420, 264, 17291, 307, 411, 498, 321, 853, 281, 5407, 1629, 2372, 295, 1589, 490, 264, 4188, 11, 550, 264, 1687, 8669, 2316, 486, 312, 8895, 281, 312, 544, 13956, 281, 819, 2010, 295, 24617, 562, 321, 2845, 411, 264, 4487, 382, 264, 10938, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11849175966702975, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.0005975927342660725}, {"id": 1048, "seek": 938780, "start": 9387.8, "end": 9408.8, "text": " And later on, more complicated degradation process have been proposed. For example, we can add a sequence of different types of degradation operations to the image, the low resolution image before sending as a condition to the super resolution models.", "tokens": [50364, 400, 1780, 322, 11, 544, 6179, 40519, 1399, 362, 668, 10348, 13, 1171, 1365, 11, 321, 393, 909, 257, 8310, 295, 819, 3467, 295, 40519, 7705, 281, 264, 3256, 11, 264, 2295, 8669, 3256, 949, 7750, 382, 257, 4188, 281, 264, 1687, 8669, 5245, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08790981526277503, "compression_ratio": 1.5886075949367089, "no_speech_prob": 0.0005111978971399367}, {"id": 1049, "seek": 940880, "start": 9409.8, "end": 9431.8, "text": " Okay, so here's a summary of these parts. So in this advanced techniques session, we learn to answer two questions. The first one is how we can accelerate the sampling process, and we introduce several important techniques from the aspects of advanced forward process, reverse process and modeling itself.", "tokens": [50414, 1033, 11, 370, 510, 311, 257, 12691, 295, 613, 3166, 13, 407, 294, 341, 7339, 7512, 5481, 11, 321, 1466, 281, 1867, 732, 1651, 13, 440, 700, 472, 307, 577, 321, 393, 21341, 264, 21179, 1399, 11, 293, 321, 5366, 2940, 1021, 7512, 490, 264, 7270, 295, 7339, 2128, 1399, 11, 9943, 1399, 293, 15983, 2564, 13, 51514], "temperature": 0.0, "avg_logprob": -0.16455819176845862, "compression_ratio": 1.5885416666666667, "no_speech_prob": 0.0017543385038152337}, {"id": 1050, "seek": 943180, "start": 9431.8, "end": 9449.8, "text": " And the second question is how we can do high resolution conditional generation using diffusion models, and we discuss the general framework of conditional diffusion models, classifier and classifier free guidance, as well as cascade generation pipeline.", "tokens": [50364, 400, 264, 1150, 1168, 307, 577, 321, 393, 360, 1090, 8669, 27708, 5125, 1228, 25242, 5245, 11, 293, 321, 2248, 264, 2674, 8388, 295, 27708, 25242, 5245, 11, 1508, 9902, 293, 1508, 9902, 1737, 10056, 11, 382, 731, 382, 50080, 5125, 15517, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12086069924490793, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.0021481525618582964}, {"id": 1051, "seek": 943180, "start": 9449.8, "end": 9458.8, "text": " So in the application section, we will see how all those techniques will benefit in terms of various tasks.", "tokens": [51264, 407, 294, 264, 3861, 3541, 11, 321, 486, 536, 577, 439, 729, 7512, 486, 5121, 294, 2115, 295, 3683, 9608, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12086069924490793, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.0021481525618582964}, {"id": 1052, "seek": 945880, "start": 9458.8, "end": 9473.8, "text": " Hi, everyone. Welcome to the first section of applications of diffusion models. So in this section, we're going to study applications of diffusion models in terms of image synthesized control generation as well as text to image generation.", "tokens": [50364, 2421, 11, 1518, 13, 4027, 281, 264, 700, 3541, 295, 5821, 295, 25242, 5245, 13, 407, 294, 341, 3541, 11, 321, 434, 516, 281, 2979, 5821, 295, 25242, 5245, 294, 2115, 295, 3256, 26617, 1602, 1969, 5125, 382, 731, 382, 2487, 281, 3256, 5125, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1255429968776473, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.0003149579861201346}, {"id": 1053, "seek": 945880, "start": 9473.8, "end": 9485.8, "text": " So let's run text to image generation. So in the past two years, this task has been shown to be extremely suitable for diffusion models to work on.", "tokens": [51114, 407, 718, 311, 1190, 2487, 281, 3256, 5125, 13, 407, 294, 264, 1791, 732, 924, 11, 341, 5633, 575, 668, 4898, 281, 312, 4664, 12873, 337, 25242, 5245, 281, 589, 322, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1255429968776473, "compression_ratio": 1.8516746411483254, "no_speech_prob": 0.0003149579861201346}, {"id": 1054, "seek": 948580, "start": 9485.8, "end": 9497.8, "text": " So basically, this task is the inverse of the image captioning tasks, where we are given a text prompt C and we are trying to generate high resolution images X, as shown in this video.", "tokens": [50364, 407, 1936, 11, 341, 5633, 307, 264, 17340, 295, 264, 3256, 31974, 278, 9608, 11, 689, 321, 366, 2212, 257, 2487, 12391, 383, 293, 321, 366, 1382, 281, 8460, 1090, 8669, 5267, 1783, 11, 382, 4898, 294, 341, 960, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18058504377092635, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.001987284515053034}, {"id": 1055, "seek": 948580, "start": 9497.8, "end": 9506.8, "text": " So this video shows the generative image images by a text to image generation model called imagine as we will show later.", "tokens": [50964, 407, 341, 960, 3110, 264, 1337, 1166, 3256, 5267, 538, 257, 2487, 281, 3256, 5125, 2316, 1219, 3811, 382, 321, 486, 855, 1780, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18058504377092635, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.001987284515053034}, {"id": 1056, "seek": 950680, "start": 9507.8, "end": 9526.8, "text": " And let's start from this glide model by opening in last year. So this is essentially a cascading generation diffusion models, which contains 64 by 64 base model, and a 64 by 64 to 56 by 256 super resolution model.", "tokens": [50414, 400, 718, 311, 722, 490, 341, 41848, 2316, 538, 5193, 294, 1036, 1064, 13, 407, 341, 307, 4476, 257, 3058, 66, 8166, 5125, 25242, 5245, 11, 597, 8306, 12145, 538, 12145, 3096, 2316, 11, 293, 257, 12145, 538, 12145, 281, 19687, 538, 38882, 1687, 8669, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24192131266874425, "compression_ratio": 1.436241610738255, "no_speech_prob": 0.010649662464857101}, {"id": 1057, "seek": 952680, "start": 9527.8, "end": 9541.8, "text": " And they have tried to use classifier free guidance and clip guidance. So I will talk about the clip guidance in details later, and they generally found that classifier free guidance works better than the clip guidance.", "tokens": [50414, 400, 436, 362, 3031, 281, 764, 1508, 9902, 1737, 10056, 293, 7353, 10056, 13, 407, 286, 486, 751, 466, 264, 7353, 10056, 294, 4365, 1780, 11, 293, 436, 5101, 1352, 300, 1508, 9902, 1737, 10056, 1985, 1101, 813, 264, 7353, 10056, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12575161975363028, "compression_ratio": 1.6717557251908397, "no_speech_prob": 0.005383848212659359}, {"id": 1058, "seek": 954180, "start": 9541.8, "end": 9556.8, "text": " And those figures shows the generative samples from this model. And as we can see, the model is capable of generate fun, normal conversations of concepts that have never been seen from the data set.", "tokens": [50364, 400, 729, 9624, 3110, 264, 1337, 1166, 10938, 490, 341, 2316, 13, 400, 382, 321, 393, 536, 11, 264, 2316, 307, 8189, 295, 8460, 1019, 11, 2710, 7315, 295, 10392, 300, 362, 1128, 668, 1612, 490, 264, 1412, 992, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17209118062799628, "compression_ratio": 1.5114503816793894, "no_speech_prob": 0.00781433004885912}, {"id": 1059, "seek": 955680, "start": 9556.8, "end": 9565.8, "text": " For example, a hedge dog using a calculator and robots meditating in a vipassana retreat or etc.", "tokens": [50364, 1171, 1365, 11, 257, 25304, 3000, 1228, 257, 24993, 293, 14733, 46850, 294, 257, 371, 647, 640, 2095, 15505, 420, 5183, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21057678671444163, "compression_ratio": 1.446808510638298, "no_speech_prob": 0.008059137500822544}, {"id": 1060, "seek": 955680, "start": 9565.8, "end": 9573.8, "text": " So a bit introduction of the clip guidance, it can be treated as a special form of the classifier guidance.", "tokens": [50814, 407, 257, 857, 9339, 295, 264, 7353, 10056, 11, 309, 393, 312, 8668, 382, 257, 2121, 1254, 295, 264, 1508, 9902, 10056, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21057678671444163, "compression_ratio": 1.446808510638298, "no_speech_prob": 0.008059137500822544}, {"id": 1061, "seek": 957380, "start": 9573.8, "end": 9588.8, "text": " And in terms of a clean model contains two components, a text encoder, G, and image encoder F, and during training batches of the image and caption pairs assembled from a large datasets.", "tokens": [50364, 400, 294, 2115, 295, 257, 2541, 2316, 8306, 732, 6677, 11, 257, 2487, 2058, 19866, 11, 460, 11, 293, 3256, 2058, 19866, 479, 11, 293, 1830, 3097, 15245, 279, 295, 264, 3256, 293, 31974, 15494, 24204, 490, 257, 2416, 42856, 13, 51114], "temperature": 0.0, "avg_logprob": -0.22847156524658202, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.0030272731091827154}, {"id": 1062, "seek": 958880, "start": 9588.8, "end": 9609.8, "text": " And the model optimize a contrasting cross entropy loss, which encourages high dog product between this F and G, if the image X and C comes from the same image caption pair, and it encourages low product if X and C comes from different image caption pairs.", "tokens": [50364, 400, 264, 2316, 19719, 257, 8712, 278, 3278, 30867, 4470, 11, 597, 28071, 1090, 3000, 1674, 1296, 341, 479, 293, 460, 11, 498, 264, 3256, 1783, 293, 383, 1487, 490, 264, 912, 3256, 31974, 6119, 11, 293, 309, 28071, 2295, 1674, 498, 1783, 293, 383, 1487, 490, 819, 3256, 31974, 15494, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16972427708762033, "compression_ratio": 1.6516129032258065, "no_speech_prob": 0.0010986372362822294}, {"id": 1063, "seek": 960980, "start": 9609.8, "end": 9628.8, "text": " And it can be proved that the optimal value of this FX times GC is given by log PXC divided by P of X times P of C, which equals to log P, C given X minus log PC. So given this conclusion,", "tokens": [50364, 400, 309, 393, 312, 14617, 300, 264, 16252, 2158, 295, 341, 37849, 1413, 29435, 307, 2212, 538, 3565, 430, 55, 34, 6666, 538, 430, 295, 1783, 1413, 430, 295, 383, 11, 597, 6915, 281, 3565, 430, 11, 383, 2212, 1783, 3175, 3565, 6465, 13, 407, 2212, 341, 10063, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1744192771191867, "compression_ratio": 1.3823529411764706, "no_speech_prob": 0.013844642788171768}, {"id": 1064, "seek": 962880, "start": 9629.8, "end": 9645.8, "text": " we will be able to use this clip model as the classifier in the classifier in the classifier guidance in the following sense. So recall that for the classifier guidance, we want to modify the score in this formulation.", "tokens": [50414, 321, 486, 312, 1075, 281, 764, 341, 7353, 2316, 382, 264, 1508, 9902, 294, 264, 1508, 9902, 294, 264, 1508, 9902, 10056, 294, 264, 3480, 2020, 13, 407, 9901, 300, 337, 264, 1508, 9902, 10056, 11, 321, 528, 281, 16927, 264, 6175, 294, 341, 37642, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12413020133972168, "compression_ratio": 1.744, "no_speech_prob": 0.007010418921709061}, {"id": 1065, "seek": 964580, "start": 9645.8, "end": 9663.8, "text": " And we can consider augment the augmenting the second term by a minus log PC term, because when we take gradient over X, then this part just disappeared. And then we can see that these two terms together can be modeled by a clip model.", "tokens": [50364, 400, 321, 393, 1949, 29919, 264, 29919, 278, 264, 1150, 1433, 538, 257, 3175, 3565, 6465, 1433, 11, 570, 562, 321, 747, 16235, 670, 1783, 11, 550, 341, 644, 445, 13954, 13, 400, 550, 321, 393, 536, 300, 613, 732, 2115, 1214, 393, 312, 37140, 538, 257, 7353, 2316, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10964399796945078, "compression_ratio": 1.4873417721518987, "no_speech_prob": 0.005729289259761572}, {"id": 1066, "seek": 966380, "start": 9664.8, "end": 9679.8, "text": " So basically replace this part by the dog product between FX and GC. So that is the clip guidance. However, in Glide they show that the clip guidance is less favored compared to the classifier free guidance.", "tokens": [50414, 407, 1936, 7406, 341, 644, 538, 264, 3000, 1674, 1296, 37849, 293, 29435, 13, 407, 300, 307, 264, 7353, 10056, 13, 2908, 11, 294, 5209, 482, 436, 855, 300, 264, 7353, 10056, 307, 1570, 44420, 5347, 281, 264, 1508, 9902, 1737, 10056, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14763089443774932, "compression_ratio": 1.4375, "no_speech_prob": 0.00027801707619801164}, {"id": 1067, "seek": 967980, "start": 9680.8, "end": 9701.8, "text": " And besides pure text-to-image generation, the Glide has shown that it is possible to fine-tune the model for text-guided impending tasks. Basically, they try to fine-tune the trained text-to-image Glide model by fitting randomly occluded images with an additional mask channel as the input.", "tokens": [50414, 400, 11868, 6075, 2487, 12, 1353, 12, 26624, 5125, 11, 264, 5209, 482, 575, 4898, 300, 309, 307, 1944, 281, 2489, 12, 83, 2613, 264, 2316, 337, 2487, 12, 2794, 2112, 704, 2029, 9608, 13, 8537, 11, 436, 853, 281, 2489, 12, 83, 2613, 264, 8895, 2487, 12, 1353, 12, 26624, 5209, 482, 2316, 538, 15669, 16979, 2678, 44412, 5267, 365, 364, 4497, 6094, 2269, 382, 264, 4846, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18737256037045832, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.0004954866017214954}, {"id": 1068, "seek": 970180, "start": 9702.8, "end": 9723.8, "text": " So using this fine-tune model, they are able to change or do image editing by changing the prompt. For example, given an old car in a green forest, they will be able to edit the background to a snowy forest. Similarly, they can add a white hat to a man's hat.", "tokens": [50414, 407, 1228, 341, 2489, 12, 83, 2613, 2316, 11, 436, 366, 1075, 281, 1319, 420, 360, 3256, 10000, 538, 4473, 264, 12391, 13, 1171, 1365, 11, 2212, 364, 1331, 1032, 294, 257, 3092, 6719, 11, 436, 486, 312, 1075, 281, 8129, 264, 3678, 281, 257, 5756, 88, 6719, 13, 13157, 11, 436, 393, 909, 257, 2418, 2385, 281, 257, 587, 311, 2385, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08098576673820837, "compression_ratio": 1.532544378698225, "no_speech_prob": 0.001366803189739585}, {"id": 1069, "seek": 972380, "start": 9724.8, "end": 9744.8, "text": " And later on, this DAO-E2 further scale up the Glide model to support 1K by 1K text-to-image generation. And this DAO-E2 has been shown to outperform the first version of text-to-image 1K by 1K generation by OpenEye, which is this DAO-E, which is an unregressive transformer-based model.", "tokens": [50414, 400, 1780, 322, 11, 341, 9578, 46, 12, 36, 17, 3052, 4373, 493, 264, 5209, 482, 2316, 281, 1406, 502, 42, 538, 502, 42, 2487, 12, 1353, 12, 26624, 5125, 13, 400, 341, 9578, 46, 12, 36, 17, 575, 668, 4898, 281, 484, 26765, 264, 700, 3037, 295, 2487, 12, 1353, 12, 26624, 502, 42, 538, 502, 42, 5125, 538, 7238, 36, 1200, 11, 597, 307, 341, 9578, 46, 12, 36, 11, 597, 307, 364, 517, 3375, 22733, 31782, 12, 6032, 2316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1702673901086566, "compression_ratio": 1.5683060109289617, "no_speech_prob": 0.0007551708840765059}, {"id": 1070, "seek": 974480, "start": 9745.8, "end": 9767.8, "text": " And in terms of the model components of DAO-E2, it's built up on a pre-trained clip model. More specifically, a clip model is first pre-trained, and the image embedding and text embedding are grabbed from this pre-trained clip embedding and frozen.", "tokens": [50414, 400, 294, 2115, 295, 264, 2316, 6677, 295, 9578, 46, 12, 36, 17, 11, 309, 311, 3094, 493, 322, 257, 659, 12, 17227, 2001, 7353, 2316, 13, 5048, 4682, 11, 257, 7353, 2316, 307, 700, 659, 12, 17227, 2001, 11, 293, 264, 3256, 12240, 3584, 293, 2487, 12240, 3584, 366, 18607, 490, 341, 659, 12, 17227, 2001, 7353, 12240, 3584, 293, 12496, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10248404830249387, "compression_ratio": 1.6, "no_speech_prob": 0.0030744841787964106}, {"id": 1071, "seek": 976780, "start": 9768.8, "end": 9789.8, "text": " And after that, this pipeline has been built to generate images from text. Basically, this generation model contains two parts. The first is a prior model. This prior model tries to produce clip image embeddings conditioned on the input caption.", "tokens": [50414, 400, 934, 300, 11, 341, 15517, 575, 668, 3094, 281, 8460, 5267, 490, 2487, 13, 8537, 11, 341, 5125, 2316, 8306, 732, 3166, 13, 440, 700, 307, 257, 4059, 2316, 13, 639, 4059, 2316, 9898, 281, 5258, 7353, 3256, 12240, 29432, 35833, 322, 264, 4846, 31974, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11418494991227693, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0034291932824999094}, {"id": 1072, "seek": 978980, "start": 9790.8, "end": 9799.8, "text": " And then the second part is a decoder part, which produces the images conditioned on the clip image embedding as well as the text.", "tokens": [50414, 400, 550, 264, 1150, 644, 307, 257, 979, 19866, 644, 11, 597, 14725, 264, 5267, 35833, 322, 264, 7353, 3256, 12240, 3584, 382, 731, 382, 264, 2487, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11433529172624861, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0022868916857987642}, {"id": 1073, "seek": 978980, "start": 9800.8, "end": 9812.8, "text": " So one natural question is why we want to condition the decoder on the clip image embeddings, right? So why not we just directly condition this decoder on text only?", "tokens": [50914, 407, 472, 3303, 1168, 307, 983, 321, 528, 281, 4188, 264, 979, 19866, 322, 264, 7353, 3256, 12240, 29432, 11, 558, 30, 407, 983, 406, 321, 445, 3838, 4188, 341, 979, 19866, 322, 2487, 787, 30, 51514], "temperature": 0.0, "avg_logprob": -0.11433529172624861, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0022868916857987642}, {"id": 1074, "seek": 981280, "start": 9812.8, "end": 9835.8, "text": " So the hypothesis here is that for the total amount of entropy of an input signal, for example, images, so there's certain part that captures the high-level semantic meanings while there's still a large proportion of the entropy, which corresponds to just low-level details, either perceptual-visible or even perceptual-invisible.", "tokens": [50364, 407, 264, 17291, 510, 307, 300, 337, 264, 3217, 2372, 295, 30867, 295, 364, 4846, 6358, 11, 337, 1365, 11, 5267, 11, 370, 456, 311, 1629, 644, 300, 27986, 264, 1090, 12, 12418, 47982, 28138, 1339, 456, 311, 920, 257, 2416, 16068, 295, 264, 30867, 11, 597, 23249, 281, 445, 2295, 12, 12418, 4365, 11, 2139, 43276, 901, 12, 48990, 420, 754, 43276, 901, 12, 259, 48990, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10989215638902453, "compression_ratio": 1.625615763546798, "no_speech_prob": 0.0022869007661938667}, {"id": 1075, "seek": 983580, "start": 9836.8, "end": 9850.8, "text": " So the hypothesis is that clip image embeddings tend to have a higher chance to capture the high-level semantic meaning of an input signal, especially those related to the caption information.", "tokens": [50414, 407, 264, 17291, 307, 300, 7353, 3256, 12240, 29432, 3928, 281, 362, 257, 2946, 2931, 281, 7983, 264, 1090, 12, 12418, 47982, 3620, 295, 364, 4846, 6358, 11, 2318, 729, 4077, 281, 264, 31974, 1589, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07134896355706292, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0033237861935049295}, {"id": 1076, "seek": 983580, "start": 9851.8, "end": 9860.8, "text": " And by conditioning on this high-level semantic meaning, the decoder is able to capture or catch up those low-level details of the images more quickly.", "tokens": [51164, 400, 538, 21901, 322, 341, 1090, 12, 12418, 47982, 3620, 11, 264, 979, 19866, 307, 1075, 281, 7983, 420, 3745, 493, 729, 2295, 12, 12418, 4365, 295, 264, 5267, 544, 2661, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07134896355706292, "compression_ratio": 1.6699029126213591, "no_speech_prob": 0.0033237861935049295}, {"id": 1077, "seek": 986080, "start": 9861.8, "end": 9876.8, "text": " And later on, this paper also shows that this by-part later repetitions of the clip image embedding as well as the latency in the decoder model enables several text-guided image manipulation tasks, as we will show later.", "tokens": [50414, 400, 1780, 322, 11, 341, 3035, 611, 3110, 300, 341, 538, 12, 6971, 1780, 13645, 2451, 295, 264, 7353, 3256, 12240, 3584, 382, 731, 382, 264, 27043, 294, 264, 979, 19866, 2316, 17077, 2940, 2487, 12, 2794, 2112, 3256, 26475, 9608, 11, 382, 321, 486, 855, 1780, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16585001578697792, "compression_ratio": 1.4965986394557824, "no_speech_prob": 0.0010482723591849208}, {"id": 1078, "seek": 987680, "start": 9877.8, "end": 9883.8, "text": " And a bit more details of the model architecture of the prior and the decoder models.", "tokens": [50414, 400, 257, 857, 544, 4365, 295, 264, 2316, 9482, 295, 264, 4059, 293, 264, 979, 19866, 5245, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1094713556593743, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0008424800471402705}, {"id": 1079, "seek": 987680, "start": 9884.8, "end": 9896.8, "text": " For the prior, the paper tries two options. The first one is the auto-regressive pair, where they quantize the image embedding to a sequence of discrete codes and predict them auto-regressively.", "tokens": [50764, 1171, 264, 4059, 11, 264, 3035, 9898, 732, 3956, 13, 440, 700, 472, 307, 264, 8399, 12, 265, 3091, 488, 6119, 11, 689, 436, 4426, 1125, 264, 3256, 12240, 3584, 281, 257, 8310, 295, 27706, 14211, 293, 6069, 552, 8399, 12, 265, 3091, 3413, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1094713556593743, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0008424800471402705}, {"id": 1080, "seek": 989680, "start": 9897.8, "end": 9911.8, "text": " And the second option is to model the prior by diffusion models, where they directly train diffusion models based on the continuous image embedding as well as the caption input.", "tokens": [50414, 400, 264, 1150, 3614, 307, 281, 2316, 264, 4059, 538, 25242, 5245, 11, 689, 436, 3838, 3847, 25242, 5245, 2361, 322, 264, 10957, 3256, 12240, 3584, 382, 731, 382, 264, 31974, 4846, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12348138584810145, "compression_ratio": 1.651006711409396, "no_speech_prob": 0.000487829209305346}, {"id": 1081, "seek": 989680, "start": 9912.8, "end": 9916.8, "text": " And the paper shows that the second option gives better performance.", "tokens": [51164, 400, 264, 3035, 3110, 300, 264, 1150, 3614, 2709, 1101, 3389, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12348138584810145, "compression_ratio": 1.651006711409396, "no_speech_prob": 0.000487829209305346}, {"id": 1082, "seek": 991680, "start": 9917.8, "end": 9927.8, "text": " And in terms of the decoder, it's again a cascaded diffusion models, which contains a one-base model and two super resolution models.", "tokens": [50414, 400, 294, 2115, 295, 264, 979, 19866, 11, 309, 311, 797, 257, 3058, 66, 12777, 25242, 5245, 11, 597, 8306, 257, 472, 12, 17429, 2316, 293, 732, 1687, 8669, 5245, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1366400271654129, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453520428389311}, {"id": 1083, "seek": 991680, "start": 9927.8, "end": 9939.8, "text": " And to save the compute and make the training more efficient, the largest super resolution model is trained on image patches of one quarter size.", "tokens": [50914, 400, 281, 3155, 264, 14722, 293, 652, 264, 3097, 544, 7148, 11, 264, 6443, 1687, 8669, 2316, 307, 8895, 322, 3256, 26531, 295, 472, 6555, 2744, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1366400271654129, "compression_ratio": 1.603448275862069, "no_speech_prob": 0.0002453520428389311}, {"id": 1084, "seek": 993980, "start": 9939.8, "end": 9946.8, "text": " But during inference, the model will take the full resolution inputs and directly do the inference on the full resolution.", "tokens": [50364, 583, 1830, 38253, 11, 264, 2316, 486, 747, 264, 1577, 8669, 15743, 293, 3838, 360, 264, 38253, 322, 264, 1577, 8669, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10460058280399867, "compression_ratio": 1.5917159763313609, "no_speech_prob": 0.0006985908839851618}, {"id": 1085, "seek": 993980, "start": 9947.8, "end": 9956.8, "text": " And this paper also shows that the classifier-free guidance and noise conditioning augmentation are super important to make the decoder work well.", "tokens": [50764, 400, 341, 3035, 611, 3110, 300, 264, 1508, 9902, 12, 10792, 10056, 293, 5658, 21901, 14501, 19631, 366, 1687, 1021, 281, 652, 264, 979, 19866, 589, 731, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10460058280399867, "compression_ratio": 1.5917159763313609, "no_speech_prob": 0.0006985908839851618}, {"id": 1086, "seek": 995680, "start": 9957.8, "end": 9969.8, "text": " And a little bit more detail about the bipartisan latent representations. So given an input image, we can get the bipartisan latent representations in the following sense.", "tokens": [50414, 400, 257, 707, 857, 544, 2607, 466, 264, 31954, 48994, 33358, 13, 407, 2212, 364, 4846, 3256, 11, 321, 393, 483, 264, 31954, 48994, 33358, 294, 264, 3480, 2020, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17967686388227674, "compression_ratio": 1.7, "no_speech_prob": 0.0008968489710241556}, {"id": 1087, "seek": 995680, "start": 9969.8, "end": 9971.8, "text": " So it contains two parts.", "tokens": [51014, 407, 309, 8306, 732, 3166, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17967686388227674, "compression_ratio": 1.7, "no_speech_prob": 0.0008968489710241556}, {"id": 1088, "seek": 995680, "start": 9971.8, "end": 9980.8, "text": " First is this latent variable Z, which is the clip image embeddings, and it can be derived by running the clip image encoder.", "tokens": [51114, 2386, 307, 341, 48994, 7006, 1176, 11, 597, 307, 264, 7353, 3256, 12240, 29432, 11, 293, 309, 393, 312, 18949, 538, 2614, 264, 7353, 3256, 2058, 19866, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17967686388227674, "compression_ratio": 1.7, "no_speech_prob": 0.0008968489710241556}, {"id": 1089, "seek": 998080, "start": 9981.8, "end": 9985.8, "text": " And the second part is this xt, which is the latency from the decoder.", "tokens": [50414, 400, 264, 1150, 644, 307, 341, 220, 734, 11, 597, 307, 264, 27043, 490, 264, 979, 19866, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1071120023727417, "compression_ratio": 1.6733668341708543, "no_speech_prob": 0.001500979415141046}, {"id": 1090, "seek": 998080, "start": 9985.8, "end": 9993.8, "text": " And this part can be derived by running the inversion of an ddim sampler for the decoder.", "tokens": [50614, 400, 341, 644, 393, 312, 18949, 538, 2614, 264, 43576, 295, 364, 274, 13595, 3247, 22732, 337, 264, 979, 19866, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1071120023727417, "compression_ratio": 1.6733668341708543, "no_speech_prob": 0.001500979415141046}, {"id": 1091, "seek": 998080, "start": 9993.8, "end": 10005.8, "text": " And after getting these two latent representations, the paper shows that it is possible to run this decoder and get near-perfect reconstruction of the original input image.", "tokens": [51014, 400, 934, 1242, 613, 732, 48994, 33358, 11, 264, 3035, 3110, 300, 309, 307, 1944, 281, 1190, 341, 979, 19866, 293, 483, 2651, 12, 610, 1836, 31565, 295, 264, 3380, 4846, 3256, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1071120023727417, "compression_ratio": 1.6733668341708543, "no_speech_prob": 0.001500979415141046}, {"id": 1092, "seek": 1000580, "start": 10005.8, "end": 10014.8, "text": " And given these bipartisan representations, the paper shows that it is possible to do several image manipulation tasks.", "tokens": [50364, 400, 2212, 613, 31954, 33358, 11, 264, 3035, 3110, 300, 309, 307, 1944, 281, 360, 2940, 3256, 26475, 9608, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13387961387634278, "compression_ratio": 1.627027027027027, "no_speech_prob": 0.00013981029042042792}, {"id": 1093, "seek": 1000580, "start": 10014.8, "end": 10030.8, "text": " For example, this image variation tasks target at getting multiple variants of an input image, while with the hope of preserving the high-level semantic meanings of the input image.", "tokens": [50814, 1171, 1365, 11, 341, 3256, 12990, 9608, 3779, 412, 1242, 3866, 21669, 295, 364, 4846, 3256, 11, 1339, 365, 264, 1454, 295, 33173, 264, 1090, 12, 12418, 47982, 28138, 295, 264, 4846, 3256, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13387961387634278, "compression_ratio": 1.627027027027027, "no_speech_prob": 0.00013981029042042792}, {"id": 1094, "seek": 1003080, "start": 10030.8, "end": 10041.8, "text": " And this is achieved by fixing the clip embedding Z, while changing to different latent xt in the decoder.", "tokens": [50364, 400, 341, 307, 11042, 538, 19442, 264, 7353, 12240, 3584, 1176, 11, 1339, 4473, 281, 819, 48994, 220, 734, 294, 264, 979, 19866, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15558857028767215, "compression_ratio": 1.4805194805194806, "no_speech_prob": 0.0008826805860735476}, {"id": 1095, "seek": 1003080, "start": 10041.8, "end": 10050.8, "text": " And as shown in this image panel, the first one is the input image, and the rest are the image variants generated by .e2.", "tokens": [50914, 400, 382, 4898, 294, 341, 3256, 4831, 11, 264, 700, 472, 307, 264, 4846, 3256, 11, 293, 264, 1472, 366, 264, 3256, 21669, 10833, 538, 2411, 68, 17, 13, 51364], "temperature": 0.0, "avg_logprob": -0.15558857028767215, "compression_ratio": 1.4805194805194806, "no_speech_prob": 0.0008826805860735476}, {"id": 1096, "seek": 1005080, "start": 10050.8, "end": 10058.8, "text": " And we can see that certain high-level semantic meanings are preserved, for example, the artist's style.", "tokens": [50364, 400, 321, 393, 536, 300, 1629, 1090, 12, 12418, 47982, 28138, 366, 22242, 11, 337, 1365, 11, 264, 5748, 311, 3758, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1007484920689317, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.0018382648704573512}, {"id": 1097, "seek": 1005080, "start": 10058.8, "end": 10068.8, "text": " And this clock is preserved in all the image variants, but with different details in the image variants.", "tokens": [50764, 400, 341, 7830, 307, 22242, 294, 439, 264, 3256, 21669, 11, 457, 365, 819, 4365, 294, 264, 3256, 21669, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1007484920689317, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.0018382648704573512}, {"id": 1098, "seek": 1005080, "start": 10068.8, "end": 10072.8, "text": " And the second task is this image interpolation task.", "tokens": [51264, 400, 264, 1150, 5633, 307, 341, 3256, 44902, 399, 5633, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1007484920689317, "compression_ratio": 1.5748502994011977, "no_speech_prob": 0.0018382648704573512}, {"id": 1099, "seek": 1007280, "start": 10072.8, "end": 10082.8, "text": " So given two input images, it's possible to use .e2 to do interpolation by interpolating the image clip embeddings of these two input images.", "tokens": [50364, 407, 2212, 732, 4846, 5267, 11, 309, 311, 1944, 281, 764, 2411, 68, 17, 281, 360, 44902, 399, 538, 44902, 990, 264, 3256, 7353, 12240, 29432, 295, 613, 732, 4846, 5267, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07376338541507721, "compression_ratio": 1.6832298136645962, "no_speech_prob": 0.01716955564916134}, {"id": 1100, "seek": 1007280, "start": 10082.8, "end": 10091.8, "text": " And we can get different interpolation trajectories by using different xt along these trajectories, as shown in those three rows.", "tokens": [50864, 400, 321, 393, 483, 819, 44902, 399, 18257, 2083, 538, 1228, 819, 220, 734, 2051, 613, 18257, 2083, 11, 382, 4898, 294, 729, 1045, 13241, 13, 51314], "temperature": 0.0, "avg_logprob": -0.07376338541507721, "compression_ratio": 1.6832298136645962, "no_speech_prob": 0.01716955564916134}, {"id": 1101, "seek": 1009180, "start": 10092.8, "end": 10106.8, "text": " And as we can see, although the trajectories are different, but the high-level semantic meanings are kept well for the two input images for all those three interpolation trajectories.", "tokens": [50414, 400, 382, 321, 393, 536, 11, 4878, 264, 18257, 2083, 366, 819, 11, 457, 264, 1090, 12, 12418, 47982, 28138, 366, 4305, 731, 337, 264, 732, 4846, 5267, 337, 439, 729, 1045, 44902, 399, 18257, 2083, 13, 51114], "temperature": 0.0, "avg_logprob": -0.08144393781336343, "compression_ratio": 1.4409448818897639, "no_speech_prob": 0.011864274740219116}, {"id": 1102, "seek": 1010680, "start": 10106.8, "end": 10123.8, "text": " And the last task, the most interesting task that they show that can be done by .e2 is this text div task, which means like given an input image and the corresponding description, we would like to add this image towards a different prompt.", "tokens": [50364, 400, 264, 1036, 5633, 11, 264, 881, 1880, 5633, 300, 436, 855, 300, 393, 312, 1096, 538, 2411, 68, 17, 307, 341, 2487, 3414, 5633, 11, 597, 1355, 411, 2212, 364, 4846, 3256, 293, 264, 11760, 3855, 11, 321, 576, 411, 281, 909, 341, 3256, 3030, 257, 819, 12391, 13, 51214], "temperature": 0.0, "avg_logprob": -0.16359482871161568, "compression_ratio": 1.5126582278481013, "no_speech_prob": 0.002714030910283327}, {"id": 1103, "seek": 1012380, "start": 10124.8, "end": 10129.8, "text": " And this corresponds to an arithmetic operation in the latent space.", "tokens": [50414, 400, 341, 23249, 281, 364, 42973, 6916, 294, 264, 48994, 1901, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06762663757099825, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.005639269482344389}, {"id": 1104, "seek": 1012380, "start": 10129.8, "end": 10139.8, "text": " More specifically, they first try to compute the difference between the text clip embeddings of the original prompt and the target prompt.", "tokens": [50664, 5048, 4682, 11, 436, 700, 853, 281, 14722, 264, 2649, 1296, 264, 2487, 7353, 12240, 29432, 295, 264, 3380, 12391, 293, 264, 3779, 12391, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06762663757099825, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.005639269482344389}, {"id": 1105, "seek": 1012380, "start": 10139.8, "end": 10147.8, "text": " And then they try to change the image clip embedding of the given input towards the difference between the text prompts.", "tokens": [51164, 400, 550, 436, 853, 281, 1319, 264, 3256, 7353, 12240, 3584, 295, 264, 2212, 4846, 3030, 264, 2649, 1296, 264, 2487, 41095, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06762663757099825, "compression_ratio": 1.7634408602150538, "no_speech_prob": 0.005639269482344389}, {"id": 1106, "seek": 1014780, "start": 10147.8, "end": 10157.8, "text": " And using this approach, they show that it's possible to do this text-guided image editing by changing the prompt.", "tokens": [50364, 400, 1228, 341, 3109, 11, 436, 855, 300, 309, 311, 1944, 281, 360, 341, 2487, 12, 2794, 2112, 3256, 10000, 538, 4473, 264, 12391, 13, 50864], "temperature": 0.0, "avg_logprob": -0.12995252889745376, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0008828097488731146}, {"id": 1107, "seek": 1014780, "start": 10157.8, "end": 10165.8, "text": " And the last task to image diffusion model I want to talk about is this imagined model by Google Brain Team.", "tokens": [50864, 400, 264, 1036, 5633, 281, 3256, 25242, 2316, 286, 528, 281, 751, 466, 307, 341, 16590, 2316, 538, 3329, 29783, 7606, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12995252889745376, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0008828097488731146}, {"id": 1108, "seek": 1014780, "start": 10165.8, "end": 10168.8, "text": " So again, the task is the same as .e2.", "tokens": [51264, 407, 797, 11, 264, 5633, 307, 264, 912, 382, 2411, 68, 17, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12995252889745376, "compression_ratio": 1.4886363636363635, "no_speech_prob": 0.0008828097488731146}, {"id": 1109, "seek": 1016880, "start": 10168.8, "end": 10178.8, "text": " So we are given some text prompts as input, and we are trying to output 1k by 1k images aligned with the input text.", "tokens": [50364, 407, 321, 366, 2212, 512, 2487, 41095, 382, 4846, 11, 293, 321, 366, 1382, 281, 5598, 502, 74, 538, 502, 74, 5267, 17962, 365, 264, 4846, 2487, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13762448050759055, "compression_ratio": 1.346774193548387, "no_speech_prob": 0.003764074994251132}, {"id": 1110, "seek": 1016880, "start": 10178.8, "end": 10181.8, "text": " And the highlight of imagined model is as follows.", "tokens": [50864, 400, 264, 5078, 295, 16590, 2316, 307, 382, 10002, 13, 51014], "temperature": 0.0, "avg_logprob": -0.13762448050759055, "compression_ratio": 1.346774193548387, "no_speech_prob": 0.003764074994251132}, {"id": 1111, "seek": 1018180, "start": 10181.8, "end": 10196.8, "text": " First, it provides an unprecedented degree of photorealisticism in terms of state-of-the-art automatic scores, such as FID scores, as well as state-of-the-art human ratings.", "tokens": [50364, 2386, 11, 309, 6417, 364, 21555, 4314, 295, 2409, 418, 304, 3142, 1434, 294, 2115, 295, 1785, 12, 2670, 12, 3322, 12, 446, 12509, 13444, 11, 1270, 382, 479, 2777, 13444, 11, 382, 731, 382, 1785, 12, 2670, 12, 3322, 12, 446, 1952, 24603, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13292459760393416, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.018247324973344803}, {"id": 1112, "seek": 1018180, "start": 10196.8, "end": 10203.8, "text": " And it provides a deep level of language understanding, as can be told by the generated samples.", "tokens": [51114, 400, 309, 6417, 257, 2452, 1496, 295, 2856, 3701, 11, 382, 393, 312, 1907, 538, 264, 10833, 10938, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13292459760393416, "compression_ratio": 1.5340909090909092, "no_speech_prob": 0.018247324973344803}, {"id": 1113, "seek": 1020380, "start": 10203.8, "end": 10209.8, "text": " And it is extremely simple, so there is no latent space and no compensation.", "tokens": [50364, 400, 309, 307, 4664, 2199, 11, 370, 456, 307, 572, 48994, 1901, 293, 572, 19644, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1424430451303158, "compression_ratio": 1.3877551020408163, "no_speech_prob": 0.0023592887446284294}, {"id": 1114, "seek": 1020380, "start": 10209.8, "end": 10215.8, "text": " And as we will see later, it's just like a pure cascaded diffusion model.", "tokens": [50664, 400, 382, 321, 486, 536, 1780, 11, 309, 311, 445, 411, 257, 6075, 3058, 66, 12777, 25242, 2316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1424430451303158, "compression_ratio": 1.3877551020408163, "no_speech_prob": 0.0023592887446284294}, {"id": 1115, "seek": 1020380, "start": 10215.8, "end": 10226.8, "text": " So I will first present several examples of imagined.", "tokens": [50964, 407, 286, 486, 700, 1974, 2940, 5110, 295, 16590, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1424430451303158, "compression_ratio": 1.3877551020408163, "no_speech_prob": 0.0023592887446284294}, {"id": 1116, "seek": 1022680, "start": 10226.8, "end": 10234.8, "text": " Yeah, this is my favorite one because I just created it to make it related to CVPR.", "tokens": [50364, 865, 11, 341, 307, 452, 2954, 472, 570, 286, 445, 2942, 309, 281, 652, 309, 4077, 281, 22995, 15958, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10651233196258544, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.0015245263930410147}, {"id": 1117, "seek": 1022680, "start": 10234.8, "end": 10248.8, "text": " And in terms of the key modeling components of imagined, like I mentioned, it is a pure cascaded diffusion model containing one base model and two super resolution models.", "tokens": [50764, 400, 294, 2115, 295, 264, 2141, 15983, 6677, 295, 16590, 11, 411, 286, 2835, 11, 309, 307, 257, 6075, 3058, 66, 12777, 25242, 2316, 19273, 472, 3096, 2316, 293, 732, 1687, 8669, 5245, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10651233196258544, "compression_ratio": 1.4488636363636365, "no_speech_prob": 0.0015245263930410147}, {"id": 1118, "seek": 1024880, "start": 10248.8, "end": 10256.8, "text": " And it uses classifier-free guidance and the dynamic thresholding, as I will talk about later.", "tokens": [50364, 400, 309, 4960, 1508, 9902, 12, 10792, 10056, 293, 264, 8546, 14678, 278, 11, 382, 286, 486, 751, 466, 1780, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20798393299705104, "compression_ratio": 1.52, "no_speech_prob": 0.008574351668357849}, {"id": 1119, "seek": 1024880, "start": 10256.8, "end": 10277.8, "text": " And unlike DAI2, which uses clip text embedding as the, using this clip text encoder, this imagined used frozen large pre-train language models as the text encoders, more specifically this variant of T5 model.", "tokens": [50764, 400, 8343, 9578, 40, 17, 11, 597, 4960, 7353, 2487, 12240, 3584, 382, 264, 11, 1228, 341, 7353, 2487, 2058, 19866, 11, 341, 16590, 1143, 12496, 2416, 659, 12, 83, 7146, 2856, 5245, 382, 264, 2487, 2058, 378, 433, 11, 544, 4682, 341, 17501, 295, 314, 20, 2316, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20798393299705104, "compression_ratio": 1.52, "no_speech_prob": 0.008574351668357849}, {"id": 1120, "seek": 1027780, "start": 10277.8, "end": 10281.8, "text": " And there are several key observations from imagined.", "tokens": [50364, 400, 456, 366, 2940, 2141, 18163, 490, 16590, 13, 50564], "temperature": 0.0, "avg_logprob": -0.08577564581116634, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.0015483575407415628}, {"id": 1121, "seek": 1027780, "start": 10281.8, "end": 10287.8, "text": " First, it is beneficial to use text conditioning for all the super resolution models.", "tokens": [50564, 2386, 11, 309, 307, 14072, 281, 764, 2487, 21901, 337, 439, 264, 1687, 8669, 5245, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08577564581116634, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.0015483575407415628}, {"id": 1122, "seek": 1027780, "start": 10287.8, "end": 10298.8, "text": " The explanation is as follows. So remember, like for cascaded diffusion models, we need to use this noise conditioning augmentation technique to reduce the compounding error.", "tokens": [50864, 440, 10835, 307, 382, 10002, 13, 407, 1604, 11, 411, 337, 3058, 66, 12777, 25242, 5245, 11, 321, 643, 281, 764, 341, 5658, 21901, 14501, 19631, 6532, 281, 5407, 264, 14154, 278, 6713, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08577564581116634, "compression_ratio": 1.5544554455445545, "no_speech_prob": 0.0015483575407415628}, {"id": 1123, "seek": 1029880, "start": 10298.8, "end": 10305.8, "text": " But however, this technique has a chance to weaken the information from the low resolution models.", "tokens": [50364, 583, 4461, 11, 341, 6532, 575, 257, 2931, 281, 48576, 264, 1589, 490, 264, 2295, 8669, 5245, 13, 50714], "temperature": 0.0, "avg_logprob": -0.0695165306774538, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0012840970885008574}, {"id": 1124, "seek": 1029880, "start": 10305.8, "end": 10313.8, "text": " Thus, we really need the text conditioning as extra information input to support the super resolution models.", "tokens": [50714, 13827, 11, 321, 534, 643, 264, 2487, 21901, 382, 2857, 1589, 4846, 281, 1406, 264, 1687, 8669, 5245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.0695165306774538, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0012840970885008574}, {"id": 1125, "seek": 1029880, "start": 10313.8, "end": 10321.8, "text": " And second observation is that scaling the text encoder is extremely efficient in terms of improving the performance of imagined.", "tokens": [51114, 400, 1150, 14816, 307, 300, 21589, 264, 2487, 2058, 19866, 307, 4664, 7148, 294, 2115, 295, 11470, 264, 3389, 295, 16590, 13, 51514], "temperature": 0.0, "avg_logprob": -0.0695165306774538, "compression_ratio": 1.681592039800995, "no_speech_prob": 0.0012840970885008574}, {"id": 1126, "seek": 1032180, "start": 10322.8, "end": 10329.8, "text": " And it has been shown that this is even more important than scaling the diffusion model side.", "tokens": [50414, 400, 309, 575, 668, 4898, 300, 341, 307, 754, 544, 1021, 813, 21589, 264, 25242, 2316, 1252, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10450391035813551, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.006190924439579248}, {"id": 1127, "seek": 1032180, "start": 10329.8, "end": 10347.8, "text": " And lastly, comparing using the pre-trained large language model as the encoder versus the clip encoder, human readers actually prefer the large language model over the clip encoder on certain data sets.", "tokens": [50764, 400, 16386, 11, 15763, 1228, 264, 659, 12, 17227, 2001, 2416, 2856, 2316, 382, 264, 2058, 19866, 5717, 264, 7353, 2058, 19866, 11, 1952, 17147, 767, 4382, 264, 2416, 2856, 2316, 670, 264, 7353, 2058, 19866, 322, 1629, 1412, 6352, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10450391035813551, "compression_ratio": 1.6779661016949152, "no_speech_prob": 0.006190924439579248}, {"id": 1128, "seek": 1034780, "start": 10347.8, "end": 10353.8, "text": " And this dynamic thresholding is a new technique introduced by imagined.", "tokens": [50364, 400, 341, 8546, 14678, 278, 307, 257, 777, 6532, 7268, 538, 16590, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11475093548114483, "compression_ratio": 1.6, "no_speech_prob": 0.002590133110061288}, {"id": 1129, "seek": 1034780, "start": 10353.8, "end": 10364.8, "text": " This is mainly to solve the trade-off problem of using large classifier-free guidance weights, more specifically as we also discussed in the previous part.", "tokens": [50664, 639, 307, 8704, 281, 5039, 264, 4923, 12, 4506, 1154, 295, 1228, 2416, 1508, 9902, 12, 10792, 10056, 17443, 11, 544, 4682, 382, 321, 611, 7152, 294, 264, 3894, 644, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11475093548114483, "compression_ratio": 1.6, "no_speech_prob": 0.002590133110061288}, {"id": 1130, "seek": 1034780, "start": 10364.8, "end": 10375.8, "text": " So when we use large classifier guidance weights, there is a chance that it gives us better text alignment but worse image quality.", "tokens": [51214, 407, 562, 321, 764, 2416, 1508, 9902, 10056, 17443, 11, 456, 307, 257, 2931, 300, 309, 2709, 505, 1101, 2487, 18515, 457, 5324, 3256, 3125, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11475093548114483, "compression_ratio": 1.6, "no_speech_prob": 0.002590133110061288}, {"id": 1131, "seek": 1037580, "start": 10376.8, "end": 10384.8, "text": " So as we use large free guidance weights, the clip score, which corresponds to a better text alignment, increases.", "tokens": [50414, 407, 382, 321, 764, 2416, 1737, 10056, 17443, 11, 264, 7353, 6175, 11, 597, 23249, 281, 257, 1101, 2487, 18515, 11, 8637, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1962887864363821, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.005553025286644697}, {"id": 1132, "seek": 1037580, "start": 10384.8, "end": 10391.8, "text": " However, the IFID score also increases, which corresponds to worse sample quality.", "tokens": [50814, 2908, 11, 264, 26080, 2777, 6175, 611, 8637, 11, 597, 23249, 281, 5324, 6889, 3125, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1962887864363821, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.005553025286644697}, {"id": 1133, "seek": 1037580, "start": 10391.8, "end": 10401.8, "text": " So to elevate this issue, because we really want both the sample quality, like the good sample quality as well as good text elements, right?", "tokens": [51164, 407, 281, 33054, 341, 2734, 11, 570, 321, 534, 528, 1293, 264, 6889, 3125, 11, 411, 264, 665, 6889, 3125, 382, 731, 382, 665, 2487, 4959, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.1962887864363821, "compression_ratio": 1.6732673267326732, "no_speech_prob": 0.005553025286644697}, {"id": 1134, "seek": 1040180, "start": 10401.8, "end": 10416.8, "text": " So to elevate this trade-off issue, the hypothesis this paper made is that the reason why the sample quality decreases at large guidance weights is that at large guidance weights,", "tokens": [50364, 407, 281, 33054, 341, 4923, 12, 4506, 2734, 11, 264, 17291, 341, 3035, 1027, 307, 300, 264, 1778, 983, 264, 6889, 3125, 24108, 412, 2416, 10056, 17443, 307, 300, 412, 2416, 10056, 17443, 11, 51114], "temperature": 0.0, "avg_logprob": -0.06823302570142244, "compression_ratio": 1.543103448275862, "no_speech_prob": 0.00011060449469368905}, {"id": 1135, "seek": 1041680, "start": 10416.8, "end": 10431.8, "text": " usually it corresponds to very large sample gradients in inference, and then the generated samples have a chance to be saturated because of the very large gradient updates.", "tokens": [50364, 2673, 309, 23249, 281, 588, 2416, 6889, 2771, 2448, 294, 38253, 11, 293, 550, 264, 10833, 10938, 362, 257, 2931, 281, 312, 25408, 570, 295, 264, 588, 2416, 16235, 9205, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1282499040876116, "compression_ratio": 1.4700854700854702, "no_speech_prob": 0.0006461227894760668}, {"id": 1136, "seek": 1043180, "start": 10431.8, "end": 10449.8, "text": " So the solution they propose is this dynamic thresholding, meaning that at each sampling step, we adjust the pixel values of the samples to be within a dynamic range, and this dynamic range is computed over the statistics of the current samples.", "tokens": [50364, 407, 264, 3827, 436, 17421, 307, 341, 8546, 14678, 278, 11, 3620, 300, 412, 1184, 21179, 1823, 11, 321, 4369, 264, 19261, 4190, 295, 264, 10938, 281, 312, 1951, 257, 8546, 3613, 11, 293, 341, 8546, 3613, 307, 40610, 670, 264, 12523, 295, 264, 2190, 10938, 13, 51264], "temperature": 0.0, "avg_logprob": -0.076828026304058, "compression_ratio": 1.611842105263158, "no_speech_prob": 0.0018380866385996342}, {"id": 1137, "seek": 1044980, "start": 10449.8, "end": 10458.8, "text": " And these two panels shows the qualitative comparisons between static thresholding and dynamic thresholding.", "tokens": [50364, 400, 613, 732, 13419, 3110, 264, 31312, 33157, 1296, 13437, 14678, 278, 293, 8546, 14678, 278, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1461940164919253, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0007320267031900585}, {"id": 1138, "seek": 1044980, "start": 10458.8, "end": 10470.8, "text": " And you can see if we use this static thresholding, the images look kind of saturated, while the dynamic thresholding, the samples look more realistic.", "tokens": [50814, 400, 291, 393, 536, 498, 321, 764, 341, 13437, 14678, 278, 11, 264, 5267, 574, 733, 295, 25408, 11, 1339, 264, 8546, 14678, 278, 11, 264, 10938, 574, 544, 12465, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1461940164919253, "compression_ratio": 1.6993464052287581, "no_speech_prob": 0.0007320267031900585}, {"id": 1139, "seek": 1047080, "start": 10470.8, "end": 10478.8, "text": " And another contribution of Imagine is that they introduce a new benchmark, especially for this text-to-image evaluations.", "tokens": [50364, 400, 1071, 13150, 295, 11739, 307, 300, 436, 5366, 257, 777, 18927, 11, 2318, 337, 341, 2487, 12, 1353, 12, 26624, 43085, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13674715088634956, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.002396302530542016}, {"id": 1140, "seek": 1047080, "start": 10478.8, "end": 10489.8, "text": " So the motivation of introducing new benchmarks is that for existing datasets, for example, COCO, the text problem is kind of limited and is kind of easy.", "tokens": [50764, 407, 264, 12335, 295, 15424, 777, 43751, 307, 300, 337, 6741, 42856, 11, 337, 1365, 11, 3002, 12322, 11, 264, 2487, 1154, 307, 733, 295, 5567, 293, 307, 733, 295, 1858, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13674715088634956, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.002396302530542016}, {"id": 1141, "seek": 1047080, "start": 10489.8, "end": 10497.8, "text": " So this benchmark introduced more challenging prompts to evaluate text-to-image models across multiple dimensions.", "tokens": [51314, 407, 341, 18927, 7268, 544, 7595, 41095, 281, 13059, 2487, 12, 1353, 12, 26624, 5245, 2108, 3866, 12819, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13674715088634956, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.002396302530542016}, {"id": 1142, "seek": 1049780, "start": 10497.8, "end": 10510.8, "text": " For example, it tries to evaluate the ability of the model to facefully render different colors, numbers of objects, spatial relations, text in the scene, unusual interactions between objects.", "tokens": [50364, 1171, 1365, 11, 309, 9898, 281, 13059, 264, 3485, 295, 264, 2316, 281, 1851, 2277, 15529, 819, 4577, 11, 3547, 295, 6565, 11, 23598, 2299, 11, 2487, 294, 264, 4145, 11, 10901, 13280, 1296, 6565, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09779987970987956, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.002799991751089692}, {"id": 1143, "seek": 1049780, "start": 10510.8, "end": 10524.8, "text": " And it also contains some complex prompts, for example, long and intricate descriptions, wire words, and even misspelled prompts to test the robustness of your model.", "tokens": [51014, 400, 309, 611, 8306, 512, 3997, 41095, 11, 337, 1365, 11, 938, 293, 38015, 24406, 11, 6234, 2283, 11, 293, 754, 1713, 33000, 41095, 281, 1500, 264, 13956, 1287, 295, 428, 2316, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09779987970987956, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.002799991751089692}, {"id": 1144, "seek": 1052480, "start": 10524.8, "end": 10537.8, "text": " And this figure shows several examples of the text prompts in this benchmark, and the corresponding generated images from Imagine using these text prompts.", "tokens": [50364, 400, 341, 2573, 3110, 2940, 5110, 295, 264, 2487, 41095, 294, 341, 18927, 11, 293, 264, 11760, 10833, 5267, 490, 11739, 1228, 613, 2487, 41095, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10658801988113759, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0004802104376722127}, {"id": 1145, "seek": 1052480, "start": 10537.8, "end": 10542.8, "text": " And a bit more of the quantitative evaluations of Imagine.", "tokens": [51014, 400, 257, 857, 544, 295, 264, 27778, 43085, 295, 11739, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10658801988113759, "compression_ratio": 1.4758620689655173, "no_speech_prob": 0.0004802104376722127}, {"id": 1146, "seek": 1054280, "start": 10542.8, "end": 10557.8, "text": " Imagine got state-of-the-art automatic evaluation scores on the COCO dataset, and it's also preferred over reasoned work by human readers in both sample quality and image text alignment on the drawbench dataset.", "tokens": [50364, 11739, 658, 1785, 12, 2670, 12, 3322, 12, 446, 12509, 13344, 13444, 322, 264, 3002, 12322, 28872, 11, 293, 309, 311, 611, 16494, 670, 1778, 292, 589, 538, 1952, 17147, 294, 1293, 6889, 3125, 293, 3256, 2487, 18515, 322, 264, 2642, 47244, 28872, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17787537923673305, "compression_ratio": 1.4632034632034632, "no_speech_prob": 0.006387363653630018}, {"id": 1147, "seek": 1054280, "start": 10557.8, "end": 10568.8, "text": " And the reasoned work compared by Imagine includes this DAO-E2 slide, VQGAM plus clip, as well as the latent diffusion models.", "tokens": [51114, 400, 264, 1778, 292, 589, 5347, 538, 11739, 5974, 341, 9578, 46, 12, 36, 17, 4137, 11, 691, 48, 38, 2865, 1804, 7353, 11, 382, 731, 382, 264, 48994, 25242, 5245, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17787537923673305, "compression_ratio": 1.4632034632034632, "no_speech_prob": 0.006387363653630018}, {"id": 1148, "seek": 1056880, "start": 10568.8, "end": 10575.8, "text": " Okay, so besides text-to-image generation, I also want to talk about the controllable generation using diffusion models.", "tokens": [50364, 1033, 11, 370, 11868, 2487, 12, 1353, 12, 26624, 5125, 11, 286, 611, 528, 281, 751, 466, 264, 45159, 712, 5125, 1228, 25242, 5245, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13759196656090872, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.001098503707908094}, {"id": 1149, "seek": 1056880, "start": 10575.8, "end": 10586.8, "text": " And a representative work is this diffusion autoencoders, which propose to incorporate a semantic meaningful latent variables to diffusion models.", "tokens": [50714, 400, 257, 12424, 589, 307, 341, 25242, 8399, 22660, 378, 433, 11, 597, 17421, 281, 16091, 257, 47982, 10995, 48994, 9102, 281, 25242, 5245, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13759196656090872, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.001098503707908094}, {"id": 1150, "seek": 1058680, "start": 10586.8, "end": 10595.8, "text": " So more precisely, so given an input image, semantic encoders learn in this framework to generate this Z-SIM.", "tokens": [50364, 407, 544, 13402, 11, 370, 2212, 364, 4846, 3256, 11, 47982, 2058, 378, 433, 1466, 294, 341, 8388, 281, 8460, 341, 1176, 12, 50, 6324, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1424006957274217, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.002396416151896119}, {"id": 1151, "seek": 1058680, "start": 10595.8, "end": 10603.8, "text": " And this Z-SIM is fit into a conditional diffusion models to further predict the clean samples.", "tokens": [50814, 400, 341, 1176, 12, 50, 6324, 307, 3318, 666, 257, 27708, 25242, 5245, 281, 3052, 6069, 264, 2541, 10938, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1424006957274217, "compression_ratio": 1.3758389261744965, "no_speech_prob": 0.002396416151896119}, {"id": 1152, "seek": 1060380, "start": 10603.8, "end": 10618.8, "text": " And this leads to some like the bipod latent representation similar to the DAO-E2 model, where we have this Z-SIM with the hope that it can capture high-level semantics.", "tokens": [50364, 400, 341, 6689, 281, 512, 411, 264, 19016, 378, 48994, 10290, 2531, 281, 264, 9578, 46, 12, 36, 17, 2316, 11, 689, 321, 362, 341, 1176, 12, 50, 6324, 365, 264, 1454, 300, 309, 393, 7983, 1090, 12, 12418, 4361, 45298, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14565259666853053, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.0070103988982737064}, {"id": 1153, "seek": 1060380, "start": 10618.8, "end": 10624.8, "text": " And we also have this X-Big-T, which is the inversion of this conditional DTIM sampler.", "tokens": [51114, 400, 321, 611, 362, 341, 1783, 12, 32023, 12, 51, 11, 597, 307, 264, 43576, 295, 341, 27708, 413, 5422, 44, 3247, 22732, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14565259666853053, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.0070103988982737064}, {"id": 1154, "seek": 1060380, "start": 10624.8, "end": 10630.8, "text": " And the hope is that it captures the low-level stochastic variations of the images.", "tokens": [51414, 400, 264, 1454, 307, 300, 309, 27986, 264, 2295, 12, 12418, 342, 8997, 2750, 17840, 295, 264, 5267, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14565259666853053, "compression_ratio": 1.5570776255707763, "no_speech_prob": 0.0070103988982737064}, {"id": 1155, "seek": 1063080, "start": 10630.8, "end": 10639.8, "text": " And if we want to do unconditional sampling from this model, optionally, we can learn another diffusion model in the latent space of the Z-SIM.", "tokens": [50364, 400, 498, 321, 528, 281, 360, 47916, 21179, 490, 341, 2316, 11, 3614, 379, 11, 321, 393, 1466, 1071, 25242, 2316, 294, 264, 48994, 1901, 295, 264, 1176, 12, 50, 6324, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12965761025746664, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.0010321643203496933}, {"id": 1156, "seek": 1063080, "start": 10639.8, "end": 10648.8, "text": " Very similar to the latent diffusion models we talked about in the last part, to support this unconditional generation test.", "tokens": [50814, 4372, 2531, 281, 264, 48994, 25242, 5245, 321, 2825, 466, 294, 264, 1036, 644, 11, 281, 1406, 341, 47916, 5125, 1500, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12965761025746664, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.0010321643203496933}, {"id": 1157, "seek": 1064880, "start": 10648.8, "end": 10661.8, "text": " Interestingly, they found that by assuming a low-dimensional semantic vector Z, they are able to learn different semantic meanings for different dimensions of this latent vector Z.", "tokens": [50364, 30564, 11, 436, 1352, 300, 538, 11926, 257, 2295, 12, 18759, 47982, 8062, 1176, 11, 436, 366, 1075, 281, 1466, 819, 47982, 28138, 337, 819, 12819, 295, 341, 48994, 8062, 1176, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1149839481837313, "compression_ratio": 1.8238341968911918, "no_speech_prob": 0.21172598004341125}, {"id": 1158, "seek": 1064880, "start": 10661.8, "end": 10675.8, "text": " For example, by changing the certain dimension of this latent Z, they are trying to identify different semantic meanings such as the higher style, the expression, the age,", "tokens": [51014, 1171, 1365, 11, 538, 4473, 264, 1629, 10139, 295, 341, 48994, 1176, 11, 436, 366, 1382, 281, 5876, 819, 47982, 28138, 1270, 382, 264, 2946, 3758, 11, 264, 6114, 11, 264, 3205, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1149839481837313, "compression_ratio": 1.8238341968911918, "no_speech_prob": 0.21172598004341125}, {"id": 1159, "seek": 1067580, "start": 10675.8, "end": 10681.8, "text": " and also the color of the hair.", "tokens": [50364, 293, 611, 264, 2017, 295, 264, 2578, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11124671132940996, "compression_ratio": 1.5, "no_speech_prob": 0.00044412538409233093}, {"id": 1160, "seek": 1067580, "start": 10681.8, "end": 10697.8, "text": " And they also are assuming that if we fix the Z-SIM for each row, and we change the latent X-Big-T in the conditional diffusion model, we can see it only corresponds to very tiny details in this image,", "tokens": [50664, 400, 436, 611, 366, 11926, 300, 498, 321, 3191, 264, 1176, 12, 50, 6324, 337, 1184, 5386, 11, 293, 321, 1319, 264, 48994, 1783, 12, 32023, 12, 51, 294, 264, 27708, 25242, 2316, 11, 321, 393, 536, 309, 787, 23249, 281, 588, 5870, 4365, 294, 341, 3256, 11, 51464], "temperature": 0.0, "avg_logprob": -0.11124671132940996, "compression_ratio": 1.5, "no_speech_prob": 0.00044412538409233093}, {"id": 1161, "seek": 1067580, "start": 10697.8, "end": 10703.8, "text": " and perhaps other like perceptually invisible features in the images.", "tokens": [51464, 293, 4317, 661, 411, 43276, 671, 14603, 4122, 294, 264, 5267, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11124671132940996, "compression_ratio": 1.5, "no_speech_prob": 0.00044412538409233093}, {"id": 1162, "seek": 1070380, "start": 10703.8, "end": 10710.8, "text": " So that ends the first part of the application. Thanks for listening.", "tokens": [50364, 407, 300, 5314, 264, 700, 644, 295, 264, 3861, 13, 2561, 337, 4764, 13, 50714], "temperature": 0.0, "avg_logprob": -0.141812175899357, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.005358091555535793}, {"id": 1163, "seek": 1070380, "start": 10710.8, "end": 10718.8, "text": " Awesome. Thanks Richie for the nice introduction of the first group of applications.", "tokens": [50714, 10391, 13, 2561, 6781, 414, 337, 264, 1481, 9339, 295, 264, 700, 1594, 295, 5821, 13, 51114], "temperature": 0.0, "avg_logprob": -0.141812175899357, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.005358091555535793}, {"id": 1164, "seek": 1070380, "start": 10718.8, "end": 10731.8, "text": " Here I'm going to start with the second group of applications. And in this part, I will mostly focus on image editing, image to image translation, super resolution, and semantic segmentation.", "tokens": [51114, 1692, 286, 478, 516, 281, 722, 365, 264, 1150, 1594, 295, 5821, 13, 400, 294, 341, 644, 11, 286, 486, 5240, 1879, 322, 3256, 10000, 11, 3256, 281, 3256, 12853, 11, 1687, 8669, 11, 293, 47982, 9469, 399, 13, 51764], "temperature": 0.0, "avg_logprob": -0.141812175899357, "compression_ratio": 1.696078431372549, "no_speech_prob": 0.005358091555535793}, {"id": 1165, "seek": 1073180, "start": 10731.8, "end": 10745.8, "text": " This is a super resolution. I'll start with talking about this work called super resolution, but we are repeated refinements, or SR3, which was proposed by Sahari et al at Google.", "tokens": [50364, 639, 307, 257, 1687, 8669, 13, 286, 603, 722, 365, 1417, 466, 341, 589, 1219, 1687, 8669, 11, 457, 321, 366, 10477, 44395, 6400, 11, 420, 20840, 18, 11, 597, 390, 10348, 538, 18280, 3504, 1030, 419, 412, 3329, 13, 51064], "temperature": 0.0, "avg_logprob": -0.24298438158902255, "compression_ratio": 1.335820895522388, "no_speech_prob": 0.005785342305898666}, {"id": 1166, "seek": 1074580, "start": 10745.8, "end": 10761.8, "text": " In image super resolution, we can consider this problem as training a conditional model P of X given Y, where Y is the low resolution image and X is the corresponding high resolution image.", "tokens": [50364, 682, 3256, 1687, 8669, 11, 321, 393, 1949, 341, 1154, 382, 3097, 257, 27708, 2316, 430, 295, 1783, 2212, 398, 11, 689, 398, 307, 264, 2295, 8669, 3256, 293, 1783, 307, 264, 11760, 1090, 8669, 3256, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13881171134210402, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.004818886052817106}, {"id": 1167, "seek": 1074580, "start": 10761.8, "end": 10768.8, "text": " So we want to be able to change the high resolution images given some input low resolution image.", "tokens": [51164, 407, 321, 528, 281, 312, 1075, 281, 1319, 264, 1090, 8669, 5267, 2212, 512, 4846, 2295, 8669, 3256, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13881171134210402, "compression_ratio": 1.771604938271605, "no_speech_prob": 0.004818886052817106}, {"id": 1168, "seek": 1076880, "start": 10768.8, "end": 10777.8, "text": " In order to tackle this problem, the authors proposed to train a diffusion model, a conditional diffusion model using this objective.", "tokens": [50364, 682, 1668, 281, 14896, 341, 1154, 11, 264, 16552, 10348, 281, 3847, 257, 25242, 2316, 11, 257, 27708, 25242, 2316, 1228, 341, 10024, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13660721685372146, "compression_ratio": 1.62, "no_speech_prob": 0.0026248646900057793}, {"id": 1169, "seek": 1076880, "start": 10777.8, "end": 10786.8, "text": " Here in this objective, we have expectation over pairs of high resolution image X and low resolution image Y.", "tokens": [50814, 1692, 294, 341, 10024, 11, 321, 362, 14334, 670, 15494, 295, 1090, 8669, 3256, 1783, 293, 2295, 8669, 3256, 398, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13660721685372146, "compression_ratio": 1.62, "no_speech_prob": 0.0026248646900057793}, {"id": 1170, "seek": 1078680, "start": 10786.8, "end": 10800.8, "text": " We have expectation over epsilon, which is drawn from standard normal distribution, and we have expectation over time, where time varies from, for example, zero to capital T corresponding to the diffusion process.", "tokens": [50364, 492, 362, 14334, 670, 17889, 11, 597, 307, 10117, 490, 3832, 2710, 7316, 11, 293, 321, 362, 14334, 670, 565, 11, 689, 565, 21716, 490, 11, 337, 1365, 11, 4018, 281, 4238, 314, 11760, 281, 264, 25242, 1399, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1662037871604742, "compression_ratio": 1.5, "no_speech_prob": 0.006789490580558777}, {"id": 1171, "seek": 1080080, "start": 10800.8, "end": 10816.8, "text": " We have this excellent setup. This is a noise prediction network that takes diffused high resolution image, XT, the time, as well as this Y, this is the low resolution image that is provided as conditioning into epsilon prediction", "tokens": [50364, 492, 362, 341, 7103, 8657, 13, 639, 307, 257, 5658, 17630, 3209, 300, 2516, 7593, 4717, 1090, 8669, 3256, 11, 1783, 51, 11, 264, 565, 11, 382, 731, 382, 341, 398, 11, 341, 307, 264, 2295, 8669, 3256, 300, 307, 5649, 382, 21901, 666, 17889, 17630, 51164], "temperature": 0.0, "avg_logprob": -0.12475371972108498, "compression_ratio": 1.9375, "no_speech_prob": 0.0026238977443426847}, {"id": 1172, "seek": 1080080, "start": 10816.8, "end": 10825.8, "text": " network, and we train this epsilon prediction network to predict the noise that was used in order to generate diffused high resolution image.", "tokens": [51164, 3209, 11, 293, 321, 3847, 341, 17889, 17630, 3209, 281, 6069, 264, 5658, 300, 390, 1143, 294, 1668, 281, 8460, 7593, 4717, 1090, 8669, 3256, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12475371972108498, "compression_ratio": 1.9375, "no_speech_prob": 0.0026238977443426847}, {"id": 1173, "seek": 1082580, "start": 10825.8, "end": 10843.8, "text": " The authors in this paper proposed to use different norms for minimizing this objective. They introduced L1, L2 norm, and they observed that one can trade quality for diversity by using L1 norm is L2 norm.", "tokens": [50364, 440, 16552, 294, 341, 3035, 10348, 281, 764, 819, 24357, 337, 46608, 341, 10024, 13, 814, 7268, 441, 16, 11, 441, 17, 2026, 11, 293, 436, 13095, 300, 472, 393, 4923, 3125, 337, 8811, 538, 1228, 441, 16, 2026, 307, 441, 17, 2026, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14668241300080953, "compression_ratio": 1.5458715596330275, "no_speech_prob": 0.011813494376838207}, {"id": 1174, "seek": 1082580, "start": 10843.8, "end": 10851.8, "text": " Since we are training a conditional model now you have in mind that we need to modify the unit that is used for epsilon prediction.", "tokens": [51264, 4162, 321, 366, 3097, 257, 27708, 2316, 586, 291, 362, 294, 1575, 300, 321, 643, 281, 16927, 264, 4985, 300, 307, 1143, 337, 17889, 17630, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14668241300080953, "compression_ratio": 1.5458715596330275, "no_speech_prob": 0.011813494376838207}, {"id": 1175, "seek": 1085180, "start": 10851.8, "end": 10862.8, "text": " For the input of the unit, we will have access to diffused high resolution image, as well as this low resolution conditioning input.", "tokens": [50364, 1171, 264, 4846, 295, 264, 4985, 11, 321, 486, 362, 2105, 281, 7593, 4717, 1090, 8669, 3256, 11, 382, 731, 382, 341, 2295, 8669, 21901, 4846, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15486150764557252, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.007514516822993755}, {"id": 1176, "seek": 1085180, "start": 10862.8, "end": 10880.8, "text": " Since these two images don't have the same special dimensions, the author proposed to use just simple image resizing algorithms to up sample the input low resolution image and concatenate it with diffused high resolution image and the channel dimension, and they provide", "tokens": [50914, 4162, 613, 732, 5267, 500, 380, 362, 264, 912, 2121, 12819, 11, 264, 3793, 10348, 281, 764, 445, 2199, 3256, 725, 3319, 14642, 281, 493, 6889, 264, 4846, 2295, 8669, 3256, 293, 1588, 7186, 473, 309, 365, 7593, 4717, 1090, 8669, 3256, 293, 264, 2269, 10139, 11, 293, 436, 2893, 51814], "temperature": 0.0, "avg_logprob": -0.15486150764557252, "compression_ratio": 1.8401826484018264, "no_speech_prob": 0.007514516822993755}, {"id": 1177, "seek": 1088080, "start": 10880.8, "end": 10893.8, "text": " to the unit diffusion model or the epsilon prediction model and the network is trained to predict the noise that was used when generating the high resolution image.", "tokens": [50364, 281, 264, 4985, 25242, 2316, 420, 264, 17889, 17630, 2316, 293, 264, 3209, 307, 8895, 281, 6069, 264, 5658, 300, 390, 1143, 562, 17746, 264, 1090, 8669, 3256, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2516736695260713, "compression_ratio": 1.490909090909091, "no_speech_prob": 0.005581103265285492}, {"id": 1178, "seek": 1089380, "start": 10893.8, "end": 10910.8, "text": " This method achieves very high quality results for super resolution. For example, here you can see super resolution results with 64 by 64 pixel input when the output is 256 by 256.", "tokens": [50364, 639, 3170, 3538, 977, 588, 1090, 3125, 3542, 337, 1687, 8669, 13, 1171, 1365, 11, 510, 291, 393, 536, 1687, 8669, 3542, 365, 12145, 538, 12145, 19261, 4846, 562, 264, 5598, 307, 38882, 538, 38882, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1704202651977539, "compression_ratio": 1.3636363636363635, "no_speech_prob": 0.033387985080480576}, {"id": 1179, "seek": 1091080, "start": 10910.8, "end": 10928.8, "text": " And you can see that this method here shown in this column achieves a really high quality result compared to, for example, regression models or just simple image resizing algorithms or using by cubic interpolation.", "tokens": [50364, 400, 291, 393, 536, 300, 341, 3170, 510, 4898, 294, 341, 7738, 3538, 977, 257, 534, 1090, 3125, 1874, 5347, 281, 11, 337, 1365, 11, 24590, 5245, 420, 445, 2199, 3256, 725, 3319, 14642, 420, 1228, 538, 28733, 44902, 399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19580388069152832, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.004954454954713583}, {"id": 1180, "seek": 1091080, "start": 10928.8, "end": 10935.8, "text": " And you can see that this actually does a good job of generating low level details.", "tokens": [51264, 400, 291, 393, 536, 300, 341, 767, 775, 257, 665, 1691, 295, 17746, 2295, 1496, 4365, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19580388069152832, "compression_ratio": 1.568421052631579, "no_speech_prob": 0.004954454954713583}, {"id": 1181, "seek": 1093580, "start": 10935.8, "end": 10948.8, "text": " Another work that I like to mention is called palette, image to mesh diffusion models. This method is also this paper is also proposed by same authors as a previous method.", "tokens": [50364, 3996, 589, 300, 286, 411, 281, 2152, 307, 1219, 15851, 11, 3256, 281, 17407, 25242, 5245, 13, 639, 3170, 307, 611, 341, 3035, 307, 611, 10348, 538, 912, 16552, 382, 257, 3894, 3170, 13, 51014], "temperature": 0.0, "avg_logprob": -0.3547613849378612, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.00595426419749856}, {"id": 1182, "seek": 1093580, "start": 10948.8, "end": 10952.8, "text": " So the author's area at home.", "tokens": [51014, 407, 264, 3793, 311, 1859, 412, 1280, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3547613849378612, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.00595426419749856}, {"id": 1183, "seek": 1093580, "start": 10952.8, "end": 10961.8, "text": " Similar to super resolution, many image to image translation applications can be considered as training a conditional model X given why.", "tokens": [51214, 10905, 281, 1687, 8669, 11, 867, 3256, 281, 3256, 12853, 5821, 393, 312, 4888, 382, 3097, 257, 27708, 2316, 1783, 2212, 983, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3547613849378612, "compression_ratio": 1.6066350710900474, "no_speech_prob": 0.00595426419749856}, {"id": 1184, "seek": 1096180, "start": 10961.8, "end": 10973.8, "text": " This is the input image. For example, if you consider colorization problem X is the output color image and why is the grade level input.", "tokens": [50364, 639, 307, 264, 4846, 3256, 13, 1171, 1365, 11, 498, 291, 1949, 2017, 2144, 1154, 1783, 307, 264, 5598, 2017, 3256, 293, 983, 307, 264, 7204, 1496, 4846, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2980400432239879, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.004512818064540625}, {"id": 1185, "seek": 1096180, "start": 10973.8, "end": 10984.8, "text": " So similar to the previous part, or previous slide we're going to again use, we're going to again trade and condition diffusion model using this objective.", "tokens": [50964, 407, 2531, 281, 264, 3894, 644, 11, 420, 3894, 4137, 321, 434, 516, 281, 797, 764, 11, 321, 434, 516, 281, 797, 4923, 293, 4188, 25242, 2316, 1228, 341, 10024, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2980400432239879, "compression_ratio": 1.5783783783783785, "no_speech_prob": 0.004512818064540625}, {"id": 1186, "seek": 1098480, "start": 10984.8, "end": 10993.8, "text": " Similarly, we have again expectation over pairs of input conditioning why, for example, why is again great image X is the output color image.", "tokens": [50364, 13157, 11, 321, 362, 797, 14334, 670, 15494, 295, 4846, 21901, 983, 11, 337, 1365, 11, 983, 307, 797, 869, 3256, 1783, 307, 264, 5598, 2017, 3256, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3114718168209761, "compression_ratio": 1.8097345132743363, "no_speech_prob": 0.005370480474084616}, {"id": 1187, "seek": 1098480, "start": 10993.8, "end": 11011.8, "text": " We have expectation over epsilon drawn from standard normal distribution expecting over time, we're training a conditional diffusion model that takes input grade level image for some input conditioning time as well as the diffuse output image that we want to generate", "tokens": [50814, 492, 362, 14334, 670, 17889, 10117, 490, 3832, 2710, 7316, 9650, 670, 565, 11, 321, 434, 3097, 257, 27708, 25242, 2316, 300, 2516, 4846, 7204, 1496, 3256, 337, 512, 4846, 21901, 565, 382, 731, 382, 264, 42165, 5598, 3256, 300, 321, 528, 281, 8460, 51714], "temperature": 0.0, "avg_logprob": -0.3114718168209761, "compression_ratio": 1.8097345132743363, "no_speech_prob": 0.005370480474084616}, {"id": 1188, "seek": 1101180, "start": 11011.8, "end": 11017.8, "text": " and then the model is trained to predict the noise that was used to generate diffuse samples.", "tokens": [50364, 293, 550, 264, 2316, 307, 8895, 281, 6069, 264, 5658, 300, 390, 1143, 281, 8460, 42165, 10938, 13, 50664], "temperature": 0.0, "avg_logprob": -0.26765591448003595, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.003844100283458829}, {"id": 1189, "seek": 1101180, "start": 11017.8, "end": 11038.8, "text": " Similar to previous part, again, we need to give a pair input to the unit model. And here, because for example, if we're attacking the color problem, we're going to have this grade level image and diffuse color image as input to unit and similarly it's trained to predict the noise that was trained for", "tokens": [50664, 10905, 281, 3894, 644, 11, 797, 11, 321, 643, 281, 976, 257, 6119, 4846, 281, 264, 4985, 2316, 13, 400, 510, 11, 570, 337, 1365, 11, 498, 321, 434, 15010, 264, 2017, 1154, 11, 321, 434, 516, 281, 362, 341, 7204, 1496, 3256, 293, 42165, 2017, 3256, 382, 4846, 281, 4985, 293, 14138, 309, 311, 8895, 281, 6069, 264, 5658, 300, 390, 8895, 337, 51714], "temperature": 0.0, "avg_logprob": -0.26765591448003595, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.003844100283458829}, {"id": 1190, "seek": 1103880, "start": 11038.8, "end": 11042.8, "text": " generating diffuse sample.", "tokens": [50364, 17746, 42165, 6889, 13, 50564], "temperature": 0.0, "avg_logprob": -0.39854016985212054, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.004734791815280914}, {"id": 1191, "seek": 1103880, "start": 11042.8, "end": 11065.8, "text": " The others tried their image to mist translation image to mist diffusion model on four different tasks, including colorization in painting jpeg restoration and uncropping, which is basically given this image, they wanted to extend the image and provide the copper part of this, what they called uncropping.", "tokens": [50564, 440, 2357, 3031, 641, 3256, 281, 3544, 12853, 3256, 281, 3544, 25242, 2316, 322, 1451, 819, 9608, 11, 3009, 2017, 2144, 294, 5370, 361, 494, 70, 23722, 293, 6219, 340, 3759, 11, 597, 307, 1936, 2212, 341, 3256, 11, 436, 1415, 281, 10101, 264, 3256, 293, 2893, 264, 15007, 644, 295, 341, 11, 437, 436, 1219, 6219, 340, 3759, 13, 51714], "temperature": 0.0, "avg_logprob": -0.39854016985212054, "compression_ratio": 1.6818181818181819, "no_speech_prob": 0.004734791815280914}, {"id": 1192, "seek": 1106580, "start": 11065.8, "end": 11071.8, "text": " This paper shows that actually diffusion walls can achieve very good results on these four tasks.", "tokens": [50364, 639, 3035, 3110, 300, 767, 25242, 7920, 393, 4584, 588, 665, 3542, 322, 613, 1451, 9608, 13, 50664], "temperature": 0.0, "avg_logprob": -0.173804995039819, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0010600771056488156}, {"id": 1193, "seek": 1106580, "start": 11071.8, "end": 11088.8, "text": " We should have in mind that this problem this particular paper assumes that you have access to pairs of input and output data so they're they're training a conditional model assuming that they have input image and the output image that we want to generate.", "tokens": [50664, 492, 820, 362, 294, 1575, 300, 341, 1154, 341, 1729, 3035, 37808, 300, 291, 362, 2105, 281, 15494, 295, 4846, 293, 5598, 1412, 370, 436, 434, 436, 434, 3097, 257, 27708, 2316, 11926, 300, 436, 362, 4846, 3256, 293, 264, 5598, 3256, 300, 321, 528, 281, 8460, 13, 51514], "temperature": 0.0, "avg_logprob": -0.173804995039819, "compression_ratio": 1.6857142857142857, "no_speech_prob": 0.0010600771056488156}, {"id": 1194, "seek": 1108880, "start": 11088.8, "end": 11100.8, "text": " If we don't make that assumption, what we can do we can potentially take an unconditional model that is trained on that for example natural images and we can modify it for a particular task.", "tokens": [50364, 759, 321, 500, 380, 652, 300, 15302, 11, 437, 321, 393, 360, 321, 393, 7263, 747, 364, 47916, 2316, 300, 307, 8895, 322, 300, 337, 1365, 3303, 5267, 293, 321, 393, 16927, 309, 337, 257, 1729, 5633, 13, 50964], "temperature": 0.0, "avg_logprob": -0.23172291649712456, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.005371272563934326}, {"id": 1195, "seek": 1108880, "start": 11100.8, "end": 11115.8, "text": " So, as example of that approach, I want to mention I'd like to mention this paper called iterative latent variable refinement or by LBR for short, that was proposed by Joey et al at ICCB 2021.", "tokens": [50964, 407, 11, 382, 1365, 295, 300, 3109, 11, 286, 528, 281, 2152, 286, 1116, 411, 281, 2152, 341, 3035, 1219, 17138, 1166, 48994, 7006, 1895, 30229, 420, 538, 441, 11609, 337, 2099, 11, 300, 390, 10348, 538, 23764, 1030, 419, 412, 286, 11717, 33, 7201, 13, 51714], "temperature": 0.0, "avg_logprob": -0.23172291649712456, "compression_ratio": 1.5826446280991735, "no_speech_prob": 0.005371272563934326}, {"id": 1196, "seek": 1111580, "start": 11115.8, "end": 11131.8, "text": " This paper proposes an approach where, given a reference image, the authors like to modify the generative process of the fusion model, such that the output of the diffusion model can be similar to the reference image.", "tokens": [50364, 639, 3035, 2365, 4201, 364, 3109, 689, 11, 2212, 257, 6408, 3256, 11, 264, 16552, 411, 281, 16927, 264, 1337, 1166, 1399, 295, 264, 23100, 2316, 11, 1270, 300, 264, 5598, 295, 264, 25242, 2316, 393, 312, 2531, 281, 264, 6408, 3256, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14126575754043905, "compression_ratio": 1.5611510791366907, "no_speech_prob": 0.0021393727511167526}, {"id": 1197, "seek": 1113180, "start": 11131.8, "end": 11146.8, "text": " Again, we have a reference image and we want to modify the reverse generative SDEs or the reverse generative diffusion process, such that we can generate images that correspond to a reference input image.", "tokens": [50364, 3764, 11, 321, 362, 257, 6408, 3256, 293, 321, 528, 281, 16927, 264, 9943, 1337, 1166, 14638, 20442, 420, 264, 9943, 1337, 1166, 25242, 1399, 11, 1270, 300, 321, 393, 8460, 5267, 300, 6805, 281, 257, 6408, 4846, 3256, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21453972296281296, "compression_ratio": 1.5572519083969465, "no_speech_prob": 0.0032490098383277655}, {"id": 1198, "seek": 1114680, "start": 11146.8, "end": 11162.8, "text": " So, and the authors and the speaker proposes to do this through using some unconditional model that is not trained for this specific task, it's just the unconditional model is trained to generate realistic for example faces on this slide.", "tokens": [50364, 407, 11, 293, 264, 16552, 293, 264, 8145, 2365, 4201, 281, 360, 341, 807, 1228, 512, 47916, 2316, 300, 307, 406, 8895, 337, 341, 2685, 5633, 11, 309, 311, 445, 264, 47916, 2316, 307, 8895, 281, 8460, 12465, 337, 1365, 8475, 322, 341, 4137, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20991967648875956, "compression_ratio": 1.576158940397351, "no_speech_prob": 0.005792043171823025}, {"id": 1199, "seek": 1116280, "start": 11162.8, "end": 11173.8, "text": " So, so the basic idea is to modify the reverse denoting process, such that we can pull the samples towards the reference image.", "tokens": [50364, 407, 11, 370, 264, 3875, 1558, 307, 281, 16927, 264, 9943, 1441, 17001, 1399, 11, 1270, 300, 321, 393, 2235, 264, 10938, 3030, 264, 6408, 3256, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1939270111822313, "compression_ratio": 1.2574257425742574, "no_speech_prob": 0.0049550095573067665}, {"id": 1200, "seek": 1117380, "start": 11173.8, "end": 11193.8, "text": " So here you have the algorithm proposed in this paper, the algorithm starts from capital T goes to one so this is the reverse denoting process at every step we draw a random nodes vector vector from standard normal distribution, we sample from the reverse denoting distribution", "tokens": [50364, 407, 510, 291, 362, 264, 9284, 10348, 294, 341, 3035, 11, 264, 9284, 3719, 490, 4238, 314, 1709, 281, 472, 370, 341, 307, 264, 9943, 1441, 17001, 1399, 412, 633, 1823, 321, 2642, 257, 4974, 13891, 8062, 8062, 490, 3832, 2710, 7316, 11, 321, 6889, 490, 264, 9943, 1441, 17001, 7316, 51364], "temperature": 0.0, "avg_logprob": -0.2315871845592152, "compression_ratio": 1.7098765432098766, "no_speech_prob": 0.01345265656709671}, {"id": 1201, "seek": 1119380, "start": 11193.8, "end": 11203.8, "text": " to generate this proposal of this color x prime t minus one is the proposed denotes sample to run from the denoting model.", "tokens": [50364, 281, 8460, 341, 11494, 295, 341, 2017, 2031, 5835, 256, 3175, 472, 307, 264, 10348, 1441, 17251, 6889, 281, 1190, 490, 264, 1441, 17001, 2316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.25476109142034825, "compression_ratio": 1.8579545454545454, "no_speech_prob": 0.0060811699368059635}, {"id": 1202, "seek": 1119380, "start": 11203.8, "end": 11219.8, "text": " Why here represent the reference image so we're going to use the forward diffusion kernel to generate the diffused version of reference image so we're going to go forward in time for this reference image.", "tokens": [50864, 1545, 510, 2906, 264, 6408, 3256, 370, 321, 434, 516, 281, 764, 264, 2128, 25242, 28256, 281, 8460, 264, 7593, 4717, 3037, 295, 6408, 3256, 370, 321, 434, 516, 281, 352, 2128, 294, 565, 337, 341, 6408, 3256, 13, 51664], "temperature": 0.0, "avg_logprob": -0.25476109142034825, "compression_ratio": 1.8579545454545454, "no_speech_prob": 0.0060811699368059635}, {"id": 1203, "seek": 1121980, "start": 11220.8, "end": 11230.8, "text": " So I did what we want to do we want to make this proposed denotes image x prime t minus one to be more similar to whitey minus one.", "tokens": [50414, 407, 286, 630, 437, 321, 528, 281, 360, 321, 528, 281, 652, 341, 10348, 1441, 17251, 3256, 2031, 5835, 256, 3175, 472, 281, 312, 544, 2531, 281, 2418, 88, 3175, 472, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3067228672868114, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.016063012182712555}, {"id": 1204, "seek": 1121980, "start": 11230.8, "end": 11241.8, "text": " So to do so that this paper proposes this simple operation here fine and represents some low pass filter.", "tokens": [50914, 407, 281, 360, 370, 300, 341, 3035, 2365, 4201, 341, 2199, 6916, 510, 2489, 293, 8855, 512, 2295, 1320, 6608, 13, 51464], "temperature": 0.0, "avg_logprob": -0.3067228672868114, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.016063012182712555}, {"id": 1205, "seek": 1124180, "start": 11241.8, "end": 11260.8, "text": " So this operation is very simple, we have this proposed denotes image x prime t minus one, we subtract the low pass filter applied to this x t minus one, and we add the back low pass filter output of whitey minus one.", "tokens": [50364, 407, 341, 6916, 307, 588, 2199, 11, 321, 362, 341, 10348, 1441, 17251, 3256, 2031, 5835, 256, 3175, 472, 11, 321, 16390, 264, 2295, 1320, 6608, 6456, 281, 341, 2031, 256, 3175, 472, 11, 293, 321, 909, 264, 646, 2295, 1320, 6608, 5598, 295, 2418, 88, 3175, 472, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20234451653822413, "compression_ratio": 1.583941605839416, "no_speech_prob": 0.0019514976302161813}, {"id": 1206, "seek": 1126080, "start": 11260.8, "end": 11273.8, "text": " So you can think of this operation as operation that takes x t minus one, the x prime t minus one, this is the proposed denotes image, it removes its low pass filter low frequency content.", "tokens": [50364, 407, 291, 393, 519, 295, 341, 6916, 382, 6916, 300, 2516, 2031, 256, 3175, 472, 11, 264, 2031, 5835, 256, 3175, 472, 11, 341, 307, 264, 10348, 1441, 17251, 3256, 11, 309, 30445, 1080, 2295, 1320, 6608, 2295, 7893, 2701, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1994186425820375, "compression_ratio": 1.952662721893491, "no_speech_prob": 0.0054293605498969555}, {"id": 1207, "seek": 1126080, "start": 11273.8, "end": 11282.8, "text": " So, here we are removing the low frequency content of x prime t minus one, and the adding back the low frequency content of whitey minus one.", "tokens": [51014, 407, 11, 510, 321, 366, 12720, 264, 2295, 7893, 2701, 295, 2031, 5835, 256, 3175, 472, 11, 293, 264, 5127, 646, 264, 2295, 7893, 2701, 295, 2418, 88, 3175, 472, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1994186425820375, "compression_ratio": 1.952662721893491, "no_speech_prob": 0.0054293605498969555}, {"id": 1208, "seek": 1128280, "start": 11282.8, "end": 11294.8, "text": " So basically we're putting the low frequency content of whitey minus one, this is the reference image into x prime, into x prime denotes the proposal sample.", "tokens": [50364, 407, 1936, 321, 434, 3372, 264, 2295, 7893, 2701, 295, 2418, 88, 3175, 472, 11, 341, 307, 264, 6408, 3256, 666, 2031, 5835, 11, 666, 2031, 5835, 1441, 17251, 264, 11494, 6889, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2205830074491955, "compression_ratio": 1.9170731707317072, "no_speech_prob": 0.0046378313563764095}, {"id": 1209, "seek": 1128280, "start": 11294.8, "end": 11307.8, "text": " What we are doing we're basically making sure that in the reverse process, we're generating a sample where the low frequency content is similar to the reference image so the most of the structure is very similar to the reference image.", "tokens": [50964, 708, 321, 366, 884, 321, 434, 1936, 1455, 988, 300, 294, 264, 9943, 1399, 11, 321, 434, 17746, 257, 6889, 689, 264, 2295, 7893, 2701, 307, 2531, 281, 264, 6408, 3256, 370, 264, 881, 295, 264, 3877, 307, 588, 2531, 281, 264, 6408, 3256, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2205830074491955, "compression_ratio": 1.9170731707317072, "no_speech_prob": 0.0046378313563764095}, {"id": 1210, "seek": 1130780, "start": 11307.8, "end": 11325.8, "text": " So, as I mentioned, fine and it's just a low pass filter. And in order to implement this the artist proposed to use simply done sampling up sampling operation where n represents the, the, the, the down sampling ratio used for in this operation for something", "tokens": [50364, 407, 11, 382, 286, 2835, 11, 2489, 293, 309, 311, 445, 257, 2295, 1320, 6608, 13, 400, 294, 1668, 281, 4445, 341, 264, 5748, 10348, 281, 764, 2935, 1096, 21179, 493, 21179, 6916, 689, 297, 8855, 264, 11, 264, 11, 264, 11, 264, 760, 21179, 8509, 1143, 337, 294, 341, 6916, 337, 746, 51264], "temperature": 0.0, "avg_logprob": -0.31334234538831207, "compression_ratio": 1.6474358974358974, "no_speech_prob": 0.007417246233671904}, {"id": 1211, "seek": 1132580, "start": 11325.8, "end": 11336.8, "text": " n is equal to two, it means we're going to just take reference image or take this input down sample by factor of two and then up sample again by factor of two.", "tokens": [50364, 297, 307, 2681, 281, 732, 11, 309, 1355, 321, 434, 516, 281, 445, 747, 6408, 3256, 420, 747, 341, 4846, 760, 6889, 538, 5952, 295, 732, 293, 550, 493, 6889, 797, 538, 5952, 295, 732, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2629035370690482, "compression_ratio": 1.5, "no_speech_prob": 0.00290044117718935}, {"id": 1212, "seek": 1132580, "start": 11336.8, "end": 11342.8, "text": " So, which is which corresponds to to a low pass filter operation.", "tokens": [50914, 407, 11, 597, 307, 597, 23249, 281, 281, 257, 2295, 1320, 6608, 6916, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2629035370690482, "compression_ratio": 1.5, "no_speech_prob": 0.00290044117718935}, {"id": 1213, "seek": 1134280, "start": 11342.8, "end": 11359.8, "text": " Here you can see this reference, these two reference images, and how we can generate images with different values for n. So when n equals to four, it means that we actually take this during the generation we don't sample that for low pass filter we don't sample the", "tokens": [50364, 1692, 291, 393, 536, 341, 6408, 11, 613, 732, 6408, 5267, 11, 293, 577, 321, 393, 8460, 5267, 365, 819, 4190, 337, 297, 13, 407, 562, 297, 6915, 281, 1451, 11, 309, 1355, 300, 321, 767, 747, 341, 1830, 264, 5125, 321, 500, 380, 6889, 300, 337, 2295, 1320, 6608, 321, 500, 380, 6889, 264, 51214], "temperature": 0.0, "avg_logprob": -0.24708649263543597, "compression_ratio": 1.6358024691358024, "no_speech_prob": 0.005658808164298534}, {"id": 1214, "seek": 1135980, "start": 11359.8, "end": 11370.8, "text": " sample images by factor of four, by factor of four. Since this factor is a small, this means that most of the structure in the generation will be similar to reference image.", "tokens": [50364, 6889, 5267, 538, 5952, 295, 1451, 11, 538, 5952, 295, 1451, 13, 4162, 341, 5952, 307, 257, 1359, 11, 341, 1355, 300, 881, 295, 264, 3877, 294, 264, 5125, 486, 312, 2531, 281, 6408, 3256, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21955341297191577, "compression_ratio": 1.894273127753304, "no_speech_prob": 0.002826013835147023}, {"id": 1215, "seek": 1135980, "start": 11370.8, "end": 11385.8, "text": " So you can see that in fact we're generating an image that is very similar to reference image. And as we increase this and we can see that now different levels of details can, can be generated through the diffusion model and the more global characteristics", "tokens": [50914, 407, 291, 393, 536, 300, 294, 1186, 321, 434, 17746, 364, 3256, 300, 307, 588, 2531, 281, 6408, 3256, 13, 400, 382, 321, 3488, 341, 293, 321, 393, 536, 300, 586, 819, 4358, 295, 4365, 393, 11, 393, 312, 10833, 807, 264, 25242, 2316, 293, 264, 544, 4338, 10891, 51664], "temperature": 0.0, "avg_logprob": -0.21955341297191577, "compression_ratio": 1.894273127753304, "no_speech_prob": 0.002826013835147023}, {"id": 1216, "seek": 1138580, "start": 11385.8, "end": 11395.8, "text": " like more global arrangements or low pass, low frequency content of the image is still remains the same as the reference image.", "tokens": [50364, 411, 544, 4338, 22435, 420, 2295, 1320, 11, 2295, 7893, 2701, 295, 264, 3256, 307, 920, 7023, 264, 912, 382, 264, 6408, 3256, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2246479004148453, "compression_ratio": 1.6368421052631579, "no_speech_prob": 0.006316181737929583}, {"id": 1217, "seek": 1138580, "start": 11395.8, "end": 11404.8, "text": " And now to show that actually you can do this for different tasks image translation given a portrait image they can generate a realistic image that corresponded to the portrait image.", "tokens": [50864, 400, 586, 281, 855, 300, 767, 291, 393, 360, 341, 337, 819, 9608, 3256, 12853, 2212, 257, 17126, 3256, 436, 393, 8460, 257, 12465, 3256, 300, 6805, 292, 281, 264, 17126, 3256, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2246479004148453, "compression_ratio": 1.6368421052631579, "no_speech_prob": 0.006316181737929583}, {"id": 1218, "seek": 1140480, "start": 11404.8, "end": 11419.8, "text": " So they can do paint to image so they can take all painting and generate realistic image, and they can do some simple editing, and for something can add back this watermark into.", "tokens": [50364, 407, 436, 393, 360, 4225, 281, 3256, 370, 436, 393, 747, 439, 5370, 293, 8460, 12465, 3256, 11, 293, 436, 393, 360, 512, 2199, 10000, 11, 293, 337, 746, 393, 909, 646, 341, 1281, 5638, 666, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2775657932932784, "compression_ratio": 1.5344827586206897, "no_speech_prob": 0.0077712684869766235}, {"id": 1219, "seek": 1141980, "start": 11419.8, "end": 11431.8, "text": " In this part I'd like to talk about how we can take representation learned through diffusion models and use them for sometimes through applications such as semantic segmentation.", "tokens": [50364, 682, 341, 644, 286, 1116, 411, 281, 751, 466, 577, 321, 393, 747, 10290, 3264, 807, 25242, 5245, 293, 764, 552, 337, 2171, 807, 5821, 1270, 382, 47982, 9469, 399, 13, 50964], "temperature": 0.0, "avg_logprob": -0.22925746974660388, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.006981594953685999}, {"id": 1220, "seek": 1141980, "start": 11431.8, "end": 11443.8, "text": " I'm talking about a specific paper called label efficient semantic segmentation with diffusion models that was proposed by one joke at all at ICLAR 2022.", "tokens": [50964, 286, 478, 1417, 466, 257, 2685, 3035, 1219, 7645, 7148, 47982, 9469, 399, 365, 25242, 5245, 300, 390, 10348, 538, 472, 7647, 412, 439, 412, 14360, 43, 1899, 20229, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22925746974660388, "compression_ratio": 1.6116504854368932, "no_speech_prob": 0.006981594953685999}, {"id": 1221, "seek": 1144380, "start": 11443.8, "end": 11462.8, "text": " So it's pretty a paper, propose a simple approach for using the representation trained in diffusion model for semantic segmentation. The others proposed to take input image and diffuse it by adding by following the forward diffusion process, and they only go to small steps", "tokens": [50364, 407, 309, 311, 1238, 257, 3035, 11, 17421, 257, 2199, 3109, 337, 1228, 264, 10290, 8895, 294, 25242, 2316, 337, 47982, 9469, 399, 13, 440, 2357, 10348, 281, 747, 4846, 3256, 293, 42165, 309, 538, 5127, 538, 3480, 264, 2128, 25242, 1399, 11, 293, 436, 787, 352, 281, 1359, 4439, 51314], "temperature": 0.0, "avg_logprob": -0.29950654065167465, "compression_ratio": 1.5780346820809248, "no_speech_prob": 0.006661790423095226}, {"id": 1222, "seek": 1146280, "start": 11462.8, "end": 11475.8, "text": " by following the forward diffusion step which corresponds to adding just a bit of noise into input image, then they pass this diffuse image to the denoising diffusion model the unit model the epsilon prediction model.", "tokens": [50364, 538, 3480, 264, 2128, 25242, 1823, 597, 23249, 281, 5127, 445, 257, 857, 295, 5658, 666, 4846, 3256, 11, 550, 436, 1320, 341, 42165, 3256, 281, 264, 1441, 78, 3436, 25242, 2316, 264, 4985, 2316, 264, 17889, 17630, 2316, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2435758590698242, "compression_ratio": 1.8010204081632653, "no_speech_prob": 0.024295000359416008}, {"id": 1223, "seek": 1146280, "start": 11475.8, "end": 11487.8, "text": " And they, they extract representation form, the internal representation form in this unit at different resolutions of the unit decoder.", "tokens": [51014, 400, 436, 11, 436, 8947, 10290, 1254, 11, 264, 6920, 10290, 1254, 294, 341, 4985, 412, 819, 32179, 295, 264, 4985, 979, 19866, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2435758590698242, "compression_ratio": 1.8010204081632653, "no_speech_prob": 0.024295000359416008}, {"id": 1224, "seek": 1148780, "start": 11487.8, "end": 11501.8, "text": " So given these representations the up sample, all these intermediate representation, so that they have the same dimensional spatial dimensionality as input in so we have these up sampling layers.", "tokens": [50364, 407, 2212, 613, 33358, 264, 493, 6889, 11, 439, 613, 19376, 10290, 11, 370, 300, 436, 362, 264, 912, 18795, 23598, 10139, 1860, 382, 4846, 294, 370, 321, 362, 613, 493, 21179, 7914, 13, 51064], "temperature": 0.0, "avg_logprob": -0.25056342645124957, "compression_ratio": 2.0184331797235022, "no_speech_prob": 0.010957983322441578}, {"id": 1225, "seek": 1148780, "start": 11501.8, "end": 11516.8, "text": " We have these feature maps that have the same dimensionality as the input image, and now they simply concatenate all these intermediate feature maps, and they pass them to one by one convolutions that would do semantic segmentation per pixel.", "tokens": [51064, 492, 362, 613, 4111, 11317, 300, 362, 264, 912, 10139, 1860, 382, 264, 4846, 3256, 11, 293, 586, 436, 2935, 1588, 7186, 473, 439, 613, 19376, 4111, 11317, 11, 293, 436, 1320, 552, 281, 472, 538, 472, 3754, 15892, 300, 576, 360, 47982, 9469, 399, 680, 19261, 13, 51814], "temperature": 0.0, "avg_logprob": -0.25056342645124957, "compression_ratio": 2.0184331797235022, "no_speech_prob": 0.010957983322441578}, {"id": 1226, "seek": 1151680, "start": 11516.8, "end": 11527.8, "text": " So you can think of these as a pixel classifiers that just classify each pixel for each semantic object goals or semantic goals.", "tokens": [50364, 407, 291, 393, 519, 295, 613, 382, 257, 19261, 1508, 23463, 300, 445, 33872, 1184, 19261, 337, 1184, 47982, 2657, 5493, 420, 47982, 5493, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21580358411444991, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015804172726348042}, {"id": 1227, "seek": 1151680, "start": 11527.8, "end": 11535.8, "text": " So, in order to train this model, that was supposed to use a pre-trained diffusion model and they're only training this component here.", "tokens": [50914, 407, 11, 294, 1668, 281, 3847, 341, 2316, 11, 300, 390, 3442, 281, 764, 257, 659, 12, 17227, 2001, 25242, 2316, 293, 436, 434, 787, 3097, 341, 6542, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.21580358411444991, "compression_ratio": 1.5903614457831325, "no_speech_prob": 0.0015804172726348042}, {"id": 1228, "seek": 1153580, "start": 11535.8, "end": 11548.8, "text": " Up sampling component doesn't have usually any training parameters, but most of the parameters, the additional parameters are basically here in these one by one convolutional networks.", "tokens": [50364, 5858, 21179, 6542, 1177, 380, 362, 2673, 604, 3097, 9834, 11, 457, 881, 295, 264, 9834, 11, 264, 4497, 9834, 366, 1936, 510, 294, 613, 472, 538, 472, 45216, 304, 9590, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18032382191091345, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.007184326648712158}, {"id": 1229, "seek": 1153580, "start": 11548.8, "end": 11560.8, "text": " This paper particularly shows that this approach is labeled efficient using very few labeled instances they can train diffusion models on several datasets as you can see on this slide.", "tokens": [51014, 639, 3035, 4098, 3110, 300, 341, 3109, 307, 21335, 7148, 1228, 588, 1326, 21335, 14519, 436, 393, 3847, 25242, 5245, 322, 2940, 42856, 382, 291, 393, 536, 322, 341, 4137, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18032382191091345, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.007184326648712158}, {"id": 1230, "seek": 1156080, "start": 11560.8, "end": 11574.8, "text": " And the other show that actually diffusion based segmentation models cannot perform mass encoders, can or VAE based models on this test, which is very interesting.", "tokens": [50364, 400, 264, 661, 855, 300, 767, 25242, 2361, 9469, 399, 5245, 2644, 2042, 2758, 2058, 378, 433, 11, 393, 420, 18527, 36, 2361, 5245, 322, 341, 1500, 11, 597, 307, 588, 1880, 13, 51064], "temperature": 0.0, "avg_logprob": -0.27879395978204136, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.003010521875694394}, {"id": 1231, "seek": 1156080, "start": 11574.8, "end": 11583.8, "text": " It shows that actually representation learning diffusion models can be used for downstream applications such as segmentation.", "tokens": [51064, 467, 3110, 300, 767, 10290, 2539, 25242, 5245, 393, 312, 1143, 337, 30621, 5821, 1270, 382, 9469, 399, 13, 51514], "temperature": 0.0, "avg_logprob": -0.27879395978204136, "compression_ratio": 1.6514285714285715, "no_speech_prob": 0.003010521875694394}, {"id": 1232, "seek": 1158380, "start": 11583.8, "end": 11591.8, "text": " In this part, I like to talk about the particular paper parameter that is proposed for image editing. It's called SDE edit.", "tokens": [50364, 682, 341, 644, 11, 286, 411, 281, 751, 466, 264, 1729, 3035, 13075, 300, 307, 10348, 337, 3256, 10000, 13, 467, 311, 1219, 14638, 36, 8129, 13, 50764], "temperature": 0.0, "avg_logprob": -0.31190569647427263, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.005438771564513445}, {"id": 1233, "seek": 1158380, "start": 11591.8, "end": 11602.8, "text": " This paper was proposed by Ming Itala at Stanford University, and it was proposed, it was presented at ICLR 2022.", "tokens": [50764, 639, 3035, 390, 10348, 538, 19352, 467, 5159, 412, 20374, 3535, 11, 293, 309, 390, 10348, 11, 309, 390, 8212, 412, 14360, 31722, 20229, 13, 51314], "temperature": 0.0, "avg_logprob": -0.31190569647427263, "compression_ratio": 1.4539877300613497, "no_speech_prob": 0.005438771564513445}, {"id": 1234, "seek": 1160280, "start": 11602.8, "end": 11614.8, "text": " What this paper is trying to tackle is that, given this stroke painting, the authors propose a simple approach to generate realistic images that corresponds to that stroke painting.", "tokens": [50364, 708, 341, 3035, 307, 1382, 281, 14896, 307, 300, 11, 2212, 341, 12403, 5370, 11, 264, 16552, 17421, 257, 2199, 3109, 281, 8460, 12465, 5267, 300, 23249, 281, 300, 12403, 5370, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1625690171212861, "compression_ratio": 1.8722222222222222, "no_speech_prob": 0.0037036901339888573}, {"id": 1235, "seek": 1160280, "start": 11614.8, "end": 11624.8, "text": " The main intuition or the main idea in this paper is that the distribution of real images and stroke painting images, these are two different distribution.", "tokens": [50964, 440, 2135, 24002, 420, 264, 2135, 1558, 294, 341, 3035, 307, 300, 264, 7316, 295, 957, 5267, 293, 12403, 5370, 5267, 11, 613, 366, 732, 819, 7316, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1625690171212861, "compression_ratio": 1.8722222222222222, "no_speech_prob": 0.0037036901339888573}, {"id": 1236, "seek": 1162480, "start": 11624.8, "end": 11635.8, "text": " If you have some smashes, they're not the same and in the, in the data space, they, they are not completely overlapping each other because of these differences.", "tokens": [50364, 759, 291, 362, 512, 899, 12808, 11, 436, 434, 406, 264, 912, 293, 294, 264, 11, 294, 264, 1412, 1901, 11, 436, 11, 436, 366, 406, 2584, 33535, 1184, 661, 570, 295, 613, 7300, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2546172871309168, "compression_ratio": 1.9678899082568808, "no_speech_prob": 0.014047547243535519}, {"id": 1237, "seek": 1162480, "start": 11635.8, "end": 11650.8, "text": " But if you follow the forward diffusion process, if we have this distribution realistic images distribution stroke painted, if we follow the forward diffusion process these two distribution will stop having overlaps with each other because of the definition of forward", "tokens": [50914, 583, 498, 291, 1524, 264, 2128, 25242, 1399, 11, 498, 321, 362, 341, 7316, 12465, 5267, 7316, 12403, 11797, 11, 498, 321, 1524, 264, 2128, 25242, 1399, 613, 732, 7316, 486, 1590, 1419, 15986, 2382, 365, 1184, 661, 570, 295, 264, 7123, 295, 2128, 51664], "temperature": 0.0, "avg_logprob": -0.2546172871309168, "compression_ratio": 1.9678899082568808, "no_speech_prob": 0.014047547243535519}, {"id": 1238, "seek": 1165080, "start": 11650.8, "end": 11659.8, "text": " distribution, because we know that actually, if you have two distribution and you diffuse the samples in those two distribution, the distribution will start having overlap.", "tokens": [50364, 7316, 11, 570, 321, 458, 300, 767, 11, 498, 291, 362, 732, 7316, 293, 291, 42165, 264, 10938, 294, 729, 732, 7316, 11, 264, 7316, 486, 722, 1419, 19959, 13, 50814], "temperature": 0.0, "avg_logprob": -0.2623100719232669, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0007546533015556633}, {"id": 1239, "seek": 1165080, "start": 11659.8, "end": 11666.8, "text": " This forward diffusion simply corresponds to adding noise into input stroke painting.", "tokens": [50814, 639, 2128, 25242, 2935, 23249, 281, 5127, 5658, 666, 4846, 12403, 5370, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2623100719232669, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0007546533015556633}, {"id": 1240, "seek": 1165080, "start": 11666.8, "end": 11677.8, "text": " Now that we know these two distribution are overlapping, we can use just a generative model train of real images to solve the reverse SDE, reverse denoising SDE.", "tokens": [51164, 823, 300, 321, 458, 613, 732, 7316, 366, 33535, 11, 321, 393, 764, 445, 257, 1337, 1166, 2316, 3847, 295, 957, 5267, 281, 5039, 264, 9943, 14638, 36, 11, 9943, 1441, 78, 3436, 14638, 36, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2623100719232669, "compression_ratio": 1.794871794871795, "no_speech_prob": 0.0007546533015556633}, {"id": 1241, "seek": 1167780, "start": 11677.8, "end": 11686.8, "text": " That will start from this diffused stroke painting and try to generate a realistic image that corresponds to this noisy input.", "tokens": [50364, 663, 486, 722, 490, 341, 7593, 4717, 12403, 5370, 293, 853, 281, 8460, 257, 12465, 3256, 300, 23249, 281, 341, 24518, 4846, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23558489481608072, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0038739293813705444}, {"id": 1242, "seek": 1167780, "start": 11686.8, "end": 11701.8, "text": " And the authors show that they're actually using a generative model. This is an unconditional model again, train unrealistic images, they can come back to realistic images that where the colors here are very similar to this stroke painting colors.", "tokens": [50814, 400, 264, 16552, 855, 300, 436, 434, 767, 1228, 257, 1337, 1166, 2316, 13, 639, 307, 364, 47916, 2316, 797, 11, 3847, 42867, 5267, 11, 436, 393, 808, 646, 281, 12465, 5267, 300, 689, 264, 4577, 510, 366, 588, 2531, 281, 341, 12403, 5370, 4577, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23558489481608072, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0038739293813705444}, {"id": 1243, "seek": 1170180, "start": 11701.8, "end": 11711.8, "text": " So, this is, I think, a very clever idea to take stroke paintings and generate realistic images that correspond to those stroke paintings.", "tokens": [50364, 407, 11, 341, 307, 11, 286, 519, 11, 257, 588, 13494, 1558, 281, 747, 12403, 14880, 293, 8460, 12465, 5267, 300, 6805, 281, 729, 12403, 14880, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19316523588156398, "compression_ratio": 1.8743961352657006, "no_speech_prob": 0.0012399050174281001}, {"id": 1244, "seek": 1170180, "start": 11711.8, "end": 11727.8, "text": " However, actually, we should have in mind that this train in a conditional setting, however, we should have in mind that this approach mostly relies on the color information in order to take this stroke painting and generate the corresponding image.", "tokens": [50864, 2908, 11, 767, 11, 321, 820, 362, 294, 1575, 300, 341, 3847, 294, 257, 27708, 3287, 11, 4461, 11, 321, 820, 362, 294, 1575, 300, 341, 3109, 5240, 30910, 322, 264, 2017, 1589, 294, 1668, 281, 747, 341, 12403, 5370, 293, 8460, 264, 11760, 3256, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19316523588156398, "compression_ratio": 1.8743961352657006, "no_speech_prob": 0.0012399050174281001}, {"id": 1245, "seek": 1172780, "start": 11727.8, "end": 11736.8, "text": " And this is a bit different than, for example, methods that would use the semantic layout, semantic mask of objects in order to tackle this problem.", "tokens": [50364, 400, 341, 307, 257, 857, 819, 813, 11, 337, 1365, 11, 7150, 300, 576, 764, 264, 47982, 13333, 11, 47982, 6094, 295, 6565, 294, 1668, 281, 14896, 341, 1154, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1581597795673445, "compression_ratio": 1.4539473684210527, "no_speech_prob": 0.0013598606456071138}, {"id": 1246, "seek": 1172780, "start": 11736.8, "end": 11741.8, "text": " So, it has some advantage and disadvantages that we should have in mind.", "tokens": [50814, 407, 11, 309, 575, 512, 5002, 293, 37431, 300, 321, 820, 362, 294, 1575, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1581597795673445, "compression_ratio": 1.4539473684210527, "no_speech_prob": 0.0013598606456071138}, {"id": 1247, "seek": 1174180, "start": 11741.8, "end": 11762.8, "text": " Delta shows very interesting results on different data sets. Here you can see stroke paintings for models train on this on bedroom, this on church and set up a and you can see here in these two row, how a generative model using SDE not can be used to generate realistic images that correspond to stroke.", "tokens": [50364, 18183, 3110, 588, 1880, 3542, 322, 819, 1412, 6352, 13, 1692, 291, 393, 536, 12403, 14880, 337, 5245, 3847, 322, 341, 322, 11211, 11, 341, 322, 4128, 293, 992, 493, 257, 293, 291, 393, 536, 510, 294, 613, 732, 5386, 11, 577, 257, 1337, 1166, 2316, 1228, 14638, 36, 406, 393, 312, 1143, 281, 8460, 12465, 5267, 300, 6805, 281, 12403, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3839737863251657, "compression_ratio": 1.5863874345549738, "no_speech_prob": 0.0025828597135841846}, {"id": 1248, "seek": 1176280, "start": 11763.8, "end": 11778.8, "text": " Lastly, in this part, I'd like to talk about particular work that we did at NVIDIA for adversarial robustness and in this particular work we introduced diffusion models for adversarial clarification.", "tokens": [50414, 18072, 11, 294, 341, 644, 11, 286, 1116, 411, 281, 751, 466, 1729, 589, 300, 321, 630, 412, 426, 3958, 6914, 337, 17641, 44745, 13956, 1287, 293, 294, 341, 1729, 589, 321, 7268, 25242, 5245, 337, 17641, 44745, 34449, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24604981595819647, "compression_ratio": 1.4525547445255473, "no_speech_prob": 0.009533489122986794}, {"id": 1249, "seek": 1177880, "start": 11779.8, "end": 11798.8, "text": " So the basic problem we want to add this is that given an adversially perturbed image, we want to see if we can use diffusion models to remove adversarial perturbations and clean this image such that maybe apply classifier on these adversially perturbed image images, we can actually get robust classification.", "tokens": [50414, 407, 264, 3875, 1154, 321, 528, 281, 909, 341, 307, 300, 2212, 364, 17641, 2270, 13269, 374, 2883, 3256, 11, 321, 528, 281, 536, 498, 321, 393, 764, 25242, 5245, 281, 4159, 17641, 44745, 40468, 763, 293, 2541, 341, 3256, 1270, 300, 1310, 3079, 1508, 9902, 322, 613, 17641, 2270, 13269, 374, 2883, 3256, 5267, 11, 321, 393, 767, 483, 13956, 21538, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2273431322467861, "compression_ratio": 1.7415730337078652, "no_speech_prob": 0.0037604081444442272}, {"id": 1250, "seek": 1179880, "start": 11799.8, "end": 11810.8, "text": " So this, the proposed idea here is similar to SDE edit mostly applied for adversarial clarification, given this adversarial perturbed image.", "tokens": [50414, 407, 341, 11, 264, 10348, 1558, 510, 307, 2531, 281, 14638, 36, 8129, 5240, 6456, 337, 17641, 44745, 34449, 11, 2212, 341, 17641, 44745, 13269, 374, 2883, 3256, 13, 50964], "temperature": 0.0, "avg_logprob": -0.31914549162893585, "compression_ratio": 1.2727272727272727, "no_speech_prob": 0.0038140988908708096}, {"id": 1251, "seek": 1181080, "start": 11810.8, "end": 11823.8, "text": " So what we propose to do is we propose to follow the forward SDE, which correspond to basically adding noise and diffusing the input adversarial perturbed image, using just a forward diffusion cannon.", "tokens": [50364, 407, 437, 321, 17421, 281, 360, 307, 321, 17421, 281, 1524, 264, 2128, 14638, 36, 11, 597, 6805, 281, 1936, 5127, 5658, 293, 7593, 7981, 264, 4846, 17641, 44745, 13269, 374, 2883, 3256, 11, 1228, 445, 257, 2128, 25242, 25938, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2795349884033203, "compression_ratio": 1.7243243243243243, "no_speech_prob": 0.02839210256934166}, {"id": 1252, "seek": 1181080, "start": 11823.8, "end": 11831.8, "text": " So we go to particular times that T star we call, and we, and we simply diffuse the input adversarial perturbed image.", "tokens": [51014, 407, 321, 352, 281, 1729, 1413, 300, 314, 3543, 321, 818, 11, 293, 321, 11, 293, 321, 2935, 42165, 264, 4846, 17641, 44745, 13269, 374, 2883, 3256, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2795349884033203, "compression_ratio": 1.7243243243243243, "no_speech_prob": 0.02839210256934166}, {"id": 1253, "seek": 1183180, "start": 11831.8, "end": 11839.8, "text": " We know that by adding noise, we can now wash out all these adversarial perturbations that are applied into image.", "tokens": [50364, 492, 458, 300, 538, 5127, 5658, 11, 321, 393, 586, 5675, 484, 439, 613, 17641, 44745, 40468, 763, 300, 366, 6456, 666, 3256, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1682261859669405, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.015169879421591759}, {"id": 1254, "seek": 1183180, "start": 11839.8, "end": 11850.8, "text": " Now that we have this noisy image, we use the reverse genitive SDE or reverse noise SDE to start from this noisy input and generate clean image that corresponds to this noisy image.", "tokens": [50764, 823, 300, 321, 362, 341, 24518, 3256, 11, 321, 764, 264, 9943, 1049, 2187, 14638, 36, 420, 9943, 5658, 14638, 36, 281, 722, 490, 341, 24518, 4846, 293, 8460, 2541, 3256, 300, 23249, 281, 341, 24518, 3256, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1682261859669405, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.015169879421591759}, {"id": 1255, "seek": 1183180, "start": 11850.8, "end": 11860.8, "text": " And we know that through this process, we can remove all the noise that was injected as well as all the adversarial perturbations presented here in this image.", "tokens": [51314, 400, 321, 458, 300, 807, 341, 1399, 11, 321, 393, 4159, 439, 264, 5658, 300, 390, 36967, 382, 731, 382, 439, 264, 17641, 44745, 40468, 763, 8212, 510, 294, 341, 3256, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1682261859669405, "compression_ratio": 1.892116182572614, "no_speech_prob": 0.015169879421591759}, {"id": 1256, "seek": 1186080, "start": 11860.8, "end": 11869.8, "text": " So if we have this clean image or purified image, we can just pass it to classifier and hopefully make a robust classification prediction.", "tokens": [50364, 407, 498, 321, 362, 341, 2541, 3256, 420, 1864, 2587, 3256, 11, 321, 393, 445, 1320, 309, 281, 1508, 9902, 293, 4696, 652, 257, 13956, 21538, 17630, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1458801709688627, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.002428318839520216}, {"id": 1257, "seek": 1186080, "start": 11869.8, "end": 11880.8, "text": " Like any adversarial difference mechanism, we need to be able to attack this model, evaluate our performance, we need to be able to attack this model.", "tokens": [50814, 1743, 604, 17641, 44745, 2649, 7513, 11, 321, 643, 281, 312, 1075, 281, 2690, 341, 2316, 11, 13059, 527, 3389, 11, 321, 643, 281, 312, 1075, 281, 2690, 341, 2316, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1458801709688627, "compression_ratio": 1.670520231213873, "no_speech_prob": 0.002428318839520216}, {"id": 1258, "seek": 1188080, "start": 11880.8, "end": 11890.8, "text": " And we also show how we can attack this model by backfogating end-to-end through classifier as well as our purification algorithm.", "tokens": [50364, 400, 321, 611, 855, 577, 321, 393, 2690, 341, 2316, 538, 646, 69, 664, 990, 917, 12, 1353, 12, 521, 807, 1508, 9902, 382, 731, 382, 527, 1864, 3774, 9284, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1820279312133789, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0018633719300851226}, {"id": 1259, "seek": 1188080, "start": 11890.8, "end": 11904.8, "text": " And this involves basically backfogating through this reverse SDE. So we showed in this paper how we can do this and how we can attack this mechanism end-to-end.", "tokens": [50864, 400, 341, 11626, 1936, 646, 69, 664, 990, 807, 341, 9943, 14638, 36, 13, 407, 321, 4712, 294, 341, 3035, 577, 321, 393, 360, 341, 293, 577, 321, 393, 2690, 341, 7513, 917, 12, 1353, 12, 521, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1820279312133789, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.0018633719300851226}, {"id": 1260, "seek": 1190480, "start": 11904.8, "end": 11913.8, "text": " On the left side in this slide, you can see an example of adversarial perturb images and first column on a set of A data set.", "tokens": [50364, 1282, 264, 1411, 1252, 294, 341, 4137, 11, 291, 393, 536, 364, 1365, 295, 17641, 44745, 40468, 5267, 293, 700, 7738, 322, 257, 992, 295, 316, 1412, 992, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1623380766974555, "compression_ratio": 1.9386792452830188, "no_speech_prob": 0.003368812147527933}, {"id": 1261, "seek": 1190480, "start": 11913.8, "end": 11920.8, "text": " Here, we intentionally increase the magnitude of adversarial perturbations so that they are visible to us.", "tokens": [50814, 1692, 11, 321, 22062, 3488, 264, 15668, 295, 17641, 44745, 40468, 763, 370, 300, 436, 366, 8974, 281, 505, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1623380766974555, "compression_ratio": 1.9386792452830188, "no_speech_prob": 0.003368812147527933}, {"id": 1262, "seek": 1190480, "start": 11920.8, "end": 11932.8, "text": " These two groups are representing, these two images are representing adversarial perturbation for a smining class and these two represent adversarial perturbation for eyeglasses.", "tokens": [51164, 1981, 732, 3935, 366, 13460, 11, 613, 732, 5267, 366, 13460, 17641, 44745, 40468, 399, 337, 257, 899, 1760, 1508, 293, 613, 732, 2906, 17641, 44745, 40468, 399, 337, 9817, 1146, 25479, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1623380766974555, "compression_ratio": 1.9386792452830188, "no_speech_prob": 0.003368812147527933}, {"id": 1263, "seek": 1193280, "start": 11932.8, "end": 11941.8, "text": " Here you can see diffuse samples that are generated by following the forward diffusion. This is simply corresponds to sampling from diffusion kernel.", "tokens": [50364, 1692, 291, 393, 536, 42165, 10938, 300, 366, 10833, 538, 3480, 264, 2128, 25242, 13, 639, 307, 2935, 23249, 281, 21179, 490, 25242, 28256, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1390056204288564, "compression_ratio": 1.7935222672064777, "no_speech_prob": 0.0019516015890985727}, {"id": 1264, "seek": 1193280, "start": 11941.8, "end": 11949.8, "text": " And then here in these two columns, you can see samples that are generated when we're solving the reverse genetic SDE.", "tokens": [50814, 400, 550, 510, 294, 613, 732, 13766, 11, 291, 393, 536, 10938, 300, 366, 10833, 562, 321, 434, 12606, 264, 9943, 12462, 14638, 36, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1390056204288564, "compression_ratio": 1.7935222672064777, "no_speech_prob": 0.0019516015890985727}, {"id": 1265, "seek": 1193280, "start": 11949.8, "end": 11959.8, "text": " And as you can see, at time equals to zero, we can remove not only the adversarial perturbations as well as all the nodes that was injected through forward diffusion process.", "tokens": [51214, 400, 382, 291, 393, 536, 11, 412, 565, 6915, 281, 4018, 11, 321, 393, 4159, 406, 787, 264, 17641, 44745, 40468, 763, 382, 731, 382, 439, 264, 13891, 300, 390, 36967, 807, 2128, 25242, 1399, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1390056204288564, "compression_ratio": 1.7935222672064777, "no_speech_prob": 0.0019516015890985727}, {"id": 1266, "seek": 1195980, "start": 11959.8, "end": 11966.8, "text": " And you can see that our generated energy equals to zero are very similar to input clean original images.", "tokens": [50364, 400, 291, 393, 536, 300, 527, 10833, 2281, 6915, 281, 4018, 366, 588, 2531, 281, 4846, 2541, 3380, 5267, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15762442350387573, "compression_ratio": 1.832, "no_speech_prob": 0.0027929050847887993}, {"id": 1267, "seek": 1195980, "start": 11966.8, "end": 11977.8, "text": " And we can see that the semantic attributes of these images are very similar to semantic attributes of the original images.", "tokens": [50714, 400, 321, 393, 536, 300, 264, 47982, 17212, 295, 613, 5267, 366, 588, 2531, 281, 47982, 17212, 295, 264, 3380, 5267, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15762442350387573, "compression_ratio": 1.832, "no_speech_prob": 0.0027929050847887993}, {"id": 1268, "seek": 1197780, "start": 11977.8, "end": 11987.8, "text": " The nice thing about using a generative model for adversarial purification is that these modes are not trained for specific attacks and specific classifiers.", "tokens": [50364, 440, 1481, 551, 466, 1228, 257, 1337, 1166, 2316, 337, 17641, 44745, 1864, 3774, 307, 300, 613, 14068, 366, 406, 8895, 337, 2685, 8122, 293, 2685, 1508, 23463, 13, 50864], "temperature": 0.0, "avg_logprob": -0.24250671137934146, "compression_ratio": 1.7903225806451613, "no_speech_prob": 0.006867669057101011}, {"id": 1269, "seek": 1197780, "start": 11987.8, "end": 11993.8, "text": " So at the test time, we can just apply them for unseen adversarial attacks.", "tokens": [50864, 407, 412, 264, 1500, 565, 11, 321, 393, 445, 3079, 552, 337, 40608, 17641, 44745, 8122, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24250671137934146, "compression_ratio": 1.7903225806451613, "no_speech_prob": 0.006867669057101011}, {"id": 1270, "seek": 1197780, "start": 11993.8, "end": 12005.8, "text": " In comparison to the state of dark methods that are designed for similar situation for unseen threats, we actually see that our proposed diffusion prefabrication method outperform these methods by large margin.", "tokens": [51164, 682, 9660, 281, 264, 1785, 295, 2877, 7150, 300, 366, 4761, 337, 2531, 2590, 337, 40608, 14909, 11, 321, 767, 536, 300, 527, 10348, 25242, 18417, 455, 1341, 399, 3170, 484, 26765, 613, 7150, 538, 2416, 10270, 13, 51764], "temperature": 0.0, "avg_logprob": -0.24250671137934146, "compression_ratio": 1.7903225806451613, "no_speech_prob": 0.006867669057101011}, {"id": 1271, "seek": 1200580, "start": 12005.8, "end": 12015.8, "text": " And we believe that, in fact, the fusion models can can be very strong models for designing adversarial purification techniques.", "tokens": [50364, 400, 321, 1697, 300, 11, 294, 1186, 11, 264, 23100, 5245, 393, 393, 312, 588, 2068, 5245, 337, 14685, 17641, 44745, 1864, 3774, 7512, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1770311083112444, "compression_ratio": 1.675531914893617, "no_speech_prob": 0.0018221320351585746}, {"id": 1272, "seek": 1200580, "start": 12015.8, "end": 12029.8, "text": " And this is probably because the fusion models can generate very high quality images and potential can use for removing all the artifacts that are generated by adversarial perturbations.", "tokens": [50864, 400, 341, 307, 1391, 570, 264, 23100, 5245, 393, 8460, 588, 1090, 3125, 5267, 293, 3995, 393, 764, 337, 12720, 439, 264, 24617, 300, 366, 10833, 538, 17641, 44745, 40468, 763, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1770311083112444, "compression_ratio": 1.675531914893617, "no_speech_prob": 0.0018221320351585746}, {"id": 1273, "seek": 1202980, "start": 12029.8, "end": 12044.8, "text": " This brings me to the end of the second part of applications. Next person will will continue with the third part of applications.", "tokens": [50364, 639, 5607, 385, 281, 264, 917, 295, 264, 1150, 644, 295, 5821, 13, 3087, 954, 486, 486, 2354, 365, 264, 2636, 644, 295, 5821, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2355956481053279, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0018368881428614259}, {"id": 1274, "seek": 1202980, "start": 12044.8, "end": 12053.8, "text": " All right, I will not talk about video synthesis medical imaging 3d generation and discrete state diffusion models.", "tokens": [51114, 1057, 558, 11, 286, 486, 406, 751, 466, 960, 30252, 4625, 25036, 805, 67, 5125, 293, 27706, 1785, 25242, 5245, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2355956481053279, "compression_ratio": 1.5123456790123457, "no_speech_prob": 0.0018368881428614259}, {"id": 1275, "seek": 1205380, "start": 12053.8, "end": 12056.8, "text": " Let's get started with video generation.", "tokens": [50364, 961, 311, 483, 1409, 365, 960, 5125, 13, 50514], "temperature": 0.0, "avg_logprob": -0.29032847346091756, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.007807367946952581}, {"id": 1276, "seek": 1205380, "start": 12056.8, "end": 12065.8, "text": " There are samples from a text conditioned video diffusion model like Jonathan how at all, where we condition on the string fireworks.", "tokens": [50514, 821, 366, 10938, 490, 257, 2487, 35833, 960, 25242, 2316, 411, 15471, 577, 412, 439, 11, 689, 321, 4188, 322, 264, 6798, 28453, 13, 50964], "temperature": 0.0, "avg_logprob": -0.29032847346091756, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.007807367946952581}, {"id": 1277, "seek": 1205380, "start": 12065.8, "end": 12070.8, "text": " So I think these samples look pretty convincing.", "tokens": [50964, 407, 286, 519, 613, 10938, 574, 1238, 24823, 13, 51214], "temperature": 0.0, "avg_logprob": -0.29032847346091756, "compression_ratio": 1.4294871794871795, "no_speech_prob": 0.007807367946952581}, {"id": 1278, "seek": 1207080, "start": 12070.8, "end": 12083.8, "text": " So they're actually in general different video generation tasks. For instance, so it's unconditional generation where we want to generate all frames of the video on scratch without conditioning on anything.", "tokens": [50364, 407, 436, 434, 767, 294, 2674, 819, 960, 5125, 9608, 13, 1171, 5197, 11, 370, 309, 311, 47916, 5125, 689, 321, 528, 281, 8460, 439, 12083, 295, 264, 960, 322, 8459, 1553, 21901, 322, 1340, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1745813056214215, "compression_ratio": 1.8592964824120604, "no_speech_prob": 0.027127591893076897}, {"id": 1279, "seek": 1207080, "start": 12083.8, "end": 12093.8, "text": " There was a future prediction where we want to generate future frames conditioning on one or more past frames. We can also do past prediction the other way around.", "tokens": [51014, 821, 390, 257, 2027, 17630, 689, 321, 528, 281, 8460, 2027, 12083, 21901, 322, 472, 420, 544, 1791, 12083, 13, 492, 393, 611, 360, 1791, 17630, 264, 661, 636, 926, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1745813056214215, "compression_ratio": 1.8592964824120604, "no_speech_prob": 0.027127591893076897}, {"id": 1280, "seek": 1209380, "start": 12093.8, "end": 12104.8, "text": " We can also do interpolation, when we have some frames and we want to generate in between frames, just for instance useful to increase the frame rate of the video.", "tokens": [50364, 492, 393, 611, 360, 44902, 399, 11, 562, 321, 362, 512, 12083, 293, 321, 528, 281, 8460, 294, 1296, 12083, 11, 445, 337, 5197, 4420, 281, 3488, 264, 3920, 3314, 295, 264, 960, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3205260134291375, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.017426572740077972}, {"id": 1281, "seek": 1209380, "start": 12104.8, "end": 12119.8, "text": " All these generation tasks can basically fall under one modeling framework. In all cases, we basically want to learn a model of form setter of xt one to xtk given x, how one to x, how I am.", "tokens": [50914, 1057, 613, 5125, 9608, 393, 1936, 2100, 833, 472, 15983, 8388, 13, 682, 439, 3331, 11, 321, 1936, 528, 281, 1466, 257, 2316, 295, 1254, 992, 391, 295, 220, 734, 472, 281, 220, 734, 74, 2212, 2031, 11, 577, 472, 281, 2031, 11, 577, 286, 669, 13, 51664], "temperature": 0.0, "avg_logprob": -0.3205260134291375, "compression_ratio": 1.6495327102803738, "no_speech_prob": 0.017426572740077972}, {"id": 1282, "seek": 1211980, "start": 12119.8, "end": 12127.8, "text": " So for the t's and tiles, you know the times for frames that you want to generate and for the frames that we condition on.", "tokens": [50364, 407, 337, 264, 256, 311, 293, 21982, 11, 291, 458, 264, 1413, 337, 12083, 300, 291, 528, 281, 8460, 293, 337, 264, 12083, 300, 321, 4188, 322, 13, 50764], "temperature": 0.0, "avg_logprob": -0.29225794474283856, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.009549496695399284}, {"id": 1283, "seek": 1211980, "start": 12127.8, "end": 12137.8, "text": " So for future predictions, these tiles will already smaller than the t's unconditional generation and we wouldn't have any tiles and so on and so forth.", "tokens": [50764, 407, 337, 2027, 21264, 11, 613, 21982, 486, 1217, 4356, 813, 264, 256, 311, 47916, 5125, 293, 321, 2759, 380, 362, 604, 21982, 293, 370, 322, 293, 370, 5220, 13, 51264], "temperature": 0.0, "avg_logprob": -0.29225794474283856, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.009549496695399284}, {"id": 1284, "seek": 1211980, "start": 12137.8, "end": 12145.8, "text": " Something we see in multiple of these recent works is that they try to learn one diffusion model for everything.", "tokens": [51264, 6595, 321, 536, 294, 3866, 295, 613, 5162, 1985, 307, 300, 436, 853, 281, 1466, 472, 25242, 2316, 337, 1203, 13, 51664], "temperature": 0.0, "avg_logprob": -0.29225794474283856, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.009549496695399284}, {"id": 1285, "seek": 1214580, "start": 12145.8, "end": 12154.8, "text": " What they do is they concatenate and combine both the frames to be predicted and the conditioning frames together.", "tokens": [50364, 708, 436, 360, 307, 436, 1588, 7186, 473, 293, 10432, 1293, 264, 12083, 281, 312, 19147, 293, 264, 21901, 12083, 1214, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16672553291803674, "compression_ratio": 1.8844221105527639, "no_speech_prob": 0.010467443615198135}, {"id": 1286, "seek": 1214580, "start": 12154.8, "end": 12158.8, "text": " And then some of these frames are masked out, so once to be predicted.", "tokens": [50814, 400, 550, 512, 295, 613, 12083, 366, 45249, 484, 11, 370, 1564, 281, 312, 19147, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16672553291803674, "compression_ratio": 1.8844221105527639, "no_speech_prob": 0.010467443615198135}, {"id": 1287, "seek": 1214580, "start": 12158.8, "end": 12171.8, "text": " And yeah, based on the conditioning frames, those are then generated and varying the masking and conditioning combinations during training, we can train one model for these different tasks.", "tokens": [51014, 400, 1338, 11, 2361, 322, 264, 21901, 12083, 11, 729, 366, 550, 10833, 293, 22984, 264, 31226, 293, 21901, 21267, 1830, 3097, 11, 321, 393, 3847, 472, 2316, 337, 613, 819, 9608, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16672553291803674, "compression_ratio": 1.8844221105527639, "no_speech_prob": 0.010467443615198135}, {"id": 1288, "seek": 1217180, "start": 12171.8, "end": 12182.8, "text": " In training, we would also tell the model which frames are masked out and we would feed the model time position encodings to encode the times for the different frames.", "tokens": [50364, 682, 3097, 11, 321, 576, 611, 980, 264, 2316, 597, 12083, 366, 45249, 484, 293, 321, 576, 3154, 264, 2316, 565, 2535, 2058, 378, 1109, 281, 2058, 1429, 264, 1413, 337, 264, 819, 12083, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19909736984654477, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0033203703351318836}, {"id": 1289, "seek": 1217180, "start": 12182.8, "end": 12185.8, "text": " That's visualized here, for instance.", "tokens": [50914, 663, 311, 5056, 1602, 510, 11, 337, 5197, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19909736984654477, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0033203703351318836}, {"id": 1290, "seek": 1217180, "start": 12185.8, "end": 12193.8, "text": " In terms of architecture, these models are still using these units, which we already know from the image based diffusion models.", "tokens": [51064, 682, 2115, 295, 9482, 11, 613, 5245, 366, 920, 1228, 613, 6815, 11, 597, 321, 1217, 458, 490, 264, 3256, 2361, 25242, 5245, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19909736984654477, "compression_ratio": 1.645320197044335, "no_speech_prob": 0.0033203703351318836}, {"id": 1291, "seek": 1219380, "start": 12193.8, "end": 12204.8, "text": " We know like small detail. So, of course, now our data is higher dimensional, because in addition to the image and height and width dimensions, as well as the channel dimensions.", "tokens": [50364, 492, 458, 411, 1359, 2607, 13, 407, 11, 295, 1164, 11, 586, 527, 1412, 307, 2946, 18795, 11, 570, 294, 4500, 281, 264, 3256, 293, 6681, 293, 11402, 12819, 11, 382, 731, 382, 264, 2269, 12819, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22972580790519714, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.01742527075111866}, {"id": 1292, "seek": 1219380, "start": 12204.8, "end": 12212.8, "text": " We now also have the time dimensions with the number of frames, so the data is essentially four dimensional.", "tokens": [50914, 492, 586, 611, 362, 264, 565, 12819, 365, 264, 1230, 295, 12083, 11, 370, 264, 1412, 307, 4476, 1451, 18795, 13, 51314], "temperature": 0.0, "avg_logprob": -0.22972580790519714, "compression_ratio": 1.6494252873563218, "no_speech_prob": 0.01742527075111866}, {"id": 1293, "seek": 1221280, "start": 12213.8, "end": 12225.8, "text": " One way is to use now 3D convolution instead of 2D convolutions to run convolutions over height width and the frames. This can be computationally expensive.", "tokens": [50414, 1485, 636, 307, 281, 764, 586, 805, 35, 45216, 2602, 295, 568, 35, 3754, 15892, 281, 1190, 3754, 15892, 670, 6681, 11402, 293, 264, 12083, 13, 639, 393, 312, 24903, 379, 5124, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24277319446686776, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.030182061716914177}, {"id": 1294, "seek": 1221280, "start": 12225.8, "end": 12233.8, "text": " Another option is, for instance, to keep special 2D convolutions and use attention layers along the frame axis.", "tokens": [51014, 3996, 3614, 307, 11, 337, 5197, 11, 281, 1066, 2121, 568, 35, 3754, 15892, 293, 764, 3202, 7914, 2051, 264, 3920, 10298, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24277319446686776, "compression_ratio": 1.5952380952380953, "no_speech_prob": 0.030182061716914177}, {"id": 1295, "seek": 1223380, "start": 12234.8, "end": 12244.8, "text": " This has the additional advantage that ignoring those attention layers, a model can be trained additionally on pure image data, which is kind of nice.", "tokens": [50414, 639, 575, 264, 4497, 5002, 300, 26258, 729, 3202, 7914, 11, 257, 2316, 393, 312, 8895, 43181, 322, 6075, 3256, 1412, 11, 597, 307, 733, 295, 1481, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16922458012898764, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.006286591291427612}, {"id": 1296, "seek": 1223380, "start": 12244.8, "end": 12257.8, "text": " So, let's see some results. It turns out that these video generation diffusion models, they can actually generate really long-term video in a hierarchical manner, which is quite impressive.", "tokens": [50914, 407, 11, 718, 311, 536, 512, 3542, 13, 467, 4523, 484, 300, 613, 960, 5125, 25242, 5245, 11, 436, 393, 767, 8460, 534, 938, 12, 7039, 960, 294, 257, 35250, 804, 9060, 11, 597, 307, 1596, 8992, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16922458012898764, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.006286591291427612}, {"id": 1297, "seek": 1225780, "start": 12257.8, "end": 12266.8, "text": " And there, we precisely leverage these masking schemes that we just had, and these generalized video diffusion frameworks.", "tokens": [50364, 400, 456, 11, 321, 13402, 13982, 613, 31226, 26954, 300, 321, 445, 632, 11, 293, 613, 44498, 960, 25242, 29834, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1200147994021152, "compression_ratio": 1.6747967479674797, "no_speech_prob": 0.0007791370735503733}, {"id": 1298, "seek": 1225780, "start": 12266.8, "end": 12276.8, "text": " So, one thing you can do, for instance, is we generate future frames in a sparse manner by conditioning on frames far back. This gives us long-term consistency.", "tokens": [50814, 407, 11, 472, 551, 291, 393, 360, 11, 337, 5197, 11, 307, 321, 8460, 2027, 12083, 294, 257, 637, 11668, 9060, 538, 21901, 322, 12083, 1400, 646, 13, 639, 2709, 505, 938, 12, 7039, 14416, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1200147994021152, "compression_ratio": 1.6747967479674797, "no_speech_prob": 0.0007791370735503733}, {"id": 1299, "seek": 1225780, "start": 12276.8, "end": 12285.8, "text": " And then we interpolate the in-between frames afterwards. So, we kind of generate the video in a stage-wise hierarchical manner.", "tokens": [51314, 400, 550, 321, 44902, 473, 264, 294, 12, 32387, 12083, 10543, 13, 407, 11, 321, 733, 295, 8460, 264, 960, 294, 257, 3233, 12, 3711, 35250, 804, 9060, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1200147994021152, "compression_ratio": 1.6747967479674797, "no_speech_prob": 0.0007791370735503733}, {"id": 1300, "seek": 1228580, "start": 12285.8, "end": 12297.8, "text": " And with that, it's possible to actually generate really long-length, one-hour coherent videos, which is quite impressive. So, here are some samples from this recent Harvey et al. work.", "tokens": [50364, 400, 365, 300, 11, 309, 311, 1944, 281, 767, 8460, 534, 938, 12, 45390, 11, 472, 12, 18048, 36239, 2145, 11, 597, 307, 1596, 8992, 13, 407, 11, 510, 366, 512, 10938, 490, 341, 5162, 28796, 1030, 419, 13, 589, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12156683603922526, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.0003624974051490426}, {"id": 1301, "seek": 1228580, "start": 12297.8, "end": 12308.8, "text": " All right. Let us now talk about another application of diffusion models, which is solving inverse problems in medical imaging, another very relevant application.", "tokens": [50964, 1057, 558, 13, 961, 505, 586, 751, 466, 1071, 3861, 295, 25242, 5245, 11, 597, 307, 12606, 17340, 2740, 294, 4625, 25036, 11, 1071, 588, 7340, 3861, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12156683603922526, "compression_ratio": 1.5130434782608695, "no_speech_prob": 0.0003624974051490426}, {"id": 1302, "seek": 1230880, "start": 12308.8, "end": 12316.8, "text": " So, medical imaging may refer to computer tomography or magnetic resonance imaging.", "tokens": [50364, 407, 11, 4625, 25036, 815, 2864, 281, 3820, 2916, 5820, 420, 12688, 30944, 25036, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10524448534337486, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.008439297787845135}, {"id": 1303, "seek": 1230880, "start": 12316.8, "end": 12327.8, "text": " In those cases, we're basically interested in an image X, but that is not what we're actually measuring from the CT scanner or MI scanner.", "tokens": [50764, 682, 729, 3331, 11, 321, 434, 1936, 3102, 294, 364, 3256, 1783, 11, 457, 300, 307, 406, 437, 321, 434, 767, 13389, 490, 264, 19529, 30211, 420, 13696, 30211, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10524448534337486, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.008439297787845135}, {"id": 1304, "seek": 1230880, "start": 12327.8, "end": 12336.8, "text": " So, let's consider the measurement process. For instance, in computer tomography, the forward measurement process can be modeled in the following form.", "tokens": [51314, 407, 11, 718, 311, 1949, 264, 13160, 1399, 13, 1171, 5197, 11, 294, 3820, 2916, 5820, 11, 264, 2128, 13160, 1399, 393, 312, 37140, 294, 264, 3480, 1254, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10524448534337486, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.008439297787845135}, {"id": 1305, "seek": 1233680, "start": 12336.8, "end": 12345.8, "text": " In the image, we are basically performing a radon transform, which gives us a sinogram, and then maybe this is sparsely sampled.", "tokens": [50364, 682, 264, 3256, 11, 321, 366, 1936, 10205, 257, 2843, 266, 4088, 11, 597, 2709, 505, 257, 3343, 12820, 11, 293, 550, 1310, 341, 307, 637, 685, 736, 3247, 15551, 13, 50814], "temperature": 0.0, "avg_logprob": -0.21718920730962987, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0022159458603709936}, {"id": 1306, "seek": 1233680, "start": 12345.8, "end": 12358.8, "text": " So, we end up, this is sparsely sampled sinogram Y. And now, the task is that needs to be solved to reconstruct the image given this measurement Y. So, this is an inverse problem.", "tokens": [50814, 407, 11, 321, 917, 493, 11, 341, 307, 637, 685, 736, 3247, 15551, 3343, 12820, 398, 13, 400, 586, 11, 264, 5633, 307, 300, 2203, 281, 312, 13041, 281, 31499, 264, 3256, 2212, 341, 13160, 398, 13, 407, 11, 341, 307, 364, 17340, 1154, 13, 51464], "temperature": 0.0, "avg_logprob": -0.21718920730962987, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0022159458603709936}, {"id": 1307, "seek": 1235880, "start": 12358.8, "end": 12370.8, "text": " This is a similar and magnetic resonance imaging, just that the forward process is now basically modeled with a Fourier transform, which is then sparsely sampled.", "tokens": [50364, 639, 307, 257, 2531, 293, 12688, 30944, 25036, 11, 445, 300, 264, 2128, 1399, 307, 586, 1936, 37140, 365, 257, 36810, 4088, 11, 597, 307, 550, 637, 685, 736, 3247, 15551, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20575186263683232, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0005032469634898007}, {"id": 1308, "seek": 1235880, "start": 12370.8, "end": 12385.8, "text": " So, and this is where diffusion models now come in. So, they can actually be really used in this task. And the highly ideal is here to learn a generative diffusion model as a prior over the images we want to reconstruct.", "tokens": [50964, 407, 11, 293, 341, 307, 689, 25242, 5245, 586, 808, 294, 13, 407, 11, 436, 393, 767, 312, 534, 1143, 294, 341, 5633, 13, 400, 264, 5405, 7157, 307, 510, 281, 1466, 257, 1337, 1166, 25242, 2316, 382, 257, 4059, 670, 264, 5267, 321, 528, 281, 31499, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20575186263683232, "compression_ratio": 1.6367521367521367, "no_speech_prob": 0.0005032469634898007}, {"id": 1309, "seek": 1238580, "start": 12385.8, "end": 12396.8, "text": " And while sampling from the diffusion model, we guide synthesis condition while conditioning on the sparse observations that we have. This is the idea.", "tokens": [50364, 400, 1339, 21179, 490, 264, 25242, 2316, 11, 321, 5934, 30252, 4188, 1339, 21901, 322, 264, 637, 11668, 18163, 300, 321, 362, 13, 639, 307, 264, 1558, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1816718101501465, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.002432280220091343}, {"id": 1310, "seek": 1238580, "start": 12396.8, "end": 12405.8, "text": " And it turns out that doing this actually performs really, really well. And even this outperforms even fully supervised methods sometimes.", "tokens": [50914, 400, 309, 4523, 484, 300, 884, 341, 767, 26213, 534, 11, 534, 731, 13, 400, 754, 341, 484, 26765, 82, 754, 4498, 46533, 7150, 2171, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1816718101501465, "compression_ratio": 1.6477272727272727, "no_speech_prob": 0.002432280220091343}, {"id": 1311, "seek": 1240580, "start": 12405.8, "end": 12420.8, "text": " Specifically, the thing is, when we train this fusion model over these CT or MRI images, we really just need the images, we do not need paired image measurement data to train this.", "tokens": [50364, 26058, 11, 264, 551, 307, 11, 562, 321, 3847, 341, 23100, 2316, 670, 613, 19529, 420, 32812, 5267, 11, 321, 534, 445, 643, 264, 5267, 11, 321, 360, 406, 643, 25699, 3256, 13160, 1412, 281, 3847, 341, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17734527587890625, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.012424807995557785}, {"id": 1312, "seek": 1240580, "start": 12420.8, "end": 12434.8, "text": " So yeah, there is actually a lot of work in that direction because it's a really high impact application, of course, and there are some citations that you are interested in learning more about this.", "tokens": [51114, 407, 1338, 11, 456, 307, 767, 257, 688, 295, 589, 294, 300, 3513, 570, 309, 311, 257, 534, 1090, 2712, 3861, 11, 295, 1164, 11, 293, 456, 366, 512, 4814, 763, 300, 291, 366, 3102, 294, 2539, 544, 466, 341, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17734527587890625, "compression_ratio": 1.6406926406926408, "no_speech_prob": 0.012424807995557785}, {"id": 1313, "seek": 1243480, "start": 12434.8, "end": 12445.8, "text": " So let's move on to the next application topic, which is 3D shaped generation. Also, 3D shaped generation has recently been tackled with diffusion models.", "tokens": [50364, 407, 718, 311, 1286, 322, 281, 264, 958, 3861, 4829, 11, 597, 307, 805, 35, 13475, 5125, 13, 2743, 11, 805, 35, 13475, 5125, 575, 3938, 668, 9426, 1493, 365, 25242, 5245, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20464922487735748, "compression_ratio": 1.4829545454545454, "no_speech_prob": 0.005548830144107342}, {"id": 1314, "seek": 1243480, "start": 12445.8, "end": 12453.8, "text": " So let us consider this work by zoo at our, for instance, here, 3D shapes are represented as point clouds.", "tokens": [50914, 407, 718, 505, 1949, 341, 589, 538, 25347, 412, 527, 11, 337, 5197, 11, 510, 11, 805, 35, 10854, 366, 10379, 382, 935, 12193, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20464922487735748, "compression_ratio": 1.4829545454545454, "no_speech_prob": 0.005548830144107342}, {"id": 1315, "seek": 1245380, "start": 12453.8, "end": 12474.8, "text": " And this has the advantage that they can be diffused really easily and intuitively, we see this here at the bottom right so where the diffusion actually goes from the right to the left, we have a bunch of points and they are perturbed and 3D towards this, yeah, Gaussian noise ball kind of, and the generation goes into the other direction.", "tokens": [50364, 400, 341, 575, 264, 5002, 300, 436, 393, 312, 7593, 4717, 534, 3612, 293, 46506, 11, 321, 536, 341, 510, 412, 264, 2767, 558, 370, 689, 264, 25242, 767, 1709, 490, 264, 558, 281, 264, 1411, 11, 321, 362, 257, 3840, 295, 2793, 293, 436, 366, 13269, 374, 2883, 293, 805, 35, 3030, 341, 11, 1338, 11, 39148, 5658, 2594, 733, 295, 11, 293, 264, 5125, 1709, 666, 264, 661, 3513, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2579880764609889, "compression_ratio": 1.6267942583732058, "no_speech_prob": 0.03727129474282265}, {"id": 1316, "seek": 1247480, "start": 12474.8, "end": 12492.8, "text": " So in those cases, the architectures that we use to implement the denoiser network are like typical point state of the art modern point cloud processing networks like no point net advanced versions of point and point boxes and so on and so forth.", "tokens": [50364, 407, 294, 729, 3331, 11, 264, 6331, 1303, 300, 321, 764, 281, 4445, 264, 1441, 78, 6694, 3209, 366, 411, 7476, 935, 1785, 295, 264, 1523, 4363, 935, 4588, 9007, 9590, 411, 572, 935, 2533, 7339, 9606, 295, 935, 293, 935, 9002, 293, 370, 322, 293, 370, 5220, 13, 51264], "temperature": 0.0, "avg_logprob": -0.26175606635309034, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.002180833602324128}, {"id": 1317, "seek": 1247480, "start": 12492.8, "end": 12494.8, "text": " How does this look like then.", "tokens": [51264, 1012, 775, 341, 574, 411, 550, 13, 51364], "temperature": 0.0, "avg_logprob": -0.26175606635309034, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.002180833602324128}, {"id": 1318, "seek": 1249480, "start": 12494.8, "end": 12499.8, "text": " Another animation.", "tokens": [50364, 3996, 9603, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2611287332350208, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.013002251274883747}, {"id": 1319, "seek": 1249480, "start": 12499.8, "end": 12506.8, "text": " I think this is quite nice. So, yeah, we can generate these pretty good shapes.", "tokens": [50614, 286, 519, 341, 307, 1596, 1481, 13, 407, 11, 1338, 11, 321, 393, 8460, 613, 1238, 665, 10854, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2611287332350208, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.013002251274883747}, {"id": 1320, "seek": 1249480, "start": 12506.8, "end": 12519.8, "text": " We can also train conditional shape completion diffusion models very condition, for instance, unlike that or like some sparse points like this, and then complete those shapes.", "tokens": [50964, 492, 393, 611, 3847, 27708, 3909, 19372, 25242, 5245, 588, 4188, 11, 337, 5197, 11, 8343, 300, 420, 411, 512, 637, 11668, 2793, 411, 341, 11, 293, 550, 3566, 729, 10854, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2611287332350208, "compression_ratio": 1.5568181818181819, "no_speech_prob": 0.013002251274883747}, {"id": 1321, "seek": 1251980, "start": 12519.8, "end": 12532.8, "text": " So in the multimodal fashion, for instance, in this example we have some legs of the chair given. And now we have like different plausible completions of the chair here.", "tokens": [50364, 407, 294, 264, 32972, 378, 304, 6700, 11, 337, 5197, 11, 294, 341, 1365, 321, 362, 512, 5668, 295, 264, 6090, 2212, 13, 400, 586, 321, 362, 411, 819, 39925, 1557, 626, 295, 264, 6090, 510, 13, 51014], "temperature": 0.0, "avg_logprob": -0.29370052058522295, "compression_ratio": 1.3852459016393444, "no_speech_prob": 0.022244753316044807}, {"id": 1322, "seek": 1253280, "start": 12532.8, "end": 12550.8, "text": " Another thing that is also quite cool is that he works on real data. I think here the model was trained only on synthetic shape net data, and yet we can see the model images that we take these images and generate plausible 3D objects.", "tokens": [50364, 3996, 551, 300, 307, 611, 1596, 1627, 307, 300, 415, 1985, 322, 957, 1412, 13, 286, 519, 510, 264, 2316, 390, 8895, 787, 322, 23420, 3909, 2533, 1412, 11, 293, 1939, 321, 393, 536, 264, 2316, 5267, 300, 321, 747, 613, 5267, 293, 8460, 39925, 805, 35, 6565, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19791713254205112, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.003482345025986433}, {"id": 1323, "seek": 1253280, "start": 12550.8, "end": 12555.8, "text": " Very nice.", "tokens": [51264, 4372, 1481, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19791713254205112, "compression_ratio": 1.4848484848484849, "no_speech_prob": 0.003482345025986433}, {"id": 1324, "seek": 1255580, "start": 12555.8, "end": 12571.8, "text": " Finally, I would like to talk about discrete states to fusion models. This is less of an application but slightly different type of diffusion model, but I think it is worth mentioning as part of this tutorial.", "tokens": [50364, 6288, 11, 286, 576, 411, 281, 751, 466, 27706, 4368, 281, 23100, 5245, 13, 639, 307, 1570, 295, 364, 3861, 457, 4748, 819, 2010, 295, 25242, 2316, 11, 457, 286, 519, 309, 307, 3163, 18315, 382, 644, 295, 341, 7073, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17930662367078992, "compression_ratio": 1.4315068493150684, "no_speech_prob": 0.0005355786997824907}, {"id": 1325, "seek": 1257180, "start": 12571.8, "end": 12584.8, "text": " So, so far, we have only been considered continuous diffusion entity noising processes, which I mean with that is, we basically kind of assume our data is of a continuous nature.", "tokens": [50364, 407, 11, 370, 1400, 11, 321, 362, 787, 668, 4888, 10957, 25242, 13977, 572, 3436, 7555, 11, 597, 286, 914, 365, 300, 307, 11, 321, 1936, 733, 295, 6552, 527, 1412, 307, 295, 257, 10957, 3687, 13, 51014], "temperature": 0.0, "avg_logprob": -0.17433463825899012, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.008057208731770515}, {"id": 1326, "seek": 1257180, "start": 12584.8, "end": 12600.8, "text": " And we could add a little bit of Gaussian noise to it in a meaningful way. So both our fixed forward diffusion process and also our reverse generative process are usually were usually implemented as Gaussian distributions like here.", "tokens": [51014, 400, 321, 727, 909, 257, 707, 857, 295, 39148, 5658, 281, 309, 294, 257, 10995, 636, 13, 407, 1293, 527, 6806, 2128, 25242, 1399, 293, 611, 527, 9943, 1337, 1166, 1399, 366, 2673, 645, 2673, 12270, 382, 39148, 37870, 411, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.17433463825899012, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.008057208731770515}, {"id": 1327, "seek": 1260080, "start": 12600.8, "end": 12618.8, "text": " But what if our data is discrete, categorical, then continuous perturbations are not meaningful, they are not possible. Imagine, for instance, our data as text data, you know, pixel wise segmentation labels, or discrete image encodings.", "tokens": [50364, 583, 437, 498, 527, 1412, 307, 27706, 11, 19250, 804, 11, 550, 10957, 40468, 763, 366, 406, 10995, 11, 436, 366, 406, 1944, 13, 11739, 11, 337, 5197, 11, 527, 1412, 382, 2487, 1412, 11, 291, 458, 11, 19261, 10829, 9469, 399, 16949, 11, 420, 27706, 3256, 2058, 378, 1109, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22816970131613992, "compression_ratio": 1.5031847133757963, "no_speech_prob": 0.0028420446906238794}, {"id": 1328, "seek": 1261880, "start": 12618.8, "end": 12633.8, "text": " Yeah, if our data is discrete, adding Gaussian continuous noise to it doesn't really make much sense. So, can we also generalize this diffusion concept to like discrete state situations.", "tokens": [50364, 865, 11, 498, 527, 1412, 307, 27706, 11, 5127, 39148, 10957, 5658, 281, 309, 1177, 380, 534, 652, 709, 2020, 13, 407, 11, 393, 321, 611, 2674, 1125, 341, 25242, 3410, 281, 411, 27706, 1785, 6851, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16425629933675132, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.00128391373436898}, {"id": 1329, "seek": 1261880, "start": 12633.8, "end": 12647.8, "text": " In fact, there are categorical diffusion models. And in those cases, the forward diffusion process or like the perturbation now is defined using categorical distributions.", "tokens": [51114, 682, 1186, 11, 456, 366, 19250, 804, 25242, 5245, 13, 400, 294, 729, 3331, 11, 264, 2128, 25242, 1399, 420, 411, 264, 40468, 399, 586, 307, 7642, 1228, 19250, 804, 37870, 13, 51814], "temperature": 0.0, "avg_logprob": -0.16425629933675132, "compression_ratio": 1.634703196347032, "no_speech_prob": 0.00128391373436898}, {"id": 1330, "seek": 1264780, "start": 12647.8, "end": 12664.8, "text": " So consider perturbation corner q of xt given xt minus one, that is supposed to put up the discrete data. So this can now be a categorical distribution, where the probability to sample one of the teachers is now given by some transition", "tokens": [50364, 407, 1949, 40468, 399, 4538, 9505, 295, 220, 734, 2212, 220, 734, 3175, 472, 11, 300, 307, 3442, 281, 829, 493, 264, 27706, 1412, 13, 407, 341, 393, 586, 312, 257, 19250, 804, 7316, 11, 689, 264, 8482, 281, 6889, 472, 295, 264, 6023, 307, 586, 2212, 538, 512, 6034, 51214], "temperature": 0.0, "avg_logprob": -0.2331916165639119, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.006384524051100016}, {"id": 1331, "seek": 1264780, "start": 12664.8, "end": 12675.8, "text": " matrix q multiplied together with the state we are in xt minus one. So the probability to sample like the new state xt.", "tokens": [51214, 8141, 9505, 17207, 1214, 365, 264, 1785, 321, 366, 294, 220, 734, 3175, 472, 13, 407, 264, 8482, 281, 6889, 411, 264, 777, 1785, 220, 734, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2331916165639119, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.006384524051100016}, {"id": 1332, "seek": 1267580, "start": 12676.8, "end": 12688.8, "text": " So this xt is usually a one-hot state factor describing the state we're in. And yeah, this transition matrix multiplied with will then give us probabilities to sample the next state.", "tokens": [50414, 407, 341, 220, 734, 307, 2673, 257, 472, 12, 12194, 1785, 5952, 16141, 264, 1785, 321, 434, 294, 13, 400, 1338, 11, 341, 6034, 8141, 17207, 365, 486, 550, 976, 505, 33783, 281, 6889, 264, 958, 1785, 13, 51014], "temperature": 0.0, "avg_logprob": -0.29167971535334514, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.001225203275680542}, {"id": 1333, "seek": 1267580, "start": 12688.8, "end": 12698.8, "text": " So with that we can put up complex distributions categorical distributions towards like very random discrete distributions.", "tokens": [51014, 407, 365, 300, 321, 393, 829, 493, 3997, 37870, 19250, 804, 37870, 3030, 411, 588, 4974, 27706, 37870, 13, 51514], "temperature": 0.0, "avg_logprob": -0.29167971535334514, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.001225203275680542}, {"id": 1334, "seek": 1269880, "start": 12698.8, "end": 12708.8, "text": " We choose this transition matrix accordingly. So for instance, in this example, if we look at the right, this may now be a complex data distribution.", "tokens": [50364, 492, 2826, 341, 6034, 8141, 19717, 13, 407, 337, 5197, 11, 294, 341, 1365, 11, 498, 321, 574, 412, 264, 558, 11, 341, 815, 586, 312, 257, 3997, 1412, 7316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20340827595103872, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.008708100765943527}, {"id": 1335, "seek": 1269880, "start": 12708.8, "end": 12716.8, "text": " We can perturb this towards a uniform discrete distribution over these three different states 13123.", "tokens": [50864, 492, 393, 40468, 341, 3030, 257, 9452, 27706, 7316, 670, 613, 1045, 819, 4368, 3705, 4762, 18, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20340827595103872, "compression_ratio": 1.4880952380952381, "no_speech_prob": 0.008708100765943527}, {"id": 1336, "seek": 1271680, "start": 12716.8, "end": 12728.8, "text": " So again, the reverse process for generation, and which is then implemented through a neural network, we can also parameterize as a categorical distribution.", "tokens": [50364, 407, 797, 11, 264, 9943, 1399, 337, 5125, 11, 293, 597, 307, 550, 12270, 807, 257, 18161, 3209, 11, 321, 393, 611, 13075, 1125, 382, 257, 19250, 804, 7316, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1964301889592951, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.006486011669039726}, {"id": 1337, "seek": 1271680, "start": 12728.8, "end": 12734.8, "text": " In fact, there are different options for this perturbation process, this forward perturbation process.", "tokens": [50964, 682, 1186, 11, 456, 366, 819, 3956, 337, 341, 40468, 399, 1399, 11, 341, 2128, 40468, 399, 1399, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1964301889592951, "compression_ratio": 1.5662650602409638, "no_speech_prob": 0.006486011669039726}, {"id": 1338, "seek": 1273480, "start": 12734.8, "end": 12745.8, "text": " We can use uniform categorical diffusion where we pull everything towards a uniform distribution over the different categories like I've just shown.", "tokens": [50364, 492, 393, 764, 9452, 19250, 804, 25242, 689, 321, 2235, 1203, 3030, 257, 9452, 7316, 670, 264, 819, 10479, 411, 286, 600, 445, 4898, 13, 50914], "temperature": 0.0, "avg_logprob": -0.18417984165557444, "compression_ratio": 1.7198067632850242, "no_speech_prob": 0.0025108312256634235}, {"id": 1339, "seek": 1273480, "start": 12745.8, "end": 12755.8, "text": " We can also progressively kind of mask out the data where we pull everything into one particular state. We can also analytically sample from such a distribution.", "tokens": [50914, 492, 393, 611, 46667, 733, 295, 6094, 484, 264, 1412, 689, 321, 2235, 1203, 666, 472, 1729, 1785, 13, 492, 393, 611, 10783, 984, 6889, 490, 1270, 257, 7316, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18417984165557444, "compression_ratio": 1.7198067632850242, "no_speech_prob": 0.0025108312256634235}, {"id": 1340, "seek": 1273480, "start": 12755.8, "end": 12759.8, "text": " So it's also well suited for diffusion model.", "tokens": [51414, 407, 309, 311, 611, 731, 24736, 337, 25242, 2316, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18417984165557444, "compression_ratio": 1.7198067632850242, "no_speech_prob": 0.0025108312256634235}, {"id": 1341, "seek": 1275980, "start": 12759.8, "end": 12770.8, "text": " We can also tailor our diffusion processes to ordinal data and use something like discretized Gaussian diffusion process that's also possible.", "tokens": [50364, 492, 393, 611, 33068, 527, 25242, 7555, 281, 4792, 2071, 1412, 293, 764, 746, 411, 25656, 1602, 39148, 25242, 1399, 300, 311, 611, 1944, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20131855744581956, "compression_ratio": 1.4173228346456692, "no_speech_prob": 0.000570109870750457}, {"id": 1342, "seek": 1275980, "start": 12770.8, "end": 12773.8, "text": " How does this look like for instance.", "tokens": [50914, 1012, 775, 341, 574, 411, 337, 5197, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20131855744581956, "compression_ratio": 1.4173228346456692, "no_speech_prob": 0.000570109870750457}, {"id": 1343, "seek": 1277380, "start": 12773.8, "end": 12790.8, "text": " So here now I have the data distribution. It's a bit complex, but so this is basically each pixel of this image represents one categorical variable, and now the color of this pixel represents which teacher we are in.", "tokens": [50364, 407, 510, 586, 286, 362, 264, 1412, 7316, 13, 467, 311, 257, 857, 3997, 11, 457, 370, 341, 307, 1936, 1184, 19261, 295, 341, 3256, 8855, 472, 19250, 804, 7006, 11, 293, 586, 264, 2017, 295, 341, 19261, 8855, 597, 5027, 321, 366, 294, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1789593015398298, "compression_ratio": 1.4594594594594594, "no_speech_prob": 0.0031714714132249355}, {"id": 1344, "seek": 1279080, "start": 12790.8, "end": 12802.8, "text": " So now if I would do like this uniform categorical diffusion, I would kind of, you know, yeah, would look like this, where I would transition into different states everywhere in the image.", "tokens": [50364, 407, 586, 498, 286, 576, 360, 411, 341, 9452, 19250, 804, 25242, 11, 286, 576, 733, 295, 11, 291, 458, 11, 1338, 11, 576, 574, 411, 341, 11, 689, 286, 576, 6034, 666, 819, 4368, 5315, 294, 264, 3256, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18433596028221977, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.0014099512482061982}, {"id": 1345, "seek": 1279080, "start": 12802.8, "end": 12811.8, "text": " I could also do something like Gaussian diffusion where it's more like this ordinal thing that's more based transition to neighboring states.", "tokens": [50964, 286, 727, 611, 360, 746, 411, 39148, 25242, 689, 309, 311, 544, 411, 341, 4792, 2071, 551, 300, 311, 544, 2361, 6034, 281, 31521, 4368, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18433596028221977, "compression_ratio": 1.746031746031746, "no_speech_prob": 0.0014099512482061982}, {"id": 1346, "seek": 1281180, "start": 12811.8, "end": 12821.8, "text": " And then there was also this absorbing diffusion where I kind of progressively mask out or absorb my state sort of different ways to do this.", "tokens": [50364, 400, 550, 456, 390, 611, 341, 38720, 25242, 689, 286, 733, 295, 46667, 6094, 484, 420, 15631, 452, 1785, 1333, 295, 819, 2098, 281, 360, 341, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2135984528232628, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0005975777166895568}, {"id": 1347, "seek": 1281180, "start": 12821.8, "end": 12832.8, "text": " And then in the reverse process may for instance look like this. So here on the far right, this is a stationary distribution of this categorical distribution from which I can sample analytically.", "tokens": [50864, 400, 550, 294, 264, 9943, 1399, 815, 337, 5197, 574, 411, 341, 13, 407, 510, 322, 264, 1400, 558, 11, 341, 307, 257, 30452, 7316, 295, 341, 19250, 804, 7316, 490, 597, 286, 393, 6889, 10783, 984, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2135984528232628, "compression_ratio": 1.6359223300970873, "no_speech_prob": 0.0005975777166895568}, {"id": 1348, "seek": 1283280, "start": 12832.8, "end": 12842.8, "text": " And then denoising kind of progressively noises is bad towards the data distribution.", "tokens": [50364, 400, 550, 1441, 78, 3436, 733, 295, 46667, 14620, 307, 1578, 3030, 264, 1412, 7316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1740381121635437, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0003625547979027033}, {"id": 1349, "seek": 1283280, "start": 12842.8, "end": 12856.8, "text": " So yeah, one can use this and some papers have explored such discrete state diffusion models. For instance, we can also apply this on images by modeling the pixel values of images as discrete states to be in.", "tokens": [50864, 407, 1338, 11, 472, 393, 764, 341, 293, 512, 10577, 362, 24016, 1270, 27706, 1785, 25242, 5245, 13, 1171, 5197, 11, 321, 393, 611, 3079, 341, 322, 5267, 538, 15983, 264, 19261, 4190, 295, 5267, 382, 27706, 4368, 281, 312, 294, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1740381121635437, "compression_ratio": 1.5473684210526315, "no_speech_prob": 0.0003625547979027033}, {"id": 1350, "seek": 1285680, "start": 12856.8, "end": 12869.8, "text": " So this is a start from uniform uniformly distributed pixel values right here from this all gray kind of state or mask out state.", "tokens": [50364, 407, 341, 307, 257, 722, 490, 9452, 48806, 12631, 19261, 4190, 558, 510, 490, 341, 439, 10855, 733, 295, 1785, 420, 6094, 484, 1785, 13, 51014], "temperature": 0.0, "avg_logprob": -0.5006703015031486, "compression_ratio": 1.303030303030303, "no_speech_prob": 0.002358906902372837}, {"id": 1351, "seek": 1286980, "start": 12869.8, "end": 12889.8, "text": " Another application is to use this in a discrete latin space. So in this work, for instance, images are encoded using a vector quantization techniques into visual tokens in a discrete latin space and then we can use something like discrete diffusion models and similar techniques to model the distribution", "tokens": [50364, 3996, 3861, 307, 281, 764, 341, 294, 257, 27706, 4465, 259, 1901, 13, 407, 294, 341, 589, 11, 337, 5197, 11, 5267, 366, 2058, 12340, 1228, 257, 8062, 4426, 2144, 7512, 666, 5056, 22667, 294, 257, 27706, 4465, 259, 1901, 293, 550, 321, 393, 764, 746, 411, 27706, 25242, 5245, 293, 2531, 7512, 281, 2316, 264, 7316, 51364], "temperature": 0.0, "avg_logprob": -0.22738530085637018, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0012444638414308429}, {"id": 1352, "seek": 1286980, "start": 12889.8, "end": 12892.8, "text": " over the visual tokens.", "tokens": [51364, 670, 264, 5056, 22667, 13, 51514], "temperature": 0.0, "avg_logprob": -0.22738530085637018, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0012444638414308429}, {"id": 1353, "seek": 1286980, "start": 12892.8, "end": 12896.8, "text": " This is also something one can do.", "tokens": [51514, 639, 307, 611, 746, 472, 393, 360, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22738530085637018, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0012444638414308429}, {"id": 1354, "seek": 1289680, "start": 12896.8, "end": 12907.8, "text": " We can also use discrete state diffusion models to generate segmentation maps, which are also categorical distributions in pixel space.", "tokens": [50364, 492, 393, 611, 764, 27706, 1785, 25242, 5245, 281, 8460, 9469, 399, 11317, 11, 597, 366, 611, 19250, 804, 37870, 294, 19261, 1901, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22593016991248496, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0038713046815246344}, {"id": 1355, "seek": 1289680, "start": 12907.8, "end": 12923.8, "text": " And yeah, that concludes my part. And with that, I would like to pass a mic back to ours, we will now conclude our tutorial. Thank you very much.", "tokens": [50914, 400, 1338, 11, 300, 24643, 452, 644, 13, 400, 365, 300, 11, 286, 576, 411, 281, 1320, 257, 3123, 646, 281, 11896, 11, 321, 486, 586, 16886, 527, 7073, 13, 1044, 291, 588, 709, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22593016991248496, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.0038713046815246344}, {"id": 1356, "seek": 1292380, "start": 12923.8, "end": 12933.8, "text": " Thank you for being with us. This basically brings us to the last part conclusions, open problems, and final remarks.", "tokens": [50364, 1044, 291, 337, 885, 365, 505, 13, 639, 1936, 5607, 505, 281, 264, 1036, 644, 22865, 11, 1269, 2740, 11, 293, 2572, 19151, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18922940053437887, "compression_ratio": 1.6238095238095238, "no_speech_prob": 0.0062742349691689014}, {"id": 1357, "seek": 1292380, "start": 12933.8, "end": 12946.8, "text": " So today was a big day, we learned about diffusion models. At the beginning of this video, I started talking about the noise and diffusion prophecy models, which is a part, which is a type of discrete time diffusion models.", "tokens": [50864, 407, 965, 390, 257, 955, 786, 11, 321, 3264, 466, 25242, 5245, 13, 1711, 264, 2863, 295, 341, 960, 11, 286, 1409, 1417, 466, 264, 5658, 293, 25242, 23945, 5245, 11, 597, 307, 257, 644, 11, 597, 307, 257, 2010, 295, 27706, 565, 25242, 5245, 13, 51514], "temperature": 0.0, "avg_logprob": -0.18922940053437887, "compression_ratio": 1.6238095238095238, "no_speech_prob": 0.0062742349691689014}, {"id": 1358, "seek": 1294680, "start": 12946.8, "end": 12965.8, "text": " I showed you how these discrete time diffusion models can be described using two processes, a forward diffusion process that starts from data and generates those by adding those into the input, and then reverse the noise and process that learns to generate data by starting", "tokens": [50364, 286, 4712, 291, 577, 613, 27706, 565, 25242, 5245, 393, 312, 7619, 1228, 732, 7555, 11, 257, 2128, 25242, 1399, 300, 3719, 490, 1412, 293, 23815, 729, 538, 5127, 729, 666, 264, 4846, 11, 293, 550, 9943, 264, 5658, 293, 1399, 300, 27152, 281, 8460, 1412, 538, 2891, 51314], "temperature": 0.0, "avg_logprob": -0.2274353687579815, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.004743358585983515}, {"id": 1359, "seek": 1296580, "start": 12965.8, "end": 12984.8, "text": " from noise and denoising the input image one step at a time. I also talked about how we can train these diffusion models by simply generating diffuse samples and training network to predict that to predict the noise that was used to generate diffuse input images.", "tokens": [50364, 490, 5658, 293, 1441, 78, 3436, 264, 4846, 3256, 472, 1823, 412, 257, 565, 13, 286, 611, 2825, 466, 577, 321, 393, 3847, 613, 25242, 5245, 538, 2935, 17746, 42165, 10938, 293, 3097, 3209, 281, 6069, 300, 281, 6069, 264, 5658, 300, 390, 1143, 281, 8460, 42165, 4846, 5267, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2144096162584093, "compression_ratio": 1.6645569620253164, "no_speech_prob": 0.006731286179274321}, {"id": 1360, "seek": 1298480, "start": 12984.8, "end": 12993.8, "text": " In the second part, Carson talked about the score-based generative modeling with differential equation, which corresponds to continuous time diffusion models.", "tokens": [50364, 682, 264, 1150, 644, 11, 38731, 2825, 466, 264, 6175, 12, 6032, 1337, 1166, 15983, 365, 15756, 5367, 11, 597, 23249, 281, 10957, 565, 25242, 5245, 13, 50814], "temperature": 0.0, "avg_logprob": -0.17392494152118634, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00545576773583889}, {"id": 1361, "seek": 1298480, "start": 12993.8, "end": 13011.8, "text": " Specifically, Carson talked about how we can consider diffusion models in the limit of infinite number of steps and how we can define or how we can describe these forward and reverse processes using stochastic differential equations or STEs.", "tokens": [50814, 26058, 11, 38731, 2825, 466, 577, 321, 393, 1949, 25242, 5245, 294, 264, 4948, 295, 13785, 1230, 295, 4439, 293, 577, 321, 393, 6964, 420, 577, 321, 393, 6786, 613, 2128, 293, 9943, 7555, 1228, 342, 8997, 2750, 15756, 11787, 420, 4904, 20442, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17392494152118634, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.00545576773583889}, {"id": 1362, "seek": 1301180, "start": 13011.8, "end": 13027.8, "text": " I also talked about probability flow ordinary differential equations or ODEs, which describe a deterministic mapping between noise distribution and data distribution.", "tokens": [50364, 286, 611, 2825, 466, 8482, 3095, 10547, 15756, 11787, 420, 48447, 20442, 11, 597, 6786, 257, 15957, 3142, 18350, 1296, 5658, 7316, 293, 1412, 7316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.18899420102437336, "compression_ratio": 1.3070866141732282, "no_speech_prob": 0.024214059114456177}, {"id": 1363, "seek": 1302780, "start": 13027.8, "end": 13043.8, "text": " Another thing about working with stochastic differential equations or ODEs or ordinary differential equations is that we can actually use the same training that was used for training different discrete time diffusion models in the previous slide.", "tokens": [50364, 3996, 551, 466, 1364, 365, 342, 8997, 2750, 15756, 11787, 420, 48447, 20442, 420, 10547, 15756, 11787, 307, 300, 321, 393, 767, 764, 264, 912, 3097, 300, 390, 1143, 337, 3097, 819, 27706, 565, 25242, 5245, 294, 264, 3894, 4137, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10708345174789428, "compression_ratio": 1.748936170212766, "no_speech_prob": 0.023973099887371063}, {"id": 1364, "seek": 1302780, "start": 13043.8, "end": 13055.8, "text": " However, at the test time, we are free to choose different discretization or different OD or ST solvers that have been studied widely in different areas of science.", "tokens": [51164, 2908, 11, 412, 264, 1500, 565, 11, 321, 366, 1737, 281, 2826, 819, 25656, 2144, 420, 819, 48447, 420, 4904, 1404, 840, 300, 362, 668, 9454, 13371, 294, 819, 3179, 295, 3497, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10708345174789428, "compression_ratio": 1.748936170212766, "no_speech_prob": 0.023973099887371063}, {"id": 1365, "seek": 1305580, "start": 13055.8, "end": 13066.8, "text": " And those are basically to change the sampling time by using, for example, OD or ST solvers that don't require a lot of functional evaluations.", "tokens": [50364, 400, 729, 366, 1936, 281, 1319, 264, 21179, 565, 538, 1228, 11, 337, 1365, 11, 48447, 420, 4904, 1404, 840, 300, 500, 380, 3651, 257, 688, 295, 11745, 43085, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20055815752814798, "compression_ratio": 1.3212121212121213, "no_speech_prob": 0.005347022321075201}, {"id": 1366, "seek": 1305580, "start": 13066.8, "end": 13072.8, "text": " In the third part, Ruchi talked about advanced topics in diffusion models.", "tokens": [50914, 682, 264, 2636, 644, 11, 497, 30026, 2825, 466, 7339, 8378, 294, 25242, 5245, 13, 51214], "temperature": 0.0, "avg_logprob": -0.20055815752814798, "compression_ratio": 1.3212121212121213, "no_speech_prob": 0.005347022321075201}, {"id": 1367, "seek": 1307280, "start": 13072.8, "end": 13085.8, "text": " She mostly focused on accelerating sampling from diffusion models and she studied this from three different perspectives, including how we can define forward processes that accelerate sampling from diffusion models,", "tokens": [50364, 1240, 5240, 5178, 322, 34391, 21179, 490, 25242, 5245, 293, 750, 9454, 341, 490, 1045, 819, 16766, 11, 3009, 577, 321, 393, 6964, 2128, 7555, 300, 21341, 21179, 490, 25242, 5245, 11, 51014], "temperature": 0.0, "avg_logprob": -0.09315977234771286, "compression_ratio": 2.0, "no_speech_prob": 0.006574178580194712}, {"id": 1368, "seek": 1307280, "start": 13085.8, "end": 13094.8, "text": " how we can come up with better reverse processes, or how we can come with better denoising models that allows us to access sampling from diffusion models.", "tokens": [51014, 577, 321, 393, 808, 493, 365, 1101, 9943, 7555, 11, 420, 577, 321, 393, 808, 365, 1101, 1441, 78, 3436, 5245, 300, 4045, 505, 281, 2105, 21179, 490, 25242, 5245, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09315977234771286, "compression_ratio": 2.0, "no_speech_prob": 0.006574178580194712}, {"id": 1369, "seek": 1309480, "start": 13094.8, "end": 13104.8, "text": " Beyond that, she also talked about how we can scale up diffusion models to generate high resolution images in conditional and conditional setting.", "tokens": [50364, 19707, 300, 11, 750, 611, 2825, 466, 577, 321, 393, 4373, 493, 25242, 5245, 281, 8460, 1090, 8669, 5267, 294, 27708, 293, 27708, 3287, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2086080721954801, "compression_ratio": 1.6858638743455496, "no_speech_prob": 0.0013637327356263995}, {"id": 1370, "seek": 1309480, "start": 13104.8, "end": 13117.8, "text": " Actually, especially talk about cascaded models and guided diffusion models that are heavily used in the current state-of-the-art image to text diffusion models, just imagine.", "tokens": [50864, 5135, 11, 2318, 751, 466, 3058, 66, 12777, 5245, 293, 19663, 25242, 5245, 300, 366, 10950, 1143, 294, 264, 2190, 1785, 12, 2670, 12, 3322, 12, 446, 3256, 281, 2487, 25242, 5245, 11, 445, 3811, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2086080721954801, "compression_ratio": 1.6858638743455496, "no_speech_prob": 0.0013637327356263995}, {"id": 1371, "seek": 1311780, "start": 13117.8, "end": 13135.8, "text": " After talk about fundamental topics, all three of us talked about various computer vision applications that have been recently proposed and mostly applications that rely on diffusion models at their core recently.", "tokens": [50364, 2381, 751, 466, 8088, 8378, 11, 439, 1045, 295, 505, 2825, 466, 3683, 3820, 5201, 5821, 300, 362, 668, 3938, 10348, 293, 5240, 5821, 300, 10687, 322, 25242, 5245, 412, 641, 4965, 3938, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1359772431223016, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.0029607927426695824}, {"id": 1372, "seek": 1313580, "start": 13135.8, "end": 13145.8, "text": " So, now that we know about diffusion models and we know how we can use these models in practical applications, let's talk about some open problems.", "tokens": [50364, 407, 11, 586, 300, 321, 458, 466, 25242, 5245, 293, 321, 458, 577, 321, 393, 764, 613, 5245, 294, 8496, 5821, 11, 718, 311, 751, 466, 512, 1269, 2740, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10251161528796685, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.019045952707529068}, {"id": 1373, "seek": 1313580, "start": 13145.8, "end": 13160.8, "text": " I do hope that now we could make you interested in this topic and now that you know the some fundamental in this area, maybe you can think about open problems that exist in this space and together we can tackle some of these.", "tokens": [50864, 286, 360, 1454, 300, 586, 321, 727, 652, 291, 3102, 294, 341, 4829, 293, 586, 300, 291, 458, 264, 512, 8088, 294, 341, 1859, 11, 1310, 291, 393, 519, 466, 1269, 2740, 300, 2514, 294, 341, 1901, 293, 1214, 321, 393, 14896, 512, 295, 613, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10251161528796685, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.019045952707529068}, {"id": 1374, "seek": 1316080, "start": 13160.8, "end": 13171.8, "text": " The first problem I want to, first of problems I want to mention are more on the technical side and later I will talk about more applied questions that arises in practice.", "tokens": [50364, 440, 700, 1154, 286, 528, 281, 11, 700, 295, 2740, 286, 528, 281, 2152, 366, 544, 322, 264, 6191, 1252, 293, 1780, 286, 486, 751, 466, 544, 6456, 1651, 300, 27388, 294, 3124, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22550359586390054, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.00964902900159359}, {"id": 1375, "seek": 1316080, "start": 13171.8, "end": 13184.8, "text": " So, if you remember at the beginning of the talk, Mouskarsen and I talked about how diffusion models can be constructed as a special form of MIEs or continuous time normalising flows.", "tokens": [50914, 407, 11, 498, 291, 1604, 412, 264, 2863, 295, 264, 751, 11, 376, 563, 74, 685, 268, 293, 286, 2825, 466, 577, 25242, 5245, 393, 312, 17083, 382, 257, 2121, 1254, 295, 376, 6550, 82, 420, 10957, 565, 2710, 3436, 12867, 13, 51564], "temperature": 0.0, "avg_logprob": -0.22550359586390054, "compression_ratio": 1.5777777777777777, "no_speech_prob": 0.00964902900159359}, {"id": 1376, "seek": 1318480, "start": 13184.8, "end": 13202.8, "text": " We exactly don't know why diffusion models do much better than VAEs and continuous time normalising flows. If we can understand this, maybe we can take the lessons learned from diffusion models and why they do so much better than VAEs and continuous time normalising flows in order to improve these frameworks,", "tokens": [50364, 492, 2293, 500, 380, 458, 983, 25242, 5245, 360, 709, 1101, 813, 18527, 20442, 293, 10957, 565, 2710, 3436, 12867, 13, 759, 321, 393, 1223, 341, 11, 1310, 321, 393, 747, 264, 8820, 3264, 490, 25242, 5245, 293, 983, 436, 360, 370, 709, 1101, 813, 18527, 20442, 293, 10957, 565, 2710, 3436, 12867, 294, 1668, 281, 3470, 613, 29834, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1321978679923124, "compression_ratio": 2.128205128205128, "no_speech_prob": 0.026027780026197433}, {"id": 1377, "seek": 1318480, "start": 13202.8, "end": 13210.8, "text": " meaning we can maybe use the lessons learned from diffusion models to improve VAEs or normalising flows.", "tokens": [51264, 3620, 321, 393, 1310, 764, 264, 8820, 3264, 490, 25242, 5245, 281, 3470, 18527, 20442, 420, 2710, 3436, 12867, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1321978679923124, "compression_ratio": 2.128205128205128, "no_speech_prob": 0.026027780026197433}, {"id": 1378, "seek": 1321080, "start": 13210.8, "end": 13227.8, "text": " Even though there has been a tremendous progress in the community for accelerating sampling from diffusion models, we can still do, in the best case scenario, we can still do, actually, the sampling using four to 10 steps on the small matrices such as Cypher 10.", "tokens": [50364, 2754, 1673, 456, 575, 668, 257, 10048, 4205, 294, 264, 1768, 337, 34391, 21179, 490, 25242, 5245, 11, 321, 393, 920, 360, 11, 294, 264, 1151, 1389, 9005, 11, 321, 393, 920, 360, 11, 767, 11, 264, 21179, 1228, 1451, 281, 1266, 4439, 322, 264, 1359, 32284, 1270, 382, 10295, 79, 511, 1266, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2807262026030442, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.018754757940769196}, {"id": 1379, "seek": 1322780, "start": 13227.8, "end": 13243.8, "text": " However, the main question that remains on how we can get one step samples for diffusion models. And this can be very crucial for interactive applications where a user interacts with the diffusion model and this", "tokens": [50364, 2908, 11, 264, 2135, 1168, 300, 7023, 322, 577, 321, 393, 483, 472, 1823, 10938, 337, 25242, 5245, 13, 400, 341, 393, 312, 588, 11462, 337, 15141, 5821, 689, 257, 4195, 43582, 365, 264, 25242, 2316, 293, 341, 51164], "temperature": 0.0, "avg_logprob": -0.17939831780605628, "compression_ratio": 1.6467391304347827, "no_speech_prob": 0.008758733049035072}, {"id": 1380, "seek": 1322780, "start": 13243.8, "end": 13252.8, "text": " we can reduce the latency that usually users observe when they are using generative models.", "tokens": [51164, 321, 393, 5407, 264, 27043, 300, 2673, 5022, 11441, 562, 436, 366, 1228, 1337, 1166, 5245, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17939831780605628, "compression_ratio": 1.6467391304347827, "no_speech_prob": 0.008758733049035072}, {"id": 1381, "seek": 1325280, "start": 13252.8, "end": 13267.8, "text": " Some of the existing problems have to define one step samplers and part of the solution might be to come up with a better diffusion process that are intrinsically faster to generate sample from.", "tokens": [50364, 2188, 295, 264, 6741, 2740, 362, 281, 6964, 472, 1823, 3247, 564, 433, 293, 644, 295, 264, 3827, 1062, 312, 281, 808, 493, 365, 257, 1101, 25242, 1399, 300, 366, 28621, 984, 4663, 281, 8460, 6889, 490, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18551055590311685, "compression_ratio": 1.4264705882352942, "no_speech_prob": 0.002726334612816572}, {"id": 1382, "seek": 1326780, "start": 13267.8, "end": 13286.8, "text": " Diffusal models, very similar to VAEs or GANS, can be considered as latent variable models, but the latency space is very different. For example, GANS, we know in the latent space often have semantic meaning and using latent space manipulations, we can actually come up with image editing or image", "tokens": [50364, 413, 3661, 11765, 5245, 11, 588, 2531, 281, 18527, 20442, 420, 460, 25711, 11, 393, 312, 4888, 382, 48994, 7006, 5245, 11, 457, 264, 27043, 1901, 307, 588, 819, 13, 1171, 1365, 11, 460, 25711, 11, 321, 458, 294, 264, 48994, 1901, 2049, 362, 47982, 3620, 293, 1228, 48994, 1901, 9258, 4136, 11, 321, 393, 767, 808, 493, 365, 3256, 10000, 420, 3256, 51314], "temperature": 0.0, "avg_logprob": -0.2751204362556116, "compression_ratio": 1.538860103626943, "no_speech_prob": 0.0500410757958889}, {"id": 1383, "seek": 1328680, "start": 13287.8, "end": 13303.8, "text": " manipulation frameworks. But in diffusion models, the latent space does not have semantics, and it's very tricky to come up with latent space semantic manipulation diffusion models. So part of problem here is how can we define", "tokens": [50414, 26475, 29834, 13, 583, 294, 25242, 5245, 11, 264, 48994, 1901, 775, 406, 362, 4361, 45298, 11, 293, 309, 311, 588, 12414, 281, 808, 493, 365, 48994, 1901, 47982, 26475, 25242, 5245, 13, 407, 644, 295, 1154, 510, 307, 577, 393, 321, 6964, 51214], "temperature": 0.0, "avg_logprob": -0.16603439504450018, "compression_ratio": 1.8222222222222222, "no_speech_prob": 0.009842933155596256}, {"id": 1384, "seek": 1328680, "start": 13303.8, "end": 13312.8, "text": " semantically meaningful latent space for diffusion models that allow us to do semantic manipulations.", "tokens": [51214, 4361, 49505, 10995, 48994, 1901, 337, 25242, 5245, 300, 2089, 505, 281, 360, 47982, 9258, 4136, 13, 51664], "temperature": 0.0, "avg_logprob": -0.16603439504450018, "compression_ratio": 1.8222222222222222, "no_speech_prob": 0.009842933155596256}, {"id": 1385, "seek": 1331280, "start": 13312.8, "end": 13332.8, "text": " In this talk, we mostly focus on generative applications, but one open problem is how we can use diffusion models for discriminative applications. For example, one way of using diffusion models might be for representation learning, and we might be able to tackle high level tasks such as image classification", "tokens": [50364, 682, 341, 751, 11, 321, 5240, 1879, 322, 1337, 1166, 5821, 11, 457, 472, 1269, 1154, 307, 577, 321, 393, 764, 25242, 5245, 337, 20828, 1166, 5821, 13, 1171, 1365, 11, 472, 636, 295, 1228, 25242, 5245, 1062, 312, 337, 10290, 2539, 11, 293, 321, 1062, 312, 1075, 281, 14896, 1090, 1496, 9608, 1270, 382, 3256, 21538, 51364], "temperature": 0.0, "avg_logprob": -0.12140306097562195, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0014516257215291262}, {"id": 1386, "seek": 1333280, "start": 13332.8, "end": 13344.8, "text": " versus low level tasks such as semantic image segmentation. And these two may require different traits of when we're trying to use diffusion models to address these.", "tokens": [50364, 5717, 2295, 1496, 9608, 1270, 382, 47982, 3256, 9469, 399, 13, 400, 613, 732, 815, 3651, 819, 19526, 295, 562, 321, 434, 1382, 281, 764, 25242, 5245, 281, 2985, 613, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1650516348825374, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.004183272365480661}, {"id": 1387, "seek": 1333280, "start": 13344.8, "end": 13358.8, "text": " Another group of applications that may benefit from diffusion models is uncertainty estimation. One question is how can we use on diffusion models to do uncertainty estimation in downstream discriminative applications.", "tokens": [50964, 3996, 1594, 295, 5821, 300, 815, 5121, 490, 25242, 5245, 307, 15697, 35701, 13, 1485, 1168, 307, 577, 393, 321, 764, 322, 25242, 5245, 281, 360, 15697, 35701, 294, 30621, 20828, 1166, 5821, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1650516348825374, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.004183272365480661}, {"id": 1388, "seek": 1335880, "start": 13358.8, "end": 13379.8, "text": " And finally, one question that remains open is how we can define joint discriminator generator, sorry, joint discriminator generator models that not only classify images or input, they also can generate similar inputs.", "tokens": [50364, 400, 2721, 11, 472, 1168, 300, 7023, 1269, 307, 577, 321, 393, 6964, 7225, 20828, 1639, 19265, 11, 2597, 11, 7225, 20828, 1639, 19265, 5245, 300, 406, 787, 33872, 5267, 420, 4846, 11, 436, 611, 393, 8460, 2531, 15743, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11859688975594261, "compression_ratio": 1.5912408759124088, "no_speech_prob": 0.0008600066648796201}, {"id": 1389, "seek": 1337980, "start": 13379.8, "end": 13394.8, "text": " So the committee mostly have been using unit architectures for modeling the score model in the score function in diffusion models. But one question is whether we can go beyond units and come up with better architectures for diffusion models.", "tokens": [50364, 407, 264, 7482, 5240, 362, 668, 1228, 4985, 6331, 1303, 337, 15983, 264, 6175, 2316, 294, 264, 6175, 2445, 294, 25242, 5245, 13, 583, 472, 1168, 307, 1968, 321, 393, 352, 4399, 6815, 293, 808, 493, 365, 1101, 6331, 1303, 337, 25242, 5245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21580789486567178, "compression_ratio": 1.6506849315068493, "no_speech_prob": 0.006965045351535082}, {"id": 1390, "seek": 1339480, "start": 13394.8, "end": 13412.8, "text": " One specific open area is how we can feed time input or other conditioning into diffusion models, and how we can potentially improve the sampling efficiency or how we can reduce the latency of sampling from diffusion models using better network design.", "tokens": [50364, 1485, 2685, 1269, 1859, 307, 577, 321, 393, 3154, 565, 4846, 420, 661, 21901, 666, 25242, 5245, 11, 293, 577, 321, 393, 7263, 3470, 264, 21179, 10493, 420, 577, 321, 393, 5407, 264, 27043, 295, 21179, 490, 25242, 5245, 1228, 1101, 3209, 1715, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11156708002090454, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.005766365677118301}, {"id": 1391, "seek": 1341280, "start": 13412.8, "end": 13428.8, "text": " So far in this talk, we mostly focused on image generation, but we may be interested in generating other types of data. For example, 3D data that has different forms of representation, for example, it can be represented by stance function,", "tokens": [50364, 407, 1400, 294, 341, 751, 11, 321, 5240, 5178, 322, 3256, 5125, 11, 457, 321, 815, 312, 3102, 294, 17746, 661, 3467, 295, 1412, 13, 1171, 1365, 11, 805, 35, 1412, 300, 575, 819, 6422, 295, 10290, 11, 337, 1365, 11, 309, 393, 312, 10379, 538, 21033, 2445, 11, 51164], "temperature": 0.0, "avg_logprob": -0.15985956731832252, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.011936049908399582}, {"id": 1392, "seek": 1342880, "start": 13428.8, "end": 13447.8, "text": " meshes, voxels, or volumetric representation. Or we might be interested in generating video text graph, which have their own characteristics. And given these characteristics, we actually may need to come up with specific diffusion models for these particular modalities.", "tokens": [50364, 3813, 8076, 11, 1650, 87, 1625, 11, 420, 1996, 449, 17475, 10290, 13, 1610, 321, 1062, 312, 3102, 294, 17746, 960, 2487, 4295, 11, 597, 362, 641, 1065, 10891, 13, 400, 2212, 613, 10891, 11, 321, 767, 815, 643, 281, 808, 493, 365, 2685, 25242, 5245, 337, 613, 1729, 1072, 16110, 13, 51314], "temperature": 0.0, "avg_logprob": -0.15802819388253347, "compression_ratio": 1.5, "no_speech_prob": 0.011956837028265}, {"id": 1393, "seek": 1344780, "start": 13447.8, "end": 13462.8, "text": " One area of research is to do composition and controllable generation, and this will allow us to go beyond images and be able to generate larger scenes that are composed of multiple, for example, objects.", "tokens": [50364, 1485, 1859, 295, 2132, 307, 281, 360, 12686, 293, 45159, 712, 5125, 11, 293, 341, 486, 2089, 505, 281, 352, 4399, 5267, 293, 312, 1075, 281, 8460, 4833, 8026, 300, 366, 18204, 295, 3866, 11, 337, 1365, 11, 6565, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11863788691433994, "compression_ratio": 1.4676258992805755, "no_speech_prob": 0.01411036029458046}, {"id": 1394, "seek": 1346280, "start": 13462.8, "end": 13477.8, "text": " And also this will, as a technical example, allow us to have fine grain control in generation. So one interesting open problem is how we can achieve composition and controllable generation using diffusion models.", "tokens": [50364, 400, 611, 341, 486, 11, 382, 257, 6191, 1365, 11, 2089, 505, 281, 362, 2489, 12837, 1969, 294, 5125, 13, 407, 472, 1880, 1269, 1154, 307, 577, 321, 393, 4584, 12686, 293, 45159, 712, 5125, 1228, 25242, 5245, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20413797955180324, "compression_ratio": 1.4421768707482994, "no_speech_prob": 0.0026149495970457792}, {"id": 1395, "seek": 1347780, "start": 13477.8, "end": 13494.8, "text": " Finally, I think if we look back, look back to the vision community and the problems that we solved in the past few years, we see that most of the applications try to solve, most of the applications that rely on generative models, they try to solve,", "tokens": [50364, 6288, 11, 286, 519, 498, 321, 574, 646, 11, 574, 646, 281, 264, 5201, 1768, 293, 264, 2740, 300, 321, 13041, 294, 264, 1791, 1326, 924, 11, 321, 536, 300, 881, 295, 264, 5821, 853, 281, 5039, 11, 881, 295, 264, 5821, 300, 10687, 322, 1337, 1166, 5245, 11, 436, 853, 281, 5039, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1789823236136601, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.01379311177879572}, {"id": 1396, "seek": 1349480, "start": 13494.8, "end": 13507.8, "text": " and some solve with generative adversarial networks. So maybe it's a time for us to start revisiting those applications and see whether they can benefit from the, the nice properties that diffusion models have.", "tokens": [50364, 293, 512, 5039, 365, 1337, 1166, 17641, 44745, 9590, 13, 407, 1310, 309, 311, 257, 565, 337, 505, 281, 722, 20767, 1748, 729, 5821, 293, 536, 1968, 436, 393, 5121, 490, 264, 11, 264, 1481, 7221, 300, 25242, 5245, 362, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19776626314435686, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.012351739220321178}, {"id": 1397, "seek": 1349480, "start": 13507.8, "end": 13517.8, "text": " So one open question is which applications will benefit most from diffusion models, given that we have such amazing strong tool.", "tokens": [51014, 407, 472, 1269, 1168, 307, 597, 5821, 486, 5121, 881, 490, 25242, 5245, 11, 2212, 300, 321, 362, 1270, 2243, 2068, 2290, 13, 51514], "temperature": 0.0, "avg_logprob": -0.19776626314435686, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.012351739220321178}, {"id": 1398, "seek": 1351780, "start": 13517.8, "end": 13537.8, "text": " Let's go to the final slide. I want to say thank you for being with us today. This was, this is a very long video, and I do hope that we could provide some useful and fundamental background on diffusion models and how often are used in practice.", "tokens": [50364, 961, 311, 352, 281, 264, 2572, 4137, 13, 286, 528, 281, 584, 1309, 291, 337, 885, 365, 505, 965, 13, 639, 390, 11, 341, 307, 257, 588, 938, 960, 11, 293, 286, 360, 1454, 300, 321, 727, 2893, 512, 4420, 293, 8088, 3678, 322, 25242, 5245, 293, 577, 2049, 366, 1143, 294, 3124, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1483129304030846, "compression_ratio": 1.432748538011696, "no_speech_prob": 0.12856829166412354}, {"id": 1399, "seek": 1353780, "start": 13537.8, "end": 13547.8, "text": " All of us are active on Twitter, if you are interested in knowing about follow up works that we, we build on diffusion models, please make sure that you follow us.", "tokens": [50364, 1057, 295, 505, 366, 4967, 322, 5794, 11, 498, 291, 366, 3102, 294, 5276, 466, 1524, 493, 1985, 300, 321, 11, 321, 1322, 322, 25242, 5245, 11, 1767, 652, 988, 300, 291, 1524, 505, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20058208787944956, "compression_ratio": 1.5765306122448979, "no_speech_prob": 0.3636375069618225}, {"id": 1400, "seek": 1353780, "start": 13547.8, "end": 13556.8, "text": " And lastly, I want to mention that all the content on this week and this video, including slides will be available on this video on this website.", "tokens": [50864, 400, 16386, 11, 286, 528, 281, 2152, 300, 439, 264, 2701, 322, 341, 1243, 293, 341, 960, 11, 3009, 9788, 486, 312, 2435, 322, 341, 960, 322, 341, 3144, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20058208787944956, "compression_ratio": 1.5765306122448979, "no_speech_prob": 0.3636375069618225}, {"id": 1401, "seek": 1355680, "start": 13556.8, "end": 13573.8, "text": " If you happen to enjoy this video, I would like to ask you to share this video with your colleagues and collaborators, and hopefully together, we can come with more people to start looking into diffusion models and applying them to various interest applications.", "tokens": [50364, 759, 291, 1051, 281, 2103, 341, 960, 11, 286, 576, 411, 281, 1029, 291, 281, 2073, 341, 960, 365, 428, 7734, 293, 39789, 11, 293, 4696, 1214, 11, 321, 393, 808, 365, 544, 561, 281, 722, 1237, 666, 25242, 5245, 293, 9275, 552, 281, 3683, 1179, 5821, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10897222318147358, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.22101929783821106}, {"id": 1402, "seek": 1355680, "start": 13573.8, "end": 13574.8, "text": " Thanks a lot.", "tokens": [51214, 2561, 257, 688, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10897222318147358, "compression_ratio": 1.5333333333333334, "no_speech_prob": 0.22101929783821106}], "language": "en"}