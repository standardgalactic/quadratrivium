start	end	text
0	7840	Welcome to our tutorial on denoising diffusion-based gender modeling, foundations and applications.
7840	13000	My name is Arash Vathlet, I'm a Principal Research Scientist with NVIDIA Research, and
13000	18560	today I'm very excited to share this tutorial with you along with my dear friends and collaborators
18560	23440	including Karsten Kreis, who is a Senior Research Scientist with NVIDIA, as well as
23440	27800	Ruchi Gau, who is a Research Scientist with Google Brain.
27800	32040	Before starting the tutorial, I'd like to mention that the earlier version of this tutorial
32040	36640	was originally presented at CVPR 2022 in New Orleans.
36640	40080	This tutorial received a lot of interest from the research community, and given this
40080	44920	interest we decided to record our presentation again after the conference, and we'd like
44920	48720	to share this broadly with the research community through YouTube.
48720	52760	If you happen to watch this video, and you enjoy this video, we would like to encourage
52760	58880	you to share this with your friends and collaborators, and hopefully together we can create more
58880	67640	interest around denoising diffusion-based gender models.
67640	71600	If you're watching this video, most likely you'll find me with deep gender learning.
71600	75920	In deep gender learning, we assume that we have access to cliques of samples drawn from
75920	76920	unknown distribution.
76920	82120	We use this cliques of training data to train a deep neural network.
82120	87160	If everything goes smoothly, at the test time, we can use this deep neural network to
87160	92760	draw new samples that would hopefully mimic the training data distribution.
92760	99960	For example, if we use these images of cats to train our deep neural network at the test
99960	105720	time, we do hope that we can also generate images of cute handsome cats, as you can see
105720	107880	in the bottom.
108840	112720	Deep gender learning has many applications.
112720	117760	Mostly the main applications are content generation that have use cases in different
117760	126360	industries, including, for example, entertainment industry.
126360	129920	Deep gender learning can be used for representation learning as well.
129920	136680	If we have a deep gender model that generates really high quality images, mostly the internal
136680	143800	representation in that model can be used also for downstream discriminative applications,
143800	149280	such as semantic image segmentation, as you can see in this slide.
149280	153520	Deep gender models can be also used for building artistic tools.
153520	160400	In this example that you can see on this slide, we have a tool that can be used by an artist
160400	166480	who happens to be just a six-year-old kid to draw a semantic layout of a scene.
166480	171960	This tool can take the semantic layout and generate a corresponding high-quality image
171960	176840	that has the same semantic layout.
176840	181360	If you're a researcher and you watch the landscape of deep gender learning, you're going to see
181360	187160	that this landscape is filled with various frameworks ranging from generative adversarial
187160	192720	networks to virtual autoencoders, energy-based models, and autoregressive models and normalizing
192720	193720	tools.
194480	199280	Historically, the Computer Vision Committee has been using generative adversarial networks
199280	203320	as one of their main tools for training generative models.
203320	208240	In this talk, we would like to argue that there is a new and powerful generative framework
208240	210720	called the Noise and Diffusion Models.
210720	216720	These models obtain very strong results in generation, and we hopefully want to convince
216720	220720	you that these models are very strong and can be used for various applications.
220720	225800	Hopefully, in this talk, we're going to provide you with some foundational knowledge that requires
225800	230720	for using these models in practice, and we're going to even talk about how these models are
230720	236040	currently used for tackling some of the interesting applications that exist in the Computer Vision
236040	237040	Committee.
237040	243640	As I mentioned earlier, the Noise and Diffusion Models have recently emerged as a powerful
243640	247080	generative model of performing cancer.
247080	252720	In these two papers shown on this slide, one from OpenAI on the left side and one from
252720	258120	Google on the right side, researchers observe that you can use the Noise and Diffusion
258120	263360	Models to train generative models on challenging datasets such as ImageNet, and the results
263360	268880	generated by these models is often very high-quality and very diverse, something that we haven't
268880	273040	seen previously with other generative models such as GANs.
274040	279600	The Noise and Diffusion-based models have already been applied to interesting problems
279600	281600	such as super resolution.
281600	288200	In this slide, you can see a super resolution framework that takes a low-resolution Image64x64
288200	296120	dimension and generates high-resolution image in 1024x1024 pixels.
296120	301320	This results show that actually, the Super Resolution Models built on top of the Noise
301320	307320	and Diffusion Models can generate very high-quality, diverse models.
307320	312400	If you're on social media, you've been probably having a hard time not noticing a lot of excitement
312400	317040	that was created around Dolly 2 and Imagine.
317040	323000	These two frameworks are examples of text-to-image generative models that take text as input,
323000	328720	and they generate a corresponding image that can be described using that text.
328720	334680	Using their core, these models use the Noise and Diffusion generative models, and on the
334680	340440	left side, you can see for example Dolly 2 built by OpenAI can actually create this
340440	345400	image of teddy bears skateboarding in Times Square, and on the right side, you can see
345400	352600	Imagine can generate images of multiple teddy bears celebrating their colleague's birthday
352600	355480	while sitting behind a cake that looks like pizza.
355480	356640	This is impressive.
356640	361040	These models can generate very high-quality, diverse images, and they only take text as
361040	362040	input.
362040	365520	Today, not only are we going to talk about diffusion models, we're going to even talk
365520	368320	about how you can use diffusion models to create such models.
368320	377120	Towards the end of the video, Rucci will talk about these applications.
377120	384120	Today's program consists of six main parts, besides introduction and conclusion.
384120	390640	The first three parts that are shown in green are mostly the technical components of the
390640	391640	program.
391640	393520	I'm going to start with part one.
393520	398040	I will talk about the Noise and Diffusion probabilistic models, and after me, Carson
398040	402400	will talk about the score-based generative modeling with differential equations, and
402400	407000	after us, Rucci will talk about advanced techniques, mostly around accelerated sampling
407000	409000	and condition generation.
409000	413920	These parts, each one of them would be roughly around 65 minutes to 45 minutes.
413920	419000	After these parts, we're going to have three short parts around applications, and mostly
419000	425320	computer vision applications that have been recently used diffusion models in their core
425320	428800	to tackle various deep generative learning-related applications.
428800	433440	Finally, we're going to have a very short segment where I will talk about conclusions
433440	436200	of open problems and final remarks.
436840	442320	Before starting, I'd like to mention that our slides, videos, and some content will be
442320	446800	available on this website, so I'd like to encourage you to bookmark this website.
446800	451640	We're hoping to add more content in the future to this website.
453640	458920	Before starting the presentation, I'd like to just mention that we did our best to include
458920	466160	as many papers that we could, and we thought that it could be interesting to the community.
466160	471800	However, due to limited time and capacity, of course, we could not include every paper.
471800	475800	So if there's a particular paper that you are passionate about, you're very excited
475800	481800	about it, and you would like to be included in this tutorial, we apologize that we couldn't
481800	486000	do that, and what we encourage you to send us an email, let us know that there was a
486000	490640	paper that would be interesting to have in our program, and hopefully, in the future
490680	495640	versions of this tutorial, we will try to include those papers as well.
497640	503640	With that in mind, I will start the first segment, the noise and diffusion probabilistic models.
505640	508240	So part one, the noise and diffusion probabilistic model.
508240	513360	Here you can see an image of three cute dogs who are trying to understand the noise and
513360	517280	diffusion probabilistic models, and as you can see, they're a little bit lost.
517280	522440	So let's go over these models and discuss how these models can be generated.
527360	534320	So using diffusion models, officially consists of two processes, a forward diffusion process
534920	536800	that gradually adds noise to input.
537440	540480	This process is shown on this figure from left to right.
541280	543680	It starts from image of this cute cat.
544040	546040	His name is Peanut, he's my cat.
546400	551960	We can start from him, and we're going to add noise to this image one step at a time.
552680	557360	The forward diffusion process does this in so many steps such that eventually on the
557360	560360	right side, we converge to white noise distribution.
561280	567560	The second process is the reverse denoising process that learns to generate data by denoising.
568120	573680	This process is shown on this slide from right to left, and it starts from white noise and
573680	577360	it learns to generate data on the left side by denoising.
577800	583880	So this process will take a noisy image, and it will generate a less noisy version of it,
583880	588920	and it will repeat this process such that it can convert noise to data.
589760	594320	So we're going to dig deeper into these two processes, and we will see how we can define
594320	595800	these processes formally.
596240	602280	Then the forward diffusion process, as I said, starts from data and generates these intermediate
602280	606440	noisy images, by just simply adding noise one step at a time.
607320	613400	At every step, we're going to assume that we're going to use a normal distribution to generate
613400	617000	this noisy image condition on the previous image.
617800	621520	This normal distribution will have a very simple form.
621600	627440	We're going to represent this normal distribution using q that takes this x at the previous
627440	630440	step and generate x at the current step.
630440	633440	So it takes, for example, x1, and it generates x2.
634320	640840	As I said, it's a normal distribution over the current step, xt, where the mean is denoted
640840	646440	by this square root of one minus beta t times the image at the previous step, and this beta
646440	648440	t representing the variance.
649360	656360	For the moment, assume that this beta t is just simply a very small positive scalar value.
656360	660360	It can be like 0.001, some selectors.
662360	667360	Here, this normal distribution basically takes the image at the previous steps.
667360	674360	It rescale this image, the pixel values on this image, by the square root of one minus
675280	682280	beta t, and it adds a tiny bit of noise where the variance of noise is beta t.
682280	688280	So this is just a diffusion kind of we can call per time step, because we had this very
688280	695280	simple form per time step, like per step, in order to generate xt given xt minus one.
695280	701280	Now, we can also define the joint distribution for all the samples that will be generated
702200	707200	in this trajectory, starting from x1, all the way going to x capital T.
707200	712200	Capital T represents the number of steps in our model, and the joint distribution of all
712200	718200	these samples, condition of this input image of peanut, will be the product of conditionals
718200	721200	that are formed at each step.
721200	726200	So this just represents the joint distribution of all the samples that will be generated
726200	729200	on this trajectory using this Markov process.
730120	735120	This is a Markov process that generates one step, that generates examples one step at
735120	738120	the time, given the previous examples.
738120	744120	Now that we know how we can generate samples one step at a time, you may ask me, how can
744120	749120	I now just take this input image and jump to particular time the step?
749120	754120	Do I need to sample, generate samples one step at a time, or can I just take this x0
755040	760040	and generate xt, or x4, for example, here, just directly?
760040	766040	Because in the forward process, we're using a very simple Gaussian kernel to diffuse the
766040	772040	data, we can actually show that because of this simple form, we can first define this
772040	774040	scale.
774040	781040	This scalar alpha bar T is the product of one minus betas from time to step one all the
781040	783040	way up to current step T.
783960	788960	This is just defined based off the parameters of the diffusion kernel, and having defined
788960	795960	this alpha bar T, now we can define a Gaussian kernel or the diffusion kernel that will generate
795960	798960	xt, even x0.
798960	802960	So, for example, we can now generate using this Gaussian kernel, we can sample from x4
802960	809960	given x0, this would be again a normal distribution where mean is same as the input image, really
810880	815880	this square root of alpha bar T defined here alpha bar, and then the variance is also one
815880	817880	minus alpha bar T.
817880	824880	So, just remember that these betas are just parameters of the diffusion process, we can
824880	830880	compute this alpha bar T very easily, and then we can sample from xt given x0 using
830880	835880	this normal distribution, and this we're going to call this diffusion kernel that diffuses
836800	841800	input at time step zero to time step xt.
841800	846800	Recall that if you want to sample from just a simple normal distribution, you can use
846800	848800	the reparameterization trick.
848800	853800	So, if you want to draw samples from xt, you can just set xt to mean plus some white
853800	859800	nose epsilon that is drawn from standard normal distribution, rescaled with this square
859800	864800	root of one minus alpha bar T, which represents just a standard deviation of this normal distribution.
864800	871800	So, using this we can just simply generate samples at time step T given samples at time
871800	876800	step zero, so given x0 we can just diffuse it easily to time step T.
876800	883800	Beta T values are important, these are basically, we're going to call beta T values as the noise
883800	889800	schedule that defines how we're diffusing the data, and this noise schedule, this noise
889800	895800	schedule, designs such that alpha bar T, this alpha bar at the very last step, would
895800	901800	converge to zero, and when this converges to zero, if you just set alpha bar T to zero
901800	906800	here, you're going to see that because the way that the forward diffusion process is
906800	913800	defined, this diffusion kernel at last step given the x capital T given x0 would be just
913800	918800	can be approximately using normal distribution, standard normal distribution.
918800	923800	This basically means that at the end of this process, diffuse data will have just a standard
923800	928800	normal distribution, this is something that we will need later when we want to define a
928800	931800	generative point.
931800	937800	Now that I've talked about this forward process, like how we can diffuse the diffusion
937800	943800	kernel that diffuses data, let's talk about the marginal diffuse data distribution, let's
943800	948800	talk about what happens to data distribution as we go forward in the diffusion process.
948800	953800	So, have in mind that diffusion kernel that generates xt given x0 is different than the
953800	959800	diffuse data distribution, so we're going to use qxt to represent diffuse data distribution,
959800	964800	and in order to obtain this diffuse data distribution, we first need to form the joint
964800	972800	over clean data input data x0 and diffuse data xt, this joint simply can be defined as
972800	978800	product of input data distribution qx0 times this diffusion kernel, which is just a simple
978800	984800	normal distribution, and now we can marginalize at x0, we can just integrate at x0, and this
984800	991800	will give us marginal data diffuse data distribution at times the T.
991800	999800	If we just consider a very simple one dimensional data, and we hear on visualizing on visualizing
999800	1004800	the diffuse data distribution at different time steps, on the left side we have data distribution
1004800	1012800	at times zero, why access represents this just one dimensional random variable and this x axis
1012800	1018800	represents the PDF probability density function of this random variable at time step zero, this
1018800	1025800	is just the data distribution visualized for one toy example, one dimensional toy example.
1025800	1031800	Here you can see the visualization of diffuse data distributions, and as you can see, as we
1031800	1036800	go in the forward process, we just take this data distribution, we're making kind of this
1036800	1041800	distribution smoother and smoother as we go forward in time, and eventually it becomes so
1041800	1047800	smooth that we can just represent this distribution using standard normal distribution, zero mean
1047800	1054800	unit variance normal distribution. As you see this smoothing process, we can actually show
1054800	1060800	mathematically this, the diffusion kernel in the forward process, this diffusion kernel here
1060800	1066800	is kind of applying a Gaussian convolution to the data distribution, so this smoothing
1066800	1070800	process can be just represented as Gaussian convolution, a convolution in the sense of
1070800	1076800	like signal processing convolution that takes input data distribution makes it smoother and
1076800	1082800	smoother. However, we should have in mind that we actually don't have access to these, to input
1082800	1088800	data distribution and intermediate diffuse data distribution practice, usually we only have
1088800	1094800	samples from data distribution, we don't have access to the explicit form of the probability density
1094800	1102800	function of data distribution, right? So even though we don't have access to this distribution
1102800	1109800	or all the intermediate diffuse distributions, we know how to draw samples from diffuse data
1109800	1113800	distribution, so in order to generate data from diffuse data distribution, we can just simply
1113800	1119800	sample from training data x0, and then we can just follow the forward diffusion process,
1119800	1126800	sample xt to an x0, in order to draw samples from xt, and this is basically the principle
1126800	1131800	of ancestral sampling that you can just basically use in order to generate data, for example, at times
1131800	1138800	that t, you can just use the training data, sample from training data, diffuse it and sample
1138800	1147800	at xt using diffusion kernel, and that will give you samples from marginal data distribution.
1147800	1156800	Okay, so so far we talked about forward process and how this forward process just smoothens
1156800	1162800	the data distribution, now let's talk about how we can define a generative model out of this.
1162800	1169800	In order to generate data in diffusion models, what we can do, we can start from this base
1169800	1175800	distribution at the end, we know that the diffuse data distribution would converge to
1175800	1182800	0 in unit variance, standard normal distribution, so we can sample from standard normal distribution
1182800	1190800	and we can follow the reverse process, where at every step we can now sample from the denoising model
1190800	1196800	that generates the less noisy version of data given current step, so this is the reverse model
1196800	1202800	where we now use the true denoising distribution to sample from xt minus 1 given xt.
1203800	1210800	So we just need to start from xt and just use this true denoising model to generate samples at time step 0.
1210800	1216800	So the algorithm will be something like this, we sample x capital t from standard normal distribution
1216800	1223800	and we sample iteratively from xc minus 1 using this true denoising distribution.
1224800	1232800	The problem here is that in general denoising distribution xt minus 1 given xt is intractable,
1232800	1238800	we don't have access to this distribution, we can use Bayes' rule to show that this distribution
1238800	1244800	is proportional to the product of the marginal data distribution at xc minus 1 times this diffusion
1244800	1251800	at t, this kernel is simple, it's just Gaussian distribution, however this distribution marginal
1251800	1257800	data distribution is intractable and in general because of this, this product is also intractable
1257800	1264800	so we don't have it in closed form. Now that we don't have it in closed form, you may say
1264800	1271800	can I approximate this denoising distribution from data and if yes, what is the best form
1271800	1278800	I can use to represent that denoising model. So the good news is, yes, we can try to train
1278800	1286800	a model to mimic this true denoising distribution xt minus 1 given xt and if you want to model
1286800	1291800	that, the best model that you can use, the statistical model you can use to represent this denoising model
1291800	1300800	is actually a normal distribution if in the forward process we use very small noise or the variance
1300800	1305800	of the noise that we're adding at each step is very small. If we know the forward process
1305800	1313800	at very small amount of noise, we know theoretically that actually the reverse process can be also approximated
1313800	1319800	using a normal distribution, so this denoising model can be represented using denoising model.
1319800	1329800	So now that we know this, we can actually define a parametric model to mimic or to train this true denoising model.
1330800	1337800	So this formally defined our parametric reverse denoising process. Remember that reverse denoising process starts
1337800	1344800	from noise and generous data by denoising once at a time. We're going to assume that the distribution of data
1344800	1350800	at time is the capital T at the end of the forward process, so it's the beginning of the reverse process.
1350800	1356800	It will be standard normal distribution here as soon as data has a standard normal distribution.
1356800	1363800	We define this denoising distribution. This is a parametric denoising distribution as samples from XT minus from given XT.
1363800	1371800	We're going to assume that it also can be represented using normal distribution where now mean here is parametric.
1371800	1382800	It's a deep neural network that takes noisy image XT and predicts the mean of the less noisy version, less noisy image.
1383800	1388800	So this neural network is the trainable component and then we have also the variance per time step.
1388800	1395800	This is just think of the sigma T squared as just some scalar value that represents the variance per time step.
1395800	1401800	And for now just assume that it's just some parameter or we have access to it. We're going to talk about it later.
1401800	1405800	But the most important part is this mean parameter.
1405800	1414800	Remember, this is the trainable network and it takes a noisy image at XT and predicts the mean of less noisy image at XT minus 1.
1414800	1422800	Because it takes an image and predicts an image, we can actually model this using a unit that has the same input and output shape.
1422800	1430800	Or you can think of it as just a denoising autoencoder that denoises the input image to less noisy level basically.
1430800	1438800	And we're going to talk about the architecture of this denoising model, this new filter.
1438800	1444800	Now that we have these definitions, we can define the joint distribution on the full trajectory.
1444800	1456800	This joint distribution is the product of the base distribution at step XT and the product of conditionals at all these steps at T steps where the condition comes from our denoising model.
1456800	1471800	This is again just a Markov process and we can define the joint distribution on the full trajectory by the product of base distribution and the product of these individual conditionals on each denoising step.
1471800	1483800	Okay, so now so far we talked about the forward process, the reverse process. Now let's talk about how we can train these models, how we can train the denoising model.
1483800	1492800	For training the denoising model, we're going to use variation upper bond that is mostly or commonly used for training by autoencoders.
1492800	1502800	In the variation upper bond, ideally we want to optimize marginal likelihood of the data under our parametric model.
1502800	1513800	Here we have this expectation over training data distribution, where we are computing the expected value of the low to likelihood that our trainable model gives to data distribution.
1513800	1527800	Unfortunately, this is interactive, we don't have access to this quantity here, but we can define variation upper bond, where now we have this expectation over training data, we have expectation over samples drawn from the forward process.
1527800	1538800	This is forward process, you can think of it as encoder in VAE, that samples from latent space, so you can think of these as latent variable x1 to capital T.
1538800	1554800	And we have this log ratio here where the nominator is the joint distribution of the samples on the full trajectory that is given by our denoising model, P theta, and in the denominator we have the likelihood of the samples generated by encoder.
1554800	1563800	This is basically the same objective or same training object, we would see variation of bond in variation autoencoders.
1563800	1564800	This is exactly the same.
1564800	1574800	The assumption is that the forward process is kind of like an encoder and the reverse process is the genitive model in VAE.
1574800	1587800	These two papers, by the way, our papers, the links on our slides are clickable, so if you want to check these papers just find our slides and click on these references and you're going to find the paper.
1587800	1595800	So these two papers showed that this variation bond can be decomposed to several times that we're going to go one by one.
1595800	1608800	The first term is just simply the KL divergence from the diffusion kernel in the last step, x capital T given x0 to this base distribution xT.
1608800	1617800	Remember by definition, the diffusion kernel for x capital T converges to standard normal distribution, which is same distribution we assume for xT.
1617800	1627800	So we can completely ignore this term because by definition, forward process defines such that at the end of process, samples converge to 0 million units of mass distribution.
1627800	1629800	So we can ignore this term.
1629800	1638800	We have this KL term that I'm going to go into details, and then we have this term that can be considered as the reconstruction term in VAEs.
1638800	1647800	This just measures the likelihood of input clean image, even the noise image at first step under our denoising model.
1647800	1652800	This term is also very simple, and it has actually structure very similar to this other term.
1652800	1658800	So for moment, just assume that we can compute this very easily, and we just ignore for a moment.
1658800	1663800	And mostly we just need to focus on this term, which is the most kind of important term here.
1663800	1673800	This is the KL divergence between this Q, xT minus 1 given xT and x0 to our denoising model.
1673800	1677800	This is our parametric denoising model, which is xT minus 1 given xT.
1677800	1682800	This Q distribution is a pretty well not new distribution.
1682800	1685800	We're going to call it the tractable posterior distribution.
1685800	1696800	These samples from xT minus 1, less noisy image, condition on noisy image, xT, and clean image at time step 0.
1696800	1709800	So it's kind of like you have clean image, you have noisy image, and you want to predict what would be the less noisy variation of this image if you knew what was the starting point, what was the x0 for generating this noisy image.
1709800	1717800	It turns out that this distribution is also very simple because the forward process is just Gaussian distribution.
1717800	1722800	The posterior distribution here is also very simple.
1722800	1727800	This is a pretty good posterior distribution, it's condition on x0. We know what is the starting point.
1727800	1734800	So this distribution is also normal distribution with mean expressed here.
1734800	1740800	This mean is just simply weighted average of the clean image and the noisy image at xT.
1740800	1744800	So it's actually very interesting if you have clean image and you have a noisy image.
1744800	1754800	If you want to predict what is the distribution of xT minus 1, the mean that this will be normal, this expression is just normal.
1754800	1763800	And the mean of that normal is just the weighted average of these two images where the weights come from our diffusion process, parametric diffusion process.
1763800	1772800	Very simple expression and the variance of this distribution is also defined based on the parameters of the forward process.
1772800	1781800	So you can think of it like this beta tilde t can be computed very easily from the parameters of the diffusion process.
1781800	1784800	So that we know none of this here, it's interesting.
1784800	1790800	It's interesting, so we have the scale divergence from this distribution trackable posterior distribution, which is normal.
1790800	1794800	And then this denoising model, which we assume that is normal as well.
1794800	1799800	So, now that we have two normal distributions, the scale divergence there.
1799800	1803800	So I'm just writing down the same scale divergence again.
1803800	1809800	The scale divergence can be computed analytically for two normal distributions.
1809800	1823800	We can show that the scale divergence simply just boils down to the squared L2 distance between the mean of this trackable posterior and the mean of the denoising model.
1823800	1830800	Right, plus some constant terms that we can ignore these constant terms do not depend on any trainable power.
1830800	1834800	So this just basically scale divergence is very interesting.
1834800	1845800	It just boils down to the difference between the mean of this denoising, sorry, this mean of the trackable posterior and our denoising model, parametric denoising model, which is represented by mean theta.
1845800	1848800	And this weight is one over two sigma t squared.
1848800	1852800	It's just the variance used in the denoising distribution.
1852800	1854800	So you can ignore for a moment this coefficient.
1854800	1858800	So we're going to focus on these two terms.
1858800	1868800	Recall that if you want to generate xt, if you want to generate a sample at times the t, you can use this parameterization trick that we discussed earlier.
1868800	1887800	And in this paper hall et al. in New York 2020, they observed that you can also express the mean of the trackable posterior distribution I discussed as xt, the input noise image minus the noise that was used to generate that data.
1888800	1899800	To get this expression, it's very simple, you just need to do some arithmetic operations on this equation and just plug the definition of xt from the parameterization trick.
1899800	1910800	With some arithmetic operation, you will see that if you basically have a noisy image and you want to predict the less noisy version, right?
1910800	1917800	If you knew the noise that was used to generate that noisy image, you can just subtract some re-scaled version.
1917800	1919800	So this is the scaled version of those.
1919800	1928800	So you can take noise, subtract just some scaled version of that noise from xt to get mean of xt minus one.
1928800	1930800	This is kind of very interesting information.
1930800	1940800	So you basically can represent this mean in a very simple form, expression of xt and epsilon, the noise that was used to generate xt.
1940800	1944800	So this actually is the same noise that was used to generate xt.
1944800	1953800	Knowing that knowledge, it means that now if we want to parameterize this network, we can use this knowledge in the parameterization of this model.
1953800	1968800	So we can say that in order to predict this mean of less noisy images, we're going to just take xt and subtract it from a neural network that predicts the noise that was used to generate this xt.
1968800	1977800	So basically we train a neural network to predict the noise that was used in order to generate xt in order to represent this noisy model.
1977800	1980800	This is just a parameterization trick.
1980800	1990800	Instead of just representing the mean of the noisy model directly, what we can do is that we can just represent the noise that was used to generate xt.
1990800	1996800	And if you have this, you can just subtract this from xt in order to get the mean of the denoising model.
1996800	2002800	So if you assume this parameterization for the denoising model.
2002800	2006800	And now we also know that this is true for mu theta t.
2006800	2013800	If you plot these two expressions into this, you're going to actually get a very simple expression here.
2013800	2026800	So we have this lt minus one, this is the same term, lt minus one becomes just you need to draw samples from training data distribution, you draw some noise vector from standard normal distribution.
2026800	2033800	And using this noise vector, you just generate this xt using the few samples using the parameterization trick.
2033800	2045800	You can now pass this if you sample to your epsilon prediction network, the network that is trained to predict that the noise that was used in order to generate xt.
2045800	2049800	So it's basically a very simple algorithm.
2049800	2056800	You draw samples from data distribution, you draw noise, you generate xt from that noise and input data.
2056800	2063800	And you train a network to predict the noise that was used in order to generate that xt.
2063800	2069800	And these weights here are just some scalar parameters that comes from this basically one over two sigma t here.
2069800	2081800	And this is like one over square root of beta t. And these terms that we know, we can compute very easily based on the parameters of the diffusion process.
2081800	2088800	So you can ignore them for a moment. Think of them as a scalar parameters that we can compute from the diffusion parameters.
2088800	2093800	So xt minus one can be represented as this weighted objective.
2093800	2098800	So we're going to just summarize these weights as lambda t. We're going to define a new scalar lambda t.
2098800	2104800	This lambda t ensures that your training objective is weighted properly for maximum data likelihood training.
2104800	2110800	So by using this lambda t weights, you're actually maximizing data likelihood.
2110800	2118800	But however what happens is that this lambda t ends up being very large for small t's and it is small for large t's.
2118800	2125800	So it kind of monotically decreases for small t's is very high and for large t's is very small.
2125800	2130800	And this is basically how the maximum data likelihood training is formulated.
2130800	2145800	However, in this paper by Hoh et al. in Europe's 2020, they observed that if you simply drop these weights or equally just if you simply set these weights to one and train the model using this on the weighted version.
2145800	2151800	Like if you drop this way, you're going to get very high quality sample generation using diffusion models.
2151800	2157800	So they introduced this very simple objective that does not have this weighting anymore.
2157800	2159800	This weighting is one.
2159800	2169800	So again, this objective simply draws samples from data distribution, draws white noise and randomly samples from one of the time steps from one to capital t.
2169800	2177800	It generates a diffused sample using the parameterization trick and it trains a model to predict a noise injected.
2177800	2183800	So it's exactly the same objective without any weighting and it can be done very easily.
2183800	2192800	And the answer is true that actually with this weighting, you will get very high quality sample generation using diffusion models.
2192800	2200800	This objective weighting actually plays a key role in getting high quality sample generation in diffusion models.
2200800	2217800	So if you're interested in checking this area, I would encourage you to check this paper by Hoh et al. published at CUQ 2022 that discuss how you can potentially change this weighting over time to get even better high quality images from diffusion models.
2217800	2224800	So let's summarize the training and sampling from diffusion models and what we've learned so far.
2224800	2228800	In order to train diffusion models, the algorithm is extremely simple.
2228800	2231800	You draw a batch of samples from your training data distribution.
2231800	2235800	You uniformly sample from these time steps from one to capital t.
2235800	2240800	You draw some random noise that has same dimensionality as your input data.
2240800	2246800	And you use the parameterization trick to generate sample at time step t.
2246800	2256800	You give the sample to your noise prediction network and you train this noise prediction network to predict the noise that was used to generate that diffuse sample.
2256800	2264800	And to train this you just simply use squared L2 loss to train this noise prediction network.
2264800	2272800	After training, if you like to draw samples from your diffusion model, you can use the reverse diffusion process to generate data.
2272800	2275800	So we're going to start from last step x capital t.
2275800	2279800	We're going to draw samples from standard normal distribution.
2279800	2287800	And then here we have a full look that walks back in diffusion process starting on capital t all the way to t equals to 1.
2287800	2292800	At every step we just draw white noise z from standard normal distribution.
2292800	2296800	Here we're forming the mean of the nosing model.
2296800	2300800	Remember this is the parameterization we use for the nosing model.
2300800	2309800	And then we add noise rescaled with the standard deviation of the nosing minus sigma t to generate xt minus 1.
2309800	2314800	And we can repeat this t times in order to generate x0.
2314800	2320800	So very simple training and very simple generation process.
2321800	2324800	So far we talked about training and sampling.
2324800	2330800	So let's talk about the implementation details of how to form neural networks for the nosing model.
2330800	2337800	In practice, most diffusion models use unit architecture to represent the nosing model.
2337800	2340800	This unit architecture often has residual blocks.
2341800	2346800	So here different rectangles represent residual blocks at different scales.
2346800	2350800	And these residual blocks often have also self-attention layers in them.
2350800	2355800	In some layers usually produce self-attention layers.
2355800	2367800	Remember this unit takes this diffused image, diffused peanut, xt, and it predicts the noise that was used in order to generate this diffuse image.
2367800	2374800	So this epsilon prediction will be trained to produce the predicted noise that was used to generate this xt.
2374800	2376800	This network is also conditional time.
2376800	2378800	It's shared across different time steps.
2378800	2382800	So it also takes time using some time embedding.
2382800	2394800	This time embedding can be done using, for example, sinusoidal positional embeddings that are often used in transformers or random free features to represent this time embedding.
2394800	2401800	This time embedding will be a vector that will be fed to a small fully connected network.
2401800	2406800	A network consists of a few fully connected layers to access some time representation.
2406800	2410800	And this time representation is usually fed to all the residual blocks.
2410800	2415800	In order to combine this time embedding with all the residual blocks, you have a few options.
2415800	2423800	For example, you can just take this time embedding and do arithmetic sum with all the spatial features in the residual blocks.
2423800	2430800	Or you can use, for example, adaptive group normalization in order to do this, like to add time embedding into residual blocks.
2430800	2440800	So I would encourage you to check this paper that discuss fruits and trades between adaptive group normalization and spatial recognition.
2440800	2448800	So, so far we talked about forward process, reverse process, training, as well as the network's design for diffusion models.
2448800	2463800	Let's also talk about some of these hyperparameters that we have in diffusion models, mostly beta T schedule, the variance of the forward process and the variance used in the reverse process, sigma T square.
2463800	2480800	So, one common assumption is that we can, most papers follow Jonathan Hobot of Newripp's 2020 paper, where they use just simply beta T's that are defined using just a linear function.
2480800	2488800	Just these beta T's are small values and they gradually go to some larger value through some linear schedule.
2488800	2501800	And it's also common to assume that sigma T square is just equal, set equal to beta T. This works also really well in practice, especially when the number of diffusions steps is large.
2501800	2508800	But you may ask me, how can I train these palms? Is there any way I can also train beta T and sigma T square?
2508800	2521800	So, there are a couple of papers that discuss this. One of them is this paper by Kimba Tao at Notives 2022. This paper introduces a new parameterization diffusion model using a concept called signal to noise ratio.
2521800	2530800	And they show how you can actually train this noise schedule using some training objective. So, they actually propose a method for training these beta T values.
2530800	2538800	There are also a couple of papers that discuss how you can train sigma T square, the variance of the reverse process.
2538800	2548800	This first paper here shows how you can use a variational bond that we use for training diffusion models to also train sigma T, the variance of the denuzin models.
2548800	2558800	And there's only paper here, analytically P.M. by Bobo et al. in ICELER 2022. This paper actually got outstanding paper award this year at ICELER.
2558800	2575800	And they showed how you can actually post-training, after training your diffusion models, how you can use this to compute the variance of the denuzin model analytically post-training.
2575800	2593800	So, so far, we talked about how we can train and how we can also pick up these hyperparameters and diffusion models. Let's look at the diffusion process and look into what happens to, for example, images in the forward process.
2593800	2604800	We call it, in order to sample from time step T, we can use this diffusion channel, and we can use this parameterization trick to generate XT from input image X0.
2604800	2617800	Here, XT, we know this diffuse sample, in order to analyze what happens to the image, what we're going to do, we're going to use Fourier transform to convert XT to the frequency domain.
2617800	2626800	So this FXT, you can think of just Fourier transform applied to XT, it's just a representation of the image in the signal, in the frequency domain.
2626800	2643800	And we know from Fourier transform that this XT can be just represented as Fourier transform of input image, plus Fourier transform of the noise, some together with some weights corresponds to the ways that we use actually in this parameterization trick.
2643800	2648800	This is just simple rules in Fourier transform.
2648800	2659800	You should have in mind that most images actually in the frequency domain have a very high response for low frequency, and they have very low response for high frequency content.
2659800	2662800	And this is because most images are very smooth.
2663800	2676800	In general, even if they're not super smooth, when you apply Fourier transform to them, you see usually that most images have very high concentration in low frequency, and their high frequency response is very low.
2676800	2679800	This is very common in most images.
2679800	2693800	One thing you should also know that if you have a white noise or Gaussian noise and you apply Fourier transform on top of it, in the frequency domain, actually this Gaussian noise can be also represented as just Gaussian noise in the frequency domain as well.
2693800	2700800	So Fourier transform of a Gaussian noise is itself Gaussian noise, which we can use now here for analysis.
2700800	2711800	So remember for the small t's, alpha bar t is almost one. So as a result, we actually did the perturbation we apply is very small in the, in the frequency domain as well.
2711800	2726800	So in frequency domain, because most of our input signal for input image is concentrated at the small t's, and because alpha bar is almost one, we actually don't perturb the low frequency content of the image that most.
2726800	2734800	And we mostly perturb when we kind of wash out this high frequency content of the image for small t's.
2734800	2754800	And then for large t's, because alpha bar t, this coefficient here is almost zero. So what happens is that you now push down all the frequency content, so you also push down the low frequency response of the image, and you wash away all the kind of frequency content of the image.
2754800	2768800	This basically shows that there's kind of a trade off in the forward process. In the forward process what happens is that the high frequency content is perturbed faster than the low frequency content.
2768800	2775800	So at small t's, most of the low frequency content is not perturbed, it's mostly the high frequency content that is being perturbed.
2775800	2783800	But eventually at the end of process is a time when we also completely get rid of the low frequency content of the image.
2784800	2802800	This is very important to also understand what happens in the reverse process, right, so because there's kind of trade off between content and detail, you can think of low frequency response is the main content of image and the high frequency response is just detail in that generation.
2803800	2821800	These diffuser model kind of trades off between these in different steps, so you can think of when you're training a generative model, the reverse denoising model, for in the large teams, your denoising models becoming specialized at generating low frequency content of the image.
2821800	2826800	So it's mostly the course content of the image is being generated large t's.
2826800	2834800	In a small t's, then your denoising model is becoming specialized in generating the high frequency content of the image.
2834800	2841800	So most of the low level details are generated in the low, low t's, the small t's.
2841800	2856800	This is also why the weighting of training objective becomes important, right, so because you have a model that is shared in different time steps and this model is responsible for generating course content versus low level details.
2856800	2872800	By reweighting this training objective, now we can kind of keep balance between how much we want to generate this course content that is visually very appealing usually, versus how much we want to generate the high frequency because that usually we ignore, we cannot necessarily observe them.
2872800	2880800	And the weighting plays a key role in keeping this trade off, you're balancing this trade off.
2881800	2893800	So, so far I talked about the fusion was in general, now let's talk about the connections between the fusion was and the VAEs, especially hierarchical VAEs.
2893800	2901800	In hierarchical VAEs, which one of the examples can be like any VAE work I did a few years ago.
2901800	2911800	In hierarchical VAEs, we have this deterministic path, you can think of just a resonant that generates data at x at the end, this is just a generative model.
2911800	2930800	In hierarchical VAEs we usually sample from noise and we inject to this deterministic path and then we go to second group sample from second group condition on the first group, and the after generating this noise we feed it to the, to the deterministic path and we keep doing this, we're just walking down in the hierarchical model.
2930800	2942800	So the fusion models can be considered as hierarchical models as well, where these diffuse steps are just latent variables in this hierarchical model, the condition dependencies of course different.
2942800	2948800	And here we're going to discuss like what are the main differences between the fusion was and hierarchical VAEs.
2948800	2967800	One major difference is that the encoder in VAEs is often trained, whereas encoder, which is the forward diffusion in diffusion models is fixed we're not training the forward diffusion which is using a fixed diffusion process as encode.
2967800	2977800	That's one major difference. The second difference is that latent variables in hierarchical VAEs can have a different shape and different dimensionality compared to input images.
2977800	2986800	Whereas in diffusion models we assume that all the intermediate variables have the same dimension as input data.
2987800	2997800	The third major difference is that in diffusion models, if you think of the noisy model as a generative model, this is actually shared across different steps.
2997800	3006800	Whereas in hierarchical VAEs, we actually don't make an assumption, we don't share any component in this hierarchical structure, usually.
3006800	3021800	In hierarchical VAEs, we usually train these models using variational bond, whereas when we're training diffusion models, we're using some different rebating of variational bond in order to drive the training objective of diffusion.
3021800	3032800	So even though these two are related, they're not exactly the same. There are some trade-offs that occur when we are rebating the variational bond.
3032800	3039800	So this brings me to the last slide. So in this part, I reviewed the noise and diffusion probabilistic models.
3039800	3053800	I showed how these models are just simply trained by sampling from forward diffusion process and training a noisy model that simply predicts the noise that was used in order to generate diffuse samples.
3053800	3064800	We discussed these models from different perspectives. We saw what happens to data as you go in the, what happens to images as you go forward in the diffusion process.
3064800	3074800	We also discussed what happens to data distribution in the forward process. We saw how data distribution becomes smoother and smoother in forward diffusion.
3074800	3091800	But of course, like any other deep learning framework, the devil is in the details, the network architecture, objective rebating, or even diffusion parameters play a key role in getting good high-quality results with diffusion models.
3091800	3101800	So if you're interested in knowing more about important design decisions that actually play a role in getting good diffusion models, I would encourage you to check this paper by other colleagues,
3101800	3121800	called Elucidating the Design Space of Diffusion-Based Genetic Models by Karas Etal. And this paper discusses important design decisions and how they play a role in getting good diffusion models.
3121800	3137800	So with that in mind, I'd like to pass the mic and with you to my dear friend and colleague Karsten to talk about score-based genetic modeling with differential courses.
3137800	3147800	Hello everyone, I'm Karsten, and I will now talk about score-based genetic modeling with differential equations.
3147800	3154800	In order to get started, let us actually consider the diffusion process that Arash already introduced in his part.
3154800	3161800	This diffusion process is defined through this Gaussian transition kernel of this form.
3161800	3168800	But now let us consider the limit of many, many small steps, and each step being very, very tiny.
3168800	3174800	So how does sampling from this diffusion process and in practice look like?
3174800	3191800	So from this Gaussian transition kernel, we can just do essentially the parametrization trick, and we take the xt minus one, we scale it down by this one minus beta t square width term, and we add a little bit of noise from the standard normal distribution,
3191800	3195800	scaled by this square width beta t term.
3195800	3208800	With beta t, we can actually interpret it as a step size essentially, so if beta t is zero, nothing happens, this term drops out, and also no rescaling of xt minus one happens.
3208800	3217800	So let's make this a little bit more explicit and write beta t as this delta t times this function beta of t.
3217800	3230800	So beta t is explicitly our step size, and beta of t is now this time dependent function that allows us to have different step sizes along the diffusion process t.
3230800	3237800	And now in this limit of many, many small tiny steps, it is delta t that goes to zero.
3237800	3251800	If delta t goes towards zero or tiny, we can actually tailor expand this square width expression here and obtain this equation at the bottom.
3251800	3253800	I just copied that over here.
3253800	3257800	And it turns out that this equation has a particular form.
3257800	3269800	We can interpret this as some iterative update, like the new xt is given by the old xt plus some term that depends on xt itself.
3269800	3273800	So this is just a small correction and some noise added.
3273800	3288800	It turns out that this iterative update will correspond to a certain solution or a certain discretization of a stochastic differential equation, and in particular this stochastic differential equation.
3288800	3303800	If I wanted to iteratively numerically solve this stochastic differential equation, for instance with an Euler-Maruama solver, then this is exactly the iterative scheme I would end up with.
3303800	3305800	Let us not get ahead of ourselves.
3305800	3311800	I'm not sure if everybody who's listening here is an expert in differential equations.
3311800	3322800	So let us do a one-slide crash course in differential equations, and let us start with ordinary differential equations, which are a little bit simpler than stochastic ones.
3322800	3327800	Here is an ordinary differential equation that can be written in that form.
3327800	3334800	So this is now the state that we're interested in. This code, for example, the value of a pixel in an image.
3334800	3342800	And t is some continuous time variable that captures the time along which this state changes or evolves.
3342800	3351800	And ultimately one is often interested in the evolution of this state x or this pixel value x of t.
3351800	3364800	And that is not what we're given an ordinary differential equation. What we've given is an expression for the time derivative of dx to dt in the form of this function f.
3364800	3368800	So this code, for instance, will be a neural network.
3368800	3370800	So what does this mean?
3370800	3375800	This f essentially describes not x itself, but the change of x.
3375800	3379800	So if you now look here in this graph at the bottom.
3379800	3388800	So for point x, for a given time t, this f of x now describes the change. So in other words, we could look.
3388800	3392800	So this f essentially corresponds to an arrow in this graph.
3392800	3399800	And if I now wanted to get x of t, I would just follow the arrows in this thing here.
3399800	3408800	So basically you just have to integrate up this differential equation following the arrows to get my final expression x of t.
3408800	3410800	And that's what I would have to do.
3410800	3429800	However, in practice, this f is often like highly complex nonlinear function, for instance, like a neural network, and solving this integral here analytically and following these field lines exactly is often not possible.
3429800	3436800	In fact, one can solve this whole thing iteratively numerically in a stepwise fashion.
3436800	3448800	So in that case, when we add some point x, we evaluate our neural network or our, well, our nonlinear function f, or function f, yeah, at this x and time t.
3448800	3455800	And then we do like a small linear step in this direction and access to our old state x.
3455800	3461800	Continue doing that again evaluate f and again update and so on and so forth.
3461800	3468800	So we have an approximation essentially to this analytical solution.
3468800	3474800	So, yeah, and now there are also stochastic differential equations.
3474800	3486800	And once a little bit more complex, these stochastic differential equations now have an additional term system sigma of x and t times this term omega.
3486800	3488800	So what is all this.
3488800	3492800	First of all, Omega, this is called a Vina process.
3492800	3496800	And what this is in practice is really just Gaussian white noise.
3496800	3506800	What this means is that our DX of t or ODE equation now has this additional term, which is corresponds to noise injection.
3506800	3513800	And this noise is scaled by this standard deviation term essentially sigma of x t.
3513800	3516800	And this has a name, this is a diffusion coefficient.
3516800	3523800	And in that context also the other term system here is called the drift coefficient.
3523800	3530800	And we can see that these equations are sometimes written like this explicit form with derivative here.
3530800	3539800	And sometimes also like this is written on the other side and it becomes a bit of a special expression for the Vina process.
3539800	3549800	Sometimes this differential equations or stochastic differential equations here also written like this, but this essentially means the same thing for the sake of this talk.
3549800	3561800	And importantly, keep in mind this omega of t is essentially a Gaussian random variables and strong independent for each team looks like this.
3561800	3567800	So how does this now look like if I want to solve this stochastic differential equation.
3567800	3574800	So for example, numerically, so it's a little bit similar to the iterative solution we had here.
3574800	3583800	So if I'm given a stage, I, I first updated corresponding to my updated corresponding to the drift coefficient, I evaluate that.
3583800	3586800	So update a little bit with that direction.
3586800	3601800	But then I also evaluate the diffusion coefficient and add a little bit of noise that is proportional the strength of the noise is proportional to the diffusion coefficient, and additionally also to the to the time stamp.
3601800	3605800	So if I do this, so each time I do this I made more different noise variables.
3605800	3613800	So this means there is not a unique solution like there was in the ordinary differential equation case, but there is a lot of noise injected.
3613800	3618800	So if I do this multiple times, I may get slightly different trajectories.
3618800	3626800	So over all these trajectories still approximately follow this deterministic F function, but we have additional noise injection.
3626800	3629800	So, yeah, this is how it may look like.
3629800	3639800	So I may ask, is there now also like an analytical framework to instead, you know, describe this analytically like it was for the ordinary differential equation case.
3639800	3643800	So there is, but this is a little bit more involved.
3643800	3656800	Because now we are not talking about one deterministic solution that we need to describe rather given one state in each at the beginning, we now have a probability distribution over possible states where we could land.
3656800	3666800	So the definition of these probability distribution that is described by the Fokker-Planck equation, but that is beyond the scope of this tutorial here.
3666800	3676800	Anyway, I think now you should also have some intuitions for not only ordinary differential equations, but also stochastic differential equations.
3676800	3687800	Now we can go back to our stochastic differential equation that we had here, and that actually describes the forward diffusion process of diffusion models.
3687800	3698800	So this is again just copy over the equation from the last slide here at the bottom. And this is now a visualization of how this whole thing looks like in practice more or less.
3698800	3712800	So let's go through that one by one. On the left hand side, we have some distribution here of x zero, this might be defined through an empirical data distribution or here we have this one dimensional toy distribution.
3712800	3720800	In a more realistic setting this may represent a distribution of images like images of these cats here.
3720800	3731800	So now if we simulate this stochastic differential equation forward, we get this green trajectories and they evolve towards this standard normal prior distribution.
3731800	3737800	And yeah, the images come progressively noisier and noisier as we do this.
3737800	3747800	So let's also look at the form of this equation and it's kind of intuitive. We see that this is updates or in the DX.
3747800	3757800	This update direction, it's actually proportional to the negative of the state x we're in. So this means if we have a large pixel value for instance.
3757800	3769800	It will pull us to go back towards zero. So direction is always is a negative direction corresponding to my, our x.
3769800	3779800	On the other hand, while all our states are being pulled towards zero as I've just explained, and at the same time we're also injecting noise.
3779800	3787800	So this makes it intuitive that after a while of simulating this whole process, we end up with this distribution.
3787800	3803800	It means that every single point basically completely converts itself to just plain noise where, yeah, with mean zero and certain variance.
3804800	3809800	Here's another animation of that.
3809800	3824800	Note that throughout this talk, we will make a lot of use of this image of this cat washes cat peanut. We do hope that it will become a little bit famous after this talk, but let's see how that goes.
3824800	3836800	Right. So, yeah, we have this forward diffusion sd with a drift term and diffusion term, one of them pulls towards the mode of the distribution the other one injects noise.
3836800	3849800	It may be worth mentioning that in the diffusion model it which are also other differential equations have been used to define other types of diffusion processes, often just take a more general form like this.
3849800	3862800	Yeah, I don't want to go into too much detail and in this talk we will stick to this equation for simplicity, but all the concepts that we tried in this tutorial also hold for other types of SDS.
3862800	3873800	The only thing that is important is that these drift kernels and fusion terms here these are basically linear function of X.
3873800	3883800	Otherwise, we couldn't solve for the probability distributions here, which reminds me the background this where this background in these.
3883800	3897800	In these pictures here these animations, this actually defines the marginal different probability distribution of the few data, which is your multimodal, and then he becomes union model.
3897800	3906800	So great, we have not talked about the forward diffusion process, but what about the reverse direction. So, which is necessary for generation.
3906800	3912800	So, can we also have like some differential equation that is quite sad.
3912800	3914800	It turns out yes.
3914800	3925800	There is this result described by an Anderson 1982 and then used in young songs like your paper last year.
3925800	3942800	So if there is a forward differential equation of this form for STD, then there is a corresponding reverse stochastic differential equation that once in the other direction, but tracks the exact same probability distribution.
3942800	3949800	So, really like the reverse direction, which generates data from noise, not noise from data.
3949800	3955800	So, and since we first generate the fusion STD, it looks like this.
3955800	3958800	And again has drift term and a diffusion term.
3958800	3968800	It's a diffusion term and the drift term look over all somewhat similar. So the diffusion term is the same thing. So this will be a process that again injects noise.
3968800	3972800	This drift term also has this same.
3973800	3978800	X minus one half p to t of X term, but then there was this additional term.
3978800	3981800	This is this red term and that is very important.
3981800	3991800	So this is the gradient of the logarithm of the marginal used density probability density of the data.
3991800	3994800	And this is known as a score function.
3995800	4009800	So this means, if you now had access to this object here like this score function, we could do data generation from random noise by just sampling or simulating this reverse diffusion STD.
4009800	4017800	So this in practice and what look like this, like I said, this reverse diffusion STD.
4017800	4029800	It's really a diffusion process running in the other direction. And so, yeah, simulating this is generative modeling essentially.
4029800	4035800	So, are we done. So, we can simulate this and generate data.
4035800	4038800	Well, not quite.
4038800	4045800	The question is, how do we actually get this score function.
4045800	4055800	We may have the naive idea, why not learn a neural network for the score function to approximate it, and then we can take this new network, we call this new network as data.
4056800	4061800	And then we can take this network inside it and our reverse diffusion STD.
4061800	4065800	And, yeah, so we can simulate it and generate data.
4065800	4072800	Something we could do, for instance, is draw a diffusion time along this diffusion process.
4072800	4084800	Now take like data, refused data from this point in the diffusion process samples is QFT of XT.
4084800	4096800	So we take a neural network that takes us as input, maybe we also additionally give up the time and train this, maybe with some simple square two term, which we minimize.
4096800	4105800	We train it to predict this score of this diffuse data, this marginal score to diffuse data.
4105800	4120800	Well, great idea, but unfortunately that doesn't work, because the score of this marginal diffuse data is not tractable or more explicitly this diffuse data density itself QT of XT is not tractable.
4120800	4134800	We don't have an analytic expression for that, which makes it to a different sense because, well, if we had we could just put NT for zero and get our data distribution that this is what we're interested in modeling in the first place.
4135800	4137800	So, too bad.
4137800	4147800	But what we can do is something different, which is now known as denoising score matching. So what we had on the previous slide was just general score matching.
4147800	4157800	So what we can do is, instead of considering this marginal distribution QFT given X, which corresponds to the full diffuse density.
4157800	4174800	Let us consider like individual data points X zero and diffuse those. So instead, we consider the conditional density QFT of XT given X zero that distribution actually is tractable.
4174800	4182800	So that's why it's preserving stochastic differential equation, which is precisely this SDE that we find our forward diffusion process.
4182800	4203800	For that case, this conditional density QFT of XT given one particular data point X zero, it has this expression, this form. So it is this normal distribution, where the mean is given by the initial point X zero that is now simply scaled down by some gamma of T, which
4203800	4217800	is a function that starts at one and be cased towards zero. And then we also have some variance, which is the other way around, which starts at zero and grows towards one.
4217800	4227800	So we can define all denoising score matching objective. So, again, we have, we draw a diffusion time T, we have an expectation over those.
4227800	4240800	Then we draw a data sample X zero, one particular sample. Overall, we again have an expectation. Then we diffuse that particular data sample.
4240800	4267800	And now we train it to predict the score of this one particular diffused data sample X zero. And yeah, now this is tractable, you know this is just a normal distribution so we can take the logarithm of this density of this expression for the density and also calculate the gradient.
4267800	4287800	Great. So, and now there's one very beautiful result. So after this expectation over the data, when we consider that, it turns out that this neural network then will still learn to approximate the score of the marginal data of the marginal diffuse
4287800	4301800	distribution, which is exactly what we need. And this sort of makes intuitively sense, because this x t that we're feeding to the neural network, this could corresponds to this is noisy right noisy data.
4301800	4317800	So this could corresponds to many possible different x zeros and many different possible like grant true scores that we we regressed was on practice neural network has to kind of average over those.
4317800	4333800	And it turns out that, yeah, after this averaging considering the expectation, this neural network will still model the marginal as the score of the marginal diffuse data distribution. And this is exactly what we need.
4333800	4340800	What we need in our decision model, or more specifically in the reverse genitive SDE for generation.
4340800	4344800	So, this is great.
4344800	4352800	So, yeah, in practice diffusion modeling basically boils down to learning this score function.
4352800	4360800	Let us now talk about a few implementation details here and what people do in practice because that's also crucial.
4360800	4364800	This is again just copied the denoising score matching formula.
4364800	4378800	So, how do we actually sample these diffused data points. And so this is really just with time and just sampling again from this normal distribution here so we have gamma times our input x zero.
4378800	4385800	And then we add noise to us that is scaled by the standard deviation sigma t.
4385800	4397800	We explicitly write down the star function, how it now looks like. So it's a gradient of the logarithm of this conditional distribution is the given x zero.
4397800	4404800	Oh, this is a normal distribution. So this is basically some exponential times some stuff.
4404800	4416800	We have the logarithm and exponential drop out. There's also a, by the way, a normalization constant, but this does not depend on x so it's not important for the derivative.
4416800	4423800	Anyway, so you advise at this time. So now we can take the gradient of system here.
4424800	4434800	Now here for x t. This is like the, the diffuse data point that we sampled, we can actually insert this expression here.
4434800	4446800	Then we get that, but now it turns out all these terms they cancel out this gamma t x zeroes, and also one of those signals, and what we left with a step.
4446800	4465800	It's interesting, because this means that the score function. It's basically just the, the noise values that we introduced doing the we permit rest sampling of our diffused data, like, minus that noise value and scale with the inverse standard deviation from the fusion.
4465800	4468800	But still, this is cool.
4468800	4476800	So, this maybe also suggests us how we should choose our neural network and how we should parameterize this.
4476800	4493800	More specifically, for instance, we can take this new network and define it by like some, yeah, some other neural networks times minus one and divided by the standard deviation, which is inspired by this salt here.
4493800	4506800	So if we insert both of the expression for the ground to a score, which is really just this noise value, and also this neural network parameterization, what we left with is this objective here at the bottom.
4506800	4525800	No, this is interesting. So this means, if I choose this parameterization, our neural network epsilon that amount is basically tasked with predicting noise values epsilon, which are really just the noise values that were used to perturb our data.
4525800	4530800	This also makes it kind of intuitive for this is called denoising score matching.
4530800	4547800	Because if our neural network can be nice, can predict those noise values that were used for perturbation, then yeah, we can be nice and reconstruct the original data point x zero.
4547800	4551800	So there's another implementation detail here.
4551800	4563800	So, I have kind of arbitrarily motivated that we can use this squared to turn to perform, you know, I think score matching and to regress this function.
4563800	4571800	And to give different weights to this L2 term to this L2 loss for different points along the diffusion process.
4571800	4581800	Keep in mind that this is one neural network that just as input gets a noisy state and tea, it's the same neural network for all teeth along the diffusion process.
4581800	4597800	So, maybe we want to specialize in that with a little bit more for large times along the diffusion process of small times or something like this, and give like different weights to this objective for different times along the diffusion process.
4597800	4599800	And this is a loss weight in a lot of tea.
4599800	4604800	So we introduce this loss rating lambda t that the busses.
4604800	4616800	And it turns out that different loss ratings trade off between like models is different good perceptual quality, like the images and look pretty that we can generate the set and sharp.
4616800	4631800	And this is no high log likelihood. For instance, if we choose a number of teeth to be exactly this variance of the forward diffusion process here to cancel out the variants in the denominator.
4631800	4639800	Then this is just an objective that is actually that leads to good high quality perceptual quality outputs.
4639800	4654800	And if we choose for lambda, for instance, beta of T, which is hyper parameter of the forward diffusion process, then this whole objective that we have corresponds to training our model towards maximum log likelihood.
4654800	4660800	More specifically, it's kind of a negative elbow.
4660800	4672800	This is interesting. And it turns out that, yeah, this is exactly the same objectives that we derived with the variational approach and part one presented by our.
4672800	4684800	And yeah, so this means that there are some deep connections between this variation derivation and actually like score matching and noising score matching in particular.
4684800	4691800	I would also like to point out that there are like much more sophisticated model parameterizations and loss breaking possible.
4691800	4701800	I would in particular refer you to this recent paper by Tero Carlos at all, who discusses in quite some detail.
4701800	4706800	There's another implementation detail I would like to talk about.
4706800	4720800	So, we know that this variance sigma T squared and of this forward diffusion process for like using individual data points that actually goes to zero as T goes to zero.
4720800	4730800	But this means that if I sample the T here close to zero, then this loss might be heavily amplified when sampling. Yeah, T close to zero.
4730800	4738800	And that's for the case when we do not choose lambda already in function to cancel out the sigma spread.
4738800	4751800	So for some of these for these reasons we sometimes see some tricks and implementations where we train the small time cut off so we prevent sampling teeth that are extremely close to zero.
4751800	4766800	So in a mental way how this can be fixed, like I said, this is especially relevant training models towards high block likelihood versus lambda T function maybe something like beta of T and the sigma squared is not cancelled out.
4766800	4774800	In that case, we can perform important sampling with respect to this, yeah, weight of this loss.
4774800	4780800	So we have an oversampled teeth close to zero and yeah.
4780800	4789800	So the objective then looks like this. So we oversample small teeth according to the important sample distribution that has most of its weight or small T.
4789800	4798800	And then we weigh down the confusion of those to the overall loss and with one over our teeth and constant distribution.
4798800	4804800	And I don't want to go into too much detail but this is a technique you see in several papers.
4804800	4807800	So here's a visualization of what happens.
4807800	4819800	So this is not a loss value. The wet is the loss value without an important sampling here. So, yeah, if I sample T close to zero then I have this heavily amplified loss values.
4819800	4827800	But with important sampling the blue line, the variance is significantly reduced.
4827800	4833800	Before moving on, it makes sense to briefly recapitulate what we have been doing so far.
4833800	4853800	So we have been introducing this diffusion modeling framework based on continuous times now, in contrast to the first part presented by Arash, where each diffusion step had a finite size and we overall had a finite number of discrete forward fixed diffusion process steps and also denoising steps.
4853800	4864800	In this section we have considered a continuous time. This allowed us to introduce this differential equation framework and also to make these connections to score matching.
4864800	4874800	But it is important to keep in mind that we are still describing the same types of diffusion models in this section. We are just using different tools at a different framework.
4874800	4882800	So this is important to realise after all we obtained the same objectives as we have seen on the last few slides.
4882800	4890800	But bear in mind, let us now move on and we will talk now about the probability flow ordinary differential equation.
4890800	4899800	However, let us first consider the reverse generative diffusion SDE again, that one, so object you have already seen so far.
4899800	4921800	With this generative reverse diffusion SDE, we can, when sampling random noise from this standard normal prior distribution, we can generate data and more specifically we basically can sample data all along the diffused data distribution, the reddish curves here, the reddish contours here.
4922800	4934800	It turns out there is an ordinary differential equation called the probability flow ODE that is in distribution equivalent to this reverse generative diffusion SDE.
4934800	4940800	It will become clear in a minute what exactly I mean with in distribution equivalent.
4940800	4944800	Let us first have a look at this ODE itself. It is written down here.
4944800	4957800	In contrast to the generative diffusion SDE, it doesn't have the noise term and also the score function term, which we will then later learn with the neural network, it doesn't have this factor of two in front anymore.
4957800	4961800	So what do I mean with in distribution equivalent?
4961800	4980800	When I sample many initial noise values from this standard normal distribution at initialization when I want to generate the data, then simulating all these samples on backwards versus probability flow ODE towards the data.
4980800	4992800	By doing that, we will sample from the exact same probability distribution, like with the generative diffusion SDE, with the only difference that we don't have this noise anymore.
4992800	4997800	So how does it look like more specifically? We can see this on that slide.
4997800	5007800	Here again, we have the probability flow ODE, just that we now have inserted the learned score function as a pattern for the score function.
5007800	5012800	And now these trajectories defined by this ODE, they look like this.
5012800	5023800	So we see that when we sample from this standard normal prior distribution on the right, these trajectories they will all flow into the modes of the data distribution.
5023800	5028800	We see this also at these bluish lines here at the background.
5028800	5034800	Yeah, so the probability quite literally flows into the modes of the data distribution.
5034800	5039800	And that's called the probability flow ODE.
5039800	5042800	Here we have an animation how it looks like.
5042800	5051800	And I think at this point it should really become clear what I meant with they are the same in distribution and they sample the same distribution.
5051800	5058800	On the left hand side, we have the SDE that we have that I have introduced earlier already SDE framework.
5058800	5067800	And we see that these trajectories are zigzagging, I have this noise injection, but I'm still landing at the modes of the data distribution.
5067800	5085800	Why for the ODE formulation, I now have these pretty like, not exactly straight but more deterministic trajectories that still land in the modes of the data distribution when I initialize some randomly from this prior distribution.
5085800	5094800	And the visual trajectory on the right is deterministic while this is stochastic.
5094800	5105800	So this probability flow ordinary differential equation, this is actually an instance of a neural ordinary differential equations which a while ago generated a lot of attention in the literature.
5105800	5112800	More specifically, we can even see this as a continuous normalizing flow.
5112800	5118800	So, why should we care, why should we use this probability flow ODE framework.
5118800	5127800	It turns out that this ordinary differential equation framework that allows the use of advanced ordinary differential equation solvers.
5127800	5134800	It is somewhat easier to work with ordinary differential equations than with stochastic differential equation.
5134800	5141800	And there really is a broad literature on how to quickly and very efficiently solve ordinary differential equation.
5141800	5145800	So we can build on top of this literature here.
5145800	5147800	But there are more advantages.
5147800	5162800	This ordinary differential equation, I can run this in both directions, I can run it as generation, where I go from the right to the left, where I sample the noise from a prior distribution and then go to the left to generate data.
5162800	5176800	But similarly, given a data point, I can also run the probability flow ODE in the other direction and encode this data point in the latent space of this diffusion model, this prior space.
5176800	5177800	So this is interesting.
5177800	5184800	And yeah, this allows for interesting applications, for instance, or semantic image interpolation.
5184800	5191800	And to make clear what I mean with that, let's look at this slide here.
5191800	5199800	What I'm doing here is I have drawn two noise values, or let's look first at the lower left.
5199800	5205800	So here we are drawing two noise values in the latent space of the diffusion model and this noise space.
5205800	5210800	And now I can linear interpolate these noise values in this space.
5210800	5226800	However, the model was trained in such a way that every sample under this noise distribution, so also every sample along this linear interpolation path between those noise values decodes to a coherent realistic image.
5226800	5236800	So when I then interpolate, it means that this results in continuous semantically meaningful changes in the data space, right?
5236800	5243800	And keep in mind, we could not just interpolate directly linearly in pixel space, this would not be meaningful.
5243800	5252800	But we can do that in noise space and then obtain semantically meaningful interpolations in the pixel space like this.
5252800	5260800	But because this ODE is so complex, right, under the hood, this means that we will sometimes have some jumps between nodes and such like this.
5260800	5263800	And we also see this in this animation here.
5263800	5271800	So in this animation at the top, yeah, we have been doing many of such interpolations one after another.
5271800	5278800	And yeah, sometimes you see like little jumps, this basically corresponds to that.
5278800	5287800	So all this is only possible due to this deterministic encoding and decoding path with the probability flow ODE.
5287800	5295800	I think it's clear that you couldn't do this so easily with a stochastic trajectory.
5295800	5297800	All right.
5297800	5303800	So there is another advantage of the probability flow ordinary differential equation.
5303800	5310800	We can also use it for block likelihood computation as in continuous normalizing flows.
5310800	5319800	More specifically, we can take a given image or a given data sample, for instance, this image of Arash's cat peanut.
5319800	5326800	Now we can take peanut and encode peanut in the latent space of our diffusion model.
5327800	5335800	Now we can calculate the probability of peanuts and coding under the prior distribution of our diffusion model.
5335800	5348800	And additionally, we take into account this using this instantaneous change of variables formula kind of the distortion of the ODE the volume change along the ODE trajectory.
5348800	5356800	So the probability of our data sample, in our case, the image of peanut is then given by that expression.
5356800	5364800	So we're really just using the tricks from the continuous normalizing flow literature here.
5364800	5375800	What all this means is actually that in their probability flow ODE formulation, diffusion models can also be considered as continuous normalizing flows.
5375800	5382800	However, in contrast to continuous normalizing flows, we train diffusion models with score matching.
5382800	5391800	Continuous normalizing flows themselves are usually trained directly with this objective to maximize the likelihood of the data.
5391800	5404800	However, training with such this objective directly is actually a hard task because for each training iteration, I have to simulate the whole trajectory here and back propagate it through it.
5404800	5409800	On the other hand, this diffusion model training relies on score matching.
5409800	5414800	And as we have seen earlier, score matching works quite differently in score matching.
5414800	5420800	We basically have like we can train for all these different times along the diffusion process separately.
5420800	5424800	This leads to a much more scalable and robust learning objective.
5424800	5433800	And yeah, this makes diffusion models very scalable in contrast to these normalizing flows, I would argue.
5434800	5441800	So I have not talked a lot about these differential equations, derive these, derive them and so on and so forth.
5441800	5446800	However, how should we actually solve these SDS and ODE in practice?
5446800	5460800	We have already seen that we can probably not solve this analytically because these SDS and ODE are defined with very complex nonlinear functions, namely these neural networks that approximate the score function.
5460800	5463800	So let us look at that.
5463800	5467800	So let's start with the generative diffusion SDE.
5467800	5472800	So the most naive way to do this is to use Euler-Mariouama sampling.
5472800	5479800	We have already briefly talked about Euler-Mariouama sampling in this earlier one slide crash course on differential equations.
5479800	5492800	What we do in that case is we simply evaluate our function here for different for our state t and x, then we propagate for like a small time step delta t.
5492800	5498800	And we additionally add a little bit of noise, which is also scaled by the time step.
5498800	5508800	And yeah, so we do this then iteratively one after another given a new step we evaluate again and then add a little bit of noise and so on and so forth.
5508800	5525800	By the way, as a small comment here, and you may wonder about the sign flip from here to here, this is because our dt is actually negative because we're running from like large time values to small time values and this delta t here is not supposed to be like an absolute step size.
5525800	5528800	So it's positive.
5528800	5544800	So this runs out also this enchant to a sampling that I wash talked about in the first part of our tutorial, and most specifically the way he showed us how we can, how we can sample from these discrete time diffusion models.
5544800	5553800	And this, this can actually be also considered a generative SDE sampler with this particular discretization used in that part.
5553800	5560800	So let's look at the probability flow ODE. How can we generate that, or using that.
5560800	5569800	Again, we could basically use Euler's method, which is analogous to the Euler-Maiorama approach and just now without this noise injection.
5569800	5579800	We would just intuitively evaluate our network and, yeah, the ODE function essentially do a small step, linear step for small time delta t.
5579800	5583800	We evaluate and continue doing that.
5583800	5588800	However, this is usually, I think nobody really does this in practice.
5588800	5601800	In practice, we can hear, as I mentioned earlier, really build on the advanced ordinary differential equation literature and use much better solvers and much better methods and higher order methods in particular.
5601800	5610800	So what we see for instance is the use of one cutter methods of linear multi stepping methods, exponential integrators.
5610800	5614800	Yes, there was a lot of literature in that direction.
5614800	5627800	Yeah, like I just said, adaptive step size when you put a method that's been used, also called the stochastic differential equation actually adaptive step size higher order methods have been used.
5627800	5633800	We parameterize the ODE has been proposed that also accelerates sampling.
5633800	5638800	So, yeah, there's a lot of literature in that direction.
5638800	5652800	And the main reason is that one drawback of fusion models is that sampling from them can be slow and contrast to like sampling from a generative adversarial network or variation auto encoder and such methods for instance, sampling from a differential
5652800	5671800	model requires many, many function calls on what are specifically neural network evaluations in our case, because during each step of this iterative denoising, we have to call this neural network again so we often have to call it many, many times.
5671800	5683800	And yeah, so this is why we want to use efficient solvers so that we can reduce this number of neural network evaluations that we have to use.
5684800	5704800	So now I have talked about how you can use how you can solve the SDS and the ODE and practice, but what should you use, should you actually rather build on the SD or the ODE framework when you want to sample from the model.
5705800	5712800	So to shine some light into that, let us look at the generative diffusion SD a little bit closer.
5712800	5714800	So it's like that.
5714800	5718800	But now we can actually decompose this into two terms.
5718800	5721800	Right, so this is just from here to here.
5721800	5732800	And it turns out the first term is really just the probability flow ODE that we have seen already, which itself can be used for deterministic data generation like you're on the right.
5732800	5735800	But then there is this additional term.
5736800	5740800	So this code basically corresponds to the noise injection.
5740800	5743800	And yeah, it has the noise injection here.
5743800	5746800	So what what do these terms do.
5746800	5754800	So this probability flow ODE term is essentially responsible for your pushing us from the right to the left here.
5754800	5765800	And this logical diffusion SD term, what it basically does is for each individual team, it actively pushes us towards correct diffuse data distribution.
5765800	5775800	But because of this, so when I do ours during my soul during my simulation going from the right to left you're going from noise to data.
5775800	5794800	If I have ever said, then this logical diffusion SD can help us to correct these errors and actively bring us back to the right data manifold back to the fused data distribution.
5794800	5799800	So this, yeah, this is an advantage can do some sort of error correction.
5799800	5809800	On the other hand, it's, it's often slower because this term itself requires a somewhat fine discretization during the solve.
5809800	5811800	Yeah.
5811800	5814800	So now let's look at the probability flow ODE.
5814800	5818800	So in that case, we do not have this SD term.
5819800	5825800	However, we can now leverage these really fast ODE solvers.
5825800	5829800	And so this is good when we target very fast sampling.
5829800	5834800	On the other hand, there is no stochastic error correction going on here.
5834800	5841800	And because of this, what we see in practice is that this is sometimes slightly lower performing than the stochastic sampling.
5841800	5844800	When we just look at the quality of the samples.
5844800	5862800	So to summarize what we see is, if we're not concerned about our budget of like neural network evaluations and we're willing to do like very a lot of steps, then this SDE framework can be very useful.
5862800	5871800	But if we want to go as fast as possible, then probably the ODE framework is better where we then can leverage these really fast solvers.
5871800	5878800	It is also worth mentioning that we can do things in between where we have only like this logic in diffusion SDE.
5878800	5880800	Active a little bit.
5880800	5891800	We can also, you know, like kind of solve for the first half using stochastic sampling and then afterwards switch to the ODE advanced methods are possible.
5891800	5900800	I would like to refer you to this paper here with which discusses some of these things in quite some detail.
5900800	5906800	Next, I would like to talk about a connection between diffusion models and energy based models.
5906800	5910800	So what are energy based models?
5910800	5914800	Energy based models are defined like this in an energy based model.
5914800	5927800	The probability distribution that we want to model the setter of x defined through an exponential to the power of minus a scalar energy function, which is now the function of the data.
5928800	5935800	And then this thing is normalized by a normalization constant to make sure it's a well defined probability distribution.
5935800	5940800	This normalization constant is also called sometimes called the partition function.
5940800	5953800	Furthermore, in this case, I have added a time variable t because this energy based model is now supposed to represent the diffuse data distributions for different t's in our case.
5954800	5964800	When we want to sample an energy based model, we usually do that by larger than dynamics, which is if we have seen larger than dynamics already.
5964800	5971800	Basically, this is very closely connected to these stochastic differential equations we have already discussed.
5971800	5980800	So to do this larger than dynamics sampling, we basically require the gradient of this scalar energy function.
5980800	5984800	And then we also iteratively update our sample with that.
5984800	5993800	And we also have like an additional noise term atter and some step size of learning weight atter here.
5993800	6004800	The important part to realize is when we do when we use these energy based models is that in practice, even though we are learning the scalar energy function.
6004800	6014800	We only require the gradient of this energy function or more specifically the negative gradient of this energy function for sampling the model at the end.
6014800	6021800	We do not require the energy function itself, nor do we require the partition function depth setter.
6021800	6029800	By the way, this atter is implicitly defined through this energy itself that we're learning.
6029800	6039800	So now it turns out that in diffusion models, what we're basically learning is the energy gradients for all these diffuse data distributions directly.
6039800	6043800	We are not learning energies, but basically energy gradients.
6043800	6058800	And one thing I want to add is that because in this EDM spirit, we're directly learning these energies and we have the probability, an expression for the probability distribution while also taking into account this partition function.
6058800	6063800	Because of this training energy based models can be actually really complex.
6063800	6069800	This often requires advanced Markov chain Monte Carlo methods, which can be very difficult to deal with.
6070800	6073800	One of that is available, I would add.
6073800	6080800	But yeah, diffusion models, we kind of circumvent that and we only directly learn these energy gradients.
6080800	6084800	Tell me somehow show that and derive that maybe.
6084800	6103800	So to this end, let's recall again that in diffusion models, our neural network basically we're trying to approximate our model more generally, we're trying to approximate this score function of the diffuse data distributions qt of x.
6103800	6114800	Now let us suppose that our model is parametrized such that the diffuse data distributions qt are given by this energy based model here.
6114800	6115800	Right.
6115800	6121800	So now let us insert this p theta here and yeah through the map.
6121800	6129800	We apply the logarithm both here, both on e to the minus the scalar energy function and also the denominator.
6129800	6135800	However, this term drops out because the partition function does not depend on the state x.
6135800	6140800	And what we are left with is just a negative gradient of the energy.
6140800	6141800	What does this mean?
6141800	6148800	So this means that this neural network s that we usually have in diffusion models to model the score function.
6148800	6162800	It means that it essentially learns the negative energy gradients of the energy model based model that would describe the diffuse data distribution.
6162800	6180800	So yeah, once again, fusion models kind of circumvent these complications and directly model the energy gradients and say avoid modeling this partition function explicitly for instance which leads to some of these difficulties that we have in classical energy based model training.
6180800	6192800	Also, these different noise levels that we have in diffusion models. This is actually analogous to a mere sampling and energy based models.
6192800	6199800	I would like to talk about one more thing about diffusion models, which is unique identity.
6199800	6213800	It turns out that the denoising model that we're learning in these diffusion models that is supposed to approximate the score function of the diffused data to t of x p.
6213800	6222800	This denoising model is in principle uniquely determined by the data that we're given and the forward diffusion process.
6223800	6235800	And not only the score model, so and by learning the score model also these data encodings that we obtain by using the probability flow of the e to deterministically encode data in the latency space.
6235800	6240800	All this is uniquely determined by the data and the forward diffusion.
6240800	6265800	What this means is that even if we use different neural network architectures for us and different network initialization, we should at the end recover identical model outputs like identical score function outputs and data encodings in the latency space assuming we have sufficient training data model capacity and optimization accuracy.
6266800	6274800	This is in contrast, for instance, to generate adversarial networks or variational auto encoders which do not have this property.
6274800	6286800	Because these models, depending on what kind of architectures we use and what kind of initializations we use, we will always obtain like somewhat different models and yet different data encodings and so on.
6286800	6291800	This is the unique property about these diffusion models.
6291800	6307800	Here's an example. What we are seeing here is the first 100 dimensions of the latent code obtained from a random cyber tent image that was encoded in the latency space of a diffusion model using this probability flow or the e approach.
6307800	6317800	We did this, most specifically Song and I did this with two different models and model architectures that were separately trained.
6317800	6332800	However, both of these encodings distributions here as we see, they are almost the same, they are almost identical, even though these were different architectures.
6332800	6337800	With that, I would like to come to a conclusion and briefly summarize.
6337800	6346800	So in this part of this talk, I have introduced you to this continuous time diffusion framework in contrast to what Arash talked about in step one.
6346800	6353800	We do not have finite size denoising and diffusion steps anymore and only a finite number of cells.
6353800	6364800	Rather, we consider continuous perturbations, a continuous forward diffusion process and then also a continuous generative process based on differential equations.
6364800	6371800	And to train these models, we make connections to score matching, most specifically denoising score matching.
6371800	6376800	Now, maybe this appeared somewhat complex and mathematically involved.
6376800	6383800	However, why should he use this differential equation and continuous time framework?
6383800	6391800	It really has unique advantages as I have shown hopefully and hopefully I can convince you during this part of the talk.
6391800	6401800	Most importantly, this allows us to leverage this broad existing literature on advanced and fast SCE and ODE solvers when sampling from the model.
6401800	6409800	This can help us to really accelerate sampling from diffusion models, which is very crucial because they can be slow.
6409800	6426800	Furthermore, in particular, this probability flow ODE is very useful because it allows us to also perform like these deterministic data encodings and it also allows us to do like local ideal estimation like in continuous normalizing flows and so on.
6426800	6434800	Additionally, this is overall a fairly clean mathematical framework based on diffusion processes, score matching and so on.
6434800	6449800	And this allowed us to use these connections to neural ordinary differential equations to continuous normalizing flows and to energy based models, which I think provides a lot of insights into diffusion modeling.
6449800	6462800	With that, I would like to conclude my part and take the mic to Wicci who will now talk about advanced techniques, accelerated sampling, conditional generation and beyond.
6462800	6465800	Thank you very much.
6465800	6471800	Hi everyone. I'm Richie from Google Brain Team. So let's continue our study on diffusion models.
6471800	6482800	So in the third part, we're going to discuss several advanced techniques of diffusion models which corresponds to accelerating sampling, conditional generation and beyond.
6482800	6491800	So here's an outline of what we're going to cover in this part. Basically want to address two important questions of diffusion models with advanced techniques.
6491800	6508800	The first one is how to accelerate the sampling process of diffusion models. We're going to tackle this question from three aspects, advanced forward process, advanced reverse process and advanced modeling, including hybrid models and model distillation.
6508800	6514800	The second question is how to do a high resolution, optionally conditional generation.
6514800	6530800	And I will talk about several important techniques to make this happen, especially the general conditional diffusion modeling framework, the classifier classifier free guidance, as well as cascade generation pipeline.
6530800	6536800	Let's start from the first question, so how to accelerate the sampling process of diffusion models.
6536800	6547800	To see why this question is important, let's consider what makes a good generative model. So in principle, we want a good generative model to enjoy the pulling three properties.
6547800	6562800	First, it should be fast to sample from this generative model. Second, we would expect the general model capture most of the major modes of the data distribution, or in other words, they should have adequate sample diversity.
6562800	6568800	And third, of course, we want the general model to give us high quality or high fidelity samples.
6568800	6584800	However, there is a generative learning dilemma for existing generative model frameworks. For example, for generating several networks, they are usually fast to sample from, and they can give us high quality samples.
6584800	6595800	However, because of this discriminative learning framework, there is a decent chance that GANS may miss certain modes of the data distribution.
6595800	6604800	And the other type of generative models are these likelihood based models, like variational autoencoders or normalizing flows.
6604800	6614800	So those models are usually optimized by maximizing likelihood or maximizing a variant of likelihood, for example, the evidence lower bound.
6614800	6626800	So usually this type of models are good at fast sampling, and they are able to capture certain modes of the data distribution because of this maximum likelihood learning framework.
6626800	6631800	However, usually they lead to subpar sample quality.
6631800	6644800	On the other hand, the diffusion models are good at both coverage because they are also optimizing the evidence lower bound of log likelihoods, and they are able to generate high quality samples.
6644800	6654800	However, the sampling of diffusion models is pretty slow, which usually requires thousands of functional calls before getting a simple batch of samples.
6654800	6666800	So if we can find techniques to accelerate the sampling process of diffusion models, we will get a generative model framework, which enjoys all those three great properties.
6666800	6672800	That is, we can tackle the dilemma of this generative learning framework.
6672800	6680800	Before I do that, before diving into details, I would like to recap the general formulation of diffusion models.
6680800	6692800	So for diffusion models, they usually define a simple forward process which slowly maps data to noise by repeatedly adding noise to the images.
6692800	6703800	The forward process is defined to map data, or to map noise back to data, and this is where the diffusion model is defined and trained.
6703800	6721800	In terms of the diffusion model, it is usually parameterized by a unit architecture, which takes the noisy inputs at certain time step, and then it tries to predict the clean samples, or it tries to predict the noise added to this noisy inputs.
6722800	6734800	So if we think about accelerating sampling, there are some naive methods that immediately come to our mind. For example, in training, we can reduce the number of diffusion time steps.
6734800	6754800	In sampling, we sample every K time step instead of going over the whole reverse process. However, those naive acceleration methods will lead to immediate voice performance of diffusion models in terms of both the sample quality as well as the likelihood estimations.
6754800	6761800	So we really need something clever, cleverer than those naive methods.
6761800	6774800	And more precisely, we want to ask, given a limited number of functional calls, which are usually much less than thousands, so how to improve the performance of diffusion models.
6774800	6790800	And as a side note, although the following techniques I'm going to discuss take this accelerated sampling as the main motivation, they definitely provide more insights and contributions to diffusion models as we will see soon.
6790800	6806800	So we will answer this question from three aspects. The first one is advanced forward process, and second is advanced reverse process, and then lastly the advanced diffusion models.
6806800	6828800	So first let's take a look at some advanced forward process. So recall that the original whole process defines a Markov process, where we start from this x zero, it is the clean sample, and we gradually add noise until it goes to this x big T corresponding to white noise signal.
6828800	6842800	So QXT, QXT minus one is simply a Gaussian distribution, and this beta T defines the noise schedule, and they are hyper parameters that are predefined before training the models.
6842800	6854800	So we are interested in the following questions. So first, does this forward process or noise schedule have to be predefined? And does it have to be a Markovian process?
6854800	6868800	And lastly, does it, is there any faster mixing diffusion process? The faster mixing is an important concept in Markov chain Monte Carlo, as we will discuss later.
6868800	6882800	For the first work I would like to discuss here is this variational diffusion models. It basically enables us to learn the parameters in this forward process together with the rest of parameters in the diffusion models.
6882800	6902800	In this case, we can really learn the forward process. So more specifically, given the forward process defined by QXT given x zero, it follows a Gaussian distribution with square root alpha T bar x zero as the mean and one minus alpha T bar as the variance.
6902800	6906800	So this is the formulation we have learned in part one.
6906800	6923800	And this work proposed to directly parametrize the variance one minus alpha T bar through a learnable function gamma eta. And this function is definitely by a similar function to ensure that the variance is within the range from zero to one.
6923800	6936800	Gamma eta T is further parametrized by a monotonic multilayer perceptron by using strictly positive ways and monotonic activations, for example, sigmoid activations.
6936800	6954800	And recall that in part one, we have learned that these diffusion models are directly connected to hierarchical variational auto encoders in the sense that diffusion models can be considered as a hierarchical variational encoders but with fixed encoder right.
6954800	6971800	And this model is named as variational diffusion models because it is even more similar to hierarchical variational encoders because we are optimizing the parameters in the encoder together with the parameters in the decoder.
6971800	6993800	And to optimize the parameters of the forward process, this paper further derive new parametrization of the training objectives in a boring sense. So basically they have shown that optimizing the variational upper bound of the diffusion models can be simplified to the following training objectives.
6993800	7005800	Note that this gamma eta participates in this weighting of like different L2 norm of different time steps.
7005800	7025800	This is the training objectives they derive for this great time session. And they have shown that by learning this noise schedule, it actually improves the likelihood estimation of diffusion models a lot, especially when we assume that there are fewer diffusion time steps.
7025800	7039800	Note that in the second part, we learned that the diffusion models can be interpreted from the perspective of stochastic differential equation and we learned the connection between diffusion models and denoting score matching.
7039800	7058800	This gives us a hint or a knowledge that the diffusion models can also be defined in the continuous time setting. And in this paper, they explicitly derive this variational upper bound in the continuous time setting with this gamma eta notation.
7058800	7075800	Basically, they show that when we let this big T goes to infinity, meaning like we have infinity amount of diffusion time steps, this corresponds to a continuous time setting, and then the variational upper bound can be derived in the following formulation.
7075800	7091800	And here the only difference is that the weighting term of different L2 terms, L2 distance at different time steps equals to the derivative of this gamma eta t function over time t.
7091800	7104800	And more interestingly, this paper shows that if we define the signal to noise ratio equals to alpha t bar minus one divided by one minus alpha t bar.
7104800	7113800	And then this L infinity is only related to the signal to noise ratio at the endpoints of the whole forward process.
7113800	7131800	And it is invariant to the noise schedule in between the endpoints. So if we want to optimize the forward process in the continuous time setting, we only need to optimize the signal to noise ratio at the beginning and the end of the forward process.
7131800	7146800	And they further show that the in-between process can be learned to minimize the variance of the training objective. And this enables the faster training of diffusion models besides faster sampling.
7146800	7156800	And another contribution of this work is that they show it is possible to use diffusion models to get state of the art likelihood estimation results.
7156800	7174800	So before this work, the benchmark of likelihood estimation have been dominated by autoregressive types of models for many years as shown in this figure, but this model shows like actually we can use diffusion models to get a big improvement out of the autoregressive model process.
7174800	7188800	And one key factor to make this happen is to add further features to the input of the unit. And this, those four features can range from low frequency to very high frequencies.
7188800	7204800	And the hypothesis for the assumption here is that to get good likelihood estimation, the model you really need to model all the bits or all the details in the input signal, either they are in perceptual or perceptual.
7204800	7220800	However, new networks are usually bad at modeling small changes to the inputs. So adding those four features, especially those high frequency components can potentially help the network to identify those small details.
7220800	7236800	And the paper found that this trick doesn't bring like much significant improvements to the autoregressive baselines. However, it leads to significant improvements in likelihood estimation for diffusion model class.
7236800	7255800	Okay, so next, like paper or method I'm going to discuss is this denoting diffusion implicit models. So in this work, the main idea is like they try to define a family of non markovian diffusion processes and the corresponding reverse processes.
7255800	7265800	And those processes are designed such that the model can still be optimized by the same surrogate objective as original diffusion models.
7265800	7282800	We call that this is the surrogate objective right so this L simple where we remove the weighting of each L2 loss term and we just simply take the average of the L2 loss at different time steps.
7282800	7298800	And then, because they can optimize by the same surrogate objective. So one can simply take a pre train diffusion model and treat it as the model of those non markovian diffusion processes, so that they are able.
7298800	7310800	We are able to use the corresponding reverse processes to reverse the model, which means like we will have more choices of our sampling procedure.
7311800	7322800	Okay, so to see how we can define those non markovian forward processes, let's recap the duration of the KL divergence in the variational lower bound.
7322800	7343800	So we have this LT minus one is defined as the KL divergence between this posterior distribution QXT minus one given XT and X zero, and this denoting distribution P theta XT minus one given XT so this P theta is parameterized by the diffusion model.
7343800	7362800	And because these two distribution, like both of them are Gaussian distributions with the same variance sigma T square, and this can be written as the outer distance between the the mean of these two distributions times a constant.
7362800	7382800	Note that these two mean function have been parameterized by like simple linear combination of XT and if so, or the combination of XT and the predicted if so, so if some listen noise added to the queen sample to get XT.
7382800	7398800	We can further write this KL divergence in the form of lambda T times the outer distance between the true noise if some and the predicted noise if some theta by our diffusion models.
7398800	7416800	And if we really think about the duration of this LT minus one, we found that if we assume like this lambda T can be after values, because in a surrogate objective we simply set it to one right so it doesn't matter what value this alpha T is originally.
7416800	7428800	So then we can find that about formulation holds as long as first we have this QXT give X zero, it follows this normal distribution.
7428800	7447800	And we need to make sure that this XT still equals to this formulation. And we have those two assumptions. First, the follow process, like the posterior distribution QXT minus one, give XT and X zero follows a Gaussian distribution.
7447800	7453800	And the meaning of this distribution is a linear combination of XT and the epsilon.
7453800	7468800	And our reverse process to be similar to the posterior distribution, which means it is also a Gaussian, and this new theta is the same linear combination of XT and the predicted noise.
7468800	7483800	So we can further rewrite, because we know XT equals to a linear combination of X zero and epsilon. So we can replace this epsilon, epsilon by the linear combination of XT and X zero.
7483800	7500800	And for the reverse process, we can rewrite it as a linear combination of XT and predicted X zero hat. So this X zero hat is defined as the predicted clean sample, given the predicting noise.
7500800	7519800	So if we make those three assumptions, then we can see that the about duration of our T minus one still holds, which means that we don't really need to specify this QXT, even XT minus one, and we don't need to require to be a common process.
7519800	7526800	All we need to assume is the posterior distribution of XT minus one, even XT and X zero.
7526800	7542800	And that's the basic, the, the insights of this, this, like how we can define the non-marcovian forward process, which leads to the same chain objecting as the original diffusion model.
7542800	7558800	Specifically, so this is the original diffusion process. And now the diffusion process change to the right diagram, where for each XT, it depends on both the XT minus one and the X zero.
7558800	7577800	And another remaining question is how we specify the linear combination parameters A and B. So note that here we need to specify this A and B such that this QXT given X zero still follows this Gaussian distributions.
7577800	7595800	At this end, this work defines a family of forward processes that meets the above requirements, which corresponds to specifying the posterior QXT minus one given XT and X zero in this formulation.
7595800	7606800	So we can similarly define the corresponding reverse process by just replacing this X zero to a predicted X zero hat.
7606800	7623800	And note that this specifying specification of formulation doesn't require like a specific value of this sigma t-tutor, which means like this sigma t-tutor can be literally arbitrary values.
7623800	7634800	So that's why it depends, it actually defines a family of forward processes with different values of sigma t-tutor.
7634800	7655800	And more importantly, if we specify like sigma t-tutor to be zero for all the time steps, this leads to this DDI and sampler where we wish is a deterministic reverse process because this variance is zero here.
7655800	7666800	And the only randomness comes from the like starting point of the reverse process, which is the starting white noise signal.
7666800	7680800	And recall that in the second part, we also build connection between the like stochastic reverse process with a probability flow ODE, which is corresponds to a deterministic generative process.
7680800	7693800	And we can also interpret the DDI and sampler in a similar way. Specifically, this DDI and sampler can be considered as an integration role of the following ordinary differential equation.
7693800	7706800	And note that here we do a bit of change of a variable where we define this x bar equals to x divided by the scaling factor square root of alpha bar.
7706800	7714800	And we define this eta to be basically the square root of the inverse signal to noise ratio.
7714800	7731800	And if we assume this sigma, this epsilon theta to be the optimal model, and then this ODE is equivalent to a probability flow ODE of a variance is floating SDE, which is in the following formulation.
7731800	7746800	Note that although these two are equivalent, the sampling procedure can still be different, because for the above ODE, we are taking like the sampling process over this eta t.
7746800	7759800	Well for the second formulation we are taking the, for example, for both of the two equations we use the standard Euler's method, then the second one is taking the Euler step over dt.
7759800	7777800	And basically, in practice, people find that the first one works better than the second one, because it depends less on the value of t, but it depends directly on the signal to noise ratio of the current time steps.
7777800	7788800	We also found that with this DDM sampler, we are able to use less time sampling steps, but reach better like performance.
7788800	7806800	And in terms of why this is true, this, this paper by Carols et al, argues that the ODE of the DDM is favored, because I saw in those two in those three illustrations, especially the third illustration.
7806800	7824800	The definition of the solution trajectories of DDM always points towards the denoider outputs, while for the first two ODE formulation, the variance preserving ODE and the variance is pulling ODE, which are two very commonly used ODE formulation
7824800	7826800	diffusion models.
7826800	7832800	They basically have more like high coverage regions along the trajectories.
7832800	7842800	So for the DDM, we can see like for the solution trajectories, most of the trajectories are linear and with low coverage.
7842800	7858800	And it is known that low coverage really means less truncation errors accumulating over the trajectories. So if we use this kind of trajectories, like we will have a smaller chance to accumulate more errors across the trajectories.
7858800	7870800	Thus, enable us to use like fewer number of diffusion time step, fewer number of sampling steps in inference.
7870800	7880800	Okay, so the third word I'm going to discuss for advanced, the forward process is just critically down to long-distance diffusion model.
7880800	7891800	Basically, they are trying to find a faster mixing diffusion process by using certain like background knowledge from Markov Chen and Monte Carlo.
7891800	7903800	And how this forward process is related to MCMC, let's see, like this is a regular forward diffusion process, which is a stochastic differential equation.
7903800	7918800	And it is actually a special case of overdone long-distance dynamics. If we assume the target distribution or the target density of this MCMC is this standard Gaussian distribution.
7918800	7929800	Given this connection, we can actually design more like efficient forward process in terms of MCMC.
7929800	7942800	Specifically, this word proposed to introduce an auxiliary variable, this velocity V, and the diffusion process is defined on the joint space of this velocity and the input.
7942800	7958800	And during the forward process, the noise is only added in this velocity space. And this image space or input space is only erupted by the coupling between this data and the velocity.
7958800	7973800	And the resulting process as showing this figure, we can see that the forward process in the V space is still exact. However, the process in the image space or the data space are much more smoother.
7973800	7998800	This V components is analogous to the Hamiltonian components in HMC or analogous to the momentum in momentum based optimizers. So by defining this joint space or defining the diffusion in this V space, it actually enables faster mixing and faster traverse of the joint space,
7998800	8008800	which enables fast, like more smoothly and efficient, more smooth and efficient forward process.
8008800	8021800	And the second, let's see some advanced reverse process. So before that, we would like to ask a question. So remember that we use this normal approximation of the reverse process, right.
8021800	8036800	It seems like the denoting distributions are always Gaussian distributions. But if we want to use less diffusion time steps, is this normal approximation of the reverse process still true or accurate?
8036800	8048800	Unfortunately, the answer is no. So this assumption, normal assumption in this denoting distribution holds only when the adjacent
8048800	8063800	the noise added between these adjacent steps are small. If we want to use less diffusion time steps in training, then we can see that this denoting distribution is not a unimodal normal distribution anymore.
8063800	8076800	So they are tend to be like multimodal and more complicated distributions. So in that case, it means like we really need more complicated functional approximators.
8076800	8085800	So we will talk about two examples of how we can include more complicated functional approximators here.
8085800	8095800	The first word is this denoting diffusion gains. So in this word, they propose to model the denoting distribution by conditional gain model.
8095800	8110800	Specifically, the model is training this other several learning framework. And we first get the samples xt minus one and xt by running this forward diffusion process.
8110800	8130800	And then this generator takes xt as input, as well as the time step as input. And it's actually trying to model the xt minus one. But instead of just directly outputting xt minus one from the network, it's first trying to predict this
8130800	8149800	screen sample x0. And then it tries to sample xt minus one from this tractable posterior distribution. And then the discriminator takes the real xt minus one and the fake xt minus one prime as input and try to discriminate these two samples.
8149800	8157800	And again, this discriminator is also conditional xt and the corresponding time step t.
8157800	8165800	One may ask, what is the benefit of this denoting diffusion gains compared to a one shot gain model.
8165800	8177800	But this paper shows that because like right now the conditional gains only need to model like the conditional distribution of xt minus one given xt.
8177800	8187800	This turns out to be a much simpler problem for both the generator and the discriminator compared to directly model the model distribution of the clean sample.
8187800	8200800	And this simple training objective for the two models leads to stronger mode coverage properties of gains and also leads to better training stability.
8200800	8216800	And recall that in the second part we learned that there's a close connection between energy based models and diffusion models. So, therefore, a natural idea is like we try to approximate the reverse process by conditional energy based models.
8216800	8230800	Recall that an energy based model is in the form of P beta x. It is solely dependent on the normalized log density function which is f theta x.
8230800	8245800	And we can further prime trend is f theta x by minus e theta x. So usually, people call this e theta x as the energy function. And this d theta is the partition function which is usually analytical and tractable.
8246800	8256800	And we can parameterize this f theta by a neural network which takes the signal as input and output as scalar to represent the value of this f.
8256800	8265800	And the learning of energy based models can be illustrated as follows. So suppose this is the energy landscape defined by the e theta function.
8266800	8280800	And after learning, we would like to put the observation data points into the regions of low energy and put all the other inputs to the regions of the high energy.
8280800	8298800	And optimizing the energy based models require, you really require the MSMC sampling from the current model p theta x as showing this formulation, which is really highly computational expensive, especially for high dimensional data.
8299800	8315800	So if we want to parameterize the denoising distribution by conditional energy based model, we can start by assuming like at each diffusion time step marginally the data follows a energy based model in the standard formulation.
8315800	8323800	So here I removed the, the script like the time step script for similar simplicity.
8323800	8327800	And let x theta be the data at a higher noise level.
8327800	8337800	So we can derive the conditional energy based models by Bayes and Rowe, but specifically this p x, the x theta is in this formulation.
8337800	8348800	And if we compare this conditional energy based models with with the original marginal energy based models, we see that the only difference is that there's an extra projected term here.
8348800	8370800	And this actual projected term actually has the effect of localize this highly multimodal energy landscape to a like more single mode model or uni model landscape, and with the model mode focus around the higher noise level signal x theta.
8371800	8388800	Therefore, compared to training a single energy based model the sampling here is more friendly and easier to converge because the energy landscape compared to the original marginal energy landscape is more uni model and more and simpler.
8388800	8397800	So that the training could be more efficient and the conversion and CMC can give us well formed energy potential after training.
8397800	8415800	And compared to diffusion models, this energy based to using this energy based model to a parameterize the denoting distribution can give enables us to define like much less diffusion steps up to six steps.
8415800	8424800	And more specifically to learn those models we simply maximize the conditional log likelihoods at each time step.
8424800	8436800	And then after training we just get samples by progressive sampling from the energy based models from high noise levels to low noise levels.
8436800	8449800	So the last part comes to the advanced diffusion models, basically want to ask two questions. For first, can we do model distillation so that the distilled model can do faster sampling.
8449800	8457800	And second, can we lift the diffusion model to a latent space that is faster to diffuse.
8457800	8470800	So the first idea comes to the distillation so here I want to discuss one representative work in this domain, which is this progressive distillation of diffusion models.
8470800	8479800	So essentially this work proposed to distill a deterministic DDIM sampler to the same model architecture of the original model.
8479800	8498800	And it's, it is, it went into this progressive pipeline, in a sense that at each distillation stage, we will have a teacher model, and, and we will learn a student model and this student model is learned to distill.
8498800	8510800	By each two adjacent sampling steps of the teacher model to one sampling step of the student model. And after learning this student model at next distillation stage.
8510800	8520800	The student model as a previous stage will serve as the teacher model at this new stage, and then we learn another student model at the new stage.
8520800	8530800	So we need this process until we can distill the original thousands of sampling steps to a single sampling step.
8530800	8538800	And implementation wise, the learning of the student model is quite similar to the original diffusion model training pipeline.
8538800	8548800	The difference is how we define this training target of the diffusion model, specifically, given the teacher model.
8548800	8556800	And we randomly sample a time step T, and then we draw, we run the sampler for two steps.
8556800	8567800	And then the target is the, is computed to make sure that the student model can reproduce the two sampling step within one sampling step.
8567800	8584800	And then the loss is defined as Euro where we minimize the out to distance between this target and the predicted, like X hat from this diffusion model or from the student model.
8584800	8595800	And after that, we have in the number of sampling steps and repeat this process until we reach one sampling step.
8595800	8604800	Another idea is like whether we can leave the diffusion models to a latent space, which is more friendly to this diffusion process.
8604800	8616800	And here's an example of this kind of idea where we can try to leave the diffusion models to a latent space of a pre trained variation of encoder.
8616800	8633800	In the latent space, the distribution of the data in this latent space is already quite close to the Gaussian distributions, which means like we can definitely use less diffusion time steps to diffuse the data in this latent space.
8633800	8647800	The advantages are pretty straightforward. So first, because this latent space already close to normal distribution, we are able to use less diffusion time steps to enable faster sampling.
8647800	8659800	And then compared to the original variational auto encoders, which assume that the prior distribution of the Z follows a single and simple Gaussian distribution.
8659800	8675800	This kind of hybrid model assume that the PFC is modeled by a diffusion models, which means that it has a diffusion prior. So it definitely will be much more expressive compared to the original variational auto encoder model.
8675800	8690800	So we can actually record that for the current stage that diffusion models only defining a continuous data space. However, there are more domains which may have more complicated data structure.
8690800	8706800	So this data type, as long as we can find an auto encoder model which are tailored to that data type and can map the data input to a continuous latent space, we will be able to apply the diffusion models to that latent space.
8706800	8716800	So these give us more possibilities to apply diffusion models to different modalities and different data types.
8716800	8731800	And a bit of the detailed formulation. So in this work, again, we optimize the model in terms of by minimizing the variational upper bound of the negative log likelihood.
8731800	8745800	The objective contains three terms. The first two terms are similar to the variational auto encoder objecting. And the third term corresponds to the training objective of the diffusion models.
8745800	8757800	And it actually corresponds to, we treat the encoding latents from this QZ0 given X as the observed data of the diffusion models.
8757800	8768800	And in that way, we can derive the similar training objective of the diffusion models as the original one. So we first do this random sampling of time step.
8768800	8782800	And then we draw samples from this forward diffusion. And then we have this diffusion kernel. This is the forward, this is from the forward process. And then we learn this score function for DT.
8782800	8789800	And of course we have some constant that is irrelevant of the model parameters.
8789800	8797800	Okay, so the second question we want to answer is how to do high resolution optionally conditional generation using diffusion models.
8797800	8813800	In the past two years, we have seen many impressive conditional generation results using diffusion models. For example, this style II and imagine recruits diffusion models to do high resolution text to image generation.
8813800	8822800	And another examples includes the using conditional diffusion models for super resolution or colorization.
8822800	8830800	Panorama generation is another example where we take a small size input but generate this panorama.
8830800	8843800	So how can we do that? Let's first take a look at the general formulation of conditional diffusion models, which is pretty straightforward. So the only modification we need to make is in this reverse process.
8843800	8859800	We can let this denoting distribution to incorporate an additional input, which is this condition C. And this corresponds to modify the mean of this Gaussian distribution to take an additional input C.
8859800	8866800	And optionally, we can also let this variance to be learned and it takes an input C.
8866800	8886800	But in practice, like most mostly we still just use this C in this mean and the variation of upper bounds only includes a small change, which lies in this KL divergence where we plug in this new formulation of the denoting distribution.
8886800	8897800	But it is a design arc in terms of how to incorporate different types of conditions into the unit, which is used to parameterize this new data.
8897800	8912800	Specifically, like these are the things people use in practice for scalar conditioning. For example, class label, we can do something similar to what we did for time step conditioning.
8912800	8927800	Specifically, we can encode the certain scalars to a vector embedding, and then we simply add the embedding to the intermediate layers of the unit, or we do this adaptive professionalization layers.
8927800	8939800	And if it is an image conditioning, we can do channel wise concatenation of the conditional image and the input image. And if it is for text conditioning,
8940800	8952800	this contains two cases. First, if the text, we embed the text to a single vector, then we can do something similar to the vector derived from the scalar conditioning.
8952800	8964800	And if we embed this text to a sequence of vectors, then we can consider using cross attention with the intermediate layers of the unit.
8964800	8972800	And for a high resolution conditional generation, another important component is this classifier guidance.
8972800	8984800	The main idea is like recall the diffusion model correspond to learning the score of a probability, for example, the score of log px.
8984800	8999800	And right now, because we incorporate the class condition, which means like the diffusion model actually gives us a score of a class conditional model p of xt given c.
8999800	9013800	And given this, we can train an additional classifier, which gives us the probability of c given x, and then we mix the gradients of these two models during sampling.
9013800	9025800	And this corresponds to, we sample from actually a modified score, which corresponds to the gradient of log px given c plus omega times log pc given x.
9025800	9042800	And this omega controls the strength of the guidance. And it actually corresponds to approximate sampling from the distribution, which is proportional to p of x given c times p of c given x to the omega power.
9042800	9065800	And in practice, it corresponds to we modify the normal distribution where we sample from and the mean of this normal distribution corresponds to the mean predicted by the score model or predicted by the diffusion model, plus the gradients from the classifier.
9066800	9083800	And if we use larger omega, then the samples will be more concentrated around the modes of this classifier, which really leads to better individual sample quality, but if we use too large omega, it will reduce the sample diversity.
9083800	9094800	So one really needs to find a sweet point for this omega to best balance the individual sample quality and sample diversity.
9094800	9105800	And one downside for this classifier guidance is that we need to train an additional classifier to do that, right, so that adds additional model complexity.
9105800	9124800	So inspired by that work. This works tries to introduce a classifier free guidance, meaning that we can actually get an implicit classifier by joining training a conditional and unconditional diffusion model in a pulling sense.
9124800	9146800	So suppose we have this PX given C, where the the score can be derived by a conditional diffusion model, and we also have the score of a unconditional model p of x, and then we can derive an implicit classifier PC given x which should be proportional to p of x given
9146800	9161800	divided by p of x. And in practice, the gradient or the score of these two probability are estimated by randomly dropping the condition in the diffusion models at a certain chance for each iteration.
9161800	9176800	And similarly, we can derive the modified score with this implicit classifier. We call this is the original modified score. And now we replace the log PC given x by log PX given C minus log PX.
9177800	9192800	And this is the resulting modified score we will use with this classifier free guidance where this log PX given C and this log PX are both estimated by the single diffusion model.
9193800	9216800	And in this three panels, we can see the trade off between sample quality and sample diversity more clearly. So from left to right correspond to we gradually increase the strength of the guidance, and we can see clearly individually speaking, the sample quality of each image increases.
9217800	9228800	However, the samples that look like more similar to each other if we use a large classifier or classifier free guidance.
9228800	9246800	And the last thing I want to talk about is this cascade generation pipeline, which are important to high resolution generation. And this kind of idea have already been explored by other type of generating models for, for example, game model.
9246800	9263800	And it's pretty straightforward. So we've started by learning an unconditional diffusion model at the lowest resolution. And then we've learned several super resolution models, taking the down sampled training images at lower resolution as the condition.
9263800	9279800	And during sampling, we just run the progressive sampling pipeline starting from the smallest unconditional model and going, going through all those super resolution models until we reach the highest resolution.
9279800	9300800	But one notorious problem of this kind of cascaded training pipeline is this compounding error problem. It's more specifically record that during training, the conditions we fit into the super resolution model is the down sampled version of the training images from the data set.
9300800	9321800	However, during sampling, the conditions we fit to those super resolution models are actually generative samples of from the low resolution models. So if there are certain artifacts in the samples from the low resolution models, those artifacts or inaccurate samples will affect the
9321800	9332800	the sample quality of the super resolution models as well, because of this mismatch issue between the conditions in training and condition in inference.
9332800	9354800	To activate this problem, this noise conditioning augmentation is proposed in reference works. Basically, during training, we try to degrade the conditioning low resolution images by adding variance amount of Gaussian noise, or just blur those images by Gaussian kernel.
9355800	9365800	And during inference, we sweep over the optimal amount of noise added to the low resolution images, which are the conditions to the super resolution models.
9365800	9387800	So the idea or the hypothesis is like if we try to reduce certain amount of information from the condition, then the super resolution model will be trained to be more robust to different type of artifacts when we send like the conditions as the samples.
9387800	9408800	And later on, more complicated degradation process have been proposed. For example, we can add a sequence of different types of degradation operations to the image, the low resolution image before sending as a condition to the super resolution models.
9409800	9431800	Okay, so here's a summary of these parts. So in this advanced techniques session, we learn to answer two questions. The first one is how we can accelerate the sampling process, and we introduce several important techniques from the aspects of advanced forward process, reverse process and modeling itself.
9431800	9449800	And the second question is how we can do high resolution conditional generation using diffusion models, and we discuss the general framework of conditional diffusion models, classifier and classifier free guidance, as well as cascade generation pipeline.
9449800	9458800	So in the application section, we will see how all those techniques will benefit in terms of various tasks.
9458800	9473800	Hi, everyone. Welcome to the first section of applications of diffusion models. So in this section, we're going to study applications of diffusion models in terms of image synthesized control generation as well as text to image generation.
9473800	9485800	So let's run text to image generation. So in the past two years, this task has been shown to be extremely suitable for diffusion models to work on.
9485800	9497800	So basically, this task is the inverse of the image captioning tasks, where we are given a text prompt C and we are trying to generate high resolution images X, as shown in this video.
9497800	9506800	So this video shows the generative image images by a text to image generation model called imagine as we will show later.
9507800	9526800	And let's start from this glide model by opening in last year. So this is essentially a cascading generation diffusion models, which contains 64 by 64 base model, and a 64 by 64 to 56 by 256 super resolution model.
9527800	9541800	And they have tried to use classifier free guidance and clip guidance. So I will talk about the clip guidance in details later, and they generally found that classifier free guidance works better than the clip guidance.
9541800	9556800	And those figures shows the generative samples from this model. And as we can see, the model is capable of generate fun, normal conversations of concepts that have never been seen from the data set.
9556800	9565800	For example, a hedge dog using a calculator and robots meditating in a vipassana retreat or etc.
9565800	9573800	So a bit introduction of the clip guidance, it can be treated as a special form of the classifier guidance.
9573800	9588800	And in terms of a clean model contains two components, a text encoder, G, and image encoder F, and during training batches of the image and caption pairs assembled from a large datasets.
9588800	9609800	And the model optimize a contrasting cross entropy loss, which encourages high dog product between this F and G, if the image X and C comes from the same image caption pair, and it encourages low product if X and C comes from different image caption pairs.
9609800	9628800	And it can be proved that the optimal value of this FX times GC is given by log PXC divided by P of X times P of C, which equals to log P, C given X minus log PC. So given this conclusion,
9629800	9645800	we will be able to use this clip model as the classifier in the classifier in the classifier guidance in the following sense. So recall that for the classifier guidance, we want to modify the score in this formulation.
9645800	9663800	And we can consider augment the augmenting the second term by a minus log PC term, because when we take gradient over X, then this part just disappeared. And then we can see that these two terms together can be modeled by a clip model.
9664800	9679800	So basically replace this part by the dog product between FX and GC. So that is the clip guidance. However, in Glide they show that the clip guidance is less favored compared to the classifier free guidance.
9680800	9701800	And besides pure text-to-image generation, the Glide has shown that it is possible to fine-tune the model for text-guided impending tasks. Basically, they try to fine-tune the trained text-to-image Glide model by fitting randomly occluded images with an additional mask channel as the input.
9702800	9723800	So using this fine-tune model, they are able to change or do image editing by changing the prompt. For example, given an old car in a green forest, they will be able to edit the background to a snowy forest. Similarly, they can add a white hat to a man's hat.
9724800	9744800	And later on, this DAO-E2 further scale up the Glide model to support 1K by 1K text-to-image generation. And this DAO-E2 has been shown to outperform the first version of text-to-image 1K by 1K generation by OpenEye, which is this DAO-E, which is an unregressive transformer-based model.
9745800	9767800	And in terms of the model components of DAO-E2, it's built up on a pre-trained clip model. More specifically, a clip model is first pre-trained, and the image embedding and text embedding are grabbed from this pre-trained clip embedding and frozen.
9768800	9789800	And after that, this pipeline has been built to generate images from text. Basically, this generation model contains two parts. The first is a prior model. This prior model tries to produce clip image embeddings conditioned on the input caption.
9790800	9799800	And then the second part is a decoder part, which produces the images conditioned on the clip image embedding as well as the text.
9800800	9812800	So one natural question is why we want to condition the decoder on the clip image embeddings, right? So why not we just directly condition this decoder on text only?
9812800	9835800	So the hypothesis here is that for the total amount of entropy of an input signal, for example, images, so there's certain part that captures the high-level semantic meanings while there's still a large proportion of the entropy, which corresponds to just low-level details, either perceptual-visible or even perceptual-invisible.
9836800	9850800	So the hypothesis is that clip image embeddings tend to have a higher chance to capture the high-level semantic meaning of an input signal, especially those related to the caption information.
9851800	9860800	And by conditioning on this high-level semantic meaning, the decoder is able to capture or catch up those low-level details of the images more quickly.
9861800	9876800	And later on, this paper also shows that this by-part later repetitions of the clip image embedding as well as the latency in the decoder model enables several text-guided image manipulation tasks, as we will show later.
9877800	9883800	And a bit more details of the model architecture of the prior and the decoder models.
9884800	9896800	For the prior, the paper tries two options. The first one is the auto-regressive pair, where they quantize the image embedding to a sequence of discrete codes and predict them auto-regressively.
9897800	9911800	And the second option is to model the prior by diffusion models, where they directly train diffusion models based on the continuous image embedding as well as the caption input.
9912800	9916800	And the paper shows that the second option gives better performance.
9917800	9927800	And in terms of the decoder, it's again a cascaded diffusion models, which contains a one-base model and two super resolution models.
9927800	9939800	And to save the compute and make the training more efficient, the largest super resolution model is trained on image patches of one quarter size.
9939800	9946800	But during inference, the model will take the full resolution inputs and directly do the inference on the full resolution.
9947800	9956800	And this paper also shows that the classifier-free guidance and noise conditioning augmentation are super important to make the decoder work well.
9957800	9969800	And a little bit more detail about the bipartisan latent representations. So given an input image, we can get the bipartisan latent representations in the following sense.
9969800	9971800	So it contains two parts.
9971800	9980800	First is this latent variable Z, which is the clip image embeddings, and it can be derived by running the clip image encoder.
9981800	9985800	And the second part is this xt, which is the latency from the decoder.
9985800	9993800	And this part can be derived by running the inversion of an ddim sampler for the decoder.
9993800	10005800	And after getting these two latent representations, the paper shows that it is possible to run this decoder and get near-perfect reconstruction of the original input image.
10005800	10014800	And given these bipartisan representations, the paper shows that it is possible to do several image manipulation tasks.
10014800	10030800	For example, this image variation tasks target at getting multiple variants of an input image, while with the hope of preserving the high-level semantic meanings of the input image.
10030800	10041800	And this is achieved by fixing the clip embedding Z, while changing to different latent xt in the decoder.
10041800	10050800	And as shown in this image panel, the first one is the input image, and the rest are the image variants generated by .e2.
10050800	10058800	And we can see that certain high-level semantic meanings are preserved, for example, the artist's style.
10058800	10068800	And this clock is preserved in all the image variants, but with different details in the image variants.
10068800	10072800	And the second task is this image interpolation task.
10072800	10082800	So given two input images, it's possible to use .e2 to do interpolation by interpolating the image clip embeddings of these two input images.
10082800	10091800	And we can get different interpolation trajectories by using different xt along these trajectories, as shown in those three rows.
10092800	10106800	And as we can see, although the trajectories are different, but the high-level semantic meanings are kept well for the two input images for all those three interpolation trajectories.
10106800	10123800	And the last task, the most interesting task that they show that can be done by .e2 is this text div task, which means like given an input image and the corresponding description, we would like to add this image towards a different prompt.
10124800	10129800	And this corresponds to an arithmetic operation in the latent space.
10129800	10139800	More specifically, they first try to compute the difference between the text clip embeddings of the original prompt and the target prompt.
10139800	10147800	And then they try to change the image clip embedding of the given input towards the difference between the text prompts.
10147800	10157800	And using this approach, they show that it's possible to do this text-guided image editing by changing the prompt.
10157800	10165800	And the last task to image diffusion model I want to talk about is this imagined model by Google Brain Team.
10165800	10168800	So again, the task is the same as .e2.
10168800	10178800	So we are given some text prompts as input, and we are trying to output 1k by 1k images aligned with the input text.
10178800	10181800	And the highlight of imagined model is as follows.
10181800	10196800	First, it provides an unprecedented degree of photorealisticism in terms of state-of-the-art automatic scores, such as FID scores, as well as state-of-the-art human ratings.
10196800	10203800	And it provides a deep level of language understanding, as can be told by the generated samples.
10203800	10209800	And it is extremely simple, so there is no latent space and no compensation.
10209800	10215800	And as we will see later, it's just like a pure cascaded diffusion model.
10215800	10226800	So I will first present several examples of imagined.
10226800	10234800	Yeah, this is my favorite one because I just created it to make it related to CVPR.
10234800	10248800	And in terms of the key modeling components of imagined, like I mentioned, it is a pure cascaded diffusion model containing one base model and two super resolution models.
10248800	10256800	And it uses classifier-free guidance and the dynamic thresholding, as I will talk about later.
10256800	10277800	And unlike DAI2, which uses clip text embedding as the, using this clip text encoder, this imagined used frozen large pre-train language models as the text encoders, more specifically this variant of T5 model.
10277800	10281800	And there are several key observations from imagined.
10281800	10287800	First, it is beneficial to use text conditioning for all the super resolution models.
10287800	10298800	The explanation is as follows. So remember, like for cascaded diffusion models, we need to use this noise conditioning augmentation technique to reduce the compounding error.
10298800	10305800	But however, this technique has a chance to weaken the information from the low resolution models.
10305800	10313800	Thus, we really need the text conditioning as extra information input to support the super resolution models.
10313800	10321800	And second observation is that scaling the text encoder is extremely efficient in terms of improving the performance of imagined.
10322800	10329800	And it has been shown that this is even more important than scaling the diffusion model side.
10329800	10347800	And lastly, comparing using the pre-trained large language model as the encoder versus the clip encoder, human readers actually prefer the large language model over the clip encoder on certain data sets.
10347800	10353800	And this dynamic thresholding is a new technique introduced by imagined.
10353800	10364800	This is mainly to solve the trade-off problem of using large classifier-free guidance weights, more specifically as we also discussed in the previous part.
10364800	10375800	So when we use large classifier guidance weights, there is a chance that it gives us better text alignment but worse image quality.
10376800	10384800	So as we use large free guidance weights, the clip score, which corresponds to a better text alignment, increases.
10384800	10391800	However, the IFID score also increases, which corresponds to worse sample quality.
10391800	10401800	So to elevate this issue, because we really want both the sample quality, like the good sample quality as well as good text elements, right?
10401800	10416800	So to elevate this trade-off issue, the hypothesis this paper made is that the reason why the sample quality decreases at large guidance weights is that at large guidance weights,
10416800	10431800	usually it corresponds to very large sample gradients in inference, and then the generated samples have a chance to be saturated because of the very large gradient updates.
10431800	10449800	So the solution they propose is this dynamic thresholding, meaning that at each sampling step, we adjust the pixel values of the samples to be within a dynamic range, and this dynamic range is computed over the statistics of the current samples.
10449800	10458800	And these two panels shows the qualitative comparisons between static thresholding and dynamic thresholding.
10458800	10470800	And you can see if we use this static thresholding, the images look kind of saturated, while the dynamic thresholding, the samples look more realistic.
10470800	10478800	And another contribution of Imagine is that they introduce a new benchmark, especially for this text-to-image evaluations.
10478800	10489800	So the motivation of introducing new benchmarks is that for existing datasets, for example, COCO, the text problem is kind of limited and is kind of easy.
10489800	10497800	So this benchmark introduced more challenging prompts to evaluate text-to-image models across multiple dimensions.
10497800	10510800	For example, it tries to evaluate the ability of the model to facefully render different colors, numbers of objects, spatial relations, text in the scene, unusual interactions between objects.
10510800	10524800	And it also contains some complex prompts, for example, long and intricate descriptions, wire words, and even misspelled prompts to test the robustness of your model.
10524800	10537800	And this figure shows several examples of the text prompts in this benchmark, and the corresponding generated images from Imagine using these text prompts.
10537800	10542800	And a bit more of the quantitative evaluations of Imagine.
10542800	10557800	Imagine got state-of-the-art automatic evaluation scores on the COCO dataset, and it's also preferred over reasoned work by human readers in both sample quality and image text alignment on the drawbench dataset.
10557800	10568800	And the reasoned work compared by Imagine includes this DAO-E2 slide, VQGAM plus clip, as well as the latent diffusion models.
10568800	10575800	Okay, so besides text-to-image generation, I also want to talk about the controllable generation using diffusion models.
10575800	10586800	And a representative work is this diffusion autoencoders, which propose to incorporate a semantic meaningful latent variables to diffusion models.
10586800	10595800	So more precisely, so given an input image, semantic encoders learn in this framework to generate this Z-SIM.
10595800	10603800	And this Z-SIM is fit into a conditional diffusion models to further predict the clean samples.
10603800	10618800	And this leads to some like the bipod latent representation similar to the DAO-E2 model, where we have this Z-SIM with the hope that it can capture high-level semantics.
10618800	10624800	And we also have this X-Big-T, which is the inversion of this conditional DTIM sampler.
10624800	10630800	And the hope is that it captures the low-level stochastic variations of the images.
10630800	10639800	And if we want to do unconditional sampling from this model, optionally, we can learn another diffusion model in the latent space of the Z-SIM.
10639800	10648800	Very similar to the latent diffusion models we talked about in the last part, to support this unconditional generation test.
10648800	10661800	Interestingly, they found that by assuming a low-dimensional semantic vector Z, they are able to learn different semantic meanings for different dimensions of this latent vector Z.
10661800	10675800	For example, by changing the certain dimension of this latent Z, they are trying to identify different semantic meanings such as the higher style, the expression, the age,
10675800	10681800	and also the color of the hair.
10681800	10697800	And they also are assuming that if we fix the Z-SIM for each row, and we change the latent X-Big-T in the conditional diffusion model, we can see it only corresponds to very tiny details in this image,
10697800	10703800	and perhaps other like perceptually invisible features in the images.
10703800	10710800	So that ends the first part of the application. Thanks for listening.
10710800	10718800	Awesome. Thanks Richie for the nice introduction of the first group of applications.
10718800	10731800	Here I'm going to start with the second group of applications. And in this part, I will mostly focus on image editing, image to image translation, super resolution, and semantic segmentation.
10731800	10745800	This is a super resolution. I'll start with talking about this work called super resolution, but we are repeated refinements, or SR3, which was proposed by Sahari et al at Google.
10745800	10761800	In image super resolution, we can consider this problem as training a conditional model P of X given Y, where Y is the low resolution image and X is the corresponding high resolution image.
10761800	10768800	So we want to be able to change the high resolution images given some input low resolution image.
10768800	10777800	In order to tackle this problem, the authors proposed to train a diffusion model, a conditional diffusion model using this objective.
10777800	10786800	Here in this objective, we have expectation over pairs of high resolution image X and low resolution image Y.
10786800	10800800	We have expectation over epsilon, which is drawn from standard normal distribution, and we have expectation over time, where time varies from, for example, zero to capital T corresponding to the diffusion process.
10800800	10816800	We have this excellent setup. This is a noise prediction network that takes diffused high resolution image, XT, the time, as well as this Y, this is the low resolution image that is provided as conditioning into epsilon prediction
10816800	10825800	network, and we train this epsilon prediction network to predict the noise that was used in order to generate diffused high resolution image.
10825800	10843800	The authors in this paper proposed to use different norms for minimizing this objective. They introduced L1, L2 norm, and they observed that one can trade quality for diversity by using L1 norm is L2 norm.
10843800	10851800	Since we are training a conditional model now you have in mind that we need to modify the unit that is used for epsilon prediction.
10851800	10862800	For the input of the unit, we will have access to diffused high resolution image, as well as this low resolution conditioning input.
10862800	10880800	Since these two images don't have the same special dimensions, the author proposed to use just simple image resizing algorithms to up sample the input low resolution image and concatenate it with diffused high resolution image and the channel dimension, and they provide
10880800	10893800	to the unit diffusion model or the epsilon prediction model and the network is trained to predict the noise that was used when generating the high resolution image.
10893800	10910800	This method achieves very high quality results for super resolution. For example, here you can see super resolution results with 64 by 64 pixel input when the output is 256 by 256.
10910800	10928800	And you can see that this method here shown in this column achieves a really high quality result compared to, for example, regression models or just simple image resizing algorithms or using by cubic interpolation.
10928800	10935800	And you can see that this actually does a good job of generating low level details.
10935800	10948800	Another work that I like to mention is called palette, image to mesh diffusion models. This method is also this paper is also proposed by same authors as a previous method.
10948800	10952800	So the author's area at home.
10952800	10961800	Similar to super resolution, many image to image translation applications can be considered as training a conditional model X given why.
10961800	10973800	This is the input image. For example, if you consider colorization problem X is the output color image and why is the grade level input.
10973800	10984800	So similar to the previous part, or previous slide we're going to again use, we're going to again trade and condition diffusion model using this objective.
10984800	10993800	Similarly, we have again expectation over pairs of input conditioning why, for example, why is again great image X is the output color image.
10993800	11011800	We have expectation over epsilon drawn from standard normal distribution expecting over time, we're training a conditional diffusion model that takes input grade level image for some input conditioning time as well as the diffuse output image that we want to generate
11011800	11017800	and then the model is trained to predict the noise that was used to generate diffuse samples.
11017800	11038800	Similar to previous part, again, we need to give a pair input to the unit model. And here, because for example, if we're attacking the color problem, we're going to have this grade level image and diffuse color image as input to unit and similarly it's trained to predict the noise that was trained for
11038800	11042800	generating diffuse sample.
11042800	11065800	The others tried their image to mist translation image to mist diffusion model on four different tasks, including colorization in painting jpeg restoration and uncropping, which is basically given this image, they wanted to extend the image and provide the copper part of this, what they called uncropping.
11065800	11071800	This paper shows that actually diffusion walls can achieve very good results on these four tasks.
11071800	11088800	We should have in mind that this problem this particular paper assumes that you have access to pairs of input and output data so they're they're training a conditional model assuming that they have input image and the output image that we want to generate.
11088800	11100800	If we don't make that assumption, what we can do we can potentially take an unconditional model that is trained on that for example natural images and we can modify it for a particular task.
11100800	11115800	So, as example of that approach, I want to mention I'd like to mention this paper called iterative latent variable refinement or by LBR for short, that was proposed by Joey et al at ICCB 2021.
11115800	11131800	This paper proposes an approach where, given a reference image, the authors like to modify the generative process of the fusion model, such that the output of the diffusion model can be similar to the reference image.
11131800	11146800	Again, we have a reference image and we want to modify the reverse generative SDEs or the reverse generative diffusion process, such that we can generate images that correspond to a reference input image.
11146800	11162800	So, and the authors and the speaker proposes to do this through using some unconditional model that is not trained for this specific task, it's just the unconditional model is trained to generate realistic for example faces on this slide.
11162800	11173800	So, so the basic idea is to modify the reverse denoting process, such that we can pull the samples towards the reference image.
11173800	11193800	So here you have the algorithm proposed in this paper, the algorithm starts from capital T goes to one so this is the reverse denoting process at every step we draw a random nodes vector vector from standard normal distribution, we sample from the reverse denoting distribution
11193800	11203800	to generate this proposal of this color x prime t minus one is the proposed denotes sample to run from the denoting model.
11203800	11219800	Why here represent the reference image so we're going to use the forward diffusion kernel to generate the diffused version of reference image so we're going to go forward in time for this reference image.
11220800	11230800	So I did what we want to do we want to make this proposed denotes image x prime t minus one to be more similar to whitey minus one.
11230800	11241800	So to do so that this paper proposes this simple operation here fine and represents some low pass filter.
11241800	11260800	So this operation is very simple, we have this proposed denotes image x prime t minus one, we subtract the low pass filter applied to this x t minus one, and we add the back low pass filter output of whitey minus one.
11260800	11273800	So you can think of this operation as operation that takes x t minus one, the x prime t minus one, this is the proposed denotes image, it removes its low pass filter low frequency content.
11273800	11282800	So, here we are removing the low frequency content of x prime t minus one, and the adding back the low frequency content of whitey minus one.
11282800	11294800	So basically we're putting the low frequency content of whitey minus one, this is the reference image into x prime, into x prime denotes the proposal sample.
11294800	11307800	What we are doing we're basically making sure that in the reverse process, we're generating a sample where the low frequency content is similar to the reference image so the most of the structure is very similar to the reference image.
11307800	11325800	So, as I mentioned, fine and it's just a low pass filter. And in order to implement this the artist proposed to use simply done sampling up sampling operation where n represents the, the, the, the down sampling ratio used for in this operation for something
11325800	11336800	n is equal to two, it means we're going to just take reference image or take this input down sample by factor of two and then up sample again by factor of two.
11336800	11342800	So, which is which corresponds to to a low pass filter operation.
11342800	11359800	Here you can see this reference, these two reference images, and how we can generate images with different values for n. So when n equals to four, it means that we actually take this during the generation we don't sample that for low pass filter we don't sample the
11359800	11370800	sample images by factor of four, by factor of four. Since this factor is a small, this means that most of the structure in the generation will be similar to reference image.
11370800	11385800	So you can see that in fact we're generating an image that is very similar to reference image. And as we increase this and we can see that now different levels of details can, can be generated through the diffusion model and the more global characteristics
11385800	11395800	like more global arrangements or low pass, low frequency content of the image is still remains the same as the reference image.
11395800	11404800	And now to show that actually you can do this for different tasks image translation given a portrait image they can generate a realistic image that corresponded to the portrait image.
11404800	11419800	So they can do paint to image so they can take all painting and generate realistic image, and they can do some simple editing, and for something can add back this watermark into.
11419800	11431800	In this part I'd like to talk about how we can take representation learned through diffusion models and use them for sometimes through applications such as semantic segmentation.
11431800	11443800	I'm talking about a specific paper called label efficient semantic segmentation with diffusion models that was proposed by one joke at all at ICLAR 2022.
11443800	11462800	So it's pretty a paper, propose a simple approach for using the representation trained in diffusion model for semantic segmentation. The others proposed to take input image and diffuse it by adding by following the forward diffusion process, and they only go to small steps
11462800	11475800	by following the forward diffusion step which corresponds to adding just a bit of noise into input image, then they pass this diffuse image to the denoising diffusion model the unit model the epsilon prediction model.
11475800	11487800	And they, they extract representation form, the internal representation form in this unit at different resolutions of the unit decoder.
11487800	11501800	So given these representations the up sample, all these intermediate representation, so that they have the same dimensional spatial dimensionality as input in so we have these up sampling layers.
11501800	11516800	We have these feature maps that have the same dimensionality as the input image, and now they simply concatenate all these intermediate feature maps, and they pass them to one by one convolutions that would do semantic segmentation per pixel.
11516800	11527800	So you can think of these as a pixel classifiers that just classify each pixel for each semantic object goals or semantic goals.
11527800	11535800	So, in order to train this model, that was supposed to use a pre-trained diffusion model and they're only training this component here.
11535800	11548800	Up sampling component doesn't have usually any training parameters, but most of the parameters, the additional parameters are basically here in these one by one convolutional networks.
11548800	11560800	This paper particularly shows that this approach is labeled efficient using very few labeled instances they can train diffusion models on several datasets as you can see on this slide.
11560800	11574800	And the other show that actually diffusion based segmentation models cannot perform mass encoders, can or VAE based models on this test, which is very interesting.
11574800	11583800	It shows that actually representation learning diffusion models can be used for downstream applications such as segmentation.
11583800	11591800	In this part, I like to talk about the particular paper parameter that is proposed for image editing. It's called SDE edit.
11591800	11602800	This paper was proposed by Ming Itala at Stanford University, and it was proposed, it was presented at ICLR 2022.
11602800	11614800	What this paper is trying to tackle is that, given this stroke painting, the authors propose a simple approach to generate realistic images that corresponds to that stroke painting.
11614800	11624800	The main intuition or the main idea in this paper is that the distribution of real images and stroke painting images, these are two different distribution.
11624800	11635800	If you have some smashes, they're not the same and in the, in the data space, they, they are not completely overlapping each other because of these differences.
11635800	11650800	But if you follow the forward diffusion process, if we have this distribution realistic images distribution stroke painted, if we follow the forward diffusion process these two distribution will stop having overlaps with each other because of the definition of forward
11650800	11659800	distribution, because we know that actually, if you have two distribution and you diffuse the samples in those two distribution, the distribution will start having overlap.
11659800	11666800	This forward diffusion simply corresponds to adding noise into input stroke painting.
11666800	11677800	Now that we know these two distribution are overlapping, we can use just a generative model train of real images to solve the reverse SDE, reverse denoising SDE.
11677800	11686800	That will start from this diffused stroke painting and try to generate a realistic image that corresponds to this noisy input.
11686800	11701800	And the authors show that they're actually using a generative model. This is an unconditional model again, train unrealistic images, they can come back to realistic images that where the colors here are very similar to this stroke painting colors.
11701800	11711800	So, this is, I think, a very clever idea to take stroke paintings and generate realistic images that correspond to those stroke paintings.
11711800	11727800	However, actually, we should have in mind that this train in a conditional setting, however, we should have in mind that this approach mostly relies on the color information in order to take this stroke painting and generate the corresponding image.
11727800	11736800	And this is a bit different than, for example, methods that would use the semantic layout, semantic mask of objects in order to tackle this problem.
11736800	11741800	So, it has some advantage and disadvantages that we should have in mind.
11741800	11762800	Delta shows very interesting results on different data sets. Here you can see stroke paintings for models train on this on bedroom, this on church and set up a and you can see here in these two row, how a generative model using SDE not can be used to generate realistic images that correspond to stroke.
11763800	11778800	Lastly, in this part, I'd like to talk about particular work that we did at NVIDIA for adversarial robustness and in this particular work we introduced diffusion models for adversarial clarification.
11779800	11798800	So the basic problem we want to add this is that given an adversially perturbed image, we want to see if we can use diffusion models to remove adversarial perturbations and clean this image such that maybe apply classifier on these adversially perturbed image images, we can actually get robust classification.
11799800	11810800	So this, the proposed idea here is similar to SDE edit mostly applied for adversarial clarification, given this adversarial perturbed image.
11810800	11823800	So what we propose to do is we propose to follow the forward SDE, which correspond to basically adding noise and diffusing the input adversarial perturbed image, using just a forward diffusion cannon.
11823800	11831800	So we go to particular times that T star we call, and we, and we simply diffuse the input adversarial perturbed image.
11831800	11839800	We know that by adding noise, we can now wash out all these adversarial perturbations that are applied into image.
11839800	11850800	Now that we have this noisy image, we use the reverse genitive SDE or reverse noise SDE to start from this noisy input and generate clean image that corresponds to this noisy image.
11850800	11860800	And we know that through this process, we can remove all the noise that was injected as well as all the adversarial perturbations presented here in this image.
11860800	11869800	So if we have this clean image or purified image, we can just pass it to classifier and hopefully make a robust classification prediction.
11869800	11880800	Like any adversarial difference mechanism, we need to be able to attack this model, evaluate our performance, we need to be able to attack this model.
11880800	11890800	And we also show how we can attack this model by backfogating end-to-end through classifier as well as our purification algorithm.
11890800	11904800	And this involves basically backfogating through this reverse SDE. So we showed in this paper how we can do this and how we can attack this mechanism end-to-end.
11904800	11913800	On the left side in this slide, you can see an example of adversarial perturb images and first column on a set of A data set.
11913800	11920800	Here, we intentionally increase the magnitude of adversarial perturbations so that they are visible to us.
11920800	11932800	These two groups are representing, these two images are representing adversarial perturbation for a smining class and these two represent adversarial perturbation for eyeglasses.
11932800	11941800	Here you can see diffuse samples that are generated by following the forward diffusion. This is simply corresponds to sampling from diffusion kernel.
11941800	11949800	And then here in these two columns, you can see samples that are generated when we're solving the reverse genetic SDE.
11949800	11959800	And as you can see, at time equals to zero, we can remove not only the adversarial perturbations as well as all the nodes that was injected through forward diffusion process.
11959800	11966800	And you can see that our generated energy equals to zero are very similar to input clean original images.
11966800	11977800	And we can see that the semantic attributes of these images are very similar to semantic attributes of the original images.
11977800	11987800	The nice thing about using a generative model for adversarial purification is that these modes are not trained for specific attacks and specific classifiers.
11987800	11993800	So at the test time, we can just apply them for unseen adversarial attacks.
11993800	12005800	In comparison to the state of dark methods that are designed for similar situation for unseen threats, we actually see that our proposed diffusion prefabrication method outperform these methods by large margin.
12005800	12015800	And we believe that, in fact, the fusion models can can be very strong models for designing adversarial purification techniques.
12015800	12029800	And this is probably because the fusion models can generate very high quality images and potential can use for removing all the artifacts that are generated by adversarial perturbations.
12029800	12044800	This brings me to the end of the second part of applications. Next person will will continue with the third part of applications.
12044800	12053800	All right, I will not talk about video synthesis medical imaging 3d generation and discrete state diffusion models.
12053800	12056800	Let's get started with video generation.
12056800	12065800	There are samples from a text conditioned video diffusion model like Jonathan how at all, where we condition on the string fireworks.
12065800	12070800	So I think these samples look pretty convincing.
12070800	12083800	So they're actually in general different video generation tasks. For instance, so it's unconditional generation where we want to generate all frames of the video on scratch without conditioning on anything.
12083800	12093800	There was a future prediction where we want to generate future frames conditioning on one or more past frames. We can also do past prediction the other way around.
12093800	12104800	We can also do interpolation, when we have some frames and we want to generate in between frames, just for instance useful to increase the frame rate of the video.
12104800	12119800	All these generation tasks can basically fall under one modeling framework. In all cases, we basically want to learn a model of form setter of xt one to xtk given x, how one to x, how I am.
12119800	12127800	So for the t's and tiles, you know the times for frames that you want to generate and for the frames that we condition on.
12127800	12137800	So for future predictions, these tiles will already smaller than the t's unconditional generation and we wouldn't have any tiles and so on and so forth.
12137800	12145800	Something we see in multiple of these recent works is that they try to learn one diffusion model for everything.
12145800	12154800	What they do is they concatenate and combine both the frames to be predicted and the conditioning frames together.
12154800	12158800	And then some of these frames are masked out, so once to be predicted.
12158800	12171800	And yeah, based on the conditioning frames, those are then generated and varying the masking and conditioning combinations during training, we can train one model for these different tasks.
12171800	12182800	In training, we would also tell the model which frames are masked out and we would feed the model time position encodings to encode the times for the different frames.
12182800	12185800	That's visualized here, for instance.
12185800	12193800	In terms of architecture, these models are still using these units, which we already know from the image based diffusion models.
12193800	12204800	We know like small detail. So, of course, now our data is higher dimensional, because in addition to the image and height and width dimensions, as well as the channel dimensions.
12204800	12212800	We now also have the time dimensions with the number of frames, so the data is essentially four dimensional.
12213800	12225800	One way is to use now 3D convolution instead of 2D convolutions to run convolutions over height width and the frames. This can be computationally expensive.
12225800	12233800	Another option is, for instance, to keep special 2D convolutions and use attention layers along the frame axis.
12234800	12244800	This has the additional advantage that ignoring those attention layers, a model can be trained additionally on pure image data, which is kind of nice.
12244800	12257800	So, let's see some results. It turns out that these video generation diffusion models, they can actually generate really long-term video in a hierarchical manner, which is quite impressive.
12257800	12266800	And there, we precisely leverage these masking schemes that we just had, and these generalized video diffusion frameworks.
12266800	12276800	So, one thing you can do, for instance, is we generate future frames in a sparse manner by conditioning on frames far back. This gives us long-term consistency.
12276800	12285800	And then we interpolate the in-between frames afterwards. So, we kind of generate the video in a stage-wise hierarchical manner.
12285800	12297800	And with that, it's possible to actually generate really long-length, one-hour coherent videos, which is quite impressive. So, here are some samples from this recent Harvey et al. work.
12297800	12308800	All right. Let us now talk about another application of diffusion models, which is solving inverse problems in medical imaging, another very relevant application.
12308800	12316800	So, medical imaging may refer to computer tomography or magnetic resonance imaging.
12316800	12327800	In those cases, we're basically interested in an image X, but that is not what we're actually measuring from the CT scanner or MI scanner.
12327800	12336800	So, let's consider the measurement process. For instance, in computer tomography, the forward measurement process can be modeled in the following form.
12336800	12345800	In the image, we are basically performing a radon transform, which gives us a sinogram, and then maybe this is sparsely sampled.
12345800	12358800	So, we end up, this is sparsely sampled sinogram Y. And now, the task is that needs to be solved to reconstruct the image given this measurement Y. So, this is an inverse problem.
12358800	12370800	This is a similar and magnetic resonance imaging, just that the forward process is now basically modeled with a Fourier transform, which is then sparsely sampled.
12370800	12385800	So, and this is where diffusion models now come in. So, they can actually be really used in this task. And the highly ideal is here to learn a generative diffusion model as a prior over the images we want to reconstruct.
12385800	12396800	And while sampling from the diffusion model, we guide synthesis condition while conditioning on the sparse observations that we have. This is the idea.
12396800	12405800	And it turns out that doing this actually performs really, really well. And even this outperforms even fully supervised methods sometimes.
12405800	12420800	Specifically, the thing is, when we train this fusion model over these CT or MRI images, we really just need the images, we do not need paired image measurement data to train this.
12420800	12434800	So yeah, there is actually a lot of work in that direction because it's a really high impact application, of course, and there are some citations that you are interested in learning more about this.
12434800	12445800	So let's move on to the next application topic, which is 3D shaped generation. Also, 3D shaped generation has recently been tackled with diffusion models.
12445800	12453800	So let us consider this work by zoo at our, for instance, here, 3D shapes are represented as point clouds.
12453800	12474800	And this has the advantage that they can be diffused really easily and intuitively, we see this here at the bottom right so where the diffusion actually goes from the right to the left, we have a bunch of points and they are perturbed and 3D towards this, yeah, Gaussian noise ball kind of, and the generation goes into the other direction.
12474800	12492800	So in those cases, the architectures that we use to implement the denoiser network are like typical point state of the art modern point cloud processing networks like no point net advanced versions of point and point boxes and so on and so forth.
12492800	12494800	How does this look like then.
12494800	12499800	Another animation.
12499800	12506800	I think this is quite nice. So, yeah, we can generate these pretty good shapes.
12506800	12519800	We can also train conditional shape completion diffusion models very condition, for instance, unlike that or like some sparse points like this, and then complete those shapes.
12519800	12532800	So in the multimodal fashion, for instance, in this example we have some legs of the chair given. And now we have like different plausible completions of the chair here.
12532800	12550800	Another thing that is also quite cool is that he works on real data. I think here the model was trained only on synthetic shape net data, and yet we can see the model images that we take these images and generate plausible 3D objects.
12550800	12555800	Very nice.
12555800	12571800	Finally, I would like to talk about discrete states to fusion models. This is less of an application but slightly different type of diffusion model, but I think it is worth mentioning as part of this tutorial.
12571800	12584800	So, so far, we have only been considered continuous diffusion entity noising processes, which I mean with that is, we basically kind of assume our data is of a continuous nature.
12584800	12600800	And we could add a little bit of Gaussian noise to it in a meaningful way. So both our fixed forward diffusion process and also our reverse generative process are usually were usually implemented as Gaussian distributions like here.
12600800	12618800	But what if our data is discrete, categorical, then continuous perturbations are not meaningful, they are not possible. Imagine, for instance, our data as text data, you know, pixel wise segmentation labels, or discrete image encodings.
12618800	12633800	Yeah, if our data is discrete, adding Gaussian continuous noise to it doesn't really make much sense. So, can we also generalize this diffusion concept to like discrete state situations.
12633800	12647800	In fact, there are categorical diffusion models. And in those cases, the forward diffusion process or like the perturbation now is defined using categorical distributions.
12647800	12664800	So consider perturbation corner q of xt given xt minus one, that is supposed to put up the discrete data. So this can now be a categorical distribution, where the probability to sample one of the teachers is now given by some transition
12664800	12675800	matrix q multiplied together with the state we are in xt minus one. So the probability to sample like the new state xt.
12676800	12688800	So this xt is usually a one-hot state factor describing the state we're in. And yeah, this transition matrix multiplied with will then give us probabilities to sample the next state.
12688800	12698800	So with that we can put up complex distributions categorical distributions towards like very random discrete distributions.
12698800	12708800	We choose this transition matrix accordingly. So for instance, in this example, if we look at the right, this may now be a complex data distribution.
12708800	12716800	We can perturb this towards a uniform discrete distribution over these three different states 13123.
12716800	12728800	So again, the reverse process for generation, and which is then implemented through a neural network, we can also parameterize as a categorical distribution.
12728800	12734800	In fact, there are different options for this perturbation process, this forward perturbation process.
12734800	12745800	We can use uniform categorical diffusion where we pull everything towards a uniform distribution over the different categories like I've just shown.
12745800	12755800	We can also progressively kind of mask out the data where we pull everything into one particular state. We can also analytically sample from such a distribution.
12755800	12759800	So it's also well suited for diffusion model.
12759800	12770800	We can also tailor our diffusion processes to ordinal data and use something like discretized Gaussian diffusion process that's also possible.
12770800	12773800	How does this look like for instance.
12773800	12790800	So here now I have the data distribution. It's a bit complex, but so this is basically each pixel of this image represents one categorical variable, and now the color of this pixel represents which teacher we are in.
12790800	12802800	So now if I would do like this uniform categorical diffusion, I would kind of, you know, yeah, would look like this, where I would transition into different states everywhere in the image.
12802800	12811800	I could also do something like Gaussian diffusion where it's more like this ordinal thing that's more based transition to neighboring states.
12811800	12821800	And then there was also this absorbing diffusion where I kind of progressively mask out or absorb my state sort of different ways to do this.
12821800	12832800	And then in the reverse process may for instance look like this. So here on the far right, this is a stationary distribution of this categorical distribution from which I can sample analytically.
12832800	12842800	And then denoising kind of progressively noises is bad towards the data distribution.
12842800	12856800	So yeah, one can use this and some papers have explored such discrete state diffusion models. For instance, we can also apply this on images by modeling the pixel values of images as discrete states to be in.
12856800	12869800	So this is a start from uniform uniformly distributed pixel values right here from this all gray kind of state or mask out state.
12869800	12889800	Another application is to use this in a discrete latin space. So in this work, for instance, images are encoded using a vector quantization techniques into visual tokens in a discrete latin space and then we can use something like discrete diffusion models and similar techniques to model the distribution
12889800	12892800	over the visual tokens.
12892800	12896800	This is also something one can do.
12896800	12907800	We can also use discrete state diffusion models to generate segmentation maps, which are also categorical distributions in pixel space.
12907800	12923800	And yeah, that concludes my part. And with that, I would like to pass a mic back to ours, we will now conclude our tutorial. Thank you very much.
12923800	12933800	Thank you for being with us. This basically brings us to the last part conclusions, open problems, and final remarks.
12933800	12946800	So today was a big day, we learned about diffusion models. At the beginning of this video, I started talking about the noise and diffusion prophecy models, which is a part, which is a type of discrete time diffusion models.
12946800	12965800	I showed you how these discrete time diffusion models can be described using two processes, a forward diffusion process that starts from data and generates those by adding those into the input, and then reverse the noise and process that learns to generate data by starting
12965800	12984800	from noise and denoising the input image one step at a time. I also talked about how we can train these diffusion models by simply generating diffuse samples and training network to predict that to predict the noise that was used to generate diffuse input images.
12984800	12993800	In the second part, Carson talked about the score-based generative modeling with differential equation, which corresponds to continuous time diffusion models.
12993800	13011800	Specifically, Carson talked about how we can consider diffusion models in the limit of infinite number of steps and how we can define or how we can describe these forward and reverse processes using stochastic differential equations or STEs.
13011800	13027800	I also talked about probability flow ordinary differential equations or ODEs, which describe a deterministic mapping between noise distribution and data distribution.
13027800	13043800	Another thing about working with stochastic differential equations or ODEs or ordinary differential equations is that we can actually use the same training that was used for training different discrete time diffusion models in the previous slide.
13043800	13055800	However, at the test time, we are free to choose different discretization or different OD or ST solvers that have been studied widely in different areas of science.
13055800	13066800	And those are basically to change the sampling time by using, for example, OD or ST solvers that don't require a lot of functional evaluations.
13066800	13072800	In the third part, Ruchi talked about advanced topics in diffusion models.
13072800	13085800	She mostly focused on accelerating sampling from diffusion models and she studied this from three different perspectives, including how we can define forward processes that accelerate sampling from diffusion models,
13085800	13094800	how we can come up with better reverse processes, or how we can come with better denoising models that allows us to access sampling from diffusion models.
13094800	13104800	Beyond that, she also talked about how we can scale up diffusion models to generate high resolution images in conditional and conditional setting.
13104800	13117800	Actually, especially talk about cascaded models and guided diffusion models that are heavily used in the current state-of-the-art image to text diffusion models, just imagine.
13117800	13135800	After talk about fundamental topics, all three of us talked about various computer vision applications that have been recently proposed and mostly applications that rely on diffusion models at their core recently.
13135800	13145800	So, now that we know about diffusion models and we know how we can use these models in practical applications, let's talk about some open problems.
13145800	13160800	I do hope that now we could make you interested in this topic and now that you know the some fundamental in this area, maybe you can think about open problems that exist in this space and together we can tackle some of these.
13160800	13171800	The first problem I want to, first of problems I want to mention are more on the technical side and later I will talk about more applied questions that arises in practice.
13171800	13184800	So, if you remember at the beginning of the talk, Mouskarsen and I talked about how diffusion models can be constructed as a special form of MIEs or continuous time normalising flows.
13184800	13202800	We exactly don't know why diffusion models do much better than VAEs and continuous time normalising flows. If we can understand this, maybe we can take the lessons learned from diffusion models and why they do so much better than VAEs and continuous time normalising flows in order to improve these frameworks,
13202800	13210800	meaning we can maybe use the lessons learned from diffusion models to improve VAEs or normalising flows.
13210800	13227800	Even though there has been a tremendous progress in the community for accelerating sampling from diffusion models, we can still do, in the best case scenario, we can still do, actually, the sampling using four to 10 steps on the small matrices such as Cypher 10.
13227800	13243800	However, the main question that remains on how we can get one step samples for diffusion models. And this can be very crucial for interactive applications where a user interacts with the diffusion model and this
13243800	13252800	we can reduce the latency that usually users observe when they are using generative models.
13252800	13267800	Some of the existing problems have to define one step samplers and part of the solution might be to come up with a better diffusion process that are intrinsically faster to generate sample from.
13267800	13286800	Diffusal models, very similar to VAEs or GANS, can be considered as latent variable models, but the latency space is very different. For example, GANS, we know in the latent space often have semantic meaning and using latent space manipulations, we can actually come up with image editing or image
13287800	13303800	manipulation frameworks. But in diffusion models, the latent space does not have semantics, and it's very tricky to come up with latent space semantic manipulation diffusion models. So part of problem here is how can we define
13303800	13312800	semantically meaningful latent space for diffusion models that allow us to do semantic manipulations.
13312800	13332800	In this talk, we mostly focus on generative applications, but one open problem is how we can use diffusion models for discriminative applications. For example, one way of using diffusion models might be for representation learning, and we might be able to tackle high level tasks such as image classification
13332800	13344800	versus low level tasks such as semantic image segmentation. And these two may require different traits of when we're trying to use diffusion models to address these.
13344800	13358800	Another group of applications that may benefit from diffusion models is uncertainty estimation. One question is how can we use on diffusion models to do uncertainty estimation in downstream discriminative applications.
13358800	13379800	And finally, one question that remains open is how we can define joint discriminator generator, sorry, joint discriminator generator models that not only classify images or input, they also can generate similar inputs.
13379800	13394800	So the committee mostly have been using unit architectures for modeling the score model in the score function in diffusion models. But one question is whether we can go beyond units and come up with better architectures for diffusion models.
13394800	13412800	One specific open area is how we can feed time input or other conditioning into diffusion models, and how we can potentially improve the sampling efficiency or how we can reduce the latency of sampling from diffusion models using better network design.
13412800	13428800	So far in this talk, we mostly focused on image generation, but we may be interested in generating other types of data. For example, 3D data that has different forms of representation, for example, it can be represented by stance function,
13428800	13447800	meshes, voxels, or volumetric representation. Or we might be interested in generating video text graph, which have their own characteristics. And given these characteristics, we actually may need to come up with specific diffusion models for these particular modalities.
13447800	13462800	One area of research is to do composition and controllable generation, and this will allow us to go beyond images and be able to generate larger scenes that are composed of multiple, for example, objects.
13462800	13477800	And also this will, as a technical example, allow us to have fine grain control in generation. So one interesting open problem is how we can achieve composition and controllable generation using diffusion models.
13477800	13494800	Finally, I think if we look back, look back to the vision community and the problems that we solved in the past few years, we see that most of the applications try to solve, most of the applications that rely on generative models, they try to solve,
13494800	13507800	and some solve with generative adversarial networks. So maybe it's a time for us to start revisiting those applications and see whether they can benefit from the, the nice properties that diffusion models have.
13507800	13517800	So one open question is which applications will benefit most from diffusion models, given that we have such amazing strong tool.
13517800	13537800	Let's go to the final slide. I want to say thank you for being with us today. This was, this is a very long video, and I do hope that we could provide some useful and fundamental background on diffusion models and how often are used in practice.
13537800	13547800	All of us are active on Twitter, if you are interested in knowing about follow up works that we, we build on diffusion models, please make sure that you follow us.
13547800	13556800	And lastly, I want to mention that all the content on this week and this video, including slides will be available on this video on this website.
13556800	13573800	If you happen to enjoy this video, I would like to ask you to share this video with your colleagues and collaborators, and hopefully together, we can come with more people to start looking into diffusion models and applying them to various interest applications.
13573800	13574800	Thanks a lot.
