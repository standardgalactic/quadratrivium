{"text": " global digital summer school on quantum computing and I see we have reposted the link all right so oh Sausalito California that's where it's very close to where I grew up now we are thrilled to roll out the latest episode of the quantum seminar series dedicated to the research and academic communities this this seminar takes place every Friday at noon Eastern time right now at this hour on the Kiskin YouTube channel and I'm delighted to see so many of you already tuned in I'm your host like a minute from IBM quantum research and today I have privilege of hosting young we're done from Google X and and also from the Institute of quantum computing at Waterloo young will present some very nice results and hello young how are you today hi is that this let go happy to be here yeah it's a pleasure to see you last time we met I think it was right after March meeting was cancelled and we were at March meeting so I'm glad that we can continue the discussions and have everyone join us today on the call that's right the science goes on to multistimes but that's right indeed indeed I hope you've been doing well again in addition to being a social media celebrity is a human though is a PhD candidate at the University of Waterloo at the Institute of quantum computing I had a very nice visit there recently so great place he is also a research scientist at X Google X or alphabets research and development lab before this Guillaume briefly worked at Google AI quantum and was one of the co-founders of TensorFlow and project he holds a master's in math and quantum input from you Waterloo as well as in honors math and physics degree from a given and university and I think we have some very interesting questions to discuss today I think as many of you know quantum computing operates in an exponential space so how is classical machine learning and learning classical distribution for instance which also operates in an exponential space mesh how can we learn new and interesting correlations and work with not just pure states but non pure states and these are just some of the things Guillaume will tell us about today and maybe as I advertise your talk and describe the format of Guillaume maybe you can pull up your slides yep the talk format is the usual you can ask questions in the comment sidebar box on the right hand side usually or below and I will triage those questions and ask Guillaume in real time so Q&A is during and after and I think it's time we get started so it's my pleasure to turn over to you well thank you for the introductions Lacko and thank you to the invitation and it's very nice to get to speak to the whole quantum community here really enjoy watching these seminars so it's my honor to get to talk in one of these so today I guess I'll be talking broadly about quantum machine learning and some context comparing it to classical machine learning and deep learning and then getting into some recent work from various internships and during my PhD on taking inspiration from classical machine learning to create new types of quantum models and algorithms so as Lacko mentioned often in quantum machine learning there is this conception that if we go to if we use a quantum computer sense we're operating in a exponentially large space and and thus we should get exponential amounts of power of machine learning power right but that is that is somewhat of a misconception because for very long time classical computers and analog classical electronics have been able to do probabilistic computing right and as we know quantum theory is kind of an extension or generalization of probability theory to include complex numbers known as wave functions right and probabilities are obtained from wave functions by taking the amplitude squared or the absolute value squared of various these complex numbers and from that we get the probability of various outcomes right that is known as the Born Rule right but you know on the classical side we can have mixtures of zero and one right zero or one in a different combination whereas on the quantum side we have super positions it's not zero and one it's not zero or one it's a super position with certain values of complex numbers right so overall I guess the theme of the talk is to you know take inspiration from classical probability theory and take inspiration from a subset of machine learning called probabilistic machine learning to come up with new quantum models because the theories are very much analogous and can in fact be hybridized as we will see so if you have you know n-pro bits probabilistic bits they also operate in a space that is exponential right you have a probability distribution over the space of bit strings and you can have a mixture of two to the n possible bit strings right and there's many as we'll see machine learning models that are made to represent such distributions or generate such distributions and on the quantum side you know for pure states at least they're written as a wave function of the sort and have two to the n complex numbers for n qubits right so there's a lot of similarities right and there's a difference the complex number and the real number that's important that gives in a sense quantum computing its power over classical algorithms but instead of you know having this constant competition I guess between classical computing and quantum computing you know the philosophy at least of my research is to try to leverage as much classical computing as possible and including probabilistic computing and hybridize it with quantum computation in a sense we want to leverage quantum computers for what they're very they're the best at right and we want to add this such that there's a value add by using quantum computers hybridized with classical computing so this is a philosophy and it's a research and you know there are various schools of thought in quantum computing and this is the one I guess I'm vouching for so you know what what actually gives quantum computers their power right well you know there's been various demonstrations that sampling from a unitary circuit that is quite deep and has a large space-time volume a unitary quantum circuit is quite difficult for classical computers right one has to do Feynman paths or tensor networks and what not and you know the difficulty scales exponentially asymptotically with the volume of space-time right so that that we know that's the power of quantum computers is to sample from such circuits so how do we incorporate this exact thing sampling from unitaries and integrate it with the capabilities of classical modern classical machine learning to obtain you know something more powerful than either either piece individually right so quantum computers are becoming more powerful to the point of being unsimulatable you know I won't get into whether the boundary has been crossed or not that's a that's an interesting debate on its own but how do you know how even if we have this power how do you actually leverage this power for for something you know relevant that is is not just just a demonstration so in a sense the meta area of focus at least in the in the near term has been quantum AI right and what is quantum AI I like to subdivide it into two subfields that are dominant for now there are other subfields that could be analogous to the subfields of classical AI but for now it seems like the community is focused on two broad categories and one is quantum enhanced optimization so that is accelerating classical algorithms of optimization and search using quantum or quantum inspired dynamics and quantum deep learning and I have you know many people call these variational algorithms I'll justify my nomenclature in a second what I consider quantum deep learning is learning quantum representations of quantum or classical data so there's a lot to unpack there so we're going to spend a few slides trying to unpack what it means to do to have a representation or quantum data this is gonna be the focus of my talk today quantum deep learning learning a multi-layered quantum computation based representation of quantum or classical data distributions what is a computational representation of data right or a deep multi-layered computational representation of data representations right let's let's go back to classical deep representation learning theory aka deep learning and try to understand a bit of the context so so deep learning is subset of machine learning subset of AI subset of computer science and which is of course a subset of science and I guess the gist of it is that neural networks you know when they learn something they got to be able to in a sense recreate it you know Feynman said what I cannot create I do not understand and your favorite deep neural network if you could ask if you could ask it what it thinks it would probably say something similar like this quote what do we mean by recreate so here I'm gonna get more rigorous so usually you have a data set which is set of points sampled from a true distribution p true of x right and so you have a certain finite set of data points right you don't have the full distribution you could query you're trying to learn a approximative model and you're trying to approximate this distribution over a certain domain of interest that goes beyond the data set itself because you you already have the data points for that are in the data set but you're trying to extend it beyond the data set and you have a parametrized hypothesis class or in classical machine learning we call it a variational distribution a variational classical probability distribution and five here would represent a set of parameters right because usually these distributions are parametrized using something called deep neural nets as we'll see in a second but the goal is to approximate a true distribution with a variational distribution and you know the idea is to minimize the discrepancy between the true distribution and our our variational distribution over the data set and hope that it extends beyond the data set right so that would be for generative modeling we're just trying to learn the raw distribution of all our data in what is called discriminative modeling which includes you know classifiers such as like labeling a picture of a cat or a dog or regression neural regression which is trying to get a scalar out of out of data so you know maybe a certain continuous value instead of a discrete label but in general we have pairs of inputs and outputs right and discriminative learning is very similar it can be phrased in probabilistic language as we're trying to learn a conditional distribution right random variables can be correlated and they can have what are called conditional distributions hopefully my okay we can see the last line here so the idea is that deterministic functions such as most deep neural networks are actually you know they're a subset of this conditional distributions they're kind of delta functions if you're used to the delta measure in function space if you integrate over it then you you get the value y equals f of x right so it's just a very sharp distribution right so most of classical machine learning or I guess the the popular parts of deep learning often deal with kind of deterministic point-wise functions whereas the the more general theory is actually based on probability and information theory right and that's kind of the roots of machine learning and what we're trying to get back to with quantum because we are in the early days where we must understand from first principles what we're doing instead of just trying stuff and iterating on the engineering of different algorithms you know willy-nilly we want to be guided in our research so you know deep learning are algorithms tied to identify patterns in data you use multi-layered parameterized computations to learn representations of data representations are multi you know deep representations are multi-step computations that either take you from your data space to a simplified space or from a simple space to your data space right so in the case of discriminative learning you're trying to take the input space say the pictures of cats and go to the label space you know is it cat or dog a single bit instead of many many pixels right in generative learning you're starting from a very simple set of randomness say a set of Gaussian samples or a set of random coin flips and trying to turn that randomness into the randomness over say the the set of pictures of bedrooms right the possible set of pictures of bedrooms and you're trying to sample new data points from from that data set in terms of math we say you know we're searching for a sub manifold of your your your space right and if it's all continuously parameterized it's technically a manifold again this is what I just described you have some randomness a generative model would go to the some complicated space you know machine learning folks and deep learning folks really love pictures quantum folks love wave functions and nick states as we'll see but you know discriminative learning would go to a simple space and then once it's simplified it's easy to separate out the two class so again representations every time I see representations don't know freak out it's just parameterized multi-step computation and deep is multi-layer and the building block is neural networks so I think I'm going to skip over this theory this is an example of a unsupervised learning algorithm called a variational autoencoder it's a way to compress data so you go to a compressed space and by compressing you're going to be forced to decorrelate the data and get a very simple very simple randomness and you could fit that simple randomness say with Gaussians and then if you plug it through in a sense the reverse transformation you get your data set again right and I want to show this because it's going to be very similar to our quantum approach where you could go in reverse through a quantum classical transformation and then you have a very simple what is called a latent space a hidden space and then when you want to generate the data again you go from latent space to the visible space right and these are very cool because you can in latent space if you just train the network to search for interesting features in general they can find features that and then you could do kind of logic in latent space so maybe there's a vector that corresponds to glasses to gender to age and so on and you could play around in latent space and see what you generate on the other side so you know for quantum for example if you have properties of materials you're trying to detect and you're trying to generate new materials or new materials with properties having a latent space that you've detected to play with it can be very useful and of course unsupervised learning itself is also useful in classical and sorry in a discriminative learning because finding a compressed representation is already part of the job to to to separate out classes so imagine we we had you know three different classes and we compressed it to some space that's two-dimensional and then we could go from a two-dimensional space to three different class labels of which color of the blob it corresponds to so again so i'm going to be focused on unsupervised learning but a lot of this actually applies to supervised or discriminative learning so what consists of a good representation i won't go too much into this but at a high level you want the representation capacity are you able to capture or you know reproduce the data set for some value of your parameters of your model is it trainable efficiently right if you have a neural network parameterizing your computational representation to go from complicated to simple space how easy is it to train it with algorithms that are not too closely inference tractability for feedforward neural networks that's very simple but there's other types of models that just doing prediction the prediction step can be computationally costly so that's something to keep in mind and of course that is the advantage of quantum computers is that you know if we incorporate large unitary transformations into our models as we'll see theoretically there are unitary transformations that cannot be executed the prediction or sampling step on a classical computer right so it's a very important part that's why i have this slide generalization power is is the core of machine learning generalization is you know if i fit within my data set will what i've learned extend outside the data set which is important because that is the difference between learning and optimization i sense there's a question oh just i'm getting keen keen awareness um this is more of a curiosity question about the representation capacity it's uh it seems like a really powerful but what can we usually before say running numerical experiments and so forth you know how much can we say or really peer into that for a particular model you've come up with you know what are the kind of tools and techniques and how far can they allow you to can we really say a lot about that about complexity of sampling unitaries uh yeah the representation capacity like what kind of correlations and so forth you will be able to capture potentially this is more of a right and that is going to depend strongly on your your the way you parameterize your your transformation in a sense you're by having a parameterized model you have what is called a hypothesis class and depending on on the various choices you've made you're going to kind of span a sub manifold of states and the idea is that you know what is very popular right now is called the hardware efficient onsots it looks very much like a random quantum circuit like this it's very tightly packed and the idea is that if you look at in the space of possible quantum states it can represent right if all of these transformations were parameterized random you know single and two qubit rotations right then theoretically you know its complexity is growing larger and larger right and in a sense any quantum state that has a complexity uh within that complexity radius you'll be able to reach it but the problem is because you're spanning such a large space your training of your quantum neural network becomes harder and harder um because your hypothesis class is too large so you're searching over too too many possibilities and this is the result known as the the baron plateaus in the quantum neural network landscape or or the quantum version of no free lunch theorem where you can't have a one size fits all representation and that's that's where physicists come in physicists need to have you know good prior knowledge of the domain of application they're trying to do quantum machine learning and to instruct their choice of representation and parameterization to aid in in the the tractability of training um that answers the question yeah so i like that free lunch theorem uh because i guess you know you could try to say well if i have some sort of generators that i use for my model you know what is the reachability of states right since the people ask what is the reachability but i think what you're emphasizing here is that reachability is maybe only a first step uh and maybe having too much reachability sometimes at the moment so there's a trade-off maybe between capacity and efficiency exactly exactly and that that is the no free lunch theorem in a sense so that's lucky for us because uh you know at least for now it seems like physicists will be needed uh in the future when uh we're not going to be out of a job we still need to design architectures um at least for now so let's see let's see let's see where it goes but uh but that's right that's right so uh you know a lot of what i'm going to present today is not necessarily uh architectures for specific domains it's it's more uh a general framework uh based on quantum information theory of how to uh do quantum machine learning or or maybe a a very broad class of parametrizations of of models that are quantum yeah yeah i'm since i've already interpreted you there's interesting this is kind of an unusual question but why don't throw it out here anyhow from martin how much time do you take uh take it take you to learn all this i guess um i mean so okay so i guess backstory uh once there was a conference of machine learning on a monday on a friday night i decided to binge watch lectures at three times speed on youtube on the basics of deep learning i think that was 2016 uh or 2017 something like that and uh since then i've just been reading uh machine learning papers and you know i i guess have a deep math background so it helps uh and then quantum computing itself uh i guess i've been doing since i was 19 and i'm 28 now so gives you an estimate uh it's just always been my passion and uh i uh i went through theoretical physics and uh now i'm here in uh quantum machine learning so i would say four years of of interest in quantum machine learning two to three years serious uh serious focus and it looks like george baron is building on my question which probably gets into a little bit i also wanted to get into which is what are some uh quantitative quantitative metrics for representation capacity yeah that's that's interesting um i guess that's a good that's a good question i would say if you can quantify of in a sense a notion of volume and complexity space um and this is actually you know we're edging on on theoretical physics here because the notion of quantum complexity is interested in interesting in in the theory of ads cft and um you know there's lennard suskin who does a lot of work in this uh in this space uh and uh yeah i mean that's a that's a that's an open question i think i have some intuition uh as to what would be a good metric but that would be an interesting further study you know yeah there's a good quote right on we point correct like with uh with logic we prove with with intuition we discover and that's right here's the guidance that's right cool cool um so i guess i've i've i've gone through these uh this is just some text uh backing up what i've said um so okay so now that we've we have some very brief background and some intuition about deep learning because this is a quantum computing audience so we had to load that up um how can we use you know what we learn taking inspiration from vaes or uh from you know the what is needed to have a good representation to instruct our choice of how we do quantum deep learning so first of all what would be a quantum deep representation right well a classical feed for network in a cartoonish picture this is not the most general formulation but it's a it's a friendly one um you have some input you have some parameters phi and then you have some parameterized output f of x fine for a quantum neural network you have usually a pure state input a unitary evolution that is parameterized in some way and then you have a parameterized hypothesis class of pure states right which is u five times your initial state and uh you know we call the function f the feed forward operation you can have a uh loss functionals returns a scalar that depends on say your label and your your output of your network uh this could be some statistical measure of statistical distance to your data set and you want to find the uh minimum or approximate minimum subject to variations of the parameters of this loss functional so how do we get scalar values out of a quantum computer it gives us a wave function so do we what do we do with it well we have to define a loss operator which is a quantum observable right or Hermitian observable and often we decompose it into small chunks that we can measure independently um and combine later on and our goal similarly to you know in the v the case of vqe and many other variational algorithms is to find uh the minimum subject to variations variational variations the parameters uh the expectation value of this loss operator right sometimes it's called the energy or the Hamiltonian but i like to generalize it to uh you know loss for quantum machine learning so there's just a refresher this is the typical way when trains a vanilla uh what i call vanilla quantum variational algorithms or vanilla quantum neural network you have a loop between a quantum and classical computer and the quantum computer gets an expectation value feeds at the classical computer classical computer has an optimization algorithm that's classical suggests and use values of the parameter and you iterate like this in a sense you know our current quantum computers are restricted in how much quantum depth and uh you know what kind of quantum states they can represent they're restricted in depth because of the noise and so it makes sense that we search over the space uh given this constraint of low depth circuits we should search over the space to find the quantum state and this is this is why this is kind of taking over because for the nisk era or you know early fault tolerance we're going to be searching over the space of states that are uh not too big not not too you know long to quantum compute um so why learn quantum representations in the first place if you if you allow a meal modify uh Feynman's famous quote uh Feynman said you know nature is in classical gamut if you want to make a simulation of nature you better make it quantum mechanical and in our case it's if you want to learn a representation of nature you better make it quantum mechanical right so quantum states and quantum processes i think we've been we've been mentioning this can exhibit high levels of quantum forms of correlation such as entanglement and that's exponentially hard to represent in classical memory right if you have a uh random circuit producing in a highly entangled state it's very hard to approximate it and it's hard to prove uh theoretically without a doubt but every algorithm we've tried to simulate quantum circuits it seems to uh fall flat on its face at some point right um yes um quick question clarification from joe here is the loss calculated by classical computer is the loss calculated so he's talking about the expectation value of the L operator right right it takes actually several samples to estimate the expectation value right you could think of it as like sampling from a distribution if i'm trying to do an estimator of a uh random variable subject to samples from a certain distribution it takes several samples so there's there's like a mini loop in here to estimate the expectation value here and that's a mixture of uh you know doing several measurements on the quantum computer and saving saving the results on the classical computer and then the classical computer can aggregate the the various results to get an average right and maybe to paraphrase you mean you you run that same use circuit with the same inputs the same initial state the same parameters several different measurements yes well i guess the the loss operator may have several uh non-commuting uh sub operators and one wants to get an expectation value of each uh term in the sum and then one adds up all these terms to get an expectation value of the sum so that is called the quantum expectation estimation uh sub routine in a sense and here i kind of abstracted it out but it's it's an extra sub routine there's a mini loop of trying to get a precise estimate it's not to be neglected because if you want you know a 10 to the minus seven precision for an energy it could take you hours on a 10 kilohertz machine for example so it's it's an important thing to consider when designing quantum algorithms that we only have noisy or estimates of our our loss function yeah so basically if you run that um yeah so you break up the L into sub operators which you measure you know you measure by running multiple times you get you don't need the distribution here for this you only need the actual mean value yes right or at least in the in the typical vanilla case but as we'll see there's there's other other variants out there but uh usually the quantum part it's it's hard to get a scalar out of the quantum computer by something else than defining an observable and outcomes of measurements right and how important would read out errors and skew and to read out being in a instance you get a p1 probability 80 but you have to skew because of the loss t1 process and things like that that's right you get a imprecise estimate of your your loss function and uh you need to have classical algorithms on the side to compensate for that imprecision or to choose your optimizer wisely in a way that is robust in noise right and uh i see a lot of questions i i hope i can get to all of these we may we may have to yeah for the end here okay um maybe just quick one here does the quantum advantage come from generating the variational forms um i mean you know i am not claiming a quantum advantage yet but i would say that uh if there if there was a quantum machine learning advantage it would likely come from uh being able to do the inference or prediction step with your model and hence the ability to train it as well so both the training and inference are rendered possible once you have access to a quantum computer if you incorporate a model that has high quantum complexity so a large unitary that we can't simulate classically um yep and i think this next one you're going to talk about which is back propagation you know can you see yes yes yes um okay so how to practically leverage a quantum computing power well for discriminative models for example you can uh you know let's say you prepare a quantum data set because again for now we don't have a quantum internet where we can import uh data that'd be nice someday uh and you evaluate your quantum model let's say you do a feed forward or a unitary parametrize model you get the expectation value of say several observables that becomes a vector you feed that vector to a classical neural network and then it evaluates some some prediction based on this say a label or whatnot and the idea is that you can train both your classical uh part of your network and your quantum part of the network together uh via a form of quantum classical hybrid back prop and the idea is that you know your quantum neural network can can have all sorts of components uh but it could itself be a building block in a sort of meta network between quantum neural networks and classical neural networks and the idea is that if you zoom in on say a little uh sandwich of of nodes here are meta nodes of a deep neural network a quantum neural network and a deep neural network so the let's say a deep neural network or any differentiable computation feeds parameters to quantum neural network uh and then you have the measurement of several observables at the output which you feed to a classical neural network and and then you could do other stuff later on uh and you get your loss function here then you get the gradient of the loss function back propagate uh your gradient classically and effectively what's interesting is that this thing is a actually a itself is is technically an observable on this space if you could you know invert this function but the idea is you do a first order approximation so you get an effective back propagated gradient Hamiltonian which becomes or you call it a Hamiltonian because it's an observable and then it becomes just like taking gradients of a vqe to obtain the gradients of these parameters you just have an effective value of the gradient for a certain value of your your you know all your parameters over here and your loss function and you could take gradients of uh that's with respect to your parameters and you've effectively back propagated the gradient of this value through the q and n and you could keep going and this is important because you don't want to have to do a slight change do your whole chain of computations see how it changed and then backtrack it's it's more scalable uh this way so okay so there's some software uh that does this i have to plug it i mean it's one of my pet projects uh for it's been so for a while uh for now it's uh it's uh interface between cirq and tensorflow there's some open source contributors that are working on quiz kit compatibility so that's going to be exciting for the quiz kit community and we're supporting them but it allows you to you know automate this this training and integrate it into you know advanced machine learning models in tensorflow and you know tensorflow i think has the record of on ibm supercomputers for you know the biggest machine learning computation so i think it's important to uh to ideally integrate uh quantum computers with uh the power at least one of the most powerful frameworks for high performance computing on the classical side um so any question there in that vein um this is an earlier question for me to are there any data sets filled with the quantum machine learning models you can map up some notebooks for a slayer uh that's uh i think that's public that we're working on that and we're trying to work with other you know other uh companies in the space to make sure we we agree on what a form for a data set will be but in general because you can't download quantum data you can't just save you know states because they take exponential space and you don't know how to load them on your quantum computer the data set takes the form of a circuit um or a set of circuits and those define wave functions that you could then do quantum deep learning on and it's something that's uh being worked on but you'll have to stay tuned uh for that thank you cool okay so uh what can one do with hybrid feed forward networks uh i'm going to skip over this uh yeah quickly i guess there's a paper by luke in which is a convolutional neural network which uh are inspired with from the mera if you're in in the know about it uh basically it's luke using the fact that uh if you know your system is translationally invariant so it has some symmetry you reflect that symmetry in your your choice of parameterization of your quantum neural network and so this is just a quantum neural network that has translational invariance and is hierarchical and the idea is that you know maybe you can't do all the quantum layers but maybe you could do only one quantum layer and already you'll you've down sampled the problem you've reduced that dimension dimensionality and you've broken up some entanglement or you've you've like disentangled partially remember for compression you got to decorrelate everything all right so the idea is that you can do you could input various quantum data in batches you could apply various feature maps that are quantum convolutional networks and then you get kind of images from you know all your histograms of samples of your bit strings and following this you could apply classical convolutional layers and you know finish the job with fully connected and at least in our early experiments uh hybrid networks with multiple filters were better than one quantum network and that's without noise so with noise on the device it's even better uh but that's just a an example of discriminative learning i won't go too much into that but in terms of applications it would be for example classifying phases of matter detecting whether something is superconducting or not and the idea is maybe you train on a data set of a material you know is superconducting at certain value of the parameters and temperature and and then you you ask the neural network to detect for another material that you don't know whether it's superconducting or not at certain value parameters so generalizes so that's uh that's one quote-unquote killer app we think for quantum neural networks um yeah so it's just comparing the two with with our old diagram okay so i guess we'll get to the meat of the talk uh i'm not too bad halfway there i guess um so uh how can we extend these insights and how can we hybridize um in a meaningful way with classical machine learning capabilities for quantum machine learning right let's go back to our slide of deep generative modeling we have our data set we have our variational classical distribution we want to minimize our question before we deep dive here uh it's about um nlp and maybe can we use some of this quantum representation and nlp transformers to reduce the huge size of it to increase accuracy i hope that's a maybe a little bit out there question um so i mean we our team has some public work that we've used tensor networks which are you know analogous to quantum circuits in a sense to find factorizations of large matrices uh and we apply them to the transformers and at least in our demo we get a two times speed up uh and of course um you know that'd be great if um such a tensor network could be contracted on a quantum computer uh faster it's not an experiment we've tried yet but um you know it's going to come down to constant speed ups uh you know uh you know our tensor network versus a quantum computer for certain tensor networks the quantum computer is exponentially faster but for other tensor networks it's going to be similar uh potentially um so that is that is a good question um but uh i guess i guess we'll we'll have to see on that side but it's an interesting area of research in a sense uh dimensionality reduction uh using quantum circuits um and uh you know tensor networks are a first step towards that um but it's uh you know it's encouraging to see that cutting edge ml can be improved with quantum or quantum inspired methods at least today um so yeah at least in nlp that's the area that i'm confident saying something that quantum computers would be potentially useful um all right uh so so i mentioned we want uh our data set degree uh you know for our data points in general when you want two uh distributions to agree you do what is called the k l divergence uh it's not a symmetric function so be careful uh you could go you go one way or the other uh between your true distribution your data distribution and your uh variational uh distribution right and it's like here would be the expectation value specter of data of the ratio of logs right so the idea is that to evaluate this kind of gold standard of uh quantum statistic or sorry classical statistical distribution uh we need access to the log of our logarithm of our uh model for any given data point x that we sample from the data right so not every uh classical machine learning model allows you to do to do this right so gans for example don't have an explicit logarithm of the of the density of your generative model that you could query it's implicit it's only you know the discriminator telling you how well you're doing but it's not a notion of log whereas you have let's say you do a bunch of transformation that um that you know uh the determinant of the jacobian you could compute that efficiently right the determinant of jacobian if you if you continuously transform a space right and you had initially a simple Gaussian on that space and you end up with a complicated space you've kind of you know bunched it up and you've done some complicated different morphism you could back track how the notion of volume locally has changed right and uh for any you know value we target here we can kind of invert um the measure in a certain bin here to uh some set of bins over here and we know the value of a Gaussian analytically and so you can compute in a sense somewhat efficiently analytically the um the density of your your probability distribution for any point you query uh this is called a normalizing flow but there's other types of models you know there's energy based models there's auto regressive models there's a whole bunch of of cool models out there but a lot of people know gans because it's like the entry entry level thing because people understand discriminators but so we encourage you to check out other types of generative machine learning and in a sense we're we're looking to have an explicit uh notion of a log uh for reasons that are going to become apparent in a second in the quantum case so how can we extend this philosophy to quantum theory uh you know what's the intersection of quantum theory and probability theory right well there is you know just like in in black holes uh we look at black holes because they're at the intersection of quantum and gravity so they're an interesting test bed well here we look at mixed states because they're at the intersection of probability theory and probabilistic machine learning and quantum theory and quantum machine learning so mixed state in general can be a probabilistic mixture over mixed states these are matrices instead of vectors now so be careful but uh any density operator has what is called a spectral decomposition so it it's always expressible as a mixture of orthogonal pure states and this this mixture sums up to one so it has a probabilistic interpretation so we go from vectors to a density matrix and each element is in complex numbers so how would we represent mixed states so how would we represent the intersection of probability theory and quantum theory well we should have a model that composes a probabilistic model with a quantum model right and that is the idea of quantum probabilistic hybrid deep learning or deep representations hence the title of my talk so as we've seen quantum neural networks are typically unitary feedforward like this and they have a hypothesis class that is pure states we can combine here a classical parametrized probabilistic model that that we can sample and let's say this would flip your qubits you flip your qubits to prepare a bit string then you apply unitary that's parametrized and what you get at the output instead of a parametrized class of pure states is a parametrized class of mixed states right and uh you know your parametrized distribution your state at this point is a diagonal state so it's effectively classical it has no quantum correlations you can try to show this show there's no coherent neutral information exercise and then after that you tack on a unitary which is hard for classical computers to do the idea is we use classical computers and we make them you know we make them sweat right like inference of probabilistic models can be pretty computationally intense uh and then we combine them with unitaries and the quantity and let's talk about how you think we set for capital omega oh yeah so capital omega is just an index over your basis of your Hilbert space it's kind of a general formulation because we actually uh phrase the algorithm both for qubits and for continuous infinite dimensional Hilbert spaces so theoretically could be a an integral or something it's just general math but it's uh it's an index that it's a index set that runs over an index for your your entire basis that spans your whole Hilbert space of interest oh yeah and then you can choose basically any probability over basically anything but there's going to be certain types that are preferential for for training reasons as we'll see uh again you know you could parametrize anything classically but it's not every model that's easy to train again because let's say you need the log and you can't get it or can't get the gradients then it's difficult so as we'll see we can choose wisely how we parametrize things so that we can get nice gradients and can train things because how do you train continuously parametrize the hypothesis class gradient based methods so you use kind of the notion of steepest descent in the landscape of parameter space and maybe one more question I'm told I have to speak a little bit louder so hopefully for sure for I mean let's take the extreme case you take a case where you have a pure like harm mixture like you know you have a pure mixture of all states so I'm guessing that's not very useful one so in a way you want your state to be a little bit mixed that's somewhat pure or are you okay having like a purity of like zero or maximally mixed even though it's so if you were to optimize over architectures so tune and we have some new results that are not in the paper for this talk you could tune how much quantum depth you you you assign to the the unitary um theoretically this this approach could be tuneable in the sense that if the data set that you're trying to represent is purely classical and has no quantum correlation and the identities in the span of your unitary hypothesis class you could learn to just apply the identity and then it's a classical machine learning system if it's a pure state that you're learning this the probabilistic component is useless in a sense because it's going to be all unitary you're just trying to learn a pure state so it's an adaptive way to separate out the task of quantum and classical machine learning of a quantum uh mixed states and it's quite cool because you have one framework where you have as a subset uh classical generative modeling of distributions um so in a sense it it can via self-tuning it could adapt to use no quantum resources or use no classical resources or any you know continuum in between and uh we have time here two more quick questions so are you using mixed states for the input as sort of maybe in practice i guess at this level it's all a little more we're we're going to see that i guess uh it's you could use output or input so yeah um i think i still have a good number of core slides but i guess i'll i'll go through them faster a bit um we can run a little longer okay okay so you know why should we care about quantum mixed states well you know thermal states um are at finite temperature and so you know most systems in nature at finite temperature unfortunately our quantum computers are not at zero temperature so even them themselves must be modeled as mixed states uh if we were to be accurate and experimentalists know this theorists like to say it's a pure state um so you know so the ability to simulate mixed states is is crucial to nature and and the reality is like you know we're trying to use quantum computers to simulate nature but nature itself if you core screen enough you zoom out there's a quantum to classical transition right you know we're used to having classical physics having quantum physics and then there's a continuum in between so the point is to have a set of continuum of models that can model that in between at finite temperatures when quantumness dies down and uh it becomes classical uh or you know when you're very close to being fully quantum right uh most quantum systems are open quantum systems i've mentioned this and for various reasons subsystems of of quantum states uh have are mixed states because of entanglement if you take a reduced state of a pure entangled state you get a mixed state um so just looking at patches of things at a time to model them it's important so what sort of mixed states in nature can we variationally simulate using something like this well thermal states is of of great interest because they're they're omnipresent so the algorithm that leverages quantum probabilistic generative models to model thermal states is uh variational quantum thermalization and or vqt uh and uh so the problem is given a Hamiltonian and h and a target temperature uh one over beta then generate the thermal state which is the exponential that is normalized like this and this is the partition function and the idea is okay well we'll use one of our magic models of classical probabilistic distribution and a unitary uh and how are we going to converge to uh the thermal state well thermal states are the minimum of something called free energy right so free energy is you know uh roughly ignoring temperature energy minus entropy right uh so we can evaluate the energy you know just like in vqe of our model and subtract the entropy right so how do we get the entropy well because unitaries can serve entropy the actually actually the entropy comes strictly from our classical part of the model and if your classical model has ways to get gradients of entropy you're in business or you know sometimes it's simple enough you could get it analytically and this is equivalent defining you know the minimum of the relative entropy between our model and the thermal state so you know we know that the unique minimum of this function is when the two states match so if we do our job well and we parameterize things well and find the absolute optimal states uh then uh you know we've got the jackpot um state of minimal for energies thermal state so how do we parameterize our quantum probabilistic generative model i've been pretty abstract now so we're just going to zero it in slightly uh well uh the motivation for this work was to take inspiration from recent work by uh open ai and such on modern versions of energy based models where one it's it's now it's taking inspiration from physics right so you define an energy function using a classical neural network let's say from the space of bit strings or continuous values to to a scalar and you could use various algorithms that leverage gradient information such as Hamiltonian Monte Carlo or stochastic gradient Langevin dynamics uh you know all all there's a bunch of open source frameworks to to do the this part you could sample the landscape by in a sense having a noisy ball traverse this landscape and you get samples of the exponential this way or the Boltzmann distribution known in physics it's a classical Boltzmann distribution so you parameterize the energy and why why is that going to be useful well uh it has all sorts of compositionality perks it's it's very good it's comparative with GANs this is work by open ai 2019 so how do we leverage these models and integrate them with quantum computers well so you know what if we had our probabilistic part of our model was a classical energy based model like this so it's parameterized energy function and then our distribution is a Boltzmann distribution again well if you you you could define a diagonal operator which is the log after you flip some bits which is your energy function on the diagonal and what you get if you do some math with some unitaries and exponentials is that you've just parameterized a operator the diagonal is parameterized by a neural network and and the total operator is this conjugation of a unitary with this diagonal operator so you've you've parameterized a Hamiltonian operator and your hypothesis class is a set of thermal states so in a sense you you know you know you're targeting a thermal state so you might as well have a hypothesis class of thermal states right um okay so if we have this assumption that we're using an energy based model how do the gradients work out well there's a bit of math involved i've skipped many lines but it is possible to sample it essentially you have to get bit strings at the output of your model and you can evaluate you can compare the value of the energy of your your bit strings minus the the energy of your model and you could also evaluate gradients of your your model if it's parameterized by a neural network and you you do the sampling which only depends again on sampling from uh from your uh your model p theta of x and you have sampling algorithms and you can evaluate gradients in a sense it's an analytic way to guarantee that your estimates of your gradients are unbiased um and how do you get gradients for the quantum part well the quantum part is just the usual i hope you've seen this in other talks and i don't have time to cover it unfortunately today it's the parameter shift rule right which is how you take gradients in the vqe which is how do you take gradients of a a unitary a state fed through a unitary and an expectation value so i won't cover that but it's very standard it's you know a standard in the software framework and there's various papers that use this um it's a cool theory but you know does it work right uh the answer is yes you know if you if you have a target thermal states you can uh do a reconstruction like this this is for some heisenberg spin model we use very simple classical distribution here it was just Bernoulli so random coins uh and and the quantum computer could do a lot of work and and learn to represent a thermal state we've done much larger systems but you know a jarbled set of pixels is not necessarily the most aesthetic thing so we we choose to feature the smaller systems but uh we've scaled things up quite a bit and uh the idea is um you know the function we're optimizing is relative free energy but the other metrics of quantum statistical distance uh also converge uh so uh it seems to work um we've also tried some set of fermionic systems and bosonic systems for example a simple you know toy model of a superconductor that's uh bosonic uh sorry Gaussian fermionic so it's quite simple we can plot the correlation functions the target this is at iteration zero and it converges by iteration hundred of gradient descent pretty well um so this is actually a new result i i'd like to feature that's not in the paper uh but it's coming in the second version of it uh can we tune how much quantum versus classical resources we use right so suppose i look at this heisenberg model and i look at after training how how well i do in terms of trace distance and fidelity depending on the temperature and the number of quantum layers i use ah well we see there's certain sets of temperatures uh that uh you know you need more quantum layers to to model them right and it's not necessarily you know at this point it becomes trivial uh at this point there's a nice balance between quantum and classical resources and this is the fidelity is trace distance uh but uh this is kind of what you you'd like to do right you want to use as little quantum resources as possible uh in order to have an accurate representation of a state so this is something we started investigating but it's uh you know it maybe has some deep implications about what's the true quantum complexity of a quantum machine learning problem uh and uh i guess uh you know uh please take a look at these qr codes there's links to various notebooks and you know i've been advertising tensor flow quantum but there's there's obviously you know implementations in quiz kit from the community uh shout out to jack seroni and uh you know the tensor flow quantum implementations by my collaborator antonio martinez um and three two one take a picture on youtube or whatnot and look at the uh websites for these notebooks so the final component is more machine learning it's less quantum simulation is how do we use vqt to do quantum machine learning so if we're given quantum mixed state data how do we learn from quantum mixed state data so again we're going to use our quantum Hamiltonian based model because for reasons that are going to become apparent in a second so we call the task of learning to replicate right we want an approximate density matrix that approximates a data density matrix so a data density matrix could be itself a mixture of a bunch of density matrices we're just trying to approximate this thing and we want to find a set of parameters such that for the optimal parameters our hypothesis class approximates this density matrix and we assume we have access to the quantum form of the data okay and the idea is if you use a quantum Hamiltonian based model and you aim to minimize now the relative entropy in reverse from last time uh what you get is if you ignore the terms that don't depend on your parameters you get something called the cross entropy which is this the trace of stigma which is your data log row right and again because we've parametrized our hypothesis class in terms of its logarithm its quantum logarithm uh we can evaluate this energy this it's called modular energy or modular free energy and modular Hamiltonian is just a name for the log of a density matrix okay and so we're trying to learn a log of a density matrix such that the exponential replicates our data set and how you do this you plug your data you run it in reverse through your unitary of your quantum probabilistic model you sample it and then you get expectation values of the diagonal operator and this could be parametrized with a neural network so you can have more computation here the extra term here is all on the classical computer turns out you could also get gradients for these i won't go too much into it the quantum part is again parameter shift but these gradients again if you have a differentiable function for your energy you know like a neural network then you can evaluate you could sample these gradients and it's unbiased which is really cool that that's really important that we could get good estimates of the gradients and you know it works out if you don't use enough quantum or not enough layers of your quantum computer or not enough complexity of your classical distribution sometimes it doesn't work well so for various temperatures we've tested this and i guess this is a this is you know there's many things you could do once you have unsupervised learning for example you could learn a compression code so here we actually applied hopefully some of you know about bosonic quantum computing but theoretically could be applied to other forms that are not qubits and here we learn a compression code where we could throw you know 40 percent of a harmonic chain in the compressed space and still reconstruct the states so this is the error matrix of the density matrix point seven we start seeing errors and if you throw 90 percent of stuff out things go bad and there's theory that you can find the logarithm modes the modular modes of the system and so we checked it with theory that's why we looked at the system thank you but that's it that's corpse there sorry sorry what are the x and y axes on that curve again this curve on the on the left oh on this side so this is the density matrix it's uh it's the discrepancy between the so we go to compressed space it's like an auto encoder we go to compressed space and then we throw out uh what is like so okay so we we do we learn a vqt and the latent model is a product of individual um thermal states of harmonic oscillators right and those are like quantum forms of gaussians which is kind of cool and we throw out the lower entropy latent modes okay because the entropy represents a harmonic oscillator sorry or when you say a mode you mean harmonic oscillator of a harmonic oscillator yeah so this is in this is for say a bosonic continuous variable quantum computing and i did most of my time and continuous variable stuff before so myself as well in theoretical physics uh this is similar to a calculation of the hawking effect actually um i that's a whole two hours i won't go into that but actually here's the interesting thing i have this in my summer school lectures that are up and coming uh there are only two types of physicists those for whom all of physics is qubits and those for whom all of physics is oscillators i try to i try to uh play on both sides so uh hopefully someday we can have hybrid computers that'd be cool through everyone's that's right um so yeah so we agree with theory here um i could explain how this is related to the hawking and unruh effects but uh that would take some time but it's it's an interesting uh thing that quantum machine learning could theoretically understand or learn an analog of the hawking or unruh effects that you there exists a certain set of modes that an observer feels a thermal statistical fluctuations of the vacuum so this was the ground state we plug it in and if you transform it then it becomes a product of thermal states and instead of Fourier modes it's like these weird squished modes uh of the lattice so it's kind of information theoretic uh eigen modes instead of you know we're used to eigen modes in physics like the resonance but here it's kind of uh the resonance of of the log uh Hamiltonian which is the modular Hamiltonian and uh this brings us actually to the end uh of the talk and uh oh i have luckily haven't exceeded too much uh so we do have time for questions i guess but i just want to conclude i guess uh you know this is the beginning of a whole research program it's an exciting area and you know by starting from basics of information theory right we just started thinking about relative entropy and inspiring ourselves from physics we have discoveries and machine learning and hopefully now we could apply this back to the physics right so it's a feedback loop between physics and machine learning and it's that's a big part of the philosophy of our team at x uh yeah thank you yeah thank you very much um i think there were some questions during the talk that i didn't get to so maybe i'll gonna run up the chat here and get back to them again um okay i guess i'll just do a shout out to anntario my collaborator at waterloo uh he was google and x and jacob was instrumental to a lot of the vqt and and qmhl work and uh you know did a lot of the work there uh as well so big shout out to them uh but uh yeah so any questions in the chat i've seen a lot of questions in the chat here so let's uh let's get let's see how many how many we can answer i guess all right let's start with the latest one which is about temperature i think the question is um now is there a sense of critical temperature here relative to some sort of phase transition the you know the temperatures where you get noisy data closely the critical temperature some sort of phase transition in this innovative system or is that well i don't know if we purposely chose a system where we knew there was a phase transition but we can kind of see that there's different uh regimes where you need more entanglement or need less entanglement right so so seeing how many layers you need to represent a quantum state could be like seeing a dip in that could could be a way to detect different phases or quantum phases of matter i guess like you know regimes of parameter space that have very strong entanglement and regimes that are you know slightly you know almost trivial um but uh yeah i'm not sure if we purposely picked a system that we knew there was a phase transition we just observed this data for now but um maybe something to do uh better on yeah um let's see this one uh this one is about universal estimators basically can uh q and n's the quantum neural nets be used to imitate this kind of behavior i guess and we're saying you know given that three layer neural nets are regardless universal estimators in classical machine learning yeah that's a that's a good question so i guess you know you want if you have a universal functional approximator um then you know theoretically you can have a a universal uh you know you span the space of functions that you could represent of course any classical computation can be embedded uh if you write it out as a reversible classical computation using many extra registers and you keep the whole history of the computation if it's not reversible functions you can embed that right in quantum computations with toffoli gates instead of hand and so on and some of work several years ago i i showed how to take you know typical classical neural networks and make quantum circuits that implement the classical neural network in superposition um so the idea is yes i think you can use quantum neural networks to do the classical probabilistic machine learning components um though so far at least from uh the current state of the art of the theory it seems like quantum computers will have a polynomial speedup for inference probabilistic inference similar to grover speedup uh and that of course if we're competing with extremely large classical computers will be mostly relevant when quantum computers are of a size comparable to the square root of our largest supercomputer uh and um that is yeah that's i guess that's uh that's my answer so for now i guess the most practical approach is to use classical algorithms and classical computers for the classical component and use quantum computers for the truly quantum component which is the unitary that seems like a very nice sensible thing there is a question this one is interesting i think it's more of an opinion question maybe um we know that quantum Fourier transforms very t and usually digital computing um does it have a role in machine learning is it similar in here according to what is the relationship with advantage or the Fourier transform right so i guess here we parametrized our quantum neural network uh as a general uh bosonic um what is called book all above or Gaussian transformation and the discrete Fourier transform is a subset of such transformations and here we we learned these transformations so uh technically uh if we fed the whole system and we asked it to find the eigen modes and if we had a thermal state of this system say via vqt and then we fed it to quantum modular Hamiltonian learning these modes would be the Fourier modes because we know the eigen modes of of this Hamiltonian right we know how to decompose this this Hamiltonian uh into a sum of individual uh you know number operators um and it's the same you know finding this book all above transformation is what i mean by it's related to the unrefect uh calculation qft in curved spacetime i know there are some chat messages that were doubting that but uh i did my masters in quantum field theory in curved spacetime so you can trust me on that one uh but uh great so and maybe for the final question here um and taking the extra time um if we want to do research in this field where should we start or what should be out of the direction of research right i mean that's a that's a good question i guess uh you yourself have gone through this position not you know four years ago i think you mentioned right right right right so i guess in my case i started you know i started with uh the open you know source uh massively online courses mooks i just listened to that listened to a few of them and then i progressed to i wish i had all my textbooks here but uh they're they're back there but uh the good fellow the in good fellows textbook um inventor of gans and then uh murphy kevin murphy uh a goobler uh he did uh what i called a kind of the nielson and schwang or the bible of probabilistic machine learning and i think there's mckay there's information theory for machine learning that's if you want the textbook routes otherwise i think with time as the field stabilizes i guess because it's been moving so fast everybody who's involved in it is just cranking out papers rather than creating coursework uh there will be coursework um i could link a you waterloo course uh that i gave some guest guest lectures at that that featured some quantum machine learning uh but uh overall i would say it's important to understand the theory of classical machine learning at the fundamental level because you know similar to hardware engineering we're at the fundamental level of engineering a new computing stack so on the theory side we're re-engineering a whole algorithm stack so we got to start again from first principle so you know you have to trace back to like papers from the 80s of machine learning and and the fundamentals and then and then work your way back to the modern uh modern thing so i would say the the modern ml stuff is flashy and and and fun to stay up to date but i would say you know take the time go back to the the core old literature you know the foundations so um yeah uh mooks and then textbooks is the way to go that's that's what i did and here i am so uh and then maybe i can just add that now they're summer school classes coming online so i mentioned the quantum information kiscuit one i think there's a touching of ml and things like that in the last two lectures in quantum chemistry vqe's definitely on there so fantastic anyone interested in addition to to what you said you know we can add that um so i think it is that time that i get thank you again and thank the listeners for joining the quantum live seminar series uh we're back this friday um i will mention next week we're back so this at the same time uh continuing with the talk by antony miss capo from idea on quantum chemistry we could talk about variation of quantum eigen solvers q a o and things like that on chemistry things so that will be a very nice uh follow up to your talk gion and uh thank you thank you for inviting me it's uh it's been an honor and uh hopefully the quantum community is interested in quantum machine learning now uh so i've done my job there all right thank you so much follow gion on twitter quantum bird that's right all right so any final words and otherwise thank you and we'll see you next week uh that's it for me thanks again for tuning in and uh stay home stay safe everyone and uh thank you all right it was a pleasure gion we'll see you soon guys take care cheers", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.12, "text": " global digital summer school on quantum computing and I see we have reposted", "tokens": [50364, 4338, 4562, 4266, 1395, 322, 13018, 15866, 293, 286, 536, 321, 362, 1085, 555, 292, 50670], "temperature": 0.0, "avg_logprob": -0.2537589456843234, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.22817564010620117}, {"id": 1, "seek": 0, "start": 6.12, "end": 11.8, "text": " the link all right so oh Sausalito California that's where it's very close", "tokens": [50670, 264, 2113, 439, 558, 370, 1954, 6299, 11765, 3528, 5384, 300, 311, 689, 309, 311, 588, 1998, 50954], "temperature": 0.0, "avg_logprob": -0.2537589456843234, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.22817564010620117}, {"id": 2, "seek": 0, "start": 11.8, "end": 16.240000000000002, "text": " to where I grew up now we are thrilled to roll out the latest episode of the", "tokens": [50954, 281, 689, 286, 6109, 493, 586, 321, 366, 18744, 281, 3373, 484, 264, 6792, 3500, 295, 264, 51176], "temperature": 0.0, "avg_logprob": -0.2537589456843234, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.22817564010620117}, {"id": 3, "seek": 0, "start": 16.240000000000002, "end": 20.72, "text": " quantum seminar series dedicated to the research and academic communities this", "tokens": [51176, 13018, 29235, 2638, 8374, 281, 264, 2132, 293, 7778, 4456, 341, 51400], "temperature": 0.0, "avg_logprob": -0.2537589456843234, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.22817564010620117}, {"id": 4, "seek": 0, "start": 20.72, "end": 27.0, "text": " this seminar takes place every Friday at noon Eastern time right now at this hour", "tokens": [51400, 341, 29235, 2516, 1081, 633, 6984, 412, 24040, 12901, 565, 558, 586, 412, 341, 1773, 51714], "temperature": 0.0, "avg_logprob": -0.2537589456843234, "compression_ratio": 1.6483050847457628, "no_speech_prob": 0.22817564010620117}, {"id": 5, "seek": 2700, "start": 27.0, "end": 31.6, "text": " on the Kiskin YouTube channel and I'm delighted to see so many of you already", "tokens": [50364, 322, 264, 591, 271, 5843, 3088, 2269, 293, 286, 478, 18783, 281, 536, 370, 867, 295, 291, 1217, 50594], "temperature": 0.0, "avg_logprob": -0.3004908561706543, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.07029137015342712}, {"id": 6, "seek": 2700, "start": 31.6, "end": 36.32, "text": " tuned in I'm your host like a minute from IBM quantum research and today I have", "tokens": [50594, 10870, 294, 286, 478, 428, 3975, 411, 257, 3456, 490, 23487, 13018, 2132, 293, 965, 286, 362, 50830], "temperature": 0.0, "avg_logprob": -0.3004908561706543, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.07029137015342712}, {"id": 7, "seek": 2700, "start": 36.32, "end": 42.76, "text": " privilege of hosting young we're done from Google X and and also from the", "tokens": [50830, 12122, 295, 16058, 2037, 321, 434, 1096, 490, 3329, 1783, 293, 293, 611, 490, 264, 51152], "temperature": 0.0, "avg_logprob": -0.3004908561706543, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.07029137015342712}, {"id": 8, "seek": 2700, "start": 42.76, "end": 46.2, "text": " Institute of quantum computing at Waterloo young will present some very", "tokens": [51152, 9446, 295, 13018, 15866, 412, 8772, 38511, 2037, 486, 1974, 512, 588, 51324], "temperature": 0.0, "avg_logprob": -0.3004908561706543, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.07029137015342712}, {"id": 9, "seek": 2700, "start": 46.2, "end": 51.96, "text": " nice results and hello young how are you today hi is that this let go happy to", "tokens": [51324, 1481, 3542, 293, 7751, 2037, 577, 366, 291, 965, 4879, 307, 300, 341, 718, 352, 2055, 281, 51612], "temperature": 0.0, "avg_logprob": -0.3004908561706543, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.07029137015342712}, {"id": 10, "seek": 2700, "start": 51.96, "end": 56.8, "text": " be here yeah it's a pleasure to see you last time we met I think it was right", "tokens": [51612, 312, 510, 1338, 309, 311, 257, 6834, 281, 536, 291, 1036, 565, 321, 1131, 286, 519, 309, 390, 558, 51854], "temperature": 0.0, "avg_logprob": -0.3004908561706543, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.07029137015342712}, {"id": 11, "seek": 5680, "start": 56.8, "end": 61.28, "text": " after March meeting was cancelled and we were at March meeting so I'm glad", "tokens": [50364, 934, 6129, 3440, 390, 25103, 293, 321, 645, 412, 6129, 3440, 370, 286, 478, 5404, 50588], "temperature": 0.0, "avg_logprob": -0.26435439403240496, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.0034784777089953423}, {"id": 12, "seek": 5680, "start": 61.28, "end": 66.36, "text": " that we can continue the discussions and have everyone join us today on the call", "tokens": [50588, 300, 321, 393, 2354, 264, 11088, 293, 362, 1518, 3917, 505, 965, 322, 264, 818, 50842], "temperature": 0.0, "avg_logprob": -0.26435439403240496, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.0034784777089953423}, {"id": 13, "seek": 5680, "start": 66.36, "end": 71.08, "text": " that's right the science goes on to multistimes but that's right indeed", "tokens": [50842, 300, 311, 558, 264, 3497, 1709, 322, 281, 2120, 468, 1532, 457, 300, 311, 558, 6451, 51078], "temperature": 0.0, "avg_logprob": -0.26435439403240496, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.0034784777089953423}, {"id": 14, "seek": 5680, "start": 71.08, "end": 75.75999999999999, "text": " indeed I hope you've been doing well again in addition to being a social", "tokens": [51078, 6451, 286, 1454, 291, 600, 668, 884, 731, 797, 294, 4500, 281, 885, 257, 2093, 51312], "temperature": 0.0, "avg_logprob": -0.26435439403240496, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.0034784777089953423}, {"id": 15, "seek": 5680, "start": 75.75999999999999, "end": 79.24, "text": " media celebrity is a human though is a PhD candidate at the University of", "tokens": [51312, 3021, 18597, 307, 257, 1952, 1673, 307, 257, 14476, 11532, 412, 264, 3535, 295, 51486], "temperature": 0.0, "avg_logprob": -0.26435439403240496, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.0034784777089953423}, {"id": 16, "seek": 5680, "start": 79.24, "end": 83.32, "text": " Waterloo at the Institute of quantum computing I had a very nice visit there", "tokens": [51486, 8772, 38511, 412, 264, 9446, 295, 13018, 15866, 286, 632, 257, 588, 1481, 3441, 456, 51690], "temperature": 0.0, "avg_logprob": -0.26435439403240496, "compression_ratio": 1.6891385767790261, "no_speech_prob": 0.0034784777089953423}, {"id": 17, "seek": 8332, "start": 83.32, "end": 88.96, "text": " recently so great place he is also a research scientist at X Google X or", "tokens": [50364, 3938, 370, 869, 1081, 415, 307, 611, 257, 2132, 12662, 412, 1783, 3329, 1783, 420, 50646], "temperature": 0.0, "avg_logprob": -0.2550923736007125, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0066835046745836735}, {"id": 18, "seek": 8332, "start": 88.96, "end": 95.02, "text": " alphabets research and development lab before this Guillaume briefly worked at", "tokens": [50646, 419, 950, 455, 1385, 2132, 293, 3250, 2715, 949, 341, 2694, 5291, 2540, 10515, 2732, 412, 50949], "temperature": 0.0, "avg_logprob": -0.2550923736007125, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0066835046745836735}, {"id": 19, "seek": 8332, "start": 95.02, "end": 99.19999999999999, "text": " Google AI quantum and was one of the co-founders of TensorFlow and project he", "tokens": [50949, 3329, 7318, 13018, 293, 390, 472, 295, 264, 598, 12, 17493, 433, 295, 37624, 293, 1716, 415, 51158], "temperature": 0.0, "avg_logprob": -0.2550923736007125, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0066835046745836735}, {"id": 20, "seek": 8332, "start": 99.19999999999999, "end": 102.67999999999999, "text": " holds a master's in math and quantum input from you Waterloo as well as in", "tokens": [51158, 9190, 257, 4505, 311, 294, 5221, 293, 13018, 4846, 490, 291, 8772, 38511, 382, 731, 382, 294, 51332], "temperature": 0.0, "avg_logprob": -0.2550923736007125, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0066835046745836735}, {"id": 21, "seek": 8332, "start": 102.67999999999999, "end": 108.91999999999999, "text": " honors math and physics degree from a given and university and I think we have", "tokens": [51332, 26884, 5221, 293, 10649, 4314, 490, 257, 2212, 293, 5454, 293, 286, 519, 321, 362, 51644], "temperature": 0.0, "avg_logprob": -0.2550923736007125, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0066835046745836735}, {"id": 22, "seek": 8332, "start": 108.91999999999999, "end": 112.03999999999999, "text": " some very interesting questions to discuss today I think as many of you know", "tokens": [51644, 512, 588, 1880, 1651, 281, 2248, 965, 286, 519, 382, 867, 295, 291, 458, 51800], "temperature": 0.0, "avg_logprob": -0.2550923736007125, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0066835046745836735}, {"id": 23, "seek": 11204, "start": 112.08000000000001, "end": 115.08000000000001, "text": " quantum computing operates in an exponential space so how is classical", "tokens": [50366, 13018, 15866, 22577, 294, 364, 21510, 1901, 370, 577, 307, 13735, 50516], "temperature": 0.0, "avg_logprob": -0.22207882006963095, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.011844108812510967}, {"id": 24, "seek": 11204, "start": 115.08000000000001, "end": 119.36000000000001, "text": " machine learning and learning classical distribution for instance which also", "tokens": [50516, 3479, 2539, 293, 2539, 13735, 7316, 337, 5197, 597, 611, 50730], "temperature": 0.0, "avg_logprob": -0.22207882006963095, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.011844108812510967}, {"id": 25, "seek": 11204, "start": 119.36000000000001, "end": 125.12, "text": " operates in an exponential space mesh how can we learn new and interesting", "tokens": [50730, 22577, 294, 364, 21510, 1901, 17407, 577, 393, 321, 1466, 777, 293, 1880, 51018], "temperature": 0.0, "avg_logprob": -0.22207882006963095, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.011844108812510967}, {"id": 26, "seek": 11204, "start": 125.12, "end": 128.76, "text": " correlations and work with not just pure states but non pure states and these", "tokens": [51018, 13983, 763, 293, 589, 365, 406, 445, 6075, 4368, 457, 2107, 6075, 4368, 293, 613, 51200], "temperature": 0.0, "avg_logprob": -0.22207882006963095, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.011844108812510967}, {"id": 27, "seek": 11204, "start": 128.76, "end": 133.28, "text": " are just some of the things Guillaume will tell us about today and maybe as I", "tokens": [51200, 366, 445, 512, 295, 264, 721, 2694, 5291, 2540, 486, 980, 505, 466, 965, 293, 1310, 382, 286, 51426], "temperature": 0.0, "avg_logprob": -0.22207882006963095, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.011844108812510967}, {"id": 28, "seek": 11204, "start": 133.28, "end": 137.88, "text": " advertise your talk and describe the format of Guillaume maybe you can pull", "tokens": [51426, 35379, 428, 751, 293, 6786, 264, 7877, 295, 2694, 5291, 2540, 1310, 291, 393, 2235, 51656], "temperature": 0.0, "avg_logprob": -0.22207882006963095, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.011844108812510967}, {"id": 29, "seek": 13788, "start": 137.88, "end": 144.64, "text": " up your slides yep the talk format is the usual you can ask questions in the", "tokens": [50364, 493, 428, 9788, 18633, 264, 751, 7877, 307, 264, 7713, 291, 393, 1029, 1651, 294, 264, 50702], "temperature": 0.0, "avg_logprob": -0.19664904150632348, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.030125705525279045}, {"id": 30, "seek": 13788, "start": 144.64, "end": 150.32, "text": " comment sidebar box on the right hand side usually or below and I will triage", "tokens": [50702, 2871, 1252, 5356, 2424, 322, 264, 558, 1011, 1252, 2673, 420, 2507, 293, 286, 486, 1376, 609, 50986], "temperature": 0.0, "avg_logprob": -0.19664904150632348, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.030125705525279045}, {"id": 31, "seek": 13788, "start": 150.32, "end": 157.6, "text": " those questions and ask Guillaume in real time so Q&A is during and after and I", "tokens": [50986, 729, 1651, 293, 1029, 2694, 5291, 2540, 294, 957, 565, 370, 1249, 5, 32, 307, 1830, 293, 934, 293, 286, 51350], "temperature": 0.0, "avg_logprob": -0.19664904150632348, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.030125705525279045}, {"id": 32, "seek": 13788, "start": 157.6, "end": 163.6, "text": " think it's time we get started so it's my pleasure to turn over to you well thank", "tokens": [51350, 519, 309, 311, 565, 321, 483, 1409, 370, 309, 311, 452, 6834, 281, 1261, 670, 281, 291, 731, 1309, 51650], "temperature": 0.0, "avg_logprob": -0.19664904150632348, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.030125705525279045}, {"id": 33, "seek": 13788, "start": 163.6, "end": 167.6, "text": " you for the introductions Lacko and thank you to the invitation and it's very", "tokens": [51650, 291, 337, 264, 48032, 441, 501, 78, 293, 1309, 291, 281, 264, 17890, 293, 309, 311, 588, 51850], "temperature": 0.0, "avg_logprob": -0.19664904150632348, "compression_ratio": 1.7056277056277056, "no_speech_prob": 0.030125705525279045}, {"id": 34, "seek": 16760, "start": 167.6, "end": 173.64, "text": " nice to get to speak to the whole quantum community here really enjoy", "tokens": [50364, 1481, 281, 483, 281, 1710, 281, 264, 1379, 13018, 1768, 510, 534, 2103, 50666], "temperature": 0.0, "avg_logprob": -0.1567426257663303, "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.004599401727318764}, {"id": 35, "seek": 16760, "start": 173.64, "end": 179.24, "text": " watching these seminars so it's my honor to get to talk in one of these so", "tokens": [50666, 1976, 613, 43112, 370, 309, 311, 452, 5968, 281, 483, 281, 751, 294, 472, 295, 613, 370, 50946], "temperature": 0.0, "avg_logprob": -0.1567426257663303, "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.004599401727318764}, {"id": 36, "seek": 16760, "start": 179.24, "end": 184.32, "text": " today I guess I'll be talking broadly about quantum machine learning and some", "tokens": [50946, 965, 286, 2041, 286, 603, 312, 1417, 19511, 466, 13018, 3479, 2539, 293, 512, 51200], "temperature": 0.0, "avg_logprob": -0.1567426257663303, "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.004599401727318764}, {"id": 37, "seek": 16760, "start": 184.32, "end": 188.72, "text": " context comparing it to classical machine learning and deep learning and then", "tokens": [51200, 4319, 15763, 309, 281, 13735, 3479, 2539, 293, 2452, 2539, 293, 550, 51420], "temperature": 0.0, "avg_logprob": -0.1567426257663303, "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.004599401727318764}, {"id": 38, "seek": 16760, "start": 188.72, "end": 196.76, "text": " getting into some recent work from various internships and during my PhD on", "tokens": [51420, 1242, 666, 512, 5162, 589, 490, 3683, 35712, 293, 1830, 452, 14476, 322, 51822], "temperature": 0.0, "avg_logprob": -0.1567426257663303, "compression_ratio": 1.7168949771689497, "no_speech_prob": 0.004599401727318764}, {"id": 39, "seek": 19676, "start": 197.32, "end": 201.84, "text": " taking inspiration from classical machine learning to create new types of", "tokens": [50392, 1940, 10249, 490, 13735, 3479, 2539, 281, 1884, 777, 3467, 295, 50618], "temperature": 0.0, "avg_logprob": -0.1692678966219463, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0018660894129425287}, {"id": 40, "seek": 19676, "start": 201.84, "end": 209.95999999999998, "text": " quantum models and algorithms so as Lacko mentioned often in quantum machine", "tokens": [50618, 13018, 5245, 293, 14642, 370, 382, 441, 501, 78, 2835, 2049, 294, 13018, 3479, 51024], "temperature": 0.0, "avg_logprob": -0.1692678966219463, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0018660894129425287}, {"id": 41, "seek": 19676, "start": 209.95999999999998, "end": 216.23999999999998, "text": " learning there is this conception that if we go to if we use a quantum", "tokens": [51024, 2539, 456, 307, 341, 30698, 300, 498, 321, 352, 281, 498, 321, 764, 257, 13018, 51338], "temperature": 0.0, "avg_logprob": -0.1692678966219463, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0018660894129425287}, {"id": 42, "seek": 19676, "start": 216.23999999999998, "end": 221.44, "text": " computer sense we're operating in a exponentially large space and and thus", "tokens": [51338, 3820, 2020, 321, 434, 7447, 294, 257, 37330, 2416, 1901, 293, 293, 8807, 51598], "temperature": 0.0, "avg_logprob": -0.1692678966219463, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0018660894129425287}, {"id": 43, "seek": 22144, "start": 221.72, "end": 227.12, "text": " we should get exponential amounts of power of machine learning power right", "tokens": [50378, 321, 820, 483, 21510, 11663, 295, 1347, 295, 3479, 2539, 1347, 558, 50648], "temperature": 0.0, "avg_logprob": -0.14112736427620665, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.011145351454615593}, {"id": 44, "seek": 22144, "start": 227.12, "end": 231.72, "text": " but that is that is somewhat of a misconception because for very long", "tokens": [50648, 457, 300, 307, 300, 307, 8344, 295, 257, 41350, 570, 337, 588, 938, 50878], "temperature": 0.0, "avg_logprob": -0.14112736427620665, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.011145351454615593}, {"id": 45, "seek": 22144, "start": 231.72, "end": 236.16, "text": " time classical computers and analog classical electronics have been able to", "tokens": [50878, 565, 13735, 10807, 293, 16660, 13735, 20611, 362, 668, 1075, 281, 51100], "temperature": 0.0, "avg_logprob": -0.14112736427620665, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.011145351454615593}, {"id": 46, "seek": 22144, "start": 236.16, "end": 242.28, "text": " do probabilistic computing right and as we know quantum theory is kind of an", "tokens": [51100, 360, 31959, 3142, 15866, 558, 293, 382, 321, 458, 13018, 5261, 307, 733, 295, 364, 51406], "temperature": 0.0, "avg_logprob": -0.14112736427620665, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.011145351454615593}, {"id": 47, "seek": 22144, "start": 242.28, "end": 247.84, "text": " extension or generalization of probability theory to include complex", "tokens": [51406, 10320, 420, 2674, 2144, 295, 8482, 5261, 281, 4090, 3997, 51684], "temperature": 0.0, "avg_logprob": -0.14112736427620665, "compression_ratio": 1.6866359447004609, "no_speech_prob": 0.011145351454615593}, {"id": 48, "seek": 24784, "start": 247.84, "end": 254.20000000000002, "text": " numbers known as wave functions right and probabilities are obtained from wave", "tokens": [50364, 3547, 2570, 382, 5772, 6828, 558, 293, 33783, 366, 14879, 490, 5772, 50682], "temperature": 0.0, "avg_logprob": -0.138596369896406, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.008058978244662285}, {"id": 49, "seek": 24784, "start": 254.20000000000002, "end": 259.08, "text": " functions by taking the amplitude squared or the absolute value squared of", "tokens": [50682, 6828, 538, 1940, 264, 27433, 8889, 420, 264, 8236, 2158, 8889, 295, 50926], "temperature": 0.0, "avg_logprob": -0.138596369896406, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.008058978244662285}, {"id": 50, "seek": 24784, "start": 259.08, "end": 263.12, "text": " various these complex numbers and from that we get the probability of various", "tokens": [50926, 3683, 613, 3997, 3547, 293, 490, 300, 321, 483, 264, 8482, 295, 3683, 51128], "temperature": 0.0, "avg_logprob": -0.138596369896406, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.008058978244662285}, {"id": 51, "seek": 24784, "start": 263.12, "end": 268.64, "text": " outcomes right that is known as the Born Rule right but you know on the", "tokens": [51128, 10070, 558, 300, 307, 2570, 382, 264, 29808, 27533, 558, 457, 291, 458, 322, 264, 51404], "temperature": 0.0, "avg_logprob": -0.138596369896406, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.008058978244662285}, {"id": 52, "seek": 24784, "start": 268.64, "end": 275.48, "text": " classical side we can have mixtures of zero and one right zero or one in a", "tokens": [51404, 13735, 1252, 321, 393, 362, 2752, 37610, 295, 4018, 293, 472, 558, 4018, 420, 472, 294, 257, 51746], "temperature": 0.0, "avg_logprob": -0.138596369896406, "compression_ratio": 1.8439024390243903, "no_speech_prob": 0.008058978244662285}, {"id": 53, "seek": 27548, "start": 275.52000000000004, "end": 279.72, "text": " different combination whereas on the quantum side we have super positions it's", "tokens": [50366, 819, 6562, 9735, 322, 264, 13018, 1252, 321, 362, 1687, 8432, 309, 311, 50576], "temperature": 0.0, "avg_logprob": -0.1155805355165063, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.004467356950044632}, {"id": 54, "seek": 27548, "start": 279.72, "end": 285.16, "text": " not zero and one it's not zero or one it's a super position with certain values", "tokens": [50576, 406, 4018, 293, 472, 309, 311, 406, 4018, 420, 472, 309, 311, 257, 1687, 2535, 365, 1629, 4190, 50848], "temperature": 0.0, "avg_logprob": -0.1155805355165063, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.004467356950044632}, {"id": 55, "seek": 27548, "start": 285.16, "end": 293.32, "text": " of complex numbers right so overall I guess the theme of the talk is to you", "tokens": [50848, 295, 3997, 3547, 558, 370, 4787, 286, 2041, 264, 6314, 295, 264, 751, 307, 281, 291, 51256], "temperature": 0.0, "avg_logprob": -0.1155805355165063, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.004467356950044632}, {"id": 56, "seek": 27548, "start": 293.32, "end": 298.04, "text": " know take inspiration from classical probability theory and take inspiration", "tokens": [51256, 458, 747, 10249, 490, 13735, 8482, 5261, 293, 747, 10249, 51492], "temperature": 0.0, "avg_logprob": -0.1155805355165063, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.004467356950044632}, {"id": 57, "seek": 27548, "start": 298.04, "end": 302.48, "text": " from a subset of machine learning called probabilistic machine learning to come", "tokens": [51492, 490, 257, 25993, 295, 3479, 2539, 1219, 31959, 3142, 3479, 2539, 281, 808, 51714], "temperature": 0.0, "avg_logprob": -0.1155805355165063, "compression_ratio": 1.8101851851851851, "no_speech_prob": 0.004467356950044632}, {"id": 58, "seek": 30248, "start": 302.48, "end": 307.44, "text": " up with new quantum models because the theories are very much analogous and can", "tokens": [50364, 493, 365, 777, 13018, 5245, 570, 264, 13667, 366, 588, 709, 16660, 563, 293, 393, 50612], "temperature": 0.0, "avg_logprob": -0.12375255253003992, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.018536867573857307}, {"id": 59, "seek": 30248, "start": 307.44, "end": 314.76, "text": " in fact be hybridized as we will see so if you have you know n-pro bits", "tokens": [50612, 294, 1186, 312, 13051, 1602, 382, 321, 486, 536, 370, 498, 291, 362, 291, 458, 297, 12, 4318, 9239, 50978], "temperature": 0.0, "avg_logprob": -0.12375255253003992, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.018536867573857307}, {"id": 60, "seek": 30248, "start": 314.76, "end": 320.44, "text": " probabilistic bits they also operate in a space that is exponential right you", "tokens": [50978, 31959, 3142, 9239, 436, 611, 9651, 294, 257, 1901, 300, 307, 21510, 558, 291, 51262], "temperature": 0.0, "avg_logprob": -0.12375255253003992, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.018536867573857307}, {"id": 61, "seek": 30248, "start": 320.44, "end": 324.64000000000004, "text": " have a probability distribution over the space of bit strings and you can have a", "tokens": [51262, 362, 257, 8482, 7316, 670, 264, 1901, 295, 857, 13985, 293, 291, 393, 362, 257, 51472], "temperature": 0.0, "avg_logprob": -0.12375255253003992, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.018536867573857307}, {"id": 62, "seek": 30248, "start": 324.64000000000004, "end": 330.28000000000003, "text": " mixture of two to the n possible bit strings right and there's many as we'll", "tokens": [51472, 9925, 295, 732, 281, 264, 297, 1944, 857, 13985, 558, 293, 456, 311, 867, 382, 321, 603, 51754], "temperature": 0.0, "avg_logprob": -0.12375255253003992, "compression_ratio": 1.7048458149779735, "no_speech_prob": 0.018536867573857307}, {"id": 63, "seek": 33028, "start": 330.32, "end": 335.96, "text": " see machine learning models that are made to represent such distributions or", "tokens": [50366, 536, 3479, 2539, 5245, 300, 366, 1027, 281, 2906, 1270, 37870, 420, 50648], "temperature": 0.0, "avg_logprob": -0.15494800749279203, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.006287926807999611}, {"id": 64, "seek": 33028, "start": 335.96, "end": 341.0, "text": " generate such distributions and on the quantum side you know for pure states", "tokens": [50648, 8460, 1270, 37870, 293, 322, 264, 13018, 1252, 291, 458, 337, 6075, 4368, 50900], "temperature": 0.0, "avg_logprob": -0.15494800749279203, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.006287926807999611}, {"id": 65, "seek": 33028, "start": 341.0, "end": 347.88, "text": " at least they're written as a wave function of the sort and have two to the", "tokens": [50900, 412, 1935, 436, 434, 3720, 382, 257, 5772, 2445, 295, 264, 1333, 293, 362, 732, 281, 264, 51244], "temperature": 0.0, "avg_logprob": -0.15494800749279203, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.006287926807999611}, {"id": 66, "seek": 33028, "start": 347.88, "end": 352.67999999999995, "text": " n complex numbers for n qubits right so there's a lot of similarities right", "tokens": [51244, 297, 3997, 3547, 337, 297, 421, 34010, 558, 370, 456, 311, 257, 688, 295, 24197, 558, 51484], "temperature": 0.0, "avg_logprob": -0.15494800749279203, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.006287926807999611}, {"id": 67, "seek": 33028, "start": 352.67999999999995, "end": 356.4, "text": " and there's a difference the complex number and the real number that's", "tokens": [51484, 293, 456, 311, 257, 2649, 264, 3997, 1230, 293, 264, 957, 1230, 300, 311, 51670], "temperature": 0.0, "avg_logprob": -0.15494800749279203, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.006287926807999611}, {"id": 68, "seek": 35640, "start": 356.44, "end": 362.52, "text": " important that gives in a sense quantum computing its power over classical", "tokens": [50366, 1021, 300, 2709, 294, 257, 2020, 13018, 15866, 1080, 1347, 670, 13735, 50670], "temperature": 0.0, "avg_logprob": -0.12964698059918128, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005058761220425367}, {"id": 69, "seek": 35640, "start": 362.52, "end": 368.71999999999997, "text": " algorithms but instead of you know having this constant competition I guess", "tokens": [50670, 14642, 457, 2602, 295, 291, 458, 1419, 341, 5754, 6211, 286, 2041, 50980], "temperature": 0.0, "avg_logprob": -0.12964698059918128, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005058761220425367}, {"id": 70, "seek": 35640, "start": 368.71999999999997, "end": 373.84, "text": " between classical computing and quantum computing you know the philosophy at", "tokens": [50980, 1296, 13735, 15866, 293, 13018, 15866, 291, 458, 264, 10675, 412, 51236], "temperature": 0.0, "avg_logprob": -0.12964698059918128, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005058761220425367}, {"id": 71, "seek": 35640, "start": 373.84, "end": 377.76, "text": " least of my research is to try to leverage as much classical computing as", "tokens": [51236, 1935, 295, 452, 2132, 307, 281, 853, 281, 13982, 382, 709, 13735, 15866, 382, 51432], "temperature": 0.0, "avg_logprob": -0.12964698059918128, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005058761220425367}, {"id": 72, "seek": 35640, "start": 377.76, "end": 383.35999999999996, "text": " possible and including probabilistic computing and hybridize it with quantum", "tokens": [51432, 1944, 293, 3009, 31959, 3142, 15866, 293, 13051, 1125, 309, 365, 13018, 51712], "temperature": 0.0, "avg_logprob": -0.12964698059918128, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005058761220425367}, {"id": 73, "seek": 38336, "start": 383.36, "end": 387.2, "text": " computation in a sense we want to leverage quantum computers for what", "tokens": [50364, 24903, 294, 257, 2020, 321, 528, 281, 13982, 13018, 10807, 337, 437, 50556], "temperature": 0.0, "avg_logprob": -0.15736252466837566, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.01769977994263172}, {"id": 74, "seek": 38336, "start": 387.2, "end": 391.6, "text": " they're very they're the best at right and we want to add this such that there's", "tokens": [50556, 436, 434, 588, 436, 434, 264, 1151, 412, 558, 293, 321, 528, 281, 909, 341, 1270, 300, 456, 311, 50776], "temperature": 0.0, "avg_logprob": -0.15736252466837566, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.01769977994263172}, {"id": 75, "seek": 38336, "start": 391.6, "end": 396.28000000000003, "text": " a value add by using quantum computers hybridized with classical computing so", "tokens": [50776, 257, 2158, 909, 538, 1228, 13018, 10807, 13051, 1602, 365, 13735, 15866, 370, 51010], "temperature": 0.0, "avg_logprob": -0.15736252466837566, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.01769977994263172}, {"id": 76, "seek": 38336, "start": 396.28000000000003, "end": 402.72, "text": " this is a philosophy and it's a research and you know there are various schools", "tokens": [51010, 341, 307, 257, 10675, 293, 309, 311, 257, 2132, 293, 291, 458, 456, 366, 3683, 4656, 51332], "temperature": 0.0, "avg_logprob": -0.15736252466837566, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.01769977994263172}, {"id": 77, "seek": 38336, "start": 402.72, "end": 408.24, "text": " of thought in quantum computing and this is the one I guess I'm vouching for so", "tokens": [51332, 295, 1194, 294, 13018, 15866, 293, 341, 307, 264, 472, 286, 2041, 286, 478, 31007, 278, 337, 370, 51608], "temperature": 0.0, "avg_logprob": -0.15736252466837566, "compression_ratio": 1.7798165137614679, "no_speech_prob": 0.01769977994263172}, {"id": 78, "seek": 40824, "start": 409.04, "end": 414.48, "text": " you know what what actually gives quantum computers their power right well you", "tokens": [50404, 291, 458, 437, 437, 767, 2709, 13018, 10807, 641, 1347, 558, 731, 291, 50676], "temperature": 0.0, "avg_logprob": -0.21371588649519954, "compression_ratio": 1.7544642857142858, "no_speech_prob": 0.0017541981069371104}, {"id": 79, "seek": 40824, "start": 414.48, "end": 420.44, "text": " know there's been various demonstrations that sampling from a unitary circuit that", "tokens": [50676, 458, 456, 311, 668, 3683, 34714, 300, 21179, 490, 257, 517, 4109, 9048, 300, 50974], "temperature": 0.0, "avg_logprob": -0.21371588649519954, "compression_ratio": 1.7544642857142858, "no_speech_prob": 0.0017541981069371104}, {"id": 80, "seek": 40824, "start": 420.44, "end": 425.6, "text": " is quite deep and has a large space-time volume a unitary quantum circuit is", "tokens": [50974, 307, 1596, 2452, 293, 575, 257, 2416, 1901, 12, 3766, 5523, 257, 517, 4109, 13018, 9048, 307, 51232], "temperature": 0.0, "avg_logprob": -0.21371588649519954, "compression_ratio": 1.7544642857142858, "no_speech_prob": 0.0017541981069371104}, {"id": 81, "seek": 40824, "start": 425.6, "end": 429.96000000000004, "text": " quite difficult for classical computers right one has to do Feynman paths or", "tokens": [51232, 1596, 2252, 337, 13735, 10807, 558, 472, 575, 281, 360, 46530, 77, 1601, 14518, 420, 51450], "temperature": 0.0, "avg_logprob": -0.21371588649519954, "compression_ratio": 1.7544642857142858, "no_speech_prob": 0.0017541981069371104}, {"id": 82, "seek": 40824, "start": 429.96000000000004, "end": 435.72, "text": " tensor networks and what not and you know the difficulty scales exponentially", "tokens": [51450, 40863, 9590, 293, 437, 406, 293, 291, 458, 264, 10360, 17408, 37330, 51738], "temperature": 0.0, "avg_logprob": -0.21371588649519954, "compression_ratio": 1.7544642857142858, "no_speech_prob": 0.0017541981069371104}, {"id": 83, "seek": 43572, "start": 436.20000000000005, "end": 442.20000000000005, "text": " asymptotically with the volume of space-time right so that that we know", "tokens": [50388, 35114, 310, 984, 365, 264, 5523, 295, 1901, 12, 3766, 558, 370, 300, 300, 321, 458, 50688], "temperature": 0.0, "avg_logprob": -0.17068079453480395, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0024721066001802683}, {"id": 84, "seek": 43572, "start": 442.20000000000005, "end": 447.96000000000004, "text": " that's the power of quantum computers is to sample from such circuits so how do", "tokens": [50688, 300, 311, 264, 1347, 295, 13018, 10807, 307, 281, 6889, 490, 1270, 26354, 370, 577, 360, 50976], "temperature": 0.0, "avg_logprob": -0.17068079453480395, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0024721066001802683}, {"id": 85, "seek": 43572, "start": 447.96000000000004, "end": 453.16, "text": " we incorporate this exact thing sampling from unitaries and integrate it with", "tokens": [50976, 321, 16091, 341, 1900, 551, 21179, 490, 517, 3981, 530, 293, 13365, 309, 365, 51236], "temperature": 0.0, "avg_logprob": -0.17068079453480395, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0024721066001802683}, {"id": 86, "seek": 43572, "start": 453.16, "end": 457.72, "text": " the capabilities of classical modern classical machine learning to obtain", "tokens": [51236, 264, 10862, 295, 13735, 4363, 13735, 3479, 2539, 281, 12701, 51464], "temperature": 0.0, "avg_logprob": -0.17068079453480395, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0024721066001802683}, {"id": 87, "seek": 43572, "start": 457.72, "end": 464.08000000000004, "text": " you know something more powerful than either either piece individually right", "tokens": [51464, 291, 458, 746, 544, 4005, 813, 2139, 2139, 2522, 16652, 558, 51782], "temperature": 0.0, "avg_logprob": -0.17068079453480395, "compression_ratio": 1.7194570135746607, "no_speech_prob": 0.0024721066001802683}, {"id": 88, "seek": 46408, "start": 464.12, "end": 467.08, "text": " so quantum computers are becoming more powerful to the point of being", "tokens": [50366, 370, 13018, 10807, 366, 5617, 544, 4005, 281, 264, 935, 295, 885, 50514], "temperature": 0.0, "avg_logprob": -0.14279792014132725, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.001753698568791151}, {"id": 89, "seek": 46408, "start": 467.08, "end": 472.76, "text": " unsimulatable you know I won't get into whether the boundary has been crossed", "tokens": [50514, 2693, 332, 425, 31415, 291, 458, 286, 1582, 380, 483, 666, 1968, 264, 12866, 575, 668, 14622, 50798], "temperature": 0.0, "avg_logprob": -0.14279792014132725, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.001753698568791151}, {"id": 90, "seek": 46408, "start": 472.76, "end": 478.52, "text": " or not that's a that's an interesting debate on its own but how do you know", "tokens": [50798, 420, 406, 300, 311, 257, 300, 311, 364, 1880, 7958, 322, 1080, 1065, 457, 577, 360, 291, 458, 51086], "temperature": 0.0, "avg_logprob": -0.14279792014132725, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.001753698568791151}, {"id": 91, "seek": 46408, "start": 478.52, "end": 482.4, "text": " how even if we have this power how do you actually leverage this power for", "tokens": [51086, 577, 754, 498, 321, 362, 341, 1347, 577, 360, 291, 767, 13982, 341, 1347, 337, 51280], "temperature": 0.0, "avg_logprob": -0.14279792014132725, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.001753698568791151}, {"id": 92, "seek": 46408, "start": 482.4, "end": 490.28, "text": " for something you know relevant that is is not just just a demonstration so in", "tokens": [51280, 337, 746, 291, 458, 7340, 300, 307, 307, 406, 445, 445, 257, 16520, 370, 294, 51674], "temperature": 0.0, "avg_logprob": -0.14279792014132725, "compression_ratio": 1.7453703703703705, "no_speech_prob": 0.001753698568791151}, {"id": 93, "seek": 49028, "start": 490.32, "end": 495.55999999999995, "text": " a sense the meta area of focus at least in the in the near term has been quantum", "tokens": [50366, 257, 2020, 264, 19616, 1859, 295, 1879, 412, 1935, 294, 264, 294, 264, 2651, 1433, 575, 668, 13018, 50628], "temperature": 0.0, "avg_logprob": -0.12411835319117497, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.013213052414357662}, {"id": 94, "seek": 49028, "start": 495.55999999999995, "end": 501.67999999999995, "text": " AI right and what is quantum AI I like to subdivide it into two subfields that", "tokens": [50628, 7318, 558, 293, 437, 307, 13018, 7318, 286, 411, 281, 45331, 482, 309, 666, 732, 1422, 7610, 82, 300, 50934], "temperature": 0.0, "avg_logprob": -0.12411835319117497, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.013213052414357662}, {"id": 95, "seek": 49028, "start": 501.67999999999995, "end": 506.64, "text": " are dominant for now there are other subfields that could be analogous to", "tokens": [50934, 366, 15657, 337, 586, 456, 366, 661, 1422, 7610, 82, 300, 727, 312, 16660, 563, 281, 51182], "temperature": 0.0, "avg_logprob": -0.12411835319117497, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.013213052414357662}, {"id": 96, "seek": 49028, "start": 506.64, "end": 511.52, "text": " the subfields of classical AI but for now it seems like the community is focused", "tokens": [51182, 264, 1422, 7610, 82, 295, 13735, 7318, 457, 337, 586, 309, 2544, 411, 264, 1768, 307, 5178, 51426], "temperature": 0.0, "avg_logprob": -0.12411835319117497, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.013213052414357662}, {"id": 97, "seek": 49028, "start": 511.52, "end": 517.52, "text": " on two broad categories and one is quantum enhanced optimization so that is", "tokens": [51426, 322, 732, 4152, 10479, 293, 472, 307, 13018, 21191, 19618, 370, 300, 307, 51726], "temperature": 0.0, "avg_logprob": -0.12411835319117497, "compression_ratio": 1.7889908256880733, "no_speech_prob": 0.013213052414357662}, {"id": 98, "seek": 51752, "start": 517.76, "end": 522.68, "text": " accelerating classical algorithms of optimization and search using quantum or", "tokens": [50376, 34391, 13735, 14642, 295, 19618, 293, 3164, 1228, 13018, 420, 50622], "temperature": 0.0, "avg_logprob": -0.11947039365768433, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.013014626689255238}, {"id": 99, "seek": 51752, "start": 522.68, "end": 530.0, "text": " quantum inspired dynamics and quantum deep learning and I have you know many", "tokens": [50622, 13018, 7547, 15679, 293, 13018, 2452, 2539, 293, 286, 362, 291, 458, 867, 50988], "temperature": 0.0, "avg_logprob": -0.11947039365768433, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.013014626689255238}, {"id": 100, "seek": 51752, "start": 530.0, "end": 533.64, "text": " people call these variational algorithms I'll justify my nomenclature in a", "tokens": [50988, 561, 818, 613, 3034, 1478, 14642, 286, 603, 20833, 452, 297, 4726, 3474, 1503, 294, 257, 51170], "temperature": 0.0, "avg_logprob": -0.11947039365768433, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.013014626689255238}, {"id": 101, "seek": 51752, "start": 533.64, "end": 539.56, "text": " second what I consider quantum deep learning is learning quantum representations", "tokens": [51170, 1150, 437, 286, 1949, 13018, 2452, 2539, 307, 2539, 13018, 33358, 51466], "temperature": 0.0, "avg_logprob": -0.11947039365768433, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.013014626689255238}, {"id": 102, "seek": 51752, "start": 539.56, "end": 544.48, "text": " of quantum or classical data so there's a lot to unpack there so we're going to", "tokens": [51466, 295, 13018, 420, 13735, 1412, 370, 456, 311, 257, 688, 281, 26699, 456, 370, 321, 434, 516, 281, 51712], "temperature": 0.0, "avg_logprob": -0.11947039365768433, "compression_ratio": 1.8055555555555556, "no_speech_prob": 0.013014626689255238}, {"id": 103, "seek": 54448, "start": 544.48, "end": 548.64, "text": " spend a few slides trying to unpack what it means to do to have a representation", "tokens": [50364, 3496, 257, 1326, 9788, 1382, 281, 26699, 437, 309, 1355, 281, 360, 281, 362, 257, 10290, 50572], "temperature": 0.0, "avg_logprob": -0.17740979979309854, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.006096324883401394}, {"id": 104, "seek": 54448, "start": 548.64, "end": 553.6800000000001, "text": " or quantum data this is gonna be the focus of my talk today quantum deep", "tokens": [50572, 420, 13018, 1412, 341, 307, 799, 312, 264, 1879, 295, 452, 751, 965, 13018, 2452, 50824], "temperature": 0.0, "avg_logprob": -0.17740979979309854, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.006096324883401394}, {"id": 105, "seek": 54448, "start": 553.6800000000001, "end": 559.96, "text": " learning learning a multi-layered quantum computation based representation of", "tokens": [50824, 2539, 2539, 257, 4825, 12, 8376, 4073, 13018, 24903, 2361, 10290, 295, 51138], "temperature": 0.0, "avg_logprob": -0.17740979979309854, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.006096324883401394}, {"id": 106, "seek": 54448, "start": 559.96, "end": 564.64, "text": " quantum or classical data distributions what is a computational representation", "tokens": [51138, 13018, 420, 13735, 1412, 37870, 437, 307, 257, 28270, 10290, 51372], "temperature": 0.0, "avg_logprob": -0.17740979979309854, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.006096324883401394}, {"id": 107, "seek": 54448, "start": 564.64, "end": 569.04, "text": " of data right or a deep multi-layered computational representation of data", "tokens": [51372, 295, 1412, 558, 420, 257, 2452, 4825, 12, 8376, 4073, 28270, 10290, 295, 1412, 51592], "temperature": 0.0, "avg_logprob": -0.17740979979309854, "compression_ratio": 2.026315789473684, "no_speech_prob": 0.006096324883401394}, {"id": 108, "seek": 56904, "start": 569.52, "end": 574.9599999999999, "text": " representations right let's let's go back to classical deep representation learning", "tokens": [50388, 33358, 558, 718, 311, 718, 311, 352, 646, 281, 13735, 2452, 10290, 2539, 50660], "temperature": 0.0, "avg_logprob": -0.15299061523086724, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.02885756641626358}, {"id": 109, "seek": 56904, "start": 574.9599999999999, "end": 580.92, "text": " theory aka deep learning and try to understand a bit of the context so so", "tokens": [50660, 5261, 28042, 2452, 2539, 293, 853, 281, 1223, 257, 857, 295, 264, 4319, 370, 370, 50958], "temperature": 0.0, "avg_logprob": -0.15299061523086724, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.02885756641626358}, {"id": 110, "seek": 56904, "start": 580.92, "end": 584.64, "text": " deep learning is subset of machine learning subset of AI subset of computer", "tokens": [50958, 2452, 2539, 307, 25993, 295, 3479, 2539, 25993, 295, 7318, 25993, 295, 3820, 51144], "temperature": 0.0, "avg_logprob": -0.15299061523086724, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.02885756641626358}, {"id": 111, "seek": 56904, "start": 584.64, "end": 592.0, "text": " science and which is of course a subset of science and I guess the gist of it is", "tokens": [51144, 3497, 293, 597, 307, 295, 1164, 257, 25993, 295, 3497, 293, 286, 2041, 264, 290, 468, 295, 309, 307, 51512], "temperature": 0.0, "avg_logprob": -0.15299061523086724, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.02885756641626358}, {"id": 112, "seek": 56904, "start": 592.0, "end": 598.48, "text": " that neural networks you know when they learn something they got to be able to", "tokens": [51512, 300, 18161, 9590, 291, 458, 562, 436, 1466, 746, 436, 658, 281, 312, 1075, 281, 51836], "temperature": 0.0, "avg_logprob": -0.15299061523086724, "compression_ratio": 1.8714285714285714, "no_speech_prob": 0.02885756641626358}, {"id": 113, "seek": 59848, "start": 598.64, "end": 602.72, "text": " in a sense recreate it you know Feynman said what I cannot create I do not", "tokens": [50372, 294, 257, 2020, 25833, 309, 291, 458, 46530, 77, 1601, 848, 437, 286, 2644, 1884, 286, 360, 406, 50576], "temperature": 0.0, "avg_logprob": -0.13682115596273672, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.004466682206839323}, {"id": 114, "seek": 59848, "start": 602.72, "end": 606.24, "text": " understand and your favorite deep neural network if you could ask if you could", "tokens": [50576, 1223, 293, 428, 2954, 2452, 18161, 3209, 498, 291, 727, 1029, 498, 291, 727, 50752], "temperature": 0.0, "avg_logprob": -0.13682115596273672, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.004466682206839323}, {"id": 115, "seek": 59848, "start": 606.24, "end": 611.2, "text": " ask it what it thinks it would probably say something similar like this quote", "tokens": [50752, 1029, 309, 437, 309, 7309, 309, 576, 1391, 584, 746, 2531, 411, 341, 6513, 51000], "temperature": 0.0, "avg_logprob": -0.13682115596273672, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.004466682206839323}, {"id": 116, "seek": 59848, "start": 611.2, "end": 618.32, "text": " what do we mean by recreate so here I'm gonna get more rigorous so usually you", "tokens": [51000, 437, 360, 321, 914, 538, 25833, 370, 510, 286, 478, 799, 483, 544, 29882, 370, 2673, 291, 51356], "temperature": 0.0, "avg_logprob": -0.13682115596273672, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.004466682206839323}, {"id": 117, "seek": 59848, "start": 618.32, "end": 624.08, "text": " have a data set which is set of points sampled from a true distribution p true", "tokens": [51356, 362, 257, 1412, 992, 597, 307, 992, 295, 2793, 3247, 15551, 490, 257, 2074, 7316, 280, 2074, 51644], "temperature": 0.0, "avg_logprob": -0.13682115596273672, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.004466682206839323}, {"id": 118, "seek": 62408, "start": 624.08, "end": 630.6800000000001, "text": " of x right and so you have a certain finite set of data points right you", "tokens": [50364, 295, 2031, 558, 293, 370, 291, 362, 257, 1629, 19362, 992, 295, 1412, 2793, 558, 291, 50694], "temperature": 0.0, "avg_logprob": -0.13565136665521665, "compression_ratio": 1.9289340101522843, "no_speech_prob": 0.0028441380709409714}, {"id": 119, "seek": 62408, "start": 630.6800000000001, "end": 635.12, "text": " don't have the full distribution you could query you're trying to learn a", "tokens": [50694, 500, 380, 362, 264, 1577, 7316, 291, 727, 14581, 291, 434, 1382, 281, 1466, 257, 50916], "temperature": 0.0, "avg_logprob": -0.13565136665521665, "compression_ratio": 1.9289340101522843, "no_speech_prob": 0.0028441380709409714}, {"id": 120, "seek": 62408, "start": 635.12, "end": 643.0, "text": " approximative model and you're trying to approximate this distribution over a", "tokens": [50916, 8542, 1166, 2316, 293, 291, 434, 1382, 281, 30874, 341, 7316, 670, 257, 51310], "temperature": 0.0, "avg_logprob": -0.13565136665521665, "compression_ratio": 1.9289340101522843, "no_speech_prob": 0.0028441380709409714}, {"id": 121, "seek": 62408, "start": 643.0, "end": 647.2800000000001, "text": " certain domain of interest that goes beyond the data set itself because you", "tokens": [51310, 1629, 9274, 295, 1179, 300, 1709, 4399, 264, 1412, 992, 2564, 570, 291, 51524], "temperature": 0.0, "avg_logprob": -0.13565136665521665, "compression_ratio": 1.9289340101522843, "no_speech_prob": 0.0028441380709409714}, {"id": 122, "seek": 62408, "start": 647.2800000000001, "end": 652.12, "text": " you already have the data points for that are in the data set but you're trying", "tokens": [51524, 291, 1217, 362, 264, 1412, 2793, 337, 300, 366, 294, 264, 1412, 992, 457, 291, 434, 1382, 51766], "temperature": 0.0, "avg_logprob": -0.13565136665521665, "compression_ratio": 1.9289340101522843, "no_speech_prob": 0.0028441380709409714}, {"id": 123, "seek": 65212, "start": 652.12, "end": 656.4, "text": " to extend it beyond the data set and you have a parametrized hypothesis class", "tokens": [50364, 281, 10101, 309, 4399, 264, 1412, 992, 293, 291, 362, 257, 6220, 302, 470, 11312, 17291, 1508, 50578], "temperature": 0.0, "avg_logprob": -0.13670903063834983, "compression_ratio": 1.8927038626609443, "no_speech_prob": 0.0073431022465229034}, {"id": 124, "seek": 65212, "start": 656.4, "end": 661.48, "text": " or in classical machine learning we call it a variational distribution a", "tokens": [50578, 420, 294, 13735, 3479, 2539, 321, 818, 309, 257, 3034, 1478, 7316, 257, 50832], "temperature": 0.0, "avg_logprob": -0.13670903063834983, "compression_ratio": 1.8927038626609443, "no_speech_prob": 0.0073431022465229034}, {"id": 125, "seek": 65212, "start": 661.48, "end": 665.64, "text": " variational classical probability distribution and five here would", "tokens": [50832, 3034, 1478, 13735, 8482, 7316, 293, 1732, 510, 576, 51040], "temperature": 0.0, "avg_logprob": -0.13670903063834983, "compression_ratio": 1.8927038626609443, "no_speech_prob": 0.0073431022465229034}, {"id": 126, "seek": 65212, "start": 665.64, "end": 670.5600000000001, "text": " represent a set of parameters right because usually these distributions are", "tokens": [51040, 2906, 257, 992, 295, 9834, 558, 570, 2673, 613, 37870, 366, 51286], "temperature": 0.0, "avg_logprob": -0.13670903063834983, "compression_ratio": 1.8927038626609443, "no_speech_prob": 0.0073431022465229034}, {"id": 127, "seek": 65212, "start": 670.5600000000001, "end": 674.52, "text": " parametrized using something called deep neural nets as we'll see in a second", "tokens": [51286, 6220, 302, 470, 11312, 1228, 746, 1219, 2452, 18161, 36170, 382, 321, 603, 536, 294, 257, 1150, 51484], "temperature": 0.0, "avg_logprob": -0.13670903063834983, "compression_ratio": 1.8927038626609443, "no_speech_prob": 0.0073431022465229034}, {"id": 128, "seek": 65212, "start": 674.52, "end": 679.0, "text": " but the goal is to approximate a true distribution with a variational", "tokens": [51484, 457, 264, 3387, 307, 281, 30874, 257, 2074, 7316, 365, 257, 3034, 1478, 51708], "temperature": 0.0, "avg_logprob": -0.13670903063834983, "compression_ratio": 1.8927038626609443, "no_speech_prob": 0.0073431022465229034}, {"id": 129, "seek": 67900, "start": 679.0, "end": 687.04, "text": " distribution and you know the idea is to minimize the discrepancy between the", "tokens": [50364, 7316, 293, 291, 458, 264, 1558, 307, 281, 17522, 264, 2983, 265, 6040, 1344, 1296, 264, 50766], "temperature": 0.0, "avg_logprob": -0.12404211177382359, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.0031720087863504887}, {"id": 130, "seek": 67900, "start": 687.04, "end": 693.16, "text": " true distribution and our our variational distribution over the data set and", "tokens": [50766, 2074, 7316, 293, 527, 527, 3034, 1478, 7316, 670, 264, 1412, 992, 293, 51072], "temperature": 0.0, "avg_logprob": -0.12404211177382359, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.0031720087863504887}, {"id": 131, "seek": 67900, "start": 693.16, "end": 697.36, "text": " hope that it extends beyond the data set right so that would be for generative", "tokens": [51072, 1454, 300, 309, 26448, 4399, 264, 1412, 992, 558, 370, 300, 576, 312, 337, 1337, 1166, 51282], "temperature": 0.0, "avg_logprob": -0.12404211177382359, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.0031720087863504887}, {"id": 132, "seek": 67900, "start": 697.36, "end": 703.12, "text": " modeling we're just trying to learn the raw distribution of all our data in what", "tokens": [51282, 15983, 321, 434, 445, 1382, 281, 1466, 264, 8936, 7316, 295, 439, 527, 1412, 294, 437, 51570], "temperature": 0.0, "avg_logprob": -0.12404211177382359, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.0031720087863504887}, {"id": 133, "seek": 67900, "start": 703.12, "end": 706.4, "text": " is called discriminative modeling which includes you know classifiers such as", "tokens": [51570, 307, 1219, 20828, 1166, 15983, 597, 5974, 291, 458, 1508, 23463, 1270, 382, 51734], "temperature": 0.0, "avg_logprob": -0.12404211177382359, "compression_ratio": 1.893719806763285, "no_speech_prob": 0.0031720087863504887}, {"id": 134, "seek": 70640, "start": 706.48, "end": 712.0799999999999, "text": " like labeling a picture of a cat or a dog or regression neural regression which", "tokens": [50368, 411, 40244, 257, 3036, 295, 257, 3857, 420, 257, 3000, 420, 24590, 18161, 24590, 597, 50648], "temperature": 0.0, "avg_logprob": -0.151784190009622, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.001264146063476801}, {"id": 135, "seek": 70640, "start": 712.0799999999999, "end": 719.84, "text": " is trying to get a scalar out of out of data so you know maybe a certain", "tokens": [50648, 307, 1382, 281, 483, 257, 39684, 484, 295, 484, 295, 1412, 370, 291, 458, 1310, 257, 1629, 51036], "temperature": 0.0, "avg_logprob": -0.151784190009622, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.001264146063476801}, {"id": 136, "seek": 70640, "start": 719.84, "end": 725.24, "text": " continuous value instead of a discrete label but in general we have pairs of", "tokens": [51036, 10957, 2158, 2602, 295, 257, 27706, 7645, 457, 294, 2674, 321, 362, 15494, 295, 51306], "temperature": 0.0, "avg_logprob": -0.151784190009622, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.001264146063476801}, {"id": 137, "seek": 70640, "start": 725.24, "end": 730.84, "text": " inputs and outputs right and discriminative learning is very similar it", "tokens": [51306, 15743, 293, 23930, 558, 293, 20828, 1166, 2539, 307, 588, 2531, 309, 51586], "temperature": 0.0, "avg_logprob": -0.151784190009622, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.001264146063476801}, {"id": 138, "seek": 70640, "start": 730.84, "end": 734.16, "text": " can be phrased in probabilistic language as we're trying to learn a", "tokens": [51586, 393, 312, 7636, 1937, 294, 31959, 3142, 2856, 382, 321, 434, 1382, 281, 1466, 257, 51752], "temperature": 0.0, "avg_logprob": -0.151784190009622, "compression_ratio": 1.7004608294930876, "no_speech_prob": 0.001264146063476801}, {"id": 139, "seek": 73416, "start": 734.1999999999999, "end": 739.3199999999999, "text": " conditional distribution right random variables can be correlated and they can", "tokens": [50366, 27708, 7316, 558, 4974, 9102, 393, 312, 38574, 293, 436, 393, 50622], "temperature": 0.0, "avg_logprob": -0.120149835363611, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.009555535390973091}, {"id": 140, "seek": 73416, "start": 739.3199999999999, "end": 745.28, "text": " have what are called conditional distributions hopefully my okay we", "tokens": [50622, 362, 437, 366, 1219, 27708, 37870, 4696, 452, 1392, 321, 50920], "temperature": 0.0, "avg_logprob": -0.120149835363611, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.009555535390973091}, {"id": 141, "seek": 73416, "start": 745.28, "end": 750.04, "text": " can see the last line here so the idea is that deterministic functions such as", "tokens": [50920, 393, 536, 264, 1036, 1622, 510, 370, 264, 1558, 307, 300, 15957, 3142, 6828, 1270, 382, 51158], "temperature": 0.0, "avg_logprob": -0.120149835363611, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.009555535390973091}, {"id": 142, "seek": 73416, "start": 750.04, "end": 754.48, "text": " most deep neural networks are actually you know they're a subset of this", "tokens": [51158, 881, 2452, 18161, 9590, 366, 767, 291, 458, 436, 434, 257, 25993, 295, 341, 51380], "temperature": 0.0, "avg_logprob": -0.120149835363611, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.009555535390973091}, {"id": 143, "seek": 73416, "start": 754.48, "end": 758.88, "text": " conditional distributions they're kind of delta functions if you're used to the", "tokens": [51380, 27708, 37870, 436, 434, 733, 295, 8289, 6828, 498, 291, 434, 1143, 281, 264, 51600], "temperature": 0.0, "avg_logprob": -0.120149835363611, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.009555535390973091}, {"id": 144, "seek": 75888, "start": 758.88, "end": 764.64, "text": " delta measure in function space if you integrate over it then you you get the", "tokens": [50364, 8289, 3481, 294, 2445, 1901, 498, 291, 13365, 670, 309, 550, 291, 291, 483, 264, 50652], "temperature": 0.0, "avg_logprob": -0.12220626924096084, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.004330930765718222}, {"id": 145, "seek": 75888, "start": 764.64, "end": 770.64, "text": " value y equals f of x right so it's just a very sharp distribution right so", "tokens": [50652, 2158, 288, 6915, 283, 295, 2031, 558, 370, 309, 311, 445, 257, 588, 8199, 7316, 558, 370, 50952], "temperature": 0.0, "avg_logprob": -0.12220626924096084, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.004330930765718222}, {"id": 146, "seek": 75888, "start": 770.64, "end": 777.2, "text": " most of classical machine learning or I guess the the popular parts of deep", "tokens": [50952, 881, 295, 13735, 3479, 2539, 420, 286, 2041, 264, 264, 3743, 3166, 295, 2452, 51280], "temperature": 0.0, "avg_logprob": -0.12220626924096084, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.004330930765718222}, {"id": 147, "seek": 75888, "start": 777.2, "end": 781.52, "text": " learning often deal with kind of deterministic point-wise functions", "tokens": [51280, 2539, 2049, 2028, 365, 733, 295, 15957, 3142, 935, 12, 3711, 6828, 51496], "temperature": 0.0, "avg_logprob": -0.12220626924096084, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.004330930765718222}, {"id": 148, "seek": 75888, "start": 781.52, "end": 785.92, "text": " whereas the the more general theory is actually based on probability and", "tokens": [51496, 9735, 264, 264, 544, 2674, 5261, 307, 767, 2361, 322, 8482, 293, 51716], "temperature": 0.0, "avg_logprob": -0.12220626924096084, "compression_ratio": 1.6517857142857142, "no_speech_prob": 0.004330930765718222}, {"id": 149, "seek": 78592, "start": 786.16, "end": 790.4, "text": " information theory right and that's kind of the roots of machine learning and", "tokens": [50376, 1589, 5261, 558, 293, 300, 311, 733, 295, 264, 10669, 295, 3479, 2539, 293, 50588], "temperature": 0.0, "avg_logprob": -0.15192847349205796, "compression_ratio": 1.749003984063745, "no_speech_prob": 0.003221394494175911}, {"id": 150, "seek": 78592, "start": 790.4, "end": 794.0799999999999, "text": " what we're trying to get back to with quantum because we are in the early days", "tokens": [50588, 437, 321, 434, 1382, 281, 483, 646, 281, 365, 13018, 570, 321, 366, 294, 264, 2440, 1708, 50772], "temperature": 0.0, "avg_logprob": -0.15192847349205796, "compression_ratio": 1.749003984063745, "no_speech_prob": 0.003221394494175911}, {"id": 151, "seek": 78592, "start": 794.8, "end": 798.3199999999999, "text": " where we must understand from first principles what we're doing instead of", "tokens": [50808, 689, 321, 1633, 1223, 490, 700, 9156, 437, 321, 434, 884, 2602, 295, 50984], "temperature": 0.0, "avg_logprob": -0.15192847349205796, "compression_ratio": 1.749003984063745, "no_speech_prob": 0.003221394494175911}, {"id": 152, "seek": 78592, "start": 798.3199999999999, "end": 803.04, "text": " just trying stuff and iterating on the engineering of different algorithms", "tokens": [50984, 445, 1382, 1507, 293, 17138, 990, 322, 264, 7043, 295, 819, 14642, 51220], "temperature": 0.0, "avg_logprob": -0.15192847349205796, "compression_ratio": 1.749003984063745, "no_speech_prob": 0.003221394494175911}, {"id": 153, "seek": 78592, "start": 803.68, "end": 806.56, "text": " you know willy-nilly we want to be guided in our research", "tokens": [51252, 291, 458, 486, 88, 12, 77, 6917, 321, 528, 281, 312, 19663, 294, 527, 2132, 51396], "temperature": 0.0, "avg_logprob": -0.15192847349205796, "compression_ratio": 1.749003984063745, "no_speech_prob": 0.003221394494175911}, {"id": 154, "seek": 78592, "start": 808.4799999999999, "end": 812.64, "text": " so you know deep learning are algorithms tied to identify patterns in data", "tokens": [51492, 370, 291, 458, 2452, 2539, 366, 14642, 9601, 281, 5876, 8294, 294, 1412, 51700], "temperature": 0.0, "avg_logprob": -0.15192847349205796, "compression_ratio": 1.749003984063745, "no_speech_prob": 0.003221394494175911}, {"id": 155, "seek": 81264, "start": 813.6, "end": 818.0, "text": " you use multi-layered parameterized computations to learn representations of data", "tokens": [50412, 291, 764, 4825, 12, 8376, 4073, 13075, 1602, 2807, 763, 281, 1466, 33358, 295, 1412, 50632], "temperature": 0.0, "avg_logprob": -0.09795578079994278, "compression_ratio": 2.004484304932735, "no_speech_prob": 0.001548485946841538}, {"id": 156, "seek": 81264, "start": 818.56, "end": 823.52, "text": " representations are multi you know deep representations are multi-step computations", "tokens": [50660, 33358, 366, 4825, 291, 458, 2452, 33358, 366, 4825, 12, 16792, 2807, 763, 50908], "temperature": 0.0, "avg_logprob": -0.09795578079994278, "compression_ratio": 2.004484304932735, "no_speech_prob": 0.001548485946841538}, {"id": 157, "seek": 81264, "start": 823.52, "end": 829.12, "text": " that either take you from your data space to a simplified space or from a simple space to your", "tokens": [50908, 300, 2139, 747, 291, 490, 428, 1412, 1901, 281, 257, 26335, 1901, 420, 490, 257, 2199, 1901, 281, 428, 51188], "temperature": 0.0, "avg_logprob": -0.09795578079994278, "compression_ratio": 2.004484304932735, "no_speech_prob": 0.001548485946841538}, {"id": 158, "seek": 81264, "start": 829.12, "end": 836.0, "text": " data space right so in the case of discriminative learning you're trying to take the input space", "tokens": [51188, 1412, 1901, 558, 370, 294, 264, 1389, 295, 20828, 1166, 2539, 291, 434, 1382, 281, 747, 264, 4846, 1901, 51532], "temperature": 0.0, "avg_logprob": -0.09795578079994278, "compression_ratio": 2.004484304932735, "no_speech_prob": 0.001548485946841538}, {"id": 159, "seek": 81264, "start": 836.0, "end": 841.36, "text": " say the pictures of cats and go to the label space you know is it cat or dog a single bit", "tokens": [51532, 584, 264, 5242, 295, 11111, 293, 352, 281, 264, 7645, 1901, 291, 458, 307, 309, 3857, 420, 3000, 257, 2167, 857, 51800], "temperature": 0.0, "avg_logprob": -0.09795578079994278, "compression_ratio": 2.004484304932735, "no_speech_prob": 0.001548485946841538}, {"id": 160, "seek": 84136, "start": 841.36, "end": 847.6800000000001, "text": " instead of many many pixels right in generative learning you're starting from a very simple set of", "tokens": [50364, 2602, 295, 867, 867, 18668, 558, 294, 1337, 1166, 2539, 291, 434, 2891, 490, 257, 588, 2199, 992, 295, 50680], "temperature": 0.0, "avg_logprob": -0.062391874402068385, "compression_ratio": 1.9492385786802031, "no_speech_prob": 0.0015243467641994357}, {"id": 161, "seek": 84136, "start": 848.4, "end": 853.92, "text": " randomness say a set of Gaussian samples or a set of random coin flips and trying to turn that", "tokens": [50716, 4974, 1287, 584, 257, 992, 295, 39148, 10938, 420, 257, 992, 295, 4974, 11464, 40249, 293, 1382, 281, 1261, 300, 50992], "temperature": 0.0, "avg_logprob": -0.062391874402068385, "compression_ratio": 1.9492385786802031, "no_speech_prob": 0.0015243467641994357}, {"id": 162, "seek": 84136, "start": 853.92, "end": 862.4, "text": " randomness into the randomness over say the the set of pictures of bedrooms right the possible", "tokens": [50992, 4974, 1287, 666, 264, 4974, 1287, 670, 584, 264, 264, 992, 295, 5242, 295, 39955, 558, 264, 1944, 51416], "temperature": 0.0, "avg_logprob": -0.062391874402068385, "compression_ratio": 1.9492385786802031, "no_speech_prob": 0.0015243467641994357}, {"id": 163, "seek": 84136, "start": 862.4, "end": 867.52, "text": " set of pictures of bedrooms and you're trying to sample new data points from from that data set", "tokens": [51416, 992, 295, 5242, 295, 39955, 293, 291, 434, 1382, 281, 6889, 777, 1412, 2793, 490, 490, 300, 1412, 992, 51672], "temperature": 0.0, "avg_logprob": -0.062391874402068385, "compression_ratio": 1.9492385786802031, "no_speech_prob": 0.0015243467641994357}, {"id": 164, "seek": 86752, "start": 868.24, "end": 875.76, "text": " in terms of math we say you know we're searching for a sub manifold of your your your space right", "tokens": [50400, 294, 2115, 295, 5221, 321, 584, 291, 458, 321, 434, 10808, 337, 257, 1422, 47138, 295, 428, 428, 428, 1901, 558, 50776], "temperature": 0.0, "avg_logprob": -0.1403667984939203, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0018383256392553449}, {"id": 165, "seek": 86752, "start": 875.76, "end": 882.64, "text": " and if it's all continuously parameterized it's technically a manifold again this is what I just", "tokens": [50776, 293, 498, 309, 311, 439, 15684, 13075, 1602, 309, 311, 12120, 257, 47138, 797, 341, 307, 437, 286, 445, 51120], "temperature": 0.0, "avg_logprob": -0.1403667984939203, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0018383256392553449}, {"id": 166, "seek": 86752, "start": 882.64, "end": 887.92, "text": " described you have some randomness a generative model would go to the some complicated space you", "tokens": [51120, 7619, 291, 362, 512, 4974, 1287, 257, 1337, 1166, 2316, 576, 352, 281, 264, 512, 6179, 1901, 291, 51384], "temperature": 0.0, "avg_logprob": -0.1403667984939203, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0018383256392553449}, {"id": 167, "seek": 86752, "start": 887.92, "end": 893.36, "text": " know machine learning folks and deep learning folks really love pictures quantum folks love", "tokens": [51384, 458, 3479, 2539, 4024, 293, 2452, 2539, 4024, 534, 959, 5242, 13018, 4024, 959, 51656], "temperature": 0.0, "avg_logprob": -0.1403667984939203, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.0018383256392553449}, {"id": 168, "seek": 89336, "start": 893.44, "end": 899.84, "text": " wave functions and nick states as we'll see but you know discriminative learning would go to a", "tokens": [50368, 5772, 6828, 293, 15416, 4368, 382, 321, 603, 536, 457, 291, 458, 20828, 1166, 2539, 576, 352, 281, 257, 50688], "temperature": 0.0, "avg_logprob": -0.11638584369566382, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00970597192645073}, {"id": 169, "seek": 89336, "start": 899.84, "end": 906.0, "text": " simple space and then once it's simplified it's easy to separate out the two class so again", "tokens": [50688, 2199, 1901, 293, 550, 1564, 309, 311, 26335, 309, 311, 1858, 281, 4994, 484, 264, 732, 1508, 370, 797, 50996], "temperature": 0.0, "avg_logprob": -0.11638584369566382, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00970597192645073}, {"id": 170, "seek": 89336, "start": 906.0, "end": 910.16, "text": " representations every time I see representations don't know freak out it's just parameterized", "tokens": [50996, 33358, 633, 565, 286, 536, 33358, 500, 380, 458, 21853, 484, 309, 311, 445, 13075, 1602, 51204], "temperature": 0.0, "avg_logprob": -0.11638584369566382, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00970597192645073}, {"id": 171, "seek": 89336, "start": 910.16, "end": 917.04, "text": " multi-step computation and deep is multi-layer and the building block is neural networks", "tokens": [51204, 4825, 12, 16792, 24903, 293, 2452, 307, 4825, 12, 8376, 260, 293, 264, 2390, 3461, 307, 18161, 9590, 51548], "temperature": 0.0, "avg_logprob": -0.11638584369566382, "compression_ratio": 1.669683257918552, "no_speech_prob": 0.00970597192645073}, {"id": 172, "seek": 91704, "start": 917.5999999999999, "end": 926.48, "text": " so I think I'm going to skip over this theory this is an example of a unsupervised learning", "tokens": [50392, 370, 286, 519, 286, 478, 516, 281, 10023, 670, 341, 5261, 341, 307, 364, 1365, 295, 257, 2693, 12879, 24420, 2539, 50836], "temperature": 0.0, "avg_logprob": -0.07948771248693051, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0018965591443702579}, {"id": 173, "seek": 91704, "start": 926.48, "end": 931.92, "text": " algorithm called a variational autoencoder it's a way to compress data so you go to a compressed", "tokens": [50836, 9284, 1219, 257, 3034, 1478, 8399, 22660, 19866, 309, 311, 257, 636, 281, 14778, 1412, 370, 291, 352, 281, 257, 30353, 51108], "temperature": 0.0, "avg_logprob": -0.07948771248693051, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0018965591443702579}, {"id": 174, "seek": 91704, "start": 931.92, "end": 937.76, "text": " space and by compressing you're going to be forced to decorrelate the data and get a very simple", "tokens": [51108, 1901, 293, 538, 14778, 278, 291, 434, 516, 281, 312, 7579, 281, 979, 284, 4419, 473, 264, 1412, 293, 483, 257, 588, 2199, 51400], "temperature": 0.0, "avg_logprob": -0.07948771248693051, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0018965591443702579}, {"id": 175, "seek": 91704, "start": 938.9599999999999, "end": 943.68, "text": " very simple randomness and you could fit that simple randomness say with Gaussians", "tokens": [51460, 588, 2199, 4974, 1287, 293, 291, 727, 3318, 300, 2199, 4974, 1287, 584, 365, 10384, 2023, 2567, 51696], "temperature": 0.0, "avg_logprob": -0.07948771248693051, "compression_ratio": 1.719626168224299, "no_speech_prob": 0.0018965591443702579}, {"id": 176, "seek": 94368, "start": 943.76, "end": 948.0, "text": " and then if you plug it through in a sense the reverse transformation you get your data set", "tokens": [50368, 293, 550, 498, 291, 5452, 309, 807, 294, 257, 2020, 264, 9943, 9887, 291, 483, 428, 1412, 992, 50580], "temperature": 0.0, "avg_logprob": -0.05377460901553814, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.0037065192591398954}, {"id": 177, "seek": 94368, "start": 948.0, "end": 952.7199999999999, "text": " again right and I want to show this because it's going to be very similar to our quantum approach", "tokens": [50580, 797, 558, 293, 286, 528, 281, 855, 341, 570, 309, 311, 516, 281, 312, 588, 2531, 281, 527, 13018, 3109, 50816], "temperature": 0.0, "avg_logprob": -0.05377460901553814, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.0037065192591398954}, {"id": 178, "seek": 94368, "start": 952.7199999999999, "end": 958.56, "text": " where you could go in reverse through a quantum classical transformation and then you have a", "tokens": [50816, 689, 291, 727, 352, 294, 9943, 807, 257, 13018, 13735, 9887, 293, 550, 291, 362, 257, 51108], "temperature": 0.0, "avg_logprob": -0.05377460901553814, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.0037065192591398954}, {"id": 179, "seek": 94368, "start": 958.56, "end": 964.16, "text": " very simple what is called a latent space a hidden space and then when you want to generate the", "tokens": [51108, 588, 2199, 437, 307, 1219, 257, 48994, 1901, 257, 7633, 1901, 293, 550, 562, 291, 528, 281, 8460, 264, 51388], "temperature": 0.0, "avg_logprob": -0.05377460901553814, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.0037065192591398954}, {"id": 180, "seek": 94368, "start": 964.16, "end": 971.28, "text": " data again you go from latent space to the visible space right and these are very cool because you", "tokens": [51388, 1412, 797, 291, 352, 490, 48994, 1901, 281, 264, 8974, 1901, 558, 293, 613, 366, 588, 1627, 570, 291, 51744], "temperature": 0.0, "avg_logprob": -0.05377460901553814, "compression_ratio": 1.9311740890688258, "no_speech_prob": 0.0037065192591398954}, {"id": 181, "seek": 97128, "start": 971.6, "end": 978.4, "text": " can in latent space if you just train the network to search for interesting features in general", "tokens": [50380, 393, 294, 48994, 1901, 498, 291, 445, 3847, 264, 3209, 281, 3164, 337, 1880, 4122, 294, 2674, 50720], "temperature": 0.0, "avg_logprob": -0.060288075038364954, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.005138085689395666}, {"id": 182, "seek": 97128, "start": 978.4, "end": 985.04, "text": " they can find features that and then you could do kind of logic in latent space so maybe there's", "tokens": [50720, 436, 393, 915, 4122, 300, 293, 550, 291, 727, 360, 733, 295, 9952, 294, 48994, 1901, 370, 1310, 456, 311, 51052], "temperature": 0.0, "avg_logprob": -0.060288075038364954, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.005138085689395666}, {"id": 183, "seek": 97128, "start": 985.04, "end": 990.16, "text": " a vector that corresponds to glasses to gender to age and so on and you could play around in", "tokens": [51052, 257, 8062, 300, 23249, 281, 10812, 281, 7898, 281, 3205, 293, 370, 322, 293, 291, 727, 862, 926, 294, 51308], "temperature": 0.0, "avg_logprob": -0.060288075038364954, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.005138085689395666}, {"id": 184, "seek": 97128, "start": 990.16, "end": 995.92, "text": " latent space and see what you generate on the other side so you know for quantum for example if", "tokens": [51308, 48994, 1901, 293, 536, 437, 291, 8460, 322, 264, 661, 1252, 370, 291, 458, 337, 13018, 337, 1365, 498, 51596], "temperature": 0.0, "avg_logprob": -0.060288075038364954, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.005138085689395666}, {"id": 185, "seek": 97128, "start": 995.92, "end": 1000.8, "text": " you have properties of materials you're trying to detect and you're trying to generate new materials", "tokens": [51596, 291, 362, 7221, 295, 5319, 291, 434, 1382, 281, 5531, 293, 291, 434, 1382, 281, 8460, 777, 5319, 51840], "temperature": 0.0, "avg_logprob": -0.060288075038364954, "compression_ratio": 1.9357429718875503, "no_speech_prob": 0.005138085689395666}, {"id": 186, "seek": 100080, "start": 1000.8, "end": 1006.24, "text": " or new materials with properties having a latent space that you've detected to play with it can be", "tokens": [50364, 420, 777, 5319, 365, 7221, 1419, 257, 48994, 1901, 300, 291, 600, 21896, 281, 862, 365, 309, 393, 312, 50636], "temperature": 0.0, "avg_logprob": -0.08590010404586793, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0004954386386089027}, {"id": 187, "seek": 100080, "start": 1006.24, "end": 1014.4799999999999, "text": " very useful and of course unsupervised learning itself is also useful in classical and sorry in", "tokens": [50636, 588, 4420, 293, 295, 1164, 2693, 12879, 24420, 2539, 2564, 307, 611, 4420, 294, 13735, 293, 2597, 294, 51048], "temperature": 0.0, "avg_logprob": -0.08590010404586793, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0004954386386089027}, {"id": 188, "seek": 100080, "start": 1014.4799999999999, "end": 1021.52, "text": " a discriminative learning because finding a compressed representation is already part of the", "tokens": [51048, 257, 20828, 1166, 2539, 570, 5006, 257, 30353, 10290, 307, 1217, 644, 295, 264, 51400], "temperature": 0.0, "avg_logprob": -0.08590010404586793, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0004954386386089027}, {"id": 189, "seek": 100080, "start": 1021.52, "end": 1028.56, "text": " job to to to separate out classes so imagine we we had you know three different classes and we", "tokens": [51400, 1691, 281, 281, 281, 4994, 484, 5359, 370, 3811, 321, 321, 632, 291, 458, 1045, 819, 5359, 293, 321, 51752], "temperature": 0.0, "avg_logprob": -0.08590010404586793, "compression_ratio": 1.7053571428571428, "no_speech_prob": 0.0004954386386089027}, {"id": 190, "seek": 102856, "start": 1028.56, "end": 1032.96, "text": " compressed it to some space that's two-dimensional and then we could go from a two-dimensional", "tokens": [50364, 30353, 309, 281, 512, 1901, 300, 311, 732, 12, 18759, 293, 550, 321, 727, 352, 490, 257, 732, 12, 18759, 50584], "temperature": 0.0, "avg_logprob": -0.06295463713732632, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0013247949536889791}, {"id": 191, "seek": 102856, "start": 1032.96, "end": 1039.28, "text": " space to three different class labels of which color of the blob it corresponds to so again so", "tokens": [50584, 1901, 281, 1045, 819, 1508, 16949, 295, 597, 2017, 295, 264, 46115, 309, 23249, 281, 370, 797, 370, 50900], "temperature": 0.0, "avg_logprob": -0.06295463713732632, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0013247949536889791}, {"id": 192, "seek": 102856, "start": 1039.28, "end": 1044.8799999999999, "text": " i'm going to be focused on unsupervised learning but a lot of this actually applies to supervised", "tokens": [50900, 741, 478, 516, 281, 312, 5178, 322, 2693, 12879, 24420, 2539, 457, 257, 688, 295, 341, 767, 13165, 281, 46533, 51180], "temperature": 0.0, "avg_logprob": -0.06295463713732632, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0013247949536889791}, {"id": 193, "seek": 102856, "start": 1044.8799999999999, "end": 1052.32, "text": " or discriminative learning so what consists of a good representation i won't go too much into this but", "tokens": [51180, 420, 20828, 1166, 2539, 370, 437, 14689, 295, 257, 665, 10290, 741, 1582, 380, 352, 886, 709, 666, 341, 457, 51552], "temperature": 0.0, "avg_logprob": -0.06295463713732632, "compression_ratio": 1.703056768558952, "no_speech_prob": 0.0013247949536889791}, {"id": 194, "seek": 105232, "start": 1052.8799999999999, "end": 1059.12, "text": " at a high level you want the representation capacity are you able to capture or you know", "tokens": [50392, 412, 257, 1090, 1496, 291, 528, 264, 10290, 6042, 366, 291, 1075, 281, 7983, 420, 291, 458, 50704], "temperature": 0.0, "avg_logprob": -0.08402323722839355, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0029806040693074465}, {"id": 195, "seek": 105232, "start": 1059.12, "end": 1067.4399999999998, "text": " reproduce the data set for some value of your parameters of your model is it trainable efficiently", "tokens": [50704, 29501, 264, 1412, 992, 337, 512, 2158, 295, 428, 9834, 295, 428, 2316, 307, 309, 3847, 712, 19621, 51120], "temperature": 0.0, "avg_logprob": -0.08402323722839355, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0029806040693074465}, {"id": 196, "seek": 105232, "start": 1067.4399999999998, "end": 1073.76, "text": " right if you have a neural network parameterizing your computational representation to go from", "tokens": [51120, 558, 498, 291, 362, 257, 18161, 3209, 13075, 3319, 428, 28270, 10290, 281, 352, 490, 51436], "temperature": 0.0, "avg_logprob": -0.08402323722839355, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0029806040693074465}, {"id": 197, "seek": 105232, "start": 1073.76, "end": 1079.6, "text": " complicated to simple space how easy is it to train it with algorithms that are not too closely", "tokens": [51436, 6179, 281, 2199, 1901, 577, 1858, 307, 309, 281, 3847, 309, 365, 14642, 300, 366, 406, 886, 8185, 51728], "temperature": 0.0, "avg_logprob": -0.08402323722839355, "compression_ratio": 1.758139534883721, "no_speech_prob": 0.0029806040693074465}, {"id": 198, "seek": 107960, "start": 1080.56, "end": 1085.04, "text": " inference tractability for feedforward neural networks that's very simple but there's other", "tokens": [50412, 38253, 24207, 2310, 337, 3154, 13305, 18161, 9590, 300, 311, 588, 2199, 457, 456, 311, 661, 50636], "temperature": 0.0, "avg_logprob": -0.06475777779856036, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.00555348489433527}, {"id": 199, "seek": 107960, "start": 1085.04, "end": 1090.32, "text": " types of models that just doing prediction the prediction step can be computationally costly", "tokens": [50636, 3467, 295, 5245, 300, 445, 884, 17630, 264, 17630, 1823, 393, 312, 24903, 379, 28328, 50900], "temperature": 0.0, "avg_logprob": -0.06475777779856036, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.00555348489433527}, {"id": 200, "seek": 107960, "start": 1090.32, "end": 1096.56, "text": " so that's something to keep in mind and of course that is the advantage of quantum computers is that", "tokens": [50900, 370, 300, 311, 746, 281, 1066, 294, 1575, 293, 295, 1164, 300, 307, 264, 5002, 295, 13018, 10807, 307, 300, 51212], "temperature": 0.0, "avg_logprob": -0.06475777779856036, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.00555348489433527}, {"id": 201, "seek": 107960, "start": 1097.28, "end": 1102.08, "text": " you know if we incorporate large unitary transformations into our models as we'll see", "tokens": [51248, 291, 458, 498, 321, 16091, 2416, 517, 4109, 34852, 666, 527, 5245, 382, 321, 603, 536, 51488], "temperature": 0.0, "avg_logprob": -0.06475777779856036, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.00555348489433527}, {"id": 202, "seek": 107960, "start": 1102.8, "end": 1109.04, "text": " theoretically there are unitary transformations that cannot be executed the prediction or sampling", "tokens": [51524, 29400, 456, 366, 517, 4109, 34852, 300, 2644, 312, 17577, 264, 17630, 420, 21179, 51836], "temperature": 0.0, "avg_logprob": -0.06475777779856036, "compression_ratio": 1.821705426356589, "no_speech_prob": 0.00555348489433527}, {"id": 203, "seek": 110904, "start": 1109.04, "end": 1114.48, "text": " step on a classical computer right so it's a very important part that's why i have this slide", "tokens": [50364, 1823, 322, 257, 13735, 3820, 558, 370, 309, 311, 257, 588, 1021, 644, 300, 311, 983, 741, 362, 341, 4137, 50636], "temperature": 0.0, "avg_logprob": -0.07711988687515259, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.003648278769105673}, {"id": 204, "seek": 110904, "start": 1115.92, "end": 1119.44, "text": " generalization power is is the core of machine learning generalization is", "tokens": [50708, 2674, 2144, 1347, 307, 307, 264, 4965, 295, 3479, 2539, 2674, 2144, 307, 50884], "temperature": 0.0, "avg_logprob": -0.07711988687515259, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.003648278769105673}, {"id": 205, "seek": 110904, "start": 1120.32, "end": 1125.6, "text": " you know if i fit within my data set will what i've learned extend outside the data set which", "tokens": [50928, 291, 458, 498, 741, 3318, 1951, 452, 1412, 992, 486, 437, 741, 600, 3264, 10101, 2380, 264, 1412, 992, 597, 51192], "temperature": 0.0, "avg_logprob": -0.07711988687515259, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.003648278769105673}, {"id": 206, "seek": 110904, "start": 1125.6, "end": 1130.8799999999999, "text": " is important because that is the difference between learning and optimization i sense there's a question", "tokens": [51192, 307, 1021, 570, 300, 307, 264, 2649, 1296, 2539, 293, 19618, 741, 2020, 456, 311, 257, 1168, 51456], "temperature": 0.0, "avg_logprob": -0.07711988687515259, "compression_ratio": 1.7345971563981042, "no_speech_prob": 0.003648278769105673}, {"id": 207, "seek": 113088, "start": 1131.7600000000002, "end": 1138.3200000000002, "text": " oh just i'm getting keen keen awareness um this is more of a curiosity question", "tokens": [50408, 1954, 445, 741, 478, 1242, 20297, 20297, 8888, 1105, 341, 307, 544, 295, 257, 18769, 1168, 50736], "temperature": 0.0, "avg_logprob": -0.1847245635055914, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015652667731046677}, {"id": 208, "seek": 113088, "start": 1139.2, "end": 1144.8000000000002, "text": " about the representation capacity it's uh it seems like a really powerful but what can we usually", "tokens": [50780, 466, 264, 10290, 6042, 309, 311, 2232, 309, 2544, 411, 257, 534, 4005, 457, 437, 393, 321, 2673, 51060], "temperature": 0.0, "avg_logprob": -0.1847245635055914, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015652667731046677}, {"id": 209, "seek": 113088, "start": 1145.3600000000001, "end": 1150.72, "text": " before say running numerical experiments and so forth you know how much can we say or really", "tokens": [51088, 949, 584, 2614, 29054, 12050, 293, 370, 5220, 291, 458, 577, 709, 393, 321, 584, 420, 534, 51356], "temperature": 0.0, "avg_logprob": -0.1847245635055914, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015652667731046677}, {"id": 210, "seek": 113088, "start": 1150.72, "end": 1155.3600000000001, "text": " peer into that for a particular model you've come up with you know what are the kind of tools and", "tokens": [51356, 15108, 666, 300, 337, 257, 1729, 2316, 291, 600, 808, 493, 365, 291, 458, 437, 366, 264, 733, 295, 3873, 293, 51588], "temperature": 0.0, "avg_logprob": -0.1847245635055914, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.015652667731046677}, {"id": 211, "seek": 115536, "start": 1156.0, "end": 1159.9199999999998, "text": " techniques and how far can they allow you to can we really say a lot about that", "tokens": [50396, 7512, 293, 577, 1400, 393, 436, 2089, 291, 281, 393, 321, 534, 584, 257, 688, 466, 300, 50592], "temperature": 0.0, "avg_logprob": -0.128830756670163, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.005818541161715984}, {"id": 212, "seek": 115536, "start": 1160.8, "end": 1168.0, "text": " about complexity of sampling unitaries uh yeah the representation capacity like what kind of", "tokens": [50636, 466, 14024, 295, 21179, 517, 3981, 530, 2232, 1338, 264, 10290, 6042, 411, 437, 733, 295, 50996], "temperature": 0.0, "avg_logprob": -0.128830756670163, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.005818541161715984}, {"id": 213, "seek": 115536, "start": 1168.0, "end": 1175.36, "text": " correlations and so forth you will be able to capture potentially this is more of a right and", "tokens": [50996, 13983, 763, 293, 370, 5220, 291, 486, 312, 1075, 281, 7983, 7263, 341, 307, 544, 295, 257, 558, 293, 51364], "temperature": 0.0, "avg_logprob": -0.128830756670163, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.005818541161715984}, {"id": 214, "seek": 115536, "start": 1175.36, "end": 1180.9599999999998, "text": " that is going to depend strongly on your your the way you parameterize your your transformation in a", "tokens": [51364, 300, 307, 516, 281, 5672, 10613, 322, 428, 428, 264, 636, 291, 13075, 1125, 428, 428, 9887, 294, 257, 51644], "temperature": 0.0, "avg_logprob": -0.128830756670163, "compression_ratio": 1.7149532710280373, "no_speech_prob": 0.005818541161715984}, {"id": 215, "seek": 118096, "start": 1180.96, "end": 1186.32, "text": " sense you're by having a parameterized model you have what is called a hypothesis class and", "tokens": [50364, 2020, 291, 434, 538, 1419, 257, 13075, 1602, 2316, 291, 362, 437, 307, 1219, 257, 17291, 1508, 293, 50632], "temperature": 0.0, "avg_logprob": -0.09708338344798369, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0019263846334069967}, {"id": 216, "seek": 118096, "start": 1186.32, "end": 1193.1200000000001, "text": " depending on on the various choices you've made you're going to kind of span a sub manifold of", "tokens": [50632, 5413, 322, 322, 264, 3683, 7994, 291, 600, 1027, 291, 434, 516, 281, 733, 295, 16174, 257, 1422, 47138, 295, 50972], "temperature": 0.0, "avg_logprob": -0.09708338344798369, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0019263846334069967}, {"id": 217, "seek": 118096, "start": 1193.1200000000001, "end": 1200.24, "text": " states and the idea is that you know what is very popular right now is called the hardware efficient", "tokens": [50972, 4368, 293, 264, 1558, 307, 300, 291, 458, 437, 307, 588, 3743, 558, 586, 307, 1219, 264, 8837, 7148, 51328], "temperature": 0.0, "avg_logprob": -0.09708338344798369, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0019263846334069967}, {"id": 218, "seek": 118096, "start": 1201.04, "end": 1208.08, "text": " onsots it looks very much like a random quantum circuit like this it's very tightly packed", "tokens": [51368, 18818, 1971, 309, 1542, 588, 709, 411, 257, 4974, 13018, 9048, 411, 341, 309, 311, 588, 21952, 13265, 51720], "temperature": 0.0, "avg_logprob": -0.09708338344798369, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0019263846334069967}, {"id": 219, "seek": 120808, "start": 1208.8, "end": 1215.1999999999998, "text": " and the idea is that if you look at in the space of possible quantum states it can represent right", "tokens": [50400, 293, 264, 1558, 307, 300, 498, 291, 574, 412, 294, 264, 1901, 295, 1944, 13018, 4368, 309, 393, 2906, 558, 50720], "temperature": 0.0, "avg_logprob": -0.0624396829720003, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.002550434088334441}, {"id": 220, "seek": 120808, "start": 1215.1999999999998, "end": 1220.8799999999999, "text": " if all of these transformations were parameterized random you know single and two qubit rotations", "tokens": [50720, 498, 439, 295, 613, 34852, 645, 13075, 1602, 4974, 291, 458, 2167, 293, 732, 421, 5260, 44796, 51004], "temperature": 0.0, "avg_logprob": -0.0624396829720003, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.002550434088334441}, {"id": 221, "seek": 120808, "start": 1220.8799999999999, "end": 1227.04, "text": " right then theoretically you know its complexity is growing larger and larger right and in a sense", "tokens": [51004, 558, 550, 29400, 291, 458, 1080, 14024, 307, 4194, 4833, 293, 4833, 558, 293, 294, 257, 2020, 51312], "temperature": 0.0, "avg_logprob": -0.0624396829720003, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.002550434088334441}, {"id": 222, "seek": 120808, "start": 1228.08, "end": 1234.56, "text": " any quantum state that has a complexity uh within that complexity radius you'll be able to reach it", "tokens": [51364, 604, 13018, 1785, 300, 575, 257, 14024, 2232, 1951, 300, 14024, 15845, 291, 603, 312, 1075, 281, 2524, 309, 51688], "temperature": 0.0, "avg_logprob": -0.0624396829720003, "compression_ratio": 1.7792792792792793, "no_speech_prob": 0.002550434088334441}, {"id": 223, "seek": 123456, "start": 1235.12, "end": 1240.3999999999999, "text": " but the problem is because you're spanning such a large space your training of your quantum neural", "tokens": [50392, 457, 264, 1154, 307, 570, 291, 434, 47626, 1270, 257, 2416, 1901, 428, 3097, 295, 428, 13018, 18161, 50656], "temperature": 0.0, "avg_logprob": -0.07128327435786182, "compression_ratio": 1.85546875, "no_speech_prob": 0.00648678420111537}, {"id": 224, "seek": 123456, "start": 1240.3999999999999, "end": 1245.6, "text": " network becomes harder and harder um because your hypothesis class is too large so you're", "tokens": [50656, 3209, 3643, 6081, 293, 6081, 1105, 570, 428, 17291, 1508, 307, 886, 2416, 370, 291, 434, 50916], "temperature": 0.0, "avg_logprob": -0.07128327435786182, "compression_ratio": 1.85546875, "no_speech_prob": 0.00648678420111537}, {"id": 225, "seek": 123456, "start": 1245.6, "end": 1252.1599999999999, "text": " searching over too too many possibilities and this is the result known as the the baron plateaus", "tokens": [50916, 10808, 670, 886, 886, 867, 12178, 293, 341, 307, 264, 1874, 2570, 382, 264, 264, 2159, 266, 5924, 8463, 51244], "temperature": 0.0, "avg_logprob": -0.07128327435786182, "compression_ratio": 1.85546875, "no_speech_prob": 0.00648678420111537}, {"id": 226, "seek": 123456, "start": 1252.1599999999999, "end": 1258.0, "text": " in the quantum neural network landscape or or the quantum version of no free lunch theorem where", "tokens": [51244, 294, 264, 13018, 18161, 3209, 9661, 420, 420, 264, 13018, 3037, 295, 572, 1737, 6349, 20904, 689, 51536], "temperature": 0.0, "avg_logprob": -0.07128327435786182, "compression_ratio": 1.85546875, "no_speech_prob": 0.00648678420111537}, {"id": 227, "seek": 123456, "start": 1258.0, "end": 1263.44, "text": " you can't have a one size fits all representation and that's that's where physicists come in", "tokens": [51536, 291, 393, 380, 362, 257, 472, 2744, 9001, 439, 10290, 293, 300, 311, 300, 311, 689, 48716, 808, 294, 51808], "temperature": 0.0, "avg_logprob": -0.07128327435786182, "compression_ratio": 1.85546875, "no_speech_prob": 0.00648678420111537}, {"id": 228, "seek": 126344, "start": 1263.44, "end": 1270.16, "text": " physicists need to have you know good prior knowledge of the domain of application they're", "tokens": [50364, 48716, 643, 281, 362, 291, 458, 665, 4059, 3601, 295, 264, 9274, 295, 3861, 436, 434, 50700], "temperature": 0.0, "avg_logprob": -0.11265538062578366, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0005191614618524909}, {"id": 229, "seek": 126344, "start": 1270.16, "end": 1275.28, "text": " trying to do quantum machine learning and to instruct their choice of representation and", "tokens": [50700, 1382, 281, 360, 13018, 3479, 2539, 293, 281, 7232, 641, 3922, 295, 10290, 293, 50956], "temperature": 0.0, "avg_logprob": -0.11265538062578366, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0005191614618524909}, {"id": 230, "seek": 126344, "start": 1275.28, "end": 1284.56, "text": " parameterization to aid in in the the tractability of training um that answers the question yeah so", "tokens": [50956, 13075, 2144, 281, 9418, 294, 294, 264, 264, 24207, 2310, 295, 3097, 1105, 300, 6338, 264, 1168, 1338, 370, 51420], "temperature": 0.0, "avg_logprob": -0.11265538062578366, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0005191614618524909}, {"id": 231, "seek": 126344, "start": 1285.28, "end": 1290.4, "text": " i like that free lunch theorem uh because i guess you know you could try to say well if i have some", "tokens": [51456, 741, 411, 300, 1737, 6349, 20904, 2232, 570, 741, 2041, 291, 458, 291, 727, 853, 281, 584, 731, 498, 741, 362, 512, 51712], "temperature": 0.0, "avg_logprob": -0.11265538062578366, "compression_ratio": 1.6919642857142858, "no_speech_prob": 0.0005191614618524909}, {"id": 232, "seek": 129040, "start": 1290.4, "end": 1294.96, "text": " sort of generators that i use for my model you know what is the reachability of states", "tokens": [50364, 1333, 295, 38662, 300, 741, 764, 337, 452, 2316, 291, 458, 437, 307, 264, 2524, 2310, 295, 4368, 50592], "temperature": 0.0, "avg_logprob": -0.13709211349487305, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.008059286512434483}, {"id": 233, "seek": 129040, "start": 1295.76, "end": 1299.92, "text": " right since the people ask what is the reachability but i think what you're emphasizing here is that", "tokens": [50632, 558, 1670, 264, 561, 1029, 437, 307, 264, 2524, 2310, 457, 741, 519, 437, 291, 434, 45550, 510, 307, 300, 50840], "temperature": 0.0, "avg_logprob": -0.13709211349487305, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.008059286512434483}, {"id": 234, "seek": 129040, "start": 1299.92, "end": 1306.4, "text": " reachability is maybe only a first step uh and maybe having too much reachability sometimes", "tokens": [50840, 2524, 2310, 307, 1310, 787, 257, 700, 1823, 2232, 293, 1310, 1419, 886, 709, 2524, 2310, 2171, 51164], "temperature": 0.0, "avg_logprob": -0.13709211349487305, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.008059286512434483}, {"id": 235, "seek": 129040, "start": 1306.4, "end": 1312.96, "text": " at the moment so there's a trade-off maybe between capacity and efficiency exactly exactly and that", "tokens": [51164, 412, 264, 1623, 370, 456, 311, 257, 4923, 12, 4506, 1310, 1296, 6042, 293, 10493, 2293, 2293, 293, 300, 51492], "temperature": 0.0, "avg_logprob": -0.13709211349487305, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.008059286512434483}, {"id": 236, "seek": 129040, "start": 1312.96, "end": 1319.1200000000001, "text": " that is the no free lunch theorem in a sense so that's lucky for us because uh you know at least", "tokens": [51492, 300, 307, 264, 572, 1737, 6349, 20904, 294, 257, 2020, 370, 300, 311, 6356, 337, 505, 570, 2232, 291, 458, 412, 1935, 51800], "temperature": 0.0, "avg_logprob": -0.13709211349487305, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.008059286512434483}, {"id": 237, "seek": 131912, "start": 1319.12, "end": 1324.2399999999998, "text": " for now it seems like physicists will be needed uh in the future when uh we're not going to be", "tokens": [50364, 337, 586, 309, 2544, 411, 48716, 486, 312, 2978, 2232, 294, 264, 2027, 562, 2232, 321, 434, 406, 516, 281, 312, 50620], "temperature": 0.0, "avg_logprob": -0.06257035255432129, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.007228122092783451}, {"id": 238, "seek": 131912, "start": 1324.2399999999998, "end": 1331.52, "text": " out of a job we still need to design architectures um at least for now so let's see let's see let's", "tokens": [50620, 484, 295, 257, 1691, 321, 920, 643, 281, 1715, 6331, 1303, 1105, 412, 1935, 337, 586, 370, 718, 311, 536, 718, 311, 536, 718, 311, 50984], "temperature": 0.0, "avg_logprob": -0.06257035255432129, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.007228122092783451}, {"id": 239, "seek": 131912, "start": 1331.52, "end": 1336.32, "text": " see where it goes but uh but that's right that's right so uh you know a lot of what i'm going to", "tokens": [50984, 536, 689, 309, 1709, 457, 2232, 457, 300, 311, 558, 300, 311, 558, 370, 2232, 291, 458, 257, 688, 295, 437, 741, 478, 516, 281, 51224], "temperature": 0.0, "avg_logprob": -0.06257035255432129, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.007228122092783451}, {"id": 240, "seek": 131912, "start": 1336.32, "end": 1343.28, "text": " present today is not necessarily uh architectures for specific domains it's it's more uh a general", "tokens": [51224, 1974, 965, 307, 406, 4725, 2232, 6331, 1303, 337, 2685, 25514, 309, 311, 309, 311, 544, 2232, 257, 2674, 51572], "temperature": 0.0, "avg_logprob": -0.06257035255432129, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.007228122092783451}, {"id": 241, "seek": 134328, "start": 1343.36, "end": 1349.76, "text": " framework uh based on quantum information theory of how to uh do quantum machine learning or or", "tokens": [50368, 8388, 2232, 2361, 322, 13018, 1589, 5261, 295, 577, 281, 2232, 360, 13018, 3479, 2539, 420, 420, 50688], "temperature": 0.0, "avg_logprob": -0.15443534641475468, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.01853596419095993}, {"id": 242, "seek": 134328, "start": 1349.76, "end": 1358.48, "text": " maybe a a very broad class of parametrizations of of models that are quantum yeah yeah i'm since", "tokens": [50688, 1310, 257, 257, 588, 4152, 1508, 295, 6220, 302, 24959, 763, 295, 295, 5245, 300, 366, 13018, 1338, 1338, 741, 478, 1670, 51124], "temperature": 0.0, "avg_logprob": -0.15443534641475468, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.01853596419095993}, {"id": 243, "seek": 134328, "start": 1358.48, "end": 1363.04, "text": " i've already interpreted you there's interesting this is kind of an unusual question but why don't", "tokens": [51124, 741, 600, 1217, 26749, 291, 456, 311, 1880, 341, 307, 733, 295, 364, 10901, 1168, 457, 983, 500, 380, 51352], "temperature": 0.0, "avg_logprob": -0.15443534641475468, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.01853596419095993}, {"id": 244, "seek": 134328, "start": 1363.04, "end": 1369.12, "text": " throw it out here anyhow from martin how much time do you take uh take it take you to learn all this", "tokens": [51352, 3507, 309, 484, 510, 44995, 490, 12396, 259, 577, 709, 565, 360, 291, 747, 2232, 747, 309, 747, 291, 281, 1466, 439, 341, 51656], "temperature": 0.0, "avg_logprob": -0.15443534641475468, "compression_ratio": 1.7268722466960353, "no_speech_prob": 0.01853596419095993}, {"id": 245, "seek": 136912, "start": 1369.52, "end": 1377.4399999999998, "text": " i guess um i mean so okay so i guess backstory uh once there was a conference of machine", "tokens": [50384, 741, 2041, 1105, 741, 914, 370, 1392, 370, 741, 2041, 36899, 2232, 1564, 456, 390, 257, 7586, 295, 3479, 50780], "temperature": 0.0, "avg_logprob": -0.0847004473894492, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.015416700392961502}, {"id": 246, "seek": 136912, "start": 1377.4399999999998, "end": 1382.56, "text": " learning on a monday on a friday night i decided to binge watch lectures at three times speed on", "tokens": [50780, 2539, 322, 257, 17606, 320, 322, 257, 431, 4708, 1818, 741, 3047, 281, 41487, 1159, 16564, 412, 1045, 1413, 3073, 322, 51036], "temperature": 0.0, "avg_logprob": -0.0847004473894492, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.015416700392961502}, {"id": 247, "seek": 136912, "start": 1382.56, "end": 1389.4399999999998, "text": " youtube on the basics of deep learning i think that was 2016 uh or 2017 something like that", "tokens": [51036, 12487, 322, 264, 14688, 295, 2452, 2539, 741, 519, 300, 390, 6549, 2232, 420, 6591, 746, 411, 300, 51380], "temperature": 0.0, "avg_logprob": -0.0847004473894492, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.015416700392961502}, {"id": 248, "seek": 136912, "start": 1390.1599999999999, "end": 1395.28, "text": " and uh since then i've just been reading uh machine learning papers and you know i i guess", "tokens": [51416, 293, 2232, 1670, 550, 741, 600, 445, 668, 3760, 2232, 3479, 2539, 10577, 293, 291, 458, 741, 741, 2041, 51672], "temperature": 0.0, "avg_logprob": -0.0847004473894492, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.015416700392961502}, {"id": 249, "seek": 139528, "start": 1395.36, "end": 1400.8, "text": " have a deep math background so it helps uh and then quantum computing itself uh i guess", "tokens": [50368, 362, 257, 2452, 5221, 3678, 370, 309, 3665, 2232, 293, 550, 13018, 15866, 2564, 2232, 741, 2041, 50640], "temperature": 0.0, "avg_logprob": -0.0826280911763509, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.017975404858589172}, {"id": 250, "seek": 139528, "start": 1400.8, "end": 1407.36, "text": " i've been doing since i was 19 and i'm 28 now so gives you an estimate uh it's just always been", "tokens": [50640, 741, 600, 668, 884, 1670, 741, 390, 1294, 293, 741, 478, 7562, 586, 370, 2709, 291, 364, 12539, 2232, 309, 311, 445, 1009, 668, 50968], "temperature": 0.0, "avg_logprob": -0.0826280911763509, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.017975404858589172}, {"id": 251, "seek": 139528, "start": 1407.36, "end": 1414.0, "text": " my passion and uh i uh i went through theoretical physics and uh now i'm here in uh quantum machine", "tokens": [50968, 452, 5418, 293, 2232, 741, 2232, 741, 1437, 807, 20864, 10649, 293, 2232, 586, 741, 478, 510, 294, 2232, 13018, 3479, 51300], "temperature": 0.0, "avg_logprob": -0.0826280911763509, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.017975404858589172}, {"id": 252, "seek": 139528, "start": 1414.0, "end": 1421.04, "text": " learning so i would say four years of of interest in quantum machine learning two to three years", "tokens": [51300, 2539, 370, 741, 576, 584, 1451, 924, 295, 295, 1179, 294, 13018, 3479, 2539, 732, 281, 1045, 924, 51652], "temperature": 0.0, "avg_logprob": -0.0826280911763509, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.017975404858589172}, {"id": 253, "seek": 142104, "start": 1421.04, "end": 1426.24, "text": " serious uh serious focus and it looks like george baron is building on my question which", "tokens": [50364, 3156, 2232, 3156, 1879, 293, 309, 1542, 411, 1519, 4685, 2159, 266, 307, 2390, 322, 452, 1168, 597, 50624], "temperature": 0.0, "avg_logprob": -0.12280642114034514, "compression_ratio": 1.7439613526570048, "no_speech_prob": 0.0035366418305784464}, {"id": 254, "seek": 142104, "start": 1426.24, "end": 1430.3999999999999, "text": " probably gets into a little bit i also wanted to get into which is what are some uh quantitative", "tokens": [50624, 1391, 2170, 666, 257, 707, 857, 741, 611, 1415, 281, 483, 666, 597, 307, 437, 366, 512, 2232, 27778, 50832], "temperature": 0.0, "avg_logprob": -0.12280642114034514, "compression_ratio": 1.7439613526570048, "no_speech_prob": 0.0035366418305784464}, {"id": 255, "seek": 142104, "start": 1431.28, "end": 1436.24, "text": " quantitative metrics for representation capacity yeah that's that's interesting um", "tokens": [50876, 27778, 16367, 337, 10290, 6042, 1338, 300, 311, 300, 311, 1880, 1105, 51124], "temperature": 0.0, "avg_logprob": -0.12280642114034514, "compression_ratio": 1.7439613526570048, "no_speech_prob": 0.0035366418305784464}, {"id": 256, "seek": 142104, "start": 1437.6, "end": 1444.48, "text": " i guess that's a good that's a good question i would say if you can quantify of in a sense a", "tokens": [51192, 741, 2041, 300, 311, 257, 665, 300, 311, 257, 665, 1168, 741, 576, 584, 498, 291, 393, 40421, 295, 294, 257, 2020, 257, 51536], "temperature": 0.0, "avg_logprob": -0.12280642114034514, "compression_ratio": 1.7439613526570048, "no_speech_prob": 0.0035366418305784464}, {"id": 257, "seek": 144448, "start": 1444.48, "end": 1451.1200000000001, "text": " notion of volume and complexity space um and this is actually you know we're edging on on", "tokens": [50364, 10710, 295, 5523, 293, 14024, 1901, 1105, 293, 341, 307, 767, 291, 458, 321, 434, 1257, 3249, 322, 322, 50696], "temperature": 0.0, "avg_logprob": -0.13566109172084875, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.14987429976463318}, {"id": 258, "seek": 144448, "start": 1452.08, "end": 1456.16, "text": " theoretical physics here because the notion of quantum complexity is interested in interesting", "tokens": [50744, 20864, 10649, 510, 570, 264, 10710, 295, 13018, 14024, 307, 3102, 294, 1880, 50948], "temperature": 0.0, "avg_logprob": -0.13566109172084875, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.14987429976463318}, {"id": 259, "seek": 144448, "start": 1456.16, "end": 1462.8, "text": " in in the theory of ads cft and um you know there's lennard suskin who does a lot of work in this uh", "tokens": [50948, 294, 294, 264, 5261, 295, 10342, 269, 844, 293, 1105, 291, 458, 456, 311, 287, 1857, 515, 3291, 5843, 567, 775, 257, 688, 295, 589, 294, 341, 2232, 51280], "temperature": 0.0, "avg_logprob": -0.13566109172084875, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.14987429976463318}, {"id": 260, "seek": 144448, "start": 1462.8, "end": 1469.76, "text": " in this space uh and uh yeah i mean that's a that's a that's an open question i think i have", "tokens": [51280, 294, 341, 1901, 2232, 293, 2232, 1338, 741, 914, 300, 311, 257, 300, 311, 257, 300, 311, 364, 1269, 1168, 741, 519, 741, 362, 51628], "temperature": 0.0, "avg_logprob": -0.13566109172084875, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.14987429976463318}, {"id": 261, "seek": 144448, "start": 1469.76, "end": 1473.68, "text": " some intuition uh as to what would be a good metric but that would be an interesting further", "tokens": [51628, 512, 24002, 2232, 382, 281, 437, 576, 312, 257, 665, 20678, 457, 300, 576, 312, 364, 1880, 3052, 51824], "temperature": 0.0, "avg_logprob": -0.13566109172084875, "compression_ratio": 1.891566265060241, "no_speech_prob": 0.14987429976463318}, {"id": 262, "seek": 147368, "start": 1473.68, "end": 1481.04, "text": " study you know yeah there's a good quote right on we point correct like with uh with logic we prove", "tokens": [50364, 2979, 291, 458, 1338, 456, 311, 257, 665, 6513, 558, 322, 321, 935, 3006, 411, 365, 2232, 365, 9952, 321, 7081, 50732], "temperature": 0.0, "avg_logprob": -0.19844201740465667, "compression_ratio": 1.8045454545454545, "no_speech_prob": 0.0008423372055403888}, {"id": 263, "seek": 147368, "start": 1481.04, "end": 1489.68, "text": " with with intuition we discover and that's right here's the guidance that's right cool cool um so i", "tokens": [50732, 365, 365, 24002, 321, 4411, 293, 300, 311, 558, 510, 311, 264, 10056, 300, 311, 558, 1627, 1627, 1105, 370, 741, 51164], "temperature": 0.0, "avg_logprob": -0.19844201740465667, "compression_ratio": 1.8045454545454545, "no_speech_prob": 0.0008423372055403888}, {"id": 264, "seek": 147368, "start": 1489.68, "end": 1496.64, "text": " guess i've i've i've gone through these uh this is just some text uh backing up what i've said um", "tokens": [51164, 2041, 741, 600, 741, 600, 741, 600, 2780, 807, 613, 2232, 341, 307, 445, 512, 2487, 2232, 19373, 493, 437, 741, 600, 848, 1105, 51512], "temperature": 0.0, "avg_logprob": -0.19844201740465667, "compression_ratio": 1.8045454545454545, "no_speech_prob": 0.0008423372055403888}, {"id": 265, "seek": 147368, "start": 1496.64, "end": 1502.5600000000002, "text": " so okay so now that we've we have some very brief background and some intuition about deep learning", "tokens": [51512, 370, 1392, 370, 586, 300, 321, 600, 321, 362, 512, 588, 5353, 3678, 293, 512, 24002, 466, 2452, 2539, 51808], "temperature": 0.0, "avg_logprob": -0.19844201740465667, "compression_ratio": 1.8045454545454545, "no_speech_prob": 0.0008423372055403888}, {"id": 266, "seek": 150256, "start": 1502.56, "end": 1508.56, "text": " because this is a quantum computing audience so we had to load that up um how can we use you know", "tokens": [50364, 570, 341, 307, 257, 13018, 15866, 4034, 370, 321, 632, 281, 3677, 300, 493, 1105, 577, 393, 321, 764, 291, 458, 50664], "temperature": 0.0, "avg_logprob": -0.08789945500237602, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.005298706237226725}, {"id": 267, "seek": 150256, "start": 1508.56, "end": 1515.04, "text": " what we learn taking inspiration from vaes or uh from you know the what is needed to have a good", "tokens": [50664, 437, 321, 1466, 1940, 10249, 490, 2773, 279, 420, 2232, 490, 291, 458, 264, 437, 307, 2978, 281, 362, 257, 665, 50988], "temperature": 0.0, "avg_logprob": -0.08789945500237602, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.005298706237226725}, {"id": 268, "seek": 150256, "start": 1515.04, "end": 1520.32, "text": " representation to instruct our choice of how we do quantum deep learning so first of all what would", "tokens": [50988, 10290, 281, 7232, 527, 3922, 295, 577, 321, 360, 13018, 2452, 2539, 370, 700, 295, 439, 437, 576, 51252], "temperature": 0.0, "avg_logprob": -0.08789945500237602, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.005298706237226725}, {"id": 269, "seek": 150256, "start": 1520.32, "end": 1525.9199999999998, "text": " be a quantum deep representation right well a classical feed for network in a cartoonish picture", "tokens": [51252, 312, 257, 13018, 2452, 10290, 558, 731, 257, 13735, 3154, 337, 2533, 1902, 294, 257, 18569, 742, 3036, 51532], "temperature": 0.0, "avg_logprob": -0.08789945500237602, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.005298706237226725}, {"id": 270, "seek": 150256, "start": 1525.9199999999998, "end": 1531.44, "text": " this is not the most general formulation but it's a it's a friendly one um you have some input you", "tokens": [51532, 341, 307, 406, 264, 881, 2674, 37642, 457, 309, 311, 257, 309, 311, 257, 9208, 472, 1105, 291, 362, 512, 4846, 291, 51808], "temperature": 0.0, "avg_logprob": -0.08789945500237602, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.005298706237226725}, {"id": 271, "seek": 153144, "start": 1531.44, "end": 1535.6000000000001, "text": " have some parameters phi and then you have some parameterized output f of x fine for a quantum", "tokens": [50364, 362, 512, 9834, 13107, 293, 550, 291, 362, 512, 13075, 1602, 5598, 283, 295, 2031, 2489, 337, 257, 13018, 50572], "temperature": 0.0, "avg_logprob": -0.09687646599703056, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0032723145559430122}, {"id": 272, "seek": 153144, "start": 1535.6000000000001, "end": 1540.72, "text": " neural network you have usually a pure state input a unitary evolution that is parameterized in some", "tokens": [50572, 18161, 3209, 291, 362, 2673, 257, 6075, 1785, 4846, 257, 517, 4109, 9303, 300, 307, 13075, 1602, 294, 512, 50828], "temperature": 0.0, "avg_logprob": -0.09687646599703056, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0032723145559430122}, {"id": 273, "seek": 153144, "start": 1540.72, "end": 1547.68, "text": " way and then you have a parameterized hypothesis class of pure states right which is u five times", "tokens": [50828, 636, 293, 550, 291, 362, 257, 13075, 1602, 17291, 1508, 295, 6075, 4368, 558, 597, 307, 344, 1732, 1413, 51176], "temperature": 0.0, "avg_logprob": -0.09687646599703056, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0032723145559430122}, {"id": 274, "seek": 153144, "start": 1547.68, "end": 1555.28, "text": " your initial state and uh you know we call the function f the feed forward operation you can have", "tokens": [51176, 428, 5883, 1785, 293, 2232, 291, 458, 321, 818, 264, 2445, 283, 264, 3154, 2128, 6916, 291, 393, 362, 51556], "temperature": 0.0, "avg_logprob": -0.09687646599703056, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0032723145559430122}, {"id": 275, "seek": 155528, "start": 1555.28, "end": 1561.68, "text": " a uh loss functionals returns a scalar that depends on say your label and your your output of your", "tokens": [50364, 257, 2232, 4470, 2445, 1124, 11247, 257, 39684, 300, 5946, 322, 584, 428, 7645, 293, 428, 428, 5598, 295, 428, 50684], "temperature": 0.0, "avg_logprob": -0.08602251070682133, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.027579180896282196}, {"id": 276, "seek": 155528, "start": 1561.68, "end": 1566.8, "text": " network uh this could be some statistical measure of statistical distance to your data set and you", "tokens": [50684, 3209, 2232, 341, 727, 312, 512, 22820, 3481, 295, 22820, 4560, 281, 428, 1412, 992, 293, 291, 50940], "temperature": 0.0, "avg_logprob": -0.08602251070682133, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.027579180896282196}, {"id": 277, "seek": 155528, "start": 1566.8, "end": 1572.08, "text": " want to find the uh minimum or approximate minimum subject to variations of the parameters of this", "tokens": [50940, 528, 281, 915, 264, 2232, 7285, 420, 30874, 7285, 3983, 281, 17840, 295, 264, 9834, 295, 341, 51204], "temperature": 0.0, "avg_logprob": -0.08602251070682133, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.027579180896282196}, {"id": 278, "seek": 155528, "start": 1572.08, "end": 1576.72, "text": " loss functional so how do we get scalar values out of a quantum computer it gives us a wave", "tokens": [51204, 4470, 11745, 370, 577, 360, 321, 483, 39684, 4190, 484, 295, 257, 13018, 3820, 309, 2709, 505, 257, 5772, 51436], "temperature": 0.0, "avg_logprob": -0.08602251070682133, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.027579180896282196}, {"id": 279, "seek": 155528, "start": 1576.72, "end": 1581.6, "text": " function so do we what do we do with it well we have to define a loss operator which is a quantum", "tokens": [51436, 2445, 370, 360, 321, 437, 360, 321, 360, 365, 309, 731, 321, 362, 281, 6964, 257, 4470, 12973, 597, 307, 257, 13018, 51680], "temperature": 0.0, "avg_logprob": -0.08602251070682133, "compression_ratio": 1.8764478764478765, "no_speech_prob": 0.027579180896282196}, {"id": 280, "seek": 158160, "start": 1581.6, "end": 1586.32, "text": " observable right or Hermitian observable and often we decompose it into small chunks that", "tokens": [50364, 9951, 712, 558, 420, 21842, 270, 952, 9951, 712, 293, 2049, 321, 22867, 541, 309, 666, 1359, 24004, 300, 50600], "temperature": 0.0, "avg_logprob": -0.095177288515022, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0043983468785882}, {"id": 281, "seek": 158160, "start": 1586.32, "end": 1594.7199999999998, "text": " we can measure independently um and combine later on and our goal similarly to you know in the v the", "tokens": [50600, 321, 393, 3481, 21761, 1105, 293, 10432, 1780, 322, 293, 527, 3387, 14138, 281, 291, 458, 294, 264, 371, 264, 51020], "temperature": 0.0, "avg_logprob": -0.095177288515022, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0043983468785882}, {"id": 282, "seek": 158160, "start": 1594.7199999999998, "end": 1601.4399999999998, "text": " case of vqe and many other variational algorithms is to find uh the minimum subject to variations", "tokens": [51020, 1389, 295, 371, 80, 68, 293, 867, 661, 3034, 1478, 14642, 307, 281, 915, 2232, 264, 7285, 3983, 281, 17840, 51356], "temperature": 0.0, "avg_logprob": -0.095177288515022, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0043983468785882}, {"id": 283, "seek": 158160, "start": 1601.4399999999998, "end": 1607.9199999999998, "text": " variational variations the parameters uh the expectation value of this loss operator right", "tokens": [51356, 3034, 1478, 17840, 264, 9834, 2232, 264, 14334, 2158, 295, 341, 4470, 12973, 558, 51680], "temperature": 0.0, "avg_logprob": -0.095177288515022, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0043983468785882}, {"id": 284, "seek": 160792, "start": 1607.92, "end": 1613.76, "text": " sometimes it's called the energy or the Hamiltonian but i like to generalize it to uh you know loss", "tokens": [50364, 2171, 309, 311, 1219, 264, 2281, 420, 264, 18484, 952, 457, 741, 411, 281, 2674, 1125, 309, 281, 2232, 291, 458, 4470, 50656], "temperature": 0.0, "avg_logprob": -0.06825565298398335, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.005729304160922766}, {"id": 285, "seek": 160792, "start": 1614.88, "end": 1620.16, "text": " for quantum machine learning so there's just a refresher this is the typical way when trains", "tokens": [50712, 337, 13018, 3479, 2539, 370, 456, 311, 445, 257, 17368, 511, 341, 307, 264, 7476, 636, 562, 16329, 50976], "temperature": 0.0, "avg_logprob": -0.06825565298398335, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.005729304160922766}, {"id": 286, "seek": 160792, "start": 1620.16, "end": 1624.8000000000002, "text": " a vanilla uh what i call vanilla quantum variational algorithms or vanilla quantum neural network", "tokens": [50976, 257, 17528, 2232, 437, 741, 818, 17528, 13018, 3034, 1478, 14642, 420, 17528, 13018, 18161, 3209, 51208], "temperature": 0.0, "avg_logprob": -0.06825565298398335, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.005729304160922766}, {"id": 287, "seek": 160792, "start": 1625.92, "end": 1631.76, "text": " you have a loop between a quantum and classical computer and the quantum computer gets an", "tokens": [51264, 291, 362, 257, 6367, 1296, 257, 13018, 293, 13735, 3820, 293, 264, 13018, 3820, 2170, 364, 51556], "temperature": 0.0, "avg_logprob": -0.06825565298398335, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.005729304160922766}, {"id": 288, "seek": 160792, "start": 1631.76, "end": 1635.8400000000001, "text": " expectation value feeds at the classical computer classical computer has an optimization", "tokens": [51556, 14334, 2158, 23712, 412, 264, 13735, 3820, 13735, 3820, 575, 364, 19618, 51760], "temperature": 0.0, "avg_logprob": -0.06825565298398335, "compression_ratio": 1.8611111111111112, "no_speech_prob": 0.005729304160922766}, {"id": 289, "seek": 163584, "start": 1635.84, "end": 1641.1999999999998, "text": " algorithm that's classical suggests and use values of the parameter and you iterate like this in a", "tokens": [50364, 9284, 300, 311, 13735, 13409, 293, 764, 4190, 295, 264, 13075, 293, 291, 44497, 411, 341, 294, 257, 50632], "temperature": 0.0, "avg_logprob": -0.06685579413234598, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.003706053365021944}, {"id": 290, "seek": 163584, "start": 1641.1999999999998, "end": 1646.56, "text": " sense you know our current quantum computers are restricted in how much quantum depth and uh", "tokens": [50632, 2020, 291, 458, 527, 2190, 13018, 10807, 366, 20608, 294, 577, 709, 13018, 7161, 293, 2232, 50900], "temperature": 0.0, "avg_logprob": -0.06685579413234598, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.003706053365021944}, {"id": 291, "seek": 163584, "start": 1647.1999999999998, "end": 1650.8799999999999, "text": " you know what kind of quantum states they can represent they're restricted in depth because", "tokens": [50932, 291, 458, 437, 733, 295, 13018, 4368, 436, 393, 2906, 436, 434, 20608, 294, 7161, 570, 51116], "temperature": 0.0, "avg_logprob": -0.06685579413234598, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.003706053365021944}, {"id": 292, "seek": 163584, "start": 1650.8799999999999, "end": 1656.72, "text": " of the noise and so it makes sense that we search over the space uh given this constraint of low", "tokens": [51116, 295, 264, 5658, 293, 370, 309, 1669, 2020, 300, 321, 3164, 670, 264, 1901, 2232, 2212, 341, 25534, 295, 2295, 51408], "temperature": 0.0, "avg_logprob": -0.06685579413234598, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.003706053365021944}, {"id": 293, "seek": 163584, "start": 1656.72, "end": 1662.9599999999998, "text": " depth circuits we should search over the space to find the quantum state and this is this is why", "tokens": [51408, 7161, 26354, 321, 820, 3164, 670, 264, 1901, 281, 915, 264, 13018, 1785, 293, 341, 307, 341, 307, 983, 51720], "temperature": 0.0, "avg_logprob": -0.06685579413234598, "compression_ratio": 1.9156626506024097, "no_speech_prob": 0.003706053365021944}, {"id": 294, "seek": 166296, "start": 1662.96, "end": 1668.88, "text": " this is kind of taking over because for the nisk era or you know early fault tolerance we're going", "tokens": [50364, 341, 307, 733, 295, 1940, 670, 570, 337, 264, 297, 7797, 4249, 420, 291, 458, 2440, 7441, 23368, 321, 434, 516, 50660], "temperature": 0.0, "avg_logprob": -0.08593905075736667, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.004904311615973711}, {"id": 295, "seek": 166296, "start": 1668.88, "end": 1674.96, "text": " to be searching over the space of states that are uh not too big not not too you know long to", "tokens": [50660, 281, 312, 10808, 670, 264, 1901, 295, 4368, 300, 366, 2232, 406, 886, 955, 406, 406, 886, 291, 458, 938, 281, 50964], "temperature": 0.0, "avg_logprob": -0.08593905075736667, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.004904311615973711}, {"id": 296, "seek": 166296, "start": 1674.96, "end": 1680.4, "text": " quantum compute um so why learn quantum representations in the first place if you if you allow a", "tokens": [50964, 13018, 14722, 1105, 370, 983, 1466, 13018, 33358, 294, 264, 700, 1081, 498, 291, 498, 291, 2089, 257, 51236], "temperature": 0.0, "avg_logprob": -0.08593905075736667, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.004904311615973711}, {"id": 297, "seek": 166296, "start": 1680.4, "end": 1685.68, "text": " meal modify uh Feynman's famous quote uh Feynman said you know nature is in classical gamut if you", "tokens": [51236, 6791, 16927, 2232, 46530, 77, 1601, 311, 4618, 6513, 2232, 46530, 77, 1601, 848, 291, 458, 3687, 307, 294, 13735, 8019, 325, 498, 291, 51500], "temperature": 0.0, "avg_logprob": -0.08593905075736667, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.004904311615973711}, {"id": 298, "seek": 166296, "start": 1685.68, "end": 1690.24, "text": " want to make a simulation of nature you better make it quantum mechanical and in our case it's", "tokens": [51500, 528, 281, 652, 257, 16575, 295, 3687, 291, 1101, 652, 309, 13018, 12070, 293, 294, 527, 1389, 309, 311, 51728], "temperature": 0.0, "avg_logprob": -0.08593905075736667, "compression_ratio": 1.8089887640449438, "no_speech_prob": 0.004904311615973711}, {"id": 299, "seek": 169024, "start": 1690.24, "end": 1696.0, "text": " if you want to learn a representation of nature you better make it quantum mechanical right so", "tokens": [50364, 498, 291, 528, 281, 1466, 257, 10290, 295, 3687, 291, 1101, 652, 309, 13018, 12070, 558, 370, 50652], "temperature": 0.0, "avg_logprob": -0.051469297692327214, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.002115249400958419}, {"id": 300, "seek": 169024, "start": 1696.8, "end": 1701.44, "text": " quantum states and quantum processes i think we've been we've been mentioning this can exhibit", "tokens": [50692, 13018, 4368, 293, 13018, 7555, 741, 519, 321, 600, 668, 321, 600, 668, 18315, 341, 393, 20487, 50924], "temperature": 0.0, "avg_logprob": -0.051469297692327214, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.002115249400958419}, {"id": 301, "seek": 169024, "start": 1701.44, "end": 1707.04, "text": " high levels of quantum forms of correlation such as entanglement and that's exponentially hard to", "tokens": [50924, 1090, 4358, 295, 13018, 6422, 295, 20009, 1270, 382, 948, 656, 3054, 293, 300, 311, 37330, 1152, 281, 51204], "temperature": 0.0, "avg_logprob": -0.051469297692327214, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.002115249400958419}, {"id": 302, "seek": 169024, "start": 1707.04, "end": 1712.64, "text": " represent in classical memory right if you have a uh random circuit producing in a highly entangled", "tokens": [51204, 2906, 294, 13735, 4675, 558, 498, 291, 362, 257, 2232, 4974, 9048, 10501, 294, 257, 5405, 948, 39101, 51484], "temperature": 0.0, "avg_logprob": -0.051469297692327214, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.002115249400958419}, {"id": 303, "seek": 169024, "start": 1712.64, "end": 1718.24, "text": " state it's very hard to approximate it and it's hard to prove uh theoretically without a doubt", "tokens": [51484, 1785, 309, 311, 588, 1152, 281, 30874, 309, 293, 309, 311, 1152, 281, 7081, 2232, 29400, 1553, 257, 6385, 51764], "temperature": 0.0, "avg_logprob": -0.051469297692327214, "compression_ratio": 1.832699619771863, "no_speech_prob": 0.002115249400958419}, {"id": 304, "seek": 171824, "start": 1718.24, "end": 1723.76, "text": " but every algorithm we've tried to simulate quantum circuits it seems to uh fall flat on its face at", "tokens": [50364, 457, 633, 9284, 321, 600, 3031, 281, 27817, 13018, 26354, 309, 2544, 281, 2232, 2100, 4962, 322, 1080, 1851, 412, 50640], "temperature": 0.0, "avg_logprob": -0.1786255051818075, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.002756752073764801}, {"id": 305, "seek": 171824, "start": 1723.76, "end": 1732.96, "text": " some point right um yes um quick question clarification from joe here is the loss calculated", "tokens": [50640, 512, 935, 558, 1105, 2086, 1105, 1702, 1168, 34449, 490, 1488, 68, 510, 307, 264, 4470, 15598, 51100], "temperature": 0.0, "avg_logprob": -0.1786255051818075, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.002756752073764801}, {"id": 306, "seek": 171824, "start": 1732.96, "end": 1740.64, "text": " by classical computer is the loss calculated so he's talking about the expectation value of the", "tokens": [51100, 538, 13735, 3820, 307, 264, 4470, 15598, 370, 415, 311, 1417, 466, 264, 14334, 2158, 295, 264, 51484], "temperature": 0.0, "avg_logprob": -0.1786255051818075, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.002756752073764801}, {"id": 307, "seek": 171824, "start": 1740.64, "end": 1746.48, "text": " L operator right right it takes actually several samples to estimate the expectation value right", "tokens": [51484, 441, 12973, 558, 558, 309, 2516, 767, 2940, 10938, 281, 12539, 264, 14334, 2158, 558, 51776], "temperature": 0.0, "avg_logprob": -0.1786255051818075, "compression_ratio": 1.7545454545454546, "no_speech_prob": 0.002756752073764801}, {"id": 308, "seek": 174648, "start": 1746.48, "end": 1751.92, "text": " you could think of it as like sampling from a distribution if i'm trying to do an estimator", "tokens": [50364, 291, 727, 519, 295, 309, 382, 411, 21179, 490, 257, 7316, 498, 741, 478, 1382, 281, 360, 364, 8017, 1639, 50636], "temperature": 0.0, "avg_logprob": -0.03209728479385376, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.0030743591487407684}, {"id": 309, "seek": 174648, "start": 1751.92, "end": 1759.28, "text": " of a uh random variable subject to samples from a certain distribution it takes several samples", "tokens": [50636, 295, 257, 2232, 4974, 7006, 3983, 281, 10938, 490, 257, 1629, 7316, 309, 2516, 2940, 10938, 51004], "temperature": 0.0, "avg_logprob": -0.03209728479385376, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.0030743591487407684}, {"id": 310, "seek": 174648, "start": 1759.28, "end": 1764.72, "text": " so there's there's like a mini loop in here to estimate the expectation value here and that's a", "tokens": [51004, 370, 456, 311, 456, 311, 411, 257, 8382, 6367, 294, 510, 281, 12539, 264, 14334, 2158, 510, 293, 300, 311, 257, 51276], "temperature": 0.0, "avg_logprob": -0.03209728479385376, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.0030743591487407684}, {"id": 311, "seek": 174648, "start": 1764.72, "end": 1770.0, "text": " mixture of uh you know doing several measurements on the quantum computer and saving saving the", "tokens": [51276, 9925, 295, 2232, 291, 458, 884, 2940, 15383, 322, 264, 13018, 3820, 293, 6816, 6816, 264, 51540], "temperature": 0.0, "avg_logprob": -0.03209728479385376, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.0030743591487407684}, {"id": 312, "seek": 174648, "start": 1770.0, "end": 1774.56, "text": " results on the classical computer and then the classical computer can aggregate the the various", "tokens": [51540, 3542, 322, 264, 13735, 3820, 293, 550, 264, 13735, 3820, 393, 26118, 264, 264, 3683, 51768], "temperature": 0.0, "avg_logprob": -0.03209728479385376, "compression_ratio": 1.9153225806451613, "no_speech_prob": 0.0030743591487407684}, {"id": 313, "seek": 177456, "start": 1774.6399999999999, "end": 1779.28, "text": " results to get an average right and maybe to paraphrase you mean you you run that same use", "tokens": [50368, 3542, 281, 483, 364, 4274, 558, 293, 1310, 281, 36992, 1703, 651, 291, 914, 291, 291, 1190, 300, 912, 764, 50600], "temperature": 0.0, "avg_logprob": -0.08359717396856511, "compression_ratio": 1.9243697478991597, "no_speech_prob": 0.0011334181763231754}, {"id": 314, "seek": 177456, "start": 1779.28, "end": 1783.36, "text": " circuit with the same inputs the same initial state the same parameters several different", "tokens": [50600, 9048, 365, 264, 912, 15743, 264, 912, 5883, 1785, 264, 912, 9834, 2940, 819, 50804], "temperature": 0.0, "avg_logprob": -0.08359717396856511, "compression_ratio": 1.9243697478991597, "no_speech_prob": 0.0011334181763231754}, {"id": 315, "seek": 177456, "start": 1783.36, "end": 1790.8799999999999, "text": " measurements yes well i guess the the loss operator may have several uh non-commuting uh", "tokens": [50804, 15383, 2086, 731, 741, 2041, 264, 264, 4470, 12973, 815, 362, 2940, 2232, 2107, 12, 13278, 10861, 2232, 51180], "temperature": 0.0, "avg_logprob": -0.08359717396856511, "compression_ratio": 1.9243697478991597, "no_speech_prob": 0.0011334181763231754}, {"id": 316, "seek": 177456, "start": 1790.8799999999999, "end": 1799.04, "text": " sub operators and one wants to get an expectation value of each uh term in the sum and then one", "tokens": [51180, 1422, 19077, 293, 472, 2738, 281, 483, 364, 14334, 2158, 295, 1184, 2232, 1433, 294, 264, 2408, 293, 550, 472, 51588], "temperature": 0.0, "avg_logprob": -0.08359717396856511, "compression_ratio": 1.9243697478991597, "no_speech_prob": 0.0011334181763231754}, {"id": 317, "seek": 177456, "start": 1799.6799999999998, "end": 1804.48, "text": " adds up all these terms to get an expectation value of the sum so that is called the quantum", "tokens": [51620, 10860, 493, 439, 613, 2115, 281, 483, 364, 14334, 2158, 295, 264, 2408, 370, 300, 307, 1219, 264, 13018, 51860], "temperature": 0.0, "avg_logprob": -0.08359717396856511, "compression_ratio": 1.9243697478991597, "no_speech_prob": 0.0011334181763231754}, {"id": 318, "seek": 180448, "start": 1804.72, "end": 1811.6, "text": " expectation estimation uh sub routine in a sense and here i kind of abstracted it out", "tokens": [50376, 14334, 35701, 2232, 1422, 9927, 294, 257, 2020, 293, 510, 741, 733, 295, 12649, 292, 309, 484, 50720], "temperature": 0.0, "avg_logprob": -0.09262702098259559, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.0018380694091320038}, {"id": 319, "seek": 180448, "start": 1812.32, "end": 1818.32, "text": " but it's it's an extra sub routine there's a mini loop of trying to get a precise estimate", "tokens": [50756, 457, 309, 311, 309, 311, 364, 2857, 1422, 9927, 456, 311, 257, 8382, 6367, 295, 1382, 281, 483, 257, 13600, 12539, 51056], "temperature": 0.0, "avg_logprob": -0.09262702098259559, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.0018380694091320038}, {"id": 320, "seek": 180448, "start": 1818.32, "end": 1822.64, "text": " it's not to be neglected because if you want you know a 10 to the minus seven precision", "tokens": [51056, 309, 311, 406, 281, 312, 32701, 570, 498, 291, 528, 291, 458, 257, 1266, 281, 264, 3175, 3407, 18356, 51272], "temperature": 0.0, "avg_logprob": -0.09262702098259559, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.0018380694091320038}, {"id": 321, "seek": 180448, "start": 1822.64, "end": 1828.24, "text": " for an energy it could take you hours on a 10 kilohertz machine for example so it's it's an", "tokens": [51272, 337, 364, 2281, 309, 727, 747, 291, 2496, 322, 257, 1266, 21112, 35655, 3479, 337, 1365, 370, 309, 311, 309, 311, 364, 51552], "temperature": 0.0, "avg_logprob": -0.09262702098259559, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.0018380694091320038}, {"id": 322, "seek": 180448, "start": 1828.24, "end": 1833.3600000000001, "text": " important thing to consider when designing quantum algorithms that we only have noisy", "tokens": [51552, 1021, 551, 281, 1949, 562, 14685, 13018, 14642, 300, 321, 787, 362, 24518, 51808], "temperature": 0.0, "avg_logprob": -0.09262702098259559, "compression_ratio": 1.7401574803149606, "no_speech_prob": 0.0018380694091320038}, {"id": 323, "seek": 183336, "start": 1833.36, "end": 1839.6799999999998, "text": " or estimates of our our loss function yeah so basically if you run that um yeah so you", "tokens": [50364, 420, 20561, 295, 527, 527, 4470, 2445, 1338, 370, 1936, 498, 291, 1190, 300, 1105, 1338, 370, 291, 50680], "temperature": 0.0, "avg_logprob": -0.10445135901955997, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0015973906265571713}, {"id": 324, "seek": 183336, "start": 1839.6799999999998, "end": 1845.12, "text": " break up the L into sub operators which you measure you know you measure by running multiple", "tokens": [50680, 1821, 493, 264, 441, 666, 1422, 19077, 597, 291, 3481, 291, 458, 291, 3481, 538, 2614, 3866, 50952], "temperature": 0.0, "avg_logprob": -0.10445135901955997, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0015973906265571713}, {"id": 325, "seek": 183336, "start": 1845.12, "end": 1850.3999999999999, "text": " times you get you don't need the distribution here for this you only need the actual mean value", "tokens": [50952, 1413, 291, 483, 291, 500, 380, 643, 264, 7316, 510, 337, 341, 291, 787, 643, 264, 3539, 914, 2158, 51216], "temperature": 0.0, "avg_logprob": -0.10445135901955997, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0015973906265571713}, {"id": 326, "seek": 183336, "start": 1851.04, "end": 1857.1999999999998, "text": " yes right or at least in the in the typical vanilla case but as we'll see there's there's", "tokens": [51248, 2086, 558, 420, 412, 1935, 294, 264, 294, 264, 7476, 17528, 1389, 457, 382, 321, 603, 536, 456, 311, 456, 311, 51556], "temperature": 0.0, "avg_logprob": -0.10445135901955997, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.0015973906265571713}, {"id": 327, "seek": 185720, "start": 1857.2, "end": 1864.24, "text": " other other variants out there but uh usually the quantum part it's it's hard to get a scalar", "tokens": [50364, 661, 661, 21669, 484, 456, 457, 2232, 2673, 264, 13018, 644, 309, 311, 309, 311, 1152, 281, 483, 257, 39684, 50716], "temperature": 0.0, "avg_logprob": -0.19854628245035807, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.04533560946583748}, {"id": 328, "seek": 185720, "start": 1864.24, "end": 1869.3600000000001, "text": " out of the quantum computer by something else than defining an observable and outcomes of measurements", "tokens": [50716, 484, 295, 264, 13018, 3820, 538, 746, 1646, 813, 17827, 364, 9951, 712, 293, 10070, 295, 15383, 50972], "temperature": 0.0, "avg_logprob": -0.19854628245035807, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.04533560946583748}, {"id": 329, "seek": 185720, "start": 1870.48, "end": 1875.68, "text": " right and how important would read out errors and skew and to read out being in a instance you get a", "tokens": [51028, 558, 293, 577, 1021, 576, 1401, 484, 13603, 293, 8756, 86, 293, 281, 1401, 484, 885, 294, 257, 5197, 291, 483, 257, 51288], "temperature": 0.0, "avg_logprob": -0.19854628245035807, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.04533560946583748}, {"id": 330, "seek": 185720, "start": 1876.64, "end": 1882.24, "text": " p1 probability 80 but you have to skew because of the loss t1 process and things like that", "tokens": [51336, 280, 16, 8482, 4688, 457, 291, 362, 281, 8756, 86, 570, 295, 264, 4470, 256, 16, 1399, 293, 721, 411, 300, 51616], "temperature": 0.0, "avg_logprob": -0.19854628245035807, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.04533560946583748}, {"id": 331, "seek": 188224, "start": 1882.24, "end": 1889.2, "text": " that's right you get a imprecise estimate of your your loss function and uh you need to have classical", "tokens": [50364, 300, 311, 558, 291, 483, 257, 704, 13867, 908, 12539, 295, 428, 428, 4470, 2445, 293, 2232, 291, 643, 281, 362, 13735, 50712], "temperature": 0.0, "avg_logprob": -0.09968826451252416, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0007206872687675059}, {"id": 332, "seek": 188224, "start": 1889.2, "end": 1894.48, "text": " algorithms on the side to compensate for that imprecision or to choose your optimizer wisely", "tokens": [50712, 14642, 322, 264, 1252, 281, 29458, 337, 300, 704, 13867, 1991, 420, 281, 2826, 428, 5028, 6545, 37632, 50976], "temperature": 0.0, "avg_logprob": -0.09968826451252416, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0007206872687675059}, {"id": 333, "seek": 188224, "start": 1894.48, "end": 1901.6, "text": " in a way that is robust in noise right and uh i see a lot of questions i i hope i can get to all", "tokens": [50976, 294, 257, 636, 300, 307, 13956, 294, 5658, 558, 293, 2232, 741, 536, 257, 688, 295, 1651, 741, 741, 1454, 741, 393, 483, 281, 439, 51332], "temperature": 0.0, "avg_logprob": -0.09968826451252416, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0007206872687675059}, {"id": 334, "seek": 188224, "start": 1901.6, "end": 1907.6, "text": " of these we may we may have to yeah for the end here okay um maybe just quick one here does the", "tokens": [51332, 295, 613, 321, 815, 321, 815, 362, 281, 1338, 337, 264, 917, 510, 1392, 1105, 1310, 445, 1702, 472, 510, 775, 264, 51632], "temperature": 0.0, "avg_logprob": -0.09968826451252416, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0007206872687675059}, {"id": 335, "seek": 190760, "start": 1907.6799999999998, "end": 1910.9599999999998, "text": " quantum advantage come from generating the variational forms", "tokens": [50368, 13018, 5002, 808, 490, 17746, 264, 3034, 1478, 6422, 50532], "temperature": 0.0, "avg_logprob": -0.10355844903499523, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.004607045091688633}, {"id": 336, "seek": 190760, "start": 1912.1599999999999, "end": 1918.32, "text": " um i mean you know i am not claiming a quantum advantage yet but i would say that uh", "tokens": [50592, 1105, 741, 914, 291, 458, 741, 669, 406, 19232, 257, 13018, 5002, 1939, 457, 741, 576, 584, 300, 2232, 50900], "temperature": 0.0, "avg_logprob": -0.10355844903499523, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.004607045091688633}, {"id": 337, "seek": 190760, "start": 1919.28, "end": 1925.28, "text": " if there if there was a quantum machine learning advantage it would likely come from uh being able", "tokens": [50948, 498, 456, 498, 456, 390, 257, 13018, 3479, 2539, 5002, 309, 576, 3700, 808, 490, 2232, 885, 1075, 51248], "temperature": 0.0, "avg_logprob": -0.10355844903499523, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.004607045091688633}, {"id": 338, "seek": 190760, "start": 1925.28, "end": 1930.9599999999998, "text": " to do the inference or prediction step with your model and hence the ability to train it as well", "tokens": [51248, 281, 360, 264, 38253, 420, 17630, 1823, 365, 428, 2316, 293, 16678, 264, 3485, 281, 3847, 309, 382, 731, 51532], "temperature": 0.0, "avg_logprob": -0.10355844903499523, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.004607045091688633}, {"id": 339, "seek": 190760, "start": 1930.9599999999998, "end": 1935.9199999999998, "text": " so both the training and inference are rendered possible once you have access to a quantum computer", "tokens": [51532, 370, 1293, 264, 3097, 293, 38253, 366, 28748, 1944, 1564, 291, 362, 2105, 281, 257, 13018, 3820, 51780], "temperature": 0.0, "avg_logprob": -0.10355844903499523, "compression_ratio": 1.86864406779661, "no_speech_prob": 0.004607045091688633}, {"id": 340, "seek": 193592, "start": 1935.92, "end": 1941.52, "text": " if you incorporate a model that has high quantum complexity so a large unitary that we can't", "tokens": [50364, 498, 291, 16091, 257, 2316, 300, 575, 1090, 13018, 14024, 370, 257, 2416, 517, 4109, 300, 321, 393, 380, 50644], "temperature": 0.0, "avg_logprob": -0.11675359986045143, "compression_ratio": 1.7117903930131004, "no_speech_prob": 0.0006461180164478719}, {"id": 341, "seek": 193592, "start": 1941.52, "end": 1948.88, "text": " simulate classically um yep and i think this next one you're going to talk about which is back propagation", "tokens": [50644, 27817, 1508, 984, 1105, 18633, 293, 741, 519, 341, 958, 472, 291, 434, 516, 281, 751, 466, 597, 307, 646, 38377, 51012], "temperature": 0.0, "avg_logprob": -0.11675359986045143, "compression_ratio": 1.7117903930131004, "no_speech_prob": 0.0006461180164478719}, {"id": 342, "seek": 193592, "start": 1948.88, "end": 1956.88, "text": " you know can you see yes yes yes um okay so how to practically leverage a quantum computing power", "tokens": [51012, 291, 458, 393, 291, 536, 2086, 2086, 2086, 1105, 1392, 370, 577, 281, 15667, 13982, 257, 13018, 15866, 1347, 51412], "temperature": 0.0, "avg_logprob": -0.11675359986045143, "compression_ratio": 1.7117903930131004, "no_speech_prob": 0.0006461180164478719}, {"id": 343, "seek": 193592, "start": 1957.8400000000001, "end": 1962.3200000000002, "text": " well for discriminative models for example you can uh you know let's say you prepare a quantum", "tokens": [51460, 731, 337, 20828, 1166, 5245, 337, 1365, 291, 393, 2232, 291, 458, 718, 311, 584, 291, 5940, 257, 13018, 51684], "temperature": 0.0, "avg_logprob": -0.11675359986045143, "compression_ratio": 1.7117903930131004, "no_speech_prob": 0.0006461180164478719}, {"id": 344, "seek": 196232, "start": 1962.32, "end": 1967.04, "text": " data set because again for now we don't have a quantum internet where we can import uh data", "tokens": [50364, 1412, 992, 570, 797, 337, 586, 321, 500, 380, 362, 257, 13018, 4705, 689, 321, 393, 974, 2232, 1412, 50600], "temperature": 0.0, "avg_logprob": -0.09058374828762478, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.03675532341003418}, {"id": 345, "seek": 196232, "start": 1968.32, "end": 1973.6, "text": " that'd be nice someday uh and you evaluate your quantum model let's say you do a feed", "tokens": [50664, 300, 1116, 312, 1481, 19412, 2232, 293, 291, 13059, 428, 13018, 2316, 718, 311, 584, 291, 360, 257, 3154, 50928], "temperature": 0.0, "avg_logprob": -0.09058374828762478, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.03675532341003418}, {"id": 346, "seek": 196232, "start": 1973.6, "end": 1980.3999999999999, "text": " forward or a unitary parametrize model you get the expectation value of say several observables", "tokens": [50928, 2128, 420, 257, 517, 4109, 6220, 302, 470, 1381, 2316, 291, 483, 264, 14334, 2158, 295, 584, 2940, 9951, 2965, 51268], "temperature": 0.0, "avg_logprob": -0.09058374828762478, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.03675532341003418}, {"id": 347, "seek": 196232, "start": 1980.3999999999999, "end": 1986.0, "text": " that becomes a vector you feed that vector to a classical neural network and then it evaluates", "tokens": [51268, 300, 3643, 257, 8062, 291, 3154, 300, 8062, 281, 257, 13735, 18161, 3209, 293, 550, 309, 6133, 1024, 51548], "temperature": 0.0, "avg_logprob": -0.09058374828762478, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.03675532341003418}, {"id": 348, "seek": 196232, "start": 1986.0, "end": 1991.84, "text": " some some prediction based on this say a label or whatnot and the idea is that you can train both", "tokens": [51548, 512, 512, 17630, 2361, 322, 341, 584, 257, 7645, 420, 25882, 293, 264, 1558, 307, 300, 291, 393, 3847, 1293, 51840], "temperature": 0.0, "avg_logprob": -0.09058374828762478, "compression_ratio": 1.758490566037736, "no_speech_prob": 0.03675532341003418}, {"id": 349, "seek": 199184, "start": 1991.84, "end": 1998.32, "text": " your classical uh part of your network and your quantum part of the network together uh via a", "tokens": [50364, 428, 13735, 2232, 644, 295, 428, 3209, 293, 428, 13018, 644, 295, 264, 3209, 1214, 2232, 5766, 257, 50688], "temperature": 0.0, "avg_logprob": -0.07198986375188253, "compression_ratio": 2.0634920634920637, "no_speech_prob": 0.0011332744034007192}, {"id": 350, "seek": 199184, "start": 1998.32, "end": 2004.8, "text": " form of quantum classical hybrid back prop and the idea is that you know your quantum neural network", "tokens": [50688, 1254, 295, 13018, 13735, 13051, 646, 2365, 293, 264, 1558, 307, 300, 291, 458, 428, 13018, 18161, 3209, 51012], "temperature": 0.0, "avg_logprob": -0.07198986375188253, "compression_ratio": 2.0634920634920637, "no_speech_prob": 0.0011332744034007192}, {"id": 351, "seek": 199184, "start": 2004.8, "end": 2010.24, "text": " can can have all sorts of components uh but it could itself be a building block in a sort of meta", "tokens": [51012, 393, 393, 362, 439, 7527, 295, 6677, 2232, 457, 309, 727, 2564, 312, 257, 2390, 3461, 294, 257, 1333, 295, 19616, 51284], "temperature": 0.0, "avg_logprob": -0.07198986375188253, "compression_ratio": 2.0634920634920637, "no_speech_prob": 0.0011332744034007192}, {"id": 352, "seek": 199184, "start": 2010.24, "end": 2016.24, "text": " network between quantum neural networks and classical neural networks and the idea is that if you", "tokens": [51284, 3209, 1296, 13018, 18161, 9590, 293, 13735, 18161, 9590, 293, 264, 1558, 307, 300, 498, 291, 51584], "temperature": 0.0, "avg_logprob": -0.07198986375188253, "compression_ratio": 2.0634920634920637, "no_speech_prob": 0.0011332744034007192}, {"id": 353, "seek": 201624, "start": 2016.24, "end": 2022.64, "text": " zoom in on say a little uh sandwich of of nodes here are meta nodes of a deep neural network a", "tokens": [50364, 8863, 294, 322, 584, 257, 707, 2232, 11141, 295, 295, 13891, 510, 366, 19616, 13891, 295, 257, 2452, 18161, 3209, 257, 50684], "temperature": 0.0, "avg_logprob": -0.046763526583180844, "compression_ratio": 2.038793103448276, "no_speech_prob": 0.043356623500585556}, {"id": 354, "seek": 201624, "start": 2022.64, "end": 2027.28, "text": " quantum neural network and a deep neural network so the let's say a deep neural network or any", "tokens": [50684, 13018, 18161, 3209, 293, 257, 2452, 18161, 3209, 370, 264, 718, 311, 584, 257, 2452, 18161, 3209, 420, 604, 50916], "temperature": 0.0, "avg_logprob": -0.046763526583180844, "compression_ratio": 2.038793103448276, "no_speech_prob": 0.043356623500585556}, {"id": 355, "seek": 201624, "start": 2027.28, "end": 2033.52, "text": " differentiable computation feeds parameters to quantum neural network uh and then you have", "tokens": [50916, 819, 9364, 24903, 23712, 9834, 281, 13018, 18161, 3209, 2232, 293, 550, 291, 362, 51228], "temperature": 0.0, "avg_logprob": -0.046763526583180844, "compression_ratio": 2.038793103448276, "no_speech_prob": 0.043356623500585556}, {"id": 356, "seek": 201624, "start": 2033.52, "end": 2037.6, "text": " the measurement of several observables at the output which you feed to a classical neural network", "tokens": [51228, 264, 13160, 295, 2940, 9951, 2965, 412, 264, 5598, 597, 291, 3154, 281, 257, 13735, 18161, 3209, 51432], "temperature": 0.0, "avg_logprob": -0.046763526583180844, "compression_ratio": 2.038793103448276, "no_speech_prob": 0.043356623500585556}, {"id": 357, "seek": 201624, "start": 2037.6, "end": 2043.44, "text": " and and then you could do other stuff later on uh and you get your loss function here then you", "tokens": [51432, 293, 293, 550, 291, 727, 360, 661, 1507, 1780, 322, 2232, 293, 291, 483, 428, 4470, 2445, 510, 550, 291, 51724], "temperature": 0.0, "avg_logprob": -0.046763526583180844, "compression_ratio": 2.038793103448276, "no_speech_prob": 0.043356623500585556}, {"id": 358, "seek": 204344, "start": 2043.8400000000001, "end": 2049.52, "text": " get the gradient of the loss function back propagate uh your gradient classically and", "tokens": [50384, 483, 264, 16235, 295, 264, 4470, 2445, 646, 48256, 2232, 428, 16235, 1508, 984, 293, 50668], "temperature": 0.0, "avg_logprob": -0.07836156625014085, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0007321174489334226}, {"id": 359, "seek": 204344, "start": 2049.52, "end": 2058.56, "text": " effectively what's interesting is that this thing is a actually a itself is is technically", "tokens": [50668, 8659, 437, 311, 1880, 307, 300, 341, 551, 307, 257, 767, 257, 2564, 307, 307, 12120, 51120], "temperature": 0.0, "avg_logprob": -0.07836156625014085, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0007321174489334226}, {"id": 360, "seek": 204344, "start": 2058.56, "end": 2063.68, "text": " an observable on this space if you could you know invert this function but the idea is you do a first", "tokens": [51120, 364, 9951, 712, 322, 341, 1901, 498, 291, 727, 291, 458, 33966, 341, 2445, 457, 264, 1558, 307, 291, 360, 257, 700, 51376], "temperature": 0.0, "avg_logprob": -0.07836156625014085, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0007321174489334226}, {"id": 361, "seek": 204344, "start": 2063.68, "end": 2069.44, "text": " order approximation so you get an effective back propagated gradient Hamiltonian which becomes", "tokens": [51376, 1668, 28023, 370, 291, 483, 364, 4942, 646, 12425, 770, 16235, 18484, 952, 597, 3643, 51664], "temperature": 0.0, "avg_logprob": -0.07836156625014085, "compression_ratio": 1.784688995215311, "no_speech_prob": 0.0007321174489334226}, {"id": 362, "seek": 206944, "start": 2069.44, "end": 2074.32, "text": " or you call it a Hamiltonian because it's an observable and then it becomes just like taking", "tokens": [50364, 420, 291, 818, 309, 257, 18484, 952, 570, 309, 311, 364, 9951, 712, 293, 550, 309, 3643, 445, 411, 1940, 50608], "temperature": 0.0, "avg_logprob": -0.07238868335345844, "compression_ratio": 1.991769547325103, "no_speech_prob": 0.005553700495511293}, {"id": 363, "seek": 206944, "start": 2074.32, "end": 2078.48, "text": " gradients of a vqe to obtain the gradients of these parameters you just have an effective", "tokens": [50608, 2771, 2448, 295, 257, 371, 80, 68, 281, 12701, 264, 2771, 2448, 295, 613, 9834, 291, 445, 362, 364, 4942, 50816], "temperature": 0.0, "avg_logprob": -0.07238868335345844, "compression_ratio": 1.991769547325103, "no_speech_prob": 0.005553700495511293}, {"id": 364, "seek": 206944, "start": 2079.6, "end": 2084.7200000000003, "text": " value of the gradient for a certain value of your your you know all your parameters over here and your", "tokens": [50872, 2158, 295, 264, 16235, 337, 257, 1629, 2158, 295, 428, 428, 291, 458, 439, 428, 9834, 670, 510, 293, 428, 51128], "temperature": 0.0, "avg_logprob": -0.07238868335345844, "compression_ratio": 1.991769547325103, "no_speech_prob": 0.005553700495511293}, {"id": 365, "seek": 206944, "start": 2084.7200000000003, "end": 2090.0, "text": " loss function and you could take gradients of uh that's with respect to your parameters and you've", "tokens": [51128, 4470, 2445, 293, 291, 727, 747, 2771, 2448, 295, 2232, 300, 311, 365, 3104, 281, 428, 9834, 293, 291, 600, 51392], "temperature": 0.0, "avg_logprob": -0.07238868335345844, "compression_ratio": 1.991769547325103, "no_speech_prob": 0.005553700495511293}, {"id": 366, "seek": 206944, "start": 2090.0, "end": 2095.36, "text": " effectively back propagated the gradient of this value through the q and n and you could keep going", "tokens": [51392, 8659, 646, 12425, 770, 264, 16235, 295, 341, 2158, 807, 264, 9505, 293, 297, 293, 291, 727, 1066, 516, 51660], "temperature": 0.0, "avg_logprob": -0.07238868335345844, "compression_ratio": 1.991769547325103, "no_speech_prob": 0.005553700495511293}, {"id": 367, "seek": 209536, "start": 2095.36, "end": 2099.52, "text": " and this is important because you don't want to have to do a slight change do your whole", "tokens": [50364, 293, 341, 307, 1021, 570, 291, 500, 380, 528, 281, 362, 281, 360, 257, 4036, 1319, 360, 428, 1379, 50572], "temperature": 0.0, "avg_logprob": -0.09610454157779091, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.001151207135990262}, {"id": 368, "seek": 209536, "start": 2099.52, "end": 2105.36, "text": " chain of computations see how it changed and then backtrack it's it's more scalable uh this way", "tokens": [50572, 5021, 295, 2807, 763, 536, 577, 309, 3105, 293, 550, 646, 19466, 309, 311, 309, 311, 544, 38481, 2232, 341, 636, 50864], "temperature": 0.0, "avg_logprob": -0.09610454157779091, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.001151207135990262}, {"id": 369, "seek": 209536, "start": 2107.52, "end": 2115.44, "text": " so okay so there's some software uh that does this i have to plug it i mean it's one of my", "tokens": [50972, 370, 1392, 370, 456, 311, 512, 4722, 2232, 300, 775, 341, 741, 362, 281, 5452, 309, 741, 914, 309, 311, 472, 295, 452, 51368], "temperature": 0.0, "avg_logprob": -0.09610454157779091, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.001151207135990262}, {"id": 370, "seek": 209536, "start": 2115.44, "end": 2121.52, "text": " pet projects uh for it's been so for a while uh for now it's uh it's uh interface between", "tokens": [51368, 3817, 4455, 2232, 337, 309, 311, 668, 370, 337, 257, 1339, 2232, 337, 586, 309, 311, 2232, 309, 311, 2232, 9226, 1296, 51672], "temperature": 0.0, "avg_logprob": -0.09610454157779091, "compression_ratio": 1.7632850241545894, "no_speech_prob": 0.001151207135990262}, {"id": 371, "seek": 212152, "start": 2121.52, "end": 2126.32, "text": " cirq and tensorflow there's some open source contributors that are working on quiz kit", "tokens": [50364, 2450, 80, 293, 40863, 10565, 456, 311, 512, 1269, 4009, 45627, 300, 366, 1364, 322, 15450, 8260, 50604], "temperature": 0.0, "avg_logprob": -0.10813458131091429, "compression_ratio": 1.8934426229508197, "no_speech_prob": 0.021937770769000053}, {"id": 372, "seek": 212152, "start": 2126.32, "end": 2130.56, "text": " compatibility so that's going to be exciting for the quiz kit community and we're supporting them", "tokens": [50604, 34237, 370, 300, 311, 516, 281, 312, 4670, 337, 264, 15450, 8260, 1768, 293, 321, 434, 7231, 552, 50816], "temperature": 0.0, "avg_logprob": -0.10813458131091429, "compression_ratio": 1.8934426229508197, "no_speech_prob": 0.021937770769000053}, {"id": 373, "seek": 212152, "start": 2131.84, "end": 2137.92, "text": " but it allows you to you know automate this this training and integrate it into you know", "tokens": [50880, 457, 309, 4045, 291, 281, 291, 458, 31605, 341, 341, 3097, 293, 13365, 309, 666, 291, 458, 51184], "temperature": 0.0, "avg_logprob": -0.10813458131091429, "compression_ratio": 1.8934426229508197, "no_speech_prob": 0.021937770769000053}, {"id": 374, "seek": 212152, "start": 2137.92, "end": 2143.2, "text": " advanced machine learning models in tensorflow and you know tensorflow i think has the record of", "tokens": [51184, 7339, 3479, 2539, 5245, 294, 40863, 10565, 293, 291, 458, 40863, 10565, 741, 519, 575, 264, 2136, 295, 51448], "temperature": 0.0, "avg_logprob": -0.10813458131091429, "compression_ratio": 1.8934426229508197, "no_speech_prob": 0.021937770769000053}, {"id": 375, "seek": 212152, "start": 2144.32, "end": 2150.0, "text": " on ibm supercomputers for you know the biggest machine learning computation so i think it's", "tokens": [51504, 322, 39073, 76, 27839, 2582, 433, 337, 291, 458, 264, 3880, 3479, 2539, 24903, 370, 741, 519, 309, 311, 51788], "temperature": 0.0, "avg_logprob": -0.10813458131091429, "compression_ratio": 1.8934426229508197, "no_speech_prob": 0.021937770769000053}, {"id": 376, "seek": 215000, "start": 2150.0, "end": 2156.32, "text": " important to uh to ideally integrate uh quantum computers with uh the power at least one of the", "tokens": [50364, 1021, 281, 2232, 281, 22915, 13365, 2232, 13018, 10807, 365, 2232, 264, 1347, 412, 1935, 472, 295, 264, 50680], "temperature": 0.0, "avg_logprob": -0.24924577985491073, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0008556307875551283}, {"id": 377, "seek": 215000, "start": 2156.32, "end": 2163.76, "text": " most powerful frameworks for high performance computing on the classical side um so any question", "tokens": [50680, 881, 4005, 29834, 337, 1090, 3389, 15866, 322, 264, 13735, 1252, 1105, 370, 604, 1168, 51052], "temperature": 0.0, "avg_logprob": -0.24924577985491073, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0008556307875551283}, {"id": 378, "seek": 215000, "start": 2163.76, "end": 2169.76, "text": " there in that vein um this is an earlier question for me to are there any data sets filled with the", "tokens": [51052, 456, 294, 300, 30669, 1105, 341, 307, 364, 3071, 1168, 337, 385, 281, 366, 456, 604, 1412, 6352, 6412, 365, 264, 51352], "temperature": 0.0, "avg_logprob": -0.24924577985491073, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0008556307875551283}, {"id": 379, "seek": 215000, "start": 2169.76, "end": 2176.72, "text": " quantum machine learning models you can map up some notebooks for a slayer uh that's uh i think", "tokens": [51352, 13018, 3479, 2539, 5245, 291, 393, 4471, 493, 512, 43782, 337, 257, 1061, 11167, 2232, 300, 311, 2232, 741, 519, 51700], "temperature": 0.0, "avg_logprob": -0.24924577985491073, "compression_ratio": 1.7092511013215859, "no_speech_prob": 0.0008556307875551283}, {"id": 380, "seek": 217672, "start": 2176.72, "end": 2181.12, "text": " that's public that we're working on that and we're trying to work with other you know other uh", "tokens": [50364, 300, 311, 1908, 300, 321, 434, 1364, 322, 300, 293, 321, 434, 1382, 281, 589, 365, 661, 291, 458, 661, 2232, 50584], "temperature": 0.0, "avg_logprob": -0.05161720940044948, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00337599846534431}, {"id": 381, "seek": 217672, "start": 2181.7599999999998, "end": 2187.2799999999997, "text": " companies in the space to make sure we we agree on what a form for a data set will be but in general", "tokens": [50616, 3431, 294, 264, 1901, 281, 652, 988, 321, 321, 3986, 322, 437, 257, 1254, 337, 257, 1412, 992, 486, 312, 457, 294, 2674, 50892], "temperature": 0.0, "avg_logprob": -0.05161720940044948, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00337599846534431}, {"id": 382, "seek": 217672, "start": 2188.24, "end": 2193.52, "text": " because you can't download quantum data you can't just save you know states because they", "tokens": [50940, 570, 291, 393, 380, 5484, 13018, 1412, 291, 393, 380, 445, 3155, 291, 458, 4368, 570, 436, 51204], "temperature": 0.0, "avg_logprob": -0.05161720940044948, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00337599846534431}, {"id": 383, "seek": 217672, "start": 2193.52, "end": 2197.68, "text": " take exponential space and you don't know how to load them on your quantum computer the data set", "tokens": [51204, 747, 21510, 1901, 293, 291, 500, 380, 458, 577, 281, 3677, 552, 322, 428, 13018, 3820, 264, 1412, 992, 51412], "temperature": 0.0, "avg_logprob": -0.05161720940044948, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00337599846534431}, {"id": 384, "seek": 217672, "start": 2197.68, "end": 2203.3599999999997, "text": " takes the form of a circuit um or a set of circuits and those define wave functions that you could", "tokens": [51412, 2516, 264, 1254, 295, 257, 9048, 1105, 420, 257, 992, 295, 26354, 293, 729, 6964, 5772, 6828, 300, 291, 727, 51696], "temperature": 0.0, "avg_logprob": -0.05161720940044948, "compression_ratio": 1.8823529411764706, "no_speech_prob": 0.00337599846534431}, {"id": 385, "seek": 220336, "start": 2203.44, "end": 2208.48, "text": " then do quantum deep learning on and it's something that's uh being worked on but you'll", "tokens": [50368, 550, 360, 13018, 2452, 2539, 322, 293, 309, 311, 746, 300, 311, 2232, 885, 2732, 322, 457, 291, 603, 50620], "temperature": 0.0, "avg_logprob": -0.10987501956046895, "compression_ratio": 1.68, "no_speech_prob": 0.0008966911118477583}, {"id": 386, "seek": 220336, "start": 2208.48, "end": 2218.2400000000002, "text": " have to stay tuned uh for that thank you cool okay so uh what can one do with hybrid feed forward", "tokens": [50620, 362, 281, 1754, 10870, 2232, 337, 300, 1309, 291, 1627, 1392, 370, 2232, 437, 393, 472, 360, 365, 13051, 3154, 2128, 51108], "temperature": 0.0, "avg_logprob": -0.10987501956046895, "compression_ratio": 1.68, "no_speech_prob": 0.0008966911118477583}, {"id": 387, "seek": 220336, "start": 2218.2400000000002, "end": 2224.8, "text": " networks uh i'm going to skip over this uh yeah quickly i guess there's a paper by luke in which", "tokens": [51108, 9590, 2232, 741, 478, 516, 281, 10023, 670, 341, 2232, 1338, 2661, 741, 2041, 456, 311, 257, 3035, 538, 10438, 330, 294, 597, 51436], "temperature": 0.0, "avg_logprob": -0.10987501956046895, "compression_ratio": 1.68, "no_speech_prob": 0.0008966911118477583}, {"id": 388, "seek": 220336, "start": 2224.8, "end": 2230.7200000000003, "text": " is a convolutional neural network which uh are inspired with from the mera if you're in in the", "tokens": [51436, 307, 257, 45216, 304, 18161, 3209, 597, 2232, 366, 7547, 365, 490, 264, 275, 1663, 498, 291, 434, 294, 294, 264, 51732], "temperature": 0.0, "avg_logprob": -0.10987501956046895, "compression_ratio": 1.68, "no_speech_prob": 0.0008966911118477583}, {"id": 389, "seek": 223072, "start": 2230.72, "end": 2238.8799999999997, "text": " know about it uh basically it's luke using the fact that uh if you know your system is", "tokens": [50364, 458, 466, 309, 2232, 1936, 309, 311, 10438, 330, 1228, 264, 1186, 300, 2232, 498, 291, 458, 428, 1185, 307, 50772], "temperature": 0.0, "avg_logprob": -0.05517786602641261, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0043304674327373505}, {"id": 390, "seek": 223072, "start": 2238.8799999999997, "end": 2245.8399999999997, "text": " translationally invariant so it has some symmetry you reflect that symmetry in your your choice of", "tokens": [50772, 12853, 379, 33270, 394, 370, 309, 575, 512, 25440, 291, 5031, 300, 25440, 294, 428, 428, 3922, 295, 51120], "temperature": 0.0, "avg_logprob": -0.05517786602641261, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0043304674327373505}, {"id": 391, "seek": 223072, "start": 2245.8399999999997, "end": 2250.3999999999996, "text": " parameterization of your quantum neural network and so this is just a quantum neural network", "tokens": [51120, 13075, 2144, 295, 428, 13018, 18161, 3209, 293, 370, 341, 307, 445, 257, 13018, 18161, 3209, 51348], "temperature": 0.0, "avg_logprob": -0.05517786602641261, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0043304674327373505}, {"id": 392, "seek": 223072, "start": 2250.3999999999996, "end": 2258.0, "text": " that has translational invariance and is hierarchical and the idea is that you know maybe you can't do", "tokens": [51348, 300, 575, 5105, 1478, 33270, 719, 293, 307, 35250, 804, 293, 264, 1558, 307, 300, 291, 458, 1310, 291, 393, 380, 360, 51728], "temperature": 0.0, "avg_logprob": -0.05517786602641261, "compression_ratio": 1.8405797101449275, "no_speech_prob": 0.0043304674327373505}, {"id": 393, "seek": 225800, "start": 2258.0, "end": 2261.36, "text": " all the quantum layers but maybe you could do only one quantum layer and already you'll", "tokens": [50364, 439, 264, 13018, 7914, 457, 1310, 291, 727, 360, 787, 472, 13018, 4583, 293, 1217, 291, 603, 50532], "temperature": 0.0, "avg_logprob": -0.0678105780748817, "compression_ratio": 1.9109589041095891, "no_speech_prob": 0.03307076543569565}, {"id": 394, "seek": 225800, "start": 2261.36, "end": 2265.6, "text": " you've down sampled the problem you've reduced that dimension dimensionality and you've broken", "tokens": [50532, 291, 600, 760, 3247, 15551, 264, 1154, 291, 600, 9212, 300, 10139, 10139, 1860, 293, 291, 600, 5463, 50744], "temperature": 0.0, "avg_logprob": -0.0678105780748817, "compression_ratio": 1.9109589041095891, "no_speech_prob": 0.03307076543569565}, {"id": 395, "seek": 225800, "start": 2265.6, "end": 2270.08, "text": " up some entanglement or you've you've like disentangled partially remember for compression", "tokens": [50744, 493, 512, 948, 656, 3054, 420, 291, 600, 291, 600, 411, 37313, 39101, 18886, 1604, 337, 19355, 50968], "temperature": 0.0, "avg_logprob": -0.0678105780748817, "compression_ratio": 1.9109589041095891, "no_speech_prob": 0.03307076543569565}, {"id": 396, "seek": 225800, "start": 2270.08, "end": 2275.44, "text": " you got to decorrelate everything all right so the idea is that you can do you could input", "tokens": [50968, 291, 658, 281, 979, 284, 4419, 473, 1203, 439, 558, 370, 264, 1558, 307, 300, 291, 393, 360, 291, 727, 4846, 51236], "temperature": 0.0, "avg_logprob": -0.0678105780748817, "compression_ratio": 1.9109589041095891, "no_speech_prob": 0.03307076543569565}, {"id": 397, "seek": 225800, "start": 2275.44, "end": 2280.08, "text": " various quantum data in batches you could apply various feature maps that are quantum convolutional", "tokens": [51236, 3683, 13018, 1412, 294, 15245, 279, 291, 727, 3079, 3683, 4111, 11317, 300, 366, 13018, 45216, 304, 51468], "temperature": 0.0, "avg_logprob": -0.0678105780748817, "compression_ratio": 1.9109589041095891, "no_speech_prob": 0.03307076543569565}, {"id": 398, "seek": 225800, "start": 2280.08, "end": 2284.88, "text": " networks and then you get kind of images from you know all your histograms of samples of your", "tokens": [51468, 9590, 293, 550, 291, 483, 733, 295, 5267, 490, 291, 458, 439, 428, 49816, 82, 295, 10938, 295, 428, 51708], "temperature": 0.0, "avg_logprob": -0.0678105780748817, "compression_ratio": 1.9109589041095891, "no_speech_prob": 0.03307076543569565}, {"id": 399, "seek": 228488, "start": 2284.88, "end": 2290.6400000000003, "text": " bit strings and following this you could apply classical convolutional layers and you know finish", "tokens": [50364, 857, 13985, 293, 3480, 341, 291, 727, 3079, 13735, 45216, 304, 7914, 293, 291, 458, 2413, 50652], "temperature": 0.0, "avg_logprob": -0.07143419691659872, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.01098472997546196}, {"id": 400, "seek": 228488, "start": 2290.6400000000003, "end": 2296.7200000000003, "text": " the job with fully connected and at least in our early experiments uh hybrid networks with multiple", "tokens": [50652, 264, 1691, 365, 4498, 4582, 293, 412, 1935, 294, 527, 2440, 12050, 2232, 13051, 9590, 365, 3866, 50956], "temperature": 0.0, "avg_logprob": -0.07143419691659872, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.01098472997546196}, {"id": 401, "seek": 228488, "start": 2296.7200000000003, "end": 2302.56, "text": " filters were better than one quantum network and that's without noise so with noise on the device", "tokens": [50956, 15995, 645, 1101, 813, 472, 13018, 3209, 293, 300, 311, 1553, 5658, 370, 365, 5658, 322, 264, 4302, 51248], "temperature": 0.0, "avg_logprob": -0.07143419691659872, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.01098472997546196}, {"id": 402, "seek": 228488, "start": 2302.56, "end": 2307.52, "text": " it's even better uh but that's just a an example of discriminative learning i won't go too much", "tokens": [51248, 309, 311, 754, 1101, 2232, 457, 300, 311, 445, 257, 364, 1365, 295, 20828, 1166, 2539, 741, 1582, 380, 352, 886, 709, 51496], "temperature": 0.0, "avg_logprob": -0.07143419691659872, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.01098472997546196}, {"id": 403, "seek": 228488, "start": 2307.52, "end": 2312.4, "text": " into that but in terms of applications it would be for example classifying phases of matter detecting", "tokens": [51496, 666, 300, 457, 294, 2115, 295, 5821, 309, 576, 312, 337, 1365, 1508, 5489, 18764, 295, 1871, 40237, 51740], "temperature": 0.0, "avg_logprob": -0.07143419691659872, "compression_ratio": 1.7670250896057347, "no_speech_prob": 0.01098472997546196}, {"id": 404, "seek": 231240, "start": 2312.4, "end": 2317.36, "text": " whether something is superconducting or not and the idea is maybe you train on a data set of", "tokens": [50364, 1968, 746, 307, 1687, 38150, 278, 420, 406, 293, 264, 1558, 307, 1310, 291, 3847, 322, 257, 1412, 992, 295, 50612], "temperature": 0.0, "avg_logprob": -0.08376494077878578, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.007813473232090473}, {"id": 405, "seek": 231240, "start": 2318.7200000000003, "end": 2322.08, "text": " a material you know is superconducting at certain value of the parameters and temperature", "tokens": [50680, 257, 2527, 291, 458, 307, 1687, 38150, 278, 412, 1629, 2158, 295, 264, 9834, 293, 4292, 50848], "temperature": 0.0, "avg_logprob": -0.08376494077878578, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.007813473232090473}, {"id": 406, "seek": 231240, "start": 2322.88, "end": 2327.84, "text": " and and then you you ask the neural network to detect for another material that you don't know", "tokens": [50888, 293, 293, 550, 291, 291, 1029, 264, 18161, 3209, 281, 5531, 337, 1071, 2527, 300, 291, 500, 380, 458, 51136], "temperature": 0.0, "avg_logprob": -0.08376494077878578, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.007813473232090473}, {"id": 407, "seek": 231240, "start": 2327.84, "end": 2333.2000000000003, "text": " whether it's superconducting or not at certain value parameters so generalizes so that's uh", "tokens": [51136, 1968, 309, 311, 1687, 38150, 278, 420, 406, 412, 1629, 2158, 9834, 370, 2674, 5660, 370, 300, 311, 2232, 51404], "temperature": 0.0, "avg_logprob": -0.08376494077878578, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.007813473232090473}, {"id": 408, "seek": 231240, "start": 2333.2000000000003, "end": 2340.4, "text": " that's one quote-unquote killer app we think for quantum neural networks um yeah so it's just", "tokens": [51404, 300, 311, 472, 6513, 12, 409, 25016, 13364, 724, 321, 519, 337, 13018, 18161, 9590, 1105, 1338, 370, 309, 311, 445, 51764], "temperature": 0.0, "avg_logprob": -0.08376494077878578, "compression_ratio": 1.9535864978902953, "no_speech_prob": 0.007813473232090473}, {"id": 409, "seek": 234040, "start": 2340.4, "end": 2346.56, "text": " comparing the two with with our old diagram okay so i guess we'll get to the meat of the talk uh", "tokens": [50364, 15763, 264, 732, 365, 365, 527, 1331, 10686, 1392, 370, 741, 2041, 321, 603, 483, 281, 264, 4615, 295, 264, 751, 2232, 50672], "temperature": 0.0, "avg_logprob": -0.09472515366294167, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.001324863056652248}, {"id": 410, "seek": 234040, "start": 2348.08, "end": 2355.04, "text": " i'm not too bad halfway there i guess um so uh how can we extend these insights and how can we", "tokens": [50748, 741, 478, 406, 886, 1578, 15461, 456, 741, 2041, 1105, 370, 2232, 577, 393, 321, 10101, 613, 14310, 293, 577, 393, 321, 51096], "temperature": 0.0, "avg_logprob": -0.09472515366294167, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.001324863056652248}, {"id": 411, "seek": 234040, "start": 2355.04, "end": 2361.52, "text": " hybridize um in a meaningful way with classical machine learning capabilities for quantum machine", "tokens": [51096, 13051, 1125, 1105, 294, 257, 10995, 636, 365, 13735, 3479, 2539, 10862, 337, 13018, 3479, 51420], "temperature": 0.0, "avg_logprob": -0.09472515366294167, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.001324863056652248}, {"id": 412, "seek": 234040, "start": 2361.52, "end": 2368.1600000000003, "text": " learning right let's go back to our slide of deep generative modeling we have our data set", "tokens": [51420, 2539, 558, 718, 311, 352, 646, 281, 527, 4137, 295, 2452, 1337, 1166, 15983, 321, 362, 527, 1412, 992, 51752], "temperature": 0.0, "avg_logprob": -0.09472515366294167, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.001324863056652248}, {"id": 413, "seek": 236816, "start": 2368.16, "end": 2372.7999999999997, "text": " we have our variational classical distribution we want to minimize our question before we", "tokens": [50364, 321, 362, 527, 3034, 1478, 13735, 7316, 321, 528, 281, 17522, 527, 1168, 949, 321, 50596], "temperature": 0.0, "avg_logprob": -0.11868404248438844, "compression_ratio": 1.75, "no_speech_prob": 0.0023589134216308594}, {"id": 414, "seek": 236816, "start": 2373.92, "end": 2379.92, "text": " deep dive here uh it's about um nlp and maybe can we use some of this quantum representation and", "tokens": [50652, 2452, 9192, 510, 2232, 309, 311, 466, 1105, 297, 75, 79, 293, 1310, 393, 321, 764, 512, 295, 341, 13018, 10290, 293, 50952], "temperature": 0.0, "avg_logprob": -0.11868404248438844, "compression_ratio": 1.75, "no_speech_prob": 0.0023589134216308594}, {"id": 415, "seek": 236816, "start": 2379.92, "end": 2384.96, "text": " nlp transformers to reduce the huge size of it to increase accuracy i hope that's a", "tokens": [50952, 297, 75, 79, 4088, 433, 281, 5407, 264, 2603, 2744, 295, 309, 281, 3488, 14170, 741, 1454, 300, 311, 257, 51204], "temperature": 0.0, "avg_logprob": -0.11868404248438844, "compression_ratio": 1.75, "no_speech_prob": 0.0023589134216308594}, {"id": 416, "seek": 236816, "start": 2384.96, "end": 2390.96, "text": " maybe a little bit out there question um so i mean we our team has some public work that we've", "tokens": [51204, 1310, 257, 707, 857, 484, 456, 1168, 1105, 370, 741, 914, 321, 527, 1469, 575, 512, 1908, 589, 300, 321, 600, 51504], "temperature": 0.0, "avg_logprob": -0.11868404248438844, "compression_ratio": 1.75, "no_speech_prob": 0.0023589134216308594}, {"id": 417, "seek": 236816, "start": 2390.96, "end": 2397.7599999999998, "text": " used tensor networks which are you know analogous to quantum circuits in a sense to find factorizations", "tokens": [51504, 1143, 40863, 9590, 597, 366, 291, 458, 16660, 563, 281, 13018, 26354, 294, 257, 2020, 281, 915, 5952, 14455, 51844], "temperature": 0.0, "avg_logprob": -0.11868404248438844, "compression_ratio": 1.75, "no_speech_prob": 0.0023589134216308594}, {"id": 418, "seek": 239776, "start": 2397.76, "end": 2403.76, "text": " of large matrices uh and we apply them to the transformers and at least in our demo we get a", "tokens": [50364, 295, 2416, 32284, 2232, 293, 321, 3079, 552, 281, 264, 4088, 433, 293, 412, 1935, 294, 527, 10723, 321, 483, 257, 50664], "temperature": 0.0, "avg_logprob": -0.0774722629123264, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.001454650890082121}, {"id": 419, "seek": 239776, "start": 2403.76, "end": 2410.32, "text": " two times speed up uh and of course um you know that'd be great if um such a tensor network could", "tokens": [50664, 732, 1413, 3073, 493, 2232, 293, 295, 1164, 1105, 291, 458, 300, 1116, 312, 869, 498, 1105, 1270, 257, 40863, 3209, 727, 50992], "temperature": 0.0, "avg_logprob": -0.0774722629123264, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.001454650890082121}, {"id": 420, "seek": 239776, "start": 2410.32, "end": 2415.6000000000004, "text": " be contracted on a quantum computer uh faster it's not an experiment we've tried yet but um you", "tokens": [50992, 312, 37629, 322, 257, 13018, 3820, 2232, 4663, 309, 311, 406, 364, 5120, 321, 600, 3031, 1939, 457, 1105, 291, 51256], "temperature": 0.0, "avg_logprob": -0.0774722629123264, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.001454650890082121}, {"id": 421, "seek": 239776, "start": 2415.6000000000004, "end": 2420.96, "text": " know it's going to come down to constant speed ups uh you know uh you know our tensor network", "tokens": [51256, 458, 309, 311, 516, 281, 808, 760, 281, 5754, 3073, 15497, 2232, 291, 458, 2232, 291, 458, 527, 40863, 3209, 51524], "temperature": 0.0, "avg_logprob": -0.0774722629123264, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.001454650890082121}, {"id": 422, "seek": 239776, "start": 2420.96, "end": 2424.96, "text": " versus a quantum computer for certain tensor networks the quantum computer is exponentially", "tokens": [51524, 5717, 257, 13018, 3820, 337, 1629, 40863, 9590, 264, 13018, 3820, 307, 37330, 51724], "temperature": 0.0, "avg_logprob": -0.0774722629123264, "compression_ratio": 1.926530612244898, "no_speech_prob": 0.001454650890082121}, {"id": 423, "seek": 242496, "start": 2424.96, "end": 2432.64, "text": " faster but for other tensor networks it's going to be similar uh potentially um so that is that is", "tokens": [50364, 4663, 457, 337, 661, 40863, 9590, 309, 311, 516, 281, 312, 2531, 2232, 7263, 1105, 370, 300, 307, 300, 307, 50748], "temperature": 0.0, "avg_logprob": -0.0947745846163842, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006688710302114487}, {"id": 424, "seek": 242496, "start": 2432.64, "end": 2439.28, "text": " a good question um but uh i guess i guess we'll we'll have to see on that side but it's an interesting", "tokens": [50748, 257, 665, 1168, 1105, 457, 2232, 741, 2041, 741, 2041, 321, 603, 321, 603, 362, 281, 536, 322, 300, 1252, 457, 309, 311, 364, 1880, 51080], "temperature": 0.0, "avg_logprob": -0.0947745846163842, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006688710302114487}, {"id": 425, "seek": 242496, "start": 2439.28, "end": 2447.12, "text": " area of research in a sense uh dimensionality reduction uh using quantum circuits um and uh", "tokens": [51080, 1859, 295, 2132, 294, 257, 2020, 2232, 10139, 1860, 11004, 2232, 1228, 13018, 26354, 1105, 293, 2232, 51472], "temperature": 0.0, "avg_logprob": -0.0947745846163842, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006688710302114487}, {"id": 426, "seek": 242496, "start": 2447.12, "end": 2453.12, "text": " you know tensor networks are a first step towards that um but it's uh you know it's encouraging to", "tokens": [51472, 291, 458, 40863, 9590, 366, 257, 700, 1823, 3030, 300, 1105, 457, 309, 311, 2232, 291, 458, 309, 311, 14580, 281, 51772], "temperature": 0.0, "avg_logprob": -0.0947745846163842, "compression_ratio": 1.798165137614679, "no_speech_prob": 0.006688710302114487}, {"id": 427, "seek": 245312, "start": 2453.12, "end": 2458.56, "text": " see that cutting edge ml can be improved with quantum or quantum inspired methods at least today um so", "tokens": [50364, 536, 300, 6492, 4691, 23271, 393, 312, 9689, 365, 13018, 420, 13018, 7547, 7150, 412, 1935, 965, 1105, 370, 50636], "temperature": 0.0, "avg_logprob": -0.1127870047270362, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0008424814441241324}, {"id": 428, "seek": 245312, "start": 2460.4, "end": 2465.92, "text": " yeah at least in nlp that's the area that i'm confident saying something that quantum computers", "tokens": [50728, 1338, 412, 1935, 294, 297, 75, 79, 300, 311, 264, 1859, 300, 741, 478, 6679, 1566, 746, 300, 13018, 10807, 51004], "temperature": 0.0, "avg_logprob": -0.1127870047270362, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0008424814441241324}, {"id": 429, "seek": 245312, "start": 2465.92, "end": 2478.4, "text": " would be potentially useful um all right uh so so i mentioned we want uh our data set degree uh", "tokens": [51004, 576, 312, 7263, 4420, 1105, 439, 558, 2232, 370, 370, 741, 2835, 321, 528, 2232, 527, 1412, 992, 4314, 2232, 51628], "temperature": 0.0, "avg_logprob": -0.1127870047270362, "compression_ratio": 1.6153846153846154, "no_speech_prob": 0.0008424814441241324}, {"id": 430, "seek": 247840, "start": 2479.2000000000003, "end": 2484.2400000000002, "text": " you know for our data points in general when you want two uh distributions to agree", "tokens": [50404, 291, 458, 337, 527, 1412, 2793, 294, 2674, 562, 291, 528, 732, 2232, 37870, 281, 3986, 50656], "temperature": 0.0, "avg_logprob": -0.10895562753444765, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005383546929806471}, {"id": 431, "seek": 247840, "start": 2484.2400000000002, "end": 2490.2400000000002, "text": " you do what is called the k l divergence uh it's not a symmetric function so be careful uh", "tokens": [50656, 291, 360, 437, 307, 1219, 264, 350, 287, 47387, 2232, 309, 311, 406, 257, 32330, 2445, 370, 312, 5026, 2232, 50956], "temperature": 0.0, "avg_logprob": -0.10895562753444765, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005383546929806471}, {"id": 432, "seek": 247840, "start": 2490.2400000000002, "end": 2496.0, "text": " you could go you go one way or the other uh between your true distribution your data distribution and", "tokens": [50956, 291, 727, 352, 291, 352, 472, 636, 420, 264, 661, 2232, 1296, 428, 2074, 7316, 428, 1412, 7316, 293, 51244], "temperature": 0.0, "avg_logprob": -0.10895562753444765, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005383546929806471}, {"id": 433, "seek": 247840, "start": 2496.0, "end": 2502.56, "text": " your uh variational uh distribution right and it's like here would be the expectation value", "tokens": [51244, 428, 2232, 3034, 1478, 2232, 7316, 558, 293, 309, 311, 411, 510, 576, 312, 264, 14334, 2158, 51572], "temperature": 0.0, "avg_logprob": -0.10895562753444765, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005383546929806471}, {"id": 434, "seek": 250256, "start": 2502.64, "end": 2508.0, "text": " specter of data of the ratio of logs right so the idea is that to evaluate this kind of gold", "tokens": [50368, 6177, 260, 295, 1412, 295, 264, 8509, 295, 20820, 558, 370, 264, 1558, 307, 300, 281, 13059, 341, 733, 295, 3821, 50636], "temperature": 0.0, "avg_logprob": -0.09943581762767974, "compression_ratio": 1.7788461538461537, "no_speech_prob": 0.10664357244968414}, {"id": 435, "seek": 250256, "start": 2508.0, "end": 2514.56, "text": " standard of uh quantum statistic or sorry classical statistical distribution uh we need", "tokens": [50636, 3832, 295, 2232, 13018, 29588, 420, 2597, 13735, 22820, 7316, 2232, 321, 643, 50964], "temperature": 0.0, "avg_logprob": -0.09943581762767974, "compression_ratio": 1.7788461538461537, "no_speech_prob": 0.10664357244968414}, {"id": 436, "seek": 250256, "start": 2515.2799999999997, "end": 2523.6, "text": " access to the log of our logarithm of our uh model for any given data point x that we sample", "tokens": [51000, 2105, 281, 264, 3565, 295, 527, 41473, 32674, 295, 527, 2232, 2316, 337, 604, 2212, 1412, 935, 2031, 300, 321, 6889, 51416], "temperature": 0.0, "avg_logprob": -0.09943581762767974, "compression_ratio": 1.7788461538461537, "no_speech_prob": 0.10664357244968414}, {"id": 437, "seek": 250256, "start": 2523.6, "end": 2531.2, "text": " from the data right so not every uh classical machine learning model allows you to do to do this", "tokens": [51416, 490, 264, 1412, 558, 370, 406, 633, 2232, 13735, 3479, 2539, 2316, 4045, 291, 281, 360, 281, 360, 341, 51796], "temperature": 0.0, "avg_logprob": -0.09943581762767974, "compression_ratio": 1.7788461538461537, "no_speech_prob": 0.10664357244968414}, {"id": 438, "seek": 253120, "start": 2531.2, "end": 2537.7599999999998, "text": " right so gans for example don't have an explicit logarithm of the of the density of your generative", "tokens": [50364, 558, 370, 290, 599, 337, 1365, 500, 380, 362, 364, 13691, 41473, 32674, 295, 264, 295, 264, 10305, 295, 428, 1337, 1166, 50692], "temperature": 0.0, "avg_logprob": -0.06643987323926842, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.004903779830783606}, {"id": 439, "seek": 253120, "start": 2537.7599999999998, "end": 2542.3999999999996, "text": " model that you could query it's implicit it's only you know the discriminator telling you how well", "tokens": [50692, 2316, 300, 291, 727, 14581, 309, 311, 26947, 309, 311, 787, 291, 458, 264, 20828, 1639, 3585, 291, 577, 731, 50924], "temperature": 0.0, "avg_logprob": -0.06643987323926842, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.004903779830783606}, {"id": 440, "seek": 253120, "start": 2542.3999999999996, "end": 2547.7599999999998, "text": " you're doing but it's not a notion of log whereas you have let's say you do a bunch of transformation", "tokens": [50924, 291, 434, 884, 457, 309, 311, 406, 257, 10710, 295, 3565, 9735, 291, 362, 718, 311, 584, 291, 360, 257, 3840, 295, 9887, 51192], "temperature": 0.0, "avg_logprob": -0.06643987323926842, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.004903779830783606}, {"id": 441, "seek": 253120, "start": 2547.7599999999998, "end": 2553.2, "text": " that um that you know uh the determinant of the jacobian you could compute that efficiently", "tokens": [51192, 300, 1105, 300, 291, 458, 2232, 264, 41296, 295, 264, 361, 326, 996, 952, 291, 727, 14722, 300, 19621, 51464], "temperature": 0.0, "avg_logprob": -0.06643987323926842, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.004903779830783606}, {"id": 442, "seek": 253120, "start": 2553.8399999999997, "end": 2558.96, "text": " right the determinant of jacobian if you if you continuously transform a space right and you", "tokens": [51496, 558, 264, 41296, 295, 361, 326, 996, 952, 498, 291, 498, 291, 15684, 4088, 257, 1901, 558, 293, 291, 51752], "temperature": 0.0, "avg_logprob": -0.06643987323926842, "compression_ratio": 1.9246031746031746, "no_speech_prob": 0.004903779830783606}, {"id": 443, "seek": 255896, "start": 2558.96, "end": 2564.48, "text": " had initially a simple Gaussian on that space and you end up with a complicated space you've kind of", "tokens": [50364, 632, 9105, 257, 2199, 39148, 322, 300, 1901, 293, 291, 917, 493, 365, 257, 6179, 1901, 291, 600, 733, 295, 50640], "temperature": 0.0, "avg_logprob": -0.05877077186500633, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.002980615012347698}, {"id": 444, "seek": 255896, "start": 2564.48, "end": 2568.8, "text": " you know bunched it up and you've done some complicated different morphism you could back", "tokens": [50640, 291, 458, 3840, 292, 309, 493, 293, 291, 600, 1096, 512, 6179, 819, 25778, 1434, 291, 727, 646, 50856], "temperature": 0.0, "avg_logprob": -0.05877077186500633, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.002980615012347698}, {"id": 445, "seek": 255896, "start": 2568.8, "end": 2576.7200000000003, "text": " track how the notion of volume locally has changed right and uh for any you know value we target here", "tokens": [50856, 2837, 577, 264, 10710, 295, 5523, 16143, 575, 3105, 558, 293, 2232, 337, 604, 291, 458, 2158, 321, 3779, 510, 51252], "temperature": 0.0, "avg_logprob": -0.05877077186500633, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.002980615012347698}, {"id": 446, "seek": 255896, "start": 2576.7200000000003, "end": 2585.44, "text": " we can kind of invert um the measure in a certain bin here to uh some set of bins over here and we", "tokens": [51252, 321, 393, 733, 295, 33966, 1105, 264, 3481, 294, 257, 1629, 5171, 510, 281, 2232, 512, 992, 295, 41275, 670, 510, 293, 321, 51688], "temperature": 0.0, "avg_logprob": -0.05877077186500633, "compression_ratio": 1.722466960352423, "no_speech_prob": 0.002980615012347698}, {"id": 447, "seek": 258544, "start": 2585.44, "end": 2590.32, "text": " know the value of a Gaussian analytically and so you can compute in a sense somewhat efficiently", "tokens": [50364, 458, 264, 2158, 295, 257, 39148, 10783, 984, 293, 370, 291, 393, 14722, 294, 257, 2020, 8344, 19621, 50608], "temperature": 0.0, "avg_logprob": -0.09129014696393695, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.0016482537612318993}, {"id": 448, "seek": 258544, "start": 2590.32, "end": 2597.04, "text": " analytically the um the density of your your probability distribution for any point you query", "tokens": [50608, 10783, 984, 264, 1105, 264, 10305, 295, 428, 428, 8482, 7316, 337, 604, 935, 291, 14581, 50944], "temperature": 0.0, "avg_logprob": -0.09129014696393695, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.0016482537612318993}, {"id": 449, "seek": 258544, "start": 2597.04, "end": 2601.2000000000003, "text": " uh this is called a normalizing flow but there's other types of models you know there's energy", "tokens": [50944, 2232, 341, 307, 1219, 257, 2710, 3319, 3095, 457, 456, 311, 661, 3467, 295, 5245, 291, 458, 456, 311, 2281, 51152], "temperature": 0.0, "avg_logprob": -0.09129014696393695, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.0016482537612318993}, {"id": 450, "seek": 258544, "start": 2601.2000000000003, "end": 2607.76, "text": " based models there's auto regressive models there's a whole bunch of of cool models out there but a", "tokens": [51152, 2361, 5245, 456, 311, 8399, 1121, 22733, 5245, 456, 311, 257, 1379, 3840, 295, 295, 1627, 5245, 484, 456, 457, 257, 51480], "temperature": 0.0, "avg_logprob": -0.09129014696393695, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.0016482537612318993}, {"id": 451, "seek": 258544, "start": 2607.76, "end": 2612.2400000000002, "text": " lot of people know gans because it's like the entry entry level thing because people understand", "tokens": [51480, 688, 295, 561, 458, 290, 599, 570, 309, 311, 411, 264, 8729, 8729, 1496, 551, 570, 561, 1223, 51704], "temperature": 0.0, "avg_logprob": -0.09129014696393695, "compression_ratio": 1.801498127340824, "no_speech_prob": 0.0016482537612318993}, {"id": 452, "seek": 261224, "start": 2612.3199999999997, "end": 2617.12, "text": " discriminators but so we encourage you to check out other types of generative machine learning", "tokens": [50368, 20828, 3391, 457, 370, 321, 5373, 291, 281, 1520, 484, 661, 3467, 295, 1337, 1166, 3479, 2539, 50608], "temperature": 0.0, "avg_logprob": -0.08627210844547377, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.004608302842825651}, {"id": 453, "seek": 261224, "start": 2617.12, "end": 2623.04, "text": " and in a sense we're we're looking to have an explicit uh notion of a log uh for reasons that", "tokens": [50608, 293, 294, 257, 2020, 321, 434, 321, 434, 1237, 281, 362, 364, 13691, 2232, 10710, 295, 257, 3565, 2232, 337, 4112, 300, 50904], "temperature": 0.0, "avg_logprob": -0.08627210844547377, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.004608302842825651}, {"id": 454, "seek": 261224, "start": 2623.04, "end": 2628.7999999999997, "text": " are going to become apparent in a second in the quantum case so how can we extend this philosophy", "tokens": [50904, 366, 516, 281, 1813, 18335, 294, 257, 1150, 294, 264, 13018, 1389, 370, 577, 393, 321, 10101, 341, 10675, 51192], "temperature": 0.0, "avg_logprob": -0.08627210844547377, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.004608302842825651}, {"id": 455, "seek": 261224, "start": 2628.7999999999997, "end": 2634.7999999999997, "text": " to quantum theory uh you know what's the intersection of quantum theory and probability theory right", "tokens": [51192, 281, 13018, 5261, 2232, 291, 458, 437, 311, 264, 15236, 295, 13018, 5261, 293, 8482, 5261, 558, 51492], "temperature": 0.0, "avg_logprob": -0.08627210844547377, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.004608302842825651}, {"id": 456, "seek": 261224, "start": 2635.9199999999996, "end": 2641.04, "text": " well there is you know just like in in black holes uh we look at black holes because they're at the", "tokens": [51548, 731, 456, 307, 291, 458, 445, 411, 294, 294, 2211, 8118, 2232, 321, 574, 412, 2211, 8118, 570, 436, 434, 412, 264, 51804], "temperature": 0.0, "avg_logprob": -0.08627210844547377, "compression_ratio": 1.7904411764705883, "no_speech_prob": 0.004608302842825651}, {"id": 457, "seek": 264104, "start": 2641.04, "end": 2646.08, "text": " intersection of quantum and gravity so they're an interesting test bed well here we look at", "tokens": [50364, 15236, 295, 13018, 293, 12110, 370, 436, 434, 364, 1880, 1500, 2901, 731, 510, 321, 574, 412, 50616], "temperature": 0.0, "avg_logprob": -0.05506775456090127, "compression_ratio": 1.9176954732510287, "no_speech_prob": 0.0012064625043421984}, {"id": 458, "seek": 264104, "start": 2646.08, "end": 2649.92, "text": " mixed states because they're at the intersection of probability theory and probabilistic machine", "tokens": [50616, 7467, 4368, 570, 436, 434, 412, 264, 15236, 295, 8482, 5261, 293, 31959, 3142, 3479, 50808], "temperature": 0.0, "avg_logprob": -0.05506775456090127, "compression_ratio": 1.9176954732510287, "no_speech_prob": 0.0012064625043421984}, {"id": 459, "seek": 264104, "start": 2649.92, "end": 2654.96, "text": " learning and quantum theory and quantum machine learning so mixed state in general can be a", "tokens": [50808, 2539, 293, 13018, 5261, 293, 13018, 3479, 2539, 370, 7467, 1785, 294, 2674, 393, 312, 257, 51060], "temperature": 0.0, "avg_logprob": -0.05506775456090127, "compression_ratio": 1.9176954732510287, "no_speech_prob": 0.0012064625043421984}, {"id": 460, "seek": 264104, "start": 2654.96, "end": 2660.32, "text": " probabilistic mixture over mixed states these are matrices instead of vectors now so be careful", "tokens": [51060, 31959, 3142, 9925, 670, 7467, 4368, 613, 366, 32284, 2602, 295, 18875, 586, 370, 312, 5026, 51328], "temperature": 0.0, "avg_logprob": -0.05506775456090127, "compression_ratio": 1.9176954732510287, "no_speech_prob": 0.0012064625043421984}, {"id": 461, "seek": 264104, "start": 2661.2, "end": 2666.08, "text": " but uh any density operator has what is called a spectral decomposition so it it's always", "tokens": [51372, 457, 2232, 604, 10305, 12973, 575, 437, 307, 1219, 257, 42761, 48356, 370, 309, 309, 311, 1009, 51616], "temperature": 0.0, "avg_logprob": -0.05506775456090127, "compression_ratio": 1.9176954732510287, "no_speech_prob": 0.0012064625043421984}, {"id": 462, "seek": 266608, "start": 2666.08, "end": 2672.48, "text": " expressible as a mixture of orthogonal pure states and this this mixture sums up to one so it has", "tokens": [50364, 5109, 964, 382, 257, 9925, 295, 41488, 6075, 4368, 293, 341, 341, 9925, 34499, 493, 281, 472, 370, 309, 575, 50684], "temperature": 0.0, "avg_logprob": -0.07736137245274798, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0008558373083360493}, {"id": 463, "seek": 266608, "start": 2672.48, "end": 2680.16, "text": " a probabilistic interpretation so we go from vectors to a density matrix and each element", "tokens": [50684, 257, 31959, 3142, 14174, 370, 321, 352, 490, 18875, 281, 257, 10305, 8141, 293, 1184, 4478, 51068], "temperature": 0.0, "avg_logprob": -0.07736137245274798, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0008558373083360493}, {"id": 464, "seek": 266608, "start": 2680.16, "end": 2687.7599999999998, "text": " is in complex numbers so how would we represent mixed states so how would we represent the", "tokens": [51068, 307, 294, 3997, 3547, 370, 577, 576, 321, 2906, 7467, 4368, 370, 577, 576, 321, 2906, 264, 51448], "temperature": 0.0, "avg_logprob": -0.07736137245274798, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0008558373083360493}, {"id": 465, "seek": 266608, "start": 2687.7599999999998, "end": 2693.52, "text": " intersection of probability theory and quantum theory well we should have a model that composes", "tokens": [51448, 15236, 295, 8482, 5261, 293, 13018, 5261, 731, 321, 820, 362, 257, 2316, 300, 715, 4201, 51736], "temperature": 0.0, "avg_logprob": -0.07736137245274798, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.0008558373083360493}, {"id": 466, "seek": 269352, "start": 2693.52, "end": 2700.8, "text": " a probabilistic model with a quantum model right and that is the idea of quantum probabilistic", "tokens": [50364, 257, 31959, 3142, 2316, 365, 257, 13018, 2316, 558, 293, 300, 307, 264, 1558, 295, 13018, 31959, 3142, 50728], "temperature": 0.0, "avg_logprob": -0.09381622652853688, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.001838349737226963}, {"id": 467, "seek": 269352, "start": 2700.8, "end": 2709.52, "text": " hybrid deep learning or deep representations hence the title of my talk so as we've seen quantum", "tokens": [50728, 13051, 2452, 2539, 420, 2452, 33358, 16678, 264, 4876, 295, 452, 751, 370, 382, 321, 600, 1612, 13018, 51164], "temperature": 0.0, "avg_logprob": -0.09381622652853688, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.001838349737226963}, {"id": 468, "seek": 269352, "start": 2709.52, "end": 2715.28, "text": " neural networks are typically unitary feedforward like this and they have a hypothesis class that is", "tokens": [51164, 18161, 9590, 366, 5850, 517, 4109, 3154, 13305, 411, 341, 293, 436, 362, 257, 17291, 1508, 300, 307, 51452], "temperature": 0.0, "avg_logprob": -0.09381622652853688, "compression_ratio": 1.6497175141242937, "no_speech_prob": 0.001838349737226963}, {"id": 469, "seek": 271528, "start": 2716.0800000000004, "end": 2723.92, "text": " pure states we can combine here a classical parametrized probabilistic model that that we", "tokens": [50404, 6075, 4368, 321, 393, 10432, 510, 257, 13735, 6220, 302, 470, 11312, 31959, 3142, 2316, 300, 300, 321, 50796], "temperature": 0.0, "avg_logprob": -0.07941434759842722, "compression_ratio": 1.875, "no_speech_prob": 0.08878523856401443}, {"id": 470, "seek": 271528, "start": 2723.92, "end": 2729.28, "text": " can sample and let's say this would flip your qubits you flip your qubits to prepare a bit string", "tokens": [50796, 393, 6889, 293, 718, 311, 584, 341, 576, 7929, 428, 421, 34010, 291, 7929, 428, 421, 34010, 281, 5940, 257, 857, 6798, 51064], "temperature": 0.0, "avg_logprob": -0.07941434759842722, "compression_ratio": 1.875, "no_speech_prob": 0.08878523856401443}, {"id": 471, "seek": 271528, "start": 2729.84, "end": 2734.1600000000003, "text": " then you apply unitary that's parametrized and what you get at the output instead of a", "tokens": [51092, 550, 291, 3079, 517, 4109, 300, 311, 6220, 302, 470, 11312, 293, 437, 291, 483, 412, 264, 5598, 2602, 295, 257, 51308], "temperature": 0.0, "avg_logprob": -0.07941434759842722, "compression_ratio": 1.875, "no_speech_prob": 0.08878523856401443}, {"id": 472, "seek": 271528, "start": 2734.1600000000003, "end": 2742.48, "text": " parametrized class of pure states is a parametrized class of mixed states right and uh you know your", "tokens": [51308, 6220, 302, 470, 11312, 1508, 295, 6075, 4368, 307, 257, 6220, 302, 470, 11312, 1508, 295, 7467, 4368, 558, 293, 2232, 291, 458, 428, 51724], "temperature": 0.0, "avg_logprob": -0.07941434759842722, "compression_ratio": 1.875, "no_speech_prob": 0.08878523856401443}, {"id": 473, "seek": 274248, "start": 2742.48, "end": 2746.88, "text": " parametrized distribution your state at this point is a diagonal state so it's effectively", "tokens": [50364, 6220, 302, 470, 11312, 7316, 428, 1785, 412, 341, 935, 307, 257, 21539, 1785, 370, 309, 311, 8659, 50584], "temperature": 0.0, "avg_logprob": -0.06447728474934895, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.014058760367333889}, {"id": 474, "seek": 274248, "start": 2746.88, "end": 2752.08, "text": " classical it has no quantum correlations you can try to show this show there's no coherent", "tokens": [50584, 13735, 309, 575, 572, 13018, 13983, 763, 291, 393, 853, 281, 855, 341, 855, 456, 311, 572, 36239, 50844], "temperature": 0.0, "avg_logprob": -0.06447728474934895, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.014058760367333889}, {"id": 475, "seek": 274248, "start": 2752.08, "end": 2758.72, "text": " neutral information exercise and then after that you tack on a unitary which is hard for", "tokens": [50844, 10598, 1589, 5380, 293, 550, 934, 300, 291, 9426, 322, 257, 517, 4109, 597, 307, 1152, 337, 51176], "temperature": 0.0, "avg_logprob": -0.06447728474934895, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.014058760367333889}, {"id": 476, "seek": 274248, "start": 2758.72, "end": 2763.44, "text": " classical computers to do the idea is we use classical computers and we make them you know we", "tokens": [51176, 13735, 10807, 281, 360, 264, 1558, 307, 321, 764, 13735, 10807, 293, 321, 652, 552, 291, 458, 321, 51412], "temperature": 0.0, "avg_logprob": -0.06447728474934895, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.014058760367333889}, {"id": 477, "seek": 274248, "start": 2763.44, "end": 2768.88, "text": " make them sweat right like inference of probabilistic models can be pretty computationally intense", "tokens": [51412, 652, 552, 11872, 558, 411, 38253, 295, 31959, 3142, 5245, 393, 312, 1238, 24903, 379, 9447, 51684], "temperature": 0.0, "avg_logprob": -0.06447728474934895, "compression_ratio": 1.7876447876447876, "no_speech_prob": 0.014058760367333889}, {"id": 478, "seek": 276888, "start": 2769.44, "end": 2773.12, "text": " uh and then we combine them with unitaries and the quantity", "tokens": [50392, 2232, 293, 550, 321, 10432, 552, 365, 517, 3981, 530, 293, 264, 11275, 50576], "temperature": 0.0, "avg_logprob": -0.20326798756917316, "compression_ratio": 1.7033898305084745, "no_speech_prob": 0.0020817776676267385}, {"id": 479, "seek": 276888, "start": 2773.92, "end": 2778.2400000000002, "text": " and let's talk about how you think we set for capital omega", "tokens": [50616, 293, 718, 311, 751, 466, 577, 291, 519, 321, 992, 337, 4238, 10498, 50832], "temperature": 0.0, "avg_logprob": -0.20326798756917316, "compression_ratio": 1.7033898305084745, "no_speech_prob": 0.0020817776676267385}, {"id": 480, "seek": 276888, "start": 2779.84, "end": 2785.12, "text": " oh yeah so capital omega is just an index over your basis of your Hilbert space it's kind of a", "tokens": [50912, 1954, 1338, 370, 4238, 10498, 307, 445, 364, 8186, 670, 428, 5143, 295, 428, 19914, 4290, 1901, 309, 311, 733, 295, 257, 51176], "temperature": 0.0, "avg_logprob": -0.20326798756917316, "compression_ratio": 1.7033898305084745, "no_speech_prob": 0.0020817776676267385}, {"id": 481, "seek": 276888, "start": 2785.12, "end": 2790.1600000000003, "text": " general formulation because we actually uh phrase the algorithm both for qubits and for", "tokens": [51176, 2674, 37642, 570, 321, 767, 2232, 9535, 264, 9284, 1293, 337, 421, 34010, 293, 337, 51428], "temperature": 0.0, "avg_logprob": -0.20326798756917316, "compression_ratio": 1.7033898305084745, "no_speech_prob": 0.0020817776676267385}, {"id": 482, "seek": 276888, "start": 2790.1600000000003, "end": 2794.8, "text": " continuous infinite dimensional Hilbert spaces so theoretically could be a an integral or something", "tokens": [51428, 10957, 13785, 18795, 19914, 4290, 7673, 370, 29400, 727, 312, 257, 364, 11573, 420, 746, 51660], "temperature": 0.0, "avg_logprob": -0.20326798756917316, "compression_ratio": 1.7033898305084745, "no_speech_prob": 0.0020817776676267385}, {"id": 483, "seek": 279480, "start": 2794.88, "end": 2799.76, "text": " it's just general math but it's uh it's an index that it's a index set that runs over", "tokens": [50368, 309, 311, 445, 2674, 5221, 457, 309, 311, 2232, 309, 311, 364, 8186, 300, 309, 311, 257, 8186, 992, 300, 6676, 670, 50612], "temperature": 0.0, "avg_logprob": -0.11104740635041267, "compression_ratio": 1.8700361010830324, "no_speech_prob": 0.0012642998481169343}, {"id": 484, "seek": 279480, "start": 2800.7200000000003, "end": 2804.0, "text": " an index for your your entire basis that spans your whole Hilbert space of interest", "tokens": [50660, 364, 8186, 337, 428, 428, 2302, 5143, 300, 44086, 428, 1379, 19914, 4290, 1901, 295, 1179, 50824], "temperature": 0.0, "avg_logprob": -0.11104740635041267, "compression_ratio": 1.8700361010830324, "no_speech_prob": 0.0012642998481169343}, {"id": 485, "seek": 279480, "start": 2804.8, "end": 2807.84, "text": " oh yeah and then you can choose basically any probability over", "tokens": [50864, 1954, 1338, 293, 550, 291, 393, 2826, 1936, 604, 8482, 670, 51016], "temperature": 0.0, "avg_logprob": -0.11104740635041267, "compression_ratio": 1.8700361010830324, "no_speech_prob": 0.0012642998481169343}, {"id": 486, "seek": 279480, "start": 2809.92, "end": 2814.6400000000003, "text": " basically anything but there's going to be certain types that are preferential for for training", "tokens": [51120, 1936, 1340, 457, 456, 311, 516, 281, 312, 1629, 3467, 300, 366, 4382, 2549, 337, 337, 3097, 51356], "temperature": 0.0, "avg_logprob": -0.11104740635041267, "compression_ratio": 1.8700361010830324, "no_speech_prob": 0.0012642998481169343}, {"id": 487, "seek": 279480, "start": 2814.6400000000003, "end": 2819.52, "text": " reasons as we'll see uh again you know you could parametrize anything classically but it's not", "tokens": [51356, 4112, 382, 321, 603, 536, 2232, 797, 291, 458, 291, 727, 6220, 302, 470, 1381, 1340, 1508, 984, 457, 309, 311, 406, 51600], "temperature": 0.0, "avg_logprob": -0.11104740635041267, "compression_ratio": 1.8700361010830324, "no_speech_prob": 0.0012642998481169343}, {"id": 488, "seek": 279480, "start": 2819.52, "end": 2824.0, "text": " every model that's easy to train again because let's say you need the log and you can't get it", "tokens": [51600, 633, 2316, 300, 311, 1858, 281, 3847, 797, 570, 718, 311, 584, 291, 643, 264, 3565, 293, 291, 393, 380, 483, 309, 51824], "temperature": 0.0, "avg_logprob": -0.11104740635041267, "compression_ratio": 1.8700361010830324, "no_speech_prob": 0.0012642998481169343}, {"id": 489, "seek": 282400, "start": 2824.0, "end": 2830.72, "text": " or can't get the gradients then it's difficult so as we'll see we can choose wisely how we", "tokens": [50364, 420, 393, 380, 483, 264, 2771, 2448, 550, 309, 311, 2252, 370, 382, 321, 603, 536, 321, 393, 2826, 37632, 577, 321, 50700], "temperature": 0.0, "avg_logprob": -0.09907765431447071, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0011157197877764702}, {"id": 490, "seek": 282400, "start": 2830.72, "end": 2835.76, "text": " parametrize things so that we can get nice gradients and can train things because how do you", "tokens": [50700, 6220, 302, 470, 1381, 721, 370, 300, 321, 393, 483, 1481, 2771, 2448, 293, 393, 3847, 721, 570, 577, 360, 291, 50952], "temperature": 0.0, "avg_logprob": -0.09907765431447071, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0011157197877764702}, {"id": 491, "seek": 282400, "start": 2835.76, "end": 2841.68, "text": " train continuously parametrize the hypothesis class gradient based methods so you use kind of the", "tokens": [50952, 3847, 15684, 6220, 302, 470, 1381, 264, 17291, 1508, 16235, 2361, 7150, 370, 291, 764, 733, 295, 264, 51248], "temperature": 0.0, "avg_logprob": -0.09907765431447071, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0011157197877764702}, {"id": 492, "seek": 282400, "start": 2841.68, "end": 2847.52, "text": " notion of steepest descent in the landscape of parameter space and maybe one more question", "tokens": [51248, 10710, 295, 16841, 377, 23475, 294, 264, 9661, 295, 13075, 1901, 293, 1310, 472, 544, 1168, 51540], "temperature": 0.0, "avg_logprob": -0.09907765431447071, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0011157197877764702}, {"id": 493, "seek": 282400, "start": 2847.52, "end": 2853.36, "text": " I'm told I have to speak a little bit louder so hopefully for sure for I mean let's take the", "tokens": [51540, 286, 478, 1907, 286, 362, 281, 1710, 257, 707, 857, 22717, 370, 4696, 337, 988, 337, 286, 914, 718, 311, 747, 264, 51832], "temperature": 0.0, "avg_logprob": -0.09907765431447071, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.0011157197877764702}, {"id": 494, "seek": 285336, "start": 2853.44, "end": 2859.2000000000003, "text": " extreme case you take a case where you have a pure like harm mixture like you know you have a pure", "tokens": [50368, 8084, 1389, 291, 747, 257, 1389, 689, 291, 362, 257, 6075, 411, 6491, 9925, 411, 291, 458, 291, 362, 257, 6075, 50656], "temperature": 0.0, "avg_logprob": -0.13098253874943175, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0010003757197409868}, {"id": 495, "seek": 285336, "start": 2859.2000000000003, "end": 2864.96, "text": " mixture of all states so I'm guessing that's not very useful one so in a way you want your state to", "tokens": [50656, 9925, 295, 439, 4368, 370, 286, 478, 17939, 300, 311, 406, 588, 4420, 472, 370, 294, 257, 636, 291, 528, 428, 1785, 281, 50944], "temperature": 0.0, "avg_logprob": -0.13098253874943175, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0010003757197409868}, {"id": 496, "seek": 285336, "start": 2864.96, "end": 2870.96, "text": " be a little bit mixed that's somewhat pure or are you okay having like a purity of like zero", "tokens": [50944, 312, 257, 707, 857, 7467, 300, 311, 8344, 6075, 420, 366, 291, 1392, 1419, 411, 257, 34382, 295, 411, 4018, 51244], "temperature": 0.0, "avg_logprob": -0.13098253874943175, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0010003757197409868}, {"id": 497, "seek": 285336, "start": 2871.6, "end": 2877.76, "text": " or maximally mixed even though it's so if you were to optimize over architectures so tune", "tokens": [51276, 420, 5138, 379, 7467, 754, 1673, 309, 311, 370, 498, 291, 645, 281, 19719, 670, 6331, 1303, 370, 10864, 51584], "temperature": 0.0, "avg_logprob": -0.13098253874943175, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0010003757197409868}, {"id": 498, "seek": 285336, "start": 2877.76, "end": 2882.88, "text": " and we have some new results that are not in the paper for this talk you could tune how much quantum", "tokens": [51584, 293, 321, 362, 512, 777, 3542, 300, 366, 406, 294, 264, 3035, 337, 341, 751, 291, 727, 10864, 577, 709, 13018, 51840], "temperature": 0.0, "avg_logprob": -0.13098253874943175, "compression_ratio": 1.8257575757575757, "no_speech_prob": 0.0010003757197409868}, {"id": 499, "seek": 288288, "start": 2882.88, "end": 2889.84, "text": " depth you you you assign to the the unitary um theoretically this this approach could be tuneable", "tokens": [50364, 7161, 291, 291, 291, 6269, 281, 264, 264, 517, 4109, 1105, 29400, 341, 341, 3109, 727, 312, 10864, 712, 50712], "temperature": 0.0, "avg_logprob": -0.0667789550054641, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.0009107065270654857}, {"id": 500, "seek": 288288, "start": 2889.84, "end": 2894.48, "text": " in the sense that if the data set that you're trying to represent is purely classical and has no", "tokens": [50712, 294, 264, 2020, 300, 498, 264, 1412, 992, 300, 291, 434, 1382, 281, 2906, 307, 17491, 13735, 293, 575, 572, 50944], "temperature": 0.0, "avg_logprob": -0.0667789550054641, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.0009107065270654857}, {"id": 501, "seek": 288288, "start": 2894.48, "end": 2900.2400000000002, "text": " quantum correlation and the identities in the span of your unitary hypothesis class you could", "tokens": [50944, 13018, 20009, 293, 264, 24239, 294, 264, 16174, 295, 428, 517, 4109, 17291, 1508, 291, 727, 51232], "temperature": 0.0, "avg_logprob": -0.0667789550054641, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.0009107065270654857}, {"id": 502, "seek": 288288, "start": 2900.2400000000002, "end": 2906.0, "text": " learn to just apply the identity and then it's a classical machine learning system if it's a pure", "tokens": [51232, 1466, 281, 445, 3079, 264, 6575, 293, 550, 309, 311, 257, 13735, 3479, 2539, 1185, 498, 309, 311, 257, 6075, 51520], "temperature": 0.0, "avg_logprob": -0.0667789550054641, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.0009107065270654857}, {"id": 503, "seek": 288288, "start": 2906.0, "end": 2911.12, "text": " state that you're learning this the probabilistic component is useless in a sense because it's", "tokens": [51520, 1785, 300, 291, 434, 2539, 341, 264, 31959, 3142, 6542, 307, 14115, 294, 257, 2020, 570, 309, 311, 51776], "temperature": 0.0, "avg_logprob": -0.0667789550054641, "compression_ratio": 1.9473684210526316, "no_speech_prob": 0.0009107065270654857}, {"id": 504, "seek": 291112, "start": 2911.12, "end": 2915.44, "text": " going to be all unitary you're just trying to learn a pure state so it's an adaptive way to", "tokens": [50364, 516, 281, 312, 439, 517, 4109, 291, 434, 445, 1382, 281, 1466, 257, 6075, 1785, 370, 309, 311, 364, 27912, 636, 281, 50580], "temperature": 0.0, "avg_logprob": -0.10480647863343705, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00038590619806200266}, {"id": 505, "seek": 291112, "start": 2916.3199999999997, "end": 2922.08, "text": " separate out the task of quantum and classical machine learning of a quantum uh mixed states", "tokens": [50624, 4994, 484, 264, 5633, 295, 13018, 293, 13735, 3479, 2539, 295, 257, 13018, 2232, 7467, 4368, 50912], "temperature": 0.0, "avg_logprob": -0.10480647863343705, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00038590619806200266}, {"id": 506, "seek": 291112, "start": 2923.52, "end": 2929.12, "text": " and it's quite cool because you have one framework where you have as a subset uh classical", "tokens": [50984, 293, 309, 311, 1596, 1627, 570, 291, 362, 472, 8388, 689, 291, 362, 382, 257, 25993, 2232, 13735, 51264], "temperature": 0.0, "avg_logprob": -0.10480647863343705, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00038590619806200266}, {"id": 507, "seek": 291112, "start": 2929.12, "end": 2937.12, "text": " generative modeling of distributions um so in a sense it it can via self-tuning it could", "tokens": [51264, 1337, 1166, 15983, 295, 37870, 1105, 370, 294, 257, 2020, 309, 309, 393, 5766, 2698, 12, 83, 37726, 309, 727, 51664], "temperature": 0.0, "avg_logprob": -0.10480647863343705, "compression_ratio": 1.6851851851851851, "no_speech_prob": 0.00038590619806200266}, {"id": 508, "seek": 293712, "start": 2937.12, "end": 2943.04, "text": " adapt to use no quantum resources or use no classical resources or any you know continuum in between", "tokens": [50364, 6231, 281, 764, 572, 13018, 3593, 420, 764, 572, 13735, 3593, 420, 604, 291, 458, 36120, 294, 1296, 50660], "temperature": 0.0, "avg_logprob": -0.11491133577080183, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.009121865034103394}, {"id": 509, "seek": 293712, "start": 2944.56, "end": 2949.68, "text": " and uh we have time here two more quick questions so are you using mixed states for the input as", "tokens": [50736, 293, 2232, 321, 362, 565, 510, 732, 544, 1702, 1651, 370, 366, 291, 1228, 7467, 4368, 337, 264, 4846, 382, 50992], "temperature": 0.0, "avg_logprob": -0.11491133577080183, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.009121865034103394}, {"id": 510, "seek": 293712, "start": 2949.68, "end": 2953.52, "text": " sort of maybe in practice i guess at this level it's all a little more we're we're going to see", "tokens": [50992, 1333, 295, 1310, 294, 3124, 741, 2041, 412, 341, 1496, 309, 311, 439, 257, 707, 544, 321, 434, 321, 434, 516, 281, 536, 51184], "temperature": 0.0, "avg_logprob": -0.11491133577080183, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.009121865034103394}, {"id": 511, "seek": 293712, "start": 2953.52, "end": 2961.68, "text": " that i guess uh it's you could use output or input so yeah um i think i still have a good", "tokens": [51184, 300, 741, 2041, 2232, 309, 311, 291, 727, 764, 5598, 420, 4846, 370, 1338, 1105, 741, 519, 741, 920, 362, 257, 665, 51592], "temperature": 0.0, "avg_logprob": -0.11491133577080183, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.009121865034103394}, {"id": 512, "seek": 296168, "start": 2961.7599999999998, "end": 2967.44, "text": " number of core slides but i guess i'll i'll go through them faster a bit um we can run a little", "tokens": [50368, 1230, 295, 4965, 9788, 457, 741, 2041, 741, 603, 741, 603, 352, 807, 552, 4663, 257, 857, 1105, 321, 393, 1190, 257, 707, 50652], "temperature": 0.0, "avg_logprob": -0.0997548705165826, "compression_ratio": 1.9007936507936507, "no_speech_prob": 0.06751012802124023}, {"id": 513, "seek": 296168, "start": 2967.44, "end": 2973.9199999999996, "text": " longer okay okay so you know why should we care about quantum mixed states well you know thermal", "tokens": [50652, 2854, 1392, 1392, 370, 291, 458, 983, 820, 321, 1127, 466, 13018, 7467, 4368, 731, 291, 458, 15070, 50976], "temperature": 0.0, "avg_logprob": -0.0997548705165826, "compression_ratio": 1.9007936507936507, "no_speech_prob": 0.06751012802124023}, {"id": 514, "seek": 296168, "start": 2973.9199999999996, "end": 2980.7999999999997, "text": " states um are at finite temperature and so you know most systems in nature at finite temperature", "tokens": [50976, 4368, 1105, 366, 412, 19362, 4292, 293, 370, 291, 458, 881, 3652, 294, 3687, 412, 19362, 4292, 51320], "temperature": 0.0, "avg_logprob": -0.0997548705165826, "compression_ratio": 1.9007936507936507, "no_speech_prob": 0.06751012802124023}, {"id": 515, "seek": 296168, "start": 2980.7999999999997, "end": 2985.2, "text": " unfortunately our quantum computers are not at zero temperature so even them themselves must be", "tokens": [51320, 7015, 527, 13018, 10807, 366, 406, 412, 4018, 4292, 370, 754, 552, 2969, 1633, 312, 51540], "temperature": 0.0, "avg_logprob": -0.0997548705165826, "compression_ratio": 1.9007936507936507, "no_speech_prob": 0.06751012802124023}, {"id": 516, "seek": 296168, "start": 2985.2, "end": 2990.24, "text": " modeled as mixed states uh if we were to be accurate and experimentalists know this theorists", "tokens": [51540, 37140, 382, 7467, 4368, 2232, 498, 321, 645, 281, 312, 8559, 293, 17069, 1751, 458, 341, 27423, 1751, 51792], "temperature": 0.0, "avg_logprob": -0.0997548705165826, "compression_ratio": 1.9007936507936507, "no_speech_prob": 0.06751012802124023}, {"id": 517, "seek": 299024, "start": 2990.7999999999997, "end": 2998.08, "text": " like to say it's a pure state um so you know so the ability to simulate mixed states is is crucial", "tokens": [50392, 411, 281, 584, 309, 311, 257, 6075, 1785, 1105, 370, 291, 458, 370, 264, 3485, 281, 27817, 7467, 4368, 307, 307, 11462, 50756], "temperature": 0.0, "avg_logprob": -0.06803352719261532, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.0017817487241700292}, {"id": 518, "seek": 299024, "start": 2998.08, "end": 3002.8799999999997, "text": " to nature and and the reality is like you know we're trying to use quantum computers to simulate", "tokens": [50756, 281, 3687, 293, 293, 264, 4103, 307, 411, 291, 458, 321, 434, 1382, 281, 764, 13018, 10807, 281, 27817, 50996], "temperature": 0.0, "avg_logprob": -0.06803352719261532, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.0017817487241700292}, {"id": 519, "seek": 299024, "start": 3002.8799999999997, "end": 3008.0, "text": " nature but nature itself if you core screen enough you zoom out there's a quantum to classical", "tokens": [50996, 3687, 457, 3687, 2564, 498, 291, 4965, 2568, 1547, 291, 8863, 484, 456, 311, 257, 13018, 281, 13735, 51252], "temperature": 0.0, "avg_logprob": -0.06803352719261532, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.0017817487241700292}, {"id": 520, "seek": 299024, "start": 3008.0, "end": 3013.12, "text": " transition right you know we're used to having classical physics having quantum physics and", "tokens": [51252, 6034, 558, 291, 458, 321, 434, 1143, 281, 1419, 13735, 10649, 1419, 13018, 10649, 293, 51508], "temperature": 0.0, "avg_logprob": -0.06803352719261532, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.0017817487241700292}, {"id": 521, "seek": 299024, "start": 3013.12, "end": 3018.16, "text": " then there's a continuum in between so the point is to have a set of continuum of models", "tokens": [51508, 550, 456, 311, 257, 36120, 294, 1296, 370, 264, 935, 307, 281, 362, 257, 992, 295, 36120, 295, 5245, 51760], "temperature": 0.0, "avg_logprob": -0.06803352719261532, "compression_ratio": 1.9789915966386555, "no_speech_prob": 0.0017817487241700292}, {"id": 522, "seek": 301816, "start": 3018.16, "end": 3023.04, "text": " that can model that in between at finite temperatures when quantumness dies down", "tokens": [50364, 300, 393, 2316, 300, 294, 1296, 412, 19362, 12633, 562, 13018, 1287, 2714, 760, 50608], "temperature": 0.0, "avg_logprob": -0.0907478834453382, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0019264229340478778}, {"id": 523, "seek": 301816, "start": 3023.92, "end": 3030.64, "text": " and uh it becomes classical uh or you know when you're very close to being fully quantum right", "tokens": [50652, 293, 2232, 309, 3643, 13735, 2232, 420, 291, 458, 562, 291, 434, 588, 1998, 281, 885, 4498, 13018, 558, 50988], "temperature": 0.0, "avg_logprob": -0.0907478834453382, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0019264229340478778}, {"id": 524, "seek": 301816, "start": 3031.44, "end": 3034.7999999999997, "text": " uh most quantum systems are open quantum systems i've mentioned this and", "tokens": [51028, 2232, 881, 13018, 3652, 366, 1269, 13018, 3652, 741, 600, 2835, 341, 293, 51196], "temperature": 0.0, "avg_logprob": -0.0907478834453382, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0019264229340478778}, {"id": 525, "seek": 301816, "start": 3035.52, "end": 3041.12, "text": " for various reasons subsystems of of quantum states uh have are mixed states because of", "tokens": [51232, 337, 3683, 4112, 2090, 9321, 82, 295, 295, 13018, 4368, 2232, 362, 366, 7467, 4368, 570, 295, 51512], "temperature": 0.0, "avg_logprob": -0.0907478834453382, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0019264229340478778}, {"id": 526, "seek": 301816, "start": 3041.12, "end": 3044.8799999999997, "text": " entanglement if you take a reduced state of a pure entangled state you get a mixed state", "tokens": [51512, 948, 656, 3054, 498, 291, 747, 257, 9212, 1785, 295, 257, 6075, 948, 39101, 1785, 291, 483, 257, 7467, 1785, 51700], "temperature": 0.0, "avg_logprob": -0.0907478834453382, "compression_ratio": 1.8478260869565217, "no_speech_prob": 0.0019264229340478778}, {"id": 527, "seek": 304488, "start": 3045.52, "end": 3051.2000000000003, "text": " um so just looking at patches of things at a time to model them it's important so", "tokens": [50396, 1105, 370, 445, 1237, 412, 26531, 295, 721, 412, 257, 565, 281, 2316, 552, 309, 311, 1021, 370, 50680], "temperature": 0.0, "avg_logprob": -0.11654907464981079, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.0015484688337892294}, {"id": 528, "seek": 304488, "start": 3052.1600000000003, "end": 3056.0, "text": " what sort of mixed states in nature can we variationally simulate using something like this", "tokens": [50728, 437, 1333, 295, 7467, 4368, 294, 3687, 393, 321, 12990, 379, 27817, 1228, 746, 411, 341, 50920], "temperature": 0.0, "avg_logprob": -0.11654907464981079, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.0015484688337892294}, {"id": 529, "seek": 304488, "start": 3056.0, "end": 3061.84, "text": " well thermal states is of of great interest because they're they're omnipresent so the algorithm", "tokens": [50920, 731, 15070, 4368, 307, 295, 295, 869, 1179, 570, 436, 434, 436, 434, 36874, 647, 11662, 370, 264, 9284, 51212], "temperature": 0.0, "avg_logprob": -0.11654907464981079, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.0015484688337892294}, {"id": 530, "seek": 304488, "start": 3061.84, "end": 3066.32, "text": " that leverages quantum probabilistic generative models to model thermal states is uh variational", "tokens": [51212, 300, 12451, 1660, 13018, 31959, 3142, 1337, 1166, 5245, 281, 2316, 15070, 4368, 307, 2232, 3034, 1478, 51436], "temperature": 0.0, "avg_logprob": -0.11654907464981079, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.0015484688337892294}, {"id": 531, "seek": 306632, "start": 3066.32, "end": 3078.1600000000003, "text": " quantum thermalization and or vqt uh and uh so the problem is given a Hamiltonian and h and a", "tokens": [50364, 13018, 15070, 2144, 293, 420, 371, 80, 83, 2232, 293, 2232, 370, 264, 1154, 307, 2212, 257, 18484, 952, 293, 276, 293, 257, 50956], "temperature": 0.0, "avg_logprob": -0.09346914291381836, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.06006794795393944}, {"id": 532, "seek": 306632, "start": 3078.1600000000003, "end": 3084.2400000000002, "text": " target temperature uh one over beta then generate the thermal state which is the exponential that", "tokens": [50956, 3779, 4292, 2232, 472, 670, 9861, 550, 8460, 264, 15070, 1785, 597, 307, 264, 21510, 300, 51260], "temperature": 0.0, "avg_logprob": -0.09346914291381836, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.06006794795393944}, {"id": 533, "seek": 306632, "start": 3084.2400000000002, "end": 3090.48, "text": " is normalized like this and this is the partition function and the idea is okay well we'll use one", "tokens": [51260, 307, 48704, 411, 341, 293, 341, 307, 264, 24808, 2445, 293, 264, 1558, 307, 1392, 731, 321, 603, 764, 472, 51572], "temperature": 0.0, "avg_logprob": -0.09346914291381836, "compression_ratio": 1.6571428571428573, "no_speech_prob": 0.06006794795393944}, {"id": 534, "seek": 309048, "start": 3090.48, "end": 3099.2, "text": " of our magic models of classical probabilistic distribution and a unitary uh and how are we", "tokens": [50364, 295, 527, 5585, 5245, 295, 13735, 31959, 3142, 7316, 293, 257, 517, 4109, 2232, 293, 577, 366, 321, 50800], "temperature": 0.0, "avg_logprob": -0.10019470905435496, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.020328223705291748}, {"id": 535, "seek": 309048, "start": 3099.2, "end": 3104.88, "text": " going to converge to uh the thermal state well thermal states are the minimum of something", "tokens": [50800, 516, 281, 41881, 281, 2232, 264, 15070, 1785, 731, 15070, 4368, 366, 264, 7285, 295, 746, 51084], "temperature": 0.0, "avg_logprob": -0.10019470905435496, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.020328223705291748}, {"id": 536, "seek": 309048, "start": 3104.88, "end": 3112.88, "text": " called free energy right so free energy is you know uh roughly ignoring temperature energy minus", "tokens": [51084, 1219, 1737, 2281, 558, 370, 1737, 2281, 307, 291, 458, 2232, 9810, 26258, 4292, 2281, 3175, 51484], "temperature": 0.0, "avg_logprob": -0.10019470905435496, "compression_ratio": 1.6706586826347305, "no_speech_prob": 0.020328223705291748}, {"id": 537, "seek": 311288, "start": 3112.96, "end": 3120.1600000000003, "text": " entropy right uh so we can evaluate the energy you know just like in vqe of our model", "tokens": [50368, 30867, 558, 2232, 370, 321, 393, 13059, 264, 2281, 291, 458, 445, 411, 294, 371, 80, 68, 295, 527, 2316, 50728], "temperature": 0.0, "avg_logprob": -0.07137799263000488, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.1731896549463272}, {"id": 538, "seek": 311288, "start": 3120.7200000000003, "end": 3127.04, "text": " and subtract the entropy right so how do we get the entropy well because unitaries can serve", "tokens": [50756, 293, 16390, 264, 30867, 558, 370, 577, 360, 321, 483, 264, 30867, 731, 570, 517, 3981, 530, 393, 4596, 51072], "temperature": 0.0, "avg_logprob": -0.07137799263000488, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.1731896549463272}, {"id": 539, "seek": 311288, "start": 3127.04, "end": 3131.92, "text": " entropy the actually actually the entropy comes strictly from our classical part of the model", "tokens": [51072, 30867, 264, 767, 767, 264, 30867, 1487, 20792, 490, 527, 13735, 644, 295, 264, 2316, 51316], "temperature": 0.0, "avg_logprob": -0.07137799263000488, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.1731896549463272}, {"id": 540, "seek": 311288, "start": 3131.92, "end": 3137.44, "text": " and if your classical model has ways to get gradients of entropy you're in business or", "tokens": [51316, 293, 498, 428, 13735, 2316, 575, 2098, 281, 483, 2771, 2448, 295, 30867, 291, 434, 294, 1606, 420, 51592], "temperature": 0.0, "avg_logprob": -0.07137799263000488, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.1731896549463272}, {"id": 541, "seek": 311288, "start": 3137.44, "end": 3142.4, "text": " you know sometimes it's simple enough you could get it analytically and this is equivalent defining", "tokens": [51592, 291, 458, 2171, 309, 311, 2199, 1547, 291, 727, 483, 309, 10783, 984, 293, 341, 307, 10344, 17827, 51840], "temperature": 0.0, "avg_logprob": -0.07137799263000488, "compression_ratio": 1.8811475409836065, "no_speech_prob": 0.1731896549463272}, {"id": 542, "seek": 314240, "start": 3142.4, "end": 3147.52, "text": " you know the minimum of the relative entropy between our model and the thermal state so you", "tokens": [50364, 291, 458, 264, 7285, 295, 264, 4972, 30867, 1296, 527, 2316, 293, 264, 15070, 1785, 370, 291, 50620], "temperature": 0.0, "avg_logprob": -0.08299296756960312, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0014101420529186726}, {"id": 543, "seek": 314240, "start": 3147.52, "end": 3151.76, "text": " know we know that the unique minimum of this function is when the two states match so if we", "tokens": [50620, 458, 321, 458, 300, 264, 3845, 7285, 295, 341, 2445, 307, 562, 264, 732, 4368, 2995, 370, 498, 321, 50832], "temperature": 0.0, "avg_logprob": -0.08299296756960312, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0014101420529186726}, {"id": 544, "seek": 314240, "start": 3151.76, "end": 3157.76, "text": " do our job well and we parameterize things well and find the absolute optimal states uh then uh", "tokens": [50832, 360, 527, 1691, 731, 293, 321, 13075, 1125, 721, 731, 293, 915, 264, 8236, 16252, 4368, 2232, 550, 2232, 51132], "temperature": 0.0, "avg_logprob": -0.08299296756960312, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0014101420529186726}, {"id": 545, "seek": 314240, "start": 3157.76, "end": 3163.84, "text": " you know we've got the jackpot um state of minimal for energies thermal state so how do we", "tokens": [51132, 291, 458, 321, 600, 658, 264, 7109, 17698, 1105, 1785, 295, 13206, 337, 25737, 15070, 1785, 370, 577, 360, 321, 51436], "temperature": 0.0, "avg_logprob": -0.08299296756960312, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0014101420529186726}, {"id": 546, "seek": 314240, "start": 3163.84, "end": 3167.84, "text": " parameterize our quantum probabilistic generative model i've been pretty abstract now so we're just", "tokens": [51436, 13075, 1125, 527, 13018, 31959, 3142, 1337, 1166, 2316, 741, 600, 668, 1238, 12649, 586, 370, 321, 434, 445, 51636], "temperature": 0.0, "avg_logprob": -0.08299296756960312, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.0014101420529186726}, {"id": 547, "seek": 316784, "start": 3167.84, "end": 3174.1600000000003, "text": " going to zero it in slightly uh well uh the motivation for this work was to take inspiration", "tokens": [50364, 516, 281, 4018, 309, 294, 4748, 2232, 731, 2232, 264, 12335, 337, 341, 589, 390, 281, 747, 10249, 50680], "temperature": 0.0, "avg_logprob": -0.08364573392001065, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.02930273301899433}, {"id": 548, "seek": 316784, "start": 3174.1600000000003, "end": 3179.6000000000004, "text": " from recent work by uh open ai and such on modern versions of energy based models where", "tokens": [50680, 490, 5162, 589, 538, 2232, 1269, 9783, 293, 1270, 322, 4363, 9606, 295, 2281, 2361, 5245, 689, 50952], "temperature": 0.0, "avg_logprob": -0.08364573392001065, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.02930273301899433}, {"id": 549, "seek": 316784, "start": 3180.4, "end": 3185.84, "text": " one it's it's now it's taking inspiration from physics right so you define an energy function", "tokens": [50992, 472, 309, 311, 309, 311, 586, 309, 311, 1940, 10249, 490, 10649, 558, 370, 291, 6964, 364, 2281, 2445, 51264], "temperature": 0.0, "avg_logprob": -0.08364573392001065, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.02930273301899433}, {"id": 550, "seek": 316784, "start": 3185.84, "end": 3189.84, "text": " using a classical neural network let's say from the space of bit strings or continuous values to", "tokens": [51264, 1228, 257, 13735, 18161, 3209, 718, 311, 584, 490, 264, 1901, 295, 857, 13985, 420, 10957, 4190, 281, 51464], "temperature": 0.0, "avg_logprob": -0.08364573392001065, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.02930273301899433}, {"id": 551, "seek": 316784, "start": 3190.4, "end": 3195.6800000000003, "text": " to a scalar and you could use various algorithms that leverage gradient information such as", "tokens": [51492, 281, 257, 39684, 293, 291, 727, 764, 3683, 14642, 300, 13982, 16235, 1589, 1270, 382, 51756], "temperature": 0.0, "avg_logprob": -0.08364573392001065, "compression_ratio": 1.7739463601532568, "no_speech_prob": 0.02930273301899433}, {"id": 552, "seek": 319568, "start": 3195.68, "end": 3202.0, "text": " Hamiltonian Monte Carlo or stochastic gradient Langevin dynamics uh you know all all there's a", "tokens": [50364, 18484, 952, 38105, 45112, 420, 342, 8997, 2750, 16235, 441, 933, 4796, 15679, 2232, 291, 458, 439, 439, 456, 311, 257, 50680], "temperature": 0.0, "avg_logprob": -0.10461421446366743, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001432250952348113}, {"id": 553, "seek": 319568, "start": 3202.0, "end": 3207.2799999999997, "text": " bunch of open source frameworks to to do the this part you could sample the landscape by in a sense", "tokens": [50680, 3840, 295, 1269, 4009, 29834, 281, 281, 360, 264, 341, 644, 291, 727, 6889, 264, 9661, 538, 294, 257, 2020, 50944], "temperature": 0.0, "avg_logprob": -0.10461421446366743, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001432250952348113}, {"id": 554, "seek": 319568, "start": 3207.2799999999997, "end": 3213.44, "text": " having a noisy ball traverse this landscape and you get samples of the exponential this way or the", "tokens": [50944, 1419, 257, 24518, 2594, 45674, 341, 9661, 293, 291, 483, 10938, 295, 264, 21510, 341, 636, 420, 264, 51252], "temperature": 0.0, "avg_logprob": -0.10461421446366743, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001432250952348113}, {"id": 555, "seek": 319568, "start": 3213.44, "end": 3218.56, "text": " Boltzmann distribution known in physics it's a classical Boltzmann distribution so you parameterize", "tokens": [51252, 37884, 89, 14912, 7316, 2570, 294, 10649, 309, 311, 257, 13735, 37884, 89, 14912, 7316, 370, 291, 13075, 1125, 51508], "temperature": 0.0, "avg_logprob": -0.10461421446366743, "compression_ratio": 1.672340425531915, "no_speech_prob": 0.001432250952348113}, {"id": 556, "seek": 321856, "start": 3218.56, "end": 3225.84, "text": " the energy and why why is that going to be useful well uh it has all sorts of compositionality", "tokens": [50364, 264, 2281, 293, 983, 983, 307, 300, 516, 281, 312, 4420, 731, 2232, 309, 575, 439, 7527, 295, 10199, 2628, 507, 50728], "temperature": 0.0, "avg_logprob": -0.10550906631972763, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.03307761996984482}, {"id": 557, "seek": 321856, "start": 3225.84, "end": 3233.04, "text": " perks it's it's very good it's comparative with GANs this is work by open ai 2019 so how do we", "tokens": [50728, 36991, 309, 311, 309, 311, 588, 665, 309, 311, 39292, 365, 460, 1770, 82, 341, 307, 589, 538, 1269, 9783, 6071, 370, 577, 360, 321, 51088], "temperature": 0.0, "avg_logprob": -0.10550906631972763, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.03307761996984482}, {"id": 558, "seek": 321856, "start": 3233.04, "end": 3239.36, "text": " leverage these models and integrate them with quantum computers well so you know what if we had", "tokens": [51088, 13982, 613, 5245, 293, 13365, 552, 365, 13018, 10807, 731, 370, 291, 458, 437, 498, 321, 632, 51404], "temperature": 0.0, "avg_logprob": -0.10550906631972763, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.03307761996984482}, {"id": 559, "seek": 321856, "start": 3239.36, "end": 3243.2799999999997, "text": " our probabilistic part of our model was a classical energy based model like this so it's", "tokens": [51404, 527, 31959, 3142, 644, 295, 527, 2316, 390, 257, 13735, 2281, 2361, 2316, 411, 341, 370, 309, 311, 51600], "temperature": 0.0, "avg_logprob": -0.10550906631972763, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.03307761996984482}, {"id": 560, "seek": 324328, "start": 3243.28, "end": 3249.28, "text": " parameterized energy function and then our distribution is a Boltzmann distribution again", "tokens": [50364, 13075, 1602, 2281, 2445, 293, 550, 527, 7316, 307, 257, 37884, 89, 14912, 7316, 797, 50664], "temperature": 0.0, "avg_logprob": -0.09982081493699407, "compression_ratio": 1.9141414141414141, "no_speech_prob": 0.0075762770138680935}, {"id": 561, "seek": 324328, "start": 3249.28, "end": 3255.76, "text": " well if you you you could define a diagonal operator which is the log after you flip some bits", "tokens": [50664, 731, 498, 291, 291, 291, 727, 6964, 257, 21539, 12973, 597, 307, 264, 3565, 934, 291, 7929, 512, 9239, 50988], "temperature": 0.0, "avg_logprob": -0.09982081493699407, "compression_ratio": 1.9141414141414141, "no_speech_prob": 0.0075762770138680935}, {"id": 562, "seek": 324328, "start": 3256.5600000000004, "end": 3261.6000000000004, "text": " which is your energy function on the diagonal and what you get if you do some math with some", "tokens": [51028, 597, 307, 428, 2281, 2445, 322, 264, 21539, 293, 437, 291, 483, 498, 291, 360, 512, 5221, 365, 512, 51280], "temperature": 0.0, "avg_logprob": -0.09982081493699407, "compression_ratio": 1.9141414141414141, "no_speech_prob": 0.0075762770138680935}, {"id": 563, "seek": 324328, "start": 3261.6000000000004, "end": 3267.76, "text": " unitaries and exponentials is that you've just parameterized a operator the diagonal is parameterized", "tokens": [51280, 517, 3981, 530, 293, 21510, 82, 307, 300, 291, 600, 445, 13075, 1602, 257, 12973, 264, 21539, 307, 13075, 1602, 51588], "temperature": 0.0, "avg_logprob": -0.09982081493699407, "compression_ratio": 1.9141414141414141, "no_speech_prob": 0.0075762770138680935}, {"id": 564, "seek": 326776, "start": 3267.76, "end": 3274.4, "text": " by a neural network and and the total operator is this conjugation of a unitary with this diagonal", "tokens": [50364, 538, 257, 18161, 3209, 293, 293, 264, 3217, 12973, 307, 341, 29456, 399, 295, 257, 517, 4109, 365, 341, 21539, 50696], "temperature": 0.0, "avg_logprob": -0.05658265135504983, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.010011290200054646}, {"id": 565, "seek": 326776, "start": 3274.4, "end": 3281.2000000000003, "text": " operator so you've you've parameterized a Hamiltonian operator and your hypothesis class is a set of", "tokens": [50696, 12973, 370, 291, 600, 291, 600, 13075, 1602, 257, 18484, 952, 12973, 293, 428, 17291, 1508, 307, 257, 992, 295, 51036], "temperature": 0.0, "avg_logprob": -0.05658265135504983, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.010011290200054646}, {"id": 566, "seek": 326776, "start": 3281.2000000000003, "end": 3286.1600000000003, "text": " thermal states so in a sense you you know you know you're targeting a thermal state so you might as", "tokens": [51036, 15070, 4368, 370, 294, 257, 2020, 291, 291, 458, 291, 458, 291, 434, 17918, 257, 15070, 1785, 370, 291, 1062, 382, 51284], "temperature": 0.0, "avg_logprob": -0.05658265135504983, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.010011290200054646}, {"id": 567, "seek": 326776, "start": 3286.1600000000003, "end": 3295.2000000000003, "text": " well have a hypothesis class of thermal states right um okay so if we have this assumption", "tokens": [51284, 731, 362, 257, 17291, 1508, 295, 15070, 4368, 558, 1105, 1392, 370, 498, 321, 362, 341, 15302, 51736], "temperature": 0.0, "avg_logprob": -0.05658265135504983, "compression_ratio": 1.9024390243902438, "no_speech_prob": 0.010011290200054646}, {"id": 568, "seek": 329520, "start": 3295.2, "end": 3300.16, "text": " that we're using an energy based model how do the gradients work out well there's a bit of math", "tokens": [50364, 300, 321, 434, 1228, 364, 2281, 2361, 2316, 577, 360, 264, 2771, 2448, 589, 484, 731, 456, 311, 257, 857, 295, 5221, 50612], "temperature": 0.0, "avg_logprob": -0.08278097046746148, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.09939683228731155}, {"id": 569, "seek": 329520, "start": 3300.16, "end": 3307.6, "text": " involved i've skipped many lines but it is possible to sample it essentially you have to get bit", "tokens": [50612, 3288, 741, 600, 30193, 867, 3876, 457, 309, 307, 1944, 281, 6889, 309, 4476, 291, 362, 281, 483, 857, 50984], "temperature": 0.0, "avg_logprob": -0.08278097046746148, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.09939683228731155}, {"id": 570, "seek": 329520, "start": 3307.6, "end": 3316.3999999999996, "text": " strings at the output of your model and you can evaluate you can compare the value of the energy", "tokens": [50984, 13985, 412, 264, 5598, 295, 428, 2316, 293, 291, 393, 13059, 291, 393, 6794, 264, 2158, 295, 264, 2281, 51424], "temperature": 0.0, "avg_logprob": -0.08278097046746148, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.09939683228731155}, {"id": 571, "seek": 329520, "start": 3316.3999999999996, "end": 3322.64, "text": " of your your bit strings minus the the energy of your model and you could also evaluate gradients of", "tokens": [51424, 295, 428, 428, 857, 13985, 3175, 264, 264, 2281, 295, 428, 2316, 293, 291, 727, 611, 13059, 2771, 2448, 295, 51736], "temperature": 0.0, "avg_logprob": -0.08278097046746148, "compression_ratio": 1.8660287081339713, "no_speech_prob": 0.09939683228731155}, {"id": 572, "seek": 332264, "start": 3322.64, "end": 3327.6, "text": " your your model if it's parameterized by a neural network and you you do the sampling which only", "tokens": [50364, 428, 428, 2316, 498, 309, 311, 13075, 1602, 538, 257, 18161, 3209, 293, 291, 291, 360, 264, 21179, 597, 787, 50612], "temperature": 0.0, "avg_logprob": -0.06523564156521572, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.002550546545535326}, {"id": 573, "seek": 332264, "start": 3327.6, "end": 3336.72, "text": " depends again on sampling from uh from your uh your model p theta of x and you have sampling", "tokens": [50612, 5946, 797, 322, 21179, 490, 2232, 490, 428, 2232, 428, 2316, 280, 9725, 295, 2031, 293, 291, 362, 21179, 51068], "temperature": 0.0, "avg_logprob": -0.06523564156521572, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.002550546545535326}, {"id": 574, "seek": 332264, "start": 3336.72, "end": 3342.4, "text": " algorithms and you can evaluate gradients in a sense it's an analytic way to guarantee that your", "tokens": [51068, 14642, 293, 291, 393, 13059, 2771, 2448, 294, 257, 2020, 309, 311, 364, 40358, 636, 281, 10815, 300, 428, 51352], "temperature": 0.0, "avg_logprob": -0.06523564156521572, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.002550546545535326}, {"id": 575, "seek": 332264, "start": 3342.4, "end": 3348.8799999999997, "text": " estimates of your gradients are unbiased um and how do you get gradients for the quantum part", "tokens": [51352, 20561, 295, 428, 2771, 2448, 366, 517, 5614, 1937, 1105, 293, 577, 360, 291, 483, 2771, 2448, 337, 264, 13018, 644, 51676], "temperature": 0.0, "avg_logprob": -0.06523564156521572, "compression_ratio": 1.784037558685446, "no_speech_prob": 0.002550546545535326}, {"id": 576, "seek": 334888, "start": 3348.96, "end": 3353.12, "text": " well the quantum part is just the usual i hope you've seen this in other talks and", "tokens": [50368, 731, 264, 13018, 644, 307, 445, 264, 7713, 741, 1454, 291, 600, 1612, 341, 294, 661, 6686, 293, 50576], "temperature": 0.0, "avg_logprob": -0.0783387980329881, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0012841076822951436}, {"id": 577, "seek": 334888, "start": 3354.2400000000002, "end": 3358.1600000000003, "text": " i don't have time to cover it unfortunately today it's the parameter shift rule right", "tokens": [50632, 741, 500, 380, 362, 565, 281, 2060, 309, 7015, 965, 309, 311, 264, 13075, 5513, 4978, 558, 50828], "temperature": 0.0, "avg_logprob": -0.0783387980329881, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0012841076822951436}, {"id": 578, "seek": 334888, "start": 3358.8, "end": 3363.52, "text": " which is how you take gradients in the vqe which is how do you take gradients of a", "tokens": [50860, 597, 307, 577, 291, 747, 2771, 2448, 294, 264, 371, 80, 68, 597, 307, 577, 360, 291, 747, 2771, 2448, 295, 257, 51096], "temperature": 0.0, "avg_logprob": -0.0783387980329881, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0012841076822951436}, {"id": 579, "seek": 334888, "start": 3363.52, "end": 3368.48, "text": " a unitary a state fed through a unitary and an expectation value so i won't cover that but", "tokens": [51096, 257, 517, 4109, 257, 1785, 4636, 807, 257, 517, 4109, 293, 364, 14334, 2158, 370, 741, 1582, 380, 2060, 300, 457, 51344], "temperature": 0.0, "avg_logprob": -0.0783387980329881, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0012841076822951436}, {"id": 580, "seek": 334888, "start": 3368.48, "end": 3374.0, "text": " it's very standard it's you know a standard in the software framework and there's various papers", "tokens": [51344, 309, 311, 588, 3832, 309, 311, 291, 458, 257, 3832, 294, 264, 4722, 8388, 293, 456, 311, 3683, 10577, 51620], "temperature": 0.0, "avg_logprob": -0.0783387980329881, "compression_ratio": 1.7918367346938775, "no_speech_prob": 0.0012841076822951436}, {"id": 581, "seek": 337400, "start": 3374.0, "end": 3381.04, "text": " that use this um it's a cool theory but you know does it work right uh the answer is yes you know", "tokens": [50364, 300, 764, 341, 1105, 309, 311, 257, 1627, 5261, 457, 291, 458, 775, 309, 589, 558, 2232, 264, 1867, 307, 2086, 291, 458, 50716], "temperature": 0.0, "avg_logprob": -0.08124579554018767, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0075756884180009365}, {"id": 582, "seek": 337400, "start": 3381.04, "end": 3386.48, "text": " if you if you have a target thermal states you can uh do a reconstruction like this this is for", "tokens": [50716, 498, 291, 498, 291, 362, 257, 3779, 15070, 4368, 291, 393, 2232, 360, 257, 31565, 411, 341, 341, 307, 337, 50988], "temperature": 0.0, "avg_logprob": -0.08124579554018767, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0075756884180009365}, {"id": 583, "seek": 337400, "start": 3386.48, "end": 3391.44, "text": " some heisenberg spin model we use very simple classical distribution here it was just Bernoulli", "tokens": [50988, 512, 415, 11106, 6873, 6060, 2316, 321, 764, 588, 2199, 13735, 7316, 510, 309, 390, 445, 10781, 263, 16320, 51236], "temperature": 0.0, "avg_logprob": -0.08124579554018767, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0075756884180009365}, {"id": 584, "seek": 337400, "start": 3391.44, "end": 3396.56, "text": " so random coins uh and and the quantum computer could do a lot of work and and learn to represent", "tokens": [51236, 370, 4974, 13561, 2232, 293, 293, 264, 13018, 3820, 727, 360, 257, 688, 295, 589, 293, 293, 1466, 281, 2906, 51492], "temperature": 0.0, "avg_logprob": -0.08124579554018767, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0075756884180009365}, {"id": 585, "seek": 337400, "start": 3396.56, "end": 3401.68, "text": " a thermal state we've done much larger systems but you know a jarbled set of pixels is not necessarily", "tokens": [51492, 257, 15070, 1785, 321, 600, 1096, 709, 4833, 3652, 457, 291, 458, 257, 15181, 18320, 992, 295, 18668, 307, 406, 4725, 51748], "temperature": 0.0, "avg_logprob": -0.08124579554018767, "compression_ratio": 1.7562724014336917, "no_speech_prob": 0.0075756884180009365}, {"id": 586, "seek": 340168, "start": 3401.68, "end": 3407.2799999999997, "text": " the most aesthetic thing so we we choose to feature the smaller systems but uh we've scaled", "tokens": [50364, 264, 881, 20092, 551, 370, 321, 321, 2826, 281, 4111, 264, 4356, 3652, 457, 2232, 321, 600, 36039, 50644], "temperature": 0.0, "avg_logprob": -0.08900172937484015, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.002888689050450921}, {"id": 587, "seek": 340168, "start": 3407.2799999999997, "end": 3413.12, "text": " things up quite a bit and uh the idea is um you know the function we're optimizing is relative", "tokens": [50644, 721, 493, 1596, 257, 857, 293, 2232, 264, 1558, 307, 1105, 291, 458, 264, 2445, 321, 434, 40425, 307, 4972, 50936], "temperature": 0.0, "avg_logprob": -0.08900172937484015, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.002888689050450921}, {"id": 588, "seek": 340168, "start": 3413.12, "end": 3419.2799999999997, "text": " free energy but the other metrics of quantum statistical distance uh also converge uh so", "tokens": [50936, 1737, 2281, 457, 264, 661, 16367, 295, 13018, 22820, 4560, 2232, 611, 41881, 2232, 370, 51244], "temperature": 0.0, "avg_logprob": -0.08900172937484015, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.002888689050450921}, {"id": 589, "seek": 340168, "start": 3420.16, "end": 3427.52, "text": " uh it seems to work um we've also tried some set of fermionic systems and bosonic systems", "tokens": [51288, 2232, 309, 2544, 281, 589, 1105, 321, 600, 611, 3031, 512, 992, 295, 26558, 313, 299, 3652, 293, 30641, 11630, 3652, 51656], "temperature": 0.0, "avg_logprob": -0.08900172937484015, "compression_ratio": 1.6820276497695852, "no_speech_prob": 0.002888689050450921}, {"id": 590, "seek": 342752, "start": 3427.52, "end": 3433.12, "text": " for example a simple you know toy model of a superconductor that's uh bosonic uh sorry Gaussian", "tokens": [50364, 337, 1365, 257, 2199, 291, 458, 12058, 2316, 295, 257, 1687, 18882, 84, 1672, 300, 311, 2232, 30641, 11630, 2232, 2597, 39148, 50644], "temperature": 0.0, "avg_logprob": -0.09159016101918321, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0016482205828651786}, {"id": 591, "seek": 342752, "start": 3433.12, "end": 3438.96, "text": " fermionic so it's quite simple we can plot the correlation functions the target this is at iteration", "tokens": [50644, 26558, 313, 299, 370, 309, 311, 1596, 2199, 321, 393, 7542, 264, 20009, 6828, 264, 3779, 341, 307, 412, 24784, 50936], "temperature": 0.0, "avg_logprob": -0.09159016101918321, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0016482205828651786}, {"id": 592, "seek": 342752, "start": 3438.96, "end": 3446.96, "text": " zero and it converges by iteration hundred of gradient descent pretty well um so this is actually", "tokens": [50936, 4018, 293, 309, 9652, 2880, 538, 24784, 3262, 295, 16235, 23475, 1238, 731, 1105, 370, 341, 307, 767, 51336], "temperature": 0.0, "avg_logprob": -0.09159016101918321, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0016482205828651786}, {"id": 593, "seek": 342752, "start": 3446.96, "end": 3452.16, "text": " a new result i i'd like to feature that's not in the paper uh but it's coming in the second version", "tokens": [51336, 257, 777, 1874, 741, 741, 1116, 411, 281, 4111, 300, 311, 406, 294, 264, 3035, 2232, 457, 309, 311, 1348, 294, 264, 1150, 3037, 51596], "temperature": 0.0, "avg_logprob": -0.09159016101918321, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.0016482205828651786}, {"id": 594, "seek": 345216, "start": 3452.16, "end": 3458.8799999999997, "text": " of it uh can we tune how much quantum versus classical resources we use right so suppose i", "tokens": [50364, 295, 309, 2232, 393, 321, 10864, 577, 709, 13018, 5717, 13735, 3593, 321, 764, 558, 370, 7297, 741, 50700], "temperature": 0.0, "avg_logprob": -0.0810450530913939, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.05917120352387428}, {"id": 595, "seek": 345216, "start": 3458.8799999999997, "end": 3463.3599999999997, "text": " look at this heisenberg model and i look at after training how how well i do in terms of", "tokens": [50700, 574, 412, 341, 415, 11106, 6873, 2316, 293, 741, 574, 412, 934, 3097, 577, 577, 731, 741, 360, 294, 2115, 295, 50924], "temperature": 0.0, "avg_logprob": -0.0810450530913939, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.05917120352387428}, {"id": 596, "seek": 345216, "start": 3463.3599999999997, "end": 3467.44, "text": " trace distance and fidelity depending on the temperature and the number of quantum layers", "tokens": [50924, 13508, 4560, 293, 46404, 5413, 322, 264, 4292, 293, 264, 1230, 295, 13018, 7914, 51128], "temperature": 0.0, "avg_logprob": -0.0810450530913939, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.05917120352387428}, {"id": 597, "seek": 345216, "start": 3467.44, "end": 3476.08, "text": " i use ah well we see there's certain sets of temperatures uh that uh you know you need more", "tokens": [51128, 741, 764, 3716, 731, 321, 536, 456, 311, 1629, 6352, 295, 12633, 2232, 300, 2232, 291, 458, 291, 643, 544, 51560], "temperature": 0.0, "avg_logprob": -0.0810450530913939, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.05917120352387428}, {"id": 598, "seek": 347608, "start": 3476.08, "end": 3482.16, "text": " quantum layers to to model them right and it's not necessarily you know at this point it becomes", "tokens": [50364, 13018, 7914, 281, 281, 2316, 552, 558, 293, 309, 311, 406, 4725, 291, 458, 412, 341, 935, 309, 3643, 50668], "temperature": 0.0, "avg_logprob": -0.08188413892473494, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.25958460569381714}, {"id": 599, "seek": 347608, "start": 3482.16, "end": 3488.16, "text": " trivial uh at this point there's a nice balance between quantum and classical resources and this", "tokens": [50668, 26703, 2232, 412, 341, 935, 456, 311, 257, 1481, 4772, 1296, 13018, 293, 13735, 3593, 293, 341, 50968], "temperature": 0.0, "avg_logprob": -0.08188413892473494, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.25958460569381714}, {"id": 600, "seek": 347608, "start": 3488.16, "end": 3493.44, "text": " is the fidelity is trace distance uh but uh this is kind of what you you'd like to do right you", "tokens": [50968, 307, 264, 46404, 307, 13508, 4560, 2232, 457, 2232, 341, 307, 733, 295, 437, 291, 291, 1116, 411, 281, 360, 558, 291, 51232], "temperature": 0.0, "avg_logprob": -0.08188413892473494, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.25958460569381714}, {"id": 601, "seek": 347608, "start": 3493.44, "end": 3498.4, "text": " want to use as little quantum resources as possible uh in order to have an accurate representation", "tokens": [51232, 528, 281, 764, 382, 707, 13018, 3593, 382, 1944, 2232, 294, 1668, 281, 362, 364, 8559, 10290, 51480], "temperature": 0.0, "avg_logprob": -0.08188413892473494, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.25958460569381714}, {"id": 602, "seek": 347608, "start": 3498.4, "end": 3502.88, "text": " of a state so this is something we started investigating but it's uh you know it maybe", "tokens": [51480, 295, 257, 1785, 370, 341, 307, 746, 321, 1409, 22858, 457, 309, 311, 2232, 291, 458, 309, 1310, 51704], "temperature": 0.0, "avg_logprob": -0.08188413892473494, "compression_ratio": 1.8199233716475096, "no_speech_prob": 0.25958460569381714}, {"id": 603, "seek": 350288, "start": 3502.88, "end": 3507.28, "text": " has some deep implications about what's the true quantum complexity of a quantum machine learning", "tokens": [50364, 575, 512, 2452, 16602, 466, 437, 311, 264, 2074, 13018, 14024, 295, 257, 13018, 3479, 2539, 50584], "temperature": 0.0, "avg_logprob": -0.11491332797829164, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.029723845422267914}, {"id": 604, "seek": 350288, "start": 3507.28, "end": 3514.96, "text": " problem uh and uh i guess uh you know uh please take a look at these qr codes there's links to", "tokens": [50584, 1154, 2232, 293, 2232, 741, 2041, 2232, 291, 458, 2232, 1767, 747, 257, 574, 412, 613, 9505, 81, 14211, 456, 311, 6123, 281, 50968], "temperature": 0.0, "avg_logprob": -0.11491332797829164, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.029723845422267914}, {"id": 605, "seek": 350288, "start": 3514.96, "end": 3520.0, "text": " various notebooks and you know i've been advertising tensor flow quantum but there's there's obviously", "tokens": [50968, 3683, 43782, 293, 291, 458, 741, 600, 668, 13097, 40863, 3095, 13018, 457, 456, 311, 456, 311, 2745, 51220], "temperature": 0.0, "avg_logprob": -0.11491332797829164, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.029723845422267914}, {"id": 606, "seek": 350288, "start": 3520.0, "end": 3524.96, "text": " you know implementations in quiz kit from the community uh shout out to jack seroni and uh", "tokens": [51220, 291, 458, 4445, 763, 294, 15450, 8260, 490, 264, 1768, 2232, 8043, 484, 281, 7109, 816, 17049, 293, 2232, 51468], "temperature": 0.0, "avg_logprob": -0.11491332797829164, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.029723845422267914}, {"id": 607, "seek": 350288, "start": 3524.96, "end": 3530.88, "text": " you know the tensor flow quantum implementations by my collaborator antonio martinez um and three", "tokens": [51468, 291, 458, 264, 40863, 3095, 13018, 4445, 763, 538, 452, 5091, 1639, 364, 1756, 1004, 12396, 533, 89, 1105, 293, 1045, 51764], "temperature": 0.0, "avg_logprob": -0.11491332797829164, "compression_ratio": 1.8473282442748091, "no_speech_prob": 0.029723845422267914}, {"id": 608, "seek": 353088, "start": 3530.88, "end": 3537.2000000000003, "text": " two one take a picture on youtube or whatnot and look at the uh websites for these notebooks", "tokens": [50364, 732, 472, 747, 257, 3036, 322, 12487, 420, 25882, 293, 574, 412, 264, 2232, 12891, 337, 613, 43782, 50680], "temperature": 0.0, "avg_logprob": -0.1037606632008272, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.016396772116422653}, {"id": 609, "seek": 353088, "start": 3538.4, "end": 3543.84, "text": " so the final component is more machine learning it's less quantum simulation is how do we use vqt", "tokens": [50740, 370, 264, 2572, 6542, 307, 544, 3479, 2539, 309, 311, 1570, 13018, 16575, 307, 577, 360, 321, 764, 371, 80, 83, 51012], "temperature": 0.0, "avg_logprob": -0.1037606632008272, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.016396772116422653}, {"id": 610, "seek": 353088, "start": 3544.7200000000003, "end": 3549.04, "text": " to do quantum machine learning so if we're given quantum mixed state data how do we", "tokens": [51056, 281, 360, 13018, 3479, 2539, 370, 498, 321, 434, 2212, 13018, 7467, 1785, 1412, 577, 360, 321, 51272], "temperature": 0.0, "avg_logprob": -0.1037606632008272, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.016396772116422653}, {"id": 611, "seek": 353088, "start": 3550.56, "end": 3556.48, "text": " learn from quantum mixed state data so again we're going to use our quantum Hamiltonian based model", "tokens": [51348, 1466, 490, 13018, 7467, 1785, 1412, 370, 797, 321, 434, 516, 281, 764, 527, 13018, 18484, 952, 2361, 2316, 51644], "temperature": 0.0, "avg_logprob": -0.1037606632008272, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.016396772116422653}, {"id": 612, "seek": 355648, "start": 3556.48, "end": 3562.96, "text": " because for reasons that are going to become apparent in a second so we call the task of", "tokens": [50364, 570, 337, 4112, 300, 366, 516, 281, 1813, 18335, 294, 257, 1150, 370, 321, 818, 264, 5633, 295, 50688], "temperature": 0.0, "avg_logprob": -0.0464477866303687, "compression_ratio": 2.012396694214876, "no_speech_prob": 0.001572968321852386}, {"id": 613, "seek": 355648, "start": 3562.96, "end": 3570.2400000000002, "text": " learning to replicate right we want an approximate density matrix that approximates a data density", "tokens": [50688, 2539, 281, 25356, 558, 321, 528, 364, 30874, 10305, 8141, 300, 8542, 1024, 257, 1412, 10305, 51052], "temperature": 0.0, "avg_logprob": -0.0464477866303687, "compression_ratio": 2.012396694214876, "no_speech_prob": 0.001572968321852386}, {"id": 614, "seek": 355648, "start": 3570.2400000000002, "end": 3574.56, "text": " matrix so a data density matrix could be itself a mixture of a bunch of density matrices we're just", "tokens": [51052, 8141, 370, 257, 1412, 10305, 8141, 727, 312, 2564, 257, 9925, 295, 257, 3840, 295, 10305, 32284, 321, 434, 445, 51268], "temperature": 0.0, "avg_logprob": -0.0464477866303687, "compression_ratio": 2.012396694214876, "no_speech_prob": 0.001572968321852386}, {"id": 615, "seek": 355648, "start": 3574.56, "end": 3579.52, "text": " trying to approximate this thing and we want to find a set of parameters such that for the optimal", "tokens": [51268, 1382, 281, 30874, 341, 551, 293, 321, 528, 281, 915, 257, 992, 295, 9834, 1270, 300, 337, 264, 16252, 51516], "temperature": 0.0, "avg_logprob": -0.0464477866303687, "compression_ratio": 2.012396694214876, "no_speech_prob": 0.001572968321852386}, {"id": 616, "seek": 355648, "start": 3579.52, "end": 3585.36, "text": " parameters our hypothesis class approximates this density matrix and we assume we have access to the", "tokens": [51516, 9834, 527, 17291, 1508, 8542, 1024, 341, 10305, 8141, 293, 321, 6552, 321, 362, 2105, 281, 264, 51808], "temperature": 0.0, "avg_logprob": -0.0464477866303687, "compression_ratio": 2.012396694214876, "no_speech_prob": 0.001572968321852386}, {"id": 617, "seek": 358536, "start": 3585.36, "end": 3591.52, "text": " quantum form of the data okay and the idea is if you use a quantum Hamiltonian based model", "tokens": [50364, 13018, 1254, 295, 264, 1412, 1392, 293, 264, 1558, 307, 498, 291, 764, 257, 13018, 18484, 952, 2361, 2316, 50672], "temperature": 0.0, "avg_logprob": -0.059543215951254204, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0012064563343301415}, {"id": 618, "seek": 358536, "start": 3592.1600000000003, "end": 3601.04, "text": " and you aim to minimize now the relative entropy in reverse from last time uh what you get is if", "tokens": [50704, 293, 291, 5939, 281, 17522, 586, 264, 4972, 30867, 294, 9943, 490, 1036, 565, 2232, 437, 291, 483, 307, 498, 51148], "temperature": 0.0, "avg_logprob": -0.059543215951254204, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0012064563343301415}, {"id": 619, "seek": 358536, "start": 3601.04, "end": 3603.92, "text": " you ignore the terms that don't depend on your parameters you get something called the cross", "tokens": [51148, 291, 11200, 264, 2115, 300, 500, 380, 5672, 322, 428, 9834, 291, 483, 746, 1219, 264, 3278, 51292], "temperature": 0.0, "avg_logprob": -0.059543215951254204, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0012064563343301415}, {"id": 620, "seek": 358536, "start": 3603.92, "end": 3610.56, "text": " entropy which is this the trace of stigma which is your data log row right and again because we've", "tokens": [51292, 30867, 597, 307, 341, 264, 13508, 295, 27880, 597, 307, 428, 1412, 3565, 5386, 558, 293, 797, 570, 321, 600, 51624], "temperature": 0.0, "avg_logprob": -0.059543215951254204, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0012064563343301415}, {"id": 621, "seek": 361056, "start": 3610.56, "end": 3617.12, "text": " parametrized our hypothesis class in terms of its logarithm its quantum logarithm uh we can evaluate", "tokens": [50364, 6220, 302, 470, 11312, 527, 17291, 1508, 294, 2115, 295, 1080, 41473, 32674, 1080, 13018, 41473, 32674, 2232, 321, 393, 13059, 50692], "temperature": 0.0, "avg_logprob": -0.07181419121040093, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.00039202882908284664}, {"id": 622, "seek": 361056, "start": 3617.12, "end": 3623.52, "text": " this energy this it's called modular energy or modular free energy and modular Hamiltonian is", "tokens": [50692, 341, 2281, 341, 309, 311, 1219, 31111, 2281, 420, 31111, 1737, 2281, 293, 31111, 18484, 952, 307, 51012], "temperature": 0.0, "avg_logprob": -0.07181419121040093, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.00039202882908284664}, {"id": 623, "seek": 361056, "start": 3623.52, "end": 3630.0, "text": " just a name for the log of a density matrix okay and so we're trying to learn a log of a density", "tokens": [51012, 445, 257, 1315, 337, 264, 3565, 295, 257, 10305, 8141, 1392, 293, 370, 321, 434, 1382, 281, 1466, 257, 3565, 295, 257, 10305, 51336], "temperature": 0.0, "avg_logprob": -0.07181419121040093, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.00039202882908284664}, {"id": 624, "seek": 361056, "start": 3630.0, "end": 3634.88, "text": " matrix such that the exponential replicates our data set and how you do this you plug your data", "tokens": [51336, 8141, 1270, 300, 264, 21510, 3248, 299, 1024, 527, 1412, 992, 293, 577, 291, 360, 341, 291, 5452, 428, 1412, 51580], "temperature": 0.0, "avg_logprob": -0.07181419121040093, "compression_ratio": 1.7834101382488479, "no_speech_prob": 0.00039202882908284664}, {"id": 625, "seek": 363488, "start": 3634.88, "end": 3641.2000000000003, "text": " you run it in reverse through your unitary of your quantum probabilistic model you sample it and then", "tokens": [50364, 291, 1190, 309, 294, 9943, 807, 428, 517, 4109, 295, 428, 13018, 31959, 3142, 2316, 291, 6889, 309, 293, 550, 50680], "temperature": 0.0, "avg_logprob": -0.0576611978036386, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.03620651736855507}, {"id": 626, "seek": 363488, "start": 3641.2000000000003, "end": 3647.44, "text": " you get expectation values of the diagonal operator and this could be parametrized with a neural network", "tokens": [50680, 291, 483, 14334, 4190, 295, 264, 21539, 12973, 293, 341, 727, 312, 6220, 302, 470, 11312, 365, 257, 18161, 3209, 50992], "temperature": 0.0, "avg_logprob": -0.0576611978036386, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.03620651736855507}, {"id": 627, "seek": 363488, "start": 3647.44, "end": 3652.8, "text": " so you can have more computation here the extra term here is all on the classical computer", "tokens": [50992, 370, 291, 393, 362, 544, 24903, 510, 264, 2857, 1433, 510, 307, 439, 322, 264, 13735, 3820, 51260], "temperature": 0.0, "avg_logprob": -0.0576611978036386, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.03620651736855507}, {"id": 628, "seek": 363488, "start": 3653.52, "end": 3658.48, "text": " turns out you could also get gradients for these i won't go too much into it the quantum part is", "tokens": [51296, 4523, 484, 291, 727, 611, 483, 2771, 2448, 337, 613, 741, 1582, 380, 352, 886, 709, 666, 309, 264, 13018, 644, 307, 51544], "temperature": 0.0, "avg_logprob": -0.0576611978036386, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.03620651736855507}, {"id": 629, "seek": 363488, "start": 3658.48, "end": 3664.32, "text": " again parameter shift but these gradients again if you have a differentiable function for your energy", "tokens": [51544, 797, 13075, 5513, 457, 613, 2771, 2448, 797, 498, 291, 362, 257, 819, 9364, 2445, 337, 428, 2281, 51836], "temperature": 0.0, "avg_logprob": -0.0576611978036386, "compression_ratio": 1.7841726618705036, "no_speech_prob": 0.03620651736855507}, {"id": 630, "seek": 366488, "start": 3665.76, "end": 3670.56, "text": " you know like a neural network then you can evaluate you could sample these gradients and", "tokens": [50408, 291, 458, 411, 257, 18161, 3209, 550, 291, 393, 13059, 291, 727, 6889, 613, 2771, 2448, 293, 50648], "temperature": 0.0, "avg_logprob": -0.08914182244277583, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.000607006368227303}, {"id": 631, "seek": 366488, "start": 3670.56, "end": 3675.6, "text": " it's unbiased which is really cool that that's really important that we could get good estimates", "tokens": [50648, 309, 311, 517, 5614, 1937, 597, 307, 534, 1627, 300, 300, 311, 534, 1021, 300, 321, 727, 483, 665, 20561, 50900], "temperature": 0.0, "avg_logprob": -0.08914182244277583, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.000607006368227303}, {"id": 632, "seek": 366488, "start": 3675.6, "end": 3684.6400000000003, "text": " of the gradients and you know it works out if you don't use enough quantum or not enough layers", "tokens": [50900, 295, 264, 2771, 2448, 293, 291, 458, 309, 1985, 484, 498, 291, 500, 380, 764, 1547, 13018, 420, 406, 1547, 7914, 51352], "temperature": 0.0, "avg_logprob": -0.08914182244277583, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.000607006368227303}, {"id": 633, "seek": 366488, "start": 3684.6400000000003, "end": 3690.32, "text": " of your quantum computer or not enough complexity of your classical distribution sometimes it", "tokens": [51352, 295, 428, 13018, 3820, 420, 406, 1547, 14024, 295, 428, 13735, 7316, 2171, 309, 51636], "temperature": 0.0, "avg_logprob": -0.08914182244277583, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.000607006368227303}, {"id": 634, "seek": 369032, "start": 3690.32, "end": 3697.76, "text": " doesn't work well so for various temperatures we've tested this and i guess this is a this is", "tokens": [50364, 1177, 380, 589, 731, 370, 337, 3683, 12633, 321, 600, 8246, 341, 293, 741, 2041, 341, 307, 257, 341, 307, 50736], "temperature": 0.0, "avg_logprob": -0.07910870116891212, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.027570029720664024}, {"id": 635, "seek": 369032, "start": 3697.76, "end": 3700.96, "text": " you know there's many things you could do once you have unsupervised learning for example you", "tokens": [50736, 291, 458, 456, 311, 867, 721, 291, 727, 360, 1564, 291, 362, 2693, 12879, 24420, 2539, 337, 1365, 291, 50896], "temperature": 0.0, "avg_logprob": -0.07910870116891212, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.027570029720664024}, {"id": 636, "seek": 369032, "start": 3700.96, "end": 3707.2000000000003, "text": " could learn a compression code so here we actually applied hopefully some of you know about bosonic", "tokens": [50896, 727, 1466, 257, 19355, 3089, 370, 510, 321, 767, 6456, 4696, 512, 295, 291, 458, 466, 30641, 11630, 51208], "temperature": 0.0, "avg_logprob": -0.07910870116891212, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.027570029720664024}, {"id": 637, "seek": 369032, "start": 3707.2000000000003, "end": 3712.4, "text": " quantum computing but theoretically could be applied to other forms that are not qubits", "tokens": [51208, 13018, 15866, 457, 29400, 727, 312, 6456, 281, 661, 6422, 300, 366, 406, 421, 34010, 51468], "temperature": 0.0, "avg_logprob": -0.07910870116891212, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.027570029720664024}, {"id": 638, "seek": 369032, "start": 3712.4, "end": 3718.32, "text": " and here we learn a compression code where we could throw you know 40 percent of a harmonic chain", "tokens": [51468, 293, 510, 321, 1466, 257, 19355, 3089, 689, 321, 727, 3507, 291, 458, 3356, 3043, 295, 257, 32270, 5021, 51764], "temperature": 0.0, "avg_logprob": -0.07910870116891212, "compression_ratio": 1.8622047244094488, "no_speech_prob": 0.027570029720664024}, {"id": 639, "seek": 371832, "start": 3719.28, "end": 3725.28, "text": " in the compressed space and still reconstruct the states so this is the error matrix of the density", "tokens": [50412, 294, 264, 30353, 1901, 293, 920, 31499, 264, 4368, 370, 341, 307, 264, 6713, 8141, 295, 264, 10305, 50712], "temperature": 0.0, "avg_logprob": -0.10271271792325107, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.008844812400639057}, {"id": 640, "seek": 371832, "start": 3725.28, "end": 3731.92, "text": " matrix point seven we start seeing errors and if you throw 90 percent of stuff out things go bad", "tokens": [50712, 8141, 935, 3407, 321, 722, 2577, 13603, 293, 498, 291, 3507, 4289, 3043, 295, 1507, 484, 721, 352, 1578, 51044], "temperature": 0.0, "avg_logprob": -0.10271271792325107, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.008844812400639057}, {"id": 641, "seek": 371832, "start": 3731.92, "end": 3739.1200000000003, "text": " and there's theory that you can find the logarithm modes the modular modes of the system", "tokens": [51044, 293, 456, 311, 5261, 300, 291, 393, 915, 264, 41473, 32674, 14068, 264, 31111, 14068, 295, 264, 1185, 51404], "temperature": 0.0, "avg_logprob": -0.10271271792325107, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.008844812400639057}, {"id": 642, "seek": 371832, "start": 3739.1200000000003, "end": 3743.76, "text": " and so we checked it with theory that's why we looked at the system thank you but that's it that's", "tokens": [51404, 293, 370, 321, 10033, 309, 365, 5261, 300, 311, 983, 321, 2956, 412, 264, 1185, 1309, 291, 457, 300, 311, 309, 300, 311, 51636], "temperature": 0.0, "avg_logprob": -0.10271271792325107, "compression_ratio": 1.794392523364486, "no_speech_prob": 0.008844812400639057}, {"id": 643, "seek": 374376, "start": 3744.48, "end": 3751.76, "text": " corpse there sorry sorry what are the x and y axes on that curve again this curve on the on the", "tokens": [50400, 30324, 456, 2597, 2597, 437, 366, 264, 2031, 293, 288, 35387, 322, 300, 7605, 797, 341, 7605, 322, 264, 322, 264, 50764], "temperature": 0.0, "avg_logprob": -0.1339867717087871, "compression_ratio": 1.8724489795918366, "no_speech_prob": 0.006485321559011936}, {"id": 644, "seek": 374376, "start": 3751.76, "end": 3759.0400000000004, "text": " left oh on this side so this is the density matrix it's uh it's the discrepancy between", "tokens": [50764, 1411, 1954, 322, 341, 1252, 370, 341, 307, 264, 10305, 8141, 309, 311, 2232, 309, 311, 264, 2983, 265, 6040, 1344, 1296, 51128], "temperature": 0.0, "avg_logprob": -0.1339867717087871, "compression_ratio": 1.8724489795918366, "no_speech_prob": 0.006485321559011936}, {"id": 645, "seek": 374376, "start": 3759.6800000000003, "end": 3764.2400000000002, "text": " the so we go to compressed space it's like an auto encoder we go to compressed space", "tokens": [51160, 264, 370, 321, 352, 281, 30353, 1901, 309, 311, 411, 364, 8399, 2058, 19866, 321, 352, 281, 30353, 1901, 51388], "temperature": 0.0, "avg_logprob": -0.1339867717087871, "compression_ratio": 1.8724489795918366, "no_speech_prob": 0.006485321559011936}, {"id": 646, "seek": 374376, "start": 3764.2400000000002, "end": 3772.5600000000004, "text": " and then we throw out uh what is like so okay so we we do we learn a vqt and the latent model is a", "tokens": [51388, 293, 550, 321, 3507, 484, 2232, 437, 307, 411, 370, 1392, 370, 321, 321, 360, 321, 1466, 257, 371, 80, 83, 293, 264, 48994, 2316, 307, 257, 51804], "temperature": 0.0, "avg_logprob": -0.1339867717087871, "compression_ratio": 1.8724489795918366, "no_speech_prob": 0.006485321559011936}, {"id": 647, "seek": 377256, "start": 3772.56, "end": 3779.04, "text": " product of individual um thermal states of harmonic oscillators right and those are like", "tokens": [50364, 1674, 295, 2609, 1105, 15070, 4368, 295, 32270, 18225, 3391, 558, 293, 729, 366, 411, 50688], "temperature": 0.0, "avg_logprob": -0.10703458786010742, "compression_ratio": 1.9025641025641025, "no_speech_prob": 0.0007095743203535676}, {"id": 648, "seek": 377256, "start": 3779.04, "end": 3786.08, "text": " quantum forms of gaussians which is kind of cool and we throw out the lower entropy latent modes", "tokens": [50688, 13018, 6422, 295, 5959, 2023, 2567, 597, 307, 733, 295, 1627, 293, 321, 3507, 484, 264, 3126, 30867, 48994, 14068, 51040], "temperature": 0.0, "avg_logprob": -0.10703458786010742, "compression_ratio": 1.9025641025641025, "no_speech_prob": 0.0007095743203535676}, {"id": 649, "seek": 377256, "start": 3786.08, "end": 3792.96, "text": " okay because the entropy represents a harmonic oscillator sorry or when you say a mode you mean", "tokens": [51040, 1392, 570, 264, 30867, 8855, 257, 32270, 43859, 2597, 420, 562, 291, 584, 257, 4391, 291, 914, 51384], "temperature": 0.0, "avg_logprob": -0.10703458786010742, "compression_ratio": 1.9025641025641025, "no_speech_prob": 0.0007095743203535676}, {"id": 650, "seek": 377256, "start": 3793.6, "end": 3799.36, "text": " harmonic oscillator of a harmonic oscillator yeah so this is in this is for say a bosonic", "tokens": [51416, 32270, 43859, 295, 257, 32270, 43859, 1338, 370, 341, 307, 294, 341, 307, 337, 584, 257, 30641, 11630, 51704], "temperature": 0.0, "avg_logprob": -0.10703458786010742, "compression_ratio": 1.9025641025641025, "no_speech_prob": 0.0007095743203535676}, {"id": 651, "seek": 379936, "start": 3799.36, "end": 3804.6400000000003, "text": " continuous variable quantum computing and i did most of my time and continuous variable stuff", "tokens": [50364, 10957, 7006, 13018, 15866, 293, 741, 630, 881, 295, 452, 565, 293, 10957, 7006, 1507, 50628], "temperature": 0.0, "avg_logprob": -0.1370204228621263, "compression_ratio": 1.8359375, "no_speech_prob": 0.003944569267332554}, {"id": 652, "seek": 379936, "start": 3804.6400000000003, "end": 3811.36, "text": " before so myself as well in theoretical physics uh this is similar to a calculation of the", "tokens": [50628, 949, 370, 2059, 382, 731, 294, 20864, 10649, 2232, 341, 307, 2531, 281, 257, 17108, 295, 264, 50964], "temperature": 0.0, "avg_logprob": -0.1370204228621263, "compression_ratio": 1.8359375, "no_speech_prob": 0.003944569267332554}, {"id": 653, "seek": 379936, "start": 3811.36, "end": 3817.92, "text": " hawking effect actually um i that's a whole two hours i won't go into that but actually here's", "tokens": [50964, 33634, 5092, 1802, 767, 1105, 741, 300, 311, 257, 1379, 732, 2496, 741, 1582, 380, 352, 666, 300, 457, 767, 510, 311, 51292], "temperature": 0.0, "avg_logprob": -0.1370204228621263, "compression_ratio": 1.8359375, "no_speech_prob": 0.003944569267332554}, {"id": 654, "seek": 379936, "start": 3817.92, "end": 3822.32, "text": " the interesting thing i have this in my summer school lectures that are up and coming uh there", "tokens": [51292, 264, 1880, 551, 741, 362, 341, 294, 452, 4266, 1395, 16564, 300, 366, 493, 293, 1348, 2232, 456, 51512], "temperature": 0.0, "avg_logprob": -0.1370204228621263, "compression_ratio": 1.8359375, "no_speech_prob": 0.003944569267332554}, {"id": 655, "seek": 379936, "start": 3822.32, "end": 3826.8, "text": " are only two types of physicists those for whom all of physics is qubits and those for whom all", "tokens": [51512, 366, 787, 732, 3467, 295, 48716, 729, 337, 7101, 439, 295, 10649, 307, 421, 34010, 293, 729, 337, 7101, 439, 51736], "temperature": 0.0, "avg_logprob": -0.1370204228621263, "compression_ratio": 1.8359375, "no_speech_prob": 0.003944569267332554}, {"id": 656, "seek": 382680, "start": 3826.8, "end": 3834.4, "text": " of physics is oscillators i try to i try to uh play on both sides so uh hopefully someday", "tokens": [50364, 295, 10649, 307, 18225, 3391, 741, 853, 281, 741, 853, 281, 2232, 862, 322, 1293, 4881, 370, 2232, 4696, 19412, 50744], "temperature": 0.0, "avg_logprob": -0.09529036415947809, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0027145182248204947}, {"id": 657, "seek": 382680, "start": 3834.4, "end": 3841.44, "text": " we can have hybrid computers that'd be cool through everyone's that's right um so yeah so", "tokens": [50744, 321, 393, 362, 13051, 10807, 300, 1116, 312, 1627, 807, 1518, 311, 300, 311, 558, 1105, 370, 1338, 370, 51096], "temperature": 0.0, "avg_logprob": -0.09529036415947809, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0027145182248204947}, {"id": 658, "seek": 382680, "start": 3841.44, "end": 3848.4, "text": " we agree with theory here um i could explain how this is related to the hawking and unruh effects", "tokens": [51096, 321, 3986, 365, 5261, 510, 1105, 741, 727, 2903, 577, 341, 307, 4077, 281, 264, 33634, 5092, 293, 517, 894, 71, 5065, 51444], "temperature": 0.0, "avg_logprob": -0.09529036415947809, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0027145182248204947}, {"id": 659, "seek": 382680, "start": 3849.6800000000003, "end": 3855.04, "text": " but uh that would take some time but it's it's an interesting uh thing that quantum machine", "tokens": [51508, 457, 2232, 300, 576, 747, 512, 565, 457, 309, 311, 309, 311, 364, 1880, 2232, 551, 300, 13018, 3479, 51776], "temperature": 0.0, "avg_logprob": -0.09529036415947809, "compression_ratio": 1.662162162162162, "no_speech_prob": 0.0027145182248204947}, {"id": 660, "seek": 385504, "start": 3855.04, "end": 3861.2, "text": " learning could theoretically understand or learn an analog of the hawking or unruh effects that you", "tokens": [50364, 2539, 727, 29400, 1223, 420, 1466, 364, 16660, 295, 264, 33634, 5092, 420, 517, 894, 71, 5065, 300, 291, 50672], "temperature": 0.0, "avg_logprob": -0.1258099913597107, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00419718399643898}, {"id": 661, "seek": 385504, "start": 3862.16, "end": 3868.64, "text": " there exists a certain set of modes that an observer feels a thermal statistical", "tokens": [50720, 456, 8198, 257, 1629, 992, 295, 14068, 300, 364, 27878, 3417, 257, 15070, 22820, 51044], "temperature": 0.0, "avg_logprob": -0.1258099913597107, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00419718399643898}, {"id": 662, "seek": 385504, "start": 3869.44, "end": 3874.8, "text": " fluctuations of the vacuum so this was the ground state we plug it in and if you transform it then", "tokens": [51084, 45276, 295, 264, 14224, 370, 341, 390, 264, 2727, 1785, 321, 5452, 309, 294, 293, 498, 291, 4088, 309, 550, 51352], "temperature": 0.0, "avg_logprob": -0.1258099913597107, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00419718399643898}, {"id": 663, "seek": 385504, "start": 3874.8, "end": 3878.96, "text": " it becomes a product of thermal states and instead of Fourier modes it's like these weird", "tokens": [51352, 309, 3643, 257, 1674, 295, 15070, 4368, 293, 2602, 295, 36810, 14068, 309, 311, 411, 613, 3657, 51560], "temperature": 0.0, "avg_logprob": -0.1258099913597107, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.00419718399643898}, {"id": 664, "seek": 387896, "start": 3879.04, "end": 3886.32, "text": " squished modes uh of the lattice so it's kind of information theoretic uh eigen modes instead of", "tokens": [50368, 2339, 4729, 14068, 2232, 295, 264, 34011, 370, 309, 311, 733, 295, 1589, 14308, 299, 2232, 10446, 14068, 2602, 295, 50732], "temperature": 0.0, "avg_logprob": -0.1392192635484921, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.06367355585098267}, {"id": 665, "seek": 387896, "start": 3886.32, "end": 3891.52, "text": " you know we're used to eigen modes in physics like the resonance but here it's kind of uh", "tokens": [50732, 291, 458, 321, 434, 1143, 281, 10446, 14068, 294, 10649, 411, 264, 30944, 457, 510, 309, 311, 733, 295, 2232, 50992], "temperature": 0.0, "avg_logprob": -0.1392192635484921, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.06367355585098267}, {"id": 666, "seek": 387896, "start": 3891.52, "end": 3897.68, "text": " the resonance of of the log uh Hamiltonian which is the modular Hamiltonian and uh this brings us", "tokens": [50992, 264, 30944, 295, 295, 264, 3565, 2232, 18484, 952, 597, 307, 264, 31111, 18484, 952, 293, 2232, 341, 5607, 505, 51300], "temperature": 0.0, "avg_logprob": -0.1392192635484921, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.06367355585098267}, {"id": 667, "seek": 387896, "start": 3897.68, "end": 3905.68, "text": " actually to the end uh of the talk and uh oh i have luckily haven't exceeded too much uh so we do", "tokens": [51300, 767, 281, 264, 917, 2232, 295, 264, 751, 293, 2232, 1954, 741, 362, 22880, 2378, 380, 38026, 886, 709, 2232, 370, 321, 360, 51700], "temperature": 0.0, "avg_logprob": -0.1392192635484921, "compression_ratio": 1.8365384615384615, "no_speech_prob": 0.06367355585098267}, {"id": 668, "seek": 390568, "start": 3905.68, "end": 3912.16, "text": " have time for questions i guess but i just want to conclude i guess uh you know this is the beginning", "tokens": [50364, 362, 565, 337, 1651, 741, 2041, 457, 741, 445, 528, 281, 16886, 741, 2041, 2232, 291, 458, 341, 307, 264, 2863, 50688], "temperature": 0.0, "avg_logprob": -0.04145435411102918, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.019708218052983284}, {"id": 669, "seek": 390568, "start": 3912.16, "end": 3917.12, "text": " of a whole research program it's an exciting area and you know by starting from basics of", "tokens": [50688, 295, 257, 1379, 2132, 1461, 309, 311, 364, 4670, 1859, 293, 291, 458, 538, 2891, 490, 14688, 295, 50936], "temperature": 0.0, "avg_logprob": -0.04145435411102918, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.019708218052983284}, {"id": 670, "seek": 390568, "start": 3917.12, "end": 3921.8399999999997, "text": " information theory right we just started thinking about relative entropy and inspiring ourselves", "tokens": [50936, 1589, 5261, 558, 321, 445, 1409, 1953, 466, 4972, 30867, 293, 15883, 4175, 51172], "temperature": 0.0, "avg_logprob": -0.04145435411102918, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.019708218052983284}, {"id": 671, "seek": 390568, "start": 3921.8399999999997, "end": 3926.72, "text": " from physics we have discoveries and machine learning and hopefully now we could apply this back", "tokens": [51172, 490, 10649, 321, 362, 28400, 293, 3479, 2539, 293, 4696, 586, 321, 727, 3079, 341, 646, 51416], "temperature": 0.0, "avg_logprob": -0.04145435411102918, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.019708218052983284}, {"id": 672, "seek": 390568, "start": 3926.72, "end": 3932.0, "text": " to the physics right so it's a feedback loop between physics and machine learning and it's", "tokens": [51416, 281, 264, 10649, 558, 370, 309, 311, 257, 5824, 6367, 1296, 10649, 293, 3479, 2539, 293, 309, 311, 51680], "temperature": 0.0, "avg_logprob": -0.04145435411102918, "compression_ratio": 1.837837837837838, "no_speech_prob": 0.019708218052983284}, {"id": 673, "seek": 393200, "start": 3932.0, "end": 3939.44, "text": " that's a big part of the philosophy of our team at x uh yeah thank you yeah thank you very much um", "tokens": [50364, 300, 311, 257, 955, 644, 295, 264, 10675, 295, 527, 1469, 412, 2031, 2232, 1338, 1309, 291, 1338, 1309, 291, 588, 709, 1105, 50736], "temperature": 0.0, "avg_logprob": -0.14344813160060607, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00198730337433517}, {"id": 674, "seek": 393200, "start": 3939.44, "end": 3942.88, "text": " i think there were some questions during the talk that i didn't get to so maybe i'll", "tokens": [50736, 741, 519, 456, 645, 512, 1651, 1830, 264, 751, 300, 741, 994, 380, 483, 281, 370, 1310, 741, 603, 50908], "temperature": 0.0, "avg_logprob": -0.14344813160060607, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00198730337433517}, {"id": 675, "seek": 393200, "start": 3942.88, "end": 3949.92, "text": " gonna run up the chat here and get back to them again um okay i guess i'll just do a", "tokens": [50908, 799, 1190, 493, 264, 5081, 510, 293, 483, 646, 281, 552, 797, 1105, 1392, 741, 2041, 741, 603, 445, 360, 257, 51260], "temperature": 0.0, "avg_logprob": -0.14344813160060607, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00198730337433517}, {"id": 676, "seek": 393200, "start": 3949.92, "end": 3956.72, "text": " shout out to anntario my collaborator at waterloo uh he was google and x and jacob was instrumental", "tokens": [51260, 8043, 484, 281, 364, 580, 4912, 452, 5091, 1639, 412, 1281, 38511, 2232, 415, 390, 20742, 293, 2031, 293, 361, 326, 996, 390, 17388, 51600], "temperature": 0.0, "avg_logprob": -0.14344813160060607, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.00198730337433517}, {"id": 677, "seek": 395672, "start": 3956.72, "end": 3962.64, "text": " to a lot of the vqt and and qmhl work and uh you know did a lot of the work there uh as well", "tokens": [50364, 281, 257, 688, 295, 264, 371, 80, 83, 293, 293, 9505, 76, 71, 75, 589, 293, 2232, 291, 458, 630, 257, 688, 295, 264, 589, 456, 2232, 382, 731, 50660], "temperature": 0.0, "avg_logprob": -0.09931354702643629, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.012233120389282703}, {"id": 678, "seek": 395672, "start": 3962.64, "end": 3969.8399999999997, "text": " so big shout out to them uh but uh yeah so any questions in the chat i've seen a lot of questions", "tokens": [50660, 370, 955, 8043, 484, 281, 552, 2232, 457, 2232, 1338, 370, 604, 1651, 294, 264, 5081, 741, 600, 1612, 257, 688, 295, 1651, 51020], "temperature": 0.0, "avg_logprob": -0.09931354702643629, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.012233120389282703}, {"id": 679, "seek": 395672, "start": 3969.8399999999997, "end": 3976.08, "text": " in the chat here so let's uh let's get let's see how many how many we can answer i guess all right", "tokens": [51020, 294, 264, 5081, 510, 370, 718, 311, 2232, 718, 311, 483, 718, 311, 536, 577, 867, 577, 867, 321, 393, 1867, 741, 2041, 439, 558, 51332], "temperature": 0.0, "avg_logprob": -0.09931354702643629, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.012233120389282703}, {"id": 680, "seek": 395672, "start": 3976.08, "end": 3982.9599999999996, "text": " let's start with the latest one which is about temperature i think the question is um now is", "tokens": [51332, 718, 311, 722, 365, 264, 6792, 472, 597, 307, 466, 4292, 741, 519, 264, 1168, 307, 1105, 586, 307, 51676], "temperature": 0.0, "avg_logprob": -0.09931354702643629, "compression_ratio": 1.8817733990147782, "no_speech_prob": 0.012233120389282703}, {"id": 681, "seek": 398296, "start": 3982.96, "end": 3987.84, "text": " there a sense of critical temperature here relative to some sort of phase transition", "tokens": [50364, 456, 257, 2020, 295, 4924, 4292, 510, 4972, 281, 512, 1333, 295, 5574, 6034, 50608], "temperature": 0.0, "avg_logprob": -0.12198192246106206, "compression_ratio": 2.0448430493273544, "no_speech_prob": 0.0027140509337186813}, {"id": 682, "seek": 398296, "start": 3989.12, "end": 3993.12, "text": " the you know the temperatures where you get noisy data closely the critical temperature", "tokens": [50672, 264, 291, 458, 264, 12633, 689, 291, 483, 24518, 1412, 8185, 264, 4924, 4292, 50872], "temperature": 0.0, "avg_logprob": -0.12198192246106206, "compression_ratio": 2.0448430493273544, "no_speech_prob": 0.0027140509337186813}, {"id": 683, "seek": 398296, "start": 3993.12, "end": 3997.68, "text": " some sort of phase transition in this innovative system or is that well i don't know if we", "tokens": [50872, 512, 1333, 295, 5574, 6034, 294, 341, 12999, 1185, 420, 307, 300, 731, 741, 500, 380, 458, 498, 321, 51100], "temperature": 0.0, "avg_logprob": -0.12198192246106206, "compression_ratio": 2.0448430493273544, "no_speech_prob": 0.0027140509337186813}, {"id": 684, "seek": 398296, "start": 3997.68, "end": 4003.2, "text": " purposely chose a system where we knew there was a phase transition but we can kind of see that", "tokens": [51100, 41840, 5111, 257, 1185, 689, 321, 2586, 456, 390, 257, 5574, 6034, 457, 321, 393, 733, 295, 536, 300, 51376], "temperature": 0.0, "avg_logprob": -0.12198192246106206, "compression_ratio": 2.0448430493273544, "no_speech_prob": 0.0027140509337186813}, {"id": 685, "seek": 398296, "start": 4003.2, "end": 4009.2, "text": " there's different uh regimes where you need more entanglement or need less entanglement right so", "tokens": [51376, 456, 311, 819, 2232, 45738, 689, 291, 643, 544, 948, 656, 3054, 420, 643, 1570, 948, 656, 3054, 558, 370, 51676], "temperature": 0.0, "avg_logprob": -0.12198192246106206, "compression_ratio": 2.0448430493273544, "no_speech_prob": 0.0027140509337186813}, {"id": 686, "seek": 400920, "start": 4010.0, "end": 4016.3199999999997, "text": " so seeing how many layers you need to represent a quantum state could be like seeing a dip in that", "tokens": [50404, 370, 2577, 577, 867, 7914, 291, 643, 281, 2906, 257, 13018, 1785, 727, 312, 411, 2577, 257, 10460, 294, 300, 50720], "temperature": 0.0, "avg_logprob": -0.11016057332356771, "compression_ratio": 1.790874524714829, "no_speech_prob": 8.091933705145493e-05}, {"id": 687, "seek": 400920, "start": 4016.8799999999997, "end": 4021.68, "text": " could could be a way to detect different phases or quantum phases of matter i guess like you know", "tokens": [50748, 727, 727, 312, 257, 636, 281, 5531, 819, 18764, 420, 13018, 18764, 295, 1871, 741, 2041, 411, 291, 458, 50988], "temperature": 0.0, "avg_logprob": -0.11016057332356771, "compression_ratio": 1.790874524714829, "no_speech_prob": 8.091933705145493e-05}, {"id": 688, "seek": 400920, "start": 4021.68, "end": 4026.56, "text": " regimes of parameter space that have very strong entanglement and regimes that are you know", "tokens": [50988, 45738, 295, 13075, 1901, 300, 362, 588, 2068, 948, 656, 3054, 293, 45738, 300, 366, 291, 458, 51232], "temperature": 0.0, "avg_logprob": -0.11016057332356771, "compression_ratio": 1.790874524714829, "no_speech_prob": 8.091933705145493e-05}, {"id": 689, "seek": 400920, "start": 4026.56, "end": 4034.3199999999997, "text": " slightly you know almost trivial um but uh yeah i'm not sure if we purposely picked a system", "tokens": [51232, 4748, 291, 458, 1920, 26703, 1105, 457, 2232, 1338, 741, 478, 406, 988, 498, 321, 41840, 6183, 257, 1185, 51620], "temperature": 0.0, "avg_logprob": -0.11016057332356771, "compression_ratio": 1.790874524714829, "no_speech_prob": 8.091933705145493e-05}, {"id": 690, "seek": 400920, "start": 4034.3199999999997, "end": 4038.72, "text": " that we knew there was a phase transition we just observed this data for now but um maybe", "tokens": [51620, 300, 321, 2586, 456, 390, 257, 5574, 6034, 321, 445, 13095, 341, 1412, 337, 586, 457, 1105, 1310, 51840], "temperature": 0.0, "avg_logprob": -0.11016057332356771, "compression_ratio": 1.790874524714829, "no_speech_prob": 8.091933705145493e-05}, {"id": 691, "seek": 403872, "start": 4038.72, "end": 4047.52, "text": " something to do uh better on yeah um let's see this one uh this one is about universal estimators", "tokens": [50364, 746, 281, 360, 2232, 1101, 322, 1338, 1105, 718, 311, 536, 341, 472, 2232, 341, 472, 307, 466, 11455, 8017, 3391, 50804], "temperature": 0.0, "avg_logprob": -0.19867772069470635, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.004005699418485165}, {"id": 692, "seek": 403872, "start": 4047.52, "end": 4053.4399999999996, "text": " basically can uh q and n's the quantum neural nets be used to imitate this kind of behavior", "tokens": [50804, 1936, 393, 2232, 9505, 293, 297, 311, 264, 13018, 18161, 36170, 312, 1143, 281, 35556, 341, 733, 295, 5223, 51100], "temperature": 0.0, "avg_logprob": -0.19867772069470635, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.004005699418485165}, {"id": 693, "seek": 403872, "start": 4054.0, "end": 4059.52, "text": " i guess and we're saying you know given that three layer neural nets are regardless universal", "tokens": [51128, 741, 2041, 293, 321, 434, 1566, 291, 458, 2212, 300, 1045, 4583, 18161, 36170, 366, 10060, 11455, 51404], "temperature": 0.0, "avg_logprob": -0.19867772069470635, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.004005699418485165}, {"id": 694, "seek": 403872, "start": 4059.52, "end": 4065.68, "text": " estimators in classical machine learning yeah that's a that's a good question so i guess you know", "tokens": [51404, 8017, 3391, 294, 13735, 3479, 2539, 1338, 300, 311, 257, 300, 311, 257, 665, 1168, 370, 741, 2041, 291, 458, 51712], "temperature": 0.0, "avg_logprob": -0.19867772069470635, "compression_ratio": 1.7971698113207548, "no_speech_prob": 0.004005699418485165}, {"id": 695, "seek": 406568, "start": 4065.68, "end": 4071.52, "text": " you want if you have a universal functional approximator um then you know theoretically you", "tokens": [50364, 291, 528, 498, 291, 362, 257, 11455, 11745, 8542, 1639, 1105, 550, 291, 458, 29400, 291, 50656], "temperature": 0.0, "avg_logprob": -0.06089644055617483, "compression_ratio": 1.8743718592964824, "no_speech_prob": 0.003944218158721924}, {"id": 696, "seek": 406568, "start": 4071.52, "end": 4078.24, "text": " can have a a universal uh you know you span the space of functions that you could represent", "tokens": [50656, 393, 362, 257, 257, 11455, 2232, 291, 458, 291, 16174, 264, 1901, 295, 6828, 300, 291, 727, 2906, 50992], "temperature": 0.0, "avg_logprob": -0.06089644055617483, "compression_ratio": 1.8743718592964824, "no_speech_prob": 0.003944218158721924}, {"id": 697, "seek": 406568, "start": 4078.24, "end": 4085.2, "text": " of course any classical computation can be embedded uh if you write it out as a reversible", "tokens": [50992, 295, 1164, 604, 13735, 24903, 393, 312, 16741, 2232, 498, 291, 2464, 309, 484, 382, 257, 44788, 51340], "temperature": 0.0, "avg_logprob": -0.06089644055617483, "compression_ratio": 1.8743718592964824, "no_speech_prob": 0.003944218158721924}, {"id": 698, "seek": 406568, "start": 4085.2, "end": 4090.72, "text": " classical computation using many extra registers and you keep the whole history of the computation", "tokens": [51340, 13735, 24903, 1228, 867, 2857, 38351, 293, 291, 1066, 264, 1379, 2503, 295, 264, 24903, 51616], "temperature": 0.0, "avg_logprob": -0.06089644055617483, "compression_ratio": 1.8743718592964824, "no_speech_prob": 0.003944218158721924}, {"id": 699, "seek": 409072, "start": 4090.7999999999997, "end": 4095.3599999999997, "text": " if it's not reversible functions you can embed that right in quantum computations", "tokens": [50368, 498, 309, 311, 406, 44788, 6828, 291, 393, 12240, 300, 558, 294, 13018, 2807, 763, 50596], "temperature": 0.0, "avg_logprob": -0.09400466996796276, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.016146549955010414}, {"id": 700, "seek": 409072, "start": 4095.3599999999997, "end": 4101.36, "text": " with toffoli gates instead of hand and so on and some of work several years ago i i showed how to", "tokens": [50596, 365, 281, 602, 9384, 19792, 2602, 295, 1011, 293, 370, 322, 293, 512, 295, 589, 2940, 924, 2057, 741, 741, 4712, 577, 281, 50896], "temperature": 0.0, "avg_logprob": -0.09400466996796276, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.016146549955010414}, {"id": 701, "seek": 409072, "start": 4101.92, "end": 4106.719999999999, "text": " take you know typical classical neural networks and make quantum circuits that implement the", "tokens": [50924, 747, 291, 458, 7476, 13735, 18161, 9590, 293, 652, 13018, 26354, 300, 4445, 264, 51164], "temperature": 0.0, "avg_logprob": -0.09400466996796276, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.016146549955010414}, {"id": 702, "seek": 409072, "start": 4106.719999999999, "end": 4112.719999999999, "text": " classical neural network in superposition um so the idea is yes i think you can use quantum", "tokens": [51164, 13735, 18161, 3209, 294, 1687, 38078, 1105, 370, 264, 1558, 307, 2086, 741, 519, 291, 393, 764, 13018, 51464], "temperature": 0.0, "avg_logprob": -0.09400466996796276, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.016146549955010414}, {"id": 703, "seek": 409072, "start": 4112.719999999999, "end": 4118.8, "text": " neural networks to do the classical probabilistic machine learning components um though so far at", "tokens": [51464, 18161, 9590, 281, 360, 264, 13735, 31959, 3142, 3479, 2539, 6677, 1105, 1673, 370, 1400, 412, 51768], "temperature": 0.0, "avg_logprob": -0.09400466996796276, "compression_ratio": 1.8780487804878048, "no_speech_prob": 0.016146549955010414}, {"id": 704, "seek": 411880, "start": 4118.8, "end": 4124.08, "text": " least from uh the current state of the art of the theory it seems like quantum computers will", "tokens": [50364, 1935, 490, 2232, 264, 2190, 1785, 295, 264, 1523, 295, 264, 5261, 309, 2544, 411, 13018, 10807, 486, 50628], "temperature": 0.0, "avg_logprob": -0.08075421895736302, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0040055569261312485}, {"id": 705, "seek": 411880, "start": 4124.08, "end": 4132.400000000001, "text": " have a polynomial speedup for inference probabilistic inference similar to grover speedup uh and that of", "tokens": [50628, 362, 257, 26110, 3073, 1010, 337, 38253, 31959, 3142, 38253, 2531, 281, 4634, 331, 3073, 1010, 2232, 293, 300, 295, 51044], "temperature": 0.0, "avg_logprob": -0.08075421895736302, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0040055569261312485}, {"id": 706, "seek": 411880, "start": 4132.400000000001, "end": 4136.72, "text": " course if we're competing with extremely large classical computers will be mostly relevant when", "tokens": [51044, 1164, 498, 321, 434, 15439, 365, 4664, 2416, 13735, 10807, 486, 312, 5240, 7340, 562, 51260], "temperature": 0.0, "avg_logprob": -0.08075421895736302, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0040055569261312485}, {"id": 707, "seek": 411880, "start": 4136.72, "end": 4142.56, "text": " quantum computers are of a size comparable to the square root of our largest supercomputer", "tokens": [51260, 13018, 10807, 366, 295, 257, 2744, 25323, 281, 264, 3732, 5593, 295, 527, 6443, 36708, 51552], "temperature": 0.0, "avg_logprob": -0.08075421895736302, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.0040055569261312485}, {"id": 708, "seek": 414256, "start": 4142.8, "end": 4150.0, "text": " uh and um that is yeah that's i guess that's uh that's my answer so for now i guess the", "tokens": [50376, 2232, 293, 1105, 300, 307, 1338, 300, 311, 741, 2041, 300, 311, 2232, 300, 311, 452, 1867, 370, 337, 586, 741, 2041, 264, 50736], "temperature": 0.0, "avg_logprob": -0.10442553957303365, "compression_ratio": 1.9424778761061947, "no_speech_prob": 0.004680818412452936}, {"id": 709, "seek": 414256, "start": 4150.0, "end": 4154.400000000001, "text": " most practical approach is to use classical algorithms and classical computers for the", "tokens": [50736, 881, 8496, 3109, 307, 281, 764, 13735, 14642, 293, 13735, 10807, 337, 264, 50956], "temperature": 0.0, "avg_logprob": -0.10442553957303365, "compression_ratio": 1.9424778761061947, "no_speech_prob": 0.004680818412452936}, {"id": 710, "seek": 414256, "start": 4154.400000000001, "end": 4159.68, "text": " classical component and use quantum computers for the truly quantum component which is the unitary", "tokens": [50956, 13735, 6542, 293, 764, 13018, 10807, 337, 264, 4908, 13018, 6542, 597, 307, 264, 517, 4109, 51220], "temperature": 0.0, "avg_logprob": -0.10442553957303365, "compression_ratio": 1.9424778761061947, "no_speech_prob": 0.004680818412452936}, {"id": 711, "seek": 414256, "start": 4160.8, "end": 4164.0, "text": " that seems like a very nice sensible thing there is a question this one is", "tokens": [51276, 300, 2544, 411, 257, 588, 1481, 25380, 551, 456, 307, 257, 1168, 341, 472, 307, 51436], "temperature": 0.0, "avg_logprob": -0.10442553957303365, "compression_ratio": 1.9424778761061947, "no_speech_prob": 0.004680818412452936}, {"id": 712, "seek": 414256, "start": 4164.96, "end": 4169.76, "text": " interesting i think it's more of an opinion question maybe um we know that quantum Fourier", "tokens": [51484, 1880, 741, 519, 309, 311, 544, 295, 364, 4800, 1168, 1310, 1105, 321, 458, 300, 13018, 36810, 51724], "temperature": 0.0, "avg_logprob": -0.10442553957303365, "compression_ratio": 1.9424778761061947, "no_speech_prob": 0.004680818412452936}, {"id": 713, "seek": 416976, "start": 4169.76, "end": 4176.0, "text": " transforms very t and usually digital computing um does it have a role in machine learning is it", "tokens": [50364, 35592, 588, 256, 293, 2673, 4562, 15866, 1105, 775, 309, 362, 257, 3090, 294, 3479, 2539, 307, 309, 50676], "temperature": 0.0, "avg_logprob": -0.23212291032840043, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0024332019966095686}, {"id": 714, "seek": 416976, "start": 4176.0, "end": 4181.52, "text": " similar in here according to what is the relationship with advantage or the Fourier", "tokens": [50676, 2531, 294, 510, 4650, 281, 437, 307, 264, 2480, 365, 5002, 420, 264, 36810, 50952], "temperature": 0.0, "avg_logprob": -0.23212291032840043, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0024332019966095686}, {"id": 715, "seek": 416976, "start": 4181.52, "end": 4190.56, "text": " transform right so i guess here we parametrized our quantum neural network uh as a general uh", "tokens": [50952, 4088, 558, 370, 741, 2041, 510, 321, 6220, 302, 470, 11312, 527, 13018, 18161, 3209, 2232, 382, 257, 2674, 2232, 51404], "temperature": 0.0, "avg_logprob": -0.23212291032840043, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0024332019966095686}, {"id": 716, "seek": 416976, "start": 4190.56, "end": 4198.24, "text": " bosonic um what is called book all above or Gaussian transformation and the discrete Fourier", "tokens": [51404, 30641, 11630, 1105, 437, 307, 1219, 1446, 439, 3673, 420, 39148, 9887, 293, 264, 27706, 36810, 51788], "temperature": 0.0, "avg_logprob": -0.23212291032840043, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.0024332019966095686}, {"id": 717, "seek": 419824, "start": 4198.24, "end": 4203.92, "text": " transform is a subset of such transformations and here we we learned these transformations so", "tokens": [50364, 4088, 307, 257, 25993, 295, 1270, 34852, 293, 510, 321, 321, 3264, 613, 34852, 370, 50648], "temperature": 0.0, "avg_logprob": -0.08875837780180432, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.0025894527789205313}, {"id": 718, "seek": 419824, "start": 4204.639999999999, "end": 4210.08, "text": " uh technically uh if we fed the whole system and we asked it to find the eigen modes and if", "tokens": [50684, 2232, 12120, 2232, 498, 321, 4636, 264, 1379, 1185, 293, 321, 2351, 309, 281, 915, 264, 10446, 14068, 293, 498, 50956], "temperature": 0.0, "avg_logprob": -0.08875837780180432, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.0025894527789205313}, {"id": 719, "seek": 419824, "start": 4210.08, "end": 4215.44, "text": " we had a thermal state of this system say via vqt and then we fed it to quantum modular", "tokens": [50956, 321, 632, 257, 15070, 1785, 295, 341, 1185, 584, 5766, 371, 80, 83, 293, 550, 321, 4636, 309, 281, 13018, 31111, 51224], "temperature": 0.0, "avg_logprob": -0.08875837780180432, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.0025894527789205313}, {"id": 720, "seek": 419824, "start": 4215.44, "end": 4220.32, "text": " Hamiltonian learning these modes would be the Fourier modes because we know the eigen modes of", "tokens": [51224, 18484, 952, 2539, 613, 14068, 576, 312, 264, 36810, 14068, 570, 321, 458, 264, 10446, 14068, 295, 51468], "temperature": 0.0, "avg_logprob": -0.08875837780180432, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.0025894527789205313}, {"id": 721, "seek": 419824, "start": 4220.32, "end": 4227.76, "text": " of this Hamiltonian right we know how to decompose this this Hamiltonian uh into a sum of individual", "tokens": [51468, 295, 341, 18484, 952, 558, 321, 458, 577, 281, 22867, 541, 341, 341, 18484, 952, 2232, 666, 257, 2408, 295, 2609, 51840], "temperature": 0.0, "avg_logprob": -0.08875837780180432, "compression_ratio": 1.978902953586498, "no_speech_prob": 0.0025894527789205313}, {"id": 722, "seek": 422824, "start": 4228.48, "end": 4234.639999999999, "text": " uh you know number operators um and it's the same you know finding this book all above", "tokens": [50376, 2232, 291, 458, 1230, 19077, 1105, 293, 309, 311, 264, 912, 291, 458, 5006, 341, 1446, 439, 3673, 50684], "temperature": 0.0, "avg_logprob": -0.14116462794217197, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0006359892431646585}, {"id": 723, "seek": 422824, "start": 4234.639999999999, "end": 4240.48, "text": " transformation is what i mean by it's related to the unrefect uh calculation qft in curved", "tokens": [50684, 9887, 307, 437, 741, 914, 538, 309, 311, 4077, 281, 264, 20584, 1836, 2232, 17108, 9505, 844, 294, 24991, 50976], "temperature": 0.0, "avg_logprob": -0.14116462794217197, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0006359892431646585}, {"id": 724, "seek": 422824, "start": 4240.48, "end": 4245.5199999999995, "text": " spacetime i know there are some chat messages that were doubting that but uh i did my masters", "tokens": [50976, 39404, 9764, 741, 458, 456, 366, 512, 5081, 7897, 300, 645, 10831, 783, 300, 457, 2232, 741, 630, 452, 19294, 51228], "temperature": 0.0, "avg_logprob": -0.14116462794217197, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0006359892431646585}, {"id": 725, "seek": 422824, "start": 4245.5199999999995, "end": 4252.8, "text": " in quantum field theory in curved spacetime so you can trust me on that one uh but uh great so", "tokens": [51228, 294, 13018, 2519, 5261, 294, 24991, 39404, 9764, 370, 291, 393, 3361, 385, 322, 300, 472, 2232, 457, 2232, 869, 370, 51592], "temperature": 0.0, "avg_logprob": -0.14116462794217197, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.0006359892431646585}, {"id": 726, "seek": 425280, "start": 4253.4400000000005, "end": 4262.96, "text": " and maybe for the final question here um and taking the extra time um if we want to do research", "tokens": [50396, 293, 1310, 337, 264, 2572, 1168, 510, 1105, 293, 1940, 264, 2857, 565, 1105, 498, 321, 528, 281, 360, 2132, 50872], "temperature": 0.0, "avg_logprob": -0.134234592832368, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.008057337254285812}, {"id": 727, "seek": 425280, "start": 4262.96, "end": 4266.88, "text": " in this field where should we start or what should be out of the direction of research", "tokens": [50872, 294, 341, 2519, 689, 820, 321, 722, 420, 437, 820, 312, 484, 295, 264, 3513, 295, 2132, 51068], "temperature": 0.0, "avg_logprob": -0.134234592832368, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.008057337254285812}, {"id": 728, "seek": 425280, "start": 4268.08, "end": 4274.08, "text": " right i mean that's a that's a good question i guess uh you yourself have gone through this", "tokens": [51128, 558, 741, 914, 300, 311, 257, 300, 311, 257, 665, 1168, 741, 2041, 2232, 291, 1803, 362, 2780, 807, 341, 51428], "temperature": 0.0, "avg_logprob": -0.134234592832368, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.008057337254285812}, {"id": 729, "seek": 425280, "start": 4274.08, "end": 4280.56, "text": " position not you know four years ago i think you mentioned right right right right so i guess in my", "tokens": [51428, 2535, 406, 291, 458, 1451, 924, 2057, 741, 519, 291, 2835, 558, 558, 558, 558, 370, 741, 2041, 294, 452, 51752], "temperature": 0.0, "avg_logprob": -0.134234592832368, "compression_ratio": 1.8155339805825244, "no_speech_prob": 0.008057337254285812}, {"id": 730, "seek": 428056, "start": 4280.56, "end": 4287.84, "text": " case i started you know i started with uh the open you know source uh massively online courses mooks", "tokens": [50364, 1389, 741, 1409, 291, 458, 741, 1409, 365, 2232, 264, 1269, 291, 458, 4009, 2232, 29379, 2950, 7712, 705, 25500, 50728], "temperature": 0.0, "avg_logprob": -0.15952556889231612, "compression_ratio": 1.91015625, "no_speech_prob": 0.006793156266212463}, {"id": 731, "seek": 428056, "start": 4287.84, "end": 4292.240000000001, "text": " i just listened to that listened to a few of them and then i progressed to i wish i had all my", "tokens": [50728, 741, 445, 13207, 281, 300, 13207, 281, 257, 1326, 295, 552, 293, 550, 741, 36789, 281, 741, 3172, 741, 632, 439, 452, 50948], "temperature": 0.0, "avg_logprob": -0.15952556889231612, "compression_ratio": 1.91015625, "no_speech_prob": 0.006793156266212463}, {"id": 732, "seek": 428056, "start": 4292.240000000001, "end": 4297.360000000001, "text": " textbooks here but uh they're they're back there but uh the good fellow the in good fellows textbook", "tokens": [50948, 33587, 510, 457, 2232, 436, 434, 436, 434, 646, 456, 457, 2232, 264, 665, 7177, 264, 294, 665, 35595, 25591, 51204], "temperature": 0.0, "avg_logprob": -0.15952556889231612, "compression_ratio": 1.91015625, "no_speech_prob": 0.006793156266212463}, {"id": 733, "seek": 428056, "start": 4297.360000000001, "end": 4304.400000000001, "text": " um inventor of gans and then uh murphy kevin murphy uh a goobler uh he did uh what i called", "tokens": [51204, 1105, 41593, 295, 290, 599, 293, 550, 2232, 5257, 15680, 803, 4796, 5257, 15680, 2232, 257, 352, 996, 1918, 2232, 415, 630, 2232, 437, 741, 1219, 51556], "temperature": 0.0, "avg_logprob": -0.15952556889231612, "compression_ratio": 1.91015625, "no_speech_prob": 0.006793156266212463}, {"id": 734, "seek": 428056, "start": 4304.400000000001, "end": 4310.0, "text": " a kind of the nielson and schwang or the bible of probabilistic machine learning and i think there's", "tokens": [51556, 257, 733, 295, 264, 297, 1187, 3015, 293, 17932, 656, 420, 264, 34956, 295, 31959, 3142, 3479, 2539, 293, 741, 519, 456, 311, 51836], "temperature": 0.0, "avg_logprob": -0.15952556889231612, "compression_ratio": 1.91015625, "no_speech_prob": 0.006793156266212463}, {"id": 735, "seek": 431000, "start": 4310.4, "end": 4314.72, "text": " mckay there's information theory for machine learning that's if you want the textbook routes", "tokens": [50384, 275, 547, 320, 456, 311, 1589, 5261, 337, 3479, 2539, 300, 311, 498, 291, 528, 264, 25591, 18242, 50600], "temperature": 0.0, "avg_logprob": -0.12308959746628664, "compression_ratio": 1.7, "no_speech_prob": 0.0037618286442011595}, {"id": 736, "seek": 431000, "start": 4315.76, "end": 4321.28, "text": " otherwise i think with time as the field stabilizes i guess because it's been moving so fast everybody", "tokens": [50652, 5911, 741, 519, 365, 565, 382, 264, 2519, 11652, 5660, 741, 2041, 570, 309, 311, 668, 2684, 370, 2370, 2201, 50928], "temperature": 0.0, "avg_logprob": -0.12308959746628664, "compression_ratio": 1.7, "no_speech_prob": 0.0037618286442011595}, {"id": 737, "seek": 431000, "start": 4321.28, "end": 4327.28, "text": " who's involved in it is just cranking out papers rather than creating coursework uh there will be", "tokens": [50928, 567, 311, 3288, 294, 309, 307, 445, 21263, 278, 484, 10577, 2831, 813, 4084, 1164, 1902, 2232, 456, 486, 312, 51228], "temperature": 0.0, "avg_logprob": -0.12308959746628664, "compression_ratio": 1.7, "no_speech_prob": 0.0037618286442011595}, {"id": 738, "seek": 431000, "start": 4327.28, "end": 4334.72, "text": " coursework um i could link a you waterloo course uh that i gave some guest guest lectures at that", "tokens": [51228, 1164, 1902, 1105, 741, 727, 2113, 257, 291, 1281, 38511, 1164, 2232, 300, 741, 2729, 512, 8341, 8341, 16564, 412, 300, 51600], "temperature": 0.0, "avg_logprob": -0.12308959746628664, "compression_ratio": 1.7, "no_speech_prob": 0.0037618286442011595}, {"id": 739, "seek": 433472, "start": 4335.68, "end": 4341.84, "text": " that featured some quantum machine learning uh but uh overall i would say it's important to", "tokens": [50412, 300, 13822, 512, 13018, 3479, 2539, 2232, 457, 2232, 4787, 741, 576, 584, 309, 311, 1021, 281, 50720], "temperature": 0.0, "avg_logprob": -0.09060846629895662, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.03675393760204315}, {"id": 740, "seek": 433472, "start": 4341.84, "end": 4345.12, "text": " understand the theory of classical machine learning at the fundamental level because", "tokens": [50720, 1223, 264, 5261, 295, 13735, 3479, 2539, 412, 264, 8088, 1496, 570, 50884], "temperature": 0.0, "avg_logprob": -0.09060846629895662, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.03675393760204315}, {"id": 741, "seek": 433472, "start": 4346.08, "end": 4351.92, "text": " you know similar to hardware engineering we're at the fundamental level of engineering a new", "tokens": [50932, 291, 458, 2531, 281, 8837, 7043, 321, 434, 412, 264, 8088, 1496, 295, 7043, 257, 777, 51224], "temperature": 0.0, "avg_logprob": -0.09060846629895662, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.03675393760204315}, {"id": 742, "seek": 433472, "start": 4351.92, "end": 4357.280000000001, "text": " computing stack so on the theory side we're re-engineering a whole algorithm stack so we", "tokens": [51224, 15866, 8630, 370, 322, 264, 5261, 1252, 321, 434, 319, 12, 25609, 1794, 257, 1379, 9284, 8630, 370, 321, 51492], "temperature": 0.0, "avg_logprob": -0.09060846629895662, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.03675393760204315}, {"id": 743, "seek": 433472, "start": 4357.280000000001, "end": 4362.400000000001, "text": " got to start again from first principle so you know you have to trace back to like papers from", "tokens": [51492, 658, 281, 722, 797, 490, 700, 8665, 370, 291, 458, 291, 362, 281, 13508, 646, 281, 411, 10577, 490, 51748], "temperature": 0.0, "avg_logprob": -0.09060846629895662, "compression_ratio": 1.8565573770491803, "no_speech_prob": 0.03675393760204315}, {"id": 744, "seek": 436240, "start": 4362.4, "end": 4367.04, "text": " the 80s of machine learning and and the fundamentals and then and then work your way back to the", "tokens": [50364, 264, 4688, 82, 295, 3479, 2539, 293, 293, 264, 29505, 293, 550, 293, 550, 589, 428, 636, 646, 281, 264, 50596], "temperature": 0.0, "avg_logprob": -0.10227794647216797, "compression_ratio": 1.9319148936170212, "no_speech_prob": 0.00609486224129796}, {"id": 745, "seek": 436240, "start": 4367.04, "end": 4373.12, "text": " modern uh modern thing so i would say the the modern ml stuff is flashy and and and fun to", "tokens": [50596, 4363, 2232, 4363, 551, 370, 741, 576, 584, 264, 264, 4363, 23271, 1507, 307, 47873, 293, 293, 293, 1019, 281, 50900], "temperature": 0.0, "avg_logprob": -0.10227794647216797, "compression_ratio": 1.9319148936170212, "no_speech_prob": 0.00609486224129796}, {"id": 746, "seek": 436240, "start": 4373.12, "end": 4378.4, "text": " stay up to date but i would say you know take the time go back to the the core old literature", "tokens": [50900, 1754, 493, 281, 4002, 457, 741, 576, 584, 291, 458, 747, 264, 565, 352, 646, 281, 264, 264, 4965, 1331, 10394, 51164], "temperature": 0.0, "avg_logprob": -0.10227794647216797, "compression_ratio": 1.9319148936170212, "no_speech_prob": 0.00609486224129796}, {"id": 747, "seek": 436240, "start": 4378.4, "end": 4385.5199999999995, "text": " you know the foundations so um yeah uh mooks and then textbooks is the way to go that's", "tokens": [51164, 291, 458, 264, 22467, 370, 1105, 1338, 2232, 705, 25500, 293, 550, 33587, 307, 264, 636, 281, 352, 300, 311, 51520], "temperature": 0.0, "avg_logprob": -0.10227794647216797, "compression_ratio": 1.9319148936170212, "no_speech_prob": 0.00609486224129796}, {"id": 748, "seek": 436240, "start": 4385.5199999999995, "end": 4391.759999999999, "text": " that's what i did and here i am so uh and then maybe i can just add that now they're", "tokens": [51520, 300, 311, 437, 741, 630, 293, 510, 741, 669, 370, 2232, 293, 550, 1310, 741, 393, 445, 909, 300, 586, 436, 434, 51832], "temperature": 0.0, "avg_logprob": -0.10227794647216797, "compression_ratio": 1.9319148936170212, "no_speech_prob": 0.00609486224129796}, {"id": 749, "seek": 439176, "start": 4391.76, "end": 4397.280000000001, "text": " summer school classes coming online so i mentioned the quantum information kiscuit one i think there's", "tokens": [50364, 4266, 1395, 5359, 1348, 2950, 370, 741, 2835, 264, 13018, 1589, 350, 5606, 1983, 472, 741, 519, 456, 311, 50640], "temperature": 0.0, "avg_logprob": -0.1682928458027456, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.003218849189579487}, {"id": 750, "seek": 439176, "start": 4398.08, "end": 4402.96, "text": " a touching of ml and things like that in the last two lectures in quantum chemistry vqe's", "tokens": [50680, 257, 11175, 295, 23271, 293, 721, 411, 300, 294, 264, 1036, 732, 16564, 294, 13018, 12558, 371, 80, 68, 311, 50924], "temperature": 0.0, "avg_logprob": -0.1682928458027456, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.003218849189579487}, {"id": 751, "seek": 439176, "start": 4402.96, "end": 4409.04, "text": " definitely on there so fantastic anyone interested in addition to to what you said you know we can", "tokens": [50924, 2138, 322, 456, 370, 5456, 2878, 3102, 294, 4500, 281, 281, 437, 291, 848, 291, 458, 321, 393, 51228], "temperature": 0.0, "avg_logprob": -0.1682928458027456, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.003218849189579487}, {"id": 752, "seek": 439176, "start": 4409.04, "end": 4416.16, "text": " add that um so i think it is that time that i get thank you again and thank the listeners", "tokens": [51228, 909, 300, 1105, 370, 741, 519, 309, 307, 300, 565, 300, 741, 483, 1309, 291, 797, 293, 1309, 264, 23274, 51584], "temperature": 0.0, "avg_logprob": -0.1682928458027456, "compression_ratio": 1.7557603686635945, "no_speech_prob": 0.003218849189579487}, {"id": 753, "seek": 441616, "start": 4416.16, "end": 4423.04, "text": " for joining the quantum live seminar series uh we're back this friday um i will mention next", "tokens": [50364, 337, 5549, 264, 13018, 1621, 29235, 2638, 2232, 321, 434, 646, 341, 431, 4708, 1105, 741, 486, 2152, 958, 50708], "temperature": 0.0, "avg_logprob": -0.31877626997701236, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.14570289850234985}, {"id": 754, "seek": 441616, "start": 4423.04, "end": 4428.24, "text": " week we're back so this at the same time uh continuing with the talk by antony miss capo from", "tokens": [50708, 1243, 321, 434, 646, 370, 341, 412, 264, 912, 565, 2232, 9289, 365, 264, 751, 538, 364, 1756, 88, 1713, 1410, 78, 490, 50968], "temperature": 0.0, "avg_logprob": -0.31877626997701236, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.14570289850234985}, {"id": 755, "seek": 441616, "start": 4428.24, "end": 4435.44, "text": " idea on quantum chemistry we could talk about variation of quantum eigen solvers q a o and", "tokens": [50968, 1558, 322, 13018, 12558, 321, 727, 751, 466, 12990, 295, 13018, 10446, 1404, 840, 9505, 257, 277, 293, 51328], "temperature": 0.0, "avg_logprob": -0.31877626997701236, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.14570289850234985}, {"id": 756, "seek": 441616, "start": 4435.44, "end": 4440.08, "text": " things like that on chemistry things so that will be a very nice uh follow up to your talk", "tokens": [51328, 721, 411, 300, 322, 12558, 721, 370, 300, 486, 312, 257, 588, 1481, 2232, 1524, 493, 281, 428, 751, 51560], "temperature": 0.0, "avg_logprob": -0.31877626997701236, "compression_ratio": 1.7116279069767442, "no_speech_prob": 0.14570289850234985}, {"id": 757, "seek": 444008, "start": 4440.88, "end": 4448.08, "text": " gion and uh thank you thank you for inviting me it's uh it's been an honor and uh hopefully the", "tokens": [50404, 290, 313, 293, 2232, 1309, 291, 1309, 291, 337, 18202, 385, 309, 311, 2232, 309, 311, 668, 364, 5968, 293, 2232, 4696, 264, 50764], "temperature": 0.0, "avg_logprob": -0.17375020230754037, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.01940063200891018}, {"id": 758, "seek": 444008, "start": 4449.04, "end": 4454.24, "text": " quantum community is interested in quantum machine learning now uh so i've done my job", "tokens": [50812, 13018, 1768, 307, 3102, 294, 13018, 3479, 2539, 586, 2232, 370, 741, 600, 1096, 452, 1691, 51072], "temperature": 0.0, "avg_logprob": -0.17375020230754037, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.01940063200891018}, {"id": 759, "seek": 444008, "start": 4454.24, "end": 4464.08, "text": " there all right thank you so much follow gion on twitter quantum bird that's right all right", "tokens": [51072, 456, 439, 558, 1309, 291, 370, 709, 1524, 290, 313, 322, 21439, 13018, 5255, 300, 311, 558, 439, 558, 51564], "temperature": 0.0, "avg_logprob": -0.17375020230754037, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.01940063200891018}, {"id": 760, "seek": 444008, "start": 4464.08, "end": 4469.92, "text": " so any final words and otherwise thank you and we'll see you next week uh that's it for me", "tokens": [51564, 370, 604, 2572, 2283, 293, 5911, 1309, 291, 293, 321, 603, 536, 291, 958, 1243, 2232, 300, 311, 309, 337, 385, 51856], "temperature": 0.0, "avg_logprob": -0.17375020230754037, "compression_ratio": 1.7853658536585366, "no_speech_prob": 0.01940063200891018}, {"id": 761, "seek": 446992, "start": 4469.92, "end": 4475.04, "text": " thanks again for tuning in and uh stay home stay safe everyone and uh thank you all right", "tokens": [50364, 3231, 797, 337, 15164, 294, 293, 2232, 1754, 1280, 1754, 3273, 1518, 293, 2232, 1309, 291, 439, 558, 50620], "temperature": 0.0, "avg_logprob": -0.17521922211898, "compression_ratio": 1.3909090909090909, "no_speech_prob": 0.004677367862313986}, {"id": 762, "seek": 446992, "start": 4476.0, "end": 4479.68, "text": " it was a pleasure gion we'll see you soon guys take care cheers", "tokens": [50668, 309, 390, 257, 6834, 290, 313, 321, 603, 536, 291, 2321, 1074, 747, 1127, 15301, 50852], "temperature": 0.0, "avg_logprob": -0.17521922211898, "compression_ratio": 1.3909090909090909, "no_speech_prob": 0.004677367862313986}], "language": "en"}