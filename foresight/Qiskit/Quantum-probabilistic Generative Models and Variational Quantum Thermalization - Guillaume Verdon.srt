1
00:00:00,000 --> 00:00:06,120
global digital summer school on quantum computing and I see we have reposted

2
00:00:06,120 --> 00:00:11,800
the link all right so oh Sausalito California that's where it's very close

3
00:00:11,800 --> 00:00:16,240
to where I grew up now we are thrilled to roll out the latest episode of the

4
00:00:16,240 --> 00:00:20,720
quantum seminar series dedicated to the research and academic communities this

5
00:00:20,720 --> 00:00:27,000
this seminar takes place every Friday at noon Eastern time right now at this hour

6
00:00:27,000 --> 00:00:31,600
on the Kiskin YouTube channel and I'm delighted to see so many of you already

7
00:00:31,600 --> 00:00:36,320
tuned in I'm your host like a minute from IBM quantum research and today I have

8
00:00:36,320 --> 00:00:42,760
privilege of hosting young we're done from Google X and and also from the

9
00:00:42,760 --> 00:00:46,200
Institute of quantum computing at Waterloo young will present some very

10
00:00:46,200 --> 00:00:51,960
nice results and hello young how are you today hi is that this let go happy to

11
00:00:51,960 --> 00:00:56,800
be here yeah it's a pleasure to see you last time we met I think it was right

12
00:00:56,800 --> 00:01:01,280
after March meeting was cancelled and we were at March meeting so I'm glad

13
00:01:01,280 --> 00:01:06,360
that we can continue the discussions and have everyone join us today on the call

14
00:01:06,360 --> 00:01:11,080
that's right the science goes on to multistimes but that's right indeed

15
00:01:11,080 --> 00:01:15,760
indeed I hope you've been doing well again in addition to being a social

16
00:01:15,760 --> 00:01:19,240
media celebrity is a human though is a PhD candidate at the University of

17
00:01:19,240 --> 00:01:23,320
Waterloo at the Institute of quantum computing I had a very nice visit there

18
00:01:23,320 --> 00:01:28,960
recently so great place he is also a research scientist at X Google X or

19
00:01:28,960 --> 00:01:35,020
alphabets research and development lab before this Guillaume briefly worked at

20
00:01:35,020 --> 00:01:39,200
Google AI quantum and was one of the co-founders of TensorFlow and project he

21
00:01:39,200 --> 00:01:42,680
holds a master's in math and quantum input from you Waterloo as well as in

22
00:01:42,680 --> 00:01:48,920
honors math and physics degree from a given and university and I think we have

23
00:01:48,920 --> 00:01:52,040
some very interesting questions to discuss today I think as many of you know

24
00:01:52,080 --> 00:01:55,080
quantum computing operates in an exponential space so how is classical

25
00:01:55,080 --> 00:01:59,360
machine learning and learning classical distribution for instance which also

26
00:01:59,360 --> 00:02:05,120
operates in an exponential space mesh how can we learn new and interesting

27
00:02:05,120 --> 00:02:08,760
correlations and work with not just pure states but non pure states and these

28
00:02:08,760 --> 00:02:13,280
are just some of the things Guillaume will tell us about today and maybe as I

29
00:02:13,280 --> 00:02:17,880
advertise your talk and describe the format of Guillaume maybe you can pull

30
00:02:17,880 --> 00:02:24,640
up your slides yep the talk format is the usual you can ask questions in the

31
00:02:24,640 --> 00:02:30,320
comment sidebar box on the right hand side usually or below and I will triage

32
00:02:30,320 --> 00:02:37,600
those questions and ask Guillaume in real time so Q&A is during and after and I

33
00:02:37,600 --> 00:02:43,600
think it's time we get started so it's my pleasure to turn over to you well thank

34
00:02:43,600 --> 00:02:47,600
you for the introductions Lacko and thank you to the invitation and it's very

35
00:02:47,600 --> 00:02:53,640
nice to get to speak to the whole quantum community here really enjoy

36
00:02:53,640 --> 00:02:59,240
watching these seminars so it's my honor to get to talk in one of these so

37
00:02:59,240 --> 00:03:04,320
today I guess I'll be talking broadly about quantum machine learning and some

38
00:03:04,320 --> 00:03:08,720
context comparing it to classical machine learning and deep learning and then

39
00:03:08,720 --> 00:03:16,760
getting into some recent work from various internships and during my PhD on

40
00:03:17,320 --> 00:03:21,840
taking inspiration from classical machine learning to create new types of

41
00:03:21,840 --> 00:03:29,960
quantum models and algorithms so as Lacko mentioned often in quantum machine

42
00:03:29,960 --> 00:03:36,240
learning there is this conception that if we go to if we use a quantum

43
00:03:36,240 --> 00:03:41,440
computer sense we're operating in a exponentially large space and and thus

44
00:03:41,720 --> 00:03:47,120
we should get exponential amounts of power of machine learning power right

45
00:03:47,120 --> 00:03:51,720
but that is that is somewhat of a misconception because for very long

46
00:03:51,720 --> 00:03:56,160
time classical computers and analog classical electronics have been able to

47
00:03:56,160 --> 00:04:02,280
do probabilistic computing right and as we know quantum theory is kind of an

48
00:04:02,280 --> 00:04:07,840
extension or generalization of probability theory to include complex

49
00:04:07,840 --> 00:04:14,200
numbers known as wave functions right and probabilities are obtained from wave

50
00:04:14,200 --> 00:04:19,080
functions by taking the amplitude squared or the absolute value squared of

51
00:04:19,080 --> 00:04:23,120
various these complex numbers and from that we get the probability of various

52
00:04:23,120 --> 00:04:28,640
outcomes right that is known as the Born Rule right but you know on the

53
00:04:28,640 --> 00:04:35,480
classical side we can have mixtures of zero and one right zero or one in a

54
00:04:35,520 --> 00:04:39,720
different combination whereas on the quantum side we have super positions it's

55
00:04:39,720 --> 00:04:45,160
not zero and one it's not zero or one it's a super position with certain values

56
00:04:45,160 --> 00:04:53,320
of complex numbers right so overall I guess the theme of the talk is to you

57
00:04:53,320 --> 00:04:58,040
know take inspiration from classical probability theory and take inspiration

58
00:04:58,040 --> 00:05:02,480
from a subset of machine learning called probabilistic machine learning to come

59
00:05:02,480 --> 00:05:07,440
up with new quantum models because the theories are very much analogous and can

60
00:05:07,440 --> 00:05:14,760
in fact be hybridized as we will see so if you have you know n-pro bits

61
00:05:14,760 --> 00:05:20,440
probabilistic bits they also operate in a space that is exponential right you

62
00:05:20,440 --> 00:05:24,640
have a probability distribution over the space of bit strings and you can have a

63
00:05:24,640 --> 00:05:30,280
mixture of two to the n possible bit strings right and there's many as we'll

64
00:05:30,320 --> 00:05:35,960
see machine learning models that are made to represent such distributions or

65
00:05:35,960 --> 00:05:41,000
generate such distributions and on the quantum side you know for pure states

66
00:05:41,000 --> 00:05:47,880
at least they're written as a wave function of the sort and have two to the

67
00:05:47,880 --> 00:05:52,680
n complex numbers for n qubits right so there's a lot of similarities right

68
00:05:52,680 --> 00:05:56,400
and there's a difference the complex number and the real number that's

69
00:05:56,440 --> 00:06:02,520
important that gives in a sense quantum computing its power over classical

70
00:06:02,520 --> 00:06:08,720
algorithms but instead of you know having this constant competition I guess

71
00:06:08,720 --> 00:06:13,840
between classical computing and quantum computing you know the philosophy at

72
00:06:13,840 --> 00:06:17,760
least of my research is to try to leverage as much classical computing as

73
00:06:17,760 --> 00:06:23,360
possible and including probabilistic computing and hybridize it with quantum

74
00:06:23,360 --> 00:06:27,200
computation in a sense we want to leverage quantum computers for what

75
00:06:27,200 --> 00:06:31,600
they're very they're the best at right and we want to add this such that there's

76
00:06:31,600 --> 00:06:36,280
a value add by using quantum computers hybridized with classical computing so

77
00:06:36,280 --> 00:06:42,720
this is a philosophy and it's a research and you know there are various schools

78
00:06:42,720 --> 00:06:48,240
of thought in quantum computing and this is the one I guess I'm vouching for so

79
00:06:49,040 --> 00:06:54,480
you know what what actually gives quantum computers their power right well you

80
00:06:54,480 --> 00:07:00,440
know there's been various demonstrations that sampling from a unitary circuit that

81
00:07:00,440 --> 00:07:05,600
is quite deep and has a large space-time volume a unitary quantum circuit is

82
00:07:05,600 --> 00:07:09,960
quite difficult for classical computers right one has to do Feynman paths or

83
00:07:09,960 --> 00:07:15,720
tensor networks and what not and you know the difficulty scales exponentially

84
00:07:16,200 --> 00:07:22,200
asymptotically with the volume of space-time right so that that we know

85
00:07:22,200 --> 00:07:27,960
that's the power of quantum computers is to sample from such circuits so how do

86
00:07:27,960 --> 00:07:33,160
we incorporate this exact thing sampling from unitaries and integrate it with

87
00:07:33,160 --> 00:07:37,720
the capabilities of classical modern classical machine learning to obtain

88
00:07:37,720 --> 00:07:44,080
you know something more powerful than either either piece individually right

89
00:07:44,120 --> 00:07:47,080
so quantum computers are becoming more powerful to the point of being

90
00:07:47,080 --> 00:07:52,760
unsimulatable you know I won't get into whether the boundary has been crossed

91
00:07:52,760 --> 00:07:58,520
or not that's a that's an interesting debate on its own but how do you know

92
00:07:58,520 --> 00:08:02,400
how even if we have this power how do you actually leverage this power for

93
00:08:02,400 --> 00:08:10,280
for something you know relevant that is is not just just a demonstration so in

94
00:08:10,320 --> 00:08:15,560
a sense the meta area of focus at least in the in the near term has been quantum

95
00:08:15,560 --> 00:08:21,680
AI right and what is quantum AI I like to subdivide it into two subfields that

96
00:08:21,680 --> 00:08:26,640
are dominant for now there are other subfields that could be analogous to

97
00:08:26,640 --> 00:08:31,520
the subfields of classical AI but for now it seems like the community is focused

98
00:08:31,520 --> 00:08:37,520
on two broad categories and one is quantum enhanced optimization so that is

99
00:08:37,760 --> 00:08:42,680
accelerating classical algorithms of optimization and search using quantum or

100
00:08:42,680 --> 00:08:50,000
quantum inspired dynamics and quantum deep learning and I have you know many

101
00:08:50,000 --> 00:08:53,640
people call these variational algorithms I'll justify my nomenclature in a

102
00:08:53,640 --> 00:08:59,560
second what I consider quantum deep learning is learning quantum representations

103
00:08:59,560 --> 00:09:04,480
of quantum or classical data so there's a lot to unpack there so we're going to

104
00:09:04,480 --> 00:09:08,640
spend a few slides trying to unpack what it means to do to have a representation

105
00:09:08,640 --> 00:09:13,680
or quantum data this is gonna be the focus of my talk today quantum deep

106
00:09:13,680 --> 00:09:19,960
learning learning a multi-layered quantum computation based representation of

107
00:09:19,960 --> 00:09:24,640
quantum or classical data distributions what is a computational representation

108
00:09:24,640 --> 00:09:29,040
of data right or a deep multi-layered computational representation of data

109
00:09:29,520 --> 00:09:34,960
representations right let's let's go back to classical deep representation learning

110
00:09:34,960 --> 00:09:40,920
theory aka deep learning and try to understand a bit of the context so so

111
00:09:40,920 --> 00:09:44,640
deep learning is subset of machine learning subset of AI subset of computer

112
00:09:44,640 --> 00:09:52,000
science and which is of course a subset of science and I guess the gist of it is

113
00:09:52,000 --> 00:09:58,480
that neural networks you know when they learn something they got to be able to

114
00:09:58,640 --> 00:10:02,720
in a sense recreate it you know Feynman said what I cannot create I do not

115
00:10:02,720 --> 00:10:06,240
understand and your favorite deep neural network if you could ask if you could

116
00:10:06,240 --> 00:10:11,200
ask it what it thinks it would probably say something similar like this quote

117
00:10:11,200 --> 00:10:18,320
what do we mean by recreate so here I'm gonna get more rigorous so usually you

118
00:10:18,320 --> 00:10:24,080
have a data set which is set of points sampled from a true distribution p true

119
00:10:24,080 --> 00:10:30,680
of x right and so you have a certain finite set of data points right you

120
00:10:30,680 --> 00:10:35,120
don't have the full distribution you could query you're trying to learn a

121
00:10:35,120 --> 00:10:43,000
approximative model and you're trying to approximate this distribution over a

122
00:10:43,000 --> 00:10:47,280
certain domain of interest that goes beyond the data set itself because you

123
00:10:47,280 --> 00:10:52,120
you already have the data points for that are in the data set but you're trying

124
00:10:52,120 --> 00:10:56,400
to extend it beyond the data set and you have a parametrized hypothesis class

125
00:10:56,400 --> 00:11:01,480
or in classical machine learning we call it a variational distribution a

126
00:11:01,480 --> 00:11:05,640
variational classical probability distribution and five here would

127
00:11:05,640 --> 00:11:10,560
represent a set of parameters right because usually these distributions are

128
00:11:10,560 --> 00:11:14,520
parametrized using something called deep neural nets as we'll see in a second

129
00:11:14,520 --> 00:11:19,000
but the goal is to approximate a true distribution with a variational

130
00:11:19,000 --> 00:11:27,040
distribution and you know the idea is to minimize the discrepancy between the

131
00:11:27,040 --> 00:11:33,160
true distribution and our our variational distribution over the data set and

132
00:11:33,160 --> 00:11:37,360
hope that it extends beyond the data set right so that would be for generative

133
00:11:37,360 --> 00:11:43,120
modeling we're just trying to learn the raw distribution of all our data in what

134
00:11:43,120 --> 00:11:46,400
is called discriminative modeling which includes you know classifiers such as

135
00:11:46,480 --> 00:11:52,080
like labeling a picture of a cat or a dog or regression neural regression which

136
00:11:52,080 --> 00:11:59,840
is trying to get a scalar out of out of data so you know maybe a certain

137
00:11:59,840 --> 00:12:05,240
continuous value instead of a discrete label but in general we have pairs of

138
00:12:05,240 --> 00:12:10,840
inputs and outputs right and discriminative learning is very similar it

139
00:12:10,840 --> 00:12:14,160
can be phrased in probabilistic language as we're trying to learn a

140
00:12:14,200 --> 00:12:19,320
conditional distribution right random variables can be correlated and they can

141
00:12:19,320 --> 00:12:25,280
have what are called conditional distributions hopefully my okay we

142
00:12:25,280 --> 00:12:30,040
can see the last line here so the idea is that deterministic functions such as

143
00:12:30,040 --> 00:12:34,480
most deep neural networks are actually you know they're a subset of this

144
00:12:34,480 --> 00:12:38,880
conditional distributions they're kind of delta functions if you're used to the

145
00:12:38,880 --> 00:12:44,640
delta measure in function space if you integrate over it then you you get the

146
00:12:44,640 --> 00:12:50,640
value y equals f of x right so it's just a very sharp distribution right so

147
00:12:50,640 --> 00:12:57,200
most of classical machine learning or I guess the the popular parts of deep

148
00:12:57,200 --> 00:13:01,520
learning often deal with kind of deterministic point-wise functions

149
00:13:01,520 --> 00:13:05,920
whereas the the more general theory is actually based on probability and

150
00:13:06,160 --> 00:13:10,400
information theory right and that's kind of the roots of machine learning and

151
00:13:10,400 --> 00:13:14,080
what we're trying to get back to with quantum because we are in the early days

152
00:13:14,800 --> 00:13:18,320
where we must understand from first principles what we're doing instead of

153
00:13:18,320 --> 00:13:23,040
just trying stuff and iterating on the engineering of different algorithms

154
00:13:23,680 --> 00:13:26,560
you know willy-nilly we want to be guided in our research

155
00:13:28,480 --> 00:13:32,640
so you know deep learning are algorithms tied to identify patterns in data

156
00:13:33,600 --> 00:13:38,000
you use multi-layered parameterized computations to learn representations of data

157
00:13:38,560 --> 00:13:43,520
representations are multi you know deep representations are multi-step computations

158
00:13:43,520 --> 00:13:49,120
that either take you from your data space to a simplified space or from a simple space to your

159
00:13:49,120 --> 00:13:56,000
data space right so in the case of discriminative learning you're trying to take the input space

160
00:13:56,000 --> 00:14:01,360
say the pictures of cats and go to the label space you know is it cat or dog a single bit

161
00:14:01,360 --> 00:14:07,680
instead of many many pixels right in generative learning you're starting from a very simple set of

162
00:14:08,400 --> 00:14:13,920
randomness say a set of Gaussian samples or a set of random coin flips and trying to turn that

163
00:14:13,920 --> 00:14:22,400
randomness into the randomness over say the the set of pictures of bedrooms right the possible

164
00:14:22,400 --> 00:14:27,520
set of pictures of bedrooms and you're trying to sample new data points from from that data set

165
00:14:28,240 --> 00:14:35,760
in terms of math we say you know we're searching for a sub manifold of your your your space right

166
00:14:35,760 --> 00:14:42,640
and if it's all continuously parameterized it's technically a manifold again this is what I just

167
00:14:42,640 --> 00:14:47,920
described you have some randomness a generative model would go to the some complicated space you

168
00:14:47,920 --> 00:14:53,360
know machine learning folks and deep learning folks really love pictures quantum folks love

169
00:14:53,440 --> 00:14:59,840
wave functions and nick states as we'll see but you know discriminative learning would go to a

170
00:14:59,840 --> 00:15:06,000
simple space and then once it's simplified it's easy to separate out the two class so again

171
00:15:06,000 --> 00:15:10,160
representations every time I see representations don't know freak out it's just parameterized

172
00:15:10,160 --> 00:15:17,040
multi-step computation and deep is multi-layer and the building block is neural networks

173
00:15:17,600 --> 00:15:26,480
so I think I'm going to skip over this theory this is an example of a unsupervised learning

174
00:15:26,480 --> 00:15:31,920
algorithm called a variational autoencoder it's a way to compress data so you go to a compressed

175
00:15:31,920 --> 00:15:37,760
space and by compressing you're going to be forced to decorrelate the data and get a very simple

176
00:15:38,960 --> 00:15:43,680
very simple randomness and you could fit that simple randomness say with Gaussians

177
00:15:43,760 --> 00:15:48,000
and then if you plug it through in a sense the reverse transformation you get your data set

178
00:15:48,000 --> 00:15:52,720
again right and I want to show this because it's going to be very similar to our quantum approach

179
00:15:52,720 --> 00:15:58,560
where you could go in reverse through a quantum classical transformation and then you have a

180
00:15:58,560 --> 00:16:04,160
very simple what is called a latent space a hidden space and then when you want to generate the

181
00:16:04,160 --> 00:16:11,280
data again you go from latent space to the visible space right and these are very cool because you

182
00:16:11,600 --> 00:16:18,400
can in latent space if you just train the network to search for interesting features in general

183
00:16:18,400 --> 00:16:25,040
they can find features that and then you could do kind of logic in latent space so maybe there's

184
00:16:25,040 --> 00:16:30,160
a vector that corresponds to glasses to gender to age and so on and you could play around in

185
00:16:30,160 --> 00:16:35,920
latent space and see what you generate on the other side so you know for quantum for example if

186
00:16:35,920 --> 00:16:40,800
you have properties of materials you're trying to detect and you're trying to generate new materials

187
00:16:40,800 --> 00:16:46,240
or new materials with properties having a latent space that you've detected to play with it can be

188
00:16:46,240 --> 00:16:54,480
very useful and of course unsupervised learning itself is also useful in classical and sorry in

189
00:16:54,480 --> 00:17:01,520
a discriminative learning because finding a compressed representation is already part of the

190
00:17:01,520 --> 00:17:08,560
job to to to separate out classes so imagine we we had you know three different classes and we

191
00:17:08,560 --> 00:17:12,960
compressed it to some space that's two-dimensional and then we could go from a two-dimensional

192
00:17:12,960 --> 00:17:19,280
space to three different class labels of which color of the blob it corresponds to so again so

193
00:17:19,280 --> 00:17:24,880
i'm going to be focused on unsupervised learning but a lot of this actually applies to supervised

194
00:17:24,880 --> 00:17:32,320
or discriminative learning so what consists of a good representation i won't go too much into this but

195
00:17:32,880 --> 00:17:39,120
at a high level you want the representation capacity are you able to capture or you know

196
00:17:39,120 --> 00:17:47,440
reproduce the data set for some value of your parameters of your model is it trainable efficiently

197
00:17:47,440 --> 00:17:53,760
right if you have a neural network parameterizing your computational representation to go from

198
00:17:53,760 --> 00:17:59,600
complicated to simple space how easy is it to train it with algorithms that are not too closely

199
00:18:00,560 --> 00:18:05,040
inference tractability for feedforward neural networks that's very simple but there's other

200
00:18:05,040 --> 00:18:10,320
types of models that just doing prediction the prediction step can be computationally costly

201
00:18:10,320 --> 00:18:16,560
so that's something to keep in mind and of course that is the advantage of quantum computers is that

202
00:18:17,280 --> 00:18:22,080
you know if we incorporate large unitary transformations into our models as we'll see

203
00:18:22,800 --> 00:18:29,040
theoretically there are unitary transformations that cannot be executed the prediction or sampling

204
00:18:29,040 --> 00:18:34,480
step on a classical computer right so it's a very important part that's why i have this slide

205
00:18:35,920 --> 00:18:39,440
generalization power is is the core of machine learning generalization is

206
00:18:40,320 --> 00:18:45,600
you know if i fit within my data set will what i've learned extend outside the data set which

207
00:18:45,600 --> 00:18:50,880
is important because that is the difference between learning and optimization i sense there's a question

208
00:18:51,760 --> 00:18:58,320
oh just i'm getting keen keen awareness um this is more of a curiosity question

209
00:18:59,200 --> 00:19:04,800
about the representation capacity it's uh it seems like a really powerful but what can we usually

210
00:19:05,360 --> 00:19:10,720
before say running numerical experiments and so forth you know how much can we say or really

211
00:19:10,720 --> 00:19:15,360
peer into that for a particular model you've come up with you know what are the kind of tools and

212
00:19:16,000 --> 00:19:19,920
techniques and how far can they allow you to can we really say a lot about that

213
00:19:20,800 --> 00:19:28,000
about complexity of sampling unitaries uh yeah the representation capacity like what kind of

214
00:19:28,000 --> 00:19:35,360
correlations and so forth you will be able to capture potentially this is more of a right and

215
00:19:35,360 --> 00:19:40,960
that is going to depend strongly on your your the way you parameterize your your transformation in a

216
00:19:40,960 --> 00:19:46,320
sense you're by having a parameterized model you have what is called a hypothesis class and

217
00:19:46,320 --> 00:19:53,120
depending on on the various choices you've made you're going to kind of span a sub manifold of

218
00:19:53,120 --> 00:20:00,240
states and the idea is that you know what is very popular right now is called the hardware efficient

219
00:20:01,040 --> 00:20:08,080
onsots it looks very much like a random quantum circuit like this it's very tightly packed

220
00:20:08,800 --> 00:20:15,200
and the idea is that if you look at in the space of possible quantum states it can represent right

221
00:20:15,200 --> 00:20:20,880
if all of these transformations were parameterized random you know single and two qubit rotations

222
00:20:20,880 --> 00:20:27,040
right then theoretically you know its complexity is growing larger and larger right and in a sense

223
00:20:28,080 --> 00:20:34,560
any quantum state that has a complexity uh within that complexity radius you'll be able to reach it

224
00:20:35,120 --> 00:20:40,400
but the problem is because you're spanning such a large space your training of your quantum neural

225
00:20:40,400 --> 00:20:45,600
network becomes harder and harder um because your hypothesis class is too large so you're

226
00:20:45,600 --> 00:20:52,160
searching over too too many possibilities and this is the result known as the the baron plateaus

227
00:20:52,160 --> 00:20:58,000
in the quantum neural network landscape or or the quantum version of no free lunch theorem where

228
00:20:58,000 --> 00:21:03,440
you can't have a one size fits all representation and that's that's where physicists come in

229
00:21:03,440 --> 00:21:10,160
physicists need to have you know good prior knowledge of the domain of application they're

230
00:21:10,160 --> 00:21:15,280
trying to do quantum machine learning and to instruct their choice of representation and

231
00:21:15,280 --> 00:21:24,560
parameterization to aid in in the the tractability of training um that answers the question yeah so

232
00:21:25,280 --> 00:21:30,400
i like that free lunch theorem uh because i guess you know you could try to say well if i have some

233
00:21:30,400 --> 00:21:34,960
sort of generators that i use for my model you know what is the reachability of states

234
00:21:35,760 --> 00:21:39,920
right since the people ask what is the reachability but i think what you're emphasizing here is that

235
00:21:39,920 --> 00:21:46,400
reachability is maybe only a first step uh and maybe having too much reachability sometimes

236
00:21:46,400 --> 00:21:52,960
at the moment so there's a trade-off maybe between capacity and efficiency exactly exactly and that

237
00:21:52,960 --> 00:21:59,120
that is the no free lunch theorem in a sense so that's lucky for us because uh you know at least

238
00:21:59,120 --> 00:22:04,240
for now it seems like physicists will be needed uh in the future when uh we're not going to be

239
00:22:04,240 --> 00:22:11,520
out of a job we still need to design architectures um at least for now so let's see let's see let's

240
00:22:11,520 --> 00:22:16,320
see where it goes but uh but that's right that's right so uh you know a lot of what i'm going to

241
00:22:16,320 --> 00:22:23,280
present today is not necessarily uh architectures for specific domains it's it's more uh a general

242
00:22:23,360 --> 00:22:29,760
framework uh based on quantum information theory of how to uh do quantum machine learning or or

243
00:22:29,760 --> 00:22:38,480
maybe a a very broad class of parametrizations of of models that are quantum yeah yeah i'm since

244
00:22:38,480 --> 00:22:43,040
i've already interpreted you there's interesting this is kind of an unusual question but why don't

245
00:22:43,040 --> 00:22:49,120
throw it out here anyhow from martin how much time do you take uh take it take you to learn all this

246
00:22:49,520 --> 00:22:57,440
i guess um i mean so okay so i guess backstory uh once there was a conference of machine

247
00:22:57,440 --> 00:23:02,560
learning on a monday on a friday night i decided to binge watch lectures at three times speed on

248
00:23:02,560 --> 00:23:09,440
youtube on the basics of deep learning i think that was 2016 uh or 2017 something like that

249
00:23:10,160 --> 00:23:15,280
and uh since then i've just been reading uh machine learning papers and you know i i guess

250
00:23:15,360 --> 00:23:20,800
have a deep math background so it helps uh and then quantum computing itself uh i guess

251
00:23:20,800 --> 00:23:27,360
i've been doing since i was 19 and i'm 28 now so gives you an estimate uh it's just always been

252
00:23:27,360 --> 00:23:34,000
my passion and uh i uh i went through theoretical physics and uh now i'm here in uh quantum machine

253
00:23:34,000 --> 00:23:41,040
learning so i would say four years of of interest in quantum machine learning two to three years

254
00:23:41,040 --> 00:23:46,240
serious uh serious focus and it looks like george baron is building on my question which

255
00:23:46,240 --> 00:23:50,400
probably gets into a little bit i also wanted to get into which is what are some uh quantitative

256
00:23:51,280 --> 00:23:56,240
quantitative metrics for representation capacity yeah that's that's interesting um

257
00:23:57,600 --> 00:24:04,480
i guess that's a good that's a good question i would say if you can quantify of in a sense a

258
00:24:04,480 --> 00:24:11,120
notion of volume and complexity space um and this is actually you know we're edging on on

259
00:24:12,080 --> 00:24:16,160
theoretical physics here because the notion of quantum complexity is interested in interesting

260
00:24:16,160 --> 00:24:22,800
in in the theory of ads cft and um you know there's lennard suskin who does a lot of work in this uh

261
00:24:22,800 --> 00:24:29,760
in this space uh and uh yeah i mean that's a that's a that's an open question i think i have

262
00:24:29,760 --> 00:24:33,680
some intuition uh as to what would be a good metric but that would be an interesting further

263
00:24:33,680 --> 00:24:41,040
study you know yeah there's a good quote right on we point correct like with uh with logic we prove

264
00:24:41,040 --> 00:24:49,680
with with intuition we discover and that's right here's the guidance that's right cool cool um so i

265
00:24:49,680 --> 00:24:56,640
guess i've i've i've gone through these uh this is just some text uh backing up what i've said um

266
00:24:56,640 --> 00:25:02,560
so okay so now that we've we have some very brief background and some intuition about deep learning

267
00:25:02,560 --> 00:25:08,560
because this is a quantum computing audience so we had to load that up um how can we use you know

268
00:25:08,560 --> 00:25:15,040
what we learn taking inspiration from vaes or uh from you know the what is needed to have a good

269
00:25:15,040 --> 00:25:20,320
representation to instruct our choice of how we do quantum deep learning so first of all what would

270
00:25:20,320 --> 00:25:25,920
be a quantum deep representation right well a classical feed for network in a cartoonish picture

271
00:25:25,920 --> 00:25:31,440
this is not the most general formulation but it's a it's a friendly one um you have some input you

272
00:25:31,440 --> 00:25:35,600
have some parameters phi and then you have some parameterized output f of x fine for a quantum

273
00:25:35,600 --> 00:25:40,720
neural network you have usually a pure state input a unitary evolution that is parameterized in some

274
00:25:40,720 --> 00:25:47,680
way and then you have a parameterized hypothesis class of pure states right which is u five times

275
00:25:47,680 --> 00:25:55,280
your initial state and uh you know we call the function f the feed forward operation you can have

276
00:25:55,280 --> 00:26:01,680
a uh loss functionals returns a scalar that depends on say your label and your your output of your

277
00:26:01,680 --> 00:26:06,800
network uh this could be some statistical measure of statistical distance to your data set and you

278
00:26:06,800 --> 00:26:12,080
want to find the uh minimum or approximate minimum subject to variations of the parameters of this

279
00:26:12,080 --> 00:26:16,720
loss functional so how do we get scalar values out of a quantum computer it gives us a wave

280
00:26:16,720 --> 00:26:21,600
function so do we what do we do with it well we have to define a loss operator which is a quantum

281
00:26:21,600 --> 00:26:26,320
observable right or Hermitian observable and often we decompose it into small chunks that

282
00:26:26,320 --> 00:26:34,720
we can measure independently um and combine later on and our goal similarly to you know in the v the

283
00:26:34,720 --> 00:26:41,440
case of vqe and many other variational algorithms is to find uh the minimum subject to variations

284
00:26:41,440 --> 00:26:47,920
variational variations the parameters uh the expectation value of this loss operator right

285
00:26:47,920 --> 00:26:53,760
sometimes it's called the energy or the Hamiltonian but i like to generalize it to uh you know loss

286
00:26:54,880 --> 00:27:00,160
for quantum machine learning so there's just a refresher this is the typical way when trains

287
00:27:00,160 --> 00:27:04,800
a vanilla uh what i call vanilla quantum variational algorithms or vanilla quantum neural network

288
00:27:05,920 --> 00:27:11,760
you have a loop between a quantum and classical computer and the quantum computer gets an

289
00:27:11,760 --> 00:27:15,840
expectation value feeds at the classical computer classical computer has an optimization

290
00:27:15,840 --> 00:27:21,200
algorithm that's classical suggests and use values of the parameter and you iterate like this in a

291
00:27:21,200 --> 00:27:26,560
sense you know our current quantum computers are restricted in how much quantum depth and uh

292
00:27:27,200 --> 00:27:30,880
you know what kind of quantum states they can represent they're restricted in depth because

293
00:27:30,880 --> 00:27:36,720
of the noise and so it makes sense that we search over the space uh given this constraint of low

294
00:27:36,720 --> 00:27:42,960
depth circuits we should search over the space to find the quantum state and this is this is why

295
00:27:42,960 --> 00:27:48,880
this is kind of taking over because for the nisk era or you know early fault tolerance we're going

296
00:27:48,880 --> 00:27:54,960
to be searching over the space of states that are uh not too big not not too you know long to

297
00:27:54,960 --> 00:28:00,400
quantum compute um so why learn quantum representations in the first place if you if you allow a

298
00:28:00,400 --> 00:28:05,680
meal modify uh Feynman's famous quote uh Feynman said you know nature is in classical gamut if you

299
00:28:05,680 --> 00:28:10,240
want to make a simulation of nature you better make it quantum mechanical and in our case it's

300
00:28:10,240 --> 00:28:16,000
if you want to learn a representation of nature you better make it quantum mechanical right so

301
00:28:16,800 --> 00:28:21,440
quantum states and quantum processes i think we've been we've been mentioning this can exhibit

302
00:28:21,440 --> 00:28:27,040
high levels of quantum forms of correlation such as entanglement and that's exponentially hard to

303
00:28:27,040 --> 00:28:32,640
represent in classical memory right if you have a uh random circuit producing in a highly entangled

304
00:28:32,640 --> 00:28:38,240
state it's very hard to approximate it and it's hard to prove uh theoretically without a doubt

305
00:28:38,240 --> 00:28:43,760
but every algorithm we've tried to simulate quantum circuits it seems to uh fall flat on its face at

306
00:28:43,760 --> 00:28:52,960
some point right um yes um quick question clarification from joe here is the loss calculated

307
00:28:52,960 --> 00:29:00,640
by classical computer is the loss calculated so he's talking about the expectation value of the

308
00:29:00,640 --> 00:29:06,480
L operator right right it takes actually several samples to estimate the expectation value right

309
00:29:06,480 --> 00:29:11,920
you could think of it as like sampling from a distribution if i'm trying to do an estimator

310
00:29:11,920 --> 00:29:19,280
of a uh random variable subject to samples from a certain distribution it takes several samples

311
00:29:19,280 --> 00:29:24,720
so there's there's like a mini loop in here to estimate the expectation value here and that's a

312
00:29:24,720 --> 00:29:30,000
mixture of uh you know doing several measurements on the quantum computer and saving saving the

313
00:29:30,000 --> 00:29:34,560
results on the classical computer and then the classical computer can aggregate the the various

314
00:29:34,640 --> 00:29:39,280
results to get an average right and maybe to paraphrase you mean you you run that same use

315
00:29:39,280 --> 00:29:43,360
circuit with the same inputs the same initial state the same parameters several different

316
00:29:43,360 --> 00:29:50,880
measurements yes well i guess the the loss operator may have several uh non-commuting uh

317
00:29:50,880 --> 00:29:59,040
sub operators and one wants to get an expectation value of each uh term in the sum and then one

318
00:29:59,680 --> 00:30:04,480
adds up all these terms to get an expectation value of the sum so that is called the quantum

319
00:30:04,720 --> 00:30:11,600
expectation estimation uh sub routine in a sense and here i kind of abstracted it out

320
00:30:12,320 --> 00:30:18,320
but it's it's an extra sub routine there's a mini loop of trying to get a precise estimate

321
00:30:18,320 --> 00:30:22,640
it's not to be neglected because if you want you know a 10 to the minus seven precision

322
00:30:22,640 --> 00:30:28,240
for an energy it could take you hours on a 10 kilohertz machine for example so it's it's an

323
00:30:28,240 --> 00:30:33,360
important thing to consider when designing quantum algorithms that we only have noisy

324
00:30:33,360 --> 00:30:39,680
or estimates of our our loss function yeah so basically if you run that um yeah so you

325
00:30:39,680 --> 00:30:45,120
break up the L into sub operators which you measure you know you measure by running multiple

326
00:30:45,120 --> 00:30:50,400
times you get you don't need the distribution here for this you only need the actual mean value

327
00:30:51,040 --> 00:30:57,200
yes right or at least in the in the typical vanilla case but as we'll see there's there's

328
00:30:57,200 --> 00:31:04,240
other other variants out there but uh usually the quantum part it's it's hard to get a scalar

329
00:31:04,240 --> 00:31:09,360
out of the quantum computer by something else than defining an observable and outcomes of measurements

330
00:31:10,480 --> 00:31:15,680
right and how important would read out errors and skew and to read out being in a instance you get a

331
00:31:16,640 --> 00:31:22,240
p1 probability 80 but you have to skew because of the loss t1 process and things like that

332
00:31:22,240 --> 00:31:29,200
that's right you get a imprecise estimate of your your loss function and uh you need to have classical

333
00:31:29,200 --> 00:31:34,480
algorithms on the side to compensate for that imprecision or to choose your optimizer wisely

334
00:31:34,480 --> 00:31:41,600
in a way that is robust in noise right and uh i see a lot of questions i i hope i can get to all

335
00:31:41,600 --> 00:31:47,600
of these we may we may have to yeah for the end here okay um maybe just quick one here does the

336
00:31:47,680 --> 00:31:50,960
quantum advantage come from generating the variational forms

337
00:31:52,160 --> 00:31:58,320
um i mean you know i am not claiming a quantum advantage yet but i would say that uh

338
00:31:59,280 --> 00:32:05,280
if there if there was a quantum machine learning advantage it would likely come from uh being able

339
00:32:05,280 --> 00:32:10,960
to do the inference or prediction step with your model and hence the ability to train it as well

340
00:32:10,960 --> 00:32:15,920
so both the training and inference are rendered possible once you have access to a quantum computer

341
00:32:15,920 --> 00:32:21,520
if you incorporate a model that has high quantum complexity so a large unitary that we can't

342
00:32:21,520 --> 00:32:28,880
simulate classically um yep and i think this next one you're going to talk about which is back propagation

343
00:32:28,880 --> 00:32:36,880
you know can you see yes yes yes um okay so how to practically leverage a quantum computing power

344
00:32:37,840 --> 00:32:42,320
well for discriminative models for example you can uh you know let's say you prepare a quantum

345
00:32:42,320 --> 00:32:47,040
data set because again for now we don't have a quantum internet where we can import uh data

346
00:32:48,320 --> 00:32:53,600
that'd be nice someday uh and you evaluate your quantum model let's say you do a feed

347
00:32:53,600 --> 00:33:00,400
forward or a unitary parametrize model you get the expectation value of say several observables

348
00:33:00,400 --> 00:33:06,000
that becomes a vector you feed that vector to a classical neural network and then it evaluates

349
00:33:06,000 --> 00:33:11,840
some some prediction based on this say a label or whatnot and the idea is that you can train both

350
00:33:11,840 --> 00:33:18,320
your classical uh part of your network and your quantum part of the network together uh via a

351
00:33:18,320 --> 00:33:24,800
form of quantum classical hybrid back prop and the idea is that you know your quantum neural network

352
00:33:24,800 --> 00:33:30,240
can can have all sorts of components uh but it could itself be a building block in a sort of meta

353
00:33:30,240 --> 00:33:36,240
network between quantum neural networks and classical neural networks and the idea is that if you

354
00:33:36,240 --> 00:33:42,640
zoom in on say a little uh sandwich of of nodes here are meta nodes of a deep neural network a

355
00:33:42,640 --> 00:33:47,280
quantum neural network and a deep neural network so the let's say a deep neural network or any

356
00:33:47,280 --> 00:33:53,520
differentiable computation feeds parameters to quantum neural network uh and then you have

357
00:33:53,520 --> 00:33:57,600
the measurement of several observables at the output which you feed to a classical neural network

358
00:33:57,600 --> 00:34:03,440
and and then you could do other stuff later on uh and you get your loss function here then you

359
00:34:03,840 --> 00:34:09,520
get the gradient of the loss function back propagate uh your gradient classically and

360
00:34:09,520 --> 00:34:18,560
effectively what's interesting is that this thing is a actually a itself is is technically

361
00:34:18,560 --> 00:34:23,680
an observable on this space if you could you know invert this function but the idea is you do a first

362
00:34:23,680 --> 00:34:29,440
order approximation so you get an effective back propagated gradient Hamiltonian which becomes

363
00:34:29,440 --> 00:34:34,320
or you call it a Hamiltonian because it's an observable and then it becomes just like taking

364
00:34:34,320 --> 00:34:38,480
gradients of a vqe to obtain the gradients of these parameters you just have an effective

365
00:34:39,600 --> 00:34:44,720
value of the gradient for a certain value of your your you know all your parameters over here and your

366
00:34:44,720 --> 00:34:50,000
loss function and you could take gradients of uh that's with respect to your parameters and you've

367
00:34:50,000 --> 00:34:55,360
effectively back propagated the gradient of this value through the q and n and you could keep going

368
00:34:55,360 --> 00:34:59,520
and this is important because you don't want to have to do a slight change do your whole

369
00:34:59,520 --> 00:35:05,360
chain of computations see how it changed and then backtrack it's it's more scalable uh this way

370
00:35:07,520 --> 00:35:15,440
so okay so there's some software uh that does this i have to plug it i mean it's one of my

371
00:35:15,440 --> 00:35:21,520
pet projects uh for it's been so for a while uh for now it's uh it's uh interface between

372
00:35:21,520 --> 00:35:26,320
cirq and tensorflow there's some open source contributors that are working on quiz kit

373
00:35:26,320 --> 00:35:30,560
compatibility so that's going to be exciting for the quiz kit community and we're supporting them

374
00:35:31,840 --> 00:35:37,920
but it allows you to you know automate this this training and integrate it into you know

375
00:35:37,920 --> 00:35:43,200
advanced machine learning models in tensorflow and you know tensorflow i think has the record of

376
00:35:44,320 --> 00:35:50,000
on ibm supercomputers for you know the biggest machine learning computation so i think it's

377
00:35:50,000 --> 00:35:56,320
important to uh to ideally integrate uh quantum computers with uh the power at least one of the

378
00:35:56,320 --> 00:36:03,760
most powerful frameworks for high performance computing on the classical side um so any question

379
00:36:03,760 --> 00:36:09,760
there in that vein um this is an earlier question for me to are there any data sets filled with the

380
00:36:09,760 --> 00:36:16,720
quantum machine learning models you can map up some notebooks for a slayer uh that's uh i think

381
00:36:16,720 --> 00:36:21,120
that's public that we're working on that and we're trying to work with other you know other uh

382
00:36:21,760 --> 00:36:27,280
companies in the space to make sure we we agree on what a form for a data set will be but in general

383
00:36:28,240 --> 00:36:33,520
because you can't download quantum data you can't just save you know states because they

384
00:36:33,520 --> 00:36:37,680
take exponential space and you don't know how to load them on your quantum computer the data set

385
00:36:37,680 --> 00:36:43,360
takes the form of a circuit um or a set of circuits and those define wave functions that you could

386
00:36:43,440 --> 00:36:48,480
then do quantum deep learning on and it's something that's uh being worked on but you'll

387
00:36:48,480 --> 00:36:58,240
have to stay tuned uh for that thank you cool okay so uh what can one do with hybrid feed forward

388
00:36:58,240 --> 00:37:04,800
networks uh i'm going to skip over this uh yeah quickly i guess there's a paper by luke in which

389
00:37:04,800 --> 00:37:10,720
is a convolutional neural network which uh are inspired with from the mera if you're in in the

390
00:37:10,720 --> 00:37:18,880
know about it uh basically it's luke using the fact that uh if you know your system is

391
00:37:18,880 --> 00:37:25,840
translationally invariant so it has some symmetry you reflect that symmetry in your your choice of

392
00:37:25,840 --> 00:37:30,400
parameterization of your quantum neural network and so this is just a quantum neural network

393
00:37:30,400 --> 00:37:38,000
that has translational invariance and is hierarchical and the idea is that you know maybe you can't do

394
00:37:38,000 --> 00:37:41,360
all the quantum layers but maybe you could do only one quantum layer and already you'll

395
00:37:41,360 --> 00:37:45,600
you've down sampled the problem you've reduced that dimension dimensionality and you've broken

396
00:37:45,600 --> 00:37:50,080
up some entanglement or you've you've like disentangled partially remember for compression

397
00:37:50,080 --> 00:37:55,440
you got to decorrelate everything all right so the idea is that you can do you could input

398
00:37:55,440 --> 00:38:00,080
various quantum data in batches you could apply various feature maps that are quantum convolutional

399
00:38:00,080 --> 00:38:04,880
networks and then you get kind of images from you know all your histograms of samples of your

400
00:38:04,880 --> 00:38:10,640
bit strings and following this you could apply classical convolutional layers and you know finish

401
00:38:10,640 --> 00:38:16,720
the job with fully connected and at least in our early experiments uh hybrid networks with multiple

402
00:38:16,720 --> 00:38:22,560
filters were better than one quantum network and that's without noise so with noise on the device

403
00:38:22,560 --> 00:38:27,520
it's even better uh but that's just a an example of discriminative learning i won't go too much

404
00:38:27,520 --> 00:38:32,400
into that but in terms of applications it would be for example classifying phases of matter detecting

405
00:38:32,400 --> 00:38:37,360
whether something is superconducting or not and the idea is maybe you train on a data set of

406
00:38:38,720 --> 00:38:42,080
a material you know is superconducting at certain value of the parameters and temperature

407
00:38:42,880 --> 00:38:47,840
and and then you you ask the neural network to detect for another material that you don't know

408
00:38:47,840 --> 00:38:53,200
whether it's superconducting or not at certain value parameters so generalizes so that's uh

409
00:38:53,200 --> 00:39:00,400
that's one quote-unquote killer app we think for quantum neural networks um yeah so it's just

410
00:39:00,400 --> 00:39:06,560
comparing the two with with our old diagram okay so i guess we'll get to the meat of the talk uh

411
00:39:08,080 --> 00:39:15,040
i'm not too bad halfway there i guess um so uh how can we extend these insights and how can we

412
00:39:15,040 --> 00:39:21,520
hybridize um in a meaningful way with classical machine learning capabilities for quantum machine

413
00:39:21,520 --> 00:39:28,160
learning right let's go back to our slide of deep generative modeling we have our data set

414
00:39:28,160 --> 00:39:32,800
we have our variational classical distribution we want to minimize our question before we

415
00:39:33,920 --> 00:39:39,920
deep dive here uh it's about um nlp and maybe can we use some of this quantum representation and

416
00:39:39,920 --> 00:39:44,960
nlp transformers to reduce the huge size of it to increase accuracy i hope that's a

417
00:39:44,960 --> 00:39:50,960
maybe a little bit out there question um so i mean we our team has some public work that we've

418
00:39:50,960 --> 00:39:57,760
used tensor networks which are you know analogous to quantum circuits in a sense to find factorizations

419
00:39:57,760 --> 00:40:03,760
of large matrices uh and we apply them to the transformers and at least in our demo we get a

420
00:40:03,760 --> 00:40:10,320
two times speed up uh and of course um you know that'd be great if um such a tensor network could

421
00:40:10,320 --> 00:40:15,600
be contracted on a quantum computer uh faster it's not an experiment we've tried yet but um you

422
00:40:15,600 --> 00:40:20,960
know it's going to come down to constant speed ups uh you know uh you know our tensor network

423
00:40:20,960 --> 00:40:24,960
versus a quantum computer for certain tensor networks the quantum computer is exponentially

424
00:40:24,960 --> 00:40:32,640
faster but for other tensor networks it's going to be similar uh potentially um so that is that is

425
00:40:32,640 --> 00:40:39,280
a good question um but uh i guess i guess we'll we'll have to see on that side but it's an interesting

426
00:40:39,280 --> 00:40:47,120
area of research in a sense uh dimensionality reduction uh using quantum circuits um and uh

427
00:40:47,120 --> 00:40:53,120
you know tensor networks are a first step towards that um but it's uh you know it's encouraging to

428
00:40:53,120 --> 00:40:58,560
see that cutting edge ml can be improved with quantum or quantum inspired methods at least today um so

429
00:41:00,400 --> 00:41:05,920
yeah at least in nlp that's the area that i'm confident saying something that quantum computers

430
00:41:05,920 --> 00:41:18,400
would be potentially useful um all right uh so so i mentioned we want uh our data set degree uh

431
00:41:19,200 --> 00:41:24,240
you know for our data points in general when you want two uh distributions to agree

432
00:41:24,240 --> 00:41:30,240
you do what is called the k l divergence uh it's not a symmetric function so be careful uh

433
00:41:30,240 --> 00:41:36,000
you could go you go one way or the other uh between your true distribution your data distribution and

434
00:41:36,000 --> 00:41:42,560
your uh variational uh distribution right and it's like here would be the expectation value

435
00:41:42,640 --> 00:41:48,000
specter of data of the ratio of logs right so the idea is that to evaluate this kind of gold

436
00:41:48,000 --> 00:41:54,560
standard of uh quantum statistic or sorry classical statistical distribution uh we need

437
00:41:55,280 --> 00:42:03,600
access to the log of our logarithm of our uh model for any given data point x that we sample

438
00:42:03,600 --> 00:42:11,200
from the data right so not every uh classical machine learning model allows you to do to do this

439
00:42:11,200 --> 00:42:17,760
right so gans for example don't have an explicit logarithm of the of the density of your generative

440
00:42:17,760 --> 00:42:22,400
model that you could query it's implicit it's only you know the discriminator telling you how well

441
00:42:22,400 --> 00:42:27,760
you're doing but it's not a notion of log whereas you have let's say you do a bunch of transformation

442
00:42:27,760 --> 00:42:33,200
that um that you know uh the determinant of the jacobian you could compute that efficiently

443
00:42:33,840 --> 00:42:38,960
right the determinant of jacobian if you if you continuously transform a space right and you

444
00:42:38,960 --> 00:42:44,480
had initially a simple Gaussian on that space and you end up with a complicated space you've kind of

445
00:42:44,480 --> 00:42:48,800
you know bunched it up and you've done some complicated different morphism you could back

446
00:42:48,800 --> 00:42:56,720
track how the notion of volume locally has changed right and uh for any you know value we target here

447
00:42:56,720 --> 00:43:05,440
we can kind of invert um the measure in a certain bin here to uh some set of bins over here and we

448
00:43:05,440 --> 00:43:10,320
know the value of a Gaussian analytically and so you can compute in a sense somewhat efficiently

449
00:43:10,320 --> 00:43:17,040
analytically the um the density of your your probability distribution for any point you query

450
00:43:17,040 --> 00:43:21,200
uh this is called a normalizing flow but there's other types of models you know there's energy

451
00:43:21,200 --> 00:43:27,760
based models there's auto regressive models there's a whole bunch of of cool models out there but a

452
00:43:27,760 --> 00:43:32,240
lot of people know gans because it's like the entry entry level thing because people understand

453
00:43:32,320 --> 00:43:37,120
discriminators but so we encourage you to check out other types of generative machine learning

454
00:43:37,120 --> 00:43:43,040
and in a sense we're we're looking to have an explicit uh notion of a log uh for reasons that

455
00:43:43,040 --> 00:43:48,800
are going to become apparent in a second in the quantum case so how can we extend this philosophy

456
00:43:48,800 --> 00:43:54,800
to quantum theory uh you know what's the intersection of quantum theory and probability theory right

457
00:43:55,920 --> 00:44:01,040
well there is you know just like in in black holes uh we look at black holes because they're at the

458
00:44:01,040 --> 00:44:06,080
intersection of quantum and gravity so they're an interesting test bed well here we look at

459
00:44:06,080 --> 00:44:09,920
mixed states because they're at the intersection of probability theory and probabilistic machine

460
00:44:09,920 --> 00:44:14,960
learning and quantum theory and quantum machine learning so mixed state in general can be a

461
00:44:14,960 --> 00:44:20,320
probabilistic mixture over mixed states these are matrices instead of vectors now so be careful

462
00:44:21,200 --> 00:44:26,080
but uh any density operator has what is called a spectral decomposition so it it's always

463
00:44:26,080 --> 00:44:32,480
expressible as a mixture of orthogonal pure states and this this mixture sums up to one so it has

464
00:44:32,480 --> 00:44:40,160
a probabilistic interpretation so we go from vectors to a density matrix and each element

465
00:44:40,160 --> 00:44:47,760
is in complex numbers so how would we represent mixed states so how would we represent the

466
00:44:47,760 --> 00:44:53,520
intersection of probability theory and quantum theory well we should have a model that composes

467
00:44:53,520 --> 00:45:00,800
a probabilistic model with a quantum model right and that is the idea of quantum probabilistic

468
00:45:00,800 --> 00:45:09,520
hybrid deep learning or deep representations hence the title of my talk so as we've seen quantum

469
00:45:09,520 --> 00:45:15,280
neural networks are typically unitary feedforward like this and they have a hypothesis class that is

470
00:45:16,080 --> 00:45:23,920
pure states we can combine here a classical parametrized probabilistic model that that we

471
00:45:23,920 --> 00:45:29,280
can sample and let's say this would flip your qubits you flip your qubits to prepare a bit string

472
00:45:29,840 --> 00:45:34,160
then you apply unitary that's parametrized and what you get at the output instead of a

473
00:45:34,160 --> 00:45:42,480
parametrized class of pure states is a parametrized class of mixed states right and uh you know your

474
00:45:42,480 --> 00:45:46,880
parametrized distribution your state at this point is a diagonal state so it's effectively

475
00:45:46,880 --> 00:45:52,080
classical it has no quantum correlations you can try to show this show there's no coherent

476
00:45:52,080 --> 00:45:58,720
neutral information exercise and then after that you tack on a unitary which is hard for

477
00:45:58,720 --> 00:46:03,440
classical computers to do the idea is we use classical computers and we make them you know we

478
00:46:03,440 --> 00:46:08,880
make them sweat right like inference of probabilistic models can be pretty computationally intense

479
00:46:09,440 --> 00:46:13,120
uh and then we combine them with unitaries and the quantity

480
00:46:13,920 --> 00:46:18,240
and let's talk about how you think we set for capital omega

481
00:46:19,840 --> 00:46:25,120
oh yeah so capital omega is just an index over your basis of your Hilbert space it's kind of a

482
00:46:25,120 --> 00:46:30,160
general formulation because we actually uh phrase the algorithm both for qubits and for

483
00:46:30,160 --> 00:46:34,800
continuous infinite dimensional Hilbert spaces so theoretically could be a an integral or something

484
00:46:34,880 --> 00:46:39,760
it's just general math but it's uh it's an index that it's a index set that runs over

485
00:46:40,720 --> 00:46:44,000
an index for your your entire basis that spans your whole Hilbert space of interest

486
00:46:44,800 --> 00:46:47,840
oh yeah and then you can choose basically any probability over

487
00:46:49,920 --> 00:46:54,640
basically anything but there's going to be certain types that are preferential for for training

488
00:46:54,640 --> 00:46:59,520
reasons as we'll see uh again you know you could parametrize anything classically but it's not

489
00:46:59,520 --> 00:47:04,000
every model that's easy to train again because let's say you need the log and you can't get it

490
00:47:04,000 --> 00:47:10,720
or can't get the gradients then it's difficult so as we'll see we can choose wisely how we

491
00:47:10,720 --> 00:47:15,760
parametrize things so that we can get nice gradients and can train things because how do you

492
00:47:15,760 --> 00:47:21,680
train continuously parametrize the hypothesis class gradient based methods so you use kind of the

493
00:47:21,680 --> 00:47:27,520
notion of steepest descent in the landscape of parameter space and maybe one more question

494
00:47:27,520 --> 00:47:33,360
I'm told I have to speak a little bit louder so hopefully for sure for I mean let's take the

495
00:47:33,440 --> 00:47:39,200
extreme case you take a case where you have a pure like harm mixture like you know you have a pure

496
00:47:39,200 --> 00:47:44,960
mixture of all states so I'm guessing that's not very useful one so in a way you want your state to

497
00:47:44,960 --> 00:47:50,960
be a little bit mixed that's somewhat pure or are you okay having like a purity of like zero

498
00:47:51,600 --> 00:47:57,760
or maximally mixed even though it's so if you were to optimize over architectures so tune

499
00:47:57,760 --> 00:48:02,880
and we have some new results that are not in the paper for this talk you could tune how much quantum

500
00:48:02,880 --> 00:48:09,840
depth you you you assign to the the unitary um theoretically this this approach could be tuneable

501
00:48:09,840 --> 00:48:14,480
in the sense that if the data set that you're trying to represent is purely classical and has no

502
00:48:14,480 --> 00:48:20,240
quantum correlation and the identities in the span of your unitary hypothesis class you could

503
00:48:20,240 --> 00:48:26,000
learn to just apply the identity and then it's a classical machine learning system if it's a pure

504
00:48:26,000 --> 00:48:31,120
state that you're learning this the probabilistic component is useless in a sense because it's

505
00:48:31,120 --> 00:48:35,440
going to be all unitary you're just trying to learn a pure state so it's an adaptive way to

506
00:48:36,320 --> 00:48:42,080
separate out the task of quantum and classical machine learning of a quantum uh mixed states

507
00:48:43,520 --> 00:48:49,120
and it's quite cool because you have one framework where you have as a subset uh classical

508
00:48:49,120 --> 00:48:57,120
generative modeling of distributions um so in a sense it it can via self-tuning it could

509
00:48:57,120 --> 00:49:03,040
adapt to use no quantum resources or use no classical resources or any you know continuum in between

510
00:49:04,560 --> 00:49:09,680
and uh we have time here two more quick questions so are you using mixed states for the input as

511
00:49:09,680 --> 00:49:13,520
sort of maybe in practice i guess at this level it's all a little more we're we're going to see

512
00:49:13,520 --> 00:49:21,680
that i guess uh it's you could use output or input so yeah um i think i still have a good

513
00:49:21,760 --> 00:49:27,440
number of core slides but i guess i'll i'll go through them faster a bit um we can run a little

514
00:49:27,440 --> 00:49:33,920
longer okay okay so you know why should we care about quantum mixed states well you know thermal

515
00:49:33,920 --> 00:49:40,800
states um are at finite temperature and so you know most systems in nature at finite temperature

516
00:49:40,800 --> 00:49:45,200
unfortunately our quantum computers are not at zero temperature so even them themselves must be

517
00:49:45,200 --> 00:49:50,240
modeled as mixed states uh if we were to be accurate and experimentalists know this theorists

518
00:49:50,800 --> 00:49:58,080
like to say it's a pure state um so you know so the ability to simulate mixed states is is crucial

519
00:49:58,080 --> 00:50:02,880
to nature and and the reality is like you know we're trying to use quantum computers to simulate

520
00:50:02,880 --> 00:50:08,000
nature but nature itself if you core screen enough you zoom out there's a quantum to classical

521
00:50:08,000 --> 00:50:13,120
transition right you know we're used to having classical physics having quantum physics and

522
00:50:13,120 --> 00:50:18,160
then there's a continuum in between so the point is to have a set of continuum of models

523
00:50:18,160 --> 00:50:23,040
that can model that in between at finite temperatures when quantumness dies down

524
00:50:23,920 --> 00:50:30,640
and uh it becomes classical uh or you know when you're very close to being fully quantum right

525
00:50:31,440 --> 00:50:34,800
uh most quantum systems are open quantum systems i've mentioned this and

526
00:50:35,520 --> 00:50:41,120
for various reasons subsystems of of quantum states uh have are mixed states because of

527
00:50:41,120 --> 00:50:44,880
entanglement if you take a reduced state of a pure entangled state you get a mixed state

528
00:50:45,520 --> 00:50:51,200
um so just looking at patches of things at a time to model them it's important so

529
00:50:52,160 --> 00:50:56,000
what sort of mixed states in nature can we variationally simulate using something like this

530
00:50:56,000 --> 00:51:01,840
well thermal states is of of great interest because they're they're omnipresent so the algorithm

531
00:51:01,840 --> 00:51:06,320
that leverages quantum probabilistic generative models to model thermal states is uh variational

532
00:51:06,320 --> 00:51:18,160
quantum thermalization and or vqt uh and uh so the problem is given a Hamiltonian and h and a

533
00:51:18,160 --> 00:51:24,240
target temperature uh one over beta then generate the thermal state which is the exponential that

534
00:51:24,240 --> 00:51:30,480
is normalized like this and this is the partition function and the idea is okay well we'll use one

535
00:51:30,480 --> 00:51:39,200
of our magic models of classical probabilistic distribution and a unitary uh and how are we

536
00:51:39,200 --> 00:51:44,880
going to converge to uh the thermal state well thermal states are the minimum of something

537
00:51:44,880 --> 00:51:52,880
called free energy right so free energy is you know uh roughly ignoring temperature energy minus

538
00:51:52,960 --> 00:52:00,160
entropy right uh so we can evaluate the energy you know just like in vqe of our model

539
00:52:00,720 --> 00:52:07,040
and subtract the entropy right so how do we get the entropy well because unitaries can serve

540
00:52:07,040 --> 00:52:11,920
entropy the actually actually the entropy comes strictly from our classical part of the model

541
00:52:11,920 --> 00:52:17,440
and if your classical model has ways to get gradients of entropy you're in business or

542
00:52:17,440 --> 00:52:22,400
you know sometimes it's simple enough you could get it analytically and this is equivalent defining

543
00:52:22,400 --> 00:52:27,520
you know the minimum of the relative entropy between our model and the thermal state so you

544
00:52:27,520 --> 00:52:31,760
know we know that the unique minimum of this function is when the two states match so if we

545
00:52:31,760 --> 00:52:37,760
do our job well and we parameterize things well and find the absolute optimal states uh then uh

546
00:52:37,760 --> 00:52:43,840
you know we've got the jackpot um state of minimal for energies thermal state so how do we

547
00:52:43,840 --> 00:52:47,840
parameterize our quantum probabilistic generative model i've been pretty abstract now so we're just

548
00:52:47,840 --> 00:52:54,160
going to zero it in slightly uh well uh the motivation for this work was to take inspiration

549
00:52:54,160 --> 00:52:59,600
from recent work by uh open ai and such on modern versions of energy based models where

550
00:53:00,400 --> 00:53:05,840
one it's it's now it's taking inspiration from physics right so you define an energy function

551
00:53:05,840 --> 00:53:09,840
using a classical neural network let's say from the space of bit strings or continuous values to

552
00:53:10,400 --> 00:53:15,680
to a scalar and you could use various algorithms that leverage gradient information such as

553
00:53:15,680 --> 00:53:22,000
Hamiltonian Monte Carlo or stochastic gradient Langevin dynamics uh you know all all there's a

554
00:53:22,000 --> 00:53:27,280
bunch of open source frameworks to to do the this part you could sample the landscape by in a sense

555
00:53:27,280 --> 00:53:33,440
having a noisy ball traverse this landscape and you get samples of the exponential this way or the

556
00:53:33,440 --> 00:53:38,560
Boltzmann distribution known in physics it's a classical Boltzmann distribution so you parameterize

557
00:53:38,560 --> 00:53:45,840
the energy and why why is that going to be useful well uh it has all sorts of compositionality

558
00:53:45,840 --> 00:53:53,040
perks it's it's very good it's comparative with GANs this is work by open ai 2019 so how do we

559
00:53:53,040 --> 00:53:59,360
leverage these models and integrate them with quantum computers well so you know what if we had

560
00:53:59,360 --> 00:54:03,280
our probabilistic part of our model was a classical energy based model like this so it's

561
00:54:03,280 --> 00:54:09,280
parameterized energy function and then our distribution is a Boltzmann distribution again

562
00:54:09,280 --> 00:54:15,760
well if you you you could define a diagonal operator which is the log after you flip some bits

563
00:54:16,560 --> 00:54:21,600
which is your energy function on the diagonal and what you get if you do some math with some

564
00:54:21,600 --> 00:54:27,760
unitaries and exponentials is that you've just parameterized a operator the diagonal is parameterized

565
00:54:27,760 --> 00:54:34,400
by a neural network and and the total operator is this conjugation of a unitary with this diagonal

566
00:54:34,400 --> 00:54:41,200
operator so you've you've parameterized a Hamiltonian operator and your hypothesis class is a set of

567
00:54:41,200 --> 00:54:46,160
thermal states so in a sense you you know you know you're targeting a thermal state so you might as

568
00:54:46,160 --> 00:54:55,200
well have a hypothesis class of thermal states right um okay so if we have this assumption

569
00:54:55,200 --> 00:55:00,160
that we're using an energy based model how do the gradients work out well there's a bit of math

570
00:55:00,160 --> 00:55:07,600
involved i've skipped many lines but it is possible to sample it essentially you have to get bit

571
00:55:07,600 --> 00:55:16,400
strings at the output of your model and you can evaluate you can compare the value of the energy

572
00:55:16,400 --> 00:55:22,640
of your your bit strings minus the the energy of your model and you could also evaluate gradients of

573
00:55:22,640 --> 00:55:27,600
your your model if it's parameterized by a neural network and you you do the sampling which only

574
00:55:27,600 --> 00:55:36,720
depends again on sampling from uh from your uh your model p theta of x and you have sampling

575
00:55:36,720 --> 00:55:42,400
algorithms and you can evaluate gradients in a sense it's an analytic way to guarantee that your

576
00:55:42,400 --> 00:55:48,880
estimates of your gradients are unbiased um and how do you get gradients for the quantum part

577
00:55:48,960 --> 00:55:53,120
well the quantum part is just the usual i hope you've seen this in other talks and

578
00:55:54,240 --> 00:55:58,160
i don't have time to cover it unfortunately today it's the parameter shift rule right

579
00:55:58,800 --> 00:56:03,520
which is how you take gradients in the vqe which is how do you take gradients of a

580
00:56:03,520 --> 00:56:08,480
a unitary a state fed through a unitary and an expectation value so i won't cover that but

581
00:56:08,480 --> 00:56:14,000
it's very standard it's you know a standard in the software framework and there's various papers

582
00:56:14,000 --> 00:56:21,040
that use this um it's a cool theory but you know does it work right uh the answer is yes you know

583
00:56:21,040 --> 00:56:26,480
if you if you have a target thermal states you can uh do a reconstruction like this this is for

584
00:56:26,480 --> 00:56:31,440
some heisenberg spin model we use very simple classical distribution here it was just Bernoulli

585
00:56:31,440 --> 00:56:36,560
so random coins uh and and the quantum computer could do a lot of work and and learn to represent

586
00:56:36,560 --> 00:56:41,680
a thermal state we've done much larger systems but you know a jarbled set of pixels is not necessarily

587
00:56:41,680 --> 00:56:47,280
the most aesthetic thing so we we choose to feature the smaller systems but uh we've scaled

588
00:56:47,280 --> 00:56:53,120
things up quite a bit and uh the idea is um you know the function we're optimizing is relative

589
00:56:53,120 --> 00:56:59,280
free energy but the other metrics of quantum statistical distance uh also converge uh so

590
00:57:00,160 --> 00:57:07,520
uh it seems to work um we've also tried some set of fermionic systems and bosonic systems

591
00:57:07,520 --> 00:57:13,120
for example a simple you know toy model of a superconductor that's uh bosonic uh sorry Gaussian

592
00:57:13,120 --> 00:57:18,960
fermionic so it's quite simple we can plot the correlation functions the target this is at iteration

593
00:57:18,960 --> 00:57:26,960
zero and it converges by iteration hundred of gradient descent pretty well um so this is actually

594
00:57:26,960 --> 00:57:32,160
a new result i i'd like to feature that's not in the paper uh but it's coming in the second version

595
00:57:32,160 --> 00:57:38,880
of it uh can we tune how much quantum versus classical resources we use right so suppose i

596
00:57:38,880 --> 00:57:43,360
look at this heisenberg model and i look at after training how how well i do in terms of

597
00:57:43,360 --> 00:57:47,440
trace distance and fidelity depending on the temperature and the number of quantum layers

598
00:57:47,440 --> 00:57:56,080
i use ah well we see there's certain sets of temperatures uh that uh you know you need more

599
00:57:56,080 --> 00:58:02,160
quantum layers to to model them right and it's not necessarily you know at this point it becomes

600
00:58:02,160 --> 00:58:08,160
trivial uh at this point there's a nice balance between quantum and classical resources and this

601
00:58:08,160 --> 00:58:13,440
is the fidelity is trace distance uh but uh this is kind of what you you'd like to do right you

602
00:58:13,440 --> 00:58:18,400
want to use as little quantum resources as possible uh in order to have an accurate representation

603
00:58:18,400 --> 00:58:22,880
of a state so this is something we started investigating but it's uh you know it maybe

604
00:58:22,880 --> 00:58:27,280
has some deep implications about what's the true quantum complexity of a quantum machine learning

605
00:58:27,280 --> 00:58:34,960
problem uh and uh i guess uh you know uh please take a look at these qr codes there's links to

606
00:58:34,960 --> 00:58:40,000
various notebooks and you know i've been advertising tensor flow quantum but there's there's obviously

607
00:58:40,000 --> 00:58:44,960
you know implementations in quiz kit from the community uh shout out to jack seroni and uh

608
00:58:44,960 --> 00:58:50,880
you know the tensor flow quantum implementations by my collaborator antonio martinez um and three

609
00:58:50,880 --> 00:58:57,200
two one take a picture on youtube or whatnot and look at the uh websites for these notebooks

610
00:58:58,400 --> 00:59:03,840
so the final component is more machine learning it's less quantum simulation is how do we use vqt

611
00:59:04,720 --> 00:59:09,040
to do quantum machine learning so if we're given quantum mixed state data how do we

612
00:59:10,560 --> 00:59:16,480
learn from quantum mixed state data so again we're going to use our quantum Hamiltonian based model

613
00:59:16,480 --> 00:59:22,960
because for reasons that are going to become apparent in a second so we call the task of

614
00:59:22,960 --> 00:59:30,240
learning to replicate right we want an approximate density matrix that approximates a data density

615
00:59:30,240 --> 00:59:34,560
matrix so a data density matrix could be itself a mixture of a bunch of density matrices we're just

616
00:59:34,560 --> 00:59:39,520
trying to approximate this thing and we want to find a set of parameters such that for the optimal

617
00:59:39,520 --> 00:59:45,360
parameters our hypothesis class approximates this density matrix and we assume we have access to the

618
00:59:45,360 --> 00:59:51,520
quantum form of the data okay and the idea is if you use a quantum Hamiltonian based model

619
00:59:52,160 --> 01:00:01,040
and you aim to minimize now the relative entropy in reverse from last time uh what you get is if

620
01:00:01,040 --> 01:00:03,920
you ignore the terms that don't depend on your parameters you get something called the cross

621
01:00:03,920 --> 01:00:10,560
entropy which is this the trace of stigma which is your data log row right and again because we've

622
01:00:10,560 --> 01:00:17,120
parametrized our hypothesis class in terms of its logarithm its quantum logarithm uh we can evaluate

623
01:00:17,120 --> 01:00:23,520
this energy this it's called modular energy or modular free energy and modular Hamiltonian is

624
01:00:23,520 --> 01:00:30,000
just a name for the log of a density matrix okay and so we're trying to learn a log of a density

625
01:00:30,000 --> 01:00:34,880
matrix such that the exponential replicates our data set and how you do this you plug your data

626
01:00:34,880 --> 01:00:41,200
you run it in reverse through your unitary of your quantum probabilistic model you sample it and then

627
01:00:41,200 --> 01:00:47,440
you get expectation values of the diagonal operator and this could be parametrized with a neural network

628
01:00:47,440 --> 01:00:52,800
so you can have more computation here the extra term here is all on the classical computer

629
01:00:53,520 --> 01:00:58,480
turns out you could also get gradients for these i won't go too much into it the quantum part is

630
01:00:58,480 --> 01:01:04,320
again parameter shift but these gradients again if you have a differentiable function for your energy

631
01:01:05,760 --> 01:01:10,560
you know like a neural network then you can evaluate you could sample these gradients and

632
01:01:10,560 --> 01:01:15,600
it's unbiased which is really cool that that's really important that we could get good estimates

633
01:01:15,600 --> 01:01:24,640
of the gradients and you know it works out if you don't use enough quantum or not enough layers

634
01:01:24,640 --> 01:01:30,320
of your quantum computer or not enough complexity of your classical distribution sometimes it

635
01:01:30,320 --> 01:01:37,760
doesn't work well so for various temperatures we've tested this and i guess this is a this is

636
01:01:37,760 --> 01:01:40,960
you know there's many things you could do once you have unsupervised learning for example you

637
01:01:40,960 --> 01:01:47,200
could learn a compression code so here we actually applied hopefully some of you know about bosonic

638
01:01:47,200 --> 01:01:52,400
quantum computing but theoretically could be applied to other forms that are not qubits

639
01:01:52,400 --> 01:01:58,320
and here we learn a compression code where we could throw you know 40 percent of a harmonic chain

640
01:01:59,280 --> 01:02:05,280
in the compressed space and still reconstruct the states so this is the error matrix of the density

641
01:02:05,280 --> 01:02:11,920
matrix point seven we start seeing errors and if you throw 90 percent of stuff out things go bad

642
01:02:11,920 --> 01:02:19,120
and there's theory that you can find the logarithm modes the modular modes of the system

643
01:02:19,120 --> 01:02:23,760
and so we checked it with theory that's why we looked at the system thank you but that's it that's

644
01:02:24,480 --> 01:02:31,760
corpse there sorry sorry what are the x and y axes on that curve again this curve on the on the

645
01:02:31,760 --> 01:02:39,040
left oh on this side so this is the density matrix it's uh it's the discrepancy between

646
01:02:39,680 --> 01:02:44,240
the so we go to compressed space it's like an auto encoder we go to compressed space

647
01:02:44,240 --> 01:02:52,560
and then we throw out uh what is like so okay so we we do we learn a vqt and the latent model is a

648
01:02:52,560 --> 01:02:59,040
product of individual um thermal states of harmonic oscillators right and those are like

649
01:02:59,040 --> 01:03:06,080
quantum forms of gaussians which is kind of cool and we throw out the lower entropy latent modes

650
01:03:06,080 --> 01:03:12,960
okay because the entropy represents a harmonic oscillator sorry or when you say a mode you mean

651
01:03:13,600 --> 01:03:19,360
harmonic oscillator of a harmonic oscillator yeah so this is in this is for say a bosonic

652
01:03:19,360 --> 01:03:24,640
continuous variable quantum computing and i did most of my time and continuous variable stuff

653
01:03:24,640 --> 01:03:31,360
before so myself as well in theoretical physics uh this is similar to a calculation of the

654
01:03:31,360 --> 01:03:37,920
hawking effect actually um i that's a whole two hours i won't go into that but actually here's

655
01:03:37,920 --> 01:03:42,320
the interesting thing i have this in my summer school lectures that are up and coming uh there

656
01:03:42,320 --> 01:03:46,800
are only two types of physicists those for whom all of physics is qubits and those for whom all

657
01:03:46,800 --> 01:03:54,400
of physics is oscillators i try to i try to uh play on both sides so uh hopefully someday

658
01:03:54,400 --> 01:04:01,440
we can have hybrid computers that'd be cool through everyone's that's right um so yeah so

659
01:04:01,440 --> 01:04:08,400
we agree with theory here um i could explain how this is related to the hawking and unruh effects

660
01:04:09,680 --> 01:04:15,040
but uh that would take some time but it's it's an interesting uh thing that quantum machine

661
01:04:15,040 --> 01:04:21,200
learning could theoretically understand or learn an analog of the hawking or unruh effects that you

662
01:04:22,160 --> 01:04:28,640
there exists a certain set of modes that an observer feels a thermal statistical

663
01:04:29,440 --> 01:04:34,800
fluctuations of the vacuum so this was the ground state we plug it in and if you transform it then

664
01:04:34,800 --> 01:04:38,960
it becomes a product of thermal states and instead of Fourier modes it's like these weird

665
01:04:39,040 --> 01:04:46,320
squished modes uh of the lattice so it's kind of information theoretic uh eigen modes instead of

666
01:04:46,320 --> 01:04:51,520
you know we're used to eigen modes in physics like the resonance but here it's kind of uh

667
01:04:51,520 --> 01:04:57,680
the resonance of of the log uh Hamiltonian which is the modular Hamiltonian and uh this brings us

668
01:04:57,680 --> 01:05:05,680
actually to the end uh of the talk and uh oh i have luckily haven't exceeded too much uh so we do

669
01:05:05,680 --> 01:05:12,160
have time for questions i guess but i just want to conclude i guess uh you know this is the beginning

670
01:05:12,160 --> 01:05:17,120
of a whole research program it's an exciting area and you know by starting from basics of

671
01:05:17,120 --> 01:05:21,840
information theory right we just started thinking about relative entropy and inspiring ourselves

672
01:05:21,840 --> 01:05:26,720
from physics we have discoveries and machine learning and hopefully now we could apply this back

673
01:05:26,720 --> 01:05:32,000
to the physics right so it's a feedback loop between physics and machine learning and it's

674
01:05:32,000 --> 01:05:39,440
that's a big part of the philosophy of our team at x uh yeah thank you yeah thank you very much um

675
01:05:39,440 --> 01:05:42,880
i think there were some questions during the talk that i didn't get to so maybe i'll

676
01:05:42,880 --> 01:05:49,920
gonna run up the chat here and get back to them again um okay i guess i'll just do a

677
01:05:49,920 --> 01:05:56,720
shout out to anntario my collaborator at waterloo uh he was google and x and jacob was instrumental

678
01:05:56,720 --> 01:06:02,640
to a lot of the vqt and and qmhl work and uh you know did a lot of the work there uh as well

679
01:06:02,640 --> 01:06:09,840
so big shout out to them uh but uh yeah so any questions in the chat i've seen a lot of questions

680
01:06:09,840 --> 01:06:16,080
in the chat here so let's uh let's get let's see how many how many we can answer i guess all right

681
01:06:16,080 --> 01:06:22,960
let's start with the latest one which is about temperature i think the question is um now is

682
01:06:22,960 --> 01:06:27,840
there a sense of critical temperature here relative to some sort of phase transition

683
01:06:29,120 --> 01:06:33,120
the you know the temperatures where you get noisy data closely the critical temperature

684
01:06:33,120 --> 01:06:37,680
some sort of phase transition in this innovative system or is that well i don't know if we

685
01:06:37,680 --> 01:06:43,200
purposely chose a system where we knew there was a phase transition but we can kind of see that

686
01:06:43,200 --> 01:06:49,200
there's different uh regimes where you need more entanglement or need less entanglement right so

687
01:06:50,000 --> 01:06:56,320
so seeing how many layers you need to represent a quantum state could be like seeing a dip in that

688
01:06:56,880 --> 01:07:01,680
could could be a way to detect different phases or quantum phases of matter i guess like you know

689
01:07:01,680 --> 01:07:06,560
regimes of parameter space that have very strong entanglement and regimes that are you know

690
01:07:06,560 --> 01:07:14,320
slightly you know almost trivial um but uh yeah i'm not sure if we purposely picked a system

691
01:07:14,320 --> 01:07:18,720
that we knew there was a phase transition we just observed this data for now but um maybe

692
01:07:18,720 --> 01:07:27,520
something to do uh better on yeah um let's see this one uh this one is about universal estimators

693
01:07:27,520 --> 01:07:33,440
basically can uh q and n's the quantum neural nets be used to imitate this kind of behavior

694
01:07:34,000 --> 01:07:39,520
i guess and we're saying you know given that three layer neural nets are regardless universal

695
01:07:39,520 --> 01:07:45,680
estimators in classical machine learning yeah that's a that's a good question so i guess you know

696
01:07:45,680 --> 01:07:51,520
you want if you have a universal functional approximator um then you know theoretically you

697
01:07:51,520 --> 01:07:58,240
can have a a universal uh you know you span the space of functions that you could represent

698
01:07:58,240 --> 01:08:05,200
of course any classical computation can be embedded uh if you write it out as a reversible

699
01:08:05,200 --> 01:08:10,720
classical computation using many extra registers and you keep the whole history of the computation

700
01:08:10,800 --> 01:08:15,360
if it's not reversible functions you can embed that right in quantum computations

701
01:08:15,360 --> 01:08:21,360
with toffoli gates instead of hand and so on and some of work several years ago i i showed how to

702
01:08:21,920 --> 01:08:26,720
take you know typical classical neural networks and make quantum circuits that implement the

703
01:08:26,720 --> 01:08:32,720
classical neural network in superposition um so the idea is yes i think you can use quantum

704
01:08:32,720 --> 01:08:38,800
neural networks to do the classical probabilistic machine learning components um though so far at

705
01:08:38,800 --> 01:08:44,080
least from uh the current state of the art of the theory it seems like quantum computers will

706
01:08:44,080 --> 01:08:52,400
have a polynomial speedup for inference probabilistic inference similar to grover speedup uh and that of

707
01:08:52,400 --> 01:08:56,720
course if we're competing with extremely large classical computers will be mostly relevant when

708
01:08:56,720 --> 01:09:02,560
quantum computers are of a size comparable to the square root of our largest supercomputer

709
01:09:02,800 --> 01:09:10,000
uh and um that is yeah that's i guess that's uh that's my answer so for now i guess the

710
01:09:10,000 --> 01:09:14,400
most practical approach is to use classical algorithms and classical computers for the

711
01:09:14,400 --> 01:09:19,680
classical component and use quantum computers for the truly quantum component which is the unitary

712
01:09:20,800 --> 01:09:24,000
that seems like a very nice sensible thing there is a question this one is

713
01:09:24,960 --> 01:09:29,760
interesting i think it's more of an opinion question maybe um we know that quantum Fourier

714
01:09:29,760 --> 01:09:36,000
transforms very t and usually digital computing um does it have a role in machine learning is it

715
01:09:36,000 --> 01:09:41,520
similar in here according to what is the relationship with advantage or the Fourier

716
01:09:41,520 --> 01:09:50,560
transform right so i guess here we parametrized our quantum neural network uh as a general uh

717
01:09:50,560 --> 01:09:58,240
bosonic um what is called book all above or Gaussian transformation and the discrete Fourier

718
01:09:58,240 --> 01:10:03,920
transform is a subset of such transformations and here we we learned these transformations so

719
01:10:04,640 --> 01:10:10,080
uh technically uh if we fed the whole system and we asked it to find the eigen modes and if

720
01:10:10,080 --> 01:10:15,440
we had a thermal state of this system say via vqt and then we fed it to quantum modular

721
01:10:15,440 --> 01:10:20,320
Hamiltonian learning these modes would be the Fourier modes because we know the eigen modes of

722
01:10:20,320 --> 01:10:27,760
of this Hamiltonian right we know how to decompose this this Hamiltonian uh into a sum of individual

723
01:10:28,480 --> 01:10:34,640
uh you know number operators um and it's the same you know finding this book all above

724
01:10:34,640 --> 01:10:40,480
transformation is what i mean by it's related to the unrefect uh calculation qft in curved

725
01:10:40,480 --> 01:10:45,520
spacetime i know there are some chat messages that were doubting that but uh i did my masters

726
01:10:45,520 --> 01:10:52,800
in quantum field theory in curved spacetime so you can trust me on that one uh but uh great so

727
01:10:53,440 --> 01:11:02,960
and maybe for the final question here um and taking the extra time um if we want to do research

728
01:11:02,960 --> 01:11:06,880
in this field where should we start or what should be out of the direction of research

729
01:11:08,080 --> 01:11:14,080
right i mean that's a that's a good question i guess uh you yourself have gone through this

730
01:11:14,080 --> 01:11:20,560
position not you know four years ago i think you mentioned right right right right so i guess in my

731
01:11:20,560 --> 01:11:27,840
case i started you know i started with uh the open you know source uh massively online courses mooks

732
01:11:27,840 --> 01:11:32,240
i just listened to that listened to a few of them and then i progressed to i wish i had all my

733
01:11:32,240 --> 01:11:37,360
textbooks here but uh they're they're back there but uh the good fellow the in good fellows textbook

734
01:11:37,360 --> 01:11:44,400
um inventor of gans and then uh murphy kevin murphy uh a goobler uh he did uh what i called

735
01:11:44,400 --> 01:11:50,000
a kind of the nielson and schwang or the bible of probabilistic machine learning and i think there's

736
01:11:50,400 --> 01:11:54,720
mckay there's information theory for machine learning that's if you want the textbook routes

737
01:11:55,760 --> 01:12:01,280
otherwise i think with time as the field stabilizes i guess because it's been moving so fast everybody

738
01:12:01,280 --> 01:12:07,280
who's involved in it is just cranking out papers rather than creating coursework uh there will be

739
01:12:07,280 --> 01:12:14,720
coursework um i could link a you waterloo course uh that i gave some guest guest lectures at that

740
01:12:15,680 --> 01:12:21,840
that featured some quantum machine learning uh but uh overall i would say it's important to

741
01:12:21,840 --> 01:12:25,120
understand the theory of classical machine learning at the fundamental level because

742
01:12:26,080 --> 01:12:31,920
you know similar to hardware engineering we're at the fundamental level of engineering a new

743
01:12:31,920 --> 01:12:37,280
computing stack so on the theory side we're re-engineering a whole algorithm stack so we

744
01:12:37,280 --> 01:12:42,400
got to start again from first principle so you know you have to trace back to like papers from

745
01:12:42,400 --> 01:12:47,040
the 80s of machine learning and and the fundamentals and then and then work your way back to the

746
01:12:47,040 --> 01:12:53,120
modern uh modern thing so i would say the the modern ml stuff is flashy and and and fun to

747
01:12:53,120 --> 01:12:58,400
stay up to date but i would say you know take the time go back to the the core old literature

748
01:12:58,400 --> 01:13:05,520
you know the foundations so um yeah uh mooks and then textbooks is the way to go that's

749
01:13:05,520 --> 01:13:11,760
that's what i did and here i am so uh and then maybe i can just add that now they're

750
01:13:11,760 --> 01:13:17,280
summer school classes coming online so i mentioned the quantum information kiscuit one i think there's

751
01:13:18,080 --> 01:13:22,960
a touching of ml and things like that in the last two lectures in quantum chemistry vqe's

752
01:13:22,960 --> 01:13:29,040
definitely on there so fantastic anyone interested in addition to to what you said you know we can

753
01:13:29,040 --> 01:13:36,160
add that um so i think it is that time that i get thank you again and thank the listeners

754
01:13:36,160 --> 01:13:43,040
for joining the quantum live seminar series uh we're back this friday um i will mention next

755
01:13:43,040 --> 01:13:48,240
week we're back so this at the same time uh continuing with the talk by antony miss capo from

756
01:13:48,240 --> 01:13:55,440
idea on quantum chemistry we could talk about variation of quantum eigen solvers q a o and

757
01:13:55,440 --> 01:14:00,080
things like that on chemistry things so that will be a very nice uh follow up to your talk

758
01:14:00,880 --> 01:14:08,080
gion and uh thank you thank you for inviting me it's uh it's been an honor and uh hopefully the

759
01:14:09,040 --> 01:14:14,240
quantum community is interested in quantum machine learning now uh so i've done my job

760
01:14:14,240 --> 01:14:24,080
there all right thank you so much follow gion on twitter quantum bird that's right all right

761
01:14:24,080 --> 01:14:29,920
so any final words and otherwise thank you and we'll see you next week uh that's it for me

762
01:14:29,920 --> 01:14:35,040
thanks again for tuning in and uh stay home stay safe everyone and uh thank you all right

763
01:14:36,000 --> 01:14:39,680
it was a pleasure gion we'll see you soon guys take care cheers

