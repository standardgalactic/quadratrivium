start	end	text
0	6120	global digital summer school on quantum computing and I see we have reposted
6120	11800	the link all right so oh Sausalito California that's where it's very close
11800	16240	to where I grew up now we are thrilled to roll out the latest episode of the
16240	20720	quantum seminar series dedicated to the research and academic communities this
20720	27000	this seminar takes place every Friday at noon Eastern time right now at this hour
27000	31600	on the Kiskin YouTube channel and I'm delighted to see so many of you already
31600	36320	tuned in I'm your host like a minute from IBM quantum research and today I have
36320	42760	privilege of hosting young we're done from Google X and and also from the
42760	46200	Institute of quantum computing at Waterloo young will present some very
46200	51960	nice results and hello young how are you today hi is that this let go happy to
51960	56800	be here yeah it's a pleasure to see you last time we met I think it was right
56800	61280	after March meeting was cancelled and we were at March meeting so I'm glad
61280	66360	that we can continue the discussions and have everyone join us today on the call
66360	71080	that's right the science goes on to multistimes but that's right indeed
71080	75760	indeed I hope you've been doing well again in addition to being a social
75760	79240	media celebrity is a human though is a PhD candidate at the University of
79240	83320	Waterloo at the Institute of quantum computing I had a very nice visit there
83320	88960	recently so great place he is also a research scientist at X Google X or
88960	95020	alphabets research and development lab before this Guillaume briefly worked at
95020	99200	Google AI quantum and was one of the co-founders of TensorFlow and project he
99200	102680	holds a master's in math and quantum input from you Waterloo as well as in
102680	108920	honors math and physics degree from a given and university and I think we have
108920	112040	some very interesting questions to discuss today I think as many of you know
112080	115080	quantum computing operates in an exponential space so how is classical
115080	119360	machine learning and learning classical distribution for instance which also
119360	125120	operates in an exponential space mesh how can we learn new and interesting
125120	128760	correlations and work with not just pure states but non pure states and these
128760	133280	are just some of the things Guillaume will tell us about today and maybe as I
133280	137880	advertise your talk and describe the format of Guillaume maybe you can pull
137880	144640	up your slides yep the talk format is the usual you can ask questions in the
144640	150320	comment sidebar box on the right hand side usually or below and I will triage
150320	157600	those questions and ask Guillaume in real time so Q&A is during and after and I
157600	163600	think it's time we get started so it's my pleasure to turn over to you well thank
163600	167600	you for the introductions Lacko and thank you to the invitation and it's very
167600	173640	nice to get to speak to the whole quantum community here really enjoy
173640	179240	watching these seminars so it's my honor to get to talk in one of these so
179240	184320	today I guess I'll be talking broadly about quantum machine learning and some
184320	188720	context comparing it to classical machine learning and deep learning and then
188720	196760	getting into some recent work from various internships and during my PhD on
197320	201840	taking inspiration from classical machine learning to create new types of
201840	209960	quantum models and algorithms so as Lacko mentioned often in quantum machine
209960	216240	learning there is this conception that if we go to if we use a quantum
216240	221440	computer sense we're operating in a exponentially large space and and thus
221720	227120	we should get exponential amounts of power of machine learning power right
227120	231720	but that is that is somewhat of a misconception because for very long
231720	236160	time classical computers and analog classical electronics have been able to
236160	242280	do probabilistic computing right and as we know quantum theory is kind of an
242280	247840	extension or generalization of probability theory to include complex
247840	254200	numbers known as wave functions right and probabilities are obtained from wave
254200	259080	functions by taking the amplitude squared or the absolute value squared of
259080	263120	various these complex numbers and from that we get the probability of various
263120	268640	outcomes right that is known as the Born Rule right but you know on the
268640	275480	classical side we can have mixtures of zero and one right zero or one in a
275520	279720	different combination whereas on the quantum side we have super positions it's
279720	285160	not zero and one it's not zero or one it's a super position with certain values
285160	293320	of complex numbers right so overall I guess the theme of the talk is to you
293320	298040	know take inspiration from classical probability theory and take inspiration
298040	302480	from a subset of machine learning called probabilistic machine learning to come
302480	307440	up with new quantum models because the theories are very much analogous and can
307440	314760	in fact be hybridized as we will see so if you have you know n-pro bits
314760	320440	probabilistic bits they also operate in a space that is exponential right you
320440	324640	have a probability distribution over the space of bit strings and you can have a
324640	330280	mixture of two to the n possible bit strings right and there's many as we'll
330320	335960	see machine learning models that are made to represent such distributions or
335960	341000	generate such distributions and on the quantum side you know for pure states
341000	347880	at least they're written as a wave function of the sort and have two to the
347880	352680	n complex numbers for n qubits right so there's a lot of similarities right
352680	356400	and there's a difference the complex number and the real number that's
356440	362520	important that gives in a sense quantum computing its power over classical
362520	368720	algorithms but instead of you know having this constant competition I guess
368720	373840	between classical computing and quantum computing you know the philosophy at
373840	377760	least of my research is to try to leverage as much classical computing as
377760	383360	possible and including probabilistic computing and hybridize it with quantum
383360	387200	computation in a sense we want to leverage quantum computers for what
387200	391600	they're very they're the best at right and we want to add this such that there's
391600	396280	a value add by using quantum computers hybridized with classical computing so
396280	402720	this is a philosophy and it's a research and you know there are various schools
402720	408240	of thought in quantum computing and this is the one I guess I'm vouching for so
409040	414480	you know what what actually gives quantum computers their power right well you
414480	420440	know there's been various demonstrations that sampling from a unitary circuit that
420440	425600	is quite deep and has a large space-time volume a unitary quantum circuit is
425600	429960	quite difficult for classical computers right one has to do Feynman paths or
429960	435720	tensor networks and what not and you know the difficulty scales exponentially
436200	442200	asymptotically with the volume of space-time right so that that we know
442200	447960	that's the power of quantum computers is to sample from such circuits so how do
447960	453160	we incorporate this exact thing sampling from unitaries and integrate it with
453160	457720	the capabilities of classical modern classical machine learning to obtain
457720	464080	you know something more powerful than either either piece individually right
464120	467080	so quantum computers are becoming more powerful to the point of being
467080	472760	unsimulatable you know I won't get into whether the boundary has been crossed
472760	478520	or not that's a that's an interesting debate on its own but how do you know
478520	482400	how even if we have this power how do you actually leverage this power for
482400	490280	for something you know relevant that is is not just just a demonstration so in
490320	495560	a sense the meta area of focus at least in the in the near term has been quantum
495560	501680	AI right and what is quantum AI I like to subdivide it into two subfields that
501680	506640	are dominant for now there are other subfields that could be analogous to
506640	511520	the subfields of classical AI but for now it seems like the community is focused
511520	517520	on two broad categories and one is quantum enhanced optimization so that is
517760	522680	accelerating classical algorithms of optimization and search using quantum or
522680	530000	quantum inspired dynamics and quantum deep learning and I have you know many
530000	533640	people call these variational algorithms I'll justify my nomenclature in a
533640	539560	second what I consider quantum deep learning is learning quantum representations
539560	544480	of quantum or classical data so there's a lot to unpack there so we're going to
544480	548640	spend a few slides trying to unpack what it means to do to have a representation
548640	553680	or quantum data this is gonna be the focus of my talk today quantum deep
553680	559960	learning learning a multi-layered quantum computation based representation of
559960	564640	quantum or classical data distributions what is a computational representation
564640	569040	of data right or a deep multi-layered computational representation of data
569520	574960	representations right let's let's go back to classical deep representation learning
574960	580920	theory aka deep learning and try to understand a bit of the context so so
580920	584640	deep learning is subset of machine learning subset of AI subset of computer
584640	592000	science and which is of course a subset of science and I guess the gist of it is
592000	598480	that neural networks you know when they learn something they got to be able to
598640	602720	in a sense recreate it you know Feynman said what I cannot create I do not
602720	606240	understand and your favorite deep neural network if you could ask if you could
606240	611200	ask it what it thinks it would probably say something similar like this quote
611200	618320	what do we mean by recreate so here I'm gonna get more rigorous so usually you
618320	624080	have a data set which is set of points sampled from a true distribution p true
624080	630680	of x right and so you have a certain finite set of data points right you
630680	635120	don't have the full distribution you could query you're trying to learn a
635120	643000	approximative model and you're trying to approximate this distribution over a
643000	647280	certain domain of interest that goes beyond the data set itself because you
647280	652120	you already have the data points for that are in the data set but you're trying
652120	656400	to extend it beyond the data set and you have a parametrized hypothesis class
656400	661480	or in classical machine learning we call it a variational distribution a
661480	665640	variational classical probability distribution and five here would
665640	670560	represent a set of parameters right because usually these distributions are
670560	674520	parametrized using something called deep neural nets as we'll see in a second
674520	679000	but the goal is to approximate a true distribution with a variational
679000	687040	distribution and you know the idea is to minimize the discrepancy between the
687040	693160	true distribution and our our variational distribution over the data set and
693160	697360	hope that it extends beyond the data set right so that would be for generative
697360	703120	modeling we're just trying to learn the raw distribution of all our data in what
703120	706400	is called discriminative modeling which includes you know classifiers such as
706480	712080	like labeling a picture of a cat or a dog or regression neural regression which
712080	719840	is trying to get a scalar out of out of data so you know maybe a certain
719840	725240	continuous value instead of a discrete label but in general we have pairs of
725240	730840	inputs and outputs right and discriminative learning is very similar it
730840	734160	can be phrased in probabilistic language as we're trying to learn a
734200	739320	conditional distribution right random variables can be correlated and they can
739320	745280	have what are called conditional distributions hopefully my okay we
745280	750040	can see the last line here so the idea is that deterministic functions such as
750040	754480	most deep neural networks are actually you know they're a subset of this
754480	758880	conditional distributions they're kind of delta functions if you're used to the
758880	764640	delta measure in function space if you integrate over it then you you get the
764640	770640	value y equals f of x right so it's just a very sharp distribution right so
770640	777200	most of classical machine learning or I guess the the popular parts of deep
777200	781520	learning often deal with kind of deterministic point-wise functions
781520	785920	whereas the the more general theory is actually based on probability and
786160	790400	information theory right and that's kind of the roots of machine learning and
790400	794080	what we're trying to get back to with quantum because we are in the early days
794800	798320	where we must understand from first principles what we're doing instead of
798320	803040	just trying stuff and iterating on the engineering of different algorithms
803680	806560	you know willy-nilly we want to be guided in our research
808480	812640	so you know deep learning are algorithms tied to identify patterns in data
813600	818000	you use multi-layered parameterized computations to learn representations of data
818560	823520	representations are multi you know deep representations are multi-step computations
823520	829120	that either take you from your data space to a simplified space or from a simple space to your
829120	836000	data space right so in the case of discriminative learning you're trying to take the input space
836000	841360	say the pictures of cats and go to the label space you know is it cat or dog a single bit
841360	847680	instead of many many pixels right in generative learning you're starting from a very simple set of
848400	853920	randomness say a set of Gaussian samples or a set of random coin flips and trying to turn that
853920	862400	randomness into the randomness over say the the set of pictures of bedrooms right the possible
862400	867520	set of pictures of bedrooms and you're trying to sample new data points from from that data set
868240	875760	in terms of math we say you know we're searching for a sub manifold of your your your space right
875760	882640	and if it's all continuously parameterized it's technically a manifold again this is what I just
882640	887920	described you have some randomness a generative model would go to the some complicated space you
887920	893360	know machine learning folks and deep learning folks really love pictures quantum folks love
893440	899840	wave functions and nick states as we'll see but you know discriminative learning would go to a
899840	906000	simple space and then once it's simplified it's easy to separate out the two class so again
906000	910160	representations every time I see representations don't know freak out it's just parameterized
910160	917040	multi-step computation and deep is multi-layer and the building block is neural networks
917600	926480	so I think I'm going to skip over this theory this is an example of a unsupervised learning
926480	931920	algorithm called a variational autoencoder it's a way to compress data so you go to a compressed
931920	937760	space and by compressing you're going to be forced to decorrelate the data and get a very simple
938960	943680	very simple randomness and you could fit that simple randomness say with Gaussians
943760	948000	and then if you plug it through in a sense the reverse transformation you get your data set
948000	952720	again right and I want to show this because it's going to be very similar to our quantum approach
952720	958560	where you could go in reverse through a quantum classical transformation and then you have a
958560	964160	very simple what is called a latent space a hidden space and then when you want to generate the
964160	971280	data again you go from latent space to the visible space right and these are very cool because you
971600	978400	can in latent space if you just train the network to search for interesting features in general
978400	985040	they can find features that and then you could do kind of logic in latent space so maybe there's
985040	990160	a vector that corresponds to glasses to gender to age and so on and you could play around in
990160	995920	latent space and see what you generate on the other side so you know for quantum for example if
995920	1000800	you have properties of materials you're trying to detect and you're trying to generate new materials
1000800	1006240	or new materials with properties having a latent space that you've detected to play with it can be
1006240	1014480	very useful and of course unsupervised learning itself is also useful in classical and sorry in
1014480	1021520	a discriminative learning because finding a compressed representation is already part of the
1021520	1028560	job to to to separate out classes so imagine we we had you know three different classes and we
1028560	1032960	compressed it to some space that's two-dimensional and then we could go from a two-dimensional
1032960	1039280	space to three different class labels of which color of the blob it corresponds to so again so
1039280	1044880	i'm going to be focused on unsupervised learning but a lot of this actually applies to supervised
1044880	1052320	or discriminative learning so what consists of a good representation i won't go too much into this but
1052880	1059120	at a high level you want the representation capacity are you able to capture or you know
1059120	1067440	reproduce the data set for some value of your parameters of your model is it trainable efficiently
1067440	1073760	right if you have a neural network parameterizing your computational representation to go from
1073760	1079600	complicated to simple space how easy is it to train it with algorithms that are not too closely
1080560	1085040	inference tractability for feedforward neural networks that's very simple but there's other
1085040	1090320	types of models that just doing prediction the prediction step can be computationally costly
1090320	1096560	so that's something to keep in mind and of course that is the advantage of quantum computers is that
1097280	1102080	you know if we incorporate large unitary transformations into our models as we'll see
1102800	1109040	theoretically there are unitary transformations that cannot be executed the prediction or sampling
1109040	1114480	step on a classical computer right so it's a very important part that's why i have this slide
1115920	1119440	generalization power is is the core of machine learning generalization is
1120320	1125600	you know if i fit within my data set will what i've learned extend outside the data set which
1125600	1130880	is important because that is the difference between learning and optimization i sense there's a question
1131760	1138320	oh just i'm getting keen keen awareness um this is more of a curiosity question
1139200	1144800	about the representation capacity it's uh it seems like a really powerful but what can we usually
1145360	1150720	before say running numerical experiments and so forth you know how much can we say or really
1150720	1155360	peer into that for a particular model you've come up with you know what are the kind of tools and
1156000	1159920	techniques and how far can they allow you to can we really say a lot about that
1160800	1168000	about complexity of sampling unitaries uh yeah the representation capacity like what kind of
1168000	1175360	correlations and so forth you will be able to capture potentially this is more of a right and
1175360	1180960	that is going to depend strongly on your your the way you parameterize your your transformation in a
1180960	1186320	sense you're by having a parameterized model you have what is called a hypothesis class and
1186320	1193120	depending on on the various choices you've made you're going to kind of span a sub manifold of
1193120	1200240	states and the idea is that you know what is very popular right now is called the hardware efficient
1201040	1208080	onsots it looks very much like a random quantum circuit like this it's very tightly packed
1208800	1215200	and the idea is that if you look at in the space of possible quantum states it can represent right
1215200	1220880	if all of these transformations were parameterized random you know single and two qubit rotations
1220880	1227040	right then theoretically you know its complexity is growing larger and larger right and in a sense
1228080	1234560	any quantum state that has a complexity uh within that complexity radius you'll be able to reach it
1235120	1240400	but the problem is because you're spanning such a large space your training of your quantum neural
1240400	1245600	network becomes harder and harder um because your hypothesis class is too large so you're
1245600	1252160	searching over too too many possibilities and this is the result known as the the baron plateaus
1252160	1258000	in the quantum neural network landscape or or the quantum version of no free lunch theorem where
1258000	1263440	you can't have a one size fits all representation and that's that's where physicists come in
1263440	1270160	physicists need to have you know good prior knowledge of the domain of application they're
1270160	1275280	trying to do quantum machine learning and to instruct their choice of representation and
1275280	1284560	parameterization to aid in in the the tractability of training um that answers the question yeah so
1285280	1290400	i like that free lunch theorem uh because i guess you know you could try to say well if i have some
1290400	1294960	sort of generators that i use for my model you know what is the reachability of states
1295760	1299920	right since the people ask what is the reachability but i think what you're emphasizing here is that
1299920	1306400	reachability is maybe only a first step uh and maybe having too much reachability sometimes
1306400	1312960	at the moment so there's a trade-off maybe between capacity and efficiency exactly exactly and that
1312960	1319120	that is the no free lunch theorem in a sense so that's lucky for us because uh you know at least
1319120	1324240	for now it seems like physicists will be needed uh in the future when uh we're not going to be
1324240	1331520	out of a job we still need to design architectures um at least for now so let's see let's see let's
1331520	1336320	see where it goes but uh but that's right that's right so uh you know a lot of what i'm going to
1336320	1343280	present today is not necessarily uh architectures for specific domains it's it's more uh a general
1343360	1349760	framework uh based on quantum information theory of how to uh do quantum machine learning or or
1349760	1358480	maybe a a very broad class of parametrizations of of models that are quantum yeah yeah i'm since
1358480	1363040	i've already interpreted you there's interesting this is kind of an unusual question but why don't
1363040	1369120	throw it out here anyhow from martin how much time do you take uh take it take you to learn all this
1369520	1377440	i guess um i mean so okay so i guess backstory uh once there was a conference of machine
1377440	1382560	learning on a monday on a friday night i decided to binge watch lectures at three times speed on
1382560	1389440	youtube on the basics of deep learning i think that was 2016 uh or 2017 something like that
1390160	1395280	and uh since then i've just been reading uh machine learning papers and you know i i guess
1395360	1400800	have a deep math background so it helps uh and then quantum computing itself uh i guess
1400800	1407360	i've been doing since i was 19 and i'm 28 now so gives you an estimate uh it's just always been
1407360	1414000	my passion and uh i uh i went through theoretical physics and uh now i'm here in uh quantum machine
1414000	1421040	learning so i would say four years of of interest in quantum machine learning two to three years
1421040	1426240	serious uh serious focus and it looks like george baron is building on my question which
1426240	1430400	probably gets into a little bit i also wanted to get into which is what are some uh quantitative
1431280	1436240	quantitative metrics for representation capacity yeah that's that's interesting um
1437600	1444480	i guess that's a good that's a good question i would say if you can quantify of in a sense a
1444480	1451120	notion of volume and complexity space um and this is actually you know we're edging on on
1452080	1456160	theoretical physics here because the notion of quantum complexity is interested in interesting
1456160	1462800	in in the theory of ads cft and um you know there's lennard suskin who does a lot of work in this uh
1462800	1469760	in this space uh and uh yeah i mean that's a that's a that's an open question i think i have
1469760	1473680	some intuition uh as to what would be a good metric but that would be an interesting further
1473680	1481040	study you know yeah there's a good quote right on we point correct like with uh with logic we prove
1481040	1489680	with with intuition we discover and that's right here's the guidance that's right cool cool um so i
1489680	1496640	guess i've i've i've gone through these uh this is just some text uh backing up what i've said um
1496640	1502560	so okay so now that we've we have some very brief background and some intuition about deep learning
1502560	1508560	because this is a quantum computing audience so we had to load that up um how can we use you know
1508560	1515040	what we learn taking inspiration from vaes or uh from you know the what is needed to have a good
1515040	1520320	representation to instruct our choice of how we do quantum deep learning so first of all what would
1520320	1525920	be a quantum deep representation right well a classical feed for network in a cartoonish picture
1525920	1531440	this is not the most general formulation but it's a it's a friendly one um you have some input you
1531440	1535600	have some parameters phi and then you have some parameterized output f of x fine for a quantum
1535600	1540720	neural network you have usually a pure state input a unitary evolution that is parameterized in some
1540720	1547680	way and then you have a parameterized hypothesis class of pure states right which is u five times
1547680	1555280	your initial state and uh you know we call the function f the feed forward operation you can have
1555280	1561680	a uh loss functionals returns a scalar that depends on say your label and your your output of your
1561680	1566800	network uh this could be some statistical measure of statistical distance to your data set and you
1566800	1572080	want to find the uh minimum or approximate minimum subject to variations of the parameters of this
1572080	1576720	loss functional so how do we get scalar values out of a quantum computer it gives us a wave
1576720	1581600	function so do we what do we do with it well we have to define a loss operator which is a quantum
1581600	1586320	observable right or Hermitian observable and often we decompose it into small chunks that
1586320	1594720	we can measure independently um and combine later on and our goal similarly to you know in the v the
1594720	1601440	case of vqe and many other variational algorithms is to find uh the minimum subject to variations
1601440	1607920	variational variations the parameters uh the expectation value of this loss operator right
1607920	1613760	sometimes it's called the energy or the Hamiltonian but i like to generalize it to uh you know loss
1614880	1620160	for quantum machine learning so there's just a refresher this is the typical way when trains
1620160	1624800	a vanilla uh what i call vanilla quantum variational algorithms or vanilla quantum neural network
1625920	1631760	you have a loop between a quantum and classical computer and the quantum computer gets an
1631760	1635840	expectation value feeds at the classical computer classical computer has an optimization
1635840	1641200	algorithm that's classical suggests and use values of the parameter and you iterate like this in a
1641200	1646560	sense you know our current quantum computers are restricted in how much quantum depth and uh
1647200	1650880	you know what kind of quantum states they can represent they're restricted in depth because
1650880	1656720	of the noise and so it makes sense that we search over the space uh given this constraint of low
1656720	1662960	depth circuits we should search over the space to find the quantum state and this is this is why
1662960	1668880	this is kind of taking over because for the nisk era or you know early fault tolerance we're going
1668880	1674960	to be searching over the space of states that are uh not too big not not too you know long to
1674960	1680400	quantum compute um so why learn quantum representations in the first place if you if you allow a
1680400	1685680	meal modify uh Feynman's famous quote uh Feynman said you know nature is in classical gamut if you
1685680	1690240	want to make a simulation of nature you better make it quantum mechanical and in our case it's
1690240	1696000	if you want to learn a representation of nature you better make it quantum mechanical right so
1696800	1701440	quantum states and quantum processes i think we've been we've been mentioning this can exhibit
1701440	1707040	high levels of quantum forms of correlation such as entanglement and that's exponentially hard to
1707040	1712640	represent in classical memory right if you have a uh random circuit producing in a highly entangled
1712640	1718240	state it's very hard to approximate it and it's hard to prove uh theoretically without a doubt
1718240	1723760	but every algorithm we've tried to simulate quantum circuits it seems to uh fall flat on its face at
1723760	1732960	some point right um yes um quick question clarification from joe here is the loss calculated
1732960	1740640	by classical computer is the loss calculated so he's talking about the expectation value of the
1740640	1746480	L operator right right it takes actually several samples to estimate the expectation value right
1746480	1751920	you could think of it as like sampling from a distribution if i'm trying to do an estimator
1751920	1759280	of a uh random variable subject to samples from a certain distribution it takes several samples
1759280	1764720	so there's there's like a mini loop in here to estimate the expectation value here and that's a
1764720	1770000	mixture of uh you know doing several measurements on the quantum computer and saving saving the
1770000	1774560	results on the classical computer and then the classical computer can aggregate the the various
1774640	1779280	results to get an average right and maybe to paraphrase you mean you you run that same use
1779280	1783360	circuit with the same inputs the same initial state the same parameters several different
1783360	1790880	measurements yes well i guess the the loss operator may have several uh non-commuting uh
1790880	1799040	sub operators and one wants to get an expectation value of each uh term in the sum and then one
1799680	1804480	adds up all these terms to get an expectation value of the sum so that is called the quantum
1804720	1811600	expectation estimation uh sub routine in a sense and here i kind of abstracted it out
1812320	1818320	but it's it's an extra sub routine there's a mini loop of trying to get a precise estimate
1818320	1822640	it's not to be neglected because if you want you know a 10 to the minus seven precision
1822640	1828240	for an energy it could take you hours on a 10 kilohertz machine for example so it's it's an
1828240	1833360	important thing to consider when designing quantum algorithms that we only have noisy
1833360	1839680	or estimates of our our loss function yeah so basically if you run that um yeah so you
1839680	1845120	break up the L into sub operators which you measure you know you measure by running multiple
1845120	1850400	times you get you don't need the distribution here for this you only need the actual mean value
1851040	1857200	yes right or at least in the in the typical vanilla case but as we'll see there's there's
1857200	1864240	other other variants out there but uh usually the quantum part it's it's hard to get a scalar
1864240	1869360	out of the quantum computer by something else than defining an observable and outcomes of measurements
1870480	1875680	right and how important would read out errors and skew and to read out being in a instance you get a
1876640	1882240	p1 probability 80 but you have to skew because of the loss t1 process and things like that
1882240	1889200	that's right you get a imprecise estimate of your your loss function and uh you need to have classical
1889200	1894480	algorithms on the side to compensate for that imprecision or to choose your optimizer wisely
1894480	1901600	in a way that is robust in noise right and uh i see a lot of questions i i hope i can get to all
1901600	1907600	of these we may we may have to yeah for the end here okay um maybe just quick one here does the
1907680	1910960	quantum advantage come from generating the variational forms
1912160	1918320	um i mean you know i am not claiming a quantum advantage yet but i would say that uh
1919280	1925280	if there if there was a quantum machine learning advantage it would likely come from uh being able
1925280	1930960	to do the inference or prediction step with your model and hence the ability to train it as well
1930960	1935920	so both the training and inference are rendered possible once you have access to a quantum computer
1935920	1941520	if you incorporate a model that has high quantum complexity so a large unitary that we can't
1941520	1948880	simulate classically um yep and i think this next one you're going to talk about which is back propagation
1948880	1956880	you know can you see yes yes yes um okay so how to practically leverage a quantum computing power
1957840	1962320	well for discriminative models for example you can uh you know let's say you prepare a quantum
1962320	1967040	data set because again for now we don't have a quantum internet where we can import uh data
1968320	1973600	that'd be nice someday uh and you evaluate your quantum model let's say you do a feed
1973600	1980400	forward or a unitary parametrize model you get the expectation value of say several observables
1980400	1986000	that becomes a vector you feed that vector to a classical neural network and then it evaluates
1986000	1991840	some some prediction based on this say a label or whatnot and the idea is that you can train both
1991840	1998320	your classical uh part of your network and your quantum part of the network together uh via a
1998320	2004800	form of quantum classical hybrid back prop and the idea is that you know your quantum neural network
2004800	2010240	can can have all sorts of components uh but it could itself be a building block in a sort of meta
2010240	2016240	network between quantum neural networks and classical neural networks and the idea is that if you
2016240	2022640	zoom in on say a little uh sandwich of of nodes here are meta nodes of a deep neural network a
2022640	2027280	quantum neural network and a deep neural network so the let's say a deep neural network or any
2027280	2033520	differentiable computation feeds parameters to quantum neural network uh and then you have
2033520	2037600	the measurement of several observables at the output which you feed to a classical neural network
2037600	2043440	and and then you could do other stuff later on uh and you get your loss function here then you
2043840	2049520	get the gradient of the loss function back propagate uh your gradient classically and
2049520	2058560	effectively what's interesting is that this thing is a actually a itself is is technically
2058560	2063680	an observable on this space if you could you know invert this function but the idea is you do a first
2063680	2069440	order approximation so you get an effective back propagated gradient Hamiltonian which becomes
2069440	2074320	or you call it a Hamiltonian because it's an observable and then it becomes just like taking
2074320	2078480	gradients of a vqe to obtain the gradients of these parameters you just have an effective
2079600	2084720	value of the gradient for a certain value of your your you know all your parameters over here and your
2084720	2090000	loss function and you could take gradients of uh that's with respect to your parameters and you've
2090000	2095360	effectively back propagated the gradient of this value through the q and n and you could keep going
2095360	2099520	and this is important because you don't want to have to do a slight change do your whole
2099520	2105360	chain of computations see how it changed and then backtrack it's it's more scalable uh this way
2107520	2115440	so okay so there's some software uh that does this i have to plug it i mean it's one of my
2115440	2121520	pet projects uh for it's been so for a while uh for now it's uh it's uh interface between
2121520	2126320	cirq and tensorflow there's some open source contributors that are working on quiz kit
2126320	2130560	compatibility so that's going to be exciting for the quiz kit community and we're supporting them
2131840	2137920	but it allows you to you know automate this this training and integrate it into you know
2137920	2143200	advanced machine learning models in tensorflow and you know tensorflow i think has the record of
2144320	2150000	on ibm supercomputers for you know the biggest machine learning computation so i think it's
2150000	2156320	important to uh to ideally integrate uh quantum computers with uh the power at least one of the
2156320	2163760	most powerful frameworks for high performance computing on the classical side um so any question
2163760	2169760	there in that vein um this is an earlier question for me to are there any data sets filled with the
2169760	2176720	quantum machine learning models you can map up some notebooks for a slayer uh that's uh i think
2176720	2181120	that's public that we're working on that and we're trying to work with other you know other uh
2181760	2187280	companies in the space to make sure we we agree on what a form for a data set will be but in general
2188240	2193520	because you can't download quantum data you can't just save you know states because they
2193520	2197680	take exponential space and you don't know how to load them on your quantum computer the data set
2197680	2203360	takes the form of a circuit um or a set of circuits and those define wave functions that you could
2203440	2208480	then do quantum deep learning on and it's something that's uh being worked on but you'll
2208480	2218240	have to stay tuned uh for that thank you cool okay so uh what can one do with hybrid feed forward
2218240	2224800	networks uh i'm going to skip over this uh yeah quickly i guess there's a paper by luke in which
2224800	2230720	is a convolutional neural network which uh are inspired with from the mera if you're in in the
2230720	2238880	know about it uh basically it's luke using the fact that uh if you know your system is
2238880	2245840	translationally invariant so it has some symmetry you reflect that symmetry in your your choice of
2245840	2250400	parameterization of your quantum neural network and so this is just a quantum neural network
2250400	2258000	that has translational invariance and is hierarchical and the idea is that you know maybe you can't do
2258000	2261360	all the quantum layers but maybe you could do only one quantum layer and already you'll
2261360	2265600	you've down sampled the problem you've reduced that dimension dimensionality and you've broken
2265600	2270080	up some entanglement or you've you've like disentangled partially remember for compression
2270080	2275440	you got to decorrelate everything all right so the idea is that you can do you could input
2275440	2280080	various quantum data in batches you could apply various feature maps that are quantum convolutional
2280080	2284880	networks and then you get kind of images from you know all your histograms of samples of your
2284880	2290640	bit strings and following this you could apply classical convolutional layers and you know finish
2290640	2296720	the job with fully connected and at least in our early experiments uh hybrid networks with multiple
2296720	2302560	filters were better than one quantum network and that's without noise so with noise on the device
2302560	2307520	it's even better uh but that's just a an example of discriminative learning i won't go too much
2307520	2312400	into that but in terms of applications it would be for example classifying phases of matter detecting
2312400	2317360	whether something is superconducting or not and the idea is maybe you train on a data set of
2318720	2322080	a material you know is superconducting at certain value of the parameters and temperature
2322880	2327840	and and then you you ask the neural network to detect for another material that you don't know
2327840	2333200	whether it's superconducting or not at certain value parameters so generalizes so that's uh
2333200	2340400	that's one quote-unquote killer app we think for quantum neural networks um yeah so it's just
2340400	2346560	comparing the two with with our old diagram okay so i guess we'll get to the meat of the talk uh
2348080	2355040	i'm not too bad halfway there i guess um so uh how can we extend these insights and how can we
2355040	2361520	hybridize um in a meaningful way with classical machine learning capabilities for quantum machine
2361520	2368160	learning right let's go back to our slide of deep generative modeling we have our data set
2368160	2372800	we have our variational classical distribution we want to minimize our question before we
2373920	2379920	deep dive here uh it's about um nlp and maybe can we use some of this quantum representation and
2379920	2384960	nlp transformers to reduce the huge size of it to increase accuracy i hope that's a
2384960	2390960	maybe a little bit out there question um so i mean we our team has some public work that we've
2390960	2397760	used tensor networks which are you know analogous to quantum circuits in a sense to find factorizations
2397760	2403760	of large matrices uh and we apply them to the transformers and at least in our demo we get a
2403760	2410320	two times speed up uh and of course um you know that'd be great if um such a tensor network could
2410320	2415600	be contracted on a quantum computer uh faster it's not an experiment we've tried yet but um you
2415600	2420960	know it's going to come down to constant speed ups uh you know uh you know our tensor network
2420960	2424960	versus a quantum computer for certain tensor networks the quantum computer is exponentially
2424960	2432640	faster but for other tensor networks it's going to be similar uh potentially um so that is that is
2432640	2439280	a good question um but uh i guess i guess we'll we'll have to see on that side but it's an interesting
2439280	2447120	area of research in a sense uh dimensionality reduction uh using quantum circuits um and uh
2447120	2453120	you know tensor networks are a first step towards that um but it's uh you know it's encouraging to
2453120	2458560	see that cutting edge ml can be improved with quantum or quantum inspired methods at least today um so
2460400	2465920	yeah at least in nlp that's the area that i'm confident saying something that quantum computers
2465920	2478400	would be potentially useful um all right uh so so i mentioned we want uh our data set degree uh
2479200	2484240	you know for our data points in general when you want two uh distributions to agree
2484240	2490240	you do what is called the k l divergence uh it's not a symmetric function so be careful uh
2490240	2496000	you could go you go one way or the other uh between your true distribution your data distribution and
2496000	2502560	your uh variational uh distribution right and it's like here would be the expectation value
2502640	2508000	specter of data of the ratio of logs right so the idea is that to evaluate this kind of gold
2508000	2514560	standard of uh quantum statistic or sorry classical statistical distribution uh we need
2515280	2523600	access to the log of our logarithm of our uh model for any given data point x that we sample
2523600	2531200	from the data right so not every uh classical machine learning model allows you to do to do this
2531200	2537760	right so gans for example don't have an explicit logarithm of the of the density of your generative
2537760	2542400	model that you could query it's implicit it's only you know the discriminator telling you how well
2542400	2547760	you're doing but it's not a notion of log whereas you have let's say you do a bunch of transformation
2547760	2553200	that um that you know uh the determinant of the jacobian you could compute that efficiently
2553840	2558960	right the determinant of jacobian if you if you continuously transform a space right and you
2558960	2564480	had initially a simple Gaussian on that space and you end up with a complicated space you've kind of
2564480	2568800	you know bunched it up and you've done some complicated different morphism you could back
2568800	2576720	track how the notion of volume locally has changed right and uh for any you know value we target here
2576720	2585440	we can kind of invert um the measure in a certain bin here to uh some set of bins over here and we
2585440	2590320	know the value of a Gaussian analytically and so you can compute in a sense somewhat efficiently
2590320	2597040	analytically the um the density of your your probability distribution for any point you query
2597040	2601200	uh this is called a normalizing flow but there's other types of models you know there's energy
2601200	2607760	based models there's auto regressive models there's a whole bunch of of cool models out there but a
2607760	2612240	lot of people know gans because it's like the entry entry level thing because people understand
2612320	2617120	discriminators but so we encourage you to check out other types of generative machine learning
2617120	2623040	and in a sense we're we're looking to have an explicit uh notion of a log uh for reasons that
2623040	2628800	are going to become apparent in a second in the quantum case so how can we extend this philosophy
2628800	2634800	to quantum theory uh you know what's the intersection of quantum theory and probability theory right
2635920	2641040	well there is you know just like in in black holes uh we look at black holes because they're at the
2641040	2646080	intersection of quantum and gravity so they're an interesting test bed well here we look at
2646080	2649920	mixed states because they're at the intersection of probability theory and probabilistic machine
2649920	2654960	learning and quantum theory and quantum machine learning so mixed state in general can be a
2654960	2660320	probabilistic mixture over mixed states these are matrices instead of vectors now so be careful
2661200	2666080	but uh any density operator has what is called a spectral decomposition so it it's always
2666080	2672480	expressible as a mixture of orthogonal pure states and this this mixture sums up to one so it has
2672480	2680160	a probabilistic interpretation so we go from vectors to a density matrix and each element
2680160	2687760	is in complex numbers so how would we represent mixed states so how would we represent the
2687760	2693520	intersection of probability theory and quantum theory well we should have a model that composes
2693520	2700800	a probabilistic model with a quantum model right and that is the idea of quantum probabilistic
2700800	2709520	hybrid deep learning or deep representations hence the title of my talk so as we've seen quantum
2709520	2715280	neural networks are typically unitary feedforward like this and they have a hypothesis class that is
2716080	2723920	pure states we can combine here a classical parametrized probabilistic model that that we
2723920	2729280	can sample and let's say this would flip your qubits you flip your qubits to prepare a bit string
2729840	2734160	then you apply unitary that's parametrized and what you get at the output instead of a
2734160	2742480	parametrized class of pure states is a parametrized class of mixed states right and uh you know your
2742480	2746880	parametrized distribution your state at this point is a diagonal state so it's effectively
2746880	2752080	classical it has no quantum correlations you can try to show this show there's no coherent
2752080	2758720	neutral information exercise and then after that you tack on a unitary which is hard for
2758720	2763440	classical computers to do the idea is we use classical computers and we make them you know we
2763440	2768880	make them sweat right like inference of probabilistic models can be pretty computationally intense
2769440	2773120	uh and then we combine them with unitaries and the quantity
2773920	2778240	and let's talk about how you think we set for capital omega
2779840	2785120	oh yeah so capital omega is just an index over your basis of your Hilbert space it's kind of a
2785120	2790160	general formulation because we actually uh phrase the algorithm both for qubits and for
2790160	2794800	continuous infinite dimensional Hilbert spaces so theoretically could be a an integral or something
2794880	2799760	it's just general math but it's uh it's an index that it's a index set that runs over
2800720	2804000	an index for your your entire basis that spans your whole Hilbert space of interest
2804800	2807840	oh yeah and then you can choose basically any probability over
2809920	2814640	basically anything but there's going to be certain types that are preferential for for training
2814640	2819520	reasons as we'll see uh again you know you could parametrize anything classically but it's not
2819520	2824000	every model that's easy to train again because let's say you need the log and you can't get it
2824000	2830720	or can't get the gradients then it's difficult so as we'll see we can choose wisely how we
2830720	2835760	parametrize things so that we can get nice gradients and can train things because how do you
2835760	2841680	train continuously parametrize the hypothesis class gradient based methods so you use kind of the
2841680	2847520	notion of steepest descent in the landscape of parameter space and maybe one more question
2847520	2853360	I'm told I have to speak a little bit louder so hopefully for sure for I mean let's take the
2853440	2859200	extreme case you take a case where you have a pure like harm mixture like you know you have a pure
2859200	2864960	mixture of all states so I'm guessing that's not very useful one so in a way you want your state to
2864960	2870960	be a little bit mixed that's somewhat pure or are you okay having like a purity of like zero
2871600	2877760	or maximally mixed even though it's so if you were to optimize over architectures so tune
2877760	2882880	and we have some new results that are not in the paper for this talk you could tune how much quantum
2882880	2889840	depth you you you assign to the the unitary um theoretically this this approach could be tuneable
2889840	2894480	in the sense that if the data set that you're trying to represent is purely classical and has no
2894480	2900240	quantum correlation and the identities in the span of your unitary hypothesis class you could
2900240	2906000	learn to just apply the identity and then it's a classical machine learning system if it's a pure
2906000	2911120	state that you're learning this the probabilistic component is useless in a sense because it's
2911120	2915440	going to be all unitary you're just trying to learn a pure state so it's an adaptive way to
2916320	2922080	separate out the task of quantum and classical machine learning of a quantum uh mixed states
2923520	2929120	and it's quite cool because you have one framework where you have as a subset uh classical
2929120	2937120	generative modeling of distributions um so in a sense it it can via self-tuning it could
2937120	2943040	adapt to use no quantum resources or use no classical resources or any you know continuum in between
2944560	2949680	and uh we have time here two more quick questions so are you using mixed states for the input as
2949680	2953520	sort of maybe in practice i guess at this level it's all a little more we're we're going to see
2953520	2961680	that i guess uh it's you could use output or input so yeah um i think i still have a good
2961760	2967440	number of core slides but i guess i'll i'll go through them faster a bit um we can run a little
2967440	2973920	longer okay okay so you know why should we care about quantum mixed states well you know thermal
2973920	2980800	states um are at finite temperature and so you know most systems in nature at finite temperature
2980800	2985200	unfortunately our quantum computers are not at zero temperature so even them themselves must be
2985200	2990240	modeled as mixed states uh if we were to be accurate and experimentalists know this theorists
2990800	2998080	like to say it's a pure state um so you know so the ability to simulate mixed states is is crucial
2998080	3002880	to nature and and the reality is like you know we're trying to use quantum computers to simulate
3002880	3008000	nature but nature itself if you core screen enough you zoom out there's a quantum to classical
3008000	3013120	transition right you know we're used to having classical physics having quantum physics and
3013120	3018160	then there's a continuum in between so the point is to have a set of continuum of models
3018160	3023040	that can model that in between at finite temperatures when quantumness dies down
3023920	3030640	and uh it becomes classical uh or you know when you're very close to being fully quantum right
3031440	3034800	uh most quantum systems are open quantum systems i've mentioned this and
3035520	3041120	for various reasons subsystems of of quantum states uh have are mixed states because of
3041120	3044880	entanglement if you take a reduced state of a pure entangled state you get a mixed state
3045520	3051200	um so just looking at patches of things at a time to model them it's important so
3052160	3056000	what sort of mixed states in nature can we variationally simulate using something like this
3056000	3061840	well thermal states is of of great interest because they're they're omnipresent so the algorithm
3061840	3066320	that leverages quantum probabilistic generative models to model thermal states is uh variational
3066320	3078160	quantum thermalization and or vqt uh and uh so the problem is given a Hamiltonian and h and a
3078160	3084240	target temperature uh one over beta then generate the thermal state which is the exponential that
3084240	3090480	is normalized like this and this is the partition function and the idea is okay well we'll use one
3090480	3099200	of our magic models of classical probabilistic distribution and a unitary uh and how are we
3099200	3104880	going to converge to uh the thermal state well thermal states are the minimum of something
3104880	3112880	called free energy right so free energy is you know uh roughly ignoring temperature energy minus
3112960	3120160	entropy right uh so we can evaluate the energy you know just like in vqe of our model
3120720	3127040	and subtract the entropy right so how do we get the entropy well because unitaries can serve
3127040	3131920	entropy the actually actually the entropy comes strictly from our classical part of the model
3131920	3137440	and if your classical model has ways to get gradients of entropy you're in business or
3137440	3142400	you know sometimes it's simple enough you could get it analytically and this is equivalent defining
3142400	3147520	you know the minimum of the relative entropy between our model and the thermal state so you
3147520	3151760	know we know that the unique minimum of this function is when the two states match so if we
3151760	3157760	do our job well and we parameterize things well and find the absolute optimal states uh then uh
3157760	3163840	you know we've got the jackpot um state of minimal for energies thermal state so how do we
3163840	3167840	parameterize our quantum probabilistic generative model i've been pretty abstract now so we're just
3167840	3174160	going to zero it in slightly uh well uh the motivation for this work was to take inspiration
3174160	3179600	from recent work by uh open ai and such on modern versions of energy based models where
3180400	3185840	one it's it's now it's taking inspiration from physics right so you define an energy function
3185840	3189840	using a classical neural network let's say from the space of bit strings or continuous values to
3190400	3195680	to a scalar and you could use various algorithms that leverage gradient information such as
3195680	3202000	Hamiltonian Monte Carlo or stochastic gradient Langevin dynamics uh you know all all there's a
3202000	3207280	bunch of open source frameworks to to do the this part you could sample the landscape by in a sense
3207280	3213440	having a noisy ball traverse this landscape and you get samples of the exponential this way or the
3213440	3218560	Boltzmann distribution known in physics it's a classical Boltzmann distribution so you parameterize
3218560	3225840	the energy and why why is that going to be useful well uh it has all sorts of compositionality
3225840	3233040	perks it's it's very good it's comparative with GANs this is work by open ai 2019 so how do we
3233040	3239360	leverage these models and integrate them with quantum computers well so you know what if we had
3239360	3243280	our probabilistic part of our model was a classical energy based model like this so it's
3243280	3249280	parameterized energy function and then our distribution is a Boltzmann distribution again
3249280	3255760	well if you you you could define a diagonal operator which is the log after you flip some bits
3256560	3261600	which is your energy function on the diagonal and what you get if you do some math with some
3261600	3267760	unitaries and exponentials is that you've just parameterized a operator the diagonal is parameterized
3267760	3274400	by a neural network and and the total operator is this conjugation of a unitary with this diagonal
3274400	3281200	operator so you've you've parameterized a Hamiltonian operator and your hypothesis class is a set of
3281200	3286160	thermal states so in a sense you you know you know you're targeting a thermal state so you might as
3286160	3295200	well have a hypothesis class of thermal states right um okay so if we have this assumption
3295200	3300160	that we're using an energy based model how do the gradients work out well there's a bit of math
3300160	3307600	involved i've skipped many lines but it is possible to sample it essentially you have to get bit
3307600	3316400	strings at the output of your model and you can evaluate you can compare the value of the energy
3316400	3322640	of your your bit strings minus the the energy of your model and you could also evaluate gradients of
3322640	3327600	your your model if it's parameterized by a neural network and you you do the sampling which only
3327600	3336720	depends again on sampling from uh from your uh your model p theta of x and you have sampling
3336720	3342400	algorithms and you can evaluate gradients in a sense it's an analytic way to guarantee that your
3342400	3348880	estimates of your gradients are unbiased um and how do you get gradients for the quantum part
3348960	3353120	well the quantum part is just the usual i hope you've seen this in other talks and
3354240	3358160	i don't have time to cover it unfortunately today it's the parameter shift rule right
3358800	3363520	which is how you take gradients in the vqe which is how do you take gradients of a
3363520	3368480	a unitary a state fed through a unitary and an expectation value so i won't cover that but
3368480	3374000	it's very standard it's you know a standard in the software framework and there's various papers
3374000	3381040	that use this um it's a cool theory but you know does it work right uh the answer is yes you know
3381040	3386480	if you if you have a target thermal states you can uh do a reconstruction like this this is for
3386480	3391440	some heisenberg spin model we use very simple classical distribution here it was just Bernoulli
3391440	3396560	so random coins uh and and the quantum computer could do a lot of work and and learn to represent
3396560	3401680	a thermal state we've done much larger systems but you know a jarbled set of pixels is not necessarily
3401680	3407280	the most aesthetic thing so we we choose to feature the smaller systems but uh we've scaled
3407280	3413120	things up quite a bit and uh the idea is um you know the function we're optimizing is relative
3413120	3419280	free energy but the other metrics of quantum statistical distance uh also converge uh so
3420160	3427520	uh it seems to work um we've also tried some set of fermionic systems and bosonic systems
3427520	3433120	for example a simple you know toy model of a superconductor that's uh bosonic uh sorry Gaussian
3433120	3438960	fermionic so it's quite simple we can plot the correlation functions the target this is at iteration
3438960	3446960	zero and it converges by iteration hundred of gradient descent pretty well um so this is actually
3446960	3452160	a new result i i'd like to feature that's not in the paper uh but it's coming in the second version
3452160	3458880	of it uh can we tune how much quantum versus classical resources we use right so suppose i
3458880	3463360	look at this heisenberg model and i look at after training how how well i do in terms of
3463360	3467440	trace distance and fidelity depending on the temperature and the number of quantum layers
3467440	3476080	i use ah well we see there's certain sets of temperatures uh that uh you know you need more
3476080	3482160	quantum layers to to model them right and it's not necessarily you know at this point it becomes
3482160	3488160	trivial uh at this point there's a nice balance between quantum and classical resources and this
3488160	3493440	is the fidelity is trace distance uh but uh this is kind of what you you'd like to do right you
3493440	3498400	want to use as little quantum resources as possible uh in order to have an accurate representation
3498400	3502880	of a state so this is something we started investigating but it's uh you know it maybe
3502880	3507280	has some deep implications about what's the true quantum complexity of a quantum machine learning
3507280	3514960	problem uh and uh i guess uh you know uh please take a look at these qr codes there's links to
3514960	3520000	various notebooks and you know i've been advertising tensor flow quantum but there's there's obviously
3520000	3524960	you know implementations in quiz kit from the community uh shout out to jack seroni and uh
3524960	3530880	you know the tensor flow quantum implementations by my collaborator antonio martinez um and three
3530880	3537200	two one take a picture on youtube or whatnot and look at the uh websites for these notebooks
3538400	3543840	so the final component is more machine learning it's less quantum simulation is how do we use vqt
3544720	3549040	to do quantum machine learning so if we're given quantum mixed state data how do we
3550560	3556480	learn from quantum mixed state data so again we're going to use our quantum Hamiltonian based model
3556480	3562960	because for reasons that are going to become apparent in a second so we call the task of
3562960	3570240	learning to replicate right we want an approximate density matrix that approximates a data density
3570240	3574560	matrix so a data density matrix could be itself a mixture of a bunch of density matrices we're just
3574560	3579520	trying to approximate this thing and we want to find a set of parameters such that for the optimal
3579520	3585360	parameters our hypothesis class approximates this density matrix and we assume we have access to the
3585360	3591520	quantum form of the data okay and the idea is if you use a quantum Hamiltonian based model
3592160	3601040	and you aim to minimize now the relative entropy in reverse from last time uh what you get is if
3601040	3603920	you ignore the terms that don't depend on your parameters you get something called the cross
3603920	3610560	entropy which is this the trace of stigma which is your data log row right and again because we've
3610560	3617120	parametrized our hypothesis class in terms of its logarithm its quantum logarithm uh we can evaluate
3617120	3623520	this energy this it's called modular energy or modular free energy and modular Hamiltonian is
3623520	3630000	just a name for the log of a density matrix okay and so we're trying to learn a log of a density
3630000	3634880	matrix such that the exponential replicates our data set and how you do this you plug your data
3634880	3641200	you run it in reverse through your unitary of your quantum probabilistic model you sample it and then
3641200	3647440	you get expectation values of the diagonal operator and this could be parametrized with a neural network
3647440	3652800	so you can have more computation here the extra term here is all on the classical computer
3653520	3658480	turns out you could also get gradients for these i won't go too much into it the quantum part is
3658480	3664320	again parameter shift but these gradients again if you have a differentiable function for your energy
3665760	3670560	you know like a neural network then you can evaluate you could sample these gradients and
3670560	3675600	it's unbiased which is really cool that that's really important that we could get good estimates
3675600	3684640	of the gradients and you know it works out if you don't use enough quantum or not enough layers
3684640	3690320	of your quantum computer or not enough complexity of your classical distribution sometimes it
3690320	3697760	doesn't work well so for various temperatures we've tested this and i guess this is a this is
3697760	3700960	you know there's many things you could do once you have unsupervised learning for example you
3700960	3707200	could learn a compression code so here we actually applied hopefully some of you know about bosonic
3707200	3712400	quantum computing but theoretically could be applied to other forms that are not qubits
3712400	3718320	and here we learn a compression code where we could throw you know 40 percent of a harmonic chain
3719280	3725280	in the compressed space and still reconstruct the states so this is the error matrix of the density
3725280	3731920	matrix point seven we start seeing errors and if you throw 90 percent of stuff out things go bad
3731920	3739120	and there's theory that you can find the logarithm modes the modular modes of the system
3739120	3743760	and so we checked it with theory that's why we looked at the system thank you but that's it that's
3744480	3751760	corpse there sorry sorry what are the x and y axes on that curve again this curve on the on the
3751760	3759040	left oh on this side so this is the density matrix it's uh it's the discrepancy between
3759680	3764240	the so we go to compressed space it's like an auto encoder we go to compressed space
3764240	3772560	and then we throw out uh what is like so okay so we we do we learn a vqt and the latent model is a
3772560	3779040	product of individual um thermal states of harmonic oscillators right and those are like
3779040	3786080	quantum forms of gaussians which is kind of cool and we throw out the lower entropy latent modes
3786080	3792960	okay because the entropy represents a harmonic oscillator sorry or when you say a mode you mean
3793600	3799360	harmonic oscillator of a harmonic oscillator yeah so this is in this is for say a bosonic
3799360	3804640	continuous variable quantum computing and i did most of my time and continuous variable stuff
3804640	3811360	before so myself as well in theoretical physics uh this is similar to a calculation of the
3811360	3817920	hawking effect actually um i that's a whole two hours i won't go into that but actually here's
3817920	3822320	the interesting thing i have this in my summer school lectures that are up and coming uh there
3822320	3826800	are only two types of physicists those for whom all of physics is qubits and those for whom all
3826800	3834400	of physics is oscillators i try to i try to uh play on both sides so uh hopefully someday
3834400	3841440	we can have hybrid computers that'd be cool through everyone's that's right um so yeah so
3841440	3848400	we agree with theory here um i could explain how this is related to the hawking and unruh effects
3849680	3855040	but uh that would take some time but it's it's an interesting uh thing that quantum machine
3855040	3861200	learning could theoretically understand or learn an analog of the hawking or unruh effects that you
3862160	3868640	there exists a certain set of modes that an observer feels a thermal statistical
3869440	3874800	fluctuations of the vacuum so this was the ground state we plug it in and if you transform it then
3874800	3878960	it becomes a product of thermal states and instead of Fourier modes it's like these weird
3879040	3886320	squished modes uh of the lattice so it's kind of information theoretic uh eigen modes instead of
3886320	3891520	you know we're used to eigen modes in physics like the resonance but here it's kind of uh
3891520	3897680	the resonance of of the log uh Hamiltonian which is the modular Hamiltonian and uh this brings us
3897680	3905680	actually to the end uh of the talk and uh oh i have luckily haven't exceeded too much uh so we do
3905680	3912160	have time for questions i guess but i just want to conclude i guess uh you know this is the beginning
3912160	3917120	of a whole research program it's an exciting area and you know by starting from basics of
3917120	3921840	information theory right we just started thinking about relative entropy and inspiring ourselves
3921840	3926720	from physics we have discoveries and machine learning and hopefully now we could apply this back
3926720	3932000	to the physics right so it's a feedback loop between physics and machine learning and it's
3932000	3939440	that's a big part of the philosophy of our team at x uh yeah thank you yeah thank you very much um
3939440	3942880	i think there were some questions during the talk that i didn't get to so maybe i'll
3942880	3949920	gonna run up the chat here and get back to them again um okay i guess i'll just do a
3949920	3956720	shout out to anntario my collaborator at waterloo uh he was google and x and jacob was instrumental
3956720	3962640	to a lot of the vqt and and qmhl work and uh you know did a lot of the work there uh as well
3962640	3969840	so big shout out to them uh but uh yeah so any questions in the chat i've seen a lot of questions
3969840	3976080	in the chat here so let's uh let's get let's see how many how many we can answer i guess all right
3976080	3982960	let's start with the latest one which is about temperature i think the question is um now is
3982960	3987840	there a sense of critical temperature here relative to some sort of phase transition
3989120	3993120	the you know the temperatures where you get noisy data closely the critical temperature
3993120	3997680	some sort of phase transition in this innovative system or is that well i don't know if we
3997680	4003200	purposely chose a system where we knew there was a phase transition but we can kind of see that
4003200	4009200	there's different uh regimes where you need more entanglement or need less entanglement right so
4010000	4016320	so seeing how many layers you need to represent a quantum state could be like seeing a dip in that
4016880	4021680	could could be a way to detect different phases or quantum phases of matter i guess like you know
4021680	4026560	regimes of parameter space that have very strong entanglement and regimes that are you know
4026560	4034320	slightly you know almost trivial um but uh yeah i'm not sure if we purposely picked a system
4034320	4038720	that we knew there was a phase transition we just observed this data for now but um maybe
4038720	4047520	something to do uh better on yeah um let's see this one uh this one is about universal estimators
4047520	4053440	basically can uh q and n's the quantum neural nets be used to imitate this kind of behavior
4054000	4059520	i guess and we're saying you know given that three layer neural nets are regardless universal
4059520	4065680	estimators in classical machine learning yeah that's a that's a good question so i guess you know
4065680	4071520	you want if you have a universal functional approximator um then you know theoretically you
4071520	4078240	can have a a universal uh you know you span the space of functions that you could represent
4078240	4085200	of course any classical computation can be embedded uh if you write it out as a reversible
4085200	4090720	classical computation using many extra registers and you keep the whole history of the computation
4090800	4095360	if it's not reversible functions you can embed that right in quantum computations
4095360	4101360	with toffoli gates instead of hand and so on and some of work several years ago i i showed how to
4101920	4106720	take you know typical classical neural networks and make quantum circuits that implement the
4106720	4112720	classical neural network in superposition um so the idea is yes i think you can use quantum
4112720	4118800	neural networks to do the classical probabilistic machine learning components um though so far at
4118800	4124080	least from uh the current state of the art of the theory it seems like quantum computers will
4124080	4132400	have a polynomial speedup for inference probabilistic inference similar to grover speedup uh and that of
4132400	4136720	course if we're competing with extremely large classical computers will be mostly relevant when
4136720	4142560	quantum computers are of a size comparable to the square root of our largest supercomputer
4142800	4150000	uh and um that is yeah that's i guess that's uh that's my answer so for now i guess the
4150000	4154400	most practical approach is to use classical algorithms and classical computers for the
4154400	4159680	classical component and use quantum computers for the truly quantum component which is the unitary
4160800	4164000	that seems like a very nice sensible thing there is a question this one is
4164960	4169760	interesting i think it's more of an opinion question maybe um we know that quantum Fourier
4169760	4176000	transforms very t and usually digital computing um does it have a role in machine learning is it
4176000	4181520	similar in here according to what is the relationship with advantage or the Fourier
4181520	4190560	transform right so i guess here we parametrized our quantum neural network uh as a general uh
4190560	4198240	bosonic um what is called book all above or Gaussian transformation and the discrete Fourier
4198240	4203920	transform is a subset of such transformations and here we we learned these transformations so
4204640	4210080	uh technically uh if we fed the whole system and we asked it to find the eigen modes and if
4210080	4215440	we had a thermal state of this system say via vqt and then we fed it to quantum modular
4215440	4220320	Hamiltonian learning these modes would be the Fourier modes because we know the eigen modes of
4220320	4227760	of this Hamiltonian right we know how to decompose this this Hamiltonian uh into a sum of individual
4228480	4234640	uh you know number operators um and it's the same you know finding this book all above
4234640	4240480	transformation is what i mean by it's related to the unrefect uh calculation qft in curved
4240480	4245520	spacetime i know there are some chat messages that were doubting that but uh i did my masters
4245520	4252800	in quantum field theory in curved spacetime so you can trust me on that one uh but uh great so
4253440	4262960	and maybe for the final question here um and taking the extra time um if we want to do research
4262960	4266880	in this field where should we start or what should be out of the direction of research
4268080	4274080	right i mean that's a that's a good question i guess uh you yourself have gone through this
4274080	4280560	position not you know four years ago i think you mentioned right right right right so i guess in my
4280560	4287840	case i started you know i started with uh the open you know source uh massively online courses mooks
4287840	4292240	i just listened to that listened to a few of them and then i progressed to i wish i had all my
4292240	4297360	textbooks here but uh they're they're back there but uh the good fellow the in good fellows textbook
4297360	4304400	um inventor of gans and then uh murphy kevin murphy uh a goobler uh he did uh what i called
4304400	4310000	a kind of the nielson and schwang or the bible of probabilistic machine learning and i think there's
4310400	4314720	mckay there's information theory for machine learning that's if you want the textbook routes
4315760	4321280	otherwise i think with time as the field stabilizes i guess because it's been moving so fast everybody
4321280	4327280	who's involved in it is just cranking out papers rather than creating coursework uh there will be
4327280	4334720	coursework um i could link a you waterloo course uh that i gave some guest guest lectures at that
4335680	4341840	that featured some quantum machine learning uh but uh overall i would say it's important to
4341840	4345120	understand the theory of classical machine learning at the fundamental level because
4346080	4351920	you know similar to hardware engineering we're at the fundamental level of engineering a new
4351920	4357280	computing stack so on the theory side we're re-engineering a whole algorithm stack so we
4357280	4362400	got to start again from first principle so you know you have to trace back to like papers from
4362400	4367040	the 80s of machine learning and and the fundamentals and then and then work your way back to the
4367040	4373120	modern uh modern thing so i would say the the modern ml stuff is flashy and and and fun to
4373120	4378400	stay up to date but i would say you know take the time go back to the the core old literature
4378400	4385520	you know the foundations so um yeah uh mooks and then textbooks is the way to go that's
4385520	4391760	that's what i did and here i am so uh and then maybe i can just add that now they're
4391760	4397280	summer school classes coming online so i mentioned the quantum information kiscuit one i think there's
4398080	4402960	a touching of ml and things like that in the last two lectures in quantum chemistry vqe's
4402960	4409040	definitely on there so fantastic anyone interested in addition to to what you said you know we can
4409040	4416160	add that um so i think it is that time that i get thank you again and thank the listeners
4416160	4423040	for joining the quantum live seminar series uh we're back this friday um i will mention next
4423040	4428240	week we're back so this at the same time uh continuing with the talk by antony miss capo from
4428240	4435440	idea on quantum chemistry we could talk about variation of quantum eigen solvers q a o and
4435440	4440080	things like that on chemistry things so that will be a very nice uh follow up to your talk
4440880	4448080	gion and uh thank you thank you for inviting me it's uh it's been an honor and uh hopefully the
4449040	4454240	quantum community is interested in quantum machine learning now uh so i've done my job
4454240	4464080	there all right thank you so much follow gion on twitter quantum bird that's right all right
4464080	4469920	so any final words and otherwise thank you and we'll see you next week uh that's it for me
4469920	4475040	thanks again for tuning in and uh stay home stay safe everyone and uh thank you all right
4476000	4479680	it was a pleasure gion we'll see you soon guys take care cheers
