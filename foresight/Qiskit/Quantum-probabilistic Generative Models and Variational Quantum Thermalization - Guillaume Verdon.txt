global digital summer school on quantum computing and I see we have reposted
the link all right so oh Sausalito California that's where it's very close
to where I grew up now we are thrilled to roll out the latest episode of the
quantum seminar series dedicated to the research and academic communities this
this seminar takes place every Friday at noon Eastern time right now at this hour
on the Kiskin YouTube channel and I'm delighted to see so many of you already
tuned in I'm your host like a minute from IBM quantum research and today I have
privilege of hosting young we're done from Google X and and also from the
Institute of quantum computing at Waterloo young will present some very
nice results and hello young how are you today hi is that this let go happy to
be here yeah it's a pleasure to see you last time we met I think it was right
after March meeting was cancelled and we were at March meeting so I'm glad
that we can continue the discussions and have everyone join us today on the call
that's right the science goes on to multistimes but that's right indeed
indeed I hope you've been doing well again in addition to being a social
media celebrity is a human though is a PhD candidate at the University of
Waterloo at the Institute of quantum computing I had a very nice visit there
recently so great place he is also a research scientist at X Google X or
alphabets research and development lab before this Guillaume briefly worked at
Google AI quantum and was one of the co-founders of TensorFlow and project he
holds a master's in math and quantum input from you Waterloo as well as in
honors math and physics degree from a given and university and I think we have
some very interesting questions to discuss today I think as many of you know
quantum computing operates in an exponential space so how is classical
machine learning and learning classical distribution for instance which also
operates in an exponential space mesh how can we learn new and interesting
correlations and work with not just pure states but non pure states and these
are just some of the things Guillaume will tell us about today and maybe as I
advertise your talk and describe the format of Guillaume maybe you can pull
up your slides yep the talk format is the usual you can ask questions in the
comment sidebar box on the right hand side usually or below and I will triage
those questions and ask Guillaume in real time so Q&A is during and after and I
think it's time we get started so it's my pleasure to turn over to you well thank
you for the introductions Lacko and thank you to the invitation and it's very
nice to get to speak to the whole quantum community here really enjoy
watching these seminars so it's my honor to get to talk in one of these so
today I guess I'll be talking broadly about quantum machine learning and some
context comparing it to classical machine learning and deep learning and then
getting into some recent work from various internships and during my PhD on
taking inspiration from classical machine learning to create new types of
quantum models and algorithms so as Lacko mentioned often in quantum machine
learning there is this conception that if we go to if we use a quantum
computer sense we're operating in a exponentially large space and and thus
we should get exponential amounts of power of machine learning power right
but that is that is somewhat of a misconception because for very long
time classical computers and analog classical electronics have been able to
do probabilistic computing right and as we know quantum theory is kind of an
extension or generalization of probability theory to include complex
numbers known as wave functions right and probabilities are obtained from wave
functions by taking the amplitude squared or the absolute value squared of
various these complex numbers and from that we get the probability of various
outcomes right that is known as the Born Rule right but you know on the
classical side we can have mixtures of zero and one right zero or one in a
different combination whereas on the quantum side we have super positions it's
not zero and one it's not zero or one it's a super position with certain values
of complex numbers right so overall I guess the theme of the talk is to you
know take inspiration from classical probability theory and take inspiration
from a subset of machine learning called probabilistic machine learning to come
up with new quantum models because the theories are very much analogous and can
in fact be hybridized as we will see so if you have you know n-pro bits
probabilistic bits they also operate in a space that is exponential right you
have a probability distribution over the space of bit strings and you can have a
mixture of two to the n possible bit strings right and there's many as we'll
see machine learning models that are made to represent such distributions or
generate such distributions and on the quantum side you know for pure states
at least they're written as a wave function of the sort and have two to the
n complex numbers for n qubits right so there's a lot of similarities right
and there's a difference the complex number and the real number that's
important that gives in a sense quantum computing its power over classical
algorithms but instead of you know having this constant competition I guess
between classical computing and quantum computing you know the philosophy at
least of my research is to try to leverage as much classical computing as
possible and including probabilistic computing and hybridize it with quantum
computation in a sense we want to leverage quantum computers for what
they're very they're the best at right and we want to add this such that there's
a value add by using quantum computers hybridized with classical computing so
this is a philosophy and it's a research and you know there are various schools
of thought in quantum computing and this is the one I guess I'm vouching for so
you know what what actually gives quantum computers their power right well you
know there's been various demonstrations that sampling from a unitary circuit that
is quite deep and has a large space-time volume a unitary quantum circuit is
quite difficult for classical computers right one has to do Feynman paths or
tensor networks and what not and you know the difficulty scales exponentially
asymptotically with the volume of space-time right so that that we know
that's the power of quantum computers is to sample from such circuits so how do
we incorporate this exact thing sampling from unitaries and integrate it with
the capabilities of classical modern classical machine learning to obtain
you know something more powerful than either either piece individually right
so quantum computers are becoming more powerful to the point of being
unsimulatable you know I won't get into whether the boundary has been crossed
or not that's a that's an interesting debate on its own but how do you know
how even if we have this power how do you actually leverage this power for
for something you know relevant that is is not just just a demonstration so in
a sense the meta area of focus at least in the in the near term has been quantum
AI right and what is quantum AI I like to subdivide it into two subfields that
are dominant for now there are other subfields that could be analogous to
the subfields of classical AI but for now it seems like the community is focused
on two broad categories and one is quantum enhanced optimization so that is
accelerating classical algorithms of optimization and search using quantum or
quantum inspired dynamics and quantum deep learning and I have you know many
people call these variational algorithms I'll justify my nomenclature in a
second what I consider quantum deep learning is learning quantum representations
of quantum or classical data so there's a lot to unpack there so we're going to
spend a few slides trying to unpack what it means to do to have a representation
or quantum data this is gonna be the focus of my talk today quantum deep
learning learning a multi-layered quantum computation based representation of
quantum or classical data distributions what is a computational representation
of data right or a deep multi-layered computational representation of data
representations right let's let's go back to classical deep representation learning
theory aka deep learning and try to understand a bit of the context so so
deep learning is subset of machine learning subset of AI subset of computer
science and which is of course a subset of science and I guess the gist of it is
that neural networks you know when they learn something they got to be able to
in a sense recreate it you know Feynman said what I cannot create I do not
understand and your favorite deep neural network if you could ask if you could
ask it what it thinks it would probably say something similar like this quote
what do we mean by recreate so here I'm gonna get more rigorous so usually you
have a data set which is set of points sampled from a true distribution p true
of x right and so you have a certain finite set of data points right you
don't have the full distribution you could query you're trying to learn a
approximative model and you're trying to approximate this distribution over a
certain domain of interest that goes beyond the data set itself because you
you already have the data points for that are in the data set but you're trying
to extend it beyond the data set and you have a parametrized hypothesis class
or in classical machine learning we call it a variational distribution a
variational classical probability distribution and five here would
represent a set of parameters right because usually these distributions are
parametrized using something called deep neural nets as we'll see in a second
but the goal is to approximate a true distribution with a variational
distribution and you know the idea is to minimize the discrepancy between the
true distribution and our our variational distribution over the data set and
hope that it extends beyond the data set right so that would be for generative
modeling we're just trying to learn the raw distribution of all our data in what
is called discriminative modeling which includes you know classifiers such as
like labeling a picture of a cat or a dog or regression neural regression which
is trying to get a scalar out of out of data so you know maybe a certain
continuous value instead of a discrete label but in general we have pairs of
inputs and outputs right and discriminative learning is very similar it
can be phrased in probabilistic language as we're trying to learn a
conditional distribution right random variables can be correlated and they can
have what are called conditional distributions hopefully my okay we
can see the last line here so the idea is that deterministic functions such as
most deep neural networks are actually you know they're a subset of this
conditional distributions they're kind of delta functions if you're used to the
delta measure in function space if you integrate over it then you you get the
value y equals f of x right so it's just a very sharp distribution right so
most of classical machine learning or I guess the the popular parts of deep
learning often deal with kind of deterministic point-wise functions
whereas the the more general theory is actually based on probability and
information theory right and that's kind of the roots of machine learning and
what we're trying to get back to with quantum because we are in the early days
where we must understand from first principles what we're doing instead of
just trying stuff and iterating on the engineering of different algorithms
you know willy-nilly we want to be guided in our research
so you know deep learning are algorithms tied to identify patterns in data
you use multi-layered parameterized computations to learn representations of data
representations are multi you know deep representations are multi-step computations
that either take you from your data space to a simplified space or from a simple space to your
data space right so in the case of discriminative learning you're trying to take the input space
say the pictures of cats and go to the label space you know is it cat or dog a single bit
instead of many many pixels right in generative learning you're starting from a very simple set of
randomness say a set of Gaussian samples or a set of random coin flips and trying to turn that
randomness into the randomness over say the the set of pictures of bedrooms right the possible
set of pictures of bedrooms and you're trying to sample new data points from from that data set
in terms of math we say you know we're searching for a sub manifold of your your your space right
and if it's all continuously parameterized it's technically a manifold again this is what I just
described you have some randomness a generative model would go to the some complicated space you
know machine learning folks and deep learning folks really love pictures quantum folks love
wave functions and nick states as we'll see but you know discriminative learning would go to a
simple space and then once it's simplified it's easy to separate out the two class so again
representations every time I see representations don't know freak out it's just parameterized
multi-step computation and deep is multi-layer and the building block is neural networks
so I think I'm going to skip over this theory this is an example of a unsupervised learning
algorithm called a variational autoencoder it's a way to compress data so you go to a compressed
space and by compressing you're going to be forced to decorrelate the data and get a very simple
very simple randomness and you could fit that simple randomness say with Gaussians
and then if you plug it through in a sense the reverse transformation you get your data set
again right and I want to show this because it's going to be very similar to our quantum approach
where you could go in reverse through a quantum classical transformation and then you have a
very simple what is called a latent space a hidden space and then when you want to generate the
data again you go from latent space to the visible space right and these are very cool because you
can in latent space if you just train the network to search for interesting features in general
they can find features that and then you could do kind of logic in latent space so maybe there's
a vector that corresponds to glasses to gender to age and so on and you could play around in
latent space and see what you generate on the other side so you know for quantum for example if
you have properties of materials you're trying to detect and you're trying to generate new materials
or new materials with properties having a latent space that you've detected to play with it can be
very useful and of course unsupervised learning itself is also useful in classical and sorry in
a discriminative learning because finding a compressed representation is already part of the
job to to to separate out classes so imagine we we had you know three different classes and we
compressed it to some space that's two-dimensional and then we could go from a two-dimensional
space to three different class labels of which color of the blob it corresponds to so again so
i'm going to be focused on unsupervised learning but a lot of this actually applies to supervised
or discriminative learning so what consists of a good representation i won't go too much into this but
at a high level you want the representation capacity are you able to capture or you know
reproduce the data set for some value of your parameters of your model is it trainable efficiently
right if you have a neural network parameterizing your computational representation to go from
complicated to simple space how easy is it to train it with algorithms that are not too closely
inference tractability for feedforward neural networks that's very simple but there's other
types of models that just doing prediction the prediction step can be computationally costly
so that's something to keep in mind and of course that is the advantage of quantum computers is that
you know if we incorporate large unitary transformations into our models as we'll see
theoretically there are unitary transformations that cannot be executed the prediction or sampling
step on a classical computer right so it's a very important part that's why i have this slide
generalization power is is the core of machine learning generalization is
you know if i fit within my data set will what i've learned extend outside the data set which
is important because that is the difference between learning and optimization i sense there's a question
oh just i'm getting keen keen awareness um this is more of a curiosity question
about the representation capacity it's uh it seems like a really powerful but what can we usually
before say running numerical experiments and so forth you know how much can we say or really
peer into that for a particular model you've come up with you know what are the kind of tools and
techniques and how far can they allow you to can we really say a lot about that
about complexity of sampling unitaries uh yeah the representation capacity like what kind of
correlations and so forth you will be able to capture potentially this is more of a right and
that is going to depend strongly on your your the way you parameterize your your transformation in a
sense you're by having a parameterized model you have what is called a hypothesis class and
depending on on the various choices you've made you're going to kind of span a sub manifold of
states and the idea is that you know what is very popular right now is called the hardware efficient
onsots it looks very much like a random quantum circuit like this it's very tightly packed
and the idea is that if you look at in the space of possible quantum states it can represent right
if all of these transformations were parameterized random you know single and two qubit rotations
right then theoretically you know its complexity is growing larger and larger right and in a sense
any quantum state that has a complexity uh within that complexity radius you'll be able to reach it
but the problem is because you're spanning such a large space your training of your quantum neural
network becomes harder and harder um because your hypothesis class is too large so you're
searching over too too many possibilities and this is the result known as the the baron plateaus
in the quantum neural network landscape or or the quantum version of no free lunch theorem where
you can't have a one size fits all representation and that's that's where physicists come in
physicists need to have you know good prior knowledge of the domain of application they're
trying to do quantum machine learning and to instruct their choice of representation and
parameterization to aid in in the the tractability of training um that answers the question yeah so
i like that free lunch theorem uh because i guess you know you could try to say well if i have some
sort of generators that i use for my model you know what is the reachability of states
right since the people ask what is the reachability but i think what you're emphasizing here is that
reachability is maybe only a first step uh and maybe having too much reachability sometimes
at the moment so there's a trade-off maybe between capacity and efficiency exactly exactly and that
that is the no free lunch theorem in a sense so that's lucky for us because uh you know at least
for now it seems like physicists will be needed uh in the future when uh we're not going to be
out of a job we still need to design architectures um at least for now so let's see let's see let's
see where it goes but uh but that's right that's right so uh you know a lot of what i'm going to
present today is not necessarily uh architectures for specific domains it's it's more uh a general
framework uh based on quantum information theory of how to uh do quantum machine learning or or
maybe a a very broad class of parametrizations of of models that are quantum yeah yeah i'm since
i've already interpreted you there's interesting this is kind of an unusual question but why don't
throw it out here anyhow from martin how much time do you take uh take it take you to learn all this
i guess um i mean so okay so i guess backstory uh once there was a conference of machine
learning on a monday on a friday night i decided to binge watch lectures at three times speed on
youtube on the basics of deep learning i think that was 2016 uh or 2017 something like that
and uh since then i've just been reading uh machine learning papers and you know i i guess
have a deep math background so it helps uh and then quantum computing itself uh i guess
i've been doing since i was 19 and i'm 28 now so gives you an estimate uh it's just always been
my passion and uh i uh i went through theoretical physics and uh now i'm here in uh quantum machine
learning so i would say four years of of interest in quantum machine learning two to three years
serious uh serious focus and it looks like george baron is building on my question which
probably gets into a little bit i also wanted to get into which is what are some uh quantitative
quantitative metrics for representation capacity yeah that's that's interesting um
i guess that's a good that's a good question i would say if you can quantify of in a sense a
notion of volume and complexity space um and this is actually you know we're edging on on
theoretical physics here because the notion of quantum complexity is interested in interesting
in in the theory of ads cft and um you know there's lennard suskin who does a lot of work in this uh
in this space uh and uh yeah i mean that's a that's a that's an open question i think i have
some intuition uh as to what would be a good metric but that would be an interesting further
study you know yeah there's a good quote right on we point correct like with uh with logic we prove
with with intuition we discover and that's right here's the guidance that's right cool cool um so i
guess i've i've i've gone through these uh this is just some text uh backing up what i've said um
so okay so now that we've we have some very brief background and some intuition about deep learning
because this is a quantum computing audience so we had to load that up um how can we use you know
what we learn taking inspiration from vaes or uh from you know the what is needed to have a good
representation to instruct our choice of how we do quantum deep learning so first of all what would
be a quantum deep representation right well a classical feed for network in a cartoonish picture
this is not the most general formulation but it's a it's a friendly one um you have some input you
have some parameters phi and then you have some parameterized output f of x fine for a quantum
neural network you have usually a pure state input a unitary evolution that is parameterized in some
way and then you have a parameterized hypothesis class of pure states right which is u five times
your initial state and uh you know we call the function f the feed forward operation you can have
a uh loss functionals returns a scalar that depends on say your label and your your output of your
network uh this could be some statistical measure of statistical distance to your data set and you
want to find the uh minimum or approximate minimum subject to variations of the parameters of this
loss functional so how do we get scalar values out of a quantum computer it gives us a wave
function so do we what do we do with it well we have to define a loss operator which is a quantum
observable right or Hermitian observable and often we decompose it into small chunks that
we can measure independently um and combine later on and our goal similarly to you know in the v the
case of vqe and many other variational algorithms is to find uh the minimum subject to variations
variational variations the parameters uh the expectation value of this loss operator right
sometimes it's called the energy or the Hamiltonian but i like to generalize it to uh you know loss
for quantum machine learning so there's just a refresher this is the typical way when trains
a vanilla uh what i call vanilla quantum variational algorithms or vanilla quantum neural network
you have a loop between a quantum and classical computer and the quantum computer gets an
expectation value feeds at the classical computer classical computer has an optimization
algorithm that's classical suggests and use values of the parameter and you iterate like this in a
sense you know our current quantum computers are restricted in how much quantum depth and uh
you know what kind of quantum states they can represent they're restricted in depth because
of the noise and so it makes sense that we search over the space uh given this constraint of low
depth circuits we should search over the space to find the quantum state and this is this is why
this is kind of taking over because for the nisk era or you know early fault tolerance we're going
to be searching over the space of states that are uh not too big not not too you know long to
quantum compute um so why learn quantum representations in the first place if you if you allow a
meal modify uh Feynman's famous quote uh Feynman said you know nature is in classical gamut if you
want to make a simulation of nature you better make it quantum mechanical and in our case it's
if you want to learn a representation of nature you better make it quantum mechanical right so
quantum states and quantum processes i think we've been we've been mentioning this can exhibit
high levels of quantum forms of correlation such as entanglement and that's exponentially hard to
represent in classical memory right if you have a uh random circuit producing in a highly entangled
state it's very hard to approximate it and it's hard to prove uh theoretically without a doubt
but every algorithm we've tried to simulate quantum circuits it seems to uh fall flat on its face at
some point right um yes um quick question clarification from joe here is the loss calculated
by classical computer is the loss calculated so he's talking about the expectation value of the
L operator right right it takes actually several samples to estimate the expectation value right
you could think of it as like sampling from a distribution if i'm trying to do an estimator
of a uh random variable subject to samples from a certain distribution it takes several samples
so there's there's like a mini loop in here to estimate the expectation value here and that's a
mixture of uh you know doing several measurements on the quantum computer and saving saving the
results on the classical computer and then the classical computer can aggregate the the various
results to get an average right and maybe to paraphrase you mean you you run that same use
circuit with the same inputs the same initial state the same parameters several different
measurements yes well i guess the the loss operator may have several uh non-commuting uh
sub operators and one wants to get an expectation value of each uh term in the sum and then one
adds up all these terms to get an expectation value of the sum so that is called the quantum
expectation estimation uh sub routine in a sense and here i kind of abstracted it out
but it's it's an extra sub routine there's a mini loop of trying to get a precise estimate
it's not to be neglected because if you want you know a 10 to the minus seven precision
for an energy it could take you hours on a 10 kilohertz machine for example so it's it's an
important thing to consider when designing quantum algorithms that we only have noisy
or estimates of our our loss function yeah so basically if you run that um yeah so you
break up the L into sub operators which you measure you know you measure by running multiple
times you get you don't need the distribution here for this you only need the actual mean value
yes right or at least in the in the typical vanilla case but as we'll see there's there's
other other variants out there but uh usually the quantum part it's it's hard to get a scalar
out of the quantum computer by something else than defining an observable and outcomes of measurements
right and how important would read out errors and skew and to read out being in a instance you get a
p1 probability 80 but you have to skew because of the loss t1 process and things like that
that's right you get a imprecise estimate of your your loss function and uh you need to have classical
algorithms on the side to compensate for that imprecision or to choose your optimizer wisely
in a way that is robust in noise right and uh i see a lot of questions i i hope i can get to all
of these we may we may have to yeah for the end here okay um maybe just quick one here does the
quantum advantage come from generating the variational forms
um i mean you know i am not claiming a quantum advantage yet but i would say that uh
if there if there was a quantum machine learning advantage it would likely come from uh being able
to do the inference or prediction step with your model and hence the ability to train it as well
so both the training and inference are rendered possible once you have access to a quantum computer
if you incorporate a model that has high quantum complexity so a large unitary that we can't
simulate classically um yep and i think this next one you're going to talk about which is back propagation
you know can you see yes yes yes um okay so how to practically leverage a quantum computing power
well for discriminative models for example you can uh you know let's say you prepare a quantum
data set because again for now we don't have a quantum internet where we can import uh data
that'd be nice someday uh and you evaluate your quantum model let's say you do a feed
forward or a unitary parametrize model you get the expectation value of say several observables
that becomes a vector you feed that vector to a classical neural network and then it evaluates
some some prediction based on this say a label or whatnot and the idea is that you can train both
your classical uh part of your network and your quantum part of the network together uh via a
form of quantum classical hybrid back prop and the idea is that you know your quantum neural network
can can have all sorts of components uh but it could itself be a building block in a sort of meta
network between quantum neural networks and classical neural networks and the idea is that if you
zoom in on say a little uh sandwich of of nodes here are meta nodes of a deep neural network a
quantum neural network and a deep neural network so the let's say a deep neural network or any
differentiable computation feeds parameters to quantum neural network uh and then you have
the measurement of several observables at the output which you feed to a classical neural network
and and then you could do other stuff later on uh and you get your loss function here then you
get the gradient of the loss function back propagate uh your gradient classically and
effectively what's interesting is that this thing is a actually a itself is is technically
an observable on this space if you could you know invert this function but the idea is you do a first
order approximation so you get an effective back propagated gradient Hamiltonian which becomes
or you call it a Hamiltonian because it's an observable and then it becomes just like taking
gradients of a vqe to obtain the gradients of these parameters you just have an effective
value of the gradient for a certain value of your your you know all your parameters over here and your
loss function and you could take gradients of uh that's with respect to your parameters and you've
effectively back propagated the gradient of this value through the q and n and you could keep going
and this is important because you don't want to have to do a slight change do your whole
chain of computations see how it changed and then backtrack it's it's more scalable uh this way
so okay so there's some software uh that does this i have to plug it i mean it's one of my
pet projects uh for it's been so for a while uh for now it's uh it's uh interface between
cirq and tensorflow there's some open source contributors that are working on quiz kit
compatibility so that's going to be exciting for the quiz kit community and we're supporting them
but it allows you to you know automate this this training and integrate it into you know
advanced machine learning models in tensorflow and you know tensorflow i think has the record of
on ibm supercomputers for you know the biggest machine learning computation so i think it's
important to uh to ideally integrate uh quantum computers with uh the power at least one of the
most powerful frameworks for high performance computing on the classical side um so any question
there in that vein um this is an earlier question for me to are there any data sets filled with the
quantum machine learning models you can map up some notebooks for a slayer uh that's uh i think
that's public that we're working on that and we're trying to work with other you know other uh
companies in the space to make sure we we agree on what a form for a data set will be but in general
because you can't download quantum data you can't just save you know states because they
take exponential space and you don't know how to load them on your quantum computer the data set
takes the form of a circuit um or a set of circuits and those define wave functions that you could
then do quantum deep learning on and it's something that's uh being worked on but you'll
have to stay tuned uh for that thank you cool okay so uh what can one do with hybrid feed forward
networks uh i'm going to skip over this uh yeah quickly i guess there's a paper by luke in which
is a convolutional neural network which uh are inspired with from the mera if you're in in the
know about it uh basically it's luke using the fact that uh if you know your system is
translationally invariant so it has some symmetry you reflect that symmetry in your your choice of
parameterization of your quantum neural network and so this is just a quantum neural network
that has translational invariance and is hierarchical and the idea is that you know maybe you can't do
all the quantum layers but maybe you could do only one quantum layer and already you'll
you've down sampled the problem you've reduced that dimension dimensionality and you've broken
up some entanglement or you've you've like disentangled partially remember for compression
you got to decorrelate everything all right so the idea is that you can do you could input
various quantum data in batches you could apply various feature maps that are quantum convolutional
networks and then you get kind of images from you know all your histograms of samples of your
bit strings and following this you could apply classical convolutional layers and you know finish
the job with fully connected and at least in our early experiments uh hybrid networks with multiple
filters were better than one quantum network and that's without noise so with noise on the device
it's even better uh but that's just a an example of discriminative learning i won't go too much
into that but in terms of applications it would be for example classifying phases of matter detecting
whether something is superconducting or not and the idea is maybe you train on a data set of
a material you know is superconducting at certain value of the parameters and temperature
and and then you you ask the neural network to detect for another material that you don't know
whether it's superconducting or not at certain value parameters so generalizes so that's uh
that's one quote-unquote killer app we think for quantum neural networks um yeah so it's just
comparing the two with with our old diagram okay so i guess we'll get to the meat of the talk uh
i'm not too bad halfway there i guess um so uh how can we extend these insights and how can we
hybridize um in a meaningful way with classical machine learning capabilities for quantum machine
learning right let's go back to our slide of deep generative modeling we have our data set
we have our variational classical distribution we want to minimize our question before we
deep dive here uh it's about um nlp and maybe can we use some of this quantum representation and
nlp transformers to reduce the huge size of it to increase accuracy i hope that's a
maybe a little bit out there question um so i mean we our team has some public work that we've
used tensor networks which are you know analogous to quantum circuits in a sense to find factorizations
of large matrices uh and we apply them to the transformers and at least in our demo we get a
two times speed up uh and of course um you know that'd be great if um such a tensor network could
be contracted on a quantum computer uh faster it's not an experiment we've tried yet but um you
know it's going to come down to constant speed ups uh you know uh you know our tensor network
versus a quantum computer for certain tensor networks the quantum computer is exponentially
faster but for other tensor networks it's going to be similar uh potentially um so that is that is
a good question um but uh i guess i guess we'll we'll have to see on that side but it's an interesting
area of research in a sense uh dimensionality reduction uh using quantum circuits um and uh
you know tensor networks are a first step towards that um but it's uh you know it's encouraging to
see that cutting edge ml can be improved with quantum or quantum inspired methods at least today um so
yeah at least in nlp that's the area that i'm confident saying something that quantum computers
would be potentially useful um all right uh so so i mentioned we want uh our data set degree uh
you know for our data points in general when you want two uh distributions to agree
you do what is called the k l divergence uh it's not a symmetric function so be careful uh
you could go you go one way or the other uh between your true distribution your data distribution and
your uh variational uh distribution right and it's like here would be the expectation value
specter of data of the ratio of logs right so the idea is that to evaluate this kind of gold
standard of uh quantum statistic or sorry classical statistical distribution uh we need
access to the log of our logarithm of our uh model for any given data point x that we sample
from the data right so not every uh classical machine learning model allows you to do to do this
right so gans for example don't have an explicit logarithm of the of the density of your generative
model that you could query it's implicit it's only you know the discriminator telling you how well
you're doing but it's not a notion of log whereas you have let's say you do a bunch of transformation
that um that you know uh the determinant of the jacobian you could compute that efficiently
right the determinant of jacobian if you if you continuously transform a space right and you
had initially a simple Gaussian on that space and you end up with a complicated space you've kind of
you know bunched it up and you've done some complicated different morphism you could back
track how the notion of volume locally has changed right and uh for any you know value we target here
we can kind of invert um the measure in a certain bin here to uh some set of bins over here and we
know the value of a Gaussian analytically and so you can compute in a sense somewhat efficiently
analytically the um the density of your your probability distribution for any point you query
uh this is called a normalizing flow but there's other types of models you know there's energy
based models there's auto regressive models there's a whole bunch of of cool models out there but a
lot of people know gans because it's like the entry entry level thing because people understand
discriminators but so we encourage you to check out other types of generative machine learning
and in a sense we're we're looking to have an explicit uh notion of a log uh for reasons that
are going to become apparent in a second in the quantum case so how can we extend this philosophy
to quantum theory uh you know what's the intersection of quantum theory and probability theory right
well there is you know just like in in black holes uh we look at black holes because they're at the
intersection of quantum and gravity so they're an interesting test bed well here we look at
mixed states because they're at the intersection of probability theory and probabilistic machine
learning and quantum theory and quantum machine learning so mixed state in general can be a
probabilistic mixture over mixed states these are matrices instead of vectors now so be careful
but uh any density operator has what is called a spectral decomposition so it it's always
expressible as a mixture of orthogonal pure states and this this mixture sums up to one so it has
a probabilistic interpretation so we go from vectors to a density matrix and each element
is in complex numbers so how would we represent mixed states so how would we represent the
intersection of probability theory and quantum theory well we should have a model that composes
a probabilistic model with a quantum model right and that is the idea of quantum probabilistic
hybrid deep learning or deep representations hence the title of my talk so as we've seen quantum
neural networks are typically unitary feedforward like this and they have a hypothesis class that is
pure states we can combine here a classical parametrized probabilistic model that that we
can sample and let's say this would flip your qubits you flip your qubits to prepare a bit string
then you apply unitary that's parametrized and what you get at the output instead of a
parametrized class of pure states is a parametrized class of mixed states right and uh you know your
parametrized distribution your state at this point is a diagonal state so it's effectively
classical it has no quantum correlations you can try to show this show there's no coherent
neutral information exercise and then after that you tack on a unitary which is hard for
classical computers to do the idea is we use classical computers and we make them you know we
make them sweat right like inference of probabilistic models can be pretty computationally intense
uh and then we combine them with unitaries and the quantity
and let's talk about how you think we set for capital omega
oh yeah so capital omega is just an index over your basis of your Hilbert space it's kind of a
general formulation because we actually uh phrase the algorithm both for qubits and for
continuous infinite dimensional Hilbert spaces so theoretically could be a an integral or something
it's just general math but it's uh it's an index that it's a index set that runs over
an index for your your entire basis that spans your whole Hilbert space of interest
oh yeah and then you can choose basically any probability over
basically anything but there's going to be certain types that are preferential for for training
reasons as we'll see uh again you know you could parametrize anything classically but it's not
every model that's easy to train again because let's say you need the log and you can't get it
or can't get the gradients then it's difficult so as we'll see we can choose wisely how we
parametrize things so that we can get nice gradients and can train things because how do you
train continuously parametrize the hypothesis class gradient based methods so you use kind of the
notion of steepest descent in the landscape of parameter space and maybe one more question
I'm told I have to speak a little bit louder so hopefully for sure for I mean let's take the
extreme case you take a case where you have a pure like harm mixture like you know you have a pure
mixture of all states so I'm guessing that's not very useful one so in a way you want your state to
be a little bit mixed that's somewhat pure or are you okay having like a purity of like zero
or maximally mixed even though it's so if you were to optimize over architectures so tune
and we have some new results that are not in the paper for this talk you could tune how much quantum
depth you you you assign to the the unitary um theoretically this this approach could be tuneable
in the sense that if the data set that you're trying to represent is purely classical and has no
quantum correlation and the identities in the span of your unitary hypothesis class you could
learn to just apply the identity and then it's a classical machine learning system if it's a pure
state that you're learning this the probabilistic component is useless in a sense because it's
going to be all unitary you're just trying to learn a pure state so it's an adaptive way to
separate out the task of quantum and classical machine learning of a quantum uh mixed states
and it's quite cool because you have one framework where you have as a subset uh classical
generative modeling of distributions um so in a sense it it can via self-tuning it could
adapt to use no quantum resources or use no classical resources or any you know continuum in between
and uh we have time here two more quick questions so are you using mixed states for the input as
sort of maybe in practice i guess at this level it's all a little more we're we're going to see
that i guess uh it's you could use output or input so yeah um i think i still have a good
number of core slides but i guess i'll i'll go through them faster a bit um we can run a little
longer okay okay so you know why should we care about quantum mixed states well you know thermal
states um are at finite temperature and so you know most systems in nature at finite temperature
unfortunately our quantum computers are not at zero temperature so even them themselves must be
modeled as mixed states uh if we were to be accurate and experimentalists know this theorists
like to say it's a pure state um so you know so the ability to simulate mixed states is is crucial
to nature and and the reality is like you know we're trying to use quantum computers to simulate
nature but nature itself if you core screen enough you zoom out there's a quantum to classical
transition right you know we're used to having classical physics having quantum physics and
then there's a continuum in between so the point is to have a set of continuum of models
that can model that in between at finite temperatures when quantumness dies down
and uh it becomes classical uh or you know when you're very close to being fully quantum right
uh most quantum systems are open quantum systems i've mentioned this and
for various reasons subsystems of of quantum states uh have are mixed states because of
entanglement if you take a reduced state of a pure entangled state you get a mixed state
um so just looking at patches of things at a time to model them it's important so
what sort of mixed states in nature can we variationally simulate using something like this
well thermal states is of of great interest because they're they're omnipresent so the algorithm
that leverages quantum probabilistic generative models to model thermal states is uh variational
quantum thermalization and or vqt uh and uh so the problem is given a Hamiltonian and h and a
target temperature uh one over beta then generate the thermal state which is the exponential that
is normalized like this and this is the partition function and the idea is okay well we'll use one
of our magic models of classical probabilistic distribution and a unitary uh and how are we
going to converge to uh the thermal state well thermal states are the minimum of something
called free energy right so free energy is you know uh roughly ignoring temperature energy minus
entropy right uh so we can evaluate the energy you know just like in vqe of our model
and subtract the entropy right so how do we get the entropy well because unitaries can serve
entropy the actually actually the entropy comes strictly from our classical part of the model
and if your classical model has ways to get gradients of entropy you're in business or
you know sometimes it's simple enough you could get it analytically and this is equivalent defining
you know the minimum of the relative entropy between our model and the thermal state so you
know we know that the unique minimum of this function is when the two states match so if we
do our job well and we parameterize things well and find the absolute optimal states uh then uh
you know we've got the jackpot um state of minimal for energies thermal state so how do we
parameterize our quantum probabilistic generative model i've been pretty abstract now so we're just
going to zero it in slightly uh well uh the motivation for this work was to take inspiration
from recent work by uh open ai and such on modern versions of energy based models where
one it's it's now it's taking inspiration from physics right so you define an energy function
using a classical neural network let's say from the space of bit strings or continuous values to
to a scalar and you could use various algorithms that leverage gradient information such as
Hamiltonian Monte Carlo or stochastic gradient Langevin dynamics uh you know all all there's a
bunch of open source frameworks to to do the this part you could sample the landscape by in a sense
having a noisy ball traverse this landscape and you get samples of the exponential this way or the
Boltzmann distribution known in physics it's a classical Boltzmann distribution so you parameterize
the energy and why why is that going to be useful well uh it has all sorts of compositionality
perks it's it's very good it's comparative with GANs this is work by open ai 2019 so how do we
leverage these models and integrate them with quantum computers well so you know what if we had
our probabilistic part of our model was a classical energy based model like this so it's
parameterized energy function and then our distribution is a Boltzmann distribution again
well if you you you could define a diagonal operator which is the log after you flip some bits
which is your energy function on the diagonal and what you get if you do some math with some
unitaries and exponentials is that you've just parameterized a operator the diagonal is parameterized
by a neural network and and the total operator is this conjugation of a unitary with this diagonal
operator so you've you've parameterized a Hamiltonian operator and your hypothesis class is a set of
thermal states so in a sense you you know you know you're targeting a thermal state so you might as
well have a hypothesis class of thermal states right um okay so if we have this assumption
that we're using an energy based model how do the gradients work out well there's a bit of math
involved i've skipped many lines but it is possible to sample it essentially you have to get bit
strings at the output of your model and you can evaluate you can compare the value of the energy
of your your bit strings minus the the energy of your model and you could also evaluate gradients of
your your model if it's parameterized by a neural network and you you do the sampling which only
depends again on sampling from uh from your uh your model p theta of x and you have sampling
algorithms and you can evaluate gradients in a sense it's an analytic way to guarantee that your
estimates of your gradients are unbiased um and how do you get gradients for the quantum part
well the quantum part is just the usual i hope you've seen this in other talks and
i don't have time to cover it unfortunately today it's the parameter shift rule right
which is how you take gradients in the vqe which is how do you take gradients of a
a unitary a state fed through a unitary and an expectation value so i won't cover that but
it's very standard it's you know a standard in the software framework and there's various papers
that use this um it's a cool theory but you know does it work right uh the answer is yes you know
if you if you have a target thermal states you can uh do a reconstruction like this this is for
some heisenberg spin model we use very simple classical distribution here it was just Bernoulli
so random coins uh and and the quantum computer could do a lot of work and and learn to represent
a thermal state we've done much larger systems but you know a jarbled set of pixels is not necessarily
the most aesthetic thing so we we choose to feature the smaller systems but uh we've scaled
things up quite a bit and uh the idea is um you know the function we're optimizing is relative
free energy but the other metrics of quantum statistical distance uh also converge uh so
uh it seems to work um we've also tried some set of fermionic systems and bosonic systems
for example a simple you know toy model of a superconductor that's uh bosonic uh sorry Gaussian
fermionic so it's quite simple we can plot the correlation functions the target this is at iteration
zero and it converges by iteration hundred of gradient descent pretty well um so this is actually
a new result i i'd like to feature that's not in the paper uh but it's coming in the second version
of it uh can we tune how much quantum versus classical resources we use right so suppose i
look at this heisenberg model and i look at after training how how well i do in terms of
trace distance and fidelity depending on the temperature and the number of quantum layers
i use ah well we see there's certain sets of temperatures uh that uh you know you need more
quantum layers to to model them right and it's not necessarily you know at this point it becomes
trivial uh at this point there's a nice balance between quantum and classical resources and this
is the fidelity is trace distance uh but uh this is kind of what you you'd like to do right you
want to use as little quantum resources as possible uh in order to have an accurate representation
of a state so this is something we started investigating but it's uh you know it maybe
has some deep implications about what's the true quantum complexity of a quantum machine learning
problem uh and uh i guess uh you know uh please take a look at these qr codes there's links to
various notebooks and you know i've been advertising tensor flow quantum but there's there's obviously
you know implementations in quiz kit from the community uh shout out to jack seroni and uh
you know the tensor flow quantum implementations by my collaborator antonio martinez um and three
two one take a picture on youtube or whatnot and look at the uh websites for these notebooks
so the final component is more machine learning it's less quantum simulation is how do we use vqt
to do quantum machine learning so if we're given quantum mixed state data how do we
learn from quantum mixed state data so again we're going to use our quantum Hamiltonian based model
because for reasons that are going to become apparent in a second so we call the task of
learning to replicate right we want an approximate density matrix that approximates a data density
matrix so a data density matrix could be itself a mixture of a bunch of density matrices we're just
trying to approximate this thing and we want to find a set of parameters such that for the optimal
parameters our hypothesis class approximates this density matrix and we assume we have access to the
quantum form of the data okay and the idea is if you use a quantum Hamiltonian based model
and you aim to minimize now the relative entropy in reverse from last time uh what you get is if
you ignore the terms that don't depend on your parameters you get something called the cross
entropy which is this the trace of stigma which is your data log row right and again because we've
parametrized our hypothesis class in terms of its logarithm its quantum logarithm uh we can evaluate
this energy this it's called modular energy or modular free energy and modular Hamiltonian is
just a name for the log of a density matrix okay and so we're trying to learn a log of a density
matrix such that the exponential replicates our data set and how you do this you plug your data
you run it in reverse through your unitary of your quantum probabilistic model you sample it and then
you get expectation values of the diagonal operator and this could be parametrized with a neural network
so you can have more computation here the extra term here is all on the classical computer
turns out you could also get gradients for these i won't go too much into it the quantum part is
again parameter shift but these gradients again if you have a differentiable function for your energy
you know like a neural network then you can evaluate you could sample these gradients and
it's unbiased which is really cool that that's really important that we could get good estimates
of the gradients and you know it works out if you don't use enough quantum or not enough layers
of your quantum computer or not enough complexity of your classical distribution sometimes it
doesn't work well so for various temperatures we've tested this and i guess this is a this is
you know there's many things you could do once you have unsupervised learning for example you
could learn a compression code so here we actually applied hopefully some of you know about bosonic
quantum computing but theoretically could be applied to other forms that are not qubits
and here we learn a compression code where we could throw you know 40 percent of a harmonic chain
in the compressed space and still reconstruct the states so this is the error matrix of the density
matrix point seven we start seeing errors and if you throw 90 percent of stuff out things go bad
and there's theory that you can find the logarithm modes the modular modes of the system
and so we checked it with theory that's why we looked at the system thank you but that's it that's
corpse there sorry sorry what are the x and y axes on that curve again this curve on the on the
left oh on this side so this is the density matrix it's uh it's the discrepancy between
the so we go to compressed space it's like an auto encoder we go to compressed space
and then we throw out uh what is like so okay so we we do we learn a vqt and the latent model is a
product of individual um thermal states of harmonic oscillators right and those are like
quantum forms of gaussians which is kind of cool and we throw out the lower entropy latent modes
okay because the entropy represents a harmonic oscillator sorry or when you say a mode you mean
harmonic oscillator of a harmonic oscillator yeah so this is in this is for say a bosonic
continuous variable quantum computing and i did most of my time and continuous variable stuff
before so myself as well in theoretical physics uh this is similar to a calculation of the
hawking effect actually um i that's a whole two hours i won't go into that but actually here's
the interesting thing i have this in my summer school lectures that are up and coming uh there
are only two types of physicists those for whom all of physics is qubits and those for whom all
of physics is oscillators i try to i try to uh play on both sides so uh hopefully someday
we can have hybrid computers that'd be cool through everyone's that's right um so yeah so
we agree with theory here um i could explain how this is related to the hawking and unruh effects
but uh that would take some time but it's it's an interesting uh thing that quantum machine
learning could theoretically understand or learn an analog of the hawking or unruh effects that you
there exists a certain set of modes that an observer feels a thermal statistical
fluctuations of the vacuum so this was the ground state we plug it in and if you transform it then
it becomes a product of thermal states and instead of Fourier modes it's like these weird
squished modes uh of the lattice so it's kind of information theoretic uh eigen modes instead of
you know we're used to eigen modes in physics like the resonance but here it's kind of uh
the resonance of of the log uh Hamiltonian which is the modular Hamiltonian and uh this brings us
actually to the end uh of the talk and uh oh i have luckily haven't exceeded too much uh so we do
have time for questions i guess but i just want to conclude i guess uh you know this is the beginning
of a whole research program it's an exciting area and you know by starting from basics of
information theory right we just started thinking about relative entropy and inspiring ourselves
from physics we have discoveries and machine learning and hopefully now we could apply this back
to the physics right so it's a feedback loop between physics and machine learning and it's
that's a big part of the philosophy of our team at x uh yeah thank you yeah thank you very much um
i think there were some questions during the talk that i didn't get to so maybe i'll
gonna run up the chat here and get back to them again um okay i guess i'll just do a
shout out to anntario my collaborator at waterloo uh he was google and x and jacob was instrumental
to a lot of the vqt and and qmhl work and uh you know did a lot of the work there uh as well
so big shout out to them uh but uh yeah so any questions in the chat i've seen a lot of questions
in the chat here so let's uh let's get let's see how many how many we can answer i guess all right
let's start with the latest one which is about temperature i think the question is um now is
there a sense of critical temperature here relative to some sort of phase transition
the you know the temperatures where you get noisy data closely the critical temperature
some sort of phase transition in this innovative system or is that well i don't know if we
purposely chose a system where we knew there was a phase transition but we can kind of see that
there's different uh regimes where you need more entanglement or need less entanglement right so
so seeing how many layers you need to represent a quantum state could be like seeing a dip in that
could could be a way to detect different phases or quantum phases of matter i guess like you know
regimes of parameter space that have very strong entanglement and regimes that are you know
slightly you know almost trivial um but uh yeah i'm not sure if we purposely picked a system
that we knew there was a phase transition we just observed this data for now but um maybe
something to do uh better on yeah um let's see this one uh this one is about universal estimators
basically can uh q and n's the quantum neural nets be used to imitate this kind of behavior
i guess and we're saying you know given that three layer neural nets are regardless universal
estimators in classical machine learning yeah that's a that's a good question so i guess you know
you want if you have a universal functional approximator um then you know theoretically you
can have a a universal uh you know you span the space of functions that you could represent
of course any classical computation can be embedded uh if you write it out as a reversible
classical computation using many extra registers and you keep the whole history of the computation
if it's not reversible functions you can embed that right in quantum computations
with toffoli gates instead of hand and so on and some of work several years ago i i showed how to
take you know typical classical neural networks and make quantum circuits that implement the
classical neural network in superposition um so the idea is yes i think you can use quantum
neural networks to do the classical probabilistic machine learning components um though so far at
least from uh the current state of the art of the theory it seems like quantum computers will
have a polynomial speedup for inference probabilistic inference similar to grover speedup uh and that of
course if we're competing with extremely large classical computers will be mostly relevant when
quantum computers are of a size comparable to the square root of our largest supercomputer
uh and um that is yeah that's i guess that's uh that's my answer so for now i guess the
most practical approach is to use classical algorithms and classical computers for the
classical component and use quantum computers for the truly quantum component which is the unitary
that seems like a very nice sensible thing there is a question this one is
interesting i think it's more of an opinion question maybe um we know that quantum Fourier
transforms very t and usually digital computing um does it have a role in machine learning is it
similar in here according to what is the relationship with advantage or the Fourier
transform right so i guess here we parametrized our quantum neural network uh as a general uh
bosonic um what is called book all above or Gaussian transformation and the discrete Fourier
transform is a subset of such transformations and here we we learned these transformations so
uh technically uh if we fed the whole system and we asked it to find the eigen modes and if
we had a thermal state of this system say via vqt and then we fed it to quantum modular
Hamiltonian learning these modes would be the Fourier modes because we know the eigen modes of
of this Hamiltonian right we know how to decompose this this Hamiltonian uh into a sum of individual
uh you know number operators um and it's the same you know finding this book all above
transformation is what i mean by it's related to the unrefect uh calculation qft in curved
spacetime i know there are some chat messages that were doubting that but uh i did my masters
in quantum field theory in curved spacetime so you can trust me on that one uh but uh great so
and maybe for the final question here um and taking the extra time um if we want to do research
in this field where should we start or what should be out of the direction of research
right i mean that's a that's a good question i guess uh you yourself have gone through this
position not you know four years ago i think you mentioned right right right right so i guess in my
case i started you know i started with uh the open you know source uh massively online courses mooks
i just listened to that listened to a few of them and then i progressed to i wish i had all my
textbooks here but uh they're they're back there but uh the good fellow the in good fellows textbook
um inventor of gans and then uh murphy kevin murphy uh a goobler uh he did uh what i called
a kind of the nielson and schwang or the bible of probabilistic machine learning and i think there's
mckay there's information theory for machine learning that's if you want the textbook routes
otherwise i think with time as the field stabilizes i guess because it's been moving so fast everybody
who's involved in it is just cranking out papers rather than creating coursework uh there will be
coursework um i could link a you waterloo course uh that i gave some guest guest lectures at that
that featured some quantum machine learning uh but uh overall i would say it's important to
understand the theory of classical machine learning at the fundamental level because
you know similar to hardware engineering we're at the fundamental level of engineering a new
computing stack so on the theory side we're re-engineering a whole algorithm stack so we
got to start again from first principle so you know you have to trace back to like papers from
the 80s of machine learning and and the fundamentals and then and then work your way back to the
modern uh modern thing so i would say the the modern ml stuff is flashy and and and fun to
stay up to date but i would say you know take the time go back to the the core old literature
you know the foundations so um yeah uh mooks and then textbooks is the way to go that's
that's what i did and here i am so uh and then maybe i can just add that now they're
summer school classes coming online so i mentioned the quantum information kiscuit one i think there's
a touching of ml and things like that in the last two lectures in quantum chemistry vqe's
definitely on there so fantastic anyone interested in addition to to what you said you know we can
add that um so i think it is that time that i get thank you again and thank the listeners
for joining the quantum live seminar series uh we're back this friday um i will mention next
week we're back so this at the same time uh continuing with the talk by antony miss capo from
idea on quantum chemistry we could talk about variation of quantum eigen solvers q a o and
things like that on chemistry things so that will be a very nice uh follow up to your talk
gion and uh thank you thank you for inviting me it's uh it's been an honor and uh hopefully the
quantum community is interested in quantum machine learning now uh so i've done my job
there all right thank you so much follow gion on twitter quantum bird that's right all right
so any final words and otherwise thank you and we'll see you next week uh that's it for me
thanks again for tuning in and uh stay home stay safe everyone and uh thank you all right
it was a pleasure gion we'll see you soon guys take care cheers
