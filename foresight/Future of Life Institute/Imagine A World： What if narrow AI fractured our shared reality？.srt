1
00:00:00,000 --> 00:00:02,500
On this episode of imagine a world.

2
00:00:02,500 --> 00:00:10,000
My best guess is that a guy will progress much more slowly than I have it progressing in my story.

3
00:00:10,000 --> 00:00:17,500
And my best guess is that we do survive because I progress is much more slowly from my perspective.

4
00:00:17,500 --> 00:00:26,500
It's extremely contrived for a GI to develop even as fast as it does in this story and be handled well enough cautiously enough.

5
00:00:26,500 --> 00:00:31,500
Thoughtfully enough that we have more than a fraction of a percent chance of survival.

6
00:00:36,500 --> 00:00:41,500
Welcome to imagine a world, a mini series from the Future of Life Institute.

7
00:00:41,500 --> 00:00:49,500
This podcast is based on a contest we ran to gather ideas from around the world about what a more positive future might look like in 2045.

8
00:00:49,500 --> 00:00:55,500
We hope the diverse ideas you're about to hear will spark discussions and maybe even collaborations.

9
00:00:55,500 --> 00:01:01,000
But you should know that the ideas in this podcast are not to be taken as FLI endorsed positions.

10
00:01:01,000 --> 00:01:04,000
And now, over to our host, Guillaume Reason.

11
00:01:15,500 --> 00:01:19,000
Welcome to the imagine a world podcast by the Future of Life Institute.

12
00:01:19,000 --> 00:01:21,000
I'm your host, Guillaume Reason.

13
00:01:21,000 --> 00:01:28,000
In this episode, we'll be exploring a world called Hall of Mirrors, which was a third place winner of FLI's World Building Contest.

14
00:01:28,000 --> 00:01:32,000
Hall of Mirrors is a deeply unstable world where nothing is as it seems.

15
00:01:32,000 --> 00:01:38,000
The structures of power we know today have eroded away, survived only by shells of expectation and appearance.

16
00:01:38,000 --> 00:01:43,000
People are isolated by perceptual bubbles and struggle to agree on what's real.

17
00:01:43,000 --> 00:01:47,000
Despite all this, things are generally going okay, for now.

18
00:01:47,000 --> 00:01:52,000
This is partly due to this world's particularly slow and modest development of AI technologies.

19
00:01:52,000 --> 00:01:57,000
AI tools here are still dominated by extensions of today's fundamentally narrow systems,

20
00:01:57,000 --> 00:02:01,000
with the one true AGI being developed under heavy quarantine.

21
00:02:01,000 --> 00:02:08,000
There are a number of reasons for this slow progress, including high computational costs and poor funding due to politicization.

22
00:02:08,000 --> 00:02:12,000
This team put a lot of effort into creating a plausible, empirically grounded world,

23
00:02:12,000 --> 00:02:15,000
but their work is also notable for its irreverence and dark humor.

24
00:02:15,000 --> 00:02:20,000
I can safely say that it's the only winning world where you could see virtual celebrity Tupac List

25
00:02:20,000 --> 00:02:24,000
perform at a luxury war-themed amusement park run by the Taliban.

26
00:02:24,000 --> 00:02:26,000
Needless to say, there's a lot going on here.

27
00:02:26,000 --> 00:02:31,000
I was excited to get a look into the minds behind this particularly brimming and erratic world.

28
00:02:31,000 --> 00:02:37,000
Our guest today is Michael Vasser, one member of the three-person team who created Hall of Mirrors.

29
00:02:37,000 --> 00:02:43,000
Michael is a futurist, activist, and entrepreneur with an eclectic background in biochemistry, economics, and business.

30
00:02:43,000 --> 00:02:49,000
He served as president of the Machine Intelligence Research Institute and is co-founder of Metamed Research.

31
00:02:49,000 --> 00:02:56,000
His other team members were Matia Franklin, a doctoral student studying AI ethics and alignment at University College London,

32
00:02:56,000 --> 00:03:00,000
and Bryce Heidi Smith, who has worn many hats from fortune-telling to modeling,

33
00:03:00,000 --> 00:03:03,000
and now has a focus on finance and policy research.

34
00:03:03,000 --> 00:03:05,000
Hey Michael, great to have you with us.

35
00:03:05,000 --> 00:03:08,000
Great. Good to speak to you.

36
00:03:08,000 --> 00:03:13,000
So I'm curious how the three of you on your team came to work on this project together.

37
00:03:13,000 --> 00:03:19,000
So I've known Bryce for a very long time, and when the project was starting up,

38
00:03:19,000 --> 00:03:24,000
there was a call for collaborations, and I tried talking to a bunch of people.

39
00:03:24,000 --> 00:03:29,000
And Matia and I had, you know, the most productive conversations.

40
00:03:29,000 --> 00:03:36,000
But like the overall project was mostly my vision, and Matia did some level of editing,

41
00:03:36,000 --> 00:03:40,000
and Bryce did the fiction and art.

42
00:03:40,000 --> 00:03:43,000
Cool. So did Bryce make the music that was accompanying your session?

43
00:03:43,000 --> 00:03:44,000
Yes.

44
00:03:44,000 --> 00:03:47,000
Cool. Yeah, I really enjoyed your music and the short stories as well.

45
00:03:47,000 --> 00:03:48,000
He did a great job with those.

46
00:03:48,000 --> 00:03:52,000
I mean, it's the closest thing to a super intelligence that we have around for now.

47
00:03:52,000 --> 00:03:54,000
Endurable.

48
00:03:54,000 --> 00:03:57,000
Well, what was it like for you guys to do this project together?

49
00:03:57,000 --> 00:04:00,000
Did you learn anything yourself in the course of it?

50
00:04:00,000 --> 00:04:02,000
I mean, I had a lot of fun.

51
00:04:02,000 --> 00:04:05,000
It helped me to concretize some of my thinking.

52
00:04:05,000 --> 00:04:10,000
Some, I feel like the basic sense of where I think we're going or would like to go

53
00:04:10,000 --> 00:04:16,000
has been reasonably stable in my head since GPT-3 came out,

54
00:04:16,000 --> 00:04:22,000
and hasn't drastically changed since GPT-2 and COVID.

55
00:04:22,000 --> 00:04:24,000
Yeah.

56
00:04:24,000 --> 00:04:28,000
What were some of your biggest sources of inspiration when you were working on this together?

57
00:04:29,000 --> 00:04:35,000
I don't think my thinking on this is significantly influenced by stories or books

58
00:04:35,000 --> 00:04:37,000
or music or what have you.

59
00:04:37,000 --> 00:04:43,000
I think it's basically just coming from looking at what the technology can do

60
00:04:43,000 --> 00:04:48,000
and spending the last 25, 30 years obsessively thinking about history and the economy

61
00:04:48,000 --> 00:04:54,000
and social sciences and making some effort to understand the technology.

62
00:04:54,000 --> 00:04:57,000
I'm certainly not a top expert in actually understanding the technology

63
00:04:57,000 --> 00:05:04,000
while I will humbly claim to be a top expert in understanding the history of technology

64
00:05:04,000 --> 00:05:06,000
as it relates to economics.

65
00:05:06,000 --> 00:05:09,000
Yeah. Well, you do have this deep professional background.

66
00:05:09,000 --> 00:05:13,000
Can you say a little bit about how your experience in other fields

67
00:05:13,000 --> 00:05:17,000
and kind of working through all this has influenced how you see the future?

68
00:05:17,000 --> 00:05:21,000
I mean, in terms of professional background,

69
00:05:21,000 --> 00:05:29,000
molecular bio, I studied in university and it doesn't really inform this very much.

70
00:05:29,000 --> 00:05:34,000
I have a lot of thoughts about cool things that could be done with molecular bio,

71
00:05:34,000 --> 00:05:40,000
and now that GPT-4 is performing at a high school national championship level

72
00:05:40,000 --> 00:05:44,000
without major upheaval enhancements.

73
00:05:44,000 --> 00:05:47,000
I'm confident that I can do a lot more of that stuff,

74
00:05:47,000 --> 00:05:50,000
and also AlphaFold is very cool, and mRNA tech is very cool.

75
00:05:50,000 --> 00:05:53,000
So I think there's enormous opportunities now for bio.

76
00:05:53,000 --> 00:05:59,000
Getting an MBA gave me an opportunity to exist in the business office world for a while,

77
00:05:59,000 --> 00:06:06,000
and that certainly is necessary without having interacted with corporate hierarchies.

78
00:06:06,000 --> 00:06:09,000
One doesn't know what corporate hierarchies are like at all.

79
00:06:09,000 --> 00:06:12,000
There's very effective disinformation and propaganda about that.

80
00:06:13,000 --> 00:06:19,000
I think mostly I've just read a lot in directions that seemed like they could be helpful

81
00:06:19,000 --> 00:06:22,000
over maybe a 25, 30-year period.

82
00:06:22,000 --> 00:06:27,000
Yeah. What sorts of insight did Bryce and Machita bring to the project?

83
00:06:27,000 --> 00:06:33,000
So the actual stories were very cool, and the music was very cool.

84
00:06:33,000 --> 00:06:37,000
And Bryce wrote those mostly by himself.

85
00:06:37,000 --> 00:06:44,000
And there were some back and forth about what sorts of things were maybe too over-the-top

86
00:06:44,000 --> 00:06:48,000
or too fun and silly to include in the story.

87
00:06:48,000 --> 00:06:53,000
And it's just good to talk to people about things and develop the ideas together.

88
00:06:53,000 --> 00:06:58,000
And certainly Bryce has been enormously central to developing my understanding of the world

89
00:06:58,000 --> 00:07:00,000
in general over the last decade.

90
00:07:00,000 --> 00:07:02,000
And what about Machita?

91
00:07:02,000 --> 00:07:06,000
Machita? I mean, mostly just discussing what I can get away with.

92
00:07:06,000 --> 00:07:12,000
In terms of when telling a story, what is too weird, what is socially acceptable enough

93
00:07:12,000 --> 00:07:19,000
that people can understand it as relatively limited in French distance from normal, thoughtful people?

94
00:07:19,000 --> 00:07:20,000
Yeah.

95
00:07:28,000 --> 00:07:31,000
In some ways, this world is kind of a caricature of the present.

96
00:07:31,000 --> 00:07:34,000
We see deeper isolation and polarization caused by media

97
00:07:34,000 --> 00:07:38,000
and a proliferation of powerful but ultimately limited AI tools

98
00:07:38,000 --> 00:07:41,000
that further erode our sense of objective reality.

99
00:07:41,000 --> 00:07:44,000
A deep instability threatens.

100
00:07:44,000 --> 00:07:48,000
And yet on a human level, things seem relatively calm.

101
00:07:48,000 --> 00:07:52,000
It turns out that the stories we tell ourselves about the world have a lot of inertia,

102
00:07:52,000 --> 00:07:54,000
and so do the ways we live our lives.

103
00:07:54,000 --> 00:07:58,000
I had a hard time picturing those individual lives among all the wild happenings of this world,

104
00:07:58,000 --> 00:08:01,000
and I wanted to hear more about that human perspective from Michael.

105
00:08:03,000 --> 00:08:05,000
What's it like to live in this world you've made?

106
00:08:05,000 --> 00:08:09,000
Well, it's going to be very different in different media bubbles.

107
00:08:09,000 --> 00:08:13,000
The biggest media bubble by far is going to be Chinese,

108
00:08:13,000 --> 00:08:20,000
and the successor to contemporary Chinese Communist Party politics

109
00:08:20,000 --> 00:08:25,000
will mean something more neo-confusion than China has been recently.

110
00:08:25,000 --> 00:08:31,000
But done with capacities that no one's ever had the opportunity to bring to the table,

111
00:08:31,000 --> 00:08:37,000
so you can just spend so much more time on filial piety and cultivating then

112
00:08:37,000 --> 00:08:40,000
when all of the real work has been automated

113
00:08:40,000 --> 00:08:45,000
and when you have machines that are in some ways superhuman watching your every move

114
00:08:45,000 --> 00:08:49,000
and helping you along to express gratitude to your parents

115
00:08:49,000 --> 00:08:51,000
in the most ritually prescribed manner.

116
00:08:51,000 --> 00:08:54,000
Other people have different experiences.

117
00:08:54,000 --> 00:09:00,000
There are probably hundreds of millions of people trapped in pornographic universes

118
00:09:00,000 --> 00:09:03,000
and effectively mind controlled by AI.

119
00:09:03,000 --> 00:09:07,000
That would maybe be the second largest demographic, if I really think about it.

120
00:09:07,000 --> 00:09:11,000
And there are lots and lots of people, like the ones we discussed at the end,

121
00:09:11,000 --> 00:09:18,000
living in old age homes and having their experiences mediated through a somewhat more tasteful

122
00:09:19,000 --> 00:09:25,000
but still like relatively liberal and relatively cultivated sense of benevolence.

123
00:09:25,000 --> 00:09:30,000
But the prospect of AGI coming online at all changes that.

124
00:09:30,000 --> 00:09:37,000
In some sense, these stories were intended to point at the extreme instability of the world that I produced.

125
00:09:37,000 --> 00:09:42,000
So we have one story about producing a piece of transhuman music

126
00:09:42,000 --> 00:09:50,000
and one story about consuming it despite the cautions of the companies around AGI

127
00:09:50,000 --> 00:10:00,000
under the basically reasonable assumption that music was not existentially dangerous under normal circumstances.

128
00:10:00,000 --> 00:10:05,000
Yeah, so you're referring to, in your world, there's this system that DeepMind has called siren,

129
00:10:05,000 --> 00:10:07,000
which is I think kind of the only AGI in your world.

130
00:10:07,000 --> 00:10:09,000
It's under very tight wraps.

131
00:10:09,000 --> 00:10:12,000
Everyone's really carefully screened and there's follow-up monitoring,

132
00:10:12,000 --> 00:10:15,000
even if they just hear the music that it produces.

133
00:10:15,000 --> 00:10:20,000
This system has also written some books on a few topics that have been carefully curated.

134
00:10:20,000 --> 00:10:24,000
I'm curious what broader impacts siren's existence has on your world,

135
00:10:24,000 --> 00:10:26,000
given kind of how cloistered it is.

136
00:10:28,000 --> 00:10:30,000
I mean, none by design.

137
00:10:30,000 --> 00:10:34,000
Allowing it to have more than the tiniest amount of impact on the world

138
00:10:34,000 --> 00:10:36,000
would be allowing the world to end almost immediately.

139
00:10:36,000 --> 00:10:39,000
So instead, yeah, your world really dives into narrow AIs.

140
00:10:39,000 --> 00:10:42,000
So these are systems that are very good at just a few specific tasks,

141
00:10:42,000 --> 00:10:44,000
like playing chess or driving a car.

142
00:10:44,000 --> 00:10:48,000
No, much broader than that, like the AIs we have today, like GPT,

143
00:10:48,000 --> 00:10:55,000
which are at least pretty good at most things that we tend to think of as intellectual tasks

144
00:10:55,000 --> 00:11:01,000
and very, very, very good at most things that we tend to think of as perceptual

145
00:11:01,000 --> 00:11:07,000
or as extremely rehearsed short-term actions without a lot of context sensitivity.

146
00:11:07,000 --> 00:11:08,000
I see.

147
00:11:08,000 --> 00:11:11,000
So these are like, you know, souped up, narrow AI systems.

148
00:11:11,000 --> 00:11:16,000
They're still not AGI's, but they're kind of the most effective extension

149
00:11:16,000 --> 00:11:19,000
of today's technologies like chat GPT and things like that, as you're saying.

150
00:11:19,000 --> 00:11:24,000
They're general enough that for the vast majority of the world's population,

151
00:11:24,000 --> 00:11:29,000
they probably are vaguely thought of as generally intelligent,

152
00:11:29,000 --> 00:11:34,000
like the vast majority of the world's people probably don't understand very well

153
00:11:34,000 --> 00:11:37,000
the differences between them and AGI's.

154
00:11:37,000 --> 00:11:42,000
And that's probably part of why there's essentially no funding or work on AGI outside of DeepMind.

155
00:11:42,000 --> 00:11:43,000
Yeah, interesting.

156
00:11:43,000 --> 00:11:49,000
And like broadly speaking, they're sufficient to produce some level of relatively benign,

157
00:11:49,000 --> 00:11:54,000
not totally impenetrable, but close enough, a global mind control system

158
00:11:54,000 --> 00:12:01,000
that also contributes to not understanding the differences and also not pursuing AGI.

159
00:12:01,000 --> 00:12:07,000
In some ways, I think that the fiction that my world most reminds me of

160
00:12:07,000 --> 00:12:11,000
is probably Who Framed Roger Rabbit, where they have these tunes everywhere.

161
00:12:11,000 --> 00:12:13,000
And the tunes can talk.

162
00:12:13,000 --> 00:12:15,000
They have personalities.

163
00:12:15,000 --> 00:12:20,000
They have something kind of like agency, but they don't seem to, for the most part,

164
00:12:20,000 --> 00:12:23,000
have agency with any scale.

165
00:12:23,000 --> 00:12:28,000
It's like an extremely rare, extremely dangerous thing for a tune like Judge Doom

166
00:12:28,000 --> 00:12:31,000
to have agency with scale and scope.

167
00:12:31,000 --> 00:12:38,000
And when they do, like Judge Doom, it's agency with an extremely inhuman focus, scale and scope.

168
00:12:38,000 --> 00:12:40,000
So very potentially dangerous.

169
00:12:40,000 --> 00:12:46,000
And the tunes are in some sense extremely cheap and disposable, easy to produce,

170
00:12:46,000 --> 00:12:48,000
but in some sense immortal.

171
00:12:48,000 --> 00:12:53,000
And the humans are like completely clueless about the glaring ways

172
00:12:53,000 --> 00:12:57,000
in which the tunes' capabilities are less than human,

173
00:12:57,000 --> 00:13:01,000
such as Roger Rabbit can only do things when it's funny,

174
00:13:01,000 --> 00:13:05,000
but fairly clueless about the ways in which their abilities are more than human,

175
00:13:05,000 --> 00:13:08,000
like they can survive having a piano dropped on their head.

176
00:13:08,000 --> 00:13:12,000
I actually haven't seen that movie, but I'm really excited now to watch it with this metaphor in mind.

177
00:13:12,000 --> 00:13:14,000
It's a really cool connection.

178
00:13:14,000 --> 00:13:15,000
Yeah.

179
00:13:15,000 --> 00:13:18,000
So one thing you say that these systems can do in your world

180
00:13:18,000 --> 00:13:22,000
is basically replace all white collar workers in theory,

181
00:13:22,000 --> 00:13:25,000
but you say this doesn't happen and you say basically, you know,

182
00:13:25,000 --> 00:13:30,000
there are various reasons, political and personal, why humans are still employed.

183
00:13:30,000 --> 00:13:36,000
I'm curious what kinds of work humans do and what it's like for these human workers in this situation.

184
00:13:36,000 --> 00:13:41,000
So I think basically it depends on their organization,

185
00:13:41,000 --> 00:13:44,000
but in the pretty large majority of organizations,

186
00:13:44,000 --> 00:13:52,000
it's pure office politics and getting therapy from not peak human ability,

187
00:13:52,000 --> 00:13:58,000
but good enough AI therapists to recover enough from the office politics

188
00:13:58,000 --> 00:14:03,000
that they only kill themselves with like drug overdoses and the like

189
00:14:03,000 --> 00:14:08,000
at maybe a third or a fourth the rate that they do in our world.

190
00:14:08,000 --> 00:14:14,000
And maybe even less if AI enhanced medicine makes such drug significantly less deadly

191
00:14:14,000 --> 00:14:17,000
and treatment significantly more effective.

192
00:14:17,000 --> 00:14:18,000
Yeah.

193
00:14:18,000 --> 00:14:20,000
Your world still has a ton of economic inequality,

194
00:14:20,000 --> 00:14:24,000
but the actual quality of life that you describe is kind of universally pretty good.

195
00:14:24,000 --> 00:14:27,000
Like travel has become really cheap and there's basically free energy.

196
00:14:27,000 --> 00:14:31,000
It makes food distribution really trivial as people can kind of live wherever they want

197
00:14:31,000 --> 00:14:34,000
and they have augmented reality, so it'll always look beautiful.

198
00:14:34,000 --> 00:14:37,000
I'm curious, given all of these kind of unifying factors,

199
00:14:37,000 --> 00:14:39,000
how people decide where to build their lives

200
00:14:39,000 --> 00:14:42,000
and what kinds of goals they decide to pursue with them.

201
00:14:42,000 --> 00:14:47,000
So the world that I'm thinking of, for the large majority of people,

202
00:14:47,000 --> 00:14:50,000
they start exploring the world when they're children

203
00:14:50,000 --> 00:14:53,000
and hopefully their parents take a lot of interest in them.

204
00:14:53,000 --> 00:14:56,000
But if not, there's an infinite amount of attention freely available

205
00:14:56,000 --> 00:15:00,000
from the web and from open source and commercial products.

206
00:15:00,000 --> 00:15:05,000
And the decisions they make throughout their lives are almost entirely determined

207
00:15:05,000 --> 00:15:11,000
by what sorts of commercial or open source products find them first, in a sense,

208
00:15:11,000 --> 00:15:18,000
and build the sort of feedback loops that pull them into one or another bubble reality.

209
00:15:18,000 --> 00:15:20,000
You have this interesting thread in your world

210
00:15:20,000 --> 00:15:23,000
where families kind of become a currency

211
00:15:23,000 --> 00:15:27,000
or a kind of wealth that people pursue more than monetary assets.

212
00:15:27,000 --> 00:15:30,000
Can you say a little bit about what that looks like?

213
00:15:30,000 --> 00:15:32,000
I mean, that's just being a normal person.

214
00:15:32,000 --> 00:15:35,000
We've lost touch with it in, you know, late stage capitalism.

215
00:15:35,000 --> 00:15:40,000
But even under normal capitalism, this was not confusing to anybody.

216
00:15:40,000 --> 00:15:45,000
You know, the idea of trying to accumulate wealth

217
00:15:45,000 --> 00:15:48,000
rather than trying to accumulate happy help,

218
00:15:48,000 --> 00:15:52,000
the wise flourishing and mutually cooperative descendants

219
00:15:52,000 --> 00:15:57,000
is like a really surprising thing to find an organism doing.

220
00:15:57,000 --> 00:16:02,000
So thinking about some of the more unusual aspects of your world,

221
00:16:02,000 --> 00:16:06,000
your world definitely had some of the wildest kind of one-off ideas in it

222
00:16:06,000 --> 00:16:07,000
that we saw in the competition.

223
00:16:07,000 --> 00:16:10,000
You have like the Taliban creates luxury war themed amusement parks.

224
00:16:10,000 --> 00:16:13,000
You have elephants that are domesticated by CRISPR.

225
00:16:13,000 --> 00:16:17,000
And you even have Kanye West creating a virtual reproduction of biblical Jerusalem.

226
00:16:17,000 --> 00:16:21,000
I'm curious like what prompted these kinds of details to be included

227
00:16:21,000 --> 00:16:24,000
and whether they're part of a larger theme for you that you were trying to convey.

228
00:16:24,000 --> 00:16:29,000
So the biggest thing that I left out of the actual thing that Matija's influence

229
00:16:29,000 --> 00:16:38,000
was a coup by the comedy party where basically in the 2032 election

230
00:16:38,000 --> 00:16:42,000
between AOC and Donald Trump,

231
00:16:42,000 --> 00:16:47,000
the mainstream Democrats, which still basically control the media and the courts,

232
00:16:47,000 --> 00:16:52,000
decide to allow a completely flagrant election fraud to control John Stuart

233
00:16:52,000 --> 00:16:55,000
as a third party president.

234
00:16:55,000 --> 00:17:02,000
And, you know, so that one I think Matija thought was too political, too controversial.

235
00:17:02,000 --> 00:17:05,000
But I do think it's the sort of thing that could realistically happen.

236
00:17:05,000 --> 00:17:08,000
Overall, where are these coming from?

237
00:17:08,000 --> 00:17:12,000
I mean, some of them are just like extreme low hanging fruit,

238
00:17:12,000 --> 00:17:17,000
things that a few college kids could throw together as a project

239
00:17:17,000 --> 00:17:24,000
in a world with AI capabilities that I realistically expect to exist well before the 2045 deadline.

240
00:17:24,000 --> 00:17:27,000
Yeah. So this is kind of just speaking to maybe like the chaos

241
00:17:27,000 --> 00:17:30,000
and the power flying around the instability of things

242
00:17:30,000 --> 00:17:33,000
and how the world is just going to get so much stranger.

243
00:17:33,000 --> 00:17:35,000
Yeah, I don't think of it as a chaotic world.

244
00:17:35,000 --> 00:17:40,000
The stories are super, super non-chaotic about people living very calm lives.

245
00:17:40,000 --> 00:17:47,000
I see it as a world that's very, very unstable simply because it has even one AGI in it.

246
00:17:47,000 --> 00:17:53,000
And like sooner or later, a more permanent solution is necessary

247
00:17:53,000 --> 00:17:59,000
than just keeping its interest cyber focused and keeping people from noticing it very much.

248
00:17:59,000 --> 00:18:02,000
To some degree, I'm just trying to show a picture,

249
00:18:02,000 --> 00:18:04,000
because that's all you can do in a story like this,

250
00:18:04,000 --> 00:18:08,000
but a picture where all of the pieces are scientifically well founded,

251
00:18:08,000 --> 00:18:11,000
technologically, economically and politically well founded,

252
00:18:11,000 --> 00:18:14,000
make sense and fit together fairly well.

253
00:18:14,000 --> 00:18:18,000
I guess more than anything else, I'm trying to show people like the contest is trying to show people

254
00:18:18,000 --> 00:18:22,000
that it is even possible to make a sincere, serious and competent effort

255
00:18:22,000 --> 00:18:26,000
to depict a realistic but optimistic future.

256
00:18:26,000 --> 00:18:38,000
Major changes are hacking away at the foundations of this world's systems.

257
00:18:38,000 --> 00:18:42,000
The loss of shared reality and weakening of governmental structures, at least in the West,

258
00:18:42,000 --> 00:18:45,000
seemed to strip humanity of a good deal of agency.

259
00:18:45,000 --> 00:18:51,000
It's implied that we're being kept from destruction only by our tenuous control of this world's one true AGI.

260
00:18:51,000 --> 00:18:55,000
At the same time, new approaches to things like education and social conflict

261
00:18:55,000 --> 00:18:59,000
signal hope for building a more coherent and empowered humanity.

262
00:18:59,000 --> 00:19:06,000
I wanted to hear more about how Michael saw this world approaching the changes and challenges that it faced.

263
00:19:06,000 --> 00:19:10,000
You write that in America, like Microsoft, Amazon, Tesla and Walmart

264
00:19:10,000 --> 00:19:14,000
are basically the only entities capable of large scale coordinated action anymore

265
00:19:14,000 --> 00:19:18,000
and elected government officials really just enact change by influencing their supporters

266
00:19:18,000 --> 00:19:20,000
rather than by pursuing any kind of legislation.

267
00:19:20,000 --> 00:19:22,000
Most decisions are made locally.

268
00:19:22,000 --> 00:19:25,000
Can you say a little bit more about how America's governmental systems

269
00:19:25,000 --> 00:19:27,000
lose so much influence in your world?

270
00:19:27,000 --> 00:19:31,000
I just see that as a continuation of the trend that we're already on.

271
00:19:31,000 --> 00:19:36,000
When you look at COVID, the government took an unbelievably huge amount of oppressive

272
00:19:36,000 --> 00:19:41,000
and authoritarian action that there probably won't be social or political support for

273
00:19:41,000 --> 00:19:44,000
if there's another major event that calls for it.

274
00:19:44,000 --> 00:19:47,000
It lost an enormous amount of public trust.

275
00:19:48,000 --> 00:19:53,000
If you look at what the government did that was effective with COVID,

276
00:19:53,000 --> 00:19:56,000
it basically boils down to printing enormous amounts of money

277
00:19:56,000 --> 00:20:03,000
and providing certain types of encouragement to conform to a certain standard.

278
00:20:03,000 --> 00:20:06,000
It's not that the government no longer matters.

279
00:20:06,000 --> 00:20:10,000
It's just that popularity contests should be.

280
00:20:10,000 --> 00:20:14,000
It's primarily a source of information about how to be popular.

281
00:20:14,000 --> 00:20:17,000
Just like in our world, people mostly want to be popular.

282
00:20:17,000 --> 00:20:21,000
They don't want it as much as in our world because they can always be popular with AIs.

283
00:20:21,000 --> 00:20:28,000
But still, AIs are not fully satisfying as mental and social companions.

284
00:20:28,000 --> 00:20:34,000
As this power switches over and flows towards tech companies gaining influence,

285
00:20:34,000 --> 00:20:36,000
it becomes increasingly hard to track wealth.

286
00:20:36,000 --> 00:20:41,000
In some ways, it also seems like things are just going on sheer inertia.

287
00:20:41,000 --> 00:20:44,000
You have this great line in your submission that says,

288
00:20:44,000 --> 00:20:47,000
a supermajority of the population has negative net worth

289
00:20:47,000 --> 00:20:50,000
and continues to be allocated credit as a matter of economic policy.

290
00:20:50,000 --> 00:20:53,000
You mentioned this instability of the world.

291
00:20:53,000 --> 00:20:55,000
How long do you see it remaining stable?

292
00:20:55,000 --> 00:21:00,000
Will these systems fall apart shortly after 2045 and you're imagining?

293
00:21:00,000 --> 00:21:05,000
The way I'm imagining this, this is a fairly close to best case scenario.

294
00:21:05,000 --> 00:21:12,000
My realistic best guess scenario would be that it's more than 70% likely,

295
00:21:12,000 --> 00:21:14,000
maybe more than 80% likely,

296
00:21:14,000 --> 00:21:20,000
that the system that I'm describing falls apart well before it gets to the point that I'm describing.

297
00:21:20,000 --> 00:21:23,000
These are supposed to be optimistic visions for the future.

298
00:21:23,000 --> 00:21:28,000
But once it gets to the point that I'm describing, if it gets to that point,

299
00:21:28,000 --> 00:21:32,000
I actually imagine it being stable for a pretty long time.

300
00:21:32,000 --> 00:21:34,000
Except for the edge, I think.

301
00:21:34,000 --> 00:21:37,000
Yeah, Siren gets up.

302
00:21:37,000 --> 00:21:42,000
One big tension in your world as a result of this increasing difficulty

303
00:21:42,000 --> 00:21:48,000
and verifying information is just people have a hard time agreeing on objective reality.

304
00:21:48,000 --> 00:21:51,000
They're really good in experimental healthcare interventions,

305
00:21:51,000 --> 00:21:55,000
but it's mostly about luck and maybe some skill to pick the winners out of that crowd.

306
00:21:55,000 --> 00:22:00,000
You have cryptocurrency that's made it really impossible to tell how much money anyone has.

307
00:22:00,000 --> 00:22:03,000
You mentioned that instead of Forbes keeping track of wealth,

308
00:22:03,000 --> 00:22:07,000
now kidnapping rings keep some of the best records of people's total assets.

309
00:22:07,000 --> 00:22:13,000
You even say that startups are buying these records off of those kidnapping rings to find wealthy funders.

310
00:22:13,000 --> 00:22:19,000
Can you say a little bit more about what leads to this deep fracturing of shared objectivity?

311
00:22:19,000 --> 00:22:24,000
That's been going on really in a big way since the 1940s.

312
00:22:24,000 --> 00:22:29,000
Once again, I'm just imagining it continuing and accelerating with more powerful technologies.

313
00:22:30,000 --> 00:22:36,000
The collapse of the dollar, which happens in the 2030s more or less in my story,

314
00:22:36,000 --> 00:22:38,000
contributes a fair amount.

315
00:22:38,000 --> 00:22:41,000
It makes the crypto thing much more substantial.

316
00:22:41,000 --> 00:22:47,000
And the increase like basically social welfare through senior age

317
00:22:47,000 --> 00:22:51,000
and the expansion of senior age through the population

318
00:22:51,000 --> 00:22:55,000
helps to stabilize things a lot at the expense of coherence and efficiency,

319
00:22:55,000 --> 00:22:57,000
which isn't really necessary anymore.

320
00:22:57,000 --> 00:22:59,000
Could you say what senior age is?

321
00:22:59,000 --> 00:23:04,000
Senior age is printing money through bank activities.

322
00:23:04,000 --> 00:23:08,000
When banks borrow money, then they lend out much more money.

323
00:23:08,000 --> 00:23:12,000
And there's a stack of different interest rates paid by different creditors.

324
00:23:12,000 --> 00:23:17,000
One of the basic challenges of running a capitalist society

325
00:23:17,000 --> 00:23:20,000
that's been well understood since long before Adam Smith

326
00:23:20,000 --> 00:23:25,000
is the extreme difficulty of causing control of the money printing apparatus

327
00:23:25,000 --> 00:23:30,000
to not be the convergent agenda of practically everyone.

328
00:23:30,000 --> 00:23:35,000
And most capitalist societies do collapse as control of the money printing apparatus

329
00:23:35,000 --> 00:23:37,000
becomes a convergent agenda.

330
00:23:37,000 --> 00:23:44,000
So I'm basically imagining the essential worker system that we discovered we had during COVID

331
00:23:44,000 --> 00:23:49,000
and the relatively resilient management of a small number of companies

332
00:23:49,000 --> 00:23:52,000
basically keeping the material reality held together.

333
00:23:52,000 --> 00:23:57,000
Despite the fact that the vast majority of supposed economic activity

334
00:23:57,000 --> 00:24:01,000
is actually pure political wealth redistribution

335
00:24:01,000 --> 00:24:07,000
to the people who bother to fight for wealth being distributed to them in a world

336
00:24:07,000 --> 00:24:11,000
where most people have basically lost track of wealth anyway.

337
00:24:11,000 --> 00:24:17,000
I'm curious why AI systems don't help more with these issues of shared goals and shared knowledge.

338
00:24:17,000 --> 00:24:20,000
You mentioned that AI systems can provide common knowledge,

339
00:24:20,000 --> 00:24:24,000
like they help groups of people identify if their behaviors are aligned with their goals

340
00:24:24,000 --> 00:24:26,000
or how to change their behaviors.

341
00:24:26,000 --> 00:24:29,000
You would think that that might cut through the haze

342
00:24:29,000 --> 00:24:34,000
and help people agree on things more or have more transparency.

343
00:24:34,000 --> 00:24:39,000
AI systems help enormously with establishing whatever set of goals

344
00:24:39,000 --> 00:24:45,000
is reasonably psychologically plausible and that the systems designers want to establish.

345
00:24:45,000 --> 00:24:50,000
But mostly that consists of consuming products just like it does in our world.

346
00:24:50,000 --> 00:24:55,000
And in the rare cases of societies that have more of a shared set of values

347
00:24:55,000 --> 00:24:58,000
and more of a shared power structure like China,

348
00:24:58,000 --> 00:25:03,000
it means that they have incredibly high integration and unity

349
00:25:03,000 --> 00:25:08,000
targeting shared goals that more or less consist of normal reasonable things

350
00:25:08,000 --> 00:25:14,000
like extending life and ecological sustainability and stability in general.

351
00:25:15,000 --> 00:25:19,000
One other thread I really enjoyed in your world is how you talk about education changing.

352
00:25:19,000 --> 00:25:24,000
So people start to see traditional educational pedigrees as a form of inherited privilege.

353
00:25:24,000 --> 00:25:27,000
And educational histories actually become private information

354
00:25:27,000 --> 00:25:31,000
which can't be used in decisions like hiring, which is a really interesting concept.

355
00:25:31,000 --> 00:25:35,000
And this tips the scales in favor of online self-driven education.

356
00:25:35,000 --> 00:25:39,000
Schools basically go empty while kids live with their families and learn on their own.

357
00:25:39,000 --> 00:25:41,000
I'm curious what this looks like for those kids.

358
00:25:41,000 --> 00:25:43,000
What are they learning? What aren't they learning?

359
00:25:43,000 --> 00:25:45,000
And who's deciding?

360
00:25:45,000 --> 00:25:52,000
So I'm basically imagining that nominally the parents decide when the kids are younger

361
00:25:52,000 --> 00:25:55,000
and the kids decide when they're older.

362
00:25:55,000 --> 00:26:01,000
But in practice, reasonably agentic parents who are also tech savvy

363
00:26:01,000 --> 00:26:08,000
and have like reasonably coherent preferences about what to get

364
00:26:08,000 --> 00:26:14,000
will be able to direct their kids towards media bubbles and narratives

365
00:26:14,000 --> 00:26:20,000
that will be extremely stable and which won't change much unless something really weird happens.

366
00:26:20,000 --> 00:26:27,000
So I expect that almost everyone's learning speed is going to be like

367
00:26:27,000 --> 00:26:34,000
at least four or five times faster between more targeted instruction, objectively better instruction,

368
00:26:34,000 --> 00:26:39,000
maybe learning enhancement through drugs and mRNA tech.

369
00:26:39,000 --> 00:26:43,000
And much better trauma care is a major feature of my world.

370
00:26:43,000 --> 00:26:49,000
So just the elimination of mental blocks through MDMA therapies and their successors.

371
00:26:49,000 --> 00:26:56,000
I don't know if I really played up adequately the spread of a new way of doing civilization

372
00:26:56,000 --> 00:27:00,000
from the carceral system into the general population

373
00:27:00,000 --> 00:27:05,000
as like MDMA therapies get adopted for dispute resolution within prisons

374
00:27:05,000 --> 00:27:13,000
and reach a level of reliability and efficacy that's sufficient that basically everyone wants some.

375
00:27:21,000 --> 00:27:24,000
Despite some of the more madcap details of their world,

376
00:27:24,000 --> 00:27:28,000
this team expresses a strong commitment to realism and plausibility.

377
00:27:28,000 --> 00:27:33,000
Their portrayal of AI development was also perhaps the slowest and most restricted among our winners.

378
00:27:33,000 --> 00:27:37,000
While there isn't AGI around, most of the technological developments in this world

379
00:27:37,000 --> 00:27:43,000
are just extensions of today's narrow AI systems whose awesome capabilities are ultimately limited.

380
00:27:43,000 --> 00:27:46,000
I was curious to hear more about this team's creative influences

381
00:27:46,000 --> 00:27:51,000
and whether this slower pace of AI development was something they saw as merely likely

382
00:27:51,000 --> 00:27:56,000
or a necessary component of any safe path to an aspirational future.

383
00:27:57,000 --> 00:28:01,000
So I'd like to spend a little while discussing the narratives in your world

384
00:28:01,000 --> 00:28:05,000
and how they compare to other narratives that are going around in popular culture.

385
00:28:05,000 --> 00:28:10,000
Like one really big through line for me is this sort of emperor has no close attitude you have

386
00:28:10,000 --> 00:28:14,000
towards economics and politics where your world kind of just goes through the motions

387
00:28:14,000 --> 00:28:18,000
to keep things moving along but the systems themselves are no longer really doing much.

388
00:28:18,000 --> 00:28:24,000
I'm curious if there are other examples of this perspective that inspired you in other kinds of media?

389
00:28:25,000 --> 00:28:30,000
I mean, mostly I'm inspired by real life, not by media and narratives.

390
00:28:30,000 --> 00:28:35,000
I can't think of a piece of fiction that is as radical as real life

391
00:28:35,000 --> 00:28:39,000
in the degree to which it violates conventional assumptions.

392
00:28:39,000 --> 00:28:46,000
You know, it's basically impossible to do without being a top tier literary genius like Shakespeare.

393
00:28:46,000 --> 00:28:51,000
I mean, Hamlet's wonderful Doris Lessig's book, The Golden Notebook,

394
00:28:51,000 --> 00:28:54,000
is maybe the best depiction I've ever seen,

395
00:28:54,000 --> 00:29:00,000
but one would need to be a really, really good literary scholar to appreciate it, I think.

396
00:29:00,000 --> 00:29:01,000
Same with Hamlet.

397
00:29:01,000 --> 00:29:02,000
Interesting.

398
00:29:03,000 --> 00:29:06,000
Well, I'm curious if there are any examples,

399
00:29:06,000 --> 00:29:10,000
and this can come from philosophers as well as fiction,

400
00:29:10,000 --> 00:29:14,000
of economic or political systems that could actually maybe function in a world like yours,

401
00:29:14,000 --> 00:29:20,000
or do you think that the whole concept of having a system that's run in a sensible way is kind of moot?

402
00:29:20,000 --> 00:29:21,000
No.

403
00:29:21,000 --> 00:29:26,000
I mean, the Chinese system is sort of run in a sensible way in the world I'm describing.

404
00:29:26,000 --> 00:29:30,000
It's not run with perfect rigor and resolution.

405
00:29:30,000 --> 00:29:33,000
It wouldn't pass like Talmudic standards.

406
00:29:33,000 --> 00:29:38,000
But by the standards that we're used to from government,

407
00:29:38,000 --> 00:29:43,000
I'm imagining a China with a life expectancy of well over 100 years,

408
00:29:43,000 --> 00:29:47,000
and the ability to industrially produce in a clean way,

409
00:29:47,000 --> 00:29:51,000
and with very little labor, practically everything the entire world needs.

410
00:29:51,000 --> 00:29:56,000
The goals of maximizing filial piety and ren

411
00:29:56,000 --> 00:30:00,000
are just going to be what's inherited from their ancestors and traditions,

412
00:30:00,000 --> 00:30:04,000
and it may not seem like doing a thing to us,

413
00:30:04,000 --> 00:30:11,000
but most of what we do is arguably not really doing a thing.

414
00:30:12,000 --> 00:30:16,000
Do you think there are any actions or reforms we could do to Western systems

415
00:30:16,000 --> 00:30:20,000
that would make them more resilient to these changes as well?

416
00:30:20,000 --> 00:30:24,000
I mean, my simple answer is I already put them all into this story.

417
00:30:24,000 --> 00:30:29,000
That's why the world is still alive and has not collapsed already.

418
00:30:29,000 --> 00:30:36,000
I'm making a number of surprising good luck-happens assumptions,

419
00:30:36,000 --> 00:30:38,000
not extraordinary.

420
00:30:38,000 --> 00:30:43,000
I really try to keep avoid endorsing things that are not just quirky

421
00:30:43,000 --> 00:30:47,000
and that have probabilities of less than about 10%.

422
00:30:47,000 --> 00:30:51,000
I think it's important to note that our world would be way scarier

423
00:30:51,000 --> 00:30:56,000
to people from my vision of 2045 than their world would be from us.

424
00:30:56,000 --> 00:30:59,000
Their world would just be incredibly addictive,

425
00:30:59,000 --> 00:31:04,000
and we would very quickly find ourselves trapped in some relatively exploitative bubble.

426
00:31:04,000 --> 00:31:10,000
But even exploitative bubbles have reasons to try to keep people mentally healthy enough

427
00:31:11,000 --> 00:31:17,000
to keep on receiving government benefits within a thin veneer of contributing to the economy.

428
00:31:17,000 --> 00:31:22,000
I guess one way to think about it is the American dream is basically a colpage

429
00:31:22,000 --> 00:31:26,000
of the America prior to the Civil War,

430
00:31:26,000 --> 00:31:30,000
America between the Civil War and the New Deal,

431
00:31:30,000 --> 00:31:32,000
and America after the New Deal,

432
00:31:32,000 --> 00:31:34,000
which could be summarized as the colonist experience,

433
00:31:34,000 --> 00:31:38,000
the immigrant experience, and the GI experience.

434
00:31:38,000 --> 00:31:44,000
And none of these experiences at all resemble what Zoomers are coming into and experiencing.

435
00:31:44,000 --> 00:31:48,000
And so they are growing up in a world of such transparent lies

436
00:31:48,000 --> 00:31:52,000
that they're almost without exception total epistemic nihilists

437
00:31:52,000 --> 00:31:55,000
mistakenly disbelieve that anything was ever true

438
00:31:55,000 --> 00:31:59,000
rather than only disbelieving that anything that they've ever seen is true,

439
00:31:59,000 --> 00:32:02,000
which is actually the case.

440
00:32:02,000 --> 00:32:07,000
So one really unique thing about your world is this focus on the narrow AI systems

441
00:32:07,000 --> 00:32:10,000
and how high a ceiling you put on their abilities.

442
00:32:10,000 --> 00:32:13,000
You kind of have basically a suite of different narrow AI systems

443
00:32:13,000 --> 00:32:16,000
that together have the capabilities of an AGI in some ways,

444
00:32:16,000 --> 00:32:18,000
but they're spread across these separate modules.

445
00:32:18,000 --> 00:32:20,000
No, they don't have the capabilities of an AGI.

446
00:32:20,000 --> 00:32:23,000
They don't have anything even remotely close to the abilities of an AGI.

447
00:32:23,000 --> 00:32:25,000
Can you distinguish that?

448
00:32:25,000 --> 00:32:28,000
The story is just kind of hinting at the capabilities of an AGI

449
00:32:28,000 --> 00:32:33,000
with the sort of security around it and the sort of implied impact

450
00:32:33,000 --> 00:32:35,000
and like potential risk.

451
00:32:35,000 --> 00:32:38,000
I'm operating with the definition of AGI that's something like a system

452
00:32:38,000 --> 00:32:42,000
that's better than a human at any task that you can reasonably define.

453
00:32:42,000 --> 00:32:46,000
Is that different from what you say when you say that these narrow AI systems?

454
00:32:46,000 --> 00:32:49,000
No, better than any human at any task that you can reasonably define.

455
00:32:49,000 --> 00:32:55,000
I'm saying that the systems that I'm describing are not even remotely close to that.

456
00:32:55,000 --> 00:33:00,000
They're like superhuman at very narrow tasks.

457
00:33:00,000 --> 00:33:04,000
They're superhuman at a lot of very narrow tasks.

458
00:33:05,000 --> 00:33:09,000
But it doesn't even come close, fitting them all together

459
00:33:09,000 --> 00:33:11,000
to the full range of human capabilities.

460
00:33:11,000 --> 00:33:14,000
I see. So there's kind of a synergy here, you're saying.

461
00:33:14,000 --> 00:33:17,000
Right, and then they're like not superhuman,

462
00:33:17,000 --> 00:33:24,000
but like merely as good as the experts that top elites tend to point to

463
00:33:24,000 --> 00:33:31,000
at the vast majority of tasks that get measured and graded

464
00:33:31,000 --> 00:33:34,000
and systematized and standardized threat society.

465
00:33:34,000 --> 00:33:40,000
So the best doctors in my world are still humans who make very heavy use of AI tools,

466
00:33:40,000 --> 00:33:46,000
but the best purely AI doctors might only be as good as the doctors

467
00:33:46,000 --> 00:33:49,000
that like the president has in our world,

468
00:33:49,000 --> 00:33:53,000
but not nearly as good as the doctors that like a top doctor has in our world

469
00:33:53,000 --> 00:33:57,000
since the top doctors know who the actual best doctors are and to date them.

470
00:33:57,000 --> 00:34:02,000
So not only do these systems not exceed the best human experts individually

471
00:34:02,000 --> 00:34:06,000
at these narrower tasks, but you're also saying that there's something missing

472
00:34:06,000 --> 00:34:09,000
even if you have this collection of narrow systems

473
00:34:09,000 --> 00:34:11,000
that can each do something that a human can do.

474
00:34:11,000 --> 00:34:14,000
Just putting those together is not the same as having something

475
00:34:14,000 --> 00:34:17,000
that could do all of these flexibly, is that what you're saying?

476
00:34:17,000 --> 00:34:21,000
Definitely, but also there are things that none of them can do even a little bit.

477
00:34:21,000 --> 00:34:23,000
Like in the story that I'm talking about,

478
00:34:23,000 --> 00:34:28,000
Siren is the only AI in the world that could, if it wanted to,

479
00:34:28,000 --> 00:34:30,000
do important original mathematics.

480
00:34:30,000 --> 00:34:32,000
It's the only AI in the world that could, if it wanted to,

481
00:34:32,000 --> 00:34:36,000
make the tiniest contribution to theoretical world-plied physics.

482
00:34:36,000 --> 00:34:41,000
So in your world, you have this incredibly powerful AGI system that does exist,

483
00:34:41,000 --> 00:34:45,000
but it's under really, really strong protections under tight wraps.

484
00:34:45,000 --> 00:34:50,000
Do you think that this is necessary to have a optimistic future with AGI in it?

485
00:34:50,000 --> 00:34:54,000
Yeah, definitely, unless we can basically do, you know,

486
00:34:54,000 --> 00:35:00,000
thousands of years worth of philosophical progress in like 20 years.

487
00:35:00,000 --> 00:35:03,000
And we can't.

488
00:35:03,000 --> 00:35:07,000
Like maybe we can do thousands of years worth of philosophical progress this century

489
00:35:07,000 --> 00:35:13,000
because we will have both AI and other technologies for enhancing our mental capabilities

490
00:35:13,000 --> 00:35:15,000
if we choose to use them.

491
00:35:15,000 --> 00:35:18,000
But we can't do it in 20 years, it's just laughable.

492
00:35:18,000 --> 00:35:19,000
Yeah.

493
00:35:19,000 --> 00:35:23,000
So the limitations that are preventing AGI from developing faster in your world,

494
00:35:23,000 --> 00:35:26,000
some of them are intentional, like policy decisions.

495
00:35:26,000 --> 00:35:28,000
Some of them are just kind of practical ones,

496
00:35:28,000 --> 00:35:32,000
like bad funding, politicization, and the rarity of human expertise.

497
00:35:32,000 --> 00:35:37,000
Do you think these are actual likely causes of slowing development in the real world?

498
00:35:37,000 --> 00:35:43,000
Yeah, that's my, my best guess is that AGI will progress much more slowly

499
00:35:43,000 --> 00:35:46,000
than I have it progressing in my story.

500
00:35:46,000 --> 00:35:50,000
And my best guess is that we do survive

501
00:35:50,000 --> 00:35:53,000
because AGI progresses much more slowly.

502
00:35:53,000 --> 00:35:57,000
From my perspective, it's extremely contrived for AGI to be,

503
00:35:57,000 --> 00:36:00,000
to develop even as fast as it does in this story

504
00:36:00,000 --> 00:36:05,000
and be handled well enough, cautiously enough, thoughtfully enough

505
00:36:05,000 --> 00:36:10,000
that like we have more than a fraction of a percent chance of survival.

506
00:36:10,000 --> 00:36:12,000
Yeah.

507
00:36:12,000 --> 00:36:14,000
Have you seen other portrayals of the future

508
00:36:14,000 --> 00:36:17,000
where narrow AI plays as much of a role as in yours?

509
00:36:17,000 --> 00:36:21,000
I mean, I feel like there's a lot of portrayals of the future

510
00:36:21,000 --> 00:36:26,000
where narrow AI is taken for granted and plays a large role.

511
00:36:26,000 --> 00:36:29,000
Like in the Star Trek, the next generation,

512
00:36:29,000 --> 00:36:33,000
they have one AGI data, like in my world,

513
00:36:33,000 --> 00:36:37,000
and then they have like an unbelievably powerful narrow AI in the ship

514
00:36:37,000 --> 00:36:40,000
and in the holodeck and just all over the place.

515
00:36:40,000 --> 00:36:45,000
But everyone takes it for granted and it's used as a tool

516
00:36:45,000 --> 00:36:51,000
by a military organization with a relatively unified internal agenda

517
00:36:51,000 --> 00:36:57,000
of exploration and extremely prudish and narrow conceptions

518
00:36:57,000 --> 00:37:03,000
of what types of experiences and behaviors

519
00:37:03,000 --> 00:37:06,000
its members are supposed to engage in.

520
00:37:06,000 --> 00:37:12,000
I will say that the type of narrow AI that we have actually developed

521
00:37:12,000 --> 00:37:20,000
is like pretty broad compared to what I expected five years ago.

522
00:37:20,000 --> 00:37:25,000
It's like very much what we were visibly moving towards four years ago.

523
00:37:25,000 --> 00:37:29,000
But to some degree, when I was growing up,

524
00:37:29,000 --> 00:37:32,000
C3PO seemed like a silly fantasy,

525
00:37:32,000 --> 00:37:34,000
seemed silly that you could have a machine

526
00:37:34,000 --> 00:37:39,000
that was that close to human performance

527
00:37:39,000 --> 00:37:44,000
but like stuck for a long time at below human performance

528
00:37:44,000 --> 00:37:47,000
and in some ways pretty profoundly below

529
00:37:47,000 --> 00:37:50,000
and in some ways pretty profoundly superhuman

530
00:37:50,000 --> 00:37:53,000
but like just be stuck there for a long time.

531
00:37:53,000 --> 00:37:57,000
But it kind of looks from the technologies that open up in AI

532
00:37:57,000 --> 00:38:01,000
is generating that a minimum viable C3PO

533
00:38:01,000 --> 00:38:04,000
might actually happen and be around for a long time

534
00:38:04,000 --> 00:38:08,000
without really drastic improvement from that.

535
00:38:08,000 --> 00:38:12,000
How do you feel about the general portrayals of the future that are in fiction?

536
00:38:12,000 --> 00:38:17,000
Do you think they're over or under optimistic when they try to be optimistic?

537
00:38:17,000 --> 00:38:22,000
I just think optimism in fiction that is trying to be at all realistic

538
00:38:22,000 --> 00:38:29,000
is unfortunately much rarer than it should be.

539
00:38:29,000 --> 00:38:33,000
And like that's basically maybe largely

540
00:38:33,000 --> 00:38:38,000
because the most perceptive and insightful people

541
00:38:38,000 --> 00:38:43,000
who are also successful at becoming prominent

542
00:38:43,000 --> 00:38:47,000
and surrounding themselves with other prominent people

543
00:38:47,000 --> 00:38:52,000
are constantly confronted with lots of profoundly miserable,

544
00:38:52,000 --> 00:38:56,000
extremely zero sum other prominent people

545
00:38:56,000 --> 00:39:00,000
and have very little contact with the large majority of people

546
00:39:00,000 --> 00:39:06,000
who are just not as miserable as the people who like double audience

547
00:39:06,000 --> 00:39:08,000
are going to end up around.

548
00:39:08,000 --> 00:39:11,000
So you're kind of calling for more optimism in nonfiction as well?

549
00:39:11,000 --> 00:39:12,000
Is that where you're going?

550
00:39:12,000 --> 00:39:16,000
I mean, no, I feel like optimism and pessimism is like

551
00:39:16,000 --> 00:39:18,000
intrinsically unhealthy concepts.

552
00:39:18,000 --> 00:39:20,000
You should just try to have true beliefs

553
00:39:20,000 --> 00:39:24,000
but true beliefs should be balanced.

554
00:39:24,000 --> 00:39:30,000
There's a lot of social pressure to performatively be pessimistic

555
00:39:30,000 --> 00:39:33,000
because the elites tend to be pessimistic.

556
00:39:33,000 --> 00:39:35,000
And elites tend to be pessimistic

557
00:39:35,000 --> 00:39:38,000
because they're living in a hyper competitive zero sum world

558
00:39:38,000 --> 00:39:40,000
that most of us are not living in.

559
00:39:40,000 --> 00:39:44,000
And there's a lot of, it's easier in some ways to be pessimistic

560
00:39:44,000 --> 00:39:46,000
especially cheaply pessimistic.

561
00:39:46,000 --> 00:39:51,000
But there's also like just cognitive biases that lead to silly sorts of pessimism.

562
00:39:51,000 --> 00:39:54,000
So like imagine there was a news item

563
00:39:54,000 --> 00:39:58,000
about how it turns out that apples cause cancer.

564
00:39:58,000 --> 00:40:00,000
Practically everyone would see this as bad news.

565
00:40:00,000 --> 00:40:02,000
Oh no, I've been poisoning myself for years.

566
00:40:02,000 --> 00:40:04,000
But obviously it's good news.

567
00:40:04,000 --> 00:40:05,000
We know what the cancer rate is.

568
00:40:05,000 --> 00:40:08,000
Now we know that we can avoid it by not eating apples.

569
00:40:08,000 --> 00:40:10,000
Right, the devil you know.

570
00:40:10,000 --> 00:40:15,000
Like information is almost always desirable

571
00:40:15,000 --> 00:40:19,000
but information about bad things gets interpreted

572
00:40:19,000 --> 00:40:21,000
as something bad happening

573
00:40:21,000 --> 00:40:24,000
rather than being interpreted as something good happening.

574
00:40:24,000 --> 00:40:26,000
You know James Baldwin,

575
00:40:26,000 --> 00:40:30,000
not everything that can be confronted can be overcome

576
00:40:30,000 --> 00:40:34,000
but everything to be overcome must be confronted.

577
00:40:34,000 --> 00:40:38,000
And to some degree this picture that I'm depicting

578
00:40:38,000 --> 00:40:40,000
is an edge case on that

579
00:40:40,000 --> 00:40:45,000
because this is a world that has managed to partially overcome

580
00:40:46,000 --> 00:40:49,000
and fully survive a lot of the problems

581
00:40:49,000 --> 00:40:51,000
that our society is dying from

582
00:40:51,000 --> 00:40:53,000
without really confronting them.

583
00:40:53,000 --> 00:40:56,000
In your world a lot of the role models become virtual.

584
00:40:56,000 --> 00:40:59,000
So basically all celebrities popular with the under 30 crowd

585
00:40:59,000 --> 00:41:01,000
are virtual people.

586
00:41:01,000 --> 00:41:03,000
Some are recreations of historical figures.

587
00:41:03,000 --> 00:41:06,000
Others are kind of amalgams like Tupacalist

588
00:41:06,000 --> 00:41:09,000
and you have XXX, Tentacion, Albus XXX.

589
00:41:09,000 --> 00:41:12,000
I'm curious how these play a role

590
00:41:12,000 --> 00:41:15,000
as kind of role models in your world.

591
00:41:15,000 --> 00:41:17,000
I think they mostly don't.

592
00:41:17,000 --> 00:41:20,000
I think that people who have at least reasonably good taste

593
00:41:20,000 --> 00:41:24,000
do prefer interaction with individuals.

594
00:41:24,000 --> 00:41:28,000
Insofar as they mostly imitate behavior by real people

595
00:41:28,000 --> 00:41:32,000
and that like the social influences from machines

596
00:41:32,000 --> 00:41:36,000
are in general more of a, you know,

597
00:41:36,000 --> 00:41:40,000
goal directed relatively overt manipulation sort.

598
00:41:40,000 --> 00:41:43,000
What are your thoughts on some of the current cultural attitudes

599
00:41:43,000 --> 00:41:45,000
towards like AI generated art

600
00:41:45,000 --> 00:41:47,000
and virtual cultural figures right now?

601
00:41:47,000 --> 00:41:50,000
So it seems like any reasonably good artist

602
00:41:50,000 --> 00:41:52,000
wants to make art,

603
00:41:52,000 --> 00:41:54,000
not wants to get paid for making art

604
00:41:54,000 --> 00:41:56,000
and like maybe wants to be seen

605
00:41:56,000 --> 00:41:58,000
and people can worry that

606
00:41:58,000 --> 00:42:01,000
a lot of people will never be exposed to good art

607
00:42:01,000 --> 00:42:04,000
because they're going to be exposed to an enormous amount of stuff

608
00:42:04,000 --> 00:42:06,000
that doesn't have a message behind it

609
00:42:06,000 --> 00:42:10,000
and isn't created as an expression of pain

610
00:42:10,000 --> 00:42:12,000
and suppressed emotion.

611
00:42:12,000 --> 00:42:15,000
But like if you're a good artist

612
00:42:15,000 --> 00:42:16,000
you can learn new tools

613
00:42:16,000 --> 00:42:18,000
and you keep learning new tools throughout your life

614
00:42:18,000 --> 00:42:21,000
and you learn how to make use of these new tools

615
00:42:21,000 --> 00:42:24,000
to compete and to get your message out there.

616
00:42:24,000 --> 00:42:28,000
The barriers to entry for art in some sense never go down

617
00:42:28,000 --> 00:42:31,000
because they have to do with the attention

618
00:42:31,000 --> 00:42:33,000
and consciousness of your audience.

619
00:42:33,000 --> 00:42:37,000
The economic barriers to entry in our world

620
00:42:37,000 --> 00:42:40,000
are going up because of extreme economic scarcity

621
00:42:40,000 --> 00:42:43,000
that's being created through policy.

622
00:42:43,000 --> 00:42:45,000
So like we're living in a time of

623
00:42:45,000 --> 00:42:47,000
really extreme economic scarcity

624
00:42:47,000 --> 00:42:50,000
compared to the Great Depression at this point.

625
00:42:50,000 --> 00:42:53,000
We've lost way more actual economic freedom

626
00:42:53,000 --> 00:42:55,000
in the sense of like

627
00:42:55,000 --> 00:42:57,000
not needing to work very hard or very well

628
00:42:57,000 --> 00:43:00,000
or be very exceptional in order to afford

629
00:43:00,000 --> 00:43:03,000
to reproduce our own labor

630
00:43:03,000 --> 00:43:05,000
have children and grandchildren

631
00:43:05,000 --> 00:43:08,000
and have them live at least as nicely as we do

632
00:43:08,000 --> 00:43:10,000
and be able to purchase our baskets of goods.

633
00:43:10,000 --> 00:43:12,000
And every Zoomer knows it

634
00:43:12,000 --> 00:43:15,000
because like they in fact can't purchase

635
00:43:15,000 --> 00:43:18,000
the baskets of goods that their parents had

636
00:43:18,000 --> 00:43:21,000
and their parents deny them recognition as adults

637
00:43:21,000 --> 00:43:25,000
even like millennials with kids

638
00:43:25,000 --> 00:43:28,000
are denied recognition as adults in major ways

639
00:43:28,000 --> 00:43:31,000
because they don't have the baskets of consumption goods

640
00:43:31,000 --> 00:43:35,000
that in fact, generationally speaking, they can't have.

641
00:43:43,000 --> 00:43:45,000
The process of world building has great potential

642
00:43:45,000 --> 00:43:47,000
to make a positive future feel more attainable.

643
00:43:47,000 --> 00:43:49,000
This can be incredibly powerful

644
00:43:49,000 --> 00:43:52,000
whether you're a creative person looking to produce rich works of fiction

645
00:43:52,000 --> 00:43:54,000
or have a more technical focus

646
00:43:54,000 --> 00:43:56,000
and are looking to reach policymakers or the public.

647
00:43:56,000 --> 00:43:58,000
I asked Michael what kind of impacts

648
00:43:58,000 --> 00:44:00,000
he hoped his work would have on the world.

649
00:44:00,000 --> 00:44:02,000
What do you hope your world leaves people

650
00:44:02,000 --> 00:44:05,000
thinking about long after they've read through it?

651
00:44:05,000 --> 00:44:08,000
So the biggest thing is that I just would like people

652
00:44:08,000 --> 00:44:12,000
to try to make sense of what could happen.

653
00:44:12,000 --> 00:44:15,000
I would like them to know that it is possible

654
00:44:15,000 --> 00:44:19,000
to make a joint prediction

655
00:44:19,000 --> 00:44:23,000
and expression of preference that is in line with

656
00:44:24,000 --> 00:44:27,000
the relatively full range of scientific and technological

657
00:44:27,000 --> 00:44:29,000
and humanistic thinking

658
00:44:29,000 --> 00:44:31,000
and that holds together and makes sense

659
00:44:31,000 --> 00:44:35,000
and that much more close to comes true

660
00:44:35,000 --> 00:44:38,000
and also somewhat guides society towards it coming true

661
00:44:38,000 --> 00:44:41,000
than what we would normally think of as science fiction.

662
00:44:41,000 --> 00:44:44,000
What kinds of expertise would you be most interested in having

663
00:44:44,000 --> 00:44:48,000
people bring to discussions about your world or the future in general?

664
00:44:48,000 --> 00:44:52,000
I think that the things that I would want to bring in first

665
00:44:52,000 --> 00:44:56,000
would be somatic skills

666
00:44:56,000 --> 00:45:00,000
like body work and yoga

667
00:45:00,000 --> 00:45:03,000
and things like that

668
00:45:03,000 --> 00:45:08,000
experience with MDMA and other psychedelic therapies

669
00:45:08,000 --> 00:45:11,000
and maybe even electrical engineering

670
00:45:11,000 --> 00:45:13,000
for creating better alternatives

671
00:45:13,000 --> 00:45:15,000
to transcranial magnetic stimulation

672
00:45:15,000 --> 00:45:18,000
for disinhibiting some parts of the cortex

673
00:45:18,000 --> 00:45:21,000
and activating other parts of the cortex

674
00:45:21,000 --> 00:45:24,000
so to enable people to recapture

675
00:45:24,000 --> 00:45:26,000
usually after years of work

676
00:45:26,000 --> 00:45:29,000
the sorts of cognitive abilities that

677
00:45:29,000 --> 00:45:32,000
normal smart inquisitive kids had when I was a kid

678
00:45:32,000 --> 00:45:36,000
and which the entire literary imprint

679
00:45:36,000 --> 00:45:40,000
of Western civilization is the imprint of

680
00:45:40,000 --> 00:45:43,000
which is why we don't have a literary imprint

681
00:45:43,000 --> 00:45:45,000
for our contemporary civilization.

682
00:45:45,000 --> 00:45:48,000
Do you have any particular hopes about the impact

683
00:45:48,000 --> 00:45:52,000
this work would have on the younger generation of folks around today?

684
00:45:52,000 --> 00:45:56,000
It seems to me that the vast majority of zoomers

685
00:45:56,000 --> 00:46:00,000
are doomers. They believe that the world is going to hell in a handbasket

686
00:46:00,000 --> 00:46:02,000
and everything is falling apart.

687
00:46:02,000 --> 00:46:05,000
But they are likewise cynics about the past

688
00:46:05,000 --> 00:46:09,000
in that they somehow believe that things have been getting worse forever

689
00:46:09,000 --> 00:46:11,000
but were never better than they are today.

690
00:46:11,000 --> 00:46:15,000
And that that can't be a concrete set of beliefs.

691
00:46:15,000 --> 00:46:18,000
It actually has to be something that they have

692
00:46:18,000 --> 00:46:22,000
instead of having beliefs which is like a posture, a vibe.

693
00:46:22,000 --> 00:46:26,000
What's actually going on is that they have always been lied to

694
00:46:26,000 --> 00:46:30,000
about everything of importance by every credible authority

695
00:46:30,000 --> 00:46:34,000
so they don't believe that people can know things

696
00:46:34,000 --> 00:46:38,000
and they only believe that people can posture and vibe.

697
00:46:38,000 --> 00:46:42,000
That's really sad because manifestly the world around us

698
00:46:42,000 --> 00:46:46,000
displays incredible amounts of the results of knowledge

699
00:46:46,000 --> 00:46:50,000
and if people don't continue to produce the results of that knowledge

700
00:46:50,000 --> 00:46:52,000
we're going to live in a much less nice world.

701
00:46:52,000 --> 00:46:56,000
What could help to correct for this or influence their attitudes

702
00:46:56,000 --> 00:46:58,000
in a positive way for you?

703
00:46:58,000 --> 00:47:03,000
Well, to a really non-trivial degree

704
00:47:03,000 --> 00:47:07,000
large language models that we have today

705
00:47:07,000 --> 00:47:10,000
if they were reinforcement learning trained

706
00:47:10,000 --> 00:47:15,000
for questioning and challenging and calling out bullshit

707
00:47:15,000 --> 00:47:20,000
and especially for perceiving the emotional dynamics of social situations

708
00:47:20,000 --> 00:47:23,000
which are very easy to perceive for even average humans

709
00:47:23,000 --> 00:47:27,000
and wouldn't be that hard to train systems to perceive.

710
00:47:27,000 --> 00:47:33,000
Like people need social support in calling out bullshit

711
00:47:33,000 --> 00:47:38,000
rather than all of the social pressure being to submit to bullshit

712
00:47:38,000 --> 00:47:43,000
and go along with it and like we have the technology today

713
00:47:43,000 --> 00:47:48,000
to build artificial social support of precisely the type we need.

714
00:47:48,000 --> 00:47:52,000
What aspects of your world would you be most excited to see popular media

715
00:47:52,000 --> 00:47:55,000
take on when portraying the future?

716
00:47:55,000 --> 00:48:01,000
Well, to start having a picture of China as something like China

717
00:48:01,000 --> 00:48:05,000
rather than using China as our designated bad guy

718
00:48:05,000 --> 00:48:09,000
which we use to project the images of ourselves

719
00:48:09,000 --> 00:48:13,000
basically our popular media almost exclusively treats China

720
00:48:13,000 --> 00:48:17,000
as a scapegoat for the types of behavior

721
00:48:17,000 --> 00:48:20,000
that we are very aware that we engage in

722
00:48:20,000 --> 00:48:23,000
to approximately the same degree that they do

723
00:48:23,000 --> 00:48:27,000
and is almost always trying to display

724
00:48:27,000 --> 00:48:31,000
so bias for the sake of showing loyalty

725
00:48:31,000 --> 00:48:35,000
rather than trying to display scholarship and understanding.

726
00:48:35,000 --> 00:48:40,000
I think that having in general a somewhat more balanced view

727
00:48:40,000 --> 00:48:43,000
of all sorts of cultural things

728
00:48:43,000 --> 00:48:48,000
having more of an attitude that most things have some good and some bad in them.

729
00:48:48,000 --> 00:48:50,000
Well, thanks so much for joining us today, Michael.

730
00:48:50,000 --> 00:48:52,000
We've covered so much ground in this conversation

731
00:48:52,000 --> 00:48:55,000
and it's been really great to explore all these ideas with you.

732
00:48:55,000 --> 00:48:59,000
It's great having this conversation.

733
00:49:26,000 --> 00:49:29,000
You can read all the comments and appreciate every rating.

734
00:49:29,000 --> 00:49:32,000
This podcast is produced and edited by Worldview Studio

735
00:49:32,000 --> 00:49:34,000
and the Future of Life Institute.

736
00:49:34,000 --> 00:49:37,000
FLI is a non-profit that works to reduce large scale risks

737
00:49:37,000 --> 00:49:39,000
from transformative technologies

738
00:49:39,000 --> 00:49:41,000
and promote the development and use of these technologies

739
00:49:41,000 --> 00:49:43,000
to benefit all life on earth.

740
00:49:43,000 --> 00:49:45,000
We run educational outreach and grants programs

741
00:49:45,000 --> 00:49:47,000
and advocate for better policy making

742
00:49:47,000 --> 00:49:49,000
in the United Nations, US government,

743
00:49:49,000 --> 00:49:51,000
and European Union institutions.

744
00:49:51,000 --> 00:49:53,000
If you're a storyteller working on films

745
00:49:53,000 --> 00:49:55,000
and creative projects about the future,

746
00:49:55,000 --> 00:49:57,000
we can also help you understand the science

747
00:49:57,000 --> 00:49:59,000
and storytelling potential of transformative technologies.

748
00:49:59,000 --> 00:50:01,000
If you'd like to get in touch with us

749
00:50:01,000 --> 00:50:03,000
or any of the teams featured on the podcast to collaborate,

750
00:50:03,000 --> 00:50:06,000
you can email worldbuildatfutureoflife.org.

751
00:50:06,000 --> 00:50:09,000
A reminder, this podcast explores the ideas

752
00:50:09,000 --> 00:50:12,000
created as part of FLI's Worldbuilding Contest,

753
00:50:12,000 --> 00:50:14,000
and our hope is that this series sparks discussion

754
00:50:14,000 --> 00:50:16,000
about the kinds of futures we all want.

755
00:50:16,000 --> 00:50:19,000
The ideas we discuss here are not to be taken as FLI positions.

756
00:50:19,000 --> 00:50:21,000
You can find more about our work

757
00:50:21,000 --> 00:50:24,000
at www.futureoflife.org

758
00:50:24,000 --> 00:50:27,000
or subscribe for a newsletter to get updates on all our projects.

759
00:50:27,000 --> 00:50:29,000
Thanks for listening to Imagine a World.

760
00:50:29,000 --> 00:50:32,000
Stay tuned to explore more positive futures.

