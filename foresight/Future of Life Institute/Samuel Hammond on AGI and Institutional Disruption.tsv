start	end	text
0	3360	Welcome to the Future of Life Institute podcast.
3360	4520	My name is Gus Docher,
4520	6480	and I'm here with Samuel Hammond,
6480	8760	who is a senior economist at
8760	11160	the Foundation for American Innovation.
11160	13040	Samuel, welcome to the podcast.
13040	14400	I guess. Thanks for having me.
14400	16000	Fantastic. All right.
16000	18280	I have so much I want to talk to you about,
18280	21240	but I think a natural place to start here would be with
21240	24080	your timelines to AGI.
24080	29280	Why is it that you expect AGI to get here before most people?
29280	31120	Well, I don't really know what most people think.
31120	34720	I think the world divides into people who are paying
34720	38560	attention and people who are basically normies.
38560	42480	In my day job, I work on Capitol Hill and in Washington,
42480	44960	D.C., talking to folks about AGI.
44960	48400	If you think about what people's implicit timelines are,
48400	52040	you can read out people's implicit timelines by their behavior.
52040	55200	I know Paul Cristiano has short timelines
55200	58040	because he's doubled up into the stock market.
58440	61160	He's practicing what he preaches.
61160	64960	But then when you have Sam Altman testifying to work to Congress,
64960	67960	I like to say people are taking him seriously,
67960	69800	but not literally. He's saying,
69800	71660	we're going to develop something like AGI,
71660	72880	potentially this decade,
72880	75440	and superintelligence thereafter.
75440	78120	Then you have folks like Sandra Marshall Blackburn
78120	80920	being like, what will this mean for music royalties?
81920	86000	When the focus of policymakers is things like
86040	89520	music royalties or impact on copyright,
89520	91680	it's not that those are invalid issues.
91680	95440	It's that they belie relatively longer timelines.
95440	97520	Then we also have this definitional confusion
97520	102040	where folks like John Lacoon would say AGI is probably decades away
102040	105440	because he is using AGI to mean something that learns,
105440	108280	like a human learns in the sense that it's born
108280	111400	as a relative blank slate and can acquire language
111400	114320	with very few examples.
114320	116680	People have these moving goalposts of what they mean.
116680	121680	For me, I think we can avoid those definitional conflicts
121760	124680	if we just talk about human level intelligence,
124680	128480	and humans are quite general or generally intelligent.
128480	132920	That's what separates us from animals in a lot of respects.
132920	135880	And when you look at how machine learning models
135880	137280	are being trained today,
137280	140280	like large language models and now multimodal models,
140280	142000	they're being trained on human data,
142000	147680	and they're being trained to reproduce the kinds of behaviors
147680	150880	and tasks and outputs that humans output.
150880	153000	And so they're kind of like an indirect way
153000	155080	of emulating human intelligence.
155080	159760	And then so if you benchmark AI progress to that,
159760	162320	then you can sort of put information theoretic bounds
162320	165480	on what's the likely timeline
165480	168360	to basically an ideal human emulator,
168360	172640	something that can extract the sort of base representations,
172640	175000	the internal representations of our brain
175000	179160	through the indirect path of the data that our brain generates.
179160	181600	Yeah, you have an interesting sentence where you write that
181600	184240	AI can advance by emulating the generator
184240	187680	of human-generated data, which is simply the brain.
187680	191280	Do you think this paradigm holds all the way to AGI?
191280	194120	I think it holds this decade to systems
194120	198440	that in principle can in context learn anything humans do.
198440	199880	Again, this is a semantic question.
199880	201640	Do you want to call that AGI or not?
201640	205960	I think there are still outstanding issues around the limits
205960	208840	of other aggressive models for autonomy
208840	212720	and the question of sort of real-time learning,
212720	214280	the way we train these models,
214280	216280	we sort of are freezing a crystal in place
216280	220120	and humans are continuously learning.
220120	225720	So there still are genuine potential architectural gaps,
225720	228840	but from the practical point of view,
228840	231080	from the economic point of view,
231080	233200	we don't need to debate whether something is conscious
233200	236640	or whether something learns strictly the way humans learn
236640	240160	if it demonstrably can do the things humans do, right?
240160	245240	And that goes to the original insight of the Turing test, right?
245240	246840	It's sometimes presented as a thought experiment,
246840	248320	but what Alan Turing was getting at
249040	252680	was if you can't distinguish between the human and a computer,
252680	257200	in some ways, indistinguishability implies competence, right?
257200	259480	And we can broaden that from just language
259480	262200	because arguably we've surpassed the Turing test,
262200	264000	at least a weaker version of it,
264000	267560	to human performance on tasks in general, right?
267560	271720	If we have a system that can output a scientific manuscript
271720	274560	that experts in the field can't distinguish from a human,
274560	280320	then debating whether this is real AGI or not
280320	283520	is, I feel, academic.
283520	287480	It is surprising in a sense that when you interact with GPT-4,
287480	290880	for example, and it can do all kinds of amazing things
290880	294120	and organize information, present information to you,
294120	296560	but then it can't, or at least at some point,
296560	298680	it couldn't answer questions about the world
298680	302920	after September 2021 or a date like that.
302920	306080	That would be surprising if you presented that fact
306080	309120	to an AI scientist 20 years ago.
309120	311400	For how long do you think we'll remain in this paradigm
311400	313880	of training a foundational model
313880	316080	and then deploying that model?
316080	316920	I mean, it's worse than that.
316920	319040	I think it was surprised people five years ago.
319040	320520	Progress is sort of moving along two tracks.
320520	321440	There's the industry track
321440	324360	and the peer research academic track,
324360	327400	and they're obviously having feedback with one another.
327400	331080	The peer industry track is just looking to create tools
331080	333480	that have practical value
333480	335320	and can improve products and so forth.
335320	338840	And so, Meta has their own GPU cluster
338840	340080	and their training models,
340080	343920	so they can have fun chatbots in their messenger.
343920	347560	And so, those kinds of things are going to progress,
347560	350800	I think, well within the current paradigm
350800	353720	because we know the paradigm works,
353720	357520	basically deep learning and transformers.
357520	359640	And there's lots of marginality on the side,
359640	363560	but that basic framework seems to be quite effective.
363560	364520	And just scaling that up
364520	367240	because we haven't sort of hit the range
367240	370160	of irreducible loss and what transformers can do.
370160	373040	Meanwhile, there's also this parallel peer research track
373040	375560	where people on seemingly a weekly basis
375560	379640	are finding better ways of specifying the loss function,
379640	382640	ways of improving upon power loss scaling
382640	384440	and all these different,
385600	386720	sometimes they're new architectures,
386720	388920	but often they're just like bags of tricks.
389920	392160	And those bags of tricks then,
392160	395160	to the extent that they comport with
395160	398520	the paradigm industries running with,
398520	399880	they can be reintegrated
399880	402760	and end up accelerating progress and industry as well.
403880	407760	So, do you think scale of compute is the main barrier
407760	410080	to getting to human level AI?
411000	414320	Yes, right, I mean, it's not all we need,
414320	417600	but it's the main unlock, right?
417600	421720	To what extent can more compute be used to trade off
421720	426720	for lower quality data or for lower quality algorithms?
427360	428800	Can you just throw more compute
428800	432760	and solve the other factors in that equation?
432760	434880	It depends on the thing you're trying to solve for.
434880	439120	In principle, if we're talking about mapping inputs to outputs,
439120	441200	then transformers are known
441200	443200	to be universal function approximators.
443200	446800	And so the answer is yes.
446800	448840	That doesn't mean that they're necessarily efficient
448840	451520	at approximating everything we want them to approximate.
451520	455160	And sometimes universal function approximation theorems
455160	457360	can be kind of trivial because they'll be like, okay,
457360	459040	if your neural network is infinite width,
459040	461440	then yes, we can approximate everything.
461440	465680	The key fact is both that they're universal approximators
465680	468520	and also that they're relatively sample efficient,
468520	470720	at least relative to things we found in the past.
470720	471960	And so that to me suggests that yes,
471960	475760	they can compensate for things that they're bad at.
475760	477960	On the other hand, the way research is trending
477960	479640	is towards these mixed models,
480680	483000	ensembles of different kinds of architectures,
483000	484960	things like the recent Q transformer
484960	486680	announced from Google DeepMind,
486680	489640	which just sort of uses a combination of transformers
489640	493440	and Q learning to have sort of the associational memory
493440	496640	and sample efficiency of transformers
496640	499840	with the ability to assign policies to do tasks
499840	501640	that you get from reinforcement learning.
501640	503480	So I imagine that there's going to be all kinds
503480	505140	of mixing and matching.
505580	509580	The key point is that in that space of architectures,
509580	513220	it's a relatively sort of finite search space, right?
513220	515940	And as an economist, economists believe
515940	519060	that supply is long run elastic, right?
519060	522100	And so there's this famous bet from the team,
522100	525500	Paul Ehrlich and Julian Simon vis-a-vis the population bomb
525500	527340	and whether population growth would lead
527340	531340	to sort of a mothusian purge.
531340	533860	And Julian Simon being the economist
533860	537580	recognized that if prices rise for these core commodities,
537580	540180	then that will spur research and development
540180	542380	into extracting new resources, right?
542380	544060	So he didn't have to know
544060	546340	that fracking would be a technology.
546340	549580	He understood that if oil prices went too high,
549580	552300	people would find new oil reserves.
552300	554900	And I think there's, I have an analogous instinct
554900	558060	when it comes to progress and deep learning,
558060	560060	meaning you can become too anchored
560060	563420	to sort of the current state of the literature,
563420	566900	but over a tenure horizon, you can say,
566900	569900	well, there's a huge search on a huge gold rush
569900	574580	to find the right way of blending these architectures.
574580	577020	And I don't need to know in advance,
577020	578940	which is the right way to do that
578940	581940	to have high confidence that someone will find it.
581940	585180	Yeah, we can sometimes, if we're too deep in the literature,
585180	589060	we might lose the focus on the forest for the trees
589060	589900	in a sense.
589900	592100	And if we zoom out, we can just see
592100	595140	that there's more investment, there's more talent
595140	597060	pouring into AI, and so we can predict
597060	599460	that something is gonna come out of that.
599460	601220	You have lots of interesting insights
601220	602460	about information theory
602460	605980	and how this can help us predict AI.
605980	608900	What's the most important lessons from information theory?
608900	613460	The reason I start there is because sort of,
613460	614980	within the conceptual realm,
614980	617540	it's sort of like the most general thing
617540	620020	that bounds everything else.
620020	622460	And when you look back at the record of, say,
622460	626460	Ray Kurzweil, I first read The Age of Spiritual Machines
626460	629100	when I was a kid, and in there,
629100	630700	he makes a prediction that we'll have
630700	634580	AIs that pass the Turing test by 2029 or so.
634580	636820	And when was this book written?
636820	638020	1999.
638020	639500	Yeah, that's pretty good.
639500	642340	Right, and so, and people will complain
642340	643180	that he got things wrong,
643180	647220	because he said, well, I'll be wearing AR glasses by 2019,
648140	651300	when in fact, Google Glass came out in 2013,
651300	654620	and now we have Meta Glasses five years later.
654620	657100	So he was wrong on the exact timing,
657100	659020	but sort of right where the technology was wrong,
659020	662060	where the minimal viable product was.
662060	665220	But nonetheless, if you look at his track record,
665220	668740	it's quite good for a methodology
668740	673100	as relatively stupid as just staring at Moore's Law,
673100	675220	and extrapolating it out.
675220	677860	And I think that reveals the power
677860	681580	of these information theoretic methodologies to forecasting
681580	684260	because they set bounds on what will be possible.
684260	687220	The team at epoch.ai have a forecast
687220	690020	called the direct approach where it's sort of,
690020	693220	you can think of it sort of like a way of putting bounds
693220	697380	on when we'll have AIs that can emulate human performance
697380	699580	through an information theoretic lens
699580	701940	where they're looking at sort of how much entropy
701940	703420	does the brain sort of process
703460	706340	and how much compute will we have over time
706340	709220	and what's implied by AI scaling laws.
709220	711860	And you sort of put those three things together
711860	713220	and you can sort of set bounds
713220	716180	on when we'll basically be able to brute force
716180	718660	human level intelligence.
718660	720020	And of course, that's an upper bound
720020	721540	because we're going to do better than brute force.
721540	723100	We're going to also have insights
723100	725740	from cognitive science and neuroscience
725740	730740	and also ways of distilling neural networks and so forth
730740	732300	and better ways of curating data.
732300	736060	So their modal estimate for human level AI is 2029
736060	737740	and their meeting is like 2036.
737740	740620	And I've talked to the authors and they lean towards
740620	744940	the 2029, 2030 for their own personal forecasts.
744940	748940	And so going back to, is this a net liar?
748940	750820	Am I out on a limb here?
750820	752980	I think among our circles probably not,
752980	756220	but among Congress and among the broader public,
756220	758060	I think people are seeing sort of,
758060	760660	they think everything's an asymptote, right?
761180	763020	They're imagining, okay, we have these chatbots
763020	765180	and they're not seeing the next step.
765180	767940	I see a very smooth path from here to systems
767940	770540	that can basically in context learn
770540	772460	any arbitrary human task.
772460	773780	And so what does that look like?
773780	777020	It looks like systems that can basically sit over your shoulder
777020	779900	or can monitor your desktop, your operating system as you work
779900	783220	and watch you for an hour or two and then take over.
783220	786500	And that'll be key to overcoming lack of training data
786500	790540	or why is it important that they can learn in context?
790740	794580	Well, in context learning is sort of the secret source
794580	796580	of the power of transformer models.
796580	799180	They learn these inductive biases and induction heads
799180	801140	and so forth that let them,
801140	802660	a few shot learn different tasks.
802660	805660	So, GPT-4 is very good at zero shot learning
805660	807140	on a variety of different things,
807140	809580	but it's incredibly good at few shot learning.
809580	810740	If you give it a few examples,
810740	812820	it can kind of pick up where you left off.
812820	813860	You know, when I think about myself,
813860	816040	when I wanna learn a new recipe, right?
816040	817140	I can go read a recipe book,
817140	819500	but often what I prefer to do is to go on YouTube
819500	821980	and watch someone make that recipe, right?
821980	825820	And just by watching that person put together the stir fry,
825820	827540	I have enough of a world model
827540	830120	and enough of knowledge of how to cook in general
830120	833700	that I can sort of in context learn
833700	836380	how to pick up from there and do that recipe myself.
836380	837460	LLMs do that already.
837460	840140	Multimodal models are increasingly doing that.
840140	842980	Some of the recent progress in robotics,
843800	845920	like I mentioned, the Q-transformer paper,
845920	848100	it shows that you can basically build robots
848140	849660	as a basic world model
849660	851340	and then have it learn new tasks
851340	854380	with fewer than 100 examples of the human demonstration.
854380	855860	So the human sort of demonstrates the task
855860	859340	and the robot can pick it up and take it from there.
859340	861700	And why that's important is both
861700	864700	for understanding the trajectory of AI,
864700	868420	but also its economic implementation
868420	871140	because we're sort of used to automation
871140	873940	being this thing where you get a contract from IBM
873940	877460	and you spend many millions of dollars with consultants
877460	878740	and they build you some bespoke thing
878740	880500	that doesn't really work very well
880500	881940	and requires lots of maintenance.
881940	885220	And so people have this sort of prior that AI,
885220	888940	even if it's near, will be rate limited by the real world
888940	891860	because of all the complexity of implementation.
891860	894780	But the point is if you have things that can in context learn
894780	897620	and perform sort of as humans perform,
897620	899980	then you don't need to change the process.
899980	902980	You can take human designed processes
902980	906220	and have the AI just fill in for the human.
906260	907420	And so it leads to this paradox
907420	908940	where we're probably going to have AGI
908940	911220	before we get rid of the last fax machine.
911220	916220	Yeah, when we think of say old IT systems
916940	920700	in large institutions, we might think of moving
920700	925700	from analog storage of information to the cloud.
925900	928340	That's still going on in some institutions.
928340	931340	That transformation has taken over a decade now.
931340	934660	And so what exactly is it that makes AI different here?
934660	937060	It is that AI plugs in directly
937060	938700	where the human worker would be?
938700	940420	Yeah, precisely.
940420	944180	You don't need to redesign existing process
944180	947340	to sort of plug into the automation.
947340	950860	And that applies both for sort of the structure of tasks.
950860	953740	So much of a mechanical automation
953740	957260	takes something like the sort of artisanal work
957260	961140	of a shoemaker and has to translate it
961140	963540	into something repetitive that a machine
963540	967300	or an automatic seamstress can do over and over and over
967300	969460	or against our older school kind of automation
969460	972980	requires sort of collapsing a task into a lower dimension
972980	976660	so that simple automations can handle it.
976660	980100	But when you have AGI, the whole point is generality.
980100	983140	It's a flexible intelligence
983140	986780	that can map to existing kinds of processes.
986780	989380	So that's sort of why I think this will catch people
989380	992220	by surprise because it's not just that AGI
992220	995180	could be this decade, but that when it arrives
995180	997660	and sort of crosses some thresholds of reliability,
997660	999980	the implementation frictions could be very low.
999980	1004700	And do you expect, would AI have to get all the way there
1004700	1007300	in order to substitute for a human worker?
1007300	1010020	I mean, I would expect it to be a bit more gradual
1010020	1012620	than that, taking over say 20% of tasks
1012620	1015420	before 40% of tasks, 60% of tasks and so on.
1015420	1018940	But here we're imagining that the AI kind of plugs in
1018940	1020820	for the human worker for all tasks
1020820	1022620	or what do you have in mind?
1022620	1024740	These things are, yeah, you're right, much more continuous.
1024740	1027380	It's not an on or off switch in part
1027380	1031300	because the requisite threshold of reliability
1031300	1033780	varies by the type of task.
1033780	1037220	Arguably self-driving cars like Waymo or Tesla
1037220	1039540	have matched human performance,
1039540	1041940	but regulators want them to be 100x better than human
1041940	1046780	before they're loose on the road because of safety.
1046780	1051780	Codex and coding models are arguably still much worse today
1051780	1055460	than elite programmers, but everyone is using them
1055460	1058660	because even if it generates 50% of your code
1058660	1059900	and you have to go back in and debug,
1059900	1062580	it's still a huge productivity boost.
1062580	1065580	So I think it will vary by occupation,
1065580	1068780	by sort of task category, sort of modulo,
1068780	1072780	the risks and stakes involved in those tasks.
1072780	1073980	Yeah, I guess then the question is,
1073980	1076540	how many of our jobs fall into the,
1076540	1078060	is more like self-driving cars
1078060	1081540	and how many of our jobs is more like programming?
1081540	1085220	Right, I mean, I've been in a manager position before
1085220	1087660	and I've had research assistants and interns
1087660	1090700	and I know that they're like a very lossy compression
1090700	1093340	of the thing I want to do.
1093340	1096580	And so they require oversight and sort of co-poniting.
1096580	1098220	We're sort of in that stage now with AIs
1098220	1100900	in a variety of different kinds of tasks.
1101100	1105020	I recently read a paper evaluating the use of GPT-4
1105020	1107020	for peer review and science
1107020	1112020	and it found that GPT-4 would write reviews of work
1112780	1114660	that bore some striking correlations
1114660	1118780	with the points raised by human reviewers,
1118780	1119940	but also let some things out.
1119940	1123420	And so it concluded by saying GPT-4
1123420	1127500	could be an invaluable tool for scientific review,
1127500	1130220	but it's not about to replace people.
1130220	1134320	And that's just a case of like, okay, give it five years.
1135460	1137940	Yeah, this is a phenomena you often see
1137940	1141140	with some AI models out there
1141140	1144700	and it has some capabilities, but lacks other capabilities.
1144700	1147900	And then people might kind of over anchor
1147900	1149900	on the present capabilities
1149900	1153180	and not foresee the way the development is going.
1153180	1155700	I think people are continually surprised
1155700	1157860	at the advancement of AI.
1157860	1158940	Yeah, absolutely.
1158940	1163260	Ramiz Naam, the sci-fi author and futurist
1163260	1167380	and energy investor, he gives his talk on solar energy
1167380	1170780	and other renewables and he has this famous graph
1170780	1175140	where he shows the International Energy Agency, the IEA.
1175140	1179420	Every year they put out this projection of solar buildout
1179420	1181700	and every year it's like a flat line,
1181700	1183260	but it's like a flat line on an exponential,
1183260	1185060	like the real curve is like going vertical
1185060	1186660	and every year their projection is that it's just going
1186660	1190780	flat-toe and I feel like people make that same mistake.
1190780	1194500	And it sort of has this sort of ironic lesson,
1194500	1197940	to the extent that we're drawing sort of parallels
1197940	1200500	with the way our brain works and the way these models work,
1200500	1202020	it seems like humans have a very strong
1202020	1203420	autoregressive bias.
1203420	1204460	So what's going on there?
1204460	1206660	Is it an institutional problem
1206660	1209420	or is it a psychological problem?
1209420	1214020	Why is it that we can't project correctly in many cases?
1214860	1217860	Well, to what I just said, I think it's probably both,
1217860	1219580	but largely psychological, right?
1219580	1221340	Our brains are evolved for, you know,
1221340	1223580	hunter-gatherer societies that didn't really change
1223580	1228580	over millennia and, you know, even the last 40, 50 years
1229180	1230700	have been a period of relative stagnation
1230700	1233060	where we have a lot of sort of pseudo-innovation.
1233060	1237780	And so I think people are just a bit sort of disabused.
1237780	1240140	Okay, you have some super interesting points
1240180	1243660	about comparing the human brain,
1243660	1247580	how the human brain works to how neural networks learn.
1247580	1251820	What is universality in the context of brain learning
1251820	1253860	and neural network learning?
1253860	1258220	So universality is a term of our sort of refers to
1258220	1260020	the fact that different neural networks
1260020	1263500	independently trained, you know, even on different data
1263500	1266940	will often converge on very similar representations
1266940	1269580	in their embedding space of that data.
1269580	1273300	And you can extend that to striking parallels
1273300	1275220	or isomorphisms between the representations
1275220	1277420	that artificial neural networks learn
1277420	1279180	and that our brain appears to learn.
1279180	1281500	Probably the area of the brain that's been studied the most
1281500	1283180	is the visual cortex.
1283180	1287420	And it seems to me as like a layperson
1287420	1290260	that the broad consensus in neuroscience
1290260	1293180	is that the visual cortex is very similar
1293180	1295100	to a deep convolutional neural network.
1295100	1299020	It's basically isomorphic to our artificial
1299020	1300940	deep convolutional neural networks.
1300940	1305020	And you train CCN on image data
1305020	1309760	and our brain is trained on our sensory data.
1309760	1311540	And it turns out they end up learning
1311540	1314500	strikingly similar representations.
1314500	1315920	And there are a few reasons for that, right?
1315920	1320360	So, you know, one is sort of hierarchies of abstraction.
1320360	1323500	It makes sense that early layers in a neural network
1323500	1326220	will learn things like edges and simple shapes
1326220	1328660	and only later in the only deeper in the network
1328700	1330780	will you learn more subtle features.
1330780	1333860	So there's that sort of sequencing part of it.
1333860	1335620	And then there's also just the energy constraint.
1335620	1338100	You know, gradient descent isn't costless, right?
1338100	1340100	It requires energy, it requires a lot of energy.
1340100	1342940	That's, you know, these data centers suck up a lot of energy.
1342940	1344380	The same is true of our brain.
1344380	1347580	You know, our brain consumes a lot of energy,
1347580	1349380	like 25% of our calories.
1349380	1352300	And especially when it's, and when we're young,
1352300	1354460	there's a very strong metabolic cost
1354460	1356280	associated with neural plasticity.
1356280	1357900	Our brain being something shaped by evolution
1357940	1359420	was obviously very energy conscious.
1359420	1363580	And so those energy constraints greatly shrink the landscape
1363580	1367900	of possible representations from sort of this infinite
1367900	1371260	landscape of all the ways you could represent certain data
1371260	1374140	to a much more manageable set of representations.
1374140	1376820	And that doesn't guarantee that we'll converge
1376820	1378980	on the same representations.
1378980	1382140	At least suggestive of a weak universality
1382140	1384900	where even when we don't have the exact same representations,
1384900	1386900	they're often a coordinate transformation
1386900	1388540	that play from each other.
1388540	1391940	It's actually a bit surprising, so as you mentioned,
1391940	1393220	when we train neural networks,
1393220	1395300	we don't have the same energy constraints
1395300	1398460	as the brain had during our evolution.
1398460	1401380	And I would expect, again, from evolution,
1401380	1405540	that the human brains have many more inbuilt biases
1405540	1407300	and heuristics.
1407300	1409580	But if we then compare the representations
1409580	1412300	in a neural network to those in a human brain,
1412300	1413980	we found that they are quite similar.
1413980	1416580	Isn't that the whole point of universality?
1416580	1420660	So does the neural network have the same heuristics
1420660	1424020	and biases that we have, or what's going on here?
1424020	1428700	Well, one of the primary biases in stochastic gradient
1428700	1432620	descent is sometimes called a simplicity preference,
1432620	1435980	basically an inductive bias for more parsimonious
1435980	1437380	representations.
1437380	1440900	Parsimonious in the sense of Occam's razor.
1440900	1444660	And that's a byproduct of this information
1444660	1448020	heuristic concept of Kolmogorff complexity,
1448020	1450140	where Kolmogorff complexity means
1450140	1453500	is measured by, is there a short program that
1453500	1456140	can reproduce this longer sequence?
1456140	1458060	And if you can find a short program that's
1458060	1460460	sort of a more compact or more compressed way of representing
1460460	1462340	it, and when you're under energy constraints,
1462340	1466300	you're looking for those more compressed representations.
1466300	1468500	And so that simplicity bias seems
1468500	1472260	to be also the origin of generalization,
1472340	1476900	of our ability to go beyond merely memorizing data,
1476900	1480340	overfitting our parameters, to finding a simpler way of
1480340	1482100	representing those parameters, right?
1482100	1485860	Where we go from sort of fitting a bunch of data points
1485860	1487820	to recognizing, oh, these data points are being generated
1487820	1490700	by a sine function, so I can replace all these data points
1490700	1492900	by a simple circuit for that sine function
1492900	1493860	or something like that.
1493860	1495980	What can we learn about AI progress
1495980	1499460	when we consider the hard steps that humans
1499500	1503660	and our ancestors have gone through in evolution?
1503660	1504940	It's beyond evolution.
1506460	1507700	This is often comes up in the discussion
1507700	1509580	of the Fermi Paradox.
1509580	1512500	Life on Earth to exist at all, let alone intelligent life,
1512500	1514460	had to pass through many hard steps, right?
1514460	1517340	We had to have a planet in a habitable zone.
1518220	1522420	We had to have, you know, the right mix
1522420	1527420	of organic chemicals in the Earth's crust and so forth.
1527580	1530900	We had to have the conditions for abiogenesis,
1530900	1533260	the emergence of the very earliest sort of
1533260	1535420	non-living replicators, probably, you know,
1535420	1538140	some kind of polymer type of crystal structure.
1538140	1539380	Then we had to have, you know,
1539380	1543940	the transition from single cell to multicellular organisms,
1543940	1546300	to transition through the Cambrian explosion, right?
1546300	1547980	Every one of these steps,
1547980	1550980	you could think of as a very unlikely improbable thing.
1550980	1552540	All the way up to, you know,
1552540	1555260	the development of warm-blooded mammals
1555300	1559820	and sort of social animals that were heavily selected for,
1559820	1564820	for brain size, to then the sociocultural hard steps
1564940	1568380	of like moving from small group primates
1568380	1573380	to sort of settled technological cultures.
1574060	1575900	Then, you know, technological hard steps,
1575900	1577100	like the discovery of the printing press
1577100	1579460	or the discovery of the transistor.
1579460	1581460	You put those all together and life seems
1581460	1584060	just incredibly unlikely.
1584060	1587060	And, you know, this often goes to the point of view
1587060	1589500	that, you know, creationists or intelligent designers
1589500	1591220	would put forward.
1591220	1592660	But then you zoom out and then you recognize,
1592660	1596860	oh, wait, there are, you know, trillions of galaxies
1596860	1598740	each with trillions, you know, hundreds of billions
1598740	1602220	of stars and hundreds of trillions of planets.
1602220	1607220	There's an awful lot of potential variation out there.
1607420	1610820	And then meanwhile, every one of these hard steps
1610860	1615460	seems characterized by a search problem that is very hard.
1615460	1618140	But then once you find the correct thing,
1618140	1621020	like the earliest self-replicator,
1621020	1623180	things kind of take off, right?
1623180	1626660	So you imagine that before the earliest self-replicator,
1626660	1629820	there were millions or billions of attempts
1629820	1633420	to self-replicate, like that didn't succeed.
1633420	1636300	Yeah, it's just a huge search problem, right?
1636300	1639060	And, you know, maybe there are more gradual intermediate
1639060	1642980	stages where you have sort of, you know,
1642980	1645260	everything in biology ends up looking way more gradual
1645260	1646420	more you learn about it.
1646420	1649620	But there are these phase transitions where you tip over
1649620	1650980	and you get the Cambrian explosion
1650980	1653380	or you get the printing press and the printing revolution.
1653380	1656900	And so those hard steps end up looking relatively,
1656900	1659060	they look more easy in retrospect
1659060	1660380	because even though the search was hard,
1660380	1663620	once you've tripped over the correct solution,
1663620	1666660	there's sort of an autocatalytic self-reinforcing loop
1666660	1669220	that pulls you into a new regime.
1669220	1673260	And indeed, when you look at the emergence of life on Earth
1673260	1676300	relative to the age of the universe,
1677220	1681140	and Avi Lo with some co-authors have done this,
1681140	1683460	life on Earth is incredibly early.
1683460	1686820	Like, you know, the universe is 13.7 billion years old,
1686820	1691220	but life couldn't emerge really much sooner.
1691220	1694980	The reason being the universe started out as hot and dense,
1694980	1697500	it had to cool down, stars had to form,
1697500	1699180	those stars had to supernovae
1699180	1701260	so they could produce the heavy elements
1701260	1703740	that are essential to life.
1703740	1706660	And then those solar systems had to then take shape
1706660	1708780	and then had to further cool
1708780	1713340	so the solar system wasn't being irradiated constantly.
1713340	1715460	And when you put all those factors together,
1715460	1719420	human life emerged basically as soon as it was possible
1719420	1720900	for life to emerge anywhere.
1720900	1722820	And so this is one way to answer the Fermi paradox
1722860	1725380	that we're just in the first cohort, right?
1725380	1727220	But it also should give you strong priors
1727220	1728740	that passing through those hard steps
1728740	1730700	isn't as hard as it looks.
1730700	1733100	And what's the lesson for AI here?
1733100	1735660	Developing AGI is sort of a hard step.
1737300	1740300	We're doing this kind of gradient search
1740300	1744260	for the right algorithms, for the right, what have you.
1744260	1746740	And we seem to be now in a slow takeoff
1746740	1749820	where we've figured out the core ingredients
1749820	1752260	and there's now an autocatalytic process
1752300	1753980	that's pulling us into a new phase.
1753980	1756020	And what do you mean by autocatalytic?
1756020	1759540	Suffering, forcing, once it gets started,
1759540	1764540	it pulls itself, it sort of has an as if teleology, right?
1764820	1766020	You see this in nature,
1766020	1768860	but you also see this in capitalism.
1769780	1772580	And you would expect us to get to advanced AI
1772580	1776420	basically as soon as it's computationally possible.
1777300	1779180	It basically seemed that way, right?
1779180	1781380	Like, there was a kind of tacit collusion
1781420	1784620	between Google and other players in the space
1784620	1788300	to they had transformer models since 2017,
1788300	1790140	but really, there's some of the precursors
1790140	1792260	to transformers go back to the early 90s.
1792260	1795140	But once you have this sort of profit opportunity
1795140	1797420	that's in the background,
1797420	1799500	it's hard in the competitive environment
1799500	1802620	to stop an open AI from being like,
1802620	1805540	oh, let's chase those profits.
1805540	1807300	And then once that ball gets rolling,
1807300	1809140	it's basically impossible to stop.
1809620	1814020	This is why whatever the merits of the pause letter,
1814020	1817740	it's virtually impossible to really have a pause
1817740	1821940	in AI development because everything is sort of structured
1821940	1825500	by these game theoretic incentives to just keep going faster.
1825500	1827460	Once you've stumbled on the gold reserve,
1827460	1830660	it's hard to keep the prospectors from running there.
1830660	1835100	Samuel, is the US government prepared for advanced AI?
1835100	1835940	No.
1835940	1841140	No, I mean, where do I start?
1841140	1844420	I mean, the US government, if you think of it
1844420	1847820	from a firmware level, many countries have national IDs.
1847820	1848900	The US doesn't have a national ID.
1848900	1850140	We have social security numbers.
1850140	1852100	There are these like nine digit numbers
1852100	1854820	that date back to 1935.
1854820	1859100	We have the core administrative laws
1859100	1861980	date back to the early 40s.
1861980	1864540	Much of our sort of technical infrastructure,
1864540	1868980	like the system the IRS runs on,
1868980	1870660	date back to the Kennedy administration
1870660	1872860	and are written in assembly code.
1872860	1874820	There's also been this general decline
1874820	1878140	in what you could call state capacity, sort of the ability
1878140	1881900	for the US government to execute on things.
1881900	1883260	And you hear about this all the time.
1883260	1886020	You hear about how the Golden Gate Bridge was built
1886020	1888700	in four years or something like that.
1888700	1891380	And now it takes like 10 years to build an access road.
1891420	1895860	One of the reasons for that goes to what the legal scholar
1895860	1899300	Nicholas Bagley has called the procedural fetish.
1899300	1903260	Really, since the 70s, the machinery of the US government
1903260	1909300	has shifted towards a reliance on explicit process.
1909300	1913100	And proceduralism has pluses and minuses.
1913100	1916500	If you have a clear process, government
1916500	1918820	can kind of run an autopilot to an extent.
1918860	1921820	But it also means you limit the room for discretion
1921820	1925420	and you limit the flexibility of government to move quickly.
1925420	1927820	And moreover, in our adversarial legal system,
1927820	1932580	you also open up avenues for sort of continuous judicial review
1932580	1938900	and legal challenge, where famously New York has taken
1938900	1941900	over three years to approve congestion pricing
1941900	1943900	on one of their bridges because that's
1943900	1945140	to undergo environmental review.
1945140	1946500	And people who don't want to pay the congestion price
1946500	1947620	keep suing.
1947620	1949580	Do you think having more procedures
1949580	1954140	would make it easier for AI to interface with government?
1954140	1955820	I would say having fewer procedures
1955820	1957700	would make it easier for government to adapt.
1957700	1960300	My assumption would be that having something written down,
1960300	1963100	having a procedure for something would make it easier for AI
1963100	1965580	to plug AI into that procedure.
1965580	1970980	If it's less opaque and more kind of almost like an algorithm
1970980	1972500	step by step.
1972500	1974140	Yes.
1974180	1979180	But the analogy I would give is to the Manhattan Project.
1979180	1982340	The original Manhattan Project was run like a startup.
1982340	1985700	You had Oppenheimer and General Leslie Groves sort
1985700	1990340	of being the technical founder and the Type A
1990340	1991940	get things done founder.
1991940	1993820	And they broke all the rules.
1993820	1996420	They pushed as hard as they could.
1996420	2000620	They were managing at its peak like 100,000 people in secret.
2000620	2004020	And they built the nuclear bomb in three years.
2004020	2007020	And so the way we would do that today
2007020	2009540	under procedural fetish framework
2009540	2012660	would be to put out a bunch of request for proposals
2012660	2017460	and have some kind of competitive bid.
2017460	2020460	And then we'd probably get the lowest cost bid.
2020460	2021940	And it would be Lockheed Martin.
2021940	2026340	And they would build half an atom bomb.
2026340	2030660	And it would take 20 years and five times the budget.
2030660	2033020	And so that's sort of what I'm getting at.
2033060	2036300	It's not about process versus discretion per se.
2036300	2039500	It's about the way process hobbles and straight jackets
2039500	2041980	are ability to adapt and sort of represents
2041980	2047420	a kind of sclerosis, a kind of crystallized intelligence.
2047420	2052660	We lay down the things that worked in the past as process
2052660	2055700	and sort of freeze those processes in place,
2055700	2059900	ossifying a particular modality.
2059900	2061540	And when the motor production shifts
2061540	2063980	and you need to completely tear up
2063980	2066740	that process, root and branch, is very difficult.
2066740	2070660	Because often there's no process for changing the process.
2070660	2073380	Yeah, I wonder if there are lessons
2073380	2076260	for how government will respond to AI
2076260	2078300	and thinking about how governments responded
2078300	2082340	to, say, historical technical innovations
2082340	2086380	of a similar magnitude, like the Industrial Revolution
2086380	2090380	or the printing press or maybe the internet computer.
2090420	2092260	Do you think we can draw general lessons?
2092260	2095860	Or is it so specific that we can't really
2095860	2098180	extract information about the future from them?
2098180	2100300	I think there are very powerful general lessons.
2100300	2101860	I think one of the first general lessons
2101860	2105060	is that every major technological transformation
2105060	2110340	in human history has preceded a institutional transformation.
2110340	2113300	Whether it's the shift from nomadic to settled city
2113300	2115460	states with the agricultural revolution
2115460	2118740	or the rise of modern nation states
2118740	2122380	or the end of feudalism with the printing press
2122380	2125020	to in the New Deal era, the sort of transition
2125020	2127020	with industrialization from the kind of laissez-faire,
2127020	2130780	classical liberal phase of 18th century America
2130780	2134780	to an America with a robust welfare state
2134780	2137460	and administrative bureaucracies and really
2137460	2140140	in all new constitutional order.
2140140	2142940	And so there's sort of better and worse ways
2142940	2144260	for this transition to happen.
2144260	2146540	There's sort of the internal regime change model.
2146540	2150260	And you can think of Abraham Lincoln or FDR
2150260	2154540	as inaugurating a new republic, a new American republic.
2154540	2156900	Or there's a scenario where we don't change
2156900	2159820	because we're too crystallized and sort of like an innovator's
2159820	2162380	dilemma get displaced by some new upstart.
2162380	2164980	And there are different countries have different abilities
2164980	2168820	and different sort of capacities for that internal adaptation.
2168820	2171340	As a Canadian, I'm a big fan of Westminster-style
2171340	2172260	parliamentary systems.
2172260	2174660	And one of the reasons is because it's
2174660	2177460	very easy for parliamentary systems
2177460	2180500	to shut down ministries, open up new ministries,
2180500	2184820	to reorganize the civil service because it's sort
2184820	2187660	of vertically integrated under the Prime Minister's office
2187660	2189180	or what have you.
2189180	2192620	In the US, it's much worse because given
2192620	2196020	the separation of powers, Congress and the executive
2196020	2202860	are often not working well together as an understatement.
2202900	2205500	But then moreover, the different federal agencies
2205500	2207500	have it sort of a life of their own.
2207500	2209820	Often they're self-funded and all these other things
2209820	2211740	that make it very difficult to reform.
2211740	2213580	Do you think Canada responded better
2213580	2216260	to the rise of the internet than the US, for example?
2216260	2217980	Isn't there something wrong with the story
2217980	2219940	because the US kind of birthed the internet
2219940	2224060	and Canada adopted the internet from the US?
2224060	2227460	Let's compare, first of all, the impact of the internet
2227460	2230540	on weaker states because Canada and the US
2230700	2235220	are similar or sort of in one quadrant.
2235220	2236860	They have differences, but the differences
2236860	2238580	are small compared to other countries.
2238580	2240860	If you think about internet safety discussions that
2240860	2243020	would have been taking place in the early 2000s,
2243020	2245340	people would have been talking about identity theft,
2245340	2248940	credit card theft, child exploitation, these kind
2248940	2252620	of direct first order potential harms from the internet.
2252620	2258220	They didn't foresee that concurrent with the rise of mobile
2258220	2261660	and social media that the internet would enable tools
2261660	2263460	for mass mobilization simultaneous
2263460	2265780	with a kind of legitimacy crisis where
2265780	2270100	the sort of new transparency and information access
2270100	2272620	that the internet provided eroded trust and government
2272620	2274220	and trust in other institutions.
2274220	2277100	So you have these two forces interacting,
2277100	2279660	the internet exposing government and exposing corruption
2279660	2282900	and leading to a decline in trust while also creating
2282900	2286620	a platform for people to rise up and mobilize against that
2286620	2287580	corruption.
2287620	2289500	And it's something that kind of rhymes
2289500	2291940	with the printing press and the printing revolution
2291940	2296340	where you had these sort of dormant suppressed minority
2296340	2299140	groups like the Puritans or the Presbyterians,
2299140	2302700	the nonconformists, and with the collapse
2302700	2306420	of the censorship printing licensing regime.
2306420	2310540	They actually had a licensing regime in the UK parliament
2310540	2312060	back circa 1630.
2313820	2315260	That licensing regime collapsed,
2315260	2317260	there I think 1634 or something around there,
2317260	2320820	and that was like five years before the English Civil War.
2320820	2323620	And you see something like this in the Arab Spring
2323620	2327980	where the internet quite directly led
2327980	2331780	to mass mobilization in Cairo and Tunisia and elsewhere
2331780	2333380	and led to actual regime change,
2333380	2336220	in some cases sort of temporary state collapse.
2336220	2337820	And that's because those were weaker states
2337820	2339740	that hadn't democratized,
2339740	2342620	that hadn't sort of had their own information revolution
2343580	2345660	earlier in their history the way we did, right?
2345740	2347180	In some ways like the American Republic
2347180	2351740	is sort of a founder country built on the backbone
2351740	2353420	of the printing revolution.
2353420	2357500	So we were a little bit more robust to that
2357500	2359700	because it's sort of part of our ethos
2359700	2362980	to have this open disagreeable society.
2362980	2366660	But clearly the internet has also affected
2366660	2368940	the legitimacy of Western democracies.
2368940	2371940	I think it's clear, clearly one of the major inputs
2371940	2374420	in sort of rising populism,
2374420	2376860	the mass mobilizations that we see,
2376860	2381660	whether in the US context, the 2020 racial awakening
2381660	2385340	or the January 6th sort of peasant rebellion, right?
2385340	2389540	These sort of look like the kind of color revolutions
2389540	2391420	that we see abroad.
2391420	2394100	And some people want to ascribe conspiracy theories
2394100	2396260	to that, I think there's a simpler explanation,
2396260	2398980	which is that people will self-organize
2398980	2400340	with the right tools.
2400340	2401740	Our state hasn't collapsed yet,
2402580	2406820	but there's clearly a lot of cracks in the foundation,
2406820	2407740	if you will.
2407740	2409460	Is it, would it be fair to say that the main lesson
2409460	2412340	for you from history is that technological change
2412340	2414740	brings institutional change?
2414740	2416900	Yeah, not necessarily one for one.
2416900	2420580	I'm not kind of a vulgar Marxist on this, but yes.
2420580	2424020	And the reason for that is because institutions themselves
2424020	2427220	exist due to a certain cost structure.
2427220	2429100	And if you have general purpose technologies
2429100	2431220	that dramatically change the nature
2431220	2433460	of that cost structure, then institutional change will follow.
2433460	2434980	Yeah, and I think we want to get to that.
2434980	2438180	But before we do, I think we should discuss AI's impact
2438180	2439860	on the broader economy.
2439860	2442820	So not just the government, but the economy in general.
2442820	2447820	Economists have this fallacy they point out often,
2448060	2449500	the lump of labor fallacy.
2449500	2450900	Maybe you could explain that.
2450900	2452780	The lump of labor fallacy is essentially the idea
2452780	2455060	that there's a fixed amount of work to be done.
2455060	2458740	If you were thinking about the Industrial Revolution
2458740	2460300	and what would happen to the 50% of people
2460300	2462580	who are in agriculture, you couldn't imagine
2462580	2464060	the new jobs that would be created.
2464060	2465540	But new jobs were created.
2465540	2470060	And the reason is because human wants are infinite.
2470060	2473820	And so demand will always fill supply.
2474980	2476980	The second reason is because there's a kind of circular flow
2476980	2479660	in the economy where one person's cost
2479660	2481300	is another person's income.
2481300	2483420	Society would collapse if we had true technological
2483420	2486900	unemployment because there'd be things being produced
2486900	2488300	but no one to pay for them.
2489300	2491380	And so that ends up kind of bootstrapping new industries
2491380	2493700	and new sources of production.
2493700	2496500	There's still this open question, is this time different?
2496500	2499540	Yeah, that's exactly what I wanna know.
2499540	2502820	Because for me, it's in retrospect, let's say.
2502820	2506700	It's easy to see how workers could move from fields
2506700	2509460	to factories into offices.
2509460	2512020	But if we have truly general AI,
2512020	2514700	it's difficult for me to see where workers would move.
2514700	2518380	Especially if we have also functional robots
2518380	2523380	and perhaps AIs that are better at taking care of people
2523620	2525620	than other people are.
2526780	2528860	I'm not asking you to predict specific jobs,
2528860	2531260	but I'm asking you whether you think
2531260	2534700	this historical trend will hold with the advent
2534700	2536540	of advanced AI.
2536540	2539220	You know, the first thing to say is, you know,
2539220	2543060	when Keynes wrote economic possibilities
2543060	2546780	for our grandchildren, a famous text where he predicted
2546780	2548380	that technological progress would lead
2548380	2551860	to the growth of a leisure society.
2551860	2555740	And this was in the 1930s, yeah.
2555740	2559180	You know, people have dismissed him as being wrong,
2559180	2561780	but actually you look at time use data
2561780	2564460	and employment data and people are working less.
2564460	2567380	You know, it's not, it didn't match his,
2567380	2569860	the optimism of his projection, right?
2569860	2574140	Because it turns out, you know, maybe if we fixed
2574140	2576660	living standards at what he expected,
2576660	2580100	people want more and people will work more for more.
2580100	2582180	But overall, people are working less.
2582180	2583940	People do have more leisure.
2583940	2586860	We've sort of moved to a de facto four-day work week.
2586860	2590220	So there's one world where rapid technological progress
2590220	2594460	sort of continues that trend and we all work less.
2594460	2595860	It's sort of a technological unemployment
2595860	2599700	that's spread across people and is enabled in part
2599700	2603700	because in a world of AGI, maybe you only have to work,
2603700	2607820	you know, a few hours a day to make $100,000 a year.
2607820	2611820	There's another possibility which is that,
2611820	2615460	well, AGI could in principle be a perfect emulation
2615460	2618140	of humans on specific tasks.
2618140	2622140	It can't emulate the historical formation of that person.
2622140	2623060	Right?
2623060	2626780	So what I mean by that is if you had a perfect
2626820	2629860	Adam by Adam replication of the Mona Lisa,
2630820	2633700	it wouldn't sell at auction, right?
2633700	2637500	Because people aren't just buying the physical substrate,
2637500	2642500	they're also buying the kind of world line of that thing.
2643140	2647260	And that's clearly the case in humans as well.
2647260	2649700	Like there are certain, you know, talking heads
2649700	2653500	that I go and enjoy not because they are the smartest
2653500	2655260	or would have you because I'm interested
2655260	2657420	in what that person thinks on this
2657420	2658700	because they have a particular personality,
2658700	2659980	a particular world line.
2659980	2664980	And then the third factor is sort of artificial scarcity.
2665700	2666540	Right?
2666540	2671540	And so even in a world with abundance and supply
2671940	2674180	in services and goods, there are still things
2674180	2675940	that will be intrinsically scarce,
2675940	2678580	real estate being probably the canonical thing,
2678580	2680620	but also energy and commodities and so forth.
2680620	2682340	And the reason real estate is intrinsically scarce
2682420	2686180	because people want to live near other people
2686180	2691180	and people want to live in particular areas of a city.
2691180	2693540	They want to live in the posh part of town, right?
2693540	2694820	And those are positional goods.
2694820	2698260	We can't all live in the trendy loft.
2698260	2700500	So that builds in a kind of artificial scarcity.
2700500	2704100	And so people will still be competing over those things.
2704100	2705860	This is sort of related to artificial scarcity,
2705860	2707900	but there's also sort of break it out
2707900	2710940	into a fourth possibility, which are sort of tournaments
2710940	2713060	and things that are structured as tournaments.
2713060	2715180	Having chess spots that are strictly better
2715180	2719860	than humans at chess hasn't killed people playing chess.
2719860	2722100	If anything, more people play chess today
2722100	2723460	than they've had in human history.
2723460	2725500	Yeah, it's more popular than ever.
2725500	2726340	Yeah.
2726340	2729020	And the reason is because people like to watch
2729020	2731740	other humans playing and also they're structured
2731740	2733620	as sort of zero sum tournaments
2733620	2735980	where there can only be the best human.
2735980	2737460	You look at other things that have been created
2737500	2741780	just in the last 15, 20 years, the X games, right?
2741780	2743900	I think people will still want to watch other people
2743900	2745660	do the Olympics or do motocross
2745660	2746900	and all these other things.
2746900	2749540	And so maybe more of our life shifts into,
2749540	2753220	both maybe greater leisure on the one hand,
2753220	2756180	more competition over positional goods
2756180	2761180	and more production that is structured as a tournament.
2761540	2763580	Yeah, yeah, I can see many of those points.
2763580	2766700	I'm just thinking, again, with fully general AI,
2766740	2768580	you would be able to generate
2768580	2772100	a much more interesting person playing chess
2772100	2775340	or at least a simulation of a very charismatic
2775340	2777860	and interesting human chess player.
2777860	2780940	Why wouldn't people watch that chess player
2780940	2784660	as opposed to the best human?
2786180	2789700	Maybe they will, sorry to know.
2789700	2792180	The question is who's producing that video stream
2792180	2796060	because you still need the human behind it
2797020	2798980	that had the idea, right?
2798980	2802260	And you could imagine people being dishonest
2802260	2805740	about the history of this chess player.
2805740	2810420	The simulated chess player could be a fully digital,
2810420	2812740	fully fictional, so to speak,
2812740	2815140	and just pretending to be human.
2815140	2817380	Right, so they could fool people.
2817380	2818700	That's the case too.
2818700	2820260	No, I can't rule that out,
2820260	2821900	but I would just say that
2821900	2824100	however that person is monetizing,
2824140	2826540	their deep fake chess player,
2826540	2828260	they're making money, which they're then spending back
2828260	2830740	into the economy, and so they'll produce jobs somewhere.
2830740	2833340	Do you think more people will move into,
2833340	2837340	say, people-focused industries like nursing and teaching?
2837340	2842340	Is that a possible way for us to maintain jobs?
2842740	2844820	Maybe nursing, at least in the short run.
2846020	2848140	I'm not very long on education
2848140	2851780	being labor-intensive for much longer.
2851780	2853740	But you don't think education,
2853740	2856220	at least say, great school education,
2856220	2860460	is that really about teaching people or conveying knowledge?
2860460	2862980	Or to what extent is it about conveying knowledge?
2862980	2865740	And to what extent is it about the social interaction
2865740	2870740	and specializing your teaching to the individual student?
2872020	2875260	Well, AI is very good at customization
2875260	2877020	and sort of mastery tutoring.
2877020	2879420	Education is a bundle of things.
2879420	2883980	And for younger ages, it's also daycare.
2883980	2886180	It is socialization, like you said.
2886180	2890820	At the very least, it's just a reorganization
2890820	2892060	of the division of labor,
2892060	2894740	because the types of teachers that you would select
2894740	2898060	or hire for may differ if the education component
2898060	2900340	of that bundle is being done by AI.
2900340	2902060	Maybe you select for people who are,
2902060	2903700	maybe don't have any subject matter expertise,
2903700	2907020	but are just highly conscientious and go to around kids.
2907060	2910260	Or maybe you one bundle from public education altogether,
2910260	2914220	and it re-bundles around a jujitsu school
2914220	2918300	or a chess academy, because you'll have the AI tutor
2918300	2919140	that will teach you math,
2919140	2921580	but you'll still want to grapple with the human.
2921580	2922500	Yeah, yeah.
2922500	2925740	What about industries with occupational licensings
2925740	2927580	like law or medicine?
2927580	2931620	Will they be able to keep up their quite high wages
2931620	2936380	in the face of AI being able to be a pretty good doctor
2936420	2938100	and a pretty good lawyer?
2938100	2941660	It's easy to solve for the long-term equilibrium.
2941660	2942500	With the rise of the internet,
2942500	2946500	you can do a comparison of the wage distribution
2946500	2948700	for lawyers pre and post internet.
2948700	2951220	And, you know, circa the early 90s,
2951220	2953260	lawyer incomes were normally distributed
2953260	2955180	around $60,000 a year.
2955180	2958940	You know, after in the 2000s, they become bimodal.
2958940	2960900	And so you have one mode that's still around
2960900	2963860	that $60,000 range, those are like the family lawyers.
2963860	2965180	And then you have this other mode
2965180	2967060	that's into the six figures.
2967060	2968900	And those are like big law, right?
2968900	2971420	It's the emergence of these law firms
2971420	2972620	where you have a few partners on top
2972620	2975340	and maybe hundreds of associates
2975340	2977420	who are doing kind of grant work using Westlaw
2977420	2980980	and Lexus Nexus and these other legal search engines
2980980	2985780	to accelerate drafting and legal analysis.
2985780	2987660	So if that pattern repeats,
2987660	2992660	I could imagine these various high-skill knowledge sectors
2993660	2997700	to also become bimodal where in the short run,
2997700	3000660	AI serves as a co-pilot, sort of like Westlaw
3000660	3003380	or Lexus Nexus was for legal research
3003380	3006140	and enables the kind of 100x lawyer.
3006140	3010340	And so there's a kind of averages over dynamic.
3010340	3015340	The longer run, you know, you start to see the possibility
3015740	3020020	of doing an end run around existing accreditation
3020020	3022460	and licensing monopolies
3022500	3026220	where obviously the American Medical Association
3026220	3031220	and medical boards will be highly resistant to an AI doctor.
3031580	3033620	I tend to think that they'll probably end up self cannibalizing
3033620	3036060	because the value prop is so great
3036060	3039460	even for doctors to do simple things
3039460	3042940	like automate insurance paperwork and stuff like that.
3042940	3044740	But to the extent that there is a resistance,
3044740	3047700	to the extent that in 10 years there's still a requirement
3047700	3051940	that you must have the doctor prescribe the treatment
3051940	3053020	or refer you to a specialist,
3053020	3055100	even though the AI is doing all the work
3055100	3057780	and they're just sort of like the elevator person
3057780	3061140	that's like actually just pushing the button for you.
3061140	3063060	It'll be very easy to end run that
3063060	3067460	because AI is both transforming the task itself
3067460	3070740	but also transforming it's the means of distribution.
3070740	3073260	And if you can go to GPT-4 and ask for,
3073260	3075340	put in your blood work and get a diagnosis,
3076420	3078180	but no regulator's going to stop that, right?
3078180	3080140	And so, you know, GPT-4 becomes sort of
3080140	3082220	the ultimate doctor of up orders.
3082220	3085180	You write a lot about transaction costs
3085180	3088860	and how changes in transaction costs
3088860	3091340	change institutional structures.
3091340	3093620	First of all, what are transaction costs
3093620	3096380	and how do you think they'll be affected by AI?
3096380	3098500	So transaction cost is sort of an umbrella term
3098500	3102780	for different kinds of costs associated with market exchange.
3102780	3105620	And this goes back to Ronald Coase's famous paper
3105620	3108300	on the theory of the firm where he asked the question,
3108300	3110540	why do we have corporations in the first place?
3110540	3111860	If free markets are so great,
3111860	3116140	why don't we just go up and spot contract for everything?
3116140	3119780	And the answer is, well, market exchange itself has a cost.
3119780	3121100	There's the cost of monitoring.
3121100	3122220	You know, if you hire a contractor,
3122220	3124380	you don't know exactly what they're doing.
3124380	3125820	There's the cost of bargaining.
3125820	3128860	You know, having to haggle with a taxi cab driver
3128860	3131700	is a friction.
3131700	3132940	And there's the cost of searching,
3132940	3134780	the associate of searching information.
3134780	3136620	So taking those three things together,
3136620	3138020	they're not all that companies do,
3138020	3140900	but they structure the boundary of the corporation.
3140900	3142780	They explain why some things are done in-house
3142780	3144940	and some things are done through contracts.
3144940	3146380	If there's high monitoring costs,
3146380	3147860	you want to pull that part of the production
3147860	3150900	into the company so that you can monitor
3150900	3153540	and manage the people doing the production.
3153540	3155740	And some of the same effects go for
3155740	3158340	the existence of governments, right?
3158340	3160660	Yes, because governments, you know,
3160660	3161860	with a certain gestalt,
3161860	3163780	governments and corporations aren't that different.
3163780	3165380	There are kinds of institutional structures
3165380	3166740	that pull certain things in-house
3166740	3171220	and certain things are left for contracting or outsourced.
3171220	3174700	And, you know, you even see sort of different kinds
3174700	3177060	of governments having different parallels
3177060	3179420	with different kinds of corporate governance, right?
3179420	3181460	Relatively egalitarian democratic societies
3181460	3185140	like Denmark are kind of like mutual insurers.
3185140	3189700	Whereas more hierarchical authoritarian countries
3189700	3191820	are more like, you know, like Singapore, say,
3191820	3195220	is more of a joint stock corporation.
3195220	3197540	And indeed, you know, Singapore was founded
3197540	3198620	as a, as a, uh,
3198620	3201380	an entrepot for the East India Company.
3201380	3204580	So there are very deep parallels.
3204580	3206260	And it's also essential.
3206260	3207420	Transaction costs are essential
3207420	3209100	to understand why governments do certain things
3209100	3210180	and other things.
3210180	3212220	All Western developed governments
3212220	3215780	guarantee some amount of basic health care, right?
3215780	3219300	But, um, most, you know,
3219300	3221940	outside of, say, the National Health Service in,
3221940	3224500	in the UK, most of these countries, uh,
3225260	3226260	guarantee the insurance.
3226260	3230020	They don't necessarily nationalize the actual providers,
3230020	3230860	right?
3230860	3232540	And the reason goes to transaction costs
3232540	3235100	and sort of an analysis of the market failure
3235100	3236380	and insurance.
3236380	3239060	Likewise with roads, uh, you know,
3239060	3242260	it's possible to build roads through purely private means.
3242260	3245380	And indeed, um, you know, countries like Sweden,
3245380	3248140	a lot of the roads are run by private associations.
3248140	3250900	But, uh, if you have lots of different boundaries,
3250900	3253460	different micro jurisdictions and so forth,
3253460	3255220	there can be huge transaction costs
3255220	3258100	to, uh, negotiating up to a,
3258100	3260700	to a interstate highway system.
3260700	3262420	Um, and, and those transaction costs
3262420	3266060	then necessitate public infrastructure projects.
3266060	3267620	So the transaction costs in this case
3267620	3270540	would be being a private road provider.
3270540	3274860	You'd have to go negotiate with 500 different landowners
3274860	3276100	about building a highway,
3276100	3279220	whereas a government can do some expropriation
3279220	3282140	and simply build the road much, much faster,
3282140	3285300	or with less transaction costs at least.
3285300	3286260	Yeah, precisely.
3286260	3288900	And we're seeing this, this dynamic in the US
3288900	3290860	with, uh, you know, permitting for,
3290860	3292820	for great infrastructure and transmission.
3292820	3294300	You know, we're, we're building all this,
3294300	3295620	all the solar and renewable energy,
3295620	3299820	but to build the actual transmission, uh, infrastructure
3299820	3302980	to get the electrons from where it's sunny to where,
3302980	3306300	where it's cold requires building, you know,
3306300	3309420	high voltage, uh, lines across state lines
3309420	3311260	across different grid regions.
3311260	3313300	And there are all kinds of NIMBs
3313300	3317220	and negotiation costs involved, holdouts and so forth.
3317220	3319260	And so that the more those kind of costs exist,
3319260	3321780	the more it militates towards a kind of, um,
3321780	3324620	larger scale intervention that, you know,
3324620	3326820	federalizes that process.
3326820	3329180	Yeah, the big question then is how will AI
3329180	3331820	change these transaction costs?
3331820	3334420	What, what will the effects be here?
3334420	3337060	It's easy to say that they will be affected.
3337060	3338940	And, you know, obviously the internet affected them
3338940	3339940	to, to an extent.
3339940	3342980	And we were talking, we talk about sort of the ease
3342980	3346820	of mobilizing protest movements or the kind of, uh,
3346820	3348780	the sunlight that was put on government corruption.
3348780	3351500	Those are, those are reflecting declines
3351500	3356180	in the cost associated information and coordination.
3356180	3357620	I think AI takes us to another level.
3357620	3361020	And I think it's, it's important to, to think through
3361020	3366020	in part because right now the AI safety debate,
3366460	3368880	at least in the United States is very polarized
3368920	3372360	between people who are like, everything's going to be great.
3372360	3375440	And people who are like, this is like a terminator scenario
3375440	3377560	or an AI kill us all existential risk.
3377560	3380960	You know, even if we accept the, you know,
3380960	3383400	existential risk framing, there's still going to be
3383400	3387120	many intermediate stages of AI before we flip
3387120	3388600	on the superintelligence.
3388600	3391560	And those intermediate stages have enormous implications
3391560	3393920	for the structure of the very institutions
3393920	3397400	that we'll need to respond to superintelligence
3397400	3398360	or what have you.
3398360	3402720	The ways we can see this is because all these information
3402720	3407000	and, and monitoring and, and, uh, bargaining costs
3407880	3411280	are directly implicated by commoditized intelligence.
3411280	3413440	You know, start with the principal agent problem.
3413440	3415040	You know, there is no principal agent problem
3415040	3419160	if your agent does exactly as you ask and works 24 seven
3419160	3421240	doesn't steal from the till, right?
3421240	3425760	And so AI agents dramatically collapse agency cost
3425760	3426960	monitoring.
3426960	3429160	Now that we have multimodal models in principle,
3429160	3431520	we could have cameras in every house that are just being
3431520	3433880	prompted to say, you know, is someone committing a crime
3433880	3434720	right now?
3435920	3438280	Whether we wanted to go that direction or not,
3438280	3440840	it gives you a sense that of how, you know,
3440840	3442720	the cost of monitoring have basically plummeted
3442720	3445320	over the last two years and are going to go way lower.
3445320	3446920	And so you're starting to see this rolled out
3446920	3448520	in the private sector with, you know,
3448520	3451280	Activision has announced that they're going to be using
3451280	3455160	language models for moderating voice chat
3455160	3457200	and call duty, right?
3457200	3459920	And, and this, this is a more robust form of monitoring
3459920	3463240	because in the past you would have to like ban certain words
3463240	3467480	like certain swear words or things associated with
3467480	3470440	sexual violence, but then people could always get around
3470440	3474160	those by using euphemisms, right?
3474160	3476560	Like on YouTube, you know, the algorithm will ding you
3476560	3480040	if you talk about coronavirus or if you talk about murder
3480040	3483280	or suicide, these, these things that throw off at flags.
3483280	3486240	So what people have taken doing is saying they were
3486240	3488880	unalived rather than murdered, right?
3488880	3492440	And that doesn't fool a language model.
3492440	3493880	If you ask a language model, you know,
3493880	3496280	if you prompt it in a way to look for sort of broad
3496280	3500680	semantic categories, not just a narrow, a narrow word,
3500680	3502160	it's much more robust.
3502160	3503160	And so what that means, you know,
3503160	3504680	what you already start to see it, like I said,
3504680	3507920	with Activision and the use of LLAMS and content moderation,
3507920	3511040	you're going to start, you're going to see it in the use of
3511080	3514920	multimodal models for productivity management and tracking.
3514920	3518360	You know, Microsoft is unveiling their 365 co-pilot
3518360	3521920	where you're going to have GPT-4 and Word and Excel
3521920	3524200	and Teams and Outlook, but at the same time,
3524200	3526600	you're also going to have a manager who is going to be able
3526600	3529080	to say, you know, to prompt the model,
3529080	3531600	tell me who is the most productive this week, right?
3531600	3533200	Something as vague as that.
3533200	3536320	And so you see this diffusion in the private sector.
3536320	3540000	The question is, does it diffuse in the public sector?
3540000	3543760	There's obvious ways that it would be a huge boom, right?
3543760	3547160	You know, Inspector General GPT could tell you exactly,
3547160	3549320	you know, how the civil service is working,
3549320	3550160	whether there's corruption,
3550160	3551480	whether there's a deep state of conspiracy
3551480	3553240	or something like that, right?
3553240	3556680	And at first blush, a lot of what government does
3556680	3559880	is kind of a fleshy API.
3559880	3564880	Bureaucracies are nodes that apply a degree of context
3564880	3566440	between printing out a PDF and scanning it
3566440	3568080	back into the computer.
3568240	3569080	It varies.
3569080	3572720	There's degrees of human judgment that are required,
3572720	3575800	but on first order, government bureaucracies
3575800	3578640	seem incredibly exposed to this technology
3578640	3580480	in a way that could diffuse really rapidly
3580480	3583480	because, you know, going back to Microsoft 365 Copilot,
3583480	3586360	Microsoft is the biggest IT vendor in US government, right?
3586360	3590240	And so you can imagine once everyone has this pre-installed
3590240	3592600	on their computer that the person at the Bureau
3592600	3594040	of Labor Statistics who's in charge
3594040	3597360	of doing the monthly employment situation report,
3597800	3599560	the jobs report, you know, at some point
3599560	3602160	he's gonna be walking into work and hitting a button, right?
3602160	3606080	That, you know, asking Excel to find
3606080	3608480	the five most interesting trends and generate charts
3608480	3610440	and the report is done.
3610440	3612440	And in the private sector, that person would be reallocated
3612440	3615320	and maybe doing things that the computer's not good at yet,
3615320	3618640	but these positions are much stickier in government.
3618640	3620160	To the extent that diffusion is inhibited
3620160	3621640	on the public sector side,
3621640	3624640	I worry about the kind of disruption and displacement
3624640	3627040	of government services by a private sector
3627440	3629480	that's adopting the technology really fast.
3629480	3631520	This is something we'll talk about in a moment.
3631520	3634320	Before that, I just wanna get to your complaints
3634320	3636200	about isolated thinking about AI.
3636200	3641200	You've sketched out some complaint about people
3641200	3645120	thinking about AI only applying to one domain
3645120	3647480	and then not really seeing the bigger picture.
3647480	3649480	What are some examples here?
3649480	3652000	Why do you worry about isolated thinking?
3652000	3653120	A few dimensions to this.
3653120	3656080	One is what I've called the horse's carriage fallacy.
3657040	3658840	Right, the kind of view that, you know,
3658840	3663040	what automobiles were was just a carriage with the horse.
3663040	3666320	Right, and so that anchors you to the older paradigm
3666320	3667760	and it's like you're changing one thing
3667760	3669160	and everything else stays the same.
3669160	3671800	And you neglect all the second order ways
3671800	3674160	that the development of the automobile,
3674160	3677640	you know, enabled the build out of highway systems,
3677640	3682560	the total reconfiguration of sort of the economic geography.
3682560	3686040	Right, and then implications for institutions
3686040	3688480	at the state where, you know, once you have road networks
3688480	3690200	or telegraph networks or any of these,
3690200	3691720	these kind of networks, it suddenly becomes easier
3691720	3694720	to monitor agents of the state
3694720	3695800	and other parts of the country.
3695800	3697400	And so you can, you know, build out
3697400	3698840	more of a federal bureaucracy.
3698840	3700760	And so all these things were second order
3700760	3702720	and were kind of neglected if you just were too focused
3702720	3706440	on the first order effects of displacing the horses.
3706440	3709440	And in a sense, the second order effects
3709440	3712680	turned out to be much more consequential in the end.
3712680	3714800	Yes, it seemed to always be.
3714800	3717880	And likewise with the internet and sort of the,
3717880	3719920	I think this comes up a lot in the,
3719920	3723360	in how to think about AI use and misuse.
3723360	3724720	There's lots of valid discussions there,
3724720	3726920	but they're always very first order.
3726920	3728200	And when you think about the way the internet
3728200	3730160	has disrupted legacy institutions,
3730160	3731720	yes, there's disinformation,
3731720	3736240	but often the thing that's disrupting is not fake news.
3736240	3738640	It's real news that's being repeated
3738640	3741440	with misleading frequency, right?
3741440	3744680	That's like throwing off our availability heuristic.
3744680	3748400	Or it's valid things that, you know,
3748400	3750600	valid complaints, whether, you know,
3750600	3751760	the protests in Iran, right?
3751760	3754280	The protests in Iran have this like striking parallel
3754280	3756840	to the protests following the George Floyd protest
3757800	3760160	and protesting in other countries
3760160	3762780	where they even have like a three word chant, right?
3762780	3767780	Or the case of the Arab Spring in Tunisia
3767920	3770900	that started with the person self-immolating, right?
3770900	3772360	There's sort of like the structure that repeats
3772360	3773280	where you have like a martyr
3773280	3775440	or like some shocking event.
3775440	3777960	And because of the way social media is organized,
3777960	3781320	it synchronizes people around the event
3782440	3784240	in a way that's kind of stochastic.
3784240	3785720	Like it's like lightning striking.
3785720	3787600	You don't know what event it's going to strike on,
3787600	3789320	but once we're synchronized,
3789320	3791280	then we start, you know, moving back and forth
3791280	3794280	in a way that like causes the bridge to buckle.
3794280	3797040	Nothing about that is a misuse, right?
3797040	3798840	Those are all valid uses,
3798840	3803040	but their use is under collective action.
3803040	3805200	It's sort of solving not just for the partial equilibrium
3805200	3808400	but the general equilibrium when everyone is doing this.
3808400	3810920	And I think the person who wrote the best
3810920	3813840	on this sort of conceptually was Thomas Schelling
3813840	3816240	and one of his little books,
3816240	3818120	Micromotives, Macro Behavior,
3818120	3819280	A Big Influence on Me as a Kid,
3819280	3822200	where he talks about all these sort of like toy models
3822200	3825160	where you're at a hockey game or a basketball game
3826080	3827600	and something is happening,
3827600	3830600	something exciting is happening in the arena.
3830600	3832400	And so people in front of you stand up
3832400	3834520	to get a better view and then you have to stand up
3834520	3836680	to get a better view of them over them and so on.
3836680	3838760	And so it cascades and suddenly everyone went from sitting
3838760	3839640	to everyone went to standing
3839640	3842080	and no one's view has improved, right?
3842080	3844880	And so these sort of general equilibria
3844880	3848120	where you sort of solve for everyone's micro incentives
3848120	3850880	and the kind of new Nash equilibrium that emerges,
3850880	3853480	that ends up being the thing that drives
3853480	3855440	a kind of multiple equilibrium shift
3855440	3857680	from one regime to another.
3857680	3861200	And throughout, there may be no actual examples
3861200	3862040	of misuse involved.
3862040	3865360	It may just be people following their individual incentives.
3865360	3868200	I think it's worth the stressing this point you make
3868200	3872400	about the effects of earlier AI systems
3872400	3873800	on our institutions,
3873800	3877160	that they might have effects that deteriorate our institutions
3877160	3881200	such that we can't handle later and more advanced AI.
3881200	3885160	And ignoring this would be an example of isolated thinking
3885160	3888120	and ignoring the second order effects, right?
3888120	3888960	Yeah.
3888960	3893960	And they also, it also changes the sort of agenda, right?
3894720	3896960	The AI safety agenda shouldn't just be
3896960	3899480	about the first order of things or in alignment,
3899480	3901240	you know, very important,
3901240	3905240	but you know, it's led to a discussion of
3905240	3906520	do we need a new federal agency?
3906520	3908840	And if so, what kind of agency?
3908840	3911680	Whereas it may be more appropriate to think
3911680	3913120	not what new agency do we need,
3913120	3917720	but how do all the agencies change, right?
3917720	3920000	And how do we sort of like brace for impact
3920000	3924040	and enable a degree of co-evolution
3924040	3925800	rather than displacement?
3925800	3928600	I don't know whether the question of
3928600	3932480	how to get our institutions to respond appropriately
3932480	3935280	is more difficult or less difficult
3935280	3937080	than the problem of aligning AI.
3937080	3938920	But it certainly seems very difficult to me.
3938920	3942520	So is there, are we making it harder on ourselves
3942520	3944720	if we focus on the effects,
3944720	3947440	on the second order effects on institutions?
3947440	3948920	I mean, it's unavoidable.
3948920	3952360	I mean, we can't pick and choose what kind of problems,
3952360	3955240	but you know, the alignment problem,
3955240	3958240	the hard version is yet to be solved,
3958240	3960920	but we have many examples of governments
3960920	3964280	building state capacity and having kind of,
3964280	3965960	you know, shifting from very,
3965960	3970200	very like clientelistic, sticky corrupt governments
3970200	3972520	to sort of modernized governments
3972520	3974840	where you know, state capacity is built
3974840	3976520	and then that government can sort of break out
3976520	3978760	of the middle income trap and become rich.
3978760	3983440	You mentioned Estonia as an example of a country
3983440	3985520	that's pretty advanced on the IT front,
3985520	3986480	on the technology side.
3986480	3988880	Maybe you could talk a bit about Estonia.
3988880	3990920	Yeah, I would just say in general,
3990920	3994440	it's hard for any organization to reform itself from within
3994440	3995320	when there is path dependency,
3995320	3999120	but I would say at least we have examples of it being done
3999120	4002040	where we don't have examples of alignment being solved yet.
4003360	4005440	When it comes to Estonia,
4005440	4006840	you know, Estonia is an interesting case.
4006840	4008960	It's sort of an exceptional case
4008960	4010960	because after the fall of the Soviet Union
4010960	4014360	and the breakup of the peripheral former Soviet states,
4014360	4017200	they kind of had a blank slate, right?
4017200	4018640	They also had a very young population
4018640	4021840	and people who had a kind of hacker ethic
4021840	4023400	within their civil service.
4023400	4026400	And so with that blank slate and with that hacker ethic,
4026400	4029680	they were very early to adopt and to foresee
4029680	4031440	the way the internet was going to shape government
4031440	4034880	through a variety of e-government reforms.
4034880	4037520	So early in the late 90s and into the 2000s,
4037520	4040360	they were some of the earliest to digitize
4040360	4043440	their banking system like e-banking
4043440	4047760	to build this system called X-Road,
4047760	4050080	which is kind of like a cryptographically secured
4050080	4052840	data exchange layer that resembles a blockchain,
4052840	4055920	but it was about a decade before blockchain was invented.
4055920	4057320	For exchanging information
4057320	4059960	between different government entities,
4059960	4063840	your medical information could be uploaded to the system
4063840	4065600	and then be available to all systems
4065600	4068800	that have the right to see that information.
4068800	4071160	Exactly, in a way that's cryptographically secured
4071160	4072000	and distributed.
4072000	4075480	So if a missile hit the Department of Education,
4075480	4076920	you don't lose your education records
4076920	4078960	because it's distributed.
4078960	4083080	And that also enabled an enormous amount of automation
4083080	4086000	where, for instance, this is my understanding,
4086000	4087120	a child born in Estonia,
4087120	4089360	once you file that birth record,
4089360	4093600	it more or less initiates a clock in the system
4093600	4096520	that will then enroll your child in school
4096520	4099160	when they turn four or five automatically
4099160	4101840	because it knows that your child has aged
4101840	4104840	and then unless it had a death record to cancel that out.
4104840	4107720	That also means you can do taxes and transfers
4107720	4112440	much simpler, you get your benefit within a week.
4112440	4114960	It can integrate across different parts
4114960	4117160	of public infrastructure,
4117160	4121480	like use the same card to ride the bus
4121520	4123880	as you do to launch a new business.
4123880	4125560	And it also serves as a kind of platform
4125560	4130360	for the private sector to do government by API,
4130360	4134600	to build new services on top of government as a platform
4134600	4137760	and integrate with government databases.
4137760	4138920	Yeah, and so the point here for us
4138920	4141960	is that institutional reform is possible,
4141960	4143800	modernizing government is possible,
4143800	4145280	at least under certain circumstances.
4145280	4150000	We have proofs of concepts of this happening.
4150000	4152000	The hard thing is the path dependency.
4152000	4153240	There's always a strong instinct
4153240	4154360	to wanna start from scratch
4154360	4156200	and it's normally not advisable
4156200	4158960	because it's too hard.
4158960	4163960	And so this is why it's hard in the US.
4164280	4165960	This is why you have African countries
4165960	4168280	that leap progress in payment systems and so forth.
4168280	4171080	The challenge of this decade or century
4171080	4173760	is how do we solve that path dependency problem
4173760	4176000	and how do we get to Estonia?
4176000	4177280	It used to be get to Denmark.
4177280	4179760	Now let's get to Estonia
4179760	4183800	and find that sort of that pathway up mountain probable.
4183800	4187880	Great, let's get to your wonderful series of blog posts
4187880	4189720	on AI and Leviathan.
4189720	4192640	In this context, what do we mean by Leviathan?
4192640	4193600	Well, it's all interrelates.
4193600	4196560	So Leviathan was the book Hobbes,
4196560	4200320	Thomas Hobbes wrote at the start of the interregnum
4200320	4202040	after the English Civil War.
4202040	4206160	And it was basically his early political science,
4206160	4208560	early defense of absolutist monarchy
4208560	4212400	as a way to restore peace and order
4212400	4216040	after a decade of infighting.
4217000	4222000	And Hobbes kind of hit on some basic sort of structural
4222000	4226720	game theoretic properties of why we have governments at all.
4226720	4229080	He talked about life being nasty British and short
4229080	4231720	in the state of nature, war of all against all.
4231720	4233840	And peace is only restored
4233880	4236960	when people who don't trust each other
4236960	4240720	offload enforcement and policing responsibilities
4240720	4244480	to a higher power that can then restore
4244480	4246200	a degree of peace and order.
4246200	4249280	AI and Leviathan is talking about,
4249280	4251520	how does AI change the story?
4251520	4253840	Does it reinforce the Leviathan?
4253840	4257100	Does it lead to a digital police state China?
4257100	4260680	Or is it something that we impose on ourselves?
4260680	4262840	And we talked about how multimodal models
4262840	4265560	could in principle be used to put a camera
4265560	4267560	in everyone's house and have it just continuously monitoring
4267560	4269280	for people doing any kind of crime.
4269280	4271680	That's something that North Korea might do.
4271680	4275040	In the US context, it's something that we're very liable
4275040	4277760	to just voluntarily do to ourselves
4277760	4280000	because we want to have ring cameras
4280000	4283600	and Alexa assistants and so forth.
4283600	4286440	And so that leads to a kind of bottom up Leviathan
4286440	4289360	that is potentially no less oppressive
4289360	4290440	and maybe even more oppressive
4290440	4294480	because there's no one that we can appeal to
4294480	4296400	to change the rules.
4296400	4299000	Yeah, so Leviathan is one way to respond
4299000	4302200	to technological change, but you mentioned two other ways
4302200	4304800	we could alternatively respond.
4304800	4307040	Right, so basically any time a technology
4307040	4309320	greatly empowers the individual,
4309320	4313400	it creates a potential negative externality, right?
4313400	4316000	Hobbes called these our natural liberties.
4316000	4317640	In the state of nature, I have a natural liberty
4317640	4320440	to kill you or to strong arm you.
4320440	4325440	And governments exist to revoke those natural liberties, right?
4325480	4328120	But for a higher form of freedom, right?
4328120	4331240	And so there are sort of any time a technology greatly
4331240	4332760	increases human capabilities,
4332760	4334440	these are the other humans.
4334440	4336760	The three canonical ways we can adjust are,
4336760	4339520	you know, ceding more authority to that higher power,
4339520	4341400	the Leviathan option.
4341400	4343200	And then the other two options are,
4343200	4345240	you know, adaptation and mitigation
4345240	4347240	and normative evolution.
4347240	4349560	So the example I give is, you know,
4349560	4351680	if suddenly we all had X-ray glasses
4351680	4353840	and you could see through walls and see through clothing.
4353840	4357280	You know, one option, we have a draconian,
4357280	4360480	totalitarian crackdown that tries to seize
4360480	4362280	all those X-ray glasses.
4362280	4365920	Another option is we adjust normatively, culturally,
4365920	4369040	that we, our privacy norms wither away
4369040	4371440	and we stop caring about nudity.
4372480	4374720	And then the other option is adaptation and mitigation
4374720	4377240	where we put in, you know, mesh into our walls
4377240	4380920	and where we're leaded shirts and pants.
4382880	4386120	Yeah, I guess continuing that analogy a bit
4386120	4390000	between the smart glasses and AI,
4390000	4393400	you have this amazing write-up of ways
4393400	4398280	in which AI can increase the informational resolution
4398280	4399520	of the universe.
4399520	4401440	So you give some examples that are,
4401440	4404920	I think specifically of AI identifying people
4404920	4406560	by gate, for example.
4406560	4408480	Right, so gate recognition is nothing new.
4408480	4411760	China has had advanced forms of gate recognition
4411760	4413320	for a while now.
4413320	4415560	So, you know, even if you cover your face,
4415560	4417880	it turns out we're constantly throwing off
4417880	4421640	sort of ambient information about ourselves, about everything.
4421640	4426520	And the way you walk, the particular gate that you have
4426520	4428720	is a unique identifier.
4428720	4431600	Another example is galaxy surveys.
4431600	4434400	There's, we've had from Hubble telescope to now,
4434400	4439400	the JWST, tons of astronomical surveys
4439720	4442000	of distant galaxies and so forth.
4442000	4445000	And all of a sudden, all that old data,
4445000	4448880	it's like that same data set is now more useful
4448880	4451480	because applying more modern deep learning techniques,
4451480	4455000	we can extract entropy that was in that data set,
4455000	4456920	but we didn't have the tools to extract yet
4457120	4460120	and discover that, you know, there are new galaxies
4460120	4462920	or other phenomena that we missed.
4462920	4467720	Another example you give is listening for keystrokes
4467720	4469560	on a keyboard and extracting information
4469560	4473080	about a password being typed in, for example,
4473080	4475960	which is something that of course humans can do,
4475960	4478680	but we can do with AI models.
4478680	4481880	Yeah, so that was a paper showing that
4481880	4484880	you can reconstruct keystrokes from an audio recording,
4484880	4486160	including a Zoom conversation.
4486160	4487440	So I hope you haven't typed in your password
4487440	4489760	because people in the future, and so this goes to,
4489760	4491600	you know, the fact that it's sort of retroactive,
4491600	4494400	that like, even if the technology wasn't diffused yet,
4494400	4496240	any Zoom conversation, any recording
4496240	4498320	where someone typed their password in the future
4498320	4499960	will be like those galaxy surveys
4499960	4502120	where someone will go backwards in time
4502120	4504480	and, you know, turn up the information resolution
4504480	4505640	of that data.
4505640	4507440	Yeah, this is pure speculation,
4507440	4511080	but I wonder if, I mean, imagine anonymized people
4511080	4513400	in interviews, say 10 years ago,
4513400	4515360	whether they will be able to stay anonymous
4515400	4518160	or whether AI will be able to extract the data
4518160	4520240	about their face or their voice
4520240	4524680	that wasn't technically possible when the interview aired.
4524680	4527600	Yeah, exactly, there are already systems
4527600	4531280	for like, depixelating, you probably do something similar
4531280	4534400	for the voice modulation, and then also sort of, you know,
4534400	4537520	again, going back to like, this ambient information
4537520	4541920	we're always shedding, identifiers in the way we write,
4541920	4544440	you know, the kind, where we place a comma,
4544440	4547440	the kinds of adverbs we like to use and so forth.
4547440	4549280	People just dramatically underrate, you know,
4549280	4550840	how much information we're shedding,
4550840	4552680	in part because we're blind to it.
4552680	4555000	Some people who are taking great efforts
4555000	4559560	to stay anonymous online, people in the cryptography space,
4559560	4562040	for example, will put their writings
4562040	4563800	through Google Translate to French
4563800	4567320	and then back to English to erase subtle clues
4567320	4571240	to how they, that could identify them personally.
4571240	4573920	Why is AI so much better at tasks
4573920	4577160	like the ones we just mentioned, compared to humans?
4577160	4579760	Well, it goes back to what we were talking about
4579760	4582920	with sort of putting information theoretic bounds on AGI.
4582920	4585040	When you minimize the loss function
4585040	4585880	in a machine learning model,
4585880	4588800	you're trying to minimize the cross entropy loss.
4588800	4591120	The cross entropy is, how many bits does it take
4591120	4593880	to distinguish between two data streams?
4593880	4596160	And if it takes a lot of bits to distinguish between the two,
4596160	4598720	that means they're relatively indistinguishable.
4598720	4600240	So that's going, again, to the Turing test.
4600240	4603040	Like, if we have a Turing test where I can tell right away
4603080	4605680	that the AI is different than the human,
4605680	4607640	that suggests a high cross entropy.
4607640	4610440	But if I could talk to it for days
4610440	4612840	and do all kinds of adversarial questioning,
4612840	4614200	I might still be able to, in the end,
4614200	4615360	tell the difference between the two,
4615360	4618200	but we've minimized that cross entropy loss.
4618200	4622040	And so when you have any arbitrary data distribution
4622040	4623600	that you're trying to predict,
4623600	4627000	whether it's trying to predict galaxies
4627000	4629320	and astronomical data or passwords
4629320	4633240	from fingerprint data on a phone screen,
4633240	4637520	all these things embed a kind of physical memory
4637520	4639280	of the thing in question
4639280	4640760	and can often be reconstructed
4640760	4643760	through this kind of loss minimization,
4643760	4646800	where you have a system that asymptotically
4646800	4650280	extracts the entropy that was latent in the data.
4650280	4651320	And this can be done in a way
4651320	4653760	that is often quite striking,
4653760	4657840	where we can, with stable diffusion,
4657840	4659080	make fairly accurate predictions
4659080	4662320	of what people are imagining in their mind
4662320	4664240	using fMRI data.
4664240	4666600	And fMRI data is like blood flow data in the brain.
4666600	4668640	It's a very lossy representation
4668640	4670520	of what ever's happening in the brain.
4670520	4673200	But there's still enough latent entropy in there
4673200	4674680	that we can kind of reverse engineer
4674680	4679000	or decompress it into a folder picture.
4679000	4682080	And this could turn into a form of lie detection.
4682080	4685560	Yeah, I think it already basically has.
4686360	4692160	If you have fMRI data or EEGs
4692160	4693480	or other kinds of like direct brain data,
4693480	4696040	it's probably a lot easier,
4696040	4700000	but we already have systems that are over 95% accurate
4700000	4704800	at detecting deception from just visual video recordings.
4704800	4707480	We can see how all of this information
4707480	4709200	that we are continually shedding
4709200	4712800	gives rise to the possibility of alibiath
4712800	4716800	than either of the private or of the government's kind.
4716800	4719160	I wonder what role do you see
4719160	4721600	open-sourcing AI models playing here?
4721600	4726600	What are the trade-offs and risks in open-sourcing AI?
4726600	4729040	Among the people who are most bullish to open-source,
4729040	4734200	there's often a kind of libertarian ethic undergirding it.
4734200	4738520	Regardless of whether that's a good idea or not,
4738520	4741360	one of the things I'm trying to communicate to that group
4741360	4744320	is to say that be careful what you wish for
4744320	4748520	because of these kind of paradoxical Hobbesian dynamics.
4748520	4749880	The fact that in America,
4749880	4752520	you never know if someone has a gun or not.
4752520	4755320	On the one hand, the Second Amendment enhances our freedom.
4755320	4757840	On another hand, you don't get the sort of
4757840	4761000	like everyone's doors unlocked and people are,
4761000	4764440	like the police in England don't even have guns.
4764440	4766120	There's a certain freedom that derives
4766120	4770160	from us not all being heavily armed.
4772120	4778920	Likewise, with open-sourcing powerful AI capabilities,
4778920	4781480	it empowers you as an individual,
4781480	4783400	but in general equilibrium,
4783400	4785080	once we all have the capabilities,
4785080	4787800	the world could look much more oppressive
4787800	4790240	either because we're all spying on each other all the time
4790240	4791720	and we can all see through each other's walls
4791720	4793960	or because there's a backlash and the introduction
4793960	4796320	of the viathan type solutions
4796320	4799240	to restrict our ability to spy on each other all the time.
4799920	4803480	My general sense is that we can only delay
4803480	4805320	and we can't really prevent things
4805320	4806760	from being open-source over the long run
4806760	4808960	because there's a sort of trickle-down
4808960	4810560	of compute requirements.
4810560	4812360	But in the interim,
4812360	4815040	there are definitely things that are valuable to open-source,
4815040	4819360	having 70 billion parameter language models, not a threat.
4819360	4822480	In fact, I think it's probably useful for alignment research
4822480	4825080	for something like that to be open-source.
4825080	4826600	But if you are a researcher
4826600	4830160	and you've developed a emotional recognition model
4830160	4834880	that can tell if, you know, with like 99% accuracy,
4834880	4838240	whether someone is lying or not lying
4838240	4840720	and whether your girlfriend loves you or not,
4840720	4844760	like these things or the ability to see through walls
4844760	4849760	using, like I talk about the use of Wi-Fi displacement.
4849760	4851760	There are people who have built pose recognition models
4851760	4855480	using the displacement of the electromagnetic frequency
4855480	4857600	of your Wi-Fi and they can see,
4857600	4860520	they can, there's wall penetrating
4860520	4861560	so that you can see through walls.
4861560	4866560	Like, what's the rush to put that on hugging face
4867400	4871200	and to like make it as democratized as quickly as possible?
4871200	4875480	I would say that if we value the adaptation
4875480	4880320	and mitigation pathway as opposed to the Leviathan pathway,
4880320	4882800	then there's a value in, you know,
4882800	4885040	slow rolling some of these things.
4885080	4888360	How do you think government power will be,
4888360	4892080	or relative government power will be affected by AI?
4892080	4895840	So you write somewhere in this long series of blog posts
4895840	4899320	that AI will cause a net weakening of governments
4899320	4901360	relative to the private sector.
4901360	4902800	Why is that?
4902800	4906040	Yeah, specifically Western liberal governments
4906040	4908040	under constitutional constraints.
4909040	4912880	So if you imagine society being on this kind of knife edge,
4913880	4915960	I talked about this in the context of
4915960	4918440	Theranosso Mogul's book, The Neural Corridor,
4918440	4919760	where he describes liberal democracy
4919760	4921120	as sort of being in this corridor
4921120	4923040	between despotism on the one hand
4923040	4924480	and anarchy on the other.
4924480	4928240	And we sort of this day in this saddle path
4928240	4931480	where society and the state are kept in balance.
4931480	4934360	If you veer off that path, you can, on the one hand,
4934360	4936320	you know, the state could become all powerful
4936320	4939120	and that's the sort of China model
4939120	4941080	or authoritarian digital surveillance state.
4941080	4943720	And indeed, you know, China built up
4943720	4944920	their digital surveillance state
4944920	4947320	and their internet firewalls and so forth
4947320	4949440	after watching the Arab Spring
4949440	4951760	and seeing how the internet was destabilizing
4951760	4953320	to weaker governments.
4953320	4956720	And so I fully expect that AI will be very empowering
4956720	4959440	and self-reinforcing of the power of the Chinese government.
4959440	4961960	And indeed, there are draft regulations
4961960	4964680	for large language models stipulate
4964680	4965520	that you can't use the model
4965520	4968760	to undermine national unity or challenge the government.
4968760	4970200	And so they're baking that in.
4970200	4973520	In liberal democracies, we think of ourselves
4973520	4974640	as open societies.
4975560	4979640	And the issue is that we're only open at the meta level.
4979640	4981040	There's a public sphere, right?
4981040	4982520	There's freedom of information laws.
4982520	4984640	We have freedom of speech.
4984640	4987800	I don't have freedom of speech if I walk into a Walmart.
4987800	4988640	Wait, right?
4988640	4990080	The Walmart is private property.
4990080	4992240	In open societies, it's not that we don't have
4992240	4995880	social credit scores and forms of,
4995880	4997360	thicker forms of social regulation.
4997360	4999520	It's just that we offload those functions
4999520	5002080	onto competing private actors,
5002080	5006520	whether it's a church that has very strict doctrines
5006520	5010080	to be a member or other kinds of social clubs.
5010080	5011400	The fact that these days,
5011400	5013600	if you want to go to a comedy club,
5013600	5015400	they'll often confiscate your phone at the door
5015400	5017000	because they don't want you recording
5017000	5019760	the comedian's set and putting it online.
5019760	5022600	My anticipation is that because of those constitutional
5022600	5027360	constraints that limit the ability of liberal democracies
5027400	5030520	to go the China route, right?
5030520	5034080	Because of our civil laws or bills of rights and so forth.
5034080	5036360	And also because of a lot of these procedural constraints.
5036360	5039200	This will naturally shift into the private sector.
5039200	5043360	And we see that already with the use of AI
5043360	5045320	for monitoring and employment,
5045320	5049320	for policing speech in ways that would be illegal
5049320	5050160	if done by the state,
5050160	5052580	but are fine if done by Facebook.
5052580	5054720	To the extent that the AI continues
5054720	5056400	to increase these kind of negative externalities
5056640	5058720	and therefore puts more value
5058720	5061880	on having a sort of vertically integrated experience,
5061880	5066440	a walled garden that can strip out the negative forms of AI
5066440	5069880	and reinstate the degree of harmony between people
5070920	5075000	that more and more of our social life will be mediated
5075000	5078280	through these sort of private organizations
5078280	5081480	rather than through a kind of open public sphere.
5081480	5085200	Or you're imagining that government services
5085200	5088680	will be gradually replaced by private services
5088680	5091920	that are better able to respond.
5091920	5095480	Won't governments fight to uphold individual rights?
5095480	5097840	In Walmart or on Facebook,
5097840	5099760	you are regulated in ways
5099760	5101120	that the government couldn't regulate you,
5101120	5104200	but you still have the choice to go to a target
5104200	5108180	instead of Walmart or to go to or X instead of Facebook.
5108180	5109440	Isn't that the fundamental thing?
5109440	5111760	So the fundamental thing is the choice between services
5111760	5115440	and won't governments uphold citizens' rights
5115440	5118360	to make those kinds of choices?
5118360	5119760	Yeah, no, I agree.
5119760	5124760	And so this would be the defense of the liberal model
5124880	5128720	is that we allow thicker forms of social regulation
5128720	5131880	because it's moderated by choice and competition.
5131880	5135560	And the issue with Chinese Confucian integralism
5136400	5141400	isn't the fact that it's super oppressive.
5141920	5144240	It's the fact that you only have one choice
5144240	5146360	and you don't have voice or exit.
5146360	5151360	So, but it's obviously a matter of degree, right?
5152880	5156240	When ride hailing first arose,
5156240	5159960	I remember back in 2013, 2014, it wasn't that long ago.
5159960	5163240	I think Uber was founded in 2009,
5163240	5167000	but it really only started taking off in the early 2010s.
5167000	5170080	No, people thought it was crazy to ride a car
5170080	5171440	with a stranger.
5171440	5173120	And then within five years,
5173120	5177440	it was the dominant mode of ride hailing.
5177440	5179240	And in that five-year period,
5179240	5182680	essentially we saw a kind of regime change in micro
5182680	5186560	where taxis went from being something
5186560	5187760	that was regulated by the state
5187760	5191040	through these commissions that were granted,
5191160	5195880	legal monopolies and used licensing and exams
5195880	5198840	and other sort of brute force ways of ensuring quality
5199800	5201360	to competing private platforms
5201360	5204240	where you have Lyft or Uber to choose from.
5204240	5208000	And they replaced the explicit governance of legal mandates
5208000	5212640	with the competing governance of reputation mechanisms
5212640	5217640	of dispute resolution systems of structured marketplaces
5218080	5220880	that collapse the bargaining frictions, right?
5220880	5222600	You never have to haggle with an Uber driver,
5222600	5223960	you just sort of get in.
5223960	5226560	And that was obviously a much better way
5226560	5229520	of doing ride hailing.
5229520	5232160	So even though there was sort of a violent resistance early on,
5232160	5233080	literally like in France,
5233080	5234480	they were throwing rocks off of bridges
5234480	5237360	and cab drivers in New York were killing themselves.
5237360	5238440	So for the people affected,
5238440	5240400	it was a very dramatic sort of regime change,
5240400	5243520	but for everyone else, it was a huge positive improvement.
5243520	5245600	And yet it's only made possible
5245600	5248240	because Uber has a social credit score.
5248240	5249720	If your Uber rating goes too low,
5249720	5251680	you'll get kicked off the platform.
5251680	5254440	And so we're fine with social credit scores.
5254440	5258280	It's when you only have one and don't have an option
5258280	5261120	and it can follow you across all these different verticals
5261120	5262200	that becomes a problem.
5262200	5266240	Do you imagine that because of rising danger in the world,
5266240	5268320	you talk about the externalities
5268320	5273320	from the widespread implementation of AI all across society
5274320	5276920	because of those dangers, those externalities,
5276920	5279960	you know, you will either use Uber or whatever service
5279960	5283200	or you kind of can't participate in society.
5283200	5286400	Do you imagine increased pressure in that direction?
5286400	5288640	It does seem to be a longer-term trend.
5288640	5291600	I don't know if AI will accelerate it.
5291600	5296840	I have another series of essays that I call separation anxiety.
5297840	5301880	And it's a reference to the fact that in insurance markets,
5302840	5304320	there's kind of two equilibria.
5304320	5305720	There's the pooling equilibria
5305720	5308160	where we're pooled together into one risk pool
5308160	5309680	and there's a separating equilibria
5309680	5311800	where the insurance pool unravels
5311800	5315000	and we break up into the great power insurance
5315000	5317840	for senior citizens who'd never had an accident
5317840	5319040	and stuff like that.
5319040	5320320	And it turns out that insurance markets
5320320	5321720	are competitively unstable
5321720	5326640	that without government regulation or social insurance,
5326640	5330080	that insurance markets will naturally tend to unravel
5330080	5331480	because of adverse selection
5331480	5334920	into, you know, the high-risk people being in one pool
5334920	5337360	and the low-risk people being in another pool.
5337360	5339360	And it turns out you can sort of use that as a mental model
5339360	5343800	to look at other kinds of implicit pooling equilibria, right?
5343800	5348800	So within company wage distributions,
5349160	5351960	often there is, you know, 20% of the workers
5351960	5353760	who are doing 80% of the work,
5353760	5357280	but they're pooled together under one wage structure.
5357280	5359400	And that was sort of the dominant structure
5360000	5362400	of the period of wage compression
5362400	5364760	in the United States in the 50s and 60s.
5364760	5366680	And once we had better monitoring technologies
5366680	5368560	and were able to tell who were the 20%
5368560	5370000	that were doing 80% of the work,
5370000	5375000	it suddenly became possible to differentiate pay structure
5375320	5378240	and a lot of the rise and inequality in the United States
5378240	5380800	is actually between firm.
5380800	5382080	So what happens is, you know,
5382080	5384200	Ezra Klein is like the most productive wiz kid
5384200	5385600	at the Washington Post and he realizes,
5385600	5388640	why don't I just go start my own website, right?
5388640	5390400	And so that dynamics are played out
5390400	5392960	across a variety of domains,
5392960	5395160	leads to a world that, you know,
5395160	5397760	to the extent that these features are correlated,
5397760	5399480	that does separate, right?
5399480	5402640	Where you have, you know, the one-star Uber riders
5402640	5404800	driving the one-star Uber drivers,
5404800	5406640	the drivers driving the riders.
5406640	5408760	And, you know, people who have, you know,
5408760	5411760	the five-star Uber ratings and the perfect credit scores,
5411760	5413880	self-sort into communities with other people
5413880	5415840	with perfect driving records and perfect credit scores.
5416080	5418600	And, you know, we see that to an extent already
5418600	5421560	with the, you know, enclaves of, you know,
5421560	5423800	rich zip codes with private schools
5423800	5426320	and everyone is sort of self-selected.
5426320	5429920	AI could, it seems to me that AI would exacerbate that.
5429920	5431860	I mean, at first blush, just because it,
5431860	5433640	going back to the point about signal extraction,
5433640	5436080	it can find all these different ways,
5436080	5438800	you're a high-risk type and I'm a low-risk type and so forth,
5438800	5442200	that are probably latent in all kinds of data
5442200	5443480	that we don't even need to get permission
5443480	5444320	to the insurance company.
5444320	5446360	They'll just like, the same way that they use,
5446360	5449800	like smoking or going to a gym as a proxy,
5449800	5451200	there's all kinds of proxies they could use
5451200	5454140	and likewise for employers and how they pay people.
5454140	5459080	Society kind of runs on us not being entirely open
5459080	5460800	and entirely honest all the time.
5460800	5463320	Otherwise, you wouldn't be able to have
5463320	5466200	kind of smooth social interactions and so on.
5466200	5469920	Won't these norms be inherited by the way we use AI?
5469920	5473000	Yeah, I think this is a really big issue.
5473000	5474560	And I'm a big fan of Robin Hansen
5474560	5479560	and a lot of his writing on social status and signaling
5480160	5484600	is sort of presenting humans as basically hypocrites,
5484600	5488680	like we're constantly deceiving other people
5488680	5491060	and we often deceive ourselves
5491060	5493360	so it's better to deceive others
5493360	5498160	as the evolutionary biologist Robert Trivers
5498160	5499000	has pointed out.
5500000	5505000	So, all the kinds of polite lies that we tell
5505800	5509280	are I think critical lubricants to the social interaction
5509280	5513800	and actually like it's good that there's a gap
5513800	5515880	between our stated and revealed preference.
5515880	5517680	I think a world where we all lived
5517680	5520240	our stated preference could be hellish
5520240	5521880	because we don't actually mean it.
5524400	5526760	And AI has a direct implication on that
5526800	5529120	because if I can have a pair of AR glasses on
5529120	5530920	that will tell me if you're interested,
5530920	5533680	if you're bored, if you're over on a date
5533680	5536720	and are you really attracted to me,
5537800	5542160	all that sort of polite veneer that social veil
5542160	5544720	could be lifted in a way that
5544720	5548520	we'll probably want to coordinate to not do, right?
5548520	5550240	But again, it's this Nash equilibrium
5550240	5551760	where it's in my interest to know
5551760	5554400	whether you're interested or bored.
5554400	5556120	And so I'll wanna have the glasses on
5556120	5559000	and my ideal world is where only I have the glasses
5559000	5559840	and you don't.
5561320	5564560	And the other way that our hypocrisy is being exposed
5564560	5568400	and challenged is the need to explicate
5568400	5570280	the utility function that we want these models
5570280	5571440	to work under.
5572720	5575360	We need to formalize human values
5575360	5577160	if we want to align these models.
5577160	5579600	And so then we have to be honest and open
5579600	5582400	about the fact that our stated preferences
5582400	5585040	probably aren't our true preferences.
5585040	5587320	And that's a very challenging thing
5587320	5591600	because it cuts right to the nature of the human condition
5591600	5595080	and involves topics that are intrinsically things
5595080	5597320	that we lie to ourselves about.
5597320	5599960	You have what you call a timeline
5599960	5602880	of a techno feudalist future,
5602880	5604640	which I found quite interesting.
5604640	5607440	Yeah, it's great writing and it's very detailed.
5607440	5609760	We don't have to go through it in all of its detail,
5609760	5612200	but maybe you could tell the story of what happens
5612200	5614280	in what you call the default scenario.
5614280	5617440	This is the scenario in which Western liberal democracies
5617440	5619320	are too slow to adapt to AI.
5619320	5621400	And so we get something like a replacement
5621400	5624520	of government services with more private services.
5624520	5628000	What happens in the techno feudalist future?
5628000	5629960	Right, and it's sort of piggybacks
5629960	5631800	in everything you've just been discussing, right?
5631800	5633960	And I don't want techno feudalists
5633960	5636160	to carry too much of a pejorative.
5636160	5637880	I'm sort of using it descriptively.
5639000	5643680	And certainly some people would prefer this world
5644320	5647880	so the example of Uber and Lyft displacing taxi caps
5647880	5650400	is sort of a version of this in micro
5650400	5653960	where we go from this regulated taxi commission
5653960	5657000	to competing private platforms that use various forms
5657000	5661240	of artificial intelligence and information technology
5661240	5663560	to replace the thing that was being done
5663560	5665080	by explicit regulation.
5666040	5670760	And as AI progresses and both creates a variety
5670760	5672000	of new negative externalities,
5672040	5674880	whether it's like suicide drones
5677200	5679400	or the ability to spy on each other,
5680480	5683600	there's going to be a demand for new forms of security
5683600	5687880	and also kinds of opt-in jurisdictions
5687880	5689400	that tie our hands in the same way
5689400	5694000	that we give up our phone before we go into the comedy club.
5694000	5697360	And so I think this leads to a kind of development
5697360	5701480	of clubs, the kind of club structure
5701480	5703120	that may be at the city level
5703120	5706760	as the vertically integrated walled garden
5706760	5711280	that will police and build defensive technologies
5711280	5714320	around the misuse of AI and at the same time
5714320	5718200	provide a variety of like new AI native public goods
5718200	5721480	that are only possible once AI unlocks them.
5721480	5724600	And it's easy to see how this could very quickly displace
5725520	5729360	and eat away at formal government services
5729360	5731640	both because we saw it already with Uber,
5731640	5734560	but also if you map that model
5734560	5737120	to other areas of regulatory life,
5738680	5742040	does it make sense to have a USDA farm inspector,
5742040	5744240	a human person has to go to a commercial farm
5744240	5747160	and maybe only goes to that farm once every few years
5747160	5749800	because there's so many farms and only so many people.
5749800	5751320	And it does a little checklist and says,
5751320	5753320	oh, you're not abusing the animals
5753320	5755360	and you get all the process in place
5755360	5758080	and you get the USDA stamp of approval
5758080	5762960	or does it make more sense to have multimodal cameras on
5762960	5767960	in the farm 24-7 that are continuously generating reports
5768680	5770960	that throw up a red flag anytime someone sneezes
5770960	5772080	on the conveyor belt.
5772080	5775920	And to the extent that government is going to be slow
5775920	5779440	at adopting that, will there be a push
5779440	5783800	for the kind of Uber model of governance as a platform
5783800	5787320	where you have the kind of AI underwriter,
5787320	5791920	the consumer reports that sells these farms,
5791920	5793800	the camera technology and the monitoring technology
5793800	5796920	and builds their own set of compliant standards.
5796920	5798720	And then you want to go to those farms
5798720	5800880	or would have you that have the stamp of approval
5800880	5803600	of the underwriter because it's much higher trust.
5803600	5807160	It's sort of like the end of asymmetric information.
5807160	5812160	And you can map that from food safety to product safety
5813400	5816720	to OSHA and workplace safety.
5816720	5817960	There's other parts of government
5817960	5820600	that maybe just rendered completely obsolete, right?
5820600	5822000	Like once we have self-driving cars
5822000	5824760	that are a thousand next more safe than humans,
5824760	5827560	do we need a national highway traffic safety administration?
5829160	5832160	Once we have sensors that are privately owned everywhere
5832160	5834640	and can model weather patterns better
5834640	5837280	than the national oceanic administration,
5837280	5839320	do we need a national weather service
5839320	5841440	or could we bootstrap that ourselves?
5841440	5844880	And then once we have AI,
5844880	5847080	accelerated drug discovery,
5847080	5850960	do we want to rely on the FDA to be a kind of choke point
5850960	5854680	to do these sort of frequentist clinical trials
5854680	5858560	that are inherently slow and don't capture
5858560	5863400	the kind of idiosyncrasies and heterogeneity
5863400	5866440	that could be unlocked by personalized medicine?
5866440	5871040	Or do we move to an alternative drug approval process
5871040	5872280	that is maybe non-governmental,
5872280	5875440	but much more rapid and much more personalized?
5875440	5876880	So that's the overall picture.
5876880	5879960	I'll just run through the timeline here,
5879960	5882160	picking up on some of your comments
5882160	5884880	that I thought were especially interesting.
5884880	5888760	You write, this is in 2024 to 2027,
5888760	5893280	you write that the internet will become vulcanized
5893280	5898280	and it will become more secure and more private in a sense.
5898600	5900040	Why does that happen?
5900040	5903400	We're already starting to see this a little bit, right?
5903400	5906040	Once people realize that the data
5906040	5907840	that's being generated on Stack Overflow
5907840	5910400	or Reddit or whatever is valuable
5910400	5911440	for training these models,
5911440	5913240	suddenly everyone's closing their API
5914560	5917640	and consequently Google Search and the Google index
5917640	5920720	have sort of started to degrade already.
5920720	5922240	So I think that will continue
5922240	5926120	for the kind of privatization of data reasons.
5926120	5927800	Then when you also think about
5927800	5932280	how websites are going to handle sort of the growth of bots
5932280	5935040	and catfishes and catfish attacks
5935040	5938240	and cyber attacks and so forth,
5938240	5939760	it makes sense that we're going to move
5939760	5944080	from a sort of open, everything goes kind of Twitter-esque
5944080	5947640	platform to things that are much more closed
5947640	5951920	because they require human verification
5951920	5955560	and identity verification to sort of build the trust
5955600	5958560	that you're talking to other people and not deepfakes.
5958560	5960600	And then medium-term, again,
5960600	5963240	over this sort of 2024 to 2027 horizon,
5964200	5967160	you could also start to see the emergence
5967160	5972160	of intelligent malware, sort of modern AI native cyber attacks
5974240	5978320	that could be devastating to legacy cyber security
5978320	5981640	infrastructure in a way that I talk about good heart
5981640	5983240	and back to the famous Moore's worm
5983240	5985240	that in the late 80s,
5985240	5986720	basically shut down the early internet.
5986720	5988880	They literally had to partition the internet
5988880	5993080	and turn it off so they could read the network at the worm.
5993080	5994200	So for all those reasons,
5994200	5996120	I think you start to see the internet balkanize
5996120	5998160	and then particularly at the international level,
5998160	6002280	we're already starting to see sort of the semiconductor
6002280	6005320	supply chain become critical part of national security.
6005320	6007680	The growth of the Chinese firewall,
6007680	6010960	the European Union is going to have to have their own
6010960	6013160	quasi firewall and they kind of already do with GDPR
6013160	6015880	and the EU AI Act.
6015880	6017600	And so the kind of nationalization of compute
6017600	6019240	and telecommunications infrastructure
6019240	6022120	that will take off once people understand
6022120	6024480	both the security risks and the value prop
6024480	6028400	of owning the infrastructure for the AI revolution.
6028400	6031960	Yeah, in 2028 to 2031,
6031960	6035800	you write about alignment turning out to be easier
6035800	6039560	than we thought with the increasing scale of the model.
6039760	6041040	That was somewhat surprising to me.
6041040	6043520	Why does alignment turn out to be easier?
6044760	6047960	And part of this is imagining a scenario
6047960	6050040	where alignment is easy.
6050040	6053880	So we can talk about what happens if alignment is easy.
6053880	6055200	But I think there are reasons to think
6055200	6056880	that the classic alignment problem
6056880	6059360	will be easier than people think.
6059360	6062000	I think that some of the early intuitions
6062000	6064680	about the hardness of the alignment problem
6064680	6067520	were rooted in a view of maybe AI turns out
6067520	6070480	to be a very simple algorithm
6070480	6073480	rather than like a deep neural network
6073480	6076280	that achieves its generality because of its depth.
6076280	6079760	Clearly the kind of value,
6079760	6082320	I forget what Eliezer Kewski called it,
6082320	6085880	but there's like a value alignment problem
6085880	6089120	where how do we teach the model our values?
6089120	6092240	But that part of the alignment problem seems trivial now
6092240	6095240	because our large English models aren't like
6095240	6099800	autistic savants, they're actually incredibly sensitive
6099800	6104800	to soft human concepts of value and context.
6105640	6107480	They're not going to have a,
6107480	6109800	the paperclip maximizer sort of monkey paw
6109800	6113040	kind of threat models don't really make sense in that world.
6113040	6116640	But there's a difference between the output of the model
6116640	6120280	and the weights or what the model has learned.
6120280	6123400	And so just because a model can say,
6123440	6125400	it can say the right words that we wanted to say,
6125400	6127640	but what has it actually learned?
6127640	6129040	We are not entirely sure.
6129040	6132280	And so it has learned to satisfy human values to some extent,
6132280	6136560	but has it learned to want to comply with human value
6138320	6141600	out of distribution sort of, yeah, in other domains
6141600	6145120	and in a deep sense, I'm not sure about that.
6145120	6145960	No, I agree.
6145960	6147920	So I'm sort of just laying some of my groundwork
6147920	6150640	for to expand my priors on this.
6150640	6152440	I agree, like, you know, reinforcement learning
6152440	6155760	from human feedback is not alignment.
6157200	6158640	You know, the same way that, you know,
6158640	6161720	you could argue that like the co-evolution of cats and dogs
6161720	6164620	with humans led to a kind of reinforcement learning
6164620	6168320	from human feedback in their, in their short run evolution
6168320	6170840	that, you know, made them look up, appear as if they,
6170840	6174080	you know, they experienced guilt and shame
6174080	6175880	and these human emotions when in fact they're,
6175880	6178080	they're just sort of a simulacra of those emotions
6178080	6181320	because it means that we'll give them a treat.
6181320	6184720	But I've done plenty of episodes on deceptions
6184720	6186280	in these models and so on.
6186280	6187960	We don't have to go through that,
6187960	6189840	but I just wanted to point out that, yeah,
6189840	6191880	maybe there's some complexities there.
6191880	6194680	So the first, my first prior is that these models
6194680	6198680	aren't autistic savants the way they might have been.
6198680	6202240	The second is going back to universality.
6202240	6206720	Well, it is true that you, there are, that, you know,
6206720	6208920	it's possible through reinforcement learning
6208920	6210000	from human feedback, for example,
6210120	6212800	that you, you're, you're not selecting for honesty
6212800	6216280	or selecting for a deep pick up honesty,
6216280	6218760	but in the bigger picture, the intuition
6218760	6220960	that these models are converging or convergent
6220960	6224960	with human representations should give you some confidence
6224960	6226680	that they're not going to be as alien
6226680	6228600	as we, as we think they will be.
6228600	6233600	It's also useful input for thinking about interpretability.
6233880	6236880	You know, some recent work showing that discussing
6236880	6238520	sort of representation, interpretability
6238520	6241320	where, where instead of trying to interpret
6241320	6243560	individual neurons, you interpret sort of collections
6243560	6246480	of neurons and, and, and circuitry
6246480	6249680	through sort of human interpretable representations.
6249680	6251400	And one of the, one of the lessons of universality
6251400	6253640	is that like some of these high level human concepts,
6253640	6257400	like happiness or, or anxiety,
6257400	6262280	like these seem like vague psychological abstractions
6262280	6264240	that there's no way they can correspond
6264240	6266800	to like the micro foundations of the way our brain works.
6266800	6269640	But in fact, they may actually be very efficient,
6269640	6271400	low dimensional ways of talking about
6271400	6272720	what's happening in our brain.
6272720	6274760	And then the third thing is, I think that I just
6274760	6277440	have seen, you know, my sense is that the work
6277440	6280480	on interpretability is actually making some,
6280480	6282360	some good progress, you know,
6282360	6283920	whether it can scale is another question,
6283920	6286360	but I think we'll get there.
6286360	6289760	In my timeline, I talk about sort of AGI level models
6289760	6293240	within the human limit, human emulator plus domain.
6293240	6295280	I do later on talk about like super intelligence
6295280	6297360	emerging maybe in the 2040s.
6297360	6298440	And that's another story, right?
6298440	6301440	And so I think some of this stuff maybe goes out the window
6301440	6303360	if we have, you know, models that are bigger
6303360	6306080	than all the brains combined and have like
6306080	6308240	strong situational awareness.
6308240	6311000	But I don't think that happens this decade.
6311000	6314360	Certainly, certainly not with the current way
6314360	6315200	we're building these models,
6315200	6316560	with the way we're currently building these models,
6316560	6318720	I think it's comes much closer to a stimuli
6318720	6320200	lack of the human brain.
6320200	6321040	Got it.
6321040	6326040	In 2036 to 2039, you talk about robotics
6327480	6329240	being solved to the same extent,
6329240	6330960	or maybe even in the same way
6330960	6333720	as we are now solving a language.
6333720	6335640	That would, I found that super interesting.
6335640	6339600	Explain to me why, why would robotics
6339600	6343680	suddenly or quite relatively suddenly become much easier?
6343680	6345840	Robotics have been fighting for decades
6345840	6349640	to get these models to walk relatively
6350640	6354400	unencumbered and it's been an uphill battle.
6354400	6357880	Yeah, why can we solve robotics in the 2030s?
6357880	6360720	This may end up happening sooner than I'd project,
6360720	6363760	but I mean, if you look at LLMs,
6363760	6366320	what one of the stylized sort of trends
6366320	6369080	with large language models is that, you know,
6369080	6373240	that natural language processing went from being this,
6373240	6375160	you know, the study of how to make machines
6375160	6377560	understand language went from being, you know,
6377640	6379520	a dozen different sub-disciplines,
6379520	6380840	you know, people working on parsing,
6380840	6382000	people are working on syntax,
6382000	6382960	people are working on semantics,
6382960	6386360	people are working on summarization and classification.
6386360	6388480	And these are all different, you know, directions,
6388480	6389560	research directions.
6389560	6391680	And then along comes transformer models
6391680	6394280	and, you know, it's just supplants everything
6394280	6396360	and LLMs can do it all.
6396360	6400520	And I think robotics is sort of still in that ancient regime
6400520	6403880	where a lot of, you know, what Boston Dynamics does
6403880	6406080	is ad hoc control models,
6406080	6409960	analytically solvable, you know, differential equations,
6409960	6412680	different kinds of object recognition modules
6412680	6416280	and control action loops and so forth.
6416280	6418920	And so it's still in that like early NLP phase
6418920	6421080	where they have 12 different sub-disciplines
6421080	6422880	and they're sort of mashing them together.
6422880	6425200	And of course you get something that's not very robust.
6425200	6428920	I think we're already starting to see that paradigm shift
6428920	6432440	to, you know, end-to-end neural network trained models,
6432480	6435600	like, you know, Tesla, for instance,
6436680	6439840	I think one of the reasons why Tesla cars
6439840	6443960	had a sort of temporary decline in performance
6443960	6445840	was because they were undergoing the transition
6445840	6448080	from these ad hoc lane detectors
6448080	6450640	and stop sign detectors and stuff like that
6450640	6454920	to a fully end-to-end neural network transformer-based model.
6454920	6457880	And that turned out to be much more robust way
6457880	6460240	to train the model because, you know,
6460240	6461760	stop signs look different in different countries
6461760	6463760	and like maybe stop sign isn't the thing you care about,
6463760	6465560	really, so on and so forth.
6466920	6469160	And so I think the transformer sort of scale,
6469160	6473600	deep learning revolution is only now coming to robotics
6473600	6476920	and people in that field have, are a little bit cynical
6476920	6481520	because they're used to relatively small RL models
6481520	6485040	thinking that like the fit with, you know, actuators
6485040	6488160	and some of the hardware is like a really challenging problem
6488160	6491440	and also believing that we don't have the data sets
6491440	6494920	for it, but then you look at, you know,
6494920	6498560	there's recent RoboDog that you may have seen on Twitter,
6498560	6501160	fully open source robot model
6501160	6503600	for a Boston Dynamics style dog.
6503600	6506960	It was trained on H100s, you know,
6506960	6511200	10,000 human years of training and simulation
6511200	6514400	and then some fine-tuning on real-world data
6514400	6519000	and they have a very robust robot control model
6519000	6520600	that you could plug into all kinds
6520600	6523720	of different form factors and have something that can,
6523720	6525840	you know, hop gaps and climb stairs
6525840	6529640	and do all the things that Boston Dynamics robots
6529640	6532040	don't do very well outside of their distribution.
6532040	6534200	You think we'll have a general purpose algorithm
6534200	6538280	that we can plug into basically arbitrarily shaped robots
6538280	6541720	that can then navigate the, navigate our apartments
6541720	6545040	or our construction sites or maybe our highways.
6545040	6548200	That's an interesting vision.
6548200	6551800	Why is it that we achieve this level of generality?
6551800	6554920	If you look at humans, you know, humans are very good at,
6554920	6556840	you know, if we've suffered an amputation
6556840	6558680	or you have to go through physical therapy
6558680	6560880	and it's not easy necessarily,
6560880	6565280	but humans are able to adapt to different kinds
6565280	6569000	of physical layouts of our body.
6569000	6572160	And I think there will be a trend
6572160	6577000	towards unified robotic control models
6577000	6580520	that aren't like super tailored to, you know,
6580520	6584040	two legs and two arms and so on and so forth.
6584040	6584960	You know, once you've installed it
6584960	6586320	through a little bit of in-context learning
6586320	6589080	or fine-tuning or reinforcement learning,
6589080	6591600	adapt to that particular form factor.
6591600	6593640	And this will parallel the kind
6593640	6596640	of pre-trained foundation model paradigm
6596640	6599120	that is currently taking place in LLMs
6599120	6602280	where you have like the really big foundation model
6602280	6604680	that can sort of do everything reasonably well
6604680	6606320	and then you can fine-tune it beyond that.
6606320	6609280	If we get to the 2040s in your timeline,
6609280	6613320	you talk about massive amounts of compute being available.
6613320	6616800	You talk about post-scarcity in everything
6616800	6620040	except for land and capital.
6620040	6622040	And then you also talk about the development
6622040	6625520	potentially of superintelligence at that point.
6625520	6627120	What happens there?
6627120	6630720	Who is in control of the superintelligence, if anyone?
6630720	6632200	Yeah, this is sort of where I start
6632200	6633680	to get a little bit tongue-in-cheek,
6633680	6640680	but first of all, I talk about how I tend to think
6640680	6645760	that once we have exascale computing and I think DOE
6645760	6647960	just built their first exascale computer,
6647960	6649400	and maybe it was private company,
6649400	6652440	but we have like one exascale computer in the world.
6652440	6655840	By the 2040s, they'll be commonplace.
6655840	6658840	And if we are ever worried about sort of controlling
6658840	6663480	the supply of GPUs, I don't know exactly
6663560	6665760	how much compute will be on our smartphones,
6665760	6670920	but it will definitely be possible to train a GP5 model
6670920	6672600	from your home computer.
6672600	6675920	And so any kind of AICT regime that we build today
6675920	6679120	that doesn't take into account that falling costs of compute
6679120	6680800	will probably break down.
6680800	6685320	And therefore, amid this broader sort of fragmentation
6685320	6689400	of the machinery of government, the state,
6689400	6691680	I expect more and more government functions
6691720	6696440	to be offloaded into basically private cities,
6696440	6698520	HOAs, GATIC communities.
6698520	6701160	And likewise with the internet, I expect more and more
6701160	6705840	of our sort of permissioning regime for new AI models
6705840	6709640	and deployment to shift to the infrastructure layer
6709640	6712640	where telecommunication providers will be monitoring
6712640	6716760	network traffic for unvetted AI models and so forth,
6716760	6718640	and we'll have like Chinese style firewalls
6718880	6721800	that are specific to a particular local area network.
6722880	6725440	And at that point, the world looks,
6725440	6727440	the United States where this takes place
6727440	6731520	looks more like an archipelago of micro jurisdictions.
6731520	6735720	I tend to think that like a post scarcity
6735720	6739480	political economy looks a lot like the Gulf States,
6739480	6740520	Gulf State monarchies, right?
6740520	6742600	Because Gulf State monarchies are basically
6742600	6743760	living post scarcity, right?
6743760	6746240	They have a spigot of oil they can turn on,
6746480	6749200	and then they can go build mega projects in the desert,
6749200	6750640	and they have like infinite labor
6750640	6753480	because they can just import guest workers.
6753480	6755800	And so you end up with like this,
6755800	6757760	but if we can't have a Gulf State monarchy
6757760	6759360	in the United States, instead we have a bunch
6759360	6764160	of micro monarchies dotting the country.
6764160	6767200	So I sort of jokingly say, you know,
6768080	6771080	who's going to stop the free city of California
6771080	6774600	that's like home to all the trillionaire ML engineers
6774600	6776920	and tech founders from the decade prior
6776920	6781800	from plugging in their humanity sized supercomputer
6781800	6783800	into a fusion reactor and turning it on.
6783800	6786800	Yeah, and this is really your kind of end point
6786800	6790440	of the discussion or your main point
6790440	6792760	of institutions being eroded,
6792760	6797760	and then afterwards being unable to respond to strong AI.
6798600	6800720	Yeah, and leading up to this,
6800720	6803440	it sounds like a scary dystopian type of thing.
6803440	6805560	It doesn't have to be, right?
6806600	6809680	Uber is not dystopian, Airbnb is not dystopian,
6809680	6812120	private airports in other countries are way better
6812120	6815400	than the public airports in the United States.
6815400	6818320	So privatization and the sort of techno feudalist paradigm
6818320	6820320	doesn't have to be bad,
6820320	6823440	but what it is is more adversarial, right?
6823440	6826320	And you know, people have sometimes speculated,
6826320	6830080	you know, did the crumbling of the Roman Empire
6830080	6833280	was a kind of prerequisite to a renaissance, right?
6833280	6835440	Because it allowed for these principalities
6835440	6839480	to sort of compete and to get the Florentine,
6839480	6841240	you know, creativity and so forth.
6841240	6843880	I think, you know, the next couple of decades
6843880	6845520	could similarly be a renaissance
6845520	6849120	for science and technology and for understanding the world,
6849120	6851880	but it's probably a renaissance
6851880	6854560	because we'll be moving into a much more competitive
6854560	6857200	adversarial world where, you know,
6857200	6861440	these city-states and so forth will be hard to coordinate.
6861440	6863880	And so to the extent that there are still these,
6863880	6868800	like, meta-risks where we would value some large-scale,
6868800	6871800	intra- and international coordination,
6871800	6873320	like peace treaties and so forth,
6873320	6876200	the disintegration of the United States
6876200	6878400	where this revolution is occurring
6878400	6880400	would be bad for that.
6881280	6885160	You talk about or you hint at an alternative path.
6885160	6888040	What we've been talking about your timeline here
6888040	6889640	is the default path.
6889640	6892760	You hint at a path where we have something
6892760	6895520	you call constrained leviath.
6895520	6897520	What is constrained leviath?
6897520	6898760	It's the limited government, right?
6898760	6903080	So this is a D'Arnaz and Mogul's word
6903080	6904600	for it from the narrow corridor.
6904600	6909600	And if you trace the rise of sort of what we associate
6910200	6911480	with liberal democracy,
6911480	6915560	it is part of a particular technological equilibrium,
6915560	6917000	in particular an equilibrium
6917000	6920160	that favored centralized governments
6920160	6922200	with impersonal rule of law
6922200	6925520	and impersonal tax administration and so on and so forth.
6925520	6927400	So we associate today with libertarians
6927400	6928720	with like being anti-government,
6928720	6931080	but the basic idea of liberalism
6931080	6934240	is actually associated with strong government,
6934240	6935840	a strong impersonal government
6935840	6937520	that can impose the rule of law.
6937520	6940480	And so if we want to maintain that kind of equilibrium
6941560	6944760	in a world where AI is diffusing on the society level
6944800	6948120	faster than it is on the state and elite level,
6949080	6951240	then we want to accelerate the diffusion
6951240	6952880	of AI within government.
6952880	6955200	And there's obviously lots of low hanging fruit.
6955200	6956760	We talked about how bureaucracies
6956760	6958800	are basically fleshy APIs.
6958800	6962840	Even today, I have a friend at the FTC,
6962840	6964560	the Federal Trade Commission.
6964560	6965760	They have like a 30 person team
6965760	6969080	that is part of the healthcare division
6969080	6971480	and they're in charge of policing
6971480	6973440	the entire pharmaceutical industry
6973440	6976320	in the United States for competition.
6976320	6978800	His day job right now looks like manually
6978800	6980160	reading through 40,000 emails
6980160	6983440	that they subpoenaed from a pharmaceutical CEO, right?
6983440	6985680	And today you could take those emails
6985680	6988480	and put them into a Claude II or something
6988480	6991080	like it with a big context window and ask,
6991080	6994880	find me the five most egregious examples of misconduct.
6994880	6996360	And it would do that.
6996360	6997120	It might not be perfect,
6997120	6999360	but it's a hell of a lot more efficient
6999360	7001000	than reading through them manually.
7001000	7003000	And obviously big law is going to be doing that.
7003000	7006440	And the Pharma CEO and his personal attorneys
7006440	7008480	will be doing that conversely.
7008480	7013160	To maintain our state capacity in the face of AI
7013160	7016320	is to run in this arms race.
7016320	7020480	And you can kind of liken it to an evolutionary biology
7020480	7022440	they call the Rig Queen dynamic,
7022440	7024240	which comes from Alice in Wonderland
7024240	7025600	where the Rig Queen tells Alice
7025600	7027920	that sometimes you need to run just to stay in place.
7027920	7030240	And so I think our government needs to be adopting
7030240	7032120	this technology as rapidly as possible
7032120	7034680	so that they can basically tread water.
7034680	7039200	And that means both diffusing it in existing institutions,
7039200	7042600	but also being open to radical reconfigurations
7042600	7044760	of the machinery of government
7044760	7047880	and addressing some of those firmware level constraints
7047880	7048720	that we talked about,
7048720	7051680	whether it's the lack of a national identification system
7051680	7056200	or the outdated atmoded information technology infrastructure
7056200	7059920	or the accumulation of old procedural
7059920	7062120	kinds of methods of governance.
7062120	7066080	A focused way of doing this is what you've called for
7066080	7067880	in a political article,
7067880	7071400	which is a Manhattan project for AI safety.
7071400	7072720	A first question here,
7072720	7075640	would it be better to call it an Apollo project
7075640	7077600	as opposed to a Manhattan project?
7077600	7079000	I mean, the Manhattan project
7079000	7081160	created some pretty dangerous weapons,
7081160	7084320	whereas the Apollo project might have been more benign.
7084320	7085800	I mean, what the Apollo project
7085800	7087160	and the Manhattan project have in common
7087160	7089000	is that they came from an era of US government
7089040	7090720	where we still dealt things,
7090720	7092720	where we still had competent state capacity,
7092720	7094920	where we still had a lot of in-house expertise
7094920	7097840	and we weren't saddled with all these constraints.
7097840	7101840	So today, we couldn't go to the moon in 10 years,
7101840	7104520	NASA couldn't, SpaceX can.
7104520	7108120	And so our modern Apollo projects
7108120	7109560	are being done by the private sector
7109560	7111880	through competitive contracts.
7111880	7114080	And so one of the messages of my piece
7114080	7116120	on the Manhattan project is to say,
7116120	7117400	the reason I make this analogy
7117400	7121640	is not just because AI is a Oppenheimer like technology,
7121640	7123600	but also because responding to it
7123600	7127520	will require a throwback to those kind of institutional forms
7127520	7131480	where we gave the people at the top a lot of discretion
7131480	7133440	and sort of gave them an outcome
7133440	7135160	and let them solve for that outcome
7135160	7137720	without having much of prescriptive rules
7137720	7139720	about how to solve for that outcome.
7139720	7142920	And then the second reason to make the analogy is,
7142920	7144320	open AI and anthropic,
7145320	7147800	they both have contingency plans
7149080	7154080	for developing AGI and having like a runaway market power, right?
7155520	7156520	And in the case of open AI,
7156520	7158880	it's their nonprofit structure.
7158880	7161920	In the case of anthropic, it's their public benefit trust
7161920	7163240	where they both are envisioning a world
7163240	7165000	where they could potentially be the first to build AGI
7165000	7166280	and become basically trillionaires.
7166280	7167520	And so at that point,
7167520	7171280	they need to become basically governed by a nonprofit board.
7172280	7173840	You know, at that point,
7174840	7176600	and that's not where progress ends, obviously,
7176600	7178720	like there's going to be continued research.
7178720	7181760	It would make sense for the US government to step in
7181760	7184280	and say, let's do this as a joint venture,
7184280	7185720	or we're no longer competing.
7185720	7188640	In fact, the basic structures of capitalism
7188640	7191320	and market competition are starting to break down.
7191320	7193800	Let's just pull this together into a joint venture,
7193800	7198280	study the things that require huge amounts of capital
7198280	7200920	that the private sector doesn't have, but the government can.
7201520	7203560	The US government spent $26 billion
7203560	7205400	on the Manhattan Project in today's dollars.
7205400	7208400	When you think about the financial resources
7208400	7211360	of nation-state actors to put behind scaling,
7211360	7214320	it's nothing like what Microsoft or Google have.
7214320	7219120	What's our first $200 billion training run, right?
7219120	7221000	What kind of things can come out of that?
7221000	7223120	I think that's something that you want to do
7223120	7225560	with the Defense Department's involvement
7225560	7228360	and working with these companies in a joint way
7228360	7230760	through secured data centers
7230760	7233520	and doing gain-of-function-style research
7233520	7235240	that really is dangerous
7236480	7241040	and more Manhattan Project than Apollo Project.
7241040	7242800	What would be the advantages here?
7242800	7247360	We would be able to slow down capabilities research
7247360	7249840	and spend more of the resources
7249840	7252120	on, say, mechanistic interpretability
7252120	7257120	or evaluations or alignment in general,
7257480	7260480	because now the top AI corporations
7260480	7262560	have kind of combined their efforts
7262560	7264880	on the one government group.
7264880	7267640	Yeah, and in my vision,
7267640	7272160	they're still allowed to pursue their commercial verticals.
7272160	7274680	And I have an extended version of the proposal
7274680	7279720	where I talk about needing sort of bio-safety-style categories
7279720	7283400	for high-risk, medium-risk, and low-risk styles of AI
7283400	7285000	that very closely parallels
7285000	7287360	what Anthropic recently put out with their recommendations
7287360	7291120	for sort of a BSL categorization of AI research.
7291120	7293720	So I'm really talking about that BSL4 lab
7293720	7295640	and beyond-style stuff.
7295640	7297200	And some of that stuff,
7297200	7300720	some of it will be to accelerate alignment
7300720	7302040	and interpretability research
7302040	7306040	to sort of do versions of the OpenAI Superalignment Project
7306040	7308400	where they're dedicating 20% of their compute
7308400	7309960	to study alignment.
7309960	7313000	Another part of it will be to forestall
7313000	7315480	competitive race-to-the-bottom dynamics
7315480	7320480	so that they can coordinate and not violate antitrust laws.
7321200	7324240	And then the third thing is sort of the gain-of-function stuff
7324240	7326080	that we really only want to be doing
7326080	7330720	with very strict oversight, compartmentalization,
7330720	7334120	kind of pooling of talent and resources
7334120	7338120	so we can share knowledge on alignment and safety.
7339000	7342560	But then also because government has this huge spending power,
7343000	7344520	relative to the product sector,
7344520	7346360	anytime you build a supercomputer,
7346360	7349000	you're basically borrowing from the future.
7349000	7351400	You're trying to see what like the smartphones
7351400	7354200	20 years from now will be capable of.
7354200	7358000	And so if we want to sort of get ahead of the curve
7358000	7359720	and see where scaling is leading,
7359720	7362080	then I think governments are really the only actor
7362080	7363840	that can waste a bunch of money
7363840	7365920	basically scaling up a system
7365920	7368280	and seeing what comes out of it.
7368280	7371680	Yeah, when we talk about gain-of-function research in AI,
7371680	7375080	it's an analogy to the gain-of-function research
7375080	7379000	that's done on viruses in biolabs, but done for AI models.
7379000	7381840	And this could be experimenting
7381840	7384520	with creating more agent-like models
7384520	7388200	or inducing deception in a model
7388200	7390400	and planting it in a simulated environment,
7390400	7395400	seeing what it does or enticing it to acquire more resources.
7395840	7399760	But again, perhaps in a safely,
7399760	7401680	if this is even possible
7401680	7404960	in a safely constrained, simulated environment.
7404960	7407440	And this is the type of research that we could do
7407440	7410560	in this Manhattan project, this government lab,
7410560	7413400	because we would have excellent cybersecurity
7413400	7416520	and secure data centers and the combined efforts
7416520	7420840	of the most capable people in AI research.
7420840	7423320	If you've watched Oppenheimer, the movie,
7423320	7426040	a lot of that revolved around suspicions
7426040	7428640	of coming to spies and so on.
7428640	7431400	And we really don't have great insight
7431400	7435960	into the operational security of the major AGI labs.
7435960	7438520	And that's something that bringing it in house
7438520	7440920	of the defense department,
7440920	7444320	they would necessarily have to disclose
7444320	7446560	everything they're doing, but also hopefully
7446560	7448600	beef up their operational security.
7448600	7452160	Yeah, they're kind of stuck with a startup mindset,
7452160	7455440	but they're not developing a startup product.
7455440	7458000	They're developing something that, in my opinion,
7458120	7460960	could be more dangerous than the average startup.
7460960	7462920	Yeah, and Dari Amade has said as much
7462920	7465720	that we should just assume that there are Chinese spies
7465720	7468000	at all the major AI companies
7468000	7469360	and at Microsoft and Google.
7469360	7472720	When we think about gain of function research in AI,
7472720	7476200	how do you think about the value of gaining information
7476200	7477880	about what the models can do
7477880	7482320	and what the models can do versus the risk we're running?
7482320	7487320	It would be a tragic and ironic death for humanity
7487520	7490920	if we experimented with dangerous AI models
7490920	7493400	to see whether they would destroy us
7493400	7495560	and then we hadn't constrained them properly
7495560	7497760	and they actually destroyed us.
7497760	7499960	So how do you think of that trade-off
7499960	7504960	between gaining information and avoiding lab leaks?
7505000	7508720	Yeah, hopefully lab leaks are less likely
7508720	7512120	than in the biology context where, you know,
7512120	7515960	getting a little bit of blood or urine on your shoes
7515960	7517440	as you walk at the door.
7517440	7520120	Now, it's a difficult thing to talk about in part
7520120	7522520	because we just went through a pandemic
7522520	7526880	that very probably was caused by a BSL4 lab leak.
7526880	7532040	And so, you know, one saving grace is that AI models
7532040	7533960	don't get caught in your respiratory system.
7536520	7539000	And so hopefully there's forms of compartmentalization
7539000	7542360	that are much more robust than in the biology context.
7542360	7545200	And to the extent that this research
7545240	7546880	is going to be done anyway,
7546880	7549640	you know, it would be much better to move it off-site
7549640	7552720	and hopefully in a way that facilities are air-gapped
7552720	7554680	and so forth, rather than, you know,
7554680	7556360	what Microsoft is doing right now,
7556360	7559600	that Microsoft just recently announced their AutoGen AI,
7559600	7562240	which are sort of agent-based models,
7563200	7567040	very similar to like AutoGPT, but like that work.
7567040	7570800	And they're doing this through Creative Commons,
7570800	7573480	totally open source framework.
7573480	7576280	All this capabilities work is gain a function research,
7576280	7578600	where we draw the line between doing things
7578600	7580080	that are intentionally dangerous
7580080	7581400	or doing things that are dangerous,
7581400	7586080	but we're kind of pretending that they're not, is hard.
7586080	7589200	I do think there's, and Paul Cristiano is also agreed
7589200	7591000	with this sort of threat models
7591000	7594840	that would be valuable to be running in virtual machines
7594840	7598600	and to see, you know, if the AI develops awareness,
7598600	7600720	situational awareness and tries to escape,
7600800	7603560	but it escapes into a simulated world that we built for it.
7603560	7607720	Okay, let's end by talking about a recent critique
7607720	7612720	of expecting AGI to arrive pretty in a short time.
7613600	7616080	This revolves around interest rates.
7616080	7620040	And I guess the basic argument is,
7620040	7623640	or the basic question is, if AGI is imminent,
7623640	7626800	why are real interest rates low?
7626800	7628480	I can explain it, but you're the economist,
7628520	7631000	so maybe you can explain the reason in here.
7631000	7634680	So it's really a question of how efficient are markets
7634680	7636880	and how much foresight do markets have.
7637720	7638880	You know, we're coming out of a world
7638880	7640760	of very low interest rates, of ultra low interest rates,
7640760	7642480	near zero interest rates.
7642480	7644760	And one way to think about that is there's a surplus
7644760	7646720	of savings relative to investment.
7646720	7647880	And so one of the reasons interest rates
7647880	7649240	have been in secular decline
7649240	7651920	is because populations are aging,
7651920	7655640	and so all people have a huge amount of savings built up.
7655640	7656880	And meanwhile, we're going through
7656880	7658320	the sort of technological stagnation.
7658320	7660880	So the amount of savings relative
7660880	7663680	to the amount of profitable investments was out of whack,
7663680	7665680	and so that pushes interest rates down.
7665680	7667680	In a world where AGI takes off,
7668760	7672240	it's a world where we have enormous investment opportunities,
7672240	7674040	where we'll be building data centers left and right,
7674040	7675280	and we can't do it fast enough,
7675280	7676080	where there's new products,
7676080	7680440	new commercial opportunities left and right.
7680440	7683400	And so you would expect in that world
7683400	7685880	where the singularity is near, so to speak,
7685880	7687760	to be one where the markets begin forecasting
7687760	7689360	rapidly rising interest rates
7689360	7691280	because the savings to investment balance
7691280	7692640	is starting to shift.
7692640	7695840	And in addition, there's a long run stylized fact
7695840	7699360	that real interest rates track growth rates.
7699360	7702360	And so if GDP growth takes off,
7702360	7706480	you'd also expect at least nominal rates to also take off.
7706480	7710400	And so some have argued that looking at current interest
7710400	7712720	rate data, like the five-year, 10-year,
7712720	7714640	30-year treasury bonds,
7714640	7718240	that the markets are not predicting AGI.
7718240	7720800	You know, the two responses to that are,
7720800	7723160	one, first of all, interest rates are up quite a bit.
7723160	7725120	Nothing's mono-causal.
7725120	7727360	There's lots of confounding factors.
7727360	7730440	Is this, to some extent, the markets anticipating
7731480	7732600	an investment boom?
7732600	7735000	You know, maybe they're not anticipating full AGI,
7735000	7738960	but they're seeing the way LLMs are going to impact
7738960	7742200	enterprise and sort of picking some of that in.
7742200	7744000	And then the second piece would be,
7744040	7747960	okay, to the extent that they're not pricing in AGI,
7747960	7749960	how much foresight do markets have anyway?
7749960	7752720	Before we discuss market efficiency,
7752720	7756280	I just want to just give a couple of intuitions here.
7756280	7761280	If AGI was imminent and it was unaligned, say,
7761320	7764440	and it would destroy the world in five years,
7764440	7767520	well, then it doesn't make a lot of sense to save money.
7767520	7772440	Similarly, if AGI is about to explode growth rates,
7772480	7775760	well, then a lot of money will be available in the future.
7775760	7777400	You're about to become very rich,
7777400	7779880	so it doesn't make sense to save a lot now.
7779880	7783240	And the pool of available savings
7783240	7785840	determine what's available for lending,
7785840	7788440	which determines interest rates.
7788440	7793440	But let's discuss whether markets then are efficient
7794240	7797320	on this issue or to what extent they're efficient.
7797320	7799160	Right, so this is the efficient market hypothesis,
7800040	7803080	which comes in strong and weak forms.
7803080	7805480	So the strong form of the efficient market hypothesis
7805480	7808720	would say that markets aggregate all of available information
7808720	7810640	and are our best sort of point estimate
7810640	7812000	of anything we care about.
7812920	7817080	The weaker form, which I think is more defensible,
7817080	7819080	is that markets can be wrong,
7819080	7822240	but they can be wrong longer than you can be solvent, right?
7822240	7825520	And so you can try to short a company that, like Herbalife,
7825520	7827760	famously, there's a big short position on that,
7827760	7829200	and because Herbalife sort of looks like
7829200	7831800	it's a multi-level marketing Ponzi scheme,
7831800	7834480	but yet the hedge fund that did that
7834480	7837120	lost several billions of dollars before they
7837120	7839160	ended their position because the markets
7839160	7842000	stayed irrational longer than they could stay solvent.
7842000	7843840	The second factor is the weaker versions
7843840	7845160	of the efficient market hypothesis
7845160	7849080	are sort of based on a no arbitrage condition, right?
7849080	7852240	They say markets are efficient only insofar
7852240	7857280	as you can arbitrage an inefficiency, right?
7857280	7860920	And so you look at some prediction markets, for example,
7860920	7861760	they predict it.
7862680	7867440	They'll often have very clear inconsistencies
7867440	7871400	across markets that look like they're irrational,
7871400	7873360	but then you realize, oh, I can only make
7873360	7876280	like $7,000 total on the website
7876280	7878240	and there are transaction fees
7878240	7881160	and there's work involved.
7881160	7884400	And so if the market isn't very deep or liquid,
7884400	7886080	there may be inefficiencies that exist
7886120	7887880	not because the market's inefficient,
7887880	7891000	but as efficient as it can be under the circumstances.
7891000	7895680	And when it comes to AI, how do you arbitrage?
7895680	7898800	I've been thinking for a while now that Shutterstock,
7898800	7901040	their market cap should be collapsing, right?
7901040	7905760	Because we have image generation that is proliferating.
7905760	7907160	And yes, people will make the argument though,
7907160	7909520	Shutterstock has all this image data
7909520	7911840	that could build a better image model.
7911840	7913880	Maybe it seems like it's cannibalizing their business
7914000	7916720	or turning a moat into a commodity.
7916720	7919360	And yet Shutterstock's market cap
7919360	7922200	has basically held constant throughout
7922200	7927200	this recent rebirth of image generation models.
7927440	7930120	What if you borrow a lot of money cheaply
7930120	7933640	and then put it into an index of semiconductor stocks
7933640	7936040	or just IT companies in general,
7936040	7939360	even just the general S&P 500 say,
7939360	7943280	would that be a way of arbitraging this AGI forecast?
7943920	7946120	Yeah, I would say if you have short timelines,
7946120	7949440	you should be putting a lot of money into equities.
7949440	7952440	This is not financial advice, I should say.
7952440	7955720	Right, and I mentioned earlier that Paul Christiana
7955720	7957920	has said in interviews that he's twice levered
7957920	7959160	into the stock market.
7959160	7961280	He basically owns a bunch of AI exposed companies
7961280	7966280	and he's borrowed enough money to double his investments.
7966800	7969080	So that's putting your money where your mouth is.
7969080	7971720	When you look at market behavior
7971760	7973400	over the long stretch of time,
7973400	7977080	markets didn't anticipate the internet very well.
7978440	7980440	There was a short run bubble
7981600	7985680	that led to a boom and bust of .com stocks.
7985680	7987520	But in terms of the real economy,
7987520	7988720	the internet just kept chugging along
7988720	7989560	and kept being built out
7989560	7992240	and eventually a lot of those investments
7992240	7995280	ended up paying off even if you rode through the bubble.
7995280	7996720	Markets are made of people.
7996720	7998720	Some of the biggest capital holders in the markets
7998720	8002240	are institutional investors, pension funds,
8002240	8005320	life insurance companies, governments,
8005320	8010320	like the Saudi Arabia or the Norwegian pension fund.
8010920	8015920	And often these are making safe bets.
8016200	8019280	You know, they're not taking very heterodox views
8019280	8020720	on markets.
8021920	8025400	And so as a result, markets can be a little bit
8025400	8027800	autoregressive, they're a little bit biased to the past,
8027800	8029360	and past this prologue,
8029360	8032840	and prone to kind of multiple equilibria,
8032840	8036560	where there's two prices that the shutter stock can be.
8036560	8038720	The shutter stock could be a $50 stock
8038720	8040120	or it could be a $0 stock,
8040120	8042160	and at some point the market will update
8042160	8044360	and will wander go through like the great repricing
8044360	8046360	and all these asset prices will flip
8046360	8047680	in relatively short order.
8047680	8050240	The efficient market hypothesis has to be false,
8050240	8052760	or else we wouldn't have Silicon Valley.
8052760	8054840	Right, we wouldn't have founders
8054840	8057360	that we wouldn't have Elon Musk, right?
8057840	8059840	So I would just say the markets are wrong.
8059840	8062600	And partly they're wrong because to be right
8062600	8067600	would require having a bunch of relatively bespoke
8067760	8070880	and kind of esoteric priors about the direction
8070880	8074200	of technology that are only now just sort of
8074200	8076000	percolating into the mainstream.
8076000	8078640	Yeah, and that the big kind of capital allocators
8078640	8081440	can't really respond to because they're risk averse.
8081440	8082640	Exactly.
8082640	8084360	Now that doesn't mean like Renaissance technologies
8084360	8085440	won't respond to it,
8085440	8087080	but they're not gonna move the market.
8087120	8088920	Samuel, thanks for this conversation
8088920	8090480	and I've learned a lot.
8090480	8091320	Thank you.
