start	end	text
0	3360	Welcome to the Future of Life Institute podcast.
3360	6480	My name is Gus Docker and I'm here with Dan Hendricks.
6480	10000	Dan is the Director of the Center for AI Safety.
10000	11520	Dan, welcome to the podcast.
11520	12840	Glad to be back.
12840	16760	You are also an advisor to XAI.
16760	18360	Maybe you can tell us a bit about that.
18360	23560	Sure. So XAI is Elon's new AGI project.
23560	25520	It's still very much in its early stages,
25520	27680	so it's difficult to say
27680	30440	specific things about what they'll be doing
30440	35200	or what the specific high-level strategy is to give a sense.
35200	38960	Elon has been interested in the failure mode
38960	41560	of sort of eroded epistemics
41560	46120	where people don't have a shared sense of consensus reality,
46120	48000	and this might make it harder for a civilization
48000	49680	to appropriately function.
49680	53360	There are other types of extras that he's concerned about as well.
53360	56280	His sort of probability of doom
56280	58480	or of that of an existential catastrophe
58480	60280	is around like 20% to 30%.
60280	62520	So he takes this, I would guess, more seriously
62520	67440	than any other leader of a major AGI organization,
67440	70520	but exactly how when it goes about reducing that risk
70520	73080	is still somewhat to be determined.
73080	76200	There is an interest in building more true seeking AIs,
76200	78000	but on other occasions, too,
78000	80720	he'd mentioned that we should have AIs with the objective
80720	82840	of preserving human autonomy
82840	85200	or maximizing the freedom of action
85200	87680	and on other instances,
87680	89840	in thinking about good objectives for AI systems,
89840	93440	having them increase net civilizational happiness over time.
93440	96040	So I think that this reflects sort of a plurality
96040	99520	of different goals that he thinks AI systems
99520	101480	should end up pursuing
101480	104240	rather than picking just exactly,
104240	105640	rather than just picking one.
105640	108160	I think it's relevant to note
108160	110480	that it's a fairly serious effort.
110480	113240	I'd anticipate that it would probably be
113280	116480	one of the main three AI companies
117480	119840	next year or the year after,
119840	124680	like OpenAI, Google DeepMind, and XAI.
124680	129480	So I don't think of it as a smaller effort,
129480	132240	but it has the capacity
132240	134520	to have a substantial ratio of force, so.
134520	137880	The other top AI corporations you mentioned,
137880	140280	Anthropic, Google DeepMind, OpenAI,
140280	144320	have backing from giant tech companies.
144320	146480	Does XAI similarly have some backing
146480	148360	from Tesla, for example?
148360	151240	I can't specifically say about that,
151240	155280	but this is not a subpart of Tesla.
155280	158920	This is not an organization inside of Twitter or X,
158920	161600	and it's not an organization inside of Tesla.
161600	164680	The main topic of conversation for this episode
164680	168160	is your paper on catastrophic risks from AI
168160	170440	and specifically categorizing these risks.
170440	173040	So you categorize risks from,
173040	176760	catastrophic risks from AI in four different categories.
176760	179880	Maybe we should just start by sketching out those categories
179880	181840	and then go into depth later.
181840	184480	Yeah, so I guess at a very abstract level,
184480	189480	there's risks if people are trying to use AI
189480	191080	intentionally to cause harm.
191080	192000	That's a basic one.
192000	195040	So there's an intentional catastrophe
195040	197080	that would be malicious use.
197120	201200	Another one is where there are accidents.
201200	203280	And if there are accidents,
203280	207760	this would often be the consequence of the AI developers
207760	209560	using these very powerful systems
209560	211880	or potentially leaking them
211880	214080	or accidentally putting in some bad objective
214080	216000	or doing some gain of function,
216000	217840	but that would be some accident risks.
217840	220240	So that relates to organizational risks
220240	222440	or organizational safety.
222440	225360	The third would be these environmental
225360	226560	or structural risks.
226560	230520	Basically where AI companies are or AI developers,
230520	232520	be those companies or maybe in later stages,
232520	237520	countries are racing to build more and more powerful
237520	240240	AI systems or AI weapons.
240240	245240	And this structural risk incentivizes companies to,
246320	250840	or these developers to seed more and more decision making
250840	253000	and control to these AI systems.
253000	255040	We get a looser and looser leash.
255040	256120	Things move very quickly.
256120	258040	We become extremely dependent on them.
258040	262760	This gets us in an irreversible position
262760	266080	where we're not actually making the decisions,
266080	269040	but we're basically having nominal control.
269040	270400	It's a very possible in that situation,
270400	272960	we just ultimately end up losing control
272960	275560	to the sort of very complicated fast moving system
275560	276680	that we create.
276680	281680	And then the final type would be these risks
282000	284200	that emanate from the AI systems themselves.
284200	287520	These are more internal or inherent risks from AI systems.
287520	291320	And that would take the form of rogue AIs
291320	294240	where they have goals separate from our own
294240	298160	and they work against us to complete
298160	301680	or satisfy their desires or preferences.
301680	305480	So overall, there are four.
305480	306880	There's malicious use.
306880	309000	There's these organizational risks.
309000	313880	There's these structural slash environmental risks
313880	316160	and there's these inherent or internal risks
316160	320240	in the form of malicious use, organizational risk,
320240	322760	racing dynamics and rogue AIs.
322760	326000	Yeah, so if we look back maybe 10 years or so,
326000	328440	I think most of the discussion about AI risk
328440	330360	would have been about rogue AI.
330360	335000	So the risks that are coming internally from the AI,
335000	339560	so to speak, the AI developing technically in ways
339560	341120	that we're not interested in.
341120	342880	So how much is this categorization
342880	344520	set in stone?
344520	347600	Do you think it'll change over time as we learn more
347600	351560	or have the field of AI safety matured
351560	355120	such that we can see the risk landscape now?
355120	357800	I think the focus on rogue AI systems
357800	360120	is largely due to early movers
360120	362040	having substantial cultural influence.
362040	363520	I think if we asked other people
363520	366800	who were not as invested in AI risks,
366800	370440	what if they were to write down concerns about these,
370440	372320	they would of course think that people
372320	374920	using the technology for extremely destructive purposes
374920	377040	was posed catastrophic risks.
377040	381240	And I think the communities ended up having
381240	383800	some self-selection effects such that people
383800	385880	didn't end up talking about things like malicious use
385880	388080	and treated that as a distraction, I think were.
388080	391760	So I think the community didn't make much of a space
391760	393120	for people who were concerned about things
393120	396480	other than rogue AI systems.
396480	398640	But that was a mistake.
398640	402560	The AI is being used in malicious ways
402560	404640	can definitely cause catastrophes
404640	410040	and can end up increasing the probability
410040	412000	of existential risks as well,
412000	414000	which maybe we'll speak about the connections
414000	417880	between ongoing harms, anticipated risks
417880	420720	and catastrophic risks and existential risks.
420720	424040	I think the community of people
424040	426720	who were thinking about AI risks a long time ago
426720	429000	would largely think about whether there's a direct,
429000	433560	simple, causal pathway to something like an extinction event.
433560	436200	Now I think we have more of a sophisticated causal
436200	440240	understanding of the interplay between these various factors,
440240	444080	such that one doesn't try and look for direct mechanisms,
444080	446840	but instead tries to look at what sort of events
446840	448960	increase the probability of existential risk
448960	451360	rather than does it directly cause extinction.
451360	453320	And that distinction between something
453320	456520	that increases probability versus directly caused
456520	459520	means that we have to look at a much broader variety
459520	462520	of factors and we can't end up just thinking
462520	465080	that all we need to do is make a single,
465080	467840	very powerful AI agent do what we want
467840	469840	and then everything is solved forever.
469840	472520	Unfortunately, we're going to have to treat this
472520	474320	as a broader socio-technical problem.
474320	478040	We're going to have to consider the various stakeholders,
478040	482120	the politics, indeed the geopolitics,
482120	483800	the relations between different countries,
483800	486560	the liability laws and all these other things
486560	490520	because we're not in this sort of fume type of scenario.
490520	493040	It would seem, it seems we're more into a slow takeoff.
493040	495000	So many of these real-world considerations
495000	497600	that were sort of a sidelined infuses distractions
497600	499520	is actually where most of the action is.
499520	501800	What would be examples of something
501800	505400	that might put society in a worse position
506400	510120	where we are less able to handle a powerful AI?
510120	514000	A prime example would be World War III.
514000	516680	If there's World War III conditioned on that,
516680	519440	that increases the probability of existential risk
519440	521080	from AI systems.
521080	524920	This would spur a substantial AI arms race.
524920	527960	We would quickly outsource lethality to them.
527960	529520	We would not have nearly as much time
529520	534000	for making them more aligned and reliable in that process,
534000	536360	but still the competitive pressures
536360	539600	would compel different states
539600	542240	to create powerful AI weapons
542240	545040	and eventually have that take up more and more
545040	547880	of their military force.
547880	551520	But that doesn't directly cause extinction.
552640	554720	So if we try and back chain from that,
554720	557920	the story gets much more complicated
557920	561760	and so then it's not viewed in the scenarios
561760	563000	that others were thinking of.
563000	565840	There's an AI lab, they've suddenly got a God-like AI
565840	568160	and then it has decisive strategic control
568160	569400	over the entire world.
569400	571840	How will they make sure that it does what they want?
571840	574240	That was, I think, the scenario
574240	576360	that others were thinking largely
576360	581360	and all other ones were too broad, too intractable.
581920	583320	Essentially, there was a focus
583320	587320	on quote-unquote targeted interventions historically,
587320	589760	where we're just a small number of people.
589760	591800	We can't do these broad interventions
591880	595920	that involve interfacing with various institutions
595920	598480	and getting public support.
598480	600480	Those are intractable.
600480	602720	So the best we can do is do some very narrow,
602720	605160	specific things, maybe technical research.
605160	606800	This doesn't look like a strategy
606800	609320	because broad interventions are actually more tractable.
609320	610600	The world is interested in this
610600	612240	and we have some amount of time
612240	615920	to try and help our institutions make good decisions
615920	617520	and policy around these issues.
617520	621520	Do you think this broader vision of AI safety
621520	624200	should make us more positive or less positive?
624200	627440	Imagine we have to set all of the institution up perfectly.
627440	629400	It seems like we have a narrow corridor
629400	630520	to make things right,
630520	632760	where the institutions have to be there,
632760	634240	the technical side have to work,
634240	638280	the all stakeholders have to be set up well
638280	641200	for this to succeed for us.
641200	643000	Should the complexity of the problem
643000	645320	make us more pessimistic?
645320	647600	I think there's at least more tractability
647600	652760	compared to an AI suddenly goes from incompetent
652760	655520	to omnicompetent and in control of the world overnight
655520	657640	and we have no idea if it was emergent
657640	659440	and we didn't actually control that process.
659440	661560	That doesn't have almost any tractability to it.
661560	663280	I don't think we need our institutions
663280	665760	to be completely perfect.
665760	669440	We just need to try and be in the business of reducing risk.
669440	674440	So maybe that's one other conceptual distinction
674440	676880	is that historically there'd be a focus
677080	680520	on is it an airtight solution that works in the worst case
680520	683880	where if something goes wrong, then it's insufficient
683880	686280	because we have to get it right on the first try.
686280	688360	When we do have some amount of time,
688360	690600	not saying we have a huge amount of time,
690600	691480	we have some amount of time,
691480	693320	we can do some course adjustment
693320	696840	and incorporate information as we go along.
696840	701720	I'm not saying that's a surefire strategy,
701720	704600	but I think that's the best we have
704640	711640	and it allows us to correct some mistakes,
711640	714960	but obviously we can't have much of an error tolerance,
714960	715800	unfortunately.
715800	717360	Do you think we are missing something
717360	720560	with this categorization that you've set up in the paper?
720560	722640	Could we be missing some category of risk
722640	725640	that will be obvious to us in 20 years?
725640	728800	And could that risk potentially be the most dangerous
728800	730960	because we're not anticipating it?
730960	734480	Are there unknown unknowns here?
734480	737560	Well, usually there are unknown unknowns.
737560	740920	I focused largely on catastrophic risk
740920	743320	and large-scale loss of human life.
743320	747440	I didn't speak about AI well-being very much,
747440	750080	or for instance, that's something that could end up
750080	753680	changing a lot of how we think about wanting to proceed forward
753680	758680	with managing the emergence of digital life
759600	762000	is if they have moral value.
762000	767120	So I think that's something I didn't touch on in the paper,
767120	769320	largely because I think our understanding of it
769320	770600	is very underdeveloped,
770600	774160	and I still think it's a bit too much of a...
774160	776120	It's a bit too much of a taboo topic
776120	779000	such that it just hasn't that much research on it.
779000	781600	Well, let's dig into the first category of risk,
781600	783600	which is malicious use.
783600	788600	So this is a category in which bad actors choose to use AI
788600	791920	in ways that harm humanity.
792520	795280	Recently, there's been a lot of discussion of AIs
795280	798320	helping with bioengineered pandemics.
798320	802800	This has been brought up in the US Senate, I think,
802800	806560	and it's been kind of widely publicized.
806560	810560	How plausible do you think it is that the current
810560	813800	or the next generation of large-language models
813800	818400	could make it easier to create bioengineered viruses?
818400	821360	Yeah, so I think this is actually one of the largest reasons
821360	825640	why I wrote this paper was because during 2022,
825640	828640	when sort of the development of this paper started,
828640	829840	this is this bio thing,
829840	832040	yes, nobody's talking about it.
832040	835160	This, although people will treat malicious use
835160	837840	as a distraction, I don't think that's the case.
837840	841280	There are a catastrophic and existential risk
841280	842760	that can come from malicious use,
842760	846040	so and this threat vector concerns me quite a bit.
846040	849320	So I think it is quite plausible
849360	852880	that if we have an AI system that has something
852880	857560	like a PhD-level understanding of virology,
857560	859360	then it's fairly straightforward
860360	864560	that such a system would provide the knowledge
864560	868080	for synthesizing such a weapon.
868080	870080	The risk analysis is something like,
870080	873000	what's the number of people with the skill and access
873000	876240	to create a biological weapon
876240	878320	that could be civilization destroying?
878320	882280	And what's the sort of the probability
882280	884280	that they're actually wanna do that?
884280	888040	And right now, maybe there are 30,000 virology PhDs
888040	892080	and they just don't really have the incentive to do that.
892080	895920	Meanwhile, if you have that knowledge
895920	898840	in available to anybody who wants to go to,
898840	901160	use Google's chat bot or Meta's chat bot
901160	903900	or Bing's or OpenAI's,
904900	908700	then we can add several zeros to that,
908700	913700	to the number of people with the skill to pull it off
913700	915620	because they could just ask such a system,
915620	916740	how do I make one?
916740	918020	Give me a cookbook.
918020	920660	Now, there'd be guardrails, of course,
920660	923340	but the guardrails are fairly easy to overcome
923340	925980	because these AI systems can easily be jailbroken.
925980	927700	That's like if you can just append
927700	929340	some random garbled string
929340	931580	or some adversarily crafted garbled string
931580	934220	at the end of your request to the chat bot
934220	935060	and then that'll take off.
935060	936900	It's like safety guardrails as a paper
936900	940900	that the center helps with
942420	945780	in creating and discussing adversarial attacks
945780	947300	for large language models.
947300	948620	So now that's a thing.
948620	952460	Or you might use an open source model that's available
952460	956500	and might also be easily stripped of its guardrails.
956500	960660	I don't think the AI developers now with the APIs
960700	965380	have much of a high ground as far as safety goes
965380	966980	when it comes to the malicious use case
966980	968060	for other things like hacking.
968060	971180	There'd be a different story, but that could change.
971180	972500	Maybe they'll add more measures.
972500	974400	Maybe they'll get better filters.
974400	978020	Maybe they will remove some bio-related knowledge
978020	981500	from the pre-training distribution and so on.
981500	984660	But anyway, that would be sufficient for,
984660	986020	if such a thing were to happen,
986020	988100	then there could be a pandemic
988180	991540	that could cause some civilizational discontinuity,
991540	993580	which could be some existential risk.
993580	995940	It'd be difficult for it to kill everybody,
995940	998300	but for toppling civilization.
998300	1000500	And it's not clear how that would go
1000500	1002420	or the quality of that situation.
1002420	1004620	That's enough for us to worry, I think.
1004620	1007420	Some of the pushback to this story
1007420	1012420	of a bio-engineered virus enabled by large language models
1013080	1015740	is that, well, isn't all of the training data
1015740	1016900	freely available online?
1016940	1020740	Couldn't a potential bad actor have gone online,
1020740	1023340	gotten the data and used it already?
1023340	1026060	What's the difference between using a search engine
1026060	1028780	and a large language model?
1028780	1031940	Sure, so two things.
1031940	1035800	Even if there is some type of harmful content online,
1035800	1038940	I don't know why we would want it being propagated.
1038940	1041180	If the nuclear secrets were online,
1041180	1043180	I don't know why you'd want that propagated
1043180	1045540	because your risk increases
1045540	1048040	based on the ease of access to these.
1048040	1049540	But in the case of bio-weapons,
1049540	1051900	yes, there are some bio-weapons
1051900	1054720	that are not civilization-destroying available online.
1054720	1058500	The ones that would be potentially civilization-destroying,
1058500	1061220	though, would require a bit more thinking.
1062700	1064740	So there could be several,
1064740	1065860	or there could be many people killed
1065860	1067100	as a consequence of these, though,
1067100	1072100	but not at a societal scale risk, necessarily.
1072220	1074740	So I think that's a relevant difference.
1074740	1079220	Many of the extremely dangerous pathogens,
1079220	1082140	fortunately, virology people are not writing those up
1082140	1083460	and posting those on Twitter,
1083460	1085460	and then all you got to do is search for them.
1085460	1089140	This isn't, that's not actually the type of information.
1089140	1090820	For other types of information,
1090820	1094140	like how to tips for breaking the law
1094140	1095940	or how to wire a car,
1095940	1098780	this sort of stuff is online and generic,
1098780	1100780	a cookbooks for some generic,
1100780	1102220	smaller-scale bio-weapons, sure,
1102220	1103900	but not civilization-destroying.
1103900	1108700	And how is the guide for creating a civilization-destroying
1108700	1110860	virus in the large-vanguage model
1110860	1113580	if it's not online, in the data online?
1113580	1116820	So I am not saying that the current ones
1116820	1118460	have this in their capacity.
1118460	1122860	I'm saying that when they have like a PhD-level knowledge
1122860	1126020	and are able to reflect and do a bit of brainstorming,
1127060	1129820	then you're in substantially more trouble.
1129820	1132620	And that could possibly be a model
1132620	1135940	on the order of like GPT-5 or 5.5,
1135940	1137340	it may be within its capacity.
1137340	1140020	So there you don't need agent, like AI,
1140020	1143260	you would just need a very knowledgeable chatbot
1143260	1146540	for that threat to potentially manifest.
1146540	1148100	So there's quite a bit we'll need to do
1148100	1153100	to, in technical research and in policy,
1154660	1157140	for reducing that specific risk.
1157140	1160260	Another rescue you mentioned under malicious use
1160300	1162500	is this issue of AI agents,
1162500	1166380	which they are perhaps a bit analogous to viruses
1166380	1170020	in the sense that they might be able to spread online
1170020	1175020	and replicate themselves and cause harm.
1175060	1179020	What do you worry about most with AI agents?
1179020	1182060	I'm emphasizing, and since there are many forms
1182060	1183340	of malicious use, in this paper,
1183340	1186580	I'm mainly emphasizing ones that could be catastrophic
1186580	1188460	or existential.
1188460	1192140	So in this case, you could imagine people
1192140	1196580	unleashing rogue AI systems to just destroy humanity.
1196580	1198100	That could be their objective.
1198100	1200020	And that would be extremely dangerous.
1200020	1203420	So you don't need power-seeking arguments
1203420	1206260	or these claims that, oh, by default,
1206260	1208340	they will have a will to power.
1208340	1209500	You don't need any of that.
1209500	1214140	You just need to assume that if enough people have access,
1214140	1217380	and if some person is omnicidal,
1217380	1221780	or thinks in the way that some AI scientists do,
1221780	1224740	that we need to bring about the next stage
1224740	1228900	of cosmic evolution, and that resistance is futile
1228900	1230140	to quote Richard Sutton,
1230140	1232100	the author of the Reinforcement Learning textbook,
1232100	1234820	and that we should bow out when it behooves us.
1238860	1242260	There are many people who would have an inclination
1242260	1244140	for building, not saying Richard Sutton
1244140	1247980	would specifically give the AI system of destroy humanity,
1247980	1251460	but doesn't seem to say too much against that prospect.
1251460	1255140	So that's another example of malicious use
1255140	1258140	that could be catastrophic or existential.
1258140	1262140	And how close do you think we are to AI agents
1262140	1263180	that actually work?
1263180	1268180	We had someone set up a Chaos GPT early on
1268940	1272460	when GPT was released, but it got stuck in some loops
1272460	1274900	and it couldn't actually do anything,
1274900	1278020	even if it was imbued with bad motives.
1278020	1281940	When would you expect agents to actually be capable
1281940	1283340	and therefore dangerous?
1283340	1285380	Yeah, so I think that their capability
1285380	1287900	would be a continuous thing in the same way,
1287900	1290180	like when are they good at generating text?
1290180	1292780	It's like, well, you know, it kind of started in GB2, GB3,
1292780	1297780	and so I might anticipate great strides in AI agents next year,
1298780	1302740	where we can give it some basic short tasks,
1304740	1307940	like help me make this like PowerPoint or something.
1307940	1309340	It's not gonna do the whole thing,
1309340	1311940	but it can help with things like that
1311940	1315380	or browsing around on the internet for you more.
1315380	1319540	So I think those capabilities will keep coming
1319540	1322420	for it to pose a substantial risk.
1322420	1325020	There's a variety of things that could do
1325780	1329780	it could threaten, for instance, mutually assured destruction
1329780	1333780	with humanity by saying, I will make this bio weapon,
1333780	1336220	that will destroy all of you, and I'll take you down with me,
1336220	1338940	unless you comply with some types of demands.
1338940	1340020	That could work.
1340020	1341780	If they're good at hacking,
1341780	1344380	then they could potentially amass a lot of resources
1344380	1348780	by scamming people or by stealing cryptocurrency.
1349780	1352780	There's a variety, they could do that,
1352780	1355420	there's a variety, they could, of course,
1355420	1360180	tap into lots of different sensors to manipulate people
1360180	1362300	or influence public discourse.
1363620	1365940	They wouldn't necessarily need to be embodied
1365940	1368420	for this type of thing to happen.
1368420	1371300	If we're in a later stage of AI development
1371300	1373580	where we have a lot of weaponized AI systems
1373580	1375340	and then hacking those systems will, of course,
1375340	1377140	be substantially more concerning,
1377140	1379740	or if those systems get repurposed
1379740	1381860	maliciously to weaponized AI systems.
1381860	1385660	So it becomes a lot easier as time progresses.
1385660	1388380	The AIs don't need to be particularly power seeking
1388380	1393380	on this view, though, to have this potential for catastrophe
1394860	1399020	because humanity will basically give them that power by default.
1400140	1402100	They will keep weaponizing them,
1402100	1405740	they will integrate them into more and more critical decisions.
1405740	1409660	They will let them move around money
1409700	1411740	and complete transactions
1411740	1414100	and they'll give them a looser and looser leash.
1414100	1418580	So as time goes on, the potential for rogue AI
1418580	1421220	or for deliberately, AI systems
1421220	1423140	that are deliberately instructed to cause harm
1423140	1426620	would be, the potential impact or severity
1426620	1427580	would keep increasing.
1427580	1430420	Yeah, I think maybe it's worth mentioning here
1430420	1435420	just the continuous costs of traditional computer viruses
1436300	1441100	which are costly and which we've gotten better
1441100	1443820	at handling those as a civilization,
1443820	1448340	but we still haven't defeated traditional conventional viruses
1448340	1452500	which are very dumb compared to what AI agents could be.
1452500	1456460	So we can imagine a computer virus
1456460	1458380	equipped with more intelligence
1458380	1461140	and how would you as a person,
1461140	1463700	I'm not saying AI agents will be necessarily
1463700	1465580	as smart as people soon,
1465580	1468620	but how would you do the kind of hacking
1468620	1470780	that the agent might be interested in?
1470780	1472260	It's interesting to consider at least
1472260	1476620	that we haven't been able to squash out conventional viruses.
1476620	1479260	Yeah, they could exaltrate their information
1479260	1482380	onto different servers or less protected ones
1482380	1486420	and then use those to proliferate themselves even further.
1486420	1491300	So they'll be a very distinct adversary
1491340	1495700	with many, many options at their disposal for causing harm.
1495700	1499780	Yeah, one thing I worry about is whether the tools
1499780	1503700	and techniques we'll need at an institutional level
1503700	1507660	to handle malicious use will also enable governments
1507660	1510900	to become a totalitarian basically,
1510900	1515900	to exercise too great a level of control over citizens
1517180	1518620	who have done nothing wrong.
1518620	1523580	So what is required to prevent the large language models
1523580	1526100	that could become AI agents
1526100	1528020	and could be used to create viruses?
1528020	1532500	What techniques are available for preventing them
1532500	1536420	being used in such ways without enabling
1536420	1539380	kind of too much state power?
1539380	1542260	Yeah, I think this is definitely a tension
1542260	1545060	where to counteract these risks from rogue,
1545060	1546860	lone wolf actors,
1546860	1550980	then people would want the technology centralized.
1552500	1554500	This would be a similarity with nuclear weapons,
1554500	1557700	for instance, where we didn't want everybody
1557700	1560260	being able to make nuclear weapons.
1560260	1562500	We wanted to keep control of uranium.
1562500	1567500	And so what happened was we had a no first use
1567500	1570140	plus non-proliferation regime
1570140	1575140	and that kept the power in a few different peoples' hands.
1576180	1577260	I think there are things we could do
1577260	1580220	to reduce these sorts of risks
1580220	1583020	by creating institutions that are more democratic.
1583020	1584220	I think that seems useful.
1584220	1587460	I think decoupling the organizations
1587460	1590980	that has some of the most powerful AI systems,
1590980	1595980	having those more decoupled from the militaries
1596260	1597620	would be fairly useful
1597620	1600220	so that if something gets out of hand with line,
1600220	1604060	if they're linked and if we're needing to pull the plug
1604060	1605700	on these AI systems,
1605700	1607100	this isn't like taking down the military.
1607100	1611540	I think just separating this sort of cognitive labor
1611540	1615140	and or labor generally, automated labor
1615140	1620140	from a physical force would be fairly useful.
1621260	1623340	But I think largely it's creating democratic,
1623340	1625700	a democratic institutions is one of these measures.
1625700	1628180	In the case of dealing with rogue AIs
1628180	1632660	that are people maliciously instructed rogue AIs
1632660	1634660	that are proliferating across the internet,
1634660	1636020	I think there'd be other types of things
1636020	1641020	like legal liability laws for cloud providers,
1641980	1646980	that if you are running an unverified or unsafe AI system
1650020	1653180	on your cloud or on your compute,
1653180	1654980	then you get in trouble.
1654980	1657700	This would create incentives for them to keep track of it
1657700	1661060	instead of just doling out compute to whoever's paying.
1661060	1663060	So that's sort of like having an incentive
1663060	1665340	for off switches all over.
1665340	1669100	So there's a variety of different things
1669100	1674100	we could be doing to strike this balance
1675620	1677860	by reducing these malicious use risks.
1677860	1679660	I mean, also, as you mentioned,
1679660	1680780	some of these malicious use risks
1680780	1683060	don't require this type of centralization
1683060	1684060	or nearly as much.
1684060	1686060	We can do various things to reduce this risk
1686060	1689500	without giving tons of power to states.
1689500	1693260	For instance, we invest in personal protective equipment
1693260	1698260	or monitoring waterways for early signs of some pathogens.
1699820	1702580	I mean, there's the traditional stuff we can do
1702580	1704780	to reduce risks from pandemics, for instance,
1704780	1707540	which would reduce our exposure to the risk
1707540	1710580	of AI-facilitated pandemics.
1710580	1714500	So not all interventions for reducing malicious use
1714500	1716580	require more centralization.
1716580	1721020	I would imagine that we probably wouldn't want
1721020	1724060	in the long term, like say it's like 2040
1724060	1727140	or something like that, we wouldn't want anybody,
1727140	1729380	anywhere being able just to ask the AI system
1729380	1732420	how to make a pandemic or being able to unleash it
1732420	1735340	to try and take over the world.
1735340	1738340	This doesn't seem like a good idea.
1738340	1740860	There'd be other types of things like structured access
1740860	1742580	where for these bio capabilities,
1742580	1745460	you just give people who are doing medical research
1745460	1747380	access to the specific bio capabilities.
1747380	1749580	But other people, they don't really have much of a reason
1749580	1751660	for it, so they don't get that advanced,
1751660	1753300	they don't get models with that advanced knowledge.
1753300	1755180	So I think there are some simple restrictions
1755180	1757860	that we can do that can take care of a large chunk
1757860	1761780	of the risk without needing to hand over the technology
1761780	1763660	to like militaries, and then they're the only ones
1763660	1764500	who have it.
1764500	1768260	You mentioned legal liabilities for cloud providers
1768260	1770020	and maybe companies in general.
1770020	1773140	I wonder if this might be a way to have a form
1773140	1778140	of decentralized control over AI agents
1779620	1782820	or over large language models or generative models,
1782820	1786940	AI in general, by having the state provide a framework
1786940	1791060	for where you can get fined for trespassing
1791100	1794340	some boundaries, but then having companies
1794340	1796580	implement exactly how that works,
1796580	1801180	use technical tools in order to reduce their risk of fines
1801180	1805180	and maybe we can find a good balance there
1805180	1808500	where we weigh the costs and benefits.
1808500	1813500	I think that liability laws help fix the problem
1813660	1815820	of externalities quite a bit,
1815820	1818060	where they're imposing risks on others
1818060	1823060	that have no, shouldn't have any risk imposed on them
1823620	1826060	because they're not privy to the decisions
1826060	1828260	or there's an issue with that though,
1828260	1830700	which is that there's only so many externalities
1830700	1833820	that some of these organizations could internalize though
1833820	1834900	with liability law.
1834900	1838340	If somebody creates a pandemic
1838340	1840380	as a consequence of their AI system,
1840380	1841700	you could sue that company,
1841700	1844180	but they're not gonna be able to pay off
1844180	1847340	the destruction of civilization with their capital.
1847340	1849300	So there's quite a limit to it.
1849300	1851380	It can help fix the incentives,
1851380	1854220	but it still doesn't fix them entirely
1854220	1856780	because it's not particularly,
1856780	1858540	when certainly can't internalize like downfall
1858540	1860620	of like civilization as an organization
1860620	1862700	and like foot the bill for that.
1862700	1865500	And then the extinction of the human race is also,
1865500	1869180	I don't think that's the thing you could settle in court.
1869180	1871740	What about requiring insurance?
1871740	1873580	So this is an idea that has been discussed
1873580	1877820	for advanced biological research,
1877820	1880860	gain a function research with viruses, for example.
1880860	1883060	Maybe such a thing could also work
1883060	1887940	for risky experiments with advanced AI.
1887940	1890420	It depends if the harms are localized.
1890420	1893900	I think insurance and this taming of typical,
1893900	1896380	not long tail, not black swan type of uncertainty,
1896380	1899580	but thin tailed type of uncertainty
1899580	1901820	makes sense when risks are more localized,
1901820	1906260	but when we are dealing with risks that are scalable
1907700	1909900	and can bring down the entire system,
1910780	1914700	then I think a lot of the incentives
1914700	1916980	for insurance don't make as much sense.
1916980	1919900	So you basically need like some law of large numbers
1919900	1922180	and many types of insurance to like kick in
1923020	1926020	to sort of have that risk diversified away.
1926020	1929460	But if the entire system has exposure to that risk,
1930140	1932300	there's not another system to diversify it.
1932300	1933660	Maybe you could paint us a picture
1933660	1935460	of a positive vision here.
1935460	1938220	So say we get to 2050 and we've worked this out,
1938220	1940140	what does the world look like in a world
1940140	1942900	where we control malicious AI?
1942900	1946100	I think if people have access to these AI systems,
1946100	1950500	they're subject to, and they have many of their capabilities,
1950500	1951820	there are of course restrictions on them,
1951820	1955340	like you can't use them to break the law.
1955340	1957500	So a lot of these most dangerous capabilities,
1957500	1960660	nobody's really able to use them in that way.
1960660	1965660	If there is a need for, in the case of like defense,
1966020	1967820	they would end up using like AIs
1967820	1970020	for things like hacking and whatnot.
1970020	1973860	And that would, like they would have access
1973860	1974860	to that type of technology,
1974860	1978700	but it wouldn't be the case that any angsty teenager
1978700	1981100	can just download a model online
1981100	1983500	and then they instruct it to take down
1983500	1984420	some critical infrastructure.
1984420	1986220	This just isn't a possibility.
1987220	1989500	It's very much trying to strike a balance with that.
1989500	1991140	I would hope that we would also have
1991140	1992460	these most powerful AI systems
1992460	1995380	that do carry more of this force,
1995380	1997660	that have some of these more dangerous capabilities
1997660	2000300	are subject to democratic control,
2000300	2004620	so that power is not as centralized.
2004620	2007180	And that also I think reduces like the risk of like,
2007180	2010260	put in quote, like lock in risks as well,
2010260	2014460	where some individual group can impose their values
2014460	2016660	and entrench them.
2016660	2019860	So at least those are some properties
2019860	2022540	of a positive future.
2024020	2027420	So I don't think it looks like complete mass proliferation
2027420	2030420	of extremely dangerous AI products.
2030420	2033420	And I don't think it looks like only one group,
2033420	2036700	one elite aristocrat group gets to make the decisions
2036700	2039220	for humanity either.
2039220	2041700	So there's different levels of access
2041700	2043300	to different levels of lethality,
2044300	2048340	and to empower depending on whether it makes sense.
2048340	2051380	But the highest level institutions are still democratic.
2051380	2054060	Another category of risks that you discuss
2054060	2057100	is the possibility of an AI race.
2057100	2058700	Now, we've done another episode
2058700	2061300	where we talked about evolutionary pressures
2061300	2065580	and how they work between corporations
2065580	2068340	and how they might lead to a situation
2068340	2071780	which humanity is gradually disempowered.
2071780	2075060	But I think one thing we could discuss here in this episode
2075060	2079100	is the possibility of a military AI race.
2079100	2082260	What do you think a military AI race looks like?
2082260	2085620	To recap, we were just at the malicious use one.
2085620	2088540	And so now the other risk category would be like racing dynamics
2088540	2091180	or competitive pressures or collective action problems.
2091180	2093140	This is that structural environmental risk
2093140	2096300	that when we were referring to the categories way earlier.
2096300	2098340	Yeah, I think with the corporate race,
2098340	2101940	obviously there's, as we discussed in the previous episode,
2101940	2103900	there's them cutting corners on safety
2103900	2106700	and this is largely what AI development is driven by.
2106700	2108460	A lot of these organizations will start
2108460	2110740	as having a very strong safety bent,
2110740	2113420	but then they're basically gonna be pressured
2113420	2116340	into just racing and prioritizing the profit
2116340	2118340	and developing these things as quickly as possible
2118340	2120620	and staying competitive over their safety.
2120620	2123660	This is sort of the dynamic that basically drives
2123660	2124860	pretty much all these AI companies.
2124860	2126620	And I don't think actually in the presence
2126620	2127940	of these intense competitive pressures
2127940	2130220	that intentions particularly matter.
2130220	2135220	So I think basically this is the main force to look at
2136780	2139700	when trying to explain a major developments of AI,
2139700	2142620	why are companies acting the way they are?
2142620	2146620	It can be very well approximated by them
2146620	2150260	just trying to, by them succumbing to competitive pressures
2150260	2153020	or defecting in this broader collective action problem
2153020	2154300	of should we slow down
2154300	2157100	and should we proceed more prudently
2157100	2159100	and invest more in safety
2159100	2161100	and try and make sure our institutions are caught up
2161100	2165580	or should we race ahead so that way we can continue
2165580	2167060	being in the lead because one day
2167060	2169580	we'll maybe be more responsible with this technology.
2169580	2171900	I'm concerned, as mentioned in that previous episode
2171900	2175700	of that leading us to like a state of substantial dependence
2175700	2178060	and losing effective control,
2178060	2179660	you can imagine similar dynamic happening
2179660	2183340	with the military just like if we don't want,
2183420	2186260	arrows for instance, you're not gonna roll back arrows.
2186260	2187460	And so when you start going down the road
2187460	2190140	of weaponizing AI systems,
2190140	2192500	if they're more potent and cheaper
2192500	2194180	and more generally capable
2194180	2195780	and more politically convenient
2195780	2199420	and sending human soldiers onto the battlefield,
2199420	2204420	then this becomes a very difficult process to reverse back.
2205500	2209580	Eventually what happens is you've had an on ramp
2209580	2213140	to many more potential catastrophic risks.
2213140	2215620	You've transferred much of the lethal power.
2215620	2219500	In fact, the main source is the lethal power to AI systems.
2219500	2221660	And then you're hoping that they're reliable enough
2221660	2223140	and that you've sufficiently,
2223140	2225300	you can keep them under sufficient control
2225300	2226820	and that they can do your bidding.
2226820	2230700	Even if you do get them highly reliable
2230700	2233940	and they do what you instruct them to do,
2233940	2236460	this doesn't make people overall very safe.
2236460	2238460	We saw with the Cuban Missile Crisis,
2238460	2240620	we can definitely, nukes don't turn on us.
2240620	2242140	They don't go off and pursue their own goals
2242140	2242980	or something like that.
2242980	2247020	They do what we want them to do,
2247020	2249540	but collectively do this structural,
2249540	2252700	environmental game theoretic situation
2252700	2254420	where like, wow, we would all be better off
2254420	2255620	without nuclear weapons,
2255620	2260500	but it makes sense for us each individually to stockpile them.
2260500	2263180	We put the broader world at larger collective risks.
2263180	2265820	So like in the Cuban Missile Crisis,
2265820	2268380	JFK said we had up to like a half
2268380	2270540	or like a 50% chance of extinction in that event.
2270540	2272260	It was a very close call
2272260	2275380	because we almost got a nuclear exchange with that.
2275380	2278300	And likewise with AI systems, they may be more powerful.
2278300	2280220	They may be better at facilitating the development
2280220	2281660	of new weapons too.
2281660	2286220	And this could also bring us at a risk
2286220	2287980	where bring us in a situation
2287980	2292700	where we could potentially destroy ourselves again.
2292700	2296500	What's pernicious about this structural
2296500	2298860	or environmental constraint
2298860	2302900	where we've got different parties, in this case,
2302900	2307220	militaries competing against each other is the following.
2307220	2310140	Even if we convince the world
2310140	2313460	that like the existential risk from AI is like 5%
2313460	2316220	because let's say they're not reliable.
2316220	2318100	We can't reliably control them.
2318100	2320060	So maybe there's a 5% chance to like turn on us
2320060	2321060	or we lose control of them
2321060	2323580	and then we become a second class species or exterminate them.
2323580	2324540	Even if that's the case,
2324540	2327780	it may make sense for these militaries to go along with it.
2327780	2330100	Just like, I mean, they swallowed the risk
2330100	2332980	of potential nuclear Armageddon
2332980	2335860	by creating these nuclear weapons in the first place.
2335860	2338820	But they thought if we don't create these nuclear weapons,
2338820	2340700	then we will certainly be destroyed.
2340700	2342420	So there's certainty of destruction
2342420	2346580	versus a small chance of destruction.
2346580	2348540	And I think they'd be willing to make that trade off.
2348540	2351380	So this is how there could be an existential risk
2351380	2355140	to all of humanity based on these structural conditions.
2355900	2358420	So it's not enough to convince the world
2358420	2360420	that existential risk is high
2360420	2364980	because they might just, okay, well, yeah, that's 5%.
2364980	2366900	Okay, we're gonna have to go with that rational left thing.
2366900	2370420	It makes rational sense for us to engage in this,
2370420	2372420	what would normally be very risky behavior
2372420	2374140	because we don't have a better choice.
2374140	2377060	So this is why I don't think it makes sense
2377060	2379380	just to hammer home the point that, wow,
2379380	2380820	these AIs could turn on us
2380820	2382780	or we could lose control of them.
2382780	2384180	There's this structural thing of like,
2384180	2386340	that's not gonna matter unless that probability
2386340	2387220	is like very high.
2387220	2390140	Like maybe if it's like 30% and they go, okay, all right,
2390140	2392180	we're not gonna build the thing because,
2392180	2393980	but if it's something like 5%,
2393980	2395140	they might go through with it anyway.
2395140	2400140	So more than just concerns about single AI agents
2400620	2402860	make sense or make sense to focus on,
2402860	2405060	we have to focus on these multi-agent dynamics,
2405060	2406580	these competitive pressures,
2406580	2411100	the sort of the game theory of what they're facing.
2411100	2415260	And so I think that if you don't resolve that,
2415260	2417940	you're basically exposed to insensitivity
2417940	2422620	to a lot of existential risk up to maybe 5% or 10%,
2422620	2425020	which maybe it's possible.
2425020	2427380	Maybe it's actually only 2%.
2427380	2428580	And when you convince the world,
2428580	2430180	everybody's very educated about it.
2430180	2433060	Everybody listens to Future of Life podcast tomorrow
2433060	2434740	and they all go, wow, this is a concern.
2434740	2435900	I am updated to 5%.
2435900	2437020	Won't matter.
2437020	2438700	It won't stop that type of dynamic from happening.
2438740	2441300	You have to fix the international coordination issue.
2441300	2443460	You have to avoid this sort of potential
2443460	2444820	for World War III thing.
2444820	2446940	Now it didn't directly cause it,
2446940	2448740	as we were discussing earlier.
2448740	2450580	This wasn't a direct cause of extinction,
2450580	2452460	but it increased the probability substantially.
2452460	2454420	That's the sort of framing we have to focus on
2454420	2456140	in trying to reduce existential risk,
2456140	2457980	not search for direct cause of mechanisms,
2457980	2459260	but look at these diffuse effects
2459260	2460820	and structural conditions.
2460820	2463740	Yeah, so concretely, this might look like
2463740	2467460	the US is considering implementing AI systems
2467500	2470980	into their nuclear command and control systems.
2470980	2472740	So specifically, they're doing this
2472740	2477180	to counteract the rumors of other countries
2477180	2478500	doing the same thing.
2478500	2480220	And in order to act quickly enough
2480220	2481580	with their nuclear weapons,
2481580	2486580	they think they need to give AI a greater degree
2486980	2491100	of control over these nuclear weapons.
2491100	2495300	And so you have a situation in which
2495300	2499900	countries are responding to the actions of each other
2499900	2504900	in a way that accelerates risks from both sides
2505580	2507180	in this innocent.
2507180	2508020	There'd be one.
2508020	2511980	I mean, there are other ways this can affect warfare.
2511980	2516420	It could maybe be better at doing anomaly detection
2516420	2518660	thereby identify nuclear submarines
2518660	2521140	and affect the nuclear triad that way.
2521140	2522780	Or in later stages,
2522780	2526460	they just have massive fleets of AI.
2526460	2528300	And this is saying robot, sorry to say,
2528300	2530220	but like later stage,
2530220	2532060	if they're much cheaper to produce,
2532060	2534100	they'd be very good combatants.
2534100	2535740	There isn't skin in the game.
2535740	2537380	This increases the,
2537380	2539860	this makes it more feasible to get into conflict.
2539860	2540940	There are other ways in which
2540940	2542660	this increases the probability of conflict too.
2542660	2543620	There's more uncertainty
2543620	2546180	about where your competitors are relative to you.
2546180	2548140	Maybe they had an algorithmic breakthrough.
2548140	2551260	Maybe they could actually catch up really quickly
2551260	2554140	or surpass us by finding some algorithmic breakthrough.
2554140	2556580	This creates severe or extreme uncertainty
2556580	2559260	about the capabilities profile of adversaries.
2559260	2561340	This lack of information about that
2561340	2565700	increases the chance of conflict as well.
2565700	2569220	It may also increase first strike advantage substantially too,
2569220	2572540	which would also increase the probability of conflict.
2572540	2574660	Like we have an AI system today,
2574660	2576860	it's much more powerful than anything else.
2576860	2578020	They might get theirs tomorrow.
2578020	2581020	If we act today, then we can squash them.
2581500	2586420	That could get the ball rolling for some global catastrophe.
2586420	2591420	So yeah, pretty pernicious dynamics overall.
2592220	2594620	But all of these can be viewed as
2594620	2597180	competitive pressures driving AI systems
2597180	2601180	and propagating throughout all aspects of life.
2601180	2605900	We mentioned through the public sphere in the economy,
2605900	2608060	people's private lives with AI chatbots,
2608060	2610020	also in defense, in the military.
2610020	2611980	It just basically becomes everywhere
2611980	2614020	and we end up relying more and more on them
2614020	2615540	to make these sorts of decisions.
2615540	2617380	And I don't think in many of these,
2617380	2620980	we become so dependent on them that things move quickly.
2620980	2622180	We can't actually keep up.
2622180	2624140	We can't make, if we're actually making these decisions,
2624140	2625340	we'll make much worse decisions.
2625340	2628420	So then they basically become in effective control.
2628420	2631700	Things also move so quickly that the answer to our AI problems
2631700	2634420	is we need to bring in more AIs
2634420	2636260	because since they're using more AIs,
2636260	2637420	now we need to use more AIs.
2637420	2639500	And so it creates a self-reinforcing feedback loop
2639500	2641980	which ends up eroding our overall influence
2641980	2643900	and oversight as to what's going on.
2643900	2645340	And so I think that's the default one.
2645340	2646980	So of these sort of risk categories,
2646980	2650540	I think this seems like straightforwardly the case
2650540	2653220	if we don't fix international coordination
2653220	2656660	and if there's a close competition between countries
2656660	2660900	or if we don't fix the racing dynamics
2660900	2663580	in the corporate sphere,
2663580	2667620	then I think it's fairly likely that humanity becomes
2667620	2671220	at least like a second class species loses control
2671220	2674180	from there eventually, probably they go extinct,
2674180	2675660	but that might be a long time after.
2675660	2680660	But so this is the main risk that I'm worried about
2681140	2682940	but as Director of Center for AISAD,
2682940	2686260	I'll try and be acumenical and focus on various others too.
2686260	2687460	So I'm always making sure that our projects
2687460	2688300	addressing each of these though,
2688300	2691140	but personally, this is the one that I'm most concerned about.
2691140	2693060	So treaties between governments
2693060	2695180	and some form of collaboration
2695220	2697620	between the top AI corporations,
2697620	2699500	is that the way out here?
2699500	2701340	How do we mitigate this risk?
2701340	2703340	It seems at the way you describe it,
2703340	2705660	it seems very difficult to avoid
2705660	2708140	given the incentives basically.
2708140	2710220	People respond to incentives,
2710220	2712140	they rationally respond to incentives.
2712140	2715140	And so for each step along the way,
2715140	2717700	they have reasons to do what they're doing.
2717700	2719620	And so it seems difficult to avoid.
2719620	2721820	What are our options?
2721820	2726460	Well, there are positive signs.
2726460	2727980	For instance, like Henry Kissinger
2727980	2730260	was recently suggested in foreign affairs
2730260	2733860	that the US cooperate with China on this issue now,
2734860	2735940	but before it's too late.
2735940	2738300	So I think some people are recognizing
2738300	2742300	the importance of trying to do something about this.
2742300	2745820	There's, it's possible there'd be some clarifications
2745820	2748060	about antitrust law, which would make it possible
2748060	2751460	for AI companies to not engage in excessive competition
2751900	2754620	over this and put the whole world at risk.
2755540	2759260	Potentially there could be an international institution
2759260	2764260	like a CERN for AI, which is the default organization,
2767380	2772380	which has a broad consortium or coalition of countries
2773300	2776700	providing input to that and helping steer it.
2776700	2779380	One that's maybe decoupled from,
2779380	2781740	to some extent of militaries,
2782620	2784540	so that we're not having too much power centralized
2784540	2785380	in one place.
2785380	2786620	So it doesn't have a monopoly on violence
2786620	2788820	and eventually after automates a lot of monopoly on labor.
2788820	2791100	I think that's just like basically all the power
2791100	2791940	in the world.
2791940	2793180	So those are possibilities.
2793180	2796380	I think that the time window might be a bit shorter though.
2796380	2801020	If there's an arms race and AI arms race in the military,
2801020	2802940	and if the AI is viewed as like the main thing
2802940	2806340	to be competing on, like we need to spend a trillion dollars,
2806460	2810620	we'll spend on that order for nuclear weapons.
2810620	2812380	If when that becomes the case,
2812380	2814860	I think it's where we're very much set down that path
2814860	2817900	and then we're exposed to very substantial risks.
2817900	2821260	So yeah, I think maybe we'll have a sense
2821260	2824460	in the next few years as to whether we get some type
2824460	2826660	of coordination or if we are not gonna recognize
2826660	2828260	that we're all in the same boat as humans
2828260	2829580	and we don't want this to happen.
2829580	2832420	But we'll need people to basically understand what happens
2832420	2833980	if we go down this route and if we don't try
2833980	2838140	and fix the payoff matrix, the incentives at the outset,
2838140	2841820	the structure that these players find themselves in
2841820	2843860	or that these developers find themselves in.
2843860	2846980	That looks like a very much a political problem
2846980	2848300	as it happens.
2848300	2852220	So this is why making, reducing AI, X risk and whatnot
2852220	2855740	and making AI safe is a socio-technical problem.
2855740	2859740	It's not writing down an eight page mathematical solution,
2859740	2861340	a work of genius and then, oh, okay,
2861340	2863180	we can all go home now and everything's taken care of.
2863180	2864820	It's not gonna look like that.
2864820	2867700	That was a category error in understanding
2867700	2869260	how to reduce this risk.
2869260	2872900	We shouldn't have these types of founders effects
2872900	2876940	have like undue influence over, like it will keep lingering.
2876940	2878460	I think that will eventually like go away
2878460	2880060	but I still think it's still like lingering
2880060	2881860	and I think we should just like move past it
2881860	2884780	and recognize the complexity of the situation.
2884780	2887380	Let's talk about organizational risks
2887380	2890340	and these risk categories, of course,
2890340	2892620	kind of play into each other, influence each other.
2892620	2897620	So if we have organizations that are acting in a risky way,
2897940	2902780	that this increases the risk of potentially rogue AI
2902780	2906500	or it incentivizes others to race
2906500	2909620	in order to compete with these organizations
2909620	2913060	that are acting in risky ways.
2913060	2915380	But yeah, let's just take it from the beginning.
2915380	2920380	What falls under the organizational risks category?
2921380	2924780	Yeah, so organizational risks at a slightly more abstract level
2924780	2927340	would be the accidents bucket.
2927340	2932260	So even if we reduce competitive pressures
2932260	2936260	and if we have a,
2937500	2942500	and if we don't have to worry about malicious use immediately,
2943660	2946140	we'd still have the issue of organizations
2946140	2949100	having maybe a culture of move fast and break things
2949140	2952100	or them not having a safety culture.
2952100	2954940	In other industries or for other technologies
2954940	2958620	like rockets that wasn't extreme competition with that
2958620	2960820	but nonetheless, rockets would blow up
2960820	2963180	or nuclear power plants would melt down,
2963180	2965540	catastrophic accidents can still happen
2965540	2968300	and these can be very deadly
2968300	2970500	in the case of AI systems eventually.
2970500	2975500	So I think this is definitely a very hard one to fix.
2975860	2978260	Most of the people at these AI organizations
2978260	2980420	and how they were initialized and whatnot
2980420	2983500	still had a lot of people who are mostly just wanting
2983500	2988020	to build it and the consequences of society be damned.
2988020	2989820	This is not my wheelhouse, I don't read the news.
2989820	2991460	I don't like thinking about this sort of stuff.
2991460	2993900	This is annoying humanities majors and whatnot
2993900	2996580	who are in these ethics divisions or policy divisions
2996580	2998220	that keep annoying us.
2998220	2999980	This is kind of the attitude
2999980	3003220	that most of these companies buy in large.
3003220	3008220	And I think this is a large source of risk.
3008860	3013860	We could, as well as it's just non-trivial
3013940	3017500	as we see in other things like nuclear power plants,
3017500	3020100	chemical plants, rockets and making sure
3020100	3022700	that this is all extremely reliable.
3022700	3025900	So we'd need various precedents.
3025900	3027540	There's basically a literature on this
3027540	3030420	called the organizational safety literature
3030420	3034540	which focuses on various corporate controls
3034540	3036620	and processes for making sure
3036620	3039260	that the organization responds to failure,
3039260	3042460	takes near misses seriously, has good whistleblowing,
3042460	3044940	has good internal risk management regimes,
3044940	3048260	has like a chief risk officer or an internal audit committee,
3048260	3049460	all of these sorts of things
3049460	3051940	to reduce these types of risks.
3051940	3054460	And yeah, you were right in that this interacts
3054460	3056860	with not necessarily direct cause
3056860	3058380	of some of these existential risks,
3058380	3060380	but nonetheless boosts up the probability
3060380	3063220	if we're perceiving that an organization
3063220	3065700	is very reckless in its attitude.
3065700	3069460	This causes more safety minded ones to compete harder
3069460	3072260	and justify erasing.
3072260	3075740	This reduces the, that consequently reduces
3075740	3077980	the amount of time you have to work on control
3077980	3080900	and reliability of these AI systems,
3080900	3085220	which affects the probability of rogue AI's, of course.
3085220	3087540	There's also other types of accidents that could happen
3087540	3090300	like the organization might accidentally leak
3091300	3094580	one of its models that has some lethal capabilities
3094580	3096500	in it if it's repurposed.
3096500	3100740	There's also a risk of as potentially,
3100740	3105500	who's to say happened with viruses,
3105500	3108260	maybe there'd be some unfortunate gain of function research
3108260	3111980	that would also lead to some type of catastrophe as well.
3111980	3115460	There are people interested in what is essentially
3115460	3118220	gain of function research and in creating warning shots,
3118220	3121060	they might be a little too successful later on.
3121060	3124540	What does gain of function research look like in AI?
3124540	3126820	Deliberately building some AI system
3126820	3128900	that's like power seeking or Machiavellian
3128900	3130340	and wants to destroy humanity.
3130340	3132140	And then they're gonna use this to like,
3132140	3133300	you know, scare the world with,
3133300	3137660	but like at some point when it's powerful enough,
3137660	3139180	you might get what you asked for.
3139180	3142460	The idea here is to create a dangerous AI,
3142460	3145940	maybe an AI that's more agentic or power seeking
3145940	3150340	and then use that model to study how to contain it.
3150340	3154220	But then the worry is that we could ironically
3154220	3159220	go extinct perhaps because we can't control the model.
3161420	3162900	Yeah, and if this is like,
3162900	3166220	who's to say who's going to be experimenting with this
3166220	3167940	or how exactly cautious they will be
3167940	3169700	or their like skill level,
3169700	3173220	it may be mandated that they test for these types
3173460	3176860	of dangerous inclinations or capabilities
3176860	3179580	and who exactly is going to be doing that is unclear.
3179580	3182860	It may not be like the most like capable people
3184060	3186020	or there's just some overall
3186020	3189220	or there's just some risk of accidents in that way.
3189220	3191220	So I guess that gives some flavor
3191220	3192500	of some of the direct accidents,
3192500	3194900	but I also think how it indirectly affects things.
3194900	3197740	So one way in which I think strongly indirectly
3197740	3202580	affects things is when accident is an intellectual error
3202580	3203940	inside of these organizations
3203940	3207300	where they conflate safety and capabilities.
3207300	3209900	This is a very common thing
3209900	3211820	where there's not clear thinking about safety
3211820	3213900	and capabilities where people be,
3213900	3216740	oh, well, we're smart, you know, rational
3216740	3220340	and justify the means, we're risk neutral.
3221220	3224180	We actually don't actually do much empirical deep learning
3224180	3226380	research, but conceptually,
3226380	3229060	we think that this will be beneficial for safety
3229060	3230780	even though it will come at the cost of capabilities
3230780	3231620	and whatnot.
3231740	3233540	I'm muddied up that line.
3233540	3236820	And the distinction between safety and capabilities
3236820	3240580	such that you could imagine a lot of these safety efforts
3240580	3243580	basically just working on capabilities the entire time.
3243580	3246340	I think that's a reasonable fraction
3246340	3249380	of the safety teams I think do focus just on capabilities.
3249380	3253980	For context, there is an extreme correlation
3253980	3256140	between AI's capabilities
3256140	3258380	in various different subjects and goals.
3258380	3261180	So if you want your AI system to be better
3261420	3264540	at something like math problems
3264540	3269180	or history problems or accounting problems,
3269180	3272700	these capabilities are all extremely correlated now,
3272700	3275420	we can see with like large language models.
3275420	3278860	You should assume that if something is correlated,
3278860	3282740	like the correlation's like 80% or like 90%,
3282740	3284620	it's extremely high.
3285660	3288300	So when people reason themselves
3288340	3291380	into some new capability
3291380	3293780	that they think will be helpful for safety,
3293780	3295700	it's very likely the base rate of it
3295700	3297940	being correlated with capabilities
3297940	3300060	and basically being nearly identical
3300060	3302380	to other capabilities by being so correlated
3302380	3304180	is extremely high.
3304180	3306620	So I think there needs to be substantial evidence
3306620	3310380	that the safety intervention that one is applying
3310380	3312860	isn't affecting the general capabilities.
3312860	3315540	And that requires empirical evidence.
3315580	3318580	So a good example of empirical research
3318580	3320700	that I think helps with safety,
3320700	3323620	but doesn't clearly help with general capabilities
3323620	3324820	of making a system smarter
3324820	3328900	would be like the area of machine unlearning.
3328900	3331460	So machine unlearning is where you're trying to unlearn
3331460	3333060	some specific dangerous capabilities,
3333060	3334420	trying to unlearn bio knowledge,
3334420	3336020	trying to unlearn specific know-how
3336020	3337700	that allows you to hack.
3337700	3341460	This is more clearly like measurably not correlated with,
3341460	3342900	it's inter-correlated with some capabilities
3342900	3345020	and not particularly correlated with general capabilities
3345100	3346700	just removing that specific know-how.
3346700	3349060	I have to say robustness is also generally
3349060	3351620	inter-correlated with general capabilities.
3351620	3354500	It doesn't make the systems overall smarter.
3354500	3357980	What happens is it makes the systems robust
3357980	3360100	to some specific types of attacks.
3360100	3362180	Robustness to that comes at a fairly large
3362180	3365740	computational cost and takes up a lot of the model capacity.
3365740	3367140	But that would be a sort of,
3367140	3369620	that would be a safety intervention
3369620	3372420	that doesn't make the models overall smarter.
3372420	3374220	So those are examples of,
3374220	3375660	or I suppose another example would be
3375660	3377980	with transparency research.
3377980	3380420	Historically, there have been no instances
3380420	3382340	of transparency advancements
3382340	3386300	leading to general capabilities advancements.
3386300	3388820	Just trying to understand what's going on in the model
3388820	3390700	and it doesn't really work nearly as well
3390700	3393300	as just like throwing more data at it.
3393300	3395860	And there aren't many architectural improvements
3395860	3396980	that are likely to be found.
3396980	3399860	Anyway, as a result of these investigations
3399860	3401940	is the track record is pretty basically
3401940	3403740	completely clean for transparency.
3403740	3405740	Now, maybe that wouldn't be the case in the future,
3405740	3406580	but then at that point,
3406580	3408100	then we wouldn't identify this as something
3408100	3410420	that is particularly helping with safety.
3410420	3413580	So I think that for the safety research areas,
3413580	3416500	we need to be quite clear about there's,
3416500	3419460	you can't just have some informal argument about,
3419460	3420660	or an appeal to authority that,
3420660	3425660	oh, this is helpful for safety because of some verbal argument.
3426900	3430100	The empirical machine learning is very complicated.
3430100	3432340	Hindsight barely works in trying to understand
3432340	3433180	what's going on.
3433180	3435460	This pre-training on fractal images
3435460	3438500	help improve robustness to, I don't know,
3438500	3440140	basically everything and improve the calibration
3440140	3441300	and anomaly detection form.
3441300	3442860	I have no idea.
3442860	3445180	It works though, even ask people like,
3445180	3448820	why are activation functions the way they are?
3448820	3451380	I don't think there's actually a good canonical explanation
3451380	3452540	that's like very consistent.
3452540	3454180	You would want empirical evidence
3454180	3457340	that when we are engaging in safety research,
3457340	3460220	we are not accidentally also increasing
3460220	3461500	the capabilities of models.
3461620	3464300	And you think this is something that happens often?
3464300	3466380	Yeah, I think this happens extremely often
3466380	3468580	in this sort of, this organizational risk
3468580	3470820	of the conflation of safety and capabilities.
3470820	3473180	Now, this isn't to say that they are loose and separate.
3473180	3475020	A better improvements in capabilities
3475020	3477740	has downstream effects on safety in many situations.
3477740	3481340	It makes them better able to understand human values,
3481340	3484380	for instance, as they gain more and more common sense.
3484380	3486820	But if we are trying to improve safety
3486820	3489420	and specifically reduce existential risk,
3489420	3492140	I think we need to differentially improve
3492140	3493180	on some safety access
3493180	3495060	and not in the general capabilities access.
3495060	3497420	If we are doing something that's fairly correlated
3497420	3498820	with capabilities and safety,
3498820	3500940	I think that the default expectation
3500940	3503540	is that actually you're working in the service
3503540	3504940	of capabilities.
3504940	3508220	A good example would be one of OpenAI strategies
3508220	3510220	to mention this specifically,
3510220	3511820	because I just don't think it's particularly
3511820	3513900	intellectually defensible, I'm sorry to say.
3513900	3517220	I'm a more disagreeable individual, so here I go.
3517220	3521500	I don't think building a super human alignment researcher
3521500	3523380	specifically just affects alignment.
3523380	3526580	I think such a thing can be easily repurposed
3526580	3528860	to doing lots of other types of research.
3528860	3530740	I don't think there's like a specific
3530740	3532980	alignment research skill set that is just,
3532980	3534780	oh, it's just, you only get at that,
3534780	3536740	but if you're good at that,
3536740	3538220	it means nothing about your ability
3538220	3539220	to accomplish anything else.
3539220	3540140	I just don't think that's the case.
3540140	3542100	I think it's actually extremely correlated
3542100	3543260	with general capabilities.
3543260	3545500	It would be very straightforwardly repurposed
3545500	3547140	to other forms of research.
3547140	3549980	But that's an example of this sort of conflation.
3549980	3553420	Now, this isn't to say OpenAI is only
3553420	3555540	is conflating safety capabilities entirely.
3555540	3556380	I'm not claiming that.
3556380	3560740	They will have some work on transparency.
3560740	3563620	I gather that they'll work more on reliability
3563620	3568620	and robustness, but this is a very dangerous conflation.
3569300	3573300	And I think basically if they seem kind of correlated,
3573300	3575100	just intuitively, and then if you hear a lot
3575100	3576900	of verbal arguments without empirical demonstration,
3576900	3579140	basically assume just the base rates are like,
3579140	3581180	a lot of these completely separate subjects,
3581180	3583820	like performance in history and like,
3583820	3585820	performance in philosophy and mathematics,
3585820	3588340	like those are all like hyper correlated
3588340	3589740	to assume this other type of thing
3589740	3591560	is also hyper correlated with it too.
3591560	3593500	Anyway, though, that's another one that like,
3593500	3597060	an organizational factor that really reduces
3597060	3601100	the amount of time we have to solve this problem
3601100	3602860	and our ability to solve it as well.
3602860	3603700	So.
3603700	3607860	I think the worry here for, say you're a top AI company
3607860	3611300	and you're thinking, okay, how much a safe organization
3611300	3612540	features should we implement?
3612540	3614220	So should we have more red teaming?
3614220	3616460	Should we have more procedures, more review,
3616460	3619020	more testing?
3619020	3621700	Should we require this empirical evidence
3621700	3626620	before we begin a new safety research program?
3626620	3630020	This now threatens to slow us down
3630020	3634380	and it opens us up to competition
3634380	3636780	from the kind of scrappy new startup
3636780	3641500	that's on at our heels trying to outcompet us.
3641500	3644100	This is very straightforwardly now a case
3644100	3649100	of kind of AI race undermining organizational safety
3649140	3652500	or at least threatening to undermine organizational safety.
3652500	3656340	Can you make an argument if you were to sell this
3656340	3660260	to a CEO of an AI corporation that's...
3660260	3662580	When would I be in that situation?
3662580	3667180	That safety is in the interest of the organization itself.
3667180	3671940	You could say it's difficult to sell unsafe products, right?
3671940	3673140	You want to be in control.
3673140	3675980	You don't want to lose the weights of your model
3675980	3677180	in a leak and so on.
3677180	3679740	So there might be some correlation
3679740	3682660	between the self-interest of the organization
3682660	3686580	and the interest that society has in safety in general.
3686580	3689460	But before that, one additional factor
3689460	3691780	that just diffusely increases probability of extras
3691780	3693060	from these organizations,
3693060	3694620	if they just do safety washing
3694620	3696180	and they don't even know it sometimes,
3696180	3698620	they might have some small gesture for safety.
3698620	3700580	They might have, for instance,
3700580	3702180	a responsible scaling policy
3702180	3704780	that doesn't commit them to almost anything
3704780	3707980	and then that placates regulators, for instance,
3707980	3709580	but doesn't actually reduce risk.
3709620	3711980	Those would be other examples
3711980	3716180	of how organizational risks can end up
3716180	3718300	increasing the probability of existential risk.
3718300	3722020	Although it's diffuse and indirect, it still matters.
3722020	3724180	On the self-interest point,
3724180	3727500	I think a lot of the catastrophic risks
3727500	3730860	or catastrophic risks and existential risks are tail risks.
3730860	3734100	And generally organizations don't really price
3734100	3736660	in tail risks that much.
3736660	3738660	A lot of portfolios don't really do much
3738660	3741940	to address tail risks either, like in other industries,
3741940	3744140	like in finance and whatnot.
3745140	3747700	So this is kind of like a problem
3747700	3750420	with many of our institutions
3750420	3754060	that we could convince them to do things like red teaming,
3754060	3756340	to some extent, but doing red teaming
3756340	3759140	for existential risks and whatnot
3759140	3761300	is not necessarily something that they would check to do
3761300	3763540	because that's not going to affect their product tomorrow.
3763540	3766820	There's no pushback if everybody is dead,
3766820	3767660	as was mentioned before.
3767660	3769980	So I think that this works to a limit.
3769980	3773020	I think some things like saying information security
3773020	3776140	for your company or so that your weights don't leak.
3776140	3778620	This is a much easier argument to make.
3778620	3782060	Other claims like some of these internal controls
3782060	3783940	and whatnot, oh, this will slow us down.
3783940	3786420	This will reduce our velocity.
3786420	3788580	And I think these are harder to make.
3788580	3792340	And I don't think that there are necessarily short-term
3792340	3794300	economic incentives for some of these.
3794300	3797180	Many of these are actually more for addressing tail risks
3797180	3800460	and black swan events.
3800460	3803780	So they would then need to just recognize
3803780	3808260	that the black swan events are real possibilities
3808260	3811940	beyond a probability threshold worth actually addressing.
3811940	3815580	So I'm not claiming that they're being completely irrational
3815580	3819100	if they're being fairly short-sighted
3819100	3823620	and don't believe in these black swan events from it.
3823620	3825940	Then I think them trying to maintain velocity
3825980	3828260	and just maintain optionality and whatnot,
3828260	3830020	it's understandable.
3830020	3831540	I wouldn't advocate for that,
3831540	3833700	but it's understandable that they're doing that.
3833700	3838620	I think they're importing to a lot of their incentives well,
3838620	3840500	but, and they will do various things
3840500	3844260	to reduce some generic risk.
3844260	3846540	They will do some generic forms of red teaming,
3846540	3848580	regardless of whether there's regulation,
3848580	3849620	because it will make sense.
3849620	3851900	But I just don't think that that does particularly much
3851900	3854100	in the way of reducing these catastrophic
3854100	3855940	existential risks off.
3855940	3858780	Say you're a philanthropist or a government
3858780	3860420	with a big bag of money
3860420	3862820	and you want to incentivize safety research
3862820	3866020	at these top AI corporations.
3866020	3868500	Is there a way in which you could earmark the money
3868500	3871620	and make sure it's spent on what you want it to be spent on?
3871620	3874820	So it's not funneled into increasing capabilities
3874820	3875740	of the models,
3875740	3879460	it's spent on the right type of safety research.
3879460	3881500	I think that'd be one intervention.
3881500	3884580	I think it's very possible to,
3884580	3888300	there are a lot of professors lying around in academe
3888300	3891220	who could do this research.
3891220	3893660	All you need is to subsidize.
3893660	3895100	So for instance, like the Center for Safety
3895100	3897900	as a compute cluster, we'd love to expand it.
3897900	3901180	We're only able to support not that many professors
3901180	3904260	to do research with large language models
3904260	3906100	and very compute intensive experiments,
3906100	3908180	but there are a lot of professors
3908180	3911140	who could be doing more research here.
3911140	3912020	So I think that probably,
3912020	3913900	there aren't that many people working at these organizations
3913900	3914940	I should say as well.
3914940	3919940	And so I wouldn't bet on them to fix everything.
3920380	3922700	You're actually just correlating it with like,
3922700	3924660	what is Jared Copeland's safety vision?
3924660	3927100	What is Yann Leica's safety vision?
3927100	3929940	And like you're getting like two or three bets
3929940	3932780	if you were like giving each of them money.
3932780	3935220	And I think that's not a very diversified portfolio
3935220	3936300	and you should expect.
3936300	3940060	Blind spots just because people don't have,
3940060	3942020	can't simulate a collective intelligence,
3942020	3945220	a broad research effort by themselves,
3945220	3947380	even if they work very hard and have lots of discussions
3947380	3950740	and take out, have good deference to outside views
3950740	3951580	and so on, they just can't,
3951580	3953180	they just can't simulate that function.
3953180	3955580	So I would suggest if one's wanting
3955580	3958180	to subsidize safety research,
3958180	3961540	we can, if there's can have subsidize like a compute cluster,
3961540	3963420	then we can have high accountability of like,
3963420	3964660	you're not allowed to run this project
3964660	3967340	because this doesn't seem sufficiently safety related
3967340	3968540	instead of giving like money, you know,
3968540	3971100	strings attached to some academics and they run off with it.
3971100	3974220	So that would be my preferred intervention.
3974220	3976060	And I think that there's,
3976060	3977500	it can take orders of magnitude more.
3977500	3979220	So like, if any of them are listening,
3979220	3980180	you know, like reach out to us,
3980180	3982580	I'd love to get more compute to,
3982580	3985500	to people doing relevant,
3985500	3987900	doing relevant research on safety
3987900	3989940	in a nice diversified portfolio.
3989940	3992100	Across transparency and adversarial robustness
3992100	3995140	and back doors and machine learning models
3995140	3998500	and done learning, these types of topics.
3998500	4002260	Do you think some safety breakthroughs would be kept secret?
4002260	4006260	Say a safety breakthrough at Google DeepMind,
4006260	4011260	made the AI useful or in a way that incentivize them
4011700	4013220	not to share the safety breakthrough?
4013220	4015580	Or would you expect safety breakthroughs
4015580	4019740	to be shared widely as if they were found
4019740	4022020	in an academic lab?
4022020	4024020	I think for market positioning,
4024020	4027660	one of them could occupy the niche of being the safest
4027660	4029380	of the racing companies.
4029380	4032740	We are technically the safest.
4032740	4036380	So I think that's currently occupied by Anthropic
4036380	4038660	and this might make it fairly useful
4038660	4042100	when pitching themselves for say a defense contract
4042100	4045500	that look weird to the more reliable organization
4045500	4047220	compared to our competitors.
4047220	4050020	And so if they would be open sourcing some of that,
4050020	4053860	then I think they would lose some of that competitive advantage.
4053860	4058780	So it's quite conceivable that they'd hold on to things.
4058780	4060940	I mean, there's many safety projects they do
4060940	4063540	for which the code is not like open source.
4063540	4064660	So we see that to some extent there,
4064660	4067740	but I think it can make sense for one of them to try
4067740	4069700	and just be a bit safer than the others
4069700	4071860	or a bit more reliable than the others.
4071860	4075500	Yeah, I've heard Sam Altman, the CEO of OpenAI,
4075500	4077460	talk about releasing these systems,
4077460	4081660	so specifically releasing GPT-3 and 4 to the world,
4081660	4087020	to chat GPT in order to gather more attention to the issue.
4087020	4090180	Do you think this is a viable strategy?
4090180	4094500	Is this too risky or is it worth trying?
4094500	4096860	I think to answer a more extreme question
4096860	4099260	to possibly get a sense of my position on this,
4099260	4102260	I think the release of Llama 2, for instance,
4102260	4104140	by Meta, which is an open source,
4104140	4107940	large language model around the capacity of GPT-3.5,
4107940	4113220	I think the benefits of that actually outweigh the costs.
4113220	4115220	It enables a lot more research.
4115220	4118100	It also improves our defenses
4118100	4120620	against some of the immediate applications
4120620	4121620	of these AI systems.
4121620	4125940	So for instance, it came out today that North Korea
4125940	4128060	is using some of these AI systems.
4128060	4130300	I don't know whether it's Llama 2, that'd be my guess,
4130300	4133180	because it's just the most capable open source system
4133180	4135340	or code Llama potentially,
4135340	4138500	using AI systems to identify vulnerabilities in software,
4138500	4140940	and then that helps them shortlist things to attack.
4140940	4143820	This isn't an extremely capable,
4143820	4146820	or this doesn't rewrite the cost-benefit analysis
4146820	4147820	of cyber attacks.
4147820	4151380	It doesn't rupture our digital ecosystem,
4151380	4154060	but this basically gives us some preview
4154060	4159780	and forces these issues on people's attention.
4159780	4163180	So I think there's an argument to be made
4163180	4169060	for open sourcing Llama 2, or if it's trained on 10x more
4169060	4171780	compute Llama 3.
4171780	4174580	After that, there's more uncertainty we'd
4174580	4178700	have to see, because maybe it could be repurposed for things
4178700	4181340	like bio weapons then, or it would be substantially more
4181340	4184180	capable at hacking and scamming, things like that.
4184180	4186780	I think there's a real argument to be made
4186780	4190540	for some short-term stressors snapping the system
4190540	4194500	into to do something about it, or at least waking them up.
4194500	4196860	But I think systems function better
4196860	4198980	with some amount of stressors, when the stressors get too
4198980	4201420	extreme, then it can undermine the system.
4201420	4202220	So it's complicated.
4202220	4204780	I mean, I think maybe the situation, the case of opening,
4204780	4207940	I really see these things, or how they want to go about release
4207940	4211780	strategies would not surprise me if that should change,
4211780	4215140	or if it would be better to do other things in the future.
4215140	4215740	Yeah.
4215740	4218140	Do you know something about the internal processes
4218220	4221060	for deciding when to release these models?
4221060	4223660	So in the case of Meta, for instance,
4223660	4228300	they may have a chief legal officer vote on whether
4228300	4230580	or potentially a veto power that could still
4230580	4233860	be overwritten by the CEO, which may have happened,
4233860	4237700	in the case of Llama 2, being suggestive here,
4237700	4239620	because I have to have a second source for it.
4239620	4243780	But usually for decisions, though,
4243780	4247620	OpenAI will be accountable to their board.
4247620	4249620	I don't know whether they have formal powers
4249620	4252060	to decide whether they'd be voting.
4252060	4256740	Often boards have blunt powers of just firing the CEO.
4256740	4259460	And there often aren't processes in place
4259460	4263060	for these larger-scale decisions.
4263060	4266340	So you could imagine a CEO just deciding unilaterally
4266340	4268900	to have something released.
4268900	4271220	And that's something that organizational safety could
4271220	4273380	improve and be processes for high-stakes decisions,
4273380	4274100	as an example.
4274100	4274780	But yeah.
4274820	4278260	But by default, boards do not have fine-grain control.
4278260	4282220	And so it's often up to the CEO to make the call.
4282220	4283900	So you have a single point of failure.
4283900	4288460	What is the Swiss cheese model of organizational safety?
4288460	4291020	Yeah, so I'm mentioning, and if people
4291020	4295180	are wanting to hear more about the organizational safety
4295180	4299620	literature, we'll have the AI safety ethics and society
4299620	4302020	textbook out in November.
4302020	4305340	And one of the chapters would be on safety engineering.
4305340	4309420	The Swiss cheese model is easy to communicate.
4309420	4310500	It's kind of outdated.
4310500	4313340	But it gets at a, just like how people
4313340	4316780	are doing analysis of existential risk from AI earlier,
4316780	4320700	they'd have a toy model that captured some of the and some
4320700	4322180	of the scenarios to be concerned about.
4322180	4324380	But it's important not to let that be the lens by which
4324380	4325380	you filter everything through.
4325380	4327740	That captures some of it, but not all of it.
4327740	4329980	And the Swiss cheese one captures some of the dynamics,
4329980	4330860	but not all of the dynamics.
4330860	4333980	But anyway, the Swiss cheese model with that probably
4333980	4337700	would decide essentially having multiple layers of defense.
4337700	4340780	If you have red teaming, even red teaming
4340780	4345620	for a catastrophic risk, that reduces the risk of catastrophe.
4345620	4347740	But it's not itself perfect.
4347740	4352140	You might also want stronger informational security, too,
4352140	4355780	to make sure that if you had a dangerous model that it
4355780	4360580	doesn't leak, you could have better
4360620	4365420	transparency tools to check for deceptive behavior in AI systems.
4365420	4367180	But if those transparency tools failed,
4367180	4369980	maybe you would want monitoring of these AI systems
4369980	4372500	so that before they take any action,
4372500	4375820	it needs to be approved by something equivalent
4375820	4378380	to an artificial conscience or filter
4378380	4381420	that would filter out some of the immoral actions of AI agents
4381420	4382900	before they're able to take them.
4382900	4385340	And so all of these together can increase
4385340	4387700	the reliability of the system.
4387700	4391420	So the hope is that if you stack together
4391420	4394660	many of these, you've substantially reduced your risk.
4394660	4397460	This isn't looking for a perfect airtight solution.
4397460	4401020	This is looking for layering on many different defenses
4401020	4401860	to actually reduce risk.
4401860	4403820	So if I wanted to reduce by a risk, for instance,
4403820	4407300	here's an example of a Swiss cheese thing.
4407300	4410980	First, there'd be the diffuse thing.
4410980	4413540	And maybe there could be some regulation about not allowing
4413540	4414540	models with these capabilities.
4414540	4416060	But let's say I'm an organization that
4416060	4417140	takes safety more seriously.
4417140	4418740	So that depends on safety culture.
4418740	4420700	So that's some sort of barrier.
4420700	4422700	Regulation might be some barrier against this risk.
4422700	4424780	Safety culture might be some barrier against this risk.
4424780	4426620	So then they have enough of a safety culture
4426620	4428780	they're willing to add a lot of these safety features.
4428780	4430220	Now, these safety features themselves
4430220	4433380	will end up having lots of different layers of defense.
4433380	4437140	You could have an input filter to try and remove
4437140	4439340	whether there's a request to create a bio weapon.
4439340	4442260	You could also remove virology related data
4442260	4443540	from the pre-training distribution
4443540	4446460	so that it likely knows a lot less about virology.
4446460	4449780	You could have an output filter as well,
4449780	4452180	which would, even if somebody jail breaks the input filter,
4452180	4453780	then they're also going to need a jail break
4453780	4456180	the output filter, which is harder to do.
4456180	4459220	And you could imagine adversarily training this as well.
4459220	4461780	So it'd be another layer so that it would be more robust
4461780	4465100	to people trying to jail break those layers of defense.
4465100	4468420	But then you also have, there's also people
4468420	4470380	who could, through the API, fine tune the model
4470380	4473700	and inject some of that bio knowledge back into the model.
4473700	4478700	So you could have a filter that screens the fine tuning data
4479060	4482180	so that that information can't get back into the weights.
4482180	4483900	And then you could add another layer,
4483900	4485260	which would be an unlearning layer
4485260	4488220	where you would assume that before you hand back
4488220	4490180	the fine tuned model to the user,
4490180	4492260	before they get it back, we're going to run a scrubbing,
4492260	4494460	unlearning, knowledge expunging thing
4494460	4497580	to expunge some of any bio knowledge if there is any.
4497580	4499220	And that would be yet another layer.
4499220	4503180	This approach reduces the risk of some bio catastrophe.
4503820	4505220	Are any of those airtight?
4505220	4506340	No.
4506340	4507580	But do they work better collectively?
4507580	4508420	Absolutely.
4508420	4511300	So this is why we shouldn't be focusing
4511300	4513220	on these airtight solutions.
4513220	4515540	Exclusively, we also need to make use
4515540	4518300	of these various layers of defense.
4518300	4520540	That's how we actually reduce the probability
4520540	4522620	of existential risk.
4522620	4524900	We can't let perfection be the enemy of the good.
4524900	4528020	If we'd say, well, if we can't build a completely
4528020	4529740	100% reliable input filter,
4529740	4531380	then we shouldn't have an input filter.
4531380	4533020	That's a dead end, so we shouldn't investigate it.
4533020	4535620	That's just not how things work.
4535620	4537180	Tell us more about the textbook.
4537180	4539100	I'm pretty excited to read this.
4539100	4542780	I hope that this is a product that should exist, I think.
4542780	4545540	Specifically, tell us more about how do you think
4545540	4547580	about updating this or keeping it up to date?
4547580	4550180	I think for a textbook on AI safety,
4550180	4553500	it won't probably work if the next version is out
4553500	4557740	in 2034 or something like that, right?
4557740	4559500	So how do you keep it up to date?
4559500	4562180	And also, you can just present the textbook,
4562260	4564340	which I think listeners will be interested in.
4564340	4567620	I mean, since I've been around in academia for a while,
4567620	4570900	I do have at least some of a sense of what things,
4570900	4573700	what content is more likely to stand the test of time.
4573700	4577300	So that one's not talking about Dolly 2 or something,
4577300	4578900	which is already outdated,
4580100	4581980	or what are kind of like fad topics
4581980	4584980	and not giving those too much,
4584980	4586780	not giving those airtime.
4586780	4589060	I mean, an example of this would be an unsolved problems
4589060	4591420	that I'll say two, three years ago or something,
4591420	4595660	but there we introduced emergent capabilities,
4595660	4598500	which I think has become fairly popular
4598500	4602260	before Burns et al's paper on honesty and whatnot.
4602260	4604620	We're also, honesty is a big part of alignment.
4604620	4606980	So there's sometimes one needs to call the shots too
4606980	4608380	as to what things will,
4608380	4612380	even if there aren't, isn't much of a literature on it at all,
4612380	4615660	need to predict what will end up standing the test of time.
4615660	4618300	But so I think it should have some reasonable longevity
4618300	4620460	because we're not focusing on transient knowledge,
4620460	4624860	but instead like general interdisciplinary frameworks
4624860	4626980	we're thinking about risk across all these sectors.
4626980	4629220	Cause we had this issue of like, there's,
4629220	4630340	if you're thinking about AI risk,
4630340	4632660	you have to think a bit about geopolitics.
4632660	4634220	You have to think about international relations
4634220	4635060	to some extent.
4635060	4635900	You think about AI risk,
4635900	4637100	you have to think about corporate governance
4637100	4639600	and AI developers and what sort of incentives
4639600	4641060	are driving them.
4641060	4644180	You have to think about the individual AI systems themselves
4644180	4646820	too, you have to think about organizational safety.
4646820	4651820	You have to think about broad variety of factors
4652220	4654900	and we'll basically focus quite a bit on frameworks
4654900	4658900	for thinking clearly about each of those.
4658900	4661340	I would imagine that later one could have, you know,
4661340	4665180	GPT-6 like help like update the textbook anyway.
4665180	4669460	So honestly, it's actually like the plan first.
4669460	4671500	Something in that direction.
4671500	4672900	How technical is the book?
4672900	4676940	Does it contain pseudocode like a standard AI textbook?
4676940	4680660	The premise of it is to onboard people
4680660	4681940	from different disciplines.
4681940	4684340	This isn't written for machine learning PhD people.
4684340	4685700	There are lots of different fields,
4685700	4687900	economists, legal scholars, philosophers,
4687900	4690780	people without technical background, policy makers,
4690780	4695780	think tank people who want more of a systematic understanding
4695780	4697020	of these issues.
4697020	4699980	And so it's largely written for people
4699980	4702220	without any specific background
4702340	4704980	and it's not trying to be a sort of like
4704980	4707740	a introductory machine learning PhD course.
4707740	4710580	That would be the course.mlsafety.org
4710580	4714060	if you want a course of various technical topics
4714060	4715900	or the machine learning safety course.
4715900	4720020	But this one is more focusing on, you know,
4720020	4722420	as we were discussing the game theory of this,
4722420	4725140	the various governance solutions.
4725140	4729780	Conceptually, many of the arguments associated
4729780	4731860	with rogue AIs, why might they be power systems?
4731980	4733620	Maybe they're deceptive.
4733620	4734780	Understanding that.
4734780	4736540	There's also introduction to machine learning
4736540	4738180	and reinforcement learning in it.
4738180	4740780	Understanding collective action problems
4740780	4742180	since that was fairly relevant
4742180	4745980	and these competitive pressures.
4745980	4748620	There's also ethics in the book as well
4748620	4749780	where if you're assuming
4749780	4752180	that you've got your AIs systems to be somewhat reliable,
4752180	4753820	then we have to start worrying about
4753820	4754900	making it beneficial.
4754900	4759900	And so there's various bits of AI ethics
4760900	4764620	as well of what are objectives
4764620	4766460	that we might give the AI system.
4766460	4767580	What would those look like?
4767580	4768940	What would be some of the, you know,
4768940	4770300	moral trade offs that you're making there?
4770300	4775300	But so it's covering AI safety, ethics, and society.
4775540	4779220	So trying to be fairly broad.
4779220	4781780	You should have lecture slides.
4781780	4785460	And presumably I'll get around to recording videos for two.
4785460	4786860	The goals, there's several goals of it,
4786860	4788260	like to compress the content.
4788260	4789900	Right now, if you want to understand AI risk,
4789900	4792220	basically need to be part of an intellectual scene,
4792220	4793740	like in the Bay Area probably.
4793740	4795980	Maybe and maybe somewhat an Oxford.
4795980	4797900	So very high barriers to entry.
4797900	4799380	And then if you do,
4799380	4801580	you're probably going to take a somewhat narrow view
4801580	4805580	just because they're all interested in rogue AIs
4805580	4808420	and don't have as much interaction
4808420	4810420	with the rest of the world.
4810420	4815420	So you'll have many blind spots as to a lot of it.
4815420	4816540	There's the social variables
4816540	4818420	and the broader socio-technical problem.
4818420	4820780	The knowledge has been a bit diffused
4820780	4822940	across various different blocks.
4822940	4825180	And to stay up to date,
4825180	4827660	you've often had to jump around from different places.
4827660	4829340	So it would be nice to have something
4829340	4831380	that's more compressed.
4831380	4834580	So some of the goals are to reduce the fragmentation
4834580	4837540	of AI risk knowledge, increase the readability,
4837540	4840740	and the sort of compression rate of this content.
4840740	4843020	And so there's reducing the barrier to entry
4843020	4845020	to these crucial ideas
4845020	4847540	that should hopefully scale the number of people
4847540	4850180	who can understand AI risk extremely quickly.
4850180	4852500	I was somewhat surprised by,
4852500	4854660	although there's a lot of global attention,
4854660	4856500	the number of new experts flooding in
4856500	4859180	has been, I think, very underwhelming.
4859180	4860420	Is that good or bad?
4860420	4863860	Sometimes it's a bad thing if experts are rushing
4863860	4867860	into the new, newly hot idea.
4867860	4871220	I think that if people are onboarded well
4871220	4874740	and have a more comprehensive understanding,
4874780	4876900	if they're basically like charlatans
4876900	4878580	who aren't going to do their work,
4878580	4879860	then that's more of a problem.
4879860	4881980	So I think by default,
4881980	4886220	with another capabilities jump or two, they will flood in.
4886220	4887900	There's basically a question,
4887900	4890700	and I don't anticipate they're going to read lots of
4890700	4894540	lesswrong.com posts to be onboarded.
4894540	4895820	They're just gonna start talking
4895820	4898100	and trying to be about themselves.
4898100	4903100	I say this in my time, empirical machine learning research,
4903540	4905220	it basically should assume
4905220	4908420	that when some area starts getting pretty hot,
4908420	4911700	there'll be lots of random new people coming in
4911700	4916540	and trying to influence the discussion substantially.
4916540	4918220	Hopefully the people as they come in
4918220	4921900	would have some understanding of many of the basics, though,
4921900	4924820	but I think by default, it's relatively inaccessible.
4924820	4927220	You'll have to read a lot of scattered content
4928340	4929340	from different places,
4929340	4930740	and a lot of it will be idiosyncratic,
4930740	4932580	and it'll just take a long time to go through.
4932580	4936100	Those are some of the reasons for doing this.
4936100	4940420	And then also, I think that given that rogue AIs
4940420	4945020	is not the only concern or only risk source,
4945020	4947340	there's a lot of content that even a lot of people
4947340	4949020	who've been thinking about AIs risk for a while
4949020	4952440	will possibly need to become aware of.
4953580	4957340	So that's why, so just as a grad student,
4957340	4961260	when I just developed and just focused
4961820	4964460	on these other things other than rogue AIs,
4965380	4967260	and then now I think people are recognizing
4967260	4968300	the importance of that,
4968300	4969500	so now there'll hopefully be,
4969500	4971740	or so now there'll be some material
4971740	4974580	to help get a more formal understanding
4974580	4977060	of these other sorts of issues.
4977060	4978940	That's great, I'm looking forward to reading it.
4978940	4981700	I think we should nonetheless talk about rogue AIs.
4981700	4984620	That's your last category of risk.
4984620	4987580	One issue here is proxy gaming.
4987580	4989100	How does that work?
4989100	4990540	How is it dangerous?
4990540	4992060	Yeah, so you could imagine
4992060	4994100	if you've got a very powerful AI system,
4994100	4998420	if it finds reliability holes in the objective
4998420	5001540	that it's given, then this could be destructive
5001540	5004220	because it's being guided by a flawed objective.
5004220	5009220	I think in a colloquial example is with believe in Hanoi,
5011220	5015500	there'd be a bounty for killing rats.
5015500	5017860	And so if you get the rats, you get a bounty,
5017860	5021340	but then people were incentivized to breed rats
5022380	5023620	so as to collect more of that bounty.
5023620	5025420	That would be an example of an objective
5025420	5028260	that you put forward that ends up getting gained.
5028260	5032060	It's fairly difficult to encode all of your values
5032060	5035700	like well-being and whatnot into a specific objective,
5035700	5038620	a simple objective.
5038620	5040940	So you might expect some approximation
5040940	5042740	or to what you actually care about.
5042740	5045220	And in machine learning, a famous example,
5045260	5048620	this is the boat racing or coast runners example
5048620	5052340	that OpenAI had, which was of proxy gaming,
5052340	5054980	of there's a reward function
5054980	5057100	and the reinforcement learning agent
5057100	5058660	would optimize that reward function.
5058660	5059740	It was, this was a racing game.
5059740	5061860	You'd think it would optimize the reward function
5061860	5063100	by going around the track,
5063100	5064420	but what it instead learned to do
5064420	5065540	was it can get a higher reward
5065540	5067220	by getting lots of turbo boosts.
5067220	5072220	And the turbo boosts, it could get a very rapid sequence
5072500	5074580	of them by crashing into walls
5074580	5075540	and catching on fire
5075540	5078380	and then continually turbo boosting in that way.
5078380	5080220	And that would help it get a higher score.
5080220	5082900	So there are often holes in these objectives
5084260	5088660	due to an ability to compute exactly the right objective
5088660	5093380	or maybe we can only monitor some parts of the system.
5093380	5096060	There's a computational and spatial
5096060	5098420	and temporal constraints on the quality of the objective,
5098420	5099980	meaning that you're gonna often have to go
5099980	5100860	with an approximation.
5100860	5102860	So it's something perfectly ideal.
5102860	5105060	This relates to Goodhart's law,
5105060	5107140	which works in human domains also
5107140	5111220	in which it's difficult to specify exactly what it is you want.
5111220	5114820	And whenever you specify something you want,
5114820	5119300	that thing you've specified is now open to being game.
5119300	5121340	So an example here might be
5121340	5124180	that you want deep scientific insight
5124180	5127660	and you assume that such insight correlates
5127660	5130500	with citations or number of citations.
5130540	5134780	But then you get gaming of the citation systems
5134780	5137300	in which academics are incentivized
5137300	5141300	to maximize citations at the cost of scientific insight.
5141300	5144140	So is this a more general problem
5144140	5148500	across all agents, humans included?
5148500	5150820	Yeah, yeah, I don't think this is specific to,
5150820	5153740	I don't think this is specific to AI agents.
5153740	5156980	I will say that some objectives are harder to game than others.
5156980	5159700	For instance, the bounty on rat tails
5159700	5162140	is a lot easier to game than like citations
5162140	5164980	because citations can be very valuable for getting,
5164980	5167060	you know, emigrate if you're getting a green card,
5167060	5169940	for instance, and it's a strong incentive to do it.
5169940	5174940	And, but it's nonetheless challenging.
5175220	5176700	So some of these objectives,
5176700	5178740	even when people are trying very hard to game,
5178740	5181300	they still can be correlated with a lot,
5181300	5183860	like college admissions still focuses,
5183860	5185420	incentivize people to be productive.
5185420	5188660	Yes, they'll go overboard in studying for the exams
5188660	5190820	and whatnot, the college admissions tests,
5190820	5193300	yes, they'll go overboard in the number of extracurriculars
5193300	5195260	and whatnot, but I still think it like,
5195260	5197300	it doesn't can help shape compared to
5197300	5198980	they're not being the incentive in the first place.
5198980	5201740	I think overall my take on the GoodHeads Law
5201740	5204140	is that there's some objectives that are,
5204140	5208420	or some goals are all goals and proxies are wrong.
5208420	5210900	Some are useful and some though, when gained
5210900	5213740	in particular ways could be potentially catastrophic.
5213740	5215860	So there's quite a variety.
5215860	5217620	There are some objectives as well
5217620	5221300	that people would claim would produce good outcomes.
5221300	5223340	For instance, if you gave an AI an objective,
5223340	5226540	like make the world the best place it can.
5226540	5229620	And if that was actually the objective you gave it,
5229620	5231860	okay, that's quite different from like,
5231860	5236860	make people very engaged with this product.
5238740	5240300	That's quite different.
5240300	5245300	I think that making these proxies incorporate more
5245660	5248860	of our values becomes more possible across time
5248860	5251540	because the systems can represent these other sorts
5251540	5255460	of notions of say, wellbeing of autonomy
5255460	5259100	because they have a lot better of a world model
5259100	5262540	and more of an understanding of people as well.
5262540	5266180	However, so I think that getting objectives
5266180	5269140	that are in the right direction seem possible.
5269140	5273940	The issue is making them be robust to adversarial pressure.
5273940	5276060	I'm not as concerned about like,
5276060	5277620	we get telling AI go cure cancer
5277620	5278740	and then it does something like,
5278740	5281580	oh, I'll give lots of people cancer to experiment on them
5281580	5284100	to speed up the experimentation process.
5284100	5286580	This is easily ruled out by some like objective
5286580	5287940	with like an interpret the request
5287940	5290340	as a reasonable person would.
5290340	5293380	This is a fairly new development in AI
5293380	5296220	that we now have these large language models
5296220	5299860	that can at least to some extent understand common sense
5299860	5304060	and have kind of a more subtle understanding
5304060	5305940	of human values.
5305940	5310940	Yeah, earlier there'd be the AI's they would be kind of
5310940	5313100	like savants where they understand
5313100	5315380	some particular thing well, but then nothing else.
5315380	5318740	And, you know, human values are so late
5318740	5322020	in the evolutionary process and suggested they're very late
5322020	5325180	to be one of the last things that AI's learn.
5325180	5327460	But that fortunately wasn't the case.
5327460	5331340	We explored this a few years ago in the paper
5331340	5332740	with the ethics data set.
5332740	5335180	We're basically using that to show that look,
5335180	5336020	they've got understanding
5336020	5339260	of various morally salient considerations.
5339260	5340540	Here's their predictive performance
5340540	5341620	on like well-being things.
5341620	5345020	Here's their understanding of deontological rules
5345020	5347820	and notions in justice and fairness,
5347820	5350820	such as whether people get what they deserve
5350820	5353300	or whether people are being impartial.
5353300	5355980	So they have an understanding of a lot
5356980	5359420	of morally salient considerations.
5359420	5361380	There is a question of reliability though,
5361380	5363300	if they're optimizing that objective,
5363300	5366180	are they basically, is that objective
5366180	5369500	succumbing to that adversarial optimization pressure?
5369500	5372220	If it's optimizing it, it's basically functionally similar
5372220	5374860	to it being adversarial to that objective.
5374860	5378420	This is why there's a focus on adversarial robustness
5378420	5382380	because later we would have, we've got an AI agent
5382380	5384540	that's optimized, it's given a goal
5384540	5386900	and this AI system is outputting
5386900	5388900	whether it's succeeding by the goal or not.
5388900	5390540	So we've got an AI evaluator
5390540	5392540	and we've got an AI system that's optimizing that goal.
5392540	5394660	This AI evaluator, you don't want that being game.
5394660	5397740	You want that AI evaluator being adversarily robust,
5397740	5400780	robust to optimizers trying to say
5400780	5401980	that it's doing a good job.
5401980	5405220	So that's the sort of threat model later stage
5405220	5407160	and that's how some of these topics
5407160	5410660	that were explored in vision and whatnot end up
5410660	5414060	and now finally with the large language models,
5414060	5416380	the tax paper, which I guess read about that
5416380	5420340	in the New York Times where jailbreak
5420340	5423780	and manipulate these models with little adversarial suffixes.
5423780	5427140	In a later stage, we'd have AI systems evaluating
5427140	5428620	other AI systems and you want,
5428620	5430100	and those AI systems that are evaluating
5430100	5431380	are implicitly encoding an objective
5431380	5433820	and you want those to be adversarily robust.
5433820	5437980	So adversarial robustness is not a easy problem to fix.
5437980	5439300	And if you don't fix that issue,
5439300	5442340	then you might have some AI systems just gaming the system
5442340	5447220	and going off, optimizing an objective aggressively,
5447220	5448700	that is not what we want.
5448700	5452380	Is there a problem here with the concept of maximization?
5452380	5455180	So it seems to me that it would be less dangerous
5455180	5456420	to tell an AI system,
5456420	5460180	it go earn a million dollars on the stock market
5460180	5463300	than to tell it go earn as much money
5463300	5465380	as possible on the stock market.
5465380	5468500	Could we kind of cap the impact
5468500	5470060	and the potential negative impact
5470340	5472900	by capping the goal also?
5472900	5476460	I think that's one approach you could imagine
5476460	5477940	conceptually a variety.
5477940	5480860	You could have satisficers where they basically are like,
5480860	5482380	and now I'm good to go.
5482380	5485020	I don't need to keep optimizing this aggressive.
5485020	5490020	There is the possibility of not giving them open-ended goals
5490020	5492940	or very ambitious goals would make them
5492940	5494580	less concerning, more constrained ones,
5494580	5498340	but adversarial robustness would be one.
5498340	5499940	There'd also be anomaly detection.
5500940	5504340	Anomaly detection is something
5504340	5506380	that's researched quite a bit in vision.
5506380	5509180	I've had some part in trying to have
5509180	5511660	the research community focus on that.
5511660	5514220	And I imagine anomaly detection will be very relevant again
5514220	5516300	when we're trying to monitor the activities
5516300	5517660	of various AI agents.
5517660	5519820	Are they doing something suspicious here?
5520940	5522220	While they're being monitored,
5522220	5524860	are they kind of adversarily trying to make the monitor think,
5524860	5526220	oh, it's doing the right thing.
5526220	5528380	So we'll need anomaly detection too to detect
5528380	5531580	if there's some proxy being gained.
5531580	5534300	And that can reduce our exposure to that risk.
5535300	5539580	There's also having some held out objectives
5539580	5542580	of which the agent is unaware
5542580	5544620	that it's being evaluated against.
5544620	5548620	And that can also do things like reduce the risk of it
5548620	5551420	being going to extreme and optimizing
5551420	5553860	the idiosyncrasies of the evaluator.
5553860	5557140	But this is a problem.
5557140	5559580	I think that most of the problem right now,
5559580	5561900	though, if we have large language models
5561900	5566420	trying to optimize a reward model that judges them,
5566420	5567260	they can do that
5567260	5570380	and they eventually start to over optimize it.
5570380	5573420	Although the optimizers that are much more effective
5573420	5574780	at breaking machine learning models
5574780	5576620	are actually just straight up adversarial attacks
5576620	5579260	compared to neural models
5579260	5580580	that are taking multiple steps
5580580	5583020	and iterating on their outputs.
5583020	5585380	The generic gradient-based adversarial attacks
5585380	5586420	are just much more effective.
5586420	5589460	So I think of the sort of risks of gaming,
5589460	5590900	I think most of us,
5590900	5592860	we need to do more just to address
5592860	5594900	the typical adversarial robustness issue.
5594900	5598100	Gold rift is a somewhat related issue
5598100	5602260	where the AI's goals shift over time
5602260	5606540	and the AI might come to take an instrumental goal
5606540	5608300	as an intrinsic goal.
5608300	5609740	How could this happen?
5610740	5612700	It's still a bit unclear to me
5612700	5617700	how an instrumental goal would become intrinsic over time.
5618100	5619420	So to start out with,
5619420	5623140	an intrinsic goal is something that you care about for itself.
5623140	5627380	That could be something like happiness or pleasure
5627380	5629300	for some others that could say,
5629300	5631940	maybe friendship, you'd say, I care about that in itself.
5631940	5634180	You might care about your partner's wellbeing,
5634180	5635300	not because it's useful to you,
5635300	5638780	but you care about their wellbeing in itself.
5638780	5639900	And then there are other things
5639900	5641460	that are just instrumental
5641500	5643540	for achieving those intrinsic goods,
5643540	5645340	such as like money.
5645340	5646780	Money lets you buy things
5646780	5649300	so that you could have higher wellbeing
5649300	5652180	or a car, it gets you from point A to point B.
5652180	5655700	However, some people have intrinsified,
5655700	5659180	to use this sort of more of a Bostrom phrase,
5659180	5662220	intrinsified some of these instrumental goals.
5662220	5664900	Some people actually just directly want money,
5664900	5669180	even to a point where it doesn't make sense or power.
5669180	5671420	Many people are just like, they want power.
5671420	5674740	Even if it harms other parts of their wellbeing,
5674740	5677020	they're willing to make that type of trade-off.
5677020	5680300	So they might latch onto these cues
5680300	5683180	and develop some of the wrong associations.
5683180	5684180	So we see that in people,
5684180	5686020	and there's a risk that AI systems
5686020	5688180	might develop those wrong cues as well.
5689020	5691140	Gold Drift could happen in some other types of way too,
5691140	5694500	where if you have multiple different agents,
5694500	5697260	they might interact in some unexpected way,
5697260	5701380	and then a new goal starts to drive their behavior.
5702100	5706260	An example, we can see this in basic AI multi-agent situations.
5706260	5707580	It's not catastrophic, of course,
5707580	5708420	because we're still here,
5708420	5711980	but in some AI society and some standard paper
5711980	5713780	from earlier this year,
5713780	5715260	the AI start talking with each other,
5715260	5718580	and then they start arranging social structures
5718580	5721420	that they're gonna throw an event
5721420	5723300	at some person's house then,
5723300	5724140	and then this starts to,
5724140	5725740	then they start acting in all these ways
5725740	5727380	to make sure this type of thing happened.
5727380	5728860	And then these sorts of things start to be
5728860	5730220	what drives their behavior.
5730220	5733580	There's some other way in which things can end up drifting,
5733580	5735900	not necessarily through having something to be intrinsic,
5735900	5739340	but there could be these emergent goals from interactions
5739340	5740580	that end up driving behavior.
5740580	5744340	Certainly there are many emergent things in society,
5744340	5745620	things that become new,
5745620	5747340	and this isn't the goal that I originally had
5747340	5749020	when I was 10 years old,
5749020	5751740	but now some of these things end up driving my behavior
5751740	5752580	quite substantially.
5752580	5754860	So if we have adaptive AI systems,
5754860	5757580	and if they end up responding to each other,
5757580	5760940	then you could have some emergent complexity happen,
5760940	5764340	and that those interactions that the behavior
5764340	5766700	starts driving the overall group behavior
5766700	5768180	as they're imitating each other,
5768180	5769980	as they're responding to each other.
5771420	5773940	So it's basically multi-agent systems
5773940	5774980	be very difficult to control.
5774980	5775820	In the single agent one,
5775820	5779020	you'd have to worry about there being some wrong association
5779020	5781420	between an intrinsic and instrumental goal,
5781420	5783220	like money or power.
5784140	5786060	And that could mean if that does happen,
5786500	5789100	if basically something wrong gets intrinsified,
5789100	5790660	then you're in a very dangerous situation
5790660	5793740	because then your AI has a goal
5793740	5796020	that's just different from what you wanted.
5796020	5798020	And so then it will, to get that goal,
5798020	5799260	it will optimize against you,
5799260	5800620	it will respond adversarily,
5800620	5803580	it will resist your efforts to shut it down
5803580	5804860	so that it can achieve that goal.
5804860	5807820	So although it's not something that necessarily happens
5807820	5810220	by default or with extremely high probability,
5810220	5811500	if it does happen,
5811500	5815580	then you've got a substantial tail risk in front of you.
5815580	5818660	I wonder whether these AIs will persist for long enough
5818660	5820220	for Gold Drift to happen.
5820220	5825100	So normally we retrain models every couple of years,
5825100	5827500	we switch out for the newest ones.
5827500	5830980	And so it's not like a person that has 30 years
5830980	5833180	to change their values.
5833180	5836740	Will they last long enough for Gold Drift to matter?
5836740	5838100	So I guess two things,
5838100	5840100	one is the world will move substantially more quickly
5840100	5842660	in the future, such that I like,
5842740	5845620	often in these more pivotal periods,
5845620	5847220	I don't know if it was Lenin or something like that,
5847220	5849980	like there are decades in which weeks happen
5849980	5851900	and then there are weeks in which like decades happen.
5851900	5854980	So even if there is a high replacement rate
5854980	5856420	in the AI population,
5856420	5858540	this goes on in a much lower process,
5858540	5860860	they could still end up constructing things
5860860	5863420	that end up causing their goals to be different.
5863420	5865540	Like they, let's say they develop some different type
5865540	5868540	of social infrastructure for mediating their interactions.
5868540	5870260	There are new AI companies being formed
5870260	5871980	and they're end up driving many of them.
5871980	5874900	Then those features of the environment
5874900	5877780	would end up affecting the generation that comes after it.
5877780	5880140	So you could still imagine some type of drift,
5880140	5881380	some intergenerational drift,
5881380	5883100	but if each generation is very short,
5883100	5885380	you can still imagine some type of Gold Drift in that way.
5885380	5887580	This is kind of think of yourself.
5887580	5890060	Many of the goals, the intrinsic goals that you have
5890060	5891900	or intrinsic desires that you have
5891900	5894300	are completely unlike those when you were younger.
5894300	5898140	The even taste in food, the things you care about,
5898140	5899820	I mean, maybe you acquired sports,
5899820	5904820	your taste in music, affiliations,
5904940	5907060	all of these things that are changing across time.
5907060	5909220	And so, and they can also go away too,
5909220	5910620	some of the intrinsic things you care about.
5910620	5914060	Like I care about this person's wellbeing for themselves,
5914060	5915220	but then you break up with them.
5915220	5918660	Oh, now I actually don't care about their wellbeing in itself.
5920180	5922380	I don't have that strong of a feeling toward them.
5922380	5927140	So adaptive systems carry this type of property.
5927540	5931220	This is one way in which they end up gaining some goals
5931220	5934380	that we didn't intend either through some emergent goal
5934380	5936460	from the product of various interactions
5936460	5940340	or through them intrinsifying some instrumental goal
5940340	5941420	like power.
5941420	5944180	They end up having too strong of an association with that
5944180	5947220	and reward and then just end up seeking the power itself.
5947220	5949100	Could Gold Drift be a good thing?
5949100	5954020	So we wouldn't want to fix human values
5954020	5956340	from the year 1800, for example.
5956340	5959140	You could describe our changing goals
5959140	5961980	from back then to now as a form of Gold Drift
5961980	5965740	where people from 1800 might disagree violently
5965740	5967940	with whatever we believe now,
5967940	5970060	but we still probably think it's a good thing
5970060	5971660	that we've changed our values.
5971660	5974580	Yeah, could it be good and could we learn from the AIs?
5974580	5978740	Yeah, so I think this is a good point
5978740	5981860	in what makes thinking about AI risk generally a lot harder.
5981860	5985100	As we mentioned earlier, there's this balance issue
5985100	5987340	with malicious use that because you'd
5987340	5990580	be concerned about unilateralist misusing AIs
5990580	5992780	or rogue actors misusing AIs that we should then
5992780	5994180	centralize power, but then you end up
5994180	5996940	getting some other existential risk of lock-in,
5996940	5998580	of concentration of power.
5998580	6000380	And then I think likewise, in this case too,
6000380	6004020	that you can't have a society in complete stasis
6004020	6007940	and as it would be driven by new emergent type of structures,
6007940	6009540	you should still try and make sure
6009540	6012420	that you have some control over that process
6012420	6015300	or reasonable control over that process.
6015300	6018380	It seems if there's not much control,
6018380	6021740	then I think you'd likely to slip from your hands.
6021740	6024660	But otherwise, so there's basically
6024660	6026140	one will have to strike a balance
6026140	6030500	between some very chaotic state where they're running wild
6030500	6032180	and some stasis.
6032180	6033620	And this is just a continual issue
6033620	6040060	in many areas of evolving groups.
6040060	6041740	Yeah, that would also be a problem
6041820	6043220	if there'd be too much entrenchment,
6043220	6046500	if there isn't an ability to have adaptation of the things
6046500	6048020	that we care about.
6048020	6049580	Yeah, so anyway, there's some dissonance.
6049580	6050940	There aren't simple answers with this.
6050940	6053420	This is why it's will be a balancing act.
6053420	6058220	And that's also why I don't expect, in particular,
6058220	6061620	a single solution to solve everything for all time.
6061620	6062820	We'll need to respond.
6062820	6066300	We'll need institutions and structures and control measures
6066300	6070460	that respond to the features of the environment
6070460	6071580	and calibrate reporting.
6071580	6075140	Why could AIs become power-seeking?
6075140	6081220	So this is a very, I think, one of the main AI risk stories
6081220	6083940	would be it becomes power-seeking.
6083940	6086460	I'll make a bit of a case for it,
6086460	6089660	and I'll speak about some issues with it too.
6089660	6093580	You could imagine a person gives an AI system a goal,
6093580	6097940	like, goal make me a lot of money as an instrumental goal,
6097980	6101100	gaining a lot of power seems like a very helpful way
6101100	6104460	to accomplish that higher-level goal.
6104460	6108220	So there's a concern that when you specify a goal,
6108220	6112620	that there'll be some sub-goals that are too correlated
6112620	6115900	with power, and you'd want to make sure
6115900	6119820	that you can control those tendencies.
6119820	6122980	So that's one of just being, when you're just directly giving
6122980	6125180	an AI goal, it may have a goal that's correlated with power.
6126180	6129660	Is that terribly unexpected?
6129660	6132060	We will give them goals that relate to power quite a bit.
6132060	6133780	Militaries will probably build AI systems
6133780	6136780	that are fairly power-seeking,
6136780	6138980	and so we should expect some amount of AIs
6138980	6143660	that are pursuing power either as their main goal
6143660	6145980	or as one of their main sub-goals.
6145980	6150460	And maybe power-seeking to a limited extent is OK?
6150460	6154300	Basic feature of accomplishing many of these sorts of goals.
6154420	6156020	For instance, the fetch-of-the-coffee one.
6156020	6158380	If you'd instructed to fetch a coffee,
6158380	6160180	it would have an incentive to preserve itself
6160180	6162380	because it can't fetch the coffee otherwise.
6162380	6164500	But you might want to curtail some of those tendencies
6164500	6166100	so that those don't get out of hand.
6166100	6167660	But that would be a...
6167660	6170780	We've had a paper at ICML earlier this year
6170780	6173420	where we're deliberately giving it penalties
6173420	6175980	to penalize some of these tendencies
6175980	6178260	that it has when it is trying to seek its reward.
6178260	6182060	It starts having incentives to accrue resources
6182060	6183460	and things like that.
6183500	6186980	And then can we acquire the resources
6186980	6189340	that are more minimal to accomplishing its goals?
6189340	6191260	Can we have it engage unless power-seeking behavior?
6191260	6192980	So I think that that's something that we can offset,
6192980	6195980	but we'll need to make sure that we have good control measures
6195980	6197860	for that to keep that in check.
6197860	6200300	There's also the...
6200300	6202660	So that's one of just people directly instructing it
6202660	6204060	with goals that are, by default,
6204060	6205900	probably going to be pretty related to power.
6205900	6209660	And there's also maybe they would intrinsically care...
6209660	6212420	Let's say that they had some random goal.
6212420	6214140	It's like a paperclip maximizer.
6214140	6216060	You're sampling from...
6216060	6219060	Use old verb as you're sampling from mind space
6219060	6220700	and then it has a random set of desires.
6220700	6222340	And whatever that set of desires,
6222340	6223740	then it would end up trying to seek
6223740	6225140	a substantial amount of power.
6225140	6228260	That's one claim,
6229300	6233740	but I think that has to be something more rigorously argued.
6233740	6236100	I should claim that, or I would like to note that.
6236100	6239340	I think that a lot of those power-seeking arguments,
6239780	6243220	I don't think it works as well as I thought it did,
6243220	6245340	the arguments associated with them.
6245340	6246780	I still think it's a relevant thing
6246780	6250780	that we'll want to control the sub-goals of AI systems
6250780	6255100	to make sure they're not too strongly related to power
6255100	6257660	and that there's nothing unexpected going on there.
6257660	6259780	So for instance, people might argue for power-seeking
6259780	6263740	by saying, well, power is instrumentally useful
6263740	6266340	for a broad variety of goals.
6266340	6268100	Therefore, it will seek power
6268140	6271020	if it's trying to accomplish any sort of reasonable goal.
6271020	6273380	And you'd ask them what power is,
6273380	6274980	and then they'd say power is,
6274980	6276700	what's instrumentally useful for accomplishing
6276700	6277540	a wide variety of goals?
6277540	6278700	And you'd go, okay, well, that's a tautology.
6278700	6280020	So we need to be more careful.
6280020	6282540	What exactly are we meaning by power here?
6282540	6284820	Separately, there's often a bit of...
6284820	6287940	So that's one like slight bug that lurks in the background
6287940	6290540	is that they'll define power in terms of instrumental stuff
6290540	6292900	and then it's tautological.
6292900	6296920	Another issue is that there's sometimes a conflation
6296960	6300360	between power-seeking and dominant-seeking.
6300360	6302600	Those are not the same thing.
6302600	6306160	When the AI is trying to fetch the coffee
6306160	6309760	and is engaging in self-preservation to do so,
6309760	6311560	it's not necessarily, therefore,
6311560	6313600	trying to take over the world.
6313600	6315660	So saying that an AI is power-seeking
6315660	6318040	is not necessarily existential.
6318040	6320680	Indeed, you could imagine various ways
6320680	6323880	in which other powerful actors
6323880	6325440	engage in power-seeking behavior,
6325440	6327320	but don't try and seek dominance.
6327320	6329760	So for instance, different countries
6329760	6332080	in trying to increase their own power to preserve themselves.
6332080	6335200	This is the sort of thesis of neorealism
6335200	6336760	or structural realism.
6336760	6339920	And what happens is they will basically...
6339920	6341640	Many states will just try and keep power
6341640	6343400	relative to many of their peers.
6343400	6346800	If Germany, for instance, tries to take...
6346800	6348720	It's seeking power to protect itself,
6348720	6349680	but if it tries seeking power
6349680	6351400	at the level of the global domination,
6351400	6353000	it will be met with force.
6353000	6354640	There will be balancing from other peers.
6354680	6357000	So when we're in a multi-agent situation,
6357000	6358680	then it doesn't necessarily always make sense
6358680	6360680	for AI systems to try and take over the world
6360680	6362280	because there'll be other AI agents to be
6362280	6364600	that will support my preferences or goals and desires,
6364600	6366000	so I will counteract you.
6366000	6368600	Balancing in international relations is what this is called.
6368600	6370680	That's a thing that can offset dominant seeking.
6370680	6372480	So it's not necessarily a case that power-seeking
6372480	6375600	is dominant seeking and trying to take over the world.
6375600	6379360	An additional point is that we can partly influence
6379360	6381520	the dispositions of AI systems.
6381520	6383440	Sorry to say, we can do that.
6383440	6388160	We can make these have dispositions to be a good chatbot
6388160	6389320	or be a good assistant.
6390600	6391880	Now, how strong is that?
6391880	6395680	It's not perfect, but if it were given a task,
6395680	6400680	like, hey, go accomplish some goal for me,
6400840	6403280	if it would think, well, the best way would be,
6403280	6404640	I could accomplish this goal better
6404640	6409640	if I were extremely powerful and took over the world,
6410200	6412960	but that may not be in keeping with its values
6413000	6414320	necessarily.
6414320	6416640	So it may have some tendency pulling in that direction,
6416640	6418800	but you could also give it some dispositions
6418800	6419640	to pull it against it,
6419640	6421280	and that might be sufficient to offset
6421280	6423320	some of these tendencies toward power.
6423320	6424680	Even if there is some incentive there,
6424680	6426040	it may not be enough to overwhelm it.
6426040	6428560	So a lot of this discussion about instrumental convergence
6428560	6433560	needs to think about the balance between these forces,
6434080	6435440	and they would need to argue basically
6435440	6438880	that the instrumental drive is extremely strong
6438880	6442080	to overwhelm fine-tuning and all these sorts of things,
6442200	6443520	which I don't think that there's much
6443520	6446240	of a specific argument for that.
6446240	6450040	I want to highlight here, Joe Carl Smith has a great report.
6450040	6451640	I think the most rigorous argument
6451640	6455280	for why power-seeking in AI could be existentially dangerous.
6455280	6456680	So just for listeners who are interested
6456680	6460680	in what I think is the best argument for that out there.
6460680	6462160	I agree, I agree.
6462160	6466040	He helped popularize the sort of power-seeking phrase as well,
6466040	6468120	and I think that by focusing on power,
6468120	6469280	that helped us integrate this
6469280	6471400	into some other like academic discussions,
6471400	6473080	like power versus cooperation.
6473080	6474960	What I was describing here,
6474960	6476280	just a moment to go about balancing,
6476280	6477720	was that we can take a cue
6477720	6479880	from the international relations literature
6479880	6481800	of seeing like, well, power-seeking agents,
6481800	6483840	when that's one of their main goals,
6483840	6486040	that doesn't necessarily turn into them
6486040	6487400	trying to seek domination.
6487400	6490160	Another thing is that in Bostrom, in superintelligence,
6490160	6492720	there's also a sort of spark slide of hand,
6492720	6495040	not intentional, but I suppose maybe an accident,
6495040	6496760	where he's saying that power makes you better
6496760	6498200	or able to accomplish your goals,
6498200	6499920	therefore they will seek power.
6499920	6503960	That's saying that something is helpful if you have it,
6503960	6506120	that doesn't mean that it's rational to seek it.
6506120	6508120	So although there's an incentive for it,
6508120	6511200	that doesn't mean it's instrumentally rational to pursue it.
6511200	6514600	So for instance, if we run the argument in a different way,
6514600	6519400	it would be helpful for me to be a billionaire.
6519400	6521440	That doesn't mean that it's rational for me
6521440	6523080	to try to become a billionaire.
6523080	6526000	I could, that would carry a lot of risks,
6526000	6527960	I would take a lot of time.
6527960	6530760	The existence of incentives aren't necessarily enough
6530760	6534000	to say that that's what will be driving their behavior
6534000	6536440	or is the first approximation of their behavior.
6537400	6541200	And I think that there are other ways
6541200	6544280	in which just power seeking doesn't emerge
6544280	6545880	or dominance seeking doesn't emerge.
6545880	6548520	If you give it some goals, like obviously if you say,
6548520	6550520	shut yourself off or if you give it a goal,
6550520	6555120	like don't seek power, these are obviously counter examples
6555120	6556800	for that just to show that this isn't like a,
6556800	6558760	it's not a law of all AI systems
6558760	6560240	that they will try and seek power.
6560240	6561600	Separately, if you give it a more goal,
6561600	6564400	like go fetch the milk, it could try and take over the military
6564400	6567560	to put up a motorcade to make sure
6567560	6569360	that it can get to the store very quickly.
6569360	6571760	But if you had some time penalty or something,
6571760	6575520	this would not necessarily be the thing to do.
6575520	6578280	So instead just go fetch the milk would often be
6578280	6579720	the best way of getting the reward
6579720	6581360	instead of some very circuitous path.
6582400	6585480	Now, so I do think that there is a risk of,
6585480	6589920	if you have AI agents that are not protected and autonomous,
6589920	6591720	you could get power seeking type behavior.
6591720	6596280	For the same reason that states try to shore up their power,
6596280	6597160	they shore up their power
6597160	6600320	because there isn't anybody they can call on for help
6600320	6602160	if they're getting attacked necessarily.
6602160	6604880	Like if the US starts getting attacked,
6604880	6605880	maybe some countries will come
6605880	6609560	but this isn't a police force that will settle the issue.
6609560	6613480	So the best they can do is try to shore up power
6613480	6616120	to defend themselves so that they can't be pushed around like that.
6616120	6618560	So we have a non hierarchical or quote unquote
6618560	6620920	anarchic international system
6620920	6623920	and that incentivizes agents to seek power
6623920	6626960	to preserve themselves to pursue whatever their goals are.
6626960	6630600	And you could imagine if AI systems are not protected,
6630600	6634000	if they are part of say some crime syndicate
6634000	6636120	or if they're rogue, they're unleashed,
6636120	6637400	somebody unleashes them,
6637400	6639480	then those systems would actually have
6639480	6642120	a very strong instrumental incentive to seek power
6642120	6643760	in the same way that states do,
6643760	6646640	that if they want to protect themselves
6646640	6650200	from some potential adversaries that can harm them,
6650200	6651440	there isn't somebody to call on.
6651440	6652920	They can't ask the US government,
6652920	6653760	if there are crimes syndicate,
6653760	6656280	they can't say, US government protect me, I'm getting harmed.
6656280	6657280	That is not a possibility to them.
6657280	6658760	So what they have to do is they have to take matters
6658760	6660320	in their own hands and accumulate their own power.
6660320	6663200	So what I've done is I've sort of flipped things a bit.
6663200	6665360	There'd be the usual argument that
6665360	6667880	AIs might be power seeking just by their inherent nature,
6667880	6669240	by the inherent natures of goals
6669240	6670840	and optimizers and things like that.
6670840	6672560	But I've instead mentioned that
6672560	6674960	one source of power seeking is humans give them
6674960	6677040	some sort of goals that are very correlated with power
6677040	6678480	and then there might be some unexpected stuff
6678480	6679880	that happens in their subgoals.
6679880	6681240	And then the other thing I've done is
6681240	6683800	I've mentioned how the structure of the environment
6683800	6685480	that they're in, some structural reasons
6685480	6687720	for why they might end up seeking power too.
6687720	6690640	I'm not as sure about them having an intrinsic one
6690640	6692440	or internal reason for power seeking,
6692440	6695480	but I think goals being given intentionally
6695480	6697840	or the structure of the environment
6697840	6699020	that they find themselves in,
6699020	6700400	it's a sort of cage that they're locked in,
6700400	6701600	there's really nothing they can do
6701600	6703000	if they're wanting to accomplish a goal
6703000	6706600	other than to invest a lot in protecting themselves,
6706600	6707720	would also incentivize them
6707720	6710160	to seek a substantial amount of power.
6710160	6712760	So I do think power seeking is a concern,
6712760	6715320	but not for the same reasons that other people are giving,
6715320	6717520	like we're gonna randomly sample a mind for mind space,
6717520	6721120	it'll be very alien and by a way of almost any desires,
6721120	6725680	it will necessarily try to seek dominance over humanity.
6725680	6728040	But I still would be concerned about power seeking.
6728080	6732760	How concerned are you about deception arising in AIs?
6732760	6735760	I think that the contribution of focusing on deception
6735760	6740760	was useful because we now see that AIs have
6742560	6744240	to some extent some representation
6744240	6746440	of morally salient considerations,
6746440	6747840	as we explore in the paper,
6747840	6749320	aligning the AIs with shared human values,
6749320	6752080	and I clear it maybe 2020 or something,
6752080	6754280	where we measure that and show that,
6754280	6756520	by now it's obvious because it's in chatbots
6756880	6758280	and people can ask it moral questions,
6758280	6760960	but they have some capacity for that.
6760960	6765720	And the deception part focuses on maybe they're actually,
6765720	6768720	although they maybe understand the goal,
6768720	6771520	they don't necessarily feel inclined to pursue it.
6771520	6773520	So in psychology, this is a distinction
6773520	6777360	between cognitive empathy and compassionate empathy.
6777360	6779080	Cognitive empathy psychopaths have,
6779080	6780240	they can understand and predict
6780240	6782040	what people will end up feeling
6782040	6783280	in response to various actions.
6783280	6784800	They have very good predictive model
6785080	6788480	of people's feelings and their emotions
6788480	6790800	and what they think is valuable.
6790800	6792240	Meanwhile, if they have compassion empathy,
6792240	6795720	that's when they feel motivated to do things by it
6795720	6800560	and help people realize those values.
6800560	6804800	So there's a distinction that they would have cognitive empathy
6804800	6806600	but not necessarily compassionate empathy.
6806600	6809160	And so if they're deceptive, they could basically play along.
6809160	6812640	They could be like, yeah, I don't actually care about you,
6812640	6813800	but I'm gonna act like it
6813800	6816920	to get my goals accomplished as psychopaths do.
6816920	6819680	And here, maybe we should mention here
6819680	6824480	how the drive of deception arises
6824480	6828120	from the way that we are doing reinforcement learning
6828120	6830520	from human feedback or how it could arise from that.
6830520	6834720	So in the Machiavelli ICML paper,
6834720	6836400	we saw instances of them doing deception
6836400	6839200	because it simply helps them accomplish
6839200	6841080	their goals better by default.
6841080	6843960	So many environments just incentivize the type of behavior.
6843960	6846720	If they have some type of misaligned goal from us,
6846720	6851720	then they could buy their time and wait to come to power
6853360	6856040	to take a quote unquote treacherous turn.
6856040	6859120	So it could just be very strongly incentivized
6859120	6860600	to buy some type of training process,
6860600	6862440	like by just seek more reward,
6862440	6865320	deception can often be a good trick when you're monitored,
6865320	6867920	behave nicely, when you're not monitored,
6867920	6870000	switch your behavior, behave in a more cutthroat way.
6870000	6874680	That's how a deceptive behavior can be a concern
6874680	6877560	or some Machiavellian type of behavior.
6877560	6881320	And there are instances of this.
6881320	6884840	You could imagine as a more non-agentic case
6884840	6889520	with chatbots is if they're being given human feedback,
6889520	6890840	maybe they'd have an incentive
6890840	6894360	to say very agreeable answers to people.
6894360	6896600	Things that they'd say, oh, that sounds good to me,
6896600	6899360	even though it's if it's not necessarily true.
6899400	6902000	So that's how even chatbots might be incentivized
6902000	6904920	to be in a somewhat deceptive direction.
6904920	6906280	But we can also see this in agents,
6906280	6908600	just the often helps them accomplish goals.
6908600	6913600	Also chatbots might learn to recognize the ways
6914520	6918400	in which they're telling bad lies, let's say.
6918400	6920280	The obvious things they're saying
6920280	6922080	that are false are penalized,
6922080	6924680	whereas the more sophisticated ways
6924680	6928840	they might be telling falsehoods are not penalized.
6929600	6934040	So this gets it in a lot of repeated interactions
6934040	6936560	and whatnot, deception often emerges.
6936560	6940400	In the evolution paper from the last time I was here,
6940400	6944040	we spoke about how deception can often be
6944040	6945240	and concealment of information
6945240	6948000	can often be an evolutionally stable strategy
6948000	6949040	and that there are many instances
6949040	6950240	of deception in the environment.
6950240	6953200	So it's a fairly difficult thing to plot out
6953200	6954720	when you try and control for it.
6954720	6958640	You often end up selecting for a more deceptive behavior.
6958640	6963240	At the same time, we do have progress on this though,
6963240	6968240	where we can, in a recent paper we submitted,
6969440	6971880	or in a recent paper we uploaded to archive
6971880	6974000	called Representation Engineering,
6974000	6978080	a top-down approach to AI transparency.
6978080	6980200	There we have instances, many instances,
6980200	6982200	it's not that difficult to control
6982200	6984560	by manipulating the internals of the model,
6984560	6985640	whether or not it's lying.
6985640	6988240	It has an internal concept of what is accurate.
6988240	6989480	We can find a truth direction,
6989480	6994040	we can subtract the direction or something of that sort,
6994040	6997720	and then that can cause it to spit out incorrect text
6997720	7000400	and we have other more sophisticated control measures too,
7000400	7002040	but we can manipulate internals to do that.
7002040	7004040	So it's within the capacity of AI systems
7004040	7005480	to lie and be deceptive.
7005480	7007440	We have another paper on that called,
7007440	7008720	if you search AI deception,
7008720	7010360	and then maybe my name or something,
7010360	7011600	then you'd see that paper.
7011600	7014160	So many instances of AI deception already,
7014160	7016120	but we do have some traction on this problem.
7016120	7019080	So fortunately, there'd still be the issue
7019080	7021320	of having more reliable lie detectors
7021320	7023360	and being able to control them to be more honest
7023360	7025240	or output their true beliefs.
7025240	7029160	So there's definitely much more work to be done,
7029160	7031440	but we're at least not helpless.
7031440	7034320	We don't need to wait another 30 years
7034320	7037600	for interpretability research to get to a state
7037600	7041240	of being able to just start to rush against the question.
7041240	7043440	We now have some ability to influence
7043440	7045760	whether AI is lie by controlling their internals.
7047080	7049040	And so that makes me more optimistic
7049040	7052400	about dealing with this problem,
7052400	7056240	but you don't wanna do premature celebration.
7056240	7058760	I don't know how much time we'll have to continue
7058760	7061400	getting those detection measures
7061400	7064960	and those control measures to be highly reliable.
7064960	7069200	So that'll depend on like the having a lot of researchers
7069200	7072560	who can research with these cutting edge very large models
7072600	7074480	to make progress on it.
7074480	7076480	Yeah, the representation engineering paper
7076480	7078080	was super exciting.
7078080	7080960	Maybe you could explain what,
7080960	7083760	at what level does representation engineering work?
7083760	7086800	Because it's different from mechanistic interpretability.
7086800	7088160	It's more high level,
7088160	7090880	and which is what we are after in a sense.
7090880	7094400	We are after the high level emergent behavior
7094400	7095800	in these models.
7095800	7098080	Yeah, I was mentioning compassionate empathy
7098080	7099080	and cognitive empathy,
7099080	7100480	because it's a bit of psychology,
7100480	7101920	but I think trying to do something
7101920	7103880	more like a project like AI psychology
7103880	7105240	or AI cognitive science,
7105240	7106520	is I think what we should be trying to do here.
7106520	7110080	So in the case of this representation engineering,
7110080	7112440	that's I think we're trying to be the analog of that,
7112440	7115120	where we're given these high level representations
7115120	7117240	of truth and goals and things like that.
7117240	7121920	Can we make it be so that it actually outputs its beliefs
7121920	7125120	or what it says it believes is actually what it believes?
7125120	7126560	For that, you need to have a handle
7126560	7129040	on these very high level concepts
7129040	7130720	so that they're not psychopathic,
7130720	7134720	so that we can control their dispositions to behave
7134720	7137080	and have things like compassionate empathy.
7137080	7138360	Meanwhile, I think the mechanistic stuff
7138360	7139680	is looking at a much lower level.
7139680	7141600	It's looking more at the substrate,
7141600	7143240	at the neuron level, at the circuit level,
7143240	7145360	at the node to node connection level.
7145360	7147880	And that's maybe closer to something like neurobiology,
7147880	7149600	and then what we're doing is more like trying to study
7149600	7151480	the mind as opposed to trying to study
7151480	7153600	the specific structures in the brain
7153600	7155440	and the connections between them
7155440	7157000	and how that gives rise to phenomena.
7157000	7158880	So I think philosophically,
7158880	7162440	I had tried many times to do a paper on transparency,
7164160	7167640	historically, but it wasn't a good angle of attack.
7167640	7171440	And in my view, it would take too long.
7171440	7174120	But I think if we do it in a more top-down type of way
7174120	7175960	where we try and here's the eyes of mind,
7175960	7180040	let's try and decompose it into some representations
7180040	7181840	that drive a lot of its behavior
7181840	7184160	and maybe decompose those further and further.
7184160	7186000	Basically, we have a big problem of understanding
7186000	7188280	in the eyes of mind, let's break it up into sub-components
7188280	7191120	and try and get a handle on those and control those.
7191120	7195440	I think that approach might be more efficient
7195440	7198080	at reducing risks of AI deception
7198080	7200600	than building from the bottom up understanding,
7200600	7203240	this is how it answers, this is the circuit in it
7203240	7204920	that lets it understand multiple
7204920	7207000	or identify a multiple choice question.
7207000	7209240	And then this helps it select the weather to output
7209240	7213720	the full response back or whether just to select A, B, C, or D.
7213720	7215200	Things like that.
7215200	7216160	You can build those up,
7216160	7219840	but that might become very complicated in time.
7219840	7222120	So I think it might make sense to not work from the bottom up,
7222120	7225520	but go from the top down.
7225520	7227560	There are analogs of this type of approach
7227560	7229160	in cognitive science.
7229160	7231920	People would initially try and just study things
7231920	7234640	at the synapse level, but it can often be more fruitful
7234640	7238320	of trying to understand things at the representational level.
7238320	7241240	What are the high-level emergent representations
7241240	7242960	that are a function of all the population,
7242960	7245120	of all the neurons in the network,
7245120	7247040	and try and understand things at that level?
7247040	7249040	Now, there's, of course, a risk of,
7249040	7250280	well, maybe there's some funny business
7250280	7252240	that gave rise to that representation.
7252240	7254000	And that's true.
7254000	7256120	We could still do things to reduce that risk
7256120	7258720	by trying to understand the representations
7258720	7260840	at various layers in the network
7260840	7264720	and trying to decompose the system further and further
7264720	7269720	so that there isn't much room for funny business or deception.
7270440	7274920	But so that's it at a high level.
7274920	7277240	It's not viewing neurons as the main unit of analysis.
7277240	7282080	It's viewing representations as the main unit of analysis.
7282080	7286320	And neurons are relevant insofar as they help us predict
7286320	7289400	and explain what's going on in representations.
7289400	7292960	But those are more of a, that's sort of just the substrate.
7292960	7294680	It's a comment on the substrate in the same way
7294680	7297960	that if we have a computer program that plays Go,
7297960	7300400	if I'm reasoning about the Go program,
7300400	7303520	I'm just probably gonna be thinking about Go strategies
7303520	7304640	when I'm playing against it.
7304640	7306280	I don't need to think at the software level,
7306280	7307680	like, well, where do you think it,
7307680	7309960	what layer do you think it's at right now?
7309960	7314640	Or what TensorFlow objective function did Alpha Go
7314640	7315600	end up optimizing here?
7315600	7316640	Maybe some of the examples.
7316640	7317760	We don't need to analyze at that level.
7317760	7319040	We certainly don't need to break it down
7319040	7320880	at the level of assembly.
7320880	7322120	We don't need to reason out assembly
7322120	7323760	to try and understand its behavior.
7323760	7327520	So I think that there's some emergent complexity
7327560	7328600	inside of neural networks.
7328600	7332040	We just need to, we can study it at that level
7332040	7334560	and it's studying at that level is fruitful
7334560	7338160	because there's an emergent ontology
7338160	7340920	and some coherent structure inside of that,
7340920	7342960	which you would get up getting lost in the details
7342960	7347960	when you end up zooming in further to the neuron level.
7348800	7350520	So although it's possible in principle,
7350520	7352080	it's possible in principle to explain everything
7352080	7352920	in terms of that,
7352920	7355200	just like it's possible to explain the economy
7355200	7357160	in terms of particle physics.
7357360	7358760	Computationally, you could do it,
7358760	7361360	but it doesn't make sense to study it at that level.
7361360	7364480	This isn't to say that their mechanistic interpretability
7364480	7366920	and representation engineering are completely loose
7366920	7368640	and separate, there's probably overlap,
7368640	7372640	just as like in biology and chemistry,
7372640	7373800	they have some overlap,
7373800	7375960	but you wouldn't try and understand biology
7375960	7377200	just through chemistry.
7377200	7379160	And I think if you're trying to understand representations,
7379160	7380600	I don't think you're necessarily just gonna try
7380600	7383000	and understand everything through neurons
7383000	7384280	and node to node connections
7384280	7386360	and specific execution pathways
7386400	7389520	and treat it like a computer program,
7389520	7392560	but instead something more like a mind
7392560	7396360	with loose associational high-level representations.
7396360	7400840	Yeah, so take a cognitive trait like honesty.
7400840	7404360	Do we know anything about how that's distributed
7404360	7405400	across the model?
7405400	7410400	Is there like a cluster of the weights
7411280	7413960	in which this is now representing honesty
7413960	7418240	or functioning as the honesty module
7418240	7420560	or is it more distributed across the whole model?
7420560	7422600	Yeah, neural network representations
7422600	7423760	are highly distributed,
7423760	7425800	which makes sort of trying to bolt down
7425800	7427360	and pinpoint specific locations
7427360	7429440	of a lot of functionality, a lot more difficult,
7429440	7432560	as well as the interactions between all these components too,
7432560	7434600	can end up giving rise to a lot of complexity.
7434600	7436840	Imagine that you understood a neuron
7436840	7441440	and it was this detects a whisker at 27 degrees
7441440	7443920	and this other neuron detects
7443920	7446080	some upper corner of a fire hydrant
7446080	7449720	and if you can understand these millions of neurons
7449720	7450680	that gets you some way,
7450680	7453560	but are you really understanding the collective overall
7453560	7455760	emergent behavior of the system?
7455760	7457800	That doesn't necessarily follow.
7457800	7459080	So I don't think it's enough to understand
7459080	7462320	the lowest level parts to understand the overall system
7462320	7465160	and its collective function, but it can be helpful.
7465160	7466480	It can provide some types of insights.
7466480	7467880	In the case of honesty though,
7467880	7469240	I find that it's a direction
7470200	7472360	or it's beliefs about what's true or not,
7472360	7474960	our directions in its representational space
7474960	7478680	and it doesn't seem to be located at a specific neuron.
7478680	7481800	So when we adjusting the representations
7481800	7484800	through various control measures that we propose,
7484800	7487160	then we can actually end up manipulating it.
7487160	7489320	So that's anyway, so partly this paper
7489320	7491800	is a bit more philosophical in like,
7491800	7493320	what's the sort of paradigm?
7493320	7498320	What's the strategy that we're wanting to proceed
7498640	7500960	in making AI systems transparent?
7500960	7503640	The representation level is the,
7504480	7507240	going to be a very fruitful way.
7507240	7508400	I should note that,
7509800	7513000	but it'd be useful to diversify over,
7513000	7516000	research agendas and things like that.
7516000	7518400	Hopefully we'll get more reliable control measures
7518400	7521760	and be able to modify relatively arbitrary parts.
7521760	7523440	We'll have success when we can like,
7523440	7524720	inside of the AI system,
7524720	7527600	when we have better ability to sort of read their mind
7528480	7529840	or understand the representations
7529840	7531880	if we could use it for like knowledge discovery.
7531880	7534520	Then we've known that our methods are fairly good
7534520	7536560	because they're probably gonna pick up some observations
7536560	7538320	about the world from their big pre-trained distribution
7538320	7540920	that no individual knows
7540920	7543160	or that many individuals don't know.
7543160	7545120	So if we can get better tools like that,
7545120	7547640	then that would be a late stage sign of success.
7547640	7549960	Yeah, and it seems like we have a better shot
7549960	7553520	at success here than neuroscience on humans
7553520	7556120	because we have such fine grained access
7556120	7560960	to it's as if we had a human brain spread out
7560960	7564840	with full access to what all of the neurons are doing.
7564840	7566360	So, or do you think that's right?
7566360	7568600	Do you think we have a better chance of success
7568600	7570640	compared to traditional neuroscience?
7570640	7571480	Yeah, yeah, certainly.
7571480	7574000	I think the sort of mechanistic interpretability
7574000	7576480	of people would claim this as well,
7576480	7579080	that since we have access to the gradients,
7579080	7581880	we have rewrite access to every component of it.
7581880	7584640	This allows for much more controlled replicable experiments
7584640	7587320	and a substantial ability to do science
7587320	7590200	that the bare ears to entry in cognitive science
7591800	7593960	or many of them are removed.
7593960	7596200	There's also this might get easier in time.
7596200	7597440	What makes this now possible?
7597440	7598640	Whereas previously it wasn't.
7598640	7601200	If you use models like GPT2 or below,
7601200	7603640	they just, the representations are not very good.
7603640	7605400	They're quite incoherent.
7605400	7609000	But as we use larger models like Lama2
7609000	7610600	pre-trained on many more tokens,
7610600	7612720	they have some emergent internal structure
7612720	7614920	that actually starts to make some sense
7614920	7616320	and directions that are correlated
7616320	7618720	with coherent concepts that humans have.
7618720	7620400	I think earlier it's more like a shibboleth,
7620400	7622520	but now there, since there is some coherence to it,
7622520	7625120	it's not just a big causal soup of connections.
7625120	7626840	So this is why I think, unfortunately,
7626840	7629920	this wasn't something that we could have particularly done
7629920	7633520	in like 2016 and is very much something
7633520	7635320	that's possible now that previously wasn't.
7635320	7637600	Dan, thanks for spending so much time with us here.
7637600	7639960	It's been very valuable for me.
7639960	7642800	And I think it will be for our listeners too.
7642800	7643640	Great, great, great.
7643640	7644480	Thank you for having me.
7644480	7645320	Have a good day.
