start	end	text
0	11040	Welcome to the Future of Life Institute podcast. My name is Gus Ducker. This is a special episode of the podcast featuring Nathan Labence interviewing Flo Crivello.
11040	26700	Flo is an AI entrepreneur and the founder of Lindy AI. Nathan is the co-host of the Cognitive Revolution podcast, which I recommend for staying up to date on AI. Here is Flo and Nathan.
26940	46940	Man, you know, it's, it's a tough time for somebody that tries to keep up with everything going on in AI. It's like it's gone from, you know, in 2022, I felt like I could largely keep up and, you know, wasn't like missing whole major arcs of, you know, important stories.
47420	63180	And now I'm like, yeah, I'm like totally let go of like AI art generation, for example. And this policy stuff is really hard to keep up with, especially this week. Of course, it's like hitting a, you know, a fever pitch all at once.
63660	76260	But, you know, it's, I love it. So I can't really complain at all. It's just, at some point, you know, got to admit that I have to maybe narrow scope somehow or just let some things fall off. I'm kind of wrestling with that a little bit.
76780	89460	Which, which I think is just like a natural. Yeah, I mean, you know, I hear you. I think it's just a natural part of like the industry evolving. It's like, imagine, you know, talking about like keeping up with computers, right, in like the 80s or something.
89460	98060	It's like, I'm sure at some point it was possible to keep up with computers at large, you know, it's like keeping up with tech is just like, it's like, okay, dude, it's like, it's like half the GDP over, right?
98580	103860	You're doing all this in your second language, right? This is, I assume English is your second at least.
104300	109180	I have an excuse. Yeah, second, I'm actually getting my American citizenship. I had the interview just yesterday.
109580	126580	Wow, congratulations. That's great. I know it's not an easy process, although maybe it's about to get streamlined. I haven't even read that part of the executive order yet, but I understand that there is kind of an accelerated path for AI expertise. Have you seen what that is?
127180	135780	No, but generally, there's good stuff being done in immigration, like they're relaxing a lot of these requirements, they're like closing a lot of loopholes, they're doing a lot of good stuff.
136540	156140	Yeah, I've been thinking of just as kind of a general communication strategy, if nothing else, calling out the domains in which I am accelerationist, which are in fact many, I think, you know, you and I are pretty similar in this respect, where it's like, um, perhaps the singular question of the day.
156140	175060	I am not an accelerationist, but on so many other things, I very much am an accelerationist and like streamlining immigration would be one of those, you know, I would sooner sign up for the one billion Americans plan than kind of, you know, the build the wall plan, certainly.
175700	195380	And I just did a right before this was doing an episode on autonomy and, you know, self-driving. And that's another one where I'm like, holy moly, you know, I don't know if you have a take on this, but the, the recent cruise episode, I find to be, you know, kind of bringing my internal marketing,
195380	212980	Jason, very much to the four where I'm like, we're going to let one incident a like shut down this, you know, whole thing in California. That seems crazy enough. But then the fact that they go out and like do this whole sort of performative self, I mean, whether it's performative or not, maybe it's
212980	223940	sincere, but do this whole self-flagellation thing and, you know, shut the whole thing down nationwide. I'm like, can we, where is our inner Travis on this people? You know, somebody has to stand up for something here at some point.
224020	239020	Totally. I agree. I think it's just the natural order of things, right? It's like, I don't know if you know that piece of history about when the automobile came about. There was this insane law that said you need to have someone walking with a flag in front of the automobile.
239020	250180	That's no more than like four miles an hour. Right. So it's part of the process, man. It's infuriating. I hate it, but in some way, and maybe it's cool, but I made peace with it. I'm like, it's part of the process. You can't really stop. All right.
250180	262540	That's going to do its thing. So, and it doesn't really matter anyway, because like the self-driving cars are not really deploying at a very large scale. And so I'm like, you know, it's not a bottleneck anyway. I don't think it is.
262900	280060	I guess I have two reactions to that. One is like, it feels like if they, if nobody kind of fights through this moment, then there is like this potential for kind of the nuclear outcome where, you know, we just kind of get stuck and it's like, sorry, you know, the standards are so insane, you've got to be, you know, we do have a
280060	296060	little bit of like a chicken and egg problem where, you know, if you had a perfect self-driving car, they'd let you deploy it, but you're not going to get to perfect unless you can kind of deploy it. And, you know, to me, this technology is just an incredible example of where, you know, the relative risk is already pretty high.
296060	309820	As far as I can tell, they already do seem to be as safe or marginally safer, you know, maybe as much as order of magnitude safer already, depending on exactly what stats you look at. And I would just hate to see us get kind of, you know, is we're
309820	338820	like kind of close to maybe some sort of tipping point threshold, whatever, to get stuck in a bad equilibrium of, you know, never get, and then, you know, maybe get stuck and never get out of that chicken and egg thing would just be so frustrating. I drive a 2002 trailblazer that I have sworn never to replace unless it's with a self-driving car. And it's becoming increasingly difficult to keep this thing going, you know, so I'm like, how long do I have to do that?
339820	364820	I have to wait. My other take on this is, I think Tesla is actually like really good. I've borrowed a neighbor's. I don't know if you've done the FSD mode recently. My grandmother came up for a visit. It was fun. I actually took, you know, my 90-year-old grandmother on a trip back to her home, which is like a four-hour drive there, and then I did four hours back all in one kind of big FSD experiment.
364820	387820	I had my laptop in the back, put a seatbelt on my laptop, so it was like recording me and recording us, you know, driving so I could kind of look at the tape later. And I was like, man, this is really good. I had no doubt in my mind coming out of that experience that it's a better driver than like other people I have been in the car with, you know, for starters.
387820	413820	So I'm thinking through my personal life, like, yeah, I'd rather be in the car with an FSD than this person and that person and this other person, you know, and I'd be definitely more likely to let it drive my kids than this other person. So I felt like it was really good. And then the other thing that was really striking to me was the things where it messed up, I mean, there weren't many mess ups for one thing, but like the few mess ups that we had, there were a couple in an eight hour thing.
413820	442820	It was like, if we actually had any mojo and we went around kind of cleaning up the environment, we could solve a lot of this stuff. Like there was one that my neighbor who lent me the car said, you know, you're going to get to this intersection right there on the way to the highway and it's going to miss the stop sign because there's a tree in the way. And I was like, you know, for one thing, probably people miss that too, like, let's trim the trees, you know, and then there's another one where you're getting off the highway and there's a stop sign that's kind of ambiguous, like, it's meant for the people on the service road.
442820	456820	But it appears to be facing you as you're coming off the highway. And so the car saw that and stopped there. And that was probably the most dangerous thing that it did was, you know, stopping where people, you know, coming up the off ramp, like, do not want you or expect you to be stopped there.
456820	470820	But that's another one where you could just go like, put up a little blinder, you know, to just very easily solve that problem. And I imagine people must have that problem too. And we just have no, no will, you know, when it comes to that.
470820	476820	And again, it's I feel like I'm turning into Mark Andreessen. The more I think about self driving over the last few days.
476820	478820	No, I'm with you on that.
478820	491820	So where else are you accelerationist that may not be obvious as we kind of think about this, you know, this kind of AI safety and regulation moment that we're in?
492820	511820	You know, pretty much everywhere, man, like I'm a Libertarian, like I used to work at Uber where I saw regulatory capture and I saw cocktails and I do believe, you know, it's the deepest level that cocktails and regulatory capture and generally, I think it's Menker Olsen who calls them
512820	521820	Extractive institutions, who are just in the business of they don't want to grow the pie, they just want to grab a little bit more of the pie for themselves.
521820	528820	Even if it actually shrinks the pie, they don't care as much as they get bigger chunk. And I think that's the world is just rotten.
528820	540820	With thousands and thousands of these institutions, whether without private or without unions or governmental, it doesn't matter. We just have so many of these cartels floating around and it's killing everything.
540820	557820	Right. It's a tragedy. And I totally understand how folks like Mark Andreessen would be, they have built such a deep and justified hatred and reaction for this nonsense that is destroying everything.
557820	567820	That is immediately just the pattern recognition immediately triggers when they see what's happening with the eye. They're like, ah, it's happening again. They're doing it again.
567820	577820	It's like chill. I totally get it. But this time is really different. This is really something special that's happening, not just in the markets, not just in the economy, not just in the country, in the universe.
577820	585820	Like there is a new form of life that's being built. And this is, we're like a new territory and we need to be careful right now.
585820	598820	Right. And so that's, that's where I'm coming from is like, I totally see that point of view. And I'm like, regulation, for sure there's going to be cartels for sure we're going to screw up 90% of it.
598820	603820	Politics is going to get messy and trench interest are going to get into play.
603820	614820	And it's all worth it because what may very well be on the line, it sounds alarmist, but I'm sorry that we need to say the word may be literally human extinction.
614820	625820	Right. And this is not some tinfoil hat theory. There's a more and more experts that are coming around and saying that it's actually funny. Mark Andreessen, if you dig it up, I'm sure you could find it.
625820	631820	I think it was an interview from him. I want to say between 2017 and 2020 that doesn't help him because he gives so many of those.
631820	637820	But I think he said something like, at the time he was actually appealing to an argument of authority.
637820	645820	He was like, look, he was saying the same things he's saying today, poverty is good. It's just a tool. And by the way, the experts say there's nothing to worry about.
645820	652820	So I don't know. You guys don't know anything about AI. I don't know anything about AI. They do. And they're telling us there's nothing to worry about.
652820	657820	The argument isn't true anymore. The experts are telling us there is something to worry about.
657820	663820	And now it's just like, oh, arbitrary, like a regulatory capture. No, no, it's not regulatory capture.
663820	671820	Like OpenAI was founded on that premise from day one. So if it was regulatory capture, there's like one hell of a plan.
671820	676820	It's like, oh my God, we're going to create this industry and we're going to start regulatory capturing right now.
676820	679820	Right. It's like, that makes no sense. It was literally the plan from day one.
679820	692820	Yeah, that's all I'm coming from. I'm largely in the EAC camp. I am in team technology, team property, team anti-regulation, but here's something very special and potentially very dangerous.
692820	698820	So let's go back to your use of the phrase a new form of life.
698820	707820	I, as you may recall, am very anti-analogy as a way to understand AI because I think it's so often misleading.
707820	716820	And I often kind of say AI, artificial intelligence, alien intelligence, it may be tempting for people to kind of hear or not tempting,
716820	721820	but it may be sort of natural for people to hear you say a new form of life and understand that as an analogy.
721820	732820	But do you mean it as an analogy or I guess we might start to think about like, is that actually just literally true and what conditions would need to exist for it to be literally true?
732820	742820	And you might think about things like, can AI systems reproduce themselves? Are they subject to the laws of evolution?
742820	748820	But for starters, how literal do you mean it when you say that there's this new form of life in AI?
748820	755820	I mean it's pretty literally, I think if you zoom all the way out literally from the birth of the universe,
755820	762820	the evolution of the universe has been towards greater and greater degrees of self-organization of matter.
762820	769820	And there's actually a case to be made that this is just a natural consequence of the second law of thermodynamics,
769820	772820	this amazing book that Iac people love to quote.
772820	774820	Yeah, I was going to say, you're sounding very Iac all of a sudden.
774820	777820	It's a good point. It's called Every Life is on Fire.
777820	784820	And so if you look at the Big Bang, a few fractions of a second after the Big Bang,
784820	788820	it was just subatomic particles and then they ganged up together and formed atoms.
788820	793820	And then the stage after that was the atoms ganged up together and formed molecules.
793820	800820	And then the stage after that, the molecules became bigger and bigger because the stars exploded and caused all sorts of reactions.
800820	806820	And so a few generations of stars later, we have like pretty big molecules and pretty heavy ones.
806820	812820	And then these molecules formed into sort of like protein and RNA and forms of proto-life.
812820	816820	We don't totally understand, there's a chain here that we don't totally understand,
816820	820820	but there's a form of proto-life that formed and then life.
820820	827820	And so you can think of like, I think it was just a DNA, actually it was RNA, DNA, nucleus of a cell,
827820	830820	mitochondria came into that, and then, okay, good, we have a cell.
830820	834820	And then the cells started ganging up together and now we have multicellular organisms.
834820	839820	And then we have brains at some point, like there's like a big leap, but we have brains,
839820	843820	like on that great march towards greater and greater degrees of step-organization.
843820	847820	And at some point we have us, which with a little bit of hubris perhaps,
847820	851820	considering the apex of that thing for now.
851820	855820	It just seems crazy to me that everybody is saying like, one, this is totally normal.
855820	857820	Oh, this is normal.
857820	863820	This is quintillions of atoms that are organized in this weird, super coherent fashion
863820	865820	that are pursuing a goal in the universe.
865820	869820	Like what's happening right now on Earth is all the weird to begin with.
869820	873820	So people are all deep thinking that this is normal and that's what it is,
873820	876820	and that this march is going to stop at them.
876820	879820	And they're like, well, maybe we're going to get slightly smarter,
879820	884820	or maybe we're going to get augmented and I'm like, you are such a leap compared to an atom,
884820	888820	or compared to a bacteria, that there is no reason to expect that there wouldn't be
888820	892820	another thing above you that is as much more complex or bigger than you,
892820	894820	as the new world to the bacteria.
894820	897820	Like there's nothing in the universe that forbids that from happening.
897820	901820	From a being to exist that is about as big as the planet or the galaxy.
901820	904820	Like there's nothing forbidding that in the universe from happening.
904820	908820	And from the first time now, if you squint, we can sort of see how that happens.
908820	914820	And silicon-based intelligence certainly seems to have a lot of strengths
914820	917820	at its sleeve versus carbon-based intelligence.
917820	920820	And so no, I actually sort of mean that pretty vitrally.
920820	924820	It is sort of in line with the march of the universe and this is the next step,
924820	926820	perhaps it's significant.
926820	933820	And so I am hopeful that we can manage this transition without us being destroyed.
933820	935820	That's what I want to have.
935820	941820	Does that imply an inevitability to advanced AI?
941820	946820	I guess a lot of people out there would say, hey, let's pause it,
946820	950820	slow the whole thing down, and then you get kind of the response from an open AI
950820	954820	where they're sort of saying, yeah, we do take these risks very seriously
954820	957820	and we want to do everything we can to avoid them.
957820	960820	But we can't really pause or we don't think that would be wise
960820	963820	because then the compute overhang is just going to grow
963820	967820	and then things might even be more sudden and disruptive in the future.
967820	976820	Where are you on kind of the inevitability of this increasingly capable AI coming online?
976820	979820	I don't think it's totally inevitable.
979820	983820	I am generally a huge believer in human agency.
983820	987820	I think we can do pretty much anything we set our minds to.
987820	991820	I see a contradiction, by the way, in the EACC argument that like,
991820	993820	on the one hand it's inevitable and try to stop it,
993820	996820	on the other hand, oh my god, if you do this, I'm going to stop it.
996820	998820	It's like, you got to decide here.
998820	1001820	So unfortunately, it's not necessarily inevitable.
1001820	1004820	I am actually worried as much as the next guy, I agree,
1004820	1008820	there is a risk that we over-regulate and miss out on the upside.
1008820	1010820	And the upside is significant.
1010820	1013820	And if you look like during the Middle Ages,
1013820	1016820	we successfully as a civilization stopped progress
1016820	1019820	and in a lot of countries, if you look at North Korea, they did it.
1019820	1021820	They successfully stopped progress.
1021820	1022820	So you can stop progress.
1022820	1023820	Progress is not inevitable.
1023820	1025820	Or maybe it is actually quite fragile.
1025820	1027820	So no, I don't think it's inevitable.
1027820	1029820	And I'm hopeful that we can, again,
1029820	1033820	I want us to get the upside without experiencing the downside.
1033820	1037820	The North Korea example is an interesting one.
1037820	1040820	If I was going to kind of dig in there a little bit more, I might say,
1040820	1045820	okay, I can understand how if things go totally off track,
1045820	1052820	then we could maybe enter into a low or no or even negative progress trajectory.
1052820	1057820	If there were a nuclear war, then we may not come back from that for a long time.
1057820	1062820	Or if whatever, an asteroid hit the earth or a pandemic wiped out 99%,
1062820	1066820	like there's extreme scenarios where it's pretty intuitive for me
1066820	1072820	to imagine how progress might stop or just be whatever,
1072820	1074820	greatly reversed or whatever.
1074820	1079820	If I'm imagining kind of a continuation-ish of where we are,
1079820	1087820	then it's harder for me to imagine how we don't kind of keep on this track.
1087820	1091820	Because it just seems like everything is, we're in this, I would call it,
1091820	1093820	I don't know if it's going to be a long-term exponential,
1093820	1098820	but we seem to be entering a steep part of an S-curve where hardware is coming
1098820	1102820	on lines by the order of magnitude and at the same time,
1102820	1106820	like algorithmic improvements are taking out a lot of the compute requirements.
1106820	1109820	And we're just seeing all these existence proofs of what's possible
1109820	1113820	and all sorts of little clever things and scaffolding along the lines
1113820	1116820	of some of the stuff that you're building is getting better and better.
1116820	1120820	Is there a way that we can, do you think it is realistic to think we could
1121820	1128820	kind of meaningfully pause or even stop without a total derailment of civilization?
1128820	1132820	The derailment of civilization thing, you could imagine the most extreme scenario
1132820	1135820	which I am not proposing, but you could imagine the most extreme scenario
1135820	1137820	which is no more Warsaw.
1137820	1141820	You do not exponentially improve your semi-conductors anymore.
1141820	1145820	That'd be crazy, right? But there wouldn't derail civilization.
1145820	1148820	Civilization is not predicated upon Warsaw.
1148820	1151820	We would do just fine with the chips we've got today.
1151820	1155820	And if anything, I think we have a lot of overhang from the chips we have today,
1155820	1157820	a few to choose overhang, right?
1157820	1162820	So I actually think it is possible to do that if we wanted to.
1162820	1166820	And I don't think that even this, which I think is the most extreme scenario,
1166820	1168820	would actually derail civilization.
1168820	1175820	Well, we are actually lucky in that there are a few choke points in the industry.
1175820	1180820	Actually, more than a few. There is ASML, there's TSMC, there's NVIDIA,
1180820	1183820	like all of those three are individually at our choke point.
1183820	1186820	Like every regulator could at any point grab one of them and be like,
1186820	1188820	no more, you just stop, right?
1188820	1191820	Or you add this chip into all of your GPUs moving forward,
1191820	1193820	so we have a kill switch. At the very least, we have that.
1193820	1197820	So if shit really hits the fan, we have an automatic thing in place
1197820	1200820	that shuts down the very GPU on this, right?
1200820	1204820	Now that would be disruptive, but potentially less disruptive than the rogue ASI.
1204820	1208820	So no, I actually think it is very much possible.
1208820	1212820	This thing's all on the table, and I don't think there would be all that disruptive.
1212820	1217820	So maybe that's a good transition to kind of where we are right now, right?
1217820	1219820	We just had this executive order put out this week,
1219820	1224820	and I think everybody's still kind of absorbing the 100-plus pages
1224820	1226820	and trying to figure out exactly what it means.
1226820	1228820	What's your high-level reaction to it?
1228820	1231820	And then I'll get into some of the specifics.
1231820	1236820	First of all, it's an executive order for now. It is very early.
1236820	1241820	Overall, I am pleasantly surprised, not by the specifics,
1241820	1246820	but by the facts that were reacting quickly,
1246820	1251820	by the facts that the measures that are proposed are not insane.
1251820	1255820	Like, I was afraid of, like, there's a really good case to be made.
1255820	1257820	The second look, we have a different processing case.
1257820	1260820	Now, a bunch of 70, 80-year-olds go running as they don't know anything
1260820	1262820	when they were born. There was no mobile phone, right?
1262820	1264820	Can't really blame them for not really understanding anything.
1264820	1267820	And so I was afraid that the regulation would go something like,
1267820	1271820	if you install Microsoft Office in your AI, then you have to make a report.
1271820	1274820	So the regulation actually sort of makes sense.
1274820	1277820	It's talking about Flops. It's talking about all those things of training.
1277820	1279820	So I think it's a step in the right direction.
1279820	1283820	I'm actually happy about what's happening with this executive order.
1283820	1288820	Now, the specifics, look, the problem is that it's almost impossible
1288820	1293820	to regulate AI in a way that doesn't have any loophole.
1293820	1297820	So they're regulating it according to the new old Flops, and that's okay.
1297820	1299820	But that's the end of the day, and then you get stuck into,
1299820	1302820	okay, what happens when you have algorithmic improvements?
1302820	1305820	What happens when you do URL instead of computing?
1305820	1308820	And like, that's just a lot of different loopholes that researchers are going to find.
1308820	1311820	And so I think, overall, it's an encouraging first step.
1311820	1314820	It's funny. You know, there have been proposals around even, like,
1314820	1317820	a flop threshold that would drop progressively over time
1317820	1322820	in kind of anticipation of the algorithmic improvements.
1322820	1328820	That's even a more probably challenging one to put out into the world,
1328820	1333820	especially given people are not in general great at extrapolating technology trends
1333820	1339820	or don't want to accept regulation in advance of stuff actually being invented.
1339820	1344820	So we've got this flop threshold thing where basically, as I understand it so far,
1344820	1346820	like, if you're going to do something this big,
1346820	1349820	you have to tell the government that you're going to do it
1349820	1353820	and you have to bring your test results to the government.
1353820	1356820	I would agree with that. That seems like a pretty good start.
1356820	1362820	And also the threshold seems like pretty reasonably chosen at 10 to the 26.
1362820	1369820	Any, you know, kind of refinements on that or quibbles that you would put forward
1369820	1373820	that you think like, you know, maybe the next evolution of this should take into account?
1373820	1377820	I think ultimately we're tiptoeing around the issue,
1377820	1384820	but ultimately we need to come to an actual technical blanket solution.
1384820	1392820	Like, we will not solve ASI alignment by asking for reports from AI companies.
1392820	1394820	That's not how it's going to happen.
1394820	1396820	So again, I think it's a step in our direction.
1396820	1399820	I'm happy with the action. I'm happy the action is not totally nonsensical.
1399820	1404820	But at the end of the day, we're going to have to talk about the kill switch.
1404820	1409820	The proposal I just made is one that I see more and more talked about
1409820	1412820	and that's the one that I would feel best about.
1412820	1416820	You've got to put this chip into your H100s and the government
1416820	1421820	and there's like a centralized entity that can shut down all GPUs all at once.
1421820	1423820	And by the way, it wouldn't necessarily shut down every computer
1423820	1427820	because your laptop doesn't have an H100, your iPhone doesn't have an H100.
1427820	1428820	That's fine.
1428820	1431820	Over the long term, Moore's Law makes it so that your laptop and your phone
1431820	1435820	actually end up with an H100, but at least that dies us a few years
1435820	1438820	to make progress on AI safety and alignment.
1438820	1444820	Ideally, we would then automate just like reportedly the Russians did during the Cold War.
1444820	1445820	We would automate.
1445820	1450820	Like, we would set up some detection systems to God knows how we would do that.
1450820	1452820	But hey, there's no ASI going wrong.
1452820	1455820	Like, the world is really changing rapidly.
1455820	1460820	We're assuming it's not too late, which it may be because at that point God knows.
1460820	1464820	But you could very basically that would give us the best weapon against the ASI.
1464820	1468820	We would have like a gun against the ASI's hand and kill all the GPUs.
1468820	1470820	You cannot operate anymore.
1470820	1473820	God knows how effective that would be because at that point all bets are off.
1473820	1475820	If you have an ASI God knows what it does and how it connects itself.
1475820	1478820	But that would be what it would feel best about.
1478820	1482820	Do you have any sense for how that would be implemented technically?
1482820	1487820	It seems like you would almost want it to be something that you could kind of broadcast.
1487820	1495820	You know, you almost want like a receiver on chip that would react to a particular broadcast signal
1495820	1499820	and just kind of because you would not want to have like, you know, an elaborate chain of command
1499820	1504820	or, you know, relying on like the dude who happens to be on the night shift at the, you know,
1504820	1508820	the individual data centers to go through and like, you know, pull some lever, right?
1508820	1511820	Do you know of anybody who's done kind of advanced thinking on that?
1511820	1514820	That stuff is like, you know, you hear a lot of these like kill switch things,
1514820	1520820	but in terms of how that actually happens so that it's not dependent on, you know,
1520820	1525820	a lot of people coming through in a key moment, I haven't heard too much, to be honest.
1525820	1528820	No, I haven't seen too much results on that.
1528820	1532820	But, you know, I think the technical challenge does nothing in principle that makes the technical challenge
1532820	1539820	unsolvable. Like we've already had a chip that can be broadcasted to for like a dollar from space,
1539820	1543820	like the GPS chip does a lot of chips and like it has one on your phone.
1543820	1545820	And so why not put the GPS like chip?
1545820	1547820	Maybe we could literally piggyback the GPS protocol.
1547820	1551820	I don't know, but why not put the chip like that in every, in every GPU?
1551820	1558820	Again, if you have an ASI, God knows, like maybe it hacks the chips before we get a chance,
1558820	1561820	you know, it hacks the satellites that forecast the thing, I have no idea.
1561820	1568820	But again, I think pointing in this direction is what I would like things to go into the limit.
1568820	1573820	I think the basically, and that's like the most extreme version of this proposal,
1573820	1576820	but like the Yutkovsky airstrike proposal.
1576820	1582820	That's like, you cannot accumulate billions and billions of dollars of H100s and build this thing.
1582820	1585820	Else we will go up to airstrike.
1585820	1591820	That's the most extreme version of this, but that actually I think is directionally correct.
1591820	1597820	Like we, this is going to be the most powerful force in human history, maybe even in the universe.
1597820	1604820	You cannot accumulate that stuff anymore than you can accumulate enriched plutonium, right?
1604820	1607820	We've got a, we've got a four-bit stat that's the lowest level possible.
1607820	1611820	And so that level cannot be the application layer because the application layer is just,
1611820	1615820	it's just to diffuse those like a thousand startups everywhere and you get in the garage can build one.
1615820	1619820	It's got to be at a took point and the took point today is the city code.
1619820	1625820	Yeah, let's unpack that a little bit more because I think that has been an interesting debate recently.
1625820	1631820	You'll hear this kind of call for let's not regulate model development.
1631820	1637820	Let's regulate applications and then, you know, we can kind of have medical regulation for the medical
1637820	1640820	and everything can be more appropriate and like fit for purpose.
1641820	1646820	And, you know, maybe there's something else to be said for that.
1646820	1654820	But yeah, I mean, if you're really worried about tail risk, it's like probably not going to be sort of medical, you know,
1654820	1663820	device style regulation of, you know, diagnostic models or whatever that is going to keep things under control.
1663820	1670820	So maybe you could even do a better job of steelmanning the case for the application level regulation.
1670820	1676820	But I guess, you know, why do you think that give your account of why that's not viable in a little bit more detail?
1676820	1683820	Yeah, I think the steelman here is like, look, people are going to use forks to poke each other in the eye.
1683820	1685820	That's not a reason to forbid the fork.
1685820	1686820	Like forks are awesome.
1686820	1689820	We love forks just for people from poking each other in the eye with them, right?
1689820	1694820	The problem is that as the fork in this analogy becomes more and more powerful,
1694820	1700820	the argument loses more and more of its defense because ultimately it's just a risk benefit analysis.
1700820	1701820	Right.
1701820	1706820	And so the risk becomes greater and greater as the artifact becomes more and more powerful.
1706820	1709820	So more powerful than the fork and al 15.
1709820	1712820	And so, you know, the opinions vary about that.
1712820	1715820	But look at at this point, if you look at the data,
1715820	1719820	you actually save lives by heavily regulating the sale of al 15.
1719820	1723820	You can't just be like, oh, sell them to everyone and just for big people from shooting each other with them.
1723820	1724820	It's like, it's an al 15.
1724820	1726820	What do you expect people to do with them?
1726820	1730820	Now, in the more in the most extreme scenario, enriched uranium,
1730820	1733820	you can't be like, you can buy all the enriched uranium you want.
1733820	1735820	You don't even need to fill up a form, which by the way,
1735820	1738820	that is all the executive order says right now, at least fill up a form.
1738820	1741820	Can you please at least tell us what you have to do?
1741820	1744820	So hey, you can build, you can build all the enriched uranium you want.
1744820	1747820	Just don't bond us with us with it, please.
1747820	1749820	Like when we roll it in this disappear, you can do it.
1749820	1751820	Oh, no, that's not, that's not how it works.
1751820	1756820	So that, that, that is why I think it's important to regulate the, the silicone layer.
1756820	1763820	Do you have an intuition for sort of how likely things are to get crazy at kind of either various
1763820	1766820	time scales or potentially various like compute thresholds?
1766820	1771820	I was realizing, I did an episode with Jan Tallin a couple of months back,
1771820	1775820	just in the wake of the GPT-4 deployment.
1775820	1779820	And he said, we dodged a bullet with GPT-4 or something like that.
1779820	1784820	Like in his mind, we didn't know if, you know, even at the GPT-4 scale,
1784820	1789820	like that might have already been, you know, no, no real principled reason to believe that
1789820	1795820	with any, with like super high confidence that the GPT-4 scale was not going to cross
1795820	1799820	some, you know, critical threshold or whatever.
1799820	1803820	I guess I don't really have a great sense for this.
1803820	1808820	I just kind of feel like, and this was purely like gut level intuition that, yeah,
1808820	1811820	we could probably do like GPT-5 and it'll probably be fine.
1811820	1813820	And then kind of beyond that, I'm like, I have no idea.
1813820	1819820	Do you have anything more specific that you are working with in terms of a framework of like how,
1819820	1824820	you know, when you hear, for example, Mustafa from inflection say, oh yeah,
1824820	1827820	we're definitely going to train, you know, orders of magnitude bigger than GPT-4
1827820	1828820	over the next couple of years.
1828820	1832820	Are you like, well, as long as you stay to two to three orders of magnitude more,
1832820	1833820	we'll be okay.
1833820	1837820	Or like, I just have no, you know, we're just flying so blind,
1837820	1840820	but I wonder if maybe you're flying slightly less blind than I am.
1840820	1846820	I am of the opinion that GPT-4 is the most critical component for AGI.
1846820	1851820	And that's the gap from GPT-4 to proper AGI is not research, it's engineering.
1851820	1854820	It sits outside the model.
1854820	1860820	So I think we have a capabilities overhang here that can turn GPT-4, as it is today,
1860820	1864820	into AGI, into proper AGI.
1864820	1867820	I think generally that's the case for any technology.
1867820	1872820	If you look, for example, at Bitcoin, what changed from a technological standpoint
1872820	1874820	that allowed Bitcoin to happen?
1874820	1877820	It was the same technology we'd had for a while and yet Bitcoin,
1877820	1879820	it's going to go wild to happen.
1879820	1884820	So there was this overhang and Bitcoin, whatever your opinion about crypto,
1884820	1887820	changed a lot of games, right?
1887820	1889820	I think there's this huge overhang with GPT-4.
1889820	1893820	I think we basically have a reasoning module of AGI.
1893820	1897820	I don't know if you saw this paper that found literally just asking it,
1897820	1899820	hey, take a deep breath and take a step back.
1899820	1901820	Just take a step back apparently also makes a huge difference.
1901820	1904820	So I think there's a lot of tricks like that that will make a difference.
1904820	1909820	The sort of cognitive architectural layers around GPT-4 I think can bring it to AGI.
1909820	1914820	That is also why you asked me about what sort of regulation I wish was put into place.
1914820	1916820	We need to stop open sourcing this model.
1916820	1918820	We don't know what kind of overhang exists out there.
1918820	1922820	I don't think Lama-2 is there, but like I said, I think GPT-4 is there.
1922820	1925820	So Lama-3, if it's GPT-4 level, boom, it's too late.
1925820	1926820	The weights are out there.
1926820	1930820	Okay, now you can do, maybe you can put strap on there.
1930820	1933820	So we need to stop open sourcing this next.
1933820	1939820	I expect my timelines for proper AGI to emerge is two to eight years.
1939820	1944820	I think there's a more than even chance of AGI emerging in two to eight years.
1944820	1947820	I think the base scenario is things are going to go well just for the record.
1947820	1949820	I don't think there's like a 99% chance of doom.
1949820	1953820	But even if it's 10%, I think it's worth being very, very worried about.
1953820	1954820	That's enough for me.
1954820	1959820	10% of all of us dying like I'm talking about it, please.
1959820	1965820	So two to eight years, 50% chance of AGI, things probably will go well,
1965820	1968820	except for, you know, CVD digital disruption.
1968820	1969820	There's going to be like stuff.
1969820	1972820	There's going to be a crazy shit happening, but two to eight years.
1972820	1973820	And after that, all bets off.
1973820	1977820	I have no idea what the bootstrapping to ASI look like,
1977820	1981820	but I don't expect ASI to take more than 30 years.
1981820	1984820	So I expect that you and I in our lifetimes are going to see ASI.
1984820	1986820	So that's a pretty striking claim.
1986820	1989820	I think you probably puts you in a pretty small minority.
1989820	1995820	And I don't think I'm really there with you when you say that you think GPT-4
1995820	2002820	kind of already contains the, you know, the kind of necessary core element for an AGI.
2002820	2004820	So I'd like to understand that a little bit better.
2004820	2011820	I mean, you'll have a lot of people who will say, you know, look, it can't play tic-tac-toe.
2011820	2019820	I think on some level, those kind of, oh, look at these like simple failure objections are kind of lame
2019820	2023820	and sort of miss the point because of all things obviously can do.
2023820	2027820	But I do, you know, if I'm thinking like, does this system seem like it has this kind of sufficiently
2027820	2029820	well-developed world model?
2029820	2034820	Or, you know, I'm not even sure exactly how you're conceiving of the core thing.
2034820	2040820	But, you know, for a question like that, I would say those failures maybe are kind of illuminating.
2040820	2048820	On the other hand, I'm sure you've seen this Eureka paper out of NVIDIA recently where they used GPT-4
2048820	2054820	as a superhuman reward model author to teach robot hands to do stuff.
2054820	2060820	And I thought that one was pretty striking because as far as I know, and I actually used the term Eureka moment,
2060820	2069820	many times said, we don't see yet Eureka moments coming from highly general systems.
2069820	2072820	You know, we see Eureka moments from like an alpha go.
2072820	2076820	We haven't really seen like Eureka moments from a GPT-4 until maybe this.
2076820	2083820	This seems like maybe one of the first things where it's like, wow, GPT-4 at a task that requires a lot of expertise.
2083820	2088820	That is designing reward functions for robot learning, robot reinforcement learning.
2088820	2093820	GPT-4 is meaningfully outperforming human experts.
2093820	2096820	And so I think it's very appropriate that they call it Eureka.
2096820	2098820	What do you think is the core thing?
2098820	2100820	You know, is it this like ability to have Eureka moments?
2100820	2101820	Is it something else?
2101820	2103820	Why do you feel like it's there?
2103820	2106820	And does it not trouble you that it can't play tic-tac-toe?
2106820	2115820	For the sake of this conversation, I'm going to define a GI as a seed AI, an AI that can recursively self-improve.
2115820	2120820	That's a much more narrow definition of a GI than most people use, but that's actually what I care about.
2120820	2125820	Can we enter this recursive loop of self-improvement that puts track surface to ASI?
2125820	2127820	In order to get there, you don't need to play tic-tac-toe.
2127820	2133820	You need to be good enough, and the world good enough here is important,
2133820	2140820	a good enough either software engineer or chip designer or AI and ML researcher.
2140820	2141820	One of these things.
2141820	2144820	So something that can get you to put track.
2144820	2147820	And so good enough does not mean better than the best human.
2147820	2150820	It doesn't even mean better than the average human.
2150820	2157820	It just means good enough that you can make a difference, a positive difference in your own ability to get better.
2157820	2158820	Right?
2158820	2162820	So if you enter the recursive loop of self-improvement, then mathematically it's over.
2162820	2165820	And yeah, when I see the NVIDIA paper, I see that.
2165820	2168820	When I see our own experience with the model.
2168820	2171820	So today we are using Lindy to write her own integrations,
2171820	2174820	and Lindy is writing more and more of her own code.
2174820	2175820	I see that.
2175820	2180820	Even as it belongs to AI researchers and ML researchers,
2180820	2187820	my hypothesis is that OpenAI is using GPT-4 more and more internally to perform AI research.
2187820	2192820	My not hypothesis is the fact is that NVIDIA is releasing papers that's like,
2192820	2195820	well, not only can we use it for AI research through this Eureka paper,
2195820	2197820	but we can also use it for chip design.
2197820	2198820	It works super well.
2198820	2201820	We trained an AI model that does chip design super well.
2201820	2206820	So we are starting to see the glimpses of that kind of recursive loop of self-improvement.
2206820	2211820	Basically, the world model question kind of on the sidestep,
2211820	2215820	because I feel like at this point the debate has become silly for people who argue that it's bad
2215820	2217820	or doesn't have a world model.
2217820	2219820	What matters is, is it good enough?
2219820	2223820	And so even if it just overfits its training set,
2223820	2226820	even if it's just predicting the next token and not actually understanding anything,
2226820	2229820	I actually really do believe it understands a lot.
2229820	2233820	But even if it's not, you can imagine it does this many-dimensional space
2233820	2235820	with a ton of data points in there.
2235820	2237820	And it's good by interpolating between the data points
2237820	2241820	and it needs much more data points to understand anything than a human.
2241820	2246820	And so there's that envelope in that space where the data points are dense enough
2246820	2248820	that it can perform.
2248820	2250820	And so that's called the convex hole.
2250820	2252820	And then there's data points outside that convex hole,
2252820	2255820	and it does really poorly outside the convex hole, much more poorly than humans.
2255820	2259820	It's convex hole requires a lot more density than humans do exist.
2259820	2264820	There's multiple questions, which are, one, all these data points inside,
2264820	2267820	the convex hole is the sum of all human knowledge.
2267820	2269820	GP for today knows more than you.
2269820	2271820	I don't know that it can reason better than you,
2271820	2273820	that's the expanding the convex hole thing,
2273820	2275820	but it knows more than you inside that convex hole.
2275820	2279820	And so inside that convex hole, an AI researcher that's read every paper ever,
2279820	2281820	not just in AI, but in mass and biology,
2281820	2283820	every paper ever finds the entirety of the internet,
2283820	2287820	is it better than a human AI researcher?
2287820	2290820	I think the answer is yes.
2290820	2293820	Even if it's not better, there's the outside of that convex hole,
2293820	2296820	and this is my point about the capabilities of a hang,
2296820	2301820	can we get this AI model through prompting, through cognitive architecture,
2301820	2303820	to do better outside its convex hole?
2303820	2305820	And we'll see that all the time,
2305820	2307820	seeing papers come out to bed like,
2307820	2310820	hey, we have found an automatic way to rewrite a prompt that makes it a lot better.
2310820	2314820	We have found a way that people that came out a few days ago,
2314820	2318820	that's like, hey, if you ask the model to take a step back
2318820	2322820	and to rephrase the problem you're giving it in terms of a universal problem,
2322820	2324820	it performs a lot better.
2324820	2326820	And that makes total sense,
2326820	2332820	because the specific of the problem is probably not seen as that specific problem in its dataset,
2332820	2336820	but if you ask it to reframe it, it's basically translating the problem into a form
2336820	2338820	in which it's comfortable with.
2338820	2342820	And so we're actually getting it to grow its convex hole like that.
2342820	2347820	That's my take, is I think the convex hole is good enough to get to that good enough point,
2347820	2349820	and I think we can grow that convex hole.
2349820	2354820	And so I think that basically, if GPT-4 isn't a CDI, it will still GPT-5 is one.
2354820	2356820	Yeah, that's an interesting framing.
2356820	2358820	I find your analysis there pretty compelling.
2358820	2365820	The idea that, you know, given what we have seen from like a Eureka,
2365820	2370820	you know, with this robot training, or there was another interesting one recently,
2370820	2374820	I think it was out of Microsoft, I covered this in one of the research rundown episodes,
2374820	2380820	on recursive or iterative improvement on a software improver.
2380820	2383820	So they basically take a real simple software improver, you know,
2383820	2387820	that can improve a piece of software, and then they feed that software improver to itself
2387820	2390820	and just run that on itself over and over again.
2390820	2395820	You know, it kind of tops out because it's not, it doesn't, you know,
2395820	2398820	in this framework, it doesn't have access to like tinkering with, you know,
2398820	2404820	possible methods for training itself, but it makes significant improvement
2404820	2409820	and gets us some pretty advanced algorithms where it starts to do like genetic search
2409820	2411820	and, you know, a variety of things where I'm like,
2411820	2414820	I don't even really know what that is, you know, like simulated annealing algorithms.
2414820	2418820	I'm like, what, you know, but it comes up with that and, you know,
2418820	2420820	uses that to improve the improver.
2420820	2424820	And, you know, this is all measured by how effectively it can do the downstream task.
2424820	2430820	It does seem like it's not a huge stretch to say that, you know,
2430820	2435820	could you take the architecture of GPT-4 and start to do, you know,
2435820	2442820	parameter sweeps and start to, you know, mutate the architecture itself.
2442820	2444820	It seems like it probably can do that.
2444820	2446820	And I would agree, you know, it probably does.
2446820	2450820	Yeah, certainly just based on what I do, you know, with GPT-4 for coding,
2450820	2454820	I would have to imagine that it is in heavy use as they're, you know,
2454820	2459820	performing all that kind of exploratory work, you know, within an open AI.
2459820	2462820	And so, yeah, and I think to your point,
2462820	2466820	we all see enough of these signs of life across the board in a lot of different areas.
2466820	2469820	A lot of institutions are like, ah, a little bit of very good stuff
2469820	2471820	from here, a little bit here, a little bit here.
2471820	2474820	It's not very hard to imagine it getting to a state velocity,
2474820	2477820	to imagine it going super critical and pass a certain threshold where I say,
2477820	2479820	okay, now boom, it can ready take off.
2479820	2484820	So, and I've actually heard multiple people from open AI say that they believe,
2484820	2487820	and I agree with their conclusion.
2487820	2489820	And they actually told me that before I agreed with them,
2489820	2492820	they told me that at the very beginning of the year,
2492820	2494820	so before GPT-4 was widely available.
2494820	2497820	And they told me, you know, I think we're like, you know,
2497820	2500820	we have a GEI and we're in a slow take off.
2500820	2502820	And I felt something like this, crazy.
2502820	2508820	Well, they didn't say, sorry, they basically were talking about GPT-4.
2508820	2514820	I think, and I am not representing that this is the universal position of open AI,
2514820	2517820	but I've heard multiple people from open AI and other labs tell me that.
2517820	2520820	We have a GEI and we're in a slow take off.
2520820	2524820	So, given that, okay, we've got this compute threshold.
2524820	2527820	We maybe need a kill switch.
2527820	2531820	Now I'm getting, we started this conversation with me, with my, you know,
2531820	2536820	IAC side coming out and, you know, being like,
2536820	2540820	why can't we get myself driving car on the road and tolerate,
2540820	2543820	you know, some reasonable amount of risk to do that.
2543820	2546820	Now my other side is coming out and I'm like,
2546820	2548820	okay, what else might we do, right?
2548820	2551820	We've got the AI safety summit going on right now in the UK.
2551820	2555820	I thought it was cool to see today that there's some kind of joint statements
2555820	2559820	between Chinese and Western academics and, you know,
2559820	2561820	thought leaders in the space where they're kind of saying,
2561820	2563820	yeah, we need to work together on this.
2563820	2568820	Human extinction is something that we think could happen if we're not careful.
2568820	2572820	Do you have a point of view on kind of collaborating with China
2572820	2575820	or coordinating with China?
2575820	2577820	I mean, that's a tough question, obviously.
2577820	2579820	Nobody really knows China.
2579820	2581820	I don't think super well, but what do you think about that?
2581820	2584820	I mean, are we naive to hope?
2584820	2586820	I kind of feel like what else are we going to do except give it a shot?
2586820	2588820	Yeah, 100%.
2588820	2590820	And there is ample precedent.
2590820	2594820	You know, everybody is always talking about these coordination problems.
2594820	2597820	They've taken like the one-on-one course of game theory and they're like,
2597820	2598820	look, we can't coordinate.
2598820	2603820	Well, like if you take game theory one or two, it's like solutions to the coordination problem, right?
2603820	2609820	And so the solution to the collaboration problem is few players in a very iterated game.
2609820	2611820	And that is the game right now.
2611820	2614820	There's very few players and they're all in a very iterated game.
2614820	2617820	They're not the best buddies, but they are actually able to agree on a lot of things.
2617820	2620820	And so we can coordinate with China.
2620820	2623820	And again, to your point, what choice do we have anyway, right?
2623820	2627820	And even if we do not coordinate with them, again, there's enough truth holds
2627820	2630820	enough of which are American, right?
2630820	2632820	NVDI is an American company, last time I checked.
2632820	2636820	And so there's enough truth holds that we could actually do very much
2636820	2640820	not give them a choice like, hey, your GPUs now have the chip right here.
2640820	2643820	And so whether you like it or not, we have a satellite up here
2643820	2646820	and we can tell the GPUs out there.
2646820	2651820	And that wouldn't be, we could even just downright for big GPUs,
2651820	2652820	by the way, to be sold in China.
2652820	2654820	Like we've done stuff like that before.
2654820	2658820	So no, I think coordination is definitely possible
2658820	2660820	and I actually think it's going to happen.
2660820	2664820	I'm actually really very much encouraged by, well, winning.
2664820	2667820	Like I think the safety side is making really good progress.
2667820	2669820	There is rising public awareness.
2669820	2671820	I think Duffington is doing an amazing work here.
2671820	2672820	The regulation is coming.
2672820	2674820	It's mostly sensical.
2674820	2678820	There's this sort of progress that's happening across the board.
2678820	2681820	AI labs are investing more and more in safety and alignment.
2681820	2684820	Even from a technical standpoint, the work that AnsibleTik is doing,
2684820	2686820	I think is absolutely brilliant.
2686820	2689820	So we're making really good progress across the board here.
2689820	2692820	I don't want to represent that it will be on the board.
2692820	2693820	Yeah, I totally agree.
2693820	2697820	I would say my kind of high level narrative on this recently has been,
2697820	2701820	it feels like we're at the beginning of chapter two of the overall AI story.
2701820	2704820	And chapter one was largely, you know,
2704820	2708820	characterized by a lot of speculation about what might happen.
2708820	2711820	And amazingly, kind of at the end of chapter one,
2711820	2713820	beginning of chapter two, not all,
2713820	2718820	but like a large chair of the key players seem to be really serious minded
2718820	2721820	and, you know, well aware of the risks.
2721820	2725820	And it's easy to imagine for me a very different scenario where everybody,
2725820	2729820	you know, all the leading developers are like highly dismissive of the potential problems.
2729820	2733820	But it's hard for me to imagine a scenario that would be like all that much better
2733820	2737820	than, you know, the current dynamic.
2737820	2743820	So I do feel, you know, like overall, you know, pretty, pretty lucky
2743820	2747820	or pretty grateful that, you know, things are shaping up at least, you know,
2747820	2751820	to give us a good chance to try to get a handle on all this sort of stuff.
2751820	2752820	One last question.
2752820	2753820	This is super philosophical.
2753820	2761820	I know you got to go, but how much depends in your mind on whether or not,
2761820	2766820	let's say Silicon base intelligence or AI systems or whatever might become
2766820	2770820	or maybe already are, you know, I'm not sure how we would ever tell the kinds of things
2770820	2772820	that have subjective experience.
2772820	2777820	You know, does it matter to you if it feels like something to be GPT for?
2777820	2780820	Have you heard of the world move?
2780820	2784820	I think it's Zen philosophy in Buddhism.
2784820	2789820	There's this story that's like someone asks someone else like, hey, does kind of Doug
2789820	2791820	have the essence of a Buddha?
2791820	2795820	If the Buddha is everywhere and he never being kind of Doug have the essence of a Buddha.
2795820	2798820	And the answer to that is move.
2798820	2801820	And move means neither yes or no.
2801820	2803820	It's a way to unask the question.
2803820	2806820	It's a way to reject the premise of the question.
2807820	2813820	And basically in this sense, it means there is no such thing as the essence of the Buddha.
2813820	2817820	It's like the same question is like, hey, what happened before the universe existed?
2817820	2818820	Move.
2818820	2821820	There was no before because the bills of the universe was the bills of time.
2821820	2824820	So the will to be full only makes a sense in the context of the universe.
2824820	2826820	And so anyway, that's all of my insight.
2826820	2830820	Whenever I ask a question, whenever someone asks me questions about subjective experience
2830820	2832820	and consciousness, I'm like move.
2832820	2833820	It doesn't exist.
2833820	2834820	It doesn't matter.
2834820	2835820	It's immeasurable.
2835820	2837820	It's not a scientific thing.
2837820	2839820	And so move.
2839820	2840820	All right.
2840820	2844820	Well, some questions bound to remain unanswered.
2844820	2846820	And I appreciate your time today.
2846820	2848820	This is always super lively.
2848820	2850820	Next time I want to get the Lindy update.
2850820	2852820	And at some point I want to get access.
2852820	2854820	But for now, I'll just say Fluckravello.
2854820	2856820	Thank you for being part of the Cognitive Revolution.
2856820	2858820	Thanks, Mason.
