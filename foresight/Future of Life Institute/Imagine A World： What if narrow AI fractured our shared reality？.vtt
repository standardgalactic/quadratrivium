WEBVTT

00:00.000 --> 00:02.500
On this episode of imagine a world.

00:02.500 --> 00:10.000
My best guess is that a guy will progress much more slowly than I have it progressing in my story.

00:10.000 --> 00:17.500
And my best guess is that we do survive because I progress is much more slowly from my perspective.

00:17.500 --> 00:26.500
It's extremely contrived for a GI to develop even as fast as it does in this story and be handled well enough cautiously enough.

00:26.500 --> 00:31.500
Thoughtfully enough that we have more than a fraction of a percent chance of survival.

00:36.500 --> 00:41.500
Welcome to imagine a world, a mini series from the Future of Life Institute.

00:41.500 --> 00:49.500
This podcast is based on a contest we ran to gather ideas from around the world about what a more positive future might look like in 2045.

00:49.500 --> 00:55.500
We hope the diverse ideas you're about to hear will spark discussions and maybe even collaborations.

00:55.500 --> 01:01.000
But you should know that the ideas in this podcast are not to be taken as FLI endorsed positions.

01:01.000 --> 01:04.000
And now, over to our host, Guillaume Reason.

01:15.500 --> 01:19.000
Welcome to the imagine a world podcast by the Future of Life Institute.

01:19.000 --> 01:21.000
I'm your host, Guillaume Reason.

01:21.000 --> 01:28.000
In this episode, we'll be exploring a world called Hall of Mirrors, which was a third place winner of FLI's World Building Contest.

01:28.000 --> 01:32.000
Hall of Mirrors is a deeply unstable world where nothing is as it seems.

01:32.000 --> 01:38.000
The structures of power we know today have eroded away, survived only by shells of expectation and appearance.

01:38.000 --> 01:43.000
People are isolated by perceptual bubbles and struggle to agree on what's real.

01:43.000 --> 01:47.000
Despite all this, things are generally going okay, for now.

01:47.000 --> 01:52.000
This is partly due to this world's particularly slow and modest development of AI technologies.

01:52.000 --> 01:57.000
AI tools here are still dominated by extensions of today's fundamentally narrow systems,

01:57.000 --> 02:01.000
with the one true AGI being developed under heavy quarantine.

02:01.000 --> 02:08.000
There are a number of reasons for this slow progress, including high computational costs and poor funding due to politicization.

02:08.000 --> 02:12.000
This team put a lot of effort into creating a plausible, empirically grounded world,

02:12.000 --> 02:15.000
but their work is also notable for its irreverence and dark humor.

02:15.000 --> 02:20.000
I can safely say that it's the only winning world where you could see virtual celebrity Tupac List

02:20.000 --> 02:24.000
perform at a luxury war-themed amusement park run by the Taliban.

02:24.000 --> 02:26.000
Needless to say, there's a lot going on here.

02:26.000 --> 02:31.000
I was excited to get a look into the minds behind this particularly brimming and erratic world.

02:31.000 --> 02:37.000
Our guest today is Michael Vasser, one member of the three-person team who created Hall of Mirrors.

02:37.000 --> 02:43.000
Michael is a futurist, activist, and entrepreneur with an eclectic background in biochemistry, economics, and business.

02:43.000 --> 02:49.000
He served as president of the Machine Intelligence Research Institute and is co-founder of Metamed Research.

02:49.000 --> 02:56.000
His other team members were Matia Franklin, a doctoral student studying AI ethics and alignment at University College London,

02:56.000 --> 03:00.000
and Bryce Heidi Smith, who has worn many hats from fortune-telling to modeling,

03:00.000 --> 03:03.000
and now has a focus on finance and policy research.

03:03.000 --> 03:05.000
Hey Michael, great to have you with us.

03:05.000 --> 03:08.000
Great. Good to speak to you.

03:08.000 --> 03:13.000
So I'm curious how the three of you on your team came to work on this project together.

03:13.000 --> 03:19.000
So I've known Bryce for a very long time, and when the project was starting up,

03:19.000 --> 03:24.000
there was a call for collaborations, and I tried talking to a bunch of people.

03:24.000 --> 03:29.000
And Matia and I had, you know, the most productive conversations.

03:29.000 --> 03:36.000
But like the overall project was mostly my vision, and Matia did some level of editing,

03:36.000 --> 03:40.000
and Bryce did the fiction and art.

03:40.000 --> 03:43.000
Cool. So did Bryce make the music that was accompanying your session?

03:43.000 --> 03:44.000
Yes.

03:44.000 --> 03:47.000
Cool. Yeah, I really enjoyed your music and the short stories as well.

03:47.000 --> 03:48.000
He did a great job with those.

03:48.000 --> 03:52.000
I mean, it's the closest thing to a super intelligence that we have around for now.

03:52.000 --> 03:54.000
Endurable.

03:54.000 --> 03:57.000
Well, what was it like for you guys to do this project together?

03:57.000 --> 04:00.000
Did you learn anything yourself in the course of it?

04:00.000 --> 04:02.000
I mean, I had a lot of fun.

04:02.000 --> 04:05.000
It helped me to concretize some of my thinking.

04:05.000 --> 04:10.000
Some, I feel like the basic sense of where I think we're going or would like to go

04:10.000 --> 04:16.000
has been reasonably stable in my head since GPT-3 came out,

04:16.000 --> 04:22.000
and hasn't drastically changed since GPT-2 and COVID.

04:22.000 --> 04:24.000
Yeah.

04:24.000 --> 04:28.000
What were some of your biggest sources of inspiration when you were working on this together?

04:29.000 --> 04:35.000
I don't think my thinking on this is significantly influenced by stories or books

04:35.000 --> 04:37.000
or music or what have you.

04:37.000 --> 04:43.000
I think it's basically just coming from looking at what the technology can do

04:43.000 --> 04:48.000
and spending the last 25, 30 years obsessively thinking about history and the economy

04:48.000 --> 04:54.000
and social sciences and making some effort to understand the technology.

04:54.000 --> 04:57.000
I'm certainly not a top expert in actually understanding the technology

04:57.000 --> 05:04.000
while I will humbly claim to be a top expert in understanding the history of technology

05:04.000 --> 05:06.000
as it relates to economics.

05:06.000 --> 05:09.000
Yeah. Well, you do have this deep professional background.

05:09.000 --> 05:13.000
Can you say a little bit about how your experience in other fields

05:13.000 --> 05:17.000
and kind of working through all this has influenced how you see the future?

05:17.000 --> 05:21.000
I mean, in terms of professional background,

05:21.000 --> 05:29.000
molecular bio, I studied in university and it doesn't really inform this very much.

05:29.000 --> 05:34.000
I have a lot of thoughts about cool things that could be done with molecular bio,

05:34.000 --> 05:40.000
and now that GPT-4 is performing at a high school national championship level

05:40.000 --> 05:44.000
without major upheaval enhancements.

05:44.000 --> 05:47.000
I'm confident that I can do a lot more of that stuff,

05:47.000 --> 05:50.000
and also AlphaFold is very cool, and mRNA tech is very cool.

05:50.000 --> 05:53.000
So I think there's enormous opportunities now for bio.

05:53.000 --> 05:59.000
Getting an MBA gave me an opportunity to exist in the business office world for a while,

05:59.000 --> 06:06.000
and that certainly is necessary without having interacted with corporate hierarchies.

06:06.000 --> 06:09.000
One doesn't know what corporate hierarchies are like at all.

06:09.000 --> 06:12.000
There's very effective disinformation and propaganda about that.

06:13.000 --> 06:19.000
I think mostly I've just read a lot in directions that seemed like they could be helpful

06:19.000 --> 06:22.000
over maybe a 25, 30-year period.

06:22.000 --> 06:27.000
Yeah. What sorts of insight did Bryce and Machita bring to the project?

06:27.000 --> 06:33.000
So the actual stories were very cool, and the music was very cool.

06:33.000 --> 06:37.000
And Bryce wrote those mostly by himself.

06:37.000 --> 06:44.000
And there were some back and forth about what sorts of things were maybe too over-the-top

06:44.000 --> 06:48.000
or too fun and silly to include in the story.

06:48.000 --> 06:53.000
And it's just good to talk to people about things and develop the ideas together.

06:53.000 --> 06:58.000
And certainly Bryce has been enormously central to developing my understanding of the world

06:58.000 --> 07:00.000
in general over the last decade.

07:00.000 --> 07:02.000
And what about Machita?

07:02.000 --> 07:06.000
Machita? I mean, mostly just discussing what I can get away with.

07:06.000 --> 07:12.000
In terms of when telling a story, what is too weird, what is socially acceptable enough

07:12.000 --> 07:19.000
that people can understand it as relatively limited in French distance from normal, thoughtful people?

07:19.000 --> 07:20.000
Yeah.

07:28.000 --> 07:31.000
In some ways, this world is kind of a caricature of the present.

07:31.000 --> 07:34.000
We see deeper isolation and polarization caused by media

07:34.000 --> 07:38.000
and a proliferation of powerful but ultimately limited AI tools

07:38.000 --> 07:41.000
that further erode our sense of objective reality.

07:41.000 --> 07:44.000
A deep instability threatens.

07:44.000 --> 07:48.000
And yet on a human level, things seem relatively calm.

07:48.000 --> 07:52.000
It turns out that the stories we tell ourselves about the world have a lot of inertia,

07:52.000 --> 07:54.000
and so do the ways we live our lives.

07:54.000 --> 07:58.000
I had a hard time picturing those individual lives among all the wild happenings of this world,

07:58.000 --> 08:01.000
and I wanted to hear more about that human perspective from Michael.

08:03.000 --> 08:05.000
What's it like to live in this world you've made?

08:05.000 --> 08:09.000
Well, it's going to be very different in different media bubbles.

08:09.000 --> 08:13.000
The biggest media bubble by far is going to be Chinese,

08:13.000 --> 08:20.000
and the successor to contemporary Chinese Communist Party politics

08:20.000 --> 08:25.000
will mean something more neo-confusion than China has been recently.

08:25.000 --> 08:31.000
But done with capacities that no one's ever had the opportunity to bring to the table,

08:31.000 --> 08:37.000
so you can just spend so much more time on filial piety and cultivating then

08:37.000 --> 08:40.000
when all of the real work has been automated

08:40.000 --> 08:45.000
and when you have machines that are in some ways superhuman watching your every move

08:45.000 --> 08:49.000
and helping you along to express gratitude to your parents

08:49.000 --> 08:51.000
in the most ritually prescribed manner.

08:51.000 --> 08:54.000
Other people have different experiences.

08:54.000 --> 09:00.000
There are probably hundreds of millions of people trapped in pornographic universes

09:00.000 --> 09:03.000
and effectively mind controlled by AI.

09:03.000 --> 09:07.000
That would maybe be the second largest demographic, if I really think about it.

09:07.000 --> 09:11.000
And there are lots and lots of people, like the ones we discussed at the end,

09:11.000 --> 09:18.000
living in old age homes and having their experiences mediated through a somewhat more tasteful

09:19.000 --> 09:25.000
but still like relatively liberal and relatively cultivated sense of benevolence.

09:25.000 --> 09:30.000
But the prospect of AGI coming online at all changes that.

09:30.000 --> 09:37.000
In some sense, these stories were intended to point at the extreme instability of the world that I produced.

09:37.000 --> 09:42.000
So we have one story about producing a piece of transhuman music

09:42.000 --> 09:50.000
and one story about consuming it despite the cautions of the companies around AGI

09:50.000 --> 10:00.000
under the basically reasonable assumption that music was not existentially dangerous under normal circumstances.

10:00.000 --> 10:05.000
Yeah, so you're referring to, in your world, there's this system that DeepMind has called siren,

10:05.000 --> 10:07.000
which is I think kind of the only AGI in your world.

10:07.000 --> 10:09.000
It's under very tight wraps.

10:09.000 --> 10:12.000
Everyone's really carefully screened and there's follow-up monitoring,

10:12.000 --> 10:15.000
even if they just hear the music that it produces.

10:15.000 --> 10:20.000
This system has also written some books on a few topics that have been carefully curated.

10:20.000 --> 10:24.000
I'm curious what broader impacts siren's existence has on your world,

10:24.000 --> 10:26.000
given kind of how cloistered it is.

10:28.000 --> 10:30.000
I mean, none by design.

10:30.000 --> 10:34.000
Allowing it to have more than the tiniest amount of impact on the world

10:34.000 --> 10:36.000
would be allowing the world to end almost immediately.

10:36.000 --> 10:39.000
So instead, yeah, your world really dives into narrow AIs.

10:39.000 --> 10:42.000
So these are systems that are very good at just a few specific tasks,

10:42.000 --> 10:44.000
like playing chess or driving a car.

10:44.000 --> 10:48.000
No, much broader than that, like the AIs we have today, like GPT,

10:48.000 --> 10:55.000
which are at least pretty good at most things that we tend to think of as intellectual tasks

10:55.000 --> 11:01.000
and very, very, very good at most things that we tend to think of as perceptual

11:01.000 --> 11:07.000
or as extremely rehearsed short-term actions without a lot of context sensitivity.

11:07.000 --> 11:08.000
I see.

11:08.000 --> 11:11.000
So these are like, you know, souped up, narrow AI systems.

11:11.000 --> 11:16.000
They're still not AGI's, but they're kind of the most effective extension

11:16.000 --> 11:19.000
of today's technologies like chat GPT and things like that, as you're saying.

11:19.000 --> 11:24.000
They're general enough that for the vast majority of the world's population,

11:24.000 --> 11:29.000
they probably are vaguely thought of as generally intelligent,

11:29.000 --> 11:34.000
like the vast majority of the world's people probably don't understand very well

11:34.000 --> 11:37.000
the differences between them and AGI's.

11:37.000 --> 11:42.000
And that's probably part of why there's essentially no funding or work on AGI outside of DeepMind.

11:42.000 --> 11:43.000
Yeah, interesting.

11:43.000 --> 11:49.000
And like broadly speaking, they're sufficient to produce some level of relatively benign,

11:49.000 --> 11:54.000
not totally impenetrable, but close enough, a global mind control system

11:54.000 --> 12:01.000
that also contributes to not understanding the differences and also not pursuing AGI.

12:01.000 --> 12:07.000
In some ways, I think that the fiction that my world most reminds me of

12:07.000 --> 12:11.000
is probably Who Framed Roger Rabbit, where they have these tunes everywhere.

12:11.000 --> 12:13.000
And the tunes can talk.

12:13.000 --> 12:15.000
They have personalities.

12:15.000 --> 12:20.000
They have something kind of like agency, but they don't seem to, for the most part,

12:20.000 --> 12:23.000
have agency with any scale.

12:23.000 --> 12:28.000
It's like an extremely rare, extremely dangerous thing for a tune like Judge Doom

12:28.000 --> 12:31.000
to have agency with scale and scope.

12:31.000 --> 12:38.000
And when they do, like Judge Doom, it's agency with an extremely inhuman focus, scale and scope.

12:38.000 --> 12:40.000
So very potentially dangerous.

12:40.000 --> 12:46.000
And the tunes are in some sense extremely cheap and disposable, easy to produce,

12:46.000 --> 12:48.000
but in some sense immortal.

12:48.000 --> 12:53.000
And the humans are like completely clueless about the glaring ways

12:53.000 --> 12:57.000
in which the tunes' capabilities are less than human,

12:57.000 --> 13:01.000
such as Roger Rabbit can only do things when it's funny,

13:01.000 --> 13:05.000
but fairly clueless about the ways in which their abilities are more than human,

13:05.000 --> 13:08.000
like they can survive having a piano dropped on their head.

13:08.000 --> 13:12.000
I actually haven't seen that movie, but I'm really excited now to watch it with this metaphor in mind.

13:12.000 --> 13:14.000
It's a really cool connection.

13:14.000 --> 13:15.000
Yeah.

13:15.000 --> 13:18.000
So one thing you say that these systems can do in your world

13:18.000 --> 13:22.000
is basically replace all white collar workers in theory,

13:22.000 --> 13:25.000
but you say this doesn't happen and you say basically, you know,

13:25.000 --> 13:30.000
there are various reasons, political and personal, why humans are still employed.

13:30.000 --> 13:36.000
I'm curious what kinds of work humans do and what it's like for these human workers in this situation.

13:36.000 --> 13:41.000
So I think basically it depends on their organization,

13:41.000 --> 13:44.000
but in the pretty large majority of organizations,

13:44.000 --> 13:52.000
it's pure office politics and getting therapy from not peak human ability,

13:52.000 --> 13:58.000
but good enough AI therapists to recover enough from the office politics

13:58.000 --> 14:03.000
that they only kill themselves with like drug overdoses and the like

14:03.000 --> 14:08.000
at maybe a third or a fourth the rate that they do in our world.

14:08.000 --> 14:14.000
And maybe even less if AI enhanced medicine makes such drug significantly less deadly

14:14.000 --> 14:17.000
and treatment significantly more effective.

14:17.000 --> 14:18.000
Yeah.

14:18.000 --> 14:20.000
Your world still has a ton of economic inequality,

14:20.000 --> 14:24.000
but the actual quality of life that you describe is kind of universally pretty good.

14:24.000 --> 14:27.000
Like travel has become really cheap and there's basically free energy.

14:27.000 --> 14:31.000
It makes food distribution really trivial as people can kind of live wherever they want

14:31.000 --> 14:34.000
and they have augmented reality, so it'll always look beautiful.

14:34.000 --> 14:37.000
I'm curious, given all of these kind of unifying factors,

14:37.000 --> 14:39.000
how people decide where to build their lives

14:39.000 --> 14:42.000
and what kinds of goals they decide to pursue with them.

14:42.000 --> 14:47.000
So the world that I'm thinking of, for the large majority of people,

14:47.000 --> 14:50.000
they start exploring the world when they're children

14:50.000 --> 14:53.000
and hopefully their parents take a lot of interest in them.

14:53.000 --> 14:56.000
But if not, there's an infinite amount of attention freely available

14:56.000 --> 15:00.000
from the web and from open source and commercial products.

15:00.000 --> 15:05.000
And the decisions they make throughout their lives are almost entirely determined

15:05.000 --> 15:11.000
by what sorts of commercial or open source products find them first, in a sense,

15:11.000 --> 15:18.000
and build the sort of feedback loops that pull them into one or another bubble reality.

15:18.000 --> 15:20.000
You have this interesting thread in your world

15:20.000 --> 15:23.000
where families kind of become a currency

15:23.000 --> 15:27.000
or a kind of wealth that people pursue more than monetary assets.

15:27.000 --> 15:30.000
Can you say a little bit about what that looks like?

15:30.000 --> 15:32.000
I mean, that's just being a normal person.

15:32.000 --> 15:35.000
We've lost touch with it in, you know, late stage capitalism.

15:35.000 --> 15:40.000
But even under normal capitalism, this was not confusing to anybody.

15:40.000 --> 15:45.000
You know, the idea of trying to accumulate wealth

15:45.000 --> 15:48.000
rather than trying to accumulate happy help,

15:48.000 --> 15:52.000
the wise flourishing and mutually cooperative descendants

15:52.000 --> 15:57.000
is like a really surprising thing to find an organism doing.

15:57.000 --> 16:02.000
So thinking about some of the more unusual aspects of your world,

16:02.000 --> 16:06.000
your world definitely had some of the wildest kind of one-off ideas in it

16:06.000 --> 16:07.000
that we saw in the competition.

16:07.000 --> 16:10.000
You have like the Taliban creates luxury war themed amusement parks.

16:10.000 --> 16:13.000
You have elephants that are domesticated by CRISPR.

16:13.000 --> 16:17.000
And you even have Kanye West creating a virtual reproduction of biblical Jerusalem.

16:17.000 --> 16:21.000
I'm curious like what prompted these kinds of details to be included

16:21.000 --> 16:24.000
and whether they're part of a larger theme for you that you were trying to convey.

16:24.000 --> 16:29.000
So the biggest thing that I left out of the actual thing that Matija's influence

16:29.000 --> 16:38.000
was a coup by the comedy party where basically in the 2032 election

16:38.000 --> 16:42.000
between AOC and Donald Trump,

16:42.000 --> 16:47.000
the mainstream Democrats, which still basically control the media and the courts,

16:47.000 --> 16:52.000
decide to allow a completely flagrant election fraud to control John Stuart

16:52.000 --> 16:55.000
as a third party president.

16:55.000 --> 17:02.000
And, you know, so that one I think Matija thought was too political, too controversial.

17:02.000 --> 17:05.000
But I do think it's the sort of thing that could realistically happen.

17:05.000 --> 17:08.000
Overall, where are these coming from?

17:08.000 --> 17:12.000
I mean, some of them are just like extreme low hanging fruit,

17:12.000 --> 17:17.000
things that a few college kids could throw together as a project

17:17.000 --> 17:24.000
in a world with AI capabilities that I realistically expect to exist well before the 2045 deadline.

17:24.000 --> 17:27.000
Yeah. So this is kind of just speaking to maybe like the chaos

17:27.000 --> 17:30.000
and the power flying around the instability of things

17:30.000 --> 17:33.000
and how the world is just going to get so much stranger.

17:33.000 --> 17:35.000
Yeah, I don't think of it as a chaotic world.

17:35.000 --> 17:40.000
The stories are super, super non-chaotic about people living very calm lives.

17:40.000 --> 17:47.000
I see it as a world that's very, very unstable simply because it has even one AGI in it.

17:47.000 --> 17:53.000
And like sooner or later, a more permanent solution is necessary

17:53.000 --> 17:59.000
than just keeping its interest cyber focused and keeping people from noticing it very much.

17:59.000 --> 18:02.000
To some degree, I'm just trying to show a picture,

18:02.000 --> 18:04.000
because that's all you can do in a story like this,

18:04.000 --> 18:08.000
but a picture where all of the pieces are scientifically well founded,

18:08.000 --> 18:11.000
technologically, economically and politically well founded,

18:11.000 --> 18:14.000
make sense and fit together fairly well.

18:14.000 --> 18:18.000
I guess more than anything else, I'm trying to show people like the contest is trying to show people

18:18.000 --> 18:22.000
that it is even possible to make a sincere, serious and competent effort

18:22.000 --> 18:26.000
to depict a realistic but optimistic future.

18:26.000 --> 18:38.000
Major changes are hacking away at the foundations of this world's systems.

18:38.000 --> 18:42.000
The loss of shared reality and weakening of governmental structures, at least in the West,

18:42.000 --> 18:45.000
seemed to strip humanity of a good deal of agency.

18:45.000 --> 18:51.000
It's implied that we're being kept from destruction only by our tenuous control of this world's one true AGI.

18:51.000 --> 18:55.000
At the same time, new approaches to things like education and social conflict

18:55.000 --> 18:59.000
signal hope for building a more coherent and empowered humanity.

18:59.000 --> 19:06.000
I wanted to hear more about how Michael saw this world approaching the changes and challenges that it faced.

19:06.000 --> 19:10.000
You write that in America, like Microsoft, Amazon, Tesla and Walmart

19:10.000 --> 19:14.000
are basically the only entities capable of large scale coordinated action anymore

19:14.000 --> 19:18.000
and elected government officials really just enact change by influencing their supporters

19:18.000 --> 19:20.000
rather than by pursuing any kind of legislation.

19:20.000 --> 19:22.000
Most decisions are made locally.

19:22.000 --> 19:25.000
Can you say a little bit more about how America's governmental systems

19:25.000 --> 19:27.000
lose so much influence in your world?

19:27.000 --> 19:31.000
I just see that as a continuation of the trend that we're already on.

19:31.000 --> 19:36.000
When you look at COVID, the government took an unbelievably huge amount of oppressive

19:36.000 --> 19:41.000
and authoritarian action that there probably won't be social or political support for

19:41.000 --> 19:44.000
if there's another major event that calls for it.

19:44.000 --> 19:47.000
It lost an enormous amount of public trust.

19:48.000 --> 19:53.000
If you look at what the government did that was effective with COVID,

19:53.000 --> 19:56.000
it basically boils down to printing enormous amounts of money

19:56.000 --> 20:03.000
and providing certain types of encouragement to conform to a certain standard.

20:03.000 --> 20:06.000
It's not that the government no longer matters.

20:06.000 --> 20:10.000
It's just that popularity contests should be.

20:10.000 --> 20:14.000
It's primarily a source of information about how to be popular.

20:14.000 --> 20:17.000
Just like in our world, people mostly want to be popular.

20:17.000 --> 20:21.000
They don't want it as much as in our world because they can always be popular with AIs.

20:21.000 --> 20:28.000
But still, AIs are not fully satisfying as mental and social companions.

20:28.000 --> 20:34.000
As this power switches over and flows towards tech companies gaining influence,

20:34.000 --> 20:36.000
it becomes increasingly hard to track wealth.

20:36.000 --> 20:41.000
In some ways, it also seems like things are just going on sheer inertia.

20:41.000 --> 20:44.000
You have this great line in your submission that says,

20:44.000 --> 20:47.000
a supermajority of the population has negative net worth

20:47.000 --> 20:50.000
and continues to be allocated credit as a matter of economic policy.

20:50.000 --> 20:53.000
You mentioned this instability of the world.

20:53.000 --> 20:55.000
How long do you see it remaining stable?

20:55.000 --> 21:00.000
Will these systems fall apart shortly after 2045 and you're imagining?

21:00.000 --> 21:05.000
The way I'm imagining this, this is a fairly close to best case scenario.

21:05.000 --> 21:12.000
My realistic best guess scenario would be that it's more than 70% likely,

21:12.000 --> 21:14.000
maybe more than 80% likely,

21:14.000 --> 21:20.000
that the system that I'm describing falls apart well before it gets to the point that I'm describing.

21:20.000 --> 21:23.000
These are supposed to be optimistic visions for the future.

21:23.000 --> 21:28.000
But once it gets to the point that I'm describing, if it gets to that point,

21:28.000 --> 21:32.000
I actually imagine it being stable for a pretty long time.

21:32.000 --> 21:34.000
Except for the edge, I think.

21:34.000 --> 21:37.000
Yeah, Siren gets up.

21:37.000 --> 21:42.000
One big tension in your world as a result of this increasing difficulty

21:42.000 --> 21:48.000
and verifying information is just people have a hard time agreeing on objective reality.

21:48.000 --> 21:51.000
They're really good in experimental healthcare interventions,

21:51.000 --> 21:55.000
but it's mostly about luck and maybe some skill to pick the winners out of that crowd.

21:55.000 --> 22:00.000
You have cryptocurrency that's made it really impossible to tell how much money anyone has.

22:00.000 --> 22:03.000
You mentioned that instead of Forbes keeping track of wealth,

22:03.000 --> 22:07.000
now kidnapping rings keep some of the best records of people's total assets.

22:07.000 --> 22:13.000
You even say that startups are buying these records off of those kidnapping rings to find wealthy funders.

22:13.000 --> 22:19.000
Can you say a little bit more about what leads to this deep fracturing of shared objectivity?

22:19.000 --> 22:24.000
That's been going on really in a big way since the 1940s.

22:24.000 --> 22:29.000
Once again, I'm just imagining it continuing and accelerating with more powerful technologies.

22:30.000 --> 22:36.000
The collapse of the dollar, which happens in the 2030s more or less in my story,

22:36.000 --> 22:38.000
contributes a fair amount.

22:38.000 --> 22:41.000
It makes the crypto thing much more substantial.

22:41.000 --> 22:47.000
And the increase like basically social welfare through senior age

22:47.000 --> 22:51.000
and the expansion of senior age through the population

22:51.000 --> 22:55.000
helps to stabilize things a lot at the expense of coherence and efficiency,

22:55.000 --> 22:57.000
which isn't really necessary anymore.

22:57.000 --> 22:59.000
Could you say what senior age is?

22:59.000 --> 23:04.000
Senior age is printing money through bank activities.

23:04.000 --> 23:08.000
When banks borrow money, then they lend out much more money.

23:08.000 --> 23:12.000
And there's a stack of different interest rates paid by different creditors.

23:12.000 --> 23:17.000
One of the basic challenges of running a capitalist society

23:17.000 --> 23:20.000
that's been well understood since long before Adam Smith

23:20.000 --> 23:25.000
is the extreme difficulty of causing control of the money printing apparatus

23:25.000 --> 23:30.000
to not be the convergent agenda of practically everyone.

23:30.000 --> 23:35.000
And most capitalist societies do collapse as control of the money printing apparatus

23:35.000 --> 23:37.000
becomes a convergent agenda.

23:37.000 --> 23:44.000
So I'm basically imagining the essential worker system that we discovered we had during COVID

23:44.000 --> 23:49.000
and the relatively resilient management of a small number of companies

23:49.000 --> 23:52.000
basically keeping the material reality held together.

23:52.000 --> 23:57.000
Despite the fact that the vast majority of supposed economic activity

23:57.000 --> 24:01.000
is actually pure political wealth redistribution

24:01.000 --> 24:07.000
to the people who bother to fight for wealth being distributed to them in a world

24:07.000 --> 24:11.000
where most people have basically lost track of wealth anyway.

24:11.000 --> 24:17.000
I'm curious why AI systems don't help more with these issues of shared goals and shared knowledge.

24:17.000 --> 24:20.000
You mentioned that AI systems can provide common knowledge,

24:20.000 --> 24:24.000
like they help groups of people identify if their behaviors are aligned with their goals

24:24.000 --> 24:26.000
or how to change their behaviors.

24:26.000 --> 24:29.000
You would think that that might cut through the haze

24:29.000 --> 24:34.000
and help people agree on things more or have more transparency.

24:34.000 --> 24:39.000
AI systems help enormously with establishing whatever set of goals

24:39.000 --> 24:45.000
is reasonably psychologically plausible and that the systems designers want to establish.

24:45.000 --> 24:50.000
But mostly that consists of consuming products just like it does in our world.

24:50.000 --> 24:55.000
And in the rare cases of societies that have more of a shared set of values

24:55.000 --> 24:58.000
and more of a shared power structure like China,

24:58.000 --> 25:03.000
it means that they have incredibly high integration and unity

25:03.000 --> 25:08.000
targeting shared goals that more or less consist of normal reasonable things

25:08.000 --> 25:14.000
like extending life and ecological sustainability and stability in general.

25:15.000 --> 25:19.000
One other thread I really enjoyed in your world is how you talk about education changing.

25:19.000 --> 25:24.000
So people start to see traditional educational pedigrees as a form of inherited privilege.

25:24.000 --> 25:27.000
And educational histories actually become private information

25:27.000 --> 25:31.000
which can't be used in decisions like hiring, which is a really interesting concept.

25:31.000 --> 25:35.000
And this tips the scales in favor of online self-driven education.

25:35.000 --> 25:39.000
Schools basically go empty while kids live with their families and learn on their own.

25:39.000 --> 25:41.000
I'm curious what this looks like for those kids.

25:41.000 --> 25:43.000
What are they learning? What aren't they learning?

25:43.000 --> 25:45.000
And who's deciding?

25:45.000 --> 25:52.000
So I'm basically imagining that nominally the parents decide when the kids are younger

25:52.000 --> 25:55.000
and the kids decide when they're older.

25:55.000 --> 26:01.000
But in practice, reasonably agentic parents who are also tech savvy

26:01.000 --> 26:08.000
and have like reasonably coherent preferences about what to get

26:08.000 --> 26:14.000
will be able to direct their kids towards media bubbles and narratives

26:14.000 --> 26:20.000
that will be extremely stable and which won't change much unless something really weird happens.

26:20.000 --> 26:27.000
So I expect that almost everyone's learning speed is going to be like

26:27.000 --> 26:34.000
at least four or five times faster between more targeted instruction, objectively better instruction,

26:34.000 --> 26:39.000
maybe learning enhancement through drugs and mRNA tech.

26:39.000 --> 26:43.000
And much better trauma care is a major feature of my world.

26:43.000 --> 26:49.000
So just the elimination of mental blocks through MDMA therapies and their successors.

26:49.000 --> 26:56.000
I don't know if I really played up adequately the spread of a new way of doing civilization

26:56.000 --> 27:00.000
from the carceral system into the general population

27:00.000 --> 27:05.000
as like MDMA therapies get adopted for dispute resolution within prisons

27:05.000 --> 27:13.000
and reach a level of reliability and efficacy that's sufficient that basically everyone wants some.

27:21.000 --> 27:24.000
Despite some of the more madcap details of their world,

27:24.000 --> 27:28.000
this team expresses a strong commitment to realism and plausibility.

27:28.000 --> 27:33.000
Their portrayal of AI development was also perhaps the slowest and most restricted among our winners.

27:33.000 --> 27:37.000
While there isn't AGI around, most of the technological developments in this world

27:37.000 --> 27:43.000
are just extensions of today's narrow AI systems whose awesome capabilities are ultimately limited.

27:43.000 --> 27:46.000
I was curious to hear more about this team's creative influences

27:46.000 --> 27:51.000
and whether this slower pace of AI development was something they saw as merely likely

27:51.000 --> 27:56.000
or a necessary component of any safe path to an aspirational future.

27:57.000 --> 28:01.000
So I'd like to spend a little while discussing the narratives in your world

28:01.000 --> 28:05.000
and how they compare to other narratives that are going around in popular culture.

28:05.000 --> 28:10.000
Like one really big through line for me is this sort of emperor has no close attitude you have

28:10.000 --> 28:14.000
towards economics and politics where your world kind of just goes through the motions

28:14.000 --> 28:18.000
to keep things moving along but the systems themselves are no longer really doing much.

28:18.000 --> 28:24.000
I'm curious if there are other examples of this perspective that inspired you in other kinds of media?

28:25.000 --> 28:30.000
I mean, mostly I'm inspired by real life, not by media and narratives.

28:30.000 --> 28:35.000
I can't think of a piece of fiction that is as radical as real life

28:35.000 --> 28:39.000
in the degree to which it violates conventional assumptions.

28:39.000 --> 28:46.000
You know, it's basically impossible to do without being a top tier literary genius like Shakespeare.

28:46.000 --> 28:51.000
I mean, Hamlet's wonderful Doris Lessig's book, The Golden Notebook,

28:51.000 --> 28:54.000
is maybe the best depiction I've ever seen,

28:54.000 --> 29:00.000
but one would need to be a really, really good literary scholar to appreciate it, I think.

29:00.000 --> 29:01.000
Same with Hamlet.

29:01.000 --> 29:02.000
Interesting.

29:03.000 --> 29:06.000
Well, I'm curious if there are any examples,

29:06.000 --> 29:10.000
and this can come from philosophers as well as fiction,

29:10.000 --> 29:14.000
of economic or political systems that could actually maybe function in a world like yours,

29:14.000 --> 29:20.000
or do you think that the whole concept of having a system that's run in a sensible way is kind of moot?

29:20.000 --> 29:21.000
No.

29:21.000 --> 29:26.000
I mean, the Chinese system is sort of run in a sensible way in the world I'm describing.

29:26.000 --> 29:30.000
It's not run with perfect rigor and resolution.

29:30.000 --> 29:33.000
It wouldn't pass like Talmudic standards.

29:33.000 --> 29:38.000
But by the standards that we're used to from government,

29:38.000 --> 29:43.000
I'm imagining a China with a life expectancy of well over 100 years,

29:43.000 --> 29:47.000
and the ability to industrially produce in a clean way,

29:47.000 --> 29:51.000
and with very little labor, practically everything the entire world needs.

29:51.000 --> 29:56.000
The goals of maximizing filial piety and ren

29:56.000 --> 30:00.000
are just going to be what's inherited from their ancestors and traditions,

30:00.000 --> 30:04.000
and it may not seem like doing a thing to us,

30:04.000 --> 30:11.000
but most of what we do is arguably not really doing a thing.

30:12.000 --> 30:16.000
Do you think there are any actions or reforms we could do to Western systems

30:16.000 --> 30:20.000
that would make them more resilient to these changes as well?

30:20.000 --> 30:24.000
I mean, my simple answer is I already put them all into this story.

30:24.000 --> 30:29.000
That's why the world is still alive and has not collapsed already.

30:29.000 --> 30:36.000
I'm making a number of surprising good luck-happens assumptions,

30:36.000 --> 30:38.000
not extraordinary.

30:38.000 --> 30:43.000
I really try to keep avoid endorsing things that are not just quirky

30:43.000 --> 30:47.000
and that have probabilities of less than about 10%.

30:47.000 --> 30:51.000
I think it's important to note that our world would be way scarier

30:51.000 --> 30:56.000
to people from my vision of 2045 than their world would be from us.

30:56.000 --> 30:59.000
Their world would just be incredibly addictive,

30:59.000 --> 31:04.000
and we would very quickly find ourselves trapped in some relatively exploitative bubble.

31:04.000 --> 31:10.000
But even exploitative bubbles have reasons to try to keep people mentally healthy enough

31:11.000 --> 31:17.000
to keep on receiving government benefits within a thin veneer of contributing to the economy.

31:17.000 --> 31:22.000
I guess one way to think about it is the American dream is basically a colpage

31:22.000 --> 31:26.000
of the America prior to the Civil War,

31:26.000 --> 31:30.000
America between the Civil War and the New Deal,

31:30.000 --> 31:32.000
and America after the New Deal,

31:32.000 --> 31:34.000
which could be summarized as the colonist experience,

31:34.000 --> 31:38.000
the immigrant experience, and the GI experience.

31:38.000 --> 31:44.000
And none of these experiences at all resemble what Zoomers are coming into and experiencing.

31:44.000 --> 31:48.000
And so they are growing up in a world of such transparent lies

31:48.000 --> 31:52.000
that they're almost without exception total epistemic nihilists

31:52.000 --> 31:55.000
mistakenly disbelieve that anything was ever true

31:55.000 --> 31:59.000
rather than only disbelieving that anything that they've ever seen is true,

31:59.000 --> 32:02.000
which is actually the case.

32:02.000 --> 32:07.000
So one really unique thing about your world is this focus on the narrow AI systems

32:07.000 --> 32:10.000
and how high a ceiling you put on their abilities.

32:10.000 --> 32:13.000
You kind of have basically a suite of different narrow AI systems

32:13.000 --> 32:16.000
that together have the capabilities of an AGI in some ways,

32:16.000 --> 32:18.000
but they're spread across these separate modules.

32:18.000 --> 32:20.000
No, they don't have the capabilities of an AGI.

32:20.000 --> 32:23.000
They don't have anything even remotely close to the abilities of an AGI.

32:23.000 --> 32:25.000
Can you distinguish that?

32:25.000 --> 32:28.000
The story is just kind of hinting at the capabilities of an AGI

32:28.000 --> 32:33.000
with the sort of security around it and the sort of implied impact

32:33.000 --> 32:35.000
and like potential risk.

32:35.000 --> 32:38.000
I'm operating with the definition of AGI that's something like a system

32:38.000 --> 32:42.000
that's better than a human at any task that you can reasonably define.

32:42.000 --> 32:46.000
Is that different from what you say when you say that these narrow AI systems?

32:46.000 --> 32:49.000
No, better than any human at any task that you can reasonably define.

32:49.000 --> 32:55.000
I'm saying that the systems that I'm describing are not even remotely close to that.

32:55.000 --> 33:00.000
They're like superhuman at very narrow tasks.

33:00.000 --> 33:04.000
They're superhuman at a lot of very narrow tasks.

33:05.000 --> 33:09.000
But it doesn't even come close, fitting them all together

33:09.000 --> 33:11.000
to the full range of human capabilities.

33:11.000 --> 33:14.000
I see. So there's kind of a synergy here, you're saying.

33:14.000 --> 33:17.000
Right, and then they're like not superhuman,

33:17.000 --> 33:24.000
but like merely as good as the experts that top elites tend to point to

33:24.000 --> 33:31.000
at the vast majority of tasks that get measured and graded

33:31.000 --> 33:34.000
and systematized and standardized threat society.

33:34.000 --> 33:40.000
So the best doctors in my world are still humans who make very heavy use of AI tools,

33:40.000 --> 33:46.000
but the best purely AI doctors might only be as good as the doctors

33:46.000 --> 33:49.000
that like the president has in our world,

33:49.000 --> 33:53.000
but not nearly as good as the doctors that like a top doctor has in our world

33:53.000 --> 33:57.000
since the top doctors know who the actual best doctors are and to date them.

33:57.000 --> 34:02.000
So not only do these systems not exceed the best human experts individually

34:02.000 --> 34:06.000
at these narrower tasks, but you're also saying that there's something missing

34:06.000 --> 34:09.000
even if you have this collection of narrow systems

34:09.000 --> 34:11.000
that can each do something that a human can do.

34:11.000 --> 34:14.000
Just putting those together is not the same as having something

34:14.000 --> 34:17.000
that could do all of these flexibly, is that what you're saying?

34:17.000 --> 34:21.000
Definitely, but also there are things that none of them can do even a little bit.

34:21.000 --> 34:23.000
Like in the story that I'm talking about,

34:23.000 --> 34:28.000
Siren is the only AI in the world that could, if it wanted to,

34:28.000 --> 34:30.000
do important original mathematics.

34:30.000 --> 34:32.000
It's the only AI in the world that could, if it wanted to,

34:32.000 --> 34:36.000
make the tiniest contribution to theoretical world-plied physics.

34:36.000 --> 34:41.000
So in your world, you have this incredibly powerful AGI system that does exist,

34:41.000 --> 34:45.000
but it's under really, really strong protections under tight wraps.

34:45.000 --> 34:50.000
Do you think that this is necessary to have a optimistic future with AGI in it?

34:50.000 --> 34:54.000
Yeah, definitely, unless we can basically do, you know,

34:54.000 --> 35:00.000
thousands of years worth of philosophical progress in like 20 years.

35:00.000 --> 35:03.000
And we can't.

35:03.000 --> 35:07.000
Like maybe we can do thousands of years worth of philosophical progress this century

35:07.000 --> 35:13.000
because we will have both AI and other technologies for enhancing our mental capabilities

35:13.000 --> 35:15.000
if we choose to use them.

35:15.000 --> 35:18.000
But we can't do it in 20 years, it's just laughable.

35:18.000 --> 35:19.000
Yeah.

35:19.000 --> 35:23.000
So the limitations that are preventing AGI from developing faster in your world,

35:23.000 --> 35:26.000
some of them are intentional, like policy decisions.

35:26.000 --> 35:28.000
Some of them are just kind of practical ones,

35:28.000 --> 35:32.000
like bad funding, politicization, and the rarity of human expertise.

35:32.000 --> 35:37.000
Do you think these are actual likely causes of slowing development in the real world?

35:37.000 --> 35:43.000
Yeah, that's my, my best guess is that AGI will progress much more slowly

35:43.000 --> 35:46.000
than I have it progressing in my story.

35:46.000 --> 35:50.000
And my best guess is that we do survive

35:50.000 --> 35:53.000
because AGI progresses much more slowly.

35:53.000 --> 35:57.000
From my perspective, it's extremely contrived for AGI to be,

35:57.000 --> 36:00.000
to develop even as fast as it does in this story

36:00.000 --> 36:05.000
and be handled well enough, cautiously enough, thoughtfully enough

36:05.000 --> 36:10.000
that like we have more than a fraction of a percent chance of survival.

36:10.000 --> 36:12.000
Yeah.

36:12.000 --> 36:14.000
Have you seen other portrayals of the future

36:14.000 --> 36:17.000
where narrow AI plays as much of a role as in yours?

36:17.000 --> 36:21.000
I mean, I feel like there's a lot of portrayals of the future

36:21.000 --> 36:26.000
where narrow AI is taken for granted and plays a large role.

36:26.000 --> 36:29.000
Like in the Star Trek, the next generation,

36:29.000 --> 36:33.000
they have one AGI data, like in my world,

36:33.000 --> 36:37.000
and then they have like an unbelievably powerful narrow AI in the ship

36:37.000 --> 36:40.000
and in the holodeck and just all over the place.

36:40.000 --> 36:45.000
But everyone takes it for granted and it's used as a tool

36:45.000 --> 36:51.000
by a military organization with a relatively unified internal agenda

36:51.000 --> 36:57.000
of exploration and extremely prudish and narrow conceptions

36:57.000 --> 37:03.000
of what types of experiences and behaviors

37:03.000 --> 37:06.000
its members are supposed to engage in.

37:06.000 --> 37:12.000
I will say that the type of narrow AI that we have actually developed

37:12.000 --> 37:20.000
is like pretty broad compared to what I expected five years ago.

37:20.000 --> 37:25.000
It's like very much what we were visibly moving towards four years ago.

37:25.000 --> 37:29.000
But to some degree, when I was growing up,

37:29.000 --> 37:32.000
C3PO seemed like a silly fantasy,

37:32.000 --> 37:34.000
seemed silly that you could have a machine

37:34.000 --> 37:39.000
that was that close to human performance

37:39.000 --> 37:44.000
but like stuck for a long time at below human performance

37:44.000 --> 37:47.000
and in some ways pretty profoundly below

37:47.000 --> 37:50.000
and in some ways pretty profoundly superhuman

37:50.000 --> 37:53.000
but like just be stuck there for a long time.

37:53.000 --> 37:57.000
But it kind of looks from the technologies that open up in AI

37:57.000 --> 38:01.000
is generating that a minimum viable C3PO

38:01.000 --> 38:04.000
might actually happen and be around for a long time

38:04.000 --> 38:08.000
without really drastic improvement from that.

38:08.000 --> 38:12.000
How do you feel about the general portrayals of the future that are in fiction?

38:12.000 --> 38:17.000
Do you think they're over or under optimistic when they try to be optimistic?

38:17.000 --> 38:22.000
I just think optimism in fiction that is trying to be at all realistic

38:22.000 --> 38:29.000
is unfortunately much rarer than it should be.

38:29.000 --> 38:33.000
And like that's basically maybe largely

38:33.000 --> 38:38.000
because the most perceptive and insightful people

38:38.000 --> 38:43.000
who are also successful at becoming prominent

38:43.000 --> 38:47.000
and surrounding themselves with other prominent people

38:47.000 --> 38:52.000
are constantly confronted with lots of profoundly miserable,

38:52.000 --> 38:56.000
extremely zero sum other prominent people

38:56.000 --> 39:00.000
and have very little contact with the large majority of people

39:00.000 --> 39:06.000
who are just not as miserable as the people who like double audience

39:06.000 --> 39:08.000
are going to end up around.

39:08.000 --> 39:11.000
So you're kind of calling for more optimism in nonfiction as well?

39:11.000 --> 39:12.000
Is that where you're going?

39:12.000 --> 39:16.000
I mean, no, I feel like optimism and pessimism is like

39:16.000 --> 39:18.000
intrinsically unhealthy concepts.

39:18.000 --> 39:20.000
You should just try to have true beliefs

39:20.000 --> 39:24.000
but true beliefs should be balanced.

39:24.000 --> 39:30.000
There's a lot of social pressure to performatively be pessimistic

39:30.000 --> 39:33.000
because the elites tend to be pessimistic.

39:33.000 --> 39:35.000
And elites tend to be pessimistic

39:35.000 --> 39:38.000
because they're living in a hyper competitive zero sum world

39:38.000 --> 39:40.000
that most of us are not living in.

39:40.000 --> 39:44.000
And there's a lot of, it's easier in some ways to be pessimistic

39:44.000 --> 39:46.000
especially cheaply pessimistic.

39:46.000 --> 39:51.000
But there's also like just cognitive biases that lead to silly sorts of pessimism.

39:51.000 --> 39:54.000
So like imagine there was a news item

39:54.000 --> 39:58.000
about how it turns out that apples cause cancer.

39:58.000 --> 40:00.000
Practically everyone would see this as bad news.

40:00.000 --> 40:02.000
Oh no, I've been poisoning myself for years.

40:02.000 --> 40:04.000
But obviously it's good news.

40:04.000 --> 40:05.000
We know what the cancer rate is.

40:05.000 --> 40:08.000
Now we know that we can avoid it by not eating apples.

40:08.000 --> 40:10.000
Right, the devil you know.

40:10.000 --> 40:15.000
Like information is almost always desirable

40:15.000 --> 40:19.000
but information about bad things gets interpreted

40:19.000 --> 40:21.000
as something bad happening

40:21.000 --> 40:24.000
rather than being interpreted as something good happening.

40:24.000 --> 40:26.000
You know James Baldwin,

40:26.000 --> 40:30.000
not everything that can be confronted can be overcome

40:30.000 --> 40:34.000
but everything to be overcome must be confronted.

40:34.000 --> 40:38.000
And to some degree this picture that I'm depicting

40:38.000 --> 40:40.000
is an edge case on that

40:40.000 --> 40:45.000
because this is a world that has managed to partially overcome

40:46.000 --> 40:49.000
and fully survive a lot of the problems

40:49.000 --> 40:51.000
that our society is dying from

40:51.000 --> 40:53.000
without really confronting them.

40:53.000 --> 40:56.000
In your world a lot of the role models become virtual.

40:56.000 --> 40:59.000
So basically all celebrities popular with the under 30 crowd

40:59.000 --> 41:01.000
are virtual people.

41:01.000 --> 41:03.000
Some are recreations of historical figures.

41:03.000 --> 41:06.000
Others are kind of amalgams like Tupacalist

41:06.000 --> 41:09.000
and you have XXX, Tentacion, Albus XXX.

41:09.000 --> 41:12.000
I'm curious how these play a role

41:12.000 --> 41:15.000
as kind of role models in your world.

41:15.000 --> 41:17.000
I think they mostly don't.

41:17.000 --> 41:20.000
I think that people who have at least reasonably good taste

41:20.000 --> 41:24.000
do prefer interaction with individuals.

41:24.000 --> 41:28.000
Insofar as they mostly imitate behavior by real people

41:28.000 --> 41:32.000
and that like the social influences from machines

41:32.000 --> 41:36.000
are in general more of a, you know,

41:36.000 --> 41:40.000
goal directed relatively overt manipulation sort.

41:40.000 --> 41:43.000
What are your thoughts on some of the current cultural attitudes

41:43.000 --> 41:45.000
towards like AI generated art

41:45.000 --> 41:47.000
and virtual cultural figures right now?

41:47.000 --> 41:50.000
So it seems like any reasonably good artist

41:50.000 --> 41:52.000
wants to make art,

41:52.000 --> 41:54.000
not wants to get paid for making art

41:54.000 --> 41:56.000
and like maybe wants to be seen

41:56.000 --> 41:58.000
and people can worry that

41:58.000 --> 42:01.000
a lot of people will never be exposed to good art

42:01.000 --> 42:04.000
because they're going to be exposed to an enormous amount of stuff

42:04.000 --> 42:06.000
that doesn't have a message behind it

42:06.000 --> 42:10.000
and isn't created as an expression of pain

42:10.000 --> 42:12.000
and suppressed emotion.

42:12.000 --> 42:15.000
But like if you're a good artist

42:15.000 --> 42:16.000
you can learn new tools

42:16.000 --> 42:18.000
and you keep learning new tools throughout your life

42:18.000 --> 42:21.000
and you learn how to make use of these new tools

42:21.000 --> 42:24.000
to compete and to get your message out there.

42:24.000 --> 42:28.000
The barriers to entry for art in some sense never go down

42:28.000 --> 42:31.000
because they have to do with the attention

42:31.000 --> 42:33.000
and consciousness of your audience.

42:33.000 --> 42:37.000
The economic barriers to entry in our world

42:37.000 --> 42:40.000
are going up because of extreme economic scarcity

42:40.000 --> 42:43.000
that's being created through policy.

42:43.000 --> 42:45.000
So like we're living in a time of

42:45.000 --> 42:47.000
really extreme economic scarcity

42:47.000 --> 42:50.000
compared to the Great Depression at this point.

42:50.000 --> 42:53.000
We've lost way more actual economic freedom

42:53.000 --> 42:55.000
in the sense of like

42:55.000 --> 42:57.000
not needing to work very hard or very well

42:57.000 --> 43:00.000
or be very exceptional in order to afford

43:00.000 --> 43:03.000
to reproduce our own labor

43:03.000 --> 43:05.000
have children and grandchildren

43:05.000 --> 43:08.000
and have them live at least as nicely as we do

43:08.000 --> 43:10.000
and be able to purchase our baskets of goods.

43:10.000 --> 43:12.000
And every Zoomer knows it

43:12.000 --> 43:15.000
because like they in fact can't purchase

43:15.000 --> 43:18.000
the baskets of goods that their parents had

43:18.000 --> 43:21.000
and their parents deny them recognition as adults

43:21.000 --> 43:25.000
even like millennials with kids

43:25.000 --> 43:28.000
are denied recognition as adults in major ways

43:28.000 --> 43:31.000
because they don't have the baskets of consumption goods

43:31.000 --> 43:35.000
that in fact, generationally speaking, they can't have.

43:43.000 --> 43:45.000
The process of world building has great potential

43:45.000 --> 43:47.000
to make a positive future feel more attainable.

43:47.000 --> 43:49.000
This can be incredibly powerful

43:49.000 --> 43:52.000
whether you're a creative person looking to produce rich works of fiction

43:52.000 --> 43:54.000
or have a more technical focus

43:54.000 --> 43:56.000
and are looking to reach policymakers or the public.

43:56.000 --> 43:58.000
I asked Michael what kind of impacts

43:58.000 --> 44:00.000
he hoped his work would have on the world.

44:00.000 --> 44:02.000
What do you hope your world leaves people

44:02.000 --> 44:05.000
thinking about long after they've read through it?

44:05.000 --> 44:08.000
So the biggest thing is that I just would like people

44:08.000 --> 44:12.000
to try to make sense of what could happen.

44:12.000 --> 44:15.000
I would like them to know that it is possible

44:15.000 --> 44:19.000
to make a joint prediction

44:19.000 --> 44:23.000
and expression of preference that is in line with

44:24.000 --> 44:27.000
the relatively full range of scientific and technological

44:27.000 --> 44:29.000
and humanistic thinking

44:29.000 --> 44:31.000
and that holds together and makes sense

44:31.000 --> 44:35.000
and that much more close to comes true

44:35.000 --> 44:38.000
and also somewhat guides society towards it coming true

44:38.000 --> 44:41.000
than what we would normally think of as science fiction.

44:41.000 --> 44:44.000
What kinds of expertise would you be most interested in having

44:44.000 --> 44:48.000
people bring to discussions about your world or the future in general?

44:48.000 --> 44:52.000
I think that the things that I would want to bring in first

44:52.000 --> 44:56.000
would be somatic skills

44:56.000 --> 45:00.000
like body work and yoga

45:00.000 --> 45:03.000
and things like that

45:03.000 --> 45:08.000
experience with MDMA and other psychedelic therapies

45:08.000 --> 45:11.000
and maybe even electrical engineering

45:11.000 --> 45:13.000
for creating better alternatives

45:13.000 --> 45:15.000
to transcranial magnetic stimulation

45:15.000 --> 45:18.000
for disinhibiting some parts of the cortex

45:18.000 --> 45:21.000
and activating other parts of the cortex

45:21.000 --> 45:24.000
so to enable people to recapture

45:24.000 --> 45:26.000
usually after years of work

45:26.000 --> 45:29.000
the sorts of cognitive abilities that

45:29.000 --> 45:32.000
normal smart inquisitive kids had when I was a kid

45:32.000 --> 45:36.000
and which the entire literary imprint

45:36.000 --> 45:40.000
of Western civilization is the imprint of

45:40.000 --> 45:43.000
which is why we don't have a literary imprint

45:43.000 --> 45:45.000
for our contemporary civilization.

45:45.000 --> 45:48.000
Do you have any particular hopes about the impact

45:48.000 --> 45:52.000
this work would have on the younger generation of folks around today?

45:52.000 --> 45:56.000
It seems to me that the vast majority of zoomers

45:56.000 --> 46:00.000
are doomers. They believe that the world is going to hell in a handbasket

46:00.000 --> 46:02.000
and everything is falling apart.

46:02.000 --> 46:05.000
But they are likewise cynics about the past

46:05.000 --> 46:09.000
in that they somehow believe that things have been getting worse forever

46:09.000 --> 46:11.000
but were never better than they are today.

46:11.000 --> 46:15.000
And that that can't be a concrete set of beliefs.

46:15.000 --> 46:18.000
It actually has to be something that they have

46:18.000 --> 46:22.000
instead of having beliefs which is like a posture, a vibe.

46:22.000 --> 46:26.000
What's actually going on is that they have always been lied to

46:26.000 --> 46:30.000
about everything of importance by every credible authority

46:30.000 --> 46:34.000
so they don't believe that people can know things

46:34.000 --> 46:38.000
and they only believe that people can posture and vibe.

46:38.000 --> 46:42.000
That's really sad because manifestly the world around us

46:42.000 --> 46:46.000
displays incredible amounts of the results of knowledge

46:46.000 --> 46:50.000
and if people don't continue to produce the results of that knowledge

46:50.000 --> 46:52.000
we're going to live in a much less nice world.

46:52.000 --> 46:56.000
What could help to correct for this or influence their attitudes

46:56.000 --> 46:58.000
in a positive way for you?

46:58.000 --> 47:03.000
Well, to a really non-trivial degree

47:03.000 --> 47:07.000
large language models that we have today

47:07.000 --> 47:10.000
if they were reinforcement learning trained

47:10.000 --> 47:15.000
for questioning and challenging and calling out bullshit

47:15.000 --> 47:20.000
and especially for perceiving the emotional dynamics of social situations

47:20.000 --> 47:23.000
which are very easy to perceive for even average humans

47:23.000 --> 47:27.000
and wouldn't be that hard to train systems to perceive.

47:27.000 --> 47:33.000
Like people need social support in calling out bullshit

47:33.000 --> 47:38.000
rather than all of the social pressure being to submit to bullshit

47:38.000 --> 47:43.000
and go along with it and like we have the technology today

47:43.000 --> 47:48.000
to build artificial social support of precisely the type we need.

47:48.000 --> 47:52.000
What aspects of your world would you be most excited to see popular media

47:52.000 --> 47:55.000
take on when portraying the future?

47:55.000 --> 48:01.000
Well, to start having a picture of China as something like China

48:01.000 --> 48:05.000
rather than using China as our designated bad guy

48:05.000 --> 48:09.000
which we use to project the images of ourselves

48:09.000 --> 48:13.000
basically our popular media almost exclusively treats China

48:13.000 --> 48:17.000
as a scapegoat for the types of behavior

48:17.000 --> 48:20.000
that we are very aware that we engage in

48:20.000 --> 48:23.000
to approximately the same degree that they do

48:23.000 --> 48:27.000
and is almost always trying to display

48:27.000 --> 48:31.000
so bias for the sake of showing loyalty

48:31.000 --> 48:35.000
rather than trying to display scholarship and understanding.

48:35.000 --> 48:40.000
I think that having in general a somewhat more balanced view

48:40.000 --> 48:43.000
of all sorts of cultural things

48:43.000 --> 48:48.000
having more of an attitude that most things have some good and some bad in them.

48:48.000 --> 48:50.000
Well, thanks so much for joining us today, Michael.

48:50.000 --> 48:52.000
We've covered so much ground in this conversation

48:52.000 --> 48:55.000
and it's been really great to explore all these ideas with you.

48:55.000 --> 48:59.000
It's great having this conversation.

49:26.000 --> 49:29.000
You can read all the comments and appreciate every rating.

49:29.000 --> 49:32.000
This podcast is produced and edited by Worldview Studio

49:32.000 --> 49:34.000
and the Future of Life Institute.

49:34.000 --> 49:37.000
FLI is a non-profit that works to reduce large scale risks

49:37.000 --> 49:39.000
from transformative technologies

49:39.000 --> 49:41.000
and promote the development and use of these technologies

49:41.000 --> 49:43.000
to benefit all life on earth.

49:43.000 --> 49:45.000
We run educational outreach and grants programs

49:45.000 --> 49:47.000
and advocate for better policy making

49:47.000 --> 49:49.000
in the United Nations, US government,

49:49.000 --> 49:51.000
and European Union institutions.

49:51.000 --> 49:53.000
If you're a storyteller working on films

49:53.000 --> 49:55.000
and creative projects about the future,

49:55.000 --> 49:57.000
we can also help you understand the science

49:57.000 --> 49:59.000
and storytelling potential of transformative technologies.

49:59.000 --> 50:01.000
If you'd like to get in touch with us

50:01.000 --> 50:03.000
or any of the teams featured on the podcast to collaborate,

50:03.000 --> 50:06.000
you can email worldbuildatfutureoflife.org.

50:06.000 --> 50:09.000
A reminder, this podcast explores the ideas

50:09.000 --> 50:12.000
created as part of FLI's Worldbuilding Contest,

50:12.000 --> 50:14.000
and our hope is that this series sparks discussion

50:14.000 --> 50:16.000
about the kinds of futures we all want.

50:16.000 --> 50:19.000
The ideas we discuss here are not to be taken as FLI positions.

50:19.000 --> 50:21.000
You can find more about our work

50:21.000 --> 50:24.000
at www.futureoflife.org

50:24.000 --> 50:27.000
or subscribe for a newsletter to get updates on all our projects.

50:27.000 --> 50:29.000
Thanks for listening to Imagine a World.

50:29.000 --> 50:32.000
Stay tuned to explore more positive futures.

