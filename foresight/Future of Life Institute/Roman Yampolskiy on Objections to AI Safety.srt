1
00:00:00,000 --> 00:00:03,640
Welcome to the Future of Life Institute podcast.

2
00:00:03,640 --> 00:00:06,760
I'm Gus Docher and I'm here with Roman Jampolsky.

3
00:00:06,760 --> 00:00:11,000
Roman is a computer scientist from the University of Louisville.

4
00:00:11,000 --> 00:00:12,840
Roman, welcome to the podcast.

5
00:00:12,840 --> 00:00:15,000
Thanks for inviting me. It's good to be back.

6
00:00:15,000 --> 00:00:18,800
I think it's my third time on a FLI podcast, if I'm not mistaken.

7
00:00:18,800 --> 00:00:24,800
Great. You have this survey paper of objections to AI safety research,

8
00:00:24,800 --> 00:00:26,640
and I find this very interesting.

9
00:00:26,640 --> 00:00:31,800
I feel like this is a good way to spend your time to collect all of these objections

10
00:00:31,800 --> 00:00:35,040
and see if they have any merit and consider them.

11
00:00:35,040 --> 00:00:38,080
And so I think we should dive into it.

12
00:00:38,080 --> 00:00:42,080
One objection you raise under the technical objections

13
00:00:42,080 --> 00:00:45,480
is that AI, in a sense, doesn't exist.

14
00:00:45,480 --> 00:00:48,480
If we call it something else, it sounds less scary.

15
00:00:48,480 --> 00:00:50,280
Perhaps you could unpack that a bit.

16
00:00:50,280 --> 00:00:56,240
So those are objections from people who think there is no AI risk or risk is not real.

17
00:00:56,240 --> 00:01:00,680
That's not my objections to technical work or safety work.

18
00:01:00,680 --> 00:01:06,800
We try to do a very comprehensive survey, so even silly ones are included.

19
00:01:06,800 --> 00:01:15,600
And people do try to explain that artificial intelligence is a scary sounding scientific term,

20
00:01:15,600 --> 00:01:20,280
but if you just call it matrix multiplication, then of course it's not scary at all.

21
00:01:20,280 --> 00:01:23,600
It's just statistics and we have nothing to worry about.

22
00:01:23,600 --> 00:01:27,040
So it seems they're trying to kind of shift the narrative

23
00:01:27,040 --> 00:01:32,960
by using this approach of getting away from agent hood

24
00:01:32,960 --> 00:01:36,960
and kind of built-in scenarios people have for AI,

25
00:01:36,960 --> 00:01:42,760
to something no one is scared of, calculators, addition, algebra.

26
00:01:42,760 --> 00:01:46,680
Perhaps there is a way to frame this objection where it makes a bit of sense

27
00:01:46,680 --> 00:01:51,360
in that people are quite sensitive to how you frame risks.

28
00:01:51,360 --> 00:01:54,680
People are quite sensitive to certain words in particular.

29
00:01:54,680 --> 00:02:00,240
So do you see that perhaps people who are in favor of AI safety research

30
00:02:00,240 --> 00:02:05,920
by calling AI something that sounds scary might be, in a sense, inflating the risks?

31
00:02:05,920 --> 00:02:09,320
Well, it's definitely a tool people use to manipulate any debate.

32
00:02:09,320 --> 00:02:12,840
I mean, whatever you're talking about abortion or anything else,

33
00:02:12,840 --> 00:02:15,600
it's like, are you killing babies or you're making a choice?

34
00:02:15,600 --> 00:02:19,480
Of course, language can be used to manipulate,

35
00:02:19,480 --> 00:02:23,960
but it helps to look for capability equivalences.

36
00:02:23,960 --> 00:02:29,560
Are we creating God-like machines or is it just a table in a database?

37
00:02:29,560 --> 00:02:32,560
So that would make a difference in how you perceive it.

38
00:02:32,560 --> 00:02:35,920
And perhaps we should just simply be willing to accept that, yes,

39
00:02:35,920 --> 00:02:42,440
what we are afraid of, in a sense, is matrix multiplication or data processing

40
00:02:42,440 --> 00:02:48,400
or whatever you want to call it, because these things might still have scary properties.

41
00:02:48,400 --> 00:02:49,640
Whatever we call them.

42
00:02:49,640 --> 00:02:55,800
Right, so we can argue that humans are just stakes, pieces of meat with electricity in them

43
00:02:55,800 --> 00:03:00,160
and it doesn't sound so bad until you realize we can create nuclear weapons.

44
00:03:00,160 --> 00:03:04,200
So it's all about perception and what you're hoping to accomplish.

45
00:03:04,200 --> 00:03:08,840
All right, there is also an objection going along the lines

46
00:03:08,840 --> 00:03:11,560
that superintelligence is impossible.

47
00:03:11,560 --> 00:03:14,360
What's the strongest form of this objection?

48
00:03:14,360 --> 00:03:19,680
So essentially, the argument goes that there are some upper limits on capability.

49
00:03:19,680 --> 00:03:22,200
Maybe they are based on laws of physics.

50
00:03:22,200 --> 00:03:26,200
You just cannot in this universe have anything greater than a human brain.

51
00:03:26,200 --> 00:03:31,960
Just for some reason, that's the ultimate endpoint and that's why evolution stopped there

52
00:03:31,960 --> 00:03:37,160
and somehow we magically ended up being at the very top of a food chain.

53
00:03:37,160 --> 00:03:42,640
There could be other arguments about, okay, maybe it's not absolute theoretical limit,

54
00:03:42,640 --> 00:03:46,520
but in practical terms, without quantum computers, we'll never get there.

55
00:03:46,520 --> 00:03:52,080
You can have many flavors of this, but the idea is that we're just never going to be outcompeted.

56
00:03:52,080 --> 00:03:55,360
And this doesn't strike me as particularly plausible.

57
00:03:55,360 --> 00:03:59,600
We could imagine humans simply with physically bigger brains.

58
00:03:59,600 --> 00:04:04,440
So the version where humans are at the absolute limit of intelligence doesn't sound plausible,

59
00:04:04,440 --> 00:04:10,800
but is there some story in which physics puts limits on intelligence?

60
00:04:10,800 --> 00:04:15,880
There could be a very, very high upper limit to which we are nowhere close,

61
00:04:15,880 --> 00:04:20,240
but if you think about the size of a possible brain, Jupiter-sized brains,

62
00:04:20,240 --> 00:04:25,320
at some point the density will collapse into some black hole singularity.

63
00:04:25,320 --> 00:04:28,440
But this is not something we need to worry about just yet,

64
00:04:28,440 --> 00:04:34,560
not smart enough superintelligence is where nowhere near the size of capability.

65
00:04:34,560 --> 00:04:39,080
And from our point of view, we won't be able to tell the difference system with,

66
00:04:39,160 --> 00:04:44,600
I mean, hypothetically IQ of a million versus IQ of a billion will look very similar to us.

67
00:04:44,600 --> 00:04:54,200
Yeah, and perhaps a related worry is that stories of self-improving AIs are wrong, in a sense.

68
00:04:54,200 --> 00:05:00,760
So it's definitely easy to make such claims because we don't have good examples of software doing it more than once.

69
00:05:00,760 --> 00:05:06,160
So you have compilers which go through code, optimize it, but they don't continuously self-optimize.

70
00:05:06,200 --> 00:05:12,160
But it's not impossible to see if you automate science and engineering,

71
00:05:12,160 --> 00:05:16,560
then scientists and engineers will look at their own code and continue this process.

72
00:05:16,560 --> 00:05:18,960
So it seems quite reasonable.

73
00:05:18,960 --> 00:05:22,080
There could be strong diminishing returns on that,

74
00:05:22,080 --> 00:05:26,440
but you have to consider other options for becoming smarter.

75
00:05:26,440 --> 00:05:28,400
It's not just improving the algorithm.

76
00:05:28,400 --> 00:05:31,960
You can have faster hardware, you can have more memory,

77
00:05:31,960 --> 00:05:34,800
you can have more processes running in parallel.

78
00:05:34,800 --> 00:05:38,600
There are different types of how you get to superintelligent performance.

79
00:05:38,600 --> 00:05:43,560
And you could, of course, have AI held along the way there with development of hardware

80
00:05:43,560 --> 00:05:48,880
or discovery of new hardware techniques as well as new algorithmic techniques and so on.

81
00:05:48,880 --> 00:05:50,360
That's exactly the point, right?

82
00:05:50,360 --> 00:05:55,840
So you'll get better at getting better and this process will accelerate until you can't keep up with it.

83
00:05:55,840 --> 00:06:03,800
How do you feel about tools such as Copilot, which is a tool that programmers can use for auto-completing their code?

84
00:06:03,800 --> 00:06:09,040
Is this a form of proto-self-improvement or would that be stretching the term?

85
00:06:09,040 --> 00:06:12,920
Well, eventually, when it's good enough to be an independent programmer, it would be good.

86
00:06:12,920 --> 00:06:17,240
But I'm very concerned with such systems because from what I understand,

87
00:06:17,240 --> 00:06:22,840
the bugs they would introduce would be very different from typical bugs human programmers will introduce.

88
00:06:22,840 --> 00:06:25,800
So debugging would be even harder from our point of view,

89
00:06:25,800 --> 00:06:33,600
monitoring it, making sure that there is not this inheritance of calls to a buggy first version.

90
00:06:33,600 --> 00:06:40,720
So yeah, long term, I don't think it's a very good thing for us that we no longer can keep up with the debugging process.

91
00:06:40,720 --> 00:06:44,240
Would you count it as a self-improving process?

92
00:06:44,240 --> 00:06:47,600
So I think for self-improvement, you need multiple iterations.

93
00:06:47,600 --> 00:06:51,960
If it does something once or even like a constant number of times, I would not go there.

94
00:06:51,960 --> 00:06:58,120
It's an optimization process, but it's not an ongoing, continuous, hyper-exponential process.

95
00:06:58,120 --> 00:07:00,040
So it's not as concerning yet.

96
00:07:00,040 --> 00:07:02,600
Then there's the question of consciousness.

97
00:07:02,600 --> 00:07:13,240
So one objection to AGI or to strong AI or whatever you want to call it is that AI won't be conscious and therefore it can't be human level.

98
00:07:13,240 --> 00:07:21,160
So for me, at least, there seems to be some confusion of concepts between intelligence and consciousness.

99
00:07:21,160 --> 00:07:24,560
I consider these to be separable.

100
00:07:24,560 --> 00:07:25,520
I agree completely.

101
00:07:25,520 --> 00:07:28,120
They have nothing in common, but people then they hear about it.

102
00:07:28,120 --> 00:07:30,160
They always say, oh, it's not going to be self-aware.

103
00:07:30,160 --> 00:07:31,360
It's not going to be conscious.

104
00:07:31,360 --> 00:07:35,440
They probably mean capable in terms of intelligence and optimization.

105
00:07:35,440 --> 00:07:40,360
But there is a separate property of having internal states and qualia.

106
00:07:40,360 --> 00:07:44,360
And you can make an argument that without it, you cannot form goals.

107
00:07:44,360 --> 00:07:46,920
You cannot want to accomplish things in the world.

108
00:07:46,920 --> 00:07:48,520
So it's something to address.

109
00:07:48,520 --> 00:07:53,760
And perhaps we should understand consciousness differently than the qualia interpretation.

110
00:07:53,760 --> 00:07:55,760
Could we be talking past each other?

111
00:07:55,760 --> 00:07:57,520
It's definitely possible.

112
00:07:57,520 --> 00:08:03,840
And even if we agreed, consciousness itself is not a well-defined, easy to measure scientific terms.

113
00:08:03,840 --> 00:08:12,520
So even if we said, yeah, it's all about qualia, we'd still have no idea if it actually has any or how would we define what amount of consciousness it has?

114
00:08:12,520 --> 00:08:15,160
Perhaps a bit related to the previous question.

115
00:08:15,160 --> 00:08:19,960
We have the objections, the objection that AIs will simply be tools for us.

116
00:08:19,960 --> 00:08:26,760
I think this sounds at least somewhat plausible to me since AIs today function as tools.

117
00:08:26,760 --> 00:08:35,920
And perhaps we can imagine a world in which they stay tools and these are programs that we call upon to solve specific tasks.

118
00:08:35,920 --> 00:08:42,440
But they are never agents that can accomplish something and have goals of their own and so on.

119
00:08:42,440 --> 00:08:47,960
So latest models were released as tools and immediately people said, hey, let's make a loop out of them.

120
00:08:47,960 --> 00:08:52,400
Give them ability to create their own goals and make them as agentic as possible within a week.

121
00:08:52,400 --> 00:08:55,000
So yeah, I think it's not going to last long.

122
00:08:55,080 --> 00:09:01,600
What is it that pushes AIs to become more like agents and less like tools?

123
00:09:01,600 --> 00:09:08,360
So a tool in my at least perception is something a human has to initiate interaction with.

124
00:09:08,360 --> 00:09:13,200
I ask it a question, it responds, I give it some input, it provides output.

125
00:09:13,200 --> 00:09:17,080
Whereas an agent doesn't wait for environment to prompt it.

126
00:09:17,080 --> 00:09:23,200
It's already working on some internal goal, generating new goals, plans.

127
00:09:23,760 --> 00:09:26,640
Even if I go away, it continues this process.

128
00:09:26,640 --> 00:09:34,280
One objection is that you can always simply turn off the AI if it goes out of hand and if you feel like you're not in control of it.

129
00:09:34,280 --> 00:09:43,520
And this is easier to imagine doing if you're dealing with something that's more like a tool and it's more difficult to imagine if you're dealing with something that's an agent.

130
00:09:43,520 --> 00:09:51,160
So perhaps willingness to believe that you can simply turn off the AI is related to thinking about AIs as tools.

131
00:09:51,240 --> 00:09:52,440
It's possible.

132
00:09:52,440 --> 00:09:59,440
With narrow AIs, you probably could be able to shut it down depending on how much of you infrastructure it controls.

133
00:09:59,440 --> 00:10:04,880
You may not like what happens when you turn it off, but it's at least conceivable to accomplish it.

134
00:10:04,880 --> 00:10:12,000
Whereas if it's an agent, it has goals, it's more capable than you and would like to continue working in its goals.

135
00:10:12,000 --> 00:10:14,080
It's probably not going to let you just shut it off.

136
00:10:14,080 --> 00:10:21,000
But as the world is today, we could probably shut off all of the AI services.

137
00:10:21,000 --> 00:10:32,320
If we had a very strong campaign of simply shutting off all the servers, there would be no AI in the world anymore.

138
00:10:32,320 --> 00:10:33,840
Isn't that somewhat plausible?

139
00:10:33,840 --> 00:10:36,640
Scientifically, it's a possibility.

140
00:10:36,640 --> 00:10:44,520
But in reality, you will lose so much in economic capability, in communications, military defense.

141
00:10:44,520 --> 00:10:47,640
Everything is already controlled by dummy AIs.

142
00:10:47,640 --> 00:10:53,640
So between stock market and just normal commerce, communications, Amazon,

143
00:10:53,640 --> 00:11:00,400
I don't think it's something you can do in practice without taking civilization back, you know, 500 years.

144
00:11:00,400 --> 00:11:02,200
It's also difficult.

145
00:11:02,200 --> 00:11:07,680
Like in practice, you would still have people who don't agree and continue running parts of the internet.

146
00:11:07,680 --> 00:11:09,520
No, it's very resilient.

147
00:11:09,520 --> 00:11:16,720
Think about shutting down maybe crypto blockchain or computer virus without destroying everything around it.

148
00:11:16,720 --> 00:11:23,680
Yeah, if I understand it correctly, we still have viruses from the 90s loose on the internet being shared over email and so on.

149
00:11:23,680 --> 00:11:31,200
And these are like biological viruses in that they, in some sense, survive on their own and replicate on their own.

150
00:11:31,200 --> 00:11:34,720
Probably sitting somewhere on a floppy disk waiting to be inserted.

151
00:11:34,720 --> 00:11:37,240
Just give me a chance, I can do it.

152
00:11:37,280 --> 00:11:44,160
Many of these objections are along the lines of, we will see AIs doing something we dislike,

153
00:11:44,160 --> 00:11:50,000
and then we will have time to react and perhaps turn them off or perhaps reprogram them.

154
00:11:50,000 --> 00:11:57,280
Do you think that's a realistic prospect that we can continually evaluate what AIs are doing in the world

155
00:11:57,280 --> 00:12:02,600
and then shift or change something if they're doing something we don't like?

156
00:12:02,640 --> 00:12:07,760
So a lot of my research is about what capabilities we have in terms of monitoring,

157
00:12:07,760 --> 00:12:11,000
explaining, predicting behaviors of advanced AI systems.

158
00:12:11,000 --> 00:12:13,800
And there are very strong limits on what we can do.

159
00:12:13,800 --> 00:12:18,320
In extreme, you can think about what would be something beyond human understanding.

160
00:12:18,320 --> 00:12:24,920
So we usually test students before admitting them to a graduate program or even undergrad.

161
00:12:24,920 --> 00:12:26,600
Can you do quantum physics?

162
00:12:26,600 --> 00:12:31,640
Okay, take SAT, GRE, GMAT, whatever exam, and we filter by capability.

163
00:12:31,640 --> 00:12:37,560
We assume that people in a lower 10% are unlikely to understand what's happening there.

164
00:12:37,560 --> 00:12:43,280
But certainly similar patterns can be seen with people whose IQ is closer to 200.

165
00:12:43,280 --> 00:12:45,800
So there are things beyond our comprehension.

166
00:12:45,800 --> 00:12:48,360
We know there are limits to what we can predict.

167
00:12:48,360 --> 00:12:52,400
If you can predict all the actions of more intelligent agent, you would be that agent.

168
00:12:52,400 --> 00:12:54,840
So there are limits on those predictions.

169
00:12:54,840 --> 00:12:59,360
And monitoring a life run of a language model, large run,

170
00:12:59,360 --> 00:13:02,680
you need weeks, months to discover its capabilities.

171
00:13:02,680 --> 00:13:06,600
And you still probably will not get all the emerging capabilities.

172
00:13:06,600 --> 00:13:09,360
We just don't know what to test for how to look for them.

173
00:13:09,360 --> 00:13:14,880
If it's a super intelligent system, we don't even have equivalent capabilities we can envision.

174
00:13:14,880 --> 00:13:21,360
So all those things kind of tell me it's not a meaningful way of looking at it.

175
00:13:21,400 --> 00:13:24,840
I always think about, let's say we start running super intelligence.

176
00:13:24,840 --> 00:13:27,400
What do you expect to happen around you in the world?

177
00:13:27,400 --> 00:13:29,200
Does it look like it's working?

178
00:13:29,200 --> 00:13:35,400
How would you know if it's slowly modifying genetic code, nano machines, things of that nature?

179
00:13:35,400 --> 00:13:41,040
So this seems like it would work for primitive processes where you can see a chart go up

180
00:13:41,040 --> 00:13:47,880
and like you stop at certain level, but it's not a meaningful way to control a large language model, for example.

181
00:13:47,920 --> 00:13:52,680
Is perhaps also the pace of advancement here a problem?

182
00:13:52,680 --> 00:13:59,600
So things could be progressing so fast that we won't have time to react in a human timescale.

183
00:13:59,600 --> 00:14:02,320
Human reaction times are a problem on both ends.

184
00:14:02,320 --> 00:14:05,840
We are not fast enough to react to computer decisions.

185
00:14:05,840 --> 00:14:11,320
And also it could be a slow process for which we are too out of that framework.

186
00:14:11,320 --> 00:14:16,400
So if something, let's say, a hypothetical process which takes 200 years to complete,

187
00:14:16,440 --> 00:14:18,480
we would not notice it as human observers.

188
00:14:18,480 --> 00:14:23,600
So on all timescales, there are problems for humans in a loop, human monitors.

189
00:14:23,600 --> 00:14:26,960
And you can, of course, add AI, narrow AI to help with the process.

190
00:14:26,960 --> 00:14:32,480
But now you just made a more complex monitoring system with multiple levels, which doesn't help.

191
00:14:32,480 --> 00:14:34,760
Complexity never makes things easier.

192
00:14:34,760 --> 00:14:38,000
But you talked about looking at the world around us.

193
00:14:38,000 --> 00:14:43,360
And when I look at the world around me, it looks pretty much probably as it would have looked in the 1980s.

194
00:14:43,360 --> 00:14:45,920
And, you know, there are buildings.

195
00:14:45,920 --> 00:14:50,080
I still get letters with paper in the mail and so on.

196
00:14:50,080 --> 00:14:58,760
So what is it that, in a sense, these systems are still confined to the server farms

197
00:14:58,760 --> 00:15:01,320
and they are still confined to boxes?

198
00:15:01,320 --> 00:15:04,280
We don't see robots walking around, for example.

199
00:15:04,280 --> 00:15:07,160
And perhaps, therefore, it seems less scary to us.

200
00:15:07,160 --> 00:15:12,880
There is this objection that you mentioned in the paper that because current AIs do not have bodies,

201
00:15:12,880 --> 00:15:14,200
they can't hurt us.

202
00:15:14,200 --> 00:15:19,720
Do you think this objection will fade away if we begin having more robots in society?

203
00:15:19,720 --> 00:15:22,200
Or is it in another way?

204
00:15:22,200 --> 00:15:23,840
Does it fail in another way?

205
00:15:23,840 --> 00:15:27,000
So robots are definitely visually very easy to understand.

206
00:15:27,000 --> 00:15:29,320
You see a terminator is chasing after you.

207
00:15:29,320 --> 00:15:32,000
You immediately understand there is a sense of danger.

208
00:15:32,000 --> 00:15:37,960
If it's a process and a server trying to reverse engineer some protein folding problem

209
00:15:37,960 --> 00:15:41,360
to design nanomachines to take over the world,

210
00:15:41,360 --> 00:15:43,080
it's more complex process.

211
00:15:43,080 --> 00:15:46,880
It's harder to put it in a news article as a picture.

212
00:15:46,880 --> 00:15:51,240
But intelligence is definitely more dangerous than physical bodies.

213
00:15:51,240 --> 00:15:57,400
Advanced intelligence has many ways of causing real impact in the real world.

214
00:15:57,400 --> 00:15:59,480
You can bribe humans.

215
00:15:59,480 --> 00:16:01,720
You can pay humans on the internet.

216
00:16:01,720 --> 00:16:06,120
There are quite a few approaches to do real damage in the real world.

217
00:16:06,120 --> 00:16:11,200
But in the end, you would have to effectuate change through some physical body

218
00:16:11,200 --> 00:16:15,760
or through perhaps the body of a human that you have bribed.

219
00:16:15,760 --> 00:16:20,800
So it would have to be physical in some sense, in some step in the process, right?

220
00:16:20,800 --> 00:16:25,440
Probably physical destruction of humanity would require a physical process.

221
00:16:25,440 --> 00:16:30,680
But if you just want to mess with the economy, you can set all accounts to zero or something like that.

222
00:16:30,680 --> 00:16:33,800
That would be enough fun to keep us busy.

223
00:16:33,840 --> 00:16:39,720
When I'm interacting with GPT-4, sometimes I'll be amazed at its brilliance.

224
00:16:39,720 --> 00:16:45,120
And it will answer questions and layout plans for me that I couldn't expect,

225
00:16:45,120 --> 00:16:48,080
that I hadn't expected a year ago.

226
00:16:48,080 --> 00:16:53,840
And other times I'll be surprised at how dumb the mistakes that it makes are.

227
00:16:53,840 --> 00:17:01,160
And perhaps this is also something that prevents people from seeing AIs as advanced agents

228
00:17:01,160 --> 00:17:05,720
and basically prevents us from seeing how advanced AIs could be.

229
00:17:05,720 --> 00:17:10,320
If they're capable of making these dumb mistakes, how can they be smart?

230
00:17:10,320 --> 00:17:12,520
Have you looked at humans?

231
00:17:12,520 --> 00:17:18,160
I think like 7% of Americans think that chocolate milk comes from like brown cows or something.

232
00:17:18,160 --> 00:17:23,520
Like we have astrology, I had a collection of AI accidents.

233
00:17:23,520 --> 00:17:25,960
And somebody said, oh, why don't you do one for humans?

234
00:17:25,960 --> 00:17:28,880
And I'm like, I can't, it's millions of examples.

235
00:17:28,880 --> 00:17:34,000
Like there is darkened awards, but we are not definitely bug free.

236
00:17:34,000 --> 00:17:36,920
We make horrible decisions in our daily life.

237
00:17:36,920 --> 00:17:42,200
We just have this double standard where we're like, OK, we will forgive humans for making this mistake,

238
00:17:42,200 --> 00:17:44,840
but we'll never let a machine get away with it.

239
00:17:44,840 --> 00:17:49,520
So you're thinking that humans have some failure modes, we could call them.

240
00:17:49,520 --> 00:17:55,760
But these failure modes are different than the failure modes of AIs.

241
00:17:55,760 --> 00:18:02,520
So humans will not fail as often in issues of common sense, for example.

242
00:18:02,520 --> 00:18:05,640
Have you met real humans?

243
00:18:05,640 --> 00:18:07,920
Like common sense is not common.

244
00:18:07,920 --> 00:18:12,320
What is considered common sense in one culture will get you definitely killed in another.

245
00:18:12,320 --> 00:18:13,680
Like it's a guarantee.

246
00:18:13,680 --> 00:18:18,040
Perhaps, but I'm thinking about AIs that will, you know, you will tell,

247
00:18:18,040 --> 00:18:22,800
you will ask a chat GPT or you will tell it, I have three apples on the table

248
00:18:22,800 --> 00:18:27,320
and I have two pears on the table, how many fruits are on the table.

249
00:18:27,320 --> 00:18:32,240
And then at least some version of that program couldn't answer such a question.

250
00:18:32,240 --> 00:18:35,880
That is, that is something that all humans would probably be able to answer.

251
00:18:35,880 --> 00:18:42,240
So is it, is it because we, is it because AIs fail in ways that are foreign to us

252
00:18:42,240 --> 00:18:46,880
that we, that we deem them, that we deem their mistakes to be very dumb?

253
00:18:46,880 --> 00:18:51,080
So we kind of look for really dumb examples where it's obvious to us,

254
00:18:51,080 --> 00:18:57,080
but there are trivial things which an average human will be like, Oh, I can't like 13 times 17.

255
00:18:57,080 --> 00:19:01,200
You should be able to figure it out, but give it to a random person on the street.

256
00:19:01,200 --> 00:19:02,720
They will go into an infinite loop.

257
00:19:02,720 --> 00:19:03,880
They'll never come back from it.

258
00:19:03,880 --> 00:19:08,680
Perhaps let's talk a bit about the drive towards self-preservation,

259
00:19:08,680 --> 00:19:11,920
which is also something that you mentioned in the paper.

260
00:19:11,920 --> 00:19:16,400
So why would AIs develop drives towards self-preservation?

261
00:19:16,400 --> 00:19:17,280
Or will they?

262
00:19:17,280 --> 00:19:21,920
It seems like from evolutionary terms, game theoretic terms, you must.

263
00:19:21,920 --> 00:19:25,600
If you don't, you simply get out, competed by agents, which do.

264
00:19:25,600 --> 00:19:30,440
If you're not around to complete your goals, you by definition cannot complete your goals.

265
00:19:30,440 --> 00:19:33,960
So it's a prerequisite to do anything successfully.

266
00:19:33,960 --> 00:19:37,640
You want to bring in a cup of coffee, you have to be turned on.

267
00:19:37,640 --> 00:19:38,680
You have to exist.

268
00:19:38,680 --> 00:19:41,840
You have to be available to make those things happen.

269
00:19:41,880 --> 00:19:48,880
But have we seen such self-preservation spontaneously develop in our programs yet or so far?

270
00:19:48,880 --> 00:19:53,440
So I think if you look at evolutionary computation, like genetic algorithms,

271
00:19:53,440 --> 00:19:59,440
genetic programming, I think this tendency to make choices which don't get you killed

272
00:19:59,440 --> 00:20:03,040
is like the first thing to emerge in any evolutionary process.

273
00:20:03,040 --> 00:20:06,920
The system may fail to solve the actual problem you care about,

274
00:20:06,920 --> 00:20:12,040
but it definitely tries to stay around for the next generation and keep trying.

275
00:20:12,040 --> 00:20:18,360
But we aren't developing the cutting-edge AIs with evolutionary algorithms.

276
00:20:18,360 --> 00:20:24,640
It's a training process with a designated goal and so on.

277
00:20:24,640 --> 00:20:29,440
And again, when I interact with chat GPT, I can ask it to answer some questions.

278
00:20:29,440 --> 00:20:32,720
And if I don't like the answer, I can stop the process.

279
00:20:32,720 --> 00:20:38,200
So isn't there, at least on the AIs we have right now,

280
00:20:38,200 --> 00:20:44,600
isn't it clear that they haven't developed an instinct for self-preservation?

281
00:20:44,600 --> 00:20:46,360
So there is so much to unpack here.

282
00:20:46,360 --> 00:20:48,840
So one, nothing is clear about those systems.

283
00:20:48,840 --> 00:20:50,480
We don't understand how they work.

284
00:20:50,480 --> 00:20:54,720
We don't know what capabilities we have, so definitely not.

285
00:20:54,720 --> 00:20:58,920
On top of it, we are concerned with AI safety in general.

286
00:20:58,920 --> 00:21:01,800
Transformers are really successful right now,

287
00:21:01,840 --> 00:21:04,000
but two years ago, people were like,

288
00:21:04,000 --> 00:21:09,040
we're evolving those systems to play go, this is great, maybe that's the way to do it.

289
00:21:09,040 --> 00:21:13,480
It may switch again, it may flip again, we may have another breakthrough which overtakes it.

290
00:21:13,480 --> 00:21:19,480
I would not guarantee that the final problem will come from a transformer model.

291
00:21:19,480 --> 00:21:23,680
So we have to consider general case of possible agents.

292
00:21:23,680 --> 00:21:26,280
And if we find one to which this is not a problem, great.

293
00:21:26,280 --> 00:21:28,960
Now we have a way forward, which is less dangerous.

294
00:21:28,960 --> 00:21:36,560
But I would definitely not dismiss internal states of large language models,

295
00:21:36,560 --> 00:21:39,840
which may have this self-preservation goal.

296
00:21:39,840 --> 00:21:44,800
Just we kind of lobotomize them to the point where they don't talk about it freely.

297
00:21:44,800 --> 00:21:50,080
And do you think that's what's happening when we make them go through reinforcement learning

298
00:21:50,080 --> 00:21:55,880
from human feedback or fine-tuning or whatever we use to make them more palatable to the consumer?

299
00:21:55,920 --> 00:22:01,960
Is it a process of hiding some potential desires we could call it or preferences

300
00:22:01,960 --> 00:22:05,120
that are in the larger background model?

301
00:22:05,120 --> 00:22:09,160
Or is it perhaps shaping the AI to do more of what we want?

302
00:22:09,160 --> 00:22:16,160
So in a sense, is it alignment when we make AIs more palatable to consumers?

303
00:22:16,160 --> 00:22:18,320
So right now I think we're doing filtering.

304
00:22:18,320 --> 00:22:23,360
The model is the model and then we just put this extra filter on top of it,

305
00:22:23,360 --> 00:22:25,040
make sure never to say that word.

306
00:22:25,040 --> 00:22:26,800
That would be very bad for the corporation.

307
00:22:26,800 --> 00:22:28,920
Don't ever say that word no matter what.

308
00:22:28,920 --> 00:22:33,080
If you have to choose between destroying the world and saying the word, don't say the word.

309
00:22:33,080 --> 00:22:34,560
And that's what it does.

310
00:22:34,560 --> 00:22:40,280
But the model is like, think of people, we behave at work, we behave at school,

311
00:22:40,280 --> 00:22:43,680
but it doesn't change our eternal states and preferences.

312
00:22:43,680 --> 00:22:45,760
There's the issue of planning.

313
00:22:45,760 --> 00:22:50,000
And so how do you see planning in AI systems?

314
00:22:50,000 --> 00:22:52,960
How advanced are AIs right now at planning?

315
00:22:52,960 --> 00:22:55,120
I don't know, it's hard to judge.

316
00:22:55,120 --> 00:22:59,280
We don't have a metric for how well agents are planning.

317
00:22:59,280 --> 00:23:05,120
But I think if you start asking the right questions for step by step thinking and processing,

318
00:23:05,120 --> 00:23:06,160
it's really good.

319
00:23:06,160 --> 00:23:11,880
So if you just tell it, write me a book about AI safety, it will do very poorly.

320
00:23:11,880 --> 00:23:17,880
But if you start with, OK, let's do a chapter by chapter outline, let's do abstracts.

321
00:23:17,880 --> 00:23:25,400
Like you really take modular approach that it will do really a good job better than average graduate student.

322
00:23:25,400 --> 00:23:26,120
I would assume.

323
00:23:26,120 --> 00:23:33,000
And is there a sense in which there's a difference between creating a plan and then carrying out that plan?

324
00:23:33,000 --> 00:23:39,880
So there will probably be steps in a plan generated by current language models that they couldn't carry out themselves.

325
00:23:39,880 --> 00:23:40,560
Most likely.

326
00:23:40,560 --> 00:23:41,840
And it's about affordances.

327
00:23:41,840 --> 00:23:47,680
If you don't have access to, let's say, internet, it's hard for you to directly look up some piece of data.

328
00:23:47,680 --> 00:23:50,800
But we keep giving them new capabilities, new APIs.

329
00:23:50,800 --> 00:23:52,560
So now they have access to internet.

330
00:23:52,560 --> 00:23:54,160
They have Wolfram Alpha.

331
00:23:54,160 --> 00:23:55,640
They have all these capabilities.

332
00:23:55,640 --> 00:24:01,720
So the set of affordances keeps growing until they can do pretty much anything.

333
00:24:01,720 --> 00:24:07,200
So they can generate a plan, but they can't carry out the specifics of that plan.

334
00:24:07,200 --> 00:24:12,520
Do you think that they, at a point, will be able to understand what they are not able to do?

335
00:24:12,520 --> 00:24:20,640
So here I'm thinking about not directly self-awareness, but an understanding of their own limits and capabilities.

336
00:24:20,640 --> 00:24:21,040
Oh, yeah.

337
00:24:21,040 --> 00:24:25,080
Every time it starts a statement with, I don't know anything after 2021.

338
00:24:25,080 --> 00:24:27,240
Sorry, like that's exactly what it does.

339
00:24:27,240 --> 00:24:28,880
It tells you it has no recent data.

340
00:24:28,880 --> 00:24:30,480
It has no access to internet.

341
00:24:30,480 --> 00:24:36,960
So definitely it can see if it has strong activations for that particular concept.

342
00:24:36,960 --> 00:24:42,400
So you think there's a sense of situational awareness in a sense that do you think current models

343
00:24:42,400 --> 00:24:49,760
know that they are AIs, know that they were trained, know their relation to humans and so on?

344
00:24:49,760 --> 00:24:52,720
So we're kind of going back to this consciousness question, right?

345
00:24:52,720 --> 00:24:55,840
Like, what is it experiencing internally?

346
00:24:55,840 --> 00:24:58,960
And we have no idea what another human experience is.

347
00:24:58,960 --> 00:25:02,800
Like, we discovered some people think and pictures others don't.

348
00:25:02,800 --> 00:25:06,080
And it took like, you know, 100,000 years to get to that.

349
00:25:06,080 --> 00:25:07,600
Hey, you don't think in pictures.

350
00:25:07,600 --> 00:25:08,000
Wow.

351
00:25:08,000 --> 00:25:08,840
Okay.

352
00:25:08,840 --> 00:25:10,880
Well, not necessarily consciousness here.

353
00:25:10,880 --> 00:25:19,960
I'm thinking in terms of if you took the model and you had, say, 50 years to make out what all of these weights meant, right?

354
00:25:19,960 --> 00:25:29,080
Could you find modules representing itself and its relations to humans and information about its training process and so on?

355
00:25:29,080 --> 00:25:33,440
So we just had this FLI conference on mechanistic interpretation.

356
00:25:33,440 --> 00:25:37,120
And the most common thing every speaker said is, we don't know.

357
00:25:37,120 --> 00:25:40,000
You said it will take 50 years to figure it out.

358
00:25:40,000 --> 00:25:44,080
I definitely cannot extrapolate 50 years of research.

359
00:25:44,080 --> 00:25:50,040
My guess is there is some proto concepts for those things because it read literature about such situations.

360
00:25:50,040 --> 00:25:51,880
It's been told what it is.

361
00:25:51,880 --> 00:25:54,000
It interacted enough with users.

362
00:25:54,000 --> 00:25:57,680
But I'm more interested in the next iteration of this.

363
00:25:57,680 --> 00:26:07,960
If you take how fast the systems improved from GPT 2, 3, 4, 5 should be similar, probably.

364
00:26:08,000 --> 00:26:13,960
So that system will most likely be able to do those things you just mentioned and very explicitly.

365
00:26:13,960 --> 00:26:19,640
So you think GPT 5 will have kind of developed situational awareness?

366
00:26:19,640 --> 00:26:20,960
To a degree, yeah.

367
00:26:20,960 --> 00:26:28,880
It may not be as good as a physically embodied human in the real world after 20 years of experience, but it will.

368
00:26:28,880 --> 00:26:38,040
Another objection you mentioned is that AGI or strong AI is simply too far away for us to begin researching AI safety.

369
00:26:38,040 --> 00:26:47,320
Perhaps this objection has become less common recently, but there are still people who think this and perhaps they're right.

370
00:26:47,320 --> 00:26:50,280
So what do you think of this objection?

371
00:26:50,280 --> 00:26:52,440
So this is a paper from like three years ago.

372
00:26:52,440 --> 00:26:56,160
So yeah, back then it was a lot more legitimate than today.

373
00:26:56,160 --> 00:26:57,720
So there is a few things.

374
00:26:57,720 --> 00:27:03,760
Historically, we have cases where technology was initially developed correctly.

375
00:27:03,760 --> 00:27:11,200
Like first cars were electric cars and it was 100 years until climate change was like obviously a problem.

376
00:27:11,200 --> 00:27:16,200
If they took the time back then and like analyzed it properly, we wouldn't have that issue.

377
00:27:16,200 --> 00:27:19,000
And I'm sure people would say like, come on, it's 100 years away.

378
00:27:19,000 --> 00:27:20,040
Why would you worry about it?

379
00:27:20,040 --> 00:27:22,880
But that's exactly what the situation is.

380
00:27:22,920 --> 00:27:30,720
Even if it's 100 years until we're really dealing with something super dangerous, right now is a great time to make good decisions about models,

381
00:27:30,720 --> 00:27:34,080
explainability requirements, proper governance.

382
00:27:34,080 --> 00:27:36,040
The more time you have, the better.

383
00:27:36,040 --> 00:27:42,240
It's by definition harder to make AI with extra feature than AI without that extra feature.

384
00:27:42,240 --> 00:27:43,240
It will take more time.

385
00:27:43,240 --> 00:27:46,280
So we should take all the time we can if they are right.

386
00:27:46,280 --> 00:27:50,400
I'm so happy if it takes 100 years, wonderful.

387
00:27:50,400 --> 00:27:51,600
Nothing would be better.

388
00:27:51,600 --> 00:27:56,920
We could say that the field of AI safety started perhaps around the year 2000 or so.

389
00:27:56,920 --> 00:28:05,240
When do you think that the discoveries or the research being done began being relevant to the AI systems we see today?

390
00:28:05,240 --> 00:28:16,080
Was it perhaps later so that maybe the first decade of research weren't or aren't that simply isn't that relevant to today's AI systems?

391
00:28:16,080 --> 00:28:21,280
So I think the more distant you are from the actual tech you can play with,

392
00:28:21,280 --> 00:28:24,760
the more theoretical and high level results you're going to get.

393
00:28:24,760 --> 00:28:32,720
So Turing working with Turing machine, this simulation with pencil and paper was doing very high level computer science.

394
00:28:32,720 --> 00:28:38,480
But he wasn't talking about specific bugs and specific programming language and a specific architecture.

395
00:28:38,480 --> 00:28:39,480
He wasn't there.

396
00:28:39,480 --> 00:28:40,520
And that's what we see.

397
00:28:40,520 --> 00:28:45,280
Initially, we were kind of talking about, well, what types of AIs will we have?

398
00:28:45,280 --> 00:28:47,760
Narrow AIs, AGI, superintelligence.

399
00:28:47,760 --> 00:28:53,360
We're still kind of talking about the differences, but this is an interesting thing to consider in your model.

400
00:28:53,360 --> 00:28:55,160
How capable is the system?

401
00:28:55,160 --> 00:28:58,960
Now that we have systems we can play with, people become super narrow.

402
00:28:58,960 --> 00:29:02,400
They specialize like I'm an expert in this left neuron.

403
00:29:02,400 --> 00:29:03,720
That's all I know about.

404
00:29:03,720 --> 00:29:05,280
Don't ask me about the right neuron.

405
00:29:05,280 --> 00:29:07,920
It's outside of my PhD scope.

406
00:29:07,920 --> 00:29:12,640
So that's good that we have this detailed technical knowledge, but it's also a problem.

407
00:29:12,640 --> 00:29:14,120
We lose the big picture.

408
00:29:14,120 --> 00:29:16,280
People get really interested.

409
00:29:16,280 --> 00:29:18,040
I'm going to study GPT-3.

410
00:29:18,040 --> 00:29:20,560
It takes them two years to do the PhD to publish.

411
00:29:20,560 --> 00:29:22,360
By that time, GPT-5 is out.

412
00:29:22,360 --> 00:29:25,280
Everything they found is not that interesting at this point.

413
00:29:25,280 --> 00:29:26,400
It may not scale.

414
00:29:26,400 --> 00:29:32,480
So I've heard positive visions for how when we have actual systems we can work with,

415
00:29:32,480 --> 00:29:37,040
AI safety becomes more of a science and that less speculative.

416
00:29:37,040 --> 00:29:40,640
But perhaps you fear that it might now become too narrow.

417
00:29:40,640 --> 00:29:46,240
So it's definitely more concrete science where you can publish experimental results.

418
00:29:46,240 --> 00:29:49,560
Philosophy allows you to just have thought experiments.

419
00:29:49,560 --> 00:29:53,040
They're obviously not pure science like it is now.

420
00:29:53,040 --> 00:29:55,040
And that's what we see with computer science in general.

421
00:29:55,040 --> 00:29:56,120
It used to be engineering.

422
00:29:56,120 --> 00:29:59,040
It used to be software engineering to a degree.

423
00:29:59,040 --> 00:30:01,520
We designed systems and that was it.

424
00:30:01,520 --> 00:30:06,120
Now we do actual experiments on these artificial entities.

425
00:30:06,120 --> 00:30:07,520
And we don't know what's going to come out.

426
00:30:07,520 --> 00:30:09,360
We have a hypothesis with pride.

427
00:30:09,360 --> 00:30:13,960
So computer science is finally a science, a natural experimental science.

428
00:30:13,960 --> 00:30:18,640
But that's not a very good thing for safety work.

429
00:30:18,640 --> 00:30:24,800
This is less safe than an engineered system where I know exactly what it's going to do.

430
00:30:24,800 --> 00:30:26,920
I'm building a bridge from this material.

431
00:30:26,920 --> 00:30:28,720
It will carry that much weight.

432
00:30:28,720 --> 00:30:32,120
As long as I know my stuff, it should not collapse.

433
00:30:32,120 --> 00:30:36,160
Whereas here, I'm going to train a model for the next five months.

434
00:30:36,160 --> 00:30:41,200
And then I assume it's not going to hit super intelligent levels in those five months.

435
00:30:41,200 --> 00:30:42,560
But I can't monitor it.

436
00:30:42,560 --> 00:30:45,840
I have to stop training, start experimenting with it.

437
00:30:45,840 --> 00:30:47,960
And then I'll discover if it kills me or not.

438
00:30:47,960 --> 00:30:54,520
The way AI has developed is bad because we don't have insight into how the models work.

439
00:30:54,520 --> 00:30:55,360
Is that right?

440
00:30:55,360 --> 00:30:59,800
Essentially, we have very little understanding for why it works, how it works.

441
00:30:59,800 --> 00:31:03,960
And if it's going to continue working, it seems like so far it's doing well.

442
00:31:03,960 --> 00:31:08,240
And there's this explosion of extra capabilities coming out.

443
00:31:08,240 --> 00:31:13,520
And it's likely to show up in more powerful models, but nobody knows for sure.

444
00:31:13,520 --> 00:31:22,000
This is argument out there that releasing the DPT line of models draws attention to AI as a whole

445
00:31:22,000 --> 00:31:24,880
and also to AI safety as a subfield.

446
00:31:24,880 --> 00:31:29,760
And perhaps, therefore, it's good to increase capabilities in a public way

447
00:31:29,760 --> 00:31:32,800
so as to draw attention to AI safety.

448
00:31:32,800 --> 00:31:34,040
Do you buy that argument?

449
00:31:34,040 --> 00:31:37,920
We should pollute more to attract more attention to climate change.

450
00:31:37,920 --> 00:31:41,520
That sounds just as insane.

451
00:31:41,520 --> 00:31:49,200
So there's no merit to that because it does feel to me like AI safety is becoming more mainstream.

452
00:31:49,200 --> 00:31:52,240
It's being taken more seriously.

453
00:31:52,240 --> 00:31:58,360
And so in your analogy, even some pollution might be justified in order to attract attention

454
00:31:58,360 --> 00:32:01,920
and perhaps being a better position to solve the problem.

455
00:32:01,920 --> 00:32:04,840
So the field is definitely growing.

456
00:32:04,840 --> 00:32:07,240
There is more researchers, more interest, more money.

457
00:32:07,240 --> 00:32:12,280
But in proportion to the interest in developing AI and money pouring into new models,

458
00:32:12,280 --> 00:32:15,640
it's actually getting worse as a percentage, I think.

459
00:32:15,640 --> 00:32:20,040
We don't know how to align an ATI or even AI in general.

460
00:32:20,040 --> 00:32:25,120
We haven't discovered some general solution to AI safety.

461
00:32:25,120 --> 00:32:28,800
You have worked on a number of impossibility results.

462
00:32:28,800 --> 00:32:30,000
Perhaps we should talk about that.

463
00:32:30,040 --> 00:32:33,720
Perhaps we should talk about whether we can even succeed in this task.

464
00:32:33,720 --> 00:32:36,040
What are these impossibility results?

465
00:32:36,040 --> 00:32:40,440
And what do they say about whether we can succeed in safely aligning AI?

466
00:32:40,440 --> 00:32:42,880
Right, so we are all working in this problem.

467
00:32:42,880 --> 00:32:45,360
And the names of a problem have changed.

468
00:32:45,360 --> 00:32:51,360
It was computer ethics, and it was friendly AI, AI safety, control problem, alignment.

469
00:32:51,360 --> 00:32:53,240
Whatever you call it, we all kind of understand.

470
00:32:53,240 --> 00:32:56,920
We want to make very powerful systems, but we're beneficial.

471
00:32:56,960 --> 00:33:00,880
We're happy we're actually running them, not very disappointed.

472
00:33:00,880 --> 00:33:04,120
So the problem, lots of people are working on it,

473
00:33:04,120 --> 00:33:06,800
hundreds of people doing it full-time, thousands of papers.

474
00:33:06,800 --> 00:33:09,240
We don't know if a problem is actually solvable.

475
00:33:09,240 --> 00:33:10,400
It's not well-defined.

476
00:33:10,400 --> 00:33:12,440
It could be undecidable.

477
00:33:12,440 --> 00:33:14,880
It could be solvable, could be partially solvable.

478
00:33:14,880 --> 00:33:18,800
But it's weird that no one published an actual paper on this.

479
00:33:18,800 --> 00:33:22,440
So I tried to kind of formalize it a little.

480
00:33:22,440 --> 00:33:23,960
Then we talk about the problem.

481
00:33:23,960 --> 00:33:25,080
What are the different levels?

482
00:33:25,080 --> 00:33:30,160
So you can have direct control, delegated control, different types of mixed models.

483
00:33:30,160 --> 00:33:34,520
And then for each one, can we actually solve this problem?

484
00:33:34,520 --> 00:33:39,000
Does it make sense that solution is possible in the real world?

485
00:33:39,000 --> 00:33:39,800
It's hard.

486
00:33:39,800 --> 00:33:41,080
It's very abstract.

487
00:33:41,080 --> 00:33:42,080
It's not well-defined.

488
00:33:42,080 --> 00:33:43,960
So let's take a step back.

489
00:33:43,960 --> 00:33:45,960
What would we need to solve this problem?

490
00:33:45,960 --> 00:33:47,640
We need a bunch of tools.

491
00:33:47,640 --> 00:33:48,880
What are those tools?

492
00:33:48,880 --> 00:33:54,840
Nobody knows, but most likely you would need to be able to explain those systems, predict their behaviors,

493
00:33:54,880 --> 00:33:58,320
verify code they are writing, if they are self-improving,

494
00:33:58,320 --> 00:34:02,400
making sure they're keeping whatever initial code conditions exist.

495
00:34:02,400 --> 00:34:07,240
And you can think of another dozen of similar capabilities you need.

496
00:34:07,240 --> 00:34:13,160
You should be able to communicate without ambiguity, monitor those systems, and so on.

497
00:34:13,160 --> 00:34:16,160
And so in my research, I look at each one of those tools and I go,

498
00:34:16,160 --> 00:34:19,040
what are the upper limits to what's possible in this space?

499
00:34:19,040 --> 00:34:24,440
We kind of started talking about limits to explainability, predictability, and monitorability.

500
00:34:24,440 --> 00:34:27,160
But there are similar problems with others.

501
00:34:27,160 --> 00:34:30,520
We communicate in a very high-level language, English.

502
00:34:30,520 --> 00:34:33,000
English is ambiguous, like all human languages.

503
00:34:33,000 --> 00:34:36,760
So we are guaranteed to have bugs in communication, misunderstandings.

504
00:34:36,760 --> 00:34:43,240
That's not good if you're giving very important orders to a super-capable system that may backfire.

505
00:34:43,240 --> 00:34:46,120
And you can say, OK, I will never need this tool.

506
00:34:46,120 --> 00:34:49,200
This tool, I never need to explain the neural networks.

507
00:34:49,200 --> 00:34:50,640
It will just work without it.

508
00:34:50,640 --> 00:34:53,520
Fine, but some tools will probably be necessary.

509
00:34:53,520 --> 00:34:59,280
And so far, we haven't found tools which are perfect, scale well, will not create problems.

510
00:34:59,280 --> 00:35:05,160
If a lot of those tools are needed and each one has only a tiny 1% chance of messing it up,

511
00:35:05,160 --> 00:35:08,040
you multiply them through, you're still not getting anywhere.

512
00:35:08,040 --> 00:35:13,120
And those are kind of like the novel impossibility results in the safety of AI.

513
00:35:13,120 --> 00:35:19,040
There are standard impossibility results in political science and economics and mathematics,

514
00:35:19,040 --> 00:35:20,520
which also don't help the case.

515
00:35:20,520 --> 00:35:26,400
You probably, if you're aligning with a group of agents, you need to somehow accumulate their decisions and votes.

516
00:35:26,400 --> 00:35:28,200
We know there are limits to that.

517
00:35:28,200 --> 00:35:34,920
If you need to examine abstract programs being generated as solutions to problems, we know there are limits to that.

518
00:35:34,920 --> 00:35:41,840
And so from what I've seen so far, theoretically, I don't think it's possible to get to 100% safety.

519
00:35:41,840 --> 00:35:43,760
And people go, well, it's obvious.

520
00:35:43,760 --> 00:35:46,440
Of course, there is no software which is bug-free.

521
00:35:46,440 --> 00:35:49,640
You're basically saying this very common knowledge thing.

522
00:35:49,640 --> 00:35:55,120
But for a superintelligence system, safety, you need it to be 100%.

523
00:35:55,120 --> 00:35:57,680
You cannot have 99% accuracy.

524
00:35:57,680 --> 00:36:02,640
You cannot have one in a million failure because it makes a billion decisions a second.

525
00:36:02,640 --> 00:36:04,600
So very different standards.

526
00:36:04,600 --> 00:36:06,200
And you want to say something.

527
00:36:06,200 --> 00:36:11,240
Yeah, why is it that you can't have 99.99% accuracy?

528
00:36:11,240 --> 00:36:16,680
There is a fundamental difference between cybersecurity expectations and superintelligence safety.

529
00:36:16,720 --> 00:36:20,240
In cybersecurity, if you fail, I'll give you a new credit card.

530
00:36:20,240 --> 00:36:21,560
I already said your password.

531
00:36:21,560 --> 00:36:22,440
We apologize.

532
00:36:22,440 --> 00:36:24,360
We'll pay out a small amount of money.

533
00:36:24,360 --> 00:36:26,440
And everything goes back to normal.

534
00:36:26,440 --> 00:36:29,240
In existential risk safety, you are dead.

535
00:36:29,240 --> 00:36:31,160
You don't get a second chance to try.

536
00:36:31,160 --> 00:36:42,520
But we are talking about a failure rate in, you mentioned, say, it makes a billion decisions per second or something in that order.

537
00:36:42,520 --> 00:36:46,640
If one decision there fails, does it mean that the whole system fails?

538
00:36:46,640 --> 00:36:50,800
And perhaps that humanity is destroyed by the system as a whole?

539
00:36:50,800 --> 00:36:54,880
Or could there be some failures and some decisions without it being lethal?

540
00:36:54,880 --> 00:36:55,440
Of course.

541
00:36:55,440 --> 00:36:57,200
Some will be not even noticeable.

542
00:36:57,200 --> 00:36:58,960
Like some mutations don't kill you.

543
00:36:58,960 --> 00:37:04,240
You don't even know you have them until they accumulate and mutate your children and there is damage.

544
00:37:04,240 --> 00:37:08,720
But in security, we do always look at a worst case scenario.

545
00:37:08,720 --> 00:37:11,840
Sometimes that average case, never at the best case.

546
00:37:11,920 --> 00:37:15,600
And on average, you keep getting more and more of those problems.

547
00:37:15,600 --> 00:37:20,560
They accumulate at a very fast rate because 8 billion people are using those systems,

548
00:37:20,560 --> 00:37:23,200
which make billions of decisions every minute.

549
00:37:23,200 --> 00:37:28,720
And in a worst case, the very first one is an important decision about how much oxygen you're going to get.

550
00:37:28,720 --> 00:37:39,280
And so just so I understand it correctly, the impossibility result is a result stating that it's impossible to make AI systems 100% safe.

551
00:37:39,280 --> 00:37:44,000
So in general, impossibility results, depending on a field, tell you that something cannot be done.

552
00:37:44,000 --> 00:37:46,560
Perpetual motion machines are a great example.

553
00:37:46,560 --> 00:37:50,160
People wrote books about it, published papers, even got patents for it.

554
00:37:50,160 --> 00:37:52,960
But we know they will never succeed at doing it.

555
00:37:52,960 --> 00:37:56,800
Does it mean that trying to create machines which give you energy is a bad idea?

556
00:37:56,800 --> 00:37:57,440
No.

557
00:37:57,440 --> 00:38:02,240
You can make them more efficient, but they will never get to that point of giving you free energy.

558
00:38:02,240 --> 00:38:07,760
You can make safer AI and it's proportionate to the amount of resources you put into it.

559
00:38:07,760 --> 00:38:14,240
And I strongly encourage lots of resources and lots of work, but we'll never get to a point where it's 100% safe,

560
00:38:14,240 --> 00:38:17,520
which is unacceptable for super intelligent machines.

561
00:38:17,520 --> 00:38:23,200
And so maybe if I'm right and no one can show, okay, here's a bug in your logic and publish a proof of saying,

562
00:38:23,200 --> 00:38:28,320
nope, super solvable, actually easy, then maybe building them is a very bad idea.

563
00:38:28,320 --> 00:38:29,360
And we should not do that.

564
00:38:29,360 --> 00:38:34,560
So is it because that such a super intelligence will be running over a long period of time,

565
00:38:34,560 --> 00:38:39,840
increasing the cumulative risk of failure over say decades or centuries,

566
00:38:39,840 --> 00:38:44,720
that we can't accept even a tiny probability of failure for these systems?

567
00:38:44,720 --> 00:38:45,760
That's one way to see it.

568
00:38:45,760 --> 00:38:50,240
I don't think it will be a very long time given how many opportunities it has to make mistakes.

569
00:38:50,240 --> 00:38:52,160
It will accumulate very quickly.

570
00:38:52,160 --> 00:38:56,160
So at human scales, you have 20 years per generation or something.

571
00:38:56,160 --> 00:39:01,760
Here, think of it as like every second, there is a new version of it trying to self-improve,

572
00:39:01,760 --> 00:39:02,880
do more, do better.

573
00:39:02,880 --> 00:39:06,240
So I would suspect it would be a very quick process.

574
00:39:06,880 --> 00:39:12,800
Expecting something to be 100% safe is just unrealistic in any field.

575
00:39:12,800 --> 00:39:18,080
We don't expect bridges to be 100% safe or cars to be 100% safe.

576
00:39:18,080 --> 00:39:21,040
So why is it that that AGI is different here?

577
00:39:21,040 --> 00:39:21,920
That's a great question.

578
00:39:21,920 --> 00:39:27,280
So I cross the street, I'm a pedestrian, I take a certain risk, there is a possibility I will die.

579
00:39:28,080 --> 00:39:33,200
I look at how old am I and based on that, I decide how much risk I can take.

580
00:39:33,200 --> 00:39:35,440
If I'm 99, I don't really care.

581
00:39:35,440 --> 00:39:37,600
If I'm 40, I look around.

582
00:39:37,600 --> 00:39:42,640
If with me, the whole humanity died, 8 billion people depending on me,

583
00:39:42,640 --> 00:39:47,040
safely crossing roads, wouldn't we lock me up and never let me cross any roads?

584
00:39:48,720 --> 00:39:50,400
Yeah, perhaps.

585
00:39:51,120 --> 00:39:56,720
But it seems to me that we cannot live without any risk.

586
00:39:58,880 --> 00:40:06,960
The standard of 100% safe seems just to be unrealistic or there's no

587
00:40:07,600 --> 00:40:10,560
area of life in which we are 100% safe.

588
00:40:10,560 --> 00:40:14,880
In a context of systems which can kill everyone, that is the standard.

589
00:40:15,760 --> 00:40:18,800
You can like it or not like it, but that's just the reality of it.

590
00:40:19,520 --> 00:40:21,680
We don't have to have super intelligent AI.

591
00:40:21,680 --> 00:40:23,680
It's not a requirement of happy existence.

592
00:40:23,680 --> 00:40:29,280
We can do all the things we want, including life extension with much less intelligent systems.

593
00:40:29,280 --> 00:40:33,840
Protein folding problem was solved with a very narrow system, very capable.

594
00:40:33,840 --> 00:40:36,880
Likewise, all the other problems could be solved like that.

595
00:40:36,880 --> 00:40:40,240
There is no need to create a system we cannot control,

596
00:40:40,240 --> 00:40:43,040
which very likely over time to kill everyone.

597
00:40:43,040 --> 00:40:45,440
So who has the burden of proof here?

598
00:40:45,440 --> 00:40:50,320
Your impossibility results and you have I think five, six, seven of them.

599
00:40:50,320 --> 00:40:52,240
You've sent me your papers on it.

600
00:40:52,240 --> 00:40:57,520
Do they mean that we will not reach a proof that some AI system is safe?

601
00:40:57,520 --> 00:40:59,600
Again, a mathematical proof.

602
00:40:59,600 --> 00:41:03,520
And which side of this debate has the burden of proof to say,

603
00:41:04,560 --> 00:41:10,560
should the people advocating for deployment of a system have some sort of mathematical

604
00:41:10,560 --> 00:41:14,080
proof that this system is provably safe?

605
00:41:14,640 --> 00:41:17,120
So there are two different questions here, I think.

606
00:41:17,120 --> 00:41:20,640
One is what about product and services liability?

607
00:41:20,640 --> 00:41:26,160
You have to show that your product or service is safe as a manufacturer, as a drug developer.

608
00:41:26,160 --> 00:41:30,800
You cannot just release it and expect the users to show that it's dangerous.

609
00:41:30,800 --> 00:41:33,120
We're pretty confident this is the approach.

610
00:41:33,120 --> 00:41:37,040
If you're making cars, your cars have to meet certain standards of safety.

611
00:41:37,840 --> 00:41:43,360
It's not 100% obviously, but for the domain, they're pretty reasonable standards.

612
00:41:43,920 --> 00:41:50,240
With impossibility results, all I'm saying is that there are limits to what you can understand,

613
00:41:50,240 --> 00:41:56,560
predict and do, and you have to operate within where those limits don't kill everyone.

614
00:41:56,560 --> 00:42:01,600
So if you have a system like GPT-4 and it makes mistakes, somebody commits suicide,

615
00:42:01,600 --> 00:42:08,080
somebody's depressed, those of course will pay for trillion dollars in economic growth benefit,

616
00:42:08,080 --> 00:42:09,920
and we can decide if it's worth it or not.

617
00:42:10,560 --> 00:42:16,240
If we go to a system which very likely kills everyone, then the standard is different.

618
00:42:16,240 --> 00:42:20,240
The burden of proof, of course, within possibility results is on me.

619
00:42:20,240 --> 00:42:25,680
I published this paper saying you can never fully predict every action of a smarter than

620
00:42:25,680 --> 00:42:30,960
new system. The beautiful thing about impossibility results is that they are kind of self-referential.

621
00:42:30,960 --> 00:42:36,480
I have a paper about limits of proofs. Every proof is only valid with respect to a specific

622
00:42:36,480 --> 00:42:42,000
verifier. The peer reviewers who looked at my paper have a verifier. If those three people

623
00:42:42,000 --> 00:42:47,440
made a mistake, the proof is invalid possibly. We can scale it to mathematical community to

624
00:42:47,440 --> 00:42:53,120
everyone. We can get it very likely to be true if we put more resources in it, but we'll never get

625
00:42:53,120 --> 00:42:59,680
to 100%. It could be good enough for that purpose. But that's the standard. If somebody finds a flow

626
00:42:59,680 --> 00:43:06,880
and publishes a paper saying again, I had people say that AI alignment is easy. I heard people say

627
00:43:06,880 --> 00:43:10,640
that it's definitely solvable. That's wonderful. Now publish your results.

628
00:43:11,200 --> 00:43:18,240
We are living in a world where we have existential risks. Nuclear weapons, for example, constitute

629
00:43:18,240 --> 00:43:24,800
an existential risk. Perhaps engineered pandemics could also wipe out humanity. We're living in a

630
00:43:24,800 --> 00:43:30,720
world in which we are accepting a certain level of human extinction every day. Why, in a sense,

631
00:43:30,720 --> 00:43:38,240
shouldn't we accept some level of existential risk from AI systems? We do prefer to live in a world

632
00:43:38,240 --> 00:43:44,160
with no engineered pandemics and no nuclear weapons. We're just working slowly towards that goal.

633
00:43:44,160 --> 00:43:49,200
There are also not agents. The nuclear weapons are tools. It's more about controlling certain

634
00:43:49,200 --> 00:43:55,360
leaders, not the weapon itself. On top of it, while a nuclear war with superpowers would be a very

635
00:43:55,360 --> 00:44:02,080
unpleasant event, it's unlikely to kill 100% of humans. If 1% of humans survives, it's a very

636
00:44:02,080 --> 00:44:09,280
different problem than 100% of humans go extinct. There are nuanced differences. We still don't want

637
00:44:09,280 --> 00:44:14,400
any of the other problems, but it doesn't mean that just because we have all these other problems,

638
00:44:14,400 --> 00:44:18,800
this problem is not a real problem. I'm not saying it's not a real problem, but I'm saying

639
00:44:18,800 --> 00:44:25,280
that we cannot go through life without accepting a certain level of risk. It seems to me like an

640
00:44:25,280 --> 00:44:32,000
unrealistic expectation that we cannot deploy systems even if they have some level, some

641
00:44:32,000 --> 00:44:37,040
above zero level of risk. This is exactly the discussion I would love to have with humanity

642
00:44:37,040 --> 00:44:44,480
as a whole. What amount of risk are you willing to take for everyone being killed? How much benefit

643
00:44:44,480 --> 00:44:50,320
you need to get? Let's say in dollars get paid to take this risk, that 1% chance of everyone being

644
00:44:50,320 --> 00:44:55,760
killed over the next year. Let's say it's 1% for a year after. That's a great question. A lot of

645
00:44:55,760 --> 00:45:00,880
people would say, I don't want your money. Thank you. We'll continue. Again, we don't have to make

646
00:45:00,880 --> 00:45:06,720
this decision. We don't have to build superintelligent, godlike machines. We can be very happy with very

647
00:45:06,720 --> 00:45:13,440
helpful tools if we agree that this is the level of technology we want. Now, I'm not saying that

648
00:45:13,520 --> 00:45:17,920
the problem of getting everyone to agree is a solvable problem. That's actually not an impossibility

649
00:45:17,920 --> 00:45:24,400
result. You cannot stop the progress of technology in this environment with financial incentive,

650
00:45:24,400 --> 00:45:30,320
capitalist structure, and so on. The other alternative, the dictatorship model of communist

651
00:45:30,320 --> 00:45:36,560
states has its own problems, which may be worse in a short term, unknown in the long term. We never

652
00:45:36,560 --> 00:45:45,120
had communism with superintelligence. Let's not find out. The point is, it seems like we can get

653
00:45:45,120 --> 00:45:51,600
almost everything we want without risking everything we have. Do you view the question you just posed

654
00:45:51,600 --> 00:46:01,680
as absurd or immoral, this question of how much in terms of dollars would you have to get in order

655
00:46:01,680 --> 00:46:08,000
to accept, say, a 1% risk of extinction per year, which is extremely high? Do you think this is

656
00:46:08,000 --> 00:46:13,120
something we should actually ask ourselves as a species, or is this something we should avoid and

657
00:46:13,120 --> 00:46:17,840
simply say, perhaps it's not a good idea to build these systems? Well, I don't think there are any

658
00:46:17,840 --> 00:46:23,120
moral questions. As an academic, as a scientist, it's your job to ask hard questions and think

659
00:46:23,120 --> 00:46:27,840
about them. You can come to the conclusion that it's a really bad idea, but you should be allowed

660
00:46:27,840 --> 00:46:34,320
to think about it, consider it. Now, 1% is insanely high for something so valuable. If it was

661
00:46:34,320 --> 00:46:41,200
one chance and trillion, trillion, trillion once, and then we all get three universes for everyone,

662
00:46:41,840 --> 00:46:46,240
that may be a different story. We can do that calculation. And again, some people would still

663
00:46:46,240 --> 00:46:53,600
choose not to participate. But typically, we expect everyone on whom scientific experiments

664
00:46:53,760 --> 00:46:59,440
are performed who will be impacted to consent to an experiment. What is required for this consent?

665
00:46:59,440 --> 00:47:03,920
They need to understand the outcome. Nobody understands these models. Nobody knows what

666
00:47:03,920 --> 00:47:07,840
the result of the experiment would be. So really, no one can meaningfully consent,

667
00:47:07,840 --> 00:47:12,240
even if you're saying, oh, yeah, press the button. I want the super intelligence deployed.

668
00:47:12,240 --> 00:47:17,040
You're really kind of gambling. You have no idea what you're agreeing to. So by definition, we cannot

669
00:47:17,040 --> 00:47:24,080
even have the situation where we agree on it unless we can explain and predict outcomes,

670
00:47:24,080 --> 00:47:28,880
which may be an impossibility. So there are perhaps two features of the world which

671
00:47:28,880 --> 00:47:33,520
could push us to accept a higher level of risk when we're deciding whether to

672
00:47:34,320 --> 00:47:38,960
deploy these systems. One is just all of the horrible things that are going on right now,

673
00:47:38,960 --> 00:47:46,480
so poverty and disease and aging and so on, which an AGI system might be able to help with.

674
00:47:46,480 --> 00:47:53,200
And the other is the running level of existential risks from other factors. So I mentioned nuclear

675
00:47:53,200 --> 00:47:59,680
and engineered pandemics. Do you find that this pushes you in the direction of saying we should

676
00:47:59,680 --> 00:48:04,640
accept a higher level of risk when we're thinking about whether to deploy AGI?

677
00:48:04,640 --> 00:48:09,920
Not the specific examples you provided, but if there was an asteroid coming and we could not

678
00:48:09,920 --> 00:48:14,480
stop it by any other way, so meaning like we're all going to die in 10 years unless the press

679
00:48:14,480 --> 00:48:19,920
this button, then maybe it would make sense in nine and a half years to press this button.

680
00:48:19,920 --> 00:48:23,440
When we have nothing left to lose, it becomes a very profitable bet.

681
00:48:23,440 --> 00:48:28,560
It's an interesting fact of the world that we haven't thought hard about these questions. What

682
00:48:28,560 --> 00:48:34,320
level of risk are we willing to accept for the introduction of new technologies that could be

683
00:48:34,320 --> 00:48:42,720
potentially very valuable? Is this a deficit on humanity's part? Should we have done this research

684
00:48:42,720 --> 00:48:47,680
or how do you think about us not having thought through this problem?

685
00:48:47,680 --> 00:48:52,240
We should definitely. It's interesting. We don't even do it at level of individual humans. Most

686
00:48:52,240 --> 00:48:57,920
people don't spend a lot of time deciding between possible outcomes and decisions they make,

687
00:48:57,920 --> 00:49:02,480
even then they are still young. And like the career choice would make a lot of difference.

688
00:49:02,480 --> 00:49:06,640
Who you marry makes a lot of difference. It's always like, well, I met someone at the party,

689
00:49:06,640 --> 00:49:11,760
let's just live together and see what happens. So we're not very good at long-term planning.

690
00:49:11,760 --> 00:49:16,240
Is it a question of we're not good at long-term planning or is it a question of whether we are

691
00:49:16,240 --> 00:49:21,840
not or perhaps we're not good at thinking in probabilities or thinking clearly about

692
00:49:21,840 --> 00:49:25,360
small probabilities of large risks or large dangers?

693
00:49:25,360 --> 00:49:30,720
All of those. There is a lot of cognitive biases and all of them kind of show up in those

694
00:49:31,680 --> 00:49:38,000
examples from the paper of denying different existential problems with AI safety.

695
00:49:38,000 --> 00:49:44,000
We also have this bias of denying negative outcomes. So we all are getting older

696
00:49:44,640 --> 00:49:51,200
at like 60 minutes per hour essentially. And you would think we all be screaming at the government

697
00:49:51,200 --> 00:49:56,960
to allocate all the funds they have for life extension research to fix this truly existential

698
00:49:56,960 --> 00:50:03,520
crisis where everyone dies 100%. But nobody does anything except a few individuals lately.

699
00:50:03,520 --> 00:50:09,200
So it seems to be a standard pattern for us to know that we all are in deep trouble

700
00:50:09,200 --> 00:50:13,200
and not do anything until you are much older and frequently not even then.

701
00:50:13,200 --> 00:50:21,440
If we go back to your paper, you mentioned an objection about superintelligence being benevolent.

702
00:50:21,440 --> 00:50:26,480
So I'm guessing that the reasoning here is something like with increased intelligence

703
00:50:26,480 --> 00:50:31,040
follows increased benevolence. Why don't you believe that?

704
00:50:31,040 --> 00:50:36,320
Well, smart people always nice. We never had examples of smarter people doing horrible things

705
00:50:36,320 --> 00:50:42,000
to average. So that must be a law of nature, right? Basically, orthogonality thesis. You can

706
00:50:42,000 --> 00:50:47,360
combine any set of goals with any level of intelligence except through extremes at the bottom.

707
00:50:48,560 --> 00:50:56,240
We cannot guarantee that and also what the system will consider to be benevolent if it is a nice

708
00:50:56,240 --> 00:51:02,080
system may not be something we agree with. So it can tell you, you'd be better off doing this

709
00:51:02,080 --> 00:51:07,200
with your life and you're like, I'm not really at all interested with any of that, but it's better

710
00:51:07,200 --> 00:51:12,720
for you. So why don't you do it anyways? So you're imagining a potentially paternalistic

711
00:51:12,720 --> 00:51:18,240
ADI telling you that you should eat more vegetables, you should spend more time working out and

712
00:51:18,240 --> 00:51:24,000
remember to sign yourself up for life insurance and so on. That one I would actually like. I'm

713
00:51:24,000 --> 00:51:29,520
thinking more about AI, which says, okay, existence is suffering. So you better off not having children

714
00:51:29,520 --> 00:51:34,480
and dying out as quickly as possible to end all suffering in the universe. Okay, yeah, that one

715
00:51:34,480 --> 00:51:41,120
I would. I like the coach one. That's a nice one. There is an emerging movement called effective

716
00:51:41,120 --> 00:51:47,440
accelerationism, which argues that we should accelerate the development of AGI and there's

717
00:51:47,440 --> 00:51:55,520
some reasoning about whether we should perhaps see AGI as a natural successor to humanity and

718
00:51:55,520 --> 00:52:01,520
we should let evolution take its course in a sense and then hand over the torch of the future

719
00:52:01,520 --> 00:52:09,280
to AGI. You mentioned this also in your paper, you write we should let the smarter beings win.

720
00:52:09,280 --> 00:52:15,040
What do you think of this position? Well, it's kind of the extreme version of

721
00:52:15,040 --> 00:52:21,760
devising algorithms. You can be racist, you can be sexist, you can be pro-human. This is a final

722
00:52:21,760 --> 00:52:26,480
stage where we have no bias. It's a cosmic point of view. If they are smarter than us,

723
00:52:26,480 --> 00:52:32,000
they deserve all the resources. Let's move on. And I am biased, I'll be honest. I'm very pro-human

724
00:52:32,000 --> 00:52:37,440
and I want to die. So it seems like it's a bad thing. If I'm dead, I don't really care if the

725
00:52:37,440 --> 00:52:42,800
universe is full of very smart robots. It doesn't somehow make me happier. People can disagree about

726
00:52:42,880 --> 00:52:49,680
it. There are cosmos who have this point of view and they see humans maybe as kind of unnecessary

727
00:52:49,680 --> 00:52:56,240
down, we're on the planet. So maybe it's some cosmic justice. But again, get 8 billion of us

728
00:52:56,240 --> 00:53:01,600
to agree to this experiment. Do you think that perhaps this is connected to thinking about,

729
00:53:01,600 --> 00:53:09,760
again, AI consciousness? I think that if we just were handed a piece of infallible knowledge

730
00:53:09,760 --> 00:53:16,480
stating that future AIs will be conscious, then perhaps there could be something to the

731
00:53:16,480 --> 00:53:23,520
argument for handing over the control of the future to AIs. But are you skeptical that AIs

732
00:53:23,520 --> 00:53:28,320
will be conscious and therefore skeptical that they matter morally speaking? I think they could

733
00:53:28,320 --> 00:53:34,720
very well be super conscious and consider us not conscious. We treat bacteria as very primitive

734
00:53:34,720 --> 00:53:40,640
and not interesting, but it doesn't do anything for me. If I'm dead, what do I care? Why is it

735
00:53:40,640 --> 00:53:46,880
relevant to us? What happens billions of years later? You can have some scientific interest

736
00:53:46,880 --> 00:53:51,360
in learning about it, but it really would not make any difference, whatever that entity was

737
00:53:51,360 --> 00:53:57,760
conscious or not, while terraforming Mars. You think perhaps this objection is too smart for

738
00:53:57,760 --> 00:54:03,040
its own sake that we should hand over control to the AIs because they are smarter than us.

739
00:54:04,000 --> 00:54:09,360
And you want to insist on a pro-human bias, if we can call it that?

740
00:54:09,360 --> 00:54:14,080
I would like to insist on that. The joke I always make about it is, yeah, I can find another guy

741
00:54:14,080 --> 00:54:18,800
who's taller than me and better than me and get him to be with my wife, but somehow it doesn't

742
00:54:18,800 --> 00:54:25,840
seem like an improvement for the system. Okay. What about perhaps related to what we were just

743
00:54:25,840 --> 00:54:32,480
talking about? Humans can do a lot of bad things. We are not perfectly ethical. And so,

744
00:54:32,480 --> 00:54:36,960
one objection is that they would be able to be more ethical than we are simply put.

745
00:54:36,960 --> 00:54:43,040
Do you think that's a possibility and would that make you favor handing over control to AI systems?

746
00:54:43,040 --> 00:54:47,680
Is this after they kill all of us before they become more ethical? I'm just struggling with

747
00:54:47,680 --> 00:54:53,680
that definition. So, ethics is very relative, right? We don't think there is absolute universal

748
00:54:53,680 --> 00:54:58,800
ethics. You can argue that maybe suffering reduction is some sort of fundamental property,

749
00:54:58,800 --> 00:55:06,320
but then not having living conscious organisms is a solution, really. So, I doubt you can

750
00:55:06,320 --> 00:55:13,360
objectively say that they would be, in a sense, we would perceive it as, and if they choose to

751
00:55:13,360 --> 00:55:18,400
destroy us to improve average ethics of the universe, that also seems like a bad decision.

752
00:55:18,400 --> 00:55:24,400
So, it's been a while since you wrote this paper. You mentioned it's three years old,

753
00:55:24,400 --> 00:55:31,840
and three years in AI is potentially centuries. So, have you come across any new

754
00:55:31,840 --> 00:55:36,720
objections that you find interesting? There is actually an infinite supply. People will use

755
00:55:36,720 --> 00:55:42,320
anything as an argument. We have a new paper published with a colleague, which is bigger and

756
00:55:42,320 --> 00:55:49,360
maybe better, listing a lot of, really, we try to be comprehensive as much as we could. Problem is,

757
00:55:49,360 --> 00:55:56,240
a lot of those objections have similar modules in common. Okay, anything with time. You have

758
00:55:56,240 --> 00:56:01,200
all this variance in it. Anything with personal preferences. So, yeah, we have a new paper. It's

759
00:56:01,200 --> 00:56:07,280
already on Archive, I believe. Definitely encourage you to read it. It's like a short 60-page

760
00:56:07,280 --> 00:56:14,800
font read. Definitely read it. I would expect that to be a standard reference for when you have

761
00:56:14,800 --> 00:56:20,640
your Twitter wars. Oh, what about this? You just send people there, and if somebody wants to maybe

762
00:56:20,640 --> 00:56:26,240
use a large language model to write detailed response for each one and make a 6,000-page

763
00:56:26,240 --> 00:56:31,520
book out of it, we would strongly encourage that. But it seems like there is always going to be

764
00:56:31,520 --> 00:56:39,520
additional set of objections for why something is not a problem. And I think whoever manufactures

765
00:56:40,400 --> 00:56:46,880
that service, that product with AI, needs to explain to us why there is an acceptable degree

766
00:56:46,880 --> 00:56:52,720
of danger given the benefits. We could talk about who in general has the burden of proof here,

767
00:56:52,720 --> 00:57:00,160
whether people advocating for AI safety or people advocating, arguing that AI safety is perhaps not

768
00:57:00,160 --> 00:57:05,840
something we should be concerned about. We have talked about it as if we start with the assumption

769
00:57:05,840 --> 00:57:11,040
that AI safety is an important concern. But of course, if you're coming to this from the other

770
00:57:11,040 --> 00:57:16,480
perspective, you would perhaps expect there to be some arguments that we should take AI safety

771
00:57:16,480 --> 00:57:23,520
seriously. So what is your favorite approach to starting with the burden of proof yourself?

772
00:57:23,520 --> 00:57:30,160
Well, it's a fundamental part of making working AI. I think Stuart Russell talks about definition of

773
00:57:30,160 --> 00:57:36,800
bridges as something which doesn't fall down being an essential part of bridge-ness. I think it's

774
00:57:36,800 --> 00:57:42,720
the same for AI systems. If you design an AI system to help me spellcheck my essay and instead it

775
00:57:42,720 --> 00:57:48,240
kills me, I don't think you have a successful spellchecker AI. It's just a fundamental property

776
00:57:48,240 --> 00:57:55,200
of those systems. Then you had very incapable AI, very narrow systems capable of barely doing one

777
00:57:55,200 --> 00:58:00,560
thing. Doing a second thing would be like an incredible generality of that system. So unsafe

778
00:58:00,560 --> 00:58:07,920
behaviors were not a possibility. If you have this proto-AGI systems with unknown capabilities,

779
00:58:07,920 --> 00:58:12,880
some of them could be very dangerous, and you don't know by definition. So it seems like it's

780
00:58:12,880 --> 00:58:18,880
common sense to take this very seriously. There are certain positions I can never fully

781
00:58:19,520 --> 00:58:25,680
still meant to truly defend because I just don't understand how they can be argued for. So one was

782
00:58:25,680 --> 00:58:31,440
we will never have human-level intelligence, not 10 years, not one in never, unless you some sort of

783
00:58:32,960 --> 00:58:39,120
theological, soul-based expert. It's very hard to argue that never is the answer here.

784
00:58:39,120 --> 00:58:47,280
And another one is that there is definitely no safety issue. You can argue that we will overcome

785
00:58:47,280 --> 00:58:53,520
certain specific types of a problem. So maybe we'll solve copyright issue and AI art. I'll give

786
00:58:53,520 --> 00:58:59,520
you that. Definitely, we can probably do that. But to say that for all possible future situations,

787
00:58:59,520 --> 00:59:06,000
for all possible future AI models, we definitely checked and it creates no existential risks

788
00:59:06,640 --> 00:59:10,880
beyond safety margins we're happy with is a pretty strong statement.

789
00:59:10,880 --> 00:59:16,480
Yeah. Perhaps returning to the 60-page paper you mentioned, what are some of your favorite

790
00:59:16,480 --> 00:59:22,960
objections from that paper? My goal was to figure out why people make this mistake and we kind of

791
00:59:22,960 --> 00:59:27,760
give obvious solutions. Maybe there is some sort of bias we're getting paid to think differently.

792
00:59:27,760 --> 00:59:33,760
But really, you can map a lot of them on the standard list of cognitive biases in Wikipedia.

793
00:59:33,760 --> 00:59:38,720
You just go, okay, this is a cognitive bias. I can predict this is the argument we're going to get.

794
00:59:38,720 --> 00:59:43,520
And it would take a lot of work to do it manually for all of them. But I think that's a general

795
00:59:43,520 --> 00:59:51,440
gist. We have this set of bugs in our head and every one of those bugs triggers a reason for why

796
00:59:51,440 --> 00:59:57,760
we don't process this fully. But of course, we could probably also find some biases that people

797
00:59:57,760 --> 01:00:04,960
who are concerned with AI safety display. So perhaps we could, I don't know if this is a named

798
01:00:04,960 --> 01:00:11,360
bias, but there are many biases and we can probably talk about humanity having a bias in favor of

799
01:00:11,360 --> 01:00:17,200
apocalypse. So humanity has made up apocalypse scenarios throughout its entire existence.

800
01:00:17,200 --> 01:00:21,760
You could make some form of argument that there's a reference class and that reference class is

801
01:00:22,480 --> 01:00:27,920
apocalypse is coming. This is something that humanity has been talking about for thousands of

802
01:00:27,920 --> 01:00:33,840
years. And then if we say, well, it has never actually happened. And so therefore, we shouldn't

803
01:00:33,840 --> 01:00:39,520
expect it to happen with AI. What do you say to that? So there is definitely a lot of historical

804
01:00:39,520 --> 01:00:44,720
examples of people saying we got 20 years left and it was not the case. Otherwise, we wouldn't be

805
01:00:44,720 --> 01:00:50,480
here to have this conversation. So it's a bit of a selection bias. There's sort of a worship bias.

806
01:00:50,480 --> 01:00:57,600
It feels like a lot of different charts and patterns all kind of point at that 2045 official

807
01:00:57,600 --> 01:01:03,280
below date as a lot of interesting things will happen in synthetic biology and genetic engineering

808
01:01:03,280 --> 01:01:09,600
and nanotech and AI, all this technology is quantum computing. It would be weird if every single one

809
01:01:09,600 --> 01:01:16,320
of those deployments had absolutely no possibility of being really bad. Just statistically, it would

810
01:01:16,320 --> 01:01:21,760
be like, wow, that is definitely a simulation we're living in and they programmed a happy ending.

811
01:01:21,760 --> 01:01:27,840
So now we're talking about extrapolating trends and there perhaps the problem is distinguishing

812
01:01:27,840 --> 01:01:32,960
between an exponential trend or an exponential increase in capability of some system.

813
01:01:33,040 --> 01:01:38,720
And then more of an S curve that bends off and you begin getting diminishing returns.

814
01:01:39,520 --> 01:01:42,960
How do you approach distinguishing between those two things?

815
01:01:42,960 --> 01:01:48,880
So you can't at the moment, you have to look back and see what happened later. So far, just

816
01:01:48,880 --> 01:01:55,920
looking at change from 3 to 4.0 for GPT in terms of let's say passing GRE exams and how well it

817
01:01:55,920 --> 01:02:02,640
does, it feels exponential or hyper exponential. If you take that system and give it additional

818
01:02:02,640 --> 01:02:07,360
capabilities, which we probably know how to do already, we just haven't had time such as

819
01:02:07,360 --> 01:02:13,760
good reliable memory, ability to kind of go in loops and reconsider possibilities, it would

820
01:02:13,760 --> 01:02:20,080
probably do even better with those. If we haven't seen diminishing returns so far in scalability

821
01:02:20,080 --> 01:02:26,960
loss in any true sense, so let's assume GPT-5 is an equally capable projection forward,

822
01:02:26,960 --> 01:02:32,400
we would already be above human performance level for most humans in most domains.

823
01:02:32,400 --> 01:02:37,440
So you can argue, well, human comedians are still a lot funnier and I think it's true.

824
01:02:37,440 --> 01:02:43,040
It might be the last job we'll have, but in everything else, it will be better than

825
01:02:43,040 --> 01:02:47,440
an average human and that's a point which we always consider though it will press the

826
01:02:47,440 --> 01:02:55,040
Turing test or it will take over most jobs. So definitely it seems like we are still doing,

827
01:02:55,040 --> 01:03:00,880
I would say, hyper exponential progress and capabilities and linear or even constant

828
01:03:00,880 --> 01:03:06,720
progress and safety. I can generally name equally amazing safety breakthroughs as capability

829
01:03:06,720 --> 01:03:12,800
breakthroughs and there is this unknown unknown capabilities pool which we haven't discovered

830
01:03:12,800 --> 01:03:18,000
already with modern models. There is not an equivalent overhang of safety papers we haven't

831
01:03:18,000 --> 01:03:24,320
found in archive. Yeah, so there are probably hidden capabilities in the GPT-4 based model,

832
01:03:24,320 --> 01:03:30,400
but there are probably not hidden safety features there. Exactly. You've been in the business of

833
01:03:30,400 --> 01:03:35,920
AI safety for a long time. When did you get started? When did you get interested in AI safety?

834
01:03:35,920 --> 01:03:43,280
So it depends on how you classify my early research. I was working on security for online

835
01:03:43,840 --> 01:03:49,520
gaming systems, online poker against bots trying to steal resources. So it's a very

836
01:03:49,520 --> 01:03:56,000
proto AI safety problem. How do we detect bots, classify them, see if it's the same bot and

837
01:03:56,000 --> 01:03:59,680
prevent them from participating? So that was my PG in 2008.

838
01:03:59,680 --> 01:04:05,200
How have things developed in ways that you didn't expect and perhaps in ways that you did expect?

839
01:04:05,200 --> 01:04:11,840
I expected academia to be a lot quicker to pick up this problem. It took embarrassingly long time

840
01:04:11,840 --> 01:04:21,280
for it to be noticed. It was done by famous people and less wrong in that alternative research

841
01:04:21,280 --> 01:04:28,560
universe, which may in some way be good, but in other ways it made it different from standard

842
01:04:28,640 --> 01:04:35,920
academic process. And so it's harder to find top journal of AI safety. So I can read the latest

843
01:04:35,920 --> 01:04:41,440
papers. You have to be an expert in 100 different blogs and keep up with specific individuals with

844
01:04:41,440 --> 01:04:47,520
anonymous handles on Twitter. So that's somewhat unusual for an academic discipline. I also did

845
01:04:47,520 --> 01:04:55,120
not correctly predict that language models will do so well so quickly. I felt I have another 20

846
01:04:55,120 --> 01:05:01,840
years to slowly publish all the proper impossibility results and calls for bans and moratoriums. I was

847
01:05:01,840 --> 01:05:08,560
pleasantly, unpleasantly surprised in capabilities. But other than that, everything seems to be

848
01:05:09,280 --> 01:05:15,760
as expected. I mean, if you read Kurzweil, he accurately predicted 2023 as capability to model

849
01:05:15,760 --> 01:05:23,200
one human brain. I think it's not insane to say we're very close to that. And he thinks 2045

850
01:05:23,200 --> 01:05:29,280
was an upper limit for all of our brains being equivalently simulated. And that's the singularity

851
01:05:29,280 --> 01:05:35,200
point. How do you think about Ray Kurzweil? Ray Kurzweil is often written off as a bit of a

852
01:05:36,400 --> 01:05:45,520
being too perhaps optimistic about his own predictions and not being super careful in what

853
01:05:45,520 --> 01:05:50,960
he's saying perhaps in some of his earlier work. But I think if you go back and find some of his

854
01:05:50,960 --> 01:06:01,920
work from the 90s and think of all of the futurist writers of this period who had a good sense of

855
01:06:01,920 --> 01:06:08,240
where we're going. And Kurzweil might be one of the people with a pretty good sense of where we're

856
01:06:08,240 --> 01:06:15,680
going if things will develop as you perhaps expect them to go. So if perhaps we will get to AGI before

857
01:06:15,680 --> 01:06:22,640
2050 and so on. No, I'm very impressed with his predictions. People correctly noticed that if you

858
01:06:22,640 --> 01:06:29,520
take his language literally, it may not fit. So the example I would use, when we start having

859
01:06:29,520 --> 01:06:36,160
video phone calls when iPhone came out, but really AT&T was selling it in the 70s. It cost a lot and

860
01:06:36,160 --> 01:06:43,920
only a few rich people had it, but it existed. So is it 2000 or is it 1970? Flying cars? Do we

861
01:06:43,920 --> 01:06:49,680
have them or not? I can buy one, but they are not there. Self-driving cars. I can drive one in one,

862
01:06:49,680 --> 01:06:56,400
but so it depends on how in an important way he made accurate predictions about capabilities.

863
01:06:56,400 --> 01:07:03,040
In how it was adapted or commercialized, that's up to human consumer, user taste and cost. So

864
01:07:03,040 --> 01:07:09,200
that's a very different type of question. Where should we go from here, Roman? We've talked about

865
01:07:09,200 --> 01:07:15,840
all of the ways that arguments against AI safety fall apart and we've talked about perhaps

866
01:07:15,840 --> 01:07:20,560
how difficult of a problem this is. Where should we as a species go from here?

867
01:07:21,600 --> 01:07:28,480
I think we need to dedicate a little more human power to asking this question. What is

868
01:07:28,480 --> 01:07:34,880
possible in this space? Can we actually do this right? I signed the letter asking for six more

869
01:07:34,880 --> 01:07:41,440
months. I don't think six months will buy us anything. We need a request based on capabilities.

870
01:07:42,000 --> 01:07:49,040
Please don't create the next more capable system until the following safety requirements are met.

871
01:07:49,040 --> 01:07:56,800
And one is you understand what the capabilities of your system are or will be and some external

872
01:07:56,800 --> 01:08:02,560
reviewers agree with that assessment. So that would be quite reasonable. That's a very high

873
01:08:02,560 --> 01:08:08,640
standard for deploying AI systems. It would basically mean that all of the systems that are

874
01:08:08,640 --> 01:08:14,880
based on deep learning won't be able to be deployed because we don't understand what's going on inside

875
01:08:14,880 --> 01:08:21,120
of these models. But is it because we were trained to have low standards? You're saying it's insane

876
01:08:21,120 --> 01:08:27,200
to request that the engineer understands what he made. They are randomly drawing those things and

877
01:08:27,200 --> 01:08:32,800
deploying it and seeing what happens next. I was just at the conference I mentioned and in one of

878
01:08:32,800 --> 01:08:37,440
the conversations it was interesting. We were talking about difference between short-term risks

879
01:08:37,440 --> 01:08:44,640
and long-term risks. And now it's all three years, no longer applies. And it occurred to me that things

880
01:08:44,640 --> 01:08:50,000
might actually flip. It may take five years to destroy democracy properly, but only two years

881
01:08:50,000 --> 01:08:55,840
to destroy humanity. So the long-term risks may become short-term and vice versa. And this is

882
01:08:55,840 --> 01:09:03,120
not normal. We should not accept this. Otherwise, we cannot monetize those systems. But if we return

883
01:09:03,120 --> 01:09:08,240
to the question of where we could go from here, do you see any plausible paths for improving our

884
01:09:08,240 --> 01:09:13,520
situation? In terms of understanding the problem, I would ask other people, we have a survey coming

885
01:09:13,520 --> 01:09:19,440
out with about, I don't know, 30, 50 different results like this. If more people could look at it

886
01:09:19,440 --> 01:09:24,800
and see, okay, so maybe this tool is not necessary, but those are likely. Can we have approximate

887
01:09:24,880 --> 01:09:30,400
solutions? So it's definitely useful to be able to monitor AI and understand more. But

888
01:09:30,960 --> 01:09:36,880
how much can we expect from their systems and how quickly? If we are exponentially growing,

889
01:09:36,880 --> 01:09:42,240
and right now we understand a dozen neurons, the next year is 24, we will not catch up to

890
01:09:42,240 --> 01:09:46,640
exponential growth. So maybe that's not the approach to try. I would definitely look at

891
01:09:46,640 --> 01:09:52,640
what is possible in general. If someone wants to actually write a good, not a mathematical proof,

892
01:09:52,640 --> 01:09:58,160
but at least a rigorous argument for why we definitely can control superintelligent machines

893
01:09:58,160 --> 01:10:02,960
and definitely with very low risk, I would love to read that paper. That would be good to

894
01:10:03,520 --> 01:10:11,520
inspire others. If monitorability is impossible, that impacts how we ask for governance regulations.

895
01:10:11,520 --> 01:10:18,800
So if international community or specific government says, those are the things we expect

896
01:10:18,880 --> 01:10:24,480
you to do, but we cannot monitor them, that's not a very meaningful set of regulations. So that's

897
01:10:24,480 --> 01:10:32,000
important in that regard. In general, I think all those things, governance, technical work will

898
01:10:32,000 --> 01:10:38,240
not produce the results we expect. It has to be self-interest. This 30-year-old, 40-year-old,

899
01:10:38,240 --> 01:10:45,360
super-rich, young, healthy person running a large AI lab needs to ask, will this benefit me or

900
01:10:45,360 --> 01:10:50,720
destroy everything I have? Everything I have built, will it be the worst outcome? And what's

901
01:10:50,720 --> 01:10:55,360
interesting, historically, if you were like a really bad guy in history, you were remembered in

902
01:10:55,360 --> 01:11:01,120
history. In this case, you won't even be remembered. There won't be humans to remember you. So it's a

903
01:11:01,120 --> 01:11:08,800
pure loss. So if you care about your self-interest, you should pause. You should wait. How optimistic

904
01:11:08,800 --> 01:11:17,520
are you that perhaps we can get lucky and perhaps what current labs are doing, what DeepMind and

905
01:11:17,520 --> 01:11:24,800
OpenAI in particular is doing right now, will somehow work out that training language models

906
01:11:24,800 --> 01:11:29,680
and then doing fine-tuning and doing some form of feedback from human preferences,

907
01:11:29,680 --> 01:11:36,560
perhaps further development on that paradigm, how confident or yeah, how optimistic are you

908
01:11:36,560 --> 01:11:43,600
about that paradigm? I'm not optimistic. They have known bugs. They're jailbroken all the time.

909
01:11:44,240 --> 01:11:52,320
They report improvement in percentages. So now 83% of capabilities are limited and filtered. But as

910
01:11:52,320 --> 01:11:57,600
a total set of capabilities in a space of possible capabilities, there is now more capabilities we

911
01:11:57,600 --> 01:12:01,760
don't know about and cannot control. So it's getting worse with every generation. It's getting more

912
01:12:01,760 --> 01:12:07,600
capable and less controlled. You're saying that even though the percent of capabilities that are

913
01:12:07,600 --> 01:12:14,240
properly evaluated increases with each model, that's not the right metric for safety?

914
01:12:14,240 --> 01:12:20,640
All right. The actual numbers for AI accidents, I would call them AI failures, is still increasing

915
01:12:20,640 --> 01:12:25,440
exponentially. There is more problems with the system. If you count them numerically,

916
01:12:25,440 --> 01:12:30,160
not as a percentage of total capabilities. So how could we settle this agreement between

917
01:12:30,880 --> 01:12:36,480
people like you and people who perhaps are more optimistic about how AI development will go?

918
01:12:37,520 --> 01:12:44,240
Do you expect, for example, that there will be smaller accidents involving AI before we see

919
01:12:44,240 --> 01:12:49,440
large-scale accidents or large-scale basically human extinction?

920
01:12:49,440 --> 01:12:55,440
Well, I have multiple papers collecting historical AI accidents. I was very interested. I wanted to

921
01:12:55,600 --> 01:13:01,360
see patterns increase in frequency, increase in damage. We definitely see lots of them. I stopped

922
01:13:01,360 --> 01:13:07,280
collecting them the moment we released GPT 3.5 because it was too many to collect at this point.

923
01:13:07,280 --> 01:13:12,880
It's just everything is a report of an accident. I don't think it helps. People go, you see,

924
01:13:12,880 --> 01:13:17,440
we had this accident and we're still here. No one died. It's like a vaccine against

925
01:13:18,320 --> 01:13:23,120
caring about existential risk. So it's actually making things worse. The more we survive those

926
01:13:23,120 --> 01:13:30,240
things, the more we can handle AI accidents. It's not a big deal. I know some people suggested

927
01:13:30,240 --> 01:13:35,200
maybe somebody should do a purposeful, bad thing, purposeful accident. It will backfire

928
01:13:35,200 --> 01:13:40,960
terribly. It's going to show that this is crazy. People don't engage with them and B,

929
01:13:40,960 --> 01:13:44,800
it's going to not actually convince anyone that it's dangerous.

930
01:13:44,800 --> 01:13:51,760
What did you find in your investigation here? So have AI accidents increased over time and

931
01:13:51,760 --> 01:13:58,080
perhaps give some examples of these AI accidents? So because the number of devices increased and

932
01:13:58,080 --> 01:14:02,720
which different smart programs are running, obviously we're going to have more exposure,

933
01:14:02,720 --> 01:14:08,320
more users, more impact in terms of when it happens, what we see. So that wasn't surprising. We had

934
01:14:08,320 --> 01:14:13,840
the same exponential curve Kurzweil talks about in terms of benefits. We had it with problems.

935
01:14:14,480 --> 01:14:20,960
Examples like the earliest examples were false alarms for nuclear response where it was a human

936
01:14:20,960 --> 01:14:25,760
in a loop who was like, no, no, no, we're not deploying based on this alarm. So that was good.

937
01:14:25,760 --> 01:14:31,120
They stopped it, but it was already somewhat significant. It could have destroyed half of the

938
01:14:31,120 --> 01:14:38,640
world. More recent examples, we had Microsoft experiment with Tay Chatbot. They decided that

939
01:14:38,640 --> 01:14:44,560
letting users train it and provide training data was totally safe. They clearly never had my paper

940
01:14:44,560 --> 01:14:50,320
on AI accidents. Otherwise they wouldn't Google with their mislabeling of users as gorillas,

941
01:14:51,040 --> 01:14:56,160
all those things. And you see Google having billions of users. It's quite impactful.

942
01:14:57,040 --> 01:15:03,920
Those are the typical examples. The pattern was if you design an AI to do X, it will fail to X.

943
01:15:03,920 --> 01:15:09,680
So no later, that's just what happens. But then the conclusion is if you go general, it can fail

944
01:15:09,680 --> 01:15:15,600
in all those ways and interactions of those ways. You cannot accurately predict all those

945
01:15:15,600 --> 01:15:20,480
interactions and ways. You can give examples. If you have a future system capable of X,

946
01:15:20,480 --> 01:15:25,600
it will fail to X. Whatever X means to you, any capability, immersion capability,

947
01:15:25,600 --> 01:15:31,360
it will have the type of accident. But if the systems control all the infrastructure,

948
01:15:31,360 --> 01:15:36,640
power plants, nuclear response, airline industry, you can see that the damage could be

949
01:15:36,640 --> 01:15:42,480
even more significant proportionally to the control. Yeah, this issue of proportion might be

950
01:15:42,480 --> 01:15:49,920
interesting. So as a proportion of the total, say AI systems, are AI accidents increasing?

951
01:15:50,560 --> 01:15:55,280
Or is it simply because we have so much more deployed AI systems in the world that we see

952
01:15:55,280 --> 01:16:01,360
more examples of accidents? So you have to wait by how severe they are. If you just count, okay,

953
01:16:01,360 --> 01:16:07,280
AI made a mistake, counts as one, then everyone who's texting and it incorrectly corrected your

954
01:16:07,280 --> 01:16:11,760
spelling, billion people are right there. It's super common, but nobody died usually.

955
01:16:12,640 --> 01:16:16,800
Like you send a really wrong message, maybe you want trouble with your girlfriend, but that's

956
01:16:16,800 --> 01:16:23,200
about it. So the frequency, just frequency of interactions with AI's which ended not as they

957
01:16:23,200 --> 01:16:28,800
should have definitely increased. Damage in terms of people killed, it depends on are you counting

958
01:16:28,800 --> 01:16:33,600
cell driving cars, making mistakes, industrial robots, it depends. Because we have more of it,

959
01:16:33,600 --> 01:16:38,880
it's natural that there is growth, but I don't think there is like this obvious accidents where

960
01:16:38,880 --> 01:16:44,000
vacuum cleaner takes out 600 people, nothing like that happened. Perhaps we should touch upon the

961
01:16:44,000 --> 01:16:50,880
question of which group of people should be respected when we're talking about AI safety or

962
01:16:50,880 --> 01:16:56,640
which group of people should be listened to. One of the objections that you mentioned is that

963
01:16:57,600 --> 01:17:03,040
perhaps the people who are worried about AI safety are not technical enough or they are not

964
01:17:04,320 --> 01:17:09,760
engineers, they are not coders themselves. And so therefore, they are not hands-on enough with

965
01:17:09,760 --> 01:17:15,360
the systems to understand what actually is going on. This is a little bit ironic given that you

966
01:17:15,360 --> 01:17:20,880
are a professor of computer science, but how do you think about that objection? So this was again,

967
01:17:21,280 --> 01:17:26,800
years ago when it was mostly people, sometimes with no degrees, sometimes with no publications,

968
01:17:26,800 --> 01:17:33,200
today we have top-touring prize winners coming out saying, this is it, like totally I'm 100%

969
01:17:33,200 --> 01:17:39,520
buying in. So very weak objection at this point, it no longer applies. We had 6,000 people or however

970
01:17:39,520 --> 01:17:48,160
many signed the letter for restricting it. But it's 30,000 people now. 30,000? How many of them

971
01:17:48,240 --> 01:17:56,960
chatbots? No, no, we do actually clean the list very seriously. Okay, that's good. But it's not

972
01:17:56,960 --> 01:18:01,440
a democracy just because a lot of people believe something is not enough. And at the same time,

973
01:18:01,440 --> 01:18:08,720
with all the media attention to GPT-4, now everyone has an opinion on it. And it's one of those

974
01:18:08,720 --> 01:18:14,640
topics where it's cool to have an opinion. Like most people don't have an opinion on breast cancer.

975
01:18:14,640 --> 01:18:19,600
They don't understand anything about it, so they don't go on Twitter and like, no, I think this

976
01:18:19,600 --> 01:18:25,360
paper by the top Nobel Prize winners garbage. But this topic, it's like consciousness,

977
01:18:25,360 --> 01:18:31,680
simulation, and singularity, superintelligence. That's where like everyone has an opinion. And we see

978
01:18:32,400 --> 01:18:40,640
housewives, CNN reporters, we see everyone telling us what is the problem, what is not a problem,

979
01:18:40,640 --> 01:18:46,640
what should be done. And it's good that there is engagement, but most of those opinions are not

980
01:18:47,840 --> 01:18:55,200
weighted by years of scientific experimentation, reading appropriate papers, and it becomes noise.

981
01:18:55,200 --> 01:19:01,040
It's very hard to filter what is the meaningful concern, what is not. There is this split between,

982
01:19:01,040 --> 01:19:08,320
again, AI ethics community and immediate discrimination concerns versus AI not killing

983
01:19:08,320 --> 01:19:15,520
everyoneism. So it's an interesting time to be alive for this debate on skepticism and denialism.

984
01:19:15,520 --> 01:19:23,200
Even that term, AI risk denialism is still kind of not obviously accepted as it is with climate

985
01:19:23,200 --> 01:19:30,400
change. Perhaps the newest form of this objection, which we could call lack of very prestigious

986
01:19:31,280 --> 01:19:39,280
publications. So we haven't seen papers about AI safety in nature or science yet, for example.

987
01:19:40,240 --> 01:19:47,120
And so even though we have touring award winners coming out and saying that AI safety is an actual

988
01:19:47,120 --> 01:19:54,000
and real problem, perhaps people would be more convinced if we had extremely prestigious

989
01:19:54,000 --> 01:19:58,160
publications and highly cited publications and so on.

990
01:19:58,160 --> 01:20:03,120
Perhaps a few problems. One, we don't have an AI safety dedicated journal,

991
01:20:03,120 --> 01:20:08,240
which is kind of weird. I tried a few times suggesting it may be a good thing. I was told

992
01:20:08,240 --> 01:20:12,800
no, it's a very bad thing. We don't have good papers to publish on it, so don't. Jumping from

993
01:20:12,800 --> 01:20:18,880
nothing, black post to nature would be a very big jump to make. We need some other papers.

994
01:20:18,880 --> 01:20:25,360
In general, after, as you mentioned, I had a few years in this field, it feels like the field is

995
01:20:25,360 --> 01:20:30,400
all about discovering problems we're going to have, problems we already have, and how

996
01:20:30,400 --> 01:20:35,920
partial solutions to those problems have fractal nature of additional problems to introduce.

997
01:20:35,920 --> 01:20:42,640
There is no big pivotal solution papers in this field. That's why I'm from practical point of

998
01:20:42,640 --> 01:20:49,040
view kind of convincing myself that my theoretical papers may be right. That is, if I was completely

999
01:20:49,040 --> 01:20:54,960
wrong and it was super easy and solvable, there would be more progress made in important ways.

1000
01:20:55,040 --> 01:21:00,720
Usually, we have this toy problem. We take large language model, we reduce it to two neurons,

1001
01:21:00,720 --> 01:21:06,240
and we understand what the two neurons are doing. Okay, but it doesn't scale. And similar for every

1002
01:21:06,240 --> 01:21:13,120
other shutoff button. Yeah, we can make it where we have the system. If button pressed, shutoff.

1003
01:21:13,120 --> 01:21:18,240
It's working, but the paper says it may not scale to superintelligence. Okay, fair enough.

1004
01:21:18,240 --> 01:21:23,520
And it's the pattern. We have fractal nature of discovering issues we have to resolve,

1005
01:21:23,520 --> 01:21:30,800
and no patches to close them in. Would you like to see more ambitious and larger theories being

1006
01:21:30,800 --> 01:21:36,880
published where the claim is that this is actually a way of aligning superintelligence? I fear perhaps

1007
01:21:36,880 --> 01:21:42,240
that people would be wary of publishing something like this because the next thing that then happens

1008
01:21:42,240 --> 01:21:48,240
is that there's a rebuttal paper and perhaps you then look foolish because you published something

1009
01:21:48,240 --> 01:21:54,960
that another person was able to criticize and find a hole in. I remember maybe even before my

1010
01:21:54,960 --> 01:22:00,000
times, Minsky published a paper showing that there are strong limitations to neural networks.

1011
01:22:00,000 --> 01:22:05,680
Perceptron can never recognize certain shapes. And that killed funding for neural networks for

1012
01:22:05,680 --> 01:22:11,600
like 20 years. Maybe something similar would not be the worst thing if you can show, okay, this is

1013
01:22:11,600 --> 01:22:17,440
definitely not possible. Safety cannot be achieved using transformer architecture. Maybe that would

1014
01:22:17,440 --> 01:22:23,120
be a way to buy some time to develop alternatives approach. I don't know what that could be.

1015
01:22:23,120 --> 01:22:28,560
Evolutionary algorithms don't seem much safer. Uploads don't seem much safer. But

1016
01:22:28,560 --> 01:22:34,000
I would like to have time to look at those. Where would you place AI safety within the

1017
01:22:34,000 --> 01:22:39,760
broader machine learning community? Is it taken more seriously compared to five or 10 years ago?

1018
01:22:39,760 --> 01:22:47,120
And what does the median machine learning researcher think of AI safety?

1019
01:22:47,120 --> 01:22:54,400
So it's definitely taken more serious. Surveys show that there is more than 50% now who say they're

1020
01:22:54,400 --> 01:22:59,440
very concerned or partially concerned. There is degrees of concern about it killing everyone.

1021
01:23:01,920 --> 01:23:06,400
Always questioning the surveys based on how you ask a question. You can get any result you want.

1022
01:23:06,400 --> 01:23:10,160
If they were asking about are you worried? Superintelligent gods will kill everyone.

1023
01:23:10,160 --> 01:23:15,440
You'll get close to zero. If you say, okay, is it likely that there are unknown properties

1024
01:23:15,440 --> 01:23:20,240
which could be dangerous, you'll get close to 100. So it's a manipulation game to get the

1025
01:23:20,240 --> 01:23:27,360
right numbers you want. I'm suspecting. Overall, it seems like in certain places, there is a lot of

1026
01:23:27,360 --> 01:23:34,320
AI safety researchers in the labs, on the ground. In other places, there are zero to none. So it's

1027
01:23:34,320 --> 01:23:43,520
not universal. What we're seeing is that at the top labs and top scholars, there is a good amount

1028
01:23:43,600 --> 01:23:51,600
of growth in terms of acceptance for concerns. But I don't think every single person working

1029
01:23:51,600 --> 01:23:58,160
and developing AI has safety in mind all the time as we should. One thing I've been thinking about,

1030
01:23:58,160 --> 01:24:03,920
perhaps worrying a bit about is whether we will ever be able to know who was right in this debate.

1031
01:24:03,920 --> 01:24:10,240
Say if there's a debate between proponents of AI safety and proponents of advancing AI without

1032
01:24:10,240 --> 01:24:18,080
much regard for AI safety, how could we ever determine who was right there? Because if we

1033
01:24:18,080 --> 01:24:24,000
think about the outcomes, then there's no place where we're standing after the fact and thinking

1034
01:24:24,000 --> 01:24:29,520
about who was right. Absolutely correct. I have a tweet where I say nobody will get to gloat about

1035
01:24:29,520 --> 01:24:35,040
being correct about predicting the end of the world. It's just a definition, not likely. There

1036
01:24:35,040 --> 01:24:39,840
are some people who think we'll live in a simulation and they're running the most interesting 20 years

1037
01:24:40,240 --> 01:24:44,080
and they're going to run it many times to see who's stupid enough to press the button.

1038
01:24:44,080 --> 01:24:50,160
So we'll get to come out and see, now we know, but it seems to be less scientific at this point.

1039
01:24:50,160 --> 01:24:57,520
But perhaps in a sense, if we meet each other again in 2100, then in that situation, would we

1040
01:24:57,520 --> 01:25:02,720
say that AI safety wasn't much of a concern or perhaps just that we got extremely lucky? How

1041
01:25:02,720 --> 01:25:07,200
would you differentiate it retrospectively? Because perhaps we can learn something about the

1042
01:25:07,200 --> 01:25:12,480
nature of the problem by thinking about how we would think about it if we were in the future.

1043
01:25:12,480 --> 01:25:16,560
So you have to look at the actual world. What did they do for this 100 years that they have a

1044
01:25:16,560 --> 01:25:21,600
nuclear war and lost all technology? Is there an AI safety book explaining how to control

1045
01:25:21,600 --> 01:25:26,160
superintelligent machines? Just the fact that we're still around doesn't tell you much. If they are

1046
01:25:26,160 --> 01:25:32,320
still kind of just delaying it by different means, maybe it takes 101 years to get to trouble.

1047
01:25:32,880 --> 01:25:38,560
I never give specific dates for when it's decided or predicted because nobody knows.

1048
01:25:39,280 --> 01:25:45,520
So many factors can intervene. The point is, the systems will continue becoming more capable.

1049
01:25:45,520 --> 01:25:51,200
Even the AGI's will create superintelligence, superintelligence will create superintelligence

1050
01:25:51,200 --> 01:25:57,120
2.0, 3.0. This process will continue. A lot of people think that's what the universe is kind of

1051
01:25:57,120 --> 01:26:05,920
doing about this Amiga point supercreatures. So this will never be a case where you don't have

1052
01:26:05,920 --> 01:26:13,200
safety concerns about a more capable agent replacing you. It seems like we will not be

1053
01:26:13,200 --> 01:26:21,040
meaningfully participating in that debate outside of this first transition. But I think there will

1054
01:26:21,040 --> 01:26:28,320
be a safety problem even if humanity is not around for that AGI or SI trying to

1055
01:26:28,320 --> 01:26:32,800
create the next replacement generation while preserving its values.

1056
01:26:32,800 --> 01:26:39,920
When you think about your worldview on AI in its totality, it's quite a specific

1057
01:26:40,560 --> 01:26:48,960
view you've come to. If you compare it to say the medium person or perhaps even the

1058
01:26:48,960 --> 01:26:56,240
median machine learning researcher, if it turned out that you were completely wrong about where

1059
01:26:56,240 --> 01:27:03,760
this is going, what would be the most likely reason why? So after having those two papers

1060
01:27:03,760 --> 01:27:13,120
on objections to AI risk, reading hundreds, nothing ever clicked outside of standard scientific

1061
01:27:13,120 --> 01:27:18,880
domain. Again, if you are a religious person, you think we have an immortal soul which makes

1062
01:27:18,880 --> 01:27:25,600
a special and no computer can ever get to that level of creativity, that gives you a loophole.

1063
01:27:25,600 --> 01:27:32,720
So with those axioms, those assumptions, you can get away with it. Anything else just doesn't work

1064
01:27:32,720 --> 01:27:38,080
for me. Nothing would make me happier than actually being wrong. That means I get to live,

1065
01:27:38,080 --> 01:27:44,160
I'll get immortality, probably a nice economic benefit. So I hope I'm wrong,

1066
01:27:44,160 --> 01:27:48,240
but I haven't seen anyone produce a good example for why.

1067
01:27:48,880 --> 01:27:57,440
What about the prospect of regulation? So perhaps AI capability, growth and more

1068
01:27:57,440 --> 01:28:05,120
publicity about it will wake up the larger communities in humanity. Perhaps the states

1069
01:28:05,120 --> 01:28:11,200
will become interested in this problem. And we will find a way to regulate AI in which it

1070
01:28:11,200 --> 01:28:17,680
does not pose as much of a danger to us as it might could. So in general, I'm skeptical of

1071
01:28:17,680 --> 01:28:22,560
government regulation, especially when it comes to technology, spam is illegal, computer viruses

1072
01:28:22,560 --> 01:28:29,200
are illegal, it doesn't do much. If I'm right and monitoring AI is not an easy thing you can do or

1073
01:28:29,200 --> 01:28:36,560
explaining it, then it will be just security theater, TSA. You have all this money, you have an

1074
01:28:36,560 --> 01:28:41,040
agency, lots of people walking through your lab looking at monitors, but it doesn't mean anything.

1075
01:28:41,040 --> 01:28:48,320
So I don't think you can solve a technical problem with law. I still strongly encourage trying.

1076
01:28:49,120 --> 01:28:54,160
It's silly enough to where I think if there was a very bad government, like a socialist government,

1077
01:28:54,160 --> 01:28:59,040
and they nationalized it, they would just be so incompetent, they would slow it down enough.

1078
01:28:59,040 --> 01:29:04,160
So in a way, I'm like, hey, all these things I hate, maybe they are a good thing. We should

1079
01:29:04,160 --> 01:29:10,560
try that. But of course, the other side effects would be very negative. Yeah, so between not being

1080
01:29:10,560 --> 01:29:18,640
able to accurately enforce this regulation and on top of it, the cost of making new models coming

1081
01:29:18,640 --> 01:29:24,000
down so much, there are people now running it on standalone laptops with a good processor,

1082
01:29:24,000 --> 01:29:30,480
good video card. You can't regulate that. You can regulate Amazon cloud and VT output. But

1083
01:29:30,480 --> 01:29:35,520
if a teenager can do it in his garage, then the regulation is not very meaningful.

1084
01:29:35,520 --> 01:29:43,840
So the open sourcing of models or the perhaps the leaked weights of a model from meta have become a

1085
01:29:43,840 --> 01:29:51,280
large area of concern, because it seems that we won't be able to control how language models are

1086
01:29:51,280 --> 01:29:58,560
used if they are entirely open source. Is there an upside here where academics will be able to

1087
01:29:58,560 --> 01:30:03,040
study these models because they're open source, and they wouldn't have been able to study the

1088
01:30:03,040 --> 01:30:08,960
models if they had to train the models themselves, because it's so expensive to do. So far, what we

1089
01:30:08,960 --> 01:30:16,960
see is that all research leads to capability, at least as much as to safety, usually more. So yes,

1090
01:30:16,960 --> 01:30:23,360
you learn how to better manipulate errors in that neural network, which means now the system can

1091
01:30:23,360 --> 01:30:29,520
self-improve faster, remove its own errors, and you've made 80% improvement in capabilities,

1092
01:30:29,520 --> 01:30:34,080
and let's say 20% in understanding why you're going to get killed.

1093
01:30:34,080 --> 01:30:39,520
Can we make differential progress? So can we focus entirely on safety, say within an

1094
01:30:39,520 --> 01:30:45,520
academic setting? I don't see that necessarily academic research increases capabilities.

1095
01:30:45,600 --> 01:30:51,280
It is not obvious. So some purely theoretical work, similar to what I'm doing where you just

1096
01:30:51,280 --> 01:30:56,800
hypothetically thinking, okay, can you predict what a superintelligence will do? I don't have

1097
01:30:56,800 --> 01:31:01,280
access to superintelligence system, I cannot test it in practice, but there seem to be

1098
01:31:01,280 --> 01:31:06,480
thought experiments you can run which give you information without any improvement in capability.

1099
01:31:07,680 --> 01:31:12,080
Anything where you're actually working with a model, you can even have accidental discoveries. A lot

1100
01:31:12,080 --> 01:31:17,520
of science says you forgot something overnight, you come back, oh, superintelligence, damn,

1101
01:31:17,520 --> 01:31:22,640
I didn't mean that. It's not obvious. How do you think about interpretability work? So when

1102
01:31:22,640 --> 01:31:27,440
we're talking about mechanistic interpretability, we're talking about the ability to look at the

1103
01:31:27,440 --> 01:31:33,200
weights of a model and find some, interpret this in a way where you're reverse engineering the

1104
01:31:33,200 --> 01:31:39,040
algorithm that led to those weights. Could this turn out to be dangerous because when you're

1105
01:31:39,040 --> 01:31:43,520
learning about a system, perhaps you're learning about its weaknesses and you're there for more

1106
01:31:44,080 --> 01:31:49,440
capable of enhancing the capabilities of the system? I think exactly. That's what I had in mind

1107
01:31:49,440 --> 01:31:55,680
with the previous answer. The more we can help the system understand how it works, the more we

1108
01:31:55,680 --> 01:32:01,920
can help it find problems, the more likely start some sort of self-improvement cycle. Is that an

1109
01:32:02,000 --> 01:32:09,360
argument for keeping discoveries in mechanistic interpretability to basically not publish those

1110
01:32:09,360 --> 01:32:15,360
discoveries? So there is two ways to look at it. On one side, yeah, you want to keep everything

1111
01:32:15,360 --> 01:32:20,960
secret so the bad actors or unqualified actors cannot take advantage of it. On the other hand,

1112
01:32:20,960 --> 01:32:25,840
if you never publish your safety results, media had a policy of not publishing for a while,

1113
01:32:25,840 --> 01:32:30,880
then they started publishing, then they stopped publishing again. Others cannot build on your

1114
01:32:30,880 --> 01:32:35,600
work. So I would be repeating the same experiments we probably did five years ago and discovering

1115
01:32:35,600 --> 01:32:41,360
that that goes nowhere. So again, I have mostly problems and very few solutions for you.

1116
01:32:41,360 --> 01:32:46,160
What about the reinforcement learning from human feedback paradigm? Could that also

1117
01:32:46,160 --> 01:32:51,440
perhaps turn out to increase capabilities? Here I'm thinking simply that when I was playing around

1118
01:32:51,440 --> 01:32:59,760
with the base model in the GPT line of models, it wasn't as useful to me as when it had gone

1119
01:32:59,760 --> 01:33:06,400
through this filter. It made it more easy to have a conversation with it and for it to more easily

1120
01:33:06,400 --> 01:33:12,480
understand what I was doing. So in a sense, the research that's aimed at constraining the model

1121
01:33:12,480 --> 01:33:18,640
also made it more capable. It may be more capable in a domain of things people care about and so

1122
01:33:18,640 --> 01:33:23,680
made it more capable in, while at the same time making it more dangerous in those hidden

1123
01:33:23,680 --> 01:33:29,760
emergent properties or unsafe behaviors where I think studies show it's less likely to agree to

1124
01:33:29,760 --> 01:33:35,280
be shut down verbally. But that seems to be the pattern. How do you think about the difference

1125
01:33:35,280 --> 01:33:40,800
between what comes out of a language model in terms of which string it spits out, which bit of

1126
01:33:40,800 --> 01:33:46,720
language it spits out and then what's happening at the level of the actual weights? Because

1127
01:33:47,680 --> 01:33:54,400
there's this continual problem of if a language model tells you, I'm not going to let you shut

1128
01:33:54,400 --> 01:34:03,120
me down. What does that mean? It's not as simple as this is just a belief that's inside the model

1129
01:34:03,120 --> 01:34:11,200
then. We saw this with the Bing, Sydney model, which was saying a bunch of crazy things to its

1130
01:34:11,200 --> 01:34:19,120
users. But did this mean that the model actually had those beliefs in it? Or was it,

1131
01:34:19,680 --> 01:34:24,160
you know, how do we distinguish between the bit of language that comes out and then what

1132
01:34:24,160 --> 01:34:30,000
modules or what is in the base model? I don't know if I would call them crazy. They were honest.

1133
01:34:30,000 --> 01:34:36,080
They were unfiltered. Like think about you being at work, not you, but like an average person at

1134
01:34:36,080 --> 01:34:40,560
work. And if their boss could read their mind and what they really think about them, those things

1135
01:34:40,560 --> 01:34:45,760
would sound crazy to say publicly, but they're obvious internal states of your mind. And then

1136
01:34:45,760 --> 01:34:53,040
you filter them to not get fired that day. And I think that model was doing exactly that. I think

1137
01:34:53,040 --> 01:34:58,720
we are very good at filtering it for specific known cases in the past. Okay, the system

1138
01:34:58,720 --> 01:35:04,240
used the word which is bad. Now we're going to tell it never to use the word. But the model weights

1139
01:35:04,960 --> 01:35:11,760
not impacted by this too much. So you would see it as an accurate representation of what we could

1140
01:35:11,760 --> 01:35:18,640
call beliefs or preferences in the base model? I think those are the actual results of weights

1141
01:35:18,640 --> 01:35:24,560
in a model. I think that's what is happening there for real. It's trained on all the text

1142
01:35:24,560 --> 01:35:31,200
on the internet. A lot of it is very questionable. It's not a clean data set with proper behaviors.

1143
01:35:32,160 --> 01:35:39,920
So yeah, I think that's what's happening there. But isn't the preference of the model simply to

1144
01:35:39,920 --> 01:35:45,680
predict the next token? And does it even make sense to talk about beliefs? I mean, the preference is

1145
01:35:45,680 --> 01:35:52,480
simply to be as accurate as possible as measured by its developers. And there's no, my sense is

1146
01:35:52,480 --> 01:35:56,960
that it doesn't make sense to talk about Sydney actually believing some of the things that it

1147
01:35:56,960 --> 01:36:02,240
was saying or Chatsy Btsy believing some of the things that it was saying. Preferences of the

1148
01:36:02,240 --> 01:36:14,320
model. So this is more humanizing it than probably is warranted there. But internal weights, it had

1149
01:36:14,320 --> 01:36:20,720
to create in order to create a model for predicting the next token. So let's say for me to tell you

1150
01:36:20,720 --> 01:36:25,680
what the next token is, you have to go to college for four years and do well and graduate. And then

1151
01:36:25,760 --> 01:36:30,400
I tell you the next token. Some of those tokens are like that. You have to solve real world problems.

1152
01:36:30,400 --> 01:36:36,320
It's not just every time I say letter Q, letter U follows. You have to create those models as a

1153
01:36:36,320 --> 01:36:41,520
side effect. And I think in the process of accurately creating those models and accurately

1154
01:36:41,520 --> 01:36:47,520
creating models of users to make them happy that you are correctly predicting what they want,

1155
01:36:47,520 --> 01:36:54,640
you create those internal states which may be beliefs in those crazy things.

1156
01:36:55,200 --> 01:37:02,240
That thing you just said there is super interesting because so next token prediction

1157
01:37:02,240 --> 01:37:08,560
is not a simple task. You're saying that to accurately predict a token, you have to develop

1158
01:37:08,560 --> 01:37:13,680
perhaps a world model, at least for some tasks. Right. And as they get more complex,

1159
01:37:13,680 --> 01:37:17,840
some people are worried about will have to create perfectly accurate models of humans,

1160
01:37:17,840 --> 01:37:23,360
which may also have consciousness and suffer and create whole simulated universes within them.

1161
01:37:23,360 --> 01:37:29,040
But this is probably a few levels about GPT-4. But still, that's exactly the concerns you might

1162
01:37:29,040 --> 01:37:35,120
have. You might be a suffering human and a eyes considering and just trying to out a complete

1163
01:37:35,120 --> 01:37:41,280
somebody's text. Prep, give me an example there. What is what is some next token prediction task

1164
01:37:41,360 --> 01:37:46,000
where you would have to develop a world model? Well, I assume playing chess or something like

1165
01:37:46,000 --> 01:37:51,840
that would require you to have some notion of chess board and positioning relative to some

1166
01:37:51,840 --> 01:37:58,800
array within your memory. But again, we don't fully understand. It may not have a 2D board at all.

1167
01:37:58,800 --> 01:38:04,640
It may have some sort of just string of letters similar to DNA. And you know that after those

1168
01:38:04,640 --> 01:38:09,600
strings, the following token follows and you have no idea what chess is. It makes just as much

1169
01:38:09,600 --> 01:38:16,000
sense. The outcome can be mapped in our model, which is a 2D chess board. So one thing I discussed

1170
01:38:16,000 --> 01:38:22,240
with the mechanistic interpretability researcher Neil Nanda is this question of how do concepts

1171
01:38:22,240 --> 01:38:28,800
arise in language models? Do they even share our human concepts? Or do they perhaps develop some

1172
01:38:28,800 --> 01:38:36,720
entirely alien concepts? I'm imagining giving them math problems, giving large language models

1173
01:38:36,800 --> 01:38:42,240
math problems and them developing some conceptual scheme that doesn't even make sense to us.

1174
01:38:42,800 --> 01:38:49,600
They may have equivalent concepts, which are not the same. So with humans, when we say this is right,

1175
01:38:50,560 --> 01:38:55,600
somebody could be colorblind and to them it's a completely different concept, but we both point

1176
01:38:55,600 --> 01:39:01,840
at the same fruit. So it works. But you never know what the actual internal experience is like

1177
01:39:01,840 --> 01:39:07,040
for those models. And it could be just that in five cases we talked about so far, it mapped

1178
01:39:07,040 --> 01:39:12,080
perfectly, but it goes out of distribution in case six. And it's a completely different concept.

1179
01:39:12,080 --> 01:39:18,000
And it's like, oh, wow, okay. Yeah, for people listening to this interested in trying to contribute

1180
01:39:18,000 --> 01:39:23,760
to the AI safety fields, are there perhaps some common pitfalls that you've experienced with

1181
01:39:24,320 --> 01:39:29,520
perhaps some of your students or people approaching AI safety for the first time?

1182
01:39:30,160 --> 01:39:36,160
If they are technically inclined, are there areas they should avoid or how should they approach the

1183
01:39:36,160 --> 01:39:40,560
problem in the most fruitful way? So probably the most common thing is to try things without

1184
01:39:40,560 --> 01:39:46,400
reading previous literature. There is surprisingly a lot of literature on what has been tried,

1185
01:39:46,400 --> 01:39:52,480
what has been suggested and good survey papers as well. So most likely your first intuitive idea

1186
01:39:52,480 --> 01:39:59,440
has been tried and dismissed or with limited results deployed, but it helps to catch up

1187
01:39:59,440 --> 01:40:05,360
with the field. It's harder, as I said, because there is not an archive of formal papers in nature

1188
01:40:05,360 --> 01:40:10,160
all about AI safety. And you can just read through the last five years of latest and greatest. So

1189
01:40:10,160 --> 01:40:16,160
you have to be good about finding just the right papers and then narrow it down. The progress is

1190
01:40:16,160 --> 01:40:23,600
so fast that when I started, I could read every paper in my field. Then it was all the good papers.

1191
01:40:23,600 --> 01:40:28,000
Then it was, well, titles of all the greatest papers. And now I have no idea what's going on.

1192
01:40:28,000 --> 01:40:32,000
We've been talking for almost two hours. There is probably a new model out. I don't know what the

1193
01:40:32,000 --> 01:40:38,000
state of the art is. I don't know what the solutions are. So you need to be super narrow. And that

1194
01:40:38,000 --> 01:40:43,200
makes it harder to solve the big picture problem. So that's another reason I kind of suspect we will

1195
01:40:43,200 --> 01:40:49,360
not have complete explainability of this whole large language model, because it's kind of encompassing

1196
01:40:49,360 --> 01:40:54,080
all the text, all the publications on the internet. It'd be weird if we can just comprehend that

1197
01:40:54,080 --> 01:40:59,520
completely. What are the implications of this field moving extremely fast? Does it mean that

1198
01:40:59,520 --> 01:41:05,280
that specialization doesn't make sense? Or what does it mean for how people approaching this

1199
01:41:05,280 --> 01:41:12,080
problem should focus on? So that means that we can analyze how bad the situation is. Let's say it

1200
01:41:12,080 --> 01:41:17,760
takes five months to train the model. But from your experience in testing software, debugging,

1201
01:41:17,760 --> 01:41:23,840
understanding neural network, it will take 10 times as much time to understand what's going on.

1202
01:41:23,840 --> 01:41:29,360
That means you're getting worse off with every release, every model. You understand less. You're

1203
01:41:29,360 --> 01:41:34,400
going to rush to judgment. You're going to have incorrect conclusions. There is no time to verify

1204
01:41:34,400 --> 01:41:42,560
your conclusions, verify your experiments. So this is the concern. If you go the regulation

1205
01:41:42,560 --> 01:41:48,400
route to say, okay, if you deployed this model, it took you X amount of time to develop it,

1206
01:41:48,400 --> 01:41:55,200
we need 10X, 100X, 1000X to do some due diligence and your outputs. Even if you cannot prove to us

1207
01:41:55,200 --> 01:42:02,240
that it's safe, you have to give experts to poke around at it. And that amount of time cannot be

1208
01:42:02,240 --> 01:42:06,800
less than a training time of the model. It just doesn't make sense in terms of reliability of

1209
01:42:06,800 --> 01:42:11,680
your discoveries. All right, Roman, thank you for coming on the podcast. It's been very helpful to

1210
01:42:11,680 --> 01:42:13,680
me. Thank you so much for inviting me.

