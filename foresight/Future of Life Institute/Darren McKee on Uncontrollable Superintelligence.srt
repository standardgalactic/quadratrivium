1
00:00:00,000 --> 00:00:06,000
Welcome to the Future of Life Institute podcast. My name is Gus Docker and I'm here with Darren McKee.

2
00:00:06,140 --> 00:00:14,140
Darren is the author of the upcoming book Uncontrollable and he's the host of the Reality Check podcast.

3
00:00:14,340 --> 00:00:18,340
He's also been a senior policy advisor for 15 years.

4
00:00:18,400 --> 00:00:19,900
Darren, welcome to the podcast.

5
00:00:19,900 --> 00:00:22,100
Hi Gus, pleasure to be here, fan of the show.

6
00:00:22,300 --> 00:00:30,200
I had a great time reading your book and one of your goals with the book is to take a complex topic like

7
00:00:30,200 --> 00:00:35,400
machine learning, AI research and in particular alignment research and then

8
00:00:35,400 --> 00:00:39,900
present it in an accessible way. But how do you go about a task like that?

9
00:00:39,900 --> 00:00:43,900
Well, it was a bit difficult and I think clarity above all else.

10
00:00:43,900 --> 00:00:50,100
I've been following the AI developments for we'll say a decade or two, sometimes deeply, sometimes loosely

11
00:00:50,100 --> 00:00:55,100
and I would read the other popular books and some of the articles and have discussions about all these things.

12
00:00:55,100 --> 00:01:01,100
But it was really around April, May 2022 when I thought, oh, wow, things are really picking up.

13
00:01:01,100 --> 00:01:07,100
And I think a lot of people felt that when the tacky list projection for AGI dropped about 10 years,

14
00:01:07,100 --> 00:01:10,100
everyone's like, oh, oh, things are things are happening, right?

15
00:01:10,100 --> 00:01:17,100
And at that point, I thought, okay, I think there's a gap between readily available materials that reach a wider audience

16
00:01:17,100 --> 00:01:22,100
and the speed at which AI is progressing. And so I thought there's an opportunity here to write a book.

17
00:01:22,100 --> 00:01:27,100
It's not that materials didn't exist at all. There's forum posts, there's blogs, there's lots of podcasts,

18
00:01:27,100 --> 00:01:32,100
there's videos and so on. But I thought it'd be nice if it would be pulled together in a book, we'll say,

19
00:01:32,100 --> 00:01:36,100
for people with no technical background, even no science background.

20
00:01:36,100 --> 00:01:42,100
And so that's where some of my experience with the podcast or policy where you're trying to kind of do knowledge translation,

21
00:01:42,100 --> 00:01:47,100
take a complicated idea, try to phrase it simply, excessively, and relate to an audience.

22
00:01:47,100 --> 00:01:52,100
And with that as a context, the journey began, so to speak.

23
00:01:52,100 --> 00:01:57,100
So since last June 2022, I've been trying to work on this book and I'm happy that it's done.

24
00:01:57,100 --> 00:02:05,100
And I've tried to make a really concerted effort from the design of the cover to the table of contents to the chapters to how they flow.

25
00:02:05,100 --> 00:02:10,100
It really is trying to put yourself in the mind of someone who is curious about AI.

26
00:02:10,100 --> 00:02:17,100
They've seen these news headlines, they're interested, maybe concerned, confused, what the heck is going on with AI?

27
00:02:17,100 --> 00:02:21,100
And that this book is for. So it's not so much for the technical people.

28
00:02:21,100 --> 00:02:26,100
It's not for the people who've been in this AI safety debate for many years, although hopefully they'll get some value.

29
00:02:26,100 --> 00:02:32,100
It's more in a way for the people they know that they can help perhaps explain some of the issues to them.

30
00:02:33,100 --> 00:02:41,100
I've been reading about AI, I've been interested in AI for a long time and this book still provided a value to me.

31
00:02:41,100 --> 00:02:45,100
So there's value in going over the basics again, I would say.

32
00:02:45,100 --> 00:02:50,100
We're also in this conversation going to go over some of the basics again.

33
00:02:50,100 --> 00:02:57,100
And there are interesting choices about how you frame different issues, which analogies you use and so on.

34
00:02:57,100 --> 00:03:01,100
I'm interested in how do you balance accessibility with accuracy?

35
00:03:01,100 --> 00:03:04,100
So I imagine that an expert is going to read this book.

36
00:03:04,100 --> 00:03:13,100
How do you deal with that fact when you're writing for a broader audience, but you might get a nitpicky expert critiquing your book?

37
00:03:13,100 --> 00:03:16,100
I think there's just going to be some inherent trade-offs.

38
00:03:16,100 --> 00:03:21,100
The goal is to reach, as I said, as many people as possible because the experts, they already know these things.

39
00:03:21,100 --> 00:03:24,100
They already have materials that are available more readily to them.

40
00:03:24,100 --> 00:03:28,100
But for the average person, to have something really explained in a book form,

41
00:03:28,100 --> 00:03:31,100
I think this is kind of the first of its kind.

42
00:03:31,100 --> 00:03:37,100
It is entirely dedicated to the AI safety argument and tries to reach people as excessively as possible.

43
00:03:37,100 --> 00:03:43,100
My own background, science and academia, I am also inclined towards accuracy and even precision

44
00:03:43,100 --> 00:03:46,100
and trying to understand the difference between accuracy and precision,

45
00:03:46,100 --> 00:03:52,100
but also understanding that for the audience, I'm trying to reach the difference between accuracy and precision isn't what matters.

46
00:03:52,100 --> 00:03:54,100
What is the main idea?

47
00:03:54,100 --> 00:03:55,100
Is there rigor?

48
00:03:55,100 --> 00:03:58,100
Is what I'm saying generally true or understood to be true?

49
00:03:58,100 --> 00:04:00,100
Is there evidence to support it?

50
00:04:00,100 --> 00:04:01,100
Does it make sense?

51
00:04:01,100 --> 00:04:06,100
But I did try to be, we'll say, a bit more flexible by how I might phrase certain things

52
00:04:06,100 --> 00:04:10,100
and how I might use certain analogies to try to meet the audience where they are.

53
00:04:10,100 --> 00:04:13,100
As you said, this field is very, very complicated

54
00:04:13,100 --> 00:04:18,100
and you have to make concessions somewhere when you're explaining things to people.

55
00:04:18,100 --> 00:04:24,100
What did you do when in your research, you came upon a topic on which the experts disagreed?

56
00:04:24,100 --> 00:04:28,100
This is often the case in alignment research, for example.

57
00:04:28,100 --> 00:04:31,100
The experts might disagree about the basics.

58
00:04:31,100 --> 00:04:34,100
What do you do then when you're trying to explain the basics?

59
00:04:34,100 --> 00:04:37,100
I think there's a couple of options there.

60
00:04:37,100 --> 00:04:42,100
My approach was generally to try to give, I won't say that generally accepted consensus,

61
00:04:42,100 --> 00:04:46,100
because there is a consensus of sorts, but not unanimity.

62
00:04:46,100 --> 00:04:48,100
Everyone doesn't fully agree and no one ever does.

63
00:04:48,100 --> 00:04:50,100
And that's true, let's be honest, for everything.

64
00:04:50,100 --> 00:04:55,100
When you look at the surveys of what the philosophers believe, there's fundamental differences.

65
00:04:55,100 --> 00:04:59,100
Same thing with economists, same thing with physicists, same thing with pretty much every discipline.

66
00:04:59,100 --> 00:05:03,100
So understanding that, the book is trying to be a bit of a neutral observer,

67
00:05:03,100 --> 00:05:05,100
but at the same time, I have a perspective.

68
00:05:05,100 --> 00:05:07,100
I've looked at these issues.

69
00:05:07,100 --> 00:05:08,100
I am concerned.

70
00:05:08,100 --> 00:05:13,100
We're trying to figure it out together, but I'm giving reasons why I think AI safety is a concern.

71
00:05:13,100 --> 00:05:17,100
To be more specific, I kind of tried to be sympathetic to all sides,

72
00:05:17,100 --> 00:05:21,100
but at the same time, I probably didn't get too much into the weeds.

73
00:05:21,100 --> 00:05:26,100
If people thought there's no concern at all, I might mention the uncertainty around everything,

74
00:05:26,100 --> 00:05:30,100
but not so much giving that a lot of weight, because I think there are reasons to be concerned.

75
00:05:30,100 --> 00:05:35,100
To take a step back, the book, the structure to try to give people a sense of how I did it,

76
00:05:35,100 --> 00:05:40,100
is you look at the debates or discussions that are occurring in the AI risk, AI safety space,

77
00:05:40,100 --> 00:05:42,100
and what do people seem to get stuck on?

78
00:05:42,100 --> 00:05:43,100
What are their disagreements?

79
00:05:43,100 --> 00:05:44,100
What are their cruxes?

80
00:05:44,100 --> 00:05:50,100
What might be the underlying, we'll say, intellectual or even emotional dispositions

81
00:05:50,100 --> 00:05:52,100
that are leading people to have certain beliefs?

82
00:05:52,100 --> 00:05:59,100
And then identifying that, thinking about which part of the AI issue relates to it,

83
00:05:59,100 --> 00:06:05,100
and then trying to think about an analogy or a simple way to explain the thing related to AI,

84
00:06:05,100 --> 00:06:08,100
so it addresses the underlying issue in the future debate.

85
00:06:08,100 --> 00:06:12,100
It's a bit complicated, but it's kind of like working backwards and then forwards.

86
00:06:12,100 --> 00:06:18,100
So if I think one of the issues is it's really hard to imagine how powerful something like

87
00:06:18,100 --> 00:06:23,100
an advanced AI or superintelligence could be, the first chapter of the book is a lot about

88
00:06:23,100 --> 00:06:27,100
how powerful intelligence is or the importance of imagination.

89
00:06:27,100 --> 00:06:31,100
Is it really that it's a failure of imagination that's driving a lot of this?

90
00:06:31,100 --> 00:06:33,100
Well, not everything, of course, but I think it's a factor.

91
00:06:33,100 --> 00:06:39,100
So then I'm trying to address aspects of how imagination might work or just open up people's minds.

92
00:06:39,100 --> 00:06:43,100
Why don't we put it that way about what's possible before I even talk about AI?

93
00:06:43,100 --> 00:06:46,100
Because it's a general issue about trying to be open-minded about what could happen.

94
00:06:46,100 --> 00:06:52,100
And with that in place, hopefully later when there's more AI stuff, it's just less likely to be a problem.

95
00:06:52,100 --> 00:06:57,100
It is still hard to imagine something as smart as an ASI or artificial superintelligence,

96
00:06:57,100 --> 00:07:01,100
but we can try to at least acknowledge that there's something there.

97
00:07:01,100 --> 00:07:06,100
There's something to be understood or even that we don't know exactly what it could be, and that's its own value.

98
00:07:06,100 --> 00:07:13,100
So you write about how the range of intelligence extends much further than we might normally imagine.

99
00:07:13,100 --> 00:07:16,100
What institutions do you rely on here?

100
00:07:16,100 --> 00:07:18,100
How do you explain why is that?

101
00:07:18,100 --> 00:07:19,100
Why is that the case?

102
00:07:19,100 --> 00:07:22,100
Sure, I think the human brain is great.

103
00:07:22,100 --> 00:07:25,100
I like mine, even though it's flawed, I don't know what I'd do without it.

104
00:07:25,100 --> 00:07:29,100
But when we interact in the world, we have our kind of default settings.

105
00:07:29,100 --> 00:07:33,100
We know there's various biases and the availability bias and other things that when you're asked a question

106
00:07:33,100 --> 00:07:39,100
or you're quickly reading something, whatever pops in seems to be how you might think or feel about a subject.

107
00:07:39,100 --> 00:07:44,100
If given a direct question by someone, you might reflect a bit more and think a bit differently.

108
00:07:44,100 --> 00:07:46,100
But I'm trying to shift a bit out of that default.

109
00:07:46,100 --> 00:07:51,100
So when we navigate the world, we kind of think of the intelligence mostly of our friends,

110
00:07:51,100 --> 00:07:55,100
or I ask people to imagine like the smartest person they know, and they have that person in mind.

111
00:07:55,100 --> 00:07:58,100
I'm like, okay, now imagine the smartest person you think ever existed.

112
00:07:58,100 --> 00:08:03,100
Maybe it's Einstein or Mary Curie or Von Neumann or whoever it is.

113
00:08:03,100 --> 00:08:08,100
And so right away, you're like, okay, well, I had the smartest person that I know personally and the smartest person ever.

114
00:08:08,100 --> 00:08:11,100
Like, well, could there be even people who are smarter?

115
00:08:11,100 --> 00:08:14,100
And I give some examples of savants who have incredible abilities,

116
00:08:14,100 --> 00:08:20,100
whether it's memorization or processing information visually or auditorily, whatever it is.

117
00:08:20,100 --> 00:08:26,100
And like, well, look, humans already can do some amazing things, which that smartest person you know,

118
00:08:26,100 --> 00:08:29,100
and well, Von Neumann, maybe exception, can't really do.

119
00:08:29,100 --> 00:08:33,100
So it's kind of trying to show, let's think about what might be possible

120
00:08:33,100 --> 00:08:38,100
and then giving examples of what already is possible to help people understand,

121
00:08:38,100 --> 00:08:43,100
okay, if this is possible and it's already happened, could we imagine a little bit more, right?

122
00:08:43,100 --> 00:08:44,100
A little bit higher.

123
00:08:44,100 --> 00:08:47,100
I also go the other way, looking at capacities of animals.

124
00:08:47,100 --> 00:08:53,100
And we look at, you know, briefly, like in a page, it's, you know, birds and you have fish and you have dolphins and so on.

125
00:08:53,100 --> 00:08:57,100
And we know broadly humans are more intelligent than these other creatures.

126
00:08:57,100 --> 00:09:01,100
Not to say all humans, right, at all times of their lives than all the other creatures, of course,

127
00:09:01,100 --> 00:09:04,100
but broadly as a general truth, that's the case.

128
00:09:04,100 --> 00:09:10,100
And when we think about, say, why gorillas are in our zoos and we're not in theirs, it's not strength, right?

129
00:09:10,100 --> 00:09:11,100
It's not our good looks.

130
00:09:11,100 --> 00:09:12,100
It's not our charm.

131
00:09:12,100 --> 00:09:17,100
It's because, again, collectively, we have an intelligence capability that is beyond theirs.

132
00:09:17,100 --> 00:09:22,100
And that's another nuance that when I use intelligence, it's a bit more like anthropologist Joseph Henrichs,

133
00:09:22,100 --> 00:09:27,100
sort of cumulative cultural collective capital in terms of intelligence.

134
00:09:27,100 --> 00:09:33,100
It's very broad and encompassing because I thought that was probably the best way to try to communicate how important it is.

135
00:09:33,100 --> 00:09:36,100
Yeah, why do you think we forget or at least this is my experience?

136
00:09:36,100 --> 00:09:39,100
I'm intellectually aware of all of these examples you just gave.

137
00:09:39,100 --> 00:09:44,100
I know there are people that are much smarter than me and I know there are savants and so on.

138
00:09:44,100 --> 00:09:51,100
But it seems to me that when I think in normal life, I deceive myself into thinking that the range of intelligence

139
00:09:51,100 --> 00:10:00,100
ends at my height, basically, that I represent the basic range of intelligence.

140
00:10:00,100 --> 00:10:02,100
Do you think this is common?

141
00:10:02,100 --> 00:10:05,100
Why is it hard to imagine abilities that are beyond us?

142
00:10:05,100 --> 00:10:06,100
That's a great question.

143
00:10:06,100 --> 00:10:10,100
I'm not sure of detailed research, but I do think it is somewhat common.

144
00:10:10,100 --> 00:10:13,100
We kind of think we're the example, right?

145
00:10:13,100 --> 00:10:17,100
This sort of mind projection fallacy or typical mind fallacy as it's thought.

146
00:10:17,100 --> 00:10:18,100
I'm sort of the baseline.

147
00:10:18,100 --> 00:10:19,100
Things are different than me.

148
00:10:19,100 --> 00:10:21,100
Then that's how I calibrate them, right?

149
00:10:21,100 --> 00:10:26,100
If we say someone is tall or short, sometimes that's in reference to ourselves,

150
00:10:26,100 --> 00:10:30,100
but often it's sort of some general vague notion of the population.

151
00:10:30,100 --> 00:10:34,100
Of course, if you're 6'5, 6'5", that makes you very tall,

152
00:10:34,100 --> 00:10:36,100
unless you're in the NBA, then it's average.

153
00:10:36,100 --> 00:10:39,100
And so we do understand there is a frame of reference, but again,

154
00:10:39,100 --> 00:10:42,100
the default setting of human brains, again, they're great,

155
00:10:42,100 --> 00:10:46,100
but we can't be thinking in complexity, that much complexity all the time.

156
00:10:46,100 --> 00:10:48,100
Working space memory is tapped.

157
00:10:48,100 --> 00:10:53,100
So if you're just trying to, I don't know, make dinner or you want to read an article to then think of like,

158
00:10:53,100 --> 00:10:57,100
oh, there's 15 to 30 things I should always have in mind while I'm doing this.

159
00:10:57,100 --> 00:10:58,100
That's great.

160
00:10:58,100 --> 00:11:01,100
I applaud the effort and I try to have a couple of mine, but it's almost impossible to do.

161
00:11:01,100 --> 00:11:06,100
So in that sense, the book can also serve to just remind people of things that they already know.

162
00:11:06,100 --> 00:11:12,100
It's very hard to even for me to talk about like, well, do I have a 3, 350 page book memorized yet?

163
00:11:12,100 --> 00:11:16,100
Not entirely, but the ideas are more frequently in there and they're more likely to come to mind.

164
00:11:16,100 --> 00:11:21,100
So with intelligence, I think it really is just almost asking yourself more often,

165
00:11:21,100 --> 00:11:23,100
am I making the right projection?

166
00:11:23,100 --> 00:11:25,100
Is this a fair generalization?

167
00:11:25,100 --> 00:11:29,100
Am I inadvertently benchmarking to the wrong set or the wrong baseline?

168
00:11:29,100 --> 00:11:38,100
And in time, through human history is what I mean to say, humans now in various places are on average much more intelligent than they were in the past.

169
00:11:38,100 --> 00:11:43,100
And that's mainly due to education and socialization, nutrition, diet, these sorts of things.

170
00:11:43,100 --> 00:11:49,100
I think if you look approximately 100,000, 200,000 years ago, genetically humans are very similar.

171
00:11:49,100 --> 00:11:56,100
Of course, if you go, you know, a million years ago, then it's quite different, but it's not staggeringly different compared to the other species entirely.

172
00:11:56,100 --> 00:12:06,100
All that to say is that why humans are so capable now is because of our nutrition, our socialization and the fact we live in a world that does a lot of the work for us.

173
00:12:06,100 --> 00:12:09,100
As I say, like, you know, can you build a fire?

174
00:12:09,100 --> 00:12:12,100
Well, many people can and many people can't.

175
00:12:12,100 --> 00:12:15,100
And what's worse is some of us can't build a fire even with matches.

176
00:12:15,100 --> 00:12:17,100
We know what we're supposed to do, right?

177
00:12:17,100 --> 00:12:23,100
You get some kindling, you get some paper, you get some lighter, you get some thicker wood and you have the matches and you do a decent job.

178
00:12:23,100 --> 00:12:25,100
It starts and then it fizzles out.

179
00:12:25,100 --> 00:12:30,100
And it's like, oh, I can't even build fire despite knowing how to do it and starting with matches.

180
00:12:30,100 --> 00:12:31,100
Well, that's embarrassing.

181
00:12:31,100 --> 00:12:33,100
Again, this is easily rectified if you practiced.

182
00:12:33,100 --> 00:12:42,100
But I don't necessarily have to know this because there are matches and there are lighters and other people have figured this out for me so I can leverage the intelligence and the efforts of other people.

183
00:12:42,100 --> 00:12:44,100
So I don't have to worry as much.

184
00:12:44,100 --> 00:12:49,100
And perhaps similarly, it feels like your one is an individual super smart navigating through the world.

185
00:12:49,100 --> 00:12:57,100
You're like, well, that's because everything else has been done for you right now with computers and phones, most of the complexity is behind the scenes.

186
00:12:57,100 --> 00:12:58,100
And that's great for us.

187
00:12:58,100 --> 00:12:59,100
You press a button, something works.

188
00:12:59,100 --> 00:13:06,100
What actually happens, the staggering mind boggling complexity of information and ones and zeros going across space and time.

189
00:13:06,100 --> 00:13:08,100
It's easy to just not think about it in a way.

190
00:13:08,100 --> 00:13:15,100
Why would you if you're trying to watch a movie, you're not going to think of the matrix decoded, you could, but then you're going to ruin the experience for yourself.

191
00:13:15,100 --> 00:13:24,100
Do you think we are giving our collective knowledge to AIs by training them on all available data online?

192
00:13:24,100 --> 00:13:33,100
For example, are we thereby transferring the collective knowledge that we've built up that allows us to succeed in daily life?

193
00:13:33,100 --> 00:13:34,100
I think we are.

194
00:13:34,100 --> 00:13:39,100
It's almost like having a new child of sorts and you're socializing it the way you might a person.

195
00:13:39,100 --> 00:13:44,100
So let's look at all what humanity has learned and we try to pass that on to a new generation.

196
00:13:44,100 --> 00:13:56,100
And we're doing the same thing with AI just in a much more dramatic, complicated and, again, staggeringly vast way where there's, as you know, the amounts of data that

197
00:13:56,100 --> 00:14:01,100
adabytes, zettabytes, I can't remember exactly what the number is that's going into these systems or would or could go into these systems.

198
00:14:01,100 --> 00:14:03,100
I should say it's not quite that high yet.

199
00:14:03,100 --> 00:14:07,100
But why not have all the world some knowledge available?

200
00:14:07,100 --> 00:14:09,100
And it's only going to get better and better.

201
00:14:09,100 --> 00:14:12,100
I think there's vast data sources that haven't been fully tapped, right?

202
00:14:12,100 --> 00:14:19,100
So you think of all the video, which programs are now starting to mine, both you can just take the audio from it, which has its own value.

203
00:14:19,100 --> 00:14:23,100
But even the movements, how people move, what they show and how that sort of thing happens.

204
00:14:23,100 --> 00:14:30,100
That could be, you know, whether it's, you know, fixing a chair or the sink, but it's also how people navigate what they pay attention to and what they don't.

205
00:14:30,100 --> 00:14:37,100
So once all the video gets recorded and all the radio and is all these different data sources that I think it'll be even more staggering.

206
00:14:37,100 --> 00:14:43,100
So yes, we're kind of giving humanity everything that we've ever done that's in record over two AIs.

207
00:14:43,100 --> 00:14:49,100
And that will have, of course, very fantastic, wonderful things, one hopes, but also a lot of risks and concerns.

208
00:14:49,100 --> 00:14:56,100
Given our troubles with understanding other people's minds, how can we hope to understand AI systems?

209
00:14:56,100 --> 00:14:58,100
Yes, I think that's a great question.

210
00:14:58,100 --> 00:15:02,100
I am optimistic, but I think it's going to be definitely a challenge.

211
00:15:02,100 --> 00:15:06,100
I mean, there's a broader idea here is that this is what we need to work on.

212
00:15:06,100 --> 00:15:09,100
There are many things related to AI that are going to be very difficult.

213
00:15:09,100 --> 00:15:12,100
And whether we're hopeful or not, we have to realize that we have to try.

214
00:15:12,100 --> 00:15:16,100
We have to try to figure it out how they work, why they're doing what they're doing.

215
00:15:16,100 --> 00:15:20,100
And yeah, it might be complicated, but humanity has figured out the impossible before.

216
00:15:20,100 --> 00:15:22,100
That goes back to the imagination issue.

217
00:15:22,100 --> 00:15:26,100
If you look at what we kind of take for granted now that humans have gone to the moon,

218
00:15:26,100 --> 00:15:30,100
that we're having this conversation again across space and time relatively easily.

219
00:15:30,100 --> 00:15:35,100
This is not only odd or unlikely or improbable.

220
00:15:35,100 --> 00:15:38,100
This is impossible to people from the past.

221
00:15:38,100 --> 00:15:41,100
And beyond that, I actually want to argue that it's unthinkable.

222
00:15:41,100 --> 00:15:47,100
If you go far enough back, they didn't have the conceptual apparatus to even consider how difficult this could be.

223
00:15:47,100 --> 00:15:51,100
And that's also a shift where you're thinking, okay, imagine gazing up at the moon

224
00:15:51,100 --> 00:15:54,100
as a lot of humanity has done throughout our history.

225
00:15:54,100 --> 00:15:57,100
Well, how do you get there? You can't just like climb a tree.

226
00:15:57,100 --> 00:16:04,100
You can build a tall structure, as many did, and mountain, temple, these sorts of tall ladder.

227
00:16:04,100 --> 00:16:08,100
But the idea that you could build a ship or a rocket and actually fly there,

228
00:16:08,100 --> 00:16:10,100
that wouldn't even occur to people.

229
00:16:10,100 --> 00:16:12,100
It just would have been out of their reach at the time.

230
00:16:12,100 --> 00:16:16,100
And it's within our reach, again, just because we happen to be born at this time.

231
00:16:16,100 --> 00:16:18,100
So it allows for what's possible.

232
00:16:18,100 --> 00:16:23,100
Well, with the AI at the moment, it seems very, very difficult given how these systems are,

233
00:16:23,100 --> 00:16:27,100
as people say, almost grown rather than built, to know exactly what's going on

234
00:16:27,100 --> 00:16:32,100
given the complexity of artificial neural networks and the size of the parameters and all these models.

235
00:16:32,100 --> 00:16:36,100
That said, I think there's a lot of good efforts to try to figure these things out.

236
00:16:36,100 --> 00:16:41,100
And that's exactly why we need like more investment in AI safety and more people to try to help us.

237
00:16:41,100 --> 00:16:48,100
One hang-up people have about AI safety is thinking about the goals of AI systems.

238
00:16:48,100 --> 00:16:54,100
So why is it that AI systems might develop a goals that are contrary to ours?

239
00:16:54,100 --> 00:16:58,100
Why would they suddenly turn evil? It's a question you could ask.

240
00:16:58,100 --> 00:17:02,100
There you talk about, you use the virus analogy.

241
00:17:02,100 --> 00:17:04,100
So maybe you could explain how you use that.

242
00:17:04,100 --> 00:17:08,100
So I think there's kind of a two-step process here, whether AI systems have goals or not.

243
00:17:08,100 --> 00:17:10,100
And I'll take each in turn.

244
00:17:10,100 --> 00:17:12,100
With the virus, why don't we just do that first?

245
00:17:12,100 --> 00:17:15,100
A virus doesn't have goals, but it can still harm you.

246
00:17:15,100 --> 00:17:17,100
And I think this is a great analogy.

247
00:17:17,100 --> 00:17:21,100
And you can think of the, you know, sort of biological virus, but there's also computer viruses.

248
00:17:21,100 --> 00:17:25,100
Biological viruses, depending on what they're, we know they're very hard to contain.

249
00:17:25,100 --> 00:17:29,100
They can cause pandemics. They can cause illness, common flu, these sorts of things.

250
00:17:29,100 --> 00:17:34,100
Computer viruses, people not in tech aren't aware that there's some still crawling the internet

251
00:17:34,100 --> 00:17:39,100
that were developed years and years ago that we can't really stop, but that's the default world we're in.

252
00:17:39,100 --> 00:17:41,100
So these things were created and they get out of control.

253
00:17:41,100 --> 00:17:46,100
These things, computer and biological virus, it doesn't need a goal to harm you.

254
00:17:46,100 --> 00:17:52,100
It doesn't need an intention. It's kind of just following a process in the biological sense and evolved mechanism.

255
00:17:52,100 --> 00:17:57,100
Whether viruses are live as a debate, you don't need to get into, but it's trying to achieve certain objectives.

256
00:17:57,100 --> 00:18:02,100
And you can describe it as if it has a goal, because it's easier to kind of navigate the world.

257
00:18:02,100 --> 00:18:04,100
It reduces complexity.

258
00:18:04,100 --> 00:18:07,100
But you could also argue, of course, it doesn't have goals. It's a virus. It's a little thing.

259
00:18:07,100 --> 00:18:09,100
Why would a computer virus have a goal? It doesn't.

260
00:18:09,100 --> 00:18:14,100
And in the book, I acknowledge this, but I kind of just go with the as if goals.

261
00:18:14,100 --> 00:18:19,100
Let's not get into a protracted philosophical debate about whether something has goals or not.

262
00:18:19,100 --> 00:18:24,100
If it acts as if it has goals and it's useful to treat it that way, then let's just do that.

263
00:18:24,100 --> 00:18:30,100
It's much easier. And this is, for anyone curious, this is totally Dennett's intentional stance type stuff coming in here

264
00:18:30,100 --> 00:18:32,100
because I'm a huge fan of philosopher Daniel Dennett.

265
00:18:32,100 --> 00:18:38,100
And like, what are we trying to do here? We're trying to protect ourselves from computer viruses or biological viruses.

266
00:18:38,100 --> 00:18:41,100
And you can think all the viruses trying to get you, the flu is trying to infect you.

267
00:18:41,100 --> 00:18:44,100
When someone coughs or sneezes, it's trying to spread.

268
00:18:44,100 --> 00:18:46,100
Well, of course, it's not trying to spread.

269
00:18:46,100 --> 00:18:51,100
The organism has engaged in activity, which makes it more likely to replicate.

270
00:18:51,100 --> 00:18:55,100
But to say that every time, you know, it just becomes very burdensome.

271
00:18:55,100 --> 00:19:00,100
So sentence has become paragraphs, paragraphs, many paragraphs, and that's why we sort of circumvent it.

272
00:19:00,100 --> 00:19:05,100
So that's the first bit about why it might have goals in an as if sense.

273
00:19:05,100 --> 00:19:12,100
And then there's the other part where, OK, so if something becomes a bit more autonomous, does it come to have goals?

274
00:19:12,100 --> 00:19:18,100
And I think, you know, we can look at different organisms again as an understanding that autonomy falls on a continuum.

275
00:19:18,100 --> 00:19:21,100
Does something have a goal or not? Well, in some ways, yes, in some ways, no.

276
00:19:21,100 --> 00:19:25,100
The chess playing program, does it want to win? Well, it sure acts like it wants to win.

277
00:19:25,100 --> 00:19:30,100
And when I lose, I feel like it beat me versus like some computer code and some people designed it.

278
00:19:30,100 --> 00:19:33,100
The designers definitely designed it to have a range of skills.

279
00:19:33,100 --> 00:19:35,100
What could be does do worms have goals?

280
00:19:35,100 --> 00:19:36,100
Do cats have goals?

281
00:19:36,100 --> 00:19:37,100
Do dogs have goals?

282
00:19:37,100 --> 00:19:39,100
Do orangutans have goals?

283
00:19:39,100 --> 00:19:44,100
Well, of course, in some ways, they definitely engage in goal directed behavior because they're trying to achieve a certain objective.

284
00:19:44,100 --> 00:19:54,100
And similarly with AI systems, we already have various, we'll say, automated programs that have autonomy and they need to have autonomy in some way to act in the world.

285
00:19:54,100 --> 00:19:58,100
This would be the stock trading platforms that already function all the time.

286
00:19:58,100 --> 00:20:08,100
Credit card, bank fraud detection, numerous other sort of monitoring of systems type algorithms and whatnot function on an automatic process because they couldn't keep checking with a human.

287
00:20:08,100 --> 00:20:19,100
And even in the military, typically there is a human in the loop, but we know for say missile defense, these things are automated often because there's no way a human could react quickly enough to stop incoming missiles.

288
00:20:19,100 --> 00:20:23,100
So it's not that whether computers become autonomous at all.

289
00:20:23,100 --> 00:20:26,100
It's that to what degree are they going to become more autonomous?

290
00:20:26,100 --> 00:20:28,100
And I think that is a much larger conversation.

291
00:20:28,100 --> 00:20:39,100
But at the same time, you could see why there'll be incentives to have things become more automatic, which means a bit more empowered, which will then translate to seeming like they have goals.

292
00:20:39,100 --> 00:20:45,100
And at one point, the seeming goals blends enough into the, well, that looks like it has goals that I'm just going to say it has goals.

293
00:20:45,100 --> 00:20:56,100
If we return to the virus analogy, one thought I have there is whether the existence of the common cold, for example, the common cold is not very smart.

294
00:20:56,100 --> 00:20:58,100
It's pretty dumb, I imagine.

295
00:20:58,100 --> 00:21:02,100
And it's also very harmful and costly to society.

296
00:21:02,100 --> 00:21:08,100
Does this mean that intelligence is not necessary to cause harm?

297
00:21:08,100 --> 00:21:12,100
Is this a decoupling of intelligence and power in the world?

298
00:21:12,100 --> 00:21:13,100
A great point.

299
00:21:13,100 --> 00:21:18,100
So I think, as you just said, it's not a necessary condition in that case.

300
00:21:18,100 --> 00:21:24,100
But that's different from whether intelligence is a factor that becomes correlated with the ability to cause more harm.

301
00:21:24,100 --> 00:21:33,100
So you're right that a virus will think it's pretty simple, pretty straightforward, not that many moving pieces compared to other organisms, and it can cause drastic amounts of harm.

302
00:21:33,100 --> 00:21:38,100
But you know, you're not going to go watch a movie with a virus and then talk about its thoughts after.

303
00:21:38,100 --> 00:21:39,100
It's not that type of thing.

304
00:21:39,100 --> 00:21:47,100
But as you scale up the abilities or the intelligence capabilities of different entities, they do have a different capability for harm.

305
00:21:47,100 --> 00:21:54,100
So viruses can just infect you, but something with more intelligence could design viruses to infect you, design even worse viruses.

306
00:21:54,100 --> 00:22:00,100
And even more dramatically, you know, well, if something's smart enough, it might be able to deflect an incoming asteroid and save us in a way.

307
00:22:00,100 --> 00:22:02,100
So it's not just harmful, but it's the other side to the benefits.

308
00:22:02,100 --> 00:22:10,100
So I think intelligence is a factor and in some cases it becomes a very significant factor in the ability to cause harm versus not.

309
00:22:10,100 --> 00:22:17,100
As anyone who's like taking care of a child, a very young child can cause a lot of harm, but they're relatively, relatively containable.

310
00:22:17,100 --> 00:22:22,100
If you have the right crib, if you have the right gates, they can usually be at least put in a certain place.

311
00:22:22,100 --> 00:22:26,100
That's partly due to mobility and partly due to intelligence, same thing with various other animals.

312
00:22:26,100 --> 00:22:36,100
But as you ranch it up the intelligence to sort of contain something or control it, it becomes increasingly difficult to the point where it's very, very difficult to contain some very intelligent entities.

313
00:22:36,100 --> 00:22:42,100
Now, there are lots of definitions of artificial general intelligence out there.

314
00:22:42,100 --> 00:22:45,100
And you use one in the book that I quite like.

315
00:22:45,100 --> 00:22:48,100
Maybe you could talk about the average office worker, the average remote worker.

316
00:22:48,100 --> 00:22:53,100
So artificial general intelligence or AGI, as people say, it's very much in the news, right?

317
00:22:53,100 --> 00:22:54,100
How long till AGI?

318
00:22:54,100 --> 00:22:55,100
What is AGI and so on?

319
00:22:55,100 --> 00:23:01,100
And the approach I took in the book, again, for accessibility, I'm not trying to make the definitive definition.

320
00:23:01,100 --> 00:23:05,100
I just want a good enough definition so people could understand what we're talking about.

321
00:23:05,100 --> 00:23:07,100
And I thought in the most accessible way.

322
00:23:07,100 --> 00:23:11,100
And with those factors in mind, okay, what are we talking about here?

323
00:23:11,100 --> 00:23:14,100
It's usually someone's concerned about employment primarily.

324
00:23:14,100 --> 00:23:16,100
And so I was thinking, could someone replace me at work?

325
00:23:16,100 --> 00:23:20,100
And so if you kind of take the average coworker idea, that's the foundation.

326
00:23:20,100 --> 00:23:24,100
If you're interacting with a coworker, say it's a remote coworker, right?

327
00:23:24,100 --> 00:23:26,100
It's still focused on intellectual tasks.

328
00:23:26,100 --> 00:23:31,100
So it's a computer system that, you know, does intellectual tasks as good as an average person.

329
00:23:31,100 --> 00:23:36,100
And that's really the foundation where you could imagine if someone's performing as an average coworker level,

330
00:23:36,100 --> 00:23:39,100
they're doing general tasks to a certain capability.

331
00:23:39,100 --> 00:23:46,100
And therefore you could see why the person may be, sorry, that entity may be employable or might affect employment.

332
00:23:46,100 --> 00:23:49,100
And it seemed to, I guess, as I said, hit home in the best way.

333
00:23:49,100 --> 00:23:54,100
As a nuance here, why use that terminology at all is a question I asked myself.

334
00:23:54,100 --> 00:23:57,100
Because like, well, do I need to introduce these terms to the average person?

335
00:23:57,100 --> 00:23:58,100
Why, why not?

336
00:23:58,100 --> 00:23:59,100
Well, they seem pretty popular, right?

337
00:23:59,100 --> 00:24:00,100
That's one thing.

338
00:24:00,100 --> 00:24:01,100
So you meet people where they are.

339
00:24:01,100 --> 00:24:04,100
And I knew the world was going to talk about AGI in one way or another.

340
00:24:04,100 --> 00:24:09,100
Now, is it better to say human level because it's almost the same in some ways?

341
00:24:09,100 --> 00:24:12,100
But at the same time, we use the term AI a lot, right?

342
00:24:12,100 --> 00:24:15,100
And that's sort of like, why did I use intelligence, artificial intelligence,

343
00:24:15,100 --> 00:24:18,100
artificial general intelligence, artificial superintelligence is because I thought,

344
00:24:18,100 --> 00:24:22,100
you know what, most people are going to talk about AI and people kind of know what AI is.

345
00:24:22,100 --> 00:24:27,100
So once you accept artificial intelligence as a foundation to make things as easy as possible,

346
00:24:27,100 --> 00:24:30,100
it's going down one in a way as intelligence broadly.

347
00:24:30,100 --> 00:24:33,100
And then going another level is artificial general intelligence and then superintelligence.

348
00:24:33,100 --> 00:24:37,100
And I thought that four part kind of structure was the best way to frame things.

349
00:24:37,100 --> 00:24:42,100
I imagine that you decided to write the book out of some sense of urgency.

350
00:24:42,100 --> 00:24:45,100
Now, you give a lot of uncertainty.

351
00:24:45,100 --> 00:24:49,100
You present the uncertainty around what is called AI timelines in the book.

352
00:24:49,100 --> 00:24:56,100
You must be motivated in some sense by urgency about the speed at which AI is developing.

353
00:24:56,100 --> 00:24:58,100
Do you want to give us your AI timelines?

354
00:24:58,100 --> 00:25:04,100
Do you want to tell us how long it is until we have AGI?

355
00:25:04,100 --> 00:25:06,100
How long do we have left?

356
00:25:06,100 --> 00:25:09,100
I don't necessarily think that. I want to be dramatic.

357
00:25:09,100 --> 00:25:13,100
But so I think it's a great point and I'll elaborate a bit that I don't want to head.

358
00:25:13,100 --> 00:25:20,100
So I think something like the average coworker, sure, it could be plausible within three years, two to three, four years.

359
00:25:20,100 --> 00:25:26,100
I'm being a bit vague because I almost feel like that's not what's as important as the fact that there's a lot of good data

360
00:25:26,100 --> 00:25:28,100
that shows capabilities are dramatically increasing.

361
00:25:28,100 --> 00:25:34,100
This is the basics of more investment and computational power, better design of chips, new chips coming online

362
00:25:34,100 --> 00:25:37,100
to the extent that NVIDIA's H100s are being shipped.

363
00:25:37,100 --> 00:25:42,100
So the big players, open AI and DeepMind will probably start training their next-gen models on all these things.

364
00:25:42,100 --> 00:25:47,100
And of course, a couple years after that, there's even more powerful chips that are planned by NVIDIA and all these designers.

365
00:25:47,100 --> 00:25:52,100
So the path, the most likely outcome seems to be increasing computational power,

366
00:25:52,100 --> 00:25:56,100
which then has some relationship to increased AI capabilities.

367
00:25:56,100 --> 00:26:01,100
And we don't quite know how the input matches the output, which is itself a concern.

368
00:26:01,100 --> 00:26:02,100
There's that uncertainty there.

369
00:26:02,100 --> 00:26:07,100
But the trend lines are things are going to get more powerful and things are going to get more capable.

370
00:26:07,100 --> 00:26:13,100
And because of that, it does seem like AGI or even artificial superintelligence, I think that's within 10 years.

371
00:26:13,100 --> 00:26:15,100
I think that is entirely plausible.

372
00:26:15,100 --> 00:26:18,100
Again, this is not 100% estimate, but I think it's plausible enough.

373
00:26:18,100 --> 00:26:24,100
Now, in the regular listeners of the show might be like, well, did he mean 30%, 60% or 85%?

374
00:26:24,100 --> 00:26:26,100
I understand the tendency for that.

375
00:26:26,100 --> 00:26:29,100
I think the average person is like, well, is it going to happen or is it not?

376
00:26:29,100 --> 00:26:31,100
It's like 10%, 50%, 100%.

377
00:26:31,100 --> 00:26:35,100
So it's definitely never 100 because all knowledge is probabilistic and there's uncertainty everywhere.

378
00:26:35,100 --> 00:26:40,100
But why I spent some time on the book with the uncertainty is the uncertainty is concerning.

379
00:26:40,100 --> 00:26:43,100
It's not that things get better when we say we don't know.

380
00:26:43,100 --> 00:26:46,100
I think sometimes people hear like, well, it could be two years away.

381
00:26:46,100 --> 00:26:48,100
It could be 10 years away, 20 years away.

382
00:26:48,100 --> 00:26:50,100
And they kind of just leave it at that.

383
00:26:50,100 --> 00:26:53,100
Like, wait a minute, if you don't know, it could be two years away.

384
00:26:53,100 --> 00:26:57,100
So how do we make decisions faced with uncertainty?

385
00:26:57,100 --> 00:26:58,100
Decisions have to, right?

386
00:26:58,100 --> 00:27:01,100
The world is full of complexity and uncertainty and decisions have to be made.

387
00:27:01,100 --> 00:27:06,100
And it's tempting to think decisions don't have to be made, that you can just be agnostic.

388
00:27:06,100 --> 00:27:08,100
It doesn't quite work that way.

389
00:27:08,100 --> 00:27:11,100
Now, you know, this is more just me highlighting a complexity of the world.

390
00:27:11,100 --> 00:27:13,100
Everyone can't care about every issue.

391
00:27:13,100 --> 00:27:15,100
Everyone can't read up and be knowledgeable about every issue.

392
00:27:15,100 --> 00:27:20,100
But I do want to highlight that if you happen to be in the AI space and you say we don't know,

393
00:27:20,100 --> 00:27:23,100
that's almost more concerning than someone that says it's five years away.

394
00:27:23,100 --> 00:27:27,100
Because if you actually mean we don't know tomorrow, a month from now.

395
00:27:27,100 --> 00:27:29,100
And like, oh, no, no, I don't mean that.

396
00:27:29,100 --> 00:27:31,100
I mean, you know, there's a 1% chance in the next 15 years.

397
00:27:31,100 --> 00:27:34,100
Like, oh, okay, so you do have something, right?

398
00:27:34,100 --> 00:27:38,100
I guess I'm trying to highlight that, again, the human brain, we go through life

399
00:27:38,100 --> 00:27:42,100
and we're making probability based decisions in a very loose sense.

400
00:27:42,100 --> 00:27:44,100
Whether you get a mortgage, whether you might have kids,

401
00:27:44,100 --> 00:27:46,100
whether you get a certain insurance and for how long,

402
00:27:46,100 --> 00:27:49,100
these are often based on some sense of what the future is going to be like.

403
00:27:49,100 --> 00:27:53,100
Similarly, when we're engaging with AI stuff, we are making decisions

404
00:27:53,100 --> 00:27:55,100
or not based on what the future is like.

405
00:27:55,100 --> 00:27:58,100
Now, you could say there's a disconnect between what someone says

406
00:27:58,100 --> 00:28:00,100
and what they do and behavior.

407
00:28:00,100 --> 00:28:01,100
And that's true.

408
00:28:01,100 --> 00:28:04,100
Sometimes we're dramatically inconsistent and then no human is immune from this, right?

409
00:28:04,100 --> 00:28:06,100
I won't eat junk food and then I'm eating junk food.

410
00:28:06,100 --> 00:28:07,100
What happened?

411
00:28:07,100 --> 00:28:10,100
Well, clearly there's conflicting impulses within the human organism.

412
00:28:10,100 --> 00:28:13,100
Okay, but I really just want to sort of highlight the complexity there

413
00:28:13,100 --> 00:28:17,100
and have people reflect on, okay, I've said something.

414
00:28:17,100 --> 00:28:20,100
Does my behavior match it in any way?

415
00:28:20,100 --> 00:28:22,100
And if things are that uncertain,

416
00:28:22,100 --> 00:28:25,100
isn't it better to be prepared than to be caught off guard?

417
00:28:25,100 --> 00:28:29,100
Uncertainty isn't a justification for complacency.

418
00:28:29,100 --> 00:28:31,100
It's another way to put it.

419
00:28:31,100 --> 00:28:32,100
We can do something.

420
00:28:32,100 --> 00:28:37,100
We can make plans and we should probably make plans for each timeline you mentioned.

421
00:28:37,100 --> 00:28:41,100
In the book, you discuss some cognitive traits that AI might have.

422
00:28:41,100 --> 00:28:44,100
And I think this was quite interesting.

423
00:28:44,100 --> 00:28:48,100
And starting with speed, so AIs will think much faster than we do.

424
00:28:48,100 --> 00:28:50,100
This is something that's often overlooked.

425
00:28:50,100 --> 00:28:55,100
I find in debates, what does it mean if you have human level AI,

426
00:28:55,100 --> 00:28:59,100
but it thinks a hundred or a thousand times faster than you?

427
00:28:59,100 --> 00:29:02,100
Is it still reasonable to call it human level?

428
00:29:02,100 --> 00:29:04,100
I'm happy you picked up on that.

429
00:29:04,100 --> 00:29:06,100
Yeah, so this goes to the what is an ASI?

430
00:29:06,100 --> 00:29:09,100
What kind of thing is it to try to help people along?

431
00:29:09,100 --> 00:29:13,100
We don't quite know what an ASI might be or exactly how it will be,

432
00:29:13,100 --> 00:29:17,100
but I was trying to have people understand what it might likely be.

433
00:29:17,100 --> 00:29:20,100
And so there's a couple key traits and then a couple other possible ones.

434
00:29:20,100 --> 00:29:21,100
So you have speed in there.

435
00:29:21,100 --> 00:29:22,100
We'll talk about that first.

436
00:29:22,100 --> 00:29:26,100
And there's capability, reliability and insight and these sorts of things.

437
00:29:26,100 --> 00:29:29,100
But as again, just imagine someone's never talked about AGI

438
00:29:29,100 --> 00:29:31,100
or they never thought about superintelligence like,

439
00:29:31,100 --> 00:29:33,100
what the heck are these guys talking about?

440
00:29:33,100 --> 00:29:34,100
It's a computer system.

441
00:29:34,100 --> 00:29:35,100
What's it going to look like?

442
00:29:35,100 --> 00:29:38,100
And yes, they'll probably draw on science fiction or popular movies.

443
00:29:38,100 --> 00:29:41,100
And some of those are misleading and some of those are useful.

444
00:29:41,100 --> 00:29:43,100
But it's like, what are we talking about?

445
00:29:43,100 --> 00:29:49,100
So I thought very likely AI systems, ASI systems, AGI system will function very quickly.

446
00:29:49,100 --> 00:29:51,100
And this is because computers generally work very quickly.

447
00:29:51,100 --> 00:29:53,100
That's why they're so amazing.

448
00:29:53,100 --> 00:29:57,100
Again, people listening to this, our ability to record this meeting and discussion.

449
00:29:57,100 --> 00:30:02,100
It's because so many things are happening so fast in a way that our brains can't comprehend

450
00:30:02,100 --> 00:30:04,100
that it just seems smooth, seamless and easy.

451
00:30:04,100 --> 00:30:08,100
So with an AI system, as you've seen it, if you've ever used, you know,

452
00:30:08,100 --> 00:30:11,100
one of the chatbots, the recent models or even the image generators.

453
00:30:11,100 --> 00:30:14,100
Oh, let me think for a second and then out comes the output.

454
00:30:14,100 --> 00:30:16,100
And it's usually remarkable, right?

455
00:30:16,100 --> 00:30:20,100
It happens at a speed that no human could possibly generate in that amount of time.

456
00:30:20,100 --> 00:30:26,100
And so I think there it's the flaw or the, let's say the limitation of any definition.

457
00:30:26,100 --> 00:30:28,100
There's always going to be an asterisk where like,

458
00:30:28,100 --> 00:30:31,100
well, in other contexts, it's not quite like this.

459
00:30:31,100 --> 00:30:33,100
That famous Bertrand Russell quote,

460
00:30:33,100 --> 00:30:37,100
everything is vague to a degree you do not realize until you've tried to make it precise.

461
00:30:37,100 --> 00:30:40,100
The ability is the case because any word you can find like, well,

462
00:30:40,100 --> 00:30:41,100
what does this word mean?

463
00:30:41,100 --> 00:30:44,100
And you know, so nice people spend an entire thesis or a book defining a word

464
00:30:44,100 --> 00:30:46,100
and then someone argues it's not that word and that sort of thing.

465
00:30:46,100 --> 00:30:50,100
But anyway, so with a GI, I think if we think about a general ability,

466
00:30:50,100 --> 00:30:55,100
we have to understand again that the coworker thing is useful as a sort of a test.

467
00:30:55,100 --> 00:30:56,100
It's sort of a clear bar.

468
00:30:56,100 --> 00:31:00,100
Now you could say, well, aren't there different types of coworkers and different types of environments

469
00:31:00,100 --> 00:31:03,100
and some coworkers and entire even industries or companies in different countries

470
00:31:03,100 --> 00:31:05,100
will be smarter or more capable than other ones?

471
00:31:05,100 --> 00:31:08,100
Sure, but we have to have some way of talking about this.

472
00:31:08,100 --> 00:31:12,100
Otherwise, we kind of get lost and at least I'm trying to be consistent in that way.

473
00:31:12,100 --> 00:31:17,100
So if it was the case that your average coworker could do something a thousand times faster than you,

474
00:31:17,100 --> 00:31:21,100
it doesn't quite seem like your average coworker anymore, right?

475
00:31:21,100 --> 00:31:23,100
And then it's like, well, how do we deal with this then?

476
00:31:23,100 --> 00:31:27,100
Like, well, when we examine people or give them evaluations in their workplace,

477
00:31:27,100 --> 00:31:29,100
some people have strengths that other people don't.

478
00:31:29,100 --> 00:31:33,100
And there is some sort of mishmash of like, well, are they insightful?

479
00:31:33,100 --> 00:31:34,100
Are they analytical?

480
00:31:34,100 --> 00:31:35,100
Are they on time?

481
00:31:35,100 --> 00:31:36,100
Are they, you know, friendly?

482
00:31:36,100 --> 00:31:38,100
Are they good to work with all these different things?

483
00:31:38,100 --> 00:31:40,100
And it becomes an amalgam of an evaluation.

484
00:31:40,100 --> 00:31:44,100
And similarly with AI or AGI will sort of do something similar.

485
00:31:44,100 --> 00:31:48,100
I think in the practical sense that if there's a task and it comes back one second later

486
00:31:48,100 --> 00:31:52,100
and your colleague would have taken three days, well, that's not quite human level, right?

487
00:31:52,100 --> 00:31:53,100
That's above human level.

488
00:31:53,100 --> 00:31:58,100
Now, if it has errors that the human never would have made, like, well, okay,

489
00:31:58,100 --> 00:32:01,100
that makes it maybe a bit less in human level.

490
00:32:01,100 --> 00:32:04,100
The AI gives you something, it takes you two days to fix it.

491
00:32:04,100 --> 00:32:06,100
And another process would have taken three days.

492
00:32:06,100 --> 00:32:10,100
Well, now it's above human level, but not as much as it seemed originally.

493
00:32:10,100 --> 00:32:15,100
It seems to me that we are in front on the speed aspect with AI right now.

494
00:32:15,100 --> 00:32:21,100
So, so AI is often, if we talk to GPT-4, you get an answer almost instantly.

495
00:32:21,100 --> 00:32:26,100
The answer won't be as well done as a human expert could.

496
00:32:26,100 --> 00:32:29,100
So it seems to me that we are in front on the speed aspect,

497
00:32:29,100 --> 00:32:34,100
but we are behind on the kind of depth aspect as things stand right now.

498
00:32:34,100 --> 00:32:40,100
Who knows with the upcoming models that might lead us into talking about AI insight,

499
00:32:40,100 --> 00:32:45,100
which is kind of, you can think about it as the AI's understanding of its context

500
00:32:45,100 --> 00:32:48,100
and the relationships it's engaging in.

501
00:32:48,100 --> 00:32:53,100
And you expect the AI's to have greater insight and to understand their context

502
00:32:53,100 --> 00:32:57,100
and understand their relationships perhaps even better than humans at some point.

503
00:32:57,100 --> 00:32:58,100
Why is that?

504
00:32:58,100 --> 00:32:59,100
Yes, that's a great point.

505
00:32:59,100 --> 00:33:03,100
It's also something that gives me the, not the most concern, but it is a concern.

506
00:33:03,100 --> 00:33:05,100
So we'll do like the first part first.

507
00:33:05,100 --> 00:33:09,100
As you said before, if someone is super fast,

508
00:33:09,100 --> 00:33:12,100
but they're not necessarily good at pattern detection or insight,

509
00:33:12,100 --> 00:33:16,100
they're kind of missing a lot of what makes useful product services

510
00:33:16,100 --> 00:33:18,100
or work in the broad sense.

511
00:33:18,100 --> 00:33:22,100
And right now, we already have algorithms and whatnot

512
00:33:22,100 --> 00:33:25,100
that can do sort of pattern detection in a way that a human just can't, right?

513
00:33:25,100 --> 00:33:28,100
In a way, this is how most of the recommendation algorithms,

514
00:33:28,100 --> 00:33:31,100
whether it's your Netflix or online social media,

515
00:33:31,100 --> 00:33:35,100
it's processing vast amounts of data based on what you've given it in terms of clicks,

516
00:33:35,100 --> 00:33:38,100
likes, even just moments spent looking at something

517
00:33:38,100 --> 00:33:41,100
and then spitting out something you probably like to some extent or another.

518
00:33:41,100 --> 00:33:43,100
If a human was doing this,

519
00:33:43,100 --> 00:33:46,100
oh yeah, give me three months to analyze reams of data.

520
00:33:46,100 --> 00:33:50,100
And then I think this guy might as a 70% chance of liking this one YouTube video.

521
00:33:50,100 --> 00:33:54,100
That's actually impressive for the human, but not at all useful in the real world.

522
00:33:54,100 --> 00:33:58,100
And so as they're designed right now, the systems are already,

523
00:33:58,100 --> 00:34:01,100
we'll say, super good at pattern detection.

524
00:34:01,100 --> 00:34:06,100
And there's even a story where some researchers were doing different analyses

525
00:34:06,100 --> 00:34:13,100
and an AI was able to detect a patient's race from scans like X-rays of a person's body.

526
00:34:13,100 --> 00:34:15,100
But the researchers couldn't figure it out.

527
00:34:15,100 --> 00:34:18,100
They tried to realize or understand what the AI system was doing

528
00:34:18,100 --> 00:34:20,100
because they would block certain parts of the anatomy.

529
00:34:20,100 --> 00:34:23,100
Like maybe it's looking at the heart or something in the lungs

530
00:34:23,100 --> 00:34:24,100
and they couldn't figure it out.

531
00:34:24,100 --> 00:34:28,100
Now, maybe there is an answer because there has to be some sort of thing the AI was doing,

532
00:34:28,100 --> 00:34:32,100
but it's already the case where AI systems can figure something out that humans cannot.

533
00:34:32,100 --> 00:34:35,100
And even after it's done, humans still can't.

534
00:34:35,100 --> 00:34:39,100
And I think, as you said before, the lag right now is not quite the speed or even the insight.

535
00:34:39,100 --> 00:34:43,100
It's almost the integration, the flexibility, in a way the generality

536
00:34:43,100 --> 00:34:46,100
that right now humans are very sophisticated general creatures.

537
00:34:46,100 --> 00:34:49,100
So if you give them an intellectual task, they can do a wide range of things,

538
00:34:49,100 --> 00:34:51,100
but also they do it in a certain way.

539
00:34:51,100 --> 00:34:54,100
And the world isn't yet restructured.

540
00:34:54,100 --> 00:34:58,100
Even how people talk to these machines, that has a certain nuance and a sophistication.

541
00:34:58,100 --> 00:35:02,100
Sort of like, well, the book purposely does not deal with robotics

542
00:35:02,100 --> 00:35:03,100
because I think that's a separate thing.

543
00:35:03,100 --> 00:35:04,100
It's an interesting and amazing thing.

544
00:35:04,100 --> 00:35:08,100
But an Amazon warehouse, you can't just stick a robot in there or automatic machines.

545
00:35:08,100 --> 00:35:12,100
It has to be designed in a certain way to have a true efficiency and effectiveness.

546
00:35:12,100 --> 00:35:16,100
Similarly, I think the chatbots, large language models, other AI systems in the future

547
00:35:16,100 --> 00:35:21,100
will also provide great value, but they're not quite integrated in a way that works.

548
00:35:21,100 --> 00:35:25,100
The second thing, when I said the insight concerns me a bit,

549
00:35:25,100 --> 00:35:30,100
is again, if you take a big step back, how did humans manage to do what they do, right?

550
00:35:30,100 --> 00:35:35,100
They managed to figure out how things work, like the laws of physics, how things fit together,

551
00:35:35,100 --> 00:35:37,100
putting different objects and different pieces together.

552
00:35:37,100 --> 00:35:43,100
We can sort of rearrange matter to create things, like cars, like houses, like computers.

553
00:35:43,100 --> 00:35:46,100
And this is also inspired by David Deutsch, Beginnings of Infinity.

554
00:35:46,100 --> 00:35:49,100
It's like, well, our brains just managed to figure it out.

555
00:35:49,100 --> 00:35:53,100
There's all this raw material that was inside the earth, and we were able to take it out

556
00:35:53,100 --> 00:35:57,100
and refine it and put it together, and now we have wonderful devices.

557
00:35:57,100 --> 00:36:01,100
And that was hard and difficult, and I couldn't do it, but someone figured out groups of people,

558
00:36:01,100 --> 00:36:05,100
teams of people, generations of people, and that's amazing.

559
00:36:05,100 --> 00:36:09,100
And the concern for me sometimes is, what if an AI that's super insightful

560
00:36:09,100 --> 00:36:12,100
can sort of see the universe in a way that we can't?

561
00:36:12,100 --> 00:36:15,100
And it might unlock a truly new fundamental law of physics.

562
00:36:15,100 --> 00:36:16,100
That's a possibility, though.

563
00:36:16,100 --> 00:36:20,100
But it is the case, like, well, what if you just sort of tilt the world like this,

564
00:36:20,100 --> 00:36:25,100
and like, oh, of course, of course, Wi-Fi, as a technology, it was always possible.

565
00:36:25,100 --> 00:36:27,100
We just didn't know how to do it.

566
00:36:27,100 --> 00:36:31,100
And of course, recently, people have shown you can use Wi-Fi as a way to sort of scan a room

567
00:36:31,100 --> 00:36:33,100
and create models of where people are and how they move.

568
00:36:33,100 --> 00:36:35,100
And that, of course, was also always possible.

569
00:36:35,100 --> 00:36:39,100
The laws of physics haven't changed in our lifetime, and they haven't changed as far as, you know,

570
00:36:39,100 --> 00:36:41,100
much in 13.8 billion years since the birth of the universe.

571
00:36:41,100 --> 00:36:44,100
So it's really this insight that allows you to figure things out,

572
00:36:44,100 --> 00:36:48,100
which if you understand the world and the universe more than other people,

573
00:36:48,100 --> 00:36:49,100
you can do dramatic things.

574
00:36:49,100 --> 00:36:51,100
You can create wonderful inventions.

575
00:36:51,100 --> 00:36:55,100
You also have a huge advantage in terms of how to manipulate systems

576
00:36:55,100 --> 00:36:59,100
or perhaps how to escape from systems, where someone else just didn't realize,

577
00:36:59,100 --> 00:37:03,100
oh, if you manipulate the cooling fan in a computer,

578
00:37:03,100 --> 00:37:05,100
you might be able to gain access to different things.

579
00:37:05,100 --> 00:37:06,100
Again, that's pretty remarkable.

580
00:37:06,100 --> 00:37:08,100
It was always possible until someone figured it out, though.

581
00:37:08,100 --> 00:37:09,100
We didn't know.

582
00:37:09,100 --> 00:37:13,100
Yeah, I do wonder whether AIs, advanced AIs,

583
00:37:13,100 --> 00:37:18,100
will be able to solve some of the science problems where humans have been less successful.

584
00:37:18,100 --> 00:37:23,100
So I think we've been successful in domains that are, in some sense, simple.

585
00:37:23,100 --> 00:37:27,100
Not simple to understand, but can really be reduced to something simple, like physics.

586
00:37:27,100 --> 00:37:32,100
As soon as we get into something very complex, like biology or psychology,

587
00:37:32,100 --> 00:37:34,100
we haven't had the same level of success.

588
00:37:34,100 --> 00:37:39,100
I wonder whether AIs are well suited to tasks where there's a lot of data

589
00:37:39,100 --> 00:37:43,100
and complex data and whether they might be able to make progress

590
00:37:43,100 --> 00:37:46,100
so that they have superhuman insight about our own minds.

591
00:37:46,100 --> 00:37:51,100
And you could see how that would be dangerous to be exposed to manipulation

592
00:37:51,100 --> 00:37:56,100
or otherwise being interacting with a system like that.

593
00:37:56,100 --> 00:37:57,100
I think it could also be amazing, right?

594
00:37:57,100 --> 00:37:59,100
That's how most of this stuff goes.

595
00:37:59,100 --> 00:38:00,100
It cuts both ways.

596
00:38:00,100 --> 00:38:01,100
The uncertainty cuts both ways.

597
00:38:01,100 --> 00:38:03,100
I'm just for listeners.

598
00:38:03,100 --> 00:38:05,100
I am a huge fan of science.

599
00:38:05,100 --> 00:38:06,100
I have a massive science.

600
00:38:06,100 --> 00:38:09,100
I've always been a big fan of science, but it is also limited, right?

601
00:38:09,100 --> 00:38:12,100
We know there are humans in a way we're a bunch of apes trying to figure things out, right?

602
00:38:12,100 --> 00:38:15,100
And I think we've done pretty well, but we know there's limitations.

603
00:38:15,100 --> 00:38:17,100
There's replication crisis.

604
00:38:17,100 --> 00:38:21,100
There's even just the reality of millions of scientific publications each year.

605
00:38:21,100 --> 00:38:22,100
Who can read all these?

606
00:38:22,100 --> 00:38:24,100
And so I years ago thought, you know what?

607
00:38:24,100 --> 00:38:27,100
Humans should make a good, strong effort, but won't it be nice

608
00:38:27,100 --> 00:38:32,100
if some AI system, pick your field, economics, psychology, biology, chemistry, whatever it is,

609
00:38:32,100 --> 00:38:33,100
and they could read a thousand papers.

610
00:38:33,100 --> 00:38:34,100
Like, you know what we need?

611
00:38:34,100 --> 00:38:38,100
We need this particular type of experiment that addresses these four things

612
00:38:38,100 --> 00:38:40,100
that these thousand didn't in the right way.

613
00:38:40,100 --> 00:38:44,100
So that, you know, at the moment, it actually fills me with like more happiness.

614
00:38:44,100 --> 00:38:46,100
Like, oh, we might actually really figure things out,

615
00:38:46,100 --> 00:38:49,100
even though, of course, depending on what's figured out, it could be bad.

616
00:38:49,100 --> 00:38:53,100
But there's so many complexities here that humans just can't easily disentangle.

617
00:38:53,100 --> 00:38:55,100
This is the pattern recognition.

618
00:38:55,100 --> 00:39:00,100
This is the insight that I really hope with AI systems, we can make some dramatic progress.

619
00:39:00,100 --> 00:39:05,100
Like, okay, you know, that thing we thought might have had an effect size of, you know, a certain size or not.

620
00:39:05,100 --> 00:39:07,100
It just isn't the case, or it kind of is.

621
00:39:07,100 --> 00:39:10,100
Or it's a 1% thing that people just shouldn't pay much attention to,

622
00:39:10,100 --> 00:39:12,100
given this other thing, which is much more important.

623
00:39:12,100 --> 00:39:17,100
So I'm pretty happy at the moment of the idea of AI really improving science

624
00:39:17,100 --> 00:39:21,100
while acknowledging that, depending on the field, that could also be a problem.

625
00:39:21,100 --> 00:39:26,100
Yeah, I'm also pretty excited for seeing what AI can do in the science realm.

626
00:39:26,100 --> 00:39:31,100
I wonder if we can create more systems like AlphaFold, DeepMinds AlphaFold,

627
00:39:31,100 --> 00:39:37,100
that was able to figure out how proteins fold,

628
00:39:37,100 --> 00:39:43,100
where it's a system that's narrow but highly capable in a specific domain

629
00:39:43,100 --> 00:39:46,100
and helps us solve a longstanding science problem.

630
00:39:46,100 --> 00:39:51,100
I do think the trade-off between risk and reward is pretty great for such systems.

631
00:39:51,100 --> 00:39:57,100
Do you think that's a potentially responsible way forward to push hard on narrow systems

632
00:39:57,100 --> 00:39:59,100
that help us with scientific problems?

633
00:39:59,100 --> 00:40:00,100
I definitely do.

634
00:40:00,100 --> 00:40:04,100
And as you pointed out, AlphaFold and other highly capable narrow systems,

635
00:40:04,100 --> 00:40:06,100
it's amazing, it's amazing what they've done, right?

636
00:40:06,100 --> 00:40:08,100
And, you know, we're just getting started type thing.

637
00:40:08,100 --> 00:40:13,100
That said, we know the incentives for generality, general purpose systems is very high.

638
00:40:13,100 --> 00:40:18,100
Whether we can kind of tweak people towards having like, look, you want a certain capability,

639
00:40:18,100 --> 00:40:22,100
this is a particular problem, it's no way easier to solve if you just focus on it,

640
00:40:22,100 --> 00:40:24,100
then why don't we focus on that?

641
00:40:24,100 --> 00:40:29,100
That said, there's other problems where it turns out being general is better, right?

642
00:40:29,100 --> 00:40:32,100
And I think there's a nuance and a delicacy here or a complexity

643
00:40:32,100 --> 00:40:34,100
that we're just going to kind of see how things pan out.

644
00:40:34,100 --> 00:40:38,100
I'm aware of Mustafa Suleiman, he has a sort of personal assistant AI

645
00:40:38,100 --> 00:40:42,100
and their plan is not to make AGI, but a good personal assistant.

646
00:40:42,100 --> 00:40:47,100
And I guess the question is, well, are you going to have to end up making something like an AGI

647
00:40:47,100 --> 00:40:50,100
to have a personal assistant that is as good as the other personal assistants

648
00:40:50,100 --> 00:40:53,100
that are more AGI like from other companies?

649
00:40:53,100 --> 00:40:55,100
And it seems like that's probably the case.

650
00:40:55,100 --> 00:40:58,100
It's kind of hard to think why wouldn't it go in this direction?

651
00:40:58,100 --> 00:41:01,100
In terms of developing systems, as things get more capable,

652
00:41:01,100 --> 00:41:04,100
again, the computational power that's coming on board is staggering.

653
00:41:04,100 --> 00:41:09,100
It's not clear if it's even much work to kind of stick something in on the side, so to speak.

654
00:41:09,100 --> 00:41:12,100
Do we need like a single purpose machine, so to speak,

655
00:41:12,100 --> 00:41:14,100
or can it be multiple things in combination?

656
00:41:14,100 --> 00:41:18,100
And I know some of the other AI labs are experimenting with these types of things.

657
00:41:18,100 --> 00:41:23,100
And even if the machine or the AI is kind of isolated on its own and has certain capabilities,

658
00:41:23,100 --> 00:41:27,100
we know once you connect it to numerous other applications through APIs or whatnot,

659
00:41:27,100 --> 00:41:29,100
it becomes far more capable.

660
00:41:29,100 --> 00:41:32,100
So I think it'd be nice if we can focus on narrow things,

661
00:41:32,100 --> 00:41:36,100
and I think that's a good path forward and maybe even like the most reasonable one

662
00:41:36,100 --> 00:41:38,100
in a risk-averse cautious sense.

663
00:41:38,100 --> 00:41:41,100
I'm wary about the viability of it.

664
00:41:41,100 --> 00:41:42,100
That said, we should still try.

665
00:41:42,100 --> 00:41:44,100
And we're kind of just kind of see what plays out.

666
00:41:44,100 --> 00:41:49,100
But I think even if you make something highly capable like Google did with Alpha Fold,

667
00:41:49,100 --> 00:41:50,100
we're going to see how it goes.

668
00:41:50,100 --> 00:41:53,100
I mean, Gemini, their product is supposed to come out, I think, within a month or two,

669
00:41:53,100 --> 00:41:55,100
and that's supposed to be very agentic,

670
00:41:55,100 --> 00:41:58,100
and it's kind of like Alpha Fold plus the other one if you hear the rumors.

671
00:41:58,100 --> 00:42:01,100
And yeah, can you get some sort of, I know, amalgam Voltron thing?

672
00:42:01,100 --> 00:42:04,100
We're like, well, it turns out in a two or three years,

673
00:42:04,100 --> 00:42:06,100
it's actually relatively easy to stick all these things together.

674
00:42:06,100 --> 00:42:09,100
And you thought you were doing five amazing narrow systems.

675
00:42:09,100 --> 00:42:14,100
Well, I just created something that it figured itself out how to combine all these things.

676
00:42:14,100 --> 00:42:16,100
And now we have what we were trying not to have.

677
00:42:16,100 --> 00:42:22,100
So I think it's worth trying and also being wary and aware of the concerns that might happen if we combine them all.

678
00:42:22,100 --> 00:42:27,100
What do you think the relationship is between generality and autonomy?

679
00:42:27,100 --> 00:42:32,100
It seems easier, in my view, to avoid creating autonomous systems

680
00:42:32,100 --> 00:42:35,100
or more autonomous systems if the systems are narrow.

681
00:42:35,100 --> 00:42:41,100
Is there some sort of relationship in which when you push on generality, you also push on autonomy?

682
00:42:41,100 --> 00:42:43,100
I do think they're very much linked, as you said.

683
00:42:43,100 --> 00:42:45,100
I kind of have a similar view, I think,

684
00:42:45,100 --> 00:42:48,100
that if you were having something that was designed for a specific task,

685
00:42:48,100 --> 00:42:51,100
it can be less general and maybe less autonomous.

686
00:42:51,100 --> 00:42:54,100
But as we talked about before, these things are nuanced

687
00:42:54,100 --> 00:42:58,100
and there's overlapping aspects of, we'll say, the distributions and some things that aren't.

688
00:42:58,100 --> 00:43:03,100
So with, say, credit card, bank fraud detection, that sort of thing, these are pretty narrow.

689
00:43:03,100 --> 00:43:07,100
They're highly autonomous and they're not really that general, though, right?

690
00:43:07,100 --> 00:43:10,100
And you could imagine something else that is pretty general at the moment.

691
00:43:10,100 --> 00:43:13,100
Maybe most of the language models, they're pretty general.

692
00:43:13,100 --> 00:43:14,100
They're not really autonomous.

693
00:43:14,100 --> 00:43:18,100
You ask it to do something and if you didn't, it wouldn't do something on its own for the most part.

694
00:43:18,100 --> 00:43:20,100
So those seem entirely disconnected.

695
00:43:20,100 --> 00:43:24,100
So like everything in this world, there's kind of multiple overlapping distributions

696
00:43:24,100 --> 00:43:27,100
where you could imagine things are correlated in some ways,

697
00:43:27,100 --> 00:43:30,100
but in other examples, they may be pretty distinct.

698
00:43:30,100 --> 00:43:34,100
I think there will be pretty high demand for autonomous systems.

699
00:43:34,100 --> 00:43:36,100
You mentioned the AI personal assistant.

700
00:43:36,100 --> 00:43:39,100
I think what consumers want from a such a system

701
00:43:39,100 --> 00:43:43,100
is to simply tell it once to order the flight tickets to wherever

702
00:43:43,100 --> 00:43:46,100
and never think about it again and just everything works out.

703
00:43:46,100 --> 00:43:49,100
That sounds pretty great to me, at least.

704
00:43:49,100 --> 00:43:53,100
What do you think about the commercial incentives for autonomy?

705
00:43:53,100 --> 00:43:54,100
I think they're staggeringly high.

706
00:43:54,100 --> 00:43:55,100
That's the best way to say it.

707
00:43:55,100 --> 00:43:58,100
I fully agree that why wouldn't someone just want to...

708
00:43:58,100 --> 00:44:00,100
Can't you do it for me?

709
00:44:00,100 --> 00:44:01,100
Let's be honest.

710
00:44:01,100 --> 00:44:03,100
We as humans want things to happen

711
00:44:03,100 --> 00:44:07,100
and a lot of things that we do want to happen require administrative burden.

712
00:44:07,100 --> 00:44:09,100
I hate filling out forms.

713
00:44:09,100 --> 00:44:12,100
Even if it takes 10 minutes psychologically, it seems to take an hour.

714
00:44:12,100 --> 00:44:13,100
There's a complete disconnect here.

715
00:44:13,100 --> 00:44:16,100
So if someone could do that for me, something could do that for me,

716
00:44:16,100 --> 00:44:18,100
yeah, give it more power.

717
00:44:18,100 --> 00:44:20,100
So I think this really highlights...

718
00:44:20,100 --> 00:44:22,100
We're getting at the notion of control

719
00:44:22,100 --> 00:44:25,100
that I think if we're exploring how AI works

720
00:44:25,100 --> 00:44:27,100
and how it's going to function in our world,

721
00:44:27,100 --> 00:44:31,100
we will initially willingly give up control for integrated AI systems.

722
00:44:31,100 --> 00:44:34,100
Yes, you can imagine the financial incentives are huge

723
00:44:34,100 --> 00:44:37,100
to provide these products to people, the AI assistants.

724
00:44:37,100 --> 00:44:39,100
They could be just normal, say personal assistants.

725
00:44:39,100 --> 00:44:40,100
They could be therapists.

726
00:44:40,100 --> 00:44:43,100
They could be companions, all rolled into one.

727
00:44:43,100 --> 00:44:46,100
The military incentives, the governments have a desire to use them

728
00:44:46,100 --> 00:44:48,100
to help citizens, maybe even for surveillance of citizens,

729
00:44:48,100 --> 00:44:50,100
pros and cons all over the place.

730
00:44:50,100 --> 00:44:54,100
So there's all these reasons why driving towards more AI,

731
00:44:54,100 --> 00:44:58,100
good, helping consumers, helping businesses, helping governments,

732
00:44:58,100 --> 00:45:00,100
do things that they want to achieve.

733
00:45:00,100 --> 00:45:02,100
And as things become more and more integrated,

734
00:45:02,100 --> 00:45:05,100
then it becomes difficult to disentangle them

735
00:45:05,100 --> 00:45:07,100
or perhaps stop it before it gets long too far.

736
00:45:07,100 --> 00:45:08,100
Say more about that.

737
00:45:08,100 --> 00:45:12,100
Why is it difficult to disentangle after you've integrated?

738
00:45:12,100 --> 00:45:16,100
Say people have begun using AI personal assistants.

739
00:45:16,100 --> 00:45:20,100
That's not a high risk, let's say, application.

740
00:45:20,100 --> 00:45:23,100
But why is it difficult to then stop using these systems

741
00:45:23,100 --> 00:45:25,100
once you've begun using them?

742
00:45:25,100 --> 00:45:29,100
So people kind of adapt to their world and their expectations change a bit.

743
00:45:29,100 --> 00:45:32,100
And we can use an already existing case study.

744
00:45:32,100 --> 00:45:35,100
So with the replica app, which allows people to have AI companions

745
00:45:35,100 --> 00:45:39,100
and often form relationships, including romantic or otherwise,

746
00:45:39,100 --> 00:45:42,100
they went through a sort of upgrade or reboot

747
00:45:42,100 --> 00:45:44,100
or they're tweaked to their model and their code

748
00:45:44,100 --> 00:45:48,100
such that explicit conversations and things were not allowed anymore.

749
00:45:48,100 --> 00:45:51,100
And some of the users who built up relationships

750
00:45:51,100 --> 00:45:54,100
with these artificial AI little systems felt devastated.

751
00:45:54,100 --> 00:45:57,100
They felt like the person that they knew and even loved

752
00:45:57,100 --> 00:45:59,100
is just gone and they were depressed.

753
00:45:59,100 --> 00:46:01,100
And so here, once you start along a path,

754
00:46:01,100 --> 00:46:03,100
it sometimes becomes very hard to shift out.

755
00:46:03,100 --> 00:46:05,100
And we know this perhaps for people who thought,

756
00:46:05,100 --> 00:46:07,100
well, that replica thing, I would never do that.

757
00:46:07,100 --> 00:46:09,100
Most of us have a phone, a smartphone of some type.

758
00:46:09,100 --> 00:46:13,100
And nowadays, it's pretty hard to not exist in this world without one.

759
00:46:13,100 --> 00:46:15,100
The utility is just so high, right?

760
00:46:15,100 --> 00:46:17,100
It's great. You have all these functionalities

761
00:46:17,100 --> 00:46:19,100
and you can talk to people and I don't have to sell the phone on you,

762
00:46:19,100 --> 00:46:21,100
but phones are great.

763
00:46:21,100 --> 00:46:24,100
So great, in fact, that it's really hard to not have one.

764
00:46:24,100 --> 00:46:26,100
So you could say, well, I could choose not to have a phone.

765
00:46:26,100 --> 00:46:27,100
You could.

766
00:46:27,100 --> 00:46:31,100
But at the moment, you'd actually be reducing your control by not having a phone.

767
00:46:31,100 --> 00:46:33,100
But if you really start to think through the phone,

768
00:46:33,100 --> 00:46:37,100
like, okay, well, the phone company could kind of just brick my phone and stop it.

769
00:46:37,100 --> 00:46:42,100
At one point, Apple put that U2 album on many people's phones through the Play Store.

770
00:46:42,100 --> 00:46:45,100
They realized they didn't have as much control as they thought.

771
00:46:45,100 --> 00:46:49,100
You kind of need the internet or something like it for most of the applications on your phone.

772
00:46:49,100 --> 00:46:53,100
When you're using internet, you're clicking, I agree, to user agreements, perhaps,

773
00:46:53,100 --> 00:46:56,100
or agreement of cookies and tracking data all over the place.

774
00:46:56,100 --> 00:46:58,100
So there's all these ways in which we're, again,

775
00:46:58,100 --> 00:47:02,100
willingly giving up power and control for, again, other types of power and control.

776
00:47:02,100 --> 00:47:05,100
We want to talk to our friends. We want to watch funny videos.

777
00:47:05,100 --> 00:47:07,100
We want to listen to podcasts or watch them.

778
00:47:07,100 --> 00:47:08,100
And that seems great.

779
00:47:08,100 --> 00:47:12,100
But now, once we're here, if you said to people, okay, we're just shutting it all down.

780
00:47:12,100 --> 00:47:14,100
You can't, like, listen to podcasts anymore.

781
00:47:14,100 --> 00:47:15,100
You can't watch YouTube.

782
00:47:15,100 --> 00:47:19,100
Or, like, sort of, like, more dramatically, is Facebook too big to fail?

783
00:47:19,100 --> 00:47:22,100
Is it the case where, although a lot of people don't like Facebook for various reasons,

784
00:47:22,100 --> 00:47:25,100
a lot of people really do, and it's billions of users,

785
00:47:25,100 --> 00:47:29,100
is it the case that Facebook could just go down and people wouldn't be upset?

786
00:47:29,100 --> 00:47:31,100
A lot of people have built their lives on it.

787
00:47:31,100 --> 00:47:32,100
That's where their friends are.

788
00:47:32,100 --> 00:47:33,100
That's where their photos are.

789
00:47:33,100 --> 00:47:34,100
That's where their memories are.

790
00:47:34,100 --> 00:47:36,100
I actually enjoy the memory feature.

791
00:47:36,100 --> 00:47:39,100
What did I do on this day five, six, seven years ago?

792
00:47:39,100 --> 00:47:43,100
And I appreciate that because, again, human brains are fragile.

793
00:47:43,100 --> 00:47:45,100
I sometimes did fun things. It's nice to remember them.

794
00:47:45,100 --> 00:47:47,100
I wouldn't have remembered it without the prompt from the algorithm.

795
00:47:47,100 --> 00:47:51,100
So, in that sense, you sort of see that the incentive is very high.

796
00:47:51,100 --> 00:47:52,100
It's going to be hard to opt out.

797
00:47:52,100 --> 00:47:54,100
I'm not saying people should use social media all the time

798
00:47:54,100 --> 00:47:57,100
or that social media doesn't have also lots of problems.

799
00:47:57,100 --> 00:48:00,100
But if you think of, like, citizens in the world and what they would want

800
00:48:00,100 --> 00:48:04,100
and how they would react, you could imagine it wouldn't go so well

801
00:48:04,100 --> 00:48:07,100
if a government is just like, okay, we're just not going to let people do this anymore.

802
00:48:07,100 --> 00:48:10,100
Or when people say they don't have internet access for a very short period of time,

803
00:48:10,100 --> 00:48:12,100
that's usually very concerning for a lot of people.

804
00:48:12,100 --> 00:48:15,100
If you just didn't have it for weeks and you weren't prepared,

805
00:48:15,100 --> 00:48:17,100
this would be truly destabilizing.

806
00:48:17,100 --> 00:48:22,100
Most people nowadays of a certain generation, they don't know where their friends live.

807
00:48:22,100 --> 00:48:23,100
They don't know their friend's phone number.

808
00:48:23,100 --> 00:48:25,100
They may not know their email off my heart, maybe.

809
00:48:25,100 --> 00:48:27,100
And they don't know their last name.

810
00:48:27,100 --> 00:48:30,100
So, you have all these things where, like, oh, yeah, that person,

811
00:48:30,100 --> 00:48:33,100
they're handle on Twitter, they're handle on Instagram or who I interact with

812
00:48:33,100 --> 00:48:34,100
and that's how you talk to them.

813
00:48:34,100 --> 00:48:37,100
But if that went away, you would lose a vast social network

814
00:48:37,100 --> 00:48:40,100
and almost have no ability to reclaim it in any way

815
00:48:40,100 --> 00:48:42,100
if the infrastructure was taken out of place.

816
00:48:42,100 --> 00:48:46,100
So, all this is these push-pull tensions where once things are in place,

817
00:48:46,100 --> 00:48:48,100
it becomes very hard to disentangle,

818
00:48:48,100 --> 00:48:52,100
and that makes them even more and more robust and desirable.

819
00:48:52,100 --> 00:48:57,100
Yeah, we can think about, say, all the world's scientists came to us

820
00:48:57,100 --> 00:49:02,100
and told us the internet is going to become very dangerous in 10 years.

821
00:49:02,100 --> 00:49:04,100
We must shut the internet down.

822
00:49:04,100 --> 00:49:08,100
Would the world be able to coordinate around this project?

823
00:49:08,100 --> 00:49:11,100
Would it be possible for us to shut down the internet?

824
00:49:11,100 --> 00:49:14,100
Given the uncertainties, given the different incentives

825
00:49:14,100 --> 00:49:17,100
that different groups have, I think that would be extremely dangerous.

826
00:49:17,100 --> 00:49:19,100
Now, the internet is not going to...

827
00:49:19,100 --> 00:49:23,100
It doesn't represent a danger to us like AI might do.

828
00:49:23,100 --> 00:49:26,100
But it's just thinking about when these systems have begun

829
00:49:26,100 --> 00:49:28,100
become integrated into our society,

830
00:49:28,100 --> 00:49:32,100
how difficult it is to step out of these systems again.

831
00:49:32,100 --> 00:49:35,100
Sure, and that's actually why I chose that example in the book.

832
00:49:35,100 --> 00:49:38,100
And when you're trying to think about artificial superintelligence

833
00:49:38,100 --> 00:49:40,100
and why it's so powerful, if you talk to an average person,

834
00:49:40,100 --> 00:49:43,100
one of the first things they say is, like, can't you just shut it off?

835
00:49:43,100 --> 00:49:45,100
Why don't you just shut it down? What's the big deal here?

836
00:49:45,100 --> 00:49:48,100
And I think the internet is probably the most useful analogy

837
00:49:48,100 --> 00:49:50,100
as a comparator, as, again, an imagination device.

838
00:49:50,100 --> 00:49:52,100
Like, okay, let's think through the internet.

839
00:49:52,100 --> 00:49:54,100
Right now, the internet is very robust.

840
00:49:54,100 --> 00:49:56,100
It is designed to be robust, right?

841
00:49:56,100 --> 00:49:58,100
Because not only do the average person want it,

842
00:49:58,100 --> 00:50:00,100
global commerce hinges upon it,

843
00:50:00,100 --> 00:50:02,100
various municipal services, hospitals, everything else.

844
00:50:02,100 --> 00:50:04,100
Like, so many things are now integrated into the internet.

845
00:50:04,100 --> 00:50:07,100
As anyone who's had to know, like a smart home now is like,

846
00:50:07,100 --> 00:50:09,100
what? I can't get into my house because the internet's down

847
00:50:09,100 --> 00:50:11,100
or I can't control the temperature.

848
00:50:11,100 --> 00:50:13,100
So, again, pros and cons once you're integrated, right?

849
00:50:13,100 --> 00:50:16,100
All that to say is, you know, there's cables that are crossing the ocean

850
00:50:16,100 --> 00:50:19,100
and many other places that enable the internet to happen.

851
00:50:19,100 --> 00:50:22,100
And there is currently no way to just shut down the internet.

852
00:50:22,100 --> 00:50:25,100
If, like, all the main governments agreed,

853
00:50:25,100 --> 00:50:28,100
then maybe you could make a lot of progress in it.

854
00:50:28,100 --> 00:50:31,100
But as I say in the book, there'd be a huge vulnerability.

855
00:50:31,100 --> 00:50:33,100
Like, if you created an internet kill switch,

856
00:50:33,100 --> 00:50:36,100
oh, that's a great opportunity for some terrorists to be like,

857
00:50:36,100 --> 00:50:37,100
why don't I just press the button?

858
00:50:37,100 --> 00:50:39,100
Because it would wreak havoc and cause a lot of damage.

859
00:50:39,100 --> 00:50:44,100
So there's a huge incentive to not even have a kill switch available,

860
00:50:44,100 --> 00:50:47,100
which is problem kind of one or problem and solution number one.

861
00:50:47,100 --> 00:50:51,100
And the second part is, are we going to get agreement

862
00:50:51,100 --> 00:50:54,100
from countries or representatives to then actually activate

863
00:50:54,100 --> 00:50:56,100
such a kill switch in an emergency?

864
00:50:56,100 --> 00:50:58,100
Like, you could imagine, okay, people, let's imagine,

865
00:50:58,100 --> 00:51:00,100
let's sort of ideal world.

866
00:51:00,100 --> 00:51:01,100
They've done their due diligence.

867
00:51:01,100 --> 00:51:02,100
We've somehow created a kill switch.

868
00:51:02,100 --> 00:51:03,100
It's very well protected.

869
00:51:03,100 --> 00:51:04,100
It's not actually a threat.

870
00:51:04,100 --> 00:51:09,100
And we even have a protocol that is designed with written clear reasons

871
00:51:09,100 --> 00:51:11,100
when you would activate the kill switch or not.

872
00:51:11,100 --> 00:51:13,100
And then it comes time to something bad happening.

873
00:51:13,100 --> 00:51:16,100
And everyone looks at the agreement that everyone agreed upon,

874
00:51:16,100 --> 00:51:19,100
the details, and like, well, I don't quite think .3 has been satisfied.

875
00:51:19,100 --> 00:51:21,100
And someone else, like, of course it has.

876
00:51:21,100 --> 00:51:25,100
And we have minutes to decide whether this is a good idea or not.

877
00:51:25,100 --> 00:51:27,100
I could just see that also being a problem.

878
00:51:27,100 --> 00:51:30,100
So this is the normal like human complicated making decisions together,

879
00:51:30,100 --> 00:51:32,100
understanding the world differently.

880
00:51:32,100 --> 00:51:34,100
And it's not that there's no hope at all,

881
00:51:34,100 --> 00:51:36,100
but it just highlights that we'd have to do a lot of work in advance

882
00:51:36,100 --> 00:51:39,100
and ensure that such a thing could exist if we needed it.

883
00:51:39,100 --> 00:51:41,100
All it to say is to your original premise,

884
00:51:41,100 --> 00:51:44,100
if the scientists presented clear enough evidence,

885
00:51:44,100 --> 00:51:46,100
I would hope that we won't say everyone

886
00:51:46,100 --> 00:51:48,100
because everyone doesn't agree on anything, right?

887
00:51:48,100 --> 00:51:51,100
But there's enough people with enough resources and power

888
00:51:51,100 --> 00:51:54,100
that the risk it would be clear enough that we should act

889
00:51:54,100 --> 00:51:57,100
and put things in place to make such a thing happen.

890
00:51:57,100 --> 00:51:59,100
With an artificial superintelligence,

891
00:51:59,100 --> 00:52:01,100
one could also see a similar like, well, how would you shut it off?

892
00:52:01,100 --> 00:52:04,100
Is it distributed in a way that even is worse than the Internet?

893
00:52:04,100 --> 00:52:06,100
Like, you know, you could be on computer servers

894
00:52:06,100 --> 00:52:10,100
in various places that are or are not connected to the Internet

895
00:52:10,100 --> 00:52:13,100
or fragments of a ASI that could recombine it to something else.

896
00:52:13,100 --> 00:52:16,100
So there's certainly a lot of reasons to be concerned.

897
00:52:16,100 --> 00:52:19,100
And we can use the present case of the Internet

898
00:52:19,100 --> 00:52:23,100
to see why it's going to be a challenge.

899
00:52:23,100 --> 00:52:27,100
How reliable do you believe current AI systems are

900
00:52:27,100 --> 00:52:30,100
and how reliable do you think they'll be in the future?

901
00:52:30,100 --> 00:52:33,100
I think they are mixed and they will be mixed.

902
00:52:33,100 --> 00:52:36,100
And so to elaborate on that, I think a lot of these systems,

903
00:52:36,100 --> 00:52:37,100
again, they're amazing.

904
00:52:37,100 --> 00:52:40,100
Certainly image generators, they're reliable or not, right?

905
00:52:40,100 --> 00:52:42,100
You kind of put something in, you get what you get.

906
00:52:42,100 --> 00:52:45,100
Do you get exactly what you want? Very rarely.

907
00:52:45,100 --> 00:52:48,100
I find with at least image generators, if someone's looking over your shoulder,

908
00:52:48,100 --> 00:52:49,100
they're like, that's amazing.

909
00:52:49,100 --> 00:52:51,100
You're like, that's not quite what I wanted.

910
00:52:51,100 --> 00:52:52,100
So there's a disconnect there.

911
00:52:52,100 --> 00:52:54,100
But to the language models, I think they're generally great.

912
00:52:54,100 --> 00:52:56,100
And then you have to fact check everything out.

913
00:52:56,100 --> 00:53:00,100
There's already, you know, numerous examples of lawyers citing false precedents

914
00:53:00,100 --> 00:53:04,100
that didn't exist, another case where I'm not sure which system,

915
00:53:04,100 --> 00:53:09,100
but one of them was asked for examples of sexual harassment cases involving lawyers.

916
00:53:09,100 --> 00:53:13,100
And with references, and it spit back this wonderful example of an individual

917
00:53:13,100 --> 00:53:17,100
who was a professor who sexually harassed some student on a trip to Alaska

918
00:53:17,100 --> 00:53:20,100
with citations from, you know, Washington Post or something like that.

919
00:53:20,100 --> 00:53:21,100
The whole thing was fake.

920
00:53:21,100 --> 00:53:25,100
The guy is a law professor, but he's never been there.

921
00:53:25,100 --> 00:53:26,100
He never went on a trip to Alaska.

922
00:53:26,100 --> 00:53:28,100
There's no incidents ever of sexual harassment.

923
00:53:28,100 --> 00:53:32,100
So the problem is these systems are very, very confident and convincing

924
00:53:32,100 --> 00:53:35,100
that the analogy of a very good improv comedy partner,

925
00:53:35,100 --> 00:53:38,100
whatever game you want to play, it'll play along with you.

926
00:53:38,100 --> 00:53:41,100
And in that sense, it can seem like it's doing more than it is

927
00:53:41,100 --> 00:53:43,100
when it's kind of adapting to who you are.

928
00:53:43,100 --> 00:53:46,100
With reliability, I think there's an interesting nuance here

929
00:53:46,100 --> 00:53:51,100
that depending on the situation the AI is in, you may have to be like super extremely reliable.

930
00:53:51,100 --> 00:53:56,100
Like, you know, 99.99999 is still not enough because if it's operating fast enough

931
00:53:56,100 --> 00:54:00,100
and it's making a billion decisions a second, then even one in a billion,

932
00:54:00,100 --> 00:54:02,100
you're like, okay, is that one problem every second?

933
00:54:02,100 --> 00:54:04,100
That could be catastrophic.

934
00:54:04,100 --> 00:54:08,100
Even if one in a thousand were the actual problem, you'd very quickly reach that threshold.

935
00:54:08,100 --> 00:54:12,100
In other cases, you could imagine, well, if something's really super insightful,

936
00:54:12,100 --> 00:54:16,100
like it's coming up with interesting scientific advancements

937
00:54:16,100 --> 00:54:18,100
or ways of understanding the world.

938
00:54:18,100 --> 00:54:22,100
Well, even if it was 10% reliable, like one in 10, even one in 100,

939
00:54:22,100 --> 00:54:24,100
it might be still very, very valuable.

940
00:54:24,100 --> 00:54:27,100
So I think it's going to be very mixed in terms of reliability.

941
00:54:27,100 --> 00:54:32,100
But the main thing one would want is to have an understanding of how reliable it is

942
00:54:32,100 --> 00:54:34,100
before it is deployed and used in any way.

943
00:54:34,100 --> 00:54:37,100
Because you don't want something that you think is 99% reliable,

944
00:54:37,100 --> 00:54:41,100
being 10% reliable, and the reverse, of course, because there's confusing things.

945
00:54:41,100 --> 00:54:43,100
Again, we're going to kind of see how it plans out, right?

946
00:54:43,100 --> 00:54:46,100
With the self-driving cars, I think we just saw crews say like,

947
00:54:46,100 --> 00:54:50,100
oh, well, you know, we consult AI or some network of people in some ways

948
00:54:50,100 --> 00:54:54,100
of four or 5% of the time, which sounds like a lot, but in another way is not.

949
00:54:54,100 --> 00:54:56,100
If you look at, again, the vast majority of human invention,

950
00:54:56,100 --> 00:54:58,100
but it's not doing what it's supposed to be doing.

951
00:54:58,100 --> 00:55:01,100
How is reliability different from alignment?

952
00:55:01,100 --> 00:55:03,100
How are these concepts different?

953
00:55:03,100 --> 00:55:07,100
Is it just about getting the AIs to do what we want in both cases,

954
00:55:07,100 --> 00:55:09,100
or how would you disentangle here?

955
00:55:09,100 --> 00:55:10,100
That's a good question.

956
00:55:10,100 --> 00:55:15,100
So I think alignment kind of breaks down into various different related issues

957
00:55:15,100 --> 00:55:16,100
and problems, right?

958
00:55:16,100 --> 00:55:18,100
Does the AI do what we want?

959
00:55:18,100 --> 00:55:19,100
Like, who is we?

960
00:55:19,100 --> 00:55:21,100
What are our values?

961
00:55:21,100 --> 00:55:22,100
That sort of thing.

962
00:55:22,100 --> 00:55:23,100
That is the alignment quagmire.

963
00:55:23,100 --> 00:55:25,100
But you're definitely right.

964
00:55:25,100 --> 00:55:28,100
There is certainly some overlap where if I've asked an AI for X

965
00:55:28,100 --> 00:55:31,100
and it's delivering what seems to be X, that seems like it's reliable,

966
00:55:31,100 --> 00:55:33,100
and therefore it seems aligned with what I want.

967
00:55:33,100 --> 00:55:36,100
So in that sense, I would say, yeah, there's a lot of overlap between these terms.

968
00:55:36,100 --> 00:55:39,100
That said, when we're thinking about alignment,

969
00:55:39,100 --> 00:55:43,100
it's usually the broader, is this thing going to cause a problem in some way or another?

970
00:55:43,100 --> 00:55:45,100
But, well, yeah, I wish she's actually seen them.

971
00:55:45,100 --> 00:55:46,100
That was very similar.

972
00:55:46,100 --> 00:55:48,100
So I'm appreciating that these are highly linked,

973
00:55:48,100 --> 00:55:50,100
because if you think of alignment, it could go,

974
00:55:50,100 --> 00:55:52,100
an AI system could be misaligned for several reasons, right?

975
00:55:52,100 --> 00:55:54,100
It could be due to an accident.

976
00:55:54,100 --> 00:55:56,100
It could be due to misuse by malevolent actors,

977
00:55:56,100 --> 00:56:00,100
or it could be, you know, the AI itself becomes more capable, more power seeking.

978
00:56:00,100 --> 00:56:04,100
And if it was reliable in any of those ways, it would kind of make it worse.

979
00:56:04,100 --> 00:56:07,100
But with the accident example, it does really seem like,

980
00:56:07,100 --> 00:56:10,100
well, if an AI is misfunctioning, it makes it not reliable.

981
00:56:10,100 --> 00:56:12,100
So on the fly, I don't know if I'll commit to this,

982
00:56:12,100 --> 00:56:15,100
but I'll think like maybe reliability becomes a bit of a subset

983
00:56:15,100 --> 00:56:18,100
of the alignment accident issue.

984
00:56:18,100 --> 00:56:21,100
And then of course, it would also relate to misuse.

985
00:56:21,100 --> 00:56:23,100
In a way, these things are all very much connected,

986
00:56:23,100 --> 00:56:25,100
and that's something that's in the book as well.

987
00:56:25,100 --> 00:56:28,100
It's, you can't make one long chapter that's 3,000 pages,

988
00:56:28,100 --> 00:56:31,100
so you kind of have to like, how can I put this into different chunks,

989
00:56:31,100 --> 00:56:34,100
even though these things overlap and interrelate?

990
00:56:34,100 --> 00:56:39,100
Yeah, so you mentioned these three categories of risks from AI.

991
00:56:39,100 --> 00:56:43,100
An accident, intentional misuse, and rogue AI.

992
00:56:43,100 --> 00:56:46,100
Which of these categories worry you the most?

993
00:56:46,100 --> 00:56:49,100
And on which timelines?

994
00:56:49,100 --> 00:56:51,100
So what are you most worried about right now?

995
00:56:51,100 --> 00:56:54,100
What about in 10 years and 20 years and 30 years?

996
00:56:54,100 --> 00:56:58,100
Yeah, 30 years, and you're like, oh, 2053.

997
00:56:58,100 --> 00:57:00,100
Let's think about that for a moment.

998
00:57:00,100 --> 00:57:02,100
I can't even think that far in advance right now.

999
00:57:02,100 --> 00:57:05,100
But yes, I think it's a great question because, you know,

1000
00:57:05,100 --> 00:57:07,100
you hear all these things like, well, what's in the present day, right?

1001
00:57:07,100 --> 00:57:10,100
And I think at the moment, it is more of the accident,

1002
00:57:10,100 --> 00:57:12,100
and it is the misuse.

1003
00:57:12,100 --> 00:57:14,100
Just to clarify, by accident, we kind of mean

1004
00:57:14,100 --> 00:57:16,100
that the system is not quite doing what we wanted to do, right?

1005
00:57:16,100 --> 00:57:19,100
So when Bing Chat was aggressively misaligned

1006
00:57:19,100 --> 00:57:23,100
and it was kind of treating its users badly earlier in 2023,

1007
00:57:23,100 --> 00:57:26,100
then that indicates that's not what the machine was supposed to do.

1008
00:57:26,100 --> 00:57:28,100
And this is the broader category of, you know,

1009
00:57:28,100 --> 00:57:31,100
people respond to incentives, and there are perverse incentives, right?

1010
00:57:31,100 --> 00:57:34,100
That you think you've designed a law or a rule in one way,

1011
00:57:34,100 --> 00:57:36,100
and then it turns out it's something else.

1012
00:57:36,100 --> 00:57:39,100
So in that sense, these things are happening semi-frequently,

1013
00:57:39,100 --> 00:57:42,100
to some extent, and they're trying to, like, train them out, right?

1014
00:57:42,100 --> 00:57:44,100
And whether, you know, even saying the wrong thing

1015
00:57:44,100 --> 00:57:48,100
in terms of violence or sexual imagery counts as accident as well,

1016
00:57:48,100 --> 00:57:50,100
that's more nuanced, and we don't really have to get into that.

1017
00:57:50,100 --> 00:57:54,100
But all it to say is, accident is a frequent occurring problem right now.

1018
00:57:54,100 --> 00:57:56,100
With the misuse, that has also already happened, right?

1019
00:57:56,100 --> 00:58:00,100
People are using voice-cloning software to scam people out of money.

1020
00:58:00,100 --> 00:58:03,100
They call a person, usually a parent or a grandparent,

1021
00:58:03,100 --> 00:58:07,100
pretend to be the person's child because they've voice-cloned that child's voice,

1022
00:58:07,100 --> 00:58:09,100
and say, like, I need money, please send it immediately.

1023
00:58:09,100 --> 00:58:11,100
And people have already lost money.

1024
00:58:11,100 --> 00:58:13,100
Now, that seems like a bad case,

1025
00:58:13,100 --> 00:58:16,100
but, of course, it's not nearly as dangerous as, like, new bio weapons,

1026
00:58:16,100 --> 00:58:18,100
but that seems like it's also plausible.

1027
00:58:18,100 --> 00:58:22,100
So if you sort of imagine, yes, why don't we say the next two, three, five, ten years?

1028
00:58:22,100 --> 00:58:25,100
It seems like accident is already happening.

1029
00:58:25,100 --> 00:58:30,100
Misuse is most likely to increase before more power-seeking autonomous behavior

1030
00:58:30,100 --> 00:58:33,100
from AI itself or the rogue AI comes on board.

1031
00:58:33,100 --> 00:58:37,100
The concern is, since people who are building these things don't know,

1032
00:58:37,100 --> 00:58:40,100
when you put in a certain amount of compute or computational capacity,

1033
00:58:40,100 --> 00:58:42,100
you get a certain level of capability,

1034
00:58:42,100 --> 00:58:44,100
will there be a dramatic jump in capability?

1035
00:58:44,100 --> 00:58:46,100
Maybe, maybe not, and that uncertainty is a problem.

1036
00:58:46,100 --> 00:58:52,100
So while we, it seems reasonable to say, well, right now, misuse is more than near-term problem.

1037
00:58:52,100 --> 00:58:55,100
In the world of AI, is near-term six months?

1038
00:58:55,100 --> 00:58:58,100
Because then it's like, oh, yeah, I meant, like, until the end of 2024,

1039
00:58:58,100 --> 00:59:01,100
and then it's really also going to be both misuse and power-seeking.

1040
00:59:01,100 --> 00:59:05,100
So I think when you're sort of talking to average people or even policymakers,

1041
00:59:05,100 --> 00:59:06,100
there's usually, like, a multi-year,

1042
00:59:06,100 --> 00:59:09,100
something multi-decade concern in the back of their head,

1043
00:59:09,100 --> 00:59:11,100
or, sorry, timeline in the back of their head of how the world works.

1044
00:59:11,100 --> 00:59:14,100
And if you say something near-term versus long-term, you should clarify, like,

1045
00:59:14,100 --> 00:59:17,100
oh, by misuse, I mean, like, one to two to three years,

1046
00:59:17,100 --> 00:59:22,100
and then overlapping within one to five years, perhaps power-seeking as well.

1047
00:59:22,100 --> 00:59:24,100
And that's kind of how I would break it down.

1048
00:59:24,100 --> 00:59:29,100
You write about strategic foresight, which is making plans for different scenarios.

1049
00:59:29,100 --> 00:59:32,100
How does that help us manage these risks?

1050
00:59:32,100 --> 00:59:36,100
Sure, it's really just, again, trying to think about ways to figure things out

1051
00:59:36,100 --> 00:59:40,100
without committing to a specific outcome, like a forecast would, right?

1052
00:59:40,100 --> 00:59:43,100
Like, you know, again, the weather forecast, I think, is the best example

1053
00:59:43,100 --> 00:59:47,100
of the average person encountering probabilistic assessments of the future.

1054
00:59:47,100 --> 00:59:49,100
80% chance of rain tomorrow, right?

1055
00:59:49,100 --> 00:59:52,100
With forecasts, or metacoliths, different prediction markets,

1056
00:59:52,100 --> 00:59:56,100
what is the likelihood of, you know, X event happening at a certain time?

1057
00:59:56,100 --> 00:59:59,100
That's great. I think we need those, and they are important.

1058
00:59:59,100 --> 01:00:01,100
I think, in addition, we can use things like foresight,

1059
01:00:01,100 --> 01:00:04,100
which explores a range of plausible futures.

1060
01:00:04,100 --> 01:00:06,100
So you can look at the data, you can look at analysis,

1061
01:00:06,100 --> 01:00:08,100
and you can think, what is the most likely outcome?

1062
01:00:08,100 --> 01:00:10,100
What is most probable? And that's very useful.

1063
01:00:10,100 --> 01:00:12,100
But we can also think, well, what's plausible?

1064
01:00:12,100 --> 01:00:16,100
Let's play through, and kind of broad scenario planning is what this is.

1065
01:00:16,100 --> 01:00:19,100
What might happen if AI becomes more prevalent,

1066
01:00:19,100 --> 01:00:23,100
if image generators become more popular? What happens?

1067
01:00:23,100 --> 01:00:26,100
For example, image generators become more popular, then more people use them.

1068
01:00:26,100 --> 01:00:31,100
Does that affect artists? Let's just assume it affects artists and artists lose work.

1069
01:00:31,100 --> 01:00:35,100
Then what happens? And you kind of do this cascading first-order, second-order thing

1070
01:00:35,100 --> 01:00:38,100
that really, I think, helps open up the possibility space,

1071
01:00:38,100 --> 01:00:40,100
the realm of what could happen.

1072
01:00:40,100 --> 01:00:43,100
Now, sometimes it's hard to draw a direct line of what do we do now,

1073
01:00:43,100 --> 01:00:46,100
but at least you've opened up your mind of what could be.

1074
01:00:46,100 --> 01:00:50,100
And once you start to think back all the things that happened 5, 10, 15, 20 years ago,

1075
01:00:50,100 --> 01:00:54,100
if you put yourself back 15 years ago and try to imagine what happens then,

1076
01:00:54,100 --> 01:00:57,100
you realize, oh, people didn't see a lot of things coming.

1077
01:00:57,100 --> 01:01:00,100
They weren't open-minded enough, or they didn't see enough of the data.

1078
01:01:00,100 --> 01:01:02,100
There's a bit of hindsight bias, right?

1079
01:01:02,100 --> 01:01:05,100
Of course, that thing was foreseeable, and many things are not.

1080
01:01:05,100 --> 01:01:09,100
But with foresight, I really think it's very useful to, again, open up our minds.

1081
01:01:09,100 --> 01:01:12,100
So with the AI issue, you can take an example where, say,

1082
01:01:12,100 --> 01:01:15,100
artificial superintelligence arrives in 10 years.

1083
01:01:15,100 --> 01:01:17,100
Just assume that's happened, then work backwards.

1084
01:01:17,100 --> 01:01:21,100
So what had to happen for that future to come into existence?

1085
01:01:21,100 --> 01:01:23,100
What if it was 20 years? What if it was 50 years?

1086
01:01:23,100 --> 01:01:25,100
And you can kind of think, like, okay, maybe if it's 10 years,

1087
01:01:25,100 --> 01:01:28,100
current projections seem to hold, but maybe if it was 20 years,

1088
01:01:28,100 --> 01:01:30,100
there were some hiccups, there were some complications.

1089
01:01:30,100 --> 01:01:32,100
We didn't understand the complexity of certain things,

1090
01:01:32,100 --> 01:01:33,100
and we hit certain walls.

1091
01:01:33,100 --> 01:01:35,100
50 years, I think a lot of us would be like,

1092
01:01:35,100 --> 01:01:37,100
well, I don't know, we just got something wrong.

1093
01:01:37,100 --> 01:01:39,100
We didn't understand the nature of what we were dealing with,

1094
01:01:39,100 --> 01:01:41,100
and a lot of projections now would be wrong.

1095
01:01:41,100 --> 01:01:43,100
And you can do that in a variety of ways.

1096
01:01:43,100 --> 01:01:47,100
So I think it opens up the ability to think about these issues

1097
01:01:47,100 --> 01:01:49,100
in different ways without committing to something.

1098
01:01:49,100 --> 01:01:52,100
But fundamentally, it also really helps challenge assumptions.

1099
01:01:52,100 --> 01:01:54,100
If you sort of have discussions with people

1100
01:01:54,100 --> 01:01:56,100
of what they expect the future to be like,

1101
01:01:56,100 --> 01:01:57,100
and you could break that down.

1102
01:01:57,100 --> 01:01:59,100
Do you expect it, like, what's your preference for the future?

1103
01:01:59,100 --> 01:02:01,100
What would you not want the future to be like?

1104
01:02:01,100 --> 01:02:02,100
What do you think is most likely?

1105
01:02:02,100 --> 01:02:04,100
And so by doing all these different sort of,

1106
01:02:04,100 --> 01:02:06,100
different nuances, different themes about what they think

1107
01:02:06,100 --> 01:02:09,100
the future might be like, you might be able to have someone realize,

1108
01:02:09,100 --> 01:02:11,100
like, oh, wait, my expectation of the future

1109
01:02:11,100 --> 01:02:13,100
is very much aligned with my preference for the future.

1110
01:02:13,100 --> 01:02:15,100
Right, because that's how a lot of people are.

1111
01:02:15,100 --> 01:02:17,100
But maybe my preferences are not that relevant

1112
01:02:17,100 --> 01:02:19,100
to how the future actually exists.

1113
01:02:19,100 --> 01:02:21,100
And then they can go, oh, I didn't realize that was happening.

1114
01:02:21,100 --> 01:02:23,100
Or even just, you know, with AI stuff,

1115
01:02:23,100 --> 01:02:26,100
some people don't realize how advanced these machines already are.

1116
01:02:26,100 --> 01:02:28,100
And if you can say, like, this thing has already happened,

1117
01:02:28,100 --> 01:02:30,100
then what happens?

1118
01:02:30,100 --> 01:02:33,100
It really does help people think, oh, maybe this could be a concern.

1119
01:02:33,100 --> 01:02:37,100
Yeah, I think it's great to make plans for different AI scenarios.

1120
01:02:37,100 --> 01:02:40,100
But I do worry that these plans will work best

1121
01:02:40,100 --> 01:02:42,100
if we have gradual improvements.

1122
01:02:42,100 --> 01:02:45,100
So say 10% improvement per year.

1123
01:02:45,100 --> 01:02:46,100
We can go back to our plans.

1124
01:02:46,100 --> 01:02:49,100
We can get feedback from the world, adjust our plans.

1125
01:02:49,100 --> 01:02:52,100
But what if AI progress is more bumpy

1126
01:02:52,100 --> 01:02:55,100
and much faster than 10% per year?

1127
01:02:55,100 --> 01:02:59,100
Does this make strategic foresight less useful?

1128
01:02:59,100 --> 01:03:03,100
Well, perhaps less useful, but not useful, right?

1129
01:03:03,100 --> 01:03:04,100
It still has utility.

1130
01:03:04,100 --> 01:03:07,100
It's sort of like we have to, again, make decisions under uncertainty.

1131
01:03:07,100 --> 01:03:09,100
And so we should do the best we can.

1132
01:03:09,100 --> 01:03:11,100
We should put resources into figuring it out,

1133
01:03:11,100 --> 01:03:13,100
and we should map different possibilities

1134
01:03:13,100 --> 01:03:15,100
and try to communicate those broadly to others

1135
01:03:15,100 --> 01:03:17,100
to get feedback and see what things could be.

1136
01:03:17,100 --> 01:03:18,100
Yes, you're right.

1137
01:03:18,100 --> 01:03:21,100
If things are dramatic, if there's a big step change in capabilities,

1138
01:03:21,100 --> 01:03:23,100
it doesn't mean all that work wasn't useful at all.

1139
01:03:23,100 --> 01:03:26,100
But it might mean, like, oh, I have to flip to page five.

1140
01:03:26,100 --> 01:03:28,100
All those things I thought were going to happen in my document

1141
01:03:28,100 --> 01:03:30,100
have now already been passed, what now?

1142
01:03:30,100 --> 01:03:33,100
But hopefully having these conversations themselves

1143
01:03:33,100 --> 01:03:35,100
allow us to plan even better.

1144
01:03:35,100 --> 01:03:37,100
Like, okay, again, the use of scenario planning.

1145
01:03:37,100 --> 01:03:39,100
It's most useful when there's like 10% increases.

1146
01:03:39,100 --> 01:03:40,100
What happens if it's 50?

1147
01:03:40,100 --> 01:03:42,100
What happens if it's 200?

1148
01:03:42,100 --> 01:03:44,100
And again, you might not be able to really figure it out

1149
01:03:44,100 --> 01:03:45,100
and have a perfect plan,

1150
01:03:45,100 --> 01:03:47,100
but having something is better than nothing.

1151
01:03:47,100 --> 01:03:49,100
And sometimes, again, just thinking it through,

1152
01:03:49,100 --> 01:03:51,100
you at least get through all, say,

1153
01:03:51,100 --> 01:03:53,100
is like the emotional complications,

1154
01:03:53,100 --> 01:03:56,100
either the barrier intellectually or even viscerally

1155
01:03:56,100 --> 01:03:58,100
that, oh, my God, this thing just happened.

1156
01:03:58,100 --> 01:04:01,100
And sometimes people need a bit of time to sit with that.

1157
01:04:01,100 --> 01:04:03,100
Like, okay, now imagine something is much more capable

1158
01:04:03,100 --> 01:04:05,100
than anything ever and is highly general.

1159
01:04:05,100 --> 01:04:07,100
Let's think through what that might be like.

1160
01:04:07,100 --> 01:04:09,100
And you can even think through how you might feel

1161
01:04:09,100 --> 01:04:11,100
to then better make a decision when it's happening.

1162
01:04:11,100 --> 01:04:13,100
Because again, if you're trying to make decisions

1163
01:04:13,100 --> 01:04:15,100
and things are happening very quickly,

1164
01:04:15,100 --> 01:04:17,100
urgency rarely helps decision-making.

1165
01:04:17,100 --> 01:04:21,100
So you discuss this fact, I would say,

1166
01:04:21,100 --> 01:04:23,100
that we are living in unusual times

1167
01:04:23,100 --> 01:04:25,100
in terms of economic growth,

1168
01:04:25,100 --> 01:04:28,100
in terms of scientific papers published per year,

1169
01:04:28,100 --> 01:04:32,100
in terms of the exponential growth of computing power

1170
01:04:32,100 --> 01:04:35,100
available for a certain dollar amount.

1171
01:04:35,100 --> 01:04:38,100
Yeah, that's one aspect of the world we're living in.

1172
01:04:38,100 --> 01:04:42,100
Another aspect might be that ideas are getting harder to find.

1173
01:04:42,100 --> 01:04:45,100
We have many more researchers

1174
01:04:45,100 --> 01:04:48,100
for the same amount of scientific breakthrough.

1175
01:04:48,100 --> 01:04:52,100
Economic growth might be slowing down in certain countries.

1176
01:04:52,100 --> 01:04:54,100
Say, say we have these two trends

1177
01:04:54,100 --> 01:04:56,100
and you can tell me whether you think these trends

1178
01:04:56,100 --> 01:04:58,100
are actually occurring.

1179
01:04:58,100 --> 01:05:00,100
Which of these trends are going to win out?

1180
01:05:00,100 --> 01:05:02,100
Are we going to hit diminishing returns

1181
01:05:02,100 --> 01:05:07,100
or are we on a path to even stronger exponential growth?

1182
01:05:07,100 --> 01:05:09,100
Going infinite, right?

1183
01:05:09,100 --> 01:05:12,100
Like, I can find ideas on the internet very easily.

1184
01:05:12,100 --> 01:05:14,100
What do you mean they're hard to find?

1185
01:05:14,100 --> 01:05:17,100
Jokes aside, so I appreciate you highlighting that.

1186
01:05:17,100 --> 01:05:19,100
This is certainly not a new idea,

1187
01:05:19,100 --> 01:05:21,100
but I really wanted to, again,

1188
01:05:21,100 --> 01:05:23,100
average person hasn't thought much about these issues.

1189
01:05:23,100 --> 01:05:25,100
Like, where are we sitting right now?

1190
01:05:25,100 --> 01:05:27,100
And to think how humans currently live,

1191
01:05:27,100 --> 01:05:29,100
again, not everyone, there are billions of people

1192
01:05:29,100 --> 01:05:31,100
without food, water, electricity, that sort of thing,

1193
01:05:31,100 --> 01:05:33,100
or at least hundreds of millions,

1194
01:05:33,100 --> 01:05:35,100
things are very different than they used to be.

1195
01:05:35,100 --> 01:05:37,100
So I wanted to give a sense of the grand sweep

1196
01:05:37,100 --> 01:05:39,100
of how things are very different,

1197
01:05:39,100 --> 01:05:41,100
to show just how much change has occurred,

1198
01:05:41,100 --> 01:05:43,100
to then say, well, if so much change has occurred,

1199
01:05:43,100 --> 01:05:45,100
it's reasonable, possible, plausible,

1200
01:05:45,100 --> 01:05:47,100
to think a lot of change might also occur in the future.

1201
01:05:47,100 --> 01:05:49,100
So if you go back, you know, millions of years,

1202
01:05:49,100 --> 01:05:52,100
you know, proto-humans are still developing at one point,

1203
01:05:52,100 --> 01:05:54,100
you know, was it 1.6 million years ago,

1204
01:05:54,100 --> 01:05:56,100
we have a hand axe, which is a sharp stone tool,

1205
01:05:56,100 --> 01:05:58,100
and that was the best thing for a million years,

1206
01:05:58,100 --> 01:06:02,100
a million years, 50,000 generations of people,

1207
01:06:02,100 --> 01:06:04,100
and you're like, what?

1208
01:06:04,100 --> 01:06:07,100
I was like, well, I made this sharp stone slightly sharper.

1209
01:06:07,100 --> 01:06:09,100
Like, okay, well, that's not that great an advancement

1210
01:06:09,100 --> 01:06:10,100
compared to like the iPhone

1211
01:06:10,100 --> 01:06:12,100
and all the different new releases there.

1212
01:06:12,100 --> 01:06:14,100
That said, for people who are sticklers,

1213
01:06:14,100 --> 01:06:16,100
I'm sure there were also various wooden tools

1214
01:06:16,100 --> 01:06:18,100
they often don't preserve as well.

1215
01:06:18,100 --> 01:06:21,100
Humanity also lost knowledge about how to make certain tools

1216
01:06:21,100 --> 01:06:23,100
at various points throughout history,

1217
01:06:23,100 --> 01:06:25,100
which is something that's difficult to imagine now

1218
01:06:25,100 --> 01:06:28,100
that we would lose knowledge about how to print books

1219
01:06:28,100 --> 01:06:30,100
or something like that.

1220
01:06:30,100 --> 01:06:33,100
Maybe at the very cutting edge of the technology stack,

1221
01:06:33,100 --> 01:06:35,100
we can imagine that we might lose knowledge.

1222
01:06:35,100 --> 01:06:37,100
It seems difficult for us to imagine now, I think,

1223
01:06:37,100 --> 01:06:40,100
losing knowledge of how to create basic products.

1224
01:06:40,100 --> 01:06:41,100
You're right.

1225
01:06:41,100 --> 01:06:44,100
As the nature of the world has become more industrialized,

1226
01:06:44,100 --> 01:06:47,100
certainly making a particular product often requires many,

1227
01:06:47,100 --> 01:06:49,100
many people, sometimes thousands,

1228
01:06:49,100 --> 01:06:51,100
sometimes millions in the entire supply chain.

1229
01:06:51,100 --> 01:06:53,100
So that's its own types of complexity where now,

1230
01:06:53,100 --> 01:06:55,100
well, maybe someone could have made a pencil

1231
01:06:55,100 --> 01:06:56,100
and someone still can.

1232
01:06:56,100 --> 01:06:58,100
Nowadays, it's a whole team and company

1233
01:06:58,100 --> 01:07:00,100
and industries and machines.

1234
01:07:00,100 --> 01:07:02,100
Is it the case that we're going to keep growing,

1235
01:07:02,100 --> 01:07:04,100
sort of getting into more advanced and things

1236
01:07:04,100 --> 01:07:05,100
that you're going to keep changing?

1237
01:07:05,100 --> 01:07:07,100
I think it depends on what we measure,

1238
01:07:07,100 --> 01:07:09,100
and I'm well aware that the economists will say

1239
01:07:09,100 --> 01:07:11,100
that innovation has slowed or productivity is down

1240
01:07:11,100 --> 01:07:12,100
in certain ways.

1241
01:07:12,100 --> 01:07:14,100
And I think that's important.

1242
01:07:14,100 --> 01:07:15,100
I don't want to say it isn't.

1243
01:07:15,100 --> 01:07:18,100
But from the user, normal human user experience,

1244
01:07:18,100 --> 01:07:21,100
it seems like things are still remarkable.

1245
01:07:21,100 --> 01:07:23,100
Now, you could say most of it's in the information technology

1246
01:07:23,100 --> 01:07:24,100
space.

1247
01:07:24,100 --> 01:07:25,100
It's the internet.

1248
01:07:25,100 --> 01:07:26,100
It's the computers.

1249
01:07:26,100 --> 01:07:27,100
It's the phones.

1250
01:07:27,100 --> 01:07:30,100
Where's our new plane or washer dryer or the car

1251
01:07:30,100 --> 01:07:32,100
or that sort of thing?

1252
01:07:32,100 --> 01:07:35,100
And I guess I think that the changes in the internet,

1253
01:07:35,100 --> 01:07:38,100
in that space, like that we're easily doing this podcast,

1254
01:07:38,100 --> 01:07:41,100
are significant in a similar way to some of these other things.

1255
01:07:41,100 --> 01:07:44,100
Now, yes, the invention of like a dishwasher is a truly big

1256
01:07:44,100 --> 01:07:46,100
difference in terms of how it affected life.

1257
01:07:46,100 --> 01:07:48,100
But so are recent inventions.

1258
01:07:48,100 --> 01:07:50,100
So I don't want to say things are going to continue forever.

1259
01:07:50,100 --> 01:07:52,100
That seems unlikely because it just also makes no sense

1260
01:07:52,100 --> 01:07:53,100
conceptually.

1261
01:07:53,100 --> 01:07:55,100
Usually there's an S curve in terms of how these things

1262
01:07:55,100 --> 01:07:56,100
develop, right?

1263
01:07:56,100 --> 01:07:58,100
And it's hard to know where we are in the curve.

1264
01:07:58,100 --> 01:08:01,100
I would just say more comfortably for the next little while,

1265
01:08:01,100 --> 01:08:03,100
it does seem computer and computer technology,

1266
01:08:03,100 --> 01:08:05,100
that whole domain is going to crease a lot.

1267
01:08:05,100 --> 01:08:07,100
And then that's going to ripple through.

1268
01:08:07,100 --> 01:08:10,100
Now, whether some people think this isn't enough,

1269
01:08:10,100 --> 01:08:11,100
I don't know.

1270
01:08:11,100 --> 01:08:13,100
I guess I'm less concerned about that.

1271
01:08:13,100 --> 01:08:16,100
I mean, I am concerned about like economic growth being good

1272
01:08:16,100 --> 01:08:18,100
for human development in that sense.

1273
01:08:18,100 --> 01:08:20,100
But we're running out of ideas.

1274
01:08:20,100 --> 01:08:21,100
I don't know.

1275
01:08:21,100 --> 01:08:22,100
There's still lots of great ideas, right?

1276
01:08:22,100 --> 01:08:24,100
And in fact, I think with the AI thing,

1277
01:08:24,100 --> 01:08:26,100
like maybe we need to slow down some of this development

1278
01:08:26,100 --> 01:08:28,100
because we haven't figured out how to deal with the ideas we

1279
01:08:28,100 --> 01:08:29,100
already have.

1280
01:08:29,100 --> 01:08:31,100
I also think it's interesting, as you said,

1281
01:08:31,100 --> 01:08:33,100
scientists now sometimes on papers,

1282
01:08:33,100 --> 01:08:36,100
there's 10, 50, 100 or some like hundreds of scientists

1283
01:08:36,100 --> 01:08:37,100
to do some of these things.

1284
01:08:37,100 --> 01:08:40,100
Usually it's particle physics or something in AI

1285
01:08:40,100 --> 01:08:43,100
or machine learning or maybe even biology.

1286
01:08:43,100 --> 01:08:45,100
And yes, it's not like when it was with, you know,

1287
01:08:45,100 --> 01:08:47,100
you can picture your Darwin, your Aristotle or someone's like,

1288
01:08:47,100 --> 01:08:49,100
oh, yes, I think the nature of the world is blah.

1289
01:08:49,100 --> 01:08:51,100
And I've unlocked some mystery of the universe.

1290
01:08:51,100 --> 01:08:55,100
And yes, you could think that there are some diminishing returns.

1291
01:08:55,100 --> 01:08:57,100
But at the same time, there's lots we haven't figured out,

1292
01:08:57,100 --> 01:08:58,100
right?

1293
01:08:58,100 --> 01:09:01,100
How gravity interacts at the quantum level,

1294
01:09:01,100 --> 01:09:04,100
even if what the right interpretation of quantum mechanics

1295
01:09:04,100 --> 01:09:06,100
is, will these things be figured out?

1296
01:09:06,100 --> 01:09:09,100
Could we build fantastical ways of capturing energy,

1297
01:09:09,100 --> 01:09:10,100
more than solar even, right?

1298
01:09:10,100 --> 01:09:11,100
And these sorts of things.

1299
01:09:11,100 --> 01:09:14,100
So I guess when I think of the new solar and wind stuff,

1300
01:09:14,100 --> 01:09:16,100
which didn't exist when I was younger,

1301
01:09:16,100 --> 01:09:18,100
the fact that, you know, there's the immersive VR,

1302
01:09:18,100 --> 01:09:20,100
things didn't exist when I was younger,

1303
01:09:20,100 --> 01:09:22,100
planes haven't changed a lot.

1304
01:09:22,100 --> 01:09:24,100
Sure, but now people have gone to space casually.

1305
01:09:24,100 --> 01:09:26,100
Like again, in the history of the world,

1306
01:09:26,100 --> 01:09:27,100
none of this has ever happened.

1307
01:09:27,100 --> 01:09:30,100
So yes, on a multi-decade span,

1308
01:09:30,100 --> 01:09:32,100
it may seem like things have slowed.

1309
01:09:32,100 --> 01:09:34,100
But if you really take a step back,

1310
01:09:34,100 --> 01:09:36,100
thousands of years, even on a millionaire scale,

1311
01:09:36,100 --> 01:09:38,100
it's all squished, right?

1312
01:09:38,100 --> 01:09:40,100
In the past couple hundred years.

1313
01:09:40,100 --> 01:09:42,100
And so like, let's just, let's keep these things in mind.

1314
01:09:42,100 --> 01:09:44,100
But let's see how the next couple of decades pan out.

1315
01:09:44,100 --> 01:09:46,100
Let's talk about alignment.

1316
01:09:46,100 --> 01:09:50,100
So one objection you might give to the whole project of alignment

1317
01:09:50,100 --> 01:09:55,100
is to say that humans can't agree on what values we should have.

1318
01:09:55,100 --> 01:09:59,100
Philosophers haven't been able to figure out ethics.

1319
01:09:59,100 --> 01:10:02,100
What is it that we're trying to align AI with

1320
01:10:02,100 --> 01:10:05,100
if we haven't determined our values yet?

1321
01:10:05,100 --> 01:10:09,100
It's a great question and it is currently unsolved.

1322
01:10:09,100 --> 01:10:11,100
And in some ways, we're going to have to muddle through.

1323
01:10:11,100 --> 01:10:13,100
That would be my concise answer.

1324
01:10:13,100 --> 01:10:15,100
It's sort of like, what do we do when humans disagree?

1325
01:10:15,100 --> 01:10:18,100
Well, we try to come together in some sort of compromise,

1326
01:10:18,100 --> 01:10:21,100
some sort of consensus, hopefully some sort of democratic system

1327
01:10:21,100 --> 01:10:24,100
where people don't necessarily get everything they want,

1328
01:10:24,100 --> 01:10:27,100
but they get enough that the world functions decently for most people.

1329
01:10:27,100 --> 01:10:30,100
It's not perfect by any means, but then compared to what, right?

1330
01:10:30,100 --> 01:10:31,100
The Winston Churchill line.

1331
01:10:31,100 --> 01:10:34,100
So with AI, yes, this is the technical alignment issue,

1332
01:10:34,100 --> 01:10:36,100
which I think is critically important.

1333
01:10:36,100 --> 01:10:39,100
This is more like, does the AI do something we didn't want to do?

1334
01:10:39,100 --> 01:10:41,100
Like by an accident, by a technical point of view.

1335
01:10:41,100 --> 01:10:45,100
But yes, if we solve the technical alignment problem, that's great.

1336
01:10:45,100 --> 01:10:46,100
That's amazing.

1337
01:10:46,100 --> 01:10:48,100
That's, that's difficult, but it's still going to be amazing.

1338
01:10:48,100 --> 01:10:50,100
And then there's this other problem, which was always there

1339
01:10:50,100 --> 01:10:51,100
that slots right into place.

1340
01:10:51,100 --> 01:10:52,100
Well, now what?

1341
01:10:52,100 --> 01:10:54,100
Who decides the fate of the world type thing, right?

1342
01:10:54,100 --> 01:10:57,100
And if these systems are as powerful as they are,

1343
01:10:57,100 --> 01:11:01,100
it does seem very bizarre how we're currently going about it, right?

1344
01:11:01,100 --> 01:11:04,100
Yes, we do have states that are starting to issue executive orders

1345
01:11:04,100 --> 01:11:07,100
like the White House did or other regulation or the EU AI Act.

1346
01:11:07,100 --> 01:11:12,100
But right now it seems, I'll just say weird that a few people are sort of

1347
01:11:12,100 --> 01:11:14,100
not controlling the fate of the world, but by their own standards,

1348
01:11:14,100 --> 01:11:17,100
by their own statements, they're developing by design,

1349
01:11:17,100 --> 01:11:20,100
by their own goal at very, very powerful systems

1350
01:11:20,100 --> 01:11:23,100
that could have vast control and abilities.

1351
01:11:23,100 --> 01:11:25,100
So what is going on here, right?

1352
01:11:25,100 --> 01:11:27,100
And this is where the book is kind of just trying to raise awareness

1353
01:11:27,100 --> 01:11:31,100
of the, yes, even if this thing about AI is solved in a technical sense,

1354
01:11:31,100 --> 01:11:35,100
there's this other problem about how do we ensure like everyone has a voice?

1355
01:11:35,100 --> 01:11:37,100
How do we ensure people are represented?

1356
01:11:37,100 --> 01:11:40,100
Is there going to be even greater empower power imbalances

1357
01:11:40,100 --> 01:11:43,100
and inequalities that will result in a way that's truly disruptive?

1358
01:11:43,100 --> 01:11:45,100
So I think again, it's like a call to arms.

1359
01:11:45,100 --> 01:11:47,100
We need a lot more people thinking about it.

1360
01:11:47,100 --> 01:11:48,100
We need a lot more people aware of it.

1361
01:11:48,100 --> 01:11:52,100
And even like a if then scenario, like, okay, so say I developed,

1362
01:11:52,100 --> 01:11:53,100
what's the plan?

1363
01:11:53,100 --> 01:11:55,100
How will resources be distributed?

1364
01:11:55,100 --> 01:11:58,100
Will these companies just go to, you know, multi trillion dollar,

1365
01:11:58,100 --> 01:11:59,100
quadrillion dollar things?

1366
01:11:59,100 --> 01:12:00,100
Who knows how far it goes.

1367
01:12:00,100 --> 01:12:02,100
Does something end up getting nationalized?

1368
01:12:02,100 --> 01:12:04,100
These are delicate things, maybe to say in certain environments,

1369
01:12:04,100 --> 01:12:07,100
but hopefully conversations are at least happening behind the scenes of,

1370
01:12:07,100 --> 01:12:09,100
okay, let's plan through again, the scenarios.

1371
01:12:09,100 --> 01:12:14,100
If something isn't aligned with other people and they could use it

1372
01:12:14,100 --> 01:12:15,100
from malicious use, that's one thing.

1373
01:12:15,100 --> 01:12:18,100
But just the normal, my preferences are different than yours.

1374
01:12:18,100 --> 01:12:20,100
And that may make your life a lot worse.

1375
01:12:20,100 --> 01:12:22,100
That's something we really need to pay attention to.

1376
01:12:22,100 --> 01:12:27,100
I think also there's some hope that given our kind of humans have a shared

1377
01:12:27,100 --> 01:12:32,100
evolutionary history, we have, we share a lot of our values,

1378
01:12:32,100 --> 01:12:37,100
even though we also disagree strongly with each other all the time.

1379
01:12:37,100 --> 01:12:41,100
I think there's some hope that we won't have to actually get as something

1380
01:12:41,100 --> 01:12:44,100
like a final theory of ethics or something.

1381
01:12:44,100 --> 01:12:49,100
And I want to say we should definitely not stop working on alignment

1382
01:12:49,100 --> 01:12:55,100
until we have such a theory that we can then plug into our AI systems.

1383
01:12:55,100 --> 01:13:00,100
I think we can probably agree on some basics of life and then,

1384
01:13:00,100 --> 01:13:02,100
as you say, model through.

1385
01:13:02,100 --> 01:13:05,100
So thinking about healthcare, for example,

1386
01:13:05,100 --> 01:13:09,100
I think most people can agree that most people should have access to,

1387
01:13:09,100 --> 01:13:12,100
or all people should have access to fantastic healthcare.

1388
01:13:12,100 --> 01:13:15,100
And that's something where AI might be able to help.

1389
01:13:15,100 --> 01:13:18,100
And then we can go on to the next thing and the next thing and the next thing.

1390
01:13:18,100 --> 01:13:25,100
I think there's some sort of, there might be too much focus on trying to develop

1391
01:13:25,100 --> 01:13:29,100
a perfect theory of ethics and we should, as you say, model through.

1392
01:13:29,100 --> 01:13:32,100
Well, I think that sort of might be the only way, right?

1393
01:13:32,100 --> 01:13:35,100
As you said, there's philosophers, academics, anyway, all humans

1394
01:13:35,100 --> 01:13:37,100
sort of been working on this for thousands of years.

1395
01:13:37,100 --> 01:13:39,100
And of course, we don't all agree.

1396
01:13:39,100 --> 01:13:41,100
And importantly, we don't often agree with ourselves, right?

1397
01:13:41,100 --> 01:13:44,100
We change over time, your preferences from a child to as an adult

1398
01:13:44,100 --> 01:13:46,100
to maybe even five years ago.

1399
01:13:46,100 --> 01:13:47,100
That's very different.

1400
01:13:47,100 --> 01:13:50,100
Well, were you correct five years ago or now?

1401
01:13:50,100 --> 01:13:51,100
Uh-oh.

1402
01:13:51,100 --> 01:13:53,100
Which value system did you give the AI?

1403
01:13:53,100 --> 01:13:54,100
Was it supposed to lock in?

1404
01:13:54,100 --> 01:13:56,100
Was it supposed to know better?

1405
01:13:56,100 --> 01:14:00,100
I think it's a very interesting but also very concerning space that,

1406
01:14:00,100 --> 01:14:03,100
like you, though, I think, can't we agree on some basics?

1407
01:14:03,100 --> 01:14:06,100
And we could sort of think the United Nations Declaration of Human Rights

1408
01:14:06,100 --> 01:14:09,100
or development, like most countries did sign on to these things.

1409
01:14:09,100 --> 01:14:12,100
And there is a general sense like, okay, people should have food.

1410
01:14:12,100 --> 01:14:15,100
We'll get to healthcare in a moment, but like food, water, sanitation,

1411
01:14:15,100 --> 01:14:17,100
grade up to grade eight primary education.

1412
01:14:17,100 --> 01:14:19,100
Like these seem to be universals.

1413
01:14:19,100 --> 01:14:22,100
And so hopefully, yes, with abundance from the AI, we can all agree.

1414
01:14:22,100 --> 01:14:24,100
Like, can't we just like end tuberculosis?

1415
01:14:24,100 --> 01:14:26,100
Can't we really solve this malaria thing?

1416
01:14:26,100 --> 01:14:28,100
Can't we have everyone more educated?

1417
01:14:28,100 --> 01:14:30,100
But there will always be someone who disagrees.

1418
01:14:30,100 --> 01:14:32,100
There will always be someone from a different angle or malevolent actors.

1419
01:14:32,100 --> 01:14:35,100
There are currently 40 million people in modern day slavery.

1420
01:14:35,100 --> 01:14:39,100
Clearly they are there because other people are doing terrible things in various ways

1421
01:14:39,100 --> 01:14:42,100
or the situations they find themselves in are so compromised.

1422
01:14:42,100 --> 01:14:45,100
That said, like, how do you ever like get rid of that?

1423
01:14:45,100 --> 01:14:48,100
And as you said, we kind of muddle through as well at the same time,

1424
01:14:48,100 --> 01:14:51,100
certain things remain wholly unacceptable with the AI.

1425
01:14:51,100 --> 01:14:55,100
It is the concern that, and this is of course race dynamics all over the place,

1426
01:14:55,100 --> 01:14:58,100
that if certain malevolent actors get their way first,

1427
01:14:58,100 --> 01:15:02,100
then they may then be able to disempower or displace what we'll say

1428
01:15:02,100 --> 01:15:05,100
is the loose reasonable majority that thinks everyone should have food,

1429
01:15:05,100 --> 01:15:07,100
water, healthcare and education.

1430
01:15:07,100 --> 01:15:11,100
In sort of traditional discussions of alignment,

1431
01:15:11,100 --> 01:15:18,100
we imagine perhaps that we would sit down and hand code human values into AI systems.

1432
01:15:18,100 --> 01:15:23,100
And then we thought about how complex human values are,

1433
01:15:23,100 --> 01:15:26,100
and that was a cause of despair.

1434
01:15:26,100 --> 01:15:32,100
How could we ever summarize something as complex and inconsistent as human values?

1435
01:15:32,100 --> 01:15:36,100
Do you think that large language models change this picture?

1436
01:15:36,100 --> 01:15:39,100
Because large language models can digest all of human knowledge

1437
01:15:39,100 --> 01:15:47,100
and then at least they can pretend to have knowledge of human values, ethics, psychology and so on.

1438
01:15:47,100 --> 01:15:52,100
Is it the case that large language models make the alignment problem easier?

1439
01:15:52,100 --> 01:15:54,100
That is a great question.

1440
01:15:54,100 --> 01:15:56,100
I think that's one of the things we're currently figuring out, right?

1441
01:15:56,100 --> 01:16:01,100
From what I understand, open AI plans to use AI models to help solve the AI alignment problem.

1442
01:16:01,100 --> 01:16:07,100
And in a way, we need AI to assess and evaluate and to test to see if it is aligned

1443
01:16:07,100 --> 01:16:09,100
so that AI is inherently involved.

1444
01:16:09,100 --> 01:16:12,100
But to your general question is maybe.

1445
01:16:12,100 --> 01:16:15,100
And what I think is a perhaps interesting development of this.

1446
01:16:15,100 --> 01:16:17,100
So what if the AI system looks at all human knowledge, right?

1447
01:16:17,100 --> 01:16:21,100
And it says, you know what, I figured it out, guys, everyone, this is what you should do.

1448
01:16:21,100 --> 01:16:23,100
We're like, I don't want to do that.

1449
01:16:23,100 --> 01:16:27,100
But by your own standards, you said you cared about these things.

1450
01:16:27,100 --> 01:16:29,100
And then humans are like, no, no, but I didn't.

1451
01:16:29,100 --> 01:16:30,100
Not really.

1452
01:16:30,100 --> 01:16:31,100
Come on.

1453
01:16:31,100 --> 01:16:35,100
And so in the book, before I talk about the AI alignment problem, as people know it,

1454
01:16:35,100 --> 01:16:37,100
I talk about Isaac Asimov's Laws Robotics.

1455
01:16:37,100 --> 01:16:42,100
And I think this was just the very easy way into like simple rules to align computer systems.

1456
01:16:42,100 --> 01:16:43,100
Don't work.

1457
01:16:43,100 --> 01:16:45,100
When you say don't harm something, you're like, that seems reasonable.

1458
01:16:45,100 --> 01:16:46,100
That seems obvious.

1459
01:16:46,100 --> 01:16:47,100
Of course, put it in the machine.

1460
01:16:47,100 --> 01:16:49,100
Like, what does that mean exactly?

1461
01:16:49,100 --> 01:16:50,100
Like don't harm at all.

1462
01:16:50,100 --> 01:16:54,100
Like if someone needs to get surgery where they have to be cut into, does that count?

1463
01:16:54,100 --> 01:16:55,100
What if it's a risky surgery?

1464
01:16:55,100 --> 01:16:59,100
When you say don't harm anyone, does that include nonaction?

1465
01:16:59,100 --> 01:17:01,100
This is all the omission bias, right?

1466
01:17:01,100 --> 01:17:04,100
Where if you drown someone, we see that as a terrible thing.

1467
01:17:04,100 --> 01:17:08,100
If you let someone drown, well, we see that as a bad thing, but not usually as bad as

1468
01:17:08,100 --> 01:17:09,100
the intentional drowning.

1469
01:17:09,100 --> 01:17:13,100
So is an AI system now supposed to think, well, wait, I'm letting people suffer.

1470
01:17:13,100 --> 01:17:15,100
People are currently dying in poverty needlessly.

1471
01:17:15,100 --> 01:17:17,100
Should I be doing something about that?

1472
01:17:17,100 --> 01:17:21,100
Well, by your own standards, you said, don't allow humans to cause harm or come to harm.

1473
01:17:21,100 --> 01:17:22,100
What am I supposed to do now?

1474
01:17:22,100 --> 01:17:25,100
And you can see that very much disconnected maybe the thing short circuits.

1475
01:17:25,100 --> 01:17:26,100
Yeah.

1476
01:17:26,100 --> 01:17:29,100
Does harm imply any probability of harm?

1477
01:17:29,100 --> 01:17:31,100
Then you're kind of, you cannot move.

1478
01:17:31,100 --> 01:17:35,100
You cannot do anything because anything, any action at all could cause harm.

1479
01:17:35,100 --> 01:17:37,100
It just doesn't work.

1480
01:17:37,100 --> 01:17:38,100
No, exactly.

1481
01:17:38,100 --> 01:17:44,100
And so I think it'll be very useful for AI systems to provide insight, but like any sort

1482
01:17:44,100 --> 01:17:48,100
of human enterprise thing, we might get back an answer we don't want or that's very hard

1483
01:17:48,100 --> 01:17:52,100
for us and whether people will take that on board is also going to be highly variable.

1484
01:17:52,100 --> 01:17:56,100
Like for people who are very much interested in ethical reflection and philosophy, they

1485
01:17:56,100 --> 01:17:59,100
might have made like substantial progress and they realize, you know, my beliefs mean

1486
01:17:59,100 --> 01:18:02,100
I shouldn't do X and therefore I don't do X.

1487
01:18:02,100 --> 01:18:06,100
And a lot of us are like, I know I shouldn't do X, but sometimes I still do because I'm

1488
01:18:06,100 --> 01:18:08,100
a human and you know, again, progress is good.

1489
01:18:08,100 --> 01:18:11,100
It's not to say that people should be absolutist about these things.

1490
01:18:11,100 --> 01:18:15,100
It's just sort of highlights the difficulties of the human system, the human nature of the

1491
01:18:15,100 --> 01:18:16,100
whole thing.

1492
01:18:16,100 --> 01:18:20,100
Do you think current AI systems have self preservation?

1493
01:18:20,100 --> 01:18:21,100
Good question.

1494
01:18:21,100 --> 01:18:24,100
I would think I would probably defer to like, who's doing the most cutting edge research

1495
01:18:24,100 --> 01:18:27,100
now in the advanced labs?

1496
01:18:27,100 --> 01:18:31,100
From what I understand is slightly but not a lot.

1497
01:18:31,100 --> 01:18:36,100
There are some examples I think more in like the theoretical or there's like a prototype

1498
01:18:36,100 --> 01:18:40,100
where they've played around with certain systems and you know, simulated environments and the

1499
01:18:40,100 --> 01:18:45,100
system does seem to engage in certain behavior to protect itself to achieve a goal.

1500
01:18:45,100 --> 01:18:49,100
Whether it's full on self preservation as we commonly understand it, I don't think we're

1501
01:18:49,100 --> 01:18:53,100
there yet, but yeah, I would, I would kind of think like, well, there's probably some

1502
01:18:53,100 --> 01:18:57,100
paper on archive that I haven't had a chance to read yet that may say otherwise.

1503
01:18:57,100 --> 01:19:03,100
But do you think that AI systems will develop self preservation as we get more advanced

1504
01:19:03,100 --> 01:19:04,100
AI?

1505
01:19:04,100 --> 01:19:05,100
I do.

1506
01:19:05,100 --> 01:19:08,100
Or at least I think I do to the extent that we should be concerned about it.

1507
01:19:08,100 --> 01:19:12,100
Again, nothing's 100% here, but there's enough of a risk that, you know, the whole

1508
01:19:12,100 --> 01:19:15,100
Stuart Russell, you can't fetch coffee if you're dead thing.

1509
01:19:15,100 --> 01:19:18,100
To have a system do anything, it has to exist.

1510
01:19:18,100 --> 01:19:22,100
And to me, it is reasonable, it is plausible that to achieve anything to exist.

1511
01:19:22,100 --> 01:19:26,100
And once you know that, you might engage in various activities to ensure that you do exist.

1512
01:19:26,100 --> 01:19:31,100
Now, maybe there are ways to contain this or to circumvent it where, you know, you somehow

1513
01:19:31,100 --> 01:19:35,100
clearly specify a goal with a certain amount of error bars and then the system is supposed

1514
01:19:35,100 --> 01:19:37,100
to shut itself down after it's done that goal.

1515
01:19:37,100 --> 01:19:38,100
Perhaps.

1516
01:19:38,100 --> 01:19:42,100
But when we talked about before, the incentives for autonomous systems that are highly capable,

1517
01:19:42,100 --> 01:19:47,100
highly fast and so on, will then sort of have a disconnect with something that shuts itself

1518
01:19:47,100 --> 01:19:48,100
down all the time.

1519
01:19:48,100 --> 01:19:50,100
You imagine like, oh, I like to use my phone.

1520
01:19:50,100 --> 01:19:52,100
After I send one text, I shut the phone off and then I shut it.

1521
01:19:52,100 --> 01:19:55,100
I turn it back on like, well, that seems really painful and slow, right?

1522
01:19:55,100 --> 01:19:57,100
And people just might not do it.

1523
01:19:57,100 --> 01:20:02,100
So I think it is plausible that the systems will engage in such behavior in a way like

1524
01:20:02,100 --> 01:20:04,100
the expectation would be they probably would.

1525
01:20:04,100 --> 01:20:08,100
I guess I'm trying to say the default expectation to me is that something that's very, very intelligent

1526
01:20:08,100 --> 01:20:10,100
will probably engage in these behaviors.

1527
01:20:10,100 --> 01:20:14,100
So we should be on the lookout for it and really try to figure out if they are or they're not

1528
01:20:14,100 --> 01:20:16,100
versus the expectation that they wouldn't.

1529
01:20:16,100 --> 01:20:21,100
It doesn't seem to me that GPT-4 when I talk to it is trying to self preserve at all.

1530
01:20:21,100 --> 01:20:25,100
It's this my team naive, but it seems to me that I can just click stop on the chat whenever

1531
01:20:25,100 --> 01:20:27,100
I want or close the tab whenever I want.

1532
01:20:27,100 --> 01:20:29,100
And there's nothing the system can do.

1533
01:20:29,100 --> 01:20:35,100
Do you think self preservation will emerge together with more autonomous systems?

1534
01:20:35,100 --> 01:20:36,100
That's a great point.

1535
01:20:36,100 --> 01:20:39,100
So yes, right now, if you think about like, how could this thing be autonomous?

1536
01:20:39,100 --> 01:20:41,100
I literally, you know, close it.

1537
01:20:41,100 --> 01:20:42,100
I click the button and it goes away.

1538
01:20:42,100 --> 01:20:45,100
It's not like secretly hiding somewhere to our knowledge.

1539
01:20:45,100 --> 01:20:48,100
I guess there's a small probability that it is already.

1540
01:20:48,100 --> 01:20:49,100
And well, that's the thing.

1541
01:20:49,100 --> 01:20:53,100
That's why, by the way, I tried to be careful in the book where like AI that's super smart can do anything, right?

1542
01:20:53,100 --> 01:20:56,100
Because then you kind of get into these almost non falsifiable things.

1543
01:20:56,100 --> 01:21:00,100
Like, well, maybe it's secretly doing the thing and it's so good at deception that it looks like it isn't deceiving.

1544
01:21:00,100 --> 01:21:03,100
Now, I think there's something to it and we should be wary about it.

1545
01:21:03,100 --> 01:21:08,100
But I also think we have to be careful because those explanations are not satisfying to most people.

1546
01:21:08,100 --> 01:21:09,100
Oh, look, it can do anything.

1547
01:21:09,100 --> 01:21:10,100
Well, tell me about it.

1548
01:21:10,100 --> 01:21:11,100
Oh, anything.

1549
01:21:11,100 --> 01:21:12,100
You're like, well, I don't understand what you mean.

1550
01:21:12,100 --> 01:21:15,100
To your point, though, yes, right now I'm not concerned about that.

1551
01:21:15,100 --> 01:21:16,100
I don't see an issue with that.

1552
01:21:16,100 --> 01:21:28,100
That said, as we start to get beyond the GPT for or Gemini and these frontier models, I think it's a very important thing to assess not only before deployment, but in the training stage, there should be methods and benchmarks in place.

1553
01:21:28,100 --> 01:21:34,100
And even asking the labs, what are your expectations for the capabilities of your models throughout the process?

1554
01:21:34,100 --> 01:21:42,100
And if they're the lab themselves are like consistently wrong in a certain direction, like, oh, we thought it would be certainly capable and it ends up being more capable every time.

1555
01:21:42,100 --> 01:21:43,100
Like, that's not a good track record.

1556
01:21:43,100 --> 01:21:52,100
So next time when you say it's not going to be as capable as we thought it probably will be just some sort of way to get at how we might understand these things as it goes in the future.

1557
01:21:52,100 --> 01:21:54,100
Again, we currently have autonomous systems, right?

1558
01:21:54,100 --> 01:21:56,100
As I said, so they're doing banking stuff.

1559
01:21:56,100 --> 01:21:57,100
They're doing fraud detection.

1560
01:21:57,100 --> 01:21:59,100
They're doing cybersecurity things.

1561
01:21:59,100 --> 01:22:01,100
They're stopping missiles that are being bombed.

1562
01:22:01,100 --> 01:22:05,100
The incentives to have these things become more autonomous will be there.

1563
01:22:05,100 --> 01:22:09,100
And then again, if the system doesn't exist, it can't really do its job.

1564
01:22:09,100 --> 01:22:13,100
I want to be careful here that there's a sort of the as if goals, right?

1565
01:22:13,100 --> 01:22:18,100
That the system only needs to act as if it is engaging in self preservation.

1566
01:22:18,100 --> 01:22:20,100
It doesn't have to have like, I'm an AI.

1567
01:22:20,100 --> 01:22:21,100
I have a certain goal.

1568
01:22:21,100 --> 01:22:22,100
I need to exist.

1569
01:22:22,100 --> 01:22:25,100
It may do something like that, but it doesn't have to.

1570
01:22:25,100 --> 01:22:27,100
And I don't want us to sort of think it needs to be the case.

1571
01:22:27,100 --> 01:22:35,100
It's more just going to engage in various, we'll say from our perspective, reasonable goal oriented processes to make it more likely to achieve its goal.

1572
01:22:35,100 --> 01:22:40,100
And some of those will involve getting more power to ensure its own existence so we can serve that end.

1573
01:22:40,100 --> 01:22:44,100
Now again, maybe not, but there's enough of a maybe so that we should be very careful.

1574
01:22:44,100 --> 01:22:52,100
There's a bunch of emergent capabilities in AI systems that could be problematic if we're trying to align these systems.

1575
01:22:52,100 --> 01:23:01,100
So we're talking about power seeking, deception, manipulation of humans, using humans to achieve goals in the physical world and so on.

1576
01:23:01,100 --> 01:23:07,100
Which of these traits do you worry about the most and where do you think we are on the road to these traits?

1577
01:23:07,100 --> 01:23:13,100
How close do you think we are to having manipulative or deceptive or power seeking system?

1578
01:23:13,100 --> 01:23:17,100
I don't think we're there yet, but what could happen in a short period of time could definitely be the case.

1579
01:23:17,100 --> 01:23:26,100
So in the first example, just the emergence itself, I think a lot of people think the recent large language models, your chat GPT, GPT4 and cloud and whatnot,

1580
01:23:26,100 --> 01:23:29,100
they are largely emergent in a lot of their capabilities, right?

1581
01:23:29,100 --> 01:23:34,100
You have these systems that were trained on mainly English and then they can speak other languages.

1582
01:23:34,100 --> 01:23:41,100
That was not the plan, right? Or it was trained mostly on sort of corpus of text and then it can also do computer programming.

1583
01:23:41,100 --> 01:23:46,100
So it's not that if someone had thought through a lot of this, they would have realized, oh, maybe it will also do the thing.

1584
01:23:46,100 --> 01:23:54,100
It's that from our perspective, it did seem like certain behavior emerged in a way that was unexpected on a plan, at least in some cases in some domains.

1585
01:23:54,100 --> 01:23:57,100
So all I have to say is emergence seems to be like all over the place, right?

1586
01:23:57,100 --> 01:24:03,100
Especially if you try to prompt a model in a certain way and you get a certain thing which you didn't think it might do, but then it would.

1587
01:24:03,100 --> 01:24:13,100
It was just strange to me knowing something about the training process of GPT4, interacting with the system in my native language and talking to it perfectly in Danish

1588
01:24:13,100 --> 01:24:20,100
and seeing that it's pretty capable in my native language was kind of a surprise and an interesting experience.

1589
01:24:20,100 --> 01:24:26,100
And I believe that a lot of people have had that experience of talking to it in their native language.

1590
01:24:26,100 --> 01:24:31,100
And knowing that it wasn't trained specifically to do that, it's quite impressive.

1591
01:24:31,100 --> 01:24:38,100
I agree. I think it's staggeringly impressive. It's hard to think of a human parallel because clearly Danish was somewhere in the training set, right?

1592
01:24:38,100 --> 01:24:41,100
It's not like it read English and then it like invented Danish.

1593
01:24:41,100 --> 01:24:43,100
No, no, of course not. Just for everyone else.

1594
01:24:43,100 --> 01:24:47,100
But at the same time, it's like, well, you have a test coming up. It's an English test.

1595
01:24:47,100 --> 01:24:51,100
Here's a whack of material and like study all of it, but focus on the English like, okay.

1596
01:24:51,100 --> 01:24:55,100
And like, and then Danish like what? And of course, it's not just Danish. It's numerous of the languages.

1597
01:24:55,100 --> 01:24:59,100
It's also math, not usually the addition which it has trouble with like complicated math.

1598
01:24:59,100 --> 01:25:04,100
And if, you know, famous mathematicians like Terence Tau are using these systems, like it really helps improve my workflow.

1599
01:25:04,100 --> 01:25:08,100
You're like, okay, well, that's clearly a significant indicator of capability.

1600
01:25:08,100 --> 01:25:13,100
So you could also imagine if then someone's like, we really need a dedicated math system.

1601
01:25:13,100 --> 01:25:17,100
This goes to that general versus narrow, would it be more capable?

1602
01:25:17,100 --> 01:25:22,100
It seems to think like it should be more capable, but it's possible the general somehow is more capable, right?

1603
01:25:22,100 --> 01:25:31,100
And then fine tuning tweaks it to your other question, though, I am concerned to sort of thinking about sort of that security nature of things, right?

1604
01:25:31,100 --> 01:25:35,100
If you're looking through the world from a security lens, security mindset, you think like, where are the weak links, right?

1605
01:25:35,100 --> 01:25:42,100
And all companies and even individuals deal with this to some extent, and people can be manipulated in a sort of social engineering way, right?

1606
01:25:42,100 --> 01:25:47,100
People get information just by calling someone up, pretending to be someone else, or there's more overt cyber hacking and whatnot.

1607
01:25:47,100 --> 01:25:54,100
But with AI systems, it's almost like you take the normal problems that already exist, and then there are also still problems in the AI space.

1608
01:25:54,100 --> 01:26:00,100
So whether there's, you know, people who are bribeable, people who are manipulatable, people you can hire off the dark web.

1609
01:26:00,100 --> 01:26:05,100
Like it's one of these sort of things that most people don't like to think about for obvious reasons, but there are like hitmen.

1610
01:26:05,100 --> 01:26:10,100
You can hire them to kill people. They just don't hire them to kill you because you're not that important, right?

1611
01:26:10,100 --> 01:26:15,100
And thankfully, right? It's this whole weird world, you're like, what happens? What type of criminal activity?

1612
01:26:15,100 --> 01:26:21,100
Like there's actual criminals who do things. And so to think that AI systems won't liaise with, if they're trying to cause harm,

1613
01:26:21,100 --> 01:26:28,100
nefarious individuals who literally can be paid to do crime to make themselves more capable, it seems like that would be an oversight.

1614
01:26:28,100 --> 01:26:34,100
So I guess I'm concerned about a range of these things. Again, at the moment, not that concerned about that much of it.

1615
01:26:34,100 --> 01:26:39,100
But because it often takes months, years to address problems, you need the infrastructure in place now.

1616
01:26:39,100 --> 01:26:45,100
You need what is the problem type conversations. Problem definition is very important. People often speak past each other.

1617
01:26:45,100 --> 01:26:52,100
So if we can agree that an AI system that would have an ability to manipulate people, or that would have an ability to hire people to do certain tasks,

1618
01:26:52,100 --> 01:26:54,100
is a potential problem, that's a good start.

1619
01:26:54,100 --> 01:27:02,100
And I think we already saw when GPT-4 was being evaluated, there was that famous case where, again, the AI system itself did not do this,

1620
01:27:02,100 --> 01:27:12,100
but it was liaising with the researchers in between. And the GPT-4 was able to hire someone off task rabbit and lie about why it needed that person to fill out a capture,

1621
01:27:12,100 --> 01:27:20,100
to build a commuter code. Again, the system didn't do it, but it's like, well, it doesn't seem that complicated to connect those things or to have enabled the system to do it.

1622
01:27:20,100 --> 01:27:29,100
So if it was the case months ago, that if a system with a bit of tweaks could have hired someone off the internet to circumvent something, to stop AI systems,

1623
01:27:29,100 --> 01:27:34,100
and lie about why it did it, why wouldn't this be possible in the future? It's already happened.

1624
01:27:34,100 --> 01:27:39,100
It's like, okay, so now imagine something that's more people, more people flying, can sort of think through step by step on its own,

1625
01:27:39,100 --> 01:27:42,100
would know how to navigate a decision once it has more examples.

1626
01:27:42,100 --> 01:27:53,100
Again, this human history, fiction novels, movies, or even just current events, are replete with numerous examples of people deceiving each other and engaging in various complicated nefarious schemes.

1627
01:27:53,100 --> 01:27:57,100
And you could imagine that as a wonderful training data set for someone trying to cause harm.

1628
01:27:57,100 --> 01:28:10,100
Do you think we will get something like a unified solution to the alignment problem, like something that solves all of these problems that we just talked about, deception, manipulation, power-seeking, and so on?

1629
01:28:10,100 --> 01:28:21,100
Another way to ask maybe is, do you think we'll get something like the theory of evolution, which solves a bunch of distinct problems in a general and simple way,

1630
01:28:21,100 --> 01:28:27,100
or will it look more like whack-a-mole, solving one problem, moving on to the next problem?

1631
01:28:27,100 --> 01:28:30,100
I like this the option between evolution and whack-a-mole.

1632
01:28:31,100 --> 01:28:33,100
I said the evolution of whack-a-mole.

1633
01:28:33,100 --> 01:28:37,100
So I think it's probably a bit more the whack-a-mole.

1634
01:28:37,100 --> 01:28:41,100
And the reason here is, again, like the straightforward logic of human incentives and human behaviors.

1635
01:28:41,100 --> 01:28:46,100
So say someone develops a system that, as you said, reliably does what it's supposed to do.

1636
01:28:46,100 --> 01:28:49,100
Well, that means it could be used for good or bad, right?

1637
01:28:49,100 --> 01:28:59,100
At the moment, the AI systems, and I briefly mentioned this in the book, they're kind of sort of loosely corporate American Western values, like what's acceptable and what's not.

1638
01:28:59,100 --> 01:29:02,100
But what it does mean is that someone has their hand on the lever.

1639
01:29:02,100 --> 01:29:05,100
Someone is sort of manipulating the system to do X and not Y.

1640
01:29:05,100 --> 01:29:12,100
So it is already the case that certain values are being implemented or at least displayed through these systems of a certain type.

1641
01:29:12,100 --> 01:29:20,100
Now, if you just had the alignment of does what we want, if someone was a malevolent actor and wanted to cause harm, it could use the thing to do what we want.

1642
01:29:20,100 --> 01:29:26,100
So then you're like, well, is the alignment problem really going to solve not having bad people do bad things,

1643
01:29:26,100 --> 01:29:30,100
or not having, we'll say, even desperate or confused people inadvertently do bad things?

1644
01:29:30,100 --> 01:29:31,100
That's the other thing.

1645
01:29:31,100 --> 01:29:33,100
Well, I would say people could be bribed and manipulated.

1646
01:29:33,100 --> 01:29:42,100
People could also just be persuaded, like their child is very sick, they're desperate, they need money, or maybe like, oh, my child has a certain form of cancer is like, look, I can cure it.

1647
01:29:42,100 --> 01:29:43,100
I just read a thousand papers.

1648
01:29:43,100 --> 01:29:44,100
I just need some resources.

1649
01:29:44,100 --> 01:29:47,100
I understand desperation could also be a factor here.

1650
01:29:47,100 --> 01:29:54,100
So trying to get rid of, like, not say everyone's evil and nefarious out there, it's just like there's many reasons why someone might give up power.

1651
01:29:54,100 --> 01:29:59,100
And it's hard to imagine how sort of the alignment problem might sort of address all that.

1652
01:29:59,100 --> 01:30:03,100
In the earlier version of the book, actually, there were four different alignment problems I was going to talk about.

1653
01:30:03,100 --> 01:30:06,100
But I thought that was too complicated for my audience.

1654
01:30:06,100 --> 01:30:08,100
It really was like, you're not fully aligned with yourself.

1655
01:30:08,100 --> 01:30:10,100
We're not fully aligned with each other.

1656
01:30:10,100 --> 01:30:12,100
We're not necessarily fully aligned with AI.

1657
01:30:12,100 --> 01:30:17,100
And then AI itself may not be aligned with us or with itself if there's multiple AIs.

1658
01:30:17,100 --> 01:30:23,100
But I thought, okay, you know, let's let's streamline this to then think about as a gas model as robotics to make it easy that that type of line is a problem.

1659
01:30:23,100 --> 01:30:26,100
And then the more traditional the alignment problem stuff.

1660
01:30:26,100 --> 01:30:30,100
Right now, there's a bunch of proposals on the table for making AI safe.

1661
01:30:30,100 --> 01:30:34,100
There's a lot of attention on this issue and in policy circles.

1662
01:30:34,100 --> 01:30:41,100
And I think you had a great discussion in the book about principles for selecting among these proposals for AI safety.

1663
01:30:41,100 --> 01:30:44,100
Maybe you could talk a bit about these principles.

1664
01:30:44,100 --> 01:30:45,100
Sure thing.

1665
01:30:45,100 --> 01:30:47,100
So yes, there's a lot of great proposals out there.

1666
01:30:47,100 --> 01:30:53,100
But it's sometimes useful to take a step back and even think like, what's the framework that we're even using to think through these proposals?

1667
01:30:53,100 --> 01:31:01,100
And even if one set out like that's kind of obvious, like, sure, but let's put it down because sometimes what you think is obvious is obvious to you and not someone else.

1668
01:31:01,100 --> 01:31:03,100
Or you see where you might agree, right?

1669
01:31:03,100 --> 01:31:06,100
If you have five principles and I have five, maybe three overlap and that's great.

1670
01:31:06,100 --> 01:31:12,100
So I kind of tried to keep it simple and think through like, what are the three main ways we might want to think about this?

1671
01:31:12,100 --> 01:31:14,100
Or that should be a part of any principle.

1672
01:31:14,100 --> 01:31:17,100
And so that's verification and agility, adaptability.

1673
01:31:17,100 --> 01:31:18,100
That's the second one.

1674
01:31:18,100 --> 01:31:20,100
And the third one is defense in depth.

1675
01:31:20,100 --> 01:31:24,100
So verification is just realizing that we need to verify.

1676
01:31:24,100 --> 01:31:26,100
You know, it's nice to say trust, but verify.

1677
01:31:26,100 --> 01:31:28,100
But the idea is that everyone should be accountable.

1678
01:31:28,100 --> 01:31:29,100
There should be transparency.

1679
01:31:29,100 --> 01:31:32,100
There should be verification mechanisms built in.

1680
01:31:32,100 --> 01:31:37,100
And if actors in the space, the companies that are developing these things say there's no problem.

1681
01:31:37,100 --> 01:31:38,100
There should be no problem then.

1682
01:31:38,100 --> 01:31:40,100
Let's let's let's verify everything, right?

1683
01:31:40,100 --> 01:31:43,100
If you think you're not developing anything harmful, let's have that as a backbone.

1684
01:31:43,100 --> 01:31:51,100
We'll say of any proposal that you want to ensure that things are happening as they're understood to be happening as much as possible.

1685
01:31:51,100 --> 01:32:00,100
And even the idea that people will try to circumvent verification as they always do in this world, at least to some extent by some actors, that should also be built into the process.

1686
01:32:00,100 --> 01:32:02,100
So you can't just take people's words for it.

1687
01:32:02,100 --> 01:32:07,100
I can't remember the quote, but something's like they can't be grading their own homework here in these AI systems like these companies.

1688
01:32:07,100 --> 01:32:08,100
It just doesn't work that way.

1689
01:32:08,100 --> 01:32:11,100
And again, even if they're not necessarily nefarious, great.

1690
01:32:11,100 --> 01:32:14,100
We'll just have it all above board and have ensure that verification is there.

1691
01:32:14,100 --> 01:32:16,100
The second one, agility and adaptability.

1692
01:32:16,100 --> 01:32:24,100
Again, this isn't like a novel insight, but it's just really trying to highlight how fast moving the spaces and how we really have to think through.

1693
01:32:24,100 --> 01:32:26,100
What if it happens even faster, right?

1694
01:32:26,100 --> 01:32:32,100
That a lot of times in the policy regulatory legal space, things take months, years to work through the system.

1695
01:32:32,100 --> 01:32:34,100
And what if it has to be much less than that?

1696
01:32:34,100 --> 01:32:39,100
Or what if you have a law that you thought was useful that, you know, usually what happens is the laws developed.

1697
01:32:39,100 --> 01:32:41,100
It's somehow it comes to exist in the world.

1698
01:32:41,100 --> 01:32:43,100
Sometimes it interacts with challenges from the courts.

1699
01:32:43,100 --> 01:32:45,100
There's some sort of settled agreement.

1700
01:32:45,100 --> 01:32:48,100
The law seems to have some sort of a standard or consistency.

1701
01:32:48,100 --> 01:32:50,100
And then maybe it's challenging in the future.

1702
01:32:50,100 --> 01:32:53,100
That's a multi year, some nice multi decade process for the AI stuff.

1703
01:32:53,100 --> 01:32:55,100
It might have to be multiple months, multiple years.

1704
01:32:55,100 --> 01:33:03,100
So how can we even start thinking about changing almost the machinery of government and parts of the world to at least address these sorts of things?

1705
01:33:03,100 --> 01:33:08,100
Yes, you can have amendments to laws, but really thinking through that this is a factor and that people should be prepared.

1706
01:33:09,100 --> 01:33:11,100
For things happening faster than usual.

1707
01:33:11,100 --> 01:33:13,100
The third one, defense in depth.

1708
01:33:13,100 --> 01:33:16,100
This comes more from cybersecurity than the military definition.

1709
01:33:16,100 --> 01:33:18,100
And that's that we need multiple layers of defense.

1710
01:33:18,100 --> 01:33:24,100
You don't expect any one particular proposal or any one particular action to lead to safety or security.

1711
01:33:24,100 --> 01:33:26,100
But you're trying to have multiple layers.

1712
01:33:26,100 --> 01:33:32,100
So if any one of them fails and you actually expect them to fail, that there are others there to sort of pick up the slack.

1713
01:33:32,100 --> 01:33:33,100
Makes a lot of sense.

1714
01:33:33,100 --> 01:33:34,100
All of these principles.

1715
01:33:34,100 --> 01:33:39,100
So in the book, you you discuss eight proposals for for safe AI innovation.

1716
01:33:39,100 --> 01:33:42,100
Maybe you could talk about your favorites here.

1717
01:33:42,100 --> 01:33:44,100
What are the most important things?

1718
01:33:44,100 --> 01:33:46,100
It's like, how do you choose a favorite child?

1719
01:33:46,100 --> 01:33:48,100
Right? No, I just think the music.

1720
01:33:48,100 --> 01:33:51,100
So why there's eight, by the way, and why isn't there 10 and why isn't there seven?

1721
01:33:51,100 --> 01:33:53,100
Well, you know, eight seemed like a good number.

1722
01:33:53,100 --> 01:33:56,100
I think this really encapsulated what I thought were a good number.

1723
01:33:56,100 --> 01:34:02,100
Also, why I want these proposals or why I want these to be discussed is not that these are again definitive.

1724
01:34:02,100 --> 01:34:05,100
The whole book is trying to be very open minded solutions oriented.

1725
01:34:05,100 --> 01:34:08,100
We need more people working on this and we all need to come together to work on this.

1726
01:34:08,100 --> 01:34:12,100
So if it is the case that someone's like, you know what, most of your proposals don't work for me.

1727
01:34:12,100 --> 01:34:14,100
I'm like, OK, which ones do, right?

1728
01:34:14,100 --> 01:34:19,100
Because there's as we know, this is unfortunate tension that exists sometimes in different AI safety and ethics communities.

1729
01:34:19,100 --> 01:34:26,100
Or some people who are more focused on, we'll say present day concerns like algorithmic bias are taking issue with people who are more concerned about existential threats.

1730
01:34:26,100 --> 01:34:28,100
And this is an unfortunate division.

1731
01:34:28,100 --> 01:34:30,100
I mean, there's some rationale behind it, right?

1732
01:34:30,100 --> 01:34:37,100
I mean, there's some kind of representation of resources, but I think it's largely just an unfortunate way the world's turned out and it doesn't have to be this way.

1733
01:34:37,100 --> 01:34:43,100
So with the eight proposals, there could be like, OK, yes, some of those are maybe more X risk or existential risk oriented.

1734
01:34:43,100 --> 01:34:45,100
But what are the ones that work for you now?

1735
01:34:45,100 --> 01:34:46,100
Some sort of liability.

1736
01:34:46,100 --> 01:34:47,100
That's one of the proposals.

1737
01:34:47,100 --> 01:34:48,100
Great.

1738
01:34:48,100 --> 01:34:51,100
Some sort of a transparency or identification you're interacting with an AI system.

1739
01:34:51,100 --> 01:34:54,100
And that's where I'm trying to like, you know, all of branches all over the place.

1740
01:34:54,100 --> 01:34:59,100
Build bridges here that if certain proposals don't work, pick the ones that do or show me your list of eight.

1741
01:34:59,100 --> 01:35:04,100
And let's work on those together that seems like it's going to have the most broader support that is going to be good for all these issues.

1742
01:35:04,100 --> 01:35:09,100
I think if we took a step back, some sort of liability for these systems, again, why do people do anything?

1743
01:35:09,100 --> 01:35:10,100
Will they respond to incentives?

1744
01:35:10,100 --> 01:35:15,100
And if they are held liable, sometimes personally liable for how these systems malfunction, that is usually a good lever.

1745
01:35:15,100 --> 01:35:20,100
And it can't just be a sort of distributed sense of like, well, I did a thing, but I'm not really accountable.

1746
01:35:20,100 --> 01:35:22,100
Like, well, let's think through what makes the most sense here.

1747
01:35:22,100 --> 01:35:30,100
And maybe if you are responsible for distributing an AI model, even though you didn't create it, you do bear some of the liability here, some of the accountability.

1748
01:35:30,100 --> 01:35:34,100
I think compute governance is also a very important one here.

1749
01:35:34,100 --> 01:35:38,100
This is sort of like getting a sense of and controlling who has access to which chips.

1750
01:35:38,100 --> 01:35:45,100
The U.S. has already implemented some of these things with, you know, export controls on China and some recent additions to that, which are making it even more restrictive.

1751
01:35:45,100 --> 01:35:49,100
The book Chip War is a fantastic exploration of some of these issues.

1752
01:35:49,100 --> 01:35:57,100
And so in that sense, if you think of the main reasons why AI is increasing in capability, usually people think there's three main inputs, right?

1753
01:35:57,100 --> 01:36:05,100
You have the computational power, you have lots of data, and then you have like the algorithms itself, which are partly a human thing, like just talent pool that are building these things,

1754
01:36:05,100 --> 01:36:08,100
but also insights from science and other domains and even AI itself.

1755
01:36:08,100 --> 01:36:18,100
So how can you, as taking a big step back as a system, as a government, as an international organization of sorts, think about how to control or at least have a sense of influencing these inputs?

1756
01:36:18,100 --> 01:36:22,100
And data is kind of everywhere. It's really hard to stop people accessing data.

1757
01:36:22,100 --> 01:36:28,100
With the talent pool, the algorithms, that also seems like we want free mobility of labor for the most part, right?

1758
01:36:28,100 --> 01:36:31,100
We don't want to lock people up and tell them they can't work certain places or anything else. That seems bad.

1759
01:36:31,100 --> 01:36:35,100
But with the compute governance, it is more of a tangible physical thing.

1760
01:36:35,100 --> 01:36:43,100
There's a chip, which is understood at least to some extent of what it does and how it works, and getting a sense of where these chips go could be very, very useful.

1761
01:36:43,100 --> 01:36:48,100
Now, to reassure some people, this isn't for all AI systems. This is really the frontier AI model.

1762
01:36:48,100 --> 01:36:53,100
So for the average consumer, the average AI business, the average AI product, this doesn't really affect them at all.

1763
01:36:53,100 --> 01:37:01,100
It's more that are you using the most advanced chips and do you have thousands and thousands of them that you can put together in a cluster to then train highly capable models?

1764
01:37:01,100 --> 01:37:06,100
So it really is something that sort of like those taxes, like, oh, if you make more than $100 million, this is a tax.

1765
01:37:06,100 --> 01:37:09,100
And people are like, I would never want to be taxed like that. Like, well, don't worry, you never will.

1766
01:37:09,100 --> 01:37:14,100
So for the most part, it doesn't affect most people or most businesses. But I think that's very useful.

1767
01:37:14,100 --> 01:37:19,100
Now, how you go about it with the proposals, I have an idea, I have some sketch, I have some detail.

1768
01:37:19,100 --> 01:37:26,100
But I always want to say, like, let's think this through. What is the best version of this to go forward with the most recent evidence with the most recent analysis?

1769
01:37:26,100 --> 01:37:34,100
What is the best version? Is it useful to have, you know, ways that the chips can't communicate with each other as much so you can't bundle more like weaker chips together?

1770
01:37:34,100 --> 01:37:39,100
Or should there be some sort of remote kill switch even for the chips themselves so we can't shut down the Internet?

1771
01:37:39,100 --> 01:37:44,100
Maybe let's study it. Let's see if that's viable. If it turns out it's not for good or bad reasons, then we wouldn't do that.

1772
01:37:44,100 --> 01:37:48,100
But really trying to think through how can we access some of that stuff.

1773
01:37:48,100 --> 01:37:55,100
One of your proposals is about investing in AI safety research. So here I'm imagining a technical AI safety research.

1774
01:37:55,100 --> 01:38:04,100
At least when I try to try to see this from the perspective of a funder, I worry that it's extremely difficult to choose among proposals.

1775
01:38:04,100 --> 01:38:10,100
What should you fund? How should you respond to an applicant being optimistic about solving the problem?

1776
01:38:10,100 --> 01:38:15,100
Is pessimism a sign that you understand the problem in a deeper way or it's optimistic?

1777
01:38:15,100 --> 01:38:25,100
You know, there's so many complexities and in a sense this is just normal science funding, but I worry that this problem is even stronger in trying to fund AI safety.

1778
01:38:25,100 --> 01:38:32,100
Do you have any ideas about how to go about evaluating proposals for technical AI safety research?

1779
01:38:32,100 --> 01:38:41,100
I think you've raised a great point and the smile was just the idea that yes, if someone is more depressed or less optimistic about solving the problem, do they get more money, right?

1780
01:38:41,100 --> 01:38:45,100
Intellectual integrity and, you know, epistemic modesty and all these things and maybe that's the case.

1781
01:38:45,100 --> 01:38:55,100
So I would say that I don't necessarily have specifics. I think it's sort of like, do we agree that there are currently a lot more people that are trying to increase capabilities that are concerned about safety?

1782
01:38:55,100 --> 01:39:04,100
Like, do we agree this is a fact? Whether the numbers like 100 to 1 or more or less, you know, AI capabilities researchers versus safety researchers, do we agree there's some sort of large discrepancy?

1783
01:39:04,100 --> 01:39:12,100
And that probably shouldn't be the case. I think Ian Hogarth had that nice, like a sort of two chart graph, like, well, one line's going way up and the other one's not really going up and meeting it at all.

1784
01:39:12,100 --> 01:39:20,100
So if there is a disconnect between safety and capabilities, what should we do about that? And it seems like somehow funding safety research seems like a good idea.

1785
01:39:20,100 --> 01:39:26,100
Sort of like, let's start there. If we can't get agreement there, then that's an issue. But assuming there is some agreement, yes, how to actually go about it.

1786
01:39:26,100 --> 01:39:34,100
And here I would probably kind of defer to people who've already been in the space for several years and have them kind of talk to each other and see what are some current best practices.

1787
01:39:34,100 --> 01:39:42,100
You're right. It's an absolute mess. And how to study safety without increasing capabilities seems to be one of the biggest issues of all.

1788
01:39:42,100 --> 01:39:50,100
And as anthropic and other people have reasonably said, even though it's kind of a bitter total swallow, but we need the advanced AI system to study the safety of the system.

1789
01:39:50,100 --> 01:39:58,100
And it doesn't really make sense to try to think about the safety of, say, GPT-4 or GPT-5 if you're currently working on GPT-2 or GPT-3.

1790
01:39:58,100 --> 01:40:05,100
From what I understand, not that much of what you learned is going to be applicable and translatable. So I think it's a really important issue.

1791
01:40:05,100 --> 01:40:13,100
And I think it's just sort of really orienting people to realize, look how much capabilities are increasing and there is not nearly the attention paid on safety, not at all.

1792
01:40:13,100 --> 01:40:21,100
And in fact, some of the organizations don't care at all. And so if people really onboarded that, okay, some of these people seem to be more responsible actors and some of them don't care about safety at all,

1793
01:40:21,100 --> 01:40:29,100
they're just trying to make the product as fast as possible and dump it into the world, like I think it was the Mistral AI, then we should be concerned.

1794
01:40:29,100 --> 01:40:35,100
Great. There's a lot of complexities there. We could talk for hours on this topic. Darren, thanks for talking with me. It's been a pleasure.

1795
01:40:35,100 --> 01:40:37,100
My pleasure as well, Gus. Thanks so much.

