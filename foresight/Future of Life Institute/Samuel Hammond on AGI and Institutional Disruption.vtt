WEBVTT

00:00.000 --> 00:03.360
Welcome to the Future of Life Institute podcast.

00:03.360 --> 00:04.520
My name is Gus Docher,

00:04.520 --> 00:06.480
and I'm here with Samuel Hammond,

00:06.480 --> 00:08.760
who is a senior economist at

00:08.760 --> 00:11.160
the Foundation for American Innovation.

00:11.160 --> 00:13.040
Samuel, welcome to the podcast.

00:13.040 --> 00:14.400
I guess. Thanks for having me.

00:14.400 --> 00:16.000
Fantastic. All right.

00:16.000 --> 00:18.280
I have so much I want to talk to you about,

00:18.280 --> 00:21.240
but I think a natural place to start here would be with

00:21.240 --> 00:24.080
your timelines to AGI.

00:24.080 --> 00:29.280
Why is it that you expect AGI to get here before most people?

00:29.280 --> 00:31.120
Well, I don't really know what most people think.

00:31.120 --> 00:34.720
I think the world divides into people who are paying

00:34.720 --> 00:38.560
attention and people who are basically normies.

00:38.560 --> 00:42.480
In my day job, I work on Capitol Hill and in Washington,

00:42.480 --> 00:44.960
D.C., talking to folks about AGI.

00:44.960 --> 00:48.400
If you think about what people's implicit timelines are,

00:48.400 --> 00:52.040
you can read out people's implicit timelines by their behavior.

00:52.040 --> 00:55.200
I know Paul Cristiano has short timelines

00:55.200 --> 00:58.040
because he's doubled up into the stock market.

00:58.440 --> 01:01.160
He's practicing what he preaches.

01:01.160 --> 01:04.960
But then when you have Sam Altman testifying to work to Congress,

01:04.960 --> 01:07.960
I like to say people are taking him seriously,

01:07.960 --> 01:09.800
but not literally. He's saying,

01:09.800 --> 01:11.660
we're going to develop something like AGI,

01:11.660 --> 01:12.880
potentially this decade,

01:12.880 --> 01:15.440
and superintelligence thereafter.

01:15.440 --> 01:18.120
Then you have folks like Sandra Marshall Blackburn

01:18.120 --> 01:20.920
being like, what will this mean for music royalties?

01:21.920 --> 01:26.000
When the focus of policymakers is things like

01:26.040 --> 01:29.520
music royalties or impact on copyright,

01:29.520 --> 01:31.680
it's not that those are invalid issues.

01:31.680 --> 01:35.440
It's that they belie relatively longer timelines.

01:35.440 --> 01:37.520
Then we also have this definitional confusion

01:37.520 --> 01:42.040
where folks like John Lacoon would say AGI is probably decades away

01:42.040 --> 01:45.440
because he is using AGI to mean something that learns,

01:45.440 --> 01:48.280
like a human learns in the sense that it's born

01:48.280 --> 01:51.400
as a relative blank slate and can acquire language

01:51.400 --> 01:54.320
with very few examples.

01:54.320 --> 01:56.680
People have these moving goalposts of what they mean.

01:56.680 --> 02:01.680
For me, I think we can avoid those definitional conflicts

02:01.760 --> 02:04.680
if we just talk about human level intelligence,

02:04.680 --> 02:08.480
and humans are quite general or generally intelligent.

02:08.480 --> 02:12.920
That's what separates us from animals in a lot of respects.

02:12.920 --> 02:15.880
And when you look at how machine learning models

02:15.880 --> 02:17.280
are being trained today,

02:17.280 --> 02:20.280
like large language models and now multimodal models,

02:20.280 --> 02:22.000
they're being trained on human data,

02:22.000 --> 02:27.680
and they're being trained to reproduce the kinds of behaviors

02:27.680 --> 02:30.880
and tasks and outputs that humans output.

02:30.880 --> 02:33.000
And so they're kind of like an indirect way

02:33.000 --> 02:35.080
of emulating human intelligence.

02:35.080 --> 02:39.760
And then so if you benchmark AI progress to that,

02:39.760 --> 02:42.320
then you can sort of put information theoretic bounds

02:42.320 --> 02:45.480
on what's the likely timeline

02:45.480 --> 02:48.360
to basically an ideal human emulator,

02:48.360 --> 02:52.640
something that can extract the sort of base representations,

02:52.640 --> 02:55.000
the internal representations of our brain

02:55.000 --> 02:59.160
through the indirect path of the data that our brain generates.

02:59.160 --> 03:01.600
Yeah, you have an interesting sentence where you write that

03:01.600 --> 03:04.240
AI can advance by emulating the generator

03:04.240 --> 03:07.680
of human-generated data, which is simply the brain.

03:07.680 --> 03:11.280
Do you think this paradigm holds all the way to AGI?

03:11.280 --> 03:14.120
I think it holds this decade to systems

03:14.120 --> 03:18.440
that in principle can in context learn anything humans do.

03:18.440 --> 03:19.880
Again, this is a semantic question.

03:19.880 --> 03:21.640
Do you want to call that AGI or not?

03:21.640 --> 03:25.960
I think there are still outstanding issues around the limits

03:25.960 --> 03:28.840
of other aggressive models for autonomy

03:28.840 --> 03:32.720
and the question of sort of real-time learning,

03:32.720 --> 03:34.280
the way we train these models,

03:34.280 --> 03:36.280
we sort of are freezing a crystal in place

03:36.280 --> 03:40.120
and humans are continuously learning.

03:40.120 --> 03:45.720
So there still are genuine potential architectural gaps,

03:45.720 --> 03:48.840
but from the practical point of view,

03:48.840 --> 03:51.080
from the economic point of view,

03:51.080 --> 03:53.200
we don't need to debate whether something is conscious

03:53.200 --> 03:56.640
or whether something learns strictly the way humans learn

03:56.640 --> 04:00.160
if it demonstrably can do the things humans do, right?

04:00.160 --> 04:05.240
And that goes to the original insight of the Turing test, right?

04:05.240 --> 04:06.840
It's sometimes presented as a thought experiment,

04:06.840 --> 04:08.320
but what Alan Turing was getting at

04:09.040 --> 04:12.680
was if you can't distinguish between the human and a computer,

04:12.680 --> 04:17.200
in some ways, indistinguishability implies competence, right?

04:17.200 --> 04:19.480
And we can broaden that from just language

04:19.480 --> 04:22.200
because arguably we've surpassed the Turing test,

04:22.200 --> 04:24.000
at least a weaker version of it,

04:24.000 --> 04:27.560
to human performance on tasks in general, right?

04:27.560 --> 04:31.720
If we have a system that can output a scientific manuscript

04:31.720 --> 04:34.560
that experts in the field can't distinguish from a human,

04:34.560 --> 04:40.320
then debating whether this is real AGI or not

04:40.320 --> 04:43.520
is, I feel, academic.

04:43.520 --> 04:47.480
It is surprising in a sense that when you interact with GPT-4,

04:47.480 --> 04:50.880
for example, and it can do all kinds of amazing things

04:50.880 --> 04:54.120
and organize information, present information to you,

04:54.120 --> 04:56.560
but then it can't, or at least at some point,

04:56.560 --> 04:58.680
it couldn't answer questions about the world

04:58.680 --> 05:02.920
after September 2021 or a date like that.

05:02.920 --> 05:06.080
That would be surprising if you presented that fact

05:06.080 --> 05:09.120
to an AI scientist 20 years ago.

05:09.120 --> 05:11.400
For how long do you think we'll remain in this paradigm

05:11.400 --> 05:13.880
of training a foundational model

05:13.880 --> 05:16.080
and then deploying that model?

05:16.080 --> 05:16.920
I mean, it's worse than that.

05:16.920 --> 05:19.040
I think it was surprised people five years ago.

05:19.040 --> 05:20.520
Progress is sort of moving along two tracks.

05:20.520 --> 05:21.440
There's the industry track

05:21.440 --> 05:24.360
and the peer research academic track,

05:24.360 --> 05:27.400
and they're obviously having feedback with one another.

05:27.400 --> 05:31.080
The peer industry track is just looking to create tools

05:31.080 --> 05:33.480
that have practical value

05:33.480 --> 05:35.320
and can improve products and so forth.

05:35.320 --> 05:38.840
And so, Meta has their own GPU cluster

05:38.840 --> 05:40.080
and their training models,

05:40.080 --> 05:43.920
so they can have fun chatbots in their messenger.

05:43.920 --> 05:47.560
And so, those kinds of things are going to progress,

05:47.560 --> 05:50.800
I think, well within the current paradigm

05:50.800 --> 05:53.720
because we know the paradigm works,

05:53.720 --> 05:57.520
basically deep learning and transformers.

05:57.520 --> 05:59.640
And there's lots of marginality on the side,

05:59.640 --> 06:03.560
but that basic framework seems to be quite effective.

06:03.560 --> 06:04.520
And just scaling that up

06:04.520 --> 06:07.240
because we haven't sort of hit the range

06:07.240 --> 06:10.160
of irreducible loss and what transformers can do.

06:10.160 --> 06:13.040
Meanwhile, there's also this parallel peer research track

06:13.040 --> 06:15.560
where people on seemingly a weekly basis

06:15.560 --> 06:19.640
are finding better ways of specifying the loss function,

06:19.640 --> 06:22.640
ways of improving upon power loss scaling

06:22.640 --> 06:24.440
and all these different,

06:25.600 --> 06:26.720
sometimes they're new architectures,

06:26.720 --> 06:28.920
but often they're just like bags of tricks.

06:29.920 --> 06:32.160
And those bags of tricks then,

06:32.160 --> 06:35.160
to the extent that they comport with

06:35.160 --> 06:38.520
the paradigm industries running with,

06:38.520 --> 06:39.880
they can be reintegrated

06:39.880 --> 06:42.760
and end up accelerating progress and industry as well.

06:43.880 --> 06:47.760
So, do you think scale of compute is the main barrier

06:47.760 --> 06:50.080
to getting to human level AI?

06:51.000 --> 06:54.320
Yes, right, I mean, it's not all we need,

06:54.320 --> 06:57.600
but it's the main unlock, right?

06:57.600 --> 07:01.720
To what extent can more compute be used to trade off

07:01.720 --> 07:06.720
for lower quality data or for lower quality algorithms?

07:07.360 --> 07:08.800
Can you just throw more compute

07:08.800 --> 07:12.760
and solve the other factors in that equation?

07:12.760 --> 07:14.880
It depends on the thing you're trying to solve for.

07:14.880 --> 07:19.120
In principle, if we're talking about mapping inputs to outputs,

07:19.120 --> 07:21.200
then transformers are known

07:21.200 --> 07:23.200
to be universal function approximators.

07:23.200 --> 07:26.800
And so the answer is yes.

07:26.800 --> 07:28.840
That doesn't mean that they're necessarily efficient

07:28.840 --> 07:31.520
at approximating everything we want them to approximate.

07:31.520 --> 07:35.160
And sometimes universal function approximation theorems

07:35.160 --> 07:37.360
can be kind of trivial because they'll be like, okay,

07:37.360 --> 07:39.040
if your neural network is infinite width,

07:39.040 --> 07:41.440
then yes, we can approximate everything.

07:41.440 --> 07:45.680
The key fact is both that they're universal approximators

07:45.680 --> 07:48.520
and also that they're relatively sample efficient,

07:48.520 --> 07:50.720
at least relative to things we found in the past.

07:50.720 --> 07:51.960
And so that to me suggests that yes,

07:51.960 --> 07:55.760
they can compensate for things that they're bad at.

07:55.760 --> 07:57.960
On the other hand, the way research is trending

07:57.960 --> 07:59.640
is towards these mixed models,

08:00.680 --> 08:03.000
ensembles of different kinds of architectures,

08:03.000 --> 08:04.960
things like the recent Q transformer

08:04.960 --> 08:06.680
announced from Google DeepMind,

08:06.680 --> 08:09.640
which just sort of uses a combination of transformers

08:09.640 --> 08:13.440
and Q learning to have sort of the associational memory

08:13.440 --> 08:16.640
and sample efficiency of transformers

08:16.640 --> 08:19.840
with the ability to assign policies to do tasks

08:19.840 --> 08:21.640
that you get from reinforcement learning.

08:21.640 --> 08:23.480
So I imagine that there's going to be all kinds

08:23.480 --> 08:25.140
of mixing and matching.

08:25.580 --> 08:29.580
The key point is that in that space of architectures,

08:29.580 --> 08:33.220
it's a relatively sort of finite search space, right?

08:33.220 --> 08:35.940
And as an economist, economists believe

08:35.940 --> 08:39.060
that supply is long run elastic, right?

08:39.060 --> 08:42.100
And so there's this famous bet from the team,

08:42.100 --> 08:45.500
Paul Ehrlich and Julian Simon vis-a-vis the population bomb

08:45.500 --> 08:47.340
and whether population growth would lead

08:47.340 --> 08:51.340
to sort of a mothusian purge.

08:51.340 --> 08:53.860
And Julian Simon being the economist

08:53.860 --> 08:57.580
recognized that if prices rise for these core commodities,

08:57.580 --> 09:00.180
then that will spur research and development

09:00.180 --> 09:02.380
into extracting new resources, right?

09:02.380 --> 09:04.060
So he didn't have to know

09:04.060 --> 09:06.340
that fracking would be a technology.

09:06.340 --> 09:09.580
He understood that if oil prices went too high,

09:09.580 --> 09:12.300
people would find new oil reserves.

09:12.300 --> 09:14.900
And I think there's, I have an analogous instinct

09:14.900 --> 09:18.060
when it comes to progress and deep learning,

09:18.060 --> 09:20.060
meaning you can become too anchored

09:20.060 --> 09:23.420
to sort of the current state of the literature,

09:23.420 --> 09:26.900
but over a tenure horizon, you can say,

09:26.900 --> 09:29.900
well, there's a huge search on a huge gold rush

09:29.900 --> 09:34.580
to find the right way of blending these architectures.

09:34.580 --> 09:37.020
And I don't need to know in advance,

09:37.020 --> 09:38.940
which is the right way to do that

09:38.940 --> 09:41.940
to have high confidence that someone will find it.

09:41.940 --> 09:45.180
Yeah, we can sometimes, if we're too deep in the literature,

09:45.180 --> 09:49.060
we might lose the focus on the forest for the trees

09:49.060 --> 09:49.900
in a sense.

09:49.900 --> 09:52.100
And if we zoom out, we can just see

09:52.100 --> 09:55.140
that there's more investment, there's more talent

09:55.140 --> 09:57.060
pouring into AI, and so we can predict

09:57.060 --> 09:59.460
that something is gonna come out of that.

09:59.460 --> 10:01.220
You have lots of interesting insights

10:01.220 --> 10:02.460
about information theory

10:02.460 --> 10:05.980
and how this can help us predict AI.

10:05.980 --> 10:08.900
What's the most important lessons from information theory?

10:08.900 --> 10:13.460
The reason I start there is because sort of,

10:13.460 --> 10:14.980
within the conceptual realm,

10:14.980 --> 10:17.540
it's sort of like the most general thing

10:17.540 --> 10:20.020
that bounds everything else.

10:20.020 --> 10:22.460
And when you look back at the record of, say,

10:22.460 --> 10:26.460
Ray Kurzweil, I first read The Age of Spiritual Machines

10:26.460 --> 10:29.100
when I was a kid, and in there,

10:29.100 --> 10:30.700
he makes a prediction that we'll have

10:30.700 --> 10:34.580
AIs that pass the Turing test by 2029 or so.

10:34.580 --> 10:36.820
And when was this book written?

10:36.820 --> 10:38.020
1999.

10:38.020 --> 10:39.500
Yeah, that's pretty good.

10:39.500 --> 10:42.340
Right, and so, and people will complain

10:42.340 --> 10:43.180
that he got things wrong,

10:43.180 --> 10:47.220
because he said, well, I'll be wearing AR glasses by 2019,

10:48.140 --> 10:51.300
when in fact, Google Glass came out in 2013,

10:51.300 --> 10:54.620
and now we have Meta Glasses five years later.

10:54.620 --> 10:57.100
So he was wrong on the exact timing,

10:57.100 --> 10:59.020
but sort of right where the technology was wrong,

10:59.020 --> 11:02.060
where the minimal viable product was.

11:02.060 --> 11:05.220
But nonetheless, if you look at his track record,

11:05.220 --> 11:08.740
it's quite good for a methodology

11:08.740 --> 11:13.100
as relatively stupid as just staring at Moore's Law,

11:13.100 --> 11:15.220
and extrapolating it out.

11:15.220 --> 11:17.860
And I think that reveals the power

11:17.860 --> 11:21.580
of these information theoretic methodologies to forecasting

11:21.580 --> 11:24.260
because they set bounds on what will be possible.

11:24.260 --> 11:27.220
The team at epoch.ai have a forecast

11:27.220 --> 11:30.020
called the direct approach where it's sort of,

11:30.020 --> 11:33.220
you can think of it sort of like a way of putting bounds

11:33.220 --> 11:37.380
on when we'll have AIs that can emulate human performance

11:37.380 --> 11:39.580
through an information theoretic lens

11:39.580 --> 11:41.940
where they're looking at sort of how much entropy

11:41.940 --> 11:43.420
does the brain sort of process

11:43.460 --> 11:46.340
and how much compute will we have over time

11:46.340 --> 11:49.220
and what's implied by AI scaling laws.

11:49.220 --> 11:51.860
And you sort of put those three things together

11:51.860 --> 11:53.220
and you can sort of set bounds

11:53.220 --> 11:56.180
on when we'll basically be able to brute force

11:56.180 --> 11:58.660
human level intelligence.

11:58.660 --> 12:00.020
And of course, that's an upper bound

12:00.020 --> 12:01.540
because we're going to do better than brute force.

12:01.540 --> 12:03.100
We're going to also have insights

12:03.100 --> 12:05.740
from cognitive science and neuroscience

12:05.740 --> 12:10.740
and also ways of distilling neural networks and so forth

12:10.740 --> 12:12.300
and better ways of curating data.

12:12.300 --> 12:16.060
So their modal estimate for human level AI is 2029

12:16.060 --> 12:17.740
and their meeting is like 2036.

12:17.740 --> 12:20.620
And I've talked to the authors and they lean towards

12:20.620 --> 12:24.940
the 2029, 2030 for their own personal forecasts.

12:24.940 --> 12:28.940
And so going back to, is this a net liar?

12:28.940 --> 12:30.820
Am I out on a limb here?

12:30.820 --> 12:32.980
I think among our circles probably not,

12:32.980 --> 12:36.220
but among Congress and among the broader public,

12:36.220 --> 12:38.060
I think people are seeing sort of,

12:38.060 --> 12:40.660
they think everything's an asymptote, right?

12:41.180 --> 12:43.020
They're imagining, okay, we have these chatbots

12:43.020 --> 12:45.180
and they're not seeing the next step.

12:45.180 --> 12:47.940
I see a very smooth path from here to systems

12:47.940 --> 12:50.540
that can basically in context learn

12:50.540 --> 12:52.460
any arbitrary human task.

12:52.460 --> 12:53.780
And so what does that look like?

12:53.780 --> 12:57.020
It looks like systems that can basically sit over your shoulder

12:57.020 --> 12:59.900
or can monitor your desktop, your operating system as you work

12:59.900 --> 13:03.220
and watch you for an hour or two and then take over.

13:03.220 --> 13:06.500
And that'll be key to overcoming lack of training data

13:06.500 --> 13:10.540
or why is it important that they can learn in context?

13:10.740 --> 13:14.580
Well, in context learning is sort of the secret source

13:14.580 --> 13:16.580
of the power of transformer models.

13:16.580 --> 13:19.180
They learn these inductive biases and induction heads

13:19.180 --> 13:21.140
and so forth that let them,

13:21.140 --> 13:22.660
a few shot learn different tasks.

13:22.660 --> 13:25.660
So, GPT-4 is very good at zero shot learning

13:25.660 --> 13:27.140
on a variety of different things,

13:27.140 --> 13:29.580
but it's incredibly good at few shot learning.

13:29.580 --> 13:30.740
If you give it a few examples,

13:30.740 --> 13:32.820
it can kind of pick up where you left off.

13:32.820 --> 13:33.860
You know, when I think about myself,

13:33.860 --> 13:36.040
when I wanna learn a new recipe, right?

13:36.040 --> 13:37.140
I can go read a recipe book,

13:37.140 --> 13:39.500
but often what I prefer to do is to go on YouTube

13:39.500 --> 13:41.980
and watch someone make that recipe, right?

13:41.980 --> 13:45.820
And just by watching that person put together the stir fry,

13:45.820 --> 13:47.540
I have enough of a world model

13:47.540 --> 13:50.120
and enough of knowledge of how to cook in general

13:50.120 --> 13:53.700
that I can sort of in context learn

13:53.700 --> 13:56.380
how to pick up from there and do that recipe myself.

13:56.380 --> 13:57.460
LLMs do that already.

13:57.460 --> 14:00.140
Multimodal models are increasingly doing that.

14:00.140 --> 14:02.980
Some of the recent progress in robotics,

14:03.800 --> 14:05.920
like I mentioned, the Q-transformer paper,

14:05.920 --> 14:08.100
it shows that you can basically build robots

14:08.140 --> 14:09.660
as a basic world model

14:09.660 --> 14:11.340
and then have it learn new tasks

14:11.340 --> 14:14.380
with fewer than 100 examples of the human demonstration.

14:14.380 --> 14:15.860
So the human sort of demonstrates the task

14:15.860 --> 14:19.340
and the robot can pick it up and take it from there.

14:19.340 --> 14:21.700
And why that's important is both

14:21.700 --> 14:24.700
for understanding the trajectory of AI,

14:24.700 --> 14:28.420
but also its economic implementation

14:28.420 --> 14:31.140
because we're sort of used to automation

14:31.140 --> 14:33.940
being this thing where you get a contract from IBM

14:33.940 --> 14:37.460
and you spend many millions of dollars with consultants

14:37.460 --> 14:38.740
and they build you some bespoke thing

14:38.740 --> 14:40.500
that doesn't really work very well

14:40.500 --> 14:41.940
and requires lots of maintenance.

14:41.940 --> 14:45.220
And so people have this sort of prior that AI,

14:45.220 --> 14:48.940
even if it's near, will be rate limited by the real world

14:48.940 --> 14:51.860
because of all the complexity of implementation.

14:51.860 --> 14:54.780
But the point is if you have things that can in context learn

14:54.780 --> 14:57.620
and perform sort of as humans perform,

14:57.620 --> 14:59.980
then you don't need to change the process.

14:59.980 --> 15:02.980
You can take human designed processes

15:02.980 --> 15:06.220
and have the AI just fill in for the human.

15:06.260 --> 15:07.420
And so it leads to this paradox

15:07.420 --> 15:08.940
where we're probably going to have AGI

15:08.940 --> 15:11.220
before we get rid of the last fax machine.

15:11.220 --> 15:16.220
Yeah, when we think of say old IT systems

15:16.940 --> 15:20.700
in large institutions, we might think of moving

15:20.700 --> 15:25.700
from analog storage of information to the cloud.

15:25.900 --> 15:28.340
That's still going on in some institutions.

15:28.340 --> 15:31.340
That transformation has taken over a decade now.

15:31.340 --> 15:34.660
And so what exactly is it that makes AI different here?

15:34.660 --> 15:37.060
It is that AI plugs in directly

15:37.060 --> 15:38.700
where the human worker would be?

15:38.700 --> 15:40.420
Yeah, precisely.

15:40.420 --> 15:44.180
You don't need to redesign existing process

15:44.180 --> 15:47.340
to sort of plug into the automation.

15:47.340 --> 15:50.860
And that applies both for sort of the structure of tasks.

15:50.860 --> 15:53.740
So much of a mechanical automation

15:53.740 --> 15:57.260
takes something like the sort of artisanal work

15:57.260 --> 16:01.140
of a shoemaker and has to translate it

16:01.140 --> 16:03.540
into something repetitive that a machine

16:03.540 --> 16:07.300
or an automatic seamstress can do over and over and over

16:07.300 --> 16:09.460
or against our older school kind of automation

16:09.460 --> 16:12.980
requires sort of collapsing a task into a lower dimension

16:12.980 --> 16:16.660
so that simple automations can handle it.

16:16.660 --> 16:20.100
But when you have AGI, the whole point is generality.

16:20.100 --> 16:23.140
It's a flexible intelligence

16:23.140 --> 16:26.780
that can map to existing kinds of processes.

16:26.780 --> 16:29.380
So that's sort of why I think this will catch people

16:29.380 --> 16:32.220
by surprise because it's not just that AGI

16:32.220 --> 16:35.180
could be this decade, but that when it arrives

16:35.180 --> 16:37.660
and sort of crosses some thresholds of reliability,

16:37.660 --> 16:39.980
the implementation frictions could be very low.

16:39.980 --> 16:44.700
And do you expect, would AI have to get all the way there

16:44.700 --> 16:47.300
in order to substitute for a human worker?

16:47.300 --> 16:50.020
I mean, I would expect it to be a bit more gradual

16:50.020 --> 16:52.620
than that, taking over say 20% of tasks

16:52.620 --> 16:55.420
before 40% of tasks, 60% of tasks and so on.

16:55.420 --> 16:58.940
But here we're imagining that the AI kind of plugs in

16:58.940 --> 17:00.820
for the human worker for all tasks

17:00.820 --> 17:02.620
or what do you have in mind?

17:02.620 --> 17:04.740
These things are, yeah, you're right, much more continuous.

17:04.740 --> 17:07.380
It's not an on or off switch in part

17:07.380 --> 17:11.300
because the requisite threshold of reliability

17:11.300 --> 17:13.780
varies by the type of task.

17:13.780 --> 17:17.220
Arguably self-driving cars like Waymo or Tesla

17:17.220 --> 17:19.540
have matched human performance,

17:19.540 --> 17:21.940
but regulators want them to be 100x better than human

17:21.940 --> 17:26.780
before they're loose on the road because of safety.

17:26.780 --> 17:31.780
Codex and coding models are arguably still much worse today

17:31.780 --> 17:35.460
than elite programmers, but everyone is using them

17:35.460 --> 17:38.660
because even if it generates 50% of your code

17:38.660 --> 17:39.900
and you have to go back in and debug,

17:39.900 --> 17:42.580
it's still a huge productivity boost.

17:42.580 --> 17:45.580
So I think it will vary by occupation,

17:45.580 --> 17:48.780
by sort of task category, sort of modulo,

17:48.780 --> 17:52.780
the risks and stakes involved in those tasks.

17:52.780 --> 17:53.980
Yeah, I guess then the question is,

17:53.980 --> 17:56.540
how many of our jobs fall into the,

17:56.540 --> 17:58.060
is more like self-driving cars

17:58.060 --> 18:01.540
and how many of our jobs is more like programming?

18:01.540 --> 18:05.220
Right, I mean, I've been in a manager position before

18:05.220 --> 18:07.660
and I've had research assistants and interns

18:07.660 --> 18:10.700
and I know that they're like a very lossy compression

18:10.700 --> 18:13.340
of the thing I want to do.

18:13.340 --> 18:16.580
And so they require oversight and sort of co-poniting.

18:16.580 --> 18:18.220
We're sort of in that stage now with AIs

18:18.220 --> 18:20.900
in a variety of different kinds of tasks.

18:21.100 --> 18:25.020
I recently read a paper evaluating the use of GPT-4

18:25.020 --> 18:27.020
for peer review and science

18:27.020 --> 18:32.020
and it found that GPT-4 would write reviews of work

18:32.780 --> 18:34.660
that bore some striking correlations

18:34.660 --> 18:38.780
with the points raised by human reviewers,

18:38.780 --> 18:39.940
but also let some things out.

18:39.940 --> 18:43.420
And so it concluded by saying GPT-4

18:43.420 --> 18:47.500
could be an invaluable tool for scientific review,

18:47.500 --> 18:50.220
but it's not about to replace people.

18:50.220 --> 18:54.320
And that's just a case of like, okay, give it five years.

18:55.460 --> 18:57.940
Yeah, this is a phenomena you often see

18:57.940 --> 19:01.140
with some AI models out there

19:01.140 --> 19:04.700
and it has some capabilities, but lacks other capabilities.

19:04.700 --> 19:07.900
And then people might kind of over anchor

19:07.900 --> 19:09.900
on the present capabilities

19:09.900 --> 19:13.180
and not foresee the way the development is going.

19:13.180 --> 19:15.700
I think people are continually surprised

19:15.700 --> 19:17.860
at the advancement of AI.

19:17.860 --> 19:18.940
Yeah, absolutely.

19:18.940 --> 19:23.260
Ramiz Naam, the sci-fi author and futurist

19:23.260 --> 19:27.380
and energy investor, he gives his talk on solar energy

19:27.380 --> 19:30.780
and other renewables and he has this famous graph

19:30.780 --> 19:35.140
where he shows the International Energy Agency, the IEA.

19:35.140 --> 19:39.420
Every year they put out this projection of solar buildout

19:39.420 --> 19:41.700
and every year it's like a flat line,

19:41.700 --> 19:43.260
but it's like a flat line on an exponential,

19:43.260 --> 19:45.060
like the real curve is like going vertical

19:45.060 --> 19:46.660
and every year their projection is that it's just going

19:46.660 --> 19:50.780
flat-toe and I feel like people make that same mistake.

19:50.780 --> 19:54.500
And it sort of has this sort of ironic lesson,

19:54.500 --> 19:57.940
to the extent that we're drawing sort of parallels

19:57.940 --> 20:00.500
with the way our brain works and the way these models work,

20:00.500 --> 20:02.020
it seems like humans have a very strong

20:02.020 --> 20:03.420
autoregressive bias.

20:03.420 --> 20:04.460
So what's going on there?

20:04.460 --> 20:06.660
Is it an institutional problem

20:06.660 --> 20:09.420
or is it a psychological problem?

20:09.420 --> 20:14.020
Why is it that we can't project correctly in many cases?

20:14.860 --> 20:17.860
Well, to what I just said, I think it's probably both,

20:17.860 --> 20:19.580
but largely psychological, right?

20:19.580 --> 20:21.340
Our brains are evolved for, you know,

20:21.340 --> 20:23.580
hunter-gatherer societies that didn't really change

20:23.580 --> 20:28.580
over millennia and, you know, even the last 40, 50 years

20:29.180 --> 20:30.700
have been a period of relative stagnation

20:30.700 --> 20:33.060
where we have a lot of sort of pseudo-innovation.

20:33.060 --> 20:37.780
And so I think people are just a bit sort of disabused.

20:37.780 --> 20:40.140
Okay, you have some super interesting points

20:40.180 --> 20:43.660
about comparing the human brain,

20:43.660 --> 20:47.580
how the human brain works to how neural networks learn.

20:47.580 --> 20:51.820
What is universality in the context of brain learning

20:51.820 --> 20:53.860
and neural network learning?

20:53.860 --> 20:58.220
So universality is a term of our sort of refers to

20:58.220 --> 21:00.020
the fact that different neural networks

21:00.020 --> 21:03.500
independently trained, you know, even on different data

21:03.500 --> 21:06.940
will often converge on very similar representations

21:06.940 --> 21:09.580
in their embedding space of that data.

21:09.580 --> 21:13.300
And you can extend that to striking parallels

21:13.300 --> 21:15.220
or isomorphisms between the representations

21:15.220 --> 21:17.420
that artificial neural networks learn

21:17.420 --> 21:19.180
and that our brain appears to learn.

21:19.180 --> 21:21.500
Probably the area of the brain that's been studied the most

21:21.500 --> 21:23.180
is the visual cortex.

21:23.180 --> 21:27.420
And it seems to me as like a layperson

21:27.420 --> 21:30.260
that the broad consensus in neuroscience

21:30.260 --> 21:33.180
is that the visual cortex is very similar

21:33.180 --> 21:35.100
to a deep convolutional neural network.

21:35.100 --> 21:39.020
It's basically isomorphic to our artificial

21:39.020 --> 21:40.940
deep convolutional neural networks.

21:40.940 --> 21:45.020
And you train CCN on image data

21:45.020 --> 21:49.760
and our brain is trained on our sensory data.

21:49.760 --> 21:51.540
And it turns out they end up learning

21:51.540 --> 21:54.500
strikingly similar representations.

21:54.500 --> 21:55.920
And there are a few reasons for that, right?

21:55.920 --> 22:00.360
So, you know, one is sort of hierarchies of abstraction.

22:00.360 --> 22:03.500
It makes sense that early layers in a neural network

22:03.500 --> 22:06.220
will learn things like edges and simple shapes

22:06.220 --> 22:08.660
and only later in the only deeper in the network

22:08.700 --> 22:10.780
will you learn more subtle features.

22:10.780 --> 22:13.860
So there's that sort of sequencing part of it.

22:13.860 --> 22:15.620
And then there's also just the energy constraint.

22:15.620 --> 22:18.100
You know, gradient descent isn't costless, right?

22:18.100 --> 22:20.100
It requires energy, it requires a lot of energy.

22:20.100 --> 22:22.940
That's, you know, these data centers suck up a lot of energy.

22:22.940 --> 22:24.380
The same is true of our brain.

22:24.380 --> 22:27.580
You know, our brain consumes a lot of energy,

22:27.580 --> 22:29.380
like 25% of our calories.

22:29.380 --> 22:32.300
And especially when it's, and when we're young,

22:32.300 --> 22:34.460
there's a very strong metabolic cost

22:34.460 --> 22:36.280
associated with neural plasticity.

22:36.280 --> 22:37.900
Our brain being something shaped by evolution

22:37.940 --> 22:39.420
was obviously very energy conscious.

22:39.420 --> 22:43.580
And so those energy constraints greatly shrink the landscape

22:43.580 --> 22:47.900
of possible representations from sort of this infinite

22:47.900 --> 22:51.260
landscape of all the ways you could represent certain data

22:51.260 --> 22:54.140
to a much more manageable set of representations.

22:54.140 --> 22:56.820
And that doesn't guarantee that we'll converge

22:56.820 --> 22:58.980
on the same representations.

22:58.980 --> 23:02.140
At least suggestive of a weak universality

23:02.140 --> 23:04.900
where even when we don't have the exact same representations,

23:04.900 --> 23:06.900
they're often a coordinate transformation

23:06.900 --> 23:08.540
that play from each other.

23:08.540 --> 23:11.940
It's actually a bit surprising, so as you mentioned,

23:11.940 --> 23:13.220
when we train neural networks,

23:13.220 --> 23:15.300
we don't have the same energy constraints

23:15.300 --> 23:18.460
as the brain had during our evolution.

23:18.460 --> 23:21.380
And I would expect, again, from evolution,

23:21.380 --> 23:25.540
that the human brains have many more inbuilt biases

23:25.540 --> 23:27.300
and heuristics.

23:27.300 --> 23:29.580
But if we then compare the representations

23:29.580 --> 23:32.300
in a neural network to those in a human brain,

23:32.300 --> 23:33.980
we found that they are quite similar.

23:33.980 --> 23:36.580
Isn't that the whole point of universality?

23:36.580 --> 23:40.660
So does the neural network have the same heuristics

23:40.660 --> 23:44.020
and biases that we have, or what's going on here?

23:44.020 --> 23:48.700
Well, one of the primary biases in stochastic gradient

23:48.700 --> 23:52.620
descent is sometimes called a simplicity preference,

23:52.620 --> 23:55.980
basically an inductive bias for more parsimonious

23:55.980 --> 23:57.380
representations.

23:57.380 --> 24:00.900
Parsimonious in the sense of Occam's razor.

24:00.900 --> 24:04.660
And that's a byproduct of this information

24:04.660 --> 24:08.020
heuristic concept of Kolmogorff complexity,

24:08.020 --> 24:10.140
where Kolmogorff complexity means

24:10.140 --> 24:13.500
is measured by, is there a short program that

24:13.500 --> 24:16.140
can reproduce this longer sequence?

24:16.140 --> 24:18.060
And if you can find a short program that's

24:18.060 --> 24:20.460
sort of a more compact or more compressed way of representing

24:20.460 --> 24:22.340
it, and when you're under energy constraints,

24:22.340 --> 24:26.300
you're looking for those more compressed representations.

24:26.300 --> 24:28.500
And so that simplicity bias seems

24:28.500 --> 24:32.260
to be also the origin of generalization,

24:32.340 --> 24:36.900
of our ability to go beyond merely memorizing data,

24:36.900 --> 24:40.340
overfitting our parameters, to finding a simpler way of

24:40.340 --> 24:42.100
representing those parameters, right?

24:42.100 --> 24:45.860
Where we go from sort of fitting a bunch of data points

24:45.860 --> 24:47.820
to recognizing, oh, these data points are being generated

24:47.820 --> 24:50.700
by a sine function, so I can replace all these data points

24:50.700 --> 24:52.900
by a simple circuit for that sine function

24:52.900 --> 24:53.860
or something like that.

24:53.860 --> 24:55.980
What can we learn about AI progress

24:55.980 --> 24:59.460
when we consider the hard steps that humans

24:59.500 --> 25:03.660
and our ancestors have gone through in evolution?

25:03.660 --> 25:04.940
It's beyond evolution.

25:06.460 --> 25:07.700
This is often comes up in the discussion

25:07.700 --> 25:09.580
of the Fermi Paradox.

25:09.580 --> 25:12.500
Life on Earth to exist at all, let alone intelligent life,

25:12.500 --> 25:14.460
had to pass through many hard steps, right?

25:14.460 --> 25:17.340
We had to have a planet in a habitable zone.

25:18.220 --> 25:22.420
We had to have, you know, the right mix

25:22.420 --> 25:27.420
of organic chemicals in the Earth's crust and so forth.

25:27.580 --> 25:30.900
We had to have the conditions for abiogenesis,

25:30.900 --> 25:33.260
the emergence of the very earliest sort of

25:33.260 --> 25:35.420
non-living replicators, probably, you know,

25:35.420 --> 25:38.140
some kind of polymer type of crystal structure.

25:38.140 --> 25:39.380
Then we had to have, you know,

25:39.380 --> 25:43.940
the transition from single cell to multicellular organisms,

25:43.940 --> 25:46.300
to transition through the Cambrian explosion, right?

25:46.300 --> 25:47.980
Every one of these steps,

25:47.980 --> 25:50.980
you could think of as a very unlikely improbable thing.

25:50.980 --> 25:52.540
All the way up to, you know,

25:52.540 --> 25:55.260
the development of warm-blooded mammals

25:55.300 --> 25:59.820
and sort of social animals that were heavily selected for,

25:59.820 --> 26:04.820
for brain size, to then the sociocultural hard steps

26:04.940 --> 26:08.380
of like moving from small group primates

26:08.380 --> 26:13.380
to sort of settled technological cultures.

26:14.060 --> 26:15.900
Then, you know, technological hard steps,

26:15.900 --> 26:17.100
like the discovery of the printing press

26:17.100 --> 26:19.460
or the discovery of the transistor.

26:19.460 --> 26:21.460
You put those all together and life seems

26:21.460 --> 26:24.060
just incredibly unlikely.

26:24.060 --> 26:27.060
And, you know, this often goes to the point of view

26:27.060 --> 26:29.500
that, you know, creationists or intelligent designers

26:29.500 --> 26:31.220
would put forward.

26:31.220 --> 26:32.660
But then you zoom out and then you recognize,

26:32.660 --> 26:36.860
oh, wait, there are, you know, trillions of galaxies

26:36.860 --> 26:38.740
each with trillions, you know, hundreds of billions

26:38.740 --> 26:42.220
of stars and hundreds of trillions of planets.

26:42.220 --> 26:47.220
There's an awful lot of potential variation out there.

26:47.420 --> 26:50.820
And then meanwhile, every one of these hard steps

26:50.860 --> 26:55.460
seems characterized by a search problem that is very hard.

26:55.460 --> 26:58.140
But then once you find the correct thing,

26:58.140 --> 27:01.020
like the earliest self-replicator,

27:01.020 --> 27:03.180
things kind of take off, right?

27:03.180 --> 27:06.660
So you imagine that before the earliest self-replicator,

27:06.660 --> 27:09.820
there were millions or billions of attempts

27:09.820 --> 27:13.420
to self-replicate, like that didn't succeed.

27:13.420 --> 27:16.300
Yeah, it's just a huge search problem, right?

27:16.300 --> 27:19.060
And, you know, maybe there are more gradual intermediate

27:19.060 --> 27:22.980
stages where you have sort of, you know,

27:22.980 --> 27:25.260
everything in biology ends up looking way more gradual

27:25.260 --> 27:26.420
more you learn about it.

27:26.420 --> 27:29.620
But there are these phase transitions where you tip over

27:29.620 --> 27:30.980
and you get the Cambrian explosion

27:30.980 --> 27:33.380
or you get the printing press and the printing revolution.

27:33.380 --> 27:36.900
And so those hard steps end up looking relatively,

27:36.900 --> 27:39.060
they look more easy in retrospect

27:39.060 --> 27:40.380
because even though the search was hard,

27:40.380 --> 27:43.620
once you've tripped over the correct solution,

27:43.620 --> 27:46.660
there's sort of an autocatalytic self-reinforcing loop

27:46.660 --> 27:49.220
that pulls you into a new regime.

27:49.220 --> 27:53.260
And indeed, when you look at the emergence of life on Earth

27:53.260 --> 27:56.300
relative to the age of the universe,

27:57.220 --> 28:01.140
and Avi Lo with some co-authors have done this,

28:01.140 --> 28:03.460
life on Earth is incredibly early.

28:03.460 --> 28:06.820
Like, you know, the universe is 13.7 billion years old,

28:06.820 --> 28:11.220
but life couldn't emerge really much sooner.

28:11.220 --> 28:14.980
The reason being the universe started out as hot and dense,

28:14.980 --> 28:17.500
it had to cool down, stars had to form,

28:17.500 --> 28:19.180
those stars had to supernovae

28:19.180 --> 28:21.260
so they could produce the heavy elements

28:21.260 --> 28:23.740
that are essential to life.

28:23.740 --> 28:26.660
And then those solar systems had to then take shape

28:26.660 --> 28:28.780
and then had to further cool

28:28.780 --> 28:33.340
so the solar system wasn't being irradiated constantly.

28:33.340 --> 28:35.460
And when you put all those factors together,

28:35.460 --> 28:39.420
human life emerged basically as soon as it was possible

28:39.420 --> 28:40.900
for life to emerge anywhere.

28:40.900 --> 28:42.820
And so this is one way to answer the Fermi paradox

28:42.860 --> 28:45.380
that we're just in the first cohort, right?

28:45.380 --> 28:47.220
But it also should give you strong priors

28:47.220 --> 28:48.740
that passing through those hard steps

28:48.740 --> 28:50.700
isn't as hard as it looks.

28:50.700 --> 28:53.100
And what's the lesson for AI here?

28:53.100 --> 28:55.660
Developing AGI is sort of a hard step.

28:57.300 --> 29:00.300
We're doing this kind of gradient search

29:00.300 --> 29:04.260
for the right algorithms, for the right, what have you.

29:04.260 --> 29:06.740
And we seem to be now in a slow takeoff

29:06.740 --> 29:09.820
where we've figured out the core ingredients

29:09.820 --> 29:12.260
and there's now an autocatalytic process

29:12.300 --> 29:13.980
that's pulling us into a new phase.

29:13.980 --> 29:16.020
And what do you mean by autocatalytic?

29:16.020 --> 29:19.540
Suffering, forcing, once it gets started,

29:19.540 --> 29:24.540
it pulls itself, it sort of has an as if teleology, right?

29:24.820 --> 29:26.020
You see this in nature,

29:26.020 --> 29:28.860
but you also see this in capitalism.

29:29.780 --> 29:32.580
And you would expect us to get to advanced AI

29:32.580 --> 29:36.420
basically as soon as it's computationally possible.

29:37.300 --> 29:39.180
It basically seemed that way, right?

29:39.180 --> 29:41.380
Like, there was a kind of tacit collusion

29:41.420 --> 29:44.620
between Google and other players in the space

29:44.620 --> 29:48.300
to they had transformer models since 2017,

29:48.300 --> 29:50.140
but really, there's some of the precursors

29:50.140 --> 29:52.260
to transformers go back to the early 90s.

29:52.260 --> 29:55.140
But once you have this sort of profit opportunity

29:55.140 --> 29:57.420
that's in the background,

29:57.420 --> 29:59.500
it's hard in the competitive environment

29:59.500 --> 30:02.620
to stop an open AI from being like,

30:02.620 --> 30:05.540
oh, let's chase those profits.

30:05.540 --> 30:07.300
And then once that ball gets rolling,

30:07.300 --> 30:09.140
it's basically impossible to stop.

30:09.620 --> 30:14.020
This is why whatever the merits of the pause letter,

30:14.020 --> 30:17.740
it's virtually impossible to really have a pause

30:17.740 --> 30:21.940
in AI development because everything is sort of structured

30:21.940 --> 30:25.500
by these game theoretic incentives to just keep going faster.

30:25.500 --> 30:27.460
Once you've stumbled on the gold reserve,

30:27.460 --> 30:30.660
it's hard to keep the prospectors from running there.

30:30.660 --> 30:35.100
Samuel, is the US government prepared for advanced AI?

30:35.100 --> 30:35.940
No.

30:35.940 --> 30:41.140
No, I mean, where do I start?

30:41.140 --> 30:44.420
I mean, the US government, if you think of it

30:44.420 --> 30:47.820
from a firmware level, many countries have national IDs.

30:47.820 --> 30:48.900
The US doesn't have a national ID.

30:48.900 --> 30:50.140
We have social security numbers.

30:50.140 --> 30:52.100
There are these like nine digit numbers

30:52.100 --> 30:54.820
that date back to 1935.

30:54.820 --> 30:59.100
We have the core administrative laws

30:59.100 --> 31:01.980
date back to the early 40s.

31:01.980 --> 31:04.540
Much of our sort of technical infrastructure,

31:04.540 --> 31:08.980
like the system the IRS runs on,

31:08.980 --> 31:10.660
date back to the Kennedy administration

31:10.660 --> 31:12.860
and are written in assembly code.

31:12.860 --> 31:14.820
There's also been this general decline

31:14.820 --> 31:18.140
in what you could call state capacity, sort of the ability

31:18.140 --> 31:21.900
for the US government to execute on things.

31:21.900 --> 31:23.260
And you hear about this all the time.

31:23.260 --> 31:26.020
You hear about how the Golden Gate Bridge was built

31:26.020 --> 31:28.700
in four years or something like that.

31:28.700 --> 31:31.380
And now it takes like 10 years to build an access road.

31:31.420 --> 31:35.860
One of the reasons for that goes to what the legal scholar

31:35.860 --> 31:39.300
Nicholas Bagley has called the procedural fetish.

31:39.300 --> 31:43.260
Really, since the 70s, the machinery of the US government

31:43.260 --> 31:49.300
has shifted towards a reliance on explicit process.

31:49.300 --> 31:53.100
And proceduralism has pluses and minuses.

31:53.100 --> 31:56.500
If you have a clear process, government

31:56.500 --> 31:58.820
can kind of run an autopilot to an extent.

31:58.860 --> 32:01.820
But it also means you limit the room for discretion

32:01.820 --> 32:05.420
and you limit the flexibility of government to move quickly.

32:05.420 --> 32:07.820
And moreover, in our adversarial legal system,

32:07.820 --> 32:12.580
you also open up avenues for sort of continuous judicial review

32:12.580 --> 32:18.900
and legal challenge, where famously New York has taken

32:18.900 --> 32:21.900
over three years to approve congestion pricing

32:21.900 --> 32:23.900
on one of their bridges because that's

32:23.900 --> 32:25.140
to undergo environmental review.

32:25.140 --> 32:26.500
And people who don't want to pay the congestion price

32:26.500 --> 32:27.620
keep suing.

32:27.620 --> 32:29.580
Do you think having more procedures

32:29.580 --> 32:34.140
would make it easier for AI to interface with government?

32:34.140 --> 32:35.820
I would say having fewer procedures

32:35.820 --> 32:37.700
would make it easier for government to adapt.

32:37.700 --> 32:40.300
My assumption would be that having something written down,

32:40.300 --> 32:43.100
having a procedure for something would make it easier for AI

32:43.100 --> 32:45.580
to plug AI into that procedure.

32:45.580 --> 32:50.980
If it's less opaque and more kind of almost like an algorithm

32:50.980 --> 32:52.500
step by step.

32:52.500 --> 32:54.140
Yes.

32:54.180 --> 32:59.180
But the analogy I would give is to the Manhattan Project.

32:59.180 --> 33:02.340
The original Manhattan Project was run like a startup.

33:02.340 --> 33:05.700
You had Oppenheimer and General Leslie Groves sort

33:05.700 --> 33:10.340
of being the technical founder and the Type A

33:10.340 --> 33:11.940
get things done founder.

33:11.940 --> 33:13.820
And they broke all the rules.

33:13.820 --> 33:16.420
They pushed as hard as they could.

33:16.420 --> 33:20.620
They were managing at its peak like 100,000 people in secret.

33:20.620 --> 33:24.020
And they built the nuclear bomb in three years.

33:24.020 --> 33:27.020
And so the way we would do that today

33:27.020 --> 33:29.540
under procedural fetish framework

33:29.540 --> 33:32.660
would be to put out a bunch of request for proposals

33:32.660 --> 33:37.460
and have some kind of competitive bid.

33:37.460 --> 33:40.460
And then we'd probably get the lowest cost bid.

33:40.460 --> 33:41.940
And it would be Lockheed Martin.

33:41.940 --> 33:46.340
And they would build half an atom bomb.

33:46.340 --> 33:50.660
And it would take 20 years and five times the budget.

33:50.660 --> 33:53.020
And so that's sort of what I'm getting at.

33:53.060 --> 33:56.300
It's not about process versus discretion per se.

33:56.300 --> 33:59.500
It's about the way process hobbles and straight jackets

33:59.500 --> 34:01.980
are ability to adapt and sort of represents

34:01.980 --> 34:07.420
a kind of sclerosis, a kind of crystallized intelligence.

34:07.420 --> 34:12.660
We lay down the things that worked in the past as process

34:12.660 --> 34:15.700
and sort of freeze those processes in place,

34:15.700 --> 34:19.900
ossifying a particular modality.

34:19.900 --> 34:21.540
And when the motor production shifts

34:21.540 --> 34:23.980
and you need to completely tear up

34:23.980 --> 34:26.740
that process, root and branch, is very difficult.

34:26.740 --> 34:30.660
Because often there's no process for changing the process.

34:30.660 --> 34:33.380
Yeah, I wonder if there are lessons

34:33.380 --> 34:36.260
for how government will respond to AI

34:36.260 --> 34:38.300
and thinking about how governments responded

34:38.300 --> 34:42.340
to, say, historical technical innovations

34:42.340 --> 34:46.380
of a similar magnitude, like the Industrial Revolution

34:46.380 --> 34:50.380
or the printing press or maybe the internet computer.

34:50.420 --> 34:52.260
Do you think we can draw general lessons?

34:52.260 --> 34:55.860
Or is it so specific that we can't really

34:55.860 --> 34:58.180
extract information about the future from them?

34:58.180 --> 35:00.300
I think there are very powerful general lessons.

35:00.300 --> 35:01.860
I think one of the first general lessons

35:01.860 --> 35:05.060
is that every major technological transformation

35:05.060 --> 35:10.340
in human history has preceded a institutional transformation.

35:10.340 --> 35:13.300
Whether it's the shift from nomadic to settled city

35:13.300 --> 35:15.460
states with the agricultural revolution

35:15.460 --> 35:18.740
or the rise of modern nation states

35:18.740 --> 35:22.380
or the end of feudalism with the printing press

35:22.380 --> 35:25.020
to in the New Deal era, the sort of transition

35:25.020 --> 35:27.020
with industrialization from the kind of laissez-faire,

35:27.020 --> 35:30.780
classical liberal phase of 18th century America

35:30.780 --> 35:34.780
to an America with a robust welfare state

35:34.780 --> 35:37.460
and administrative bureaucracies and really

35:37.460 --> 35:40.140
in all new constitutional order.

35:40.140 --> 35:42.940
And so there's sort of better and worse ways

35:42.940 --> 35:44.260
for this transition to happen.

35:44.260 --> 35:46.540
There's sort of the internal regime change model.

35:46.540 --> 35:50.260
And you can think of Abraham Lincoln or FDR

35:50.260 --> 35:54.540
as inaugurating a new republic, a new American republic.

35:54.540 --> 35:56.900
Or there's a scenario where we don't change

35:56.900 --> 35:59.820
because we're too crystallized and sort of like an innovator's

35:59.820 --> 36:02.380
dilemma get displaced by some new upstart.

36:02.380 --> 36:04.980
And there are different countries have different abilities

36:04.980 --> 36:08.820
and different sort of capacities for that internal adaptation.

36:08.820 --> 36:11.340
As a Canadian, I'm a big fan of Westminster-style

36:11.340 --> 36:12.260
parliamentary systems.

36:12.260 --> 36:14.660
And one of the reasons is because it's

36:14.660 --> 36:17.460
very easy for parliamentary systems

36:17.460 --> 36:20.500
to shut down ministries, open up new ministries,

36:20.500 --> 36:24.820
to reorganize the civil service because it's sort

36:24.820 --> 36:27.660
of vertically integrated under the Prime Minister's office

36:27.660 --> 36:29.180
or what have you.

36:29.180 --> 36:32.620
In the US, it's much worse because given

36:32.620 --> 36:36.020
the separation of powers, Congress and the executive

36:36.020 --> 36:42.860
are often not working well together as an understatement.

36:42.900 --> 36:45.500
But then moreover, the different federal agencies

36:45.500 --> 36:47.500
have it sort of a life of their own.

36:47.500 --> 36:49.820
Often they're self-funded and all these other things

36:49.820 --> 36:51.740
that make it very difficult to reform.

36:51.740 --> 36:53.580
Do you think Canada responded better

36:53.580 --> 36:56.260
to the rise of the internet than the US, for example?

36:56.260 --> 36:57.980
Isn't there something wrong with the story

36:57.980 --> 36:59.940
because the US kind of birthed the internet

36:59.940 --> 37:04.060
and Canada adopted the internet from the US?

37:04.060 --> 37:07.460
Let's compare, first of all, the impact of the internet

37:07.460 --> 37:10.540
on weaker states because Canada and the US

37:10.700 --> 37:15.220
are similar or sort of in one quadrant.

37:15.220 --> 37:16.860
They have differences, but the differences

37:16.860 --> 37:18.580
are small compared to other countries.

37:18.580 --> 37:20.860
If you think about internet safety discussions that

37:20.860 --> 37:23.020
would have been taking place in the early 2000s,

37:23.020 --> 37:25.340
people would have been talking about identity theft,

37:25.340 --> 37:28.940
credit card theft, child exploitation, these kind

37:28.940 --> 37:32.620
of direct first order potential harms from the internet.

37:32.620 --> 37:38.220
They didn't foresee that concurrent with the rise of mobile

37:38.220 --> 37:41.660
and social media that the internet would enable tools

37:41.660 --> 37:43.460
for mass mobilization simultaneous

37:43.460 --> 37:45.780
with a kind of legitimacy crisis where

37:45.780 --> 37:50.100
the sort of new transparency and information access

37:50.100 --> 37:52.620
that the internet provided eroded trust and government

37:52.620 --> 37:54.220
and trust in other institutions.

37:54.220 --> 37:57.100
So you have these two forces interacting,

37:57.100 --> 37:59.660
the internet exposing government and exposing corruption

37:59.660 --> 38:02.900
and leading to a decline in trust while also creating

38:02.900 --> 38:06.620
a platform for people to rise up and mobilize against that

38:06.620 --> 38:07.580
corruption.

38:07.620 --> 38:09.500
And it's something that kind of rhymes

38:09.500 --> 38:11.940
with the printing press and the printing revolution

38:11.940 --> 38:16.340
where you had these sort of dormant suppressed minority

38:16.340 --> 38:19.140
groups like the Puritans or the Presbyterians,

38:19.140 --> 38:22.700
the nonconformists, and with the collapse

38:22.700 --> 38:26.420
of the censorship printing licensing regime.

38:26.420 --> 38:30.540
They actually had a licensing regime in the UK parliament

38:30.540 --> 38:32.060
back circa 1630.

38:33.820 --> 38:35.260
That licensing regime collapsed,

38:35.260 --> 38:37.260
there I think 1634 or something around there,

38:37.260 --> 38:40.820
and that was like five years before the English Civil War.

38:40.820 --> 38:43.620
And you see something like this in the Arab Spring

38:43.620 --> 38:47.980
where the internet quite directly led

38:47.980 --> 38:51.780
to mass mobilization in Cairo and Tunisia and elsewhere

38:51.780 --> 38:53.380
and led to actual regime change,

38:53.380 --> 38:56.220
in some cases sort of temporary state collapse.

38:56.220 --> 38:57.820
And that's because those were weaker states

38:57.820 --> 38:59.740
that hadn't democratized,

38:59.740 --> 39:02.620
that hadn't sort of had their own information revolution

39:03.580 --> 39:05.660
earlier in their history the way we did, right?

39:05.740 --> 39:07.180
In some ways like the American Republic

39:07.180 --> 39:11.740
is sort of a founder country built on the backbone

39:11.740 --> 39:13.420
of the printing revolution.

39:13.420 --> 39:17.500
So we were a little bit more robust to that

39:17.500 --> 39:19.700
because it's sort of part of our ethos

39:19.700 --> 39:22.980
to have this open disagreeable society.

39:22.980 --> 39:26.660
But clearly the internet has also affected

39:26.660 --> 39:28.940
the legitimacy of Western democracies.

39:28.940 --> 39:31.940
I think it's clear, clearly one of the major inputs

39:31.940 --> 39:34.420
in sort of rising populism,

39:34.420 --> 39:36.860
the mass mobilizations that we see,

39:36.860 --> 39:41.660
whether in the US context, the 2020 racial awakening

39:41.660 --> 39:45.340
or the January 6th sort of peasant rebellion, right?

39:45.340 --> 39:49.540
These sort of look like the kind of color revolutions

39:49.540 --> 39:51.420
that we see abroad.

39:51.420 --> 39:54.100
And some people want to ascribe conspiracy theories

39:54.100 --> 39:56.260
to that, I think there's a simpler explanation,

39:56.260 --> 39:58.980
which is that people will self-organize

39:58.980 --> 40:00.340
with the right tools.

40:00.340 --> 40:01.740
Our state hasn't collapsed yet,

40:02.580 --> 40:06.820
but there's clearly a lot of cracks in the foundation,

40:06.820 --> 40:07.740
if you will.

40:07.740 --> 40:09.460
Is it, would it be fair to say that the main lesson

40:09.460 --> 40:12.340
for you from history is that technological change

40:12.340 --> 40:14.740
brings institutional change?

40:14.740 --> 40:16.900
Yeah, not necessarily one for one.

40:16.900 --> 40:20.580
I'm not kind of a vulgar Marxist on this, but yes.

40:20.580 --> 40:24.020
And the reason for that is because institutions themselves

40:24.020 --> 40:27.220
exist due to a certain cost structure.

40:27.220 --> 40:29.100
And if you have general purpose technologies

40:29.100 --> 40:31.220
that dramatically change the nature

40:31.220 --> 40:33.460
of that cost structure, then institutional change will follow.

40:33.460 --> 40:34.980
Yeah, and I think we want to get to that.

40:34.980 --> 40:38.180
But before we do, I think we should discuss AI's impact

40:38.180 --> 40:39.860
on the broader economy.

40:39.860 --> 40:42.820
So not just the government, but the economy in general.

40:42.820 --> 40:47.820
Economists have this fallacy they point out often,

40:48.060 --> 40:49.500
the lump of labor fallacy.

40:49.500 --> 40:50.900
Maybe you could explain that.

40:50.900 --> 40:52.780
The lump of labor fallacy is essentially the idea

40:52.780 --> 40:55.060
that there's a fixed amount of work to be done.

40:55.060 --> 40:58.740
If you were thinking about the Industrial Revolution

40:58.740 --> 41:00.300
and what would happen to the 50% of people

41:00.300 --> 41:02.580
who are in agriculture, you couldn't imagine

41:02.580 --> 41:04.060
the new jobs that would be created.

41:04.060 --> 41:05.540
But new jobs were created.

41:05.540 --> 41:10.060
And the reason is because human wants are infinite.

41:10.060 --> 41:13.820
And so demand will always fill supply.

41:14.980 --> 41:16.980
The second reason is because there's a kind of circular flow

41:16.980 --> 41:19.660
in the economy where one person's cost

41:19.660 --> 41:21.300
is another person's income.

41:21.300 --> 41:23.420
Society would collapse if we had true technological

41:23.420 --> 41:26.900
unemployment because there'd be things being produced

41:26.900 --> 41:28.300
but no one to pay for them.

41:29.300 --> 41:31.380
And so that ends up kind of bootstrapping new industries

41:31.380 --> 41:33.700
and new sources of production.

41:33.700 --> 41:36.500
There's still this open question, is this time different?

41:36.500 --> 41:39.540
Yeah, that's exactly what I wanna know.

41:39.540 --> 41:42.820
Because for me, it's in retrospect, let's say.

41:42.820 --> 41:46.700
It's easy to see how workers could move from fields

41:46.700 --> 41:49.460
to factories into offices.

41:49.460 --> 41:52.020
But if we have truly general AI,

41:52.020 --> 41:54.700
it's difficult for me to see where workers would move.

41:54.700 --> 41:58.380
Especially if we have also functional robots

41:58.380 --> 42:03.380
and perhaps AIs that are better at taking care of people

42:03.620 --> 42:05.620
than other people are.

42:06.780 --> 42:08.860
I'm not asking you to predict specific jobs,

42:08.860 --> 42:11.260
but I'm asking you whether you think

42:11.260 --> 42:14.700
this historical trend will hold with the advent

42:14.700 --> 42:16.540
of advanced AI.

42:16.540 --> 42:19.220
You know, the first thing to say is, you know,

42:19.220 --> 42:23.060
when Keynes wrote economic possibilities

42:23.060 --> 42:26.780
for our grandchildren, a famous text where he predicted

42:26.780 --> 42:28.380
that technological progress would lead

42:28.380 --> 42:31.860
to the growth of a leisure society.

42:31.860 --> 42:35.740
And this was in the 1930s, yeah.

42:35.740 --> 42:39.180
You know, people have dismissed him as being wrong,

42:39.180 --> 42:41.780
but actually you look at time use data

42:41.780 --> 42:44.460
and employment data and people are working less.

42:44.460 --> 42:47.380
You know, it's not, it didn't match his,

42:47.380 --> 42:49.860
the optimism of his projection, right?

42:49.860 --> 42:54.140
Because it turns out, you know, maybe if we fixed

42:54.140 --> 42:56.660
living standards at what he expected,

42:56.660 --> 43:00.100
people want more and people will work more for more.

43:00.100 --> 43:02.180
But overall, people are working less.

43:02.180 --> 43:03.940
People do have more leisure.

43:03.940 --> 43:06.860
We've sort of moved to a de facto four-day work week.

43:06.860 --> 43:10.220
So there's one world where rapid technological progress

43:10.220 --> 43:14.460
sort of continues that trend and we all work less.

43:14.460 --> 43:15.860
It's sort of a technological unemployment

43:15.860 --> 43:19.700
that's spread across people and is enabled in part

43:19.700 --> 43:23.700
because in a world of AGI, maybe you only have to work,

43:23.700 --> 43:27.820
you know, a few hours a day to make $100,000 a year.

43:27.820 --> 43:31.820
There's another possibility which is that,

43:31.820 --> 43:35.460
well, AGI could in principle be a perfect emulation

43:35.460 --> 43:38.140
of humans on specific tasks.

43:38.140 --> 43:42.140
It can't emulate the historical formation of that person.

43:42.140 --> 43:43.060
Right?

43:43.060 --> 43:46.780
So what I mean by that is if you had a perfect

43:46.820 --> 43:49.860
Adam by Adam replication of the Mona Lisa,

43:50.820 --> 43:53.700
it wouldn't sell at auction, right?

43:53.700 --> 43:57.500
Because people aren't just buying the physical substrate,

43:57.500 --> 44:02.500
they're also buying the kind of world line of that thing.

44:03.140 --> 44:07.260
And that's clearly the case in humans as well.

44:07.260 --> 44:09.700
Like there are certain, you know, talking heads

44:09.700 --> 44:13.500
that I go and enjoy not because they are the smartest

44:13.500 --> 44:15.260
or would have you because I'm interested

44:15.260 --> 44:17.420
in what that person thinks on this

44:17.420 --> 44:18.700
because they have a particular personality,

44:18.700 --> 44:19.980
a particular world line.

44:19.980 --> 44:24.980
And then the third factor is sort of artificial scarcity.

44:25.700 --> 44:26.540
Right?

44:26.540 --> 44:31.540
And so even in a world with abundance and supply

44:31.940 --> 44:34.180
in services and goods, there are still things

44:34.180 --> 44:35.940
that will be intrinsically scarce,

44:35.940 --> 44:38.580
real estate being probably the canonical thing,

44:38.580 --> 44:40.620
but also energy and commodities and so forth.

44:40.620 --> 44:42.340
And the reason real estate is intrinsically scarce

44:42.420 --> 44:46.180
because people want to live near other people

44:46.180 --> 44:51.180
and people want to live in particular areas of a city.

44:51.180 --> 44:53.540
They want to live in the posh part of town, right?

44:53.540 --> 44:54.820
And those are positional goods.

44:54.820 --> 44:58.260
We can't all live in the trendy loft.

44:58.260 --> 45:00.500
So that builds in a kind of artificial scarcity.

45:00.500 --> 45:04.100
And so people will still be competing over those things.

45:04.100 --> 45:05.860
This is sort of related to artificial scarcity,

45:05.860 --> 45:07.900
but there's also sort of break it out

45:07.900 --> 45:10.940
into a fourth possibility, which are sort of tournaments

45:10.940 --> 45:13.060
and things that are structured as tournaments.

45:13.060 --> 45:15.180
Having chess spots that are strictly better

45:15.180 --> 45:19.860
than humans at chess hasn't killed people playing chess.

45:19.860 --> 45:22.100
If anything, more people play chess today

45:22.100 --> 45:23.460
than they've had in human history.

45:23.460 --> 45:25.500
Yeah, it's more popular than ever.

45:25.500 --> 45:26.340
Yeah.

45:26.340 --> 45:29.020
And the reason is because people like to watch

45:29.020 --> 45:31.740
other humans playing and also they're structured

45:31.740 --> 45:33.620
as sort of zero sum tournaments

45:33.620 --> 45:35.980
where there can only be the best human.

45:35.980 --> 45:37.460
You look at other things that have been created

45:37.500 --> 45:41.780
just in the last 15, 20 years, the X games, right?

45:41.780 --> 45:43.900
I think people will still want to watch other people

45:43.900 --> 45:45.660
do the Olympics or do motocross

45:45.660 --> 45:46.900
and all these other things.

45:46.900 --> 45:49.540
And so maybe more of our life shifts into,

45:49.540 --> 45:53.220
both maybe greater leisure on the one hand,

45:53.220 --> 45:56.180
more competition over positional goods

45:56.180 --> 46:01.180
and more production that is structured as a tournament.

46:01.540 --> 46:03.580
Yeah, yeah, I can see many of those points.

46:03.580 --> 46:06.700
I'm just thinking, again, with fully general AI,

46:06.740 --> 46:08.580
you would be able to generate

46:08.580 --> 46:12.100
a much more interesting person playing chess

46:12.100 --> 46:15.340
or at least a simulation of a very charismatic

46:15.340 --> 46:17.860
and interesting human chess player.

46:17.860 --> 46:20.940
Why wouldn't people watch that chess player

46:20.940 --> 46:24.660
as opposed to the best human?

46:26.180 --> 46:29.700
Maybe they will, sorry to know.

46:29.700 --> 46:32.180
The question is who's producing that video stream

46:32.180 --> 46:36.060
because you still need the human behind it

46:37.020 --> 46:38.980
that had the idea, right?

46:38.980 --> 46:42.260
And you could imagine people being dishonest

46:42.260 --> 46:45.740
about the history of this chess player.

46:45.740 --> 46:50.420
The simulated chess player could be a fully digital,

46:50.420 --> 46:52.740
fully fictional, so to speak,

46:52.740 --> 46:55.140
and just pretending to be human.

46:55.140 --> 46:57.380
Right, so they could fool people.

46:57.380 --> 46:58.700
That's the case too.

46:58.700 --> 47:00.260
No, I can't rule that out,

47:00.260 --> 47:01.900
but I would just say that

47:01.900 --> 47:04.100
however that person is monetizing,

47:04.140 --> 47:06.540
their deep fake chess player,

47:06.540 --> 47:08.260
they're making money, which they're then spending back

47:08.260 --> 47:10.740
into the economy, and so they'll produce jobs somewhere.

47:10.740 --> 47:13.340
Do you think more people will move into,

47:13.340 --> 47:17.340
say, people-focused industries like nursing and teaching?

47:17.340 --> 47:22.340
Is that a possible way for us to maintain jobs?

47:22.740 --> 47:24.820
Maybe nursing, at least in the short run.

47:26.020 --> 47:28.140
I'm not very long on education

47:28.140 --> 47:31.780
being labor-intensive for much longer.

47:31.780 --> 47:33.740
But you don't think education,

47:33.740 --> 47:36.220
at least say, great school education,

47:36.220 --> 47:40.460
is that really about teaching people or conveying knowledge?

47:40.460 --> 47:42.980
Or to what extent is it about conveying knowledge?

47:42.980 --> 47:45.740
And to what extent is it about the social interaction

47:45.740 --> 47:50.740
and specializing your teaching to the individual student?

47:52.020 --> 47:55.260
Well, AI is very good at customization

47:55.260 --> 47:57.020
and sort of mastery tutoring.

47:57.020 --> 47:59.420
Education is a bundle of things.

47:59.420 --> 48:03.980
And for younger ages, it's also daycare.

48:03.980 --> 48:06.180
It is socialization, like you said.

48:06.180 --> 48:10.820
At the very least, it's just a reorganization

48:10.820 --> 48:12.060
of the division of labor,

48:12.060 --> 48:14.740
because the types of teachers that you would select

48:14.740 --> 48:18.060
or hire for may differ if the education component

48:18.060 --> 48:20.340
of that bundle is being done by AI.

48:20.340 --> 48:22.060
Maybe you select for people who are,

48:22.060 --> 48:23.700
maybe don't have any subject matter expertise,

48:23.700 --> 48:27.020
but are just highly conscientious and go to around kids.

48:27.060 --> 48:30.260
Or maybe you one bundle from public education altogether,

48:30.260 --> 48:34.220
and it re-bundles around a jujitsu school

48:34.220 --> 48:38.300
or a chess academy, because you'll have the AI tutor

48:38.300 --> 48:39.140
that will teach you math,

48:39.140 --> 48:41.580
but you'll still want to grapple with the human.

48:41.580 --> 48:42.500
Yeah, yeah.

48:42.500 --> 48:45.740
What about industries with occupational licensings

48:45.740 --> 48:47.580
like law or medicine?

48:47.580 --> 48:51.620
Will they be able to keep up their quite high wages

48:51.620 --> 48:56.380
in the face of AI being able to be a pretty good doctor

48:56.420 --> 48:58.100
and a pretty good lawyer?

48:58.100 --> 49:01.660
It's easy to solve for the long-term equilibrium.

49:01.660 --> 49:02.500
With the rise of the internet,

49:02.500 --> 49:06.500
you can do a comparison of the wage distribution

49:06.500 --> 49:08.700
for lawyers pre and post internet.

49:08.700 --> 49:11.220
And, you know, circa the early 90s,

49:11.220 --> 49:13.260
lawyer incomes were normally distributed

49:13.260 --> 49:15.180
around $60,000 a year.

49:15.180 --> 49:18.940
You know, after in the 2000s, they become bimodal.

49:18.940 --> 49:20.900
And so you have one mode that's still around

49:20.900 --> 49:23.860
that $60,000 range, those are like the family lawyers.

49:23.860 --> 49:25.180
And then you have this other mode

49:25.180 --> 49:27.060
that's into the six figures.

49:27.060 --> 49:28.900
And those are like big law, right?

49:28.900 --> 49:31.420
It's the emergence of these law firms

49:31.420 --> 49:32.620
where you have a few partners on top

49:32.620 --> 49:35.340
and maybe hundreds of associates

49:35.340 --> 49:37.420
who are doing kind of grant work using Westlaw

49:37.420 --> 49:40.980
and Lexus Nexus and these other legal search engines

49:40.980 --> 49:45.780
to accelerate drafting and legal analysis.

49:45.780 --> 49:47.660
So if that pattern repeats,

49:47.660 --> 49:52.660
I could imagine these various high-skill knowledge sectors

49:53.660 --> 49:57.700
to also become bimodal where in the short run,

49:57.700 --> 50:00.660
AI serves as a co-pilot, sort of like Westlaw

50:00.660 --> 50:03.380
or Lexus Nexus was for legal research

50:03.380 --> 50:06.140
and enables the kind of 100x lawyer.

50:06.140 --> 50:10.340
And so there's a kind of averages over dynamic.

50:10.340 --> 50:15.340
The longer run, you know, you start to see the possibility

50:15.740 --> 50:20.020
of doing an end run around existing accreditation

50:20.020 --> 50:22.460
and licensing monopolies

50:22.500 --> 50:26.220
where obviously the American Medical Association

50:26.220 --> 50:31.220
and medical boards will be highly resistant to an AI doctor.

50:31.580 --> 50:33.620
I tend to think that they'll probably end up self cannibalizing

50:33.620 --> 50:36.060
because the value prop is so great

50:36.060 --> 50:39.460
even for doctors to do simple things

50:39.460 --> 50:42.940
like automate insurance paperwork and stuff like that.

50:42.940 --> 50:44.740
But to the extent that there is a resistance,

50:44.740 --> 50:47.700
to the extent that in 10 years there's still a requirement

50:47.700 --> 50:51.940
that you must have the doctor prescribe the treatment

50:51.940 --> 50:53.020
or refer you to a specialist,

50:53.020 --> 50:55.100
even though the AI is doing all the work

50:55.100 --> 50:57.780
and they're just sort of like the elevator person

50:57.780 --> 51:01.140
that's like actually just pushing the button for you.

51:01.140 --> 51:03.060
It'll be very easy to end run that

51:03.060 --> 51:07.460
because AI is both transforming the task itself

51:07.460 --> 51:10.740
but also transforming it's the means of distribution.

51:10.740 --> 51:13.260
And if you can go to GPT-4 and ask for,

51:13.260 --> 51:15.340
put in your blood work and get a diagnosis,

51:16.420 --> 51:18.180
but no regulator's going to stop that, right?

51:18.180 --> 51:20.140
And so, you know, GPT-4 becomes sort of

51:20.140 --> 51:22.220
the ultimate doctor of up orders.

51:22.220 --> 51:25.180
You write a lot about transaction costs

51:25.180 --> 51:28.860
and how changes in transaction costs

51:28.860 --> 51:31.340
change institutional structures.

51:31.340 --> 51:33.620
First of all, what are transaction costs

51:33.620 --> 51:36.380
and how do you think they'll be affected by AI?

51:36.380 --> 51:38.500
So transaction cost is sort of an umbrella term

51:38.500 --> 51:42.780
for different kinds of costs associated with market exchange.

51:42.780 --> 51:45.620
And this goes back to Ronald Coase's famous paper

51:45.620 --> 51:48.300
on the theory of the firm where he asked the question,

51:48.300 --> 51:50.540
why do we have corporations in the first place?

51:50.540 --> 51:51.860
If free markets are so great,

51:51.860 --> 51:56.140
why don't we just go up and spot contract for everything?

51:56.140 --> 51:59.780
And the answer is, well, market exchange itself has a cost.

51:59.780 --> 52:01.100
There's the cost of monitoring.

52:01.100 --> 52:02.220
You know, if you hire a contractor,

52:02.220 --> 52:04.380
you don't know exactly what they're doing.

52:04.380 --> 52:05.820
There's the cost of bargaining.

52:05.820 --> 52:08.860
You know, having to haggle with a taxi cab driver

52:08.860 --> 52:11.700
is a friction.

52:11.700 --> 52:12.940
And there's the cost of searching,

52:12.940 --> 52:14.780
the associate of searching information.

52:14.780 --> 52:16.620
So taking those three things together,

52:16.620 --> 52:18.020
they're not all that companies do,

52:18.020 --> 52:20.900
but they structure the boundary of the corporation.

52:20.900 --> 52:22.780
They explain why some things are done in-house

52:22.780 --> 52:24.940
and some things are done through contracts.

52:24.940 --> 52:26.380
If there's high monitoring costs,

52:26.380 --> 52:27.860
you want to pull that part of the production

52:27.860 --> 52:30.900
into the company so that you can monitor

52:30.900 --> 52:33.540
and manage the people doing the production.

52:33.540 --> 52:35.740
And some of the same effects go for

52:35.740 --> 52:38.340
the existence of governments, right?

52:38.340 --> 52:40.660
Yes, because governments, you know,

52:40.660 --> 52:41.860
with a certain gestalt,

52:41.860 --> 52:43.780
governments and corporations aren't that different.

52:43.780 --> 52:45.380
There are kinds of institutional structures

52:45.380 --> 52:46.740
that pull certain things in-house

52:46.740 --> 52:51.220
and certain things are left for contracting or outsourced.

52:51.220 --> 52:54.700
And, you know, you even see sort of different kinds

52:54.700 --> 52:57.060
of governments having different parallels

52:57.060 --> 52:59.420
with different kinds of corporate governance, right?

52:59.420 --> 53:01.460
Relatively egalitarian democratic societies

53:01.460 --> 53:05.140
like Denmark are kind of like mutual insurers.

53:05.140 --> 53:09.700
Whereas more hierarchical authoritarian countries

53:09.700 --> 53:11.820
are more like, you know, like Singapore, say,

53:11.820 --> 53:15.220
is more of a joint stock corporation.

53:15.220 --> 53:17.540
And indeed, you know, Singapore was founded

53:17.540 --> 53:18.620
as a, as a, uh,

53:18.620 --> 53:21.380
an entrepot for the East India Company.

53:21.380 --> 53:24.580
So there are very deep parallels.

53:24.580 --> 53:26.260
And it's also essential.

53:26.260 --> 53:27.420
Transaction costs are essential

53:27.420 --> 53:29.100
to understand why governments do certain things

53:29.100 --> 53:30.180
and other things.

53:30.180 --> 53:32.220
All Western developed governments

53:32.220 --> 53:35.780
guarantee some amount of basic health care, right?

53:35.780 --> 53:39.300
But, um, most, you know,

53:39.300 --> 53:41.940
outside of, say, the National Health Service in,

53:41.940 --> 53:44.500
in the UK, most of these countries, uh,

53:45.260 --> 53:46.260
guarantee the insurance.

53:46.260 --> 53:50.020
They don't necessarily nationalize the actual providers,

53:50.020 --> 53:50.860
right?

53:50.860 --> 53:52.540
And the reason goes to transaction costs

53:52.540 --> 53:55.100
and sort of an analysis of the market failure

53:55.100 --> 53:56.380
and insurance.

53:56.380 --> 53:59.060
Likewise with roads, uh, you know,

53:59.060 --> 54:02.260
it's possible to build roads through purely private means.

54:02.260 --> 54:05.380
And indeed, um, you know, countries like Sweden,

54:05.380 --> 54:08.140
a lot of the roads are run by private associations.

54:08.140 --> 54:10.900
But, uh, if you have lots of different boundaries,

54:10.900 --> 54:13.460
different micro jurisdictions and so forth,

54:13.460 --> 54:15.220
there can be huge transaction costs

54:15.220 --> 54:18.100
to, uh, negotiating up to a,

54:18.100 --> 54:20.700
to a interstate highway system.

54:20.700 --> 54:22.420
Um, and, and those transaction costs

54:22.420 --> 54:26.060
then necessitate public infrastructure projects.

54:26.060 --> 54:27.620
So the transaction costs in this case

54:27.620 --> 54:30.540
would be being a private road provider.

54:30.540 --> 54:34.860
You'd have to go negotiate with 500 different landowners

54:34.860 --> 54:36.100
about building a highway,

54:36.100 --> 54:39.220
whereas a government can do some expropriation

54:39.220 --> 54:42.140
and simply build the road much, much faster,

54:42.140 --> 54:45.300
or with less transaction costs at least.

54:45.300 --> 54:46.260
Yeah, precisely.

54:46.260 --> 54:48.900
And we're seeing this, this dynamic in the US

54:48.900 --> 54:50.860
with, uh, you know, permitting for,

54:50.860 --> 54:52.820
for great infrastructure and transmission.

54:52.820 --> 54:54.300
You know, we're, we're building all this,

54:54.300 --> 54:55.620
all the solar and renewable energy,

54:55.620 --> 54:59.820
but to build the actual transmission, uh, infrastructure

54:59.820 --> 55:02.980
to get the electrons from where it's sunny to where,

55:02.980 --> 55:06.300
where it's cold requires building, you know,

55:06.300 --> 55:09.420
high voltage, uh, lines across state lines

55:09.420 --> 55:11.260
across different grid regions.

55:11.260 --> 55:13.300
And there are all kinds of NIMBs

55:13.300 --> 55:17.220
and negotiation costs involved, holdouts and so forth.

55:17.220 --> 55:19.260
And so that the more those kind of costs exist,

55:19.260 --> 55:21.780
the more it militates towards a kind of, um,

55:21.780 --> 55:24.620
larger scale intervention that, you know,

55:24.620 --> 55:26.820
federalizes that process.

55:26.820 --> 55:29.180
Yeah, the big question then is how will AI

55:29.180 --> 55:31.820
change these transaction costs?

55:31.820 --> 55:34.420
What, what will the effects be here?

55:34.420 --> 55:37.060
It's easy to say that they will be affected.

55:37.060 --> 55:38.940
And, you know, obviously the internet affected them

55:38.940 --> 55:39.940
to, to an extent.

55:39.940 --> 55:42.980
And we were talking, we talk about sort of the ease

55:42.980 --> 55:46.820
of mobilizing protest movements or the kind of, uh,

55:46.820 --> 55:48.780
the sunlight that was put on government corruption.

55:48.780 --> 55:51.500
Those are, those are reflecting declines

55:51.500 --> 55:56.180
in the cost associated information and coordination.

55:56.180 --> 55:57.620
I think AI takes us to another level.

55:57.620 --> 56:01.020
And I think it's, it's important to, to think through

56:01.020 --> 56:06.020
in part because right now the AI safety debate,

56:06.460 --> 56:08.880
at least in the United States is very polarized

56:08.920 --> 56:12.360
between people who are like, everything's going to be great.

56:12.360 --> 56:15.440
And people who are like, this is like a terminator scenario

56:15.440 --> 56:17.560
or an AI kill us all existential risk.

56:17.560 --> 56:20.960
You know, even if we accept the, you know,

56:20.960 --> 56:23.400
existential risk framing, there's still going to be

56:23.400 --> 56:27.120
many intermediate stages of AI before we flip

56:27.120 --> 56:28.600
on the superintelligence.

56:28.600 --> 56:31.560
And those intermediate stages have enormous implications

56:31.560 --> 56:33.920
for the structure of the very institutions

56:33.920 --> 56:37.400
that we'll need to respond to superintelligence

56:37.400 --> 56:38.360
or what have you.

56:38.360 --> 56:42.720
The ways we can see this is because all these information

56:42.720 --> 56:47.000
and, and monitoring and, and, uh, bargaining costs

56:47.880 --> 56:51.280
are directly implicated by commoditized intelligence.

56:51.280 --> 56:53.440
You know, start with the principal agent problem.

56:53.440 --> 56:55.040
You know, there is no principal agent problem

56:55.040 --> 56:59.160
if your agent does exactly as you ask and works 24 seven

56:59.160 --> 57:01.240
doesn't steal from the till, right?

57:01.240 --> 57:05.760
And so AI agents dramatically collapse agency cost

57:05.760 --> 57:06.960
monitoring.

57:06.960 --> 57:09.160
Now that we have multimodal models in principle,

57:09.160 --> 57:11.520
we could have cameras in every house that are just being

57:11.520 --> 57:13.880
prompted to say, you know, is someone committing a crime

57:13.880 --> 57:14.720
right now?

57:15.920 --> 57:18.280
Whether we wanted to go that direction or not,

57:18.280 --> 57:20.840
it gives you a sense that of how, you know,

57:20.840 --> 57:22.720
the cost of monitoring have basically plummeted

57:22.720 --> 57:25.320
over the last two years and are going to go way lower.

57:25.320 --> 57:26.920
And so you're starting to see this rolled out

57:26.920 --> 57:28.520
in the private sector with, you know,

57:28.520 --> 57:31.280
Activision has announced that they're going to be using

57:31.280 --> 57:35.160
language models for moderating voice chat

57:35.160 --> 57:37.200
and call duty, right?

57:37.200 --> 57:39.920
And, and this, this is a more robust form of monitoring

57:39.920 --> 57:43.240
because in the past you would have to like ban certain words

57:43.240 --> 57:47.480
like certain swear words or things associated with

57:47.480 --> 57:50.440
sexual violence, but then people could always get around

57:50.440 --> 57:54.160
those by using euphemisms, right?

57:54.160 --> 57:56.560
Like on YouTube, you know, the algorithm will ding you

57:56.560 --> 58:00.040
if you talk about coronavirus or if you talk about murder

58:00.040 --> 58:03.280
or suicide, these, these things that throw off at flags.

58:03.280 --> 58:06.240
So what people have taken doing is saying they were

58:06.240 --> 58:08.880
unalived rather than murdered, right?

58:08.880 --> 58:12.440
And that doesn't fool a language model.

58:12.440 --> 58:13.880
If you ask a language model, you know,

58:13.880 --> 58:16.280
if you prompt it in a way to look for sort of broad

58:16.280 --> 58:20.680
semantic categories, not just a narrow, a narrow word,

58:20.680 --> 58:22.160
it's much more robust.

58:22.160 --> 58:23.160
And so what that means, you know,

58:23.160 --> 58:24.680
what you already start to see it, like I said,

58:24.680 --> 58:27.920
with Activision and the use of LLAMS and content moderation,

58:27.920 --> 58:31.040
you're going to start, you're going to see it in the use of

58:31.080 --> 58:34.920
multimodal models for productivity management and tracking.

58:34.920 --> 58:38.360
You know, Microsoft is unveiling their 365 co-pilot

58:38.360 --> 58:41.920
where you're going to have GPT-4 and Word and Excel

58:41.920 --> 58:44.200
and Teams and Outlook, but at the same time,

58:44.200 --> 58:46.600
you're also going to have a manager who is going to be able

58:46.600 --> 58:49.080
to say, you know, to prompt the model,

58:49.080 --> 58:51.600
tell me who is the most productive this week, right?

58:51.600 --> 58:53.200
Something as vague as that.

58:53.200 --> 58:56.320
And so you see this diffusion in the private sector.

58:56.320 --> 59:00.000
The question is, does it diffuse in the public sector?

59:00.000 --> 59:03.760
There's obvious ways that it would be a huge boom, right?

59:03.760 --> 59:07.160
You know, Inspector General GPT could tell you exactly,

59:07.160 --> 59:09.320
you know, how the civil service is working,

59:09.320 --> 59:10.160
whether there's corruption,

59:10.160 --> 59:11.480
whether there's a deep state of conspiracy

59:11.480 --> 59:13.240
or something like that, right?

59:13.240 --> 59:16.680
And at first blush, a lot of what government does

59:16.680 --> 59:19.880
is kind of a fleshy API.

59:19.880 --> 59:24.880
Bureaucracies are nodes that apply a degree of context

59:24.880 --> 59:26.440
between printing out a PDF and scanning it

59:26.440 --> 59:28.080
back into the computer.

59:28.240 --> 59:29.080
It varies.

59:29.080 --> 59:32.720
There's degrees of human judgment that are required,

59:32.720 --> 59:35.800
but on first order, government bureaucracies

59:35.800 --> 59:38.640
seem incredibly exposed to this technology

59:38.640 --> 59:40.480
in a way that could diffuse really rapidly

59:40.480 --> 59:43.480
because, you know, going back to Microsoft 365 Copilot,

59:43.480 --> 59:46.360
Microsoft is the biggest IT vendor in US government, right?

59:46.360 --> 59:50.240
And so you can imagine once everyone has this pre-installed

59:50.240 --> 59:52.600
on their computer that the person at the Bureau

59:52.600 --> 59:54.040
of Labor Statistics who's in charge

59:54.040 --> 59:57.360
of doing the monthly employment situation report,

59:57.800 --> 59:59.560
the jobs report, you know, at some point

59:59.560 --> 01:00:02.160
he's gonna be walking into work and hitting a button, right?

01:00:02.160 --> 01:00:06.080
That, you know, asking Excel to find

01:00:06.080 --> 01:00:08.480
the five most interesting trends and generate charts

01:00:08.480 --> 01:00:10.440
and the report is done.

01:00:10.440 --> 01:00:12.440
And in the private sector, that person would be reallocated

01:00:12.440 --> 01:00:15.320
and maybe doing things that the computer's not good at yet,

01:00:15.320 --> 01:00:18.640
but these positions are much stickier in government.

01:00:18.640 --> 01:00:20.160
To the extent that diffusion is inhibited

01:00:20.160 --> 01:00:21.640
on the public sector side,

01:00:21.640 --> 01:00:24.640
I worry about the kind of disruption and displacement

01:00:24.640 --> 01:00:27.040
of government services by a private sector

01:00:27.440 --> 01:00:29.480
that's adopting the technology really fast.

01:00:29.480 --> 01:00:31.520
This is something we'll talk about in a moment.

01:00:31.520 --> 01:00:34.320
Before that, I just wanna get to your complaints

01:00:34.320 --> 01:00:36.200
about isolated thinking about AI.

01:00:36.200 --> 01:00:41.200
You've sketched out some complaint about people

01:00:41.200 --> 01:00:45.120
thinking about AI only applying to one domain

01:00:45.120 --> 01:00:47.480
and then not really seeing the bigger picture.

01:00:47.480 --> 01:00:49.480
What are some examples here?

01:00:49.480 --> 01:00:52.000
Why do you worry about isolated thinking?

01:00:52.000 --> 01:00:53.120
A few dimensions to this.

01:00:53.120 --> 01:00:56.080
One is what I've called the horse's carriage fallacy.

01:00:57.040 --> 01:00:58.840
Right, the kind of view that, you know,

01:00:58.840 --> 01:01:03.040
what automobiles were was just a carriage with the horse.

01:01:03.040 --> 01:01:06.320
Right, and so that anchors you to the older paradigm

01:01:06.320 --> 01:01:07.760
and it's like you're changing one thing

01:01:07.760 --> 01:01:09.160
and everything else stays the same.

01:01:09.160 --> 01:01:11.800
And you neglect all the second order ways

01:01:11.800 --> 01:01:14.160
that the development of the automobile,

01:01:14.160 --> 01:01:17.640
you know, enabled the build out of highway systems,

01:01:17.640 --> 01:01:22.560
the total reconfiguration of sort of the economic geography.

01:01:22.560 --> 01:01:26.040
Right, and then implications for institutions

01:01:26.040 --> 01:01:28.480
at the state where, you know, once you have road networks

01:01:28.480 --> 01:01:30.200
or telegraph networks or any of these,

01:01:30.200 --> 01:01:31.720
these kind of networks, it suddenly becomes easier

01:01:31.720 --> 01:01:34.720
to monitor agents of the state

01:01:34.720 --> 01:01:35.800
and other parts of the country.

01:01:35.800 --> 01:01:37.400
And so you can, you know, build out

01:01:37.400 --> 01:01:38.840
more of a federal bureaucracy.

01:01:38.840 --> 01:01:40.760
And so all these things were second order

01:01:40.760 --> 01:01:42.720
and were kind of neglected if you just were too focused

01:01:42.720 --> 01:01:46.440
on the first order effects of displacing the horses.

01:01:46.440 --> 01:01:49.440
And in a sense, the second order effects

01:01:49.440 --> 01:01:52.680
turned out to be much more consequential in the end.

01:01:52.680 --> 01:01:54.800
Yes, it seemed to always be.

01:01:54.800 --> 01:01:57.880
And likewise with the internet and sort of the,

01:01:57.880 --> 01:01:59.920
I think this comes up a lot in the,

01:01:59.920 --> 01:02:03.360
in how to think about AI use and misuse.

01:02:03.360 --> 01:02:04.720
There's lots of valid discussions there,

01:02:04.720 --> 01:02:06.920
but they're always very first order.

01:02:06.920 --> 01:02:08.200
And when you think about the way the internet

01:02:08.200 --> 01:02:10.160
has disrupted legacy institutions,

01:02:10.160 --> 01:02:11.720
yes, there's disinformation,

01:02:11.720 --> 01:02:16.240
but often the thing that's disrupting is not fake news.

01:02:16.240 --> 01:02:18.640
It's real news that's being repeated

01:02:18.640 --> 01:02:21.440
with misleading frequency, right?

01:02:21.440 --> 01:02:24.680
That's like throwing off our availability heuristic.

01:02:24.680 --> 01:02:28.400
Or it's valid things that, you know,

01:02:28.400 --> 01:02:30.600
valid complaints, whether, you know,

01:02:30.600 --> 01:02:31.760
the protests in Iran, right?

01:02:31.760 --> 01:02:34.280
The protests in Iran have this like striking parallel

01:02:34.280 --> 01:02:36.840
to the protests following the George Floyd protest

01:02:37.800 --> 01:02:40.160
and protesting in other countries

01:02:40.160 --> 01:02:42.780
where they even have like a three word chant, right?

01:02:42.780 --> 01:02:47.780
Or the case of the Arab Spring in Tunisia

01:02:47.920 --> 01:02:50.900
that started with the person self-immolating, right?

01:02:50.900 --> 01:02:52.360
There's sort of like the structure that repeats

01:02:52.360 --> 01:02:53.280
where you have like a martyr

01:02:53.280 --> 01:02:55.440
or like some shocking event.

01:02:55.440 --> 01:02:57.960
And because of the way social media is organized,

01:02:57.960 --> 01:03:01.320
it synchronizes people around the event

01:03:02.440 --> 01:03:04.240
in a way that's kind of stochastic.

01:03:04.240 --> 01:03:05.720
Like it's like lightning striking.

01:03:05.720 --> 01:03:07.600
You don't know what event it's going to strike on,

01:03:07.600 --> 01:03:09.320
but once we're synchronized,

01:03:09.320 --> 01:03:11.280
then we start, you know, moving back and forth

01:03:11.280 --> 01:03:14.280
in a way that like causes the bridge to buckle.

01:03:14.280 --> 01:03:17.040
Nothing about that is a misuse, right?

01:03:17.040 --> 01:03:18.840
Those are all valid uses,

01:03:18.840 --> 01:03:23.040
but their use is under collective action.

01:03:23.040 --> 01:03:25.200
It's sort of solving not just for the partial equilibrium

01:03:25.200 --> 01:03:28.400
but the general equilibrium when everyone is doing this.

01:03:28.400 --> 01:03:30.920
And I think the person who wrote the best

01:03:30.920 --> 01:03:33.840
on this sort of conceptually was Thomas Schelling

01:03:33.840 --> 01:03:36.240
and one of his little books,

01:03:36.240 --> 01:03:38.120
Micromotives, Macro Behavior,

01:03:38.120 --> 01:03:39.280
A Big Influence on Me as a Kid,

01:03:39.280 --> 01:03:42.200
where he talks about all these sort of like toy models

01:03:42.200 --> 01:03:45.160
where you're at a hockey game or a basketball game

01:03:46.080 --> 01:03:47.600
and something is happening,

01:03:47.600 --> 01:03:50.600
something exciting is happening in the arena.

01:03:50.600 --> 01:03:52.400
And so people in front of you stand up

01:03:52.400 --> 01:03:54.520
to get a better view and then you have to stand up

01:03:54.520 --> 01:03:56.680
to get a better view of them over them and so on.

01:03:56.680 --> 01:03:58.760
And so it cascades and suddenly everyone went from sitting

01:03:58.760 --> 01:03:59.640
to everyone went to standing

01:03:59.640 --> 01:04:02.080
and no one's view has improved, right?

01:04:02.080 --> 01:04:04.880
And so these sort of general equilibria

01:04:04.880 --> 01:04:08.120
where you sort of solve for everyone's micro incentives

01:04:08.120 --> 01:04:10.880
and the kind of new Nash equilibrium that emerges,

01:04:10.880 --> 01:04:13.480
that ends up being the thing that drives

01:04:13.480 --> 01:04:15.440
a kind of multiple equilibrium shift

01:04:15.440 --> 01:04:17.680
from one regime to another.

01:04:17.680 --> 01:04:21.200
And throughout, there may be no actual examples

01:04:21.200 --> 01:04:22.040
of misuse involved.

01:04:22.040 --> 01:04:25.360
It may just be people following their individual incentives.

01:04:25.360 --> 01:04:28.200
I think it's worth the stressing this point you make

01:04:28.200 --> 01:04:32.400
about the effects of earlier AI systems

01:04:32.400 --> 01:04:33.800
on our institutions,

01:04:33.800 --> 01:04:37.160
that they might have effects that deteriorate our institutions

01:04:37.160 --> 01:04:41.200
such that we can't handle later and more advanced AI.

01:04:41.200 --> 01:04:45.160
And ignoring this would be an example of isolated thinking

01:04:45.160 --> 01:04:48.120
and ignoring the second order effects, right?

01:04:48.120 --> 01:04:48.960
Yeah.

01:04:48.960 --> 01:04:53.960
And they also, it also changes the sort of agenda, right?

01:04:54.720 --> 01:04:56.960
The AI safety agenda shouldn't just be

01:04:56.960 --> 01:04:59.480
about the first order of things or in alignment,

01:04:59.480 --> 01:05:01.240
you know, very important,

01:05:01.240 --> 01:05:05.240
but you know, it's led to a discussion of

01:05:05.240 --> 01:05:06.520
do we need a new federal agency?

01:05:06.520 --> 01:05:08.840
And if so, what kind of agency?

01:05:08.840 --> 01:05:11.680
Whereas it may be more appropriate to think

01:05:11.680 --> 01:05:13.120
not what new agency do we need,

01:05:13.120 --> 01:05:17.720
but how do all the agencies change, right?

01:05:17.720 --> 01:05:20.000
And how do we sort of like brace for impact

01:05:20.000 --> 01:05:24.040
and enable a degree of co-evolution

01:05:24.040 --> 01:05:25.800
rather than displacement?

01:05:25.800 --> 01:05:28.600
I don't know whether the question of

01:05:28.600 --> 01:05:32.480
how to get our institutions to respond appropriately

01:05:32.480 --> 01:05:35.280
is more difficult or less difficult

01:05:35.280 --> 01:05:37.080
than the problem of aligning AI.

01:05:37.080 --> 01:05:38.920
But it certainly seems very difficult to me.

01:05:38.920 --> 01:05:42.520
So is there, are we making it harder on ourselves

01:05:42.520 --> 01:05:44.720
if we focus on the effects,

01:05:44.720 --> 01:05:47.440
on the second order effects on institutions?

01:05:47.440 --> 01:05:48.920
I mean, it's unavoidable.

01:05:48.920 --> 01:05:52.360
I mean, we can't pick and choose what kind of problems,

01:05:52.360 --> 01:05:55.240
but you know, the alignment problem,

01:05:55.240 --> 01:05:58.240
the hard version is yet to be solved,

01:05:58.240 --> 01:06:00.920
but we have many examples of governments

01:06:00.920 --> 01:06:04.280
building state capacity and having kind of,

01:06:04.280 --> 01:06:05.960
you know, shifting from very,

01:06:05.960 --> 01:06:10.200
very like clientelistic, sticky corrupt governments

01:06:10.200 --> 01:06:12.520
to sort of modernized governments

01:06:12.520 --> 01:06:14.840
where you know, state capacity is built

01:06:14.840 --> 01:06:16.520
and then that government can sort of break out

01:06:16.520 --> 01:06:18.760
of the middle income trap and become rich.

01:06:18.760 --> 01:06:23.440
You mentioned Estonia as an example of a country

01:06:23.440 --> 01:06:25.520
that's pretty advanced on the IT front,

01:06:25.520 --> 01:06:26.480
on the technology side.

01:06:26.480 --> 01:06:28.880
Maybe you could talk a bit about Estonia.

01:06:28.880 --> 01:06:30.920
Yeah, I would just say in general,

01:06:30.920 --> 01:06:34.440
it's hard for any organization to reform itself from within

01:06:34.440 --> 01:06:35.320
when there is path dependency,

01:06:35.320 --> 01:06:39.120
but I would say at least we have examples of it being done

01:06:39.120 --> 01:06:42.040
where we don't have examples of alignment being solved yet.

01:06:43.360 --> 01:06:45.440
When it comes to Estonia,

01:06:45.440 --> 01:06:46.840
you know, Estonia is an interesting case.

01:06:46.840 --> 01:06:48.960
It's sort of an exceptional case

01:06:48.960 --> 01:06:50.960
because after the fall of the Soviet Union

01:06:50.960 --> 01:06:54.360
and the breakup of the peripheral former Soviet states,

01:06:54.360 --> 01:06:57.200
they kind of had a blank slate, right?

01:06:57.200 --> 01:06:58.640
They also had a very young population

01:06:58.640 --> 01:07:01.840
and people who had a kind of hacker ethic

01:07:01.840 --> 01:07:03.400
within their civil service.

01:07:03.400 --> 01:07:06.400
And so with that blank slate and with that hacker ethic,

01:07:06.400 --> 01:07:09.680
they were very early to adopt and to foresee

01:07:09.680 --> 01:07:11.440
the way the internet was going to shape government

01:07:11.440 --> 01:07:14.880
through a variety of e-government reforms.

01:07:14.880 --> 01:07:17.520
So early in the late 90s and into the 2000s,

01:07:17.520 --> 01:07:20.360
they were some of the earliest to digitize

01:07:20.360 --> 01:07:23.440
their banking system like e-banking

01:07:23.440 --> 01:07:27.760
to build this system called X-Road,

01:07:27.760 --> 01:07:30.080
which is kind of like a cryptographically secured

01:07:30.080 --> 01:07:32.840
data exchange layer that resembles a blockchain,

01:07:32.840 --> 01:07:35.920
but it was about a decade before blockchain was invented.

01:07:35.920 --> 01:07:37.320
For exchanging information

01:07:37.320 --> 01:07:39.960
between different government entities,

01:07:39.960 --> 01:07:43.840
your medical information could be uploaded to the system

01:07:43.840 --> 01:07:45.600
and then be available to all systems

01:07:45.600 --> 01:07:48.800
that have the right to see that information.

01:07:48.800 --> 01:07:51.160
Exactly, in a way that's cryptographically secured

01:07:51.160 --> 01:07:52.000
and distributed.

01:07:52.000 --> 01:07:55.480
So if a missile hit the Department of Education,

01:07:55.480 --> 01:07:56.920
you don't lose your education records

01:07:56.920 --> 01:07:58.960
because it's distributed.

01:07:58.960 --> 01:08:03.080
And that also enabled an enormous amount of automation

01:08:03.080 --> 01:08:06.000
where, for instance, this is my understanding,

01:08:06.000 --> 01:08:07.120
a child born in Estonia,

01:08:07.120 --> 01:08:09.360
once you file that birth record,

01:08:09.360 --> 01:08:13.600
it more or less initiates a clock in the system

01:08:13.600 --> 01:08:16.520
that will then enroll your child in school

01:08:16.520 --> 01:08:19.160
when they turn four or five automatically

01:08:19.160 --> 01:08:21.840
because it knows that your child has aged

01:08:21.840 --> 01:08:24.840
and then unless it had a death record to cancel that out.

01:08:24.840 --> 01:08:27.720
That also means you can do taxes and transfers

01:08:27.720 --> 01:08:32.440
much simpler, you get your benefit within a week.

01:08:32.440 --> 01:08:34.960
It can integrate across different parts

01:08:34.960 --> 01:08:37.160
of public infrastructure,

01:08:37.160 --> 01:08:41.480
like use the same card to ride the bus

01:08:41.520 --> 01:08:43.880
as you do to launch a new business.

01:08:43.880 --> 01:08:45.560
And it also serves as a kind of platform

01:08:45.560 --> 01:08:50.360
for the private sector to do government by API,

01:08:50.360 --> 01:08:54.600
to build new services on top of government as a platform

01:08:54.600 --> 01:08:57.760
and integrate with government databases.

01:08:57.760 --> 01:08:58.920
Yeah, and so the point here for us

01:08:58.920 --> 01:09:01.960
is that institutional reform is possible,

01:09:01.960 --> 01:09:03.800
modernizing government is possible,

01:09:03.800 --> 01:09:05.280
at least under certain circumstances.

01:09:05.280 --> 01:09:10.000
We have proofs of concepts of this happening.

01:09:10.000 --> 01:09:12.000
The hard thing is the path dependency.

01:09:12.000 --> 01:09:13.240
There's always a strong instinct

01:09:13.240 --> 01:09:14.360
to wanna start from scratch

01:09:14.360 --> 01:09:16.200
and it's normally not advisable

01:09:16.200 --> 01:09:18.960
because it's too hard.

01:09:18.960 --> 01:09:23.960
And so this is why it's hard in the US.

01:09:24.280 --> 01:09:25.960
This is why you have African countries

01:09:25.960 --> 01:09:28.280
that leap progress in payment systems and so forth.

01:09:28.280 --> 01:09:31.080
The challenge of this decade or century

01:09:31.080 --> 01:09:33.760
is how do we solve that path dependency problem

01:09:33.760 --> 01:09:36.000
and how do we get to Estonia?

01:09:36.000 --> 01:09:37.280
It used to be get to Denmark.

01:09:37.280 --> 01:09:39.760
Now let's get to Estonia

01:09:39.760 --> 01:09:43.800
and find that sort of that pathway up mountain probable.

01:09:43.800 --> 01:09:47.880
Great, let's get to your wonderful series of blog posts

01:09:47.880 --> 01:09:49.720
on AI and Leviathan.

01:09:49.720 --> 01:09:52.640
In this context, what do we mean by Leviathan?

01:09:52.640 --> 01:09:53.600
Well, it's all interrelates.

01:09:53.600 --> 01:09:56.560
So Leviathan was the book Hobbes,

01:09:56.560 --> 01:10:00.320
Thomas Hobbes wrote at the start of the interregnum

01:10:00.320 --> 01:10:02.040
after the English Civil War.

01:10:02.040 --> 01:10:06.160
And it was basically his early political science,

01:10:06.160 --> 01:10:08.560
early defense of absolutist monarchy

01:10:08.560 --> 01:10:12.400
as a way to restore peace and order

01:10:12.400 --> 01:10:16.040
after a decade of infighting.

01:10:17.000 --> 01:10:22.000
And Hobbes kind of hit on some basic sort of structural

01:10:22.000 --> 01:10:26.720
game theoretic properties of why we have governments at all.

01:10:26.720 --> 01:10:29.080
He talked about life being nasty British and short

01:10:29.080 --> 01:10:31.720
in the state of nature, war of all against all.

01:10:31.720 --> 01:10:33.840
And peace is only restored

01:10:33.880 --> 01:10:36.960
when people who don't trust each other

01:10:36.960 --> 01:10:40.720
offload enforcement and policing responsibilities

01:10:40.720 --> 01:10:44.480
to a higher power that can then restore

01:10:44.480 --> 01:10:46.200
a degree of peace and order.

01:10:46.200 --> 01:10:49.280
AI and Leviathan is talking about,

01:10:49.280 --> 01:10:51.520
how does AI change the story?

01:10:51.520 --> 01:10:53.840
Does it reinforce the Leviathan?

01:10:53.840 --> 01:10:57.100
Does it lead to a digital police state China?

01:10:57.100 --> 01:11:00.680
Or is it something that we impose on ourselves?

01:11:00.680 --> 01:11:02.840
And we talked about how multimodal models

01:11:02.840 --> 01:11:05.560
could in principle be used to put a camera

01:11:05.560 --> 01:11:07.560
in everyone's house and have it just continuously monitoring

01:11:07.560 --> 01:11:09.280
for people doing any kind of crime.

01:11:09.280 --> 01:11:11.680
That's something that North Korea might do.

01:11:11.680 --> 01:11:15.040
In the US context, it's something that we're very liable

01:11:15.040 --> 01:11:17.760
to just voluntarily do to ourselves

01:11:17.760 --> 01:11:20.000
because we want to have ring cameras

01:11:20.000 --> 01:11:23.600
and Alexa assistants and so forth.

01:11:23.600 --> 01:11:26.440
And so that leads to a kind of bottom up Leviathan

01:11:26.440 --> 01:11:29.360
that is potentially no less oppressive

01:11:29.360 --> 01:11:30.440
and maybe even more oppressive

01:11:30.440 --> 01:11:34.480
because there's no one that we can appeal to

01:11:34.480 --> 01:11:36.400
to change the rules.

01:11:36.400 --> 01:11:39.000
Yeah, so Leviathan is one way to respond

01:11:39.000 --> 01:11:42.200
to technological change, but you mentioned two other ways

01:11:42.200 --> 01:11:44.800
we could alternatively respond.

01:11:44.800 --> 01:11:47.040
Right, so basically any time a technology

01:11:47.040 --> 01:11:49.320
greatly empowers the individual,

01:11:49.320 --> 01:11:53.400
it creates a potential negative externality, right?

01:11:53.400 --> 01:11:56.000
Hobbes called these our natural liberties.

01:11:56.000 --> 01:11:57.640
In the state of nature, I have a natural liberty

01:11:57.640 --> 01:12:00.440
to kill you or to strong arm you.

01:12:00.440 --> 01:12:05.440
And governments exist to revoke those natural liberties, right?

01:12:05.480 --> 01:12:08.120
But for a higher form of freedom, right?

01:12:08.120 --> 01:12:11.240
And so there are sort of any time a technology greatly

01:12:11.240 --> 01:12:12.760
increases human capabilities,

01:12:12.760 --> 01:12:14.440
these are the other humans.

01:12:14.440 --> 01:12:16.760
The three canonical ways we can adjust are,

01:12:16.760 --> 01:12:19.520
you know, ceding more authority to that higher power,

01:12:19.520 --> 01:12:21.400
the Leviathan option.

01:12:21.400 --> 01:12:23.200
And then the other two options are,

01:12:23.200 --> 01:12:25.240
you know, adaptation and mitigation

01:12:25.240 --> 01:12:27.240
and normative evolution.

01:12:27.240 --> 01:12:29.560
So the example I give is, you know,

01:12:29.560 --> 01:12:31.680
if suddenly we all had X-ray glasses

01:12:31.680 --> 01:12:33.840
and you could see through walls and see through clothing.

01:12:33.840 --> 01:12:37.280
You know, one option, we have a draconian,

01:12:37.280 --> 01:12:40.480
totalitarian crackdown that tries to seize

01:12:40.480 --> 01:12:42.280
all those X-ray glasses.

01:12:42.280 --> 01:12:45.920
Another option is we adjust normatively, culturally,

01:12:45.920 --> 01:12:49.040
that we, our privacy norms wither away

01:12:49.040 --> 01:12:51.440
and we stop caring about nudity.

01:12:52.480 --> 01:12:54.720
And then the other option is adaptation and mitigation

01:12:54.720 --> 01:12:57.240
where we put in, you know, mesh into our walls

01:12:57.240 --> 01:13:00.920
and where we're leaded shirts and pants.

01:13:02.880 --> 01:13:06.120
Yeah, I guess continuing that analogy a bit

01:13:06.120 --> 01:13:10.000
between the smart glasses and AI,

01:13:10.000 --> 01:13:13.400
you have this amazing write-up of ways

01:13:13.400 --> 01:13:18.280
in which AI can increase the informational resolution

01:13:18.280 --> 01:13:19.520
of the universe.

01:13:19.520 --> 01:13:21.440
So you give some examples that are,

01:13:21.440 --> 01:13:24.920
I think specifically of AI identifying people

01:13:24.920 --> 01:13:26.560
by gate, for example.

01:13:26.560 --> 01:13:28.480
Right, so gate recognition is nothing new.

01:13:28.480 --> 01:13:31.760
China has had advanced forms of gate recognition

01:13:31.760 --> 01:13:33.320
for a while now.

01:13:33.320 --> 01:13:35.560
So, you know, even if you cover your face,

01:13:35.560 --> 01:13:37.880
it turns out we're constantly throwing off

01:13:37.880 --> 01:13:41.640
sort of ambient information about ourselves, about everything.

01:13:41.640 --> 01:13:46.520
And the way you walk, the particular gate that you have

01:13:46.520 --> 01:13:48.720
is a unique identifier.

01:13:48.720 --> 01:13:51.600
Another example is galaxy surveys.

01:13:51.600 --> 01:13:54.400
There's, we've had from Hubble telescope to now,

01:13:54.400 --> 01:13:59.400
the JWST, tons of astronomical surveys

01:13:59.720 --> 01:14:02.000
of distant galaxies and so forth.

01:14:02.000 --> 01:14:05.000
And all of a sudden, all that old data,

01:14:05.000 --> 01:14:08.880
it's like that same data set is now more useful

01:14:08.880 --> 01:14:11.480
because applying more modern deep learning techniques,

01:14:11.480 --> 01:14:15.000
we can extract entropy that was in that data set,

01:14:15.000 --> 01:14:16.920
but we didn't have the tools to extract yet

01:14:17.120 --> 01:14:20.120
and discover that, you know, there are new galaxies

01:14:20.120 --> 01:14:22.920
or other phenomena that we missed.

01:14:22.920 --> 01:14:27.720
Another example you give is listening for keystrokes

01:14:27.720 --> 01:14:29.560
on a keyboard and extracting information

01:14:29.560 --> 01:14:33.080
about a password being typed in, for example,

01:14:33.080 --> 01:14:35.960
which is something that of course humans can do,

01:14:35.960 --> 01:14:38.680
but we can do with AI models.

01:14:38.680 --> 01:14:41.880
Yeah, so that was a paper showing that

01:14:41.880 --> 01:14:44.880
you can reconstruct keystrokes from an audio recording,

01:14:44.880 --> 01:14:46.160
including a Zoom conversation.

01:14:46.160 --> 01:14:47.440
So I hope you haven't typed in your password

01:14:47.440 --> 01:14:49.760
because people in the future, and so this goes to,

01:14:49.760 --> 01:14:51.600
you know, the fact that it's sort of retroactive,

01:14:51.600 --> 01:14:54.400
that like, even if the technology wasn't diffused yet,

01:14:54.400 --> 01:14:56.240
any Zoom conversation, any recording

01:14:56.240 --> 01:14:58.320
where someone typed their password in the future

01:14:58.320 --> 01:14:59.960
will be like those galaxy surveys

01:14:59.960 --> 01:15:02.120
where someone will go backwards in time

01:15:02.120 --> 01:15:04.480
and, you know, turn up the information resolution

01:15:04.480 --> 01:15:05.640
of that data.

01:15:05.640 --> 01:15:07.440
Yeah, this is pure speculation,

01:15:07.440 --> 01:15:11.080
but I wonder if, I mean, imagine anonymized people

01:15:11.080 --> 01:15:13.400
in interviews, say 10 years ago,

01:15:13.400 --> 01:15:15.360
whether they will be able to stay anonymous

01:15:15.400 --> 01:15:18.160
or whether AI will be able to extract the data

01:15:18.160 --> 01:15:20.240
about their face or their voice

01:15:20.240 --> 01:15:24.680
that wasn't technically possible when the interview aired.

01:15:24.680 --> 01:15:27.600
Yeah, exactly, there are already systems

01:15:27.600 --> 01:15:31.280
for like, depixelating, you probably do something similar

01:15:31.280 --> 01:15:34.400
for the voice modulation, and then also sort of, you know,

01:15:34.400 --> 01:15:37.520
again, going back to like, this ambient information

01:15:37.520 --> 01:15:41.920
we're always shedding, identifiers in the way we write,

01:15:41.920 --> 01:15:44.440
you know, the kind, where we place a comma,

01:15:44.440 --> 01:15:47.440
the kinds of adverbs we like to use and so forth.

01:15:47.440 --> 01:15:49.280
People just dramatically underrate, you know,

01:15:49.280 --> 01:15:50.840
how much information we're shedding,

01:15:50.840 --> 01:15:52.680
in part because we're blind to it.

01:15:52.680 --> 01:15:55.000
Some people who are taking great efforts

01:15:55.000 --> 01:15:59.560
to stay anonymous online, people in the cryptography space,

01:15:59.560 --> 01:16:02.040
for example, will put their writings

01:16:02.040 --> 01:16:03.800
through Google Translate to French

01:16:03.800 --> 01:16:07.320
and then back to English to erase subtle clues

01:16:07.320 --> 01:16:11.240
to how they, that could identify them personally.

01:16:11.240 --> 01:16:13.920
Why is AI so much better at tasks

01:16:13.920 --> 01:16:17.160
like the ones we just mentioned, compared to humans?

01:16:17.160 --> 01:16:19.760
Well, it goes back to what we were talking about

01:16:19.760 --> 01:16:22.920
with sort of putting information theoretic bounds on AGI.

01:16:22.920 --> 01:16:25.040
When you minimize the loss function

01:16:25.040 --> 01:16:25.880
in a machine learning model,

01:16:25.880 --> 01:16:28.800
you're trying to minimize the cross entropy loss.

01:16:28.800 --> 01:16:31.120
The cross entropy is, how many bits does it take

01:16:31.120 --> 01:16:33.880
to distinguish between two data streams?

01:16:33.880 --> 01:16:36.160
And if it takes a lot of bits to distinguish between the two,

01:16:36.160 --> 01:16:38.720
that means they're relatively indistinguishable.

01:16:38.720 --> 01:16:40.240
So that's going, again, to the Turing test.

01:16:40.240 --> 01:16:43.040
Like, if we have a Turing test where I can tell right away

01:16:43.080 --> 01:16:45.680
that the AI is different than the human,

01:16:45.680 --> 01:16:47.640
that suggests a high cross entropy.

01:16:47.640 --> 01:16:50.440
But if I could talk to it for days

01:16:50.440 --> 01:16:52.840
and do all kinds of adversarial questioning,

01:16:52.840 --> 01:16:54.200
I might still be able to, in the end,

01:16:54.200 --> 01:16:55.360
tell the difference between the two,

01:16:55.360 --> 01:16:58.200
but we've minimized that cross entropy loss.

01:16:58.200 --> 01:17:02.040
And so when you have any arbitrary data distribution

01:17:02.040 --> 01:17:03.600
that you're trying to predict,

01:17:03.600 --> 01:17:07.000
whether it's trying to predict galaxies

01:17:07.000 --> 01:17:09.320
and astronomical data or passwords

01:17:09.320 --> 01:17:13.240
from fingerprint data on a phone screen,

01:17:13.240 --> 01:17:17.520
all these things embed a kind of physical memory

01:17:17.520 --> 01:17:19.280
of the thing in question

01:17:19.280 --> 01:17:20.760
and can often be reconstructed

01:17:20.760 --> 01:17:23.760
through this kind of loss minimization,

01:17:23.760 --> 01:17:26.800
where you have a system that asymptotically

01:17:26.800 --> 01:17:30.280
extracts the entropy that was latent in the data.

01:17:30.280 --> 01:17:31.320
And this can be done in a way

01:17:31.320 --> 01:17:33.760
that is often quite striking,

01:17:33.760 --> 01:17:37.840
where we can, with stable diffusion,

01:17:37.840 --> 01:17:39.080
make fairly accurate predictions

01:17:39.080 --> 01:17:42.320
of what people are imagining in their mind

01:17:42.320 --> 01:17:44.240
using fMRI data.

01:17:44.240 --> 01:17:46.600
And fMRI data is like blood flow data in the brain.

01:17:46.600 --> 01:17:48.640
It's a very lossy representation

01:17:48.640 --> 01:17:50.520
of what ever's happening in the brain.

01:17:50.520 --> 01:17:53.200
But there's still enough latent entropy in there

01:17:53.200 --> 01:17:54.680
that we can kind of reverse engineer

01:17:54.680 --> 01:17:59.000
or decompress it into a folder picture.

01:17:59.000 --> 01:18:02.080
And this could turn into a form of lie detection.

01:18:02.080 --> 01:18:05.560
Yeah, I think it already basically has.

01:18:06.360 --> 01:18:12.160
If you have fMRI data or EEGs

01:18:12.160 --> 01:18:13.480
or other kinds of like direct brain data,

01:18:13.480 --> 01:18:16.040
it's probably a lot easier,

01:18:16.040 --> 01:18:20.000
but we already have systems that are over 95% accurate

01:18:20.000 --> 01:18:24.800
at detecting deception from just visual video recordings.

01:18:24.800 --> 01:18:27.480
We can see how all of this information

01:18:27.480 --> 01:18:29.200
that we are continually shedding

01:18:29.200 --> 01:18:32.800
gives rise to the possibility of alibiath

01:18:32.800 --> 01:18:36.800
than either of the private or of the government's kind.

01:18:36.800 --> 01:18:39.160
I wonder what role do you see

01:18:39.160 --> 01:18:41.600
open-sourcing AI models playing here?

01:18:41.600 --> 01:18:46.600
What are the trade-offs and risks in open-sourcing AI?

01:18:46.600 --> 01:18:49.040
Among the people who are most bullish to open-source,

01:18:49.040 --> 01:18:54.200
there's often a kind of libertarian ethic undergirding it.

01:18:54.200 --> 01:18:58.520
Regardless of whether that's a good idea or not,

01:18:58.520 --> 01:19:01.360
one of the things I'm trying to communicate to that group

01:19:01.360 --> 01:19:04.320
is to say that be careful what you wish for

01:19:04.320 --> 01:19:08.520
because of these kind of paradoxical Hobbesian dynamics.

01:19:08.520 --> 01:19:09.880
The fact that in America,

01:19:09.880 --> 01:19:12.520
you never know if someone has a gun or not.

01:19:12.520 --> 01:19:15.320
On the one hand, the Second Amendment enhances our freedom.

01:19:15.320 --> 01:19:17.840
On another hand, you don't get the sort of

01:19:17.840 --> 01:19:21.000
like everyone's doors unlocked and people are,

01:19:21.000 --> 01:19:24.440
like the police in England don't even have guns.

01:19:24.440 --> 01:19:26.120
There's a certain freedom that derives

01:19:26.120 --> 01:19:30.160
from us not all being heavily armed.

01:19:32.120 --> 01:19:38.920
Likewise, with open-sourcing powerful AI capabilities,

01:19:38.920 --> 01:19:41.480
it empowers you as an individual,

01:19:41.480 --> 01:19:43.400
but in general equilibrium,

01:19:43.400 --> 01:19:45.080
once we all have the capabilities,

01:19:45.080 --> 01:19:47.800
the world could look much more oppressive

01:19:47.800 --> 01:19:50.240
either because we're all spying on each other all the time

01:19:50.240 --> 01:19:51.720
and we can all see through each other's walls

01:19:51.720 --> 01:19:53.960
or because there's a backlash and the introduction

01:19:53.960 --> 01:19:56.320
of the viathan type solutions

01:19:56.320 --> 01:19:59.240
to restrict our ability to spy on each other all the time.

01:19:59.920 --> 01:20:03.480
My general sense is that we can only delay

01:20:03.480 --> 01:20:05.320
and we can't really prevent things

01:20:05.320 --> 01:20:06.760
from being open-source over the long run

01:20:06.760 --> 01:20:08.960
because there's a sort of trickle-down

01:20:08.960 --> 01:20:10.560
of compute requirements.

01:20:10.560 --> 01:20:12.360
But in the interim,

01:20:12.360 --> 01:20:15.040
there are definitely things that are valuable to open-source,

01:20:15.040 --> 01:20:19.360
having 70 billion parameter language models, not a threat.

01:20:19.360 --> 01:20:22.480
In fact, I think it's probably useful for alignment research

01:20:22.480 --> 01:20:25.080
for something like that to be open-source.

01:20:25.080 --> 01:20:26.600
But if you are a researcher

01:20:26.600 --> 01:20:30.160
and you've developed a emotional recognition model

01:20:30.160 --> 01:20:34.880
that can tell if, you know, with like 99% accuracy,

01:20:34.880 --> 01:20:38.240
whether someone is lying or not lying

01:20:38.240 --> 01:20:40.720
and whether your girlfriend loves you or not,

01:20:40.720 --> 01:20:44.760
like these things or the ability to see through walls

01:20:44.760 --> 01:20:49.760
using, like I talk about the use of Wi-Fi displacement.

01:20:49.760 --> 01:20:51.760
There are people who have built pose recognition models

01:20:51.760 --> 01:20:55.480
using the displacement of the electromagnetic frequency

01:20:55.480 --> 01:20:57.600
of your Wi-Fi and they can see,

01:20:57.600 --> 01:21:00.520
they can, there's wall penetrating

01:21:00.520 --> 01:21:01.560
so that you can see through walls.

01:21:01.560 --> 01:21:06.560
Like, what's the rush to put that on hugging face

01:21:07.400 --> 01:21:11.200
and to like make it as democratized as quickly as possible?

01:21:11.200 --> 01:21:15.480
I would say that if we value the adaptation

01:21:15.480 --> 01:21:20.320
and mitigation pathway as opposed to the Leviathan pathway,

01:21:20.320 --> 01:21:22.800
then there's a value in, you know,

01:21:22.800 --> 01:21:25.040
slow rolling some of these things.

01:21:25.080 --> 01:21:28.360
How do you think government power will be,

01:21:28.360 --> 01:21:32.080
or relative government power will be affected by AI?

01:21:32.080 --> 01:21:35.840
So you write somewhere in this long series of blog posts

01:21:35.840 --> 01:21:39.320
that AI will cause a net weakening of governments

01:21:39.320 --> 01:21:41.360
relative to the private sector.

01:21:41.360 --> 01:21:42.800
Why is that?

01:21:42.800 --> 01:21:46.040
Yeah, specifically Western liberal governments

01:21:46.040 --> 01:21:48.040
under constitutional constraints.

01:21:49.040 --> 01:21:52.880
So if you imagine society being on this kind of knife edge,

01:21:53.880 --> 01:21:55.960
I talked about this in the context of

01:21:55.960 --> 01:21:58.440
Theranosso Mogul's book, The Neural Corridor,

01:21:58.440 --> 01:21:59.760
where he describes liberal democracy

01:21:59.760 --> 01:22:01.120
as sort of being in this corridor

01:22:01.120 --> 01:22:03.040
between despotism on the one hand

01:22:03.040 --> 01:22:04.480
and anarchy on the other.

01:22:04.480 --> 01:22:08.240
And we sort of this day in this saddle path

01:22:08.240 --> 01:22:11.480
where society and the state are kept in balance.

01:22:11.480 --> 01:22:14.360
If you veer off that path, you can, on the one hand,

01:22:14.360 --> 01:22:16.320
you know, the state could become all powerful

01:22:16.320 --> 01:22:19.120
and that's the sort of China model

01:22:19.120 --> 01:22:21.080
or authoritarian digital surveillance state.

01:22:21.080 --> 01:22:23.720
And indeed, you know, China built up

01:22:23.720 --> 01:22:24.920
their digital surveillance state

01:22:24.920 --> 01:22:27.320
and their internet firewalls and so forth

01:22:27.320 --> 01:22:29.440
after watching the Arab Spring

01:22:29.440 --> 01:22:31.760
and seeing how the internet was destabilizing

01:22:31.760 --> 01:22:33.320
to weaker governments.

01:22:33.320 --> 01:22:36.720
And so I fully expect that AI will be very empowering

01:22:36.720 --> 01:22:39.440
and self-reinforcing of the power of the Chinese government.

01:22:39.440 --> 01:22:41.960
And indeed, there are draft regulations

01:22:41.960 --> 01:22:44.680
for large language models stipulate

01:22:44.680 --> 01:22:45.520
that you can't use the model

01:22:45.520 --> 01:22:48.760
to undermine national unity or challenge the government.

01:22:48.760 --> 01:22:50.200
And so they're baking that in.

01:22:50.200 --> 01:22:53.520
In liberal democracies, we think of ourselves

01:22:53.520 --> 01:22:54.640
as open societies.

01:22:55.560 --> 01:22:59.640
And the issue is that we're only open at the meta level.

01:22:59.640 --> 01:23:01.040
There's a public sphere, right?

01:23:01.040 --> 01:23:02.520
There's freedom of information laws.

01:23:02.520 --> 01:23:04.640
We have freedom of speech.

01:23:04.640 --> 01:23:07.800
I don't have freedom of speech if I walk into a Walmart.

01:23:07.800 --> 01:23:08.640
Wait, right?

01:23:08.640 --> 01:23:10.080
The Walmart is private property.

01:23:10.080 --> 01:23:12.240
In open societies, it's not that we don't have

01:23:12.240 --> 01:23:15.880
social credit scores and forms of,

01:23:15.880 --> 01:23:17.360
thicker forms of social regulation.

01:23:17.360 --> 01:23:19.520
It's just that we offload those functions

01:23:19.520 --> 01:23:22.080
onto competing private actors,

01:23:22.080 --> 01:23:26.520
whether it's a church that has very strict doctrines

01:23:26.520 --> 01:23:30.080
to be a member or other kinds of social clubs.

01:23:30.080 --> 01:23:31.400
The fact that these days,

01:23:31.400 --> 01:23:33.600
if you want to go to a comedy club,

01:23:33.600 --> 01:23:35.400
they'll often confiscate your phone at the door

01:23:35.400 --> 01:23:37.000
because they don't want you recording

01:23:37.000 --> 01:23:39.760
the comedian's set and putting it online.

01:23:39.760 --> 01:23:42.600
My anticipation is that because of those constitutional

01:23:42.600 --> 01:23:47.360
constraints that limit the ability of liberal democracies

01:23:47.400 --> 01:23:50.520
to go the China route, right?

01:23:50.520 --> 01:23:54.080
Because of our civil laws or bills of rights and so forth.

01:23:54.080 --> 01:23:56.360
And also because of a lot of these procedural constraints.

01:23:56.360 --> 01:23:59.200
This will naturally shift into the private sector.

01:23:59.200 --> 01:24:03.360
And we see that already with the use of AI

01:24:03.360 --> 01:24:05.320
for monitoring and employment,

01:24:05.320 --> 01:24:09.320
for policing speech in ways that would be illegal

01:24:09.320 --> 01:24:10.160
if done by the state,

01:24:10.160 --> 01:24:12.580
but are fine if done by Facebook.

01:24:12.580 --> 01:24:14.720
To the extent that the AI continues

01:24:14.720 --> 01:24:16.400
to increase these kind of negative externalities

01:24:16.640 --> 01:24:18.720
and therefore puts more value

01:24:18.720 --> 01:24:21.880
on having a sort of vertically integrated experience,

01:24:21.880 --> 01:24:26.440
a walled garden that can strip out the negative forms of AI

01:24:26.440 --> 01:24:29.880
and reinstate the degree of harmony between people

01:24:30.920 --> 01:24:35.000
that more and more of our social life will be mediated

01:24:35.000 --> 01:24:38.280
through these sort of private organizations

01:24:38.280 --> 01:24:41.480
rather than through a kind of open public sphere.

01:24:41.480 --> 01:24:45.200
Or you're imagining that government services

01:24:45.200 --> 01:24:48.680
will be gradually replaced by private services

01:24:48.680 --> 01:24:51.920
that are better able to respond.

01:24:51.920 --> 01:24:55.480
Won't governments fight to uphold individual rights?

01:24:55.480 --> 01:24:57.840
In Walmart or on Facebook,

01:24:57.840 --> 01:24:59.760
you are regulated in ways

01:24:59.760 --> 01:25:01.120
that the government couldn't regulate you,

01:25:01.120 --> 01:25:04.200
but you still have the choice to go to a target

01:25:04.200 --> 01:25:08.180
instead of Walmart or to go to or X instead of Facebook.

01:25:08.180 --> 01:25:09.440
Isn't that the fundamental thing?

01:25:09.440 --> 01:25:11.760
So the fundamental thing is the choice between services

01:25:11.760 --> 01:25:15.440
and won't governments uphold citizens' rights

01:25:15.440 --> 01:25:18.360
to make those kinds of choices?

01:25:18.360 --> 01:25:19.760
Yeah, no, I agree.

01:25:19.760 --> 01:25:24.760
And so this would be the defense of the liberal model

01:25:24.880 --> 01:25:28.720
is that we allow thicker forms of social regulation

01:25:28.720 --> 01:25:31.880
because it's moderated by choice and competition.

01:25:31.880 --> 01:25:35.560
And the issue with Chinese Confucian integralism

01:25:36.400 --> 01:25:41.400
isn't the fact that it's super oppressive.

01:25:41.920 --> 01:25:44.240
It's the fact that you only have one choice

01:25:44.240 --> 01:25:46.360
and you don't have voice or exit.

01:25:46.360 --> 01:25:51.360
So, but it's obviously a matter of degree, right?

01:25:52.880 --> 01:25:56.240
When ride hailing first arose,

01:25:56.240 --> 01:25:59.960
I remember back in 2013, 2014, it wasn't that long ago.

01:25:59.960 --> 01:26:03.240
I think Uber was founded in 2009,

01:26:03.240 --> 01:26:07.000
but it really only started taking off in the early 2010s.

01:26:07.000 --> 01:26:10.080
No, people thought it was crazy to ride a car

01:26:10.080 --> 01:26:11.440
with a stranger.

01:26:11.440 --> 01:26:13.120
And then within five years,

01:26:13.120 --> 01:26:17.440
it was the dominant mode of ride hailing.

01:26:17.440 --> 01:26:19.240
And in that five-year period,

01:26:19.240 --> 01:26:22.680
essentially we saw a kind of regime change in micro

01:26:22.680 --> 01:26:26.560
where taxis went from being something

01:26:26.560 --> 01:26:27.760
that was regulated by the state

01:26:27.760 --> 01:26:31.040
through these commissions that were granted,

01:26:31.160 --> 01:26:35.880
legal monopolies and used licensing and exams

01:26:35.880 --> 01:26:38.840
and other sort of brute force ways of ensuring quality

01:26:39.800 --> 01:26:41.360
to competing private platforms

01:26:41.360 --> 01:26:44.240
where you have Lyft or Uber to choose from.

01:26:44.240 --> 01:26:48.000
And they replaced the explicit governance of legal mandates

01:26:48.000 --> 01:26:52.640
with the competing governance of reputation mechanisms

01:26:52.640 --> 01:26:57.640
of dispute resolution systems of structured marketplaces

01:26:58.080 --> 01:27:00.880
that collapse the bargaining frictions, right?

01:27:00.880 --> 01:27:02.600
You never have to haggle with an Uber driver,

01:27:02.600 --> 01:27:03.960
you just sort of get in.

01:27:03.960 --> 01:27:06.560
And that was obviously a much better way

01:27:06.560 --> 01:27:09.520
of doing ride hailing.

01:27:09.520 --> 01:27:12.160
So even though there was sort of a violent resistance early on,

01:27:12.160 --> 01:27:13.080
literally like in France,

01:27:13.080 --> 01:27:14.480
they were throwing rocks off of bridges

01:27:14.480 --> 01:27:17.360
and cab drivers in New York were killing themselves.

01:27:17.360 --> 01:27:18.440
So for the people affected,

01:27:18.440 --> 01:27:20.400
it was a very dramatic sort of regime change,

01:27:20.400 --> 01:27:23.520
but for everyone else, it was a huge positive improvement.

01:27:23.520 --> 01:27:25.600
And yet it's only made possible

01:27:25.600 --> 01:27:28.240
because Uber has a social credit score.

01:27:28.240 --> 01:27:29.720
If your Uber rating goes too low,

01:27:29.720 --> 01:27:31.680
you'll get kicked off the platform.

01:27:31.680 --> 01:27:34.440
And so we're fine with social credit scores.

01:27:34.440 --> 01:27:38.280
It's when you only have one and don't have an option

01:27:38.280 --> 01:27:41.120
and it can follow you across all these different verticals

01:27:41.120 --> 01:27:42.200
that becomes a problem.

01:27:42.200 --> 01:27:46.240
Do you imagine that because of rising danger in the world,

01:27:46.240 --> 01:27:48.320
you talk about the externalities

01:27:48.320 --> 01:27:53.320
from the widespread implementation of AI all across society

01:27:54.320 --> 01:27:56.920
because of those dangers, those externalities,

01:27:56.920 --> 01:27:59.960
you know, you will either use Uber or whatever service

01:27:59.960 --> 01:28:03.200
or you kind of can't participate in society.

01:28:03.200 --> 01:28:06.400
Do you imagine increased pressure in that direction?

01:28:06.400 --> 01:28:08.640
It does seem to be a longer-term trend.

01:28:08.640 --> 01:28:11.600
I don't know if AI will accelerate it.

01:28:11.600 --> 01:28:16.840
I have another series of essays that I call separation anxiety.

01:28:17.840 --> 01:28:21.880
And it's a reference to the fact that in insurance markets,

01:28:22.840 --> 01:28:24.320
there's kind of two equilibria.

01:28:24.320 --> 01:28:25.720
There's the pooling equilibria

01:28:25.720 --> 01:28:28.160
where we're pooled together into one risk pool

01:28:28.160 --> 01:28:29.680
and there's a separating equilibria

01:28:29.680 --> 01:28:31.800
where the insurance pool unravels

01:28:31.800 --> 01:28:35.000
and we break up into the great power insurance

01:28:35.000 --> 01:28:37.840
for senior citizens who'd never had an accident

01:28:37.840 --> 01:28:39.040
and stuff like that.

01:28:39.040 --> 01:28:40.320
And it turns out that insurance markets

01:28:40.320 --> 01:28:41.720
are competitively unstable

01:28:41.720 --> 01:28:46.640
that without government regulation or social insurance,

01:28:46.640 --> 01:28:50.080
that insurance markets will naturally tend to unravel

01:28:50.080 --> 01:28:51.480
because of adverse selection

01:28:51.480 --> 01:28:54.920
into, you know, the high-risk people being in one pool

01:28:54.920 --> 01:28:57.360
and the low-risk people being in another pool.

01:28:57.360 --> 01:28:59.360
And it turns out you can sort of use that as a mental model

01:28:59.360 --> 01:29:03.800
to look at other kinds of implicit pooling equilibria, right?

01:29:03.800 --> 01:29:08.800
So within company wage distributions,

01:29:09.160 --> 01:29:11.960
often there is, you know, 20% of the workers

01:29:11.960 --> 01:29:13.760
who are doing 80% of the work,

01:29:13.760 --> 01:29:17.280
but they're pooled together under one wage structure.

01:29:17.280 --> 01:29:19.400
And that was sort of the dominant structure

01:29:20.000 --> 01:29:22.400
of the period of wage compression

01:29:22.400 --> 01:29:24.760
in the United States in the 50s and 60s.

01:29:24.760 --> 01:29:26.680
And once we had better monitoring technologies

01:29:26.680 --> 01:29:28.560
and were able to tell who were the 20%

01:29:28.560 --> 01:29:30.000
that were doing 80% of the work,

01:29:30.000 --> 01:29:35.000
it suddenly became possible to differentiate pay structure

01:29:35.320 --> 01:29:38.240
and a lot of the rise and inequality in the United States

01:29:38.240 --> 01:29:40.800
is actually between firm.

01:29:40.800 --> 01:29:42.080
So what happens is, you know,

01:29:42.080 --> 01:29:44.200
Ezra Klein is like the most productive wiz kid

01:29:44.200 --> 01:29:45.600
at the Washington Post and he realizes,

01:29:45.600 --> 01:29:48.640
why don't I just go start my own website, right?

01:29:48.640 --> 01:29:50.400
And so that dynamics are played out

01:29:50.400 --> 01:29:52.960
across a variety of domains,

01:29:52.960 --> 01:29:55.160
leads to a world that, you know,

01:29:55.160 --> 01:29:57.760
to the extent that these features are correlated,

01:29:57.760 --> 01:29:59.480
that does separate, right?

01:29:59.480 --> 01:30:02.640
Where you have, you know, the one-star Uber riders

01:30:02.640 --> 01:30:04.800
driving the one-star Uber drivers,

01:30:04.800 --> 01:30:06.640
the drivers driving the riders.

01:30:06.640 --> 01:30:08.760
And, you know, people who have, you know,

01:30:08.760 --> 01:30:11.760
the five-star Uber ratings and the perfect credit scores,

01:30:11.760 --> 01:30:13.880
self-sort into communities with other people

01:30:13.880 --> 01:30:15.840
with perfect driving records and perfect credit scores.

01:30:16.080 --> 01:30:18.600
And, you know, we see that to an extent already

01:30:18.600 --> 01:30:21.560
with the, you know, enclaves of, you know,

01:30:21.560 --> 01:30:23.800
rich zip codes with private schools

01:30:23.800 --> 01:30:26.320
and everyone is sort of self-selected.

01:30:26.320 --> 01:30:29.920
AI could, it seems to me that AI would exacerbate that.

01:30:29.920 --> 01:30:31.860
I mean, at first blush, just because it,

01:30:31.860 --> 01:30:33.640
going back to the point about signal extraction,

01:30:33.640 --> 01:30:36.080
it can find all these different ways,

01:30:36.080 --> 01:30:38.800
you're a high-risk type and I'm a low-risk type and so forth,

01:30:38.800 --> 01:30:42.200
that are probably latent in all kinds of data

01:30:42.200 --> 01:30:43.480
that we don't even need to get permission

01:30:43.480 --> 01:30:44.320
to the insurance company.

01:30:44.320 --> 01:30:46.360
They'll just like, the same way that they use,

01:30:46.360 --> 01:30:49.800
like smoking or going to a gym as a proxy,

01:30:49.800 --> 01:30:51.200
there's all kinds of proxies they could use

01:30:51.200 --> 01:30:54.140
and likewise for employers and how they pay people.

01:30:54.140 --> 01:30:59.080
Society kind of runs on us not being entirely open

01:30:59.080 --> 01:31:00.800
and entirely honest all the time.

01:31:00.800 --> 01:31:03.320
Otherwise, you wouldn't be able to have

01:31:03.320 --> 01:31:06.200
kind of smooth social interactions and so on.

01:31:06.200 --> 01:31:09.920
Won't these norms be inherited by the way we use AI?

01:31:09.920 --> 01:31:13.000
Yeah, I think this is a really big issue.

01:31:13.000 --> 01:31:14.560
And I'm a big fan of Robin Hansen

01:31:14.560 --> 01:31:19.560
and a lot of his writing on social status and signaling

01:31:20.160 --> 01:31:24.600
is sort of presenting humans as basically hypocrites,

01:31:24.600 --> 01:31:28.680
like we're constantly deceiving other people

01:31:28.680 --> 01:31:31.060
and we often deceive ourselves

01:31:31.060 --> 01:31:33.360
so it's better to deceive others

01:31:33.360 --> 01:31:38.160
as the evolutionary biologist Robert Trivers

01:31:38.160 --> 01:31:39.000
has pointed out.

01:31:40.000 --> 01:31:45.000
So, all the kinds of polite lies that we tell

01:31:45.800 --> 01:31:49.280
are I think critical lubricants to the social interaction

01:31:49.280 --> 01:31:53.800
and actually like it's good that there's a gap

01:31:53.800 --> 01:31:55.880
between our stated and revealed preference.

01:31:55.880 --> 01:31:57.680
I think a world where we all lived

01:31:57.680 --> 01:32:00.240
our stated preference could be hellish

01:32:00.240 --> 01:32:01.880
because we don't actually mean it.

01:32:04.400 --> 01:32:06.760
And AI has a direct implication on that

01:32:06.800 --> 01:32:09.120
because if I can have a pair of AR glasses on

01:32:09.120 --> 01:32:10.920
that will tell me if you're interested,

01:32:10.920 --> 01:32:13.680
if you're bored, if you're over on a date

01:32:13.680 --> 01:32:16.720
and are you really attracted to me,

01:32:17.800 --> 01:32:22.160
all that sort of polite veneer that social veil

01:32:22.160 --> 01:32:24.720
could be lifted in a way that

01:32:24.720 --> 01:32:28.520
we'll probably want to coordinate to not do, right?

01:32:28.520 --> 01:32:30.240
But again, it's this Nash equilibrium

01:32:30.240 --> 01:32:31.760
where it's in my interest to know

01:32:31.760 --> 01:32:34.400
whether you're interested or bored.

01:32:34.400 --> 01:32:36.120
And so I'll wanna have the glasses on

01:32:36.120 --> 01:32:39.000
and my ideal world is where only I have the glasses

01:32:39.000 --> 01:32:39.840
and you don't.

01:32:41.320 --> 01:32:44.560
And the other way that our hypocrisy is being exposed

01:32:44.560 --> 01:32:48.400
and challenged is the need to explicate

01:32:48.400 --> 01:32:50.280
the utility function that we want these models

01:32:50.280 --> 01:32:51.440
to work under.

01:32:52.720 --> 01:32:55.360
We need to formalize human values

01:32:55.360 --> 01:32:57.160
if we want to align these models.

01:32:57.160 --> 01:32:59.600
And so then we have to be honest and open

01:32:59.600 --> 01:33:02.400
about the fact that our stated preferences

01:33:02.400 --> 01:33:05.040
probably aren't our true preferences.

01:33:05.040 --> 01:33:07.320
And that's a very challenging thing

01:33:07.320 --> 01:33:11.600
because it cuts right to the nature of the human condition

01:33:11.600 --> 01:33:15.080
and involves topics that are intrinsically things

01:33:15.080 --> 01:33:17.320
that we lie to ourselves about.

01:33:17.320 --> 01:33:19.960
You have what you call a timeline

01:33:19.960 --> 01:33:22.880
of a techno feudalist future,

01:33:22.880 --> 01:33:24.640
which I found quite interesting.

01:33:24.640 --> 01:33:27.440
Yeah, it's great writing and it's very detailed.

01:33:27.440 --> 01:33:29.760
We don't have to go through it in all of its detail,

01:33:29.760 --> 01:33:32.200
but maybe you could tell the story of what happens

01:33:32.200 --> 01:33:34.280
in what you call the default scenario.

01:33:34.280 --> 01:33:37.440
This is the scenario in which Western liberal democracies

01:33:37.440 --> 01:33:39.320
are too slow to adapt to AI.

01:33:39.320 --> 01:33:41.400
And so we get something like a replacement

01:33:41.400 --> 01:33:44.520
of government services with more private services.

01:33:44.520 --> 01:33:48.000
What happens in the techno feudalist future?

01:33:48.000 --> 01:33:49.960
Right, and it's sort of piggybacks

01:33:49.960 --> 01:33:51.800
in everything you've just been discussing, right?

01:33:51.800 --> 01:33:53.960
And I don't want techno feudalists

01:33:53.960 --> 01:33:56.160
to carry too much of a pejorative.

01:33:56.160 --> 01:33:57.880
I'm sort of using it descriptively.

01:33:59.000 --> 01:34:03.680
And certainly some people would prefer this world

01:34:04.320 --> 01:34:07.880
so the example of Uber and Lyft displacing taxi caps

01:34:07.880 --> 01:34:10.400
is sort of a version of this in micro

01:34:10.400 --> 01:34:13.960
where we go from this regulated taxi commission

01:34:13.960 --> 01:34:17.000
to competing private platforms that use various forms

01:34:17.000 --> 01:34:21.240
of artificial intelligence and information technology

01:34:21.240 --> 01:34:23.560
to replace the thing that was being done

01:34:23.560 --> 01:34:25.080
by explicit regulation.

01:34:26.040 --> 01:34:30.760
And as AI progresses and both creates a variety

01:34:30.760 --> 01:34:32.000
of new negative externalities,

01:34:32.040 --> 01:34:34.880
whether it's like suicide drones

01:34:37.200 --> 01:34:39.400
or the ability to spy on each other,

01:34:40.480 --> 01:34:43.600
there's going to be a demand for new forms of security

01:34:43.600 --> 01:34:47.880
and also kinds of opt-in jurisdictions

01:34:47.880 --> 01:34:49.400
that tie our hands in the same way

01:34:49.400 --> 01:34:54.000
that we give up our phone before we go into the comedy club.

01:34:54.000 --> 01:34:57.360
And so I think this leads to a kind of development

01:34:57.360 --> 01:35:01.480
of clubs, the kind of club structure

01:35:01.480 --> 01:35:03.120
that may be at the city level

01:35:03.120 --> 01:35:06.760
as the vertically integrated walled garden

01:35:06.760 --> 01:35:11.280
that will police and build defensive technologies

01:35:11.280 --> 01:35:14.320
around the misuse of AI and at the same time

01:35:14.320 --> 01:35:18.200
provide a variety of like new AI native public goods

01:35:18.200 --> 01:35:21.480
that are only possible once AI unlocks them.

01:35:21.480 --> 01:35:24.600
And it's easy to see how this could very quickly displace

01:35:25.520 --> 01:35:29.360
and eat away at formal government services

01:35:29.360 --> 01:35:31.640
both because we saw it already with Uber,

01:35:31.640 --> 01:35:34.560
but also if you map that model

01:35:34.560 --> 01:35:37.120
to other areas of regulatory life,

01:35:38.680 --> 01:35:42.040
does it make sense to have a USDA farm inspector,

01:35:42.040 --> 01:35:44.240
a human person has to go to a commercial farm

01:35:44.240 --> 01:35:47.160
and maybe only goes to that farm once every few years

01:35:47.160 --> 01:35:49.800
because there's so many farms and only so many people.

01:35:49.800 --> 01:35:51.320
And it does a little checklist and says,

01:35:51.320 --> 01:35:53.320
oh, you're not abusing the animals

01:35:53.320 --> 01:35:55.360
and you get all the process in place

01:35:55.360 --> 01:35:58.080
and you get the USDA stamp of approval

01:35:58.080 --> 01:36:02.960
or does it make more sense to have multimodal cameras on

01:36:02.960 --> 01:36:07.960
in the farm 24-7 that are continuously generating reports

01:36:08.680 --> 01:36:10.960
that throw up a red flag anytime someone sneezes

01:36:10.960 --> 01:36:12.080
on the conveyor belt.

01:36:12.080 --> 01:36:15.920
And to the extent that government is going to be slow

01:36:15.920 --> 01:36:19.440
at adopting that, will there be a push

01:36:19.440 --> 01:36:23.800
for the kind of Uber model of governance as a platform

01:36:23.800 --> 01:36:27.320
where you have the kind of AI underwriter,

01:36:27.320 --> 01:36:31.920
the consumer reports that sells these farms,

01:36:31.920 --> 01:36:33.800
the camera technology and the monitoring technology

01:36:33.800 --> 01:36:36.920
and builds their own set of compliant standards.

01:36:36.920 --> 01:36:38.720
And then you want to go to those farms

01:36:38.720 --> 01:36:40.880
or would have you that have the stamp of approval

01:36:40.880 --> 01:36:43.600
of the underwriter because it's much higher trust.

01:36:43.600 --> 01:36:47.160
It's sort of like the end of asymmetric information.

01:36:47.160 --> 01:36:52.160
And you can map that from food safety to product safety

01:36:53.400 --> 01:36:56.720
to OSHA and workplace safety.

01:36:56.720 --> 01:36:57.960
There's other parts of government

01:36:57.960 --> 01:37:00.600
that maybe just rendered completely obsolete, right?

01:37:00.600 --> 01:37:02.000
Like once we have self-driving cars

01:37:02.000 --> 01:37:04.760
that are a thousand next more safe than humans,

01:37:04.760 --> 01:37:07.560
do we need a national highway traffic safety administration?

01:37:09.160 --> 01:37:12.160
Once we have sensors that are privately owned everywhere

01:37:12.160 --> 01:37:14.640
and can model weather patterns better

01:37:14.640 --> 01:37:17.280
than the national oceanic administration,

01:37:17.280 --> 01:37:19.320
do we need a national weather service

01:37:19.320 --> 01:37:21.440
or could we bootstrap that ourselves?

01:37:21.440 --> 01:37:24.880
And then once we have AI,

01:37:24.880 --> 01:37:27.080
accelerated drug discovery,

01:37:27.080 --> 01:37:30.960
do we want to rely on the FDA to be a kind of choke point

01:37:30.960 --> 01:37:34.680
to do these sort of frequentist clinical trials

01:37:34.680 --> 01:37:38.560
that are inherently slow and don't capture

01:37:38.560 --> 01:37:43.400
the kind of idiosyncrasies and heterogeneity

01:37:43.400 --> 01:37:46.440
that could be unlocked by personalized medicine?

01:37:46.440 --> 01:37:51.040
Or do we move to an alternative drug approval process

01:37:51.040 --> 01:37:52.280
that is maybe non-governmental,

01:37:52.280 --> 01:37:55.440
but much more rapid and much more personalized?

01:37:55.440 --> 01:37:56.880
So that's the overall picture.

01:37:56.880 --> 01:37:59.960
I'll just run through the timeline here,

01:37:59.960 --> 01:38:02.160
picking up on some of your comments

01:38:02.160 --> 01:38:04.880
that I thought were especially interesting.

01:38:04.880 --> 01:38:08.760
You write, this is in 2024 to 2027,

01:38:08.760 --> 01:38:13.280
you write that the internet will become vulcanized

01:38:13.280 --> 01:38:18.280
and it will become more secure and more private in a sense.

01:38:18.600 --> 01:38:20.040
Why does that happen?

01:38:20.040 --> 01:38:23.400
We're already starting to see this a little bit, right?

01:38:23.400 --> 01:38:26.040
Once people realize that the data

01:38:26.040 --> 01:38:27.840
that's being generated on Stack Overflow

01:38:27.840 --> 01:38:30.400
or Reddit or whatever is valuable

01:38:30.400 --> 01:38:31.440
for training these models,

01:38:31.440 --> 01:38:33.240
suddenly everyone's closing their API

01:38:34.560 --> 01:38:37.640
and consequently Google Search and the Google index

01:38:37.640 --> 01:38:40.720
have sort of started to degrade already.

01:38:40.720 --> 01:38:42.240
So I think that will continue

01:38:42.240 --> 01:38:46.120
for the kind of privatization of data reasons.

01:38:46.120 --> 01:38:47.800
Then when you also think about

01:38:47.800 --> 01:38:52.280
how websites are going to handle sort of the growth of bots

01:38:52.280 --> 01:38:55.040
and catfishes and catfish attacks

01:38:55.040 --> 01:38:58.240
and cyber attacks and so forth,

01:38:58.240 --> 01:38:59.760
it makes sense that we're going to move

01:38:59.760 --> 01:39:04.080
from a sort of open, everything goes kind of Twitter-esque

01:39:04.080 --> 01:39:07.640
platform to things that are much more closed

01:39:07.640 --> 01:39:11.920
because they require human verification

01:39:11.920 --> 01:39:15.560
and identity verification to sort of build the trust

01:39:15.600 --> 01:39:18.560
that you're talking to other people and not deepfakes.

01:39:18.560 --> 01:39:20.600
And then medium-term, again,

01:39:20.600 --> 01:39:23.240
over this sort of 2024 to 2027 horizon,

01:39:24.200 --> 01:39:27.160
you could also start to see the emergence

01:39:27.160 --> 01:39:32.160
of intelligent malware, sort of modern AI native cyber attacks

01:39:34.240 --> 01:39:38.320
that could be devastating to legacy cyber security

01:39:38.320 --> 01:39:41.640
infrastructure in a way that I talk about good heart

01:39:41.640 --> 01:39:43.240
and back to the famous Moore's worm

01:39:43.240 --> 01:39:45.240
that in the late 80s,

01:39:45.240 --> 01:39:46.720
basically shut down the early internet.

01:39:46.720 --> 01:39:48.880
They literally had to partition the internet

01:39:48.880 --> 01:39:53.080
and turn it off so they could read the network at the worm.

01:39:53.080 --> 01:39:54.200
So for all those reasons,

01:39:54.200 --> 01:39:56.120
I think you start to see the internet balkanize

01:39:56.120 --> 01:39:58.160
and then particularly at the international level,

01:39:58.160 --> 01:40:02.280
we're already starting to see sort of the semiconductor

01:40:02.280 --> 01:40:05.320
supply chain become critical part of national security.

01:40:05.320 --> 01:40:07.680
The growth of the Chinese firewall,

01:40:07.680 --> 01:40:10.960
the European Union is going to have to have their own

01:40:10.960 --> 01:40:13.160
quasi firewall and they kind of already do with GDPR

01:40:13.160 --> 01:40:15.880
and the EU AI Act.

01:40:15.880 --> 01:40:17.600
And so the kind of nationalization of compute

01:40:17.600 --> 01:40:19.240
and telecommunications infrastructure

01:40:19.240 --> 01:40:22.120
that will take off once people understand

01:40:22.120 --> 01:40:24.480
both the security risks and the value prop

01:40:24.480 --> 01:40:28.400
of owning the infrastructure for the AI revolution.

01:40:28.400 --> 01:40:31.960
Yeah, in 2028 to 2031,

01:40:31.960 --> 01:40:35.800
you write about alignment turning out to be easier

01:40:35.800 --> 01:40:39.560
than we thought with the increasing scale of the model.

01:40:39.760 --> 01:40:41.040
That was somewhat surprising to me.

01:40:41.040 --> 01:40:43.520
Why does alignment turn out to be easier?

01:40:44.760 --> 01:40:47.960
And part of this is imagining a scenario

01:40:47.960 --> 01:40:50.040
where alignment is easy.

01:40:50.040 --> 01:40:53.880
So we can talk about what happens if alignment is easy.

01:40:53.880 --> 01:40:55.200
But I think there are reasons to think

01:40:55.200 --> 01:40:56.880
that the classic alignment problem

01:40:56.880 --> 01:40:59.360
will be easier than people think.

01:40:59.360 --> 01:41:02.000
I think that some of the early intuitions

01:41:02.000 --> 01:41:04.680
about the hardness of the alignment problem

01:41:04.680 --> 01:41:07.520
were rooted in a view of maybe AI turns out

01:41:07.520 --> 01:41:10.480
to be a very simple algorithm

01:41:10.480 --> 01:41:13.480
rather than like a deep neural network

01:41:13.480 --> 01:41:16.280
that achieves its generality because of its depth.

01:41:16.280 --> 01:41:19.760
Clearly the kind of value,

01:41:19.760 --> 01:41:22.320
I forget what Eliezer Kewski called it,

01:41:22.320 --> 01:41:25.880
but there's like a value alignment problem

01:41:25.880 --> 01:41:29.120
where how do we teach the model our values?

01:41:29.120 --> 01:41:32.240
But that part of the alignment problem seems trivial now

01:41:32.240 --> 01:41:35.240
because our large English models aren't like

01:41:35.240 --> 01:41:39.800
autistic savants, they're actually incredibly sensitive

01:41:39.800 --> 01:41:44.800
to soft human concepts of value and context.

01:41:45.640 --> 01:41:47.480
They're not going to have a,

01:41:47.480 --> 01:41:49.800
the paperclip maximizer sort of monkey paw

01:41:49.800 --> 01:41:53.040
kind of threat models don't really make sense in that world.

01:41:53.040 --> 01:41:56.640
But there's a difference between the output of the model

01:41:56.640 --> 01:42:00.280
and the weights or what the model has learned.

01:42:00.280 --> 01:42:03.400
And so just because a model can say,

01:42:03.440 --> 01:42:05.400
it can say the right words that we wanted to say,

01:42:05.400 --> 01:42:07.640
but what has it actually learned?

01:42:07.640 --> 01:42:09.040
We are not entirely sure.

01:42:09.040 --> 01:42:12.280
And so it has learned to satisfy human values to some extent,

01:42:12.280 --> 01:42:16.560
but has it learned to want to comply with human value

01:42:18.320 --> 01:42:21.600
out of distribution sort of, yeah, in other domains

01:42:21.600 --> 01:42:25.120
and in a deep sense, I'm not sure about that.

01:42:25.120 --> 01:42:25.960
No, I agree.

01:42:25.960 --> 01:42:27.920
So I'm sort of just laying some of my groundwork

01:42:27.920 --> 01:42:30.640
for to expand my priors on this.

01:42:30.640 --> 01:42:32.440
I agree, like, you know, reinforcement learning

01:42:32.440 --> 01:42:35.760
from human feedback is not alignment.

01:42:37.200 --> 01:42:38.640
You know, the same way that, you know,

01:42:38.640 --> 01:42:41.720
you could argue that like the co-evolution of cats and dogs

01:42:41.720 --> 01:42:44.620
with humans led to a kind of reinforcement learning

01:42:44.620 --> 01:42:48.320
from human feedback in their, in their short run evolution

01:42:48.320 --> 01:42:50.840
that, you know, made them look up, appear as if they,

01:42:50.840 --> 01:42:54.080
you know, they experienced guilt and shame

01:42:54.080 --> 01:42:55.880
and these human emotions when in fact they're,

01:42:55.880 --> 01:42:58.080
they're just sort of a simulacra of those emotions

01:42:58.080 --> 01:43:01.320
because it means that we'll give them a treat.

01:43:01.320 --> 01:43:04.720
But I've done plenty of episodes on deceptions

01:43:04.720 --> 01:43:06.280
in these models and so on.

01:43:06.280 --> 01:43:07.960
We don't have to go through that,

01:43:07.960 --> 01:43:09.840
but I just wanted to point out that, yeah,

01:43:09.840 --> 01:43:11.880
maybe there's some complexities there.

01:43:11.880 --> 01:43:14.680
So the first, my first prior is that these models

01:43:14.680 --> 01:43:18.680
aren't autistic savants the way they might have been.

01:43:18.680 --> 01:43:22.240
The second is going back to universality.

01:43:22.240 --> 01:43:26.720
Well, it is true that you, there are, that, you know,

01:43:26.720 --> 01:43:28.920
it's possible through reinforcement learning

01:43:28.920 --> 01:43:30.000
from human feedback, for example,

01:43:30.120 --> 01:43:32.800
that you, you're, you're not selecting for honesty

01:43:32.800 --> 01:43:36.280
or selecting for a deep pick up honesty,

01:43:36.280 --> 01:43:38.760
but in the bigger picture, the intuition

01:43:38.760 --> 01:43:40.960
that these models are converging or convergent

01:43:40.960 --> 01:43:44.960
with human representations should give you some confidence

01:43:44.960 --> 01:43:46.680
that they're not going to be as alien

01:43:46.680 --> 01:43:48.600
as we, as we think they will be.

01:43:48.600 --> 01:43:53.600
It's also useful input for thinking about interpretability.

01:43:53.880 --> 01:43:56.880
You know, some recent work showing that discussing

01:43:56.880 --> 01:43:58.520
sort of representation, interpretability

01:43:58.520 --> 01:44:01.320
where, where instead of trying to interpret

01:44:01.320 --> 01:44:03.560
individual neurons, you interpret sort of collections

01:44:03.560 --> 01:44:06.480
of neurons and, and, and circuitry

01:44:06.480 --> 01:44:09.680
through sort of human interpretable representations.

01:44:09.680 --> 01:44:11.400
And one of the, one of the lessons of universality

01:44:11.400 --> 01:44:13.640
is that like some of these high level human concepts,

01:44:13.640 --> 01:44:17.400
like happiness or, or anxiety,

01:44:17.400 --> 01:44:22.280
like these seem like vague psychological abstractions

01:44:22.280 --> 01:44:24.240
that there's no way they can correspond

01:44:24.240 --> 01:44:26.800
to like the micro foundations of the way our brain works.

01:44:26.800 --> 01:44:29.640
But in fact, they may actually be very efficient,

01:44:29.640 --> 01:44:31.400
low dimensional ways of talking about

01:44:31.400 --> 01:44:32.720
what's happening in our brain.

01:44:32.720 --> 01:44:34.760
And then the third thing is, I think that I just

01:44:34.760 --> 01:44:37.440
have seen, you know, my sense is that the work

01:44:37.440 --> 01:44:40.480
on interpretability is actually making some,

01:44:40.480 --> 01:44:42.360
some good progress, you know,

01:44:42.360 --> 01:44:43.920
whether it can scale is another question,

01:44:43.920 --> 01:44:46.360
but I think we'll get there.

01:44:46.360 --> 01:44:49.760
In my timeline, I talk about sort of AGI level models

01:44:49.760 --> 01:44:53.240
within the human limit, human emulator plus domain.

01:44:53.240 --> 01:44:55.280
I do later on talk about like super intelligence

01:44:55.280 --> 01:44:57.360
emerging maybe in the 2040s.

01:44:57.360 --> 01:44:58.440
And that's another story, right?

01:44:58.440 --> 01:45:01.440
And so I think some of this stuff maybe goes out the window

01:45:01.440 --> 01:45:03.360
if we have, you know, models that are bigger

01:45:03.360 --> 01:45:06.080
than all the brains combined and have like

01:45:06.080 --> 01:45:08.240
strong situational awareness.

01:45:08.240 --> 01:45:11.000
But I don't think that happens this decade.

01:45:11.000 --> 01:45:14.360
Certainly, certainly not with the current way

01:45:14.360 --> 01:45:15.200
we're building these models,

01:45:15.200 --> 01:45:16.560
with the way we're currently building these models,

01:45:16.560 --> 01:45:18.720
I think it's comes much closer to a stimuli

01:45:18.720 --> 01:45:20.200
lack of the human brain.

01:45:20.200 --> 01:45:21.040
Got it.

01:45:21.040 --> 01:45:26.040
In 2036 to 2039, you talk about robotics

01:45:27.480 --> 01:45:29.240
being solved to the same extent,

01:45:29.240 --> 01:45:30.960
or maybe even in the same way

01:45:30.960 --> 01:45:33.720
as we are now solving a language.

01:45:33.720 --> 01:45:35.640
That would, I found that super interesting.

01:45:35.640 --> 01:45:39.600
Explain to me why, why would robotics

01:45:39.600 --> 01:45:43.680
suddenly or quite relatively suddenly become much easier?

01:45:43.680 --> 01:45:45.840
Robotics have been fighting for decades

01:45:45.840 --> 01:45:49.640
to get these models to walk relatively

01:45:50.640 --> 01:45:54.400
unencumbered and it's been an uphill battle.

01:45:54.400 --> 01:45:57.880
Yeah, why can we solve robotics in the 2030s?

01:45:57.880 --> 01:46:00.720
This may end up happening sooner than I'd project,

01:46:00.720 --> 01:46:03.760
but I mean, if you look at LLMs,

01:46:03.760 --> 01:46:06.320
what one of the stylized sort of trends

01:46:06.320 --> 01:46:09.080
with large language models is that, you know,

01:46:09.080 --> 01:46:13.240
that natural language processing went from being this,

01:46:13.240 --> 01:46:15.160
you know, the study of how to make machines

01:46:15.160 --> 01:46:17.560
understand language went from being, you know,

01:46:17.640 --> 01:46:19.520
a dozen different sub-disciplines,

01:46:19.520 --> 01:46:20.840
you know, people working on parsing,

01:46:20.840 --> 01:46:22.000
people are working on syntax,

01:46:22.000 --> 01:46:22.960
people are working on semantics,

01:46:22.960 --> 01:46:26.360
people are working on summarization and classification.

01:46:26.360 --> 01:46:28.480
And these are all different, you know, directions,

01:46:28.480 --> 01:46:29.560
research directions.

01:46:29.560 --> 01:46:31.680
And then along comes transformer models

01:46:31.680 --> 01:46:34.280
and, you know, it's just supplants everything

01:46:34.280 --> 01:46:36.360
and LLMs can do it all.

01:46:36.360 --> 01:46:40.520
And I think robotics is sort of still in that ancient regime

01:46:40.520 --> 01:46:43.880
where a lot of, you know, what Boston Dynamics does

01:46:43.880 --> 01:46:46.080
is ad hoc control models,

01:46:46.080 --> 01:46:49.960
analytically solvable, you know, differential equations,

01:46:49.960 --> 01:46:52.680
different kinds of object recognition modules

01:46:52.680 --> 01:46:56.280
and control action loops and so forth.

01:46:56.280 --> 01:46:58.920
And so it's still in that like early NLP phase

01:46:58.920 --> 01:47:01.080
where they have 12 different sub-disciplines

01:47:01.080 --> 01:47:02.880
and they're sort of mashing them together.

01:47:02.880 --> 01:47:05.200
And of course you get something that's not very robust.

01:47:05.200 --> 01:47:08.920
I think we're already starting to see that paradigm shift

01:47:08.920 --> 01:47:12.440
to, you know, end-to-end neural network trained models,

01:47:12.480 --> 01:47:15.600
like, you know, Tesla, for instance,

01:47:16.680 --> 01:47:19.840
I think one of the reasons why Tesla cars

01:47:19.840 --> 01:47:23.960
had a sort of temporary decline in performance

01:47:23.960 --> 01:47:25.840
was because they were undergoing the transition

01:47:25.840 --> 01:47:28.080
from these ad hoc lane detectors

01:47:28.080 --> 01:47:30.640
and stop sign detectors and stuff like that

01:47:30.640 --> 01:47:34.920
to a fully end-to-end neural network transformer-based model.

01:47:34.920 --> 01:47:37.880
And that turned out to be much more robust way

01:47:37.880 --> 01:47:40.240
to train the model because, you know,

01:47:40.240 --> 01:47:41.760
stop signs look different in different countries

01:47:41.760 --> 01:47:43.760
and like maybe stop sign isn't the thing you care about,

01:47:43.760 --> 01:47:45.560
really, so on and so forth.

01:47:46.920 --> 01:47:49.160
And so I think the transformer sort of scale,

01:47:49.160 --> 01:47:53.600
deep learning revolution is only now coming to robotics

01:47:53.600 --> 01:47:56.920
and people in that field have, are a little bit cynical

01:47:56.920 --> 01:48:01.520
because they're used to relatively small RL models

01:48:01.520 --> 01:48:05.040
thinking that like the fit with, you know, actuators

01:48:05.040 --> 01:48:08.160
and some of the hardware is like a really challenging problem

01:48:08.160 --> 01:48:11.440
and also believing that we don't have the data sets

01:48:11.440 --> 01:48:14.920
for it, but then you look at, you know,

01:48:14.920 --> 01:48:18.560
there's recent RoboDog that you may have seen on Twitter,

01:48:18.560 --> 01:48:21.160
fully open source robot model

01:48:21.160 --> 01:48:23.600
for a Boston Dynamics style dog.

01:48:23.600 --> 01:48:26.960
It was trained on H100s, you know,

01:48:26.960 --> 01:48:31.200
10,000 human years of training and simulation

01:48:31.200 --> 01:48:34.400
and then some fine-tuning on real-world data

01:48:34.400 --> 01:48:39.000
and they have a very robust robot control model

01:48:39.000 --> 01:48:40.600
that you could plug into all kinds

01:48:40.600 --> 01:48:43.720
of different form factors and have something that can,

01:48:43.720 --> 01:48:45.840
you know, hop gaps and climb stairs

01:48:45.840 --> 01:48:49.640
and do all the things that Boston Dynamics robots

01:48:49.640 --> 01:48:52.040
don't do very well outside of their distribution.

01:48:52.040 --> 01:48:54.200
You think we'll have a general purpose algorithm

01:48:54.200 --> 01:48:58.280
that we can plug into basically arbitrarily shaped robots

01:48:58.280 --> 01:49:01.720
that can then navigate the, navigate our apartments

01:49:01.720 --> 01:49:05.040
or our construction sites or maybe our highways.

01:49:05.040 --> 01:49:08.200
That's an interesting vision.

01:49:08.200 --> 01:49:11.800
Why is it that we achieve this level of generality?

01:49:11.800 --> 01:49:14.920
If you look at humans, you know, humans are very good at,

01:49:14.920 --> 01:49:16.840
you know, if we've suffered an amputation

01:49:16.840 --> 01:49:18.680
or you have to go through physical therapy

01:49:18.680 --> 01:49:20.880
and it's not easy necessarily,

01:49:20.880 --> 01:49:25.280
but humans are able to adapt to different kinds

01:49:25.280 --> 01:49:29.000
of physical layouts of our body.

01:49:29.000 --> 01:49:32.160
And I think there will be a trend

01:49:32.160 --> 01:49:37.000
towards unified robotic control models

01:49:37.000 --> 01:49:40.520
that aren't like super tailored to, you know,

01:49:40.520 --> 01:49:44.040
two legs and two arms and so on and so forth.

01:49:44.040 --> 01:49:44.960
You know, once you've installed it

01:49:44.960 --> 01:49:46.320
through a little bit of in-context learning

01:49:46.320 --> 01:49:49.080
or fine-tuning or reinforcement learning,

01:49:49.080 --> 01:49:51.600
adapt to that particular form factor.

01:49:51.600 --> 01:49:53.640
And this will parallel the kind

01:49:53.640 --> 01:49:56.640
of pre-trained foundation model paradigm

01:49:56.640 --> 01:49:59.120
that is currently taking place in LLMs

01:49:59.120 --> 01:50:02.280
where you have like the really big foundation model

01:50:02.280 --> 01:50:04.680
that can sort of do everything reasonably well

01:50:04.680 --> 01:50:06.320
and then you can fine-tune it beyond that.

01:50:06.320 --> 01:50:09.280
If we get to the 2040s in your timeline,

01:50:09.280 --> 01:50:13.320
you talk about massive amounts of compute being available.

01:50:13.320 --> 01:50:16.800
You talk about post-scarcity in everything

01:50:16.800 --> 01:50:20.040
except for land and capital.

01:50:20.040 --> 01:50:22.040
And then you also talk about the development

01:50:22.040 --> 01:50:25.520
potentially of superintelligence at that point.

01:50:25.520 --> 01:50:27.120
What happens there?

01:50:27.120 --> 01:50:30.720
Who is in control of the superintelligence, if anyone?

01:50:30.720 --> 01:50:32.200
Yeah, this is sort of where I start

01:50:32.200 --> 01:50:33.680
to get a little bit tongue-in-cheek,

01:50:33.680 --> 01:50:40.680
but first of all, I talk about how I tend to think

01:50:40.680 --> 01:50:45.760
that once we have exascale computing and I think DOE

01:50:45.760 --> 01:50:47.960
just built their first exascale computer,

01:50:47.960 --> 01:50:49.400
and maybe it was private company,

01:50:49.400 --> 01:50:52.440
but we have like one exascale computer in the world.

01:50:52.440 --> 01:50:55.840
By the 2040s, they'll be commonplace.

01:50:55.840 --> 01:50:58.840
And if we are ever worried about sort of controlling

01:50:58.840 --> 01:51:03.480
the supply of GPUs, I don't know exactly

01:51:03.560 --> 01:51:05.760
how much compute will be on our smartphones,

01:51:05.760 --> 01:51:10.920
but it will definitely be possible to train a GP5 model

01:51:10.920 --> 01:51:12.600
from your home computer.

01:51:12.600 --> 01:51:15.920
And so any kind of AICT regime that we build today

01:51:15.920 --> 01:51:19.120
that doesn't take into account that falling costs of compute

01:51:19.120 --> 01:51:20.800
will probably break down.

01:51:20.800 --> 01:51:25.320
And therefore, amid this broader sort of fragmentation

01:51:25.320 --> 01:51:29.400
of the machinery of government, the state,

01:51:29.400 --> 01:51:31.680
I expect more and more government functions

01:51:31.720 --> 01:51:36.440
to be offloaded into basically private cities,

01:51:36.440 --> 01:51:38.520
HOAs, GATIC communities.

01:51:38.520 --> 01:51:41.160
And likewise with the internet, I expect more and more

01:51:41.160 --> 01:51:45.840
of our sort of permissioning regime for new AI models

01:51:45.840 --> 01:51:49.640
and deployment to shift to the infrastructure layer

01:51:49.640 --> 01:51:52.640
where telecommunication providers will be monitoring

01:51:52.640 --> 01:51:56.760
network traffic for unvetted AI models and so forth,

01:51:56.760 --> 01:51:58.640
and we'll have like Chinese style firewalls

01:51:58.880 --> 01:52:01.800
that are specific to a particular local area network.

01:52:02.880 --> 01:52:05.440
And at that point, the world looks,

01:52:05.440 --> 01:52:07.440
the United States where this takes place

01:52:07.440 --> 01:52:11.520
looks more like an archipelago of micro jurisdictions.

01:52:11.520 --> 01:52:15.720
I tend to think that like a post scarcity

01:52:15.720 --> 01:52:19.480
political economy looks a lot like the Gulf States,

01:52:19.480 --> 01:52:20.520
Gulf State monarchies, right?

01:52:20.520 --> 01:52:22.600
Because Gulf State monarchies are basically

01:52:22.600 --> 01:52:23.760
living post scarcity, right?

01:52:23.760 --> 01:52:26.240
They have a spigot of oil they can turn on,

01:52:26.480 --> 01:52:29.200
and then they can go build mega projects in the desert,

01:52:29.200 --> 01:52:30.640
and they have like infinite labor

01:52:30.640 --> 01:52:33.480
because they can just import guest workers.

01:52:33.480 --> 01:52:35.800
And so you end up with like this,

01:52:35.800 --> 01:52:37.760
but if we can't have a Gulf State monarchy

01:52:37.760 --> 01:52:39.360
in the United States, instead we have a bunch

01:52:39.360 --> 01:52:44.160
of micro monarchies dotting the country.

01:52:44.160 --> 01:52:47.200
So I sort of jokingly say, you know,

01:52:48.080 --> 01:52:51.080
who's going to stop the free city of California

01:52:51.080 --> 01:52:54.600
that's like home to all the trillionaire ML engineers

01:52:54.600 --> 01:52:56.920
and tech founders from the decade prior

01:52:56.920 --> 01:53:01.800
from plugging in their humanity sized supercomputer

01:53:01.800 --> 01:53:03.800
into a fusion reactor and turning it on.

01:53:03.800 --> 01:53:06.800
Yeah, and this is really your kind of end point

01:53:06.800 --> 01:53:10.440
of the discussion or your main point

01:53:10.440 --> 01:53:12.760
of institutions being eroded,

01:53:12.760 --> 01:53:17.760
and then afterwards being unable to respond to strong AI.

01:53:18.600 --> 01:53:20.720
Yeah, and leading up to this,

01:53:20.720 --> 01:53:23.440
it sounds like a scary dystopian type of thing.

01:53:23.440 --> 01:53:25.560
It doesn't have to be, right?

01:53:26.600 --> 01:53:29.680
Uber is not dystopian, Airbnb is not dystopian,

01:53:29.680 --> 01:53:32.120
private airports in other countries are way better

01:53:32.120 --> 01:53:35.400
than the public airports in the United States.

01:53:35.400 --> 01:53:38.320
So privatization and the sort of techno feudalist paradigm

01:53:38.320 --> 01:53:40.320
doesn't have to be bad,

01:53:40.320 --> 01:53:43.440
but what it is is more adversarial, right?

01:53:43.440 --> 01:53:46.320
And you know, people have sometimes speculated,

01:53:46.320 --> 01:53:50.080
you know, did the crumbling of the Roman Empire

01:53:50.080 --> 01:53:53.280
was a kind of prerequisite to a renaissance, right?

01:53:53.280 --> 01:53:55.440
Because it allowed for these principalities

01:53:55.440 --> 01:53:59.480
to sort of compete and to get the Florentine,

01:53:59.480 --> 01:54:01.240
you know, creativity and so forth.

01:54:01.240 --> 01:54:03.880
I think, you know, the next couple of decades

01:54:03.880 --> 01:54:05.520
could similarly be a renaissance

01:54:05.520 --> 01:54:09.120
for science and technology and for understanding the world,

01:54:09.120 --> 01:54:11.880
but it's probably a renaissance

01:54:11.880 --> 01:54:14.560
because we'll be moving into a much more competitive

01:54:14.560 --> 01:54:17.200
adversarial world where, you know,

01:54:17.200 --> 01:54:21.440
these city-states and so forth will be hard to coordinate.

01:54:21.440 --> 01:54:23.880
And so to the extent that there are still these,

01:54:23.880 --> 01:54:28.800
like, meta-risks where we would value some large-scale,

01:54:28.800 --> 01:54:31.800
intra- and international coordination,

01:54:31.800 --> 01:54:33.320
like peace treaties and so forth,

01:54:33.320 --> 01:54:36.200
the disintegration of the United States

01:54:36.200 --> 01:54:38.400
where this revolution is occurring

01:54:38.400 --> 01:54:40.400
would be bad for that.

01:54:41.280 --> 01:54:45.160
You talk about or you hint at an alternative path.

01:54:45.160 --> 01:54:48.040
What we've been talking about your timeline here

01:54:48.040 --> 01:54:49.640
is the default path.

01:54:49.640 --> 01:54:52.760
You hint at a path where we have something

01:54:52.760 --> 01:54:55.520
you call constrained leviath.

01:54:55.520 --> 01:54:57.520
What is constrained leviath?

01:54:57.520 --> 01:54:58.760
It's the limited government, right?

01:54:58.760 --> 01:55:03.080
So this is a D'Arnaz and Mogul's word

01:55:03.080 --> 01:55:04.600
for it from the narrow corridor.

01:55:04.600 --> 01:55:09.600
And if you trace the rise of sort of what we associate

01:55:10.200 --> 01:55:11.480
with liberal democracy,

01:55:11.480 --> 01:55:15.560
it is part of a particular technological equilibrium,

01:55:15.560 --> 01:55:17.000
in particular an equilibrium

01:55:17.000 --> 01:55:20.160
that favored centralized governments

01:55:20.160 --> 01:55:22.200
with impersonal rule of law

01:55:22.200 --> 01:55:25.520
and impersonal tax administration and so on and so forth.

01:55:25.520 --> 01:55:27.400
So we associate today with libertarians

01:55:27.400 --> 01:55:28.720
with like being anti-government,

01:55:28.720 --> 01:55:31.080
but the basic idea of liberalism

01:55:31.080 --> 01:55:34.240
is actually associated with strong government,

01:55:34.240 --> 01:55:35.840
a strong impersonal government

01:55:35.840 --> 01:55:37.520
that can impose the rule of law.

01:55:37.520 --> 01:55:40.480
And so if we want to maintain that kind of equilibrium

01:55:41.560 --> 01:55:44.760
in a world where AI is diffusing on the society level

01:55:44.800 --> 01:55:48.120
faster than it is on the state and elite level,

01:55:49.080 --> 01:55:51.240
then we want to accelerate the diffusion

01:55:51.240 --> 01:55:52.880
of AI within government.

01:55:52.880 --> 01:55:55.200
And there's obviously lots of low hanging fruit.

01:55:55.200 --> 01:55:56.760
We talked about how bureaucracies

01:55:56.760 --> 01:55:58.800
are basically fleshy APIs.

01:55:58.800 --> 01:56:02.840
Even today, I have a friend at the FTC,

01:56:02.840 --> 01:56:04.560
the Federal Trade Commission.

01:56:04.560 --> 01:56:05.760
They have like a 30 person team

01:56:05.760 --> 01:56:09.080
that is part of the healthcare division

01:56:09.080 --> 01:56:11.480
and they're in charge of policing

01:56:11.480 --> 01:56:13.440
the entire pharmaceutical industry

01:56:13.440 --> 01:56:16.320
in the United States for competition.

01:56:16.320 --> 01:56:18.800
His day job right now looks like manually

01:56:18.800 --> 01:56:20.160
reading through 40,000 emails

01:56:20.160 --> 01:56:23.440
that they subpoenaed from a pharmaceutical CEO, right?

01:56:23.440 --> 01:56:25.680
And today you could take those emails

01:56:25.680 --> 01:56:28.480
and put them into a Claude II or something

01:56:28.480 --> 01:56:31.080
like it with a big context window and ask,

01:56:31.080 --> 01:56:34.880
find me the five most egregious examples of misconduct.

01:56:34.880 --> 01:56:36.360
And it would do that.

01:56:36.360 --> 01:56:37.120
It might not be perfect,

01:56:37.120 --> 01:56:39.360
but it's a hell of a lot more efficient

01:56:39.360 --> 01:56:41.000
than reading through them manually.

01:56:41.000 --> 01:56:43.000
And obviously big law is going to be doing that.

01:56:43.000 --> 01:56:46.440
And the Pharma CEO and his personal attorneys

01:56:46.440 --> 01:56:48.480
will be doing that conversely.

01:56:48.480 --> 01:56:53.160
To maintain our state capacity in the face of AI

01:56:53.160 --> 01:56:56.320
is to run in this arms race.

01:56:56.320 --> 01:57:00.480
And you can kind of liken it to an evolutionary biology

01:57:00.480 --> 01:57:02.440
they call the Rig Queen dynamic,

01:57:02.440 --> 01:57:04.240
which comes from Alice in Wonderland

01:57:04.240 --> 01:57:05.600
where the Rig Queen tells Alice

01:57:05.600 --> 01:57:07.920
that sometimes you need to run just to stay in place.

01:57:07.920 --> 01:57:10.240
And so I think our government needs to be adopting

01:57:10.240 --> 01:57:12.120
this technology as rapidly as possible

01:57:12.120 --> 01:57:14.680
so that they can basically tread water.

01:57:14.680 --> 01:57:19.200
And that means both diffusing it in existing institutions,

01:57:19.200 --> 01:57:22.600
but also being open to radical reconfigurations

01:57:22.600 --> 01:57:24.760
of the machinery of government

01:57:24.760 --> 01:57:27.880
and addressing some of those firmware level constraints

01:57:27.880 --> 01:57:28.720
that we talked about,

01:57:28.720 --> 01:57:31.680
whether it's the lack of a national identification system

01:57:31.680 --> 01:57:36.200
or the outdated atmoded information technology infrastructure

01:57:36.200 --> 01:57:39.920
or the accumulation of old procedural

01:57:39.920 --> 01:57:42.120
kinds of methods of governance.

01:57:42.120 --> 01:57:46.080
A focused way of doing this is what you've called for

01:57:46.080 --> 01:57:47.880
in a political article,

01:57:47.880 --> 01:57:51.400
which is a Manhattan project for AI safety.

01:57:51.400 --> 01:57:52.720
A first question here,

01:57:52.720 --> 01:57:55.640
would it be better to call it an Apollo project

01:57:55.640 --> 01:57:57.600
as opposed to a Manhattan project?

01:57:57.600 --> 01:57:59.000
I mean, the Manhattan project

01:57:59.000 --> 01:58:01.160
created some pretty dangerous weapons,

01:58:01.160 --> 01:58:04.320
whereas the Apollo project might have been more benign.

01:58:04.320 --> 01:58:05.800
I mean, what the Apollo project

01:58:05.800 --> 01:58:07.160
and the Manhattan project have in common

01:58:07.160 --> 01:58:09.000
is that they came from an era of US government

01:58:09.040 --> 01:58:10.720
where we still dealt things,

01:58:10.720 --> 01:58:12.720
where we still had competent state capacity,

01:58:12.720 --> 01:58:14.920
where we still had a lot of in-house expertise

01:58:14.920 --> 01:58:17.840
and we weren't saddled with all these constraints.

01:58:17.840 --> 01:58:21.840
So today, we couldn't go to the moon in 10 years,

01:58:21.840 --> 01:58:24.520
NASA couldn't, SpaceX can.

01:58:24.520 --> 01:58:28.120
And so our modern Apollo projects

01:58:28.120 --> 01:58:29.560
are being done by the private sector

01:58:29.560 --> 01:58:31.880
through competitive contracts.

01:58:31.880 --> 01:58:34.080
And so one of the messages of my piece

01:58:34.080 --> 01:58:36.120
on the Manhattan project is to say,

01:58:36.120 --> 01:58:37.400
the reason I make this analogy

01:58:37.400 --> 01:58:41.640
is not just because AI is a Oppenheimer like technology,

01:58:41.640 --> 01:58:43.600
but also because responding to it

01:58:43.600 --> 01:58:47.520
will require a throwback to those kind of institutional forms

01:58:47.520 --> 01:58:51.480
where we gave the people at the top a lot of discretion

01:58:51.480 --> 01:58:53.440
and sort of gave them an outcome

01:58:53.440 --> 01:58:55.160
and let them solve for that outcome

01:58:55.160 --> 01:58:57.720
without having much of prescriptive rules

01:58:57.720 --> 01:58:59.720
about how to solve for that outcome.

01:58:59.720 --> 01:59:02.920
And then the second reason to make the analogy is,

01:59:02.920 --> 01:59:04.320
open AI and anthropic,

01:59:05.320 --> 01:59:07.800
they both have contingency plans

01:59:09.080 --> 01:59:14.080
for developing AGI and having like a runaway market power, right?

01:59:15.520 --> 01:59:16.520
And in the case of open AI,

01:59:16.520 --> 01:59:18.880
it's their nonprofit structure.

01:59:18.880 --> 01:59:21.920
In the case of anthropic, it's their public benefit trust

01:59:21.920 --> 01:59:23.240
where they both are envisioning a world

01:59:23.240 --> 01:59:25.000
where they could potentially be the first to build AGI

01:59:25.000 --> 01:59:26.280
and become basically trillionaires.

01:59:26.280 --> 01:59:27.520
And so at that point,

01:59:27.520 --> 01:59:31.280
they need to become basically governed by a nonprofit board.

01:59:32.280 --> 01:59:33.840
You know, at that point,

01:59:34.840 --> 01:59:36.600
and that's not where progress ends, obviously,

01:59:36.600 --> 01:59:38.720
like there's going to be continued research.

01:59:38.720 --> 01:59:41.760
It would make sense for the US government to step in

01:59:41.760 --> 01:59:44.280
and say, let's do this as a joint venture,

01:59:44.280 --> 01:59:45.720
or we're no longer competing.

01:59:45.720 --> 01:59:48.640
In fact, the basic structures of capitalism

01:59:48.640 --> 01:59:51.320
and market competition are starting to break down.

01:59:51.320 --> 01:59:53.800
Let's just pull this together into a joint venture,

01:59:53.800 --> 01:59:58.280
study the things that require huge amounts of capital

01:59:58.280 --> 02:00:00.920
that the private sector doesn't have, but the government can.

02:00:01.520 --> 02:00:03.560
The US government spent $26 billion

02:00:03.560 --> 02:00:05.400
on the Manhattan Project in today's dollars.

02:00:05.400 --> 02:00:08.400
When you think about the financial resources

02:00:08.400 --> 02:00:11.360
of nation-state actors to put behind scaling,

02:00:11.360 --> 02:00:14.320
it's nothing like what Microsoft or Google have.

02:00:14.320 --> 02:00:19.120
What's our first $200 billion training run, right?

02:00:19.120 --> 02:00:21.000
What kind of things can come out of that?

02:00:21.000 --> 02:00:23.120
I think that's something that you want to do

02:00:23.120 --> 02:00:25.560
with the Defense Department's involvement

02:00:25.560 --> 02:00:28.360
and working with these companies in a joint way

02:00:28.360 --> 02:00:30.760
through secured data centers

02:00:30.760 --> 02:00:33.520
and doing gain-of-function-style research

02:00:33.520 --> 02:00:35.240
that really is dangerous

02:00:36.480 --> 02:00:41.040
and more Manhattan Project than Apollo Project.

02:00:41.040 --> 02:00:42.800
What would be the advantages here?

02:00:42.800 --> 02:00:47.360
We would be able to slow down capabilities research

02:00:47.360 --> 02:00:49.840
and spend more of the resources

02:00:49.840 --> 02:00:52.120
on, say, mechanistic interpretability

02:00:52.120 --> 02:00:57.120
or evaluations or alignment in general,

02:00:57.480 --> 02:01:00.480
because now the top AI corporations

02:01:00.480 --> 02:01:02.560
have kind of combined their efforts

02:01:02.560 --> 02:01:04.880
on the one government group.

02:01:04.880 --> 02:01:07.640
Yeah, and in my vision,

02:01:07.640 --> 02:01:12.160
they're still allowed to pursue their commercial verticals.

02:01:12.160 --> 02:01:14.680
And I have an extended version of the proposal

02:01:14.680 --> 02:01:19.720
where I talk about needing sort of bio-safety-style categories

02:01:19.720 --> 02:01:23.400
for high-risk, medium-risk, and low-risk styles of AI

02:01:23.400 --> 02:01:25.000
that very closely parallels

02:01:25.000 --> 02:01:27.360
what Anthropic recently put out with their recommendations

02:01:27.360 --> 02:01:31.120
for sort of a BSL categorization of AI research.

02:01:31.120 --> 02:01:33.720
So I'm really talking about that BSL4 lab

02:01:33.720 --> 02:01:35.640
and beyond-style stuff.

02:01:35.640 --> 02:01:37.200
And some of that stuff,

02:01:37.200 --> 02:01:40.720
some of it will be to accelerate alignment

02:01:40.720 --> 02:01:42.040
and interpretability research

02:01:42.040 --> 02:01:46.040
to sort of do versions of the OpenAI Superalignment Project

02:01:46.040 --> 02:01:48.400
where they're dedicating 20% of their compute

02:01:48.400 --> 02:01:49.960
to study alignment.

02:01:49.960 --> 02:01:53.000
Another part of it will be to forestall

02:01:53.000 --> 02:01:55.480
competitive race-to-the-bottom dynamics

02:01:55.480 --> 02:02:00.480
so that they can coordinate and not violate antitrust laws.

02:02:01.200 --> 02:02:04.240
And then the third thing is sort of the gain-of-function stuff

02:02:04.240 --> 02:02:06.080
that we really only want to be doing

02:02:06.080 --> 02:02:10.720
with very strict oversight, compartmentalization,

02:02:10.720 --> 02:02:14.120
kind of pooling of talent and resources

02:02:14.120 --> 02:02:18.120
so we can share knowledge on alignment and safety.

02:02:19.000 --> 02:02:22.560
But then also because government has this huge spending power,

02:02:23.000 --> 02:02:24.520
relative to the product sector,

02:02:24.520 --> 02:02:26.360
anytime you build a supercomputer,

02:02:26.360 --> 02:02:29.000
you're basically borrowing from the future.

02:02:29.000 --> 02:02:31.400
You're trying to see what like the smartphones

02:02:31.400 --> 02:02:34.200
20 years from now will be capable of.

02:02:34.200 --> 02:02:38.000
And so if we want to sort of get ahead of the curve

02:02:38.000 --> 02:02:39.720
and see where scaling is leading,

02:02:39.720 --> 02:02:42.080
then I think governments are really the only actor

02:02:42.080 --> 02:02:43.840
that can waste a bunch of money

02:02:43.840 --> 02:02:45.920
basically scaling up a system

02:02:45.920 --> 02:02:48.280
and seeing what comes out of it.

02:02:48.280 --> 02:02:51.680
Yeah, when we talk about gain-of-function research in AI,

02:02:51.680 --> 02:02:55.080
it's an analogy to the gain-of-function research

02:02:55.080 --> 02:02:59.000
that's done on viruses in biolabs, but done for AI models.

02:02:59.000 --> 02:03:01.840
And this could be experimenting

02:03:01.840 --> 02:03:04.520
with creating more agent-like models

02:03:04.520 --> 02:03:08.200
or inducing deception in a model

02:03:08.200 --> 02:03:10.400
and planting it in a simulated environment,

02:03:10.400 --> 02:03:15.400
seeing what it does or enticing it to acquire more resources.

02:03:15.840 --> 02:03:19.760
But again, perhaps in a safely,

02:03:19.760 --> 02:03:21.680
if this is even possible

02:03:21.680 --> 02:03:24.960
in a safely constrained, simulated environment.

02:03:24.960 --> 02:03:27.440
And this is the type of research that we could do

02:03:27.440 --> 02:03:30.560
in this Manhattan project, this government lab,

02:03:30.560 --> 02:03:33.400
because we would have excellent cybersecurity

02:03:33.400 --> 02:03:36.520
and secure data centers and the combined efforts

02:03:36.520 --> 02:03:40.840
of the most capable people in AI research.

02:03:40.840 --> 02:03:43.320
If you've watched Oppenheimer, the movie,

02:03:43.320 --> 02:03:46.040
a lot of that revolved around suspicions

02:03:46.040 --> 02:03:48.640
of coming to spies and so on.

02:03:48.640 --> 02:03:51.400
And we really don't have great insight

02:03:51.400 --> 02:03:55.960
into the operational security of the major AGI labs.

02:03:55.960 --> 02:03:58.520
And that's something that bringing it in house

02:03:58.520 --> 02:04:00.920
of the defense department,

02:04:00.920 --> 02:04:04.320
they would necessarily have to disclose

02:04:04.320 --> 02:04:06.560
everything they're doing, but also hopefully

02:04:06.560 --> 02:04:08.600
beef up their operational security.

02:04:08.600 --> 02:04:12.160
Yeah, they're kind of stuck with a startup mindset,

02:04:12.160 --> 02:04:15.440
but they're not developing a startup product.

02:04:15.440 --> 02:04:18.000
They're developing something that, in my opinion,

02:04:18.120 --> 02:04:20.960
could be more dangerous than the average startup.

02:04:20.960 --> 02:04:22.920
Yeah, and Dari Amade has said as much

02:04:22.920 --> 02:04:25.720
that we should just assume that there are Chinese spies

02:04:25.720 --> 02:04:28.000
at all the major AI companies

02:04:28.000 --> 02:04:29.360
and at Microsoft and Google.

02:04:29.360 --> 02:04:32.720
When we think about gain of function research in AI,

02:04:32.720 --> 02:04:36.200
how do you think about the value of gaining information

02:04:36.200 --> 02:04:37.880
about what the models can do

02:04:37.880 --> 02:04:42.320
and what the models can do versus the risk we're running?

02:04:42.320 --> 02:04:47.320
It would be a tragic and ironic death for humanity

02:04:47.520 --> 02:04:50.920
if we experimented with dangerous AI models

02:04:50.920 --> 02:04:53.400
to see whether they would destroy us

02:04:53.400 --> 02:04:55.560
and then we hadn't constrained them properly

02:04:55.560 --> 02:04:57.760
and they actually destroyed us.

02:04:57.760 --> 02:04:59.960
So how do you think of that trade-off

02:04:59.960 --> 02:05:04.960
between gaining information and avoiding lab leaks?

02:05:05.000 --> 02:05:08.720
Yeah, hopefully lab leaks are less likely

02:05:08.720 --> 02:05:12.120
than in the biology context where, you know,

02:05:12.120 --> 02:05:15.960
getting a little bit of blood or urine on your shoes

02:05:15.960 --> 02:05:17.440
as you walk at the door.

02:05:17.440 --> 02:05:20.120
Now, it's a difficult thing to talk about in part

02:05:20.120 --> 02:05:22.520
because we just went through a pandemic

02:05:22.520 --> 02:05:26.880
that very probably was caused by a BSL4 lab leak.

02:05:26.880 --> 02:05:32.040
And so, you know, one saving grace is that AI models

02:05:32.040 --> 02:05:33.960
don't get caught in your respiratory system.

02:05:36.520 --> 02:05:39.000
And so hopefully there's forms of compartmentalization

02:05:39.000 --> 02:05:42.360
that are much more robust than in the biology context.

02:05:42.360 --> 02:05:45.200
And to the extent that this research

02:05:45.240 --> 02:05:46.880
is going to be done anyway,

02:05:46.880 --> 02:05:49.640
you know, it would be much better to move it off-site

02:05:49.640 --> 02:05:52.720
and hopefully in a way that facilities are air-gapped

02:05:52.720 --> 02:05:54.680
and so forth, rather than, you know,

02:05:54.680 --> 02:05:56.360
what Microsoft is doing right now,

02:05:56.360 --> 02:05:59.600
that Microsoft just recently announced their AutoGen AI,

02:05:59.600 --> 02:06:02.240
which are sort of agent-based models,

02:06:03.200 --> 02:06:07.040
very similar to like AutoGPT, but like that work.

02:06:07.040 --> 02:06:10.800
And they're doing this through Creative Commons,

02:06:10.800 --> 02:06:13.480
totally open source framework.

02:06:13.480 --> 02:06:16.280
All this capabilities work is gain a function research,

02:06:16.280 --> 02:06:18.600
where we draw the line between doing things

02:06:18.600 --> 02:06:20.080
that are intentionally dangerous

02:06:20.080 --> 02:06:21.400
or doing things that are dangerous,

02:06:21.400 --> 02:06:26.080
but we're kind of pretending that they're not, is hard.

02:06:26.080 --> 02:06:29.200
I do think there's, and Paul Cristiano is also agreed

02:06:29.200 --> 02:06:31.000
with this sort of threat models

02:06:31.000 --> 02:06:34.840
that would be valuable to be running in virtual machines

02:06:34.840 --> 02:06:38.600
and to see, you know, if the AI develops awareness,

02:06:38.600 --> 02:06:40.720
situational awareness and tries to escape,

02:06:40.800 --> 02:06:43.560
but it escapes into a simulated world that we built for it.

02:06:43.560 --> 02:06:47.720
Okay, let's end by talking about a recent critique

02:06:47.720 --> 02:06:52.720
of expecting AGI to arrive pretty in a short time.

02:06:53.600 --> 02:06:56.080
This revolves around interest rates.

02:06:56.080 --> 02:07:00.040
And I guess the basic argument is,

02:07:00.040 --> 02:07:03.640
or the basic question is, if AGI is imminent,

02:07:03.640 --> 02:07:06.800
why are real interest rates low?

02:07:06.800 --> 02:07:08.480
I can explain it, but you're the economist,

02:07:08.520 --> 02:07:11.000
so maybe you can explain the reason in here.

02:07:11.000 --> 02:07:14.680
So it's really a question of how efficient are markets

02:07:14.680 --> 02:07:16.880
and how much foresight do markets have.

02:07:17.720 --> 02:07:18.880
You know, we're coming out of a world

02:07:18.880 --> 02:07:20.760
of very low interest rates, of ultra low interest rates,

02:07:20.760 --> 02:07:22.480
near zero interest rates.

02:07:22.480 --> 02:07:24.760
And one way to think about that is there's a surplus

02:07:24.760 --> 02:07:26.720
of savings relative to investment.

02:07:26.720 --> 02:07:27.880
And so one of the reasons interest rates

02:07:27.880 --> 02:07:29.240
have been in secular decline

02:07:29.240 --> 02:07:31.920
is because populations are aging,

02:07:31.920 --> 02:07:35.640
and so all people have a huge amount of savings built up.

02:07:35.640 --> 02:07:36.880
And meanwhile, we're going through

02:07:36.880 --> 02:07:38.320
the sort of technological stagnation.

02:07:38.320 --> 02:07:40.880
So the amount of savings relative

02:07:40.880 --> 02:07:43.680
to the amount of profitable investments was out of whack,

02:07:43.680 --> 02:07:45.680
and so that pushes interest rates down.

02:07:45.680 --> 02:07:47.680
In a world where AGI takes off,

02:07:48.760 --> 02:07:52.240
it's a world where we have enormous investment opportunities,

02:07:52.240 --> 02:07:54.040
where we'll be building data centers left and right,

02:07:54.040 --> 02:07:55.280
and we can't do it fast enough,

02:07:55.280 --> 02:07:56.080
where there's new products,

02:07:56.080 --> 02:08:00.440
new commercial opportunities left and right.

02:08:00.440 --> 02:08:03.400
And so you would expect in that world

02:08:03.400 --> 02:08:05.880
where the singularity is near, so to speak,

02:08:05.880 --> 02:08:07.760
to be one where the markets begin forecasting

02:08:07.760 --> 02:08:09.360
rapidly rising interest rates

02:08:09.360 --> 02:08:11.280
because the savings to investment balance

02:08:11.280 --> 02:08:12.640
is starting to shift.

02:08:12.640 --> 02:08:15.840
And in addition, there's a long run stylized fact

02:08:15.840 --> 02:08:19.360
that real interest rates track growth rates.

02:08:19.360 --> 02:08:22.360
And so if GDP growth takes off,

02:08:22.360 --> 02:08:26.480
you'd also expect at least nominal rates to also take off.

02:08:26.480 --> 02:08:30.400
And so some have argued that looking at current interest

02:08:30.400 --> 02:08:32.720
rate data, like the five-year, 10-year,

02:08:32.720 --> 02:08:34.640
30-year treasury bonds,

02:08:34.640 --> 02:08:38.240
that the markets are not predicting AGI.

02:08:38.240 --> 02:08:40.800
You know, the two responses to that are,

02:08:40.800 --> 02:08:43.160
one, first of all, interest rates are up quite a bit.

02:08:43.160 --> 02:08:45.120
Nothing's mono-causal.

02:08:45.120 --> 02:08:47.360
There's lots of confounding factors.

02:08:47.360 --> 02:08:50.440
Is this, to some extent, the markets anticipating

02:08:51.480 --> 02:08:52.600
an investment boom?

02:08:52.600 --> 02:08:55.000
You know, maybe they're not anticipating full AGI,

02:08:55.000 --> 02:08:58.960
but they're seeing the way LLMs are going to impact

02:08:58.960 --> 02:09:02.200
enterprise and sort of picking some of that in.

02:09:02.200 --> 02:09:04.000
And then the second piece would be,

02:09:04.040 --> 02:09:07.960
okay, to the extent that they're not pricing in AGI,

02:09:07.960 --> 02:09:09.960
how much foresight do markets have anyway?

02:09:09.960 --> 02:09:12.720
Before we discuss market efficiency,

02:09:12.720 --> 02:09:16.280
I just want to just give a couple of intuitions here.

02:09:16.280 --> 02:09:21.280
If AGI was imminent and it was unaligned, say,

02:09:21.320 --> 02:09:24.440
and it would destroy the world in five years,

02:09:24.440 --> 02:09:27.520
well, then it doesn't make a lot of sense to save money.

02:09:27.520 --> 02:09:32.440
Similarly, if AGI is about to explode growth rates,

02:09:32.480 --> 02:09:35.760
well, then a lot of money will be available in the future.

02:09:35.760 --> 02:09:37.400
You're about to become very rich,

02:09:37.400 --> 02:09:39.880
so it doesn't make sense to save a lot now.

02:09:39.880 --> 02:09:43.240
And the pool of available savings

02:09:43.240 --> 02:09:45.840
determine what's available for lending,

02:09:45.840 --> 02:09:48.440
which determines interest rates.

02:09:48.440 --> 02:09:53.440
But let's discuss whether markets then are efficient

02:09:54.240 --> 02:09:57.320
on this issue or to what extent they're efficient.

02:09:57.320 --> 02:09:59.160
Right, so this is the efficient market hypothesis,

02:10:00.040 --> 02:10:03.080
which comes in strong and weak forms.

02:10:03.080 --> 02:10:05.480
So the strong form of the efficient market hypothesis

02:10:05.480 --> 02:10:08.720
would say that markets aggregate all of available information

02:10:08.720 --> 02:10:10.640
and are our best sort of point estimate

02:10:10.640 --> 02:10:12.000
of anything we care about.

02:10:12.920 --> 02:10:17.080
The weaker form, which I think is more defensible,

02:10:17.080 --> 02:10:19.080
is that markets can be wrong,

02:10:19.080 --> 02:10:22.240
but they can be wrong longer than you can be solvent, right?

02:10:22.240 --> 02:10:25.520
And so you can try to short a company that, like Herbalife,

02:10:25.520 --> 02:10:27.760
famously, there's a big short position on that,

02:10:27.760 --> 02:10:29.200
and because Herbalife sort of looks like

02:10:29.200 --> 02:10:31.800
it's a multi-level marketing Ponzi scheme,

02:10:31.800 --> 02:10:34.480
but yet the hedge fund that did that

02:10:34.480 --> 02:10:37.120
lost several billions of dollars before they

02:10:37.120 --> 02:10:39.160
ended their position because the markets

02:10:39.160 --> 02:10:42.000
stayed irrational longer than they could stay solvent.

02:10:42.000 --> 02:10:43.840
The second factor is the weaker versions

02:10:43.840 --> 02:10:45.160
of the efficient market hypothesis

02:10:45.160 --> 02:10:49.080
are sort of based on a no arbitrage condition, right?

02:10:49.080 --> 02:10:52.240
They say markets are efficient only insofar

02:10:52.240 --> 02:10:57.280
as you can arbitrage an inefficiency, right?

02:10:57.280 --> 02:11:00.920
And so you look at some prediction markets, for example,

02:11:00.920 --> 02:11:01.760
they predict it.

02:11:02.680 --> 02:11:07.440
They'll often have very clear inconsistencies

02:11:07.440 --> 02:11:11.400
across markets that look like they're irrational,

02:11:11.400 --> 02:11:13.360
but then you realize, oh, I can only make

02:11:13.360 --> 02:11:16.280
like $7,000 total on the website

02:11:16.280 --> 02:11:18.240
and there are transaction fees

02:11:18.240 --> 02:11:21.160
and there's work involved.

02:11:21.160 --> 02:11:24.400
And so if the market isn't very deep or liquid,

02:11:24.400 --> 02:11:26.080
there may be inefficiencies that exist

02:11:26.120 --> 02:11:27.880
not because the market's inefficient,

02:11:27.880 --> 02:11:31.000
but as efficient as it can be under the circumstances.

02:11:31.000 --> 02:11:35.680
And when it comes to AI, how do you arbitrage?

02:11:35.680 --> 02:11:38.800
I've been thinking for a while now that Shutterstock,

02:11:38.800 --> 02:11:41.040
their market cap should be collapsing, right?

02:11:41.040 --> 02:11:45.760
Because we have image generation that is proliferating.

02:11:45.760 --> 02:11:47.160
And yes, people will make the argument though,

02:11:47.160 --> 02:11:49.520
Shutterstock has all this image data

02:11:49.520 --> 02:11:51.840
that could build a better image model.

02:11:51.840 --> 02:11:53.880
Maybe it seems like it's cannibalizing their business

02:11:54.000 --> 02:11:56.720
or turning a moat into a commodity.

02:11:56.720 --> 02:11:59.360
And yet Shutterstock's market cap

02:11:59.360 --> 02:12:02.200
has basically held constant throughout

02:12:02.200 --> 02:12:07.200
this recent rebirth of image generation models.

02:12:07.440 --> 02:12:10.120
What if you borrow a lot of money cheaply

02:12:10.120 --> 02:12:13.640
and then put it into an index of semiconductor stocks

02:12:13.640 --> 02:12:16.040
or just IT companies in general,

02:12:16.040 --> 02:12:19.360
even just the general S&P 500 say,

02:12:19.360 --> 02:12:23.280
would that be a way of arbitraging this AGI forecast?

02:12:23.920 --> 02:12:26.120
Yeah, I would say if you have short timelines,

02:12:26.120 --> 02:12:29.440
you should be putting a lot of money into equities.

02:12:29.440 --> 02:12:32.440
This is not financial advice, I should say.

02:12:32.440 --> 02:12:35.720
Right, and I mentioned earlier that Paul Christiana

02:12:35.720 --> 02:12:37.920
has said in interviews that he's twice levered

02:12:37.920 --> 02:12:39.160
into the stock market.

02:12:39.160 --> 02:12:41.280
He basically owns a bunch of AI exposed companies

02:12:41.280 --> 02:12:46.280
and he's borrowed enough money to double his investments.

02:12:46.800 --> 02:12:49.080
So that's putting your money where your mouth is.

02:12:49.080 --> 02:12:51.720
When you look at market behavior

02:12:51.760 --> 02:12:53.400
over the long stretch of time,

02:12:53.400 --> 02:12:57.080
markets didn't anticipate the internet very well.

02:12:58.440 --> 02:13:00.440
There was a short run bubble

02:13:01.600 --> 02:13:05.680
that led to a boom and bust of .com stocks.

02:13:05.680 --> 02:13:07.520
But in terms of the real economy,

02:13:07.520 --> 02:13:08.720
the internet just kept chugging along

02:13:08.720 --> 02:13:09.560
and kept being built out

02:13:09.560 --> 02:13:12.240
and eventually a lot of those investments

02:13:12.240 --> 02:13:15.280
ended up paying off even if you rode through the bubble.

02:13:15.280 --> 02:13:16.720
Markets are made of people.

02:13:16.720 --> 02:13:18.720
Some of the biggest capital holders in the markets

02:13:18.720 --> 02:13:22.240
are institutional investors, pension funds,

02:13:22.240 --> 02:13:25.320
life insurance companies, governments,

02:13:25.320 --> 02:13:30.320
like the Saudi Arabia or the Norwegian pension fund.

02:13:30.920 --> 02:13:35.920
And often these are making safe bets.

02:13:36.200 --> 02:13:39.280
You know, they're not taking very heterodox views

02:13:39.280 --> 02:13:40.720
on markets.

02:13:41.920 --> 02:13:45.400
And so as a result, markets can be a little bit

02:13:45.400 --> 02:13:47.800
autoregressive, they're a little bit biased to the past,

02:13:47.800 --> 02:13:49.360
and past this prologue,

02:13:49.360 --> 02:13:52.840
and prone to kind of multiple equilibria,

02:13:52.840 --> 02:13:56.560
where there's two prices that the shutter stock can be.

02:13:56.560 --> 02:13:58.720
The shutter stock could be a $50 stock

02:13:58.720 --> 02:14:00.120
or it could be a $0 stock,

02:14:00.120 --> 02:14:02.160
and at some point the market will update

02:14:02.160 --> 02:14:04.360
and will wander go through like the great repricing

02:14:04.360 --> 02:14:06.360
and all these asset prices will flip

02:14:06.360 --> 02:14:07.680
in relatively short order.

02:14:07.680 --> 02:14:10.240
The efficient market hypothesis has to be false,

02:14:10.240 --> 02:14:12.760
or else we wouldn't have Silicon Valley.

02:14:12.760 --> 02:14:14.840
Right, we wouldn't have founders

02:14:14.840 --> 02:14:17.360
that we wouldn't have Elon Musk, right?

02:14:17.840 --> 02:14:19.840
So I would just say the markets are wrong.

02:14:19.840 --> 02:14:22.600
And partly they're wrong because to be right

02:14:22.600 --> 02:14:27.600
would require having a bunch of relatively bespoke

02:14:27.760 --> 02:14:30.880
and kind of esoteric priors about the direction

02:14:30.880 --> 02:14:34.200
of technology that are only now just sort of

02:14:34.200 --> 02:14:36.000
percolating into the mainstream.

02:14:36.000 --> 02:14:38.640
Yeah, and that the big kind of capital allocators

02:14:38.640 --> 02:14:41.440
can't really respond to because they're risk averse.

02:14:41.440 --> 02:14:42.640
Exactly.

02:14:42.640 --> 02:14:44.360
Now that doesn't mean like Renaissance technologies

02:14:44.360 --> 02:14:45.440
won't respond to it,

02:14:45.440 --> 02:14:47.080
but they're not gonna move the market.

02:14:47.120 --> 02:14:48.920
Samuel, thanks for this conversation

02:14:48.920 --> 02:14:50.480
and I've learned a lot.

02:14:50.480 --> 02:14:51.320
Thank you.

