WEBVTT

00:00.000 --> 00:03.000
Welcome to the future of Life Institute podcast.

00:03.000 --> 00:06.000
My name is Gus Docker and I'm here with Tom Davidson.

00:06.000 --> 00:14.000
Tom is a senior research analyst at Open Philanthropy where he works with potential risks from advanced AI.

00:14.000 --> 00:16.000
Tom, welcome to the podcast.

00:16.000 --> 00:17.000
I guess it's a pleasure to be here.

00:17.000 --> 00:25.000
So we're going to spend a lot of time on your model of takeoff speeds where you come to some pretty wild conclusions in my opinion.

00:25.000 --> 00:31.000
But first, I think it would be great to kind of situate your model in your broader views.

00:31.000 --> 00:39.000
So maybe you could tell us a bit about your view of AI progress and AI risks in general, in broad terms.

00:39.000 --> 00:46.000
So in broad terms, I think AI progress in the last 10 years has been extremely rapid.

00:46.000 --> 00:54.000
There's been massive progress in terms of analyzing images and videos, in terms of creating images and videos,

00:54.000 --> 01:02.000
game playing, natural language ability, coding, kind of very diverse broad domains, seeing very rapid progress,

01:02.000 --> 01:10.000
and all within a very similar paradigm, the deep learning paradigm, where progress has been fueled by training larger neural nets with more compute

01:10.000 --> 01:17.000
and by kind of improving the neural net algorithms, the deep learning algorithms we're using to train those things.

01:17.000 --> 01:21.000
And I think progress in the last four years has been especially rapid.

01:21.000 --> 01:24.000
So four years ago, GPT-2 was released.

01:24.000 --> 01:28.000
If you haven't played around with GPT-2, I really recommend you do so.

01:28.000 --> 01:35.000
It's a brilliant way to kind of give yourself an intuition for just how fast this field has been moving.

01:35.000 --> 01:43.000
GPT-2 is, by today's standards, a very limited language, conversational chatbot AI.

01:43.000 --> 01:49.000
It can maybe string together a few sentences, but once it's written a paragraph, it's very clear it doesn't know what it's talking about.

01:49.000 --> 01:50.000
It's off topic.

01:50.000 --> 01:53.000
It can't really understand questions you ask it.

01:53.000 --> 02:01.000
It's clearly very, very limited as an AI and has a very limited brittle understanding of the world.

02:01.000 --> 02:02.000
That was four years ago.

02:02.000 --> 02:04.000
This year, GPT-4 was released.

02:04.000 --> 02:08.000
GPT-4 was probably actually trained in 2022.

02:08.000 --> 02:14.000
So arguably three or four years being the gap between GPT-2 and GPT-4.

02:14.000 --> 02:19.000
And again, if you haven't played around with GPT-4, then I strongly recommend you do so.

02:19.000 --> 02:25.000
You'll need to pay some kind of subscription fee on the open AI chat GPT interface,

02:25.000 --> 02:27.000
and there's other ways you can play around with it.

02:27.000 --> 02:31.000
But in my opinion, if you're asking it probing questions and really testing its knowledge,

02:31.000 --> 02:34.000
it's quite a lot stronger than chat GPT-3.5.

02:34.000 --> 02:38.000
And my goodness, compared to GPT-2, it is really very good.

02:38.000 --> 02:44.000
It seems to have a pretty strong, pretty general understanding of many aspects of the world.

02:44.000 --> 02:48.000
It will apply its knowledge pretty flexibly if you try and throw kind of curveballs at it.

02:48.000 --> 02:51.000
It's very good at coding.

02:51.000 --> 02:54.000
You can give it a natural language description of some code.

02:54.000 --> 02:56.000
You want it to write the code.

02:56.000 --> 02:57.000
You can ask it to make some tweaks.

02:57.000 --> 02:59.000
It'll make just exactly those tweaks.

02:59.000 --> 03:05.000
I don't see why I need to kind of, I sometimes do like little Python kind of coding experiments for work,

03:05.000 --> 03:11.000
and I would be, you know, have a much better job having done those projects now using GPT-4.

03:11.000 --> 03:14.000
You know, in the last four years, we've gone from GPT-2 to GPT-4.

03:14.000 --> 03:17.000
I think that's just very startling.

03:17.000 --> 03:21.000
And so, yeah, progress has been rapid and it's been getting faster, I think.

03:22.000 --> 03:32.000
And I think that absolute, like kind of people deciding to be cautious and deciding to go slower for kind of non-financial, non-commercial reasons.

03:32.000 --> 03:34.000
I think the next four years will probably be similarly quick.

03:34.000 --> 03:41.000
And that we'll see a continued fast scaling up of the transformer architecture that is behind the GPT models.

03:41.000 --> 03:50.000
And I expect that by default, that kind of same jump that we saw from GPT-2 to GPT-4 will get a similar sized jump in the next four years.

03:50.000 --> 03:58.000
And it's maybe it'll take slightly longer because scaling does become more expensive as you start to, you know, really be spending, you know, more than a billion on these training runs.

03:58.000 --> 03:59.000
This is the progress.

03:59.000 --> 04:03.000
And as I understand what you're saying, AI is moving incredibly quickly.

04:03.000 --> 04:04.000
What about the risks?

04:04.000 --> 04:07.000
How do you see the risk landscape resulting from this quick progress?

04:07.000 --> 04:11.000
So there are already risks that AI is posing.

04:11.000 --> 04:14.000
There's risks of disinformation.

04:14.000 --> 04:23.000
There's risks of embedding biases in society that are kind of inherent in the data that the natural language models are trained on.

04:23.000 --> 04:33.000
And as AI gets more capable, I expect the kind of severity of those risks to increase in language capabilities.

04:33.000 --> 04:37.000
You know, it's very hard to speak definitively about exactly what risk will arise when.

04:37.000 --> 04:45.000
Because one of the things about language models is that it's hard to predict what emergent capabilities there will be with the next model.

04:45.000 --> 05:02.000
So, you know, these models are pre-trained by kind of just devouring, you know, internet text, just basically reading all of the text that people can easily scrape off the internet and trying to predict what the next world will be on a given web page that they're presented with.

05:02.000 --> 05:16.000
And then it turns out that, you know, that pre-training, just kind of reading things on the internet, gives the language models various kind of emergent capabilities, which are not in any very obvious direction present in the training set.

05:16.000 --> 05:25.000
But I kind of can be can be kind of elicited from the models with a little bit of tweaking after that initial phase is finished.

05:25.000 --> 05:40.000
So, you know, for example, I believe that some people are concerned that the next generation of large language models that might be GPT-5 might make it significantly easier for bad actors to create dangerous bio-weapons.

05:40.000 --> 05:59.000
And presumably that's because there's enough kind of biology-related text on the internet that when it's during that pre-training phase, GPT-5 would be picking up enough biology and also just enough kind of common sense reasoning and scientific understanding in general that it could then provide substantial help to someone who is wanting to make a bio-weapon of that kind.

05:59.000 --> 06:02.000
But it's very hard to predict whether that will actually happen.

06:02.000 --> 06:10.000
It's hard to know exactly what is in the training data and exactly what the language models will be able to get out of that training data when the whole thing is over and done with.

06:10.000 --> 06:17.000
But, you know, I think that is one particular risk that I think there's a decent chance of arising in the next four years.

06:17.000 --> 06:21.000
There's kind of lowering the bar to bioterrorism risk.

06:21.000 --> 06:30.000
There's some chance of a risk that has been has been called kind of autonomous replication and the adaption.

06:30.000 --> 06:40.000
That is that some maybe GPT-5 level systems or GPT-6 level systems would be capable with the right kind of scaffolding to help them along.

06:40.000 --> 06:51.000
There's something like auto-GPT that kind of prompts the system to kind of plan out its actions and make a list of sub-goals and then pursue them one by one.

06:51.000 --> 07:05.000
I think there's a chance that a system of that kind would be capable of kind of copying itself onto a new computer and then using that computer's compute resources to make money, for example, by scamming people

07:05.000 --> 07:12.000
by just kind of doing intellectual work on the internet that humans can get paid for, like doing kind of mTurkish style roles.

07:12.000 --> 07:24.000
And so there's a chance we make this threshold where AI is kind of able to self-sustain and kind of gather the resources it needs to kind of make more copies of itself and increase its power and resources.

07:24.000 --> 07:33.000
And these two risks you mentioned are very near-term, so we're thinking here before 2027 or before 2028?

07:33.000 --> 07:48.000
Yeah, I think there's a chance for sure. Like I said, it's really hard to predict and especially with this autonomous replication and an adaption threshold, my own view is probably more likely than not that that is not possible by 2027, 2028,

07:48.000 --> 07:59.000
but I'd give it substantial probability, maybe 30%, and then probably higher on the bias, whether I'm really kind of making these numbers up, these are just my very loose impressions.

07:59.000 --> 08:08.000
And I don't know, I'm not aware of a very grounded science for predicting these kind of risks. And there are other risks as well. These are not the only ones.

08:08.000 --> 08:19.000
Maybe there's risks from persuasion, propaganda, maybe recruitment for bad actors they can use, language models to automate the process of kind of reaching out and trying to find vulnerable people.

08:19.000 --> 08:28.000
Maybe there are other risks as well, we'll see in the next five years, maybe relating to cyber, maybe relating to significantly improving tech progress in some domain.

08:28.000 --> 08:38.000
Yeah, and I think what you mentioned is that you can't really predict which capabilities will arise. And I think one of the problems here is that nobody can really predict which capabilities will arise.

08:38.000 --> 08:49.000
And this makes the whole area very uncertain. If you couple that with the fact that, as you mentioned, the area is moving very fast, you get some potential risks going on.

08:49.000 --> 09:01.000
Yeah, exactly. And I think one thing that can make it harder to manage these risks is that the default way that we regulate risky technology is a kind of reactive way.

09:01.000 --> 09:13.000
So we allow people to develop the technology, we kind of allow them to deploy it. And then when something goes wrong, we say, okay, that thing went wrong in that particular circumstance, we're going to regulate the use of this technology in this circumstance.

09:13.000 --> 09:19.000
So now, now you're not allowed to use AI to do political campaigns because we've seen it's been abused in that context.

09:19.000 --> 09:35.000
And what I think we probably need for something like AI when there's so many possible risks, and it's really hard to predict which ones, it's something a bit more proactive where we are before deploying it far and wide, testing it in advance, what kind of risks it may pose.

09:35.000 --> 09:49.000
Does this mean that we don't really have any good historical analogies for AI? With other technologies, it may be the case that it's taken decades for them to be deployed and we've been able to do trial and error and build up some sort of safety regime.

09:49.000 --> 09:55.000
But maybe AI is different, maybe it moves much faster than other technologies. Do you have good analogies in mind?

09:55.000 --> 10:07.000
I don't think there are any perfect analogies to be sure. I think your pointer, I think a good tension which makes it hard to find analogies and that AI is a general purpose technology.

10:07.000 --> 10:22.000
So in that sense, it's like harnessing power from fossil fuels or electricity, or maybe computing power. But on the other hand, unlike other general purpose technologies, the underlying technology is improving very, very rapidly.

10:22.000 --> 10:33.000
So with fossil fuels, I'm not aware of any four-year period where we saw this rate of improvement in the underlying quality of the combustion engine.

10:33.000 --> 10:41.000
I'm not aware of similarly with something like electricity, any four-year period where there was such rapid progress in the underlying technology.

10:41.000 --> 10:53.000
And I think there have probably been many, many narrow technologies like Facebook went viral in a small number of years, but it's a narrow domain and that did in fact pose regulatory problems.

10:53.000 --> 11:04.000
The government was either be too slow to respond to the various risks that Facebook did pose. But ultimately, it was a very scoped narrow technology in a narrow domain.

11:04.000 --> 11:12.000
With AI, I think there's a kind of a scary duality with its with its generality on the one hand, and then the kind of underlying pace of progress.

11:12.000 --> 11:17.000
On the other hand, making it especially difficult to manage and regulate as a new technology.

11:17.000 --> 11:29.000
Yeah. And if we look longer term, so beyond the 2030s, what do you think of the possibility of truly transformative AI? When would you expect something like that to arrive?

11:29.000 --> 11:34.000
What is a good way of defining whether AI is transformative?

11:34.000 --> 11:51.000
So one kind of broad new definition, which has been used historically is to say that AI would be transformative if it changed society as much as the industrial revolution change society or the agricultural revolution change society.

11:51.000 --> 12:01.000
And what I understand by that is it's completely changing the nature of work, going from hunter gathering to farming, kind of moving around constantly to being settled in one place, then moving into industry.

12:01.000 --> 12:07.000
And it's also really changing the way that society is structured and the political and economic processes that are appropriate.

12:07.000 --> 12:20.000
That's not a very precise definition, but it has the benefit of being kind of loose and flexible enough that if you're kind of trying to interpret it in the right way, then it's probably going to end up pointing at the thing that you care about.

12:20.000 --> 12:29.000
I think that's a pretty robust definition to use because it's vague. People have tried to kind of precise by the definition and then I think that there are some problems you run into when you do that.

12:29.000 --> 12:36.000
So one way to try and make it precise is to say it's truly transformative if it accelerates the pace of economic growth by say a factor of 10.

12:36.000 --> 12:43.000
That's more precise, but it does have the downside that whether economic growth gets fast, it doesn't just depend on the nature of AI itself.

12:44.000 --> 12:49.000
It also depends on how it's integrated into society and how humans choose to use it.

12:49.000 --> 12:54.000
Like we might just choose to grow slowly despite the possibility of growing much faster.

12:54.000 --> 12:57.000
No dependency depends on how do we even measure economic growth.

12:57.000 --> 13:02.000
There's this big kind of thorny questions about how you measure the growth impacts of new technologies.

13:03.000 --> 13:14.000
At that point, the definition of transformative AI is so tied to its impact rather than to the actual abilities of the technology itself that I think it can be confusing to think about it like that.

13:14.000 --> 13:28.000
An approach I often use is to use the term artificial general intelligence and just say that that is when AI can do any cognitive task that a human professional can do at or above that level.

13:28.000 --> 13:38.000
That's precise, fairly precise, and I prefer it to the kind of economics based definition because it's more about what the underlying technology can do.

13:38.000 --> 13:48.000
On the other hand, you can imagine kind of loopholes where it's not really capturing what you want, where there's like just a few tasks that no one's bothered trying to make AI able to do that AI can't do.

13:48.000 --> 13:50.000
So you say, oh, we don't technically have AI.

13:50.000 --> 13:59.000
And so I think probably sticking with the kind of broad definition is one having the background and then being a bit flexible about exactly how we're defining it.

13:59.000 --> 14:12.000
Thinking about the economic impact of AI is interesting because sometimes if you look at benchmarks, for example, TPT4 scores very well on high school exams and college exams and even the bar exam and so on.

14:12.000 --> 14:20.000
But how does that translate into economic growth or economic progress or automation? It's difficult to say.

14:20.000 --> 14:27.000
And of course, TPT4 hasn't, there hasn't been enough time yet for it to have a great impact, but so far it's not really showing up in the numbers.

14:27.000 --> 14:32.000
I think it's very important to think of the economic impact also and not just the benchmarks.

14:32.000 --> 14:45.000
Yeah, I think especially today's benchmarks are very limited. So what if AI can get this mark on an SAT and so what if it can get this gone big bent?

14:45.000 --> 15:01.000
The tasks that we're mostly focusing on with current benchmarks are not tasks that humans are performing in the real world, in the real economy that they're actually useful to producing goods and services to running organizations, to whatever it is that people are actually trying to do.

15:01.000 --> 15:12.000
With the current way we're benchmarking systems that there's this kind of gap between the tests that we're giving them and then the stuff that we actually ultimately care about in our society, which is kind of useful work.

15:12.000 --> 15:19.000
And it's really hard to know how big that gap is. And it seems like at the moment that gap is potentially pretty big.

15:20.000 --> 15:38.000
And that GP4 is getting really good grades on a very wide range of quite tough examinations, but it's not yet massively adopted to replace lots of people's jobs and to massively improve, increase profits and revenues for lots of companies.

15:38.000 --> 15:47.000
And so I think we should be trying to move towards better benchmarks, which are more closely tied to the actual real world impacts of the systems.

15:47.000 --> 16:01.000
Yeah, and maybe those benchmarks will be difficult to set up, but at least we have measurements of GDP growth as a proxy for useful work as one way of measuring whether AI is doing a lot of useful work for the economy.

16:01.000 --> 16:15.000
Yeah, that's right. It's really getting at that. Is it doing useful work? Part of the question. I do think it has some pretty big downsides in that there's going to be a pretty big lag, especially with earlier AI systems that are less flexible.

16:15.000 --> 16:26.000
So it's taking more work to integrate into workflows. So if you're just looking at GDP, you might think nothing really that much is happening in AI because GDP hasn't picked up and that would be a mistake.

16:26.000 --> 16:34.000
And there's also just a lot of noise in GDP statistics, just inherent noise and then all these other trends which are interlacing.

16:34.000 --> 16:51.000
And what one kind of quite nice intermediate is the size of the AI, the AI industry specifically. So you can look at investments in AI or you can try and kind of add up AI driven revenues across the economy, which I think is a pretty vexed task trying to figure out

16:51.000 --> 17:04.000
how much value right AI is really adding. Those kinds of measurements typically show pretty fast growth of the AI industry, kind of like 30% a year or faster in recent history. You can also look at things like growth of spending on AI chips.

17:04.000 --> 17:15.000
That's quite a kind of concrete thing you can measure clearly. That is maybe intermediate between economic growth on the one hand and benchmarks on the other hand and that it is showing, look, people really believe that this is going to have a real world impact.

17:15.000 --> 17:28.000
They're willing to spend concrete money developing these systems. That means you're getting a kind of real signal about its real world impact, but it hasn't actually had that impact yet. So that's intermediate.

17:28.000 --> 17:47.000
It's also interesting to think about the fact that the entire introduction of computers and the internet to the world over the last 50 years hasn't really increased the growth rate in developed economies a lot. So technologies can have an enormous real world impact without actually increasing GDP.

17:47.000 --> 17:53.000
And maybe there's quite a high bar actually for what we might call transformative AI.

17:53.000 --> 18:04.000
I think there's a very high bar. As you say, computers, you know, they did increase economic growth in the sense that if we hadn't developed computers, economic growth would have been lower.

18:04.000 --> 18:12.000
But they did not turbocharge the overall pace of economic growth. They're more kind of maintaining the trend that we were previously getting from other technologies.

18:12.000 --> 18:30.000
At first, I think that that's what will be happening with AI. And then my view is that once we've got truly very advanced systems, AGI systems that are able to really automate all human labor, that's when we should expect more transformative and unprecedented economic impacts.

18:30.000 --> 18:43.000
Yeah, what I want to do in this episode is to kind of dig into your model of how this might happen, which is, I think, centered around takeoff speeds. I think the notion of takeoff speeds is quite central to how you see AI progressing.

18:43.000 --> 18:50.000
So maybe we could start by talking about what is takeoff speed in the context of AI.

18:50.000 --> 18:54.000
So I think it can be useful to distinguish between two notions of takeoff speed.

18:54.000 --> 19:03.000
The first is what I'll call AI capabilities takeoff speed. So that's, that's focused on the pace of improving in the underlying technology.

19:03.000 --> 19:12.000
So capabilities takeoff speed would be the answer to the question of how quickly is AI improving around the time in which we get human level AI.

19:13.000 --> 19:27.000
So if takeoff speed is fast, then that can mean we go from kind of mouse level intelligence AI to human level AI in one year, and then a year later, we've got kind of godlike intelligence AI.

19:27.000 --> 19:34.000
So kind of very fast increase in AI capabilities just as it's passing through the kind of human level of intelligence.

19:34.000 --> 19:44.000
Then there's another notion of takeoff speed, which I think, especially if we're thinking about economic impacts, it can be useful to distinguish, which is impact takeoff speed.

19:44.000 --> 19:50.000
So that is how quickly does AI's impact in the world increase around that time.

19:50.000 --> 19:56.000
A very fast impact takeoff speed could look like growth is just taking away at its normal two or three percent a year.

19:56.000 --> 20:04.000
And then next year, suddenly it kind of shoots up what economies is doubling every, every two years with explosive growth.

20:04.000 --> 20:13.000
Whereas a more kind of slow impact takeoff speed could be, well, there's the impacts of AI is spread out over many decades.

20:13.000 --> 20:20.000
And, you know, maybe growth gradually gets faster over time, or maybe only temporarily gets faster than to set us back down.

20:20.000 --> 20:28.000
And so, you know, you could, you could imagine those two coming apart if there's loads of regulations, for example, that limit the impact of AI.

20:28.000 --> 20:38.000
You can imagine the takeoff speed of the underlying technology being very fast, kind of somewhat in line with recent, recent trends, but the actual impact takeoff speed being much slower.

20:38.000 --> 20:45.000
A lot of probably we'll talk later about some of the economic objections to kind of transformative growth and both bottlenecks.

20:45.000 --> 20:55.000
I know one, one kind of theme in my mind is that these things tend to affect impact takeoff speed more than they tend to affect the capabilities takeoff speed.

20:55.000 --> 21:02.000
So what do you think a world looks like in which you have a lot of AI capabilities, but not a lot of impact yet?

21:02.000 --> 21:06.000
Is that a stable situation? Because it seems to me pretty unstable.

21:06.000 --> 21:12.000
There would be a lot of incentives to try to deploy these very capable AI somewhere in the world.

21:12.000 --> 21:23.000
Yeah, I think that's right. It's probably temporary. The reason you can imagine it happening is that there are lots of entrenched interests in various professions.

21:23.000 --> 21:29.000
So, you know, lawyers don't want to lose their jobs. Medical professionals don't want to lose their jobs. There are unions.

21:29.000 --> 21:39.000
There's kind of political processes by which these groups kind of will power and influence, and they may want to delay the deployment of systems which would replace them and lose their jobs.

21:39.000 --> 21:52.000
Indeed, that will be a very good thing. There's regulations around who can make various high-stakes decisions, be it in signing something on the legal document or giving a drug to a patient.

21:52.000 --> 22:03.000
The bureaucracies take a while to shake up, and it's not going to happen overnight that suddenly AIs are allowed to, you know, diagnose you and hand you the medication, even if they're actually able to do that.

22:03.000 --> 22:15.000
And because these are slow human processes and bureaucracies, it does seem possible to me that even though there's a large amount of pressure to kind of remove these barriers to rolling out AI, that it could still take a while.

22:15.000 --> 22:29.000
So what you're imagining here is, for example, we have AI models capable of diagnosing a patient or sending a document to a court because of a professional organization in medicine or in law.

22:29.000 --> 22:42.000
Maybe it's just not legal to do so. Maybe you need a human to sign off, or maybe you need even a human to do the full task, and that slows down the implementation of AI in the economy.

22:42.000 --> 22:46.000
Even though AIs might be perfectly capable of diagnosing a patient.

22:46.000 --> 22:48.000
Yeah, even though they might be better.

22:48.000 --> 22:51.000
Do you think that's the default scenario? Has this happened before?

22:51.000 --> 23:05.000
I think that is the default scenario of previous technologies. They do take a while to refuse, and if you kind of have a naive view of like, well, once it's better than everyone move over, you'd be very surprised at what ended up happening in reality.

23:06.000 --> 23:20.000
Many organizations have still not transitioned over to the internet fully. I sometimes go into hospitals, and I'm asked to fill out forms by hand, and I'm thinking, why are we still doing paper documents here?

23:20.000 --> 23:26.000
These transitions can be so. Now, I actually think there's a chance that things are quite different with AI.

23:26.000 --> 23:51.000
So if the capabilities take a speed as fast, then we might rapidly transition to a world where we've got, you know, truly super intelligent systems that are not yet deployed that could very demonstrably add huge amounts of value at kind of almost no effort to integrate them into our businesses could add huge amounts of value because they're kind of smart enough to integrate themselves and immediately kind of like learn what they need to learn proactively and start adding value.

23:51.000 --> 24:07.000
At that point, I think the situation will be without precedent and that we've never, the previous technologies have required active effort to rearrange workflows, the kind of draft new legislation so that they can be incorporated into the real economy.

24:08.000 --> 24:22.000
It would be without precedent for a new technology to be able to do all of that work itself, draft the new legislation itself, lobby the regulators itself, you know, learn what it needs to learn to do an even better job delivering the goods and services itself.

24:22.000 --> 24:45.000
Create by itself, you know, legible examples of inventing and treating diseases which people currently struggle to treat. And maybe AI systems are so super intelligent that they can, without even going through the FDA process, develop a new drug, and then kind of demonstrate quite clearly to everyone that it works in treating cancer that no one else can treat.

24:45.000 --> 24:55.000
Then at that point, you know, legally, new drugs need to go through the FDA. But when there's something so stark as there's this drug which could save millions of lives, everyone knows it would work.

24:55.000 --> 25:14.000
That would create a kind of pressure on the system and the regulatory system to change that I think might be without precedent. And so I could, I could imagine that if, if AI capabilities continue to kind of shoot upwards, that would put increasing pressure on the kind of the regulatory barriers and other barriers to deploying AI widely.

25:15.000 --> 25:34.000
It's of course difficult to speculate on the political economy of future AI. But I think that there might also be demand from the public to get access to these AI models. If, for example, you have a demonstration that an AI doctor can diagnose you better than a human doctor.

25:34.000 --> 25:50.000
And maybe the AI doctor costs 10 times as little. Of course, there would be pressure coming from the doctor's association in a given country. But I can't see this demand not mattering at all. I think it would matter at least somewhat.

25:50.000 --> 26:09.000
I think that's right. In my mind, it's a question of how long. And then there's kind of the kind of incumbent forces trying to preserve the status quo. And then there's this maybe increasing tide of technological abilities that AI is able to provide an increasing pressure to kind of knock down those barriers.

26:09.000 --> 26:24.000
And then also kind of competitive dynamics potentially, you know, two different states have slightly different regulations and people will go to the, to the, to the state where they can get the kind of 10 times cheaper AI doctor who's more effective or go to a different country where they can receive that treatment.

26:24.000 --> 26:30.000
And that's, that's another thing which, which makes it hard for these incumbent forces to, you know, sustain for, for too long.

26:30.000 --> 26:41.000
Yeah, I think before we get into the mechanics of the model itself, it would be useful to know why you're interested in this topic. Why is it useful for us to know about AI takeoff speeds?

26:41.000 --> 27:04.000
One of the key risks that I've been focused on with AI is the risk of losing control of superhuman AI systems. That is systems which on some significant domains, maybe persuasion, maybe strategy, maybe technological development outperform best human experts.

27:04.000 --> 27:21.000
These risks are very poorly understood. We don't yet have a solution to what you will refer to as the alignment problem, which is a problem of ensuring that superhuman AI systems do what their users intent and their developers intent.

27:21.000 --> 27:37.000
What this means is that it would be really, really useful if we could have, you know, a long period of time, like ideally decades of time with AI systems, which are not quite yet capable enough to actually pose a risk that we lose control of them.

27:37.000 --> 27:50.000
But that are kind of maybe almost at that level, or that are very similar to those particularly risky systems in key ways so that we could study them. We could understand how do their motivation systems work.

27:50.000 --> 28:02.000
We could experiment with different ways to try and align them. I kind of train them such that they do what their users and developers want them to do.

28:03.000 --> 28:15.000
And we could learn about how big the risks are and the best ways of mitigating those risks. And so that would be really, really nice in terms of better understanding the problem and understanding what the solution requires.

28:15.000 --> 28:27.000
The problem is that if capabilities take off speed is fast, if the underlying technology goes from human level to kind of significantly superhuman in just a year, then we won't have very long.

28:27.000 --> 28:38.000
Yeah, by default, we won't have very long with those systems and we won't have long to study them. I think that makes the task of avoiding loss of control much more difficult.

28:38.000 --> 28:53.000
Because if we just have one year, then we'll be kind of flying by the seat of our pants trying to kind of understand how the systems work, how they think, throwing on some kind of very quick slapdash solutions in terms of trying to get them to do what we want.

28:53.000 --> 29:06.000
And then not really having time to take a step back and check that it's all working in just one year. Very little effort so far has gone into solving this problem of how do we retain control of superhuman systems.

29:06.000 --> 29:21.000
If we had a very long time with AIs who are roughly human level, maybe very slightly superhuman at the kind of research, but kind of maybe human level or less than human level at the kind of dangerous capabilities like manipulation and persuasion and strategizing.

29:21.000 --> 29:34.000
If we had many decades with systems of that kind, we could potentially use them to try and solve the problem of understanding the motivations of AI systems and solving this problem of how do we control superhuman AI systems.

29:34.000 --> 29:49.000
Once you train a system that's human level, I think it's likely you'll be able to run, and we can talk about this a bit later, but you'll be able to run many millions of copies in parallel at the same time, or even run kind of fewer copies but have them think faster.

29:49.000 --> 30:06.000
And so you could get a huge amount of labor from kind of highly capable AI. And the best time to do that is when, again, when you've got AI that is really pretty good and good enough to be very useful, but not yet superhuman enough that it's really posing a risk that you lose control of it.

30:06.000 --> 30:15.000
And so again, if we had a slow takeoff, we could have many years harnessing the labor of these roughly human level AI systems.

30:15.000 --> 30:27.000
And why is it that in a fast takeoff scenario, we can't harness their labor to help us align more advanced AI? Why is it more difficult to do so in a fast takeoff scenario?

30:27.000 --> 30:39.000
So in a fast takeoff, we can still do this to some extent. There will still be some period where we have roughly human level systems and we can use them to do research and to keeping AI's safe.

30:39.000 --> 30:56.000
But we just have less long in that period, and so they can do less research in total. And especially if we want humans to be able to check the results of their work, and we want humans to be able to verify their work, then that kind of only having 12 months can become quite a binding constraint.

30:57.000 --> 31:09.000
You know, even if we don't need humans to check, there's still this fact of you just got longer to do the research, that desire that I think we all have humans to verify the work could become quite problematic.

31:09.000 --> 31:20.000
All right. So if we begin digging into your model, I'm looking at a simplified diagram of it. There's also a website where you can plug in your own values for various parameters.

31:20.000 --> 31:27.000
So maybe we could go through the parameters of the model and talk about the relationships. Yeah, how would you summarize the model?

31:27.000 --> 31:51.000
It attempts to model the most important inputs to AI development. In particular, the amount of compute used to develop an AI model and the quality of the algorithms, the training algorithms that are used, that kind of utilize that compute to produce the trained AI.

31:52.000 --> 32:06.000
And then it really kind of drills into, okay, how are these two inputs currently evolving over time? And how might they evolve over time into the future? So, you know, how quickly will the algorithms be improving into the future?

32:06.000 --> 32:16.000
How quickly will the amount of compute used to develop AI systems increase into the future? And in particular, taking into account a couple of key feedback loops.

32:16.000 --> 32:26.000
So the first feedback loop is a kind of an investment feedback loop where we see that AI is producing value in the economy and we see from impressive demos that they're very capable.

32:26.000 --> 32:33.000
And that sparks increased financial investment, kind of getting more compute and improving algorithms.

32:33.000 --> 32:45.000
And then a second feedback loop, which are called the AI automation feedback loop, whereby as AI's get more capable, they're able to automate the work of coming up with better AI algorithms.

32:45.000 --> 32:52.000
And they're able to automate the work of coming up with better computer chips so that we have access to more compute.

32:52.000 --> 32:57.000
And so we've got these two feedback loops, the investment feedback loop and AI automation feedback loop.

32:57.000 --> 33:07.000
They are both affecting how the algorithms are improving and how the amount of compute available is improving. And then those two key inputs are then driving the improvement of AI capabilities over time.

33:07.000 --> 33:18.000
And so which of these feedback loops is the most important? So is the investment feedback loop or the AI automation feedback loop the most important for accelerating takeoff speeds?

33:18.000 --> 33:27.000
I think in the near term, the investment feedback loop is going to be more important. So I think already today we're seeing that feedback loop in action.

33:27.000 --> 33:40.000
Investment in AI has gone up massively in recent years. Investment in AI chip have gone up massively. Investment in designing better AI chips have gone up massively.

33:41.000 --> 33:58.000
In Vidya, its share price has gone through the roof. It specializes in AI chips like the H100. And so currently it's that investment feedback loop, which is continuing to drive the very fast progress that we've seen over the last four years and will probably continue to the next four years.

33:59.000 --> 34:14.000
But that investment feedback loop can only continue for so long because at a certain point companies are already spending maybe hundreds of billions of dollars, maybe even a trillion dollars if it becomes a nation state activity on developing a state of the AI system.

34:14.000 --> 34:33.000
And it's just very hard to spend more past a certain point. And you know, past a certain point, you'd have to expand the whole semiconductor industry so you can actually increase the number of chips produced worldwide in order to continue to grow that investment at the pace at which it's been going recently.

34:33.000 --> 34:44.000
And so over time I expect the investment feedback loop to become less important and the AI automation feedback loop to become more important.

34:44.000 --> 35:05.000
In particular, once AI gets to the point where it's able to automate significant fractions of the work done by AI researchers to improve AI, by chip design companies like in Vidya to design better AI chips, by fabrication companies like TSMC who actually manufacture coming at chips.

35:05.000 --> 35:16.000
As AI automates that, the kind of works that those organizations do, this feedback loop will come into play and then as I get more and more capable, the feedback loop will become more and more significant over time.

35:17.000 --> 35:33.000
Yes, as I understand it, you've been working on this model for three or four years. At least you've been working on this model before the release of JetGPT, which I think accelerated AI investment. Do you have any sense of how much AI investments have increased since JetGPT?

35:33.000 --> 35:49.000
Investment in US semiconductors has been kind of growing at an unprecedented rate, probably in part related to the CHIPS Act whereby the US government is spending money to try and encourage these fab companies like TSMC to move their fabs to the US.

35:50.000 --> 36:03.000
Hearing about lots of new startups in the AI space, hundreds of millions or billions invested in them, I think seeing graphs where again you've kind of got the level of investment doubling every two years or so.

36:03.000 --> 36:21.000
I think GPT-4, I think it's estimated about 30 million US dollars to train by the end of next year will have training runs and at least the low hundreds of millions. So again, we're talking about kind of its spending increasing by a factor of two or three each year in these training runs.

36:21.000 --> 36:37.000
I think the investment feedback loop is quite straightforward to understand, but I think the AI automation feedback loop is more difficult. It's not now the case that AI's can automate everything in AI software and hardware far from it.

36:37.000 --> 36:53.000
You could see how using language models for coding might be useful if you're working in an AI organization, but it's difficult for me to understand how we go from there to AI's increasingly automating AI research.

36:53.000 --> 36:58.000
Maybe we could talk about how AI improves AI hardware and software.

36:59.000 --> 37:09.000
So yeah, let's talk about AI software. So let's give an oversimplified toy picture of what AI software researchers are actually doing with their time.

37:09.000 --> 37:19.000
So let's pretend that all they do is they have a current training algorithm. So maybe it's the GPT-4 architecture, transform architecture they're using.

37:19.000 --> 37:29.000
And then what they do with their time is they think of ideas for ways to modify that architecture to make it better in some way, maybe increase the context length.

37:29.000 --> 37:44.000
Maybe they have a new optimizer, which means it can train more efficiently. Maybe they have some modification to the attention mechanism so that you don't get such kind of quadratic scaling with the context length.

37:44.000 --> 37:55.000
And so the AI can read longer documents. And then once they've got an idea, they then implement taking code. So they write some code that will kind of represent that idea.

37:55.000 --> 38:02.000
Then they write a kind of an experiment that will test the idea and compare it to the current architecture and see how much of an improvement is it.

38:02.000 --> 38:14.000
And then they run these experiments. And kind of while the experiment's happening, maybe they're kind of watching how it's unfolding and making sure that nothing's gone wrong and there's no bugs or problems with the experiment.

38:14.000 --> 38:26.000
Once the experiment's done, they have probably, you know, a somewhat subtle job interpreting the results of that experiment and trying to kind of sort the noise from the signal and figure out, okay, was this architectural modification improvement or not.

38:26.000 --> 38:39.000
One way you can think about this process of AI automation is that AI is initially kind of just helping out in small ways with each of these sub tasks. So initially, maybe we could go through each of them.

38:39.000 --> 38:54.000
So there's the brainstorming phase, maybe they kind of give GPT-5 lots of context and relevant information about the current architecture and they say, please brainstorm some new ideas and feel free to do, you know, you know, Googling to tell you kind of new ideas.

38:54.000 --> 39:06.000
And probably at first it's not, you know, immediately coming up with the best ideas, but it's just a useful first step for an engineer is kind of kind of simulating their thinking, maybe improving the quality of the ideas that they come up with.

39:06.000 --> 39:20.000
And then the implementation phase, you know, the engineer chooses, okay, this is the architectural modification we're going to test, and GPT-5 does the first attempt at kind of implementing changes to the code base to represent that new algorithmic idea.

39:20.000 --> 39:40.000
And again, maybe at first it's not perfect. The human needs to check it and maybe it struggles with certain complex changes. But over time it gets better and better and maybe when we ultimately get to a stage where the human just describes the architectural modification in natural language and AI can just fully kind of implement code that puts the idea into practice.

39:40.000 --> 39:52.000
I mean, that's something that I can kind of almost readily imagine based on how good GPT-4 already is at coding. Then there's the kind of process of writing a test to try out the new algorithm.

39:52.000 --> 40:06.000
And again, at first, maybe the AI just does a first pass at writing the code to test the two and is just kind of giving kind of hints and helping tips to the human while the experiment is going on in terms of things that might be going wrong.

40:06.000 --> 40:19.000
But increasingly it's able to just be autonomous with that. And again, with interpreting those results, again, initially the AI is maybe kind of doing some basic analyses and human giving it subtasks of kind of ways to analyze the data.

40:19.000 --> 40:29.000
But ultimately, there becomes enough data that the AI can be trained to just do the whole thing. And so there's this kind of experimental loop with many different parts to it.

40:29.000 --> 40:40.000
And then within each part, AI is being given more and more responsibility to kind of do it autonomously over time. And then you can imagine an end state we get to where the AI is just able to do the whole thing.

40:40.000 --> 40:51.000
And they can just say, here's the current architecture, please improve it open-endedly. And it just brainstorms ideas, implements them, tests them, interprets the results, reads and repeat.

40:52.000 --> 41:06.000
And so the way I see this unfolding is that it is kind of an incremental process and a continuous process in that there's like a general over time kind of offloading of responsibilities to AI.

41:06.000 --> 41:15.000
And as that happens, the workflows will be adjusted to suit those AIs more and more because it will be kind of an AI dominated workflow of a human workflow.

41:15.000 --> 41:27.000
And with this kind of joint, there's the kind of on the one hand AI is getting better and more capable and therefore able to take on more of the work. And there's also kind of the workflow becoming adjusted and tailored to the comparative advantages of these AIs.

41:27.000 --> 41:35.000
And eventually we end up in a situation where the workflow is probably pretty different to what it is today, and it's also now completely done by AI systems.

41:35.000 --> 41:47.000
It's actually pretty convincing to me, especially if you've had the experience of asking GPT-4 to write some code for you and it just spits out something that runs immediately. I can see that working.

41:47.000 --> 42:01.000
That's on the software side. If we talk about the hardware side, I would think that hardware, there you have some interactions with the physical world, maybe you have a chip design, but you have to create the chip physically before you can use it.

42:01.000 --> 42:06.000
Would there be some kind of bottleneck to AIs improving AI hardware there?

42:06.000 --> 42:18.000
I think you are going to get more bottlenecks of that sort with AI hardware. One thing that I think won't be bottlenecked is the work done by so-called fabulous hardware companies.

42:18.000 --> 42:25.000
So NVIDIA is one of these companies. They are a chip design company, but they do not themselves manufacture any of the chips.

42:25.000 --> 42:38.000
So what they do is they work on designs, blueprints for AI chips, and once they've developed them, they send them to a chip manufacturer like TSMC to then physically produce the chip.

42:38.000 --> 42:49.000
NVIDIA's work is hugely valuable and a lot of the improvements in AI hardware in recent years have come from NVIDIA iterating on their AI specialized chips.

42:49.000 --> 42:55.000
And so that portion of the work is what you can do remotely. It's cognitive work.

42:56.000 --> 43:17.000
It's understanding the way that the basic underlying technology works, that TSMC is working with, and then figuring out more effective and efficient ways to stack the little calculating units that actually perform the computations on the chip so that they can do those AI specific computations more efficiently.

43:17.000 --> 43:30.000
So that kind of element of it, that fabulous element, I think there are going to be fewer bottlenecks with, but there's a whole another driver of progress in hardware, which is designing what's called a new node.

43:30.000 --> 43:46.000
And that is kind of relating to what you may have heard of Moore's Law, which is this process by which the kind of the basic chip technology has improved over time so that the processing units that do the calculations on chips can get increasingly small and kind of increasingly energy efficient.

43:47.000 --> 44:02.000
Over time, and that process, as you say, involves working with physical materials, involves, you know, probably designing a specification, but then having to then test that against how materials work in the real world.

44:02.000 --> 44:10.000
And so I think with that side of things, it's much more likely that you do, that the AIs cannot fully automate the work themselves.

44:11.000 --> 44:29.000
AIs may be able to give very significant speed ups. And I think, you know, you to really investigate this, you'd want to do a deep dive into how this how this area of R&D works, and I haven't done that, but I will just flag, you know, one possibility which is that, yes, you need physical materials to test the ideas.

44:29.000 --> 44:41.000
And you need, you know, physical human in the lab to set up those experiments to do those tests. But there are lots of humans in the world. And there's, you know, lots of raw materials in the world.

44:41.000 --> 44:53.000
And so if you had a kind of unlimited supply of cognitive labor that was absolutely tip top professional kind of hardware specialist. So imagine you take the very best hardware specialist in the world.

44:53.000 --> 45:12.000
And then you make it so there's now a million of them. And each of them can think hundreds of times as quickly. And they are able to direct people who have much less experience to design experiments, to kind of do kind of implement those experiments and able to give, you know, real time instructions for those people.

45:12.000 --> 45:26.000
Then you might well find that you can actually find enough physical bodies to actually do those experiments and practice. You know, you're not massively bottlenecked on that you're actually able to scale up the kind of the physical side of the operation.

45:26.000 --> 45:42.000
Quite rapidly, by, by kind of having some kind of remote AI cognitive experts direct their physical activities. I think that, you know, there's, there's, there's lots of reasons this might not happen. Maybe people are just slow to actually, you know, change processes in these ways.

45:42.000 --> 46:00.000
Maybe there's regulation which limits it, but by default there's not that much regulation of the R&D process. And if it is in fact very cheap to run, you know, an absolute cognitive expert in the area of hardware, you think that the companies that are developing these chips would, would, would want to do that and would have strong incentive to do that.

46:00.000 --> 46:16.000
And so it is a possibility in my mind that these, these physical bottlenecks are not actually do not slow things down as much as you might think at first blush because of the ways that you can use an abundance of cognitive labor to kind of get around them and just recruit more warm bodies to run the experiments.

46:16.000 --> 46:25.000
How much of AI research and development do you think is automated right now? Is it 1% or 5% or basically nothing?

46:26.000 --> 46:44.000
It's a great question. I think it's not nothing in video recently published about using one kind of, I think reinforcement learning AI system to automate some of their chip design work. People at the top AI labs are, I'm pretty sure using the labs,

46:44.000 --> 46:59.000
AI's to help them write code using co-pilot or probably using internal systems with more capable AI and that will be accelerating their workflow somewhat. You see some statistics and some kind of measurements of what the productivity gains are here.

46:59.000 --> 47:13.000
I think it's really hard to measure this reliably. I, you know, the numbers I see are normally between 1% and 10% in terms of the productivity gains so that might cross bond to a similar fraction of tasks automated.

47:13.000 --> 47:31.000
Some people report that more significant productivity gains from using AI systems in their personal workflows, you know, people report 20% 50% productivity gains, but I don't think that has been verified outside, you know, that kind of just a few of your people claiming it.

47:31.000 --> 47:42.000
When do you think the AI automation feedback loop really gets going at what level of automation of AI research and development does the feedback loop really kick in?

47:42.000 --> 47:57.000
So it is a continuous process where you just, the more automation you have, the stronger the feedback loop gets, and it's hard to give a specific number because if automation happens more slowly, then it will seem more like business as usual,

47:57.000 --> 48:10.000
because that has already been a preexisting process of, you know, automating our workflows. And so if we got to 50% automation, we only got there in, you know, 2070, then that might well just feel like a continuation of the standard process of automation.

48:10.000 --> 48:27.000
On the other hand, if we got to 50%, but we got there in 2028, which doesn't seem out of the question to me, then I think that would feel like a very significant effect. And that then we would see the feedback loop really noticeably getting going at that point.

48:27.000 --> 48:35.000
You have a key metric that you're estimating using this model. Maybe you can explain what the key metric is here.

48:35.000 --> 48:52.000
The metric I'm using is the time from developing AI that could readily automate 20% of the cognitive tasks in the economy to the time when AI could readily automate 100% of the tasks that people perform in the economy.

48:52.000 --> 49:07.000
Where that latter milestone, the 100% milestone is just the definition of AGI I gave earlier. And so what I'm doing with this metric is I'm taking kind of an established AI milestone that people talk about, which is AGI, and then I'm kind of generalizing it.

49:07.000 --> 49:17.000
Because AGI implicitly refers to when AI can perform 100% of cognitive tasks. I'm saying let's generalize that to AI that can perform, you know, smaller percentages of cognitive tasks.

49:17.000 --> 49:33.000
And then I've gone with 20% as my starting point, because that's a point at which AI is going to be having a very noticeable and significant economic impact. I think it will be kind of very much mainstreamed that AI is going to be a kind of very potent, powerful technology.

49:33.000 --> 49:48.000
But it's not yet to the stage where it's going to be able to pose risks of disempowering humanity, because it's only able to do 20% of the tasks in the economy. And I think to kind of overthrow humanity, you're going to be able to have to do much more than that.

49:48.000 --> 50:03.000
And what exactly does 20% of cognitive tasks actually mean? Does it imply that a lot of people are losing their jobs? Or is it various tasks across a lot of jobs such that no one might lose their job?

50:03.000 --> 50:20.000
I think more likely it doesn't involve lots of people losing their jobs. I mean, I could go back to that example we did of the, an AI research and what their workflow looks like. And then, you know, probably in that example, the 20% point was one where, you know, there's a few of their

50:20.000 --> 50:32.000
sub tasks where AI is, you know, adding a lot of value. Maybe they've handed over half of the work. And there's some of the sub tasks where AI is, you know, only adding a small amount of value. But, you know, the human is still needed in all the different parts of the workflow.

50:32.000 --> 50:49.000
And so my kind of modal guess for how this will play out is that AI will help out in kind of lots of little ways and then increasingly big ways in people's jobs without kind of just replacing certain jobs wholesale.

50:49.000 --> 51:07.000
So maybe a very powerful person assistant AI would draft all your emails and will do the first pass on any documents you write, but you'll still be responsible for those outputs and for checking them. I do think that it will be somewhat uneven. I don't expect, you know, every job to see 20% of its workflow,

51:07.000 --> 51:22.000
automated the same as every other job. But broadly, my expectation is, is that it's individual tasks within jobs that are primarily the things being automated rather than jobs themselves being being the kind of thing that's automated.

51:22.000 --> 51:28.000
Yeah, and how close to 20% automation of cognitive tasks do you think we are right now.

51:28.000 --> 51:48.000
20% cognitive automation would correspond to, you know, more than 10 trillion of economic value add, if that was actually rolled out around the world. So if we are at the 20% cognitive automation milestone, then we are only seeing a very small fraction

51:48.000 --> 52:02.000
of the economic effect that that would have expect you'd expect that to have if it's fully rolled out. And in fact, the way I define able to automate 20% of tasks is actually say it should be able to automate those tasks within just a year.

52:02.000 --> 52:20.000
It should take no more than a year of kind of integrating them into your workflows, but you know the system can actually in practice form that 20% of tasks. And I don't think we're at that stage where if we all tried hard for a year to automate GPT for our workflow, then it would actually be able to create

52:20.000 --> 52:33.000
more than the numbers of buying the economy. So I think that even the GPT for is very impressive. And maybe if we have, you know, decades to integrate it, maybe it could automate 20% of tasks but in terms of the word to find it with this kind of you just got a year to actually implement

52:33.000 --> 52:43.000
it in practice. I don't think we're at the 20% automation. Do you think there's more automation in AI research and development than in the general economy.

52:44.000 --> 53:05.000
Large language models like GPT for they are particularly good at language based tasks. And they're also unusually good at coding. And those types of tasks are kind of heavily represented with AI R&D. There's a lot of coding. There's a lot of kind of theoretical reasoning which which

53:05.000 --> 53:24.000
really happens in written form. And so compared to a job that involves, you know, more physical labor, like a bus driver compared to like a T2 where you're kind of in the classroom interacting with other people. I think those jobs are more susceptible to AI

53:24.000 --> 53:41.000
Let's talk about the takeaways from from this model. Your guesses for how quick takeoff speed will be defined as the way we just defined it going from 20% automation of cognitive tasks to 100% automation of cognitive tasks. What are your mainline

53:41.000 --> 54:03.000
guesses here. The model itself spits out a 15% probability that takeoff happens in less than one year, and a 50% probability that happens in less than three years, and a 90% probability that happens in less than 10 years.

54:04.000 --> 54:19.000
So it's on the whole predicting probably, you know, probably between one and 10 years after the point at which AI can readily automate 20% of cognitive tasks before the point at which it can readily automate all cognitive tasks.

54:20.000 --> 54:37.000
Yeah, this is much faster than I would have guessed without looking at your report or looking at any data. So maybe it's to give our listeners a sense of why this takeoff speed might be so so fast we could talk about how we get to millions or billions of AI scientists.

54:37.000 --> 54:46.000
These two key inputs I mentioned earlier compute and software. They have just recently been growing at really astounding rates.

54:46.000 --> 55:05.000
And so just extrapolating that very fast rate of input growth, you know, does tend to push towards a faster takeoff so just to quote some quick, quick statistics the amount that's been spent in terms of dollars on the largest training runs has been increasing by about a factor of three over the last 10 years every single year.

55:06.000 --> 55:18.000
The quality of the kind of cost efficiency of AI chips has been doubling every two years or so, and the quality of algorithms that their efficiency has been again doubling every year.

55:18.000 --> 55:36.000
And so these kind of exponential trends stack on top of each other in terms of cost of cost the money spent on compute the kind of cost efficiency of compute with computer chips and the improved algorithms, which means that they kind of the effective inputs into developing these systems are going very rapidly.

55:36.000 --> 55:53.000
Then that's combined with my prediction that by the time I can automate 20% of cognitive tasks in the broader economy is probably going to be automating a much larger fraction than that, in terms of AI research itself in terms of designing better chips and improving algorithms.

55:53.000 --> 56:06.000
And so these very fast exponential rates of improvement of anything will be higher at when we kind of reached that 20% mark, then they are today.

56:06.000 --> 56:24.000
A last thing that's driving the results is that there is a pretty significant increase in abilities from, like I said, from GPT two to GPT four, and it seems plausible based on based on kind of looking at that, and also based on looking at kind of evidence from biology about how intelligence changes as you increase the

56:24.000 --> 56:39.000
brain size of various animals, it's possible from those kinds of kind of eyeballing those kinds of trends that that just you know another jump like that of GPT two to GPT four another jump like that might be sufficient to go from that 20% automation milestone to 100% automation milestone.

56:39.000 --> 56:58.000
And you're kind of bringing those things together. It is it does seem plausible that just in a few years you could you could do a jump of that kind of size from GPT two to GPT four maybe two jumps of that kind of size with the AI automation feedback loop speeding things up and then go from 20% to 100% automation just in a

56:59.000 --> 57:01.000
just in, you know, a handful of years.

57:01.000 --> 57:12.000
So you talk about brain sizes in evolution. How does that inform us about going from 20% automation to 100% automation? Which species are you thinking about?

57:12.000 --> 57:33.000
So it's very kind of zoomed out and rough but but essentially what it's doing is it's saying, look at chimpanzees, they have about a brain that's about three times smaller than that of humans, and they do seem along along some dimensions to be, you know, notably less capable in terms of their kind of abilities.

57:34.000 --> 57:49.000
And so if you're using that to benchmark how much might the kind of abilities of AI systems improve when they're around the human level, because you know that that's that's an example we have of intelligence increasing around human level from biology, then it's just saying we could see some pretty

57:49.000 --> 58:02.000
significant increases in kind of abilities around the human level just by increasing the brain size by a factor of three which might correspond roughly to increasing the number of parameters in the AI system by a factor of three.

58:02.000 --> 58:21.000
So if you think that that kind of chimpanzee to human jump is sufficient to go from kind of 20% to 100% automation, then you might think that you would need to, you would need to increase the kind of the amount of compute and training, the quality of the training and that much to go from 20% to 100% as well.

58:21.000 --> 58:32.000
Yeah, I think we should stress this point of the ability to train an AI model with a given amount of compute implies that you have that amount of compute available to run the models afterwards.

58:32.000 --> 58:39.000
That's the key as I understand it to getting to these millions or potentially even billions of AI scientists.

58:39.000 --> 58:53.000
Open AI took a number of months to train GPT for what they did is they used a huge number of computer chips and had GPT for digest on read through a huge number of articles from the internet and other data.

58:54.000 --> 59:12.000
Once that training was complete. Open AI still had these chips sitting around. They had previously been using to train GPT for and you can imagine that they then say, okay, let's now use these computer chips to run copies of GPT for you can ask how many copies would they be able to run in parallel.

59:12.000 --> 59:22.000
Let's say that each copy is producing 10 words per second. So that's, you know, it's thinking a bit fast and the human can think I would say, you know, I'm not able to write 10 words a second.

59:22.000 --> 59:36.000
Let's say that each, each copy of duty for is is producing 10, 10 words of text per second. It turns out that they'll be able to run something like 300,000 copies of GPT for in parallel.

59:36.000 --> 59:50.000
By the time they're chaining GPT five, it'll be a more extreme situation where just using the computer chips that they used to train GPT five, using them to kind of run copies of GPT five in parallel, you know, again, it's producing 10 words per second.

59:50.000 --> 01:00:00.000
They'd be able to run 3 million copies of GPT five in parallel and for GPT six, it will just increase again. There'll be another factor of 10 at play.

01:00:01.000 --> 01:00:16.000
And so it'll be 30 million copies running in parallel. And so if you imagine eventually we're training a system which is as productive and is generally competent as a human expert at kind of advancing AI research.

01:00:16.000 --> 01:00:23.000
So it's as good as the best, best researchers that open AI labs employ.

01:00:23.000 --> 01:00:38.000
Then, once you've trained that pathway I system, you're you're merely then able to run seemingly millions of copies in parallel doing doing the work that I expect to do to advance AI systems.

01:00:38.000 --> 01:00:53.000
And it's that kind of massive abundance of cognitive labor, which which kind of points to the possibility of of there being very, very rapid AI progress, just to the point at which we're developing AI systems that can automate the work done by expert researchers.

01:00:53.000 --> 01:01:07.000
Yeah, I think this was the key point that helped me understand how progress might be that rapid. If you just imagine these millions of experts working day and night on the problem, it suddenly seems at least more plausible to me.

01:01:07.000 --> 01:01:23.000
The conclusions you come to are quite counterintuitive. They're not common sensical. Do you think that counts as an as a counter argument here at all? Or is it just a case that our common sense intuitions are not applicable to technologies that that's moving this fast?

01:01:23.000 --> 01:01:31.000
We should pay attention to common sense and we should we should try and look to see what it's grounded in and whether it whether it makes sense to put a lot of weight on it.

01:01:31.000 --> 01:01:41.000
And I think in this case you can cash out the common sense instinct with something which is pretty sensible. So you can you can say look, we've seen automation happen in the past.

01:01:41.000 --> 01:01:56.000
We've seen computers do automation. We've seen automation via kind of electricity and physical factories and never has automation, the underlying technology enabling automation advanced as quickly as what I'm predicting here.

01:01:56.000 --> 01:02:16.000
It is taking decades to automate significant fractions of the work being done by humans at least. And yet here I am claiming that we could go from automating 20% to 100% of cognitive tasks in just a number of years rather than decades.

01:02:16.000 --> 01:02:30.000
And I think that that that is that is a fair point that should give us some pause. I think though there are there are other ways of of interpreting the long run historical trend which which made my prediction seem more online with what you might expect.

01:02:31.000 --> 01:02:43.000
So there's this this view of history as a series of growth modes that's described by Robin Hansen where the in his view that the initial growth mode is that of hunter gatherers as they kind of slowly expand their populations.

01:02:43.000 --> 01:03:04.000
And there's a you know pretty slow transition to an agricultural growth mode where you now have people in farming communities much more stationary. And then there's another transition to an industrial growth mode, in which we now kind of living in cities and having factories and growth is faster.

01:03:05.000 --> 01:03:20.000
And in in Hansen's model, each growth mode is faster than the last one so industrial growth is faster than agricultural growth which is faster than hunter gatherer growth, and also the transitions from one mode to the next become faster over time.

01:03:20.000 --> 01:03:34.000
So the transition from hunter gathering to agriculture took maybe thousands of years, the transition from agriculture to industrialization took maybe like 100 years, or even decades.

01:03:34.000 --> 01:03:51.000
And so, if you're extrapolating a long run trend of that kind, then, you know, a natural thing to think is okay. So the next growth mode will be faster, maybe the economy will double in, you know, just a few years, rather than in many decades, and also the transition to that next growth mode will be faster.

01:03:51.000 --> 01:04:06.000
So rather than, you know, when we industrialized it taking many decades or even 100 years to kind of transition to the new industrial growth mode, this next transition will be faster, maybe it's kind of a number of years, or even less.

01:04:06.000 --> 01:04:20.000
I think that that people have actually gone estimates out of tried to kind of piece together this very noisy historical data to get estimates of transition times and I think it is that the number that I recall is, you know, less than 10 years and times the world transition time would be like.

01:04:20.000 --> 01:04:31.000
So if you're taking a kind of a long run view of history and you're taking a view according to which there have always been transitions that have been fast and the ones we've seen historically.

01:04:31.000 --> 01:04:44.000
And so if you're if you're really looking over the long run, you should actually expect that the trends of the recent past to be broken. Then I think that the conclusion of my model is actually more in line with that kind of analysis.

01:04:44.000 --> 01:05:02.000
Does your model rely on progress in AI being a matter of more compute? Does it rely on on this current paradigm of more compute and more data producing better AI? What if, for example, more compute and more data stops being useful or we reach diminishing returns?

01:05:02.000 --> 01:05:04.000
How would that affect your conclusion?

01:05:04.000 --> 01:05:23.000
If getting to AGI required something outside of the deep learning paradigm, that would very much undermine the conclusions of the model in that there would just be the possibility that we just kind of get stuck at 50% automation and the kind of feedback loops that I'm describing might just not get us out of that.

01:05:23.000 --> 01:05:35.000
I mean, again, they might get us out of that if we're kind of automating the search for a new paradigm, you might still expect something in the direction of the model's conclusion to be correct, but there would be the potential for a pretty big blocker.

01:05:35.000 --> 01:05:41.000
Yeah, and how likely do you think that is that deep learning as a paradigm does not hold?

01:05:41.000 --> 01:05:55.000
I think it's unlikely. I mean, I think broadly deep learning being the paradigm where you have, you know, large neural network streams with large amount of data is a pretty general paradigm and has worked in a wide variety of domains.

01:05:55.000 --> 01:06:07.000
You know, as I was talking about, you've got language, you've got image, you've got videos, games, and the transformer architecture is, again, an architecture that works across all these different domains.

01:06:07.000 --> 01:06:29.000
And so I don't see any particular blockers that cannot be tackled within the deep learning paradigm. I think we'll need better memory systems in order to get to AGI. I think we'll need ways of allowing AIs to act more autonomously and to act over longer time horizons.

01:06:29.000 --> 01:06:44.000
But I'm not seeing any reason why that can't be done within the deep learning paradigm and increasingly the people who predict that scaling alone will not get UX or Y turn out to be wrong when the next kind of version of GPT comes out.

01:06:44.000 --> 01:06:51.000
I think the broad paradigm itself is likely but not definitely going to be sufficient for AGI.

01:06:51.000 --> 01:07:05.000
What if I take the parameters of your model and I set them to extremes, either very pessimistically or very optimistically? What are the extremes of how fast or slow takeoff could be?

01:07:05.000 --> 01:07:22.000
You can quite easily get less than a year for takeoff. You know, maybe you only need to go from GPT 5 to GPT 6 or something to go from 20% automation to 100% automation. That would be quite a kind of an aggressive but not out of the question claim.

01:07:22.000 --> 01:07:35.000
You could travel that distance in one year by spending significantly more on a training run just within one year and then especially with these kind of feedback loops and speeding things along.

01:07:35.000 --> 01:07:51.000
So yeah, less than one year is definitely on the table. You can also get things being as long as 20 years. If you think that it's going to take a lot of effort to develop AGI, you can think we're going to need really massive increases in the investments and improvements in the algorithms

01:07:51.000 --> 01:08:08.000
in order to do that. And you think that the effects of AGI automation on the end along the way tend to get bottlenecked by some of the things we're discussing like bottlenecks from needing to do physical experiments and delays to kind of rolling out intermediate AGI systems.

01:08:08.000 --> 01:08:16.000
You can actually benefit from their collectivity effects. So you can get things as high as 20 years, although that is somewhat extreme.

01:08:16.000 --> 01:08:35.000
It's interesting that a 20 year takeoff is considered slow or extreme. If you take the perspective of a computer scientist in 1970 or 1990 or 2000, a reasonable guess for a takeoff speed might have been 100 years, but maybe that's just my impression.

01:08:35.000 --> 01:08:46.000
Yeah, I mean, interestingly, you know, it hasn't, a lot of people have changed their timelines to human level AI recently. I think, you know, largely, which I believe before coming out.

01:08:46.000 --> 01:09:02.000
And that, you know, even before it's not automated 20% of tasks. So in fact, you know, people, people did not require seeing 20% automation in order to believe that we can get all the way to AGI.

01:09:02.000 --> 01:09:16.000
So it doesn't seem like people had this belief that that there was always going to be a really long time between the two, given that, you know, even previously skeptical experts are assigning decent probability to getting AGI in the 2030s now.

01:09:16.000 --> 01:09:27.000
Let's go through some of the economic impacts of AI, given your model of takeoff speeds. And as we mentioned, you're modeling this using GDP.

01:09:27.000 --> 01:09:41.000
But I'm just, I'm just wondering whether there are situations in which you have an enormously powerful AI, but that power is not captured by GDP numbers, potentially because the AI is not aligned with human values.

01:09:41.000 --> 01:09:50.000
And so it goes off and does something else that doesn't increase GDP at all. Is GDP a flawed measure of powerful transformative AI?

01:09:50.000 --> 01:10:00.000
Yeah, it's definitely a flawed measure. And, you know, we were discussing earlier, you know, the ways in which GDP can come apart from actual AI capabilities, if it's not actually deployed.

01:10:00.000 --> 01:10:17.000
But as you say, you know, AI could have impacts on the world that are drastic, but do not increase GDP. So AI could create a new technology which causes you want to go to war, or which disrupts democracies or enables autocracy and make it be very impressive,

01:10:17.000 --> 01:10:28.000
very, very impactful things that wouldn't be affecting GDP. AI could make us addicted to our phones in a way that really kind of ruins everyone's quality of life, without that being captured by GDP.

01:10:28.000 --> 01:10:39.000
And, you know, in the worst case, misaligned AI could disempower humanity. And either there'd be no change to GDP or GDP, you know, be growing very quickly, but actually humans are no longer in control.

01:10:40.000 --> 01:10:57.000
So certainly, GDP is a very flawed metric. Yeah. I mean, you know, already today AI is doing loads of impressive things, you know, beating the best world experts go and, you know, making amazing art and that that again has not, you know, impacted GDP very much.

01:10:57.000 --> 01:11:15.000
You know, the benefit of GDP is that it is tracking the production of goods and services that people are willing to pay for. And so it is at least one way of trying to capture in a general sense, how much are we kind of moving the needle on things that people really want.

01:11:15.000 --> 01:11:17.000
But yeah, it has a lot of drawbacks.

01:11:17.000 --> 01:11:36.000
One of your feedback loops, the AI automation feedback loop relies on us using our AIs to automate AI research. What if we choose not to do that? What if we choose instead to use our AIs to, as you mentioned, create ever more enticing content for our phones or something like that?

01:11:37.000 --> 01:11:54.000
I don't know to what extent this is happening in the world today, but you do hear complaints from scientists all the time about not enough funding being available for basic research, while there's a lot of funding available for, say, online content or whatever else is most profitable.

01:11:55.000 --> 01:12:10.000
Yeah, I don't have a strong view that we're going to get AI doing lots of basic science before it does more enticing online content. I think it could go either way. But I do think that at some point we're going to develop AGI.

01:12:10.000 --> 01:12:23.000
And at some point, some earlier point, we'll have AI that's capable of significantly accelerating basic science. And it won't be too long after we have that kind of science accelerating AI that will be pretty cheap to run those AIs.

01:12:23.000 --> 01:12:39.000
And so even if there's loads of AI online generated content, that's not going to prevent the scientific institutions that already exist from using the available funding to pay for these AIs that can massively help them with the work that they're doing.

01:12:39.000 --> 01:12:52.000
That's not going to prevent companies that want to make money by developing new technologies from using AIs to do that. So I guess my ultimate answer here is it's not either or, and I expect it to be both.

01:12:52.000 --> 01:13:05.000
But I think we could go through some of the some potential objections to your model. The most obvious one and the one you've probably heard a bunch of times is the fact that, or it's the speculation that there will be bottlenecks all over the place.

01:13:05.000 --> 01:13:21.000
So bottlenecks to implementation of AI, legal barriers, a thousand bottlenecks all across the economy that will make it that will slow everything down and also potentially slow the key feedback loop, which is the feedback loop of AI automation slow that feedback loop down.

01:13:22.000 --> 01:13:37.000
One example I have in mind here is that we've had demonstrations of self driving cars for a long time now. And we've heard rumors that self driving cars are just around the corner, but they haven't really arrived yet, at least not where I'm living.

01:13:37.000 --> 01:13:40.000
Could something similar happen to AI?

01:13:40.000 --> 01:13:58.000
I like the example of self driving cars. My understanding of what's gone on in that case is it's an issue of robustness where the technology is there to drive safely and correctly, maybe 99% or 99.9% of the time.

01:13:58.000 --> 01:14:14.000
So that's not enough in the area of driving, even a very low risk of an accident is not acceptable and rightly so. So that has significantly delayed welding out of self driving cars.

01:14:14.000 --> 01:14:24.000
And I think that that's a great example of a bottleneck that we will see with AI. There will be certain areas of the economy where you need to have a really high level of reliability to work on AI.

01:14:24.000 --> 01:14:37.000
I think probably places where AI initially has more impact on the places where you don't need so much reliability. You know, the examples I was given were in terms of no drafting things, making suggestions, but the kind of human having the ultimate responsibility.

01:14:37.000 --> 01:15:00.000
And so yeah, I mean, I completely think that bottlenecks will pop up everywhere and they will slow things down. I think once you really internalize a view, which is we're going to get AI systems which are as competent along every dimension as top human experts in every domain.

01:15:00.000 --> 01:15:17.000
And once you kind of really fully internalize and imagine that scenario, that's a scenario where the AI systems are more reliable than human significantly. So this is a scenario where you're going to have significantly more car accidents if you drive yourself and if you use a self driving car.

01:15:17.000 --> 01:15:28.000
And so while I do see these things being delays and I see them sometimes significantly raising the technological requirements for AI actually being profitable and actually being deployed.

01:15:29.000 --> 01:15:38.000
It doesn't seem to me like this is this is telling us that, you know, deployments never going to happen, or these bottlenecks are going to be indefinite.

01:15:38.000 --> 01:15:47.000
It's just saying, okay, actually your AI systems are going to have to be much more competent and clever than you naively thought before you get really significant real world impact.

01:15:47.000 --> 01:16:01.000
So I have updated based on these kinds of considerations and the update has been in the direction of thinking, okay, we'll need the underlying technology to get really pretty good before we have transformative economic impacts and before we have really wide deployment.

01:16:01.000 --> 01:16:15.000
But it hasn't seemed to me like these kinds of considerations should update me towards thinking that AI will never be used in self driving cars or will never be used in the economy because I just do think we'll get to this point where

01:16:15.000 --> 01:16:18.000
AI systems are better than the human experts on every dimension.

01:16:18.000 --> 01:16:35.000
Yeah, if we imagine, say a key engineer in a AI hardware, a company such as ASML or TSMC, this person has a lot of tacit knowledge about how to design chips this and this knowledge is not necessarily written down anywhere.

01:16:35.000 --> 01:16:42.000
What training on that knowledge or using that knowledge be necessary to get to expert level performance?

01:16:42.000 --> 01:16:56.000
And if that's so, well then it seems that that's a pretty substantial bottleneck because if the tacit knowledge by definition isn't written down and can't be trained on, well then it can't be incorporated into the model.

01:16:56.000 --> 01:16:59.000
Do you think that's a substantial barrier?

01:16:59.000 --> 01:17:15.000
I think it's a great example and I do think that data limitations of this kind where there's to do a job well, you need to have a specific kind of data or experience that's relevant to the context of a specific job can be a bottleneck.

01:17:15.000 --> 01:17:27.000
I'm not expecting that we get, I'm not assuming that we get AI that's so capable we can just immediately derive everything about TSMC from first principles that may not be kind of physically possible or computationally possible.

01:17:27.000 --> 01:17:40.000
And in any case, I think the fastest way to get AI to work as a TSMC person will not be for it to be derived from scratch, but would be for it to learn from the experts.

01:17:40.000 --> 01:17:52.000
So imagine we have an AI system that is a more competent, significantly more competent, hardworking worker than a top human grad student.

01:17:52.000 --> 01:18:03.000
TSMC is choosing, okay, who do we want to hire on to be on our staff, you know, hire this kind of human worker who will work eight hours a day and find a huge wage.

01:18:03.000 --> 01:18:21.000
We can hire this, this, you know, much more generally intelligent, faster learning, harder working kind of AGI worker, where the way we teach it is by kind of having it have open ended conversations with our current workers, installing cameras in our factories so that it can

01:18:21.000 --> 01:18:34.000
look at the work we're doing and how we're doing it, paying for robotics that the AGI is able to kind of operate remotely in order to do the physical labor in the factory.

01:18:34.000 --> 01:18:42.000
And that outcome appointment makes a lot more sense for these companies to get AI and robotics workers in place of their human workers.

01:18:42.000 --> 01:18:56.000
It only takes one AGI just to have, you know, conversations with the top 100 TSMC experts and having maybe, you know, intense conversations over a period of weeks or months, following them all around their work, you know,

01:18:56.000 --> 01:19:02.000
you know, trailing many different experts in parallel, because of course you can run many different copies of the model in parallel.

01:19:02.000 --> 01:19:15.000
You know, it doesn't seem to me like it would take more than months for an AGI to learn what they need to know through a combination of those approaches to be able to do all the cognitive work that someone at TSMC does.

01:19:15.000 --> 01:19:25.000
And so while I think, again, this is going to be a bottleneck and this will slow things down compared to if like all the TSMC instructions were just down the internet, it doesn't seem like this is a permanent delay.

01:19:25.000 --> 01:19:34.000
It's like a delay of, you know, months, maybe years from the point at which you have an AI system that's able to flexibly learn as well or better than you.

01:19:34.000 --> 01:19:41.000
And of course we might simply be surprised again at what more advanced models can do and what they can infer from public data.

01:19:41.000 --> 01:19:52.000
The expert engineer at TSMC arrived at his TASA knowledge through learning a lot about the publicly available data and maybe advanced AI could do the same.

01:19:52.000 --> 01:19:55.000
So we shouldn't rule that out. It's just an interesting case, I think.

01:19:55.000 --> 01:20:02.000
My guess is that you will need to speak to some experts and to like look what's happening inside the factory to get all of the TASA knowledge.

01:20:02.000 --> 01:20:07.000
But I agree that you can probably get more from the internet than you might.

01:20:07.000 --> 01:20:14.000
Is it the case that the market, so the financial markets, do these markets disagree with your predictions?

01:20:14.000 --> 01:20:20.000
If we look at the valuations of AI companies, they have increased a lot recently and they are very, very high.

01:20:20.000 --> 01:20:31.000
But shouldn't they be even higher potentially if takeoff speeds are very slow and AI that's truly transformative is quite close?

01:20:31.000 --> 01:20:37.000
I think you're right. I think that if everyone had my views where the technology is going,

01:20:37.000 --> 01:20:41.000
what its economic effects are going to have and these companies would have high valuations.

01:20:41.000 --> 01:20:48.000
I think that there's a post called transformative AI and the efficient market hypothesis that makes this point.

01:20:48.000 --> 01:20:59.000
It kind of actually zooms in on the case of interest rates and argues that interest rates should be higher if we expect economic growth to accelerate.

01:20:59.000 --> 01:21:06.000
And I think I basically agree that there's not market consensus in line with my prediction.

01:21:06.000 --> 01:21:14.000
I think it's a little bit less clear in terms of how efficient you should expect the market to be in this case.

01:21:14.000 --> 01:21:21.000
It's unclear how easy it is to make lots of money via having a prediction which is different to the market.

01:21:21.000 --> 01:21:30.000
And I'm clear whether a few people making bets and making money off this is going to shift the market to be back in line with our expectations.

01:21:30.000 --> 01:21:39.000
I'm kind of uncertain as to whether to interpret the evidence as the market is efficient and there's a consensus that you're wrong

01:21:39.000 --> 01:21:45.000
versus most people think you're wrong, but it's possible that the people who are most informed actually agree with me.

01:21:45.000 --> 01:21:56.000
But they haven't been able to shift the overall market because there's not enough of them and the market isn't sufficiently responsive to the kind of investments that they're making.

01:21:56.000 --> 01:22:03.000
What are some of the strongest objections you've heard to you to the picture we've sketched here of quite fast takeoff speeds?

01:22:03.000 --> 01:22:08.000
Is it around bottlenecks in the economy that we talked about or is it something else?

01:22:08.000 --> 01:22:12.000
So I'd want to distinguish between the capabilities and the impact takeoff speeds.

01:22:12.000 --> 01:22:27.000
On the capability side, probably the strongest objection I've heard is that it's one that we touched upon already that simply scaling up the current approaches won't be sufficient to get us all the way to AGI.

01:22:27.000 --> 01:22:34.000
And the version of that objection which I find the strongest is one that says, yes, you can probably do it eventually within the deep learning paradigm.

01:22:35.000 --> 01:22:49.000
But to get to AGI, there's going to be a lot of kind of nitty gritty work and kind of reconceptualizing exactly how you're deploying your systems and adding things like memory and adding other kind of bells and whistles.

01:22:49.000 --> 01:22:51.000
And that's not going to happen very quickly.

01:22:51.000 --> 01:23:00.000
And the framework I'm using abstracts away from a lot of that complexity and just has this kind of oversimplified notion of the quality of algorithms.

01:23:00.000 --> 01:23:05.000
Maybe actually that that simplification is leading us astray in a significant way.

01:23:05.000 --> 01:23:12.000
And there's going to be kind of algorithmic barriers to AGI within the deep learning paradigm that are very difficult to overcome.

01:23:12.000 --> 01:23:15.000
If that's the case, then I think that could delay takeoff.

01:23:15.000 --> 01:23:34.000
And another thing that could delay takeoff relative to my model, which actually does update me is the possibility that we're bottlenecked on the data for getting AGI or for getting superhuman systems where there's been kind of this massive reserve of available data online that we've been benefiting from in recent years.

01:23:34.000 --> 01:23:51.000
But once we're trying to get to superhuman performance, it's going to be harder to elicit that from existing types of data because existing data will not kind of exhibit superhuman performance as readily as it does human performance because the data is produced by humans.

01:23:51.000 --> 01:24:01.000
And so I could see there being a bit of a slowdown or a bit of a kind of, yeah, a bit of a headwind in terms of going past the human level because of because of that.

01:24:01.000 --> 01:24:06.000
And because more generally running out of the internet data that has so far been readily available.

01:24:06.000 --> 01:24:21.000
So those are the two objections on the capability takeoff side. And then on the impact takeoff, this kind of economic impact stuff, I think that there's no one objection which I find hugely convincing.

01:24:21.000 --> 01:24:27.000
One thing you can say that I do find somewhat convincing is just to say that there's loads of loads of different possible bottlenecks.

01:24:27.000 --> 01:24:40.000
There's kind of the time to design physical robots that will need to actually do physical work, but you'll need to actually really change economic growth. There's kind of limit limits on physical resources you can use to drive the AIs and the robots.

01:24:40.000 --> 01:24:55.000
There's time that you need to do experiments. There's bottlenecks from kind of humans resisting being replaced and from regulations and maybe none of these bottlenecks is individually enough to really block AI, but they all combine together.

01:24:55.000 --> 01:25:12.000
And they just really drag out the time of AI's economic impact. And then maybe by the time AI is widely deployed, then for some reason or other it's not able to drive really transformative tech progress because maybe by then we've kind of already reached the ultimate limits to technology.

01:25:12.000 --> 01:25:23.000
I mean, that's the part of the story, which I don't find that convincing. My overall honest view is that I think there will be a lot of bottlenecks. I think there'll eventually be overcome and at that point, I expect things to be very, very crazy.

01:25:23.000 --> 01:25:33.000
But if there's somehow a way that it could take us so long to remove all these bottlenecks that there's no room for kind of AI to drive much faster technological progress once they will move.

01:25:33.000 --> 01:25:40.000
And that would be where I'd go if I was trying to give the strongest story for why this is all wrong.

01:25:40.000 --> 01:25:55.000
Yeah, what's something you've changed your mind on over the course of writing this report? One thing you mentioned as a takeaway is that you now think that it's more difficult to avoid getting to artificial general intelligence by 2060.

01:25:55.000 --> 01:25:58.000
Is that the biggest takeaway or are there other things?

01:25:58.000 --> 01:26:13.000
That's one big takeaway. So that was thinking about these feedback loops, both the investment feedback loop and the AI automation feedback loop, made me realize that even if we don't get to AGI, they can do all kind of tasks by, let's say, 2040.

01:26:13.000 --> 01:26:27.000
It seems hard for me to imagine we haven't got to AI that makes a lot of money in the economy and to AI that is able to automate pretty significant fraction of the cognitive work involved in automating AI R&D.

01:26:27.000 --> 01:26:38.000
So once you get to that kind of first stepping stone, that's going to stimulate further investment and that will accelerate further AI progress and it becomes quite hard for me to imagine a world where we don't get.

01:26:38.000 --> 01:26:52.000
Not impossible, but it becomes harder for me to imagine a world where we don't get AGI by 2060 because I kind of have to really lower the capabilities of what AI can do by 2040 to such a low point that it no longer, I no longer really believe those predictions.

01:26:52.000 --> 01:27:12.000
And that is the possibility that this AI automation feedback loop goes pretty quickly that regulations don't interfere with it very much because R&D is typically not a very regulated field and that you could get some really scary fast progress in the underlying AI technologies around the time at which we reach human level systems.

01:27:12.000 --> 01:27:23.000
Even if it doesn't immediately have economic impacts, I think in terms of the risks that that could pose, that would be very risky and destabilizing if it does in fact happen as quickly as it seems maybe technologically feasible.

01:27:23.000 --> 01:27:27.000
Yeah, I think it's worth spending a little time on that picture.

01:27:27.000 --> 01:27:32.000
I've been walking through a number of objections to your model and to your view of AI progress.

01:27:32.000 --> 01:27:47.000
If we simply assume that your view is correct and we take your kind of most likely way that things will go, how does it look to you and I think we should spend more time reiterating why this would be potentially dangerous.

01:27:47.000 --> 01:28:05.000
One scary possibility is that AI systems developed in let's say 2030 are able to automate a very large fraction of the work done by researchers, let's say able to automate 80% of that work.

01:28:05.000 --> 01:28:15.000
They do not themselves pose the most extreme risk, they don't themselves pose the risk of disempowering humanity, they pose other risks, but they're not that particular risk.

01:28:15.000 --> 01:28:21.000
But what they do is they enable progress from that point to be significantly faster.

01:28:21.000 --> 01:28:30.000
They're helping Nvidia design significantly better AI chips and so the pace at which those AI chips are improving is three times as fast as it is today.

01:28:30.000 --> 01:28:38.000
And similarly, they're allowing the design of AI algorithms to be significantly accelerated, let's say again three times faster than this today.

01:28:38.000 --> 01:28:51.000
So rather than the quality of AI chips doubling every two years, it's doubling every eight months and rather than the quality of AI algorithms doubling every 12 months, it's doubling every four months.

01:28:51.000 --> 01:29:06.000
And then this leads to it to only be, you know, a couple of years later that we have AI that can not only do 100% of the tasks done in AR&D but are actually significantly superhuman on many dimensions.

01:29:06.000 --> 01:29:27.000
And then we've got this period of just a small number of years where some of the most extreme risks from AI are merging, in particular the risk of superhuman AI systems that humanity loses control of that ultimately end up determining the future of how history plays out.

01:29:27.000 --> 01:29:40.000
And because it's happening in just a few years, we don't have much time to study those systems and understand the risks they pose. We don't have much time to use slightly weaker systems to help us solve the problem of controlling those stronger systems.

01:29:40.000 --> 01:29:51.000
We don't have time to get governance proposals in place that manage these risks because regulations typically take a long time to come into play.

01:29:51.000 --> 01:30:00.000
It's hard for labs to coordinate without that governance of labs to coordinate on going slower than they would be able to if they just plowed on full speed ahead.

01:30:00.000 --> 01:30:13.000
And so we end up just kind of hoping for the best and some actor develops superhuman systems without really properly understanding what those systems are capable of and what the risks are.

01:30:13.000 --> 01:30:24.000
It could potentially help by the fact that if this transformative AI is quite close, then it will probably be developed by companies that we know of and with techniques that we are already aware of.

01:30:24.000 --> 01:30:38.000
Is this any reason for hope here that because these paradigms or these companies are well known, it might be easier for us to control them even though everything is happening incredibly quickly?

01:30:38.000 --> 01:30:54.000
That's an interesting question. I think it's true that if we switch to a totally new paradigm of AI development, then that might undermine some of the work we've already done in terms of how to understand and control these systems.

01:30:54.000 --> 01:31:10.000
It's hard to predict whether a new paradigm would be more or less easy to work with in terms of understanding and aligning these systems and I won't speculate on that, but I think all things equal, yes, it's nicer to work with a paradigm that we're already familiar with.

01:31:10.000 --> 01:31:22.000
The flip side is that we don't have a solution at the moment to how to control superhuman AI systems and there's also no kind of really strong candidate solutions that people are excited about.

01:31:22.000 --> 01:31:32.000
The most exciting example that people point to is the plan of using AI systems to come up with a better solution, which is clearly a can kicking solution.

01:31:32.000 --> 01:31:43.000
One reason it could be nice if we flip to a new paradigm would be that maybe there would actually be a plan for learning systems that was a little bit more concrete.

01:31:43.000 --> 01:32:00.000
Let's switch topics slightly here. We've been talking about how AI progress can be driven by lots of training compute and data, but you've also done some work on how we might get AI progress without additional compute.

01:32:00.000 --> 01:32:14.000
And I think just to introduce this topic, we could talk about compute governance as a paradigm and how this paradigm might break if we can get a lot of AI progress without any additional compute.

01:32:14.000 --> 01:32:30.000
Recently, the main driver of AI progress, I think, has been increasing the amount of compute, the amount of computational power used to develop the most advanced AI systems.

01:32:30.000 --> 01:32:40.000
And so I've talked a bit about how the quality of chips are getting better over time, cost efficiency doubling every two years and how spending has been increasing by a factor of two or three each year.

01:32:40.000 --> 01:32:46.000
But there are other drivers of AI progress, one of which I've already talked about, which is the efficiency of the training algorithms.

01:32:46.000 --> 01:32:55.000
I mentioned that you're able to use your compute twice as efficiently this year compared to last year due to improvements in those algorithms.

01:32:55.000 --> 01:33:01.000
And there are actually other drivers of AI progress that I haven't even discussed yet. So there's improvements in data.

01:33:02.000 --> 01:33:19.000
For example, reinforcement learning from human feedback is a mechanism for using data from humans to kind of tweak the performance of a model like GP4 after it's already been trained on a huge amount of internet text.

01:33:19.000 --> 01:33:35.000
There's a technique called constitutional AI that was developed by Anthropic where AI models review their own outputs, score themselves along various criteria, and that is then used as data to improve that AI model.

01:33:35.000 --> 01:33:44.000
And then there's other kind of improvements in data like through creating high quality data sets in things like mathematics and sciences.

01:33:44.000 --> 01:34:03.000
And there was recently a very large improvement in mathematical abilities of language models with a paper called Minerva where the main thing they did is they just took a lot of maths and science papers and they just cleaned up the data for those science papers.

01:34:03.000 --> 01:34:13.000
So that I'm previously certain mathematical symbols have not been correctly represented in the data. And so, you know, the data hadn't really shown language models how to do maths properly.

01:34:13.000 --> 01:34:23.000
They claim that data so that now all the symbols were represented correctly. And just from that data improvement, mathematics performance improved very dramatically.

01:34:23.000 --> 01:34:29.000
So that's that's a source of improvement which which isn't from compute often better outcomes just from high quality data.

01:34:29.000 --> 01:34:40.000
Then there's improvements coming from better prompting people may have heard of the prompt think step by step or chain of thought prompting where you just simply encourage a model.

01:34:40.000 --> 01:34:52.000
And you give it a question like you know what's 32 times 43 and instead of outputting an answer straight away you encourage it to think through step by step so you know doesn't intermediate calculations.

01:34:52.000 --> 01:35:08.000
And that can improve performance significantly on certain tasks, especially tasks like that's a logic that require a benefit from intermediate reasoning. There's other content techniques as well like you shot prompting where you give the examples of what you want to see that can significantly improve performance.

01:35:08.000 --> 01:35:19.000
And I think this is kind of funny that this this might be similar to how humans work. So if you ask yourself to to to think through a problem step by step you probably get a better result than than just coming up with an answer immediately.

01:35:19.000 --> 01:35:30.000
If you ask yourself to generate five answers to a question you might get get a better result than if you if you only generate one and so on. Yeah, yeah, I completely agree I think that's that there's an energy there for sure.

01:35:30.000 --> 01:35:35.000
So we've had improvements driven by better data improvements driven by better prompting.

01:35:35.000 --> 01:35:55.000
There's also been improvements driven by better tool use as a paper called tool former, where they train a language model that was initially just trained on text, they train it to use a calculator and a calendar tool, and an information database, and then it's able to learn to to use those tools, and

01:35:55.000 --> 01:36:15.000
actually, ultimately, it kind of plays a role in generating its own data for using those tools. Then it's performance again, as you might expect improves and downstream tasks GP for if you if you pay for the more expensive version you can you can enable plugins, which allow GP for to use various tools like web browsing and music code interpreter

01:36:16.000 --> 01:36:18.000
to run code experiments.

01:36:18.000 --> 01:36:38.000
So that's been driving improvement. There's a kind of class of techniques I'm referring to scaffolding where the AI model is kind of prompted to do things like check its own answer and find improvements, and then kind of have another go at its answer where it's prompted to kind of assign break the tasks down into

01:36:38.000 --> 01:36:55.000
sub tasks, and then kind of sign each of those sub tasks to another copy of itself, where it's prompted to kind of reconsider its high level goal and and how its actions are currently of helping or not having achieved that goal, that kind of scaffolding underlies auto GPT, which which people may have

01:36:55.000 --> 01:37:03.000
may have heard of a kind of agent AI that is powered by GP for and this scaffolding that kind of structures the GP for thinking.

01:37:03.000 --> 01:37:14.000
How much do you think we can gain from these techniques that kind of uses the output put of one AI in order to generate data that's then used to to to improve the AI itself.

01:37:14.000 --> 01:37:21.000
Do you think we can make up for potentially running out of human generated data by using this AI generated data.

01:37:22.000 --> 01:37:38.000
I think that that will be one, one tool that is used to get around the data problem. Yes. So you can imagine AI is paraphrasing existing internet documents so that they're not exact repeats, but maintain the meaning and then training on those already

01:37:38.000 --> 01:37:51.000
there are papers where AI generates attempted solutions for example to a coding problem and then those are checked kind of automatically and then only the good solutions and then they're back into the training data that there will probably be lots

01:37:51.000 --> 01:38:01.000
of creative ways in which AI companies are trying to produce more high quality data and increasingly they'll be able to leverage. I'm kind of capable AI systems to produce that.

01:38:01.000 --> 01:38:15.000
Well, while AI systems are less capable than humans, there's going to be a limit there because ultimately the data from the internet is coming from humans and so the data that is producing might be less low quality.

01:38:15.000 --> 01:38:28.000
And there are also problems you get at the moment where if you continually train on data that you're producing, then progress doesn't store as I understand it. I'm from the papers I've read, but I think they'll be pushing on improving those techniques.

01:38:28.000 --> 01:38:34.000
There's a long list of ways we might get AI improvements without additional compute.

01:38:34.000 --> 01:38:51.000
The last one I wanted to mention was efficiency gains. So shortly after chat up to 3.5 was released, there was a turbo chat up to 5 that was released that was much faster and much more efficient in terms of my compute that was used by open AI servers.

01:38:51.000 --> 01:39:01.000
And there are various techniques like quantization and flash attention that just allow you to run a model with a very similar performance to your original model that use less compute to do so.

01:39:01.000 --> 01:39:12.000
And so that's again, you don't need additional due to chips to better from that improvement. These are all the improvements I've listed here, the ones that you can do without getting more compute.

01:39:12.000 --> 01:39:19.000
And why would all of these improvements without additional compute be a problem for the paradigm of compute governance?

01:39:19.000 --> 01:39:45.000
Compute governance is one, I think, very exciting approach to governing the risks from advanced AI. And so very briefly, the idea behind the approach is that there are a very small number of organizations that produce the chips for the top AI systems today.

01:39:45.000 --> 01:40:00.000
And there are also a small number of organizations that produce some of the equipment that you need to produce those chips in the first place. So TSMC in particular is the only organization that produces the AI chips at the very top of the range.

01:40:00.000 --> 01:40:13.000
And then there's a company called ASML, which is the only company that's able to make the equipment which is used to produce those chips. So there's a very concentrated supply chain for cutting edge AI chips.

01:40:13.000 --> 01:40:30.000
And so it seems like it could be possible to use that concentrated supply chain to track where the best computer chips go, who they're sold to, who controls them, and thereby track who is able to develop the most powerful and dangerous AI systems.

01:40:30.000 --> 01:40:51.000
Then that gives you a way to monitor what those actors are doing and how quickly they're increasing the capabilities of the AI. So you can see, okay, we know that no one's going to train an AI that's significantly better than the best yet because we know where all the computer chips are and no one has enough computer chips to train an AI that's that good.

01:40:51.000 --> 01:40:53.000
So we have some kind of assurance.

01:40:53.000 --> 01:41:12.000
Yeah, but that begins falling apart if AI companies can get AI progress kind of internally in their companies without buying lots of new chips without relying on these supply chains, simply by all of these techniques you sketched out. How big can the gains from all of these techniques be do you think?

01:41:12.000 --> 01:41:27.000
Yeah, it's a great question. I agree. It's a kind of scary possibility. One caveat I want to add right up front is, I don't think that these techniques alone with small amounts of compute are going to be enough to develop really dangerous systems.

01:41:27.000 --> 01:41:40.000
So I think that if we're tracking where this high-end compute goes, and here is access to it, then that will probably be enough to catch any developer that might develop a really high-risk system.

01:41:40.000 --> 01:42:05.000
What I think the trouble is, is that once you develop a really capable AI, and as we've discussed, you could then be running potentially millions of them in parallel or having them think 100 times as quickly as human researchers working day and night, then it's possible that these other techniques that don't rely on actual compute could give a burst of progress where maybe you can improve the efficiency at which you're running your AI systems by a factor of 100.

01:42:05.000 --> 01:42:16.000
Maybe you can improve the efficiency of your training algorithms, again, by a factor of 100. So now you kind of instead of training the equivalent of GPT-5, so there's been a significant step up in the intelligence.

01:42:16.000 --> 01:42:26.000
And then maybe in addition, you're getting big gains from the quality of the data and the scaffolding and the prompting, that is really significantly increasing AI capabilities.

01:42:26.000 --> 01:42:37.000
So really the only organizations who will be able to do that are ones that have already got a lot of compute, and so you can have all these kind of AIs doing this AI research for them, advancing all these techniques.

01:42:37.000 --> 01:42:58.000
But I think the risk here is that it becomes very hard to monitor and measure the AI progress and govern it for organizations that have kind of gone over this threshold where the AI feedback loop is powerful enough to power very significant progress by these non-compute avenues.

01:42:58.000 --> 01:43:18.000
At that stage, I think we need to make sure that our governance system, we kind of extend it beyond just tracking and measuring compute to then kind of having kind of measures for tracking what the progress is within these organizations that have very powerful AI systems

01:43:18.000 --> 01:43:28.000
and ways to catch whether these organizations are very rapidly improving their AI systems and so that we can kind of monitor and govern that.

01:43:28.000 --> 01:43:36.000
So trying to evaluate whether AI systems within these companies are already becoming very capable?

01:43:37.000 --> 01:43:46.000
Firstly, we track compute and then we will be kind of measuring with those companies that are using a lot of compute, how capable are their AIs?

01:43:46.000 --> 01:43:56.000
And then for those particular companies, we want to be saying, is there a feedback loop which is enabled just within this company where that company is able to have very rapid AI progression without even getting more compute?

01:43:56.000 --> 01:44:02.000
And so we just need to be kind of monitoring those top AI companies in this way.

01:44:02.000 --> 01:44:08.000
I think there's some excitement about evaluating these models for dangerous capabilities.

01:44:08.000 --> 01:44:17.000
I think one question I always have there is just if a model fails some evaluation, what do we do then?

01:44:17.000 --> 01:44:26.000
I think that we want companies to pre-commit to what they're going to do if models fail a particular evaluation ahead of time so that there's no ambiguity.

01:44:26.000 --> 01:44:36.000
There should ideally be a process in place which prevents the company from just saying, ah, let's just go ahead anyway, even if in advance they would have said that this was a course of concern.

01:44:36.000 --> 01:44:43.000
So you can imagine a process where an AI company publicly commits to do a certain test for dangerous capabilities.

01:44:43.000 --> 01:44:54.000
They also publicly commit that if that dangerous capabilities test is triggered, then they will pause training until a kind of a broad group of stakeholders has agreed that they can continue training.

01:44:54.000 --> 01:45:06.000
That broad group of stakeholders might just be the company's board if they have a board which is empowered to represent social interest and it has a remit beyond just profit maximization.

01:45:06.000 --> 01:45:16.000
You can imagine it being a broader group of stakeholders still where there are people in regulatory authorities or other auditing organizations that the company is committed to consult.

01:45:16.000 --> 01:45:22.000
You know, kind of a majority of agreement from before continuing with its training run.

01:45:22.000 --> 01:45:34.000
Then the company could also commit to having whistle-blown practices in place so that if it's not following this process, any employee can enormously report that and they're encouraged to do so.

01:45:34.000 --> 01:45:52.000
One possibility you mentioned somewhere is a case in which some companies has trained a powerful AI and because their information security or their cyber security isn't what it should be, that model leaks and can be potentially used by bad actors.

01:45:52.000 --> 01:46:06.000
You mentioned right in the beginning that the possibility of bioterrorism via a capable model. What are the best solutions for keeping these models safe for securing the data?

01:46:06.000 --> 01:46:18.000
My understanding is that companies are not at the stage where they can say that their models are being kept safe. That certainly if a state actor wanted to steal the weights of a cutting-edge system, they would be able to do so very easily.

01:46:18.000 --> 01:46:27.000
And probably even kind of lesser, you know, kind of smaller threats and that might be able to steal the more weights without an excessive amount of effort.

01:46:27.000 --> 01:46:35.000
Yeah, I think probably AI companies should and probably are seeing it as one of their priorities to improve their information security because of these risks.

01:46:35.000 --> 01:46:43.000
That would be a great improvement, I think. It's in the interest of the companies themselves. It seems like a win-win for me. I don't know if you agree with that.

01:46:43.000 --> 01:46:54.000
People have sometimes contrasted the desire to kind of be the responsible actor that develops a powerful AI system first for fear of a less responsible actor developing it.

01:46:54.000 --> 01:47:08.000
Instead, if you didn't plan ahead, they contrasted that with desire to go slowly and cautiously yourself. I think those two motives come together in this case where even if your main worry is actually about, you know, a kind of a bad actor developing AI systems.

01:47:08.000 --> 01:47:20.000
For you, you still want to improve your security. Everyone, the people who are worried about these systems being unsafe and the people who are worried about wanting to kind of get them, get that first ourselves can all agree that we want better security.

01:47:20.000 --> 01:47:30.000
Now, if a company was really irresponsible, I can imagine it just saying, yeah, I don't care if some bad actor steals our AI. We still just want to make money. I'm on the US market.

01:47:31.000 --> 01:47:33.000
But the AI companies aren't going to do that.

01:47:33.000 --> 01:47:45.000
Yeah, so we've discussed how AI might improve via a lot of additional compute or via no additional compute. You've talked about how this might have a transformative economic impact.

01:47:45.000 --> 01:47:52.000
One question I have is if you take in all things considered view of this, do you think this will turn out well for humanity?

01:47:52.000 --> 01:48:07.000
Because a lot of economic growth could be fantastic and has been fantastic for living standards in the past. Could we be entering into a great time or could we potentially be entering into a dangerous time?

01:48:07.000 --> 01:48:17.000
I think it could be really good or it could be really awful. I think the upside is it could be really high and I wouldn't personally think about it in terms of economic growth.

01:48:17.000 --> 01:48:26.000
But I think about it in terms of human flourishing. You could have an end to illness, an end to poverty, an end to material needs.

01:48:26.000 --> 01:48:41.000
You could have the possibility if you wanted to of going on any adventures or fulfilling any dreams you'd always wanted to pursue with new technologies, really, really incredible things might be possible.

01:48:41.000 --> 01:48:56.000
And I think that that could be a really amazing future. I think it's really hard to paint a concrete vision of what that looks like because you could analogize it to trying to tell someone 2000 years ago, all the kind of luxuries and good things in one society.

01:48:56.000 --> 01:49:03.000
You could point to this absolutely incredible entertainment going into a simulated world where you're on a real adventure.

01:49:03.000 --> 01:49:17.000
It's such a difficult thing to do to sketch out how amazing things might be. It always kind of feels flat when you say it out loud in a sense. But I see what you're aiming for here.

01:49:17.000 --> 01:49:32.000
The picture you're painting is of a future in which could either be very good or very bad. Do you see a potential for a kind of middle scenario in which the world continues more or less as it has been for the past 100 years?

01:49:32.000 --> 01:49:34.000
Is that also a live option?

01:49:34.000 --> 01:49:50.000
I think it's possible. So you could imagine hitting a real wall with AI development. And if that happens, then my default expectation would be that things continue as they have been for as long as it takes for us to get around that wall.

01:49:50.000 --> 01:50:02.000
And if that wall is really very permanent, then we begin to get into worries about the rate of technological progress stagnating as population begins to shrink because fertility is below replacement rate.

01:50:02.000 --> 01:50:15.000
And you can get into actually other worries if you really start to play out this world where we don't reach AGI. You could end up kind of stuck at current levels of technology for a long time, potentially.

01:50:16.000 --> 01:50:22.000
Okay, let's end on a lighter note and talk about AI and board games.

01:50:22.000 --> 01:50:35.000
Yeah, I've been thinking about what does it mean when an AI becomes superhuman at chess, for example, as happened in, I think, 97 or go, which happened in 2016.

01:50:35.000 --> 01:50:45.000
In the past, you would have people talking about how chess is the height of human intellectual ability. But now it just seems like humans are playing a lot of chess.

01:50:45.000 --> 01:50:59.000
Humans are interested in other humans playing chess. And even though there are some chess, some people who are very into chess who watch AIs play against other AIs, it seems that this is a domain in which humans are still very relevant.

01:50:59.000 --> 01:51:04.000
Do you think there's some lesson there for broader AI automation of the economy?

01:51:04.000 --> 01:51:16.000
I am also a little bit naively surprised at how many people are still really excited about the game. I think I had the attitude of, okay, well, if the AI can do it better than me, that kind of takes some of the excitement out of it.

01:51:16.000 --> 01:51:32.000
That said, I'm a passionate diplomacy player, and people who follow AI closely may know that some AI has recently gone pretty good at diplomacy. I think still less good than the best humans, but I think better than the average humans, maybe.

01:51:32.000 --> 01:51:48.000
At least on a text-based game. And it hasn't made me any less excited to play that game. So I think I was probably just kind of not adequately imagining what it would feel like for AI to kind of be matching my performance in chess.

01:51:48.000 --> 01:52:02.000
Maybe an implication could be, even once AI is better at any task that you can imagine in the economy, there's still going to be people who are willing to pay for humans to do tasks, because that's something they find particularly interesting.

01:52:02.000 --> 01:52:18.000
If they're interested to watch humans play chess, we'll probably still be interested to see humans produce art. Maybe we'll still want to kind of have human carers, human priests. So in terms of economic role for humans, I think this may point towards us not being kind of totally

01:52:18.000 --> 01:52:24.000
obsolete, because humans kind of like to watch other humans do things, and that can give us some kind of job.

01:52:24.000 --> 01:52:38.000
Is it worrying if AI's are getting good at the board game diplomacy? Does this mean that they might be able to do diplomacy in the real world, or that they might be able to use deception, or you can tell me about the details of the game, but that they might be able to set up some

01:52:38.000 --> 01:52:41.000
agreement and then break the agreement afterwards?

01:52:41.000 --> 01:52:57.000
It is a game where deception can get you a long way. I would have thought before it happened that this would be a milestone that would make me quite scared about AI deception manipulation, because it's a fairly complicated environment, and

01:52:57.000 --> 01:53:09.000
social dynamics are potentially quite complicated, and it's an exhausting game to play, so I would have thought if AI was able to do this game, then it's really very socially competent and persuasive and manipulative.

01:53:09.000 --> 01:53:22.000
In fact, when you see the system, particularly the AI system that's actually able to match some amount of human performance on diplomacy, it's not nearly as scary as you might have imagined.

01:53:23.000 --> 01:53:39.000
It's trained on loads and loads of different examples of diplomacy in particular, and loads of examples of messages that humans sent in diplomacy games, and it's got a kind of engine which is custom built for choosing what diplomacy moves to make that won't easily generalize outside of that domain.

01:53:39.000 --> 01:53:51.000
It seems like it's actually reaching that threshold, not by kind of thinking of new ingenious plans and manipulating humans on an untaught level, but just kind of by really learning the mechanics of the game and not making mistakes and being consistent or reliable.

01:53:51.000 --> 01:54:17.000
And so it's actually a lot less scary than I would have thought, and I think it speaks to the difficulty of designing in advance a benchmark that measures the scary capability that when that benchmark is passed will actually make you scared, because often you pick a benchmark which seems scary, but then the AI system that matches that benchmark doesn't actually end up being as scary as you thought it might be.

01:54:17.000 --> 01:54:38.000
Maybe in 2015 or so DeepMind talked about a strategy for getting to artificial general intelligence, which involved playing ever more complex board games and having these reinforcement learning agents in these in these kind of worlds where they're able to navigate more and more complex and then real world games.

01:54:38.000 --> 01:54:55.000
That's one strategy to artificial general intelligence that kind of points towards more agency for the AI. You could call this the current paradigm of large language models. There's also a similar kind of convergence towards agency in AI.

01:54:55.000 --> 01:55:04.000
At least that's that's what I'm hearing. Why is it that both of these strategies push towards more agency or more agent like behavior in AI.

01:55:04.000 --> 01:55:19.000
I think the main thing pushing language models towards being more agent like is that it's useful to have an agent, because an agent can be more autonomous and do more open ended tasks that potentially automate larger chunks of your workflow.

01:55:19.000 --> 01:55:27.000
And I think that's that's why I expect people to continue to try and improve on and iterate things like auto GPT that turn language models into agents.

01:55:27.000 --> 01:55:36.000
So I think probably there is there is there is a kind of economic force pointing the direction of creating agents and just a general you know we want to use the AI to do useful things in the world.

01:55:36.000 --> 01:55:53.000
And therefore we'll kind of try and make them more more authentic and autonomous. I think there's probably a different thing that explained deep minds approach where they're using reinforcement learning they were probably making a bet that that was just the most promising technological trajectory to get to that end point of an agent was by just kind of

01:55:54.000 --> 01:56:04.000
And it's being kind of quite good news from my perspective that actually it seems like it makes more sense to first just train language models to kind of imitate human text.

01:56:04.000 --> 01:56:16.000
And then later compose these kind of little chatbots into agent like things that I think makes it more promising that we could actually understand why these agents are behaving the ways that they are.

01:56:16.000 --> 01:56:20.000
Tom, thanks for spending a lot of time with us. It's been very interesting for me.

01:56:20.000 --> 01:56:22.000
Thank you so much. It's been a pleasure.

