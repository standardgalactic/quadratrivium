1
00:00:00,000 --> 00:00:11,040
Welcome to the Future of Life Institute podcast. My name is Gus Ducker. This is a special episode of the podcast featuring Nathan Labence interviewing Flo Crivello.

2
00:00:11,040 --> 00:00:26,700
Flo is an AI entrepreneur and the founder of Lindy AI. Nathan is the co-host of the Cognitive Revolution podcast, which I recommend for staying up to date on AI. Here is Flo and Nathan.

3
00:00:26,940 --> 00:00:46,940
Man, you know, it's, it's a tough time for somebody that tries to keep up with everything going on in AI. It's like it's gone from, you know, in 2022, I felt like I could largely keep up and, you know, wasn't like missing whole major arcs of, you know, important stories.

4
00:00:47,420 --> 00:01:03,180
And now I'm like, yeah, I'm like totally let go of like AI art generation, for example. And this policy stuff is really hard to keep up with, especially this week. Of course, it's like hitting a, you know, a fever pitch all at once.

5
00:01:03,660 --> 00:01:16,260
But, you know, it's, I love it. So I can't really complain at all. It's just, at some point, you know, got to admit that I have to maybe narrow scope somehow or just let some things fall off. I'm kind of wrestling with that a little bit.

6
00:01:16,780 --> 00:01:29,460
Which, which I think is just like a natural. Yeah, I mean, you know, I hear you. I think it's just a natural part of like the industry evolving. It's like, imagine, you know, talking about like keeping up with computers, right, in like the 80s or something.

7
00:01:29,460 --> 00:01:38,060
It's like, I'm sure at some point it was possible to keep up with computers at large, you know, it's like keeping up with tech is just like, it's like, okay, dude, it's like, it's like half the GDP over, right?

8
00:01:38,580 --> 00:01:43,860
You're doing all this in your second language, right? This is, I assume English is your second at least.

9
00:01:44,300 --> 00:01:49,180
I have an excuse. Yeah, second, I'm actually getting my American citizenship. I had the interview just yesterday.

10
00:01:49,580 --> 00:02:06,580
Wow, congratulations. That's great. I know it's not an easy process, although maybe it's about to get streamlined. I haven't even read that part of the executive order yet, but I understand that there is kind of an accelerated path for AI expertise. Have you seen what that is?

11
00:02:07,180 --> 00:02:15,780
No, but generally, there's good stuff being done in immigration, like they're relaxing a lot of these requirements, they're like closing a lot of loopholes, they're doing a lot of good stuff.

12
00:02:16,540 --> 00:02:36,140
Yeah, I've been thinking of just as kind of a general communication strategy, if nothing else, calling out the domains in which I am accelerationist, which are in fact many, I think, you know, you and I are pretty similar in this respect, where it's like, um, perhaps the singular question of the day.

13
00:02:36,140 --> 00:02:55,060
I am not an accelerationist, but on so many other things, I very much am an accelerationist and like streamlining immigration would be one of those, you know, I would sooner sign up for the one billion Americans plan than kind of, you know, the build the wall plan, certainly.

14
00:02:55,700 --> 00:03:15,380
And I just did a right before this was doing an episode on autonomy and, you know, self-driving. And that's another one where I'm like, holy moly, you know, I don't know if you have a take on this, but the, the recent cruise episode, I find to be, you know, kind of bringing my internal marketing,

15
00:03:15,380 --> 00:03:32,980
Jason, very much to the four where I'm like, we're going to let one incident a like shut down this, you know, whole thing in California. That seems crazy enough. But then the fact that they go out and like do this whole sort of performative self, I mean, whether it's performative or not, maybe it's

16
00:03:32,980 --> 00:03:43,940
sincere, but do this whole self-flagellation thing and, you know, shut the whole thing down nationwide. I'm like, can we, where is our inner Travis on this people? You know, somebody has to stand up for something here at some point.

17
00:03:44,020 --> 00:03:59,020
Totally. I agree. I think it's just the natural order of things, right? It's like, I don't know if you know that piece of history about when the automobile came about. There was this insane law that said you need to have someone walking with a flag in front of the automobile.

18
00:03:59,020 --> 00:04:10,180
That's no more than like four miles an hour. Right. So it's part of the process, man. It's infuriating. I hate it, but in some way, and maybe it's cool, but I made peace with it. I'm like, it's part of the process. You can't really stop. All right.

19
00:04:10,180 --> 00:04:22,540
That's going to do its thing. So, and it doesn't really matter anyway, because like the self-driving cars are not really deploying at a very large scale. And so I'm like, you know, it's not a bottleneck anyway. I don't think it is.

20
00:04:22,900 --> 00:04:40,060
I guess I have two reactions to that. One is like, it feels like if they, if nobody kind of fights through this moment, then there is like this potential for kind of the nuclear outcome where, you know, we just kind of get stuck and it's like, sorry, you know, the standards are so insane, you've got to be, you know, we do have a

21
00:04:40,060 --> 00:04:56,060
little bit of like a chicken and egg problem where, you know, if you had a perfect self-driving car, they'd let you deploy it, but you're not going to get to perfect unless you can kind of deploy it. And, you know, to me, this technology is just an incredible example of where, you know, the relative risk is already pretty high.

22
00:04:56,060 --> 00:05:09,820
As far as I can tell, they already do seem to be as safe or marginally safer, you know, maybe as much as order of magnitude safer already, depending on exactly what stats you look at. And I would just hate to see us get kind of, you know, is we're

23
00:05:09,820 --> 00:05:38,820
like kind of close to maybe some sort of tipping point threshold, whatever, to get stuck in a bad equilibrium of, you know, never get, and then, you know, maybe get stuck and never get out of that chicken and egg thing would just be so frustrating. I drive a 2002 trailblazer that I have sworn never to replace unless it's with a self-driving car. And it's becoming increasingly difficult to keep this thing going, you know, so I'm like, how long do I have to do that?

24
00:05:39,820 --> 00:06:04,820
I have to wait. My other take on this is, I think Tesla is actually like really good. I've borrowed a neighbor's. I don't know if you've done the FSD mode recently. My grandmother came up for a visit. It was fun. I actually took, you know, my 90-year-old grandmother on a trip back to her home, which is like a four-hour drive there, and then I did four hours back all in one kind of big FSD experiment.

25
00:06:04,820 --> 00:06:27,820
I had my laptop in the back, put a seatbelt on my laptop, so it was like recording me and recording us, you know, driving so I could kind of look at the tape later. And I was like, man, this is really good. I had no doubt in my mind coming out of that experience that it's a better driver than like other people I have been in the car with, you know, for starters.

26
00:06:27,820 --> 00:06:53,820
So I'm thinking through my personal life, like, yeah, I'd rather be in the car with an FSD than this person and that person and this other person, you know, and I'd be definitely more likely to let it drive my kids than this other person. So I felt like it was really good. And then the other thing that was really striking to me was the things where it messed up, I mean, there weren't many mess ups for one thing, but like the few mess ups that we had, there were a couple in an eight hour thing.

27
00:06:53,820 --> 00:07:22,820
It was like, if we actually had any mojo and we went around kind of cleaning up the environment, we could solve a lot of this stuff. Like there was one that my neighbor who lent me the car said, you know, you're going to get to this intersection right there on the way to the highway and it's going to miss the stop sign because there's a tree in the way. And I was like, you know, for one thing, probably people miss that too, like, let's trim the trees, you know, and then there's another one where you're getting off the highway and there's a stop sign that's kind of ambiguous, like, it's meant for the people on the service road.

28
00:07:22,820 --> 00:07:36,820
But it appears to be facing you as you're coming off the highway. And so the car saw that and stopped there. And that was probably the most dangerous thing that it did was, you know, stopping where people, you know, coming up the off ramp, like, do not want you or expect you to be stopped there.

29
00:07:36,820 --> 00:07:50,820
But that's another one where you could just go like, put up a little blinder, you know, to just very easily solve that problem. And I imagine people must have that problem too. And we just have no, no will, you know, when it comes to that.

30
00:07:50,820 --> 00:07:56,820
And again, it's I feel like I'm turning into Mark Andreessen. The more I think about self driving over the last few days.

31
00:07:56,820 --> 00:07:58,820
No, I'm with you on that.

32
00:07:58,820 --> 00:08:11,820
So where else are you accelerationist that may not be obvious as we kind of think about this, you know, this kind of AI safety and regulation moment that we're in?

33
00:08:12,820 --> 00:08:31,820
You know, pretty much everywhere, man, like I'm a Libertarian, like I used to work at Uber where I saw regulatory capture and I saw cocktails and I do believe, you know, it's the deepest level that cocktails and regulatory capture and generally, I think it's Menker Olsen who calls them

34
00:08:32,820 --> 00:08:41,820
Extractive institutions, who are just in the business of they don't want to grow the pie, they just want to grab a little bit more of the pie for themselves.

35
00:08:41,820 --> 00:08:48,820
Even if it actually shrinks the pie, they don't care as much as they get bigger chunk. And I think that's the world is just rotten.

36
00:08:48,820 --> 00:09:00,820
With thousands and thousands of these institutions, whether without private or without unions or governmental, it doesn't matter. We just have so many of these cartels floating around and it's killing everything.

37
00:09:00,820 --> 00:09:17,820
Right. It's a tragedy. And I totally understand how folks like Mark Andreessen would be, they have built such a deep and justified hatred and reaction for this nonsense that is destroying everything.

38
00:09:17,820 --> 00:09:27,820
That is immediately just the pattern recognition immediately triggers when they see what's happening with the eye. They're like, ah, it's happening again. They're doing it again.

39
00:09:27,820 --> 00:09:37,820
It's like chill. I totally get it. But this time is really different. This is really something special that's happening, not just in the markets, not just in the economy, not just in the country, in the universe.

40
00:09:37,820 --> 00:09:45,820
Like there is a new form of life that's being built. And this is, we're like a new territory and we need to be careful right now.

41
00:09:45,820 --> 00:09:58,820
Right. And so that's, that's where I'm coming from is like, I totally see that point of view. And I'm like, regulation, for sure there's going to be cartels for sure we're going to screw up 90% of it.

42
00:09:58,820 --> 00:10:03,820
Politics is going to get messy and trench interest are going to get into play.

43
00:10:03,820 --> 00:10:14,820
And it's all worth it because what may very well be on the line, it sounds alarmist, but I'm sorry that we need to say the word may be literally human extinction.

44
00:10:14,820 --> 00:10:25,820
Right. And this is not some tinfoil hat theory. There's a more and more experts that are coming around and saying that it's actually funny. Mark Andreessen, if you dig it up, I'm sure you could find it.

45
00:10:25,820 --> 00:10:31,820
I think it was an interview from him. I want to say between 2017 and 2020 that doesn't help him because he gives so many of those.

46
00:10:31,820 --> 00:10:37,820
But I think he said something like, at the time he was actually appealing to an argument of authority.

47
00:10:37,820 --> 00:10:45,820
He was like, look, he was saying the same things he's saying today, poverty is good. It's just a tool. And by the way, the experts say there's nothing to worry about.

48
00:10:45,820 --> 00:10:52,820
So I don't know. You guys don't know anything about AI. I don't know anything about AI. They do. And they're telling us there's nothing to worry about.

49
00:10:52,820 --> 00:10:57,820
The argument isn't true anymore. The experts are telling us there is something to worry about.

50
00:10:57,820 --> 00:11:03,820
And now it's just like, oh, arbitrary, like a regulatory capture. No, no, it's not regulatory capture.

51
00:11:03,820 --> 00:11:11,820
Like OpenAI was founded on that premise from day one. So if it was regulatory capture, there's like one hell of a plan.

52
00:11:11,820 --> 00:11:16,820
It's like, oh my God, we're going to create this industry and we're going to start regulatory capturing right now.

53
00:11:16,820 --> 00:11:19,820
Right. It's like, that makes no sense. It was literally the plan from day one.

54
00:11:19,820 --> 00:11:32,820
Yeah, that's all I'm coming from. I'm largely in the EAC camp. I am in team technology, team property, team anti-regulation, but here's something very special and potentially very dangerous.

55
00:11:32,820 --> 00:11:38,820
So let's go back to your use of the phrase a new form of life.

56
00:11:38,820 --> 00:11:47,820
I, as you may recall, am very anti-analogy as a way to understand AI because I think it's so often misleading.

57
00:11:47,820 --> 00:11:56,820
And I often kind of say AI, artificial intelligence, alien intelligence, it may be tempting for people to kind of hear or not tempting,

58
00:11:56,820 --> 00:12:01,820
but it may be sort of natural for people to hear you say a new form of life and understand that as an analogy.

59
00:12:01,820 --> 00:12:12,820
But do you mean it as an analogy or I guess we might start to think about like, is that actually just literally true and what conditions would need to exist for it to be literally true?

60
00:12:12,820 --> 00:12:22,820
And you might think about things like, can AI systems reproduce themselves? Are they subject to the laws of evolution?

61
00:12:22,820 --> 00:12:28,820
But for starters, how literal do you mean it when you say that there's this new form of life in AI?

62
00:12:28,820 --> 00:12:35,820
I mean it's pretty literally, I think if you zoom all the way out literally from the birth of the universe,

63
00:12:35,820 --> 00:12:42,820
the evolution of the universe has been towards greater and greater degrees of self-organization of matter.

64
00:12:42,820 --> 00:12:49,820
And there's actually a case to be made that this is just a natural consequence of the second law of thermodynamics,

65
00:12:49,820 --> 00:12:52,820
this amazing book that Iac people love to quote.

66
00:12:52,820 --> 00:12:54,820
Yeah, I was going to say, you're sounding very Iac all of a sudden.

67
00:12:54,820 --> 00:12:57,820
It's a good point. It's called Every Life is on Fire.

68
00:12:57,820 --> 00:13:04,820
And so if you look at the Big Bang, a few fractions of a second after the Big Bang,

69
00:13:04,820 --> 00:13:08,820
it was just subatomic particles and then they ganged up together and formed atoms.

70
00:13:08,820 --> 00:13:13,820
And then the stage after that was the atoms ganged up together and formed molecules.

71
00:13:13,820 --> 00:13:20,820
And then the stage after that, the molecules became bigger and bigger because the stars exploded and caused all sorts of reactions.

72
00:13:20,820 --> 00:13:26,820
And so a few generations of stars later, we have like pretty big molecules and pretty heavy ones.

73
00:13:26,820 --> 00:13:32,820
And then these molecules formed into sort of like protein and RNA and forms of proto-life.

74
00:13:32,820 --> 00:13:36,820
We don't totally understand, there's a chain here that we don't totally understand,

75
00:13:36,820 --> 00:13:40,820
but there's a form of proto-life that formed and then life.

76
00:13:40,820 --> 00:13:47,820
And so you can think of like, I think it was just a DNA, actually it was RNA, DNA, nucleus of a cell,

77
00:13:47,820 --> 00:13:50,820
mitochondria came into that, and then, okay, good, we have a cell.

78
00:13:50,820 --> 00:13:54,820
And then the cells started ganging up together and now we have multicellular organisms.

79
00:13:54,820 --> 00:13:59,820
And then we have brains at some point, like there's like a big leap, but we have brains,

80
00:13:59,820 --> 00:14:03,820
like on that great march towards greater and greater degrees of step-organization.

81
00:14:03,820 --> 00:14:07,820
And at some point we have us, which with a little bit of hubris perhaps,

82
00:14:07,820 --> 00:14:11,820
considering the apex of that thing for now.

83
00:14:11,820 --> 00:14:15,820
It just seems crazy to me that everybody is saying like, one, this is totally normal.

84
00:14:15,820 --> 00:14:17,820
Oh, this is normal.

85
00:14:17,820 --> 00:14:23,820
This is quintillions of atoms that are organized in this weird, super coherent fashion

86
00:14:23,820 --> 00:14:25,820
that are pursuing a goal in the universe.

87
00:14:25,820 --> 00:14:29,820
Like what's happening right now on Earth is all the weird to begin with.

88
00:14:29,820 --> 00:14:33,820
So people are all deep thinking that this is normal and that's what it is,

89
00:14:33,820 --> 00:14:36,820
and that this march is going to stop at them.

90
00:14:36,820 --> 00:14:39,820
And they're like, well, maybe we're going to get slightly smarter,

91
00:14:39,820 --> 00:14:44,820
or maybe we're going to get augmented and I'm like, you are such a leap compared to an atom,

92
00:14:44,820 --> 00:14:48,820
or compared to a bacteria, that there is no reason to expect that there wouldn't be

93
00:14:48,820 --> 00:14:52,820
another thing above you that is as much more complex or bigger than you,

94
00:14:52,820 --> 00:14:54,820
as the new world to the bacteria.

95
00:14:54,820 --> 00:14:57,820
Like there's nothing in the universe that forbids that from happening.

96
00:14:57,820 --> 00:15:01,820
From a being to exist that is about as big as the planet or the galaxy.

97
00:15:01,820 --> 00:15:04,820
Like there's nothing forbidding that in the universe from happening.

98
00:15:04,820 --> 00:15:08,820
And from the first time now, if you squint, we can sort of see how that happens.

99
00:15:08,820 --> 00:15:14,820
And silicon-based intelligence certainly seems to have a lot of strengths

100
00:15:14,820 --> 00:15:17,820
at its sleeve versus carbon-based intelligence.

101
00:15:17,820 --> 00:15:20,820
And so no, I actually sort of mean that pretty vitrally.

102
00:15:20,820 --> 00:15:24,820
It is sort of in line with the march of the universe and this is the next step,

103
00:15:24,820 --> 00:15:26,820
perhaps it's significant.

104
00:15:26,820 --> 00:15:33,820
And so I am hopeful that we can manage this transition without us being destroyed.

105
00:15:33,820 --> 00:15:35,820
That's what I want to have.

106
00:15:35,820 --> 00:15:41,820
Does that imply an inevitability to advanced AI?

107
00:15:41,820 --> 00:15:46,820
I guess a lot of people out there would say, hey, let's pause it,

108
00:15:46,820 --> 00:15:50,820
slow the whole thing down, and then you get kind of the response from an open AI

109
00:15:50,820 --> 00:15:54,820
where they're sort of saying, yeah, we do take these risks very seriously

110
00:15:54,820 --> 00:15:57,820
and we want to do everything we can to avoid them.

111
00:15:57,820 --> 00:16:00,820
But we can't really pause or we don't think that would be wise

112
00:16:00,820 --> 00:16:03,820
because then the compute overhang is just going to grow

113
00:16:03,820 --> 00:16:07,820
and then things might even be more sudden and disruptive in the future.

114
00:16:07,820 --> 00:16:16,820
Where are you on kind of the inevitability of this increasingly capable AI coming online?

115
00:16:16,820 --> 00:16:19,820
I don't think it's totally inevitable.

116
00:16:19,820 --> 00:16:23,820
I am generally a huge believer in human agency.

117
00:16:23,820 --> 00:16:27,820
I think we can do pretty much anything we set our minds to.

118
00:16:27,820 --> 00:16:31,820
I see a contradiction, by the way, in the EACC argument that like,

119
00:16:31,820 --> 00:16:33,820
on the one hand it's inevitable and try to stop it,

120
00:16:33,820 --> 00:16:36,820
on the other hand, oh my god, if you do this, I'm going to stop it.

121
00:16:36,820 --> 00:16:38,820
It's like, you got to decide here.

122
00:16:38,820 --> 00:16:41,820
So unfortunately, it's not necessarily inevitable.

123
00:16:41,820 --> 00:16:44,820
I am actually worried as much as the next guy, I agree,

124
00:16:44,820 --> 00:16:48,820
there is a risk that we over-regulate and miss out on the upside.

125
00:16:48,820 --> 00:16:50,820
And the upside is significant.

126
00:16:50,820 --> 00:16:53,820
And if you look like during the Middle Ages,

127
00:16:53,820 --> 00:16:56,820
we successfully as a civilization stopped progress

128
00:16:56,820 --> 00:16:59,820
and in a lot of countries, if you look at North Korea, they did it.

129
00:16:59,820 --> 00:17:01,820
They successfully stopped progress.

130
00:17:01,820 --> 00:17:02,820
So you can stop progress.

131
00:17:02,820 --> 00:17:03,820
Progress is not inevitable.

132
00:17:03,820 --> 00:17:05,820
Or maybe it is actually quite fragile.

133
00:17:05,820 --> 00:17:07,820
So no, I don't think it's inevitable.

134
00:17:07,820 --> 00:17:09,820
And I'm hopeful that we can, again,

135
00:17:09,820 --> 00:17:13,820
I want us to get the upside without experiencing the downside.

136
00:17:13,820 --> 00:17:17,820
The North Korea example is an interesting one.

137
00:17:17,820 --> 00:17:20,820
If I was going to kind of dig in there a little bit more, I might say,

138
00:17:20,820 --> 00:17:25,820
okay, I can understand how if things go totally off track,

139
00:17:25,820 --> 00:17:32,820
then we could maybe enter into a low or no or even negative progress trajectory.

140
00:17:32,820 --> 00:17:37,820
If there were a nuclear war, then we may not come back from that for a long time.

141
00:17:37,820 --> 00:17:42,820
Or if whatever, an asteroid hit the earth or a pandemic wiped out 99%,

142
00:17:42,820 --> 00:17:46,820
like there's extreme scenarios where it's pretty intuitive for me

143
00:17:46,820 --> 00:17:52,820
to imagine how progress might stop or just be whatever,

144
00:17:52,820 --> 00:17:54,820
greatly reversed or whatever.

145
00:17:54,820 --> 00:17:59,820
If I'm imagining kind of a continuation-ish of where we are,

146
00:17:59,820 --> 00:18:07,820
then it's harder for me to imagine how we don't kind of keep on this track.

147
00:18:07,820 --> 00:18:11,820
Because it just seems like everything is, we're in this, I would call it,

148
00:18:11,820 --> 00:18:13,820
I don't know if it's going to be a long-term exponential,

149
00:18:13,820 --> 00:18:18,820
but we seem to be entering a steep part of an S-curve where hardware is coming

150
00:18:18,820 --> 00:18:22,820
on lines by the order of magnitude and at the same time,

151
00:18:22,820 --> 00:18:26,820
like algorithmic improvements are taking out a lot of the compute requirements.

152
00:18:26,820 --> 00:18:29,820
And we're just seeing all these existence proofs of what's possible

153
00:18:29,820 --> 00:18:33,820
and all sorts of little clever things and scaffolding along the lines

154
00:18:33,820 --> 00:18:36,820
of some of the stuff that you're building is getting better and better.

155
00:18:36,820 --> 00:18:40,820
Is there a way that we can, do you think it is realistic to think we could

156
00:18:41,820 --> 00:18:48,820
kind of meaningfully pause or even stop without a total derailment of civilization?

157
00:18:48,820 --> 00:18:52,820
The derailment of civilization thing, you could imagine the most extreme scenario

158
00:18:52,820 --> 00:18:55,820
which I am not proposing, but you could imagine the most extreme scenario

159
00:18:55,820 --> 00:18:57,820
which is no more Warsaw.

160
00:18:57,820 --> 00:19:01,820
You do not exponentially improve your semi-conductors anymore.

161
00:19:01,820 --> 00:19:05,820
That'd be crazy, right? But there wouldn't derail civilization.

162
00:19:05,820 --> 00:19:08,820
Civilization is not predicated upon Warsaw.

163
00:19:08,820 --> 00:19:11,820
We would do just fine with the chips we've got today.

164
00:19:11,820 --> 00:19:15,820
And if anything, I think we have a lot of overhang from the chips we have today,

165
00:19:15,820 --> 00:19:17,820
a few to choose overhang, right?

166
00:19:17,820 --> 00:19:22,820
So I actually think it is possible to do that if we wanted to.

167
00:19:22,820 --> 00:19:26,820
And I don't think that even this, which I think is the most extreme scenario,

168
00:19:26,820 --> 00:19:28,820
would actually derail civilization.

169
00:19:28,820 --> 00:19:35,820
Well, we are actually lucky in that there are a few choke points in the industry.

170
00:19:35,820 --> 00:19:40,820
Actually, more than a few. There is ASML, there's TSMC, there's NVIDIA,

171
00:19:40,820 --> 00:19:43,820
like all of those three are individually at our choke point.

172
00:19:43,820 --> 00:19:46,820
Like every regulator could at any point grab one of them and be like,

173
00:19:46,820 --> 00:19:48,820
no more, you just stop, right?

174
00:19:48,820 --> 00:19:51,820
Or you add this chip into all of your GPUs moving forward,

175
00:19:51,820 --> 00:19:53,820
so we have a kill switch. At the very least, we have that.

176
00:19:53,820 --> 00:19:57,820
So if shit really hits the fan, we have an automatic thing in place

177
00:19:57,820 --> 00:20:00,820
that shuts down the very GPU on this, right?

178
00:20:00,820 --> 00:20:04,820
Now that would be disruptive, but potentially less disruptive than the rogue ASI.

179
00:20:04,820 --> 00:20:08,820
So no, I actually think it is very much possible.

180
00:20:08,820 --> 00:20:12,820
This thing's all on the table, and I don't think there would be all that disruptive.

181
00:20:12,820 --> 00:20:17,820
So maybe that's a good transition to kind of where we are right now, right?

182
00:20:17,820 --> 00:20:19,820
We just had this executive order put out this week,

183
00:20:19,820 --> 00:20:24,820
and I think everybody's still kind of absorbing the 100-plus pages

184
00:20:24,820 --> 00:20:26,820
and trying to figure out exactly what it means.

185
00:20:26,820 --> 00:20:28,820
What's your high-level reaction to it?

186
00:20:28,820 --> 00:20:31,820
And then I'll get into some of the specifics.

187
00:20:31,820 --> 00:20:36,820
First of all, it's an executive order for now. It is very early.

188
00:20:36,820 --> 00:20:41,820
Overall, I am pleasantly surprised, not by the specifics,

189
00:20:41,820 --> 00:20:46,820
but by the facts that were reacting quickly,

190
00:20:46,820 --> 00:20:51,820
by the facts that the measures that are proposed are not insane.

191
00:20:51,820 --> 00:20:55,820
Like, I was afraid of, like, there's a really good case to be made.

192
00:20:55,820 --> 00:20:57,820
The second look, we have a different processing case.

193
00:20:57,820 --> 00:21:00,820
Now, a bunch of 70, 80-year-olds go running as they don't know anything

194
00:21:00,820 --> 00:21:02,820
when they were born. There was no mobile phone, right?

195
00:21:02,820 --> 00:21:04,820
Can't really blame them for not really understanding anything.

196
00:21:04,820 --> 00:21:07,820
And so I was afraid that the regulation would go something like,

197
00:21:07,820 --> 00:21:11,820
if you install Microsoft Office in your AI, then you have to make a report.

198
00:21:11,820 --> 00:21:14,820
So the regulation actually sort of makes sense.

199
00:21:14,820 --> 00:21:17,820
It's talking about Flops. It's talking about all those things of training.

200
00:21:17,820 --> 00:21:19,820
So I think it's a step in the right direction.

201
00:21:19,820 --> 00:21:23,820
I'm actually happy about what's happening with this executive order.

202
00:21:23,820 --> 00:21:28,820
Now, the specifics, look, the problem is that it's almost impossible

203
00:21:28,820 --> 00:21:33,820
to regulate AI in a way that doesn't have any loophole.

204
00:21:33,820 --> 00:21:37,820
So they're regulating it according to the new old Flops, and that's okay.

205
00:21:37,820 --> 00:21:39,820
But that's the end of the day, and then you get stuck into,

206
00:21:39,820 --> 00:21:42,820
okay, what happens when you have algorithmic improvements?

207
00:21:42,820 --> 00:21:45,820
What happens when you do URL instead of computing?

208
00:21:45,820 --> 00:21:48,820
And like, that's just a lot of different loopholes that researchers are going to find.

209
00:21:48,820 --> 00:21:51,820
And so I think, overall, it's an encouraging first step.

210
00:21:51,820 --> 00:21:54,820
It's funny. You know, there have been proposals around even, like,

211
00:21:54,820 --> 00:21:57,820
a flop threshold that would drop progressively over time

212
00:21:57,820 --> 00:22:02,820
in kind of anticipation of the algorithmic improvements.

213
00:22:02,820 --> 00:22:08,820
That's even a more probably challenging one to put out into the world,

214
00:22:08,820 --> 00:22:13,820
especially given people are not in general great at extrapolating technology trends

215
00:22:13,820 --> 00:22:19,820
or don't want to accept regulation in advance of stuff actually being invented.

216
00:22:19,820 --> 00:22:24,820
So we've got this flop threshold thing where basically, as I understand it so far,

217
00:22:24,820 --> 00:22:26,820
like, if you're going to do something this big,

218
00:22:26,820 --> 00:22:29,820
you have to tell the government that you're going to do it

219
00:22:29,820 --> 00:22:33,820
and you have to bring your test results to the government.

220
00:22:33,820 --> 00:22:36,820
I would agree with that. That seems like a pretty good start.

221
00:22:36,820 --> 00:22:42,820
And also the threshold seems like pretty reasonably chosen at 10 to the 26.

222
00:22:42,820 --> 00:22:49,820
Any, you know, kind of refinements on that or quibbles that you would put forward

223
00:22:49,820 --> 00:22:53,820
that you think like, you know, maybe the next evolution of this should take into account?

224
00:22:53,820 --> 00:22:57,820
I think ultimately we're tiptoeing around the issue,

225
00:22:57,820 --> 00:23:04,820
but ultimately we need to come to an actual technical blanket solution.

226
00:23:04,820 --> 00:23:12,820
Like, we will not solve ASI alignment by asking for reports from AI companies.

227
00:23:12,820 --> 00:23:14,820
That's not how it's going to happen.

228
00:23:14,820 --> 00:23:16,820
So again, I think it's a step in our direction.

229
00:23:16,820 --> 00:23:19,820
I'm happy with the action. I'm happy the action is not totally nonsensical.

230
00:23:19,820 --> 00:23:24,820
But at the end of the day, we're going to have to talk about the kill switch.

231
00:23:24,820 --> 00:23:29,820
The proposal I just made is one that I see more and more talked about

232
00:23:29,820 --> 00:23:32,820
and that's the one that I would feel best about.

233
00:23:32,820 --> 00:23:36,820
You've got to put this chip into your H100s and the government

234
00:23:36,820 --> 00:23:41,820
and there's like a centralized entity that can shut down all GPUs all at once.

235
00:23:41,820 --> 00:23:43,820
And by the way, it wouldn't necessarily shut down every computer

236
00:23:43,820 --> 00:23:47,820
because your laptop doesn't have an H100, your iPhone doesn't have an H100.

237
00:23:47,820 --> 00:23:48,820
That's fine.

238
00:23:48,820 --> 00:23:51,820
Over the long term, Moore's Law makes it so that your laptop and your phone

239
00:23:51,820 --> 00:23:55,820
actually end up with an H100, but at least that dies us a few years

240
00:23:55,820 --> 00:23:58,820
to make progress on AI safety and alignment.

241
00:23:58,820 --> 00:24:04,820
Ideally, we would then automate just like reportedly the Russians did during the Cold War.

242
00:24:04,820 --> 00:24:05,820
We would automate.

243
00:24:05,820 --> 00:24:10,820
Like, we would set up some detection systems to God knows how we would do that.

244
00:24:10,820 --> 00:24:12,820
But hey, there's no ASI going wrong.

245
00:24:12,820 --> 00:24:15,820
Like, the world is really changing rapidly.

246
00:24:15,820 --> 00:24:20,820
We're assuming it's not too late, which it may be because at that point God knows.

247
00:24:20,820 --> 00:24:24,820
But you could very basically that would give us the best weapon against the ASI.

248
00:24:24,820 --> 00:24:28,820
We would have like a gun against the ASI's hand and kill all the GPUs.

249
00:24:28,820 --> 00:24:30,820
You cannot operate anymore.

250
00:24:30,820 --> 00:24:33,820
God knows how effective that would be because at that point all bets are off.

251
00:24:33,820 --> 00:24:35,820
If you have an ASI God knows what it does and how it connects itself.

252
00:24:35,820 --> 00:24:38,820
But that would be what it would feel best about.

253
00:24:38,820 --> 00:24:42,820
Do you have any sense for how that would be implemented technically?

254
00:24:42,820 --> 00:24:47,820
It seems like you would almost want it to be something that you could kind of broadcast.

255
00:24:47,820 --> 00:24:55,820
You know, you almost want like a receiver on chip that would react to a particular broadcast signal

256
00:24:55,820 --> 00:24:59,820
and just kind of because you would not want to have like, you know, an elaborate chain of command

257
00:24:59,820 --> 00:25:04,820
or, you know, relying on like the dude who happens to be on the night shift at the, you know,

258
00:25:04,820 --> 00:25:08,820
the individual data centers to go through and like, you know, pull some lever, right?

259
00:25:08,820 --> 00:25:11,820
Do you know of anybody who's done kind of advanced thinking on that?

260
00:25:11,820 --> 00:25:14,820
That stuff is like, you know, you hear a lot of these like kill switch things,

261
00:25:14,820 --> 00:25:20,820
but in terms of how that actually happens so that it's not dependent on, you know,

262
00:25:20,820 --> 00:25:25,820
a lot of people coming through in a key moment, I haven't heard too much, to be honest.

263
00:25:25,820 --> 00:25:28,820
No, I haven't seen too much results on that.

264
00:25:28,820 --> 00:25:32,820
But, you know, I think the technical challenge does nothing in principle that makes the technical challenge

265
00:25:32,820 --> 00:25:39,820
unsolvable. Like we've already had a chip that can be broadcasted to for like a dollar from space,

266
00:25:39,820 --> 00:25:43,820
like the GPS chip does a lot of chips and like it has one on your phone.

267
00:25:43,820 --> 00:25:45,820
And so why not put the GPS like chip?

268
00:25:45,820 --> 00:25:47,820
Maybe we could literally piggyback the GPS protocol.

269
00:25:47,820 --> 00:25:51,820
I don't know, but why not put the chip like that in every, in every GPU?

270
00:25:51,820 --> 00:25:58,820
Again, if you have an ASI, God knows, like maybe it hacks the chips before we get a chance,

271
00:25:58,820 --> 00:26:01,820
you know, it hacks the satellites that forecast the thing, I have no idea.

272
00:26:01,820 --> 00:26:08,820
But again, I think pointing in this direction is what I would like things to go into the limit.

273
00:26:08,820 --> 00:26:13,820
I think the basically, and that's like the most extreme version of this proposal,

274
00:26:13,820 --> 00:26:16,820
but like the Yutkovsky airstrike proposal.

275
00:26:16,820 --> 00:26:22,820
That's like, you cannot accumulate billions and billions of dollars of H100s and build this thing.

276
00:26:22,820 --> 00:26:25,820
Else we will go up to airstrike.

277
00:26:25,820 --> 00:26:31,820
That's the most extreme version of this, but that actually I think is directionally correct.

278
00:26:31,820 --> 00:26:37,820
Like we, this is going to be the most powerful force in human history, maybe even in the universe.

279
00:26:37,820 --> 00:26:44,820
You cannot accumulate that stuff anymore than you can accumulate enriched plutonium, right?

280
00:26:44,820 --> 00:26:47,820
We've got a, we've got a four-bit stat that's the lowest level possible.

281
00:26:47,820 --> 00:26:51,820
And so that level cannot be the application layer because the application layer is just,

282
00:26:51,820 --> 00:26:55,820
it's just to diffuse those like a thousand startups everywhere and you get in the garage can build one.

283
00:26:55,820 --> 00:26:59,820
It's got to be at a took point and the took point today is the city code.

284
00:26:59,820 --> 00:27:05,820
Yeah, let's unpack that a little bit more because I think that has been an interesting debate recently.

285
00:27:05,820 --> 00:27:11,820
You'll hear this kind of call for let's not regulate model development.

286
00:27:11,820 --> 00:27:17,820
Let's regulate applications and then, you know, we can kind of have medical regulation for the medical

287
00:27:17,820 --> 00:27:20,820
and everything can be more appropriate and like fit for purpose.

288
00:27:21,820 --> 00:27:26,820
And, you know, maybe there's something else to be said for that.

289
00:27:26,820 --> 00:27:34,820
But yeah, I mean, if you're really worried about tail risk, it's like probably not going to be sort of medical, you know,

290
00:27:34,820 --> 00:27:43,820
device style regulation of, you know, diagnostic models or whatever that is going to keep things under control.

291
00:27:43,820 --> 00:27:50,820
So maybe you could even do a better job of steelmanning the case for the application level regulation.

292
00:27:50,820 --> 00:27:56,820
But I guess, you know, why do you think that give your account of why that's not viable in a little bit more detail?

293
00:27:56,820 --> 00:28:03,820
Yeah, I think the steelman here is like, look, people are going to use forks to poke each other in the eye.

294
00:28:03,820 --> 00:28:05,820
That's not a reason to forbid the fork.

295
00:28:05,820 --> 00:28:06,820
Like forks are awesome.

296
00:28:06,820 --> 00:28:09,820
We love forks just for people from poking each other in the eye with them, right?

297
00:28:09,820 --> 00:28:14,820
The problem is that as the fork in this analogy becomes more and more powerful,

298
00:28:14,820 --> 00:28:20,820
the argument loses more and more of its defense because ultimately it's just a risk benefit analysis.

299
00:28:20,820 --> 00:28:21,820
Right.

300
00:28:21,820 --> 00:28:26,820
And so the risk becomes greater and greater as the artifact becomes more and more powerful.

301
00:28:26,820 --> 00:28:29,820
So more powerful than the fork and al 15.

302
00:28:29,820 --> 00:28:32,820
And so, you know, the opinions vary about that.

303
00:28:32,820 --> 00:28:35,820
But look at at this point, if you look at the data,

304
00:28:35,820 --> 00:28:39,820
you actually save lives by heavily regulating the sale of al 15.

305
00:28:39,820 --> 00:28:43,820
You can't just be like, oh, sell them to everyone and just for big people from shooting each other with them.

306
00:28:43,820 --> 00:28:44,820
It's like, it's an al 15.

307
00:28:44,820 --> 00:28:46,820
What do you expect people to do with them?

308
00:28:46,820 --> 00:28:50,820
Now, in the more in the most extreme scenario, enriched uranium,

309
00:28:50,820 --> 00:28:53,820
you can't be like, you can buy all the enriched uranium you want.

310
00:28:53,820 --> 00:28:55,820
You don't even need to fill up a form, which by the way,

311
00:28:55,820 --> 00:28:58,820
that is all the executive order says right now, at least fill up a form.

312
00:28:58,820 --> 00:29:01,820
Can you please at least tell us what you have to do?

313
00:29:01,820 --> 00:29:04,820
So hey, you can build, you can build all the enriched uranium you want.

314
00:29:04,820 --> 00:29:07,820
Just don't bond us with us with it, please.

315
00:29:07,820 --> 00:29:09,820
Like when we roll it in this disappear, you can do it.

316
00:29:09,820 --> 00:29:11,820
Oh, no, that's not, that's not how it works.

317
00:29:11,820 --> 00:29:16,820
So that, that, that is why I think it's important to regulate the, the silicone layer.

318
00:29:16,820 --> 00:29:23,820
Do you have an intuition for sort of how likely things are to get crazy at kind of either various

319
00:29:23,820 --> 00:29:26,820
time scales or potentially various like compute thresholds?

320
00:29:26,820 --> 00:29:31,820
I was realizing, I did an episode with Jan Tallin a couple of months back,

321
00:29:31,820 --> 00:29:35,820
just in the wake of the GPT-4 deployment.

322
00:29:35,820 --> 00:29:39,820
And he said, we dodged a bullet with GPT-4 or something like that.

323
00:29:39,820 --> 00:29:44,820
Like in his mind, we didn't know if, you know, even at the GPT-4 scale,

324
00:29:44,820 --> 00:29:49,820
like that might have already been, you know, no, no real principled reason to believe that

325
00:29:49,820 --> 00:29:55,820
with any, with like super high confidence that the GPT-4 scale was not going to cross

326
00:29:55,820 --> 00:29:59,820
some, you know, critical threshold or whatever.

327
00:29:59,820 --> 00:30:03,820
I guess I don't really have a great sense for this.

328
00:30:03,820 --> 00:30:08,820
I just kind of feel like, and this was purely like gut level intuition that, yeah,

329
00:30:08,820 --> 00:30:11,820
we could probably do like GPT-5 and it'll probably be fine.

330
00:30:11,820 --> 00:30:13,820
And then kind of beyond that, I'm like, I have no idea.

331
00:30:13,820 --> 00:30:19,820
Do you have anything more specific that you are working with in terms of a framework of like how,

332
00:30:19,820 --> 00:30:24,820
you know, when you hear, for example, Mustafa from inflection say, oh yeah,

333
00:30:24,820 --> 00:30:27,820
we're definitely going to train, you know, orders of magnitude bigger than GPT-4

334
00:30:27,820 --> 00:30:28,820
over the next couple of years.

335
00:30:28,820 --> 00:30:32,820
Are you like, well, as long as you stay to two to three orders of magnitude more,

336
00:30:32,820 --> 00:30:33,820
we'll be okay.

337
00:30:33,820 --> 00:30:37,820
Or like, I just have no, you know, we're just flying so blind,

338
00:30:37,820 --> 00:30:40,820
but I wonder if maybe you're flying slightly less blind than I am.

339
00:30:40,820 --> 00:30:46,820
I am of the opinion that GPT-4 is the most critical component for AGI.

340
00:30:46,820 --> 00:30:51,820
And that's the gap from GPT-4 to proper AGI is not research, it's engineering.

341
00:30:51,820 --> 00:30:54,820
It sits outside the model.

342
00:30:54,820 --> 00:31:00,820
So I think we have a capabilities overhang here that can turn GPT-4, as it is today,

343
00:31:00,820 --> 00:31:04,820
into AGI, into proper AGI.

344
00:31:04,820 --> 00:31:07,820
I think generally that's the case for any technology.

345
00:31:07,820 --> 00:31:12,820
If you look, for example, at Bitcoin, what changed from a technological standpoint

346
00:31:12,820 --> 00:31:14,820
that allowed Bitcoin to happen?

347
00:31:14,820 --> 00:31:17,820
It was the same technology we'd had for a while and yet Bitcoin,

348
00:31:17,820 --> 00:31:19,820
it's going to go wild to happen.

349
00:31:19,820 --> 00:31:24,820
So there was this overhang and Bitcoin, whatever your opinion about crypto,

350
00:31:24,820 --> 00:31:27,820
changed a lot of games, right?

351
00:31:27,820 --> 00:31:29,820
I think there's this huge overhang with GPT-4.

352
00:31:29,820 --> 00:31:33,820
I think we basically have a reasoning module of AGI.

353
00:31:33,820 --> 00:31:37,820
I don't know if you saw this paper that found literally just asking it,

354
00:31:37,820 --> 00:31:39,820
hey, take a deep breath and take a step back.

355
00:31:39,820 --> 00:31:41,820
Just take a step back apparently also makes a huge difference.

356
00:31:41,820 --> 00:31:44,820
So I think there's a lot of tricks like that that will make a difference.

357
00:31:44,820 --> 00:31:49,820
The sort of cognitive architectural layers around GPT-4 I think can bring it to AGI.

358
00:31:49,820 --> 00:31:54,820
That is also why you asked me about what sort of regulation I wish was put into place.

359
00:31:54,820 --> 00:31:56,820
We need to stop open sourcing this model.

360
00:31:56,820 --> 00:31:58,820
We don't know what kind of overhang exists out there.

361
00:31:58,820 --> 00:32:02,820
I don't think Lama-2 is there, but like I said, I think GPT-4 is there.

362
00:32:02,820 --> 00:32:05,820
So Lama-3, if it's GPT-4 level, boom, it's too late.

363
00:32:05,820 --> 00:32:06,820
The weights are out there.

364
00:32:06,820 --> 00:32:10,820
Okay, now you can do, maybe you can put strap on there.

365
00:32:10,820 --> 00:32:13,820
So we need to stop open sourcing this next.

366
00:32:13,820 --> 00:32:19,820
I expect my timelines for proper AGI to emerge is two to eight years.

367
00:32:19,820 --> 00:32:24,820
I think there's a more than even chance of AGI emerging in two to eight years.

368
00:32:24,820 --> 00:32:27,820
I think the base scenario is things are going to go well just for the record.

369
00:32:27,820 --> 00:32:29,820
I don't think there's like a 99% chance of doom.

370
00:32:29,820 --> 00:32:33,820
But even if it's 10%, I think it's worth being very, very worried about.

371
00:32:33,820 --> 00:32:34,820
That's enough for me.

372
00:32:34,820 --> 00:32:39,820
10% of all of us dying like I'm talking about it, please.

373
00:32:39,820 --> 00:32:45,820
So two to eight years, 50% chance of AGI, things probably will go well,

374
00:32:45,820 --> 00:32:48,820
except for, you know, CVD digital disruption.

375
00:32:48,820 --> 00:32:49,820
There's going to be like stuff.

376
00:32:49,820 --> 00:32:52,820
There's going to be a crazy shit happening, but two to eight years.

377
00:32:52,820 --> 00:32:53,820
And after that, all bets off.

378
00:32:53,820 --> 00:32:57,820
I have no idea what the bootstrapping to ASI look like,

379
00:32:57,820 --> 00:33:01,820
but I don't expect ASI to take more than 30 years.

380
00:33:01,820 --> 00:33:04,820
So I expect that you and I in our lifetimes are going to see ASI.

381
00:33:04,820 --> 00:33:06,820
So that's a pretty striking claim.

382
00:33:06,820 --> 00:33:09,820
I think you probably puts you in a pretty small minority.

383
00:33:09,820 --> 00:33:15,820
And I don't think I'm really there with you when you say that you think GPT-4

384
00:33:15,820 --> 00:33:22,820
kind of already contains the, you know, the kind of necessary core element for an AGI.

385
00:33:22,820 --> 00:33:24,820
So I'd like to understand that a little bit better.

386
00:33:24,820 --> 00:33:31,820
I mean, you'll have a lot of people who will say, you know, look, it can't play tic-tac-toe.

387
00:33:31,820 --> 00:33:39,820
I think on some level, those kind of, oh, look at these like simple failure objections are kind of lame

388
00:33:39,820 --> 00:33:43,820
and sort of miss the point because of all things obviously can do.

389
00:33:43,820 --> 00:33:47,820
But I do, you know, if I'm thinking like, does this system seem like it has this kind of sufficiently

390
00:33:47,820 --> 00:33:49,820
well-developed world model?

391
00:33:49,820 --> 00:33:54,820
Or, you know, I'm not even sure exactly how you're conceiving of the core thing.

392
00:33:54,820 --> 00:34:00,820
But, you know, for a question like that, I would say those failures maybe are kind of illuminating.

393
00:34:00,820 --> 00:34:08,820
On the other hand, I'm sure you've seen this Eureka paper out of NVIDIA recently where they used GPT-4

394
00:34:08,820 --> 00:34:14,820
as a superhuman reward model author to teach robot hands to do stuff.

395
00:34:14,820 --> 00:34:20,820
And I thought that one was pretty striking because as far as I know, and I actually used the term Eureka moment,

396
00:34:20,820 --> 00:34:29,820
many times said, we don't see yet Eureka moments coming from highly general systems.

397
00:34:29,820 --> 00:34:32,820
You know, we see Eureka moments from like an alpha go.

398
00:34:32,820 --> 00:34:36,820
We haven't really seen like Eureka moments from a GPT-4 until maybe this.

399
00:34:36,820 --> 00:34:43,820
This seems like maybe one of the first things where it's like, wow, GPT-4 at a task that requires a lot of expertise.

400
00:34:43,820 --> 00:34:48,820
That is designing reward functions for robot learning, robot reinforcement learning.

401
00:34:48,820 --> 00:34:53,820
GPT-4 is meaningfully outperforming human experts.

402
00:34:53,820 --> 00:34:56,820
And so I think it's very appropriate that they call it Eureka.

403
00:34:56,820 --> 00:34:58,820
What do you think is the core thing?

404
00:34:58,820 --> 00:35:00,820
You know, is it this like ability to have Eureka moments?

405
00:35:00,820 --> 00:35:01,820
Is it something else?

406
00:35:01,820 --> 00:35:03,820
Why do you feel like it's there?

407
00:35:03,820 --> 00:35:06,820
And does it not trouble you that it can't play tic-tac-toe?

408
00:35:06,820 --> 00:35:15,820
For the sake of this conversation, I'm going to define a GI as a seed AI, an AI that can recursively self-improve.

409
00:35:15,820 --> 00:35:20,820
That's a much more narrow definition of a GI than most people use, but that's actually what I care about.

410
00:35:20,820 --> 00:35:25,820
Can we enter this recursive loop of self-improvement that puts track surface to ASI?

411
00:35:25,820 --> 00:35:27,820
In order to get there, you don't need to play tic-tac-toe.

412
00:35:27,820 --> 00:35:33,820
You need to be good enough, and the world good enough here is important,

413
00:35:33,820 --> 00:35:40,820
a good enough either software engineer or chip designer or AI and ML researcher.

414
00:35:40,820 --> 00:35:41,820
One of these things.

415
00:35:41,820 --> 00:35:44,820
So something that can get you to put track.

416
00:35:44,820 --> 00:35:47,820
And so good enough does not mean better than the best human.

417
00:35:47,820 --> 00:35:50,820
It doesn't even mean better than the average human.

418
00:35:50,820 --> 00:35:57,820
It just means good enough that you can make a difference, a positive difference in your own ability to get better.

419
00:35:57,820 --> 00:35:58,820
Right?

420
00:35:58,820 --> 00:36:02,820
So if you enter the recursive loop of self-improvement, then mathematically it's over.

421
00:36:02,820 --> 00:36:05,820
And yeah, when I see the NVIDIA paper, I see that.

422
00:36:05,820 --> 00:36:08,820
When I see our own experience with the model.

423
00:36:08,820 --> 00:36:11,820
So today we are using Lindy to write her own integrations,

424
00:36:11,820 --> 00:36:14,820
and Lindy is writing more and more of her own code.

425
00:36:14,820 --> 00:36:15,820
I see that.

426
00:36:15,820 --> 00:36:20,820
Even as it belongs to AI researchers and ML researchers,

427
00:36:20,820 --> 00:36:27,820
my hypothesis is that OpenAI is using GPT-4 more and more internally to perform AI research.

428
00:36:27,820 --> 00:36:32,820
My not hypothesis is the fact is that NVIDIA is releasing papers that's like,

429
00:36:32,820 --> 00:36:35,820
well, not only can we use it for AI research through this Eureka paper,

430
00:36:35,820 --> 00:36:37,820
but we can also use it for chip design.

431
00:36:37,820 --> 00:36:38,820
It works super well.

432
00:36:38,820 --> 00:36:41,820
We trained an AI model that does chip design super well.

433
00:36:41,820 --> 00:36:46,820
So we are starting to see the glimpses of that kind of recursive loop of self-improvement.

434
00:36:46,820 --> 00:36:51,820
Basically, the world model question kind of on the sidestep,

435
00:36:51,820 --> 00:36:55,820
because I feel like at this point the debate has become silly for people who argue that it's bad

436
00:36:55,820 --> 00:36:57,820
or doesn't have a world model.

437
00:36:57,820 --> 00:36:59,820
What matters is, is it good enough?

438
00:36:59,820 --> 00:37:03,820
And so even if it just overfits its training set,

439
00:37:03,820 --> 00:37:06,820
even if it's just predicting the next token and not actually understanding anything,

440
00:37:06,820 --> 00:37:09,820
I actually really do believe it understands a lot.

441
00:37:09,820 --> 00:37:13,820
But even if it's not, you can imagine it does this many-dimensional space

442
00:37:13,820 --> 00:37:15,820
with a ton of data points in there.

443
00:37:15,820 --> 00:37:17,820
And it's good by interpolating between the data points

444
00:37:17,820 --> 00:37:21,820
and it needs much more data points to understand anything than a human.

445
00:37:21,820 --> 00:37:26,820
And so there's that envelope in that space where the data points are dense enough

446
00:37:26,820 --> 00:37:28,820
that it can perform.

447
00:37:28,820 --> 00:37:30,820
And so that's called the convex hole.

448
00:37:30,820 --> 00:37:32,820
And then there's data points outside that convex hole,

449
00:37:32,820 --> 00:37:35,820
and it does really poorly outside the convex hole, much more poorly than humans.

450
00:37:35,820 --> 00:37:39,820
It's convex hole requires a lot more density than humans do exist.

451
00:37:39,820 --> 00:37:44,820
There's multiple questions, which are, one, all these data points inside,

452
00:37:44,820 --> 00:37:47,820
the convex hole is the sum of all human knowledge.

453
00:37:47,820 --> 00:37:49,820
GP for today knows more than you.

454
00:37:49,820 --> 00:37:51,820
I don't know that it can reason better than you,

455
00:37:51,820 --> 00:37:53,820
that's the expanding the convex hole thing,

456
00:37:53,820 --> 00:37:55,820
but it knows more than you inside that convex hole.

457
00:37:55,820 --> 00:37:59,820
And so inside that convex hole, an AI researcher that's read every paper ever,

458
00:37:59,820 --> 00:38:01,820
not just in AI, but in mass and biology,

459
00:38:01,820 --> 00:38:03,820
every paper ever finds the entirety of the internet,

460
00:38:03,820 --> 00:38:07,820
is it better than a human AI researcher?

461
00:38:07,820 --> 00:38:10,820
I think the answer is yes.

462
00:38:10,820 --> 00:38:13,820
Even if it's not better, there's the outside of that convex hole,

463
00:38:13,820 --> 00:38:16,820
and this is my point about the capabilities of a hang,

464
00:38:16,820 --> 00:38:21,820
can we get this AI model through prompting, through cognitive architecture,

465
00:38:21,820 --> 00:38:23,820
to do better outside its convex hole?

466
00:38:23,820 --> 00:38:25,820
And we'll see that all the time,

467
00:38:25,820 --> 00:38:27,820
seeing papers come out to bed like,

468
00:38:27,820 --> 00:38:30,820
hey, we have found an automatic way to rewrite a prompt that makes it a lot better.

469
00:38:30,820 --> 00:38:34,820
We have found a way that people that came out a few days ago,

470
00:38:34,820 --> 00:38:38,820
that's like, hey, if you ask the model to take a step back

471
00:38:38,820 --> 00:38:42,820
and to rephrase the problem you're giving it in terms of a universal problem,

472
00:38:42,820 --> 00:38:44,820
it performs a lot better.

473
00:38:44,820 --> 00:38:46,820
And that makes total sense,

474
00:38:46,820 --> 00:38:52,820
because the specific of the problem is probably not seen as that specific problem in its dataset,

475
00:38:52,820 --> 00:38:56,820
but if you ask it to reframe it, it's basically translating the problem into a form

476
00:38:56,820 --> 00:38:58,820
in which it's comfortable with.

477
00:38:58,820 --> 00:39:02,820
And so we're actually getting it to grow its convex hole like that.

478
00:39:02,820 --> 00:39:07,820
That's my take, is I think the convex hole is good enough to get to that good enough point,

479
00:39:07,820 --> 00:39:09,820
and I think we can grow that convex hole.

480
00:39:09,820 --> 00:39:14,820
And so I think that basically, if GPT-4 isn't a CDI, it will still GPT-5 is one.

481
00:39:14,820 --> 00:39:16,820
Yeah, that's an interesting framing.

482
00:39:16,820 --> 00:39:18,820
I find your analysis there pretty compelling.

483
00:39:18,820 --> 00:39:25,820
The idea that, you know, given what we have seen from like a Eureka,

484
00:39:25,820 --> 00:39:30,820
you know, with this robot training, or there was another interesting one recently,

485
00:39:30,820 --> 00:39:34,820
I think it was out of Microsoft, I covered this in one of the research rundown episodes,

486
00:39:34,820 --> 00:39:40,820
on recursive or iterative improvement on a software improver.

487
00:39:40,820 --> 00:39:43,820
So they basically take a real simple software improver, you know,

488
00:39:43,820 --> 00:39:47,820
that can improve a piece of software, and then they feed that software improver to itself

489
00:39:47,820 --> 00:39:50,820
and just run that on itself over and over again.

490
00:39:50,820 --> 00:39:55,820
You know, it kind of tops out because it's not, it doesn't, you know,

491
00:39:55,820 --> 00:39:58,820
in this framework, it doesn't have access to like tinkering with, you know,

492
00:39:58,820 --> 00:40:04,820
possible methods for training itself, but it makes significant improvement

493
00:40:04,820 --> 00:40:09,820
and gets us some pretty advanced algorithms where it starts to do like genetic search

494
00:40:09,820 --> 00:40:11,820
and, you know, a variety of things where I'm like,

495
00:40:11,820 --> 00:40:14,820
I don't even really know what that is, you know, like simulated annealing algorithms.

496
00:40:14,820 --> 00:40:18,820
I'm like, what, you know, but it comes up with that and, you know,

497
00:40:18,820 --> 00:40:20,820
uses that to improve the improver.

498
00:40:20,820 --> 00:40:24,820
And, you know, this is all measured by how effectively it can do the downstream task.

499
00:40:24,820 --> 00:40:30,820
It does seem like it's not a huge stretch to say that, you know,

500
00:40:30,820 --> 00:40:35,820
could you take the architecture of GPT-4 and start to do, you know,

501
00:40:35,820 --> 00:40:42,820
parameter sweeps and start to, you know, mutate the architecture itself.

502
00:40:42,820 --> 00:40:44,820
It seems like it probably can do that.

503
00:40:44,820 --> 00:40:46,820
And I would agree, you know, it probably does.

504
00:40:46,820 --> 00:40:50,820
Yeah, certainly just based on what I do, you know, with GPT-4 for coding,

505
00:40:50,820 --> 00:40:54,820
I would have to imagine that it is in heavy use as they're, you know,

506
00:40:54,820 --> 00:40:59,820
performing all that kind of exploratory work, you know, within an open AI.

507
00:40:59,820 --> 00:41:02,820
And so, yeah, and I think to your point,

508
00:41:02,820 --> 00:41:06,820
we all see enough of these signs of life across the board in a lot of different areas.

509
00:41:06,820 --> 00:41:09,820
A lot of institutions are like, ah, a little bit of very good stuff

510
00:41:09,820 --> 00:41:11,820
from here, a little bit here, a little bit here.

511
00:41:11,820 --> 00:41:14,820
It's not very hard to imagine it getting to a state velocity,

512
00:41:14,820 --> 00:41:17,820
to imagine it going super critical and pass a certain threshold where I say,

513
00:41:17,820 --> 00:41:19,820
okay, now boom, it can ready take off.

514
00:41:19,820 --> 00:41:24,820
So, and I've actually heard multiple people from open AI say that they believe,

515
00:41:24,820 --> 00:41:27,820
and I agree with their conclusion.

516
00:41:27,820 --> 00:41:29,820
And they actually told me that before I agreed with them,

517
00:41:29,820 --> 00:41:32,820
they told me that at the very beginning of the year,

518
00:41:32,820 --> 00:41:34,820
so before GPT-4 was widely available.

519
00:41:34,820 --> 00:41:37,820
And they told me, you know, I think we're like, you know,

520
00:41:37,820 --> 00:41:40,820
we have a GEI and we're in a slow take off.

521
00:41:40,820 --> 00:41:42,820
And I felt something like this, crazy.

522
00:41:42,820 --> 00:41:48,820
Well, they didn't say, sorry, they basically were talking about GPT-4.

523
00:41:48,820 --> 00:41:54,820
I think, and I am not representing that this is the universal position of open AI,

524
00:41:54,820 --> 00:41:57,820
but I've heard multiple people from open AI and other labs tell me that.

525
00:41:57,820 --> 00:42:00,820
We have a GEI and we're in a slow take off.

526
00:42:00,820 --> 00:42:04,820
So, given that, okay, we've got this compute threshold.

527
00:42:04,820 --> 00:42:07,820
We maybe need a kill switch.

528
00:42:07,820 --> 00:42:11,820
Now I'm getting, we started this conversation with me, with my, you know,

529
00:42:11,820 --> 00:42:16,820
IAC side coming out and, you know, being like,

530
00:42:16,820 --> 00:42:20,820
why can't we get myself driving car on the road and tolerate,

531
00:42:20,820 --> 00:42:23,820
you know, some reasonable amount of risk to do that.

532
00:42:23,820 --> 00:42:26,820
Now my other side is coming out and I'm like,

533
00:42:26,820 --> 00:42:28,820
okay, what else might we do, right?

534
00:42:28,820 --> 00:42:31,820
We've got the AI safety summit going on right now in the UK.

535
00:42:31,820 --> 00:42:35,820
I thought it was cool to see today that there's some kind of joint statements

536
00:42:35,820 --> 00:42:39,820
between Chinese and Western academics and, you know,

537
00:42:39,820 --> 00:42:41,820
thought leaders in the space where they're kind of saying,

538
00:42:41,820 --> 00:42:43,820
yeah, we need to work together on this.

539
00:42:43,820 --> 00:42:48,820
Human extinction is something that we think could happen if we're not careful.

540
00:42:48,820 --> 00:42:52,820
Do you have a point of view on kind of collaborating with China

541
00:42:52,820 --> 00:42:55,820
or coordinating with China?

542
00:42:55,820 --> 00:42:57,820
I mean, that's a tough question, obviously.

543
00:42:57,820 --> 00:42:59,820
Nobody really knows China.

544
00:42:59,820 --> 00:43:01,820
I don't think super well, but what do you think about that?

545
00:43:01,820 --> 00:43:04,820
I mean, are we naive to hope?

546
00:43:04,820 --> 00:43:06,820
I kind of feel like what else are we going to do except give it a shot?

547
00:43:06,820 --> 00:43:08,820
Yeah, 100%.

548
00:43:08,820 --> 00:43:10,820
And there is ample precedent.

549
00:43:10,820 --> 00:43:14,820
You know, everybody is always talking about these coordination problems.

550
00:43:14,820 --> 00:43:17,820
They've taken like the one-on-one course of game theory and they're like,

551
00:43:17,820 --> 00:43:18,820
look, we can't coordinate.

552
00:43:18,820 --> 00:43:23,820
Well, like if you take game theory one or two, it's like solutions to the coordination problem, right?

553
00:43:23,820 --> 00:43:29,820
And so the solution to the collaboration problem is few players in a very iterated game.

554
00:43:29,820 --> 00:43:31,820
And that is the game right now.

555
00:43:31,820 --> 00:43:34,820
There's very few players and they're all in a very iterated game.

556
00:43:34,820 --> 00:43:37,820
They're not the best buddies, but they are actually able to agree on a lot of things.

557
00:43:37,820 --> 00:43:40,820
And so we can coordinate with China.

558
00:43:40,820 --> 00:43:43,820
And again, to your point, what choice do we have anyway, right?

559
00:43:43,820 --> 00:43:47,820
And even if we do not coordinate with them, again, there's enough truth holds

560
00:43:47,820 --> 00:43:50,820
enough of which are American, right?

561
00:43:50,820 --> 00:43:52,820
NVDI is an American company, last time I checked.

562
00:43:52,820 --> 00:43:56,820
And so there's enough truth holds that we could actually do very much

563
00:43:56,820 --> 00:44:00,820
not give them a choice like, hey, your GPUs now have the chip right here.

564
00:44:00,820 --> 00:44:03,820
And so whether you like it or not, we have a satellite up here

565
00:44:03,820 --> 00:44:06,820
and we can tell the GPUs out there.

566
00:44:06,820 --> 00:44:11,820
And that wouldn't be, we could even just downright for big GPUs,

567
00:44:11,820 --> 00:44:12,820
by the way, to be sold in China.

568
00:44:12,820 --> 00:44:14,820
Like we've done stuff like that before.

569
00:44:14,820 --> 00:44:18,820
So no, I think coordination is definitely possible

570
00:44:18,820 --> 00:44:20,820
and I actually think it's going to happen.

571
00:44:20,820 --> 00:44:24,820
I'm actually really very much encouraged by, well, winning.

572
00:44:24,820 --> 00:44:27,820
Like I think the safety side is making really good progress.

573
00:44:27,820 --> 00:44:29,820
There is rising public awareness.

574
00:44:29,820 --> 00:44:31,820
I think Duffington is doing an amazing work here.

575
00:44:31,820 --> 00:44:32,820
The regulation is coming.

576
00:44:32,820 --> 00:44:34,820
It's mostly sensical.

577
00:44:34,820 --> 00:44:38,820
There's this sort of progress that's happening across the board.

578
00:44:38,820 --> 00:44:41,820
AI labs are investing more and more in safety and alignment.

579
00:44:41,820 --> 00:44:44,820
Even from a technical standpoint, the work that AnsibleTik is doing,

580
00:44:44,820 --> 00:44:46,820
I think is absolutely brilliant.

581
00:44:46,820 --> 00:44:49,820
So we're making really good progress across the board here.

582
00:44:49,820 --> 00:44:52,820
I don't want to represent that it will be on the board.

583
00:44:52,820 --> 00:44:53,820
Yeah, I totally agree.

584
00:44:53,820 --> 00:44:57,820
I would say my kind of high level narrative on this recently has been,

585
00:44:57,820 --> 00:45:01,820
it feels like we're at the beginning of chapter two of the overall AI story.

586
00:45:01,820 --> 00:45:04,820
And chapter one was largely, you know,

587
00:45:04,820 --> 00:45:08,820
characterized by a lot of speculation about what might happen.

588
00:45:08,820 --> 00:45:11,820
And amazingly, kind of at the end of chapter one,

589
00:45:11,820 --> 00:45:13,820
beginning of chapter two, not all,

590
00:45:13,820 --> 00:45:18,820
but like a large chair of the key players seem to be really serious minded

591
00:45:18,820 --> 00:45:21,820
and, you know, well aware of the risks.

592
00:45:21,820 --> 00:45:25,820
And it's easy to imagine for me a very different scenario where everybody,

593
00:45:25,820 --> 00:45:29,820
you know, all the leading developers are like highly dismissive of the potential problems.

594
00:45:29,820 --> 00:45:33,820
But it's hard for me to imagine a scenario that would be like all that much better

595
00:45:33,820 --> 00:45:37,820
than, you know, the current dynamic.

596
00:45:37,820 --> 00:45:43,820
So I do feel, you know, like overall, you know, pretty, pretty lucky

597
00:45:43,820 --> 00:45:47,820
or pretty grateful that, you know, things are shaping up at least, you know,

598
00:45:47,820 --> 00:45:51,820
to give us a good chance to try to get a handle on all this sort of stuff.

599
00:45:51,820 --> 00:45:52,820
One last question.

600
00:45:52,820 --> 00:45:53,820
This is super philosophical.

601
00:45:53,820 --> 00:46:01,820
I know you got to go, but how much depends in your mind on whether or not,

602
00:46:01,820 --> 00:46:06,820
let's say Silicon base intelligence or AI systems or whatever might become

603
00:46:06,820 --> 00:46:10,820
or maybe already are, you know, I'm not sure how we would ever tell the kinds of things

604
00:46:10,820 --> 00:46:12,820
that have subjective experience.

605
00:46:12,820 --> 00:46:17,820
You know, does it matter to you if it feels like something to be GPT for?

606
00:46:17,820 --> 00:46:20,820
Have you heard of the world move?

607
00:46:20,820 --> 00:46:24,820
I think it's Zen philosophy in Buddhism.

608
00:46:24,820 --> 00:46:29,820
There's this story that's like someone asks someone else like, hey, does kind of Doug

609
00:46:29,820 --> 00:46:31,820
have the essence of a Buddha?

610
00:46:31,820 --> 00:46:35,820
If the Buddha is everywhere and he never being kind of Doug have the essence of a Buddha.

611
00:46:35,820 --> 00:46:38,820
And the answer to that is move.

612
00:46:38,820 --> 00:46:41,820
And move means neither yes or no.

613
00:46:41,820 --> 00:46:43,820
It's a way to unask the question.

614
00:46:43,820 --> 00:46:46,820
It's a way to reject the premise of the question.

615
00:46:47,820 --> 00:46:53,820
And basically in this sense, it means there is no such thing as the essence of the Buddha.

616
00:46:53,820 --> 00:46:57,820
It's like the same question is like, hey, what happened before the universe existed?

617
00:46:57,820 --> 00:46:58,820
Move.

618
00:46:58,820 --> 00:47:01,820
There was no before because the bills of the universe was the bills of time.

619
00:47:01,820 --> 00:47:04,820
So the will to be full only makes a sense in the context of the universe.

620
00:47:04,820 --> 00:47:06,820
And so anyway, that's all of my insight.

621
00:47:06,820 --> 00:47:10,820
Whenever I ask a question, whenever someone asks me questions about subjective experience

622
00:47:10,820 --> 00:47:12,820
and consciousness, I'm like move.

623
00:47:12,820 --> 00:47:13,820
It doesn't exist.

624
00:47:13,820 --> 00:47:14,820
It doesn't matter.

625
00:47:14,820 --> 00:47:15,820
It's immeasurable.

626
00:47:15,820 --> 00:47:17,820
It's not a scientific thing.

627
00:47:17,820 --> 00:47:19,820
And so move.

628
00:47:19,820 --> 00:47:20,820
All right.

629
00:47:20,820 --> 00:47:24,820
Well, some questions bound to remain unanswered.

630
00:47:24,820 --> 00:47:26,820
And I appreciate your time today.

631
00:47:26,820 --> 00:47:28,820
This is always super lively.

632
00:47:28,820 --> 00:47:30,820
Next time I want to get the Lindy update.

633
00:47:30,820 --> 00:47:32,820
And at some point I want to get access.

634
00:47:32,820 --> 00:47:34,820
But for now, I'll just say Fluckravello.

635
00:47:34,820 --> 00:47:36,820
Thank you for being part of the Cognitive Revolution.

636
00:47:36,820 --> 00:47:38,820
Thanks, Mason.

