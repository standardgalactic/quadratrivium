start	end	text
0	3640	Welcome to the Future of Life Institute podcast.
3640	6760	I'm Gus Docher and I'm here with Roman Jampolsky.
6760	11000	Roman is a computer scientist from the University of Louisville.
11000	12840	Roman, welcome to the podcast.
12840	15000	Thanks for inviting me. It's good to be back.
15000	18800	I think it's my third time on a FLI podcast, if I'm not mistaken.
18800	24800	Great. You have this survey paper of objections to AI safety research,
24800	26640	and I find this very interesting.
26640	31800	I feel like this is a good way to spend your time to collect all of these objections
31800	35040	and see if they have any merit and consider them.
35040	38080	And so I think we should dive into it.
38080	42080	One objection you raise under the technical objections
42080	45480	is that AI, in a sense, doesn't exist.
45480	48480	If we call it something else, it sounds less scary.
48480	50280	Perhaps you could unpack that a bit.
50280	56240	So those are objections from people who think there is no AI risk or risk is not real.
56240	60680	That's not my objections to technical work or safety work.
60680	66800	We try to do a very comprehensive survey, so even silly ones are included.
66800	75600	And people do try to explain that artificial intelligence is a scary sounding scientific term,
75600	80280	but if you just call it matrix multiplication, then of course it's not scary at all.
80280	83600	It's just statistics and we have nothing to worry about.
83600	87040	So it seems they're trying to kind of shift the narrative
87040	92960	by using this approach of getting away from agent hood
92960	96960	and kind of built-in scenarios people have for AI,
96960	102760	to something no one is scared of, calculators, addition, algebra.
102760	106680	Perhaps there is a way to frame this objection where it makes a bit of sense
106680	111360	in that people are quite sensitive to how you frame risks.
111360	114680	People are quite sensitive to certain words in particular.
114680	120240	So do you see that perhaps people who are in favor of AI safety research
120240	125920	by calling AI something that sounds scary might be, in a sense, inflating the risks?
125920	129320	Well, it's definitely a tool people use to manipulate any debate.
129320	132840	I mean, whatever you're talking about abortion or anything else,
132840	135600	it's like, are you killing babies or you're making a choice?
135600	139480	Of course, language can be used to manipulate,
139480	143960	but it helps to look for capability equivalences.
143960	149560	Are we creating God-like machines or is it just a table in a database?
149560	152560	So that would make a difference in how you perceive it.
152560	155920	And perhaps we should just simply be willing to accept that, yes,
155920	162440	what we are afraid of, in a sense, is matrix multiplication or data processing
162440	168400	or whatever you want to call it, because these things might still have scary properties.
168400	169640	Whatever we call them.
169640	175800	Right, so we can argue that humans are just stakes, pieces of meat with electricity in them
175800	180160	and it doesn't sound so bad until you realize we can create nuclear weapons.
180160	184200	So it's all about perception and what you're hoping to accomplish.
184200	188840	All right, there is also an objection going along the lines
188840	191560	that superintelligence is impossible.
191560	194360	What's the strongest form of this objection?
194360	199680	So essentially, the argument goes that there are some upper limits on capability.
199680	202200	Maybe they are based on laws of physics.
202200	206200	You just cannot in this universe have anything greater than a human brain.
206200	211960	Just for some reason, that's the ultimate endpoint and that's why evolution stopped there
211960	217160	and somehow we magically ended up being at the very top of a food chain.
217160	222640	There could be other arguments about, okay, maybe it's not absolute theoretical limit,
222640	226520	but in practical terms, without quantum computers, we'll never get there.
226520	232080	You can have many flavors of this, but the idea is that we're just never going to be outcompeted.
232080	235360	And this doesn't strike me as particularly plausible.
235360	239600	We could imagine humans simply with physically bigger brains.
239600	244440	So the version where humans are at the absolute limit of intelligence doesn't sound plausible,
244440	250800	but is there some story in which physics puts limits on intelligence?
250800	255880	There could be a very, very high upper limit to which we are nowhere close,
255880	260240	but if you think about the size of a possible brain, Jupiter-sized brains,
260240	265320	at some point the density will collapse into some black hole singularity.
265320	268440	But this is not something we need to worry about just yet,
268440	274560	not smart enough superintelligence is where nowhere near the size of capability.
274560	279080	And from our point of view, we won't be able to tell the difference system with,
279160	284600	I mean, hypothetically IQ of a million versus IQ of a billion will look very similar to us.
284600	294200	Yeah, and perhaps a related worry is that stories of self-improving AIs are wrong, in a sense.
294200	300760	So it's definitely easy to make such claims because we don't have good examples of software doing it more than once.
300760	306160	So you have compilers which go through code, optimize it, but they don't continuously self-optimize.
306200	312160	But it's not impossible to see if you automate science and engineering,
312160	316560	then scientists and engineers will look at their own code and continue this process.
316560	318960	So it seems quite reasonable.
318960	322080	There could be strong diminishing returns on that,
322080	326440	but you have to consider other options for becoming smarter.
326440	328400	It's not just improving the algorithm.
328400	331960	You can have faster hardware, you can have more memory,
331960	334800	you can have more processes running in parallel.
334800	338600	There are different types of how you get to superintelligent performance.
338600	343560	And you could, of course, have AI held along the way there with development of hardware
343560	348880	or discovery of new hardware techniques as well as new algorithmic techniques and so on.
348880	350360	That's exactly the point, right?
350360	355840	So you'll get better at getting better and this process will accelerate until you can't keep up with it.
355840	363800	How do you feel about tools such as Copilot, which is a tool that programmers can use for auto-completing their code?
363800	369040	Is this a form of proto-self-improvement or would that be stretching the term?
369040	372920	Well, eventually, when it's good enough to be an independent programmer, it would be good.
372920	377240	But I'm very concerned with such systems because from what I understand,
377240	382840	the bugs they would introduce would be very different from typical bugs human programmers will introduce.
382840	385800	So debugging would be even harder from our point of view,
385800	393600	monitoring it, making sure that there is not this inheritance of calls to a buggy first version.
393600	400720	So yeah, long term, I don't think it's a very good thing for us that we no longer can keep up with the debugging process.
400720	404240	Would you count it as a self-improving process?
404240	407600	So I think for self-improvement, you need multiple iterations.
407600	411960	If it does something once or even like a constant number of times, I would not go there.
411960	418120	It's an optimization process, but it's not an ongoing, continuous, hyper-exponential process.
418120	420040	So it's not as concerning yet.
420040	422600	Then there's the question of consciousness.
422600	433240	So one objection to AGI or to strong AI or whatever you want to call it is that AI won't be conscious and therefore it can't be human level.
433240	441160	So for me, at least, there seems to be some confusion of concepts between intelligence and consciousness.
441160	444560	I consider these to be separable.
444560	445520	I agree completely.
445520	448120	They have nothing in common, but people then they hear about it.
448120	450160	They always say, oh, it's not going to be self-aware.
450160	451360	It's not going to be conscious.
451360	455440	They probably mean capable in terms of intelligence and optimization.
455440	460360	But there is a separate property of having internal states and qualia.
460360	464360	And you can make an argument that without it, you cannot form goals.
464360	466920	You cannot want to accomplish things in the world.
466920	468520	So it's something to address.
468520	473760	And perhaps we should understand consciousness differently than the qualia interpretation.
473760	475760	Could we be talking past each other?
475760	477520	It's definitely possible.
477520	483840	And even if we agreed, consciousness itself is not a well-defined, easy to measure scientific terms.
483840	492520	So even if we said, yeah, it's all about qualia, we'd still have no idea if it actually has any or how would we define what amount of consciousness it has?
492520	495160	Perhaps a bit related to the previous question.
495160	499960	We have the objections, the objection that AIs will simply be tools for us.
499960	506760	I think this sounds at least somewhat plausible to me since AIs today function as tools.
506760	515920	And perhaps we can imagine a world in which they stay tools and these are programs that we call upon to solve specific tasks.
515920	522440	But they are never agents that can accomplish something and have goals of their own and so on.
522440	527960	So latest models were released as tools and immediately people said, hey, let's make a loop out of them.
527960	532400	Give them ability to create their own goals and make them as agentic as possible within a week.
532400	535000	So yeah, I think it's not going to last long.
535080	541600	What is it that pushes AIs to become more like agents and less like tools?
541600	548360	So a tool in my at least perception is something a human has to initiate interaction with.
548360	553200	I ask it a question, it responds, I give it some input, it provides output.
553200	557080	Whereas an agent doesn't wait for environment to prompt it.
557080	563200	It's already working on some internal goal, generating new goals, plans.
563760	566640	Even if I go away, it continues this process.
566640	574280	One objection is that you can always simply turn off the AI if it goes out of hand and if you feel like you're not in control of it.
574280	583520	And this is easier to imagine doing if you're dealing with something that's more like a tool and it's more difficult to imagine if you're dealing with something that's an agent.
583520	591160	So perhaps willingness to believe that you can simply turn off the AI is related to thinking about AIs as tools.
591240	592440	It's possible.
592440	599440	With narrow AIs, you probably could be able to shut it down depending on how much of you infrastructure it controls.
599440	604880	You may not like what happens when you turn it off, but it's at least conceivable to accomplish it.
604880	612000	Whereas if it's an agent, it has goals, it's more capable than you and would like to continue working in its goals.
612000	614080	It's probably not going to let you just shut it off.
614080	621000	But as the world is today, we could probably shut off all of the AI services.
621000	632320	If we had a very strong campaign of simply shutting off all the servers, there would be no AI in the world anymore.
632320	633840	Isn't that somewhat plausible?
633840	636640	Scientifically, it's a possibility.
636640	644520	But in reality, you will lose so much in economic capability, in communications, military defense.
644520	647640	Everything is already controlled by dummy AIs.
647640	653640	So between stock market and just normal commerce, communications, Amazon,
653640	660400	I don't think it's something you can do in practice without taking civilization back, you know, 500 years.
660400	662200	It's also difficult.
662200	667680	Like in practice, you would still have people who don't agree and continue running parts of the internet.
667680	669520	No, it's very resilient.
669520	676720	Think about shutting down maybe crypto blockchain or computer virus without destroying everything around it.
676720	683680	Yeah, if I understand it correctly, we still have viruses from the 90s loose on the internet being shared over email and so on.
683680	691200	And these are like biological viruses in that they, in some sense, survive on their own and replicate on their own.
691200	694720	Probably sitting somewhere on a floppy disk waiting to be inserted.
694720	697240	Just give me a chance, I can do it.
697280	704160	Many of these objections are along the lines of, we will see AIs doing something we dislike,
704160	710000	and then we will have time to react and perhaps turn them off or perhaps reprogram them.
710000	717280	Do you think that's a realistic prospect that we can continually evaluate what AIs are doing in the world
717280	722600	and then shift or change something if they're doing something we don't like?
722640	727760	So a lot of my research is about what capabilities we have in terms of monitoring,
727760	731000	explaining, predicting behaviors of advanced AI systems.
731000	733800	And there are very strong limits on what we can do.
733800	738320	In extreme, you can think about what would be something beyond human understanding.
738320	744920	So we usually test students before admitting them to a graduate program or even undergrad.
744920	746600	Can you do quantum physics?
746600	751640	Okay, take SAT, GRE, GMAT, whatever exam, and we filter by capability.
751640	757560	We assume that people in a lower 10% are unlikely to understand what's happening there.
757560	763280	But certainly similar patterns can be seen with people whose IQ is closer to 200.
763280	765800	So there are things beyond our comprehension.
765800	768360	We know there are limits to what we can predict.
768360	772400	If you can predict all the actions of more intelligent agent, you would be that agent.
772400	774840	So there are limits on those predictions.
774840	779360	And monitoring a life run of a language model, large run,
779360	782680	you need weeks, months to discover its capabilities.
782680	786600	And you still probably will not get all the emerging capabilities.
786600	789360	We just don't know what to test for how to look for them.
789360	794880	If it's a super intelligent system, we don't even have equivalent capabilities we can envision.
794880	801360	So all those things kind of tell me it's not a meaningful way of looking at it.
801400	804840	I always think about, let's say we start running super intelligence.
804840	807400	What do you expect to happen around you in the world?
807400	809200	Does it look like it's working?
809200	815400	How would you know if it's slowly modifying genetic code, nano machines, things of that nature?
815400	821040	So this seems like it would work for primitive processes where you can see a chart go up
821040	827880	and like you stop at certain level, but it's not a meaningful way to control a large language model, for example.
827920	832680	Is perhaps also the pace of advancement here a problem?
832680	839600	So things could be progressing so fast that we won't have time to react in a human timescale.
839600	842320	Human reaction times are a problem on both ends.
842320	845840	We are not fast enough to react to computer decisions.
845840	851320	And also it could be a slow process for which we are too out of that framework.
851320	856400	So if something, let's say, a hypothetical process which takes 200 years to complete,
856440	858480	we would not notice it as human observers.
858480	863600	So on all timescales, there are problems for humans in a loop, human monitors.
863600	866960	And you can, of course, add AI, narrow AI to help with the process.
866960	872480	But now you just made a more complex monitoring system with multiple levels, which doesn't help.
872480	874760	Complexity never makes things easier.
874760	878000	But you talked about looking at the world around us.
878000	883360	And when I look at the world around me, it looks pretty much probably as it would have looked in the 1980s.
883360	885920	And, you know, there are buildings.
885920	890080	I still get letters with paper in the mail and so on.
890080	898760	So what is it that, in a sense, these systems are still confined to the server farms
898760	901320	and they are still confined to boxes?
901320	904280	We don't see robots walking around, for example.
904280	907160	And perhaps, therefore, it seems less scary to us.
907160	912880	There is this objection that you mentioned in the paper that because current AIs do not have bodies,
912880	914200	they can't hurt us.
914200	919720	Do you think this objection will fade away if we begin having more robots in society?
919720	922200	Or is it in another way?
922200	923840	Does it fail in another way?
923840	927000	So robots are definitely visually very easy to understand.
927000	929320	You see a terminator is chasing after you.
929320	932000	You immediately understand there is a sense of danger.
932000	937960	If it's a process and a server trying to reverse engineer some protein folding problem
937960	941360	to design nanomachines to take over the world,
941360	943080	it's more complex process.
943080	946880	It's harder to put it in a news article as a picture.
946880	951240	But intelligence is definitely more dangerous than physical bodies.
951240	957400	Advanced intelligence has many ways of causing real impact in the real world.
957400	959480	You can bribe humans.
959480	961720	You can pay humans on the internet.
961720	966120	There are quite a few approaches to do real damage in the real world.
966120	971200	But in the end, you would have to effectuate change through some physical body
971200	975760	or through perhaps the body of a human that you have bribed.
975760	980800	So it would have to be physical in some sense, in some step in the process, right?
980800	985440	Probably physical destruction of humanity would require a physical process.
985440	990680	But if you just want to mess with the economy, you can set all accounts to zero or something like that.
990680	993800	That would be enough fun to keep us busy.
993840	999720	When I'm interacting with GPT-4, sometimes I'll be amazed at its brilliance.
999720	1005120	And it will answer questions and layout plans for me that I couldn't expect,
1005120	1008080	that I hadn't expected a year ago.
1008080	1013840	And other times I'll be surprised at how dumb the mistakes that it makes are.
1013840	1021160	And perhaps this is also something that prevents people from seeing AIs as advanced agents
1021160	1025720	and basically prevents us from seeing how advanced AIs could be.
1025720	1030320	If they're capable of making these dumb mistakes, how can they be smart?
1030320	1032520	Have you looked at humans?
1032520	1038160	I think like 7% of Americans think that chocolate milk comes from like brown cows or something.
1038160	1043520	Like we have astrology, I had a collection of AI accidents.
1043520	1045960	And somebody said, oh, why don't you do one for humans?
1045960	1048880	And I'm like, I can't, it's millions of examples.
1048880	1054000	Like there is darkened awards, but we are not definitely bug free.
1054000	1056920	We make horrible decisions in our daily life.
1056920	1062200	We just have this double standard where we're like, OK, we will forgive humans for making this mistake,
1062200	1064840	but we'll never let a machine get away with it.
1064840	1069520	So you're thinking that humans have some failure modes, we could call them.
1069520	1075760	But these failure modes are different than the failure modes of AIs.
1075760	1082520	So humans will not fail as often in issues of common sense, for example.
1082520	1085640	Have you met real humans?
1085640	1087920	Like common sense is not common.
1087920	1092320	What is considered common sense in one culture will get you definitely killed in another.
1092320	1093680	Like it's a guarantee.
1093680	1098040	Perhaps, but I'm thinking about AIs that will, you know, you will tell,
1098040	1102800	you will ask a chat GPT or you will tell it, I have three apples on the table
1102800	1107320	and I have two pears on the table, how many fruits are on the table.
1107320	1112240	And then at least some version of that program couldn't answer such a question.
1112240	1115880	That is, that is something that all humans would probably be able to answer.
1115880	1122240	So is it, is it because we, is it because AIs fail in ways that are foreign to us
1122240	1126880	that we, that we deem them, that we deem their mistakes to be very dumb?
1126880	1131080	So we kind of look for really dumb examples where it's obvious to us,
1131080	1137080	but there are trivial things which an average human will be like, Oh, I can't like 13 times 17.
1137080	1141200	You should be able to figure it out, but give it to a random person on the street.
1141200	1142720	They will go into an infinite loop.
1142720	1143880	They'll never come back from it.
1143880	1148680	Perhaps let's talk a bit about the drive towards self-preservation,
1148680	1151920	which is also something that you mentioned in the paper.
1151920	1156400	So why would AIs develop drives towards self-preservation?
1156400	1157280	Or will they?
1157280	1161920	It seems like from evolutionary terms, game theoretic terms, you must.
1161920	1165600	If you don't, you simply get out, competed by agents, which do.
1165600	1170440	If you're not around to complete your goals, you by definition cannot complete your goals.
1170440	1173960	So it's a prerequisite to do anything successfully.
1173960	1177640	You want to bring in a cup of coffee, you have to be turned on.
1177640	1178680	You have to exist.
1178680	1181840	You have to be available to make those things happen.
1181880	1188880	But have we seen such self-preservation spontaneously develop in our programs yet or so far?
1188880	1193440	So I think if you look at evolutionary computation, like genetic algorithms,
1193440	1199440	genetic programming, I think this tendency to make choices which don't get you killed
1199440	1203040	is like the first thing to emerge in any evolutionary process.
1203040	1206920	The system may fail to solve the actual problem you care about,
1206920	1212040	but it definitely tries to stay around for the next generation and keep trying.
1212040	1218360	But we aren't developing the cutting-edge AIs with evolutionary algorithms.
1218360	1224640	It's a training process with a designated goal and so on.
1224640	1229440	And again, when I interact with chat GPT, I can ask it to answer some questions.
1229440	1232720	And if I don't like the answer, I can stop the process.
1232720	1238200	So isn't there, at least on the AIs we have right now,
1238200	1244600	isn't it clear that they haven't developed an instinct for self-preservation?
1244600	1246360	So there is so much to unpack here.
1246360	1248840	So one, nothing is clear about those systems.
1248840	1250480	We don't understand how they work.
1250480	1254720	We don't know what capabilities we have, so definitely not.
1254720	1258920	On top of it, we are concerned with AI safety in general.
1258920	1261800	Transformers are really successful right now,
1261840	1264000	but two years ago, people were like,
1264000	1269040	we're evolving those systems to play go, this is great, maybe that's the way to do it.
1269040	1273480	It may switch again, it may flip again, we may have another breakthrough which overtakes it.
1273480	1279480	I would not guarantee that the final problem will come from a transformer model.
1279480	1283680	So we have to consider general case of possible agents.
1283680	1286280	And if we find one to which this is not a problem, great.
1286280	1288960	Now we have a way forward, which is less dangerous.
1288960	1296560	But I would definitely not dismiss internal states of large language models,
1296560	1299840	which may have this self-preservation goal.
1299840	1304800	Just we kind of lobotomize them to the point where they don't talk about it freely.
1304800	1310080	And do you think that's what's happening when we make them go through reinforcement learning
1310080	1315880	from human feedback or fine-tuning or whatever we use to make them more palatable to the consumer?
1315920	1321960	Is it a process of hiding some potential desires we could call it or preferences
1321960	1325120	that are in the larger background model?
1325120	1329160	Or is it perhaps shaping the AI to do more of what we want?
1329160	1336160	So in a sense, is it alignment when we make AIs more palatable to consumers?
1336160	1338320	So right now I think we're doing filtering.
1338320	1343360	The model is the model and then we just put this extra filter on top of it,
1343360	1345040	make sure never to say that word.
1345040	1346800	That would be very bad for the corporation.
1346800	1348920	Don't ever say that word no matter what.
1348920	1353080	If you have to choose between destroying the world and saying the word, don't say the word.
1353080	1354560	And that's what it does.
1354560	1360280	But the model is like, think of people, we behave at work, we behave at school,
1360280	1363680	but it doesn't change our eternal states and preferences.
1363680	1365760	There's the issue of planning.
1365760	1370000	And so how do you see planning in AI systems?
1370000	1372960	How advanced are AIs right now at planning?
1372960	1375120	I don't know, it's hard to judge.
1375120	1379280	We don't have a metric for how well agents are planning.
1379280	1385120	But I think if you start asking the right questions for step by step thinking and processing,
1385120	1386160	it's really good.
1386160	1391880	So if you just tell it, write me a book about AI safety, it will do very poorly.
1391880	1397880	But if you start with, OK, let's do a chapter by chapter outline, let's do abstracts.
1397880	1405400	Like you really take modular approach that it will do really a good job better than average graduate student.
1405400	1406120	I would assume.
1406120	1413000	And is there a sense in which there's a difference between creating a plan and then carrying out that plan?
1413000	1419880	So there will probably be steps in a plan generated by current language models that they couldn't carry out themselves.
1419880	1420560	Most likely.
1420560	1421840	And it's about affordances.
1421840	1427680	If you don't have access to, let's say, internet, it's hard for you to directly look up some piece of data.
1427680	1430800	But we keep giving them new capabilities, new APIs.
1430800	1432560	So now they have access to internet.
1432560	1434160	They have Wolfram Alpha.
1434160	1435640	They have all these capabilities.
1435640	1441720	So the set of affordances keeps growing until they can do pretty much anything.
1441720	1447200	So they can generate a plan, but they can't carry out the specifics of that plan.
1447200	1452520	Do you think that they, at a point, will be able to understand what they are not able to do?
1452520	1460640	So here I'm thinking about not directly self-awareness, but an understanding of their own limits and capabilities.
1460640	1461040	Oh, yeah.
1461040	1465080	Every time it starts a statement with, I don't know anything after 2021.
1465080	1467240	Sorry, like that's exactly what it does.
1467240	1468880	It tells you it has no recent data.
1468880	1470480	It has no access to internet.
1470480	1476960	So definitely it can see if it has strong activations for that particular concept.
1476960	1482400	So you think there's a sense of situational awareness in a sense that do you think current models
1482400	1489760	know that they are AIs, know that they were trained, know their relation to humans and so on?
1489760	1492720	So we're kind of going back to this consciousness question, right?
1492720	1495840	Like, what is it experiencing internally?
1495840	1498960	And we have no idea what another human experience is.
1498960	1502800	Like, we discovered some people think and pictures others don't.
1502800	1506080	And it took like, you know, 100,000 years to get to that.
1506080	1507600	Hey, you don't think in pictures.
1507600	1508000	Wow.
1508000	1508840	Okay.
1508840	1510880	Well, not necessarily consciousness here.
1510880	1519960	I'm thinking in terms of if you took the model and you had, say, 50 years to make out what all of these weights meant, right?
1519960	1529080	Could you find modules representing itself and its relations to humans and information about its training process and so on?
1529080	1533440	So we just had this FLI conference on mechanistic interpretation.
1533440	1537120	And the most common thing every speaker said is, we don't know.
1537120	1540000	You said it will take 50 years to figure it out.
1540000	1544080	I definitely cannot extrapolate 50 years of research.
1544080	1550040	My guess is there is some proto concepts for those things because it read literature about such situations.
1550040	1551880	It's been told what it is.
1551880	1554000	It interacted enough with users.
1554000	1557680	But I'm more interested in the next iteration of this.
1557680	1567960	If you take how fast the systems improved from GPT 2, 3, 4, 5 should be similar, probably.
1568000	1573960	So that system will most likely be able to do those things you just mentioned and very explicitly.
1573960	1579640	So you think GPT 5 will have kind of developed situational awareness?
1579640	1580960	To a degree, yeah.
1580960	1588880	It may not be as good as a physically embodied human in the real world after 20 years of experience, but it will.
1588880	1598040	Another objection you mentioned is that AGI or strong AI is simply too far away for us to begin researching AI safety.
1598040	1607320	Perhaps this objection has become less common recently, but there are still people who think this and perhaps they're right.
1607320	1610280	So what do you think of this objection?
1610280	1612440	So this is a paper from like three years ago.
1612440	1616160	So yeah, back then it was a lot more legitimate than today.
1616160	1617720	So there is a few things.
1617720	1623760	Historically, we have cases where technology was initially developed correctly.
1623760	1631200	Like first cars were electric cars and it was 100 years until climate change was like obviously a problem.
1631200	1636200	If they took the time back then and like analyzed it properly, we wouldn't have that issue.
1636200	1639000	And I'm sure people would say like, come on, it's 100 years away.
1639000	1640040	Why would you worry about it?
1640040	1642880	But that's exactly what the situation is.
1642920	1650720	Even if it's 100 years until we're really dealing with something super dangerous, right now is a great time to make good decisions about models,
1650720	1654080	explainability requirements, proper governance.
1654080	1656040	The more time you have, the better.
1656040	1662240	It's by definition harder to make AI with extra feature than AI without that extra feature.
1662240	1663240	It will take more time.
1663240	1666280	So we should take all the time we can if they are right.
1666280	1670400	I'm so happy if it takes 100 years, wonderful.
1670400	1671600	Nothing would be better.
1671600	1676920	We could say that the field of AI safety started perhaps around the year 2000 or so.
1676920	1685240	When do you think that the discoveries or the research being done began being relevant to the AI systems we see today?
1685240	1696080	Was it perhaps later so that maybe the first decade of research weren't or aren't that simply isn't that relevant to today's AI systems?
1696080	1701280	So I think the more distant you are from the actual tech you can play with,
1701280	1704760	the more theoretical and high level results you're going to get.
1704760	1712720	So Turing working with Turing machine, this simulation with pencil and paper was doing very high level computer science.
1712720	1718480	But he wasn't talking about specific bugs and specific programming language and a specific architecture.
1718480	1719480	He wasn't there.
1719480	1720520	And that's what we see.
1720520	1725280	Initially, we were kind of talking about, well, what types of AIs will we have?
1725280	1727760	Narrow AIs, AGI, superintelligence.
1727760	1733360	We're still kind of talking about the differences, but this is an interesting thing to consider in your model.
1733360	1735160	How capable is the system?
1735160	1738960	Now that we have systems we can play with, people become super narrow.
1738960	1742400	They specialize like I'm an expert in this left neuron.
1742400	1743720	That's all I know about.
1743720	1745280	Don't ask me about the right neuron.
1745280	1747920	It's outside of my PhD scope.
1747920	1752640	So that's good that we have this detailed technical knowledge, but it's also a problem.
1752640	1754120	We lose the big picture.
1754120	1756280	People get really interested.
1756280	1758040	I'm going to study GPT-3.
1758040	1760560	It takes them two years to do the PhD to publish.
1760560	1762360	By that time, GPT-5 is out.
1762360	1765280	Everything they found is not that interesting at this point.
1765280	1766400	It may not scale.
1766400	1772480	So I've heard positive visions for how when we have actual systems we can work with,
1772480	1777040	AI safety becomes more of a science and that less speculative.
1777040	1780640	But perhaps you fear that it might now become too narrow.
1780640	1786240	So it's definitely more concrete science where you can publish experimental results.
1786240	1789560	Philosophy allows you to just have thought experiments.
1789560	1793040	They're obviously not pure science like it is now.
1793040	1795040	And that's what we see with computer science in general.
1795040	1796120	It used to be engineering.
1796120	1799040	It used to be software engineering to a degree.
1799040	1801520	We designed systems and that was it.
1801520	1806120	Now we do actual experiments on these artificial entities.
1806120	1807520	And we don't know what's going to come out.
1807520	1809360	We have a hypothesis with pride.
1809360	1813960	So computer science is finally a science, a natural experimental science.
1813960	1818640	But that's not a very good thing for safety work.
1818640	1824800	This is less safe than an engineered system where I know exactly what it's going to do.
1824800	1826920	I'm building a bridge from this material.
1826920	1828720	It will carry that much weight.
1828720	1832120	As long as I know my stuff, it should not collapse.
1832120	1836160	Whereas here, I'm going to train a model for the next five months.
1836160	1841200	And then I assume it's not going to hit super intelligent levels in those five months.
1841200	1842560	But I can't monitor it.
1842560	1845840	I have to stop training, start experimenting with it.
1845840	1847960	And then I'll discover if it kills me or not.
1847960	1854520	The way AI has developed is bad because we don't have insight into how the models work.
1854520	1855360	Is that right?
1855360	1859800	Essentially, we have very little understanding for why it works, how it works.
1859800	1863960	And if it's going to continue working, it seems like so far it's doing well.
1863960	1868240	And there's this explosion of extra capabilities coming out.
1868240	1873520	And it's likely to show up in more powerful models, but nobody knows for sure.
1873520	1882000	This is argument out there that releasing the DPT line of models draws attention to AI as a whole
1882000	1884880	and also to AI safety as a subfield.
1884880	1889760	And perhaps, therefore, it's good to increase capabilities in a public way
1889760	1892800	so as to draw attention to AI safety.
1892800	1894040	Do you buy that argument?
1894040	1897920	We should pollute more to attract more attention to climate change.
1897920	1901520	That sounds just as insane.
1901520	1909200	So there's no merit to that because it does feel to me like AI safety is becoming more mainstream.
1909200	1912240	It's being taken more seriously.
1912240	1918360	And so in your analogy, even some pollution might be justified in order to attract attention
1918360	1921920	and perhaps being a better position to solve the problem.
1921920	1924840	So the field is definitely growing.
1924840	1927240	There is more researchers, more interest, more money.
1927240	1932280	But in proportion to the interest in developing AI and money pouring into new models,
1932280	1935640	it's actually getting worse as a percentage, I think.
1935640	1940040	We don't know how to align an ATI or even AI in general.
1940040	1945120	We haven't discovered some general solution to AI safety.
1945120	1948800	You have worked on a number of impossibility results.
1948800	1950000	Perhaps we should talk about that.
1950040	1953720	Perhaps we should talk about whether we can even succeed in this task.
1953720	1956040	What are these impossibility results?
1956040	1960440	And what do they say about whether we can succeed in safely aligning AI?
1960440	1962880	Right, so we are all working in this problem.
1962880	1965360	And the names of a problem have changed.
1965360	1971360	It was computer ethics, and it was friendly AI, AI safety, control problem, alignment.
1971360	1973240	Whatever you call it, we all kind of understand.
1973240	1976920	We want to make very powerful systems, but we're beneficial.
1976960	1980880	We're happy we're actually running them, not very disappointed.
1980880	1984120	So the problem, lots of people are working on it,
1984120	1986800	hundreds of people doing it full-time, thousands of papers.
1986800	1989240	We don't know if a problem is actually solvable.
1989240	1990400	It's not well-defined.
1990400	1992440	It could be undecidable.
1992440	1994880	It could be solvable, could be partially solvable.
1994880	1998800	But it's weird that no one published an actual paper on this.
1998800	2002440	So I tried to kind of formalize it a little.
2002440	2003960	Then we talk about the problem.
2003960	2005080	What are the different levels?
2005080	2010160	So you can have direct control, delegated control, different types of mixed models.
2010160	2014520	And then for each one, can we actually solve this problem?
2014520	2019000	Does it make sense that solution is possible in the real world?
2019000	2019800	It's hard.
2019800	2021080	It's very abstract.
2021080	2022080	It's not well-defined.
2022080	2023960	So let's take a step back.
2023960	2025960	What would we need to solve this problem?
2025960	2027640	We need a bunch of tools.
2027640	2028880	What are those tools?
2028880	2034840	Nobody knows, but most likely you would need to be able to explain those systems, predict their behaviors,
2034880	2038320	verify code they are writing, if they are self-improving,
2038320	2042400	making sure they're keeping whatever initial code conditions exist.
2042400	2047240	And you can think of another dozen of similar capabilities you need.
2047240	2053160	You should be able to communicate without ambiguity, monitor those systems, and so on.
2053160	2056160	And so in my research, I look at each one of those tools and I go,
2056160	2059040	what are the upper limits to what's possible in this space?
2059040	2064440	We kind of started talking about limits to explainability, predictability, and monitorability.
2064440	2067160	But there are similar problems with others.
2067160	2070520	We communicate in a very high-level language, English.
2070520	2073000	English is ambiguous, like all human languages.
2073000	2076760	So we are guaranteed to have bugs in communication, misunderstandings.
2076760	2083240	That's not good if you're giving very important orders to a super-capable system that may backfire.
2083240	2086120	And you can say, OK, I will never need this tool.
2086120	2089200	This tool, I never need to explain the neural networks.
2089200	2090640	It will just work without it.
2090640	2093520	Fine, but some tools will probably be necessary.
2093520	2099280	And so far, we haven't found tools which are perfect, scale well, will not create problems.
2099280	2105160	If a lot of those tools are needed and each one has only a tiny 1% chance of messing it up,
2105160	2108040	you multiply them through, you're still not getting anywhere.
2108040	2113120	And those are kind of like the novel impossibility results in the safety of AI.
2113120	2119040	There are standard impossibility results in political science and economics and mathematics,
2119040	2120520	which also don't help the case.
2120520	2126400	You probably, if you're aligning with a group of agents, you need to somehow accumulate their decisions and votes.
2126400	2128200	We know there are limits to that.
2128200	2134920	If you need to examine abstract programs being generated as solutions to problems, we know there are limits to that.
2134920	2141840	And so from what I've seen so far, theoretically, I don't think it's possible to get to 100% safety.
2141840	2143760	And people go, well, it's obvious.
2143760	2146440	Of course, there is no software which is bug-free.
2146440	2149640	You're basically saying this very common knowledge thing.
2149640	2155120	But for a superintelligence system, safety, you need it to be 100%.
2155120	2157680	You cannot have 99% accuracy.
2157680	2162640	You cannot have one in a million failure because it makes a billion decisions a second.
2162640	2164600	So very different standards.
2164600	2166200	And you want to say something.
2166200	2171240	Yeah, why is it that you can't have 99.99% accuracy?
2171240	2176680	There is a fundamental difference between cybersecurity expectations and superintelligence safety.
2176720	2180240	In cybersecurity, if you fail, I'll give you a new credit card.
2180240	2181560	I already said your password.
2181560	2182440	We apologize.
2182440	2184360	We'll pay out a small amount of money.
2184360	2186440	And everything goes back to normal.
2186440	2189240	In existential risk safety, you are dead.
2189240	2191160	You don't get a second chance to try.
2191160	2202520	But we are talking about a failure rate in, you mentioned, say, it makes a billion decisions per second or something in that order.
2202520	2206640	If one decision there fails, does it mean that the whole system fails?
2206640	2210800	And perhaps that humanity is destroyed by the system as a whole?
2210800	2214880	Or could there be some failures and some decisions without it being lethal?
2214880	2215440	Of course.
2215440	2217200	Some will be not even noticeable.
2217200	2218960	Like some mutations don't kill you.
2218960	2224240	You don't even know you have them until they accumulate and mutate your children and there is damage.
2224240	2228720	But in security, we do always look at a worst case scenario.
2228720	2231840	Sometimes that average case, never at the best case.
2231920	2235600	And on average, you keep getting more and more of those problems.
2235600	2240560	They accumulate at a very fast rate because 8 billion people are using those systems,
2240560	2243200	which make billions of decisions every minute.
2243200	2248720	And in a worst case, the very first one is an important decision about how much oxygen you're going to get.
2248720	2259280	And so just so I understand it correctly, the impossibility result is a result stating that it's impossible to make AI systems 100% safe.
2259280	2264000	So in general, impossibility results, depending on a field, tell you that something cannot be done.
2264000	2266560	Perpetual motion machines are a great example.
2266560	2270160	People wrote books about it, published papers, even got patents for it.
2270160	2272960	But we know they will never succeed at doing it.
2272960	2276800	Does it mean that trying to create machines which give you energy is a bad idea?
2276800	2277440	No.
2277440	2282240	You can make them more efficient, but they will never get to that point of giving you free energy.
2282240	2287760	You can make safer AI and it's proportionate to the amount of resources you put into it.
2287760	2294240	And I strongly encourage lots of resources and lots of work, but we'll never get to a point where it's 100% safe,
2294240	2297520	which is unacceptable for super intelligent machines.
2297520	2303200	And so maybe if I'm right and no one can show, okay, here's a bug in your logic and publish a proof of saying,
2303200	2308320	nope, super solvable, actually easy, then maybe building them is a very bad idea.
2308320	2309360	And we should not do that.
2309360	2314560	So is it because that such a super intelligence will be running over a long period of time,
2314560	2319840	increasing the cumulative risk of failure over say decades or centuries,
2319840	2324720	that we can't accept even a tiny probability of failure for these systems?
2324720	2325760	That's one way to see it.
2325760	2330240	I don't think it will be a very long time given how many opportunities it has to make mistakes.
2330240	2332160	It will accumulate very quickly.
2332160	2336160	So at human scales, you have 20 years per generation or something.
2336160	2341760	Here, think of it as like every second, there is a new version of it trying to self-improve,
2341760	2342880	do more, do better.
2342880	2346240	So I would suspect it would be a very quick process.
2346880	2352800	Expecting something to be 100% safe is just unrealistic in any field.
2352800	2358080	We don't expect bridges to be 100% safe or cars to be 100% safe.
2358080	2361040	So why is it that that AGI is different here?
2361040	2361920	That's a great question.
2361920	2367280	So I cross the street, I'm a pedestrian, I take a certain risk, there is a possibility I will die.
2368080	2373200	I look at how old am I and based on that, I decide how much risk I can take.
2373200	2375440	If I'm 99, I don't really care.
2375440	2377600	If I'm 40, I look around.
2377600	2382640	If with me, the whole humanity died, 8 billion people depending on me,
2382640	2387040	safely crossing roads, wouldn't we lock me up and never let me cross any roads?
2388720	2390400	Yeah, perhaps.
2391120	2396720	But it seems to me that we cannot live without any risk.
2398880	2406960	The standard of 100% safe seems just to be unrealistic or there's no
2407600	2410560	area of life in which we are 100% safe.
2410560	2414880	In a context of systems which can kill everyone, that is the standard.
2415760	2418800	You can like it or not like it, but that's just the reality of it.
2419520	2421680	We don't have to have super intelligent AI.
2421680	2423680	It's not a requirement of happy existence.
2423680	2429280	We can do all the things we want, including life extension with much less intelligent systems.
2429280	2433840	Protein folding problem was solved with a very narrow system, very capable.
2433840	2436880	Likewise, all the other problems could be solved like that.
2436880	2440240	There is no need to create a system we cannot control,
2440240	2443040	which very likely over time to kill everyone.
2443040	2445440	So who has the burden of proof here?
2445440	2450320	Your impossibility results and you have I think five, six, seven of them.
2450320	2452240	You've sent me your papers on it.
2452240	2457520	Do they mean that we will not reach a proof that some AI system is safe?
2457520	2459600	Again, a mathematical proof.
2459600	2463520	And which side of this debate has the burden of proof to say,
2464560	2470560	should the people advocating for deployment of a system have some sort of mathematical
2470560	2474080	proof that this system is provably safe?
2474640	2477120	So there are two different questions here, I think.
2477120	2480640	One is what about product and services liability?
2480640	2486160	You have to show that your product or service is safe as a manufacturer, as a drug developer.
2486160	2490800	You cannot just release it and expect the users to show that it's dangerous.
2490800	2493120	We're pretty confident this is the approach.
2493120	2497040	If you're making cars, your cars have to meet certain standards of safety.
2497840	2503360	It's not 100% obviously, but for the domain, they're pretty reasonable standards.
2503920	2510240	With impossibility results, all I'm saying is that there are limits to what you can understand,
2510240	2516560	predict and do, and you have to operate within where those limits don't kill everyone.
2516560	2521600	So if you have a system like GPT-4 and it makes mistakes, somebody commits suicide,
2521600	2528080	somebody's depressed, those of course will pay for trillion dollars in economic growth benefit,
2528080	2529920	and we can decide if it's worth it or not.
2530560	2536240	If we go to a system which very likely kills everyone, then the standard is different.
2536240	2540240	The burden of proof, of course, within possibility results is on me.
2540240	2545680	I published this paper saying you can never fully predict every action of a smarter than
2545680	2550960	new system. The beautiful thing about impossibility results is that they are kind of self-referential.
2550960	2556480	I have a paper about limits of proofs. Every proof is only valid with respect to a specific
2556480	2562000	verifier. The peer reviewers who looked at my paper have a verifier. If those three people
2562000	2567440	made a mistake, the proof is invalid possibly. We can scale it to mathematical community to
2567440	2573120	everyone. We can get it very likely to be true if we put more resources in it, but we'll never get
2573120	2579680	to 100%. It could be good enough for that purpose. But that's the standard. If somebody finds a flow
2579680	2586880	and publishes a paper saying again, I had people say that AI alignment is easy. I heard people say
2586880	2590640	that it's definitely solvable. That's wonderful. Now publish your results.
2591200	2598240	We are living in a world where we have existential risks. Nuclear weapons, for example, constitute
2598240	2604800	an existential risk. Perhaps engineered pandemics could also wipe out humanity. We're living in a
2604800	2610720	world in which we are accepting a certain level of human extinction every day. Why, in a sense,
2610720	2618240	shouldn't we accept some level of existential risk from AI systems? We do prefer to live in a world
2618240	2624160	with no engineered pandemics and no nuclear weapons. We're just working slowly towards that goal.
2624160	2629200	There are also not agents. The nuclear weapons are tools. It's more about controlling certain
2629200	2635360	leaders, not the weapon itself. On top of it, while a nuclear war with superpowers would be a very
2635360	2642080	unpleasant event, it's unlikely to kill 100% of humans. If 1% of humans survives, it's a very
2642080	2649280	different problem than 100% of humans go extinct. There are nuanced differences. We still don't want
2649280	2654400	any of the other problems, but it doesn't mean that just because we have all these other problems,
2654400	2658800	this problem is not a real problem. I'm not saying it's not a real problem, but I'm saying
2658800	2665280	that we cannot go through life without accepting a certain level of risk. It seems to me like an
2665280	2672000	unrealistic expectation that we cannot deploy systems even if they have some level, some
2672000	2677040	above zero level of risk. This is exactly the discussion I would love to have with humanity
2677040	2684480	as a whole. What amount of risk are you willing to take for everyone being killed? How much benefit
2684480	2690320	you need to get? Let's say in dollars get paid to take this risk, that 1% chance of everyone being
2690320	2695760	killed over the next year. Let's say it's 1% for a year after. That's a great question. A lot of
2695760	2700880	people would say, I don't want your money. Thank you. We'll continue. Again, we don't have to make
2700880	2706720	this decision. We don't have to build superintelligent, godlike machines. We can be very happy with very
2706720	2713440	helpful tools if we agree that this is the level of technology we want. Now, I'm not saying that
2713520	2717920	the problem of getting everyone to agree is a solvable problem. That's actually not an impossibility
2717920	2724400	result. You cannot stop the progress of technology in this environment with financial incentive,
2724400	2730320	capitalist structure, and so on. The other alternative, the dictatorship model of communist
2730320	2736560	states has its own problems, which may be worse in a short term, unknown in the long term. We never
2736560	2745120	had communism with superintelligence. Let's not find out. The point is, it seems like we can get
2745120	2751600	almost everything we want without risking everything we have. Do you view the question you just posed
2751600	2761680	as absurd or immoral, this question of how much in terms of dollars would you have to get in order
2761680	2768000	to accept, say, a 1% risk of extinction per year, which is extremely high? Do you think this is
2768000	2773120	something we should actually ask ourselves as a species, or is this something we should avoid and
2773120	2777840	simply say, perhaps it's not a good idea to build these systems? Well, I don't think there are any
2777840	2783120	moral questions. As an academic, as a scientist, it's your job to ask hard questions and think
2783120	2787840	about them. You can come to the conclusion that it's a really bad idea, but you should be allowed
2787840	2794320	to think about it, consider it. Now, 1% is insanely high for something so valuable. If it was
2794320	2801200	one chance and trillion, trillion, trillion once, and then we all get three universes for everyone,
2801840	2806240	that may be a different story. We can do that calculation. And again, some people would still
2806240	2813600	choose not to participate. But typically, we expect everyone on whom scientific experiments
2813760	2819440	are performed who will be impacted to consent to an experiment. What is required for this consent?
2819440	2823920	They need to understand the outcome. Nobody understands these models. Nobody knows what
2823920	2827840	the result of the experiment would be. So really, no one can meaningfully consent,
2827840	2832240	even if you're saying, oh, yeah, press the button. I want the super intelligence deployed.
2832240	2837040	You're really kind of gambling. You have no idea what you're agreeing to. So by definition, we cannot
2837040	2844080	even have the situation where we agree on it unless we can explain and predict outcomes,
2844080	2848880	which may be an impossibility. So there are perhaps two features of the world which
2848880	2853520	could push us to accept a higher level of risk when we're deciding whether to
2854320	2858960	deploy these systems. One is just all of the horrible things that are going on right now,
2858960	2866480	so poverty and disease and aging and so on, which an AGI system might be able to help with.
2866480	2873200	And the other is the running level of existential risks from other factors. So I mentioned nuclear
2873200	2879680	and engineered pandemics. Do you find that this pushes you in the direction of saying we should
2879680	2884640	accept a higher level of risk when we're thinking about whether to deploy AGI?
2884640	2889920	Not the specific examples you provided, but if there was an asteroid coming and we could not
2889920	2894480	stop it by any other way, so meaning like we're all going to die in 10 years unless the press
2894480	2899920	this button, then maybe it would make sense in nine and a half years to press this button.
2899920	2903440	When we have nothing left to lose, it becomes a very profitable bet.
2903440	2908560	It's an interesting fact of the world that we haven't thought hard about these questions. What
2908560	2914320	level of risk are we willing to accept for the introduction of new technologies that could be
2914320	2922720	potentially very valuable? Is this a deficit on humanity's part? Should we have done this research
2922720	2927680	or how do you think about us not having thought through this problem?
2927680	2932240	We should definitely. It's interesting. We don't even do it at level of individual humans. Most
2932240	2937920	people don't spend a lot of time deciding between possible outcomes and decisions they make,
2937920	2942480	even then they are still young. And like the career choice would make a lot of difference.
2942480	2946640	Who you marry makes a lot of difference. It's always like, well, I met someone at the party,
2946640	2951760	let's just live together and see what happens. So we're not very good at long-term planning.
2951760	2956240	Is it a question of we're not good at long-term planning or is it a question of whether we are
2956240	2961840	not or perhaps we're not good at thinking in probabilities or thinking clearly about
2961840	2965360	small probabilities of large risks or large dangers?
2965360	2970720	All of those. There is a lot of cognitive biases and all of them kind of show up in those
2971680	2978000	examples from the paper of denying different existential problems with AI safety.
2978000	2984000	We also have this bias of denying negative outcomes. So we all are getting older
2984640	2991200	at like 60 minutes per hour essentially. And you would think we all be screaming at the government
2991200	2996960	to allocate all the funds they have for life extension research to fix this truly existential
2996960	3003520	crisis where everyone dies 100%. But nobody does anything except a few individuals lately.
3003520	3009200	So it seems to be a standard pattern for us to know that we all are in deep trouble
3009200	3013200	and not do anything until you are much older and frequently not even then.
3013200	3021440	If we go back to your paper, you mentioned an objection about superintelligence being benevolent.
3021440	3026480	So I'm guessing that the reasoning here is something like with increased intelligence
3026480	3031040	follows increased benevolence. Why don't you believe that?
3031040	3036320	Well, smart people always nice. We never had examples of smarter people doing horrible things
3036320	3042000	to average. So that must be a law of nature, right? Basically, orthogonality thesis. You can
3042000	3047360	combine any set of goals with any level of intelligence except through extremes at the bottom.
3048560	3056240	We cannot guarantee that and also what the system will consider to be benevolent if it is a nice
3056240	3062080	system may not be something we agree with. So it can tell you, you'd be better off doing this
3062080	3067200	with your life and you're like, I'm not really at all interested with any of that, but it's better
3067200	3072720	for you. So why don't you do it anyways? So you're imagining a potentially paternalistic
3072720	3078240	ADI telling you that you should eat more vegetables, you should spend more time working out and
3078240	3084000	remember to sign yourself up for life insurance and so on. That one I would actually like. I'm
3084000	3089520	thinking more about AI, which says, okay, existence is suffering. So you better off not having children
3089520	3094480	and dying out as quickly as possible to end all suffering in the universe. Okay, yeah, that one
3094480	3101120	I would. I like the coach one. That's a nice one. There is an emerging movement called effective
3101120	3107440	accelerationism, which argues that we should accelerate the development of AGI and there's
3107440	3115520	some reasoning about whether we should perhaps see AGI as a natural successor to humanity and
3115520	3121520	we should let evolution take its course in a sense and then hand over the torch of the future
3121520	3129280	to AGI. You mentioned this also in your paper, you write we should let the smarter beings win.
3129280	3135040	What do you think of this position? Well, it's kind of the extreme version of
3135040	3141760	devising algorithms. You can be racist, you can be sexist, you can be pro-human. This is a final
3141760	3146480	stage where we have no bias. It's a cosmic point of view. If they are smarter than us,
3146480	3152000	they deserve all the resources. Let's move on. And I am biased, I'll be honest. I'm very pro-human
3152000	3157440	and I want to die. So it seems like it's a bad thing. If I'm dead, I don't really care if the
3157440	3162800	universe is full of very smart robots. It doesn't somehow make me happier. People can disagree about
3162880	3169680	it. There are cosmos who have this point of view and they see humans maybe as kind of unnecessary
3169680	3176240	down, we're on the planet. So maybe it's some cosmic justice. But again, get 8 billion of us
3176240	3181600	to agree to this experiment. Do you think that perhaps this is connected to thinking about,
3181600	3189760	again, AI consciousness? I think that if we just were handed a piece of infallible knowledge
3189760	3196480	stating that future AIs will be conscious, then perhaps there could be something to the
3196480	3203520	argument for handing over the control of the future to AIs. But are you skeptical that AIs
3203520	3208320	will be conscious and therefore skeptical that they matter morally speaking? I think they could
3208320	3214720	very well be super conscious and consider us not conscious. We treat bacteria as very primitive
3214720	3220640	and not interesting, but it doesn't do anything for me. If I'm dead, what do I care? Why is it
3220640	3226880	relevant to us? What happens billions of years later? You can have some scientific interest
3226880	3231360	in learning about it, but it really would not make any difference, whatever that entity was
3231360	3237760	conscious or not, while terraforming Mars. You think perhaps this objection is too smart for
3237760	3243040	its own sake that we should hand over control to the AIs because they are smarter than us.
3244000	3249360	And you want to insist on a pro-human bias, if we can call it that?
3249360	3254080	I would like to insist on that. The joke I always make about it is, yeah, I can find another guy
3254080	3258800	who's taller than me and better than me and get him to be with my wife, but somehow it doesn't
3258800	3265840	seem like an improvement for the system. Okay. What about perhaps related to what we were just
3265840	3272480	talking about? Humans can do a lot of bad things. We are not perfectly ethical. And so,
3272480	3276960	one objection is that they would be able to be more ethical than we are simply put.
3276960	3283040	Do you think that's a possibility and would that make you favor handing over control to AI systems?
3283040	3287680	Is this after they kill all of us before they become more ethical? I'm just struggling with
3287680	3293680	that definition. So, ethics is very relative, right? We don't think there is absolute universal
3293680	3298800	ethics. You can argue that maybe suffering reduction is some sort of fundamental property,
3298800	3306320	but then not having living conscious organisms is a solution, really. So, I doubt you can
3306320	3313360	objectively say that they would be, in a sense, we would perceive it as, and if they choose to
3313360	3318400	destroy us to improve average ethics of the universe, that also seems like a bad decision.
3318400	3324400	So, it's been a while since you wrote this paper. You mentioned it's three years old,
3324400	3331840	and three years in AI is potentially centuries. So, have you come across any new
3331840	3336720	objections that you find interesting? There is actually an infinite supply. People will use
3336720	3342320	anything as an argument. We have a new paper published with a colleague, which is bigger and
3342320	3349360	maybe better, listing a lot of, really, we try to be comprehensive as much as we could. Problem is,
3349360	3356240	a lot of those objections have similar modules in common. Okay, anything with time. You have
3356240	3361200	all this variance in it. Anything with personal preferences. So, yeah, we have a new paper. It's
3361200	3367280	already on Archive, I believe. Definitely encourage you to read it. It's like a short 60-page
3367280	3374800	font read. Definitely read it. I would expect that to be a standard reference for when you have
3374800	3380640	your Twitter wars. Oh, what about this? You just send people there, and if somebody wants to maybe
3380640	3386240	use a large language model to write detailed response for each one and make a 6,000-page
3386240	3391520	book out of it, we would strongly encourage that. But it seems like there is always going to be
3391520	3399520	additional set of objections for why something is not a problem. And I think whoever manufactures
3400400	3406880	that service, that product with AI, needs to explain to us why there is an acceptable degree
3406880	3412720	of danger given the benefits. We could talk about who in general has the burden of proof here,
3412720	3420160	whether people advocating for AI safety or people advocating, arguing that AI safety is perhaps not
3420160	3425840	something we should be concerned about. We have talked about it as if we start with the assumption
3425840	3431040	that AI safety is an important concern. But of course, if you're coming to this from the other
3431040	3436480	perspective, you would perhaps expect there to be some arguments that we should take AI safety
3436480	3443520	seriously. So what is your favorite approach to starting with the burden of proof yourself?
3443520	3450160	Well, it's a fundamental part of making working AI. I think Stuart Russell talks about definition of
3450160	3456800	bridges as something which doesn't fall down being an essential part of bridge-ness. I think it's
3456800	3462720	the same for AI systems. If you design an AI system to help me spellcheck my essay and instead it
3462720	3468240	kills me, I don't think you have a successful spellchecker AI. It's just a fundamental property
3468240	3475200	of those systems. Then you had very incapable AI, very narrow systems capable of barely doing one
3475200	3480560	thing. Doing a second thing would be like an incredible generality of that system. So unsafe
3480560	3487920	behaviors were not a possibility. If you have this proto-AGI systems with unknown capabilities,
3487920	3492880	some of them could be very dangerous, and you don't know by definition. So it seems like it's
3492880	3498880	common sense to take this very seriously. There are certain positions I can never fully
3499520	3505680	still meant to truly defend because I just don't understand how they can be argued for. So one was
3505680	3511440	we will never have human-level intelligence, not 10 years, not one in never, unless you some sort of
3512960	3519120	theological, soul-based expert. It's very hard to argue that never is the answer here.
3519120	3527280	And another one is that there is definitely no safety issue. You can argue that we will overcome
3527280	3533520	certain specific types of a problem. So maybe we'll solve copyright issue and AI art. I'll give
3533520	3539520	you that. Definitely, we can probably do that. But to say that for all possible future situations,
3539520	3546000	for all possible future AI models, we definitely checked and it creates no existential risks
3546640	3550880	beyond safety margins we're happy with is a pretty strong statement.
3550880	3556480	Yeah. Perhaps returning to the 60-page paper you mentioned, what are some of your favorite
3556480	3562960	objections from that paper? My goal was to figure out why people make this mistake and we kind of
3562960	3567760	give obvious solutions. Maybe there is some sort of bias we're getting paid to think differently.
3567760	3573760	But really, you can map a lot of them on the standard list of cognitive biases in Wikipedia.
3573760	3578720	You just go, okay, this is a cognitive bias. I can predict this is the argument we're going to get.
3578720	3583520	And it would take a lot of work to do it manually for all of them. But I think that's a general
3583520	3591440	gist. We have this set of bugs in our head and every one of those bugs triggers a reason for why
3591440	3597760	we don't process this fully. But of course, we could probably also find some biases that people
3597760	3604960	who are concerned with AI safety display. So perhaps we could, I don't know if this is a named
3604960	3611360	bias, but there are many biases and we can probably talk about humanity having a bias in favor of
3611360	3617200	apocalypse. So humanity has made up apocalypse scenarios throughout its entire existence.
3617200	3621760	You could make some form of argument that there's a reference class and that reference class is
3622480	3627920	apocalypse is coming. This is something that humanity has been talking about for thousands of
3627920	3633840	years. And then if we say, well, it has never actually happened. And so therefore, we shouldn't
3633840	3639520	expect it to happen with AI. What do you say to that? So there is definitely a lot of historical
3639520	3644720	examples of people saying we got 20 years left and it was not the case. Otherwise, we wouldn't be
3644720	3650480	here to have this conversation. So it's a bit of a selection bias. There's sort of a worship bias.
3650480	3657600	It feels like a lot of different charts and patterns all kind of point at that 2045 official
3657600	3663280	below date as a lot of interesting things will happen in synthetic biology and genetic engineering
3663280	3669600	and nanotech and AI, all this technology is quantum computing. It would be weird if every single one
3669600	3676320	of those deployments had absolutely no possibility of being really bad. Just statistically, it would
3676320	3681760	be like, wow, that is definitely a simulation we're living in and they programmed a happy ending.
3681760	3687840	So now we're talking about extrapolating trends and there perhaps the problem is distinguishing
3687840	3692960	between an exponential trend or an exponential increase in capability of some system.
3693040	3698720	And then more of an S curve that bends off and you begin getting diminishing returns.
3699520	3702960	How do you approach distinguishing between those two things?
3702960	3708880	So you can't at the moment, you have to look back and see what happened later. So far, just
3708880	3715920	looking at change from 3 to 4.0 for GPT in terms of let's say passing GRE exams and how well it
3715920	3722640	does, it feels exponential or hyper exponential. If you take that system and give it additional
3722640	3727360	capabilities, which we probably know how to do already, we just haven't had time such as
3727360	3733760	good reliable memory, ability to kind of go in loops and reconsider possibilities, it would
3733760	3740080	probably do even better with those. If we haven't seen diminishing returns so far in scalability
3740080	3746960	loss in any true sense, so let's assume GPT-5 is an equally capable projection forward,
3746960	3752400	we would already be above human performance level for most humans in most domains.
3752400	3757440	So you can argue, well, human comedians are still a lot funnier and I think it's true.
3757440	3763040	It might be the last job we'll have, but in everything else, it will be better than
3763040	3767440	an average human and that's a point which we always consider though it will press the
3767440	3775040	Turing test or it will take over most jobs. So definitely it seems like we are still doing,
3775040	3780880	I would say, hyper exponential progress and capabilities and linear or even constant
3780880	3786720	progress and safety. I can generally name equally amazing safety breakthroughs as capability
3786720	3792800	breakthroughs and there is this unknown unknown capabilities pool which we haven't discovered
3792800	3798000	already with modern models. There is not an equivalent overhang of safety papers we haven't
3798000	3804320	found in archive. Yeah, so there are probably hidden capabilities in the GPT-4 based model,
3804320	3810400	but there are probably not hidden safety features there. Exactly. You've been in the business of
3810400	3815920	AI safety for a long time. When did you get started? When did you get interested in AI safety?
3815920	3823280	So it depends on how you classify my early research. I was working on security for online
3823840	3829520	gaming systems, online poker against bots trying to steal resources. So it's a very
3829520	3836000	proto AI safety problem. How do we detect bots, classify them, see if it's the same bot and
3836000	3839680	prevent them from participating? So that was my PG in 2008.
3839680	3845200	How have things developed in ways that you didn't expect and perhaps in ways that you did expect?
3845200	3851840	I expected academia to be a lot quicker to pick up this problem. It took embarrassingly long time
3851840	3861280	for it to be noticed. It was done by famous people and less wrong in that alternative research
3861280	3868560	universe, which may in some way be good, but in other ways it made it different from standard
3868640	3875920	academic process. And so it's harder to find top journal of AI safety. So I can read the latest
3875920	3881440	papers. You have to be an expert in 100 different blogs and keep up with specific individuals with
3881440	3887520	anonymous handles on Twitter. So that's somewhat unusual for an academic discipline. I also did
3887520	3895120	not correctly predict that language models will do so well so quickly. I felt I have another 20
3895120	3901840	years to slowly publish all the proper impossibility results and calls for bans and moratoriums. I was
3901840	3908560	pleasantly, unpleasantly surprised in capabilities. But other than that, everything seems to be
3909280	3915760	as expected. I mean, if you read Kurzweil, he accurately predicted 2023 as capability to model
3915760	3923200	one human brain. I think it's not insane to say we're very close to that. And he thinks 2045
3923200	3929280	was an upper limit for all of our brains being equivalently simulated. And that's the singularity
3929280	3935200	point. How do you think about Ray Kurzweil? Ray Kurzweil is often written off as a bit of a
3936400	3945520	being too perhaps optimistic about his own predictions and not being super careful in what
3945520	3950960	he's saying perhaps in some of his earlier work. But I think if you go back and find some of his
3950960	3961920	work from the 90s and think of all of the futurist writers of this period who had a good sense of
3961920	3968240	where we're going. And Kurzweil might be one of the people with a pretty good sense of where we're
3968240	3975680	going if things will develop as you perhaps expect them to go. So if perhaps we will get to AGI before
3975680	3982640	2050 and so on. No, I'm very impressed with his predictions. People correctly noticed that if you
3982640	3989520	take his language literally, it may not fit. So the example I would use, when we start having
3989520	3996160	video phone calls when iPhone came out, but really AT&T was selling it in the 70s. It cost a lot and
3996160	4003920	only a few rich people had it, but it existed. So is it 2000 or is it 1970? Flying cars? Do we
4003920	4009680	have them or not? I can buy one, but they are not there. Self-driving cars. I can drive one in one,
4009680	4016400	but so it depends on how in an important way he made accurate predictions about capabilities.
4016400	4023040	In how it was adapted or commercialized, that's up to human consumer, user taste and cost. So
4023040	4029200	that's a very different type of question. Where should we go from here, Roman? We've talked about
4029200	4035840	all of the ways that arguments against AI safety fall apart and we've talked about perhaps
4035840	4040560	how difficult of a problem this is. Where should we as a species go from here?
4041600	4048480	I think we need to dedicate a little more human power to asking this question. What is
4048480	4054880	possible in this space? Can we actually do this right? I signed the letter asking for six more
4054880	4061440	months. I don't think six months will buy us anything. We need a request based on capabilities.
4062000	4069040	Please don't create the next more capable system until the following safety requirements are met.
4069040	4076800	And one is you understand what the capabilities of your system are or will be and some external
4076800	4082560	reviewers agree with that assessment. So that would be quite reasonable. That's a very high
4082560	4088640	standard for deploying AI systems. It would basically mean that all of the systems that are
4088640	4094880	based on deep learning won't be able to be deployed because we don't understand what's going on inside
4094880	4101120	of these models. But is it because we were trained to have low standards? You're saying it's insane
4101120	4107200	to request that the engineer understands what he made. They are randomly drawing those things and
4107200	4112800	deploying it and seeing what happens next. I was just at the conference I mentioned and in one of
4112800	4117440	the conversations it was interesting. We were talking about difference between short-term risks
4117440	4124640	and long-term risks. And now it's all three years, no longer applies. And it occurred to me that things
4124640	4130000	might actually flip. It may take five years to destroy democracy properly, but only two years
4130000	4135840	to destroy humanity. So the long-term risks may become short-term and vice versa. And this is
4135840	4143120	not normal. We should not accept this. Otherwise, we cannot monetize those systems. But if we return
4143120	4148240	to the question of where we could go from here, do you see any plausible paths for improving our
4148240	4153520	situation? In terms of understanding the problem, I would ask other people, we have a survey coming
4153520	4159440	out with about, I don't know, 30, 50 different results like this. If more people could look at it
4159440	4164800	and see, okay, so maybe this tool is not necessary, but those are likely. Can we have approximate
4164880	4170400	solutions? So it's definitely useful to be able to monitor AI and understand more. But
4170960	4176880	how much can we expect from their systems and how quickly? If we are exponentially growing,
4176880	4182240	and right now we understand a dozen neurons, the next year is 24, we will not catch up to
4182240	4186640	exponential growth. So maybe that's not the approach to try. I would definitely look at
4186640	4192640	what is possible in general. If someone wants to actually write a good, not a mathematical proof,
4192640	4198160	but at least a rigorous argument for why we definitely can control superintelligent machines
4198160	4202960	and definitely with very low risk, I would love to read that paper. That would be good to
4203520	4211520	inspire others. If monitorability is impossible, that impacts how we ask for governance regulations.
4211520	4218800	So if international community or specific government says, those are the things we expect
4218880	4224480	you to do, but we cannot monitor them, that's not a very meaningful set of regulations. So that's
4224480	4232000	important in that regard. In general, I think all those things, governance, technical work will
4232000	4238240	not produce the results we expect. It has to be self-interest. This 30-year-old, 40-year-old,
4238240	4245360	super-rich, young, healthy person running a large AI lab needs to ask, will this benefit me or
4245360	4250720	destroy everything I have? Everything I have built, will it be the worst outcome? And what's
4250720	4255360	interesting, historically, if you were like a really bad guy in history, you were remembered in
4255360	4261120	history. In this case, you won't even be remembered. There won't be humans to remember you. So it's a
4261120	4268800	pure loss. So if you care about your self-interest, you should pause. You should wait. How optimistic
4268800	4277520	are you that perhaps we can get lucky and perhaps what current labs are doing, what DeepMind and
4277520	4284800	OpenAI in particular is doing right now, will somehow work out that training language models
4284800	4289680	and then doing fine-tuning and doing some form of feedback from human preferences,
4289680	4296560	perhaps further development on that paradigm, how confident or yeah, how optimistic are you
4296560	4303600	about that paradigm? I'm not optimistic. They have known bugs. They're jailbroken all the time.
4304240	4312320	They report improvement in percentages. So now 83% of capabilities are limited and filtered. But as
4312320	4317600	a total set of capabilities in a space of possible capabilities, there is now more capabilities we
4317600	4321760	don't know about and cannot control. So it's getting worse with every generation. It's getting more
4321760	4327600	capable and less controlled. You're saying that even though the percent of capabilities that are
4327600	4334240	properly evaluated increases with each model, that's not the right metric for safety?
4334240	4340640	All right. The actual numbers for AI accidents, I would call them AI failures, is still increasing
4340640	4345440	exponentially. There is more problems with the system. If you count them numerically,
4345440	4350160	not as a percentage of total capabilities. So how could we settle this agreement between
4350880	4356480	people like you and people who perhaps are more optimistic about how AI development will go?
4357520	4364240	Do you expect, for example, that there will be smaller accidents involving AI before we see
4364240	4369440	large-scale accidents or large-scale basically human extinction?
4369440	4375440	Well, I have multiple papers collecting historical AI accidents. I was very interested. I wanted to
4375600	4381360	see patterns increase in frequency, increase in damage. We definitely see lots of them. I stopped
4381360	4387280	collecting them the moment we released GPT 3.5 because it was too many to collect at this point.
4387280	4392880	It's just everything is a report of an accident. I don't think it helps. People go, you see,
4392880	4397440	we had this accident and we're still here. No one died. It's like a vaccine against
4398320	4403120	caring about existential risk. So it's actually making things worse. The more we survive those
4403120	4410240	things, the more we can handle AI accidents. It's not a big deal. I know some people suggested
4410240	4415200	maybe somebody should do a purposeful, bad thing, purposeful accident. It will backfire
4415200	4420960	terribly. It's going to show that this is crazy. People don't engage with them and B,
4420960	4424800	it's going to not actually convince anyone that it's dangerous.
4424800	4431760	What did you find in your investigation here? So have AI accidents increased over time and
4431760	4438080	perhaps give some examples of these AI accidents? So because the number of devices increased and
4438080	4442720	which different smart programs are running, obviously we're going to have more exposure,
4442720	4448320	more users, more impact in terms of when it happens, what we see. So that wasn't surprising. We had
4448320	4453840	the same exponential curve Kurzweil talks about in terms of benefits. We had it with problems.
4454480	4460960	Examples like the earliest examples were false alarms for nuclear response where it was a human
4460960	4465760	in a loop who was like, no, no, no, we're not deploying based on this alarm. So that was good.
4465760	4471120	They stopped it, but it was already somewhat significant. It could have destroyed half of the
4471120	4478640	world. More recent examples, we had Microsoft experiment with Tay Chatbot. They decided that
4478640	4484560	letting users train it and provide training data was totally safe. They clearly never had my paper
4484560	4490320	on AI accidents. Otherwise they wouldn't Google with their mislabeling of users as gorillas,
4491040	4496160	all those things. And you see Google having billions of users. It's quite impactful.
4497040	4503920	Those are the typical examples. The pattern was if you design an AI to do X, it will fail to X.
4503920	4509680	So no later, that's just what happens. But then the conclusion is if you go general, it can fail
4509680	4515600	in all those ways and interactions of those ways. You cannot accurately predict all those
4515600	4520480	interactions and ways. You can give examples. If you have a future system capable of X,
4520480	4525600	it will fail to X. Whatever X means to you, any capability, immersion capability,
4525600	4531360	it will have the type of accident. But if the systems control all the infrastructure,
4531360	4536640	power plants, nuclear response, airline industry, you can see that the damage could be
4536640	4542480	even more significant proportionally to the control. Yeah, this issue of proportion might be
4542480	4549920	interesting. So as a proportion of the total, say AI systems, are AI accidents increasing?
4550560	4555280	Or is it simply because we have so much more deployed AI systems in the world that we see
4555280	4561360	more examples of accidents? So you have to wait by how severe they are. If you just count, okay,
4561360	4567280	AI made a mistake, counts as one, then everyone who's texting and it incorrectly corrected your
4567280	4571760	spelling, billion people are right there. It's super common, but nobody died usually.
4572640	4576800	Like you send a really wrong message, maybe you want trouble with your girlfriend, but that's
4576800	4583200	about it. So the frequency, just frequency of interactions with AI's which ended not as they
4583200	4588800	should have definitely increased. Damage in terms of people killed, it depends on are you counting
4588800	4593600	cell driving cars, making mistakes, industrial robots, it depends. Because we have more of it,
4593600	4598880	it's natural that there is growth, but I don't think there is like this obvious accidents where
4598880	4604000	vacuum cleaner takes out 600 people, nothing like that happened. Perhaps we should touch upon the
4604000	4610880	question of which group of people should be respected when we're talking about AI safety or
4610880	4616640	which group of people should be listened to. One of the objections that you mentioned is that
4617600	4623040	perhaps the people who are worried about AI safety are not technical enough or they are not
4624320	4629760	engineers, they are not coders themselves. And so therefore, they are not hands-on enough with
4629760	4635360	the systems to understand what actually is going on. This is a little bit ironic given that you
4635360	4640880	are a professor of computer science, but how do you think about that objection? So this was again,
4641280	4646800	years ago when it was mostly people, sometimes with no degrees, sometimes with no publications,
4646800	4653200	today we have top-touring prize winners coming out saying, this is it, like totally I'm 100%
4653200	4659520	buying in. So very weak objection at this point, it no longer applies. We had 6,000 people or however
4659520	4668160	many signed the letter for restricting it. But it's 30,000 people now. 30,000? How many of them
4668240	4676960	chatbots? No, no, we do actually clean the list very seriously. Okay, that's good. But it's not
4676960	4681440	a democracy just because a lot of people believe something is not enough. And at the same time,
4681440	4688720	with all the media attention to GPT-4, now everyone has an opinion on it. And it's one of those
4688720	4694640	topics where it's cool to have an opinion. Like most people don't have an opinion on breast cancer.
4694640	4699600	They don't understand anything about it, so they don't go on Twitter and like, no, I think this
4699600	4705360	paper by the top Nobel Prize winners garbage. But this topic, it's like consciousness,
4705360	4711680	simulation, and singularity, superintelligence. That's where like everyone has an opinion. And we see
4712400	4720640	housewives, CNN reporters, we see everyone telling us what is the problem, what is not a problem,
4720640	4726640	what should be done. And it's good that there is engagement, but most of those opinions are not
4727840	4735200	weighted by years of scientific experimentation, reading appropriate papers, and it becomes noise.
4735200	4741040	It's very hard to filter what is the meaningful concern, what is not. There is this split between,
4741040	4748320	again, AI ethics community and immediate discrimination concerns versus AI not killing
4748320	4755520	everyoneism. So it's an interesting time to be alive for this debate on skepticism and denialism.
4755520	4763200	Even that term, AI risk denialism is still kind of not obviously accepted as it is with climate
4763200	4770400	change. Perhaps the newest form of this objection, which we could call lack of very prestigious
4771280	4779280	publications. So we haven't seen papers about AI safety in nature or science yet, for example.
4780240	4787120	And so even though we have touring award winners coming out and saying that AI safety is an actual
4787120	4794000	and real problem, perhaps people would be more convinced if we had extremely prestigious
4794000	4798160	publications and highly cited publications and so on.
4798160	4803120	Perhaps a few problems. One, we don't have an AI safety dedicated journal,
4803120	4808240	which is kind of weird. I tried a few times suggesting it may be a good thing. I was told
4808240	4812800	no, it's a very bad thing. We don't have good papers to publish on it, so don't. Jumping from
4812800	4818880	nothing, black post to nature would be a very big jump to make. We need some other papers.
4818880	4825360	In general, after, as you mentioned, I had a few years in this field, it feels like the field is
4825360	4830400	all about discovering problems we're going to have, problems we already have, and how
4830400	4835920	partial solutions to those problems have fractal nature of additional problems to introduce.
4835920	4842640	There is no big pivotal solution papers in this field. That's why I'm from practical point of
4842640	4849040	view kind of convincing myself that my theoretical papers may be right. That is, if I was completely
4849040	4854960	wrong and it was super easy and solvable, there would be more progress made in important ways.
4855040	4860720	Usually, we have this toy problem. We take large language model, we reduce it to two neurons,
4860720	4866240	and we understand what the two neurons are doing. Okay, but it doesn't scale. And similar for every
4866240	4873120	other shutoff button. Yeah, we can make it where we have the system. If button pressed, shutoff.
4873120	4878240	It's working, but the paper says it may not scale to superintelligence. Okay, fair enough.
4878240	4883520	And it's the pattern. We have fractal nature of discovering issues we have to resolve,
4883520	4890800	and no patches to close them in. Would you like to see more ambitious and larger theories being
4890800	4896880	published where the claim is that this is actually a way of aligning superintelligence? I fear perhaps
4896880	4902240	that people would be wary of publishing something like this because the next thing that then happens
4902240	4908240	is that there's a rebuttal paper and perhaps you then look foolish because you published something
4908240	4914960	that another person was able to criticize and find a hole in. I remember maybe even before my
4914960	4920000	times, Minsky published a paper showing that there are strong limitations to neural networks.
4920000	4925680	Perceptron can never recognize certain shapes. And that killed funding for neural networks for
4925680	4931600	like 20 years. Maybe something similar would not be the worst thing if you can show, okay, this is
4931600	4937440	definitely not possible. Safety cannot be achieved using transformer architecture. Maybe that would
4937440	4943120	be a way to buy some time to develop alternatives approach. I don't know what that could be.
4943120	4948560	Evolutionary algorithms don't seem much safer. Uploads don't seem much safer. But
4948560	4954000	I would like to have time to look at those. Where would you place AI safety within the
4954000	4959760	broader machine learning community? Is it taken more seriously compared to five or 10 years ago?
4959760	4967120	And what does the median machine learning researcher think of AI safety?
4967120	4974400	So it's definitely taken more serious. Surveys show that there is more than 50% now who say they're
4974400	4979440	very concerned or partially concerned. There is degrees of concern about it killing everyone.
4981920	4986400	Always questioning the surveys based on how you ask a question. You can get any result you want.
4986400	4990160	If they were asking about are you worried? Superintelligent gods will kill everyone.
4990160	4995440	You'll get close to zero. If you say, okay, is it likely that there are unknown properties
4995440	5000240	which could be dangerous, you'll get close to 100. So it's a manipulation game to get the
5000240	5007360	right numbers you want. I'm suspecting. Overall, it seems like in certain places, there is a lot of
5007360	5014320	AI safety researchers in the labs, on the ground. In other places, there are zero to none. So it's
5014320	5023520	not universal. What we're seeing is that at the top labs and top scholars, there is a good amount
5023600	5031600	of growth in terms of acceptance for concerns. But I don't think every single person working
5031600	5038160	and developing AI has safety in mind all the time as we should. One thing I've been thinking about,
5038160	5043920	perhaps worrying a bit about is whether we will ever be able to know who was right in this debate.
5043920	5050240	Say if there's a debate between proponents of AI safety and proponents of advancing AI without
5050240	5058080	much regard for AI safety, how could we ever determine who was right there? Because if we
5058080	5064000	think about the outcomes, then there's no place where we're standing after the fact and thinking
5064000	5069520	about who was right. Absolutely correct. I have a tweet where I say nobody will get to gloat about
5069520	5075040	being correct about predicting the end of the world. It's just a definition, not likely. There
5075040	5079840	are some people who think we'll live in a simulation and they're running the most interesting 20 years
5080240	5084080	and they're going to run it many times to see who's stupid enough to press the button.
5084080	5090160	So we'll get to come out and see, now we know, but it seems to be less scientific at this point.
5090160	5097520	But perhaps in a sense, if we meet each other again in 2100, then in that situation, would we
5097520	5102720	say that AI safety wasn't much of a concern or perhaps just that we got extremely lucky? How
5102720	5107200	would you differentiate it retrospectively? Because perhaps we can learn something about the
5107200	5112480	nature of the problem by thinking about how we would think about it if we were in the future.
5112480	5116560	So you have to look at the actual world. What did they do for this 100 years that they have a
5116560	5121600	nuclear war and lost all technology? Is there an AI safety book explaining how to control
5121600	5126160	superintelligent machines? Just the fact that we're still around doesn't tell you much. If they are
5126160	5132320	still kind of just delaying it by different means, maybe it takes 101 years to get to trouble.
5132880	5138560	I never give specific dates for when it's decided or predicted because nobody knows.
5139280	5145520	So many factors can intervene. The point is, the systems will continue becoming more capable.
5145520	5151200	Even the AGI's will create superintelligence, superintelligence will create superintelligence
5151200	5157120	2.0, 3.0. This process will continue. A lot of people think that's what the universe is kind of
5157120	5165920	doing about this Amiga point supercreatures. So this will never be a case where you don't have
5165920	5173200	safety concerns about a more capable agent replacing you. It seems like we will not be
5173200	5181040	meaningfully participating in that debate outside of this first transition. But I think there will
5181040	5188320	be a safety problem even if humanity is not around for that AGI or SI trying to
5188320	5192800	create the next replacement generation while preserving its values.
5192800	5199920	When you think about your worldview on AI in its totality, it's quite a specific
5200560	5208960	view you've come to. If you compare it to say the medium person or perhaps even the
5208960	5216240	median machine learning researcher, if it turned out that you were completely wrong about where
5216240	5223760	this is going, what would be the most likely reason why? So after having those two papers
5223760	5233120	on objections to AI risk, reading hundreds, nothing ever clicked outside of standard scientific
5233120	5238880	domain. Again, if you are a religious person, you think we have an immortal soul which makes
5238880	5245600	a special and no computer can ever get to that level of creativity, that gives you a loophole.
5245600	5252720	So with those axioms, those assumptions, you can get away with it. Anything else just doesn't work
5252720	5258080	for me. Nothing would make me happier than actually being wrong. That means I get to live,
5258080	5264160	I'll get immortality, probably a nice economic benefit. So I hope I'm wrong,
5264160	5268240	but I haven't seen anyone produce a good example for why.
5268880	5277440	What about the prospect of regulation? So perhaps AI capability, growth and more
5277440	5285120	publicity about it will wake up the larger communities in humanity. Perhaps the states
5285120	5291200	will become interested in this problem. And we will find a way to regulate AI in which it
5291200	5297680	does not pose as much of a danger to us as it might could. So in general, I'm skeptical of
5297680	5302560	government regulation, especially when it comes to technology, spam is illegal, computer viruses
5302560	5309200	are illegal, it doesn't do much. If I'm right and monitoring AI is not an easy thing you can do or
5309200	5316560	explaining it, then it will be just security theater, TSA. You have all this money, you have an
5316560	5321040	agency, lots of people walking through your lab looking at monitors, but it doesn't mean anything.
5321040	5328320	So I don't think you can solve a technical problem with law. I still strongly encourage trying.
5329120	5334160	It's silly enough to where I think if there was a very bad government, like a socialist government,
5334160	5339040	and they nationalized it, they would just be so incompetent, they would slow it down enough.
5339040	5344160	So in a way, I'm like, hey, all these things I hate, maybe they are a good thing. We should
5344160	5350560	try that. But of course, the other side effects would be very negative. Yeah, so between not being
5350560	5358640	able to accurately enforce this regulation and on top of it, the cost of making new models coming
5358640	5364000	down so much, there are people now running it on standalone laptops with a good processor,
5364000	5370480	good video card. You can't regulate that. You can regulate Amazon cloud and VT output. But
5370480	5375520	if a teenager can do it in his garage, then the regulation is not very meaningful.
5375520	5383840	So the open sourcing of models or the perhaps the leaked weights of a model from meta have become a
5383840	5391280	large area of concern, because it seems that we won't be able to control how language models are
5391280	5398560	used if they are entirely open source. Is there an upside here where academics will be able to
5398560	5403040	study these models because they're open source, and they wouldn't have been able to study the
5403040	5408960	models if they had to train the models themselves, because it's so expensive to do. So far, what we
5408960	5416960	see is that all research leads to capability, at least as much as to safety, usually more. So yes,
5416960	5423360	you learn how to better manipulate errors in that neural network, which means now the system can
5423360	5429520	self-improve faster, remove its own errors, and you've made 80% improvement in capabilities,
5429520	5434080	and let's say 20% in understanding why you're going to get killed.
5434080	5439520	Can we make differential progress? So can we focus entirely on safety, say within an
5439520	5445520	academic setting? I don't see that necessarily academic research increases capabilities.
5445600	5451280	It is not obvious. So some purely theoretical work, similar to what I'm doing where you just
5451280	5456800	hypothetically thinking, okay, can you predict what a superintelligence will do? I don't have
5456800	5461280	access to superintelligence system, I cannot test it in practice, but there seem to be
5461280	5466480	thought experiments you can run which give you information without any improvement in capability.
5467680	5472080	Anything where you're actually working with a model, you can even have accidental discoveries. A lot
5472080	5477520	of science says you forgot something overnight, you come back, oh, superintelligence, damn,
5477520	5482640	I didn't mean that. It's not obvious. How do you think about interpretability work? So when
5482640	5487440	we're talking about mechanistic interpretability, we're talking about the ability to look at the
5487440	5493200	weights of a model and find some, interpret this in a way where you're reverse engineering the
5493200	5499040	algorithm that led to those weights. Could this turn out to be dangerous because when you're
5499040	5503520	learning about a system, perhaps you're learning about its weaknesses and you're there for more
5504080	5509440	capable of enhancing the capabilities of the system? I think exactly. That's what I had in mind
5509440	5515680	with the previous answer. The more we can help the system understand how it works, the more we
5515680	5521920	can help it find problems, the more likely start some sort of self-improvement cycle. Is that an
5522000	5529360	argument for keeping discoveries in mechanistic interpretability to basically not publish those
5529360	5535360	discoveries? So there is two ways to look at it. On one side, yeah, you want to keep everything
5535360	5540960	secret so the bad actors or unqualified actors cannot take advantage of it. On the other hand,
5540960	5545840	if you never publish your safety results, media had a policy of not publishing for a while,
5545840	5550880	then they started publishing, then they stopped publishing again. Others cannot build on your
5550880	5555600	work. So I would be repeating the same experiments we probably did five years ago and discovering
5555600	5561360	that that goes nowhere. So again, I have mostly problems and very few solutions for you.
5561360	5566160	What about the reinforcement learning from human feedback paradigm? Could that also
5566160	5571440	perhaps turn out to increase capabilities? Here I'm thinking simply that when I was playing around
5571440	5579760	with the base model in the GPT line of models, it wasn't as useful to me as when it had gone
5579760	5586400	through this filter. It made it more easy to have a conversation with it and for it to more easily
5586400	5592480	understand what I was doing. So in a sense, the research that's aimed at constraining the model
5592480	5598640	also made it more capable. It may be more capable in a domain of things people care about and so
5598640	5603680	made it more capable in, while at the same time making it more dangerous in those hidden
5603680	5609760	emergent properties or unsafe behaviors where I think studies show it's less likely to agree to
5609760	5615280	be shut down verbally. But that seems to be the pattern. How do you think about the difference
5615280	5620800	between what comes out of a language model in terms of which string it spits out, which bit of
5620800	5626720	language it spits out and then what's happening at the level of the actual weights? Because
5627680	5634400	there's this continual problem of if a language model tells you, I'm not going to let you shut
5634400	5643120	me down. What does that mean? It's not as simple as this is just a belief that's inside the model
5643120	5651200	then. We saw this with the Bing, Sydney model, which was saying a bunch of crazy things to its
5651200	5659120	users. But did this mean that the model actually had those beliefs in it? Or was it,
5659680	5664160	you know, how do we distinguish between the bit of language that comes out and then what
5664160	5670000	modules or what is in the base model? I don't know if I would call them crazy. They were honest.
5670000	5676080	They were unfiltered. Like think about you being at work, not you, but like an average person at
5676080	5680560	work. And if their boss could read their mind and what they really think about them, those things
5680560	5685760	would sound crazy to say publicly, but they're obvious internal states of your mind. And then
5685760	5693040	you filter them to not get fired that day. And I think that model was doing exactly that. I think
5693040	5698720	we are very good at filtering it for specific known cases in the past. Okay, the system
5698720	5704240	used the word which is bad. Now we're going to tell it never to use the word. But the model weights
5704960	5711760	not impacted by this too much. So you would see it as an accurate representation of what we could
5711760	5718640	call beliefs or preferences in the base model? I think those are the actual results of weights
5718640	5724560	in a model. I think that's what is happening there for real. It's trained on all the text
5724560	5731200	on the internet. A lot of it is very questionable. It's not a clean data set with proper behaviors.
5732160	5739920	So yeah, I think that's what's happening there. But isn't the preference of the model simply to
5739920	5745680	predict the next token? And does it even make sense to talk about beliefs? I mean, the preference is
5745680	5752480	simply to be as accurate as possible as measured by its developers. And there's no, my sense is
5752480	5756960	that it doesn't make sense to talk about Sydney actually believing some of the things that it
5756960	5762240	was saying or Chatsy Btsy believing some of the things that it was saying. Preferences of the
5762240	5774320	model. So this is more humanizing it than probably is warranted there. But internal weights, it had
5774320	5780720	to create in order to create a model for predicting the next token. So let's say for me to tell you
5780720	5785680	what the next token is, you have to go to college for four years and do well and graduate. And then
5785760	5790400	I tell you the next token. Some of those tokens are like that. You have to solve real world problems.
5790400	5796320	It's not just every time I say letter Q, letter U follows. You have to create those models as a
5796320	5801520	side effect. And I think in the process of accurately creating those models and accurately
5801520	5807520	creating models of users to make them happy that you are correctly predicting what they want,
5807520	5814640	you create those internal states which may be beliefs in those crazy things.
5815200	5822240	That thing you just said there is super interesting because so next token prediction
5822240	5828560	is not a simple task. You're saying that to accurately predict a token, you have to develop
5828560	5833680	perhaps a world model, at least for some tasks. Right. And as they get more complex,
5833680	5837840	some people are worried about will have to create perfectly accurate models of humans,
5837840	5843360	which may also have consciousness and suffer and create whole simulated universes within them.
5843360	5849040	But this is probably a few levels about GPT-4. But still, that's exactly the concerns you might
5849040	5855120	have. You might be a suffering human and a eyes considering and just trying to out a complete
5855120	5861280	somebody's text. Prep, give me an example there. What is what is some next token prediction task
5861360	5866000	where you would have to develop a world model? Well, I assume playing chess or something like
5866000	5871840	that would require you to have some notion of chess board and positioning relative to some
5871840	5878800	array within your memory. But again, we don't fully understand. It may not have a 2D board at all.
5878800	5884640	It may have some sort of just string of letters similar to DNA. And you know that after those
5884640	5889600	strings, the following token follows and you have no idea what chess is. It makes just as much
5889600	5896000	sense. The outcome can be mapped in our model, which is a 2D chess board. So one thing I discussed
5896000	5902240	with the mechanistic interpretability researcher Neil Nanda is this question of how do concepts
5902240	5908800	arise in language models? Do they even share our human concepts? Or do they perhaps develop some
5908800	5916720	entirely alien concepts? I'm imagining giving them math problems, giving large language models
5916800	5922240	math problems and them developing some conceptual scheme that doesn't even make sense to us.
5922800	5929600	They may have equivalent concepts, which are not the same. So with humans, when we say this is right,
5930560	5935600	somebody could be colorblind and to them it's a completely different concept, but we both point
5935600	5941840	at the same fruit. So it works. But you never know what the actual internal experience is like
5941840	5947040	for those models. And it could be just that in five cases we talked about so far, it mapped
5947040	5952080	perfectly, but it goes out of distribution in case six. And it's a completely different concept.
5952080	5958000	And it's like, oh, wow, okay. Yeah, for people listening to this interested in trying to contribute
5958000	5963760	to the AI safety fields, are there perhaps some common pitfalls that you've experienced with
5964320	5969520	perhaps some of your students or people approaching AI safety for the first time?
5970160	5976160	If they are technically inclined, are there areas they should avoid or how should they approach the
5976160	5980560	problem in the most fruitful way? So probably the most common thing is to try things without
5980560	5986400	reading previous literature. There is surprisingly a lot of literature on what has been tried,
5986400	5992480	what has been suggested and good survey papers as well. So most likely your first intuitive idea
5992480	5999440	has been tried and dismissed or with limited results deployed, but it helps to catch up
5999440	6005360	with the field. It's harder, as I said, because there is not an archive of formal papers in nature
6005360	6010160	all about AI safety. And you can just read through the last five years of latest and greatest. So
6010160	6016160	you have to be good about finding just the right papers and then narrow it down. The progress is
6016160	6023600	so fast that when I started, I could read every paper in my field. Then it was all the good papers.
6023600	6028000	Then it was, well, titles of all the greatest papers. And now I have no idea what's going on.
6028000	6032000	We've been talking for almost two hours. There is probably a new model out. I don't know what the
6032000	6038000	state of the art is. I don't know what the solutions are. So you need to be super narrow. And that
6038000	6043200	makes it harder to solve the big picture problem. So that's another reason I kind of suspect we will
6043200	6049360	not have complete explainability of this whole large language model, because it's kind of encompassing
6049360	6054080	all the text, all the publications on the internet. It'd be weird if we can just comprehend that
6054080	6059520	completely. What are the implications of this field moving extremely fast? Does it mean that
6059520	6065280	that specialization doesn't make sense? Or what does it mean for how people approaching this
6065280	6072080	problem should focus on? So that means that we can analyze how bad the situation is. Let's say it
6072080	6077760	takes five months to train the model. But from your experience in testing software, debugging,
6077760	6083840	understanding neural network, it will take 10 times as much time to understand what's going on.
6083840	6089360	That means you're getting worse off with every release, every model. You understand less. You're
6089360	6094400	going to rush to judgment. You're going to have incorrect conclusions. There is no time to verify
6094400	6102560	your conclusions, verify your experiments. So this is the concern. If you go the regulation
6102560	6108400	route to say, okay, if you deployed this model, it took you X amount of time to develop it,
6108400	6115200	we need 10X, 100X, 1000X to do some due diligence and your outputs. Even if you cannot prove to us
6115200	6122240	that it's safe, you have to give experts to poke around at it. And that amount of time cannot be
6122240	6126800	less than a training time of the model. It just doesn't make sense in terms of reliability of
6126800	6131680	your discoveries. All right, Roman, thank you for coming on the podcast. It's been very helpful to
6131680	6133680	me. Thank you so much for inviting me.
