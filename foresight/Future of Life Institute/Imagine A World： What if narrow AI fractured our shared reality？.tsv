start	end	text
0	2500	On this episode of imagine a world.
2500	10000	My best guess is that a guy will progress much more slowly than I have it progressing in my story.
10000	17500	And my best guess is that we do survive because I progress is much more slowly from my perspective.
17500	26500	It's extremely contrived for a GI to develop even as fast as it does in this story and be handled well enough cautiously enough.
26500	31500	Thoughtfully enough that we have more than a fraction of a percent chance of survival.
36500	41500	Welcome to imagine a world, a mini series from the Future of Life Institute.
41500	49500	This podcast is based on a contest we ran to gather ideas from around the world about what a more positive future might look like in 2045.
49500	55500	We hope the diverse ideas you're about to hear will spark discussions and maybe even collaborations.
55500	61000	But you should know that the ideas in this podcast are not to be taken as FLI endorsed positions.
61000	64000	And now, over to our host, Guillaume Reason.
75500	79000	Welcome to the imagine a world podcast by the Future of Life Institute.
79000	81000	I'm your host, Guillaume Reason.
81000	88000	In this episode, we'll be exploring a world called Hall of Mirrors, which was a third place winner of FLI's World Building Contest.
88000	92000	Hall of Mirrors is a deeply unstable world where nothing is as it seems.
92000	98000	The structures of power we know today have eroded away, survived only by shells of expectation and appearance.
98000	103000	People are isolated by perceptual bubbles and struggle to agree on what's real.
103000	107000	Despite all this, things are generally going okay, for now.
107000	112000	This is partly due to this world's particularly slow and modest development of AI technologies.
112000	117000	AI tools here are still dominated by extensions of today's fundamentally narrow systems,
117000	121000	with the one true AGI being developed under heavy quarantine.
121000	128000	There are a number of reasons for this slow progress, including high computational costs and poor funding due to politicization.
128000	132000	This team put a lot of effort into creating a plausible, empirically grounded world,
132000	135000	but their work is also notable for its irreverence and dark humor.
135000	140000	I can safely say that it's the only winning world where you could see virtual celebrity Tupac List
140000	144000	perform at a luxury war-themed amusement park run by the Taliban.
144000	146000	Needless to say, there's a lot going on here.
146000	151000	I was excited to get a look into the minds behind this particularly brimming and erratic world.
151000	157000	Our guest today is Michael Vasser, one member of the three-person team who created Hall of Mirrors.
157000	163000	Michael is a futurist, activist, and entrepreneur with an eclectic background in biochemistry, economics, and business.
163000	169000	He served as president of the Machine Intelligence Research Institute and is co-founder of Metamed Research.
169000	176000	His other team members were Matia Franklin, a doctoral student studying AI ethics and alignment at University College London,
176000	180000	and Bryce Heidi Smith, who has worn many hats from fortune-telling to modeling,
180000	183000	and now has a focus on finance and policy research.
183000	185000	Hey Michael, great to have you with us.
185000	188000	Great. Good to speak to you.
188000	193000	So I'm curious how the three of you on your team came to work on this project together.
193000	199000	So I've known Bryce for a very long time, and when the project was starting up,
199000	204000	there was a call for collaborations, and I tried talking to a bunch of people.
204000	209000	And Matia and I had, you know, the most productive conversations.
209000	216000	But like the overall project was mostly my vision, and Matia did some level of editing,
216000	220000	and Bryce did the fiction and art.
220000	223000	Cool. So did Bryce make the music that was accompanying your session?
223000	224000	Yes.
224000	227000	Cool. Yeah, I really enjoyed your music and the short stories as well.
227000	228000	He did a great job with those.
228000	232000	I mean, it's the closest thing to a super intelligence that we have around for now.
232000	234000	Endurable.
234000	237000	Well, what was it like for you guys to do this project together?
237000	240000	Did you learn anything yourself in the course of it?
240000	242000	I mean, I had a lot of fun.
242000	245000	It helped me to concretize some of my thinking.
245000	250000	Some, I feel like the basic sense of where I think we're going or would like to go
250000	256000	has been reasonably stable in my head since GPT-3 came out,
256000	262000	and hasn't drastically changed since GPT-2 and COVID.
262000	264000	Yeah.
264000	268000	What were some of your biggest sources of inspiration when you were working on this together?
269000	275000	I don't think my thinking on this is significantly influenced by stories or books
275000	277000	or music or what have you.
277000	283000	I think it's basically just coming from looking at what the technology can do
283000	288000	and spending the last 25, 30 years obsessively thinking about history and the economy
288000	294000	and social sciences and making some effort to understand the technology.
294000	297000	I'm certainly not a top expert in actually understanding the technology
297000	304000	while I will humbly claim to be a top expert in understanding the history of technology
304000	306000	as it relates to economics.
306000	309000	Yeah. Well, you do have this deep professional background.
309000	313000	Can you say a little bit about how your experience in other fields
313000	317000	and kind of working through all this has influenced how you see the future?
317000	321000	I mean, in terms of professional background,
321000	329000	molecular bio, I studied in university and it doesn't really inform this very much.
329000	334000	I have a lot of thoughts about cool things that could be done with molecular bio,
334000	340000	and now that GPT-4 is performing at a high school national championship level
340000	344000	without major upheaval enhancements.
344000	347000	I'm confident that I can do a lot more of that stuff,
347000	350000	and also AlphaFold is very cool, and mRNA tech is very cool.
350000	353000	So I think there's enormous opportunities now for bio.
353000	359000	Getting an MBA gave me an opportunity to exist in the business office world for a while,
359000	366000	and that certainly is necessary without having interacted with corporate hierarchies.
366000	369000	One doesn't know what corporate hierarchies are like at all.
369000	372000	There's very effective disinformation and propaganda about that.
373000	379000	I think mostly I've just read a lot in directions that seemed like they could be helpful
379000	382000	over maybe a 25, 30-year period.
382000	387000	Yeah. What sorts of insight did Bryce and Machita bring to the project?
387000	393000	So the actual stories were very cool, and the music was very cool.
393000	397000	And Bryce wrote those mostly by himself.
397000	404000	And there were some back and forth about what sorts of things were maybe too over-the-top
404000	408000	or too fun and silly to include in the story.
408000	413000	And it's just good to talk to people about things and develop the ideas together.
413000	418000	And certainly Bryce has been enormously central to developing my understanding of the world
418000	420000	in general over the last decade.
420000	422000	And what about Machita?
422000	426000	Machita? I mean, mostly just discussing what I can get away with.
426000	432000	In terms of when telling a story, what is too weird, what is socially acceptable enough
432000	439000	that people can understand it as relatively limited in French distance from normal, thoughtful people?
439000	440000	Yeah.
448000	451000	In some ways, this world is kind of a caricature of the present.
451000	454000	We see deeper isolation and polarization caused by media
454000	458000	and a proliferation of powerful but ultimately limited AI tools
458000	461000	that further erode our sense of objective reality.
461000	464000	A deep instability threatens.
464000	468000	And yet on a human level, things seem relatively calm.
468000	472000	It turns out that the stories we tell ourselves about the world have a lot of inertia,
472000	474000	and so do the ways we live our lives.
474000	478000	I had a hard time picturing those individual lives among all the wild happenings of this world,
478000	481000	and I wanted to hear more about that human perspective from Michael.
483000	485000	What's it like to live in this world you've made?
485000	489000	Well, it's going to be very different in different media bubbles.
489000	493000	The biggest media bubble by far is going to be Chinese,
493000	500000	and the successor to contemporary Chinese Communist Party politics
500000	505000	will mean something more neo-confusion than China has been recently.
505000	511000	But done with capacities that no one's ever had the opportunity to bring to the table,
511000	517000	so you can just spend so much more time on filial piety and cultivating then
517000	520000	when all of the real work has been automated
520000	525000	and when you have machines that are in some ways superhuman watching your every move
525000	529000	and helping you along to express gratitude to your parents
529000	531000	in the most ritually prescribed manner.
531000	534000	Other people have different experiences.
534000	540000	There are probably hundreds of millions of people trapped in pornographic universes
540000	543000	and effectively mind controlled by AI.
543000	547000	That would maybe be the second largest demographic, if I really think about it.
547000	551000	And there are lots and lots of people, like the ones we discussed at the end,
551000	558000	living in old age homes and having their experiences mediated through a somewhat more tasteful
559000	565000	but still like relatively liberal and relatively cultivated sense of benevolence.
565000	570000	But the prospect of AGI coming online at all changes that.
570000	577000	In some sense, these stories were intended to point at the extreme instability of the world that I produced.
577000	582000	So we have one story about producing a piece of transhuman music
582000	590000	and one story about consuming it despite the cautions of the companies around AGI
590000	600000	under the basically reasonable assumption that music was not existentially dangerous under normal circumstances.
600000	605000	Yeah, so you're referring to, in your world, there's this system that DeepMind has called siren,
605000	607000	which is I think kind of the only AGI in your world.
607000	609000	It's under very tight wraps.
609000	612000	Everyone's really carefully screened and there's follow-up monitoring,
612000	615000	even if they just hear the music that it produces.
615000	620000	This system has also written some books on a few topics that have been carefully curated.
620000	624000	I'm curious what broader impacts siren's existence has on your world,
624000	626000	given kind of how cloistered it is.
628000	630000	I mean, none by design.
630000	634000	Allowing it to have more than the tiniest amount of impact on the world
634000	636000	would be allowing the world to end almost immediately.
636000	639000	So instead, yeah, your world really dives into narrow AIs.
639000	642000	So these are systems that are very good at just a few specific tasks,
642000	644000	like playing chess or driving a car.
644000	648000	No, much broader than that, like the AIs we have today, like GPT,
648000	655000	which are at least pretty good at most things that we tend to think of as intellectual tasks
655000	661000	and very, very, very good at most things that we tend to think of as perceptual
661000	667000	or as extremely rehearsed short-term actions without a lot of context sensitivity.
667000	668000	I see.
668000	671000	So these are like, you know, souped up, narrow AI systems.
671000	676000	They're still not AGI's, but they're kind of the most effective extension
676000	679000	of today's technologies like chat GPT and things like that, as you're saying.
679000	684000	They're general enough that for the vast majority of the world's population,
684000	689000	they probably are vaguely thought of as generally intelligent,
689000	694000	like the vast majority of the world's people probably don't understand very well
694000	697000	the differences between them and AGI's.
697000	702000	And that's probably part of why there's essentially no funding or work on AGI outside of DeepMind.
702000	703000	Yeah, interesting.
703000	709000	And like broadly speaking, they're sufficient to produce some level of relatively benign,
709000	714000	not totally impenetrable, but close enough, a global mind control system
714000	721000	that also contributes to not understanding the differences and also not pursuing AGI.
721000	727000	In some ways, I think that the fiction that my world most reminds me of
727000	731000	is probably Who Framed Roger Rabbit, where they have these tunes everywhere.
731000	733000	And the tunes can talk.
733000	735000	They have personalities.
735000	740000	They have something kind of like agency, but they don't seem to, for the most part,
740000	743000	have agency with any scale.
743000	748000	It's like an extremely rare, extremely dangerous thing for a tune like Judge Doom
748000	751000	to have agency with scale and scope.
751000	758000	And when they do, like Judge Doom, it's agency with an extremely inhuman focus, scale and scope.
758000	760000	So very potentially dangerous.
760000	766000	And the tunes are in some sense extremely cheap and disposable, easy to produce,
766000	768000	but in some sense immortal.
768000	773000	And the humans are like completely clueless about the glaring ways
773000	777000	in which the tunes' capabilities are less than human,
777000	781000	such as Roger Rabbit can only do things when it's funny,
781000	785000	but fairly clueless about the ways in which their abilities are more than human,
785000	788000	like they can survive having a piano dropped on their head.
788000	792000	I actually haven't seen that movie, but I'm really excited now to watch it with this metaphor in mind.
792000	794000	It's a really cool connection.
794000	795000	Yeah.
795000	798000	So one thing you say that these systems can do in your world
798000	802000	is basically replace all white collar workers in theory,
802000	805000	but you say this doesn't happen and you say basically, you know,
805000	810000	there are various reasons, political and personal, why humans are still employed.
810000	816000	I'm curious what kinds of work humans do and what it's like for these human workers in this situation.
816000	821000	So I think basically it depends on their organization,
821000	824000	but in the pretty large majority of organizations,
824000	832000	it's pure office politics and getting therapy from not peak human ability,
832000	838000	but good enough AI therapists to recover enough from the office politics
838000	843000	that they only kill themselves with like drug overdoses and the like
843000	848000	at maybe a third or a fourth the rate that they do in our world.
848000	854000	And maybe even less if AI enhanced medicine makes such drug significantly less deadly
854000	857000	and treatment significantly more effective.
857000	858000	Yeah.
858000	860000	Your world still has a ton of economic inequality,
860000	864000	but the actual quality of life that you describe is kind of universally pretty good.
864000	867000	Like travel has become really cheap and there's basically free energy.
867000	871000	It makes food distribution really trivial as people can kind of live wherever they want
871000	874000	and they have augmented reality, so it'll always look beautiful.
874000	877000	I'm curious, given all of these kind of unifying factors,
877000	879000	how people decide where to build their lives
879000	882000	and what kinds of goals they decide to pursue with them.
882000	887000	So the world that I'm thinking of, for the large majority of people,
887000	890000	they start exploring the world when they're children
890000	893000	and hopefully their parents take a lot of interest in them.
893000	896000	But if not, there's an infinite amount of attention freely available
896000	900000	from the web and from open source and commercial products.
900000	905000	And the decisions they make throughout their lives are almost entirely determined
905000	911000	by what sorts of commercial or open source products find them first, in a sense,
911000	918000	and build the sort of feedback loops that pull them into one or another bubble reality.
918000	920000	You have this interesting thread in your world
920000	923000	where families kind of become a currency
923000	927000	or a kind of wealth that people pursue more than monetary assets.
927000	930000	Can you say a little bit about what that looks like?
930000	932000	I mean, that's just being a normal person.
932000	935000	We've lost touch with it in, you know, late stage capitalism.
935000	940000	But even under normal capitalism, this was not confusing to anybody.
940000	945000	You know, the idea of trying to accumulate wealth
945000	948000	rather than trying to accumulate happy help,
948000	952000	the wise flourishing and mutually cooperative descendants
952000	957000	is like a really surprising thing to find an organism doing.
957000	962000	So thinking about some of the more unusual aspects of your world,
962000	966000	your world definitely had some of the wildest kind of one-off ideas in it
966000	967000	that we saw in the competition.
967000	970000	You have like the Taliban creates luxury war themed amusement parks.
970000	973000	You have elephants that are domesticated by CRISPR.
973000	977000	And you even have Kanye West creating a virtual reproduction of biblical Jerusalem.
977000	981000	I'm curious like what prompted these kinds of details to be included
981000	984000	and whether they're part of a larger theme for you that you were trying to convey.
984000	989000	So the biggest thing that I left out of the actual thing that Matija's influence
989000	998000	was a coup by the comedy party where basically in the 2032 election
998000	1002000	between AOC and Donald Trump,
1002000	1007000	the mainstream Democrats, which still basically control the media and the courts,
1007000	1012000	decide to allow a completely flagrant election fraud to control John Stuart
1012000	1015000	as a third party president.
1015000	1022000	And, you know, so that one I think Matija thought was too political, too controversial.
1022000	1025000	But I do think it's the sort of thing that could realistically happen.
1025000	1028000	Overall, where are these coming from?
1028000	1032000	I mean, some of them are just like extreme low hanging fruit,
1032000	1037000	things that a few college kids could throw together as a project
1037000	1044000	in a world with AI capabilities that I realistically expect to exist well before the 2045 deadline.
1044000	1047000	Yeah. So this is kind of just speaking to maybe like the chaos
1047000	1050000	and the power flying around the instability of things
1050000	1053000	and how the world is just going to get so much stranger.
1053000	1055000	Yeah, I don't think of it as a chaotic world.
1055000	1060000	The stories are super, super non-chaotic about people living very calm lives.
1060000	1067000	I see it as a world that's very, very unstable simply because it has even one AGI in it.
1067000	1073000	And like sooner or later, a more permanent solution is necessary
1073000	1079000	than just keeping its interest cyber focused and keeping people from noticing it very much.
1079000	1082000	To some degree, I'm just trying to show a picture,
1082000	1084000	because that's all you can do in a story like this,
1084000	1088000	but a picture where all of the pieces are scientifically well founded,
1088000	1091000	technologically, economically and politically well founded,
1091000	1094000	make sense and fit together fairly well.
1094000	1098000	I guess more than anything else, I'm trying to show people like the contest is trying to show people
1098000	1102000	that it is even possible to make a sincere, serious and competent effort
1102000	1106000	to depict a realistic but optimistic future.
1106000	1118000	Major changes are hacking away at the foundations of this world's systems.
1118000	1122000	The loss of shared reality and weakening of governmental structures, at least in the West,
1122000	1125000	seemed to strip humanity of a good deal of agency.
1125000	1131000	It's implied that we're being kept from destruction only by our tenuous control of this world's one true AGI.
1131000	1135000	At the same time, new approaches to things like education and social conflict
1135000	1139000	signal hope for building a more coherent and empowered humanity.
1139000	1146000	I wanted to hear more about how Michael saw this world approaching the changes and challenges that it faced.
1146000	1150000	You write that in America, like Microsoft, Amazon, Tesla and Walmart
1150000	1154000	are basically the only entities capable of large scale coordinated action anymore
1154000	1158000	and elected government officials really just enact change by influencing their supporters
1158000	1160000	rather than by pursuing any kind of legislation.
1160000	1162000	Most decisions are made locally.
1162000	1165000	Can you say a little bit more about how America's governmental systems
1165000	1167000	lose so much influence in your world?
1167000	1171000	I just see that as a continuation of the trend that we're already on.
1171000	1176000	When you look at COVID, the government took an unbelievably huge amount of oppressive
1176000	1181000	and authoritarian action that there probably won't be social or political support for
1181000	1184000	if there's another major event that calls for it.
1184000	1187000	It lost an enormous amount of public trust.
1188000	1193000	If you look at what the government did that was effective with COVID,
1193000	1196000	it basically boils down to printing enormous amounts of money
1196000	1203000	and providing certain types of encouragement to conform to a certain standard.
1203000	1206000	It's not that the government no longer matters.
1206000	1210000	It's just that popularity contests should be.
1210000	1214000	It's primarily a source of information about how to be popular.
1214000	1217000	Just like in our world, people mostly want to be popular.
1217000	1221000	They don't want it as much as in our world because they can always be popular with AIs.
1221000	1228000	But still, AIs are not fully satisfying as mental and social companions.
1228000	1234000	As this power switches over and flows towards tech companies gaining influence,
1234000	1236000	it becomes increasingly hard to track wealth.
1236000	1241000	In some ways, it also seems like things are just going on sheer inertia.
1241000	1244000	You have this great line in your submission that says,
1244000	1247000	a supermajority of the population has negative net worth
1247000	1250000	and continues to be allocated credit as a matter of economic policy.
1250000	1253000	You mentioned this instability of the world.
1253000	1255000	How long do you see it remaining stable?
1255000	1260000	Will these systems fall apart shortly after 2045 and you're imagining?
1260000	1265000	The way I'm imagining this, this is a fairly close to best case scenario.
1265000	1272000	My realistic best guess scenario would be that it's more than 70% likely,
1272000	1274000	maybe more than 80% likely,
1274000	1280000	that the system that I'm describing falls apart well before it gets to the point that I'm describing.
1280000	1283000	These are supposed to be optimistic visions for the future.
1283000	1288000	But once it gets to the point that I'm describing, if it gets to that point,
1288000	1292000	I actually imagine it being stable for a pretty long time.
1292000	1294000	Except for the edge, I think.
1294000	1297000	Yeah, Siren gets up.
1297000	1302000	One big tension in your world as a result of this increasing difficulty
1302000	1308000	and verifying information is just people have a hard time agreeing on objective reality.
1308000	1311000	They're really good in experimental healthcare interventions,
1311000	1315000	but it's mostly about luck and maybe some skill to pick the winners out of that crowd.
1315000	1320000	You have cryptocurrency that's made it really impossible to tell how much money anyone has.
1320000	1323000	You mentioned that instead of Forbes keeping track of wealth,
1323000	1327000	now kidnapping rings keep some of the best records of people's total assets.
1327000	1333000	You even say that startups are buying these records off of those kidnapping rings to find wealthy funders.
1333000	1339000	Can you say a little bit more about what leads to this deep fracturing of shared objectivity?
1339000	1344000	That's been going on really in a big way since the 1940s.
1344000	1349000	Once again, I'm just imagining it continuing and accelerating with more powerful technologies.
1350000	1356000	The collapse of the dollar, which happens in the 2030s more or less in my story,
1356000	1358000	contributes a fair amount.
1358000	1361000	It makes the crypto thing much more substantial.
1361000	1367000	And the increase like basically social welfare through senior age
1367000	1371000	and the expansion of senior age through the population
1371000	1375000	helps to stabilize things a lot at the expense of coherence and efficiency,
1375000	1377000	which isn't really necessary anymore.
1377000	1379000	Could you say what senior age is?
1379000	1384000	Senior age is printing money through bank activities.
1384000	1388000	When banks borrow money, then they lend out much more money.
1388000	1392000	And there's a stack of different interest rates paid by different creditors.
1392000	1397000	One of the basic challenges of running a capitalist society
1397000	1400000	that's been well understood since long before Adam Smith
1400000	1405000	is the extreme difficulty of causing control of the money printing apparatus
1405000	1410000	to not be the convergent agenda of practically everyone.
1410000	1415000	And most capitalist societies do collapse as control of the money printing apparatus
1415000	1417000	becomes a convergent agenda.
1417000	1424000	So I'm basically imagining the essential worker system that we discovered we had during COVID
1424000	1429000	and the relatively resilient management of a small number of companies
1429000	1432000	basically keeping the material reality held together.
1432000	1437000	Despite the fact that the vast majority of supposed economic activity
1437000	1441000	is actually pure political wealth redistribution
1441000	1447000	to the people who bother to fight for wealth being distributed to them in a world
1447000	1451000	where most people have basically lost track of wealth anyway.
1451000	1457000	I'm curious why AI systems don't help more with these issues of shared goals and shared knowledge.
1457000	1460000	You mentioned that AI systems can provide common knowledge,
1460000	1464000	like they help groups of people identify if their behaviors are aligned with their goals
1464000	1466000	or how to change their behaviors.
1466000	1469000	You would think that that might cut through the haze
1469000	1474000	and help people agree on things more or have more transparency.
1474000	1479000	AI systems help enormously with establishing whatever set of goals
1479000	1485000	is reasonably psychologically plausible and that the systems designers want to establish.
1485000	1490000	But mostly that consists of consuming products just like it does in our world.
1490000	1495000	And in the rare cases of societies that have more of a shared set of values
1495000	1498000	and more of a shared power structure like China,
1498000	1503000	it means that they have incredibly high integration and unity
1503000	1508000	targeting shared goals that more or less consist of normal reasonable things
1508000	1514000	like extending life and ecological sustainability and stability in general.
1515000	1519000	One other thread I really enjoyed in your world is how you talk about education changing.
1519000	1524000	So people start to see traditional educational pedigrees as a form of inherited privilege.
1524000	1527000	And educational histories actually become private information
1527000	1531000	which can't be used in decisions like hiring, which is a really interesting concept.
1531000	1535000	And this tips the scales in favor of online self-driven education.
1535000	1539000	Schools basically go empty while kids live with their families and learn on their own.
1539000	1541000	I'm curious what this looks like for those kids.
1541000	1543000	What are they learning? What aren't they learning?
1543000	1545000	And who's deciding?
1545000	1552000	So I'm basically imagining that nominally the parents decide when the kids are younger
1552000	1555000	and the kids decide when they're older.
1555000	1561000	But in practice, reasonably agentic parents who are also tech savvy
1561000	1568000	and have like reasonably coherent preferences about what to get
1568000	1574000	will be able to direct their kids towards media bubbles and narratives
1574000	1580000	that will be extremely stable and which won't change much unless something really weird happens.
1580000	1587000	So I expect that almost everyone's learning speed is going to be like
1587000	1594000	at least four or five times faster between more targeted instruction, objectively better instruction,
1594000	1599000	maybe learning enhancement through drugs and mRNA tech.
1599000	1603000	And much better trauma care is a major feature of my world.
1603000	1609000	So just the elimination of mental blocks through MDMA therapies and their successors.
1609000	1616000	I don't know if I really played up adequately the spread of a new way of doing civilization
1616000	1620000	from the carceral system into the general population
1620000	1625000	as like MDMA therapies get adopted for dispute resolution within prisons
1625000	1633000	and reach a level of reliability and efficacy that's sufficient that basically everyone wants some.
1641000	1644000	Despite some of the more madcap details of their world,
1644000	1648000	this team expresses a strong commitment to realism and plausibility.
1648000	1653000	Their portrayal of AI development was also perhaps the slowest and most restricted among our winners.
1653000	1657000	While there isn't AGI around, most of the technological developments in this world
1657000	1663000	are just extensions of today's narrow AI systems whose awesome capabilities are ultimately limited.
1663000	1666000	I was curious to hear more about this team's creative influences
1666000	1671000	and whether this slower pace of AI development was something they saw as merely likely
1671000	1676000	or a necessary component of any safe path to an aspirational future.
1677000	1681000	So I'd like to spend a little while discussing the narratives in your world
1681000	1685000	and how they compare to other narratives that are going around in popular culture.
1685000	1690000	Like one really big through line for me is this sort of emperor has no close attitude you have
1690000	1694000	towards economics and politics where your world kind of just goes through the motions
1694000	1698000	to keep things moving along but the systems themselves are no longer really doing much.
1698000	1704000	I'm curious if there are other examples of this perspective that inspired you in other kinds of media?
1705000	1710000	I mean, mostly I'm inspired by real life, not by media and narratives.
1710000	1715000	I can't think of a piece of fiction that is as radical as real life
1715000	1719000	in the degree to which it violates conventional assumptions.
1719000	1726000	You know, it's basically impossible to do without being a top tier literary genius like Shakespeare.
1726000	1731000	I mean, Hamlet's wonderful Doris Lessig's book, The Golden Notebook,
1731000	1734000	is maybe the best depiction I've ever seen,
1734000	1740000	but one would need to be a really, really good literary scholar to appreciate it, I think.
1740000	1741000	Same with Hamlet.
1741000	1742000	Interesting.
1743000	1746000	Well, I'm curious if there are any examples,
1746000	1750000	and this can come from philosophers as well as fiction,
1750000	1754000	of economic or political systems that could actually maybe function in a world like yours,
1754000	1760000	or do you think that the whole concept of having a system that's run in a sensible way is kind of moot?
1760000	1761000	No.
1761000	1766000	I mean, the Chinese system is sort of run in a sensible way in the world I'm describing.
1766000	1770000	It's not run with perfect rigor and resolution.
1770000	1773000	It wouldn't pass like Talmudic standards.
1773000	1778000	But by the standards that we're used to from government,
1778000	1783000	I'm imagining a China with a life expectancy of well over 100 years,
1783000	1787000	and the ability to industrially produce in a clean way,
1787000	1791000	and with very little labor, practically everything the entire world needs.
1791000	1796000	The goals of maximizing filial piety and ren
1796000	1800000	are just going to be what's inherited from their ancestors and traditions,
1800000	1804000	and it may not seem like doing a thing to us,
1804000	1811000	but most of what we do is arguably not really doing a thing.
1812000	1816000	Do you think there are any actions or reforms we could do to Western systems
1816000	1820000	that would make them more resilient to these changes as well?
1820000	1824000	I mean, my simple answer is I already put them all into this story.
1824000	1829000	That's why the world is still alive and has not collapsed already.
1829000	1836000	I'm making a number of surprising good luck-happens assumptions,
1836000	1838000	not extraordinary.
1838000	1843000	I really try to keep avoid endorsing things that are not just quirky
1843000	1847000	and that have probabilities of less than about 10%.
1847000	1851000	I think it's important to note that our world would be way scarier
1851000	1856000	to people from my vision of 2045 than their world would be from us.
1856000	1859000	Their world would just be incredibly addictive,
1859000	1864000	and we would very quickly find ourselves trapped in some relatively exploitative bubble.
1864000	1870000	But even exploitative bubbles have reasons to try to keep people mentally healthy enough
1871000	1877000	to keep on receiving government benefits within a thin veneer of contributing to the economy.
1877000	1882000	I guess one way to think about it is the American dream is basically a colpage
1882000	1886000	of the America prior to the Civil War,
1886000	1890000	America between the Civil War and the New Deal,
1890000	1892000	and America after the New Deal,
1892000	1894000	which could be summarized as the colonist experience,
1894000	1898000	the immigrant experience, and the GI experience.
1898000	1904000	And none of these experiences at all resemble what Zoomers are coming into and experiencing.
1904000	1908000	And so they are growing up in a world of such transparent lies
1908000	1912000	that they're almost without exception total epistemic nihilists
1912000	1915000	mistakenly disbelieve that anything was ever true
1915000	1919000	rather than only disbelieving that anything that they've ever seen is true,
1919000	1922000	which is actually the case.
1922000	1927000	So one really unique thing about your world is this focus on the narrow AI systems
1927000	1930000	and how high a ceiling you put on their abilities.
1930000	1933000	You kind of have basically a suite of different narrow AI systems
1933000	1936000	that together have the capabilities of an AGI in some ways,
1936000	1938000	but they're spread across these separate modules.
1938000	1940000	No, they don't have the capabilities of an AGI.
1940000	1943000	They don't have anything even remotely close to the abilities of an AGI.
1943000	1945000	Can you distinguish that?
1945000	1948000	The story is just kind of hinting at the capabilities of an AGI
1948000	1953000	with the sort of security around it and the sort of implied impact
1953000	1955000	and like potential risk.
1955000	1958000	I'm operating with the definition of AGI that's something like a system
1958000	1962000	that's better than a human at any task that you can reasonably define.
1962000	1966000	Is that different from what you say when you say that these narrow AI systems?
1966000	1969000	No, better than any human at any task that you can reasonably define.
1969000	1975000	I'm saying that the systems that I'm describing are not even remotely close to that.
1975000	1980000	They're like superhuman at very narrow tasks.
1980000	1984000	They're superhuman at a lot of very narrow tasks.
1985000	1989000	But it doesn't even come close, fitting them all together
1989000	1991000	to the full range of human capabilities.
1991000	1994000	I see. So there's kind of a synergy here, you're saying.
1994000	1997000	Right, and then they're like not superhuman,
1997000	2004000	but like merely as good as the experts that top elites tend to point to
2004000	2011000	at the vast majority of tasks that get measured and graded
2011000	2014000	and systematized and standardized threat society.
2014000	2020000	So the best doctors in my world are still humans who make very heavy use of AI tools,
2020000	2026000	but the best purely AI doctors might only be as good as the doctors
2026000	2029000	that like the president has in our world,
2029000	2033000	but not nearly as good as the doctors that like a top doctor has in our world
2033000	2037000	since the top doctors know who the actual best doctors are and to date them.
2037000	2042000	So not only do these systems not exceed the best human experts individually
2042000	2046000	at these narrower tasks, but you're also saying that there's something missing
2046000	2049000	even if you have this collection of narrow systems
2049000	2051000	that can each do something that a human can do.
2051000	2054000	Just putting those together is not the same as having something
2054000	2057000	that could do all of these flexibly, is that what you're saying?
2057000	2061000	Definitely, but also there are things that none of them can do even a little bit.
2061000	2063000	Like in the story that I'm talking about,
2063000	2068000	Siren is the only AI in the world that could, if it wanted to,
2068000	2070000	do important original mathematics.
2070000	2072000	It's the only AI in the world that could, if it wanted to,
2072000	2076000	make the tiniest contribution to theoretical world-plied physics.
2076000	2081000	So in your world, you have this incredibly powerful AGI system that does exist,
2081000	2085000	but it's under really, really strong protections under tight wraps.
2085000	2090000	Do you think that this is necessary to have a optimistic future with AGI in it?
2090000	2094000	Yeah, definitely, unless we can basically do, you know,
2094000	2100000	thousands of years worth of philosophical progress in like 20 years.
2100000	2103000	And we can't.
2103000	2107000	Like maybe we can do thousands of years worth of philosophical progress this century
2107000	2113000	because we will have both AI and other technologies for enhancing our mental capabilities
2113000	2115000	if we choose to use them.
2115000	2118000	But we can't do it in 20 years, it's just laughable.
2118000	2119000	Yeah.
2119000	2123000	So the limitations that are preventing AGI from developing faster in your world,
2123000	2126000	some of them are intentional, like policy decisions.
2126000	2128000	Some of them are just kind of practical ones,
2128000	2132000	like bad funding, politicization, and the rarity of human expertise.
2132000	2137000	Do you think these are actual likely causes of slowing development in the real world?
2137000	2143000	Yeah, that's my, my best guess is that AGI will progress much more slowly
2143000	2146000	than I have it progressing in my story.
2146000	2150000	And my best guess is that we do survive
2150000	2153000	because AGI progresses much more slowly.
2153000	2157000	From my perspective, it's extremely contrived for AGI to be,
2157000	2160000	to develop even as fast as it does in this story
2160000	2165000	and be handled well enough, cautiously enough, thoughtfully enough
2165000	2170000	that like we have more than a fraction of a percent chance of survival.
2170000	2172000	Yeah.
2172000	2174000	Have you seen other portrayals of the future
2174000	2177000	where narrow AI plays as much of a role as in yours?
2177000	2181000	I mean, I feel like there's a lot of portrayals of the future
2181000	2186000	where narrow AI is taken for granted and plays a large role.
2186000	2189000	Like in the Star Trek, the next generation,
2189000	2193000	they have one AGI data, like in my world,
2193000	2197000	and then they have like an unbelievably powerful narrow AI in the ship
2197000	2200000	and in the holodeck and just all over the place.
2200000	2205000	But everyone takes it for granted and it's used as a tool
2205000	2211000	by a military organization with a relatively unified internal agenda
2211000	2217000	of exploration and extremely prudish and narrow conceptions
2217000	2223000	of what types of experiences and behaviors
2223000	2226000	its members are supposed to engage in.
2226000	2232000	I will say that the type of narrow AI that we have actually developed
2232000	2240000	is like pretty broad compared to what I expected five years ago.
2240000	2245000	It's like very much what we were visibly moving towards four years ago.
2245000	2249000	But to some degree, when I was growing up,
2249000	2252000	C3PO seemed like a silly fantasy,
2252000	2254000	seemed silly that you could have a machine
2254000	2259000	that was that close to human performance
2259000	2264000	but like stuck for a long time at below human performance
2264000	2267000	and in some ways pretty profoundly below
2267000	2270000	and in some ways pretty profoundly superhuman
2270000	2273000	but like just be stuck there for a long time.
2273000	2277000	But it kind of looks from the technologies that open up in AI
2277000	2281000	is generating that a minimum viable C3PO
2281000	2284000	might actually happen and be around for a long time
2284000	2288000	without really drastic improvement from that.
2288000	2292000	How do you feel about the general portrayals of the future that are in fiction?
2292000	2297000	Do you think they're over or under optimistic when they try to be optimistic?
2297000	2302000	I just think optimism in fiction that is trying to be at all realistic
2302000	2309000	is unfortunately much rarer than it should be.
2309000	2313000	And like that's basically maybe largely
2313000	2318000	because the most perceptive and insightful people
2318000	2323000	who are also successful at becoming prominent
2323000	2327000	and surrounding themselves with other prominent people
2327000	2332000	are constantly confronted with lots of profoundly miserable,
2332000	2336000	extremely zero sum other prominent people
2336000	2340000	and have very little contact with the large majority of people
2340000	2346000	who are just not as miserable as the people who like double audience
2346000	2348000	are going to end up around.
2348000	2351000	So you're kind of calling for more optimism in nonfiction as well?
2351000	2352000	Is that where you're going?
2352000	2356000	I mean, no, I feel like optimism and pessimism is like
2356000	2358000	intrinsically unhealthy concepts.
2358000	2360000	You should just try to have true beliefs
2360000	2364000	but true beliefs should be balanced.
2364000	2370000	There's a lot of social pressure to performatively be pessimistic
2370000	2373000	because the elites tend to be pessimistic.
2373000	2375000	And elites tend to be pessimistic
2375000	2378000	because they're living in a hyper competitive zero sum world
2378000	2380000	that most of us are not living in.
2380000	2384000	And there's a lot of, it's easier in some ways to be pessimistic
2384000	2386000	especially cheaply pessimistic.
2386000	2391000	But there's also like just cognitive biases that lead to silly sorts of pessimism.
2391000	2394000	So like imagine there was a news item
2394000	2398000	about how it turns out that apples cause cancer.
2398000	2400000	Practically everyone would see this as bad news.
2400000	2402000	Oh no, I've been poisoning myself for years.
2402000	2404000	But obviously it's good news.
2404000	2405000	We know what the cancer rate is.
2405000	2408000	Now we know that we can avoid it by not eating apples.
2408000	2410000	Right, the devil you know.
2410000	2415000	Like information is almost always desirable
2415000	2419000	but information about bad things gets interpreted
2419000	2421000	as something bad happening
2421000	2424000	rather than being interpreted as something good happening.
2424000	2426000	You know James Baldwin,
2426000	2430000	not everything that can be confronted can be overcome
2430000	2434000	but everything to be overcome must be confronted.
2434000	2438000	And to some degree this picture that I'm depicting
2438000	2440000	is an edge case on that
2440000	2445000	because this is a world that has managed to partially overcome
2446000	2449000	and fully survive a lot of the problems
2449000	2451000	that our society is dying from
2451000	2453000	without really confronting them.
2453000	2456000	In your world a lot of the role models become virtual.
2456000	2459000	So basically all celebrities popular with the under 30 crowd
2459000	2461000	are virtual people.
2461000	2463000	Some are recreations of historical figures.
2463000	2466000	Others are kind of amalgams like Tupacalist
2466000	2469000	and you have XXX, Tentacion, Albus XXX.
2469000	2472000	I'm curious how these play a role
2472000	2475000	as kind of role models in your world.
2475000	2477000	I think they mostly don't.
2477000	2480000	I think that people who have at least reasonably good taste
2480000	2484000	do prefer interaction with individuals.
2484000	2488000	Insofar as they mostly imitate behavior by real people
2488000	2492000	and that like the social influences from machines
2492000	2496000	are in general more of a, you know,
2496000	2500000	goal directed relatively overt manipulation sort.
2500000	2503000	What are your thoughts on some of the current cultural attitudes
2503000	2505000	towards like AI generated art
2505000	2507000	and virtual cultural figures right now?
2507000	2510000	So it seems like any reasonably good artist
2510000	2512000	wants to make art,
2512000	2514000	not wants to get paid for making art
2514000	2516000	and like maybe wants to be seen
2516000	2518000	and people can worry that
2518000	2521000	a lot of people will never be exposed to good art
2521000	2524000	because they're going to be exposed to an enormous amount of stuff
2524000	2526000	that doesn't have a message behind it
2526000	2530000	and isn't created as an expression of pain
2530000	2532000	and suppressed emotion.
2532000	2535000	But like if you're a good artist
2535000	2536000	you can learn new tools
2536000	2538000	and you keep learning new tools throughout your life
2538000	2541000	and you learn how to make use of these new tools
2541000	2544000	to compete and to get your message out there.
2544000	2548000	The barriers to entry for art in some sense never go down
2548000	2551000	because they have to do with the attention
2551000	2553000	and consciousness of your audience.
2553000	2557000	The economic barriers to entry in our world
2557000	2560000	are going up because of extreme economic scarcity
2560000	2563000	that's being created through policy.
2563000	2565000	So like we're living in a time of
2565000	2567000	really extreme economic scarcity
2567000	2570000	compared to the Great Depression at this point.
2570000	2573000	We've lost way more actual economic freedom
2573000	2575000	in the sense of like
2575000	2577000	not needing to work very hard or very well
2577000	2580000	or be very exceptional in order to afford
2580000	2583000	to reproduce our own labor
2583000	2585000	have children and grandchildren
2585000	2588000	and have them live at least as nicely as we do
2588000	2590000	and be able to purchase our baskets of goods.
2590000	2592000	And every Zoomer knows it
2592000	2595000	because like they in fact can't purchase
2595000	2598000	the baskets of goods that their parents had
2598000	2601000	and their parents deny them recognition as adults
2601000	2605000	even like millennials with kids
2605000	2608000	are denied recognition as adults in major ways
2608000	2611000	because they don't have the baskets of consumption goods
2611000	2615000	that in fact, generationally speaking, they can't have.
2623000	2625000	The process of world building has great potential
2625000	2627000	to make a positive future feel more attainable.
2627000	2629000	This can be incredibly powerful
2629000	2632000	whether you're a creative person looking to produce rich works of fiction
2632000	2634000	or have a more technical focus
2634000	2636000	and are looking to reach policymakers or the public.
2636000	2638000	I asked Michael what kind of impacts
2638000	2640000	he hoped his work would have on the world.
2640000	2642000	What do you hope your world leaves people
2642000	2645000	thinking about long after they've read through it?
2645000	2648000	So the biggest thing is that I just would like people
2648000	2652000	to try to make sense of what could happen.
2652000	2655000	I would like them to know that it is possible
2655000	2659000	to make a joint prediction
2659000	2663000	and expression of preference that is in line with
2664000	2667000	the relatively full range of scientific and technological
2667000	2669000	and humanistic thinking
2669000	2671000	and that holds together and makes sense
2671000	2675000	and that much more close to comes true
2675000	2678000	and also somewhat guides society towards it coming true
2678000	2681000	than what we would normally think of as science fiction.
2681000	2684000	What kinds of expertise would you be most interested in having
2684000	2688000	people bring to discussions about your world or the future in general?
2688000	2692000	I think that the things that I would want to bring in first
2692000	2696000	would be somatic skills
2696000	2700000	like body work and yoga
2700000	2703000	and things like that
2703000	2708000	experience with MDMA and other psychedelic therapies
2708000	2711000	and maybe even electrical engineering
2711000	2713000	for creating better alternatives
2713000	2715000	to transcranial magnetic stimulation
2715000	2718000	for disinhibiting some parts of the cortex
2718000	2721000	and activating other parts of the cortex
2721000	2724000	so to enable people to recapture
2724000	2726000	usually after years of work
2726000	2729000	the sorts of cognitive abilities that
2729000	2732000	normal smart inquisitive kids had when I was a kid
2732000	2736000	and which the entire literary imprint
2736000	2740000	of Western civilization is the imprint of
2740000	2743000	which is why we don't have a literary imprint
2743000	2745000	for our contemporary civilization.
2745000	2748000	Do you have any particular hopes about the impact
2748000	2752000	this work would have on the younger generation of folks around today?
2752000	2756000	It seems to me that the vast majority of zoomers
2756000	2760000	are doomers. They believe that the world is going to hell in a handbasket
2760000	2762000	and everything is falling apart.
2762000	2765000	But they are likewise cynics about the past
2765000	2769000	in that they somehow believe that things have been getting worse forever
2769000	2771000	but were never better than they are today.
2771000	2775000	And that that can't be a concrete set of beliefs.
2775000	2778000	It actually has to be something that they have
2778000	2782000	instead of having beliefs which is like a posture, a vibe.
2782000	2786000	What's actually going on is that they have always been lied to
2786000	2790000	about everything of importance by every credible authority
2790000	2794000	so they don't believe that people can know things
2794000	2798000	and they only believe that people can posture and vibe.
2798000	2802000	That's really sad because manifestly the world around us
2802000	2806000	displays incredible amounts of the results of knowledge
2806000	2810000	and if people don't continue to produce the results of that knowledge
2810000	2812000	we're going to live in a much less nice world.
2812000	2816000	What could help to correct for this or influence their attitudes
2816000	2818000	in a positive way for you?
2818000	2823000	Well, to a really non-trivial degree
2823000	2827000	large language models that we have today
2827000	2830000	if they were reinforcement learning trained
2830000	2835000	for questioning and challenging and calling out bullshit
2835000	2840000	and especially for perceiving the emotional dynamics of social situations
2840000	2843000	which are very easy to perceive for even average humans
2843000	2847000	and wouldn't be that hard to train systems to perceive.
2847000	2853000	Like people need social support in calling out bullshit
2853000	2858000	rather than all of the social pressure being to submit to bullshit
2858000	2863000	and go along with it and like we have the technology today
2863000	2868000	to build artificial social support of precisely the type we need.
2868000	2872000	What aspects of your world would you be most excited to see popular media
2872000	2875000	take on when portraying the future?
2875000	2881000	Well, to start having a picture of China as something like China
2881000	2885000	rather than using China as our designated bad guy
2885000	2889000	which we use to project the images of ourselves
2889000	2893000	basically our popular media almost exclusively treats China
2893000	2897000	as a scapegoat for the types of behavior
2897000	2900000	that we are very aware that we engage in
2900000	2903000	to approximately the same degree that they do
2903000	2907000	and is almost always trying to display
2907000	2911000	so bias for the sake of showing loyalty
2911000	2915000	rather than trying to display scholarship and understanding.
2915000	2920000	I think that having in general a somewhat more balanced view
2920000	2923000	of all sorts of cultural things
2923000	2928000	having more of an attitude that most things have some good and some bad in them.
2928000	2930000	Well, thanks so much for joining us today, Michael.
2930000	2932000	We've covered so much ground in this conversation
2932000	2935000	and it's been really great to explore all these ideas with you.
2935000	2939000	It's great having this conversation.
2966000	2969000	You can read all the comments and appreciate every rating.
2969000	2972000	This podcast is produced and edited by Worldview Studio
2972000	2974000	and the Future of Life Institute.
2974000	2977000	FLI is a non-profit that works to reduce large scale risks
2977000	2979000	from transformative technologies
2979000	2981000	and promote the development and use of these technologies
2981000	2983000	to benefit all life on earth.
2983000	2985000	We run educational outreach and grants programs
2985000	2987000	and advocate for better policy making
2987000	2989000	in the United Nations, US government,
2989000	2991000	and European Union institutions.
2991000	2993000	If you're a storyteller working on films
2993000	2995000	and creative projects about the future,
2995000	2997000	we can also help you understand the science
2997000	2999000	and storytelling potential of transformative technologies.
2999000	3001000	If you'd like to get in touch with us
3001000	3003000	or any of the teams featured on the podcast to collaborate,
3003000	3006000	you can email worldbuildatfutureoflife.org.
3006000	3009000	A reminder, this podcast explores the ideas
3009000	3012000	created as part of FLI's Worldbuilding Contest,
3012000	3014000	and our hope is that this series sparks discussion
3014000	3016000	about the kinds of futures we all want.
3016000	3019000	The ideas we discuss here are not to be taken as FLI positions.
3019000	3021000	You can find more about our work
3021000	3024000	at www.futureoflife.org
3024000	3027000	or subscribe for a newsletter to get updates on all our projects.
3027000	3029000	Thanks for listening to Imagine a World.
3029000	3032000	Stay tuned to explore more positive futures.
