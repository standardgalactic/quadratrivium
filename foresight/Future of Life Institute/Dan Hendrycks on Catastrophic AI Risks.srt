1
00:00:00,000 --> 00:00:03,360
Welcome to the Future of Life Institute podcast.

2
00:00:03,360 --> 00:00:06,480
My name is Gus Docker and I'm here with Dan Hendricks.

3
00:00:06,480 --> 00:00:10,000
Dan is the Director of the Center for AI Safety.

4
00:00:10,000 --> 00:00:11,520
Dan, welcome to the podcast.

5
00:00:11,520 --> 00:00:12,840
Glad to be back.

6
00:00:12,840 --> 00:00:16,760
You are also an advisor to XAI.

7
00:00:16,760 --> 00:00:18,360
Maybe you can tell us a bit about that.

8
00:00:18,360 --> 00:00:23,560
Sure. So XAI is Elon's new AGI project.

9
00:00:23,560 --> 00:00:25,520
It's still very much in its early stages,

10
00:00:25,520 --> 00:00:27,680
so it's difficult to say

11
00:00:27,680 --> 00:00:30,440
specific things about what they'll be doing

12
00:00:30,440 --> 00:00:35,200
or what the specific high-level strategy is to give a sense.

13
00:00:35,200 --> 00:00:38,960
Elon has been interested in the failure mode

14
00:00:38,960 --> 00:00:41,560
of sort of eroded epistemics

15
00:00:41,560 --> 00:00:46,120
where people don't have a shared sense of consensus reality,

16
00:00:46,120 --> 00:00:48,000
and this might make it harder for a civilization

17
00:00:48,000 --> 00:00:49,680
to appropriately function.

18
00:00:49,680 --> 00:00:53,360
There are other types of extras that he's concerned about as well.

19
00:00:53,360 --> 00:00:56,280
His sort of probability of doom

20
00:00:56,280 --> 00:00:58,480
or of that of an existential catastrophe

21
00:00:58,480 --> 00:01:00,280
is around like 20% to 30%.

22
00:01:00,280 --> 00:01:02,520
So he takes this, I would guess, more seriously

23
00:01:02,520 --> 00:01:07,440
than any other leader of a major AGI organization,

24
00:01:07,440 --> 00:01:10,520
but exactly how when it goes about reducing that risk

25
00:01:10,520 --> 00:01:13,080
is still somewhat to be determined.

26
00:01:13,080 --> 00:01:16,200
There is an interest in building more true seeking AIs,

27
00:01:16,200 --> 00:01:18,000
but on other occasions, too,

28
00:01:18,000 --> 00:01:20,720
he'd mentioned that we should have AIs with the objective

29
00:01:20,720 --> 00:01:22,840
of preserving human autonomy

30
00:01:22,840 --> 00:01:25,200
or maximizing the freedom of action

31
00:01:25,200 --> 00:01:27,680
and on other instances,

32
00:01:27,680 --> 00:01:29,840
in thinking about good objectives for AI systems,

33
00:01:29,840 --> 00:01:33,440
having them increase net civilizational happiness over time.

34
00:01:33,440 --> 00:01:36,040
So I think that this reflects sort of a plurality

35
00:01:36,040 --> 00:01:39,520
of different goals that he thinks AI systems

36
00:01:39,520 --> 00:01:41,480
should end up pursuing

37
00:01:41,480 --> 00:01:44,240
rather than picking just exactly,

38
00:01:44,240 --> 00:01:45,640
rather than just picking one.

39
00:01:45,640 --> 00:01:48,160
I think it's relevant to note

40
00:01:48,160 --> 00:01:50,480
that it's a fairly serious effort.

41
00:01:50,480 --> 00:01:53,240
I'd anticipate that it would probably be

42
00:01:53,280 --> 00:01:56,480
one of the main three AI companies

43
00:01:57,480 --> 00:01:59,840
next year or the year after,

44
00:01:59,840 --> 00:02:04,680
like OpenAI, Google DeepMind, and XAI.

45
00:02:04,680 --> 00:02:09,480
So I don't think of it as a smaller effort,

46
00:02:09,480 --> 00:02:12,240
but it has the capacity

47
00:02:12,240 --> 00:02:14,520
to have a substantial ratio of force, so.

48
00:02:14,520 --> 00:02:17,880
The other top AI corporations you mentioned,

49
00:02:17,880 --> 00:02:20,280
Anthropic, Google DeepMind, OpenAI,

50
00:02:20,280 --> 00:02:24,320
have backing from giant tech companies.

51
00:02:24,320 --> 00:02:26,480
Does XAI similarly have some backing

52
00:02:26,480 --> 00:02:28,360
from Tesla, for example?

53
00:02:28,360 --> 00:02:31,240
I can't specifically say about that,

54
00:02:31,240 --> 00:02:35,280
but this is not a subpart of Tesla.

55
00:02:35,280 --> 00:02:38,920
This is not an organization inside of Twitter or X,

56
00:02:38,920 --> 00:02:41,600
and it's not an organization inside of Tesla.

57
00:02:41,600 --> 00:02:44,680
The main topic of conversation for this episode

58
00:02:44,680 --> 00:02:48,160
is your paper on catastrophic risks from AI

59
00:02:48,160 --> 00:02:50,440
and specifically categorizing these risks.

60
00:02:50,440 --> 00:02:53,040
So you categorize risks from,

61
00:02:53,040 --> 00:02:56,760
catastrophic risks from AI in four different categories.

62
00:02:56,760 --> 00:02:59,880
Maybe we should just start by sketching out those categories

63
00:02:59,880 --> 00:03:01,840
and then go into depth later.

64
00:03:01,840 --> 00:03:04,480
Yeah, so I guess at a very abstract level,

65
00:03:04,480 --> 00:03:09,480
there's risks if people are trying to use AI

66
00:03:09,480 --> 00:03:11,080
intentionally to cause harm.

67
00:03:11,080 --> 00:03:12,000
That's a basic one.

68
00:03:12,000 --> 00:03:15,040
So there's an intentional catastrophe

69
00:03:15,040 --> 00:03:17,080
that would be malicious use.

70
00:03:17,120 --> 00:03:21,200
Another one is where there are accidents.

71
00:03:21,200 --> 00:03:23,280
And if there are accidents,

72
00:03:23,280 --> 00:03:27,760
this would often be the consequence of the AI developers

73
00:03:27,760 --> 00:03:29,560
using these very powerful systems

74
00:03:29,560 --> 00:03:31,880
or potentially leaking them

75
00:03:31,880 --> 00:03:34,080
or accidentally putting in some bad objective

76
00:03:34,080 --> 00:03:36,000
or doing some gain of function,

77
00:03:36,000 --> 00:03:37,840
but that would be some accident risks.

78
00:03:37,840 --> 00:03:40,240
So that relates to organizational risks

79
00:03:40,240 --> 00:03:42,440
or organizational safety.

80
00:03:42,440 --> 00:03:45,360
The third would be these environmental

81
00:03:45,360 --> 00:03:46,560
or structural risks.

82
00:03:46,560 --> 00:03:50,520
Basically where AI companies are or AI developers,

83
00:03:50,520 --> 00:03:52,520
be those companies or maybe in later stages,

84
00:03:52,520 --> 00:03:57,520
countries are racing to build more and more powerful

85
00:03:57,520 --> 00:04:00,240
AI systems or AI weapons.

86
00:04:00,240 --> 00:04:05,240
And this structural risk incentivizes companies to,

87
00:04:06,320 --> 00:04:10,840
or these developers to seed more and more decision making

88
00:04:10,840 --> 00:04:13,000
and control to these AI systems.

89
00:04:13,000 --> 00:04:15,040
We get a looser and looser leash.

90
00:04:15,040 --> 00:04:16,120
Things move very quickly.

91
00:04:16,120 --> 00:04:18,040
We become extremely dependent on them.

92
00:04:18,040 --> 00:04:22,760
This gets us in an irreversible position

93
00:04:22,760 --> 00:04:26,080
where we're not actually making the decisions,

94
00:04:26,080 --> 00:04:29,040
but we're basically having nominal control.

95
00:04:29,040 --> 00:04:30,400
It's a very possible in that situation,

96
00:04:30,400 --> 00:04:32,960
we just ultimately end up losing control

97
00:04:32,960 --> 00:04:35,560
to the sort of very complicated fast moving system

98
00:04:35,560 --> 00:04:36,680
that we create.

99
00:04:36,680 --> 00:04:41,680
And then the final type would be these risks

100
00:04:42,000 --> 00:04:44,200
that emanate from the AI systems themselves.

101
00:04:44,200 --> 00:04:47,520
These are more internal or inherent risks from AI systems.

102
00:04:47,520 --> 00:04:51,320
And that would take the form of rogue AIs

103
00:04:51,320 --> 00:04:54,240
where they have goals separate from our own

104
00:04:54,240 --> 00:04:58,160
and they work against us to complete

105
00:04:58,160 --> 00:05:01,680
or satisfy their desires or preferences.

106
00:05:01,680 --> 00:05:05,480
So overall, there are four.

107
00:05:05,480 --> 00:05:06,880
There's malicious use.

108
00:05:06,880 --> 00:05:09,000
There's these organizational risks.

109
00:05:09,000 --> 00:05:13,880
There's these structural slash environmental risks

110
00:05:13,880 --> 00:05:16,160
and there's these inherent or internal risks

111
00:05:16,160 --> 00:05:20,240
in the form of malicious use, organizational risk,

112
00:05:20,240 --> 00:05:22,760
racing dynamics and rogue AIs.

113
00:05:22,760 --> 00:05:26,000
Yeah, so if we look back maybe 10 years or so,

114
00:05:26,000 --> 00:05:28,440
I think most of the discussion about AI risk

115
00:05:28,440 --> 00:05:30,360
would have been about rogue AI.

116
00:05:30,360 --> 00:05:35,000
So the risks that are coming internally from the AI,

117
00:05:35,000 --> 00:05:39,560
so to speak, the AI developing technically in ways

118
00:05:39,560 --> 00:05:41,120
that we're not interested in.

119
00:05:41,120 --> 00:05:42,880
So how much is this categorization

120
00:05:42,880 --> 00:05:44,520
set in stone?

121
00:05:44,520 --> 00:05:47,600
Do you think it'll change over time as we learn more

122
00:05:47,600 --> 00:05:51,560
or have the field of AI safety matured

123
00:05:51,560 --> 00:05:55,120
such that we can see the risk landscape now?

124
00:05:55,120 --> 00:05:57,800
I think the focus on rogue AI systems

125
00:05:57,800 --> 00:06:00,120
is largely due to early movers

126
00:06:00,120 --> 00:06:02,040
having substantial cultural influence.

127
00:06:02,040 --> 00:06:03,520
I think if we asked other people

128
00:06:03,520 --> 00:06:06,800
who were not as invested in AI risks,

129
00:06:06,800 --> 00:06:10,440
what if they were to write down concerns about these,

130
00:06:10,440 --> 00:06:12,320
they would of course think that people

131
00:06:12,320 --> 00:06:14,920
using the technology for extremely destructive purposes

132
00:06:14,920 --> 00:06:17,040
was posed catastrophic risks.

133
00:06:17,040 --> 00:06:21,240
And I think the communities ended up having

134
00:06:21,240 --> 00:06:23,800
some self-selection effects such that people

135
00:06:23,800 --> 00:06:25,880
didn't end up talking about things like malicious use

136
00:06:25,880 --> 00:06:28,080
and treated that as a distraction, I think were.

137
00:06:28,080 --> 00:06:31,760
So I think the community didn't make much of a space

138
00:06:31,760 --> 00:06:33,120
for people who were concerned about things

139
00:06:33,120 --> 00:06:36,480
other than rogue AI systems.

140
00:06:36,480 --> 00:06:38,640
But that was a mistake.

141
00:06:38,640 --> 00:06:42,560
The AI is being used in malicious ways

142
00:06:42,560 --> 00:06:44,640
can definitely cause catastrophes

143
00:06:44,640 --> 00:06:50,040
and can end up increasing the probability

144
00:06:50,040 --> 00:06:52,000
of existential risks as well,

145
00:06:52,000 --> 00:06:54,000
which maybe we'll speak about the connections

146
00:06:54,000 --> 00:06:57,880
between ongoing harms, anticipated risks

147
00:06:57,880 --> 00:07:00,720
and catastrophic risks and existential risks.

148
00:07:00,720 --> 00:07:04,040
I think the community of people

149
00:07:04,040 --> 00:07:06,720
who were thinking about AI risks a long time ago

150
00:07:06,720 --> 00:07:09,000
would largely think about whether there's a direct,

151
00:07:09,000 --> 00:07:13,560
simple, causal pathway to something like an extinction event.

152
00:07:13,560 --> 00:07:16,200
Now I think we have more of a sophisticated causal

153
00:07:16,200 --> 00:07:20,240
understanding of the interplay between these various factors,

154
00:07:20,240 --> 00:07:24,080
such that one doesn't try and look for direct mechanisms,

155
00:07:24,080 --> 00:07:26,840
but instead tries to look at what sort of events

156
00:07:26,840 --> 00:07:28,960
increase the probability of existential risk

157
00:07:28,960 --> 00:07:31,360
rather than does it directly cause extinction.

158
00:07:31,360 --> 00:07:33,320
And that distinction between something

159
00:07:33,320 --> 00:07:36,520
that increases probability versus directly caused

160
00:07:36,520 --> 00:07:39,520
means that we have to look at a much broader variety

161
00:07:39,520 --> 00:07:42,520
of factors and we can't end up just thinking

162
00:07:42,520 --> 00:07:45,080
that all we need to do is make a single,

163
00:07:45,080 --> 00:07:47,840
very powerful AI agent do what we want

164
00:07:47,840 --> 00:07:49,840
and then everything is solved forever.

165
00:07:49,840 --> 00:07:52,520
Unfortunately, we're going to have to treat this

166
00:07:52,520 --> 00:07:54,320
as a broader socio-technical problem.

167
00:07:54,320 --> 00:07:58,040
We're going to have to consider the various stakeholders,

168
00:07:58,040 --> 00:08:02,120
the politics, indeed the geopolitics,

169
00:08:02,120 --> 00:08:03,800
the relations between different countries,

170
00:08:03,800 --> 00:08:06,560
the liability laws and all these other things

171
00:08:06,560 --> 00:08:10,520
because we're not in this sort of fume type of scenario.

172
00:08:10,520 --> 00:08:13,040
It would seem, it seems we're more into a slow takeoff.

173
00:08:13,040 --> 00:08:15,000
So many of these real-world considerations

174
00:08:15,000 --> 00:08:17,600
that were sort of a sidelined infuses distractions

175
00:08:17,600 --> 00:08:19,520
is actually where most of the action is.

176
00:08:19,520 --> 00:08:21,800
What would be examples of something

177
00:08:21,800 --> 00:08:25,400
that might put society in a worse position

178
00:08:26,400 --> 00:08:30,120
where we are less able to handle a powerful AI?

179
00:08:30,120 --> 00:08:34,000
A prime example would be World War III.

180
00:08:34,000 --> 00:08:36,680
If there's World War III conditioned on that,

181
00:08:36,680 --> 00:08:39,440
that increases the probability of existential risk

182
00:08:39,440 --> 00:08:41,080
from AI systems.

183
00:08:41,080 --> 00:08:44,920
This would spur a substantial AI arms race.

184
00:08:44,920 --> 00:08:47,960
We would quickly outsource lethality to them.

185
00:08:47,960 --> 00:08:49,520
We would not have nearly as much time

186
00:08:49,520 --> 00:08:54,000
for making them more aligned and reliable in that process,

187
00:08:54,000 --> 00:08:56,360
but still the competitive pressures

188
00:08:56,360 --> 00:08:59,600
would compel different states

189
00:08:59,600 --> 00:09:02,240
to create powerful AI weapons

190
00:09:02,240 --> 00:09:05,040
and eventually have that take up more and more

191
00:09:05,040 --> 00:09:07,880
of their military force.

192
00:09:07,880 --> 00:09:11,520
But that doesn't directly cause extinction.

193
00:09:12,640 --> 00:09:14,720
So if we try and back chain from that,

194
00:09:14,720 --> 00:09:17,920
the story gets much more complicated

195
00:09:17,920 --> 00:09:21,760
and so then it's not viewed in the scenarios

196
00:09:21,760 --> 00:09:23,000
that others were thinking of.

197
00:09:23,000 --> 00:09:25,840
There's an AI lab, they've suddenly got a God-like AI

198
00:09:25,840 --> 00:09:28,160
and then it has decisive strategic control

199
00:09:28,160 --> 00:09:29,400
over the entire world.

200
00:09:29,400 --> 00:09:31,840
How will they make sure that it does what they want?

201
00:09:31,840 --> 00:09:34,240
That was, I think, the scenario

202
00:09:34,240 --> 00:09:36,360
that others were thinking largely

203
00:09:36,360 --> 00:09:41,360
and all other ones were too broad, too intractable.

204
00:09:41,920 --> 00:09:43,320
Essentially, there was a focus

205
00:09:43,320 --> 00:09:47,320
on quote-unquote targeted interventions historically,

206
00:09:47,320 --> 00:09:49,760
where we're just a small number of people.

207
00:09:49,760 --> 00:09:51,800
We can't do these broad interventions

208
00:09:51,880 --> 00:09:55,920
that involve interfacing with various institutions

209
00:09:55,920 --> 00:09:58,480
and getting public support.

210
00:09:58,480 --> 00:10:00,480
Those are intractable.

211
00:10:00,480 --> 00:10:02,720
So the best we can do is do some very narrow,

212
00:10:02,720 --> 00:10:05,160
specific things, maybe technical research.

213
00:10:05,160 --> 00:10:06,800
This doesn't look like a strategy

214
00:10:06,800 --> 00:10:09,320
because broad interventions are actually more tractable.

215
00:10:09,320 --> 00:10:10,600
The world is interested in this

216
00:10:10,600 --> 00:10:12,240
and we have some amount of time

217
00:10:12,240 --> 00:10:15,920
to try and help our institutions make good decisions

218
00:10:15,920 --> 00:10:17,520
and policy around these issues.

219
00:10:17,520 --> 00:10:21,520
Do you think this broader vision of AI safety

220
00:10:21,520 --> 00:10:24,200
should make us more positive or less positive?

221
00:10:24,200 --> 00:10:27,440
Imagine we have to set all of the institution up perfectly.

222
00:10:27,440 --> 00:10:29,400
It seems like we have a narrow corridor

223
00:10:29,400 --> 00:10:30,520
to make things right,

224
00:10:30,520 --> 00:10:32,760
where the institutions have to be there,

225
00:10:32,760 --> 00:10:34,240
the technical side have to work,

226
00:10:34,240 --> 00:10:38,280
the all stakeholders have to be set up well

227
00:10:38,280 --> 00:10:41,200
for this to succeed for us.

228
00:10:41,200 --> 00:10:43,000
Should the complexity of the problem

229
00:10:43,000 --> 00:10:45,320
make us more pessimistic?

230
00:10:45,320 --> 00:10:47,600
I think there's at least more tractability

231
00:10:47,600 --> 00:10:52,760
compared to an AI suddenly goes from incompetent

232
00:10:52,760 --> 00:10:55,520
to omnicompetent and in control of the world overnight

233
00:10:55,520 --> 00:10:57,640
and we have no idea if it was emergent

234
00:10:57,640 --> 00:10:59,440
and we didn't actually control that process.

235
00:10:59,440 --> 00:11:01,560
That doesn't have almost any tractability to it.

236
00:11:01,560 --> 00:11:03,280
I don't think we need our institutions

237
00:11:03,280 --> 00:11:05,760
to be completely perfect.

238
00:11:05,760 --> 00:11:09,440
We just need to try and be in the business of reducing risk.

239
00:11:09,440 --> 00:11:14,440
So maybe that's one other conceptual distinction

240
00:11:14,440 --> 00:11:16,880
is that historically there'd be a focus

241
00:11:17,080 --> 00:11:20,520
on is it an airtight solution that works in the worst case

242
00:11:20,520 --> 00:11:23,880
where if something goes wrong, then it's insufficient

243
00:11:23,880 --> 00:11:26,280
because we have to get it right on the first try.

244
00:11:26,280 --> 00:11:28,360
When we do have some amount of time,

245
00:11:28,360 --> 00:11:30,600
not saying we have a huge amount of time,

246
00:11:30,600 --> 00:11:31,480
we have some amount of time,

247
00:11:31,480 --> 00:11:33,320
we can do some course adjustment

248
00:11:33,320 --> 00:11:36,840
and incorporate information as we go along.

249
00:11:36,840 --> 00:11:41,720
I'm not saying that's a surefire strategy,

250
00:11:41,720 --> 00:11:44,600
but I think that's the best we have

251
00:11:44,640 --> 00:11:51,640
and it allows us to correct some mistakes,

252
00:11:51,640 --> 00:11:54,960
but obviously we can't have much of an error tolerance,

253
00:11:54,960 --> 00:11:55,800
unfortunately.

254
00:11:55,800 --> 00:11:57,360
Do you think we are missing something

255
00:11:57,360 --> 00:12:00,560
with this categorization that you've set up in the paper?

256
00:12:00,560 --> 00:12:02,640
Could we be missing some category of risk

257
00:12:02,640 --> 00:12:05,640
that will be obvious to us in 20 years?

258
00:12:05,640 --> 00:12:08,800
And could that risk potentially be the most dangerous

259
00:12:08,800 --> 00:12:10,960
because we're not anticipating it?

260
00:12:10,960 --> 00:12:14,480
Are there unknown unknowns here?

261
00:12:14,480 --> 00:12:17,560
Well, usually there are unknown unknowns.

262
00:12:17,560 --> 00:12:20,920
I focused largely on catastrophic risk

263
00:12:20,920 --> 00:12:23,320
and large-scale loss of human life.

264
00:12:23,320 --> 00:12:27,440
I didn't speak about AI well-being very much,

265
00:12:27,440 --> 00:12:30,080
or for instance, that's something that could end up

266
00:12:30,080 --> 00:12:33,680
changing a lot of how we think about wanting to proceed forward

267
00:12:33,680 --> 00:12:38,680
with managing the emergence of digital life

268
00:12:39,600 --> 00:12:42,000
is if they have moral value.

269
00:12:42,000 --> 00:12:47,120
So I think that's something I didn't touch on in the paper,

270
00:12:47,120 --> 00:12:49,320
largely because I think our understanding of it

271
00:12:49,320 --> 00:12:50,600
is very underdeveloped,

272
00:12:50,600 --> 00:12:54,160
and I still think it's a bit too much of a...

273
00:12:54,160 --> 00:12:56,120
It's a bit too much of a taboo topic

274
00:12:56,120 --> 00:12:59,000
such that it just hasn't that much research on it.

275
00:12:59,000 --> 00:13:01,600
Well, let's dig into the first category of risk,

276
00:13:01,600 --> 00:13:03,600
which is malicious use.

277
00:13:03,600 --> 00:13:08,600
So this is a category in which bad actors choose to use AI

278
00:13:08,600 --> 00:13:11,920
in ways that harm humanity.

279
00:13:12,520 --> 00:13:15,280
Recently, there's been a lot of discussion of AIs

280
00:13:15,280 --> 00:13:18,320
helping with bioengineered pandemics.

281
00:13:18,320 --> 00:13:22,800
This has been brought up in the US Senate, I think,

282
00:13:22,800 --> 00:13:26,560
and it's been kind of widely publicized.

283
00:13:26,560 --> 00:13:30,560
How plausible do you think it is that the current

284
00:13:30,560 --> 00:13:33,800
or the next generation of large-language models

285
00:13:33,800 --> 00:13:38,400
could make it easier to create bioengineered viruses?

286
00:13:38,400 --> 00:13:41,360
Yeah, so I think this is actually one of the largest reasons

287
00:13:41,360 --> 00:13:45,640
why I wrote this paper was because during 2022,

288
00:13:45,640 --> 00:13:48,640
when sort of the development of this paper started,

289
00:13:48,640 --> 00:13:49,840
this is this bio thing,

290
00:13:49,840 --> 00:13:52,040
yes, nobody's talking about it.

291
00:13:52,040 --> 00:13:55,160
This, although people will treat malicious use

292
00:13:55,160 --> 00:13:57,840
as a distraction, I don't think that's the case.

293
00:13:57,840 --> 00:14:01,280
There are a catastrophic and existential risk

294
00:14:01,280 --> 00:14:02,760
that can come from malicious use,

295
00:14:02,760 --> 00:14:06,040
so and this threat vector concerns me quite a bit.

296
00:14:06,040 --> 00:14:09,320
So I think it is quite plausible

297
00:14:09,360 --> 00:14:12,880
that if we have an AI system that has something

298
00:14:12,880 --> 00:14:17,560
like a PhD-level understanding of virology,

299
00:14:17,560 --> 00:14:19,360
then it's fairly straightforward

300
00:14:20,360 --> 00:14:24,560
that such a system would provide the knowledge

301
00:14:24,560 --> 00:14:28,080
for synthesizing such a weapon.

302
00:14:28,080 --> 00:14:30,080
The risk analysis is something like,

303
00:14:30,080 --> 00:14:33,000
what's the number of people with the skill and access

304
00:14:33,000 --> 00:14:36,240
to create a biological weapon

305
00:14:36,240 --> 00:14:38,320
that could be civilization destroying?

306
00:14:38,320 --> 00:14:42,280
And what's the sort of the probability

307
00:14:42,280 --> 00:14:44,280
that they're actually wanna do that?

308
00:14:44,280 --> 00:14:48,040
And right now, maybe there are 30,000 virology PhDs

309
00:14:48,040 --> 00:14:52,080
and they just don't really have the incentive to do that.

310
00:14:52,080 --> 00:14:55,920
Meanwhile, if you have that knowledge

311
00:14:55,920 --> 00:14:58,840
in available to anybody who wants to go to,

312
00:14:58,840 --> 00:15:01,160
use Google's chat bot or Meta's chat bot

313
00:15:01,160 --> 00:15:03,900
or Bing's or OpenAI's,

314
00:15:04,900 --> 00:15:08,700
then we can add several zeros to that,

315
00:15:08,700 --> 00:15:13,700
to the number of people with the skill to pull it off

316
00:15:13,700 --> 00:15:15,620
because they could just ask such a system,

317
00:15:15,620 --> 00:15:16,740
how do I make one?

318
00:15:16,740 --> 00:15:18,020
Give me a cookbook.

319
00:15:18,020 --> 00:15:20,660
Now, there'd be guardrails, of course,

320
00:15:20,660 --> 00:15:23,340
but the guardrails are fairly easy to overcome

321
00:15:23,340 --> 00:15:25,980
because these AI systems can easily be jailbroken.

322
00:15:25,980 --> 00:15:27,700
That's like if you can just append

323
00:15:27,700 --> 00:15:29,340
some random garbled string

324
00:15:29,340 --> 00:15:31,580
or some adversarily crafted garbled string

325
00:15:31,580 --> 00:15:34,220
at the end of your request to the chat bot

326
00:15:34,220 --> 00:15:35,060
and then that'll take off.

327
00:15:35,060 --> 00:15:36,900
It's like safety guardrails as a paper

328
00:15:36,900 --> 00:15:40,900
that the center helps with

329
00:15:42,420 --> 00:15:45,780
in creating and discussing adversarial attacks

330
00:15:45,780 --> 00:15:47,300
for large language models.

331
00:15:47,300 --> 00:15:48,620
So now that's a thing.

332
00:15:48,620 --> 00:15:52,460
Or you might use an open source model that's available

333
00:15:52,460 --> 00:15:56,500
and might also be easily stripped of its guardrails.

334
00:15:56,500 --> 00:16:00,660
I don't think the AI developers now with the APIs

335
00:16:00,700 --> 00:16:05,380
have much of a high ground as far as safety goes

336
00:16:05,380 --> 00:16:06,980
when it comes to the malicious use case

337
00:16:06,980 --> 00:16:08,060
for other things like hacking.

338
00:16:08,060 --> 00:16:11,180
There'd be a different story, but that could change.

339
00:16:11,180 --> 00:16:12,500
Maybe they'll add more measures.

340
00:16:12,500 --> 00:16:14,400
Maybe they'll get better filters.

341
00:16:14,400 --> 00:16:18,020
Maybe they will remove some bio-related knowledge

342
00:16:18,020 --> 00:16:21,500
from the pre-training distribution and so on.

343
00:16:21,500 --> 00:16:24,660
But anyway, that would be sufficient for,

344
00:16:24,660 --> 00:16:26,020
if such a thing were to happen,

345
00:16:26,020 --> 00:16:28,100
then there could be a pandemic

346
00:16:28,180 --> 00:16:31,540
that could cause some civilizational discontinuity,

347
00:16:31,540 --> 00:16:33,580
which could be some existential risk.

348
00:16:33,580 --> 00:16:35,940
It'd be difficult for it to kill everybody,

349
00:16:35,940 --> 00:16:38,300
but for toppling civilization.

350
00:16:38,300 --> 00:16:40,500
And it's not clear how that would go

351
00:16:40,500 --> 00:16:42,420
or the quality of that situation.

352
00:16:42,420 --> 00:16:44,620
That's enough for us to worry, I think.

353
00:16:44,620 --> 00:16:47,420
Some of the pushback to this story

354
00:16:47,420 --> 00:16:52,420
of a bio-engineered virus enabled by large language models

355
00:16:53,080 --> 00:16:55,740
is that, well, isn't all of the training data

356
00:16:55,740 --> 00:16:56,900
freely available online?

357
00:16:56,940 --> 00:17:00,740
Couldn't a potential bad actor have gone online,

358
00:17:00,740 --> 00:17:03,340
gotten the data and used it already?

359
00:17:03,340 --> 00:17:06,060
What's the difference between using a search engine

360
00:17:06,060 --> 00:17:08,780
and a large language model?

361
00:17:08,780 --> 00:17:11,940
Sure, so two things.

362
00:17:11,940 --> 00:17:15,800
Even if there is some type of harmful content online,

363
00:17:15,800 --> 00:17:18,940
I don't know why we would want it being propagated.

364
00:17:18,940 --> 00:17:21,180
If the nuclear secrets were online,

365
00:17:21,180 --> 00:17:23,180
I don't know why you'd want that propagated

366
00:17:23,180 --> 00:17:25,540
because your risk increases

367
00:17:25,540 --> 00:17:28,040
based on the ease of access to these.

368
00:17:28,040 --> 00:17:29,540
But in the case of bio-weapons,

369
00:17:29,540 --> 00:17:31,900
yes, there are some bio-weapons

370
00:17:31,900 --> 00:17:34,720
that are not civilization-destroying available online.

371
00:17:34,720 --> 00:17:38,500
The ones that would be potentially civilization-destroying,

372
00:17:38,500 --> 00:17:41,220
though, would require a bit more thinking.

373
00:17:42,700 --> 00:17:44,740
So there could be several,

374
00:17:44,740 --> 00:17:45,860
or there could be many people killed

375
00:17:45,860 --> 00:17:47,100
as a consequence of these, though,

376
00:17:47,100 --> 00:17:52,100
but not at a societal scale risk, necessarily.

377
00:17:52,220 --> 00:17:54,740
So I think that's a relevant difference.

378
00:17:54,740 --> 00:17:59,220
Many of the extremely dangerous pathogens,

379
00:17:59,220 --> 00:18:02,140
fortunately, virology people are not writing those up

380
00:18:02,140 --> 00:18:03,460
and posting those on Twitter,

381
00:18:03,460 --> 00:18:05,460
and then all you got to do is search for them.

382
00:18:05,460 --> 00:18:09,140
This isn't, that's not actually the type of information.

383
00:18:09,140 --> 00:18:10,820
For other types of information,

384
00:18:10,820 --> 00:18:14,140
like how to tips for breaking the law

385
00:18:14,140 --> 00:18:15,940
or how to wire a car,

386
00:18:15,940 --> 00:18:18,780
this sort of stuff is online and generic,

387
00:18:18,780 --> 00:18:20,780
a cookbooks for some generic,

388
00:18:20,780 --> 00:18:22,220
smaller-scale bio-weapons, sure,

389
00:18:22,220 --> 00:18:23,900
but not civilization-destroying.

390
00:18:23,900 --> 00:18:28,700
And how is the guide for creating a civilization-destroying

391
00:18:28,700 --> 00:18:30,860
virus in the large-vanguage model

392
00:18:30,860 --> 00:18:33,580
if it's not online, in the data online?

393
00:18:33,580 --> 00:18:36,820
So I am not saying that the current ones

394
00:18:36,820 --> 00:18:38,460
have this in their capacity.

395
00:18:38,460 --> 00:18:42,860
I'm saying that when they have like a PhD-level knowledge

396
00:18:42,860 --> 00:18:46,020
and are able to reflect and do a bit of brainstorming,

397
00:18:47,060 --> 00:18:49,820
then you're in substantially more trouble.

398
00:18:49,820 --> 00:18:52,620
And that could possibly be a model

399
00:18:52,620 --> 00:18:55,940
on the order of like GPT-5 or 5.5,

400
00:18:55,940 --> 00:18:57,340
it may be within its capacity.

401
00:18:57,340 --> 00:19:00,020
So there you don't need agent, like AI,

402
00:19:00,020 --> 00:19:03,260
you would just need a very knowledgeable chatbot

403
00:19:03,260 --> 00:19:06,540
for that threat to potentially manifest.

404
00:19:06,540 --> 00:19:08,100
So there's quite a bit we'll need to do

405
00:19:08,100 --> 00:19:13,100
to, in technical research and in policy,

406
00:19:14,660 --> 00:19:17,140
for reducing that specific risk.

407
00:19:17,140 --> 00:19:20,260
Another rescue you mentioned under malicious use

408
00:19:20,300 --> 00:19:22,500
is this issue of AI agents,

409
00:19:22,500 --> 00:19:26,380
which they are perhaps a bit analogous to viruses

410
00:19:26,380 --> 00:19:30,020
in the sense that they might be able to spread online

411
00:19:30,020 --> 00:19:35,020
and replicate themselves and cause harm.

412
00:19:35,060 --> 00:19:39,020
What do you worry about most with AI agents?

413
00:19:39,020 --> 00:19:42,060
I'm emphasizing, and since there are many forms

414
00:19:42,060 --> 00:19:43,340
of malicious use, in this paper,

415
00:19:43,340 --> 00:19:46,580
I'm mainly emphasizing ones that could be catastrophic

416
00:19:46,580 --> 00:19:48,460
or existential.

417
00:19:48,460 --> 00:19:52,140
So in this case, you could imagine people

418
00:19:52,140 --> 00:19:56,580
unleashing rogue AI systems to just destroy humanity.

419
00:19:56,580 --> 00:19:58,100
That could be their objective.

420
00:19:58,100 --> 00:20:00,020
And that would be extremely dangerous.

421
00:20:00,020 --> 00:20:03,420
So you don't need power-seeking arguments

422
00:20:03,420 --> 00:20:06,260
or these claims that, oh, by default,

423
00:20:06,260 --> 00:20:08,340
they will have a will to power.

424
00:20:08,340 --> 00:20:09,500
You don't need any of that.

425
00:20:09,500 --> 00:20:14,140
You just need to assume that if enough people have access,

426
00:20:14,140 --> 00:20:17,380
and if some person is omnicidal,

427
00:20:17,380 --> 00:20:21,780
or thinks in the way that some AI scientists do,

428
00:20:21,780 --> 00:20:24,740
that we need to bring about the next stage

429
00:20:24,740 --> 00:20:28,900
of cosmic evolution, and that resistance is futile

430
00:20:28,900 --> 00:20:30,140
to quote Richard Sutton,

431
00:20:30,140 --> 00:20:32,100
the author of the Reinforcement Learning textbook,

432
00:20:32,100 --> 00:20:34,820
and that we should bow out when it behooves us.

433
00:20:38,860 --> 00:20:42,260
There are many people who would have an inclination

434
00:20:42,260 --> 00:20:44,140
for building, not saying Richard Sutton

435
00:20:44,140 --> 00:20:47,980
would specifically give the AI system of destroy humanity,

436
00:20:47,980 --> 00:20:51,460
but doesn't seem to say too much against that prospect.

437
00:20:51,460 --> 00:20:55,140
So that's another example of malicious use

438
00:20:55,140 --> 00:20:58,140
that could be catastrophic or existential.

439
00:20:58,140 --> 00:21:02,140
And how close do you think we are to AI agents

440
00:21:02,140 --> 00:21:03,180
that actually work?

441
00:21:03,180 --> 00:21:08,180
We had someone set up a Chaos GPT early on

442
00:21:08,940 --> 00:21:12,460
when GPT was released, but it got stuck in some loops

443
00:21:12,460 --> 00:21:14,900
and it couldn't actually do anything,

444
00:21:14,900 --> 00:21:18,020
even if it was imbued with bad motives.

445
00:21:18,020 --> 00:21:21,940
When would you expect agents to actually be capable

446
00:21:21,940 --> 00:21:23,340
and therefore dangerous?

447
00:21:23,340 --> 00:21:25,380
Yeah, so I think that their capability

448
00:21:25,380 --> 00:21:27,900
would be a continuous thing in the same way,

449
00:21:27,900 --> 00:21:30,180
like when are they good at generating text?

450
00:21:30,180 --> 00:21:32,780
It's like, well, you know, it kind of started in GB2, GB3,

451
00:21:32,780 --> 00:21:37,780
and so I might anticipate great strides in AI agents next year,

452
00:21:38,780 --> 00:21:42,740
where we can give it some basic short tasks,

453
00:21:44,740 --> 00:21:47,940
like help me make this like PowerPoint or something.

454
00:21:47,940 --> 00:21:49,340
It's not gonna do the whole thing,

455
00:21:49,340 --> 00:21:51,940
but it can help with things like that

456
00:21:51,940 --> 00:21:55,380
or browsing around on the internet for you more.

457
00:21:55,380 --> 00:21:59,540
So I think those capabilities will keep coming

458
00:21:59,540 --> 00:22:02,420
for it to pose a substantial risk.

459
00:22:02,420 --> 00:22:05,020
There's a variety of things that could do

460
00:22:05,780 --> 00:22:09,780
it could threaten, for instance, mutually assured destruction

461
00:22:09,780 --> 00:22:13,780
with humanity by saying, I will make this bio weapon,

462
00:22:13,780 --> 00:22:16,220
that will destroy all of you, and I'll take you down with me,

463
00:22:16,220 --> 00:22:18,940
unless you comply with some types of demands.

464
00:22:18,940 --> 00:22:20,020
That could work.

465
00:22:20,020 --> 00:22:21,780
If they're good at hacking,

466
00:22:21,780 --> 00:22:24,380
then they could potentially amass a lot of resources

467
00:22:24,380 --> 00:22:28,780
by scamming people or by stealing cryptocurrency.

468
00:22:29,780 --> 00:22:32,780
There's a variety, they could do that,

469
00:22:32,780 --> 00:22:35,420
there's a variety, they could, of course,

470
00:22:35,420 --> 00:22:40,180
tap into lots of different sensors to manipulate people

471
00:22:40,180 --> 00:22:42,300
or influence public discourse.

472
00:22:43,620 --> 00:22:45,940
They wouldn't necessarily need to be embodied

473
00:22:45,940 --> 00:22:48,420
for this type of thing to happen.

474
00:22:48,420 --> 00:22:51,300
If we're in a later stage of AI development

475
00:22:51,300 --> 00:22:53,580
where we have a lot of weaponized AI systems

476
00:22:53,580 --> 00:22:55,340
and then hacking those systems will, of course,

477
00:22:55,340 --> 00:22:57,140
be substantially more concerning,

478
00:22:57,140 --> 00:22:59,740
or if those systems get repurposed

479
00:22:59,740 --> 00:23:01,860
maliciously to weaponized AI systems.

480
00:23:01,860 --> 00:23:05,660
So it becomes a lot easier as time progresses.

481
00:23:05,660 --> 00:23:08,380
The AIs don't need to be particularly power seeking

482
00:23:08,380 --> 00:23:13,380
on this view, though, to have this potential for catastrophe

483
00:23:14,860 --> 00:23:19,020
because humanity will basically give them that power by default.

484
00:23:20,140 --> 00:23:22,100
They will keep weaponizing them,

485
00:23:22,100 --> 00:23:25,740
they will integrate them into more and more critical decisions.

486
00:23:25,740 --> 00:23:29,660
They will let them move around money

487
00:23:29,700 --> 00:23:31,740
and complete transactions

488
00:23:31,740 --> 00:23:34,100
and they'll give them a looser and looser leash.

489
00:23:34,100 --> 00:23:38,580
So as time goes on, the potential for rogue AI

490
00:23:38,580 --> 00:23:41,220
or for deliberately, AI systems

491
00:23:41,220 --> 00:23:43,140
that are deliberately instructed to cause harm

492
00:23:43,140 --> 00:23:46,620
would be, the potential impact or severity

493
00:23:46,620 --> 00:23:47,580
would keep increasing.

494
00:23:47,580 --> 00:23:50,420
Yeah, I think maybe it's worth mentioning here

495
00:23:50,420 --> 00:23:55,420
just the continuous costs of traditional computer viruses

496
00:23:56,300 --> 00:24:01,100
which are costly and which we've gotten better

497
00:24:01,100 --> 00:24:03,820
at handling those as a civilization,

498
00:24:03,820 --> 00:24:08,340
but we still haven't defeated traditional conventional viruses

499
00:24:08,340 --> 00:24:12,500
which are very dumb compared to what AI agents could be.

500
00:24:12,500 --> 00:24:16,460
So we can imagine a computer virus

501
00:24:16,460 --> 00:24:18,380
equipped with more intelligence

502
00:24:18,380 --> 00:24:21,140
and how would you as a person,

503
00:24:21,140 --> 00:24:23,700
I'm not saying AI agents will be necessarily

504
00:24:23,700 --> 00:24:25,580
as smart as people soon,

505
00:24:25,580 --> 00:24:28,620
but how would you do the kind of hacking

506
00:24:28,620 --> 00:24:30,780
that the agent might be interested in?

507
00:24:30,780 --> 00:24:32,260
It's interesting to consider at least

508
00:24:32,260 --> 00:24:36,620
that we haven't been able to squash out conventional viruses.

509
00:24:36,620 --> 00:24:39,260
Yeah, they could exaltrate their information

510
00:24:39,260 --> 00:24:42,380
onto different servers or less protected ones

511
00:24:42,380 --> 00:24:46,420
and then use those to proliferate themselves even further.

512
00:24:46,420 --> 00:24:51,300
So they'll be a very distinct adversary

513
00:24:51,340 --> 00:24:55,700
with many, many options at their disposal for causing harm.

514
00:24:55,700 --> 00:24:59,780
Yeah, one thing I worry about is whether the tools

515
00:24:59,780 --> 00:25:03,700
and techniques we'll need at an institutional level

516
00:25:03,700 --> 00:25:07,660
to handle malicious use will also enable governments

517
00:25:07,660 --> 00:25:10,900
to become a totalitarian basically,

518
00:25:10,900 --> 00:25:15,900
to exercise too great a level of control over citizens

519
00:25:17,180 --> 00:25:18,620
who have done nothing wrong.

520
00:25:18,620 --> 00:25:23,580
So what is required to prevent the large language models

521
00:25:23,580 --> 00:25:26,100
that could become AI agents

522
00:25:26,100 --> 00:25:28,020
and could be used to create viruses?

523
00:25:28,020 --> 00:25:32,500
What techniques are available for preventing them

524
00:25:32,500 --> 00:25:36,420
being used in such ways without enabling

525
00:25:36,420 --> 00:25:39,380
kind of too much state power?

526
00:25:39,380 --> 00:25:42,260
Yeah, I think this is definitely a tension

527
00:25:42,260 --> 00:25:45,060
where to counteract these risks from rogue,

528
00:25:45,060 --> 00:25:46,860
lone wolf actors,

529
00:25:46,860 --> 00:25:50,980
then people would want the technology centralized.

530
00:25:52,500 --> 00:25:54,500
This would be a similarity with nuclear weapons,

531
00:25:54,500 --> 00:25:57,700
for instance, where we didn't want everybody

532
00:25:57,700 --> 00:26:00,260
being able to make nuclear weapons.

533
00:26:00,260 --> 00:26:02,500
We wanted to keep control of uranium.

534
00:26:02,500 --> 00:26:07,500
And so what happened was we had a no first use

535
00:26:07,500 --> 00:26:10,140
plus non-proliferation regime

536
00:26:10,140 --> 00:26:15,140
and that kept the power in a few different peoples' hands.

537
00:26:16,180 --> 00:26:17,260
I think there are things we could do

538
00:26:17,260 --> 00:26:20,220
to reduce these sorts of risks

539
00:26:20,220 --> 00:26:23,020
by creating institutions that are more democratic.

540
00:26:23,020 --> 00:26:24,220
I think that seems useful.

541
00:26:24,220 --> 00:26:27,460
I think decoupling the organizations

542
00:26:27,460 --> 00:26:30,980
that has some of the most powerful AI systems,

543
00:26:30,980 --> 00:26:35,980
having those more decoupled from the militaries

544
00:26:36,260 --> 00:26:37,620
would be fairly useful

545
00:26:37,620 --> 00:26:40,220
so that if something gets out of hand with line,

546
00:26:40,220 --> 00:26:44,060
if they're linked and if we're needing to pull the plug

547
00:26:44,060 --> 00:26:45,700
on these AI systems,

548
00:26:45,700 --> 00:26:47,100
this isn't like taking down the military.

549
00:26:47,100 --> 00:26:51,540
I think just separating this sort of cognitive labor

550
00:26:51,540 --> 00:26:55,140
and or labor generally, automated labor

551
00:26:55,140 --> 00:27:00,140
from a physical force would be fairly useful.

552
00:27:01,260 --> 00:27:03,340
But I think largely it's creating democratic,

553
00:27:03,340 --> 00:27:05,700
a democratic institutions is one of these measures.

554
00:27:05,700 --> 00:27:08,180
In the case of dealing with rogue AIs

555
00:27:08,180 --> 00:27:12,660
that are people maliciously instructed rogue AIs

556
00:27:12,660 --> 00:27:14,660
that are proliferating across the internet,

557
00:27:14,660 --> 00:27:16,020
I think there'd be other types of things

558
00:27:16,020 --> 00:27:21,020
like legal liability laws for cloud providers,

559
00:27:21,980 --> 00:27:26,980
that if you are running an unverified or unsafe AI system

560
00:27:30,020 --> 00:27:33,180
on your cloud or on your compute,

561
00:27:33,180 --> 00:27:34,980
then you get in trouble.

562
00:27:34,980 --> 00:27:37,700
This would create incentives for them to keep track of it

563
00:27:37,700 --> 00:27:41,060
instead of just doling out compute to whoever's paying.

564
00:27:41,060 --> 00:27:43,060
So that's sort of like having an incentive

565
00:27:43,060 --> 00:27:45,340
for off switches all over.

566
00:27:45,340 --> 00:27:49,100
So there's a variety of different things

567
00:27:49,100 --> 00:27:54,100
we could be doing to strike this balance

568
00:27:55,620 --> 00:27:57,860
by reducing these malicious use risks.

569
00:27:57,860 --> 00:27:59,660
I mean, also, as you mentioned,

570
00:27:59,660 --> 00:28:00,780
some of these malicious use risks

571
00:28:00,780 --> 00:28:03,060
don't require this type of centralization

572
00:28:03,060 --> 00:28:04,060
or nearly as much.

573
00:28:04,060 --> 00:28:06,060
We can do various things to reduce this risk

574
00:28:06,060 --> 00:28:09,500
without giving tons of power to states.

575
00:28:09,500 --> 00:28:13,260
For instance, we invest in personal protective equipment

576
00:28:13,260 --> 00:28:18,260
or monitoring waterways for early signs of some pathogens.

577
00:28:19,820 --> 00:28:22,580
I mean, there's the traditional stuff we can do

578
00:28:22,580 --> 00:28:24,780
to reduce risks from pandemics, for instance,

579
00:28:24,780 --> 00:28:27,540
which would reduce our exposure to the risk

580
00:28:27,540 --> 00:28:30,580
of AI-facilitated pandemics.

581
00:28:30,580 --> 00:28:34,500
So not all interventions for reducing malicious use

582
00:28:34,500 --> 00:28:36,580
require more centralization.

583
00:28:36,580 --> 00:28:41,020
I would imagine that we probably wouldn't want

584
00:28:41,020 --> 00:28:44,060
in the long term, like say it's like 2040

585
00:28:44,060 --> 00:28:47,140
or something like that, we wouldn't want anybody,

586
00:28:47,140 --> 00:28:49,380
anywhere being able just to ask the AI system

587
00:28:49,380 --> 00:28:52,420
how to make a pandemic or being able to unleash it

588
00:28:52,420 --> 00:28:55,340
to try and take over the world.

589
00:28:55,340 --> 00:28:58,340
This doesn't seem like a good idea.

590
00:28:58,340 --> 00:29:00,860
There'd be other types of things like structured access

591
00:29:00,860 --> 00:29:02,580
where for these bio capabilities,

592
00:29:02,580 --> 00:29:05,460
you just give people who are doing medical research

593
00:29:05,460 --> 00:29:07,380
access to the specific bio capabilities.

594
00:29:07,380 --> 00:29:09,580
But other people, they don't really have much of a reason

595
00:29:09,580 --> 00:29:11,660
for it, so they don't get that advanced,

596
00:29:11,660 --> 00:29:13,300
they don't get models with that advanced knowledge.

597
00:29:13,300 --> 00:29:15,180
So I think there are some simple restrictions

598
00:29:15,180 --> 00:29:17,860
that we can do that can take care of a large chunk

599
00:29:17,860 --> 00:29:21,780
of the risk without needing to hand over the technology

600
00:29:21,780 --> 00:29:23,660
to like militaries, and then they're the only ones

601
00:29:23,660 --> 00:29:24,500
who have it.

602
00:29:24,500 --> 00:29:28,260
You mentioned legal liabilities for cloud providers

603
00:29:28,260 --> 00:29:30,020
and maybe companies in general.

604
00:29:30,020 --> 00:29:33,140
I wonder if this might be a way to have a form

605
00:29:33,140 --> 00:29:38,140
of decentralized control over AI agents

606
00:29:39,620 --> 00:29:42,820
or over large language models or generative models,

607
00:29:42,820 --> 00:29:46,940
AI in general, by having the state provide a framework

608
00:29:46,940 --> 00:29:51,060
for where you can get fined for trespassing

609
00:29:51,100 --> 00:29:54,340
some boundaries, but then having companies

610
00:29:54,340 --> 00:29:56,580
implement exactly how that works,

611
00:29:56,580 --> 00:30:01,180
use technical tools in order to reduce their risk of fines

612
00:30:01,180 --> 00:30:05,180
and maybe we can find a good balance there

613
00:30:05,180 --> 00:30:08,500
where we weigh the costs and benefits.

614
00:30:08,500 --> 00:30:13,500
I think that liability laws help fix the problem

615
00:30:13,660 --> 00:30:15,820
of externalities quite a bit,

616
00:30:15,820 --> 00:30:18,060
where they're imposing risks on others

617
00:30:18,060 --> 00:30:23,060
that have no, shouldn't have any risk imposed on them

618
00:30:23,620 --> 00:30:26,060
because they're not privy to the decisions

619
00:30:26,060 --> 00:30:28,260
or there's an issue with that though,

620
00:30:28,260 --> 00:30:30,700
which is that there's only so many externalities

621
00:30:30,700 --> 00:30:33,820
that some of these organizations could internalize though

622
00:30:33,820 --> 00:30:34,900
with liability law.

623
00:30:34,900 --> 00:30:38,340
If somebody creates a pandemic

624
00:30:38,340 --> 00:30:40,380
as a consequence of their AI system,

625
00:30:40,380 --> 00:30:41,700
you could sue that company,

626
00:30:41,700 --> 00:30:44,180
but they're not gonna be able to pay off

627
00:30:44,180 --> 00:30:47,340
the destruction of civilization with their capital.

628
00:30:47,340 --> 00:30:49,300
So there's quite a limit to it.

629
00:30:49,300 --> 00:30:51,380
It can help fix the incentives,

630
00:30:51,380 --> 00:30:54,220
but it still doesn't fix them entirely

631
00:30:54,220 --> 00:30:56,780
because it's not particularly,

632
00:30:56,780 --> 00:30:58,540
when certainly can't internalize like downfall

633
00:30:58,540 --> 00:31:00,620
of like civilization as an organization

634
00:31:00,620 --> 00:31:02,700
and like foot the bill for that.

635
00:31:02,700 --> 00:31:05,500
And then the extinction of the human race is also,

636
00:31:05,500 --> 00:31:09,180
I don't think that's the thing you could settle in court.

637
00:31:09,180 --> 00:31:11,740
What about requiring insurance?

638
00:31:11,740 --> 00:31:13,580
So this is an idea that has been discussed

639
00:31:13,580 --> 00:31:17,820
for advanced biological research,

640
00:31:17,820 --> 00:31:20,860
gain a function research with viruses, for example.

641
00:31:20,860 --> 00:31:23,060
Maybe such a thing could also work

642
00:31:23,060 --> 00:31:27,940
for risky experiments with advanced AI.

643
00:31:27,940 --> 00:31:30,420
It depends if the harms are localized.

644
00:31:30,420 --> 00:31:33,900
I think insurance and this taming of typical,

645
00:31:33,900 --> 00:31:36,380
not long tail, not black swan type of uncertainty,

646
00:31:36,380 --> 00:31:39,580
but thin tailed type of uncertainty

647
00:31:39,580 --> 00:31:41,820
makes sense when risks are more localized,

648
00:31:41,820 --> 00:31:46,260
but when we are dealing with risks that are scalable

649
00:31:47,700 --> 00:31:49,900
and can bring down the entire system,

650
00:31:50,780 --> 00:31:54,700
then I think a lot of the incentives

651
00:31:54,700 --> 00:31:56,980
for insurance don't make as much sense.

652
00:31:56,980 --> 00:31:59,900
So you basically need like some law of large numbers

653
00:31:59,900 --> 00:32:02,180
and many types of insurance to like kick in

654
00:32:03,020 --> 00:32:06,020
to sort of have that risk diversified away.

655
00:32:06,020 --> 00:32:09,460
But if the entire system has exposure to that risk,

656
00:32:10,140 --> 00:32:12,300
there's not another system to diversify it.

657
00:32:12,300 --> 00:32:13,660
Maybe you could paint us a picture

658
00:32:13,660 --> 00:32:15,460
of a positive vision here.

659
00:32:15,460 --> 00:32:18,220
So say we get to 2050 and we've worked this out,

660
00:32:18,220 --> 00:32:20,140
what does the world look like in a world

661
00:32:20,140 --> 00:32:22,900
where we control malicious AI?

662
00:32:22,900 --> 00:32:26,100
I think if people have access to these AI systems,

663
00:32:26,100 --> 00:32:30,500
they're subject to, and they have many of their capabilities,

664
00:32:30,500 --> 00:32:31,820
there are of course restrictions on them,

665
00:32:31,820 --> 00:32:35,340
like you can't use them to break the law.

666
00:32:35,340 --> 00:32:37,500
So a lot of these most dangerous capabilities,

667
00:32:37,500 --> 00:32:40,660
nobody's really able to use them in that way.

668
00:32:40,660 --> 00:32:45,660
If there is a need for, in the case of like defense,

669
00:32:46,020 --> 00:32:47,820
they would end up using like AIs

670
00:32:47,820 --> 00:32:50,020
for things like hacking and whatnot.

671
00:32:50,020 --> 00:32:53,860
And that would, like they would have access

672
00:32:53,860 --> 00:32:54,860
to that type of technology,

673
00:32:54,860 --> 00:32:58,700
but it wouldn't be the case that any angsty teenager

674
00:32:58,700 --> 00:33:01,100
can just download a model online

675
00:33:01,100 --> 00:33:03,500
and then they instruct it to take down

676
00:33:03,500 --> 00:33:04,420
some critical infrastructure.

677
00:33:04,420 --> 00:33:06,220
This just isn't a possibility.

678
00:33:07,220 --> 00:33:09,500
It's very much trying to strike a balance with that.

679
00:33:09,500 --> 00:33:11,140
I would hope that we would also have

680
00:33:11,140 --> 00:33:12,460
these most powerful AI systems

681
00:33:12,460 --> 00:33:15,380
that do carry more of this force,

682
00:33:15,380 --> 00:33:17,660
that have some of these more dangerous capabilities

683
00:33:17,660 --> 00:33:20,300
are subject to democratic control,

684
00:33:20,300 --> 00:33:24,620
so that power is not as centralized.

685
00:33:24,620 --> 00:33:27,180
And that also I think reduces like the risk of like,

686
00:33:27,180 --> 00:33:30,260
put in quote, like lock in risks as well,

687
00:33:30,260 --> 00:33:34,460
where some individual group can impose their values

688
00:33:34,460 --> 00:33:36,660
and entrench them.

689
00:33:36,660 --> 00:33:39,860
So at least those are some properties

690
00:33:39,860 --> 00:33:42,540
of a positive future.

691
00:33:44,020 --> 00:33:47,420
So I don't think it looks like complete mass proliferation

692
00:33:47,420 --> 00:33:50,420
of extremely dangerous AI products.

693
00:33:50,420 --> 00:33:53,420
And I don't think it looks like only one group,

694
00:33:53,420 --> 00:33:56,700
one elite aristocrat group gets to make the decisions

695
00:33:56,700 --> 00:33:59,220
for humanity either.

696
00:33:59,220 --> 00:34:01,700
So there's different levels of access

697
00:34:01,700 --> 00:34:03,300
to different levels of lethality,

698
00:34:04,300 --> 00:34:08,340
and to empower depending on whether it makes sense.

699
00:34:08,340 --> 00:34:11,380
But the highest level institutions are still democratic.

700
00:34:11,380 --> 00:34:14,060
Another category of risks that you discuss

701
00:34:14,060 --> 00:34:17,100
is the possibility of an AI race.

702
00:34:17,100 --> 00:34:18,700
Now, we've done another episode

703
00:34:18,700 --> 00:34:21,300
where we talked about evolutionary pressures

704
00:34:21,300 --> 00:34:25,580
and how they work between corporations

705
00:34:25,580 --> 00:34:28,340
and how they might lead to a situation

706
00:34:28,340 --> 00:34:31,780
which humanity is gradually disempowered.

707
00:34:31,780 --> 00:34:35,060
But I think one thing we could discuss here in this episode

708
00:34:35,060 --> 00:34:39,100
is the possibility of a military AI race.

709
00:34:39,100 --> 00:34:42,260
What do you think a military AI race looks like?

710
00:34:42,260 --> 00:34:45,620
To recap, we were just at the malicious use one.

711
00:34:45,620 --> 00:34:48,540
And so now the other risk category would be like racing dynamics

712
00:34:48,540 --> 00:34:51,180
or competitive pressures or collective action problems.

713
00:34:51,180 --> 00:34:53,140
This is that structural environmental risk

714
00:34:53,140 --> 00:34:56,300
that when we were referring to the categories way earlier.

715
00:34:56,300 --> 00:34:58,340
Yeah, I think with the corporate race,

716
00:34:58,340 --> 00:35:01,940
obviously there's, as we discussed in the previous episode,

717
00:35:01,940 --> 00:35:03,900
there's them cutting corners on safety

718
00:35:03,900 --> 00:35:06,700
and this is largely what AI development is driven by.

719
00:35:06,700 --> 00:35:08,460
A lot of these organizations will start

720
00:35:08,460 --> 00:35:10,740
as having a very strong safety bent,

721
00:35:10,740 --> 00:35:13,420
but then they're basically gonna be pressured

722
00:35:13,420 --> 00:35:16,340
into just racing and prioritizing the profit

723
00:35:16,340 --> 00:35:18,340
and developing these things as quickly as possible

724
00:35:18,340 --> 00:35:20,620
and staying competitive over their safety.

725
00:35:20,620 --> 00:35:23,660
This is sort of the dynamic that basically drives

726
00:35:23,660 --> 00:35:24,860
pretty much all these AI companies.

727
00:35:24,860 --> 00:35:26,620
And I don't think actually in the presence

728
00:35:26,620 --> 00:35:27,940
of these intense competitive pressures

729
00:35:27,940 --> 00:35:30,220
that intentions particularly matter.

730
00:35:30,220 --> 00:35:35,220
So I think basically this is the main force to look at

731
00:35:36,780 --> 00:35:39,700
when trying to explain a major developments of AI,

732
00:35:39,700 --> 00:35:42,620
why are companies acting the way they are?

733
00:35:42,620 --> 00:35:46,620
It can be very well approximated by them

734
00:35:46,620 --> 00:35:50,260
just trying to, by them succumbing to competitive pressures

735
00:35:50,260 --> 00:35:53,020
or defecting in this broader collective action problem

736
00:35:53,020 --> 00:35:54,300
of should we slow down

737
00:35:54,300 --> 00:35:57,100
and should we proceed more prudently

738
00:35:57,100 --> 00:35:59,100
and invest more in safety

739
00:35:59,100 --> 00:36:01,100
and try and make sure our institutions are caught up

740
00:36:01,100 --> 00:36:05,580
or should we race ahead so that way we can continue

741
00:36:05,580 --> 00:36:07,060
being in the lead because one day

742
00:36:07,060 --> 00:36:09,580
we'll maybe be more responsible with this technology.

743
00:36:09,580 --> 00:36:11,900
I'm concerned, as mentioned in that previous episode

744
00:36:11,900 --> 00:36:15,700
of that leading us to like a state of substantial dependence

745
00:36:15,700 --> 00:36:18,060
and losing effective control,

746
00:36:18,060 --> 00:36:19,660
you can imagine similar dynamic happening

747
00:36:19,660 --> 00:36:23,340
with the military just like if we don't want,

748
00:36:23,420 --> 00:36:26,260
arrows for instance, you're not gonna roll back arrows.

749
00:36:26,260 --> 00:36:27,460
And so when you start going down the road

750
00:36:27,460 --> 00:36:30,140
of weaponizing AI systems,

751
00:36:30,140 --> 00:36:32,500
if they're more potent and cheaper

752
00:36:32,500 --> 00:36:34,180
and more generally capable

753
00:36:34,180 --> 00:36:35,780
and more politically convenient

754
00:36:35,780 --> 00:36:39,420
and sending human soldiers onto the battlefield,

755
00:36:39,420 --> 00:36:44,420
then this becomes a very difficult process to reverse back.

756
00:36:45,500 --> 00:36:49,580
Eventually what happens is you've had an on ramp

757
00:36:49,580 --> 00:36:53,140
to many more potential catastrophic risks.

758
00:36:53,140 --> 00:36:55,620
You've transferred much of the lethal power.

759
00:36:55,620 --> 00:36:59,500
In fact, the main source is the lethal power to AI systems.

760
00:36:59,500 --> 00:37:01,660
And then you're hoping that they're reliable enough

761
00:37:01,660 --> 00:37:03,140
and that you've sufficiently,

762
00:37:03,140 --> 00:37:05,300
you can keep them under sufficient control

763
00:37:05,300 --> 00:37:06,820
and that they can do your bidding.

764
00:37:06,820 --> 00:37:10,700
Even if you do get them highly reliable

765
00:37:10,700 --> 00:37:13,940
and they do what you instruct them to do,

766
00:37:13,940 --> 00:37:16,460
this doesn't make people overall very safe.

767
00:37:16,460 --> 00:37:18,460
We saw with the Cuban Missile Crisis,

768
00:37:18,460 --> 00:37:20,620
we can definitely, nukes don't turn on us.

769
00:37:20,620 --> 00:37:22,140
They don't go off and pursue their own goals

770
00:37:22,140 --> 00:37:22,980
or something like that.

771
00:37:22,980 --> 00:37:27,020
They do what we want them to do,

772
00:37:27,020 --> 00:37:29,540
but collectively do this structural,

773
00:37:29,540 --> 00:37:32,700
environmental game theoretic situation

774
00:37:32,700 --> 00:37:34,420
where like, wow, we would all be better off

775
00:37:34,420 --> 00:37:35,620
without nuclear weapons,

776
00:37:35,620 --> 00:37:40,500
but it makes sense for us each individually to stockpile them.

777
00:37:40,500 --> 00:37:43,180
We put the broader world at larger collective risks.

778
00:37:43,180 --> 00:37:45,820
So like in the Cuban Missile Crisis,

779
00:37:45,820 --> 00:37:48,380
JFK said we had up to like a half

780
00:37:48,380 --> 00:37:50,540
or like a 50% chance of extinction in that event.

781
00:37:50,540 --> 00:37:52,260
It was a very close call

782
00:37:52,260 --> 00:37:55,380
because we almost got a nuclear exchange with that.

783
00:37:55,380 --> 00:37:58,300
And likewise with AI systems, they may be more powerful.

784
00:37:58,300 --> 00:38:00,220
They may be better at facilitating the development

785
00:38:00,220 --> 00:38:01,660
of new weapons too.

786
00:38:01,660 --> 00:38:06,220
And this could also bring us at a risk

787
00:38:06,220 --> 00:38:07,980
where bring us in a situation

788
00:38:07,980 --> 00:38:12,700
where we could potentially destroy ourselves again.

789
00:38:12,700 --> 00:38:16,500
What's pernicious about this structural

790
00:38:16,500 --> 00:38:18,860
or environmental constraint

791
00:38:18,860 --> 00:38:22,900
where we've got different parties, in this case,

792
00:38:22,900 --> 00:38:27,220
militaries competing against each other is the following.

793
00:38:27,220 --> 00:38:30,140
Even if we convince the world

794
00:38:30,140 --> 00:38:33,460
that like the existential risk from AI is like 5%

795
00:38:33,460 --> 00:38:36,220
because let's say they're not reliable.

796
00:38:36,220 --> 00:38:38,100
We can't reliably control them.

797
00:38:38,100 --> 00:38:40,060
So maybe there's a 5% chance to like turn on us

798
00:38:40,060 --> 00:38:41,060
or we lose control of them

799
00:38:41,060 --> 00:38:43,580
and then we become a second class species or exterminate them.

800
00:38:43,580 --> 00:38:44,540
Even if that's the case,

801
00:38:44,540 --> 00:38:47,780
it may make sense for these militaries to go along with it.

802
00:38:47,780 --> 00:38:50,100
Just like, I mean, they swallowed the risk

803
00:38:50,100 --> 00:38:52,980
of potential nuclear Armageddon

804
00:38:52,980 --> 00:38:55,860
by creating these nuclear weapons in the first place.

805
00:38:55,860 --> 00:38:58,820
But they thought if we don't create these nuclear weapons,

806
00:38:58,820 --> 00:39:00,700
then we will certainly be destroyed.

807
00:39:00,700 --> 00:39:02,420
So there's certainty of destruction

808
00:39:02,420 --> 00:39:06,580
versus a small chance of destruction.

809
00:39:06,580 --> 00:39:08,540
And I think they'd be willing to make that trade off.

810
00:39:08,540 --> 00:39:11,380
So this is how there could be an existential risk

811
00:39:11,380 --> 00:39:15,140
to all of humanity based on these structural conditions.

812
00:39:15,900 --> 00:39:18,420
So it's not enough to convince the world

813
00:39:18,420 --> 00:39:20,420
that existential risk is high

814
00:39:20,420 --> 00:39:24,980
because they might just, okay, well, yeah, that's 5%.

815
00:39:24,980 --> 00:39:26,900
Okay, we're gonna have to go with that rational left thing.

816
00:39:26,900 --> 00:39:30,420
It makes rational sense for us to engage in this,

817
00:39:30,420 --> 00:39:32,420
what would normally be very risky behavior

818
00:39:32,420 --> 00:39:34,140
because we don't have a better choice.

819
00:39:34,140 --> 00:39:37,060
So this is why I don't think it makes sense

820
00:39:37,060 --> 00:39:39,380
just to hammer home the point that, wow,

821
00:39:39,380 --> 00:39:40,820
these AIs could turn on us

822
00:39:40,820 --> 00:39:42,780
or we could lose control of them.

823
00:39:42,780 --> 00:39:44,180
There's this structural thing of like,

824
00:39:44,180 --> 00:39:46,340
that's not gonna matter unless that probability

825
00:39:46,340 --> 00:39:47,220
is like very high.

826
00:39:47,220 --> 00:39:50,140
Like maybe if it's like 30% and they go, okay, all right,

827
00:39:50,140 --> 00:39:52,180
we're not gonna build the thing because,

828
00:39:52,180 --> 00:39:53,980
but if it's something like 5%,

829
00:39:53,980 --> 00:39:55,140
they might go through with it anyway.

830
00:39:55,140 --> 00:40:00,140
So more than just concerns about single AI agents

831
00:40:00,620 --> 00:40:02,860
make sense or make sense to focus on,

832
00:40:02,860 --> 00:40:05,060
we have to focus on these multi-agent dynamics,

833
00:40:05,060 --> 00:40:06,580
these competitive pressures,

834
00:40:06,580 --> 00:40:11,100
the sort of the game theory of what they're facing.

835
00:40:11,100 --> 00:40:15,260
And so I think that if you don't resolve that,

836
00:40:15,260 --> 00:40:17,940
you're basically exposed to insensitivity

837
00:40:17,940 --> 00:40:22,620
to a lot of existential risk up to maybe 5% or 10%,

838
00:40:22,620 --> 00:40:25,020
which maybe it's possible.

839
00:40:25,020 --> 00:40:27,380
Maybe it's actually only 2%.

840
00:40:27,380 --> 00:40:28,580
And when you convince the world,

841
00:40:28,580 --> 00:40:30,180
everybody's very educated about it.

842
00:40:30,180 --> 00:40:33,060
Everybody listens to Future of Life podcast tomorrow

843
00:40:33,060 --> 00:40:34,740
and they all go, wow, this is a concern.

844
00:40:34,740 --> 00:40:35,900
I am updated to 5%.

845
00:40:35,900 --> 00:40:37,020
Won't matter.

846
00:40:37,020 --> 00:40:38,700
It won't stop that type of dynamic from happening.

847
00:40:38,740 --> 00:40:41,300
You have to fix the international coordination issue.

848
00:40:41,300 --> 00:40:43,460
You have to avoid this sort of potential

849
00:40:43,460 --> 00:40:44,820
for World War III thing.

850
00:40:44,820 --> 00:40:46,940
Now it didn't directly cause it,

851
00:40:46,940 --> 00:40:48,740
as we were discussing earlier.

852
00:40:48,740 --> 00:40:50,580
This wasn't a direct cause of extinction,

853
00:40:50,580 --> 00:40:52,460
but it increased the probability substantially.

854
00:40:52,460 --> 00:40:54,420
That's the sort of framing we have to focus on

855
00:40:54,420 --> 00:40:56,140
in trying to reduce existential risk,

856
00:40:56,140 --> 00:40:57,980
not search for direct cause of mechanisms,

857
00:40:57,980 --> 00:40:59,260
but look at these diffuse effects

858
00:40:59,260 --> 00:41:00,820
and structural conditions.

859
00:41:00,820 --> 00:41:03,740
Yeah, so concretely, this might look like

860
00:41:03,740 --> 00:41:07,460
the US is considering implementing AI systems

861
00:41:07,500 --> 00:41:10,980
into their nuclear command and control systems.

862
00:41:10,980 --> 00:41:12,740
So specifically, they're doing this

863
00:41:12,740 --> 00:41:17,180
to counteract the rumors of other countries

864
00:41:17,180 --> 00:41:18,500
doing the same thing.

865
00:41:18,500 --> 00:41:20,220
And in order to act quickly enough

866
00:41:20,220 --> 00:41:21,580
with their nuclear weapons,

867
00:41:21,580 --> 00:41:26,580
they think they need to give AI a greater degree

868
00:41:26,980 --> 00:41:31,100
of control over these nuclear weapons.

869
00:41:31,100 --> 00:41:35,300
And so you have a situation in which

870
00:41:35,300 --> 00:41:39,900
countries are responding to the actions of each other

871
00:41:39,900 --> 00:41:44,900
in a way that accelerates risks from both sides

872
00:41:45,580 --> 00:41:47,180
in this innocent.

873
00:41:47,180 --> 00:41:48,020
There'd be one.

874
00:41:48,020 --> 00:41:51,980
I mean, there are other ways this can affect warfare.

875
00:41:51,980 --> 00:41:56,420
It could maybe be better at doing anomaly detection

876
00:41:56,420 --> 00:41:58,660
thereby identify nuclear submarines

877
00:41:58,660 --> 00:42:01,140
and affect the nuclear triad that way.

878
00:42:01,140 --> 00:42:02,780
Or in later stages,

879
00:42:02,780 --> 00:42:06,460
they just have massive fleets of AI.

880
00:42:06,460 --> 00:42:08,300
And this is saying robot, sorry to say,

881
00:42:08,300 --> 00:42:10,220
but like later stage,

882
00:42:10,220 --> 00:42:12,060
if they're much cheaper to produce,

883
00:42:12,060 --> 00:42:14,100
they'd be very good combatants.

884
00:42:14,100 --> 00:42:15,740
There isn't skin in the game.

885
00:42:15,740 --> 00:42:17,380
This increases the,

886
00:42:17,380 --> 00:42:19,860
this makes it more feasible to get into conflict.

887
00:42:19,860 --> 00:42:20,940
There are other ways in which

888
00:42:20,940 --> 00:42:22,660
this increases the probability of conflict too.

889
00:42:22,660 --> 00:42:23,620
There's more uncertainty

890
00:42:23,620 --> 00:42:26,180
about where your competitors are relative to you.

891
00:42:26,180 --> 00:42:28,140
Maybe they had an algorithmic breakthrough.

892
00:42:28,140 --> 00:42:31,260
Maybe they could actually catch up really quickly

893
00:42:31,260 --> 00:42:34,140
or surpass us by finding some algorithmic breakthrough.

894
00:42:34,140 --> 00:42:36,580
This creates severe or extreme uncertainty

895
00:42:36,580 --> 00:42:39,260
about the capabilities profile of adversaries.

896
00:42:39,260 --> 00:42:41,340
This lack of information about that

897
00:42:41,340 --> 00:42:45,700
increases the chance of conflict as well.

898
00:42:45,700 --> 00:42:49,220
It may also increase first strike advantage substantially too,

899
00:42:49,220 --> 00:42:52,540
which would also increase the probability of conflict.

900
00:42:52,540 --> 00:42:54,660
Like we have an AI system today,

901
00:42:54,660 --> 00:42:56,860
it's much more powerful than anything else.

902
00:42:56,860 --> 00:42:58,020
They might get theirs tomorrow.

903
00:42:58,020 --> 00:43:01,020
If we act today, then we can squash them.

904
00:43:01,500 --> 00:43:06,420
That could get the ball rolling for some global catastrophe.

905
00:43:06,420 --> 00:43:11,420
So yeah, pretty pernicious dynamics overall.

906
00:43:12,220 --> 00:43:14,620
But all of these can be viewed as

907
00:43:14,620 --> 00:43:17,180
competitive pressures driving AI systems

908
00:43:17,180 --> 00:43:21,180
and propagating throughout all aspects of life.

909
00:43:21,180 --> 00:43:25,900
We mentioned through the public sphere in the economy,

910
00:43:25,900 --> 00:43:28,060
people's private lives with AI chatbots,

911
00:43:28,060 --> 00:43:30,020
also in defense, in the military.

912
00:43:30,020 --> 00:43:31,980
It just basically becomes everywhere

913
00:43:31,980 --> 00:43:34,020
and we end up relying more and more on them

914
00:43:34,020 --> 00:43:35,540
to make these sorts of decisions.

915
00:43:35,540 --> 00:43:37,380
And I don't think in many of these,

916
00:43:37,380 --> 00:43:40,980
we become so dependent on them that things move quickly.

917
00:43:40,980 --> 00:43:42,180
We can't actually keep up.

918
00:43:42,180 --> 00:43:44,140
We can't make, if we're actually making these decisions,

919
00:43:44,140 --> 00:43:45,340
we'll make much worse decisions.

920
00:43:45,340 --> 00:43:48,420
So then they basically become in effective control.

921
00:43:48,420 --> 00:43:51,700
Things also move so quickly that the answer to our AI problems

922
00:43:51,700 --> 00:43:54,420
is we need to bring in more AIs

923
00:43:54,420 --> 00:43:56,260
because since they're using more AIs,

924
00:43:56,260 --> 00:43:57,420
now we need to use more AIs.

925
00:43:57,420 --> 00:43:59,500
And so it creates a self-reinforcing feedback loop

926
00:43:59,500 --> 00:44:01,980
which ends up eroding our overall influence

927
00:44:01,980 --> 00:44:03,900
and oversight as to what's going on.

928
00:44:03,900 --> 00:44:05,340
And so I think that's the default one.

929
00:44:05,340 --> 00:44:06,980
So of these sort of risk categories,

930
00:44:06,980 --> 00:44:10,540
I think this seems like straightforwardly the case

931
00:44:10,540 --> 00:44:13,220
if we don't fix international coordination

932
00:44:13,220 --> 00:44:16,660
and if there's a close competition between countries

933
00:44:16,660 --> 00:44:20,900
or if we don't fix the racing dynamics

934
00:44:20,900 --> 00:44:23,580
in the corporate sphere,

935
00:44:23,580 --> 00:44:27,620
then I think it's fairly likely that humanity becomes

936
00:44:27,620 --> 00:44:31,220
at least like a second class species loses control

937
00:44:31,220 --> 00:44:34,180
from there eventually, probably they go extinct,

938
00:44:34,180 --> 00:44:35,660
but that might be a long time after.

939
00:44:35,660 --> 00:44:40,660
But so this is the main risk that I'm worried about

940
00:44:41,140 --> 00:44:42,940
but as Director of Center for AISAD,

941
00:44:42,940 --> 00:44:46,260
I'll try and be acumenical and focus on various others too.

942
00:44:46,260 --> 00:44:47,460
So I'm always making sure that our projects

943
00:44:47,460 --> 00:44:48,300
addressing each of these though,

944
00:44:48,300 --> 00:44:51,140
but personally, this is the one that I'm most concerned about.

945
00:44:51,140 --> 00:44:53,060
So treaties between governments

946
00:44:53,060 --> 00:44:55,180
and some form of collaboration

947
00:44:55,220 --> 00:44:57,620
between the top AI corporations,

948
00:44:57,620 --> 00:44:59,500
is that the way out here?

949
00:44:59,500 --> 00:45:01,340
How do we mitigate this risk?

950
00:45:01,340 --> 00:45:03,340
It seems at the way you describe it,

951
00:45:03,340 --> 00:45:05,660
it seems very difficult to avoid

952
00:45:05,660 --> 00:45:08,140
given the incentives basically.

953
00:45:08,140 --> 00:45:10,220
People respond to incentives,

954
00:45:10,220 --> 00:45:12,140
they rationally respond to incentives.

955
00:45:12,140 --> 00:45:15,140
And so for each step along the way,

956
00:45:15,140 --> 00:45:17,700
they have reasons to do what they're doing.

957
00:45:17,700 --> 00:45:19,620
And so it seems difficult to avoid.

958
00:45:19,620 --> 00:45:21,820
What are our options?

959
00:45:21,820 --> 00:45:26,460
Well, there are positive signs.

960
00:45:26,460 --> 00:45:27,980
For instance, like Henry Kissinger

961
00:45:27,980 --> 00:45:30,260
was recently suggested in foreign affairs

962
00:45:30,260 --> 00:45:33,860
that the US cooperate with China on this issue now,

963
00:45:34,860 --> 00:45:35,940
but before it's too late.

964
00:45:35,940 --> 00:45:38,300
So I think some people are recognizing

965
00:45:38,300 --> 00:45:42,300
the importance of trying to do something about this.

966
00:45:42,300 --> 00:45:45,820
There's, it's possible there'd be some clarifications

967
00:45:45,820 --> 00:45:48,060
about antitrust law, which would make it possible

968
00:45:48,060 --> 00:45:51,460
for AI companies to not engage in excessive competition

969
00:45:51,900 --> 00:45:54,620
over this and put the whole world at risk.

970
00:45:55,540 --> 00:45:59,260
Potentially there could be an international institution

971
00:45:59,260 --> 00:46:04,260
like a CERN for AI, which is the default organization,

972
00:46:07,380 --> 00:46:12,380
which has a broad consortium or coalition of countries

973
00:46:13,300 --> 00:46:16,700
providing input to that and helping steer it.

974
00:46:16,700 --> 00:46:19,380
One that's maybe decoupled from,

975
00:46:19,380 --> 00:46:21,740
to some extent of militaries,

976
00:46:22,620 --> 00:46:24,540
so that we're not having too much power centralized

977
00:46:24,540 --> 00:46:25,380
in one place.

978
00:46:25,380 --> 00:46:26,620
So it doesn't have a monopoly on violence

979
00:46:26,620 --> 00:46:28,820
and eventually after automates a lot of monopoly on labor.

980
00:46:28,820 --> 00:46:31,100
I think that's just like basically all the power

981
00:46:31,100 --> 00:46:31,940
in the world.

982
00:46:31,940 --> 00:46:33,180
So those are possibilities.

983
00:46:33,180 --> 00:46:36,380
I think that the time window might be a bit shorter though.

984
00:46:36,380 --> 00:46:41,020
If there's an arms race and AI arms race in the military,

985
00:46:41,020 --> 00:46:42,940
and if the AI is viewed as like the main thing

986
00:46:42,940 --> 00:46:46,340
to be competing on, like we need to spend a trillion dollars,

987
00:46:46,460 --> 00:46:50,620
we'll spend on that order for nuclear weapons.

988
00:46:50,620 --> 00:46:52,380
If when that becomes the case,

989
00:46:52,380 --> 00:46:54,860
I think it's where we're very much set down that path

990
00:46:54,860 --> 00:46:57,900
and then we're exposed to very substantial risks.

991
00:46:57,900 --> 00:47:01,260
So yeah, I think maybe we'll have a sense

992
00:47:01,260 --> 00:47:04,460
in the next few years as to whether we get some type

993
00:47:04,460 --> 00:47:06,660
of coordination or if we are not gonna recognize

994
00:47:06,660 --> 00:47:08,260
that we're all in the same boat as humans

995
00:47:08,260 --> 00:47:09,580
and we don't want this to happen.

996
00:47:09,580 --> 00:47:12,420
But we'll need people to basically understand what happens

997
00:47:12,420 --> 00:47:13,980
if we go down this route and if we don't try

998
00:47:13,980 --> 00:47:18,140
and fix the payoff matrix, the incentives at the outset,

999
00:47:18,140 --> 00:47:21,820
the structure that these players find themselves in

1000
00:47:21,820 --> 00:47:23,860
or that these developers find themselves in.

1001
00:47:23,860 --> 00:47:26,980
That looks like a very much a political problem

1002
00:47:26,980 --> 00:47:28,300
as it happens.

1003
00:47:28,300 --> 00:47:32,220
So this is why making, reducing AI, X risk and whatnot

1004
00:47:32,220 --> 00:47:35,740
and making AI safe is a socio-technical problem.

1005
00:47:35,740 --> 00:47:39,740
It's not writing down an eight page mathematical solution,

1006
00:47:39,740 --> 00:47:41,340
a work of genius and then, oh, okay,

1007
00:47:41,340 --> 00:47:43,180
we can all go home now and everything's taken care of.

1008
00:47:43,180 --> 00:47:44,820
It's not gonna look like that.

1009
00:47:44,820 --> 00:47:47,700
That was a category error in understanding

1010
00:47:47,700 --> 00:47:49,260
how to reduce this risk.

1011
00:47:49,260 --> 00:47:52,900
We shouldn't have these types of founders effects

1012
00:47:52,900 --> 00:47:56,940
have like undue influence over, like it will keep lingering.

1013
00:47:56,940 --> 00:47:58,460
I think that will eventually like go away

1014
00:47:58,460 --> 00:48:00,060
but I still think it's still like lingering

1015
00:48:00,060 --> 00:48:01,860
and I think we should just like move past it

1016
00:48:01,860 --> 00:48:04,780
and recognize the complexity of the situation.

1017
00:48:04,780 --> 00:48:07,380
Let's talk about organizational risks

1018
00:48:07,380 --> 00:48:10,340
and these risk categories, of course,

1019
00:48:10,340 --> 00:48:12,620
kind of play into each other, influence each other.

1020
00:48:12,620 --> 00:48:17,620
So if we have organizations that are acting in a risky way,

1021
00:48:17,940 --> 00:48:22,780
that this increases the risk of potentially rogue AI

1022
00:48:22,780 --> 00:48:26,500
or it incentivizes others to race

1023
00:48:26,500 --> 00:48:29,620
in order to compete with these organizations

1024
00:48:29,620 --> 00:48:33,060
that are acting in risky ways.

1025
00:48:33,060 --> 00:48:35,380
But yeah, let's just take it from the beginning.

1026
00:48:35,380 --> 00:48:40,380
What falls under the organizational risks category?

1027
00:48:41,380 --> 00:48:44,780
Yeah, so organizational risks at a slightly more abstract level

1028
00:48:44,780 --> 00:48:47,340
would be the accidents bucket.

1029
00:48:47,340 --> 00:48:52,260
So even if we reduce competitive pressures

1030
00:48:52,260 --> 00:48:56,260
and if we have a,

1031
00:48:57,500 --> 00:49:02,500
and if we don't have to worry about malicious use immediately,

1032
00:49:03,660 --> 00:49:06,140
we'd still have the issue of organizations

1033
00:49:06,140 --> 00:49:09,100
having maybe a culture of move fast and break things

1034
00:49:09,140 --> 00:49:12,100
or them not having a safety culture.

1035
00:49:12,100 --> 00:49:14,940
In other industries or for other technologies

1036
00:49:14,940 --> 00:49:18,620
like rockets that wasn't extreme competition with that

1037
00:49:18,620 --> 00:49:20,820
but nonetheless, rockets would blow up

1038
00:49:20,820 --> 00:49:23,180
or nuclear power plants would melt down,

1039
00:49:23,180 --> 00:49:25,540
catastrophic accidents can still happen

1040
00:49:25,540 --> 00:49:28,300
and these can be very deadly

1041
00:49:28,300 --> 00:49:30,500
in the case of AI systems eventually.

1042
00:49:30,500 --> 00:49:35,500
So I think this is definitely a very hard one to fix.

1043
00:49:35,860 --> 00:49:38,260
Most of the people at these AI organizations

1044
00:49:38,260 --> 00:49:40,420
and how they were initialized and whatnot

1045
00:49:40,420 --> 00:49:43,500
still had a lot of people who are mostly just wanting

1046
00:49:43,500 --> 00:49:48,020
to build it and the consequences of society be damned.

1047
00:49:48,020 --> 00:49:49,820
This is not my wheelhouse, I don't read the news.

1048
00:49:49,820 --> 00:49:51,460
I don't like thinking about this sort of stuff.

1049
00:49:51,460 --> 00:49:53,900
This is annoying humanities majors and whatnot

1050
00:49:53,900 --> 00:49:56,580
who are in these ethics divisions or policy divisions

1051
00:49:56,580 --> 00:49:58,220
that keep annoying us.

1052
00:49:58,220 --> 00:49:59,980
This is kind of the attitude

1053
00:49:59,980 --> 00:50:03,220
that most of these companies buy in large.

1054
00:50:03,220 --> 00:50:08,220
And I think this is a large source of risk.

1055
00:50:08,860 --> 00:50:13,860
We could, as well as it's just non-trivial

1056
00:50:13,940 --> 00:50:17,500
as we see in other things like nuclear power plants,

1057
00:50:17,500 --> 00:50:20,100
chemical plants, rockets and making sure

1058
00:50:20,100 --> 00:50:22,700
that this is all extremely reliable.

1059
00:50:22,700 --> 00:50:25,900
So we'd need various precedents.

1060
00:50:25,900 --> 00:50:27,540
There's basically a literature on this

1061
00:50:27,540 --> 00:50:30,420
called the organizational safety literature

1062
00:50:30,420 --> 00:50:34,540
which focuses on various corporate controls

1063
00:50:34,540 --> 00:50:36,620
and processes for making sure

1064
00:50:36,620 --> 00:50:39,260
that the organization responds to failure,

1065
00:50:39,260 --> 00:50:42,460
takes near misses seriously, has good whistleblowing,

1066
00:50:42,460 --> 00:50:44,940
has good internal risk management regimes,

1067
00:50:44,940 --> 00:50:48,260
has like a chief risk officer or an internal audit committee,

1068
00:50:48,260 --> 00:50:49,460
all of these sorts of things

1069
00:50:49,460 --> 00:50:51,940
to reduce these types of risks.

1070
00:50:51,940 --> 00:50:54,460
And yeah, you were right in that this interacts

1071
00:50:54,460 --> 00:50:56,860
with not necessarily direct cause

1072
00:50:56,860 --> 00:50:58,380
of some of these existential risks,

1073
00:50:58,380 --> 00:51:00,380
but nonetheless boosts up the probability

1074
00:51:00,380 --> 00:51:03,220
if we're perceiving that an organization

1075
00:51:03,220 --> 00:51:05,700
is very reckless in its attitude.

1076
00:51:05,700 --> 00:51:09,460
This causes more safety minded ones to compete harder

1077
00:51:09,460 --> 00:51:12,260
and justify erasing.

1078
00:51:12,260 --> 00:51:15,740
This reduces the, that consequently reduces

1079
00:51:15,740 --> 00:51:17,980
the amount of time you have to work on control

1080
00:51:17,980 --> 00:51:20,900
and reliability of these AI systems,

1081
00:51:20,900 --> 00:51:25,220
which affects the probability of rogue AI's, of course.

1082
00:51:25,220 --> 00:51:27,540
There's also other types of accidents that could happen

1083
00:51:27,540 --> 00:51:30,300
like the organization might accidentally leak

1084
00:51:31,300 --> 00:51:34,580
one of its models that has some lethal capabilities

1085
00:51:34,580 --> 00:51:36,500
in it if it's repurposed.

1086
00:51:36,500 --> 00:51:40,740
There's also a risk of as potentially,

1087
00:51:40,740 --> 00:51:45,500
who's to say happened with viruses,

1088
00:51:45,500 --> 00:51:48,260
maybe there'd be some unfortunate gain of function research

1089
00:51:48,260 --> 00:51:51,980
that would also lead to some type of catastrophe as well.

1090
00:51:51,980 --> 00:51:55,460
There are people interested in what is essentially

1091
00:51:55,460 --> 00:51:58,220
gain of function research and in creating warning shots,

1092
00:51:58,220 --> 00:52:01,060
they might be a little too successful later on.

1093
00:52:01,060 --> 00:52:04,540
What does gain of function research look like in AI?

1094
00:52:04,540 --> 00:52:06,820
Deliberately building some AI system

1095
00:52:06,820 --> 00:52:08,900
that's like power seeking or Machiavellian

1096
00:52:08,900 --> 00:52:10,340
and wants to destroy humanity.

1097
00:52:10,340 --> 00:52:12,140
And then they're gonna use this to like,

1098
00:52:12,140 --> 00:52:13,300
you know, scare the world with,

1099
00:52:13,300 --> 00:52:17,660
but like at some point when it's powerful enough,

1100
00:52:17,660 --> 00:52:19,180
you might get what you asked for.

1101
00:52:19,180 --> 00:52:22,460
The idea here is to create a dangerous AI,

1102
00:52:22,460 --> 00:52:25,940
maybe an AI that's more agentic or power seeking

1103
00:52:25,940 --> 00:52:30,340
and then use that model to study how to contain it.

1104
00:52:30,340 --> 00:52:34,220
But then the worry is that we could ironically

1105
00:52:34,220 --> 00:52:39,220
go extinct perhaps because we can't control the model.

1106
00:52:41,420 --> 00:52:42,900
Yeah, and if this is like,

1107
00:52:42,900 --> 00:52:46,220
who's to say who's going to be experimenting with this

1108
00:52:46,220 --> 00:52:47,940
or how exactly cautious they will be

1109
00:52:47,940 --> 00:52:49,700
or their like skill level,

1110
00:52:49,700 --> 00:52:53,220
it may be mandated that they test for these types

1111
00:52:53,460 --> 00:52:56,860
of dangerous inclinations or capabilities

1112
00:52:56,860 --> 00:52:59,580
and who exactly is going to be doing that is unclear.

1113
00:52:59,580 --> 00:53:02,860
It may not be like the most like capable people

1114
00:53:04,060 --> 00:53:06,020
or there's just some overall

1115
00:53:06,020 --> 00:53:09,220
or there's just some risk of accidents in that way.

1116
00:53:09,220 --> 00:53:11,220
So I guess that gives some flavor

1117
00:53:11,220 --> 00:53:12,500
of some of the direct accidents,

1118
00:53:12,500 --> 00:53:14,900
but I also think how it indirectly affects things.

1119
00:53:14,900 --> 00:53:17,740
So one way in which I think strongly indirectly

1120
00:53:17,740 --> 00:53:22,580
affects things is when accident is an intellectual error

1121
00:53:22,580 --> 00:53:23,940
inside of these organizations

1122
00:53:23,940 --> 00:53:27,300
where they conflate safety and capabilities.

1123
00:53:27,300 --> 00:53:29,900
This is a very common thing

1124
00:53:29,900 --> 00:53:31,820
where there's not clear thinking about safety

1125
00:53:31,820 --> 00:53:33,900
and capabilities where people be,

1126
00:53:33,900 --> 00:53:36,740
oh, well, we're smart, you know, rational

1127
00:53:36,740 --> 00:53:40,340
and justify the means, we're risk neutral.

1128
00:53:41,220 --> 00:53:44,180
We actually don't actually do much empirical deep learning

1129
00:53:44,180 --> 00:53:46,380
research, but conceptually,

1130
00:53:46,380 --> 00:53:49,060
we think that this will be beneficial for safety

1131
00:53:49,060 --> 00:53:50,780
even though it will come at the cost of capabilities

1132
00:53:50,780 --> 00:53:51,620
and whatnot.

1133
00:53:51,740 --> 00:53:53,540
I'm muddied up that line.

1134
00:53:53,540 --> 00:53:56,820
And the distinction between safety and capabilities

1135
00:53:56,820 --> 00:54:00,580
such that you could imagine a lot of these safety efforts

1136
00:54:00,580 --> 00:54:03,580
basically just working on capabilities the entire time.

1137
00:54:03,580 --> 00:54:06,340
I think that's a reasonable fraction

1138
00:54:06,340 --> 00:54:09,380
of the safety teams I think do focus just on capabilities.

1139
00:54:09,380 --> 00:54:13,980
For context, there is an extreme correlation

1140
00:54:13,980 --> 00:54:16,140
between AI's capabilities

1141
00:54:16,140 --> 00:54:18,380
in various different subjects and goals.

1142
00:54:18,380 --> 00:54:21,180
So if you want your AI system to be better

1143
00:54:21,420 --> 00:54:24,540
at something like math problems

1144
00:54:24,540 --> 00:54:29,180
or history problems or accounting problems,

1145
00:54:29,180 --> 00:54:32,700
these capabilities are all extremely correlated now,

1146
00:54:32,700 --> 00:54:35,420
we can see with like large language models.

1147
00:54:35,420 --> 00:54:38,860
You should assume that if something is correlated,

1148
00:54:38,860 --> 00:54:42,740
like the correlation's like 80% or like 90%,

1149
00:54:42,740 --> 00:54:44,620
it's extremely high.

1150
00:54:45,660 --> 00:54:48,300
So when people reason themselves

1151
00:54:48,340 --> 00:54:51,380
into some new capability

1152
00:54:51,380 --> 00:54:53,780
that they think will be helpful for safety,

1153
00:54:53,780 --> 00:54:55,700
it's very likely the base rate of it

1154
00:54:55,700 --> 00:54:57,940
being correlated with capabilities

1155
00:54:57,940 --> 00:55:00,060
and basically being nearly identical

1156
00:55:00,060 --> 00:55:02,380
to other capabilities by being so correlated

1157
00:55:02,380 --> 00:55:04,180
is extremely high.

1158
00:55:04,180 --> 00:55:06,620
So I think there needs to be substantial evidence

1159
00:55:06,620 --> 00:55:10,380
that the safety intervention that one is applying

1160
00:55:10,380 --> 00:55:12,860
isn't affecting the general capabilities.

1161
00:55:12,860 --> 00:55:15,540
And that requires empirical evidence.

1162
00:55:15,580 --> 00:55:18,580
So a good example of empirical research

1163
00:55:18,580 --> 00:55:20,700
that I think helps with safety,

1164
00:55:20,700 --> 00:55:23,620
but doesn't clearly help with general capabilities

1165
00:55:23,620 --> 00:55:24,820
of making a system smarter

1166
00:55:24,820 --> 00:55:28,900
would be like the area of machine unlearning.

1167
00:55:28,900 --> 00:55:31,460
So machine unlearning is where you're trying to unlearn

1168
00:55:31,460 --> 00:55:33,060
some specific dangerous capabilities,

1169
00:55:33,060 --> 00:55:34,420
trying to unlearn bio knowledge,

1170
00:55:34,420 --> 00:55:36,020
trying to unlearn specific know-how

1171
00:55:36,020 --> 00:55:37,700
that allows you to hack.

1172
00:55:37,700 --> 00:55:41,460
This is more clearly like measurably not correlated with,

1173
00:55:41,460 --> 00:55:42,900
it's inter-correlated with some capabilities

1174
00:55:42,900 --> 00:55:45,020
and not particularly correlated with general capabilities

1175
00:55:45,100 --> 00:55:46,700
just removing that specific know-how.

1176
00:55:46,700 --> 00:55:49,060
I have to say robustness is also generally

1177
00:55:49,060 --> 00:55:51,620
inter-correlated with general capabilities.

1178
00:55:51,620 --> 00:55:54,500
It doesn't make the systems overall smarter.

1179
00:55:54,500 --> 00:55:57,980
What happens is it makes the systems robust

1180
00:55:57,980 --> 00:56:00,100
to some specific types of attacks.

1181
00:56:00,100 --> 00:56:02,180
Robustness to that comes at a fairly large

1182
00:56:02,180 --> 00:56:05,740
computational cost and takes up a lot of the model capacity.

1183
00:56:05,740 --> 00:56:07,140
But that would be a sort of,

1184
00:56:07,140 --> 00:56:09,620
that would be a safety intervention

1185
00:56:09,620 --> 00:56:12,420
that doesn't make the models overall smarter.

1186
00:56:12,420 --> 00:56:14,220
So those are examples of,

1187
00:56:14,220 --> 00:56:15,660
or I suppose another example would be

1188
00:56:15,660 --> 00:56:17,980
with transparency research.

1189
00:56:17,980 --> 00:56:20,420
Historically, there have been no instances

1190
00:56:20,420 --> 00:56:22,340
of transparency advancements

1191
00:56:22,340 --> 00:56:26,300
leading to general capabilities advancements.

1192
00:56:26,300 --> 00:56:28,820
Just trying to understand what's going on in the model

1193
00:56:28,820 --> 00:56:30,700
and it doesn't really work nearly as well

1194
00:56:30,700 --> 00:56:33,300
as just like throwing more data at it.

1195
00:56:33,300 --> 00:56:35,860
And there aren't many architectural improvements

1196
00:56:35,860 --> 00:56:36,980
that are likely to be found.

1197
00:56:36,980 --> 00:56:39,860
Anyway, as a result of these investigations

1198
00:56:39,860 --> 00:56:41,940
is the track record is pretty basically

1199
00:56:41,940 --> 00:56:43,740
completely clean for transparency.

1200
00:56:43,740 --> 00:56:45,740
Now, maybe that wouldn't be the case in the future,

1201
00:56:45,740 --> 00:56:46,580
but then at that point,

1202
00:56:46,580 --> 00:56:48,100
then we wouldn't identify this as something

1203
00:56:48,100 --> 00:56:50,420
that is particularly helping with safety.

1204
00:56:50,420 --> 00:56:53,580
So I think that for the safety research areas,

1205
00:56:53,580 --> 00:56:56,500
we need to be quite clear about there's,

1206
00:56:56,500 --> 00:56:59,460
you can't just have some informal argument about,

1207
00:56:59,460 --> 00:57:00,660
or an appeal to authority that,

1208
00:57:00,660 --> 00:57:05,660
oh, this is helpful for safety because of some verbal argument.

1209
00:57:06,900 --> 00:57:10,100
The empirical machine learning is very complicated.

1210
00:57:10,100 --> 00:57:12,340
Hindsight barely works in trying to understand

1211
00:57:12,340 --> 00:57:13,180
what's going on.

1212
00:57:13,180 --> 00:57:15,460
This pre-training on fractal images

1213
00:57:15,460 --> 00:57:18,500
help improve robustness to, I don't know,

1214
00:57:18,500 --> 00:57:20,140
basically everything and improve the calibration

1215
00:57:20,140 --> 00:57:21,300
and anomaly detection form.

1216
00:57:21,300 --> 00:57:22,860
I have no idea.

1217
00:57:22,860 --> 00:57:25,180
It works though, even ask people like,

1218
00:57:25,180 --> 00:57:28,820
why are activation functions the way they are?

1219
00:57:28,820 --> 00:57:31,380
I don't think there's actually a good canonical explanation

1220
00:57:31,380 --> 00:57:32,540
that's like very consistent.

1221
00:57:32,540 --> 00:57:34,180
You would want empirical evidence

1222
00:57:34,180 --> 00:57:37,340
that when we are engaging in safety research,

1223
00:57:37,340 --> 00:57:40,220
we are not accidentally also increasing

1224
00:57:40,220 --> 00:57:41,500
the capabilities of models.

1225
00:57:41,620 --> 00:57:44,300
And you think this is something that happens often?

1226
00:57:44,300 --> 00:57:46,380
Yeah, I think this happens extremely often

1227
00:57:46,380 --> 00:57:48,580
in this sort of, this organizational risk

1228
00:57:48,580 --> 00:57:50,820
of the conflation of safety and capabilities.

1229
00:57:50,820 --> 00:57:53,180
Now, this isn't to say that they are loose and separate.

1230
00:57:53,180 --> 00:57:55,020
A better improvements in capabilities

1231
00:57:55,020 --> 00:57:57,740
has downstream effects on safety in many situations.

1232
00:57:57,740 --> 00:58:01,340
It makes them better able to understand human values,

1233
00:58:01,340 --> 00:58:04,380
for instance, as they gain more and more common sense.

1234
00:58:04,380 --> 00:58:06,820
But if we are trying to improve safety

1235
00:58:06,820 --> 00:58:09,420
and specifically reduce existential risk,

1236
00:58:09,420 --> 00:58:12,140
I think we need to differentially improve

1237
00:58:12,140 --> 00:58:13,180
on some safety access

1238
00:58:13,180 --> 00:58:15,060
and not in the general capabilities access.

1239
00:58:15,060 --> 00:58:17,420
If we are doing something that's fairly correlated

1240
00:58:17,420 --> 00:58:18,820
with capabilities and safety,

1241
00:58:18,820 --> 00:58:20,940
I think that the default expectation

1242
00:58:20,940 --> 00:58:23,540
is that actually you're working in the service

1243
00:58:23,540 --> 00:58:24,940
of capabilities.

1244
00:58:24,940 --> 00:58:28,220
A good example would be one of OpenAI strategies

1245
00:58:28,220 --> 00:58:30,220
to mention this specifically,

1246
00:58:30,220 --> 00:58:31,820
because I just don't think it's particularly

1247
00:58:31,820 --> 00:58:33,900
intellectually defensible, I'm sorry to say.

1248
00:58:33,900 --> 00:58:37,220
I'm a more disagreeable individual, so here I go.

1249
00:58:37,220 --> 00:58:41,500
I don't think building a super human alignment researcher

1250
00:58:41,500 --> 00:58:43,380
specifically just affects alignment.

1251
00:58:43,380 --> 00:58:46,580
I think such a thing can be easily repurposed

1252
00:58:46,580 --> 00:58:48,860
to doing lots of other types of research.

1253
00:58:48,860 --> 00:58:50,740
I don't think there's like a specific

1254
00:58:50,740 --> 00:58:52,980
alignment research skill set that is just,

1255
00:58:52,980 --> 00:58:54,780
oh, it's just, you only get at that,

1256
00:58:54,780 --> 00:58:56,740
but if you're good at that,

1257
00:58:56,740 --> 00:58:58,220
it means nothing about your ability

1258
00:58:58,220 --> 00:58:59,220
to accomplish anything else.

1259
00:58:59,220 --> 00:59:00,140
I just don't think that's the case.

1260
00:59:00,140 --> 00:59:02,100
I think it's actually extremely correlated

1261
00:59:02,100 --> 00:59:03,260
with general capabilities.

1262
00:59:03,260 --> 00:59:05,500
It would be very straightforwardly repurposed

1263
00:59:05,500 --> 00:59:07,140
to other forms of research.

1264
00:59:07,140 --> 00:59:09,980
But that's an example of this sort of conflation.

1265
00:59:09,980 --> 00:59:13,420
Now, this isn't to say OpenAI is only

1266
00:59:13,420 --> 00:59:15,540
is conflating safety capabilities entirely.

1267
00:59:15,540 --> 00:59:16,380
I'm not claiming that.

1268
00:59:16,380 --> 00:59:20,740
They will have some work on transparency.

1269
00:59:20,740 --> 00:59:23,620
I gather that they'll work more on reliability

1270
00:59:23,620 --> 00:59:28,620
and robustness, but this is a very dangerous conflation.

1271
00:59:29,300 --> 00:59:33,300
And I think basically if they seem kind of correlated,

1272
00:59:33,300 --> 00:59:35,100
just intuitively, and then if you hear a lot

1273
00:59:35,100 --> 00:59:36,900
of verbal arguments without empirical demonstration,

1274
00:59:36,900 --> 00:59:39,140
basically assume just the base rates are like,

1275
00:59:39,140 --> 00:59:41,180
a lot of these completely separate subjects,

1276
00:59:41,180 --> 00:59:43,820
like performance in history and like,

1277
00:59:43,820 --> 00:59:45,820
performance in philosophy and mathematics,

1278
00:59:45,820 --> 00:59:48,340
like those are all like hyper correlated

1279
00:59:48,340 --> 00:59:49,740
to assume this other type of thing

1280
00:59:49,740 --> 00:59:51,560
is also hyper correlated with it too.

1281
00:59:51,560 --> 00:59:53,500
Anyway, though, that's another one that like,

1282
00:59:53,500 --> 00:59:57,060
an organizational factor that really reduces

1283
00:59:57,060 --> 01:00:01,100
the amount of time we have to solve this problem

1284
01:00:01,100 --> 01:00:02,860
and our ability to solve it as well.

1285
01:00:02,860 --> 01:00:03,700
So.

1286
01:00:03,700 --> 01:00:07,860
I think the worry here for, say you're a top AI company

1287
01:00:07,860 --> 01:00:11,300
and you're thinking, okay, how much a safe organization

1288
01:00:11,300 --> 01:00:12,540
features should we implement?

1289
01:00:12,540 --> 01:00:14,220
So should we have more red teaming?

1290
01:00:14,220 --> 01:00:16,460
Should we have more procedures, more review,

1291
01:00:16,460 --> 01:00:19,020
more testing?

1292
01:00:19,020 --> 01:00:21,700
Should we require this empirical evidence

1293
01:00:21,700 --> 01:00:26,620
before we begin a new safety research program?

1294
01:00:26,620 --> 01:00:30,020
This now threatens to slow us down

1295
01:00:30,020 --> 01:00:34,380
and it opens us up to competition

1296
01:00:34,380 --> 01:00:36,780
from the kind of scrappy new startup

1297
01:00:36,780 --> 01:00:41,500
that's on at our heels trying to outcompet us.

1298
01:00:41,500 --> 01:00:44,100
This is very straightforwardly now a case

1299
01:00:44,100 --> 01:00:49,100
of kind of AI race undermining organizational safety

1300
01:00:49,140 --> 01:00:52,500
or at least threatening to undermine organizational safety.

1301
01:00:52,500 --> 01:00:56,340
Can you make an argument if you were to sell this

1302
01:00:56,340 --> 01:01:00,260
to a CEO of an AI corporation that's...

1303
01:01:00,260 --> 01:01:02,580
When would I be in that situation?

1304
01:01:02,580 --> 01:01:07,180
That safety is in the interest of the organization itself.

1305
01:01:07,180 --> 01:01:11,940
You could say it's difficult to sell unsafe products, right?

1306
01:01:11,940 --> 01:01:13,140
You want to be in control.

1307
01:01:13,140 --> 01:01:15,980
You don't want to lose the weights of your model

1308
01:01:15,980 --> 01:01:17,180
in a leak and so on.

1309
01:01:17,180 --> 01:01:19,740
So there might be some correlation

1310
01:01:19,740 --> 01:01:22,660
between the self-interest of the organization

1311
01:01:22,660 --> 01:01:26,580
and the interest that society has in safety in general.

1312
01:01:26,580 --> 01:01:29,460
But before that, one additional factor

1313
01:01:29,460 --> 01:01:31,780
that just diffusely increases probability of extras

1314
01:01:31,780 --> 01:01:33,060
from these organizations,

1315
01:01:33,060 --> 01:01:34,620
if they just do safety washing

1316
01:01:34,620 --> 01:01:36,180
and they don't even know it sometimes,

1317
01:01:36,180 --> 01:01:38,620
they might have some small gesture for safety.

1318
01:01:38,620 --> 01:01:40,580
They might have, for instance,

1319
01:01:40,580 --> 01:01:42,180
a responsible scaling policy

1320
01:01:42,180 --> 01:01:44,780
that doesn't commit them to almost anything

1321
01:01:44,780 --> 01:01:47,980
and then that placates regulators, for instance,

1322
01:01:47,980 --> 01:01:49,580
but doesn't actually reduce risk.

1323
01:01:49,620 --> 01:01:51,980
Those would be other examples

1324
01:01:51,980 --> 01:01:56,180
of how organizational risks can end up

1325
01:01:56,180 --> 01:01:58,300
increasing the probability of existential risk.

1326
01:01:58,300 --> 01:02:02,020
Although it's diffuse and indirect, it still matters.

1327
01:02:02,020 --> 01:02:04,180
On the self-interest point,

1328
01:02:04,180 --> 01:02:07,500
I think a lot of the catastrophic risks

1329
01:02:07,500 --> 01:02:10,860
or catastrophic risks and existential risks are tail risks.

1330
01:02:10,860 --> 01:02:14,100
And generally organizations don't really price

1331
01:02:14,100 --> 01:02:16,660
in tail risks that much.

1332
01:02:16,660 --> 01:02:18,660
A lot of portfolios don't really do much

1333
01:02:18,660 --> 01:02:21,940
to address tail risks either, like in other industries,

1334
01:02:21,940 --> 01:02:24,140
like in finance and whatnot.

1335
01:02:25,140 --> 01:02:27,700
So this is kind of like a problem

1336
01:02:27,700 --> 01:02:30,420
with many of our institutions

1337
01:02:30,420 --> 01:02:34,060
that we could convince them to do things like red teaming,

1338
01:02:34,060 --> 01:02:36,340
to some extent, but doing red teaming

1339
01:02:36,340 --> 01:02:39,140
for existential risks and whatnot

1340
01:02:39,140 --> 01:02:41,300
is not necessarily something that they would check to do

1341
01:02:41,300 --> 01:02:43,540
because that's not going to affect their product tomorrow.

1342
01:02:43,540 --> 01:02:46,820
There's no pushback if everybody is dead,

1343
01:02:46,820 --> 01:02:47,660
as was mentioned before.

1344
01:02:47,660 --> 01:02:49,980
So I think that this works to a limit.

1345
01:02:49,980 --> 01:02:53,020
I think some things like saying information security

1346
01:02:53,020 --> 01:02:56,140
for your company or so that your weights don't leak.

1347
01:02:56,140 --> 01:02:58,620
This is a much easier argument to make.

1348
01:02:58,620 --> 01:03:02,060
Other claims like some of these internal controls

1349
01:03:02,060 --> 01:03:03,940
and whatnot, oh, this will slow us down.

1350
01:03:03,940 --> 01:03:06,420
This will reduce our velocity.

1351
01:03:06,420 --> 01:03:08,580
And I think these are harder to make.

1352
01:03:08,580 --> 01:03:12,340
And I don't think that there are necessarily short-term

1353
01:03:12,340 --> 01:03:14,300
economic incentives for some of these.

1354
01:03:14,300 --> 01:03:17,180
Many of these are actually more for addressing tail risks

1355
01:03:17,180 --> 01:03:20,460
and black swan events.

1356
01:03:20,460 --> 01:03:23,780
So they would then need to just recognize

1357
01:03:23,780 --> 01:03:28,260
that the black swan events are real possibilities

1358
01:03:28,260 --> 01:03:31,940
beyond a probability threshold worth actually addressing.

1359
01:03:31,940 --> 01:03:35,580
So I'm not claiming that they're being completely irrational

1360
01:03:35,580 --> 01:03:39,100
if they're being fairly short-sighted

1361
01:03:39,100 --> 01:03:43,620
and don't believe in these black swan events from it.

1362
01:03:43,620 --> 01:03:45,940
Then I think them trying to maintain velocity

1363
01:03:45,980 --> 01:03:48,260
and just maintain optionality and whatnot,

1364
01:03:48,260 --> 01:03:50,020
it's understandable.

1365
01:03:50,020 --> 01:03:51,540
I wouldn't advocate for that,

1366
01:03:51,540 --> 01:03:53,700
but it's understandable that they're doing that.

1367
01:03:53,700 --> 01:03:58,620
I think they're importing to a lot of their incentives well,

1368
01:03:58,620 --> 01:04:00,500
but, and they will do various things

1369
01:04:00,500 --> 01:04:04,260
to reduce some generic risk.

1370
01:04:04,260 --> 01:04:06,540
They will do some generic forms of red teaming,

1371
01:04:06,540 --> 01:04:08,580
regardless of whether there's regulation,

1372
01:04:08,580 --> 01:04:09,620
because it will make sense.

1373
01:04:09,620 --> 01:04:11,900
But I just don't think that that does particularly much

1374
01:04:11,900 --> 01:04:14,100
in the way of reducing these catastrophic

1375
01:04:14,100 --> 01:04:15,940
existential risks off.

1376
01:04:15,940 --> 01:04:18,780
Say you're a philanthropist or a government

1377
01:04:18,780 --> 01:04:20,420
with a big bag of money

1378
01:04:20,420 --> 01:04:22,820
and you want to incentivize safety research

1379
01:04:22,820 --> 01:04:26,020
at these top AI corporations.

1380
01:04:26,020 --> 01:04:28,500
Is there a way in which you could earmark the money

1381
01:04:28,500 --> 01:04:31,620
and make sure it's spent on what you want it to be spent on?

1382
01:04:31,620 --> 01:04:34,820
So it's not funneled into increasing capabilities

1383
01:04:34,820 --> 01:04:35,740
of the models,

1384
01:04:35,740 --> 01:04:39,460
it's spent on the right type of safety research.

1385
01:04:39,460 --> 01:04:41,500
I think that'd be one intervention.

1386
01:04:41,500 --> 01:04:44,580
I think it's very possible to,

1387
01:04:44,580 --> 01:04:48,300
there are a lot of professors lying around in academe

1388
01:04:48,300 --> 01:04:51,220
who could do this research.

1389
01:04:51,220 --> 01:04:53,660
All you need is to subsidize.

1390
01:04:53,660 --> 01:04:55,100
So for instance, like the Center for Safety

1391
01:04:55,100 --> 01:04:57,900
as a compute cluster, we'd love to expand it.

1392
01:04:57,900 --> 01:05:01,180
We're only able to support not that many professors

1393
01:05:01,180 --> 01:05:04,260
to do research with large language models

1394
01:05:04,260 --> 01:05:06,100
and very compute intensive experiments,

1395
01:05:06,100 --> 01:05:08,180
but there are a lot of professors

1396
01:05:08,180 --> 01:05:11,140
who could be doing more research here.

1397
01:05:11,140 --> 01:05:12,020
So I think that probably,

1398
01:05:12,020 --> 01:05:13,900
there aren't that many people working at these organizations

1399
01:05:13,900 --> 01:05:14,940
I should say as well.

1400
01:05:14,940 --> 01:05:19,940
And so I wouldn't bet on them to fix everything.

1401
01:05:20,380 --> 01:05:22,700
You're actually just correlating it with like,

1402
01:05:22,700 --> 01:05:24,660
what is Jared Copeland's safety vision?

1403
01:05:24,660 --> 01:05:27,100
What is Yann Leica's safety vision?

1404
01:05:27,100 --> 01:05:29,940
And like you're getting like two or three bets

1405
01:05:29,940 --> 01:05:32,780
if you were like giving each of them money.

1406
01:05:32,780 --> 01:05:35,220
And I think that's not a very diversified portfolio

1407
01:05:35,220 --> 01:05:36,300
and you should expect.

1408
01:05:36,300 --> 01:05:40,060
Blind spots just because people don't have,

1409
01:05:40,060 --> 01:05:42,020
can't simulate a collective intelligence,

1410
01:05:42,020 --> 01:05:45,220
a broad research effort by themselves,

1411
01:05:45,220 --> 01:05:47,380
even if they work very hard and have lots of discussions

1412
01:05:47,380 --> 01:05:50,740
and take out, have good deference to outside views

1413
01:05:50,740 --> 01:05:51,580
and so on, they just can't,

1414
01:05:51,580 --> 01:05:53,180
they just can't simulate that function.

1415
01:05:53,180 --> 01:05:55,580
So I would suggest if one's wanting

1416
01:05:55,580 --> 01:05:58,180
to subsidize safety research,

1417
01:05:58,180 --> 01:06:01,540
we can, if there's can have subsidize like a compute cluster,

1418
01:06:01,540 --> 01:06:03,420
then we can have high accountability of like,

1419
01:06:03,420 --> 01:06:04,660
you're not allowed to run this project

1420
01:06:04,660 --> 01:06:07,340
because this doesn't seem sufficiently safety related

1421
01:06:07,340 --> 01:06:08,540
instead of giving like money, you know,

1422
01:06:08,540 --> 01:06:11,100
strings attached to some academics and they run off with it.

1423
01:06:11,100 --> 01:06:14,220
So that would be my preferred intervention.

1424
01:06:14,220 --> 01:06:16,060
And I think that there's,

1425
01:06:16,060 --> 01:06:17,500
it can take orders of magnitude more.

1426
01:06:17,500 --> 01:06:19,220
So like, if any of them are listening,

1427
01:06:19,220 --> 01:06:20,180
you know, like reach out to us,

1428
01:06:20,180 --> 01:06:22,580
I'd love to get more compute to,

1429
01:06:22,580 --> 01:06:25,500
to people doing relevant,

1430
01:06:25,500 --> 01:06:27,900
doing relevant research on safety

1431
01:06:27,900 --> 01:06:29,940
in a nice diversified portfolio.

1432
01:06:29,940 --> 01:06:32,100
Across transparency and adversarial robustness

1433
01:06:32,100 --> 01:06:35,140
and back doors and machine learning models

1434
01:06:35,140 --> 01:06:38,500
and done learning, these types of topics.

1435
01:06:38,500 --> 01:06:42,260
Do you think some safety breakthroughs would be kept secret?

1436
01:06:42,260 --> 01:06:46,260
Say a safety breakthrough at Google DeepMind,

1437
01:06:46,260 --> 01:06:51,260
made the AI useful or in a way that incentivize them

1438
01:06:51,700 --> 01:06:53,220
not to share the safety breakthrough?

1439
01:06:53,220 --> 01:06:55,580
Or would you expect safety breakthroughs

1440
01:06:55,580 --> 01:06:59,740
to be shared widely as if they were found

1441
01:06:59,740 --> 01:07:02,020
in an academic lab?

1442
01:07:02,020 --> 01:07:04,020
I think for market positioning,

1443
01:07:04,020 --> 01:07:07,660
one of them could occupy the niche of being the safest

1444
01:07:07,660 --> 01:07:09,380
of the racing companies.

1445
01:07:09,380 --> 01:07:12,740
We are technically the safest.

1446
01:07:12,740 --> 01:07:16,380
So I think that's currently occupied by Anthropic

1447
01:07:16,380 --> 01:07:18,660
and this might make it fairly useful

1448
01:07:18,660 --> 01:07:22,100
when pitching themselves for say a defense contract

1449
01:07:22,100 --> 01:07:25,500
that look weird to the more reliable organization

1450
01:07:25,500 --> 01:07:27,220
compared to our competitors.

1451
01:07:27,220 --> 01:07:30,020
And so if they would be open sourcing some of that,

1452
01:07:30,020 --> 01:07:33,860
then I think they would lose some of that competitive advantage.

1453
01:07:33,860 --> 01:07:38,780
So it's quite conceivable that they'd hold on to things.

1454
01:07:38,780 --> 01:07:40,940
I mean, there's many safety projects they do

1455
01:07:40,940 --> 01:07:43,540
for which the code is not like open source.

1456
01:07:43,540 --> 01:07:44,660
So we see that to some extent there,

1457
01:07:44,660 --> 01:07:47,740
but I think it can make sense for one of them to try

1458
01:07:47,740 --> 01:07:49,700
and just be a bit safer than the others

1459
01:07:49,700 --> 01:07:51,860
or a bit more reliable than the others.

1460
01:07:51,860 --> 01:07:55,500
Yeah, I've heard Sam Altman, the CEO of OpenAI,

1461
01:07:55,500 --> 01:07:57,460
talk about releasing these systems,

1462
01:07:57,460 --> 01:08:01,660
so specifically releasing GPT-3 and 4 to the world,

1463
01:08:01,660 --> 01:08:07,020
to chat GPT in order to gather more attention to the issue.

1464
01:08:07,020 --> 01:08:10,180
Do you think this is a viable strategy?

1465
01:08:10,180 --> 01:08:14,500
Is this too risky or is it worth trying?

1466
01:08:14,500 --> 01:08:16,860
I think to answer a more extreme question

1467
01:08:16,860 --> 01:08:19,260
to possibly get a sense of my position on this,

1468
01:08:19,260 --> 01:08:22,260
I think the release of Llama 2, for instance,

1469
01:08:22,260 --> 01:08:24,140
by Meta, which is an open source,

1470
01:08:24,140 --> 01:08:27,940
large language model around the capacity of GPT-3.5,

1471
01:08:27,940 --> 01:08:33,220
I think the benefits of that actually outweigh the costs.

1472
01:08:33,220 --> 01:08:35,220
It enables a lot more research.

1473
01:08:35,220 --> 01:08:38,100
It also improves our defenses

1474
01:08:38,100 --> 01:08:40,620
against some of the immediate applications

1475
01:08:40,620 --> 01:08:41,620
of these AI systems.

1476
01:08:41,620 --> 01:08:45,940
So for instance, it came out today that North Korea

1477
01:08:45,940 --> 01:08:48,060
is using some of these AI systems.

1478
01:08:48,060 --> 01:08:50,300
I don't know whether it's Llama 2, that'd be my guess,

1479
01:08:50,300 --> 01:08:53,180
because it's just the most capable open source system

1480
01:08:53,180 --> 01:08:55,340
or code Llama potentially,

1481
01:08:55,340 --> 01:08:58,500
using AI systems to identify vulnerabilities in software,

1482
01:08:58,500 --> 01:09:00,940
and then that helps them shortlist things to attack.

1483
01:09:00,940 --> 01:09:03,820
This isn't an extremely capable,

1484
01:09:03,820 --> 01:09:06,820
or this doesn't rewrite the cost-benefit analysis

1485
01:09:06,820 --> 01:09:07,820
of cyber attacks.

1486
01:09:07,820 --> 01:09:11,380
It doesn't rupture our digital ecosystem,

1487
01:09:11,380 --> 01:09:14,060
but this basically gives us some preview

1488
01:09:14,060 --> 01:09:19,780
and forces these issues on people's attention.

1489
01:09:19,780 --> 01:09:23,180
So I think there's an argument to be made

1490
01:09:23,180 --> 01:09:29,060
for open sourcing Llama 2, or if it's trained on 10x more

1491
01:09:29,060 --> 01:09:31,780
compute Llama 3.

1492
01:09:31,780 --> 01:09:34,580
After that, there's more uncertainty we'd

1493
01:09:34,580 --> 01:09:38,700
have to see, because maybe it could be repurposed for things

1494
01:09:38,700 --> 01:09:41,340
like bio weapons then, or it would be substantially more

1495
01:09:41,340 --> 01:09:44,180
capable at hacking and scamming, things like that.

1496
01:09:44,180 --> 01:09:46,780
I think there's a real argument to be made

1497
01:09:46,780 --> 01:09:50,540
for some short-term stressors snapping the system

1498
01:09:50,540 --> 01:09:54,500
into to do something about it, or at least waking them up.

1499
01:09:54,500 --> 01:09:56,860
But I think systems function better

1500
01:09:56,860 --> 01:09:58,980
with some amount of stressors, when the stressors get too

1501
01:09:58,980 --> 01:10:01,420
extreme, then it can undermine the system.

1502
01:10:01,420 --> 01:10:02,220
So it's complicated.

1503
01:10:02,220 --> 01:10:04,780
I mean, I think maybe the situation, the case of opening,

1504
01:10:04,780 --> 01:10:07,940
I really see these things, or how they want to go about release

1505
01:10:07,940 --> 01:10:11,780
strategies would not surprise me if that should change,

1506
01:10:11,780 --> 01:10:15,140
or if it would be better to do other things in the future.

1507
01:10:15,140 --> 01:10:15,740
Yeah.

1508
01:10:15,740 --> 01:10:18,140
Do you know something about the internal processes

1509
01:10:18,220 --> 01:10:21,060
for deciding when to release these models?

1510
01:10:21,060 --> 01:10:23,660
So in the case of Meta, for instance,

1511
01:10:23,660 --> 01:10:28,300
they may have a chief legal officer vote on whether

1512
01:10:28,300 --> 01:10:30,580
or potentially a veto power that could still

1513
01:10:30,580 --> 01:10:33,860
be overwritten by the CEO, which may have happened,

1514
01:10:33,860 --> 01:10:37,700
in the case of Llama 2, being suggestive here,

1515
01:10:37,700 --> 01:10:39,620
because I have to have a second source for it.

1516
01:10:39,620 --> 01:10:43,780
But usually for decisions, though,

1517
01:10:43,780 --> 01:10:47,620
OpenAI will be accountable to their board.

1518
01:10:47,620 --> 01:10:49,620
I don't know whether they have formal powers

1519
01:10:49,620 --> 01:10:52,060
to decide whether they'd be voting.

1520
01:10:52,060 --> 01:10:56,740
Often boards have blunt powers of just firing the CEO.

1521
01:10:56,740 --> 01:10:59,460
And there often aren't processes in place

1522
01:10:59,460 --> 01:11:03,060
for these larger-scale decisions.

1523
01:11:03,060 --> 01:11:06,340
So you could imagine a CEO just deciding unilaterally

1524
01:11:06,340 --> 01:11:08,900
to have something released.

1525
01:11:08,900 --> 01:11:11,220
And that's something that organizational safety could

1526
01:11:11,220 --> 01:11:13,380
improve and be processes for high-stakes decisions,

1527
01:11:13,380 --> 01:11:14,100
as an example.

1528
01:11:14,100 --> 01:11:14,780
But yeah.

1529
01:11:14,820 --> 01:11:18,260
But by default, boards do not have fine-grain control.

1530
01:11:18,260 --> 01:11:22,220
And so it's often up to the CEO to make the call.

1531
01:11:22,220 --> 01:11:23,900
So you have a single point of failure.

1532
01:11:23,900 --> 01:11:28,460
What is the Swiss cheese model of organizational safety?

1533
01:11:28,460 --> 01:11:31,020
Yeah, so I'm mentioning, and if people

1534
01:11:31,020 --> 01:11:35,180
are wanting to hear more about the organizational safety

1535
01:11:35,180 --> 01:11:39,620
literature, we'll have the AI safety ethics and society

1536
01:11:39,620 --> 01:11:42,020
textbook out in November.

1537
01:11:42,020 --> 01:11:45,340
And one of the chapters would be on safety engineering.

1538
01:11:45,340 --> 01:11:49,420
The Swiss cheese model is easy to communicate.

1539
01:11:49,420 --> 01:11:50,500
It's kind of outdated.

1540
01:11:50,500 --> 01:11:53,340
But it gets at a, just like how people

1541
01:11:53,340 --> 01:11:56,780
are doing analysis of existential risk from AI earlier,

1542
01:11:56,780 --> 01:12:00,700
they'd have a toy model that captured some of the and some

1543
01:12:00,700 --> 01:12:02,180
of the scenarios to be concerned about.

1544
01:12:02,180 --> 01:12:04,380
But it's important not to let that be the lens by which

1545
01:12:04,380 --> 01:12:05,380
you filter everything through.

1546
01:12:05,380 --> 01:12:07,740
That captures some of it, but not all of it.

1547
01:12:07,740 --> 01:12:09,980
And the Swiss cheese one captures some of the dynamics,

1548
01:12:09,980 --> 01:12:10,860
but not all of the dynamics.

1549
01:12:10,860 --> 01:12:13,980
But anyway, the Swiss cheese model with that probably

1550
01:12:13,980 --> 01:12:17,700
would decide essentially having multiple layers of defense.

1551
01:12:17,700 --> 01:12:20,780
If you have red teaming, even red teaming

1552
01:12:20,780 --> 01:12:25,620
for a catastrophic risk, that reduces the risk of catastrophe.

1553
01:12:25,620 --> 01:12:27,740
But it's not itself perfect.

1554
01:12:27,740 --> 01:12:32,140
You might also want stronger informational security, too,

1555
01:12:32,140 --> 01:12:35,780
to make sure that if you had a dangerous model that it

1556
01:12:35,780 --> 01:12:40,580
doesn't leak, you could have better

1557
01:12:40,620 --> 01:12:45,420
transparency tools to check for deceptive behavior in AI systems.

1558
01:12:45,420 --> 01:12:47,180
But if those transparency tools failed,

1559
01:12:47,180 --> 01:12:49,980
maybe you would want monitoring of these AI systems

1560
01:12:49,980 --> 01:12:52,500
so that before they take any action,

1561
01:12:52,500 --> 01:12:55,820
it needs to be approved by something equivalent

1562
01:12:55,820 --> 01:12:58,380
to an artificial conscience or filter

1563
01:12:58,380 --> 01:13:01,420
that would filter out some of the immoral actions of AI agents

1564
01:13:01,420 --> 01:13:02,900
before they're able to take them.

1565
01:13:02,900 --> 01:13:05,340
And so all of these together can increase

1566
01:13:05,340 --> 01:13:07,700
the reliability of the system.

1567
01:13:07,700 --> 01:13:11,420
So the hope is that if you stack together

1568
01:13:11,420 --> 01:13:14,660
many of these, you've substantially reduced your risk.

1569
01:13:14,660 --> 01:13:17,460
This isn't looking for a perfect airtight solution.

1570
01:13:17,460 --> 01:13:21,020
This is looking for layering on many different defenses

1571
01:13:21,020 --> 01:13:21,860
to actually reduce risk.

1572
01:13:21,860 --> 01:13:23,820
So if I wanted to reduce by a risk, for instance,

1573
01:13:23,820 --> 01:13:27,300
here's an example of a Swiss cheese thing.

1574
01:13:27,300 --> 01:13:30,980
First, there'd be the diffuse thing.

1575
01:13:30,980 --> 01:13:33,540
And maybe there could be some regulation about not allowing

1576
01:13:33,540 --> 01:13:34,540
models with these capabilities.

1577
01:13:34,540 --> 01:13:36,060
But let's say I'm an organization that

1578
01:13:36,060 --> 01:13:37,140
takes safety more seriously.

1579
01:13:37,140 --> 01:13:38,740
So that depends on safety culture.

1580
01:13:38,740 --> 01:13:40,700
So that's some sort of barrier.

1581
01:13:40,700 --> 01:13:42,700
Regulation might be some barrier against this risk.

1582
01:13:42,700 --> 01:13:44,780
Safety culture might be some barrier against this risk.

1583
01:13:44,780 --> 01:13:46,620
So then they have enough of a safety culture

1584
01:13:46,620 --> 01:13:48,780
they're willing to add a lot of these safety features.

1585
01:13:48,780 --> 01:13:50,220
Now, these safety features themselves

1586
01:13:50,220 --> 01:13:53,380
will end up having lots of different layers of defense.

1587
01:13:53,380 --> 01:13:57,140
You could have an input filter to try and remove

1588
01:13:57,140 --> 01:13:59,340
whether there's a request to create a bio weapon.

1589
01:13:59,340 --> 01:14:02,260
You could also remove virology related data

1590
01:14:02,260 --> 01:14:03,540
from the pre-training distribution

1591
01:14:03,540 --> 01:14:06,460
so that it likely knows a lot less about virology.

1592
01:14:06,460 --> 01:14:09,780
You could have an output filter as well,

1593
01:14:09,780 --> 01:14:12,180
which would, even if somebody jail breaks the input filter,

1594
01:14:12,180 --> 01:14:13,780
then they're also going to need a jail break

1595
01:14:13,780 --> 01:14:16,180
the output filter, which is harder to do.

1596
01:14:16,180 --> 01:14:19,220
And you could imagine adversarily training this as well.

1597
01:14:19,220 --> 01:14:21,780
So it'd be another layer so that it would be more robust

1598
01:14:21,780 --> 01:14:25,100
to people trying to jail break those layers of defense.

1599
01:14:25,100 --> 01:14:28,420
But then you also have, there's also people

1600
01:14:28,420 --> 01:14:30,380
who could, through the API, fine tune the model

1601
01:14:30,380 --> 01:14:33,700
and inject some of that bio knowledge back into the model.

1602
01:14:33,700 --> 01:14:38,700
So you could have a filter that screens the fine tuning data

1603
01:14:39,060 --> 01:14:42,180
so that that information can't get back into the weights.

1604
01:14:42,180 --> 01:14:43,900
And then you could add another layer,

1605
01:14:43,900 --> 01:14:45,260
which would be an unlearning layer

1606
01:14:45,260 --> 01:14:48,220
where you would assume that before you hand back

1607
01:14:48,220 --> 01:14:50,180
the fine tuned model to the user,

1608
01:14:50,180 --> 01:14:52,260
before they get it back, we're going to run a scrubbing,

1609
01:14:52,260 --> 01:14:54,460
unlearning, knowledge expunging thing

1610
01:14:54,460 --> 01:14:57,580
to expunge some of any bio knowledge if there is any.

1611
01:14:57,580 --> 01:14:59,220
And that would be yet another layer.

1612
01:14:59,220 --> 01:15:03,180
This approach reduces the risk of some bio catastrophe.

1613
01:15:03,820 --> 01:15:05,220
Are any of those airtight?

1614
01:15:05,220 --> 01:15:06,340
No.

1615
01:15:06,340 --> 01:15:07,580
But do they work better collectively?

1616
01:15:07,580 --> 01:15:08,420
Absolutely.

1617
01:15:08,420 --> 01:15:11,300
So this is why we shouldn't be focusing

1618
01:15:11,300 --> 01:15:13,220
on these airtight solutions.

1619
01:15:13,220 --> 01:15:15,540
Exclusively, we also need to make use

1620
01:15:15,540 --> 01:15:18,300
of these various layers of defense.

1621
01:15:18,300 --> 01:15:20,540
That's how we actually reduce the probability

1622
01:15:20,540 --> 01:15:22,620
of existential risk.

1623
01:15:22,620 --> 01:15:24,900
We can't let perfection be the enemy of the good.

1624
01:15:24,900 --> 01:15:28,020
If we'd say, well, if we can't build a completely

1625
01:15:28,020 --> 01:15:29,740
100% reliable input filter,

1626
01:15:29,740 --> 01:15:31,380
then we shouldn't have an input filter.

1627
01:15:31,380 --> 01:15:33,020
That's a dead end, so we shouldn't investigate it.

1628
01:15:33,020 --> 01:15:35,620
That's just not how things work.

1629
01:15:35,620 --> 01:15:37,180
Tell us more about the textbook.

1630
01:15:37,180 --> 01:15:39,100
I'm pretty excited to read this.

1631
01:15:39,100 --> 01:15:42,780
I hope that this is a product that should exist, I think.

1632
01:15:42,780 --> 01:15:45,540
Specifically, tell us more about how do you think

1633
01:15:45,540 --> 01:15:47,580
about updating this or keeping it up to date?

1634
01:15:47,580 --> 01:15:50,180
I think for a textbook on AI safety,

1635
01:15:50,180 --> 01:15:53,500
it won't probably work if the next version is out

1636
01:15:53,500 --> 01:15:57,740
in 2034 or something like that, right?

1637
01:15:57,740 --> 01:15:59,500
So how do you keep it up to date?

1638
01:15:59,500 --> 01:16:02,180
And also, you can just present the textbook,

1639
01:16:02,260 --> 01:16:04,340
which I think listeners will be interested in.

1640
01:16:04,340 --> 01:16:07,620
I mean, since I've been around in academia for a while,

1641
01:16:07,620 --> 01:16:10,900
I do have at least some of a sense of what things,

1642
01:16:10,900 --> 01:16:13,700
what content is more likely to stand the test of time.

1643
01:16:13,700 --> 01:16:17,300
So that one's not talking about Dolly 2 or something,

1644
01:16:17,300 --> 01:16:18,900
which is already outdated,

1645
01:16:20,100 --> 01:16:21,980
or what are kind of like fad topics

1646
01:16:21,980 --> 01:16:24,980
and not giving those too much,

1647
01:16:24,980 --> 01:16:26,780
not giving those airtime.

1648
01:16:26,780 --> 01:16:29,060
I mean, an example of this would be an unsolved problems

1649
01:16:29,060 --> 01:16:31,420
that I'll say two, three years ago or something,

1650
01:16:31,420 --> 01:16:35,660
but there we introduced emergent capabilities,

1651
01:16:35,660 --> 01:16:38,500
which I think has become fairly popular

1652
01:16:38,500 --> 01:16:42,260
before Burns et al's paper on honesty and whatnot.

1653
01:16:42,260 --> 01:16:44,620
We're also, honesty is a big part of alignment.

1654
01:16:44,620 --> 01:16:46,980
So there's sometimes one needs to call the shots too

1655
01:16:46,980 --> 01:16:48,380
as to what things will,

1656
01:16:48,380 --> 01:16:52,380
even if there aren't, isn't much of a literature on it at all,

1657
01:16:52,380 --> 01:16:55,660
need to predict what will end up standing the test of time.

1658
01:16:55,660 --> 01:16:58,300
But so I think it should have some reasonable longevity

1659
01:16:58,300 --> 01:17:00,460
because we're not focusing on transient knowledge,

1660
01:17:00,460 --> 01:17:04,860
but instead like general interdisciplinary frameworks

1661
01:17:04,860 --> 01:17:06,980
we're thinking about risk across all these sectors.

1662
01:17:06,980 --> 01:17:09,220
Cause we had this issue of like, there's,

1663
01:17:09,220 --> 01:17:10,340
if you're thinking about AI risk,

1664
01:17:10,340 --> 01:17:12,660
you have to think a bit about geopolitics.

1665
01:17:12,660 --> 01:17:14,220
You have to think about international relations

1666
01:17:14,220 --> 01:17:15,060
to some extent.

1667
01:17:15,060 --> 01:17:15,900
You think about AI risk,

1668
01:17:15,900 --> 01:17:17,100
you have to think about corporate governance

1669
01:17:17,100 --> 01:17:19,600
and AI developers and what sort of incentives

1670
01:17:19,600 --> 01:17:21,060
are driving them.

1671
01:17:21,060 --> 01:17:24,180
You have to think about the individual AI systems themselves

1672
01:17:24,180 --> 01:17:26,820
too, you have to think about organizational safety.

1673
01:17:26,820 --> 01:17:31,820
You have to think about broad variety of factors

1674
01:17:32,220 --> 01:17:34,900
and we'll basically focus quite a bit on frameworks

1675
01:17:34,900 --> 01:17:38,900
for thinking clearly about each of those.

1676
01:17:38,900 --> 01:17:41,340
I would imagine that later one could have, you know,

1677
01:17:41,340 --> 01:17:45,180
GPT-6 like help like update the textbook anyway.

1678
01:17:45,180 --> 01:17:49,460
So honestly, it's actually like the plan first.

1679
01:17:49,460 --> 01:17:51,500
Something in that direction.

1680
01:17:51,500 --> 01:17:52,900
How technical is the book?

1681
01:17:52,900 --> 01:17:56,940
Does it contain pseudocode like a standard AI textbook?

1682
01:17:56,940 --> 01:18:00,660
The premise of it is to onboard people

1683
01:18:00,660 --> 01:18:01,940
from different disciplines.

1684
01:18:01,940 --> 01:18:04,340
This isn't written for machine learning PhD people.

1685
01:18:04,340 --> 01:18:05,700
There are lots of different fields,

1686
01:18:05,700 --> 01:18:07,900
economists, legal scholars, philosophers,

1687
01:18:07,900 --> 01:18:10,780
people without technical background, policy makers,

1688
01:18:10,780 --> 01:18:15,780
think tank people who want more of a systematic understanding

1689
01:18:15,780 --> 01:18:17,020
of these issues.

1690
01:18:17,020 --> 01:18:19,980
And so it's largely written for people

1691
01:18:19,980 --> 01:18:22,220
without any specific background

1692
01:18:22,340 --> 01:18:24,980
and it's not trying to be a sort of like

1693
01:18:24,980 --> 01:18:27,740
a introductory machine learning PhD course.

1694
01:18:27,740 --> 01:18:30,580
That would be the course.mlsafety.org

1695
01:18:30,580 --> 01:18:34,060
if you want a course of various technical topics

1696
01:18:34,060 --> 01:18:35,900
or the machine learning safety course.

1697
01:18:35,900 --> 01:18:40,020
But this one is more focusing on, you know,

1698
01:18:40,020 --> 01:18:42,420
as we were discussing the game theory of this,

1699
01:18:42,420 --> 01:18:45,140
the various governance solutions.

1700
01:18:45,140 --> 01:18:49,780
Conceptually, many of the arguments associated

1701
01:18:49,780 --> 01:18:51,860
with rogue AIs, why might they be power systems?

1702
01:18:51,980 --> 01:18:53,620
Maybe they're deceptive.

1703
01:18:53,620 --> 01:18:54,780
Understanding that.

1704
01:18:54,780 --> 01:18:56,540
There's also introduction to machine learning

1705
01:18:56,540 --> 01:18:58,180
and reinforcement learning in it.

1706
01:18:58,180 --> 01:19:00,780
Understanding collective action problems

1707
01:19:00,780 --> 01:19:02,180
since that was fairly relevant

1708
01:19:02,180 --> 01:19:05,980
and these competitive pressures.

1709
01:19:05,980 --> 01:19:08,620
There's also ethics in the book as well

1710
01:19:08,620 --> 01:19:09,780
where if you're assuming

1711
01:19:09,780 --> 01:19:12,180
that you've got your AIs systems to be somewhat reliable,

1712
01:19:12,180 --> 01:19:13,820
then we have to start worrying about

1713
01:19:13,820 --> 01:19:14,900
making it beneficial.

1714
01:19:14,900 --> 01:19:19,900
And so there's various bits of AI ethics

1715
01:19:20,900 --> 01:19:24,620
as well of what are objectives

1716
01:19:24,620 --> 01:19:26,460
that we might give the AI system.

1717
01:19:26,460 --> 01:19:27,580
What would those look like?

1718
01:19:27,580 --> 01:19:28,940
What would be some of the, you know,

1719
01:19:28,940 --> 01:19:30,300
moral trade offs that you're making there?

1720
01:19:30,300 --> 01:19:35,300
But so it's covering AI safety, ethics, and society.

1721
01:19:35,540 --> 01:19:39,220
So trying to be fairly broad.

1722
01:19:39,220 --> 01:19:41,780
You should have lecture slides.

1723
01:19:41,780 --> 01:19:45,460
And presumably I'll get around to recording videos for two.

1724
01:19:45,460 --> 01:19:46,860
The goals, there's several goals of it,

1725
01:19:46,860 --> 01:19:48,260
like to compress the content.

1726
01:19:48,260 --> 01:19:49,900
Right now, if you want to understand AI risk,

1727
01:19:49,900 --> 01:19:52,220
basically need to be part of an intellectual scene,

1728
01:19:52,220 --> 01:19:53,740
like in the Bay Area probably.

1729
01:19:53,740 --> 01:19:55,980
Maybe and maybe somewhat an Oxford.

1730
01:19:55,980 --> 01:19:57,900
So very high barriers to entry.

1731
01:19:57,900 --> 01:19:59,380
And then if you do,

1732
01:19:59,380 --> 01:20:01,580
you're probably going to take a somewhat narrow view

1733
01:20:01,580 --> 01:20:05,580
just because they're all interested in rogue AIs

1734
01:20:05,580 --> 01:20:08,420
and don't have as much interaction

1735
01:20:08,420 --> 01:20:10,420
with the rest of the world.

1736
01:20:10,420 --> 01:20:15,420
So you'll have many blind spots as to a lot of it.

1737
01:20:15,420 --> 01:20:16,540
There's the social variables

1738
01:20:16,540 --> 01:20:18,420
and the broader socio-technical problem.

1739
01:20:18,420 --> 01:20:20,780
The knowledge has been a bit diffused

1740
01:20:20,780 --> 01:20:22,940
across various different blocks.

1741
01:20:22,940 --> 01:20:25,180
And to stay up to date,

1742
01:20:25,180 --> 01:20:27,660
you've often had to jump around from different places.

1743
01:20:27,660 --> 01:20:29,340
So it would be nice to have something

1744
01:20:29,340 --> 01:20:31,380
that's more compressed.

1745
01:20:31,380 --> 01:20:34,580
So some of the goals are to reduce the fragmentation

1746
01:20:34,580 --> 01:20:37,540
of AI risk knowledge, increase the readability,

1747
01:20:37,540 --> 01:20:40,740
and the sort of compression rate of this content.

1748
01:20:40,740 --> 01:20:43,020
And so there's reducing the barrier to entry

1749
01:20:43,020 --> 01:20:45,020
to these crucial ideas

1750
01:20:45,020 --> 01:20:47,540
that should hopefully scale the number of people

1751
01:20:47,540 --> 01:20:50,180
who can understand AI risk extremely quickly.

1752
01:20:50,180 --> 01:20:52,500
I was somewhat surprised by,

1753
01:20:52,500 --> 01:20:54,660
although there's a lot of global attention,

1754
01:20:54,660 --> 01:20:56,500
the number of new experts flooding in

1755
01:20:56,500 --> 01:20:59,180
has been, I think, very underwhelming.

1756
01:20:59,180 --> 01:21:00,420
Is that good or bad?

1757
01:21:00,420 --> 01:21:03,860
Sometimes it's a bad thing if experts are rushing

1758
01:21:03,860 --> 01:21:07,860
into the new, newly hot idea.

1759
01:21:07,860 --> 01:21:11,220
I think that if people are onboarded well

1760
01:21:11,220 --> 01:21:14,740
and have a more comprehensive understanding,

1761
01:21:14,780 --> 01:21:16,900
if they're basically like charlatans

1762
01:21:16,900 --> 01:21:18,580
who aren't going to do their work,

1763
01:21:18,580 --> 01:21:19,860
then that's more of a problem.

1764
01:21:19,860 --> 01:21:21,980
So I think by default,

1765
01:21:21,980 --> 01:21:26,220
with another capabilities jump or two, they will flood in.

1766
01:21:26,220 --> 01:21:27,900
There's basically a question,

1767
01:21:27,900 --> 01:21:30,700
and I don't anticipate they're going to read lots of

1768
01:21:30,700 --> 01:21:34,540
lesswrong.com posts to be onboarded.

1769
01:21:34,540 --> 01:21:35,820
They're just gonna start talking

1770
01:21:35,820 --> 01:21:38,100
and trying to be about themselves.

1771
01:21:38,100 --> 01:21:43,100
I say this in my time, empirical machine learning research,

1772
01:21:43,540 --> 01:21:45,220
it basically should assume

1773
01:21:45,220 --> 01:21:48,420
that when some area starts getting pretty hot,

1774
01:21:48,420 --> 01:21:51,700
there'll be lots of random new people coming in

1775
01:21:51,700 --> 01:21:56,540
and trying to influence the discussion substantially.

1776
01:21:56,540 --> 01:21:58,220
Hopefully the people as they come in

1777
01:21:58,220 --> 01:22:01,900
would have some understanding of many of the basics, though,

1778
01:22:01,900 --> 01:22:04,820
but I think by default, it's relatively inaccessible.

1779
01:22:04,820 --> 01:22:07,220
You'll have to read a lot of scattered content

1780
01:22:08,340 --> 01:22:09,340
from different places,

1781
01:22:09,340 --> 01:22:10,740
and a lot of it will be idiosyncratic,

1782
01:22:10,740 --> 01:22:12,580
and it'll just take a long time to go through.

1783
01:22:12,580 --> 01:22:16,100
Those are some of the reasons for doing this.

1784
01:22:16,100 --> 01:22:20,420
And then also, I think that given that rogue AIs

1785
01:22:20,420 --> 01:22:25,020
is not the only concern or only risk source,

1786
01:22:25,020 --> 01:22:27,340
there's a lot of content that even a lot of people

1787
01:22:27,340 --> 01:22:29,020
who've been thinking about AIs risk for a while

1788
01:22:29,020 --> 01:22:32,440
will possibly need to become aware of.

1789
01:22:33,580 --> 01:22:37,340
So that's why, so just as a grad student,

1790
01:22:37,340 --> 01:22:41,260
when I just developed and just focused

1791
01:22:41,820 --> 01:22:44,460
on these other things other than rogue AIs,

1792
01:22:45,380 --> 01:22:47,260
and then now I think people are recognizing

1793
01:22:47,260 --> 01:22:48,300
the importance of that,

1794
01:22:48,300 --> 01:22:49,500
so now there'll hopefully be,

1795
01:22:49,500 --> 01:22:51,740
or so now there'll be some material

1796
01:22:51,740 --> 01:22:54,580
to help get a more formal understanding

1797
01:22:54,580 --> 01:22:57,060
of these other sorts of issues.

1798
01:22:57,060 --> 01:22:58,940
That's great, I'm looking forward to reading it.

1799
01:22:58,940 --> 01:23:01,700
I think we should nonetheless talk about rogue AIs.

1800
01:23:01,700 --> 01:23:04,620
That's your last category of risk.

1801
01:23:04,620 --> 01:23:07,580
One issue here is proxy gaming.

1802
01:23:07,580 --> 01:23:09,100
How does that work?

1803
01:23:09,100 --> 01:23:10,540
How is it dangerous?

1804
01:23:10,540 --> 01:23:12,060
Yeah, so you could imagine

1805
01:23:12,060 --> 01:23:14,100
if you've got a very powerful AI system,

1806
01:23:14,100 --> 01:23:18,420
if it finds reliability holes in the objective

1807
01:23:18,420 --> 01:23:21,540
that it's given, then this could be destructive

1808
01:23:21,540 --> 01:23:24,220
because it's being guided by a flawed objective.

1809
01:23:24,220 --> 01:23:29,220
I think in a colloquial example is with believe in Hanoi,

1810
01:23:31,220 --> 01:23:35,500
there'd be a bounty for killing rats.

1811
01:23:35,500 --> 01:23:37,860
And so if you get the rats, you get a bounty,

1812
01:23:37,860 --> 01:23:41,340
but then people were incentivized to breed rats

1813
01:23:42,380 --> 01:23:43,620
so as to collect more of that bounty.

1814
01:23:43,620 --> 01:23:45,420
That would be an example of an objective

1815
01:23:45,420 --> 01:23:48,260
that you put forward that ends up getting gained.

1816
01:23:48,260 --> 01:23:52,060
It's fairly difficult to encode all of your values

1817
01:23:52,060 --> 01:23:55,700
like well-being and whatnot into a specific objective,

1818
01:23:55,700 --> 01:23:58,620
a simple objective.

1819
01:23:58,620 --> 01:24:00,940
So you might expect some approximation

1820
01:24:00,940 --> 01:24:02,740
or to what you actually care about.

1821
01:24:02,740 --> 01:24:05,220
And in machine learning, a famous example,

1822
01:24:05,260 --> 01:24:08,620
this is the boat racing or coast runners example

1823
01:24:08,620 --> 01:24:12,340
that OpenAI had, which was of proxy gaming,

1824
01:24:12,340 --> 01:24:14,980
of there's a reward function

1825
01:24:14,980 --> 01:24:17,100
and the reinforcement learning agent

1826
01:24:17,100 --> 01:24:18,660
would optimize that reward function.

1827
01:24:18,660 --> 01:24:19,740
It was, this was a racing game.

1828
01:24:19,740 --> 01:24:21,860
You'd think it would optimize the reward function

1829
01:24:21,860 --> 01:24:23,100
by going around the track,

1830
01:24:23,100 --> 01:24:24,420
but what it instead learned to do

1831
01:24:24,420 --> 01:24:25,540
was it can get a higher reward

1832
01:24:25,540 --> 01:24:27,220
by getting lots of turbo boosts.

1833
01:24:27,220 --> 01:24:32,220
And the turbo boosts, it could get a very rapid sequence

1834
01:24:32,500 --> 01:24:34,580
of them by crashing into walls

1835
01:24:34,580 --> 01:24:35,540
and catching on fire

1836
01:24:35,540 --> 01:24:38,380
and then continually turbo boosting in that way.

1837
01:24:38,380 --> 01:24:40,220
And that would help it get a higher score.

1838
01:24:40,220 --> 01:24:42,900
So there are often holes in these objectives

1839
01:24:44,260 --> 01:24:48,660
due to an ability to compute exactly the right objective

1840
01:24:48,660 --> 01:24:53,380
or maybe we can only monitor some parts of the system.

1841
01:24:53,380 --> 01:24:56,060
There's a computational and spatial

1842
01:24:56,060 --> 01:24:58,420
and temporal constraints on the quality of the objective,

1843
01:24:58,420 --> 01:24:59,980
meaning that you're gonna often have to go

1844
01:24:59,980 --> 01:25:00,860
with an approximation.

1845
01:25:00,860 --> 01:25:02,860
So it's something perfectly ideal.

1846
01:25:02,860 --> 01:25:05,060
This relates to Goodhart's law,

1847
01:25:05,060 --> 01:25:07,140
which works in human domains also

1848
01:25:07,140 --> 01:25:11,220
in which it's difficult to specify exactly what it is you want.

1849
01:25:11,220 --> 01:25:14,820
And whenever you specify something you want,

1850
01:25:14,820 --> 01:25:19,300
that thing you've specified is now open to being game.

1851
01:25:19,300 --> 01:25:21,340
So an example here might be

1852
01:25:21,340 --> 01:25:24,180
that you want deep scientific insight

1853
01:25:24,180 --> 01:25:27,660
and you assume that such insight correlates

1854
01:25:27,660 --> 01:25:30,500
with citations or number of citations.

1855
01:25:30,540 --> 01:25:34,780
But then you get gaming of the citation systems

1856
01:25:34,780 --> 01:25:37,300
in which academics are incentivized

1857
01:25:37,300 --> 01:25:41,300
to maximize citations at the cost of scientific insight.

1858
01:25:41,300 --> 01:25:44,140
So is this a more general problem

1859
01:25:44,140 --> 01:25:48,500
across all agents, humans included?

1860
01:25:48,500 --> 01:25:50,820
Yeah, yeah, I don't think this is specific to,

1861
01:25:50,820 --> 01:25:53,740
I don't think this is specific to AI agents.

1862
01:25:53,740 --> 01:25:56,980
I will say that some objectives are harder to game than others.

1863
01:25:56,980 --> 01:25:59,700
For instance, the bounty on rat tails

1864
01:25:59,700 --> 01:26:02,140
is a lot easier to game than like citations

1865
01:26:02,140 --> 01:26:04,980
because citations can be very valuable for getting,

1866
01:26:04,980 --> 01:26:07,060
you know, emigrate if you're getting a green card,

1867
01:26:07,060 --> 01:26:09,940
for instance, and it's a strong incentive to do it.

1868
01:26:09,940 --> 01:26:14,940
And, but it's nonetheless challenging.

1869
01:26:15,220 --> 01:26:16,700
So some of these objectives,

1870
01:26:16,700 --> 01:26:18,740
even when people are trying very hard to game,

1871
01:26:18,740 --> 01:26:21,300
they still can be correlated with a lot,

1872
01:26:21,300 --> 01:26:23,860
like college admissions still focuses,

1873
01:26:23,860 --> 01:26:25,420
incentivize people to be productive.

1874
01:26:25,420 --> 01:26:28,660
Yes, they'll go overboard in studying for the exams

1875
01:26:28,660 --> 01:26:30,820
and whatnot, the college admissions tests,

1876
01:26:30,820 --> 01:26:33,300
yes, they'll go overboard in the number of extracurriculars

1877
01:26:33,300 --> 01:26:35,260
and whatnot, but I still think it like,

1878
01:26:35,260 --> 01:26:37,300
it doesn't can help shape compared to

1879
01:26:37,300 --> 01:26:38,980
they're not being the incentive in the first place.

1880
01:26:38,980 --> 01:26:41,740
I think overall my take on the GoodHeads Law

1881
01:26:41,740 --> 01:26:44,140
is that there's some objectives that are,

1882
01:26:44,140 --> 01:26:48,420
or some goals are all goals and proxies are wrong.

1883
01:26:48,420 --> 01:26:50,900
Some are useful and some though, when gained

1884
01:26:50,900 --> 01:26:53,740
in particular ways could be potentially catastrophic.

1885
01:26:53,740 --> 01:26:55,860
So there's quite a variety.

1886
01:26:55,860 --> 01:26:57,620
There are some objectives as well

1887
01:26:57,620 --> 01:27:01,300
that people would claim would produce good outcomes.

1888
01:27:01,300 --> 01:27:03,340
For instance, if you gave an AI an objective,

1889
01:27:03,340 --> 01:27:06,540
like make the world the best place it can.

1890
01:27:06,540 --> 01:27:09,620
And if that was actually the objective you gave it,

1891
01:27:09,620 --> 01:27:11,860
okay, that's quite different from like,

1892
01:27:11,860 --> 01:27:16,860
make people very engaged with this product.

1893
01:27:18,740 --> 01:27:20,300
That's quite different.

1894
01:27:20,300 --> 01:27:25,300
I think that making these proxies incorporate more

1895
01:27:25,660 --> 01:27:28,860
of our values becomes more possible across time

1896
01:27:28,860 --> 01:27:31,540
because the systems can represent these other sorts

1897
01:27:31,540 --> 01:27:35,460
of notions of say, wellbeing of autonomy

1898
01:27:35,460 --> 01:27:39,100
because they have a lot better of a world model

1899
01:27:39,100 --> 01:27:42,540
and more of an understanding of people as well.

1900
01:27:42,540 --> 01:27:46,180
However, so I think that getting objectives

1901
01:27:46,180 --> 01:27:49,140
that are in the right direction seem possible.

1902
01:27:49,140 --> 01:27:53,940
The issue is making them be robust to adversarial pressure.

1903
01:27:53,940 --> 01:27:56,060
I'm not as concerned about like,

1904
01:27:56,060 --> 01:27:57,620
we get telling AI go cure cancer

1905
01:27:57,620 --> 01:27:58,740
and then it does something like,

1906
01:27:58,740 --> 01:28:01,580
oh, I'll give lots of people cancer to experiment on them

1907
01:28:01,580 --> 01:28:04,100
to speed up the experimentation process.

1908
01:28:04,100 --> 01:28:06,580
This is easily ruled out by some like objective

1909
01:28:06,580 --> 01:28:07,940
with like an interpret the request

1910
01:28:07,940 --> 01:28:10,340
as a reasonable person would.

1911
01:28:10,340 --> 01:28:13,380
This is a fairly new development in AI

1912
01:28:13,380 --> 01:28:16,220
that we now have these large language models

1913
01:28:16,220 --> 01:28:19,860
that can at least to some extent understand common sense

1914
01:28:19,860 --> 01:28:24,060
and have kind of a more subtle understanding

1915
01:28:24,060 --> 01:28:25,940
of human values.

1916
01:28:25,940 --> 01:28:30,940
Yeah, earlier there'd be the AI's they would be kind of

1917
01:28:30,940 --> 01:28:33,100
like savants where they understand

1918
01:28:33,100 --> 01:28:35,380
some particular thing well, but then nothing else.

1919
01:28:35,380 --> 01:28:38,740
And, you know, human values are so late

1920
01:28:38,740 --> 01:28:42,020
in the evolutionary process and suggested they're very late

1921
01:28:42,020 --> 01:28:45,180
to be one of the last things that AI's learn.

1922
01:28:45,180 --> 01:28:47,460
But that fortunately wasn't the case.

1923
01:28:47,460 --> 01:28:51,340
We explored this a few years ago in the paper

1924
01:28:51,340 --> 01:28:52,740
with the ethics data set.

1925
01:28:52,740 --> 01:28:55,180
We're basically using that to show that look,

1926
01:28:55,180 --> 01:28:56,020
they've got understanding

1927
01:28:56,020 --> 01:28:59,260
of various morally salient considerations.

1928
01:28:59,260 --> 01:29:00,540
Here's their predictive performance

1929
01:29:00,540 --> 01:29:01,620
on like well-being things.

1930
01:29:01,620 --> 01:29:05,020
Here's their understanding of deontological rules

1931
01:29:05,020 --> 01:29:07,820
and notions in justice and fairness,

1932
01:29:07,820 --> 01:29:10,820
such as whether people get what they deserve

1933
01:29:10,820 --> 01:29:13,300
or whether people are being impartial.

1934
01:29:13,300 --> 01:29:15,980
So they have an understanding of a lot

1935
01:29:16,980 --> 01:29:19,420
of morally salient considerations.

1936
01:29:19,420 --> 01:29:21,380
There is a question of reliability though,

1937
01:29:21,380 --> 01:29:23,300
if they're optimizing that objective,

1938
01:29:23,300 --> 01:29:26,180
are they basically, is that objective

1939
01:29:26,180 --> 01:29:29,500
succumbing to that adversarial optimization pressure?

1940
01:29:29,500 --> 01:29:32,220
If it's optimizing it, it's basically functionally similar

1941
01:29:32,220 --> 01:29:34,860
to it being adversarial to that objective.

1942
01:29:34,860 --> 01:29:38,420
This is why there's a focus on adversarial robustness

1943
01:29:38,420 --> 01:29:42,380
because later we would have, we've got an AI agent

1944
01:29:42,380 --> 01:29:44,540
that's optimized, it's given a goal

1945
01:29:44,540 --> 01:29:46,900
and this AI system is outputting

1946
01:29:46,900 --> 01:29:48,900
whether it's succeeding by the goal or not.

1947
01:29:48,900 --> 01:29:50,540
So we've got an AI evaluator

1948
01:29:50,540 --> 01:29:52,540
and we've got an AI system that's optimizing that goal.

1949
01:29:52,540 --> 01:29:54,660
This AI evaluator, you don't want that being game.

1950
01:29:54,660 --> 01:29:57,740
You want that AI evaluator being adversarily robust,

1951
01:29:57,740 --> 01:30:00,780
robust to optimizers trying to say

1952
01:30:00,780 --> 01:30:01,980
that it's doing a good job.

1953
01:30:01,980 --> 01:30:05,220
So that's the sort of threat model later stage

1954
01:30:05,220 --> 01:30:07,160
and that's how some of these topics

1955
01:30:07,160 --> 01:30:10,660
that were explored in vision and whatnot end up

1956
01:30:10,660 --> 01:30:14,060
and now finally with the large language models,

1957
01:30:14,060 --> 01:30:16,380
the tax paper, which I guess read about that

1958
01:30:16,380 --> 01:30:20,340
in the New York Times where jailbreak

1959
01:30:20,340 --> 01:30:23,780
and manipulate these models with little adversarial suffixes.

1960
01:30:23,780 --> 01:30:27,140
In a later stage, we'd have AI systems evaluating

1961
01:30:27,140 --> 01:30:28,620
other AI systems and you want,

1962
01:30:28,620 --> 01:30:30,100
and those AI systems that are evaluating

1963
01:30:30,100 --> 01:30:31,380
are implicitly encoding an objective

1964
01:30:31,380 --> 01:30:33,820
and you want those to be adversarily robust.

1965
01:30:33,820 --> 01:30:37,980
So adversarial robustness is not a easy problem to fix.

1966
01:30:37,980 --> 01:30:39,300
And if you don't fix that issue,

1967
01:30:39,300 --> 01:30:42,340
then you might have some AI systems just gaming the system

1968
01:30:42,340 --> 01:30:47,220
and going off, optimizing an objective aggressively,

1969
01:30:47,220 --> 01:30:48,700
that is not what we want.

1970
01:30:48,700 --> 01:30:52,380
Is there a problem here with the concept of maximization?

1971
01:30:52,380 --> 01:30:55,180
So it seems to me that it would be less dangerous

1972
01:30:55,180 --> 01:30:56,420
to tell an AI system,

1973
01:30:56,420 --> 01:31:00,180
it go earn a million dollars on the stock market

1974
01:31:00,180 --> 01:31:03,300
than to tell it go earn as much money

1975
01:31:03,300 --> 01:31:05,380
as possible on the stock market.

1976
01:31:05,380 --> 01:31:08,500
Could we kind of cap the impact

1977
01:31:08,500 --> 01:31:10,060
and the potential negative impact

1978
01:31:10,340 --> 01:31:12,900
by capping the goal also?

1979
01:31:12,900 --> 01:31:16,460
I think that's one approach you could imagine

1980
01:31:16,460 --> 01:31:17,940
conceptually a variety.

1981
01:31:17,940 --> 01:31:20,860
You could have satisficers where they basically are like,

1982
01:31:20,860 --> 01:31:22,380
and now I'm good to go.

1983
01:31:22,380 --> 01:31:25,020
I don't need to keep optimizing this aggressive.

1984
01:31:25,020 --> 01:31:30,020
There is the possibility of not giving them open-ended goals

1985
01:31:30,020 --> 01:31:32,940
or very ambitious goals would make them

1986
01:31:32,940 --> 01:31:34,580
less concerning, more constrained ones,

1987
01:31:34,580 --> 01:31:38,340
but adversarial robustness would be one.

1988
01:31:38,340 --> 01:31:39,940
There'd also be anomaly detection.

1989
01:31:40,940 --> 01:31:44,340
Anomaly detection is something

1990
01:31:44,340 --> 01:31:46,380
that's researched quite a bit in vision.

1991
01:31:46,380 --> 01:31:49,180
I've had some part in trying to have

1992
01:31:49,180 --> 01:31:51,660
the research community focus on that.

1993
01:31:51,660 --> 01:31:54,220
And I imagine anomaly detection will be very relevant again

1994
01:31:54,220 --> 01:31:56,300
when we're trying to monitor the activities

1995
01:31:56,300 --> 01:31:57,660
of various AI agents.

1996
01:31:57,660 --> 01:31:59,820
Are they doing something suspicious here?

1997
01:32:00,940 --> 01:32:02,220
While they're being monitored,

1998
01:32:02,220 --> 01:32:04,860
are they kind of adversarily trying to make the monitor think,

1999
01:32:04,860 --> 01:32:06,220
oh, it's doing the right thing.

2000
01:32:06,220 --> 01:32:08,380
So we'll need anomaly detection too to detect

2001
01:32:08,380 --> 01:32:11,580
if there's some proxy being gained.

2002
01:32:11,580 --> 01:32:14,300
And that can reduce our exposure to that risk.

2003
01:32:15,300 --> 01:32:19,580
There's also having some held out objectives

2004
01:32:19,580 --> 01:32:22,580
of which the agent is unaware

2005
01:32:22,580 --> 01:32:24,620
that it's being evaluated against.

2006
01:32:24,620 --> 01:32:28,620
And that can also do things like reduce the risk of it

2007
01:32:28,620 --> 01:32:31,420
being going to extreme and optimizing

2008
01:32:31,420 --> 01:32:33,860
the idiosyncrasies of the evaluator.

2009
01:32:33,860 --> 01:32:37,140
But this is a problem.

2010
01:32:37,140 --> 01:32:39,580
I think that most of the problem right now,

2011
01:32:39,580 --> 01:32:41,900
though, if we have large language models

2012
01:32:41,900 --> 01:32:46,420
trying to optimize a reward model that judges them,

2013
01:32:46,420 --> 01:32:47,260
they can do that

2014
01:32:47,260 --> 01:32:50,380
and they eventually start to over optimize it.

2015
01:32:50,380 --> 01:32:53,420
Although the optimizers that are much more effective

2016
01:32:53,420 --> 01:32:54,780
at breaking machine learning models

2017
01:32:54,780 --> 01:32:56,620
are actually just straight up adversarial attacks

2018
01:32:56,620 --> 01:32:59,260
compared to neural models

2019
01:32:59,260 --> 01:33:00,580
that are taking multiple steps

2020
01:33:00,580 --> 01:33:03,020
and iterating on their outputs.

2021
01:33:03,020 --> 01:33:05,380
The generic gradient-based adversarial attacks

2022
01:33:05,380 --> 01:33:06,420
are just much more effective.

2023
01:33:06,420 --> 01:33:09,460
So I think of the sort of risks of gaming,

2024
01:33:09,460 --> 01:33:10,900
I think most of us,

2025
01:33:10,900 --> 01:33:12,860
we need to do more just to address

2026
01:33:12,860 --> 01:33:14,900
the typical adversarial robustness issue.

2027
01:33:14,900 --> 01:33:18,100
Gold rift is a somewhat related issue

2028
01:33:18,100 --> 01:33:22,260
where the AI's goals shift over time

2029
01:33:22,260 --> 01:33:26,540
and the AI might come to take an instrumental goal

2030
01:33:26,540 --> 01:33:28,300
as an intrinsic goal.

2031
01:33:28,300 --> 01:33:29,740
How could this happen?

2032
01:33:30,740 --> 01:33:32,700
It's still a bit unclear to me

2033
01:33:32,700 --> 01:33:37,700
how an instrumental goal would become intrinsic over time.

2034
01:33:38,100 --> 01:33:39,420
So to start out with,

2035
01:33:39,420 --> 01:33:43,140
an intrinsic goal is something that you care about for itself.

2036
01:33:43,140 --> 01:33:47,380
That could be something like happiness or pleasure

2037
01:33:47,380 --> 01:33:49,300
for some others that could say,

2038
01:33:49,300 --> 01:33:51,940
maybe friendship, you'd say, I care about that in itself.

2039
01:33:51,940 --> 01:33:54,180
You might care about your partner's wellbeing,

2040
01:33:54,180 --> 01:33:55,300
not because it's useful to you,

2041
01:33:55,300 --> 01:33:58,780
but you care about their wellbeing in itself.

2042
01:33:58,780 --> 01:33:59,900
And then there are other things

2043
01:33:59,900 --> 01:34:01,460
that are just instrumental

2044
01:34:01,500 --> 01:34:03,540
for achieving those intrinsic goods,

2045
01:34:03,540 --> 01:34:05,340
such as like money.

2046
01:34:05,340 --> 01:34:06,780
Money lets you buy things

2047
01:34:06,780 --> 01:34:09,300
so that you could have higher wellbeing

2048
01:34:09,300 --> 01:34:12,180
or a car, it gets you from point A to point B.

2049
01:34:12,180 --> 01:34:15,700
However, some people have intrinsified,

2050
01:34:15,700 --> 01:34:19,180
to use this sort of more of a Bostrom phrase,

2051
01:34:19,180 --> 01:34:22,220
intrinsified some of these instrumental goals.

2052
01:34:22,220 --> 01:34:24,900
Some people actually just directly want money,

2053
01:34:24,900 --> 01:34:29,180
even to a point where it doesn't make sense or power.

2054
01:34:29,180 --> 01:34:31,420
Many people are just like, they want power.

2055
01:34:31,420 --> 01:34:34,740
Even if it harms other parts of their wellbeing,

2056
01:34:34,740 --> 01:34:37,020
they're willing to make that type of trade-off.

2057
01:34:37,020 --> 01:34:40,300
So they might latch onto these cues

2058
01:34:40,300 --> 01:34:43,180
and develop some of the wrong associations.

2059
01:34:43,180 --> 01:34:44,180
So we see that in people,

2060
01:34:44,180 --> 01:34:46,020
and there's a risk that AI systems

2061
01:34:46,020 --> 01:34:48,180
might develop those wrong cues as well.

2062
01:34:49,020 --> 01:34:51,140
Gold Drift could happen in some other types of way too,

2063
01:34:51,140 --> 01:34:54,500
where if you have multiple different agents,

2064
01:34:54,500 --> 01:34:57,260
they might interact in some unexpected way,

2065
01:34:57,260 --> 01:35:01,380
and then a new goal starts to drive their behavior.

2066
01:35:02,100 --> 01:35:06,260
An example, we can see this in basic AI multi-agent situations.

2067
01:35:06,260 --> 01:35:07,580
It's not catastrophic, of course,

2068
01:35:07,580 --> 01:35:08,420
because we're still here,

2069
01:35:08,420 --> 01:35:11,980
but in some AI society and some standard paper

2070
01:35:11,980 --> 01:35:13,780
from earlier this year,

2071
01:35:13,780 --> 01:35:15,260
the AI start talking with each other,

2072
01:35:15,260 --> 01:35:18,580
and then they start arranging social structures

2073
01:35:18,580 --> 01:35:21,420
that they're gonna throw an event

2074
01:35:21,420 --> 01:35:23,300
at some person's house then,

2075
01:35:23,300 --> 01:35:24,140
and then this starts to,

2076
01:35:24,140 --> 01:35:25,740
then they start acting in all these ways

2077
01:35:25,740 --> 01:35:27,380
to make sure this type of thing happened.

2078
01:35:27,380 --> 01:35:28,860
And then these sorts of things start to be

2079
01:35:28,860 --> 01:35:30,220
what drives their behavior.

2080
01:35:30,220 --> 01:35:33,580
There's some other way in which things can end up drifting,

2081
01:35:33,580 --> 01:35:35,900
not necessarily through having something to be intrinsic,

2082
01:35:35,900 --> 01:35:39,340
but there could be these emergent goals from interactions

2083
01:35:39,340 --> 01:35:40,580
that end up driving behavior.

2084
01:35:40,580 --> 01:35:44,340
Certainly there are many emergent things in society,

2085
01:35:44,340 --> 01:35:45,620
things that become new,

2086
01:35:45,620 --> 01:35:47,340
and this isn't the goal that I originally had

2087
01:35:47,340 --> 01:35:49,020
when I was 10 years old,

2088
01:35:49,020 --> 01:35:51,740
but now some of these things end up driving my behavior

2089
01:35:51,740 --> 01:35:52,580
quite substantially.

2090
01:35:52,580 --> 01:35:54,860
So if we have adaptive AI systems,

2091
01:35:54,860 --> 01:35:57,580
and if they end up responding to each other,

2092
01:35:57,580 --> 01:36:00,940
then you could have some emergent complexity happen,

2093
01:36:00,940 --> 01:36:04,340
and that those interactions that the behavior

2094
01:36:04,340 --> 01:36:06,700
starts driving the overall group behavior

2095
01:36:06,700 --> 01:36:08,180
as they're imitating each other,

2096
01:36:08,180 --> 01:36:09,980
as they're responding to each other.

2097
01:36:11,420 --> 01:36:13,940
So it's basically multi-agent systems

2098
01:36:13,940 --> 01:36:14,980
be very difficult to control.

2099
01:36:14,980 --> 01:36:15,820
In the single agent one,

2100
01:36:15,820 --> 01:36:19,020
you'd have to worry about there being some wrong association

2101
01:36:19,020 --> 01:36:21,420
between an intrinsic and instrumental goal,

2102
01:36:21,420 --> 01:36:23,220
like money or power.

2103
01:36:24,140 --> 01:36:26,060
And that could mean if that does happen,

2104
01:36:26,500 --> 01:36:29,100
if basically something wrong gets intrinsified,

2105
01:36:29,100 --> 01:36:30,660
then you're in a very dangerous situation

2106
01:36:30,660 --> 01:36:33,740
because then your AI has a goal

2107
01:36:33,740 --> 01:36:36,020
that's just different from what you wanted.

2108
01:36:36,020 --> 01:36:38,020
And so then it will, to get that goal,

2109
01:36:38,020 --> 01:36:39,260
it will optimize against you,

2110
01:36:39,260 --> 01:36:40,620
it will respond adversarily,

2111
01:36:40,620 --> 01:36:43,580
it will resist your efforts to shut it down

2112
01:36:43,580 --> 01:36:44,860
so that it can achieve that goal.

2113
01:36:44,860 --> 01:36:47,820
So although it's not something that necessarily happens

2114
01:36:47,820 --> 01:36:50,220
by default or with extremely high probability,

2115
01:36:50,220 --> 01:36:51,500
if it does happen,

2116
01:36:51,500 --> 01:36:55,580
then you've got a substantial tail risk in front of you.

2117
01:36:55,580 --> 01:36:58,660
I wonder whether these AIs will persist for long enough

2118
01:36:58,660 --> 01:37:00,220
for Gold Drift to happen.

2119
01:37:00,220 --> 01:37:05,100
So normally we retrain models every couple of years,

2120
01:37:05,100 --> 01:37:07,500
we switch out for the newest ones.

2121
01:37:07,500 --> 01:37:10,980
And so it's not like a person that has 30 years

2122
01:37:10,980 --> 01:37:13,180
to change their values.

2123
01:37:13,180 --> 01:37:16,740
Will they last long enough for Gold Drift to matter?

2124
01:37:16,740 --> 01:37:18,100
So I guess two things,

2125
01:37:18,100 --> 01:37:20,100
one is the world will move substantially more quickly

2126
01:37:20,100 --> 01:37:22,660
in the future, such that I like,

2127
01:37:22,740 --> 01:37:25,620
often in these more pivotal periods,

2128
01:37:25,620 --> 01:37:27,220
I don't know if it was Lenin or something like that,

2129
01:37:27,220 --> 01:37:29,980
like there are decades in which weeks happen

2130
01:37:29,980 --> 01:37:31,900
and then there are weeks in which like decades happen.

2131
01:37:31,900 --> 01:37:34,980
So even if there is a high replacement rate

2132
01:37:34,980 --> 01:37:36,420
in the AI population,

2133
01:37:36,420 --> 01:37:38,540
this goes on in a much lower process,

2134
01:37:38,540 --> 01:37:40,860
they could still end up constructing things

2135
01:37:40,860 --> 01:37:43,420
that end up causing their goals to be different.

2136
01:37:43,420 --> 01:37:45,540
Like they, let's say they develop some different type

2137
01:37:45,540 --> 01:37:48,540
of social infrastructure for mediating their interactions.

2138
01:37:48,540 --> 01:37:50,260
There are new AI companies being formed

2139
01:37:50,260 --> 01:37:51,980
and they're end up driving many of them.

2140
01:37:51,980 --> 01:37:54,900
Then those features of the environment

2141
01:37:54,900 --> 01:37:57,780
would end up affecting the generation that comes after it.

2142
01:37:57,780 --> 01:38:00,140
So you could still imagine some type of drift,

2143
01:38:00,140 --> 01:38:01,380
some intergenerational drift,

2144
01:38:01,380 --> 01:38:03,100
but if each generation is very short,

2145
01:38:03,100 --> 01:38:05,380
you can still imagine some type of Gold Drift in that way.

2146
01:38:05,380 --> 01:38:07,580
This is kind of think of yourself.

2147
01:38:07,580 --> 01:38:10,060
Many of the goals, the intrinsic goals that you have

2148
01:38:10,060 --> 01:38:11,900
or intrinsic desires that you have

2149
01:38:11,900 --> 01:38:14,300
are completely unlike those when you were younger.

2150
01:38:14,300 --> 01:38:18,140
The even taste in food, the things you care about,

2151
01:38:18,140 --> 01:38:19,820
I mean, maybe you acquired sports,

2152
01:38:19,820 --> 01:38:24,820
your taste in music, affiliations,

2153
01:38:24,940 --> 01:38:27,060
all of these things that are changing across time.

2154
01:38:27,060 --> 01:38:29,220
And so, and they can also go away too,

2155
01:38:29,220 --> 01:38:30,620
some of the intrinsic things you care about.

2156
01:38:30,620 --> 01:38:34,060
Like I care about this person's wellbeing for themselves,

2157
01:38:34,060 --> 01:38:35,220
but then you break up with them.

2158
01:38:35,220 --> 01:38:38,660
Oh, now I actually don't care about their wellbeing in itself.

2159
01:38:40,180 --> 01:38:42,380
I don't have that strong of a feeling toward them.

2160
01:38:42,380 --> 01:38:47,140
So adaptive systems carry this type of property.

2161
01:38:47,540 --> 01:38:51,220
This is one way in which they end up gaining some goals

2162
01:38:51,220 --> 01:38:54,380
that we didn't intend either through some emergent goal

2163
01:38:54,380 --> 01:38:56,460
from the product of various interactions

2164
01:38:56,460 --> 01:39:00,340
or through them intrinsifying some instrumental goal

2165
01:39:00,340 --> 01:39:01,420
like power.

2166
01:39:01,420 --> 01:39:04,180
They end up having too strong of an association with that

2167
01:39:04,180 --> 01:39:07,220
and reward and then just end up seeking the power itself.

2168
01:39:07,220 --> 01:39:09,100
Could Gold Drift be a good thing?

2169
01:39:09,100 --> 01:39:14,020
So we wouldn't want to fix human values

2170
01:39:14,020 --> 01:39:16,340
from the year 1800, for example.

2171
01:39:16,340 --> 01:39:19,140
You could describe our changing goals

2172
01:39:19,140 --> 01:39:21,980
from back then to now as a form of Gold Drift

2173
01:39:21,980 --> 01:39:25,740
where people from 1800 might disagree violently

2174
01:39:25,740 --> 01:39:27,940
with whatever we believe now,

2175
01:39:27,940 --> 01:39:30,060
but we still probably think it's a good thing

2176
01:39:30,060 --> 01:39:31,660
that we've changed our values.

2177
01:39:31,660 --> 01:39:34,580
Yeah, could it be good and could we learn from the AIs?

2178
01:39:34,580 --> 01:39:38,740
Yeah, so I think this is a good point

2179
01:39:38,740 --> 01:39:41,860
in what makes thinking about AI risk generally a lot harder.

2180
01:39:41,860 --> 01:39:45,100
As we mentioned earlier, there's this balance issue

2181
01:39:45,100 --> 01:39:47,340
with malicious use that because you'd

2182
01:39:47,340 --> 01:39:50,580
be concerned about unilateralist misusing AIs

2183
01:39:50,580 --> 01:39:52,780
or rogue actors misusing AIs that we should then

2184
01:39:52,780 --> 01:39:54,180
centralize power, but then you end up

2185
01:39:54,180 --> 01:39:56,940
getting some other existential risk of lock-in,

2186
01:39:56,940 --> 01:39:58,580
of concentration of power.

2187
01:39:58,580 --> 01:40:00,380
And then I think likewise, in this case too,

2188
01:40:00,380 --> 01:40:04,020
that you can't have a society in complete stasis

2189
01:40:04,020 --> 01:40:07,940
and as it would be driven by new emergent type of structures,

2190
01:40:07,940 --> 01:40:09,540
you should still try and make sure

2191
01:40:09,540 --> 01:40:12,420
that you have some control over that process

2192
01:40:12,420 --> 01:40:15,300
or reasonable control over that process.

2193
01:40:15,300 --> 01:40:18,380
It seems if there's not much control,

2194
01:40:18,380 --> 01:40:21,740
then I think you'd likely to slip from your hands.

2195
01:40:21,740 --> 01:40:24,660
But otherwise, so there's basically

2196
01:40:24,660 --> 01:40:26,140
one will have to strike a balance

2197
01:40:26,140 --> 01:40:30,500
between some very chaotic state where they're running wild

2198
01:40:30,500 --> 01:40:32,180
and some stasis.

2199
01:40:32,180 --> 01:40:33,620
And this is just a continual issue

2200
01:40:33,620 --> 01:40:40,060
in many areas of evolving groups.

2201
01:40:40,060 --> 01:40:41,740
Yeah, that would also be a problem

2202
01:40:41,820 --> 01:40:43,220
if there'd be too much entrenchment,

2203
01:40:43,220 --> 01:40:46,500
if there isn't an ability to have adaptation of the things

2204
01:40:46,500 --> 01:40:48,020
that we care about.

2205
01:40:48,020 --> 01:40:49,580
Yeah, so anyway, there's some dissonance.

2206
01:40:49,580 --> 01:40:50,940
There aren't simple answers with this.

2207
01:40:50,940 --> 01:40:53,420
This is why it's will be a balancing act.

2208
01:40:53,420 --> 01:40:58,220
And that's also why I don't expect, in particular,

2209
01:40:58,220 --> 01:41:01,620
a single solution to solve everything for all time.

2210
01:41:01,620 --> 01:41:02,820
We'll need to respond.

2211
01:41:02,820 --> 01:41:06,300
We'll need institutions and structures and control measures

2212
01:41:06,300 --> 01:41:10,460
that respond to the features of the environment

2213
01:41:10,460 --> 01:41:11,580
and calibrate reporting.

2214
01:41:11,580 --> 01:41:15,140
Why could AIs become power-seeking?

2215
01:41:15,140 --> 01:41:21,220
So this is a very, I think, one of the main AI risk stories

2216
01:41:21,220 --> 01:41:23,940
would be it becomes power-seeking.

2217
01:41:23,940 --> 01:41:26,460
I'll make a bit of a case for it,

2218
01:41:26,460 --> 01:41:29,660
and I'll speak about some issues with it too.

2219
01:41:29,660 --> 01:41:33,580
You could imagine a person gives an AI system a goal,

2220
01:41:33,580 --> 01:41:37,940
like, goal make me a lot of money as an instrumental goal,

2221
01:41:37,980 --> 01:41:41,100
gaining a lot of power seems like a very helpful way

2222
01:41:41,100 --> 01:41:44,460
to accomplish that higher-level goal.

2223
01:41:44,460 --> 01:41:48,220
So there's a concern that when you specify a goal,

2224
01:41:48,220 --> 01:41:52,620
that there'll be some sub-goals that are too correlated

2225
01:41:52,620 --> 01:41:55,900
with power, and you'd want to make sure

2226
01:41:55,900 --> 01:41:59,820
that you can control those tendencies.

2227
01:41:59,820 --> 01:42:02,980
So that's one of just being, when you're just directly giving

2228
01:42:02,980 --> 01:42:05,180
an AI goal, it may have a goal that's correlated with power.

2229
01:42:06,180 --> 01:42:09,660
Is that terribly unexpected?

2230
01:42:09,660 --> 01:42:12,060
We will give them goals that relate to power quite a bit.

2231
01:42:12,060 --> 01:42:13,780
Militaries will probably build AI systems

2232
01:42:13,780 --> 01:42:16,780
that are fairly power-seeking,

2233
01:42:16,780 --> 01:42:18,980
and so we should expect some amount of AIs

2234
01:42:18,980 --> 01:42:23,660
that are pursuing power either as their main goal

2235
01:42:23,660 --> 01:42:25,980
or as one of their main sub-goals.

2236
01:42:25,980 --> 01:42:30,460
And maybe power-seeking to a limited extent is OK?

2237
01:42:30,460 --> 01:42:34,300
Basic feature of accomplishing many of these sorts of goals.

2238
01:42:34,420 --> 01:42:36,020
For instance, the fetch-of-the-coffee one.

2239
01:42:36,020 --> 01:42:38,380
If you'd instructed to fetch a coffee,

2240
01:42:38,380 --> 01:42:40,180
it would have an incentive to preserve itself

2241
01:42:40,180 --> 01:42:42,380
because it can't fetch the coffee otherwise.

2242
01:42:42,380 --> 01:42:44,500
But you might want to curtail some of those tendencies

2243
01:42:44,500 --> 01:42:46,100
so that those don't get out of hand.

2244
01:42:46,100 --> 01:42:47,660
But that would be a...

2245
01:42:47,660 --> 01:42:50,780
We've had a paper at ICML earlier this year

2246
01:42:50,780 --> 01:42:53,420
where we're deliberately giving it penalties

2247
01:42:53,420 --> 01:42:55,980
to penalize some of these tendencies

2248
01:42:55,980 --> 01:42:58,260
that it has when it is trying to seek its reward.

2249
01:42:58,260 --> 01:43:02,060
It starts having incentives to accrue resources

2250
01:43:02,060 --> 01:43:03,460
and things like that.

2251
01:43:03,500 --> 01:43:06,980
And then can we acquire the resources

2252
01:43:06,980 --> 01:43:09,340
that are more minimal to accomplishing its goals?

2253
01:43:09,340 --> 01:43:11,260
Can we have it engage unless power-seeking behavior?

2254
01:43:11,260 --> 01:43:12,980
So I think that that's something that we can offset,

2255
01:43:12,980 --> 01:43:15,980
but we'll need to make sure that we have good control measures

2256
01:43:15,980 --> 01:43:17,860
for that to keep that in check.

2257
01:43:17,860 --> 01:43:20,300
There's also the...

2258
01:43:20,300 --> 01:43:22,660
So that's one of just people directly instructing it

2259
01:43:22,660 --> 01:43:24,060
with goals that are, by default,

2260
01:43:24,060 --> 01:43:25,900
probably going to be pretty related to power.

2261
01:43:25,900 --> 01:43:29,660
And there's also maybe they would intrinsically care...

2262
01:43:29,660 --> 01:43:32,420
Let's say that they had some random goal.

2263
01:43:32,420 --> 01:43:34,140
It's like a paperclip maximizer.

2264
01:43:34,140 --> 01:43:36,060
You're sampling from...

2265
01:43:36,060 --> 01:43:39,060
Use old verb as you're sampling from mind space

2266
01:43:39,060 --> 01:43:40,700
and then it has a random set of desires.

2267
01:43:40,700 --> 01:43:42,340
And whatever that set of desires,

2268
01:43:42,340 --> 01:43:43,740
then it would end up trying to seek

2269
01:43:43,740 --> 01:43:45,140
a substantial amount of power.

2270
01:43:45,140 --> 01:43:48,260
That's one claim,

2271
01:43:49,300 --> 01:43:53,740
but I think that has to be something more rigorously argued.

2272
01:43:53,740 --> 01:43:56,100
I should claim that, or I would like to note that.

2273
01:43:56,100 --> 01:43:59,340
I think that a lot of those power-seeking arguments,

2274
01:43:59,780 --> 01:44:03,220
I don't think it works as well as I thought it did,

2275
01:44:03,220 --> 01:44:05,340
the arguments associated with them.

2276
01:44:05,340 --> 01:44:06,780
I still think it's a relevant thing

2277
01:44:06,780 --> 01:44:10,780
that we'll want to control the sub-goals of AI systems

2278
01:44:10,780 --> 01:44:15,100
to make sure they're not too strongly related to power

2279
01:44:15,100 --> 01:44:17,660
and that there's nothing unexpected going on there.

2280
01:44:17,660 --> 01:44:19,780
So for instance, people might argue for power-seeking

2281
01:44:19,780 --> 01:44:23,740
by saying, well, power is instrumentally useful

2282
01:44:23,740 --> 01:44:26,340
for a broad variety of goals.

2283
01:44:26,340 --> 01:44:28,100
Therefore, it will seek power

2284
01:44:28,140 --> 01:44:31,020
if it's trying to accomplish any sort of reasonable goal.

2285
01:44:31,020 --> 01:44:33,380
And you'd ask them what power is,

2286
01:44:33,380 --> 01:44:34,980
and then they'd say power is,

2287
01:44:34,980 --> 01:44:36,700
what's instrumentally useful for accomplishing

2288
01:44:36,700 --> 01:44:37,540
a wide variety of goals?

2289
01:44:37,540 --> 01:44:38,700
And you'd go, okay, well, that's a tautology.

2290
01:44:38,700 --> 01:44:40,020
So we need to be more careful.

2291
01:44:40,020 --> 01:44:42,540
What exactly are we meaning by power here?

2292
01:44:42,540 --> 01:44:44,820
Separately, there's often a bit of...

2293
01:44:44,820 --> 01:44:47,940
So that's one like slight bug that lurks in the background

2294
01:44:47,940 --> 01:44:50,540
is that they'll define power in terms of instrumental stuff

2295
01:44:50,540 --> 01:44:52,900
and then it's tautological.

2296
01:44:52,900 --> 01:44:56,920
Another issue is that there's sometimes a conflation

2297
01:44:56,960 --> 01:45:00,360
between power-seeking and dominant-seeking.

2298
01:45:00,360 --> 01:45:02,600
Those are not the same thing.

2299
01:45:02,600 --> 01:45:06,160
When the AI is trying to fetch the coffee

2300
01:45:06,160 --> 01:45:09,760
and is engaging in self-preservation to do so,

2301
01:45:09,760 --> 01:45:11,560
it's not necessarily, therefore,

2302
01:45:11,560 --> 01:45:13,600
trying to take over the world.

2303
01:45:13,600 --> 01:45:15,660
So saying that an AI is power-seeking

2304
01:45:15,660 --> 01:45:18,040
is not necessarily existential.

2305
01:45:18,040 --> 01:45:20,680
Indeed, you could imagine various ways

2306
01:45:20,680 --> 01:45:23,880
in which other powerful actors

2307
01:45:23,880 --> 01:45:25,440
engage in power-seeking behavior,

2308
01:45:25,440 --> 01:45:27,320
but don't try and seek dominance.

2309
01:45:27,320 --> 01:45:29,760
So for instance, different countries

2310
01:45:29,760 --> 01:45:32,080
in trying to increase their own power to preserve themselves.

2311
01:45:32,080 --> 01:45:35,200
This is the sort of thesis of neorealism

2312
01:45:35,200 --> 01:45:36,760
or structural realism.

2313
01:45:36,760 --> 01:45:39,920
And what happens is they will basically...

2314
01:45:39,920 --> 01:45:41,640
Many states will just try and keep power

2315
01:45:41,640 --> 01:45:43,400
relative to many of their peers.

2316
01:45:43,400 --> 01:45:46,800
If Germany, for instance, tries to take...

2317
01:45:46,800 --> 01:45:48,720
It's seeking power to protect itself,

2318
01:45:48,720 --> 01:45:49,680
but if it tries seeking power

2319
01:45:49,680 --> 01:45:51,400
at the level of the global domination,

2320
01:45:51,400 --> 01:45:53,000
it will be met with force.

2321
01:45:53,000 --> 01:45:54,640
There will be balancing from other peers.

2322
01:45:54,680 --> 01:45:57,000
So when we're in a multi-agent situation,

2323
01:45:57,000 --> 01:45:58,680
then it doesn't necessarily always make sense

2324
01:45:58,680 --> 01:46:00,680
for AI systems to try and take over the world

2325
01:46:00,680 --> 01:46:02,280
because there'll be other AI agents to be

2326
01:46:02,280 --> 01:46:04,600
that will support my preferences or goals and desires,

2327
01:46:04,600 --> 01:46:06,000
so I will counteract you.

2328
01:46:06,000 --> 01:46:08,600
Balancing in international relations is what this is called.

2329
01:46:08,600 --> 01:46:10,680
That's a thing that can offset dominant seeking.

2330
01:46:10,680 --> 01:46:12,480
So it's not necessarily a case that power-seeking

2331
01:46:12,480 --> 01:46:15,600
is dominant seeking and trying to take over the world.

2332
01:46:15,600 --> 01:46:19,360
An additional point is that we can partly influence

2333
01:46:19,360 --> 01:46:21,520
the dispositions of AI systems.

2334
01:46:21,520 --> 01:46:23,440
Sorry to say, we can do that.

2335
01:46:23,440 --> 01:46:28,160
We can make these have dispositions to be a good chatbot

2336
01:46:28,160 --> 01:46:29,320
or be a good assistant.

2337
01:46:30,600 --> 01:46:31,880
Now, how strong is that?

2338
01:46:31,880 --> 01:46:35,680
It's not perfect, but if it were given a task,

2339
01:46:35,680 --> 01:46:40,680
like, hey, go accomplish some goal for me,

2340
01:46:40,840 --> 01:46:43,280
if it would think, well, the best way would be,

2341
01:46:43,280 --> 01:46:44,640
I could accomplish this goal better

2342
01:46:44,640 --> 01:46:49,640
if I were extremely powerful and took over the world,

2343
01:46:50,200 --> 01:46:52,960
but that may not be in keeping with its values

2344
01:46:53,000 --> 01:46:54,320
necessarily.

2345
01:46:54,320 --> 01:46:56,640
So it may have some tendency pulling in that direction,

2346
01:46:56,640 --> 01:46:58,800
but you could also give it some dispositions

2347
01:46:58,800 --> 01:46:59,640
to pull it against it,

2348
01:46:59,640 --> 01:47:01,280
and that might be sufficient to offset

2349
01:47:01,280 --> 01:47:03,320
some of these tendencies toward power.

2350
01:47:03,320 --> 01:47:04,680
Even if there is some incentive there,

2351
01:47:04,680 --> 01:47:06,040
it may not be enough to overwhelm it.

2352
01:47:06,040 --> 01:47:08,560
So a lot of this discussion about instrumental convergence

2353
01:47:08,560 --> 01:47:13,560
needs to think about the balance between these forces,

2354
01:47:14,080 --> 01:47:15,440
and they would need to argue basically

2355
01:47:15,440 --> 01:47:18,880
that the instrumental drive is extremely strong

2356
01:47:18,880 --> 01:47:22,080
to overwhelm fine-tuning and all these sorts of things,

2357
01:47:22,200 --> 01:47:23,520
which I don't think that there's much

2358
01:47:23,520 --> 01:47:26,240
of a specific argument for that.

2359
01:47:26,240 --> 01:47:30,040
I want to highlight here, Joe Carl Smith has a great report.

2360
01:47:30,040 --> 01:47:31,640
I think the most rigorous argument

2361
01:47:31,640 --> 01:47:35,280
for why power-seeking in AI could be existentially dangerous.

2362
01:47:35,280 --> 01:47:36,680
So just for listeners who are interested

2363
01:47:36,680 --> 01:47:40,680
in what I think is the best argument for that out there.

2364
01:47:40,680 --> 01:47:42,160
I agree, I agree.

2365
01:47:42,160 --> 01:47:46,040
He helped popularize the sort of power-seeking phrase as well,

2366
01:47:46,040 --> 01:47:48,120
and I think that by focusing on power,

2367
01:47:48,120 --> 01:47:49,280
that helped us integrate this

2368
01:47:49,280 --> 01:47:51,400
into some other like academic discussions,

2369
01:47:51,400 --> 01:47:53,080
like power versus cooperation.

2370
01:47:53,080 --> 01:47:54,960
What I was describing here,

2371
01:47:54,960 --> 01:47:56,280
just a moment to go about balancing,

2372
01:47:56,280 --> 01:47:57,720
was that we can take a cue

2373
01:47:57,720 --> 01:47:59,880
from the international relations literature

2374
01:47:59,880 --> 01:48:01,800
of seeing like, well, power-seeking agents,

2375
01:48:01,800 --> 01:48:03,840
when that's one of their main goals,

2376
01:48:03,840 --> 01:48:06,040
that doesn't necessarily turn into them

2377
01:48:06,040 --> 01:48:07,400
trying to seek domination.

2378
01:48:07,400 --> 01:48:10,160
Another thing is that in Bostrom, in superintelligence,

2379
01:48:10,160 --> 01:48:12,720
there's also a sort of spark slide of hand,

2380
01:48:12,720 --> 01:48:15,040
not intentional, but I suppose maybe an accident,

2381
01:48:15,040 --> 01:48:16,760
where he's saying that power makes you better

2382
01:48:16,760 --> 01:48:18,200
or able to accomplish your goals,

2383
01:48:18,200 --> 01:48:19,920
therefore they will seek power.

2384
01:48:19,920 --> 01:48:23,960
That's saying that something is helpful if you have it,

2385
01:48:23,960 --> 01:48:26,120
that doesn't mean that it's rational to seek it.

2386
01:48:26,120 --> 01:48:28,120
So although there's an incentive for it,

2387
01:48:28,120 --> 01:48:31,200
that doesn't mean it's instrumentally rational to pursue it.

2388
01:48:31,200 --> 01:48:34,600
So for instance, if we run the argument in a different way,

2389
01:48:34,600 --> 01:48:39,400
it would be helpful for me to be a billionaire.

2390
01:48:39,400 --> 01:48:41,440
That doesn't mean that it's rational for me

2391
01:48:41,440 --> 01:48:43,080
to try to become a billionaire.

2392
01:48:43,080 --> 01:48:46,000
I could, that would carry a lot of risks,

2393
01:48:46,000 --> 01:48:47,960
I would take a lot of time.

2394
01:48:47,960 --> 01:48:50,760
The existence of incentives aren't necessarily enough

2395
01:48:50,760 --> 01:48:54,000
to say that that's what will be driving their behavior

2396
01:48:54,000 --> 01:48:56,440
or is the first approximation of their behavior.

2397
01:48:57,400 --> 01:49:01,200
And I think that there are other ways

2398
01:49:01,200 --> 01:49:04,280
in which just power seeking doesn't emerge

2399
01:49:04,280 --> 01:49:05,880
or dominance seeking doesn't emerge.

2400
01:49:05,880 --> 01:49:08,520
If you give it some goals, like obviously if you say,

2401
01:49:08,520 --> 01:49:10,520
shut yourself off or if you give it a goal,

2402
01:49:10,520 --> 01:49:15,120
like don't seek power, these are obviously counter examples

2403
01:49:15,120 --> 01:49:16,800
for that just to show that this isn't like a,

2404
01:49:16,800 --> 01:49:18,760
it's not a law of all AI systems

2405
01:49:18,760 --> 01:49:20,240
that they will try and seek power.

2406
01:49:20,240 --> 01:49:21,600
Separately, if you give it a more goal,

2407
01:49:21,600 --> 01:49:24,400
like go fetch the milk, it could try and take over the military

2408
01:49:24,400 --> 01:49:27,560
to put up a motorcade to make sure

2409
01:49:27,560 --> 01:49:29,360
that it can get to the store very quickly.

2410
01:49:29,360 --> 01:49:31,760
But if you had some time penalty or something,

2411
01:49:31,760 --> 01:49:35,520
this would not necessarily be the thing to do.

2412
01:49:35,520 --> 01:49:38,280
So instead just go fetch the milk would often be

2413
01:49:38,280 --> 01:49:39,720
the best way of getting the reward

2414
01:49:39,720 --> 01:49:41,360
instead of some very circuitous path.

2415
01:49:42,400 --> 01:49:45,480
Now, so I do think that there is a risk of,

2416
01:49:45,480 --> 01:49:49,920
if you have AI agents that are not protected and autonomous,

2417
01:49:49,920 --> 01:49:51,720
you could get power seeking type behavior.

2418
01:49:51,720 --> 01:49:56,280
For the same reason that states try to shore up their power,

2419
01:49:56,280 --> 01:49:57,160
they shore up their power

2420
01:49:57,160 --> 01:50:00,320
because there isn't anybody they can call on for help

2421
01:50:00,320 --> 01:50:02,160
if they're getting attacked necessarily.

2422
01:50:02,160 --> 01:50:04,880
Like if the US starts getting attacked,

2423
01:50:04,880 --> 01:50:05,880
maybe some countries will come

2424
01:50:05,880 --> 01:50:09,560
but this isn't a police force that will settle the issue.

2425
01:50:09,560 --> 01:50:13,480
So the best they can do is try to shore up power

2426
01:50:13,480 --> 01:50:16,120
to defend themselves so that they can't be pushed around like that.

2427
01:50:16,120 --> 01:50:18,560
So we have a non hierarchical or quote unquote

2428
01:50:18,560 --> 01:50:20,920
anarchic international system

2429
01:50:20,920 --> 01:50:23,920
and that incentivizes agents to seek power

2430
01:50:23,920 --> 01:50:26,960
to preserve themselves to pursue whatever their goals are.

2431
01:50:26,960 --> 01:50:30,600
And you could imagine if AI systems are not protected,

2432
01:50:30,600 --> 01:50:34,000
if they are part of say some crime syndicate

2433
01:50:34,000 --> 01:50:36,120
or if they're rogue, they're unleashed,

2434
01:50:36,120 --> 01:50:37,400
somebody unleashes them,

2435
01:50:37,400 --> 01:50:39,480
then those systems would actually have

2436
01:50:39,480 --> 01:50:42,120
a very strong instrumental incentive to seek power

2437
01:50:42,120 --> 01:50:43,760
in the same way that states do,

2438
01:50:43,760 --> 01:50:46,640
that if they want to protect themselves

2439
01:50:46,640 --> 01:50:50,200
from some potential adversaries that can harm them,

2440
01:50:50,200 --> 01:50:51,440
there isn't somebody to call on.

2441
01:50:51,440 --> 01:50:52,920
They can't ask the US government,

2442
01:50:52,920 --> 01:50:53,760
if there are crimes syndicate,

2443
01:50:53,760 --> 01:50:56,280
they can't say, US government protect me, I'm getting harmed.

2444
01:50:56,280 --> 01:50:57,280
That is not a possibility to them.

2445
01:50:57,280 --> 01:50:58,760
So what they have to do is they have to take matters

2446
01:50:58,760 --> 01:51:00,320
in their own hands and accumulate their own power.

2447
01:51:00,320 --> 01:51:03,200
So what I've done is I've sort of flipped things a bit.

2448
01:51:03,200 --> 01:51:05,360
There'd be the usual argument that

2449
01:51:05,360 --> 01:51:07,880
AIs might be power seeking just by their inherent nature,

2450
01:51:07,880 --> 01:51:09,240
by the inherent natures of goals

2451
01:51:09,240 --> 01:51:10,840
and optimizers and things like that.

2452
01:51:10,840 --> 01:51:12,560
But I've instead mentioned that

2453
01:51:12,560 --> 01:51:14,960
one source of power seeking is humans give them

2454
01:51:14,960 --> 01:51:17,040
some sort of goals that are very correlated with power

2455
01:51:17,040 --> 01:51:18,480
and then there might be some unexpected stuff

2456
01:51:18,480 --> 01:51:19,880
that happens in their subgoals.

2457
01:51:19,880 --> 01:51:21,240
And then the other thing I've done is

2458
01:51:21,240 --> 01:51:23,800
I've mentioned how the structure of the environment

2459
01:51:23,800 --> 01:51:25,480
that they're in, some structural reasons

2460
01:51:25,480 --> 01:51:27,720
for why they might end up seeking power too.

2461
01:51:27,720 --> 01:51:30,640
I'm not as sure about them having an intrinsic one

2462
01:51:30,640 --> 01:51:32,440
or internal reason for power seeking,

2463
01:51:32,440 --> 01:51:35,480
but I think goals being given intentionally

2464
01:51:35,480 --> 01:51:37,840
or the structure of the environment

2465
01:51:37,840 --> 01:51:39,020
that they find themselves in,

2466
01:51:39,020 --> 01:51:40,400
it's a sort of cage that they're locked in,

2467
01:51:40,400 --> 01:51:41,600
there's really nothing they can do

2468
01:51:41,600 --> 01:51:43,000
if they're wanting to accomplish a goal

2469
01:51:43,000 --> 01:51:46,600
other than to invest a lot in protecting themselves,

2470
01:51:46,600 --> 01:51:47,720
would also incentivize them

2471
01:51:47,720 --> 01:51:50,160
to seek a substantial amount of power.

2472
01:51:50,160 --> 01:51:52,760
So I do think power seeking is a concern,

2473
01:51:52,760 --> 01:51:55,320
but not for the same reasons that other people are giving,

2474
01:51:55,320 --> 01:51:57,520
like we're gonna randomly sample a mind for mind space,

2475
01:51:57,520 --> 01:52:01,120
it'll be very alien and by a way of almost any desires,

2476
01:52:01,120 --> 01:52:05,680
it will necessarily try to seek dominance over humanity.

2477
01:52:05,680 --> 01:52:08,040
But I still would be concerned about power seeking.

2478
01:52:08,080 --> 01:52:12,760
How concerned are you about deception arising in AIs?

2479
01:52:12,760 --> 01:52:15,760
I think that the contribution of focusing on deception

2480
01:52:15,760 --> 01:52:20,760
was useful because we now see that AIs have

2481
01:52:22,560 --> 01:52:24,240
to some extent some representation

2482
01:52:24,240 --> 01:52:26,440
of morally salient considerations,

2483
01:52:26,440 --> 01:52:27,840
as we explore in the paper,

2484
01:52:27,840 --> 01:52:29,320
aligning the AIs with shared human values,

2485
01:52:29,320 --> 01:52:32,080
and I clear it maybe 2020 or something,

2486
01:52:32,080 --> 01:52:34,280
where we measure that and show that,

2487
01:52:34,280 --> 01:52:36,520
by now it's obvious because it's in chatbots

2488
01:52:36,880 --> 01:52:38,280
and people can ask it moral questions,

2489
01:52:38,280 --> 01:52:40,960
but they have some capacity for that.

2490
01:52:40,960 --> 01:52:45,720
And the deception part focuses on maybe they're actually,

2491
01:52:45,720 --> 01:52:48,720
although they maybe understand the goal,

2492
01:52:48,720 --> 01:52:51,520
they don't necessarily feel inclined to pursue it.

2493
01:52:51,520 --> 01:52:53,520
So in psychology, this is a distinction

2494
01:52:53,520 --> 01:52:57,360
between cognitive empathy and compassionate empathy.

2495
01:52:57,360 --> 01:52:59,080
Cognitive empathy psychopaths have,

2496
01:52:59,080 --> 01:53:00,240
they can understand and predict

2497
01:53:00,240 --> 01:53:02,040
what people will end up feeling

2498
01:53:02,040 --> 01:53:03,280
in response to various actions.

2499
01:53:03,280 --> 01:53:04,800
They have very good predictive model

2500
01:53:05,080 --> 01:53:08,480
of people's feelings and their emotions

2501
01:53:08,480 --> 01:53:10,800
and what they think is valuable.

2502
01:53:10,800 --> 01:53:12,240
Meanwhile, if they have compassion empathy,

2503
01:53:12,240 --> 01:53:15,720
that's when they feel motivated to do things by it

2504
01:53:15,720 --> 01:53:20,560
and help people realize those values.

2505
01:53:20,560 --> 01:53:24,800
So there's a distinction that they would have cognitive empathy

2506
01:53:24,800 --> 01:53:26,600
but not necessarily compassionate empathy.

2507
01:53:26,600 --> 01:53:29,160
And so if they're deceptive, they could basically play along.

2508
01:53:29,160 --> 01:53:32,640
They could be like, yeah, I don't actually care about you,

2509
01:53:32,640 --> 01:53:33,800
but I'm gonna act like it

2510
01:53:33,800 --> 01:53:36,920
to get my goals accomplished as psychopaths do.

2511
01:53:36,920 --> 01:53:39,680
And here, maybe we should mention here

2512
01:53:39,680 --> 01:53:44,480
how the drive of deception arises

2513
01:53:44,480 --> 01:53:48,120
from the way that we are doing reinforcement learning

2514
01:53:48,120 --> 01:53:50,520
from human feedback or how it could arise from that.

2515
01:53:50,520 --> 01:53:54,720
So in the Machiavelli ICML paper,

2516
01:53:54,720 --> 01:53:56,400
we saw instances of them doing deception

2517
01:53:56,400 --> 01:53:59,200
because it simply helps them accomplish

2518
01:53:59,200 --> 01:54:01,080
their goals better by default.

2519
01:54:01,080 --> 01:54:03,960
So many environments just incentivize the type of behavior.

2520
01:54:03,960 --> 01:54:06,720
If they have some type of misaligned goal from us,

2521
01:54:06,720 --> 01:54:11,720
then they could buy their time and wait to come to power

2522
01:54:13,360 --> 01:54:16,040
to take a quote unquote treacherous turn.

2523
01:54:16,040 --> 01:54:19,120
So it could just be very strongly incentivized

2524
01:54:19,120 --> 01:54:20,600
to buy some type of training process,

2525
01:54:20,600 --> 01:54:22,440
like by just seek more reward,

2526
01:54:22,440 --> 01:54:25,320
deception can often be a good trick when you're monitored,

2527
01:54:25,320 --> 01:54:27,920
behave nicely, when you're not monitored,

2528
01:54:27,920 --> 01:54:30,000
switch your behavior, behave in a more cutthroat way.

2529
01:54:30,000 --> 01:54:34,680
That's how a deceptive behavior can be a concern

2530
01:54:34,680 --> 01:54:37,560
or some Machiavellian type of behavior.

2531
01:54:37,560 --> 01:54:41,320
And there are instances of this.

2532
01:54:41,320 --> 01:54:44,840
You could imagine as a more non-agentic case

2533
01:54:44,840 --> 01:54:49,520
with chatbots is if they're being given human feedback,

2534
01:54:49,520 --> 01:54:50,840
maybe they'd have an incentive

2535
01:54:50,840 --> 01:54:54,360
to say very agreeable answers to people.

2536
01:54:54,360 --> 01:54:56,600
Things that they'd say, oh, that sounds good to me,

2537
01:54:56,600 --> 01:54:59,360
even though it's if it's not necessarily true.

2538
01:54:59,400 --> 01:55:02,000
So that's how even chatbots might be incentivized

2539
01:55:02,000 --> 01:55:04,920
to be in a somewhat deceptive direction.

2540
01:55:04,920 --> 01:55:06,280
But we can also see this in agents,

2541
01:55:06,280 --> 01:55:08,600
just the often helps them accomplish goals.

2542
01:55:08,600 --> 01:55:13,600
Also chatbots might learn to recognize the ways

2543
01:55:14,520 --> 01:55:18,400
in which they're telling bad lies, let's say.

2544
01:55:18,400 --> 01:55:20,280
The obvious things they're saying

2545
01:55:20,280 --> 01:55:22,080
that are false are penalized,

2546
01:55:22,080 --> 01:55:24,680
whereas the more sophisticated ways

2547
01:55:24,680 --> 01:55:28,840
they might be telling falsehoods are not penalized.

2548
01:55:29,600 --> 01:55:34,040
So this gets it in a lot of repeated interactions

2549
01:55:34,040 --> 01:55:36,560
and whatnot, deception often emerges.

2550
01:55:36,560 --> 01:55:40,400
In the evolution paper from the last time I was here,

2551
01:55:40,400 --> 01:55:44,040
we spoke about how deception can often be

2552
01:55:44,040 --> 01:55:45,240
and concealment of information

2553
01:55:45,240 --> 01:55:48,000
can often be an evolutionally stable strategy

2554
01:55:48,000 --> 01:55:49,040
and that there are many instances

2555
01:55:49,040 --> 01:55:50,240
of deception in the environment.

2556
01:55:50,240 --> 01:55:53,200
So it's a fairly difficult thing to plot out

2557
01:55:53,200 --> 01:55:54,720
when you try and control for it.

2558
01:55:54,720 --> 01:55:58,640
You often end up selecting for a more deceptive behavior.

2559
01:55:58,640 --> 01:56:03,240
At the same time, we do have progress on this though,

2560
01:56:03,240 --> 01:56:08,240
where we can, in a recent paper we submitted,

2561
01:56:09,440 --> 01:56:11,880
or in a recent paper we uploaded to archive

2562
01:56:11,880 --> 01:56:14,000
called Representation Engineering,

2563
01:56:14,000 --> 01:56:18,080
a top-down approach to AI transparency.

2564
01:56:18,080 --> 01:56:20,200
There we have instances, many instances,

2565
01:56:20,200 --> 01:56:22,200
it's not that difficult to control

2566
01:56:22,200 --> 01:56:24,560
by manipulating the internals of the model,

2567
01:56:24,560 --> 01:56:25,640
whether or not it's lying.

2568
01:56:25,640 --> 01:56:28,240
It has an internal concept of what is accurate.

2569
01:56:28,240 --> 01:56:29,480
We can find a truth direction,

2570
01:56:29,480 --> 01:56:34,040
we can subtract the direction or something of that sort,

2571
01:56:34,040 --> 01:56:37,720
and then that can cause it to spit out incorrect text

2572
01:56:37,720 --> 01:56:40,400
and we have other more sophisticated control measures too,

2573
01:56:40,400 --> 01:56:42,040
but we can manipulate internals to do that.

2574
01:56:42,040 --> 01:56:44,040
So it's within the capacity of AI systems

2575
01:56:44,040 --> 01:56:45,480
to lie and be deceptive.

2576
01:56:45,480 --> 01:56:47,440
We have another paper on that called,

2577
01:56:47,440 --> 01:56:48,720
if you search AI deception,

2578
01:56:48,720 --> 01:56:50,360
and then maybe my name or something,

2579
01:56:50,360 --> 01:56:51,600
then you'd see that paper.

2580
01:56:51,600 --> 01:56:54,160
So many instances of AI deception already,

2581
01:56:54,160 --> 01:56:56,120
but we do have some traction on this problem.

2582
01:56:56,120 --> 01:56:59,080
So fortunately, there'd still be the issue

2583
01:56:59,080 --> 01:57:01,320
of having more reliable lie detectors

2584
01:57:01,320 --> 01:57:03,360
and being able to control them to be more honest

2585
01:57:03,360 --> 01:57:05,240
or output their true beliefs.

2586
01:57:05,240 --> 01:57:09,160
So there's definitely much more work to be done,

2587
01:57:09,160 --> 01:57:11,440
but we're at least not helpless.

2588
01:57:11,440 --> 01:57:14,320
We don't need to wait another 30 years

2589
01:57:14,320 --> 01:57:17,600
for interpretability research to get to a state

2590
01:57:17,600 --> 01:57:21,240
of being able to just start to rush against the question.

2591
01:57:21,240 --> 01:57:23,440
We now have some ability to influence

2592
01:57:23,440 --> 01:57:25,760
whether AI is lie by controlling their internals.

2593
01:57:27,080 --> 01:57:29,040
And so that makes me more optimistic

2594
01:57:29,040 --> 01:57:32,400
about dealing with this problem,

2595
01:57:32,400 --> 01:57:36,240
but you don't wanna do premature celebration.

2596
01:57:36,240 --> 01:57:38,760
I don't know how much time we'll have to continue

2597
01:57:38,760 --> 01:57:41,400
getting those detection measures

2598
01:57:41,400 --> 01:57:44,960
and those control measures to be highly reliable.

2599
01:57:44,960 --> 01:57:49,200
So that'll depend on like the having a lot of researchers

2600
01:57:49,200 --> 01:57:52,560
who can research with these cutting edge very large models

2601
01:57:52,600 --> 01:57:54,480
to make progress on it.

2602
01:57:54,480 --> 01:57:56,480
Yeah, the representation engineering paper

2603
01:57:56,480 --> 01:57:58,080
was super exciting.

2604
01:57:58,080 --> 01:58:00,960
Maybe you could explain what,

2605
01:58:00,960 --> 01:58:03,760
at what level does representation engineering work?

2606
01:58:03,760 --> 01:58:06,800
Because it's different from mechanistic interpretability.

2607
01:58:06,800 --> 01:58:08,160
It's more high level,

2608
01:58:08,160 --> 01:58:10,880
and which is what we are after in a sense.

2609
01:58:10,880 --> 01:58:14,400
We are after the high level emergent behavior

2610
01:58:14,400 --> 01:58:15,800
in these models.

2611
01:58:15,800 --> 01:58:18,080
Yeah, I was mentioning compassionate empathy

2612
01:58:18,080 --> 01:58:19,080
and cognitive empathy,

2613
01:58:19,080 --> 01:58:20,480
because it's a bit of psychology,

2614
01:58:20,480 --> 01:58:21,920
but I think trying to do something

2615
01:58:21,920 --> 01:58:23,880
more like a project like AI psychology

2616
01:58:23,880 --> 01:58:25,240
or AI cognitive science,

2617
01:58:25,240 --> 01:58:26,520
is I think what we should be trying to do here.

2618
01:58:26,520 --> 01:58:30,080
So in the case of this representation engineering,

2619
01:58:30,080 --> 01:58:32,440
that's I think we're trying to be the analog of that,

2620
01:58:32,440 --> 01:58:35,120
where we're given these high level representations

2621
01:58:35,120 --> 01:58:37,240
of truth and goals and things like that.

2622
01:58:37,240 --> 01:58:41,920
Can we make it be so that it actually outputs its beliefs

2623
01:58:41,920 --> 01:58:45,120
or what it says it believes is actually what it believes?

2624
01:58:45,120 --> 01:58:46,560
For that, you need to have a handle

2625
01:58:46,560 --> 01:58:49,040
on these very high level concepts

2626
01:58:49,040 --> 01:58:50,720
so that they're not psychopathic,

2627
01:58:50,720 --> 01:58:54,720
so that we can control their dispositions to behave

2628
01:58:54,720 --> 01:58:57,080
and have things like compassionate empathy.

2629
01:58:57,080 --> 01:58:58,360
Meanwhile, I think the mechanistic stuff

2630
01:58:58,360 --> 01:58:59,680
is looking at a much lower level.

2631
01:58:59,680 --> 01:59:01,600
It's looking more at the substrate,

2632
01:59:01,600 --> 01:59:03,240
at the neuron level, at the circuit level,

2633
01:59:03,240 --> 01:59:05,360
at the node to node connection level.

2634
01:59:05,360 --> 01:59:07,880
And that's maybe closer to something like neurobiology,

2635
01:59:07,880 --> 01:59:09,600
and then what we're doing is more like trying to study

2636
01:59:09,600 --> 01:59:11,480
the mind as opposed to trying to study

2637
01:59:11,480 --> 01:59:13,600
the specific structures in the brain

2638
01:59:13,600 --> 01:59:15,440
and the connections between them

2639
01:59:15,440 --> 01:59:17,000
and how that gives rise to phenomena.

2640
01:59:17,000 --> 01:59:18,880
So I think philosophically,

2641
01:59:18,880 --> 01:59:22,440
I had tried many times to do a paper on transparency,

2642
01:59:24,160 --> 01:59:27,640
historically, but it wasn't a good angle of attack.

2643
01:59:27,640 --> 01:59:31,440
And in my view, it would take too long.

2644
01:59:31,440 --> 01:59:34,120
But I think if we do it in a more top-down type of way

2645
01:59:34,120 --> 01:59:35,960
where we try and here's the eyes of mind,

2646
01:59:35,960 --> 01:59:40,040
let's try and decompose it into some representations

2647
01:59:40,040 --> 01:59:41,840
that drive a lot of its behavior

2648
01:59:41,840 --> 01:59:44,160
and maybe decompose those further and further.

2649
01:59:44,160 --> 01:59:46,000
Basically, we have a big problem of understanding

2650
01:59:46,000 --> 01:59:48,280
in the eyes of mind, let's break it up into sub-components

2651
01:59:48,280 --> 01:59:51,120
and try and get a handle on those and control those.

2652
01:59:51,120 --> 01:59:55,440
I think that approach might be more efficient

2653
01:59:55,440 --> 01:59:58,080
at reducing risks of AI deception

2654
01:59:58,080 --> 02:00:00,600
than building from the bottom up understanding,

2655
02:00:00,600 --> 02:00:03,240
this is how it answers, this is the circuit in it

2656
02:00:03,240 --> 02:00:04,920
that lets it understand multiple

2657
02:00:04,920 --> 02:00:07,000
or identify a multiple choice question.

2658
02:00:07,000 --> 02:00:09,240
And then this helps it select the weather to output

2659
02:00:09,240 --> 02:00:13,720
the full response back or whether just to select A, B, C, or D.

2660
02:00:13,720 --> 02:00:15,200
Things like that.

2661
02:00:15,200 --> 02:00:16,160
You can build those up,

2662
02:00:16,160 --> 02:00:19,840
but that might become very complicated in time.

2663
02:00:19,840 --> 02:00:22,120
So I think it might make sense to not work from the bottom up,

2664
02:00:22,120 --> 02:00:25,520
but go from the top down.

2665
02:00:25,520 --> 02:00:27,560
There are analogs of this type of approach

2666
02:00:27,560 --> 02:00:29,160
in cognitive science.

2667
02:00:29,160 --> 02:00:31,920
People would initially try and just study things

2668
02:00:31,920 --> 02:00:34,640
at the synapse level, but it can often be more fruitful

2669
02:00:34,640 --> 02:00:38,320
of trying to understand things at the representational level.

2670
02:00:38,320 --> 02:00:41,240
What are the high-level emergent representations

2671
02:00:41,240 --> 02:00:42,960
that are a function of all the population,

2672
02:00:42,960 --> 02:00:45,120
of all the neurons in the network,

2673
02:00:45,120 --> 02:00:47,040
and try and understand things at that level?

2674
02:00:47,040 --> 02:00:49,040
Now, there's, of course, a risk of,

2675
02:00:49,040 --> 02:00:50,280
well, maybe there's some funny business

2676
02:00:50,280 --> 02:00:52,240
that gave rise to that representation.

2677
02:00:52,240 --> 02:00:54,000
And that's true.

2678
02:00:54,000 --> 02:00:56,120
We could still do things to reduce that risk

2679
02:00:56,120 --> 02:00:58,720
by trying to understand the representations

2680
02:00:58,720 --> 02:01:00,840
at various layers in the network

2681
02:01:00,840 --> 02:01:04,720
and trying to decompose the system further and further

2682
02:01:04,720 --> 02:01:09,720
so that there isn't much room for funny business or deception.

2683
02:01:10,440 --> 02:01:14,920
But so that's it at a high level.

2684
02:01:14,920 --> 02:01:17,240
It's not viewing neurons as the main unit of analysis.

2685
02:01:17,240 --> 02:01:22,080
It's viewing representations as the main unit of analysis.

2686
02:01:22,080 --> 02:01:26,320
And neurons are relevant insofar as they help us predict

2687
02:01:26,320 --> 02:01:29,400
and explain what's going on in representations.

2688
02:01:29,400 --> 02:01:32,960
But those are more of a, that's sort of just the substrate.

2689
02:01:32,960 --> 02:01:34,680
It's a comment on the substrate in the same way

2690
02:01:34,680 --> 02:01:37,960
that if we have a computer program that plays Go,

2691
02:01:37,960 --> 02:01:40,400
if I'm reasoning about the Go program,

2692
02:01:40,400 --> 02:01:43,520
I'm just probably gonna be thinking about Go strategies

2693
02:01:43,520 --> 02:01:44,640
when I'm playing against it.

2694
02:01:44,640 --> 02:01:46,280
I don't need to think at the software level,

2695
02:01:46,280 --> 02:01:47,680
like, well, where do you think it,

2696
02:01:47,680 --> 02:01:49,960
what layer do you think it's at right now?

2697
02:01:49,960 --> 02:01:54,640
Or what TensorFlow objective function did Alpha Go

2698
02:01:54,640 --> 02:01:55,600
end up optimizing here?

2699
02:01:55,600 --> 02:01:56,640
Maybe some of the examples.

2700
02:01:56,640 --> 02:01:57,760
We don't need to analyze at that level.

2701
02:01:57,760 --> 02:01:59,040
We certainly don't need to break it down

2702
02:01:59,040 --> 02:02:00,880
at the level of assembly.

2703
02:02:00,880 --> 02:02:02,120
We don't need to reason out assembly

2704
02:02:02,120 --> 02:02:03,760
to try and understand its behavior.

2705
02:02:03,760 --> 02:02:07,520
So I think that there's some emergent complexity

2706
02:02:07,560 --> 02:02:08,600
inside of neural networks.

2707
02:02:08,600 --> 02:02:12,040
We just need to, we can study it at that level

2708
02:02:12,040 --> 02:02:14,560
and it's studying at that level is fruitful

2709
02:02:14,560 --> 02:02:18,160
because there's an emergent ontology

2710
02:02:18,160 --> 02:02:20,920
and some coherent structure inside of that,

2711
02:02:20,920 --> 02:02:22,960
which you would get up getting lost in the details

2712
02:02:22,960 --> 02:02:27,960
when you end up zooming in further to the neuron level.

2713
02:02:28,800 --> 02:02:30,520
So although it's possible in principle,

2714
02:02:30,520 --> 02:02:32,080
it's possible in principle to explain everything

2715
02:02:32,080 --> 02:02:32,920
in terms of that,

2716
02:02:32,920 --> 02:02:35,200
just like it's possible to explain the economy

2717
02:02:35,200 --> 02:02:37,160
in terms of particle physics.

2718
02:02:37,360 --> 02:02:38,760
Computationally, you could do it,

2719
02:02:38,760 --> 02:02:41,360
but it doesn't make sense to study it at that level.

2720
02:02:41,360 --> 02:02:44,480
This isn't to say that their mechanistic interpretability

2721
02:02:44,480 --> 02:02:46,920
and representation engineering are completely loose

2722
02:02:46,920 --> 02:02:48,640
and separate, there's probably overlap,

2723
02:02:48,640 --> 02:02:52,640
just as like in biology and chemistry,

2724
02:02:52,640 --> 02:02:53,800
they have some overlap,

2725
02:02:53,800 --> 02:02:55,960
but you wouldn't try and understand biology

2726
02:02:55,960 --> 02:02:57,200
just through chemistry.

2727
02:02:57,200 --> 02:02:59,160
And I think if you're trying to understand representations,

2728
02:02:59,160 --> 02:03:00,600
I don't think you're necessarily just gonna try

2729
02:03:00,600 --> 02:03:03,000
and understand everything through neurons

2730
02:03:03,000 --> 02:03:04,280
and node to node connections

2731
02:03:04,280 --> 02:03:06,360
and specific execution pathways

2732
02:03:06,400 --> 02:03:09,520
and treat it like a computer program,

2733
02:03:09,520 --> 02:03:12,560
but instead something more like a mind

2734
02:03:12,560 --> 02:03:16,360
with loose associational high-level representations.

2735
02:03:16,360 --> 02:03:20,840
Yeah, so take a cognitive trait like honesty.

2736
02:03:20,840 --> 02:03:24,360
Do we know anything about how that's distributed

2737
02:03:24,360 --> 02:03:25,400
across the model?

2738
02:03:25,400 --> 02:03:30,400
Is there like a cluster of the weights

2739
02:03:31,280 --> 02:03:33,960
in which this is now representing honesty

2740
02:03:33,960 --> 02:03:38,240
or functioning as the honesty module

2741
02:03:38,240 --> 02:03:40,560
or is it more distributed across the whole model?

2742
02:03:40,560 --> 02:03:42,600
Yeah, neural network representations

2743
02:03:42,600 --> 02:03:43,760
are highly distributed,

2744
02:03:43,760 --> 02:03:45,800
which makes sort of trying to bolt down

2745
02:03:45,800 --> 02:03:47,360
and pinpoint specific locations

2746
02:03:47,360 --> 02:03:49,440
of a lot of functionality, a lot more difficult,

2747
02:03:49,440 --> 02:03:52,560
as well as the interactions between all these components too,

2748
02:03:52,560 --> 02:03:54,600
can end up giving rise to a lot of complexity.

2749
02:03:54,600 --> 02:03:56,840
Imagine that you understood a neuron

2750
02:03:56,840 --> 02:04:01,440
and it was this detects a whisker at 27 degrees

2751
02:04:01,440 --> 02:04:03,920
and this other neuron detects

2752
02:04:03,920 --> 02:04:06,080
some upper corner of a fire hydrant

2753
02:04:06,080 --> 02:04:09,720
and if you can understand these millions of neurons

2754
02:04:09,720 --> 02:04:10,680
that gets you some way,

2755
02:04:10,680 --> 02:04:13,560
but are you really understanding the collective overall

2756
02:04:13,560 --> 02:04:15,760
emergent behavior of the system?

2757
02:04:15,760 --> 02:04:17,800
That doesn't necessarily follow.

2758
02:04:17,800 --> 02:04:19,080
So I don't think it's enough to understand

2759
02:04:19,080 --> 02:04:22,320
the lowest level parts to understand the overall system

2760
02:04:22,320 --> 02:04:25,160
and its collective function, but it can be helpful.

2761
02:04:25,160 --> 02:04:26,480
It can provide some types of insights.

2762
02:04:26,480 --> 02:04:27,880
In the case of honesty though,

2763
02:04:27,880 --> 02:04:29,240
I find that it's a direction

2764
02:04:30,200 --> 02:04:32,360
or it's beliefs about what's true or not,

2765
02:04:32,360 --> 02:04:34,960
our directions in its representational space

2766
02:04:34,960 --> 02:04:38,680
and it doesn't seem to be located at a specific neuron.

2767
02:04:38,680 --> 02:04:41,800
So when we adjusting the representations

2768
02:04:41,800 --> 02:04:44,800
through various control measures that we propose,

2769
02:04:44,800 --> 02:04:47,160
then we can actually end up manipulating it.

2770
02:04:47,160 --> 02:04:49,320
So that's anyway, so partly this paper

2771
02:04:49,320 --> 02:04:51,800
is a bit more philosophical in like,

2772
02:04:51,800 --> 02:04:53,320
what's the sort of paradigm?

2773
02:04:53,320 --> 02:04:58,320
What's the strategy that we're wanting to proceed

2774
02:04:58,640 --> 02:05:00,960
in making AI systems transparent?

2775
02:05:00,960 --> 02:05:03,640
The representation level is the,

2776
02:05:04,480 --> 02:05:07,240
going to be a very fruitful way.

2777
02:05:07,240 --> 02:05:08,400
I should note that,

2778
02:05:09,800 --> 02:05:13,000
but it'd be useful to diversify over,

2779
02:05:13,000 --> 02:05:16,000
research agendas and things like that.

2780
02:05:16,000 --> 02:05:18,400
Hopefully we'll get more reliable control measures

2781
02:05:18,400 --> 02:05:21,760
and be able to modify relatively arbitrary parts.

2782
02:05:21,760 --> 02:05:23,440
We'll have success when we can like,

2783
02:05:23,440 --> 02:05:24,720
inside of the AI system,

2784
02:05:24,720 --> 02:05:27,600
when we have better ability to sort of read their mind

2785
02:05:28,480 --> 02:05:29,840
or understand the representations

2786
02:05:29,840 --> 02:05:31,880
if we could use it for like knowledge discovery.

2787
02:05:31,880 --> 02:05:34,520
Then we've known that our methods are fairly good

2788
02:05:34,520 --> 02:05:36,560
because they're probably gonna pick up some observations

2789
02:05:36,560 --> 02:05:38,320
about the world from their big pre-trained distribution

2790
02:05:38,320 --> 02:05:40,920
that no individual knows

2791
02:05:40,920 --> 02:05:43,160
or that many individuals don't know.

2792
02:05:43,160 --> 02:05:45,120
So if we can get better tools like that,

2793
02:05:45,120 --> 02:05:47,640
then that would be a late stage sign of success.

2794
02:05:47,640 --> 02:05:49,960
Yeah, and it seems like we have a better shot

2795
02:05:49,960 --> 02:05:53,520
at success here than neuroscience on humans

2796
02:05:53,520 --> 02:05:56,120
because we have such fine grained access

2797
02:05:56,120 --> 02:06:00,960
to it's as if we had a human brain spread out

2798
02:06:00,960 --> 02:06:04,840
with full access to what all of the neurons are doing.

2799
02:06:04,840 --> 02:06:06,360
So, or do you think that's right?

2800
02:06:06,360 --> 02:06:08,600
Do you think we have a better chance of success

2801
02:06:08,600 --> 02:06:10,640
compared to traditional neuroscience?

2802
02:06:10,640 --> 02:06:11,480
Yeah, yeah, certainly.

2803
02:06:11,480 --> 02:06:14,000
I think the sort of mechanistic interpretability

2804
02:06:14,000 --> 02:06:16,480
of people would claim this as well,

2805
02:06:16,480 --> 02:06:19,080
that since we have access to the gradients,

2806
02:06:19,080 --> 02:06:21,880
we have rewrite access to every component of it.

2807
02:06:21,880 --> 02:06:24,640
This allows for much more controlled replicable experiments

2808
02:06:24,640 --> 02:06:27,320
and a substantial ability to do science

2809
02:06:27,320 --> 02:06:30,200
that the bare ears to entry in cognitive science

2810
02:06:31,800 --> 02:06:33,960
or many of them are removed.

2811
02:06:33,960 --> 02:06:36,200
There's also this might get easier in time.

2812
02:06:36,200 --> 02:06:37,440
What makes this now possible?

2813
02:06:37,440 --> 02:06:38,640
Whereas previously it wasn't.

2814
02:06:38,640 --> 02:06:41,200
If you use models like GPT2 or below,

2815
02:06:41,200 --> 02:06:43,640
they just, the representations are not very good.

2816
02:06:43,640 --> 02:06:45,400
They're quite incoherent.

2817
02:06:45,400 --> 02:06:49,000
But as we use larger models like Lama2

2818
02:06:49,000 --> 02:06:50,600
pre-trained on many more tokens,

2819
02:06:50,600 --> 02:06:52,720
they have some emergent internal structure

2820
02:06:52,720 --> 02:06:54,920
that actually starts to make some sense

2821
02:06:54,920 --> 02:06:56,320
and directions that are correlated

2822
02:06:56,320 --> 02:06:58,720
with coherent concepts that humans have.

2823
02:06:58,720 --> 02:07:00,400
I think earlier it's more like a shibboleth,

2824
02:07:00,400 --> 02:07:02,520
but now there, since there is some coherence to it,

2825
02:07:02,520 --> 02:07:05,120
it's not just a big causal soup of connections.

2826
02:07:05,120 --> 02:07:06,840
So this is why I think, unfortunately,

2827
02:07:06,840 --> 02:07:09,920
this wasn't something that we could have particularly done

2828
02:07:09,920 --> 02:07:13,520
in like 2016 and is very much something

2829
02:07:13,520 --> 02:07:15,320
that's possible now that previously wasn't.

2830
02:07:15,320 --> 02:07:17,600
Dan, thanks for spending so much time with us here.

2831
02:07:17,600 --> 02:07:19,960
It's been very valuable for me.

2832
02:07:19,960 --> 02:07:22,800
And I think it will be for our listeners too.

2833
02:07:22,800 --> 02:07:23,640
Great, great, great.

2834
02:07:23,640 --> 02:07:24,480
Thank you for having me.

2835
02:07:24,480 --> 02:07:25,320
Have a good day.

