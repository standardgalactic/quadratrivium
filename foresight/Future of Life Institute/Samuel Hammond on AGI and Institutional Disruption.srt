1
00:00:00,000 --> 00:00:03,360
Welcome to the Future of Life Institute podcast.

2
00:00:03,360 --> 00:00:04,520
My name is Gus Docher,

3
00:00:04,520 --> 00:00:06,480
and I'm here with Samuel Hammond,

4
00:00:06,480 --> 00:00:08,760
who is a senior economist at

5
00:00:08,760 --> 00:00:11,160
the Foundation for American Innovation.

6
00:00:11,160 --> 00:00:13,040
Samuel, welcome to the podcast.

7
00:00:13,040 --> 00:00:14,400
I guess. Thanks for having me.

8
00:00:14,400 --> 00:00:16,000
Fantastic. All right.

9
00:00:16,000 --> 00:00:18,280
I have so much I want to talk to you about,

10
00:00:18,280 --> 00:00:21,240
but I think a natural place to start here would be with

11
00:00:21,240 --> 00:00:24,080
your timelines to AGI.

12
00:00:24,080 --> 00:00:29,280
Why is it that you expect AGI to get here before most people?

13
00:00:29,280 --> 00:00:31,120
Well, I don't really know what most people think.

14
00:00:31,120 --> 00:00:34,720
I think the world divides into people who are paying

15
00:00:34,720 --> 00:00:38,560
attention and people who are basically normies.

16
00:00:38,560 --> 00:00:42,480
In my day job, I work on Capitol Hill and in Washington,

17
00:00:42,480 --> 00:00:44,960
D.C., talking to folks about AGI.

18
00:00:44,960 --> 00:00:48,400
If you think about what people's implicit timelines are,

19
00:00:48,400 --> 00:00:52,040
you can read out people's implicit timelines by their behavior.

20
00:00:52,040 --> 00:00:55,200
I know Paul Cristiano has short timelines

21
00:00:55,200 --> 00:00:58,040
because he's doubled up into the stock market.

22
00:00:58,440 --> 00:01:01,160
He's practicing what he preaches.

23
00:01:01,160 --> 00:01:04,960
But then when you have Sam Altman testifying to work to Congress,

24
00:01:04,960 --> 00:01:07,960
I like to say people are taking him seriously,

25
00:01:07,960 --> 00:01:09,800
but not literally. He's saying,

26
00:01:09,800 --> 00:01:11,660
we're going to develop something like AGI,

27
00:01:11,660 --> 00:01:12,880
potentially this decade,

28
00:01:12,880 --> 00:01:15,440
and superintelligence thereafter.

29
00:01:15,440 --> 00:01:18,120
Then you have folks like Sandra Marshall Blackburn

30
00:01:18,120 --> 00:01:20,920
being like, what will this mean for music royalties?

31
00:01:21,920 --> 00:01:26,000
When the focus of policymakers is things like

32
00:01:26,040 --> 00:01:29,520
music royalties or impact on copyright,

33
00:01:29,520 --> 00:01:31,680
it's not that those are invalid issues.

34
00:01:31,680 --> 00:01:35,440
It's that they belie relatively longer timelines.

35
00:01:35,440 --> 00:01:37,520
Then we also have this definitional confusion

36
00:01:37,520 --> 00:01:42,040
where folks like John Lacoon would say AGI is probably decades away

37
00:01:42,040 --> 00:01:45,440
because he is using AGI to mean something that learns,

38
00:01:45,440 --> 00:01:48,280
like a human learns in the sense that it's born

39
00:01:48,280 --> 00:01:51,400
as a relative blank slate and can acquire language

40
00:01:51,400 --> 00:01:54,320
with very few examples.

41
00:01:54,320 --> 00:01:56,680
People have these moving goalposts of what they mean.

42
00:01:56,680 --> 00:02:01,680
For me, I think we can avoid those definitional conflicts

43
00:02:01,760 --> 00:02:04,680
if we just talk about human level intelligence,

44
00:02:04,680 --> 00:02:08,480
and humans are quite general or generally intelligent.

45
00:02:08,480 --> 00:02:12,920
That's what separates us from animals in a lot of respects.

46
00:02:12,920 --> 00:02:15,880
And when you look at how machine learning models

47
00:02:15,880 --> 00:02:17,280
are being trained today,

48
00:02:17,280 --> 00:02:20,280
like large language models and now multimodal models,

49
00:02:20,280 --> 00:02:22,000
they're being trained on human data,

50
00:02:22,000 --> 00:02:27,680
and they're being trained to reproduce the kinds of behaviors

51
00:02:27,680 --> 00:02:30,880
and tasks and outputs that humans output.

52
00:02:30,880 --> 00:02:33,000
And so they're kind of like an indirect way

53
00:02:33,000 --> 00:02:35,080
of emulating human intelligence.

54
00:02:35,080 --> 00:02:39,760
And then so if you benchmark AI progress to that,

55
00:02:39,760 --> 00:02:42,320
then you can sort of put information theoretic bounds

56
00:02:42,320 --> 00:02:45,480
on what's the likely timeline

57
00:02:45,480 --> 00:02:48,360
to basically an ideal human emulator,

58
00:02:48,360 --> 00:02:52,640
something that can extract the sort of base representations,

59
00:02:52,640 --> 00:02:55,000
the internal representations of our brain

60
00:02:55,000 --> 00:02:59,160
through the indirect path of the data that our brain generates.

61
00:02:59,160 --> 00:03:01,600
Yeah, you have an interesting sentence where you write that

62
00:03:01,600 --> 00:03:04,240
AI can advance by emulating the generator

63
00:03:04,240 --> 00:03:07,680
of human-generated data, which is simply the brain.

64
00:03:07,680 --> 00:03:11,280
Do you think this paradigm holds all the way to AGI?

65
00:03:11,280 --> 00:03:14,120
I think it holds this decade to systems

66
00:03:14,120 --> 00:03:18,440
that in principle can in context learn anything humans do.

67
00:03:18,440 --> 00:03:19,880
Again, this is a semantic question.

68
00:03:19,880 --> 00:03:21,640
Do you want to call that AGI or not?

69
00:03:21,640 --> 00:03:25,960
I think there are still outstanding issues around the limits

70
00:03:25,960 --> 00:03:28,840
of other aggressive models for autonomy

71
00:03:28,840 --> 00:03:32,720
and the question of sort of real-time learning,

72
00:03:32,720 --> 00:03:34,280
the way we train these models,

73
00:03:34,280 --> 00:03:36,280
we sort of are freezing a crystal in place

74
00:03:36,280 --> 00:03:40,120
and humans are continuously learning.

75
00:03:40,120 --> 00:03:45,720
So there still are genuine potential architectural gaps,

76
00:03:45,720 --> 00:03:48,840
but from the practical point of view,

77
00:03:48,840 --> 00:03:51,080
from the economic point of view,

78
00:03:51,080 --> 00:03:53,200
we don't need to debate whether something is conscious

79
00:03:53,200 --> 00:03:56,640
or whether something learns strictly the way humans learn

80
00:03:56,640 --> 00:04:00,160
if it demonstrably can do the things humans do, right?

81
00:04:00,160 --> 00:04:05,240
And that goes to the original insight of the Turing test, right?

82
00:04:05,240 --> 00:04:06,840
It's sometimes presented as a thought experiment,

83
00:04:06,840 --> 00:04:08,320
but what Alan Turing was getting at

84
00:04:09,040 --> 00:04:12,680
was if you can't distinguish between the human and a computer,

85
00:04:12,680 --> 00:04:17,200
in some ways, indistinguishability implies competence, right?

86
00:04:17,200 --> 00:04:19,480
And we can broaden that from just language

87
00:04:19,480 --> 00:04:22,200
because arguably we've surpassed the Turing test,

88
00:04:22,200 --> 00:04:24,000
at least a weaker version of it,

89
00:04:24,000 --> 00:04:27,560
to human performance on tasks in general, right?

90
00:04:27,560 --> 00:04:31,720
If we have a system that can output a scientific manuscript

91
00:04:31,720 --> 00:04:34,560
that experts in the field can't distinguish from a human,

92
00:04:34,560 --> 00:04:40,320
then debating whether this is real AGI or not

93
00:04:40,320 --> 00:04:43,520
is, I feel, academic.

94
00:04:43,520 --> 00:04:47,480
It is surprising in a sense that when you interact with GPT-4,

95
00:04:47,480 --> 00:04:50,880
for example, and it can do all kinds of amazing things

96
00:04:50,880 --> 00:04:54,120
and organize information, present information to you,

97
00:04:54,120 --> 00:04:56,560
but then it can't, or at least at some point,

98
00:04:56,560 --> 00:04:58,680
it couldn't answer questions about the world

99
00:04:58,680 --> 00:05:02,920
after September 2021 or a date like that.

100
00:05:02,920 --> 00:05:06,080
That would be surprising if you presented that fact

101
00:05:06,080 --> 00:05:09,120
to an AI scientist 20 years ago.

102
00:05:09,120 --> 00:05:11,400
For how long do you think we'll remain in this paradigm

103
00:05:11,400 --> 00:05:13,880
of training a foundational model

104
00:05:13,880 --> 00:05:16,080
and then deploying that model?

105
00:05:16,080 --> 00:05:16,920
I mean, it's worse than that.

106
00:05:16,920 --> 00:05:19,040
I think it was surprised people five years ago.

107
00:05:19,040 --> 00:05:20,520
Progress is sort of moving along two tracks.

108
00:05:20,520 --> 00:05:21,440
There's the industry track

109
00:05:21,440 --> 00:05:24,360
and the peer research academic track,

110
00:05:24,360 --> 00:05:27,400
and they're obviously having feedback with one another.

111
00:05:27,400 --> 00:05:31,080
The peer industry track is just looking to create tools

112
00:05:31,080 --> 00:05:33,480
that have practical value

113
00:05:33,480 --> 00:05:35,320
and can improve products and so forth.

114
00:05:35,320 --> 00:05:38,840
And so, Meta has their own GPU cluster

115
00:05:38,840 --> 00:05:40,080
and their training models,

116
00:05:40,080 --> 00:05:43,920
so they can have fun chatbots in their messenger.

117
00:05:43,920 --> 00:05:47,560
And so, those kinds of things are going to progress,

118
00:05:47,560 --> 00:05:50,800
I think, well within the current paradigm

119
00:05:50,800 --> 00:05:53,720
because we know the paradigm works,

120
00:05:53,720 --> 00:05:57,520
basically deep learning and transformers.

121
00:05:57,520 --> 00:05:59,640
And there's lots of marginality on the side,

122
00:05:59,640 --> 00:06:03,560
but that basic framework seems to be quite effective.

123
00:06:03,560 --> 00:06:04,520
And just scaling that up

124
00:06:04,520 --> 00:06:07,240
because we haven't sort of hit the range

125
00:06:07,240 --> 00:06:10,160
of irreducible loss and what transformers can do.

126
00:06:10,160 --> 00:06:13,040
Meanwhile, there's also this parallel peer research track

127
00:06:13,040 --> 00:06:15,560
where people on seemingly a weekly basis

128
00:06:15,560 --> 00:06:19,640
are finding better ways of specifying the loss function,

129
00:06:19,640 --> 00:06:22,640
ways of improving upon power loss scaling

130
00:06:22,640 --> 00:06:24,440
and all these different,

131
00:06:25,600 --> 00:06:26,720
sometimes they're new architectures,

132
00:06:26,720 --> 00:06:28,920
but often they're just like bags of tricks.

133
00:06:29,920 --> 00:06:32,160
And those bags of tricks then,

134
00:06:32,160 --> 00:06:35,160
to the extent that they comport with

135
00:06:35,160 --> 00:06:38,520
the paradigm industries running with,

136
00:06:38,520 --> 00:06:39,880
they can be reintegrated

137
00:06:39,880 --> 00:06:42,760
and end up accelerating progress and industry as well.

138
00:06:43,880 --> 00:06:47,760
So, do you think scale of compute is the main barrier

139
00:06:47,760 --> 00:06:50,080
to getting to human level AI?

140
00:06:51,000 --> 00:06:54,320
Yes, right, I mean, it's not all we need,

141
00:06:54,320 --> 00:06:57,600
but it's the main unlock, right?

142
00:06:57,600 --> 00:07:01,720
To what extent can more compute be used to trade off

143
00:07:01,720 --> 00:07:06,720
for lower quality data or for lower quality algorithms?

144
00:07:07,360 --> 00:07:08,800
Can you just throw more compute

145
00:07:08,800 --> 00:07:12,760
and solve the other factors in that equation?

146
00:07:12,760 --> 00:07:14,880
It depends on the thing you're trying to solve for.

147
00:07:14,880 --> 00:07:19,120
In principle, if we're talking about mapping inputs to outputs,

148
00:07:19,120 --> 00:07:21,200
then transformers are known

149
00:07:21,200 --> 00:07:23,200
to be universal function approximators.

150
00:07:23,200 --> 00:07:26,800
And so the answer is yes.

151
00:07:26,800 --> 00:07:28,840
That doesn't mean that they're necessarily efficient

152
00:07:28,840 --> 00:07:31,520
at approximating everything we want them to approximate.

153
00:07:31,520 --> 00:07:35,160
And sometimes universal function approximation theorems

154
00:07:35,160 --> 00:07:37,360
can be kind of trivial because they'll be like, okay,

155
00:07:37,360 --> 00:07:39,040
if your neural network is infinite width,

156
00:07:39,040 --> 00:07:41,440
then yes, we can approximate everything.

157
00:07:41,440 --> 00:07:45,680
The key fact is both that they're universal approximators

158
00:07:45,680 --> 00:07:48,520
and also that they're relatively sample efficient,

159
00:07:48,520 --> 00:07:50,720
at least relative to things we found in the past.

160
00:07:50,720 --> 00:07:51,960
And so that to me suggests that yes,

161
00:07:51,960 --> 00:07:55,760
they can compensate for things that they're bad at.

162
00:07:55,760 --> 00:07:57,960
On the other hand, the way research is trending

163
00:07:57,960 --> 00:07:59,640
is towards these mixed models,

164
00:08:00,680 --> 00:08:03,000
ensembles of different kinds of architectures,

165
00:08:03,000 --> 00:08:04,960
things like the recent Q transformer

166
00:08:04,960 --> 00:08:06,680
announced from Google DeepMind,

167
00:08:06,680 --> 00:08:09,640
which just sort of uses a combination of transformers

168
00:08:09,640 --> 00:08:13,440
and Q learning to have sort of the associational memory

169
00:08:13,440 --> 00:08:16,640
and sample efficiency of transformers

170
00:08:16,640 --> 00:08:19,840
with the ability to assign policies to do tasks

171
00:08:19,840 --> 00:08:21,640
that you get from reinforcement learning.

172
00:08:21,640 --> 00:08:23,480
So I imagine that there's going to be all kinds

173
00:08:23,480 --> 00:08:25,140
of mixing and matching.

174
00:08:25,580 --> 00:08:29,580
The key point is that in that space of architectures,

175
00:08:29,580 --> 00:08:33,220
it's a relatively sort of finite search space, right?

176
00:08:33,220 --> 00:08:35,940
And as an economist, economists believe

177
00:08:35,940 --> 00:08:39,060
that supply is long run elastic, right?

178
00:08:39,060 --> 00:08:42,100
And so there's this famous bet from the team,

179
00:08:42,100 --> 00:08:45,500
Paul Ehrlich and Julian Simon vis-a-vis the population bomb

180
00:08:45,500 --> 00:08:47,340
and whether population growth would lead

181
00:08:47,340 --> 00:08:51,340
to sort of a mothusian purge.

182
00:08:51,340 --> 00:08:53,860
And Julian Simon being the economist

183
00:08:53,860 --> 00:08:57,580
recognized that if prices rise for these core commodities,

184
00:08:57,580 --> 00:09:00,180
then that will spur research and development

185
00:09:00,180 --> 00:09:02,380
into extracting new resources, right?

186
00:09:02,380 --> 00:09:04,060
So he didn't have to know

187
00:09:04,060 --> 00:09:06,340
that fracking would be a technology.

188
00:09:06,340 --> 00:09:09,580
He understood that if oil prices went too high,

189
00:09:09,580 --> 00:09:12,300
people would find new oil reserves.

190
00:09:12,300 --> 00:09:14,900
And I think there's, I have an analogous instinct

191
00:09:14,900 --> 00:09:18,060
when it comes to progress and deep learning,

192
00:09:18,060 --> 00:09:20,060
meaning you can become too anchored

193
00:09:20,060 --> 00:09:23,420
to sort of the current state of the literature,

194
00:09:23,420 --> 00:09:26,900
but over a tenure horizon, you can say,

195
00:09:26,900 --> 00:09:29,900
well, there's a huge search on a huge gold rush

196
00:09:29,900 --> 00:09:34,580
to find the right way of blending these architectures.

197
00:09:34,580 --> 00:09:37,020
And I don't need to know in advance,

198
00:09:37,020 --> 00:09:38,940
which is the right way to do that

199
00:09:38,940 --> 00:09:41,940
to have high confidence that someone will find it.

200
00:09:41,940 --> 00:09:45,180
Yeah, we can sometimes, if we're too deep in the literature,

201
00:09:45,180 --> 00:09:49,060
we might lose the focus on the forest for the trees

202
00:09:49,060 --> 00:09:49,900
in a sense.

203
00:09:49,900 --> 00:09:52,100
And if we zoom out, we can just see

204
00:09:52,100 --> 00:09:55,140
that there's more investment, there's more talent

205
00:09:55,140 --> 00:09:57,060
pouring into AI, and so we can predict

206
00:09:57,060 --> 00:09:59,460
that something is gonna come out of that.

207
00:09:59,460 --> 00:10:01,220
You have lots of interesting insights

208
00:10:01,220 --> 00:10:02,460
about information theory

209
00:10:02,460 --> 00:10:05,980
and how this can help us predict AI.

210
00:10:05,980 --> 00:10:08,900
What's the most important lessons from information theory?

211
00:10:08,900 --> 00:10:13,460
The reason I start there is because sort of,

212
00:10:13,460 --> 00:10:14,980
within the conceptual realm,

213
00:10:14,980 --> 00:10:17,540
it's sort of like the most general thing

214
00:10:17,540 --> 00:10:20,020
that bounds everything else.

215
00:10:20,020 --> 00:10:22,460
And when you look back at the record of, say,

216
00:10:22,460 --> 00:10:26,460
Ray Kurzweil, I first read The Age of Spiritual Machines

217
00:10:26,460 --> 00:10:29,100
when I was a kid, and in there,

218
00:10:29,100 --> 00:10:30,700
he makes a prediction that we'll have

219
00:10:30,700 --> 00:10:34,580
AIs that pass the Turing test by 2029 or so.

220
00:10:34,580 --> 00:10:36,820
And when was this book written?

221
00:10:36,820 --> 00:10:38,020
1999.

222
00:10:38,020 --> 00:10:39,500
Yeah, that's pretty good.

223
00:10:39,500 --> 00:10:42,340
Right, and so, and people will complain

224
00:10:42,340 --> 00:10:43,180
that he got things wrong,

225
00:10:43,180 --> 00:10:47,220
because he said, well, I'll be wearing AR glasses by 2019,

226
00:10:48,140 --> 00:10:51,300
when in fact, Google Glass came out in 2013,

227
00:10:51,300 --> 00:10:54,620
and now we have Meta Glasses five years later.

228
00:10:54,620 --> 00:10:57,100
So he was wrong on the exact timing,

229
00:10:57,100 --> 00:10:59,020
but sort of right where the technology was wrong,

230
00:10:59,020 --> 00:11:02,060
where the minimal viable product was.

231
00:11:02,060 --> 00:11:05,220
But nonetheless, if you look at his track record,

232
00:11:05,220 --> 00:11:08,740
it's quite good for a methodology

233
00:11:08,740 --> 00:11:13,100
as relatively stupid as just staring at Moore's Law,

234
00:11:13,100 --> 00:11:15,220
and extrapolating it out.

235
00:11:15,220 --> 00:11:17,860
And I think that reveals the power

236
00:11:17,860 --> 00:11:21,580
of these information theoretic methodologies to forecasting

237
00:11:21,580 --> 00:11:24,260
because they set bounds on what will be possible.

238
00:11:24,260 --> 00:11:27,220
The team at epoch.ai have a forecast

239
00:11:27,220 --> 00:11:30,020
called the direct approach where it's sort of,

240
00:11:30,020 --> 00:11:33,220
you can think of it sort of like a way of putting bounds

241
00:11:33,220 --> 00:11:37,380
on when we'll have AIs that can emulate human performance

242
00:11:37,380 --> 00:11:39,580
through an information theoretic lens

243
00:11:39,580 --> 00:11:41,940
where they're looking at sort of how much entropy

244
00:11:41,940 --> 00:11:43,420
does the brain sort of process

245
00:11:43,460 --> 00:11:46,340
and how much compute will we have over time

246
00:11:46,340 --> 00:11:49,220
and what's implied by AI scaling laws.

247
00:11:49,220 --> 00:11:51,860
And you sort of put those three things together

248
00:11:51,860 --> 00:11:53,220
and you can sort of set bounds

249
00:11:53,220 --> 00:11:56,180
on when we'll basically be able to brute force

250
00:11:56,180 --> 00:11:58,660
human level intelligence.

251
00:11:58,660 --> 00:12:00,020
And of course, that's an upper bound

252
00:12:00,020 --> 00:12:01,540
because we're going to do better than brute force.

253
00:12:01,540 --> 00:12:03,100
We're going to also have insights

254
00:12:03,100 --> 00:12:05,740
from cognitive science and neuroscience

255
00:12:05,740 --> 00:12:10,740
and also ways of distilling neural networks and so forth

256
00:12:10,740 --> 00:12:12,300
and better ways of curating data.

257
00:12:12,300 --> 00:12:16,060
So their modal estimate for human level AI is 2029

258
00:12:16,060 --> 00:12:17,740
and their meeting is like 2036.

259
00:12:17,740 --> 00:12:20,620
And I've talked to the authors and they lean towards

260
00:12:20,620 --> 00:12:24,940
the 2029, 2030 for their own personal forecasts.

261
00:12:24,940 --> 00:12:28,940
And so going back to, is this a net liar?

262
00:12:28,940 --> 00:12:30,820
Am I out on a limb here?

263
00:12:30,820 --> 00:12:32,980
I think among our circles probably not,

264
00:12:32,980 --> 00:12:36,220
but among Congress and among the broader public,

265
00:12:36,220 --> 00:12:38,060
I think people are seeing sort of,

266
00:12:38,060 --> 00:12:40,660
they think everything's an asymptote, right?

267
00:12:41,180 --> 00:12:43,020
They're imagining, okay, we have these chatbots

268
00:12:43,020 --> 00:12:45,180
and they're not seeing the next step.

269
00:12:45,180 --> 00:12:47,940
I see a very smooth path from here to systems

270
00:12:47,940 --> 00:12:50,540
that can basically in context learn

271
00:12:50,540 --> 00:12:52,460
any arbitrary human task.

272
00:12:52,460 --> 00:12:53,780
And so what does that look like?

273
00:12:53,780 --> 00:12:57,020
It looks like systems that can basically sit over your shoulder

274
00:12:57,020 --> 00:12:59,900
or can monitor your desktop, your operating system as you work

275
00:12:59,900 --> 00:13:03,220
and watch you for an hour or two and then take over.

276
00:13:03,220 --> 00:13:06,500
And that'll be key to overcoming lack of training data

277
00:13:06,500 --> 00:13:10,540
or why is it important that they can learn in context?

278
00:13:10,740 --> 00:13:14,580
Well, in context learning is sort of the secret source

279
00:13:14,580 --> 00:13:16,580
of the power of transformer models.

280
00:13:16,580 --> 00:13:19,180
They learn these inductive biases and induction heads

281
00:13:19,180 --> 00:13:21,140
and so forth that let them,

282
00:13:21,140 --> 00:13:22,660
a few shot learn different tasks.

283
00:13:22,660 --> 00:13:25,660
So, GPT-4 is very good at zero shot learning

284
00:13:25,660 --> 00:13:27,140
on a variety of different things,

285
00:13:27,140 --> 00:13:29,580
but it's incredibly good at few shot learning.

286
00:13:29,580 --> 00:13:30,740
If you give it a few examples,

287
00:13:30,740 --> 00:13:32,820
it can kind of pick up where you left off.

288
00:13:32,820 --> 00:13:33,860
You know, when I think about myself,

289
00:13:33,860 --> 00:13:36,040
when I wanna learn a new recipe, right?

290
00:13:36,040 --> 00:13:37,140
I can go read a recipe book,

291
00:13:37,140 --> 00:13:39,500
but often what I prefer to do is to go on YouTube

292
00:13:39,500 --> 00:13:41,980
and watch someone make that recipe, right?

293
00:13:41,980 --> 00:13:45,820
And just by watching that person put together the stir fry,

294
00:13:45,820 --> 00:13:47,540
I have enough of a world model

295
00:13:47,540 --> 00:13:50,120
and enough of knowledge of how to cook in general

296
00:13:50,120 --> 00:13:53,700
that I can sort of in context learn

297
00:13:53,700 --> 00:13:56,380
how to pick up from there and do that recipe myself.

298
00:13:56,380 --> 00:13:57,460
LLMs do that already.

299
00:13:57,460 --> 00:14:00,140
Multimodal models are increasingly doing that.

300
00:14:00,140 --> 00:14:02,980
Some of the recent progress in robotics,

301
00:14:03,800 --> 00:14:05,920
like I mentioned, the Q-transformer paper,

302
00:14:05,920 --> 00:14:08,100
it shows that you can basically build robots

303
00:14:08,140 --> 00:14:09,660
as a basic world model

304
00:14:09,660 --> 00:14:11,340
and then have it learn new tasks

305
00:14:11,340 --> 00:14:14,380
with fewer than 100 examples of the human demonstration.

306
00:14:14,380 --> 00:14:15,860
So the human sort of demonstrates the task

307
00:14:15,860 --> 00:14:19,340
and the robot can pick it up and take it from there.

308
00:14:19,340 --> 00:14:21,700
And why that's important is both

309
00:14:21,700 --> 00:14:24,700
for understanding the trajectory of AI,

310
00:14:24,700 --> 00:14:28,420
but also its economic implementation

311
00:14:28,420 --> 00:14:31,140
because we're sort of used to automation

312
00:14:31,140 --> 00:14:33,940
being this thing where you get a contract from IBM

313
00:14:33,940 --> 00:14:37,460
and you spend many millions of dollars with consultants

314
00:14:37,460 --> 00:14:38,740
and they build you some bespoke thing

315
00:14:38,740 --> 00:14:40,500
that doesn't really work very well

316
00:14:40,500 --> 00:14:41,940
and requires lots of maintenance.

317
00:14:41,940 --> 00:14:45,220
And so people have this sort of prior that AI,

318
00:14:45,220 --> 00:14:48,940
even if it's near, will be rate limited by the real world

319
00:14:48,940 --> 00:14:51,860
because of all the complexity of implementation.

320
00:14:51,860 --> 00:14:54,780
But the point is if you have things that can in context learn

321
00:14:54,780 --> 00:14:57,620
and perform sort of as humans perform,

322
00:14:57,620 --> 00:14:59,980
then you don't need to change the process.

323
00:14:59,980 --> 00:15:02,980
You can take human designed processes

324
00:15:02,980 --> 00:15:06,220
and have the AI just fill in for the human.

325
00:15:06,260 --> 00:15:07,420
And so it leads to this paradox

326
00:15:07,420 --> 00:15:08,940
where we're probably going to have AGI

327
00:15:08,940 --> 00:15:11,220
before we get rid of the last fax machine.

328
00:15:11,220 --> 00:15:16,220
Yeah, when we think of say old IT systems

329
00:15:16,940 --> 00:15:20,700
in large institutions, we might think of moving

330
00:15:20,700 --> 00:15:25,700
from analog storage of information to the cloud.

331
00:15:25,900 --> 00:15:28,340
That's still going on in some institutions.

332
00:15:28,340 --> 00:15:31,340
That transformation has taken over a decade now.

333
00:15:31,340 --> 00:15:34,660
And so what exactly is it that makes AI different here?

334
00:15:34,660 --> 00:15:37,060
It is that AI plugs in directly

335
00:15:37,060 --> 00:15:38,700
where the human worker would be?

336
00:15:38,700 --> 00:15:40,420
Yeah, precisely.

337
00:15:40,420 --> 00:15:44,180
You don't need to redesign existing process

338
00:15:44,180 --> 00:15:47,340
to sort of plug into the automation.

339
00:15:47,340 --> 00:15:50,860
And that applies both for sort of the structure of tasks.

340
00:15:50,860 --> 00:15:53,740
So much of a mechanical automation

341
00:15:53,740 --> 00:15:57,260
takes something like the sort of artisanal work

342
00:15:57,260 --> 00:16:01,140
of a shoemaker and has to translate it

343
00:16:01,140 --> 00:16:03,540
into something repetitive that a machine

344
00:16:03,540 --> 00:16:07,300
or an automatic seamstress can do over and over and over

345
00:16:07,300 --> 00:16:09,460
or against our older school kind of automation

346
00:16:09,460 --> 00:16:12,980
requires sort of collapsing a task into a lower dimension

347
00:16:12,980 --> 00:16:16,660
so that simple automations can handle it.

348
00:16:16,660 --> 00:16:20,100
But when you have AGI, the whole point is generality.

349
00:16:20,100 --> 00:16:23,140
It's a flexible intelligence

350
00:16:23,140 --> 00:16:26,780
that can map to existing kinds of processes.

351
00:16:26,780 --> 00:16:29,380
So that's sort of why I think this will catch people

352
00:16:29,380 --> 00:16:32,220
by surprise because it's not just that AGI

353
00:16:32,220 --> 00:16:35,180
could be this decade, but that when it arrives

354
00:16:35,180 --> 00:16:37,660
and sort of crosses some thresholds of reliability,

355
00:16:37,660 --> 00:16:39,980
the implementation frictions could be very low.

356
00:16:39,980 --> 00:16:44,700
And do you expect, would AI have to get all the way there

357
00:16:44,700 --> 00:16:47,300
in order to substitute for a human worker?

358
00:16:47,300 --> 00:16:50,020
I mean, I would expect it to be a bit more gradual

359
00:16:50,020 --> 00:16:52,620
than that, taking over say 20% of tasks

360
00:16:52,620 --> 00:16:55,420
before 40% of tasks, 60% of tasks and so on.

361
00:16:55,420 --> 00:16:58,940
But here we're imagining that the AI kind of plugs in

362
00:16:58,940 --> 00:17:00,820
for the human worker for all tasks

363
00:17:00,820 --> 00:17:02,620
or what do you have in mind?

364
00:17:02,620 --> 00:17:04,740
These things are, yeah, you're right, much more continuous.

365
00:17:04,740 --> 00:17:07,380
It's not an on or off switch in part

366
00:17:07,380 --> 00:17:11,300
because the requisite threshold of reliability

367
00:17:11,300 --> 00:17:13,780
varies by the type of task.

368
00:17:13,780 --> 00:17:17,220
Arguably self-driving cars like Waymo or Tesla

369
00:17:17,220 --> 00:17:19,540
have matched human performance,

370
00:17:19,540 --> 00:17:21,940
but regulators want them to be 100x better than human

371
00:17:21,940 --> 00:17:26,780
before they're loose on the road because of safety.

372
00:17:26,780 --> 00:17:31,780
Codex and coding models are arguably still much worse today

373
00:17:31,780 --> 00:17:35,460
than elite programmers, but everyone is using them

374
00:17:35,460 --> 00:17:38,660
because even if it generates 50% of your code

375
00:17:38,660 --> 00:17:39,900
and you have to go back in and debug,

376
00:17:39,900 --> 00:17:42,580
it's still a huge productivity boost.

377
00:17:42,580 --> 00:17:45,580
So I think it will vary by occupation,

378
00:17:45,580 --> 00:17:48,780
by sort of task category, sort of modulo,

379
00:17:48,780 --> 00:17:52,780
the risks and stakes involved in those tasks.

380
00:17:52,780 --> 00:17:53,980
Yeah, I guess then the question is,

381
00:17:53,980 --> 00:17:56,540
how many of our jobs fall into the,

382
00:17:56,540 --> 00:17:58,060
is more like self-driving cars

383
00:17:58,060 --> 00:18:01,540
and how many of our jobs is more like programming?

384
00:18:01,540 --> 00:18:05,220
Right, I mean, I've been in a manager position before

385
00:18:05,220 --> 00:18:07,660
and I've had research assistants and interns

386
00:18:07,660 --> 00:18:10,700
and I know that they're like a very lossy compression

387
00:18:10,700 --> 00:18:13,340
of the thing I want to do.

388
00:18:13,340 --> 00:18:16,580
And so they require oversight and sort of co-poniting.

389
00:18:16,580 --> 00:18:18,220
We're sort of in that stage now with AIs

390
00:18:18,220 --> 00:18:20,900
in a variety of different kinds of tasks.

391
00:18:21,100 --> 00:18:25,020
I recently read a paper evaluating the use of GPT-4

392
00:18:25,020 --> 00:18:27,020
for peer review and science

393
00:18:27,020 --> 00:18:32,020
and it found that GPT-4 would write reviews of work

394
00:18:32,780 --> 00:18:34,660
that bore some striking correlations

395
00:18:34,660 --> 00:18:38,780
with the points raised by human reviewers,

396
00:18:38,780 --> 00:18:39,940
but also let some things out.

397
00:18:39,940 --> 00:18:43,420
And so it concluded by saying GPT-4

398
00:18:43,420 --> 00:18:47,500
could be an invaluable tool for scientific review,

399
00:18:47,500 --> 00:18:50,220
but it's not about to replace people.

400
00:18:50,220 --> 00:18:54,320
And that's just a case of like, okay, give it five years.

401
00:18:55,460 --> 00:18:57,940
Yeah, this is a phenomena you often see

402
00:18:57,940 --> 00:19:01,140
with some AI models out there

403
00:19:01,140 --> 00:19:04,700
and it has some capabilities, but lacks other capabilities.

404
00:19:04,700 --> 00:19:07,900
And then people might kind of over anchor

405
00:19:07,900 --> 00:19:09,900
on the present capabilities

406
00:19:09,900 --> 00:19:13,180
and not foresee the way the development is going.

407
00:19:13,180 --> 00:19:15,700
I think people are continually surprised

408
00:19:15,700 --> 00:19:17,860
at the advancement of AI.

409
00:19:17,860 --> 00:19:18,940
Yeah, absolutely.

410
00:19:18,940 --> 00:19:23,260
Ramiz Naam, the sci-fi author and futurist

411
00:19:23,260 --> 00:19:27,380
and energy investor, he gives his talk on solar energy

412
00:19:27,380 --> 00:19:30,780
and other renewables and he has this famous graph

413
00:19:30,780 --> 00:19:35,140
where he shows the International Energy Agency, the IEA.

414
00:19:35,140 --> 00:19:39,420
Every year they put out this projection of solar buildout

415
00:19:39,420 --> 00:19:41,700
and every year it's like a flat line,

416
00:19:41,700 --> 00:19:43,260
but it's like a flat line on an exponential,

417
00:19:43,260 --> 00:19:45,060
like the real curve is like going vertical

418
00:19:45,060 --> 00:19:46,660
and every year their projection is that it's just going

419
00:19:46,660 --> 00:19:50,780
flat-toe and I feel like people make that same mistake.

420
00:19:50,780 --> 00:19:54,500
And it sort of has this sort of ironic lesson,

421
00:19:54,500 --> 00:19:57,940
to the extent that we're drawing sort of parallels

422
00:19:57,940 --> 00:20:00,500
with the way our brain works and the way these models work,

423
00:20:00,500 --> 00:20:02,020
it seems like humans have a very strong

424
00:20:02,020 --> 00:20:03,420
autoregressive bias.

425
00:20:03,420 --> 00:20:04,460
So what's going on there?

426
00:20:04,460 --> 00:20:06,660
Is it an institutional problem

427
00:20:06,660 --> 00:20:09,420
or is it a psychological problem?

428
00:20:09,420 --> 00:20:14,020
Why is it that we can't project correctly in many cases?

429
00:20:14,860 --> 00:20:17,860
Well, to what I just said, I think it's probably both,

430
00:20:17,860 --> 00:20:19,580
but largely psychological, right?

431
00:20:19,580 --> 00:20:21,340
Our brains are evolved for, you know,

432
00:20:21,340 --> 00:20:23,580
hunter-gatherer societies that didn't really change

433
00:20:23,580 --> 00:20:28,580
over millennia and, you know, even the last 40, 50 years

434
00:20:29,180 --> 00:20:30,700
have been a period of relative stagnation

435
00:20:30,700 --> 00:20:33,060
where we have a lot of sort of pseudo-innovation.

436
00:20:33,060 --> 00:20:37,780
And so I think people are just a bit sort of disabused.

437
00:20:37,780 --> 00:20:40,140
Okay, you have some super interesting points

438
00:20:40,180 --> 00:20:43,660
about comparing the human brain,

439
00:20:43,660 --> 00:20:47,580
how the human brain works to how neural networks learn.

440
00:20:47,580 --> 00:20:51,820
What is universality in the context of brain learning

441
00:20:51,820 --> 00:20:53,860
and neural network learning?

442
00:20:53,860 --> 00:20:58,220
So universality is a term of our sort of refers to

443
00:20:58,220 --> 00:21:00,020
the fact that different neural networks

444
00:21:00,020 --> 00:21:03,500
independently trained, you know, even on different data

445
00:21:03,500 --> 00:21:06,940
will often converge on very similar representations

446
00:21:06,940 --> 00:21:09,580
in their embedding space of that data.

447
00:21:09,580 --> 00:21:13,300
And you can extend that to striking parallels

448
00:21:13,300 --> 00:21:15,220
or isomorphisms between the representations

449
00:21:15,220 --> 00:21:17,420
that artificial neural networks learn

450
00:21:17,420 --> 00:21:19,180
and that our brain appears to learn.

451
00:21:19,180 --> 00:21:21,500
Probably the area of the brain that's been studied the most

452
00:21:21,500 --> 00:21:23,180
is the visual cortex.

453
00:21:23,180 --> 00:21:27,420
And it seems to me as like a layperson

454
00:21:27,420 --> 00:21:30,260
that the broad consensus in neuroscience

455
00:21:30,260 --> 00:21:33,180
is that the visual cortex is very similar

456
00:21:33,180 --> 00:21:35,100
to a deep convolutional neural network.

457
00:21:35,100 --> 00:21:39,020
It's basically isomorphic to our artificial

458
00:21:39,020 --> 00:21:40,940
deep convolutional neural networks.

459
00:21:40,940 --> 00:21:45,020
And you train CCN on image data

460
00:21:45,020 --> 00:21:49,760
and our brain is trained on our sensory data.

461
00:21:49,760 --> 00:21:51,540
And it turns out they end up learning

462
00:21:51,540 --> 00:21:54,500
strikingly similar representations.

463
00:21:54,500 --> 00:21:55,920
And there are a few reasons for that, right?

464
00:21:55,920 --> 00:22:00,360
So, you know, one is sort of hierarchies of abstraction.

465
00:22:00,360 --> 00:22:03,500
It makes sense that early layers in a neural network

466
00:22:03,500 --> 00:22:06,220
will learn things like edges and simple shapes

467
00:22:06,220 --> 00:22:08,660
and only later in the only deeper in the network

468
00:22:08,700 --> 00:22:10,780
will you learn more subtle features.

469
00:22:10,780 --> 00:22:13,860
So there's that sort of sequencing part of it.

470
00:22:13,860 --> 00:22:15,620
And then there's also just the energy constraint.

471
00:22:15,620 --> 00:22:18,100
You know, gradient descent isn't costless, right?

472
00:22:18,100 --> 00:22:20,100
It requires energy, it requires a lot of energy.

473
00:22:20,100 --> 00:22:22,940
That's, you know, these data centers suck up a lot of energy.

474
00:22:22,940 --> 00:22:24,380
The same is true of our brain.

475
00:22:24,380 --> 00:22:27,580
You know, our brain consumes a lot of energy,

476
00:22:27,580 --> 00:22:29,380
like 25% of our calories.

477
00:22:29,380 --> 00:22:32,300
And especially when it's, and when we're young,

478
00:22:32,300 --> 00:22:34,460
there's a very strong metabolic cost

479
00:22:34,460 --> 00:22:36,280
associated with neural plasticity.

480
00:22:36,280 --> 00:22:37,900
Our brain being something shaped by evolution

481
00:22:37,940 --> 00:22:39,420
was obviously very energy conscious.

482
00:22:39,420 --> 00:22:43,580
And so those energy constraints greatly shrink the landscape

483
00:22:43,580 --> 00:22:47,900
of possible representations from sort of this infinite

484
00:22:47,900 --> 00:22:51,260
landscape of all the ways you could represent certain data

485
00:22:51,260 --> 00:22:54,140
to a much more manageable set of representations.

486
00:22:54,140 --> 00:22:56,820
And that doesn't guarantee that we'll converge

487
00:22:56,820 --> 00:22:58,980
on the same representations.

488
00:22:58,980 --> 00:23:02,140
At least suggestive of a weak universality

489
00:23:02,140 --> 00:23:04,900
where even when we don't have the exact same representations,

490
00:23:04,900 --> 00:23:06,900
they're often a coordinate transformation

491
00:23:06,900 --> 00:23:08,540
that play from each other.

492
00:23:08,540 --> 00:23:11,940
It's actually a bit surprising, so as you mentioned,

493
00:23:11,940 --> 00:23:13,220
when we train neural networks,

494
00:23:13,220 --> 00:23:15,300
we don't have the same energy constraints

495
00:23:15,300 --> 00:23:18,460
as the brain had during our evolution.

496
00:23:18,460 --> 00:23:21,380
And I would expect, again, from evolution,

497
00:23:21,380 --> 00:23:25,540
that the human brains have many more inbuilt biases

498
00:23:25,540 --> 00:23:27,300
and heuristics.

499
00:23:27,300 --> 00:23:29,580
But if we then compare the representations

500
00:23:29,580 --> 00:23:32,300
in a neural network to those in a human brain,

501
00:23:32,300 --> 00:23:33,980
we found that they are quite similar.

502
00:23:33,980 --> 00:23:36,580
Isn't that the whole point of universality?

503
00:23:36,580 --> 00:23:40,660
So does the neural network have the same heuristics

504
00:23:40,660 --> 00:23:44,020
and biases that we have, or what's going on here?

505
00:23:44,020 --> 00:23:48,700
Well, one of the primary biases in stochastic gradient

506
00:23:48,700 --> 00:23:52,620
descent is sometimes called a simplicity preference,

507
00:23:52,620 --> 00:23:55,980
basically an inductive bias for more parsimonious

508
00:23:55,980 --> 00:23:57,380
representations.

509
00:23:57,380 --> 00:24:00,900
Parsimonious in the sense of Occam's razor.

510
00:24:00,900 --> 00:24:04,660
And that's a byproduct of this information

511
00:24:04,660 --> 00:24:08,020
heuristic concept of Kolmogorff complexity,

512
00:24:08,020 --> 00:24:10,140
where Kolmogorff complexity means

513
00:24:10,140 --> 00:24:13,500
is measured by, is there a short program that

514
00:24:13,500 --> 00:24:16,140
can reproduce this longer sequence?

515
00:24:16,140 --> 00:24:18,060
And if you can find a short program that's

516
00:24:18,060 --> 00:24:20,460
sort of a more compact or more compressed way of representing

517
00:24:20,460 --> 00:24:22,340
it, and when you're under energy constraints,

518
00:24:22,340 --> 00:24:26,300
you're looking for those more compressed representations.

519
00:24:26,300 --> 00:24:28,500
And so that simplicity bias seems

520
00:24:28,500 --> 00:24:32,260
to be also the origin of generalization,

521
00:24:32,340 --> 00:24:36,900
of our ability to go beyond merely memorizing data,

522
00:24:36,900 --> 00:24:40,340
overfitting our parameters, to finding a simpler way of

523
00:24:40,340 --> 00:24:42,100
representing those parameters, right?

524
00:24:42,100 --> 00:24:45,860
Where we go from sort of fitting a bunch of data points

525
00:24:45,860 --> 00:24:47,820
to recognizing, oh, these data points are being generated

526
00:24:47,820 --> 00:24:50,700
by a sine function, so I can replace all these data points

527
00:24:50,700 --> 00:24:52,900
by a simple circuit for that sine function

528
00:24:52,900 --> 00:24:53,860
or something like that.

529
00:24:53,860 --> 00:24:55,980
What can we learn about AI progress

530
00:24:55,980 --> 00:24:59,460
when we consider the hard steps that humans

531
00:24:59,500 --> 00:25:03,660
and our ancestors have gone through in evolution?

532
00:25:03,660 --> 00:25:04,940
It's beyond evolution.

533
00:25:06,460 --> 00:25:07,700
This is often comes up in the discussion

534
00:25:07,700 --> 00:25:09,580
of the Fermi Paradox.

535
00:25:09,580 --> 00:25:12,500
Life on Earth to exist at all, let alone intelligent life,

536
00:25:12,500 --> 00:25:14,460
had to pass through many hard steps, right?

537
00:25:14,460 --> 00:25:17,340
We had to have a planet in a habitable zone.

538
00:25:18,220 --> 00:25:22,420
We had to have, you know, the right mix

539
00:25:22,420 --> 00:25:27,420
of organic chemicals in the Earth's crust and so forth.

540
00:25:27,580 --> 00:25:30,900
We had to have the conditions for abiogenesis,

541
00:25:30,900 --> 00:25:33,260
the emergence of the very earliest sort of

542
00:25:33,260 --> 00:25:35,420
non-living replicators, probably, you know,

543
00:25:35,420 --> 00:25:38,140
some kind of polymer type of crystal structure.

544
00:25:38,140 --> 00:25:39,380
Then we had to have, you know,

545
00:25:39,380 --> 00:25:43,940
the transition from single cell to multicellular organisms,

546
00:25:43,940 --> 00:25:46,300
to transition through the Cambrian explosion, right?

547
00:25:46,300 --> 00:25:47,980
Every one of these steps,

548
00:25:47,980 --> 00:25:50,980
you could think of as a very unlikely improbable thing.

549
00:25:50,980 --> 00:25:52,540
All the way up to, you know,

550
00:25:52,540 --> 00:25:55,260
the development of warm-blooded mammals

551
00:25:55,300 --> 00:25:59,820
and sort of social animals that were heavily selected for,

552
00:25:59,820 --> 00:26:04,820
for brain size, to then the sociocultural hard steps

553
00:26:04,940 --> 00:26:08,380
of like moving from small group primates

554
00:26:08,380 --> 00:26:13,380
to sort of settled technological cultures.

555
00:26:14,060 --> 00:26:15,900
Then, you know, technological hard steps,

556
00:26:15,900 --> 00:26:17,100
like the discovery of the printing press

557
00:26:17,100 --> 00:26:19,460
or the discovery of the transistor.

558
00:26:19,460 --> 00:26:21,460
You put those all together and life seems

559
00:26:21,460 --> 00:26:24,060
just incredibly unlikely.

560
00:26:24,060 --> 00:26:27,060
And, you know, this often goes to the point of view

561
00:26:27,060 --> 00:26:29,500
that, you know, creationists or intelligent designers

562
00:26:29,500 --> 00:26:31,220
would put forward.

563
00:26:31,220 --> 00:26:32,660
But then you zoom out and then you recognize,

564
00:26:32,660 --> 00:26:36,860
oh, wait, there are, you know, trillions of galaxies

565
00:26:36,860 --> 00:26:38,740
each with trillions, you know, hundreds of billions

566
00:26:38,740 --> 00:26:42,220
of stars and hundreds of trillions of planets.

567
00:26:42,220 --> 00:26:47,220
There's an awful lot of potential variation out there.

568
00:26:47,420 --> 00:26:50,820
And then meanwhile, every one of these hard steps

569
00:26:50,860 --> 00:26:55,460
seems characterized by a search problem that is very hard.

570
00:26:55,460 --> 00:26:58,140
But then once you find the correct thing,

571
00:26:58,140 --> 00:27:01,020
like the earliest self-replicator,

572
00:27:01,020 --> 00:27:03,180
things kind of take off, right?

573
00:27:03,180 --> 00:27:06,660
So you imagine that before the earliest self-replicator,

574
00:27:06,660 --> 00:27:09,820
there were millions or billions of attempts

575
00:27:09,820 --> 00:27:13,420
to self-replicate, like that didn't succeed.

576
00:27:13,420 --> 00:27:16,300
Yeah, it's just a huge search problem, right?

577
00:27:16,300 --> 00:27:19,060
And, you know, maybe there are more gradual intermediate

578
00:27:19,060 --> 00:27:22,980
stages where you have sort of, you know,

579
00:27:22,980 --> 00:27:25,260
everything in biology ends up looking way more gradual

580
00:27:25,260 --> 00:27:26,420
more you learn about it.

581
00:27:26,420 --> 00:27:29,620
But there are these phase transitions where you tip over

582
00:27:29,620 --> 00:27:30,980
and you get the Cambrian explosion

583
00:27:30,980 --> 00:27:33,380
or you get the printing press and the printing revolution.

584
00:27:33,380 --> 00:27:36,900
And so those hard steps end up looking relatively,

585
00:27:36,900 --> 00:27:39,060
they look more easy in retrospect

586
00:27:39,060 --> 00:27:40,380
because even though the search was hard,

587
00:27:40,380 --> 00:27:43,620
once you've tripped over the correct solution,

588
00:27:43,620 --> 00:27:46,660
there's sort of an autocatalytic self-reinforcing loop

589
00:27:46,660 --> 00:27:49,220
that pulls you into a new regime.

590
00:27:49,220 --> 00:27:53,260
And indeed, when you look at the emergence of life on Earth

591
00:27:53,260 --> 00:27:56,300
relative to the age of the universe,

592
00:27:57,220 --> 00:28:01,140
and Avi Lo with some co-authors have done this,

593
00:28:01,140 --> 00:28:03,460
life on Earth is incredibly early.

594
00:28:03,460 --> 00:28:06,820
Like, you know, the universe is 13.7 billion years old,

595
00:28:06,820 --> 00:28:11,220
but life couldn't emerge really much sooner.

596
00:28:11,220 --> 00:28:14,980
The reason being the universe started out as hot and dense,

597
00:28:14,980 --> 00:28:17,500
it had to cool down, stars had to form,

598
00:28:17,500 --> 00:28:19,180
those stars had to supernovae

599
00:28:19,180 --> 00:28:21,260
so they could produce the heavy elements

600
00:28:21,260 --> 00:28:23,740
that are essential to life.

601
00:28:23,740 --> 00:28:26,660
And then those solar systems had to then take shape

602
00:28:26,660 --> 00:28:28,780
and then had to further cool

603
00:28:28,780 --> 00:28:33,340
so the solar system wasn't being irradiated constantly.

604
00:28:33,340 --> 00:28:35,460
And when you put all those factors together,

605
00:28:35,460 --> 00:28:39,420
human life emerged basically as soon as it was possible

606
00:28:39,420 --> 00:28:40,900
for life to emerge anywhere.

607
00:28:40,900 --> 00:28:42,820
And so this is one way to answer the Fermi paradox

608
00:28:42,860 --> 00:28:45,380
that we're just in the first cohort, right?

609
00:28:45,380 --> 00:28:47,220
But it also should give you strong priors

610
00:28:47,220 --> 00:28:48,740
that passing through those hard steps

611
00:28:48,740 --> 00:28:50,700
isn't as hard as it looks.

612
00:28:50,700 --> 00:28:53,100
And what's the lesson for AI here?

613
00:28:53,100 --> 00:28:55,660
Developing AGI is sort of a hard step.

614
00:28:57,300 --> 00:29:00,300
We're doing this kind of gradient search

615
00:29:00,300 --> 00:29:04,260
for the right algorithms, for the right, what have you.

616
00:29:04,260 --> 00:29:06,740
And we seem to be now in a slow takeoff

617
00:29:06,740 --> 00:29:09,820
where we've figured out the core ingredients

618
00:29:09,820 --> 00:29:12,260
and there's now an autocatalytic process

619
00:29:12,300 --> 00:29:13,980
that's pulling us into a new phase.

620
00:29:13,980 --> 00:29:16,020
And what do you mean by autocatalytic?

621
00:29:16,020 --> 00:29:19,540
Suffering, forcing, once it gets started,

622
00:29:19,540 --> 00:29:24,540
it pulls itself, it sort of has an as if teleology, right?

623
00:29:24,820 --> 00:29:26,020
You see this in nature,

624
00:29:26,020 --> 00:29:28,860
but you also see this in capitalism.

625
00:29:29,780 --> 00:29:32,580
And you would expect us to get to advanced AI

626
00:29:32,580 --> 00:29:36,420
basically as soon as it's computationally possible.

627
00:29:37,300 --> 00:29:39,180
It basically seemed that way, right?

628
00:29:39,180 --> 00:29:41,380
Like, there was a kind of tacit collusion

629
00:29:41,420 --> 00:29:44,620
between Google and other players in the space

630
00:29:44,620 --> 00:29:48,300
to they had transformer models since 2017,

631
00:29:48,300 --> 00:29:50,140
but really, there's some of the precursors

632
00:29:50,140 --> 00:29:52,260
to transformers go back to the early 90s.

633
00:29:52,260 --> 00:29:55,140
But once you have this sort of profit opportunity

634
00:29:55,140 --> 00:29:57,420
that's in the background,

635
00:29:57,420 --> 00:29:59,500
it's hard in the competitive environment

636
00:29:59,500 --> 00:30:02,620
to stop an open AI from being like,

637
00:30:02,620 --> 00:30:05,540
oh, let's chase those profits.

638
00:30:05,540 --> 00:30:07,300
And then once that ball gets rolling,

639
00:30:07,300 --> 00:30:09,140
it's basically impossible to stop.

640
00:30:09,620 --> 00:30:14,020
This is why whatever the merits of the pause letter,

641
00:30:14,020 --> 00:30:17,740
it's virtually impossible to really have a pause

642
00:30:17,740 --> 00:30:21,940
in AI development because everything is sort of structured

643
00:30:21,940 --> 00:30:25,500
by these game theoretic incentives to just keep going faster.

644
00:30:25,500 --> 00:30:27,460
Once you've stumbled on the gold reserve,

645
00:30:27,460 --> 00:30:30,660
it's hard to keep the prospectors from running there.

646
00:30:30,660 --> 00:30:35,100
Samuel, is the US government prepared for advanced AI?

647
00:30:35,100 --> 00:30:35,940
No.

648
00:30:35,940 --> 00:30:41,140
No, I mean, where do I start?

649
00:30:41,140 --> 00:30:44,420
I mean, the US government, if you think of it

650
00:30:44,420 --> 00:30:47,820
from a firmware level, many countries have national IDs.

651
00:30:47,820 --> 00:30:48,900
The US doesn't have a national ID.

652
00:30:48,900 --> 00:30:50,140
We have social security numbers.

653
00:30:50,140 --> 00:30:52,100
There are these like nine digit numbers

654
00:30:52,100 --> 00:30:54,820
that date back to 1935.

655
00:30:54,820 --> 00:30:59,100
We have the core administrative laws

656
00:30:59,100 --> 00:31:01,980
date back to the early 40s.

657
00:31:01,980 --> 00:31:04,540
Much of our sort of technical infrastructure,

658
00:31:04,540 --> 00:31:08,980
like the system the IRS runs on,

659
00:31:08,980 --> 00:31:10,660
date back to the Kennedy administration

660
00:31:10,660 --> 00:31:12,860
and are written in assembly code.

661
00:31:12,860 --> 00:31:14,820
There's also been this general decline

662
00:31:14,820 --> 00:31:18,140
in what you could call state capacity, sort of the ability

663
00:31:18,140 --> 00:31:21,900
for the US government to execute on things.

664
00:31:21,900 --> 00:31:23,260
And you hear about this all the time.

665
00:31:23,260 --> 00:31:26,020
You hear about how the Golden Gate Bridge was built

666
00:31:26,020 --> 00:31:28,700
in four years or something like that.

667
00:31:28,700 --> 00:31:31,380
And now it takes like 10 years to build an access road.

668
00:31:31,420 --> 00:31:35,860
One of the reasons for that goes to what the legal scholar

669
00:31:35,860 --> 00:31:39,300
Nicholas Bagley has called the procedural fetish.

670
00:31:39,300 --> 00:31:43,260
Really, since the 70s, the machinery of the US government

671
00:31:43,260 --> 00:31:49,300
has shifted towards a reliance on explicit process.

672
00:31:49,300 --> 00:31:53,100
And proceduralism has pluses and minuses.

673
00:31:53,100 --> 00:31:56,500
If you have a clear process, government

674
00:31:56,500 --> 00:31:58,820
can kind of run an autopilot to an extent.

675
00:31:58,860 --> 00:32:01,820
But it also means you limit the room for discretion

676
00:32:01,820 --> 00:32:05,420
and you limit the flexibility of government to move quickly.

677
00:32:05,420 --> 00:32:07,820
And moreover, in our adversarial legal system,

678
00:32:07,820 --> 00:32:12,580
you also open up avenues for sort of continuous judicial review

679
00:32:12,580 --> 00:32:18,900
and legal challenge, where famously New York has taken

680
00:32:18,900 --> 00:32:21,900
over three years to approve congestion pricing

681
00:32:21,900 --> 00:32:23,900
on one of their bridges because that's

682
00:32:23,900 --> 00:32:25,140
to undergo environmental review.

683
00:32:25,140 --> 00:32:26,500
And people who don't want to pay the congestion price

684
00:32:26,500 --> 00:32:27,620
keep suing.

685
00:32:27,620 --> 00:32:29,580
Do you think having more procedures

686
00:32:29,580 --> 00:32:34,140
would make it easier for AI to interface with government?

687
00:32:34,140 --> 00:32:35,820
I would say having fewer procedures

688
00:32:35,820 --> 00:32:37,700
would make it easier for government to adapt.

689
00:32:37,700 --> 00:32:40,300
My assumption would be that having something written down,

690
00:32:40,300 --> 00:32:43,100
having a procedure for something would make it easier for AI

691
00:32:43,100 --> 00:32:45,580
to plug AI into that procedure.

692
00:32:45,580 --> 00:32:50,980
If it's less opaque and more kind of almost like an algorithm

693
00:32:50,980 --> 00:32:52,500
step by step.

694
00:32:52,500 --> 00:32:54,140
Yes.

695
00:32:54,180 --> 00:32:59,180
But the analogy I would give is to the Manhattan Project.

696
00:32:59,180 --> 00:33:02,340
The original Manhattan Project was run like a startup.

697
00:33:02,340 --> 00:33:05,700
You had Oppenheimer and General Leslie Groves sort

698
00:33:05,700 --> 00:33:10,340
of being the technical founder and the Type A

699
00:33:10,340 --> 00:33:11,940
get things done founder.

700
00:33:11,940 --> 00:33:13,820
And they broke all the rules.

701
00:33:13,820 --> 00:33:16,420
They pushed as hard as they could.

702
00:33:16,420 --> 00:33:20,620
They were managing at its peak like 100,000 people in secret.

703
00:33:20,620 --> 00:33:24,020
And they built the nuclear bomb in three years.

704
00:33:24,020 --> 00:33:27,020
And so the way we would do that today

705
00:33:27,020 --> 00:33:29,540
under procedural fetish framework

706
00:33:29,540 --> 00:33:32,660
would be to put out a bunch of request for proposals

707
00:33:32,660 --> 00:33:37,460
and have some kind of competitive bid.

708
00:33:37,460 --> 00:33:40,460
And then we'd probably get the lowest cost bid.

709
00:33:40,460 --> 00:33:41,940
And it would be Lockheed Martin.

710
00:33:41,940 --> 00:33:46,340
And they would build half an atom bomb.

711
00:33:46,340 --> 00:33:50,660
And it would take 20 years and five times the budget.

712
00:33:50,660 --> 00:33:53,020
And so that's sort of what I'm getting at.

713
00:33:53,060 --> 00:33:56,300
It's not about process versus discretion per se.

714
00:33:56,300 --> 00:33:59,500
It's about the way process hobbles and straight jackets

715
00:33:59,500 --> 00:34:01,980
are ability to adapt and sort of represents

716
00:34:01,980 --> 00:34:07,420
a kind of sclerosis, a kind of crystallized intelligence.

717
00:34:07,420 --> 00:34:12,660
We lay down the things that worked in the past as process

718
00:34:12,660 --> 00:34:15,700
and sort of freeze those processes in place,

719
00:34:15,700 --> 00:34:19,900
ossifying a particular modality.

720
00:34:19,900 --> 00:34:21,540
And when the motor production shifts

721
00:34:21,540 --> 00:34:23,980
and you need to completely tear up

722
00:34:23,980 --> 00:34:26,740
that process, root and branch, is very difficult.

723
00:34:26,740 --> 00:34:30,660
Because often there's no process for changing the process.

724
00:34:30,660 --> 00:34:33,380
Yeah, I wonder if there are lessons

725
00:34:33,380 --> 00:34:36,260
for how government will respond to AI

726
00:34:36,260 --> 00:34:38,300
and thinking about how governments responded

727
00:34:38,300 --> 00:34:42,340
to, say, historical technical innovations

728
00:34:42,340 --> 00:34:46,380
of a similar magnitude, like the Industrial Revolution

729
00:34:46,380 --> 00:34:50,380
or the printing press or maybe the internet computer.

730
00:34:50,420 --> 00:34:52,260
Do you think we can draw general lessons?

731
00:34:52,260 --> 00:34:55,860
Or is it so specific that we can't really

732
00:34:55,860 --> 00:34:58,180
extract information about the future from them?

733
00:34:58,180 --> 00:35:00,300
I think there are very powerful general lessons.

734
00:35:00,300 --> 00:35:01,860
I think one of the first general lessons

735
00:35:01,860 --> 00:35:05,060
is that every major technological transformation

736
00:35:05,060 --> 00:35:10,340
in human history has preceded a institutional transformation.

737
00:35:10,340 --> 00:35:13,300
Whether it's the shift from nomadic to settled city

738
00:35:13,300 --> 00:35:15,460
states with the agricultural revolution

739
00:35:15,460 --> 00:35:18,740
or the rise of modern nation states

740
00:35:18,740 --> 00:35:22,380
or the end of feudalism with the printing press

741
00:35:22,380 --> 00:35:25,020
to in the New Deal era, the sort of transition

742
00:35:25,020 --> 00:35:27,020
with industrialization from the kind of laissez-faire,

743
00:35:27,020 --> 00:35:30,780
classical liberal phase of 18th century America

744
00:35:30,780 --> 00:35:34,780
to an America with a robust welfare state

745
00:35:34,780 --> 00:35:37,460
and administrative bureaucracies and really

746
00:35:37,460 --> 00:35:40,140
in all new constitutional order.

747
00:35:40,140 --> 00:35:42,940
And so there's sort of better and worse ways

748
00:35:42,940 --> 00:35:44,260
for this transition to happen.

749
00:35:44,260 --> 00:35:46,540
There's sort of the internal regime change model.

750
00:35:46,540 --> 00:35:50,260
And you can think of Abraham Lincoln or FDR

751
00:35:50,260 --> 00:35:54,540
as inaugurating a new republic, a new American republic.

752
00:35:54,540 --> 00:35:56,900
Or there's a scenario where we don't change

753
00:35:56,900 --> 00:35:59,820
because we're too crystallized and sort of like an innovator's

754
00:35:59,820 --> 00:36:02,380
dilemma get displaced by some new upstart.

755
00:36:02,380 --> 00:36:04,980
And there are different countries have different abilities

756
00:36:04,980 --> 00:36:08,820
and different sort of capacities for that internal adaptation.

757
00:36:08,820 --> 00:36:11,340
As a Canadian, I'm a big fan of Westminster-style

758
00:36:11,340 --> 00:36:12,260
parliamentary systems.

759
00:36:12,260 --> 00:36:14,660
And one of the reasons is because it's

760
00:36:14,660 --> 00:36:17,460
very easy for parliamentary systems

761
00:36:17,460 --> 00:36:20,500
to shut down ministries, open up new ministries,

762
00:36:20,500 --> 00:36:24,820
to reorganize the civil service because it's sort

763
00:36:24,820 --> 00:36:27,660
of vertically integrated under the Prime Minister's office

764
00:36:27,660 --> 00:36:29,180
or what have you.

765
00:36:29,180 --> 00:36:32,620
In the US, it's much worse because given

766
00:36:32,620 --> 00:36:36,020
the separation of powers, Congress and the executive

767
00:36:36,020 --> 00:36:42,860
are often not working well together as an understatement.

768
00:36:42,900 --> 00:36:45,500
But then moreover, the different federal agencies

769
00:36:45,500 --> 00:36:47,500
have it sort of a life of their own.

770
00:36:47,500 --> 00:36:49,820
Often they're self-funded and all these other things

771
00:36:49,820 --> 00:36:51,740
that make it very difficult to reform.

772
00:36:51,740 --> 00:36:53,580
Do you think Canada responded better

773
00:36:53,580 --> 00:36:56,260
to the rise of the internet than the US, for example?

774
00:36:56,260 --> 00:36:57,980
Isn't there something wrong with the story

775
00:36:57,980 --> 00:36:59,940
because the US kind of birthed the internet

776
00:36:59,940 --> 00:37:04,060
and Canada adopted the internet from the US?

777
00:37:04,060 --> 00:37:07,460
Let's compare, first of all, the impact of the internet

778
00:37:07,460 --> 00:37:10,540
on weaker states because Canada and the US

779
00:37:10,700 --> 00:37:15,220
are similar or sort of in one quadrant.

780
00:37:15,220 --> 00:37:16,860
They have differences, but the differences

781
00:37:16,860 --> 00:37:18,580
are small compared to other countries.

782
00:37:18,580 --> 00:37:20,860
If you think about internet safety discussions that

783
00:37:20,860 --> 00:37:23,020
would have been taking place in the early 2000s,

784
00:37:23,020 --> 00:37:25,340
people would have been talking about identity theft,

785
00:37:25,340 --> 00:37:28,940
credit card theft, child exploitation, these kind

786
00:37:28,940 --> 00:37:32,620
of direct first order potential harms from the internet.

787
00:37:32,620 --> 00:37:38,220
They didn't foresee that concurrent with the rise of mobile

788
00:37:38,220 --> 00:37:41,660
and social media that the internet would enable tools

789
00:37:41,660 --> 00:37:43,460
for mass mobilization simultaneous

790
00:37:43,460 --> 00:37:45,780
with a kind of legitimacy crisis where

791
00:37:45,780 --> 00:37:50,100
the sort of new transparency and information access

792
00:37:50,100 --> 00:37:52,620
that the internet provided eroded trust and government

793
00:37:52,620 --> 00:37:54,220
and trust in other institutions.

794
00:37:54,220 --> 00:37:57,100
So you have these two forces interacting,

795
00:37:57,100 --> 00:37:59,660
the internet exposing government and exposing corruption

796
00:37:59,660 --> 00:38:02,900
and leading to a decline in trust while also creating

797
00:38:02,900 --> 00:38:06,620
a platform for people to rise up and mobilize against that

798
00:38:06,620 --> 00:38:07,580
corruption.

799
00:38:07,620 --> 00:38:09,500
And it's something that kind of rhymes

800
00:38:09,500 --> 00:38:11,940
with the printing press and the printing revolution

801
00:38:11,940 --> 00:38:16,340
where you had these sort of dormant suppressed minority

802
00:38:16,340 --> 00:38:19,140
groups like the Puritans or the Presbyterians,

803
00:38:19,140 --> 00:38:22,700
the nonconformists, and with the collapse

804
00:38:22,700 --> 00:38:26,420
of the censorship printing licensing regime.

805
00:38:26,420 --> 00:38:30,540
They actually had a licensing regime in the UK parliament

806
00:38:30,540 --> 00:38:32,060
back circa 1630.

807
00:38:33,820 --> 00:38:35,260
That licensing regime collapsed,

808
00:38:35,260 --> 00:38:37,260
there I think 1634 or something around there,

809
00:38:37,260 --> 00:38:40,820
and that was like five years before the English Civil War.

810
00:38:40,820 --> 00:38:43,620
And you see something like this in the Arab Spring

811
00:38:43,620 --> 00:38:47,980
where the internet quite directly led

812
00:38:47,980 --> 00:38:51,780
to mass mobilization in Cairo and Tunisia and elsewhere

813
00:38:51,780 --> 00:38:53,380
and led to actual regime change,

814
00:38:53,380 --> 00:38:56,220
in some cases sort of temporary state collapse.

815
00:38:56,220 --> 00:38:57,820
And that's because those were weaker states

816
00:38:57,820 --> 00:38:59,740
that hadn't democratized,

817
00:38:59,740 --> 00:39:02,620
that hadn't sort of had their own information revolution

818
00:39:03,580 --> 00:39:05,660
earlier in their history the way we did, right?

819
00:39:05,740 --> 00:39:07,180
In some ways like the American Republic

820
00:39:07,180 --> 00:39:11,740
is sort of a founder country built on the backbone

821
00:39:11,740 --> 00:39:13,420
of the printing revolution.

822
00:39:13,420 --> 00:39:17,500
So we were a little bit more robust to that

823
00:39:17,500 --> 00:39:19,700
because it's sort of part of our ethos

824
00:39:19,700 --> 00:39:22,980
to have this open disagreeable society.

825
00:39:22,980 --> 00:39:26,660
But clearly the internet has also affected

826
00:39:26,660 --> 00:39:28,940
the legitimacy of Western democracies.

827
00:39:28,940 --> 00:39:31,940
I think it's clear, clearly one of the major inputs

828
00:39:31,940 --> 00:39:34,420
in sort of rising populism,

829
00:39:34,420 --> 00:39:36,860
the mass mobilizations that we see,

830
00:39:36,860 --> 00:39:41,660
whether in the US context, the 2020 racial awakening

831
00:39:41,660 --> 00:39:45,340
or the January 6th sort of peasant rebellion, right?

832
00:39:45,340 --> 00:39:49,540
These sort of look like the kind of color revolutions

833
00:39:49,540 --> 00:39:51,420
that we see abroad.

834
00:39:51,420 --> 00:39:54,100
And some people want to ascribe conspiracy theories

835
00:39:54,100 --> 00:39:56,260
to that, I think there's a simpler explanation,

836
00:39:56,260 --> 00:39:58,980
which is that people will self-organize

837
00:39:58,980 --> 00:40:00,340
with the right tools.

838
00:40:00,340 --> 00:40:01,740
Our state hasn't collapsed yet,

839
00:40:02,580 --> 00:40:06,820
but there's clearly a lot of cracks in the foundation,

840
00:40:06,820 --> 00:40:07,740
if you will.

841
00:40:07,740 --> 00:40:09,460
Is it, would it be fair to say that the main lesson

842
00:40:09,460 --> 00:40:12,340
for you from history is that technological change

843
00:40:12,340 --> 00:40:14,740
brings institutional change?

844
00:40:14,740 --> 00:40:16,900
Yeah, not necessarily one for one.

845
00:40:16,900 --> 00:40:20,580
I'm not kind of a vulgar Marxist on this, but yes.

846
00:40:20,580 --> 00:40:24,020
And the reason for that is because institutions themselves

847
00:40:24,020 --> 00:40:27,220
exist due to a certain cost structure.

848
00:40:27,220 --> 00:40:29,100
And if you have general purpose technologies

849
00:40:29,100 --> 00:40:31,220
that dramatically change the nature

850
00:40:31,220 --> 00:40:33,460
of that cost structure, then institutional change will follow.

851
00:40:33,460 --> 00:40:34,980
Yeah, and I think we want to get to that.

852
00:40:34,980 --> 00:40:38,180
But before we do, I think we should discuss AI's impact

853
00:40:38,180 --> 00:40:39,860
on the broader economy.

854
00:40:39,860 --> 00:40:42,820
So not just the government, but the economy in general.

855
00:40:42,820 --> 00:40:47,820
Economists have this fallacy they point out often,

856
00:40:48,060 --> 00:40:49,500
the lump of labor fallacy.

857
00:40:49,500 --> 00:40:50,900
Maybe you could explain that.

858
00:40:50,900 --> 00:40:52,780
The lump of labor fallacy is essentially the idea

859
00:40:52,780 --> 00:40:55,060
that there's a fixed amount of work to be done.

860
00:40:55,060 --> 00:40:58,740
If you were thinking about the Industrial Revolution

861
00:40:58,740 --> 00:41:00,300
and what would happen to the 50% of people

862
00:41:00,300 --> 00:41:02,580
who are in agriculture, you couldn't imagine

863
00:41:02,580 --> 00:41:04,060
the new jobs that would be created.

864
00:41:04,060 --> 00:41:05,540
But new jobs were created.

865
00:41:05,540 --> 00:41:10,060
And the reason is because human wants are infinite.

866
00:41:10,060 --> 00:41:13,820
And so demand will always fill supply.

867
00:41:14,980 --> 00:41:16,980
The second reason is because there's a kind of circular flow

868
00:41:16,980 --> 00:41:19,660
in the economy where one person's cost

869
00:41:19,660 --> 00:41:21,300
is another person's income.

870
00:41:21,300 --> 00:41:23,420
Society would collapse if we had true technological

871
00:41:23,420 --> 00:41:26,900
unemployment because there'd be things being produced

872
00:41:26,900 --> 00:41:28,300
but no one to pay for them.

873
00:41:29,300 --> 00:41:31,380
And so that ends up kind of bootstrapping new industries

874
00:41:31,380 --> 00:41:33,700
and new sources of production.

875
00:41:33,700 --> 00:41:36,500
There's still this open question, is this time different?

876
00:41:36,500 --> 00:41:39,540
Yeah, that's exactly what I wanna know.

877
00:41:39,540 --> 00:41:42,820
Because for me, it's in retrospect, let's say.

878
00:41:42,820 --> 00:41:46,700
It's easy to see how workers could move from fields

879
00:41:46,700 --> 00:41:49,460
to factories into offices.

880
00:41:49,460 --> 00:41:52,020
But if we have truly general AI,

881
00:41:52,020 --> 00:41:54,700
it's difficult for me to see where workers would move.

882
00:41:54,700 --> 00:41:58,380
Especially if we have also functional robots

883
00:41:58,380 --> 00:42:03,380
and perhaps AIs that are better at taking care of people

884
00:42:03,620 --> 00:42:05,620
than other people are.

885
00:42:06,780 --> 00:42:08,860
I'm not asking you to predict specific jobs,

886
00:42:08,860 --> 00:42:11,260
but I'm asking you whether you think

887
00:42:11,260 --> 00:42:14,700
this historical trend will hold with the advent

888
00:42:14,700 --> 00:42:16,540
of advanced AI.

889
00:42:16,540 --> 00:42:19,220
You know, the first thing to say is, you know,

890
00:42:19,220 --> 00:42:23,060
when Keynes wrote economic possibilities

891
00:42:23,060 --> 00:42:26,780
for our grandchildren, a famous text where he predicted

892
00:42:26,780 --> 00:42:28,380
that technological progress would lead

893
00:42:28,380 --> 00:42:31,860
to the growth of a leisure society.

894
00:42:31,860 --> 00:42:35,740
And this was in the 1930s, yeah.

895
00:42:35,740 --> 00:42:39,180
You know, people have dismissed him as being wrong,

896
00:42:39,180 --> 00:42:41,780
but actually you look at time use data

897
00:42:41,780 --> 00:42:44,460
and employment data and people are working less.

898
00:42:44,460 --> 00:42:47,380
You know, it's not, it didn't match his,

899
00:42:47,380 --> 00:42:49,860
the optimism of his projection, right?

900
00:42:49,860 --> 00:42:54,140
Because it turns out, you know, maybe if we fixed

901
00:42:54,140 --> 00:42:56,660
living standards at what he expected,

902
00:42:56,660 --> 00:43:00,100
people want more and people will work more for more.

903
00:43:00,100 --> 00:43:02,180
But overall, people are working less.

904
00:43:02,180 --> 00:43:03,940
People do have more leisure.

905
00:43:03,940 --> 00:43:06,860
We've sort of moved to a de facto four-day work week.

906
00:43:06,860 --> 00:43:10,220
So there's one world where rapid technological progress

907
00:43:10,220 --> 00:43:14,460
sort of continues that trend and we all work less.

908
00:43:14,460 --> 00:43:15,860
It's sort of a technological unemployment

909
00:43:15,860 --> 00:43:19,700
that's spread across people and is enabled in part

910
00:43:19,700 --> 00:43:23,700
because in a world of AGI, maybe you only have to work,

911
00:43:23,700 --> 00:43:27,820
you know, a few hours a day to make $100,000 a year.

912
00:43:27,820 --> 00:43:31,820
There's another possibility which is that,

913
00:43:31,820 --> 00:43:35,460
well, AGI could in principle be a perfect emulation

914
00:43:35,460 --> 00:43:38,140
of humans on specific tasks.

915
00:43:38,140 --> 00:43:42,140
It can't emulate the historical formation of that person.

916
00:43:42,140 --> 00:43:43,060
Right?

917
00:43:43,060 --> 00:43:46,780
So what I mean by that is if you had a perfect

918
00:43:46,820 --> 00:43:49,860
Adam by Adam replication of the Mona Lisa,

919
00:43:50,820 --> 00:43:53,700
it wouldn't sell at auction, right?

920
00:43:53,700 --> 00:43:57,500
Because people aren't just buying the physical substrate,

921
00:43:57,500 --> 00:44:02,500
they're also buying the kind of world line of that thing.

922
00:44:03,140 --> 00:44:07,260
And that's clearly the case in humans as well.

923
00:44:07,260 --> 00:44:09,700
Like there are certain, you know, talking heads

924
00:44:09,700 --> 00:44:13,500
that I go and enjoy not because they are the smartest

925
00:44:13,500 --> 00:44:15,260
or would have you because I'm interested

926
00:44:15,260 --> 00:44:17,420
in what that person thinks on this

927
00:44:17,420 --> 00:44:18,700
because they have a particular personality,

928
00:44:18,700 --> 00:44:19,980
a particular world line.

929
00:44:19,980 --> 00:44:24,980
And then the third factor is sort of artificial scarcity.

930
00:44:25,700 --> 00:44:26,540
Right?

931
00:44:26,540 --> 00:44:31,540
And so even in a world with abundance and supply

932
00:44:31,940 --> 00:44:34,180
in services and goods, there are still things

933
00:44:34,180 --> 00:44:35,940
that will be intrinsically scarce,

934
00:44:35,940 --> 00:44:38,580
real estate being probably the canonical thing,

935
00:44:38,580 --> 00:44:40,620
but also energy and commodities and so forth.

936
00:44:40,620 --> 00:44:42,340
And the reason real estate is intrinsically scarce

937
00:44:42,420 --> 00:44:46,180
because people want to live near other people

938
00:44:46,180 --> 00:44:51,180
and people want to live in particular areas of a city.

939
00:44:51,180 --> 00:44:53,540
They want to live in the posh part of town, right?

940
00:44:53,540 --> 00:44:54,820
And those are positional goods.

941
00:44:54,820 --> 00:44:58,260
We can't all live in the trendy loft.

942
00:44:58,260 --> 00:45:00,500
So that builds in a kind of artificial scarcity.

943
00:45:00,500 --> 00:45:04,100
And so people will still be competing over those things.

944
00:45:04,100 --> 00:45:05,860
This is sort of related to artificial scarcity,

945
00:45:05,860 --> 00:45:07,900
but there's also sort of break it out

946
00:45:07,900 --> 00:45:10,940
into a fourth possibility, which are sort of tournaments

947
00:45:10,940 --> 00:45:13,060
and things that are structured as tournaments.

948
00:45:13,060 --> 00:45:15,180
Having chess spots that are strictly better

949
00:45:15,180 --> 00:45:19,860
than humans at chess hasn't killed people playing chess.

950
00:45:19,860 --> 00:45:22,100
If anything, more people play chess today

951
00:45:22,100 --> 00:45:23,460
than they've had in human history.

952
00:45:23,460 --> 00:45:25,500
Yeah, it's more popular than ever.

953
00:45:25,500 --> 00:45:26,340
Yeah.

954
00:45:26,340 --> 00:45:29,020
And the reason is because people like to watch

955
00:45:29,020 --> 00:45:31,740
other humans playing and also they're structured

956
00:45:31,740 --> 00:45:33,620
as sort of zero sum tournaments

957
00:45:33,620 --> 00:45:35,980
where there can only be the best human.

958
00:45:35,980 --> 00:45:37,460
You look at other things that have been created

959
00:45:37,500 --> 00:45:41,780
just in the last 15, 20 years, the X games, right?

960
00:45:41,780 --> 00:45:43,900
I think people will still want to watch other people

961
00:45:43,900 --> 00:45:45,660
do the Olympics or do motocross

962
00:45:45,660 --> 00:45:46,900
and all these other things.

963
00:45:46,900 --> 00:45:49,540
And so maybe more of our life shifts into,

964
00:45:49,540 --> 00:45:53,220
both maybe greater leisure on the one hand,

965
00:45:53,220 --> 00:45:56,180
more competition over positional goods

966
00:45:56,180 --> 00:46:01,180
and more production that is structured as a tournament.

967
00:46:01,540 --> 00:46:03,580
Yeah, yeah, I can see many of those points.

968
00:46:03,580 --> 00:46:06,700
I'm just thinking, again, with fully general AI,

969
00:46:06,740 --> 00:46:08,580
you would be able to generate

970
00:46:08,580 --> 00:46:12,100
a much more interesting person playing chess

971
00:46:12,100 --> 00:46:15,340
or at least a simulation of a very charismatic

972
00:46:15,340 --> 00:46:17,860
and interesting human chess player.

973
00:46:17,860 --> 00:46:20,940
Why wouldn't people watch that chess player

974
00:46:20,940 --> 00:46:24,660
as opposed to the best human?

975
00:46:26,180 --> 00:46:29,700
Maybe they will, sorry to know.

976
00:46:29,700 --> 00:46:32,180
The question is who's producing that video stream

977
00:46:32,180 --> 00:46:36,060
because you still need the human behind it

978
00:46:37,020 --> 00:46:38,980
that had the idea, right?

979
00:46:38,980 --> 00:46:42,260
And you could imagine people being dishonest

980
00:46:42,260 --> 00:46:45,740
about the history of this chess player.

981
00:46:45,740 --> 00:46:50,420
The simulated chess player could be a fully digital,

982
00:46:50,420 --> 00:46:52,740
fully fictional, so to speak,

983
00:46:52,740 --> 00:46:55,140
and just pretending to be human.

984
00:46:55,140 --> 00:46:57,380
Right, so they could fool people.

985
00:46:57,380 --> 00:46:58,700
That's the case too.

986
00:46:58,700 --> 00:47:00,260
No, I can't rule that out,

987
00:47:00,260 --> 00:47:01,900
but I would just say that

988
00:47:01,900 --> 00:47:04,100
however that person is monetizing,

989
00:47:04,140 --> 00:47:06,540
their deep fake chess player,

990
00:47:06,540 --> 00:47:08,260
they're making money, which they're then spending back

991
00:47:08,260 --> 00:47:10,740
into the economy, and so they'll produce jobs somewhere.

992
00:47:10,740 --> 00:47:13,340
Do you think more people will move into,

993
00:47:13,340 --> 00:47:17,340
say, people-focused industries like nursing and teaching?

994
00:47:17,340 --> 00:47:22,340
Is that a possible way for us to maintain jobs?

995
00:47:22,740 --> 00:47:24,820
Maybe nursing, at least in the short run.

996
00:47:26,020 --> 00:47:28,140
I'm not very long on education

997
00:47:28,140 --> 00:47:31,780
being labor-intensive for much longer.

998
00:47:31,780 --> 00:47:33,740
But you don't think education,

999
00:47:33,740 --> 00:47:36,220
at least say, great school education,

1000
00:47:36,220 --> 00:47:40,460
is that really about teaching people or conveying knowledge?

1001
00:47:40,460 --> 00:47:42,980
Or to what extent is it about conveying knowledge?

1002
00:47:42,980 --> 00:47:45,740
And to what extent is it about the social interaction

1003
00:47:45,740 --> 00:47:50,740
and specializing your teaching to the individual student?

1004
00:47:52,020 --> 00:47:55,260
Well, AI is very good at customization

1005
00:47:55,260 --> 00:47:57,020
and sort of mastery tutoring.

1006
00:47:57,020 --> 00:47:59,420
Education is a bundle of things.

1007
00:47:59,420 --> 00:48:03,980
And for younger ages, it's also daycare.

1008
00:48:03,980 --> 00:48:06,180
It is socialization, like you said.

1009
00:48:06,180 --> 00:48:10,820
At the very least, it's just a reorganization

1010
00:48:10,820 --> 00:48:12,060
of the division of labor,

1011
00:48:12,060 --> 00:48:14,740
because the types of teachers that you would select

1012
00:48:14,740 --> 00:48:18,060
or hire for may differ if the education component

1013
00:48:18,060 --> 00:48:20,340
of that bundle is being done by AI.

1014
00:48:20,340 --> 00:48:22,060
Maybe you select for people who are,

1015
00:48:22,060 --> 00:48:23,700
maybe don't have any subject matter expertise,

1016
00:48:23,700 --> 00:48:27,020
but are just highly conscientious and go to around kids.

1017
00:48:27,060 --> 00:48:30,260
Or maybe you one bundle from public education altogether,

1018
00:48:30,260 --> 00:48:34,220
and it re-bundles around a jujitsu school

1019
00:48:34,220 --> 00:48:38,300
or a chess academy, because you'll have the AI tutor

1020
00:48:38,300 --> 00:48:39,140
that will teach you math,

1021
00:48:39,140 --> 00:48:41,580
but you'll still want to grapple with the human.

1022
00:48:41,580 --> 00:48:42,500
Yeah, yeah.

1023
00:48:42,500 --> 00:48:45,740
What about industries with occupational licensings

1024
00:48:45,740 --> 00:48:47,580
like law or medicine?

1025
00:48:47,580 --> 00:48:51,620
Will they be able to keep up their quite high wages

1026
00:48:51,620 --> 00:48:56,380
in the face of AI being able to be a pretty good doctor

1027
00:48:56,420 --> 00:48:58,100
and a pretty good lawyer?

1028
00:48:58,100 --> 00:49:01,660
It's easy to solve for the long-term equilibrium.

1029
00:49:01,660 --> 00:49:02,500
With the rise of the internet,

1030
00:49:02,500 --> 00:49:06,500
you can do a comparison of the wage distribution

1031
00:49:06,500 --> 00:49:08,700
for lawyers pre and post internet.

1032
00:49:08,700 --> 00:49:11,220
And, you know, circa the early 90s,

1033
00:49:11,220 --> 00:49:13,260
lawyer incomes were normally distributed

1034
00:49:13,260 --> 00:49:15,180
around $60,000 a year.

1035
00:49:15,180 --> 00:49:18,940
You know, after in the 2000s, they become bimodal.

1036
00:49:18,940 --> 00:49:20,900
And so you have one mode that's still around

1037
00:49:20,900 --> 00:49:23,860
that $60,000 range, those are like the family lawyers.

1038
00:49:23,860 --> 00:49:25,180
And then you have this other mode

1039
00:49:25,180 --> 00:49:27,060
that's into the six figures.

1040
00:49:27,060 --> 00:49:28,900
And those are like big law, right?

1041
00:49:28,900 --> 00:49:31,420
It's the emergence of these law firms

1042
00:49:31,420 --> 00:49:32,620
where you have a few partners on top

1043
00:49:32,620 --> 00:49:35,340
and maybe hundreds of associates

1044
00:49:35,340 --> 00:49:37,420
who are doing kind of grant work using Westlaw

1045
00:49:37,420 --> 00:49:40,980
and Lexus Nexus and these other legal search engines

1046
00:49:40,980 --> 00:49:45,780
to accelerate drafting and legal analysis.

1047
00:49:45,780 --> 00:49:47,660
So if that pattern repeats,

1048
00:49:47,660 --> 00:49:52,660
I could imagine these various high-skill knowledge sectors

1049
00:49:53,660 --> 00:49:57,700
to also become bimodal where in the short run,

1050
00:49:57,700 --> 00:50:00,660
AI serves as a co-pilot, sort of like Westlaw

1051
00:50:00,660 --> 00:50:03,380
or Lexus Nexus was for legal research

1052
00:50:03,380 --> 00:50:06,140
and enables the kind of 100x lawyer.

1053
00:50:06,140 --> 00:50:10,340
And so there's a kind of averages over dynamic.

1054
00:50:10,340 --> 00:50:15,340
The longer run, you know, you start to see the possibility

1055
00:50:15,740 --> 00:50:20,020
of doing an end run around existing accreditation

1056
00:50:20,020 --> 00:50:22,460
and licensing monopolies

1057
00:50:22,500 --> 00:50:26,220
where obviously the American Medical Association

1058
00:50:26,220 --> 00:50:31,220
and medical boards will be highly resistant to an AI doctor.

1059
00:50:31,580 --> 00:50:33,620
I tend to think that they'll probably end up self cannibalizing

1060
00:50:33,620 --> 00:50:36,060
because the value prop is so great

1061
00:50:36,060 --> 00:50:39,460
even for doctors to do simple things

1062
00:50:39,460 --> 00:50:42,940
like automate insurance paperwork and stuff like that.

1063
00:50:42,940 --> 00:50:44,740
But to the extent that there is a resistance,

1064
00:50:44,740 --> 00:50:47,700
to the extent that in 10 years there's still a requirement

1065
00:50:47,700 --> 00:50:51,940
that you must have the doctor prescribe the treatment

1066
00:50:51,940 --> 00:50:53,020
or refer you to a specialist,

1067
00:50:53,020 --> 00:50:55,100
even though the AI is doing all the work

1068
00:50:55,100 --> 00:50:57,780
and they're just sort of like the elevator person

1069
00:50:57,780 --> 00:51:01,140
that's like actually just pushing the button for you.

1070
00:51:01,140 --> 00:51:03,060
It'll be very easy to end run that

1071
00:51:03,060 --> 00:51:07,460
because AI is both transforming the task itself

1072
00:51:07,460 --> 00:51:10,740
but also transforming it's the means of distribution.

1073
00:51:10,740 --> 00:51:13,260
And if you can go to GPT-4 and ask for,

1074
00:51:13,260 --> 00:51:15,340
put in your blood work and get a diagnosis,

1075
00:51:16,420 --> 00:51:18,180
but no regulator's going to stop that, right?

1076
00:51:18,180 --> 00:51:20,140
And so, you know, GPT-4 becomes sort of

1077
00:51:20,140 --> 00:51:22,220
the ultimate doctor of up orders.

1078
00:51:22,220 --> 00:51:25,180
You write a lot about transaction costs

1079
00:51:25,180 --> 00:51:28,860
and how changes in transaction costs

1080
00:51:28,860 --> 00:51:31,340
change institutional structures.

1081
00:51:31,340 --> 00:51:33,620
First of all, what are transaction costs

1082
00:51:33,620 --> 00:51:36,380
and how do you think they'll be affected by AI?

1083
00:51:36,380 --> 00:51:38,500
So transaction cost is sort of an umbrella term

1084
00:51:38,500 --> 00:51:42,780
for different kinds of costs associated with market exchange.

1085
00:51:42,780 --> 00:51:45,620
And this goes back to Ronald Coase's famous paper

1086
00:51:45,620 --> 00:51:48,300
on the theory of the firm where he asked the question,

1087
00:51:48,300 --> 00:51:50,540
why do we have corporations in the first place?

1088
00:51:50,540 --> 00:51:51,860
If free markets are so great,

1089
00:51:51,860 --> 00:51:56,140
why don't we just go up and spot contract for everything?

1090
00:51:56,140 --> 00:51:59,780
And the answer is, well, market exchange itself has a cost.

1091
00:51:59,780 --> 00:52:01,100
There's the cost of monitoring.

1092
00:52:01,100 --> 00:52:02,220
You know, if you hire a contractor,

1093
00:52:02,220 --> 00:52:04,380
you don't know exactly what they're doing.

1094
00:52:04,380 --> 00:52:05,820
There's the cost of bargaining.

1095
00:52:05,820 --> 00:52:08,860
You know, having to haggle with a taxi cab driver

1096
00:52:08,860 --> 00:52:11,700
is a friction.

1097
00:52:11,700 --> 00:52:12,940
And there's the cost of searching,

1098
00:52:12,940 --> 00:52:14,780
the associate of searching information.

1099
00:52:14,780 --> 00:52:16,620
So taking those three things together,

1100
00:52:16,620 --> 00:52:18,020
they're not all that companies do,

1101
00:52:18,020 --> 00:52:20,900
but they structure the boundary of the corporation.

1102
00:52:20,900 --> 00:52:22,780
They explain why some things are done in-house

1103
00:52:22,780 --> 00:52:24,940
and some things are done through contracts.

1104
00:52:24,940 --> 00:52:26,380
If there's high monitoring costs,

1105
00:52:26,380 --> 00:52:27,860
you want to pull that part of the production

1106
00:52:27,860 --> 00:52:30,900
into the company so that you can monitor

1107
00:52:30,900 --> 00:52:33,540
and manage the people doing the production.

1108
00:52:33,540 --> 00:52:35,740
And some of the same effects go for

1109
00:52:35,740 --> 00:52:38,340
the existence of governments, right?

1110
00:52:38,340 --> 00:52:40,660
Yes, because governments, you know,

1111
00:52:40,660 --> 00:52:41,860
with a certain gestalt,

1112
00:52:41,860 --> 00:52:43,780
governments and corporations aren't that different.

1113
00:52:43,780 --> 00:52:45,380
There are kinds of institutional structures

1114
00:52:45,380 --> 00:52:46,740
that pull certain things in-house

1115
00:52:46,740 --> 00:52:51,220
and certain things are left for contracting or outsourced.

1116
00:52:51,220 --> 00:52:54,700
And, you know, you even see sort of different kinds

1117
00:52:54,700 --> 00:52:57,060
of governments having different parallels

1118
00:52:57,060 --> 00:52:59,420
with different kinds of corporate governance, right?

1119
00:52:59,420 --> 00:53:01,460
Relatively egalitarian democratic societies

1120
00:53:01,460 --> 00:53:05,140
like Denmark are kind of like mutual insurers.

1121
00:53:05,140 --> 00:53:09,700
Whereas more hierarchical authoritarian countries

1122
00:53:09,700 --> 00:53:11,820
are more like, you know, like Singapore, say,

1123
00:53:11,820 --> 00:53:15,220
is more of a joint stock corporation.

1124
00:53:15,220 --> 00:53:17,540
And indeed, you know, Singapore was founded

1125
00:53:17,540 --> 00:53:18,620
as a, as a, uh,

1126
00:53:18,620 --> 00:53:21,380
an entrepot for the East India Company.

1127
00:53:21,380 --> 00:53:24,580
So there are very deep parallels.

1128
00:53:24,580 --> 00:53:26,260
And it's also essential.

1129
00:53:26,260 --> 00:53:27,420
Transaction costs are essential

1130
00:53:27,420 --> 00:53:29,100
to understand why governments do certain things

1131
00:53:29,100 --> 00:53:30,180
and other things.

1132
00:53:30,180 --> 00:53:32,220
All Western developed governments

1133
00:53:32,220 --> 00:53:35,780
guarantee some amount of basic health care, right?

1134
00:53:35,780 --> 00:53:39,300
But, um, most, you know,

1135
00:53:39,300 --> 00:53:41,940
outside of, say, the National Health Service in,

1136
00:53:41,940 --> 00:53:44,500
in the UK, most of these countries, uh,

1137
00:53:45,260 --> 00:53:46,260
guarantee the insurance.

1138
00:53:46,260 --> 00:53:50,020
They don't necessarily nationalize the actual providers,

1139
00:53:50,020 --> 00:53:50,860
right?

1140
00:53:50,860 --> 00:53:52,540
And the reason goes to transaction costs

1141
00:53:52,540 --> 00:53:55,100
and sort of an analysis of the market failure

1142
00:53:55,100 --> 00:53:56,380
and insurance.

1143
00:53:56,380 --> 00:53:59,060
Likewise with roads, uh, you know,

1144
00:53:59,060 --> 00:54:02,260
it's possible to build roads through purely private means.

1145
00:54:02,260 --> 00:54:05,380
And indeed, um, you know, countries like Sweden,

1146
00:54:05,380 --> 00:54:08,140
a lot of the roads are run by private associations.

1147
00:54:08,140 --> 00:54:10,900
But, uh, if you have lots of different boundaries,

1148
00:54:10,900 --> 00:54:13,460
different micro jurisdictions and so forth,

1149
00:54:13,460 --> 00:54:15,220
there can be huge transaction costs

1150
00:54:15,220 --> 00:54:18,100
to, uh, negotiating up to a,

1151
00:54:18,100 --> 00:54:20,700
to a interstate highway system.

1152
00:54:20,700 --> 00:54:22,420
Um, and, and those transaction costs

1153
00:54:22,420 --> 00:54:26,060
then necessitate public infrastructure projects.

1154
00:54:26,060 --> 00:54:27,620
So the transaction costs in this case

1155
00:54:27,620 --> 00:54:30,540
would be being a private road provider.

1156
00:54:30,540 --> 00:54:34,860
You'd have to go negotiate with 500 different landowners

1157
00:54:34,860 --> 00:54:36,100
about building a highway,

1158
00:54:36,100 --> 00:54:39,220
whereas a government can do some expropriation

1159
00:54:39,220 --> 00:54:42,140
and simply build the road much, much faster,

1160
00:54:42,140 --> 00:54:45,300
or with less transaction costs at least.

1161
00:54:45,300 --> 00:54:46,260
Yeah, precisely.

1162
00:54:46,260 --> 00:54:48,900
And we're seeing this, this dynamic in the US

1163
00:54:48,900 --> 00:54:50,860
with, uh, you know, permitting for,

1164
00:54:50,860 --> 00:54:52,820
for great infrastructure and transmission.

1165
00:54:52,820 --> 00:54:54,300
You know, we're, we're building all this,

1166
00:54:54,300 --> 00:54:55,620
all the solar and renewable energy,

1167
00:54:55,620 --> 00:54:59,820
but to build the actual transmission, uh, infrastructure

1168
00:54:59,820 --> 00:55:02,980
to get the electrons from where it's sunny to where,

1169
00:55:02,980 --> 00:55:06,300
where it's cold requires building, you know,

1170
00:55:06,300 --> 00:55:09,420
high voltage, uh, lines across state lines

1171
00:55:09,420 --> 00:55:11,260
across different grid regions.

1172
00:55:11,260 --> 00:55:13,300
And there are all kinds of NIMBs

1173
00:55:13,300 --> 00:55:17,220
and negotiation costs involved, holdouts and so forth.

1174
00:55:17,220 --> 00:55:19,260
And so that the more those kind of costs exist,

1175
00:55:19,260 --> 00:55:21,780
the more it militates towards a kind of, um,

1176
00:55:21,780 --> 00:55:24,620
larger scale intervention that, you know,

1177
00:55:24,620 --> 00:55:26,820
federalizes that process.

1178
00:55:26,820 --> 00:55:29,180
Yeah, the big question then is how will AI

1179
00:55:29,180 --> 00:55:31,820
change these transaction costs?

1180
00:55:31,820 --> 00:55:34,420
What, what will the effects be here?

1181
00:55:34,420 --> 00:55:37,060
It's easy to say that they will be affected.

1182
00:55:37,060 --> 00:55:38,940
And, you know, obviously the internet affected them

1183
00:55:38,940 --> 00:55:39,940
to, to an extent.

1184
00:55:39,940 --> 00:55:42,980
And we were talking, we talk about sort of the ease

1185
00:55:42,980 --> 00:55:46,820
of mobilizing protest movements or the kind of, uh,

1186
00:55:46,820 --> 00:55:48,780
the sunlight that was put on government corruption.

1187
00:55:48,780 --> 00:55:51,500
Those are, those are reflecting declines

1188
00:55:51,500 --> 00:55:56,180
in the cost associated information and coordination.

1189
00:55:56,180 --> 00:55:57,620
I think AI takes us to another level.

1190
00:55:57,620 --> 00:56:01,020
And I think it's, it's important to, to think through

1191
00:56:01,020 --> 00:56:06,020
in part because right now the AI safety debate,

1192
00:56:06,460 --> 00:56:08,880
at least in the United States is very polarized

1193
00:56:08,920 --> 00:56:12,360
between people who are like, everything's going to be great.

1194
00:56:12,360 --> 00:56:15,440
And people who are like, this is like a terminator scenario

1195
00:56:15,440 --> 00:56:17,560
or an AI kill us all existential risk.

1196
00:56:17,560 --> 00:56:20,960
You know, even if we accept the, you know,

1197
00:56:20,960 --> 00:56:23,400
existential risk framing, there's still going to be

1198
00:56:23,400 --> 00:56:27,120
many intermediate stages of AI before we flip

1199
00:56:27,120 --> 00:56:28,600
on the superintelligence.

1200
00:56:28,600 --> 00:56:31,560
And those intermediate stages have enormous implications

1201
00:56:31,560 --> 00:56:33,920
for the structure of the very institutions

1202
00:56:33,920 --> 00:56:37,400
that we'll need to respond to superintelligence

1203
00:56:37,400 --> 00:56:38,360
or what have you.

1204
00:56:38,360 --> 00:56:42,720
The ways we can see this is because all these information

1205
00:56:42,720 --> 00:56:47,000
and, and monitoring and, and, uh, bargaining costs

1206
00:56:47,880 --> 00:56:51,280
are directly implicated by commoditized intelligence.

1207
00:56:51,280 --> 00:56:53,440
You know, start with the principal agent problem.

1208
00:56:53,440 --> 00:56:55,040
You know, there is no principal agent problem

1209
00:56:55,040 --> 00:56:59,160
if your agent does exactly as you ask and works 24 seven

1210
00:56:59,160 --> 00:57:01,240
doesn't steal from the till, right?

1211
00:57:01,240 --> 00:57:05,760
And so AI agents dramatically collapse agency cost

1212
00:57:05,760 --> 00:57:06,960
monitoring.

1213
00:57:06,960 --> 00:57:09,160
Now that we have multimodal models in principle,

1214
00:57:09,160 --> 00:57:11,520
we could have cameras in every house that are just being

1215
00:57:11,520 --> 00:57:13,880
prompted to say, you know, is someone committing a crime

1216
00:57:13,880 --> 00:57:14,720
right now?

1217
00:57:15,920 --> 00:57:18,280
Whether we wanted to go that direction or not,

1218
00:57:18,280 --> 00:57:20,840
it gives you a sense that of how, you know,

1219
00:57:20,840 --> 00:57:22,720
the cost of monitoring have basically plummeted

1220
00:57:22,720 --> 00:57:25,320
over the last two years and are going to go way lower.

1221
00:57:25,320 --> 00:57:26,920
And so you're starting to see this rolled out

1222
00:57:26,920 --> 00:57:28,520
in the private sector with, you know,

1223
00:57:28,520 --> 00:57:31,280
Activision has announced that they're going to be using

1224
00:57:31,280 --> 00:57:35,160
language models for moderating voice chat

1225
00:57:35,160 --> 00:57:37,200
and call duty, right?

1226
00:57:37,200 --> 00:57:39,920
And, and this, this is a more robust form of monitoring

1227
00:57:39,920 --> 00:57:43,240
because in the past you would have to like ban certain words

1228
00:57:43,240 --> 00:57:47,480
like certain swear words or things associated with

1229
00:57:47,480 --> 00:57:50,440
sexual violence, but then people could always get around

1230
00:57:50,440 --> 00:57:54,160
those by using euphemisms, right?

1231
00:57:54,160 --> 00:57:56,560
Like on YouTube, you know, the algorithm will ding you

1232
00:57:56,560 --> 00:58:00,040
if you talk about coronavirus or if you talk about murder

1233
00:58:00,040 --> 00:58:03,280
or suicide, these, these things that throw off at flags.

1234
00:58:03,280 --> 00:58:06,240
So what people have taken doing is saying they were

1235
00:58:06,240 --> 00:58:08,880
unalived rather than murdered, right?

1236
00:58:08,880 --> 00:58:12,440
And that doesn't fool a language model.

1237
00:58:12,440 --> 00:58:13,880
If you ask a language model, you know,

1238
00:58:13,880 --> 00:58:16,280
if you prompt it in a way to look for sort of broad

1239
00:58:16,280 --> 00:58:20,680
semantic categories, not just a narrow, a narrow word,

1240
00:58:20,680 --> 00:58:22,160
it's much more robust.

1241
00:58:22,160 --> 00:58:23,160
And so what that means, you know,

1242
00:58:23,160 --> 00:58:24,680
what you already start to see it, like I said,

1243
00:58:24,680 --> 00:58:27,920
with Activision and the use of LLAMS and content moderation,

1244
00:58:27,920 --> 00:58:31,040
you're going to start, you're going to see it in the use of

1245
00:58:31,080 --> 00:58:34,920
multimodal models for productivity management and tracking.

1246
00:58:34,920 --> 00:58:38,360
You know, Microsoft is unveiling their 365 co-pilot

1247
00:58:38,360 --> 00:58:41,920
where you're going to have GPT-4 and Word and Excel

1248
00:58:41,920 --> 00:58:44,200
and Teams and Outlook, but at the same time,

1249
00:58:44,200 --> 00:58:46,600
you're also going to have a manager who is going to be able

1250
00:58:46,600 --> 00:58:49,080
to say, you know, to prompt the model,

1251
00:58:49,080 --> 00:58:51,600
tell me who is the most productive this week, right?

1252
00:58:51,600 --> 00:58:53,200
Something as vague as that.

1253
00:58:53,200 --> 00:58:56,320
And so you see this diffusion in the private sector.

1254
00:58:56,320 --> 00:59:00,000
The question is, does it diffuse in the public sector?

1255
00:59:00,000 --> 00:59:03,760
There's obvious ways that it would be a huge boom, right?

1256
00:59:03,760 --> 00:59:07,160
You know, Inspector General GPT could tell you exactly,

1257
00:59:07,160 --> 00:59:09,320
you know, how the civil service is working,

1258
00:59:09,320 --> 00:59:10,160
whether there's corruption,

1259
00:59:10,160 --> 00:59:11,480
whether there's a deep state of conspiracy

1260
00:59:11,480 --> 00:59:13,240
or something like that, right?

1261
00:59:13,240 --> 00:59:16,680
And at first blush, a lot of what government does

1262
00:59:16,680 --> 00:59:19,880
is kind of a fleshy API.

1263
00:59:19,880 --> 00:59:24,880
Bureaucracies are nodes that apply a degree of context

1264
00:59:24,880 --> 00:59:26,440
between printing out a PDF and scanning it

1265
00:59:26,440 --> 00:59:28,080
back into the computer.

1266
00:59:28,240 --> 00:59:29,080
It varies.

1267
00:59:29,080 --> 00:59:32,720
There's degrees of human judgment that are required,

1268
00:59:32,720 --> 00:59:35,800
but on first order, government bureaucracies

1269
00:59:35,800 --> 00:59:38,640
seem incredibly exposed to this technology

1270
00:59:38,640 --> 00:59:40,480
in a way that could diffuse really rapidly

1271
00:59:40,480 --> 00:59:43,480
because, you know, going back to Microsoft 365 Copilot,

1272
00:59:43,480 --> 00:59:46,360
Microsoft is the biggest IT vendor in US government, right?

1273
00:59:46,360 --> 00:59:50,240
And so you can imagine once everyone has this pre-installed

1274
00:59:50,240 --> 00:59:52,600
on their computer that the person at the Bureau

1275
00:59:52,600 --> 00:59:54,040
of Labor Statistics who's in charge

1276
00:59:54,040 --> 00:59:57,360
of doing the monthly employment situation report,

1277
00:59:57,800 --> 00:59:59,560
the jobs report, you know, at some point

1278
00:59:59,560 --> 01:00:02,160
he's gonna be walking into work and hitting a button, right?

1279
01:00:02,160 --> 01:00:06,080
That, you know, asking Excel to find

1280
01:00:06,080 --> 01:00:08,480
the five most interesting trends and generate charts

1281
01:00:08,480 --> 01:00:10,440
and the report is done.

1282
01:00:10,440 --> 01:00:12,440
And in the private sector, that person would be reallocated

1283
01:00:12,440 --> 01:00:15,320
and maybe doing things that the computer's not good at yet,

1284
01:00:15,320 --> 01:00:18,640
but these positions are much stickier in government.

1285
01:00:18,640 --> 01:00:20,160
To the extent that diffusion is inhibited

1286
01:00:20,160 --> 01:00:21,640
on the public sector side,

1287
01:00:21,640 --> 01:00:24,640
I worry about the kind of disruption and displacement

1288
01:00:24,640 --> 01:00:27,040
of government services by a private sector

1289
01:00:27,440 --> 01:00:29,480
that's adopting the technology really fast.

1290
01:00:29,480 --> 01:00:31,520
This is something we'll talk about in a moment.

1291
01:00:31,520 --> 01:00:34,320
Before that, I just wanna get to your complaints

1292
01:00:34,320 --> 01:00:36,200
about isolated thinking about AI.

1293
01:00:36,200 --> 01:00:41,200
You've sketched out some complaint about people

1294
01:00:41,200 --> 01:00:45,120
thinking about AI only applying to one domain

1295
01:00:45,120 --> 01:00:47,480
and then not really seeing the bigger picture.

1296
01:00:47,480 --> 01:00:49,480
What are some examples here?

1297
01:00:49,480 --> 01:00:52,000
Why do you worry about isolated thinking?

1298
01:00:52,000 --> 01:00:53,120
A few dimensions to this.

1299
01:00:53,120 --> 01:00:56,080
One is what I've called the horse's carriage fallacy.

1300
01:00:57,040 --> 01:00:58,840
Right, the kind of view that, you know,

1301
01:00:58,840 --> 01:01:03,040
what automobiles were was just a carriage with the horse.

1302
01:01:03,040 --> 01:01:06,320
Right, and so that anchors you to the older paradigm

1303
01:01:06,320 --> 01:01:07,760
and it's like you're changing one thing

1304
01:01:07,760 --> 01:01:09,160
and everything else stays the same.

1305
01:01:09,160 --> 01:01:11,800
And you neglect all the second order ways

1306
01:01:11,800 --> 01:01:14,160
that the development of the automobile,

1307
01:01:14,160 --> 01:01:17,640
you know, enabled the build out of highway systems,

1308
01:01:17,640 --> 01:01:22,560
the total reconfiguration of sort of the economic geography.

1309
01:01:22,560 --> 01:01:26,040
Right, and then implications for institutions

1310
01:01:26,040 --> 01:01:28,480
at the state where, you know, once you have road networks

1311
01:01:28,480 --> 01:01:30,200
or telegraph networks or any of these,

1312
01:01:30,200 --> 01:01:31,720
these kind of networks, it suddenly becomes easier

1313
01:01:31,720 --> 01:01:34,720
to monitor agents of the state

1314
01:01:34,720 --> 01:01:35,800
and other parts of the country.

1315
01:01:35,800 --> 01:01:37,400
And so you can, you know, build out

1316
01:01:37,400 --> 01:01:38,840
more of a federal bureaucracy.

1317
01:01:38,840 --> 01:01:40,760
And so all these things were second order

1318
01:01:40,760 --> 01:01:42,720
and were kind of neglected if you just were too focused

1319
01:01:42,720 --> 01:01:46,440
on the first order effects of displacing the horses.

1320
01:01:46,440 --> 01:01:49,440
And in a sense, the second order effects

1321
01:01:49,440 --> 01:01:52,680
turned out to be much more consequential in the end.

1322
01:01:52,680 --> 01:01:54,800
Yes, it seemed to always be.

1323
01:01:54,800 --> 01:01:57,880
And likewise with the internet and sort of the,

1324
01:01:57,880 --> 01:01:59,920
I think this comes up a lot in the,

1325
01:01:59,920 --> 01:02:03,360
in how to think about AI use and misuse.

1326
01:02:03,360 --> 01:02:04,720
There's lots of valid discussions there,

1327
01:02:04,720 --> 01:02:06,920
but they're always very first order.

1328
01:02:06,920 --> 01:02:08,200
And when you think about the way the internet

1329
01:02:08,200 --> 01:02:10,160
has disrupted legacy institutions,

1330
01:02:10,160 --> 01:02:11,720
yes, there's disinformation,

1331
01:02:11,720 --> 01:02:16,240
but often the thing that's disrupting is not fake news.

1332
01:02:16,240 --> 01:02:18,640
It's real news that's being repeated

1333
01:02:18,640 --> 01:02:21,440
with misleading frequency, right?

1334
01:02:21,440 --> 01:02:24,680
That's like throwing off our availability heuristic.

1335
01:02:24,680 --> 01:02:28,400
Or it's valid things that, you know,

1336
01:02:28,400 --> 01:02:30,600
valid complaints, whether, you know,

1337
01:02:30,600 --> 01:02:31,760
the protests in Iran, right?

1338
01:02:31,760 --> 01:02:34,280
The protests in Iran have this like striking parallel

1339
01:02:34,280 --> 01:02:36,840
to the protests following the George Floyd protest

1340
01:02:37,800 --> 01:02:40,160
and protesting in other countries

1341
01:02:40,160 --> 01:02:42,780
where they even have like a three word chant, right?

1342
01:02:42,780 --> 01:02:47,780
Or the case of the Arab Spring in Tunisia

1343
01:02:47,920 --> 01:02:50,900
that started with the person self-immolating, right?

1344
01:02:50,900 --> 01:02:52,360
There's sort of like the structure that repeats

1345
01:02:52,360 --> 01:02:53,280
where you have like a martyr

1346
01:02:53,280 --> 01:02:55,440
or like some shocking event.

1347
01:02:55,440 --> 01:02:57,960
And because of the way social media is organized,

1348
01:02:57,960 --> 01:03:01,320
it synchronizes people around the event

1349
01:03:02,440 --> 01:03:04,240
in a way that's kind of stochastic.

1350
01:03:04,240 --> 01:03:05,720
Like it's like lightning striking.

1351
01:03:05,720 --> 01:03:07,600
You don't know what event it's going to strike on,

1352
01:03:07,600 --> 01:03:09,320
but once we're synchronized,

1353
01:03:09,320 --> 01:03:11,280
then we start, you know, moving back and forth

1354
01:03:11,280 --> 01:03:14,280
in a way that like causes the bridge to buckle.

1355
01:03:14,280 --> 01:03:17,040
Nothing about that is a misuse, right?

1356
01:03:17,040 --> 01:03:18,840
Those are all valid uses,

1357
01:03:18,840 --> 01:03:23,040
but their use is under collective action.

1358
01:03:23,040 --> 01:03:25,200
It's sort of solving not just for the partial equilibrium

1359
01:03:25,200 --> 01:03:28,400
but the general equilibrium when everyone is doing this.

1360
01:03:28,400 --> 01:03:30,920
And I think the person who wrote the best

1361
01:03:30,920 --> 01:03:33,840
on this sort of conceptually was Thomas Schelling

1362
01:03:33,840 --> 01:03:36,240
and one of his little books,

1363
01:03:36,240 --> 01:03:38,120
Micromotives, Macro Behavior,

1364
01:03:38,120 --> 01:03:39,280
A Big Influence on Me as a Kid,

1365
01:03:39,280 --> 01:03:42,200
where he talks about all these sort of like toy models

1366
01:03:42,200 --> 01:03:45,160
where you're at a hockey game or a basketball game

1367
01:03:46,080 --> 01:03:47,600
and something is happening,

1368
01:03:47,600 --> 01:03:50,600
something exciting is happening in the arena.

1369
01:03:50,600 --> 01:03:52,400
And so people in front of you stand up

1370
01:03:52,400 --> 01:03:54,520
to get a better view and then you have to stand up

1371
01:03:54,520 --> 01:03:56,680
to get a better view of them over them and so on.

1372
01:03:56,680 --> 01:03:58,760
And so it cascades and suddenly everyone went from sitting

1373
01:03:58,760 --> 01:03:59,640
to everyone went to standing

1374
01:03:59,640 --> 01:04:02,080
and no one's view has improved, right?

1375
01:04:02,080 --> 01:04:04,880
And so these sort of general equilibria

1376
01:04:04,880 --> 01:04:08,120
where you sort of solve for everyone's micro incentives

1377
01:04:08,120 --> 01:04:10,880
and the kind of new Nash equilibrium that emerges,

1378
01:04:10,880 --> 01:04:13,480
that ends up being the thing that drives

1379
01:04:13,480 --> 01:04:15,440
a kind of multiple equilibrium shift

1380
01:04:15,440 --> 01:04:17,680
from one regime to another.

1381
01:04:17,680 --> 01:04:21,200
And throughout, there may be no actual examples

1382
01:04:21,200 --> 01:04:22,040
of misuse involved.

1383
01:04:22,040 --> 01:04:25,360
It may just be people following their individual incentives.

1384
01:04:25,360 --> 01:04:28,200
I think it's worth the stressing this point you make

1385
01:04:28,200 --> 01:04:32,400
about the effects of earlier AI systems

1386
01:04:32,400 --> 01:04:33,800
on our institutions,

1387
01:04:33,800 --> 01:04:37,160
that they might have effects that deteriorate our institutions

1388
01:04:37,160 --> 01:04:41,200
such that we can't handle later and more advanced AI.

1389
01:04:41,200 --> 01:04:45,160
And ignoring this would be an example of isolated thinking

1390
01:04:45,160 --> 01:04:48,120
and ignoring the second order effects, right?

1391
01:04:48,120 --> 01:04:48,960
Yeah.

1392
01:04:48,960 --> 01:04:53,960
And they also, it also changes the sort of agenda, right?

1393
01:04:54,720 --> 01:04:56,960
The AI safety agenda shouldn't just be

1394
01:04:56,960 --> 01:04:59,480
about the first order of things or in alignment,

1395
01:04:59,480 --> 01:05:01,240
you know, very important,

1396
01:05:01,240 --> 01:05:05,240
but you know, it's led to a discussion of

1397
01:05:05,240 --> 01:05:06,520
do we need a new federal agency?

1398
01:05:06,520 --> 01:05:08,840
And if so, what kind of agency?

1399
01:05:08,840 --> 01:05:11,680
Whereas it may be more appropriate to think

1400
01:05:11,680 --> 01:05:13,120
not what new agency do we need,

1401
01:05:13,120 --> 01:05:17,720
but how do all the agencies change, right?

1402
01:05:17,720 --> 01:05:20,000
And how do we sort of like brace for impact

1403
01:05:20,000 --> 01:05:24,040
and enable a degree of co-evolution

1404
01:05:24,040 --> 01:05:25,800
rather than displacement?

1405
01:05:25,800 --> 01:05:28,600
I don't know whether the question of

1406
01:05:28,600 --> 01:05:32,480
how to get our institutions to respond appropriately

1407
01:05:32,480 --> 01:05:35,280
is more difficult or less difficult

1408
01:05:35,280 --> 01:05:37,080
than the problem of aligning AI.

1409
01:05:37,080 --> 01:05:38,920
But it certainly seems very difficult to me.

1410
01:05:38,920 --> 01:05:42,520
So is there, are we making it harder on ourselves

1411
01:05:42,520 --> 01:05:44,720
if we focus on the effects,

1412
01:05:44,720 --> 01:05:47,440
on the second order effects on institutions?

1413
01:05:47,440 --> 01:05:48,920
I mean, it's unavoidable.

1414
01:05:48,920 --> 01:05:52,360
I mean, we can't pick and choose what kind of problems,

1415
01:05:52,360 --> 01:05:55,240
but you know, the alignment problem,

1416
01:05:55,240 --> 01:05:58,240
the hard version is yet to be solved,

1417
01:05:58,240 --> 01:06:00,920
but we have many examples of governments

1418
01:06:00,920 --> 01:06:04,280
building state capacity and having kind of,

1419
01:06:04,280 --> 01:06:05,960
you know, shifting from very,

1420
01:06:05,960 --> 01:06:10,200
very like clientelistic, sticky corrupt governments

1421
01:06:10,200 --> 01:06:12,520
to sort of modernized governments

1422
01:06:12,520 --> 01:06:14,840
where you know, state capacity is built

1423
01:06:14,840 --> 01:06:16,520
and then that government can sort of break out

1424
01:06:16,520 --> 01:06:18,760
of the middle income trap and become rich.

1425
01:06:18,760 --> 01:06:23,440
You mentioned Estonia as an example of a country

1426
01:06:23,440 --> 01:06:25,520
that's pretty advanced on the IT front,

1427
01:06:25,520 --> 01:06:26,480
on the technology side.

1428
01:06:26,480 --> 01:06:28,880
Maybe you could talk a bit about Estonia.

1429
01:06:28,880 --> 01:06:30,920
Yeah, I would just say in general,

1430
01:06:30,920 --> 01:06:34,440
it's hard for any organization to reform itself from within

1431
01:06:34,440 --> 01:06:35,320
when there is path dependency,

1432
01:06:35,320 --> 01:06:39,120
but I would say at least we have examples of it being done

1433
01:06:39,120 --> 01:06:42,040
where we don't have examples of alignment being solved yet.

1434
01:06:43,360 --> 01:06:45,440
When it comes to Estonia,

1435
01:06:45,440 --> 01:06:46,840
you know, Estonia is an interesting case.

1436
01:06:46,840 --> 01:06:48,960
It's sort of an exceptional case

1437
01:06:48,960 --> 01:06:50,960
because after the fall of the Soviet Union

1438
01:06:50,960 --> 01:06:54,360
and the breakup of the peripheral former Soviet states,

1439
01:06:54,360 --> 01:06:57,200
they kind of had a blank slate, right?

1440
01:06:57,200 --> 01:06:58,640
They also had a very young population

1441
01:06:58,640 --> 01:07:01,840
and people who had a kind of hacker ethic

1442
01:07:01,840 --> 01:07:03,400
within their civil service.

1443
01:07:03,400 --> 01:07:06,400
And so with that blank slate and with that hacker ethic,

1444
01:07:06,400 --> 01:07:09,680
they were very early to adopt and to foresee

1445
01:07:09,680 --> 01:07:11,440
the way the internet was going to shape government

1446
01:07:11,440 --> 01:07:14,880
through a variety of e-government reforms.

1447
01:07:14,880 --> 01:07:17,520
So early in the late 90s and into the 2000s,

1448
01:07:17,520 --> 01:07:20,360
they were some of the earliest to digitize

1449
01:07:20,360 --> 01:07:23,440
their banking system like e-banking

1450
01:07:23,440 --> 01:07:27,760
to build this system called X-Road,

1451
01:07:27,760 --> 01:07:30,080
which is kind of like a cryptographically secured

1452
01:07:30,080 --> 01:07:32,840
data exchange layer that resembles a blockchain,

1453
01:07:32,840 --> 01:07:35,920
but it was about a decade before blockchain was invented.

1454
01:07:35,920 --> 01:07:37,320
For exchanging information

1455
01:07:37,320 --> 01:07:39,960
between different government entities,

1456
01:07:39,960 --> 01:07:43,840
your medical information could be uploaded to the system

1457
01:07:43,840 --> 01:07:45,600
and then be available to all systems

1458
01:07:45,600 --> 01:07:48,800
that have the right to see that information.

1459
01:07:48,800 --> 01:07:51,160
Exactly, in a way that's cryptographically secured

1460
01:07:51,160 --> 01:07:52,000
and distributed.

1461
01:07:52,000 --> 01:07:55,480
So if a missile hit the Department of Education,

1462
01:07:55,480 --> 01:07:56,920
you don't lose your education records

1463
01:07:56,920 --> 01:07:58,960
because it's distributed.

1464
01:07:58,960 --> 01:08:03,080
And that also enabled an enormous amount of automation

1465
01:08:03,080 --> 01:08:06,000
where, for instance, this is my understanding,

1466
01:08:06,000 --> 01:08:07,120
a child born in Estonia,

1467
01:08:07,120 --> 01:08:09,360
once you file that birth record,

1468
01:08:09,360 --> 01:08:13,600
it more or less initiates a clock in the system

1469
01:08:13,600 --> 01:08:16,520
that will then enroll your child in school

1470
01:08:16,520 --> 01:08:19,160
when they turn four or five automatically

1471
01:08:19,160 --> 01:08:21,840
because it knows that your child has aged

1472
01:08:21,840 --> 01:08:24,840
and then unless it had a death record to cancel that out.

1473
01:08:24,840 --> 01:08:27,720
That also means you can do taxes and transfers

1474
01:08:27,720 --> 01:08:32,440
much simpler, you get your benefit within a week.

1475
01:08:32,440 --> 01:08:34,960
It can integrate across different parts

1476
01:08:34,960 --> 01:08:37,160
of public infrastructure,

1477
01:08:37,160 --> 01:08:41,480
like use the same card to ride the bus

1478
01:08:41,520 --> 01:08:43,880
as you do to launch a new business.

1479
01:08:43,880 --> 01:08:45,560
And it also serves as a kind of platform

1480
01:08:45,560 --> 01:08:50,360
for the private sector to do government by API,

1481
01:08:50,360 --> 01:08:54,600
to build new services on top of government as a platform

1482
01:08:54,600 --> 01:08:57,760
and integrate with government databases.

1483
01:08:57,760 --> 01:08:58,920
Yeah, and so the point here for us

1484
01:08:58,920 --> 01:09:01,960
is that institutional reform is possible,

1485
01:09:01,960 --> 01:09:03,800
modernizing government is possible,

1486
01:09:03,800 --> 01:09:05,280
at least under certain circumstances.

1487
01:09:05,280 --> 01:09:10,000
We have proofs of concepts of this happening.

1488
01:09:10,000 --> 01:09:12,000
The hard thing is the path dependency.

1489
01:09:12,000 --> 01:09:13,240
There's always a strong instinct

1490
01:09:13,240 --> 01:09:14,360
to wanna start from scratch

1491
01:09:14,360 --> 01:09:16,200
and it's normally not advisable

1492
01:09:16,200 --> 01:09:18,960
because it's too hard.

1493
01:09:18,960 --> 01:09:23,960
And so this is why it's hard in the US.

1494
01:09:24,280 --> 01:09:25,960
This is why you have African countries

1495
01:09:25,960 --> 01:09:28,280
that leap progress in payment systems and so forth.

1496
01:09:28,280 --> 01:09:31,080
The challenge of this decade or century

1497
01:09:31,080 --> 01:09:33,760
is how do we solve that path dependency problem

1498
01:09:33,760 --> 01:09:36,000
and how do we get to Estonia?

1499
01:09:36,000 --> 01:09:37,280
It used to be get to Denmark.

1500
01:09:37,280 --> 01:09:39,760
Now let's get to Estonia

1501
01:09:39,760 --> 01:09:43,800
and find that sort of that pathway up mountain probable.

1502
01:09:43,800 --> 01:09:47,880
Great, let's get to your wonderful series of blog posts

1503
01:09:47,880 --> 01:09:49,720
on AI and Leviathan.

1504
01:09:49,720 --> 01:09:52,640
In this context, what do we mean by Leviathan?

1505
01:09:52,640 --> 01:09:53,600
Well, it's all interrelates.

1506
01:09:53,600 --> 01:09:56,560
So Leviathan was the book Hobbes,

1507
01:09:56,560 --> 01:10:00,320
Thomas Hobbes wrote at the start of the interregnum

1508
01:10:00,320 --> 01:10:02,040
after the English Civil War.

1509
01:10:02,040 --> 01:10:06,160
And it was basically his early political science,

1510
01:10:06,160 --> 01:10:08,560
early defense of absolutist monarchy

1511
01:10:08,560 --> 01:10:12,400
as a way to restore peace and order

1512
01:10:12,400 --> 01:10:16,040
after a decade of infighting.

1513
01:10:17,000 --> 01:10:22,000
And Hobbes kind of hit on some basic sort of structural

1514
01:10:22,000 --> 01:10:26,720
game theoretic properties of why we have governments at all.

1515
01:10:26,720 --> 01:10:29,080
He talked about life being nasty British and short

1516
01:10:29,080 --> 01:10:31,720
in the state of nature, war of all against all.

1517
01:10:31,720 --> 01:10:33,840
And peace is only restored

1518
01:10:33,880 --> 01:10:36,960
when people who don't trust each other

1519
01:10:36,960 --> 01:10:40,720
offload enforcement and policing responsibilities

1520
01:10:40,720 --> 01:10:44,480
to a higher power that can then restore

1521
01:10:44,480 --> 01:10:46,200
a degree of peace and order.

1522
01:10:46,200 --> 01:10:49,280
AI and Leviathan is talking about,

1523
01:10:49,280 --> 01:10:51,520
how does AI change the story?

1524
01:10:51,520 --> 01:10:53,840
Does it reinforce the Leviathan?

1525
01:10:53,840 --> 01:10:57,100
Does it lead to a digital police state China?

1526
01:10:57,100 --> 01:11:00,680
Or is it something that we impose on ourselves?

1527
01:11:00,680 --> 01:11:02,840
And we talked about how multimodal models

1528
01:11:02,840 --> 01:11:05,560
could in principle be used to put a camera

1529
01:11:05,560 --> 01:11:07,560
in everyone's house and have it just continuously monitoring

1530
01:11:07,560 --> 01:11:09,280
for people doing any kind of crime.

1531
01:11:09,280 --> 01:11:11,680
That's something that North Korea might do.

1532
01:11:11,680 --> 01:11:15,040
In the US context, it's something that we're very liable

1533
01:11:15,040 --> 01:11:17,760
to just voluntarily do to ourselves

1534
01:11:17,760 --> 01:11:20,000
because we want to have ring cameras

1535
01:11:20,000 --> 01:11:23,600
and Alexa assistants and so forth.

1536
01:11:23,600 --> 01:11:26,440
And so that leads to a kind of bottom up Leviathan

1537
01:11:26,440 --> 01:11:29,360
that is potentially no less oppressive

1538
01:11:29,360 --> 01:11:30,440
and maybe even more oppressive

1539
01:11:30,440 --> 01:11:34,480
because there's no one that we can appeal to

1540
01:11:34,480 --> 01:11:36,400
to change the rules.

1541
01:11:36,400 --> 01:11:39,000
Yeah, so Leviathan is one way to respond

1542
01:11:39,000 --> 01:11:42,200
to technological change, but you mentioned two other ways

1543
01:11:42,200 --> 01:11:44,800
we could alternatively respond.

1544
01:11:44,800 --> 01:11:47,040
Right, so basically any time a technology

1545
01:11:47,040 --> 01:11:49,320
greatly empowers the individual,

1546
01:11:49,320 --> 01:11:53,400
it creates a potential negative externality, right?

1547
01:11:53,400 --> 01:11:56,000
Hobbes called these our natural liberties.

1548
01:11:56,000 --> 01:11:57,640
In the state of nature, I have a natural liberty

1549
01:11:57,640 --> 01:12:00,440
to kill you or to strong arm you.

1550
01:12:00,440 --> 01:12:05,440
And governments exist to revoke those natural liberties, right?

1551
01:12:05,480 --> 01:12:08,120
But for a higher form of freedom, right?

1552
01:12:08,120 --> 01:12:11,240
And so there are sort of any time a technology greatly

1553
01:12:11,240 --> 01:12:12,760
increases human capabilities,

1554
01:12:12,760 --> 01:12:14,440
these are the other humans.

1555
01:12:14,440 --> 01:12:16,760
The three canonical ways we can adjust are,

1556
01:12:16,760 --> 01:12:19,520
you know, ceding more authority to that higher power,

1557
01:12:19,520 --> 01:12:21,400
the Leviathan option.

1558
01:12:21,400 --> 01:12:23,200
And then the other two options are,

1559
01:12:23,200 --> 01:12:25,240
you know, adaptation and mitigation

1560
01:12:25,240 --> 01:12:27,240
and normative evolution.

1561
01:12:27,240 --> 01:12:29,560
So the example I give is, you know,

1562
01:12:29,560 --> 01:12:31,680
if suddenly we all had X-ray glasses

1563
01:12:31,680 --> 01:12:33,840
and you could see through walls and see through clothing.

1564
01:12:33,840 --> 01:12:37,280
You know, one option, we have a draconian,

1565
01:12:37,280 --> 01:12:40,480
totalitarian crackdown that tries to seize

1566
01:12:40,480 --> 01:12:42,280
all those X-ray glasses.

1567
01:12:42,280 --> 01:12:45,920
Another option is we adjust normatively, culturally,

1568
01:12:45,920 --> 01:12:49,040
that we, our privacy norms wither away

1569
01:12:49,040 --> 01:12:51,440
and we stop caring about nudity.

1570
01:12:52,480 --> 01:12:54,720
And then the other option is adaptation and mitigation

1571
01:12:54,720 --> 01:12:57,240
where we put in, you know, mesh into our walls

1572
01:12:57,240 --> 01:13:00,920
and where we're leaded shirts and pants.

1573
01:13:02,880 --> 01:13:06,120
Yeah, I guess continuing that analogy a bit

1574
01:13:06,120 --> 01:13:10,000
between the smart glasses and AI,

1575
01:13:10,000 --> 01:13:13,400
you have this amazing write-up of ways

1576
01:13:13,400 --> 01:13:18,280
in which AI can increase the informational resolution

1577
01:13:18,280 --> 01:13:19,520
of the universe.

1578
01:13:19,520 --> 01:13:21,440
So you give some examples that are,

1579
01:13:21,440 --> 01:13:24,920
I think specifically of AI identifying people

1580
01:13:24,920 --> 01:13:26,560
by gate, for example.

1581
01:13:26,560 --> 01:13:28,480
Right, so gate recognition is nothing new.

1582
01:13:28,480 --> 01:13:31,760
China has had advanced forms of gate recognition

1583
01:13:31,760 --> 01:13:33,320
for a while now.

1584
01:13:33,320 --> 01:13:35,560
So, you know, even if you cover your face,

1585
01:13:35,560 --> 01:13:37,880
it turns out we're constantly throwing off

1586
01:13:37,880 --> 01:13:41,640
sort of ambient information about ourselves, about everything.

1587
01:13:41,640 --> 01:13:46,520
And the way you walk, the particular gate that you have

1588
01:13:46,520 --> 01:13:48,720
is a unique identifier.

1589
01:13:48,720 --> 01:13:51,600
Another example is galaxy surveys.

1590
01:13:51,600 --> 01:13:54,400
There's, we've had from Hubble telescope to now,

1591
01:13:54,400 --> 01:13:59,400
the JWST, tons of astronomical surveys

1592
01:13:59,720 --> 01:14:02,000
of distant galaxies and so forth.

1593
01:14:02,000 --> 01:14:05,000
And all of a sudden, all that old data,

1594
01:14:05,000 --> 01:14:08,880
it's like that same data set is now more useful

1595
01:14:08,880 --> 01:14:11,480
because applying more modern deep learning techniques,

1596
01:14:11,480 --> 01:14:15,000
we can extract entropy that was in that data set,

1597
01:14:15,000 --> 01:14:16,920
but we didn't have the tools to extract yet

1598
01:14:17,120 --> 01:14:20,120
and discover that, you know, there are new galaxies

1599
01:14:20,120 --> 01:14:22,920
or other phenomena that we missed.

1600
01:14:22,920 --> 01:14:27,720
Another example you give is listening for keystrokes

1601
01:14:27,720 --> 01:14:29,560
on a keyboard and extracting information

1602
01:14:29,560 --> 01:14:33,080
about a password being typed in, for example,

1603
01:14:33,080 --> 01:14:35,960
which is something that of course humans can do,

1604
01:14:35,960 --> 01:14:38,680
but we can do with AI models.

1605
01:14:38,680 --> 01:14:41,880
Yeah, so that was a paper showing that

1606
01:14:41,880 --> 01:14:44,880
you can reconstruct keystrokes from an audio recording,

1607
01:14:44,880 --> 01:14:46,160
including a Zoom conversation.

1608
01:14:46,160 --> 01:14:47,440
So I hope you haven't typed in your password

1609
01:14:47,440 --> 01:14:49,760
because people in the future, and so this goes to,

1610
01:14:49,760 --> 01:14:51,600
you know, the fact that it's sort of retroactive,

1611
01:14:51,600 --> 01:14:54,400
that like, even if the technology wasn't diffused yet,

1612
01:14:54,400 --> 01:14:56,240
any Zoom conversation, any recording

1613
01:14:56,240 --> 01:14:58,320
where someone typed their password in the future

1614
01:14:58,320 --> 01:14:59,960
will be like those galaxy surveys

1615
01:14:59,960 --> 01:15:02,120
where someone will go backwards in time

1616
01:15:02,120 --> 01:15:04,480
and, you know, turn up the information resolution

1617
01:15:04,480 --> 01:15:05,640
of that data.

1618
01:15:05,640 --> 01:15:07,440
Yeah, this is pure speculation,

1619
01:15:07,440 --> 01:15:11,080
but I wonder if, I mean, imagine anonymized people

1620
01:15:11,080 --> 01:15:13,400
in interviews, say 10 years ago,

1621
01:15:13,400 --> 01:15:15,360
whether they will be able to stay anonymous

1622
01:15:15,400 --> 01:15:18,160
or whether AI will be able to extract the data

1623
01:15:18,160 --> 01:15:20,240
about their face or their voice

1624
01:15:20,240 --> 01:15:24,680
that wasn't technically possible when the interview aired.

1625
01:15:24,680 --> 01:15:27,600
Yeah, exactly, there are already systems

1626
01:15:27,600 --> 01:15:31,280
for like, depixelating, you probably do something similar

1627
01:15:31,280 --> 01:15:34,400
for the voice modulation, and then also sort of, you know,

1628
01:15:34,400 --> 01:15:37,520
again, going back to like, this ambient information

1629
01:15:37,520 --> 01:15:41,920
we're always shedding, identifiers in the way we write,

1630
01:15:41,920 --> 01:15:44,440
you know, the kind, where we place a comma,

1631
01:15:44,440 --> 01:15:47,440
the kinds of adverbs we like to use and so forth.

1632
01:15:47,440 --> 01:15:49,280
People just dramatically underrate, you know,

1633
01:15:49,280 --> 01:15:50,840
how much information we're shedding,

1634
01:15:50,840 --> 01:15:52,680
in part because we're blind to it.

1635
01:15:52,680 --> 01:15:55,000
Some people who are taking great efforts

1636
01:15:55,000 --> 01:15:59,560
to stay anonymous online, people in the cryptography space,

1637
01:15:59,560 --> 01:16:02,040
for example, will put their writings

1638
01:16:02,040 --> 01:16:03,800
through Google Translate to French

1639
01:16:03,800 --> 01:16:07,320
and then back to English to erase subtle clues

1640
01:16:07,320 --> 01:16:11,240
to how they, that could identify them personally.

1641
01:16:11,240 --> 01:16:13,920
Why is AI so much better at tasks

1642
01:16:13,920 --> 01:16:17,160
like the ones we just mentioned, compared to humans?

1643
01:16:17,160 --> 01:16:19,760
Well, it goes back to what we were talking about

1644
01:16:19,760 --> 01:16:22,920
with sort of putting information theoretic bounds on AGI.

1645
01:16:22,920 --> 01:16:25,040
When you minimize the loss function

1646
01:16:25,040 --> 01:16:25,880
in a machine learning model,

1647
01:16:25,880 --> 01:16:28,800
you're trying to minimize the cross entropy loss.

1648
01:16:28,800 --> 01:16:31,120
The cross entropy is, how many bits does it take

1649
01:16:31,120 --> 01:16:33,880
to distinguish between two data streams?

1650
01:16:33,880 --> 01:16:36,160
And if it takes a lot of bits to distinguish between the two,

1651
01:16:36,160 --> 01:16:38,720
that means they're relatively indistinguishable.

1652
01:16:38,720 --> 01:16:40,240
So that's going, again, to the Turing test.

1653
01:16:40,240 --> 01:16:43,040
Like, if we have a Turing test where I can tell right away

1654
01:16:43,080 --> 01:16:45,680
that the AI is different than the human,

1655
01:16:45,680 --> 01:16:47,640
that suggests a high cross entropy.

1656
01:16:47,640 --> 01:16:50,440
But if I could talk to it for days

1657
01:16:50,440 --> 01:16:52,840
and do all kinds of adversarial questioning,

1658
01:16:52,840 --> 01:16:54,200
I might still be able to, in the end,

1659
01:16:54,200 --> 01:16:55,360
tell the difference between the two,

1660
01:16:55,360 --> 01:16:58,200
but we've minimized that cross entropy loss.

1661
01:16:58,200 --> 01:17:02,040
And so when you have any arbitrary data distribution

1662
01:17:02,040 --> 01:17:03,600
that you're trying to predict,

1663
01:17:03,600 --> 01:17:07,000
whether it's trying to predict galaxies

1664
01:17:07,000 --> 01:17:09,320
and astronomical data or passwords

1665
01:17:09,320 --> 01:17:13,240
from fingerprint data on a phone screen,

1666
01:17:13,240 --> 01:17:17,520
all these things embed a kind of physical memory

1667
01:17:17,520 --> 01:17:19,280
of the thing in question

1668
01:17:19,280 --> 01:17:20,760
and can often be reconstructed

1669
01:17:20,760 --> 01:17:23,760
through this kind of loss minimization,

1670
01:17:23,760 --> 01:17:26,800
where you have a system that asymptotically

1671
01:17:26,800 --> 01:17:30,280
extracts the entropy that was latent in the data.

1672
01:17:30,280 --> 01:17:31,320
And this can be done in a way

1673
01:17:31,320 --> 01:17:33,760
that is often quite striking,

1674
01:17:33,760 --> 01:17:37,840
where we can, with stable diffusion,

1675
01:17:37,840 --> 01:17:39,080
make fairly accurate predictions

1676
01:17:39,080 --> 01:17:42,320
of what people are imagining in their mind

1677
01:17:42,320 --> 01:17:44,240
using fMRI data.

1678
01:17:44,240 --> 01:17:46,600
And fMRI data is like blood flow data in the brain.

1679
01:17:46,600 --> 01:17:48,640
It's a very lossy representation

1680
01:17:48,640 --> 01:17:50,520
of what ever's happening in the brain.

1681
01:17:50,520 --> 01:17:53,200
But there's still enough latent entropy in there

1682
01:17:53,200 --> 01:17:54,680
that we can kind of reverse engineer

1683
01:17:54,680 --> 01:17:59,000
or decompress it into a folder picture.

1684
01:17:59,000 --> 01:18:02,080
And this could turn into a form of lie detection.

1685
01:18:02,080 --> 01:18:05,560
Yeah, I think it already basically has.

1686
01:18:06,360 --> 01:18:12,160
If you have fMRI data or EEGs

1687
01:18:12,160 --> 01:18:13,480
or other kinds of like direct brain data,

1688
01:18:13,480 --> 01:18:16,040
it's probably a lot easier,

1689
01:18:16,040 --> 01:18:20,000
but we already have systems that are over 95% accurate

1690
01:18:20,000 --> 01:18:24,800
at detecting deception from just visual video recordings.

1691
01:18:24,800 --> 01:18:27,480
We can see how all of this information

1692
01:18:27,480 --> 01:18:29,200
that we are continually shedding

1693
01:18:29,200 --> 01:18:32,800
gives rise to the possibility of alibiath

1694
01:18:32,800 --> 01:18:36,800
than either of the private or of the government's kind.

1695
01:18:36,800 --> 01:18:39,160
I wonder what role do you see

1696
01:18:39,160 --> 01:18:41,600
open-sourcing AI models playing here?

1697
01:18:41,600 --> 01:18:46,600
What are the trade-offs and risks in open-sourcing AI?

1698
01:18:46,600 --> 01:18:49,040
Among the people who are most bullish to open-source,

1699
01:18:49,040 --> 01:18:54,200
there's often a kind of libertarian ethic undergirding it.

1700
01:18:54,200 --> 01:18:58,520
Regardless of whether that's a good idea or not,

1701
01:18:58,520 --> 01:19:01,360
one of the things I'm trying to communicate to that group

1702
01:19:01,360 --> 01:19:04,320
is to say that be careful what you wish for

1703
01:19:04,320 --> 01:19:08,520
because of these kind of paradoxical Hobbesian dynamics.

1704
01:19:08,520 --> 01:19:09,880
The fact that in America,

1705
01:19:09,880 --> 01:19:12,520
you never know if someone has a gun or not.

1706
01:19:12,520 --> 01:19:15,320
On the one hand, the Second Amendment enhances our freedom.

1707
01:19:15,320 --> 01:19:17,840
On another hand, you don't get the sort of

1708
01:19:17,840 --> 01:19:21,000
like everyone's doors unlocked and people are,

1709
01:19:21,000 --> 01:19:24,440
like the police in England don't even have guns.

1710
01:19:24,440 --> 01:19:26,120
There's a certain freedom that derives

1711
01:19:26,120 --> 01:19:30,160
from us not all being heavily armed.

1712
01:19:32,120 --> 01:19:38,920
Likewise, with open-sourcing powerful AI capabilities,

1713
01:19:38,920 --> 01:19:41,480
it empowers you as an individual,

1714
01:19:41,480 --> 01:19:43,400
but in general equilibrium,

1715
01:19:43,400 --> 01:19:45,080
once we all have the capabilities,

1716
01:19:45,080 --> 01:19:47,800
the world could look much more oppressive

1717
01:19:47,800 --> 01:19:50,240
either because we're all spying on each other all the time

1718
01:19:50,240 --> 01:19:51,720
and we can all see through each other's walls

1719
01:19:51,720 --> 01:19:53,960
or because there's a backlash and the introduction

1720
01:19:53,960 --> 01:19:56,320
of the viathan type solutions

1721
01:19:56,320 --> 01:19:59,240
to restrict our ability to spy on each other all the time.

1722
01:19:59,920 --> 01:20:03,480
My general sense is that we can only delay

1723
01:20:03,480 --> 01:20:05,320
and we can't really prevent things

1724
01:20:05,320 --> 01:20:06,760
from being open-source over the long run

1725
01:20:06,760 --> 01:20:08,960
because there's a sort of trickle-down

1726
01:20:08,960 --> 01:20:10,560
of compute requirements.

1727
01:20:10,560 --> 01:20:12,360
But in the interim,

1728
01:20:12,360 --> 01:20:15,040
there are definitely things that are valuable to open-source,

1729
01:20:15,040 --> 01:20:19,360
having 70 billion parameter language models, not a threat.

1730
01:20:19,360 --> 01:20:22,480
In fact, I think it's probably useful for alignment research

1731
01:20:22,480 --> 01:20:25,080
for something like that to be open-source.

1732
01:20:25,080 --> 01:20:26,600
But if you are a researcher

1733
01:20:26,600 --> 01:20:30,160
and you've developed a emotional recognition model

1734
01:20:30,160 --> 01:20:34,880
that can tell if, you know, with like 99% accuracy,

1735
01:20:34,880 --> 01:20:38,240
whether someone is lying or not lying

1736
01:20:38,240 --> 01:20:40,720
and whether your girlfriend loves you or not,

1737
01:20:40,720 --> 01:20:44,760
like these things or the ability to see through walls

1738
01:20:44,760 --> 01:20:49,760
using, like I talk about the use of Wi-Fi displacement.

1739
01:20:49,760 --> 01:20:51,760
There are people who have built pose recognition models

1740
01:20:51,760 --> 01:20:55,480
using the displacement of the electromagnetic frequency

1741
01:20:55,480 --> 01:20:57,600
of your Wi-Fi and they can see,

1742
01:20:57,600 --> 01:21:00,520
they can, there's wall penetrating

1743
01:21:00,520 --> 01:21:01,560
so that you can see through walls.

1744
01:21:01,560 --> 01:21:06,560
Like, what's the rush to put that on hugging face

1745
01:21:07,400 --> 01:21:11,200
and to like make it as democratized as quickly as possible?

1746
01:21:11,200 --> 01:21:15,480
I would say that if we value the adaptation

1747
01:21:15,480 --> 01:21:20,320
and mitigation pathway as opposed to the Leviathan pathway,

1748
01:21:20,320 --> 01:21:22,800
then there's a value in, you know,

1749
01:21:22,800 --> 01:21:25,040
slow rolling some of these things.

1750
01:21:25,080 --> 01:21:28,360
How do you think government power will be,

1751
01:21:28,360 --> 01:21:32,080
or relative government power will be affected by AI?

1752
01:21:32,080 --> 01:21:35,840
So you write somewhere in this long series of blog posts

1753
01:21:35,840 --> 01:21:39,320
that AI will cause a net weakening of governments

1754
01:21:39,320 --> 01:21:41,360
relative to the private sector.

1755
01:21:41,360 --> 01:21:42,800
Why is that?

1756
01:21:42,800 --> 01:21:46,040
Yeah, specifically Western liberal governments

1757
01:21:46,040 --> 01:21:48,040
under constitutional constraints.

1758
01:21:49,040 --> 01:21:52,880
So if you imagine society being on this kind of knife edge,

1759
01:21:53,880 --> 01:21:55,960
I talked about this in the context of

1760
01:21:55,960 --> 01:21:58,440
Theranosso Mogul's book, The Neural Corridor,

1761
01:21:58,440 --> 01:21:59,760
where he describes liberal democracy

1762
01:21:59,760 --> 01:22:01,120
as sort of being in this corridor

1763
01:22:01,120 --> 01:22:03,040
between despotism on the one hand

1764
01:22:03,040 --> 01:22:04,480
and anarchy on the other.

1765
01:22:04,480 --> 01:22:08,240
And we sort of this day in this saddle path

1766
01:22:08,240 --> 01:22:11,480
where society and the state are kept in balance.

1767
01:22:11,480 --> 01:22:14,360
If you veer off that path, you can, on the one hand,

1768
01:22:14,360 --> 01:22:16,320
you know, the state could become all powerful

1769
01:22:16,320 --> 01:22:19,120
and that's the sort of China model

1770
01:22:19,120 --> 01:22:21,080
or authoritarian digital surveillance state.

1771
01:22:21,080 --> 01:22:23,720
And indeed, you know, China built up

1772
01:22:23,720 --> 01:22:24,920
their digital surveillance state

1773
01:22:24,920 --> 01:22:27,320
and their internet firewalls and so forth

1774
01:22:27,320 --> 01:22:29,440
after watching the Arab Spring

1775
01:22:29,440 --> 01:22:31,760
and seeing how the internet was destabilizing

1776
01:22:31,760 --> 01:22:33,320
to weaker governments.

1777
01:22:33,320 --> 01:22:36,720
And so I fully expect that AI will be very empowering

1778
01:22:36,720 --> 01:22:39,440
and self-reinforcing of the power of the Chinese government.

1779
01:22:39,440 --> 01:22:41,960
And indeed, there are draft regulations

1780
01:22:41,960 --> 01:22:44,680
for large language models stipulate

1781
01:22:44,680 --> 01:22:45,520
that you can't use the model

1782
01:22:45,520 --> 01:22:48,760
to undermine national unity or challenge the government.

1783
01:22:48,760 --> 01:22:50,200
And so they're baking that in.

1784
01:22:50,200 --> 01:22:53,520
In liberal democracies, we think of ourselves

1785
01:22:53,520 --> 01:22:54,640
as open societies.

1786
01:22:55,560 --> 01:22:59,640
And the issue is that we're only open at the meta level.

1787
01:22:59,640 --> 01:23:01,040
There's a public sphere, right?

1788
01:23:01,040 --> 01:23:02,520
There's freedom of information laws.

1789
01:23:02,520 --> 01:23:04,640
We have freedom of speech.

1790
01:23:04,640 --> 01:23:07,800
I don't have freedom of speech if I walk into a Walmart.

1791
01:23:07,800 --> 01:23:08,640
Wait, right?

1792
01:23:08,640 --> 01:23:10,080
The Walmart is private property.

1793
01:23:10,080 --> 01:23:12,240
In open societies, it's not that we don't have

1794
01:23:12,240 --> 01:23:15,880
social credit scores and forms of,

1795
01:23:15,880 --> 01:23:17,360
thicker forms of social regulation.

1796
01:23:17,360 --> 01:23:19,520
It's just that we offload those functions

1797
01:23:19,520 --> 01:23:22,080
onto competing private actors,

1798
01:23:22,080 --> 01:23:26,520
whether it's a church that has very strict doctrines

1799
01:23:26,520 --> 01:23:30,080
to be a member or other kinds of social clubs.

1800
01:23:30,080 --> 01:23:31,400
The fact that these days,

1801
01:23:31,400 --> 01:23:33,600
if you want to go to a comedy club,

1802
01:23:33,600 --> 01:23:35,400
they'll often confiscate your phone at the door

1803
01:23:35,400 --> 01:23:37,000
because they don't want you recording

1804
01:23:37,000 --> 01:23:39,760
the comedian's set and putting it online.

1805
01:23:39,760 --> 01:23:42,600
My anticipation is that because of those constitutional

1806
01:23:42,600 --> 01:23:47,360
constraints that limit the ability of liberal democracies

1807
01:23:47,400 --> 01:23:50,520
to go the China route, right?

1808
01:23:50,520 --> 01:23:54,080
Because of our civil laws or bills of rights and so forth.

1809
01:23:54,080 --> 01:23:56,360
And also because of a lot of these procedural constraints.

1810
01:23:56,360 --> 01:23:59,200
This will naturally shift into the private sector.

1811
01:23:59,200 --> 01:24:03,360
And we see that already with the use of AI

1812
01:24:03,360 --> 01:24:05,320
for monitoring and employment,

1813
01:24:05,320 --> 01:24:09,320
for policing speech in ways that would be illegal

1814
01:24:09,320 --> 01:24:10,160
if done by the state,

1815
01:24:10,160 --> 01:24:12,580
but are fine if done by Facebook.

1816
01:24:12,580 --> 01:24:14,720
To the extent that the AI continues

1817
01:24:14,720 --> 01:24:16,400
to increase these kind of negative externalities

1818
01:24:16,640 --> 01:24:18,720
and therefore puts more value

1819
01:24:18,720 --> 01:24:21,880
on having a sort of vertically integrated experience,

1820
01:24:21,880 --> 01:24:26,440
a walled garden that can strip out the negative forms of AI

1821
01:24:26,440 --> 01:24:29,880
and reinstate the degree of harmony between people

1822
01:24:30,920 --> 01:24:35,000
that more and more of our social life will be mediated

1823
01:24:35,000 --> 01:24:38,280
through these sort of private organizations

1824
01:24:38,280 --> 01:24:41,480
rather than through a kind of open public sphere.

1825
01:24:41,480 --> 01:24:45,200
Or you're imagining that government services

1826
01:24:45,200 --> 01:24:48,680
will be gradually replaced by private services

1827
01:24:48,680 --> 01:24:51,920
that are better able to respond.

1828
01:24:51,920 --> 01:24:55,480
Won't governments fight to uphold individual rights?

1829
01:24:55,480 --> 01:24:57,840
In Walmart or on Facebook,

1830
01:24:57,840 --> 01:24:59,760
you are regulated in ways

1831
01:24:59,760 --> 01:25:01,120
that the government couldn't regulate you,

1832
01:25:01,120 --> 01:25:04,200
but you still have the choice to go to a target

1833
01:25:04,200 --> 01:25:08,180
instead of Walmart or to go to or X instead of Facebook.

1834
01:25:08,180 --> 01:25:09,440
Isn't that the fundamental thing?

1835
01:25:09,440 --> 01:25:11,760
So the fundamental thing is the choice between services

1836
01:25:11,760 --> 01:25:15,440
and won't governments uphold citizens' rights

1837
01:25:15,440 --> 01:25:18,360
to make those kinds of choices?

1838
01:25:18,360 --> 01:25:19,760
Yeah, no, I agree.

1839
01:25:19,760 --> 01:25:24,760
And so this would be the defense of the liberal model

1840
01:25:24,880 --> 01:25:28,720
is that we allow thicker forms of social regulation

1841
01:25:28,720 --> 01:25:31,880
because it's moderated by choice and competition.

1842
01:25:31,880 --> 01:25:35,560
And the issue with Chinese Confucian integralism

1843
01:25:36,400 --> 01:25:41,400
isn't the fact that it's super oppressive.

1844
01:25:41,920 --> 01:25:44,240
It's the fact that you only have one choice

1845
01:25:44,240 --> 01:25:46,360
and you don't have voice or exit.

1846
01:25:46,360 --> 01:25:51,360
So, but it's obviously a matter of degree, right?

1847
01:25:52,880 --> 01:25:56,240
When ride hailing first arose,

1848
01:25:56,240 --> 01:25:59,960
I remember back in 2013, 2014, it wasn't that long ago.

1849
01:25:59,960 --> 01:26:03,240
I think Uber was founded in 2009,

1850
01:26:03,240 --> 01:26:07,000
but it really only started taking off in the early 2010s.

1851
01:26:07,000 --> 01:26:10,080
No, people thought it was crazy to ride a car

1852
01:26:10,080 --> 01:26:11,440
with a stranger.

1853
01:26:11,440 --> 01:26:13,120
And then within five years,

1854
01:26:13,120 --> 01:26:17,440
it was the dominant mode of ride hailing.

1855
01:26:17,440 --> 01:26:19,240
And in that five-year period,

1856
01:26:19,240 --> 01:26:22,680
essentially we saw a kind of regime change in micro

1857
01:26:22,680 --> 01:26:26,560
where taxis went from being something

1858
01:26:26,560 --> 01:26:27,760
that was regulated by the state

1859
01:26:27,760 --> 01:26:31,040
through these commissions that were granted,

1860
01:26:31,160 --> 01:26:35,880
legal monopolies and used licensing and exams

1861
01:26:35,880 --> 01:26:38,840
and other sort of brute force ways of ensuring quality

1862
01:26:39,800 --> 01:26:41,360
to competing private platforms

1863
01:26:41,360 --> 01:26:44,240
where you have Lyft or Uber to choose from.

1864
01:26:44,240 --> 01:26:48,000
And they replaced the explicit governance of legal mandates

1865
01:26:48,000 --> 01:26:52,640
with the competing governance of reputation mechanisms

1866
01:26:52,640 --> 01:26:57,640
of dispute resolution systems of structured marketplaces

1867
01:26:58,080 --> 01:27:00,880
that collapse the bargaining frictions, right?

1868
01:27:00,880 --> 01:27:02,600
You never have to haggle with an Uber driver,

1869
01:27:02,600 --> 01:27:03,960
you just sort of get in.

1870
01:27:03,960 --> 01:27:06,560
And that was obviously a much better way

1871
01:27:06,560 --> 01:27:09,520
of doing ride hailing.

1872
01:27:09,520 --> 01:27:12,160
So even though there was sort of a violent resistance early on,

1873
01:27:12,160 --> 01:27:13,080
literally like in France,

1874
01:27:13,080 --> 01:27:14,480
they were throwing rocks off of bridges

1875
01:27:14,480 --> 01:27:17,360
and cab drivers in New York were killing themselves.

1876
01:27:17,360 --> 01:27:18,440
So for the people affected,

1877
01:27:18,440 --> 01:27:20,400
it was a very dramatic sort of regime change,

1878
01:27:20,400 --> 01:27:23,520
but for everyone else, it was a huge positive improvement.

1879
01:27:23,520 --> 01:27:25,600
And yet it's only made possible

1880
01:27:25,600 --> 01:27:28,240
because Uber has a social credit score.

1881
01:27:28,240 --> 01:27:29,720
If your Uber rating goes too low,

1882
01:27:29,720 --> 01:27:31,680
you'll get kicked off the platform.

1883
01:27:31,680 --> 01:27:34,440
And so we're fine with social credit scores.

1884
01:27:34,440 --> 01:27:38,280
It's when you only have one and don't have an option

1885
01:27:38,280 --> 01:27:41,120
and it can follow you across all these different verticals

1886
01:27:41,120 --> 01:27:42,200
that becomes a problem.

1887
01:27:42,200 --> 01:27:46,240
Do you imagine that because of rising danger in the world,

1888
01:27:46,240 --> 01:27:48,320
you talk about the externalities

1889
01:27:48,320 --> 01:27:53,320
from the widespread implementation of AI all across society

1890
01:27:54,320 --> 01:27:56,920
because of those dangers, those externalities,

1891
01:27:56,920 --> 01:27:59,960
you know, you will either use Uber or whatever service

1892
01:27:59,960 --> 01:28:03,200
or you kind of can't participate in society.

1893
01:28:03,200 --> 01:28:06,400
Do you imagine increased pressure in that direction?

1894
01:28:06,400 --> 01:28:08,640
It does seem to be a longer-term trend.

1895
01:28:08,640 --> 01:28:11,600
I don't know if AI will accelerate it.

1896
01:28:11,600 --> 01:28:16,840
I have another series of essays that I call separation anxiety.

1897
01:28:17,840 --> 01:28:21,880
And it's a reference to the fact that in insurance markets,

1898
01:28:22,840 --> 01:28:24,320
there's kind of two equilibria.

1899
01:28:24,320 --> 01:28:25,720
There's the pooling equilibria

1900
01:28:25,720 --> 01:28:28,160
where we're pooled together into one risk pool

1901
01:28:28,160 --> 01:28:29,680
and there's a separating equilibria

1902
01:28:29,680 --> 01:28:31,800
where the insurance pool unravels

1903
01:28:31,800 --> 01:28:35,000
and we break up into the great power insurance

1904
01:28:35,000 --> 01:28:37,840
for senior citizens who'd never had an accident

1905
01:28:37,840 --> 01:28:39,040
and stuff like that.

1906
01:28:39,040 --> 01:28:40,320
And it turns out that insurance markets

1907
01:28:40,320 --> 01:28:41,720
are competitively unstable

1908
01:28:41,720 --> 01:28:46,640
that without government regulation or social insurance,

1909
01:28:46,640 --> 01:28:50,080
that insurance markets will naturally tend to unravel

1910
01:28:50,080 --> 01:28:51,480
because of adverse selection

1911
01:28:51,480 --> 01:28:54,920
into, you know, the high-risk people being in one pool

1912
01:28:54,920 --> 01:28:57,360
and the low-risk people being in another pool.

1913
01:28:57,360 --> 01:28:59,360
And it turns out you can sort of use that as a mental model

1914
01:28:59,360 --> 01:29:03,800
to look at other kinds of implicit pooling equilibria, right?

1915
01:29:03,800 --> 01:29:08,800
So within company wage distributions,

1916
01:29:09,160 --> 01:29:11,960
often there is, you know, 20% of the workers

1917
01:29:11,960 --> 01:29:13,760
who are doing 80% of the work,

1918
01:29:13,760 --> 01:29:17,280
but they're pooled together under one wage structure.

1919
01:29:17,280 --> 01:29:19,400
And that was sort of the dominant structure

1920
01:29:20,000 --> 01:29:22,400
of the period of wage compression

1921
01:29:22,400 --> 01:29:24,760
in the United States in the 50s and 60s.

1922
01:29:24,760 --> 01:29:26,680
And once we had better monitoring technologies

1923
01:29:26,680 --> 01:29:28,560
and were able to tell who were the 20%

1924
01:29:28,560 --> 01:29:30,000
that were doing 80% of the work,

1925
01:29:30,000 --> 01:29:35,000
it suddenly became possible to differentiate pay structure

1926
01:29:35,320 --> 01:29:38,240
and a lot of the rise and inequality in the United States

1927
01:29:38,240 --> 01:29:40,800
is actually between firm.

1928
01:29:40,800 --> 01:29:42,080
So what happens is, you know,

1929
01:29:42,080 --> 01:29:44,200
Ezra Klein is like the most productive wiz kid

1930
01:29:44,200 --> 01:29:45,600
at the Washington Post and he realizes,

1931
01:29:45,600 --> 01:29:48,640
why don't I just go start my own website, right?

1932
01:29:48,640 --> 01:29:50,400
And so that dynamics are played out

1933
01:29:50,400 --> 01:29:52,960
across a variety of domains,

1934
01:29:52,960 --> 01:29:55,160
leads to a world that, you know,

1935
01:29:55,160 --> 01:29:57,760
to the extent that these features are correlated,

1936
01:29:57,760 --> 01:29:59,480
that does separate, right?

1937
01:29:59,480 --> 01:30:02,640
Where you have, you know, the one-star Uber riders

1938
01:30:02,640 --> 01:30:04,800
driving the one-star Uber drivers,

1939
01:30:04,800 --> 01:30:06,640
the drivers driving the riders.

1940
01:30:06,640 --> 01:30:08,760
And, you know, people who have, you know,

1941
01:30:08,760 --> 01:30:11,760
the five-star Uber ratings and the perfect credit scores,

1942
01:30:11,760 --> 01:30:13,880
self-sort into communities with other people

1943
01:30:13,880 --> 01:30:15,840
with perfect driving records and perfect credit scores.

1944
01:30:16,080 --> 01:30:18,600
And, you know, we see that to an extent already

1945
01:30:18,600 --> 01:30:21,560
with the, you know, enclaves of, you know,

1946
01:30:21,560 --> 01:30:23,800
rich zip codes with private schools

1947
01:30:23,800 --> 01:30:26,320
and everyone is sort of self-selected.

1948
01:30:26,320 --> 01:30:29,920
AI could, it seems to me that AI would exacerbate that.

1949
01:30:29,920 --> 01:30:31,860
I mean, at first blush, just because it,

1950
01:30:31,860 --> 01:30:33,640
going back to the point about signal extraction,

1951
01:30:33,640 --> 01:30:36,080
it can find all these different ways,

1952
01:30:36,080 --> 01:30:38,800
you're a high-risk type and I'm a low-risk type and so forth,

1953
01:30:38,800 --> 01:30:42,200
that are probably latent in all kinds of data

1954
01:30:42,200 --> 01:30:43,480
that we don't even need to get permission

1955
01:30:43,480 --> 01:30:44,320
to the insurance company.

1956
01:30:44,320 --> 01:30:46,360
They'll just like, the same way that they use,

1957
01:30:46,360 --> 01:30:49,800
like smoking or going to a gym as a proxy,

1958
01:30:49,800 --> 01:30:51,200
there's all kinds of proxies they could use

1959
01:30:51,200 --> 01:30:54,140
and likewise for employers and how they pay people.

1960
01:30:54,140 --> 01:30:59,080
Society kind of runs on us not being entirely open

1961
01:30:59,080 --> 01:31:00,800
and entirely honest all the time.

1962
01:31:00,800 --> 01:31:03,320
Otherwise, you wouldn't be able to have

1963
01:31:03,320 --> 01:31:06,200
kind of smooth social interactions and so on.

1964
01:31:06,200 --> 01:31:09,920
Won't these norms be inherited by the way we use AI?

1965
01:31:09,920 --> 01:31:13,000
Yeah, I think this is a really big issue.

1966
01:31:13,000 --> 01:31:14,560
And I'm a big fan of Robin Hansen

1967
01:31:14,560 --> 01:31:19,560
and a lot of his writing on social status and signaling

1968
01:31:20,160 --> 01:31:24,600
is sort of presenting humans as basically hypocrites,

1969
01:31:24,600 --> 01:31:28,680
like we're constantly deceiving other people

1970
01:31:28,680 --> 01:31:31,060
and we often deceive ourselves

1971
01:31:31,060 --> 01:31:33,360
so it's better to deceive others

1972
01:31:33,360 --> 01:31:38,160
as the evolutionary biologist Robert Trivers

1973
01:31:38,160 --> 01:31:39,000
has pointed out.

1974
01:31:40,000 --> 01:31:45,000
So, all the kinds of polite lies that we tell

1975
01:31:45,800 --> 01:31:49,280
are I think critical lubricants to the social interaction

1976
01:31:49,280 --> 01:31:53,800
and actually like it's good that there's a gap

1977
01:31:53,800 --> 01:31:55,880
between our stated and revealed preference.

1978
01:31:55,880 --> 01:31:57,680
I think a world where we all lived

1979
01:31:57,680 --> 01:32:00,240
our stated preference could be hellish

1980
01:32:00,240 --> 01:32:01,880
because we don't actually mean it.

1981
01:32:04,400 --> 01:32:06,760
And AI has a direct implication on that

1982
01:32:06,800 --> 01:32:09,120
because if I can have a pair of AR glasses on

1983
01:32:09,120 --> 01:32:10,920
that will tell me if you're interested,

1984
01:32:10,920 --> 01:32:13,680
if you're bored, if you're over on a date

1985
01:32:13,680 --> 01:32:16,720
and are you really attracted to me,

1986
01:32:17,800 --> 01:32:22,160
all that sort of polite veneer that social veil

1987
01:32:22,160 --> 01:32:24,720
could be lifted in a way that

1988
01:32:24,720 --> 01:32:28,520
we'll probably want to coordinate to not do, right?

1989
01:32:28,520 --> 01:32:30,240
But again, it's this Nash equilibrium

1990
01:32:30,240 --> 01:32:31,760
where it's in my interest to know

1991
01:32:31,760 --> 01:32:34,400
whether you're interested or bored.

1992
01:32:34,400 --> 01:32:36,120
And so I'll wanna have the glasses on

1993
01:32:36,120 --> 01:32:39,000
and my ideal world is where only I have the glasses

1994
01:32:39,000 --> 01:32:39,840
and you don't.

1995
01:32:41,320 --> 01:32:44,560
And the other way that our hypocrisy is being exposed

1996
01:32:44,560 --> 01:32:48,400
and challenged is the need to explicate

1997
01:32:48,400 --> 01:32:50,280
the utility function that we want these models

1998
01:32:50,280 --> 01:32:51,440
to work under.

1999
01:32:52,720 --> 01:32:55,360
We need to formalize human values

2000
01:32:55,360 --> 01:32:57,160
if we want to align these models.

2001
01:32:57,160 --> 01:32:59,600
And so then we have to be honest and open

2002
01:32:59,600 --> 01:33:02,400
about the fact that our stated preferences

2003
01:33:02,400 --> 01:33:05,040
probably aren't our true preferences.

2004
01:33:05,040 --> 01:33:07,320
And that's a very challenging thing

2005
01:33:07,320 --> 01:33:11,600
because it cuts right to the nature of the human condition

2006
01:33:11,600 --> 01:33:15,080
and involves topics that are intrinsically things

2007
01:33:15,080 --> 01:33:17,320
that we lie to ourselves about.

2008
01:33:17,320 --> 01:33:19,960
You have what you call a timeline

2009
01:33:19,960 --> 01:33:22,880
of a techno feudalist future,

2010
01:33:22,880 --> 01:33:24,640
which I found quite interesting.

2011
01:33:24,640 --> 01:33:27,440
Yeah, it's great writing and it's very detailed.

2012
01:33:27,440 --> 01:33:29,760
We don't have to go through it in all of its detail,

2013
01:33:29,760 --> 01:33:32,200
but maybe you could tell the story of what happens

2014
01:33:32,200 --> 01:33:34,280
in what you call the default scenario.

2015
01:33:34,280 --> 01:33:37,440
This is the scenario in which Western liberal democracies

2016
01:33:37,440 --> 01:33:39,320
are too slow to adapt to AI.

2017
01:33:39,320 --> 01:33:41,400
And so we get something like a replacement

2018
01:33:41,400 --> 01:33:44,520
of government services with more private services.

2019
01:33:44,520 --> 01:33:48,000
What happens in the techno feudalist future?

2020
01:33:48,000 --> 01:33:49,960
Right, and it's sort of piggybacks

2021
01:33:49,960 --> 01:33:51,800
in everything you've just been discussing, right?

2022
01:33:51,800 --> 01:33:53,960
And I don't want techno feudalists

2023
01:33:53,960 --> 01:33:56,160
to carry too much of a pejorative.

2024
01:33:56,160 --> 01:33:57,880
I'm sort of using it descriptively.

2025
01:33:59,000 --> 01:34:03,680
And certainly some people would prefer this world

2026
01:34:04,320 --> 01:34:07,880
so the example of Uber and Lyft displacing taxi caps

2027
01:34:07,880 --> 01:34:10,400
is sort of a version of this in micro

2028
01:34:10,400 --> 01:34:13,960
where we go from this regulated taxi commission

2029
01:34:13,960 --> 01:34:17,000
to competing private platforms that use various forms

2030
01:34:17,000 --> 01:34:21,240
of artificial intelligence and information technology

2031
01:34:21,240 --> 01:34:23,560
to replace the thing that was being done

2032
01:34:23,560 --> 01:34:25,080
by explicit regulation.

2033
01:34:26,040 --> 01:34:30,760
And as AI progresses and both creates a variety

2034
01:34:30,760 --> 01:34:32,000
of new negative externalities,

2035
01:34:32,040 --> 01:34:34,880
whether it's like suicide drones

2036
01:34:37,200 --> 01:34:39,400
or the ability to spy on each other,

2037
01:34:40,480 --> 01:34:43,600
there's going to be a demand for new forms of security

2038
01:34:43,600 --> 01:34:47,880
and also kinds of opt-in jurisdictions

2039
01:34:47,880 --> 01:34:49,400
that tie our hands in the same way

2040
01:34:49,400 --> 01:34:54,000
that we give up our phone before we go into the comedy club.

2041
01:34:54,000 --> 01:34:57,360
And so I think this leads to a kind of development

2042
01:34:57,360 --> 01:35:01,480
of clubs, the kind of club structure

2043
01:35:01,480 --> 01:35:03,120
that may be at the city level

2044
01:35:03,120 --> 01:35:06,760
as the vertically integrated walled garden

2045
01:35:06,760 --> 01:35:11,280
that will police and build defensive technologies

2046
01:35:11,280 --> 01:35:14,320
around the misuse of AI and at the same time

2047
01:35:14,320 --> 01:35:18,200
provide a variety of like new AI native public goods

2048
01:35:18,200 --> 01:35:21,480
that are only possible once AI unlocks them.

2049
01:35:21,480 --> 01:35:24,600
And it's easy to see how this could very quickly displace

2050
01:35:25,520 --> 01:35:29,360
and eat away at formal government services

2051
01:35:29,360 --> 01:35:31,640
both because we saw it already with Uber,

2052
01:35:31,640 --> 01:35:34,560
but also if you map that model

2053
01:35:34,560 --> 01:35:37,120
to other areas of regulatory life,

2054
01:35:38,680 --> 01:35:42,040
does it make sense to have a USDA farm inspector,

2055
01:35:42,040 --> 01:35:44,240
a human person has to go to a commercial farm

2056
01:35:44,240 --> 01:35:47,160
and maybe only goes to that farm once every few years

2057
01:35:47,160 --> 01:35:49,800
because there's so many farms and only so many people.

2058
01:35:49,800 --> 01:35:51,320
And it does a little checklist and says,

2059
01:35:51,320 --> 01:35:53,320
oh, you're not abusing the animals

2060
01:35:53,320 --> 01:35:55,360
and you get all the process in place

2061
01:35:55,360 --> 01:35:58,080
and you get the USDA stamp of approval

2062
01:35:58,080 --> 01:36:02,960
or does it make more sense to have multimodal cameras on

2063
01:36:02,960 --> 01:36:07,960
in the farm 24-7 that are continuously generating reports

2064
01:36:08,680 --> 01:36:10,960
that throw up a red flag anytime someone sneezes

2065
01:36:10,960 --> 01:36:12,080
on the conveyor belt.

2066
01:36:12,080 --> 01:36:15,920
And to the extent that government is going to be slow

2067
01:36:15,920 --> 01:36:19,440
at adopting that, will there be a push

2068
01:36:19,440 --> 01:36:23,800
for the kind of Uber model of governance as a platform

2069
01:36:23,800 --> 01:36:27,320
where you have the kind of AI underwriter,

2070
01:36:27,320 --> 01:36:31,920
the consumer reports that sells these farms,

2071
01:36:31,920 --> 01:36:33,800
the camera technology and the monitoring technology

2072
01:36:33,800 --> 01:36:36,920
and builds their own set of compliant standards.

2073
01:36:36,920 --> 01:36:38,720
And then you want to go to those farms

2074
01:36:38,720 --> 01:36:40,880
or would have you that have the stamp of approval

2075
01:36:40,880 --> 01:36:43,600
of the underwriter because it's much higher trust.

2076
01:36:43,600 --> 01:36:47,160
It's sort of like the end of asymmetric information.

2077
01:36:47,160 --> 01:36:52,160
And you can map that from food safety to product safety

2078
01:36:53,400 --> 01:36:56,720
to OSHA and workplace safety.

2079
01:36:56,720 --> 01:36:57,960
There's other parts of government

2080
01:36:57,960 --> 01:37:00,600
that maybe just rendered completely obsolete, right?

2081
01:37:00,600 --> 01:37:02,000
Like once we have self-driving cars

2082
01:37:02,000 --> 01:37:04,760
that are a thousand next more safe than humans,

2083
01:37:04,760 --> 01:37:07,560
do we need a national highway traffic safety administration?

2084
01:37:09,160 --> 01:37:12,160
Once we have sensors that are privately owned everywhere

2085
01:37:12,160 --> 01:37:14,640
and can model weather patterns better

2086
01:37:14,640 --> 01:37:17,280
than the national oceanic administration,

2087
01:37:17,280 --> 01:37:19,320
do we need a national weather service

2088
01:37:19,320 --> 01:37:21,440
or could we bootstrap that ourselves?

2089
01:37:21,440 --> 01:37:24,880
And then once we have AI,

2090
01:37:24,880 --> 01:37:27,080
accelerated drug discovery,

2091
01:37:27,080 --> 01:37:30,960
do we want to rely on the FDA to be a kind of choke point

2092
01:37:30,960 --> 01:37:34,680
to do these sort of frequentist clinical trials

2093
01:37:34,680 --> 01:37:38,560
that are inherently slow and don't capture

2094
01:37:38,560 --> 01:37:43,400
the kind of idiosyncrasies and heterogeneity

2095
01:37:43,400 --> 01:37:46,440
that could be unlocked by personalized medicine?

2096
01:37:46,440 --> 01:37:51,040
Or do we move to an alternative drug approval process

2097
01:37:51,040 --> 01:37:52,280
that is maybe non-governmental,

2098
01:37:52,280 --> 01:37:55,440
but much more rapid and much more personalized?

2099
01:37:55,440 --> 01:37:56,880
So that's the overall picture.

2100
01:37:56,880 --> 01:37:59,960
I'll just run through the timeline here,

2101
01:37:59,960 --> 01:38:02,160
picking up on some of your comments

2102
01:38:02,160 --> 01:38:04,880
that I thought were especially interesting.

2103
01:38:04,880 --> 01:38:08,760
You write, this is in 2024 to 2027,

2104
01:38:08,760 --> 01:38:13,280
you write that the internet will become vulcanized

2105
01:38:13,280 --> 01:38:18,280
and it will become more secure and more private in a sense.

2106
01:38:18,600 --> 01:38:20,040
Why does that happen?

2107
01:38:20,040 --> 01:38:23,400
We're already starting to see this a little bit, right?

2108
01:38:23,400 --> 01:38:26,040
Once people realize that the data

2109
01:38:26,040 --> 01:38:27,840
that's being generated on Stack Overflow

2110
01:38:27,840 --> 01:38:30,400
or Reddit or whatever is valuable

2111
01:38:30,400 --> 01:38:31,440
for training these models,

2112
01:38:31,440 --> 01:38:33,240
suddenly everyone's closing their API

2113
01:38:34,560 --> 01:38:37,640
and consequently Google Search and the Google index

2114
01:38:37,640 --> 01:38:40,720
have sort of started to degrade already.

2115
01:38:40,720 --> 01:38:42,240
So I think that will continue

2116
01:38:42,240 --> 01:38:46,120
for the kind of privatization of data reasons.

2117
01:38:46,120 --> 01:38:47,800
Then when you also think about

2118
01:38:47,800 --> 01:38:52,280
how websites are going to handle sort of the growth of bots

2119
01:38:52,280 --> 01:38:55,040
and catfishes and catfish attacks

2120
01:38:55,040 --> 01:38:58,240
and cyber attacks and so forth,

2121
01:38:58,240 --> 01:38:59,760
it makes sense that we're going to move

2122
01:38:59,760 --> 01:39:04,080
from a sort of open, everything goes kind of Twitter-esque

2123
01:39:04,080 --> 01:39:07,640
platform to things that are much more closed

2124
01:39:07,640 --> 01:39:11,920
because they require human verification

2125
01:39:11,920 --> 01:39:15,560
and identity verification to sort of build the trust

2126
01:39:15,600 --> 01:39:18,560
that you're talking to other people and not deepfakes.

2127
01:39:18,560 --> 01:39:20,600
And then medium-term, again,

2128
01:39:20,600 --> 01:39:23,240
over this sort of 2024 to 2027 horizon,

2129
01:39:24,200 --> 01:39:27,160
you could also start to see the emergence

2130
01:39:27,160 --> 01:39:32,160
of intelligent malware, sort of modern AI native cyber attacks

2131
01:39:34,240 --> 01:39:38,320
that could be devastating to legacy cyber security

2132
01:39:38,320 --> 01:39:41,640
infrastructure in a way that I talk about good heart

2133
01:39:41,640 --> 01:39:43,240
and back to the famous Moore's worm

2134
01:39:43,240 --> 01:39:45,240
that in the late 80s,

2135
01:39:45,240 --> 01:39:46,720
basically shut down the early internet.

2136
01:39:46,720 --> 01:39:48,880
They literally had to partition the internet

2137
01:39:48,880 --> 01:39:53,080
and turn it off so they could read the network at the worm.

2138
01:39:53,080 --> 01:39:54,200
So for all those reasons,

2139
01:39:54,200 --> 01:39:56,120
I think you start to see the internet balkanize

2140
01:39:56,120 --> 01:39:58,160
and then particularly at the international level,

2141
01:39:58,160 --> 01:40:02,280
we're already starting to see sort of the semiconductor

2142
01:40:02,280 --> 01:40:05,320
supply chain become critical part of national security.

2143
01:40:05,320 --> 01:40:07,680
The growth of the Chinese firewall,

2144
01:40:07,680 --> 01:40:10,960
the European Union is going to have to have their own

2145
01:40:10,960 --> 01:40:13,160
quasi firewall and they kind of already do with GDPR

2146
01:40:13,160 --> 01:40:15,880
and the EU AI Act.

2147
01:40:15,880 --> 01:40:17,600
And so the kind of nationalization of compute

2148
01:40:17,600 --> 01:40:19,240
and telecommunications infrastructure

2149
01:40:19,240 --> 01:40:22,120
that will take off once people understand

2150
01:40:22,120 --> 01:40:24,480
both the security risks and the value prop

2151
01:40:24,480 --> 01:40:28,400
of owning the infrastructure for the AI revolution.

2152
01:40:28,400 --> 01:40:31,960
Yeah, in 2028 to 2031,

2153
01:40:31,960 --> 01:40:35,800
you write about alignment turning out to be easier

2154
01:40:35,800 --> 01:40:39,560
than we thought with the increasing scale of the model.

2155
01:40:39,760 --> 01:40:41,040
That was somewhat surprising to me.

2156
01:40:41,040 --> 01:40:43,520
Why does alignment turn out to be easier?

2157
01:40:44,760 --> 01:40:47,960
And part of this is imagining a scenario

2158
01:40:47,960 --> 01:40:50,040
where alignment is easy.

2159
01:40:50,040 --> 01:40:53,880
So we can talk about what happens if alignment is easy.

2160
01:40:53,880 --> 01:40:55,200
But I think there are reasons to think

2161
01:40:55,200 --> 01:40:56,880
that the classic alignment problem

2162
01:40:56,880 --> 01:40:59,360
will be easier than people think.

2163
01:40:59,360 --> 01:41:02,000
I think that some of the early intuitions

2164
01:41:02,000 --> 01:41:04,680
about the hardness of the alignment problem

2165
01:41:04,680 --> 01:41:07,520
were rooted in a view of maybe AI turns out

2166
01:41:07,520 --> 01:41:10,480
to be a very simple algorithm

2167
01:41:10,480 --> 01:41:13,480
rather than like a deep neural network

2168
01:41:13,480 --> 01:41:16,280
that achieves its generality because of its depth.

2169
01:41:16,280 --> 01:41:19,760
Clearly the kind of value,

2170
01:41:19,760 --> 01:41:22,320
I forget what Eliezer Kewski called it,

2171
01:41:22,320 --> 01:41:25,880
but there's like a value alignment problem

2172
01:41:25,880 --> 01:41:29,120
where how do we teach the model our values?

2173
01:41:29,120 --> 01:41:32,240
But that part of the alignment problem seems trivial now

2174
01:41:32,240 --> 01:41:35,240
because our large English models aren't like

2175
01:41:35,240 --> 01:41:39,800
autistic savants, they're actually incredibly sensitive

2176
01:41:39,800 --> 01:41:44,800
to soft human concepts of value and context.

2177
01:41:45,640 --> 01:41:47,480
They're not going to have a,

2178
01:41:47,480 --> 01:41:49,800
the paperclip maximizer sort of monkey paw

2179
01:41:49,800 --> 01:41:53,040
kind of threat models don't really make sense in that world.

2180
01:41:53,040 --> 01:41:56,640
But there's a difference between the output of the model

2181
01:41:56,640 --> 01:42:00,280
and the weights or what the model has learned.

2182
01:42:00,280 --> 01:42:03,400
And so just because a model can say,

2183
01:42:03,440 --> 01:42:05,400
it can say the right words that we wanted to say,

2184
01:42:05,400 --> 01:42:07,640
but what has it actually learned?

2185
01:42:07,640 --> 01:42:09,040
We are not entirely sure.

2186
01:42:09,040 --> 01:42:12,280
And so it has learned to satisfy human values to some extent,

2187
01:42:12,280 --> 01:42:16,560
but has it learned to want to comply with human value

2188
01:42:18,320 --> 01:42:21,600
out of distribution sort of, yeah, in other domains

2189
01:42:21,600 --> 01:42:25,120
and in a deep sense, I'm not sure about that.

2190
01:42:25,120 --> 01:42:25,960
No, I agree.

2191
01:42:25,960 --> 01:42:27,920
So I'm sort of just laying some of my groundwork

2192
01:42:27,920 --> 01:42:30,640
for to expand my priors on this.

2193
01:42:30,640 --> 01:42:32,440
I agree, like, you know, reinforcement learning

2194
01:42:32,440 --> 01:42:35,760
from human feedback is not alignment.

2195
01:42:37,200 --> 01:42:38,640
You know, the same way that, you know,

2196
01:42:38,640 --> 01:42:41,720
you could argue that like the co-evolution of cats and dogs

2197
01:42:41,720 --> 01:42:44,620
with humans led to a kind of reinforcement learning

2198
01:42:44,620 --> 01:42:48,320
from human feedback in their, in their short run evolution

2199
01:42:48,320 --> 01:42:50,840
that, you know, made them look up, appear as if they,

2200
01:42:50,840 --> 01:42:54,080
you know, they experienced guilt and shame

2201
01:42:54,080 --> 01:42:55,880
and these human emotions when in fact they're,

2202
01:42:55,880 --> 01:42:58,080
they're just sort of a simulacra of those emotions

2203
01:42:58,080 --> 01:43:01,320
because it means that we'll give them a treat.

2204
01:43:01,320 --> 01:43:04,720
But I've done plenty of episodes on deceptions

2205
01:43:04,720 --> 01:43:06,280
in these models and so on.

2206
01:43:06,280 --> 01:43:07,960
We don't have to go through that,

2207
01:43:07,960 --> 01:43:09,840
but I just wanted to point out that, yeah,

2208
01:43:09,840 --> 01:43:11,880
maybe there's some complexities there.

2209
01:43:11,880 --> 01:43:14,680
So the first, my first prior is that these models

2210
01:43:14,680 --> 01:43:18,680
aren't autistic savants the way they might have been.

2211
01:43:18,680 --> 01:43:22,240
The second is going back to universality.

2212
01:43:22,240 --> 01:43:26,720
Well, it is true that you, there are, that, you know,

2213
01:43:26,720 --> 01:43:28,920
it's possible through reinforcement learning

2214
01:43:28,920 --> 01:43:30,000
from human feedback, for example,

2215
01:43:30,120 --> 01:43:32,800
that you, you're, you're not selecting for honesty

2216
01:43:32,800 --> 01:43:36,280
or selecting for a deep pick up honesty,

2217
01:43:36,280 --> 01:43:38,760
but in the bigger picture, the intuition

2218
01:43:38,760 --> 01:43:40,960
that these models are converging or convergent

2219
01:43:40,960 --> 01:43:44,960
with human representations should give you some confidence

2220
01:43:44,960 --> 01:43:46,680
that they're not going to be as alien

2221
01:43:46,680 --> 01:43:48,600
as we, as we think they will be.

2222
01:43:48,600 --> 01:43:53,600
It's also useful input for thinking about interpretability.

2223
01:43:53,880 --> 01:43:56,880
You know, some recent work showing that discussing

2224
01:43:56,880 --> 01:43:58,520
sort of representation, interpretability

2225
01:43:58,520 --> 01:44:01,320
where, where instead of trying to interpret

2226
01:44:01,320 --> 01:44:03,560
individual neurons, you interpret sort of collections

2227
01:44:03,560 --> 01:44:06,480
of neurons and, and, and circuitry

2228
01:44:06,480 --> 01:44:09,680
through sort of human interpretable representations.

2229
01:44:09,680 --> 01:44:11,400
And one of the, one of the lessons of universality

2230
01:44:11,400 --> 01:44:13,640
is that like some of these high level human concepts,

2231
01:44:13,640 --> 01:44:17,400
like happiness or, or anxiety,

2232
01:44:17,400 --> 01:44:22,280
like these seem like vague psychological abstractions

2233
01:44:22,280 --> 01:44:24,240
that there's no way they can correspond

2234
01:44:24,240 --> 01:44:26,800
to like the micro foundations of the way our brain works.

2235
01:44:26,800 --> 01:44:29,640
But in fact, they may actually be very efficient,

2236
01:44:29,640 --> 01:44:31,400
low dimensional ways of talking about

2237
01:44:31,400 --> 01:44:32,720
what's happening in our brain.

2238
01:44:32,720 --> 01:44:34,760
And then the third thing is, I think that I just

2239
01:44:34,760 --> 01:44:37,440
have seen, you know, my sense is that the work

2240
01:44:37,440 --> 01:44:40,480
on interpretability is actually making some,

2241
01:44:40,480 --> 01:44:42,360
some good progress, you know,

2242
01:44:42,360 --> 01:44:43,920
whether it can scale is another question,

2243
01:44:43,920 --> 01:44:46,360
but I think we'll get there.

2244
01:44:46,360 --> 01:44:49,760
In my timeline, I talk about sort of AGI level models

2245
01:44:49,760 --> 01:44:53,240
within the human limit, human emulator plus domain.

2246
01:44:53,240 --> 01:44:55,280
I do later on talk about like super intelligence

2247
01:44:55,280 --> 01:44:57,360
emerging maybe in the 2040s.

2248
01:44:57,360 --> 01:44:58,440
And that's another story, right?

2249
01:44:58,440 --> 01:45:01,440
And so I think some of this stuff maybe goes out the window

2250
01:45:01,440 --> 01:45:03,360
if we have, you know, models that are bigger

2251
01:45:03,360 --> 01:45:06,080
than all the brains combined and have like

2252
01:45:06,080 --> 01:45:08,240
strong situational awareness.

2253
01:45:08,240 --> 01:45:11,000
But I don't think that happens this decade.

2254
01:45:11,000 --> 01:45:14,360
Certainly, certainly not with the current way

2255
01:45:14,360 --> 01:45:15,200
we're building these models,

2256
01:45:15,200 --> 01:45:16,560
with the way we're currently building these models,

2257
01:45:16,560 --> 01:45:18,720
I think it's comes much closer to a stimuli

2258
01:45:18,720 --> 01:45:20,200
lack of the human brain.

2259
01:45:20,200 --> 01:45:21,040
Got it.

2260
01:45:21,040 --> 01:45:26,040
In 2036 to 2039, you talk about robotics

2261
01:45:27,480 --> 01:45:29,240
being solved to the same extent,

2262
01:45:29,240 --> 01:45:30,960
or maybe even in the same way

2263
01:45:30,960 --> 01:45:33,720
as we are now solving a language.

2264
01:45:33,720 --> 01:45:35,640
That would, I found that super interesting.

2265
01:45:35,640 --> 01:45:39,600
Explain to me why, why would robotics

2266
01:45:39,600 --> 01:45:43,680
suddenly or quite relatively suddenly become much easier?

2267
01:45:43,680 --> 01:45:45,840
Robotics have been fighting for decades

2268
01:45:45,840 --> 01:45:49,640
to get these models to walk relatively

2269
01:45:50,640 --> 01:45:54,400
unencumbered and it's been an uphill battle.

2270
01:45:54,400 --> 01:45:57,880
Yeah, why can we solve robotics in the 2030s?

2271
01:45:57,880 --> 01:46:00,720
This may end up happening sooner than I'd project,

2272
01:46:00,720 --> 01:46:03,760
but I mean, if you look at LLMs,

2273
01:46:03,760 --> 01:46:06,320
what one of the stylized sort of trends

2274
01:46:06,320 --> 01:46:09,080
with large language models is that, you know,

2275
01:46:09,080 --> 01:46:13,240
that natural language processing went from being this,

2276
01:46:13,240 --> 01:46:15,160
you know, the study of how to make machines

2277
01:46:15,160 --> 01:46:17,560
understand language went from being, you know,

2278
01:46:17,640 --> 01:46:19,520
a dozen different sub-disciplines,

2279
01:46:19,520 --> 01:46:20,840
you know, people working on parsing,

2280
01:46:20,840 --> 01:46:22,000
people are working on syntax,

2281
01:46:22,000 --> 01:46:22,960
people are working on semantics,

2282
01:46:22,960 --> 01:46:26,360
people are working on summarization and classification.

2283
01:46:26,360 --> 01:46:28,480
And these are all different, you know, directions,

2284
01:46:28,480 --> 01:46:29,560
research directions.

2285
01:46:29,560 --> 01:46:31,680
And then along comes transformer models

2286
01:46:31,680 --> 01:46:34,280
and, you know, it's just supplants everything

2287
01:46:34,280 --> 01:46:36,360
and LLMs can do it all.

2288
01:46:36,360 --> 01:46:40,520
And I think robotics is sort of still in that ancient regime

2289
01:46:40,520 --> 01:46:43,880
where a lot of, you know, what Boston Dynamics does

2290
01:46:43,880 --> 01:46:46,080
is ad hoc control models,

2291
01:46:46,080 --> 01:46:49,960
analytically solvable, you know, differential equations,

2292
01:46:49,960 --> 01:46:52,680
different kinds of object recognition modules

2293
01:46:52,680 --> 01:46:56,280
and control action loops and so forth.

2294
01:46:56,280 --> 01:46:58,920
And so it's still in that like early NLP phase

2295
01:46:58,920 --> 01:47:01,080
where they have 12 different sub-disciplines

2296
01:47:01,080 --> 01:47:02,880
and they're sort of mashing them together.

2297
01:47:02,880 --> 01:47:05,200
And of course you get something that's not very robust.

2298
01:47:05,200 --> 01:47:08,920
I think we're already starting to see that paradigm shift

2299
01:47:08,920 --> 01:47:12,440
to, you know, end-to-end neural network trained models,

2300
01:47:12,480 --> 01:47:15,600
like, you know, Tesla, for instance,

2301
01:47:16,680 --> 01:47:19,840
I think one of the reasons why Tesla cars

2302
01:47:19,840 --> 01:47:23,960
had a sort of temporary decline in performance

2303
01:47:23,960 --> 01:47:25,840
was because they were undergoing the transition

2304
01:47:25,840 --> 01:47:28,080
from these ad hoc lane detectors

2305
01:47:28,080 --> 01:47:30,640
and stop sign detectors and stuff like that

2306
01:47:30,640 --> 01:47:34,920
to a fully end-to-end neural network transformer-based model.

2307
01:47:34,920 --> 01:47:37,880
And that turned out to be much more robust way

2308
01:47:37,880 --> 01:47:40,240
to train the model because, you know,

2309
01:47:40,240 --> 01:47:41,760
stop signs look different in different countries

2310
01:47:41,760 --> 01:47:43,760
and like maybe stop sign isn't the thing you care about,

2311
01:47:43,760 --> 01:47:45,560
really, so on and so forth.

2312
01:47:46,920 --> 01:47:49,160
And so I think the transformer sort of scale,

2313
01:47:49,160 --> 01:47:53,600
deep learning revolution is only now coming to robotics

2314
01:47:53,600 --> 01:47:56,920
and people in that field have, are a little bit cynical

2315
01:47:56,920 --> 01:48:01,520
because they're used to relatively small RL models

2316
01:48:01,520 --> 01:48:05,040
thinking that like the fit with, you know, actuators

2317
01:48:05,040 --> 01:48:08,160
and some of the hardware is like a really challenging problem

2318
01:48:08,160 --> 01:48:11,440
and also believing that we don't have the data sets

2319
01:48:11,440 --> 01:48:14,920
for it, but then you look at, you know,

2320
01:48:14,920 --> 01:48:18,560
there's recent RoboDog that you may have seen on Twitter,

2321
01:48:18,560 --> 01:48:21,160
fully open source robot model

2322
01:48:21,160 --> 01:48:23,600
for a Boston Dynamics style dog.

2323
01:48:23,600 --> 01:48:26,960
It was trained on H100s, you know,

2324
01:48:26,960 --> 01:48:31,200
10,000 human years of training and simulation

2325
01:48:31,200 --> 01:48:34,400
and then some fine-tuning on real-world data

2326
01:48:34,400 --> 01:48:39,000
and they have a very robust robot control model

2327
01:48:39,000 --> 01:48:40,600
that you could plug into all kinds

2328
01:48:40,600 --> 01:48:43,720
of different form factors and have something that can,

2329
01:48:43,720 --> 01:48:45,840
you know, hop gaps and climb stairs

2330
01:48:45,840 --> 01:48:49,640
and do all the things that Boston Dynamics robots

2331
01:48:49,640 --> 01:48:52,040
don't do very well outside of their distribution.

2332
01:48:52,040 --> 01:48:54,200
You think we'll have a general purpose algorithm

2333
01:48:54,200 --> 01:48:58,280
that we can plug into basically arbitrarily shaped robots

2334
01:48:58,280 --> 01:49:01,720
that can then navigate the, navigate our apartments

2335
01:49:01,720 --> 01:49:05,040
or our construction sites or maybe our highways.

2336
01:49:05,040 --> 01:49:08,200
That's an interesting vision.

2337
01:49:08,200 --> 01:49:11,800
Why is it that we achieve this level of generality?

2338
01:49:11,800 --> 01:49:14,920
If you look at humans, you know, humans are very good at,

2339
01:49:14,920 --> 01:49:16,840
you know, if we've suffered an amputation

2340
01:49:16,840 --> 01:49:18,680
or you have to go through physical therapy

2341
01:49:18,680 --> 01:49:20,880
and it's not easy necessarily,

2342
01:49:20,880 --> 01:49:25,280
but humans are able to adapt to different kinds

2343
01:49:25,280 --> 01:49:29,000
of physical layouts of our body.

2344
01:49:29,000 --> 01:49:32,160
And I think there will be a trend

2345
01:49:32,160 --> 01:49:37,000
towards unified robotic control models

2346
01:49:37,000 --> 01:49:40,520
that aren't like super tailored to, you know,

2347
01:49:40,520 --> 01:49:44,040
two legs and two arms and so on and so forth.

2348
01:49:44,040 --> 01:49:44,960
You know, once you've installed it

2349
01:49:44,960 --> 01:49:46,320
through a little bit of in-context learning

2350
01:49:46,320 --> 01:49:49,080
or fine-tuning or reinforcement learning,

2351
01:49:49,080 --> 01:49:51,600
adapt to that particular form factor.

2352
01:49:51,600 --> 01:49:53,640
And this will parallel the kind

2353
01:49:53,640 --> 01:49:56,640
of pre-trained foundation model paradigm

2354
01:49:56,640 --> 01:49:59,120
that is currently taking place in LLMs

2355
01:49:59,120 --> 01:50:02,280
where you have like the really big foundation model

2356
01:50:02,280 --> 01:50:04,680
that can sort of do everything reasonably well

2357
01:50:04,680 --> 01:50:06,320
and then you can fine-tune it beyond that.

2358
01:50:06,320 --> 01:50:09,280
If we get to the 2040s in your timeline,

2359
01:50:09,280 --> 01:50:13,320
you talk about massive amounts of compute being available.

2360
01:50:13,320 --> 01:50:16,800
You talk about post-scarcity in everything

2361
01:50:16,800 --> 01:50:20,040
except for land and capital.

2362
01:50:20,040 --> 01:50:22,040
And then you also talk about the development

2363
01:50:22,040 --> 01:50:25,520
potentially of superintelligence at that point.

2364
01:50:25,520 --> 01:50:27,120
What happens there?

2365
01:50:27,120 --> 01:50:30,720
Who is in control of the superintelligence, if anyone?

2366
01:50:30,720 --> 01:50:32,200
Yeah, this is sort of where I start

2367
01:50:32,200 --> 01:50:33,680
to get a little bit tongue-in-cheek,

2368
01:50:33,680 --> 01:50:40,680
but first of all, I talk about how I tend to think

2369
01:50:40,680 --> 01:50:45,760
that once we have exascale computing and I think DOE

2370
01:50:45,760 --> 01:50:47,960
just built their first exascale computer,

2371
01:50:47,960 --> 01:50:49,400
and maybe it was private company,

2372
01:50:49,400 --> 01:50:52,440
but we have like one exascale computer in the world.

2373
01:50:52,440 --> 01:50:55,840
By the 2040s, they'll be commonplace.

2374
01:50:55,840 --> 01:50:58,840
And if we are ever worried about sort of controlling

2375
01:50:58,840 --> 01:51:03,480
the supply of GPUs, I don't know exactly

2376
01:51:03,560 --> 01:51:05,760
how much compute will be on our smartphones,

2377
01:51:05,760 --> 01:51:10,920
but it will definitely be possible to train a GP5 model

2378
01:51:10,920 --> 01:51:12,600
from your home computer.

2379
01:51:12,600 --> 01:51:15,920
And so any kind of AICT regime that we build today

2380
01:51:15,920 --> 01:51:19,120
that doesn't take into account that falling costs of compute

2381
01:51:19,120 --> 01:51:20,800
will probably break down.

2382
01:51:20,800 --> 01:51:25,320
And therefore, amid this broader sort of fragmentation

2383
01:51:25,320 --> 01:51:29,400
of the machinery of government, the state,

2384
01:51:29,400 --> 01:51:31,680
I expect more and more government functions

2385
01:51:31,720 --> 01:51:36,440
to be offloaded into basically private cities,

2386
01:51:36,440 --> 01:51:38,520
HOAs, GATIC communities.

2387
01:51:38,520 --> 01:51:41,160
And likewise with the internet, I expect more and more

2388
01:51:41,160 --> 01:51:45,840
of our sort of permissioning regime for new AI models

2389
01:51:45,840 --> 01:51:49,640
and deployment to shift to the infrastructure layer

2390
01:51:49,640 --> 01:51:52,640
where telecommunication providers will be monitoring

2391
01:51:52,640 --> 01:51:56,760
network traffic for unvetted AI models and so forth,

2392
01:51:56,760 --> 01:51:58,640
and we'll have like Chinese style firewalls

2393
01:51:58,880 --> 01:52:01,800
that are specific to a particular local area network.

2394
01:52:02,880 --> 01:52:05,440
And at that point, the world looks,

2395
01:52:05,440 --> 01:52:07,440
the United States where this takes place

2396
01:52:07,440 --> 01:52:11,520
looks more like an archipelago of micro jurisdictions.

2397
01:52:11,520 --> 01:52:15,720
I tend to think that like a post scarcity

2398
01:52:15,720 --> 01:52:19,480
political economy looks a lot like the Gulf States,

2399
01:52:19,480 --> 01:52:20,520
Gulf State monarchies, right?

2400
01:52:20,520 --> 01:52:22,600
Because Gulf State monarchies are basically

2401
01:52:22,600 --> 01:52:23,760
living post scarcity, right?

2402
01:52:23,760 --> 01:52:26,240
They have a spigot of oil they can turn on,

2403
01:52:26,480 --> 01:52:29,200
and then they can go build mega projects in the desert,

2404
01:52:29,200 --> 01:52:30,640
and they have like infinite labor

2405
01:52:30,640 --> 01:52:33,480
because they can just import guest workers.

2406
01:52:33,480 --> 01:52:35,800
And so you end up with like this,

2407
01:52:35,800 --> 01:52:37,760
but if we can't have a Gulf State monarchy

2408
01:52:37,760 --> 01:52:39,360
in the United States, instead we have a bunch

2409
01:52:39,360 --> 01:52:44,160
of micro monarchies dotting the country.

2410
01:52:44,160 --> 01:52:47,200
So I sort of jokingly say, you know,

2411
01:52:48,080 --> 01:52:51,080
who's going to stop the free city of California

2412
01:52:51,080 --> 01:52:54,600
that's like home to all the trillionaire ML engineers

2413
01:52:54,600 --> 01:52:56,920
and tech founders from the decade prior

2414
01:52:56,920 --> 01:53:01,800
from plugging in their humanity sized supercomputer

2415
01:53:01,800 --> 01:53:03,800
into a fusion reactor and turning it on.

2416
01:53:03,800 --> 01:53:06,800
Yeah, and this is really your kind of end point

2417
01:53:06,800 --> 01:53:10,440
of the discussion or your main point

2418
01:53:10,440 --> 01:53:12,760
of institutions being eroded,

2419
01:53:12,760 --> 01:53:17,760
and then afterwards being unable to respond to strong AI.

2420
01:53:18,600 --> 01:53:20,720
Yeah, and leading up to this,

2421
01:53:20,720 --> 01:53:23,440
it sounds like a scary dystopian type of thing.

2422
01:53:23,440 --> 01:53:25,560
It doesn't have to be, right?

2423
01:53:26,600 --> 01:53:29,680
Uber is not dystopian, Airbnb is not dystopian,

2424
01:53:29,680 --> 01:53:32,120
private airports in other countries are way better

2425
01:53:32,120 --> 01:53:35,400
than the public airports in the United States.

2426
01:53:35,400 --> 01:53:38,320
So privatization and the sort of techno feudalist paradigm

2427
01:53:38,320 --> 01:53:40,320
doesn't have to be bad,

2428
01:53:40,320 --> 01:53:43,440
but what it is is more adversarial, right?

2429
01:53:43,440 --> 01:53:46,320
And you know, people have sometimes speculated,

2430
01:53:46,320 --> 01:53:50,080
you know, did the crumbling of the Roman Empire

2431
01:53:50,080 --> 01:53:53,280
was a kind of prerequisite to a renaissance, right?

2432
01:53:53,280 --> 01:53:55,440
Because it allowed for these principalities

2433
01:53:55,440 --> 01:53:59,480
to sort of compete and to get the Florentine,

2434
01:53:59,480 --> 01:54:01,240
you know, creativity and so forth.

2435
01:54:01,240 --> 01:54:03,880
I think, you know, the next couple of decades

2436
01:54:03,880 --> 01:54:05,520
could similarly be a renaissance

2437
01:54:05,520 --> 01:54:09,120
for science and technology and for understanding the world,

2438
01:54:09,120 --> 01:54:11,880
but it's probably a renaissance

2439
01:54:11,880 --> 01:54:14,560
because we'll be moving into a much more competitive

2440
01:54:14,560 --> 01:54:17,200
adversarial world where, you know,

2441
01:54:17,200 --> 01:54:21,440
these city-states and so forth will be hard to coordinate.

2442
01:54:21,440 --> 01:54:23,880
And so to the extent that there are still these,

2443
01:54:23,880 --> 01:54:28,800
like, meta-risks where we would value some large-scale,

2444
01:54:28,800 --> 01:54:31,800
intra- and international coordination,

2445
01:54:31,800 --> 01:54:33,320
like peace treaties and so forth,

2446
01:54:33,320 --> 01:54:36,200
the disintegration of the United States

2447
01:54:36,200 --> 01:54:38,400
where this revolution is occurring

2448
01:54:38,400 --> 01:54:40,400
would be bad for that.

2449
01:54:41,280 --> 01:54:45,160
You talk about or you hint at an alternative path.

2450
01:54:45,160 --> 01:54:48,040
What we've been talking about your timeline here

2451
01:54:48,040 --> 01:54:49,640
is the default path.

2452
01:54:49,640 --> 01:54:52,760
You hint at a path where we have something

2453
01:54:52,760 --> 01:54:55,520
you call constrained leviath.

2454
01:54:55,520 --> 01:54:57,520
What is constrained leviath?

2455
01:54:57,520 --> 01:54:58,760
It's the limited government, right?

2456
01:54:58,760 --> 01:55:03,080
So this is a D'Arnaz and Mogul's word

2457
01:55:03,080 --> 01:55:04,600
for it from the narrow corridor.

2458
01:55:04,600 --> 01:55:09,600
And if you trace the rise of sort of what we associate

2459
01:55:10,200 --> 01:55:11,480
with liberal democracy,

2460
01:55:11,480 --> 01:55:15,560
it is part of a particular technological equilibrium,

2461
01:55:15,560 --> 01:55:17,000
in particular an equilibrium

2462
01:55:17,000 --> 01:55:20,160
that favored centralized governments

2463
01:55:20,160 --> 01:55:22,200
with impersonal rule of law

2464
01:55:22,200 --> 01:55:25,520
and impersonal tax administration and so on and so forth.

2465
01:55:25,520 --> 01:55:27,400
So we associate today with libertarians

2466
01:55:27,400 --> 01:55:28,720
with like being anti-government,

2467
01:55:28,720 --> 01:55:31,080
but the basic idea of liberalism

2468
01:55:31,080 --> 01:55:34,240
is actually associated with strong government,

2469
01:55:34,240 --> 01:55:35,840
a strong impersonal government

2470
01:55:35,840 --> 01:55:37,520
that can impose the rule of law.

2471
01:55:37,520 --> 01:55:40,480
And so if we want to maintain that kind of equilibrium

2472
01:55:41,560 --> 01:55:44,760
in a world where AI is diffusing on the society level

2473
01:55:44,800 --> 01:55:48,120
faster than it is on the state and elite level,

2474
01:55:49,080 --> 01:55:51,240
then we want to accelerate the diffusion

2475
01:55:51,240 --> 01:55:52,880
of AI within government.

2476
01:55:52,880 --> 01:55:55,200
And there's obviously lots of low hanging fruit.

2477
01:55:55,200 --> 01:55:56,760
We talked about how bureaucracies

2478
01:55:56,760 --> 01:55:58,800
are basically fleshy APIs.

2479
01:55:58,800 --> 01:56:02,840
Even today, I have a friend at the FTC,

2480
01:56:02,840 --> 01:56:04,560
the Federal Trade Commission.

2481
01:56:04,560 --> 01:56:05,760
They have like a 30 person team

2482
01:56:05,760 --> 01:56:09,080
that is part of the healthcare division

2483
01:56:09,080 --> 01:56:11,480
and they're in charge of policing

2484
01:56:11,480 --> 01:56:13,440
the entire pharmaceutical industry

2485
01:56:13,440 --> 01:56:16,320
in the United States for competition.

2486
01:56:16,320 --> 01:56:18,800
His day job right now looks like manually

2487
01:56:18,800 --> 01:56:20,160
reading through 40,000 emails

2488
01:56:20,160 --> 01:56:23,440
that they subpoenaed from a pharmaceutical CEO, right?

2489
01:56:23,440 --> 01:56:25,680
And today you could take those emails

2490
01:56:25,680 --> 01:56:28,480
and put them into a Claude II or something

2491
01:56:28,480 --> 01:56:31,080
like it with a big context window and ask,

2492
01:56:31,080 --> 01:56:34,880
find me the five most egregious examples of misconduct.

2493
01:56:34,880 --> 01:56:36,360
And it would do that.

2494
01:56:36,360 --> 01:56:37,120
It might not be perfect,

2495
01:56:37,120 --> 01:56:39,360
but it's a hell of a lot more efficient

2496
01:56:39,360 --> 01:56:41,000
than reading through them manually.

2497
01:56:41,000 --> 01:56:43,000
And obviously big law is going to be doing that.

2498
01:56:43,000 --> 01:56:46,440
And the Pharma CEO and his personal attorneys

2499
01:56:46,440 --> 01:56:48,480
will be doing that conversely.

2500
01:56:48,480 --> 01:56:53,160
To maintain our state capacity in the face of AI

2501
01:56:53,160 --> 01:56:56,320
is to run in this arms race.

2502
01:56:56,320 --> 01:57:00,480
And you can kind of liken it to an evolutionary biology

2503
01:57:00,480 --> 01:57:02,440
they call the Rig Queen dynamic,

2504
01:57:02,440 --> 01:57:04,240
which comes from Alice in Wonderland

2505
01:57:04,240 --> 01:57:05,600
where the Rig Queen tells Alice

2506
01:57:05,600 --> 01:57:07,920
that sometimes you need to run just to stay in place.

2507
01:57:07,920 --> 01:57:10,240
And so I think our government needs to be adopting

2508
01:57:10,240 --> 01:57:12,120
this technology as rapidly as possible

2509
01:57:12,120 --> 01:57:14,680
so that they can basically tread water.

2510
01:57:14,680 --> 01:57:19,200
And that means both diffusing it in existing institutions,

2511
01:57:19,200 --> 01:57:22,600
but also being open to radical reconfigurations

2512
01:57:22,600 --> 01:57:24,760
of the machinery of government

2513
01:57:24,760 --> 01:57:27,880
and addressing some of those firmware level constraints

2514
01:57:27,880 --> 01:57:28,720
that we talked about,

2515
01:57:28,720 --> 01:57:31,680
whether it's the lack of a national identification system

2516
01:57:31,680 --> 01:57:36,200
or the outdated atmoded information technology infrastructure

2517
01:57:36,200 --> 01:57:39,920
or the accumulation of old procedural

2518
01:57:39,920 --> 01:57:42,120
kinds of methods of governance.

2519
01:57:42,120 --> 01:57:46,080
A focused way of doing this is what you've called for

2520
01:57:46,080 --> 01:57:47,880
in a political article,

2521
01:57:47,880 --> 01:57:51,400
which is a Manhattan project for AI safety.

2522
01:57:51,400 --> 01:57:52,720
A first question here,

2523
01:57:52,720 --> 01:57:55,640
would it be better to call it an Apollo project

2524
01:57:55,640 --> 01:57:57,600
as opposed to a Manhattan project?

2525
01:57:57,600 --> 01:57:59,000
I mean, the Manhattan project

2526
01:57:59,000 --> 01:58:01,160
created some pretty dangerous weapons,

2527
01:58:01,160 --> 01:58:04,320
whereas the Apollo project might have been more benign.

2528
01:58:04,320 --> 01:58:05,800
I mean, what the Apollo project

2529
01:58:05,800 --> 01:58:07,160
and the Manhattan project have in common

2530
01:58:07,160 --> 01:58:09,000
is that they came from an era of US government

2531
01:58:09,040 --> 01:58:10,720
where we still dealt things,

2532
01:58:10,720 --> 01:58:12,720
where we still had competent state capacity,

2533
01:58:12,720 --> 01:58:14,920
where we still had a lot of in-house expertise

2534
01:58:14,920 --> 01:58:17,840
and we weren't saddled with all these constraints.

2535
01:58:17,840 --> 01:58:21,840
So today, we couldn't go to the moon in 10 years,

2536
01:58:21,840 --> 01:58:24,520
NASA couldn't, SpaceX can.

2537
01:58:24,520 --> 01:58:28,120
And so our modern Apollo projects

2538
01:58:28,120 --> 01:58:29,560
are being done by the private sector

2539
01:58:29,560 --> 01:58:31,880
through competitive contracts.

2540
01:58:31,880 --> 01:58:34,080
And so one of the messages of my piece

2541
01:58:34,080 --> 01:58:36,120
on the Manhattan project is to say,

2542
01:58:36,120 --> 01:58:37,400
the reason I make this analogy

2543
01:58:37,400 --> 01:58:41,640
is not just because AI is a Oppenheimer like technology,

2544
01:58:41,640 --> 01:58:43,600
but also because responding to it

2545
01:58:43,600 --> 01:58:47,520
will require a throwback to those kind of institutional forms

2546
01:58:47,520 --> 01:58:51,480
where we gave the people at the top a lot of discretion

2547
01:58:51,480 --> 01:58:53,440
and sort of gave them an outcome

2548
01:58:53,440 --> 01:58:55,160
and let them solve for that outcome

2549
01:58:55,160 --> 01:58:57,720
without having much of prescriptive rules

2550
01:58:57,720 --> 01:58:59,720
about how to solve for that outcome.

2551
01:58:59,720 --> 01:59:02,920
And then the second reason to make the analogy is,

2552
01:59:02,920 --> 01:59:04,320
open AI and anthropic,

2553
01:59:05,320 --> 01:59:07,800
they both have contingency plans

2554
01:59:09,080 --> 01:59:14,080
for developing AGI and having like a runaway market power, right?

2555
01:59:15,520 --> 01:59:16,520
And in the case of open AI,

2556
01:59:16,520 --> 01:59:18,880
it's their nonprofit structure.

2557
01:59:18,880 --> 01:59:21,920
In the case of anthropic, it's their public benefit trust

2558
01:59:21,920 --> 01:59:23,240
where they both are envisioning a world

2559
01:59:23,240 --> 01:59:25,000
where they could potentially be the first to build AGI

2560
01:59:25,000 --> 01:59:26,280
and become basically trillionaires.

2561
01:59:26,280 --> 01:59:27,520
And so at that point,

2562
01:59:27,520 --> 01:59:31,280
they need to become basically governed by a nonprofit board.

2563
01:59:32,280 --> 01:59:33,840
You know, at that point,

2564
01:59:34,840 --> 01:59:36,600
and that's not where progress ends, obviously,

2565
01:59:36,600 --> 01:59:38,720
like there's going to be continued research.

2566
01:59:38,720 --> 01:59:41,760
It would make sense for the US government to step in

2567
01:59:41,760 --> 01:59:44,280
and say, let's do this as a joint venture,

2568
01:59:44,280 --> 01:59:45,720
or we're no longer competing.

2569
01:59:45,720 --> 01:59:48,640
In fact, the basic structures of capitalism

2570
01:59:48,640 --> 01:59:51,320
and market competition are starting to break down.

2571
01:59:51,320 --> 01:59:53,800
Let's just pull this together into a joint venture,

2572
01:59:53,800 --> 01:59:58,280
study the things that require huge amounts of capital

2573
01:59:58,280 --> 02:00:00,920
that the private sector doesn't have, but the government can.

2574
02:00:01,520 --> 02:00:03,560
The US government spent $26 billion

2575
02:00:03,560 --> 02:00:05,400
on the Manhattan Project in today's dollars.

2576
02:00:05,400 --> 02:00:08,400
When you think about the financial resources

2577
02:00:08,400 --> 02:00:11,360
of nation-state actors to put behind scaling,

2578
02:00:11,360 --> 02:00:14,320
it's nothing like what Microsoft or Google have.

2579
02:00:14,320 --> 02:00:19,120
What's our first $200 billion training run, right?

2580
02:00:19,120 --> 02:00:21,000
What kind of things can come out of that?

2581
02:00:21,000 --> 02:00:23,120
I think that's something that you want to do

2582
02:00:23,120 --> 02:00:25,560
with the Defense Department's involvement

2583
02:00:25,560 --> 02:00:28,360
and working with these companies in a joint way

2584
02:00:28,360 --> 02:00:30,760
through secured data centers

2585
02:00:30,760 --> 02:00:33,520
and doing gain-of-function-style research

2586
02:00:33,520 --> 02:00:35,240
that really is dangerous

2587
02:00:36,480 --> 02:00:41,040
and more Manhattan Project than Apollo Project.

2588
02:00:41,040 --> 02:00:42,800
What would be the advantages here?

2589
02:00:42,800 --> 02:00:47,360
We would be able to slow down capabilities research

2590
02:00:47,360 --> 02:00:49,840
and spend more of the resources

2591
02:00:49,840 --> 02:00:52,120
on, say, mechanistic interpretability

2592
02:00:52,120 --> 02:00:57,120
or evaluations or alignment in general,

2593
02:00:57,480 --> 02:01:00,480
because now the top AI corporations

2594
02:01:00,480 --> 02:01:02,560
have kind of combined their efforts

2595
02:01:02,560 --> 02:01:04,880
on the one government group.

2596
02:01:04,880 --> 02:01:07,640
Yeah, and in my vision,

2597
02:01:07,640 --> 02:01:12,160
they're still allowed to pursue their commercial verticals.

2598
02:01:12,160 --> 02:01:14,680
And I have an extended version of the proposal

2599
02:01:14,680 --> 02:01:19,720
where I talk about needing sort of bio-safety-style categories

2600
02:01:19,720 --> 02:01:23,400
for high-risk, medium-risk, and low-risk styles of AI

2601
02:01:23,400 --> 02:01:25,000
that very closely parallels

2602
02:01:25,000 --> 02:01:27,360
what Anthropic recently put out with their recommendations

2603
02:01:27,360 --> 02:01:31,120
for sort of a BSL categorization of AI research.

2604
02:01:31,120 --> 02:01:33,720
So I'm really talking about that BSL4 lab

2605
02:01:33,720 --> 02:01:35,640
and beyond-style stuff.

2606
02:01:35,640 --> 02:01:37,200
And some of that stuff,

2607
02:01:37,200 --> 02:01:40,720
some of it will be to accelerate alignment

2608
02:01:40,720 --> 02:01:42,040
and interpretability research

2609
02:01:42,040 --> 02:01:46,040
to sort of do versions of the OpenAI Superalignment Project

2610
02:01:46,040 --> 02:01:48,400
where they're dedicating 20% of their compute

2611
02:01:48,400 --> 02:01:49,960
to study alignment.

2612
02:01:49,960 --> 02:01:53,000
Another part of it will be to forestall

2613
02:01:53,000 --> 02:01:55,480
competitive race-to-the-bottom dynamics

2614
02:01:55,480 --> 02:02:00,480
so that they can coordinate and not violate antitrust laws.

2615
02:02:01,200 --> 02:02:04,240
And then the third thing is sort of the gain-of-function stuff

2616
02:02:04,240 --> 02:02:06,080
that we really only want to be doing

2617
02:02:06,080 --> 02:02:10,720
with very strict oversight, compartmentalization,

2618
02:02:10,720 --> 02:02:14,120
kind of pooling of talent and resources

2619
02:02:14,120 --> 02:02:18,120
so we can share knowledge on alignment and safety.

2620
02:02:19,000 --> 02:02:22,560
But then also because government has this huge spending power,

2621
02:02:23,000 --> 02:02:24,520
relative to the product sector,

2622
02:02:24,520 --> 02:02:26,360
anytime you build a supercomputer,

2623
02:02:26,360 --> 02:02:29,000
you're basically borrowing from the future.

2624
02:02:29,000 --> 02:02:31,400
You're trying to see what like the smartphones

2625
02:02:31,400 --> 02:02:34,200
20 years from now will be capable of.

2626
02:02:34,200 --> 02:02:38,000
And so if we want to sort of get ahead of the curve

2627
02:02:38,000 --> 02:02:39,720
and see where scaling is leading,

2628
02:02:39,720 --> 02:02:42,080
then I think governments are really the only actor

2629
02:02:42,080 --> 02:02:43,840
that can waste a bunch of money

2630
02:02:43,840 --> 02:02:45,920
basically scaling up a system

2631
02:02:45,920 --> 02:02:48,280
and seeing what comes out of it.

2632
02:02:48,280 --> 02:02:51,680
Yeah, when we talk about gain-of-function research in AI,

2633
02:02:51,680 --> 02:02:55,080
it's an analogy to the gain-of-function research

2634
02:02:55,080 --> 02:02:59,000
that's done on viruses in biolabs, but done for AI models.

2635
02:02:59,000 --> 02:03:01,840
And this could be experimenting

2636
02:03:01,840 --> 02:03:04,520
with creating more agent-like models

2637
02:03:04,520 --> 02:03:08,200
or inducing deception in a model

2638
02:03:08,200 --> 02:03:10,400
and planting it in a simulated environment,

2639
02:03:10,400 --> 02:03:15,400
seeing what it does or enticing it to acquire more resources.

2640
02:03:15,840 --> 02:03:19,760
But again, perhaps in a safely,

2641
02:03:19,760 --> 02:03:21,680
if this is even possible

2642
02:03:21,680 --> 02:03:24,960
in a safely constrained, simulated environment.

2643
02:03:24,960 --> 02:03:27,440
And this is the type of research that we could do

2644
02:03:27,440 --> 02:03:30,560
in this Manhattan project, this government lab,

2645
02:03:30,560 --> 02:03:33,400
because we would have excellent cybersecurity

2646
02:03:33,400 --> 02:03:36,520
and secure data centers and the combined efforts

2647
02:03:36,520 --> 02:03:40,840
of the most capable people in AI research.

2648
02:03:40,840 --> 02:03:43,320
If you've watched Oppenheimer, the movie,

2649
02:03:43,320 --> 02:03:46,040
a lot of that revolved around suspicions

2650
02:03:46,040 --> 02:03:48,640
of coming to spies and so on.

2651
02:03:48,640 --> 02:03:51,400
And we really don't have great insight

2652
02:03:51,400 --> 02:03:55,960
into the operational security of the major AGI labs.

2653
02:03:55,960 --> 02:03:58,520
And that's something that bringing it in house

2654
02:03:58,520 --> 02:04:00,920
of the defense department,

2655
02:04:00,920 --> 02:04:04,320
they would necessarily have to disclose

2656
02:04:04,320 --> 02:04:06,560
everything they're doing, but also hopefully

2657
02:04:06,560 --> 02:04:08,600
beef up their operational security.

2658
02:04:08,600 --> 02:04:12,160
Yeah, they're kind of stuck with a startup mindset,

2659
02:04:12,160 --> 02:04:15,440
but they're not developing a startup product.

2660
02:04:15,440 --> 02:04:18,000
They're developing something that, in my opinion,

2661
02:04:18,120 --> 02:04:20,960
could be more dangerous than the average startup.

2662
02:04:20,960 --> 02:04:22,920
Yeah, and Dari Amade has said as much

2663
02:04:22,920 --> 02:04:25,720
that we should just assume that there are Chinese spies

2664
02:04:25,720 --> 02:04:28,000
at all the major AI companies

2665
02:04:28,000 --> 02:04:29,360
and at Microsoft and Google.

2666
02:04:29,360 --> 02:04:32,720
When we think about gain of function research in AI,

2667
02:04:32,720 --> 02:04:36,200
how do you think about the value of gaining information

2668
02:04:36,200 --> 02:04:37,880
about what the models can do

2669
02:04:37,880 --> 02:04:42,320
and what the models can do versus the risk we're running?

2670
02:04:42,320 --> 02:04:47,320
It would be a tragic and ironic death for humanity

2671
02:04:47,520 --> 02:04:50,920
if we experimented with dangerous AI models

2672
02:04:50,920 --> 02:04:53,400
to see whether they would destroy us

2673
02:04:53,400 --> 02:04:55,560
and then we hadn't constrained them properly

2674
02:04:55,560 --> 02:04:57,760
and they actually destroyed us.

2675
02:04:57,760 --> 02:04:59,960
So how do you think of that trade-off

2676
02:04:59,960 --> 02:05:04,960
between gaining information and avoiding lab leaks?

2677
02:05:05,000 --> 02:05:08,720
Yeah, hopefully lab leaks are less likely

2678
02:05:08,720 --> 02:05:12,120
than in the biology context where, you know,

2679
02:05:12,120 --> 02:05:15,960
getting a little bit of blood or urine on your shoes

2680
02:05:15,960 --> 02:05:17,440
as you walk at the door.

2681
02:05:17,440 --> 02:05:20,120
Now, it's a difficult thing to talk about in part

2682
02:05:20,120 --> 02:05:22,520
because we just went through a pandemic

2683
02:05:22,520 --> 02:05:26,880
that very probably was caused by a BSL4 lab leak.

2684
02:05:26,880 --> 02:05:32,040
And so, you know, one saving grace is that AI models

2685
02:05:32,040 --> 02:05:33,960
don't get caught in your respiratory system.

2686
02:05:36,520 --> 02:05:39,000
And so hopefully there's forms of compartmentalization

2687
02:05:39,000 --> 02:05:42,360
that are much more robust than in the biology context.

2688
02:05:42,360 --> 02:05:45,200
And to the extent that this research

2689
02:05:45,240 --> 02:05:46,880
is going to be done anyway,

2690
02:05:46,880 --> 02:05:49,640
you know, it would be much better to move it off-site

2691
02:05:49,640 --> 02:05:52,720
and hopefully in a way that facilities are air-gapped

2692
02:05:52,720 --> 02:05:54,680
and so forth, rather than, you know,

2693
02:05:54,680 --> 02:05:56,360
what Microsoft is doing right now,

2694
02:05:56,360 --> 02:05:59,600
that Microsoft just recently announced their AutoGen AI,

2695
02:05:59,600 --> 02:06:02,240
which are sort of agent-based models,

2696
02:06:03,200 --> 02:06:07,040
very similar to like AutoGPT, but like that work.

2697
02:06:07,040 --> 02:06:10,800
And they're doing this through Creative Commons,

2698
02:06:10,800 --> 02:06:13,480
totally open source framework.

2699
02:06:13,480 --> 02:06:16,280
All this capabilities work is gain a function research,

2700
02:06:16,280 --> 02:06:18,600
where we draw the line between doing things

2701
02:06:18,600 --> 02:06:20,080
that are intentionally dangerous

2702
02:06:20,080 --> 02:06:21,400
or doing things that are dangerous,

2703
02:06:21,400 --> 02:06:26,080
but we're kind of pretending that they're not, is hard.

2704
02:06:26,080 --> 02:06:29,200
I do think there's, and Paul Cristiano is also agreed

2705
02:06:29,200 --> 02:06:31,000
with this sort of threat models

2706
02:06:31,000 --> 02:06:34,840
that would be valuable to be running in virtual machines

2707
02:06:34,840 --> 02:06:38,600
and to see, you know, if the AI develops awareness,

2708
02:06:38,600 --> 02:06:40,720
situational awareness and tries to escape,

2709
02:06:40,800 --> 02:06:43,560
but it escapes into a simulated world that we built for it.

2710
02:06:43,560 --> 02:06:47,720
Okay, let's end by talking about a recent critique

2711
02:06:47,720 --> 02:06:52,720
of expecting AGI to arrive pretty in a short time.

2712
02:06:53,600 --> 02:06:56,080
This revolves around interest rates.

2713
02:06:56,080 --> 02:07:00,040
And I guess the basic argument is,

2714
02:07:00,040 --> 02:07:03,640
or the basic question is, if AGI is imminent,

2715
02:07:03,640 --> 02:07:06,800
why are real interest rates low?

2716
02:07:06,800 --> 02:07:08,480
I can explain it, but you're the economist,

2717
02:07:08,520 --> 02:07:11,000
so maybe you can explain the reason in here.

2718
02:07:11,000 --> 02:07:14,680
So it's really a question of how efficient are markets

2719
02:07:14,680 --> 02:07:16,880
and how much foresight do markets have.

2720
02:07:17,720 --> 02:07:18,880
You know, we're coming out of a world

2721
02:07:18,880 --> 02:07:20,760
of very low interest rates, of ultra low interest rates,

2722
02:07:20,760 --> 02:07:22,480
near zero interest rates.

2723
02:07:22,480 --> 02:07:24,760
And one way to think about that is there's a surplus

2724
02:07:24,760 --> 02:07:26,720
of savings relative to investment.

2725
02:07:26,720 --> 02:07:27,880
And so one of the reasons interest rates

2726
02:07:27,880 --> 02:07:29,240
have been in secular decline

2727
02:07:29,240 --> 02:07:31,920
is because populations are aging,

2728
02:07:31,920 --> 02:07:35,640
and so all people have a huge amount of savings built up.

2729
02:07:35,640 --> 02:07:36,880
And meanwhile, we're going through

2730
02:07:36,880 --> 02:07:38,320
the sort of technological stagnation.

2731
02:07:38,320 --> 02:07:40,880
So the amount of savings relative

2732
02:07:40,880 --> 02:07:43,680
to the amount of profitable investments was out of whack,

2733
02:07:43,680 --> 02:07:45,680
and so that pushes interest rates down.

2734
02:07:45,680 --> 02:07:47,680
In a world where AGI takes off,

2735
02:07:48,760 --> 02:07:52,240
it's a world where we have enormous investment opportunities,

2736
02:07:52,240 --> 02:07:54,040
where we'll be building data centers left and right,

2737
02:07:54,040 --> 02:07:55,280
and we can't do it fast enough,

2738
02:07:55,280 --> 02:07:56,080
where there's new products,

2739
02:07:56,080 --> 02:08:00,440
new commercial opportunities left and right.

2740
02:08:00,440 --> 02:08:03,400
And so you would expect in that world

2741
02:08:03,400 --> 02:08:05,880
where the singularity is near, so to speak,

2742
02:08:05,880 --> 02:08:07,760
to be one where the markets begin forecasting

2743
02:08:07,760 --> 02:08:09,360
rapidly rising interest rates

2744
02:08:09,360 --> 02:08:11,280
because the savings to investment balance

2745
02:08:11,280 --> 02:08:12,640
is starting to shift.

2746
02:08:12,640 --> 02:08:15,840
And in addition, there's a long run stylized fact

2747
02:08:15,840 --> 02:08:19,360
that real interest rates track growth rates.

2748
02:08:19,360 --> 02:08:22,360
And so if GDP growth takes off,

2749
02:08:22,360 --> 02:08:26,480
you'd also expect at least nominal rates to also take off.

2750
02:08:26,480 --> 02:08:30,400
And so some have argued that looking at current interest

2751
02:08:30,400 --> 02:08:32,720
rate data, like the five-year, 10-year,

2752
02:08:32,720 --> 02:08:34,640
30-year treasury bonds,

2753
02:08:34,640 --> 02:08:38,240
that the markets are not predicting AGI.

2754
02:08:38,240 --> 02:08:40,800
You know, the two responses to that are,

2755
02:08:40,800 --> 02:08:43,160
one, first of all, interest rates are up quite a bit.

2756
02:08:43,160 --> 02:08:45,120
Nothing's mono-causal.

2757
02:08:45,120 --> 02:08:47,360
There's lots of confounding factors.

2758
02:08:47,360 --> 02:08:50,440
Is this, to some extent, the markets anticipating

2759
02:08:51,480 --> 02:08:52,600
an investment boom?

2760
02:08:52,600 --> 02:08:55,000
You know, maybe they're not anticipating full AGI,

2761
02:08:55,000 --> 02:08:58,960
but they're seeing the way LLMs are going to impact

2762
02:08:58,960 --> 02:09:02,200
enterprise and sort of picking some of that in.

2763
02:09:02,200 --> 02:09:04,000
And then the second piece would be,

2764
02:09:04,040 --> 02:09:07,960
okay, to the extent that they're not pricing in AGI,

2765
02:09:07,960 --> 02:09:09,960
how much foresight do markets have anyway?

2766
02:09:09,960 --> 02:09:12,720
Before we discuss market efficiency,

2767
02:09:12,720 --> 02:09:16,280
I just want to just give a couple of intuitions here.

2768
02:09:16,280 --> 02:09:21,280
If AGI was imminent and it was unaligned, say,

2769
02:09:21,320 --> 02:09:24,440
and it would destroy the world in five years,

2770
02:09:24,440 --> 02:09:27,520
well, then it doesn't make a lot of sense to save money.

2771
02:09:27,520 --> 02:09:32,440
Similarly, if AGI is about to explode growth rates,

2772
02:09:32,480 --> 02:09:35,760
well, then a lot of money will be available in the future.

2773
02:09:35,760 --> 02:09:37,400
You're about to become very rich,

2774
02:09:37,400 --> 02:09:39,880
so it doesn't make sense to save a lot now.

2775
02:09:39,880 --> 02:09:43,240
And the pool of available savings

2776
02:09:43,240 --> 02:09:45,840
determine what's available for lending,

2777
02:09:45,840 --> 02:09:48,440
which determines interest rates.

2778
02:09:48,440 --> 02:09:53,440
But let's discuss whether markets then are efficient

2779
02:09:54,240 --> 02:09:57,320
on this issue or to what extent they're efficient.

2780
02:09:57,320 --> 02:09:59,160
Right, so this is the efficient market hypothesis,

2781
02:10:00,040 --> 02:10:03,080
which comes in strong and weak forms.

2782
02:10:03,080 --> 02:10:05,480
So the strong form of the efficient market hypothesis

2783
02:10:05,480 --> 02:10:08,720
would say that markets aggregate all of available information

2784
02:10:08,720 --> 02:10:10,640
and are our best sort of point estimate

2785
02:10:10,640 --> 02:10:12,000
of anything we care about.

2786
02:10:12,920 --> 02:10:17,080
The weaker form, which I think is more defensible,

2787
02:10:17,080 --> 02:10:19,080
is that markets can be wrong,

2788
02:10:19,080 --> 02:10:22,240
but they can be wrong longer than you can be solvent, right?

2789
02:10:22,240 --> 02:10:25,520
And so you can try to short a company that, like Herbalife,

2790
02:10:25,520 --> 02:10:27,760
famously, there's a big short position on that,

2791
02:10:27,760 --> 02:10:29,200
and because Herbalife sort of looks like

2792
02:10:29,200 --> 02:10:31,800
it's a multi-level marketing Ponzi scheme,

2793
02:10:31,800 --> 02:10:34,480
but yet the hedge fund that did that

2794
02:10:34,480 --> 02:10:37,120
lost several billions of dollars before they

2795
02:10:37,120 --> 02:10:39,160
ended their position because the markets

2796
02:10:39,160 --> 02:10:42,000
stayed irrational longer than they could stay solvent.

2797
02:10:42,000 --> 02:10:43,840
The second factor is the weaker versions

2798
02:10:43,840 --> 02:10:45,160
of the efficient market hypothesis

2799
02:10:45,160 --> 02:10:49,080
are sort of based on a no arbitrage condition, right?

2800
02:10:49,080 --> 02:10:52,240
They say markets are efficient only insofar

2801
02:10:52,240 --> 02:10:57,280
as you can arbitrage an inefficiency, right?

2802
02:10:57,280 --> 02:11:00,920
And so you look at some prediction markets, for example,

2803
02:11:00,920 --> 02:11:01,760
they predict it.

2804
02:11:02,680 --> 02:11:07,440
They'll often have very clear inconsistencies

2805
02:11:07,440 --> 02:11:11,400
across markets that look like they're irrational,

2806
02:11:11,400 --> 02:11:13,360
but then you realize, oh, I can only make

2807
02:11:13,360 --> 02:11:16,280
like $7,000 total on the website

2808
02:11:16,280 --> 02:11:18,240
and there are transaction fees

2809
02:11:18,240 --> 02:11:21,160
and there's work involved.

2810
02:11:21,160 --> 02:11:24,400
And so if the market isn't very deep or liquid,

2811
02:11:24,400 --> 02:11:26,080
there may be inefficiencies that exist

2812
02:11:26,120 --> 02:11:27,880
not because the market's inefficient,

2813
02:11:27,880 --> 02:11:31,000
but as efficient as it can be under the circumstances.

2814
02:11:31,000 --> 02:11:35,680
And when it comes to AI, how do you arbitrage?

2815
02:11:35,680 --> 02:11:38,800
I've been thinking for a while now that Shutterstock,

2816
02:11:38,800 --> 02:11:41,040
their market cap should be collapsing, right?

2817
02:11:41,040 --> 02:11:45,760
Because we have image generation that is proliferating.

2818
02:11:45,760 --> 02:11:47,160
And yes, people will make the argument though,

2819
02:11:47,160 --> 02:11:49,520
Shutterstock has all this image data

2820
02:11:49,520 --> 02:11:51,840
that could build a better image model.

2821
02:11:51,840 --> 02:11:53,880
Maybe it seems like it's cannibalizing their business

2822
02:11:54,000 --> 02:11:56,720
or turning a moat into a commodity.

2823
02:11:56,720 --> 02:11:59,360
And yet Shutterstock's market cap

2824
02:11:59,360 --> 02:12:02,200
has basically held constant throughout

2825
02:12:02,200 --> 02:12:07,200
this recent rebirth of image generation models.

2826
02:12:07,440 --> 02:12:10,120
What if you borrow a lot of money cheaply

2827
02:12:10,120 --> 02:12:13,640
and then put it into an index of semiconductor stocks

2828
02:12:13,640 --> 02:12:16,040
or just IT companies in general,

2829
02:12:16,040 --> 02:12:19,360
even just the general S&P 500 say,

2830
02:12:19,360 --> 02:12:23,280
would that be a way of arbitraging this AGI forecast?

2831
02:12:23,920 --> 02:12:26,120
Yeah, I would say if you have short timelines,

2832
02:12:26,120 --> 02:12:29,440
you should be putting a lot of money into equities.

2833
02:12:29,440 --> 02:12:32,440
This is not financial advice, I should say.

2834
02:12:32,440 --> 02:12:35,720
Right, and I mentioned earlier that Paul Christiana

2835
02:12:35,720 --> 02:12:37,920
has said in interviews that he's twice levered

2836
02:12:37,920 --> 02:12:39,160
into the stock market.

2837
02:12:39,160 --> 02:12:41,280
He basically owns a bunch of AI exposed companies

2838
02:12:41,280 --> 02:12:46,280
and he's borrowed enough money to double his investments.

2839
02:12:46,800 --> 02:12:49,080
So that's putting your money where your mouth is.

2840
02:12:49,080 --> 02:12:51,720
When you look at market behavior

2841
02:12:51,760 --> 02:12:53,400
over the long stretch of time,

2842
02:12:53,400 --> 02:12:57,080
markets didn't anticipate the internet very well.

2843
02:12:58,440 --> 02:13:00,440
There was a short run bubble

2844
02:13:01,600 --> 02:13:05,680
that led to a boom and bust of .com stocks.

2845
02:13:05,680 --> 02:13:07,520
But in terms of the real economy,

2846
02:13:07,520 --> 02:13:08,720
the internet just kept chugging along

2847
02:13:08,720 --> 02:13:09,560
and kept being built out

2848
02:13:09,560 --> 02:13:12,240
and eventually a lot of those investments

2849
02:13:12,240 --> 02:13:15,280
ended up paying off even if you rode through the bubble.

2850
02:13:15,280 --> 02:13:16,720
Markets are made of people.

2851
02:13:16,720 --> 02:13:18,720
Some of the biggest capital holders in the markets

2852
02:13:18,720 --> 02:13:22,240
are institutional investors, pension funds,

2853
02:13:22,240 --> 02:13:25,320
life insurance companies, governments,

2854
02:13:25,320 --> 02:13:30,320
like the Saudi Arabia or the Norwegian pension fund.

2855
02:13:30,920 --> 02:13:35,920
And often these are making safe bets.

2856
02:13:36,200 --> 02:13:39,280
You know, they're not taking very heterodox views

2857
02:13:39,280 --> 02:13:40,720
on markets.

2858
02:13:41,920 --> 02:13:45,400
And so as a result, markets can be a little bit

2859
02:13:45,400 --> 02:13:47,800
autoregressive, they're a little bit biased to the past,

2860
02:13:47,800 --> 02:13:49,360
and past this prologue,

2861
02:13:49,360 --> 02:13:52,840
and prone to kind of multiple equilibria,

2862
02:13:52,840 --> 02:13:56,560
where there's two prices that the shutter stock can be.

2863
02:13:56,560 --> 02:13:58,720
The shutter stock could be a $50 stock

2864
02:13:58,720 --> 02:14:00,120
or it could be a $0 stock,

2865
02:14:00,120 --> 02:14:02,160
and at some point the market will update

2866
02:14:02,160 --> 02:14:04,360
and will wander go through like the great repricing

2867
02:14:04,360 --> 02:14:06,360
and all these asset prices will flip

2868
02:14:06,360 --> 02:14:07,680
in relatively short order.

2869
02:14:07,680 --> 02:14:10,240
The efficient market hypothesis has to be false,

2870
02:14:10,240 --> 02:14:12,760
or else we wouldn't have Silicon Valley.

2871
02:14:12,760 --> 02:14:14,840
Right, we wouldn't have founders

2872
02:14:14,840 --> 02:14:17,360
that we wouldn't have Elon Musk, right?

2873
02:14:17,840 --> 02:14:19,840
So I would just say the markets are wrong.

2874
02:14:19,840 --> 02:14:22,600
And partly they're wrong because to be right

2875
02:14:22,600 --> 02:14:27,600
would require having a bunch of relatively bespoke

2876
02:14:27,760 --> 02:14:30,880
and kind of esoteric priors about the direction

2877
02:14:30,880 --> 02:14:34,200
of technology that are only now just sort of

2878
02:14:34,200 --> 02:14:36,000
percolating into the mainstream.

2879
02:14:36,000 --> 02:14:38,640
Yeah, and that the big kind of capital allocators

2880
02:14:38,640 --> 02:14:41,440
can't really respond to because they're risk averse.

2881
02:14:41,440 --> 02:14:42,640
Exactly.

2882
02:14:42,640 --> 02:14:44,360
Now that doesn't mean like Renaissance technologies

2883
02:14:44,360 --> 02:14:45,440
won't respond to it,

2884
02:14:45,440 --> 02:14:47,080
but they're not gonna move the market.

2885
02:14:47,120 --> 02:14:48,920
Samuel, thanks for this conversation

2886
02:14:48,920 --> 02:14:50,480
and I've learned a lot.

2887
02:14:50,480 --> 02:14:51,320
Thank you.

