WEBVTT

00:00.000 --> 00:06.000
Welcome to the Future of Life Institute podcast. My name is Gus Docker and I'm here with Darren McKee.

00:06.140 --> 00:14.140
Darren is the author of the upcoming book Uncontrollable and he's the host of the Reality Check podcast.

00:14.340 --> 00:18.340
He's also been a senior policy advisor for 15 years.

00:18.400 --> 00:19.900
Darren, welcome to the podcast.

00:19.900 --> 00:22.100
Hi Gus, pleasure to be here, fan of the show.

00:22.300 --> 00:30.200
I had a great time reading your book and one of your goals with the book is to take a complex topic like

00:30.200 --> 00:35.400
machine learning, AI research and in particular alignment research and then

00:35.400 --> 00:39.900
present it in an accessible way. But how do you go about a task like that?

00:39.900 --> 00:43.900
Well, it was a bit difficult and I think clarity above all else.

00:43.900 --> 00:50.100
I've been following the AI developments for we'll say a decade or two, sometimes deeply, sometimes loosely

00:50.100 --> 00:55.100
and I would read the other popular books and some of the articles and have discussions about all these things.

00:55.100 --> 01:01.100
But it was really around April, May 2022 when I thought, oh, wow, things are really picking up.

01:01.100 --> 01:07.100
And I think a lot of people felt that when the tacky list projection for AGI dropped about 10 years,

01:07.100 --> 01:10.100
everyone's like, oh, oh, things are things are happening, right?

01:10.100 --> 01:17.100
And at that point, I thought, okay, I think there's a gap between readily available materials that reach a wider audience

01:17.100 --> 01:22.100
and the speed at which AI is progressing. And so I thought there's an opportunity here to write a book.

01:22.100 --> 01:27.100
It's not that materials didn't exist at all. There's forum posts, there's blogs, there's lots of podcasts,

01:27.100 --> 01:32.100
there's videos and so on. But I thought it'd be nice if it would be pulled together in a book, we'll say,

01:32.100 --> 01:36.100
for people with no technical background, even no science background.

01:36.100 --> 01:42.100
And so that's where some of my experience with the podcast or policy where you're trying to kind of do knowledge translation,

01:42.100 --> 01:47.100
take a complicated idea, try to phrase it simply, excessively, and relate to an audience.

01:47.100 --> 01:52.100
And with that as a context, the journey began, so to speak.

01:52.100 --> 01:57.100
So since last June 2022, I've been trying to work on this book and I'm happy that it's done.

01:57.100 --> 02:05.100
And I've tried to make a really concerted effort from the design of the cover to the table of contents to the chapters to how they flow.

02:05.100 --> 02:10.100
It really is trying to put yourself in the mind of someone who is curious about AI.

02:10.100 --> 02:17.100
They've seen these news headlines, they're interested, maybe concerned, confused, what the heck is going on with AI?

02:17.100 --> 02:21.100
And that this book is for. So it's not so much for the technical people.

02:21.100 --> 02:26.100
It's not for the people who've been in this AI safety debate for many years, although hopefully they'll get some value.

02:26.100 --> 02:32.100
It's more in a way for the people they know that they can help perhaps explain some of the issues to them.

02:33.100 --> 02:41.100
I've been reading about AI, I've been interested in AI for a long time and this book still provided a value to me.

02:41.100 --> 02:45.100
So there's value in going over the basics again, I would say.

02:45.100 --> 02:50.100
We're also in this conversation going to go over some of the basics again.

02:50.100 --> 02:57.100
And there are interesting choices about how you frame different issues, which analogies you use and so on.

02:57.100 --> 03:01.100
I'm interested in how do you balance accessibility with accuracy?

03:01.100 --> 03:04.100
So I imagine that an expert is going to read this book.

03:04.100 --> 03:13.100
How do you deal with that fact when you're writing for a broader audience, but you might get a nitpicky expert critiquing your book?

03:13.100 --> 03:16.100
I think there's just going to be some inherent trade-offs.

03:16.100 --> 03:21.100
The goal is to reach, as I said, as many people as possible because the experts, they already know these things.

03:21.100 --> 03:24.100
They already have materials that are available more readily to them.

03:24.100 --> 03:28.100
But for the average person, to have something really explained in a book form,

03:28.100 --> 03:31.100
I think this is kind of the first of its kind.

03:31.100 --> 03:37.100
It is entirely dedicated to the AI safety argument and tries to reach people as excessively as possible.

03:37.100 --> 03:43.100
My own background, science and academia, I am also inclined towards accuracy and even precision

03:43.100 --> 03:46.100
and trying to understand the difference between accuracy and precision,

03:46.100 --> 03:52.100
but also understanding that for the audience, I'm trying to reach the difference between accuracy and precision isn't what matters.

03:52.100 --> 03:54.100
What is the main idea?

03:54.100 --> 03:55.100
Is there rigor?

03:55.100 --> 03:58.100
Is what I'm saying generally true or understood to be true?

03:58.100 --> 04:00.100
Is there evidence to support it?

04:00.100 --> 04:01.100
Does it make sense?

04:01.100 --> 04:06.100
But I did try to be, we'll say, a bit more flexible by how I might phrase certain things

04:06.100 --> 04:10.100
and how I might use certain analogies to try to meet the audience where they are.

04:10.100 --> 04:13.100
As you said, this field is very, very complicated

04:13.100 --> 04:18.100
and you have to make concessions somewhere when you're explaining things to people.

04:18.100 --> 04:24.100
What did you do when in your research, you came upon a topic on which the experts disagreed?

04:24.100 --> 04:28.100
This is often the case in alignment research, for example.

04:28.100 --> 04:31.100
The experts might disagree about the basics.

04:31.100 --> 04:34.100
What do you do then when you're trying to explain the basics?

04:34.100 --> 04:37.100
I think there's a couple of options there.

04:37.100 --> 04:42.100
My approach was generally to try to give, I won't say that generally accepted consensus,

04:42.100 --> 04:46.100
because there is a consensus of sorts, but not unanimity.

04:46.100 --> 04:48.100
Everyone doesn't fully agree and no one ever does.

04:48.100 --> 04:50.100
And that's true, let's be honest, for everything.

04:50.100 --> 04:55.100
When you look at the surveys of what the philosophers believe, there's fundamental differences.

04:55.100 --> 04:59.100
Same thing with economists, same thing with physicists, same thing with pretty much every discipline.

04:59.100 --> 05:03.100
So understanding that, the book is trying to be a bit of a neutral observer,

05:03.100 --> 05:05.100
but at the same time, I have a perspective.

05:05.100 --> 05:07.100
I've looked at these issues.

05:07.100 --> 05:08.100
I am concerned.

05:08.100 --> 05:13.100
We're trying to figure it out together, but I'm giving reasons why I think AI safety is a concern.

05:13.100 --> 05:17.100
To be more specific, I kind of tried to be sympathetic to all sides,

05:17.100 --> 05:21.100
but at the same time, I probably didn't get too much into the weeds.

05:21.100 --> 05:26.100
If people thought there's no concern at all, I might mention the uncertainty around everything,

05:26.100 --> 05:30.100
but not so much giving that a lot of weight, because I think there are reasons to be concerned.

05:30.100 --> 05:35.100
To take a step back, the book, the structure to try to give people a sense of how I did it,

05:35.100 --> 05:40.100
is you look at the debates or discussions that are occurring in the AI risk, AI safety space,

05:40.100 --> 05:42.100
and what do people seem to get stuck on?

05:42.100 --> 05:43.100
What are their disagreements?

05:43.100 --> 05:44.100
What are their cruxes?

05:44.100 --> 05:50.100
What might be the underlying, we'll say, intellectual or even emotional dispositions

05:50.100 --> 05:52.100
that are leading people to have certain beliefs?

05:52.100 --> 05:59.100
And then identifying that, thinking about which part of the AI issue relates to it,

05:59.100 --> 06:05.100
and then trying to think about an analogy or a simple way to explain the thing related to AI,

06:05.100 --> 06:08.100
so it addresses the underlying issue in the future debate.

06:08.100 --> 06:12.100
It's a bit complicated, but it's kind of like working backwards and then forwards.

06:12.100 --> 06:18.100
So if I think one of the issues is it's really hard to imagine how powerful something like

06:18.100 --> 06:23.100
an advanced AI or superintelligence could be, the first chapter of the book is a lot about

06:23.100 --> 06:27.100
how powerful intelligence is or the importance of imagination.

06:27.100 --> 06:31.100
Is it really that it's a failure of imagination that's driving a lot of this?

06:31.100 --> 06:33.100
Well, not everything, of course, but I think it's a factor.

06:33.100 --> 06:39.100
So then I'm trying to address aspects of how imagination might work or just open up people's minds.

06:39.100 --> 06:43.100
Why don't we put it that way about what's possible before I even talk about AI?

06:43.100 --> 06:46.100
Because it's a general issue about trying to be open-minded about what could happen.

06:46.100 --> 06:52.100
And with that in place, hopefully later when there's more AI stuff, it's just less likely to be a problem.

06:52.100 --> 06:57.100
It is still hard to imagine something as smart as an ASI or artificial superintelligence,

06:57.100 --> 07:01.100
but we can try to at least acknowledge that there's something there.

07:01.100 --> 07:06.100
There's something to be understood or even that we don't know exactly what it could be, and that's its own value.

07:06.100 --> 07:13.100
So you write about how the range of intelligence extends much further than we might normally imagine.

07:13.100 --> 07:16.100
What institutions do you rely on here?

07:16.100 --> 07:18.100
How do you explain why is that?

07:18.100 --> 07:19.100
Why is that the case?

07:19.100 --> 07:22.100
Sure, I think the human brain is great.

07:22.100 --> 07:25.100
I like mine, even though it's flawed, I don't know what I'd do without it.

07:25.100 --> 07:29.100
But when we interact in the world, we have our kind of default settings.

07:29.100 --> 07:33.100
We know there's various biases and the availability bias and other things that when you're asked a question

07:33.100 --> 07:39.100
or you're quickly reading something, whatever pops in seems to be how you might think or feel about a subject.

07:39.100 --> 07:44.100
If given a direct question by someone, you might reflect a bit more and think a bit differently.

07:44.100 --> 07:46.100
But I'm trying to shift a bit out of that default.

07:46.100 --> 07:51.100
So when we navigate the world, we kind of think of the intelligence mostly of our friends,

07:51.100 --> 07:55.100
or I ask people to imagine like the smartest person they know, and they have that person in mind.

07:55.100 --> 07:58.100
I'm like, okay, now imagine the smartest person you think ever existed.

07:58.100 --> 08:03.100
Maybe it's Einstein or Mary Curie or Von Neumann or whoever it is.

08:03.100 --> 08:08.100
And so right away, you're like, okay, well, I had the smartest person that I know personally and the smartest person ever.

08:08.100 --> 08:11.100
Like, well, could there be even people who are smarter?

08:11.100 --> 08:14.100
And I give some examples of savants who have incredible abilities,

08:14.100 --> 08:20.100
whether it's memorization or processing information visually or auditorily, whatever it is.

08:20.100 --> 08:26.100
And like, well, look, humans already can do some amazing things, which that smartest person you know,

08:26.100 --> 08:29.100
and well, Von Neumann, maybe exception, can't really do.

08:29.100 --> 08:33.100
So it's kind of trying to show, let's think about what might be possible

08:33.100 --> 08:38.100
and then giving examples of what already is possible to help people understand,

08:38.100 --> 08:43.100
okay, if this is possible and it's already happened, could we imagine a little bit more, right?

08:43.100 --> 08:44.100
A little bit higher.

08:44.100 --> 08:47.100
I also go the other way, looking at capacities of animals.

08:47.100 --> 08:53.100
And we look at, you know, briefly, like in a page, it's, you know, birds and you have fish and you have dolphins and so on.

08:53.100 --> 08:57.100
And we know broadly humans are more intelligent than these other creatures.

08:57.100 --> 09:01.100
Not to say all humans, right, at all times of their lives than all the other creatures, of course,

09:01.100 --> 09:04.100
but broadly as a general truth, that's the case.

09:04.100 --> 09:10.100
And when we think about, say, why gorillas are in our zoos and we're not in theirs, it's not strength, right?

09:10.100 --> 09:11.100
It's not our good looks.

09:11.100 --> 09:12.100
It's not our charm.

09:12.100 --> 09:17.100
It's because, again, collectively, we have an intelligence capability that is beyond theirs.

09:17.100 --> 09:22.100
And that's another nuance that when I use intelligence, it's a bit more like anthropologist Joseph Henrichs,

09:22.100 --> 09:27.100
sort of cumulative cultural collective capital in terms of intelligence.

09:27.100 --> 09:33.100
It's very broad and encompassing because I thought that was probably the best way to try to communicate how important it is.

09:33.100 --> 09:36.100
Yeah, why do you think we forget or at least this is my experience?

09:36.100 --> 09:39.100
I'm intellectually aware of all of these examples you just gave.

09:39.100 --> 09:44.100
I know there are people that are much smarter than me and I know there are savants and so on.

09:44.100 --> 09:51.100
But it seems to me that when I think in normal life, I deceive myself into thinking that the range of intelligence

09:51.100 --> 10:00.100
ends at my height, basically, that I represent the basic range of intelligence.

10:00.100 --> 10:02.100
Do you think this is common?

10:02.100 --> 10:05.100
Why is it hard to imagine abilities that are beyond us?

10:05.100 --> 10:06.100
That's a great question.

10:06.100 --> 10:10.100
I'm not sure of detailed research, but I do think it is somewhat common.

10:10.100 --> 10:13.100
We kind of think we're the example, right?

10:13.100 --> 10:17.100
This sort of mind projection fallacy or typical mind fallacy as it's thought.

10:17.100 --> 10:18.100
I'm sort of the baseline.

10:18.100 --> 10:19.100
Things are different than me.

10:19.100 --> 10:21.100
Then that's how I calibrate them, right?

10:21.100 --> 10:26.100
If we say someone is tall or short, sometimes that's in reference to ourselves,

10:26.100 --> 10:30.100
but often it's sort of some general vague notion of the population.

10:30.100 --> 10:34.100
Of course, if you're 6'5, 6'5", that makes you very tall,

10:34.100 --> 10:36.100
unless you're in the NBA, then it's average.

10:36.100 --> 10:39.100
And so we do understand there is a frame of reference, but again,

10:39.100 --> 10:42.100
the default setting of human brains, again, they're great,

10:42.100 --> 10:46.100
but we can't be thinking in complexity, that much complexity all the time.

10:46.100 --> 10:48.100
Working space memory is tapped.

10:48.100 --> 10:53.100
So if you're just trying to, I don't know, make dinner or you want to read an article to then think of like,

10:53.100 --> 10:57.100
oh, there's 15 to 30 things I should always have in mind while I'm doing this.

10:57.100 --> 10:58.100
That's great.

10:58.100 --> 11:01.100
I applaud the effort and I try to have a couple of mine, but it's almost impossible to do.

11:01.100 --> 11:06.100
So in that sense, the book can also serve to just remind people of things that they already know.

11:06.100 --> 11:12.100
It's very hard to even for me to talk about like, well, do I have a 3, 350 page book memorized yet?

11:12.100 --> 11:16.100
Not entirely, but the ideas are more frequently in there and they're more likely to come to mind.

11:16.100 --> 11:21.100
So with intelligence, I think it really is just almost asking yourself more often,

11:21.100 --> 11:23.100
am I making the right projection?

11:23.100 --> 11:25.100
Is this a fair generalization?

11:25.100 --> 11:29.100
Am I inadvertently benchmarking to the wrong set or the wrong baseline?

11:29.100 --> 11:38.100
And in time, through human history is what I mean to say, humans now in various places are on average much more intelligent than they were in the past.

11:38.100 --> 11:43.100
And that's mainly due to education and socialization, nutrition, diet, these sorts of things.

11:43.100 --> 11:49.100
I think if you look approximately 100,000, 200,000 years ago, genetically humans are very similar.

11:49.100 --> 11:56.100
Of course, if you go, you know, a million years ago, then it's quite different, but it's not staggeringly different compared to the other species entirely.

11:56.100 --> 12:06.100
All that to say is that why humans are so capable now is because of our nutrition, our socialization and the fact we live in a world that does a lot of the work for us.

12:06.100 --> 12:09.100
As I say, like, you know, can you build a fire?

12:09.100 --> 12:12.100
Well, many people can and many people can't.

12:12.100 --> 12:15.100
And what's worse is some of us can't build a fire even with matches.

12:15.100 --> 12:17.100
We know what we're supposed to do, right?

12:17.100 --> 12:23.100
You get some kindling, you get some paper, you get some lighter, you get some thicker wood and you have the matches and you do a decent job.

12:23.100 --> 12:25.100
It starts and then it fizzles out.

12:25.100 --> 12:30.100
And it's like, oh, I can't even build fire despite knowing how to do it and starting with matches.

12:30.100 --> 12:31.100
Well, that's embarrassing.

12:31.100 --> 12:33.100
Again, this is easily rectified if you practiced.

12:33.100 --> 12:42.100
But I don't necessarily have to know this because there are matches and there are lighters and other people have figured this out for me so I can leverage the intelligence and the efforts of other people.

12:42.100 --> 12:44.100
So I don't have to worry as much.

12:44.100 --> 12:49.100
And perhaps similarly, it feels like your one is an individual super smart navigating through the world.

12:49.100 --> 12:57.100
You're like, well, that's because everything else has been done for you right now with computers and phones, most of the complexity is behind the scenes.

12:57.100 --> 12:58.100
And that's great for us.

12:58.100 --> 12:59.100
You press a button, something works.

12:59.100 --> 13:06.100
What actually happens, the staggering mind boggling complexity of information and ones and zeros going across space and time.

13:06.100 --> 13:08.100
It's easy to just not think about it in a way.

13:08.100 --> 13:15.100
Why would you if you're trying to watch a movie, you're not going to think of the matrix decoded, you could, but then you're going to ruin the experience for yourself.

13:15.100 --> 13:24.100
Do you think we are giving our collective knowledge to AIs by training them on all available data online?

13:24.100 --> 13:33.100
For example, are we thereby transferring the collective knowledge that we've built up that allows us to succeed in daily life?

13:33.100 --> 13:34.100
I think we are.

13:34.100 --> 13:39.100
It's almost like having a new child of sorts and you're socializing it the way you might a person.

13:39.100 --> 13:44.100
So let's look at all what humanity has learned and we try to pass that on to a new generation.

13:44.100 --> 13:56.100
And we're doing the same thing with AI just in a much more dramatic, complicated and, again, staggeringly vast way where there's, as you know, the amounts of data that

13:56.100 --> 14:01.100
adabytes, zettabytes, I can't remember exactly what the number is that's going into these systems or would or could go into these systems.

14:01.100 --> 14:03.100
I should say it's not quite that high yet.

14:03.100 --> 14:07.100
But why not have all the world some knowledge available?

14:07.100 --> 14:09.100
And it's only going to get better and better.

14:09.100 --> 14:12.100
I think there's vast data sources that haven't been fully tapped, right?

14:12.100 --> 14:19.100
So you think of all the video, which programs are now starting to mine, both you can just take the audio from it, which has its own value.

14:19.100 --> 14:23.100
But even the movements, how people move, what they show and how that sort of thing happens.

14:23.100 --> 14:30.100
That could be, you know, whether it's, you know, fixing a chair or the sink, but it's also how people navigate what they pay attention to and what they don't.

14:30.100 --> 14:37.100
So once all the video gets recorded and all the radio and is all these different data sources that I think it'll be even more staggering.

14:37.100 --> 14:43.100
So yes, we're kind of giving humanity everything that we've ever done that's in record over two AIs.

14:43.100 --> 14:49.100
And that will have, of course, very fantastic, wonderful things, one hopes, but also a lot of risks and concerns.

14:49.100 --> 14:56.100
Given our troubles with understanding other people's minds, how can we hope to understand AI systems?

14:56.100 --> 14:58.100
Yes, I think that's a great question.

14:58.100 --> 15:02.100
I am optimistic, but I think it's going to be definitely a challenge.

15:02.100 --> 15:06.100
I mean, there's a broader idea here is that this is what we need to work on.

15:06.100 --> 15:09.100
There are many things related to AI that are going to be very difficult.

15:09.100 --> 15:12.100
And whether we're hopeful or not, we have to realize that we have to try.

15:12.100 --> 15:16.100
We have to try to figure it out how they work, why they're doing what they're doing.

15:16.100 --> 15:20.100
And yeah, it might be complicated, but humanity has figured out the impossible before.

15:20.100 --> 15:22.100
That goes back to the imagination issue.

15:22.100 --> 15:26.100
If you look at what we kind of take for granted now that humans have gone to the moon,

15:26.100 --> 15:30.100
that we're having this conversation again across space and time relatively easily.

15:30.100 --> 15:35.100
This is not only odd or unlikely or improbable.

15:35.100 --> 15:38.100
This is impossible to people from the past.

15:38.100 --> 15:41.100
And beyond that, I actually want to argue that it's unthinkable.

15:41.100 --> 15:47.100
If you go far enough back, they didn't have the conceptual apparatus to even consider how difficult this could be.

15:47.100 --> 15:51.100
And that's also a shift where you're thinking, okay, imagine gazing up at the moon

15:51.100 --> 15:54.100
as a lot of humanity has done throughout our history.

15:54.100 --> 15:57.100
Well, how do you get there? You can't just like climb a tree.

15:57.100 --> 16:04.100
You can build a tall structure, as many did, and mountain, temple, these sorts of tall ladder.

16:04.100 --> 16:08.100
But the idea that you could build a ship or a rocket and actually fly there,

16:08.100 --> 16:10.100
that wouldn't even occur to people.

16:10.100 --> 16:12.100
It just would have been out of their reach at the time.

16:12.100 --> 16:16.100
And it's within our reach, again, just because we happen to be born at this time.

16:16.100 --> 16:18.100
So it allows for what's possible.

16:18.100 --> 16:23.100
Well, with the AI at the moment, it seems very, very difficult given how these systems are,

16:23.100 --> 16:27.100
as people say, almost grown rather than built, to know exactly what's going on

16:27.100 --> 16:32.100
given the complexity of artificial neural networks and the size of the parameters and all these models.

16:32.100 --> 16:36.100
That said, I think there's a lot of good efforts to try to figure these things out.

16:36.100 --> 16:41.100
And that's exactly why we need like more investment in AI safety and more people to try to help us.

16:41.100 --> 16:48.100
One hang-up people have about AI safety is thinking about the goals of AI systems.

16:48.100 --> 16:54.100
So why is it that AI systems might develop a goals that are contrary to ours?

16:54.100 --> 16:58.100
Why would they suddenly turn evil? It's a question you could ask.

16:58.100 --> 17:02.100
There you talk about, you use the virus analogy.

17:02.100 --> 17:04.100
So maybe you could explain how you use that.

17:04.100 --> 17:08.100
So I think there's kind of a two-step process here, whether AI systems have goals or not.

17:08.100 --> 17:10.100
And I'll take each in turn.

17:10.100 --> 17:12.100
With the virus, why don't we just do that first?

17:12.100 --> 17:15.100
A virus doesn't have goals, but it can still harm you.

17:15.100 --> 17:17.100
And I think this is a great analogy.

17:17.100 --> 17:21.100
And you can think of the, you know, sort of biological virus, but there's also computer viruses.

17:21.100 --> 17:25.100
Biological viruses, depending on what they're, we know they're very hard to contain.

17:25.100 --> 17:29.100
They can cause pandemics. They can cause illness, common flu, these sorts of things.

17:29.100 --> 17:34.100
Computer viruses, people not in tech aren't aware that there's some still crawling the internet

17:34.100 --> 17:39.100
that were developed years and years ago that we can't really stop, but that's the default world we're in.

17:39.100 --> 17:41.100
So these things were created and they get out of control.

17:41.100 --> 17:46.100
These things, computer and biological virus, it doesn't need a goal to harm you.

17:46.100 --> 17:52.100
It doesn't need an intention. It's kind of just following a process in the biological sense and evolved mechanism.

17:52.100 --> 17:57.100
Whether viruses are live as a debate, you don't need to get into, but it's trying to achieve certain objectives.

17:57.100 --> 18:02.100
And you can describe it as if it has a goal, because it's easier to kind of navigate the world.

18:02.100 --> 18:04.100
It reduces complexity.

18:04.100 --> 18:07.100
But you could also argue, of course, it doesn't have goals. It's a virus. It's a little thing.

18:07.100 --> 18:09.100
Why would a computer virus have a goal? It doesn't.

18:09.100 --> 18:14.100
And in the book, I acknowledge this, but I kind of just go with the as if goals.

18:14.100 --> 18:19.100
Let's not get into a protracted philosophical debate about whether something has goals or not.

18:19.100 --> 18:24.100
If it acts as if it has goals and it's useful to treat it that way, then let's just do that.

18:24.100 --> 18:30.100
It's much easier. And this is, for anyone curious, this is totally Dennett's intentional stance type stuff coming in here

18:30.100 --> 18:32.100
because I'm a huge fan of philosopher Daniel Dennett.

18:32.100 --> 18:38.100
And like, what are we trying to do here? We're trying to protect ourselves from computer viruses or biological viruses.

18:38.100 --> 18:41.100
And you can think all the viruses trying to get you, the flu is trying to infect you.

18:41.100 --> 18:44.100
When someone coughs or sneezes, it's trying to spread.

18:44.100 --> 18:46.100
Well, of course, it's not trying to spread.

18:46.100 --> 18:51.100
The organism has engaged in activity, which makes it more likely to replicate.

18:51.100 --> 18:55.100
But to say that every time, you know, it just becomes very burdensome.

18:55.100 --> 19:00.100
So sentence has become paragraphs, paragraphs, many paragraphs, and that's why we sort of circumvent it.

19:00.100 --> 19:05.100
So that's the first bit about why it might have goals in an as if sense.

19:05.100 --> 19:12.100
And then there's the other part where, OK, so if something becomes a bit more autonomous, does it come to have goals?

19:12.100 --> 19:18.100
And I think, you know, we can look at different organisms again as an understanding that autonomy falls on a continuum.

19:18.100 --> 19:21.100
Does something have a goal or not? Well, in some ways, yes, in some ways, no.

19:21.100 --> 19:25.100
The chess playing program, does it want to win? Well, it sure acts like it wants to win.

19:25.100 --> 19:30.100
And when I lose, I feel like it beat me versus like some computer code and some people designed it.

19:30.100 --> 19:33.100
The designers definitely designed it to have a range of skills.

19:33.100 --> 19:35.100
What could be does do worms have goals?

19:35.100 --> 19:36.100
Do cats have goals?

19:36.100 --> 19:37.100
Do dogs have goals?

19:37.100 --> 19:39.100
Do orangutans have goals?

19:39.100 --> 19:44.100
Well, of course, in some ways, they definitely engage in goal directed behavior because they're trying to achieve a certain objective.

19:44.100 --> 19:54.100
And similarly with AI systems, we already have various, we'll say, automated programs that have autonomy and they need to have autonomy in some way to act in the world.

19:54.100 --> 19:58.100
This would be the stock trading platforms that already function all the time.

19:58.100 --> 20:08.100
Credit card, bank fraud detection, numerous other sort of monitoring of systems type algorithms and whatnot function on an automatic process because they couldn't keep checking with a human.

20:08.100 --> 20:19.100
And even in the military, typically there is a human in the loop, but we know for say missile defense, these things are automated often because there's no way a human could react quickly enough to stop incoming missiles.

20:19.100 --> 20:23.100
So it's not that whether computers become autonomous at all.

20:23.100 --> 20:26.100
It's that to what degree are they going to become more autonomous?

20:26.100 --> 20:28.100
And I think that is a much larger conversation.

20:28.100 --> 20:39.100
But at the same time, you could see why there'll be incentives to have things become more automatic, which means a bit more empowered, which will then translate to seeming like they have goals.

20:39.100 --> 20:45.100
And at one point, the seeming goals blends enough into the, well, that looks like it has goals that I'm just going to say it has goals.

20:45.100 --> 20:56.100
If we return to the virus analogy, one thought I have there is whether the existence of the common cold, for example, the common cold is not very smart.

20:56.100 --> 20:58.100
It's pretty dumb, I imagine.

20:58.100 --> 21:02.100
And it's also very harmful and costly to society.

21:02.100 --> 21:08.100
Does this mean that intelligence is not necessary to cause harm?

21:08.100 --> 21:12.100
Is this a decoupling of intelligence and power in the world?

21:12.100 --> 21:13.100
A great point.

21:13.100 --> 21:18.100
So I think, as you just said, it's not a necessary condition in that case.

21:18.100 --> 21:24.100
But that's different from whether intelligence is a factor that becomes correlated with the ability to cause more harm.

21:24.100 --> 21:33.100
So you're right that a virus will think it's pretty simple, pretty straightforward, not that many moving pieces compared to other organisms, and it can cause drastic amounts of harm.

21:33.100 --> 21:38.100
But you know, you're not going to go watch a movie with a virus and then talk about its thoughts after.

21:38.100 --> 21:39.100
It's not that type of thing.

21:39.100 --> 21:47.100
But as you scale up the abilities or the intelligence capabilities of different entities, they do have a different capability for harm.

21:47.100 --> 21:54.100
So viruses can just infect you, but something with more intelligence could design viruses to infect you, design even worse viruses.

21:54.100 --> 22:00.100
And even more dramatically, you know, well, if something's smart enough, it might be able to deflect an incoming asteroid and save us in a way.

22:00.100 --> 22:02.100
So it's not just harmful, but it's the other side to the benefits.

22:02.100 --> 22:10.100
So I think intelligence is a factor and in some cases it becomes a very significant factor in the ability to cause harm versus not.

22:10.100 --> 22:17.100
As anyone who's like taking care of a child, a very young child can cause a lot of harm, but they're relatively, relatively containable.

22:17.100 --> 22:22.100
If you have the right crib, if you have the right gates, they can usually be at least put in a certain place.

22:22.100 --> 22:26.100
That's partly due to mobility and partly due to intelligence, same thing with various other animals.

22:26.100 --> 22:36.100
But as you ranch it up the intelligence to sort of contain something or control it, it becomes increasingly difficult to the point where it's very, very difficult to contain some very intelligent entities.

22:36.100 --> 22:42.100
Now, there are lots of definitions of artificial general intelligence out there.

22:42.100 --> 22:45.100
And you use one in the book that I quite like.

22:45.100 --> 22:48.100
Maybe you could talk about the average office worker, the average remote worker.

22:48.100 --> 22:53.100
So artificial general intelligence or AGI, as people say, it's very much in the news, right?

22:53.100 --> 22:54.100
How long till AGI?

22:54.100 --> 22:55.100
What is AGI and so on?

22:55.100 --> 23:01.100
And the approach I took in the book, again, for accessibility, I'm not trying to make the definitive definition.

23:01.100 --> 23:05.100
I just want a good enough definition so people could understand what we're talking about.

23:05.100 --> 23:07.100
And I thought in the most accessible way.

23:07.100 --> 23:11.100
And with those factors in mind, okay, what are we talking about here?

23:11.100 --> 23:14.100
It's usually someone's concerned about employment primarily.

23:14.100 --> 23:16.100
And so I was thinking, could someone replace me at work?

23:16.100 --> 23:20.100
And so if you kind of take the average coworker idea, that's the foundation.

23:20.100 --> 23:24.100
If you're interacting with a coworker, say it's a remote coworker, right?

23:24.100 --> 23:26.100
It's still focused on intellectual tasks.

23:26.100 --> 23:31.100
So it's a computer system that, you know, does intellectual tasks as good as an average person.

23:31.100 --> 23:36.100
And that's really the foundation where you could imagine if someone's performing as an average coworker level,

23:36.100 --> 23:39.100
they're doing general tasks to a certain capability.

23:39.100 --> 23:46.100
And therefore you could see why the person may be, sorry, that entity may be employable or might affect employment.

23:46.100 --> 23:49.100
And it seemed to, I guess, as I said, hit home in the best way.

23:49.100 --> 23:54.100
As a nuance here, why use that terminology at all is a question I asked myself.

23:54.100 --> 23:57.100
Because like, well, do I need to introduce these terms to the average person?

23:57.100 --> 23:58.100
Why, why not?

23:58.100 --> 23:59.100
Well, they seem pretty popular, right?

23:59.100 --> 24:00.100
That's one thing.

24:00.100 --> 24:01.100
So you meet people where they are.

24:01.100 --> 24:04.100
And I knew the world was going to talk about AGI in one way or another.

24:04.100 --> 24:09.100
Now, is it better to say human level because it's almost the same in some ways?

24:09.100 --> 24:12.100
But at the same time, we use the term AI a lot, right?

24:12.100 --> 24:15.100
And that's sort of like, why did I use intelligence, artificial intelligence,

24:15.100 --> 24:18.100
artificial general intelligence, artificial superintelligence is because I thought,

24:18.100 --> 24:22.100
you know what, most people are going to talk about AI and people kind of know what AI is.

24:22.100 --> 24:27.100
So once you accept artificial intelligence as a foundation to make things as easy as possible,

24:27.100 --> 24:30.100
it's going down one in a way as intelligence broadly.

24:30.100 --> 24:33.100
And then going another level is artificial general intelligence and then superintelligence.

24:33.100 --> 24:37.100
And I thought that four part kind of structure was the best way to frame things.

24:37.100 --> 24:42.100
I imagine that you decided to write the book out of some sense of urgency.

24:42.100 --> 24:45.100
Now, you give a lot of uncertainty.

24:45.100 --> 24:49.100
You present the uncertainty around what is called AI timelines in the book.

24:49.100 --> 24:56.100
You must be motivated in some sense by urgency about the speed at which AI is developing.

24:56.100 --> 24:58.100
Do you want to give us your AI timelines?

24:58.100 --> 25:04.100
Do you want to tell us how long it is until we have AGI?

25:04.100 --> 25:06.100
How long do we have left?

25:06.100 --> 25:09.100
I don't necessarily think that. I want to be dramatic.

25:09.100 --> 25:13.100
But so I think it's a great point and I'll elaborate a bit that I don't want to head.

25:13.100 --> 25:20.100
So I think something like the average coworker, sure, it could be plausible within three years, two to three, four years.

25:20.100 --> 25:26.100
I'm being a bit vague because I almost feel like that's not what's as important as the fact that there's a lot of good data

25:26.100 --> 25:28.100
that shows capabilities are dramatically increasing.

25:28.100 --> 25:34.100
This is the basics of more investment and computational power, better design of chips, new chips coming online

25:34.100 --> 25:37.100
to the extent that NVIDIA's H100s are being shipped.

25:37.100 --> 25:42.100
So the big players, open AI and DeepMind will probably start training their next-gen models on all these things.

25:42.100 --> 25:47.100
And of course, a couple years after that, there's even more powerful chips that are planned by NVIDIA and all these designers.

25:47.100 --> 25:52.100
So the path, the most likely outcome seems to be increasing computational power,

25:52.100 --> 25:56.100
which then has some relationship to increased AI capabilities.

25:56.100 --> 26:01.100
And we don't quite know how the input matches the output, which is itself a concern.

26:01.100 --> 26:02.100
There's that uncertainty there.

26:02.100 --> 26:07.100
But the trend lines are things are going to get more powerful and things are going to get more capable.

26:07.100 --> 26:13.100
And because of that, it does seem like AGI or even artificial superintelligence, I think that's within 10 years.

26:13.100 --> 26:15.100
I think that is entirely plausible.

26:15.100 --> 26:18.100
Again, this is not 100% estimate, but I think it's plausible enough.

26:18.100 --> 26:24.100
Now, in the regular listeners of the show might be like, well, did he mean 30%, 60% or 85%?

26:24.100 --> 26:26.100
I understand the tendency for that.

26:26.100 --> 26:29.100
I think the average person is like, well, is it going to happen or is it not?

26:29.100 --> 26:31.100
It's like 10%, 50%, 100%.

26:31.100 --> 26:35.100
So it's definitely never 100 because all knowledge is probabilistic and there's uncertainty everywhere.

26:35.100 --> 26:40.100
But why I spent some time on the book with the uncertainty is the uncertainty is concerning.

26:40.100 --> 26:43.100
It's not that things get better when we say we don't know.

26:43.100 --> 26:46.100
I think sometimes people hear like, well, it could be two years away.

26:46.100 --> 26:48.100
It could be 10 years away, 20 years away.

26:48.100 --> 26:50.100
And they kind of just leave it at that.

26:50.100 --> 26:53.100
Like, wait a minute, if you don't know, it could be two years away.

26:53.100 --> 26:57.100
So how do we make decisions faced with uncertainty?

26:57.100 --> 26:58.100
Decisions have to, right?

26:58.100 --> 27:01.100
The world is full of complexity and uncertainty and decisions have to be made.

27:01.100 --> 27:06.100
And it's tempting to think decisions don't have to be made, that you can just be agnostic.

27:06.100 --> 27:08.100
It doesn't quite work that way.

27:08.100 --> 27:11.100
Now, you know, this is more just me highlighting a complexity of the world.

27:11.100 --> 27:13.100
Everyone can't care about every issue.

27:13.100 --> 27:15.100
Everyone can't read up and be knowledgeable about every issue.

27:15.100 --> 27:20.100
But I do want to highlight that if you happen to be in the AI space and you say we don't know,

27:20.100 --> 27:23.100
that's almost more concerning than someone that says it's five years away.

27:23.100 --> 27:27.100
Because if you actually mean we don't know tomorrow, a month from now.

27:27.100 --> 27:29.100
And like, oh, no, no, I don't mean that.

27:29.100 --> 27:31.100
I mean, you know, there's a 1% chance in the next 15 years.

27:31.100 --> 27:34.100
Like, oh, okay, so you do have something, right?

27:34.100 --> 27:38.100
I guess I'm trying to highlight that, again, the human brain, we go through life

27:38.100 --> 27:42.100
and we're making probability based decisions in a very loose sense.

27:42.100 --> 27:44.100
Whether you get a mortgage, whether you might have kids,

27:44.100 --> 27:46.100
whether you get a certain insurance and for how long,

27:46.100 --> 27:49.100
these are often based on some sense of what the future is going to be like.

27:49.100 --> 27:53.100
Similarly, when we're engaging with AI stuff, we are making decisions

27:53.100 --> 27:55.100
or not based on what the future is like.

27:55.100 --> 27:58.100
Now, you could say there's a disconnect between what someone says

27:58.100 --> 28:00.100
and what they do and behavior.

28:00.100 --> 28:01.100
And that's true.

28:01.100 --> 28:04.100
Sometimes we're dramatically inconsistent and then no human is immune from this, right?

28:04.100 --> 28:06.100
I won't eat junk food and then I'm eating junk food.

28:06.100 --> 28:07.100
What happened?

28:07.100 --> 28:10.100
Well, clearly there's conflicting impulses within the human organism.

28:10.100 --> 28:13.100
Okay, but I really just want to sort of highlight the complexity there

28:13.100 --> 28:17.100
and have people reflect on, okay, I've said something.

28:17.100 --> 28:20.100
Does my behavior match it in any way?

28:20.100 --> 28:22.100
And if things are that uncertain,

28:22.100 --> 28:25.100
isn't it better to be prepared than to be caught off guard?

28:25.100 --> 28:29.100
Uncertainty isn't a justification for complacency.

28:29.100 --> 28:31.100
It's another way to put it.

28:31.100 --> 28:32.100
We can do something.

28:32.100 --> 28:37.100
We can make plans and we should probably make plans for each timeline you mentioned.

28:37.100 --> 28:41.100
In the book, you discuss some cognitive traits that AI might have.

28:41.100 --> 28:44.100
And I think this was quite interesting.

28:44.100 --> 28:48.100
And starting with speed, so AIs will think much faster than we do.

28:48.100 --> 28:50.100
This is something that's often overlooked.

28:50.100 --> 28:55.100
I find in debates, what does it mean if you have human level AI,

28:55.100 --> 28:59.100
but it thinks a hundred or a thousand times faster than you?

28:59.100 --> 29:02.100
Is it still reasonable to call it human level?

29:02.100 --> 29:04.100
I'm happy you picked up on that.

29:04.100 --> 29:06.100
Yeah, so this goes to the what is an ASI?

29:06.100 --> 29:09.100
What kind of thing is it to try to help people along?

29:09.100 --> 29:13.100
We don't quite know what an ASI might be or exactly how it will be,

29:13.100 --> 29:17.100
but I was trying to have people understand what it might likely be.

29:17.100 --> 29:20.100
And so there's a couple key traits and then a couple other possible ones.

29:20.100 --> 29:21.100
So you have speed in there.

29:21.100 --> 29:22.100
We'll talk about that first.

29:22.100 --> 29:26.100
And there's capability, reliability and insight and these sorts of things.

29:26.100 --> 29:29.100
But as again, just imagine someone's never talked about AGI

29:29.100 --> 29:31.100
or they never thought about superintelligence like,

29:31.100 --> 29:33.100
what the heck are these guys talking about?

29:33.100 --> 29:34.100
It's a computer system.

29:34.100 --> 29:35.100
What's it going to look like?

29:35.100 --> 29:38.100
And yes, they'll probably draw on science fiction or popular movies.

29:38.100 --> 29:41.100
And some of those are misleading and some of those are useful.

29:41.100 --> 29:43.100
But it's like, what are we talking about?

29:43.100 --> 29:49.100
So I thought very likely AI systems, ASI systems, AGI system will function very quickly.

29:49.100 --> 29:51.100
And this is because computers generally work very quickly.

29:51.100 --> 29:53.100
That's why they're so amazing.

29:53.100 --> 29:57.100
Again, people listening to this, our ability to record this meeting and discussion.

29:57.100 --> 30:02.100
It's because so many things are happening so fast in a way that our brains can't comprehend

30:02.100 --> 30:04.100
that it just seems smooth, seamless and easy.

30:04.100 --> 30:08.100
So with an AI system, as you've seen it, if you've ever used, you know,

30:08.100 --> 30:11.100
one of the chatbots, the recent models or even the image generators.

30:11.100 --> 30:14.100
Oh, let me think for a second and then out comes the output.

30:14.100 --> 30:16.100
And it's usually remarkable, right?

30:16.100 --> 30:20.100
It happens at a speed that no human could possibly generate in that amount of time.

30:20.100 --> 30:26.100
And so I think there it's the flaw or the, let's say the limitation of any definition.

30:26.100 --> 30:28.100
There's always going to be an asterisk where like,

30:28.100 --> 30:31.100
well, in other contexts, it's not quite like this.

30:31.100 --> 30:33.100
That famous Bertrand Russell quote,

30:33.100 --> 30:37.100
everything is vague to a degree you do not realize until you've tried to make it precise.

30:37.100 --> 30:40.100
The ability is the case because any word you can find like, well,

30:40.100 --> 30:41.100
what does this word mean?

30:41.100 --> 30:44.100
And you know, so nice people spend an entire thesis or a book defining a word

30:44.100 --> 30:46.100
and then someone argues it's not that word and that sort of thing.

30:46.100 --> 30:50.100
But anyway, so with a GI, I think if we think about a general ability,

30:50.100 --> 30:55.100
we have to understand again that the coworker thing is useful as a sort of a test.

30:55.100 --> 30:56.100
It's sort of a clear bar.

30:56.100 --> 31:00.100
Now you could say, well, aren't there different types of coworkers and different types of environments

31:00.100 --> 31:03.100
and some coworkers and entire even industries or companies in different countries

31:03.100 --> 31:05.100
will be smarter or more capable than other ones?

31:05.100 --> 31:08.100
Sure, but we have to have some way of talking about this.

31:08.100 --> 31:12.100
Otherwise, we kind of get lost and at least I'm trying to be consistent in that way.

31:12.100 --> 31:17.100
So if it was the case that your average coworker could do something a thousand times faster than you,

31:17.100 --> 31:21.100
it doesn't quite seem like your average coworker anymore, right?

31:21.100 --> 31:23.100
And then it's like, well, how do we deal with this then?

31:23.100 --> 31:27.100
Like, well, when we examine people or give them evaluations in their workplace,

31:27.100 --> 31:29.100
some people have strengths that other people don't.

31:29.100 --> 31:33.100
And there is some sort of mishmash of like, well, are they insightful?

31:33.100 --> 31:34.100
Are they analytical?

31:34.100 --> 31:35.100
Are they on time?

31:35.100 --> 31:36.100
Are they, you know, friendly?

31:36.100 --> 31:38.100
Are they good to work with all these different things?

31:38.100 --> 31:40.100
And it becomes an amalgam of an evaluation.

31:40.100 --> 31:44.100
And similarly with AI or AGI will sort of do something similar.

31:44.100 --> 31:48.100
I think in the practical sense that if there's a task and it comes back one second later

31:48.100 --> 31:52.100
and your colleague would have taken three days, well, that's not quite human level, right?

31:52.100 --> 31:53.100
That's above human level.

31:53.100 --> 31:58.100
Now, if it has errors that the human never would have made, like, well, okay,

31:58.100 --> 32:01.100
that makes it maybe a bit less in human level.

32:01.100 --> 32:04.100
The AI gives you something, it takes you two days to fix it.

32:04.100 --> 32:06.100
And another process would have taken three days.

32:06.100 --> 32:10.100
Well, now it's above human level, but not as much as it seemed originally.

32:10.100 --> 32:15.100
It seems to me that we are in front on the speed aspect with AI right now.

32:15.100 --> 32:21.100
So, so AI is often, if we talk to GPT-4, you get an answer almost instantly.

32:21.100 --> 32:26.100
The answer won't be as well done as a human expert could.

32:26.100 --> 32:29.100
So it seems to me that we are in front on the speed aspect,

32:29.100 --> 32:34.100
but we are behind on the kind of depth aspect as things stand right now.

32:34.100 --> 32:40.100
Who knows with the upcoming models that might lead us into talking about AI insight,

32:40.100 --> 32:45.100
which is kind of, you can think about it as the AI's understanding of its context

32:45.100 --> 32:48.100
and the relationships it's engaging in.

32:48.100 --> 32:53.100
And you expect the AI's to have greater insight and to understand their context

32:53.100 --> 32:57.100
and understand their relationships perhaps even better than humans at some point.

32:57.100 --> 32:58.100
Why is that?

32:58.100 --> 32:59.100
Yes, that's a great point.

32:59.100 --> 33:03.100
It's also something that gives me the, not the most concern, but it is a concern.

33:03.100 --> 33:05.100
So we'll do like the first part first.

33:05.100 --> 33:09.100
As you said before, if someone is super fast,

33:09.100 --> 33:12.100
but they're not necessarily good at pattern detection or insight,

33:12.100 --> 33:16.100
they're kind of missing a lot of what makes useful product services

33:16.100 --> 33:18.100
or work in the broad sense.

33:18.100 --> 33:22.100
And right now, we already have algorithms and whatnot

33:22.100 --> 33:25.100
that can do sort of pattern detection in a way that a human just can't, right?

33:25.100 --> 33:28.100
In a way, this is how most of the recommendation algorithms,

33:28.100 --> 33:31.100
whether it's your Netflix or online social media,

33:31.100 --> 33:35.100
it's processing vast amounts of data based on what you've given it in terms of clicks,

33:35.100 --> 33:38.100
likes, even just moments spent looking at something

33:38.100 --> 33:41.100
and then spitting out something you probably like to some extent or another.

33:41.100 --> 33:43.100
If a human was doing this,

33:43.100 --> 33:46.100
oh yeah, give me three months to analyze reams of data.

33:46.100 --> 33:50.100
And then I think this guy might as a 70% chance of liking this one YouTube video.

33:50.100 --> 33:54.100
That's actually impressive for the human, but not at all useful in the real world.

33:54.100 --> 33:58.100
And so as they're designed right now, the systems are already,

33:58.100 --> 34:01.100
we'll say, super good at pattern detection.

34:01.100 --> 34:06.100
And there's even a story where some researchers were doing different analyses

34:06.100 --> 34:13.100
and an AI was able to detect a patient's race from scans like X-rays of a person's body.

34:13.100 --> 34:15.100
But the researchers couldn't figure it out.

34:15.100 --> 34:18.100
They tried to realize or understand what the AI system was doing

34:18.100 --> 34:20.100
because they would block certain parts of the anatomy.

34:20.100 --> 34:23.100
Like maybe it's looking at the heart or something in the lungs

34:23.100 --> 34:24.100
and they couldn't figure it out.

34:24.100 --> 34:28.100
Now, maybe there is an answer because there has to be some sort of thing the AI was doing,

34:28.100 --> 34:32.100
but it's already the case where AI systems can figure something out that humans cannot.

34:32.100 --> 34:35.100
And even after it's done, humans still can't.

34:35.100 --> 34:39.100
And I think, as you said before, the lag right now is not quite the speed or even the insight.

34:39.100 --> 34:43.100
It's almost the integration, the flexibility, in a way the generality

34:43.100 --> 34:46.100
that right now humans are very sophisticated general creatures.

34:46.100 --> 34:49.100
So if you give them an intellectual task, they can do a wide range of things,

34:49.100 --> 34:51.100
but also they do it in a certain way.

34:51.100 --> 34:54.100
And the world isn't yet restructured.

34:54.100 --> 34:58.100
Even how people talk to these machines, that has a certain nuance and a sophistication.

34:58.100 --> 35:02.100
Sort of like, well, the book purposely does not deal with robotics

35:02.100 --> 35:03.100
because I think that's a separate thing.

35:03.100 --> 35:04.100
It's an interesting and amazing thing.

35:04.100 --> 35:08.100
But an Amazon warehouse, you can't just stick a robot in there or automatic machines.

35:08.100 --> 35:12.100
It has to be designed in a certain way to have a true efficiency and effectiveness.

35:12.100 --> 35:16.100
Similarly, I think the chatbots, large language models, other AI systems in the future

35:16.100 --> 35:21.100
will also provide great value, but they're not quite integrated in a way that works.

35:21.100 --> 35:25.100
The second thing, when I said the insight concerns me a bit,

35:25.100 --> 35:30.100
is again, if you take a big step back, how did humans manage to do what they do, right?

35:30.100 --> 35:35.100
They managed to figure out how things work, like the laws of physics, how things fit together,

35:35.100 --> 35:37.100
putting different objects and different pieces together.

35:37.100 --> 35:43.100
We can sort of rearrange matter to create things, like cars, like houses, like computers.

35:43.100 --> 35:46.100
And this is also inspired by David Deutsch, Beginnings of Infinity.

35:46.100 --> 35:49.100
It's like, well, our brains just managed to figure it out.

35:49.100 --> 35:53.100
There's all this raw material that was inside the earth, and we were able to take it out

35:53.100 --> 35:57.100
and refine it and put it together, and now we have wonderful devices.

35:57.100 --> 36:01.100
And that was hard and difficult, and I couldn't do it, but someone figured out groups of people,

36:01.100 --> 36:05.100
teams of people, generations of people, and that's amazing.

36:05.100 --> 36:09.100
And the concern for me sometimes is, what if an AI that's super insightful

36:09.100 --> 36:12.100
can sort of see the universe in a way that we can't?

36:12.100 --> 36:15.100
And it might unlock a truly new fundamental law of physics.

36:15.100 --> 36:16.100
That's a possibility, though.

36:16.100 --> 36:20.100
But it is the case, like, well, what if you just sort of tilt the world like this,

36:20.100 --> 36:25.100
and like, oh, of course, of course, Wi-Fi, as a technology, it was always possible.

36:25.100 --> 36:27.100
We just didn't know how to do it.

36:27.100 --> 36:31.100
And of course, recently, people have shown you can use Wi-Fi as a way to sort of scan a room

36:31.100 --> 36:33.100
and create models of where people are and how they move.

36:33.100 --> 36:35.100
And that, of course, was also always possible.

36:35.100 --> 36:39.100
The laws of physics haven't changed in our lifetime, and they haven't changed as far as, you know,

36:39.100 --> 36:41.100
much in 13.8 billion years since the birth of the universe.

36:41.100 --> 36:44.100
So it's really this insight that allows you to figure things out,

36:44.100 --> 36:48.100
which if you understand the world and the universe more than other people,

36:48.100 --> 36:49.100
you can do dramatic things.

36:49.100 --> 36:51.100
You can create wonderful inventions.

36:51.100 --> 36:55.100
You also have a huge advantage in terms of how to manipulate systems

36:55.100 --> 36:59.100
or perhaps how to escape from systems, where someone else just didn't realize,

36:59.100 --> 37:03.100
oh, if you manipulate the cooling fan in a computer,

37:03.100 --> 37:05.100
you might be able to gain access to different things.

37:05.100 --> 37:06.100
Again, that's pretty remarkable.

37:06.100 --> 37:08.100
It was always possible until someone figured it out, though.

37:08.100 --> 37:09.100
We didn't know.

37:09.100 --> 37:13.100
Yeah, I do wonder whether AIs, advanced AIs,

37:13.100 --> 37:18.100
will be able to solve some of the science problems where humans have been less successful.

37:18.100 --> 37:23.100
So I think we've been successful in domains that are, in some sense, simple.

37:23.100 --> 37:27.100
Not simple to understand, but can really be reduced to something simple, like physics.

37:27.100 --> 37:32.100
As soon as we get into something very complex, like biology or psychology,

37:32.100 --> 37:34.100
we haven't had the same level of success.

37:34.100 --> 37:39.100
I wonder whether AIs are well suited to tasks where there's a lot of data

37:39.100 --> 37:43.100
and complex data and whether they might be able to make progress

37:43.100 --> 37:46.100
so that they have superhuman insight about our own minds.

37:46.100 --> 37:51.100
And you could see how that would be dangerous to be exposed to manipulation

37:51.100 --> 37:56.100
or otherwise being interacting with a system like that.

37:56.100 --> 37:57.100
I think it could also be amazing, right?

37:57.100 --> 37:59.100
That's how most of this stuff goes.

37:59.100 --> 38:00.100
It cuts both ways.

38:00.100 --> 38:01.100
The uncertainty cuts both ways.

38:01.100 --> 38:03.100
I'm just for listeners.

38:03.100 --> 38:05.100
I am a huge fan of science.

38:05.100 --> 38:06.100
I have a massive science.

38:06.100 --> 38:09.100
I've always been a big fan of science, but it is also limited, right?

38:09.100 --> 38:12.100
We know there are humans in a way we're a bunch of apes trying to figure things out, right?

38:12.100 --> 38:15.100
And I think we've done pretty well, but we know there's limitations.

38:15.100 --> 38:17.100
There's replication crisis.

38:17.100 --> 38:21.100
There's even just the reality of millions of scientific publications each year.

38:21.100 --> 38:22.100
Who can read all these?

38:22.100 --> 38:24.100
And so I years ago thought, you know what?

38:24.100 --> 38:27.100
Humans should make a good, strong effort, but won't it be nice

38:27.100 --> 38:32.100
if some AI system, pick your field, economics, psychology, biology, chemistry, whatever it is,

38:32.100 --> 38:33.100
and they could read a thousand papers.

38:33.100 --> 38:34.100
Like, you know what we need?

38:34.100 --> 38:38.100
We need this particular type of experiment that addresses these four things

38:38.100 --> 38:40.100
that these thousand didn't in the right way.

38:40.100 --> 38:44.100
So that, you know, at the moment, it actually fills me with like more happiness.

38:44.100 --> 38:46.100
Like, oh, we might actually really figure things out,

38:46.100 --> 38:49.100
even though, of course, depending on what's figured out, it could be bad.

38:49.100 --> 38:53.100
But there's so many complexities here that humans just can't easily disentangle.

38:53.100 --> 38:55.100
This is the pattern recognition.

38:55.100 --> 39:00.100
This is the insight that I really hope with AI systems, we can make some dramatic progress.

39:00.100 --> 39:05.100
Like, okay, you know, that thing we thought might have had an effect size of, you know, a certain size or not.

39:05.100 --> 39:07.100
It just isn't the case, or it kind of is.

39:07.100 --> 39:10.100
Or it's a 1% thing that people just shouldn't pay much attention to,

39:10.100 --> 39:12.100
given this other thing, which is much more important.

39:12.100 --> 39:17.100
So I'm pretty happy at the moment of the idea of AI really improving science

39:17.100 --> 39:21.100
while acknowledging that, depending on the field, that could also be a problem.

39:21.100 --> 39:26.100
Yeah, I'm also pretty excited for seeing what AI can do in the science realm.

39:26.100 --> 39:31.100
I wonder if we can create more systems like AlphaFold, DeepMinds AlphaFold,

39:31.100 --> 39:37.100
that was able to figure out how proteins fold,

39:37.100 --> 39:43.100
where it's a system that's narrow but highly capable in a specific domain

39:43.100 --> 39:46.100
and helps us solve a longstanding science problem.

39:46.100 --> 39:51.100
I do think the trade-off between risk and reward is pretty great for such systems.

39:51.100 --> 39:57.100
Do you think that's a potentially responsible way forward to push hard on narrow systems

39:57.100 --> 39:59.100
that help us with scientific problems?

39:59.100 --> 40:00.100
I definitely do.

40:00.100 --> 40:04.100
And as you pointed out, AlphaFold and other highly capable narrow systems,

40:04.100 --> 40:06.100
it's amazing, it's amazing what they've done, right?

40:06.100 --> 40:08.100
And, you know, we're just getting started type thing.

40:08.100 --> 40:13.100
That said, we know the incentives for generality, general purpose systems is very high.

40:13.100 --> 40:18.100
Whether we can kind of tweak people towards having like, look, you want a certain capability,

40:18.100 --> 40:22.100
this is a particular problem, it's no way easier to solve if you just focus on it,

40:22.100 --> 40:24.100
then why don't we focus on that?

40:24.100 --> 40:29.100
That said, there's other problems where it turns out being general is better, right?

40:29.100 --> 40:32.100
And I think there's a nuance and a delicacy here or a complexity

40:32.100 --> 40:34.100
that we're just going to kind of see how things pan out.

40:34.100 --> 40:38.100
I'm aware of Mustafa Suleiman, he has a sort of personal assistant AI

40:38.100 --> 40:42.100
and their plan is not to make AGI, but a good personal assistant.

40:42.100 --> 40:47.100
And I guess the question is, well, are you going to have to end up making something like an AGI

40:47.100 --> 40:50.100
to have a personal assistant that is as good as the other personal assistants

40:50.100 --> 40:53.100
that are more AGI like from other companies?

40:53.100 --> 40:55.100
And it seems like that's probably the case.

40:55.100 --> 40:58.100
It's kind of hard to think why wouldn't it go in this direction?

40:58.100 --> 41:01.100
In terms of developing systems, as things get more capable,

41:01.100 --> 41:04.100
again, the computational power that's coming on board is staggering.

41:04.100 --> 41:09.100
It's not clear if it's even much work to kind of stick something in on the side, so to speak.

41:09.100 --> 41:12.100
Do we need like a single purpose machine, so to speak,

41:12.100 --> 41:14.100
or can it be multiple things in combination?

41:14.100 --> 41:18.100
And I know some of the other AI labs are experimenting with these types of things.

41:18.100 --> 41:23.100
And even if the machine or the AI is kind of isolated on its own and has certain capabilities,

41:23.100 --> 41:27.100
we know once you connect it to numerous other applications through APIs or whatnot,

41:27.100 --> 41:29.100
it becomes far more capable.

41:29.100 --> 41:32.100
So I think it'd be nice if we can focus on narrow things,

41:32.100 --> 41:36.100
and I think that's a good path forward and maybe even like the most reasonable one

41:36.100 --> 41:38.100
in a risk-averse cautious sense.

41:38.100 --> 41:41.100
I'm wary about the viability of it.

41:41.100 --> 41:42.100
That said, we should still try.

41:42.100 --> 41:44.100
And we're kind of just kind of see what plays out.

41:44.100 --> 41:49.100
But I think even if you make something highly capable like Google did with Alpha Fold,

41:49.100 --> 41:50.100
we're going to see how it goes.

41:50.100 --> 41:53.100
I mean, Gemini, their product is supposed to come out, I think, within a month or two,

41:53.100 --> 41:55.100
and that's supposed to be very agentic,

41:55.100 --> 41:58.100
and it's kind of like Alpha Fold plus the other one if you hear the rumors.

41:58.100 --> 42:01.100
And yeah, can you get some sort of, I know, amalgam Voltron thing?

42:01.100 --> 42:04.100
We're like, well, it turns out in a two or three years,

42:04.100 --> 42:06.100
it's actually relatively easy to stick all these things together.

42:06.100 --> 42:09.100
And you thought you were doing five amazing narrow systems.

42:09.100 --> 42:14.100
Well, I just created something that it figured itself out how to combine all these things.

42:14.100 --> 42:16.100
And now we have what we were trying not to have.

42:16.100 --> 42:22.100
So I think it's worth trying and also being wary and aware of the concerns that might happen if we combine them all.

42:22.100 --> 42:27.100
What do you think the relationship is between generality and autonomy?

42:27.100 --> 42:32.100
It seems easier, in my view, to avoid creating autonomous systems

42:32.100 --> 42:35.100
or more autonomous systems if the systems are narrow.

42:35.100 --> 42:41.100
Is there some sort of relationship in which when you push on generality, you also push on autonomy?

42:41.100 --> 42:43.100
I do think they're very much linked, as you said.

42:43.100 --> 42:45.100
I kind of have a similar view, I think,

42:45.100 --> 42:48.100
that if you were having something that was designed for a specific task,

42:48.100 --> 42:51.100
it can be less general and maybe less autonomous.

42:51.100 --> 42:54.100
But as we talked about before, these things are nuanced

42:54.100 --> 42:58.100
and there's overlapping aspects of, we'll say, the distributions and some things that aren't.

42:58.100 --> 43:03.100
So with, say, credit card, bank fraud detection, that sort of thing, these are pretty narrow.

43:03.100 --> 43:07.100
They're highly autonomous and they're not really that general, though, right?

43:07.100 --> 43:10.100
And you could imagine something else that is pretty general at the moment.

43:10.100 --> 43:13.100
Maybe most of the language models, they're pretty general.

43:13.100 --> 43:14.100
They're not really autonomous.

43:14.100 --> 43:18.100
You ask it to do something and if you didn't, it wouldn't do something on its own for the most part.

43:18.100 --> 43:20.100
So those seem entirely disconnected.

43:20.100 --> 43:24.100
So like everything in this world, there's kind of multiple overlapping distributions

43:24.100 --> 43:27.100
where you could imagine things are correlated in some ways,

43:27.100 --> 43:30.100
but in other examples, they may be pretty distinct.

43:30.100 --> 43:34.100
I think there will be pretty high demand for autonomous systems.

43:34.100 --> 43:36.100
You mentioned the AI personal assistant.

43:36.100 --> 43:39.100
I think what consumers want from a such a system

43:39.100 --> 43:43.100
is to simply tell it once to order the flight tickets to wherever

43:43.100 --> 43:46.100
and never think about it again and just everything works out.

43:46.100 --> 43:49.100
That sounds pretty great to me, at least.

43:49.100 --> 43:53.100
What do you think about the commercial incentives for autonomy?

43:53.100 --> 43:54.100
I think they're staggeringly high.

43:54.100 --> 43:55.100
That's the best way to say it.

43:55.100 --> 43:58.100
I fully agree that why wouldn't someone just want to...

43:58.100 --> 44:00.100
Can't you do it for me?

44:00.100 --> 44:01.100
Let's be honest.

44:01.100 --> 44:03.100
We as humans want things to happen

44:03.100 --> 44:07.100
and a lot of things that we do want to happen require administrative burden.

44:07.100 --> 44:09.100
I hate filling out forms.

44:09.100 --> 44:12.100
Even if it takes 10 minutes psychologically, it seems to take an hour.

44:12.100 --> 44:13.100
There's a complete disconnect here.

44:13.100 --> 44:16.100
So if someone could do that for me, something could do that for me,

44:16.100 --> 44:18.100
yeah, give it more power.

44:18.100 --> 44:20.100
So I think this really highlights...

44:20.100 --> 44:22.100
We're getting at the notion of control

44:22.100 --> 44:25.100
that I think if we're exploring how AI works

44:25.100 --> 44:27.100
and how it's going to function in our world,

44:27.100 --> 44:31.100
we will initially willingly give up control for integrated AI systems.

44:31.100 --> 44:34.100
Yes, you can imagine the financial incentives are huge

44:34.100 --> 44:37.100
to provide these products to people, the AI assistants.

44:37.100 --> 44:39.100
They could be just normal, say personal assistants.

44:39.100 --> 44:40.100
They could be therapists.

44:40.100 --> 44:43.100
They could be companions, all rolled into one.

44:43.100 --> 44:46.100
The military incentives, the governments have a desire to use them

44:46.100 --> 44:48.100
to help citizens, maybe even for surveillance of citizens,

44:48.100 --> 44:50.100
pros and cons all over the place.

44:50.100 --> 44:54.100
So there's all these reasons why driving towards more AI,

44:54.100 --> 44:58.100
good, helping consumers, helping businesses, helping governments,

44:58.100 --> 45:00.100
do things that they want to achieve.

45:00.100 --> 45:02.100
And as things become more and more integrated,

45:02.100 --> 45:05.100
then it becomes difficult to disentangle them

45:05.100 --> 45:07.100
or perhaps stop it before it gets long too far.

45:07.100 --> 45:08.100
Say more about that.

45:08.100 --> 45:12.100
Why is it difficult to disentangle after you've integrated?

45:12.100 --> 45:16.100
Say people have begun using AI personal assistants.

45:16.100 --> 45:20.100
That's not a high risk, let's say, application.

45:20.100 --> 45:23.100
But why is it difficult to then stop using these systems

45:23.100 --> 45:25.100
once you've begun using them?

45:25.100 --> 45:29.100
So people kind of adapt to their world and their expectations change a bit.

45:29.100 --> 45:32.100
And we can use an already existing case study.

45:32.100 --> 45:35.100
So with the replica app, which allows people to have AI companions

45:35.100 --> 45:39.100
and often form relationships, including romantic or otherwise,

45:39.100 --> 45:42.100
they went through a sort of upgrade or reboot

45:42.100 --> 45:44.100
or they're tweaked to their model and their code

45:44.100 --> 45:48.100
such that explicit conversations and things were not allowed anymore.

45:48.100 --> 45:51.100
And some of the users who built up relationships

45:51.100 --> 45:54.100
with these artificial AI little systems felt devastated.

45:54.100 --> 45:57.100
They felt like the person that they knew and even loved

45:57.100 --> 45:59.100
is just gone and they were depressed.

45:59.100 --> 46:01.100
And so here, once you start along a path,

46:01.100 --> 46:03.100
it sometimes becomes very hard to shift out.

46:03.100 --> 46:05.100
And we know this perhaps for people who thought,

46:05.100 --> 46:07.100
well, that replica thing, I would never do that.

46:07.100 --> 46:09.100
Most of us have a phone, a smartphone of some type.

46:09.100 --> 46:13.100
And nowadays, it's pretty hard to not exist in this world without one.

46:13.100 --> 46:15.100
The utility is just so high, right?

46:15.100 --> 46:17.100
It's great. You have all these functionalities

46:17.100 --> 46:19.100
and you can talk to people and I don't have to sell the phone on you,

46:19.100 --> 46:21.100
but phones are great.

46:21.100 --> 46:24.100
So great, in fact, that it's really hard to not have one.

46:24.100 --> 46:26.100
So you could say, well, I could choose not to have a phone.

46:26.100 --> 46:27.100
You could.

46:27.100 --> 46:31.100
But at the moment, you'd actually be reducing your control by not having a phone.

46:31.100 --> 46:33.100
But if you really start to think through the phone,

46:33.100 --> 46:37.100
like, okay, well, the phone company could kind of just brick my phone and stop it.

46:37.100 --> 46:42.100
At one point, Apple put that U2 album on many people's phones through the Play Store.

46:42.100 --> 46:45.100
They realized they didn't have as much control as they thought.

46:45.100 --> 46:49.100
You kind of need the internet or something like it for most of the applications on your phone.

46:49.100 --> 46:53.100
When you're using internet, you're clicking, I agree, to user agreements, perhaps,

46:53.100 --> 46:56.100
or agreement of cookies and tracking data all over the place.

46:56.100 --> 46:58.100
So there's all these ways in which we're, again,

46:58.100 --> 47:02.100
willingly giving up power and control for, again, other types of power and control.

47:02.100 --> 47:05.100
We want to talk to our friends. We want to watch funny videos.

47:05.100 --> 47:07.100
We want to listen to podcasts or watch them.

47:07.100 --> 47:08.100
And that seems great.

47:08.100 --> 47:12.100
But now, once we're here, if you said to people, okay, we're just shutting it all down.

47:12.100 --> 47:14.100
You can't, like, listen to podcasts anymore.

47:14.100 --> 47:15.100
You can't watch YouTube.

47:15.100 --> 47:19.100
Or, like, sort of, like, more dramatically, is Facebook too big to fail?

47:19.100 --> 47:22.100
Is it the case where, although a lot of people don't like Facebook for various reasons,

47:22.100 --> 47:25.100
a lot of people really do, and it's billions of users,

47:25.100 --> 47:29.100
is it the case that Facebook could just go down and people wouldn't be upset?

47:29.100 --> 47:31.100
A lot of people have built their lives on it.

47:31.100 --> 47:32.100
That's where their friends are.

47:32.100 --> 47:33.100
That's where their photos are.

47:33.100 --> 47:34.100
That's where their memories are.

47:34.100 --> 47:36.100
I actually enjoy the memory feature.

47:36.100 --> 47:39.100
What did I do on this day five, six, seven years ago?

47:39.100 --> 47:43.100
And I appreciate that because, again, human brains are fragile.

47:43.100 --> 47:45.100
I sometimes did fun things. It's nice to remember them.

47:45.100 --> 47:47.100
I wouldn't have remembered it without the prompt from the algorithm.

47:47.100 --> 47:51.100
So, in that sense, you sort of see that the incentive is very high.

47:51.100 --> 47:52.100
It's going to be hard to opt out.

47:52.100 --> 47:54.100
I'm not saying people should use social media all the time

47:54.100 --> 47:57.100
or that social media doesn't have also lots of problems.

47:57.100 --> 48:00.100
But if you think of, like, citizens in the world and what they would want

48:00.100 --> 48:04.100
and how they would react, you could imagine it wouldn't go so well

48:04.100 --> 48:07.100
if a government is just like, okay, we're just not going to let people do this anymore.

48:07.100 --> 48:10.100
Or when people say they don't have internet access for a very short period of time,

48:10.100 --> 48:12.100
that's usually very concerning for a lot of people.

48:12.100 --> 48:15.100
If you just didn't have it for weeks and you weren't prepared,

48:15.100 --> 48:17.100
this would be truly destabilizing.

48:17.100 --> 48:22.100
Most people nowadays of a certain generation, they don't know where their friends live.

48:22.100 --> 48:23.100
They don't know their friend's phone number.

48:23.100 --> 48:25.100
They may not know their email off my heart, maybe.

48:25.100 --> 48:27.100
And they don't know their last name.

48:27.100 --> 48:30.100
So, you have all these things where, like, oh, yeah, that person,

48:30.100 --> 48:33.100
they're handle on Twitter, they're handle on Instagram or who I interact with

48:33.100 --> 48:34.100
and that's how you talk to them.

48:34.100 --> 48:37.100
But if that went away, you would lose a vast social network

48:37.100 --> 48:40.100
and almost have no ability to reclaim it in any way

48:40.100 --> 48:42.100
if the infrastructure was taken out of place.

48:42.100 --> 48:46.100
So, all this is these push-pull tensions where once things are in place,

48:46.100 --> 48:48.100
it becomes very hard to disentangle,

48:48.100 --> 48:52.100
and that makes them even more and more robust and desirable.

48:52.100 --> 48:57.100
Yeah, we can think about, say, all the world's scientists came to us

48:57.100 --> 49:02.100
and told us the internet is going to become very dangerous in 10 years.

49:02.100 --> 49:04.100
We must shut the internet down.

49:04.100 --> 49:08.100
Would the world be able to coordinate around this project?

49:08.100 --> 49:11.100
Would it be possible for us to shut down the internet?

49:11.100 --> 49:14.100
Given the uncertainties, given the different incentives

49:14.100 --> 49:17.100
that different groups have, I think that would be extremely dangerous.

49:17.100 --> 49:19.100
Now, the internet is not going to...

49:19.100 --> 49:23.100
It doesn't represent a danger to us like AI might do.

49:23.100 --> 49:26.100
But it's just thinking about when these systems have begun

49:26.100 --> 49:28.100
become integrated into our society,

49:28.100 --> 49:32.100
how difficult it is to step out of these systems again.

49:32.100 --> 49:35.100
Sure, and that's actually why I chose that example in the book.

49:35.100 --> 49:38.100
And when you're trying to think about artificial superintelligence

49:38.100 --> 49:40.100
and why it's so powerful, if you talk to an average person,

49:40.100 --> 49:43.100
one of the first things they say is, like, can't you just shut it off?

49:43.100 --> 49:45.100
Why don't you just shut it down? What's the big deal here?

49:45.100 --> 49:48.100
And I think the internet is probably the most useful analogy

49:48.100 --> 49:50.100
as a comparator, as, again, an imagination device.

49:50.100 --> 49:52.100
Like, okay, let's think through the internet.

49:52.100 --> 49:54.100
Right now, the internet is very robust.

49:54.100 --> 49:56.100
It is designed to be robust, right?

49:56.100 --> 49:58.100
Because not only do the average person want it,

49:58.100 --> 50:00.100
global commerce hinges upon it,

50:00.100 --> 50:02.100
various municipal services, hospitals, everything else.

50:02.100 --> 50:04.100
Like, so many things are now integrated into the internet.

50:04.100 --> 50:07.100
As anyone who's had to know, like a smart home now is like,

50:07.100 --> 50:09.100
what? I can't get into my house because the internet's down

50:09.100 --> 50:11.100
or I can't control the temperature.

50:11.100 --> 50:13.100
So, again, pros and cons once you're integrated, right?

50:13.100 --> 50:16.100
All that to say is, you know, there's cables that are crossing the ocean

50:16.100 --> 50:19.100
and many other places that enable the internet to happen.

50:19.100 --> 50:22.100
And there is currently no way to just shut down the internet.

50:22.100 --> 50:25.100
If, like, all the main governments agreed,

50:25.100 --> 50:28.100
then maybe you could make a lot of progress in it.

50:28.100 --> 50:31.100
But as I say in the book, there'd be a huge vulnerability.

50:31.100 --> 50:33.100
Like, if you created an internet kill switch,

50:33.100 --> 50:36.100
oh, that's a great opportunity for some terrorists to be like,

50:36.100 --> 50:37.100
why don't I just press the button?

50:37.100 --> 50:39.100
Because it would wreak havoc and cause a lot of damage.

50:39.100 --> 50:44.100
So there's a huge incentive to not even have a kill switch available,

50:44.100 --> 50:47.100
which is problem kind of one or problem and solution number one.

50:47.100 --> 50:51.100
And the second part is, are we going to get agreement

50:51.100 --> 50:54.100
from countries or representatives to then actually activate

50:54.100 --> 50:56.100
such a kill switch in an emergency?

50:56.100 --> 50:58.100
Like, you could imagine, okay, people, let's imagine,

50:58.100 --> 51:00.100
let's sort of ideal world.

51:00.100 --> 51:01.100
They've done their due diligence.

51:01.100 --> 51:02.100
We've somehow created a kill switch.

51:02.100 --> 51:03.100
It's very well protected.

51:03.100 --> 51:04.100
It's not actually a threat.

51:04.100 --> 51:09.100
And we even have a protocol that is designed with written clear reasons

51:09.100 --> 51:11.100
when you would activate the kill switch or not.

51:11.100 --> 51:13.100
And then it comes time to something bad happening.

51:13.100 --> 51:16.100
And everyone looks at the agreement that everyone agreed upon,

51:16.100 --> 51:19.100
the details, and like, well, I don't quite think .3 has been satisfied.

51:19.100 --> 51:21.100
And someone else, like, of course it has.

51:21.100 --> 51:25.100
And we have minutes to decide whether this is a good idea or not.

51:25.100 --> 51:27.100
I could just see that also being a problem.

51:27.100 --> 51:30.100
So this is the normal like human complicated making decisions together,

51:30.100 --> 51:32.100
understanding the world differently.

51:32.100 --> 51:34.100
And it's not that there's no hope at all,

51:34.100 --> 51:36.100
but it just highlights that we'd have to do a lot of work in advance

51:36.100 --> 51:39.100
and ensure that such a thing could exist if we needed it.

51:39.100 --> 51:41.100
All it to say is to your original premise,

51:41.100 --> 51:44.100
if the scientists presented clear enough evidence,

51:44.100 --> 51:46.100
I would hope that we won't say everyone

51:46.100 --> 51:48.100
because everyone doesn't agree on anything, right?

51:48.100 --> 51:51.100
But there's enough people with enough resources and power

51:51.100 --> 51:54.100
that the risk it would be clear enough that we should act

51:54.100 --> 51:57.100
and put things in place to make such a thing happen.

51:57.100 --> 51:59.100
With an artificial superintelligence,

51:59.100 --> 52:01.100
one could also see a similar like, well, how would you shut it off?

52:01.100 --> 52:04.100
Is it distributed in a way that even is worse than the Internet?

52:04.100 --> 52:06.100
Like, you know, you could be on computer servers

52:06.100 --> 52:10.100
in various places that are or are not connected to the Internet

52:10.100 --> 52:13.100
or fragments of a ASI that could recombine it to something else.

52:13.100 --> 52:16.100
So there's certainly a lot of reasons to be concerned.

52:16.100 --> 52:19.100
And we can use the present case of the Internet

52:19.100 --> 52:23.100
to see why it's going to be a challenge.

52:23.100 --> 52:27.100
How reliable do you believe current AI systems are

52:27.100 --> 52:30.100
and how reliable do you think they'll be in the future?

52:30.100 --> 52:33.100
I think they are mixed and they will be mixed.

52:33.100 --> 52:36.100
And so to elaborate on that, I think a lot of these systems,

52:36.100 --> 52:37.100
again, they're amazing.

52:37.100 --> 52:40.100
Certainly image generators, they're reliable or not, right?

52:40.100 --> 52:42.100
You kind of put something in, you get what you get.

52:42.100 --> 52:45.100
Do you get exactly what you want? Very rarely.

52:45.100 --> 52:48.100
I find with at least image generators, if someone's looking over your shoulder,

52:48.100 --> 52:49.100
they're like, that's amazing.

52:49.100 --> 52:51.100
You're like, that's not quite what I wanted.

52:51.100 --> 52:52.100
So there's a disconnect there.

52:52.100 --> 52:54.100
But to the language models, I think they're generally great.

52:54.100 --> 52:56.100
And then you have to fact check everything out.

52:56.100 --> 53:00.100
There's already, you know, numerous examples of lawyers citing false precedents

53:00.100 --> 53:04.100
that didn't exist, another case where I'm not sure which system,

53:04.100 --> 53:09.100
but one of them was asked for examples of sexual harassment cases involving lawyers.

53:09.100 --> 53:13.100
And with references, and it spit back this wonderful example of an individual

53:13.100 --> 53:17.100
who was a professor who sexually harassed some student on a trip to Alaska

53:17.100 --> 53:20.100
with citations from, you know, Washington Post or something like that.

53:20.100 --> 53:21.100
The whole thing was fake.

53:21.100 --> 53:25.100
The guy is a law professor, but he's never been there.

53:25.100 --> 53:26.100
He never went on a trip to Alaska.

53:26.100 --> 53:28.100
There's no incidents ever of sexual harassment.

53:28.100 --> 53:32.100
So the problem is these systems are very, very confident and convincing

53:32.100 --> 53:35.100
that the analogy of a very good improv comedy partner,

53:35.100 --> 53:38.100
whatever game you want to play, it'll play along with you.

53:38.100 --> 53:41.100
And in that sense, it can seem like it's doing more than it is

53:41.100 --> 53:43.100
when it's kind of adapting to who you are.

53:43.100 --> 53:46.100
With reliability, I think there's an interesting nuance here

53:46.100 --> 53:51.100
that depending on the situation the AI is in, you may have to be like super extremely reliable.

53:51.100 --> 53:56.100
Like, you know, 99.99999 is still not enough because if it's operating fast enough

53:56.100 --> 54:00.100
and it's making a billion decisions a second, then even one in a billion,

54:00.100 --> 54:02.100
you're like, okay, is that one problem every second?

54:02.100 --> 54:04.100
That could be catastrophic.

54:04.100 --> 54:08.100
Even if one in a thousand were the actual problem, you'd very quickly reach that threshold.

54:08.100 --> 54:12.100
In other cases, you could imagine, well, if something's really super insightful,

54:12.100 --> 54:16.100
like it's coming up with interesting scientific advancements

54:16.100 --> 54:18.100
or ways of understanding the world.

54:18.100 --> 54:22.100
Well, even if it was 10% reliable, like one in 10, even one in 100,

54:22.100 --> 54:24.100
it might be still very, very valuable.

54:24.100 --> 54:27.100
So I think it's going to be very mixed in terms of reliability.

54:27.100 --> 54:32.100
But the main thing one would want is to have an understanding of how reliable it is

54:32.100 --> 54:34.100
before it is deployed and used in any way.

54:34.100 --> 54:37.100
Because you don't want something that you think is 99% reliable,

54:37.100 --> 54:41.100
being 10% reliable, and the reverse, of course, because there's confusing things.

54:41.100 --> 54:43.100
Again, we're going to kind of see how it plans out, right?

54:43.100 --> 54:46.100
With the self-driving cars, I think we just saw crews say like,

54:46.100 --> 54:50.100
oh, well, you know, we consult AI or some network of people in some ways

54:50.100 --> 54:54.100
of four or 5% of the time, which sounds like a lot, but in another way is not.

54:54.100 --> 54:56.100
If you look at, again, the vast majority of human invention,

54:56.100 --> 54:58.100
but it's not doing what it's supposed to be doing.

54:58.100 --> 55:01.100
How is reliability different from alignment?

55:01.100 --> 55:03.100
How are these concepts different?

55:03.100 --> 55:07.100
Is it just about getting the AIs to do what we want in both cases,

55:07.100 --> 55:09.100
or how would you disentangle here?

55:09.100 --> 55:10.100
That's a good question.

55:10.100 --> 55:15.100
So I think alignment kind of breaks down into various different related issues

55:15.100 --> 55:16.100
and problems, right?

55:16.100 --> 55:18.100
Does the AI do what we want?

55:18.100 --> 55:19.100
Like, who is we?

55:19.100 --> 55:21.100
What are our values?

55:21.100 --> 55:22.100
That sort of thing.

55:22.100 --> 55:23.100
That is the alignment quagmire.

55:23.100 --> 55:25.100
But you're definitely right.

55:25.100 --> 55:28.100
There is certainly some overlap where if I've asked an AI for X

55:28.100 --> 55:31.100
and it's delivering what seems to be X, that seems like it's reliable,

55:31.100 --> 55:33.100
and therefore it seems aligned with what I want.

55:33.100 --> 55:36.100
So in that sense, I would say, yeah, there's a lot of overlap between these terms.

55:36.100 --> 55:39.100
That said, when we're thinking about alignment,

55:39.100 --> 55:43.100
it's usually the broader, is this thing going to cause a problem in some way or another?

55:43.100 --> 55:45.100
But, well, yeah, I wish she's actually seen them.

55:45.100 --> 55:46.100
That was very similar.

55:46.100 --> 55:48.100
So I'm appreciating that these are highly linked,

55:48.100 --> 55:50.100
because if you think of alignment, it could go,

55:50.100 --> 55:52.100
an AI system could be misaligned for several reasons, right?

55:52.100 --> 55:54.100
It could be due to an accident.

55:54.100 --> 55:56.100
It could be due to misuse by malevolent actors,

55:56.100 --> 56:00.100
or it could be, you know, the AI itself becomes more capable, more power seeking.

56:00.100 --> 56:04.100
And if it was reliable in any of those ways, it would kind of make it worse.

56:04.100 --> 56:07.100
But with the accident example, it does really seem like,

56:07.100 --> 56:10.100
well, if an AI is misfunctioning, it makes it not reliable.

56:10.100 --> 56:12.100
So on the fly, I don't know if I'll commit to this,

56:12.100 --> 56:15.100
but I'll think like maybe reliability becomes a bit of a subset

56:15.100 --> 56:18.100
of the alignment accident issue.

56:18.100 --> 56:21.100
And then of course, it would also relate to misuse.

56:21.100 --> 56:23.100
In a way, these things are all very much connected,

56:23.100 --> 56:25.100
and that's something that's in the book as well.

56:25.100 --> 56:28.100
It's, you can't make one long chapter that's 3,000 pages,

56:28.100 --> 56:31.100
so you kind of have to like, how can I put this into different chunks,

56:31.100 --> 56:34.100
even though these things overlap and interrelate?

56:34.100 --> 56:39.100
Yeah, so you mentioned these three categories of risks from AI.

56:39.100 --> 56:43.100
An accident, intentional misuse, and rogue AI.

56:43.100 --> 56:46.100
Which of these categories worry you the most?

56:46.100 --> 56:49.100
And on which timelines?

56:49.100 --> 56:51.100
So what are you most worried about right now?

56:51.100 --> 56:54.100
What about in 10 years and 20 years and 30 years?

56:54.100 --> 56:58.100
Yeah, 30 years, and you're like, oh, 2053.

56:58.100 --> 57:00.100
Let's think about that for a moment.

57:00.100 --> 57:02.100
I can't even think that far in advance right now.

57:02.100 --> 57:05.100
But yes, I think it's a great question because, you know,

57:05.100 --> 57:07.100
you hear all these things like, well, what's in the present day, right?

57:07.100 --> 57:10.100
And I think at the moment, it is more of the accident,

57:10.100 --> 57:12.100
and it is the misuse.

57:12.100 --> 57:14.100
Just to clarify, by accident, we kind of mean

57:14.100 --> 57:16.100
that the system is not quite doing what we wanted to do, right?

57:16.100 --> 57:19.100
So when Bing Chat was aggressively misaligned

57:19.100 --> 57:23.100
and it was kind of treating its users badly earlier in 2023,

57:23.100 --> 57:26.100
then that indicates that's not what the machine was supposed to do.

57:26.100 --> 57:28.100
And this is the broader category of, you know,

57:28.100 --> 57:31.100
people respond to incentives, and there are perverse incentives, right?

57:31.100 --> 57:34.100
That you think you've designed a law or a rule in one way,

57:34.100 --> 57:36.100
and then it turns out it's something else.

57:36.100 --> 57:39.100
So in that sense, these things are happening semi-frequently,

57:39.100 --> 57:42.100
to some extent, and they're trying to, like, train them out, right?

57:42.100 --> 57:44.100
And whether, you know, even saying the wrong thing

57:44.100 --> 57:48.100
in terms of violence or sexual imagery counts as accident as well,

57:48.100 --> 57:50.100
that's more nuanced, and we don't really have to get into that.

57:50.100 --> 57:54.100
But all it to say is, accident is a frequent occurring problem right now.

57:54.100 --> 57:56.100
With the misuse, that has also already happened, right?

57:56.100 --> 58:00.100
People are using voice-cloning software to scam people out of money.

58:00.100 --> 58:03.100
They call a person, usually a parent or a grandparent,

58:03.100 --> 58:07.100
pretend to be the person's child because they've voice-cloned that child's voice,

58:07.100 --> 58:09.100
and say, like, I need money, please send it immediately.

58:09.100 --> 58:11.100
And people have already lost money.

58:11.100 --> 58:13.100
Now, that seems like a bad case,

58:13.100 --> 58:16.100
but, of course, it's not nearly as dangerous as, like, new bio weapons,

58:16.100 --> 58:18.100
but that seems like it's also plausible.

58:18.100 --> 58:22.100
So if you sort of imagine, yes, why don't we say the next two, three, five, ten years?

58:22.100 --> 58:25.100
It seems like accident is already happening.

58:25.100 --> 58:30.100
Misuse is most likely to increase before more power-seeking autonomous behavior

58:30.100 --> 58:33.100
from AI itself or the rogue AI comes on board.

58:33.100 --> 58:37.100
The concern is, since people who are building these things don't know,

58:37.100 --> 58:40.100
when you put in a certain amount of compute or computational capacity,

58:40.100 --> 58:42.100
you get a certain level of capability,

58:42.100 --> 58:44.100
will there be a dramatic jump in capability?

58:44.100 --> 58:46.100
Maybe, maybe not, and that uncertainty is a problem.

58:46.100 --> 58:52.100
So while we, it seems reasonable to say, well, right now, misuse is more than near-term problem.

58:52.100 --> 58:55.100
In the world of AI, is near-term six months?

58:55.100 --> 58:58.100
Because then it's like, oh, yeah, I meant, like, until the end of 2024,

58:58.100 --> 59:01.100
and then it's really also going to be both misuse and power-seeking.

59:01.100 --> 59:05.100
So I think when you're sort of talking to average people or even policymakers,

59:05.100 --> 59:06.100
there's usually, like, a multi-year,

59:06.100 --> 59:09.100
something multi-decade concern in the back of their head,

59:09.100 --> 59:11.100
or, sorry, timeline in the back of their head of how the world works.

59:11.100 --> 59:14.100
And if you say something near-term versus long-term, you should clarify, like,

59:14.100 --> 59:17.100
oh, by misuse, I mean, like, one to two to three years,

59:17.100 --> 59:22.100
and then overlapping within one to five years, perhaps power-seeking as well.

59:22.100 --> 59:24.100
And that's kind of how I would break it down.

59:24.100 --> 59:29.100
You write about strategic foresight, which is making plans for different scenarios.

59:29.100 --> 59:32.100
How does that help us manage these risks?

59:32.100 --> 59:36.100
Sure, it's really just, again, trying to think about ways to figure things out

59:36.100 --> 59:40.100
without committing to a specific outcome, like a forecast would, right?

59:40.100 --> 59:43.100
Like, you know, again, the weather forecast, I think, is the best example

59:43.100 --> 59:47.100
of the average person encountering probabilistic assessments of the future.

59:47.100 --> 59:49.100
80% chance of rain tomorrow, right?

59:49.100 --> 59:52.100
With forecasts, or metacoliths, different prediction markets,

59:52.100 --> 59:56.100
what is the likelihood of, you know, X event happening at a certain time?

59:56.100 --> 59:59.100
That's great. I think we need those, and they are important.

59:59.100 --> 01:00:01.100
I think, in addition, we can use things like foresight,

01:00:01.100 --> 01:00:04.100
which explores a range of plausible futures.

01:00:04.100 --> 01:00:06.100
So you can look at the data, you can look at analysis,

01:00:06.100 --> 01:00:08.100
and you can think, what is the most likely outcome?

01:00:08.100 --> 01:00:10.100
What is most probable? And that's very useful.

01:00:10.100 --> 01:00:12.100
But we can also think, well, what's plausible?

01:00:12.100 --> 01:00:16.100
Let's play through, and kind of broad scenario planning is what this is.

01:00:16.100 --> 01:00:19.100
What might happen if AI becomes more prevalent,

01:00:19.100 --> 01:00:23.100
if image generators become more popular? What happens?

01:00:23.100 --> 01:00:26.100
For example, image generators become more popular, then more people use them.

01:00:26.100 --> 01:00:31.100
Does that affect artists? Let's just assume it affects artists and artists lose work.

01:00:31.100 --> 01:00:35.100
Then what happens? And you kind of do this cascading first-order, second-order thing

01:00:35.100 --> 01:00:38.100
that really, I think, helps open up the possibility space,

01:00:38.100 --> 01:00:40.100
the realm of what could happen.

01:00:40.100 --> 01:00:43.100
Now, sometimes it's hard to draw a direct line of what do we do now,

01:00:43.100 --> 01:00:46.100
but at least you've opened up your mind of what could be.

01:00:46.100 --> 01:00:50.100
And once you start to think back all the things that happened 5, 10, 15, 20 years ago,

01:00:50.100 --> 01:00:54.100
if you put yourself back 15 years ago and try to imagine what happens then,

01:00:54.100 --> 01:00:57.100
you realize, oh, people didn't see a lot of things coming.

01:00:57.100 --> 01:01:00.100
They weren't open-minded enough, or they didn't see enough of the data.

01:01:00.100 --> 01:01:02.100
There's a bit of hindsight bias, right?

01:01:02.100 --> 01:01:05.100
Of course, that thing was foreseeable, and many things are not.

01:01:05.100 --> 01:01:09.100
But with foresight, I really think it's very useful to, again, open up our minds.

01:01:09.100 --> 01:01:12.100
So with the AI issue, you can take an example where, say,

01:01:12.100 --> 01:01:15.100
artificial superintelligence arrives in 10 years.

01:01:15.100 --> 01:01:17.100
Just assume that's happened, then work backwards.

01:01:17.100 --> 01:01:21.100
So what had to happen for that future to come into existence?

01:01:21.100 --> 01:01:23.100
What if it was 20 years? What if it was 50 years?

01:01:23.100 --> 01:01:25.100
And you can kind of think, like, okay, maybe if it's 10 years,

01:01:25.100 --> 01:01:28.100
current projections seem to hold, but maybe if it was 20 years,

01:01:28.100 --> 01:01:30.100
there were some hiccups, there were some complications.

01:01:30.100 --> 01:01:32.100
We didn't understand the complexity of certain things,

01:01:32.100 --> 01:01:33.100
and we hit certain walls.

01:01:33.100 --> 01:01:35.100
50 years, I think a lot of us would be like,

01:01:35.100 --> 01:01:37.100
well, I don't know, we just got something wrong.

01:01:37.100 --> 01:01:39.100
We didn't understand the nature of what we were dealing with,

01:01:39.100 --> 01:01:41.100
and a lot of projections now would be wrong.

01:01:41.100 --> 01:01:43.100
And you can do that in a variety of ways.

01:01:43.100 --> 01:01:47.100
So I think it opens up the ability to think about these issues

01:01:47.100 --> 01:01:49.100
in different ways without committing to something.

01:01:49.100 --> 01:01:52.100
But fundamentally, it also really helps challenge assumptions.

01:01:52.100 --> 01:01:54.100
If you sort of have discussions with people

01:01:54.100 --> 01:01:56.100
of what they expect the future to be like,

01:01:56.100 --> 01:01:57.100
and you could break that down.

01:01:57.100 --> 01:01:59.100
Do you expect it, like, what's your preference for the future?

01:01:59.100 --> 01:02:01.100
What would you not want the future to be like?

01:02:01.100 --> 01:02:02.100
What do you think is most likely?

01:02:02.100 --> 01:02:04.100
And so by doing all these different sort of,

01:02:04.100 --> 01:02:06.100
different nuances, different themes about what they think

01:02:06.100 --> 01:02:09.100
the future might be like, you might be able to have someone realize,

01:02:09.100 --> 01:02:11.100
like, oh, wait, my expectation of the future

01:02:11.100 --> 01:02:13.100
is very much aligned with my preference for the future.

01:02:13.100 --> 01:02:15.100
Right, because that's how a lot of people are.

01:02:15.100 --> 01:02:17.100
But maybe my preferences are not that relevant

01:02:17.100 --> 01:02:19.100
to how the future actually exists.

01:02:19.100 --> 01:02:21.100
And then they can go, oh, I didn't realize that was happening.

01:02:21.100 --> 01:02:23.100
Or even just, you know, with AI stuff,

01:02:23.100 --> 01:02:26.100
some people don't realize how advanced these machines already are.

01:02:26.100 --> 01:02:28.100
And if you can say, like, this thing has already happened,

01:02:28.100 --> 01:02:30.100
then what happens?

01:02:30.100 --> 01:02:33.100
It really does help people think, oh, maybe this could be a concern.

01:02:33.100 --> 01:02:37.100
Yeah, I think it's great to make plans for different AI scenarios.

01:02:37.100 --> 01:02:40.100
But I do worry that these plans will work best

01:02:40.100 --> 01:02:42.100
if we have gradual improvements.

01:02:42.100 --> 01:02:45.100
So say 10% improvement per year.

01:02:45.100 --> 01:02:46.100
We can go back to our plans.

01:02:46.100 --> 01:02:49.100
We can get feedback from the world, adjust our plans.

01:02:49.100 --> 01:02:52.100
But what if AI progress is more bumpy

01:02:52.100 --> 01:02:55.100
and much faster than 10% per year?

01:02:55.100 --> 01:02:59.100
Does this make strategic foresight less useful?

01:02:59.100 --> 01:03:03.100
Well, perhaps less useful, but not useful, right?

01:03:03.100 --> 01:03:04.100
It still has utility.

01:03:04.100 --> 01:03:07.100
It's sort of like we have to, again, make decisions under uncertainty.

01:03:07.100 --> 01:03:09.100
And so we should do the best we can.

01:03:09.100 --> 01:03:11.100
We should put resources into figuring it out,

01:03:11.100 --> 01:03:13.100
and we should map different possibilities

01:03:13.100 --> 01:03:15.100
and try to communicate those broadly to others

01:03:15.100 --> 01:03:17.100
to get feedback and see what things could be.

01:03:17.100 --> 01:03:18.100
Yes, you're right.

01:03:18.100 --> 01:03:21.100
If things are dramatic, if there's a big step change in capabilities,

01:03:21.100 --> 01:03:23.100
it doesn't mean all that work wasn't useful at all.

01:03:23.100 --> 01:03:26.100
But it might mean, like, oh, I have to flip to page five.

01:03:26.100 --> 01:03:28.100
All those things I thought were going to happen in my document

01:03:28.100 --> 01:03:30.100
have now already been passed, what now?

01:03:30.100 --> 01:03:33.100
But hopefully having these conversations themselves

01:03:33.100 --> 01:03:35.100
allow us to plan even better.

01:03:35.100 --> 01:03:37.100
Like, okay, again, the use of scenario planning.

01:03:37.100 --> 01:03:39.100
It's most useful when there's like 10% increases.

01:03:39.100 --> 01:03:40.100
What happens if it's 50?

01:03:40.100 --> 01:03:42.100
What happens if it's 200?

01:03:42.100 --> 01:03:44.100
And again, you might not be able to really figure it out

01:03:44.100 --> 01:03:45.100
and have a perfect plan,

01:03:45.100 --> 01:03:47.100
but having something is better than nothing.

01:03:47.100 --> 01:03:49.100
And sometimes, again, just thinking it through,

01:03:49.100 --> 01:03:51.100
you at least get through all, say,

01:03:51.100 --> 01:03:53.100
is like the emotional complications,

01:03:53.100 --> 01:03:56.100
either the barrier intellectually or even viscerally

01:03:56.100 --> 01:03:58.100
that, oh, my God, this thing just happened.

01:03:58.100 --> 01:04:01.100
And sometimes people need a bit of time to sit with that.

01:04:01.100 --> 01:04:03.100
Like, okay, now imagine something is much more capable

01:04:03.100 --> 01:04:05.100
than anything ever and is highly general.

01:04:05.100 --> 01:04:07.100
Let's think through what that might be like.

01:04:07.100 --> 01:04:09.100
And you can even think through how you might feel

01:04:09.100 --> 01:04:11.100
to then better make a decision when it's happening.

01:04:11.100 --> 01:04:13.100
Because again, if you're trying to make decisions

01:04:13.100 --> 01:04:15.100
and things are happening very quickly,

01:04:15.100 --> 01:04:17.100
urgency rarely helps decision-making.

01:04:17.100 --> 01:04:21.100
So you discuss this fact, I would say,

01:04:21.100 --> 01:04:23.100
that we are living in unusual times

01:04:23.100 --> 01:04:25.100
in terms of economic growth,

01:04:25.100 --> 01:04:28.100
in terms of scientific papers published per year,

01:04:28.100 --> 01:04:32.100
in terms of the exponential growth of computing power

01:04:32.100 --> 01:04:35.100
available for a certain dollar amount.

01:04:35.100 --> 01:04:38.100
Yeah, that's one aspect of the world we're living in.

01:04:38.100 --> 01:04:42.100
Another aspect might be that ideas are getting harder to find.

01:04:42.100 --> 01:04:45.100
We have many more researchers

01:04:45.100 --> 01:04:48.100
for the same amount of scientific breakthrough.

01:04:48.100 --> 01:04:52.100
Economic growth might be slowing down in certain countries.

01:04:52.100 --> 01:04:54.100
Say, say we have these two trends

01:04:54.100 --> 01:04:56.100
and you can tell me whether you think these trends

01:04:56.100 --> 01:04:58.100
are actually occurring.

01:04:58.100 --> 01:05:00.100
Which of these trends are going to win out?

01:05:00.100 --> 01:05:02.100
Are we going to hit diminishing returns

01:05:02.100 --> 01:05:07.100
or are we on a path to even stronger exponential growth?

01:05:07.100 --> 01:05:09.100
Going infinite, right?

01:05:09.100 --> 01:05:12.100
Like, I can find ideas on the internet very easily.

01:05:12.100 --> 01:05:14.100
What do you mean they're hard to find?

01:05:14.100 --> 01:05:17.100
Jokes aside, so I appreciate you highlighting that.

01:05:17.100 --> 01:05:19.100
This is certainly not a new idea,

01:05:19.100 --> 01:05:21.100
but I really wanted to, again,

01:05:21.100 --> 01:05:23.100
average person hasn't thought much about these issues.

01:05:23.100 --> 01:05:25.100
Like, where are we sitting right now?

01:05:25.100 --> 01:05:27.100
And to think how humans currently live,

01:05:27.100 --> 01:05:29.100
again, not everyone, there are billions of people

01:05:29.100 --> 01:05:31.100
without food, water, electricity, that sort of thing,

01:05:31.100 --> 01:05:33.100
or at least hundreds of millions,

01:05:33.100 --> 01:05:35.100
things are very different than they used to be.

01:05:35.100 --> 01:05:37.100
So I wanted to give a sense of the grand sweep

01:05:37.100 --> 01:05:39.100
of how things are very different,

01:05:39.100 --> 01:05:41.100
to show just how much change has occurred,

01:05:41.100 --> 01:05:43.100
to then say, well, if so much change has occurred,

01:05:43.100 --> 01:05:45.100
it's reasonable, possible, plausible,

01:05:45.100 --> 01:05:47.100
to think a lot of change might also occur in the future.

01:05:47.100 --> 01:05:49.100
So if you go back, you know, millions of years,

01:05:49.100 --> 01:05:52.100
you know, proto-humans are still developing at one point,

01:05:52.100 --> 01:05:54.100
you know, was it 1.6 million years ago,

01:05:54.100 --> 01:05:56.100
we have a hand axe, which is a sharp stone tool,

01:05:56.100 --> 01:05:58.100
and that was the best thing for a million years,

01:05:58.100 --> 01:06:02.100
a million years, 50,000 generations of people,

01:06:02.100 --> 01:06:04.100
and you're like, what?

01:06:04.100 --> 01:06:07.100
I was like, well, I made this sharp stone slightly sharper.

01:06:07.100 --> 01:06:09.100
Like, okay, well, that's not that great an advancement

01:06:09.100 --> 01:06:10.100
compared to like the iPhone

01:06:10.100 --> 01:06:12.100
and all the different new releases there.

01:06:12.100 --> 01:06:14.100
That said, for people who are sticklers,

01:06:14.100 --> 01:06:16.100
I'm sure there were also various wooden tools

01:06:16.100 --> 01:06:18.100
they often don't preserve as well.

01:06:18.100 --> 01:06:21.100
Humanity also lost knowledge about how to make certain tools

01:06:21.100 --> 01:06:23.100
at various points throughout history,

01:06:23.100 --> 01:06:25.100
which is something that's difficult to imagine now

01:06:25.100 --> 01:06:28.100
that we would lose knowledge about how to print books

01:06:28.100 --> 01:06:30.100
or something like that.

01:06:30.100 --> 01:06:33.100
Maybe at the very cutting edge of the technology stack,

01:06:33.100 --> 01:06:35.100
we can imagine that we might lose knowledge.

01:06:35.100 --> 01:06:37.100
It seems difficult for us to imagine now, I think,

01:06:37.100 --> 01:06:40.100
losing knowledge of how to create basic products.

01:06:40.100 --> 01:06:41.100
You're right.

01:06:41.100 --> 01:06:44.100
As the nature of the world has become more industrialized,

01:06:44.100 --> 01:06:47.100
certainly making a particular product often requires many,

01:06:47.100 --> 01:06:49.100
many people, sometimes thousands,

01:06:49.100 --> 01:06:51.100
sometimes millions in the entire supply chain.

01:06:51.100 --> 01:06:53.100
So that's its own types of complexity where now,

01:06:53.100 --> 01:06:55.100
well, maybe someone could have made a pencil

01:06:55.100 --> 01:06:56.100
and someone still can.

01:06:56.100 --> 01:06:58.100
Nowadays, it's a whole team and company

01:06:58.100 --> 01:07:00.100
and industries and machines.

01:07:00.100 --> 01:07:02.100
Is it the case that we're going to keep growing,

01:07:02.100 --> 01:07:04.100
sort of getting into more advanced and things

01:07:04.100 --> 01:07:05.100
that you're going to keep changing?

01:07:05.100 --> 01:07:07.100
I think it depends on what we measure,

01:07:07.100 --> 01:07:09.100
and I'm well aware that the economists will say

01:07:09.100 --> 01:07:11.100
that innovation has slowed or productivity is down

01:07:11.100 --> 01:07:12.100
in certain ways.

01:07:12.100 --> 01:07:14.100
And I think that's important.

01:07:14.100 --> 01:07:15.100
I don't want to say it isn't.

01:07:15.100 --> 01:07:18.100
But from the user, normal human user experience,

01:07:18.100 --> 01:07:21.100
it seems like things are still remarkable.

01:07:21.100 --> 01:07:23.100
Now, you could say most of it's in the information technology

01:07:23.100 --> 01:07:24.100
space.

01:07:24.100 --> 01:07:25.100
It's the internet.

01:07:25.100 --> 01:07:26.100
It's the computers.

01:07:26.100 --> 01:07:27.100
It's the phones.

01:07:27.100 --> 01:07:30.100
Where's our new plane or washer dryer or the car

01:07:30.100 --> 01:07:32.100
or that sort of thing?

01:07:32.100 --> 01:07:35.100
And I guess I think that the changes in the internet,

01:07:35.100 --> 01:07:38.100
in that space, like that we're easily doing this podcast,

01:07:38.100 --> 01:07:41.100
are significant in a similar way to some of these other things.

01:07:41.100 --> 01:07:44.100
Now, yes, the invention of like a dishwasher is a truly big

01:07:44.100 --> 01:07:46.100
difference in terms of how it affected life.

01:07:46.100 --> 01:07:48.100
But so are recent inventions.

01:07:48.100 --> 01:07:50.100
So I don't want to say things are going to continue forever.

01:07:50.100 --> 01:07:52.100
That seems unlikely because it just also makes no sense

01:07:52.100 --> 01:07:53.100
conceptually.

01:07:53.100 --> 01:07:55.100
Usually there's an S curve in terms of how these things

01:07:55.100 --> 01:07:56.100
develop, right?

01:07:56.100 --> 01:07:58.100
And it's hard to know where we are in the curve.

01:07:58.100 --> 01:08:01.100
I would just say more comfortably for the next little while,

01:08:01.100 --> 01:08:03.100
it does seem computer and computer technology,

01:08:03.100 --> 01:08:05.100
that whole domain is going to crease a lot.

01:08:05.100 --> 01:08:07.100
And then that's going to ripple through.

01:08:07.100 --> 01:08:10.100
Now, whether some people think this isn't enough,

01:08:10.100 --> 01:08:11.100
I don't know.

01:08:11.100 --> 01:08:13.100
I guess I'm less concerned about that.

01:08:13.100 --> 01:08:16.100
I mean, I am concerned about like economic growth being good

01:08:16.100 --> 01:08:18.100
for human development in that sense.

01:08:18.100 --> 01:08:20.100
But we're running out of ideas.

01:08:20.100 --> 01:08:21.100
I don't know.

01:08:21.100 --> 01:08:22.100
There's still lots of great ideas, right?

01:08:22.100 --> 01:08:24.100
And in fact, I think with the AI thing,

01:08:24.100 --> 01:08:26.100
like maybe we need to slow down some of this development

01:08:26.100 --> 01:08:28.100
because we haven't figured out how to deal with the ideas we

01:08:28.100 --> 01:08:29.100
already have.

01:08:29.100 --> 01:08:31.100
I also think it's interesting, as you said,

01:08:31.100 --> 01:08:33.100
scientists now sometimes on papers,

01:08:33.100 --> 01:08:36.100
there's 10, 50, 100 or some like hundreds of scientists

01:08:36.100 --> 01:08:37.100
to do some of these things.

01:08:37.100 --> 01:08:40.100
Usually it's particle physics or something in AI

01:08:40.100 --> 01:08:43.100
or machine learning or maybe even biology.

01:08:43.100 --> 01:08:45.100
And yes, it's not like when it was with, you know,

01:08:45.100 --> 01:08:47.100
you can picture your Darwin, your Aristotle or someone's like,

01:08:47.100 --> 01:08:49.100
oh, yes, I think the nature of the world is blah.

01:08:49.100 --> 01:08:51.100
And I've unlocked some mystery of the universe.

01:08:51.100 --> 01:08:55.100
And yes, you could think that there are some diminishing returns.

01:08:55.100 --> 01:08:57.100
But at the same time, there's lots we haven't figured out,

01:08:57.100 --> 01:08:58.100
right?

01:08:58.100 --> 01:09:01.100
How gravity interacts at the quantum level,

01:09:01.100 --> 01:09:04.100
even if what the right interpretation of quantum mechanics

01:09:04.100 --> 01:09:06.100
is, will these things be figured out?

01:09:06.100 --> 01:09:09.100
Could we build fantastical ways of capturing energy,

01:09:09.100 --> 01:09:10.100
more than solar even, right?

01:09:10.100 --> 01:09:11.100
And these sorts of things.

01:09:11.100 --> 01:09:14.100
So I guess when I think of the new solar and wind stuff,

01:09:14.100 --> 01:09:16.100
which didn't exist when I was younger,

01:09:16.100 --> 01:09:18.100
the fact that, you know, there's the immersive VR,

01:09:18.100 --> 01:09:20.100
things didn't exist when I was younger,

01:09:20.100 --> 01:09:22.100
planes haven't changed a lot.

01:09:22.100 --> 01:09:24.100
Sure, but now people have gone to space casually.

01:09:24.100 --> 01:09:26.100
Like again, in the history of the world,

01:09:26.100 --> 01:09:27.100
none of this has ever happened.

01:09:27.100 --> 01:09:30.100
So yes, on a multi-decade span,

01:09:30.100 --> 01:09:32.100
it may seem like things have slowed.

01:09:32.100 --> 01:09:34.100
But if you really take a step back,

01:09:34.100 --> 01:09:36.100
thousands of years, even on a millionaire scale,

01:09:36.100 --> 01:09:38.100
it's all squished, right?

01:09:38.100 --> 01:09:40.100
In the past couple hundred years.

01:09:40.100 --> 01:09:42.100
And so like, let's just, let's keep these things in mind.

01:09:42.100 --> 01:09:44.100
But let's see how the next couple of decades pan out.

01:09:44.100 --> 01:09:46.100
Let's talk about alignment.

01:09:46.100 --> 01:09:50.100
So one objection you might give to the whole project of alignment

01:09:50.100 --> 01:09:55.100
is to say that humans can't agree on what values we should have.

01:09:55.100 --> 01:09:59.100
Philosophers haven't been able to figure out ethics.

01:09:59.100 --> 01:10:02.100
What is it that we're trying to align AI with

01:10:02.100 --> 01:10:05.100
if we haven't determined our values yet?

01:10:05.100 --> 01:10:09.100
It's a great question and it is currently unsolved.

01:10:09.100 --> 01:10:11.100
And in some ways, we're going to have to muddle through.

01:10:11.100 --> 01:10:13.100
That would be my concise answer.

01:10:13.100 --> 01:10:15.100
It's sort of like, what do we do when humans disagree?

01:10:15.100 --> 01:10:18.100
Well, we try to come together in some sort of compromise,

01:10:18.100 --> 01:10:21.100
some sort of consensus, hopefully some sort of democratic system

01:10:21.100 --> 01:10:24.100
where people don't necessarily get everything they want,

01:10:24.100 --> 01:10:27.100
but they get enough that the world functions decently for most people.

01:10:27.100 --> 01:10:30.100
It's not perfect by any means, but then compared to what, right?

01:10:30.100 --> 01:10:31.100
The Winston Churchill line.

01:10:31.100 --> 01:10:34.100
So with AI, yes, this is the technical alignment issue,

01:10:34.100 --> 01:10:36.100
which I think is critically important.

01:10:36.100 --> 01:10:39.100
This is more like, does the AI do something we didn't want to do?

01:10:39.100 --> 01:10:41.100
Like by an accident, by a technical point of view.

01:10:41.100 --> 01:10:45.100
But yes, if we solve the technical alignment problem, that's great.

01:10:45.100 --> 01:10:46.100
That's amazing.

01:10:46.100 --> 01:10:48.100
That's, that's difficult, but it's still going to be amazing.

01:10:48.100 --> 01:10:50.100
And then there's this other problem, which was always there

01:10:50.100 --> 01:10:51.100
that slots right into place.

01:10:51.100 --> 01:10:52.100
Well, now what?

01:10:52.100 --> 01:10:54.100
Who decides the fate of the world type thing, right?

01:10:54.100 --> 01:10:57.100
And if these systems are as powerful as they are,

01:10:57.100 --> 01:11:01.100
it does seem very bizarre how we're currently going about it, right?

01:11:01.100 --> 01:11:04.100
Yes, we do have states that are starting to issue executive orders

01:11:04.100 --> 01:11:07.100
like the White House did or other regulation or the EU AI Act.

01:11:07.100 --> 01:11:12.100
But right now it seems, I'll just say weird that a few people are sort of

01:11:12.100 --> 01:11:14.100
not controlling the fate of the world, but by their own standards,

01:11:14.100 --> 01:11:17.100
by their own statements, they're developing by design,

01:11:17.100 --> 01:11:20.100
by their own goal at very, very powerful systems

01:11:20.100 --> 01:11:23.100
that could have vast control and abilities.

01:11:23.100 --> 01:11:25.100
So what is going on here, right?

01:11:25.100 --> 01:11:27.100
And this is where the book is kind of just trying to raise awareness

01:11:27.100 --> 01:11:31.100
of the, yes, even if this thing about AI is solved in a technical sense,

01:11:31.100 --> 01:11:35.100
there's this other problem about how do we ensure like everyone has a voice?

01:11:35.100 --> 01:11:37.100
How do we ensure people are represented?

01:11:37.100 --> 01:11:40.100
Is there going to be even greater empower power imbalances

01:11:40.100 --> 01:11:43.100
and inequalities that will result in a way that's truly disruptive?

01:11:43.100 --> 01:11:45.100
So I think again, it's like a call to arms.

01:11:45.100 --> 01:11:47.100
We need a lot more people thinking about it.

01:11:47.100 --> 01:11:48.100
We need a lot more people aware of it.

01:11:48.100 --> 01:11:52.100
And even like a if then scenario, like, okay, so say I developed,

01:11:52.100 --> 01:11:53.100
what's the plan?

01:11:53.100 --> 01:11:55.100
How will resources be distributed?

01:11:55.100 --> 01:11:58.100
Will these companies just go to, you know, multi trillion dollar,

01:11:58.100 --> 01:11:59.100
quadrillion dollar things?

01:11:59.100 --> 01:12:00.100
Who knows how far it goes.

01:12:00.100 --> 01:12:02.100
Does something end up getting nationalized?

01:12:02.100 --> 01:12:04.100
These are delicate things, maybe to say in certain environments,

01:12:04.100 --> 01:12:07.100
but hopefully conversations are at least happening behind the scenes of,

01:12:07.100 --> 01:12:09.100
okay, let's plan through again, the scenarios.

01:12:09.100 --> 01:12:14.100
If something isn't aligned with other people and they could use it

01:12:14.100 --> 01:12:15.100
from malicious use, that's one thing.

01:12:15.100 --> 01:12:18.100
But just the normal, my preferences are different than yours.

01:12:18.100 --> 01:12:20.100
And that may make your life a lot worse.

01:12:20.100 --> 01:12:22.100
That's something we really need to pay attention to.

01:12:22.100 --> 01:12:27.100
I think also there's some hope that given our kind of humans have a shared

01:12:27.100 --> 01:12:32.100
evolutionary history, we have, we share a lot of our values,

01:12:32.100 --> 01:12:37.100
even though we also disagree strongly with each other all the time.

01:12:37.100 --> 01:12:41.100
I think there's some hope that we won't have to actually get as something

01:12:41.100 --> 01:12:44.100
like a final theory of ethics or something.

01:12:44.100 --> 01:12:49.100
And I want to say we should definitely not stop working on alignment

01:12:49.100 --> 01:12:55.100
until we have such a theory that we can then plug into our AI systems.

01:12:55.100 --> 01:13:00.100
I think we can probably agree on some basics of life and then,

01:13:00.100 --> 01:13:02.100
as you say, model through.

01:13:02.100 --> 01:13:05.100
So thinking about healthcare, for example,

01:13:05.100 --> 01:13:09.100
I think most people can agree that most people should have access to,

01:13:09.100 --> 01:13:12.100
or all people should have access to fantastic healthcare.

01:13:12.100 --> 01:13:15.100
And that's something where AI might be able to help.

01:13:15.100 --> 01:13:18.100
And then we can go on to the next thing and the next thing and the next thing.

01:13:18.100 --> 01:13:25.100
I think there's some sort of, there might be too much focus on trying to develop

01:13:25.100 --> 01:13:29.100
a perfect theory of ethics and we should, as you say, model through.

01:13:29.100 --> 01:13:32.100
Well, I think that sort of might be the only way, right?

01:13:32.100 --> 01:13:35.100
As you said, there's philosophers, academics, anyway, all humans

01:13:35.100 --> 01:13:37.100
sort of been working on this for thousands of years.

01:13:37.100 --> 01:13:39.100
And of course, we don't all agree.

01:13:39.100 --> 01:13:41.100
And importantly, we don't often agree with ourselves, right?

01:13:41.100 --> 01:13:44.100
We change over time, your preferences from a child to as an adult

01:13:44.100 --> 01:13:46.100
to maybe even five years ago.

01:13:46.100 --> 01:13:47.100
That's very different.

01:13:47.100 --> 01:13:50.100
Well, were you correct five years ago or now?

01:13:50.100 --> 01:13:51.100
Uh-oh.

01:13:51.100 --> 01:13:53.100
Which value system did you give the AI?

01:13:53.100 --> 01:13:54.100
Was it supposed to lock in?

01:13:54.100 --> 01:13:56.100
Was it supposed to know better?

01:13:56.100 --> 01:14:00.100
I think it's a very interesting but also very concerning space that,

01:14:00.100 --> 01:14:03.100
like you, though, I think, can't we agree on some basics?

01:14:03.100 --> 01:14:06.100
And we could sort of think the United Nations Declaration of Human Rights

01:14:06.100 --> 01:14:09.100
or development, like most countries did sign on to these things.

01:14:09.100 --> 01:14:12.100
And there is a general sense like, okay, people should have food.

01:14:12.100 --> 01:14:15.100
We'll get to healthcare in a moment, but like food, water, sanitation,

01:14:15.100 --> 01:14:17.100
grade up to grade eight primary education.

01:14:17.100 --> 01:14:19.100
Like these seem to be universals.

01:14:19.100 --> 01:14:22.100
And so hopefully, yes, with abundance from the AI, we can all agree.

01:14:22.100 --> 01:14:24.100
Like, can't we just like end tuberculosis?

01:14:24.100 --> 01:14:26.100
Can't we really solve this malaria thing?

01:14:26.100 --> 01:14:28.100
Can't we have everyone more educated?

01:14:28.100 --> 01:14:30.100
But there will always be someone who disagrees.

01:14:30.100 --> 01:14:32.100
There will always be someone from a different angle or malevolent actors.

01:14:32.100 --> 01:14:35.100
There are currently 40 million people in modern day slavery.

01:14:35.100 --> 01:14:39.100
Clearly they are there because other people are doing terrible things in various ways

01:14:39.100 --> 01:14:42.100
or the situations they find themselves in are so compromised.

01:14:42.100 --> 01:14:45.100
That said, like, how do you ever like get rid of that?

01:14:45.100 --> 01:14:48.100
And as you said, we kind of muddle through as well at the same time,

01:14:48.100 --> 01:14:51.100
certain things remain wholly unacceptable with the AI.

01:14:51.100 --> 01:14:55.100
It is the concern that, and this is of course race dynamics all over the place,

01:14:55.100 --> 01:14:58.100
that if certain malevolent actors get their way first,

01:14:58.100 --> 01:15:02.100
then they may then be able to disempower or displace what we'll say

01:15:02.100 --> 01:15:05.100
is the loose reasonable majority that thinks everyone should have food,

01:15:05.100 --> 01:15:07.100
water, healthcare and education.

01:15:07.100 --> 01:15:11.100
In sort of traditional discussions of alignment,

01:15:11.100 --> 01:15:18.100
we imagine perhaps that we would sit down and hand code human values into AI systems.

01:15:18.100 --> 01:15:23.100
And then we thought about how complex human values are,

01:15:23.100 --> 01:15:26.100
and that was a cause of despair.

01:15:26.100 --> 01:15:32.100
How could we ever summarize something as complex and inconsistent as human values?

01:15:32.100 --> 01:15:36.100
Do you think that large language models change this picture?

01:15:36.100 --> 01:15:39.100
Because large language models can digest all of human knowledge

01:15:39.100 --> 01:15:47.100
and then at least they can pretend to have knowledge of human values, ethics, psychology and so on.

01:15:47.100 --> 01:15:52.100
Is it the case that large language models make the alignment problem easier?

01:15:52.100 --> 01:15:54.100
That is a great question.

01:15:54.100 --> 01:15:56.100
I think that's one of the things we're currently figuring out, right?

01:15:56.100 --> 01:16:01.100
From what I understand, open AI plans to use AI models to help solve the AI alignment problem.

01:16:01.100 --> 01:16:07.100
And in a way, we need AI to assess and evaluate and to test to see if it is aligned

01:16:07.100 --> 01:16:09.100
so that AI is inherently involved.

01:16:09.100 --> 01:16:12.100
But to your general question is maybe.

01:16:12.100 --> 01:16:15.100
And what I think is a perhaps interesting development of this.

01:16:15.100 --> 01:16:17.100
So what if the AI system looks at all human knowledge, right?

01:16:17.100 --> 01:16:21.100
And it says, you know what, I figured it out, guys, everyone, this is what you should do.

01:16:21.100 --> 01:16:23.100
We're like, I don't want to do that.

01:16:23.100 --> 01:16:27.100
But by your own standards, you said you cared about these things.

01:16:27.100 --> 01:16:29.100
And then humans are like, no, no, but I didn't.

01:16:29.100 --> 01:16:30.100
Not really.

01:16:30.100 --> 01:16:31.100
Come on.

01:16:31.100 --> 01:16:35.100
And so in the book, before I talk about the AI alignment problem, as people know it,

01:16:35.100 --> 01:16:37.100
I talk about Isaac Asimov's Laws Robotics.

01:16:37.100 --> 01:16:42.100
And I think this was just the very easy way into like simple rules to align computer systems.

01:16:42.100 --> 01:16:43.100
Don't work.

01:16:43.100 --> 01:16:45.100
When you say don't harm something, you're like, that seems reasonable.

01:16:45.100 --> 01:16:46.100
That seems obvious.

01:16:46.100 --> 01:16:47.100
Of course, put it in the machine.

01:16:47.100 --> 01:16:49.100
Like, what does that mean exactly?

01:16:49.100 --> 01:16:50.100
Like don't harm at all.

01:16:50.100 --> 01:16:54.100
Like if someone needs to get surgery where they have to be cut into, does that count?

01:16:54.100 --> 01:16:55.100
What if it's a risky surgery?

01:16:55.100 --> 01:16:59.100
When you say don't harm anyone, does that include nonaction?

01:16:59.100 --> 01:17:01.100
This is all the omission bias, right?

01:17:01.100 --> 01:17:04.100
Where if you drown someone, we see that as a terrible thing.

01:17:04.100 --> 01:17:08.100
If you let someone drown, well, we see that as a bad thing, but not usually as bad as

01:17:08.100 --> 01:17:09.100
the intentional drowning.

01:17:09.100 --> 01:17:13.100
So is an AI system now supposed to think, well, wait, I'm letting people suffer.

01:17:13.100 --> 01:17:15.100
People are currently dying in poverty needlessly.

01:17:15.100 --> 01:17:17.100
Should I be doing something about that?

01:17:17.100 --> 01:17:21.100
Well, by your own standards, you said, don't allow humans to cause harm or come to harm.

01:17:21.100 --> 01:17:22.100
What am I supposed to do now?

01:17:22.100 --> 01:17:25.100
And you can see that very much disconnected maybe the thing short circuits.

01:17:25.100 --> 01:17:26.100
Yeah.

01:17:26.100 --> 01:17:29.100
Does harm imply any probability of harm?

01:17:29.100 --> 01:17:31.100
Then you're kind of, you cannot move.

01:17:31.100 --> 01:17:35.100
You cannot do anything because anything, any action at all could cause harm.

01:17:35.100 --> 01:17:37.100
It just doesn't work.

01:17:37.100 --> 01:17:38.100
No, exactly.

01:17:38.100 --> 01:17:44.100
And so I think it'll be very useful for AI systems to provide insight, but like any sort

01:17:44.100 --> 01:17:48.100
of human enterprise thing, we might get back an answer we don't want or that's very hard

01:17:48.100 --> 01:17:52.100
for us and whether people will take that on board is also going to be highly variable.

01:17:52.100 --> 01:17:56.100
Like for people who are very much interested in ethical reflection and philosophy, they

01:17:56.100 --> 01:17:59.100
might have made like substantial progress and they realize, you know, my beliefs mean

01:17:59.100 --> 01:18:02.100
I shouldn't do X and therefore I don't do X.

01:18:02.100 --> 01:18:06.100
And a lot of us are like, I know I shouldn't do X, but sometimes I still do because I'm

01:18:06.100 --> 01:18:08.100
a human and you know, again, progress is good.

01:18:08.100 --> 01:18:11.100
It's not to say that people should be absolutist about these things.

01:18:11.100 --> 01:18:15.100
It's just sort of highlights the difficulties of the human system, the human nature of the

01:18:15.100 --> 01:18:16.100
whole thing.

01:18:16.100 --> 01:18:20.100
Do you think current AI systems have self preservation?

01:18:20.100 --> 01:18:21.100
Good question.

01:18:21.100 --> 01:18:24.100
I would think I would probably defer to like, who's doing the most cutting edge research

01:18:24.100 --> 01:18:27.100
now in the advanced labs?

01:18:27.100 --> 01:18:31.100
From what I understand is slightly but not a lot.

01:18:31.100 --> 01:18:36.100
There are some examples I think more in like the theoretical or there's like a prototype

01:18:36.100 --> 01:18:40.100
where they've played around with certain systems and you know, simulated environments and the

01:18:40.100 --> 01:18:45.100
system does seem to engage in certain behavior to protect itself to achieve a goal.

01:18:45.100 --> 01:18:49.100
Whether it's full on self preservation as we commonly understand it, I don't think we're

01:18:49.100 --> 01:18:53.100
there yet, but yeah, I would, I would kind of think like, well, there's probably some

01:18:53.100 --> 01:18:57.100
paper on archive that I haven't had a chance to read yet that may say otherwise.

01:18:57.100 --> 01:19:03.100
But do you think that AI systems will develop self preservation as we get more advanced

01:19:03.100 --> 01:19:04.100
AI?

01:19:04.100 --> 01:19:05.100
I do.

01:19:05.100 --> 01:19:08.100
Or at least I think I do to the extent that we should be concerned about it.

01:19:08.100 --> 01:19:12.100
Again, nothing's 100% here, but there's enough of a risk that, you know, the whole

01:19:12.100 --> 01:19:15.100
Stuart Russell, you can't fetch coffee if you're dead thing.

01:19:15.100 --> 01:19:18.100
To have a system do anything, it has to exist.

01:19:18.100 --> 01:19:22.100
And to me, it is reasonable, it is plausible that to achieve anything to exist.

01:19:22.100 --> 01:19:26.100
And once you know that, you might engage in various activities to ensure that you do exist.

01:19:26.100 --> 01:19:31.100
Now, maybe there are ways to contain this or to circumvent it where, you know, you somehow

01:19:31.100 --> 01:19:35.100
clearly specify a goal with a certain amount of error bars and then the system is supposed

01:19:35.100 --> 01:19:37.100
to shut itself down after it's done that goal.

01:19:37.100 --> 01:19:38.100
Perhaps.

01:19:38.100 --> 01:19:42.100
But when we talked about before, the incentives for autonomous systems that are highly capable,

01:19:42.100 --> 01:19:47.100
highly fast and so on, will then sort of have a disconnect with something that shuts itself

01:19:47.100 --> 01:19:48.100
down all the time.

01:19:48.100 --> 01:19:50.100
You imagine like, oh, I like to use my phone.

01:19:50.100 --> 01:19:52.100
After I send one text, I shut the phone off and then I shut it.

01:19:52.100 --> 01:19:55.100
I turn it back on like, well, that seems really painful and slow, right?

01:19:55.100 --> 01:19:57.100
And people just might not do it.

01:19:57.100 --> 01:20:02.100
So I think it is plausible that the systems will engage in such behavior in a way like

01:20:02.100 --> 01:20:04.100
the expectation would be they probably would.

01:20:04.100 --> 01:20:08.100
I guess I'm trying to say the default expectation to me is that something that's very, very intelligent

01:20:08.100 --> 01:20:10.100
will probably engage in these behaviors.

01:20:10.100 --> 01:20:14.100
So we should be on the lookout for it and really try to figure out if they are or they're not

01:20:14.100 --> 01:20:16.100
versus the expectation that they wouldn't.

01:20:16.100 --> 01:20:21.100
It doesn't seem to me that GPT-4 when I talk to it is trying to self preserve at all.

01:20:21.100 --> 01:20:25.100
It's this my team naive, but it seems to me that I can just click stop on the chat whenever

01:20:25.100 --> 01:20:27.100
I want or close the tab whenever I want.

01:20:27.100 --> 01:20:29.100
And there's nothing the system can do.

01:20:29.100 --> 01:20:35.100
Do you think self preservation will emerge together with more autonomous systems?

01:20:35.100 --> 01:20:36.100
That's a great point.

01:20:36.100 --> 01:20:39.100
So yes, right now, if you think about like, how could this thing be autonomous?

01:20:39.100 --> 01:20:41.100
I literally, you know, close it.

01:20:41.100 --> 01:20:42.100
I click the button and it goes away.

01:20:42.100 --> 01:20:45.100
It's not like secretly hiding somewhere to our knowledge.

01:20:45.100 --> 01:20:48.100
I guess there's a small probability that it is already.

01:20:48.100 --> 01:20:49.100
And well, that's the thing.

01:20:49.100 --> 01:20:53.100
That's why, by the way, I tried to be careful in the book where like AI that's super smart can do anything, right?

01:20:53.100 --> 01:20:56.100
Because then you kind of get into these almost non falsifiable things.

01:20:56.100 --> 01:21:00.100
Like, well, maybe it's secretly doing the thing and it's so good at deception that it looks like it isn't deceiving.

01:21:00.100 --> 01:21:03.100
Now, I think there's something to it and we should be wary about it.

01:21:03.100 --> 01:21:08.100
But I also think we have to be careful because those explanations are not satisfying to most people.

01:21:08.100 --> 01:21:09.100
Oh, look, it can do anything.

01:21:09.100 --> 01:21:10.100
Well, tell me about it.

01:21:10.100 --> 01:21:11.100
Oh, anything.

01:21:11.100 --> 01:21:12.100
You're like, well, I don't understand what you mean.

01:21:12.100 --> 01:21:15.100
To your point, though, yes, right now I'm not concerned about that.

01:21:15.100 --> 01:21:16.100
I don't see an issue with that.

01:21:16.100 --> 01:21:28.100
That said, as we start to get beyond the GPT for or Gemini and these frontier models, I think it's a very important thing to assess not only before deployment, but in the training stage, there should be methods and benchmarks in place.

01:21:28.100 --> 01:21:34.100
And even asking the labs, what are your expectations for the capabilities of your models throughout the process?

01:21:34.100 --> 01:21:42.100
And if they're the lab themselves are like consistently wrong in a certain direction, like, oh, we thought it would be certainly capable and it ends up being more capable every time.

01:21:42.100 --> 01:21:43.100
Like, that's not a good track record.

01:21:43.100 --> 01:21:52.100
So next time when you say it's not going to be as capable as we thought it probably will be just some sort of way to get at how we might understand these things as it goes in the future.

01:21:52.100 --> 01:21:54.100
Again, we currently have autonomous systems, right?

01:21:54.100 --> 01:21:56.100
As I said, so they're doing banking stuff.

01:21:56.100 --> 01:21:57.100
They're doing fraud detection.

01:21:57.100 --> 01:21:59.100
They're doing cybersecurity things.

01:21:59.100 --> 01:22:01.100
They're stopping missiles that are being bombed.

01:22:01.100 --> 01:22:05.100
The incentives to have these things become more autonomous will be there.

01:22:05.100 --> 01:22:09.100
And then again, if the system doesn't exist, it can't really do its job.

01:22:09.100 --> 01:22:13.100
I want to be careful here that there's a sort of the as if goals, right?

01:22:13.100 --> 01:22:18.100
That the system only needs to act as if it is engaging in self preservation.

01:22:18.100 --> 01:22:20.100
It doesn't have to have like, I'm an AI.

01:22:20.100 --> 01:22:21.100
I have a certain goal.

01:22:21.100 --> 01:22:22.100
I need to exist.

01:22:22.100 --> 01:22:25.100
It may do something like that, but it doesn't have to.

01:22:25.100 --> 01:22:27.100
And I don't want us to sort of think it needs to be the case.

01:22:27.100 --> 01:22:35.100
It's more just going to engage in various, we'll say from our perspective, reasonable goal oriented processes to make it more likely to achieve its goal.

01:22:35.100 --> 01:22:40.100
And some of those will involve getting more power to ensure its own existence so we can serve that end.

01:22:40.100 --> 01:22:44.100
Now again, maybe not, but there's enough of a maybe so that we should be very careful.

01:22:44.100 --> 01:22:52.100
There's a bunch of emergent capabilities in AI systems that could be problematic if we're trying to align these systems.

01:22:52.100 --> 01:23:01.100
So we're talking about power seeking, deception, manipulation of humans, using humans to achieve goals in the physical world and so on.

01:23:01.100 --> 01:23:07.100
Which of these traits do you worry about the most and where do you think we are on the road to these traits?

01:23:07.100 --> 01:23:13.100
How close do you think we are to having manipulative or deceptive or power seeking system?

01:23:13.100 --> 01:23:17.100
I don't think we're there yet, but what could happen in a short period of time could definitely be the case.

01:23:17.100 --> 01:23:26.100
So in the first example, just the emergence itself, I think a lot of people think the recent large language models, your chat GPT, GPT4 and cloud and whatnot,

01:23:26.100 --> 01:23:29.100
they are largely emergent in a lot of their capabilities, right?

01:23:29.100 --> 01:23:34.100
You have these systems that were trained on mainly English and then they can speak other languages.

01:23:34.100 --> 01:23:41.100
That was not the plan, right? Or it was trained mostly on sort of corpus of text and then it can also do computer programming.

01:23:41.100 --> 01:23:46.100
So it's not that if someone had thought through a lot of this, they would have realized, oh, maybe it will also do the thing.

01:23:46.100 --> 01:23:54.100
It's that from our perspective, it did seem like certain behavior emerged in a way that was unexpected on a plan, at least in some cases in some domains.

01:23:54.100 --> 01:23:57.100
So all I have to say is emergence seems to be like all over the place, right?

01:23:57.100 --> 01:24:03.100
Especially if you try to prompt a model in a certain way and you get a certain thing which you didn't think it might do, but then it would.

01:24:03.100 --> 01:24:13.100
It was just strange to me knowing something about the training process of GPT4, interacting with the system in my native language and talking to it perfectly in Danish

01:24:13.100 --> 01:24:20.100
and seeing that it's pretty capable in my native language was kind of a surprise and an interesting experience.

01:24:20.100 --> 01:24:26.100
And I believe that a lot of people have had that experience of talking to it in their native language.

01:24:26.100 --> 01:24:31.100
And knowing that it wasn't trained specifically to do that, it's quite impressive.

01:24:31.100 --> 01:24:38.100
I agree. I think it's staggeringly impressive. It's hard to think of a human parallel because clearly Danish was somewhere in the training set, right?

01:24:38.100 --> 01:24:41.100
It's not like it read English and then it like invented Danish.

01:24:41.100 --> 01:24:43.100
No, no, of course not. Just for everyone else.

01:24:43.100 --> 01:24:47.100
But at the same time, it's like, well, you have a test coming up. It's an English test.

01:24:47.100 --> 01:24:51.100
Here's a whack of material and like study all of it, but focus on the English like, okay.

01:24:51.100 --> 01:24:55.100
And like, and then Danish like what? And of course, it's not just Danish. It's numerous of the languages.

01:24:55.100 --> 01:24:59.100
It's also math, not usually the addition which it has trouble with like complicated math.

01:24:59.100 --> 01:25:04.100
And if, you know, famous mathematicians like Terence Tau are using these systems, like it really helps improve my workflow.

01:25:04.100 --> 01:25:08.100
You're like, okay, well, that's clearly a significant indicator of capability.

01:25:08.100 --> 01:25:13.100
So you could also imagine if then someone's like, we really need a dedicated math system.

01:25:13.100 --> 01:25:17.100
This goes to that general versus narrow, would it be more capable?

01:25:17.100 --> 01:25:22.100
It seems to think like it should be more capable, but it's possible the general somehow is more capable, right?

01:25:22.100 --> 01:25:31.100
And then fine tuning tweaks it to your other question, though, I am concerned to sort of thinking about sort of that security nature of things, right?

01:25:31.100 --> 01:25:35.100
If you're looking through the world from a security lens, security mindset, you think like, where are the weak links, right?

01:25:35.100 --> 01:25:42.100
And all companies and even individuals deal with this to some extent, and people can be manipulated in a sort of social engineering way, right?

01:25:42.100 --> 01:25:47.100
People get information just by calling someone up, pretending to be someone else, or there's more overt cyber hacking and whatnot.

01:25:47.100 --> 01:25:54.100
But with AI systems, it's almost like you take the normal problems that already exist, and then there are also still problems in the AI space.

01:25:54.100 --> 01:26:00.100
So whether there's, you know, people who are bribeable, people who are manipulatable, people you can hire off the dark web.

01:26:00.100 --> 01:26:05.100
Like it's one of these sort of things that most people don't like to think about for obvious reasons, but there are like hitmen.

01:26:05.100 --> 01:26:10.100
You can hire them to kill people. They just don't hire them to kill you because you're not that important, right?

01:26:10.100 --> 01:26:15.100
And thankfully, right? It's this whole weird world, you're like, what happens? What type of criminal activity?

01:26:15.100 --> 01:26:21.100
Like there's actual criminals who do things. And so to think that AI systems won't liaise with, if they're trying to cause harm,

01:26:21.100 --> 01:26:28.100
nefarious individuals who literally can be paid to do crime to make themselves more capable, it seems like that would be an oversight.

01:26:28.100 --> 01:26:34.100
So I guess I'm concerned about a range of these things. Again, at the moment, not that concerned about that much of it.

01:26:34.100 --> 01:26:39.100
But because it often takes months, years to address problems, you need the infrastructure in place now.

01:26:39.100 --> 01:26:45.100
You need what is the problem type conversations. Problem definition is very important. People often speak past each other.

01:26:45.100 --> 01:26:52.100
So if we can agree that an AI system that would have an ability to manipulate people, or that would have an ability to hire people to do certain tasks,

01:26:52.100 --> 01:26:54.100
is a potential problem, that's a good start.

01:26:54.100 --> 01:27:02.100
And I think we already saw when GPT-4 was being evaluated, there was that famous case where, again, the AI system itself did not do this,

01:27:02.100 --> 01:27:12.100
but it was liaising with the researchers in between. And the GPT-4 was able to hire someone off task rabbit and lie about why it needed that person to fill out a capture,

01:27:12.100 --> 01:27:20.100
to build a commuter code. Again, the system didn't do it, but it's like, well, it doesn't seem that complicated to connect those things or to have enabled the system to do it.

01:27:20.100 --> 01:27:29.100
So if it was the case months ago, that if a system with a bit of tweaks could have hired someone off the internet to circumvent something, to stop AI systems,

01:27:29.100 --> 01:27:34.100
and lie about why it did it, why wouldn't this be possible in the future? It's already happened.

01:27:34.100 --> 01:27:39.100
It's like, okay, so now imagine something that's more people, more people flying, can sort of think through step by step on its own,

01:27:39.100 --> 01:27:42.100
would know how to navigate a decision once it has more examples.

01:27:42.100 --> 01:27:53.100
Again, this human history, fiction novels, movies, or even just current events, are replete with numerous examples of people deceiving each other and engaging in various complicated nefarious schemes.

01:27:53.100 --> 01:27:57.100
And you could imagine that as a wonderful training data set for someone trying to cause harm.

01:27:57.100 --> 01:28:10.100
Do you think we will get something like a unified solution to the alignment problem, like something that solves all of these problems that we just talked about, deception, manipulation, power-seeking, and so on?

01:28:10.100 --> 01:28:21.100
Another way to ask maybe is, do you think we'll get something like the theory of evolution, which solves a bunch of distinct problems in a general and simple way,

01:28:21.100 --> 01:28:27.100
or will it look more like whack-a-mole, solving one problem, moving on to the next problem?

01:28:27.100 --> 01:28:30.100
I like this the option between evolution and whack-a-mole.

01:28:31.100 --> 01:28:33.100
I said the evolution of whack-a-mole.

01:28:33.100 --> 01:28:37.100
So I think it's probably a bit more the whack-a-mole.

01:28:37.100 --> 01:28:41.100
And the reason here is, again, like the straightforward logic of human incentives and human behaviors.

01:28:41.100 --> 01:28:46.100
So say someone develops a system that, as you said, reliably does what it's supposed to do.

01:28:46.100 --> 01:28:49.100
Well, that means it could be used for good or bad, right?

01:28:49.100 --> 01:28:59.100
At the moment, the AI systems, and I briefly mentioned this in the book, they're kind of sort of loosely corporate American Western values, like what's acceptable and what's not.

01:28:59.100 --> 01:29:02.100
But what it does mean is that someone has their hand on the lever.

01:29:02.100 --> 01:29:05.100
Someone is sort of manipulating the system to do X and not Y.

01:29:05.100 --> 01:29:12.100
So it is already the case that certain values are being implemented or at least displayed through these systems of a certain type.

01:29:12.100 --> 01:29:20.100
Now, if you just had the alignment of does what we want, if someone was a malevolent actor and wanted to cause harm, it could use the thing to do what we want.

01:29:20.100 --> 01:29:26.100
So then you're like, well, is the alignment problem really going to solve not having bad people do bad things,

01:29:26.100 --> 01:29:30.100
or not having, we'll say, even desperate or confused people inadvertently do bad things?

01:29:30.100 --> 01:29:31.100
That's the other thing.

01:29:31.100 --> 01:29:33.100
Well, I would say people could be bribed and manipulated.

01:29:33.100 --> 01:29:42.100
People could also just be persuaded, like their child is very sick, they're desperate, they need money, or maybe like, oh, my child has a certain form of cancer is like, look, I can cure it.

01:29:42.100 --> 01:29:43.100
I just read a thousand papers.

01:29:43.100 --> 01:29:44.100
I just need some resources.

01:29:44.100 --> 01:29:47.100
I understand desperation could also be a factor here.

01:29:47.100 --> 01:29:54.100
So trying to get rid of, like, not say everyone's evil and nefarious out there, it's just like there's many reasons why someone might give up power.

01:29:54.100 --> 01:29:59.100
And it's hard to imagine how sort of the alignment problem might sort of address all that.

01:29:59.100 --> 01:30:03.100
In the earlier version of the book, actually, there were four different alignment problems I was going to talk about.

01:30:03.100 --> 01:30:06.100
But I thought that was too complicated for my audience.

01:30:06.100 --> 01:30:08.100
It really was like, you're not fully aligned with yourself.

01:30:08.100 --> 01:30:10.100
We're not fully aligned with each other.

01:30:10.100 --> 01:30:12.100
We're not necessarily fully aligned with AI.

01:30:12.100 --> 01:30:17.100
And then AI itself may not be aligned with us or with itself if there's multiple AIs.

01:30:17.100 --> 01:30:23.100
But I thought, okay, you know, let's let's streamline this to then think about as a gas model as robotics to make it easy that that type of line is a problem.

01:30:23.100 --> 01:30:26.100
And then the more traditional the alignment problem stuff.

01:30:26.100 --> 01:30:30.100
Right now, there's a bunch of proposals on the table for making AI safe.

01:30:30.100 --> 01:30:34.100
There's a lot of attention on this issue and in policy circles.

01:30:34.100 --> 01:30:41.100
And I think you had a great discussion in the book about principles for selecting among these proposals for AI safety.

01:30:41.100 --> 01:30:44.100
Maybe you could talk a bit about these principles.

01:30:44.100 --> 01:30:45.100
Sure thing.

01:30:45.100 --> 01:30:47.100
So yes, there's a lot of great proposals out there.

01:30:47.100 --> 01:30:53.100
But it's sometimes useful to take a step back and even think like, what's the framework that we're even using to think through these proposals?

01:30:53.100 --> 01:31:01.100
And even if one set out like that's kind of obvious, like, sure, but let's put it down because sometimes what you think is obvious is obvious to you and not someone else.

01:31:01.100 --> 01:31:03.100
Or you see where you might agree, right?

01:31:03.100 --> 01:31:06.100
If you have five principles and I have five, maybe three overlap and that's great.

01:31:06.100 --> 01:31:12.100
So I kind of tried to keep it simple and think through like, what are the three main ways we might want to think about this?

01:31:12.100 --> 01:31:14.100
Or that should be a part of any principle.

01:31:14.100 --> 01:31:17.100
And so that's verification and agility, adaptability.

01:31:17.100 --> 01:31:18.100
That's the second one.

01:31:18.100 --> 01:31:20.100
And the third one is defense in depth.

01:31:20.100 --> 01:31:24.100
So verification is just realizing that we need to verify.

01:31:24.100 --> 01:31:26.100
You know, it's nice to say trust, but verify.

01:31:26.100 --> 01:31:28.100
But the idea is that everyone should be accountable.

01:31:28.100 --> 01:31:29.100
There should be transparency.

01:31:29.100 --> 01:31:32.100
There should be verification mechanisms built in.

01:31:32.100 --> 01:31:37.100
And if actors in the space, the companies that are developing these things say there's no problem.

01:31:37.100 --> 01:31:38.100
There should be no problem then.

01:31:38.100 --> 01:31:40.100
Let's let's let's verify everything, right?

01:31:40.100 --> 01:31:43.100
If you think you're not developing anything harmful, let's have that as a backbone.

01:31:43.100 --> 01:31:51.100
We'll say of any proposal that you want to ensure that things are happening as they're understood to be happening as much as possible.

01:31:51.100 --> 01:32:00.100
And even the idea that people will try to circumvent verification as they always do in this world, at least to some extent by some actors, that should also be built into the process.

01:32:00.100 --> 01:32:02.100
So you can't just take people's words for it.

01:32:02.100 --> 01:32:07.100
I can't remember the quote, but something's like they can't be grading their own homework here in these AI systems like these companies.

01:32:07.100 --> 01:32:08.100
It just doesn't work that way.

01:32:08.100 --> 01:32:11.100
And again, even if they're not necessarily nefarious, great.

01:32:11.100 --> 01:32:14.100
We'll just have it all above board and have ensure that verification is there.

01:32:14.100 --> 01:32:16.100
The second one, agility and adaptability.

01:32:16.100 --> 01:32:24.100
Again, this isn't like a novel insight, but it's just really trying to highlight how fast moving the spaces and how we really have to think through.

01:32:24.100 --> 01:32:26.100
What if it happens even faster, right?

01:32:26.100 --> 01:32:32.100
That a lot of times in the policy regulatory legal space, things take months, years to work through the system.

01:32:32.100 --> 01:32:34.100
And what if it has to be much less than that?

01:32:34.100 --> 01:32:39.100
Or what if you have a law that you thought was useful that, you know, usually what happens is the laws developed.

01:32:39.100 --> 01:32:41.100
It's somehow it comes to exist in the world.

01:32:41.100 --> 01:32:43.100
Sometimes it interacts with challenges from the courts.

01:32:43.100 --> 01:32:45.100
There's some sort of settled agreement.

01:32:45.100 --> 01:32:48.100
The law seems to have some sort of a standard or consistency.

01:32:48.100 --> 01:32:50.100
And then maybe it's challenging in the future.

01:32:50.100 --> 01:32:53.100
That's a multi year, some nice multi decade process for the AI stuff.

01:32:53.100 --> 01:32:55.100
It might have to be multiple months, multiple years.

01:32:55.100 --> 01:33:03.100
So how can we even start thinking about changing almost the machinery of government and parts of the world to at least address these sorts of things?

01:33:03.100 --> 01:33:08.100
Yes, you can have amendments to laws, but really thinking through that this is a factor and that people should be prepared.

01:33:09.100 --> 01:33:11.100
For things happening faster than usual.

01:33:11.100 --> 01:33:13.100
The third one, defense in depth.

01:33:13.100 --> 01:33:16.100
This comes more from cybersecurity than the military definition.

01:33:16.100 --> 01:33:18.100
And that's that we need multiple layers of defense.

01:33:18.100 --> 01:33:24.100
You don't expect any one particular proposal or any one particular action to lead to safety or security.

01:33:24.100 --> 01:33:26.100
But you're trying to have multiple layers.

01:33:26.100 --> 01:33:32.100
So if any one of them fails and you actually expect them to fail, that there are others there to sort of pick up the slack.

01:33:32.100 --> 01:33:33.100
Makes a lot of sense.

01:33:33.100 --> 01:33:34.100
All of these principles.

01:33:34.100 --> 01:33:39.100
So in the book, you you discuss eight proposals for for safe AI innovation.

01:33:39.100 --> 01:33:42.100
Maybe you could talk about your favorites here.

01:33:42.100 --> 01:33:44.100
What are the most important things?

01:33:44.100 --> 01:33:46.100
It's like, how do you choose a favorite child?

01:33:46.100 --> 01:33:48.100
Right? No, I just think the music.

01:33:48.100 --> 01:33:51.100
So why there's eight, by the way, and why isn't there 10 and why isn't there seven?

01:33:51.100 --> 01:33:53.100
Well, you know, eight seemed like a good number.

01:33:53.100 --> 01:33:56.100
I think this really encapsulated what I thought were a good number.

01:33:56.100 --> 01:34:02.100
Also, why I want these proposals or why I want these to be discussed is not that these are again definitive.

01:34:02.100 --> 01:34:05.100
The whole book is trying to be very open minded solutions oriented.

01:34:05.100 --> 01:34:08.100
We need more people working on this and we all need to come together to work on this.

01:34:08.100 --> 01:34:12.100
So if it is the case that someone's like, you know what, most of your proposals don't work for me.

01:34:12.100 --> 01:34:14.100
I'm like, OK, which ones do, right?

01:34:14.100 --> 01:34:19.100
Because there's as we know, this is unfortunate tension that exists sometimes in different AI safety and ethics communities.

01:34:19.100 --> 01:34:26.100
Or some people who are more focused on, we'll say present day concerns like algorithmic bias are taking issue with people who are more concerned about existential threats.

01:34:26.100 --> 01:34:28.100
And this is an unfortunate division.

01:34:28.100 --> 01:34:30.100
I mean, there's some rationale behind it, right?

01:34:30.100 --> 01:34:37.100
I mean, there's some kind of representation of resources, but I think it's largely just an unfortunate way the world's turned out and it doesn't have to be this way.

01:34:37.100 --> 01:34:43.100
So with the eight proposals, there could be like, OK, yes, some of those are maybe more X risk or existential risk oriented.

01:34:43.100 --> 01:34:45.100
But what are the ones that work for you now?

01:34:45.100 --> 01:34:46.100
Some sort of liability.

01:34:46.100 --> 01:34:47.100
That's one of the proposals.

01:34:47.100 --> 01:34:48.100
Great.

01:34:48.100 --> 01:34:51.100
Some sort of a transparency or identification you're interacting with an AI system.

01:34:51.100 --> 01:34:54.100
And that's where I'm trying to like, you know, all of branches all over the place.

01:34:54.100 --> 01:34:59.100
Build bridges here that if certain proposals don't work, pick the ones that do or show me your list of eight.

01:34:59.100 --> 01:35:04.100
And let's work on those together that seems like it's going to have the most broader support that is going to be good for all these issues.

01:35:04.100 --> 01:35:09.100
I think if we took a step back, some sort of liability for these systems, again, why do people do anything?

01:35:09.100 --> 01:35:10.100
Will they respond to incentives?

01:35:10.100 --> 01:35:15.100
And if they are held liable, sometimes personally liable for how these systems malfunction, that is usually a good lever.

01:35:15.100 --> 01:35:20.100
And it can't just be a sort of distributed sense of like, well, I did a thing, but I'm not really accountable.

01:35:20.100 --> 01:35:22.100
Like, well, let's think through what makes the most sense here.

01:35:22.100 --> 01:35:30.100
And maybe if you are responsible for distributing an AI model, even though you didn't create it, you do bear some of the liability here, some of the accountability.

01:35:30.100 --> 01:35:34.100
I think compute governance is also a very important one here.

01:35:34.100 --> 01:35:38.100
This is sort of like getting a sense of and controlling who has access to which chips.

01:35:38.100 --> 01:35:45.100
The U.S. has already implemented some of these things with, you know, export controls on China and some recent additions to that, which are making it even more restrictive.

01:35:45.100 --> 01:35:49.100
The book Chip War is a fantastic exploration of some of these issues.

01:35:49.100 --> 01:35:57.100
And so in that sense, if you think of the main reasons why AI is increasing in capability, usually people think there's three main inputs, right?

01:35:57.100 --> 01:36:05.100
You have the computational power, you have lots of data, and then you have like the algorithms itself, which are partly a human thing, like just talent pool that are building these things,

01:36:05.100 --> 01:36:08.100
but also insights from science and other domains and even AI itself.

01:36:08.100 --> 01:36:18.100
So how can you, as taking a big step back as a system, as a government, as an international organization of sorts, think about how to control or at least have a sense of influencing these inputs?

01:36:18.100 --> 01:36:22.100
And data is kind of everywhere. It's really hard to stop people accessing data.

01:36:22.100 --> 01:36:28.100
With the talent pool, the algorithms, that also seems like we want free mobility of labor for the most part, right?

01:36:28.100 --> 01:36:31.100
We don't want to lock people up and tell them they can't work certain places or anything else. That seems bad.

01:36:31.100 --> 01:36:35.100
But with the compute governance, it is more of a tangible physical thing.

01:36:35.100 --> 01:36:43.100
There's a chip, which is understood at least to some extent of what it does and how it works, and getting a sense of where these chips go could be very, very useful.

01:36:43.100 --> 01:36:48.100
Now, to reassure some people, this isn't for all AI systems. This is really the frontier AI model.

01:36:48.100 --> 01:36:53.100
So for the average consumer, the average AI business, the average AI product, this doesn't really affect them at all.

01:36:53.100 --> 01:37:01.100
It's more that are you using the most advanced chips and do you have thousands and thousands of them that you can put together in a cluster to then train highly capable models?

01:37:01.100 --> 01:37:06.100
So it really is something that sort of like those taxes, like, oh, if you make more than $100 million, this is a tax.

01:37:06.100 --> 01:37:09.100
And people are like, I would never want to be taxed like that. Like, well, don't worry, you never will.

01:37:09.100 --> 01:37:14.100
So for the most part, it doesn't affect most people or most businesses. But I think that's very useful.

01:37:14.100 --> 01:37:19.100
Now, how you go about it with the proposals, I have an idea, I have some sketch, I have some detail.

01:37:19.100 --> 01:37:26.100
But I always want to say, like, let's think this through. What is the best version of this to go forward with the most recent evidence with the most recent analysis?

01:37:26.100 --> 01:37:34.100
What is the best version? Is it useful to have, you know, ways that the chips can't communicate with each other as much so you can't bundle more like weaker chips together?

01:37:34.100 --> 01:37:39.100
Or should there be some sort of remote kill switch even for the chips themselves so we can't shut down the Internet?

01:37:39.100 --> 01:37:44.100
Maybe let's study it. Let's see if that's viable. If it turns out it's not for good or bad reasons, then we wouldn't do that.

01:37:44.100 --> 01:37:48.100
But really trying to think through how can we access some of that stuff.

01:37:48.100 --> 01:37:55.100
One of your proposals is about investing in AI safety research. So here I'm imagining a technical AI safety research.

01:37:55.100 --> 01:38:04.100
At least when I try to try to see this from the perspective of a funder, I worry that it's extremely difficult to choose among proposals.

01:38:04.100 --> 01:38:10.100
What should you fund? How should you respond to an applicant being optimistic about solving the problem?

01:38:10.100 --> 01:38:15.100
Is pessimism a sign that you understand the problem in a deeper way or it's optimistic?

01:38:15.100 --> 01:38:25.100
You know, there's so many complexities and in a sense this is just normal science funding, but I worry that this problem is even stronger in trying to fund AI safety.

01:38:25.100 --> 01:38:32.100
Do you have any ideas about how to go about evaluating proposals for technical AI safety research?

01:38:32.100 --> 01:38:41.100
I think you've raised a great point and the smile was just the idea that yes, if someone is more depressed or less optimistic about solving the problem, do they get more money, right?

01:38:41.100 --> 01:38:45.100
Intellectual integrity and, you know, epistemic modesty and all these things and maybe that's the case.

01:38:45.100 --> 01:38:55.100
So I would say that I don't necessarily have specifics. I think it's sort of like, do we agree that there are currently a lot more people that are trying to increase capabilities that are concerned about safety?

01:38:55.100 --> 01:39:04.100
Like, do we agree this is a fact? Whether the numbers like 100 to 1 or more or less, you know, AI capabilities researchers versus safety researchers, do we agree there's some sort of large discrepancy?

01:39:04.100 --> 01:39:12.100
And that probably shouldn't be the case. I think Ian Hogarth had that nice, like a sort of two chart graph, like, well, one line's going way up and the other one's not really going up and meeting it at all.

01:39:12.100 --> 01:39:20.100
So if there is a disconnect between safety and capabilities, what should we do about that? And it seems like somehow funding safety research seems like a good idea.

01:39:20.100 --> 01:39:26.100
Sort of like, let's start there. If we can't get agreement there, then that's an issue. But assuming there is some agreement, yes, how to actually go about it.

01:39:26.100 --> 01:39:34.100
And here I would probably kind of defer to people who've already been in the space for several years and have them kind of talk to each other and see what are some current best practices.

01:39:34.100 --> 01:39:42.100
You're right. It's an absolute mess. And how to study safety without increasing capabilities seems to be one of the biggest issues of all.

01:39:42.100 --> 01:39:50.100
And as anthropic and other people have reasonably said, even though it's kind of a bitter total swallow, but we need the advanced AI system to study the safety of the system.

01:39:50.100 --> 01:39:58.100
And it doesn't really make sense to try to think about the safety of, say, GPT-4 or GPT-5 if you're currently working on GPT-2 or GPT-3.

01:39:58.100 --> 01:40:05.100
From what I understand, not that much of what you learned is going to be applicable and translatable. So I think it's a really important issue.

01:40:05.100 --> 01:40:13.100
And I think it's just sort of really orienting people to realize, look how much capabilities are increasing and there is not nearly the attention paid on safety, not at all.

01:40:13.100 --> 01:40:21.100
And in fact, some of the organizations don't care at all. And so if people really onboarded that, okay, some of these people seem to be more responsible actors and some of them don't care about safety at all,

01:40:21.100 --> 01:40:29.100
they're just trying to make the product as fast as possible and dump it into the world, like I think it was the Mistral AI, then we should be concerned.

01:40:29.100 --> 01:40:35.100
Great. There's a lot of complexities there. We could talk for hours on this topic. Darren, thanks for talking with me. It's been a pleasure.

01:40:35.100 --> 01:40:37.100
My pleasure as well, Gus. Thanks so much.

