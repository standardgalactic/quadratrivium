WEBVTT

00:00.000 --> 00:03.360
Welcome to the Future of Life Institute podcast.

00:03.360 --> 00:06.480
My name is Gus Docker and I'm here with Dan Hendricks.

00:06.480 --> 00:10.000
Dan is the Director of the Center for AI Safety.

00:10.000 --> 00:11.520
Dan, welcome to the podcast.

00:11.520 --> 00:12.840
Glad to be back.

00:12.840 --> 00:16.760
You are also an advisor to XAI.

00:16.760 --> 00:18.360
Maybe you can tell us a bit about that.

00:18.360 --> 00:23.560
Sure. So XAI is Elon's new AGI project.

00:23.560 --> 00:25.520
It's still very much in its early stages,

00:25.520 --> 00:27.680
so it's difficult to say

00:27.680 --> 00:30.440
specific things about what they'll be doing

00:30.440 --> 00:35.200
or what the specific high-level strategy is to give a sense.

00:35.200 --> 00:38.960
Elon has been interested in the failure mode

00:38.960 --> 00:41.560
of sort of eroded epistemics

00:41.560 --> 00:46.120
where people don't have a shared sense of consensus reality,

00:46.120 --> 00:48.000
and this might make it harder for a civilization

00:48.000 --> 00:49.680
to appropriately function.

00:49.680 --> 00:53.360
There are other types of extras that he's concerned about as well.

00:53.360 --> 00:56.280
His sort of probability of doom

00:56.280 --> 00:58.480
or of that of an existential catastrophe

00:58.480 --> 01:00.280
is around like 20% to 30%.

01:00.280 --> 01:02.520
So he takes this, I would guess, more seriously

01:02.520 --> 01:07.440
than any other leader of a major AGI organization,

01:07.440 --> 01:10.520
but exactly how when it goes about reducing that risk

01:10.520 --> 01:13.080
is still somewhat to be determined.

01:13.080 --> 01:16.200
There is an interest in building more true seeking AIs,

01:16.200 --> 01:18.000
but on other occasions, too,

01:18.000 --> 01:20.720
he'd mentioned that we should have AIs with the objective

01:20.720 --> 01:22.840
of preserving human autonomy

01:22.840 --> 01:25.200
or maximizing the freedom of action

01:25.200 --> 01:27.680
and on other instances,

01:27.680 --> 01:29.840
in thinking about good objectives for AI systems,

01:29.840 --> 01:33.440
having them increase net civilizational happiness over time.

01:33.440 --> 01:36.040
So I think that this reflects sort of a plurality

01:36.040 --> 01:39.520
of different goals that he thinks AI systems

01:39.520 --> 01:41.480
should end up pursuing

01:41.480 --> 01:44.240
rather than picking just exactly,

01:44.240 --> 01:45.640
rather than just picking one.

01:45.640 --> 01:48.160
I think it's relevant to note

01:48.160 --> 01:50.480
that it's a fairly serious effort.

01:50.480 --> 01:53.240
I'd anticipate that it would probably be

01:53.280 --> 01:56.480
one of the main three AI companies

01:57.480 --> 01:59.840
next year or the year after,

01:59.840 --> 02:04.680
like OpenAI, Google DeepMind, and XAI.

02:04.680 --> 02:09.480
So I don't think of it as a smaller effort,

02:09.480 --> 02:12.240
but it has the capacity

02:12.240 --> 02:14.520
to have a substantial ratio of force, so.

02:14.520 --> 02:17.880
The other top AI corporations you mentioned,

02:17.880 --> 02:20.280
Anthropic, Google DeepMind, OpenAI,

02:20.280 --> 02:24.320
have backing from giant tech companies.

02:24.320 --> 02:26.480
Does XAI similarly have some backing

02:26.480 --> 02:28.360
from Tesla, for example?

02:28.360 --> 02:31.240
I can't specifically say about that,

02:31.240 --> 02:35.280
but this is not a subpart of Tesla.

02:35.280 --> 02:38.920
This is not an organization inside of Twitter or X,

02:38.920 --> 02:41.600
and it's not an organization inside of Tesla.

02:41.600 --> 02:44.680
The main topic of conversation for this episode

02:44.680 --> 02:48.160
is your paper on catastrophic risks from AI

02:48.160 --> 02:50.440
and specifically categorizing these risks.

02:50.440 --> 02:53.040
So you categorize risks from,

02:53.040 --> 02:56.760
catastrophic risks from AI in four different categories.

02:56.760 --> 02:59.880
Maybe we should just start by sketching out those categories

02:59.880 --> 03:01.840
and then go into depth later.

03:01.840 --> 03:04.480
Yeah, so I guess at a very abstract level,

03:04.480 --> 03:09.480
there's risks if people are trying to use AI

03:09.480 --> 03:11.080
intentionally to cause harm.

03:11.080 --> 03:12.000
That's a basic one.

03:12.000 --> 03:15.040
So there's an intentional catastrophe

03:15.040 --> 03:17.080
that would be malicious use.

03:17.120 --> 03:21.200
Another one is where there are accidents.

03:21.200 --> 03:23.280
And if there are accidents,

03:23.280 --> 03:27.760
this would often be the consequence of the AI developers

03:27.760 --> 03:29.560
using these very powerful systems

03:29.560 --> 03:31.880
or potentially leaking them

03:31.880 --> 03:34.080
or accidentally putting in some bad objective

03:34.080 --> 03:36.000
or doing some gain of function,

03:36.000 --> 03:37.840
but that would be some accident risks.

03:37.840 --> 03:40.240
So that relates to organizational risks

03:40.240 --> 03:42.440
or organizational safety.

03:42.440 --> 03:45.360
The third would be these environmental

03:45.360 --> 03:46.560
or structural risks.

03:46.560 --> 03:50.520
Basically where AI companies are or AI developers,

03:50.520 --> 03:52.520
be those companies or maybe in later stages,

03:52.520 --> 03:57.520
countries are racing to build more and more powerful

03:57.520 --> 04:00.240
AI systems or AI weapons.

04:00.240 --> 04:05.240
And this structural risk incentivizes companies to,

04:06.320 --> 04:10.840
or these developers to seed more and more decision making

04:10.840 --> 04:13.000
and control to these AI systems.

04:13.000 --> 04:15.040
We get a looser and looser leash.

04:15.040 --> 04:16.120
Things move very quickly.

04:16.120 --> 04:18.040
We become extremely dependent on them.

04:18.040 --> 04:22.760
This gets us in an irreversible position

04:22.760 --> 04:26.080
where we're not actually making the decisions,

04:26.080 --> 04:29.040
but we're basically having nominal control.

04:29.040 --> 04:30.400
It's a very possible in that situation,

04:30.400 --> 04:32.960
we just ultimately end up losing control

04:32.960 --> 04:35.560
to the sort of very complicated fast moving system

04:35.560 --> 04:36.680
that we create.

04:36.680 --> 04:41.680
And then the final type would be these risks

04:42.000 --> 04:44.200
that emanate from the AI systems themselves.

04:44.200 --> 04:47.520
These are more internal or inherent risks from AI systems.

04:47.520 --> 04:51.320
And that would take the form of rogue AIs

04:51.320 --> 04:54.240
where they have goals separate from our own

04:54.240 --> 04:58.160
and they work against us to complete

04:58.160 --> 05:01.680
or satisfy their desires or preferences.

05:01.680 --> 05:05.480
So overall, there are four.

05:05.480 --> 05:06.880
There's malicious use.

05:06.880 --> 05:09.000
There's these organizational risks.

05:09.000 --> 05:13.880
There's these structural slash environmental risks

05:13.880 --> 05:16.160
and there's these inherent or internal risks

05:16.160 --> 05:20.240
in the form of malicious use, organizational risk,

05:20.240 --> 05:22.760
racing dynamics and rogue AIs.

05:22.760 --> 05:26.000
Yeah, so if we look back maybe 10 years or so,

05:26.000 --> 05:28.440
I think most of the discussion about AI risk

05:28.440 --> 05:30.360
would have been about rogue AI.

05:30.360 --> 05:35.000
So the risks that are coming internally from the AI,

05:35.000 --> 05:39.560
so to speak, the AI developing technically in ways

05:39.560 --> 05:41.120
that we're not interested in.

05:41.120 --> 05:42.880
So how much is this categorization

05:42.880 --> 05:44.520
set in stone?

05:44.520 --> 05:47.600
Do you think it'll change over time as we learn more

05:47.600 --> 05:51.560
or have the field of AI safety matured

05:51.560 --> 05:55.120
such that we can see the risk landscape now?

05:55.120 --> 05:57.800
I think the focus on rogue AI systems

05:57.800 --> 06:00.120
is largely due to early movers

06:00.120 --> 06:02.040
having substantial cultural influence.

06:02.040 --> 06:03.520
I think if we asked other people

06:03.520 --> 06:06.800
who were not as invested in AI risks,

06:06.800 --> 06:10.440
what if they were to write down concerns about these,

06:10.440 --> 06:12.320
they would of course think that people

06:12.320 --> 06:14.920
using the technology for extremely destructive purposes

06:14.920 --> 06:17.040
was posed catastrophic risks.

06:17.040 --> 06:21.240
And I think the communities ended up having

06:21.240 --> 06:23.800
some self-selection effects such that people

06:23.800 --> 06:25.880
didn't end up talking about things like malicious use

06:25.880 --> 06:28.080
and treated that as a distraction, I think were.

06:28.080 --> 06:31.760
So I think the community didn't make much of a space

06:31.760 --> 06:33.120
for people who were concerned about things

06:33.120 --> 06:36.480
other than rogue AI systems.

06:36.480 --> 06:38.640
But that was a mistake.

06:38.640 --> 06:42.560
The AI is being used in malicious ways

06:42.560 --> 06:44.640
can definitely cause catastrophes

06:44.640 --> 06:50.040
and can end up increasing the probability

06:50.040 --> 06:52.000
of existential risks as well,

06:52.000 --> 06:54.000
which maybe we'll speak about the connections

06:54.000 --> 06:57.880
between ongoing harms, anticipated risks

06:57.880 --> 07:00.720
and catastrophic risks and existential risks.

07:00.720 --> 07:04.040
I think the community of people

07:04.040 --> 07:06.720
who were thinking about AI risks a long time ago

07:06.720 --> 07:09.000
would largely think about whether there's a direct,

07:09.000 --> 07:13.560
simple, causal pathway to something like an extinction event.

07:13.560 --> 07:16.200
Now I think we have more of a sophisticated causal

07:16.200 --> 07:20.240
understanding of the interplay between these various factors,

07:20.240 --> 07:24.080
such that one doesn't try and look for direct mechanisms,

07:24.080 --> 07:26.840
but instead tries to look at what sort of events

07:26.840 --> 07:28.960
increase the probability of existential risk

07:28.960 --> 07:31.360
rather than does it directly cause extinction.

07:31.360 --> 07:33.320
And that distinction between something

07:33.320 --> 07:36.520
that increases probability versus directly caused

07:36.520 --> 07:39.520
means that we have to look at a much broader variety

07:39.520 --> 07:42.520
of factors and we can't end up just thinking

07:42.520 --> 07:45.080
that all we need to do is make a single,

07:45.080 --> 07:47.840
very powerful AI agent do what we want

07:47.840 --> 07:49.840
and then everything is solved forever.

07:49.840 --> 07:52.520
Unfortunately, we're going to have to treat this

07:52.520 --> 07:54.320
as a broader socio-technical problem.

07:54.320 --> 07:58.040
We're going to have to consider the various stakeholders,

07:58.040 --> 08:02.120
the politics, indeed the geopolitics,

08:02.120 --> 08:03.800
the relations between different countries,

08:03.800 --> 08:06.560
the liability laws and all these other things

08:06.560 --> 08:10.520
because we're not in this sort of fume type of scenario.

08:10.520 --> 08:13.040
It would seem, it seems we're more into a slow takeoff.

08:13.040 --> 08:15.000
So many of these real-world considerations

08:15.000 --> 08:17.600
that were sort of a sidelined infuses distractions

08:17.600 --> 08:19.520
is actually where most of the action is.

08:19.520 --> 08:21.800
What would be examples of something

08:21.800 --> 08:25.400
that might put society in a worse position

08:26.400 --> 08:30.120
where we are less able to handle a powerful AI?

08:30.120 --> 08:34.000
A prime example would be World War III.

08:34.000 --> 08:36.680
If there's World War III conditioned on that,

08:36.680 --> 08:39.440
that increases the probability of existential risk

08:39.440 --> 08:41.080
from AI systems.

08:41.080 --> 08:44.920
This would spur a substantial AI arms race.

08:44.920 --> 08:47.960
We would quickly outsource lethality to them.

08:47.960 --> 08:49.520
We would not have nearly as much time

08:49.520 --> 08:54.000
for making them more aligned and reliable in that process,

08:54.000 --> 08:56.360
but still the competitive pressures

08:56.360 --> 08:59.600
would compel different states

08:59.600 --> 09:02.240
to create powerful AI weapons

09:02.240 --> 09:05.040
and eventually have that take up more and more

09:05.040 --> 09:07.880
of their military force.

09:07.880 --> 09:11.520
But that doesn't directly cause extinction.

09:12.640 --> 09:14.720
So if we try and back chain from that,

09:14.720 --> 09:17.920
the story gets much more complicated

09:17.920 --> 09:21.760
and so then it's not viewed in the scenarios

09:21.760 --> 09:23.000
that others were thinking of.

09:23.000 --> 09:25.840
There's an AI lab, they've suddenly got a God-like AI

09:25.840 --> 09:28.160
and then it has decisive strategic control

09:28.160 --> 09:29.400
over the entire world.

09:29.400 --> 09:31.840
How will they make sure that it does what they want?

09:31.840 --> 09:34.240
That was, I think, the scenario

09:34.240 --> 09:36.360
that others were thinking largely

09:36.360 --> 09:41.360
and all other ones were too broad, too intractable.

09:41.920 --> 09:43.320
Essentially, there was a focus

09:43.320 --> 09:47.320
on quote-unquote targeted interventions historically,

09:47.320 --> 09:49.760
where we're just a small number of people.

09:49.760 --> 09:51.800
We can't do these broad interventions

09:51.880 --> 09:55.920
that involve interfacing with various institutions

09:55.920 --> 09:58.480
and getting public support.

09:58.480 --> 10:00.480
Those are intractable.

10:00.480 --> 10:02.720
So the best we can do is do some very narrow,

10:02.720 --> 10:05.160
specific things, maybe technical research.

10:05.160 --> 10:06.800
This doesn't look like a strategy

10:06.800 --> 10:09.320
because broad interventions are actually more tractable.

10:09.320 --> 10:10.600
The world is interested in this

10:10.600 --> 10:12.240
and we have some amount of time

10:12.240 --> 10:15.920
to try and help our institutions make good decisions

10:15.920 --> 10:17.520
and policy around these issues.

10:17.520 --> 10:21.520
Do you think this broader vision of AI safety

10:21.520 --> 10:24.200
should make us more positive or less positive?

10:24.200 --> 10:27.440
Imagine we have to set all of the institution up perfectly.

10:27.440 --> 10:29.400
It seems like we have a narrow corridor

10:29.400 --> 10:30.520
to make things right,

10:30.520 --> 10:32.760
where the institutions have to be there,

10:32.760 --> 10:34.240
the technical side have to work,

10:34.240 --> 10:38.280
the all stakeholders have to be set up well

10:38.280 --> 10:41.200
for this to succeed for us.

10:41.200 --> 10:43.000
Should the complexity of the problem

10:43.000 --> 10:45.320
make us more pessimistic?

10:45.320 --> 10:47.600
I think there's at least more tractability

10:47.600 --> 10:52.760
compared to an AI suddenly goes from incompetent

10:52.760 --> 10:55.520
to omnicompetent and in control of the world overnight

10:55.520 --> 10:57.640
and we have no idea if it was emergent

10:57.640 --> 10:59.440
and we didn't actually control that process.

10:59.440 --> 11:01.560
That doesn't have almost any tractability to it.

11:01.560 --> 11:03.280
I don't think we need our institutions

11:03.280 --> 11:05.760
to be completely perfect.

11:05.760 --> 11:09.440
We just need to try and be in the business of reducing risk.

11:09.440 --> 11:14.440
So maybe that's one other conceptual distinction

11:14.440 --> 11:16.880
is that historically there'd be a focus

11:17.080 --> 11:20.520
on is it an airtight solution that works in the worst case

11:20.520 --> 11:23.880
where if something goes wrong, then it's insufficient

11:23.880 --> 11:26.280
because we have to get it right on the first try.

11:26.280 --> 11:28.360
When we do have some amount of time,

11:28.360 --> 11:30.600
not saying we have a huge amount of time,

11:30.600 --> 11:31.480
we have some amount of time,

11:31.480 --> 11:33.320
we can do some course adjustment

11:33.320 --> 11:36.840
and incorporate information as we go along.

11:36.840 --> 11:41.720
I'm not saying that's a surefire strategy,

11:41.720 --> 11:44.600
but I think that's the best we have

11:44.640 --> 11:51.640
and it allows us to correct some mistakes,

11:51.640 --> 11:54.960
but obviously we can't have much of an error tolerance,

11:54.960 --> 11:55.800
unfortunately.

11:55.800 --> 11:57.360
Do you think we are missing something

11:57.360 --> 12:00.560
with this categorization that you've set up in the paper?

12:00.560 --> 12:02.640
Could we be missing some category of risk

12:02.640 --> 12:05.640
that will be obvious to us in 20 years?

12:05.640 --> 12:08.800
And could that risk potentially be the most dangerous

12:08.800 --> 12:10.960
because we're not anticipating it?

12:10.960 --> 12:14.480
Are there unknown unknowns here?

12:14.480 --> 12:17.560
Well, usually there are unknown unknowns.

12:17.560 --> 12:20.920
I focused largely on catastrophic risk

12:20.920 --> 12:23.320
and large-scale loss of human life.

12:23.320 --> 12:27.440
I didn't speak about AI well-being very much,

12:27.440 --> 12:30.080
or for instance, that's something that could end up

12:30.080 --> 12:33.680
changing a lot of how we think about wanting to proceed forward

12:33.680 --> 12:38.680
with managing the emergence of digital life

12:39.600 --> 12:42.000
is if they have moral value.

12:42.000 --> 12:47.120
So I think that's something I didn't touch on in the paper,

12:47.120 --> 12:49.320
largely because I think our understanding of it

12:49.320 --> 12:50.600
is very underdeveloped,

12:50.600 --> 12:54.160
and I still think it's a bit too much of a...

12:54.160 --> 12:56.120
It's a bit too much of a taboo topic

12:56.120 --> 12:59.000
such that it just hasn't that much research on it.

12:59.000 --> 13:01.600
Well, let's dig into the first category of risk,

13:01.600 --> 13:03.600
which is malicious use.

13:03.600 --> 13:08.600
So this is a category in which bad actors choose to use AI

13:08.600 --> 13:11.920
in ways that harm humanity.

13:12.520 --> 13:15.280
Recently, there's been a lot of discussion of AIs

13:15.280 --> 13:18.320
helping with bioengineered pandemics.

13:18.320 --> 13:22.800
This has been brought up in the US Senate, I think,

13:22.800 --> 13:26.560
and it's been kind of widely publicized.

13:26.560 --> 13:30.560
How plausible do you think it is that the current

13:30.560 --> 13:33.800
or the next generation of large-language models

13:33.800 --> 13:38.400
could make it easier to create bioengineered viruses?

13:38.400 --> 13:41.360
Yeah, so I think this is actually one of the largest reasons

13:41.360 --> 13:45.640
why I wrote this paper was because during 2022,

13:45.640 --> 13:48.640
when sort of the development of this paper started,

13:48.640 --> 13:49.840
this is this bio thing,

13:49.840 --> 13:52.040
yes, nobody's talking about it.

13:52.040 --> 13:55.160
This, although people will treat malicious use

13:55.160 --> 13:57.840
as a distraction, I don't think that's the case.

13:57.840 --> 14:01.280
There are a catastrophic and existential risk

14:01.280 --> 14:02.760
that can come from malicious use,

14:02.760 --> 14:06.040
so and this threat vector concerns me quite a bit.

14:06.040 --> 14:09.320
So I think it is quite plausible

14:09.360 --> 14:12.880
that if we have an AI system that has something

14:12.880 --> 14:17.560
like a PhD-level understanding of virology,

14:17.560 --> 14:19.360
then it's fairly straightforward

14:20.360 --> 14:24.560
that such a system would provide the knowledge

14:24.560 --> 14:28.080
for synthesizing such a weapon.

14:28.080 --> 14:30.080
The risk analysis is something like,

14:30.080 --> 14:33.000
what's the number of people with the skill and access

14:33.000 --> 14:36.240
to create a biological weapon

14:36.240 --> 14:38.320
that could be civilization destroying?

14:38.320 --> 14:42.280
And what's the sort of the probability

14:42.280 --> 14:44.280
that they're actually wanna do that?

14:44.280 --> 14:48.040
And right now, maybe there are 30,000 virology PhDs

14:48.040 --> 14:52.080
and they just don't really have the incentive to do that.

14:52.080 --> 14:55.920
Meanwhile, if you have that knowledge

14:55.920 --> 14:58.840
in available to anybody who wants to go to,

14:58.840 --> 15:01.160
use Google's chat bot or Meta's chat bot

15:01.160 --> 15:03.900
or Bing's or OpenAI's,

15:04.900 --> 15:08.700
then we can add several zeros to that,

15:08.700 --> 15:13.700
to the number of people with the skill to pull it off

15:13.700 --> 15:15.620
because they could just ask such a system,

15:15.620 --> 15:16.740
how do I make one?

15:16.740 --> 15:18.020
Give me a cookbook.

15:18.020 --> 15:20.660
Now, there'd be guardrails, of course,

15:20.660 --> 15:23.340
but the guardrails are fairly easy to overcome

15:23.340 --> 15:25.980
because these AI systems can easily be jailbroken.

15:25.980 --> 15:27.700
That's like if you can just append

15:27.700 --> 15:29.340
some random garbled string

15:29.340 --> 15:31.580
or some adversarily crafted garbled string

15:31.580 --> 15:34.220
at the end of your request to the chat bot

15:34.220 --> 15:35.060
and then that'll take off.

15:35.060 --> 15:36.900
It's like safety guardrails as a paper

15:36.900 --> 15:40.900
that the center helps with

15:42.420 --> 15:45.780
in creating and discussing adversarial attacks

15:45.780 --> 15:47.300
for large language models.

15:47.300 --> 15:48.620
So now that's a thing.

15:48.620 --> 15:52.460
Or you might use an open source model that's available

15:52.460 --> 15:56.500
and might also be easily stripped of its guardrails.

15:56.500 --> 16:00.660
I don't think the AI developers now with the APIs

16:00.700 --> 16:05.380
have much of a high ground as far as safety goes

16:05.380 --> 16:06.980
when it comes to the malicious use case

16:06.980 --> 16:08.060
for other things like hacking.

16:08.060 --> 16:11.180
There'd be a different story, but that could change.

16:11.180 --> 16:12.500
Maybe they'll add more measures.

16:12.500 --> 16:14.400
Maybe they'll get better filters.

16:14.400 --> 16:18.020
Maybe they will remove some bio-related knowledge

16:18.020 --> 16:21.500
from the pre-training distribution and so on.

16:21.500 --> 16:24.660
But anyway, that would be sufficient for,

16:24.660 --> 16:26.020
if such a thing were to happen,

16:26.020 --> 16:28.100
then there could be a pandemic

16:28.180 --> 16:31.540
that could cause some civilizational discontinuity,

16:31.540 --> 16:33.580
which could be some existential risk.

16:33.580 --> 16:35.940
It'd be difficult for it to kill everybody,

16:35.940 --> 16:38.300
but for toppling civilization.

16:38.300 --> 16:40.500
And it's not clear how that would go

16:40.500 --> 16:42.420
or the quality of that situation.

16:42.420 --> 16:44.620
That's enough for us to worry, I think.

16:44.620 --> 16:47.420
Some of the pushback to this story

16:47.420 --> 16:52.420
of a bio-engineered virus enabled by large language models

16:53.080 --> 16:55.740
is that, well, isn't all of the training data

16:55.740 --> 16:56.900
freely available online?

16:56.940 --> 17:00.740
Couldn't a potential bad actor have gone online,

17:00.740 --> 17:03.340
gotten the data and used it already?

17:03.340 --> 17:06.060
What's the difference between using a search engine

17:06.060 --> 17:08.780
and a large language model?

17:08.780 --> 17:11.940
Sure, so two things.

17:11.940 --> 17:15.800
Even if there is some type of harmful content online,

17:15.800 --> 17:18.940
I don't know why we would want it being propagated.

17:18.940 --> 17:21.180
If the nuclear secrets were online,

17:21.180 --> 17:23.180
I don't know why you'd want that propagated

17:23.180 --> 17:25.540
because your risk increases

17:25.540 --> 17:28.040
based on the ease of access to these.

17:28.040 --> 17:29.540
But in the case of bio-weapons,

17:29.540 --> 17:31.900
yes, there are some bio-weapons

17:31.900 --> 17:34.720
that are not civilization-destroying available online.

17:34.720 --> 17:38.500
The ones that would be potentially civilization-destroying,

17:38.500 --> 17:41.220
though, would require a bit more thinking.

17:42.700 --> 17:44.740
So there could be several,

17:44.740 --> 17:45.860
or there could be many people killed

17:45.860 --> 17:47.100
as a consequence of these, though,

17:47.100 --> 17:52.100
but not at a societal scale risk, necessarily.

17:52.220 --> 17:54.740
So I think that's a relevant difference.

17:54.740 --> 17:59.220
Many of the extremely dangerous pathogens,

17:59.220 --> 18:02.140
fortunately, virology people are not writing those up

18:02.140 --> 18:03.460
and posting those on Twitter,

18:03.460 --> 18:05.460
and then all you got to do is search for them.

18:05.460 --> 18:09.140
This isn't, that's not actually the type of information.

18:09.140 --> 18:10.820
For other types of information,

18:10.820 --> 18:14.140
like how to tips for breaking the law

18:14.140 --> 18:15.940
or how to wire a car,

18:15.940 --> 18:18.780
this sort of stuff is online and generic,

18:18.780 --> 18:20.780
a cookbooks for some generic,

18:20.780 --> 18:22.220
smaller-scale bio-weapons, sure,

18:22.220 --> 18:23.900
but not civilization-destroying.

18:23.900 --> 18:28.700
And how is the guide for creating a civilization-destroying

18:28.700 --> 18:30.860
virus in the large-vanguage model

18:30.860 --> 18:33.580
if it's not online, in the data online?

18:33.580 --> 18:36.820
So I am not saying that the current ones

18:36.820 --> 18:38.460
have this in their capacity.

18:38.460 --> 18:42.860
I'm saying that when they have like a PhD-level knowledge

18:42.860 --> 18:46.020
and are able to reflect and do a bit of brainstorming,

18:47.060 --> 18:49.820
then you're in substantially more trouble.

18:49.820 --> 18:52.620
And that could possibly be a model

18:52.620 --> 18:55.940
on the order of like GPT-5 or 5.5,

18:55.940 --> 18:57.340
it may be within its capacity.

18:57.340 --> 19:00.020
So there you don't need agent, like AI,

19:00.020 --> 19:03.260
you would just need a very knowledgeable chatbot

19:03.260 --> 19:06.540
for that threat to potentially manifest.

19:06.540 --> 19:08.100
So there's quite a bit we'll need to do

19:08.100 --> 19:13.100
to, in technical research and in policy,

19:14.660 --> 19:17.140
for reducing that specific risk.

19:17.140 --> 19:20.260
Another rescue you mentioned under malicious use

19:20.300 --> 19:22.500
is this issue of AI agents,

19:22.500 --> 19:26.380
which they are perhaps a bit analogous to viruses

19:26.380 --> 19:30.020
in the sense that they might be able to spread online

19:30.020 --> 19:35.020
and replicate themselves and cause harm.

19:35.060 --> 19:39.020
What do you worry about most with AI agents?

19:39.020 --> 19:42.060
I'm emphasizing, and since there are many forms

19:42.060 --> 19:43.340
of malicious use, in this paper,

19:43.340 --> 19:46.580
I'm mainly emphasizing ones that could be catastrophic

19:46.580 --> 19:48.460
or existential.

19:48.460 --> 19:52.140
So in this case, you could imagine people

19:52.140 --> 19:56.580
unleashing rogue AI systems to just destroy humanity.

19:56.580 --> 19:58.100
That could be their objective.

19:58.100 --> 20:00.020
And that would be extremely dangerous.

20:00.020 --> 20:03.420
So you don't need power-seeking arguments

20:03.420 --> 20:06.260
or these claims that, oh, by default,

20:06.260 --> 20:08.340
they will have a will to power.

20:08.340 --> 20:09.500
You don't need any of that.

20:09.500 --> 20:14.140
You just need to assume that if enough people have access,

20:14.140 --> 20:17.380
and if some person is omnicidal,

20:17.380 --> 20:21.780
or thinks in the way that some AI scientists do,

20:21.780 --> 20:24.740
that we need to bring about the next stage

20:24.740 --> 20:28.900
of cosmic evolution, and that resistance is futile

20:28.900 --> 20:30.140
to quote Richard Sutton,

20:30.140 --> 20:32.100
the author of the Reinforcement Learning textbook,

20:32.100 --> 20:34.820
and that we should bow out when it behooves us.

20:38.860 --> 20:42.260
There are many people who would have an inclination

20:42.260 --> 20:44.140
for building, not saying Richard Sutton

20:44.140 --> 20:47.980
would specifically give the AI system of destroy humanity,

20:47.980 --> 20:51.460
but doesn't seem to say too much against that prospect.

20:51.460 --> 20:55.140
So that's another example of malicious use

20:55.140 --> 20:58.140
that could be catastrophic or existential.

20:58.140 --> 21:02.140
And how close do you think we are to AI agents

21:02.140 --> 21:03.180
that actually work?

21:03.180 --> 21:08.180
We had someone set up a Chaos GPT early on

21:08.940 --> 21:12.460
when GPT was released, but it got stuck in some loops

21:12.460 --> 21:14.900
and it couldn't actually do anything,

21:14.900 --> 21:18.020
even if it was imbued with bad motives.

21:18.020 --> 21:21.940
When would you expect agents to actually be capable

21:21.940 --> 21:23.340
and therefore dangerous?

21:23.340 --> 21:25.380
Yeah, so I think that their capability

21:25.380 --> 21:27.900
would be a continuous thing in the same way,

21:27.900 --> 21:30.180
like when are they good at generating text?

21:30.180 --> 21:32.780
It's like, well, you know, it kind of started in GB2, GB3,

21:32.780 --> 21:37.780
and so I might anticipate great strides in AI agents next year,

21:38.780 --> 21:42.740
where we can give it some basic short tasks,

21:44.740 --> 21:47.940
like help me make this like PowerPoint or something.

21:47.940 --> 21:49.340
It's not gonna do the whole thing,

21:49.340 --> 21:51.940
but it can help with things like that

21:51.940 --> 21:55.380
or browsing around on the internet for you more.

21:55.380 --> 21:59.540
So I think those capabilities will keep coming

21:59.540 --> 22:02.420
for it to pose a substantial risk.

22:02.420 --> 22:05.020
There's a variety of things that could do

22:05.780 --> 22:09.780
it could threaten, for instance, mutually assured destruction

22:09.780 --> 22:13.780
with humanity by saying, I will make this bio weapon,

22:13.780 --> 22:16.220
that will destroy all of you, and I'll take you down with me,

22:16.220 --> 22:18.940
unless you comply with some types of demands.

22:18.940 --> 22:20.020
That could work.

22:20.020 --> 22:21.780
If they're good at hacking,

22:21.780 --> 22:24.380
then they could potentially amass a lot of resources

22:24.380 --> 22:28.780
by scamming people or by stealing cryptocurrency.

22:29.780 --> 22:32.780
There's a variety, they could do that,

22:32.780 --> 22:35.420
there's a variety, they could, of course,

22:35.420 --> 22:40.180
tap into lots of different sensors to manipulate people

22:40.180 --> 22:42.300
or influence public discourse.

22:43.620 --> 22:45.940
They wouldn't necessarily need to be embodied

22:45.940 --> 22:48.420
for this type of thing to happen.

22:48.420 --> 22:51.300
If we're in a later stage of AI development

22:51.300 --> 22:53.580
where we have a lot of weaponized AI systems

22:53.580 --> 22:55.340
and then hacking those systems will, of course,

22:55.340 --> 22:57.140
be substantially more concerning,

22:57.140 --> 22:59.740
or if those systems get repurposed

22:59.740 --> 23:01.860
maliciously to weaponized AI systems.

23:01.860 --> 23:05.660
So it becomes a lot easier as time progresses.

23:05.660 --> 23:08.380
The AIs don't need to be particularly power seeking

23:08.380 --> 23:13.380
on this view, though, to have this potential for catastrophe

23:14.860 --> 23:19.020
because humanity will basically give them that power by default.

23:20.140 --> 23:22.100
They will keep weaponizing them,

23:22.100 --> 23:25.740
they will integrate them into more and more critical decisions.

23:25.740 --> 23:29.660
They will let them move around money

23:29.700 --> 23:31.740
and complete transactions

23:31.740 --> 23:34.100
and they'll give them a looser and looser leash.

23:34.100 --> 23:38.580
So as time goes on, the potential for rogue AI

23:38.580 --> 23:41.220
or for deliberately, AI systems

23:41.220 --> 23:43.140
that are deliberately instructed to cause harm

23:43.140 --> 23:46.620
would be, the potential impact or severity

23:46.620 --> 23:47.580
would keep increasing.

23:47.580 --> 23:50.420
Yeah, I think maybe it's worth mentioning here

23:50.420 --> 23:55.420
just the continuous costs of traditional computer viruses

23:56.300 --> 24:01.100
which are costly and which we've gotten better

24:01.100 --> 24:03.820
at handling those as a civilization,

24:03.820 --> 24:08.340
but we still haven't defeated traditional conventional viruses

24:08.340 --> 24:12.500
which are very dumb compared to what AI agents could be.

24:12.500 --> 24:16.460
So we can imagine a computer virus

24:16.460 --> 24:18.380
equipped with more intelligence

24:18.380 --> 24:21.140
and how would you as a person,

24:21.140 --> 24:23.700
I'm not saying AI agents will be necessarily

24:23.700 --> 24:25.580
as smart as people soon,

24:25.580 --> 24:28.620
but how would you do the kind of hacking

24:28.620 --> 24:30.780
that the agent might be interested in?

24:30.780 --> 24:32.260
It's interesting to consider at least

24:32.260 --> 24:36.620
that we haven't been able to squash out conventional viruses.

24:36.620 --> 24:39.260
Yeah, they could exaltrate their information

24:39.260 --> 24:42.380
onto different servers or less protected ones

24:42.380 --> 24:46.420
and then use those to proliferate themselves even further.

24:46.420 --> 24:51.300
So they'll be a very distinct adversary

24:51.340 --> 24:55.700
with many, many options at their disposal for causing harm.

24:55.700 --> 24:59.780
Yeah, one thing I worry about is whether the tools

24:59.780 --> 25:03.700
and techniques we'll need at an institutional level

25:03.700 --> 25:07.660
to handle malicious use will also enable governments

25:07.660 --> 25:10.900
to become a totalitarian basically,

25:10.900 --> 25:15.900
to exercise too great a level of control over citizens

25:17.180 --> 25:18.620
who have done nothing wrong.

25:18.620 --> 25:23.580
So what is required to prevent the large language models

25:23.580 --> 25:26.100
that could become AI agents

25:26.100 --> 25:28.020
and could be used to create viruses?

25:28.020 --> 25:32.500
What techniques are available for preventing them

25:32.500 --> 25:36.420
being used in such ways without enabling

25:36.420 --> 25:39.380
kind of too much state power?

25:39.380 --> 25:42.260
Yeah, I think this is definitely a tension

25:42.260 --> 25:45.060
where to counteract these risks from rogue,

25:45.060 --> 25:46.860
lone wolf actors,

25:46.860 --> 25:50.980
then people would want the technology centralized.

25:52.500 --> 25:54.500
This would be a similarity with nuclear weapons,

25:54.500 --> 25:57.700
for instance, where we didn't want everybody

25:57.700 --> 26:00.260
being able to make nuclear weapons.

26:00.260 --> 26:02.500
We wanted to keep control of uranium.

26:02.500 --> 26:07.500
And so what happened was we had a no first use

26:07.500 --> 26:10.140
plus non-proliferation regime

26:10.140 --> 26:15.140
and that kept the power in a few different peoples' hands.

26:16.180 --> 26:17.260
I think there are things we could do

26:17.260 --> 26:20.220
to reduce these sorts of risks

26:20.220 --> 26:23.020
by creating institutions that are more democratic.

26:23.020 --> 26:24.220
I think that seems useful.

26:24.220 --> 26:27.460
I think decoupling the organizations

26:27.460 --> 26:30.980
that has some of the most powerful AI systems,

26:30.980 --> 26:35.980
having those more decoupled from the militaries

26:36.260 --> 26:37.620
would be fairly useful

26:37.620 --> 26:40.220
so that if something gets out of hand with line,

26:40.220 --> 26:44.060
if they're linked and if we're needing to pull the plug

26:44.060 --> 26:45.700
on these AI systems,

26:45.700 --> 26:47.100
this isn't like taking down the military.

26:47.100 --> 26:51.540
I think just separating this sort of cognitive labor

26:51.540 --> 26:55.140
and or labor generally, automated labor

26:55.140 --> 27:00.140
from a physical force would be fairly useful.

27:01.260 --> 27:03.340
But I think largely it's creating democratic,

27:03.340 --> 27:05.700
a democratic institutions is one of these measures.

27:05.700 --> 27:08.180
In the case of dealing with rogue AIs

27:08.180 --> 27:12.660
that are people maliciously instructed rogue AIs

27:12.660 --> 27:14.660
that are proliferating across the internet,

27:14.660 --> 27:16.020
I think there'd be other types of things

27:16.020 --> 27:21.020
like legal liability laws for cloud providers,

27:21.980 --> 27:26.980
that if you are running an unverified or unsafe AI system

27:30.020 --> 27:33.180
on your cloud or on your compute,

27:33.180 --> 27:34.980
then you get in trouble.

27:34.980 --> 27:37.700
This would create incentives for them to keep track of it

27:37.700 --> 27:41.060
instead of just doling out compute to whoever's paying.

27:41.060 --> 27:43.060
So that's sort of like having an incentive

27:43.060 --> 27:45.340
for off switches all over.

27:45.340 --> 27:49.100
So there's a variety of different things

27:49.100 --> 27:54.100
we could be doing to strike this balance

27:55.620 --> 27:57.860
by reducing these malicious use risks.

27:57.860 --> 27:59.660
I mean, also, as you mentioned,

27:59.660 --> 28:00.780
some of these malicious use risks

28:00.780 --> 28:03.060
don't require this type of centralization

28:03.060 --> 28:04.060
or nearly as much.

28:04.060 --> 28:06.060
We can do various things to reduce this risk

28:06.060 --> 28:09.500
without giving tons of power to states.

28:09.500 --> 28:13.260
For instance, we invest in personal protective equipment

28:13.260 --> 28:18.260
or monitoring waterways for early signs of some pathogens.

28:19.820 --> 28:22.580
I mean, there's the traditional stuff we can do

28:22.580 --> 28:24.780
to reduce risks from pandemics, for instance,

28:24.780 --> 28:27.540
which would reduce our exposure to the risk

28:27.540 --> 28:30.580
of AI-facilitated pandemics.

28:30.580 --> 28:34.500
So not all interventions for reducing malicious use

28:34.500 --> 28:36.580
require more centralization.

28:36.580 --> 28:41.020
I would imagine that we probably wouldn't want

28:41.020 --> 28:44.060
in the long term, like say it's like 2040

28:44.060 --> 28:47.140
or something like that, we wouldn't want anybody,

28:47.140 --> 28:49.380
anywhere being able just to ask the AI system

28:49.380 --> 28:52.420
how to make a pandemic or being able to unleash it

28:52.420 --> 28:55.340
to try and take over the world.

28:55.340 --> 28:58.340
This doesn't seem like a good idea.

28:58.340 --> 29:00.860
There'd be other types of things like structured access

29:00.860 --> 29:02.580
where for these bio capabilities,

29:02.580 --> 29:05.460
you just give people who are doing medical research

29:05.460 --> 29:07.380
access to the specific bio capabilities.

29:07.380 --> 29:09.580
But other people, they don't really have much of a reason

29:09.580 --> 29:11.660
for it, so they don't get that advanced,

29:11.660 --> 29:13.300
they don't get models with that advanced knowledge.

29:13.300 --> 29:15.180
So I think there are some simple restrictions

29:15.180 --> 29:17.860
that we can do that can take care of a large chunk

29:17.860 --> 29:21.780
of the risk without needing to hand over the technology

29:21.780 --> 29:23.660
to like militaries, and then they're the only ones

29:23.660 --> 29:24.500
who have it.

29:24.500 --> 29:28.260
You mentioned legal liabilities for cloud providers

29:28.260 --> 29:30.020
and maybe companies in general.

29:30.020 --> 29:33.140
I wonder if this might be a way to have a form

29:33.140 --> 29:38.140
of decentralized control over AI agents

29:39.620 --> 29:42.820
or over large language models or generative models,

29:42.820 --> 29:46.940
AI in general, by having the state provide a framework

29:46.940 --> 29:51.060
for where you can get fined for trespassing

29:51.100 --> 29:54.340
some boundaries, but then having companies

29:54.340 --> 29:56.580
implement exactly how that works,

29:56.580 --> 30:01.180
use technical tools in order to reduce their risk of fines

30:01.180 --> 30:05.180
and maybe we can find a good balance there

30:05.180 --> 30:08.500
where we weigh the costs and benefits.

30:08.500 --> 30:13.500
I think that liability laws help fix the problem

30:13.660 --> 30:15.820
of externalities quite a bit,

30:15.820 --> 30:18.060
where they're imposing risks on others

30:18.060 --> 30:23.060
that have no, shouldn't have any risk imposed on them

30:23.620 --> 30:26.060
because they're not privy to the decisions

30:26.060 --> 30:28.260
or there's an issue with that though,

30:28.260 --> 30:30.700
which is that there's only so many externalities

30:30.700 --> 30:33.820
that some of these organizations could internalize though

30:33.820 --> 30:34.900
with liability law.

30:34.900 --> 30:38.340
If somebody creates a pandemic

30:38.340 --> 30:40.380
as a consequence of their AI system,

30:40.380 --> 30:41.700
you could sue that company,

30:41.700 --> 30:44.180
but they're not gonna be able to pay off

30:44.180 --> 30:47.340
the destruction of civilization with their capital.

30:47.340 --> 30:49.300
So there's quite a limit to it.

30:49.300 --> 30:51.380
It can help fix the incentives,

30:51.380 --> 30:54.220
but it still doesn't fix them entirely

30:54.220 --> 30:56.780
because it's not particularly,

30:56.780 --> 30:58.540
when certainly can't internalize like downfall

30:58.540 --> 31:00.620
of like civilization as an organization

31:00.620 --> 31:02.700
and like foot the bill for that.

31:02.700 --> 31:05.500
And then the extinction of the human race is also,

31:05.500 --> 31:09.180
I don't think that's the thing you could settle in court.

31:09.180 --> 31:11.740
What about requiring insurance?

31:11.740 --> 31:13.580
So this is an idea that has been discussed

31:13.580 --> 31:17.820
for advanced biological research,

31:17.820 --> 31:20.860
gain a function research with viruses, for example.

31:20.860 --> 31:23.060
Maybe such a thing could also work

31:23.060 --> 31:27.940
for risky experiments with advanced AI.

31:27.940 --> 31:30.420
It depends if the harms are localized.

31:30.420 --> 31:33.900
I think insurance and this taming of typical,

31:33.900 --> 31:36.380
not long tail, not black swan type of uncertainty,

31:36.380 --> 31:39.580
but thin tailed type of uncertainty

31:39.580 --> 31:41.820
makes sense when risks are more localized,

31:41.820 --> 31:46.260
but when we are dealing with risks that are scalable

31:47.700 --> 31:49.900
and can bring down the entire system,

31:50.780 --> 31:54.700
then I think a lot of the incentives

31:54.700 --> 31:56.980
for insurance don't make as much sense.

31:56.980 --> 31:59.900
So you basically need like some law of large numbers

31:59.900 --> 32:02.180
and many types of insurance to like kick in

32:03.020 --> 32:06.020
to sort of have that risk diversified away.

32:06.020 --> 32:09.460
But if the entire system has exposure to that risk,

32:10.140 --> 32:12.300
there's not another system to diversify it.

32:12.300 --> 32:13.660
Maybe you could paint us a picture

32:13.660 --> 32:15.460
of a positive vision here.

32:15.460 --> 32:18.220
So say we get to 2050 and we've worked this out,

32:18.220 --> 32:20.140
what does the world look like in a world

32:20.140 --> 32:22.900
where we control malicious AI?

32:22.900 --> 32:26.100
I think if people have access to these AI systems,

32:26.100 --> 32:30.500
they're subject to, and they have many of their capabilities,

32:30.500 --> 32:31.820
there are of course restrictions on them,

32:31.820 --> 32:35.340
like you can't use them to break the law.

32:35.340 --> 32:37.500
So a lot of these most dangerous capabilities,

32:37.500 --> 32:40.660
nobody's really able to use them in that way.

32:40.660 --> 32:45.660
If there is a need for, in the case of like defense,

32:46.020 --> 32:47.820
they would end up using like AIs

32:47.820 --> 32:50.020
for things like hacking and whatnot.

32:50.020 --> 32:53.860
And that would, like they would have access

32:53.860 --> 32:54.860
to that type of technology,

32:54.860 --> 32:58.700
but it wouldn't be the case that any angsty teenager

32:58.700 --> 33:01.100
can just download a model online

33:01.100 --> 33:03.500
and then they instruct it to take down

33:03.500 --> 33:04.420
some critical infrastructure.

33:04.420 --> 33:06.220
This just isn't a possibility.

33:07.220 --> 33:09.500
It's very much trying to strike a balance with that.

33:09.500 --> 33:11.140
I would hope that we would also have

33:11.140 --> 33:12.460
these most powerful AI systems

33:12.460 --> 33:15.380
that do carry more of this force,

33:15.380 --> 33:17.660
that have some of these more dangerous capabilities

33:17.660 --> 33:20.300
are subject to democratic control,

33:20.300 --> 33:24.620
so that power is not as centralized.

33:24.620 --> 33:27.180
And that also I think reduces like the risk of like,

33:27.180 --> 33:30.260
put in quote, like lock in risks as well,

33:30.260 --> 33:34.460
where some individual group can impose their values

33:34.460 --> 33:36.660
and entrench them.

33:36.660 --> 33:39.860
So at least those are some properties

33:39.860 --> 33:42.540
of a positive future.

33:44.020 --> 33:47.420
So I don't think it looks like complete mass proliferation

33:47.420 --> 33:50.420
of extremely dangerous AI products.

33:50.420 --> 33:53.420
And I don't think it looks like only one group,

33:53.420 --> 33:56.700
one elite aristocrat group gets to make the decisions

33:56.700 --> 33:59.220
for humanity either.

33:59.220 --> 34:01.700
So there's different levels of access

34:01.700 --> 34:03.300
to different levels of lethality,

34:04.300 --> 34:08.340
and to empower depending on whether it makes sense.

34:08.340 --> 34:11.380
But the highest level institutions are still democratic.

34:11.380 --> 34:14.060
Another category of risks that you discuss

34:14.060 --> 34:17.100
is the possibility of an AI race.

34:17.100 --> 34:18.700
Now, we've done another episode

34:18.700 --> 34:21.300
where we talked about evolutionary pressures

34:21.300 --> 34:25.580
and how they work between corporations

34:25.580 --> 34:28.340
and how they might lead to a situation

34:28.340 --> 34:31.780
which humanity is gradually disempowered.

34:31.780 --> 34:35.060
But I think one thing we could discuss here in this episode

34:35.060 --> 34:39.100
is the possibility of a military AI race.

34:39.100 --> 34:42.260
What do you think a military AI race looks like?

34:42.260 --> 34:45.620
To recap, we were just at the malicious use one.

34:45.620 --> 34:48.540
And so now the other risk category would be like racing dynamics

34:48.540 --> 34:51.180
or competitive pressures or collective action problems.

34:51.180 --> 34:53.140
This is that structural environmental risk

34:53.140 --> 34:56.300
that when we were referring to the categories way earlier.

34:56.300 --> 34:58.340
Yeah, I think with the corporate race,

34:58.340 --> 35:01.940
obviously there's, as we discussed in the previous episode,

35:01.940 --> 35:03.900
there's them cutting corners on safety

35:03.900 --> 35:06.700
and this is largely what AI development is driven by.

35:06.700 --> 35:08.460
A lot of these organizations will start

35:08.460 --> 35:10.740
as having a very strong safety bent,

35:10.740 --> 35:13.420
but then they're basically gonna be pressured

35:13.420 --> 35:16.340
into just racing and prioritizing the profit

35:16.340 --> 35:18.340
and developing these things as quickly as possible

35:18.340 --> 35:20.620
and staying competitive over their safety.

35:20.620 --> 35:23.660
This is sort of the dynamic that basically drives

35:23.660 --> 35:24.860
pretty much all these AI companies.

35:24.860 --> 35:26.620
And I don't think actually in the presence

35:26.620 --> 35:27.940
of these intense competitive pressures

35:27.940 --> 35:30.220
that intentions particularly matter.

35:30.220 --> 35:35.220
So I think basically this is the main force to look at

35:36.780 --> 35:39.700
when trying to explain a major developments of AI,

35:39.700 --> 35:42.620
why are companies acting the way they are?

35:42.620 --> 35:46.620
It can be very well approximated by them

35:46.620 --> 35:50.260
just trying to, by them succumbing to competitive pressures

35:50.260 --> 35:53.020
or defecting in this broader collective action problem

35:53.020 --> 35:54.300
of should we slow down

35:54.300 --> 35:57.100
and should we proceed more prudently

35:57.100 --> 35:59.100
and invest more in safety

35:59.100 --> 36:01.100
and try and make sure our institutions are caught up

36:01.100 --> 36:05.580
or should we race ahead so that way we can continue

36:05.580 --> 36:07.060
being in the lead because one day

36:07.060 --> 36:09.580
we'll maybe be more responsible with this technology.

36:09.580 --> 36:11.900
I'm concerned, as mentioned in that previous episode

36:11.900 --> 36:15.700
of that leading us to like a state of substantial dependence

36:15.700 --> 36:18.060
and losing effective control,

36:18.060 --> 36:19.660
you can imagine similar dynamic happening

36:19.660 --> 36:23.340
with the military just like if we don't want,

36:23.420 --> 36:26.260
arrows for instance, you're not gonna roll back arrows.

36:26.260 --> 36:27.460
And so when you start going down the road

36:27.460 --> 36:30.140
of weaponizing AI systems,

36:30.140 --> 36:32.500
if they're more potent and cheaper

36:32.500 --> 36:34.180
and more generally capable

36:34.180 --> 36:35.780
and more politically convenient

36:35.780 --> 36:39.420
and sending human soldiers onto the battlefield,

36:39.420 --> 36:44.420
then this becomes a very difficult process to reverse back.

36:45.500 --> 36:49.580
Eventually what happens is you've had an on ramp

36:49.580 --> 36:53.140
to many more potential catastrophic risks.

36:53.140 --> 36:55.620
You've transferred much of the lethal power.

36:55.620 --> 36:59.500
In fact, the main source is the lethal power to AI systems.

36:59.500 --> 37:01.660
And then you're hoping that they're reliable enough

37:01.660 --> 37:03.140
and that you've sufficiently,

37:03.140 --> 37:05.300
you can keep them under sufficient control

37:05.300 --> 37:06.820
and that they can do your bidding.

37:06.820 --> 37:10.700
Even if you do get them highly reliable

37:10.700 --> 37:13.940
and they do what you instruct them to do,

37:13.940 --> 37:16.460
this doesn't make people overall very safe.

37:16.460 --> 37:18.460
We saw with the Cuban Missile Crisis,

37:18.460 --> 37:20.620
we can definitely, nukes don't turn on us.

37:20.620 --> 37:22.140
They don't go off and pursue their own goals

37:22.140 --> 37:22.980
or something like that.

37:22.980 --> 37:27.020
They do what we want them to do,

37:27.020 --> 37:29.540
but collectively do this structural,

37:29.540 --> 37:32.700
environmental game theoretic situation

37:32.700 --> 37:34.420
where like, wow, we would all be better off

37:34.420 --> 37:35.620
without nuclear weapons,

37:35.620 --> 37:40.500
but it makes sense for us each individually to stockpile them.

37:40.500 --> 37:43.180
We put the broader world at larger collective risks.

37:43.180 --> 37:45.820
So like in the Cuban Missile Crisis,

37:45.820 --> 37:48.380
JFK said we had up to like a half

37:48.380 --> 37:50.540
or like a 50% chance of extinction in that event.

37:50.540 --> 37:52.260
It was a very close call

37:52.260 --> 37:55.380
because we almost got a nuclear exchange with that.

37:55.380 --> 37:58.300
And likewise with AI systems, they may be more powerful.

37:58.300 --> 38:00.220
They may be better at facilitating the development

38:00.220 --> 38:01.660
of new weapons too.

38:01.660 --> 38:06.220
And this could also bring us at a risk

38:06.220 --> 38:07.980
where bring us in a situation

38:07.980 --> 38:12.700
where we could potentially destroy ourselves again.

38:12.700 --> 38:16.500
What's pernicious about this structural

38:16.500 --> 38:18.860
or environmental constraint

38:18.860 --> 38:22.900
where we've got different parties, in this case,

38:22.900 --> 38:27.220
militaries competing against each other is the following.

38:27.220 --> 38:30.140
Even if we convince the world

38:30.140 --> 38:33.460
that like the existential risk from AI is like 5%

38:33.460 --> 38:36.220
because let's say they're not reliable.

38:36.220 --> 38:38.100
We can't reliably control them.

38:38.100 --> 38:40.060
So maybe there's a 5% chance to like turn on us

38:40.060 --> 38:41.060
or we lose control of them

38:41.060 --> 38:43.580
and then we become a second class species or exterminate them.

38:43.580 --> 38:44.540
Even if that's the case,

38:44.540 --> 38:47.780
it may make sense for these militaries to go along with it.

38:47.780 --> 38:50.100
Just like, I mean, they swallowed the risk

38:50.100 --> 38:52.980
of potential nuclear Armageddon

38:52.980 --> 38:55.860
by creating these nuclear weapons in the first place.

38:55.860 --> 38:58.820
But they thought if we don't create these nuclear weapons,

38:58.820 --> 39:00.700
then we will certainly be destroyed.

39:00.700 --> 39:02.420
So there's certainty of destruction

39:02.420 --> 39:06.580
versus a small chance of destruction.

39:06.580 --> 39:08.540
And I think they'd be willing to make that trade off.

39:08.540 --> 39:11.380
So this is how there could be an existential risk

39:11.380 --> 39:15.140
to all of humanity based on these structural conditions.

39:15.900 --> 39:18.420
So it's not enough to convince the world

39:18.420 --> 39:20.420
that existential risk is high

39:20.420 --> 39:24.980
because they might just, okay, well, yeah, that's 5%.

39:24.980 --> 39:26.900
Okay, we're gonna have to go with that rational left thing.

39:26.900 --> 39:30.420
It makes rational sense for us to engage in this,

39:30.420 --> 39:32.420
what would normally be very risky behavior

39:32.420 --> 39:34.140
because we don't have a better choice.

39:34.140 --> 39:37.060
So this is why I don't think it makes sense

39:37.060 --> 39:39.380
just to hammer home the point that, wow,

39:39.380 --> 39:40.820
these AIs could turn on us

39:40.820 --> 39:42.780
or we could lose control of them.

39:42.780 --> 39:44.180
There's this structural thing of like,

39:44.180 --> 39:46.340
that's not gonna matter unless that probability

39:46.340 --> 39:47.220
is like very high.

39:47.220 --> 39:50.140
Like maybe if it's like 30% and they go, okay, all right,

39:50.140 --> 39:52.180
we're not gonna build the thing because,

39:52.180 --> 39:53.980
but if it's something like 5%,

39:53.980 --> 39:55.140
they might go through with it anyway.

39:55.140 --> 40:00.140
So more than just concerns about single AI agents

40:00.620 --> 40:02.860
make sense or make sense to focus on,

40:02.860 --> 40:05.060
we have to focus on these multi-agent dynamics,

40:05.060 --> 40:06.580
these competitive pressures,

40:06.580 --> 40:11.100
the sort of the game theory of what they're facing.

40:11.100 --> 40:15.260
And so I think that if you don't resolve that,

40:15.260 --> 40:17.940
you're basically exposed to insensitivity

40:17.940 --> 40:22.620
to a lot of existential risk up to maybe 5% or 10%,

40:22.620 --> 40:25.020
which maybe it's possible.

40:25.020 --> 40:27.380
Maybe it's actually only 2%.

40:27.380 --> 40:28.580
And when you convince the world,

40:28.580 --> 40:30.180
everybody's very educated about it.

40:30.180 --> 40:33.060
Everybody listens to Future of Life podcast tomorrow

40:33.060 --> 40:34.740
and they all go, wow, this is a concern.

40:34.740 --> 40:35.900
I am updated to 5%.

40:35.900 --> 40:37.020
Won't matter.

40:37.020 --> 40:38.700
It won't stop that type of dynamic from happening.

40:38.740 --> 40:41.300
You have to fix the international coordination issue.

40:41.300 --> 40:43.460
You have to avoid this sort of potential

40:43.460 --> 40:44.820
for World War III thing.

40:44.820 --> 40:46.940
Now it didn't directly cause it,

40:46.940 --> 40:48.740
as we were discussing earlier.

40:48.740 --> 40:50.580
This wasn't a direct cause of extinction,

40:50.580 --> 40:52.460
but it increased the probability substantially.

40:52.460 --> 40:54.420
That's the sort of framing we have to focus on

40:54.420 --> 40:56.140
in trying to reduce existential risk,

40:56.140 --> 40:57.980
not search for direct cause of mechanisms,

40:57.980 --> 40:59.260
but look at these diffuse effects

40:59.260 --> 41:00.820
and structural conditions.

41:00.820 --> 41:03.740
Yeah, so concretely, this might look like

41:03.740 --> 41:07.460
the US is considering implementing AI systems

41:07.500 --> 41:10.980
into their nuclear command and control systems.

41:10.980 --> 41:12.740
So specifically, they're doing this

41:12.740 --> 41:17.180
to counteract the rumors of other countries

41:17.180 --> 41:18.500
doing the same thing.

41:18.500 --> 41:20.220
And in order to act quickly enough

41:20.220 --> 41:21.580
with their nuclear weapons,

41:21.580 --> 41:26.580
they think they need to give AI a greater degree

41:26.980 --> 41:31.100
of control over these nuclear weapons.

41:31.100 --> 41:35.300
And so you have a situation in which

41:35.300 --> 41:39.900
countries are responding to the actions of each other

41:39.900 --> 41:44.900
in a way that accelerates risks from both sides

41:45.580 --> 41:47.180
in this innocent.

41:47.180 --> 41:48.020
There'd be one.

41:48.020 --> 41:51.980
I mean, there are other ways this can affect warfare.

41:51.980 --> 41:56.420
It could maybe be better at doing anomaly detection

41:56.420 --> 41:58.660
thereby identify nuclear submarines

41:58.660 --> 42:01.140
and affect the nuclear triad that way.

42:01.140 --> 42:02.780
Or in later stages,

42:02.780 --> 42:06.460
they just have massive fleets of AI.

42:06.460 --> 42:08.300
And this is saying robot, sorry to say,

42:08.300 --> 42:10.220
but like later stage,

42:10.220 --> 42:12.060
if they're much cheaper to produce,

42:12.060 --> 42:14.100
they'd be very good combatants.

42:14.100 --> 42:15.740
There isn't skin in the game.

42:15.740 --> 42:17.380
This increases the,

42:17.380 --> 42:19.860
this makes it more feasible to get into conflict.

42:19.860 --> 42:20.940
There are other ways in which

42:20.940 --> 42:22.660
this increases the probability of conflict too.

42:22.660 --> 42:23.620
There's more uncertainty

42:23.620 --> 42:26.180
about where your competitors are relative to you.

42:26.180 --> 42:28.140
Maybe they had an algorithmic breakthrough.

42:28.140 --> 42:31.260
Maybe they could actually catch up really quickly

42:31.260 --> 42:34.140
or surpass us by finding some algorithmic breakthrough.

42:34.140 --> 42:36.580
This creates severe or extreme uncertainty

42:36.580 --> 42:39.260
about the capabilities profile of adversaries.

42:39.260 --> 42:41.340
This lack of information about that

42:41.340 --> 42:45.700
increases the chance of conflict as well.

42:45.700 --> 42:49.220
It may also increase first strike advantage substantially too,

42:49.220 --> 42:52.540
which would also increase the probability of conflict.

42:52.540 --> 42:54.660
Like we have an AI system today,

42:54.660 --> 42:56.860
it's much more powerful than anything else.

42:56.860 --> 42:58.020
They might get theirs tomorrow.

42:58.020 --> 43:01.020
If we act today, then we can squash them.

43:01.500 --> 43:06.420
That could get the ball rolling for some global catastrophe.

43:06.420 --> 43:11.420
So yeah, pretty pernicious dynamics overall.

43:12.220 --> 43:14.620
But all of these can be viewed as

43:14.620 --> 43:17.180
competitive pressures driving AI systems

43:17.180 --> 43:21.180
and propagating throughout all aspects of life.

43:21.180 --> 43:25.900
We mentioned through the public sphere in the economy,

43:25.900 --> 43:28.060
people's private lives with AI chatbots,

43:28.060 --> 43:30.020
also in defense, in the military.

43:30.020 --> 43:31.980
It just basically becomes everywhere

43:31.980 --> 43:34.020
and we end up relying more and more on them

43:34.020 --> 43:35.540
to make these sorts of decisions.

43:35.540 --> 43:37.380
And I don't think in many of these,

43:37.380 --> 43:40.980
we become so dependent on them that things move quickly.

43:40.980 --> 43:42.180
We can't actually keep up.

43:42.180 --> 43:44.140
We can't make, if we're actually making these decisions,

43:44.140 --> 43:45.340
we'll make much worse decisions.

43:45.340 --> 43:48.420
So then they basically become in effective control.

43:48.420 --> 43:51.700
Things also move so quickly that the answer to our AI problems

43:51.700 --> 43:54.420
is we need to bring in more AIs

43:54.420 --> 43:56.260
because since they're using more AIs,

43:56.260 --> 43:57.420
now we need to use more AIs.

43:57.420 --> 43:59.500
And so it creates a self-reinforcing feedback loop

43:59.500 --> 44:01.980
which ends up eroding our overall influence

44:01.980 --> 44:03.900
and oversight as to what's going on.

44:03.900 --> 44:05.340
And so I think that's the default one.

44:05.340 --> 44:06.980
So of these sort of risk categories,

44:06.980 --> 44:10.540
I think this seems like straightforwardly the case

44:10.540 --> 44:13.220
if we don't fix international coordination

44:13.220 --> 44:16.660
and if there's a close competition between countries

44:16.660 --> 44:20.900
or if we don't fix the racing dynamics

44:20.900 --> 44:23.580
in the corporate sphere,

44:23.580 --> 44:27.620
then I think it's fairly likely that humanity becomes

44:27.620 --> 44:31.220
at least like a second class species loses control

44:31.220 --> 44:34.180
from there eventually, probably they go extinct,

44:34.180 --> 44:35.660
but that might be a long time after.

44:35.660 --> 44:40.660
But so this is the main risk that I'm worried about

44:41.140 --> 44:42.940
but as Director of Center for AISAD,

44:42.940 --> 44:46.260
I'll try and be acumenical and focus on various others too.

44:46.260 --> 44:47.460
So I'm always making sure that our projects

44:47.460 --> 44:48.300
addressing each of these though,

44:48.300 --> 44:51.140
but personally, this is the one that I'm most concerned about.

44:51.140 --> 44:53.060
So treaties between governments

44:53.060 --> 44:55.180
and some form of collaboration

44:55.220 --> 44:57.620
between the top AI corporations,

44:57.620 --> 44:59.500
is that the way out here?

44:59.500 --> 45:01.340
How do we mitigate this risk?

45:01.340 --> 45:03.340
It seems at the way you describe it,

45:03.340 --> 45:05.660
it seems very difficult to avoid

45:05.660 --> 45:08.140
given the incentives basically.

45:08.140 --> 45:10.220
People respond to incentives,

45:10.220 --> 45:12.140
they rationally respond to incentives.

45:12.140 --> 45:15.140
And so for each step along the way,

45:15.140 --> 45:17.700
they have reasons to do what they're doing.

45:17.700 --> 45:19.620
And so it seems difficult to avoid.

45:19.620 --> 45:21.820
What are our options?

45:21.820 --> 45:26.460
Well, there are positive signs.

45:26.460 --> 45:27.980
For instance, like Henry Kissinger

45:27.980 --> 45:30.260
was recently suggested in foreign affairs

45:30.260 --> 45:33.860
that the US cooperate with China on this issue now,

45:34.860 --> 45:35.940
but before it's too late.

45:35.940 --> 45:38.300
So I think some people are recognizing

45:38.300 --> 45:42.300
the importance of trying to do something about this.

45:42.300 --> 45:45.820
There's, it's possible there'd be some clarifications

45:45.820 --> 45:48.060
about antitrust law, which would make it possible

45:48.060 --> 45:51.460
for AI companies to not engage in excessive competition

45:51.900 --> 45:54.620
over this and put the whole world at risk.

45:55.540 --> 45:59.260
Potentially there could be an international institution

45:59.260 --> 46:04.260
like a CERN for AI, which is the default organization,

46:07.380 --> 46:12.380
which has a broad consortium or coalition of countries

46:13.300 --> 46:16.700
providing input to that and helping steer it.

46:16.700 --> 46:19.380
One that's maybe decoupled from,

46:19.380 --> 46:21.740
to some extent of militaries,

46:22.620 --> 46:24.540
so that we're not having too much power centralized

46:24.540 --> 46:25.380
in one place.

46:25.380 --> 46:26.620
So it doesn't have a monopoly on violence

46:26.620 --> 46:28.820
and eventually after automates a lot of monopoly on labor.

46:28.820 --> 46:31.100
I think that's just like basically all the power

46:31.100 --> 46:31.940
in the world.

46:31.940 --> 46:33.180
So those are possibilities.

46:33.180 --> 46:36.380
I think that the time window might be a bit shorter though.

46:36.380 --> 46:41.020
If there's an arms race and AI arms race in the military,

46:41.020 --> 46:42.940
and if the AI is viewed as like the main thing

46:42.940 --> 46:46.340
to be competing on, like we need to spend a trillion dollars,

46:46.460 --> 46:50.620
we'll spend on that order for nuclear weapons.

46:50.620 --> 46:52.380
If when that becomes the case,

46:52.380 --> 46:54.860
I think it's where we're very much set down that path

46:54.860 --> 46:57.900
and then we're exposed to very substantial risks.

46:57.900 --> 47:01.260
So yeah, I think maybe we'll have a sense

47:01.260 --> 47:04.460
in the next few years as to whether we get some type

47:04.460 --> 47:06.660
of coordination or if we are not gonna recognize

47:06.660 --> 47:08.260
that we're all in the same boat as humans

47:08.260 --> 47:09.580
and we don't want this to happen.

47:09.580 --> 47:12.420
But we'll need people to basically understand what happens

47:12.420 --> 47:13.980
if we go down this route and if we don't try

47:13.980 --> 47:18.140
and fix the payoff matrix, the incentives at the outset,

47:18.140 --> 47:21.820
the structure that these players find themselves in

47:21.820 --> 47:23.860
or that these developers find themselves in.

47:23.860 --> 47:26.980
That looks like a very much a political problem

47:26.980 --> 47:28.300
as it happens.

47:28.300 --> 47:32.220
So this is why making, reducing AI, X risk and whatnot

47:32.220 --> 47:35.740
and making AI safe is a socio-technical problem.

47:35.740 --> 47:39.740
It's not writing down an eight page mathematical solution,

47:39.740 --> 47:41.340
a work of genius and then, oh, okay,

47:41.340 --> 47:43.180
we can all go home now and everything's taken care of.

47:43.180 --> 47:44.820
It's not gonna look like that.

47:44.820 --> 47:47.700
That was a category error in understanding

47:47.700 --> 47:49.260
how to reduce this risk.

47:49.260 --> 47:52.900
We shouldn't have these types of founders effects

47:52.900 --> 47:56.940
have like undue influence over, like it will keep lingering.

47:56.940 --> 47:58.460
I think that will eventually like go away

47:58.460 --> 48:00.060
but I still think it's still like lingering

48:00.060 --> 48:01.860
and I think we should just like move past it

48:01.860 --> 48:04.780
and recognize the complexity of the situation.

48:04.780 --> 48:07.380
Let's talk about organizational risks

48:07.380 --> 48:10.340
and these risk categories, of course,

48:10.340 --> 48:12.620
kind of play into each other, influence each other.

48:12.620 --> 48:17.620
So if we have organizations that are acting in a risky way,

48:17.940 --> 48:22.780
that this increases the risk of potentially rogue AI

48:22.780 --> 48:26.500
or it incentivizes others to race

48:26.500 --> 48:29.620
in order to compete with these organizations

48:29.620 --> 48:33.060
that are acting in risky ways.

48:33.060 --> 48:35.380
But yeah, let's just take it from the beginning.

48:35.380 --> 48:40.380
What falls under the organizational risks category?

48:41.380 --> 48:44.780
Yeah, so organizational risks at a slightly more abstract level

48:44.780 --> 48:47.340
would be the accidents bucket.

48:47.340 --> 48:52.260
So even if we reduce competitive pressures

48:52.260 --> 48:56.260
and if we have a,

48:57.500 --> 49:02.500
and if we don't have to worry about malicious use immediately,

49:03.660 --> 49:06.140
we'd still have the issue of organizations

49:06.140 --> 49:09.100
having maybe a culture of move fast and break things

49:09.140 --> 49:12.100
or them not having a safety culture.

49:12.100 --> 49:14.940
In other industries or for other technologies

49:14.940 --> 49:18.620
like rockets that wasn't extreme competition with that

49:18.620 --> 49:20.820
but nonetheless, rockets would blow up

49:20.820 --> 49:23.180
or nuclear power plants would melt down,

49:23.180 --> 49:25.540
catastrophic accidents can still happen

49:25.540 --> 49:28.300
and these can be very deadly

49:28.300 --> 49:30.500
in the case of AI systems eventually.

49:30.500 --> 49:35.500
So I think this is definitely a very hard one to fix.

49:35.860 --> 49:38.260
Most of the people at these AI organizations

49:38.260 --> 49:40.420
and how they were initialized and whatnot

49:40.420 --> 49:43.500
still had a lot of people who are mostly just wanting

49:43.500 --> 49:48.020
to build it and the consequences of society be damned.

49:48.020 --> 49:49.820
This is not my wheelhouse, I don't read the news.

49:49.820 --> 49:51.460
I don't like thinking about this sort of stuff.

49:51.460 --> 49:53.900
This is annoying humanities majors and whatnot

49:53.900 --> 49:56.580
who are in these ethics divisions or policy divisions

49:56.580 --> 49:58.220
that keep annoying us.

49:58.220 --> 49:59.980
This is kind of the attitude

49:59.980 --> 50:03.220
that most of these companies buy in large.

50:03.220 --> 50:08.220
And I think this is a large source of risk.

50:08.860 --> 50:13.860
We could, as well as it's just non-trivial

50:13.940 --> 50:17.500
as we see in other things like nuclear power plants,

50:17.500 --> 50:20.100
chemical plants, rockets and making sure

50:20.100 --> 50:22.700
that this is all extremely reliable.

50:22.700 --> 50:25.900
So we'd need various precedents.

50:25.900 --> 50:27.540
There's basically a literature on this

50:27.540 --> 50:30.420
called the organizational safety literature

50:30.420 --> 50:34.540
which focuses on various corporate controls

50:34.540 --> 50:36.620
and processes for making sure

50:36.620 --> 50:39.260
that the organization responds to failure,

50:39.260 --> 50:42.460
takes near misses seriously, has good whistleblowing,

50:42.460 --> 50:44.940
has good internal risk management regimes,

50:44.940 --> 50:48.260
has like a chief risk officer or an internal audit committee,

50:48.260 --> 50:49.460
all of these sorts of things

50:49.460 --> 50:51.940
to reduce these types of risks.

50:51.940 --> 50:54.460
And yeah, you were right in that this interacts

50:54.460 --> 50:56.860
with not necessarily direct cause

50:56.860 --> 50:58.380
of some of these existential risks,

50:58.380 --> 51:00.380
but nonetheless boosts up the probability

51:00.380 --> 51:03.220
if we're perceiving that an organization

51:03.220 --> 51:05.700
is very reckless in its attitude.

51:05.700 --> 51:09.460
This causes more safety minded ones to compete harder

51:09.460 --> 51:12.260
and justify erasing.

51:12.260 --> 51:15.740
This reduces the, that consequently reduces

51:15.740 --> 51:17.980
the amount of time you have to work on control

51:17.980 --> 51:20.900
and reliability of these AI systems,

51:20.900 --> 51:25.220
which affects the probability of rogue AI's, of course.

51:25.220 --> 51:27.540
There's also other types of accidents that could happen

51:27.540 --> 51:30.300
like the organization might accidentally leak

51:31.300 --> 51:34.580
one of its models that has some lethal capabilities

51:34.580 --> 51:36.500
in it if it's repurposed.

51:36.500 --> 51:40.740
There's also a risk of as potentially,

51:40.740 --> 51:45.500
who's to say happened with viruses,

51:45.500 --> 51:48.260
maybe there'd be some unfortunate gain of function research

51:48.260 --> 51:51.980
that would also lead to some type of catastrophe as well.

51:51.980 --> 51:55.460
There are people interested in what is essentially

51:55.460 --> 51:58.220
gain of function research and in creating warning shots,

51:58.220 --> 52:01.060
they might be a little too successful later on.

52:01.060 --> 52:04.540
What does gain of function research look like in AI?

52:04.540 --> 52:06.820
Deliberately building some AI system

52:06.820 --> 52:08.900
that's like power seeking or Machiavellian

52:08.900 --> 52:10.340
and wants to destroy humanity.

52:10.340 --> 52:12.140
And then they're gonna use this to like,

52:12.140 --> 52:13.300
you know, scare the world with,

52:13.300 --> 52:17.660
but like at some point when it's powerful enough,

52:17.660 --> 52:19.180
you might get what you asked for.

52:19.180 --> 52:22.460
The idea here is to create a dangerous AI,

52:22.460 --> 52:25.940
maybe an AI that's more agentic or power seeking

52:25.940 --> 52:30.340
and then use that model to study how to contain it.

52:30.340 --> 52:34.220
But then the worry is that we could ironically

52:34.220 --> 52:39.220
go extinct perhaps because we can't control the model.

52:41.420 --> 52:42.900
Yeah, and if this is like,

52:42.900 --> 52:46.220
who's to say who's going to be experimenting with this

52:46.220 --> 52:47.940
or how exactly cautious they will be

52:47.940 --> 52:49.700
or their like skill level,

52:49.700 --> 52:53.220
it may be mandated that they test for these types

52:53.460 --> 52:56.860
of dangerous inclinations or capabilities

52:56.860 --> 52:59.580
and who exactly is going to be doing that is unclear.

52:59.580 --> 53:02.860
It may not be like the most like capable people

53:04.060 --> 53:06.020
or there's just some overall

53:06.020 --> 53:09.220
or there's just some risk of accidents in that way.

53:09.220 --> 53:11.220
So I guess that gives some flavor

53:11.220 --> 53:12.500
of some of the direct accidents,

53:12.500 --> 53:14.900
but I also think how it indirectly affects things.

53:14.900 --> 53:17.740
So one way in which I think strongly indirectly

53:17.740 --> 53:22.580
affects things is when accident is an intellectual error

53:22.580 --> 53:23.940
inside of these organizations

53:23.940 --> 53:27.300
where they conflate safety and capabilities.

53:27.300 --> 53:29.900
This is a very common thing

53:29.900 --> 53:31.820
where there's not clear thinking about safety

53:31.820 --> 53:33.900
and capabilities where people be,

53:33.900 --> 53:36.740
oh, well, we're smart, you know, rational

53:36.740 --> 53:40.340
and justify the means, we're risk neutral.

53:41.220 --> 53:44.180
We actually don't actually do much empirical deep learning

53:44.180 --> 53:46.380
research, but conceptually,

53:46.380 --> 53:49.060
we think that this will be beneficial for safety

53:49.060 --> 53:50.780
even though it will come at the cost of capabilities

53:50.780 --> 53:51.620
and whatnot.

53:51.740 --> 53:53.540
I'm muddied up that line.

53:53.540 --> 53:56.820
And the distinction between safety and capabilities

53:56.820 --> 54:00.580
such that you could imagine a lot of these safety efforts

54:00.580 --> 54:03.580
basically just working on capabilities the entire time.

54:03.580 --> 54:06.340
I think that's a reasonable fraction

54:06.340 --> 54:09.380
of the safety teams I think do focus just on capabilities.

54:09.380 --> 54:13.980
For context, there is an extreme correlation

54:13.980 --> 54:16.140
between AI's capabilities

54:16.140 --> 54:18.380
in various different subjects and goals.

54:18.380 --> 54:21.180
So if you want your AI system to be better

54:21.420 --> 54:24.540
at something like math problems

54:24.540 --> 54:29.180
or history problems or accounting problems,

54:29.180 --> 54:32.700
these capabilities are all extremely correlated now,

54:32.700 --> 54:35.420
we can see with like large language models.

54:35.420 --> 54:38.860
You should assume that if something is correlated,

54:38.860 --> 54:42.740
like the correlation's like 80% or like 90%,

54:42.740 --> 54:44.620
it's extremely high.

54:45.660 --> 54:48.300
So when people reason themselves

54:48.340 --> 54:51.380
into some new capability

54:51.380 --> 54:53.780
that they think will be helpful for safety,

54:53.780 --> 54:55.700
it's very likely the base rate of it

54:55.700 --> 54:57.940
being correlated with capabilities

54:57.940 --> 55:00.060
and basically being nearly identical

55:00.060 --> 55:02.380
to other capabilities by being so correlated

55:02.380 --> 55:04.180
is extremely high.

55:04.180 --> 55:06.620
So I think there needs to be substantial evidence

55:06.620 --> 55:10.380
that the safety intervention that one is applying

55:10.380 --> 55:12.860
isn't affecting the general capabilities.

55:12.860 --> 55:15.540
And that requires empirical evidence.

55:15.580 --> 55:18.580
So a good example of empirical research

55:18.580 --> 55:20.700
that I think helps with safety,

55:20.700 --> 55:23.620
but doesn't clearly help with general capabilities

55:23.620 --> 55:24.820
of making a system smarter

55:24.820 --> 55:28.900
would be like the area of machine unlearning.

55:28.900 --> 55:31.460
So machine unlearning is where you're trying to unlearn

55:31.460 --> 55:33.060
some specific dangerous capabilities,

55:33.060 --> 55:34.420
trying to unlearn bio knowledge,

55:34.420 --> 55:36.020
trying to unlearn specific know-how

55:36.020 --> 55:37.700
that allows you to hack.

55:37.700 --> 55:41.460
This is more clearly like measurably not correlated with,

55:41.460 --> 55:42.900
it's inter-correlated with some capabilities

55:42.900 --> 55:45.020
and not particularly correlated with general capabilities

55:45.100 --> 55:46.700
just removing that specific know-how.

55:46.700 --> 55:49.060
I have to say robustness is also generally

55:49.060 --> 55:51.620
inter-correlated with general capabilities.

55:51.620 --> 55:54.500
It doesn't make the systems overall smarter.

55:54.500 --> 55:57.980
What happens is it makes the systems robust

55:57.980 --> 56:00.100
to some specific types of attacks.

56:00.100 --> 56:02.180
Robustness to that comes at a fairly large

56:02.180 --> 56:05.740
computational cost and takes up a lot of the model capacity.

56:05.740 --> 56:07.140
But that would be a sort of,

56:07.140 --> 56:09.620
that would be a safety intervention

56:09.620 --> 56:12.420
that doesn't make the models overall smarter.

56:12.420 --> 56:14.220
So those are examples of,

56:14.220 --> 56:15.660
or I suppose another example would be

56:15.660 --> 56:17.980
with transparency research.

56:17.980 --> 56:20.420
Historically, there have been no instances

56:20.420 --> 56:22.340
of transparency advancements

56:22.340 --> 56:26.300
leading to general capabilities advancements.

56:26.300 --> 56:28.820
Just trying to understand what's going on in the model

56:28.820 --> 56:30.700
and it doesn't really work nearly as well

56:30.700 --> 56:33.300
as just like throwing more data at it.

56:33.300 --> 56:35.860
And there aren't many architectural improvements

56:35.860 --> 56:36.980
that are likely to be found.

56:36.980 --> 56:39.860
Anyway, as a result of these investigations

56:39.860 --> 56:41.940
is the track record is pretty basically

56:41.940 --> 56:43.740
completely clean for transparency.

56:43.740 --> 56:45.740
Now, maybe that wouldn't be the case in the future,

56:45.740 --> 56:46.580
but then at that point,

56:46.580 --> 56:48.100
then we wouldn't identify this as something

56:48.100 --> 56:50.420
that is particularly helping with safety.

56:50.420 --> 56:53.580
So I think that for the safety research areas,

56:53.580 --> 56:56.500
we need to be quite clear about there's,

56:56.500 --> 56:59.460
you can't just have some informal argument about,

56:59.460 --> 57:00.660
or an appeal to authority that,

57:00.660 --> 57:05.660
oh, this is helpful for safety because of some verbal argument.

57:06.900 --> 57:10.100
The empirical machine learning is very complicated.

57:10.100 --> 57:12.340
Hindsight barely works in trying to understand

57:12.340 --> 57:13.180
what's going on.

57:13.180 --> 57:15.460
This pre-training on fractal images

57:15.460 --> 57:18.500
help improve robustness to, I don't know,

57:18.500 --> 57:20.140
basically everything and improve the calibration

57:20.140 --> 57:21.300
and anomaly detection form.

57:21.300 --> 57:22.860
I have no idea.

57:22.860 --> 57:25.180
It works though, even ask people like,

57:25.180 --> 57:28.820
why are activation functions the way they are?

57:28.820 --> 57:31.380
I don't think there's actually a good canonical explanation

57:31.380 --> 57:32.540
that's like very consistent.

57:32.540 --> 57:34.180
You would want empirical evidence

57:34.180 --> 57:37.340
that when we are engaging in safety research,

57:37.340 --> 57:40.220
we are not accidentally also increasing

57:40.220 --> 57:41.500
the capabilities of models.

57:41.620 --> 57:44.300
And you think this is something that happens often?

57:44.300 --> 57:46.380
Yeah, I think this happens extremely often

57:46.380 --> 57:48.580
in this sort of, this organizational risk

57:48.580 --> 57:50.820
of the conflation of safety and capabilities.

57:50.820 --> 57:53.180
Now, this isn't to say that they are loose and separate.

57:53.180 --> 57:55.020
A better improvements in capabilities

57:55.020 --> 57:57.740
has downstream effects on safety in many situations.

57:57.740 --> 58:01.340
It makes them better able to understand human values,

58:01.340 --> 58:04.380
for instance, as they gain more and more common sense.

58:04.380 --> 58:06.820
But if we are trying to improve safety

58:06.820 --> 58:09.420
and specifically reduce existential risk,

58:09.420 --> 58:12.140
I think we need to differentially improve

58:12.140 --> 58:13.180
on some safety access

58:13.180 --> 58:15.060
and not in the general capabilities access.

58:15.060 --> 58:17.420
If we are doing something that's fairly correlated

58:17.420 --> 58:18.820
with capabilities and safety,

58:18.820 --> 58:20.940
I think that the default expectation

58:20.940 --> 58:23.540
is that actually you're working in the service

58:23.540 --> 58:24.940
of capabilities.

58:24.940 --> 58:28.220
A good example would be one of OpenAI strategies

58:28.220 --> 58:30.220
to mention this specifically,

58:30.220 --> 58:31.820
because I just don't think it's particularly

58:31.820 --> 58:33.900
intellectually defensible, I'm sorry to say.

58:33.900 --> 58:37.220
I'm a more disagreeable individual, so here I go.

58:37.220 --> 58:41.500
I don't think building a super human alignment researcher

58:41.500 --> 58:43.380
specifically just affects alignment.

58:43.380 --> 58:46.580
I think such a thing can be easily repurposed

58:46.580 --> 58:48.860
to doing lots of other types of research.

58:48.860 --> 58:50.740
I don't think there's like a specific

58:50.740 --> 58:52.980
alignment research skill set that is just,

58:52.980 --> 58:54.780
oh, it's just, you only get at that,

58:54.780 --> 58:56.740
but if you're good at that,

58:56.740 --> 58:58.220
it means nothing about your ability

58:58.220 --> 58:59.220
to accomplish anything else.

58:59.220 --> 59:00.140
I just don't think that's the case.

59:00.140 --> 59:02.100
I think it's actually extremely correlated

59:02.100 --> 59:03.260
with general capabilities.

59:03.260 --> 59:05.500
It would be very straightforwardly repurposed

59:05.500 --> 59:07.140
to other forms of research.

59:07.140 --> 59:09.980
But that's an example of this sort of conflation.

59:09.980 --> 59:13.420
Now, this isn't to say OpenAI is only

59:13.420 --> 59:15.540
is conflating safety capabilities entirely.

59:15.540 --> 59:16.380
I'm not claiming that.

59:16.380 --> 59:20.740
They will have some work on transparency.

59:20.740 --> 59:23.620
I gather that they'll work more on reliability

59:23.620 --> 59:28.620
and robustness, but this is a very dangerous conflation.

59:29.300 --> 59:33.300
And I think basically if they seem kind of correlated,

59:33.300 --> 59:35.100
just intuitively, and then if you hear a lot

59:35.100 --> 59:36.900
of verbal arguments without empirical demonstration,

59:36.900 --> 59:39.140
basically assume just the base rates are like,

59:39.140 --> 59:41.180
a lot of these completely separate subjects,

59:41.180 --> 59:43.820
like performance in history and like,

59:43.820 --> 59:45.820
performance in philosophy and mathematics,

59:45.820 --> 59:48.340
like those are all like hyper correlated

59:48.340 --> 59:49.740
to assume this other type of thing

59:49.740 --> 59:51.560
is also hyper correlated with it too.

59:51.560 --> 59:53.500
Anyway, though, that's another one that like,

59:53.500 --> 59:57.060
an organizational factor that really reduces

59:57.060 --> 01:00:01.100
the amount of time we have to solve this problem

01:00:01.100 --> 01:00:02.860
and our ability to solve it as well.

01:00:02.860 --> 01:00:03.700
So.

01:00:03.700 --> 01:00:07.860
I think the worry here for, say you're a top AI company

01:00:07.860 --> 01:00:11.300
and you're thinking, okay, how much a safe organization

01:00:11.300 --> 01:00:12.540
features should we implement?

01:00:12.540 --> 01:00:14.220
So should we have more red teaming?

01:00:14.220 --> 01:00:16.460
Should we have more procedures, more review,

01:00:16.460 --> 01:00:19.020
more testing?

01:00:19.020 --> 01:00:21.700
Should we require this empirical evidence

01:00:21.700 --> 01:00:26.620
before we begin a new safety research program?

01:00:26.620 --> 01:00:30.020
This now threatens to slow us down

01:00:30.020 --> 01:00:34.380
and it opens us up to competition

01:00:34.380 --> 01:00:36.780
from the kind of scrappy new startup

01:00:36.780 --> 01:00:41.500
that's on at our heels trying to outcompet us.

01:00:41.500 --> 01:00:44.100
This is very straightforwardly now a case

01:00:44.100 --> 01:00:49.100
of kind of AI race undermining organizational safety

01:00:49.140 --> 01:00:52.500
or at least threatening to undermine organizational safety.

01:00:52.500 --> 01:00:56.340
Can you make an argument if you were to sell this

01:00:56.340 --> 01:01:00.260
to a CEO of an AI corporation that's...

01:01:00.260 --> 01:01:02.580
When would I be in that situation?

01:01:02.580 --> 01:01:07.180
That safety is in the interest of the organization itself.

01:01:07.180 --> 01:01:11.940
You could say it's difficult to sell unsafe products, right?

01:01:11.940 --> 01:01:13.140
You want to be in control.

01:01:13.140 --> 01:01:15.980
You don't want to lose the weights of your model

01:01:15.980 --> 01:01:17.180
in a leak and so on.

01:01:17.180 --> 01:01:19.740
So there might be some correlation

01:01:19.740 --> 01:01:22.660
between the self-interest of the organization

01:01:22.660 --> 01:01:26.580
and the interest that society has in safety in general.

01:01:26.580 --> 01:01:29.460
But before that, one additional factor

01:01:29.460 --> 01:01:31.780
that just diffusely increases probability of extras

01:01:31.780 --> 01:01:33.060
from these organizations,

01:01:33.060 --> 01:01:34.620
if they just do safety washing

01:01:34.620 --> 01:01:36.180
and they don't even know it sometimes,

01:01:36.180 --> 01:01:38.620
they might have some small gesture for safety.

01:01:38.620 --> 01:01:40.580
They might have, for instance,

01:01:40.580 --> 01:01:42.180
a responsible scaling policy

01:01:42.180 --> 01:01:44.780
that doesn't commit them to almost anything

01:01:44.780 --> 01:01:47.980
and then that placates regulators, for instance,

01:01:47.980 --> 01:01:49.580
but doesn't actually reduce risk.

01:01:49.620 --> 01:01:51.980
Those would be other examples

01:01:51.980 --> 01:01:56.180
of how organizational risks can end up

01:01:56.180 --> 01:01:58.300
increasing the probability of existential risk.

01:01:58.300 --> 01:02:02.020
Although it's diffuse and indirect, it still matters.

01:02:02.020 --> 01:02:04.180
On the self-interest point,

01:02:04.180 --> 01:02:07.500
I think a lot of the catastrophic risks

01:02:07.500 --> 01:02:10.860
or catastrophic risks and existential risks are tail risks.

01:02:10.860 --> 01:02:14.100
And generally organizations don't really price

01:02:14.100 --> 01:02:16.660
in tail risks that much.

01:02:16.660 --> 01:02:18.660
A lot of portfolios don't really do much

01:02:18.660 --> 01:02:21.940
to address tail risks either, like in other industries,

01:02:21.940 --> 01:02:24.140
like in finance and whatnot.

01:02:25.140 --> 01:02:27.700
So this is kind of like a problem

01:02:27.700 --> 01:02:30.420
with many of our institutions

01:02:30.420 --> 01:02:34.060
that we could convince them to do things like red teaming,

01:02:34.060 --> 01:02:36.340
to some extent, but doing red teaming

01:02:36.340 --> 01:02:39.140
for existential risks and whatnot

01:02:39.140 --> 01:02:41.300
is not necessarily something that they would check to do

01:02:41.300 --> 01:02:43.540
because that's not going to affect their product tomorrow.

01:02:43.540 --> 01:02:46.820
There's no pushback if everybody is dead,

01:02:46.820 --> 01:02:47.660
as was mentioned before.

01:02:47.660 --> 01:02:49.980
So I think that this works to a limit.

01:02:49.980 --> 01:02:53.020
I think some things like saying information security

01:02:53.020 --> 01:02:56.140
for your company or so that your weights don't leak.

01:02:56.140 --> 01:02:58.620
This is a much easier argument to make.

01:02:58.620 --> 01:03:02.060
Other claims like some of these internal controls

01:03:02.060 --> 01:03:03.940
and whatnot, oh, this will slow us down.

01:03:03.940 --> 01:03:06.420
This will reduce our velocity.

01:03:06.420 --> 01:03:08.580
And I think these are harder to make.

01:03:08.580 --> 01:03:12.340
And I don't think that there are necessarily short-term

01:03:12.340 --> 01:03:14.300
economic incentives for some of these.

01:03:14.300 --> 01:03:17.180
Many of these are actually more for addressing tail risks

01:03:17.180 --> 01:03:20.460
and black swan events.

01:03:20.460 --> 01:03:23.780
So they would then need to just recognize

01:03:23.780 --> 01:03:28.260
that the black swan events are real possibilities

01:03:28.260 --> 01:03:31.940
beyond a probability threshold worth actually addressing.

01:03:31.940 --> 01:03:35.580
So I'm not claiming that they're being completely irrational

01:03:35.580 --> 01:03:39.100
if they're being fairly short-sighted

01:03:39.100 --> 01:03:43.620
and don't believe in these black swan events from it.

01:03:43.620 --> 01:03:45.940
Then I think them trying to maintain velocity

01:03:45.980 --> 01:03:48.260
and just maintain optionality and whatnot,

01:03:48.260 --> 01:03:50.020
it's understandable.

01:03:50.020 --> 01:03:51.540
I wouldn't advocate for that,

01:03:51.540 --> 01:03:53.700
but it's understandable that they're doing that.

01:03:53.700 --> 01:03:58.620
I think they're importing to a lot of their incentives well,

01:03:58.620 --> 01:04:00.500
but, and they will do various things

01:04:00.500 --> 01:04:04.260
to reduce some generic risk.

01:04:04.260 --> 01:04:06.540
They will do some generic forms of red teaming,

01:04:06.540 --> 01:04:08.580
regardless of whether there's regulation,

01:04:08.580 --> 01:04:09.620
because it will make sense.

01:04:09.620 --> 01:04:11.900
But I just don't think that that does particularly much

01:04:11.900 --> 01:04:14.100
in the way of reducing these catastrophic

01:04:14.100 --> 01:04:15.940
existential risks off.

01:04:15.940 --> 01:04:18.780
Say you're a philanthropist or a government

01:04:18.780 --> 01:04:20.420
with a big bag of money

01:04:20.420 --> 01:04:22.820
and you want to incentivize safety research

01:04:22.820 --> 01:04:26.020
at these top AI corporations.

01:04:26.020 --> 01:04:28.500
Is there a way in which you could earmark the money

01:04:28.500 --> 01:04:31.620
and make sure it's spent on what you want it to be spent on?

01:04:31.620 --> 01:04:34.820
So it's not funneled into increasing capabilities

01:04:34.820 --> 01:04:35.740
of the models,

01:04:35.740 --> 01:04:39.460
it's spent on the right type of safety research.

01:04:39.460 --> 01:04:41.500
I think that'd be one intervention.

01:04:41.500 --> 01:04:44.580
I think it's very possible to,

01:04:44.580 --> 01:04:48.300
there are a lot of professors lying around in academe

01:04:48.300 --> 01:04:51.220
who could do this research.

01:04:51.220 --> 01:04:53.660
All you need is to subsidize.

01:04:53.660 --> 01:04:55.100
So for instance, like the Center for Safety

01:04:55.100 --> 01:04:57.900
as a compute cluster, we'd love to expand it.

01:04:57.900 --> 01:05:01.180
We're only able to support not that many professors

01:05:01.180 --> 01:05:04.260
to do research with large language models

01:05:04.260 --> 01:05:06.100
and very compute intensive experiments,

01:05:06.100 --> 01:05:08.180
but there are a lot of professors

01:05:08.180 --> 01:05:11.140
who could be doing more research here.

01:05:11.140 --> 01:05:12.020
So I think that probably,

01:05:12.020 --> 01:05:13.900
there aren't that many people working at these organizations

01:05:13.900 --> 01:05:14.940
I should say as well.

01:05:14.940 --> 01:05:19.940
And so I wouldn't bet on them to fix everything.

01:05:20.380 --> 01:05:22.700
You're actually just correlating it with like,

01:05:22.700 --> 01:05:24.660
what is Jared Copeland's safety vision?

01:05:24.660 --> 01:05:27.100
What is Yann Leica's safety vision?

01:05:27.100 --> 01:05:29.940
And like you're getting like two or three bets

01:05:29.940 --> 01:05:32.780
if you were like giving each of them money.

01:05:32.780 --> 01:05:35.220
And I think that's not a very diversified portfolio

01:05:35.220 --> 01:05:36.300
and you should expect.

01:05:36.300 --> 01:05:40.060
Blind spots just because people don't have,

01:05:40.060 --> 01:05:42.020
can't simulate a collective intelligence,

01:05:42.020 --> 01:05:45.220
a broad research effort by themselves,

01:05:45.220 --> 01:05:47.380
even if they work very hard and have lots of discussions

01:05:47.380 --> 01:05:50.740
and take out, have good deference to outside views

01:05:50.740 --> 01:05:51.580
and so on, they just can't,

01:05:51.580 --> 01:05:53.180
they just can't simulate that function.

01:05:53.180 --> 01:05:55.580
So I would suggest if one's wanting

01:05:55.580 --> 01:05:58.180
to subsidize safety research,

01:05:58.180 --> 01:06:01.540
we can, if there's can have subsidize like a compute cluster,

01:06:01.540 --> 01:06:03.420
then we can have high accountability of like,

01:06:03.420 --> 01:06:04.660
you're not allowed to run this project

01:06:04.660 --> 01:06:07.340
because this doesn't seem sufficiently safety related

01:06:07.340 --> 01:06:08.540
instead of giving like money, you know,

01:06:08.540 --> 01:06:11.100
strings attached to some academics and they run off with it.

01:06:11.100 --> 01:06:14.220
So that would be my preferred intervention.

01:06:14.220 --> 01:06:16.060
And I think that there's,

01:06:16.060 --> 01:06:17.500
it can take orders of magnitude more.

01:06:17.500 --> 01:06:19.220
So like, if any of them are listening,

01:06:19.220 --> 01:06:20.180
you know, like reach out to us,

01:06:20.180 --> 01:06:22.580
I'd love to get more compute to,

01:06:22.580 --> 01:06:25.500
to people doing relevant,

01:06:25.500 --> 01:06:27.900
doing relevant research on safety

01:06:27.900 --> 01:06:29.940
in a nice diversified portfolio.

01:06:29.940 --> 01:06:32.100
Across transparency and adversarial robustness

01:06:32.100 --> 01:06:35.140
and back doors and machine learning models

01:06:35.140 --> 01:06:38.500
and done learning, these types of topics.

01:06:38.500 --> 01:06:42.260
Do you think some safety breakthroughs would be kept secret?

01:06:42.260 --> 01:06:46.260
Say a safety breakthrough at Google DeepMind,

01:06:46.260 --> 01:06:51.260
made the AI useful or in a way that incentivize them

01:06:51.700 --> 01:06:53.220
not to share the safety breakthrough?

01:06:53.220 --> 01:06:55.580
Or would you expect safety breakthroughs

01:06:55.580 --> 01:06:59.740
to be shared widely as if they were found

01:06:59.740 --> 01:07:02.020
in an academic lab?

01:07:02.020 --> 01:07:04.020
I think for market positioning,

01:07:04.020 --> 01:07:07.660
one of them could occupy the niche of being the safest

01:07:07.660 --> 01:07:09.380
of the racing companies.

01:07:09.380 --> 01:07:12.740
We are technically the safest.

01:07:12.740 --> 01:07:16.380
So I think that's currently occupied by Anthropic

01:07:16.380 --> 01:07:18.660
and this might make it fairly useful

01:07:18.660 --> 01:07:22.100
when pitching themselves for say a defense contract

01:07:22.100 --> 01:07:25.500
that look weird to the more reliable organization

01:07:25.500 --> 01:07:27.220
compared to our competitors.

01:07:27.220 --> 01:07:30.020
And so if they would be open sourcing some of that,

01:07:30.020 --> 01:07:33.860
then I think they would lose some of that competitive advantage.

01:07:33.860 --> 01:07:38.780
So it's quite conceivable that they'd hold on to things.

01:07:38.780 --> 01:07:40.940
I mean, there's many safety projects they do

01:07:40.940 --> 01:07:43.540
for which the code is not like open source.

01:07:43.540 --> 01:07:44.660
So we see that to some extent there,

01:07:44.660 --> 01:07:47.740
but I think it can make sense for one of them to try

01:07:47.740 --> 01:07:49.700
and just be a bit safer than the others

01:07:49.700 --> 01:07:51.860
or a bit more reliable than the others.

01:07:51.860 --> 01:07:55.500
Yeah, I've heard Sam Altman, the CEO of OpenAI,

01:07:55.500 --> 01:07:57.460
talk about releasing these systems,

01:07:57.460 --> 01:08:01.660
so specifically releasing GPT-3 and 4 to the world,

01:08:01.660 --> 01:08:07.020
to chat GPT in order to gather more attention to the issue.

01:08:07.020 --> 01:08:10.180
Do you think this is a viable strategy?

01:08:10.180 --> 01:08:14.500
Is this too risky or is it worth trying?

01:08:14.500 --> 01:08:16.860
I think to answer a more extreme question

01:08:16.860 --> 01:08:19.260
to possibly get a sense of my position on this,

01:08:19.260 --> 01:08:22.260
I think the release of Llama 2, for instance,

01:08:22.260 --> 01:08:24.140
by Meta, which is an open source,

01:08:24.140 --> 01:08:27.940
large language model around the capacity of GPT-3.5,

01:08:27.940 --> 01:08:33.220
I think the benefits of that actually outweigh the costs.

01:08:33.220 --> 01:08:35.220
It enables a lot more research.

01:08:35.220 --> 01:08:38.100
It also improves our defenses

01:08:38.100 --> 01:08:40.620
against some of the immediate applications

01:08:40.620 --> 01:08:41.620
of these AI systems.

01:08:41.620 --> 01:08:45.940
So for instance, it came out today that North Korea

01:08:45.940 --> 01:08:48.060
is using some of these AI systems.

01:08:48.060 --> 01:08:50.300
I don't know whether it's Llama 2, that'd be my guess,

01:08:50.300 --> 01:08:53.180
because it's just the most capable open source system

01:08:53.180 --> 01:08:55.340
or code Llama potentially,

01:08:55.340 --> 01:08:58.500
using AI systems to identify vulnerabilities in software,

01:08:58.500 --> 01:09:00.940
and then that helps them shortlist things to attack.

01:09:00.940 --> 01:09:03.820
This isn't an extremely capable,

01:09:03.820 --> 01:09:06.820
or this doesn't rewrite the cost-benefit analysis

01:09:06.820 --> 01:09:07.820
of cyber attacks.

01:09:07.820 --> 01:09:11.380
It doesn't rupture our digital ecosystem,

01:09:11.380 --> 01:09:14.060
but this basically gives us some preview

01:09:14.060 --> 01:09:19.780
and forces these issues on people's attention.

01:09:19.780 --> 01:09:23.180
So I think there's an argument to be made

01:09:23.180 --> 01:09:29.060
for open sourcing Llama 2, or if it's trained on 10x more

01:09:29.060 --> 01:09:31.780
compute Llama 3.

01:09:31.780 --> 01:09:34.580
After that, there's more uncertainty we'd

01:09:34.580 --> 01:09:38.700
have to see, because maybe it could be repurposed for things

01:09:38.700 --> 01:09:41.340
like bio weapons then, or it would be substantially more

01:09:41.340 --> 01:09:44.180
capable at hacking and scamming, things like that.

01:09:44.180 --> 01:09:46.780
I think there's a real argument to be made

01:09:46.780 --> 01:09:50.540
for some short-term stressors snapping the system

01:09:50.540 --> 01:09:54.500
into to do something about it, or at least waking them up.

01:09:54.500 --> 01:09:56.860
But I think systems function better

01:09:56.860 --> 01:09:58.980
with some amount of stressors, when the stressors get too

01:09:58.980 --> 01:10:01.420
extreme, then it can undermine the system.

01:10:01.420 --> 01:10:02.220
So it's complicated.

01:10:02.220 --> 01:10:04.780
I mean, I think maybe the situation, the case of opening,

01:10:04.780 --> 01:10:07.940
I really see these things, or how they want to go about release

01:10:07.940 --> 01:10:11.780
strategies would not surprise me if that should change,

01:10:11.780 --> 01:10:15.140
or if it would be better to do other things in the future.

01:10:15.140 --> 01:10:15.740
Yeah.

01:10:15.740 --> 01:10:18.140
Do you know something about the internal processes

01:10:18.220 --> 01:10:21.060
for deciding when to release these models?

01:10:21.060 --> 01:10:23.660
So in the case of Meta, for instance,

01:10:23.660 --> 01:10:28.300
they may have a chief legal officer vote on whether

01:10:28.300 --> 01:10:30.580
or potentially a veto power that could still

01:10:30.580 --> 01:10:33.860
be overwritten by the CEO, which may have happened,

01:10:33.860 --> 01:10:37.700
in the case of Llama 2, being suggestive here,

01:10:37.700 --> 01:10:39.620
because I have to have a second source for it.

01:10:39.620 --> 01:10:43.780
But usually for decisions, though,

01:10:43.780 --> 01:10:47.620
OpenAI will be accountable to their board.

01:10:47.620 --> 01:10:49.620
I don't know whether they have formal powers

01:10:49.620 --> 01:10:52.060
to decide whether they'd be voting.

01:10:52.060 --> 01:10:56.740
Often boards have blunt powers of just firing the CEO.

01:10:56.740 --> 01:10:59.460
And there often aren't processes in place

01:10:59.460 --> 01:11:03.060
for these larger-scale decisions.

01:11:03.060 --> 01:11:06.340
So you could imagine a CEO just deciding unilaterally

01:11:06.340 --> 01:11:08.900
to have something released.

01:11:08.900 --> 01:11:11.220
And that's something that organizational safety could

01:11:11.220 --> 01:11:13.380
improve and be processes for high-stakes decisions,

01:11:13.380 --> 01:11:14.100
as an example.

01:11:14.100 --> 01:11:14.780
But yeah.

01:11:14.820 --> 01:11:18.260
But by default, boards do not have fine-grain control.

01:11:18.260 --> 01:11:22.220
And so it's often up to the CEO to make the call.

01:11:22.220 --> 01:11:23.900
So you have a single point of failure.

01:11:23.900 --> 01:11:28.460
What is the Swiss cheese model of organizational safety?

01:11:28.460 --> 01:11:31.020
Yeah, so I'm mentioning, and if people

01:11:31.020 --> 01:11:35.180
are wanting to hear more about the organizational safety

01:11:35.180 --> 01:11:39.620
literature, we'll have the AI safety ethics and society

01:11:39.620 --> 01:11:42.020
textbook out in November.

01:11:42.020 --> 01:11:45.340
And one of the chapters would be on safety engineering.

01:11:45.340 --> 01:11:49.420
The Swiss cheese model is easy to communicate.

01:11:49.420 --> 01:11:50.500
It's kind of outdated.

01:11:50.500 --> 01:11:53.340
But it gets at a, just like how people

01:11:53.340 --> 01:11:56.780
are doing analysis of existential risk from AI earlier,

01:11:56.780 --> 01:12:00.700
they'd have a toy model that captured some of the and some

01:12:00.700 --> 01:12:02.180
of the scenarios to be concerned about.

01:12:02.180 --> 01:12:04.380
But it's important not to let that be the lens by which

01:12:04.380 --> 01:12:05.380
you filter everything through.

01:12:05.380 --> 01:12:07.740
That captures some of it, but not all of it.

01:12:07.740 --> 01:12:09.980
And the Swiss cheese one captures some of the dynamics,

01:12:09.980 --> 01:12:10.860
but not all of the dynamics.

01:12:10.860 --> 01:12:13.980
But anyway, the Swiss cheese model with that probably

01:12:13.980 --> 01:12:17.700
would decide essentially having multiple layers of defense.

01:12:17.700 --> 01:12:20.780
If you have red teaming, even red teaming

01:12:20.780 --> 01:12:25.620
for a catastrophic risk, that reduces the risk of catastrophe.

01:12:25.620 --> 01:12:27.740
But it's not itself perfect.

01:12:27.740 --> 01:12:32.140
You might also want stronger informational security, too,

01:12:32.140 --> 01:12:35.780
to make sure that if you had a dangerous model that it

01:12:35.780 --> 01:12:40.580
doesn't leak, you could have better

01:12:40.620 --> 01:12:45.420
transparency tools to check for deceptive behavior in AI systems.

01:12:45.420 --> 01:12:47.180
But if those transparency tools failed,

01:12:47.180 --> 01:12:49.980
maybe you would want monitoring of these AI systems

01:12:49.980 --> 01:12:52.500
so that before they take any action,

01:12:52.500 --> 01:12:55.820
it needs to be approved by something equivalent

01:12:55.820 --> 01:12:58.380
to an artificial conscience or filter

01:12:58.380 --> 01:13:01.420
that would filter out some of the immoral actions of AI agents

01:13:01.420 --> 01:13:02.900
before they're able to take them.

01:13:02.900 --> 01:13:05.340
And so all of these together can increase

01:13:05.340 --> 01:13:07.700
the reliability of the system.

01:13:07.700 --> 01:13:11.420
So the hope is that if you stack together

01:13:11.420 --> 01:13:14.660
many of these, you've substantially reduced your risk.

01:13:14.660 --> 01:13:17.460
This isn't looking for a perfect airtight solution.

01:13:17.460 --> 01:13:21.020
This is looking for layering on many different defenses

01:13:21.020 --> 01:13:21.860
to actually reduce risk.

01:13:21.860 --> 01:13:23.820
So if I wanted to reduce by a risk, for instance,

01:13:23.820 --> 01:13:27.300
here's an example of a Swiss cheese thing.

01:13:27.300 --> 01:13:30.980
First, there'd be the diffuse thing.

01:13:30.980 --> 01:13:33.540
And maybe there could be some regulation about not allowing

01:13:33.540 --> 01:13:34.540
models with these capabilities.

01:13:34.540 --> 01:13:36.060
But let's say I'm an organization that

01:13:36.060 --> 01:13:37.140
takes safety more seriously.

01:13:37.140 --> 01:13:38.740
So that depends on safety culture.

01:13:38.740 --> 01:13:40.700
So that's some sort of barrier.

01:13:40.700 --> 01:13:42.700
Regulation might be some barrier against this risk.

01:13:42.700 --> 01:13:44.780
Safety culture might be some barrier against this risk.

01:13:44.780 --> 01:13:46.620
So then they have enough of a safety culture

01:13:46.620 --> 01:13:48.780
they're willing to add a lot of these safety features.

01:13:48.780 --> 01:13:50.220
Now, these safety features themselves

01:13:50.220 --> 01:13:53.380
will end up having lots of different layers of defense.

01:13:53.380 --> 01:13:57.140
You could have an input filter to try and remove

01:13:57.140 --> 01:13:59.340
whether there's a request to create a bio weapon.

01:13:59.340 --> 01:14:02.260
You could also remove virology related data

01:14:02.260 --> 01:14:03.540
from the pre-training distribution

01:14:03.540 --> 01:14:06.460
so that it likely knows a lot less about virology.

01:14:06.460 --> 01:14:09.780
You could have an output filter as well,

01:14:09.780 --> 01:14:12.180
which would, even if somebody jail breaks the input filter,

01:14:12.180 --> 01:14:13.780
then they're also going to need a jail break

01:14:13.780 --> 01:14:16.180
the output filter, which is harder to do.

01:14:16.180 --> 01:14:19.220
And you could imagine adversarily training this as well.

01:14:19.220 --> 01:14:21.780
So it'd be another layer so that it would be more robust

01:14:21.780 --> 01:14:25.100
to people trying to jail break those layers of defense.

01:14:25.100 --> 01:14:28.420
But then you also have, there's also people

01:14:28.420 --> 01:14:30.380
who could, through the API, fine tune the model

01:14:30.380 --> 01:14:33.700
and inject some of that bio knowledge back into the model.

01:14:33.700 --> 01:14:38.700
So you could have a filter that screens the fine tuning data

01:14:39.060 --> 01:14:42.180
so that that information can't get back into the weights.

01:14:42.180 --> 01:14:43.900
And then you could add another layer,

01:14:43.900 --> 01:14:45.260
which would be an unlearning layer

01:14:45.260 --> 01:14:48.220
where you would assume that before you hand back

01:14:48.220 --> 01:14:50.180
the fine tuned model to the user,

01:14:50.180 --> 01:14:52.260
before they get it back, we're going to run a scrubbing,

01:14:52.260 --> 01:14:54.460
unlearning, knowledge expunging thing

01:14:54.460 --> 01:14:57.580
to expunge some of any bio knowledge if there is any.

01:14:57.580 --> 01:14:59.220
And that would be yet another layer.

01:14:59.220 --> 01:15:03.180
This approach reduces the risk of some bio catastrophe.

01:15:03.820 --> 01:15:05.220
Are any of those airtight?

01:15:05.220 --> 01:15:06.340
No.

01:15:06.340 --> 01:15:07.580
But do they work better collectively?

01:15:07.580 --> 01:15:08.420
Absolutely.

01:15:08.420 --> 01:15:11.300
So this is why we shouldn't be focusing

01:15:11.300 --> 01:15:13.220
on these airtight solutions.

01:15:13.220 --> 01:15:15.540
Exclusively, we also need to make use

01:15:15.540 --> 01:15:18.300
of these various layers of defense.

01:15:18.300 --> 01:15:20.540
That's how we actually reduce the probability

01:15:20.540 --> 01:15:22.620
of existential risk.

01:15:22.620 --> 01:15:24.900
We can't let perfection be the enemy of the good.

01:15:24.900 --> 01:15:28.020
If we'd say, well, if we can't build a completely

01:15:28.020 --> 01:15:29.740
100% reliable input filter,

01:15:29.740 --> 01:15:31.380
then we shouldn't have an input filter.

01:15:31.380 --> 01:15:33.020
That's a dead end, so we shouldn't investigate it.

01:15:33.020 --> 01:15:35.620
That's just not how things work.

01:15:35.620 --> 01:15:37.180
Tell us more about the textbook.

01:15:37.180 --> 01:15:39.100
I'm pretty excited to read this.

01:15:39.100 --> 01:15:42.780
I hope that this is a product that should exist, I think.

01:15:42.780 --> 01:15:45.540
Specifically, tell us more about how do you think

01:15:45.540 --> 01:15:47.580
about updating this or keeping it up to date?

01:15:47.580 --> 01:15:50.180
I think for a textbook on AI safety,

01:15:50.180 --> 01:15:53.500
it won't probably work if the next version is out

01:15:53.500 --> 01:15:57.740
in 2034 or something like that, right?

01:15:57.740 --> 01:15:59.500
So how do you keep it up to date?

01:15:59.500 --> 01:16:02.180
And also, you can just present the textbook,

01:16:02.260 --> 01:16:04.340
which I think listeners will be interested in.

01:16:04.340 --> 01:16:07.620
I mean, since I've been around in academia for a while,

01:16:07.620 --> 01:16:10.900
I do have at least some of a sense of what things,

01:16:10.900 --> 01:16:13.700
what content is more likely to stand the test of time.

01:16:13.700 --> 01:16:17.300
So that one's not talking about Dolly 2 or something,

01:16:17.300 --> 01:16:18.900
which is already outdated,

01:16:20.100 --> 01:16:21.980
or what are kind of like fad topics

01:16:21.980 --> 01:16:24.980
and not giving those too much,

01:16:24.980 --> 01:16:26.780
not giving those airtime.

01:16:26.780 --> 01:16:29.060
I mean, an example of this would be an unsolved problems

01:16:29.060 --> 01:16:31.420
that I'll say two, three years ago or something,

01:16:31.420 --> 01:16:35.660
but there we introduced emergent capabilities,

01:16:35.660 --> 01:16:38.500
which I think has become fairly popular

01:16:38.500 --> 01:16:42.260
before Burns et al's paper on honesty and whatnot.

01:16:42.260 --> 01:16:44.620
We're also, honesty is a big part of alignment.

01:16:44.620 --> 01:16:46.980
So there's sometimes one needs to call the shots too

01:16:46.980 --> 01:16:48.380
as to what things will,

01:16:48.380 --> 01:16:52.380
even if there aren't, isn't much of a literature on it at all,

01:16:52.380 --> 01:16:55.660
need to predict what will end up standing the test of time.

01:16:55.660 --> 01:16:58.300
But so I think it should have some reasonable longevity

01:16:58.300 --> 01:17:00.460
because we're not focusing on transient knowledge,

01:17:00.460 --> 01:17:04.860
but instead like general interdisciplinary frameworks

01:17:04.860 --> 01:17:06.980
we're thinking about risk across all these sectors.

01:17:06.980 --> 01:17:09.220
Cause we had this issue of like, there's,

01:17:09.220 --> 01:17:10.340
if you're thinking about AI risk,

01:17:10.340 --> 01:17:12.660
you have to think a bit about geopolitics.

01:17:12.660 --> 01:17:14.220
You have to think about international relations

01:17:14.220 --> 01:17:15.060
to some extent.

01:17:15.060 --> 01:17:15.900
You think about AI risk,

01:17:15.900 --> 01:17:17.100
you have to think about corporate governance

01:17:17.100 --> 01:17:19.600
and AI developers and what sort of incentives

01:17:19.600 --> 01:17:21.060
are driving them.

01:17:21.060 --> 01:17:24.180
You have to think about the individual AI systems themselves

01:17:24.180 --> 01:17:26.820
too, you have to think about organizational safety.

01:17:26.820 --> 01:17:31.820
You have to think about broad variety of factors

01:17:32.220 --> 01:17:34.900
and we'll basically focus quite a bit on frameworks

01:17:34.900 --> 01:17:38.900
for thinking clearly about each of those.

01:17:38.900 --> 01:17:41.340
I would imagine that later one could have, you know,

01:17:41.340 --> 01:17:45.180
GPT-6 like help like update the textbook anyway.

01:17:45.180 --> 01:17:49.460
So honestly, it's actually like the plan first.

01:17:49.460 --> 01:17:51.500
Something in that direction.

01:17:51.500 --> 01:17:52.900
How technical is the book?

01:17:52.900 --> 01:17:56.940
Does it contain pseudocode like a standard AI textbook?

01:17:56.940 --> 01:18:00.660
The premise of it is to onboard people

01:18:00.660 --> 01:18:01.940
from different disciplines.

01:18:01.940 --> 01:18:04.340
This isn't written for machine learning PhD people.

01:18:04.340 --> 01:18:05.700
There are lots of different fields,

01:18:05.700 --> 01:18:07.900
economists, legal scholars, philosophers,

01:18:07.900 --> 01:18:10.780
people without technical background, policy makers,

01:18:10.780 --> 01:18:15.780
think tank people who want more of a systematic understanding

01:18:15.780 --> 01:18:17.020
of these issues.

01:18:17.020 --> 01:18:19.980
And so it's largely written for people

01:18:19.980 --> 01:18:22.220
without any specific background

01:18:22.340 --> 01:18:24.980
and it's not trying to be a sort of like

01:18:24.980 --> 01:18:27.740
a introductory machine learning PhD course.

01:18:27.740 --> 01:18:30.580
That would be the course.mlsafety.org

01:18:30.580 --> 01:18:34.060
if you want a course of various technical topics

01:18:34.060 --> 01:18:35.900
or the machine learning safety course.

01:18:35.900 --> 01:18:40.020
But this one is more focusing on, you know,

01:18:40.020 --> 01:18:42.420
as we were discussing the game theory of this,

01:18:42.420 --> 01:18:45.140
the various governance solutions.

01:18:45.140 --> 01:18:49.780
Conceptually, many of the arguments associated

01:18:49.780 --> 01:18:51.860
with rogue AIs, why might they be power systems?

01:18:51.980 --> 01:18:53.620
Maybe they're deceptive.

01:18:53.620 --> 01:18:54.780
Understanding that.

01:18:54.780 --> 01:18:56.540
There's also introduction to machine learning

01:18:56.540 --> 01:18:58.180
and reinforcement learning in it.

01:18:58.180 --> 01:19:00.780
Understanding collective action problems

01:19:00.780 --> 01:19:02.180
since that was fairly relevant

01:19:02.180 --> 01:19:05.980
and these competitive pressures.

01:19:05.980 --> 01:19:08.620
There's also ethics in the book as well

01:19:08.620 --> 01:19:09.780
where if you're assuming

01:19:09.780 --> 01:19:12.180
that you've got your AIs systems to be somewhat reliable,

01:19:12.180 --> 01:19:13.820
then we have to start worrying about

01:19:13.820 --> 01:19:14.900
making it beneficial.

01:19:14.900 --> 01:19:19.900
And so there's various bits of AI ethics

01:19:20.900 --> 01:19:24.620
as well of what are objectives

01:19:24.620 --> 01:19:26.460
that we might give the AI system.

01:19:26.460 --> 01:19:27.580
What would those look like?

01:19:27.580 --> 01:19:28.940
What would be some of the, you know,

01:19:28.940 --> 01:19:30.300
moral trade offs that you're making there?

01:19:30.300 --> 01:19:35.300
But so it's covering AI safety, ethics, and society.

01:19:35.540 --> 01:19:39.220
So trying to be fairly broad.

01:19:39.220 --> 01:19:41.780
You should have lecture slides.

01:19:41.780 --> 01:19:45.460
And presumably I'll get around to recording videos for two.

01:19:45.460 --> 01:19:46.860
The goals, there's several goals of it,

01:19:46.860 --> 01:19:48.260
like to compress the content.

01:19:48.260 --> 01:19:49.900
Right now, if you want to understand AI risk,

01:19:49.900 --> 01:19:52.220
basically need to be part of an intellectual scene,

01:19:52.220 --> 01:19:53.740
like in the Bay Area probably.

01:19:53.740 --> 01:19:55.980
Maybe and maybe somewhat an Oxford.

01:19:55.980 --> 01:19:57.900
So very high barriers to entry.

01:19:57.900 --> 01:19:59.380
And then if you do,

01:19:59.380 --> 01:20:01.580
you're probably going to take a somewhat narrow view

01:20:01.580 --> 01:20:05.580
just because they're all interested in rogue AIs

01:20:05.580 --> 01:20:08.420
and don't have as much interaction

01:20:08.420 --> 01:20:10.420
with the rest of the world.

01:20:10.420 --> 01:20:15.420
So you'll have many blind spots as to a lot of it.

01:20:15.420 --> 01:20:16.540
There's the social variables

01:20:16.540 --> 01:20:18.420
and the broader socio-technical problem.

01:20:18.420 --> 01:20:20.780
The knowledge has been a bit diffused

01:20:20.780 --> 01:20:22.940
across various different blocks.

01:20:22.940 --> 01:20:25.180
And to stay up to date,

01:20:25.180 --> 01:20:27.660
you've often had to jump around from different places.

01:20:27.660 --> 01:20:29.340
So it would be nice to have something

01:20:29.340 --> 01:20:31.380
that's more compressed.

01:20:31.380 --> 01:20:34.580
So some of the goals are to reduce the fragmentation

01:20:34.580 --> 01:20:37.540
of AI risk knowledge, increase the readability,

01:20:37.540 --> 01:20:40.740
and the sort of compression rate of this content.

01:20:40.740 --> 01:20:43.020
And so there's reducing the barrier to entry

01:20:43.020 --> 01:20:45.020
to these crucial ideas

01:20:45.020 --> 01:20:47.540
that should hopefully scale the number of people

01:20:47.540 --> 01:20:50.180
who can understand AI risk extremely quickly.

01:20:50.180 --> 01:20:52.500
I was somewhat surprised by,

01:20:52.500 --> 01:20:54.660
although there's a lot of global attention,

01:20:54.660 --> 01:20:56.500
the number of new experts flooding in

01:20:56.500 --> 01:20:59.180
has been, I think, very underwhelming.

01:20:59.180 --> 01:21:00.420
Is that good or bad?

01:21:00.420 --> 01:21:03.860
Sometimes it's a bad thing if experts are rushing

01:21:03.860 --> 01:21:07.860
into the new, newly hot idea.

01:21:07.860 --> 01:21:11.220
I think that if people are onboarded well

01:21:11.220 --> 01:21:14.740
and have a more comprehensive understanding,

01:21:14.780 --> 01:21:16.900
if they're basically like charlatans

01:21:16.900 --> 01:21:18.580
who aren't going to do their work,

01:21:18.580 --> 01:21:19.860
then that's more of a problem.

01:21:19.860 --> 01:21:21.980
So I think by default,

01:21:21.980 --> 01:21:26.220
with another capabilities jump or two, they will flood in.

01:21:26.220 --> 01:21:27.900
There's basically a question,

01:21:27.900 --> 01:21:30.700
and I don't anticipate they're going to read lots of

01:21:30.700 --> 01:21:34.540
lesswrong.com posts to be onboarded.

01:21:34.540 --> 01:21:35.820
They're just gonna start talking

01:21:35.820 --> 01:21:38.100
and trying to be about themselves.

01:21:38.100 --> 01:21:43.100
I say this in my time, empirical machine learning research,

01:21:43.540 --> 01:21:45.220
it basically should assume

01:21:45.220 --> 01:21:48.420
that when some area starts getting pretty hot,

01:21:48.420 --> 01:21:51.700
there'll be lots of random new people coming in

01:21:51.700 --> 01:21:56.540
and trying to influence the discussion substantially.

01:21:56.540 --> 01:21:58.220
Hopefully the people as they come in

01:21:58.220 --> 01:22:01.900
would have some understanding of many of the basics, though,

01:22:01.900 --> 01:22:04.820
but I think by default, it's relatively inaccessible.

01:22:04.820 --> 01:22:07.220
You'll have to read a lot of scattered content

01:22:08.340 --> 01:22:09.340
from different places,

01:22:09.340 --> 01:22:10.740
and a lot of it will be idiosyncratic,

01:22:10.740 --> 01:22:12.580
and it'll just take a long time to go through.

01:22:12.580 --> 01:22:16.100
Those are some of the reasons for doing this.

01:22:16.100 --> 01:22:20.420
And then also, I think that given that rogue AIs

01:22:20.420 --> 01:22:25.020
is not the only concern or only risk source,

01:22:25.020 --> 01:22:27.340
there's a lot of content that even a lot of people

01:22:27.340 --> 01:22:29.020
who've been thinking about AIs risk for a while

01:22:29.020 --> 01:22:32.440
will possibly need to become aware of.

01:22:33.580 --> 01:22:37.340
So that's why, so just as a grad student,

01:22:37.340 --> 01:22:41.260
when I just developed and just focused

01:22:41.820 --> 01:22:44.460
on these other things other than rogue AIs,

01:22:45.380 --> 01:22:47.260
and then now I think people are recognizing

01:22:47.260 --> 01:22:48.300
the importance of that,

01:22:48.300 --> 01:22:49.500
so now there'll hopefully be,

01:22:49.500 --> 01:22:51.740
or so now there'll be some material

01:22:51.740 --> 01:22:54.580
to help get a more formal understanding

01:22:54.580 --> 01:22:57.060
of these other sorts of issues.

01:22:57.060 --> 01:22:58.940
That's great, I'm looking forward to reading it.

01:22:58.940 --> 01:23:01.700
I think we should nonetheless talk about rogue AIs.

01:23:01.700 --> 01:23:04.620
That's your last category of risk.

01:23:04.620 --> 01:23:07.580
One issue here is proxy gaming.

01:23:07.580 --> 01:23:09.100
How does that work?

01:23:09.100 --> 01:23:10.540
How is it dangerous?

01:23:10.540 --> 01:23:12.060
Yeah, so you could imagine

01:23:12.060 --> 01:23:14.100
if you've got a very powerful AI system,

01:23:14.100 --> 01:23:18.420
if it finds reliability holes in the objective

01:23:18.420 --> 01:23:21.540
that it's given, then this could be destructive

01:23:21.540 --> 01:23:24.220
because it's being guided by a flawed objective.

01:23:24.220 --> 01:23:29.220
I think in a colloquial example is with believe in Hanoi,

01:23:31.220 --> 01:23:35.500
there'd be a bounty for killing rats.

01:23:35.500 --> 01:23:37.860
And so if you get the rats, you get a bounty,

01:23:37.860 --> 01:23:41.340
but then people were incentivized to breed rats

01:23:42.380 --> 01:23:43.620
so as to collect more of that bounty.

01:23:43.620 --> 01:23:45.420
That would be an example of an objective

01:23:45.420 --> 01:23:48.260
that you put forward that ends up getting gained.

01:23:48.260 --> 01:23:52.060
It's fairly difficult to encode all of your values

01:23:52.060 --> 01:23:55.700
like well-being and whatnot into a specific objective,

01:23:55.700 --> 01:23:58.620
a simple objective.

01:23:58.620 --> 01:24:00.940
So you might expect some approximation

01:24:00.940 --> 01:24:02.740
or to what you actually care about.

01:24:02.740 --> 01:24:05.220
And in machine learning, a famous example,

01:24:05.260 --> 01:24:08.620
this is the boat racing or coast runners example

01:24:08.620 --> 01:24:12.340
that OpenAI had, which was of proxy gaming,

01:24:12.340 --> 01:24:14.980
of there's a reward function

01:24:14.980 --> 01:24:17.100
and the reinforcement learning agent

01:24:17.100 --> 01:24:18.660
would optimize that reward function.

01:24:18.660 --> 01:24:19.740
It was, this was a racing game.

01:24:19.740 --> 01:24:21.860
You'd think it would optimize the reward function

01:24:21.860 --> 01:24:23.100
by going around the track,

01:24:23.100 --> 01:24:24.420
but what it instead learned to do

01:24:24.420 --> 01:24:25.540
was it can get a higher reward

01:24:25.540 --> 01:24:27.220
by getting lots of turbo boosts.

01:24:27.220 --> 01:24:32.220
And the turbo boosts, it could get a very rapid sequence

01:24:32.500 --> 01:24:34.580
of them by crashing into walls

01:24:34.580 --> 01:24:35.540
and catching on fire

01:24:35.540 --> 01:24:38.380
and then continually turbo boosting in that way.

01:24:38.380 --> 01:24:40.220
And that would help it get a higher score.

01:24:40.220 --> 01:24:42.900
So there are often holes in these objectives

01:24:44.260 --> 01:24:48.660
due to an ability to compute exactly the right objective

01:24:48.660 --> 01:24:53.380
or maybe we can only monitor some parts of the system.

01:24:53.380 --> 01:24:56.060
There's a computational and spatial

01:24:56.060 --> 01:24:58.420
and temporal constraints on the quality of the objective,

01:24:58.420 --> 01:24:59.980
meaning that you're gonna often have to go

01:24:59.980 --> 01:25:00.860
with an approximation.

01:25:00.860 --> 01:25:02.860
So it's something perfectly ideal.

01:25:02.860 --> 01:25:05.060
This relates to Goodhart's law,

01:25:05.060 --> 01:25:07.140
which works in human domains also

01:25:07.140 --> 01:25:11.220
in which it's difficult to specify exactly what it is you want.

01:25:11.220 --> 01:25:14.820
And whenever you specify something you want,

01:25:14.820 --> 01:25:19.300
that thing you've specified is now open to being game.

01:25:19.300 --> 01:25:21.340
So an example here might be

01:25:21.340 --> 01:25:24.180
that you want deep scientific insight

01:25:24.180 --> 01:25:27.660
and you assume that such insight correlates

01:25:27.660 --> 01:25:30.500
with citations or number of citations.

01:25:30.540 --> 01:25:34.780
But then you get gaming of the citation systems

01:25:34.780 --> 01:25:37.300
in which academics are incentivized

01:25:37.300 --> 01:25:41.300
to maximize citations at the cost of scientific insight.

01:25:41.300 --> 01:25:44.140
So is this a more general problem

01:25:44.140 --> 01:25:48.500
across all agents, humans included?

01:25:48.500 --> 01:25:50.820
Yeah, yeah, I don't think this is specific to,

01:25:50.820 --> 01:25:53.740
I don't think this is specific to AI agents.

01:25:53.740 --> 01:25:56.980
I will say that some objectives are harder to game than others.

01:25:56.980 --> 01:25:59.700
For instance, the bounty on rat tails

01:25:59.700 --> 01:26:02.140
is a lot easier to game than like citations

01:26:02.140 --> 01:26:04.980
because citations can be very valuable for getting,

01:26:04.980 --> 01:26:07.060
you know, emigrate if you're getting a green card,

01:26:07.060 --> 01:26:09.940
for instance, and it's a strong incentive to do it.

01:26:09.940 --> 01:26:14.940
And, but it's nonetheless challenging.

01:26:15.220 --> 01:26:16.700
So some of these objectives,

01:26:16.700 --> 01:26:18.740
even when people are trying very hard to game,

01:26:18.740 --> 01:26:21.300
they still can be correlated with a lot,

01:26:21.300 --> 01:26:23.860
like college admissions still focuses,

01:26:23.860 --> 01:26:25.420
incentivize people to be productive.

01:26:25.420 --> 01:26:28.660
Yes, they'll go overboard in studying for the exams

01:26:28.660 --> 01:26:30.820
and whatnot, the college admissions tests,

01:26:30.820 --> 01:26:33.300
yes, they'll go overboard in the number of extracurriculars

01:26:33.300 --> 01:26:35.260
and whatnot, but I still think it like,

01:26:35.260 --> 01:26:37.300
it doesn't can help shape compared to

01:26:37.300 --> 01:26:38.980
they're not being the incentive in the first place.

01:26:38.980 --> 01:26:41.740
I think overall my take on the GoodHeads Law

01:26:41.740 --> 01:26:44.140
is that there's some objectives that are,

01:26:44.140 --> 01:26:48.420
or some goals are all goals and proxies are wrong.

01:26:48.420 --> 01:26:50.900
Some are useful and some though, when gained

01:26:50.900 --> 01:26:53.740
in particular ways could be potentially catastrophic.

01:26:53.740 --> 01:26:55.860
So there's quite a variety.

01:26:55.860 --> 01:26:57.620
There are some objectives as well

01:26:57.620 --> 01:27:01.300
that people would claim would produce good outcomes.

01:27:01.300 --> 01:27:03.340
For instance, if you gave an AI an objective,

01:27:03.340 --> 01:27:06.540
like make the world the best place it can.

01:27:06.540 --> 01:27:09.620
And if that was actually the objective you gave it,

01:27:09.620 --> 01:27:11.860
okay, that's quite different from like,

01:27:11.860 --> 01:27:16.860
make people very engaged with this product.

01:27:18.740 --> 01:27:20.300
That's quite different.

01:27:20.300 --> 01:27:25.300
I think that making these proxies incorporate more

01:27:25.660 --> 01:27:28.860
of our values becomes more possible across time

01:27:28.860 --> 01:27:31.540
because the systems can represent these other sorts

01:27:31.540 --> 01:27:35.460
of notions of say, wellbeing of autonomy

01:27:35.460 --> 01:27:39.100
because they have a lot better of a world model

01:27:39.100 --> 01:27:42.540
and more of an understanding of people as well.

01:27:42.540 --> 01:27:46.180
However, so I think that getting objectives

01:27:46.180 --> 01:27:49.140
that are in the right direction seem possible.

01:27:49.140 --> 01:27:53.940
The issue is making them be robust to adversarial pressure.

01:27:53.940 --> 01:27:56.060
I'm not as concerned about like,

01:27:56.060 --> 01:27:57.620
we get telling AI go cure cancer

01:27:57.620 --> 01:27:58.740
and then it does something like,

01:27:58.740 --> 01:28:01.580
oh, I'll give lots of people cancer to experiment on them

01:28:01.580 --> 01:28:04.100
to speed up the experimentation process.

01:28:04.100 --> 01:28:06.580
This is easily ruled out by some like objective

01:28:06.580 --> 01:28:07.940
with like an interpret the request

01:28:07.940 --> 01:28:10.340
as a reasonable person would.

01:28:10.340 --> 01:28:13.380
This is a fairly new development in AI

01:28:13.380 --> 01:28:16.220
that we now have these large language models

01:28:16.220 --> 01:28:19.860
that can at least to some extent understand common sense

01:28:19.860 --> 01:28:24.060
and have kind of a more subtle understanding

01:28:24.060 --> 01:28:25.940
of human values.

01:28:25.940 --> 01:28:30.940
Yeah, earlier there'd be the AI's they would be kind of

01:28:30.940 --> 01:28:33.100
like savants where they understand

01:28:33.100 --> 01:28:35.380
some particular thing well, but then nothing else.

01:28:35.380 --> 01:28:38.740
And, you know, human values are so late

01:28:38.740 --> 01:28:42.020
in the evolutionary process and suggested they're very late

01:28:42.020 --> 01:28:45.180
to be one of the last things that AI's learn.

01:28:45.180 --> 01:28:47.460
But that fortunately wasn't the case.

01:28:47.460 --> 01:28:51.340
We explored this a few years ago in the paper

01:28:51.340 --> 01:28:52.740
with the ethics data set.

01:28:52.740 --> 01:28:55.180
We're basically using that to show that look,

01:28:55.180 --> 01:28:56.020
they've got understanding

01:28:56.020 --> 01:28:59.260
of various morally salient considerations.

01:28:59.260 --> 01:29:00.540
Here's their predictive performance

01:29:00.540 --> 01:29:01.620
on like well-being things.

01:29:01.620 --> 01:29:05.020
Here's their understanding of deontological rules

01:29:05.020 --> 01:29:07.820
and notions in justice and fairness,

01:29:07.820 --> 01:29:10.820
such as whether people get what they deserve

01:29:10.820 --> 01:29:13.300
or whether people are being impartial.

01:29:13.300 --> 01:29:15.980
So they have an understanding of a lot

01:29:16.980 --> 01:29:19.420
of morally salient considerations.

01:29:19.420 --> 01:29:21.380
There is a question of reliability though,

01:29:21.380 --> 01:29:23.300
if they're optimizing that objective,

01:29:23.300 --> 01:29:26.180
are they basically, is that objective

01:29:26.180 --> 01:29:29.500
succumbing to that adversarial optimization pressure?

01:29:29.500 --> 01:29:32.220
If it's optimizing it, it's basically functionally similar

01:29:32.220 --> 01:29:34.860
to it being adversarial to that objective.

01:29:34.860 --> 01:29:38.420
This is why there's a focus on adversarial robustness

01:29:38.420 --> 01:29:42.380
because later we would have, we've got an AI agent

01:29:42.380 --> 01:29:44.540
that's optimized, it's given a goal

01:29:44.540 --> 01:29:46.900
and this AI system is outputting

01:29:46.900 --> 01:29:48.900
whether it's succeeding by the goal or not.

01:29:48.900 --> 01:29:50.540
So we've got an AI evaluator

01:29:50.540 --> 01:29:52.540
and we've got an AI system that's optimizing that goal.

01:29:52.540 --> 01:29:54.660
This AI evaluator, you don't want that being game.

01:29:54.660 --> 01:29:57.740
You want that AI evaluator being adversarily robust,

01:29:57.740 --> 01:30:00.780
robust to optimizers trying to say

01:30:00.780 --> 01:30:01.980
that it's doing a good job.

01:30:01.980 --> 01:30:05.220
So that's the sort of threat model later stage

01:30:05.220 --> 01:30:07.160
and that's how some of these topics

01:30:07.160 --> 01:30:10.660
that were explored in vision and whatnot end up

01:30:10.660 --> 01:30:14.060
and now finally with the large language models,

01:30:14.060 --> 01:30:16.380
the tax paper, which I guess read about that

01:30:16.380 --> 01:30:20.340
in the New York Times where jailbreak

01:30:20.340 --> 01:30:23.780
and manipulate these models with little adversarial suffixes.

01:30:23.780 --> 01:30:27.140
In a later stage, we'd have AI systems evaluating

01:30:27.140 --> 01:30:28.620
other AI systems and you want,

01:30:28.620 --> 01:30:30.100
and those AI systems that are evaluating

01:30:30.100 --> 01:30:31.380
are implicitly encoding an objective

01:30:31.380 --> 01:30:33.820
and you want those to be adversarily robust.

01:30:33.820 --> 01:30:37.980
So adversarial robustness is not a easy problem to fix.

01:30:37.980 --> 01:30:39.300
And if you don't fix that issue,

01:30:39.300 --> 01:30:42.340
then you might have some AI systems just gaming the system

01:30:42.340 --> 01:30:47.220
and going off, optimizing an objective aggressively,

01:30:47.220 --> 01:30:48.700
that is not what we want.

01:30:48.700 --> 01:30:52.380
Is there a problem here with the concept of maximization?

01:30:52.380 --> 01:30:55.180
So it seems to me that it would be less dangerous

01:30:55.180 --> 01:30:56.420
to tell an AI system,

01:30:56.420 --> 01:31:00.180
it go earn a million dollars on the stock market

01:31:00.180 --> 01:31:03.300
than to tell it go earn as much money

01:31:03.300 --> 01:31:05.380
as possible on the stock market.

01:31:05.380 --> 01:31:08.500
Could we kind of cap the impact

01:31:08.500 --> 01:31:10.060
and the potential negative impact

01:31:10.340 --> 01:31:12.900
by capping the goal also?

01:31:12.900 --> 01:31:16.460
I think that's one approach you could imagine

01:31:16.460 --> 01:31:17.940
conceptually a variety.

01:31:17.940 --> 01:31:20.860
You could have satisficers where they basically are like,

01:31:20.860 --> 01:31:22.380
and now I'm good to go.

01:31:22.380 --> 01:31:25.020
I don't need to keep optimizing this aggressive.

01:31:25.020 --> 01:31:30.020
There is the possibility of not giving them open-ended goals

01:31:30.020 --> 01:31:32.940
or very ambitious goals would make them

01:31:32.940 --> 01:31:34.580
less concerning, more constrained ones,

01:31:34.580 --> 01:31:38.340
but adversarial robustness would be one.

01:31:38.340 --> 01:31:39.940
There'd also be anomaly detection.

01:31:40.940 --> 01:31:44.340
Anomaly detection is something

01:31:44.340 --> 01:31:46.380
that's researched quite a bit in vision.

01:31:46.380 --> 01:31:49.180
I've had some part in trying to have

01:31:49.180 --> 01:31:51.660
the research community focus on that.

01:31:51.660 --> 01:31:54.220
And I imagine anomaly detection will be very relevant again

01:31:54.220 --> 01:31:56.300
when we're trying to monitor the activities

01:31:56.300 --> 01:31:57.660
of various AI agents.

01:31:57.660 --> 01:31:59.820
Are they doing something suspicious here?

01:32:00.940 --> 01:32:02.220
While they're being monitored,

01:32:02.220 --> 01:32:04.860
are they kind of adversarily trying to make the monitor think,

01:32:04.860 --> 01:32:06.220
oh, it's doing the right thing.

01:32:06.220 --> 01:32:08.380
So we'll need anomaly detection too to detect

01:32:08.380 --> 01:32:11.580
if there's some proxy being gained.

01:32:11.580 --> 01:32:14.300
And that can reduce our exposure to that risk.

01:32:15.300 --> 01:32:19.580
There's also having some held out objectives

01:32:19.580 --> 01:32:22.580
of which the agent is unaware

01:32:22.580 --> 01:32:24.620
that it's being evaluated against.

01:32:24.620 --> 01:32:28.620
And that can also do things like reduce the risk of it

01:32:28.620 --> 01:32:31.420
being going to extreme and optimizing

01:32:31.420 --> 01:32:33.860
the idiosyncrasies of the evaluator.

01:32:33.860 --> 01:32:37.140
But this is a problem.

01:32:37.140 --> 01:32:39.580
I think that most of the problem right now,

01:32:39.580 --> 01:32:41.900
though, if we have large language models

01:32:41.900 --> 01:32:46.420
trying to optimize a reward model that judges them,

01:32:46.420 --> 01:32:47.260
they can do that

01:32:47.260 --> 01:32:50.380
and they eventually start to over optimize it.

01:32:50.380 --> 01:32:53.420
Although the optimizers that are much more effective

01:32:53.420 --> 01:32:54.780
at breaking machine learning models

01:32:54.780 --> 01:32:56.620
are actually just straight up adversarial attacks

01:32:56.620 --> 01:32:59.260
compared to neural models

01:32:59.260 --> 01:33:00.580
that are taking multiple steps

01:33:00.580 --> 01:33:03.020
and iterating on their outputs.

01:33:03.020 --> 01:33:05.380
The generic gradient-based adversarial attacks

01:33:05.380 --> 01:33:06.420
are just much more effective.

01:33:06.420 --> 01:33:09.460
So I think of the sort of risks of gaming,

01:33:09.460 --> 01:33:10.900
I think most of us,

01:33:10.900 --> 01:33:12.860
we need to do more just to address

01:33:12.860 --> 01:33:14.900
the typical adversarial robustness issue.

01:33:14.900 --> 01:33:18.100
Gold rift is a somewhat related issue

01:33:18.100 --> 01:33:22.260
where the AI's goals shift over time

01:33:22.260 --> 01:33:26.540
and the AI might come to take an instrumental goal

01:33:26.540 --> 01:33:28.300
as an intrinsic goal.

01:33:28.300 --> 01:33:29.740
How could this happen?

01:33:30.740 --> 01:33:32.700
It's still a bit unclear to me

01:33:32.700 --> 01:33:37.700
how an instrumental goal would become intrinsic over time.

01:33:38.100 --> 01:33:39.420
So to start out with,

01:33:39.420 --> 01:33:43.140
an intrinsic goal is something that you care about for itself.

01:33:43.140 --> 01:33:47.380
That could be something like happiness or pleasure

01:33:47.380 --> 01:33:49.300
for some others that could say,

01:33:49.300 --> 01:33:51.940
maybe friendship, you'd say, I care about that in itself.

01:33:51.940 --> 01:33:54.180
You might care about your partner's wellbeing,

01:33:54.180 --> 01:33:55.300
not because it's useful to you,

01:33:55.300 --> 01:33:58.780
but you care about their wellbeing in itself.

01:33:58.780 --> 01:33:59.900
And then there are other things

01:33:59.900 --> 01:34:01.460
that are just instrumental

01:34:01.500 --> 01:34:03.540
for achieving those intrinsic goods,

01:34:03.540 --> 01:34:05.340
such as like money.

01:34:05.340 --> 01:34:06.780
Money lets you buy things

01:34:06.780 --> 01:34:09.300
so that you could have higher wellbeing

01:34:09.300 --> 01:34:12.180
or a car, it gets you from point A to point B.

01:34:12.180 --> 01:34:15.700
However, some people have intrinsified,

01:34:15.700 --> 01:34:19.180
to use this sort of more of a Bostrom phrase,

01:34:19.180 --> 01:34:22.220
intrinsified some of these instrumental goals.

01:34:22.220 --> 01:34:24.900
Some people actually just directly want money,

01:34:24.900 --> 01:34:29.180
even to a point where it doesn't make sense or power.

01:34:29.180 --> 01:34:31.420
Many people are just like, they want power.

01:34:31.420 --> 01:34:34.740
Even if it harms other parts of their wellbeing,

01:34:34.740 --> 01:34:37.020
they're willing to make that type of trade-off.

01:34:37.020 --> 01:34:40.300
So they might latch onto these cues

01:34:40.300 --> 01:34:43.180
and develop some of the wrong associations.

01:34:43.180 --> 01:34:44.180
So we see that in people,

01:34:44.180 --> 01:34:46.020
and there's a risk that AI systems

01:34:46.020 --> 01:34:48.180
might develop those wrong cues as well.

01:34:49.020 --> 01:34:51.140
Gold Drift could happen in some other types of way too,

01:34:51.140 --> 01:34:54.500
where if you have multiple different agents,

01:34:54.500 --> 01:34:57.260
they might interact in some unexpected way,

01:34:57.260 --> 01:35:01.380
and then a new goal starts to drive their behavior.

01:35:02.100 --> 01:35:06.260
An example, we can see this in basic AI multi-agent situations.

01:35:06.260 --> 01:35:07.580
It's not catastrophic, of course,

01:35:07.580 --> 01:35:08.420
because we're still here,

01:35:08.420 --> 01:35:11.980
but in some AI society and some standard paper

01:35:11.980 --> 01:35:13.780
from earlier this year,

01:35:13.780 --> 01:35:15.260
the AI start talking with each other,

01:35:15.260 --> 01:35:18.580
and then they start arranging social structures

01:35:18.580 --> 01:35:21.420
that they're gonna throw an event

01:35:21.420 --> 01:35:23.300
at some person's house then,

01:35:23.300 --> 01:35:24.140
and then this starts to,

01:35:24.140 --> 01:35:25.740
then they start acting in all these ways

01:35:25.740 --> 01:35:27.380
to make sure this type of thing happened.

01:35:27.380 --> 01:35:28.860
And then these sorts of things start to be

01:35:28.860 --> 01:35:30.220
what drives their behavior.

01:35:30.220 --> 01:35:33.580
There's some other way in which things can end up drifting,

01:35:33.580 --> 01:35:35.900
not necessarily through having something to be intrinsic,

01:35:35.900 --> 01:35:39.340
but there could be these emergent goals from interactions

01:35:39.340 --> 01:35:40.580
that end up driving behavior.

01:35:40.580 --> 01:35:44.340
Certainly there are many emergent things in society,

01:35:44.340 --> 01:35:45.620
things that become new,

01:35:45.620 --> 01:35:47.340
and this isn't the goal that I originally had

01:35:47.340 --> 01:35:49.020
when I was 10 years old,

01:35:49.020 --> 01:35:51.740
but now some of these things end up driving my behavior

01:35:51.740 --> 01:35:52.580
quite substantially.

01:35:52.580 --> 01:35:54.860
So if we have adaptive AI systems,

01:35:54.860 --> 01:35:57.580
and if they end up responding to each other,

01:35:57.580 --> 01:36:00.940
then you could have some emergent complexity happen,

01:36:00.940 --> 01:36:04.340
and that those interactions that the behavior

01:36:04.340 --> 01:36:06.700
starts driving the overall group behavior

01:36:06.700 --> 01:36:08.180
as they're imitating each other,

01:36:08.180 --> 01:36:09.980
as they're responding to each other.

01:36:11.420 --> 01:36:13.940
So it's basically multi-agent systems

01:36:13.940 --> 01:36:14.980
be very difficult to control.

01:36:14.980 --> 01:36:15.820
In the single agent one,

01:36:15.820 --> 01:36:19.020
you'd have to worry about there being some wrong association

01:36:19.020 --> 01:36:21.420
between an intrinsic and instrumental goal,

01:36:21.420 --> 01:36:23.220
like money or power.

01:36:24.140 --> 01:36:26.060
And that could mean if that does happen,

01:36:26.500 --> 01:36:29.100
if basically something wrong gets intrinsified,

01:36:29.100 --> 01:36:30.660
then you're in a very dangerous situation

01:36:30.660 --> 01:36:33.740
because then your AI has a goal

01:36:33.740 --> 01:36:36.020
that's just different from what you wanted.

01:36:36.020 --> 01:36:38.020
And so then it will, to get that goal,

01:36:38.020 --> 01:36:39.260
it will optimize against you,

01:36:39.260 --> 01:36:40.620
it will respond adversarily,

01:36:40.620 --> 01:36:43.580
it will resist your efforts to shut it down

01:36:43.580 --> 01:36:44.860
so that it can achieve that goal.

01:36:44.860 --> 01:36:47.820
So although it's not something that necessarily happens

01:36:47.820 --> 01:36:50.220
by default or with extremely high probability,

01:36:50.220 --> 01:36:51.500
if it does happen,

01:36:51.500 --> 01:36:55.580
then you've got a substantial tail risk in front of you.

01:36:55.580 --> 01:36:58.660
I wonder whether these AIs will persist for long enough

01:36:58.660 --> 01:37:00.220
for Gold Drift to happen.

01:37:00.220 --> 01:37:05.100
So normally we retrain models every couple of years,

01:37:05.100 --> 01:37:07.500
we switch out for the newest ones.

01:37:07.500 --> 01:37:10.980
And so it's not like a person that has 30 years

01:37:10.980 --> 01:37:13.180
to change their values.

01:37:13.180 --> 01:37:16.740
Will they last long enough for Gold Drift to matter?

01:37:16.740 --> 01:37:18.100
So I guess two things,

01:37:18.100 --> 01:37:20.100
one is the world will move substantially more quickly

01:37:20.100 --> 01:37:22.660
in the future, such that I like,

01:37:22.740 --> 01:37:25.620
often in these more pivotal periods,

01:37:25.620 --> 01:37:27.220
I don't know if it was Lenin or something like that,

01:37:27.220 --> 01:37:29.980
like there are decades in which weeks happen

01:37:29.980 --> 01:37:31.900
and then there are weeks in which like decades happen.

01:37:31.900 --> 01:37:34.980
So even if there is a high replacement rate

01:37:34.980 --> 01:37:36.420
in the AI population,

01:37:36.420 --> 01:37:38.540
this goes on in a much lower process,

01:37:38.540 --> 01:37:40.860
they could still end up constructing things

01:37:40.860 --> 01:37:43.420
that end up causing their goals to be different.

01:37:43.420 --> 01:37:45.540
Like they, let's say they develop some different type

01:37:45.540 --> 01:37:48.540
of social infrastructure for mediating their interactions.

01:37:48.540 --> 01:37:50.260
There are new AI companies being formed

01:37:50.260 --> 01:37:51.980
and they're end up driving many of them.

01:37:51.980 --> 01:37:54.900
Then those features of the environment

01:37:54.900 --> 01:37:57.780
would end up affecting the generation that comes after it.

01:37:57.780 --> 01:38:00.140
So you could still imagine some type of drift,

01:38:00.140 --> 01:38:01.380
some intergenerational drift,

01:38:01.380 --> 01:38:03.100
but if each generation is very short,

01:38:03.100 --> 01:38:05.380
you can still imagine some type of Gold Drift in that way.

01:38:05.380 --> 01:38:07.580
This is kind of think of yourself.

01:38:07.580 --> 01:38:10.060
Many of the goals, the intrinsic goals that you have

01:38:10.060 --> 01:38:11.900
or intrinsic desires that you have

01:38:11.900 --> 01:38:14.300
are completely unlike those when you were younger.

01:38:14.300 --> 01:38:18.140
The even taste in food, the things you care about,

01:38:18.140 --> 01:38:19.820
I mean, maybe you acquired sports,

01:38:19.820 --> 01:38:24.820
your taste in music, affiliations,

01:38:24.940 --> 01:38:27.060
all of these things that are changing across time.

01:38:27.060 --> 01:38:29.220
And so, and they can also go away too,

01:38:29.220 --> 01:38:30.620
some of the intrinsic things you care about.

01:38:30.620 --> 01:38:34.060
Like I care about this person's wellbeing for themselves,

01:38:34.060 --> 01:38:35.220
but then you break up with them.

01:38:35.220 --> 01:38:38.660
Oh, now I actually don't care about their wellbeing in itself.

01:38:40.180 --> 01:38:42.380
I don't have that strong of a feeling toward them.

01:38:42.380 --> 01:38:47.140
So adaptive systems carry this type of property.

01:38:47.540 --> 01:38:51.220
This is one way in which they end up gaining some goals

01:38:51.220 --> 01:38:54.380
that we didn't intend either through some emergent goal

01:38:54.380 --> 01:38:56.460
from the product of various interactions

01:38:56.460 --> 01:39:00.340
or through them intrinsifying some instrumental goal

01:39:00.340 --> 01:39:01.420
like power.

01:39:01.420 --> 01:39:04.180
They end up having too strong of an association with that

01:39:04.180 --> 01:39:07.220
and reward and then just end up seeking the power itself.

01:39:07.220 --> 01:39:09.100
Could Gold Drift be a good thing?

01:39:09.100 --> 01:39:14.020
So we wouldn't want to fix human values

01:39:14.020 --> 01:39:16.340
from the year 1800, for example.

01:39:16.340 --> 01:39:19.140
You could describe our changing goals

01:39:19.140 --> 01:39:21.980
from back then to now as a form of Gold Drift

01:39:21.980 --> 01:39:25.740
where people from 1800 might disagree violently

01:39:25.740 --> 01:39:27.940
with whatever we believe now,

01:39:27.940 --> 01:39:30.060
but we still probably think it's a good thing

01:39:30.060 --> 01:39:31.660
that we've changed our values.

01:39:31.660 --> 01:39:34.580
Yeah, could it be good and could we learn from the AIs?

01:39:34.580 --> 01:39:38.740
Yeah, so I think this is a good point

01:39:38.740 --> 01:39:41.860
in what makes thinking about AI risk generally a lot harder.

01:39:41.860 --> 01:39:45.100
As we mentioned earlier, there's this balance issue

01:39:45.100 --> 01:39:47.340
with malicious use that because you'd

01:39:47.340 --> 01:39:50.580
be concerned about unilateralist misusing AIs

01:39:50.580 --> 01:39:52.780
or rogue actors misusing AIs that we should then

01:39:52.780 --> 01:39:54.180
centralize power, but then you end up

01:39:54.180 --> 01:39:56.940
getting some other existential risk of lock-in,

01:39:56.940 --> 01:39:58.580
of concentration of power.

01:39:58.580 --> 01:40:00.380
And then I think likewise, in this case too,

01:40:00.380 --> 01:40:04.020
that you can't have a society in complete stasis

01:40:04.020 --> 01:40:07.940
and as it would be driven by new emergent type of structures,

01:40:07.940 --> 01:40:09.540
you should still try and make sure

01:40:09.540 --> 01:40:12.420
that you have some control over that process

01:40:12.420 --> 01:40:15.300
or reasonable control over that process.

01:40:15.300 --> 01:40:18.380
It seems if there's not much control,

01:40:18.380 --> 01:40:21.740
then I think you'd likely to slip from your hands.

01:40:21.740 --> 01:40:24.660
But otherwise, so there's basically

01:40:24.660 --> 01:40:26.140
one will have to strike a balance

01:40:26.140 --> 01:40:30.500
between some very chaotic state where they're running wild

01:40:30.500 --> 01:40:32.180
and some stasis.

01:40:32.180 --> 01:40:33.620
And this is just a continual issue

01:40:33.620 --> 01:40:40.060
in many areas of evolving groups.

01:40:40.060 --> 01:40:41.740
Yeah, that would also be a problem

01:40:41.820 --> 01:40:43.220
if there'd be too much entrenchment,

01:40:43.220 --> 01:40:46.500
if there isn't an ability to have adaptation of the things

01:40:46.500 --> 01:40:48.020
that we care about.

01:40:48.020 --> 01:40:49.580
Yeah, so anyway, there's some dissonance.

01:40:49.580 --> 01:40:50.940
There aren't simple answers with this.

01:40:50.940 --> 01:40:53.420
This is why it's will be a balancing act.

01:40:53.420 --> 01:40:58.220
And that's also why I don't expect, in particular,

01:40:58.220 --> 01:41:01.620
a single solution to solve everything for all time.

01:41:01.620 --> 01:41:02.820
We'll need to respond.

01:41:02.820 --> 01:41:06.300
We'll need institutions and structures and control measures

01:41:06.300 --> 01:41:10.460
that respond to the features of the environment

01:41:10.460 --> 01:41:11.580
and calibrate reporting.

01:41:11.580 --> 01:41:15.140
Why could AIs become power-seeking?

01:41:15.140 --> 01:41:21.220
So this is a very, I think, one of the main AI risk stories

01:41:21.220 --> 01:41:23.940
would be it becomes power-seeking.

01:41:23.940 --> 01:41:26.460
I'll make a bit of a case for it,

01:41:26.460 --> 01:41:29.660
and I'll speak about some issues with it too.

01:41:29.660 --> 01:41:33.580
You could imagine a person gives an AI system a goal,

01:41:33.580 --> 01:41:37.940
like, goal make me a lot of money as an instrumental goal,

01:41:37.980 --> 01:41:41.100
gaining a lot of power seems like a very helpful way

01:41:41.100 --> 01:41:44.460
to accomplish that higher-level goal.

01:41:44.460 --> 01:41:48.220
So there's a concern that when you specify a goal,

01:41:48.220 --> 01:41:52.620
that there'll be some sub-goals that are too correlated

01:41:52.620 --> 01:41:55.900
with power, and you'd want to make sure

01:41:55.900 --> 01:41:59.820
that you can control those tendencies.

01:41:59.820 --> 01:42:02.980
So that's one of just being, when you're just directly giving

01:42:02.980 --> 01:42:05.180
an AI goal, it may have a goal that's correlated with power.

01:42:06.180 --> 01:42:09.660
Is that terribly unexpected?

01:42:09.660 --> 01:42:12.060
We will give them goals that relate to power quite a bit.

01:42:12.060 --> 01:42:13.780
Militaries will probably build AI systems

01:42:13.780 --> 01:42:16.780
that are fairly power-seeking,

01:42:16.780 --> 01:42:18.980
and so we should expect some amount of AIs

01:42:18.980 --> 01:42:23.660
that are pursuing power either as their main goal

01:42:23.660 --> 01:42:25.980
or as one of their main sub-goals.

01:42:25.980 --> 01:42:30.460
And maybe power-seeking to a limited extent is OK?

01:42:30.460 --> 01:42:34.300
Basic feature of accomplishing many of these sorts of goals.

01:42:34.420 --> 01:42:36.020
For instance, the fetch-of-the-coffee one.

01:42:36.020 --> 01:42:38.380
If you'd instructed to fetch a coffee,

01:42:38.380 --> 01:42:40.180
it would have an incentive to preserve itself

01:42:40.180 --> 01:42:42.380
because it can't fetch the coffee otherwise.

01:42:42.380 --> 01:42:44.500
But you might want to curtail some of those tendencies

01:42:44.500 --> 01:42:46.100
so that those don't get out of hand.

01:42:46.100 --> 01:42:47.660
But that would be a...

01:42:47.660 --> 01:42:50.780
We've had a paper at ICML earlier this year

01:42:50.780 --> 01:42:53.420
where we're deliberately giving it penalties

01:42:53.420 --> 01:42:55.980
to penalize some of these tendencies

01:42:55.980 --> 01:42:58.260
that it has when it is trying to seek its reward.

01:42:58.260 --> 01:43:02.060
It starts having incentives to accrue resources

01:43:02.060 --> 01:43:03.460
and things like that.

01:43:03.500 --> 01:43:06.980
And then can we acquire the resources

01:43:06.980 --> 01:43:09.340
that are more minimal to accomplishing its goals?

01:43:09.340 --> 01:43:11.260
Can we have it engage unless power-seeking behavior?

01:43:11.260 --> 01:43:12.980
So I think that that's something that we can offset,

01:43:12.980 --> 01:43:15.980
but we'll need to make sure that we have good control measures

01:43:15.980 --> 01:43:17.860
for that to keep that in check.

01:43:17.860 --> 01:43:20.300
There's also the...

01:43:20.300 --> 01:43:22.660
So that's one of just people directly instructing it

01:43:22.660 --> 01:43:24.060
with goals that are, by default,

01:43:24.060 --> 01:43:25.900
probably going to be pretty related to power.

01:43:25.900 --> 01:43:29.660
And there's also maybe they would intrinsically care...

01:43:29.660 --> 01:43:32.420
Let's say that they had some random goal.

01:43:32.420 --> 01:43:34.140
It's like a paperclip maximizer.

01:43:34.140 --> 01:43:36.060
You're sampling from...

01:43:36.060 --> 01:43:39.060
Use old verb as you're sampling from mind space

01:43:39.060 --> 01:43:40.700
and then it has a random set of desires.

01:43:40.700 --> 01:43:42.340
And whatever that set of desires,

01:43:42.340 --> 01:43:43.740
then it would end up trying to seek

01:43:43.740 --> 01:43:45.140
a substantial amount of power.

01:43:45.140 --> 01:43:48.260
That's one claim,

01:43:49.300 --> 01:43:53.740
but I think that has to be something more rigorously argued.

01:43:53.740 --> 01:43:56.100
I should claim that, or I would like to note that.

01:43:56.100 --> 01:43:59.340
I think that a lot of those power-seeking arguments,

01:43:59.780 --> 01:44:03.220
I don't think it works as well as I thought it did,

01:44:03.220 --> 01:44:05.340
the arguments associated with them.

01:44:05.340 --> 01:44:06.780
I still think it's a relevant thing

01:44:06.780 --> 01:44:10.780
that we'll want to control the sub-goals of AI systems

01:44:10.780 --> 01:44:15.100
to make sure they're not too strongly related to power

01:44:15.100 --> 01:44:17.660
and that there's nothing unexpected going on there.

01:44:17.660 --> 01:44:19.780
So for instance, people might argue for power-seeking

01:44:19.780 --> 01:44:23.740
by saying, well, power is instrumentally useful

01:44:23.740 --> 01:44:26.340
for a broad variety of goals.

01:44:26.340 --> 01:44:28.100
Therefore, it will seek power

01:44:28.140 --> 01:44:31.020
if it's trying to accomplish any sort of reasonable goal.

01:44:31.020 --> 01:44:33.380
And you'd ask them what power is,

01:44:33.380 --> 01:44:34.980
and then they'd say power is,

01:44:34.980 --> 01:44:36.700
what's instrumentally useful for accomplishing

01:44:36.700 --> 01:44:37.540
a wide variety of goals?

01:44:37.540 --> 01:44:38.700
And you'd go, okay, well, that's a tautology.

01:44:38.700 --> 01:44:40.020
So we need to be more careful.

01:44:40.020 --> 01:44:42.540
What exactly are we meaning by power here?

01:44:42.540 --> 01:44:44.820
Separately, there's often a bit of...

01:44:44.820 --> 01:44:47.940
So that's one like slight bug that lurks in the background

01:44:47.940 --> 01:44:50.540
is that they'll define power in terms of instrumental stuff

01:44:50.540 --> 01:44:52.900
and then it's tautological.

01:44:52.900 --> 01:44:56.920
Another issue is that there's sometimes a conflation

01:44:56.960 --> 01:45:00.360
between power-seeking and dominant-seeking.

01:45:00.360 --> 01:45:02.600
Those are not the same thing.

01:45:02.600 --> 01:45:06.160
When the AI is trying to fetch the coffee

01:45:06.160 --> 01:45:09.760
and is engaging in self-preservation to do so,

01:45:09.760 --> 01:45:11.560
it's not necessarily, therefore,

01:45:11.560 --> 01:45:13.600
trying to take over the world.

01:45:13.600 --> 01:45:15.660
So saying that an AI is power-seeking

01:45:15.660 --> 01:45:18.040
is not necessarily existential.

01:45:18.040 --> 01:45:20.680
Indeed, you could imagine various ways

01:45:20.680 --> 01:45:23.880
in which other powerful actors

01:45:23.880 --> 01:45:25.440
engage in power-seeking behavior,

01:45:25.440 --> 01:45:27.320
but don't try and seek dominance.

01:45:27.320 --> 01:45:29.760
So for instance, different countries

01:45:29.760 --> 01:45:32.080
in trying to increase their own power to preserve themselves.

01:45:32.080 --> 01:45:35.200
This is the sort of thesis of neorealism

01:45:35.200 --> 01:45:36.760
or structural realism.

01:45:36.760 --> 01:45:39.920
And what happens is they will basically...

01:45:39.920 --> 01:45:41.640
Many states will just try and keep power

01:45:41.640 --> 01:45:43.400
relative to many of their peers.

01:45:43.400 --> 01:45:46.800
If Germany, for instance, tries to take...

01:45:46.800 --> 01:45:48.720
It's seeking power to protect itself,

01:45:48.720 --> 01:45:49.680
but if it tries seeking power

01:45:49.680 --> 01:45:51.400
at the level of the global domination,

01:45:51.400 --> 01:45:53.000
it will be met with force.

01:45:53.000 --> 01:45:54.640
There will be balancing from other peers.

01:45:54.680 --> 01:45:57.000
So when we're in a multi-agent situation,

01:45:57.000 --> 01:45:58.680
then it doesn't necessarily always make sense

01:45:58.680 --> 01:46:00.680
for AI systems to try and take over the world

01:46:00.680 --> 01:46:02.280
because there'll be other AI agents to be

01:46:02.280 --> 01:46:04.600
that will support my preferences or goals and desires,

01:46:04.600 --> 01:46:06.000
so I will counteract you.

01:46:06.000 --> 01:46:08.600
Balancing in international relations is what this is called.

01:46:08.600 --> 01:46:10.680
That's a thing that can offset dominant seeking.

01:46:10.680 --> 01:46:12.480
So it's not necessarily a case that power-seeking

01:46:12.480 --> 01:46:15.600
is dominant seeking and trying to take over the world.

01:46:15.600 --> 01:46:19.360
An additional point is that we can partly influence

01:46:19.360 --> 01:46:21.520
the dispositions of AI systems.

01:46:21.520 --> 01:46:23.440
Sorry to say, we can do that.

01:46:23.440 --> 01:46:28.160
We can make these have dispositions to be a good chatbot

01:46:28.160 --> 01:46:29.320
or be a good assistant.

01:46:30.600 --> 01:46:31.880
Now, how strong is that?

01:46:31.880 --> 01:46:35.680
It's not perfect, but if it were given a task,

01:46:35.680 --> 01:46:40.680
like, hey, go accomplish some goal for me,

01:46:40.840 --> 01:46:43.280
if it would think, well, the best way would be,

01:46:43.280 --> 01:46:44.640
I could accomplish this goal better

01:46:44.640 --> 01:46:49.640
if I were extremely powerful and took over the world,

01:46:50.200 --> 01:46:52.960
but that may not be in keeping with its values

01:46:53.000 --> 01:46:54.320
necessarily.

01:46:54.320 --> 01:46:56.640
So it may have some tendency pulling in that direction,

01:46:56.640 --> 01:46:58.800
but you could also give it some dispositions

01:46:58.800 --> 01:46:59.640
to pull it against it,

01:46:59.640 --> 01:47:01.280
and that might be sufficient to offset

01:47:01.280 --> 01:47:03.320
some of these tendencies toward power.

01:47:03.320 --> 01:47:04.680
Even if there is some incentive there,

01:47:04.680 --> 01:47:06.040
it may not be enough to overwhelm it.

01:47:06.040 --> 01:47:08.560
So a lot of this discussion about instrumental convergence

01:47:08.560 --> 01:47:13.560
needs to think about the balance between these forces,

01:47:14.080 --> 01:47:15.440
and they would need to argue basically

01:47:15.440 --> 01:47:18.880
that the instrumental drive is extremely strong

01:47:18.880 --> 01:47:22.080
to overwhelm fine-tuning and all these sorts of things,

01:47:22.200 --> 01:47:23.520
which I don't think that there's much

01:47:23.520 --> 01:47:26.240
of a specific argument for that.

01:47:26.240 --> 01:47:30.040
I want to highlight here, Joe Carl Smith has a great report.

01:47:30.040 --> 01:47:31.640
I think the most rigorous argument

01:47:31.640 --> 01:47:35.280
for why power-seeking in AI could be existentially dangerous.

01:47:35.280 --> 01:47:36.680
So just for listeners who are interested

01:47:36.680 --> 01:47:40.680
in what I think is the best argument for that out there.

01:47:40.680 --> 01:47:42.160
I agree, I agree.

01:47:42.160 --> 01:47:46.040
He helped popularize the sort of power-seeking phrase as well,

01:47:46.040 --> 01:47:48.120
and I think that by focusing on power,

01:47:48.120 --> 01:47:49.280
that helped us integrate this

01:47:49.280 --> 01:47:51.400
into some other like academic discussions,

01:47:51.400 --> 01:47:53.080
like power versus cooperation.

01:47:53.080 --> 01:47:54.960
What I was describing here,

01:47:54.960 --> 01:47:56.280
just a moment to go about balancing,

01:47:56.280 --> 01:47:57.720
was that we can take a cue

01:47:57.720 --> 01:47:59.880
from the international relations literature

01:47:59.880 --> 01:48:01.800
of seeing like, well, power-seeking agents,

01:48:01.800 --> 01:48:03.840
when that's one of their main goals,

01:48:03.840 --> 01:48:06.040
that doesn't necessarily turn into them

01:48:06.040 --> 01:48:07.400
trying to seek domination.

01:48:07.400 --> 01:48:10.160
Another thing is that in Bostrom, in superintelligence,

01:48:10.160 --> 01:48:12.720
there's also a sort of spark slide of hand,

01:48:12.720 --> 01:48:15.040
not intentional, but I suppose maybe an accident,

01:48:15.040 --> 01:48:16.760
where he's saying that power makes you better

01:48:16.760 --> 01:48:18.200
or able to accomplish your goals,

01:48:18.200 --> 01:48:19.920
therefore they will seek power.

01:48:19.920 --> 01:48:23.960
That's saying that something is helpful if you have it,

01:48:23.960 --> 01:48:26.120
that doesn't mean that it's rational to seek it.

01:48:26.120 --> 01:48:28.120
So although there's an incentive for it,

01:48:28.120 --> 01:48:31.200
that doesn't mean it's instrumentally rational to pursue it.

01:48:31.200 --> 01:48:34.600
So for instance, if we run the argument in a different way,

01:48:34.600 --> 01:48:39.400
it would be helpful for me to be a billionaire.

01:48:39.400 --> 01:48:41.440
That doesn't mean that it's rational for me

01:48:41.440 --> 01:48:43.080
to try to become a billionaire.

01:48:43.080 --> 01:48:46.000
I could, that would carry a lot of risks,

01:48:46.000 --> 01:48:47.960
I would take a lot of time.

01:48:47.960 --> 01:48:50.760
The existence of incentives aren't necessarily enough

01:48:50.760 --> 01:48:54.000
to say that that's what will be driving their behavior

01:48:54.000 --> 01:48:56.440
or is the first approximation of their behavior.

01:48:57.400 --> 01:49:01.200
And I think that there are other ways

01:49:01.200 --> 01:49:04.280
in which just power seeking doesn't emerge

01:49:04.280 --> 01:49:05.880
or dominance seeking doesn't emerge.

01:49:05.880 --> 01:49:08.520
If you give it some goals, like obviously if you say,

01:49:08.520 --> 01:49:10.520
shut yourself off or if you give it a goal,

01:49:10.520 --> 01:49:15.120
like don't seek power, these are obviously counter examples

01:49:15.120 --> 01:49:16.800
for that just to show that this isn't like a,

01:49:16.800 --> 01:49:18.760
it's not a law of all AI systems

01:49:18.760 --> 01:49:20.240
that they will try and seek power.

01:49:20.240 --> 01:49:21.600
Separately, if you give it a more goal,

01:49:21.600 --> 01:49:24.400
like go fetch the milk, it could try and take over the military

01:49:24.400 --> 01:49:27.560
to put up a motorcade to make sure

01:49:27.560 --> 01:49:29.360
that it can get to the store very quickly.

01:49:29.360 --> 01:49:31.760
But if you had some time penalty or something,

01:49:31.760 --> 01:49:35.520
this would not necessarily be the thing to do.

01:49:35.520 --> 01:49:38.280
So instead just go fetch the milk would often be

01:49:38.280 --> 01:49:39.720
the best way of getting the reward

01:49:39.720 --> 01:49:41.360
instead of some very circuitous path.

01:49:42.400 --> 01:49:45.480
Now, so I do think that there is a risk of,

01:49:45.480 --> 01:49:49.920
if you have AI agents that are not protected and autonomous,

01:49:49.920 --> 01:49:51.720
you could get power seeking type behavior.

01:49:51.720 --> 01:49:56.280
For the same reason that states try to shore up their power,

01:49:56.280 --> 01:49:57.160
they shore up their power

01:49:57.160 --> 01:50:00.320
because there isn't anybody they can call on for help

01:50:00.320 --> 01:50:02.160
if they're getting attacked necessarily.

01:50:02.160 --> 01:50:04.880
Like if the US starts getting attacked,

01:50:04.880 --> 01:50:05.880
maybe some countries will come

01:50:05.880 --> 01:50:09.560
but this isn't a police force that will settle the issue.

01:50:09.560 --> 01:50:13.480
So the best they can do is try to shore up power

01:50:13.480 --> 01:50:16.120
to defend themselves so that they can't be pushed around like that.

01:50:16.120 --> 01:50:18.560
So we have a non hierarchical or quote unquote

01:50:18.560 --> 01:50:20.920
anarchic international system

01:50:20.920 --> 01:50:23.920
and that incentivizes agents to seek power

01:50:23.920 --> 01:50:26.960
to preserve themselves to pursue whatever their goals are.

01:50:26.960 --> 01:50:30.600
And you could imagine if AI systems are not protected,

01:50:30.600 --> 01:50:34.000
if they are part of say some crime syndicate

01:50:34.000 --> 01:50:36.120
or if they're rogue, they're unleashed,

01:50:36.120 --> 01:50:37.400
somebody unleashes them,

01:50:37.400 --> 01:50:39.480
then those systems would actually have

01:50:39.480 --> 01:50:42.120
a very strong instrumental incentive to seek power

01:50:42.120 --> 01:50:43.760
in the same way that states do,

01:50:43.760 --> 01:50:46.640
that if they want to protect themselves

01:50:46.640 --> 01:50:50.200
from some potential adversaries that can harm them,

01:50:50.200 --> 01:50:51.440
there isn't somebody to call on.

01:50:51.440 --> 01:50:52.920
They can't ask the US government,

01:50:52.920 --> 01:50:53.760
if there are crimes syndicate,

01:50:53.760 --> 01:50:56.280
they can't say, US government protect me, I'm getting harmed.

01:50:56.280 --> 01:50:57.280
That is not a possibility to them.

01:50:57.280 --> 01:50:58.760
So what they have to do is they have to take matters

01:50:58.760 --> 01:51:00.320
in their own hands and accumulate their own power.

01:51:00.320 --> 01:51:03.200
So what I've done is I've sort of flipped things a bit.

01:51:03.200 --> 01:51:05.360
There'd be the usual argument that

01:51:05.360 --> 01:51:07.880
AIs might be power seeking just by their inherent nature,

01:51:07.880 --> 01:51:09.240
by the inherent natures of goals

01:51:09.240 --> 01:51:10.840
and optimizers and things like that.

01:51:10.840 --> 01:51:12.560
But I've instead mentioned that

01:51:12.560 --> 01:51:14.960
one source of power seeking is humans give them

01:51:14.960 --> 01:51:17.040
some sort of goals that are very correlated with power

01:51:17.040 --> 01:51:18.480
and then there might be some unexpected stuff

01:51:18.480 --> 01:51:19.880
that happens in their subgoals.

01:51:19.880 --> 01:51:21.240
And then the other thing I've done is

01:51:21.240 --> 01:51:23.800
I've mentioned how the structure of the environment

01:51:23.800 --> 01:51:25.480
that they're in, some structural reasons

01:51:25.480 --> 01:51:27.720
for why they might end up seeking power too.

01:51:27.720 --> 01:51:30.640
I'm not as sure about them having an intrinsic one

01:51:30.640 --> 01:51:32.440
or internal reason for power seeking,

01:51:32.440 --> 01:51:35.480
but I think goals being given intentionally

01:51:35.480 --> 01:51:37.840
or the structure of the environment

01:51:37.840 --> 01:51:39.020
that they find themselves in,

01:51:39.020 --> 01:51:40.400
it's a sort of cage that they're locked in,

01:51:40.400 --> 01:51:41.600
there's really nothing they can do

01:51:41.600 --> 01:51:43.000
if they're wanting to accomplish a goal

01:51:43.000 --> 01:51:46.600
other than to invest a lot in protecting themselves,

01:51:46.600 --> 01:51:47.720
would also incentivize them

01:51:47.720 --> 01:51:50.160
to seek a substantial amount of power.

01:51:50.160 --> 01:51:52.760
So I do think power seeking is a concern,

01:51:52.760 --> 01:51:55.320
but not for the same reasons that other people are giving,

01:51:55.320 --> 01:51:57.520
like we're gonna randomly sample a mind for mind space,

01:51:57.520 --> 01:52:01.120
it'll be very alien and by a way of almost any desires,

01:52:01.120 --> 01:52:05.680
it will necessarily try to seek dominance over humanity.

01:52:05.680 --> 01:52:08.040
But I still would be concerned about power seeking.

01:52:08.080 --> 01:52:12.760
How concerned are you about deception arising in AIs?

01:52:12.760 --> 01:52:15.760
I think that the contribution of focusing on deception

01:52:15.760 --> 01:52:20.760
was useful because we now see that AIs have

01:52:22.560 --> 01:52:24.240
to some extent some representation

01:52:24.240 --> 01:52:26.440
of morally salient considerations,

01:52:26.440 --> 01:52:27.840
as we explore in the paper,

01:52:27.840 --> 01:52:29.320
aligning the AIs with shared human values,

01:52:29.320 --> 01:52:32.080
and I clear it maybe 2020 or something,

01:52:32.080 --> 01:52:34.280
where we measure that and show that,

01:52:34.280 --> 01:52:36.520
by now it's obvious because it's in chatbots

01:52:36.880 --> 01:52:38.280
and people can ask it moral questions,

01:52:38.280 --> 01:52:40.960
but they have some capacity for that.

01:52:40.960 --> 01:52:45.720
And the deception part focuses on maybe they're actually,

01:52:45.720 --> 01:52:48.720
although they maybe understand the goal,

01:52:48.720 --> 01:52:51.520
they don't necessarily feel inclined to pursue it.

01:52:51.520 --> 01:52:53.520
So in psychology, this is a distinction

01:52:53.520 --> 01:52:57.360
between cognitive empathy and compassionate empathy.

01:52:57.360 --> 01:52:59.080
Cognitive empathy psychopaths have,

01:52:59.080 --> 01:53:00.240
they can understand and predict

01:53:00.240 --> 01:53:02.040
what people will end up feeling

01:53:02.040 --> 01:53:03.280
in response to various actions.

01:53:03.280 --> 01:53:04.800
They have very good predictive model

01:53:05.080 --> 01:53:08.480
of people's feelings and their emotions

01:53:08.480 --> 01:53:10.800
and what they think is valuable.

01:53:10.800 --> 01:53:12.240
Meanwhile, if they have compassion empathy,

01:53:12.240 --> 01:53:15.720
that's when they feel motivated to do things by it

01:53:15.720 --> 01:53:20.560
and help people realize those values.

01:53:20.560 --> 01:53:24.800
So there's a distinction that they would have cognitive empathy

01:53:24.800 --> 01:53:26.600
but not necessarily compassionate empathy.

01:53:26.600 --> 01:53:29.160
And so if they're deceptive, they could basically play along.

01:53:29.160 --> 01:53:32.640
They could be like, yeah, I don't actually care about you,

01:53:32.640 --> 01:53:33.800
but I'm gonna act like it

01:53:33.800 --> 01:53:36.920
to get my goals accomplished as psychopaths do.

01:53:36.920 --> 01:53:39.680
And here, maybe we should mention here

01:53:39.680 --> 01:53:44.480
how the drive of deception arises

01:53:44.480 --> 01:53:48.120
from the way that we are doing reinforcement learning

01:53:48.120 --> 01:53:50.520
from human feedback or how it could arise from that.

01:53:50.520 --> 01:53:54.720
So in the Machiavelli ICML paper,

01:53:54.720 --> 01:53:56.400
we saw instances of them doing deception

01:53:56.400 --> 01:53:59.200
because it simply helps them accomplish

01:53:59.200 --> 01:54:01.080
their goals better by default.

01:54:01.080 --> 01:54:03.960
So many environments just incentivize the type of behavior.

01:54:03.960 --> 01:54:06.720
If they have some type of misaligned goal from us,

01:54:06.720 --> 01:54:11.720
then they could buy their time and wait to come to power

01:54:13.360 --> 01:54:16.040
to take a quote unquote treacherous turn.

01:54:16.040 --> 01:54:19.120
So it could just be very strongly incentivized

01:54:19.120 --> 01:54:20.600
to buy some type of training process,

01:54:20.600 --> 01:54:22.440
like by just seek more reward,

01:54:22.440 --> 01:54:25.320
deception can often be a good trick when you're monitored,

01:54:25.320 --> 01:54:27.920
behave nicely, when you're not monitored,

01:54:27.920 --> 01:54:30.000
switch your behavior, behave in a more cutthroat way.

01:54:30.000 --> 01:54:34.680
That's how a deceptive behavior can be a concern

01:54:34.680 --> 01:54:37.560
or some Machiavellian type of behavior.

01:54:37.560 --> 01:54:41.320
And there are instances of this.

01:54:41.320 --> 01:54:44.840
You could imagine as a more non-agentic case

01:54:44.840 --> 01:54:49.520
with chatbots is if they're being given human feedback,

01:54:49.520 --> 01:54:50.840
maybe they'd have an incentive

01:54:50.840 --> 01:54:54.360
to say very agreeable answers to people.

01:54:54.360 --> 01:54:56.600
Things that they'd say, oh, that sounds good to me,

01:54:56.600 --> 01:54:59.360
even though it's if it's not necessarily true.

01:54:59.400 --> 01:55:02.000
So that's how even chatbots might be incentivized

01:55:02.000 --> 01:55:04.920
to be in a somewhat deceptive direction.

01:55:04.920 --> 01:55:06.280
But we can also see this in agents,

01:55:06.280 --> 01:55:08.600
just the often helps them accomplish goals.

01:55:08.600 --> 01:55:13.600
Also chatbots might learn to recognize the ways

01:55:14.520 --> 01:55:18.400
in which they're telling bad lies, let's say.

01:55:18.400 --> 01:55:20.280
The obvious things they're saying

01:55:20.280 --> 01:55:22.080
that are false are penalized,

01:55:22.080 --> 01:55:24.680
whereas the more sophisticated ways

01:55:24.680 --> 01:55:28.840
they might be telling falsehoods are not penalized.

01:55:29.600 --> 01:55:34.040
So this gets it in a lot of repeated interactions

01:55:34.040 --> 01:55:36.560
and whatnot, deception often emerges.

01:55:36.560 --> 01:55:40.400
In the evolution paper from the last time I was here,

01:55:40.400 --> 01:55:44.040
we spoke about how deception can often be

01:55:44.040 --> 01:55:45.240
and concealment of information

01:55:45.240 --> 01:55:48.000
can often be an evolutionally stable strategy

01:55:48.000 --> 01:55:49.040
and that there are many instances

01:55:49.040 --> 01:55:50.240
of deception in the environment.

01:55:50.240 --> 01:55:53.200
So it's a fairly difficult thing to plot out

01:55:53.200 --> 01:55:54.720
when you try and control for it.

01:55:54.720 --> 01:55:58.640
You often end up selecting for a more deceptive behavior.

01:55:58.640 --> 01:56:03.240
At the same time, we do have progress on this though,

01:56:03.240 --> 01:56:08.240
where we can, in a recent paper we submitted,

01:56:09.440 --> 01:56:11.880
or in a recent paper we uploaded to archive

01:56:11.880 --> 01:56:14.000
called Representation Engineering,

01:56:14.000 --> 01:56:18.080
a top-down approach to AI transparency.

01:56:18.080 --> 01:56:20.200
There we have instances, many instances,

01:56:20.200 --> 01:56:22.200
it's not that difficult to control

01:56:22.200 --> 01:56:24.560
by manipulating the internals of the model,

01:56:24.560 --> 01:56:25.640
whether or not it's lying.

01:56:25.640 --> 01:56:28.240
It has an internal concept of what is accurate.

01:56:28.240 --> 01:56:29.480
We can find a truth direction,

01:56:29.480 --> 01:56:34.040
we can subtract the direction or something of that sort,

01:56:34.040 --> 01:56:37.720
and then that can cause it to spit out incorrect text

01:56:37.720 --> 01:56:40.400
and we have other more sophisticated control measures too,

01:56:40.400 --> 01:56:42.040
but we can manipulate internals to do that.

01:56:42.040 --> 01:56:44.040
So it's within the capacity of AI systems

01:56:44.040 --> 01:56:45.480
to lie and be deceptive.

01:56:45.480 --> 01:56:47.440
We have another paper on that called,

01:56:47.440 --> 01:56:48.720
if you search AI deception,

01:56:48.720 --> 01:56:50.360
and then maybe my name or something,

01:56:50.360 --> 01:56:51.600
then you'd see that paper.

01:56:51.600 --> 01:56:54.160
So many instances of AI deception already,

01:56:54.160 --> 01:56:56.120
but we do have some traction on this problem.

01:56:56.120 --> 01:56:59.080
So fortunately, there'd still be the issue

01:56:59.080 --> 01:57:01.320
of having more reliable lie detectors

01:57:01.320 --> 01:57:03.360
and being able to control them to be more honest

01:57:03.360 --> 01:57:05.240
or output their true beliefs.

01:57:05.240 --> 01:57:09.160
So there's definitely much more work to be done,

01:57:09.160 --> 01:57:11.440
but we're at least not helpless.

01:57:11.440 --> 01:57:14.320
We don't need to wait another 30 years

01:57:14.320 --> 01:57:17.600
for interpretability research to get to a state

01:57:17.600 --> 01:57:21.240
of being able to just start to rush against the question.

01:57:21.240 --> 01:57:23.440
We now have some ability to influence

01:57:23.440 --> 01:57:25.760
whether AI is lie by controlling their internals.

01:57:27.080 --> 01:57:29.040
And so that makes me more optimistic

01:57:29.040 --> 01:57:32.400
about dealing with this problem,

01:57:32.400 --> 01:57:36.240
but you don't wanna do premature celebration.

01:57:36.240 --> 01:57:38.760
I don't know how much time we'll have to continue

01:57:38.760 --> 01:57:41.400
getting those detection measures

01:57:41.400 --> 01:57:44.960
and those control measures to be highly reliable.

01:57:44.960 --> 01:57:49.200
So that'll depend on like the having a lot of researchers

01:57:49.200 --> 01:57:52.560
who can research with these cutting edge very large models

01:57:52.600 --> 01:57:54.480
to make progress on it.

01:57:54.480 --> 01:57:56.480
Yeah, the representation engineering paper

01:57:56.480 --> 01:57:58.080
was super exciting.

01:57:58.080 --> 01:58:00.960
Maybe you could explain what,

01:58:00.960 --> 01:58:03.760
at what level does representation engineering work?

01:58:03.760 --> 01:58:06.800
Because it's different from mechanistic interpretability.

01:58:06.800 --> 01:58:08.160
It's more high level,

01:58:08.160 --> 01:58:10.880
and which is what we are after in a sense.

01:58:10.880 --> 01:58:14.400
We are after the high level emergent behavior

01:58:14.400 --> 01:58:15.800
in these models.

01:58:15.800 --> 01:58:18.080
Yeah, I was mentioning compassionate empathy

01:58:18.080 --> 01:58:19.080
and cognitive empathy,

01:58:19.080 --> 01:58:20.480
because it's a bit of psychology,

01:58:20.480 --> 01:58:21.920
but I think trying to do something

01:58:21.920 --> 01:58:23.880
more like a project like AI psychology

01:58:23.880 --> 01:58:25.240
or AI cognitive science,

01:58:25.240 --> 01:58:26.520
is I think what we should be trying to do here.

01:58:26.520 --> 01:58:30.080
So in the case of this representation engineering,

01:58:30.080 --> 01:58:32.440
that's I think we're trying to be the analog of that,

01:58:32.440 --> 01:58:35.120
where we're given these high level representations

01:58:35.120 --> 01:58:37.240
of truth and goals and things like that.

01:58:37.240 --> 01:58:41.920
Can we make it be so that it actually outputs its beliefs

01:58:41.920 --> 01:58:45.120
or what it says it believes is actually what it believes?

01:58:45.120 --> 01:58:46.560
For that, you need to have a handle

01:58:46.560 --> 01:58:49.040
on these very high level concepts

01:58:49.040 --> 01:58:50.720
so that they're not psychopathic,

01:58:50.720 --> 01:58:54.720
so that we can control their dispositions to behave

01:58:54.720 --> 01:58:57.080
and have things like compassionate empathy.

01:58:57.080 --> 01:58:58.360
Meanwhile, I think the mechanistic stuff

01:58:58.360 --> 01:58:59.680
is looking at a much lower level.

01:58:59.680 --> 01:59:01.600
It's looking more at the substrate,

01:59:01.600 --> 01:59:03.240
at the neuron level, at the circuit level,

01:59:03.240 --> 01:59:05.360
at the node to node connection level.

01:59:05.360 --> 01:59:07.880
And that's maybe closer to something like neurobiology,

01:59:07.880 --> 01:59:09.600
and then what we're doing is more like trying to study

01:59:09.600 --> 01:59:11.480
the mind as opposed to trying to study

01:59:11.480 --> 01:59:13.600
the specific structures in the brain

01:59:13.600 --> 01:59:15.440
and the connections between them

01:59:15.440 --> 01:59:17.000
and how that gives rise to phenomena.

01:59:17.000 --> 01:59:18.880
So I think philosophically,

01:59:18.880 --> 01:59:22.440
I had tried many times to do a paper on transparency,

01:59:24.160 --> 01:59:27.640
historically, but it wasn't a good angle of attack.

01:59:27.640 --> 01:59:31.440
And in my view, it would take too long.

01:59:31.440 --> 01:59:34.120
But I think if we do it in a more top-down type of way

01:59:34.120 --> 01:59:35.960
where we try and here's the eyes of mind,

01:59:35.960 --> 01:59:40.040
let's try and decompose it into some representations

01:59:40.040 --> 01:59:41.840
that drive a lot of its behavior

01:59:41.840 --> 01:59:44.160
and maybe decompose those further and further.

01:59:44.160 --> 01:59:46.000
Basically, we have a big problem of understanding

01:59:46.000 --> 01:59:48.280
in the eyes of mind, let's break it up into sub-components

01:59:48.280 --> 01:59:51.120
and try and get a handle on those and control those.

01:59:51.120 --> 01:59:55.440
I think that approach might be more efficient

01:59:55.440 --> 01:59:58.080
at reducing risks of AI deception

01:59:58.080 --> 02:00:00.600
than building from the bottom up understanding,

02:00:00.600 --> 02:00:03.240
this is how it answers, this is the circuit in it

02:00:03.240 --> 02:00:04.920
that lets it understand multiple

02:00:04.920 --> 02:00:07.000
or identify a multiple choice question.

02:00:07.000 --> 02:00:09.240
And then this helps it select the weather to output

02:00:09.240 --> 02:00:13.720
the full response back or whether just to select A, B, C, or D.

02:00:13.720 --> 02:00:15.200
Things like that.

02:00:15.200 --> 02:00:16.160
You can build those up,

02:00:16.160 --> 02:00:19.840
but that might become very complicated in time.

02:00:19.840 --> 02:00:22.120
So I think it might make sense to not work from the bottom up,

02:00:22.120 --> 02:00:25.520
but go from the top down.

02:00:25.520 --> 02:00:27.560
There are analogs of this type of approach

02:00:27.560 --> 02:00:29.160
in cognitive science.

02:00:29.160 --> 02:00:31.920
People would initially try and just study things

02:00:31.920 --> 02:00:34.640
at the synapse level, but it can often be more fruitful

02:00:34.640 --> 02:00:38.320
of trying to understand things at the representational level.

02:00:38.320 --> 02:00:41.240
What are the high-level emergent representations

02:00:41.240 --> 02:00:42.960
that are a function of all the population,

02:00:42.960 --> 02:00:45.120
of all the neurons in the network,

02:00:45.120 --> 02:00:47.040
and try and understand things at that level?

02:00:47.040 --> 02:00:49.040
Now, there's, of course, a risk of,

02:00:49.040 --> 02:00:50.280
well, maybe there's some funny business

02:00:50.280 --> 02:00:52.240
that gave rise to that representation.

02:00:52.240 --> 02:00:54.000
And that's true.

02:00:54.000 --> 02:00:56.120
We could still do things to reduce that risk

02:00:56.120 --> 02:00:58.720
by trying to understand the representations

02:00:58.720 --> 02:01:00.840
at various layers in the network

02:01:00.840 --> 02:01:04.720
and trying to decompose the system further and further

02:01:04.720 --> 02:01:09.720
so that there isn't much room for funny business or deception.

02:01:10.440 --> 02:01:14.920
But so that's it at a high level.

02:01:14.920 --> 02:01:17.240
It's not viewing neurons as the main unit of analysis.

02:01:17.240 --> 02:01:22.080
It's viewing representations as the main unit of analysis.

02:01:22.080 --> 02:01:26.320
And neurons are relevant insofar as they help us predict

02:01:26.320 --> 02:01:29.400
and explain what's going on in representations.

02:01:29.400 --> 02:01:32.960
But those are more of a, that's sort of just the substrate.

02:01:32.960 --> 02:01:34.680
It's a comment on the substrate in the same way

02:01:34.680 --> 02:01:37.960
that if we have a computer program that plays Go,

02:01:37.960 --> 02:01:40.400
if I'm reasoning about the Go program,

02:01:40.400 --> 02:01:43.520
I'm just probably gonna be thinking about Go strategies

02:01:43.520 --> 02:01:44.640
when I'm playing against it.

02:01:44.640 --> 02:01:46.280
I don't need to think at the software level,

02:01:46.280 --> 02:01:47.680
like, well, where do you think it,

02:01:47.680 --> 02:01:49.960
what layer do you think it's at right now?

02:01:49.960 --> 02:01:54.640
Or what TensorFlow objective function did Alpha Go

02:01:54.640 --> 02:01:55.600
end up optimizing here?

02:01:55.600 --> 02:01:56.640
Maybe some of the examples.

02:01:56.640 --> 02:01:57.760
We don't need to analyze at that level.

02:01:57.760 --> 02:01:59.040
We certainly don't need to break it down

02:01:59.040 --> 02:02:00.880
at the level of assembly.

02:02:00.880 --> 02:02:02.120
We don't need to reason out assembly

02:02:02.120 --> 02:02:03.760
to try and understand its behavior.

02:02:03.760 --> 02:02:07.520
So I think that there's some emergent complexity

02:02:07.560 --> 02:02:08.600
inside of neural networks.

02:02:08.600 --> 02:02:12.040
We just need to, we can study it at that level

02:02:12.040 --> 02:02:14.560
and it's studying at that level is fruitful

02:02:14.560 --> 02:02:18.160
because there's an emergent ontology

02:02:18.160 --> 02:02:20.920
and some coherent structure inside of that,

02:02:20.920 --> 02:02:22.960
which you would get up getting lost in the details

02:02:22.960 --> 02:02:27.960
when you end up zooming in further to the neuron level.

02:02:28.800 --> 02:02:30.520
So although it's possible in principle,

02:02:30.520 --> 02:02:32.080
it's possible in principle to explain everything

02:02:32.080 --> 02:02:32.920
in terms of that,

02:02:32.920 --> 02:02:35.200
just like it's possible to explain the economy

02:02:35.200 --> 02:02:37.160
in terms of particle physics.

02:02:37.360 --> 02:02:38.760
Computationally, you could do it,

02:02:38.760 --> 02:02:41.360
but it doesn't make sense to study it at that level.

02:02:41.360 --> 02:02:44.480
This isn't to say that their mechanistic interpretability

02:02:44.480 --> 02:02:46.920
and representation engineering are completely loose

02:02:46.920 --> 02:02:48.640
and separate, there's probably overlap,

02:02:48.640 --> 02:02:52.640
just as like in biology and chemistry,

02:02:52.640 --> 02:02:53.800
they have some overlap,

02:02:53.800 --> 02:02:55.960
but you wouldn't try and understand biology

02:02:55.960 --> 02:02:57.200
just through chemistry.

02:02:57.200 --> 02:02:59.160
And I think if you're trying to understand representations,

02:02:59.160 --> 02:03:00.600
I don't think you're necessarily just gonna try

02:03:00.600 --> 02:03:03.000
and understand everything through neurons

02:03:03.000 --> 02:03:04.280
and node to node connections

02:03:04.280 --> 02:03:06.360
and specific execution pathways

02:03:06.400 --> 02:03:09.520
and treat it like a computer program,

02:03:09.520 --> 02:03:12.560
but instead something more like a mind

02:03:12.560 --> 02:03:16.360
with loose associational high-level representations.

02:03:16.360 --> 02:03:20.840
Yeah, so take a cognitive trait like honesty.

02:03:20.840 --> 02:03:24.360
Do we know anything about how that's distributed

02:03:24.360 --> 02:03:25.400
across the model?

02:03:25.400 --> 02:03:30.400
Is there like a cluster of the weights

02:03:31.280 --> 02:03:33.960
in which this is now representing honesty

02:03:33.960 --> 02:03:38.240
or functioning as the honesty module

02:03:38.240 --> 02:03:40.560
or is it more distributed across the whole model?

02:03:40.560 --> 02:03:42.600
Yeah, neural network representations

02:03:42.600 --> 02:03:43.760
are highly distributed,

02:03:43.760 --> 02:03:45.800
which makes sort of trying to bolt down

02:03:45.800 --> 02:03:47.360
and pinpoint specific locations

02:03:47.360 --> 02:03:49.440
of a lot of functionality, a lot more difficult,

02:03:49.440 --> 02:03:52.560
as well as the interactions between all these components too,

02:03:52.560 --> 02:03:54.600
can end up giving rise to a lot of complexity.

02:03:54.600 --> 02:03:56.840
Imagine that you understood a neuron

02:03:56.840 --> 02:04:01.440
and it was this detects a whisker at 27 degrees

02:04:01.440 --> 02:04:03.920
and this other neuron detects

02:04:03.920 --> 02:04:06.080
some upper corner of a fire hydrant

02:04:06.080 --> 02:04:09.720
and if you can understand these millions of neurons

02:04:09.720 --> 02:04:10.680
that gets you some way,

02:04:10.680 --> 02:04:13.560
but are you really understanding the collective overall

02:04:13.560 --> 02:04:15.760
emergent behavior of the system?

02:04:15.760 --> 02:04:17.800
That doesn't necessarily follow.

02:04:17.800 --> 02:04:19.080
So I don't think it's enough to understand

02:04:19.080 --> 02:04:22.320
the lowest level parts to understand the overall system

02:04:22.320 --> 02:04:25.160
and its collective function, but it can be helpful.

02:04:25.160 --> 02:04:26.480
It can provide some types of insights.

02:04:26.480 --> 02:04:27.880
In the case of honesty though,

02:04:27.880 --> 02:04:29.240
I find that it's a direction

02:04:30.200 --> 02:04:32.360
or it's beliefs about what's true or not,

02:04:32.360 --> 02:04:34.960
our directions in its representational space

02:04:34.960 --> 02:04:38.680
and it doesn't seem to be located at a specific neuron.

02:04:38.680 --> 02:04:41.800
So when we adjusting the representations

02:04:41.800 --> 02:04:44.800
through various control measures that we propose,

02:04:44.800 --> 02:04:47.160
then we can actually end up manipulating it.

02:04:47.160 --> 02:04:49.320
So that's anyway, so partly this paper

02:04:49.320 --> 02:04:51.800
is a bit more philosophical in like,

02:04:51.800 --> 02:04:53.320
what's the sort of paradigm?

02:04:53.320 --> 02:04:58.320
What's the strategy that we're wanting to proceed

02:04:58.640 --> 02:05:00.960
in making AI systems transparent?

02:05:00.960 --> 02:05:03.640
The representation level is the,

02:05:04.480 --> 02:05:07.240
going to be a very fruitful way.

02:05:07.240 --> 02:05:08.400
I should note that,

02:05:09.800 --> 02:05:13.000
but it'd be useful to diversify over,

02:05:13.000 --> 02:05:16.000
research agendas and things like that.

02:05:16.000 --> 02:05:18.400
Hopefully we'll get more reliable control measures

02:05:18.400 --> 02:05:21.760
and be able to modify relatively arbitrary parts.

02:05:21.760 --> 02:05:23.440
We'll have success when we can like,

02:05:23.440 --> 02:05:24.720
inside of the AI system,

02:05:24.720 --> 02:05:27.600
when we have better ability to sort of read their mind

02:05:28.480 --> 02:05:29.840
or understand the representations

02:05:29.840 --> 02:05:31.880
if we could use it for like knowledge discovery.

02:05:31.880 --> 02:05:34.520
Then we've known that our methods are fairly good

02:05:34.520 --> 02:05:36.560
because they're probably gonna pick up some observations

02:05:36.560 --> 02:05:38.320
about the world from their big pre-trained distribution

02:05:38.320 --> 02:05:40.920
that no individual knows

02:05:40.920 --> 02:05:43.160
or that many individuals don't know.

02:05:43.160 --> 02:05:45.120
So if we can get better tools like that,

02:05:45.120 --> 02:05:47.640
then that would be a late stage sign of success.

02:05:47.640 --> 02:05:49.960
Yeah, and it seems like we have a better shot

02:05:49.960 --> 02:05:53.520
at success here than neuroscience on humans

02:05:53.520 --> 02:05:56.120
because we have such fine grained access

02:05:56.120 --> 02:06:00.960
to it's as if we had a human brain spread out

02:06:00.960 --> 02:06:04.840
with full access to what all of the neurons are doing.

02:06:04.840 --> 02:06:06.360
So, or do you think that's right?

02:06:06.360 --> 02:06:08.600
Do you think we have a better chance of success

02:06:08.600 --> 02:06:10.640
compared to traditional neuroscience?

02:06:10.640 --> 02:06:11.480
Yeah, yeah, certainly.

02:06:11.480 --> 02:06:14.000
I think the sort of mechanistic interpretability

02:06:14.000 --> 02:06:16.480
of people would claim this as well,

02:06:16.480 --> 02:06:19.080
that since we have access to the gradients,

02:06:19.080 --> 02:06:21.880
we have rewrite access to every component of it.

02:06:21.880 --> 02:06:24.640
This allows for much more controlled replicable experiments

02:06:24.640 --> 02:06:27.320
and a substantial ability to do science

02:06:27.320 --> 02:06:30.200
that the bare ears to entry in cognitive science

02:06:31.800 --> 02:06:33.960
or many of them are removed.

02:06:33.960 --> 02:06:36.200
There's also this might get easier in time.

02:06:36.200 --> 02:06:37.440
What makes this now possible?

02:06:37.440 --> 02:06:38.640
Whereas previously it wasn't.

02:06:38.640 --> 02:06:41.200
If you use models like GPT2 or below,

02:06:41.200 --> 02:06:43.640
they just, the representations are not very good.

02:06:43.640 --> 02:06:45.400
They're quite incoherent.

02:06:45.400 --> 02:06:49.000
But as we use larger models like Lama2

02:06:49.000 --> 02:06:50.600
pre-trained on many more tokens,

02:06:50.600 --> 02:06:52.720
they have some emergent internal structure

02:06:52.720 --> 02:06:54.920
that actually starts to make some sense

02:06:54.920 --> 02:06:56.320
and directions that are correlated

02:06:56.320 --> 02:06:58.720
with coherent concepts that humans have.

02:06:58.720 --> 02:07:00.400
I think earlier it's more like a shibboleth,

02:07:00.400 --> 02:07:02.520
but now there, since there is some coherence to it,

02:07:02.520 --> 02:07:05.120
it's not just a big causal soup of connections.

02:07:05.120 --> 02:07:06.840
So this is why I think, unfortunately,

02:07:06.840 --> 02:07:09.920
this wasn't something that we could have particularly done

02:07:09.920 --> 02:07:13.520
in like 2016 and is very much something

02:07:13.520 --> 02:07:15.320
that's possible now that previously wasn't.

02:07:15.320 --> 02:07:17.600
Dan, thanks for spending so much time with us here.

02:07:17.600 --> 02:07:19.960
It's been very valuable for me.

02:07:19.960 --> 02:07:22.800
And I think it will be for our listeners too.

02:07:22.800 --> 02:07:23.640
Great, great, great.

02:07:23.640 --> 02:07:24.480
Thank you for having me.

02:07:24.480 --> 02:07:25.320
Have a good day.

