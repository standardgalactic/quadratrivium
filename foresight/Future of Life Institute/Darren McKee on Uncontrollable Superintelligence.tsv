start	end	text
0	6000	Welcome to the Future of Life Institute podcast. My name is Gus Docker and I'm here with Darren McKee.
6140	14140	Darren is the author of the upcoming book Uncontrollable and he's the host of the Reality Check podcast.
14340	18340	He's also been a senior policy advisor for 15 years.
18400	19900	Darren, welcome to the podcast.
19900	22100	Hi Gus, pleasure to be here, fan of the show.
22300	30200	I had a great time reading your book and one of your goals with the book is to take a complex topic like
30200	35400	machine learning, AI research and in particular alignment research and then
35400	39900	present it in an accessible way. But how do you go about a task like that?
39900	43900	Well, it was a bit difficult and I think clarity above all else.
43900	50100	I've been following the AI developments for we'll say a decade or two, sometimes deeply, sometimes loosely
50100	55100	and I would read the other popular books and some of the articles and have discussions about all these things.
55100	61100	But it was really around April, May 2022 when I thought, oh, wow, things are really picking up.
61100	67100	And I think a lot of people felt that when the tacky list projection for AGI dropped about 10 years,
67100	70100	everyone's like, oh, oh, things are things are happening, right?
70100	77100	And at that point, I thought, okay, I think there's a gap between readily available materials that reach a wider audience
77100	82100	and the speed at which AI is progressing. And so I thought there's an opportunity here to write a book.
82100	87100	It's not that materials didn't exist at all. There's forum posts, there's blogs, there's lots of podcasts,
87100	92100	there's videos and so on. But I thought it'd be nice if it would be pulled together in a book, we'll say,
92100	96100	for people with no technical background, even no science background.
96100	102100	And so that's where some of my experience with the podcast or policy where you're trying to kind of do knowledge translation,
102100	107100	take a complicated idea, try to phrase it simply, excessively, and relate to an audience.
107100	112100	And with that as a context, the journey began, so to speak.
112100	117100	So since last June 2022, I've been trying to work on this book and I'm happy that it's done.
117100	125100	And I've tried to make a really concerted effort from the design of the cover to the table of contents to the chapters to how they flow.
125100	130100	It really is trying to put yourself in the mind of someone who is curious about AI.
130100	137100	They've seen these news headlines, they're interested, maybe concerned, confused, what the heck is going on with AI?
137100	141100	And that this book is for. So it's not so much for the technical people.
141100	146100	It's not for the people who've been in this AI safety debate for many years, although hopefully they'll get some value.
146100	152100	It's more in a way for the people they know that they can help perhaps explain some of the issues to them.
153100	161100	I've been reading about AI, I've been interested in AI for a long time and this book still provided a value to me.
161100	165100	So there's value in going over the basics again, I would say.
165100	170100	We're also in this conversation going to go over some of the basics again.
170100	177100	And there are interesting choices about how you frame different issues, which analogies you use and so on.
177100	181100	I'm interested in how do you balance accessibility with accuracy?
181100	184100	So I imagine that an expert is going to read this book.
184100	193100	How do you deal with that fact when you're writing for a broader audience, but you might get a nitpicky expert critiquing your book?
193100	196100	I think there's just going to be some inherent trade-offs.
196100	201100	The goal is to reach, as I said, as many people as possible because the experts, they already know these things.
201100	204100	They already have materials that are available more readily to them.
204100	208100	But for the average person, to have something really explained in a book form,
208100	211100	I think this is kind of the first of its kind.
211100	217100	It is entirely dedicated to the AI safety argument and tries to reach people as excessively as possible.
217100	223100	My own background, science and academia, I am also inclined towards accuracy and even precision
223100	226100	and trying to understand the difference between accuracy and precision,
226100	232100	but also understanding that for the audience, I'm trying to reach the difference between accuracy and precision isn't what matters.
232100	234100	What is the main idea?
234100	235100	Is there rigor?
235100	238100	Is what I'm saying generally true or understood to be true?
238100	240100	Is there evidence to support it?
240100	241100	Does it make sense?
241100	246100	But I did try to be, we'll say, a bit more flexible by how I might phrase certain things
246100	250100	and how I might use certain analogies to try to meet the audience where they are.
250100	253100	As you said, this field is very, very complicated
253100	258100	and you have to make concessions somewhere when you're explaining things to people.
258100	264100	What did you do when in your research, you came upon a topic on which the experts disagreed?
264100	268100	This is often the case in alignment research, for example.
268100	271100	The experts might disagree about the basics.
271100	274100	What do you do then when you're trying to explain the basics?
274100	277100	I think there's a couple of options there.
277100	282100	My approach was generally to try to give, I won't say that generally accepted consensus,
282100	286100	because there is a consensus of sorts, but not unanimity.
286100	288100	Everyone doesn't fully agree and no one ever does.
288100	290100	And that's true, let's be honest, for everything.
290100	295100	When you look at the surveys of what the philosophers believe, there's fundamental differences.
295100	299100	Same thing with economists, same thing with physicists, same thing with pretty much every discipline.
299100	303100	So understanding that, the book is trying to be a bit of a neutral observer,
303100	305100	but at the same time, I have a perspective.
305100	307100	I've looked at these issues.
307100	308100	I am concerned.
308100	313100	We're trying to figure it out together, but I'm giving reasons why I think AI safety is a concern.
313100	317100	To be more specific, I kind of tried to be sympathetic to all sides,
317100	321100	but at the same time, I probably didn't get too much into the weeds.
321100	326100	If people thought there's no concern at all, I might mention the uncertainty around everything,
326100	330100	but not so much giving that a lot of weight, because I think there are reasons to be concerned.
330100	335100	To take a step back, the book, the structure to try to give people a sense of how I did it,
335100	340100	is you look at the debates or discussions that are occurring in the AI risk, AI safety space,
340100	342100	and what do people seem to get stuck on?
342100	343100	What are their disagreements?
343100	344100	What are their cruxes?
344100	350100	What might be the underlying, we'll say, intellectual or even emotional dispositions
350100	352100	that are leading people to have certain beliefs?
352100	359100	And then identifying that, thinking about which part of the AI issue relates to it,
359100	365100	and then trying to think about an analogy or a simple way to explain the thing related to AI,
365100	368100	so it addresses the underlying issue in the future debate.
368100	372100	It's a bit complicated, but it's kind of like working backwards and then forwards.
372100	378100	So if I think one of the issues is it's really hard to imagine how powerful something like
378100	383100	an advanced AI or superintelligence could be, the first chapter of the book is a lot about
383100	387100	how powerful intelligence is or the importance of imagination.
387100	391100	Is it really that it's a failure of imagination that's driving a lot of this?
391100	393100	Well, not everything, of course, but I think it's a factor.
393100	399100	So then I'm trying to address aspects of how imagination might work or just open up people's minds.
399100	403100	Why don't we put it that way about what's possible before I even talk about AI?
403100	406100	Because it's a general issue about trying to be open-minded about what could happen.
406100	412100	And with that in place, hopefully later when there's more AI stuff, it's just less likely to be a problem.
412100	417100	It is still hard to imagine something as smart as an ASI or artificial superintelligence,
417100	421100	but we can try to at least acknowledge that there's something there.
421100	426100	There's something to be understood or even that we don't know exactly what it could be, and that's its own value.
426100	433100	So you write about how the range of intelligence extends much further than we might normally imagine.
433100	436100	What institutions do you rely on here?
436100	438100	How do you explain why is that?
438100	439100	Why is that the case?
439100	442100	Sure, I think the human brain is great.
442100	445100	I like mine, even though it's flawed, I don't know what I'd do without it.
445100	449100	But when we interact in the world, we have our kind of default settings.
449100	453100	We know there's various biases and the availability bias and other things that when you're asked a question
453100	459100	or you're quickly reading something, whatever pops in seems to be how you might think or feel about a subject.
459100	464100	If given a direct question by someone, you might reflect a bit more and think a bit differently.
464100	466100	But I'm trying to shift a bit out of that default.
466100	471100	So when we navigate the world, we kind of think of the intelligence mostly of our friends,
471100	475100	or I ask people to imagine like the smartest person they know, and they have that person in mind.
475100	478100	I'm like, okay, now imagine the smartest person you think ever existed.
478100	483100	Maybe it's Einstein or Mary Curie or Von Neumann or whoever it is.
483100	488100	And so right away, you're like, okay, well, I had the smartest person that I know personally and the smartest person ever.
488100	491100	Like, well, could there be even people who are smarter?
491100	494100	And I give some examples of savants who have incredible abilities,
494100	500100	whether it's memorization or processing information visually or auditorily, whatever it is.
500100	506100	And like, well, look, humans already can do some amazing things, which that smartest person you know,
506100	509100	and well, Von Neumann, maybe exception, can't really do.
509100	513100	So it's kind of trying to show, let's think about what might be possible
513100	518100	and then giving examples of what already is possible to help people understand,
518100	523100	okay, if this is possible and it's already happened, could we imagine a little bit more, right?
523100	524100	A little bit higher.
524100	527100	I also go the other way, looking at capacities of animals.
527100	533100	And we look at, you know, briefly, like in a page, it's, you know, birds and you have fish and you have dolphins and so on.
533100	537100	And we know broadly humans are more intelligent than these other creatures.
537100	541100	Not to say all humans, right, at all times of their lives than all the other creatures, of course,
541100	544100	but broadly as a general truth, that's the case.
544100	550100	And when we think about, say, why gorillas are in our zoos and we're not in theirs, it's not strength, right?
550100	551100	It's not our good looks.
551100	552100	It's not our charm.
552100	557100	It's because, again, collectively, we have an intelligence capability that is beyond theirs.
557100	562100	And that's another nuance that when I use intelligence, it's a bit more like anthropologist Joseph Henrichs,
562100	567100	sort of cumulative cultural collective capital in terms of intelligence.
567100	573100	It's very broad and encompassing because I thought that was probably the best way to try to communicate how important it is.
573100	576100	Yeah, why do you think we forget or at least this is my experience?
576100	579100	I'm intellectually aware of all of these examples you just gave.
579100	584100	I know there are people that are much smarter than me and I know there are savants and so on.
584100	591100	But it seems to me that when I think in normal life, I deceive myself into thinking that the range of intelligence
591100	600100	ends at my height, basically, that I represent the basic range of intelligence.
600100	602100	Do you think this is common?
602100	605100	Why is it hard to imagine abilities that are beyond us?
605100	606100	That's a great question.
606100	610100	I'm not sure of detailed research, but I do think it is somewhat common.
610100	613100	We kind of think we're the example, right?
613100	617100	This sort of mind projection fallacy or typical mind fallacy as it's thought.
617100	618100	I'm sort of the baseline.
618100	619100	Things are different than me.
619100	621100	Then that's how I calibrate them, right?
621100	626100	If we say someone is tall or short, sometimes that's in reference to ourselves,
626100	630100	but often it's sort of some general vague notion of the population.
630100	634100	Of course, if you're 6'5, 6'5", that makes you very tall,
634100	636100	unless you're in the NBA, then it's average.
636100	639100	And so we do understand there is a frame of reference, but again,
639100	642100	the default setting of human brains, again, they're great,
642100	646100	but we can't be thinking in complexity, that much complexity all the time.
646100	648100	Working space memory is tapped.
648100	653100	So if you're just trying to, I don't know, make dinner or you want to read an article to then think of like,
653100	657100	oh, there's 15 to 30 things I should always have in mind while I'm doing this.
657100	658100	That's great.
658100	661100	I applaud the effort and I try to have a couple of mine, but it's almost impossible to do.
661100	666100	So in that sense, the book can also serve to just remind people of things that they already know.
666100	672100	It's very hard to even for me to talk about like, well, do I have a 3, 350 page book memorized yet?
672100	676100	Not entirely, but the ideas are more frequently in there and they're more likely to come to mind.
676100	681100	So with intelligence, I think it really is just almost asking yourself more often,
681100	683100	am I making the right projection?
683100	685100	Is this a fair generalization?
685100	689100	Am I inadvertently benchmarking to the wrong set or the wrong baseline?
689100	698100	And in time, through human history is what I mean to say, humans now in various places are on average much more intelligent than they were in the past.
698100	703100	And that's mainly due to education and socialization, nutrition, diet, these sorts of things.
703100	709100	I think if you look approximately 100,000, 200,000 years ago, genetically humans are very similar.
709100	716100	Of course, if you go, you know, a million years ago, then it's quite different, but it's not staggeringly different compared to the other species entirely.
716100	726100	All that to say is that why humans are so capable now is because of our nutrition, our socialization and the fact we live in a world that does a lot of the work for us.
726100	729100	As I say, like, you know, can you build a fire?
729100	732100	Well, many people can and many people can't.
732100	735100	And what's worse is some of us can't build a fire even with matches.
735100	737100	We know what we're supposed to do, right?
737100	743100	You get some kindling, you get some paper, you get some lighter, you get some thicker wood and you have the matches and you do a decent job.
743100	745100	It starts and then it fizzles out.
745100	750100	And it's like, oh, I can't even build fire despite knowing how to do it and starting with matches.
750100	751100	Well, that's embarrassing.
751100	753100	Again, this is easily rectified if you practiced.
753100	762100	But I don't necessarily have to know this because there are matches and there are lighters and other people have figured this out for me so I can leverage the intelligence and the efforts of other people.
762100	764100	So I don't have to worry as much.
764100	769100	And perhaps similarly, it feels like your one is an individual super smart navigating through the world.
769100	777100	You're like, well, that's because everything else has been done for you right now with computers and phones, most of the complexity is behind the scenes.
777100	778100	And that's great for us.
778100	779100	You press a button, something works.
779100	786100	What actually happens, the staggering mind boggling complexity of information and ones and zeros going across space and time.
786100	788100	It's easy to just not think about it in a way.
788100	795100	Why would you if you're trying to watch a movie, you're not going to think of the matrix decoded, you could, but then you're going to ruin the experience for yourself.
795100	804100	Do you think we are giving our collective knowledge to AIs by training them on all available data online?
804100	813100	For example, are we thereby transferring the collective knowledge that we've built up that allows us to succeed in daily life?
813100	814100	I think we are.
814100	819100	It's almost like having a new child of sorts and you're socializing it the way you might a person.
819100	824100	So let's look at all what humanity has learned and we try to pass that on to a new generation.
824100	836100	And we're doing the same thing with AI just in a much more dramatic, complicated and, again, staggeringly vast way where there's, as you know, the amounts of data that
836100	841100	adabytes, zettabytes, I can't remember exactly what the number is that's going into these systems or would or could go into these systems.
841100	843100	I should say it's not quite that high yet.
843100	847100	But why not have all the world some knowledge available?
847100	849100	And it's only going to get better and better.
849100	852100	I think there's vast data sources that haven't been fully tapped, right?
852100	859100	So you think of all the video, which programs are now starting to mine, both you can just take the audio from it, which has its own value.
859100	863100	But even the movements, how people move, what they show and how that sort of thing happens.
863100	870100	That could be, you know, whether it's, you know, fixing a chair or the sink, but it's also how people navigate what they pay attention to and what they don't.
870100	877100	So once all the video gets recorded and all the radio and is all these different data sources that I think it'll be even more staggering.
877100	883100	So yes, we're kind of giving humanity everything that we've ever done that's in record over two AIs.
883100	889100	And that will have, of course, very fantastic, wonderful things, one hopes, but also a lot of risks and concerns.
889100	896100	Given our troubles with understanding other people's minds, how can we hope to understand AI systems?
896100	898100	Yes, I think that's a great question.
898100	902100	I am optimistic, but I think it's going to be definitely a challenge.
902100	906100	I mean, there's a broader idea here is that this is what we need to work on.
906100	909100	There are many things related to AI that are going to be very difficult.
909100	912100	And whether we're hopeful or not, we have to realize that we have to try.
912100	916100	We have to try to figure it out how they work, why they're doing what they're doing.
916100	920100	And yeah, it might be complicated, but humanity has figured out the impossible before.
920100	922100	That goes back to the imagination issue.
922100	926100	If you look at what we kind of take for granted now that humans have gone to the moon,
926100	930100	that we're having this conversation again across space and time relatively easily.
930100	935100	This is not only odd or unlikely or improbable.
935100	938100	This is impossible to people from the past.
938100	941100	And beyond that, I actually want to argue that it's unthinkable.
941100	947100	If you go far enough back, they didn't have the conceptual apparatus to even consider how difficult this could be.
947100	951100	And that's also a shift where you're thinking, okay, imagine gazing up at the moon
951100	954100	as a lot of humanity has done throughout our history.
954100	957100	Well, how do you get there? You can't just like climb a tree.
957100	964100	You can build a tall structure, as many did, and mountain, temple, these sorts of tall ladder.
964100	968100	But the idea that you could build a ship or a rocket and actually fly there,
968100	970100	that wouldn't even occur to people.
970100	972100	It just would have been out of their reach at the time.
972100	976100	And it's within our reach, again, just because we happen to be born at this time.
976100	978100	So it allows for what's possible.
978100	983100	Well, with the AI at the moment, it seems very, very difficult given how these systems are,
983100	987100	as people say, almost grown rather than built, to know exactly what's going on
987100	992100	given the complexity of artificial neural networks and the size of the parameters and all these models.
992100	996100	That said, I think there's a lot of good efforts to try to figure these things out.
996100	1001100	And that's exactly why we need like more investment in AI safety and more people to try to help us.
1001100	1008100	One hang-up people have about AI safety is thinking about the goals of AI systems.
1008100	1014100	So why is it that AI systems might develop a goals that are contrary to ours?
1014100	1018100	Why would they suddenly turn evil? It's a question you could ask.
1018100	1022100	There you talk about, you use the virus analogy.
1022100	1024100	So maybe you could explain how you use that.
1024100	1028100	So I think there's kind of a two-step process here, whether AI systems have goals or not.
1028100	1030100	And I'll take each in turn.
1030100	1032100	With the virus, why don't we just do that first?
1032100	1035100	A virus doesn't have goals, but it can still harm you.
1035100	1037100	And I think this is a great analogy.
1037100	1041100	And you can think of the, you know, sort of biological virus, but there's also computer viruses.
1041100	1045100	Biological viruses, depending on what they're, we know they're very hard to contain.
1045100	1049100	They can cause pandemics. They can cause illness, common flu, these sorts of things.
1049100	1054100	Computer viruses, people not in tech aren't aware that there's some still crawling the internet
1054100	1059100	that were developed years and years ago that we can't really stop, but that's the default world we're in.
1059100	1061100	So these things were created and they get out of control.
1061100	1066100	These things, computer and biological virus, it doesn't need a goal to harm you.
1066100	1072100	It doesn't need an intention. It's kind of just following a process in the biological sense and evolved mechanism.
1072100	1077100	Whether viruses are live as a debate, you don't need to get into, but it's trying to achieve certain objectives.
1077100	1082100	And you can describe it as if it has a goal, because it's easier to kind of navigate the world.
1082100	1084100	It reduces complexity.
1084100	1087100	But you could also argue, of course, it doesn't have goals. It's a virus. It's a little thing.
1087100	1089100	Why would a computer virus have a goal? It doesn't.
1089100	1094100	And in the book, I acknowledge this, but I kind of just go with the as if goals.
1094100	1099100	Let's not get into a protracted philosophical debate about whether something has goals or not.
1099100	1104100	If it acts as if it has goals and it's useful to treat it that way, then let's just do that.
1104100	1110100	It's much easier. And this is, for anyone curious, this is totally Dennett's intentional stance type stuff coming in here
1110100	1112100	because I'm a huge fan of philosopher Daniel Dennett.
1112100	1118100	And like, what are we trying to do here? We're trying to protect ourselves from computer viruses or biological viruses.
1118100	1121100	And you can think all the viruses trying to get you, the flu is trying to infect you.
1121100	1124100	When someone coughs or sneezes, it's trying to spread.
1124100	1126100	Well, of course, it's not trying to spread.
1126100	1131100	The organism has engaged in activity, which makes it more likely to replicate.
1131100	1135100	But to say that every time, you know, it just becomes very burdensome.
1135100	1140100	So sentence has become paragraphs, paragraphs, many paragraphs, and that's why we sort of circumvent it.
1140100	1145100	So that's the first bit about why it might have goals in an as if sense.
1145100	1152100	And then there's the other part where, OK, so if something becomes a bit more autonomous, does it come to have goals?
1152100	1158100	And I think, you know, we can look at different organisms again as an understanding that autonomy falls on a continuum.
1158100	1161100	Does something have a goal or not? Well, in some ways, yes, in some ways, no.
1161100	1165100	The chess playing program, does it want to win? Well, it sure acts like it wants to win.
1165100	1170100	And when I lose, I feel like it beat me versus like some computer code and some people designed it.
1170100	1173100	The designers definitely designed it to have a range of skills.
1173100	1175100	What could be does do worms have goals?
1175100	1176100	Do cats have goals?
1176100	1177100	Do dogs have goals?
1177100	1179100	Do orangutans have goals?
1179100	1184100	Well, of course, in some ways, they definitely engage in goal directed behavior because they're trying to achieve a certain objective.
1184100	1194100	And similarly with AI systems, we already have various, we'll say, automated programs that have autonomy and they need to have autonomy in some way to act in the world.
1194100	1198100	This would be the stock trading platforms that already function all the time.
1198100	1208100	Credit card, bank fraud detection, numerous other sort of monitoring of systems type algorithms and whatnot function on an automatic process because they couldn't keep checking with a human.
1208100	1219100	And even in the military, typically there is a human in the loop, but we know for say missile defense, these things are automated often because there's no way a human could react quickly enough to stop incoming missiles.
1219100	1223100	So it's not that whether computers become autonomous at all.
1223100	1226100	It's that to what degree are they going to become more autonomous?
1226100	1228100	And I think that is a much larger conversation.
1228100	1239100	But at the same time, you could see why there'll be incentives to have things become more automatic, which means a bit more empowered, which will then translate to seeming like they have goals.
1239100	1245100	And at one point, the seeming goals blends enough into the, well, that looks like it has goals that I'm just going to say it has goals.
1245100	1256100	If we return to the virus analogy, one thought I have there is whether the existence of the common cold, for example, the common cold is not very smart.
1256100	1258100	It's pretty dumb, I imagine.
1258100	1262100	And it's also very harmful and costly to society.
1262100	1268100	Does this mean that intelligence is not necessary to cause harm?
1268100	1272100	Is this a decoupling of intelligence and power in the world?
1272100	1273100	A great point.
1273100	1278100	So I think, as you just said, it's not a necessary condition in that case.
1278100	1284100	But that's different from whether intelligence is a factor that becomes correlated with the ability to cause more harm.
1284100	1293100	So you're right that a virus will think it's pretty simple, pretty straightforward, not that many moving pieces compared to other organisms, and it can cause drastic amounts of harm.
1293100	1298100	But you know, you're not going to go watch a movie with a virus and then talk about its thoughts after.
1298100	1299100	It's not that type of thing.
1299100	1307100	But as you scale up the abilities or the intelligence capabilities of different entities, they do have a different capability for harm.
1307100	1314100	So viruses can just infect you, but something with more intelligence could design viruses to infect you, design even worse viruses.
1314100	1320100	And even more dramatically, you know, well, if something's smart enough, it might be able to deflect an incoming asteroid and save us in a way.
1320100	1322100	So it's not just harmful, but it's the other side to the benefits.
1322100	1330100	So I think intelligence is a factor and in some cases it becomes a very significant factor in the ability to cause harm versus not.
1330100	1337100	As anyone who's like taking care of a child, a very young child can cause a lot of harm, but they're relatively, relatively containable.
1337100	1342100	If you have the right crib, if you have the right gates, they can usually be at least put in a certain place.
1342100	1346100	That's partly due to mobility and partly due to intelligence, same thing with various other animals.
1346100	1356100	But as you ranch it up the intelligence to sort of contain something or control it, it becomes increasingly difficult to the point where it's very, very difficult to contain some very intelligent entities.
1356100	1362100	Now, there are lots of definitions of artificial general intelligence out there.
1362100	1365100	And you use one in the book that I quite like.
1365100	1368100	Maybe you could talk about the average office worker, the average remote worker.
1368100	1373100	So artificial general intelligence or AGI, as people say, it's very much in the news, right?
1373100	1374100	How long till AGI?
1374100	1375100	What is AGI and so on?
1375100	1381100	And the approach I took in the book, again, for accessibility, I'm not trying to make the definitive definition.
1381100	1385100	I just want a good enough definition so people could understand what we're talking about.
1385100	1387100	And I thought in the most accessible way.
1387100	1391100	And with those factors in mind, okay, what are we talking about here?
1391100	1394100	It's usually someone's concerned about employment primarily.
1394100	1396100	And so I was thinking, could someone replace me at work?
1396100	1400100	And so if you kind of take the average coworker idea, that's the foundation.
1400100	1404100	If you're interacting with a coworker, say it's a remote coworker, right?
1404100	1406100	It's still focused on intellectual tasks.
1406100	1411100	So it's a computer system that, you know, does intellectual tasks as good as an average person.
1411100	1416100	And that's really the foundation where you could imagine if someone's performing as an average coworker level,
1416100	1419100	they're doing general tasks to a certain capability.
1419100	1426100	And therefore you could see why the person may be, sorry, that entity may be employable or might affect employment.
1426100	1429100	And it seemed to, I guess, as I said, hit home in the best way.
1429100	1434100	As a nuance here, why use that terminology at all is a question I asked myself.
1434100	1437100	Because like, well, do I need to introduce these terms to the average person?
1437100	1438100	Why, why not?
1438100	1439100	Well, they seem pretty popular, right?
1439100	1440100	That's one thing.
1440100	1441100	So you meet people where they are.
1441100	1444100	And I knew the world was going to talk about AGI in one way or another.
1444100	1449100	Now, is it better to say human level because it's almost the same in some ways?
1449100	1452100	But at the same time, we use the term AI a lot, right?
1452100	1455100	And that's sort of like, why did I use intelligence, artificial intelligence,
1455100	1458100	artificial general intelligence, artificial superintelligence is because I thought,
1458100	1462100	you know what, most people are going to talk about AI and people kind of know what AI is.
1462100	1467100	So once you accept artificial intelligence as a foundation to make things as easy as possible,
1467100	1470100	it's going down one in a way as intelligence broadly.
1470100	1473100	And then going another level is artificial general intelligence and then superintelligence.
1473100	1477100	And I thought that four part kind of structure was the best way to frame things.
1477100	1482100	I imagine that you decided to write the book out of some sense of urgency.
1482100	1485100	Now, you give a lot of uncertainty.
1485100	1489100	You present the uncertainty around what is called AI timelines in the book.
1489100	1496100	You must be motivated in some sense by urgency about the speed at which AI is developing.
1496100	1498100	Do you want to give us your AI timelines?
1498100	1504100	Do you want to tell us how long it is until we have AGI?
1504100	1506100	How long do we have left?
1506100	1509100	I don't necessarily think that. I want to be dramatic.
1509100	1513100	But so I think it's a great point and I'll elaborate a bit that I don't want to head.
1513100	1520100	So I think something like the average coworker, sure, it could be plausible within three years, two to three, four years.
1520100	1526100	I'm being a bit vague because I almost feel like that's not what's as important as the fact that there's a lot of good data
1526100	1528100	that shows capabilities are dramatically increasing.
1528100	1534100	This is the basics of more investment and computational power, better design of chips, new chips coming online
1534100	1537100	to the extent that NVIDIA's H100s are being shipped.
1537100	1542100	So the big players, open AI and DeepMind will probably start training their next-gen models on all these things.
1542100	1547100	And of course, a couple years after that, there's even more powerful chips that are planned by NVIDIA and all these designers.
1547100	1552100	So the path, the most likely outcome seems to be increasing computational power,
1552100	1556100	which then has some relationship to increased AI capabilities.
1556100	1561100	And we don't quite know how the input matches the output, which is itself a concern.
1561100	1562100	There's that uncertainty there.
1562100	1567100	But the trend lines are things are going to get more powerful and things are going to get more capable.
1567100	1573100	And because of that, it does seem like AGI or even artificial superintelligence, I think that's within 10 years.
1573100	1575100	I think that is entirely plausible.
1575100	1578100	Again, this is not 100% estimate, but I think it's plausible enough.
1578100	1584100	Now, in the regular listeners of the show might be like, well, did he mean 30%, 60% or 85%?
1584100	1586100	I understand the tendency for that.
1586100	1589100	I think the average person is like, well, is it going to happen or is it not?
1589100	1591100	It's like 10%, 50%, 100%.
1591100	1595100	So it's definitely never 100 because all knowledge is probabilistic and there's uncertainty everywhere.
1595100	1600100	But why I spent some time on the book with the uncertainty is the uncertainty is concerning.
1600100	1603100	It's not that things get better when we say we don't know.
1603100	1606100	I think sometimes people hear like, well, it could be two years away.
1606100	1608100	It could be 10 years away, 20 years away.
1608100	1610100	And they kind of just leave it at that.
1610100	1613100	Like, wait a minute, if you don't know, it could be two years away.
1613100	1617100	So how do we make decisions faced with uncertainty?
1617100	1618100	Decisions have to, right?
1618100	1621100	The world is full of complexity and uncertainty and decisions have to be made.
1621100	1626100	And it's tempting to think decisions don't have to be made, that you can just be agnostic.
1626100	1628100	It doesn't quite work that way.
1628100	1631100	Now, you know, this is more just me highlighting a complexity of the world.
1631100	1633100	Everyone can't care about every issue.
1633100	1635100	Everyone can't read up and be knowledgeable about every issue.
1635100	1640100	But I do want to highlight that if you happen to be in the AI space and you say we don't know,
1640100	1643100	that's almost more concerning than someone that says it's five years away.
1643100	1647100	Because if you actually mean we don't know tomorrow, a month from now.
1647100	1649100	And like, oh, no, no, I don't mean that.
1649100	1651100	I mean, you know, there's a 1% chance in the next 15 years.
1651100	1654100	Like, oh, okay, so you do have something, right?
1654100	1658100	I guess I'm trying to highlight that, again, the human brain, we go through life
1658100	1662100	and we're making probability based decisions in a very loose sense.
1662100	1664100	Whether you get a mortgage, whether you might have kids,
1664100	1666100	whether you get a certain insurance and for how long,
1666100	1669100	these are often based on some sense of what the future is going to be like.
1669100	1673100	Similarly, when we're engaging with AI stuff, we are making decisions
1673100	1675100	or not based on what the future is like.
1675100	1678100	Now, you could say there's a disconnect between what someone says
1678100	1680100	and what they do and behavior.
1680100	1681100	And that's true.
1681100	1684100	Sometimes we're dramatically inconsistent and then no human is immune from this, right?
1684100	1686100	I won't eat junk food and then I'm eating junk food.
1686100	1687100	What happened?
1687100	1690100	Well, clearly there's conflicting impulses within the human organism.
1690100	1693100	Okay, but I really just want to sort of highlight the complexity there
1693100	1697100	and have people reflect on, okay, I've said something.
1697100	1700100	Does my behavior match it in any way?
1700100	1702100	And if things are that uncertain,
1702100	1705100	isn't it better to be prepared than to be caught off guard?
1705100	1709100	Uncertainty isn't a justification for complacency.
1709100	1711100	It's another way to put it.
1711100	1712100	We can do something.
1712100	1717100	We can make plans and we should probably make plans for each timeline you mentioned.
1717100	1721100	In the book, you discuss some cognitive traits that AI might have.
1721100	1724100	And I think this was quite interesting.
1724100	1728100	And starting with speed, so AIs will think much faster than we do.
1728100	1730100	This is something that's often overlooked.
1730100	1735100	I find in debates, what does it mean if you have human level AI,
1735100	1739100	but it thinks a hundred or a thousand times faster than you?
1739100	1742100	Is it still reasonable to call it human level?
1742100	1744100	I'm happy you picked up on that.
1744100	1746100	Yeah, so this goes to the what is an ASI?
1746100	1749100	What kind of thing is it to try to help people along?
1749100	1753100	We don't quite know what an ASI might be or exactly how it will be,
1753100	1757100	but I was trying to have people understand what it might likely be.
1757100	1760100	And so there's a couple key traits and then a couple other possible ones.
1760100	1761100	So you have speed in there.
1761100	1762100	We'll talk about that first.
1762100	1766100	And there's capability, reliability and insight and these sorts of things.
1766100	1769100	But as again, just imagine someone's never talked about AGI
1769100	1771100	or they never thought about superintelligence like,
1771100	1773100	what the heck are these guys talking about?
1773100	1774100	It's a computer system.
1774100	1775100	What's it going to look like?
1775100	1778100	And yes, they'll probably draw on science fiction or popular movies.
1778100	1781100	And some of those are misleading and some of those are useful.
1781100	1783100	But it's like, what are we talking about?
1783100	1789100	So I thought very likely AI systems, ASI systems, AGI system will function very quickly.
1789100	1791100	And this is because computers generally work very quickly.
1791100	1793100	That's why they're so amazing.
1793100	1797100	Again, people listening to this, our ability to record this meeting and discussion.
1797100	1802100	It's because so many things are happening so fast in a way that our brains can't comprehend
1802100	1804100	that it just seems smooth, seamless and easy.
1804100	1808100	So with an AI system, as you've seen it, if you've ever used, you know,
1808100	1811100	one of the chatbots, the recent models or even the image generators.
1811100	1814100	Oh, let me think for a second and then out comes the output.
1814100	1816100	And it's usually remarkable, right?
1816100	1820100	It happens at a speed that no human could possibly generate in that amount of time.
1820100	1826100	And so I think there it's the flaw or the, let's say the limitation of any definition.
1826100	1828100	There's always going to be an asterisk where like,
1828100	1831100	well, in other contexts, it's not quite like this.
1831100	1833100	That famous Bertrand Russell quote,
1833100	1837100	everything is vague to a degree you do not realize until you've tried to make it precise.
1837100	1840100	The ability is the case because any word you can find like, well,
1840100	1841100	what does this word mean?
1841100	1844100	And you know, so nice people spend an entire thesis or a book defining a word
1844100	1846100	and then someone argues it's not that word and that sort of thing.
1846100	1850100	But anyway, so with a GI, I think if we think about a general ability,
1850100	1855100	we have to understand again that the coworker thing is useful as a sort of a test.
1855100	1856100	It's sort of a clear bar.
1856100	1860100	Now you could say, well, aren't there different types of coworkers and different types of environments
1860100	1863100	and some coworkers and entire even industries or companies in different countries
1863100	1865100	will be smarter or more capable than other ones?
1865100	1868100	Sure, but we have to have some way of talking about this.
1868100	1872100	Otherwise, we kind of get lost and at least I'm trying to be consistent in that way.
1872100	1877100	So if it was the case that your average coworker could do something a thousand times faster than you,
1877100	1881100	it doesn't quite seem like your average coworker anymore, right?
1881100	1883100	And then it's like, well, how do we deal with this then?
1883100	1887100	Like, well, when we examine people or give them evaluations in their workplace,
1887100	1889100	some people have strengths that other people don't.
1889100	1893100	And there is some sort of mishmash of like, well, are they insightful?
1893100	1894100	Are they analytical?
1894100	1895100	Are they on time?
1895100	1896100	Are they, you know, friendly?
1896100	1898100	Are they good to work with all these different things?
1898100	1900100	And it becomes an amalgam of an evaluation.
1900100	1904100	And similarly with AI or AGI will sort of do something similar.
1904100	1908100	I think in the practical sense that if there's a task and it comes back one second later
1908100	1912100	and your colleague would have taken three days, well, that's not quite human level, right?
1912100	1913100	That's above human level.
1913100	1918100	Now, if it has errors that the human never would have made, like, well, okay,
1918100	1921100	that makes it maybe a bit less in human level.
1921100	1924100	The AI gives you something, it takes you two days to fix it.
1924100	1926100	And another process would have taken three days.
1926100	1930100	Well, now it's above human level, but not as much as it seemed originally.
1930100	1935100	It seems to me that we are in front on the speed aspect with AI right now.
1935100	1941100	So, so AI is often, if we talk to GPT-4, you get an answer almost instantly.
1941100	1946100	The answer won't be as well done as a human expert could.
1946100	1949100	So it seems to me that we are in front on the speed aspect,
1949100	1954100	but we are behind on the kind of depth aspect as things stand right now.
1954100	1960100	Who knows with the upcoming models that might lead us into talking about AI insight,
1960100	1965100	which is kind of, you can think about it as the AI's understanding of its context
1965100	1968100	and the relationships it's engaging in.
1968100	1973100	And you expect the AI's to have greater insight and to understand their context
1973100	1977100	and understand their relationships perhaps even better than humans at some point.
1977100	1978100	Why is that?
1978100	1979100	Yes, that's a great point.
1979100	1983100	It's also something that gives me the, not the most concern, but it is a concern.
1983100	1985100	So we'll do like the first part first.
1985100	1989100	As you said before, if someone is super fast,
1989100	1992100	but they're not necessarily good at pattern detection or insight,
1992100	1996100	they're kind of missing a lot of what makes useful product services
1996100	1998100	or work in the broad sense.
1998100	2002100	And right now, we already have algorithms and whatnot
2002100	2005100	that can do sort of pattern detection in a way that a human just can't, right?
2005100	2008100	In a way, this is how most of the recommendation algorithms,
2008100	2011100	whether it's your Netflix or online social media,
2011100	2015100	it's processing vast amounts of data based on what you've given it in terms of clicks,
2015100	2018100	likes, even just moments spent looking at something
2018100	2021100	and then spitting out something you probably like to some extent or another.
2021100	2023100	If a human was doing this,
2023100	2026100	oh yeah, give me three months to analyze reams of data.
2026100	2030100	And then I think this guy might as a 70% chance of liking this one YouTube video.
2030100	2034100	That's actually impressive for the human, but not at all useful in the real world.
2034100	2038100	And so as they're designed right now, the systems are already,
2038100	2041100	we'll say, super good at pattern detection.
2041100	2046100	And there's even a story where some researchers were doing different analyses
2046100	2053100	and an AI was able to detect a patient's race from scans like X-rays of a person's body.
2053100	2055100	But the researchers couldn't figure it out.
2055100	2058100	They tried to realize or understand what the AI system was doing
2058100	2060100	because they would block certain parts of the anatomy.
2060100	2063100	Like maybe it's looking at the heart or something in the lungs
2063100	2064100	and they couldn't figure it out.
2064100	2068100	Now, maybe there is an answer because there has to be some sort of thing the AI was doing,
2068100	2072100	but it's already the case where AI systems can figure something out that humans cannot.
2072100	2075100	And even after it's done, humans still can't.
2075100	2079100	And I think, as you said before, the lag right now is not quite the speed or even the insight.
2079100	2083100	It's almost the integration, the flexibility, in a way the generality
2083100	2086100	that right now humans are very sophisticated general creatures.
2086100	2089100	So if you give them an intellectual task, they can do a wide range of things,
2089100	2091100	but also they do it in a certain way.
2091100	2094100	And the world isn't yet restructured.
2094100	2098100	Even how people talk to these machines, that has a certain nuance and a sophistication.
2098100	2102100	Sort of like, well, the book purposely does not deal with robotics
2102100	2103100	because I think that's a separate thing.
2103100	2104100	It's an interesting and amazing thing.
2104100	2108100	But an Amazon warehouse, you can't just stick a robot in there or automatic machines.
2108100	2112100	It has to be designed in a certain way to have a true efficiency and effectiveness.
2112100	2116100	Similarly, I think the chatbots, large language models, other AI systems in the future
2116100	2121100	will also provide great value, but they're not quite integrated in a way that works.
2121100	2125100	The second thing, when I said the insight concerns me a bit,
2125100	2130100	is again, if you take a big step back, how did humans manage to do what they do, right?
2130100	2135100	They managed to figure out how things work, like the laws of physics, how things fit together,
2135100	2137100	putting different objects and different pieces together.
2137100	2143100	We can sort of rearrange matter to create things, like cars, like houses, like computers.
2143100	2146100	And this is also inspired by David Deutsch, Beginnings of Infinity.
2146100	2149100	It's like, well, our brains just managed to figure it out.
2149100	2153100	There's all this raw material that was inside the earth, and we were able to take it out
2153100	2157100	and refine it and put it together, and now we have wonderful devices.
2157100	2161100	And that was hard and difficult, and I couldn't do it, but someone figured out groups of people,
2161100	2165100	teams of people, generations of people, and that's amazing.
2165100	2169100	And the concern for me sometimes is, what if an AI that's super insightful
2169100	2172100	can sort of see the universe in a way that we can't?
2172100	2175100	And it might unlock a truly new fundamental law of physics.
2175100	2176100	That's a possibility, though.
2176100	2180100	But it is the case, like, well, what if you just sort of tilt the world like this,
2180100	2185100	and like, oh, of course, of course, Wi-Fi, as a technology, it was always possible.
2185100	2187100	We just didn't know how to do it.
2187100	2191100	And of course, recently, people have shown you can use Wi-Fi as a way to sort of scan a room
2191100	2193100	and create models of where people are and how they move.
2193100	2195100	And that, of course, was also always possible.
2195100	2199100	The laws of physics haven't changed in our lifetime, and they haven't changed as far as, you know,
2199100	2201100	much in 13.8 billion years since the birth of the universe.
2201100	2204100	So it's really this insight that allows you to figure things out,
2204100	2208100	which if you understand the world and the universe more than other people,
2208100	2209100	you can do dramatic things.
2209100	2211100	You can create wonderful inventions.
2211100	2215100	You also have a huge advantage in terms of how to manipulate systems
2215100	2219100	or perhaps how to escape from systems, where someone else just didn't realize,
2219100	2223100	oh, if you manipulate the cooling fan in a computer,
2223100	2225100	you might be able to gain access to different things.
2225100	2226100	Again, that's pretty remarkable.
2226100	2228100	It was always possible until someone figured it out, though.
2228100	2229100	We didn't know.
2229100	2233100	Yeah, I do wonder whether AIs, advanced AIs,
2233100	2238100	will be able to solve some of the science problems where humans have been less successful.
2238100	2243100	So I think we've been successful in domains that are, in some sense, simple.
2243100	2247100	Not simple to understand, but can really be reduced to something simple, like physics.
2247100	2252100	As soon as we get into something very complex, like biology or psychology,
2252100	2254100	we haven't had the same level of success.
2254100	2259100	I wonder whether AIs are well suited to tasks where there's a lot of data
2259100	2263100	and complex data and whether they might be able to make progress
2263100	2266100	so that they have superhuman insight about our own minds.
2266100	2271100	And you could see how that would be dangerous to be exposed to manipulation
2271100	2276100	or otherwise being interacting with a system like that.
2276100	2277100	I think it could also be amazing, right?
2277100	2279100	That's how most of this stuff goes.
2279100	2280100	It cuts both ways.
2280100	2281100	The uncertainty cuts both ways.
2281100	2283100	I'm just for listeners.
2283100	2285100	I am a huge fan of science.
2285100	2286100	I have a massive science.
2286100	2289100	I've always been a big fan of science, but it is also limited, right?
2289100	2292100	We know there are humans in a way we're a bunch of apes trying to figure things out, right?
2292100	2295100	And I think we've done pretty well, but we know there's limitations.
2295100	2297100	There's replication crisis.
2297100	2301100	There's even just the reality of millions of scientific publications each year.
2301100	2302100	Who can read all these?
2302100	2304100	And so I years ago thought, you know what?
2304100	2307100	Humans should make a good, strong effort, but won't it be nice
2307100	2312100	if some AI system, pick your field, economics, psychology, biology, chemistry, whatever it is,
2312100	2313100	and they could read a thousand papers.
2313100	2314100	Like, you know what we need?
2314100	2318100	We need this particular type of experiment that addresses these four things
2318100	2320100	that these thousand didn't in the right way.
2320100	2324100	So that, you know, at the moment, it actually fills me with like more happiness.
2324100	2326100	Like, oh, we might actually really figure things out,
2326100	2329100	even though, of course, depending on what's figured out, it could be bad.
2329100	2333100	But there's so many complexities here that humans just can't easily disentangle.
2333100	2335100	This is the pattern recognition.
2335100	2340100	This is the insight that I really hope with AI systems, we can make some dramatic progress.
2340100	2345100	Like, okay, you know, that thing we thought might have had an effect size of, you know, a certain size or not.
2345100	2347100	It just isn't the case, or it kind of is.
2347100	2350100	Or it's a 1% thing that people just shouldn't pay much attention to,
2350100	2352100	given this other thing, which is much more important.
2352100	2357100	So I'm pretty happy at the moment of the idea of AI really improving science
2357100	2361100	while acknowledging that, depending on the field, that could also be a problem.
2361100	2366100	Yeah, I'm also pretty excited for seeing what AI can do in the science realm.
2366100	2371100	I wonder if we can create more systems like AlphaFold, DeepMinds AlphaFold,
2371100	2377100	that was able to figure out how proteins fold,
2377100	2383100	where it's a system that's narrow but highly capable in a specific domain
2383100	2386100	and helps us solve a longstanding science problem.
2386100	2391100	I do think the trade-off between risk and reward is pretty great for such systems.
2391100	2397100	Do you think that's a potentially responsible way forward to push hard on narrow systems
2397100	2399100	that help us with scientific problems?
2399100	2400100	I definitely do.
2400100	2404100	And as you pointed out, AlphaFold and other highly capable narrow systems,
2404100	2406100	it's amazing, it's amazing what they've done, right?
2406100	2408100	And, you know, we're just getting started type thing.
2408100	2413100	That said, we know the incentives for generality, general purpose systems is very high.
2413100	2418100	Whether we can kind of tweak people towards having like, look, you want a certain capability,
2418100	2422100	this is a particular problem, it's no way easier to solve if you just focus on it,
2422100	2424100	then why don't we focus on that?
2424100	2429100	That said, there's other problems where it turns out being general is better, right?
2429100	2432100	And I think there's a nuance and a delicacy here or a complexity
2432100	2434100	that we're just going to kind of see how things pan out.
2434100	2438100	I'm aware of Mustafa Suleiman, he has a sort of personal assistant AI
2438100	2442100	and their plan is not to make AGI, but a good personal assistant.
2442100	2447100	And I guess the question is, well, are you going to have to end up making something like an AGI
2447100	2450100	to have a personal assistant that is as good as the other personal assistants
2450100	2453100	that are more AGI like from other companies?
2453100	2455100	And it seems like that's probably the case.
2455100	2458100	It's kind of hard to think why wouldn't it go in this direction?
2458100	2461100	In terms of developing systems, as things get more capable,
2461100	2464100	again, the computational power that's coming on board is staggering.
2464100	2469100	It's not clear if it's even much work to kind of stick something in on the side, so to speak.
2469100	2472100	Do we need like a single purpose machine, so to speak,
2472100	2474100	or can it be multiple things in combination?
2474100	2478100	And I know some of the other AI labs are experimenting with these types of things.
2478100	2483100	And even if the machine or the AI is kind of isolated on its own and has certain capabilities,
2483100	2487100	we know once you connect it to numerous other applications through APIs or whatnot,
2487100	2489100	it becomes far more capable.
2489100	2492100	So I think it'd be nice if we can focus on narrow things,
2492100	2496100	and I think that's a good path forward and maybe even like the most reasonable one
2496100	2498100	in a risk-averse cautious sense.
2498100	2501100	I'm wary about the viability of it.
2501100	2502100	That said, we should still try.
2502100	2504100	And we're kind of just kind of see what plays out.
2504100	2509100	But I think even if you make something highly capable like Google did with Alpha Fold,
2509100	2510100	we're going to see how it goes.
2510100	2513100	I mean, Gemini, their product is supposed to come out, I think, within a month or two,
2513100	2515100	and that's supposed to be very agentic,
2515100	2518100	and it's kind of like Alpha Fold plus the other one if you hear the rumors.
2518100	2521100	And yeah, can you get some sort of, I know, amalgam Voltron thing?
2521100	2524100	We're like, well, it turns out in a two or three years,
2524100	2526100	it's actually relatively easy to stick all these things together.
2526100	2529100	And you thought you were doing five amazing narrow systems.
2529100	2534100	Well, I just created something that it figured itself out how to combine all these things.
2534100	2536100	And now we have what we were trying not to have.
2536100	2542100	So I think it's worth trying and also being wary and aware of the concerns that might happen if we combine them all.
2542100	2547100	What do you think the relationship is between generality and autonomy?
2547100	2552100	It seems easier, in my view, to avoid creating autonomous systems
2552100	2555100	or more autonomous systems if the systems are narrow.
2555100	2561100	Is there some sort of relationship in which when you push on generality, you also push on autonomy?
2561100	2563100	I do think they're very much linked, as you said.
2563100	2565100	I kind of have a similar view, I think,
2565100	2568100	that if you were having something that was designed for a specific task,
2568100	2571100	it can be less general and maybe less autonomous.
2571100	2574100	But as we talked about before, these things are nuanced
2574100	2578100	and there's overlapping aspects of, we'll say, the distributions and some things that aren't.
2578100	2583100	So with, say, credit card, bank fraud detection, that sort of thing, these are pretty narrow.
2583100	2587100	They're highly autonomous and they're not really that general, though, right?
2587100	2590100	And you could imagine something else that is pretty general at the moment.
2590100	2593100	Maybe most of the language models, they're pretty general.
2593100	2594100	They're not really autonomous.
2594100	2598100	You ask it to do something and if you didn't, it wouldn't do something on its own for the most part.
2598100	2600100	So those seem entirely disconnected.
2600100	2604100	So like everything in this world, there's kind of multiple overlapping distributions
2604100	2607100	where you could imagine things are correlated in some ways,
2607100	2610100	but in other examples, they may be pretty distinct.
2610100	2614100	I think there will be pretty high demand for autonomous systems.
2614100	2616100	You mentioned the AI personal assistant.
2616100	2619100	I think what consumers want from a such a system
2619100	2623100	is to simply tell it once to order the flight tickets to wherever
2623100	2626100	and never think about it again and just everything works out.
2626100	2629100	That sounds pretty great to me, at least.
2629100	2633100	What do you think about the commercial incentives for autonomy?
2633100	2634100	I think they're staggeringly high.
2634100	2635100	That's the best way to say it.
2635100	2638100	I fully agree that why wouldn't someone just want to...
2638100	2640100	Can't you do it for me?
2640100	2641100	Let's be honest.
2641100	2643100	We as humans want things to happen
2643100	2647100	and a lot of things that we do want to happen require administrative burden.
2647100	2649100	I hate filling out forms.
2649100	2652100	Even if it takes 10 minutes psychologically, it seems to take an hour.
2652100	2653100	There's a complete disconnect here.
2653100	2656100	So if someone could do that for me, something could do that for me,
2656100	2658100	yeah, give it more power.
2658100	2660100	So I think this really highlights...
2660100	2662100	We're getting at the notion of control
2662100	2665100	that I think if we're exploring how AI works
2665100	2667100	and how it's going to function in our world,
2667100	2671100	we will initially willingly give up control for integrated AI systems.
2671100	2674100	Yes, you can imagine the financial incentives are huge
2674100	2677100	to provide these products to people, the AI assistants.
2677100	2679100	They could be just normal, say personal assistants.
2679100	2680100	They could be therapists.
2680100	2683100	They could be companions, all rolled into one.
2683100	2686100	The military incentives, the governments have a desire to use them
2686100	2688100	to help citizens, maybe even for surveillance of citizens,
2688100	2690100	pros and cons all over the place.
2690100	2694100	So there's all these reasons why driving towards more AI,
2694100	2698100	good, helping consumers, helping businesses, helping governments,
2698100	2700100	do things that they want to achieve.
2700100	2702100	And as things become more and more integrated,
2702100	2705100	then it becomes difficult to disentangle them
2705100	2707100	or perhaps stop it before it gets long too far.
2707100	2708100	Say more about that.
2708100	2712100	Why is it difficult to disentangle after you've integrated?
2712100	2716100	Say people have begun using AI personal assistants.
2716100	2720100	That's not a high risk, let's say, application.
2720100	2723100	But why is it difficult to then stop using these systems
2723100	2725100	once you've begun using them?
2725100	2729100	So people kind of adapt to their world and their expectations change a bit.
2729100	2732100	And we can use an already existing case study.
2732100	2735100	So with the replica app, which allows people to have AI companions
2735100	2739100	and often form relationships, including romantic or otherwise,
2739100	2742100	they went through a sort of upgrade or reboot
2742100	2744100	or they're tweaked to their model and their code
2744100	2748100	such that explicit conversations and things were not allowed anymore.
2748100	2751100	And some of the users who built up relationships
2751100	2754100	with these artificial AI little systems felt devastated.
2754100	2757100	They felt like the person that they knew and even loved
2757100	2759100	is just gone and they were depressed.
2759100	2761100	And so here, once you start along a path,
2761100	2763100	it sometimes becomes very hard to shift out.
2763100	2765100	And we know this perhaps for people who thought,
2765100	2767100	well, that replica thing, I would never do that.
2767100	2769100	Most of us have a phone, a smartphone of some type.
2769100	2773100	And nowadays, it's pretty hard to not exist in this world without one.
2773100	2775100	The utility is just so high, right?
2775100	2777100	It's great. You have all these functionalities
2777100	2779100	and you can talk to people and I don't have to sell the phone on you,
2779100	2781100	but phones are great.
2781100	2784100	So great, in fact, that it's really hard to not have one.
2784100	2786100	So you could say, well, I could choose not to have a phone.
2786100	2787100	You could.
2787100	2791100	But at the moment, you'd actually be reducing your control by not having a phone.
2791100	2793100	But if you really start to think through the phone,
2793100	2797100	like, okay, well, the phone company could kind of just brick my phone and stop it.
2797100	2802100	At one point, Apple put that U2 album on many people's phones through the Play Store.
2802100	2805100	They realized they didn't have as much control as they thought.
2805100	2809100	You kind of need the internet or something like it for most of the applications on your phone.
2809100	2813100	When you're using internet, you're clicking, I agree, to user agreements, perhaps,
2813100	2816100	or agreement of cookies and tracking data all over the place.
2816100	2818100	So there's all these ways in which we're, again,
2818100	2822100	willingly giving up power and control for, again, other types of power and control.
2822100	2825100	We want to talk to our friends. We want to watch funny videos.
2825100	2827100	We want to listen to podcasts or watch them.
2827100	2828100	And that seems great.
2828100	2832100	But now, once we're here, if you said to people, okay, we're just shutting it all down.
2832100	2834100	You can't, like, listen to podcasts anymore.
2834100	2835100	You can't watch YouTube.
2835100	2839100	Or, like, sort of, like, more dramatically, is Facebook too big to fail?
2839100	2842100	Is it the case where, although a lot of people don't like Facebook for various reasons,
2842100	2845100	a lot of people really do, and it's billions of users,
2845100	2849100	is it the case that Facebook could just go down and people wouldn't be upset?
2849100	2851100	A lot of people have built their lives on it.
2851100	2852100	That's where their friends are.
2852100	2853100	That's where their photos are.
2853100	2854100	That's where their memories are.
2854100	2856100	I actually enjoy the memory feature.
2856100	2859100	What did I do on this day five, six, seven years ago?
2859100	2863100	And I appreciate that because, again, human brains are fragile.
2863100	2865100	I sometimes did fun things. It's nice to remember them.
2865100	2867100	I wouldn't have remembered it without the prompt from the algorithm.
2867100	2871100	So, in that sense, you sort of see that the incentive is very high.
2871100	2872100	It's going to be hard to opt out.
2872100	2874100	I'm not saying people should use social media all the time
2874100	2877100	or that social media doesn't have also lots of problems.
2877100	2880100	But if you think of, like, citizens in the world and what they would want
2880100	2884100	and how they would react, you could imagine it wouldn't go so well
2884100	2887100	if a government is just like, okay, we're just not going to let people do this anymore.
2887100	2890100	Or when people say they don't have internet access for a very short period of time,
2890100	2892100	that's usually very concerning for a lot of people.
2892100	2895100	If you just didn't have it for weeks and you weren't prepared,
2895100	2897100	this would be truly destabilizing.
2897100	2902100	Most people nowadays of a certain generation, they don't know where their friends live.
2902100	2903100	They don't know their friend's phone number.
2903100	2905100	They may not know their email off my heart, maybe.
2905100	2907100	And they don't know their last name.
2907100	2910100	So, you have all these things where, like, oh, yeah, that person,
2910100	2913100	they're handle on Twitter, they're handle on Instagram or who I interact with
2913100	2914100	and that's how you talk to them.
2914100	2917100	But if that went away, you would lose a vast social network
2917100	2920100	and almost have no ability to reclaim it in any way
2920100	2922100	if the infrastructure was taken out of place.
2922100	2926100	So, all this is these push-pull tensions where once things are in place,
2926100	2928100	it becomes very hard to disentangle,
2928100	2932100	and that makes them even more and more robust and desirable.
2932100	2937100	Yeah, we can think about, say, all the world's scientists came to us
2937100	2942100	and told us the internet is going to become very dangerous in 10 years.
2942100	2944100	We must shut the internet down.
2944100	2948100	Would the world be able to coordinate around this project?
2948100	2951100	Would it be possible for us to shut down the internet?
2951100	2954100	Given the uncertainties, given the different incentives
2954100	2957100	that different groups have, I think that would be extremely dangerous.
2957100	2959100	Now, the internet is not going to...
2959100	2963100	It doesn't represent a danger to us like AI might do.
2963100	2966100	But it's just thinking about when these systems have begun
2966100	2968100	become integrated into our society,
2968100	2972100	how difficult it is to step out of these systems again.
2972100	2975100	Sure, and that's actually why I chose that example in the book.
2975100	2978100	And when you're trying to think about artificial superintelligence
2978100	2980100	and why it's so powerful, if you talk to an average person,
2980100	2983100	one of the first things they say is, like, can't you just shut it off?
2983100	2985100	Why don't you just shut it down? What's the big deal here?
2985100	2988100	And I think the internet is probably the most useful analogy
2988100	2990100	as a comparator, as, again, an imagination device.
2990100	2992100	Like, okay, let's think through the internet.
2992100	2994100	Right now, the internet is very robust.
2994100	2996100	It is designed to be robust, right?
2996100	2998100	Because not only do the average person want it,
2998100	3000100	global commerce hinges upon it,
3000100	3002100	various municipal services, hospitals, everything else.
3002100	3004100	Like, so many things are now integrated into the internet.
3004100	3007100	As anyone who's had to know, like a smart home now is like,
3007100	3009100	what? I can't get into my house because the internet's down
3009100	3011100	or I can't control the temperature.
3011100	3013100	So, again, pros and cons once you're integrated, right?
3013100	3016100	All that to say is, you know, there's cables that are crossing the ocean
3016100	3019100	and many other places that enable the internet to happen.
3019100	3022100	And there is currently no way to just shut down the internet.
3022100	3025100	If, like, all the main governments agreed,
3025100	3028100	then maybe you could make a lot of progress in it.
3028100	3031100	But as I say in the book, there'd be a huge vulnerability.
3031100	3033100	Like, if you created an internet kill switch,
3033100	3036100	oh, that's a great opportunity for some terrorists to be like,
3036100	3037100	why don't I just press the button?
3037100	3039100	Because it would wreak havoc and cause a lot of damage.
3039100	3044100	So there's a huge incentive to not even have a kill switch available,
3044100	3047100	which is problem kind of one or problem and solution number one.
3047100	3051100	And the second part is, are we going to get agreement
3051100	3054100	from countries or representatives to then actually activate
3054100	3056100	such a kill switch in an emergency?
3056100	3058100	Like, you could imagine, okay, people, let's imagine,
3058100	3060100	let's sort of ideal world.
3060100	3061100	They've done their due diligence.
3061100	3062100	We've somehow created a kill switch.
3062100	3063100	It's very well protected.
3063100	3064100	It's not actually a threat.
3064100	3069100	And we even have a protocol that is designed with written clear reasons
3069100	3071100	when you would activate the kill switch or not.
3071100	3073100	And then it comes time to something bad happening.
3073100	3076100	And everyone looks at the agreement that everyone agreed upon,
3076100	3079100	the details, and like, well, I don't quite think .3 has been satisfied.
3079100	3081100	And someone else, like, of course it has.
3081100	3085100	And we have minutes to decide whether this is a good idea or not.
3085100	3087100	I could just see that also being a problem.
3087100	3090100	So this is the normal like human complicated making decisions together,
3090100	3092100	understanding the world differently.
3092100	3094100	And it's not that there's no hope at all,
3094100	3096100	but it just highlights that we'd have to do a lot of work in advance
3096100	3099100	and ensure that such a thing could exist if we needed it.
3099100	3101100	All it to say is to your original premise,
3101100	3104100	if the scientists presented clear enough evidence,
3104100	3106100	I would hope that we won't say everyone
3106100	3108100	because everyone doesn't agree on anything, right?
3108100	3111100	But there's enough people with enough resources and power
3111100	3114100	that the risk it would be clear enough that we should act
3114100	3117100	and put things in place to make such a thing happen.
3117100	3119100	With an artificial superintelligence,
3119100	3121100	one could also see a similar like, well, how would you shut it off?
3121100	3124100	Is it distributed in a way that even is worse than the Internet?
3124100	3126100	Like, you know, you could be on computer servers
3126100	3130100	in various places that are or are not connected to the Internet
3130100	3133100	or fragments of a ASI that could recombine it to something else.
3133100	3136100	So there's certainly a lot of reasons to be concerned.
3136100	3139100	And we can use the present case of the Internet
3139100	3143100	to see why it's going to be a challenge.
3143100	3147100	How reliable do you believe current AI systems are
3147100	3150100	and how reliable do you think they'll be in the future?
3150100	3153100	I think they are mixed and they will be mixed.
3153100	3156100	And so to elaborate on that, I think a lot of these systems,
3156100	3157100	again, they're amazing.
3157100	3160100	Certainly image generators, they're reliable or not, right?
3160100	3162100	You kind of put something in, you get what you get.
3162100	3165100	Do you get exactly what you want? Very rarely.
3165100	3168100	I find with at least image generators, if someone's looking over your shoulder,
3168100	3169100	they're like, that's amazing.
3169100	3171100	You're like, that's not quite what I wanted.
3171100	3172100	So there's a disconnect there.
3172100	3174100	But to the language models, I think they're generally great.
3174100	3176100	And then you have to fact check everything out.
3176100	3180100	There's already, you know, numerous examples of lawyers citing false precedents
3180100	3184100	that didn't exist, another case where I'm not sure which system,
3184100	3189100	but one of them was asked for examples of sexual harassment cases involving lawyers.
3189100	3193100	And with references, and it spit back this wonderful example of an individual
3193100	3197100	who was a professor who sexually harassed some student on a trip to Alaska
3197100	3200100	with citations from, you know, Washington Post or something like that.
3200100	3201100	The whole thing was fake.
3201100	3205100	The guy is a law professor, but he's never been there.
3205100	3206100	He never went on a trip to Alaska.
3206100	3208100	There's no incidents ever of sexual harassment.
3208100	3212100	So the problem is these systems are very, very confident and convincing
3212100	3215100	that the analogy of a very good improv comedy partner,
3215100	3218100	whatever game you want to play, it'll play along with you.
3218100	3221100	And in that sense, it can seem like it's doing more than it is
3221100	3223100	when it's kind of adapting to who you are.
3223100	3226100	With reliability, I think there's an interesting nuance here
3226100	3231100	that depending on the situation the AI is in, you may have to be like super extremely reliable.
3231100	3236100	Like, you know, 99.99999 is still not enough because if it's operating fast enough
3236100	3240100	and it's making a billion decisions a second, then even one in a billion,
3240100	3242100	you're like, okay, is that one problem every second?
3242100	3244100	That could be catastrophic.
3244100	3248100	Even if one in a thousand were the actual problem, you'd very quickly reach that threshold.
3248100	3252100	In other cases, you could imagine, well, if something's really super insightful,
3252100	3256100	like it's coming up with interesting scientific advancements
3256100	3258100	or ways of understanding the world.
3258100	3262100	Well, even if it was 10% reliable, like one in 10, even one in 100,
3262100	3264100	it might be still very, very valuable.
3264100	3267100	So I think it's going to be very mixed in terms of reliability.
3267100	3272100	But the main thing one would want is to have an understanding of how reliable it is
3272100	3274100	before it is deployed and used in any way.
3274100	3277100	Because you don't want something that you think is 99% reliable,
3277100	3281100	being 10% reliable, and the reverse, of course, because there's confusing things.
3281100	3283100	Again, we're going to kind of see how it plans out, right?
3283100	3286100	With the self-driving cars, I think we just saw crews say like,
3286100	3290100	oh, well, you know, we consult AI or some network of people in some ways
3290100	3294100	of four or 5% of the time, which sounds like a lot, but in another way is not.
3294100	3296100	If you look at, again, the vast majority of human invention,
3296100	3298100	but it's not doing what it's supposed to be doing.
3298100	3301100	How is reliability different from alignment?
3301100	3303100	How are these concepts different?
3303100	3307100	Is it just about getting the AIs to do what we want in both cases,
3307100	3309100	or how would you disentangle here?
3309100	3310100	That's a good question.
3310100	3315100	So I think alignment kind of breaks down into various different related issues
3315100	3316100	and problems, right?
3316100	3318100	Does the AI do what we want?
3318100	3319100	Like, who is we?
3319100	3321100	What are our values?
3321100	3322100	That sort of thing.
3322100	3323100	That is the alignment quagmire.
3323100	3325100	But you're definitely right.
3325100	3328100	There is certainly some overlap where if I've asked an AI for X
3328100	3331100	and it's delivering what seems to be X, that seems like it's reliable,
3331100	3333100	and therefore it seems aligned with what I want.
3333100	3336100	So in that sense, I would say, yeah, there's a lot of overlap between these terms.
3336100	3339100	That said, when we're thinking about alignment,
3339100	3343100	it's usually the broader, is this thing going to cause a problem in some way or another?
3343100	3345100	But, well, yeah, I wish she's actually seen them.
3345100	3346100	That was very similar.
3346100	3348100	So I'm appreciating that these are highly linked,
3348100	3350100	because if you think of alignment, it could go,
3350100	3352100	an AI system could be misaligned for several reasons, right?
3352100	3354100	It could be due to an accident.
3354100	3356100	It could be due to misuse by malevolent actors,
3356100	3360100	or it could be, you know, the AI itself becomes more capable, more power seeking.
3360100	3364100	And if it was reliable in any of those ways, it would kind of make it worse.
3364100	3367100	But with the accident example, it does really seem like,
3367100	3370100	well, if an AI is misfunctioning, it makes it not reliable.
3370100	3372100	So on the fly, I don't know if I'll commit to this,
3372100	3375100	but I'll think like maybe reliability becomes a bit of a subset
3375100	3378100	of the alignment accident issue.
3378100	3381100	And then of course, it would also relate to misuse.
3381100	3383100	In a way, these things are all very much connected,
3383100	3385100	and that's something that's in the book as well.
3385100	3388100	It's, you can't make one long chapter that's 3,000 pages,
3388100	3391100	so you kind of have to like, how can I put this into different chunks,
3391100	3394100	even though these things overlap and interrelate?
3394100	3399100	Yeah, so you mentioned these three categories of risks from AI.
3399100	3403100	An accident, intentional misuse, and rogue AI.
3403100	3406100	Which of these categories worry you the most?
3406100	3409100	And on which timelines?
3409100	3411100	So what are you most worried about right now?
3411100	3414100	What about in 10 years and 20 years and 30 years?
3414100	3418100	Yeah, 30 years, and you're like, oh, 2053.
3418100	3420100	Let's think about that for a moment.
3420100	3422100	I can't even think that far in advance right now.
3422100	3425100	But yes, I think it's a great question because, you know,
3425100	3427100	you hear all these things like, well, what's in the present day, right?
3427100	3430100	And I think at the moment, it is more of the accident,
3430100	3432100	and it is the misuse.
3432100	3434100	Just to clarify, by accident, we kind of mean
3434100	3436100	that the system is not quite doing what we wanted to do, right?
3436100	3439100	So when Bing Chat was aggressively misaligned
3439100	3443100	and it was kind of treating its users badly earlier in 2023,
3443100	3446100	then that indicates that's not what the machine was supposed to do.
3446100	3448100	And this is the broader category of, you know,
3448100	3451100	people respond to incentives, and there are perverse incentives, right?
3451100	3454100	That you think you've designed a law or a rule in one way,
3454100	3456100	and then it turns out it's something else.
3456100	3459100	So in that sense, these things are happening semi-frequently,
3459100	3462100	to some extent, and they're trying to, like, train them out, right?
3462100	3464100	And whether, you know, even saying the wrong thing
3464100	3468100	in terms of violence or sexual imagery counts as accident as well,
3468100	3470100	that's more nuanced, and we don't really have to get into that.
3470100	3474100	But all it to say is, accident is a frequent occurring problem right now.
3474100	3476100	With the misuse, that has also already happened, right?
3476100	3480100	People are using voice-cloning software to scam people out of money.
3480100	3483100	They call a person, usually a parent or a grandparent,
3483100	3487100	pretend to be the person's child because they've voice-cloned that child's voice,
3487100	3489100	and say, like, I need money, please send it immediately.
3489100	3491100	And people have already lost money.
3491100	3493100	Now, that seems like a bad case,
3493100	3496100	but, of course, it's not nearly as dangerous as, like, new bio weapons,
3496100	3498100	but that seems like it's also plausible.
3498100	3502100	So if you sort of imagine, yes, why don't we say the next two, three, five, ten years?
3502100	3505100	It seems like accident is already happening.
3505100	3510100	Misuse is most likely to increase before more power-seeking autonomous behavior
3510100	3513100	from AI itself or the rogue AI comes on board.
3513100	3517100	The concern is, since people who are building these things don't know,
3517100	3520100	when you put in a certain amount of compute or computational capacity,
3520100	3522100	you get a certain level of capability,
3522100	3524100	will there be a dramatic jump in capability?
3524100	3526100	Maybe, maybe not, and that uncertainty is a problem.
3526100	3532100	So while we, it seems reasonable to say, well, right now, misuse is more than near-term problem.
3532100	3535100	In the world of AI, is near-term six months?
3535100	3538100	Because then it's like, oh, yeah, I meant, like, until the end of 2024,
3538100	3541100	and then it's really also going to be both misuse and power-seeking.
3541100	3545100	So I think when you're sort of talking to average people or even policymakers,
3545100	3546100	there's usually, like, a multi-year,
3546100	3549100	something multi-decade concern in the back of their head,
3549100	3551100	or, sorry, timeline in the back of their head of how the world works.
3551100	3554100	And if you say something near-term versus long-term, you should clarify, like,
3554100	3557100	oh, by misuse, I mean, like, one to two to three years,
3557100	3562100	and then overlapping within one to five years, perhaps power-seeking as well.
3562100	3564100	And that's kind of how I would break it down.
3564100	3569100	You write about strategic foresight, which is making plans for different scenarios.
3569100	3572100	How does that help us manage these risks?
3572100	3576100	Sure, it's really just, again, trying to think about ways to figure things out
3576100	3580100	without committing to a specific outcome, like a forecast would, right?
3580100	3583100	Like, you know, again, the weather forecast, I think, is the best example
3583100	3587100	of the average person encountering probabilistic assessments of the future.
3587100	3589100	80% chance of rain tomorrow, right?
3589100	3592100	With forecasts, or metacoliths, different prediction markets,
3592100	3596100	what is the likelihood of, you know, X event happening at a certain time?
3596100	3599100	That's great. I think we need those, and they are important.
3599100	3601100	I think, in addition, we can use things like foresight,
3601100	3604100	which explores a range of plausible futures.
3604100	3606100	So you can look at the data, you can look at analysis,
3606100	3608100	and you can think, what is the most likely outcome?
3608100	3610100	What is most probable? And that's very useful.
3610100	3612100	But we can also think, well, what's plausible?
3612100	3616100	Let's play through, and kind of broad scenario planning is what this is.
3616100	3619100	What might happen if AI becomes more prevalent,
3619100	3623100	if image generators become more popular? What happens?
3623100	3626100	For example, image generators become more popular, then more people use them.
3626100	3631100	Does that affect artists? Let's just assume it affects artists and artists lose work.
3631100	3635100	Then what happens? And you kind of do this cascading first-order, second-order thing
3635100	3638100	that really, I think, helps open up the possibility space,
3638100	3640100	the realm of what could happen.
3640100	3643100	Now, sometimes it's hard to draw a direct line of what do we do now,
3643100	3646100	but at least you've opened up your mind of what could be.
3646100	3650100	And once you start to think back all the things that happened 5, 10, 15, 20 years ago,
3650100	3654100	if you put yourself back 15 years ago and try to imagine what happens then,
3654100	3657100	you realize, oh, people didn't see a lot of things coming.
3657100	3660100	They weren't open-minded enough, or they didn't see enough of the data.
3660100	3662100	There's a bit of hindsight bias, right?
3662100	3665100	Of course, that thing was foreseeable, and many things are not.
3665100	3669100	But with foresight, I really think it's very useful to, again, open up our minds.
3669100	3672100	So with the AI issue, you can take an example where, say,
3672100	3675100	artificial superintelligence arrives in 10 years.
3675100	3677100	Just assume that's happened, then work backwards.
3677100	3681100	So what had to happen for that future to come into existence?
3681100	3683100	What if it was 20 years? What if it was 50 years?
3683100	3685100	And you can kind of think, like, okay, maybe if it's 10 years,
3685100	3688100	current projections seem to hold, but maybe if it was 20 years,
3688100	3690100	there were some hiccups, there were some complications.
3690100	3692100	We didn't understand the complexity of certain things,
3692100	3693100	and we hit certain walls.
3693100	3695100	50 years, I think a lot of us would be like,
3695100	3697100	well, I don't know, we just got something wrong.
3697100	3699100	We didn't understand the nature of what we were dealing with,
3699100	3701100	and a lot of projections now would be wrong.
3701100	3703100	And you can do that in a variety of ways.
3703100	3707100	So I think it opens up the ability to think about these issues
3707100	3709100	in different ways without committing to something.
3709100	3712100	But fundamentally, it also really helps challenge assumptions.
3712100	3714100	If you sort of have discussions with people
3714100	3716100	of what they expect the future to be like,
3716100	3717100	and you could break that down.
3717100	3719100	Do you expect it, like, what's your preference for the future?
3719100	3721100	What would you not want the future to be like?
3721100	3722100	What do you think is most likely?
3722100	3724100	And so by doing all these different sort of,
3724100	3726100	different nuances, different themes about what they think
3726100	3729100	the future might be like, you might be able to have someone realize,
3729100	3731100	like, oh, wait, my expectation of the future
3731100	3733100	is very much aligned with my preference for the future.
3733100	3735100	Right, because that's how a lot of people are.
3735100	3737100	But maybe my preferences are not that relevant
3737100	3739100	to how the future actually exists.
3739100	3741100	And then they can go, oh, I didn't realize that was happening.
3741100	3743100	Or even just, you know, with AI stuff,
3743100	3746100	some people don't realize how advanced these machines already are.
3746100	3748100	And if you can say, like, this thing has already happened,
3748100	3750100	then what happens?
3750100	3753100	It really does help people think, oh, maybe this could be a concern.
3753100	3757100	Yeah, I think it's great to make plans for different AI scenarios.
3757100	3760100	But I do worry that these plans will work best
3760100	3762100	if we have gradual improvements.
3762100	3765100	So say 10% improvement per year.
3765100	3766100	We can go back to our plans.
3766100	3769100	We can get feedback from the world, adjust our plans.
3769100	3772100	But what if AI progress is more bumpy
3772100	3775100	and much faster than 10% per year?
3775100	3779100	Does this make strategic foresight less useful?
3779100	3783100	Well, perhaps less useful, but not useful, right?
3783100	3784100	It still has utility.
3784100	3787100	It's sort of like we have to, again, make decisions under uncertainty.
3787100	3789100	And so we should do the best we can.
3789100	3791100	We should put resources into figuring it out,
3791100	3793100	and we should map different possibilities
3793100	3795100	and try to communicate those broadly to others
3795100	3797100	to get feedback and see what things could be.
3797100	3798100	Yes, you're right.
3798100	3801100	If things are dramatic, if there's a big step change in capabilities,
3801100	3803100	it doesn't mean all that work wasn't useful at all.
3803100	3806100	But it might mean, like, oh, I have to flip to page five.
3806100	3808100	All those things I thought were going to happen in my document
3808100	3810100	have now already been passed, what now?
3810100	3813100	But hopefully having these conversations themselves
3813100	3815100	allow us to plan even better.
3815100	3817100	Like, okay, again, the use of scenario planning.
3817100	3819100	It's most useful when there's like 10% increases.
3819100	3820100	What happens if it's 50?
3820100	3822100	What happens if it's 200?
3822100	3824100	And again, you might not be able to really figure it out
3824100	3825100	and have a perfect plan,
3825100	3827100	but having something is better than nothing.
3827100	3829100	And sometimes, again, just thinking it through,
3829100	3831100	you at least get through all, say,
3831100	3833100	is like the emotional complications,
3833100	3836100	either the barrier intellectually or even viscerally
3836100	3838100	that, oh, my God, this thing just happened.
3838100	3841100	And sometimes people need a bit of time to sit with that.
3841100	3843100	Like, okay, now imagine something is much more capable
3843100	3845100	than anything ever and is highly general.
3845100	3847100	Let's think through what that might be like.
3847100	3849100	And you can even think through how you might feel
3849100	3851100	to then better make a decision when it's happening.
3851100	3853100	Because again, if you're trying to make decisions
3853100	3855100	and things are happening very quickly,
3855100	3857100	urgency rarely helps decision-making.
3857100	3861100	So you discuss this fact, I would say,
3861100	3863100	that we are living in unusual times
3863100	3865100	in terms of economic growth,
3865100	3868100	in terms of scientific papers published per year,
3868100	3872100	in terms of the exponential growth of computing power
3872100	3875100	available for a certain dollar amount.
3875100	3878100	Yeah, that's one aspect of the world we're living in.
3878100	3882100	Another aspect might be that ideas are getting harder to find.
3882100	3885100	We have many more researchers
3885100	3888100	for the same amount of scientific breakthrough.
3888100	3892100	Economic growth might be slowing down in certain countries.
3892100	3894100	Say, say we have these two trends
3894100	3896100	and you can tell me whether you think these trends
3896100	3898100	are actually occurring.
3898100	3900100	Which of these trends are going to win out?
3900100	3902100	Are we going to hit diminishing returns
3902100	3907100	or are we on a path to even stronger exponential growth?
3907100	3909100	Going infinite, right?
3909100	3912100	Like, I can find ideas on the internet very easily.
3912100	3914100	What do you mean they're hard to find?
3914100	3917100	Jokes aside, so I appreciate you highlighting that.
3917100	3919100	This is certainly not a new idea,
3919100	3921100	but I really wanted to, again,
3921100	3923100	average person hasn't thought much about these issues.
3923100	3925100	Like, where are we sitting right now?
3925100	3927100	And to think how humans currently live,
3927100	3929100	again, not everyone, there are billions of people
3929100	3931100	without food, water, electricity, that sort of thing,
3931100	3933100	or at least hundreds of millions,
3933100	3935100	things are very different than they used to be.
3935100	3937100	So I wanted to give a sense of the grand sweep
3937100	3939100	of how things are very different,
3939100	3941100	to show just how much change has occurred,
3941100	3943100	to then say, well, if so much change has occurred,
3943100	3945100	it's reasonable, possible, plausible,
3945100	3947100	to think a lot of change might also occur in the future.
3947100	3949100	So if you go back, you know, millions of years,
3949100	3952100	you know, proto-humans are still developing at one point,
3952100	3954100	you know, was it 1.6 million years ago,
3954100	3956100	we have a hand axe, which is a sharp stone tool,
3956100	3958100	and that was the best thing for a million years,
3958100	3962100	a million years, 50,000 generations of people,
3962100	3964100	and you're like, what?
3964100	3967100	I was like, well, I made this sharp stone slightly sharper.
3967100	3969100	Like, okay, well, that's not that great an advancement
3969100	3970100	compared to like the iPhone
3970100	3972100	and all the different new releases there.
3972100	3974100	That said, for people who are sticklers,
3974100	3976100	I'm sure there were also various wooden tools
3976100	3978100	they often don't preserve as well.
3978100	3981100	Humanity also lost knowledge about how to make certain tools
3981100	3983100	at various points throughout history,
3983100	3985100	which is something that's difficult to imagine now
3985100	3988100	that we would lose knowledge about how to print books
3988100	3990100	or something like that.
3990100	3993100	Maybe at the very cutting edge of the technology stack,
3993100	3995100	we can imagine that we might lose knowledge.
3995100	3997100	It seems difficult for us to imagine now, I think,
3997100	4000100	losing knowledge of how to create basic products.
4000100	4001100	You're right.
4001100	4004100	As the nature of the world has become more industrialized,
4004100	4007100	certainly making a particular product often requires many,
4007100	4009100	many people, sometimes thousands,
4009100	4011100	sometimes millions in the entire supply chain.
4011100	4013100	So that's its own types of complexity where now,
4013100	4015100	well, maybe someone could have made a pencil
4015100	4016100	and someone still can.
4016100	4018100	Nowadays, it's a whole team and company
4018100	4020100	and industries and machines.
4020100	4022100	Is it the case that we're going to keep growing,
4022100	4024100	sort of getting into more advanced and things
4024100	4025100	that you're going to keep changing?
4025100	4027100	I think it depends on what we measure,
4027100	4029100	and I'm well aware that the economists will say
4029100	4031100	that innovation has slowed or productivity is down
4031100	4032100	in certain ways.
4032100	4034100	And I think that's important.
4034100	4035100	I don't want to say it isn't.
4035100	4038100	But from the user, normal human user experience,
4038100	4041100	it seems like things are still remarkable.
4041100	4043100	Now, you could say most of it's in the information technology
4043100	4044100	space.
4044100	4045100	It's the internet.
4045100	4046100	It's the computers.
4046100	4047100	It's the phones.
4047100	4050100	Where's our new plane or washer dryer or the car
4050100	4052100	or that sort of thing?
4052100	4055100	And I guess I think that the changes in the internet,
4055100	4058100	in that space, like that we're easily doing this podcast,
4058100	4061100	are significant in a similar way to some of these other things.
4061100	4064100	Now, yes, the invention of like a dishwasher is a truly big
4064100	4066100	difference in terms of how it affected life.
4066100	4068100	But so are recent inventions.
4068100	4070100	So I don't want to say things are going to continue forever.
4070100	4072100	That seems unlikely because it just also makes no sense
4072100	4073100	conceptually.
4073100	4075100	Usually there's an S curve in terms of how these things
4075100	4076100	develop, right?
4076100	4078100	And it's hard to know where we are in the curve.
4078100	4081100	I would just say more comfortably for the next little while,
4081100	4083100	it does seem computer and computer technology,
4083100	4085100	that whole domain is going to crease a lot.
4085100	4087100	And then that's going to ripple through.
4087100	4090100	Now, whether some people think this isn't enough,
4090100	4091100	I don't know.
4091100	4093100	I guess I'm less concerned about that.
4093100	4096100	I mean, I am concerned about like economic growth being good
4096100	4098100	for human development in that sense.
4098100	4100100	But we're running out of ideas.
4100100	4101100	I don't know.
4101100	4102100	There's still lots of great ideas, right?
4102100	4104100	And in fact, I think with the AI thing,
4104100	4106100	like maybe we need to slow down some of this development
4106100	4108100	because we haven't figured out how to deal with the ideas we
4108100	4109100	already have.
4109100	4111100	I also think it's interesting, as you said,
4111100	4113100	scientists now sometimes on papers,
4113100	4116100	there's 10, 50, 100 or some like hundreds of scientists
4116100	4117100	to do some of these things.
4117100	4120100	Usually it's particle physics or something in AI
4120100	4123100	or machine learning or maybe even biology.
4123100	4125100	And yes, it's not like when it was with, you know,
4125100	4127100	you can picture your Darwin, your Aristotle or someone's like,
4127100	4129100	oh, yes, I think the nature of the world is blah.
4129100	4131100	And I've unlocked some mystery of the universe.
4131100	4135100	And yes, you could think that there are some diminishing returns.
4135100	4137100	But at the same time, there's lots we haven't figured out,
4137100	4138100	right?
4138100	4141100	How gravity interacts at the quantum level,
4141100	4144100	even if what the right interpretation of quantum mechanics
4144100	4146100	is, will these things be figured out?
4146100	4149100	Could we build fantastical ways of capturing energy,
4149100	4150100	more than solar even, right?
4150100	4151100	And these sorts of things.
4151100	4154100	So I guess when I think of the new solar and wind stuff,
4154100	4156100	which didn't exist when I was younger,
4156100	4158100	the fact that, you know, there's the immersive VR,
4158100	4160100	things didn't exist when I was younger,
4160100	4162100	planes haven't changed a lot.
4162100	4164100	Sure, but now people have gone to space casually.
4164100	4166100	Like again, in the history of the world,
4166100	4167100	none of this has ever happened.
4167100	4170100	So yes, on a multi-decade span,
4170100	4172100	it may seem like things have slowed.
4172100	4174100	But if you really take a step back,
4174100	4176100	thousands of years, even on a millionaire scale,
4176100	4178100	it's all squished, right?
4178100	4180100	In the past couple hundred years.
4180100	4182100	And so like, let's just, let's keep these things in mind.
4182100	4184100	But let's see how the next couple of decades pan out.
4184100	4186100	Let's talk about alignment.
4186100	4190100	So one objection you might give to the whole project of alignment
4190100	4195100	is to say that humans can't agree on what values we should have.
4195100	4199100	Philosophers haven't been able to figure out ethics.
4199100	4202100	What is it that we're trying to align AI with
4202100	4205100	if we haven't determined our values yet?
4205100	4209100	It's a great question and it is currently unsolved.
4209100	4211100	And in some ways, we're going to have to muddle through.
4211100	4213100	That would be my concise answer.
4213100	4215100	It's sort of like, what do we do when humans disagree?
4215100	4218100	Well, we try to come together in some sort of compromise,
4218100	4221100	some sort of consensus, hopefully some sort of democratic system
4221100	4224100	where people don't necessarily get everything they want,
4224100	4227100	but they get enough that the world functions decently for most people.
4227100	4230100	It's not perfect by any means, but then compared to what, right?
4230100	4231100	The Winston Churchill line.
4231100	4234100	So with AI, yes, this is the technical alignment issue,
4234100	4236100	which I think is critically important.
4236100	4239100	This is more like, does the AI do something we didn't want to do?
4239100	4241100	Like by an accident, by a technical point of view.
4241100	4245100	But yes, if we solve the technical alignment problem, that's great.
4245100	4246100	That's amazing.
4246100	4248100	That's, that's difficult, but it's still going to be amazing.
4248100	4250100	And then there's this other problem, which was always there
4250100	4251100	that slots right into place.
4251100	4252100	Well, now what?
4252100	4254100	Who decides the fate of the world type thing, right?
4254100	4257100	And if these systems are as powerful as they are,
4257100	4261100	it does seem very bizarre how we're currently going about it, right?
4261100	4264100	Yes, we do have states that are starting to issue executive orders
4264100	4267100	like the White House did or other regulation or the EU AI Act.
4267100	4272100	But right now it seems, I'll just say weird that a few people are sort of
4272100	4274100	not controlling the fate of the world, but by their own standards,
4274100	4277100	by their own statements, they're developing by design,
4277100	4280100	by their own goal at very, very powerful systems
4280100	4283100	that could have vast control and abilities.
4283100	4285100	So what is going on here, right?
4285100	4287100	And this is where the book is kind of just trying to raise awareness
4287100	4291100	of the, yes, even if this thing about AI is solved in a technical sense,
4291100	4295100	there's this other problem about how do we ensure like everyone has a voice?
4295100	4297100	How do we ensure people are represented?
4297100	4300100	Is there going to be even greater empower power imbalances
4300100	4303100	and inequalities that will result in a way that's truly disruptive?
4303100	4305100	So I think again, it's like a call to arms.
4305100	4307100	We need a lot more people thinking about it.
4307100	4308100	We need a lot more people aware of it.
4308100	4312100	And even like a if then scenario, like, okay, so say I developed,
4312100	4313100	what's the plan?
4313100	4315100	How will resources be distributed?
4315100	4318100	Will these companies just go to, you know, multi trillion dollar,
4318100	4319100	quadrillion dollar things?
4319100	4320100	Who knows how far it goes.
4320100	4322100	Does something end up getting nationalized?
4322100	4324100	These are delicate things, maybe to say in certain environments,
4324100	4327100	but hopefully conversations are at least happening behind the scenes of,
4327100	4329100	okay, let's plan through again, the scenarios.
4329100	4334100	If something isn't aligned with other people and they could use it
4334100	4335100	from malicious use, that's one thing.
4335100	4338100	But just the normal, my preferences are different than yours.
4338100	4340100	And that may make your life a lot worse.
4340100	4342100	That's something we really need to pay attention to.
4342100	4347100	I think also there's some hope that given our kind of humans have a shared
4347100	4352100	evolutionary history, we have, we share a lot of our values,
4352100	4357100	even though we also disagree strongly with each other all the time.
4357100	4361100	I think there's some hope that we won't have to actually get as something
4361100	4364100	like a final theory of ethics or something.
4364100	4369100	And I want to say we should definitely not stop working on alignment
4369100	4375100	until we have such a theory that we can then plug into our AI systems.
4375100	4380100	I think we can probably agree on some basics of life and then,
4380100	4382100	as you say, model through.
4382100	4385100	So thinking about healthcare, for example,
4385100	4389100	I think most people can agree that most people should have access to,
4389100	4392100	or all people should have access to fantastic healthcare.
4392100	4395100	And that's something where AI might be able to help.
4395100	4398100	And then we can go on to the next thing and the next thing and the next thing.
4398100	4405100	I think there's some sort of, there might be too much focus on trying to develop
4405100	4409100	a perfect theory of ethics and we should, as you say, model through.
4409100	4412100	Well, I think that sort of might be the only way, right?
4412100	4415100	As you said, there's philosophers, academics, anyway, all humans
4415100	4417100	sort of been working on this for thousands of years.
4417100	4419100	And of course, we don't all agree.
4419100	4421100	And importantly, we don't often agree with ourselves, right?
4421100	4424100	We change over time, your preferences from a child to as an adult
4424100	4426100	to maybe even five years ago.
4426100	4427100	That's very different.
4427100	4430100	Well, were you correct five years ago or now?
4430100	4431100	Uh-oh.
4431100	4433100	Which value system did you give the AI?
4433100	4434100	Was it supposed to lock in?
4434100	4436100	Was it supposed to know better?
4436100	4440100	I think it's a very interesting but also very concerning space that,
4440100	4443100	like you, though, I think, can't we agree on some basics?
4443100	4446100	And we could sort of think the United Nations Declaration of Human Rights
4446100	4449100	or development, like most countries did sign on to these things.
4449100	4452100	And there is a general sense like, okay, people should have food.
4452100	4455100	We'll get to healthcare in a moment, but like food, water, sanitation,
4455100	4457100	grade up to grade eight primary education.
4457100	4459100	Like these seem to be universals.
4459100	4462100	And so hopefully, yes, with abundance from the AI, we can all agree.
4462100	4464100	Like, can't we just like end tuberculosis?
4464100	4466100	Can't we really solve this malaria thing?
4466100	4468100	Can't we have everyone more educated?
4468100	4470100	But there will always be someone who disagrees.
4470100	4472100	There will always be someone from a different angle or malevolent actors.
4472100	4475100	There are currently 40 million people in modern day slavery.
4475100	4479100	Clearly they are there because other people are doing terrible things in various ways
4479100	4482100	or the situations they find themselves in are so compromised.
4482100	4485100	That said, like, how do you ever like get rid of that?
4485100	4488100	And as you said, we kind of muddle through as well at the same time,
4488100	4491100	certain things remain wholly unacceptable with the AI.
4491100	4495100	It is the concern that, and this is of course race dynamics all over the place,
4495100	4498100	that if certain malevolent actors get their way first,
4498100	4502100	then they may then be able to disempower or displace what we'll say
4502100	4505100	is the loose reasonable majority that thinks everyone should have food,
4505100	4507100	water, healthcare and education.
4507100	4511100	In sort of traditional discussions of alignment,
4511100	4518100	we imagine perhaps that we would sit down and hand code human values into AI systems.
4518100	4523100	And then we thought about how complex human values are,
4523100	4526100	and that was a cause of despair.
4526100	4532100	How could we ever summarize something as complex and inconsistent as human values?
4532100	4536100	Do you think that large language models change this picture?
4536100	4539100	Because large language models can digest all of human knowledge
4539100	4547100	and then at least they can pretend to have knowledge of human values, ethics, psychology and so on.
4547100	4552100	Is it the case that large language models make the alignment problem easier?
4552100	4554100	That is a great question.
4554100	4556100	I think that's one of the things we're currently figuring out, right?
4556100	4561100	From what I understand, open AI plans to use AI models to help solve the AI alignment problem.
4561100	4567100	And in a way, we need AI to assess and evaluate and to test to see if it is aligned
4567100	4569100	so that AI is inherently involved.
4569100	4572100	But to your general question is maybe.
4572100	4575100	And what I think is a perhaps interesting development of this.
4575100	4577100	So what if the AI system looks at all human knowledge, right?
4577100	4581100	And it says, you know what, I figured it out, guys, everyone, this is what you should do.
4581100	4583100	We're like, I don't want to do that.
4583100	4587100	But by your own standards, you said you cared about these things.
4587100	4589100	And then humans are like, no, no, but I didn't.
4589100	4590100	Not really.
4590100	4591100	Come on.
4591100	4595100	And so in the book, before I talk about the AI alignment problem, as people know it,
4595100	4597100	I talk about Isaac Asimov's Laws Robotics.
4597100	4602100	And I think this was just the very easy way into like simple rules to align computer systems.
4602100	4603100	Don't work.
4603100	4605100	When you say don't harm something, you're like, that seems reasonable.
4605100	4606100	That seems obvious.
4606100	4607100	Of course, put it in the machine.
4607100	4609100	Like, what does that mean exactly?
4609100	4610100	Like don't harm at all.
4610100	4614100	Like if someone needs to get surgery where they have to be cut into, does that count?
4614100	4615100	What if it's a risky surgery?
4615100	4619100	When you say don't harm anyone, does that include nonaction?
4619100	4621100	This is all the omission bias, right?
4621100	4624100	Where if you drown someone, we see that as a terrible thing.
4624100	4628100	If you let someone drown, well, we see that as a bad thing, but not usually as bad as
4628100	4629100	the intentional drowning.
4629100	4633100	So is an AI system now supposed to think, well, wait, I'm letting people suffer.
4633100	4635100	People are currently dying in poverty needlessly.
4635100	4637100	Should I be doing something about that?
4637100	4641100	Well, by your own standards, you said, don't allow humans to cause harm or come to harm.
4641100	4642100	What am I supposed to do now?
4642100	4645100	And you can see that very much disconnected maybe the thing short circuits.
4645100	4646100	Yeah.
4646100	4649100	Does harm imply any probability of harm?
4649100	4651100	Then you're kind of, you cannot move.
4651100	4655100	You cannot do anything because anything, any action at all could cause harm.
4655100	4657100	It just doesn't work.
4657100	4658100	No, exactly.
4658100	4664100	And so I think it'll be very useful for AI systems to provide insight, but like any sort
4664100	4668100	of human enterprise thing, we might get back an answer we don't want or that's very hard
4668100	4672100	for us and whether people will take that on board is also going to be highly variable.
4672100	4676100	Like for people who are very much interested in ethical reflection and philosophy, they
4676100	4679100	might have made like substantial progress and they realize, you know, my beliefs mean
4679100	4682100	I shouldn't do X and therefore I don't do X.
4682100	4686100	And a lot of us are like, I know I shouldn't do X, but sometimes I still do because I'm
4686100	4688100	a human and you know, again, progress is good.
4688100	4691100	It's not to say that people should be absolutist about these things.
4691100	4695100	It's just sort of highlights the difficulties of the human system, the human nature of the
4695100	4696100	whole thing.
4696100	4700100	Do you think current AI systems have self preservation?
4700100	4701100	Good question.
4701100	4704100	I would think I would probably defer to like, who's doing the most cutting edge research
4704100	4707100	now in the advanced labs?
4707100	4711100	From what I understand is slightly but not a lot.
4711100	4716100	There are some examples I think more in like the theoretical or there's like a prototype
4716100	4720100	where they've played around with certain systems and you know, simulated environments and the
4720100	4725100	system does seem to engage in certain behavior to protect itself to achieve a goal.
4725100	4729100	Whether it's full on self preservation as we commonly understand it, I don't think we're
4729100	4733100	there yet, but yeah, I would, I would kind of think like, well, there's probably some
4733100	4737100	paper on archive that I haven't had a chance to read yet that may say otherwise.
4737100	4743100	But do you think that AI systems will develop self preservation as we get more advanced
4743100	4744100	AI?
4744100	4745100	I do.
4745100	4748100	Or at least I think I do to the extent that we should be concerned about it.
4748100	4752100	Again, nothing's 100% here, but there's enough of a risk that, you know, the whole
4752100	4755100	Stuart Russell, you can't fetch coffee if you're dead thing.
4755100	4758100	To have a system do anything, it has to exist.
4758100	4762100	And to me, it is reasonable, it is plausible that to achieve anything to exist.
4762100	4766100	And once you know that, you might engage in various activities to ensure that you do exist.
4766100	4771100	Now, maybe there are ways to contain this or to circumvent it where, you know, you somehow
4771100	4775100	clearly specify a goal with a certain amount of error bars and then the system is supposed
4775100	4777100	to shut itself down after it's done that goal.
4777100	4778100	Perhaps.
4778100	4782100	But when we talked about before, the incentives for autonomous systems that are highly capable,
4782100	4787100	highly fast and so on, will then sort of have a disconnect with something that shuts itself
4787100	4788100	down all the time.
4788100	4790100	You imagine like, oh, I like to use my phone.
4790100	4792100	After I send one text, I shut the phone off and then I shut it.
4792100	4795100	I turn it back on like, well, that seems really painful and slow, right?
4795100	4797100	And people just might not do it.
4797100	4802100	So I think it is plausible that the systems will engage in such behavior in a way like
4802100	4804100	the expectation would be they probably would.
4804100	4808100	I guess I'm trying to say the default expectation to me is that something that's very, very intelligent
4808100	4810100	will probably engage in these behaviors.
4810100	4814100	So we should be on the lookout for it and really try to figure out if they are or they're not
4814100	4816100	versus the expectation that they wouldn't.
4816100	4821100	It doesn't seem to me that GPT-4 when I talk to it is trying to self preserve at all.
4821100	4825100	It's this my team naive, but it seems to me that I can just click stop on the chat whenever
4825100	4827100	I want or close the tab whenever I want.
4827100	4829100	And there's nothing the system can do.
4829100	4835100	Do you think self preservation will emerge together with more autonomous systems?
4835100	4836100	That's a great point.
4836100	4839100	So yes, right now, if you think about like, how could this thing be autonomous?
4839100	4841100	I literally, you know, close it.
4841100	4842100	I click the button and it goes away.
4842100	4845100	It's not like secretly hiding somewhere to our knowledge.
4845100	4848100	I guess there's a small probability that it is already.
4848100	4849100	And well, that's the thing.
4849100	4853100	That's why, by the way, I tried to be careful in the book where like AI that's super smart can do anything, right?
4853100	4856100	Because then you kind of get into these almost non falsifiable things.
4856100	4860100	Like, well, maybe it's secretly doing the thing and it's so good at deception that it looks like it isn't deceiving.
4860100	4863100	Now, I think there's something to it and we should be wary about it.
4863100	4868100	But I also think we have to be careful because those explanations are not satisfying to most people.
4868100	4869100	Oh, look, it can do anything.
4869100	4870100	Well, tell me about it.
4870100	4871100	Oh, anything.
4871100	4872100	You're like, well, I don't understand what you mean.
4872100	4875100	To your point, though, yes, right now I'm not concerned about that.
4875100	4876100	I don't see an issue with that.
4876100	4888100	That said, as we start to get beyond the GPT for or Gemini and these frontier models, I think it's a very important thing to assess not only before deployment, but in the training stage, there should be methods and benchmarks in place.
4888100	4894100	And even asking the labs, what are your expectations for the capabilities of your models throughout the process?
4894100	4902100	And if they're the lab themselves are like consistently wrong in a certain direction, like, oh, we thought it would be certainly capable and it ends up being more capable every time.
4902100	4903100	Like, that's not a good track record.
4903100	4912100	So next time when you say it's not going to be as capable as we thought it probably will be just some sort of way to get at how we might understand these things as it goes in the future.
4912100	4914100	Again, we currently have autonomous systems, right?
4914100	4916100	As I said, so they're doing banking stuff.
4916100	4917100	They're doing fraud detection.
4917100	4919100	They're doing cybersecurity things.
4919100	4921100	They're stopping missiles that are being bombed.
4921100	4925100	The incentives to have these things become more autonomous will be there.
4925100	4929100	And then again, if the system doesn't exist, it can't really do its job.
4929100	4933100	I want to be careful here that there's a sort of the as if goals, right?
4933100	4938100	That the system only needs to act as if it is engaging in self preservation.
4938100	4940100	It doesn't have to have like, I'm an AI.
4940100	4941100	I have a certain goal.
4941100	4942100	I need to exist.
4942100	4945100	It may do something like that, but it doesn't have to.
4945100	4947100	And I don't want us to sort of think it needs to be the case.
4947100	4955100	It's more just going to engage in various, we'll say from our perspective, reasonable goal oriented processes to make it more likely to achieve its goal.
4955100	4960100	And some of those will involve getting more power to ensure its own existence so we can serve that end.
4960100	4964100	Now again, maybe not, but there's enough of a maybe so that we should be very careful.
4964100	4972100	There's a bunch of emergent capabilities in AI systems that could be problematic if we're trying to align these systems.
4972100	4981100	So we're talking about power seeking, deception, manipulation of humans, using humans to achieve goals in the physical world and so on.
4981100	4987100	Which of these traits do you worry about the most and where do you think we are on the road to these traits?
4987100	4993100	How close do you think we are to having manipulative or deceptive or power seeking system?
4993100	4997100	I don't think we're there yet, but what could happen in a short period of time could definitely be the case.
4997100	5006100	So in the first example, just the emergence itself, I think a lot of people think the recent large language models, your chat GPT, GPT4 and cloud and whatnot,
5006100	5009100	they are largely emergent in a lot of their capabilities, right?
5009100	5014100	You have these systems that were trained on mainly English and then they can speak other languages.
5014100	5021100	That was not the plan, right? Or it was trained mostly on sort of corpus of text and then it can also do computer programming.
5021100	5026100	So it's not that if someone had thought through a lot of this, they would have realized, oh, maybe it will also do the thing.
5026100	5034100	It's that from our perspective, it did seem like certain behavior emerged in a way that was unexpected on a plan, at least in some cases in some domains.
5034100	5037100	So all I have to say is emergence seems to be like all over the place, right?
5037100	5043100	Especially if you try to prompt a model in a certain way and you get a certain thing which you didn't think it might do, but then it would.
5043100	5053100	It was just strange to me knowing something about the training process of GPT4, interacting with the system in my native language and talking to it perfectly in Danish
5053100	5060100	and seeing that it's pretty capable in my native language was kind of a surprise and an interesting experience.
5060100	5066100	And I believe that a lot of people have had that experience of talking to it in their native language.
5066100	5071100	And knowing that it wasn't trained specifically to do that, it's quite impressive.
5071100	5078100	I agree. I think it's staggeringly impressive. It's hard to think of a human parallel because clearly Danish was somewhere in the training set, right?
5078100	5081100	It's not like it read English and then it like invented Danish.
5081100	5083100	No, no, of course not. Just for everyone else.
5083100	5087100	But at the same time, it's like, well, you have a test coming up. It's an English test.
5087100	5091100	Here's a whack of material and like study all of it, but focus on the English like, okay.
5091100	5095100	And like, and then Danish like what? And of course, it's not just Danish. It's numerous of the languages.
5095100	5099100	It's also math, not usually the addition which it has trouble with like complicated math.
5099100	5104100	And if, you know, famous mathematicians like Terence Tau are using these systems, like it really helps improve my workflow.
5104100	5108100	You're like, okay, well, that's clearly a significant indicator of capability.
5108100	5113100	So you could also imagine if then someone's like, we really need a dedicated math system.
5113100	5117100	This goes to that general versus narrow, would it be more capable?
5117100	5122100	It seems to think like it should be more capable, but it's possible the general somehow is more capable, right?
5122100	5131100	And then fine tuning tweaks it to your other question, though, I am concerned to sort of thinking about sort of that security nature of things, right?
5131100	5135100	If you're looking through the world from a security lens, security mindset, you think like, where are the weak links, right?
5135100	5142100	And all companies and even individuals deal with this to some extent, and people can be manipulated in a sort of social engineering way, right?
5142100	5147100	People get information just by calling someone up, pretending to be someone else, or there's more overt cyber hacking and whatnot.
5147100	5154100	But with AI systems, it's almost like you take the normal problems that already exist, and then there are also still problems in the AI space.
5154100	5160100	So whether there's, you know, people who are bribeable, people who are manipulatable, people you can hire off the dark web.
5160100	5165100	Like it's one of these sort of things that most people don't like to think about for obvious reasons, but there are like hitmen.
5165100	5170100	You can hire them to kill people. They just don't hire them to kill you because you're not that important, right?
5170100	5175100	And thankfully, right? It's this whole weird world, you're like, what happens? What type of criminal activity?
5175100	5181100	Like there's actual criminals who do things. And so to think that AI systems won't liaise with, if they're trying to cause harm,
5181100	5188100	nefarious individuals who literally can be paid to do crime to make themselves more capable, it seems like that would be an oversight.
5188100	5194100	So I guess I'm concerned about a range of these things. Again, at the moment, not that concerned about that much of it.
5194100	5199100	But because it often takes months, years to address problems, you need the infrastructure in place now.
5199100	5205100	You need what is the problem type conversations. Problem definition is very important. People often speak past each other.
5205100	5212100	So if we can agree that an AI system that would have an ability to manipulate people, or that would have an ability to hire people to do certain tasks,
5212100	5214100	is a potential problem, that's a good start.
5214100	5222100	And I think we already saw when GPT-4 was being evaluated, there was that famous case where, again, the AI system itself did not do this,
5222100	5232100	but it was liaising with the researchers in between. And the GPT-4 was able to hire someone off task rabbit and lie about why it needed that person to fill out a capture,
5232100	5240100	to build a commuter code. Again, the system didn't do it, but it's like, well, it doesn't seem that complicated to connect those things or to have enabled the system to do it.
5240100	5249100	So if it was the case months ago, that if a system with a bit of tweaks could have hired someone off the internet to circumvent something, to stop AI systems,
5249100	5254100	and lie about why it did it, why wouldn't this be possible in the future? It's already happened.
5254100	5259100	It's like, okay, so now imagine something that's more people, more people flying, can sort of think through step by step on its own,
5259100	5262100	would know how to navigate a decision once it has more examples.
5262100	5273100	Again, this human history, fiction novels, movies, or even just current events, are replete with numerous examples of people deceiving each other and engaging in various complicated nefarious schemes.
5273100	5277100	And you could imagine that as a wonderful training data set for someone trying to cause harm.
5277100	5290100	Do you think we will get something like a unified solution to the alignment problem, like something that solves all of these problems that we just talked about, deception, manipulation, power-seeking, and so on?
5290100	5301100	Another way to ask maybe is, do you think we'll get something like the theory of evolution, which solves a bunch of distinct problems in a general and simple way,
5301100	5307100	or will it look more like whack-a-mole, solving one problem, moving on to the next problem?
5307100	5310100	I like this the option between evolution and whack-a-mole.
5311100	5313100	I said the evolution of whack-a-mole.
5313100	5317100	So I think it's probably a bit more the whack-a-mole.
5317100	5321100	And the reason here is, again, like the straightforward logic of human incentives and human behaviors.
5321100	5326100	So say someone develops a system that, as you said, reliably does what it's supposed to do.
5326100	5329100	Well, that means it could be used for good or bad, right?
5329100	5339100	At the moment, the AI systems, and I briefly mentioned this in the book, they're kind of sort of loosely corporate American Western values, like what's acceptable and what's not.
5339100	5342100	But what it does mean is that someone has their hand on the lever.
5342100	5345100	Someone is sort of manipulating the system to do X and not Y.
5345100	5352100	So it is already the case that certain values are being implemented or at least displayed through these systems of a certain type.
5352100	5360100	Now, if you just had the alignment of does what we want, if someone was a malevolent actor and wanted to cause harm, it could use the thing to do what we want.
5360100	5366100	So then you're like, well, is the alignment problem really going to solve not having bad people do bad things,
5366100	5370100	or not having, we'll say, even desperate or confused people inadvertently do bad things?
5370100	5371100	That's the other thing.
5371100	5373100	Well, I would say people could be bribed and manipulated.
5373100	5382100	People could also just be persuaded, like their child is very sick, they're desperate, they need money, or maybe like, oh, my child has a certain form of cancer is like, look, I can cure it.
5382100	5383100	I just read a thousand papers.
5383100	5384100	I just need some resources.
5384100	5387100	I understand desperation could also be a factor here.
5387100	5394100	So trying to get rid of, like, not say everyone's evil and nefarious out there, it's just like there's many reasons why someone might give up power.
5394100	5399100	And it's hard to imagine how sort of the alignment problem might sort of address all that.
5399100	5403100	In the earlier version of the book, actually, there were four different alignment problems I was going to talk about.
5403100	5406100	But I thought that was too complicated for my audience.
5406100	5408100	It really was like, you're not fully aligned with yourself.
5408100	5410100	We're not fully aligned with each other.
5410100	5412100	We're not necessarily fully aligned with AI.
5412100	5417100	And then AI itself may not be aligned with us or with itself if there's multiple AIs.
5417100	5423100	But I thought, okay, you know, let's let's streamline this to then think about as a gas model as robotics to make it easy that that type of line is a problem.
5423100	5426100	And then the more traditional the alignment problem stuff.
5426100	5430100	Right now, there's a bunch of proposals on the table for making AI safe.
5430100	5434100	There's a lot of attention on this issue and in policy circles.
5434100	5441100	And I think you had a great discussion in the book about principles for selecting among these proposals for AI safety.
5441100	5444100	Maybe you could talk a bit about these principles.
5444100	5445100	Sure thing.
5445100	5447100	So yes, there's a lot of great proposals out there.
5447100	5453100	But it's sometimes useful to take a step back and even think like, what's the framework that we're even using to think through these proposals?
5453100	5461100	And even if one set out like that's kind of obvious, like, sure, but let's put it down because sometimes what you think is obvious is obvious to you and not someone else.
5461100	5463100	Or you see where you might agree, right?
5463100	5466100	If you have five principles and I have five, maybe three overlap and that's great.
5466100	5472100	So I kind of tried to keep it simple and think through like, what are the three main ways we might want to think about this?
5472100	5474100	Or that should be a part of any principle.
5474100	5477100	And so that's verification and agility, adaptability.
5477100	5478100	That's the second one.
5478100	5480100	And the third one is defense in depth.
5480100	5484100	So verification is just realizing that we need to verify.
5484100	5486100	You know, it's nice to say trust, but verify.
5486100	5488100	But the idea is that everyone should be accountable.
5488100	5489100	There should be transparency.
5489100	5492100	There should be verification mechanisms built in.
5492100	5497100	And if actors in the space, the companies that are developing these things say there's no problem.
5497100	5498100	There should be no problem then.
5498100	5500100	Let's let's let's verify everything, right?
5500100	5503100	If you think you're not developing anything harmful, let's have that as a backbone.
5503100	5511100	We'll say of any proposal that you want to ensure that things are happening as they're understood to be happening as much as possible.
5511100	5520100	And even the idea that people will try to circumvent verification as they always do in this world, at least to some extent by some actors, that should also be built into the process.
5520100	5522100	So you can't just take people's words for it.
5522100	5527100	I can't remember the quote, but something's like they can't be grading their own homework here in these AI systems like these companies.
5527100	5528100	It just doesn't work that way.
5528100	5531100	And again, even if they're not necessarily nefarious, great.
5531100	5534100	We'll just have it all above board and have ensure that verification is there.
5534100	5536100	The second one, agility and adaptability.
5536100	5544100	Again, this isn't like a novel insight, but it's just really trying to highlight how fast moving the spaces and how we really have to think through.
5544100	5546100	What if it happens even faster, right?
5546100	5552100	That a lot of times in the policy regulatory legal space, things take months, years to work through the system.
5552100	5554100	And what if it has to be much less than that?
5554100	5559100	Or what if you have a law that you thought was useful that, you know, usually what happens is the laws developed.
5559100	5561100	It's somehow it comes to exist in the world.
5561100	5563100	Sometimes it interacts with challenges from the courts.
5563100	5565100	There's some sort of settled agreement.
5565100	5568100	The law seems to have some sort of a standard or consistency.
5568100	5570100	And then maybe it's challenging in the future.
5570100	5573100	That's a multi year, some nice multi decade process for the AI stuff.
5573100	5575100	It might have to be multiple months, multiple years.
5575100	5583100	So how can we even start thinking about changing almost the machinery of government and parts of the world to at least address these sorts of things?
5583100	5588100	Yes, you can have amendments to laws, but really thinking through that this is a factor and that people should be prepared.
5589100	5591100	For things happening faster than usual.
5591100	5593100	The third one, defense in depth.
5593100	5596100	This comes more from cybersecurity than the military definition.
5596100	5598100	And that's that we need multiple layers of defense.
5598100	5604100	You don't expect any one particular proposal or any one particular action to lead to safety or security.
5604100	5606100	But you're trying to have multiple layers.
5606100	5612100	So if any one of them fails and you actually expect them to fail, that there are others there to sort of pick up the slack.
5612100	5613100	Makes a lot of sense.
5613100	5614100	All of these principles.
5614100	5619100	So in the book, you you discuss eight proposals for for safe AI innovation.
5619100	5622100	Maybe you could talk about your favorites here.
5622100	5624100	What are the most important things?
5624100	5626100	It's like, how do you choose a favorite child?
5626100	5628100	Right? No, I just think the music.
5628100	5631100	So why there's eight, by the way, and why isn't there 10 and why isn't there seven?
5631100	5633100	Well, you know, eight seemed like a good number.
5633100	5636100	I think this really encapsulated what I thought were a good number.
5636100	5642100	Also, why I want these proposals or why I want these to be discussed is not that these are again definitive.
5642100	5645100	The whole book is trying to be very open minded solutions oriented.
5645100	5648100	We need more people working on this and we all need to come together to work on this.
5648100	5652100	So if it is the case that someone's like, you know what, most of your proposals don't work for me.
5652100	5654100	I'm like, OK, which ones do, right?
5654100	5659100	Because there's as we know, this is unfortunate tension that exists sometimes in different AI safety and ethics communities.
5659100	5666100	Or some people who are more focused on, we'll say present day concerns like algorithmic bias are taking issue with people who are more concerned about existential threats.
5666100	5668100	And this is an unfortunate division.
5668100	5670100	I mean, there's some rationale behind it, right?
5670100	5677100	I mean, there's some kind of representation of resources, but I think it's largely just an unfortunate way the world's turned out and it doesn't have to be this way.
5677100	5683100	So with the eight proposals, there could be like, OK, yes, some of those are maybe more X risk or existential risk oriented.
5683100	5685100	But what are the ones that work for you now?
5685100	5686100	Some sort of liability.
5686100	5687100	That's one of the proposals.
5687100	5688100	Great.
5688100	5691100	Some sort of a transparency or identification you're interacting with an AI system.
5691100	5694100	And that's where I'm trying to like, you know, all of branches all over the place.
5694100	5699100	Build bridges here that if certain proposals don't work, pick the ones that do or show me your list of eight.
5699100	5704100	And let's work on those together that seems like it's going to have the most broader support that is going to be good for all these issues.
5704100	5709100	I think if we took a step back, some sort of liability for these systems, again, why do people do anything?
5709100	5710100	Will they respond to incentives?
5710100	5715100	And if they are held liable, sometimes personally liable for how these systems malfunction, that is usually a good lever.
5715100	5720100	And it can't just be a sort of distributed sense of like, well, I did a thing, but I'm not really accountable.
5720100	5722100	Like, well, let's think through what makes the most sense here.
5722100	5730100	And maybe if you are responsible for distributing an AI model, even though you didn't create it, you do bear some of the liability here, some of the accountability.
5730100	5734100	I think compute governance is also a very important one here.
5734100	5738100	This is sort of like getting a sense of and controlling who has access to which chips.
5738100	5745100	The U.S. has already implemented some of these things with, you know, export controls on China and some recent additions to that, which are making it even more restrictive.
5745100	5749100	The book Chip War is a fantastic exploration of some of these issues.
5749100	5757100	And so in that sense, if you think of the main reasons why AI is increasing in capability, usually people think there's three main inputs, right?
5757100	5765100	You have the computational power, you have lots of data, and then you have like the algorithms itself, which are partly a human thing, like just talent pool that are building these things,
5765100	5768100	but also insights from science and other domains and even AI itself.
5768100	5778100	So how can you, as taking a big step back as a system, as a government, as an international organization of sorts, think about how to control or at least have a sense of influencing these inputs?
5778100	5782100	And data is kind of everywhere. It's really hard to stop people accessing data.
5782100	5788100	With the talent pool, the algorithms, that also seems like we want free mobility of labor for the most part, right?
5788100	5791100	We don't want to lock people up and tell them they can't work certain places or anything else. That seems bad.
5791100	5795100	But with the compute governance, it is more of a tangible physical thing.
5795100	5803100	There's a chip, which is understood at least to some extent of what it does and how it works, and getting a sense of where these chips go could be very, very useful.
5803100	5808100	Now, to reassure some people, this isn't for all AI systems. This is really the frontier AI model.
5808100	5813100	So for the average consumer, the average AI business, the average AI product, this doesn't really affect them at all.
5813100	5821100	It's more that are you using the most advanced chips and do you have thousands and thousands of them that you can put together in a cluster to then train highly capable models?
5821100	5826100	So it really is something that sort of like those taxes, like, oh, if you make more than $100 million, this is a tax.
5826100	5829100	And people are like, I would never want to be taxed like that. Like, well, don't worry, you never will.
5829100	5834100	So for the most part, it doesn't affect most people or most businesses. But I think that's very useful.
5834100	5839100	Now, how you go about it with the proposals, I have an idea, I have some sketch, I have some detail.
5839100	5846100	But I always want to say, like, let's think this through. What is the best version of this to go forward with the most recent evidence with the most recent analysis?
5846100	5854100	What is the best version? Is it useful to have, you know, ways that the chips can't communicate with each other as much so you can't bundle more like weaker chips together?
5854100	5859100	Or should there be some sort of remote kill switch even for the chips themselves so we can't shut down the Internet?
5859100	5864100	Maybe let's study it. Let's see if that's viable. If it turns out it's not for good or bad reasons, then we wouldn't do that.
5864100	5868100	But really trying to think through how can we access some of that stuff.
5868100	5875100	One of your proposals is about investing in AI safety research. So here I'm imagining a technical AI safety research.
5875100	5884100	At least when I try to try to see this from the perspective of a funder, I worry that it's extremely difficult to choose among proposals.
5884100	5890100	What should you fund? How should you respond to an applicant being optimistic about solving the problem?
5890100	5895100	Is pessimism a sign that you understand the problem in a deeper way or it's optimistic?
5895100	5905100	You know, there's so many complexities and in a sense this is just normal science funding, but I worry that this problem is even stronger in trying to fund AI safety.
5905100	5912100	Do you have any ideas about how to go about evaluating proposals for technical AI safety research?
5912100	5921100	I think you've raised a great point and the smile was just the idea that yes, if someone is more depressed or less optimistic about solving the problem, do they get more money, right?
5921100	5925100	Intellectual integrity and, you know, epistemic modesty and all these things and maybe that's the case.
5925100	5935100	So I would say that I don't necessarily have specifics. I think it's sort of like, do we agree that there are currently a lot more people that are trying to increase capabilities that are concerned about safety?
5935100	5944100	Like, do we agree this is a fact? Whether the numbers like 100 to 1 or more or less, you know, AI capabilities researchers versus safety researchers, do we agree there's some sort of large discrepancy?
5944100	5952100	And that probably shouldn't be the case. I think Ian Hogarth had that nice, like a sort of two chart graph, like, well, one line's going way up and the other one's not really going up and meeting it at all.
5952100	5960100	So if there is a disconnect between safety and capabilities, what should we do about that? And it seems like somehow funding safety research seems like a good idea.
5960100	5966100	Sort of like, let's start there. If we can't get agreement there, then that's an issue. But assuming there is some agreement, yes, how to actually go about it.
5966100	5974100	And here I would probably kind of defer to people who've already been in the space for several years and have them kind of talk to each other and see what are some current best practices.
5974100	5982100	You're right. It's an absolute mess. And how to study safety without increasing capabilities seems to be one of the biggest issues of all.
5982100	5990100	And as anthropic and other people have reasonably said, even though it's kind of a bitter total swallow, but we need the advanced AI system to study the safety of the system.
5990100	5998100	And it doesn't really make sense to try to think about the safety of, say, GPT-4 or GPT-5 if you're currently working on GPT-2 or GPT-3.
5998100	6005100	From what I understand, not that much of what you learned is going to be applicable and translatable. So I think it's a really important issue.
6005100	6013100	And I think it's just sort of really orienting people to realize, look how much capabilities are increasing and there is not nearly the attention paid on safety, not at all.
6013100	6021100	And in fact, some of the organizations don't care at all. And so if people really onboarded that, okay, some of these people seem to be more responsible actors and some of them don't care about safety at all,
6021100	6029100	they're just trying to make the product as fast as possible and dump it into the world, like I think it was the Mistral AI, then we should be concerned.
6029100	6035100	Great. There's a lot of complexities there. We could talk for hours on this topic. Darren, thanks for talking with me. It's been a pleasure.
6035100	6037100	My pleasure as well, Gus. Thanks so much.
