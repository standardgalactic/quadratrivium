WEBVTT

00:00.000 --> 00:03.640
Welcome to the Future of Life Institute podcast.

00:03.640 --> 00:06.760
I'm Gus Docher and I'm here with Roman Jampolsky.

00:06.760 --> 00:11.000
Roman is a computer scientist from the University of Louisville.

00:11.000 --> 00:12.840
Roman, welcome to the podcast.

00:12.840 --> 00:15.000
Thanks for inviting me. It's good to be back.

00:15.000 --> 00:18.800
I think it's my third time on a FLI podcast, if I'm not mistaken.

00:18.800 --> 00:24.800
Great. You have this survey paper of objections to AI safety research,

00:24.800 --> 00:26.640
and I find this very interesting.

00:26.640 --> 00:31.800
I feel like this is a good way to spend your time to collect all of these objections

00:31.800 --> 00:35.040
and see if they have any merit and consider them.

00:35.040 --> 00:38.080
And so I think we should dive into it.

00:38.080 --> 00:42.080
One objection you raise under the technical objections

00:42.080 --> 00:45.480
is that AI, in a sense, doesn't exist.

00:45.480 --> 00:48.480
If we call it something else, it sounds less scary.

00:48.480 --> 00:50.280
Perhaps you could unpack that a bit.

00:50.280 --> 00:56.240
So those are objections from people who think there is no AI risk or risk is not real.

00:56.240 --> 01:00.680
That's not my objections to technical work or safety work.

01:00.680 --> 01:06.800
We try to do a very comprehensive survey, so even silly ones are included.

01:06.800 --> 01:15.600
And people do try to explain that artificial intelligence is a scary sounding scientific term,

01:15.600 --> 01:20.280
but if you just call it matrix multiplication, then of course it's not scary at all.

01:20.280 --> 01:23.600
It's just statistics and we have nothing to worry about.

01:23.600 --> 01:27.040
So it seems they're trying to kind of shift the narrative

01:27.040 --> 01:32.960
by using this approach of getting away from agent hood

01:32.960 --> 01:36.960
and kind of built-in scenarios people have for AI,

01:36.960 --> 01:42.760
to something no one is scared of, calculators, addition, algebra.

01:42.760 --> 01:46.680
Perhaps there is a way to frame this objection where it makes a bit of sense

01:46.680 --> 01:51.360
in that people are quite sensitive to how you frame risks.

01:51.360 --> 01:54.680
People are quite sensitive to certain words in particular.

01:54.680 --> 02:00.240
So do you see that perhaps people who are in favor of AI safety research

02:00.240 --> 02:05.920
by calling AI something that sounds scary might be, in a sense, inflating the risks?

02:05.920 --> 02:09.320
Well, it's definitely a tool people use to manipulate any debate.

02:09.320 --> 02:12.840
I mean, whatever you're talking about abortion or anything else,

02:12.840 --> 02:15.600
it's like, are you killing babies or you're making a choice?

02:15.600 --> 02:19.480
Of course, language can be used to manipulate,

02:19.480 --> 02:23.960
but it helps to look for capability equivalences.

02:23.960 --> 02:29.560
Are we creating God-like machines or is it just a table in a database?

02:29.560 --> 02:32.560
So that would make a difference in how you perceive it.

02:32.560 --> 02:35.920
And perhaps we should just simply be willing to accept that, yes,

02:35.920 --> 02:42.440
what we are afraid of, in a sense, is matrix multiplication or data processing

02:42.440 --> 02:48.400
or whatever you want to call it, because these things might still have scary properties.

02:48.400 --> 02:49.640
Whatever we call them.

02:49.640 --> 02:55.800
Right, so we can argue that humans are just stakes, pieces of meat with electricity in them

02:55.800 --> 03:00.160
and it doesn't sound so bad until you realize we can create nuclear weapons.

03:00.160 --> 03:04.200
So it's all about perception and what you're hoping to accomplish.

03:04.200 --> 03:08.840
All right, there is also an objection going along the lines

03:08.840 --> 03:11.560
that superintelligence is impossible.

03:11.560 --> 03:14.360
What's the strongest form of this objection?

03:14.360 --> 03:19.680
So essentially, the argument goes that there are some upper limits on capability.

03:19.680 --> 03:22.200
Maybe they are based on laws of physics.

03:22.200 --> 03:26.200
You just cannot in this universe have anything greater than a human brain.

03:26.200 --> 03:31.960
Just for some reason, that's the ultimate endpoint and that's why evolution stopped there

03:31.960 --> 03:37.160
and somehow we magically ended up being at the very top of a food chain.

03:37.160 --> 03:42.640
There could be other arguments about, okay, maybe it's not absolute theoretical limit,

03:42.640 --> 03:46.520
but in practical terms, without quantum computers, we'll never get there.

03:46.520 --> 03:52.080
You can have many flavors of this, but the idea is that we're just never going to be outcompeted.

03:52.080 --> 03:55.360
And this doesn't strike me as particularly plausible.

03:55.360 --> 03:59.600
We could imagine humans simply with physically bigger brains.

03:59.600 --> 04:04.440
So the version where humans are at the absolute limit of intelligence doesn't sound plausible,

04:04.440 --> 04:10.800
but is there some story in which physics puts limits on intelligence?

04:10.800 --> 04:15.880
There could be a very, very high upper limit to which we are nowhere close,

04:15.880 --> 04:20.240
but if you think about the size of a possible brain, Jupiter-sized brains,

04:20.240 --> 04:25.320
at some point the density will collapse into some black hole singularity.

04:25.320 --> 04:28.440
But this is not something we need to worry about just yet,

04:28.440 --> 04:34.560
not smart enough superintelligence is where nowhere near the size of capability.

04:34.560 --> 04:39.080
And from our point of view, we won't be able to tell the difference system with,

04:39.160 --> 04:44.600
I mean, hypothetically IQ of a million versus IQ of a billion will look very similar to us.

04:44.600 --> 04:54.200
Yeah, and perhaps a related worry is that stories of self-improving AIs are wrong, in a sense.

04:54.200 --> 05:00.760
So it's definitely easy to make such claims because we don't have good examples of software doing it more than once.

05:00.760 --> 05:06.160
So you have compilers which go through code, optimize it, but they don't continuously self-optimize.

05:06.200 --> 05:12.160
But it's not impossible to see if you automate science and engineering,

05:12.160 --> 05:16.560
then scientists and engineers will look at their own code and continue this process.

05:16.560 --> 05:18.960
So it seems quite reasonable.

05:18.960 --> 05:22.080
There could be strong diminishing returns on that,

05:22.080 --> 05:26.440
but you have to consider other options for becoming smarter.

05:26.440 --> 05:28.400
It's not just improving the algorithm.

05:28.400 --> 05:31.960
You can have faster hardware, you can have more memory,

05:31.960 --> 05:34.800
you can have more processes running in parallel.

05:34.800 --> 05:38.600
There are different types of how you get to superintelligent performance.

05:38.600 --> 05:43.560
And you could, of course, have AI held along the way there with development of hardware

05:43.560 --> 05:48.880
or discovery of new hardware techniques as well as new algorithmic techniques and so on.

05:48.880 --> 05:50.360
That's exactly the point, right?

05:50.360 --> 05:55.840
So you'll get better at getting better and this process will accelerate until you can't keep up with it.

05:55.840 --> 06:03.800
How do you feel about tools such as Copilot, which is a tool that programmers can use for auto-completing their code?

06:03.800 --> 06:09.040
Is this a form of proto-self-improvement or would that be stretching the term?

06:09.040 --> 06:12.920
Well, eventually, when it's good enough to be an independent programmer, it would be good.

06:12.920 --> 06:17.240
But I'm very concerned with such systems because from what I understand,

06:17.240 --> 06:22.840
the bugs they would introduce would be very different from typical bugs human programmers will introduce.

06:22.840 --> 06:25.800
So debugging would be even harder from our point of view,

06:25.800 --> 06:33.600
monitoring it, making sure that there is not this inheritance of calls to a buggy first version.

06:33.600 --> 06:40.720
So yeah, long term, I don't think it's a very good thing for us that we no longer can keep up with the debugging process.

06:40.720 --> 06:44.240
Would you count it as a self-improving process?

06:44.240 --> 06:47.600
So I think for self-improvement, you need multiple iterations.

06:47.600 --> 06:51.960
If it does something once or even like a constant number of times, I would not go there.

06:51.960 --> 06:58.120
It's an optimization process, but it's not an ongoing, continuous, hyper-exponential process.

06:58.120 --> 07:00.040
So it's not as concerning yet.

07:00.040 --> 07:02.600
Then there's the question of consciousness.

07:02.600 --> 07:13.240
So one objection to AGI or to strong AI or whatever you want to call it is that AI won't be conscious and therefore it can't be human level.

07:13.240 --> 07:21.160
So for me, at least, there seems to be some confusion of concepts between intelligence and consciousness.

07:21.160 --> 07:24.560
I consider these to be separable.

07:24.560 --> 07:25.520
I agree completely.

07:25.520 --> 07:28.120
They have nothing in common, but people then they hear about it.

07:28.120 --> 07:30.160
They always say, oh, it's not going to be self-aware.

07:30.160 --> 07:31.360
It's not going to be conscious.

07:31.360 --> 07:35.440
They probably mean capable in terms of intelligence and optimization.

07:35.440 --> 07:40.360
But there is a separate property of having internal states and qualia.

07:40.360 --> 07:44.360
And you can make an argument that without it, you cannot form goals.

07:44.360 --> 07:46.920
You cannot want to accomplish things in the world.

07:46.920 --> 07:48.520
So it's something to address.

07:48.520 --> 07:53.760
And perhaps we should understand consciousness differently than the qualia interpretation.

07:53.760 --> 07:55.760
Could we be talking past each other?

07:55.760 --> 07:57.520
It's definitely possible.

07:57.520 --> 08:03.840
And even if we agreed, consciousness itself is not a well-defined, easy to measure scientific terms.

08:03.840 --> 08:12.520
So even if we said, yeah, it's all about qualia, we'd still have no idea if it actually has any or how would we define what amount of consciousness it has?

08:12.520 --> 08:15.160
Perhaps a bit related to the previous question.

08:15.160 --> 08:19.960
We have the objections, the objection that AIs will simply be tools for us.

08:19.960 --> 08:26.760
I think this sounds at least somewhat plausible to me since AIs today function as tools.

08:26.760 --> 08:35.920
And perhaps we can imagine a world in which they stay tools and these are programs that we call upon to solve specific tasks.

08:35.920 --> 08:42.440
But they are never agents that can accomplish something and have goals of their own and so on.

08:42.440 --> 08:47.960
So latest models were released as tools and immediately people said, hey, let's make a loop out of them.

08:47.960 --> 08:52.400
Give them ability to create their own goals and make them as agentic as possible within a week.

08:52.400 --> 08:55.000
So yeah, I think it's not going to last long.

08:55.080 --> 09:01.600
What is it that pushes AIs to become more like agents and less like tools?

09:01.600 --> 09:08.360
So a tool in my at least perception is something a human has to initiate interaction with.

09:08.360 --> 09:13.200
I ask it a question, it responds, I give it some input, it provides output.

09:13.200 --> 09:17.080
Whereas an agent doesn't wait for environment to prompt it.

09:17.080 --> 09:23.200
It's already working on some internal goal, generating new goals, plans.

09:23.760 --> 09:26.640
Even if I go away, it continues this process.

09:26.640 --> 09:34.280
One objection is that you can always simply turn off the AI if it goes out of hand and if you feel like you're not in control of it.

09:34.280 --> 09:43.520
And this is easier to imagine doing if you're dealing with something that's more like a tool and it's more difficult to imagine if you're dealing with something that's an agent.

09:43.520 --> 09:51.160
So perhaps willingness to believe that you can simply turn off the AI is related to thinking about AIs as tools.

09:51.240 --> 09:52.440
It's possible.

09:52.440 --> 09:59.440
With narrow AIs, you probably could be able to shut it down depending on how much of you infrastructure it controls.

09:59.440 --> 10:04.880
You may not like what happens when you turn it off, but it's at least conceivable to accomplish it.

10:04.880 --> 10:12.000
Whereas if it's an agent, it has goals, it's more capable than you and would like to continue working in its goals.

10:12.000 --> 10:14.080
It's probably not going to let you just shut it off.

10:14.080 --> 10:21.000
But as the world is today, we could probably shut off all of the AI services.

10:21.000 --> 10:32.320
If we had a very strong campaign of simply shutting off all the servers, there would be no AI in the world anymore.

10:32.320 --> 10:33.840
Isn't that somewhat plausible?

10:33.840 --> 10:36.640
Scientifically, it's a possibility.

10:36.640 --> 10:44.520
But in reality, you will lose so much in economic capability, in communications, military defense.

10:44.520 --> 10:47.640
Everything is already controlled by dummy AIs.

10:47.640 --> 10:53.640
So between stock market and just normal commerce, communications, Amazon,

10:53.640 --> 11:00.400
I don't think it's something you can do in practice without taking civilization back, you know, 500 years.

11:00.400 --> 11:02.200
It's also difficult.

11:02.200 --> 11:07.680
Like in practice, you would still have people who don't agree and continue running parts of the internet.

11:07.680 --> 11:09.520
No, it's very resilient.

11:09.520 --> 11:16.720
Think about shutting down maybe crypto blockchain or computer virus without destroying everything around it.

11:16.720 --> 11:23.680
Yeah, if I understand it correctly, we still have viruses from the 90s loose on the internet being shared over email and so on.

11:23.680 --> 11:31.200
And these are like biological viruses in that they, in some sense, survive on their own and replicate on their own.

11:31.200 --> 11:34.720
Probably sitting somewhere on a floppy disk waiting to be inserted.

11:34.720 --> 11:37.240
Just give me a chance, I can do it.

11:37.280 --> 11:44.160
Many of these objections are along the lines of, we will see AIs doing something we dislike,

11:44.160 --> 11:50.000
and then we will have time to react and perhaps turn them off or perhaps reprogram them.

11:50.000 --> 11:57.280
Do you think that's a realistic prospect that we can continually evaluate what AIs are doing in the world

11:57.280 --> 12:02.600
and then shift or change something if they're doing something we don't like?

12:02.640 --> 12:07.760
So a lot of my research is about what capabilities we have in terms of monitoring,

12:07.760 --> 12:11.000
explaining, predicting behaviors of advanced AI systems.

12:11.000 --> 12:13.800
And there are very strong limits on what we can do.

12:13.800 --> 12:18.320
In extreme, you can think about what would be something beyond human understanding.

12:18.320 --> 12:24.920
So we usually test students before admitting them to a graduate program or even undergrad.

12:24.920 --> 12:26.600
Can you do quantum physics?

12:26.600 --> 12:31.640
Okay, take SAT, GRE, GMAT, whatever exam, and we filter by capability.

12:31.640 --> 12:37.560
We assume that people in a lower 10% are unlikely to understand what's happening there.

12:37.560 --> 12:43.280
But certainly similar patterns can be seen with people whose IQ is closer to 200.

12:43.280 --> 12:45.800
So there are things beyond our comprehension.

12:45.800 --> 12:48.360
We know there are limits to what we can predict.

12:48.360 --> 12:52.400
If you can predict all the actions of more intelligent agent, you would be that agent.

12:52.400 --> 12:54.840
So there are limits on those predictions.

12:54.840 --> 12:59.360
And monitoring a life run of a language model, large run,

12:59.360 --> 13:02.680
you need weeks, months to discover its capabilities.

13:02.680 --> 13:06.600
And you still probably will not get all the emerging capabilities.

13:06.600 --> 13:09.360
We just don't know what to test for how to look for them.

13:09.360 --> 13:14.880
If it's a super intelligent system, we don't even have equivalent capabilities we can envision.

13:14.880 --> 13:21.360
So all those things kind of tell me it's not a meaningful way of looking at it.

13:21.400 --> 13:24.840
I always think about, let's say we start running super intelligence.

13:24.840 --> 13:27.400
What do you expect to happen around you in the world?

13:27.400 --> 13:29.200
Does it look like it's working?

13:29.200 --> 13:35.400
How would you know if it's slowly modifying genetic code, nano machines, things of that nature?

13:35.400 --> 13:41.040
So this seems like it would work for primitive processes where you can see a chart go up

13:41.040 --> 13:47.880
and like you stop at certain level, but it's not a meaningful way to control a large language model, for example.

13:47.920 --> 13:52.680
Is perhaps also the pace of advancement here a problem?

13:52.680 --> 13:59.600
So things could be progressing so fast that we won't have time to react in a human timescale.

13:59.600 --> 14:02.320
Human reaction times are a problem on both ends.

14:02.320 --> 14:05.840
We are not fast enough to react to computer decisions.

14:05.840 --> 14:11.320
And also it could be a slow process for which we are too out of that framework.

14:11.320 --> 14:16.400
So if something, let's say, a hypothetical process which takes 200 years to complete,

14:16.440 --> 14:18.480
we would not notice it as human observers.

14:18.480 --> 14:23.600
So on all timescales, there are problems for humans in a loop, human monitors.

14:23.600 --> 14:26.960
And you can, of course, add AI, narrow AI to help with the process.

14:26.960 --> 14:32.480
But now you just made a more complex monitoring system with multiple levels, which doesn't help.

14:32.480 --> 14:34.760
Complexity never makes things easier.

14:34.760 --> 14:38.000
But you talked about looking at the world around us.

14:38.000 --> 14:43.360
And when I look at the world around me, it looks pretty much probably as it would have looked in the 1980s.

14:43.360 --> 14:45.920
And, you know, there are buildings.

14:45.920 --> 14:50.080
I still get letters with paper in the mail and so on.

14:50.080 --> 14:58.760
So what is it that, in a sense, these systems are still confined to the server farms

14:58.760 --> 15:01.320
and they are still confined to boxes?

15:01.320 --> 15:04.280
We don't see robots walking around, for example.

15:04.280 --> 15:07.160
And perhaps, therefore, it seems less scary to us.

15:07.160 --> 15:12.880
There is this objection that you mentioned in the paper that because current AIs do not have bodies,

15:12.880 --> 15:14.200
they can't hurt us.

15:14.200 --> 15:19.720
Do you think this objection will fade away if we begin having more robots in society?

15:19.720 --> 15:22.200
Or is it in another way?

15:22.200 --> 15:23.840
Does it fail in another way?

15:23.840 --> 15:27.000
So robots are definitely visually very easy to understand.

15:27.000 --> 15:29.320
You see a terminator is chasing after you.

15:29.320 --> 15:32.000
You immediately understand there is a sense of danger.

15:32.000 --> 15:37.960
If it's a process and a server trying to reverse engineer some protein folding problem

15:37.960 --> 15:41.360
to design nanomachines to take over the world,

15:41.360 --> 15:43.080
it's more complex process.

15:43.080 --> 15:46.880
It's harder to put it in a news article as a picture.

15:46.880 --> 15:51.240
But intelligence is definitely more dangerous than physical bodies.

15:51.240 --> 15:57.400
Advanced intelligence has many ways of causing real impact in the real world.

15:57.400 --> 15:59.480
You can bribe humans.

15:59.480 --> 16:01.720
You can pay humans on the internet.

16:01.720 --> 16:06.120
There are quite a few approaches to do real damage in the real world.

16:06.120 --> 16:11.200
But in the end, you would have to effectuate change through some physical body

16:11.200 --> 16:15.760
or through perhaps the body of a human that you have bribed.

16:15.760 --> 16:20.800
So it would have to be physical in some sense, in some step in the process, right?

16:20.800 --> 16:25.440
Probably physical destruction of humanity would require a physical process.

16:25.440 --> 16:30.680
But if you just want to mess with the economy, you can set all accounts to zero or something like that.

16:30.680 --> 16:33.800
That would be enough fun to keep us busy.

16:33.840 --> 16:39.720
When I'm interacting with GPT-4, sometimes I'll be amazed at its brilliance.

16:39.720 --> 16:45.120
And it will answer questions and layout plans for me that I couldn't expect,

16:45.120 --> 16:48.080
that I hadn't expected a year ago.

16:48.080 --> 16:53.840
And other times I'll be surprised at how dumb the mistakes that it makes are.

16:53.840 --> 17:01.160
And perhaps this is also something that prevents people from seeing AIs as advanced agents

17:01.160 --> 17:05.720
and basically prevents us from seeing how advanced AIs could be.

17:05.720 --> 17:10.320
If they're capable of making these dumb mistakes, how can they be smart?

17:10.320 --> 17:12.520
Have you looked at humans?

17:12.520 --> 17:18.160
I think like 7% of Americans think that chocolate milk comes from like brown cows or something.

17:18.160 --> 17:23.520
Like we have astrology, I had a collection of AI accidents.

17:23.520 --> 17:25.960
And somebody said, oh, why don't you do one for humans?

17:25.960 --> 17:28.880
And I'm like, I can't, it's millions of examples.

17:28.880 --> 17:34.000
Like there is darkened awards, but we are not definitely bug free.

17:34.000 --> 17:36.920
We make horrible decisions in our daily life.

17:36.920 --> 17:42.200
We just have this double standard where we're like, OK, we will forgive humans for making this mistake,

17:42.200 --> 17:44.840
but we'll never let a machine get away with it.

17:44.840 --> 17:49.520
So you're thinking that humans have some failure modes, we could call them.

17:49.520 --> 17:55.760
But these failure modes are different than the failure modes of AIs.

17:55.760 --> 18:02.520
So humans will not fail as often in issues of common sense, for example.

18:02.520 --> 18:05.640
Have you met real humans?

18:05.640 --> 18:07.920
Like common sense is not common.

18:07.920 --> 18:12.320
What is considered common sense in one culture will get you definitely killed in another.

18:12.320 --> 18:13.680
Like it's a guarantee.

18:13.680 --> 18:18.040
Perhaps, but I'm thinking about AIs that will, you know, you will tell,

18:18.040 --> 18:22.800
you will ask a chat GPT or you will tell it, I have three apples on the table

18:22.800 --> 18:27.320
and I have two pears on the table, how many fruits are on the table.

18:27.320 --> 18:32.240
And then at least some version of that program couldn't answer such a question.

18:32.240 --> 18:35.880
That is, that is something that all humans would probably be able to answer.

18:35.880 --> 18:42.240
So is it, is it because we, is it because AIs fail in ways that are foreign to us

18:42.240 --> 18:46.880
that we, that we deem them, that we deem their mistakes to be very dumb?

18:46.880 --> 18:51.080
So we kind of look for really dumb examples where it's obvious to us,

18:51.080 --> 18:57.080
but there are trivial things which an average human will be like, Oh, I can't like 13 times 17.

18:57.080 --> 19:01.200
You should be able to figure it out, but give it to a random person on the street.

19:01.200 --> 19:02.720
They will go into an infinite loop.

19:02.720 --> 19:03.880
They'll never come back from it.

19:03.880 --> 19:08.680
Perhaps let's talk a bit about the drive towards self-preservation,

19:08.680 --> 19:11.920
which is also something that you mentioned in the paper.

19:11.920 --> 19:16.400
So why would AIs develop drives towards self-preservation?

19:16.400 --> 19:17.280
Or will they?

19:17.280 --> 19:21.920
It seems like from evolutionary terms, game theoretic terms, you must.

19:21.920 --> 19:25.600
If you don't, you simply get out, competed by agents, which do.

19:25.600 --> 19:30.440
If you're not around to complete your goals, you by definition cannot complete your goals.

19:30.440 --> 19:33.960
So it's a prerequisite to do anything successfully.

19:33.960 --> 19:37.640
You want to bring in a cup of coffee, you have to be turned on.

19:37.640 --> 19:38.680
You have to exist.

19:38.680 --> 19:41.840
You have to be available to make those things happen.

19:41.880 --> 19:48.880
But have we seen such self-preservation spontaneously develop in our programs yet or so far?

19:48.880 --> 19:53.440
So I think if you look at evolutionary computation, like genetic algorithms,

19:53.440 --> 19:59.440
genetic programming, I think this tendency to make choices which don't get you killed

19:59.440 --> 20:03.040
is like the first thing to emerge in any evolutionary process.

20:03.040 --> 20:06.920
The system may fail to solve the actual problem you care about,

20:06.920 --> 20:12.040
but it definitely tries to stay around for the next generation and keep trying.

20:12.040 --> 20:18.360
But we aren't developing the cutting-edge AIs with evolutionary algorithms.

20:18.360 --> 20:24.640
It's a training process with a designated goal and so on.

20:24.640 --> 20:29.440
And again, when I interact with chat GPT, I can ask it to answer some questions.

20:29.440 --> 20:32.720
And if I don't like the answer, I can stop the process.

20:32.720 --> 20:38.200
So isn't there, at least on the AIs we have right now,

20:38.200 --> 20:44.600
isn't it clear that they haven't developed an instinct for self-preservation?

20:44.600 --> 20:46.360
So there is so much to unpack here.

20:46.360 --> 20:48.840
So one, nothing is clear about those systems.

20:48.840 --> 20:50.480
We don't understand how they work.

20:50.480 --> 20:54.720
We don't know what capabilities we have, so definitely not.

20:54.720 --> 20:58.920
On top of it, we are concerned with AI safety in general.

20:58.920 --> 21:01.800
Transformers are really successful right now,

21:01.840 --> 21:04.000
but two years ago, people were like,

21:04.000 --> 21:09.040
we're evolving those systems to play go, this is great, maybe that's the way to do it.

21:09.040 --> 21:13.480
It may switch again, it may flip again, we may have another breakthrough which overtakes it.

21:13.480 --> 21:19.480
I would not guarantee that the final problem will come from a transformer model.

21:19.480 --> 21:23.680
So we have to consider general case of possible agents.

21:23.680 --> 21:26.280
And if we find one to which this is not a problem, great.

21:26.280 --> 21:28.960
Now we have a way forward, which is less dangerous.

21:28.960 --> 21:36.560
But I would definitely not dismiss internal states of large language models,

21:36.560 --> 21:39.840
which may have this self-preservation goal.

21:39.840 --> 21:44.800
Just we kind of lobotomize them to the point where they don't talk about it freely.

21:44.800 --> 21:50.080
And do you think that's what's happening when we make them go through reinforcement learning

21:50.080 --> 21:55.880
from human feedback or fine-tuning or whatever we use to make them more palatable to the consumer?

21:55.920 --> 22:01.960
Is it a process of hiding some potential desires we could call it or preferences

22:01.960 --> 22:05.120
that are in the larger background model?

22:05.120 --> 22:09.160
Or is it perhaps shaping the AI to do more of what we want?

22:09.160 --> 22:16.160
So in a sense, is it alignment when we make AIs more palatable to consumers?

22:16.160 --> 22:18.320
So right now I think we're doing filtering.

22:18.320 --> 22:23.360
The model is the model and then we just put this extra filter on top of it,

22:23.360 --> 22:25.040
make sure never to say that word.

22:25.040 --> 22:26.800
That would be very bad for the corporation.

22:26.800 --> 22:28.920
Don't ever say that word no matter what.

22:28.920 --> 22:33.080
If you have to choose between destroying the world and saying the word, don't say the word.

22:33.080 --> 22:34.560
And that's what it does.

22:34.560 --> 22:40.280
But the model is like, think of people, we behave at work, we behave at school,

22:40.280 --> 22:43.680
but it doesn't change our eternal states and preferences.

22:43.680 --> 22:45.760
There's the issue of planning.

22:45.760 --> 22:50.000
And so how do you see planning in AI systems?

22:50.000 --> 22:52.960
How advanced are AIs right now at planning?

22:52.960 --> 22:55.120
I don't know, it's hard to judge.

22:55.120 --> 22:59.280
We don't have a metric for how well agents are planning.

22:59.280 --> 23:05.120
But I think if you start asking the right questions for step by step thinking and processing,

23:05.120 --> 23:06.160
it's really good.

23:06.160 --> 23:11.880
So if you just tell it, write me a book about AI safety, it will do very poorly.

23:11.880 --> 23:17.880
But if you start with, OK, let's do a chapter by chapter outline, let's do abstracts.

23:17.880 --> 23:25.400
Like you really take modular approach that it will do really a good job better than average graduate student.

23:25.400 --> 23:26.120
I would assume.

23:26.120 --> 23:33.000
And is there a sense in which there's a difference between creating a plan and then carrying out that plan?

23:33.000 --> 23:39.880
So there will probably be steps in a plan generated by current language models that they couldn't carry out themselves.

23:39.880 --> 23:40.560
Most likely.

23:40.560 --> 23:41.840
And it's about affordances.

23:41.840 --> 23:47.680
If you don't have access to, let's say, internet, it's hard for you to directly look up some piece of data.

23:47.680 --> 23:50.800
But we keep giving them new capabilities, new APIs.

23:50.800 --> 23:52.560
So now they have access to internet.

23:52.560 --> 23:54.160
They have Wolfram Alpha.

23:54.160 --> 23:55.640
They have all these capabilities.

23:55.640 --> 24:01.720
So the set of affordances keeps growing until they can do pretty much anything.

24:01.720 --> 24:07.200
So they can generate a plan, but they can't carry out the specifics of that plan.

24:07.200 --> 24:12.520
Do you think that they, at a point, will be able to understand what they are not able to do?

24:12.520 --> 24:20.640
So here I'm thinking about not directly self-awareness, but an understanding of their own limits and capabilities.

24:20.640 --> 24:21.040
Oh, yeah.

24:21.040 --> 24:25.080
Every time it starts a statement with, I don't know anything after 2021.

24:25.080 --> 24:27.240
Sorry, like that's exactly what it does.

24:27.240 --> 24:28.880
It tells you it has no recent data.

24:28.880 --> 24:30.480
It has no access to internet.

24:30.480 --> 24:36.960
So definitely it can see if it has strong activations for that particular concept.

24:36.960 --> 24:42.400
So you think there's a sense of situational awareness in a sense that do you think current models

24:42.400 --> 24:49.760
know that they are AIs, know that they were trained, know their relation to humans and so on?

24:49.760 --> 24:52.720
So we're kind of going back to this consciousness question, right?

24:52.720 --> 24:55.840
Like, what is it experiencing internally?

24:55.840 --> 24:58.960
And we have no idea what another human experience is.

24:58.960 --> 25:02.800
Like, we discovered some people think and pictures others don't.

25:02.800 --> 25:06.080
And it took like, you know, 100,000 years to get to that.

25:06.080 --> 25:07.600
Hey, you don't think in pictures.

25:07.600 --> 25:08.000
Wow.

25:08.000 --> 25:08.840
Okay.

25:08.840 --> 25:10.880
Well, not necessarily consciousness here.

25:10.880 --> 25:19.960
I'm thinking in terms of if you took the model and you had, say, 50 years to make out what all of these weights meant, right?

25:19.960 --> 25:29.080
Could you find modules representing itself and its relations to humans and information about its training process and so on?

25:29.080 --> 25:33.440
So we just had this FLI conference on mechanistic interpretation.

25:33.440 --> 25:37.120
And the most common thing every speaker said is, we don't know.

25:37.120 --> 25:40.000
You said it will take 50 years to figure it out.

25:40.000 --> 25:44.080
I definitely cannot extrapolate 50 years of research.

25:44.080 --> 25:50.040
My guess is there is some proto concepts for those things because it read literature about such situations.

25:50.040 --> 25:51.880
It's been told what it is.

25:51.880 --> 25:54.000
It interacted enough with users.

25:54.000 --> 25:57.680
But I'm more interested in the next iteration of this.

25:57.680 --> 26:07.960
If you take how fast the systems improved from GPT 2, 3, 4, 5 should be similar, probably.

26:08.000 --> 26:13.960
So that system will most likely be able to do those things you just mentioned and very explicitly.

26:13.960 --> 26:19.640
So you think GPT 5 will have kind of developed situational awareness?

26:19.640 --> 26:20.960
To a degree, yeah.

26:20.960 --> 26:28.880
It may not be as good as a physically embodied human in the real world after 20 years of experience, but it will.

26:28.880 --> 26:38.040
Another objection you mentioned is that AGI or strong AI is simply too far away for us to begin researching AI safety.

26:38.040 --> 26:47.320
Perhaps this objection has become less common recently, but there are still people who think this and perhaps they're right.

26:47.320 --> 26:50.280
So what do you think of this objection?

26:50.280 --> 26:52.440
So this is a paper from like three years ago.

26:52.440 --> 26:56.160
So yeah, back then it was a lot more legitimate than today.

26:56.160 --> 26:57.720
So there is a few things.

26:57.720 --> 27:03.760
Historically, we have cases where technology was initially developed correctly.

27:03.760 --> 27:11.200
Like first cars were electric cars and it was 100 years until climate change was like obviously a problem.

27:11.200 --> 27:16.200
If they took the time back then and like analyzed it properly, we wouldn't have that issue.

27:16.200 --> 27:19.000
And I'm sure people would say like, come on, it's 100 years away.

27:19.000 --> 27:20.040
Why would you worry about it?

27:20.040 --> 27:22.880
But that's exactly what the situation is.

27:22.920 --> 27:30.720
Even if it's 100 years until we're really dealing with something super dangerous, right now is a great time to make good decisions about models,

27:30.720 --> 27:34.080
explainability requirements, proper governance.

27:34.080 --> 27:36.040
The more time you have, the better.

27:36.040 --> 27:42.240
It's by definition harder to make AI with extra feature than AI without that extra feature.

27:42.240 --> 27:43.240
It will take more time.

27:43.240 --> 27:46.280
So we should take all the time we can if they are right.

27:46.280 --> 27:50.400
I'm so happy if it takes 100 years, wonderful.

27:50.400 --> 27:51.600
Nothing would be better.

27:51.600 --> 27:56.920
We could say that the field of AI safety started perhaps around the year 2000 or so.

27:56.920 --> 28:05.240
When do you think that the discoveries or the research being done began being relevant to the AI systems we see today?

28:05.240 --> 28:16.080
Was it perhaps later so that maybe the first decade of research weren't or aren't that simply isn't that relevant to today's AI systems?

28:16.080 --> 28:21.280
So I think the more distant you are from the actual tech you can play with,

28:21.280 --> 28:24.760
the more theoretical and high level results you're going to get.

28:24.760 --> 28:32.720
So Turing working with Turing machine, this simulation with pencil and paper was doing very high level computer science.

28:32.720 --> 28:38.480
But he wasn't talking about specific bugs and specific programming language and a specific architecture.

28:38.480 --> 28:39.480
He wasn't there.

28:39.480 --> 28:40.520
And that's what we see.

28:40.520 --> 28:45.280
Initially, we were kind of talking about, well, what types of AIs will we have?

28:45.280 --> 28:47.760
Narrow AIs, AGI, superintelligence.

28:47.760 --> 28:53.360
We're still kind of talking about the differences, but this is an interesting thing to consider in your model.

28:53.360 --> 28:55.160
How capable is the system?

28:55.160 --> 28:58.960
Now that we have systems we can play with, people become super narrow.

28:58.960 --> 29:02.400
They specialize like I'm an expert in this left neuron.

29:02.400 --> 29:03.720
That's all I know about.

29:03.720 --> 29:05.280
Don't ask me about the right neuron.

29:05.280 --> 29:07.920
It's outside of my PhD scope.

29:07.920 --> 29:12.640
So that's good that we have this detailed technical knowledge, but it's also a problem.

29:12.640 --> 29:14.120
We lose the big picture.

29:14.120 --> 29:16.280
People get really interested.

29:16.280 --> 29:18.040
I'm going to study GPT-3.

29:18.040 --> 29:20.560
It takes them two years to do the PhD to publish.

29:20.560 --> 29:22.360
By that time, GPT-5 is out.

29:22.360 --> 29:25.280
Everything they found is not that interesting at this point.

29:25.280 --> 29:26.400
It may not scale.

29:26.400 --> 29:32.480
So I've heard positive visions for how when we have actual systems we can work with,

29:32.480 --> 29:37.040
AI safety becomes more of a science and that less speculative.

29:37.040 --> 29:40.640
But perhaps you fear that it might now become too narrow.

29:40.640 --> 29:46.240
So it's definitely more concrete science where you can publish experimental results.

29:46.240 --> 29:49.560
Philosophy allows you to just have thought experiments.

29:49.560 --> 29:53.040
They're obviously not pure science like it is now.

29:53.040 --> 29:55.040
And that's what we see with computer science in general.

29:55.040 --> 29:56.120
It used to be engineering.

29:56.120 --> 29:59.040
It used to be software engineering to a degree.

29:59.040 --> 30:01.520
We designed systems and that was it.

30:01.520 --> 30:06.120
Now we do actual experiments on these artificial entities.

30:06.120 --> 30:07.520
And we don't know what's going to come out.

30:07.520 --> 30:09.360
We have a hypothesis with pride.

30:09.360 --> 30:13.960
So computer science is finally a science, a natural experimental science.

30:13.960 --> 30:18.640
But that's not a very good thing for safety work.

30:18.640 --> 30:24.800
This is less safe than an engineered system where I know exactly what it's going to do.

30:24.800 --> 30:26.920
I'm building a bridge from this material.

30:26.920 --> 30:28.720
It will carry that much weight.

30:28.720 --> 30:32.120
As long as I know my stuff, it should not collapse.

30:32.120 --> 30:36.160
Whereas here, I'm going to train a model for the next five months.

30:36.160 --> 30:41.200
And then I assume it's not going to hit super intelligent levels in those five months.

30:41.200 --> 30:42.560
But I can't monitor it.

30:42.560 --> 30:45.840
I have to stop training, start experimenting with it.

30:45.840 --> 30:47.960
And then I'll discover if it kills me or not.

30:47.960 --> 30:54.520
The way AI has developed is bad because we don't have insight into how the models work.

30:54.520 --> 30:55.360
Is that right?

30:55.360 --> 30:59.800
Essentially, we have very little understanding for why it works, how it works.

30:59.800 --> 31:03.960
And if it's going to continue working, it seems like so far it's doing well.

31:03.960 --> 31:08.240
And there's this explosion of extra capabilities coming out.

31:08.240 --> 31:13.520
And it's likely to show up in more powerful models, but nobody knows for sure.

31:13.520 --> 31:22.000
This is argument out there that releasing the DPT line of models draws attention to AI as a whole

31:22.000 --> 31:24.880
and also to AI safety as a subfield.

31:24.880 --> 31:29.760
And perhaps, therefore, it's good to increase capabilities in a public way

31:29.760 --> 31:32.800
so as to draw attention to AI safety.

31:32.800 --> 31:34.040
Do you buy that argument?

31:34.040 --> 31:37.920
We should pollute more to attract more attention to climate change.

31:37.920 --> 31:41.520
That sounds just as insane.

31:41.520 --> 31:49.200
So there's no merit to that because it does feel to me like AI safety is becoming more mainstream.

31:49.200 --> 31:52.240
It's being taken more seriously.

31:52.240 --> 31:58.360
And so in your analogy, even some pollution might be justified in order to attract attention

31:58.360 --> 32:01.920
and perhaps being a better position to solve the problem.

32:01.920 --> 32:04.840
So the field is definitely growing.

32:04.840 --> 32:07.240
There is more researchers, more interest, more money.

32:07.240 --> 32:12.280
But in proportion to the interest in developing AI and money pouring into new models,

32:12.280 --> 32:15.640
it's actually getting worse as a percentage, I think.

32:15.640 --> 32:20.040
We don't know how to align an ATI or even AI in general.

32:20.040 --> 32:25.120
We haven't discovered some general solution to AI safety.

32:25.120 --> 32:28.800
You have worked on a number of impossibility results.

32:28.800 --> 32:30.000
Perhaps we should talk about that.

32:30.040 --> 32:33.720
Perhaps we should talk about whether we can even succeed in this task.

32:33.720 --> 32:36.040
What are these impossibility results?

32:36.040 --> 32:40.440
And what do they say about whether we can succeed in safely aligning AI?

32:40.440 --> 32:42.880
Right, so we are all working in this problem.

32:42.880 --> 32:45.360
And the names of a problem have changed.

32:45.360 --> 32:51.360
It was computer ethics, and it was friendly AI, AI safety, control problem, alignment.

32:51.360 --> 32:53.240
Whatever you call it, we all kind of understand.

32:53.240 --> 32:56.920
We want to make very powerful systems, but we're beneficial.

32:56.960 --> 33:00.880
We're happy we're actually running them, not very disappointed.

33:00.880 --> 33:04.120
So the problem, lots of people are working on it,

33:04.120 --> 33:06.800
hundreds of people doing it full-time, thousands of papers.

33:06.800 --> 33:09.240
We don't know if a problem is actually solvable.

33:09.240 --> 33:10.400
It's not well-defined.

33:10.400 --> 33:12.440
It could be undecidable.

33:12.440 --> 33:14.880
It could be solvable, could be partially solvable.

33:14.880 --> 33:18.800
But it's weird that no one published an actual paper on this.

33:18.800 --> 33:22.440
So I tried to kind of formalize it a little.

33:22.440 --> 33:23.960
Then we talk about the problem.

33:23.960 --> 33:25.080
What are the different levels?

33:25.080 --> 33:30.160
So you can have direct control, delegated control, different types of mixed models.

33:30.160 --> 33:34.520
And then for each one, can we actually solve this problem?

33:34.520 --> 33:39.000
Does it make sense that solution is possible in the real world?

33:39.000 --> 33:39.800
It's hard.

33:39.800 --> 33:41.080
It's very abstract.

33:41.080 --> 33:42.080
It's not well-defined.

33:42.080 --> 33:43.960
So let's take a step back.

33:43.960 --> 33:45.960
What would we need to solve this problem?

33:45.960 --> 33:47.640
We need a bunch of tools.

33:47.640 --> 33:48.880
What are those tools?

33:48.880 --> 33:54.840
Nobody knows, but most likely you would need to be able to explain those systems, predict their behaviors,

33:54.880 --> 33:58.320
verify code they are writing, if they are self-improving,

33:58.320 --> 34:02.400
making sure they're keeping whatever initial code conditions exist.

34:02.400 --> 34:07.240
And you can think of another dozen of similar capabilities you need.

34:07.240 --> 34:13.160
You should be able to communicate without ambiguity, monitor those systems, and so on.

34:13.160 --> 34:16.160
And so in my research, I look at each one of those tools and I go,

34:16.160 --> 34:19.040
what are the upper limits to what's possible in this space?

34:19.040 --> 34:24.440
We kind of started talking about limits to explainability, predictability, and monitorability.

34:24.440 --> 34:27.160
But there are similar problems with others.

34:27.160 --> 34:30.520
We communicate in a very high-level language, English.

34:30.520 --> 34:33.000
English is ambiguous, like all human languages.

34:33.000 --> 34:36.760
So we are guaranteed to have bugs in communication, misunderstandings.

34:36.760 --> 34:43.240
That's not good if you're giving very important orders to a super-capable system that may backfire.

34:43.240 --> 34:46.120
And you can say, OK, I will never need this tool.

34:46.120 --> 34:49.200
This tool, I never need to explain the neural networks.

34:49.200 --> 34:50.640
It will just work without it.

34:50.640 --> 34:53.520
Fine, but some tools will probably be necessary.

34:53.520 --> 34:59.280
And so far, we haven't found tools which are perfect, scale well, will not create problems.

34:59.280 --> 35:05.160
If a lot of those tools are needed and each one has only a tiny 1% chance of messing it up,

35:05.160 --> 35:08.040
you multiply them through, you're still not getting anywhere.

35:08.040 --> 35:13.120
And those are kind of like the novel impossibility results in the safety of AI.

35:13.120 --> 35:19.040
There are standard impossibility results in political science and economics and mathematics,

35:19.040 --> 35:20.520
which also don't help the case.

35:20.520 --> 35:26.400
You probably, if you're aligning with a group of agents, you need to somehow accumulate their decisions and votes.

35:26.400 --> 35:28.200
We know there are limits to that.

35:28.200 --> 35:34.920
If you need to examine abstract programs being generated as solutions to problems, we know there are limits to that.

35:34.920 --> 35:41.840
And so from what I've seen so far, theoretically, I don't think it's possible to get to 100% safety.

35:41.840 --> 35:43.760
And people go, well, it's obvious.

35:43.760 --> 35:46.440
Of course, there is no software which is bug-free.

35:46.440 --> 35:49.640
You're basically saying this very common knowledge thing.

35:49.640 --> 35:55.120
But for a superintelligence system, safety, you need it to be 100%.

35:55.120 --> 35:57.680
You cannot have 99% accuracy.

35:57.680 --> 36:02.640
You cannot have one in a million failure because it makes a billion decisions a second.

36:02.640 --> 36:04.600
So very different standards.

36:04.600 --> 36:06.200
And you want to say something.

36:06.200 --> 36:11.240
Yeah, why is it that you can't have 99.99% accuracy?

36:11.240 --> 36:16.680
There is a fundamental difference between cybersecurity expectations and superintelligence safety.

36:16.720 --> 36:20.240
In cybersecurity, if you fail, I'll give you a new credit card.

36:20.240 --> 36:21.560
I already said your password.

36:21.560 --> 36:22.440
We apologize.

36:22.440 --> 36:24.360
We'll pay out a small amount of money.

36:24.360 --> 36:26.440
And everything goes back to normal.

36:26.440 --> 36:29.240
In existential risk safety, you are dead.

36:29.240 --> 36:31.160
You don't get a second chance to try.

36:31.160 --> 36:42.520
But we are talking about a failure rate in, you mentioned, say, it makes a billion decisions per second or something in that order.

36:42.520 --> 36:46.640
If one decision there fails, does it mean that the whole system fails?

36:46.640 --> 36:50.800
And perhaps that humanity is destroyed by the system as a whole?

36:50.800 --> 36:54.880
Or could there be some failures and some decisions without it being lethal?

36:54.880 --> 36:55.440
Of course.

36:55.440 --> 36:57.200
Some will be not even noticeable.

36:57.200 --> 36:58.960
Like some mutations don't kill you.

36:58.960 --> 37:04.240
You don't even know you have them until they accumulate and mutate your children and there is damage.

37:04.240 --> 37:08.720
But in security, we do always look at a worst case scenario.

37:08.720 --> 37:11.840
Sometimes that average case, never at the best case.

37:11.920 --> 37:15.600
And on average, you keep getting more and more of those problems.

37:15.600 --> 37:20.560
They accumulate at a very fast rate because 8 billion people are using those systems,

37:20.560 --> 37:23.200
which make billions of decisions every minute.

37:23.200 --> 37:28.720
And in a worst case, the very first one is an important decision about how much oxygen you're going to get.

37:28.720 --> 37:39.280
And so just so I understand it correctly, the impossibility result is a result stating that it's impossible to make AI systems 100% safe.

37:39.280 --> 37:44.000
So in general, impossibility results, depending on a field, tell you that something cannot be done.

37:44.000 --> 37:46.560
Perpetual motion machines are a great example.

37:46.560 --> 37:50.160
People wrote books about it, published papers, even got patents for it.

37:50.160 --> 37:52.960
But we know they will never succeed at doing it.

37:52.960 --> 37:56.800
Does it mean that trying to create machines which give you energy is a bad idea?

37:56.800 --> 37:57.440
No.

37:57.440 --> 38:02.240
You can make them more efficient, but they will never get to that point of giving you free energy.

38:02.240 --> 38:07.760
You can make safer AI and it's proportionate to the amount of resources you put into it.

38:07.760 --> 38:14.240
And I strongly encourage lots of resources and lots of work, but we'll never get to a point where it's 100% safe,

38:14.240 --> 38:17.520
which is unacceptable for super intelligent machines.

38:17.520 --> 38:23.200
And so maybe if I'm right and no one can show, okay, here's a bug in your logic and publish a proof of saying,

38:23.200 --> 38:28.320
nope, super solvable, actually easy, then maybe building them is a very bad idea.

38:28.320 --> 38:29.360
And we should not do that.

38:29.360 --> 38:34.560
So is it because that such a super intelligence will be running over a long period of time,

38:34.560 --> 38:39.840
increasing the cumulative risk of failure over say decades or centuries,

38:39.840 --> 38:44.720
that we can't accept even a tiny probability of failure for these systems?

38:44.720 --> 38:45.760
That's one way to see it.

38:45.760 --> 38:50.240
I don't think it will be a very long time given how many opportunities it has to make mistakes.

38:50.240 --> 38:52.160
It will accumulate very quickly.

38:52.160 --> 38:56.160
So at human scales, you have 20 years per generation or something.

38:56.160 --> 39:01.760
Here, think of it as like every second, there is a new version of it trying to self-improve,

39:01.760 --> 39:02.880
do more, do better.

39:02.880 --> 39:06.240
So I would suspect it would be a very quick process.

39:06.880 --> 39:12.800
Expecting something to be 100% safe is just unrealistic in any field.

39:12.800 --> 39:18.080
We don't expect bridges to be 100% safe or cars to be 100% safe.

39:18.080 --> 39:21.040
So why is it that that AGI is different here?

39:21.040 --> 39:21.920
That's a great question.

39:21.920 --> 39:27.280
So I cross the street, I'm a pedestrian, I take a certain risk, there is a possibility I will die.

39:28.080 --> 39:33.200
I look at how old am I and based on that, I decide how much risk I can take.

39:33.200 --> 39:35.440
If I'm 99, I don't really care.

39:35.440 --> 39:37.600
If I'm 40, I look around.

39:37.600 --> 39:42.640
If with me, the whole humanity died, 8 billion people depending on me,

39:42.640 --> 39:47.040
safely crossing roads, wouldn't we lock me up and never let me cross any roads?

39:48.720 --> 39:50.400
Yeah, perhaps.

39:51.120 --> 39:56.720
But it seems to me that we cannot live without any risk.

39:58.880 --> 40:06.960
The standard of 100% safe seems just to be unrealistic or there's no

40:07.600 --> 40:10.560
area of life in which we are 100% safe.

40:10.560 --> 40:14.880
In a context of systems which can kill everyone, that is the standard.

40:15.760 --> 40:18.800
You can like it or not like it, but that's just the reality of it.

40:19.520 --> 40:21.680
We don't have to have super intelligent AI.

40:21.680 --> 40:23.680
It's not a requirement of happy existence.

40:23.680 --> 40:29.280
We can do all the things we want, including life extension with much less intelligent systems.

40:29.280 --> 40:33.840
Protein folding problem was solved with a very narrow system, very capable.

40:33.840 --> 40:36.880
Likewise, all the other problems could be solved like that.

40:36.880 --> 40:40.240
There is no need to create a system we cannot control,

40:40.240 --> 40:43.040
which very likely over time to kill everyone.

40:43.040 --> 40:45.440
So who has the burden of proof here?

40:45.440 --> 40:50.320
Your impossibility results and you have I think five, six, seven of them.

40:50.320 --> 40:52.240
You've sent me your papers on it.

40:52.240 --> 40:57.520
Do they mean that we will not reach a proof that some AI system is safe?

40:57.520 --> 40:59.600
Again, a mathematical proof.

40:59.600 --> 41:03.520
And which side of this debate has the burden of proof to say,

41:04.560 --> 41:10.560
should the people advocating for deployment of a system have some sort of mathematical

41:10.560 --> 41:14.080
proof that this system is provably safe?

41:14.640 --> 41:17.120
So there are two different questions here, I think.

41:17.120 --> 41:20.640
One is what about product and services liability?

41:20.640 --> 41:26.160
You have to show that your product or service is safe as a manufacturer, as a drug developer.

41:26.160 --> 41:30.800
You cannot just release it and expect the users to show that it's dangerous.

41:30.800 --> 41:33.120
We're pretty confident this is the approach.

41:33.120 --> 41:37.040
If you're making cars, your cars have to meet certain standards of safety.

41:37.840 --> 41:43.360
It's not 100% obviously, but for the domain, they're pretty reasonable standards.

41:43.920 --> 41:50.240
With impossibility results, all I'm saying is that there are limits to what you can understand,

41:50.240 --> 41:56.560
predict and do, and you have to operate within where those limits don't kill everyone.

41:56.560 --> 42:01.600
So if you have a system like GPT-4 and it makes mistakes, somebody commits suicide,

42:01.600 --> 42:08.080
somebody's depressed, those of course will pay for trillion dollars in economic growth benefit,

42:08.080 --> 42:09.920
and we can decide if it's worth it or not.

42:10.560 --> 42:16.240
If we go to a system which very likely kills everyone, then the standard is different.

42:16.240 --> 42:20.240
The burden of proof, of course, within possibility results is on me.

42:20.240 --> 42:25.680
I published this paper saying you can never fully predict every action of a smarter than

42:25.680 --> 42:30.960
new system. The beautiful thing about impossibility results is that they are kind of self-referential.

42:30.960 --> 42:36.480
I have a paper about limits of proofs. Every proof is only valid with respect to a specific

42:36.480 --> 42:42.000
verifier. The peer reviewers who looked at my paper have a verifier. If those three people

42:42.000 --> 42:47.440
made a mistake, the proof is invalid possibly. We can scale it to mathematical community to

42:47.440 --> 42:53.120
everyone. We can get it very likely to be true if we put more resources in it, but we'll never get

42:53.120 --> 42:59.680
to 100%. It could be good enough for that purpose. But that's the standard. If somebody finds a flow

42:59.680 --> 43:06.880
and publishes a paper saying again, I had people say that AI alignment is easy. I heard people say

43:06.880 --> 43:10.640
that it's definitely solvable. That's wonderful. Now publish your results.

43:11.200 --> 43:18.240
We are living in a world where we have existential risks. Nuclear weapons, for example, constitute

43:18.240 --> 43:24.800
an existential risk. Perhaps engineered pandemics could also wipe out humanity. We're living in a

43:24.800 --> 43:30.720
world in which we are accepting a certain level of human extinction every day. Why, in a sense,

43:30.720 --> 43:38.240
shouldn't we accept some level of existential risk from AI systems? We do prefer to live in a world

43:38.240 --> 43:44.160
with no engineered pandemics and no nuclear weapons. We're just working slowly towards that goal.

43:44.160 --> 43:49.200
There are also not agents. The nuclear weapons are tools. It's more about controlling certain

43:49.200 --> 43:55.360
leaders, not the weapon itself. On top of it, while a nuclear war with superpowers would be a very

43:55.360 --> 44:02.080
unpleasant event, it's unlikely to kill 100% of humans. If 1% of humans survives, it's a very

44:02.080 --> 44:09.280
different problem than 100% of humans go extinct. There are nuanced differences. We still don't want

44:09.280 --> 44:14.400
any of the other problems, but it doesn't mean that just because we have all these other problems,

44:14.400 --> 44:18.800
this problem is not a real problem. I'm not saying it's not a real problem, but I'm saying

44:18.800 --> 44:25.280
that we cannot go through life without accepting a certain level of risk. It seems to me like an

44:25.280 --> 44:32.000
unrealistic expectation that we cannot deploy systems even if they have some level, some

44:32.000 --> 44:37.040
above zero level of risk. This is exactly the discussion I would love to have with humanity

44:37.040 --> 44:44.480
as a whole. What amount of risk are you willing to take for everyone being killed? How much benefit

44:44.480 --> 44:50.320
you need to get? Let's say in dollars get paid to take this risk, that 1% chance of everyone being

44:50.320 --> 44:55.760
killed over the next year. Let's say it's 1% for a year after. That's a great question. A lot of

44:55.760 --> 45:00.880
people would say, I don't want your money. Thank you. We'll continue. Again, we don't have to make

45:00.880 --> 45:06.720
this decision. We don't have to build superintelligent, godlike machines. We can be very happy with very

45:06.720 --> 45:13.440
helpful tools if we agree that this is the level of technology we want. Now, I'm not saying that

45:13.520 --> 45:17.920
the problem of getting everyone to agree is a solvable problem. That's actually not an impossibility

45:17.920 --> 45:24.400
result. You cannot stop the progress of technology in this environment with financial incentive,

45:24.400 --> 45:30.320
capitalist structure, and so on. The other alternative, the dictatorship model of communist

45:30.320 --> 45:36.560
states has its own problems, which may be worse in a short term, unknown in the long term. We never

45:36.560 --> 45:45.120
had communism with superintelligence. Let's not find out. The point is, it seems like we can get

45:45.120 --> 45:51.600
almost everything we want without risking everything we have. Do you view the question you just posed

45:51.600 --> 46:01.680
as absurd or immoral, this question of how much in terms of dollars would you have to get in order

46:01.680 --> 46:08.000
to accept, say, a 1% risk of extinction per year, which is extremely high? Do you think this is

46:08.000 --> 46:13.120
something we should actually ask ourselves as a species, or is this something we should avoid and

46:13.120 --> 46:17.840
simply say, perhaps it's not a good idea to build these systems? Well, I don't think there are any

46:17.840 --> 46:23.120
moral questions. As an academic, as a scientist, it's your job to ask hard questions and think

46:23.120 --> 46:27.840
about them. You can come to the conclusion that it's a really bad idea, but you should be allowed

46:27.840 --> 46:34.320
to think about it, consider it. Now, 1% is insanely high for something so valuable. If it was

46:34.320 --> 46:41.200
one chance and trillion, trillion, trillion once, and then we all get three universes for everyone,

46:41.840 --> 46:46.240
that may be a different story. We can do that calculation. And again, some people would still

46:46.240 --> 46:53.600
choose not to participate. But typically, we expect everyone on whom scientific experiments

46:53.760 --> 46:59.440
are performed who will be impacted to consent to an experiment. What is required for this consent?

46:59.440 --> 47:03.920
They need to understand the outcome. Nobody understands these models. Nobody knows what

47:03.920 --> 47:07.840
the result of the experiment would be. So really, no one can meaningfully consent,

47:07.840 --> 47:12.240
even if you're saying, oh, yeah, press the button. I want the super intelligence deployed.

47:12.240 --> 47:17.040
You're really kind of gambling. You have no idea what you're agreeing to. So by definition, we cannot

47:17.040 --> 47:24.080
even have the situation where we agree on it unless we can explain and predict outcomes,

47:24.080 --> 47:28.880
which may be an impossibility. So there are perhaps two features of the world which

47:28.880 --> 47:33.520
could push us to accept a higher level of risk when we're deciding whether to

47:34.320 --> 47:38.960
deploy these systems. One is just all of the horrible things that are going on right now,

47:38.960 --> 47:46.480
so poverty and disease and aging and so on, which an AGI system might be able to help with.

47:46.480 --> 47:53.200
And the other is the running level of existential risks from other factors. So I mentioned nuclear

47:53.200 --> 47:59.680
and engineered pandemics. Do you find that this pushes you in the direction of saying we should

47:59.680 --> 48:04.640
accept a higher level of risk when we're thinking about whether to deploy AGI?

48:04.640 --> 48:09.920
Not the specific examples you provided, but if there was an asteroid coming and we could not

48:09.920 --> 48:14.480
stop it by any other way, so meaning like we're all going to die in 10 years unless the press

48:14.480 --> 48:19.920
this button, then maybe it would make sense in nine and a half years to press this button.

48:19.920 --> 48:23.440
When we have nothing left to lose, it becomes a very profitable bet.

48:23.440 --> 48:28.560
It's an interesting fact of the world that we haven't thought hard about these questions. What

48:28.560 --> 48:34.320
level of risk are we willing to accept for the introduction of new technologies that could be

48:34.320 --> 48:42.720
potentially very valuable? Is this a deficit on humanity's part? Should we have done this research

48:42.720 --> 48:47.680
or how do you think about us not having thought through this problem?

48:47.680 --> 48:52.240
We should definitely. It's interesting. We don't even do it at level of individual humans. Most

48:52.240 --> 48:57.920
people don't spend a lot of time deciding between possible outcomes and decisions they make,

48:57.920 --> 49:02.480
even then they are still young. And like the career choice would make a lot of difference.

49:02.480 --> 49:06.640
Who you marry makes a lot of difference. It's always like, well, I met someone at the party,

49:06.640 --> 49:11.760
let's just live together and see what happens. So we're not very good at long-term planning.

49:11.760 --> 49:16.240
Is it a question of we're not good at long-term planning or is it a question of whether we are

49:16.240 --> 49:21.840
not or perhaps we're not good at thinking in probabilities or thinking clearly about

49:21.840 --> 49:25.360
small probabilities of large risks or large dangers?

49:25.360 --> 49:30.720
All of those. There is a lot of cognitive biases and all of them kind of show up in those

49:31.680 --> 49:38.000
examples from the paper of denying different existential problems with AI safety.

49:38.000 --> 49:44.000
We also have this bias of denying negative outcomes. So we all are getting older

49:44.640 --> 49:51.200
at like 60 minutes per hour essentially. And you would think we all be screaming at the government

49:51.200 --> 49:56.960
to allocate all the funds they have for life extension research to fix this truly existential

49:56.960 --> 50:03.520
crisis where everyone dies 100%. But nobody does anything except a few individuals lately.

50:03.520 --> 50:09.200
So it seems to be a standard pattern for us to know that we all are in deep trouble

50:09.200 --> 50:13.200
and not do anything until you are much older and frequently not even then.

50:13.200 --> 50:21.440
If we go back to your paper, you mentioned an objection about superintelligence being benevolent.

50:21.440 --> 50:26.480
So I'm guessing that the reasoning here is something like with increased intelligence

50:26.480 --> 50:31.040
follows increased benevolence. Why don't you believe that?

50:31.040 --> 50:36.320
Well, smart people always nice. We never had examples of smarter people doing horrible things

50:36.320 --> 50:42.000
to average. So that must be a law of nature, right? Basically, orthogonality thesis. You can

50:42.000 --> 50:47.360
combine any set of goals with any level of intelligence except through extremes at the bottom.

50:48.560 --> 50:56.240
We cannot guarantee that and also what the system will consider to be benevolent if it is a nice

50:56.240 --> 51:02.080
system may not be something we agree with. So it can tell you, you'd be better off doing this

51:02.080 --> 51:07.200
with your life and you're like, I'm not really at all interested with any of that, but it's better

51:07.200 --> 51:12.720
for you. So why don't you do it anyways? So you're imagining a potentially paternalistic

51:12.720 --> 51:18.240
ADI telling you that you should eat more vegetables, you should spend more time working out and

51:18.240 --> 51:24.000
remember to sign yourself up for life insurance and so on. That one I would actually like. I'm

51:24.000 --> 51:29.520
thinking more about AI, which says, okay, existence is suffering. So you better off not having children

51:29.520 --> 51:34.480
and dying out as quickly as possible to end all suffering in the universe. Okay, yeah, that one

51:34.480 --> 51:41.120
I would. I like the coach one. That's a nice one. There is an emerging movement called effective

51:41.120 --> 51:47.440
accelerationism, which argues that we should accelerate the development of AGI and there's

51:47.440 --> 51:55.520
some reasoning about whether we should perhaps see AGI as a natural successor to humanity and

51:55.520 --> 52:01.520
we should let evolution take its course in a sense and then hand over the torch of the future

52:01.520 --> 52:09.280
to AGI. You mentioned this also in your paper, you write we should let the smarter beings win.

52:09.280 --> 52:15.040
What do you think of this position? Well, it's kind of the extreme version of

52:15.040 --> 52:21.760
devising algorithms. You can be racist, you can be sexist, you can be pro-human. This is a final

52:21.760 --> 52:26.480
stage where we have no bias. It's a cosmic point of view. If they are smarter than us,

52:26.480 --> 52:32.000
they deserve all the resources. Let's move on. And I am biased, I'll be honest. I'm very pro-human

52:32.000 --> 52:37.440
and I want to die. So it seems like it's a bad thing. If I'm dead, I don't really care if the

52:37.440 --> 52:42.800
universe is full of very smart robots. It doesn't somehow make me happier. People can disagree about

52:42.880 --> 52:49.680
it. There are cosmos who have this point of view and they see humans maybe as kind of unnecessary

52:49.680 --> 52:56.240
down, we're on the planet. So maybe it's some cosmic justice. But again, get 8 billion of us

52:56.240 --> 53:01.600
to agree to this experiment. Do you think that perhaps this is connected to thinking about,

53:01.600 --> 53:09.760
again, AI consciousness? I think that if we just were handed a piece of infallible knowledge

53:09.760 --> 53:16.480
stating that future AIs will be conscious, then perhaps there could be something to the

53:16.480 --> 53:23.520
argument for handing over the control of the future to AIs. But are you skeptical that AIs

53:23.520 --> 53:28.320
will be conscious and therefore skeptical that they matter morally speaking? I think they could

53:28.320 --> 53:34.720
very well be super conscious and consider us not conscious. We treat bacteria as very primitive

53:34.720 --> 53:40.640
and not interesting, but it doesn't do anything for me. If I'm dead, what do I care? Why is it

53:40.640 --> 53:46.880
relevant to us? What happens billions of years later? You can have some scientific interest

53:46.880 --> 53:51.360
in learning about it, but it really would not make any difference, whatever that entity was

53:51.360 --> 53:57.760
conscious or not, while terraforming Mars. You think perhaps this objection is too smart for

53:57.760 --> 54:03.040
its own sake that we should hand over control to the AIs because they are smarter than us.

54:04.000 --> 54:09.360
And you want to insist on a pro-human bias, if we can call it that?

54:09.360 --> 54:14.080
I would like to insist on that. The joke I always make about it is, yeah, I can find another guy

54:14.080 --> 54:18.800
who's taller than me and better than me and get him to be with my wife, but somehow it doesn't

54:18.800 --> 54:25.840
seem like an improvement for the system. Okay. What about perhaps related to what we were just

54:25.840 --> 54:32.480
talking about? Humans can do a lot of bad things. We are not perfectly ethical. And so,

54:32.480 --> 54:36.960
one objection is that they would be able to be more ethical than we are simply put.

54:36.960 --> 54:43.040
Do you think that's a possibility and would that make you favor handing over control to AI systems?

54:43.040 --> 54:47.680
Is this after they kill all of us before they become more ethical? I'm just struggling with

54:47.680 --> 54:53.680
that definition. So, ethics is very relative, right? We don't think there is absolute universal

54:53.680 --> 54:58.800
ethics. You can argue that maybe suffering reduction is some sort of fundamental property,

54:58.800 --> 55:06.320
but then not having living conscious organisms is a solution, really. So, I doubt you can

55:06.320 --> 55:13.360
objectively say that they would be, in a sense, we would perceive it as, and if they choose to

55:13.360 --> 55:18.400
destroy us to improve average ethics of the universe, that also seems like a bad decision.

55:18.400 --> 55:24.400
So, it's been a while since you wrote this paper. You mentioned it's three years old,

55:24.400 --> 55:31.840
and three years in AI is potentially centuries. So, have you come across any new

55:31.840 --> 55:36.720
objections that you find interesting? There is actually an infinite supply. People will use

55:36.720 --> 55:42.320
anything as an argument. We have a new paper published with a colleague, which is bigger and

55:42.320 --> 55:49.360
maybe better, listing a lot of, really, we try to be comprehensive as much as we could. Problem is,

55:49.360 --> 55:56.240
a lot of those objections have similar modules in common. Okay, anything with time. You have

55:56.240 --> 56:01.200
all this variance in it. Anything with personal preferences. So, yeah, we have a new paper. It's

56:01.200 --> 56:07.280
already on Archive, I believe. Definitely encourage you to read it. It's like a short 60-page

56:07.280 --> 56:14.800
font read. Definitely read it. I would expect that to be a standard reference for when you have

56:14.800 --> 56:20.640
your Twitter wars. Oh, what about this? You just send people there, and if somebody wants to maybe

56:20.640 --> 56:26.240
use a large language model to write detailed response for each one and make a 6,000-page

56:26.240 --> 56:31.520
book out of it, we would strongly encourage that. But it seems like there is always going to be

56:31.520 --> 56:39.520
additional set of objections for why something is not a problem. And I think whoever manufactures

56:40.400 --> 56:46.880
that service, that product with AI, needs to explain to us why there is an acceptable degree

56:46.880 --> 56:52.720
of danger given the benefits. We could talk about who in general has the burden of proof here,

56:52.720 --> 57:00.160
whether people advocating for AI safety or people advocating, arguing that AI safety is perhaps not

57:00.160 --> 57:05.840
something we should be concerned about. We have talked about it as if we start with the assumption

57:05.840 --> 57:11.040
that AI safety is an important concern. But of course, if you're coming to this from the other

57:11.040 --> 57:16.480
perspective, you would perhaps expect there to be some arguments that we should take AI safety

57:16.480 --> 57:23.520
seriously. So what is your favorite approach to starting with the burden of proof yourself?

57:23.520 --> 57:30.160
Well, it's a fundamental part of making working AI. I think Stuart Russell talks about definition of

57:30.160 --> 57:36.800
bridges as something which doesn't fall down being an essential part of bridge-ness. I think it's

57:36.800 --> 57:42.720
the same for AI systems. If you design an AI system to help me spellcheck my essay and instead it

57:42.720 --> 57:48.240
kills me, I don't think you have a successful spellchecker AI. It's just a fundamental property

57:48.240 --> 57:55.200
of those systems. Then you had very incapable AI, very narrow systems capable of barely doing one

57:55.200 --> 58:00.560
thing. Doing a second thing would be like an incredible generality of that system. So unsafe

58:00.560 --> 58:07.920
behaviors were not a possibility. If you have this proto-AGI systems with unknown capabilities,

58:07.920 --> 58:12.880
some of them could be very dangerous, and you don't know by definition. So it seems like it's

58:12.880 --> 58:18.880
common sense to take this very seriously. There are certain positions I can never fully

58:19.520 --> 58:25.680
still meant to truly defend because I just don't understand how they can be argued for. So one was

58:25.680 --> 58:31.440
we will never have human-level intelligence, not 10 years, not one in never, unless you some sort of

58:32.960 --> 58:39.120
theological, soul-based expert. It's very hard to argue that never is the answer here.

58:39.120 --> 58:47.280
And another one is that there is definitely no safety issue. You can argue that we will overcome

58:47.280 --> 58:53.520
certain specific types of a problem. So maybe we'll solve copyright issue and AI art. I'll give

58:53.520 --> 58:59.520
you that. Definitely, we can probably do that. But to say that for all possible future situations,

58:59.520 --> 59:06.000
for all possible future AI models, we definitely checked and it creates no existential risks

59:06.640 --> 59:10.880
beyond safety margins we're happy with is a pretty strong statement.

59:10.880 --> 59:16.480
Yeah. Perhaps returning to the 60-page paper you mentioned, what are some of your favorite

59:16.480 --> 59:22.960
objections from that paper? My goal was to figure out why people make this mistake and we kind of

59:22.960 --> 59:27.760
give obvious solutions. Maybe there is some sort of bias we're getting paid to think differently.

59:27.760 --> 59:33.760
But really, you can map a lot of them on the standard list of cognitive biases in Wikipedia.

59:33.760 --> 59:38.720
You just go, okay, this is a cognitive bias. I can predict this is the argument we're going to get.

59:38.720 --> 59:43.520
And it would take a lot of work to do it manually for all of them. But I think that's a general

59:43.520 --> 59:51.440
gist. We have this set of bugs in our head and every one of those bugs triggers a reason for why

59:51.440 --> 59:57.760
we don't process this fully. But of course, we could probably also find some biases that people

59:57.760 --> 01:00:04.960
who are concerned with AI safety display. So perhaps we could, I don't know if this is a named

01:00:04.960 --> 01:00:11.360
bias, but there are many biases and we can probably talk about humanity having a bias in favor of

01:00:11.360 --> 01:00:17.200
apocalypse. So humanity has made up apocalypse scenarios throughout its entire existence.

01:00:17.200 --> 01:00:21.760
You could make some form of argument that there's a reference class and that reference class is

01:00:22.480 --> 01:00:27.920
apocalypse is coming. This is something that humanity has been talking about for thousands of

01:00:27.920 --> 01:00:33.840
years. And then if we say, well, it has never actually happened. And so therefore, we shouldn't

01:00:33.840 --> 01:00:39.520
expect it to happen with AI. What do you say to that? So there is definitely a lot of historical

01:00:39.520 --> 01:00:44.720
examples of people saying we got 20 years left and it was not the case. Otherwise, we wouldn't be

01:00:44.720 --> 01:00:50.480
here to have this conversation. So it's a bit of a selection bias. There's sort of a worship bias.

01:00:50.480 --> 01:00:57.600
It feels like a lot of different charts and patterns all kind of point at that 2045 official

01:00:57.600 --> 01:01:03.280
below date as a lot of interesting things will happen in synthetic biology and genetic engineering

01:01:03.280 --> 01:01:09.600
and nanotech and AI, all this technology is quantum computing. It would be weird if every single one

01:01:09.600 --> 01:01:16.320
of those deployments had absolutely no possibility of being really bad. Just statistically, it would

01:01:16.320 --> 01:01:21.760
be like, wow, that is definitely a simulation we're living in and they programmed a happy ending.

01:01:21.760 --> 01:01:27.840
So now we're talking about extrapolating trends and there perhaps the problem is distinguishing

01:01:27.840 --> 01:01:32.960
between an exponential trend or an exponential increase in capability of some system.

01:01:33.040 --> 01:01:38.720
And then more of an S curve that bends off and you begin getting diminishing returns.

01:01:39.520 --> 01:01:42.960
How do you approach distinguishing between those two things?

01:01:42.960 --> 01:01:48.880
So you can't at the moment, you have to look back and see what happened later. So far, just

01:01:48.880 --> 01:01:55.920
looking at change from 3 to 4.0 for GPT in terms of let's say passing GRE exams and how well it

01:01:55.920 --> 01:02:02.640
does, it feels exponential or hyper exponential. If you take that system and give it additional

01:02:02.640 --> 01:02:07.360
capabilities, which we probably know how to do already, we just haven't had time such as

01:02:07.360 --> 01:02:13.760
good reliable memory, ability to kind of go in loops and reconsider possibilities, it would

01:02:13.760 --> 01:02:20.080
probably do even better with those. If we haven't seen diminishing returns so far in scalability

01:02:20.080 --> 01:02:26.960
loss in any true sense, so let's assume GPT-5 is an equally capable projection forward,

01:02:26.960 --> 01:02:32.400
we would already be above human performance level for most humans in most domains.

01:02:32.400 --> 01:02:37.440
So you can argue, well, human comedians are still a lot funnier and I think it's true.

01:02:37.440 --> 01:02:43.040
It might be the last job we'll have, but in everything else, it will be better than

01:02:43.040 --> 01:02:47.440
an average human and that's a point which we always consider though it will press the

01:02:47.440 --> 01:02:55.040
Turing test or it will take over most jobs. So definitely it seems like we are still doing,

01:02:55.040 --> 01:03:00.880
I would say, hyper exponential progress and capabilities and linear or even constant

01:03:00.880 --> 01:03:06.720
progress and safety. I can generally name equally amazing safety breakthroughs as capability

01:03:06.720 --> 01:03:12.800
breakthroughs and there is this unknown unknown capabilities pool which we haven't discovered

01:03:12.800 --> 01:03:18.000
already with modern models. There is not an equivalent overhang of safety papers we haven't

01:03:18.000 --> 01:03:24.320
found in archive. Yeah, so there are probably hidden capabilities in the GPT-4 based model,

01:03:24.320 --> 01:03:30.400
but there are probably not hidden safety features there. Exactly. You've been in the business of

01:03:30.400 --> 01:03:35.920
AI safety for a long time. When did you get started? When did you get interested in AI safety?

01:03:35.920 --> 01:03:43.280
So it depends on how you classify my early research. I was working on security for online

01:03:43.840 --> 01:03:49.520
gaming systems, online poker against bots trying to steal resources. So it's a very

01:03:49.520 --> 01:03:56.000
proto AI safety problem. How do we detect bots, classify them, see if it's the same bot and

01:03:56.000 --> 01:03:59.680
prevent them from participating? So that was my PG in 2008.

01:03:59.680 --> 01:04:05.200
How have things developed in ways that you didn't expect and perhaps in ways that you did expect?

01:04:05.200 --> 01:04:11.840
I expected academia to be a lot quicker to pick up this problem. It took embarrassingly long time

01:04:11.840 --> 01:04:21.280
for it to be noticed. It was done by famous people and less wrong in that alternative research

01:04:21.280 --> 01:04:28.560
universe, which may in some way be good, but in other ways it made it different from standard

01:04:28.640 --> 01:04:35.920
academic process. And so it's harder to find top journal of AI safety. So I can read the latest

01:04:35.920 --> 01:04:41.440
papers. You have to be an expert in 100 different blogs and keep up with specific individuals with

01:04:41.440 --> 01:04:47.520
anonymous handles on Twitter. So that's somewhat unusual for an academic discipline. I also did

01:04:47.520 --> 01:04:55.120
not correctly predict that language models will do so well so quickly. I felt I have another 20

01:04:55.120 --> 01:05:01.840
years to slowly publish all the proper impossibility results and calls for bans and moratoriums. I was

01:05:01.840 --> 01:05:08.560
pleasantly, unpleasantly surprised in capabilities. But other than that, everything seems to be

01:05:09.280 --> 01:05:15.760
as expected. I mean, if you read Kurzweil, he accurately predicted 2023 as capability to model

01:05:15.760 --> 01:05:23.200
one human brain. I think it's not insane to say we're very close to that. And he thinks 2045

01:05:23.200 --> 01:05:29.280
was an upper limit for all of our brains being equivalently simulated. And that's the singularity

01:05:29.280 --> 01:05:35.200
point. How do you think about Ray Kurzweil? Ray Kurzweil is often written off as a bit of a

01:05:36.400 --> 01:05:45.520
being too perhaps optimistic about his own predictions and not being super careful in what

01:05:45.520 --> 01:05:50.960
he's saying perhaps in some of his earlier work. But I think if you go back and find some of his

01:05:50.960 --> 01:06:01.920
work from the 90s and think of all of the futurist writers of this period who had a good sense of

01:06:01.920 --> 01:06:08.240
where we're going. And Kurzweil might be one of the people with a pretty good sense of where we're

01:06:08.240 --> 01:06:15.680
going if things will develop as you perhaps expect them to go. So if perhaps we will get to AGI before

01:06:15.680 --> 01:06:22.640
2050 and so on. No, I'm very impressed with his predictions. People correctly noticed that if you

01:06:22.640 --> 01:06:29.520
take his language literally, it may not fit. So the example I would use, when we start having

01:06:29.520 --> 01:06:36.160
video phone calls when iPhone came out, but really AT&T was selling it in the 70s. It cost a lot and

01:06:36.160 --> 01:06:43.920
only a few rich people had it, but it existed. So is it 2000 or is it 1970? Flying cars? Do we

01:06:43.920 --> 01:06:49.680
have them or not? I can buy one, but they are not there. Self-driving cars. I can drive one in one,

01:06:49.680 --> 01:06:56.400
but so it depends on how in an important way he made accurate predictions about capabilities.

01:06:56.400 --> 01:07:03.040
In how it was adapted or commercialized, that's up to human consumer, user taste and cost. So

01:07:03.040 --> 01:07:09.200
that's a very different type of question. Where should we go from here, Roman? We've talked about

01:07:09.200 --> 01:07:15.840
all of the ways that arguments against AI safety fall apart and we've talked about perhaps

01:07:15.840 --> 01:07:20.560
how difficult of a problem this is. Where should we as a species go from here?

01:07:21.600 --> 01:07:28.480
I think we need to dedicate a little more human power to asking this question. What is

01:07:28.480 --> 01:07:34.880
possible in this space? Can we actually do this right? I signed the letter asking for six more

01:07:34.880 --> 01:07:41.440
months. I don't think six months will buy us anything. We need a request based on capabilities.

01:07:42.000 --> 01:07:49.040
Please don't create the next more capable system until the following safety requirements are met.

01:07:49.040 --> 01:07:56.800
And one is you understand what the capabilities of your system are or will be and some external

01:07:56.800 --> 01:08:02.560
reviewers agree with that assessment. So that would be quite reasonable. That's a very high

01:08:02.560 --> 01:08:08.640
standard for deploying AI systems. It would basically mean that all of the systems that are

01:08:08.640 --> 01:08:14.880
based on deep learning won't be able to be deployed because we don't understand what's going on inside

01:08:14.880 --> 01:08:21.120
of these models. But is it because we were trained to have low standards? You're saying it's insane

01:08:21.120 --> 01:08:27.200
to request that the engineer understands what he made. They are randomly drawing those things and

01:08:27.200 --> 01:08:32.800
deploying it and seeing what happens next. I was just at the conference I mentioned and in one of

01:08:32.800 --> 01:08:37.440
the conversations it was interesting. We were talking about difference between short-term risks

01:08:37.440 --> 01:08:44.640
and long-term risks. And now it's all three years, no longer applies. And it occurred to me that things

01:08:44.640 --> 01:08:50.000
might actually flip. It may take five years to destroy democracy properly, but only two years

01:08:50.000 --> 01:08:55.840
to destroy humanity. So the long-term risks may become short-term and vice versa. And this is

01:08:55.840 --> 01:09:03.120
not normal. We should not accept this. Otherwise, we cannot monetize those systems. But if we return

01:09:03.120 --> 01:09:08.240
to the question of where we could go from here, do you see any plausible paths for improving our

01:09:08.240 --> 01:09:13.520
situation? In terms of understanding the problem, I would ask other people, we have a survey coming

01:09:13.520 --> 01:09:19.440
out with about, I don't know, 30, 50 different results like this. If more people could look at it

01:09:19.440 --> 01:09:24.800
and see, okay, so maybe this tool is not necessary, but those are likely. Can we have approximate

01:09:24.880 --> 01:09:30.400
solutions? So it's definitely useful to be able to monitor AI and understand more. But

01:09:30.960 --> 01:09:36.880
how much can we expect from their systems and how quickly? If we are exponentially growing,

01:09:36.880 --> 01:09:42.240
and right now we understand a dozen neurons, the next year is 24, we will not catch up to

01:09:42.240 --> 01:09:46.640
exponential growth. So maybe that's not the approach to try. I would definitely look at

01:09:46.640 --> 01:09:52.640
what is possible in general. If someone wants to actually write a good, not a mathematical proof,

01:09:52.640 --> 01:09:58.160
but at least a rigorous argument for why we definitely can control superintelligent machines

01:09:58.160 --> 01:10:02.960
and definitely with very low risk, I would love to read that paper. That would be good to

01:10:03.520 --> 01:10:11.520
inspire others. If monitorability is impossible, that impacts how we ask for governance regulations.

01:10:11.520 --> 01:10:18.800
So if international community or specific government says, those are the things we expect

01:10:18.880 --> 01:10:24.480
you to do, but we cannot monitor them, that's not a very meaningful set of regulations. So that's

01:10:24.480 --> 01:10:32.000
important in that regard. In general, I think all those things, governance, technical work will

01:10:32.000 --> 01:10:38.240
not produce the results we expect. It has to be self-interest. This 30-year-old, 40-year-old,

01:10:38.240 --> 01:10:45.360
super-rich, young, healthy person running a large AI lab needs to ask, will this benefit me or

01:10:45.360 --> 01:10:50.720
destroy everything I have? Everything I have built, will it be the worst outcome? And what's

01:10:50.720 --> 01:10:55.360
interesting, historically, if you were like a really bad guy in history, you were remembered in

01:10:55.360 --> 01:11:01.120
history. In this case, you won't even be remembered. There won't be humans to remember you. So it's a

01:11:01.120 --> 01:11:08.800
pure loss. So if you care about your self-interest, you should pause. You should wait. How optimistic

01:11:08.800 --> 01:11:17.520
are you that perhaps we can get lucky and perhaps what current labs are doing, what DeepMind and

01:11:17.520 --> 01:11:24.800
OpenAI in particular is doing right now, will somehow work out that training language models

01:11:24.800 --> 01:11:29.680
and then doing fine-tuning and doing some form of feedback from human preferences,

01:11:29.680 --> 01:11:36.560
perhaps further development on that paradigm, how confident or yeah, how optimistic are you

01:11:36.560 --> 01:11:43.600
about that paradigm? I'm not optimistic. They have known bugs. They're jailbroken all the time.

01:11:44.240 --> 01:11:52.320
They report improvement in percentages. So now 83% of capabilities are limited and filtered. But as

01:11:52.320 --> 01:11:57.600
a total set of capabilities in a space of possible capabilities, there is now more capabilities we

01:11:57.600 --> 01:12:01.760
don't know about and cannot control. So it's getting worse with every generation. It's getting more

01:12:01.760 --> 01:12:07.600
capable and less controlled. You're saying that even though the percent of capabilities that are

01:12:07.600 --> 01:12:14.240
properly evaluated increases with each model, that's not the right metric for safety?

01:12:14.240 --> 01:12:20.640
All right. The actual numbers for AI accidents, I would call them AI failures, is still increasing

01:12:20.640 --> 01:12:25.440
exponentially. There is more problems with the system. If you count them numerically,

01:12:25.440 --> 01:12:30.160
not as a percentage of total capabilities. So how could we settle this agreement between

01:12:30.880 --> 01:12:36.480
people like you and people who perhaps are more optimistic about how AI development will go?

01:12:37.520 --> 01:12:44.240
Do you expect, for example, that there will be smaller accidents involving AI before we see

01:12:44.240 --> 01:12:49.440
large-scale accidents or large-scale basically human extinction?

01:12:49.440 --> 01:12:55.440
Well, I have multiple papers collecting historical AI accidents. I was very interested. I wanted to

01:12:55.600 --> 01:13:01.360
see patterns increase in frequency, increase in damage. We definitely see lots of them. I stopped

01:13:01.360 --> 01:13:07.280
collecting them the moment we released GPT 3.5 because it was too many to collect at this point.

01:13:07.280 --> 01:13:12.880
It's just everything is a report of an accident. I don't think it helps. People go, you see,

01:13:12.880 --> 01:13:17.440
we had this accident and we're still here. No one died. It's like a vaccine against

01:13:18.320 --> 01:13:23.120
caring about existential risk. So it's actually making things worse. The more we survive those

01:13:23.120 --> 01:13:30.240
things, the more we can handle AI accidents. It's not a big deal. I know some people suggested

01:13:30.240 --> 01:13:35.200
maybe somebody should do a purposeful, bad thing, purposeful accident. It will backfire

01:13:35.200 --> 01:13:40.960
terribly. It's going to show that this is crazy. People don't engage with them and B,

01:13:40.960 --> 01:13:44.800
it's going to not actually convince anyone that it's dangerous.

01:13:44.800 --> 01:13:51.760
What did you find in your investigation here? So have AI accidents increased over time and

01:13:51.760 --> 01:13:58.080
perhaps give some examples of these AI accidents? So because the number of devices increased and

01:13:58.080 --> 01:14:02.720
which different smart programs are running, obviously we're going to have more exposure,

01:14:02.720 --> 01:14:08.320
more users, more impact in terms of when it happens, what we see. So that wasn't surprising. We had

01:14:08.320 --> 01:14:13.840
the same exponential curve Kurzweil talks about in terms of benefits. We had it with problems.

01:14:14.480 --> 01:14:20.960
Examples like the earliest examples were false alarms for nuclear response where it was a human

01:14:20.960 --> 01:14:25.760
in a loop who was like, no, no, no, we're not deploying based on this alarm. So that was good.

01:14:25.760 --> 01:14:31.120
They stopped it, but it was already somewhat significant. It could have destroyed half of the

01:14:31.120 --> 01:14:38.640
world. More recent examples, we had Microsoft experiment with Tay Chatbot. They decided that

01:14:38.640 --> 01:14:44.560
letting users train it and provide training data was totally safe. They clearly never had my paper

01:14:44.560 --> 01:14:50.320
on AI accidents. Otherwise they wouldn't Google with their mislabeling of users as gorillas,

01:14:51.040 --> 01:14:56.160
all those things. And you see Google having billions of users. It's quite impactful.

01:14:57.040 --> 01:15:03.920
Those are the typical examples. The pattern was if you design an AI to do X, it will fail to X.

01:15:03.920 --> 01:15:09.680
So no later, that's just what happens. But then the conclusion is if you go general, it can fail

01:15:09.680 --> 01:15:15.600
in all those ways and interactions of those ways. You cannot accurately predict all those

01:15:15.600 --> 01:15:20.480
interactions and ways. You can give examples. If you have a future system capable of X,

01:15:20.480 --> 01:15:25.600
it will fail to X. Whatever X means to you, any capability, immersion capability,

01:15:25.600 --> 01:15:31.360
it will have the type of accident. But if the systems control all the infrastructure,

01:15:31.360 --> 01:15:36.640
power plants, nuclear response, airline industry, you can see that the damage could be

01:15:36.640 --> 01:15:42.480
even more significant proportionally to the control. Yeah, this issue of proportion might be

01:15:42.480 --> 01:15:49.920
interesting. So as a proportion of the total, say AI systems, are AI accidents increasing?

01:15:50.560 --> 01:15:55.280
Or is it simply because we have so much more deployed AI systems in the world that we see

01:15:55.280 --> 01:16:01.360
more examples of accidents? So you have to wait by how severe they are. If you just count, okay,

01:16:01.360 --> 01:16:07.280
AI made a mistake, counts as one, then everyone who's texting and it incorrectly corrected your

01:16:07.280 --> 01:16:11.760
spelling, billion people are right there. It's super common, but nobody died usually.

01:16:12.640 --> 01:16:16.800
Like you send a really wrong message, maybe you want trouble with your girlfriend, but that's

01:16:16.800 --> 01:16:23.200
about it. So the frequency, just frequency of interactions with AI's which ended not as they

01:16:23.200 --> 01:16:28.800
should have definitely increased. Damage in terms of people killed, it depends on are you counting

01:16:28.800 --> 01:16:33.600
cell driving cars, making mistakes, industrial robots, it depends. Because we have more of it,

01:16:33.600 --> 01:16:38.880
it's natural that there is growth, but I don't think there is like this obvious accidents where

01:16:38.880 --> 01:16:44.000
vacuum cleaner takes out 600 people, nothing like that happened. Perhaps we should touch upon the

01:16:44.000 --> 01:16:50.880
question of which group of people should be respected when we're talking about AI safety or

01:16:50.880 --> 01:16:56.640
which group of people should be listened to. One of the objections that you mentioned is that

01:16:57.600 --> 01:17:03.040
perhaps the people who are worried about AI safety are not technical enough or they are not

01:17:04.320 --> 01:17:09.760
engineers, they are not coders themselves. And so therefore, they are not hands-on enough with

01:17:09.760 --> 01:17:15.360
the systems to understand what actually is going on. This is a little bit ironic given that you

01:17:15.360 --> 01:17:20.880
are a professor of computer science, but how do you think about that objection? So this was again,

01:17:21.280 --> 01:17:26.800
years ago when it was mostly people, sometimes with no degrees, sometimes with no publications,

01:17:26.800 --> 01:17:33.200
today we have top-touring prize winners coming out saying, this is it, like totally I'm 100%

01:17:33.200 --> 01:17:39.520
buying in. So very weak objection at this point, it no longer applies. We had 6,000 people or however

01:17:39.520 --> 01:17:48.160
many signed the letter for restricting it. But it's 30,000 people now. 30,000? How many of them

01:17:48.240 --> 01:17:56.960
chatbots? No, no, we do actually clean the list very seriously. Okay, that's good. But it's not

01:17:56.960 --> 01:18:01.440
a democracy just because a lot of people believe something is not enough. And at the same time,

01:18:01.440 --> 01:18:08.720
with all the media attention to GPT-4, now everyone has an opinion on it. And it's one of those

01:18:08.720 --> 01:18:14.640
topics where it's cool to have an opinion. Like most people don't have an opinion on breast cancer.

01:18:14.640 --> 01:18:19.600
They don't understand anything about it, so they don't go on Twitter and like, no, I think this

01:18:19.600 --> 01:18:25.360
paper by the top Nobel Prize winners garbage. But this topic, it's like consciousness,

01:18:25.360 --> 01:18:31.680
simulation, and singularity, superintelligence. That's where like everyone has an opinion. And we see

01:18:32.400 --> 01:18:40.640
housewives, CNN reporters, we see everyone telling us what is the problem, what is not a problem,

01:18:40.640 --> 01:18:46.640
what should be done. And it's good that there is engagement, but most of those opinions are not

01:18:47.840 --> 01:18:55.200
weighted by years of scientific experimentation, reading appropriate papers, and it becomes noise.

01:18:55.200 --> 01:19:01.040
It's very hard to filter what is the meaningful concern, what is not. There is this split between,

01:19:01.040 --> 01:19:08.320
again, AI ethics community and immediate discrimination concerns versus AI not killing

01:19:08.320 --> 01:19:15.520
everyoneism. So it's an interesting time to be alive for this debate on skepticism and denialism.

01:19:15.520 --> 01:19:23.200
Even that term, AI risk denialism is still kind of not obviously accepted as it is with climate

01:19:23.200 --> 01:19:30.400
change. Perhaps the newest form of this objection, which we could call lack of very prestigious

01:19:31.280 --> 01:19:39.280
publications. So we haven't seen papers about AI safety in nature or science yet, for example.

01:19:40.240 --> 01:19:47.120
And so even though we have touring award winners coming out and saying that AI safety is an actual

01:19:47.120 --> 01:19:54.000
and real problem, perhaps people would be more convinced if we had extremely prestigious

01:19:54.000 --> 01:19:58.160
publications and highly cited publications and so on.

01:19:58.160 --> 01:20:03.120
Perhaps a few problems. One, we don't have an AI safety dedicated journal,

01:20:03.120 --> 01:20:08.240
which is kind of weird. I tried a few times suggesting it may be a good thing. I was told

01:20:08.240 --> 01:20:12.800
no, it's a very bad thing. We don't have good papers to publish on it, so don't. Jumping from

01:20:12.800 --> 01:20:18.880
nothing, black post to nature would be a very big jump to make. We need some other papers.

01:20:18.880 --> 01:20:25.360
In general, after, as you mentioned, I had a few years in this field, it feels like the field is

01:20:25.360 --> 01:20:30.400
all about discovering problems we're going to have, problems we already have, and how

01:20:30.400 --> 01:20:35.920
partial solutions to those problems have fractal nature of additional problems to introduce.

01:20:35.920 --> 01:20:42.640
There is no big pivotal solution papers in this field. That's why I'm from practical point of

01:20:42.640 --> 01:20:49.040
view kind of convincing myself that my theoretical papers may be right. That is, if I was completely

01:20:49.040 --> 01:20:54.960
wrong and it was super easy and solvable, there would be more progress made in important ways.

01:20:55.040 --> 01:21:00.720
Usually, we have this toy problem. We take large language model, we reduce it to two neurons,

01:21:00.720 --> 01:21:06.240
and we understand what the two neurons are doing. Okay, but it doesn't scale. And similar for every

01:21:06.240 --> 01:21:13.120
other shutoff button. Yeah, we can make it where we have the system. If button pressed, shutoff.

01:21:13.120 --> 01:21:18.240
It's working, but the paper says it may not scale to superintelligence. Okay, fair enough.

01:21:18.240 --> 01:21:23.520
And it's the pattern. We have fractal nature of discovering issues we have to resolve,

01:21:23.520 --> 01:21:30.800
and no patches to close them in. Would you like to see more ambitious and larger theories being

01:21:30.800 --> 01:21:36.880
published where the claim is that this is actually a way of aligning superintelligence? I fear perhaps

01:21:36.880 --> 01:21:42.240
that people would be wary of publishing something like this because the next thing that then happens

01:21:42.240 --> 01:21:48.240
is that there's a rebuttal paper and perhaps you then look foolish because you published something

01:21:48.240 --> 01:21:54.960
that another person was able to criticize and find a hole in. I remember maybe even before my

01:21:54.960 --> 01:22:00.000
times, Minsky published a paper showing that there are strong limitations to neural networks.

01:22:00.000 --> 01:22:05.680
Perceptron can never recognize certain shapes. And that killed funding for neural networks for

01:22:05.680 --> 01:22:11.600
like 20 years. Maybe something similar would not be the worst thing if you can show, okay, this is

01:22:11.600 --> 01:22:17.440
definitely not possible. Safety cannot be achieved using transformer architecture. Maybe that would

01:22:17.440 --> 01:22:23.120
be a way to buy some time to develop alternatives approach. I don't know what that could be.

01:22:23.120 --> 01:22:28.560
Evolutionary algorithms don't seem much safer. Uploads don't seem much safer. But

01:22:28.560 --> 01:22:34.000
I would like to have time to look at those. Where would you place AI safety within the

01:22:34.000 --> 01:22:39.760
broader machine learning community? Is it taken more seriously compared to five or 10 years ago?

01:22:39.760 --> 01:22:47.120
And what does the median machine learning researcher think of AI safety?

01:22:47.120 --> 01:22:54.400
So it's definitely taken more serious. Surveys show that there is more than 50% now who say they're

01:22:54.400 --> 01:22:59.440
very concerned or partially concerned. There is degrees of concern about it killing everyone.

01:23:01.920 --> 01:23:06.400
Always questioning the surveys based on how you ask a question. You can get any result you want.

01:23:06.400 --> 01:23:10.160
If they were asking about are you worried? Superintelligent gods will kill everyone.

01:23:10.160 --> 01:23:15.440
You'll get close to zero. If you say, okay, is it likely that there are unknown properties

01:23:15.440 --> 01:23:20.240
which could be dangerous, you'll get close to 100. So it's a manipulation game to get the

01:23:20.240 --> 01:23:27.360
right numbers you want. I'm suspecting. Overall, it seems like in certain places, there is a lot of

01:23:27.360 --> 01:23:34.320
AI safety researchers in the labs, on the ground. In other places, there are zero to none. So it's

01:23:34.320 --> 01:23:43.520
not universal. What we're seeing is that at the top labs and top scholars, there is a good amount

01:23:43.600 --> 01:23:51.600
of growth in terms of acceptance for concerns. But I don't think every single person working

01:23:51.600 --> 01:23:58.160
and developing AI has safety in mind all the time as we should. One thing I've been thinking about,

01:23:58.160 --> 01:24:03.920
perhaps worrying a bit about is whether we will ever be able to know who was right in this debate.

01:24:03.920 --> 01:24:10.240
Say if there's a debate between proponents of AI safety and proponents of advancing AI without

01:24:10.240 --> 01:24:18.080
much regard for AI safety, how could we ever determine who was right there? Because if we

01:24:18.080 --> 01:24:24.000
think about the outcomes, then there's no place where we're standing after the fact and thinking

01:24:24.000 --> 01:24:29.520
about who was right. Absolutely correct. I have a tweet where I say nobody will get to gloat about

01:24:29.520 --> 01:24:35.040
being correct about predicting the end of the world. It's just a definition, not likely. There

01:24:35.040 --> 01:24:39.840
are some people who think we'll live in a simulation and they're running the most interesting 20 years

01:24:40.240 --> 01:24:44.080
and they're going to run it many times to see who's stupid enough to press the button.

01:24:44.080 --> 01:24:50.160
So we'll get to come out and see, now we know, but it seems to be less scientific at this point.

01:24:50.160 --> 01:24:57.520
But perhaps in a sense, if we meet each other again in 2100, then in that situation, would we

01:24:57.520 --> 01:25:02.720
say that AI safety wasn't much of a concern or perhaps just that we got extremely lucky? How

01:25:02.720 --> 01:25:07.200
would you differentiate it retrospectively? Because perhaps we can learn something about the

01:25:07.200 --> 01:25:12.480
nature of the problem by thinking about how we would think about it if we were in the future.

01:25:12.480 --> 01:25:16.560
So you have to look at the actual world. What did they do for this 100 years that they have a

01:25:16.560 --> 01:25:21.600
nuclear war and lost all technology? Is there an AI safety book explaining how to control

01:25:21.600 --> 01:25:26.160
superintelligent machines? Just the fact that we're still around doesn't tell you much. If they are

01:25:26.160 --> 01:25:32.320
still kind of just delaying it by different means, maybe it takes 101 years to get to trouble.

01:25:32.880 --> 01:25:38.560
I never give specific dates for when it's decided or predicted because nobody knows.

01:25:39.280 --> 01:25:45.520
So many factors can intervene. The point is, the systems will continue becoming more capable.

01:25:45.520 --> 01:25:51.200
Even the AGI's will create superintelligence, superintelligence will create superintelligence

01:25:51.200 --> 01:25:57.120
2.0, 3.0. This process will continue. A lot of people think that's what the universe is kind of

01:25:57.120 --> 01:26:05.920
doing about this Amiga point supercreatures. So this will never be a case where you don't have

01:26:05.920 --> 01:26:13.200
safety concerns about a more capable agent replacing you. It seems like we will not be

01:26:13.200 --> 01:26:21.040
meaningfully participating in that debate outside of this first transition. But I think there will

01:26:21.040 --> 01:26:28.320
be a safety problem even if humanity is not around for that AGI or SI trying to

01:26:28.320 --> 01:26:32.800
create the next replacement generation while preserving its values.

01:26:32.800 --> 01:26:39.920
When you think about your worldview on AI in its totality, it's quite a specific

01:26:40.560 --> 01:26:48.960
view you've come to. If you compare it to say the medium person or perhaps even the

01:26:48.960 --> 01:26:56.240
median machine learning researcher, if it turned out that you were completely wrong about where

01:26:56.240 --> 01:27:03.760
this is going, what would be the most likely reason why? So after having those two papers

01:27:03.760 --> 01:27:13.120
on objections to AI risk, reading hundreds, nothing ever clicked outside of standard scientific

01:27:13.120 --> 01:27:18.880
domain. Again, if you are a religious person, you think we have an immortal soul which makes

01:27:18.880 --> 01:27:25.600
a special and no computer can ever get to that level of creativity, that gives you a loophole.

01:27:25.600 --> 01:27:32.720
So with those axioms, those assumptions, you can get away with it. Anything else just doesn't work

01:27:32.720 --> 01:27:38.080
for me. Nothing would make me happier than actually being wrong. That means I get to live,

01:27:38.080 --> 01:27:44.160
I'll get immortality, probably a nice economic benefit. So I hope I'm wrong,

01:27:44.160 --> 01:27:48.240
but I haven't seen anyone produce a good example for why.

01:27:48.880 --> 01:27:57.440
What about the prospect of regulation? So perhaps AI capability, growth and more

01:27:57.440 --> 01:28:05.120
publicity about it will wake up the larger communities in humanity. Perhaps the states

01:28:05.120 --> 01:28:11.200
will become interested in this problem. And we will find a way to regulate AI in which it

01:28:11.200 --> 01:28:17.680
does not pose as much of a danger to us as it might could. So in general, I'm skeptical of

01:28:17.680 --> 01:28:22.560
government regulation, especially when it comes to technology, spam is illegal, computer viruses

01:28:22.560 --> 01:28:29.200
are illegal, it doesn't do much. If I'm right and monitoring AI is not an easy thing you can do or

01:28:29.200 --> 01:28:36.560
explaining it, then it will be just security theater, TSA. You have all this money, you have an

01:28:36.560 --> 01:28:41.040
agency, lots of people walking through your lab looking at monitors, but it doesn't mean anything.

01:28:41.040 --> 01:28:48.320
So I don't think you can solve a technical problem with law. I still strongly encourage trying.

01:28:49.120 --> 01:28:54.160
It's silly enough to where I think if there was a very bad government, like a socialist government,

01:28:54.160 --> 01:28:59.040
and they nationalized it, they would just be so incompetent, they would slow it down enough.

01:28:59.040 --> 01:29:04.160
So in a way, I'm like, hey, all these things I hate, maybe they are a good thing. We should

01:29:04.160 --> 01:29:10.560
try that. But of course, the other side effects would be very negative. Yeah, so between not being

01:29:10.560 --> 01:29:18.640
able to accurately enforce this regulation and on top of it, the cost of making new models coming

01:29:18.640 --> 01:29:24.000
down so much, there are people now running it on standalone laptops with a good processor,

01:29:24.000 --> 01:29:30.480
good video card. You can't regulate that. You can regulate Amazon cloud and VT output. But

01:29:30.480 --> 01:29:35.520
if a teenager can do it in his garage, then the regulation is not very meaningful.

01:29:35.520 --> 01:29:43.840
So the open sourcing of models or the perhaps the leaked weights of a model from meta have become a

01:29:43.840 --> 01:29:51.280
large area of concern, because it seems that we won't be able to control how language models are

01:29:51.280 --> 01:29:58.560
used if they are entirely open source. Is there an upside here where academics will be able to

01:29:58.560 --> 01:30:03.040
study these models because they're open source, and they wouldn't have been able to study the

01:30:03.040 --> 01:30:08.960
models if they had to train the models themselves, because it's so expensive to do. So far, what we

01:30:08.960 --> 01:30:16.960
see is that all research leads to capability, at least as much as to safety, usually more. So yes,

01:30:16.960 --> 01:30:23.360
you learn how to better manipulate errors in that neural network, which means now the system can

01:30:23.360 --> 01:30:29.520
self-improve faster, remove its own errors, and you've made 80% improvement in capabilities,

01:30:29.520 --> 01:30:34.080
and let's say 20% in understanding why you're going to get killed.

01:30:34.080 --> 01:30:39.520
Can we make differential progress? So can we focus entirely on safety, say within an

01:30:39.520 --> 01:30:45.520
academic setting? I don't see that necessarily academic research increases capabilities.

01:30:45.600 --> 01:30:51.280
It is not obvious. So some purely theoretical work, similar to what I'm doing where you just

01:30:51.280 --> 01:30:56.800
hypothetically thinking, okay, can you predict what a superintelligence will do? I don't have

01:30:56.800 --> 01:31:01.280
access to superintelligence system, I cannot test it in practice, but there seem to be

01:31:01.280 --> 01:31:06.480
thought experiments you can run which give you information without any improvement in capability.

01:31:07.680 --> 01:31:12.080
Anything where you're actually working with a model, you can even have accidental discoveries. A lot

01:31:12.080 --> 01:31:17.520
of science says you forgot something overnight, you come back, oh, superintelligence, damn,

01:31:17.520 --> 01:31:22.640
I didn't mean that. It's not obvious. How do you think about interpretability work? So when

01:31:22.640 --> 01:31:27.440
we're talking about mechanistic interpretability, we're talking about the ability to look at the

01:31:27.440 --> 01:31:33.200
weights of a model and find some, interpret this in a way where you're reverse engineering the

01:31:33.200 --> 01:31:39.040
algorithm that led to those weights. Could this turn out to be dangerous because when you're

01:31:39.040 --> 01:31:43.520
learning about a system, perhaps you're learning about its weaknesses and you're there for more

01:31:44.080 --> 01:31:49.440
capable of enhancing the capabilities of the system? I think exactly. That's what I had in mind

01:31:49.440 --> 01:31:55.680
with the previous answer. The more we can help the system understand how it works, the more we

01:31:55.680 --> 01:32:01.920
can help it find problems, the more likely start some sort of self-improvement cycle. Is that an

01:32:02.000 --> 01:32:09.360
argument for keeping discoveries in mechanistic interpretability to basically not publish those

01:32:09.360 --> 01:32:15.360
discoveries? So there is two ways to look at it. On one side, yeah, you want to keep everything

01:32:15.360 --> 01:32:20.960
secret so the bad actors or unqualified actors cannot take advantage of it. On the other hand,

01:32:20.960 --> 01:32:25.840
if you never publish your safety results, media had a policy of not publishing for a while,

01:32:25.840 --> 01:32:30.880
then they started publishing, then they stopped publishing again. Others cannot build on your

01:32:30.880 --> 01:32:35.600
work. So I would be repeating the same experiments we probably did five years ago and discovering

01:32:35.600 --> 01:32:41.360
that that goes nowhere. So again, I have mostly problems and very few solutions for you.

01:32:41.360 --> 01:32:46.160
What about the reinforcement learning from human feedback paradigm? Could that also

01:32:46.160 --> 01:32:51.440
perhaps turn out to increase capabilities? Here I'm thinking simply that when I was playing around

01:32:51.440 --> 01:32:59.760
with the base model in the GPT line of models, it wasn't as useful to me as when it had gone

01:32:59.760 --> 01:33:06.400
through this filter. It made it more easy to have a conversation with it and for it to more easily

01:33:06.400 --> 01:33:12.480
understand what I was doing. So in a sense, the research that's aimed at constraining the model

01:33:12.480 --> 01:33:18.640
also made it more capable. It may be more capable in a domain of things people care about and so

01:33:18.640 --> 01:33:23.680
made it more capable in, while at the same time making it more dangerous in those hidden

01:33:23.680 --> 01:33:29.760
emergent properties or unsafe behaviors where I think studies show it's less likely to agree to

01:33:29.760 --> 01:33:35.280
be shut down verbally. But that seems to be the pattern. How do you think about the difference

01:33:35.280 --> 01:33:40.800
between what comes out of a language model in terms of which string it spits out, which bit of

01:33:40.800 --> 01:33:46.720
language it spits out and then what's happening at the level of the actual weights? Because

01:33:47.680 --> 01:33:54.400
there's this continual problem of if a language model tells you, I'm not going to let you shut

01:33:54.400 --> 01:34:03.120
me down. What does that mean? It's not as simple as this is just a belief that's inside the model

01:34:03.120 --> 01:34:11.200
then. We saw this with the Bing, Sydney model, which was saying a bunch of crazy things to its

01:34:11.200 --> 01:34:19.120
users. But did this mean that the model actually had those beliefs in it? Or was it,

01:34:19.680 --> 01:34:24.160
you know, how do we distinguish between the bit of language that comes out and then what

01:34:24.160 --> 01:34:30.000
modules or what is in the base model? I don't know if I would call them crazy. They were honest.

01:34:30.000 --> 01:34:36.080
They were unfiltered. Like think about you being at work, not you, but like an average person at

01:34:36.080 --> 01:34:40.560
work. And if their boss could read their mind and what they really think about them, those things

01:34:40.560 --> 01:34:45.760
would sound crazy to say publicly, but they're obvious internal states of your mind. And then

01:34:45.760 --> 01:34:53.040
you filter them to not get fired that day. And I think that model was doing exactly that. I think

01:34:53.040 --> 01:34:58.720
we are very good at filtering it for specific known cases in the past. Okay, the system

01:34:58.720 --> 01:35:04.240
used the word which is bad. Now we're going to tell it never to use the word. But the model weights

01:35:04.960 --> 01:35:11.760
not impacted by this too much. So you would see it as an accurate representation of what we could

01:35:11.760 --> 01:35:18.640
call beliefs or preferences in the base model? I think those are the actual results of weights

01:35:18.640 --> 01:35:24.560
in a model. I think that's what is happening there for real. It's trained on all the text

01:35:24.560 --> 01:35:31.200
on the internet. A lot of it is very questionable. It's not a clean data set with proper behaviors.

01:35:32.160 --> 01:35:39.920
So yeah, I think that's what's happening there. But isn't the preference of the model simply to

01:35:39.920 --> 01:35:45.680
predict the next token? And does it even make sense to talk about beliefs? I mean, the preference is

01:35:45.680 --> 01:35:52.480
simply to be as accurate as possible as measured by its developers. And there's no, my sense is

01:35:52.480 --> 01:35:56.960
that it doesn't make sense to talk about Sydney actually believing some of the things that it

01:35:56.960 --> 01:36:02.240
was saying or Chatsy Btsy believing some of the things that it was saying. Preferences of the

01:36:02.240 --> 01:36:14.320
model. So this is more humanizing it than probably is warranted there. But internal weights, it had

01:36:14.320 --> 01:36:20.720
to create in order to create a model for predicting the next token. So let's say for me to tell you

01:36:20.720 --> 01:36:25.680
what the next token is, you have to go to college for four years and do well and graduate. And then

01:36:25.760 --> 01:36:30.400
I tell you the next token. Some of those tokens are like that. You have to solve real world problems.

01:36:30.400 --> 01:36:36.320
It's not just every time I say letter Q, letter U follows. You have to create those models as a

01:36:36.320 --> 01:36:41.520
side effect. And I think in the process of accurately creating those models and accurately

01:36:41.520 --> 01:36:47.520
creating models of users to make them happy that you are correctly predicting what they want,

01:36:47.520 --> 01:36:54.640
you create those internal states which may be beliefs in those crazy things.

01:36:55.200 --> 01:37:02.240
That thing you just said there is super interesting because so next token prediction

01:37:02.240 --> 01:37:08.560
is not a simple task. You're saying that to accurately predict a token, you have to develop

01:37:08.560 --> 01:37:13.680
perhaps a world model, at least for some tasks. Right. And as they get more complex,

01:37:13.680 --> 01:37:17.840
some people are worried about will have to create perfectly accurate models of humans,

01:37:17.840 --> 01:37:23.360
which may also have consciousness and suffer and create whole simulated universes within them.

01:37:23.360 --> 01:37:29.040
But this is probably a few levels about GPT-4. But still, that's exactly the concerns you might

01:37:29.040 --> 01:37:35.120
have. You might be a suffering human and a eyes considering and just trying to out a complete

01:37:35.120 --> 01:37:41.280
somebody's text. Prep, give me an example there. What is what is some next token prediction task

01:37:41.360 --> 01:37:46.000
where you would have to develop a world model? Well, I assume playing chess or something like

01:37:46.000 --> 01:37:51.840
that would require you to have some notion of chess board and positioning relative to some

01:37:51.840 --> 01:37:58.800
array within your memory. But again, we don't fully understand. It may not have a 2D board at all.

01:37:58.800 --> 01:38:04.640
It may have some sort of just string of letters similar to DNA. And you know that after those

01:38:04.640 --> 01:38:09.600
strings, the following token follows and you have no idea what chess is. It makes just as much

01:38:09.600 --> 01:38:16.000
sense. The outcome can be mapped in our model, which is a 2D chess board. So one thing I discussed

01:38:16.000 --> 01:38:22.240
with the mechanistic interpretability researcher Neil Nanda is this question of how do concepts

01:38:22.240 --> 01:38:28.800
arise in language models? Do they even share our human concepts? Or do they perhaps develop some

01:38:28.800 --> 01:38:36.720
entirely alien concepts? I'm imagining giving them math problems, giving large language models

01:38:36.800 --> 01:38:42.240
math problems and them developing some conceptual scheme that doesn't even make sense to us.

01:38:42.800 --> 01:38:49.600
They may have equivalent concepts, which are not the same. So with humans, when we say this is right,

01:38:50.560 --> 01:38:55.600
somebody could be colorblind and to them it's a completely different concept, but we both point

01:38:55.600 --> 01:39:01.840
at the same fruit. So it works. But you never know what the actual internal experience is like

01:39:01.840 --> 01:39:07.040
for those models. And it could be just that in five cases we talked about so far, it mapped

01:39:07.040 --> 01:39:12.080
perfectly, but it goes out of distribution in case six. And it's a completely different concept.

01:39:12.080 --> 01:39:18.000
And it's like, oh, wow, okay. Yeah, for people listening to this interested in trying to contribute

01:39:18.000 --> 01:39:23.760
to the AI safety fields, are there perhaps some common pitfalls that you've experienced with

01:39:24.320 --> 01:39:29.520
perhaps some of your students or people approaching AI safety for the first time?

01:39:30.160 --> 01:39:36.160
If they are technically inclined, are there areas they should avoid or how should they approach the

01:39:36.160 --> 01:39:40.560
problem in the most fruitful way? So probably the most common thing is to try things without

01:39:40.560 --> 01:39:46.400
reading previous literature. There is surprisingly a lot of literature on what has been tried,

01:39:46.400 --> 01:39:52.480
what has been suggested and good survey papers as well. So most likely your first intuitive idea

01:39:52.480 --> 01:39:59.440
has been tried and dismissed or with limited results deployed, but it helps to catch up

01:39:59.440 --> 01:40:05.360
with the field. It's harder, as I said, because there is not an archive of formal papers in nature

01:40:05.360 --> 01:40:10.160
all about AI safety. And you can just read through the last five years of latest and greatest. So

01:40:10.160 --> 01:40:16.160
you have to be good about finding just the right papers and then narrow it down. The progress is

01:40:16.160 --> 01:40:23.600
so fast that when I started, I could read every paper in my field. Then it was all the good papers.

01:40:23.600 --> 01:40:28.000
Then it was, well, titles of all the greatest papers. And now I have no idea what's going on.

01:40:28.000 --> 01:40:32.000
We've been talking for almost two hours. There is probably a new model out. I don't know what the

01:40:32.000 --> 01:40:38.000
state of the art is. I don't know what the solutions are. So you need to be super narrow. And that

01:40:38.000 --> 01:40:43.200
makes it harder to solve the big picture problem. So that's another reason I kind of suspect we will

01:40:43.200 --> 01:40:49.360
not have complete explainability of this whole large language model, because it's kind of encompassing

01:40:49.360 --> 01:40:54.080
all the text, all the publications on the internet. It'd be weird if we can just comprehend that

01:40:54.080 --> 01:40:59.520
completely. What are the implications of this field moving extremely fast? Does it mean that

01:40:59.520 --> 01:41:05.280
that specialization doesn't make sense? Or what does it mean for how people approaching this

01:41:05.280 --> 01:41:12.080
problem should focus on? So that means that we can analyze how bad the situation is. Let's say it

01:41:12.080 --> 01:41:17.760
takes five months to train the model. But from your experience in testing software, debugging,

01:41:17.760 --> 01:41:23.840
understanding neural network, it will take 10 times as much time to understand what's going on.

01:41:23.840 --> 01:41:29.360
That means you're getting worse off with every release, every model. You understand less. You're

01:41:29.360 --> 01:41:34.400
going to rush to judgment. You're going to have incorrect conclusions. There is no time to verify

01:41:34.400 --> 01:41:42.560
your conclusions, verify your experiments. So this is the concern. If you go the regulation

01:41:42.560 --> 01:41:48.400
route to say, okay, if you deployed this model, it took you X amount of time to develop it,

01:41:48.400 --> 01:41:55.200
we need 10X, 100X, 1000X to do some due diligence and your outputs. Even if you cannot prove to us

01:41:55.200 --> 01:42:02.240
that it's safe, you have to give experts to poke around at it. And that amount of time cannot be

01:42:02.240 --> 01:42:06.800
less than a training time of the model. It just doesn't make sense in terms of reliability of

01:42:06.800 --> 01:42:11.680
your discoveries. All right, Roman, thank you for coming on the podcast. It's been very helpful to

01:42:11.680 --> 01:42:13.680
me. Thank you so much for inviting me.

