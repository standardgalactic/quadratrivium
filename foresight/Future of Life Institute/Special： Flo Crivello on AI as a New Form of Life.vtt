WEBVTT

00:00.000 --> 00:11.040
Welcome to the Future of Life Institute podcast. My name is Gus Ducker. This is a special episode of the podcast featuring Nathan Labence interviewing Flo Crivello.

00:11.040 --> 00:26.700
Flo is an AI entrepreneur and the founder of Lindy AI. Nathan is the co-host of the Cognitive Revolution podcast, which I recommend for staying up to date on AI. Here is Flo and Nathan.

00:26.940 --> 00:46.940
Man, you know, it's, it's a tough time for somebody that tries to keep up with everything going on in AI. It's like it's gone from, you know, in 2022, I felt like I could largely keep up and, you know, wasn't like missing whole major arcs of, you know, important stories.

00:47.420 --> 01:03.180
And now I'm like, yeah, I'm like totally let go of like AI art generation, for example. And this policy stuff is really hard to keep up with, especially this week. Of course, it's like hitting a, you know, a fever pitch all at once.

01:03.660 --> 01:16.260
But, you know, it's, I love it. So I can't really complain at all. It's just, at some point, you know, got to admit that I have to maybe narrow scope somehow or just let some things fall off. I'm kind of wrestling with that a little bit.

01:16.780 --> 01:29.460
Which, which I think is just like a natural. Yeah, I mean, you know, I hear you. I think it's just a natural part of like the industry evolving. It's like, imagine, you know, talking about like keeping up with computers, right, in like the 80s or something.

01:29.460 --> 01:38.060
It's like, I'm sure at some point it was possible to keep up with computers at large, you know, it's like keeping up with tech is just like, it's like, okay, dude, it's like, it's like half the GDP over, right?

01:38.580 --> 01:43.860
You're doing all this in your second language, right? This is, I assume English is your second at least.

01:44.300 --> 01:49.180
I have an excuse. Yeah, second, I'm actually getting my American citizenship. I had the interview just yesterday.

01:49.580 --> 02:06.580
Wow, congratulations. That's great. I know it's not an easy process, although maybe it's about to get streamlined. I haven't even read that part of the executive order yet, but I understand that there is kind of an accelerated path for AI expertise. Have you seen what that is?

02:07.180 --> 02:15.780
No, but generally, there's good stuff being done in immigration, like they're relaxing a lot of these requirements, they're like closing a lot of loopholes, they're doing a lot of good stuff.

02:16.540 --> 02:36.140
Yeah, I've been thinking of just as kind of a general communication strategy, if nothing else, calling out the domains in which I am accelerationist, which are in fact many, I think, you know, you and I are pretty similar in this respect, where it's like, um, perhaps the singular question of the day.

02:36.140 --> 02:55.060
I am not an accelerationist, but on so many other things, I very much am an accelerationist and like streamlining immigration would be one of those, you know, I would sooner sign up for the one billion Americans plan than kind of, you know, the build the wall plan, certainly.

02:55.700 --> 03:15.380
And I just did a right before this was doing an episode on autonomy and, you know, self-driving. And that's another one where I'm like, holy moly, you know, I don't know if you have a take on this, but the, the recent cruise episode, I find to be, you know, kind of bringing my internal marketing,

03:15.380 --> 03:32.980
Jason, very much to the four where I'm like, we're going to let one incident a like shut down this, you know, whole thing in California. That seems crazy enough. But then the fact that they go out and like do this whole sort of performative self, I mean, whether it's performative or not, maybe it's

03:32.980 --> 03:43.940
sincere, but do this whole self-flagellation thing and, you know, shut the whole thing down nationwide. I'm like, can we, where is our inner Travis on this people? You know, somebody has to stand up for something here at some point.

03:44.020 --> 03:59.020
Totally. I agree. I think it's just the natural order of things, right? It's like, I don't know if you know that piece of history about when the automobile came about. There was this insane law that said you need to have someone walking with a flag in front of the automobile.

03:59.020 --> 04:10.180
That's no more than like four miles an hour. Right. So it's part of the process, man. It's infuriating. I hate it, but in some way, and maybe it's cool, but I made peace with it. I'm like, it's part of the process. You can't really stop. All right.

04:10.180 --> 04:22.540
That's going to do its thing. So, and it doesn't really matter anyway, because like the self-driving cars are not really deploying at a very large scale. And so I'm like, you know, it's not a bottleneck anyway. I don't think it is.

04:22.900 --> 04:40.060
I guess I have two reactions to that. One is like, it feels like if they, if nobody kind of fights through this moment, then there is like this potential for kind of the nuclear outcome where, you know, we just kind of get stuck and it's like, sorry, you know, the standards are so insane, you've got to be, you know, we do have a

04:40.060 --> 04:56.060
little bit of like a chicken and egg problem where, you know, if you had a perfect self-driving car, they'd let you deploy it, but you're not going to get to perfect unless you can kind of deploy it. And, you know, to me, this technology is just an incredible example of where, you know, the relative risk is already pretty high.

04:56.060 --> 05:09.820
As far as I can tell, they already do seem to be as safe or marginally safer, you know, maybe as much as order of magnitude safer already, depending on exactly what stats you look at. And I would just hate to see us get kind of, you know, is we're

05:09.820 --> 05:38.820
like kind of close to maybe some sort of tipping point threshold, whatever, to get stuck in a bad equilibrium of, you know, never get, and then, you know, maybe get stuck and never get out of that chicken and egg thing would just be so frustrating. I drive a 2002 trailblazer that I have sworn never to replace unless it's with a self-driving car. And it's becoming increasingly difficult to keep this thing going, you know, so I'm like, how long do I have to do that?

05:39.820 --> 06:04.820
I have to wait. My other take on this is, I think Tesla is actually like really good. I've borrowed a neighbor's. I don't know if you've done the FSD mode recently. My grandmother came up for a visit. It was fun. I actually took, you know, my 90-year-old grandmother on a trip back to her home, which is like a four-hour drive there, and then I did four hours back all in one kind of big FSD experiment.

06:04.820 --> 06:27.820
I had my laptop in the back, put a seatbelt on my laptop, so it was like recording me and recording us, you know, driving so I could kind of look at the tape later. And I was like, man, this is really good. I had no doubt in my mind coming out of that experience that it's a better driver than like other people I have been in the car with, you know, for starters.

06:27.820 --> 06:53.820
So I'm thinking through my personal life, like, yeah, I'd rather be in the car with an FSD than this person and that person and this other person, you know, and I'd be definitely more likely to let it drive my kids than this other person. So I felt like it was really good. And then the other thing that was really striking to me was the things where it messed up, I mean, there weren't many mess ups for one thing, but like the few mess ups that we had, there were a couple in an eight hour thing.

06:53.820 --> 07:22.820
It was like, if we actually had any mojo and we went around kind of cleaning up the environment, we could solve a lot of this stuff. Like there was one that my neighbor who lent me the car said, you know, you're going to get to this intersection right there on the way to the highway and it's going to miss the stop sign because there's a tree in the way. And I was like, you know, for one thing, probably people miss that too, like, let's trim the trees, you know, and then there's another one where you're getting off the highway and there's a stop sign that's kind of ambiguous, like, it's meant for the people on the service road.

07:22.820 --> 07:36.820
But it appears to be facing you as you're coming off the highway. And so the car saw that and stopped there. And that was probably the most dangerous thing that it did was, you know, stopping where people, you know, coming up the off ramp, like, do not want you or expect you to be stopped there.

07:36.820 --> 07:50.820
But that's another one where you could just go like, put up a little blinder, you know, to just very easily solve that problem. And I imagine people must have that problem too. And we just have no, no will, you know, when it comes to that.

07:50.820 --> 07:56.820
And again, it's I feel like I'm turning into Mark Andreessen. The more I think about self driving over the last few days.

07:56.820 --> 07:58.820
No, I'm with you on that.

07:58.820 --> 08:11.820
So where else are you accelerationist that may not be obvious as we kind of think about this, you know, this kind of AI safety and regulation moment that we're in?

08:12.820 --> 08:31.820
You know, pretty much everywhere, man, like I'm a Libertarian, like I used to work at Uber where I saw regulatory capture and I saw cocktails and I do believe, you know, it's the deepest level that cocktails and regulatory capture and generally, I think it's Menker Olsen who calls them

08:32.820 --> 08:41.820
Extractive institutions, who are just in the business of they don't want to grow the pie, they just want to grab a little bit more of the pie for themselves.

08:41.820 --> 08:48.820
Even if it actually shrinks the pie, they don't care as much as they get bigger chunk. And I think that's the world is just rotten.

08:48.820 --> 09:00.820
With thousands and thousands of these institutions, whether without private or without unions or governmental, it doesn't matter. We just have so many of these cartels floating around and it's killing everything.

09:00.820 --> 09:17.820
Right. It's a tragedy. And I totally understand how folks like Mark Andreessen would be, they have built such a deep and justified hatred and reaction for this nonsense that is destroying everything.

09:17.820 --> 09:27.820
That is immediately just the pattern recognition immediately triggers when they see what's happening with the eye. They're like, ah, it's happening again. They're doing it again.

09:27.820 --> 09:37.820
It's like chill. I totally get it. But this time is really different. This is really something special that's happening, not just in the markets, not just in the economy, not just in the country, in the universe.

09:37.820 --> 09:45.820
Like there is a new form of life that's being built. And this is, we're like a new territory and we need to be careful right now.

09:45.820 --> 09:58.820
Right. And so that's, that's where I'm coming from is like, I totally see that point of view. And I'm like, regulation, for sure there's going to be cartels for sure we're going to screw up 90% of it.

09:58.820 --> 10:03.820
Politics is going to get messy and trench interest are going to get into play.

10:03.820 --> 10:14.820
And it's all worth it because what may very well be on the line, it sounds alarmist, but I'm sorry that we need to say the word may be literally human extinction.

10:14.820 --> 10:25.820
Right. And this is not some tinfoil hat theory. There's a more and more experts that are coming around and saying that it's actually funny. Mark Andreessen, if you dig it up, I'm sure you could find it.

10:25.820 --> 10:31.820
I think it was an interview from him. I want to say between 2017 and 2020 that doesn't help him because he gives so many of those.

10:31.820 --> 10:37.820
But I think he said something like, at the time he was actually appealing to an argument of authority.

10:37.820 --> 10:45.820
He was like, look, he was saying the same things he's saying today, poverty is good. It's just a tool. And by the way, the experts say there's nothing to worry about.

10:45.820 --> 10:52.820
So I don't know. You guys don't know anything about AI. I don't know anything about AI. They do. And they're telling us there's nothing to worry about.

10:52.820 --> 10:57.820
The argument isn't true anymore. The experts are telling us there is something to worry about.

10:57.820 --> 11:03.820
And now it's just like, oh, arbitrary, like a regulatory capture. No, no, it's not regulatory capture.

11:03.820 --> 11:11.820
Like OpenAI was founded on that premise from day one. So if it was regulatory capture, there's like one hell of a plan.

11:11.820 --> 11:16.820
It's like, oh my God, we're going to create this industry and we're going to start regulatory capturing right now.

11:16.820 --> 11:19.820
Right. It's like, that makes no sense. It was literally the plan from day one.

11:19.820 --> 11:32.820
Yeah, that's all I'm coming from. I'm largely in the EAC camp. I am in team technology, team property, team anti-regulation, but here's something very special and potentially very dangerous.

11:32.820 --> 11:38.820
So let's go back to your use of the phrase a new form of life.

11:38.820 --> 11:47.820
I, as you may recall, am very anti-analogy as a way to understand AI because I think it's so often misleading.

11:47.820 --> 11:56.820
And I often kind of say AI, artificial intelligence, alien intelligence, it may be tempting for people to kind of hear or not tempting,

11:56.820 --> 12:01.820
but it may be sort of natural for people to hear you say a new form of life and understand that as an analogy.

12:01.820 --> 12:12.820
But do you mean it as an analogy or I guess we might start to think about like, is that actually just literally true and what conditions would need to exist for it to be literally true?

12:12.820 --> 12:22.820
And you might think about things like, can AI systems reproduce themselves? Are they subject to the laws of evolution?

12:22.820 --> 12:28.820
But for starters, how literal do you mean it when you say that there's this new form of life in AI?

12:28.820 --> 12:35.820
I mean it's pretty literally, I think if you zoom all the way out literally from the birth of the universe,

12:35.820 --> 12:42.820
the evolution of the universe has been towards greater and greater degrees of self-organization of matter.

12:42.820 --> 12:49.820
And there's actually a case to be made that this is just a natural consequence of the second law of thermodynamics,

12:49.820 --> 12:52.820
this amazing book that Iac people love to quote.

12:52.820 --> 12:54.820
Yeah, I was going to say, you're sounding very Iac all of a sudden.

12:54.820 --> 12:57.820
It's a good point. It's called Every Life is on Fire.

12:57.820 --> 13:04.820
And so if you look at the Big Bang, a few fractions of a second after the Big Bang,

13:04.820 --> 13:08.820
it was just subatomic particles and then they ganged up together and formed atoms.

13:08.820 --> 13:13.820
And then the stage after that was the atoms ganged up together and formed molecules.

13:13.820 --> 13:20.820
And then the stage after that, the molecules became bigger and bigger because the stars exploded and caused all sorts of reactions.

13:20.820 --> 13:26.820
And so a few generations of stars later, we have like pretty big molecules and pretty heavy ones.

13:26.820 --> 13:32.820
And then these molecules formed into sort of like protein and RNA and forms of proto-life.

13:32.820 --> 13:36.820
We don't totally understand, there's a chain here that we don't totally understand,

13:36.820 --> 13:40.820
but there's a form of proto-life that formed and then life.

13:40.820 --> 13:47.820
And so you can think of like, I think it was just a DNA, actually it was RNA, DNA, nucleus of a cell,

13:47.820 --> 13:50.820
mitochondria came into that, and then, okay, good, we have a cell.

13:50.820 --> 13:54.820
And then the cells started ganging up together and now we have multicellular organisms.

13:54.820 --> 13:59.820
And then we have brains at some point, like there's like a big leap, but we have brains,

13:59.820 --> 14:03.820
like on that great march towards greater and greater degrees of step-organization.

14:03.820 --> 14:07.820
And at some point we have us, which with a little bit of hubris perhaps,

14:07.820 --> 14:11.820
considering the apex of that thing for now.

14:11.820 --> 14:15.820
It just seems crazy to me that everybody is saying like, one, this is totally normal.

14:15.820 --> 14:17.820
Oh, this is normal.

14:17.820 --> 14:23.820
This is quintillions of atoms that are organized in this weird, super coherent fashion

14:23.820 --> 14:25.820
that are pursuing a goal in the universe.

14:25.820 --> 14:29.820
Like what's happening right now on Earth is all the weird to begin with.

14:29.820 --> 14:33.820
So people are all deep thinking that this is normal and that's what it is,

14:33.820 --> 14:36.820
and that this march is going to stop at them.

14:36.820 --> 14:39.820
And they're like, well, maybe we're going to get slightly smarter,

14:39.820 --> 14:44.820
or maybe we're going to get augmented and I'm like, you are such a leap compared to an atom,

14:44.820 --> 14:48.820
or compared to a bacteria, that there is no reason to expect that there wouldn't be

14:48.820 --> 14:52.820
another thing above you that is as much more complex or bigger than you,

14:52.820 --> 14:54.820
as the new world to the bacteria.

14:54.820 --> 14:57.820
Like there's nothing in the universe that forbids that from happening.

14:57.820 --> 15:01.820
From a being to exist that is about as big as the planet or the galaxy.

15:01.820 --> 15:04.820
Like there's nothing forbidding that in the universe from happening.

15:04.820 --> 15:08.820
And from the first time now, if you squint, we can sort of see how that happens.

15:08.820 --> 15:14.820
And silicon-based intelligence certainly seems to have a lot of strengths

15:14.820 --> 15:17.820
at its sleeve versus carbon-based intelligence.

15:17.820 --> 15:20.820
And so no, I actually sort of mean that pretty vitrally.

15:20.820 --> 15:24.820
It is sort of in line with the march of the universe and this is the next step,

15:24.820 --> 15:26.820
perhaps it's significant.

15:26.820 --> 15:33.820
And so I am hopeful that we can manage this transition without us being destroyed.

15:33.820 --> 15:35.820
That's what I want to have.

15:35.820 --> 15:41.820
Does that imply an inevitability to advanced AI?

15:41.820 --> 15:46.820
I guess a lot of people out there would say, hey, let's pause it,

15:46.820 --> 15:50.820
slow the whole thing down, and then you get kind of the response from an open AI

15:50.820 --> 15:54.820
where they're sort of saying, yeah, we do take these risks very seriously

15:54.820 --> 15:57.820
and we want to do everything we can to avoid them.

15:57.820 --> 16:00.820
But we can't really pause or we don't think that would be wise

16:00.820 --> 16:03.820
because then the compute overhang is just going to grow

16:03.820 --> 16:07.820
and then things might even be more sudden and disruptive in the future.

16:07.820 --> 16:16.820
Where are you on kind of the inevitability of this increasingly capable AI coming online?

16:16.820 --> 16:19.820
I don't think it's totally inevitable.

16:19.820 --> 16:23.820
I am generally a huge believer in human agency.

16:23.820 --> 16:27.820
I think we can do pretty much anything we set our minds to.

16:27.820 --> 16:31.820
I see a contradiction, by the way, in the EACC argument that like,

16:31.820 --> 16:33.820
on the one hand it's inevitable and try to stop it,

16:33.820 --> 16:36.820
on the other hand, oh my god, if you do this, I'm going to stop it.

16:36.820 --> 16:38.820
It's like, you got to decide here.

16:38.820 --> 16:41.820
So unfortunately, it's not necessarily inevitable.

16:41.820 --> 16:44.820
I am actually worried as much as the next guy, I agree,

16:44.820 --> 16:48.820
there is a risk that we over-regulate and miss out on the upside.

16:48.820 --> 16:50.820
And the upside is significant.

16:50.820 --> 16:53.820
And if you look like during the Middle Ages,

16:53.820 --> 16:56.820
we successfully as a civilization stopped progress

16:56.820 --> 16:59.820
and in a lot of countries, if you look at North Korea, they did it.

16:59.820 --> 17:01.820
They successfully stopped progress.

17:01.820 --> 17:02.820
So you can stop progress.

17:02.820 --> 17:03.820
Progress is not inevitable.

17:03.820 --> 17:05.820
Or maybe it is actually quite fragile.

17:05.820 --> 17:07.820
So no, I don't think it's inevitable.

17:07.820 --> 17:09.820
And I'm hopeful that we can, again,

17:09.820 --> 17:13.820
I want us to get the upside without experiencing the downside.

17:13.820 --> 17:17.820
The North Korea example is an interesting one.

17:17.820 --> 17:20.820
If I was going to kind of dig in there a little bit more, I might say,

17:20.820 --> 17:25.820
okay, I can understand how if things go totally off track,

17:25.820 --> 17:32.820
then we could maybe enter into a low or no or even negative progress trajectory.

17:32.820 --> 17:37.820
If there were a nuclear war, then we may not come back from that for a long time.

17:37.820 --> 17:42.820
Or if whatever, an asteroid hit the earth or a pandemic wiped out 99%,

17:42.820 --> 17:46.820
like there's extreme scenarios where it's pretty intuitive for me

17:46.820 --> 17:52.820
to imagine how progress might stop or just be whatever,

17:52.820 --> 17:54.820
greatly reversed or whatever.

17:54.820 --> 17:59.820
If I'm imagining kind of a continuation-ish of where we are,

17:59.820 --> 18:07.820
then it's harder for me to imagine how we don't kind of keep on this track.

18:07.820 --> 18:11.820
Because it just seems like everything is, we're in this, I would call it,

18:11.820 --> 18:13.820
I don't know if it's going to be a long-term exponential,

18:13.820 --> 18:18.820
but we seem to be entering a steep part of an S-curve where hardware is coming

18:18.820 --> 18:22.820
on lines by the order of magnitude and at the same time,

18:22.820 --> 18:26.820
like algorithmic improvements are taking out a lot of the compute requirements.

18:26.820 --> 18:29.820
And we're just seeing all these existence proofs of what's possible

18:29.820 --> 18:33.820
and all sorts of little clever things and scaffolding along the lines

18:33.820 --> 18:36.820
of some of the stuff that you're building is getting better and better.

18:36.820 --> 18:40.820
Is there a way that we can, do you think it is realistic to think we could

18:41.820 --> 18:48.820
kind of meaningfully pause or even stop without a total derailment of civilization?

18:48.820 --> 18:52.820
The derailment of civilization thing, you could imagine the most extreme scenario

18:52.820 --> 18:55.820
which I am not proposing, but you could imagine the most extreme scenario

18:55.820 --> 18:57.820
which is no more Warsaw.

18:57.820 --> 19:01.820
You do not exponentially improve your semi-conductors anymore.

19:01.820 --> 19:05.820
That'd be crazy, right? But there wouldn't derail civilization.

19:05.820 --> 19:08.820
Civilization is not predicated upon Warsaw.

19:08.820 --> 19:11.820
We would do just fine with the chips we've got today.

19:11.820 --> 19:15.820
And if anything, I think we have a lot of overhang from the chips we have today,

19:15.820 --> 19:17.820
a few to choose overhang, right?

19:17.820 --> 19:22.820
So I actually think it is possible to do that if we wanted to.

19:22.820 --> 19:26.820
And I don't think that even this, which I think is the most extreme scenario,

19:26.820 --> 19:28.820
would actually derail civilization.

19:28.820 --> 19:35.820
Well, we are actually lucky in that there are a few choke points in the industry.

19:35.820 --> 19:40.820
Actually, more than a few. There is ASML, there's TSMC, there's NVIDIA,

19:40.820 --> 19:43.820
like all of those three are individually at our choke point.

19:43.820 --> 19:46.820
Like every regulator could at any point grab one of them and be like,

19:46.820 --> 19:48.820
no more, you just stop, right?

19:48.820 --> 19:51.820
Or you add this chip into all of your GPUs moving forward,

19:51.820 --> 19:53.820
so we have a kill switch. At the very least, we have that.

19:53.820 --> 19:57.820
So if shit really hits the fan, we have an automatic thing in place

19:57.820 --> 20:00.820
that shuts down the very GPU on this, right?

20:00.820 --> 20:04.820
Now that would be disruptive, but potentially less disruptive than the rogue ASI.

20:04.820 --> 20:08.820
So no, I actually think it is very much possible.

20:08.820 --> 20:12.820
This thing's all on the table, and I don't think there would be all that disruptive.

20:12.820 --> 20:17.820
So maybe that's a good transition to kind of where we are right now, right?

20:17.820 --> 20:19.820
We just had this executive order put out this week,

20:19.820 --> 20:24.820
and I think everybody's still kind of absorbing the 100-plus pages

20:24.820 --> 20:26.820
and trying to figure out exactly what it means.

20:26.820 --> 20:28.820
What's your high-level reaction to it?

20:28.820 --> 20:31.820
And then I'll get into some of the specifics.

20:31.820 --> 20:36.820
First of all, it's an executive order for now. It is very early.

20:36.820 --> 20:41.820
Overall, I am pleasantly surprised, not by the specifics,

20:41.820 --> 20:46.820
but by the facts that were reacting quickly,

20:46.820 --> 20:51.820
by the facts that the measures that are proposed are not insane.

20:51.820 --> 20:55.820
Like, I was afraid of, like, there's a really good case to be made.

20:55.820 --> 20:57.820
The second look, we have a different processing case.

20:57.820 --> 21:00.820
Now, a bunch of 70, 80-year-olds go running as they don't know anything

21:00.820 --> 21:02.820
when they were born. There was no mobile phone, right?

21:02.820 --> 21:04.820
Can't really blame them for not really understanding anything.

21:04.820 --> 21:07.820
And so I was afraid that the regulation would go something like,

21:07.820 --> 21:11.820
if you install Microsoft Office in your AI, then you have to make a report.

21:11.820 --> 21:14.820
So the regulation actually sort of makes sense.

21:14.820 --> 21:17.820
It's talking about Flops. It's talking about all those things of training.

21:17.820 --> 21:19.820
So I think it's a step in the right direction.

21:19.820 --> 21:23.820
I'm actually happy about what's happening with this executive order.

21:23.820 --> 21:28.820
Now, the specifics, look, the problem is that it's almost impossible

21:28.820 --> 21:33.820
to regulate AI in a way that doesn't have any loophole.

21:33.820 --> 21:37.820
So they're regulating it according to the new old Flops, and that's okay.

21:37.820 --> 21:39.820
But that's the end of the day, and then you get stuck into,

21:39.820 --> 21:42.820
okay, what happens when you have algorithmic improvements?

21:42.820 --> 21:45.820
What happens when you do URL instead of computing?

21:45.820 --> 21:48.820
And like, that's just a lot of different loopholes that researchers are going to find.

21:48.820 --> 21:51.820
And so I think, overall, it's an encouraging first step.

21:51.820 --> 21:54.820
It's funny. You know, there have been proposals around even, like,

21:54.820 --> 21:57.820
a flop threshold that would drop progressively over time

21:57.820 --> 22:02.820
in kind of anticipation of the algorithmic improvements.

22:02.820 --> 22:08.820
That's even a more probably challenging one to put out into the world,

22:08.820 --> 22:13.820
especially given people are not in general great at extrapolating technology trends

22:13.820 --> 22:19.820
or don't want to accept regulation in advance of stuff actually being invented.

22:19.820 --> 22:24.820
So we've got this flop threshold thing where basically, as I understand it so far,

22:24.820 --> 22:26.820
like, if you're going to do something this big,

22:26.820 --> 22:29.820
you have to tell the government that you're going to do it

22:29.820 --> 22:33.820
and you have to bring your test results to the government.

22:33.820 --> 22:36.820
I would agree with that. That seems like a pretty good start.

22:36.820 --> 22:42.820
And also the threshold seems like pretty reasonably chosen at 10 to the 26.

22:42.820 --> 22:49.820
Any, you know, kind of refinements on that or quibbles that you would put forward

22:49.820 --> 22:53.820
that you think like, you know, maybe the next evolution of this should take into account?

22:53.820 --> 22:57.820
I think ultimately we're tiptoeing around the issue,

22:57.820 --> 23:04.820
but ultimately we need to come to an actual technical blanket solution.

23:04.820 --> 23:12.820
Like, we will not solve ASI alignment by asking for reports from AI companies.

23:12.820 --> 23:14.820
That's not how it's going to happen.

23:14.820 --> 23:16.820
So again, I think it's a step in our direction.

23:16.820 --> 23:19.820
I'm happy with the action. I'm happy the action is not totally nonsensical.

23:19.820 --> 23:24.820
But at the end of the day, we're going to have to talk about the kill switch.

23:24.820 --> 23:29.820
The proposal I just made is one that I see more and more talked about

23:29.820 --> 23:32.820
and that's the one that I would feel best about.

23:32.820 --> 23:36.820
You've got to put this chip into your H100s and the government

23:36.820 --> 23:41.820
and there's like a centralized entity that can shut down all GPUs all at once.

23:41.820 --> 23:43.820
And by the way, it wouldn't necessarily shut down every computer

23:43.820 --> 23:47.820
because your laptop doesn't have an H100, your iPhone doesn't have an H100.

23:47.820 --> 23:48.820
That's fine.

23:48.820 --> 23:51.820
Over the long term, Moore's Law makes it so that your laptop and your phone

23:51.820 --> 23:55.820
actually end up with an H100, but at least that dies us a few years

23:55.820 --> 23:58.820
to make progress on AI safety and alignment.

23:58.820 --> 24:04.820
Ideally, we would then automate just like reportedly the Russians did during the Cold War.

24:04.820 --> 24:05.820
We would automate.

24:05.820 --> 24:10.820
Like, we would set up some detection systems to God knows how we would do that.

24:10.820 --> 24:12.820
But hey, there's no ASI going wrong.

24:12.820 --> 24:15.820
Like, the world is really changing rapidly.

24:15.820 --> 24:20.820
We're assuming it's not too late, which it may be because at that point God knows.

24:20.820 --> 24:24.820
But you could very basically that would give us the best weapon against the ASI.

24:24.820 --> 24:28.820
We would have like a gun against the ASI's hand and kill all the GPUs.

24:28.820 --> 24:30.820
You cannot operate anymore.

24:30.820 --> 24:33.820
God knows how effective that would be because at that point all bets are off.

24:33.820 --> 24:35.820
If you have an ASI God knows what it does and how it connects itself.

24:35.820 --> 24:38.820
But that would be what it would feel best about.

24:38.820 --> 24:42.820
Do you have any sense for how that would be implemented technically?

24:42.820 --> 24:47.820
It seems like you would almost want it to be something that you could kind of broadcast.

24:47.820 --> 24:55.820
You know, you almost want like a receiver on chip that would react to a particular broadcast signal

24:55.820 --> 24:59.820
and just kind of because you would not want to have like, you know, an elaborate chain of command

24:59.820 --> 25:04.820
or, you know, relying on like the dude who happens to be on the night shift at the, you know,

25:04.820 --> 25:08.820
the individual data centers to go through and like, you know, pull some lever, right?

25:08.820 --> 25:11.820
Do you know of anybody who's done kind of advanced thinking on that?

25:11.820 --> 25:14.820
That stuff is like, you know, you hear a lot of these like kill switch things,

25:14.820 --> 25:20.820
but in terms of how that actually happens so that it's not dependent on, you know,

25:20.820 --> 25:25.820
a lot of people coming through in a key moment, I haven't heard too much, to be honest.

25:25.820 --> 25:28.820
No, I haven't seen too much results on that.

25:28.820 --> 25:32.820
But, you know, I think the technical challenge does nothing in principle that makes the technical challenge

25:32.820 --> 25:39.820
unsolvable. Like we've already had a chip that can be broadcasted to for like a dollar from space,

25:39.820 --> 25:43.820
like the GPS chip does a lot of chips and like it has one on your phone.

25:43.820 --> 25:45.820
And so why not put the GPS like chip?

25:45.820 --> 25:47.820
Maybe we could literally piggyback the GPS protocol.

25:47.820 --> 25:51.820
I don't know, but why not put the chip like that in every, in every GPU?

25:51.820 --> 25:58.820
Again, if you have an ASI, God knows, like maybe it hacks the chips before we get a chance,

25:58.820 --> 26:01.820
you know, it hacks the satellites that forecast the thing, I have no idea.

26:01.820 --> 26:08.820
But again, I think pointing in this direction is what I would like things to go into the limit.

26:08.820 --> 26:13.820
I think the basically, and that's like the most extreme version of this proposal,

26:13.820 --> 26:16.820
but like the Yutkovsky airstrike proposal.

26:16.820 --> 26:22.820
That's like, you cannot accumulate billions and billions of dollars of H100s and build this thing.

26:22.820 --> 26:25.820
Else we will go up to airstrike.

26:25.820 --> 26:31.820
That's the most extreme version of this, but that actually I think is directionally correct.

26:31.820 --> 26:37.820
Like we, this is going to be the most powerful force in human history, maybe even in the universe.

26:37.820 --> 26:44.820
You cannot accumulate that stuff anymore than you can accumulate enriched plutonium, right?

26:44.820 --> 26:47.820
We've got a, we've got a four-bit stat that's the lowest level possible.

26:47.820 --> 26:51.820
And so that level cannot be the application layer because the application layer is just,

26:51.820 --> 26:55.820
it's just to diffuse those like a thousand startups everywhere and you get in the garage can build one.

26:55.820 --> 26:59.820
It's got to be at a took point and the took point today is the city code.

26:59.820 --> 27:05.820
Yeah, let's unpack that a little bit more because I think that has been an interesting debate recently.

27:05.820 --> 27:11.820
You'll hear this kind of call for let's not regulate model development.

27:11.820 --> 27:17.820
Let's regulate applications and then, you know, we can kind of have medical regulation for the medical

27:17.820 --> 27:20.820
and everything can be more appropriate and like fit for purpose.

27:21.820 --> 27:26.820
And, you know, maybe there's something else to be said for that.

27:26.820 --> 27:34.820
But yeah, I mean, if you're really worried about tail risk, it's like probably not going to be sort of medical, you know,

27:34.820 --> 27:43.820
device style regulation of, you know, diagnostic models or whatever that is going to keep things under control.

27:43.820 --> 27:50.820
So maybe you could even do a better job of steelmanning the case for the application level regulation.

27:50.820 --> 27:56.820
But I guess, you know, why do you think that give your account of why that's not viable in a little bit more detail?

27:56.820 --> 28:03.820
Yeah, I think the steelman here is like, look, people are going to use forks to poke each other in the eye.

28:03.820 --> 28:05.820
That's not a reason to forbid the fork.

28:05.820 --> 28:06.820
Like forks are awesome.

28:06.820 --> 28:09.820
We love forks just for people from poking each other in the eye with them, right?

28:09.820 --> 28:14.820
The problem is that as the fork in this analogy becomes more and more powerful,

28:14.820 --> 28:20.820
the argument loses more and more of its defense because ultimately it's just a risk benefit analysis.

28:20.820 --> 28:21.820
Right.

28:21.820 --> 28:26.820
And so the risk becomes greater and greater as the artifact becomes more and more powerful.

28:26.820 --> 28:29.820
So more powerful than the fork and al 15.

28:29.820 --> 28:32.820
And so, you know, the opinions vary about that.

28:32.820 --> 28:35.820
But look at at this point, if you look at the data,

28:35.820 --> 28:39.820
you actually save lives by heavily regulating the sale of al 15.

28:39.820 --> 28:43.820
You can't just be like, oh, sell them to everyone and just for big people from shooting each other with them.

28:43.820 --> 28:44.820
It's like, it's an al 15.

28:44.820 --> 28:46.820
What do you expect people to do with them?

28:46.820 --> 28:50.820
Now, in the more in the most extreme scenario, enriched uranium,

28:50.820 --> 28:53.820
you can't be like, you can buy all the enriched uranium you want.

28:53.820 --> 28:55.820
You don't even need to fill up a form, which by the way,

28:55.820 --> 28:58.820
that is all the executive order says right now, at least fill up a form.

28:58.820 --> 29:01.820
Can you please at least tell us what you have to do?

29:01.820 --> 29:04.820
So hey, you can build, you can build all the enriched uranium you want.

29:04.820 --> 29:07.820
Just don't bond us with us with it, please.

29:07.820 --> 29:09.820
Like when we roll it in this disappear, you can do it.

29:09.820 --> 29:11.820
Oh, no, that's not, that's not how it works.

29:11.820 --> 29:16.820
So that, that, that is why I think it's important to regulate the, the silicone layer.

29:16.820 --> 29:23.820
Do you have an intuition for sort of how likely things are to get crazy at kind of either various

29:23.820 --> 29:26.820
time scales or potentially various like compute thresholds?

29:26.820 --> 29:31.820
I was realizing, I did an episode with Jan Tallin a couple of months back,

29:31.820 --> 29:35.820
just in the wake of the GPT-4 deployment.

29:35.820 --> 29:39.820
And he said, we dodged a bullet with GPT-4 or something like that.

29:39.820 --> 29:44.820
Like in his mind, we didn't know if, you know, even at the GPT-4 scale,

29:44.820 --> 29:49.820
like that might have already been, you know, no, no real principled reason to believe that

29:49.820 --> 29:55.820
with any, with like super high confidence that the GPT-4 scale was not going to cross

29:55.820 --> 29:59.820
some, you know, critical threshold or whatever.

29:59.820 --> 30:03.820
I guess I don't really have a great sense for this.

30:03.820 --> 30:08.820
I just kind of feel like, and this was purely like gut level intuition that, yeah,

30:08.820 --> 30:11.820
we could probably do like GPT-5 and it'll probably be fine.

30:11.820 --> 30:13.820
And then kind of beyond that, I'm like, I have no idea.

30:13.820 --> 30:19.820
Do you have anything more specific that you are working with in terms of a framework of like how,

30:19.820 --> 30:24.820
you know, when you hear, for example, Mustafa from inflection say, oh yeah,

30:24.820 --> 30:27.820
we're definitely going to train, you know, orders of magnitude bigger than GPT-4

30:27.820 --> 30:28.820
over the next couple of years.

30:28.820 --> 30:32.820
Are you like, well, as long as you stay to two to three orders of magnitude more,

30:32.820 --> 30:33.820
we'll be okay.

30:33.820 --> 30:37.820
Or like, I just have no, you know, we're just flying so blind,

30:37.820 --> 30:40.820
but I wonder if maybe you're flying slightly less blind than I am.

30:40.820 --> 30:46.820
I am of the opinion that GPT-4 is the most critical component for AGI.

30:46.820 --> 30:51.820
And that's the gap from GPT-4 to proper AGI is not research, it's engineering.

30:51.820 --> 30:54.820
It sits outside the model.

30:54.820 --> 31:00.820
So I think we have a capabilities overhang here that can turn GPT-4, as it is today,

31:00.820 --> 31:04.820
into AGI, into proper AGI.

31:04.820 --> 31:07.820
I think generally that's the case for any technology.

31:07.820 --> 31:12.820
If you look, for example, at Bitcoin, what changed from a technological standpoint

31:12.820 --> 31:14.820
that allowed Bitcoin to happen?

31:14.820 --> 31:17.820
It was the same technology we'd had for a while and yet Bitcoin,

31:17.820 --> 31:19.820
it's going to go wild to happen.

31:19.820 --> 31:24.820
So there was this overhang and Bitcoin, whatever your opinion about crypto,

31:24.820 --> 31:27.820
changed a lot of games, right?

31:27.820 --> 31:29.820
I think there's this huge overhang with GPT-4.

31:29.820 --> 31:33.820
I think we basically have a reasoning module of AGI.

31:33.820 --> 31:37.820
I don't know if you saw this paper that found literally just asking it,

31:37.820 --> 31:39.820
hey, take a deep breath and take a step back.

31:39.820 --> 31:41.820
Just take a step back apparently also makes a huge difference.

31:41.820 --> 31:44.820
So I think there's a lot of tricks like that that will make a difference.

31:44.820 --> 31:49.820
The sort of cognitive architectural layers around GPT-4 I think can bring it to AGI.

31:49.820 --> 31:54.820
That is also why you asked me about what sort of regulation I wish was put into place.

31:54.820 --> 31:56.820
We need to stop open sourcing this model.

31:56.820 --> 31:58.820
We don't know what kind of overhang exists out there.

31:58.820 --> 32:02.820
I don't think Lama-2 is there, but like I said, I think GPT-4 is there.

32:02.820 --> 32:05.820
So Lama-3, if it's GPT-4 level, boom, it's too late.

32:05.820 --> 32:06.820
The weights are out there.

32:06.820 --> 32:10.820
Okay, now you can do, maybe you can put strap on there.

32:10.820 --> 32:13.820
So we need to stop open sourcing this next.

32:13.820 --> 32:19.820
I expect my timelines for proper AGI to emerge is two to eight years.

32:19.820 --> 32:24.820
I think there's a more than even chance of AGI emerging in two to eight years.

32:24.820 --> 32:27.820
I think the base scenario is things are going to go well just for the record.

32:27.820 --> 32:29.820
I don't think there's like a 99% chance of doom.

32:29.820 --> 32:33.820
But even if it's 10%, I think it's worth being very, very worried about.

32:33.820 --> 32:34.820
That's enough for me.

32:34.820 --> 32:39.820
10% of all of us dying like I'm talking about it, please.

32:39.820 --> 32:45.820
So two to eight years, 50% chance of AGI, things probably will go well,

32:45.820 --> 32:48.820
except for, you know, CVD digital disruption.

32:48.820 --> 32:49.820
There's going to be like stuff.

32:49.820 --> 32:52.820
There's going to be a crazy shit happening, but two to eight years.

32:52.820 --> 32:53.820
And after that, all bets off.

32:53.820 --> 32:57.820
I have no idea what the bootstrapping to ASI look like,

32:57.820 --> 33:01.820
but I don't expect ASI to take more than 30 years.

33:01.820 --> 33:04.820
So I expect that you and I in our lifetimes are going to see ASI.

33:04.820 --> 33:06.820
So that's a pretty striking claim.

33:06.820 --> 33:09.820
I think you probably puts you in a pretty small minority.

33:09.820 --> 33:15.820
And I don't think I'm really there with you when you say that you think GPT-4

33:15.820 --> 33:22.820
kind of already contains the, you know, the kind of necessary core element for an AGI.

33:22.820 --> 33:24.820
So I'd like to understand that a little bit better.

33:24.820 --> 33:31.820
I mean, you'll have a lot of people who will say, you know, look, it can't play tic-tac-toe.

33:31.820 --> 33:39.820
I think on some level, those kind of, oh, look at these like simple failure objections are kind of lame

33:39.820 --> 33:43.820
and sort of miss the point because of all things obviously can do.

33:43.820 --> 33:47.820
But I do, you know, if I'm thinking like, does this system seem like it has this kind of sufficiently

33:47.820 --> 33:49.820
well-developed world model?

33:49.820 --> 33:54.820
Or, you know, I'm not even sure exactly how you're conceiving of the core thing.

33:54.820 --> 34:00.820
But, you know, for a question like that, I would say those failures maybe are kind of illuminating.

34:00.820 --> 34:08.820
On the other hand, I'm sure you've seen this Eureka paper out of NVIDIA recently where they used GPT-4

34:08.820 --> 34:14.820
as a superhuman reward model author to teach robot hands to do stuff.

34:14.820 --> 34:20.820
And I thought that one was pretty striking because as far as I know, and I actually used the term Eureka moment,

34:20.820 --> 34:29.820
many times said, we don't see yet Eureka moments coming from highly general systems.

34:29.820 --> 34:32.820
You know, we see Eureka moments from like an alpha go.

34:32.820 --> 34:36.820
We haven't really seen like Eureka moments from a GPT-4 until maybe this.

34:36.820 --> 34:43.820
This seems like maybe one of the first things where it's like, wow, GPT-4 at a task that requires a lot of expertise.

34:43.820 --> 34:48.820
That is designing reward functions for robot learning, robot reinforcement learning.

34:48.820 --> 34:53.820
GPT-4 is meaningfully outperforming human experts.

34:53.820 --> 34:56.820
And so I think it's very appropriate that they call it Eureka.

34:56.820 --> 34:58.820
What do you think is the core thing?

34:58.820 --> 35:00.820
You know, is it this like ability to have Eureka moments?

35:00.820 --> 35:01.820
Is it something else?

35:01.820 --> 35:03.820
Why do you feel like it's there?

35:03.820 --> 35:06.820
And does it not trouble you that it can't play tic-tac-toe?

35:06.820 --> 35:15.820
For the sake of this conversation, I'm going to define a GI as a seed AI, an AI that can recursively self-improve.

35:15.820 --> 35:20.820
That's a much more narrow definition of a GI than most people use, but that's actually what I care about.

35:20.820 --> 35:25.820
Can we enter this recursive loop of self-improvement that puts track surface to ASI?

35:25.820 --> 35:27.820
In order to get there, you don't need to play tic-tac-toe.

35:27.820 --> 35:33.820
You need to be good enough, and the world good enough here is important,

35:33.820 --> 35:40.820
a good enough either software engineer or chip designer or AI and ML researcher.

35:40.820 --> 35:41.820
One of these things.

35:41.820 --> 35:44.820
So something that can get you to put track.

35:44.820 --> 35:47.820
And so good enough does not mean better than the best human.

35:47.820 --> 35:50.820
It doesn't even mean better than the average human.

35:50.820 --> 35:57.820
It just means good enough that you can make a difference, a positive difference in your own ability to get better.

35:57.820 --> 35:58.820
Right?

35:58.820 --> 36:02.820
So if you enter the recursive loop of self-improvement, then mathematically it's over.

36:02.820 --> 36:05.820
And yeah, when I see the NVIDIA paper, I see that.

36:05.820 --> 36:08.820
When I see our own experience with the model.

36:08.820 --> 36:11.820
So today we are using Lindy to write her own integrations,

36:11.820 --> 36:14.820
and Lindy is writing more and more of her own code.

36:14.820 --> 36:15.820
I see that.

36:15.820 --> 36:20.820
Even as it belongs to AI researchers and ML researchers,

36:20.820 --> 36:27.820
my hypothesis is that OpenAI is using GPT-4 more and more internally to perform AI research.

36:27.820 --> 36:32.820
My not hypothesis is the fact is that NVIDIA is releasing papers that's like,

36:32.820 --> 36:35.820
well, not only can we use it for AI research through this Eureka paper,

36:35.820 --> 36:37.820
but we can also use it for chip design.

36:37.820 --> 36:38.820
It works super well.

36:38.820 --> 36:41.820
We trained an AI model that does chip design super well.

36:41.820 --> 36:46.820
So we are starting to see the glimpses of that kind of recursive loop of self-improvement.

36:46.820 --> 36:51.820
Basically, the world model question kind of on the sidestep,

36:51.820 --> 36:55.820
because I feel like at this point the debate has become silly for people who argue that it's bad

36:55.820 --> 36:57.820
or doesn't have a world model.

36:57.820 --> 36:59.820
What matters is, is it good enough?

36:59.820 --> 37:03.820
And so even if it just overfits its training set,

37:03.820 --> 37:06.820
even if it's just predicting the next token and not actually understanding anything,

37:06.820 --> 37:09.820
I actually really do believe it understands a lot.

37:09.820 --> 37:13.820
But even if it's not, you can imagine it does this many-dimensional space

37:13.820 --> 37:15.820
with a ton of data points in there.

37:15.820 --> 37:17.820
And it's good by interpolating between the data points

37:17.820 --> 37:21.820
and it needs much more data points to understand anything than a human.

37:21.820 --> 37:26.820
And so there's that envelope in that space where the data points are dense enough

37:26.820 --> 37:28.820
that it can perform.

37:28.820 --> 37:30.820
And so that's called the convex hole.

37:30.820 --> 37:32.820
And then there's data points outside that convex hole,

37:32.820 --> 37:35.820
and it does really poorly outside the convex hole, much more poorly than humans.

37:35.820 --> 37:39.820
It's convex hole requires a lot more density than humans do exist.

37:39.820 --> 37:44.820
There's multiple questions, which are, one, all these data points inside,

37:44.820 --> 37:47.820
the convex hole is the sum of all human knowledge.

37:47.820 --> 37:49.820
GP for today knows more than you.

37:49.820 --> 37:51.820
I don't know that it can reason better than you,

37:51.820 --> 37:53.820
that's the expanding the convex hole thing,

37:53.820 --> 37:55.820
but it knows more than you inside that convex hole.

37:55.820 --> 37:59.820
And so inside that convex hole, an AI researcher that's read every paper ever,

37:59.820 --> 38:01.820
not just in AI, but in mass and biology,

38:01.820 --> 38:03.820
every paper ever finds the entirety of the internet,

38:03.820 --> 38:07.820
is it better than a human AI researcher?

38:07.820 --> 38:10.820
I think the answer is yes.

38:10.820 --> 38:13.820
Even if it's not better, there's the outside of that convex hole,

38:13.820 --> 38:16.820
and this is my point about the capabilities of a hang,

38:16.820 --> 38:21.820
can we get this AI model through prompting, through cognitive architecture,

38:21.820 --> 38:23.820
to do better outside its convex hole?

38:23.820 --> 38:25.820
And we'll see that all the time,

38:25.820 --> 38:27.820
seeing papers come out to bed like,

38:27.820 --> 38:30.820
hey, we have found an automatic way to rewrite a prompt that makes it a lot better.

38:30.820 --> 38:34.820
We have found a way that people that came out a few days ago,

38:34.820 --> 38:38.820
that's like, hey, if you ask the model to take a step back

38:38.820 --> 38:42.820
and to rephrase the problem you're giving it in terms of a universal problem,

38:42.820 --> 38:44.820
it performs a lot better.

38:44.820 --> 38:46.820
And that makes total sense,

38:46.820 --> 38:52.820
because the specific of the problem is probably not seen as that specific problem in its dataset,

38:52.820 --> 38:56.820
but if you ask it to reframe it, it's basically translating the problem into a form

38:56.820 --> 38:58.820
in which it's comfortable with.

38:58.820 --> 39:02.820
And so we're actually getting it to grow its convex hole like that.

39:02.820 --> 39:07.820
That's my take, is I think the convex hole is good enough to get to that good enough point,

39:07.820 --> 39:09.820
and I think we can grow that convex hole.

39:09.820 --> 39:14.820
And so I think that basically, if GPT-4 isn't a CDI, it will still GPT-5 is one.

39:14.820 --> 39:16.820
Yeah, that's an interesting framing.

39:16.820 --> 39:18.820
I find your analysis there pretty compelling.

39:18.820 --> 39:25.820
The idea that, you know, given what we have seen from like a Eureka,

39:25.820 --> 39:30.820
you know, with this robot training, or there was another interesting one recently,

39:30.820 --> 39:34.820
I think it was out of Microsoft, I covered this in one of the research rundown episodes,

39:34.820 --> 39:40.820
on recursive or iterative improvement on a software improver.

39:40.820 --> 39:43.820
So they basically take a real simple software improver, you know,

39:43.820 --> 39:47.820
that can improve a piece of software, and then they feed that software improver to itself

39:47.820 --> 39:50.820
and just run that on itself over and over again.

39:50.820 --> 39:55.820
You know, it kind of tops out because it's not, it doesn't, you know,

39:55.820 --> 39:58.820
in this framework, it doesn't have access to like tinkering with, you know,

39:58.820 --> 40:04.820
possible methods for training itself, but it makes significant improvement

40:04.820 --> 40:09.820
and gets us some pretty advanced algorithms where it starts to do like genetic search

40:09.820 --> 40:11.820
and, you know, a variety of things where I'm like,

40:11.820 --> 40:14.820
I don't even really know what that is, you know, like simulated annealing algorithms.

40:14.820 --> 40:18.820
I'm like, what, you know, but it comes up with that and, you know,

40:18.820 --> 40:20.820
uses that to improve the improver.

40:20.820 --> 40:24.820
And, you know, this is all measured by how effectively it can do the downstream task.

40:24.820 --> 40:30.820
It does seem like it's not a huge stretch to say that, you know,

40:30.820 --> 40:35.820
could you take the architecture of GPT-4 and start to do, you know,

40:35.820 --> 40:42.820
parameter sweeps and start to, you know, mutate the architecture itself.

40:42.820 --> 40:44.820
It seems like it probably can do that.

40:44.820 --> 40:46.820
And I would agree, you know, it probably does.

40:46.820 --> 40:50.820
Yeah, certainly just based on what I do, you know, with GPT-4 for coding,

40:50.820 --> 40:54.820
I would have to imagine that it is in heavy use as they're, you know,

40:54.820 --> 40:59.820
performing all that kind of exploratory work, you know, within an open AI.

40:59.820 --> 41:02.820
And so, yeah, and I think to your point,

41:02.820 --> 41:06.820
we all see enough of these signs of life across the board in a lot of different areas.

41:06.820 --> 41:09.820
A lot of institutions are like, ah, a little bit of very good stuff

41:09.820 --> 41:11.820
from here, a little bit here, a little bit here.

41:11.820 --> 41:14.820
It's not very hard to imagine it getting to a state velocity,

41:14.820 --> 41:17.820
to imagine it going super critical and pass a certain threshold where I say,

41:17.820 --> 41:19.820
okay, now boom, it can ready take off.

41:19.820 --> 41:24.820
So, and I've actually heard multiple people from open AI say that they believe,

41:24.820 --> 41:27.820
and I agree with their conclusion.

41:27.820 --> 41:29.820
And they actually told me that before I agreed with them,

41:29.820 --> 41:32.820
they told me that at the very beginning of the year,

41:32.820 --> 41:34.820
so before GPT-4 was widely available.

41:34.820 --> 41:37.820
And they told me, you know, I think we're like, you know,

41:37.820 --> 41:40.820
we have a GEI and we're in a slow take off.

41:40.820 --> 41:42.820
And I felt something like this, crazy.

41:42.820 --> 41:48.820
Well, they didn't say, sorry, they basically were talking about GPT-4.

41:48.820 --> 41:54.820
I think, and I am not representing that this is the universal position of open AI,

41:54.820 --> 41:57.820
but I've heard multiple people from open AI and other labs tell me that.

41:57.820 --> 42:00.820
We have a GEI and we're in a slow take off.

42:00.820 --> 42:04.820
So, given that, okay, we've got this compute threshold.

42:04.820 --> 42:07.820
We maybe need a kill switch.

42:07.820 --> 42:11.820
Now I'm getting, we started this conversation with me, with my, you know,

42:11.820 --> 42:16.820
IAC side coming out and, you know, being like,

42:16.820 --> 42:20.820
why can't we get myself driving car on the road and tolerate,

42:20.820 --> 42:23.820
you know, some reasonable amount of risk to do that.

42:23.820 --> 42:26.820
Now my other side is coming out and I'm like,

42:26.820 --> 42:28.820
okay, what else might we do, right?

42:28.820 --> 42:31.820
We've got the AI safety summit going on right now in the UK.

42:31.820 --> 42:35.820
I thought it was cool to see today that there's some kind of joint statements

42:35.820 --> 42:39.820
between Chinese and Western academics and, you know,

42:39.820 --> 42:41.820
thought leaders in the space where they're kind of saying,

42:41.820 --> 42:43.820
yeah, we need to work together on this.

42:43.820 --> 42:48.820
Human extinction is something that we think could happen if we're not careful.

42:48.820 --> 42:52.820
Do you have a point of view on kind of collaborating with China

42:52.820 --> 42:55.820
or coordinating with China?

42:55.820 --> 42:57.820
I mean, that's a tough question, obviously.

42:57.820 --> 42:59.820
Nobody really knows China.

42:59.820 --> 43:01.820
I don't think super well, but what do you think about that?

43:01.820 --> 43:04.820
I mean, are we naive to hope?

43:04.820 --> 43:06.820
I kind of feel like what else are we going to do except give it a shot?

43:06.820 --> 43:08.820
Yeah, 100%.

43:08.820 --> 43:10.820
And there is ample precedent.

43:10.820 --> 43:14.820
You know, everybody is always talking about these coordination problems.

43:14.820 --> 43:17.820
They've taken like the one-on-one course of game theory and they're like,

43:17.820 --> 43:18.820
look, we can't coordinate.

43:18.820 --> 43:23.820
Well, like if you take game theory one or two, it's like solutions to the coordination problem, right?

43:23.820 --> 43:29.820
And so the solution to the collaboration problem is few players in a very iterated game.

43:29.820 --> 43:31.820
And that is the game right now.

43:31.820 --> 43:34.820
There's very few players and they're all in a very iterated game.

43:34.820 --> 43:37.820
They're not the best buddies, but they are actually able to agree on a lot of things.

43:37.820 --> 43:40.820
And so we can coordinate with China.

43:40.820 --> 43:43.820
And again, to your point, what choice do we have anyway, right?

43:43.820 --> 43:47.820
And even if we do not coordinate with them, again, there's enough truth holds

43:47.820 --> 43:50.820
enough of which are American, right?

43:50.820 --> 43:52.820
NVDI is an American company, last time I checked.

43:52.820 --> 43:56.820
And so there's enough truth holds that we could actually do very much

43:56.820 --> 44:00.820
not give them a choice like, hey, your GPUs now have the chip right here.

44:00.820 --> 44:03.820
And so whether you like it or not, we have a satellite up here

44:03.820 --> 44:06.820
and we can tell the GPUs out there.

44:06.820 --> 44:11.820
And that wouldn't be, we could even just downright for big GPUs,

44:11.820 --> 44:12.820
by the way, to be sold in China.

44:12.820 --> 44:14.820
Like we've done stuff like that before.

44:14.820 --> 44:18.820
So no, I think coordination is definitely possible

44:18.820 --> 44:20.820
and I actually think it's going to happen.

44:20.820 --> 44:24.820
I'm actually really very much encouraged by, well, winning.

44:24.820 --> 44:27.820
Like I think the safety side is making really good progress.

44:27.820 --> 44:29.820
There is rising public awareness.

44:29.820 --> 44:31.820
I think Duffington is doing an amazing work here.

44:31.820 --> 44:32.820
The regulation is coming.

44:32.820 --> 44:34.820
It's mostly sensical.

44:34.820 --> 44:38.820
There's this sort of progress that's happening across the board.

44:38.820 --> 44:41.820
AI labs are investing more and more in safety and alignment.

44:41.820 --> 44:44.820
Even from a technical standpoint, the work that AnsibleTik is doing,

44:44.820 --> 44:46.820
I think is absolutely brilliant.

44:46.820 --> 44:49.820
So we're making really good progress across the board here.

44:49.820 --> 44:52.820
I don't want to represent that it will be on the board.

44:52.820 --> 44:53.820
Yeah, I totally agree.

44:53.820 --> 44:57.820
I would say my kind of high level narrative on this recently has been,

44:57.820 --> 45:01.820
it feels like we're at the beginning of chapter two of the overall AI story.

45:01.820 --> 45:04.820
And chapter one was largely, you know,

45:04.820 --> 45:08.820
characterized by a lot of speculation about what might happen.

45:08.820 --> 45:11.820
And amazingly, kind of at the end of chapter one,

45:11.820 --> 45:13.820
beginning of chapter two, not all,

45:13.820 --> 45:18.820
but like a large chair of the key players seem to be really serious minded

45:18.820 --> 45:21.820
and, you know, well aware of the risks.

45:21.820 --> 45:25.820
And it's easy to imagine for me a very different scenario where everybody,

45:25.820 --> 45:29.820
you know, all the leading developers are like highly dismissive of the potential problems.

45:29.820 --> 45:33.820
But it's hard for me to imagine a scenario that would be like all that much better

45:33.820 --> 45:37.820
than, you know, the current dynamic.

45:37.820 --> 45:43.820
So I do feel, you know, like overall, you know, pretty, pretty lucky

45:43.820 --> 45:47.820
or pretty grateful that, you know, things are shaping up at least, you know,

45:47.820 --> 45:51.820
to give us a good chance to try to get a handle on all this sort of stuff.

45:51.820 --> 45:52.820
One last question.

45:52.820 --> 45:53.820
This is super philosophical.

45:53.820 --> 46:01.820
I know you got to go, but how much depends in your mind on whether or not,

46:01.820 --> 46:06.820
let's say Silicon base intelligence or AI systems or whatever might become

46:06.820 --> 46:10.820
or maybe already are, you know, I'm not sure how we would ever tell the kinds of things

46:10.820 --> 46:12.820
that have subjective experience.

46:12.820 --> 46:17.820
You know, does it matter to you if it feels like something to be GPT for?

46:17.820 --> 46:20.820
Have you heard of the world move?

46:20.820 --> 46:24.820
I think it's Zen philosophy in Buddhism.

46:24.820 --> 46:29.820
There's this story that's like someone asks someone else like, hey, does kind of Doug

46:29.820 --> 46:31.820
have the essence of a Buddha?

46:31.820 --> 46:35.820
If the Buddha is everywhere and he never being kind of Doug have the essence of a Buddha.

46:35.820 --> 46:38.820
And the answer to that is move.

46:38.820 --> 46:41.820
And move means neither yes or no.

46:41.820 --> 46:43.820
It's a way to unask the question.

46:43.820 --> 46:46.820
It's a way to reject the premise of the question.

46:47.820 --> 46:53.820
And basically in this sense, it means there is no such thing as the essence of the Buddha.

46:53.820 --> 46:57.820
It's like the same question is like, hey, what happened before the universe existed?

46:57.820 --> 46:58.820
Move.

46:58.820 --> 47:01.820
There was no before because the bills of the universe was the bills of time.

47:01.820 --> 47:04.820
So the will to be full only makes a sense in the context of the universe.

47:04.820 --> 47:06.820
And so anyway, that's all of my insight.

47:06.820 --> 47:10.820
Whenever I ask a question, whenever someone asks me questions about subjective experience

47:10.820 --> 47:12.820
and consciousness, I'm like move.

47:12.820 --> 47:13.820
It doesn't exist.

47:13.820 --> 47:14.820
It doesn't matter.

47:14.820 --> 47:15.820
It's immeasurable.

47:15.820 --> 47:17.820
It's not a scientific thing.

47:17.820 --> 47:19.820
And so move.

47:19.820 --> 47:20.820
All right.

47:20.820 --> 47:24.820
Well, some questions bound to remain unanswered.

47:24.820 --> 47:26.820
And I appreciate your time today.

47:26.820 --> 47:28.820
This is always super lively.

47:28.820 --> 47:30.820
Next time I want to get the Lindy update.

47:30.820 --> 47:32.820
And at some point I want to get access.

47:32.820 --> 47:34.820
But for now, I'll just say Fluckravello.

47:34.820 --> 47:36.820
Thank you for being part of the Cognitive Revolution.

47:36.820 --> 47:38.820
Thanks, Mason.

