{"text": " All right, so this is immutable relational data. I'm Richard Feldman. So when I was first getting into Elm, there were two questions that I had, like very early on, things that I didn't really know how to do coming from JavaScript. And one was, how do I do this? Like I want to do some equivalent of courses bracket zero dot students bracket zero dot selected equals true. Like, how do I, there's a little nested record update syntax, like how do I, I don't know, I couldn't quite figure that out at first. And then another question I had was, when, if ever, should I put duplicate information in my model? Like should I ever have the same piece of information in two different places in my model? Is that a bad idea? Is that, I don't know, what are the trade-offs? I didn't really know. And as I got more into Elm, I kind of learned about different techniques for modeling data and so forth. And I ended up discovering that these two questions turned out to be related. And they have to do with the title of this talk. So we're going to go through sort of my progression of like how I learned about these things. And we're going to start with state synchronization. And then we're going to move on to talking about relational data in general. And then finally end up with immutable relational data and some techniques for dealing with it. Okay. Let's start with state synchronization. Okay. So some time ago, a friend of mine tweeted me. They said, hey, RT Feldman, remember that time in college when we were late to a potluck? And we went to a Wendy's drive-thru and got 20 orders of $1 five-piece nuggets? So we showed up to the potluck with 100 chicken nuggets, LOL. That's a true story. That did happen in college. So I saw this, I was like, oh, yeah, that was a good time. So I like, you know, went to the like button and hit like. But I got an error. I was like, no, you can't do that. It's like, what? No, no, for real. I like it. It's good times. So I hit the button again. It's error. Oh, come on. So as it turned out, what had happened was my friend, after posting this, decided this was maybe not something the entire internet needed to know. And so they ended up deleting it. And so they deleted it sometime between when I saw it and when I attempted to click like, resulting in the error. So this is essentially a sort of a mismatch between the client and the server. So my client, the web browser, was like, oh, yeah, this tweet totally exists. Sure. Here's a nice lovely like button in case you'd like to like it. But unfortunately, the server had a different idea, the server was like, oh, that's gone. What are you talking about? Like, that's been deleted for a while. So there was this mismatch between the two, and that led to the error. So this is a form of synchronization problem. Like, we had a synchronization mismatch between the client and the server. And when we have multiple sources of truth, we sort of need to synchronize them or else we can get this type of error. Now if we have multiple sources of truth, such as the client and the server, and they agree like if the client says the tweet exists and the server also believes that it exists, that's okay. That's no problem. The typical thing, which is why, you know, I was surprised to see this error and not just like, oh, yeah, another one of these just happens to me constantly. Usually despite the fact that there are multiple sources of truth, they're staying in sync so it doesn't actually matter. It doesn't sort of affect me. It's also fine if they both agree that it's gone. Like, eventually I refresh the page and then I saw, oh, the thing's gone. So they got synced up and then it was again, no problem. I was back to being in sync with the server and the multiple sources of truth were not really causing any problems there for me. So basically when multiple sources of truth remain synchronized, it's okay. There's sort of no symptoms, no problems that the end user is faced with. However, when they get out of sync, as happened to me in the tweet, then that causes a problem. So single sources of truth sort of don't have this issue. There's no synchronization because there's nothing to synchronize. It's just one source of truth. So this entire class of errors goes away, which is one of the things that's nice about having a single source of truth. So synchronization is sort of error prone. This is one of the problems with it. As we saw, this is sort of a case in point. This is a synchronization error. And these sort of things can happen all the time when you have to synchronize two pieces of information because there are multiple sources of truth. Which sort of begs the question, is it necessarily avoidable to have multiple sources of truth? Sometimes do we have to have multiple sources of truth even if we would prefer to have a single source of truth and not have to synchronize anything? Let's take this client server app as an example and see if we can arrange things such that we have a single source of truth. This is our goal. We're going to have a client and a server, multiple clients and a server, and we're going to have a single source of truth that's shared between all of them such that there's nothing that we need to synchronize. So to do this, probably what we're going to have to do is say, okay, the server is going to have to take care of rendering the UI. That's going to have to be the source of truth because that's where the database lives. I mean, if anyone ever disagrees with the database about what is true, the database is going to be right. So we're going to have the server render HTML like it's 15 years ago and it's just going to send it directly to the client on every single user interaction. Like I click anything and the server is going to render some HTML and send it to the client. So that way the server is the single source of truth, kind of. Problem is that sending stuff from the server to the client takes time. It doesn't just like appear on my screen instantaneously. There's like networks and packets and like latency and things like that. And in fact, there's so much of that that it's like perceptible, which in turn means that it's entirely possible that over the course of that network transmission while the currently up-to-date perfect single source of truth UI is headed towards my browser, my friend deletes the tweet, and now I am once again out of sync. So even trying as hard as we possibly can to have a single source of truth with a client server application, it's just not possible. You cannot have a single source of truth when you've got a client and a server. You sort of must have synchronization. It's completely unavoidable by the nature of this architecture, which is kind of a bummer because hey, clients and servers are nice. So we're sort of stuck synchronizing them. And again, even if it is unavoidable, synchronization is still error-prone. We're still going to have to deal with synchronization errors like this. So we'd sort of like to handle these synchronization errors in as graceful a way as we can. And there's sort of different levels to this. So one is basically just like fail without feedback. So this is the absolute worst, which is to say the tweet gets deleted and I hit like and then just nothing happens. I'm just like, what? Or you can do what's called optimistic updates where you're just like, I'm going to update the client state and assume it works out on the server. This can be even worse and I've been really, really badly burned by this in the past. That's basically where you hit like and it does the animation. It's like, yeah, everything worked. And then later on it just was gone. What? And then, you know, so once it actually gets in sync, you find out much after the fact that there was a problem. Having had no idea at the time that there was anything wrong, which would have at least given me a clue. Like, yeah, I want to try hitting like again in case maybe the server is down or something. So failing without feedback is sort of the worst way to handle a synchronization error. We can do a little bit better by at least informing the user that some sort of error occurred. Like error, even if you're saying like error 500, that's at least giving me some clue that something went wrong so I can, you know, react accordingly and not be surprised later on to discover that there was this mismatch between my expectation and what actually happened. Even better than that is to explain the problem. Something like this tweet was deleted before you could like it or maybe something more concise than that. But explaining the problem gives the user a better experience to understand, oh, this is not just like something went wrong, which could be so many different things and lead me to take completely futile actions such as hammering on the like button. If they explain it to me and say this tweet was deleted, then I'm like, oh, okay, well, I'm not going to keep hitting like because it's gone. I understand that now. And the best of all would be to explain and then fix the problem. So basically say, hey, this tweet was deleted and then not leave the UI in a state where I can continue to hammer on the like button at all. Say like this tweet was deleted and then actually synchronize, like change it so it doesn't look like this anymore, maybe gray it out, maybe fade out the whole tweet itself. There are any number of things you can do. But these all require different degrees of effort. So in order to synchronize while also reporting the error, it's not just that the server has to send back a 500. It also has to say, here's what the problem was and also describe how to synchronize the state in some particular format, which my client then has to be looking for and know to say, oh, I understand that I got this additional data. Now I can use this to sort of patch my understanding of the world so that I'm now synchronized with the server. It's a lot more work to deal with that synchronization error. And that's not even getting into what happens if we're trying to synchronize across multiple clients where it's not clear who the source of truth is and we have to resolve conflicts. So this can be a lot of work, like a lot, a lot of work if you want to do it right. First of all, you have to make the updates in the first place. If we have multiple sources of truth and something changes, we have to go around and propagate those updates to all of the potential places where it could get out of sync. Once we've done that, then we have to detect errors, figure out if things are out of sync as they were in the case of this tweet. And then once we've detected them, then we have to potentially resolve conflicts. If we have lack of clarity around who is the source of truth, we have multiple clients who made edits at the same time on the same document. How do we resolve those? There's a lot of techniques for doing that. That's kind of a whole field of study. And then finally, ideally, gracefully recovering. So if we want to give people a good user experience or the best user experience we can in the face of all of these potential synchronization problems, we'll look into a lot of work. All of which is to say we really want to synchronize as little as possible. The more we can have single source of truth and not have to deal with any of this and not have to think about it, not have to worry about the errors, not have to spend time handling them, resolving conflicts, repairing the state and getting things back in sync, the nicer our lives are going to be and more likely, the most likely, the better our user experience is going to be as well. Okay. So let's sort of draw a smaller box around this. We've concluded that it's not possible to have a complete client server application where we have one single source of truth. But what if we narrow it down a little bit just to the client UI state? So if we just draw a box around that, what about now? We actually can have a single source of truth. And we as Elm programmers know this because we use the Elm architecture in which we have a single source of truth. That's what model's job is. It is a single atomic immutable value that is the single source of truth for the entire client application state and everything else is built on that. And this is one of the reasons that the Elm architecture is nice to use, is that we have the single source of truth. We don't have to go around syncing a bunch of different disparate pieces of state. Like we might if we had something that were, you know, that is like one of the sources of truth and then we have a lot of other ones sprinkled around. So this is a good thing. This helps us out. So this brings me to my other question from earlier. Like when might model end up with multiple sources of truth anyway? Like when might it go from being the single source of truth to being a store that contains multiple values, each of which refer to the same piece of information and which might have different values for that piece of information depending on which part of the model I look at? Well one case where this might genuinely want to happen is caching. So and this is something that I would do for performance. So like basically let's say I have some sort of really expensive calculation that I'm doing using model data. Really in web applications this almost never happens but it's conceivable. I could have some really expensive thing that I have to do like 60 frames per second all the time using data from the model. And I can't cache it with HTML lazy for some reason but I mean usually I can and if I can that's certainly how I want to do it because that's the cache that's very nicely managed by the Elm runtime. I don't have to think about caching validation which is one of the famously hard problems in computer science and this calculation is so expensive that it's a performance bottleneck in practice. And now we're into the territory of extremely, extremely, extremely unlikely to the extremely power because really when we're talking about performance problems it's pretty much always to do with rendering stuff. It's basically never to do with calculations like this in practice. But let's say I did actually end up with something like that. This is one of the things that caching is used for is you have a piece of state that says I am sort of an intermediate value, some sort of incremental calculation and then I can base future calculations on that just like incrementing and decrementing rather than having to rebuild it all from scratch. So yes it's not like you should never, ever, ever in a bazillion years have duplicate information in the model. There is a use case but it's pretty rare in practice. So that's also good news because it means we can have a single source of truth pretty much all the time. So how about relational data that gets stored in the model? That is to say pieces of data where we have different pieces of information in the model but they relate to one another in some way. So let's look at an example of this. Relational data. So I work in a company called No Red Ink and we make stuff for English teachers. Now let's say hypothetically, just for the purposes of this example, that we decided to sort of like broaden our scope a little bit and we decided to introduce a new feature which is not English related but actually it's field trip management. This is why I'm not in charge of the product team. So we're going to build this new feature that's going to allow teachers to manage a field trip for their students where they take everybody on a bus and they go out somewhere interesting for the day and learn things there. So we've got an all day field trip coming up and the teacher wants to answer the question which students are going, sort of manage this and check off, okay, these students are going, these students are not going. So let's look at this from a data modeling perspective. So let's say we've got one of our students named Richard Feldman, going is false which is why he's making that face. Kid looks like a troublemaker. So this is our very simple data model. We have name which is a string and going which is a boolean. And for our purposes, that's going to be enough. And then the teacher has multiple courses. So let's say a teacher has a course called second period English. It's a very common name for a course that we see in practice and the teacher has a couple of students in that course. So R. Feldman, B. Knowles, A. Einstein and they can sort of check and uncheck which students are going to be going on this field trip. This is the UI we're building. They might have another course called fifth period history. So sometimes we have teachers who have, they're not dedicated English teachers. They have some courses that are English and then some courses that are another subject. And sometimes they use no reading because we teach writing among other things. And so they want their history students to be able to write more effective essays. So they'll have no reading activated for both their history course and their English course. And of course, if you're teaching multiple subjects, you can have the same student in multiple courses. So Richard Feldman could be in second period English as well as fifth period history in addition to some other number of students. So this is an entirely plausible scenario that we might end up with. And of course, this is relational data that we have students have a relationship to the course and also we have some sort of notion of identity. Like Richard Feldman is the same student whether he's in second period English or in fifth period history. So if the teacher checks one of these boxes because it's the same person, like either he's going on the field trip or he's not and it's an all day field trip. So certainly he's not going to be going for second period but not fifth period. So if the teacher checks one of the boxes because of the relationships innate in this data, it should check both of the boxes. That's otherwise, you know, we've got some sort of mistake. We should never end up with the teacher seeing this in their UI or else we've done something wrong. This would be a synchronization bug. Okay. So in JavaScript, the way that I might have done this is I would say something like going back to the very beginning of the talk, courses brackets zero dot students brackets zero dot going equals true. Which is to say the first student in the first course is now going on the field trip. When I check that box, that's what we've changed about our data model. Now if I do this and I put into a REPL courses brackets zero dot students brackets zero, it would now say, hey, this student is now going, great. And also, if I said courses bracket one, which is to say fifth period English dot students brackets zero, it would also say that that student is going. And the reason for that is that students in both cases are mutable references in JavaScript. These are JavaScript objects, JavaScript objects are mutable, and they store mutable references to other objects. So both of these are actually pointing to the same student in memory, which means if I change the one, it's going to change the other as well. Now this has various downsides that we're probably familiar with, like you pass something to a function, you're not sure if that function is going to mutate or not, plenty of downsides, but this is one of the upsides when it comes to data consistency. It means that when I mutate one, it automatically mutates all the others at the same time for me. So this is an example of relational data with a single source of truth. We don't actually have any duplicate information in this data model that we've built in this sort of JavaScript version. Okay, so now I'm, you know, was a JavaScript programmer, now I've transitioned to an Elm programmer, and so now I'm thinking in terms of immutable data. So one way that we might model this in Elm, and probably the way that I would have modeled it when I was starting out, I would say type alias student, name colon string, going is a Boolean, then I'd have type alias course, name is a string, students is a list of students, so we have nested records here, and then finally I would have my model which would have courses, which is a list of course, and then various other things, but at least within the scope of what we're talking about here, these are the relevant structures that we'd be dealing with. Now, believe it or not, already we've introduced multiple sources of truth. Even though this basically looks like a description of the same sort of schema that we had in the JavaScript version, just by virtue of the fact that we've gone from mutable objects to immutable data, we have now accidentally introduced multiple sources of truth. And this is kind of an easy thing to do, at least it was for me, when going from JavaScript to Elm because I'm like, oh, well, records look like objects, so I'll just take the thing that I would have done with an object and I'll just do it with a record. But it turns out that that mutable to immutable characteristic is actually significant implications to sort of do a more apples to apples transition, let's go from the JavaScript object that we did before to the JavaScript object notation, better known as JSON, because JavaScript objects are mutable references, but JSON is actually immutable data, which it sort of has to be to serialize. If you want to serialize mutable references to memory locations and just write them straight to the disk, it's probably not likely to work out very well when you try to deserialize them. JSON is immutable despite having the same structure as JavaScript objects. So let's look at the JSON version of this. So we have courses, we have name, second period English, students, and then an array, name Richard Feldman going false, and then another course, name, fifth period history, students, name Richard Feldman going false. So now we can kind of see more clearly that we actually do, in fact, have duplicated data now. Like before, both of those students were pointing to the same point in memory, but now we actually have two different independent pieces of data. So that means that they can now get out of sync. I can change one of them to true without changing the other one to false. That's a potential problem. So objects have mutable references, whereas records, and JSON, have immutable records. So objects have this upside of sort of implicit synchronization that sort of automatically happens whenever you change something, whereas records also have their own set of upsides, really cheap copying, equality checks that can just do reference equality, so on and so forth. But this is something that we have to be aware of when we're going from objects to records. This is a pretty significant difference. So let's move along to immutable relational data. So what are we going to do differently when we are transitioning to the world of immutability? Okay, so this was sort of the problem that we ran into. We had this duplicated data, and it could get out of sync. We could have going is true in one case and going is false in the other case, and that shouldn't be possible because it's the same student. Whether he's going on the field trip or he's not. So one way we could create a single source of truth out of this is by changing the JSON to look like this. So let's look at these differences here. So the first thing is we've introduced explicit identifiers for what the student refers to. We're no longer using memory references, we're actually using identifiers as references. So here we have students is just an array of IDs. So ID 217, we see down at the bottom there, refers to Richard Feldman. And that's the only place that Richard Feldman appears in this whole data model. Everywhere else it's an identifier referring to that one single source of truth. So we're using explicit identifiers to reference a single source of truth in a completely immutable way. So let's translate that idea into L. So we've got our type alias student course model. So this is what we had before. We had a list of students under course and we had a list of courses under model. So we're going to tweak that a little bit. Instead we're going to go from students to student IDs, which is going to be a list of student ID values. So student ID type could be whatever you want, you know, integers as we saw in the JSON example, strings, custom types, anything you want. Inside the model we're going to have students as a new value in the model. This is our single source of truth for all of our students across the entire model. And it's going to be a dictionary. So we're going to change on that same student ID and then having a single source of truth be the value of that particular student stored in that dictionary. By the way, one thing we would also want to do here, I think, that can sort of take this data model a little bit one step further, is to change from a list of student IDs to a set of student IDs. Because really, like, would it ever be useful to have the same student appear multiple times in the same course? Not really. That should be a set because sets have uniqueness. We should only have at most one student ID of the same value inside each course. OK, so now let's write a function called students in course that's going to take one of these courses, maybe something inside update, and it's going to return a list of students. So this is sort of our replacement for we want to work in terms of a list of students, which is why we originally said courses just have a list of students, but now we're just going to do a function to do the same thing. So it's going to take a course as an argument. And it's going to start off by doing a dicks.filter. What's it going to filter on? It's going to say, OK, I'm going to look at each of the students that I have, and I'm going to say, am I in this particular course? The way I can tell if I'm in this particular course is set.member, that particular student's ID, and the course's set of student IDs. So this is going to filter that dictionary of all the students down to just the ones who are in that particular course. And that's it. We have now gone from our single source of truth of all students in one expression down to a single source of truth for the list of students that are in that particular course. And now we can do whatever we want with that list of students, whatever we would have done previously by courses.students. And then, of course, if we want to actually get it from a dictionary to a list of students, we'd pipe it to dicks.values. OK, so here we have an example of immutable relational data where we've maintained a single source of truth, which means no synchronization problems. Also it means no nested record updates, which is cool. But basically, we no longer have that whole series of problems where we have to keep things updated, keep things in sync, detect errors, resolve conflicts, and so on and so forth. OK, but what about like more complex relationships? What if we had the JavaScript equivalent of something like this? Courses bracket zero.students bracket three.assignments bracket two.is done, something where we have lots of relationships and we're trying to merge them all together. How does this sort of like scale? Well, there's actually, as it turns out, an entire sort of field of study and a lot of implementations that work with these types of questions. If anyone recognizes this logo, this is a good elephant. This is PostgreSQL. You also got other databases like MySQL, SQLites, and it turns out relational data has been well studied and explored by relational databases. And we can, what's the word I'm looking for, steal shamelessly from the things that they've learned over the years about dealing with relational data without dealing with mutable references, which they don't use because they need to serialize everything to the disk. So what if we think about dictionaries as sort of like an analogy for database tables? What could that do for us? So what if we thought about things like select, where, and join in terms of SQL, and then we translated those to dictionary concepts like get, filter, intersect, and so forth. How might that look? So here's a really, really simple SQL query. Select count star from users. So that's sort of the equivalent of saying like, tick dot size users. Ha. Stake in the slides. Select count star from users where age is greater than or equal to 18. This would be voting age users in the U.S. So the equivalent to that, dicks dot filter is voting age users, which is kind of nice because it's actually a little bit more descriptive than the greater than or equal to 18. And then pipe that to dicks dot size. I got it right on this slide. So this is like a slightly more advanced query. So what if we just, okay, let's just go for it. Let's do joins. Let's do where. Let's just get some more stuff in there. Select count star from users where age is greater than or equal to 18. So this is voting age users inner join residence on residence.id equals users.id where residence.city equals stl. And now we're saying who are, like, how many users are a voting age and live in St. Louis? How do we do that using dictionaries? Well, let's start by defining a couple of type aliases because we can't just, you know, use a plain old dictionary. We're going to need to get some records involved here. So let's say we have user, which is user ID and age. And then we also have a model which has users, which is a dictionary between user ID and user. And we have residence, which is a dictionary between user ID and city ID or user ID and city. So these are, like, examples. You can imagine many more dictionaries all at the top level, but you can model as many relationships between as many different entities as we want, just like how databases have tables all at the top level, which model relationships between as many different tables as you want. So here's how we might implement this query. So we could say locals, which is to say St. Louis locals or St. Louisans or if you're part of Nellie's crew, St. Lunatics. And we would say dix.filter, user ID, city, so that user ID being the key and then the city being the value, city.name, double equals stl. So this is going to give us, among all the residents, only those whose city is St. Louis. Then we can do dix.filter on isvotingage, model.users, just like before. So now we have a dictionary of all the users who are voting age. And then to do the join, we do dix.intersect locals, which is going to take all of the users that we've filtered out for just the ones that have voting age, and then also all of those that have city name of St. Louis, because that's where they live. And then finally, dix.size. So just by using a couple of simple expressions, we end up with being able to essentially do the same types of things that we could do in a relational database. This is like pretty powerful stuff, and it's all with a single source of truth. And no nested record updates. Pretty cool. Okay. If you're curious to learn some more data modeling techniques, completely shameless plug. There's this book by a dude, Elman Action. So check it out. Okay. So let's recap. So first we talked about state synchronization, sort of some of the problems, some of the pitfalls. We talked about relational data, and then we talked about immutable relational data without using mutable references. We talked about how a single source of truth means there's nothing to synchronize. All of those problems that we saw with state synchronization, completely out the window, none of those tweet-related problems. We talked about the synchronization work that has to be done if we do have multiple sources of truth, making updates, detecting errors, resolving conflicts, gracefully recovering in the ideal case. And we talked about how relational data can be done with a single source of truth, and in fact, immutable relational data can still be done with a single source of truth, which is really what we would prefer to do if we have the option. Then we talked about dictionaries as tables, and sort of how we can use the metaphor of tables that appear in relational databases to do the same kinds of data modeling activities using dictionaries. And finally, we come back to the original two questions that I had. When should I put duplicate information in my model? The answer is basically almost never, unless performance absolutely demands a cache, and I'm really sure I have a performance problem, and I really just can't avoid it. That's really the only situation I've ever come across where I would say that that's a good idea. And how would I do something like this in Elm? Of course, it's bracket zero.studentbracketzero.selected equals true using dictionaries as tables. Thanks very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.0, "text": " All right, so this is immutable relational data.", "tokens": [50364, 1057, 558, 11, 370, 341, 307, 3397, 32148, 38444, 1412, 13, 51014], "temperature": 0.0, "avg_logprob": -0.18446175675643117, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.03785078600049019}, {"id": 1, "seek": 0, "start": 13.0, "end": 15.08, "text": " I'm Richard Feldman.", "tokens": [51014, 286, 478, 9809, 42677, 1601, 13, 51118], "temperature": 0.0, "avg_logprob": -0.18446175675643117, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.03785078600049019}, {"id": 2, "seek": 0, "start": 15.08, "end": 19.52, "text": " So when I was first getting into Elm, there were two questions that I had, like very early", "tokens": [51118, 407, 562, 286, 390, 700, 1242, 666, 2699, 76, 11, 456, 645, 732, 1651, 300, 286, 632, 11, 411, 588, 2440, 51340], "temperature": 0.0, "avg_logprob": -0.18446175675643117, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.03785078600049019}, {"id": 3, "seek": 0, "start": 19.52, "end": 23.2, "text": " on, things that I didn't really know how to do coming from JavaScript.", "tokens": [51340, 322, 11, 721, 300, 286, 994, 380, 534, 458, 577, 281, 360, 1348, 490, 15778, 13, 51524], "temperature": 0.0, "avg_logprob": -0.18446175675643117, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.03785078600049019}, {"id": 4, "seek": 0, "start": 23.2, "end": 26.560000000000002, "text": " And one was, how do I do this?", "tokens": [51524, 400, 472, 390, 11, 577, 360, 286, 360, 341, 30, 51692], "temperature": 0.0, "avg_logprob": -0.18446175675643117, "compression_ratio": 1.4162162162162162, "no_speech_prob": 0.03785078600049019}, {"id": 5, "seek": 2656, "start": 26.56, "end": 32.16, "text": " Like I want to do some equivalent of courses bracket zero dot students bracket zero dot", "tokens": [50364, 1743, 286, 528, 281, 360, 512, 10344, 295, 7712, 16904, 4018, 5893, 1731, 16904, 4018, 5893, 50644], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 6, "seek": 2656, "start": 32.16, "end": 33.4, "text": " selected equals true.", "tokens": [50644, 8209, 6915, 2074, 13, 50706], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 7, "seek": 2656, "start": 33.4, "end": 38.48, "text": " Like, how do I, there's a little nested record update syntax, like how do I, I don't know,", "tokens": [50706, 1743, 11, 577, 360, 286, 11, 456, 311, 257, 707, 15646, 292, 2136, 5623, 28431, 11, 411, 577, 360, 286, 11, 286, 500, 380, 458, 11, 50960], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 8, "seek": 2656, "start": 38.48, "end": 41.08, "text": " I couldn't quite figure that out at first.", "tokens": [50960, 286, 2809, 380, 1596, 2573, 300, 484, 412, 700, 13, 51090], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 9, "seek": 2656, "start": 41.08, "end": 45.72, "text": " And then another question I had was, when, if ever, should I put duplicate information", "tokens": [51090, 400, 550, 1071, 1168, 286, 632, 390, 11, 562, 11, 498, 1562, 11, 820, 286, 829, 23976, 1589, 51322], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 10, "seek": 2656, "start": 45.72, "end": 46.72, "text": " in my model?", "tokens": [51322, 294, 452, 2316, 30, 51372], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 11, "seek": 2656, "start": 46.72, "end": 49.72, "text": " Like should I ever have the same piece of information in two different places in my model?", "tokens": [51372, 1743, 820, 286, 1562, 362, 264, 912, 2522, 295, 1589, 294, 732, 819, 3190, 294, 452, 2316, 30, 51522], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 12, "seek": 2656, "start": 49.72, "end": 50.72, "text": " Is that a bad idea?", "tokens": [51522, 1119, 300, 257, 1578, 1558, 30, 51572], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 13, "seek": 2656, "start": 50.72, "end": 52.72, "text": " Is that, I don't know, what are the trade-offs?", "tokens": [51572, 1119, 300, 11, 286, 500, 380, 458, 11, 437, 366, 264, 4923, 12, 19231, 30, 51672], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 14, "seek": 2656, "start": 52.72, "end": 54.84, "text": " I didn't really know.", "tokens": [51672, 286, 994, 380, 534, 458, 13, 51778], "temperature": 0.0, "avg_logprob": -0.2516088814570986, "compression_ratio": 1.7945205479452055, "no_speech_prob": 0.06750628352165222}, {"id": 15, "seek": 5484, "start": 54.84, "end": 58.24, "text": " And as I got more into Elm, I kind of learned about different techniques for modeling data", "tokens": [50364, 400, 382, 286, 658, 544, 666, 2699, 76, 11, 286, 733, 295, 3264, 466, 819, 7512, 337, 15983, 1412, 50534], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 16, "seek": 5484, "start": 58.24, "end": 59.24, "text": " and so forth.", "tokens": [50534, 293, 370, 5220, 13, 50584], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 17, "seek": 5484, "start": 59.24, "end": 64.44, "text": " And I ended up discovering that these two questions turned out to be related.", "tokens": [50584, 400, 286, 4590, 493, 24773, 300, 613, 732, 1651, 3574, 484, 281, 312, 4077, 13, 50844], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 18, "seek": 5484, "start": 64.44, "end": 67.12, "text": " And they have to do with the title of this talk.", "tokens": [50844, 400, 436, 362, 281, 360, 365, 264, 4876, 295, 341, 751, 13, 50978], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 19, "seek": 5484, "start": 67.12, "end": 71.28, "text": " So we're going to go through sort of my progression of like how I learned about these things.", "tokens": [50978, 407, 321, 434, 516, 281, 352, 807, 1333, 295, 452, 18733, 295, 411, 577, 286, 3264, 466, 613, 721, 13, 51186], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 20, "seek": 5484, "start": 71.28, "end": 72.76, "text": " And we're going to start with state synchronization.", "tokens": [51186, 400, 321, 434, 516, 281, 722, 365, 1785, 19331, 2144, 13, 51260], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 21, "seek": 5484, "start": 72.76, "end": 77.0, "text": " And then we're going to move on to talking about relational data in general.", "tokens": [51260, 400, 550, 321, 434, 516, 281, 1286, 322, 281, 1417, 466, 38444, 1412, 294, 2674, 13, 51472], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 22, "seek": 5484, "start": 77.0, "end": 81.60000000000001, "text": " And then finally end up with immutable relational data and some techniques for dealing with", "tokens": [51472, 400, 550, 2721, 917, 493, 365, 3397, 32148, 38444, 1412, 293, 512, 7512, 337, 6260, 365, 51702], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 23, "seek": 5484, "start": 81.60000000000001, "end": 82.60000000000001, "text": " it.", "tokens": [51702, 309, 13, 51752], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 24, "seek": 5484, "start": 82.60000000000001, "end": 83.60000000000001, "text": " Okay.", "tokens": [51752, 1033, 13, 51802], "temperature": 0.0, "avg_logprob": -0.13858306665214704, "compression_ratio": 1.8691275167785235, "no_speech_prob": 0.0008557251421734691}, {"id": 25, "seek": 8360, "start": 83.6, "end": 85.36, "text": " Let's start with state synchronization.", "tokens": [50364, 961, 311, 722, 365, 1785, 19331, 2144, 13, 50452], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 26, "seek": 8360, "start": 85.36, "end": 86.36, "text": " Okay.", "tokens": [50452, 1033, 13, 50502], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 27, "seek": 8360, "start": 86.36, "end": 89.67999999999999, "text": " So some time ago, a friend of mine tweeted me.", "tokens": [50502, 407, 512, 565, 2057, 11, 257, 1277, 295, 3892, 25646, 385, 13, 50668], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 28, "seek": 8360, "start": 89.67999999999999, "end": 96.08, "text": " They said, hey, RT Feldman, remember that time in college when we were late to a potluck?", "tokens": [50668, 814, 848, 11, 4177, 11, 21797, 42677, 1601, 11, 1604, 300, 565, 294, 3859, 562, 321, 645, 3469, 281, 257, 1847, 75, 1134, 30, 50988], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 29, "seek": 8360, "start": 96.08, "end": 103.11999999999999, "text": " And we went to a Wendy's drive-thru and got 20 orders of $1 five-piece nuggets?", "tokens": [50988, 400, 321, 1437, 281, 257, 21850, 311, 3332, 12, 392, 894, 293, 658, 945, 9470, 295, 1848, 16, 1732, 12, 15281, 42663, 30, 51340], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 30, "seek": 8360, "start": 103.11999999999999, "end": 106.44, "text": " So we showed up to the potluck with 100 chicken nuggets, LOL.", "tokens": [51340, 407, 321, 4712, 493, 281, 264, 1847, 75, 1134, 365, 2319, 4662, 42663, 11, 15086, 13, 51506], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 31, "seek": 8360, "start": 106.44, "end": 107.44, "text": " That's a true story.", "tokens": [51506, 663, 311, 257, 2074, 1657, 13, 51556], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 32, "seek": 8360, "start": 107.44, "end": 108.44, "text": " That did happen in college.", "tokens": [51556, 663, 630, 1051, 294, 3859, 13, 51606], "temperature": 0.0, "avg_logprob": -0.23144995647927988, "compression_ratio": 1.4860557768924303, "no_speech_prob": 0.0013248531613498926}, {"id": 33, "seek": 10844, "start": 108.44, "end": 114.0, "text": " So I saw this, I was like, oh, yeah, that was a good time.", "tokens": [50364, 407, 286, 1866, 341, 11, 286, 390, 411, 11, 1954, 11, 1338, 11, 300, 390, 257, 665, 565, 13, 50642], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 34, "seek": 10844, "start": 114.0, "end": 117.75999999999999, "text": " So I like, you know, went to the like button and hit like.", "tokens": [50642, 407, 286, 411, 11, 291, 458, 11, 1437, 281, 264, 411, 2960, 293, 2045, 411, 13, 50830], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 35, "seek": 10844, "start": 117.75999999999999, "end": 118.75999999999999, "text": " But I got an error.", "tokens": [50830, 583, 286, 658, 364, 6713, 13, 50880], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 36, "seek": 10844, "start": 118.75999999999999, "end": 120.28, "text": " I was like, no, you can't do that.", "tokens": [50880, 286, 390, 411, 11, 572, 11, 291, 393, 380, 360, 300, 13, 50956], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 37, "seek": 10844, "start": 120.28, "end": 121.28, "text": " It's like, what?", "tokens": [50956, 467, 311, 411, 11, 437, 30, 51006], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 38, "seek": 10844, "start": 121.28, "end": 122.28, "text": " No, no, for real.", "tokens": [51006, 883, 11, 572, 11, 337, 957, 13, 51056], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 39, "seek": 10844, "start": 122.28, "end": 123.28, "text": " I like it.", "tokens": [51056, 286, 411, 309, 13, 51106], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 40, "seek": 10844, "start": 123.28, "end": 124.28, "text": " It's good times.", "tokens": [51106, 467, 311, 665, 1413, 13, 51156], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 41, "seek": 10844, "start": 124.28, "end": 126.28, "text": " So I hit the button again.", "tokens": [51156, 407, 286, 2045, 264, 2960, 797, 13, 51256], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 42, "seek": 10844, "start": 126.28, "end": 127.28, "text": " It's error.", "tokens": [51256, 467, 311, 6713, 13, 51306], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 43, "seek": 10844, "start": 127.28, "end": 129.16, "text": " Oh, come on.", "tokens": [51306, 876, 11, 808, 322, 13, 51400], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 44, "seek": 10844, "start": 129.16, "end": 133.92, "text": " So as it turned out, what had happened was my friend, after posting this, decided this", "tokens": [51400, 407, 382, 309, 3574, 484, 11, 437, 632, 2011, 390, 452, 1277, 11, 934, 15978, 341, 11, 3047, 341, 51638], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 45, "seek": 10844, "start": 133.92, "end": 137.52, "text": " was maybe not something the entire internet needed to know.", "tokens": [51638, 390, 1310, 406, 746, 264, 2302, 4705, 2978, 281, 458, 13, 51818], "temperature": 0.0, "avg_logprob": -0.22686247857625053, "compression_ratio": 1.778688524590164, "no_speech_prob": 0.009706229902803898}, {"id": 46, "seek": 13752, "start": 137.52, "end": 139.24, "text": " And so they ended up deleting it.", "tokens": [50364, 400, 370, 436, 4590, 493, 48946, 309, 13, 50450], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 47, "seek": 13752, "start": 139.24, "end": 145.4, "text": " And so they deleted it sometime between when I saw it and when I attempted to click like,", "tokens": [50450, 400, 370, 436, 22981, 309, 15053, 1296, 562, 286, 1866, 309, 293, 562, 286, 18997, 281, 2052, 411, 11, 50758], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 48, "seek": 13752, "start": 145.4, "end": 147.20000000000002, "text": " resulting in the error.", "tokens": [50758, 16505, 294, 264, 6713, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 49, "seek": 13752, "start": 147.20000000000002, "end": 151.36, "text": " So this is essentially a sort of a mismatch between the client and the server.", "tokens": [50848, 407, 341, 307, 4476, 257, 1333, 295, 257, 23220, 852, 1296, 264, 6423, 293, 264, 7154, 13, 51056], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 50, "seek": 13752, "start": 151.36, "end": 154.88, "text": " So my client, the web browser, was like, oh, yeah, this tweet totally exists.", "tokens": [51056, 407, 452, 6423, 11, 264, 3670, 11185, 11, 390, 411, 11, 1954, 11, 1338, 11, 341, 15258, 3879, 8198, 13, 51232], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 51, "seek": 13752, "start": 154.88, "end": 155.88, "text": " Sure.", "tokens": [51232, 4894, 13, 51282], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 52, "seek": 13752, "start": 155.88, "end": 159.04000000000002, "text": " Here's a nice lovely like button in case you'd like to like it.", "tokens": [51282, 1692, 311, 257, 1481, 7496, 411, 2960, 294, 1389, 291, 1116, 411, 281, 411, 309, 13, 51440], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 53, "seek": 13752, "start": 159.04000000000002, "end": 161.72, "text": " But unfortunately, the server had a different idea, the server was like, oh, that's gone.", "tokens": [51440, 583, 7015, 11, 264, 7154, 632, 257, 819, 1558, 11, 264, 7154, 390, 411, 11, 1954, 11, 300, 311, 2780, 13, 51574], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 54, "seek": 13752, "start": 161.72, "end": 162.72, "text": " What are you talking about?", "tokens": [51574, 708, 366, 291, 1417, 466, 30, 51624], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 55, "seek": 13752, "start": 162.72, "end": 164.84, "text": " Like, that's been deleted for a while.", "tokens": [51624, 1743, 11, 300, 311, 668, 22981, 337, 257, 1339, 13, 51730], "temperature": 0.0, "avg_logprob": -0.1678201335750214, "compression_ratio": 1.7818791946308725, "no_speech_prob": 0.0169050395488739}, {"id": 56, "seek": 16484, "start": 164.84, "end": 169.24, "text": " So there was this mismatch between the two, and that led to the error.", "tokens": [50364, 407, 456, 390, 341, 23220, 852, 1296, 264, 732, 11, 293, 300, 4684, 281, 264, 6713, 13, 50584], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 57, "seek": 16484, "start": 169.24, "end": 171.56, "text": " So this is a form of synchronization problem.", "tokens": [50584, 407, 341, 307, 257, 1254, 295, 19331, 2144, 1154, 13, 50700], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 58, "seek": 16484, "start": 171.56, "end": 175.96, "text": " Like, we had a synchronization mismatch between the client and the server.", "tokens": [50700, 1743, 11, 321, 632, 257, 19331, 2144, 23220, 852, 1296, 264, 6423, 293, 264, 7154, 13, 50920], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 59, "seek": 16484, "start": 175.96, "end": 180.48000000000002, "text": " And when we have multiple sources of truth, we sort of need to synchronize them or else", "tokens": [50920, 400, 562, 321, 362, 3866, 7139, 295, 3494, 11, 321, 1333, 295, 643, 281, 19331, 1125, 552, 420, 1646, 51146], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 60, "seek": 16484, "start": 180.48000000000002, "end": 182.12, "text": " we can get this type of error.", "tokens": [51146, 321, 393, 483, 341, 2010, 295, 6713, 13, 51228], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 61, "seek": 16484, "start": 182.12, "end": 186.12, "text": " Now if we have multiple sources of truth, such as the client and the server, and they", "tokens": [51228, 823, 498, 321, 362, 3866, 7139, 295, 3494, 11, 1270, 382, 264, 6423, 293, 264, 7154, 11, 293, 436, 51428], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 62, "seek": 16484, "start": 186.12, "end": 190.68, "text": " agree like if the client says the tweet exists and the server also believes that it exists,", "tokens": [51428, 3986, 411, 498, 264, 6423, 1619, 264, 15258, 8198, 293, 264, 7154, 611, 12307, 300, 309, 8198, 11, 51656], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 63, "seek": 16484, "start": 190.68, "end": 191.68, "text": " that's okay.", "tokens": [51656, 300, 311, 1392, 13, 51706], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 64, "seek": 16484, "start": 191.68, "end": 192.68, "text": " That's no problem.", "tokens": [51706, 663, 311, 572, 1154, 13, 51756], "temperature": 0.0, "avg_logprob": -0.14318468991447897, "compression_ratio": 2.03125, "no_speech_prob": 0.0010320785222575068}, {"id": 65, "seek": 19268, "start": 192.88, "end": 196.24, "text": " The typical thing, which is why, you know, I was surprised to see this error and not", "tokens": [50374, 440, 7476, 551, 11, 597, 307, 983, 11, 291, 458, 11, 286, 390, 6100, 281, 536, 341, 6713, 293, 406, 50542], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 66, "seek": 19268, "start": 196.24, "end": 199.92000000000002, "text": " just like, oh, yeah, another one of these just happens to me constantly.", "tokens": [50542, 445, 411, 11, 1954, 11, 1338, 11, 1071, 472, 295, 613, 445, 2314, 281, 385, 6460, 13, 50726], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 67, "seek": 19268, "start": 199.92000000000002, "end": 204.48000000000002, "text": " Usually despite the fact that there are multiple sources of truth, they're staying in sync", "tokens": [50726, 11419, 7228, 264, 1186, 300, 456, 366, 3866, 7139, 295, 3494, 11, 436, 434, 7939, 294, 20271, 50954], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 68, "seek": 19268, "start": 204.48000000000002, "end": 205.8, "text": " so it doesn't actually matter.", "tokens": [50954, 370, 309, 1177, 380, 767, 1871, 13, 51020], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 69, "seek": 19268, "start": 205.8, "end": 208.8, "text": " It doesn't sort of affect me.", "tokens": [51020, 467, 1177, 380, 1333, 295, 3345, 385, 13, 51170], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 70, "seek": 19268, "start": 208.8, "end": 210.68, "text": " It's also fine if they both agree that it's gone.", "tokens": [51170, 467, 311, 611, 2489, 498, 436, 1293, 3986, 300, 309, 311, 2780, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 71, "seek": 19268, "start": 210.68, "end": 214.12, "text": " Like, eventually I refresh the page and then I saw, oh, the thing's gone.", "tokens": [51264, 1743, 11, 4728, 286, 15134, 264, 3028, 293, 550, 286, 1866, 11, 1954, 11, 264, 551, 311, 2780, 13, 51436], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 72, "seek": 19268, "start": 214.12, "end": 216.44, "text": " So they got synced up and then it was again, no problem.", "tokens": [51436, 407, 436, 658, 5451, 1232, 493, 293, 550, 309, 390, 797, 11, 572, 1154, 13, 51552], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 73, "seek": 19268, "start": 216.44, "end": 219.64000000000001, "text": " I was back to being in sync with the server and the multiple sources of truth were not", "tokens": [51552, 286, 390, 646, 281, 885, 294, 20271, 365, 264, 7154, 293, 264, 3866, 7139, 295, 3494, 645, 406, 51712], "temperature": 0.0, "avg_logprob": -0.20774670390339642, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.000755205110181123}, {"id": 74, "seek": 21964, "start": 219.67999999999998, "end": 223.16, "text": " really causing any problems there for me.", "tokens": [50366, 534, 9853, 604, 2740, 456, 337, 385, 13, 50540], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 75, "seek": 21964, "start": 223.16, "end": 226.72, "text": " So basically when multiple sources of truth remain synchronized, it's okay.", "tokens": [50540, 407, 1936, 562, 3866, 7139, 295, 3494, 6222, 19331, 1602, 11, 309, 311, 1392, 13, 50718], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 76, "seek": 21964, "start": 226.72, "end": 230.51999999999998, "text": " There's sort of no symptoms, no problems that the end user is faced with.", "tokens": [50718, 821, 311, 1333, 295, 572, 8332, 11, 572, 2740, 300, 264, 917, 4195, 307, 11446, 365, 13, 50908], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 77, "seek": 21964, "start": 230.51999999999998, "end": 236.56, "text": " However, when they get out of sync, as happened to me in the tweet, then that causes a problem.", "tokens": [50908, 2908, 11, 562, 436, 483, 484, 295, 20271, 11, 382, 2011, 281, 385, 294, 264, 15258, 11, 550, 300, 7700, 257, 1154, 13, 51210], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 78, "seek": 21964, "start": 236.56, "end": 240.2, "text": " So single sources of truth sort of don't have this issue.", "tokens": [51210, 407, 2167, 7139, 295, 3494, 1333, 295, 500, 380, 362, 341, 2734, 13, 51392], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 79, "seek": 21964, "start": 240.2, "end": 243.51999999999998, "text": " There's no synchronization because there's nothing to synchronize.", "tokens": [51392, 821, 311, 572, 19331, 2144, 570, 456, 311, 1825, 281, 19331, 1125, 13, 51558], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 80, "seek": 21964, "start": 243.51999999999998, "end": 245.48, "text": " It's just one source of truth.", "tokens": [51558, 467, 311, 445, 472, 4009, 295, 3494, 13, 51656], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 81, "seek": 21964, "start": 245.48, "end": 249.32, "text": " So this entire class of errors goes away, which is one of the things that's nice about", "tokens": [51656, 407, 341, 2302, 1508, 295, 13603, 1709, 1314, 11, 597, 307, 472, 295, 264, 721, 300, 311, 1481, 466, 51848], "temperature": 0.0, "avg_logprob": -0.11133366200461317, "compression_ratio": 1.833910034602076, "no_speech_prob": 0.000803984294179827}, {"id": 82, "seek": 24932, "start": 249.32, "end": 252.23999999999998, "text": " having a single source of truth.", "tokens": [50364, 1419, 257, 2167, 4009, 295, 3494, 13, 50510], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 83, "seek": 24932, "start": 252.23999999999998, "end": 255.4, "text": " So synchronization is sort of error prone.", "tokens": [50510, 407, 19331, 2144, 307, 1333, 295, 6713, 25806, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 84, "seek": 24932, "start": 255.4, "end": 256.88, "text": " This is one of the problems with it.", "tokens": [50668, 639, 307, 472, 295, 264, 2740, 365, 309, 13, 50742], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 85, "seek": 24932, "start": 256.88, "end": 259.08, "text": " As we saw, this is sort of a case in point.", "tokens": [50742, 1018, 321, 1866, 11, 341, 307, 1333, 295, 257, 1389, 294, 935, 13, 50852], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 86, "seek": 24932, "start": 259.08, "end": 261.4, "text": " This is a synchronization error.", "tokens": [50852, 639, 307, 257, 19331, 2144, 6713, 13, 50968], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 87, "seek": 24932, "start": 261.4, "end": 264.4, "text": " And these sort of things can happen all the time when you have to synchronize two pieces", "tokens": [50968, 400, 613, 1333, 295, 721, 393, 1051, 439, 264, 565, 562, 291, 362, 281, 19331, 1125, 732, 3755, 51118], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 88, "seek": 24932, "start": 264.4, "end": 267.44, "text": " of information because there are multiple sources of truth.", "tokens": [51118, 295, 1589, 570, 456, 366, 3866, 7139, 295, 3494, 13, 51270], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 89, "seek": 24932, "start": 267.44, "end": 272.84, "text": " Which sort of begs the question, is it necessarily avoidable to have multiple sources of truth?", "tokens": [51270, 3013, 1333, 295, 4612, 82, 264, 1168, 11, 307, 309, 4725, 5042, 712, 281, 362, 3866, 7139, 295, 3494, 30, 51540], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 90, "seek": 24932, "start": 272.84, "end": 276.4, "text": " Sometimes do we have to have multiple sources of truth even if we would prefer to have a", "tokens": [51540, 4803, 360, 321, 362, 281, 362, 3866, 7139, 295, 3494, 754, 498, 321, 576, 4382, 281, 362, 257, 51718], "temperature": 0.0, "avg_logprob": -0.14063926871496302, "compression_ratio": 1.9588014981273407, "no_speech_prob": 0.00023773415887262672}, {"id": 91, "seek": 27640, "start": 276.4, "end": 279.76, "text": " single source of truth and not have to synchronize anything?", "tokens": [50364, 2167, 4009, 295, 3494, 293, 406, 362, 281, 19331, 1125, 1340, 30, 50532], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 92, "seek": 27640, "start": 279.76, "end": 285.2, "text": " Let's take this client server app as an example and see if we can arrange things such that", "tokens": [50532, 961, 311, 747, 341, 6423, 7154, 724, 382, 364, 1365, 293, 536, 498, 321, 393, 9424, 721, 1270, 300, 50804], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 93, "seek": 27640, "start": 285.2, "end": 287.2, "text": " we have a single source of truth.", "tokens": [50804, 321, 362, 257, 2167, 4009, 295, 3494, 13, 50904], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 94, "seek": 27640, "start": 287.2, "end": 288.2, "text": " This is our goal.", "tokens": [50904, 639, 307, 527, 3387, 13, 50954], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 95, "seek": 27640, "start": 288.2, "end": 291.76, "text": " We're going to have a client and a server, multiple clients and a server, and we're going", "tokens": [50954, 492, 434, 516, 281, 362, 257, 6423, 293, 257, 7154, 11, 3866, 6982, 293, 257, 7154, 11, 293, 321, 434, 516, 51132], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 96, "seek": 27640, "start": 291.76, "end": 294.64, "text": " to have a single source of truth that's shared between all of them such that there's nothing", "tokens": [51132, 281, 362, 257, 2167, 4009, 295, 3494, 300, 311, 5507, 1296, 439, 295, 552, 1270, 300, 456, 311, 1825, 51276], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 97, "seek": 27640, "start": 294.64, "end": 296.71999999999997, "text": " that we need to synchronize.", "tokens": [51276, 300, 321, 643, 281, 19331, 1125, 13, 51380], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 98, "seek": 27640, "start": 296.71999999999997, "end": 299.52, "text": " So to do this, probably what we're going to have to do is say, okay, the server is going", "tokens": [51380, 407, 281, 360, 341, 11, 1391, 437, 321, 434, 516, 281, 362, 281, 360, 307, 584, 11, 1392, 11, 264, 7154, 307, 516, 51520], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 99, "seek": 27640, "start": 299.52, "end": 301.08, "text": " to have to take care of rendering the UI.", "tokens": [51520, 281, 362, 281, 747, 1127, 295, 22407, 264, 15682, 13, 51598], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 100, "seek": 27640, "start": 301.08, "end": 304.47999999999996, "text": " That's going to have to be the source of truth because that's where the database lives.", "tokens": [51598, 663, 311, 516, 281, 362, 281, 312, 264, 4009, 295, 3494, 570, 300, 311, 689, 264, 8149, 2909, 13, 51768], "temperature": 0.0, "avg_logprob": -0.17487967520049125, "compression_ratio": 2.12751677852349, "no_speech_prob": 0.001987379277125001}, {"id": 101, "seek": 30448, "start": 304.84000000000003, "end": 310.32, "text": " I mean, if anyone ever disagrees with the database about what is true, the database is", "tokens": [50382, 286, 914, 11, 498, 2878, 1562, 10414, 4856, 365, 264, 8149, 466, 437, 307, 2074, 11, 264, 8149, 307, 50656], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 102, "seek": 30448, "start": 310.32, "end": 311.32, "text": " going to be right.", "tokens": [50656, 516, 281, 312, 558, 13, 50706], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 103, "seek": 30448, "start": 311.32, "end": 315.20000000000005, "text": " So we're going to have the server render HTML like it's 15 years ago and it's just going", "tokens": [50706, 407, 321, 434, 516, 281, 362, 264, 7154, 15529, 17995, 411, 309, 311, 2119, 924, 2057, 293, 309, 311, 445, 516, 50900], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 104, "seek": 30448, "start": 315.20000000000005, "end": 319.04, "text": " to send it directly to the client on every single user interaction.", "tokens": [50900, 281, 2845, 309, 3838, 281, 264, 6423, 322, 633, 2167, 4195, 9285, 13, 51092], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 105, "seek": 30448, "start": 319.04, "end": 322.20000000000005, "text": " Like I click anything and the server is going to render some HTML and send it to the client.", "tokens": [51092, 1743, 286, 2052, 1340, 293, 264, 7154, 307, 516, 281, 15529, 512, 17995, 293, 2845, 309, 281, 264, 6423, 13, 51250], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 106, "seek": 30448, "start": 322.20000000000005, "end": 327.28000000000003, "text": " So that way the server is the single source of truth, kind of.", "tokens": [51250, 407, 300, 636, 264, 7154, 307, 264, 2167, 4009, 295, 3494, 11, 733, 295, 13, 51504], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 107, "seek": 30448, "start": 327.28000000000003, "end": 331.48, "text": " Problem is that sending stuff from the server to the client takes time.", "tokens": [51504, 11676, 307, 300, 7750, 1507, 490, 264, 7154, 281, 264, 6423, 2516, 565, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 108, "seek": 30448, "start": 331.48, "end": 333.92, "text": " It doesn't just like appear on my screen instantaneously.", "tokens": [51714, 467, 1177, 380, 445, 411, 4204, 322, 452, 2568, 9836, 13131, 13, 51836], "temperature": 0.0, "avg_logprob": -0.19294412988815865, "compression_ratio": 1.8327759197324414, "no_speech_prob": 0.11268571764230728}, {"id": 109, "seek": 33392, "start": 334.36, "end": 338.64000000000004, "text": " There's like networks and packets and like latency and things like that.", "tokens": [50386, 821, 311, 411, 9590, 293, 30364, 293, 411, 27043, 293, 721, 411, 300, 13, 50600], "temperature": 0.0, "avg_logprob": -0.13823365795519926, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.005058359354734421}, {"id": 110, "seek": 33392, "start": 338.64000000000004, "end": 342.96000000000004, "text": " And in fact, there's so much of that that it's like perceptible, which in turn means", "tokens": [50600, 400, 294, 1186, 11, 456, 311, 370, 709, 295, 300, 300, 309, 311, 411, 43276, 964, 11, 597, 294, 1261, 1355, 50816], "temperature": 0.0, "avg_logprob": -0.13823365795519926, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.005058359354734421}, {"id": 111, "seek": 33392, "start": 342.96000000000004, "end": 346.64000000000004, "text": " that it's entirely possible that over the course of that network transmission while", "tokens": [50816, 300, 309, 311, 7696, 1944, 300, 670, 264, 1164, 295, 300, 3209, 11574, 1339, 51000], "temperature": 0.0, "avg_logprob": -0.13823365795519926, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.005058359354734421}, {"id": 112, "seek": 33392, "start": 346.64000000000004, "end": 352.24, "text": " the currently up-to-date perfect single source of truth UI is headed towards my browser,", "tokens": [51000, 264, 4362, 493, 12, 1353, 12, 17393, 2176, 2167, 4009, 295, 3494, 15682, 307, 12798, 3030, 452, 11185, 11, 51280], "temperature": 0.0, "avg_logprob": -0.13823365795519926, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.005058359354734421}, {"id": 113, "seek": 33392, "start": 352.24, "end": 356.92, "text": " my friend deletes the tweet, and now I am once again out of sync.", "tokens": [51280, 452, 1277, 1103, 37996, 264, 15258, 11, 293, 586, 286, 669, 1564, 797, 484, 295, 20271, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13823365795519926, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.005058359354734421}, {"id": 114, "seek": 33392, "start": 356.92, "end": 360.72, "text": " So even trying as hard as we possibly can to have a single source of truth with a client", "tokens": [51514, 407, 754, 1382, 382, 1152, 382, 321, 6264, 393, 281, 362, 257, 2167, 4009, 295, 3494, 365, 257, 6423, 51704], "temperature": 0.0, "avg_logprob": -0.13823365795519926, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.005058359354734421}, {"id": 115, "seek": 33392, "start": 360.72, "end": 363.8, "text": " server application, it's just not possible.", "tokens": [51704, 7154, 3861, 11, 309, 311, 445, 406, 1944, 13, 51858], "temperature": 0.0, "avg_logprob": -0.13823365795519926, "compression_ratio": 1.7811447811447811, "no_speech_prob": 0.005058359354734421}, {"id": 116, "seek": 36380, "start": 363.8, "end": 367.72, "text": " You cannot have a single source of truth when you've got a client and a server.", "tokens": [50364, 509, 2644, 362, 257, 2167, 4009, 295, 3494, 562, 291, 600, 658, 257, 6423, 293, 257, 7154, 13, 50560], "temperature": 0.0, "avg_logprob": -0.16279776342983904, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.0007094573811627924}, {"id": 117, "seek": 36380, "start": 367.72, "end": 370.36, "text": " You sort of must have synchronization.", "tokens": [50560, 509, 1333, 295, 1633, 362, 19331, 2144, 13, 50692], "temperature": 0.0, "avg_logprob": -0.16279776342983904, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.0007094573811627924}, {"id": 118, "seek": 36380, "start": 370.36, "end": 375.40000000000003, "text": " It's completely unavoidable by the nature of this architecture, which is kind of a bummer", "tokens": [50692, 467, 311, 2584, 36541, 17079, 712, 538, 264, 3687, 295, 341, 9482, 11, 597, 307, 733, 295, 257, 13309, 936, 50944], "temperature": 0.0, "avg_logprob": -0.16279776342983904, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.0007094573811627924}, {"id": 119, "seek": 36380, "start": 375.40000000000003, "end": 378.16, "text": " because hey, clients and servers are nice.", "tokens": [50944, 570, 4177, 11, 6982, 293, 15909, 366, 1481, 13, 51082], "temperature": 0.0, "avg_logprob": -0.16279776342983904, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.0007094573811627924}, {"id": 120, "seek": 36380, "start": 378.16, "end": 381.32, "text": " So we're sort of stuck synchronizing them.", "tokens": [51082, 407, 321, 434, 1333, 295, 5541, 19331, 3319, 552, 13, 51240], "temperature": 0.0, "avg_logprob": -0.16279776342983904, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.0007094573811627924}, {"id": 121, "seek": 36380, "start": 381.32, "end": 385.28000000000003, "text": " And again, even if it is unavoidable, synchronization is still error-prone.", "tokens": [51240, 400, 797, 11, 754, 498, 309, 307, 36541, 17079, 712, 11, 19331, 2144, 307, 920, 6713, 12, 1424, 546, 13, 51438], "temperature": 0.0, "avg_logprob": -0.16279776342983904, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.0007094573811627924}, {"id": 122, "seek": 36380, "start": 385.28000000000003, "end": 388.36, "text": " We're still going to have to deal with synchronization errors like this.", "tokens": [51438, 492, 434, 920, 516, 281, 362, 281, 2028, 365, 19331, 2144, 13603, 411, 341, 13, 51592], "temperature": 0.0, "avg_logprob": -0.16279776342983904, "compression_ratio": 1.7170542635658914, "no_speech_prob": 0.0007094573811627924}, {"id": 123, "seek": 38836, "start": 388.36, "end": 395.28000000000003, "text": " So we'd sort of like to handle these synchronization errors in as graceful a way as we can.", "tokens": [50364, 407, 321, 1116, 1333, 295, 411, 281, 4813, 613, 19331, 2144, 13603, 294, 382, 10042, 906, 257, 636, 382, 321, 393, 13, 50710], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 124, "seek": 38836, "start": 395.28000000000003, "end": 397.88, "text": " And there's sort of different levels to this.", "tokens": [50710, 400, 456, 311, 1333, 295, 819, 4358, 281, 341, 13, 50840], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 125, "seek": 38836, "start": 397.88, "end": 400.64, "text": " So one is basically just like fail without feedback.", "tokens": [50840, 407, 472, 307, 1936, 445, 411, 3061, 1553, 5824, 13, 50978], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 126, "seek": 38836, "start": 400.64, "end": 404.84000000000003, "text": " So this is the absolute worst, which is to say the tweet gets deleted and I hit like", "tokens": [50978, 407, 341, 307, 264, 8236, 5855, 11, 597, 307, 281, 584, 264, 15258, 2170, 22981, 293, 286, 2045, 411, 51188], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 127, "seek": 38836, "start": 404.84000000000003, "end": 406.28000000000003, "text": " and then just nothing happens.", "tokens": [51188, 293, 550, 445, 1825, 2314, 13, 51260], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 128, "seek": 38836, "start": 406.28000000000003, "end": 408.56, "text": " I'm just like, what?", "tokens": [51260, 286, 478, 445, 411, 11, 437, 30, 51374], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 129, "seek": 38836, "start": 408.56, "end": 411.96000000000004, "text": " Or you can do what's called optimistic updates where you're just like, I'm going to update", "tokens": [51374, 1610, 291, 393, 360, 437, 311, 1219, 19397, 9205, 689, 291, 434, 445, 411, 11, 286, 478, 516, 281, 5623, 51544], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 130, "seek": 38836, "start": 411.96000000000004, "end": 415.52000000000004, "text": " the client state and assume it works out on the server.", "tokens": [51544, 264, 6423, 1785, 293, 6552, 309, 1985, 484, 322, 264, 7154, 13, 51722], "temperature": 0.0, "avg_logprob": -0.17753338044689548, "compression_ratio": 1.6631578947368422, "no_speech_prob": 0.000829557771794498}, {"id": 131, "seek": 41552, "start": 415.52, "end": 419.2, "text": " This can be even worse and I've been really, really badly burned by this in the past.", "tokens": [50364, 639, 393, 312, 754, 5324, 293, 286, 600, 668, 534, 11, 534, 13425, 13490, 538, 341, 294, 264, 1791, 13, 50548], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 132, "seek": 41552, "start": 419.2, "end": 421.28, "text": " That's basically where you hit like and it does the animation.", "tokens": [50548, 663, 311, 1936, 689, 291, 2045, 411, 293, 309, 775, 264, 9603, 13, 50652], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 133, "seek": 41552, "start": 421.28, "end": 422.71999999999997, "text": " It's like, yeah, everything worked.", "tokens": [50652, 467, 311, 411, 11, 1338, 11, 1203, 2732, 13, 50724], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 134, "seek": 41552, "start": 422.71999999999997, "end": 424.52, "text": " And then later on it just was gone.", "tokens": [50724, 400, 550, 1780, 322, 309, 445, 390, 2780, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 135, "seek": 41552, "start": 424.52, "end": 425.52, "text": " What?", "tokens": [50814, 708, 30, 50864], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 136, "seek": 41552, "start": 425.52, "end": 428.79999999999995, "text": " And then, you know, so once it actually gets in sync, you find out much after the fact", "tokens": [50864, 400, 550, 11, 291, 458, 11, 370, 1564, 309, 767, 2170, 294, 20271, 11, 291, 915, 484, 709, 934, 264, 1186, 51028], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 137, "seek": 41552, "start": 428.79999999999995, "end": 429.79999999999995, "text": " that there was a problem.", "tokens": [51028, 300, 456, 390, 257, 1154, 13, 51078], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 138, "seek": 41552, "start": 429.79999999999995, "end": 434.24, "text": " Having had no idea at the time that there was anything wrong, which would have at least", "tokens": [51078, 10222, 632, 572, 1558, 412, 264, 565, 300, 456, 390, 1340, 2085, 11, 597, 576, 362, 412, 1935, 51300], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 139, "seek": 41552, "start": 434.24, "end": 435.24, "text": " given me a clue.", "tokens": [51300, 2212, 385, 257, 13602, 13, 51350], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 140, "seek": 41552, "start": 435.24, "end": 439.35999999999996, "text": " Like, yeah, I want to try hitting like again in case maybe the server is down or something.", "tokens": [51350, 1743, 11, 1338, 11, 286, 528, 281, 853, 8850, 411, 797, 294, 1389, 1310, 264, 7154, 307, 760, 420, 746, 13, 51556], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 141, "seek": 41552, "start": 439.35999999999996, "end": 444.35999999999996, "text": " So failing without feedback is sort of the worst way to handle a synchronization error.", "tokens": [51556, 407, 18223, 1553, 5824, 307, 1333, 295, 264, 5855, 636, 281, 4813, 257, 19331, 2144, 6713, 13, 51806], "temperature": 0.0, "avg_logprob": -0.16565525170528528, "compression_ratio": 1.723756906077348, "no_speech_prob": 0.018543453887104988}, {"id": 142, "seek": 44436, "start": 444.36, "end": 447.8, "text": " We can do a little bit better by at least informing the user that some sort of error", "tokens": [50364, 492, 393, 360, 257, 707, 857, 1101, 538, 412, 1935, 43969, 264, 4195, 300, 512, 1333, 295, 6713, 50536], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 143, "seek": 44436, "start": 447.8, "end": 448.8, "text": " occurred.", "tokens": [50536, 11068, 13, 50586], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 144, "seek": 44436, "start": 448.8, "end": 454.0, "text": " Like error, even if you're saying like error 500, that's at least giving me some clue that", "tokens": [50586, 1743, 6713, 11, 754, 498, 291, 434, 1566, 411, 6713, 5923, 11, 300, 311, 412, 1935, 2902, 385, 512, 13602, 300, 50846], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 145, "seek": 44436, "start": 454.0, "end": 458.44, "text": " something went wrong so I can, you know, react accordingly and not be surprised later on", "tokens": [50846, 746, 1437, 2085, 370, 286, 393, 11, 291, 458, 11, 4515, 19717, 293, 406, 312, 6100, 1780, 322, 51068], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 146, "seek": 44436, "start": 458.44, "end": 464.36, "text": " to discover that there was this mismatch between my expectation and what actually happened.", "tokens": [51068, 281, 4411, 300, 456, 390, 341, 23220, 852, 1296, 452, 14334, 293, 437, 767, 2011, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 147, "seek": 44436, "start": 464.36, "end": 466.8, "text": " Even better than that is to explain the problem.", "tokens": [51364, 2754, 1101, 813, 300, 307, 281, 2903, 264, 1154, 13, 51486], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 148, "seek": 44436, "start": 466.8, "end": 471.08000000000004, "text": " Something like this tweet was deleted before you could like it or maybe something more", "tokens": [51486, 6595, 411, 341, 15258, 390, 22981, 949, 291, 727, 411, 309, 420, 1310, 746, 544, 51700], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 149, "seek": 44436, "start": 471.08000000000004, "end": 472.44, "text": " concise than that.", "tokens": [51700, 44882, 813, 300, 13, 51768], "temperature": 0.0, "avg_logprob": -0.14361717661873238, "compression_ratio": 1.7251655629139073, "no_speech_prob": 0.06750382483005524}, {"id": 150, "seek": 47244, "start": 472.44, "end": 475.71999999999997, "text": " But explaining the problem gives the user a better experience to understand, oh, this", "tokens": [50364, 583, 13468, 264, 1154, 2709, 264, 4195, 257, 1101, 1752, 281, 1223, 11, 1954, 11, 341, 50528], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 151, "seek": 47244, "start": 475.71999999999997, "end": 479.76, "text": " is not just like something went wrong, which could be so many different things and lead", "tokens": [50528, 307, 406, 445, 411, 746, 1437, 2085, 11, 597, 727, 312, 370, 867, 819, 721, 293, 1477, 50730], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 152, "seek": 47244, "start": 479.76, "end": 484.8, "text": " me to take completely futile actions such as hammering on the like button.", "tokens": [50730, 385, 281, 747, 2584, 1877, 794, 5909, 1270, 382, 13017, 278, 322, 264, 411, 2960, 13, 50982], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 153, "seek": 47244, "start": 484.8, "end": 488.36, "text": " If they explain it to me and say this tweet was deleted, then I'm like, oh, okay, well,", "tokens": [50982, 759, 436, 2903, 309, 281, 385, 293, 584, 341, 15258, 390, 22981, 11, 550, 286, 478, 411, 11, 1954, 11, 1392, 11, 731, 11, 51160], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 154, "seek": 47244, "start": 488.36, "end": 490.72, "text": " I'm not going to keep hitting like because it's gone.", "tokens": [51160, 286, 478, 406, 516, 281, 1066, 8850, 411, 570, 309, 311, 2780, 13, 51278], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 155, "seek": 47244, "start": 490.72, "end": 492.8, "text": " I understand that now.", "tokens": [51278, 286, 1223, 300, 586, 13, 51382], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 156, "seek": 47244, "start": 492.8, "end": 496.36, "text": " And the best of all would be to explain and then fix the problem.", "tokens": [51382, 400, 264, 1151, 295, 439, 576, 312, 281, 2903, 293, 550, 3191, 264, 1154, 13, 51560], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 157, "seek": 47244, "start": 496.36, "end": 501.64, "text": " So basically say, hey, this tweet was deleted and then not leave the UI in a state where", "tokens": [51560, 407, 1936, 584, 11, 4177, 11, 341, 15258, 390, 22981, 293, 550, 406, 1856, 264, 15682, 294, 257, 1785, 689, 51824], "temperature": 0.0, "avg_logprob": -0.13554661803775364, "compression_ratio": 1.7917981072555205, "no_speech_prob": 0.0039446474984288216}, {"id": 158, "seek": 50164, "start": 501.64, "end": 504.28, "text": " I can continue to hammer on the like button at all.", "tokens": [50364, 286, 393, 2354, 281, 13017, 322, 264, 411, 2960, 412, 439, 13, 50496], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 159, "seek": 50164, "start": 504.28, "end": 509.52, "text": " Say like this tweet was deleted and then actually synchronize, like change it so it doesn't look", "tokens": [50496, 6463, 411, 341, 15258, 390, 22981, 293, 550, 767, 19331, 1125, 11, 411, 1319, 309, 370, 309, 1177, 380, 574, 50758], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 160, "seek": 50164, "start": 509.52, "end": 514.12, "text": " like this anymore, maybe gray it out, maybe fade out the whole tweet itself.", "tokens": [50758, 411, 341, 3602, 11, 1310, 10855, 309, 484, 11, 1310, 21626, 484, 264, 1379, 15258, 2564, 13, 50988], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 161, "seek": 50164, "start": 514.12, "end": 516.24, "text": " There are any number of things you can do.", "tokens": [50988, 821, 366, 604, 1230, 295, 721, 291, 393, 360, 13, 51094], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 162, "seek": 50164, "start": 516.24, "end": 519.1999999999999, "text": " But these all require different degrees of effort.", "tokens": [51094, 583, 613, 439, 3651, 819, 5310, 295, 4630, 13, 51242], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 163, "seek": 50164, "start": 519.1999999999999, "end": 523.36, "text": " So in order to synchronize while also reporting the error, it's not just that the server has", "tokens": [51242, 407, 294, 1668, 281, 19331, 1125, 1339, 611, 10031, 264, 6713, 11, 309, 311, 406, 445, 300, 264, 7154, 575, 51450], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 164, "seek": 50164, "start": 523.36, "end": 524.84, "text": " to send back a 500.", "tokens": [51450, 281, 2845, 646, 257, 5923, 13, 51524], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 165, "seek": 50164, "start": 524.84, "end": 529.56, "text": " It also has to say, here's what the problem was and also describe how to synchronize the", "tokens": [51524, 467, 611, 575, 281, 584, 11, 510, 311, 437, 264, 1154, 390, 293, 611, 6786, 577, 281, 19331, 1125, 264, 51760], "temperature": 0.0, "avg_logprob": -0.138968901200728, "compression_ratio": 1.7081967213114755, "no_speech_prob": 0.005910166073590517}, {"id": 166, "seek": 52956, "start": 529.56, "end": 534.3199999999999, "text": " state in some particular format, which my client then has to be looking for and know", "tokens": [50364, 1785, 294, 512, 1729, 7877, 11, 597, 452, 6423, 550, 575, 281, 312, 1237, 337, 293, 458, 50602], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 167, "seek": 52956, "start": 534.3199999999999, "end": 537.4, "text": " to say, oh, I understand that I got this additional data.", "tokens": [50602, 281, 584, 11, 1954, 11, 286, 1223, 300, 286, 658, 341, 4497, 1412, 13, 50756], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 168, "seek": 52956, "start": 537.4, "end": 541.4799999999999, "text": " Now I can use this to sort of patch my understanding of the world so that I'm now synchronized", "tokens": [50756, 823, 286, 393, 764, 341, 281, 1333, 295, 9972, 452, 3701, 295, 264, 1002, 370, 300, 286, 478, 586, 19331, 1602, 50960], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 169, "seek": 52956, "start": 541.4799999999999, "end": 542.4799999999999, "text": " with the server.", "tokens": [50960, 365, 264, 7154, 13, 51010], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 170, "seek": 52956, "start": 542.4799999999999, "end": 545.04, "text": " It's a lot more work to deal with that synchronization error.", "tokens": [51010, 467, 311, 257, 688, 544, 589, 281, 2028, 365, 300, 19331, 2144, 6713, 13, 51138], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 171, "seek": 52956, "start": 545.04, "end": 549.28, "text": " And that's not even getting into what happens if we're trying to synchronize across multiple", "tokens": [51138, 400, 300, 311, 406, 754, 1242, 666, 437, 2314, 498, 321, 434, 1382, 281, 19331, 1125, 2108, 3866, 51350], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 172, "seek": 52956, "start": 549.28, "end": 553.16, "text": " clients where it's not clear who the source of truth is and we have to resolve conflicts.", "tokens": [51350, 6982, 689, 309, 311, 406, 1850, 567, 264, 4009, 295, 3494, 307, 293, 321, 362, 281, 14151, 19807, 13, 51544], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 173, "seek": 52956, "start": 553.16, "end": 559.52, "text": " So this can be a lot of work, like a lot, a lot of work if you want to do it right.", "tokens": [51544, 407, 341, 393, 312, 257, 688, 295, 589, 11, 411, 257, 688, 11, 257, 688, 295, 589, 498, 291, 528, 281, 360, 309, 558, 13, 51862], "temperature": 0.0, "avg_logprob": -0.11807005564371745, "compression_ratio": 1.7774390243902438, "no_speech_prob": 0.004069419577717781}, {"id": 174, "seek": 55952, "start": 559.52, "end": 561.36, "text": " First of all, you have to make the updates in the first place.", "tokens": [50364, 2386, 295, 439, 11, 291, 362, 281, 652, 264, 9205, 294, 264, 700, 1081, 13, 50456], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 175, "seek": 55952, "start": 561.36, "end": 564.96, "text": " If we have multiple sources of truth and something changes, we have to go around and propagate", "tokens": [50456, 759, 321, 362, 3866, 7139, 295, 3494, 293, 746, 2962, 11, 321, 362, 281, 352, 926, 293, 48256, 50636], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 176, "seek": 55952, "start": 564.96, "end": 569.0, "text": " those updates to all of the potential places where it could get out of sync.", "tokens": [50636, 729, 9205, 281, 439, 295, 264, 3995, 3190, 689, 309, 727, 483, 484, 295, 20271, 13, 50838], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 177, "seek": 55952, "start": 569.0, "end": 572.3199999999999, "text": " Once we've done that, then we have to detect errors, figure out if things are out of sync", "tokens": [50838, 3443, 321, 600, 1096, 300, 11, 550, 321, 362, 281, 5531, 13603, 11, 2573, 484, 498, 721, 366, 484, 295, 20271, 51004], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 178, "seek": 55952, "start": 572.3199999999999, "end": 574.4, "text": " as they were in the case of this tweet.", "tokens": [51004, 382, 436, 645, 294, 264, 1389, 295, 341, 15258, 13, 51108], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 179, "seek": 55952, "start": 574.4, "end": 577.88, "text": " And then once we've detected them, then we have to potentially resolve conflicts.", "tokens": [51108, 400, 550, 1564, 321, 600, 21896, 552, 11, 550, 321, 362, 281, 7263, 14151, 19807, 13, 51282], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 180, "seek": 55952, "start": 577.88, "end": 582.72, "text": " If we have lack of clarity around who is the source of truth, we have multiple clients", "tokens": [51282, 759, 321, 362, 5011, 295, 16992, 926, 567, 307, 264, 4009, 295, 3494, 11, 321, 362, 3866, 6982, 51524], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 181, "seek": 55952, "start": 582.72, "end": 585.1999999999999, "text": " who made edits at the same time on the same document.", "tokens": [51524, 567, 1027, 41752, 412, 264, 912, 565, 322, 264, 912, 4166, 13, 51648], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 182, "seek": 55952, "start": 585.1999999999999, "end": 586.1999999999999, "text": " How do we resolve those?", "tokens": [51648, 1012, 360, 321, 14151, 729, 30, 51698], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 183, "seek": 55952, "start": 586.1999999999999, "end": 587.4, "text": " There's a lot of techniques for doing that.", "tokens": [51698, 821, 311, 257, 688, 295, 7512, 337, 884, 300, 13, 51758], "temperature": 0.0, "avg_logprob": -0.09497677407613615, "compression_ratio": 1.912536443148688, "no_speech_prob": 0.05661393329501152}, {"id": 184, "seek": 58740, "start": 587.4, "end": 589.6, "text": " That's kind of a whole field of study.", "tokens": [50364, 663, 311, 733, 295, 257, 1379, 2519, 295, 2979, 13, 50474], "temperature": 0.0, "avg_logprob": -0.12161107496781783, "compression_ratio": 1.8206896551724139, "no_speech_prob": 0.009120794013142586}, {"id": 185, "seek": 58740, "start": 589.6, "end": 592.28, "text": " And then finally, ideally, gracefully recovering.", "tokens": [50474, 400, 550, 2721, 11, 22915, 11, 10042, 2277, 29180, 13, 50608], "temperature": 0.0, "avg_logprob": -0.12161107496781783, "compression_ratio": 1.8206896551724139, "no_speech_prob": 0.009120794013142586}, {"id": 186, "seek": 58740, "start": 592.28, "end": 596.9599999999999, "text": " So if we want to give people a good user experience or the best user experience we can in the", "tokens": [50608, 407, 498, 321, 528, 281, 976, 561, 257, 665, 4195, 1752, 420, 264, 1151, 4195, 1752, 321, 393, 294, 264, 50842], "temperature": 0.0, "avg_logprob": -0.12161107496781783, "compression_ratio": 1.8206896551724139, "no_speech_prob": 0.009120794013142586}, {"id": 187, "seek": 58740, "start": 596.9599999999999, "end": 602.72, "text": " face of all of these potential synchronization problems, we'll look into a lot of work.", "tokens": [50842, 1851, 295, 439, 295, 613, 3995, 19331, 2144, 2740, 11, 321, 603, 574, 666, 257, 688, 295, 589, 13, 51130], "temperature": 0.0, "avg_logprob": -0.12161107496781783, "compression_ratio": 1.8206896551724139, "no_speech_prob": 0.009120794013142586}, {"id": 188, "seek": 58740, "start": 602.72, "end": 606.6, "text": " All of which is to say we really want to synchronize as little as possible.", "tokens": [51130, 1057, 295, 597, 307, 281, 584, 321, 534, 528, 281, 19331, 1125, 382, 707, 382, 1944, 13, 51324], "temperature": 0.0, "avg_logprob": -0.12161107496781783, "compression_ratio": 1.8206896551724139, "no_speech_prob": 0.009120794013142586}, {"id": 189, "seek": 58740, "start": 606.6, "end": 609.88, "text": " The more we can have single source of truth and not have to deal with any of this and", "tokens": [51324, 440, 544, 321, 393, 362, 2167, 4009, 295, 3494, 293, 406, 362, 281, 2028, 365, 604, 295, 341, 293, 51488], "temperature": 0.0, "avg_logprob": -0.12161107496781783, "compression_ratio": 1.8206896551724139, "no_speech_prob": 0.009120794013142586}, {"id": 190, "seek": 58740, "start": 609.88, "end": 613.52, "text": " not have to think about it, not have to worry about the errors, not have to spend time handling", "tokens": [51488, 406, 362, 281, 519, 466, 309, 11, 406, 362, 281, 3292, 466, 264, 13603, 11, 406, 362, 281, 3496, 565, 13175, 51670], "temperature": 0.0, "avg_logprob": -0.12161107496781783, "compression_ratio": 1.8206896551724139, "no_speech_prob": 0.009120794013142586}, {"id": 191, "seek": 61352, "start": 613.52, "end": 618.76, "text": " them, resolving conflicts, repairing the state and getting things back in sync, the nicer", "tokens": [50364, 552, 11, 49940, 19807, 11, 46158, 264, 1785, 293, 1242, 721, 646, 294, 20271, 11, 264, 22842, 50626], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 192, "seek": 61352, "start": 618.76, "end": 623.12, "text": " our lives are going to be and more likely, the most likely, the better our user experience", "tokens": [50626, 527, 2909, 366, 516, 281, 312, 293, 544, 3700, 11, 264, 881, 3700, 11, 264, 1101, 527, 4195, 1752, 50844], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 193, "seek": 61352, "start": 623.12, "end": 625.1999999999999, "text": " is going to be as well.", "tokens": [50844, 307, 516, 281, 312, 382, 731, 13, 50948], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 194, "seek": 61352, "start": 625.1999999999999, "end": 626.4, "text": " Okay.", "tokens": [50948, 1033, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 195, "seek": 61352, "start": 626.4, "end": 629.04, "text": " So let's sort of draw a smaller box around this.", "tokens": [51008, 407, 718, 311, 1333, 295, 2642, 257, 4356, 2424, 926, 341, 13, 51140], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 196, "seek": 61352, "start": 629.04, "end": 632.92, "text": " We've concluded that it's not possible to have a complete client server application", "tokens": [51140, 492, 600, 22960, 300, 309, 311, 406, 1944, 281, 362, 257, 3566, 6423, 7154, 3861, 51334], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 197, "seek": 61352, "start": 632.92, "end": 635.1999999999999, "text": " where we have one single source of truth.", "tokens": [51334, 689, 321, 362, 472, 2167, 4009, 295, 3494, 13, 51448], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 198, "seek": 61352, "start": 635.1999999999999, "end": 639.52, "text": " But what if we narrow it down a little bit just to the client UI state?", "tokens": [51448, 583, 437, 498, 321, 9432, 309, 760, 257, 707, 857, 445, 281, 264, 6423, 15682, 1785, 30, 51664], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 199, "seek": 61352, "start": 639.52, "end": 642.4399999999999, "text": " So if we just draw a box around that, what about now?", "tokens": [51664, 407, 498, 321, 445, 2642, 257, 2424, 926, 300, 11, 437, 466, 586, 30, 51810], "temperature": 0.0, "avg_logprob": -0.1498221339601459, "compression_ratio": 1.7263513513513513, "no_speech_prob": 0.09530102461576462}, {"id": 200, "seek": 64244, "start": 642.5200000000001, "end": 645.5600000000001, "text": " We actually can have a single source of truth.", "tokens": [50368, 492, 767, 393, 362, 257, 2167, 4009, 295, 3494, 13, 50520], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 201, "seek": 64244, "start": 645.5600000000001, "end": 650.1600000000001, "text": " And we as Elm programmers know this because we use the Elm architecture in which we have", "tokens": [50520, 400, 321, 382, 2699, 76, 41504, 458, 341, 570, 321, 764, 264, 2699, 76, 9482, 294, 597, 321, 362, 50750], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 202, "seek": 64244, "start": 650.1600000000001, "end": 651.1600000000001, "text": " a single source of truth.", "tokens": [50750, 257, 2167, 4009, 295, 3494, 13, 50800], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 203, "seek": 64244, "start": 651.1600000000001, "end": 652.32, "text": " That's what model's job is.", "tokens": [50800, 663, 311, 437, 2316, 311, 1691, 307, 13, 50858], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 204, "seek": 64244, "start": 652.32, "end": 657.6, "text": " It is a single atomic immutable value that is the single source of truth for the entire", "tokens": [50858, 467, 307, 257, 2167, 22275, 3397, 32148, 2158, 300, 307, 264, 2167, 4009, 295, 3494, 337, 264, 2302, 51122], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 205, "seek": 64244, "start": 657.6, "end": 660.72, "text": " client application state and everything else is built on that.", "tokens": [51122, 6423, 3861, 1785, 293, 1203, 1646, 307, 3094, 322, 300, 13, 51278], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 206, "seek": 64244, "start": 660.72, "end": 664.2, "text": " And this is one of the reasons that the Elm architecture is nice to use, is that we have", "tokens": [51278, 400, 341, 307, 472, 295, 264, 4112, 300, 264, 2699, 76, 9482, 307, 1481, 281, 764, 11, 307, 300, 321, 362, 51452], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 207, "seek": 64244, "start": 664.2, "end": 665.5600000000001, "text": " the single source of truth.", "tokens": [51452, 264, 2167, 4009, 295, 3494, 13, 51520], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 208, "seek": 64244, "start": 665.5600000000001, "end": 669.32, "text": " We don't have to go around syncing a bunch of different disparate pieces of state.", "tokens": [51520, 492, 500, 380, 362, 281, 352, 926, 5451, 2175, 257, 3840, 295, 819, 14548, 473, 3755, 295, 1785, 13, 51708], "temperature": 0.0, "avg_logprob": -0.14439477782318558, "compression_ratio": 1.9148936170212767, "no_speech_prob": 0.005908655934035778}, {"id": 209, "seek": 66932, "start": 669.32, "end": 673.08, "text": " Like we might if we had something that were, you know, that is like one of the sources", "tokens": [50364, 1743, 321, 1062, 498, 321, 632, 746, 300, 645, 11, 291, 458, 11, 300, 307, 411, 472, 295, 264, 7139, 50552], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 210, "seek": 66932, "start": 673.08, "end": 676.0400000000001, "text": " of truth and then we have a lot of other ones sprinkled around.", "tokens": [50552, 295, 3494, 293, 550, 321, 362, 257, 688, 295, 661, 2306, 30885, 1493, 926, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 211, "seek": 66932, "start": 676.0400000000001, "end": 677.0400000000001, "text": " So this is a good thing.", "tokens": [50700, 407, 341, 307, 257, 665, 551, 13, 50750], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 212, "seek": 66932, "start": 677.0400000000001, "end": 678.96, "text": " This helps us out.", "tokens": [50750, 639, 3665, 505, 484, 13, 50846], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 213, "seek": 66932, "start": 678.96, "end": 681.6400000000001, "text": " So this brings me to my other question from earlier.", "tokens": [50846, 407, 341, 5607, 385, 281, 452, 661, 1168, 490, 3071, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 214, "seek": 66932, "start": 681.6400000000001, "end": 686.0400000000001, "text": " Like when might model end up with multiple sources of truth anyway?", "tokens": [50980, 1743, 562, 1062, 2316, 917, 493, 365, 3866, 7139, 295, 3494, 4033, 30, 51200], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 215, "seek": 66932, "start": 686.0400000000001, "end": 690.5200000000001, "text": " Like when might it go from being the single source of truth to being a store that contains", "tokens": [51200, 1743, 562, 1062, 309, 352, 490, 885, 264, 2167, 4009, 295, 3494, 281, 885, 257, 3531, 300, 8306, 51424], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 216, "seek": 66932, "start": 690.5200000000001, "end": 694.6800000000001, "text": " multiple values, each of which refer to the same piece of information and which might", "tokens": [51424, 3866, 4190, 11, 1184, 295, 597, 2864, 281, 264, 912, 2522, 295, 1589, 293, 597, 1062, 51632], "temperature": 0.0, "avg_logprob": -0.1284254973496848, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.008844108320772648}, {"id": 217, "seek": 69468, "start": 694.7199999999999, "end": 698.7199999999999, "text": " have different values for that piece of information depending on which part of the model I look", "tokens": [50366, 362, 819, 4190, 337, 300, 2522, 295, 1589, 5413, 322, 597, 644, 295, 264, 2316, 286, 574, 50566], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 218, "seek": 69468, "start": 698.7199999999999, "end": 700.76, "text": " at?", "tokens": [50566, 412, 30, 50668], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 219, "seek": 69468, "start": 700.76, "end": 704.4399999999999, "text": " Well one case where this might genuinely want to happen is caching.", "tokens": [50668, 1042, 472, 1389, 689, 341, 1062, 17839, 528, 281, 1051, 307, 269, 2834, 13, 50852], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 220, "seek": 69468, "start": 704.4399999999999, "end": 707.2399999999999, "text": " So and this is something that I would do for performance.", "tokens": [50852, 407, 293, 341, 307, 746, 300, 286, 576, 360, 337, 3389, 13, 50992], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 221, "seek": 69468, "start": 707.2399999999999, "end": 710.88, "text": " So like basically let's say I have some sort of really expensive calculation that I'm", "tokens": [50992, 407, 411, 1936, 718, 311, 584, 286, 362, 512, 1333, 295, 534, 5124, 17108, 300, 286, 478, 51174], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 222, "seek": 69468, "start": 710.88, "end": 712.9599999999999, "text": " doing using model data.", "tokens": [51174, 884, 1228, 2316, 1412, 13, 51278], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 223, "seek": 69468, "start": 712.9599999999999, "end": 715.92, "text": " Really in web applications this almost never happens but it's conceivable.", "tokens": [51278, 4083, 294, 3670, 5821, 341, 1920, 1128, 2314, 457, 309, 311, 10413, 34376, 13, 51426], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 224, "seek": 69468, "start": 715.92, "end": 720.1999999999999, "text": " I could have some really expensive thing that I have to do like 60 frames per second all", "tokens": [51426, 286, 727, 362, 512, 534, 5124, 551, 300, 286, 362, 281, 360, 411, 4060, 12083, 680, 1150, 439, 51640], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 225, "seek": 69468, "start": 720.1999999999999, "end": 722.88, "text": " the time using data from the model.", "tokens": [51640, 264, 565, 1228, 1412, 490, 264, 2316, 13, 51774], "temperature": 0.0, "avg_logprob": -0.14625451696200634, "compression_ratio": 1.771523178807947, "no_speech_prob": 0.024413732811808586}, {"id": 226, "seek": 72288, "start": 722.88, "end": 727.24, "text": " And I can't cache it with HTML lazy for some reason but I mean usually I can and if I can", "tokens": [50364, 400, 286, 393, 380, 19459, 309, 365, 17995, 14847, 337, 512, 1778, 457, 286, 914, 2673, 286, 393, 293, 498, 286, 393, 50582], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 227, "seek": 72288, "start": 727.24, "end": 730.24, "text": " that's certainly how I want to do it because that's the cache that's very nicely managed", "tokens": [50582, 300, 311, 3297, 577, 286, 528, 281, 360, 309, 570, 300, 311, 264, 19459, 300, 311, 588, 9594, 6453, 50732], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 228, "seek": 72288, "start": 730.24, "end": 731.24, "text": " by the Elm runtime.", "tokens": [50732, 538, 264, 2699, 76, 34474, 13, 50782], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 229, "seek": 72288, "start": 731.24, "end": 734.8, "text": " I don't have to think about caching validation which is one of the famously hard problems", "tokens": [50782, 286, 500, 380, 362, 281, 519, 466, 269, 2834, 24071, 597, 307, 472, 295, 264, 34360, 1152, 2740, 50960], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 230, "seek": 72288, "start": 734.8, "end": 740.88, "text": " in computer science and this calculation is so expensive that it's a performance bottleneck", "tokens": [50960, 294, 3820, 3497, 293, 341, 17108, 307, 370, 5124, 300, 309, 311, 257, 3389, 44641, 547, 51264], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 231, "seek": 72288, "start": 740.88, "end": 741.88, "text": " in practice.", "tokens": [51264, 294, 3124, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 232, "seek": 72288, "start": 741.88, "end": 744.84, "text": " And now we're into the territory of extremely, extremely, extremely unlikely to the extremely", "tokens": [51314, 400, 586, 321, 434, 666, 264, 11360, 295, 4664, 11, 4664, 11, 4664, 17518, 281, 264, 4664, 51462], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 233, "seek": 72288, "start": 744.84, "end": 751.12, "text": " power because really when we're talking about performance problems it's pretty much always", "tokens": [51462, 1347, 570, 534, 562, 321, 434, 1417, 466, 3389, 2740, 309, 311, 1238, 709, 1009, 51776], "temperature": 0.0, "avg_logprob": -0.1299405169131151, "compression_ratio": 1.7462235649546827, "no_speech_prob": 0.004067990463227034}, {"id": 234, "seek": 75112, "start": 751.12, "end": 753.0, "text": " to do with rendering stuff.", "tokens": [50364, 281, 360, 365, 22407, 1507, 13, 50458], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 235, "seek": 75112, "start": 753.0, "end": 756.32, "text": " It's basically never to do with calculations like this in practice.", "tokens": [50458, 467, 311, 1936, 1128, 281, 360, 365, 20448, 411, 341, 294, 3124, 13, 50624], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 236, "seek": 75112, "start": 756.32, "end": 759.44, "text": " But let's say I did actually end up with something like that.", "tokens": [50624, 583, 718, 311, 584, 286, 630, 767, 917, 493, 365, 746, 411, 300, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 237, "seek": 75112, "start": 759.44, "end": 763.64, "text": " This is one of the things that caching is used for is you have a piece of state that says", "tokens": [50780, 639, 307, 472, 295, 264, 721, 300, 269, 2834, 307, 1143, 337, 307, 291, 362, 257, 2522, 295, 1785, 300, 1619, 50990], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 238, "seek": 75112, "start": 763.64, "end": 768.28, "text": " I am sort of an intermediate value, some sort of incremental calculation and then I can", "tokens": [50990, 286, 669, 1333, 295, 364, 19376, 2158, 11, 512, 1333, 295, 35759, 17108, 293, 550, 286, 393, 51222], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 239, "seek": 75112, "start": 768.28, "end": 771.64, "text": " base future calculations on that just like incrementing and decrementing rather than", "tokens": [51222, 3096, 2027, 20448, 322, 300, 445, 411, 26200, 278, 293, 6853, 518, 278, 2831, 813, 51390], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 240, "seek": 75112, "start": 771.64, "end": 774.52, "text": " having to rebuild it all from scratch.", "tokens": [51390, 1419, 281, 16877, 309, 439, 490, 8459, 13, 51534], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 241, "seek": 75112, "start": 774.52, "end": 779.92, "text": " So yes it's not like you should never, ever, ever in a bazillion years have duplicate information", "tokens": [51534, 407, 2086, 309, 311, 406, 411, 291, 820, 1128, 11, 1562, 11, 1562, 294, 257, 27147, 11836, 924, 362, 23976, 1589, 51804], "temperature": 0.0, "avg_logprob": -0.1441642072864045, "compression_ratio": 1.8382838283828382, "no_speech_prob": 0.2685670554637909}, {"id": 242, "seek": 77992, "start": 779.92, "end": 780.92, "text": " in the model.", "tokens": [50364, 294, 264, 2316, 13, 50414], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 243, "seek": 77992, "start": 780.92, "end": 784.4, "text": " There is a use case but it's pretty rare in practice.", "tokens": [50414, 821, 307, 257, 764, 1389, 457, 309, 311, 1238, 5892, 294, 3124, 13, 50588], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 244, "seek": 77992, "start": 784.4, "end": 788.4399999999999, "text": " So that's also good news because it means we can have a single source of truth pretty", "tokens": [50588, 407, 300, 311, 611, 665, 2583, 570, 309, 1355, 321, 393, 362, 257, 2167, 4009, 295, 3494, 1238, 50790], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 245, "seek": 77992, "start": 788.4399999999999, "end": 790.16, "text": " much all the time.", "tokens": [50790, 709, 439, 264, 565, 13, 50876], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 246, "seek": 77992, "start": 790.16, "end": 792.5999999999999, "text": " So how about relational data that gets stored in the model?", "tokens": [50876, 407, 577, 466, 38444, 1412, 300, 2170, 12187, 294, 264, 2316, 30, 50998], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 247, "seek": 77992, "start": 792.5999999999999, "end": 796.8, "text": " That is to say pieces of data where we have different pieces of information in the model", "tokens": [50998, 663, 307, 281, 584, 3755, 295, 1412, 689, 321, 362, 819, 3755, 295, 1589, 294, 264, 2316, 51208], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 248, "seek": 77992, "start": 796.8, "end": 799.04, "text": " but they relate to one another in some way.", "tokens": [51208, 457, 436, 10961, 281, 472, 1071, 294, 512, 636, 13, 51320], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 249, "seek": 77992, "start": 799.04, "end": 803.0, "text": " So let's look at an example of this.", "tokens": [51320, 407, 718, 311, 574, 412, 364, 1365, 295, 341, 13, 51518], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 250, "seek": 77992, "start": 803.0, "end": 804.0, "text": " Relational data.", "tokens": [51518, 8738, 1478, 1412, 13, 51568], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 251, "seek": 77992, "start": 804.0, "end": 809.28, "text": " So I work in a company called No Red Ink and we make stuff for English teachers.", "tokens": [51568, 407, 286, 589, 294, 257, 2237, 1219, 883, 4477, 31147, 293, 321, 652, 1507, 337, 3669, 6023, 13, 51832], "temperature": 0.0, "avg_logprob": -0.14438349501530928, "compression_ratio": 1.7182130584192439, "no_speech_prob": 0.047381967306137085}, {"id": 252, "seek": 80928, "start": 809.28, "end": 813.0799999999999, "text": " Now let's say hypothetically, just for the purposes of this example, that we decided", "tokens": [50364, 823, 718, 311, 584, 24371, 22652, 11, 445, 337, 264, 9932, 295, 341, 1365, 11, 300, 321, 3047, 50554], "temperature": 0.0, "avg_logprob": -0.14829358898225378, "compression_ratio": 1.700657894736842, "no_speech_prob": 0.0004438166506588459}, {"id": 253, "seek": 80928, "start": 813.0799999999999, "end": 818.36, "text": " to sort of like broaden our scope a little bit and we decided to introduce a new feature", "tokens": [50554, 281, 1333, 295, 411, 47045, 527, 11923, 257, 707, 857, 293, 321, 3047, 281, 5366, 257, 777, 4111, 50818], "temperature": 0.0, "avg_logprob": -0.14829358898225378, "compression_ratio": 1.700657894736842, "no_speech_prob": 0.0004438166506588459}, {"id": 254, "seek": 80928, "start": 818.36, "end": 823.04, "text": " which is not English related but actually it's field trip management.", "tokens": [50818, 597, 307, 406, 3669, 4077, 457, 767, 309, 311, 2519, 4931, 4592, 13, 51052], "temperature": 0.0, "avg_logprob": -0.14829358898225378, "compression_ratio": 1.700657894736842, "no_speech_prob": 0.0004438166506588459}, {"id": 255, "seek": 80928, "start": 823.04, "end": 827.72, "text": " This is why I'm not in charge of the product team.", "tokens": [51052, 639, 307, 983, 286, 478, 406, 294, 4602, 295, 264, 1674, 1469, 13, 51286], "temperature": 0.0, "avg_logprob": -0.14829358898225378, "compression_ratio": 1.700657894736842, "no_speech_prob": 0.0004438166506588459}, {"id": 256, "seek": 80928, "start": 827.72, "end": 832.04, "text": " So we're going to build this new feature that's going to allow teachers to manage a field trip", "tokens": [51286, 407, 321, 434, 516, 281, 1322, 341, 777, 4111, 300, 311, 516, 281, 2089, 6023, 281, 3067, 257, 2519, 4931, 51502], "temperature": 0.0, "avg_logprob": -0.14829358898225378, "compression_ratio": 1.700657894736842, "no_speech_prob": 0.0004438166506588459}, {"id": 257, "seek": 80928, "start": 832.04, "end": 836.16, "text": " for their students where they take everybody on a bus and they go out somewhere interesting", "tokens": [51502, 337, 641, 1731, 689, 436, 747, 2201, 322, 257, 1255, 293, 436, 352, 484, 4079, 1880, 51708], "temperature": 0.0, "avg_logprob": -0.14829358898225378, "compression_ratio": 1.700657894736842, "no_speech_prob": 0.0004438166506588459}, {"id": 258, "seek": 80928, "start": 836.16, "end": 838.56, "text": " for the day and learn things there.", "tokens": [51708, 337, 264, 786, 293, 1466, 721, 456, 13, 51828], "temperature": 0.0, "avg_logprob": -0.14829358898225378, "compression_ratio": 1.700657894736842, "no_speech_prob": 0.0004438166506588459}, {"id": 259, "seek": 83856, "start": 838.56, "end": 843.16, "text": " So we've got an all day field trip coming up and the teacher wants to answer the question", "tokens": [50364, 407, 321, 600, 658, 364, 439, 786, 2519, 4931, 1348, 493, 293, 264, 5027, 2738, 281, 1867, 264, 1168, 50594], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 260, "seek": 83856, "start": 843.16, "end": 846.8, "text": " which students are going, sort of manage this and check off, okay, these students are going,", "tokens": [50594, 597, 1731, 366, 516, 11, 1333, 295, 3067, 341, 293, 1520, 766, 11, 1392, 11, 613, 1731, 366, 516, 11, 50776], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 261, "seek": 83856, "start": 846.8, "end": 847.88, "text": " these students are not going.", "tokens": [50776, 613, 1731, 366, 406, 516, 13, 50830], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 262, "seek": 83856, "start": 847.88, "end": 851.0, "text": " So let's look at this from a data modeling perspective.", "tokens": [50830, 407, 718, 311, 574, 412, 341, 490, 257, 1412, 15983, 4585, 13, 50986], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 263, "seek": 83856, "start": 851.0, "end": 856.68, "text": " So let's say we've got one of our students named Richard Feldman, going is false which", "tokens": [50986, 407, 718, 311, 584, 321, 600, 658, 472, 295, 527, 1731, 4926, 9809, 42677, 1601, 11, 516, 307, 7908, 597, 51270], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 264, "seek": 83856, "start": 856.68, "end": 860.16, "text": " is why he's making that face.", "tokens": [51270, 307, 983, 415, 311, 1455, 300, 1851, 13, 51444], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 265, "seek": 83856, "start": 860.16, "end": 863.2399999999999, "text": " Kid looks like a troublemaker.", "tokens": [51444, 18978, 1542, 411, 257, 3455, 1113, 4003, 13, 51598], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 266, "seek": 83856, "start": 863.2399999999999, "end": 865.0799999999999, "text": " So this is our very simple data model.", "tokens": [51598, 407, 341, 307, 527, 588, 2199, 1412, 2316, 13, 51690], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 267, "seek": 83856, "start": 865.0799999999999, "end": 868.4399999999999, "text": " We have name which is a string and going which is a boolean.", "tokens": [51690, 492, 362, 1315, 597, 307, 257, 6798, 293, 516, 597, 307, 257, 748, 4812, 282, 13, 51858], "temperature": 0.0, "avg_logprob": -0.20354855578878653, "compression_ratio": 1.8428571428571427, "no_speech_prob": 9.027019405039027e-05}, {"id": 268, "seek": 86844, "start": 868.44, "end": 871.48, "text": " And for our purposes, that's going to be enough.", "tokens": [50364, 400, 337, 527, 9932, 11, 300, 311, 516, 281, 312, 1547, 13, 50516], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 269, "seek": 86844, "start": 871.48, "end": 873.7600000000001, "text": " And then the teacher has multiple courses.", "tokens": [50516, 400, 550, 264, 5027, 575, 3866, 7712, 13, 50630], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 270, "seek": 86844, "start": 873.7600000000001, "end": 876.36, "text": " So let's say a teacher has a course called second period English.", "tokens": [50630, 407, 718, 311, 584, 257, 5027, 575, 257, 1164, 1219, 1150, 2896, 3669, 13, 50760], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 271, "seek": 86844, "start": 876.36, "end": 882.0, "text": " It's a very common name for a course that we see in practice and the teacher has a couple", "tokens": [50760, 467, 311, 257, 588, 2689, 1315, 337, 257, 1164, 300, 321, 536, 294, 3124, 293, 264, 5027, 575, 257, 1916, 51042], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 272, "seek": 86844, "start": 882.0, "end": 883.0, "text": " of students in that course.", "tokens": [51042, 295, 1731, 294, 300, 1164, 13, 51092], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 273, "seek": 86844, "start": 883.0, "end": 889.6800000000001, "text": " So R. Feldman, B. Knowles, A. Einstein and they can sort of check and uncheck which students", "tokens": [51092, 407, 497, 13, 42677, 1601, 11, 363, 13, 10265, 904, 11, 316, 13, 23486, 293, 436, 393, 1333, 295, 1520, 293, 46672, 597, 1731, 51426], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 274, "seek": 86844, "start": 889.6800000000001, "end": 892.0400000000001, "text": " are going to be going on this field trip.", "tokens": [51426, 366, 516, 281, 312, 516, 322, 341, 2519, 4931, 13, 51544], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 275, "seek": 86844, "start": 892.0400000000001, "end": 893.2800000000001, "text": " This is the UI we're building.", "tokens": [51544, 639, 307, 264, 15682, 321, 434, 2390, 13, 51606], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 276, "seek": 86844, "start": 893.2800000000001, "end": 895.96, "text": " They might have another course called fifth period history.", "tokens": [51606, 814, 1062, 362, 1071, 1164, 1219, 9266, 2896, 2503, 13, 51740], "temperature": 0.0, "avg_logprob": -0.17978866172559332, "compression_ratio": 1.7703180212014133, "no_speech_prob": 0.00034583135857246816}, {"id": 277, "seek": 89596, "start": 895.96, "end": 899.6, "text": " So sometimes we have teachers who have, they're not dedicated English teachers.", "tokens": [50364, 407, 2171, 321, 362, 6023, 567, 362, 11, 436, 434, 406, 8374, 3669, 6023, 13, 50546], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 278, "seek": 89596, "start": 899.6, "end": 903.12, "text": " They have some courses that are English and then some courses that are another subject.", "tokens": [50546, 814, 362, 512, 7712, 300, 366, 3669, 293, 550, 512, 7712, 300, 366, 1071, 3983, 13, 50722], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 279, "seek": 89596, "start": 903.12, "end": 906.52, "text": " And sometimes they use no reading because we teach writing among other things.", "tokens": [50722, 400, 2171, 436, 764, 572, 3760, 570, 321, 2924, 3579, 3654, 661, 721, 13, 50892], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 280, "seek": 89596, "start": 906.52, "end": 909.72, "text": " And so they want their history students to be able to write more effective essays.", "tokens": [50892, 400, 370, 436, 528, 641, 2503, 1731, 281, 312, 1075, 281, 2464, 544, 4942, 35123, 13, 51052], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 281, "seek": 89596, "start": 909.72, "end": 914.2800000000001, "text": " So they'll have no reading activated for both their history course and their English course.", "tokens": [51052, 407, 436, 603, 362, 572, 3760, 18157, 337, 1293, 641, 2503, 1164, 293, 641, 3669, 1164, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 282, "seek": 89596, "start": 914.2800000000001, "end": 917.5600000000001, "text": " And of course, if you're teaching multiple subjects, you can have the same student in", "tokens": [51280, 400, 295, 1164, 11, 498, 291, 434, 4571, 3866, 13066, 11, 291, 393, 362, 264, 912, 3107, 294, 51444], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 283, "seek": 89596, "start": 917.5600000000001, "end": 918.5600000000001, "text": " multiple courses.", "tokens": [51444, 3866, 7712, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 284, "seek": 89596, "start": 918.5600000000001, "end": 922.8000000000001, "text": " So Richard Feldman could be in second period English as well as fifth period history in", "tokens": [51494, 407, 9809, 42677, 1601, 727, 312, 294, 1150, 2896, 3669, 382, 731, 382, 9266, 2896, 2503, 294, 51706], "temperature": 0.0, "avg_logprob": -0.1216642322824962, "compression_ratio": 1.9806451612903226, "no_speech_prob": 0.0011332958238199353}, {"id": 285, "seek": 92280, "start": 922.8, "end": 926.76, "text": " addition to some other number of students.", "tokens": [50364, 4500, 281, 512, 661, 1230, 295, 1731, 13, 50562], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 286, "seek": 92280, "start": 926.76, "end": 929.52, "text": " So this is an entirely plausible scenario that we might end up with.", "tokens": [50562, 407, 341, 307, 364, 7696, 39925, 9005, 300, 321, 1062, 917, 493, 365, 13, 50700], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 287, "seek": 92280, "start": 929.52, "end": 932.9599999999999, "text": " And of course, this is relational data that we have students have a relationship to the", "tokens": [50700, 400, 295, 1164, 11, 341, 307, 38444, 1412, 300, 321, 362, 1731, 362, 257, 2480, 281, 264, 50872], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 288, "seek": 92280, "start": 932.9599999999999, "end": 936.0, "text": " course and also we have some sort of notion of identity.", "tokens": [50872, 1164, 293, 611, 321, 362, 512, 1333, 295, 10710, 295, 6575, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 289, "seek": 92280, "start": 936.0, "end": 939.64, "text": " Like Richard Feldman is the same student whether he's in second period English or in fifth", "tokens": [51024, 1743, 9809, 42677, 1601, 307, 264, 912, 3107, 1968, 415, 311, 294, 1150, 2896, 3669, 420, 294, 9266, 51206], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 290, "seek": 92280, "start": 939.64, "end": 941.76, "text": " period history.", "tokens": [51206, 2896, 2503, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 291, "seek": 92280, "start": 941.76, "end": 945.04, "text": " So if the teacher checks one of these boxes because it's the same person, like either", "tokens": [51312, 407, 498, 264, 5027, 13834, 472, 295, 613, 9002, 570, 309, 311, 264, 912, 954, 11, 411, 2139, 51476], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 292, "seek": 92280, "start": 945.04, "end": 948.16, "text": " he's going on the field trip or he's not and it's an all day field trip.", "tokens": [51476, 415, 311, 516, 322, 264, 2519, 4931, 420, 415, 311, 406, 293, 309, 311, 364, 439, 786, 2519, 4931, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 293, "seek": 92280, "start": 948.16, "end": 952.16, "text": " So certainly he's not going to be going for second period but not fifth period.", "tokens": [51632, 407, 3297, 415, 311, 406, 516, 281, 312, 516, 337, 1150, 2896, 457, 406, 9266, 2896, 13, 51832], "temperature": 0.0, "avg_logprob": -0.1373230032369393, "compression_ratio": 1.8871473354231976, "no_speech_prob": 0.0012444895692169666}, {"id": 294, "seek": 95216, "start": 952.16, "end": 956.36, "text": " So if the teacher checks one of the boxes because of the relationships innate in this", "tokens": [50364, 407, 498, 264, 5027, 13834, 472, 295, 264, 9002, 570, 295, 264, 6159, 41766, 294, 341, 50574], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 295, "seek": 95216, "start": 956.36, "end": 958.9599999999999, "text": " data, it should check both of the boxes.", "tokens": [50574, 1412, 11, 309, 820, 1520, 1293, 295, 264, 9002, 13, 50704], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 296, "seek": 95216, "start": 958.9599999999999, "end": 961.7199999999999, "text": " That's otherwise, you know, we've got some sort of mistake.", "tokens": [50704, 663, 311, 5911, 11, 291, 458, 11, 321, 600, 658, 512, 1333, 295, 6146, 13, 50842], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 297, "seek": 95216, "start": 961.7199999999999, "end": 965.8399999999999, "text": " We should never end up with the teacher seeing this in their UI or else we've done something", "tokens": [50842, 492, 820, 1128, 917, 493, 365, 264, 5027, 2577, 341, 294, 641, 15682, 420, 1646, 321, 600, 1096, 746, 51048], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 298, "seek": 95216, "start": 965.8399999999999, "end": 966.8399999999999, "text": " wrong.", "tokens": [51048, 2085, 13, 51098], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 299, "seek": 95216, "start": 966.8399999999999, "end": 968.92, "text": " This would be a synchronization bug.", "tokens": [51098, 639, 576, 312, 257, 19331, 2144, 7426, 13, 51202], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 300, "seek": 95216, "start": 968.92, "end": 970.52, "text": " Okay.", "tokens": [51202, 1033, 13, 51282], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 301, "seek": 95216, "start": 970.52, "end": 975.36, "text": " So in JavaScript, the way that I might have done this is I would say something like going", "tokens": [51282, 407, 294, 15778, 11, 264, 636, 300, 286, 1062, 362, 1096, 341, 307, 286, 576, 584, 746, 411, 516, 51524], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 302, "seek": 95216, "start": 975.36, "end": 979.28, "text": " back to the very beginning of the talk, courses brackets zero dot students brackets zero dot", "tokens": [51524, 646, 281, 264, 588, 2863, 295, 264, 751, 11, 7712, 26179, 4018, 5893, 1731, 26179, 4018, 5893, 51720], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 303, "seek": 95216, "start": 979.28, "end": 980.6, "text": " going equals true.", "tokens": [51720, 516, 6915, 2074, 13, 51786], "temperature": 0.0, "avg_logprob": -0.17949702134772913, "compression_ratio": 1.7759197324414715, "no_speech_prob": 0.0009695319458842278}, {"id": 304, "seek": 98060, "start": 980.6, "end": 986.4, "text": " Which is to say the first student in the first course is now going on the field trip.", "tokens": [50364, 3013, 307, 281, 584, 264, 700, 3107, 294, 264, 700, 1164, 307, 586, 516, 322, 264, 2519, 4931, 13, 50654], "temperature": 0.0, "avg_logprob": -0.1845969501723591, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.0017540365224704146}, {"id": 305, "seek": 98060, "start": 986.4, "end": 990.2, "text": " When I check that box, that's what we've changed about our data model.", "tokens": [50654, 1133, 286, 1520, 300, 2424, 11, 300, 311, 437, 321, 600, 3105, 466, 527, 1412, 2316, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1845969501723591, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.0017540365224704146}, {"id": 306, "seek": 98060, "start": 990.2, "end": 994.44, "text": " Now if I do this and I put into a REPL courses brackets zero dot students brackets zero, it", "tokens": [50844, 823, 498, 286, 360, 341, 293, 286, 829, 666, 257, 31511, 43, 7712, 26179, 4018, 5893, 1731, 26179, 4018, 11, 309, 51056], "temperature": 0.0, "avg_logprob": -0.1845969501723591, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.0017540365224704146}, {"id": 307, "seek": 98060, "start": 994.44, "end": 998.36, "text": " would now say, hey, this student is now going, great.", "tokens": [51056, 576, 586, 584, 11, 4177, 11, 341, 3107, 307, 586, 516, 11, 869, 13, 51252], "temperature": 0.0, "avg_logprob": -0.1845969501723591, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.0017540365224704146}, {"id": 308, "seek": 98060, "start": 998.36, "end": 1002.96, "text": " And also, if I said courses bracket one, which is to say fifth period English dot students", "tokens": [51252, 400, 611, 11, 498, 286, 848, 7712, 16904, 472, 11, 597, 307, 281, 584, 9266, 2896, 3669, 5893, 1731, 51482], "temperature": 0.0, "avg_logprob": -0.1845969501723591, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.0017540365224704146}, {"id": 309, "seek": 98060, "start": 1002.96, "end": 1006.6800000000001, "text": " brackets zero, it would also say that that student is going.", "tokens": [51482, 26179, 4018, 11, 309, 576, 611, 584, 300, 300, 3107, 307, 516, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1845969501723591, "compression_ratio": 1.8760330578512396, "no_speech_prob": 0.0017540365224704146}, {"id": 310, "seek": 100668, "start": 1006.68, "end": 1011.1999999999999, "text": " And the reason for that is that students in both cases are mutable references in JavaScript.", "tokens": [50364, 400, 264, 1778, 337, 300, 307, 300, 1731, 294, 1293, 3331, 366, 5839, 712, 15400, 294, 15778, 13, 50590], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 311, "seek": 100668, "start": 1011.1999999999999, "end": 1015.4799999999999, "text": " These are JavaScript objects, JavaScript objects are mutable, and they store mutable references", "tokens": [50590, 1981, 366, 15778, 6565, 11, 15778, 6565, 366, 5839, 712, 11, 293, 436, 3531, 5839, 712, 15400, 50804], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 312, "seek": 100668, "start": 1015.4799999999999, "end": 1017.28, "text": " to other objects.", "tokens": [50804, 281, 661, 6565, 13, 50894], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 313, "seek": 100668, "start": 1017.28, "end": 1020.9599999999999, "text": " So both of these are actually pointing to the same student in memory, which means if", "tokens": [50894, 407, 1293, 295, 613, 366, 767, 12166, 281, 264, 912, 3107, 294, 4675, 11, 597, 1355, 498, 51078], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 314, "seek": 100668, "start": 1020.9599999999999, "end": 1023.5999999999999, "text": " I change the one, it's going to change the other as well.", "tokens": [51078, 286, 1319, 264, 472, 11, 309, 311, 516, 281, 1319, 264, 661, 382, 731, 13, 51210], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 315, "seek": 100668, "start": 1023.5999999999999, "end": 1027.6799999999998, "text": " Now this has various downsides that we're probably familiar with, like you pass something", "tokens": [51210, 823, 341, 575, 3683, 21554, 1875, 300, 321, 434, 1391, 4963, 365, 11, 411, 291, 1320, 746, 51414], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 316, "seek": 100668, "start": 1027.6799999999998, "end": 1031.48, "text": " to a function, you're not sure if that function is going to mutate or not, plenty of downsides,", "tokens": [51414, 281, 257, 2445, 11, 291, 434, 406, 988, 498, 300, 2445, 307, 516, 281, 5839, 473, 420, 406, 11, 7140, 295, 21554, 1875, 11, 51604], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 317, "seek": 100668, "start": 1031.48, "end": 1035.12, "text": " but this is one of the upsides when it comes to data consistency.", "tokens": [51604, 457, 341, 307, 472, 295, 264, 15497, 1875, 562, 309, 1487, 281, 1412, 14416, 13, 51786], "temperature": 0.0, "avg_logprob": -0.12246605445598734, "compression_ratio": 1.878125, "no_speech_prob": 0.005219310987740755}, {"id": 318, "seek": 103512, "start": 1035.1599999999999, "end": 1039.9599999999998, "text": " It means that when I mutate one, it automatically mutates all the others at the same time for", "tokens": [50366, 467, 1355, 300, 562, 286, 5839, 473, 472, 11, 309, 6772, 5839, 1024, 439, 264, 2357, 412, 264, 912, 565, 337, 50606], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 319, "seek": 103512, "start": 1039.9599999999998, "end": 1040.9599999999998, "text": " me.", "tokens": [50606, 385, 13, 50656], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 320, "seek": 103512, "start": 1040.9599999999998, "end": 1045.6, "text": " So this is an example of relational data with a single source of truth.", "tokens": [50656, 407, 341, 307, 364, 1365, 295, 38444, 1412, 365, 257, 2167, 4009, 295, 3494, 13, 50888], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 321, "seek": 103512, "start": 1045.6, "end": 1050.28, "text": " We don't actually have any duplicate information in this data model that we've built in this", "tokens": [50888, 492, 500, 380, 767, 362, 604, 23976, 1589, 294, 341, 1412, 2316, 300, 321, 600, 3094, 294, 341, 51122], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 322, "seek": 103512, "start": 1050.28, "end": 1052.4799999999998, "text": " sort of JavaScript version.", "tokens": [51122, 1333, 295, 15778, 3037, 13, 51232], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 323, "seek": 103512, "start": 1052.4799999999998, "end": 1057.0, "text": " Okay, so now I'm, you know, was a JavaScript programmer, now I've transitioned to an Elm", "tokens": [51232, 1033, 11, 370, 586, 286, 478, 11, 291, 458, 11, 390, 257, 15778, 32116, 11, 586, 286, 600, 47346, 281, 364, 2699, 76, 51458], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 324, "seek": 103512, "start": 1057.0, "end": 1060.6799999999998, "text": " programmer, and so now I'm thinking in terms of immutable data.", "tokens": [51458, 32116, 11, 293, 370, 586, 286, 478, 1953, 294, 2115, 295, 3397, 32148, 1412, 13, 51642], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 325, "seek": 103512, "start": 1060.6799999999998, "end": 1064.32, "text": " So one way that we might model this in Elm, and probably the way that I would have modeled", "tokens": [51642, 407, 472, 636, 300, 321, 1062, 2316, 341, 294, 2699, 76, 11, 293, 1391, 264, 636, 300, 286, 576, 362, 37140, 51824], "temperature": 0.0, "avg_logprob": -0.15141037927157636, "compression_ratio": 1.78, "no_speech_prob": 0.00039200432365760207}, {"id": 326, "seek": 106432, "start": 1064.36, "end": 1068.9199999999998, "text": " it when I was starting out, I would say type alias student, name colon string, going is", "tokens": [50366, 309, 562, 286, 390, 2891, 484, 11, 286, 576, 584, 2010, 419, 4609, 3107, 11, 1315, 8255, 6798, 11, 516, 307, 50594], "temperature": 0.0, "avg_logprob": -0.14340223782304404, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.0011334113078191876}, {"id": 327, "seek": 106432, "start": 1068.9199999999998, "end": 1075.76, "text": " a Boolean, then I'd have type alias course, name is a string, students is a list of students,", "tokens": [50594, 257, 23351, 28499, 11, 550, 286, 1116, 362, 2010, 419, 4609, 1164, 11, 1315, 307, 257, 6798, 11, 1731, 307, 257, 1329, 295, 1731, 11, 50936], "temperature": 0.0, "avg_logprob": -0.14340223782304404, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.0011334113078191876}, {"id": 328, "seek": 106432, "start": 1075.76, "end": 1080.56, "text": " so we have nested records here, and then finally I would have my model which would have courses,", "tokens": [50936, 370, 321, 362, 15646, 292, 7724, 510, 11, 293, 550, 2721, 286, 576, 362, 452, 2316, 597, 576, 362, 7712, 11, 51176], "temperature": 0.0, "avg_logprob": -0.14340223782304404, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.0011334113078191876}, {"id": 329, "seek": 106432, "start": 1080.56, "end": 1083.96, "text": " which is a list of course, and then various other things, but at least within the scope", "tokens": [51176, 597, 307, 257, 1329, 295, 1164, 11, 293, 550, 3683, 661, 721, 11, 457, 412, 1935, 1951, 264, 11923, 51346], "temperature": 0.0, "avg_logprob": -0.14340223782304404, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.0011334113078191876}, {"id": 330, "seek": 106432, "start": 1083.96, "end": 1087.6799999999998, "text": " of what we're talking about here, these are the relevant structures that we'd be dealing", "tokens": [51346, 295, 437, 321, 434, 1417, 466, 510, 11, 613, 366, 264, 7340, 9227, 300, 321, 1116, 312, 6260, 51532], "temperature": 0.0, "avg_logprob": -0.14340223782304404, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.0011334113078191876}, {"id": 331, "seek": 106432, "start": 1087.6799999999998, "end": 1088.6799999999998, "text": " with.", "tokens": [51532, 365, 13, 51582], "temperature": 0.0, "avg_logprob": -0.14340223782304404, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.0011334113078191876}, {"id": 332, "seek": 106432, "start": 1088.6799999999998, "end": 1093.6799999999998, "text": " Now, believe it or not, already we've introduced multiple sources of truth.", "tokens": [51582, 823, 11, 1697, 309, 420, 406, 11, 1217, 321, 600, 7268, 3866, 7139, 295, 3494, 13, 51832], "temperature": 0.0, "avg_logprob": -0.14340223782304404, "compression_ratio": 1.8645833333333333, "no_speech_prob": 0.0011334113078191876}, {"id": 333, "seek": 109368, "start": 1093.72, "end": 1097.6000000000001, "text": " Even though this basically looks like a description of the same sort of schema that we had in", "tokens": [50366, 2754, 1673, 341, 1936, 1542, 411, 257, 3855, 295, 264, 912, 1333, 295, 34078, 300, 321, 632, 294, 50560], "temperature": 0.0, "avg_logprob": -0.14646954439124282, "compression_ratio": 1.7861271676300579, "no_speech_prob": 0.0010985539993271232}, {"id": 334, "seek": 109368, "start": 1097.6000000000001, "end": 1101.96, "text": " the JavaScript version, just by virtue of the fact that we've gone from mutable objects", "tokens": [50560, 264, 15778, 3037, 11, 445, 538, 20816, 295, 264, 1186, 300, 321, 600, 2780, 490, 5839, 712, 6565, 50778], "temperature": 0.0, "avg_logprob": -0.14646954439124282, "compression_ratio": 1.7861271676300579, "no_speech_prob": 0.0010985539993271232}, {"id": 335, "seek": 109368, "start": 1101.96, "end": 1106.1200000000001, "text": " to immutable data, we have now accidentally introduced multiple sources of truth.", "tokens": [50778, 281, 3397, 32148, 1412, 11, 321, 362, 586, 15715, 7268, 3866, 7139, 295, 3494, 13, 50986], "temperature": 0.0, "avg_logprob": -0.14646954439124282, "compression_ratio": 1.7861271676300579, "no_speech_prob": 0.0010985539993271232}, {"id": 336, "seek": 109368, "start": 1106.1200000000001, "end": 1111.1200000000001, "text": " And this is kind of an easy thing to do, at least it was for me, when going from JavaScript", "tokens": [50986, 400, 341, 307, 733, 295, 364, 1858, 551, 281, 360, 11, 412, 1935, 309, 390, 337, 385, 11, 562, 516, 490, 15778, 51236], "temperature": 0.0, "avg_logprob": -0.14646954439124282, "compression_ratio": 1.7861271676300579, "no_speech_prob": 0.0010985539993271232}, {"id": 337, "seek": 109368, "start": 1111.1200000000001, "end": 1114.48, "text": " to Elm because I'm like, oh, well, records look like objects, so I'll just take the thing", "tokens": [51236, 281, 2699, 76, 570, 286, 478, 411, 11, 1954, 11, 731, 11, 7724, 574, 411, 6565, 11, 370, 286, 603, 445, 747, 264, 551, 51404], "temperature": 0.0, "avg_logprob": -0.14646954439124282, "compression_ratio": 1.7861271676300579, "no_speech_prob": 0.0010985539993271232}, {"id": 338, "seek": 109368, "start": 1114.48, "end": 1118.0, "text": " that I would have done with an object and I'll just do it with a record.", "tokens": [51404, 300, 286, 576, 362, 1096, 365, 364, 2657, 293, 286, 603, 445, 360, 309, 365, 257, 2136, 13, 51580], "temperature": 0.0, "avg_logprob": -0.14646954439124282, "compression_ratio": 1.7861271676300579, "no_speech_prob": 0.0010985539993271232}, {"id": 339, "seek": 109368, "start": 1118.0, "end": 1122.8, "text": " But it turns out that that mutable to immutable characteristic is actually significant implications", "tokens": [51580, 583, 309, 4523, 484, 300, 300, 5839, 712, 281, 3397, 32148, 16282, 307, 767, 4776, 16602, 51820], "temperature": 0.0, "avg_logprob": -0.14646954439124282, "compression_ratio": 1.7861271676300579, "no_speech_prob": 0.0010985539993271232}, {"id": 340, "seek": 112280, "start": 1123.8, "end": 1129.8, "text": " to sort of do a more apples to apples transition, let's go from the JavaScript object that", "tokens": [50414, 281, 1333, 295, 360, 257, 544, 16814, 281, 16814, 6034, 11, 718, 311, 352, 490, 264, 15778, 2657, 300, 50714], "temperature": 0.0, "avg_logprob": -0.16610059167584804, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0002780099166557193}, {"id": 341, "seek": 112280, "start": 1129.8, "end": 1135.8, "text": " we did before to the JavaScript object notation, better known as JSON, because JavaScript", "tokens": [50714, 321, 630, 949, 281, 264, 15778, 2657, 24657, 11, 1101, 2570, 382, 31828, 11, 570, 15778, 51014], "temperature": 0.0, "avg_logprob": -0.16610059167584804, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0002780099166557193}, {"id": 342, "seek": 112280, "start": 1135.8, "end": 1139.6399999999999, "text": " objects are mutable references, but JSON is actually immutable data, which it sort of", "tokens": [51014, 6565, 366, 5839, 712, 15400, 11, 457, 31828, 307, 767, 3397, 32148, 1412, 11, 597, 309, 1333, 295, 51206], "temperature": 0.0, "avg_logprob": -0.16610059167584804, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0002780099166557193}, {"id": 343, "seek": 112280, "start": 1139.6399999999999, "end": 1140.6399999999999, "text": " has to be to serialize.", "tokens": [51206, 575, 281, 312, 281, 17436, 1125, 13, 51256], "temperature": 0.0, "avg_logprob": -0.16610059167584804, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0002780099166557193}, {"id": 344, "seek": 112280, "start": 1140.6399999999999, "end": 1144.36, "text": " If you want to serialize mutable references to memory locations and just write them straight", "tokens": [51256, 759, 291, 528, 281, 17436, 1125, 5839, 712, 15400, 281, 4675, 9253, 293, 445, 2464, 552, 2997, 51442], "temperature": 0.0, "avg_logprob": -0.16610059167584804, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0002780099166557193}, {"id": 345, "seek": 112280, "start": 1144.36, "end": 1147.72, "text": " to the disk, it's probably not likely to work out very well when you try to deserialize", "tokens": [51442, 281, 264, 12355, 11, 309, 311, 1391, 406, 3700, 281, 589, 484, 588, 731, 562, 291, 853, 281, 730, 260, 831, 1125, 51610], "temperature": 0.0, "avg_logprob": -0.16610059167584804, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0002780099166557193}, {"id": 346, "seek": 112280, "start": 1147.72, "end": 1148.72, "text": " them.", "tokens": [51610, 552, 13, 51660], "temperature": 0.0, "avg_logprob": -0.16610059167584804, "compression_ratio": 1.8275862068965518, "no_speech_prob": 0.0002780099166557193}, {"id": 347, "seek": 114872, "start": 1148.72, "end": 1153.76, "text": " JSON is immutable despite having the same structure as JavaScript objects.", "tokens": [50364, 31828, 307, 3397, 32148, 7228, 1419, 264, 912, 3877, 382, 15778, 6565, 13, 50616], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 348, "seek": 114872, "start": 1153.76, "end": 1155.24, "text": " So let's look at the JSON version of this.", "tokens": [50616, 407, 718, 311, 574, 412, 264, 31828, 3037, 295, 341, 13, 50690], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 349, "seek": 114872, "start": 1155.24, "end": 1159.92, "text": " So we have courses, we have name, second period English, students, and then an array, name", "tokens": [50690, 407, 321, 362, 7712, 11, 321, 362, 1315, 11, 1150, 2896, 3669, 11, 1731, 11, 293, 550, 364, 10225, 11, 1315, 50924], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 350, "seek": 114872, "start": 1159.92, "end": 1165.84, "text": " Richard Feldman going false, and then another course, name, fifth period history, students,", "tokens": [50924, 9809, 42677, 1601, 516, 7908, 11, 293, 550, 1071, 1164, 11, 1315, 11, 9266, 2896, 2503, 11, 1731, 11, 51220], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 351, "seek": 114872, "start": 1165.84, "end": 1168.56, "text": " name Richard Feldman going false.", "tokens": [51220, 1315, 9809, 42677, 1601, 516, 7908, 13, 51356], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 352, "seek": 114872, "start": 1168.56, "end": 1172.8, "text": " So now we can kind of see more clearly that we actually do, in fact, have duplicated data", "tokens": [51356, 407, 586, 321, 393, 733, 295, 536, 544, 4448, 300, 321, 767, 360, 11, 294, 1186, 11, 362, 1581, 564, 3587, 1412, 51568], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 353, "seek": 114872, "start": 1172.8, "end": 1173.8, "text": " now.", "tokens": [51568, 586, 13, 51618], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 354, "seek": 114872, "start": 1173.8, "end": 1178.68, "text": " Like before, both of those students were pointing to the same point in memory, but now we actually", "tokens": [51618, 1743, 949, 11, 1293, 295, 729, 1731, 645, 12166, 281, 264, 912, 935, 294, 4675, 11, 457, 586, 321, 767, 51862], "temperature": 0.0, "avg_logprob": -0.18285230765665383, "compression_ratio": 1.8082191780821917, "no_speech_prob": 0.007813412696123123}, {"id": 355, "seek": 117868, "start": 1178.68, "end": 1181.5600000000002, "text": " have two different independent pieces of data.", "tokens": [50364, 362, 732, 819, 6695, 3755, 295, 1412, 13, 50508], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 356, "seek": 117868, "start": 1181.5600000000002, "end": 1183.24, "text": " So that means that they can now get out of sync.", "tokens": [50508, 407, 300, 1355, 300, 436, 393, 586, 483, 484, 295, 20271, 13, 50592], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 357, "seek": 117868, "start": 1183.24, "end": 1187.6000000000001, "text": " I can change one of them to true without changing the other one to false.", "tokens": [50592, 286, 393, 1319, 472, 295, 552, 281, 2074, 1553, 4473, 264, 661, 472, 281, 7908, 13, 50810], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 358, "seek": 117868, "start": 1187.6000000000001, "end": 1189.52, "text": " That's a potential problem.", "tokens": [50810, 663, 311, 257, 3995, 1154, 13, 50906], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 359, "seek": 117868, "start": 1189.52, "end": 1195.6000000000001, "text": " So objects have mutable references, whereas records, and JSON, have immutable records.", "tokens": [50906, 407, 6565, 362, 5839, 712, 15400, 11, 9735, 7724, 11, 293, 31828, 11, 362, 3397, 32148, 7724, 13, 51210], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 360, "seek": 117868, "start": 1195.6000000000001, "end": 1199.4, "text": " So objects have this upside of sort of implicit synchronization that sort of automatically", "tokens": [51210, 407, 6565, 362, 341, 14119, 295, 1333, 295, 26947, 19331, 2144, 300, 1333, 295, 6772, 51400], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 361, "seek": 117868, "start": 1199.4, "end": 1204.4, "text": " happens whenever you change something, whereas records also have their own set of upsides,", "tokens": [51400, 2314, 5699, 291, 1319, 746, 11, 9735, 7724, 611, 362, 641, 1065, 992, 295, 15497, 1875, 11, 51650], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 362, "seek": 117868, "start": 1204.4, "end": 1208.28, "text": " really cheap copying, equality checks that can just do reference equality, so on and", "tokens": [51650, 534, 7084, 27976, 11, 14949, 13834, 300, 393, 445, 360, 6408, 14949, 11, 370, 322, 293, 51844], "temperature": 0.0, "avg_logprob": -0.11845402490525019, "compression_ratio": 1.8125, "no_speech_prob": 0.002322727581486106}, {"id": 363, "seek": 120828, "start": 1208.28, "end": 1209.36, "text": " so forth.", "tokens": [50364, 370, 5220, 13, 50418], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 364, "seek": 120828, "start": 1209.36, "end": 1213.3999999999999, "text": " But this is something that we have to be aware of when we're going from objects to records.", "tokens": [50418, 583, 341, 307, 746, 300, 321, 362, 281, 312, 3650, 295, 562, 321, 434, 516, 490, 6565, 281, 7724, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 365, "seek": 120828, "start": 1213.3999999999999, "end": 1215.96, "text": " This is a pretty significant difference.", "tokens": [50620, 639, 307, 257, 1238, 4776, 2649, 13, 50748], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 366, "seek": 120828, "start": 1215.96, "end": 1219.8799999999999, "text": " So let's move along to immutable relational data.", "tokens": [50748, 407, 718, 311, 1286, 2051, 281, 3397, 32148, 38444, 1412, 13, 50944], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 367, "seek": 120828, "start": 1219.8799999999999, "end": 1224.2, "text": " So what are we going to do differently when we are transitioning to the world of immutability?", "tokens": [50944, 407, 437, 366, 321, 516, 281, 360, 7614, 562, 321, 366, 33777, 281, 264, 1002, 295, 3397, 325, 2310, 30, 51160], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 368, "seek": 120828, "start": 1224.2, "end": 1227.08, "text": " Okay, so this was sort of the problem that we ran into.", "tokens": [51160, 1033, 11, 370, 341, 390, 1333, 295, 264, 1154, 300, 321, 5872, 666, 13, 51304], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 369, "seek": 120828, "start": 1227.08, "end": 1229.84, "text": " We had this duplicated data, and it could get out of sync.", "tokens": [51304, 492, 632, 341, 1581, 564, 3587, 1412, 11, 293, 309, 727, 483, 484, 295, 20271, 13, 51442], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 370, "seek": 120828, "start": 1229.84, "end": 1233.3999999999999, "text": " We could have going is true in one case and going is false in the other case, and that", "tokens": [51442, 492, 727, 362, 516, 307, 2074, 294, 472, 1389, 293, 516, 307, 7908, 294, 264, 661, 1389, 11, 293, 300, 51620], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 371, "seek": 120828, "start": 1233.3999999999999, "end": 1236.08, "text": " shouldn't be possible because it's the same student.", "tokens": [51620, 4659, 380, 312, 1944, 570, 309, 311, 264, 912, 3107, 13, 51754], "temperature": 0.0, "avg_logprob": -0.12682412375866528, "compression_ratio": 1.7712418300653594, "no_speech_prob": 0.001700362772680819}, {"id": 372, "seek": 123608, "start": 1236.08, "end": 1238.9199999999998, "text": " Whether he's going on the field trip or he's not.", "tokens": [50364, 8503, 415, 311, 516, 322, 264, 2519, 4931, 420, 415, 311, 406, 13, 50506], "temperature": 0.0, "avg_logprob": -0.11512742348767202, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.001324788318015635}, {"id": 373, "seek": 123608, "start": 1238.9199999999998, "end": 1244.08, "text": " So one way we could create a single source of truth out of this is by changing the JSON", "tokens": [50506, 407, 472, 636, 321, 727, 1884, 257, 2167, 4009, 295, 3494, 484, 295, 341, 307, 538, 4473, 264, 31828, 50764], "temperature": 0.0, "avg_logprob": -0.11512742348767202, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.001324788318015635}, {"id": 374, "seek": 123608, "start": 1244.08, "end": 1246.6799999999998, "text": " to look like this.", "tokens": [50764, 281, 574, 411, 341, 13, 50894], "temperature": 0.0, "avg_logprob": -0.11512742348767202, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.001324788318015635}, {"id": 375, "seek": 123608, "start": 1246.6799999999998, "end": 1247.9199999999998, "text": " So let's look at these differences here.", "tokens": [50894, 407, 718, 311, 574, 412, 613, 7300, 510, 13, 50956], "temperature": 0.0, "avg_logprob": -0.11512742348767202, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.001324788318015635}, {"id": 376, "seek": 123608, "start": 1247.9199999999998, "end": 1254.04, "text": " So the first thing is we've introduced explicit identifiers for what the student refers to.", "tokens": [50956, 407, 264, 700, 551, 307, 321, 600, 7268, 13691, 2473, 23463, 337, 437, 264, 3107, 14942, 281, 13, 51262], "temperature": 0.0, "avg_logprob": -0.11512742348767202, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.001324788318015635}, {"id": 377, "seek": 123608, "start": 1254.04, "end": 1258.76, "text": " We're no longer using memory references, we're actually using identifiers as references.", "tokens": [51262, 492, 434, 572, 2854, 1228, 4675, 15400, 11, 321, 434, 767, 1228, 2473, 23463, 382, 15400, 13, 51498], "temperature": 0.0, "avg_logprob": -0.11512742348767202, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.001324788318015635}, {"id": 378, "seek": 123608, "start": 1258.76, "end": 1262.8, "text": " So here we have students is just an array of IDs.", "tokens": [51498, 407, 510, 321, 362, 1731, 307, 445, 364, 10225, 295, 48212, 13, 51700], "temperature": 0.0, "avg_logprob": -0.11512742348767202, "compression_ratio": 1.6784313725490196, "no_speech_prob": 0.001324788318015635}, {"id": 379, "seek": 126280, "start": 1262.8, "end": 1268.2, "text": " So ID 217, we see down at the bottom there, refers to Richard Feldman.", "tokens": [50364, 407, 7348, 5080, 22, 11, 321, 536, 760, 412, 264, 2767, 456, 11, 14942, 281, 9809, 42677, 1601, 13, 50634], "temperature": 0.0, "avg_logprob": -0.1671899159749349, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.00035694483085535467}, {"id": 380, "seek": 126280, "start": 1268.2, "end": 1271.76, "text": " And that's the only place that Richard Feldman appears in this whole data model.", "tokens": [50634, 400, 300, 311, 264, 787, 1081, 300, 9809, 42677, 1601, 7038, 294, 341, 1379, 1412, 2316, 13, 50812], "temperature": 0.0, "avg_logprob": -0.1671899159749349, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.00035694483085535467}, {"id": 381, "seek": 126280, "start": 1271.76, "end": 1275.8799999999999, "text": " Everywhere else it's an identifier referring to that one single source of truth.", "tokens": [50812, 37322, 1646, 309, 311, 364, 45690, 13761, 281, 300, 472, 2167, 4009, 295, 3494, 13, 51018], "temperature": 0.0, "avg_logprob": -0.1671899159749349, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.00035694483085535467}, {"id": 382, "seek": 126280, "start": 1275.8799999999999, "end": 1282.08, "text": " So we're using explicit identifiers to reference a single source of truth in a completely immutable", "tokens": [51018, 407, 321, 434, 1228, 13691, 2473, 23463, 281, 6408, 257, 2167, 4009, 295, 3494, 294, 257, 2584, 3397, 32148, 51328], "temperature": 0.0, "avg_logprob": -0.1671899159749349, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.00035694483085535467}, {"id": 383, "seek": 126280, "start": 1282.08, "end": 1283.8, "text": " way.", "tokens": [51328, 636, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1671899159749349, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.00035694483085535467}, {"id": 384, "seek": 126280, "start": 1283.8, "end": 1290.96, "text": " So let's translate that idea into L. So we've got our type alias student course model.", "tokens": [51414, 407, 718, 311, 13799, 300, 1558, 666, 441, 13, 407, 321, 600, 658, 527, 2010, 419, 4609, 3107, 1164, 2316, 13, 51772], "temperature": 0.0, "avg_logprob": -0.1671899159749349, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.00035694483085535467}, {"id": 385, "seek": 126280, "start": 1290.96, "end": 1292.08, "text": " So this is what we had before.", "tokens": [51772, 407, 341, 307, 437, 321, 632, 949, 13, 51828], "temperature": 0.0, "avg_logprob": -0.1671899159749349, "compression_ratio": 1.6789667896678966, "no_speech_prob": 0.00035694483085535467}, {"id": 386, "seek": 129208, "start": 1292.08, "end": 1295.76, "text": " We had a list of students under course and we had a list of courses under model.", "tokens": [50364, 492, 632, 257, 1329, 295, 1731, 833, 1164, 293, 321, 632, 257, 1329, 295, 7712, 833, 2316, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 387, "seek": 129208, "start": 1295.76, "end": 1297.9199999999998, "text": " So we're going to tweak that a little bit.", "tokens": [50548, 407, 321, 434, 516, 281, 29879, 300, 257, 707, 857, 13, 50656], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 388, "seek": 129208, "start": 1297.9199999999998, "end": 1301.72, "text": " Instead we're going to go from students to student IDs, which is going to be a list", "tokens": [50656, 7156, 321, 434, 516, 281, 352, 490, 1731, 281, 3107, 48212, 11, 597, 307, 516, 281, 312, 257, 1329, 50846], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 389, "seek": 129208, "start": 1301.72, "end": 1303.6, "text": " of student ID values.", "tokens": [50846, 295, 3107, 7348, 4190, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 390, "seek": 129208, "start": 1303.6, "end": 1308.0, "text": " So student ID type could be whatever you want, you know, integers as we saw in the JSON example,", "tokens": [50940, 407, 3107, 7348, 2010, 727, 312, 2035, 291, 528, 11, 291, 458, 11, 41674, 382, 321, 1866, 294, 264, 31828, 1365, 11, 51160], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 391, "seek": 129208, "start": 1308.0, "end": 1311.48, "text": " strings, custom types, anything you want.", "tokens": [51160, 13985, 11, 2375, 3467, 11, 1340, 291, 528, 13, 51334], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 392, "seek": 129208, "start": 1311.48, "end": 1315.56, "text": " Inside the model we're going to have students as a new value in the model.", "tokens": [51334, 15123, 264, 2316, 321, 434, 516, 281, 362, 1731, 382, 257, 777, 2158, 294, 264, 2316, 13, 51538], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 393, "seek": 129208, "start": 1315.56, "end": 1319.36, "text": " This is our single source of truth for all of our students across the entire model.", "tokens": [51538, 639, 307, 527, 2167, 4009, 295, 3494, 337, 439, 295, 527, 1731, 2108, 264, 2302, 2316, 13, 51728], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 394, "seek": 129208, "start": 1319.36, "end": 1321.28, "text": " And it's going to be a dictionary.", "tokens": [51728, 400, 309, 311, 516, 281, 312, 257, 25890, 13, 51824], "temperature": 0.0, "avg_logprob": -0.1720702502192283, "compression_ratio": 1.944636678200692, "no_speech_prob": 0.0004877839528489858}, {"id": 395, "seek": 132128, "start": 1321.32, "end": 1324.76, "text": " So we're going to change on that same student ID and then having a single source of truth", "tokens": [50366, 407, 321, 434, 516, 281, 1319, 322, 300, 912, 3107, 7348, 293, 550, 1419, 257, 2167, 4009, 295, 3494, 50538], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 396, "seek": 132128, "start": 1324.76, "end": 1329.72, "text": " be the value of that particular student stored in that dictionary.", "tokens": [50538, 312, 264, 2158, 295, 300, 1729, 3107, 12187, 294, 300, 25890, 13, 50786], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 397, "seek": 132128, "start": 1329.72, "end": 1333.32, "text": " By the way, one thing we would also want to do here, I think, that can sort of take this", "tokens": [50786, 3146, 264, 636, 11, 472, 551, 321, 576, 611, 528, 281, 360, 510, 11, 286, 519, 11, 300, 393, 1333, 295, 747, 341, 50966], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 398, "seek": 132128, "start": 1333.32, "end": 1336.92, "text": " data model a little bit one step further, is to change from a list of student IDs to", "tokens": [50966, 1412, 2316, 257, 707, 857, 472, 1823, 3052, 11, 307, 281, 1319, 490, 257, 1329, 295, 3107, 48212, 281, 51146], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 399, "seek": 132128, "start": 1336.92, "end": 1338.3999999999999, "text": " a set of student IDs.", "tokens": [51146, 257, 992, 295, 3107, 48212, 13, 51220], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 400, "seek": 132128, "start": 1338.3999999999999, "end": 1342.68, "text": " Because really, like, would it ever be useful to have the same student appear multiple times", "tokens": [51220, 1436, 534, 11, 411, 11, 576, 309, 1562, 312, 4420, 281, 362, 264, 912, 3107, 4204, 3866, 1413, 51434], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 401, "seek": 132128, "start": 1342.68, "end": 1343.8799999999999, "text": " in the same course?", "tokens": [51434, 294, 264, 912, 1164, 30, 51494], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 402, "seek": 132128, "start": 1343.8799999999999, "end": 1344.8799999999999, "text": " Not really.", "tokens": [51494, 1726, 534, 13, 51544], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 403, "seek": 132128, "start": 1344.8799999999999, "end": 1346.8, "text": " That should be a set because sets have uniqueness.", "tokens": [51544, 663, 820, 312, 257, 992, 570, 6352, 362, 48294, 13, 51640], "temperature": 0.0, "avg_logprob": -0.18177490234375, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.00521807512268424}, {"id": 404, "seek": 134680, "start": 1347.2, "end": 1353.56, "text": " We should only have at most one student ID of the same value inside each course.", "tokens": [50384, 492, 820, 787, 362, 412, 881, 472, 3107, 7348, 295, 264, 912, 2158, 1854, 1184, 1164, 13, 50702], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 405, "seek": 134680, "start": 1353.56, "end": 1357.52, "text": " OK, so now let's write a function called students in course that's going to take one", "tokens": [50702, 2264, 11, 370, 586, 718, 311, 2464, 257, 2445, 1219, 1731, 294, 1164, 300, 311, 516, 281, 747, 472, 50900], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 406, "seek": 134680, "start": 1357.52, "end": 1362.28, "text": " of these courses, maybe something inside update, and it's going to return a list of", "tokens": [50900, 295, 613, 7712, 11, 1310, 746, 1854, 5623, 11, 293, 309, 311, 516, 281, 2736, 257, 1329, 295, 51138], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 407, "seek": 134680, "start": 1362.28, "end": 1363.28, "text": " students.", "tokens": [51138, 1731, 13, 51188], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 408, "seek": 134680, "start": 1363.28, "end": 1366.96, "text": " So this is sort of our replacement for we want to work in terms of a list of students,", "tokens": [51188, 407, 341, 307, 1333, 295, 527, 14419, 337, 321, 528, 281, 589, 294, 2115, 295, 257, 1329, 295, 1731, 11, 51372], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 409, "seek": 134680, "start": 1366.96, "end": 1370.32, "text": " which is why we originally said courses just have a list of students, but now we're just", "tokens": [51372, 597, 307, 983, 321, 7993, 848, 7712, 445, 362, 257, 1329, 295, 1731, 11, 457, 586, 321, 434, 445, 51540], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 410, "seek": 134680, "start": 1370.32, "end": 1372.84, "text": " going to do a function to do the same thing.", "tokens": [51540, 516, 281, 360, 257, 2445, 281, 360, 264, 912, 551, 13, 51666], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 411, "seek": 134680, "start": 1372.84, "end": 1374.24, "text": " So it's going to take a course as an argument.", "tokens": [51666, 407, 309, 311, 516, 281, 747, 257, 1164, 382, 364, 6770, 13, 51736], "temperature": 0.0, "avg_logprob": -0.19207289639641256, "compression_ratio": 1.8888888888888888, "no_speech_prob": 0.0007095616310834885}, {"id": 412, "seek": 137424, "start": 1374.68, "end": 1377.8, "text": " And it's going to start off by doing a dicks.filter.", "tokens": [50386, 400, 309, 311, 516, 281, 722, 766, 538, 884, 257, 274, 7663, 13, 19776, 391, 13, 50542], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 413, "seek": 137424, "start": 1377.8, "end": 1379.24, "text": " What's it going to filter on?", "tokens": [50542, 708, 311, 309, 516, 281, 6608, 322, 30, 50614], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 414, "seek": 137424, "start": 1379.24, "end": 1383.36, "text": " It's going to say, OK, I'm going to look at each of the students that I have, and I'm", "tokens": [50614, 467, 311, 516, 281, 584, 11, 2264, 11, 286, 478, 516, 281, 574, 412, 1184, 295, 264, 1731, 300, 286, 362, 11, 293, 286, 478, 50820], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 415, "seek": 137424, "start": 1383.36, "end": 1386.48, "text": " going to say, am I in this particular course?", "tokens": [50820, 516, 281, 584, 11, 669, 286, 294, 341, 1729, 1164, 30, 50976], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 416, "seek": 137424, "start": 1386.48, "end": 1391.2, "text": " The way I can tell if I'm in this particular course is set.member, that particular student's", "tokens": [50976, 440, 636, 286, 393, 980, 498, 286, 478, 294, 341, 1729, 1164, 307, 992, 13, 38249, 11, 300, 1729, 3107, 311, 51212], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 417, "seek": 137424, "start": 1391.2, "end": 1395.04, "text": " ID, and the course's set of student IDs.", "tokens": [51212, 7348, 11, 293, 264, 1164, 311, 992, 295, 3107, 48212, 13, 51404], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 418, "seek": 137424, "start": 1395.04, "end": 1399.56, "text": " So this is going to filter that dictionary of all the students down to just the ones", "tokens": [51404, 407, 341, 307, 516, 281, 6608, 300, 25890, 295, 439, 264, 1731, 760, 281, 445, 264, 2306, 51630], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 419, "seek": 137424, "start": 1399.56, "end": 1401.32, "text": " who are in that particular course.", "tokens": [51630, 567, 366, 294, 300, 1729, 1164, 13, 51718], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 420, "seek": 137424, "start": 1401.32, "end": 1402.2, "text": " And that's it.", "tokens": [51718, 400, 300, 311, 309, 13, 51762], "temperature": 0.0, "avg_logprob": -0.14811584797311336, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.000755293935071677}, {"id": 421, "seek": 140220, "start": 1402.24, "end": 1407.2, "text": " We have now gone from our single source of truth of all students in one expression down", "tokens": [50366, 492, 362, 586, 2780, 490, 527, 2167, 4009, 295, 3494, 295, 439, 1731, 294, 472, 6114, 760, 50614], "temperature": 0.0, "avg_logprob": -0.12890742719173431, "compression_ratio": 1.879120879120879, "no_speech_prob": 0.00011234189878450707}, {"id": 422, "seek": 140220, "start": 1407.2, "end": 1412.28, "text": " to a single source of truth for the list of students that are in that particular course.", "tokens": [50614, 281, 257, 2167, 4009, 295, 3494, 337, 264, 1329, 295, 1731, 300, 366, 294, 300, 1729, 1164, 13, 50868], "temperature": 0.0, "avg_logprob": -0.12890742719173431, "compression_ratio": 1.879120879120879, "no_speech_prob": 0.00011234189878450707}, {"id": 423, "seek": 140220, "start": 1412.28, "end": 1415.28, "text": " And now we can do whatever we want with that list of students, whatever we would have done", "tokens": [50868, 400, 586, 321, 393, 360, 2035, 321, 528, 365, 300, 1329, 295, 1731, 11, 2035, 321, 576, 362, 1096, 51018], "temperature": 0.0, "avg_logprob": -0.12890742719173431, "compression_ratio": 1.879120879120879, "no_speech_prob": 0.00011234189878450707}, {"id": 424, "seek": 140220, "start": 1415.28, "end": 1418.72, "text": " previously by courses.students.", "tokens": [51018, 8046, 538, 7712, 13, 28349, 791, 13, 51190], "temperature": 0.0, "avg_logprob": -0.12890742719173431, "compression_ratio": 1.879120879120879, "no_speech_prob": 0.00011234189878450707}, {"id": 425, "seek": 140220, "start": 1418.72, "end": 1421.88, "text": " And then, of course, if we want to actually get it from a dictionary to a list of students,", "tokens": [51190, 400, 550, 11, 295, 1164, 11, 498, 321, 528, 281, 767, 483, 309, 490, 257, 25890, 281, 257, 1329, 295, 1731, 11, 51348], "temperature": 0.0, "avg_logprob": -0.12890742719173431, "compression_ratio": 1.879120879120879, "no_speech_prob": 0.00011234189878450707}, {"id": 426, "seek": 140220, "start": 1421.88, "end": 1424.6000000000001, "text": " we'd pipe it to dicks.values.", "tokens": [51348, 321, 1116, 11240, 309, 281, 274, 7663, 13, 46033, 13, 51484], "temperature": 0.0, "avg_logprob": -0.12890742719173431, "compression_ratio": 1.879120879120879, "no_speech_prob": 0.00011234189878450707}, {"id": 427, "seek": 140220, "start": 1424.6000000000001, "end": 1429.32, "text": " OK, so here we have an example of immutable relational data where we've maintained a single", "tokens": [51484, 2264, 11, 370, 510, 321, 362, 364, 1365, 295, 3397, 32148, 38444, 1412, 689, 321, 600, 17578, 257, 2167, 51720], "temperature": 0.0, "avg_logprob": -0.12890742719173431, "compression_ratio": 1.879120879120879, "no_speech_prob": 0.00011234189878450707}, {"id": 428, "seek": 142932, "start": 1429.32, "end": 1432.32, "text": " source of truth, which means no synchronization problems.", "tokens": [50364, 4009, 295, 3494, 11, 597, 1355, 572, 19331, 2144, 2740, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 429, "seek": 142932, "start": 1432.32, "end": 1435.8, "text": " Also it means no nested record updates, which is cool.", "tokens": [50514, 2743, 309, 1355, 572, 15646, 292, 2136, 9205, 11, 597, 307, 1627, 13, 50688], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 430, "seek": 142932, "start": 1435.8, "end": 1440.0, "text": " But basically, we no longer have that whole series of problems where we have to keep things", "tokens": [50688, 583, 1936, 11, 321, 572, 2854, 362, 300, 1379, 2638, 295, 2740, 689, 321, 362, 281, 1066, 721, 50898], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 431, "seek": 142932, "start": 1440.0, "end": 1444.8, "text": " updated, keep things in sync, detect errors, resolve conflicts, and so on and so forth.", "tokens": [50898, 10588, 11, 1066, 721, 294, 20271, 11, 5531, 13603, 11, 14151, 19807, 11, 293, 370, 322, 293, 370, 5220, 13, 51138], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 432, "seek": 142932, "start": 1444.8, "end": 1447.8799999999999, "text": " OK, but what about like more complex relationships?", "tokens": [51138, 2264, 11, 457, 437, 466, 411, 544, 3997, 6159, 30, 51292], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 433, "seek": 142932, "start": 1447.8799999999999, "end": 1450.6399999999999, "text": " What if we had the JavaScript equivalent of something like this?", "tokens": [51292, 708, 498, 321, 632, 264, 15778, 10344, 295, 746, 411, 341, 30, 51430], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 434, "seek": 142932, "start": 1450.6399999999999, "end": 1455.32, "text": " Courses bracket zero.students bracket three.assignments bracket two.is done, something where we have", "tokens": [51430, 383, 5067, 279, 16904, 4018, 13, 28349, 791, 16904, 1045, 13, 640, 788, 1117, 16904, 732, 13, 271, 1096, 11, 746, 689, 321, 362, 51664], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 435, "seek": 142932, "start": 1455.32, "end": 1459.2, "text": " lots of relationships and we're trying to merge them all together.", "tokens": [51664, 3195, 295, 6159, 293, 321, 434, 1382, 281, 22183, 552, 439, 1214, 13, 51858], "temperature": 0.0, "avg_logprob": -0.16189702809285775, "compression_ratio": 1.7808641975308641, "no_speech_prob": 0.01743749901652336}, {"id": 436, "seek": 145920, "start": 1459.2, "end": 1461.0800000000002, "text": " How does this sort of like scale?", "tokens": [50364, 1012, 775, 341, 1333, 295, 411, 4373, 30, 50458], "temperature": 0.0, "avg_logprob": -0.19304103477328433, "compression_ratio": 1.620408163265306, "no_speech_prob": 0.0031712648924440145}, {"id": 437, "seek": 145920, "start": 1461.0800000000002, "end": 1465.96, "text": " Well, there's actually, as it turns out, an entire sort of field of study and a lot", "tokens": [50458, 1042, 11, 456, 311, 767, 11, 382, 309, 4523, 484, 11, 364, 2302, 1333, 295, 2519, 295, 2979, 293, 257, 688, 50702], "temperature": 0.0, "avg_logprob": -0.19304103477328433, "compression_ratio": 1.620408163265306, "no_speech_prob": 0.0031712648924440145}, {"id": 438, "seek": 145920, "start": 1465.96, "end": 1469.72, "text": " of implementations that work with these types of questions.", "tokens": [50702, 295, 4445, 763, 300, 589, 365, 613, 3467, 295, 1651, 13, 50890], "temperature": 0.0, "avg_logprob": -0.19304103477328433, "compression_ratio": 1.620408163265306, "no_speech_prob": 0.0031712648924440145}, {"id": 439, "seek": 145920, "start": 1469.72, "end": 1473.1200000000001, "text": " If anyone recognizes this logo, this is a good elephant.", "tokens": [50890, 759, 2878, 26564, 341, 9699, 11, 341, 307, 257, 665, 19791, 13, 51060], "temperature": 0.0, "avg_logprob": -0.19304103477328433, "compression_ratio": 1.620408163265306, "no_speech_prob": 0.0031712648924440145}, {"id": 440, "seek": 145920, "start": 1473.1200000000001, "end": 1474.1200000000001, "text": " This is PostgreSQL.", "tokens": [51060, 639, 307, 10223, 33248, 39934, 13, 51110], "temperature": 0.0, "avg_logprob": -0.19304103477328433, "compression_ratio": 1.620408163265306, "no_speech_prob": 0.0031712648924440145}, {"id": 441, "seek": 145920, "start": 1474.1200000000001, "end": 1479.76, "text": " You also got other databases like MySQL, SQLites, and it turns out relational data has been", "tokens": [51110, 509, 611, 658, 661, 22380, 411, 1222, 39934, 11, 19200, 3324, 11, 293, 309, 4523, 484, 38444, 1412, 575, 668, 51392], "temperature": 0.0, "avg_logprob": -0.19304103477328433, "compression_ratio": 1.620408163265306, "no_speech_prob": 0.0031712648924440145}, {"id": 442, "seek": 145920, "start": 1479.76, "end": 1484.68, "text": " well studied and explored by relational databases.", "tokens": [51392, 731, 9454, 293, 24016, 538, 38444, 22380, 13, 51638], "temperature": 0.0, "avg_logprob": -0.19304103477328433, "compression_ratio": 1.620408163265306, "no_speech_prob": 0.0031712648924440145}, {"id": 443, "seek": 148468, "start": 1484.68, "end": 1492.96, "text": " And we can, what's the word I'm looking for, steal shamelessly from the things that they've", "tokens": [50364, 400, 321, 393, 11, 437, 311, 264, 1349, 286, 478, 1237, 337, 11, 11009, 40164, 356, 490, 264, 721, 300, 436, 600, 50778], "temperature": 0.0, "avg_logprob": -0.12320876539799205, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.019111566245555878}, {"id": 444, "seek": 148468, "start": 1492.96, "end": 1497.4, "text": " learned over the years about dealing with relational data without dealing with mutable", "tokens": [50778, 3264, 670, 264, 924, 466, 6260, 365, 38444, 1412, 1553, 6260, 365, 5839, 712, 51000], "temperature": 0.0, "avg_logprob": -0.12320876539799205, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.019111566245555878}, {"id": 445, "seek": 148468, "start": 1497.4, "end": 1501.28, "text": " references, which they don't use because they need to serialize everything to the disk.", "tokens": [51000, 15400, 11, 597, 436, 500, 380, 764, 570, 436, 643, 281, 17436, 1125, 1203, 281, 264, 12355, 13, 51194], "temperature": 0.0, "avg_logprob": -0.12320876539799205, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.019111566245555878}, {"id": 446, "seek": 148468, "start": 1501.28, "end": 1505.92, "text": " So what if we think about dictionaries as sort of like an analogy for database tables?", "tokens": [51194, 407, 437, 498, 321, 519, 466, 22352, 4889, 382, 1333, 295, 411, 364, 21663, 337, 8149, 8020, 30, 51426], "temperature": 0.0, "avg_logprob": -0.12320876539799205, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.019111566245555878}, {"id": 447, "seek": 148468, "start": 1505.92, "end": 1506.92, "text": " What could that do for us?", "tokens": [51426, 708, 727, 300, 360, 337, 505, 30, 51476], "temperature": 0.0, "avg_logprob": -0.12320876539799205, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.019111566245555878}, {"id": 448, "seek": 148468, "start": 1506.92, "end": 1511.44, "text": " So what if we thought about things like select, where, and join in terms of SQL, and then", "tokens": [51476, 407, 437, 498, 321, 1194, 466, 721, 411, 3048, 11, 689, 11, 293, 3917, 294, 2115, 295, 19200, 11, 293, 550, 51702], "temperature": 0.0, "avg_logprob": -0.12320876539799205, "compression_ratio": 1.7153284671532847, "no_speech_prob": 0.019111566245555878}, {"id": 449, "seek": 151144, "start": 1511.44, "end": 1516.76, "text": " we translated those to dictionary concepts like get, filter, intersect, and so forth.", "tokens": [50364, 321, 16805, 729, 281, 25890, 10392, 411, 483, 11, 6608, 11, 27815, 11, 293, 370, 5220, 13, 50630], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 450, "seek": 151144, "start": 1516.76, "end": 1517.76, "text": " How might that look?", "tokens": [50630, 1012, 1062, 300, 574, 30, 50680], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 451, "seek": 151144, "start": 1517.76, "end": 1520.8, "text": " So here's a really, really simple SQL query.", "tokens": [50680, 407, 510, 311, 257, 534, 11, 534, 2199, 19200, 14581, 13, 50832], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 452, "seek": 151144, "start": 1520.8, "end": 1522.56, "text": " Select count star from users.", "tokens": [50832, 13638, 1207, 3543, 490, 5022, 13, 50920], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 453, "seek": 151144, "start": 1522.56, "end": 1525.92, "text": " So that's sort of the equivalent of saying like, tick dot size users.", "tokens": [50920, 407, 300, 311, 1333, 295, 264, 10344, 295, 1566, 411, 11, 5204, 5893, 2744, 5022, 13, 51088], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 454, "seek": 151144, "start": 1525.92, "end": 1526.92, "text": " Ha.", "tokens": [51088, 4064, 13, 51138], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 455, "seek": 151144, "start": 1526.92, "end": 1528.8, "text": " Stake in the slides.", "tokens": [51138, 745, 619, 294, 264, 9788, 13, 51232], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 456, "seek": 151144, "start": 1528.8, "end": 1531.48, "text": " Select count star from users where age is greater than or equal to 18.", "tokens": [51232, 13638, 1207, 3543, 490, 5022, 689, 3205, 307, 5044, 813, 420, 2681, 281, 2443, 13, 51366], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 457, "seek": 151144, "start": 1531.48, "end": 1534.68, "text": " This would be voting age users in the U.S.", "tokens": [51366, 639, 576, 312, 10419, 3205, 5022, 294, 264, 624, 13, 50, 13, 51526], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 458, "seek": 151144, "start": 1534.68, "end": 1538.8400000000001, "text": " So the equivalent to that, dicks dot filter is voting age users, which is kind of nice", "tokens": [51526, 407, 264, 10344, 281, 300, 11, 274, 7663, 5893, 6608, 307, 10419, 3205, 5022, 11, 597, 307, 733, 295, 1481, 51734], "temperature": 0.0, "avg_logprob": -0.2243164669383656, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.0488276444375515}, {"id": 459, "seek": 153884, "start": 1538.84, "end": 1542.28, "text": " because it's actually a little bit more descriptive than the greater than or equal to 18.", "tokens": [50364, 570, 309, 311, 767, 257, 707, 857, 544, 42585, 813, 264, 5044, 813, 420, 2681, 281, 2443, 13, 50536], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 460, "seek": 153884, "start": 1542.28, "end": 1543.6799999999998, "text": " And then pipe that to dicks dot size.", "tokens": [50536, 400, 550, 11240, 300, 281, 274, 7663, 5893, 2744, 13, 50606], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 461, "seek": 153884, "start": 1543.6799999999998, "end": 1546.1999999999998, "text": " I got it right on this slide.", "tokens": [50606, 286, 658, 309, 558, 322, 341, 4137, 13, 50732], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 462, "seek": 153884, "start": 1546.1999999999998, "end": 1549.56, "text": " So this is like a slightly more advanced query.", "tokens": [50732, 407, 341, 307, 411, 257, 4748, 544, 7339, 14581, 13, 50900], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 463, "seek": 153884, "start": 1549.56, "end": 1552.04, "text": " So what if we just, okay, let's just go for it.", "tokens": [50900, 407, 437, 498, 321, 445, 11, 1392, 11, 718, 311, 445, 352, 337, 309, 13, 51024], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 464, "seek": 153884, "start": 1552.04, "end": 1553.04, "text": " Let's do joins.", "tokens": [51024, 961, 311, 360, 24397, 13, 51074], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 465, "seek": 153884, "start": 1553.04, "end": 1554.04, "text": " Let's do where.", "tokens": [51074, 961, 311, 360, 689, 13, 51124], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 466, "seek": 153884, "start": 1554.04, "end": 1555.6, "text": " Let's just get some more stuff in there.", "tokens": [51124, 961, 311, 445, 483, 512, 544, 1507, 294, 456, 13, 51202], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 467, "seek": 153884, "start": 1555.6, "end": 1558.56, "text": " Select count star from users where age is greater than or equal to 18.", "tokens": [51202, 13638, 1207, 3543, 490, 5022, 689, 3205, 307, 5044, 813, 420, 2681, 281, 2443, 13, 51350], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 468, "seek": 153884, "start": 1558.56, "end": 1565.0, "text": " So this is voting age users inner join residence on residence.id equals users.id where residence.city", "tokens": [51350, 407, 341, 307, 10419, 3205, 5022, 7284, 3917, 19607, 322, 19607, 13, 327, 6915, 5022, 13, 327, 689, 19607, 13, 26536, 51672], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 469, "seek": 153884, "start": 1565.0, "end": 1566.0, "text": " equals stl.", "tokens": [51672, 6915, 342, 75, 13, 51722], "temperature": 0.0, "avg_logprob": -0.1791064295275458, "compression_ratio": 1.8381294964028776, "no_speech_prob": 0.007575714495033026}, {"id": 470, "seek": 156600, "start": 1566.0, "end": 1573.28, "text": " And now we're saying who are, like, how many users are a voting age and live in St. Louis?", "tokens": [50364, 400, 586, 321, 434, 1566, 567, 366, 11, 411, 11, 577, 867, 5022, 366, 257, 10419, 3205, 293, 1621, 294, 745, 13, 9763, 30, 50728], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 471, "seek": 156600, "start": 1573.28, "end": 1575.28, "text": " How do we do that using dictionaries?", "tokens": [50728, 1012, 360, 321, 360, 300, 1228, 22352, 4889, 30, 50828], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 472, "seek": 156600, "start": 1575.28, "end": 1580.0, "text": " Well, let's start by defining a couple of type aliases because we can't just, you know,", "tokens": [50828, 1042, 11, 718, 311, 722, 538, 17827, 257, 1916, 295, 2010, 10198, 1957, 570, 321, 393, 380, 445, 11, 291, 458, 11, 51064], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 473, "seek": 156600, "start": 1580.0, "end": 1581.0, "text": " use a plain old dictionary.", "tokens": [51064, 764, 257, 11121, 1331, 25890, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 474, "seek": 156600, "start": 1581.0, "end": 1583.04, "text": " We're going to need to get some records involved here.", "tokens": [51114, 492, 434, 516, 281, 643, 281, 483, 512, 7724, 3288, 510, 13, 51216], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 475, "seek": 156600, "start": 1583.04, "end": 1586.0, "text": " So let's say we have user, which is user ID and age.", "tokens": [51216, 407, 718, 311, 584, 321, 362, 4195, 11, 597, 307, 4195, 7348, 293, 3205, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 476, "seek": 156600, "start": 1586.0, "end": 1590.68, "text": " And then we also have a model which has users, which is a dictionary between user ID and", "tokens": [51364, 400, 550, 321, 611, 362, 257, 2316, 597, 575, 5022, 11, 597, 307, 257, 25890, 1296, 4195, 7348, 293, 51598], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 477, "seek": 156600, "start": 1590.68, "end": 1591.68, "text": " user.", "tokens": [51598, 4195, 13, 51648], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 478, "seek": 156600, "start": 1591.68, "end": 1595.8, "text": " And we have residence, which is a dictionary between user ID and city ID or user ID and", "tokens": [51648, 400, 321, 362, 19607, 11, 597, 307, 257, 25890, 1296, 4195, 7348, 293, 2307, 7348, 420, 4195, 7348, 293, 51854], "temperature": 0.0, "avg_logprob": -0.19155074454642632, "compression_ratio": 1.8641114982578397, "no_speech_prob": 0.012049523182213306}, {"id": 479, "seek": 159580, "start": 1595.8, "end": 1596.84, "text": " city.", "tokens": [50364, 2307, 13, 50416], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 480, "seek": 159580, "start": 1596.84, "end": 1598.84, "text": " So these are, like, examples.", "tokens": [50416, 407, 613, 366, 11, 411, 11, 5110, 13, 50516], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 481, "seek": 159580, "start": 1598.84, "end": 1602.24, "text": " You can imagine many more dictionaries all at the top level, but you can model as many", "tokens": [50516, 509, 393, 3811, 867, 544, 22352, 4889, 439, 412, 264, 1192, 1496, 11, 457, 291, 393, 2316, 382, 867, 50686], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 482, "seek": 159580, "start": 1602.24, "end": 1606.72, "text": " relationships between as many different entities as we want, just like how databases have tables", "tokens": [50686, 6159, 1296, 382, 867, 819, 16667, 382, 321, 528, 11, 445, 411, 577, 22380, 362, 8020, 50910], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 483, "seek": 159580, "start": 1606.72, "end": 1611.6, "text": " all at the top level, which model relationships between as many different tables as you want.", "tokens": [50910, 439, 412, 264, 1192, 1496, 11, 597, 2316, 6159, 1296, 382, 867, 819, 8020, 382, 291, 528, 13, 51154], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 484, "seek": 159580, "start": 1611.6, "end": 1614.28, "text": " So here's how we might implement this query.", "tokens": [51154, 407, 510, 311, 577, 321, 1062, 4445, 341, 14581, 13, 51288], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 485, "seek": 159580, "start": 1614.28, "end": 1617.6399999999999, "text": " So we could say locals, which is to say St. Louis locals or St. Louisans or if you're", "tokens": [51288, 407, 321, 727, 584, 23335, 11, 597, 307, 281, 584, 745, 13, 9763, 23335, 420, 745, 13, 9763, 599, 420, 498, 291, 434, 51456], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 486, "seek": 159580, "start": 1617.6399999999999, "end": 1620.72, "text": " part of Nellie's crew, St. Lunatics.", "tokens": [51456, 644, 295, 426, 898, 414, 311, 7260, 11, 745, 13, 32077, 30292, 13, 51610], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 487, "seek": 159580, "start": 1620.72, "end": 1625.12, "text": " And we would say dix.filter, user ID, city, so that user ID being the key and then the", "tokens": [51610, 400, 321, 576, 584, 274, 970, 13, 19776, 391, 11, 4195, 7348, 11, 2307, 11, 370, 300, 4195, 7348, 885, 264, 2141, 293, 550, 264, 51830], "temperature": 0.0, "avg_logprob": -0.16469094660375025, "compression_ratio": 1.838187702265372, "no_speech_prob": 0.002322196029126644}, {"id": 488, "seek": 162512, "start": 1625.12, "end": 1628.9199999999998, "text": " city being the value, city.name, double equals stl.", "tokens": [50364, 2307, 885, 264, 2158, 11, 2307, 13, 16344, 11, 3834, 6915, 342, 75, 13, 50554], "temperature": 0.0, "avg_logprob": -0.14514106692689838, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0032723459880799055}, {"id": 489, "seek": 162512, "start": 1628.9199999999998, "end": 1633.32, "text": " So this is going to give us, among all the residents, only those whose city is St. Louis.", "tokens": [50554, 407, 341, 307, 516, 281, 976, 505, 11, 3654, 439, 264, 9630, 11, 787, 729, 6104, 2307, 307, 745, 13, 9763, 13, 50774], "temperature": 0.0, "avg_logprob": -0.14514106692689838, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0032723459880799055}, {"id": 490, "seek": 162512, "start": 1633.32, "end": 1638.12, "text": " Then we can do dix.filter on isvotingage, model.users, just like before.", "tokens": [50774, 1396, 321, 393, 360, 274, 970, 13, 19776, 391, 322, 307, 85, 17001, 609, 11, 2316, 13, 301, 433, 11, 445, 411, 949, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14514106692689838, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0032723459880799055}, {"id": 491, "seek": 162512, "start": 1638.12, "end": 1641.84, "text": " So now we have a dictionary of all the users who are voting age.", "tokens": [51014, 407, 586, 321, 362, 257, 25890, 295, 439, 264, 5022, 567, 366, 10419, 3205, 13, 51200], "temperature": 0.0, "avg_logprob": -0.14514106692689838, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0032723459880799055}, {"id": 492, "seek": 162512, "start": 1641.84, "end": 1647.52, "text": " And then to do the join, we do dix.intersect locals, which is going to take all of the", "tokens": [51200, 400, 550, 281, 360, 264, 3917, 11, 321, 360, 274, 970, 13, 5106, 9632, 23335, 11, 597, 307, 516, 281, 747, 439, 295, 264, 51484], "temperature": 0.0, "avg_logprob": -0.14514106692689838, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0032723459880799055}, {"id": 493, "seek": 162512, "start": 1647.52, "end": 1651.6799999999998, "text": " users that we've filtered out for just the ones that have voting age, and then also all", "tokens": [51484, 5022, 300, 321, 600, 37111, 484, 337, 445, 264, 2306, 300, 362, 10419, 3205, 11, 293, 550, 611, 439, 51692], "temperature": 0.0, "avg_logprob": -0.14514106692689838, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0032723459880799055}, {"id": 494, "seek": 165168, "start": 1651.68, "end": 1656.52, "text": " of those that have city name of St. Louis, because that's where they live.", "tokens": [50364, 295, 729, 300, 362, 2307, 1315, 295, 745, 13, 9763, 11, 570, 300, 311, 689, 436, 1621, 13, 50606], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 495, "seek": 165168, "start": 1656.52, "end": 1658.8400000000001, "text": " And then finally, dix.size.", "tokens": [50606, 400, 550, 2721, 11, 274, 970, 13, 27553, 13, 50722], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 496, "seek": 165168, "start": 1658.8400000000001, "end": 1662.6000000000001, "text": " So just by using a couple of simple expressions, we end up with being able to essentially do", "tokens": [50722, 407, 445, 538, 1228, 257, 1916, 295, 2199, 15277, 11, 321, 917, 493, 365, 885, 1075, 281, 4476, 360, 50910], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 497, "seek": 165168, "start": 1662.6000000000001, "end": 1666.04, "text": " the same types of things that we could do in a relational database.", "tokens": [50910, 264, 912, 3467, 295, 721, 300, 321, 727, 360, 294, 257, 38444, 8149, 13, 51082], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 498, "seek": 165168, "start": 1666.04, "end": 1670.1200000000001, "text": " This is like pretty powerful stuff, and it's all with a single source of truth.", "tokens": [51082, 639, 307, 411, 1238, 4005, 1507, 11, 293, 309, 311, 439, 365, 257, 2167, 4009, 295, 3494, 13, 51286], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 499, "seek": 165168, "start": 1670.1200000000001, "end": 1671.88, "text": " And no nested record updates.", "tokens": [51286, 400, 572, 15646, 292, 2136, 9205, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 500, "seek": 165168, "start": 1671.88, "end": 1672.88, "text": " Pretty cool.", "tokens": [51374, 10693, 1627, 13, 51424], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 501, "seek": 165168, "start": 1672.88, "end": 1673.88, "text": " Okay.", "tokens": [51424, 1033, 13, 51474], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 502, "seek": 165168, "start": 1673.88, "end": 1677.4, "text": " If you're curious to learn some more data modeling techniques, completely shameless", "tokens": [51474, 759, 291, 434, 6369, 281, 1466, 512, 544, 1412, 15983, 7512, 11, 2584, 40164, 51650], "temperature": 0.0, "avg_logprob": -0.1552656389051868, "compression_ratio": 1.5813953488372092, "no_speech_prob": 0.0030747835990041494}, {"id": 503, "seek": 167740, "start": 1677.4, "end": 1678.4, "text": " plug.", "tokens": [50364, 5452, 13, 50414], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 504, "seek": 167740, "start": 1678.4, "end": 1681.76, "text": " There's this book by a dude, Elman Action.", "tokens": [50414, 821, 311, 341, 1446, 538, 257, 6449, 11, 2699, 1601, 16261, 13, 50582], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 505, "seek": 167740, "start": 1681.76, "end": 1682.76, "text": " So check it out.", "tokens": [50582, 407, 1520, 309, 484, 13, 50632], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 506, "seek": 167740, "start": 1682.76, "end": 1683.76, "text": " Okay.", "tokens": [50632, 1033, 13, 50682], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 507, "seek": 167740, "start": 1683.76, "end": 1684.76, "text": " So let's recap.", "tokens": [50682, 407, 718, 311, 20928, 13, 50732], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 508, "seek": 167740, "start": 1684.76, "end": 1689.2800000000002, "text": " So first we talked about state synchronization, sort of some of the problems, some of the", "tokens": [50732, 407, 700, 321, 2825, 466, 1785, 19331, 2144, 11, 1333, 295, 512, 295, 264, 2740, 11, 512, 295, 264, 50958], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 509, "seek": 167740, "start": 1689.2800000000002, "end": 1690.2800000000002, "text": " pitfalls.", "tokens": [50958, 10147, 18542, 13, 51008], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 510, "seek": 167740, "start": 1690.2800000000002, "end": 1694.44, "text": " We talked about relational data, and then we talked about immutable relational data without", "tokens": [51008, 492, 2825, 466, 38444, 1412, 11, 293, 550, 321, 2825, 466, 3397, 32148, 38444, 1412, 1553, 51216], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 511, "seek": 167740, "start": 1694.44, "end": 1696.44, "text": " using mutable references.", "tokens": [51216, 1228, 5839, 712, 15400, 13, 51316], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 512, "seek": 167740, "start": 1696.44, "end": 1700.64, "text": " We talked about how a single source of truth means there's nothing to synchronize.", "tokens": [51316, 492, 2825, 466, 577, 257, 2167, 4009, 295, 3494, 1355, 456, 311, 1825, 281, 19331, 1125, 13, 51526], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 513, "seek": 167740, "start": 1700.64, "end": 1704.3600000000001, "text": " All of those problems that we saw with state synchronization, completely out the window,", "tokens": [51526, 1057, 295, 729, 2740, 300, 321, 1866, 365, 1785, 19331, 2144, 11, 2584, 484, 264, 4910, 11, 51712], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 514, "seek": 167740, "start": 1704.3600000000001, "end": 1706.92, "text": " none of those tweet-related problems.", "tokens": [51712, 6022, 295, 729, 15258, 12, 12004, 2740, 13, 51840], "temperature": 0.0, "avg_logprob": -0.19619526828292513, "compression_ratio": 1.921641791044776, "no_speech_prob": 0.004329944495111704}, {"id": 515, "seek": 170692, "start": 1706.96, "end": 1710.92, "text": " We talked about the synchronization work that has to be done if we do have multiple sources", "tokens": [50366, 492, 2825, 466, 264, 19331, 2144, 589, 300, 575, 281, 312, 1096, 498, 321, 360, 362, 3866, 7139, 50564], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 516, "seek": 170692, "start": 1710.92, "end": 1714.88, "text": " of truth, making updates, detecting errors, resolving conflicts, gracefully recovering", "tokens": [50564, 295, 3494, 11, 1455, 9205, 11, 40237, 13603, 11, 49940, 19807, 11, 10042, 2277, 29180, 50762], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 517, "seek": 170692, "start": 1714.88, "end": 1716.44, "text": " in the ideal case.", "tokens": [50762, 294, 264, 7157, 1389, 13, 50840], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 518, "seek": 170692, "start": 1716.44, "end": 1720.04, "text": " And we talked about how relational data can be done with a single source of truth, and", "tokens": [50840, 400, 321, 2825, 466, 577, 38444, 1412, 393, 312, 1096, 365, 257, 2167, 4009, 295, 3494, 11, 293, 51020], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 519, "seek": 170692, "start": 1720.04, "end": 1723.8400000000001, "text": " in fact, immutable relational data can still be done with a single source of truth, which", "tokens": [51020, 294, 1186, 11, 3397, 32148, 38444, 1412, 393, 920, 312, 1096, 365, 257, 2167, 4009, 295, 3494, 11, 597, 51210], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 520, "seek": 170692, "start": 1723.8400000000001, "end": 1728.16, "text": " is really what we would prefer to do if we have the option.", "tokens": [51210, 307, 534, 437, 321, 576, 4382, 281, 360, 498, 321, 362, 264, 3614, 13, 51426], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 521, "seek": 170692, "start": 1728.16, "end": 1732.4, "text": " Then we talked about dictionaries as tables, and sort of how we can use the metaphor of", "tokens": [51426, 1396, 321, 2825, 466, 22352, 4889, 382, 8020, 11, 293, 1333, 295, 577, 321, 393, 764, 264, 19157, 295, 51638], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 522, "seek": 170692, "start": 1732.4, "end": 1736.76, "text": " tables that appear in relational databases to do the same kinds of data modeling activities", "tokens": [51638, 8020, 300, 4204, 294, 38444, 22380, 281, 360, 264, 912, 3685, 295, 1412, 15983, 5354, 51856], "temperature": 0.0, "avg_logprob": -0.14446909207824274, "compression_ratio": 1.9492063492063492, "no_speech_prob": 0.0002780032518785447}, {"id": 523, "seek": 173676, "start": 1737.04, "end": 1738.04, "text": " using dictionaries.", "tokens": [50378, 1228, 22352, 4889, 13, 50428], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 524, "seek": 173676, "start": 1738.04, "end": 1741.84, "text": " And finally, we come back to the original two questions that I had.", "tokens": [50428, 400, 2721, 11, 321, 808, 646, 281, 264, 3380, 732, 1651, 300, 286, 632, 13, 50618], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 525, "seek": 173676, "start": 1741.84, "end": 1744.24, "text": " When should I put duplicate information in my model?", "tokens": [50618, 1133, 820, 286, 829, 23976, 1589, 294, 452, 2316, 30, 50738], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 526, "seek": 173676, "start": 1744.24, "end": 1748.8, "text": " The answer is basically almost never, unless performance absolutely demands a cache, and", "tokens": [50738, 440, 1867, 307, 1936, 1920, 1128, 11, 5969, 3389, 3122, 15107, 257, 19459, 11, 293, 50966], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 527, "seek": 173676, "start": 1748.8, "end": 1752.48, "text": " I'm really sure I have a performance problem, and I really just can't avoid it.", "tokens": [50966, 286, 478, 534, 988, 286, 362, 257, 3389, 1154, 11, 293, 286, 534, 445, 393, 380, 5042, 309, 13, 51150], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 528, "seek": 173676, "start": 1752.48, "end": 1755.84, "text": " That's really the only situation I've ever come across where I would say that that's", "tokens": [51150, 663, 311, 534, 264, 787, 2590, 286, 600, 1562, 808, 2108, 689, 286, 576, 584, 300, 300, 311, 51318], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 529, "seek": 173676, "start": 1755.84, "end": 1757.56, "text": " a good idea.", "tokens": [51318, 257, 665, 1558, 13, 51404], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 530, "seek": 173676, "start": 1757.56, "end": 1759.72, "text": " And how would I do something like this in Elm?", "tokens": [51404, 400, 577, 576, 286, 360, 746, 411, 341, 294, 2699, 76, 30, 51512], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 531, "seek": 173676, "start": 1759.72, "end": 1765.6, "text": " Of course, it's bracket zero.studentbracketzero.selected equals true using dictionaries as tables.", "tokens": [51512, 2720, 1164, 11, 309, 311, 16904, 4018, 13, 372, 24064, 1443, 501, 302, 32226, 13, 405, 1809, 292, 6915, 2074, 1228, 22352, 4889, 382, 8020, 13, 51806], "temperature": 0.0, "avg_logprob": -0.22155138829371313, "compression_ratio": 1.696319018404908, "no_speech_prob": 0.003823706414550543}, {"id": 532, "seek": 176560, "start": 1765.6, "end": 1766.6399999999999, "text": " Thanks very much.", "tokens": [50366, 2561, 588, 709, 13, 50416], "temperature": 0.0, "avg_logprob": -0.601445198059082, "compression_ratio": 0.68, "no_speech_prob": 0.10019158571958542}], "language": "en"}