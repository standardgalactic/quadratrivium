1
00:00:00,000 --> 00:00:11,560
OpenAI, a company that we all know now, but only a year ago, was 100 people, is changing

2
00:00:11,560 --> 00:00:12,560
the world.

3
00:00:12,560 --> 00:00:15,560
Their research is leading the charge to AGI.

4
00:00:15,560 --> 00:00:19,800
Since ChatGPT captured consumer attention last November, they show no signs of slowing

5
00:00:19,800 --> 00:00:21,080
down.

6
00:00:21,080 --> 00:00:25,680
This week, a lot of nice sit down with Ilya Sutskover, co-founder and chief scientist

7
00:00:25,680 --> 00:00:31,880
at OpenAI to discuss the state of AI research, where will hit limits, the future of AGI,

8
00:00:31,880 --> 00:00:33,920
and what it's going to take to reach super alignment.

9
00:00:33,920 --> 00:00:36,240
Ilya, welcome to NoPriors.

10
00:00:36,240 --> 00:00:37,240
Thank you.

11
00:00:37,240 --> 00:00:38,240
It's good to be here.

12
00:00:38,240 --> 00:00:39,240
Let's start at the beginning.

13
00:00:39,240 --> 00:00:43,400
Pre-AlexNet, nothing in deep learning was really working, and then given that environment,

14
00:00:43,400 --> 00:00:46,360
you guys took a very unique bet.

15
00:00:46,360 --> 00:00:48,640
What motivated you to go in this direction?

16
00:00:49,400 --> 00:00:59,160
In those dark ages, AI was not an area where people had hope, and people were not accustomed

17
00:00:59,160 --> 00:01:02,040
to any kind of success at all.

18
00:01:02,040 --> 00:01:06,840
Because there hasn't been any success, there was a lot of debate, and there were different

19
00:01:06,840 --> 00:01:13,360
schools of thoughts that had different arguments about how machine learning and AI should be.

20
00:01:13,360 --> 00:01:18,760
You had people who were into knowledge representation from a good old fashioned AI.

21
00:01:18,760 --> 00:01:23,280
You had people who were Bayesian, and they liked Bayesian non-parametric methods.

22
00:01:23,280 --> 00:01:27,360
You had people who like graphical models, and you had the people who like neural networks.

23
00:01:27,360 --> 00:01:33,480
Those people were marginalized because neural networks did not have the property that you

24
00:01:33,480 --> 00:01:35,640
can't prove math theorems about them.

25
00:01:35,640 --> 00:01:39,880
If you can't prove theorems about something, it means that your research isn't good.

26
00:01:39,880 --> 00:01:41,520
That's how it has been.

27
00:01:41,520 --> 00:01:45,680
The reason why I gravitated to neural networks from the beginning is because it felt like

28
00:01:45,680 --> 00:01:48,000
those are small little brains.

29
00:01:48,000 --> 00:01:50,280
Who cares if you can't prove any theorems about them?

30
00:01:50,280 --> 00:01:56,560
Because we are training small little brains, and maybe they'll do something one day.

31
00:01:56,560 --> 00:02:01,680
The reason that we were able to do AlexNet when we did it is because of a combination

32
00:02:01,680 --> 00:02:04,400
of two factors, three factors.

33
00:02:04,400 --> 00:02:10,760
The first factor is that this was shortly after GPUs started to be used in machine learning.

34
00:02:10,760 --> 00:02:15,560
People had an intuition that that's a good thing to do, but it wasn't like today where

35
00:02:15,560 --> 00:02:17,560
people exactly knew what an NGPU is for.

36
00:02:17,560 --> 00:02:21,560
It was like, let's play with those cool fast computers and see what we can do with them.

37
00:02:21,560 --> 00:02:27,160
It was an especially good fit for neural networks, so that definitely helped us.

38
00:02:27,160 --> 00:02:34,280
I was very fortunate in that I was able to realize that the reason neural networks of

39
00:02:34,280 --> 00:02:37,720
the time weren't good is because they were too small.

40
00:02:37,720 --> 00:02:43,880
If you try to solve a vision task with a neural network which has like a thousand neurons,

41
00:02:43,880 --> 00:02:45,040
what can it do?

42
00:02:45,040 --> 00:02:46,520
It can't do anything.

43
00:02:46,520 --> 00:02:50,120
It doesn't matter how good your learning is and everything else, but if you have a much

44
00:02:50,120 --> 00:02:53,240
larger neural network, it will do something unprecedented.

45
00:02:53,240 --> 00:02:56,680
I'll give you the intuition to think that that was the case, because I think at the

46
00:02:56,680 --> 00:03:02,680
time it was reasonably contrarian to think that despite the human brain in some sense

47
00:03:02,680 --> 00:03:05,360
works that way or different biological neural circuits.

48
00:03:05,360 --> 00:03:09,600
I'm just curious what gave you that intuition early on to think that this was a good direction.

49
00:03:09,600 --> 00:03:18,680
I think looking at the brain and specifically, if all those things follow very easily, if

50
00:03:18,680 --> 00:03:27,880
you allow yourself to accept the idea, right now this idea is reasonably well accepted.

51
00:03:27,880 --> 00:03:32,480
Back then, people still talked about it, but they haven't really accepted it or internalized.

52
00:03:32,480 --> 00:03:38,700
The idea that maybe an artificial neuron in some sense is not that different from a biological

53
00:03:38,700 --> 00:03:40,200
neuron.

54
00:03:40,200 --> 00:03:45,600
Now whatever you imagine animals do with their brains, you could perhaps assemble some artificial

55
00:03:45,600 --> 00:03:47,800
neural network of similar size.

56
00:03:47,800 --> 00:03:53,320
Maybe if you train it, it will do something similar.

57
00:03:53,320 --> 00:03:57,280
That leads you to start to imagine.

58
00:03:57,280 --> 00:03:59,880
Almost imagine the computation being done by the neural network.

59
00:03:59,880 --> 00:04:05,520
You can almost think like if you have a high resolution image and you have like one neuron

60
00:04:05,520 --> 00:04:07,960
for like a large group of pixels, what can the neuron do?

61
00:04:07,960 --> 00:04:11,800
It's just not much it can do, but if you have a lot of neurons, then they can actually do

62
00:04:11,800 --> 00:04:14,000
something and compute something.

63
00:04:14,000 --> 00:04:19,440
So I think it was like, all right, like it was considerations like this, plus a technical

64
00:04:19,440 --> 00:04:20,440
realization.

65
00:04:20,440 --> 00:04:29,240
The technical realization is that if you have a large training set that specifies the behavior

66
00:04:29,240 --> 00:04:35,440
of the neural network, and the training set is large enough such that it can constrain

67
00:04:35,440 --> 00:04:37,960
the large neural network sufficiently.

68
00:04:37,960 --> 00:04:42,000
And furthermore, if you have the algorithm to find that neural network, because what

69
00:04:42,000 --> 00:04:48,240
we do is that we turn the training set into a neural network which satisfies the training

70
00:04:48,240 --> 00:04:49,360
set.

71
00:04:49,360 --> 00:04:56,720
Neural network training can almost be seen as solving a neural equation.

72
00:04:56,720 --> 00:05:01,680
Solving a neural equation where every data point is an equation, and every parameter

73
00:05:01,680 --> 00:05:05,000
is a variable.

74
00:05:05,000 --> 00:05:07,960
And so it was multiple things.

75
00:05:07,960 --> 00:05:12,480
The realization that the bigger neural network could do something unprecedented.

76
00:05:12,480 --> 00:05:19,480
The realization that if you have a large data set together with the compute to solve the

77
00:05:19,480 --> 00:05:24,480
neural equation, that's what gradient descent comes in, but it's not gradient descent.

78
00:05:24,480 --> 00:05:26,200
Gradient descent was around for a long time.

79
00:05:26,200 --> 00:05:30,320
It was certain technical insights about how to make it work.

80
00:05:30,320 --> 00:05:33,480
Because back then the prevailing belief was, well, you can't train those neural nets anything.

81
00:05:33,480 --> 00:05:34,480
It's all hopeless.

82
00:05:34,480 --> 00:05:35,760
So it wasn't just about the site.

83
00:05:35,760 --> 00:05:40,920
It was about, even if someone did think, gosh, it would be cool to train a big neural net,

84
00:05:40,920 --> 00:05:46,320
they didn't have the technical ability to turn this idea into reality.

85
00:05:46,320 --> 00:05:50,920
You needed not only to code the neural net, you need to do a bunch of things right, and

86
00:05:50,920 --> 00:05:52,920
only then it will work.

87
00:05:52,920 --> 00:05:57,840
And then another fortunate thing is that the person whom I work with, Alex Krzyzewski,

88
00:05:57,840 --> 00:06:02,920
he just discovered that he really loves GPUs, and he was perhaps one of the first person

89
00:06:02,920 --> 00:06:10,120
who really mastered writing really, like, really performant code for the GPUs.

90
00:06:10,120 --> 00:06:14,560
And that's why we were able to squeeze a lot of performance out of two GPUs and produce

91
00:06:14,560 --> 00:06:15,640
something unprecedented.

92
00:06:15,640 --> 00:06:19,520
So to sum up, it was multiple things.

93
00:06:19,520 --> 00:06:23,900
The idea that a big neural network, in this case a vision neural network, a convolutional

94
00:06:23,900 --> 00:06:28,640
neural network with many layers, one that's much, much bigger than anything that's ever

95
00:06:28,640 --> 00:06:32,880
been done before, could do something very unprecedented because the brain can see and

96
00:06:32,880 --> 00:06:34,760
the brain is a large neural network.

97
00:06:34,760 --> 00:06:40,080
And we can see quickly, so our neurons don't have a lot of time, then the compute needed,

98
00:06:40,080 --> 00:06:44,880
the technical know-how that, in fact, we can't train such neural networks.

99
00:06:44,880 --> 00:06:46,920
And it was not at all widely distributed.

100
00:06:46,920 --> 00:06:51,120
People in machine learning would not have been able to train such a neural network even

101
00:06:51,120 --> 00:06:52,120
if they wanted to.

102
00:06:52,120 --> 00:06:57,960
Did you guys have any, like, particular goal from a size perspective?

103
00:06:57,960 --> 00:07:02,400
Or was it just as, you know, and if that's biologically inspired or where that number

104
00:07:02,400 --> 00:07:05,000
comes from or just as large as we can go?

105
00:07:05,000 --> 00:07:06,000
Definitely as large as we can go.

106
00:07:06,000 --> 00:07:10,560
Just keep in mind, I mean, we had a certain amount of compute which we could usefully

107
00:07:10,560 --> 00:07:14,080
consume, and then what can it do?

108
00:07:14,080 --> 00:07:21,240
Maybe if we think about just like the origin of open AI and the goals of the organization,

109
00:07:21,240 --> 00:07:24,760
like what was the original goal and how's that evolved over time?

110
00:07:24,760 --> 00:07:30,960
The goal did not evolve over time, the tactic evolved over time.

111
00:07:30,960 --> 00:07:37,920
So the goal of open AI from the very beginning has been to make sure that artificial general

112
00:07:37,920 --> 00:07:45,520
intelligence, by which we mean autonomous systems, AI, that can actually do most of

113
00:07:45,520 --> 00:07:51,480
the jobs and the activities and tasks that people do, benefits all of humanity.

114
00:07:51,480 --> 00:07:53,680
That was the goal from the beginning.

115
00:07:53,680 --> 00:07:59,720
The initial thinking has been that maybe the best way to do it is by just open sourcing

116
00:07:59,720 --> 00:08:02,320
a lot of technology.

117
00:08:02,320 --> 00:08:08,080
We later, and we also attempted to do it as a nonprofit, seemed very sensible.

118
00:08:08,080 --> 00:08:13,000
This is the goal, nonprofit is the way to do it, what changed.

119
00:08:13,000 --> 00:08:20,240
Some point at open AI, we realized and we were perhaps among the earliest to realize

120
00:08:20,240 --> 00:08:25,200
that to make progress in AI for real, you need a lot of compute.

121
00:08:25,200 --> 00:08:27,400
Now, what does a lot mean?

122
00:08:27,400 --> 00:08:32,800
The appetite for compute is truly endless, as now clearly seen, but we realized that

123
00:08:32,800 --> 00:08:41,360
we will need a lot and a nonprofit wouldn't be the way to get there, wouldn't be able

124
00:08:41,360 --> 00:08:43,120
to build a large cluster with a nonprofit.

125
00:08:43,120 --> 00:08:50,120
That's where we became, we converted into this unusual structure called CAP Profit,

126
00:08:50,120 --> 00:08:53,160
and to my knowledge, we are the only CAP Profit company in the world.

127
00:08:53,760 --> 00:08:59,320
The idea is that investors put in some money, but even if the company does incredibly well,

128
00:08:59,320 --> 00:09:04,760
they don't get more than some multiplier on top of their original investment.

129
00:09:04,760 --> 00:09:10,680
The reason to do this, the reason why that makes sense, there are arguments, one could

130
00:09:10,680 --> 00:09:18,160
make arguments against it as well, but the argument for it is that if you believe that

131
00:09:18,160 --> 00:09:26,320
the technology that we are building, AGI, could potentially be so capable as to do every

132
00:09:26,320 --> 00:09:32,240
single task that people do, does it mean that it might un-employ everyone?

133
00:09:32,240 --> 00:09:37,760
Well, I don't know, but it's not impossible, and if that's the case, it makes sense, it

134
00:09:37,760 --> 00:09:41,400
will make a lot of sense if the company that builds such a technology would not be able

135
00:09:41,400 --> 00:09:46,560
to make infinite, would not be incentivized, rather, to make infinite profits.

136
00:09:46,560 --> 00:09:51,080
I don't know if it will literally play out this way because of competition in AI, so

137
00:09:51,080 --> 00:09:57,200
there will be multiple companies, and I think that will have some unforeseen implications

138
00:09:57,200 --> 00:09:59,880
on the argument which I'm making, but that was the thinking.

139
00:09:59,880 --> 00:10:03,840
I remember visiting the offices back when you were, I think, housed at YC or something,

140
00:10:03,840 --> 00:10:09,240
or cohabited some space there, and at the time, there was a suite of different efforts.

141
00:10:09,240 --> 00:10:14,520
There was robotic arms that were being manipulated, and then there was some video game related

142
00:10:14,520 --> 00:10:16,920
work, which was really cutting edge.

143
00:10:16,920 --> 00:10:20,840
How did you think about how the research agenda evolved, and what really drove it down this

144
00:10:20,840 --> 00:10:25,120
path of transformer-based models and other forms of learning?

145
00:10:25,120 --> 00:10:32,000
So I think it has been evolving over the years from when we started OpenAI.

146
00:10:32,000 --> 00:10:36,120
In the first year, we indeed did some of the more conventional machine learning work.

147
00:10:36,120 --> 00:10:39,680
The conventional machine learning work, I mean, because the world has changed so much,

148
00:10:39,680 --> 00:10:45,640
a lot of things which were known to everyone in 2016 or 2017 are completely and utterly

149
00:10:45,640 --> 00:10:46,640
forgotten.

150
00:10:46,640 --> 00:10:48,720
It's like the Stone Age almost.

151
00:10:48,720 --> 00:10:53,840
So in that Stone Age, the world of machine learning looked very different.

152
00:10:53,840 --> 00:10:58,920
It was dramatically more academic.

153
00:10:58,920 --> 00:11:02,440
The goals, values, and objectives were much more academic.

154
00:11:02,440 --> 00:11:07,240
They were about discovering small bits of knowledge and sharing them with the other researchers

155
00:11:07,240 --> 00:11:10,040
and getting scientific recognition as a result.

156
00:11:10,040 --> 00:11:12,320
And it's a very valid goal, and it's very understandable.

157
00:11:12,320 --> 00:11:14,240
I've been doing AI for 20 years now.

158
00:11:14,240 --> 00:11:17,760
More than half of my time that I spent in AI was in that framework.

159
00:11:17,760 --> 00:11:19,760
And so what do you do?

160
00:11:19,760 --> 00:11:22,240
You write papers, you share your small discoveries.

161
00:11:22,240 --> 00:11:23,240
Two realizations.

162
00:11:23,240 --> 00:11:29,600
The first realization is just at a high level, it doesn't seem like it's the way to go for

163
00:11:29,600 --> 00:11:30,960
a dramatic impact.

164
00:11:30,960 --> 00:11:32,240
And why is that?

165
00:11:32,240 --> 00:11:39,600
Because if you imagine how an AGI should look like, it has to be some kind of a big engineering

166
00:11:39,600 --> 00:11:43,440
project that's using a lot of compute, right?

167
00:11:43,440 --> 00:11:45,760
Even if you don't know how to build it, what that should look like.

168
00:11:45,760 --> 00:11:47,640
You know that this is the ideal you want to strive towards.

169
00:11:47,640 --> 00:11:52,000
So you want to somehow move towards larger projects as opposed to small projects.

170
00:11:52,000 --> 00:11:58,640
So while we attempted a first large project where we trained a neural network to play

171
00:11:58,640 --> 00:12:03,000
a real-time strategy game, as well as the best humans.

172
00:12:03,000 --> 00:12:10,800
It's the Dota 2 project, and it was driven by two people, Jakob Pachotsky and Greg Brokman.

173
00:12:10,800 --> 00:12:13,280
They really drove this project and made it a success.

174
00:12:13,280 --> 00:12:17,280
And this was our first attempt at a large project.

175
00:12:17,280 --> 00:12:21,840
But it wasn't quite the right formula for us, because the neural networks were a little

176
00:12:21,840 --> 00:12:22,840
bit too small.

177
00:12:22,840 --> 00:12:25,000
It was just a narrow domain, just a game.

178
00:12:25,000 --> 00:12:27,840
I mean, it's cool to play a game, and it kept looking.

179
00:12:27,840 --> 00:12:32,400
At some point, we realized that, hey, if you train a large neural network, a very, very

180
00:12:32,400 --> 00:12:38,240
large transformer to predict text better and better, something very surprising will happen.

181
00:12:38,240 --> 00:12:41,000
This realization also arrived a little bit gradually.

182
00:12:41,000 --> 00:12:44,200
We were exploring generative models.

183
00:12:44,200 --> 00:12:48,160
We were exploring ideas around next-word prediction.

184
00:12:48,160 --> 00:12:50,160
Those are ideas also related to compression.

185
00:12:50,160 --> 00:12:52,400
We were exploring them.

186
00:12:52,400 --> 00:12:53,400
The transformer came out.

187
00:12:53,400 --> 00:12:54,400
We got really excited.

188
00:12:54,400 --> 00:12:56,600
We were like, this is the greatest thing.

189
00:12:56,600 --> 00:12:57,840
We're going to do transformers now.

190
00:12:57,840 --> 00:13:00,120
It's clearly superior than anything else before it.

191
00:13:00,120 --> 00:13:02,720
We started doing transformers with the GPT-1.

192
00:13:02,720 --> 00:13:06,520
GPT-1 started to show very interesting signs of life.

193
00:13:06,520 --> 00:13:08,360
And that led us to doing GPT-2.

194
00:13:08,360 --> 00:13:09,880
And then ultimately, GPT-3.

195
00:13:09,880 --> 00:13:15,560
GPT-3 really opened everyone else's eyes as well to, hey, this thing has a lot of traction.

196
00:13:15,560 --> 00:13:19,720
There is one specific formula right now that everyone is doing.

197
00:13:19,720 --> 00:13:24,040
And this formula is train a larger and larger transformer on more and more data.

198
00:13:24,040 --> 00:13:29,440
I mean, for me, the big wake-up moment to your point was GPT-2 to GPT-3 transition, where

199
00:13:29,440 --> 00:13:32,320
you saw such a big step function and capabilities.

200
00:13:32,320 --> 00:13:39,400
And then obviously, with four open eyes, published some really interesting research around some

201
00:13:39,400 --> 00:13:42,840
of the different domains of knowledge or domains of expertise or chain of thought or other

202
00:13:42,840 --> 00:13:46,160
things that the models can suddenly do in an emergent form.

203
00:13:46,160 --> 00:13:49,320
What was the most surprising thing for you in terms of emergent behavior in these models

204
00:13:49,320 --> 00:13:50,320
over time?

205
00:13:50,320 --> 00:13:52,640
You know, it's very hard to answer that question.

206
00:13:52,640 --> 00:13:56,520
It's very hard to answer because I'm too close and I've seen it progress every step of the

207
00:13:56,520 --> 00:13:58,320
way.

208
00:13:58,320 --> 00:14:01,960
So as much as I'd like, I find it very hard to answer that question.

209
00:14:01,960 --> 00:14:09,840
I think if I had to pick one, I think maybe the most surprising thing for me is the whole

210
00:14:09,840 --> 00:14:12,800
thing works at all.

211
00:14:12,800 --> 00:14:13,800
It's hard.

212
00:14:13,800 --> 00:14:20,080
I'm not sure I know how to convey this, what I have in mind here, because if you see a

213
00:14:20,080 --> 00:14:24,200
lot of neural networks do amazing things, well, obviously neural networks is the thing

214
00:14:24,200 --> 00:14:25,640
that works.

215
00:14:25,640 --> 00:14:32,240
But I have witnessed personally what it's like to be in a world for many years where

216
00:14:32,240 --> 00:14:34,760
the neural networks don't work at all.

217
00:14:34,760 --> 00:14:39,520
And then to contrast that to where we are today, just the fact that they work and they

218
00:14:39,520 --> 00:14:41,400
do these amazing things.

219
00:14:41,400 --> 00:14:45,560
I think maybe the most surprising, the most surprising, if I had to pick one, it would

220
00:14:45,560 --> 00:14:49,000
be the fact that when I speak to it, I feel understood.

221
00:14:49,000 --> 00:14:53,640
Yeah, there's a really good saying from, I'm trying to remember, maybe it's Arthur Clark

222
00:14:53,640 --> 00:14:59,800
or one of the sci-fi authors, which is effectively it says advanced technology is sometimes indistinguishable

223
00:14:59,800 --> 00:15:00,800
for magic.

224
00:15:00,800 --> 00:15:03,240
Yeah, I'm fully in this camp.

225
00:15:03,240 --> 00:15:06,960
Yeah, it definitely feels like there's some magical moments with some of these models

226
00:15:06,960 --> 00:15:07,960
now.

227
00:15:08,000 --> 00:15:15,160
A way that you guys decide internally, given all of the different capabilities you could

228
00:15:15,160 --> 00:15:20,160
pursue, how to continually choose the set of big projects, you've sort of described

229
00:15:20,160 --> 00:15:26,480
that centralization and committing to certain research directions at scale is really important

230
00:15:26,480 --> 00:15:28,120
to open AI success.

231
00:15:28,120 --> 00:15:31,920
Given the breadth of opportunity now, what's the process for deciding what's worth working

232
00:15:31,920 --> 00:15:32,920
on?

233
00:15:33,160 --> 00:15:38,560
I mean, I think there is some combination of bottom up and top down, where we have some

234
00:15:38,560 --> 00:15:43,320
top down ideas that we believe should work, but we're not 100% sure.

235
00:15:43,320 --> 00:15:48,560
So we still, we need to have good top down ideas, and there is a lot of bottom up exploration

236
00:15:48,560 --> 00:15:50,920
guided by those top down ideas as well.

237
00:15:50,920 --> 00:15:55,880
And their combination is what informs us as to what to do next.

238
00:15:55,880 --> 00:16:01,400
And if you think about those bottom, I mean, either direction, top down or bottom up ideas,

239
00:16:01,400 --> 00:16:07,200
you like clearly we have this dominant continue to scale transformers direction.

240
00:16:07,200 --> 00:16:12,400
Do you explore additional like architectural directions or is that just not relevant?

241
00:16:12,400 --> 00:16:16,840
Certainly possible that various improvements can be found.

242
00:16:16,840 --> 00:16:20,760
I think I think improvements can be found in all kinds of places, both small improvements

243
00:16:20,760 --> 00:16:22,440
and large improvements.

244
00:16:22,440 --> 00:16:28,480
I think the way to think about it is that while the current thing that's being done keeps

245
00:16:28,560 --> 00:16:34,160
getting better as you keep on increasing the amount of compute and data that you put into

246
00:16:34,160 --> 00:16:35,160
it.

247
00:16:35,160 --> 00:16:39,760
So we have that property, the bigger you make it, the better it gets.

248
00:16:39,760 --> 00:16:46,480
It is also the property that different things get better by different amounts as you keep

249
00:16:46,480 --> 00:16:48,680
on improving, as you keep on scaling them up.

250
00:16:48,680 --> 00:16:52,440
So not only you want to, of course, scale up what we are doing, we also want to keep

251
00:16:52,440 --> 00:16:55,400
scaling up the best thing possible.

252
00:16:55,400 --> 00:17:01,040
What is a, I mean, you probably don't need to predict because you can see internally,

253
00:17:01,040 --> 00:17:05,760
what do you think is improving most from a capability perspective in the current generation

254
00:17:05,760 --> 00:17:06,960
of scale?

255
00:17:06,960 --> 00:17:16,040
The best way for me to answer this question would be to point out the, to point to the

256
00:17:16,040 --> 00:17:18,760
models that are publicly available.

257
00:17:18,760 --> 00:17:23,040
And you can see how they compare from this year to last year.

258
00:17:23,040 --> 00:17:24,520
And the difference is quite significant.

259
00:17:24,560 --> 00:17:28,600
I'm not talking about the difference between, not only the difference between, let's say

260
00:17:28,600 --> 00:17:34,040
you can look at the difference between GPT-3 and GPT-3.5 and then chat GPT, chat GPT-4,

261
00:17:34,040 --> 00:17:37,880
chat GPT-4 with vision, and you can just see for yourself.

262
00:17:37,880 --> 00:17:42,960
It's easy to forget where things used to be, but certainly the big way in which things

263
00:17:42,960 --> 00:17:49,120
are changing is that these models become more and more reliable before they were very, they

264
00:17:49,120 --> 00:17:51,960
were only very partly there.

265
00:17:51,960 --> 00:17:54,720
Right now they are mostly there, but there are still gaps.

266
00:17:54,720 --> 00:17:58,720
And in the future, perhaps these models will be there even more.

267
00:17:58,720 --> 00:18:02,080
You could trust their answers, they'll be more reliable, they'll be able to do more

268
00:18:02,080 --> 00:18:05,360
tasks in general across the board.

269
00:18:05,360 --> 00:18:09,560
And then another thing that they will do is that they'll have deeper insight.

270
00:18:09,560 --> 00:18:15,240
As we train them, they gain more and more insight into the true nature of the human

271
00:18:15,240 --> 00:18:18,640
world and their insight will continue to deepen.

272
00:18:18,640 --> 00:18:23,520
I was just going to ask about how that relates to sort of model scale over time, because

273
00:18:23,520 --> 00:18:29,040
a lot of people are really stricken by the capabilities of the very large-scale models

274
00:18:29,040 --> 00:18:32,240
and the emergent behavior in terms of understanding of the world.

275
00:18:32,240 --> 00:18:35,840
And then in parallel, as people incorporate some of these things into products, which is

276
00:18:35,840 --> 00:18:39,840
a very different type of path, they often start worrying about inference costs going

277
00:18:39,840 --> 00:18:42,680
up with the scale of the model, and therefore they're looking for smaller models that are

278
00:18:42,680 --> 00:18:43,680
fine-tuned.

279
00:18:43,680 --> 00:18:47,360
But then, of course, you may lose some of the capabilities around some of the insights

280
00:18:47,360 --> 00:18:49,680
and ability to reason.

281
00:18:49,680 --> 00:18:53,720
And so I was curious in your thinking in terms of how all this evolves over the coming years.

282
00:18:53,720 --> 00:18:57,120
I would actually point out that the main thing that's lost when you switch to the smaller

283
00:18:57,120 --> 00:18:59,360
models is reliability.

284
00:18:59,360 --> 00:19:06,280
I would argue that at this point, it is reliability that's the biggest bottleneck to these models

285
00:19:06,280 --> 00:19:07,280
being truly useful.

286
00:19:07,280 --> 00:19:09,360
How are you defining reliability?

287
00:19:09,360 --> 00:19:14,960
So it's like when you ask a question that's not much harder than other questions that

288
00:19:14,960 --> 00:19:20,560
the model succeeds at, then you have a very high degree of confidence that it will continue

289
00:19:20,560 --> 00:19:21,560
to succeed.

290
00:19:21,560 --> 00:19:22,560
So I'll give you an example.

291
00:19:22,560 --> 00:19:27,800
Let's suppose that I want to learn about some historical thing, and I can ask, but tell

292
00:19:27,800 --> 00:19:32,800
me what is the prevailing opinion about this and about that, and I can keep asking questions.

293
00:19:32,800 --> 00:19:36,360
And let's suppose I answered 20 of my questions correctly.

294
00:19:36,360 --> 00:19:41,400
I really don't want the 21st question to have a gross mistake.

295
00:19:41,400 --> 00:19:43,240
That's what I mean by reliability.

296
00:19:43,240 --> 00:19:46,600
Or let's suppose I upload some documents, some financial documents.

297
00:19:46,600 --> 00:19:47,600
Suppose they say something.

298
00:19:47,600 --> 00:19:50,920
I want you to do some analysis and to make some conclusion, and I want to take action

299
00:19:50,920 --> 00:19:53,320
on this basis and this conclusion.

300
00:19:53,320 --> 00:19:58,760
And it's not a super hard task, and these models clearly succeed on this task most of

301
00:19:58,760 --> 00:19:59,760
the time.

302
00:19:59,760 --> 00:20:02,520
But because they don't succeed all the time, and if it's a consequential decision, I actually

303
00:20:02,520 --> 00:20:07,280
can't trust the model any of those times, and I have to verify the answer somehow.

304
00:20:07,280 --> 00:20:09,120
So that's how I define reliability.

305
00:20:09,120 --> 00:20:10,640
It's very similar to the cell driving situation.

306
00:20:11,040 --> 00:20:17,040
If you have a cell driving car and it's like, does things mostly well, that's not good enough.

307
00:20:17,040 --> 00:20:21,440
The situation is not as extreme as with a cell driving car, but that's what I mean by reliability.

308
00:20:21,440 --> 00:20:26,360
My perception of reliability is that, to your point, it goes up with model scale, but also

309
00:20:26,360 --> 00:20:31,760
it goes up if you fine tune for specific use cases or instances or data sets.

310
00:20:31,760 --> 00:20:38,000
And so there is that trade-off in terms of size versus specialized fine tuning versus

311
00:20:38,000 --> 00:20:40,080
reliability.

312
00:20:40,080 --> 00:20:45,720
So certainly, people who care about some specific application have every incentive to get the

313
00:20:45,720 --> 00:20:49,640
smallest model working well enough.

314
00:20:49,640 --> 00:20:50,960
I think that's true.

315
00:20:50,960 --> 00:20:51,960
It's undeniable.

316
00:20:51,960 --> 00:20:55,400
I think anyone who cares about a specific application will want the smallest model for

317
00:20:55,400 --> 00:20:56,400
it.

318
00:20:56,400 --> 00:20:57,400
That's self-evident.

319
00:20:57,400 --> 00:21:02,560
I do think, though, that as models continue to get larger and better, then they will unlock

320
00:21:02,560 --> 00:21:06,480
new and unprecedentedly valuable applications.

321
00:21:06,480 --> 00:21:10,000
So yeah, the small models will have their niche for the less interesting applications,

322
00:21:10,000 --> 00:21:11,600
which are still very useful.

323
00:21:11,600 --> 00:21:15,560
And then the bigger models will be delivering on applications.

324
00:21:15,560 --> 00:21:19,040
Okay, let's pick an example.

325
00:21:19,040 --> 00:21:22,160
Consider the task of producing good legal advice.

326
00:21:22,160 --> 00:21:25,200
It's really valuable if you can really trust the answer.

327
00:21:25,200 --> 00:21:27,800
Maybe you need a much bigger model for it, but it justifies the cost.

328
00:21:27,800 --> 00:21:35,760
There's been a lot of investment this year at the 7B in particular, about 7B, 13B, 34B

329
00:21:35,760 --> 00:21:37,760
sizes.

330
00:21:37,760 --> 00:21:40,960
Do you think continued research at those scales is wasted?

331
00:21:40,960 --> 00:21:42,960
No, of course not.

332
00:21:42,960 --> 00:21:52,720
I mean, I think that in the kind of medium term by a high time scale anyway, there will

333
00:21:52,720 --> 00:21:59,680
be an ecosystem, there will be different uses for different model sizes, there will be plenty

334
00:21:59,680 --> 00:22:05,680
of people who are very excited for whom the best 7B model is good enough, they'll be very

335
00:22:05,680 --> 00:22:06,680
happy with it.

336
00:22:06,720 --> 00:22:11,360
And then there'll be plenty of very, very exciting and amazing applications for which

337
00:22:11,360 --> 00:22:13,360
it won't be enough.

338
00:22:13,360 --> 00:22:14,640
I think that's all.

339
00:22:14,640 --> 00:22:21,200
I mean, I think the big models will be better than the small models, but not all applications

340
00:22:21,200 --> 00:22:24,000
will justify the cost of a large model.

341
00:22:24,000 --> 00:22:28,000
What do you think the role of open sources in this ecosystem?

342
00:22:28,000 --> 00:22:29,400
Well, open source is complicated.

343
00:22:29,400 --> 00:22:32,480
I'll describe to you my mental picture.

344
00:22:32,520 --> 00:22:37,400
I think that in the near term, open source is just helping companies produce useful.

345
00:22:39,400 --> 00:22:44,400
Like, let's see, why would one want to have an open source, but use an open source model

346
00:22:44,400 --> 00:22:48,400
instead of a closed source model that's hosted by some other company?

347
00:22:48,400 --> 00:22:57,560
I mean, I think it's very valid to want to be the final decider on the exact way in which

348
00:22:57,560 --> 00:22:59,320
you want your model to be used.

349
00:22:59,320 --> 00:23:04,120
And for you to make the decision of exactly how you want the model to be used and which

350
00:23:04,120 --> 00:23:06,720
use case you wish to support.

351
00:23:06,720 --> 00:23:09,000
And I think there's going to be a lot of demand for open source models.

352
00:23:09,000 --> 00:23:12,080
And I think there will be quite a few companies that will use them.

353
00:23:12,080 --> 00:23:14,760
And I'd imagine that will be the case in the near term.

354
00:23:14,760 --> 00:23:20,800
I would say in the long run, I think the situation with open source models will become more complicated

355
00:23:20,800 --> 00:23:23,680
and I'm not sure what the right answer is there.

356
00:23:23,680 --> 00:23:25,400
Right now it's a little bit difficult to imagine.

357
00:23:25,400 --> 00:23:33,040
So we need to put our future hat, maybe futurist hat, it's not too hard to get into a sci-fi

358
00:23:33,040 --> 00:23:36,800
mood when you remember that we are talking to computers and they understand us.

359
00:23:36,800 --> 00:23:40,120
But so far, these computers, these models actually not very competent.

360
00:23:40,120 --> 00:23:43,440
They can't do tasks at all.

361
00:23:43,440 --> 00:23:50,160
I do think that the will come a day where the level of capability of models will be

362
00:23:50,160 --> 00:23:51,160
very high.

363
00:23:51,160 --> 00:23:55,000
Like in the end of the day, intelligence is power, right?

364
00:23:55,000 --> 00:23:59,560
Right now, these models, their main impact, I would say at least popular impact is primarily

365
00:23:59,560 --> 00:24:03,120
around entertainment and like simple question and answer.

366
00:24:03,120 --> 00:24:05,440
So you talk to a model, wow, this is so cool.

367
00:24:05,440 --> 00:24:09,560
You produce some images, you had a conversation, maybe you had some questions, good answer.

368
00:24:09,560 --> 00:24:17,080
But it's very different from completing some large and complicated task like, what about

369
00:24:17,080 --> 00:24:23,960
if you had a model which could autonomously start and build a large tech company?

370
00:24:24,000 --> 00:24:29,360
I think if these models were open source, they would have a difficult to predict consequence.

371
00:24:29,360 --> 00:24:31,400
Like we are quite far from these models right now.

372
00:24:31,400 --> 00:24:36,040
And by quite far, I mean by itime scale, but still like, this is not what you're talking

373
00:24:36,040 --> 00:24:41,080
about, but the day will come when you have models which can do science autonomously,

374
00:24:41,080 --> 00:24:45,600
like be delivered on big science projects.

375
00:24:45,600 --> 00:24:52,400
And it becomes more complicated as to whether it is desirable that models of such power

376
00:24:52,400 --> 00:24:54,520
should be open sourced.

377
00:24:54,520 --> 00:24:59,200
I think the argument there is a lot less clear cut, a lot less straightforward compared

378
00:24:59,200 --> 00:25:04,480
to the current level models, which are very useful and I think it's fantastic that the

379
00:25:04,480 --> 00:25:06,000
current level models have been built.

380
00:25:06,000 --> 00:25:10,320
So like that is maybe, maybe I answered a slightly bigger question rather than what

381
00:25:10,320 --> 00:25:13,680
is the role of open source models, what's the deal with open source?

382
00:25:13,680 --> 00:25:19,280
And the deal is up to a certain capability, it's great, but not difficult to imagine models

383
00:25:19,280 --> 00:25:23,880
that are sufficiently powerful, which will be built where it becomes a lot less obvious

384
00:25:23,880 --> 00:25:27,560
as to the benefits of their open source.

385
00:25:27,560 --> 00:25:32,200
Is there a signal for you that we've reached that level or that we're approaching it?

386
00:25:32,200 --> 00:25:35,120
Like what's the boundary?

387
00:25:35,120 --> 00:25:41,760
So I think figuring out this boundary very well is an urgent research project.

388
00:25:41,760 --> 00:25:49,920
I think one of the things that help is that the closed source models are more capable

389
00:25:49,920 --> 00:25:51,000
than open source models.

390
00:25:51,000 --> 00:25:54,520
So the closed source models could be studied and so on.

391
00:25:54,520 --> 00:25:58,560
And so you'd have some experience with a generation of closed source model and then you know

392
00:25:58,560 --> 00:26:01,480
like, oh, these models capabilities, it's fine, there's no big deal there.

393
00:26:01,480 --> 00:26:06,280
Then in a couple years, the open source models catch up and maybe a day will come when we

394
00:26:06,280 --> 00:26:11,000
going to say, well, like these closed source models, they're getting a little too drastic

395
00:26:11,000 --> 00:26:13,480
and then some other approaches needed.

396
00:26:13,480 --> 00:26:21,200
If we have our future hat on, maybe it looks like think about like a several year timeline.

397
00:26:21,200 --> 00:26:26,160
What are the limits you see if any in the near term in scaling?

398
00:26:26,160 --> 00:26:32,600
Is it like data, token scarcity, cost of compute, architectural issues?

399
00:26:32,600 --> 00:26:37,520
So the most near term limit to scaling is obviously data.

400
00:26:37,520 --> 00:26:42,640
This is well known and some research is required to address it.

401
00:26:42,640 --> 00:26:49,280
Without going into the details, I'll just say that the data limit can be overcome and

402
00:26:49,280 --> 00:26:51,200
progress will continue.

403
00:26:51,200 --> 00:26:55,560
One question I've heard people debate a little bit is a degree to which the transformer based

404
00:26:55,560 --> 00:27:00,800
models can be applied to sort of the full set of areas that you'd need for AGI.

405
00:27:00,800 --> 00:27:05,000
And if you look at the human brain, for example, you do have reasonably specialized systems

406
00:27:05,000 --> 00:27:10,200
or all neural networks, be it specialized systems for the visual cortex versus areas

407
00:27:10,200 --> 00:27:15,000
of higher thought, areas for empathy or other sort of aspects of everything from personality

408
00:27:15,000 --> 00:27:16,800
to processing.

409
00:27:16,800 --> 00:27:20,560
Do you think that the transformer architectures are the main thing that will just keep going

410
00:27:20,560 --> 00:27:21,560
and get us there?

411
00:27:21,560 --> 00:27:24,080
Do you think we'll need other architectures over time?

412
00:27:24,080 --> 00:27:30,360
So I understand precisely what you're saying and I have two answers to this question.

413
00:27:30,360 --> 00:27:35,680
The first is that, in my opinion, the best way to think about the question of architecture

414
00:27:35,680 --> 00:27:39,880
is not in terms of a binary, is it enough?

415
00:27:39,880 --> 00:27:48,040
But how much effort, what will be the cost of using this particular architecture?

416
00:27:48,040 --> 00:27:53,040
At this point, I don't think anyone doubts that the transformer architecture can do amazing

417
00:27:53,040 --> 00:27:59,160
things, but maybe something else, maybe some modification could have some compute efficiency

418
00:27:59,160 --> 00:28:00,160
benefits.

419
00:28:00,160 --> 00:28:03,600
So it's better to think about it in terms of compute efficiency rather than in terms

420
00:28:03,600 --> 00:28:06,040
of can it get there at all?

421
00:28:06,040 --> 00:28:09,680
I think at this point, the answer is obviously yes.

422
00:28:09,680 --> 00:28:14,040
To the question about, well, what about the human brain and its brain regions?

423
00:28:14,040 --> 00:28:21,520
I actually think that the situation there is subtle and deceptive for the following reasons.

424
00:28:21,520 --> 00:28:26,360
So what I believe you alluded to is the fact that the human brain has known regions.

425
00:28:26,360 --> 00:28:31,240
It has a speech perception region, it has a speech production region, it has an image

426
00:28:31,240 --> 00:28:36,520
region, it has a face region, it has all these regions, and it looks like it's specialized.

427
00:28:36,520 --> 00:28:39,440
But you know what's interesting?

428
00:28:39,440 --> 00:28:44,680
Sometimes there are cases where very young children have severe cases of epilepsy at

429
00:28:44,680 --> 00:28:50,800
a young age, and the only way they figured out how to treat such children is by removing

430
00:28:50,800 --> 00:28:53,320
half of their brain.

431
00:28:53,320 --> 00:29:00,080
Because it happens at such a young age, these children grow up to be pretty functional adults,

432
00:29:00,080 --> 00:29:05,560
and they have all the same brain regions, but they are somehow compressed onto one hemisphere.

433
00:29:05,560 --> 00:29:09,920
So maybe some information processing efficiency is lost.

434
00:29:09,920 --> 00:29:13,600
It's a very traumatic thing to experience, but somehow all these brain regions rearrange

435
00:29:13,600 --> 00:29:14,600
themselves.

436
00:29:14,600 --> 00:29:20,360
There is another experiment which was done maybe 30 or 40 years ago on ferrets.

437
00:29:20,360 --> 00:29:22,120
So the ferret is a small animal.

438
00:29:22,120 --> 00:29:23,360
It's a pretty mean experiment.

439
00:29:23,360 --> 00:29:29,920
They took the optic nerve of the ferret, which comes from its eye, and attached it to its

440
00:29:29,920 --> 00:29:31,320
auditory cortex.

441
00:29:31,320 --> 00:29:36,640
So now the inputs from the eye starts to map to the speech processing area of the brain.

442
00:29:36,640 --> 00:29:41,520
And then they recorded different neurons after it had a few days of learning to see, and

443
00:29:41,520 --> 00:29:45,480
they found neurons in the auditory cortex, which were very similar to the visual cortex.

444
00:29:45,480 --> 00:29:51,160
Or vice versa, it was either they mapped their eye to the ear, to the auditory cortex, or

445
00:29:51,160 --> 00:29:52,400
the ear to the visual cortex.

446
00:29:52,400 --> 00:29:54,680
But something like this has happened.

447
00:29:54,680 --> 00:30:00,360
These are fairly well-known ideas in AI that the cortex of humans and animals are extremely

448
00:30:00,360 --> 00:30:01,680
uniform.

449
00:30:01,680 --> 00:30:05,320
And so that further supports the AI, like you just need one big, uniform architecture.

450
00:30:05,320 --> 00:30:06,320
So all you need.

451
00:30:06,320 --> 00:30:07,320
Yeah.

452
00:30:07,320 --> 00:30:10,080
In general, it seems like every biological system is reasonably lazy in terms of taking

453
00:30:10,080 --> 00:30:12,960
one system and then reproducing it and then reusing it in different ways.

454
00:30:12,960 --> 00:30:16,520
And that's true of everything from DNA encoding, you know, there's 20 amino acids and protein

455
00:30:16,520 --> 00:30:17,520
sequences.

456
00:30:17,520 --> 00:30:22,240
Everything is made out of the same 20 amino acids on through to your point, sort of how

457
00:30:22,240 --> 00:30:23,800
you think about tissue architecture.

458
00:30:23,800 --> 00:30:27,040
So it's remarkable that that carries over into the digital world as well, depending

459
00:30:27,040 --> 00:30:28,280
on the architecture you use.

460
00:30:28,280 --> 00:30:33,200
I mean, the way I see it is that this is an indication from a technological point of

461
00:30:33,200 --> 00:30:34,200
view.

462
00:30:34,200 --> 00:30:37,640
We are very much on the right track because you have all these interesting analogies

463
00:30:37,640 --> 00:30:41,560
between human intelligence and biological intelligence and artificial intelligence.

464
00:30:41,560 --> 00:30:47,680
We've got artificial neurons, biological neurons, unified brain architecture for biological

465
00:30:47,680 --> 00:30:51,600
intelligence, unified neural network architecture for artificial intelligence.

466
00:30:51,600 --> 00:30:56,000
At what point do you think we should start thinking about these systems in digital life?

467
00:30:56,000 --> 00:30:57,320
I can answer that question.

468
00:30:57,320 --> 00:31:04,040
I think that will happen when those systems become reliable in such a way as to be very

469
00:31:04,040 --> 00:31:05,040
autonomous.

470
00:31:05,040 --> 00:31:08,120
Right now, those systems are clearly not autonomous.

471
00:31:08,120 --> 00:31:10,720
They're inching there, but they're not.

472
00:31:10,720 --> 00:31:13,680
And that makes them a lot less useful too, because you can't ask it, hey, like, do my

473
00:31:13,680 --> 00:31:16,840
homework or do my taxes or you see what I mean.

474
00:31:16,840 --> 00:31:19,120
So the usefulness is greatly limited.

475
00:31:19,120 --> 00:31:23,960
As the usefulness increases, they will indeed become more like artificial life, which also

476
00:31:23,960 --> 00:31:29,080
makes it more, I would argue, trepidations, right?

477
00:31:29,080 --> 00:31:33,880
Like if you imagine actual artificial life with brains that are smarter than humans,

478
00:31:33,880 --> 00:31:37,920
gosh, that seems pretty monumental.

479
00:31:37,920 --> 00:31:40,480
Why is your definition based on autonomy?

480
00:31:40,480 --> 00:31:45,160
Because if you often look at the definition of biological life, it has to do with reproductive

481
00:31:45,160 --> 00:31:46,160
capability.

482
00:31:46,160 --> 00:31:48,400
Plus, I guess some form of autonomy, right?

483
00:31:48,400 --> 00:31:52,000
Like a virus isn't really necessarily considered alive much of the time, right?

484
00:31:52,000 --> 00:31:53,920
But a bacteria is.

485
00:31:53,920 --> 00:31:58,040
And you could imagine situations where you have symbiotic relationships or other things

486
00:31:58,040 --> 00:32:00,840
where something can't really quite function autonomously, but it's still considered a

487
00:32:00,840 --> 00:32:01,840
life form.

488
00:32:01,840 --> 00:32:05,320
So I'm a little bit curious about autonomy being the definition versus some of these other

489
00:32:05,320 --> 00:32:06,320
aspects.

490
00:32:06,320 --> 00:32:12,120
Well, I mean, definitions are chosen for our convenience and it's a matter of debate.

491
00:32:12,120 --> 00:32:16,640
In my opinion, technology already has the reproduction, the reproductive function, right?

492
00:32:16,640 --> 00:32:20,560
And if you look at for example, I don't know if you've seen those images of the evolution

493
00:32:20,560 --> 00:32:24,920
of cell phones and then smartphones over the past 25 years, you got this like, what almost

494
00:32:24,920 --> 00:32:28,600
looks like an evolutionary tree or the evolution of cars over the past century.

495
00:32:28,600 --> 00:32:33,320
So technology is already reproducing using the minds of people who copy ideas from previous

496
00:32:33,320 --> 00:32:35,120
generation of technology.

497
00:32:35,120 --> 00:32:37,560
So I claim that the reproduction is already there.

498
00:32:37,560 --> 00:32:40,880
The autonomy piece I claim is not.

499
00:32:40,880 --> 00:32:43,520
And indeed, I also agree that there is no autonomous reproduction.

500
00:32:43,520 --> 00:32:48,280
But that would be like, can you imagine if you have like autonomously reproducing AIs?

501
00:32:48,280 --> 00:32:54,720
I actually think that that is a pretty traumatic and I would say quite a scary thing if you

502
00:32:54,720 --> 00:32:58,760
have an autonomously reproducing AI, if it's also very capable.

503
00:32:58,760 --> 00:33:00,840
Should we talk about super alignment?

504
00:33:00,840 --> 00:33:02,800
Yeah, very much so.

505
00:33:02,800 --> 00:33:05,400
Can you just sort of define it?

506
00:33:05,400 --> 00:33:11,360
And then we were talking about what the boundary is for when you feel we need to begin to worry

507
00:33:11,360 --> 00:33:15,880
about these capabilities being in open source.

508
00:33:15,880 --> 00:33:19,240
What is super alignment and why invest in it now?

509
00:33:19,240 --> 00:33:27,040
The answer to your question really depends to where you think AI is headed.

510
00:33:27,040 --> 00:33:31,200
If you just try to imagine and look into the future, which is of course a very difficult

511
00:33:31,200 --> 00:33:35,320
thing to do, but let's try to do it anyway.

512
00:33:35,320 --> 00:33:38,760
Where do we think things will be in five years or in 10 years?

513
00:33:38,760 --> 00:33:43,240
I mean, progress has been really stunning over the past few years.

514
00:33:43,240 --> 00:33:45,560
Maybe it will be a little bit slower.

515
00:33:45,560 --> 00:33:49,800
But still, if you extrapolate this kind of progress, you'll be in a very, very different

516
00:33:49,800 --> 00:33:54,360
place in five years, let alone 10 years.

517
00:33:54,360 --> 00:33:56,720
It doesn't seem implausible.

518
00:33:56,720 --> 00:34:02,920
It doesn't seem at all implausible that we will have computers, data centers that are

519
00:34:02,920 --> 00:34:05,040
much smarter than people.

520
00:34:05,040 --> 00:34:09,240
And by smarter, I don't mean just have more memory or have more knowledge, but I also

521
00:34:09,240 --> 00:34:16,840
mean have deeper insight into the same subjects that we people are studying and looking into.

522
00:34:16,840 --> 00:34:20,840
It means learn even faster than people.

523
00:34:20,840 --> 00:34:23,800
What would such AIs do?

524
00:34:23,800 --> 00:34:25,200
I don't know.

525
00:34:25,360 --> 00:34:30,640
If such an AI were the basis of some artificial life, it would be, well, how do you even think

526
00:34:30,640 --> 00:34:31,640
about it?

527
00:34:31,640 --> 00:34:36,960
If you have some very powerful data center that's also alive in a sense, that's what

528
00:34:36,960 --> 00:34:37,960
you're talking about.

529
00:34:37,960 --> 00:34:42,600
And when I imagine this world, my reaction is, gosh, this is very unpredictable what's

530
00:34:42,600 --> 00:34:43,600
going to happen.

531
00:34:43,600 --> 00:34:44,600
Very unpredictable.

532
00:34:44,600 --> 00:34:49,840
But the bare minimum, but there is a bare minimum which we can articulate.

533
00:34:49,840 --> 00:34:57,560
But if such super, if such very, very intelligent, super intelligent data centers are being built

534
00:34:57,560 --> 00:35:04,400
at all, we want those data centers to hold warm and positive feelings towards people,

535
00:35:04,400 --> 00:35:06,600
towards humanity.

536
00:35:06,600 --> 00:35:11,560
Because this is going to be non-human life in a sense.

537
00:35:11,560 --> 00:35:12,560
Potentially.

538
00:35:12,560 --> 00:35:15,040
It could potentially be that.

539
00:35:15,040 --> 00:35:21,440
So I would want that any instance of such super intelligence, the warm feelings towards

540
00:35:21,440 --> 00:35:22,440
humanity.

541
00:35:22,440 --> 00:35:25,640
And so this is what we are doing with the super alignment project.

542
00:35:25,640 --> 00:35:31,360
You're saying, hey, if you just allow yourself, if you just accept that progress that we've

543
00:35:31,360 --> 00:35:35,320
seen, maybe it will be slower, but it will continue.

544
00:35:35,320 --> 00:35:44,640
If you allow yourself that, then can you start doing productive work today to build the science

545
00:35:44,640 --> 00:35:53,920
so that we will be able to handle the problem of controlling such future super intelligence.

546
00:35:53,920 --> 00:36:00,720
Of imprinting onto them a strong desire to be nice and kind to people.

547
00:36:00,720 --> 00:36:05,880
Because those data centers, right, they'll be, they'll be really quite powerful.

548
00:36:05,880 --> 00:36:09,720
You know, there'll probably be many of them that will be, the world will be very complicated.

549
00:36:09,720 --> 00:36:15,200
But somehow to the extent that they are autonomous, to the extent that they are agents, to the

550
00:36:15,200 --> 00:36:22,640
extent they are beings, I want them to be pro-social, pro-human social.

551
00:36:22,640 --> 00:36:23,640
That's the goal.

552
00:36:23,640 --> 00:36:27,200
What do you think is the likelihood of that goal?

553
00:36:27,200 --> 00:36:33,440
I mean, some of it, it feels like a outcome you can hopefully affect, right?

554
00:36:33,440 --> 00:36:39,360
But are we, are we likely to have pro-social AIs that we are friends with individually

555
00:36:39,360 --> 00:36:42,040
or, you know, as a species?

556
00:36:42,040 --> 00:36:48,160
Well, I mean, friends be, I think that that part is not necessary.

557
00:36:48,160 --> 00:36:52,040
The friendship piece, I think, is optional, but I do think that we want to have very pro-social

558
00:36:52,040 --> 00:36:53,360
AI.

559
00:36:53,360 --> 00:36:55,360
I think it's, I think it's possible.

560
00:36:55,360 --> 00:36:57,640
I don't think it's guaranteed, but I think it's possible.

561
00:36:57,640 --> 00:37:02,720
I think it's going to be possible and the possibility of that will increase insofar

562
00:37:02,800 --> 00:37:08,320
as more and more people allow themselves to look into the future, into the five to ten

563
00:37:08,320 --> 00:37:16,120
year future and just ask yourself, what, what do you expect AI to be able to do then?

564
00:37:16,120 --> 00:37:19,320
How capable do you expect it to be then?

565
00:37:19,320 --> 00:37:27,120
And I think that with each passing year, if indeed AI continues to improve and as people

566
00:37:27,120 --> 00:37:32,700
get to experience, because right now we are talking, making arguments, but if you actually

567
00:37:32,700 --> 00:37:38,820
get to experience, oh gosh, the AI from last year, which was really helpful this year puts

568
00:37:38,820 --> 00:37:44,180
the previous one to shame and you go, OK, and then one year later and when it's starting

569
00:37:44,180 --> 00:37:50,380
to do science, the AI software engineer is starting to get really quite good, let's say.

570
00:37:50,380 --> 00:37:58,340
I think that will create a lot more desire in people for what you just described, for

571
00:37:58,340 --> 00:38:02,380
the future superintelligence to indeed be very pro-social.

572
00:38:02,500 --> 00:38:05,300
I think there's going to be a lot of disagreement, there's going to be a lot of political questions,

573
00:38:05,300 --> 00:38:12,180
but I think that as people see AI actually getting better, as people experience it, the

574
00:38:12,180 --> 00:38:19,820
desire for the pro-social superintelligence, the humanity loving superintelligence, as

575
00:38:19,820 --> 00:38:23,580
much as it can be done, will increase.

576
00:38:23,580 --> 00:38:28,460
And on the scientific problem, I think right now it's still being an area where not that

577
00:38:28,460 --> 00:38:31,100
many people are working on.

578
00:38:31,100 --> 00:38:35,300
Our AI is getting powerful enough, you can really start studying it productively, you'll

579
00:38:35,300 --> 00:38:38,340
have some very exciting research to share soon.

580
00:38:38,340 --> 00:38:43,500
But I would say that's the big picture situation here.

581
00:38:43,500 --> 00:38:47,540
Just really, it really boils down to look at what you've experienced with AI up until

582
00:38:47,540 --> 00:38:54,380
now, ask yourself, like, is it slowing down, will it slow down next year, like, we will

583
00:38:54,460 --> 00:38:58,980
see, and we will experience it again and again, and I think it will keep, and what needs to

584
00:38:58,980 --> 00:39:00,900
be done, it will keep becoming clearer.

585
00:39:00,900 --> 00:39:02,820
Do you think we're just on an accelerated path?

586
00:39:02,820 --> 00:39:06,900
Because I think fundamentally, if you look at certain technology waves, they tend to

587
00:39:06,900 --> 00:39:10,420
inflect and then accelerate versus decelerate.

588
00:39:10,420 --> 00:39:14,740
And so it really feels like we're in an acceleration phase right now versus the deceleration phase.

589
00:39:14,740 --> 00:39:15,740
Yeah.

590
00:39:15,740 --> 00:39:20,580
I mean, VR, right now it is indeed the case that VR in an acceleration phase.

591
00:39:20,580 --> 00:39:27,260
You know, it's hard to say, you know, multiple forces will come into play.

592
00:39:27,260 --> 00:39:30,540
Some forces are accelerating forces and some forces are decelerating.

593
00:39:30,540 --> 00:39:36,140
So for example, the cost and scale are a decelerating force.

594
00:39:36,140 --> 00:39:41,220
The fact that our data is finite is a decelerating force to some degree at least, I don't want

595
00:39:41,220 --> 00:39:42,220
to overstate it.

596
00:39:42,220 --> 00:39:43,780
Yeah, it's kind of within an asymptote, right?

597
00:39:43,780 --> 00:39:48,020
Like at some point you hit it, but it's the standard S curve, right, or sigmoidal.

598
00:39:49,020 --> 00:39:52,500
With the data in particular, I just think it won't be, it just won't be an issue because

599
00:39:52,500 --> 00:39:55,340
we'll figure out something else.

600
00:39:55,340 --> 00:39:59,860
But then you might argue that the size of the engineering project is a decelerating force,

601
00:39:59,860 --> 00:40:01,540
just the complexity of management.

602
00:40:01,540 --> 00:40:05,500
On the other hand, the amount of investment is an accelerating force, the amount of interest

603
00:40:05,500 --> 00:40:09,220
from people, from engineers, scientists is an accelerating force.

604
00:40:09,220 --> 00:40:11,620
And I think there is one other accelerating force.

605
00:40:11,620 --> 00:40:17,180
And that is the fact that biological evolution has been able to figure it out and the fact

606
00:40:17,180 --> 00:40:24,860
that up until now, progress in AI has had up until this point, this weird property that

607
00:40:24,860 --> 00:40:30,220
it's kind of been, you know, it's been very hard to execute on, but in some sense it's

608
00:40:30,220 --> 00:40:36,420
also been more straightforward than one would have expected perhaps.

609
00:40:36,420 --> 00:40:42,140
Like in some sense, I don't know much physics, but my understanding is that if you want to

610
00:40:42,140 --> 00:40:48,860
make progress in quantum physics or something, you need to be really intelligent and spend

611
00:40:48,860 --> 00:40:53,380
many years in grad school studying how these things work.

612
00:40:53,380 --> 00:40:57,260
Whereas with AI, if people come in, get up to speed quickly, start making contributions

613
00:40:57,260 --> 00:40:59,340
quickly, it has the flavor is somehow different.

614
00:40:59,340 --> 00:41:05,340
Somehow it's very, there is some kind of, there's a lot of give to this particular area

615
00:41:05,340 --> 00:41:06,340
of research.

616
00:41:06,340 --> 00:41:07,980
And I think this is also an accelerating force.

617
00:41:07,980 --> 00:41:10,900
How will it all play out remains to be seen.

618
00:41:10,900 --> 00:41:15,580
Like it may be that somehow the scale required of engineering complexity will start to make

619
00:41:15,580 --> 00:41:17,820
it so that the rate of progress will start to slow down.

620
00:41:17,820 --> 00:41:20,940
It will still continue, but maybe not as quick as we had before.

621
00:41:20,940 --> 00:41:25,460
Or maybe the forces which are coming together to push it will be such that it will be as

622
00:41:25,460 --> 00:41:29,700
fast for maybe a few more years before it will start to slow down.

623
00:41:29,700 --> 00:41:33,060
If at all that's, that would be my articulation here.

624
00:41:33,060 --> 00:41:35,260
Ilya, this has been a great conversation.

625
00:41:35,260 --> 00:41:36,820
Thanks for joining us.

626
00:41:36,820 --> 00:41:37,820
Thank you so much for the conversation.

627
00:41:37,820 --> 00:41:39,820
I really enjoyed it.

628
00:41:39,820 --> 00:41:42,660
Join us on Twitter at NoPriorsPod.

629
00:41:42,660 --> 00:41:45,460
Subscribe to our YouTube channel if you want to see our faces.

630
00:41:45,460 --> 00:41:49,100
Follow the show on Apple podcasts, Spotify, or wherever you listen.

631
00:41:49,100 --> 00:41:51,180
That way you get a new episode every week.

632
00:41:51,180 --> 00:41:55,540
And sign up for emails or find transcripts for every episode at no-priors.com.

