WEBVTT

00:00.000 --> 00:11.560
OpenAI, a company that we all know now, but only a year ago, was 100 people, is changing

00:11.560 --> 00:12.560
the world.

00:12.560 --> 00:15.560
Their research is leading the charge to AGI.

00:15.560 --> 00:19.800
Since ChatGPT captured consumer attention last November, they show no signs of slowing

00:19.800 --> 00:21.080
down.

00:21.080 --> 00:25.680
This week, a lot of nice sit down with Ilya Sutskover, co-founder and chief scientist

00:25.680 --> 00:31.880
at OpenAI to discuss the state of AI research, where will hit limits, the future of AGI,

00:31.880 --> 00:33.920
and what it's going to take to reach super alignment.

00:33.920 --> 00:36.240
Ilya, welcome to NoPriors.

00:36.240 --> 00:37.240
Thank you.

00:37.240 --> 00:38.240
It's good to be here.

00:38.240 --> 00:39.240
Let's start at the beginning.

00:39.240 --> 00:43.400
Pre-AlexNet, nothing in deep learning was really working, and then given that environment,

00:43.400 --> 00:46.360
you guys took a very unique bet.

00:46.360 --> 00:48.640
What motivated you to go in this direction?

00:49.400 --> 00:59.160
In those dark ages, AI was not an area where people had hope, and people were not accustomed

00:59.160 --> 01:02.040
to any kind of success at all.

01:02.040 --> 01:06.840
Because there hasn't been any success, there was a lot of debate, and there were different

01:06.840 --> 01:13.360
schools of thoughts that had different arguments about how machine learning and AI should be.

01:13.360 --> 01:18.760
You had people who were into knowledge representation from a good old fashioned AI.

01:18.760 --> 01:23.280
You had people who were Bayesian, and they liked Bayesian non-parametric methods.

01:23.280 --> 01:27.360
You had people who like graphical models, and you had the people who like neural networks.

01:27.360 --> 01:33.480
Those people were marginalized because neural networks did not have the property that you

01:33.480 --> 01:35.640
can't prove math theorems about them.

01:35.640 --> 01:39.880
If you can't prove theorems about something, it means that your research isn't good.

01:39.880 --> 01:41.520
That's how it has been.

01:41.520 --> 01:45.680
The reason why I gravitated to neural networks from the beginning is because it felt like

01:45.680 --> 01:48.000
those are small little brains.

01:48.000 --> 01:50.280
Who cares if you can't prove any theorems about them?

01:50.280 --> 01:56.560
Because we are training small little brains, and maybe they'll do something one day.

01:56.560 --> 02:01.680
The reason that we were able to do AlexNet when we did it is because of a combination

02:01.680 --> 02:04.400
of two factors, three factors.

02:04.400 --> 02:10.760
The first factor is that this was shortly after GPUs started to be used in machine learning.

02:10.760 --> 02:15.560
People had an intuition that that's a good thing to do, but it wasn't like today where

02:15.560 --> 02:17.560
people exactly knew what an NGPU is for.

02:17.560 --> 02:21.560
It was like, let's play with those cool fast computers and see what we can do with them.

02:21.560 --> 02:27.160
It was an especially good fit for neural networks, so that definitely helped us.

02:27.160 --> 02:34.280
I was very fortunate in that I was able to realize that the reason neural networks of

02:34.280 --> 02:37.720
the time weren't good is because they were too small.

02:37.720 --> 02:43.880
If you try to solve a vision task with a neural network which has like a thousand neurons,

02:43.880 --> 02:45.040
what can it do?

02:45.040 --> 02:46.520
It can't do anything.

02:46.520 --> 02:50.120
It doesn't matter how good your learning is and everything else, but if you have a much

02:50.120 --> 02:53.240
larger neural network, it will do something unprecedented.

02:53.240 --> 02:56.680
I'll give you the intuition to think that that was the case, because I think at the

02:56.680 --> 03:02.680
time it was reasonably contrarian to think that despite the human brain in some sense

03:02.680 --> 03:05.360
works that way or different biological neural circuits.

03:05.360 --> 03:09.600
I'm just curious what gave you that intuition early on to think that this was a good direction.

03:09.600 --> 03:18.680
I think looking at the brain and specifically, if all those things follow very easily, if

03:18.680 --> 03:27.880
you allow yourself to accept the idea, right now this idea is reasonably well accepted.

03:27.880 --> 03:32.480
Back then, people still talked about it, but they haven't really accepted it or internalized.

03:32.480 --> 03:38.700
The idea that maybe an artificial neuron in some sense is not that different from a biological

03:38.700 --> 03:40.200
neuron.

03:40.200 --> 03:45.600
Now whatever you imagine animals do with their brains, you could perhaps assemble some artificial

03:45.600 --> 03:47.800
neural network of similar size.

03:47.800 --> 03:53.320
Maybe if you train it, it will do something similar.

03:53.320 --> 03:57.280
That leads you to start to imagine.

03:57.280 --> 03:59.880
Almost imagine the computation being done by the neural network.

03:59.880 --> 04:05.520
You can almost think like if you have a high resolution image and you have like one neuron

04:05.520 --> 04:07.960
for like a large group of pixels, what can the neuron do?

04:07.960 --> 04:11.800
It's just not much it can do, but if you have a lot of neurons, then they can actually do

04:11.800 --> 04:14.000
something and compute something.

04:14.000 --> 04:19.440
So I think it was like, all right, like it was considerations like this, plus a technical

04:19.440 --> 04:20.440
realization.

04:20.440 --> 04:29.240
The technical realization is that if you have a large training set that specifies the behavior

04:29.240 --> 04:35.440
of the neural network, and the training set is large enough such that it can constrain

04:35.440 --> 04:37.960
the large neural network sufficiently.

04:37.960 --> 04:42.000
And furthermore, if you have the algorithm to find that neural network, because what

04:42.000 --> 04:48.240
we do is that we turn the training set into a neural network which satisfies the training

04:48.240 --> 04:49.360
set.

04:49.360 --> 04:56.720
Neural network training can almost be seen as solving a neural equation.

04:56.720 --> 05:01.680
Solving a neural equation where every data point is an equation, and every parameter

05:01.680 --> 05:05.000
is a variable.

05:05.000 --> 05:07.960
And so it was multiple things.

05:07.960 --> 05:12.480
The realization that the bigger neural network could do something unprecedented.

05:12.480 --> 05:19.480
The realization that if you have a large data set together with the compute to solve the

05:19.480 --> 05:24.480
neural equation, that's what gradient descent comes in, but it's not gradient descent.

05:24.480 --> 05:26.200
Gradient descent was around for a long time.

05:26.200 --> 05:30.320
It was certain technical insights about how to make it work.

05:30.320 --> 05:33.480
Because back then the prevailing belief was, well, you can't train those neural nets anything.

05:33.480 --> 05:34.480
It's all hopeless.

05:34.480 --> 05:35.760
So it wasn't just about the site.

05:35.760 --> 05:40.920
It was about, even if someone did think, gosh, it would be cool to train a big neural net,

05:40.920 --> 05:46.320
they didn't have the technical ability to turn this idea into reality.

05:46.320 --> 05:50.920
You needed not only to code the neural net, you need to do a bunch of things right, and

05:50.920 --> 05:52.920
only then it will work.

05:52.920 --> 05:57.840
And then another fortunate thing is that the person whom I work with, Alex Krzyzewski,

05:57.840 --> 06:02.920
he just discovered that he really loves GPUs, and he was perhaps one of the first person

06:02.920 --> 06:10.120
who really mastered writing really, like, really performant code for the GPUs.

06:10.120 --> 06:14.560
And that's why we were able to squeeze a lot of performance out of two GPUs and produce

06:14.560 --> 06:15.640
something unprecedented.

06:15.640 --> 06:19.520
So to sum up, it was multiple things.

06:19.520 --> 06:23.900
The idea that a big neural network, in this case a vision neural network, a convolutional

06:23.900 --> 06:28.640
neural network with many layers, one that's much, much bigger than anything that's ever

06:28.640 --> 06:32.880
been done before, could do something very unprecedented because the brain can see and

06:32.880 --> 06:34.760
the brain is a large neural network.

06:34.760 --> 06:40.080
And we can see quickly, so our neurons don't have a lot of time, then the compute needed,

06:40.080 --> 06:44.880
the technical know-how that, in fact, we can't train such neural networks.

06:44.880 --> 06:46.920
And it was not at all widely distributed.

06:46.920 --> 06:51.120
People in machine learning would not have been able to train such a neural network even

06:51.120 --> 06:52.120
if they wanted to.

06:52.120 --> 06:57.960
Did you guys have any, like, particular goal from a size perspective?

06:57.960 --> 07:02.400
Or was it just as, you know, and if that's biologically inspired or where that number

07:02.400 --> 07:05.000
comes from or just as large as we can go?

07:05.000 --> 07:06.000
Definitely as large as we can go.

07:06.000 --> 07:10.560
Just keep in mind, I mean, we had a certain amount of compute which we could usefully

07:10.560 --> 07:14.080
consume, and then what can it do?

07:14.080 --> 07:21.240
Maybe if we think about just like the origin of open AI and the goals of the organization,

07:21.240 --> 07:24.760
like what was the original goal and how's that evolved over time?

07:24.760 --> 07:30.960
The goal did not evolve over time, the tactic evolved over time.

07:30.960 --> 07:37.920
So the goal of open AI from the very beginning has been to make sure that artificial general

07:37.920 --> 07:45.520
intelligence, by which we mean autonomous systems, AI, that can actually do most of

07:45.520 --> 07:51.480
the jobs and the activities and tasks that people do, benefits all of humanity.

07:51.480 --> 07:53.680
That was the goal from the beginning.

07:53.680 --> 07:59.720
The initial thinking has been that maybe the best way to do it is by just open sourcing

07:59.720 --> 08:02.320
a lot of technology.

08:02.320 --> 08:08.080
We later, and we also attempted to do it as a nonprofit, seemed very sensible.

08:08.080 --> 08:13.000
This is the goal, nonprofit is the way to do it, what changed.

08:13.000 --> 08:20.240
Some point at open AI, we realized and we were perhaps among the earliest to realize

08:20.240 --> 08:25.200
that to make progress in AI for real, you need a lot of compute.

08:25.200 --> 08:27.400
Now, what does a lot mean?

08:27.400 --> 08:32.800
The appetite for compute is truly endless, as now clearly seen, but we realized that

08:32.800 --> 08:41.360
we will need a lot and a nonprofit wouldn't be the way to get there, wouldn't be able

08:41.360 --> 08:43.120
to build a large cluster with a nonprofit.

08:43.120 --> 08:50.120
That's where we became, we converted into this unusual structure called CAP Profit,

08:50.120 --> 08:53.160
and to my knowledge, we are the only CAP Profit company in the world.

08:53.760 --> 08:59.320
The idea is that investors put in some money, but even if the company does incredibly well,

08:59.320 --> 09:04.760
they don't get more than some multiplier on top of their original investment.

09:04.760 --> 09:10.680
The reason to do this, the reason why that makes sense, there are arguments, one could

09:10.680 --> 09:18.160
make arguments against it as well, but the argument for it is that if you believe that

09:18.160 --> 09:26.320
the technology that we are building, AGI, could potentially be so capable as to do every

09:26.320 --> 09:32.240
single task that people do, does it mean that it might un-employ everyone?

09:32.240 --> 09:37.760
Well, I don't know, but it's not impossible, and if that's the case, it makes sense, it

09:37.760 --> 09:41.400
will make a lot of sense if the company that builds such a technology would not be able

09:41.400 --> 09:46.560
to make infinite, would not be incentivized, rather, to make infinite profits.

09:46.560 --> 09:51.080
I don't know if it will literally play out this way because of competition in AI, so

09:51.080 --> 09:57.200
there will be multiple companies, and I think that will have some unforeseen implications

09:57.200 --> 09:59.880
on the argument which I'm making, but that was the thinking.

09:59.880 --> 10:03.840
I remember visiting the offices back when you were, I think, housed at YC or something,

10:03.840 --> 10:09.240
or cohabited some space there, and at the time, there was a suite of different efforts.

10:09.240 --> 10:14.520
There was robotic arms that were being manipulated, and then there was some video game related

10:14.520 --> 10:16.920
work, which was really cutting edge.

10:16.920 --> 10:20.840
How did you think about how the research agenda evolved, and what really drove it down this

10:20.840 --> 10:25.120
path of transformer-based models and other forms of learning?

10:25.120 --> 10:32.000
So I think it has been evolving over the years from when we started OpenAI.

10:32.000 --> 10:36.120
In the first year, we indeed did some of the more conventional machine learning work.

10:36.120 --> 10:39.680
The conventional machine learning work, I mean, because the world has changed so much,

10:39.680 --> 10:45.640
a lot of things which were known to everyone in 2016 or 2017 are completely and utterly

10:45.640 --> 10:46.640
forgotten.

10:46.640 --> 10:48.720
It's like the Stone Age almost.

10:48.720 --> 10:53.840
So in that Stone Age, the world of machine learning looked very different.

10:53.840 --> 10:58.920
It was dramatically more academic.

10:58.920 --> 11:02.440
The goals, values, and objectives were much more academic.

11:02.440 --> 11:07.240
They were about discovering small bits of knowledge and sharing them with the other researchers

11:07.240 --> 11:10.040
and getting scientific recognition as a result.

11:10.040 --> 11:12.320
And it's a very valid goal, and it's very understandable.

11:12.320 --> 11:14.240
I've been doing AI for 20 years now.

11:14.240 --> 11:17.760
More than half of my time that I spent in AI was in that framework.

11:17.760 --> 11:19.760
And so what do you do?

11:19.760 --> 11:22.240
You write papers, you share your small discoveries.

11:22.240 --> 11:23.240
Two realizations.

11:23.240 --> 11:29.600
The first realization is just at a high level, it doesn't seem like it's the way to go for

11:29.600 --> 11:30.960
a dramatic impact.

11:30.960 --> 11:32.240
And why is that?

11:32.240 --> 11:39.600
Because if you imagine how an AGI should look like, it has to be some kind of a big engineering

11:39.600 --> 11:43.440
project that's using a lot of compute, right?

11:43.440 --> 11:45.760
Even if you don't know how to build it, what that should look like.

11:45.760 --> 11:47.640
You know that this is the ideal you want to strive towards.

11:47.640 --> 11:52.000
So you want to somehow move towards larger projects as opposed to small projects.

11:52.000 --> 11:58.640
So while we attempted a first large project where we trained a neural network to play

11:58.640 --> 12:03.000
a real-time strategy game, as well as the best humans.

12:03.000 --> 12:10.800
It's the Dota 2 project, and it was driven by two people, Jakob Pachotsky and Greg Brokman.

12:10.800 --> 12:13.280
They really drove this project and made it a success.

12:13.280 --> 12:17.280
And this was our first attempt at a large project.

12:17.280 --> 12:21.840
But it wasn't quite the right formula for us, because the neural networks were a little

12:21.840 --> 12:22.840
bit too small.

12:22.840 --> 12:25.000
It was just a narrow domain, just a game.

12:25.000 --> 12:27.840
I mean, it's cool to play a game, and it kept looking.

12:27.840 --> 12:32.400
At some point, we realized that, hey, if you train a large neural network, a very, very

12:32.400 --> 12:38.240
large transformer to predict text better and better, something very surprising will happen.

12:38.240 --> 12:41.000
This realization also arrived a little bit gradually.

12:41.000 --> 12:44.200
We were exploring generative models.

12:44.200 --> 12:48.160
We were exploring ideas around next-word prediction.

12:48.160 --> 12:50.160
Those are ideas also related to compression.

12:50.160 --> 12:52.400
We were exploring them.

12:52.400 --> 12:53.400
The transformer came out.

12:53.400 --> 12:54.400
We got really excited.

12:54.400 --> 12:56.600
We were like, this is the greatest thing.

12:56.600 --> 12:57.840
We're going to do transformers now.

12:57.840 --> 13:00.120
It's clearly superior than anything else before it.

13:00.120 --> 13:02.720
We started doing transformers with the GPT-1.

13:02.720 --> 13:06.520
GPT-1 started to show very interesting signs of life.

13:06.520 --> 13:08.360
And that led us to doing GPT-2.

13:08.360 --> 13:09.880
And then ultimately, GPT-3.

13:09.880 --> 13:15.560
GPT-3 really opened everyone else's eyes as well to, hey, this thing has a lot of traction.

13:15.560 --> 13:19.720
There is one specific formula right now that everyone is doing.

13:19.720 --> 13:24.040
And this formula is train a larger and larger transformer on more and more data.

13:24.040 --> 13:29.440
I mean, for me, the big wake-up moment to your point was GPT-2 to GPT-3 transition, where

13:29.440 --> 13:32.320
you saw such a big step function and capabilities.

13:32.320 --> 13:39.400
And then obviously, with four open eyes, published some really interesting research around some

13:39.400 --> 13:42.840
of the different domains of knowledge or domains of expertise or chain of thought or other

13:42.840 --> 13:46.160
things that the models can suddenly do in an emergent form.

13:46.160 --> 13:49.320
What was the most surprising thing for you in terms of emergent behavior in these models

13:49.320 --> 13:50.320
over time?

13:50.320 --> 13:52.640
You know, it's very hard to answer that question.

13:52.640 --> 13:56.520
It's very hard to answer because I'm too close and I've seen it progress every step of the

13:56.520 --> 13:58.320
way.

13:58.320 --> 14:01.960
So as much as I'd like, I find it very hard to answer that question.

14:01.960 --> 14:09.840
I think if I had to pick one, I think maybe the most surprising thing for me is the whole

14:09.840 --> 14:12.800
thing works at all.

14:12.800 --> 14:13.800
It's hard.

14:13.800 --> 14:20.080
I'm not sure I know how to convey this, what I have in mind here, because if you see a

14:20.080 --> 14:24.200
lot of neural networks do amazing things, well, obviously neural networks is the thing

14:24.200 --> 14:25.640
that works.

14:25.640 --> 14:32.240
But I have witnessed personally what it's like to be in a world for many years where

14:32.240 --> 14:34.760
the neural networks don't work at all.

14:34.760 --> 14:39.520
And then to contrast that to where we are today, just the fact that they work and they

14:39.520 --> 14:41.400
do these amazing things.

14:41.400 --> 14:45.560
I think maybe the most surprising, the most surprising, if I had to pick one, it would

14:45.560 --> 14:49.000
be the fact that when I speak to it, I feel understood.

14:49.000 --> 14:53.640
Yeah, there's a really good saying from, I'm trying to remember, maybe it's Arthur Clark

14:53.640 --> 14:59.800
or one of the sci-fi authors, which is effectively it says advanced technology is sometimes indistinguishable

14:59.800 --> 15:00.800
for magic.

15:00.800 --> 15:03.240
Yeah, I'm fully in this camp.

15:03.240 --> 15:06.960
Yeah, it definitely feels like there's some magical moments with some of these models

15:06.960 --> 15:07.960
now.

15:08.000 --> 15:15.160
A way that you guys decide internally, given all of the different capabilities you could

15:15.160 --> 15:20.160
pursue, how to continually choose the set of big projects, you've sort of described

15:20.160 --> 15:26.480
that centralization and committing to certain research directions at scale is really important

15:26.480 --> 15:28.120
to open AI success.

15:28.120 --> 15:31.920
Given the breadth of opportunity now, what's the process for deciding what's worth working

15:31.920 --> 15:32.920
on?

15:33.160 --> 15:38.560
I mean, I think there is some combination of bottom up and top down, where we have some

15:38.560 --> 15:43.320
top down ideas that we believe should work, but we're not 100% sure.

15:43.320 --> 15:48.560
So we still, we need to have good top down ideas, and there is a lot of bottom up exploration

15:48.560 --> 15:50.920
guided by those top down ideas as well.

15:50.920 --> 15:55.880
And their combination is what informs us as to what to do next.

15:55.880 --> 16:01.400
And if you think about those bottom, I mean, either direction, top down or bottom up ideas,

16:01.400 --> 16:07.200
you like clearly we have this dominant continue to scale transformers direction.

16:07.200 --> 16:12.400
Do you explore additional like architectural directions or is that just not relevant?

16:12.400 --> 16:16.840
Certainly possible that various improvements can be found.

16:16.840 --> 16:20.760
I think I think improvements can be found in all kinds of places, both small improvements

16:20.760 --> 16:22.440
and large improvements.

16:22.440 --> 16:28.480
I think the way to think about it is that while the current thing that's being done keeps

16:28.560 --> 16:34.160
getting better as you keep on increasing the amount of compute and data that you put into

16:34.160 --> 16:35.160
it.

16:35.160 --> 16:39.760
So we have that property, the bigger you make it, the better it gets.

16:39.760 --> 16:46.480
It is also the property that different things get better by different amounts as you keep

16:46.480 --> 16:48.680
on improving, as you keep on scaling them up.

16:48.680 --> 16:52.440
So not only you want to, of course, scale up what we are doing, we also want to keep

16:52.440 --> 16:55.400
scaling up the best thing possible.

16:55.400 --> 17:01.040
What is a, I mean, you probably don't need to predict because you can see internally,

17:01.040 --> 17:05.760
what do you think is improving most from a capability perspective in the current generation

17:05.760 --> 17:06.960
of scale?

17:06.960 --> 17:16.040
The best way for me to answer this question would be to point out the, to point to the

17:16.040 --> 17:18.760
models that are publicly available.

17:18.760 --> 17:23.040
And you can see how they compare from this year to last year.

17:23.040 --> 17:24.520
And the difference is quite significant.

17:24.560 --> 17:28.600
I'm not talking about the difference between, not only the difference between, let's say

17:28.600 --> 17:34.040
you can look at the difference between GPT-3 and GPT-3.5 and then chat GPT, chat GPT-4,

17:34.040 --> 17:37.880
chat GPT-4 with vision, and you can just see for yourself.

17:37.880 --> 17:42.960
It's easy to forget where things used to be, but certainly the big way in which things

17:42.960 --> 17:49.120
are changing is that these models become more and more reliable before they were very, they

17:49.120 --> 17:51.960
were only very partly there.

17:51.960 --> 17:54.720
Right now they are mostly there, but there are still gaps.

17:54.720 --> 17:58.720
And in the future, perhaps these models will be there even more.

17:58.720 --> 18:02.080
You could trust their answers, they'll be more reliable, they'll be able to do more

18:02.080 --> 18:05.360
tasks in general across the board.

18:05.360 --> 18:09.560
And then another thing that they will do is that they'll have deeper insight.

18:09.560 --> 18:15.240
As we train them, they gain more and more insight into the true nature of the human

18:15.240 --> 18:18.640
world and their insight will continue to deepen.

18:18.640 --> 18:23.520
I was just going to ask about how that relates to sort of model scale over time, because

18:23.520 --> 18:29.040
a lot of people are really stricken by the capabilities of the very large-scale models

18:29.040 --> 18:32.240
and the emergent behavior in terms of understanding of the world.

18:32.240 --> 18:35.840
And then in parallel, as people incorporate some of these things into products, which is

18:35.840 --> 18:39.840
a very different type of path, they often start worrying about inference costs going

18:39.840 --> 18:42.680
up with the scale of the model, and therefore they're looking for smaller models that are

18:42.680 --> 18:43.680
fine-tuned.

18:43.680 --> 18:47.360
But then, of course, you may lose some of the capabilities around some of the insights

18:47.360 --> 18:49.680
and ability to reason.

18:49.680 --> 18:53.720
And so I was curious in your thinking in terms of how all this evolves over the coming years.

18:53.720 --> 18:57.120
I would actually point out that the main thing that's lost when you switch to the smaller

18:57.120 --> 18:59.360
models is reliability.

18:59.360 --> 19:06.280
I would argue that at this point, it is reliability that's the biggest bottleneck to these models

19:06.280 --> 19:07.280
being truly useful.

19:07.280 --> 19:09.360
How are you defining reliability?

19:09.360 --> 19:14.960
So it's like when you ask a question that's not much harder than other questions that

19:14.960 --> 19:20.560
the model succeeds at, then you have a very high degree of confidence that it will continue

19:20.560 --> 19:21.560
to succeed.

19:21.560 --> 19:22.560
So I'll give you an example.

19:22.560 --> 19:27.800
Let's suppose that I want to learn about some historical thing, and I can ask, but tell

19:27.800 --> 19:32.800
me what is the prevailing opinion about this and about that, and I can keep asking questions.

19:32.800 --> 19:36.360
And let's suppose I answered 20 of my questions correctly.

19:36.360 --> 19:41.400
I really don't want the 21st question to have a gross mistake.

19:41.400 --> 19:43.240
That's what I mean by reliability.

19:43.240 --> 19:46.600
Or let's suppose I upload some documents, some financial documents.

19:46.600 --> 19:47.600
Suppose they say something.

19:47.600 --> 19:50.920
I want you to do some analysis and to make some conclusion, and I want to take action

19:50.920 --> 19:53.320
on this basis and this conclusion.

19:53.320 --> 19:58.760
And it's not a super hard task, and these models clearly succeed on this task most of

19:58.760 --> 19:59.760
the time.

19:59.760 --> 20:02.520
But because they don't succeed all the time, and if it's a consequential decision, I actually

20:02.520 --> 20:07.280
can't trust the model any of those times, and I have to verify the answer somehow.

20:07.280 --> 20:09.120
So that's how I define reliability.

20:09.120 --> 20:10.640
It's very similar to the cell driving situation.

20:11.040 --> 20:17.040
If you have a cell driving car and it's like, does things mostly well, that's not good enough.

20:17.040 --> 20:21.440
The situation is not as extreme as with a cell driving car, but that's what I mean by reliability.

20:21.440 --> 20:26.360
My perception of reliability is that, to your point, it goes up with model scale, but also

20:26.360 --> 20:31.760
it goes up if you fine tune for specific use cases or instances or data sets.

20:31.760 --> 20:38.000
And so there is that trade-off in terms of size versus specialized fine tuning versus

20:38.000 --> 20:40.080
reliability.

20:40.080 --> 20:45.720
So certainly, people who care about some specific application have every incentive to get the

20:45.720 --> 20:49.640
smallest model working well enough.

20:49.640 --> 20:50.960
I think that's true.

20:50.960 --> 20:51.960
It's undeniable.

20:51.960 --> 20:55.400
I think anyone who cares about a specific application will want the smallest model for

20:55.400 --> 20:56.400
it.

20:56.400 --> 20:57.400
That's self-evident.

20:57.400 --> 21:02.560
I do think, though, that as models continue to get larger and better, then they will unlock

21:02.560 --> 21:06.480
new and unprecedentedly valuable applications.

21:06.480 --> 21:10.000
So yeah, the small models will have their niche for the less interesting applications,

21:10.000 --> 21:11.600
which are still very useful.

21:11.600 --> 21:15.560
And then the bigger models will be delivering on applications.

21:15.560 --> 21:19.040
Okay, let's pick an example.

21:19.040 --> 21:22.160
Consider the task of producing good legal advice.

21:22.160 --> 21:25.200
It's really valuable if you can really trust the answer.

21:25.200 --> 21:27.800
Maybe you need a much bigger model for it, but it justifies the cost.

21:27.800 --> 21:35.760
There's been a lot of investment this year at the 7B in particular, about 7B, 13B, 34B

21:35.760 --> 21:37.760
sizes.

21:37.760 --> 21:40.960
Do you think continued research at those scales is wasted?

21:40.960 --> 21:42.960
No, of course not.

21:42.960 --> 21:52.720
I mean, I think that in the kind of medium term by a high time scale anyway, there will

21:52.720 --> 21:59.680
be an ecosystem, there will be different uses for different model sizes, there will be plenty

21:59.680 --> 22:05.680
of people who are very excited for whom the best 7B model is good enough, they'll be very

22:05.680 --> 22:06.680
happy with it.

22:06.720 --> 22:11.360
And then there'll be plenty of very, very exciting and amazing applications for which

22:11.360 --> 22:13.360
it won't be enough.

22:13.360 --> 22:14.640
I think that's all.

22:14.640 --> 22:21.200
I mean, I think the big models will be better than the small models, but not all applications

22:21.200 --> 22:24.000
will justify the cost of a large model.

22:24.000 --> 22:28.000
What do you think the role of open sources in this ecosystem?

22:28.000 --> 22:29.400
Well, open source is complicated.

22:29.400 --> 22:32.480
I'll describe to you my mental picture.

22:32.520 --> 22:37.400
I think that in the near term, open source is just helping companies produce useful.

22:39.400 --> 22:44.400
Like, let's see, why would one want to have an open source, but use an open source model

22:44.400 --> 22:48.400
instead of a closed source model that's hosted by some other company?

22:48.400 --> 22:57.560
I mean, I think it's very valid to want to be the final decider on the exact way in which

22:57.560 --> 22:59.320
you want your model to be used.

22:59.320 --> 23:04.120
And for you to make the decision of exactly how you want the model to be used and which

23:04.120 --> 23:06.720
use case you wish to support.

23:06.720 --> 23:09.000
And I think there's going to be a lot of demand for open source models.

23:09.000 --> 23:12.080
And I think there will be quite a few companies that will use them.

23:12.080 --> 23:14.760
And I'd imagine that will be the case in the near term.

23:14.760 --> 23:20.800
I would say in the long run, I think the situation with open source models will become more complicated

23:20.800 --> 23:23.680
and I'm not sure what the right answer is there.

23:23.680 --> 23:25.400
Right now it's a little bit difficult to imagine.

23:25.400 --> 23:33.040
So we need to put our future hat, maybe futurist hat, it's not too hard to get into a sci-fi

23:33.040 --> 23:36.800
mood when you remember that we are talking to computers and they understand us.

23:36.800 --> 23:40.120
But so far, these computers, these models actually not very competent.

23:40.120 --> 23:43.440
They can't do tasks at all.

23:43.440 --> 23:50.160
I do think that the will come a day where the level of capability of models will be

23:50.160 --> 23:51.160
very high.

23:51.160 --> 23:55.000
Like in the end of the day, intelligence is power, right?

23:55.000 --> 23:59.560
Right now, these models, their main impact, I would say at least popular impact is primarily

23:59.560 --> 24:03.120
around entertainment and like simple question and answer.

24:03.120 --> 24:05.440
So you talk to a model, wow, this is so cool.

24:05.440 --> 24:09.560
You produce some images, you had a conversation, maybe you had some questions, good answer.

24:09.560 --> 24:17.080
But it's very different from completing some large and complicated task like, what about

24:17.080 --> 24:23.960
if you had a model which could autonomously start and build a large tech company?

24:24.000 --> 24:29.360
I think if these models were open source, they would have a difficult to predict consequence.

24:29.360 --> 24:31.400
Like we are quite far from these models right now.

24:31.400 --> 24:36.040
And by quite far, I mean by itime scale, but still like, this is not what you're talking

24:36.040 --> 24:41.080
about, but the day will come when you have models which can do science autonomously,

24:41.080 --> 24:45.600
like be delivered on big science projects.

24:45.600 --> 24:52.400
And it becomes more complicated as to whether it is desirable that models of such power

24:52.400 --> 24:54.520
should be open sourced.

24:54.520 --> 24:59.200
I think the argument there is a lot less clear cut, a lot less straightforward compared

24:59.200 --> 25:04.480
to the current level models, which are very useful and I think it's fantastic that the

25:04.480 --> 25:06.000
current level models have been built.

25:06.000 --> 25:10.320
So like that is maybe, maybe I answered a slightly bigger question rather than what

25:10.320 --> 25:13.680
is the role of open source models, what's the deal with open source?

25:13.680 --> 25:19.280
And the deal is up to a certain capability, it's great, but not difficult to imagine models

25:19.280 --> 25:23.880
that are sufficiently powerful, which will be built where it becomes a lot less obvious

25:23.880 --> 25:27.560
as to the benefits of their open source.

25:27.560 --> 25:32.200
Is there a signal for you that we've reached that level or that we're approaching it?

25:32.200 --> 25:35.120
Like what's the boundary?

25:35.120 --> 25:41.760
So I think figuring out this boundary very well is an urgent research project.

25:41.760 --> 25:49.920
I think one of the things that help is that the closed source models are more capable

25:49.920 --> 25:51.000
than open source models.

25:51.000 --> 25:54.520
So the closed source models could be studied and so on.

25:54.520 --> 25:58.560
And so you'd have some experience with a generation of closed source model and then you know

25:58.560 --> 26:01.480
like, oh, these models capabilities, it's fine, there's no big deal there.

26:01.480 --> 26:06.280
Then in a couple years, the open source models catch up and maybe a day will come when we

26:06.280 --> 26:11.000
going to say, well, like these closed source models, they're getting a little too drastic

26:11.000 --> 26:13.480
and then some other approaches needed.

26:13.480 --> 26:21.200
If we have our future hat on, maybe it looks like think about like a several year timeline.

26:21.200 --> 26:26.160
What are the limits you see if any in the near term in scaling?

26:26.160 --> 26:32.600
Is it like data, token scarcity, cost of compute, architectural issues?

26:32.600 --> 26:37.520
So the most near term limit to scaling is obviously data.

26:37.520 --> 26:42.640
This is well known and some research is required to address it.

26:42.640 --> 26:49.280
Without going into the details, I'll just say that the data limit can be overcome and

26:49.280 --> 26:51.200
progress will continue.

26:51.200 --> 26:55.560
One question I've heard people debate a little bit is a degree to which the transformer based

26:55.560 --> 27:00.800
models can be applied to sort of the full set of areas that you'd need for AGI.

27:00.800 --> 27:05.000
And if you look at the human brain, for example, you do have reasonably specialized systems

27:05.000 --> 27:10.200
or all neural networks, be it specialized systems for the visual cortex versus areas

27:10.200 --> 27:15.000
of higher thought, areas for empathy or other sort of aspects of everything from personality

27:15.000 --> 27:16.800
to processing.

27:16.800 --> 27:20.560
Do you think that the transformer architectures are the main thing that will just keep going

27:20.560 --> 27:21.560
and get us there?

27:21.560 --> 27:24.080
Do you think we'll need other architectures over time?

27:24.080 --> 27:30.360
So I understand precisely what you're saying and I have two answers to this question.

27:30.360 --> 27:35.680
The first is that, in my opinion, the best way to think about the question of architecture

27:35.680 --> 27:39.880
is not in terms of a binary, is it enough?

27:39.880 --> 27:48.040
But how much effort, what will be the cost of using this particular architecture?

27:48.040 --> 27:53.040
At this point, I don't think anyone doubts that the transformer architecture can do amazing

27:53.040 --> 27:59.160
things, but maybe something else, maybe some modification could have some compute efficiency

27:59.160 --> 28:00.160
benefits.

28:00.160 --> 28:03.600
So it's better to think about it in terms of compute efficiency rather than in terms

28:03.600 --> 28:06.040
of can it get there at all?

28:06.040 --> 28:09.680
I think at this point, the answer is obviously yes.

28:09.680 --> 28:14.040
To the question about, well, what about the human brain and its brain regions?

28:14.040 --> 28:21.520
I actually think that the situation there is subtle and deceptive for the following reasons.

28:21.520 --> 28:26.360
So what I believe you alluded to is the fact that the human brain has known regions.

28:26.360 --> 28:31.240
It has a speech perception region, it has a speech production region, it has an image

28:31.240 --> 28:36.520
region, it has a face region, it has all these regions, and it looks like it's specialized.

28:36.520 --> 28:39.440
But you know what's interesting?

28:39.440 --> 28:44.680
Sometimes there are cases where very young children have severe cases of epilepsy at

28:44.680 --> 28:50.800
a young age, and the only way they figured out how to treat such children is by removing

28:50.800 --> 28:53.320
half of their brain.

28:53.320 --> 29:00.080
Because it happens at such a young age, these children grow up to be pretty functional adults,

29:00.080 --> 29:05.560
and they have all the same brain regions, but they are somehow compressed onto one hemisphere.

29:05.560 --> 29:09.920
So maybe some information processing efficiency is lost.

29:09.920 --> 29:13.600
It's a very traumatic thing to experience, but somehow all these brain regions rearrange

29:13.600 --> 29:14.600
themselves.

29:14.600 --> 29:20.360
There is another experiment which was done maybe 30 or 40 years ago on ferrets.

29:20.360 --> 29:22.120
So the ferret is a small animal.

29:22.120 --> 29:23.360
It's a pretty mean experiment.

29:23.360 --> 29:29.920
They took the optic nerve of the ferret, which comes from its eye, and attached it to its

29:29.920 --> 29:31.320
auditory cortex.

29:31.320 --> 29:36.640
So now the inputs from the eye starts to map to the speech processing area of the brain.

29:36.640 --> 29:41.520
And then they recorded different neurons after it had a few days of learning to see, and

29:41.520 --> 29:45.480
they found neurons in the auditory cortex, which were very similar to the visual cortex.

29:45.480 --> 29:51.160
Or vice versa, it was either they mapped their eye to the ear, to the auditory cortex, or

29:51.160 --> 29:52.400
the ear to the visual cortex.

29:52.400 --> 29:54.680
But something like this has happened.

29:54.680 --> 30:00.360
These are fairly well-known ideas in AI that the cortex of humans and animals are extremely

30:00.360 --> 30:01.680
uniform.

30:01.680 --> 30:05.320
And so that further supports the AI, like you just need one big, uniform architecture.

30:05.320 --> 30:06.320
So all you need.

30:06.320 --> 30:07.320
Yeah.

30:07.320 --> 30:10.080
In general, it seems like every biological system is reasonably lazy in terms of taking

30:10.080 --> 30:12.960
one system and then reproducing it and then reusing it in different ways.

30:12.960 --> 30:16.520
And that's true of everything from DNA encoding, you know, there's 20 amino acids and protein

30:16.520 --> 30:17.520
sequences.

30:17.520 --> 30:22.240
Everything is made out of the same 20 amino acids on through to your point, sort of how

30:22.240 --> 30:23.800
you think about tissue architecture.

30:23.800 --> 30:27.040
So it's remarkable that that carries over into the digital world as well, depending

30:27.040 --> 30:28.280
on the architecture you use.

30:28.280 --> 30:33.200
I mean, the way I see it is that this is an indication from a technological point of

30:33.200 --> 30:34.200
view.

30:34.200 --> 30:37.640
We are very much on the right track because you have all these interesting analogies

30:37.640 --> 30:41.560
between human intelligence and biological intelligence and artificial intelligence.

30:41.560 --> 30:47.680
We've got artificial neurons, biological neurons, unified brain architecture for biological

30:47.680 --> 30:51.600
intelligence, unified neural network architecture for artificial intelligence.

30:51.600 --> 30:56.000
At what point do you think we should start thinking about these systems in digital life?

30:56.000 --> 30:57.320
I can answer that question.

30:57.320 --> 31:04.040
I think that will happen when those systems become reliable in such a way as to be very

31:04.040 --> 31:05.040
autonomous.

31:05.040 --> 31:08.120
Right now, those systems are clearly not autonomous.

31:08.120 --> 31:10.720
They're inching there, but they're not.

31:10.720 --> 31:13.680
And that makes them a lot less useful too, because you can't ask it, hey, like, do my

31:13.680 --> 31:16.840
homework or do my taxes or you see what I mean.

31:16.840 --> 31:19.120
So the usefulness is greatly limited.

31:19.120 --> 31:23.960
As the usefulness increases, they will indeed become more like artificial life, which also

31:23.960 --> 31:29.080
makes it more, I would argue, trepidations, right?

31:29.080 --> 31:33.880
Like if you imagine actual artificial life with brains that are smarter than humans,

31:33.880 --> 31:37.920
gosh, that seems pretty monumental.

31:37.920 --> 31:40.480
Why is your definition based on autonomy?

31:40.480 --> 31:45.160
Because if you often look at the definition of biological life, it has to do with reproductive

31:45.160 --> 31:46.160
capability.

31:46.160 --> 31:48.400
Plus, I guess some form of autonomy, right?

31:48.400 --> 31:52.000
Like a virus isn't really necessarily considered alive much of the time, right?

31:52.000 --> 31:53.920
But a bacteria is.

31:53.920 --> 31:58.040
And you could imagine situations where you have symbiotic relationships or other things

31:58.040 --> 32:00.840
where something can't really quite function autonomously, but it's still considered a

32:00.840 --> 32:01.840
life form.

32:01.840 --> 32:05.320
So I'm a little bit curious about autonomy being the definition versus some of these other

32:05.320 --> 32:06.320
aspects.

32:06.320 --> 32:12.120
Well, I mean, definitions are chosen for our convenience and it's a matter of debate.

32:12.120 --> 32:16.640
In my opinion, technology already has the reproduction, the reproductive function, right?

32:16.640 --> 32:20.560
And if you look at for example, I don't know if you've seen those images of the evolution

32:20.560 --> 32:24.920
of cell phones and then smartphones over the past 25 years, you got this like, what almost

32:24.920 --> 32:28.600
looks like an evolutionary tree or the evolution of cars over the past century.

32:28.600 --> 32:33.320
So technology is already reproducing using the minds of people who copy ideas from previous

32:33.320 --> 32:35.120
generation of technology.

32:35.120 --> 32:37.560
So I claim that the reproduction is already there.

32:37.560 --> 32:40.880
The autonomy piece I claim is not.

32:40.880 --> 32:43.520
And indeed, I also agree that there is no autonomous reproduction.

32:43.520 --> 32:48.280
But that would be like, can you imagine if you have like autonomously reproducing AIs?

32:48.280 --> 32:54.720
I actually think that that is a pretty traumatic and I would say quite a scary thing if you

32:54.720 --> 32:58.760
have an autonomously reproducing AI, if it's also very capable.

32:58.760 --> 33:00.840
Should we talk about super alignment?

33:00.840 --> 33:02.800
Yeah, very much so.

33:02.800 --> 33:05.400
Can you just sort of define it?

33:05.400 --> 33:11.360
And then we were talking about what the boundary is for when you feel we need to begin to worry

33:11.360 --> 33:15.880
about these capabilities being in open source.

33:15.880 --> 33:19.240
What is super alignment and why invest in it now?

33:19.240 --> 33:27.040
The answer to your question really depends to where you think AI is headed.

33:27.040 --> 33:31.200
If you just try to imagine and look into the future, which is of course a very difficult

33:31.200 --> 33:35.320
thing to do, but let's try to do it anyway.

33:35.320 --> 33:38.760
Where do we think things will be in five years or in 10 years?

33:38.760 --> 33:43.240
I mean, progress has been really stunning over the past few years.

33:43.240 --> 33:45.560
Maybe it will be a little bit slower.

33:45.560 --> 33:49.800
But still, if you extrapolate this kind of progress, you'll be in a very, very different

33:49.800 --> 33:54.360
place in five years, let alone 10 years.

33:54.360 --> 33:56.720
It doesn't seem implausible.

33:56.720 --> 34:02.920
It doesn't seem at all implausible that we will have computers, data centers that are

34:02.920 --> 34:05.040
much smarter than people.

34:05.040 --> 34:09.240
And by smarter, I don't mean just have more memory or have more knowledge, but I also

34:09.240 --> 34:16.840
mean have deeper insight into the same subjects that we people are studying and looking into.

34:16.840 --> 34:20.840
It means learn even faster than people.

34:20.840 --> 34:23.800
What would such AIs do?

34:23.800 --> 34:25.200
I don't know.

34:25.360 --> 34:30.640
If such an AI were the basis of some artificial life, it would be, well, how do you even think

34:30.640 --> 34:31.640
about it?

34:31.640 --> 34:36.960
If you have some very powerful data center that's also alive in a sense, that's what

34:36.960 --> 34:37.960
you're talking about.

34:37.960 --> 34:42.600
And when I imagine this world, my reaction is, gosh, this is very unpredictable what's

34:42.600 --> 34:43.600
going to happen.

34:43.600 --> 34:44.600
Very unpredictable.

34:44.600 --> 34:49.840
But the bare minimum, but there is a bare minimum which we can articulate.

34:49.840 --> 34:57.560
But if such super, if such very, very intelligent, super intelligent data centers are being built

34:57.560 --> 35:04.400
at all, we want those data centers to hold warm and positive feelings towards people,

35:04.400 --> 35:06.600
towards humanity.

35:06.600 --> 35:11.560
Because this is going to be non-human life in a sense.

35:11.560 --> 35:12.560
Potentially.

35:12.560 --> 35:15.040
It could potentially be that.

35:15.040 --> 35:21.440
So I would want that any instance of such super intelligence, the warm feelings towards

35:21.440 --> 35:22.440
humanity.

35:22.440 --> 35:25.640
And so this is what we are doing with the super alignment project.

35:25.640 --> 35:31.360
You're saying, hey, if you just allow yourself, if you just accept that progress that we've

35:31.360 --> 35:35.320
seen, maybe it will be slower, but it will continue.

35:35.320 --> 35:44.640
If you allow yourself that, then can you start doing productive work today to build the science

35:44.640 --> 35:53.920
so that we will be able to handle the problem of controlling such future super intelligence.

35:53.920 --> 36:00.720
Of imprinting onto them a strong desire to be nice and kind to people.

36:00.720 --> 36:05.880
Because those data centers, right, they'll be, they'll be really quite powerful.

36:05.880 --> 36:09.720
You know, there'll probably be many of them that will be, the world will be very complicated.

36:09.720 --> 36:15.200
But somehow to the extent that they are autonomous, to the extent that they are agents, to the

36:15.200 --> 36:22.640
extent they are beings, I want them to be pro-social, pro-human social.

36:22.640 --> 36:23.640
That's the goal.

36:23.640 --> 36:27.200
What do you think is the likelihood of that goal?

36:27.200 --> 36:33.440
I mean, some of it, it feels like a outcome you can hopefully affect, right?

36:33.440 --> 36:39.360
But are we, are we likely to have pro-social AIs that we are friends with individually

36:39.360 --> 36:42.040
or, you know, as a species?

36:42.040 --> 36:48.160
Well, I mean, friends be, I think that that part is not necessary.

36:48.160 --> 36:52.040
The friendship piece, I think, is optional, but I do think that we want to have very pro-social

36:52.040 --> 36:53.360
AI.

36:53.360 --> 36:55.360
I think it's, I think it's possible.

36:55.360 --> 36:57.640
I don't think it's guaranteed, but I think it's possible.

36:57.640 --> 37:02.720
I think it's going to be possible and the possibility of that will increase insofar

37:02.800 --> 37:08.320
as more and more people allow themselves to look into the future, into the five to ten

37:08.320 --> 37:16.120
year future and just ask yourself, what, what do you expect AI to be able to do then?

37:16.120 --> 37:19.320
How capable do you expect it to be then?

37:19.320 --> 37:27.120
And I think that with each passing year, if indeed AI continues to improve and as people

37:27.120 --> 37:32.700
get to experience, because right now we are talking, making arguments, but if you actually

37:32.700 --> 37:38.820
get to experience, oh gosh, the AI from last year, which was really helpful this year puts

37:38.820 --> 37:44.180
the previous one to shame and you go, OK, and then one year later and when it's starting

37:44.180 --> 37:50.380
to do science, the AI software engineer is starting to get really quite good, let's say.

37:50.380 --> 37:58.340
I think that will create a lot more desire in people for what you just described, for

37:58.340 --> 38:02.380
the future superintelligence to indeed be very pro-social.

38:02.500 --> 38:05.300
I think there's going to be a lot of disagreement, there's going to be a lot of political questions,

38:05.300 --> 38:12.180
but I think that as people see AI actually getting better, as people experience it, the

38:12.180 --> 38:19.820
desire for the pro-social superintelligence, the humanity loving superintelligence, as

38:19.820 --> 38:23.580
much as it can be done, will increase.

38:23.580 --> 38:28.460
And on the scientific problem, I think right now it's still being an area where not that

38:28.460 --> 38:31.100
many people are working on.

38:31.100 --> 38:35.300
Our AI is getting powerful enough, you can really start studying it productively, you'll

38:35.300 --> 38:38.340
have some very exciting research to share soon.

38:38.340 --> 38:43.500
But I would say that's the big picture situation here.

38:43.500 --> 38:47.540
Just really, it really boils down to look at what you've experienced with AI up until

38:47.540 --> 38:54.380
now, ask yourself, like, is it slowing down, will it slow down next year, like, we will

38:54.460 --> 38:58.980
see, and we will experience it again and again, and I think it will keep, and what needs to

38:58.980 --> 39:00.900
be done, it will keep becoming clearer.

39:00.900 --> 39:02.820
Do you think we're just on an accelerated path?

39:02.820 --> 39:06.900
Because I think fundamentally, if you look at certain technology waves, they tend to

39:06.900 --> 39:10.420
inflect and then accelerate versus decelerate.

39:10.420 --> 39:14.740
And so it really feels like we're in an acceleration phase right now versus the deceleration phase.

39:14.740 --> 39:15.740
Yeah.

39:15.740 --> 39:20.580
I mean, VR, right now it is indeed the case that VR in an acceleration phase.

39:20.580 --> 39:27.260
You know, it's hard to say, you know, multiple forces will come into play.

39:27.260 --> 39:30.540
Some forces are accelerating forces and some forces are decelerating.

39:30.540 --> 39:36.140
So for example, the cost and scale are a decelerating force.

39:36.140 --> 39:41.220
The fact that our data is finite is a decelerating force to some degree at least, I don't want

39:41.220 --> 39:42.220
to overstate it.

39:42.220 --> 39:43.780
Yeah, it's kind of within an asymptote, right?

39:43.780 --> 39:48.020
Like at some point you hit it, but it's the standard S curve, right, or sigmoidal.

39:49.020 --> 39:52.500
With the data in particular, I just think it won't be, it just won't be an issue because

39:52.500 --> 39:55.340
we'll figure out something else.

39:55.340 --> 39:59.860
But then you might argue that the size of the engineering project is a decelerating force,

39:59.860 --> 40:01.540
just the complexity of management.

40:01.540 --> 40:05.500
On the other hand, the amount of investment is an accelerating force, the amount of interest

40:05.500 --> 40:09.220
from people, from engineers, scientists is an accelerating force.

40:09.220 --> 40:11.620
And I think there is one other accelerating force.

40:11.620 --> 40:17.180
And that is the fact that biological evolution has been able to figure it out and the fact

40:17.180 --> 40:24.860
that up until now, progress in AI has had up until this point, this weird property that

40:24.860 --> 40:30.220
it's kind of been, you know, it's been very hard to execute on, but in some sense it's

40:30.220 --> 40:36.420
also been more straightforward than one would have expected perhaps.

40:36.420 --> 40:42.140
Like in some sense, I don't know much physics, but my understanding is that if you want to

40:42.140 --> 40:48.860
make progress in quantum physics or something, you need to be really intelligent and spend

40:48.860 --> 40:53.380
many years in grad school studying how these things work.

40:53.380 --> 40:57.260
Whereas with AI, if people come in, get up to speed quickly, start making contributions

40:57.260 --> 40:59.340
quickly, it has the flavor is somehow different.

40:59.340 --> 41:05.340
Somehow it's very, there is some kind of, there's a lot of give to this particular area

41:05.340 --> 41:06.340
of research.

41:06.340 --> 41:07.980
And I think this is also an accelerating force.

41:07.980 --> 41:10.900
How will it all play out remains to be seen.

41:10.900 --> 41:15.580
Like it may be that somehow the scale required of engineering complexity will start to make

41:15.580 --> 41:17.820
it so that the rate of progress will start to slow down.

41:17.820 --> 41:20.940
It will still continue, but maybe not as quick as we had before.

41:20.940 --> 41:25.460
Or maybe the forces which are coming together to push it will be such that it will be as

41:25.460 --> 41:29.700
fast for maybe a few more years before it will start to slow down.

41:29.700 --> 41:33.060
If at all that's, that would be my articulation here.

41:33.060 --> 41:35.260
Ilya, this has been a great conversation.

41:35.260 --> 41:36.820
Thanks for joining us.

41:36.820 --> 41:37.820
Thank you so much for the conversation.

41:37.820 --> 41:39.820
I really enjoyed it.

41:39.820 --> 41:42.660
Join us on Twitter at NoPriorsPod.

41:42.660 --> 41:45.460
Subscribe to our YouTube channel if you want to see our faces.

41:45.460 --> 41:49.100
Follow the show on Apple podcasts, Spotify, or wherever you listen.

41:49.100 --> 41:51.180
That way you get a new episode every week.

41:51.180 --> 41:55.540
And sign up for emails or find transcripts for every episode at no-priors.com.

