start	end	text
0	11560	OpenAI, a company that we all know now, but only a year ago, was 100 people, is changing
11560	12560	the world.
12560	15560	Their research is leading the charge to AGI.
15560	19800	Since ChatGPT captured consumer attention last November, they show no signs of slowing
19800	21080	down.
21080	25680	This week, a lot of nice sit down with Ilya Sutskover, co-founder and chief scientist
25680	31880	at OpenAI to discuss the state of AI research, where will hit limits, the future of AGI,
31880	33920	and what it's going to take to reach super alignment.
33920	36240	Ilya, welcome to NoPriors.
36240	37240	Thank you.
37240	38240	It's good to be here.
38240	39240	Let's start at the beginning.
39240	43400	Pre-AlexNet, nothing in deep learning was really working, and then given that environment,
43400	46360	you guys took a very unique bet.
46360	48640	What motivated you to go in this direction?
49400	59160	In those dark ages, AI was not an area where people had hope, and people were not accustomed
59160	62040	to any kind of success at all.
62040	66840	Because there hasn't been any success, there was a lot of debate, and there were different
66840	73360	schools of thoughts that had different arguments about how machine learning and AI should be.
73360	78760	You had people who were into knowledge representation from a good old fashioned AI.
78760	83280	You had people who were Bayesian, and they liked Bayesian non-parametric methods.
83280	87360	You had people who like graphical models, and you had the people who like neural networks.
87360	93480	Those people were marginalized because neural networks did not have the property that you
93480	95640	can't prove math theorems about them.
95640	99880	If you can't prove theorems about something, it means that your research isn't good.
99880	101520	That's how it has been.
101520	105680	The reason why I gravitated to neural networks from the beginning is because it felt like
105680	108000	those are small little brains.
108000	110280	Who cares if you can't prove any theorems about them?
110280	116560	Because we are training small little brains, and maybe they'll do something one day.
116560	121680	The reason that we were able to do AlexNet when we did it is because of a combination
121680	124400	of two factors, three factors.
124400	130760	The first factor is that this was shortly after GPUs started to be used in machine learning.
130760	135560	People had an intuition that that's a good thing to do, but it wasn't like today where
135560	137560	people exactly knew what an NGPU is for.
137560	141560	It was like, let's play with those cool fast computers and see what we can do with them.
141560	147160	It was an especially good fit for neural networks, so that definitely helped us.
147160	154280	I was very fortunate in that I was able to realize that the reason neural networks of
154280	157720	the time weren't good is because they were too small.
157720	163880	If you try to solve a vision task with a neural network which has like a thousand neurons,
163880	165040	what can it do?
165040	166520	It can't do anything.
166520	170120	It doesn't matter how good your learning is and everything else, but if you have a much
170120	173240	larger neural network, it will do something unprecedented.
173240	176680	I'll give you the intuition to think that that was the case, because I think at the
176680	182680	time it was reasonably contrarian to think that despite the human brain in some sense
182680	185360	works that way or different biological neural circuits.
185360	189600	I'm just curious what gave you that intuition early on to think that this was a good direction.
189600	198680	I think looking at the brain and specifically, if all those things follow very easily, if
198680	207880	you allow yourself to accept the idea, right now this idea is reasonably well accepted.
207880	212480	Back then, people still talked about it, but they haven't really accepted it or internalized.
212480	218700	The idea that maybe an artificial neuron in some sense is not that different from a biological
218700	220200	neuron.
220200	225600	Now whatever you imagine animals do with their brains, you could perhaps assemble some artificial
225600	227800	neural network of similar size.
227800	233320	Maybe if you train it, it will do something similar.
233320	237280	That leads you to start to imagine.
237280	239880	Almost imagine the computation being done by the neural network.
239880	245520	You can almost think like if you have a high resolution image and you have like one neuron
245520	247960	for like a large group of pixels, what can the neuron do?
247960	251800	It's just not much it can do, but if you have a lot of neurons, then they can actually do
251800	254000	something and compute something.
254000	259440	So I think it was like, all right, like it was considerations like this, plus a technical
259440	260440	realization.
260440	269240	The technical realization is that if you have a large training set that specifies the behavior
269240	275440	of the neural network, and the training set is large enough such that it can constrain
275440	277960	the large neural network sufficiently.
277960	282000	And furthermore, if you have the algorithm to find that neural network, because what
282000	288240	we do is that we turn the training set into a neural network which satisfies the training
288240	289360	set.
289360	296720	Neural network training can almost be seen as solving a neural equation.
296720	301680	Solving a neural equation where every data point is an equation, and every parameter
301680	305000	is a variable.
305000	307960	And so it was multiple things.
307960	312480	The realization that the bigger neural network could do something unprecedented.
312480	319480	The realization that if you have a large data set together with the compute to solve the
319480	324480	neural equation, that's what gradient descent comes in, but it's not gradient descent.
324480	326200	Gradient descent was around for a long time.
326200	330320	It was certain technical insights about how to make it work.
330320	333480	Because back then the prevailing belief was, well, you can't train those neural nets anything.
333480	334480	It's all hopeless.
334480	335760	So it wasn't just about the site.
335760	340920	It was about, even if someone did think, gosh, it would be cool to train a big neural net,
340920	346320	they didn't have the technical ability to turn this idea into reality.
346320	350920	You needed not only to code the neural net, you need to do a bunch of things right, and
350920	352920	only then it will work.
352920	357840	And then another fortunate thing is that the person whom I work with, Alex Krzyzewski,
357840	362920	he just discovered that he really loves GPUs, and he was perhaps one of the first person
362920	370120	who really mastered writing really, like, really performant code for the GPUs.
370120	374560	And that's why we were able to squeeze a lot of performance out of two GPUs and produce
374560	375640	something unprecedented.
375640	379520	So to sum up, it was multiple things.
379520	383900	The idea that a big neural network, in this case a vision neural network, a convolutional
383900	388640	neural network with many layers, one that's much, much bigger than anything that's ever
388640	392880	been done before, could do something very unprecedented because the brain can see and
392880	394760	the brain is a large neural network.
394760	400080	And we can see quickly, so our neurons don't have a lot of time, then the compute needed,
400080	404880	the technical know-how that, in fact, we can't train such neural networks.
404880	406920	And it was not at all widely distributed.
406920	411120	People in machine learning would not have been able to train such a neural network even
411120	412120	if they wanted to.
412120	417960	Did you guys have any, like, particular goal from a size perspective?
417960	422400	Or was it just as, you know, and if that's biologically inspired or where that number
422400	425000	comes from or just as large as we can go?
425000	426000	Definitely as large as we can go.
426000	430560	Just keep in mind, I mean, we had a certain amount of compute which we could usefully
430560	434080	consume, and then what can it do?
434080	441240	Maybe if we think about just like the origin of open AI and the goals of the organization,
441240	444760	like what was the original goal and how's that evolved over time?
444760	450960	The goal did not evolve over time, the tactic evolved over time.
450960	457920	So the goal of open AI from the very beginning has been to make sure that artificial general
457920	465520	intelligence, by which we mean autonomous systems, AI, that can actually do most of
465520	471480	the jobs and the activities and tasks that people do, benefits all of humanity.
471480	473680	That was the goal from the beginning.
473680	479720	The initial thinking has been that maybe the best way to do it is by just open sourcing
479720	482320	a lot of technology.
482320	488080	We later, and we also attempted to do it as a nonprofit, seemed very sensible.
488080	493000	This is the goal, nonprofit is the way to do it, what changed.
493000	500240	Some point at open AI, we realized and we were perhaps among the earliest to realize
500240	505200	that to make progress in AI for real, you need a lot of compute.
505200	507400	Now, what does a lot mean?
507400	512800	The appetite for compute is truly endless, as now clearly seen, but we realized that
512800	521360	we will need a lot and a nonprofit wouldn't be the way to get there, wouldn't be able
521360	523120	to build a large cluster with a nonprofit.
523120	530120	That's where we became, we converted into this unusual structure called CAP Profit,
530120	533160	and to my knowledge, we are the only CAP Profit company in the world.
533760	539320	The idea is that investors put in some money, but even if the company does incredibly well,
539320	544760	they don't get more than some multiplier on top of their original investment.
544760	550680	The reason to do this, the reason why that makes sense, there are arguments, one could
550680	558160	make arguments against it as well, but the argument for it is that if you believe that
558160	566320	the technology that we are building, AGI, could potentially be so capable as to do every
566320	572240	single task that people do, does it mean that it might un-employ everyone?
572240	577760	Well, I don't know, but it's not impossible, and if that's the case, it makes sense, it
577760	581400	will make a lot of sense if the company that builds such a technology would not be able
581400	586560	to make infinite, would not be incentivized, rather, to make infinite profits.
586560	591080	I don't know if it will literally play out this way because of competition in AI, so
591080	597200	there will be multiple companies, and I think that will have some unforeseen implications
597200	599880	on the argument which I'm making, but that was the thinking.
599880	603840	I remember visiting the offices back when you were, I think, housed at YC or something,
603840	609240	or cohabited some space there, and at the time, there was a suite of different efforts.
609240	614520	There was robotic arms that were being manipulated, and then there was some video game related
614520	616920	work, which was really cutting edge.
616920	620840	How did you think about how the research agenda evolved, and what really drove it down this
620840	625120	path of transformer-based models and other forms of learning?
625120	632000	So I think it has been evolving over the years from when we started OpenAI.
632000	636120	In the first year, we indeed did some of the more conventional machine learning work.
636120	639680	The conventional machine learning work, I mean, because the world has changed so much,
639680	645640	a lot of things which were known to everyone in 2016 or 2017 are completely and utterly
645640	646640	forgotten.
646640	648720	It's like the Stone Age almost.
648720	653840	So in that Stone Age, the world of machine learning looked very different.
653840	658920	It was dramatically more academic.
658920	662440	The goals, values, and objectives were much more academic.
662440	667240	They were about discovering small bits of knowledge and sharing them with the other researchers
667240	670040	and getting scientific recognition as a result.
670040	672320	And it's a very valid goal, and it's very understandable.
672320	674240	I've been doing AI for 20 years now.
674240	677760	More than half of my time that I spent in AI was in that framework.
677760	679760	And so what do you do?
679760	682240	You write papers, you share your small discoveries.
682240	683240	Two realizations.
683240	689600	The first realization is just at a high level, it doesn't seem like it's the way to go for
689600	690960	a dramatic impact.
690960	692240	And why is that?
692240	699600	Because if you imagine how an AGI should look like, it has to be some kind of a big engineering
699600	703440	project that's using a lot of compute, right?
703440	705760	Even if you don't know how to build it, what that should look like.
705760	707640	You know that this is the ideal you want to strive towards.
707640	712000	So you want to somehow move towards larger projects as opposed to small projects.
712000	718640	So while we attempted a first large project where we trained a neural network to play
718640	723000	a real-time strategy game, as well as the best humans.
723000	730800	It's the Dota 2 project, and it was driven by two people, Jakob Pachotsky and Greg Brokman.
730800	733280	They really drove this project and made it a success.
733280	737280	And this was our first attempt at a large project.
737280	741840	But it wasn't quite the right formula for us, because the neural networks were a little
741840	742840	bit too small.
742840	745000	It was just a narrow domain, just a game.
745000	747840	I mean, it's cool to play a game, and it kept looking.
747840	752400	At some point, we realized that, hey, if you train a large neural network, a very, very
752400	758240	large transformer to predict text better and better, something very surprising will happen.
758240	761000	This realization also arrived a little bit gradually.
761000	764200	We were exploring generative models.
764200	768160	We were exploring ideas around next-word prediction.
768160	770160	Those are ideas also related to compression.
770160	772400	We were exploring them.
772400	773400	The transformer came out.
773400	774400	We got really excited.
774400	776600	We were like, this is the greatest thing.
776600	777840	We're going to do transformers now.
777840	780120	It's clearly superior than anything else before it.
780120	782720	We started doing transformers with the GPT-1.
782720	786520	GPT-1 started to show very interesting signs of life.
786520	788360	And that led us to doing GPT-2.
788360	789880	And then ultimately, GPT-3.
789880	795560	GPT-3 really opened everyone else's eyes as well to, hey, this thing has a lot of traction.
795560	799720	There is one specific formula right now that everyone is doing.
799720	804040	And this formula is train a larger and larger transformer on more and more data.
804040	809440	I mean, for me, the big wake-up moment to your point was GPT-2 to GPT-3 transition, where
809440	812320	you saw such a big step function and capabilities.
812320	819400	And then obviously, with four open eyes, published some really interesting research around some
819400	822840	of the different domains of knowledge or domains of expertise or chain of thought or other
822840	826160	things that the models can suddenly do in an emergent form.
826160	829320	What was the most surprising thing for you in terms of emergent behavior in these models
829320	830320	over time?
830320	832640	You know, it's very hard to answer that question.
832640	836520	It's very hard to answer because I'm too close and I've seen it progress every step of the
836520	838320	way.
838320	841960	So as much as I'd like, I find it very hard to answer that question.
841960	849840	I think if I had to pick one, I think maybe the most surprising thing for me is the whole
849840	852800	thing works at all.
852800	853800	It's hard.
853800	860080	I'm not sure I know how to convey this, what I have in mind here, because if you see a
860080	864200	lot of neural networks do amazing things, well, obviously neural networks is the thing
864200	865640	that works.
865640	872240	But I have witnessed personally what it's like to be in a world for many years where
872240	874760	the neural networks don't work at all.
874760	879520	And then to contrast that to where we are today, just the fact that they work and they
879520	881400	do these amazing things.
881400	885560	I think maybe the most surprising, the most surprising, if I had to pick one, it would
885560	889000	be the fact that when I speak to it, I feel understood.
889000	893640	Yeah, there's a really good saying from, I'm trying to remember, maybe it's Arthur Clark
893640	899800	or one of the sci-fi authors, which is effectively it says advanced technology is sometimes indistinguishable
899800	900800	for magic.
900800	903240	Yeah, I'm fully in this camp.
903240	906960	Yeah, it definitely feels like there's some magical moments with some of these models
906960	907960	now.
908000	915160	A way that you guys decide internally, given all of the different capabilities you could
915160	920160	pursue, how to continually choose the set of big projects, you've sort of described
920160	926480	that centralization and committing to certain research directions at scale is really important
926480	928120	to open AI success.
928120	931920	Given the breadth of opportunity now, what's the process for deciding what's worth working
931920	932920	on?
933160	938560	I mean, I think there is some combination of bottom up and top down, where we have some
938560	943320	top down ideas that we believe should work, but we're not 100% sure.
943320	948560	So we still, we need to have good top down ideas, and there is a lot of bottom up exploration
948560	950920	guided by those top down ideas as well.
950920	955880	And their combination is what informs us as to what to do next.
955880	961400	And if you think about those bottom, I mean, either direction, top down or bottom up ideas,
961400	967200	you like clearly we have this dominant continue to scale transformers direction.
967200	972400	Do you explore additional like architectural directions or is that just not relevant?
972400	976840	Certainly possible that various improvements can be found.
976840	980760	I think I think improvements can be found in all kinds of places, both small improvements
980760	982440	and large improvements.
982440	988480	I think the way to think about it is that while the current thing that's being done keeps
988560	994160	getting better as you keep on increasing the amount of compute and data that you put into
994160	995160	it.
995160	999760	So we have that property, the bigger you make it, the better it gets.
999760	1006480	It is also the property that different things get better by different amounts as you keep
1006480	1008680	on improving, as you keep on scaling them up.
1008680	1012440	So not only you want to, of course, scale up what we are doing, we also want to keep
1012440	1015400	scaling up the best thing possible.
1015400	1021040	What is a, I mean, you probably don't need to predict because you can see internally,
1021040	1025760	what do you think is improving most from a capability perspective in the current generation
1025760	1026960	of scale?
1026960	1036040	The best way for me to answer this question would be to point out the, to point to the
1036040	1038760	models that are publicly available.
1038760	1043040	And you can see how they compare from this year to last year.
1043040	1044520	And the difference is quite significant.
1044560	1048600	I'm not talking about the difference between, not only the difference between, let's say
1048600	1054040	you can look at the difference between GPT-3 and GPT-3.5 and then chat GPT, chat GPT-4,
1054040	1057880	chat GPT-4 with vision, and you can just see for yourself.
1057880	1062960	It's easy to forget where things used to be, but certainly the big way in which things
1062960	1069120	are changing is that these models become more and more reliable before they were very, they
1069120	1071960	were only very partly there.
1071960	1074720	Right now they are mostly there, but there are still gaps.
1074720	1078720	And in the future, perhaps these models will be there even more.
1078720	1082080	You could trust their answers, they'll be more reliable, they'll be able to do more
1082080	1085360	tasks in general across the board.
1085360	1089560	And then another thing that they will do is that they'll have deeper insight.
1089560	1095240	As we train them, they gain more and more insight into the true nature of the human
1095240	1098640	world and their insight will continue to deepen.
1098640	1103520	I was just going to ask about how that relates to sort of model scale over time, because
1103520	1109040	a lot of people are really stricken by the capabilities of the very large-scale models
1109040	1112240	and the emergent behavior in terms of understanding of the world.
1112240	1115840	And then in parallel, as people incorporate some of these things into products, which is
1115840	1119840	a very different type of path, they often start worrying about inference costs going
1119840	1122680	up with the scale of the model, and therefore they're looking for smaller models that are
1122680	1123680	fine-tuned.
1123680	1127360	But then, of course, you may lose some of the capabilities around some of the insights
1127360	1129680	and ability to reason.
1129680	1133720	And so I was curious in your thinking in terms of how all this evolves over the coming years.
1133720	1137120	I would actually point out that the main thing that's lost when you switch to the smaller
1137120	1139360	models is reliability.
1139360	1146280	I would argue that at this point, it is reliability that's the biggest bottleneck to these models
1146280	1147280	being truly useful.
1147280	1149360	How are you defining reliability?
1149360	1154960	So it's like when you ask a question that's not much harder than other questions that
1154960	1160560	the model succeeds at, then you have a very high degree of confidence that it will continue
1160560	1161560	to succeed.
1161560	1162560	So I'll give you an example.
1162560	1167800	Let's suppose that I want to learn about some historical thing, and I can ask, but tell
1167800	1172800	me what is the prevailing opinion about this and about that, and I can keep asking questions.
1172800	1176360	And let's suppose I answered 20 of my questions correctly.
1176360	1181400	I really don't want the 21st question to have a gross mistake.
1181400	1183240	That's what I mean by reliability.
1183240	1186600	Or let's suppose I upload some documents, some financial documents.
1186600	1187600	Suppose they say something.
1187600	1190920	I want you to do some analysis and to make some conclusion, and I want to take action
1190920	1193320	on this basis and this conclusion.
1193320	1198760	And it's not a super hard task, and these models clearly succeed on this task most of
1198760	1199760	the time.
1199760	1202520	But because they don't succeed all the time, and if it's a consequential decision, I actually
1202520	1207280	can't trust the model any of those times, and I have to verify the answer somehow.
1207280	1209120	So that's how I define reliability.
1209120	1210640	It's very similar to the cell driving situation.
1211040	1217040	If you have a cell driving car and it's like, does things mostly well, that's not good enough.
1217040	1221440	The situation is not as extreme as with a cell driving car, but that's what I mean by reliability.
1221440	1226360	My perception of reliability is that, to your point, it goes up with model scale, but also
1226360	1231760	it goes up if you fine tune for specific use cases or instances or data sets.
1231760	1238000	And so there is that trade-off in terms of size versus specialized fine tuning versus
1238000	1240080	reliability.
1240080	1245720	So certainly, people who care about some specific application have every incentive to get the
1245720	1249640	smallest model working well enough.
1249640	1250960	I think that's true.
1250960	1251960	It's undeniable.
1251960	1255400	I think anyone who cares about a specific application will want the smallest model for
1255400	1256400	it.
1256400	1257400	That's self-evident.
1257400	1262560	I do think, though, that as models continue to get larger and better, then they will unlock
1262560	1266480	new and unprecedentedly valuable applications.
1266480	1270000	So yeah, the small models will have their niche for the less interesting applications,
1270000	1271600	which are still very useful.
1271600	1275560	And then the bigger models will be delivering on applications.
1275560	1279040	Okay, let's pick an example.
1279040	1282160	Consider the task of producing good legal advice.
1282160	1285200	It's really valuable if you can really trust the answer.
1285200	1287800	Maybe you need a much bigger model for it, but it justifies the cost.
1287800	1295760	There's been a lot of investment this year at the 7B in particular, about 7B, 13B, 34B
1295760	1297760	sizes.
1297760	1300960	Do you think continued research at those scales is wasted?
1300960	1302960	No, of course not.
1302960	1312720	I mean, I think that in the kind of medium term by a high time scale anyway, there will
1312720	1319680	be an ecosystem, there will be different uses for different model sizes, there will be plenty
1319680	1325680	of people who are very excited for whom the best 7B model is good enough, they'll be very
1325680	1326680	happy with it.
1326720	1331360	And then there'll be plenty of very, very exciting and amazing applications for which
1331360	1333360	it won't be enough.
1333360	1334640	I think that's all.
1334640	1341200	I mean, I think the big models will be better than the small models, but not all applications
1341200	1344000	will justify the cost of a large model.
1344000	1348000	What do you think the role of open sources in this ecosystem?
1348000	1349400	Well, open source is complicated.
1349400	1352480	I'll describe to you my mental picture.
1352520	1357400	I think that in the near term, open source is just helping companies produce useful.
1359400	1364400	Like, let's see, why would one want to have an open source, but use an open source model
1364400	1368400	instead of a closed source model that's hosted by some other company?
1368400	1377560	I mean, I think it's very valid to want to be the final decider on the exact way in which
1377560	1379320	you want your model to be used.
1379320	1384120	And for you to make the decision of exactly how you want the model to be used and which
1384120	1386720	use case you wish to support.
1386720	1389000	And I think there's going to be a lot of demand for open source models.
1389000	1392080	And I think there will be quite a few companies that will use them.
1392080	1394760	And I'd imagine that will be the case in the near term.
1394760	1400800	I would say in the long run, I think the situation with open source models will become more complicated
1400800	1403680	and I'm not sure what the right answer is there.
1403680	1405400	Right now it's a little bit difficult to imagine.
1405400	1413040	So we need to put our future hat, maybe futurist hat, it's not too hard to get into a sci-fi
1413040	1416800	mood when you remember that we are talking to computers and they understand us.
1416800	1420120	But so far, these computers, these models actually not very competent.
1420120	1423440	They can't do tasks at all.
1423440	1430160	I do think that the will come a day where the level of capability of models will be
1430160	1431160	very high.
1431160	1435000	Like in the end of the day, intelligence is power, right?
1435000	1439560	Right now, these models, their main impact, I would say at least popular impact is primarily
1439560	1443120	around entertainment and like simple question and answer.
1443120	1445440	So you talk to a model, wow, this is so cool.
1445440	1449560	You produce some images, you had a conversation, maybe you had some questions, good answer.
1449560	1457080	But it's very different from completing some large and complicated task like, what about
1457080	1463960	if you had a model which could autonomously start and build a large tech company?
1464000	1469360	I think if these models were open source, they would have a difficult to predict consequence.
1469360	1471400	Like we are quite far from these models right now.
1471400	1476040	And by quite far, I mean by itime scale, but still like, this is not what you're talking
1476040	1481080	about, but the day will come when you have models which can do science autonomously,
1481080	1485600	like be delivered on big science projects.
1485600	1492400	And it becomes more complicated as to whether it is desirable that models of such power
1492400	1494520	should be open sourced.
1494520	1499200	I think the argument there is a lot less clear cut, a lot less straightforward compared
1499200	1504480	to the current level models, which are very useful and I think it's fantastic that the
1504480	1506000	current level models have been built.
1506000	1510320	So like that is maybe, maybe I answered a slightly bigger question rather than what
1510320	1513680	is the role of open source models, what's the deal with open source?
1513680	1519280	And the deal is up to a certain capability, it's great, but not difficult to imagine models
1519280	1523880	that are sufficiently powerful, which will be built where it becomes a lot less obvious
1523880	1527560	as to the benefits of their open source.
1527560	1532200	Is there a signal for you that we've reached that level or that we're approaching it?
1532200	1535120	Like what's the boundary?
1535120	1541760	So I think figuring out this boundary very well is an urgent research project.
1541760	1549920	I think one of the things that help is that the closed source models are more capable
1549920	1551000	than open source models.
1551000	1554520	So the closed source models could be studied and so on.
1554520	1558560	And so you'd have some experience with a generation of closed source model and then you know
1558560	1561480	like, oh, these models capabilities, it's fine, there's no big deal there.
1561480	1566280	Then in a couple years, the open source models catch up and maybe a day will come when we
1566280	1571000	going to say, well, like these closed source models, they're getting a little too drastic
1571000	1573480	and then some other approaches needed.
1573480	1581200	If we have our future hat on, maybe it looks like think about like a several year timeline.
1581200	1586160	What are the limits you see if any in the near term in scaling?
1586160	1592600	Is it like data, token scarcity, cost of compute, architectural issues?
1592600	1597520	So the most near term limit to scaling is obviously data.
1597520	1602640	This is well known and some research is required to address it.
1602640	1609280	Without going into the details, I'll just say that the data limit can be overcome and
1609280	1611200	progress will continue.
1611200	1615560	One question I've heard people debate a little bit is a degree to which the transformer based
1615560	1620800	models can be applied to sort of the full set of areas that you'd need for AGI.
1620800	1625000	And if you look at the human brain, for example, you do have reasonably specialized systems
1625000	1630200	or all neural networks, be it specialized systems for the visual cortex versus areas
1630200	1635000	of higher thought, areas for empathy or other sort of aspects of everything from personality
1635000	1636800	to processing.
1636800	1640560	Do you think that the transformer architectures are the main thing that will just keep going
1640560	1641560	and get us there?
1641560	1644080	Do you think we'll need other architectures over time?
1644080	1650360	So I understand precisely what you're saying and I have two answers to this question.
1650360	1655680	The first is that, in my opinion, the best way to think about the question of architecture
1655680	1659880	is not in terms of a binary, is it enough?
1659880	1668040	But how much effort, what will be the cost of using this particular architecture?
1668040	1673040	At this point, I don't think anyone doubts that the transformer architecture can do amazing
1673040	1679160	things, but maybe something else, maybe some modification could have some compute efficiency
1679160	1680160	benefits.
1680160	1683600	So it's better to think about it in terms of compute efficiency rather than in terms
1683600	1686040	of can it get there at all?
1686040	1689680	I think at this point, the answer is obviously yes.
1689680	1694040	To the question about, well, what about the human brain and its brain regions?
1694040	1701520	I actually think that the situation there is subtle and deceptive for the following reasons.
1701520	1706360	So what I believe you alluded to is the fact that the human brain has known regions.
1706360	1711240	It has a speech perception region, it has a speech production region, it has an image
1711240	1716520	region, it has a face region, it has all these regions, and it looks like it's specialized.
1716520	1719440	But you know what's interesting?
1719440	1724680	Sometimes there are cases where very young children have severe cases of epilepsy at
1724680	1730800	a young age, and the only way they figured out how to treat such children is by removing
1730800	1733320	half of their brain.
1733320	1740080	Because it happens at such a young age, these children grow up to be pretty functional adults,
1740080	1745560	and they have all the same brain regions, but they are somehow compressed onto one hemisphere.
1745560	1749920	So maybe some information processing efficiency is lost.
1749920	1753600	It's a very traumatic thing to experience, but somehow all these brain regions rearrange
1753600	1754600	themselves.
1754600	1760360	There is another experiment which was done maybe 30 or 40 years ago on ferrets.
1760360	1762120	So the ferret is a small animal.
1762120	1763360	It's a pretty mean experiment.
1763360	1769920	They took the optic nerve of the ferret, which comes from its eye, and attached it to its
1769920	1771320	auditory cortex.
1771320	1776640	So now the inputs from the eye starts to map to the speech processing area of the brain.
1776640	1781520	And then they recorded different neurons after it had a few days of learning to see, and
1781520	1785480	they found neurons in the auditory cortex, which were very similar to the visual cortex.
1785480	1791160	Or vice versa, it was either they mapped their eye to the ear, to the auditory cortex, or
1791160	1792400	the ear to the visual cortex.
1792400	1794680	But something like this has happened.
1794680	1800360	These are fairly well-known ideas in AI that the cortex of humans and animals are extremely
1800360	1801680	uniform.
1801680	1805320	And so that further supports the AI, like you just need one big, uniform architecture.
1805320	1806320	So all you need.
1806320	1807320	Yeah.
1807320	1810080	In general, it seems like every biological system is reasonably lazy in terms of taking
1810080	1812960	one system and then reproducing it and then reusing it in different ways.
1812960	1816520	And that's true of everything from DNA encoding, you know, there's 20 amino acids and protein
1816520	1817520	sequences.
1817520	1822240	Everything is made out of the same 20 amino acids on through to your point, sort of how
1822240	1823800	you think about tissue architecture.
1823800	1827040	So it's remarkable that that carries over into the digital world as well, depending
1827040	1828280	on the architecture you use.
1828280	1833200	I mean, the way I see it is that this is an indication from a technological point of
1833200	1834200	view.
1834200	1837640	We are very much on the right track because you have all these interesting analogies
1837640	1841560	between human intelligence and biological intelligence and artificial intelligence.
1841560	1847680	We've got artificial neurons, biological neurons, unified brain architecture for biological
1847680	1851600	intelligence, unified neural network architecture for artificial intelligence.
1851600	1856000	At what point do you think we should start thinking about these systems in digital life?
1856000	1857320	I can answer that question.
1857320	1864040	I think that will happen when those systems become reliable in such a way as to be very
1864040	1865040	autonomous.
1865040	1868120	Right now, those systems are clearly not autonomous.
1868120	1870720	They're inching there, but they're not.
1870720	1873680	And that makes them a lot less useful too, because you can't ask it, hey, like, do my
1873680	1876840	homework or do my taxes or you see what I mean.
1876840	1879120	So the usefulness is greatly limited.
1879120	1883960	As the usefulness increases, they will indeed become more like artificial life, which also
1883960	1889080	makes it more, I would argue, trepidations, right?
1889080	1893880	Like if you imagine actual artificial life with brains that are smarter than humans,
1893880	1897920	gosh, that seems pretty monumental.
1897920	1900480	Why is your definition based on autonomy?
1900480	1905160	Because if you often look at the definition of biological life, it has to do with reproductive
1905160	1906160	capability.
1906160	1908400	Plus, I guess some form of autonomy, right?
1908400	1912000	Like a virus isn't really necessarily considered alive much of the time, right?
1912000	1913920	But a bacteria is.
1913920	1918040	And you could imagine situations where you have symbiotic relationships or other things
1918040	1920840	where something can't really quite function autonomously, but it's still considered a
1920840	1921840	life form.
1921840	1925320	So I'm a little bit curious about autonomy being the definition versus some of these other
1925320	1926320	aspects.
1926320	1932120	Well, I mean, definitions are chosen for our convenience and it's a matter of debate.
1932120	1936640	In my opinion, technology already has the reproduction, the reproductive function, right?
1936640	1940560	And if you look at for example, I don't know if you've seen those images of the evolution
1940560	1944920	of cell phones and then smartphones over the past 25 years, you got this like, what almost
1944920	1948600	looks like an evolutionary tree or the evolution of cars over the past century.
1948600	1953320	So technology is already reproducing using the minds of people who copy ideas from previous
1953320	1955120	generation of technology.
1955120	1957560	So I claim that the reproduction is already there.
1957560	1960880	The autonomy piece I claim is not.
1960880	1963520	And indeed, I also agree that there is no autonomous reproduction.
1963520	1968280	But that would be like, can you imagine if you have like autonomously reproducing AIs?
1968280	1974720	I actually think that that is a pretty traumatic and I would say quite a scary thing if you
1974720	1978760	have an autonomously reproducing AI, if it's also very capable.
1978760	1980840	Should we talk about super alignment?
1980840	1982800	Yeah, very much so.
1982800	1985400	Can you just sort of define it?
1985400	1991360	And then we were talking about what the boundary is for when you feel we need to begin to worry
1991360	1995880	about these capabilities being in open source.
1995880	1999240	What is super alignment and why invest in it now?
1999240	2007040	The answer to your question really depends to where you think AI is headed.
2007040	2011200	If you just try to imagine and look into the future, which is of course a very difficult
2011200	2015320	thing to do, but let's try to do it anyway.
2015320	2018760	Where do we think things will be in five years or in 10 years?
2018760	2023240	I mean, progress has been really stunning over the past few years.
2023240	2025560	Maybe it will be a little bit slower.
2025560	2029800	But still, if you extrapolate this kind of progress, you'll be in a very, very different
2029800	2034360	place in five years, let alone 10 years.
2034360	2036720	It doesn't seem implausible.
2036720	2042920	It doesn't seem at all implausible that we will have computers, data centers that are
2042920	2045040	much smarter than people.
2045040	2049240	And by smarter, I don't mean just have more memory or have more knowledge, but I also
2049240	2056840	mean have deeper insight into the same subjects that we people are studying and looking into.
2056840	2060840	It means learn even faster than people.
2060840	2063800	What would such AIs do?
2063800	2065200	I don't know.
2065360	2070640	If such an AI were the basis of some artificial life, it would be, well, how do you even think
2070640	2071640	about it?
2071640	2076960	If you have some very powerful data center that's also alive in a sense, that's what
2076960	2077960	you're talking about.
2077960	2082600	And when I imagine this world, my reaction is, gosh, this is very unpredictable what's
2082600	2083600	going to happen.
2083600	2084600	Very unpredictable.
2084600	2089840	But the bare minimum, but there is a bare minimum which we can articulate.
2089840	2097560	But if such super, if such very, very intelligent, super intelligent data centers are being built
2097560	2104400	at all, we want those data centers to hold warm and positive feelings towards people,
2104400	2106600	towards humanity.
2106600	2111560	Because this is going to be non-human life in a sense.
2111560	2112560	Potentially.
2112560	2115040	It could potentially be that.
2115040	2121440	So I would want that any instance of such super intelligence, the warm feelings towards
2121440	2122440	humanity.
2122440	2125640	And so this is what we are doing with the super alignment project.
2125640	2131360	You're saying, hey, if you just allow yourself, if you just accept that progress that we've
2131360	2135320	seen, maybe it will be slower, but it will continue.
2135320	2144640	If you allow yourself that, then can you start doing productive work today to build the science
2144640	2153920	so that we will be able to handle the problem of controlling such future super intelligence.
2153920	2160720	Of imprinting onto them a strong desire to be nice and kind to people.
2160720	2165880	Because those data centers, right, they'll be, they'll be really quite powerful.
2165880	2169720	You know, there'll probably be many of them that will be, the world will be very complicated.
2169720	2175200	But somehow to the extent that they are autonomous, to the extent that they are agents, to the
2175200	2182640	extent they are beings, I want them to be pro-social, pro-human social.
2182640	2183640	That's the goal.
2183640	2187200	What do you think is the likelihood of that goal?
2187200	2193440	I mean, some of it, it feels like a outcome you can hopefully affect, right?
2193440	2199360	But are we, are we likely to have pro-social AIs that we are friends with individually
2199360	2202040	or, you know, as a species?
2202040	2208160	Well, I mean, friends be, I think that that part is not necessary.
2208160	2212040	The friendship piece, I think, is optional, but I do think that we want to have very pro-social
2212040	2213360	AI.
2213360	2215360	I think it's, I think it's possible.
2215360	2217640	I don't think it's guaranteed, but I think it's possible.
2217640	2222720	I think it's going to be possible and the possibility of that will increase insofar
2222800	2228320	as more and more people allow themselves to look into the future, into the five to ten
2228320	2236120	year future and just ask yourself, what, what do you expect AI to be able to do then?
2236120	2239320	How capable do you expect it to be then?
2239320	2247120	And I think that with each passing year, if indeed AI continues to improve and as people
2247120	2252700	get to experience, because right now we are talking, making arguments, but if you actually
2252700	2258820	get to experience, oh gosh, the AI from last year, which was really helpful this year puts
2258820	2264180	the previous one to shame and you go, OK, and then one year later and when it's starting
2264180	2270380	to do science, the AI software engineer is starting to get really quite good, let's say.
2270380	2278340	I think that will create a lot more desire in people for what you just described, for
2278340	2282380	the future superintelligence to indeed be very pro-social.
2282500	2285300	I think there's going to be a lot of disagreement, there's going to be a lot of political questions,
2285300	2292180	but I think that as people see AI actually getting better, as people experience it, the
2292180	2299820	desire for the pro-social superintelligence, the humanity loving superintelligence, as
2299820	2303580	much as it can be done, will increase.
2303580	2308460	And on the scientific problem, I think right now it's still being an area where not that
2308460	2311100	many people are working on.
2311100	2315300	Our AI is getting powerful enough, you can really start studying it productively, you'll
2315300	2318340	have some very exciting research to share soon.
2318340	2323500	But I would say that's the big picture situation here.
2323500	2327540	Just really, it really boils down to look at what you've experienced with AI up until
2327540	2334380	now, ask yourself, like, is it slowing down, will it slow down next year, like, we will
2334460	2338980	see, and we will experience it again and again, and I think it will keep, and what needs to
2338980	2340900	be done, it will keep becoming clearer.
2340900	2342820	Do you think we're just on an accelerated path?
2342820	2346900	Because I think fundamentally, if you look at certain technology waves, they tend to
2346900	2350420	inflect and then accelerate versus decelerate.
2350420	2354740	And so it really feels like we're in an acceleration phase right now versus the deceleration phase.
2354740	2355740	Yeah.
2355740	2360580	I mean, VR, right now it is indeed the case that VR in an acceleration phase.
2360580	2367260	You know, it's hard to say, you know, multiple forces will come into play.
2367260	2370540	Some forces are accelerating forces and some forces are decelerating.
2370540	2376140	So for example, the cost and scale are a decelerating force.
2376140	2381220	The fact that our data is finite is a decelerating force to some degree at least, I don't want
2381220	2382220	to overstate it.
2382220	2383780	Yeah, it's kind of within an asymptote, right?
2383780	2388020	Like at some point you hit it, but it's the standard S curve, right, or sigmoidal.
2389020	2392500	With the data in particular, I just think it won't be, it just won't be an issue because
2392500	2395340	we'll figure out something else.
2395340	2399860	But then you might argue that the size of the engineering project is a decelerating force,
2399860	2401540	just the complexity of management.
2401540	2405500	On the other hand, the amount of investment is an accelerating force, the amount of interest
2405500	2409220	from people, from engineers, scientists is an accelerating force.
2409220	2411620	And I think there is one other accelerating force.
2411620	2417180	And that is the fact that biological evolution has been able to figure it out and the fact
2417180	2424860	that up until now, progress in AI has had up until this point, this weird property that
2424860	2430220	it's kind of been, you know, it's been very hard to execute on, but in some sense it's
2430220	2436420	also been more straightforward than one would have expected perhaps.
2436420	2442140	Like in some sense, I don't know much physics, but my understanding is that if you want to
2442140	2448860	make progress in quantum physics or something, you need to be really intelligent and spend
2448860	2453380	many years in grad school studying how these things work.
2453380	2457260	Whereas with AI, if people come in, get up to speed quickly, start making contributions
2457260	2459340	quickly, it has the flavor is somehow different.
2459340	2465340	Somehow it's very, there is some kind of, there's a lot of give to this particular area
2465340	2466340	of research.
2466340	2467980	And I think this is also an accelerating force.
2467980	2470900	How will it all play out remains to be seen.
2470900	2475580	Like it may be that somehow the scale required of engineering complexity will start to make
2475580	2477820	it so that the rate of progress will start to slow down.
2477820	2480940	It will still continue, but maybe not as quick as we had before.
2480940	2485460	Or maybe the forces which are coming together to push it will be such that it will be as
2485460	2489700	fast for maybe a few more years before it will start to slow down.
2489700	2493060	If at all that's, that would be my articulation here.
2493060	2495260	Ilya, this has been a great conversation.
2495260	2496820	Thanks for joining us.
2496820	2497820	Thank you so much for the conversation.
2497820	2499820	I really enjoyed it.
2499820	2502660	Join us on Twitter at NoPriorsPod.
2502660	2505460	Subscribe to our YouTube channel if you want to see our faces.
2505460	2509100	Follow the show on Apple podcasts, Spotify, or wherever you listen.
2509100	2511180	That way you get a new episode every week.
2511180	2515540	And sign up for emails or find transcripts for every episode at no-priors.com.
