Today we're going to talk about PageRank and a lot you're going to see a lot of stuff on
PageRank.
This was a key invention that led to the founding of Google in the early 90s and I'm going to
give you some mathematical foundations.
I'm also going to talk about extensions like what's called Topic Specific PageRank
or SIPRANKs for finding similar pages, TrustRank and I will explain how web spamming works,
how that search engine optimized companies can try to increase the rank so there's a
huge gap between search engines and these guys.
Let me first start.
Let me give several definitions of PageRank.
I'm going to start with Fluid.
So there's going to be in this course both intuition and mathematics.
A lot of intuition, a lot of mathematics too.
So I'll give multiple ways of Fluid algorithm, random walk and matrix representation all for
the same algorithm.
And then I'll give you some intuition on matrices and eigenvectors, eigenvalues, which maybe
you see hopefully you get good intuition here.
So in a PageRank there's only one value for each web page.
So it's a big directed graph on the web pages and each page has a value that is computed
and the intuition is a page that is important, is pointed to by important pages and an important
page points to important pages.
So that's recursive.
That means there's a recursive iterative algorithm.
First intuition is that it's kind of like a liquid, fluid.
Each web page or node is a container and the links are hives and you start by putting
a little bit of fluid in each of the web pages and then the fluid will circulate according
to some rules.
So if you have one liter, then one of our n liters is put in every page and then you
repeat.
So there's a link from P to P prime.
So each page P has links going out and if there's n links going out, maybe there's only
three links going out, each page, each link will get one third of the fluid.
The fluid is divided between that of links and then the next step, you look at the links
coming in and for each incoming link you take the fluid.
So you see the fluid circulates like that and eventually it converges.
So here's an example of web pages.
So there's eight pages here.
A, B, C, D, E, F, G, H.
There's each page has one eighth.
So now the fluid circulates.
How does it work?
Well look at page A.
So page A has D going in, E going in, H going in, F going in, and G going in.
And so what happens is after one iteration, A has one eighth of a liter which leaves and
all the n links, the fluid comes in.
So D has one eighth of a liter but has two outgoing links.
So it's divided by two.
So you have one 16 coming from D.
E has also two outgoing links.
So there's one 16 coming from E.
H has no outgoing links.
So everything goes there.
So this is one eighth.
That's H.
F has no outgoing links.
And G neither.
So there's both one eighth there.
That's F and G.
So the total is three eighths, four eighths.
This is one half.
That's after one iteration.
It's the same for all of them.
And you keep going until it converges.
And you can prove it converges, which we will do in a little bit.
And after you keep doing that, you get these numbers.
So A has four thirteenths, B has two thirteenths, D has one thirteenth, and so on.
So this is the result of the page rank.
It's just this fixed point.
A is the most important page here.
And D, H, E, F, and G are less important.
That's the basic idea.
Of course there's a problem.
It's not going to work if you do like that.
So this basic algorithm is nice, but it doesn't always work.
So we have to fix it.
So let's say we change the graph.
F points to G and G points to F.
They don't point to A, they point to each other.
So what happens now?
The thing keeps circulating.
But look, all the fluid that comes in here never goes out.
You see that?
It just will accumulate here.
All the fluid will eventually, this part will empty, zero.
Everything will eventually come here.
F and G will each get half.
F and G has to get zero.
That's not good enough.
So you think, well, this is a stupid algorithm.
It's very bad.
Well, the insight from the smart guys who made Google is,
we can actually fix this problem.
So you see the problem now.
We can still use the fluid intuition to fix it.
Look at the Earth.
Water flows down, right?
First, everything goes into the ocean.
But why does not everything stay in the ocean?
Still, there's water on the mountains.
Because there's water also going up.
This is called up by evaporation.
So it's kind of circulating.
So you have lakes in the mountains.
And that's what they do now for the pay track algorithm.
You can, the update rule.
So it's not just what fluid goes in.
There's also fluid going out.
So you take the fluid, OK?
So you make a scale at the end of the day.
You reduce the fluid in each page by some factor s,
like 0.9 or something.
And you add a little bit of fluid, 1 minus s over n, everywhere.
So even the pages very low, very low,
pay track will get some.
So it's like a random jump, OK?
So basically the pay track evaporates from each node.
And it rains uniformly on all nodes.
So that means we have now a cycle between all nodes.
And this fixes the problem, OK?
So you have the basic update rule extended with scaling.
So this is the evaporation.
You can show it with converges, which we will show in a while.
It's matrix computation, which we'll show later.
And the real scaling factor is some number very close to 1, 0.8, 0.9.
So that means most of the time the fluid stays on the page,
like a small percentage of the time it jumps when it evaporates, OK?
And this definition, the fluid definition, is equivalent.
You can say it's also kind of, you can formulate it as a random walk definition.
This is maybe more intuitive for humans.
Let's say you have somebody who is browsing the web randomly.
I'm looking at a web page, and then I click on the web page.
So I jump from one page to the other.
And I keep clicking, and I'm surfing for K steps.
And then I get bored, and I don't get another page.
So it's like I'm walking randomly on the network, OK?
And you walk K steps.
And then you jump to some random page, you walk K steps.
Then you walk, jump, and you walk K steps.
So this is the walking is exactly the page after K iterations.
And the jump is the rain with the jumping to random page with probability 1 minus s, OK?
So it turns out they're equivalent, which we'll see in a minute.
OK?
So this is the basic algorithm.
It was modified a lot, OK?
So I won't say so much here, because we're going to go into the other slides.
Search companies will cheat.
They don't call it cheating.
They call it search engine optimization, please, SEO.
And if I want to be very high at the Google ranking, I pay SEO companies the money.
And they will do some magic, and all of a sudden I get very high scores.
And it works using something called link farms, which we'll explain.
OK?
And also something we mentioned before is advertising.
So it's the links.
You also have pay links.
And Google makes huge money with that.
OK?
So that's an extension.
OK?
So let me now go to the more in-detailed discussion of PageRank.
So these slides are actually by a colleague of mine called Sarunas Gerzioskas,
who is, I think, from Lithuania, but he's now in Sweden, KTH.
So he was very nice to get to these slides, which I modified a little.
So this goes into a lot of detail.
OK?
On different kind of things for PageRank.
So first I want to give you intuition.
We will go a little bit into the math, matrix representation for random walks,
the notion of eigenvalues.
Then I will show the PageRank algorithm, how it works exactly, formally.
And then we go beyond PageRank.
How can you search in a more focused way, topic specific,
or how you can find similar pages, using variation,
that say I'm looking for movies that are like this movie.
OK?
So, for example, I like a lot the movie Doom,
and I want to find movies that are like this movie.
So with SimRank lets me do that.
So it's similar.
And the final thing is that I want to talk, I want to explain how the cheaters work.
This is called web spamming.
So very quickly after Google invented the PageRank,
the companies invented the ways to cheat.
OK?
And this is called link farming.
So I'll show you exactly how it works.
And we'll figure out how can you increase the PageRank artificially.
And then I'll show you some algorithm called TrustRank to go bypass the cheaters.
OK?
So the first part of the talk is more general talk about random walks and matrices.
So this gives you kind of intuition on the mathematics.
So I'm going to give you mathematics, but also intuition.
So the first thing I want to talk about is random walks.
So here's a graph with vertices, 1, 2, 3, 4, 5, and edges connected to them.
And I want to do a random walk.
That means I start somewhere.
I've got node 1.
I move to a neighboring node with some probability.
1 over d of v.
So d is the degree of the vertex.
It's how many links there are.
So the node 1 has a degree of 2.
So there's one half probability of going on each of those links.
And then I keep making walk steps.
And the sequence of this technically is something called a Markov chain.
I'm not going to give you so much theory, but it's basically a sequence where each step is chosen according to some probability.
And there's no memory.
I mean, it doesn't remember where it comes from.
You just each step, it's there.
It makes another jump, but it actually has no memory where it came from.
So that's called Markov chain.
Okay?
So let's say we start at node 1.
So here I make a table.
So node 1 here.
So in the beginning at time 0, I have one probability of being at node 1 and 0 for being everywhere else.
And then I do one step, but a two-step, three-step.
And each step, I will walk randomly according to the links.
Okay?
So after one step, see how it works out, I will be either at node 2 or 3 with probability 50%.
Okay?
And then I keep going.
So the 2 can go either to 1 or 4, and the 3 can go to 1, 4, or 5.
So the 2, the 0.5, will be split between 1 and 4.
And the node 3, the 50%, will be split between 1, 4, and 5.
So 16% or 17% for each of them.
So that gives me this.
That means there's 42% chance of being at node 1.
So where does this number come from?
Well, it means I have a 25% chance coming from node 2 and a 17% chance of coming from node 3.
And 25 plus 17 is 42.
That's very quick to pick up.
Also being in node 4, I have a 42%.
And node 5 is only one way to get there.
It's from 3 because I'm not getting 4.
So there's a 1 over 6, so 17%, rounded into 2 decimals.
And the sum of all these is normally going to be 1.
Might be slightly different because of round off error, but it's supposed to be 1.
Okay?
So I keep going.
You can see now the numbers change.
After 3 steps, I will be in these places with these probabilities.
And I keep going and I can keep going.
4, 5, 6.
You see, eventually, it's like they're kind of converging, okay?
To some value.
Okay?
And it will converge.
So the initial node, I started with node 1, but of course you can start with more nodes.
So here's a longer table.
So you start from 0 up to the top and you keep going.
And it kind of looks like it's converging, right?
17, 17, 25, 25, 17.
How would you think?
It seems to be, it smells like it's converging over.
And it does actually converge.
And you can prove for any connected, non-by-part, bi-directional graph at any starting point,
the random one converges.
And now I want you to have intuition on this.
The graph has to be connected, of course.
If it's not connected, if it looks like this.
So let's say the graph looks like this.
There's two pieces.
Well, if I start here, I will never get there, okay?
For example.
Or if I have a bi-part type graph.
See, depending on if I start here or if I start here, I will converge differently.
So it's not really converging for any random starting point.
See, here I say any starting point, okay?
So if the graph is not connected, then the convergence would be different depending on the starting point, right?
So it has to be connected.
So you have to understand, intuition is important.
It has to be connected.
It has to be non-by-part type.
Because if it's bi-part type, then you can divide the graph into two pieces.
If I start here, then I will go here and it will be oscillating forever.
All the probability will be here and this will be zero.
And then it will all be here and all this will be zero.
So it's oscillating and not converging.
So that's why it has to be non-by-part type.
See that?
So you have to understand this intuition, huh?
So it's not just random words there, okay?
So make sure you understand.
Okay?
And the unique is only one result.
Unique stationary means that once you converge it never changes anymore, okay?
So this is a kind of iteration, which we call power iteration.
Okay?
So this is the final value.
17, 17, 25, 25, 17.
What are those numbers mean, actually?
Actually, we can figure it out what they have to mean.
We can actually give a formula for this.
The random walk.
So pi is the final set of probabilities.
And it's stationary, so it's staying the same.
D of V is the degree and M is the number of edges, okay, in the graph.
So in this graph, 1, 2, 3, 4, 5, 6, edges.
So 2M is 12.
So 3 is the degree 3.
1, 2, 3.
And 3 divided by 12 is 1, 4, 25%.
That's the coincidence, huh?
So 2 has two outcome edges.
2 divided by 12 is 1, 6, which is 0.17.
Amazing, huh?
Isn't it?
So we can prove that this has to be the right value.
It's actually very easy to prove, okay?
We can prove that this value is a fixed point.
That means if you do one more iteration on this value, you get the same value.
We start with pi, and we will do one iteration giving pi prime.
Okay?
And so how do you do that?
Well, the new value is all the in-going links, okay?
So for node V, we take all the links going from U to V.
And from each one of those, we have the probability on U divided by the degree.
1 over D, okay?
This one is du over 2m, and the du's cancel, so we get the sum of 1 over 2m, the links
coming from U to V.
But how many links are there going from U to V?
Well, it's exactly the degree of V.
How many links are going to this node?
1, 2, 3, okay?
That means it's the degree of V divided by 2m.
Pi prime is equal to pi.
So you could show that it's the degree divided by 2m, right?
We need that.
Now, there's another thing.
If the graph is deregular.
So deregular means that all the nodes have the same degree, D, everywhere.
If it's like that, of course, then the probability is going to be the same everywhere, right?
So uniform means it's the same.
So if I have three links on every node, it's going to be the same, uniform.
Okay, that's intuitive, right?
So you have to get the intuition, okay?
So this is just some numbers showing how it works.
So the stationary distribution is proportional to the degree of V.
The degree is how many links, okay?
So what does that mean?
There's intuition.
Well, it's pretty clear.
The more neighbors you have, the more chance that people are going to come visit you.
So the probability is higher.
See, that's kind of the intuition, okay?
So you can see how the matrix, the random walk is going to work.
Now, let's look at a more numerical form of representation, a matrix representation.
We're going to do the same thing now with matrices.
So here's my graph.
So I'm going to keep using the same example.
And that matrix called A is the adjacency matrix for this graph.
What does it mean?
Well, if you look at the rows, there's five rows and five columns.
So if there's a link from one to two, that means there's a one in element one, two.
So the first row, second column.
So let me show you.
So this link, one, two, means that there's from row one, there's a link to two here.
There's also a link from one to three.
So row one, column three, there's a link here.
And each row corresponds to one node, the origin, and the column corresponds to the destination, okay?
So how many ones are there in that graph?
In this matrix, how many ones?
You have one, two, three, four, five, six, six edges, huh?
So how many ones are going to be in there?
Two times.
Because, of course, they go both directions.
That's one to two, two to one.
So you have twelve.
They're out.
See, it's very two to go.
So that's the first matrix called adjacency matrix.
But now we want another matrix because we're going with probabilities.
We have a probability.
So we want that to be in the matrix too.
So first I want to do my special matrix called the diagonal matrix D.
And each value on the diagonal is one over one over the degree.
So for the first element, it's one over two, then one over two.
Note three has three legs, so it's one over three.
Note four has three legs, so it's one over three.
And note five has two legs, so it's one over two.
So why am I introducing this matrix?
Because it lets me define another matrix, which is called the transition matrix.
So this one is actually D times A.
So what does it mean?
Instead of ones, I now have probabilities.
So if I have a row one, if I have a node one, it is one half probability I go to two,
and one half probability I go to three.
See that?
So in each row, all the elements tell me the probability they go to the other one.
Question?
Yes.
In PageRank, we have a directed graph, correct?
Yes, we will get to that.
So yes, I'm talking now, I'm directing graph, but we will get to directed graph,
and you'll see what happens when we get there.
It's very interesting stuff, but you have to understand first this piece, this one, huh?
Yeah, yeah.
No problem, no problem.
We get there, and you will see all the neat intuition that comes when the graph becomes directed.
So here, I'm just giving you intuitions, okay?
Because eventually, we're going to be talking to PageRank, and then I want you to understand that stuff, okay?
So if you're in row two, you can go to row one, with probability half and a four with probability half.
If you're in two, you go to one or four.
See how it works, huh?
So all the ones in a row are turned into probabilities.
So the sum of the probabilities for each row is equal to one, huh?
And this matrix is actually D times A.
So there's two matrices that are really important here.
The adjacency, which gives you the graph, and the transition matrix, which gives you the probabilities, okay?
So formally, A is n by n adjacency matrix of the graph.
A, i, j is one if there's a link between nodes i and j, and otherwise it's zero, okay?
Actually, you can say it gives me all the paths that you can do in one half, huh?
Because you're making one connection, okay?
The adjacency matrix.
Now let's go do a little bit more intuition, and I'll show you and you'll see how neat the matrix representation is.
Let's say I want to count two hop paths.
I want to go one, two.
How many are there?
From a node, I want to go two hops instead of one.
You know it's so neat.
It's just A squared when you do matrix multiplication.
The A times A, the matrix multiplication, gives me the number of two hop paths, okay?
And A cubed, what does A cubed mean less?
Well, the number of three hop paths, okay?
So how does that work?
Well, I'll give you some intuition.
Okay, a small technical thing.
It's actually giving you walks.
Sorry, I have to be technical about graphs.
A path is when you don't repeat any vertices, so it's like the marking on the street.
Whereas a walk is like a person walking back and forth, so you can actually repeat vertices, okay?
So actually, we're counting walks, okay, to be very technical.
We're counting walks, okay?
So how does it work?
Actually, you have to make a diagram and a fingerprint and understand why the matrix multiplication gives you that.
Okay?
So let's say I want to do A times A.
Okay, A times A.
And here I have a row, row I, and here I have a column.
Okay?
Okay?
So, and here I have 1's and 0's up.
For example, some way, and this will give me element i, j, a, l.
So how does it work?
So these are, the one means you have one hoppa from i to 1 to center hoppa.
So how does the multiplication work?
1 times 0 plus 0 times 1 plus 1 times 0 plus 1 times 1.
Well, how does it, what does it mean in terms of hops?
Here I'm going from i to 1.
And here I'm going from 1 to j.
You see that?
So I go from i to 1 and then from 1 to j.
Then I go from i to 2 and from 2 to j.
From i to n and to j.
So each one of these combinations is going from i to j, but the node in the middle is different.
You see that?
And it's only going to be 1 if there's both 1's.
That means if I have a path from i to 2 and 2 to j, it's going to multiply 1 times 1.
So it's only 1 if the number, if there's actually a path there or a walkup.
So if you sum them all, it gives me all the possible walks from i to j with 2 hops.
And if here, and it keeps going like that, if you put the matrix here which gives you n hops,
then you hear you will add one more and you get n plus 1.
Do you see how it works?
So the matrix multiplication gives you the total number of walks.
Okay?
So what if you take the vector now, 1, 0, 0, 0, 0 and you multiply by a?
Well, it's like you start, okay?
You're starting from 1, or 0, or 0, or 0, or 0.
So the number of walks, the one where you start from is 1, 0, 0, 0, 0.
It's given by that.
So basically it tells you how many walks of length 1 that start from node 1, ending in node i.
Okay?
And you could do multiplication for that.
See how matrix multiplication is really neat, huh?
So v a squared, it gives you 2, 0, 0, 2, 1.
So now we have 2 walks going from node 1 ending up in node 1, or 2 walks from node 1 ending up in node 4.
Okay?
See what's neat, huh?
And 3 gives you how many walks of length?
3.
And it's all the same, I'm always talking about this one, huh?
And you see how the matrix really corresponds very nicely to graphs and doing things on graphs, okay?
Okay?
So far we did the adjacency matrix.
What about the random walk matrix, the transition matrix?
That's this one.
And with the probabilities, okay?
So this gives you the probability of ending up somewhere.
Okay?
If you start in node 1, the probability of ending in here is one-half.
The probability of ending in node 4 is zero.
And you could do squares.
So for example, if you keep going,
if you take this vector here and multiply again by m,
the number you get here is actually the probability that you end up in those nodes when you start with node 1.
It's neat, huh?
Question? No? It's okay?
So you can see how matrices and graphs are very close.
So this is going to help us understand page rank, yeah?
Okay?
So we have a random walk on a graph G.
Now we start doing random walks on that page rank.
We start at node v0, and after t steps, we end up in node vt.
We move to a neighbor with probability one over the degree.
Each time we go to a node, if there's multiple paths, we divide evenly, yeah?
And the chain of random nodes, it's what's called a Markov chain.
So a Markov chain is a sequence where whenever you make one step,
you make kind of random choice, some probabilities,
but you forget everything you did before.
It only depends where you are now, so there's no memory, okay?
So the initial state would be here, so you have probability one being node 1, okay?
Now you can do multiple steps.
So after t steps, the probabilities is given by this vector, pt, p0 times m power t.
So this is matrix exponential, matrix multiplications, huh?
See how it works, huh?
If you're a stickler for doing the m on the left, you have to do transpose, of course.
That means you have row mixes here, but that's a detail, okay?
So here we have pt is p0 times m to the t, and we start again with our matrix,
and we start with the 1, 0, 0, 0, 0, and we do one step.
So we have probabilities a half and a half for getting there, okay?
And then keep it doing again, and we keep going.
And eventually, I mean, it's going to converge.
Remember, I showed you the table, but we're doing the same thing here, huh?
So when pt plus 1 is equal to pt, we have reached a stationary distribution.
So let me call this vector pi, okay?
It's not 3.4, and I have not chosen this notation,
but this is what Sylvonez has chosen.
So pi times m is equal to pi.
Stationary, when we start with these probabilities, we make a step,
we end up with the same probabilities, okay?
See that?
You remember the notion of eigenvectors, have you seen that?
Well, this is an eigenvector, either young.
Remember, every matrices have special vectors called eigenvectors
that give some property in the matrix.
So v is an eigenvector of m, and lambda is the eigenvalue
if v times m is equal to lambda, which is a scalar number times v.
It's just scaling the v.
So this one is clearly an eigenvector, right?
Interesting, huh?
So pt is connected to eigenvectors.
Pi is an eigenvector of m with eigenvalue lambda equal to 1, okay?
Okay, so that's the first part.
Let me go on now.
Let me go on now by giving you some more intuition
on eigenvectors and eigenvalues in terms of graphs.
So eigenvectors and eigenvalues are very useful things,
but with graphs they have some really neat properties.
So remember this, okay?
This is what we just saw.
We have pi as an eigenvector with eigenvalue 1.
But we can actually say more things, okay?
There's something called the spectrum of a matrix.
I'm not sure if you saw that.
Have you seen that?
Have you seen that spectrum?
Yes? Good.
That means this will be very easy, right?
No?
So assume now, now I'm going to give you some intuition.
Assume you have a D regular graph.
So that's a graph where all the nodes have a degree of D.
So there are all three things coming out, for example.
Okay?
And now we have a, which is the adjacency matrix for this.
So a times x, okay?
So this is just a matrix multiplication of a times vector x
equal vector y, okay?
And an eigenvector, this is refresher.
An eigenvector is a vector x such that this is equal to some number times that, okay?
So what happens with degree D?
Now we're going to connect the eigenvalues, eigenvectors with graphs, okay?
Graphs, which are these visual topological things, like on the web.
And matrices, which is just a bunch of numbers.
How does it connect?
Suppose all the nodes have degree D and G is connected.
So it's a D regular graph, huh?
Okay.
What are some eigenvalues?
Well, let's try.
Let's say we try to vector y, y, y, y only once, this graph, okay?
Then a times x will be D, D into all the D's.
Because each node has three outgoing things, but also three incoming things.
So the number of ways of going into a node is D.
So that means D is an eigenvalue.
So if you have a graph with only degree D, well D is an eigenvalue.
So eigenvalue is somehow connected with the degree of the graph.
It's funny, huh?
Interesting, huh?
Okay.
So the vector of all the nodes has an eigenvalue of D, okay?
This is actually what's called the principal eigenvector.
But let's look at some other graphs.
So the D regular graph is a very simple uniform graph.
Let's look at some other ones.
Very interesting, okay?
In general.
Now, there's a concept called spectrograph of a graph.
So if we have now a matrix A, a JCC represents a graph.
V times A is equal to lambda times V.
Well, if it's a column vector, you can say A V is lambda V.
Here we have row vectors.
So if A is a real symmetric matrix, so for non-directed graphs it's symmetric,
real numbers, then it has n eigenvectors and n eigenvalues.
And all the eigenvalues are real numbers.
And you could order them.
Lambda 1 and lambda 2 up to lambda n.
That's something you can show.
And the eigenvectors are orthogonal.
So they're orthogonal to each other.
And if G is a D regular graph, then lambda 1 is equal to D.
Somehow, degree of the graph is connected to the eigenvalue.
Okay?
For a random walk matrix, which is not the degree of D,
which is a random walk matrix, you have normalized.
This is the one where you have probabilities.
The principal eigenvalue is 1.
Here you have probabilities.
It's not that JCC, yeah?
It's the probabilities that eigenvalue is 1.
Okay?
This I will skip.
There is another concept called graph laplation,
but I would not use it.
But Saruna wants to tell you about it,
but I'm not going to tell you about it.
And the set of all the eigenvalues is called the spectra of the graph.
Okay?
And this could be for the A, JCC,
or the transition probability matrix M,
or the L matrix, which I'm not talking about.
And there's a very interesting number called
the difference between lambda 1 and lambda 2.
So lambda 1 is the principal eigenvalue,
but lambda 2 is also interesting.
And there's a number where lambda 1 minus lambda 2,
which has a name, which is called the eigengap,
or the spectral gap.
Okay?
And for N, it's 1 minus lambda 2,
because lambda 1 is equal to 1.
Okay?
Fine.
What if the graph is disconnected?
Okay, now we're going to show some examples
to make more pure intuition,
and that will all help for later
when we start thinking of the paint track.
Okay?
What happens if the graph is disconnected?
Well, lambda 1 is equal to lambda 2.
That's weird enough.
If the graph is disconnected,
you could actually say something about these two highest eigenvalues.
Okay?
So let's see how it works.
So now I'm going to give you some intuition.
So let's say we have a graph that's not connected.
Two components.
Each one is deregular, just for simplicity.
So each one has the same number of names.
So we can have two little triangles.
So what are some eigenvalues for this?
Well, for example,
if you put 1's on the A,
and 0's on the B,
then you get a vector, right?
1, 1, 1, 0, 0, 0.
Or you can put 1's on the B,
and 0's on the A, right?
So if you put 1's on the A,
and 0's on the B,
and you multiply by A,
A times x prime here,
you add up all the degrees.
So you get d, d, d, d with 0's.
That means d is an eigenvalue.
But you also have another vector,
orthogonal, 0's and 1's on the B,
and it also gives d, d, d, d, d, d.
So you have two separate eigenvalues
that are the same.
Okay?
Notice if the degree would be different for me,
we would have different values.
We would have d1 and d2.
We would have d for both.
So they're the same.
And you can kind of do approximation now.
So we saw that if you have this,
the eigenvalues are the same.
You have two eigenvalues that are the same.
Now assume that it's almost,
almost disconnected.
We only have a small number of links here.
Well, it turns out,
lambda 1 and lambda 2 will be very close to each other.
It will be approximately 0.
So you can see that the connectedness
are somehow related to the eigenvalues.
Okay?
Okay.
For every connected,
normal bipartite, undirected graph,
the distribution converges to a limit.
Okay?
A unique stationary distribution, pi,
and it's regular, it's uniform.
Okay?
Why connected?
I mentioned already, yeah?
The graph has to be connected.
What happens if it's not connected?
What happens if it's not connected?
You see, I explained it already.
In case that you don't have one,
you need stationary distribution then.
You can have one or another one.
Okay?
What if it's bipartite?
Then you will have all the probabilities on one side,
so you can divide the graph into two sides.
All the probabilities on one side are not 0.
The other side is 0s,
and if you do one step, it all shifts to the other side,
and you have oscillation.
There's no convergence.
And what about directed graphs?
And now we've got to start going to directed graphs.
So now your body needs to be tuition.
What about directed graphs?
Okay?
So here's a directed graph.
It turns out the directed graph
has to be strongly connected.
Remember, we said what it means, huh?
That means there is a directed path
and it knows another.
What happens if it's not strongly connected?
Oh, you have to have intuition there.
So an example of that in the beginning.
If it's not strongly connected,
then they say walks will leak.
It means that if it's not strongly connected,
what happens?
Okay?
There's one thing.
Here's another one.
Notice this graph here.
So this one on the left is strongly connected.
So we're good.
The right one is also strongly connected.
No, this one is not strongly connected.
But this one has another problem.
It's not...
You have actually two cycles going like this.
Okay?
And this is a property called periodicity.
Huh?
A, the graph has to be A periodic.
So this is kind of generalizing by part-time.
Done.
Visits to some of should never be a multiple of some number.
So here, visits will be multiple of three steps.
Always.
Well, if you do that, the whole thing's going to oscillate.
Done.
Okay?
So if you do that, the whole thing will oscillate.
And so it's not going to be convergent.
So it has to be A periodic.
If you don't, you're not allowed to have that.
Okay?
So the greatest common divisor of the lengths of all the cycles is one.
So you can see some intuition here about the graph.
Okay.
So all of that was kind of preliminary stuff to get your neurons moving.
And now that we can start talking about page rank.
Okay?
So now I'm going to talk about mobile page rank.
So first, I'll give you a little history of what search.
So this all started a long time ago in prehistory.
Another millennium.
1989, okay, which is a long time ago.
It's really super long ago.
It's like, before the prince was born or something.
So here you have the first generation of search.
So you have people who are making web pages.
And people want to search them.
The first generation was manual curation.
So Yahoo!, which started back in those days.
There was little gnomes or monks sat in an office looking at all the web pages and kind of making directories.
And I went science.
I click on science.
It's trun.
Okay?
Yeah, but that's kind of not scalable.
Okay?
So then now we have second generation.
And there was a very nice one called Alfa Vista, which you have probably never used, which I used.
And this was classical information retrieval.
So basically they look at the text in the pages.
If there's lots of words on astronomy, then it's based on, then it's astronomy.
Okay?
And then the indices.
But this didn't work very well.
And it was, and it was spammed.
People did something called term spam.
It was very easy to cheat from this.
And then the third generation came.
And it basically took over.
This was Google Page Drive.
Okay?
So this is the short history of web search.
Okay?
So what I'm going to do now is I'm going to explain Google Page Drive.
And we're going to use all our intuition on graphs and matrices and all that stuff
to really see how to design it and not to make it work.
Okay?
But as noticed, it's three o'clock, so we're going to make a break first.
Okay?
Design Page Drive.
And we're going to use some of the intuition we saw before the break.
Okay?
So we have a bunch of pages.
And here we're talking about votes.
So it's like the fluid that you have above.
And you distribute the votes to outgoing links.
Okay?
So if a page J has some quotes, rj, and n outlink, each link gets rj over n.
Okay?
So that's here.
And so pages will vote.
So the idea is that the more votes you get, the more important you are.
It's like in an election.
And if your page is important, then you have more votes, rj is more.
So a vote from an important page is more.
So that's like what we saw before.
Okay?
And the importance of a page is the sum of the votes, the inlinks.
So the other pages then vote for it.
Okay?
So this is something we saw, kind of.
Okay?
This is actually very similar to random one.
So we can actually say that a Google Page Drive is the principal driving vector of the transition matrix of the web graph.
Okay?
So you have the web graph, which is huge, billions of links.
So the matrices are very big.
Okay?
So we're going to compute this.
We're going to compute the eigenvector of a matrix, which is 10 billion by 10 billion on the side.
That makes how many elements inside that matrix?
Well, 10 to the 20th elements in there.
So that's a lot, right?
So can you see any issues with that?
Well, I mean, not so many computers can store 10 to the 20.
Okay?
That's one issue.
But there's more issues.
So we're trying to make it practical now, aren't we?
So one issue is the matrix is huge.
The other issue is, well, remember this diagram?
This is the structure of the web.
Well, in order for there to be a unique stationary distribution, it has to be a strongly connected component.
So is this thing a strongly connected component?
No.
I mean, there's one inside, but it's not.
So that's a problem, okay?
That's another problem.
It's not strongly connected.
So it's a directional graph.
So here, for example, so there's two problems.
So the Google people have some very nice terminology for this.
So here we have a node which only has inlinks and no outlinks.
So this one is called a dead end.
We set dead end, and we saw it already before the break.
So you can say the votes leak out.
Everything collects there, and there's no, they're not circulating anymore.
That's one problem.
Or another issue.
So here's another graph.
So this graph has a problem too.
Can you see what the problem is here?
So here you have three nodes.
So the Google people call this a spider trap.
Something that traps spiders.
I'm not sure why they call it a spider trap, but it's called a spider trap.
That's another problem.
So the votes disappear.
They all collect there.
They're no longer circulating.
And of course it's not strongly connected.
So again, these are all problems we have to fix.
Okay?
So how do we fix it?
Well, we have to fix the graph.
We have to make it strongly connected and empiric.
Those were the two positions.
Strongly connected and empiric.
The solution of Google.
Okay?
This is very similar to the evaporation idea.
If you make very, very small leaks from every node to every other node.
So this is the evaporation.
It means there's a small probability of jumping from any node to any other node.
But the main graph is still the same.
So here's your main graph.
Then you have another graph with very tiny, tiny probabilities.
But between every node and every node is very tiny.
And you basically sum them together.
Okay?
So it's got very nice innovation and it turns into that.
Okay?
So the idea is that you have a random walk.
And when you're at a node, you will either follow the link or you will jump.
So it's basically the evaporation idea.
Some probability here, if you call it beta, you follow the real link.
And with one minus beta, you just jump to some random page.
Okay?
And usually it's very close to one.
Now, that means close to the time, you keep following links.
But at one step to five or ten links, you jump.
So it's like you walk for five steps and you jump.
So if you're in a spider trap going in a circle,
after five to ten steps, you will jump out from the spider trap.
Okay?
So if you go to a dead end, once you're in the dead end, there's no way of getting out.
So you will immediately jump out.
So dead end is a little bit special because there's no link going out.
So the only way to get out is to teleport.
Okay?
So if you're a dead end, you're always going to teleport.
So here's my graph.
So here's my transition matrix for this graph.
But this graph has maybe problems.
We don't know. Maybe you have spider traps dead ends.
So we have another graph with very small values.
Okay?
Here they're not so small because you only have five nodes.
But if I have ten billion nodes, the value of each one of these numbers will be less than one over ten billion.
Okay?
So this is what we call the teleportation matrix.
So you will combine these two matrices to get a new transition matrix.
Okay?
Combine them so you make the sum of them.
So this one is the matrix as you will do with the probability beta.
So this is the step.
And the teleportation, you will do with one minus beta.
Okay? And then you combine them.
Notice you combine them.
So this one is beta times this one.
Plus one minus beta times this.
Except here you have one.
And not one minus beta.
Well that's because it's a dead end.
You can't get out here.
Here there's nothing here.
So that means for the dead ends, you have to have an immediate teleport.
Okay?
There you have to have an immediate teleport.
So you have one minus beta if these are not all zeros.
But if it's zero you have to get out from node one.
Okay?
So that gives me this matrix here.
Notice this one, second row.
Here it's zeros.
That means you don't get out.
Here there you're stuck.
But you have to give it probabilities that add to one.
Okay?
So that means you have to multiply this teleportation matrix by one.
So that gives you one fourth probability to go into any other node.
Okay?
Okay, fine.
Is it fixed?
Well if beta is zero it doesn't work.
But we assume beta is never going to be zero.
But there's still other problems to make it really work.
Okay?
So you have this huge matrix.
How does the matrix, the paycheck matrix look if you have ten billion pages?
Pretty big, huh?
Dense matrix.
All the elements are basically non-zero.
Most of them are non-zero.
N squared, non-zero elements.
That's a lot.
It's not N.
It's not the number of nodes times the degree.
It's N squared.
So okay, the Google people were scratching their head and saying,
how do we implement this ten billion by ten billion matrix?
Can you even store it in memory?
How much memory would you need for that matrix?
What about the, can you store it in memory?
What do you think?
Is there a system in today that could store a matrix like that?
No one.
I mean ten billion by ten billion, that's ten to the twentieth elements.
Even Google today doesn't have like that kind of storage, okay?
So the fix is you have to be smart as usual.
Question?
If the universe doesn't contain enough atoms to store the geometry?
I might, sure.
Probably if you had a computer the size of the earth you could store it though.
Because sometimes we say there's ten to the seventy-six particles in the universe.
So yeah, you're fine. If you have the universe you can store it though.
I don't have the universe in my pocket.
So I can't store it.
So the idea is to be a little bit smart.
The idea is not to add the teleportation to every single number,
but to kind of keep it separate, okay?
So here you have, this is the computation you're doing.
You have some vector.
You multiply by a paint rank.
And you're basically looking for eigenvalue convergence of this thing, huh?
But you don't want to store this matrix. It's to big.
So what you do is you keep this one, the original one,
which does not have all, which has lots of zeros, does not have the teleportation.
And the teleportation is added only after that.
So this is like a tax, okay?
It's basically because it's always the same for every node.
It doesn't depend on the node.
That means you could add it in separately, okay?
So this is like one minus theta over n.
And so in that way you don't actually have to store ten to the twenty elements.
The matrix is sparse, okay?
This matrix, the number of elements, is basically similar to the number of edges, okay?
So it's much, much less, okay?
M is what's called a sparse matrix.
So most of the entries in the M are zeros.
You can implement it much more efficiently, okay?
Okay?
Of course you have to handle the dead ends, okay?
Be careful with the dead ends because you have to jump out.
For the dead ends, it's not one minus theta.
You have to be careful because you need to jump out one over n for the dead ends, huh?
Okay?
And then you get something nice.
So you get this nice figure.
Each of the nodes here is proportional to the page rank.
So B and C are very important.
And E is not important.
So what?
Okay.
Okay, so that's the basic page rank.
But once people invent page rank, there's actually lots of extras that people want to do.
So there's still kind of problems and we'll talk about some of them.
The first thing is that page rank measures the popularity in general of the page.
How popular is the page?
But maybe I'm only interested in stamp collecting
and I want to know what is the best page for stamp collecting
and I'm not interested in, in general, popularity.
Maybe there's not so many people doing stamp collecting
and there's many more people interested in Kalashnikov machine guns than stamps.
So there would be no stamp pages who had much less popularity.
So you want maybe a page rank that knows about topics.
Okay, so we'll explain that a little bit.
And another thing is you can take this, you can also have used page rank to find similarities.
That's important if I, for example, if I'm looking for pictures,
I'm looking for pictures of trees, for example, and I have this very nice palm tree
and I want similar pictures, you can use a similar algorithm than this
to find similarity, similar pages.
That's nice, okay, that's one thing.
That's, that's making it very specific.
Another problem is spam, okay, it's possible to spam or to cheat on the page rank
and this is called web spanning and you can do things like live farms
and there's a way around that.
You have to somehow know what are trusted pages.
So there's an algorithm called trust rank.
And finally, the Hudson authorities, we, there's two kinds of importance.
There's the pages that are important, but also the pages that lead to important pages.
So page rank only has one measure.
Okay, so that we saw last time.
But I'm going to talk about these two, topic specific and web spamming.
So beyond page rank, first it's topic specific.
So I have some graph here with the heavy notes and the heavy links and the light links.
But I'm interested in some kind of topic.
So I want only to look at the good pages in a particular topic.
So how do I do that?
I have to give more influence to pages that are close to some topic.
And the way to fix that is you change how you teleport.
Instead of teleporting to any random page, you teleport to pages that are kind of rubbed,
okay, that have something to do with sports or spam collecting.
But once you know that you have the teleport set, you change the teleport set,
you can still use the regular page rank.
So that works really well for the topic specific element.
Okay, how do we get this teleport set?
Well, there's different ways.
You can use classic techniques from older search engines.
You can look at the contents of the pages for that, okay?
You could also look at who is making the query, okay?
You could look at browsing history, okay?
Find type Manchester, what do I mean?
Do I mean football team or do I mean the city Manchester?
Well, it depends on my browsing history, maybe, okay?
So there's many ways to do that kind of stuff, okay?
And that's how topics specific works.
Another way you can extend the page rank is measuring similarity or proximity.
So here's an example.
Let's say we have this graph here.
Look at these two notes, one and four.
How close are they?
Some sets, or seven and one.
How close is it?
Because the shortest path is the same.
One, two.
But maybe four is closer because there's more paths.
Maybe four is closer to one than seven.
Because seven is only one path, okay?
Multiple two-out paths.
Only one path from one to seven, okay?
So that's one way of extending it.
So this is called sim rank.
So here's another example.
I have four pictures and I want to show how similar are these pictures.
So picture one has a house and a tree.
Picture two has a house and a mountain.
Picture three has a house and a tree.
And four has a tree and a mountain.
So which pictures are similar to picture one?
How similar is picture one to four, okay?
Or maybe there's ones that are closer.
One to three might be closer because they're both having a house and a tree.
So we can use something like patron to figure that out, okay?
So this is kind of like topic-specific.
Again, we want to bias the telechord sets.
And recapitulate the page rank.
So it's a kind of variation on the page rank.
So the idea is that the telechord set is the node itself.
So the node is closest to itself, okay?
Usually the telechord set is all relevant nodes.
But here the node is the one closest to itself, okay?
So that's one way that you modify the page rank.
You make random walks from a fixed node, okay?
And then you can do things like, here's an example with authors,
conferences, and tags.
So articles, authors, people writing articles on this.
So this you do it for all nodes.
This is actually used in the website called pinterest.
You may be using it for recognition.
They use something like this, okay?
So this is also known as page rank with restarts.
So here's the example with the pictures.
So what is the most related picture to picture one?
Okay?
You have your telechord set which is picture one.
And you do your random walks with the restart.
And the result of this is going to be picture three, okay?
So if you look here, here you have your graph with seven nodes.
Here you have your transition matrix, okay?
And now you do the convergence of this, okay?
And you can see, notice you have two groups.
You have the pics, the four, the four pics on the top
and the three attributes on the bottom, okay?
So you compute the page rank matrix.
And you can see that here you can do the restart, okay?
On pic one.
So from pic one, you go one, two, three, four,
attack the telechords.
And here you have the three things on the bottom, okay?
And the result is that pic one and pic three are going to be the closest
when you converge this, okay?
That's one way of extending page rank.
And in this other example, when I note one,
you can see that one and four are pretty close, 0.24,
but no seven is farther away, 0.07, okay?
So basically doing the page rank with the restart, okay?
So that gives us three kind of variations of page rank.
It's a normal page rank, which teleports randomly.
We have the topic-specific page rank,
which teleports to pages that are relevant,
and we have the same rank, which always teleports to the same node
for finding similar notes.
So these are three different ways of using page rank, okay?
So now the final thing I want to say is web spamming.
How do you, you can cheat on page rank, okay?
What is web spamming?
So web spamming is a deliberate action to boost a web page in ranking.
And there's lots of these web, there's lots of spam pages, okay,
which are created only to appear your rank results that should not be there.
So this thing, like I mentioned before, has a very dignified name
called search engine optimization, but it's basically cheating.
So Google has spent a lot of effort combating this.
So let me talk a little bit about cheating.
How do you cheat on the web?
In the early search engines, how did they work?
So they crawl the web, they look at all the pages, they index the pages,
and they respond to queries with your pages containing words.
That's the old names that go for page rank, yeah.
That's very easy to cheat there.
How do you measure the importance?
Well, in the early search engines, you look how many times the words are there,
or what is the header, if the header words, okay?
So of course it's very easy to cheat, you do that.
So as people began to use search engines, there were companies
that tried to exploit it to cheat.
So here's an example.
So I'm selling shirts, and I want lots of people to come to my website.
So I want to pretend that my website is about movies.
So how do I do that?
Well, I use a technique.
For the early search engines, it was pretty easy, okay?
How do I make my page appear to be about movies?
It's very easy.
I add the word movies a thousand times to my page,
but I make it so it's white so nobody sees it,
and search engines will see it, yeah.
And can you also spam on the importance of your page?
Yeah, this is before page rank, yeah.
So this doesn't work anymore now, giving it for history.
But there's also a way to spam page rank,
which I explained in a few minutes.
It's just called link farms, okay?
In other ways, you run the query movie,
you see what is the page that came first.
So that's the important page, and you take the text of that page,
you just copy it into your cheat.
This is called term spam, okay?
So Google made a solution to that.
Instead of using the words in the page,
use the words in the link to the page,
which is called the anchor text and the surrounding text,
so what people say about the new.
And of course, use page rank to measure the importance.
So with page rank, the short seller doesn't work.
His technique doesn't work anymore.
It doesn't matter if the short seller puts the word movies in his pages,
because no movie pages are going to link to this page, okay?
So he's not going to be ranked high for movies, or even for shirts.
But you can cheat on this.
And this is called link farms, okay?
Okay, so let's say the short seller, he creates 1,000 pages.
Each of those pages links to the movie page, okay?
Yeah.
But those pages are no in-links.
That doesn't really work, God.
Even if he makes a million pages pointing to the shirt page,
real important movie pages like IMDB,
there's no way you can beat them, okay?
But, but there is a way to beat page rank.
We span farming, kind of coordinated with that.
This has been used, and it's sometimes called Google bombing.
A few years ago, now it works too.
But if when George Bush was president,
you type miserable failure in Google,
it's a choice to the biography of George Bush.
It's nice, isn't it?
Miserable failure links to Bush,
and it still does, it's called Google bombing.
So how does that work?
It's possible to do that, to cheat.
This is called spam farming.
It's possible to create a structure in the graph
that concentrates the page rank of a particular page.
Okay? How does that work?
So, I'll show you the technique.
So, basically it works like this.
There's three, there's three kinds of pages.
So let's say I'm a spammer.
I want to cheat, okay?
I want to increase the page rank of some page.
So I look at the web,
and there's three kinds of pages for my interview.
There's inaccessible pages.
That's pages owned by other people.
Most of them are inaccessible.
Then there's accessible pages.
These are pages owned by other people,
but I can add something to the blogs, for example,
or comments, reviews.
I can post there.
I can put something there, even though I'm not the owner.
It's called accessible pages.
And finally, there's a small number of pages
that are owned by me, spammer.
These are called owned pages.
So I have these three kinds of pages.
So how can I use this?
So that's the structure.
So here's the web.
This is the whole web.
And here is the page,
and I want to increase the page rank of this one.
So how do I do it?
I want to maximize the page rank of the team.
The web has two kinds of pages.
So the green ones,
those are the ones that I can add something to,
even though I'm not the owner,
like their blogs, or I can put a comment, or something.
Then the others are inaccessible.
I have no control over them.
Most of them are inaccessible.
You might have a few pages where you can add,
but most of them are owned by other people.
So I can't do anything to her.
Fine.
So what do I do?
Well, the first thing I do
is I post on as many places as I can,
links to team.
I link to team from as many accessible pages as possible.
That's the first step.
It's not enough, but it's the first step.
That will already increase a little bit
the page rank of the team.
But that's not enough.
What I really have to do then,
is I do another step
where I construct what's called a link farm
to multiply the page rank of team.
So these red pages are my pages.
So I buy a bunch of machines,
or I rent machines,
and I can write anything I want on them.
This is my link farm, the obvious red pages.
And the link farm pages
will all point to team,
and team points back to them.
So it seems kind of weird,
all these pages are going to point to team,
and on team I make the link back.
And that's called the link farm.
And these red pages are owned by me.
So how does this work?
So this will actually improve my page rank.
Notice there can be many here,
not billions, but not billions.
But there could be millions.
So how can this work?
So let's make some computations.
Let's compute the page rank now.
So let's say there's N pages
that are getting accessible.
N is huge numbers.
And there's N pages that are owned by me.
Which is also being numbered
much smaller than N.
Let's say that little x in green
is the page rank that comes from the accessible pages.
It's not so big, but there's some numbers.
And Y is the page rank of page T.
Now we can compute according to page rank
what it's going to be.
So first of all, what is the rank
with one of these pages?
What is the rank?
Several sources of rank.
One rank, one source comes from the outlet of T.
But another one comes from the teleportation.
So it's like this.
So each form page has this rank.
So Y is the rank of T.
So it's beta Y over M
plus 1 minus beta over N.
See that?
So it's the page rank coming from T
and the teleportation rank.
Okay?
Okay, let's go on.
So what is the rank of page T?
Well, it's the page rank coming from the green pages
plus all the page rank.
These only have one update.
And they're all going to T.
So it's M times beta
and this is the page rank coming from the red pages.
Beta Y over M plus 1 minus beta over N.
You also have teleportation to page T.
So this is a very small one.
Okay?
So here we have a nice formula.
So let's simplify this
and see what we can learn from it.
Okay?
So we can simplify this a little bit.
So this Y is equal to X plus beta squared Y
plus this beta times 1 minus beta
times M over N
plus this very small number.
Okay?
This number we should remove is too small.
And then we simplify.
We have Y on both sides here.
So it's 1 minus beta squared.
So Y is equal to X divided by 1 minus beta squared
plus this number
divided by 1 minus beta squared.
So it's some constant times M over N.
So what does that mean?
Okay?
So the value of the constant is beta over 1 plus beta.
Okay?
What can we learn from this?
What can we learn from this?
How can we increase now the page rank of Y?
Notice there is this M factor here.
By making M large,
we can make Y as large as we want.
So independent of everybody else.
If we put a million pages here,
we make it large.
If we put 10 billion, it's much bigger.
You add a number proportional to M.
So that means the more pages you put in the farm,
the higher the page rank it gets.
Okay?
And that's how these big farms work.
So how can that work?
What is really the idea here?
How is this working?
Well, it's because these farm pages,
they all get teleportation is going to them.
And the more you have them,
the bigger fraction of teleportations will go there.
So the teleportation here,
1 minus beta over N times M.
So it depends on the fraction of M over N.
The bigger that M is,
the more the teleportation will go there,
and the more you can increase the page rank.
And it doesn't matter what anybody else does.
They do whatever they want.
If I'm rich enough, I just have to create a lot of pages,
and the teleportation will go there,
and I can increase that.
So the search engine optimizers,
this is what they do.
Okay?
The random teleportation from all the web to the farm nodes
gets concentrated and pushed into page T.
Okay?
And this is how the big farms work.
Okay?
Actually, you might say,
yeah, M is still going to be very small compared to M.
This is true.
But you don't have to have M be large compared to M.
It just has to be larger than your competitors.
So if you're a shirt seller,
as long as you get higher up than other shirt sellers,
you're good.
You're not competing with really popular pages,
like Amazon or someone.
You're only competing with other shirt sellers.
So M doesn't even have to be that large.
But of course, the shirt sellers will all be paying these companies
the search engine optimizers for increasing their rank.
And the one who pays the most will increase the rank the most.
Okay?
So this was a big problem for people in the early days.
They have now fixed sort of that problem.
So a lot of the problem, you see how it works up.
So a lot of the problem of this has been fixed now.
This was a big problem in the first decade of the 2000s.
Really big problem.
But so Google spent a lot of effort to try to fix this.
So the algorithm they use now is a secret.
Nobody knows what it is.
Okay?
And does it fix the problem?
I don't know.
What do you think?
Is there still doing people doing search engine optimization?
Yes.
They still do it?
Yeah.
That means Google is not that successful, right?
But here's an idea how to fix it.
This is called trust rank.
And it's based on concept of what's called spam mass.
So we want to combat this.
So this is called linked spam.
Okay?
How do you combat this?
Well, one way is that you know where the spam farms are.
You blacklist them.
But of course they're not going to announce themselves publicly.
Okay?
So that doesn't always work.
But the other way would be teleporting only to trusted pages.
This one here you're teleporting to all these pages.
Even the farm pages.
But maybe they're not trusted.
So somehow you have to find out what are trusted pages.
And you teleport mostly to them.
Okay?
And the idea is it's very rare for a good page, a non-cheating page,
to point to a spam page.
These spam pages tend to be hidden inside linked farms.
Okay?
So you change your teleport set, again, to be not everybody,
but trusted pages.
So it's a variation of topic to specific patron.
Okay?
So you do a trust computation.
You start from trusted pages and you compute trust.
And you split trust across the outline of the names.
And the degree of trust, so you have some pages trusted at.
What a New York Times trusted page.
The degree of trust decreases with the distance.
So there's some way of distributing trust.
Okay?
So this is not that easy.
Because you can't actually inspect all the pages manually.
You want to have a lot of pages that are trusted.
The more the better.
But you have to make sure they're trusted.
So how do you do that?
Well, one way is just page rank itself.
Usually the top pages produced by page rank are okay.
Because the linked farms, they can't actually push the page up so high.
Or you have certain domains where you know that it's trusted.
The dot edu domain is universities, for example.
Or government pages, Hart-Milston, for example.
Okay?
And all pages below the threshold are marked as spam.
But that's still not really good enough.
So the way that it's done in the trust rank model
is you have this concept called spam mass.
So you start with this green trusted set of pages.
And you want to say, and this is the page you're looking at.
And then you're looking at the page rank of this page.
And you want to say, how much of this page rank is coming from spam?
How much is coming from trusted pages?
Okay?
So there's some kind of estimate.
We don't really know.
RP is the page rank of page P.
RP plus is the page rank of P when you teleport into trusted pages only.
And then RP minus is the difference between the two.
And this is called the spam mass.
How much of the page rank comes from spam pages?
Okay?
So a fraction of the page rank is coming from spam pages.
So pages with high spam mass are spam.
You throw them out.
Okay?
So even pages that are not in the trusted set,
you can keep them if they have low spam mass.
Okay?
So this kind of idea, you have to do this
to not be killed by big farms.
Okay?
So this is one approach.
The question, what does Google do today?
It's not really known except for some people working at Google.
It's kind of a secret.
And that helps them, of course,
because it's hard to cheat if you don't know the algorithm.
Okay?
So let me now just summarize the page rank.
Page rank is one of many powerful techniques.
It's a kind of random block.
So we looked how that works.
We represented it with matrices.
So it's the constant eigenvalue eigenvectors can help us,
okay, for understanding it.
I showed a little bit of the history.
That is the problems.
The matrices are very big.
So how do you handle huge matrices?
How do you handle topics specific?
So there's a narrowing to topics specific, teleports specific topics.
You can also use it for finding similar nodes.
And you can spam it.
So I showed you how linked farms work.
They can increase the page rank.
But you can combat that using algorithms such as trust rank
to estimate spam mass.
And so the linked farms won't kill you.
Okay.
So let me end there.
So that gives you some idea of all of the stuff going on around the page rank.
So, of course, the page rank is very important.
It was a key invention.
It's what made Google rich at base.
And the step after that was the advertising.
It was targeted advertising with the generalized second price.
So that and page rank is what Google made their empire out of.
Okay.
