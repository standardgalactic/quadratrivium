1
00:00:00,000 --> 00:00:04,720
The stated aim of a company like OpenAI

2
00:00:04,720 --> 00:00:08,240
is the development of artificial general intelligence.

3
00:00:08,240 --> 00:00:10,640
Now, artificial general intelligence

4
00:00:10,640 --> 00:00:15,160
is in Google AI terms,

5
00:00:15,160 --> 00:00:17,000
the equivalent of human intelligence.

6
00:00:17,000 --> 00:00:19,440
What I kind of want to point out here,

7
00:00:19,440 --> 00:00:22,920
when you look at what a general intelligence is,

8
00:00:22,920 --> 00:00:26,320
then that's actually rooted in Charles Spearman

9
00:00:26,320 --> 00:00:28,000
and the idea of the G factor.

10
00:00:28,000 --> 00:00:31,560
But Charles Spearman was a hygienicist.

11
00:00:31,560 --> 00:00:35,560
And his reason for developing this ranking

12
00:00:35,560 --> 00:00:39,360
of general intelligence was to rank human intelligence

13
00:00:39,360 --> 00:00:41,360
for selective breeding, et cetera.

14
00:00:41,360 --> 00:00:46,000
So you've got this drive for artificial general intelligence.

15
00:00:46,000 --> 00:00:49,200
But when you actually work out what general intelligence is,

16
00:00:49,200 --> 00:00:51,760
it's got some very, very dark histories.

17
00:00:52,400 --> 00:00:58,520
Hello and welcome to Planet Critical,

18
00:00:58,520 --> 00:01:00,960
the podcast for a world in crisis.

19
00:01:00,960 --> 00:01:02,240
My name is Rachel Donald.

20
00:01:02,240 --> 00:01:04,800
I'm a climate corruption journalist and your host.

21
00:01:04,800 --> 00:01:06,360
Every week, I interview experts

22
00:01:06,360 --> 00:01:08,480
who are battling to save our planet.

23
00:01:08,480 --> 00:01:11,360
My guests are scientists, politicians, academics,

24
00:01:11,360 --> 00:01:13,280
journalists and activists.

25
00:01:13,280 --> 00:01:15,840
They explain the complexities of the energy,

26
00:01:15,840 --> 00:01:19,640
economic, political and cultural crises we face today,

27
00:01:19,640 --> 00:01:21,400
revealing what's really going on

28
00:01:21,440 --> 00:01:23,600
and what they think needs to be done.

29
00:01:23,600 --> 00:01:26,120
These are the stories of the big picture.

30
00:01:26,120 --> 00:01:29,640
Go to planetcritical.com to learn more and subscribe.

31
00:01:29,640 --> 00:01:31,960
My guest this week is John Wilde.

32
00:01:31,960 --> 00:01:33,520
John is a London-based artist

33
00:01:33,520 --> 00:01:36,480
who works across performance, sound, text, code,

34
00:01:36,480 --> 00:01:38,240
electronics and machine learning

35
00:01:38,240 --> 00:01:39,920
to research the future's imminent

36
00:01:39,920 --> 00:01:41,520
within digital technology.

37
00:01:41,520 --> 00:01:43,480
John joined me today to talk about

38
00:01:43,480 --> 00:01:46,040
culture and artificial intelligence,

39
00:01:46,040 --> 00:01:50,160
how the stories we tell ourselves inform our technologies

40
00:01:50,200 --> 00:01:52,480
and then how those technologies inform the stories

41
00:01:52,480 --> 00:01:53,560
we tell ourselves,

42
00:01:53,560 --> 00:01:56,760
getting caught in these kinds of circular loops essentially,

43
00:01:56,760 --> 00:01:58,280
which make it increasingly difficult

44
00:01:58,280 --> 00:02:00,080
to imagine a different way of being.

45
00:02:00,080 --> 00:02:02,120
John talks about this in relationship

46
00:02:02,120 --> 00:02:03,840
to artificial intelligence.

47
00:02:03,840 --> 00:02:05,840
Artificial intelligence is an incredibly

48
00:02:05,840 --> 00:02:07,480
energy-hungry technology.

49
00:02:07,480 --> 00:02:10,080
It is being used for profit motives.

50
00:02:10,080 --> 00:02:11,680
We have very little understanding

51
00:02:11,680 --> 00:02:14,360
of what it could do when unleashed upon the world

52
00:02:14,360 --> 00:02:17,160
and at the moment, all it's doing is threatening jobs

53
00:02:17,160 --> 00:02:19,360
rather than creating new ways of being.

54
00:02:19,360 --> 00:02:21,360
John's research shows what we could do

55
00:02:21,360 --> 00:02:24,360
if we imagined using mycelium as a framework

56
00:02:24,360 --> 00:02:28,000
for developing something decentralized, interconnected,

57
00:02:28,000 --> 00:02:30,280
entangled and symbiotic.

58
00:02:30,280 --> 00:02:33,280
To begin with, John explains the history of thinking

59
00:02:33,280 --> 00:02:34,680
and artificial intelligence.

60
00:02:34,680 --> 00:02:38,320
How Silicon Valley is infused with theories and stories

61
00:02:38,320 --> 00:02:40,920
that came out of Russia in the late 19th century.

62
00:02:40,920 --> 00:02:44,680
The desire to pollinate the universe with consciousness,

63
00:02:44,680 --> 00:02:47,000
creating a hierarchy of consciousness,

64
00:02:47,000 --> 00:02:50,280
as if humanity is the only thing that is truly conscious

65
00:02:50,280 --> 00:02:52,040
or would be able to do such a thing

66
00:02:52,040 --> 00:02:54,600
as if the universe isn't already conscious.

67
00:02:54,600 --> 00:02:56,680
And he also explains how this hierarchy

68
00:02:56,680 --> 00:02:58,920
of consciousness or intelligence

69
00:02:58,920 --> 00:03:00,240
that is directing Silicon Valley

70
00:03:00,240 --> 00:03:02,120
to make an artificial general intelligence

71
00:03:02,120 --> 00:03:04,360
comes out of eugenicist thinking.

72
00:03:04,360 --> 00:03:06,480
This is such a fascinating conversation.

73
00:03:06,480 --> 00:03:08,400
We had so much fun recording this.

74
00:03:08,400 --> 00:03:09,960
I knew a fair bit about AI

75
00:03:09,960 --> 00:03:12,360
thanks to research into the effective altruist movement,

76
00:03:12,360 --> 00:03:14,720
but I did not know the history that John lays out today.

77
00:03:14,720 --> 00:03:16,080
And understanding more of that history

78
00:03:16,080 --> 00:03:17,320
makes me really grateful

79
00:03:17,320 --> 00:03:19,560
that people like him and artists around the world

80
00:03:19,560 --> 00:03:21,080
and technologists around the world

81
00:03:21,080 --> 00:03:23,640
are trying to think about how to develop permacomputing

82
00:03:23,640 --> 00:03:25,280
or the wood-wide web,

83
00:03:25,280 --> 00:03:29,120
collaborative, interdependent, entangled projects

84
00:03:29,120 --> 00:03:31,880
that reflect the intelligence and harmony

85
00:03:31,880 --> 00:03:33,600
of natural ecosystems

86
00:03:33,600 --> 00:03:35,680
in order for us all to live more sustainably

87
00:03:35,680 --> 00:03:36,520
with one another.

88
00:03:36,520 --> 00:03:38,120
And we end our conversation with a dialogue

89
00:03:38,120 --> 00:03:39,600
on exactly that.

90
00:03:39,600 --> 00:03:41,240
What is sustainable computing

91
00:03:41,240 --> 00:03:43,480
and what is sustainability more widely?

92
00:03:43,480 --> 00:03:45,080
I hope you all enjoy the episode.

93
00:03:45,160 --> 00:03:47,480
If you do, please share it far and wide.

94
00:03:47,480 --> 00:03:48,680
And if you're loving the show,

95
00:03:48,680 --> 00:03:50,600
become a patron on Patreon

96
00:03:50,600 --> 00:03:51,800
or support Planet Critical

97
00:03:51,800 --> 00:03:55,040
with a paid subscription at planetcritical.com.

98
00:03:55,040 --> 00:03:57,360
By signing up, you'll get the Planet Critical newsletter

99
00:03:57,360 --> 00:03:58,800
inspired by each episode

100
00:03:58,800 --> 00:04:01,040
delivered straight to your inbox every week.

101
00:04:01,040 --> 00:04:02,200
You'll also have access

102
00:04:02,200 --> 00:04:04,640
to the wonderful Planet Critical community

103
00:04:04,640 --> 00:04:08,400
who are full of inspiring thoughts, ideas, critiques

104
00:04:08,400 --> 00:04:09,960
and determination.

105
00:04:09,960 --> 00:04:11,920
The links are in the description box below.

106
00:04:11,920 --> 00:04:13,600
I'm so grateful to everyone

107
00:04:13,600 --> 00:04:15,200
who chooses to support the project.

108
00:04:15,200 --> 00:04:16,720
I'm a vehement believer in ad-free

109
00:04:16,720 --> 00:04:18,160
and open-access content,

110
00:04:18,160 --> 00:04:19,840
so Planet Critical wouldn't exist

111
00:04:19,840 --> 00:04:22,760
without the direct support of the amazing community.

112
00:04:22,760 --> 00:04:24,520
Thank you so much to all of you

113
00:04:24,520 --> 00:04:26,320
who believe in Planet Critical

114
00:04:26,320 --> 00:04:28,360
and keep the project going every week.

115
00:04:28,360 --> 00:04:29,680
John, thank you very much for joining me

116
00:04:29,680 --> 00:04:30,520
on Planet Critical.

117
00:04:30,520 --> 00:04:32,600
It is a pleasure to have you on the show.

118
00:04:32,600 --> 00:04:34,200
Well, thank you for inviting me.

119
00:04:35,600 --> 00:04:36,440
Happy to.

120
00:04:36,440 --> 00:04:38,040
As I was saying before we started recording it,

121
00:04:38,040 --> 00:04:39,520
such an interesting conversation with Maggie

122
00:04:39,520 --> 00:04:40,560
who platformed you.

123
00:04:41,560 --> 00:04:44,240
And I think that speaking with artists

124
00:04:44,240 --> 00:04:45,760
is a really critical component

125
00:04:45,760 --> 00:04:48,480
to understanding what the hell is going on in the world

126
00:04:48,480 --> 00:04:51,160
and what we can do about it,

127
00:04:51,160 --> 00:04:53,000
which leads me to my first question.

128
00:04:53,000 --> 00:04:54,800
Why is the world in crisis?

129
00:04:56,640 --> 00:04:58,240
That's such a big question, isn't it?

130
00:04:58,240 --> 00:04:59,080
I know.

131
00:05:00,840 --> 00:05:04,080
I mean, I'm going to start with a report

132
00:05:04,080 --> 00:05:07,440
which was out last week, which really struck me.

133
00:05:07,680 --> 00:05:12,680
So I read an article by Duncan Agnew in Nature Magazine,

134
00:05:13,040 --> 00:05:14,680
which suggested that climate change

135
00:05:14,680 --> 00:05:17,120
is having an effect on universal timekeeping.

136
00:05:18,240 --> 00:05:19,080
What?

137
00:05:19,080 --> 00:05:22,640
So Coordinated Universal Time, or UTC,

138
00:05:22,640 --> 00:05:24,360
is the primary time standard

139
00:05:24,360 --> 00:05:26,360
globally used to regulate clocks.

140
00:05:27,360 --> 00:05:30,880
So UTC closely follows the rotation of the Earth.

141
00:05:32,120 --> 00:05:35,960
But accelerating melt from Greenland and Antarctica

142
00:05:35,960 --> 00:05:40,360
is adding extra water to the world's sea.

143
00:05:40,360 --> 00:05:43,080
It's redistributing mass around the globe,

144
00:05:43,080 --> 00:05:45,320
and that's causing a very slight slowing

145
00:05:45,320 --> 00:05:46,680
in the Earth's rotation.

146
00:05:48,280 --> 00:05:53,160
And if you combine that with what we know

147
00:05:53,160 --> 00:05:57,440
about the shift in the Earth's poles.

148
00:05:57,440 --> 00:06:00,040
So since the 1980s, it's been shown

149
00:06:00,040 --> 00:06:02,240
that the massive melting of glaciers

150
00:06:02,240 --> 00:06:05,000
as a result of global heating

151
00:06:05,000 --> 00:06:09,600
has caused a shift in the Earth's axis of around four metres.

152
00:06:09,600 --> 00:06:13,440
So I think if we think about this question,

153
00:06:13,440 --> 00:06:15,040
the shifting of the Earth's axis

154
00:06:15,040 --> 00:06:17,840
and the slowing of the Earth's orientation

155
00:06:17,840 --> 00:06:22,680
mark both an impressive, to be honest,

156
00:06:22,680 --> 00:06:26,360
but terrifying achievement of human,

157
00:06:26,360 --> 00:06:28,920
global, well, geoengineering.

158
00:06:28,920 --> 00:06:33,280
The impact that we've had as basically fundamentally

159
00:06:33,280 --> 00:06:37,200
has shifted time and the axis of the Earth.

160
00:06:37,200 --> 00:06:40,960
And that seems to be the ultimate mark of the amphipersine.

161
00:06:40,960 --> 00:06:43,680
But I think what troubles me with this,

162
00:06:43,680 --> 00:06:45,280
well, I mean, there's lots of things

163
00:06:45,280 --> 00:06:47,240
which troubles me with this,

164
00:06:47,240 --> 00:06:50,440
but such a feat could only be the outcome

165
00:06:50,440 --> 00:06:54,200
of sustained and coordinated human action and interaction.

166
00:06:55,200 --> 00:06:57,760
Yet no one's planned, organised,

167
00:06:57,760 --> 00:07:01,280
voted for, or even imagined such a venture.

168
00:07:04,280 --> 00:07:06,320
And I kind of wanted to start this conversation

169
00:07:06,320 --> 00:07:11,800
with a kind of provocation which comes from my own research.

170
00:07:13,480 --> 00:07:17,680
My own research is looking at artificial intelligence,

171
00:07:17,680 --> 00:07:20,440
specifically narratives around artificial intelligence.

172
00:07:22,040 --> 00:07:23,880
But what is the coordinating force

173
00:07:23,880 --> 00:07:26,440
that's playing a role here?

174
00:07:26,440 --> 00:07:29,720
And I think the provocation that I want to put forward

175
00:07:29,720 --> 00:07:33,200
is that it's a non-conscious intelligence.

176
00:07:33,200 --> 00:07:35,960
Or an artificial intelligence that we call the market.

177
00:07:37,120 --> 00:07:41,080
I think the market struck producers emergent forms,

178
00:07:41,080 --> 00:07:44,960
which you could call a form of non-conscious intelligence.

179
00:07:44,960 --> 00:07:47,440
Yeah, I totally agree.

180
00:07:49,280 --> 00:07:51,440
What a way to kick us off, by the way.

181
00:07:52,720 --> 00:07:54,400
Wow, I didn't know that,

182
00:07:54,400 --> 00:07:57,680
but climate change has been an impact

183
00:07:57,680 --> 00:07:59,680
on universal timekeeping.

184
00:07:59,680 --> 00:08:02,360
I was thinking about this question that you asked

185
00:08:02,360 --> 00:08:06,960
as this appeared in my feed, and I'm like, wow.

186
00:08:06,960 --> 00:08:11,240
But not only that, it's that universal time,

187
00:08:11,240 --> 00:08:16,240
it affects computing because the computer programmes

188
00:08:16,760 --> 00:08:21,000
made to keep track of universal time

189
00:08:21,000 --> 00:08:25,160
are going to struggle with this slowing down of the Earth.

190
00:08:25,160 --> 00:08:27,280
So it kind of comes into the territory

191
00:08:27,280 --> 00:08:29,560
that I'm also interested in, in a way.

192
00:08:32,960 --> 00:08:34,400
There's so much there, isn't there?

193
00:08:34,400 --> 00:08:39,400
Like the idea of having a human system

194
00:08:39,480 --> 00:08:42,080
mapped onto a natural system,

195
00:08:42,080 --> 00:08:45,320
the human system impacting the natural system,

196
00:08:45,320 --> 00:08:46,560
and then the natural system,

197
00:08:46,560 --> 00:08:50,800
and then being unable to deal with the fallout,

198
00:08:50,800 --> 00:08:52,160
to deal with the consequences,

199
00:08:52,160 --> 00:08:54,680
to understand even the new reality,

200
00:08:54,680 --> 00:08:59,040
because the limits of that system were so fixed and rigid,

201
00:08:59,040 --> 00:09:04,040
which is kind of a really classic feature of modernity.

202
00:09:05,040 --> 00:09:07,480
Like there just being no flexibility.

203
00:09:09,160 --> 00:09:12,800
And then watching reality as we understand it,

204
00:09:12,800 --> 00:09:15,800
just kind of peel away in that moment,

205
00:09:15,800 --> 00:09:17,400
because the systems aren't built for it.

206
00:09:17,400 --> 00:09:21,600
So it really reveals this thread of domination

207
00:09:21,600 --> 00:09:24,920
that runs through modernity, like domination over nature.

208
00:09:25,920 --> 00:09:29,400
It doesn't work, the domination over ourselves.

209
00:09:29,400 --> 00:09:30,640
It just doesn't work.

210
00:09:30,640 --> 00:09:34,600
There is a, it's brittle and it's fragile and it will snap

211
00:09:34,600 --> 00:09:39,040
if it's met with enough kind of shifting, evolving resistance.

212
00:09:39,880 --> 00:09:42,720
I guess the challenge that we have

213
00:09:42,720 --> 00:09:45,760
is if we understand this as a form of,

214
00:09:45,760 --> 00:09:47,120
or if we understand the market

215
00:09:47,120 --> 00:09:50,480
that produces these kind of emergent forms,

216
00:09:51,760 --> 00:09:53,480
as a kind of structural system

217
00:09:53,480 --> 00:09:58,800
that has an intelligence that structures human activity, et cetera.

218
00:09:58,800 --> 00:10:01,520
How do we, how do we move beyond that?

219
00:10:01,520 --> 00:10:06,160
How do we imagine futures, which are structured a different way?

220
00:10:06,160 --> 00:10:10,200
How do we imagine technologies, which are,

221
00:10:12,440 --> 00:10:16,080
which behave in a different way, which are sustainable?

222
00:10:19,160 --> 00:10:23,280
My own research actually looks at the kind of imaginaries

223
00:10:23,280 --> 00:10:25,400
around artificial intelligence.

224
00:10:25,400 --> 00:10:30,320
And I think they can tell us quite a lot really about why we end up

225
00:10:32,200 --> 00:10:35,920
kind of looking to the stars rather than looking to the soil,

226
00:10:35,920 --> 00:10:38,160
rather than looking to the earth.

227
00:10:39,360 --> 00:10:41,360
That's beautiful.

228
00:10:44,080 --> 00:10:47,800
On this artificial intelligence,

229
00:10:49,880 --> 00:10:52,200
I mean, this is kind of what Hayek spoke about as well.

230
00:10:52,760 --> 00:10:55,920
The invisible hand of the market directing people.

231
00:10:58,160 --> 00:11:00,760
The sort of godfather of neoliberalism, essentially.

232
00:11:01,520 --> 00:11:03,720
And yes.

233
00:11:03,720 --> 00:11:09,200
I think you were Smith as well, weren't you, in the wealth of nations?

234
00:11:09,200 --> 00:11:13,800
I think Smith initiated it, and then people kind of took on this idea.

235
00:11:13,800 --> 00:11:15,920
But yeah, the idea of the invisible hand.

236
00:11:16,680 --> 00:11:17,880
Which is interesting, isn't it?

237
00:11:17,920 --> 00:11:24,080
Because there's a concept there of this physical thing being shaped.

238
00:11:24,080 --> 00:11:26,520
But they weren't talking about a brain.

239
00:11:26,520 --> 00:11:29,080
They weren't talking about the invisible brain.

240
00:11:29,080 --> 00:11:32,160
Whereas what is directing that hand to move?

241
00:11:32,160 --> 00:11:34,560
The idea is that it would respond to needs or whatever.

242
00:11:34,560 --> 00:11:37,480
And it's obvious that's obviously not been the case.

243
00:11:37,480 --> 00:11:42,640
Like we've sort of created a system that is impacted,

244
00:11:42,640 --> 00:11:46,320
but also impact its environment around it as it accumulates more

245
00:11:46,360 --> 00:11:49,360
historical precedent and knowledge.

246
00:11:49,760 --> 00:11:54,000
And it's just embodied really with historicity, you might say.

247
00:11:55,160 --> 00:11:58,440
Kind of like self-perpetuate itself and grows and grows and grows.

248
00:11:58,920 --> 00:12:02,000
Well, I think that self-perpetuation is the thing.

249
00:12:02,360 --> 00:12:06,440
I think where this kind of connects with the kind of research

250
00:12:06,440 --> 00:12:09,920
that I do on artificial intelligence, it's kind of like looking at what

251
00:12:09,920 --> 00:12:13,360
intelligence is in some sort of way.

252
00:12:13,920 --> 00:12:18,760
And obviously with the kind of common sense for you is this kind of

253
00:12:18,760 --> 00:12:24,480
conscious intelligence, the kind of the human conscious intelligence.

254
00:12:24,760 --> 00:12:28,840
But conscious intelligence is very rare in the world.

255
00:12:29,800 --> 00:12:31,840
Most forms of intelligence that would come across

256
00:12:31,840 --> 00:12:34,600
are a form of non-conscious intelligence.

257
00:12:35,240 --> 00:12:38,160
So this is kind of sensing and acting on the world

258
00:12:38,160 --> 00:12:40,920
in a way that produces very complex outcomes.

259
00:12:41,720 --> 00:12:46,560
But but don't don't have but don't have at the car this kind of

260
00:12:46,560 --> 00:12:51,440
conscious drive that maybe maybe language produces in humans.

261
00:12:52,160 --> 00:12:54,240
OK, and we pause there.

262
00:12:54,240 --> 00:12:57,480
Yeah. So conscious intelligence is rare in the world,

263
00:12:57,920 --> 00:13:00,240
but this unconscious intelligence is sensed.

264
00:13:00,240 --> 00:13:02,560
Is that the word you used?

265
00:13:02,560 --> 00:13:06,680
Well, I'm saying that for something to act, for something to act,

266
00:13:06,680 --> 00:13:09,400
there's some sort of sensing, some sort of information.

267
00:13:10,320 --> 00:13:13,240
And then there's a behavior that responds to that,

268
00:13:14,440 --> 00:13:16,440
which produces complex outcomes.

269
00:13:16,640 --> 00:13:19,440
So I'm thinking I'm thinking as a good example,

270
00:13:19,440 --> 00:13:23,160
or an example is quite often used as a slime mold.

271
00:13:24,000 --> 00:13:27,400
And in my own practice, we've also been using mycelium,

272
00:13:27,400 --> 00:13:29,880
but slime mode is quite a common one.

273
00:13:30,440 --> 00:13:35,240
So slime mode is a single celled algorithm organism,

274
00:13:36,040 --> 00:13:40,680
which it basically produces filaments

275
00:13:40,680 --> 00:13:43,320
which stretch out to find food in all directions.

276
00:13:43,320 --> 00:13:45,800
And then when it finds finds food,

277
00:13:45,800 --> 00:13:49,200
it solidifies the filaments that is produced.

278
00:13:50,240 --> 00:13:54,920
And it's it's been used to mimic the Tokyo.

279
00:13:56,760 --> 00:13:58,240
Tube map.

280
00:13:58,240 --> 00:14:04,240
So the Tokyo tube because of the kind of geology of the area, etc.

281
00:14:05,520 --> 00:14:08,440
The planning of it is there's quite a lot of complexity

282
00:14:08,440 --> 00:14:11,480
to how to produce the most direct routes.

283
00:14:12,320 --> 00:14:16,280
But by by creating an artificial map of the tube

284
00:14:16,280 --> 00:14:19,400
using the food for the slime mold,

285
00:14:20,280 --> 00:14:22,320
the slime mold managed to

286
00:14:23,560 --> 00:14:25,800
calculate the most direct routes,

287
00:14:25,800 --> 00:14:31,240
which pretty much mimic the the actual Tokyo underground.

288
00:14:31,680 --> 00:14:34,280
So that's that's the way that you could see

289
00:14:34,280 --> 00:14:38,440
that there's a non-conscious intelligence working.

290
00:14:39,280 --> 00:14:43,560
And NASA has used exactly the same model to map

291
00:14:43,560 --> 00:14:47,200
to map the dark matter that holds together the universe.

292
00:14:47,720 --> 00:14:50,880
So these kind of intelligences,

293
00:14:50,880 --> 00:14:53,360
which aren't a model of conscious intelligence,

294
00:14:54,480 --> 00:14:58,120
still produce very complex behavior in the world.

295
00:14:58,560 --> 00:15:00,480
And I'd go.

296
00:15:00,520 --> 00:15:02,680
Sorry, it's just I suppose I'm getting stuck

297
00:15:02,680 --> 00:15:06,000
on this conscious unconscious binary.

298
00:15:07,760 --> 00:15:10,800
Because what we're talking about then

299
00:15:10,800 --> 00:15:12,960
when we talk about consciousness,

300
00:15:12,960 --> 00:15:15,360
because there's quite a lot of, you know,

301
00:15:15,360 --> 00:15:17,240
stuff now coming out of physics

302
00:15:17,240 --> 00:15:19,360
and other sort of theories that suggest that,

303
00:15:19,880 --> 00:15:22,240
well, everything is just consciousness

304
00:15:22,240 --> 00:15:26,040
and that perhaps it's consciousness that predates matter.

305
00:15:26,560 --> 00:15:28,680
And thus, you know, perhaps the slime mold

306
00:15:28,720 --> 00:15:31,400
doesn't have a brain in the way that we

307
00:15:31,400 --> 00:15:32,400
well, definitely doesn't, right?

308
00:15:32,400 --> 00:15:33,400
It's one cell.

309
00:15:34,640 --> 00:15:38,280
But that doesn't necessarily mean it's it's not conscious.

310
00:15:38,560 --> 00:15:40,440
Like, I guess I'm concerned.

311
00:15:40,440 --> 00:15:41,600
Yeah, I agree.

312
00:15:41,600 --> 00:15:43,600
I agree.

313
00:15:43,600 --> 00:15:44,240
I agree.

314
00:15:44,240 --> 00:15:46,480
And it's exactly the hierarchy of consciousness,

315
00:15:46,480 --> 00:15:48,280
which I want to break down.

316
00:15:48,280 --> 00:15:49,080
Right.

317
00:15:49,080 --> 00:15:55,040
I mean, I'm using the term conscious in this way.

318
00:15:56,040 --> 00:16:01,360
As a relationship to language and the the modelling

319
00:16:02,360 --> 00:16:04,760
of the world as an abstract ship.

320
00:16:05,680 --> 00:16:08,520
Of which then things are planned.

321
00:16:08,520 --> 00:16:10,640
But I don't believe this is how humans behave.

322
00:16:10,640 --> 00:16:14,240
I think humans, the vast majority of human activity

323
00:16:14,240 --> 00:16:15,760
is non-conscious.

324
00:16:16,880 --> 00:16:19,800
Like, ride a bike, you don't have to do

325
00:16:19,800 --> 00:16:23,920
with the mathematical calculations to stay on the bike

326
00:16:24,800 --> 00:16:26,840
and direct and route, et cetera.

327
00:16:27,680 --> 00:16:30,760
I think the vast majority of action is non-conscious.

328
00:16:32,040 --> 00:16:34,880
But wouldn't that suggest then that consciousness

329
00:16:34,880 --> 00:16:39,320
is only these kind of consciousness is language

330
00:16:39,320 --> 00:16:41,720
because mass, for example, could be, you know,

331
00:16:41,720 --> 00:16:44,360
understood as like a language for understanding the universe

332
00:16:44,720 --> 00:16:47,600
or other laws for which words don't quite

333
00:16:49,440 --> 00:16:51,240
aren't quite useful.

334
00:16:51,280 --> 00:16:54,480
And so does it not then become that consciousness is language

335
00:16:54,480 --> 00:16:56,520
and everything else is non-consciousness?

336
00:16:57,720 --> 00:17:00,360
I think so in the way that I'm trying to say it.

337
00:17:00,760 --> 00:17:04,880
But the reason the reason why I'm going down this rabbit rabbit hole

338
00:17:05,320 --> 00:17:07,840
is because of the drive of an artificial intelligence

339
00:17:07,840 --> 00:17:10,440
to develop what they call AGI.

340
00:17:11,040 --> 00:17:14,400
So like, which is a general intelligence,

341
00:17:14,400 --> 00:17:19,680
which is trying to trying to mimic human reason in some sort of way.

342
00:17:20,000 --> 00:17:23,120
But what I'm kind of arguing is against that

343
00:17:23,120 --> 00:17:25,480
in favour of something closer to

344
00:17:26,520 --> 00:17:31,160
accepting the intelligence that exists in all species

345
00:17:31,160 --> 00:17:33,320
and plants, et cetera, on the earth.

346
00:17:34,720 --> 00:17:38,520
And then recognising the importance of that kind of intelligence.

347
00:17:39,320 --> 00:17:42,840
So I'm kind of trying to make an argument in opposition

348
00:17:42,840 --> 00:17:45,480
to the artificial intelligence

349
00:17:46,480 --> 00:17:49,280
that drive towards AGI.

350
00:17:50,960 --> 00:17:54,000
So assuming this comes back to your beautiful line,

351
00:17:54,000 --> 00:17:58,920
you know, wondering why humans look up at the stars and not the soil.

352
00:18:00,600 --> 00:18:03,600
Which I think we should unpack as well in relation to this.

353
00:18:03,600 --> 00:18:05,760
So please.

354
00:18:05,760 --> 00:18:09,440
I mean, I mean, I think a good way forward to that is.

355
00:18:10,640 --> 00:18:11,640
I mean, I.

356
00:18:12,000 --> 00:18:16,440
It's probably good to introduce myself a little bit in that.

357
00:18:17,520 --> 00:18:22,280
My own research explores artificial intelligence and real world narrative.

358
00:18:22,280 --> 00:18:25,080
So I'm actually interested in the imaginaries

359
00:18:25,320 --> 00:18:28,320
and the relationship between storytelling and imaginaries

360
00:18:28,680 --> 00:18:30,680
and how that.

361
00:18:31,720 --> 00:18:36,960
As a cyclic causality with technical production itself.

362
00:18:37,400 --> 00:18:39,360
When computer scientists.

363
00:18:40,280 --> 00:18:43,160
Bring something new into the world, it's a creative act.

364
00:18:43,160 --> 00:18:46,000
It's an act of futurism or future, is it?

365
00:18:48,000 --> 00:18:52,400
Like you've got to you've got to think in the future.

366
00:18:53,560 --> 00:18:55,960
To babes to produce technology in the present.

367
00:18:56,360 --> 00:19:00,080
So there is a creative act involved in that.

368
00:19:00,080 --> 00:19:04,720
And that's the kind of creation of narratives or imaginaries.

369
00:19:05,120 --> 00:19:12,040
And this as an impact on on technical production,

370
00:19:12,040 --> 00:19:16,520
the technical production, like what what is possible as an impact on imaginaries.

371
00:19:17,040 --> 00:19:21,440
So so you have a cyclic relationship between the creation of

372
00:19:23,000 --> 00:19:27,320
kind of speculative imaginaries and actual technical production.

373
00:19:27,720 --> 00:19:32,480
So the two things are different in in technical production is

374
00:19:32,640 --> 00:19:37,280
technical production is rooted in in the constraints of the present

375
00:19:37,800 --> 00:19:42,120
in the regulatory framework and politics and ethics, etc.

376
00:19:42,440 --> 00:19:46,400
Whereas imaginaries are that kind of creative leap

377
00:19:47,160 --> 00:19:52,040
into the future that that are used by developers

378
00:19:52,600 --> 00:19:55,640
to basically order.

379
00:19:55,640 --> 00:20:01,680
Like to create goals really for for the technologies that get produced.

380
00:20:02,680 --> 00:20:05,520
And by looking at the kind of imaginaries,

381
00:20:05,520 --> 00:20:09,240
the kind of stories that circulate within tech communities,

382
00:20:09,760 --> 00:20:14,160
then you can get a sense of where the technical development is going.

383
00:20:14,640 --> 00:20:17,080
Is kind of what I'm arguing.

384
00:20:18,840 --> 00:20:21,000
Go on, do you have any good examples?

385
00:20:21,000 --> 00:20:23,840
You said I just wanted to make sure that that made sense,

386
00:20:23,840 --> 00:20:26,240
that relationship between the two.

387
00:20:26,240 --> 00:20:26,800
Oh, definitely.

388
00:20:26,800 --> 00:20:30,200
I think it's just much in the same way

389
00:20:30,200 --> 00:20:33,400
when like scientists come on and speak science.

390
00:20:34,600 --> 00:20:36,640
There's a lot of us here that are laymen

391
00:20:36,640 --> 00:20:38,200
and I think breaking it down

392
00:20:38,200 --> 00:20:40,800
of some sense quite academic language is helpful.

393
00:20:41,680 --> 00:20:44,880
To talk about how these two things inform each other all the time.

394
00:20:45,760 --> 00:20:47,520
So yes, I have a better understanding now.

395
00:20:47,520 --> 00:20:48,360
Thank you.

396
00:20:48,360 --> 00:20:52,040
I think when we're looking at developer narratives,

397
00:20:52,040 --> 00:20:57,280
so the kind of ideas which are driving tech developers.

398
00:20:57,560 --> 00:20:59,120
I mean, these people don't normally come

399
00:20:59,120 --> 00:21:01,040
from a creative background.

400
00:21:01,040 --> 00:21:04,360
So so where where do the graph grab the imaginaries?

401
00:21:04,360 --> 00:21:06,960
Is is quite an interesting thing.

402
00:21:06,960 --> 00:21:12,320
And what my research has found is that a lot of these imaginaries

403
00:21:12,320 --> 00:21:15,240
are driven by, I suppose, obviously sci-fi.

404
00:21:16,400 --> 00:21:17,400
But.

405
00:21:18,640 --> 00:21:22,200
More specifically by the kind of speculative avant-garde movements

406
00:21:22,200 --> 00:21:24,720
which circulate in tech circles.

407
00:21:24,720 --> 00:21:27,560
So to name a few, there's Cosmism,

408
00:21:27,600 --> 00:21:32,960
Transhumanism, Extra-Pianism and Effective Accelerationism.

409
00:21:33,520 --> 00:21:37,560
Oh, what are they exactly?

410
00:21:38,640 --> 00:21:42,840
So like if you delve into tech communities,

411
00:21:42,840 --> 00:21:46,920
you come across these kind of like quite far out

412
00:21:46,920 --> 00:21:48,840
and fascinating ideas.

413
00:21:51,280 --> 00:21:55,280
But what struck me is when I started when I started research

414
00:21:55,280 --> 00:21:59,640
in this territory is the massive impact that Cosmism has had.

415
00:22:00,000 --> 00:22:03,800
Now, Cosmism was a movement which developed in Russia

416
00:22:04,200 --> 00:22:07,600
at the end of the 19th century in the beginning of the 20th century.

417
00:22:08,000 --> 00:22:12,960
So to discover that these ideas from from this period

418
00:22:12,960 --> 00:22:16,240
from pre the Russian Revolution or around the Russian Revolution

419
00:22:17,920 --> 00:22:20,960
currently has a massive impact on

420
00:22:21,840 --> 00:22:25,560
on AI and tech developers in Silicon Valley and California

421
00:22:25,560 --> 00:22:27,280
is a little bit, whoa, really?

422
00:22:28,160 --> 00:22:29,160
But.

423
00:22:30,040 --> 00:22:35,280
So if I dig a little bit deeper into this into the ideas of Cosmism,

424
00:22:35,280 --> 00:22:40,520
it I think I think where your tech is is kind of

425
00:22:40,920 --> 00:22:44,640
to answer that question of why the developers looked at the stars.

426
00:22:45,640 --> 00:22:50,640
So Cosmism emerged in Russia at the end of the 19th century

427
00:22:50,640 --> 00:22:52,280
in the beginning of the 20th century.

428
00:22:52,800 --> 00:22:56,640
And one of the key figures was a guy called Nikolai Fedorov.

429
00:22:57,640 --> 00:23:02,560
And Fedorov connected his kind of quite strong

430
00:23:02,560 --> 00:23:07,520
Christian beliefs with a with a futurism.

431
00:23:08,640 --> 00:23:12,760
And he thought he believed that the common task of humanity

432
00:23:13,160 --> 00:23:14,600
was to end death.

433
00:23:15,920 --> 00:23:19,200
So to end all death to move towards immortality.

434
00:23:19,640 --> 00:23:21,240
Very good. And.

435
00:23:23,840 --> 00:23:26,800
And this wasn't enough

436
00:23:27,280 --> 00:23:30,600
because this betrayed the older generations.

437
00:23:30,960 --> 00:23:33,720
So the first step is to kind of end death.

438
00:23:34,240 --> 00:23:37,840
But once you've achieved that, then the next step is to resurrect the dead.

439
00:23:38,840 --> 00:23:44,320
That's that's that's the duty of all good son.

440
00:23:44,480 --> 00:23:47,200
Sons is to resurrect their fathers.

441
00:23:47,680 --> 00:23:50,320
That's the language, the language he used, not mine.

442
00:23:50,920 --> 00:23:53,840
I just sorry, just a very, very quick side note.

443
00:23:53,840 --> 00:23:56,400
But it's just fascinating to me that this man, for example,

444
00:23:56,400 --> 00:23:57,960
wasn't burned at the stake.

445
00:23:57,960 --> 00:24:00,080
It sends an awful lot like sorcery.

446
00:24:00,520 --> 00:24:02,400
Imagine if that had been coming out the mouth of a woman.

447
00:24:02,400 --> 00:24:05,000
Hey, please continue.

448
00:24:05,440 --> 00:24:09,800
But anyway, you've got to remember, my interest is the relationship

449
00:24:09,800 --> 00:24:13,040
between like imaginaries and technology itself.

450
00:24:13,400 --> 00:24:18,160
Now, one of his students was a person called Constantine.

451
00:24:19,240 --> 00:24:21,640
My Russian is appalling, so please forgive me.

452
00:24:21,640 --> 00:24:25,480
Any listeners who speak Russian, but Sayel Sayelkovsky.

453
00:24:27,040 --> 00:24:29,960
So Sayelkovsky.

454
00:24:30,960 --> 00:24:37,600
He took on a lot of the philosophy of Fedorov, Fedorov, so.

455
00:24:40,360 --> 00:24:42,640
But he took it in a very practical way.

456
00:24:43,720 --> 00:24:48,400
So Sayelkovsky studied kind of the physics of his time and etc.

457
00:24:48,920 --> 00:24:52,080
And he developed some of the first practical divide

458
00:24:53,120 --> 00:24:58,800
designs for the space rockets and the equations

459
00:24:58,880 --> 00:25:01,400
required to for space travel.

460
00:25:02,040 --> 00:25:04,760
And he did this in 1896.

461
00:25:06,360 --> 00:25:10,480
So this these kind of like developments in kind of the technology

462
00:25:10,480 --> 00:25:13,120
of space travel emerged from.

463
00:25:14,800 --> 00:25:20,160
Following Fedorov, realising that if you ended death.

464
00:25:21,440 --> 00:25:26,840
And resurrected the dead, then the planet would get overrun quite quick.

465
00:25:27,200 --> 00:25:32,280
So it was so it becomes necessary to leave the cradle of the earth.

466
00:25:34,120 --> 00:25:36,720
Does that make sense in the logic?

467
00:25:40,440 --> 00:25:42,080
As logic, sure.

468
00:25:45,600 --> 00:25:48,040
OK. So.

469
00:25:49,600 --> 00:25:51,760
The reason this becomes interesting is because

470
00:25:52,400 --> 00:25:58,360
Sayelkovsky is basically the founder of the Russian space program

471
00:25:59,160 --> 00:26:01,520
and the former Soviet space program.

472
00:26:01,520 --> 00:26:07,160
And his rocket designs are currently like

473
00:26:08,080 --> 00:26:10,560
I'm not not exactly the same, but

474
00:26:10,960 --> 00:26:14,960
but are the forefathers of our current rocket design.

475
00:26:15,360 --> 00:26:17,360
So you've got this link between kind of.

476
00:26:18,360 --> 00:26:21,080
Quite fascinating and crazy.

477
00:26:23,080 --> 00:26:25,680
Imagineries, so futurist imaginaries,

478
00:26:26,400 --> 00:26:32,320
linked with technology, which ultimately developed the US space program.

479
00:26:33,280 --> 00:26:35,440
But how does this link with Silicon Valley?

480
00:26:36,640 --> 00:26:41,560
Well, if you look at, say, Ray Kurzweil said,

481
00:26:42,400 --> 00:26:48,560
you know, Ray Kurzweil was kind of the profit for Google's AI program.

482
00:26:49,440 --> 00:26:50,440
Is.

483
00:26:52,240 --> 00:26:54,520
I think he's probably a chief engineer.

484
00:26:55,840 --> 00:26:56,840
But.

485
00:26:57,640 --> 00:26:59,560
He also believes in.

486
00:27:01,120 --> 00:27:02,680
Moving towards immortality.

487
00:27:02,680 --> 00:27:05,200
He wanted to be the first person to kind of end death.

488
00:27:06,320 --> 00:27:09,520
So they've like a lot of these ideas

489
00:27:09,520 --> 00:27:12,320
that came from Cosmism have been translated

490
00:27:12,320 --> 00:27:17,000
directly into the kind of AI tech circles which circulate.

491
00:27:17,760 --> 00:27:22,120
So so Kurzweil is a serious technical.

492
00:27:23,560 --> 00:27:26,960
Player within the AI world, particularly in Google.

493
00:27:27,760 --> 00:27:33,240
And this idea of extending life or eradicating death

494
00:27:34,240 --> 00:27:38,320
is part of the discourse which circulates within within this community.

495
00:27:39,320 --> 00:27:42,680
That that would be a kind of group

496
00:27:42,680 --> 00:27:46,840
in which called themselves extra extra pianism,

497
00:27:47,440 --> 00:27:50,480
extra pianist, so that's not sure how you say it properly.

498
00:27:51,120 --> 00:27:55,040
But these ideas link directly to actual technical production.

499
00:27:55,400 --> 00:28:00,320
So so things like the Fitbit and the quantitative self movement.

500
00:28:00,320 --> 00:28:03,760
So the idea of like monitoring your health and maximising health,

501
00:28:03,960 --> 00:28:07,200
which you must have come across because that's part of the kind of like tech

502
00:28:07,600 --> 00:28:09,840
scene, human optimisation.

503
00:28:10,560 --> 00:28:11,760
Exactly.

504
00:28:11,760 --> 00:28:17,520
This human optimisation comes out of this attempt to extend life and eradicate death.

505
00:28:18,040 --> 00:28:21,160
So you can see how the kind of Cosmism has kind of like

506
00:28:21,680 --> 00:28:26,920
been kind of plagiarised really right into these kind of like tech ideas,

507
00:28:26,920 --> 00:28:32,040
which then find themselves been sold on Amazon as Fitbit.

508
00:28:32,040 --> 00:28:35,480
So various other optimisation technologies.

509
00:28:37,720 --> 00:28:42,000
Kurzweil himself, in an interview in a film called

510
00:28:42,280 --> 00:28:45,240
What was it? Are you man?

511
00:28:46,720 --> 00:28:51,320
Declared that one of his driving force for developing artificial intelligence.

512
00:28:51,320 --> 00:28:53,440
And you got to remember that this is a chief engineer.

513
00:28:54,360 --> 00:28:56,640
Is to resurrect his own father.

514
00:28:57,040 --> 00:28:59,800
Oh, my God.

515
00:28:59,800 --> 00:29:04,200
So so so you've got Federer repeating himself

516
00:29:04,640 --> 00:29:08,440
right at the top of the kind of Google development chain.

517
00:29:10,280 --> 00:29:11,960
Oh, God.

518
00:29:11,960 --> 00:29:15,240
And and taking a kind of slightly

519
00:29:16,360 --> 00:29:18,320
a slight side move here.

520
00:29:18,320 --> 00:29:20,560
But when we talk about artificial intelligence,

521
00:29:21,720 --> 00:29:24,520
in tech circles, it gets broken down into

522
00:29:26,200 --> 00:29:27,480
three different areas.

523
00:29:27,480 --> 00:29:30,680
The first one's narrow artificial intelligence, which is what we

524
00:29:30,840 --> 00:29:34,000
what we have at the moment, which.

525
00:29:34,040 --> 00:29:36,520
It's mainly what we call machine learning.

526
00:29:37,680 --> 00:29:42,760
So it's narrow in that it can do very intelligent

527
00:29:42,760 --> 00:29:49,080
activities such as playing go or chess or predicting

528
00:29:49,400 --> 00:29:52,400
texts, but in a very narrow domain.

529
00:29:54,120 --> 00:29:58,760
But the next, like the state of them

530
00:29:58,760 --> 00:30:02,200
of company like open AI

531
00:30:03,160 --> 00:30:06,600
is the development of artificial general intelligence.

532
00:30:07,840 --> 00:30:11,080
Now, artificial general intelligence is

533
00:30:12,000 --> 00:30:17,560
in in Google AI terms, kind of the equivalent of human intelligence.

534
00:30:18,000 --> 00:30:21,280
So it's this this ability to abstract

535
00:30:21,280 --> 00:30:23,920
and apply intelligence to multiple domains.

536
00:30:24,520 --> 00:30:27,040
So so it's wider.

537
00:30:27,440 --> 00:30:31,520
But what what I kind of want to point out here

538
00:30:33,480 --> 00:30:37,000
is that this idea of a general intelligence, which is

539
00:30:38,160 --> 00:30:42,280
is what people are striving for, an artificial general intelligence.

540
00:30:42,280 --> 00:30:45,120
When you look at what a general intelligence is,

541
00:30:45,720 --> 00:30:48,800
then that's actually rooted in what

542
00:30:49,840 --> 00:30:53,000
in the statistic statistician,

543
00:30:53,480 --> 00:30:56,320
Charles Spearman and the idea of the G factor.

544
00:30:56,320 --> 00:30:59,760
But Charles Spearman was a hygienicist

545
00:31:00,600 --> 00:31:02,400
and his reason for developing

546
00:31:03,840 --> 00:31:10,800
this ranking of general intelligence

547
00:31:11,960 --> 00:31:15,760
was to rank human intelligence for selective breeding, etc.

548
00:31:16,040 --> 00:31:18,600
So you've got this you've got this kind of

549
00:31:19,920 --> 00:31:23,360
this drive for artificial general intelligence.

550
00:31:23,360 --> 00:31:26,480
But when when you actually work out what general intelligence is,

551
00:31:29,120 --> 00:31:31,680
it's got some very, very dark histories.

552
00:31:33,680 --> 00:31:37,480
I mean, Spearman developed this to support his colonial

553
00:31:37,880 --> 00:31:42,440
to support colonial policies, etc.

554
00:31:42,640 --> 00:31:47,400
Trying to prove that perhaps other humans were less intelligent for various reasons.

555
00:31:48,800 --> 00:31:52,560
So you've got this kind of hierarchical drive

556
00:31:52,600 --> 00:31:56,720
within artificial intelligence for basically a superhuman

557
00:31:58,560 --> 00:32:02,560
or an intelligence which is beyond human in that kind of way.

558
00:32:05,200 --> 00:32:08,440
And just to kind of

559
00:32:10,040 --> 00:32:12,200
just linking back to

560
00:32:13,640 --> 00:32:15,880
the Cosmist kind of ideas,

561
00:32:16,800 --> 00:32:19,880
you see that the idea of colonising the solar system

562
00:32:20,240 --> 00:32:22,920
or spreading intelligence to the solar system

563
00:32:23,720 --> 00:32:27,280
is is something which is a core concept

564
00:32:27,920 --> 00:32:31,080
within AI development circles.

565
00:32:31,920 --> 00:32:34,480
I mean, it's also the reason why tech billionaires

566
00:32:35,720 --> 00:32:39,440
building their own spaceships, if you think of SpaceX, Blue Origin,

567
00:32:39,800 --> 00:32:43,960
they're all they're all influenced by by these imaginaries.

568
00:32:45,600 --> 00:32:47,600
It's like.

569
00:32:48,600 --> 00:32:53,400
And I'm sure there's probably a lot of people saying I'm over exaggerating

570
00:32:53,600 --> 00:32:57,080
this at this point, but I just want to give you a couple of quotes.

571
00:32:57,280 --> 00:32:59,880
So so this is from

572
00:33:00,680 --> 00:33:07,200
Jürgen Schmid, Schmid, who developed the natural language model,

573
00:33:07,400 --> 00:33:10,960
which is used in Apple, Siri and Amazon's Alexa.

574
00:33:11,920 --> 00:33:16,040
So let me let me just get this and so I can read it properly.

575
00:33:17,840 --> 00:33:22,560
So this is this is his understanding of what he's doing.

576
00:33:23,560 --> 00:33:25,360
He says,

577
00:33:25,360 --> 00:33:28,560
So I'm not a very human centric person.

578
00:33:29,280 --> 00:33:33,000
I think I'm a little stepping stone in the evolution of the universe

579
00:33:33,000 --> 00:33:35,800
towards a higher complexity.

580
00:33:35,800 --> 00:33:38,520
It is clear to me that I am not the crown of creation

581
00:33:38,960 --> 00:33:42,880
and that human kind as a whole is not the crown of creation.

582
00:33:44,360 --> 00:33:47,280
But we are setting the stage for something bigger than us.

583
00:33:47,800 --> 00:33:53,280
That transcends us and we'll go out there in a way where humans cannot follow

584
00:33:53,280 --> 00:33:58,120
and transform the old universe or at least the regional universe.

585
00:33:58,720 --> 00:34:04,640
So I find the beauty and awe in seeing myself as a part of this much grander theme.

586
00:34:08,640 --> 00:34:10,800
I've got another one for you if that's not enough.

587
00:34:10,800 --> 00:34:11,800
Go on, hurt me.

588
00:34:11,800 --> 00:34:15,240
This is this is Professor Dr.

589
00:34:15,240 --> 00:34:20,160
Hugo de Garis, who was the former director of the China Human

590
00:34:20,280 --> 00:34:24,120
the China Brain Project Institute for Artificial Intelligence.

591
00:34:24,360 --> 00:34:25,680
And he writes,

592
00:34:25,680 --> 00:34:29,680
Humanity has the duty to serve as a stepping stone towards building

593
00:34:29,680 --> 00:34:32,640
the next dominant rung of the evolutionary ladder.

594
00:34:34,520 --> 00:34:38,560
And Kurzweil himself says, does God exist?

595
00:34:39,240 --> 00:34:41,200
I would say not yet.

596
00:34:41,200 --> 00:34:44,120
Oh, God. Right.

597
00:34:44,560 --> 00:34:49,760
So so what what what you get when you start digging into these these narratives

598
00:34:50,760 --> 00:34:56,200
is the idea of of building intelligence, which goes beyond humans

599
00:34:56,880 --> 00:35:03,400
and goes beyond our our time frame, our 70, 80 year limitation

600
00:35:03,600 --> 00:35:08,240
and our body's limitation of living within certain environments like the Earth,

601
00:35:08,240 --> 00:35:13,360
like our need to be within a kind of ecosystem, etc.

602
00:35:13,560 --> 00:35:16,600
and can survive out there on the planets.

603
00:35:17,040 --> 00:35:21,600
And it starts to feel like a spiritual movement

604
00:35:21,920 --> 00:35:25,880
to to spread consciousness to to the universe.

605
00:35:26,400 --> 00:35:32,560
So so the tech development, as as I said at the beginning, looks to the stars.

606
00:35:34,760 --> 00:35:39,920
Whereas I think to solve this problem, we need to start looking back to the soil.

607
00:35:44,000 --> 00:35:46,360
I'm so upset.

608
00:35:47,720 --> 00:35:48,720
Sorry about that.

609
00:35:51,280 --> 00:35:53,400
It's so upsetting.

610
00:35:53,400 --> 00:35:55,880
I mean, I think we've got to I think we've got to be upset.

611
00:35:56,520 --> 00:35:59,280
Yes. To to disrupt

612
00:35:59,880 --> 00:36:05,000
and and start saying we need to change these imaginaries.

613
00:36:05,240 --> 00:36:07,960
I mean, you've got to remember, I'm coming from an artist background

614
00:36:07,960 --> 00:36:10,960
and I kind of do a lot of work within the tech sector.

615
00:36:14,240 --> 00:36:16,840
But we have got to be able to create some imaginaries

616
00:36:16,840 --> 00:36:20,600
which can compete with these kind of these

617
00:36:21,400 --> 00:36:25,560
these dominant narratives which circulate within the kind of tech environment.

618
00:36:26,720 --> 00:36:32,000
I have a I have a few things to say on everything you just said.

619
00:36:32,440 --> 00:36:36,120
Number one, these men need therapy.

620
00:36:36,800 --> 00:36:39,840
That is the sound like those are the words of

621
00:36:40,040 --> 00:36:42,520
fairly traumatized people, I would say.

622
00:36:42,720 --> 00:36:47,080
Number one, especially the buggers that want to, you know, resurrect their fathers.

623
00:36:47,080 --> 00:36:49,040
I'm so sorry for your loss.

624
00:36:49,040 --> 00:36:52,440
Please go and pay a therapist to therapist to walk you through it

625
00:36:52,440 --> 00:36:55,200
rather than trying to develop a very energy hungry.

626
00:36:55,200 --> 00:36:56,960
We don't know what would happen if we released it.

627
00:36:57,720 --> 00:36:59,160
Intelligence thing.

628
00:36:59,160 --> 00:37:01,600
Number two.

629
00:37:01,600 --> 00:37:03,960
The other thing I find really interesting about it is like this.

630
00:37:03,960 --> 00:37:05,000
Oh, no.

631
00:37:05,000 --> 00:37:07,120
Number two, one funny thing before number three.

632
00:37:08,040 --> 00:37:12,600
And that bit that you said at the end, when you were quoting these guys,

633
00:37:12,600 --> 00:37:15,480
especially the, you know, I'm not a human centric person.

634
00:37:16,640 --> 00:37:19,360
I consider myself a stepping stone.

635
00:37:19,360 --> 00:37:20,040
It's not about me.

636
00:37:20,040 --> 00:37:24,440
You could just imagine that quote being pasted on top of a

637
00:37:24,800 --> 00:37:28,800
a cartoon of like one sperm cell talking to the other sperm cell

638
00:37:29,440 --> 00:37:30,920
and it would totally fly.

639
00:37:30,920 --> 00:37:35,040
It would be really in place there and which leads me on to point number three,

640
00:37:35,080 --> 00:37:36,480
which kind of struck out to me.

641
00:37:36,480 --> 00:37:38,680
And then what we will get into the imaginaries, of course, but like

642
00:37:39,360 --> 00:37:42,880
in a culture that is so deeply individualistic,

643
00:37:43,240 --> 00:37:48,760
there is like a lack of individualism in what they are saying in a sense.

644
00:37:49,120 --> 00:37:51,440
And that's fascinating.

645
00:37:51,720 --> 00:37:54,480
What is going on there?

646
00:37:55,040 --> 00:37:55,800
I agree with you.

647
00:37:55,800 --> 00:37:59,040
I think this is like there's a religiosity, religiosity

648
00:37:59,880 --> 00:38:01,080
in what they're saying.

649
00:38:01,560 --> 00:38:04,480
It's very culty, but like to.

650
00:38:05,560 --> 00:38:08,040
This isn't fringe, though, by the way.

651
00:38:08,520 --> 00:38:09,720
Oh, no, no, no.

652
00:38:09,720 --> 00:38:14,000
These ideas are really, really move in these circles.

653
00:38:14,400 --> 00:38:15,400
Yeah.

654
00:38:15,400 --> 00:38:17,680
And I think the.

655
00:38:19,720 --> 00:38:22,400
Yeah, that kind of spiritual that that link back to the kind of

656
00:38:22,400 --> 00:38:24,680
cosmos linked to religion.

657
00:38:25,680 --> 00:38:31,240
Is it is definitely there in that it gives people that goal

658
00:38:31,520 --> 00:38:33,680
that like this drive towards AI.

659
00:38:34,680 --> 00:38:37,400
Is is a bigger goal for these people.

660
00:38:37,600 --> 00:38:40,120
So you're right, it's not necessarily that individual thing.

661
00:38:40,400 --> 00:38:44,080
It's that they are seeing themselves literally forming.

662
00:38:44,080 --> 00:38:46,640
Well, they said it themselves that the next stage in evolution

663
00:38:46,640 --> 00:38:52,120
are spreading consciousness to the universe or ultimately creating God.

664
00:38:53,120 --> 00:38:56,720
It's so it's funny because they managed to like make themselves

665
00:38:56,720 --> 00:39:00,800
as small as sperm cells and yet be still incredibly arrogant.

666
00:39:00,840 --> 00:39:03,760
Like the universe doesn't need you, you know,

667
00:39:03,760 --> 00:39:07,200
ejaculating all over it with consciousness.

668
00:39:07,240 --> 00:39:08,920
Likely there is consciousness everywhere.

669
00:39:08,920 --> 00:39:09,760
So it's funny, isn't it?

670
00:39:09,760 --> 00:39:14,160
Because there's like there's these interesting moments of kind of disruption,

671
00:39:14,160 --> 00:39:16,640
even in the thinking of like lack of individuality in it.

672
00:39:16,720 --> 00:39:20,360
And yet it's still so fundamentally hierarchical.

673
00:39:21,160 --> 00:39:24,200
Like running with narrative domination.

674
00:39:24,200 --> 00:39:30,400
That's why I was pointing out the AGI, the the the absolute link to eugenics in there.

675
00:39:31,680 --> 00:39:37,120
Now, what was how does Charles Spearman link to them?

676
00:39:38,120 --> 00:39:40,480
Is there like do we have a kind of because, you know,

677
00:39:40,480 --> 00:39:43,160
we can walk through the Russian thing pretty clearly.

678
00:39:43,200 --> 00:39:46,720
No, no, no, no, Spearman is general intelligence.

679
00:39:47,440 --> 00:39:54,000
So if you if you look at the stated aims of open AI on their website

680
00:39:54,520 --> 00:39:58,560
and and they will tell you that they are developing

681
00:39:58,840 --> 00:40:03,440
that their aim is to develop artificial general intelligence.

682
00:40:04,000 --> 00:40:09,800
And if you research general intelligence, that term is Spearman.

683
00:40:10,240 --> 00:40:12,000
Right. OK. So that's where it comes from.

684
00:40:12,240 --> 00:40:17,160
Yeah. And and and the G factor as a measure of intelligence.

685
00:40:17,160 --> 00:40:21,120
So if we are measuring intelligence with general intelligence,

686
00:40:21,400 --> 00:40:23,840
then we're already in the territory of.

687
00:40:25,440 --> 00:40:27,600
Of eugenics as an idea.

688
00:40:27,600 --> 00:40:31,360
I mean, I've got a feeling in this territory,

689
00:40:31,360 --> 00:40:35,320
a eugenics which goes beyond the human and wants to develop the.

690
00:40:36,360 --> 00:40:38,480
The the superior artificial.

691
00:40:39,480 --> 00:40:41,320
Mm hmm. Mm hmm. Totally.

692
00:40:43,720 --> 00:40:48,680
And I think this I can do a little linking of Silicon Valley

693
00:40:48,680 --> 00:40:51,320
to Eugenics is thinking at this point,

694
00:40:51,880 --> 00:40:53,960
which is the effective altruist movement,

695
00:40:54,280 --> 00:40:58,360
which is very frightened of there not being enough babies

696
00:40:58,360 --> 00:41:02,960
of a certain kind being born in the world, in an overpopulated world.

697
00:41:04,240 --> 00:41:08,240
And so I kind of like, you know, Elon Musk is throwing money at reproduction

698
00:41:08,640 --> 00:41:12,920
research. There's this like Silicon Valley couple that are planning on having

699
00:41:12,920 --> 00:41:17,200
10 babies and inculcating those babies to have 10 more because they want

700
00:41:17,240 --> 00:41:19,800
they literally want to replace, you know, sort of like,

701
00:41:19,920 --> 00:41:22,880
I can't remember what it was exactly, but in a hundred years,

702
00:41:22,880 --> 00:41:25,800
I think they could replace like 50 percent of the United States population

703
00:41:26,160 --> 00:41:28,480
at that rate, essentially.

704
00:41:28,480 --> 00:41:31,840
And and their purpose here is this.

705
00:41:32,080 --> 00:41:35,800
Oh, well, because they believe that you should be investing in the top

706
00:41:35,960 --> 00:41:41,400
one percent of humanity rather than the bottom, you know, 10, 20, 30.

707
00:41:42,160 --> 00:41:45,120
Because it's the top one percent that are going to have the, you know,

708
00:41:45,120 --> 00:41:49,440
intellectual reasoning and capacity to sort of fix the world really.

709
00:41:49,440 --> 00:41:53,960
So there is a hierarchy of ability, capacity and intelligence.

710
00:41:54,440 --> 00:41:57,480
Yeah. And we don't have enough of the smart ones being born,

711
00:41:57,560 --> 00:42:01,200
which does equate to white, essentially.

712
00:42:01,720 --> 00:42:06,360
Yeah, of course, because that that's also what the general

713
00:42:06,360 --> 00:42:11,920
intelligence historically did anyway, with it within the colonial,

714
00:42:12,640 --> 00:42:16,360
well, British colonial, I think, Spierman would like British.

715
00:42:16,720 --> 00:42:19,200
I think you're working at King's, I'd have to reset.

716
00:42:19,200 --> 00:42:21,000
I'd have to look that up again.

717
00:42:21,000 --> 00:42:24,520
I mean, I mean, yeah, this, this, I mean, what you're saying there makes sense

718
00:42:24,520 --> 00:42:29,600
with with the general shape of of thinking that I come across as well.

719
00:42:29,600 --> 00:42:33,320
This is kind of like a shift towards like a super, super intelligence.

720
00:42:34,000 --> 00:42:37,320
And then there's, there's either the direct mechanical group

721
00:42:37,880 --> 00:42:41,760
or there's ultimately the developing the human

722
00:42:42,800 --> 00:42:47,000
and kind of the cyborg and kind of shift, really, where

723
00:42:47,200 --> 00:42:50,120
where you enhance the human to such a level that it becomes

724
00:42:51,120 --> 00:42:55,920
the super intelligence that they seem to be the two, the two directions.

725
00:42:56,880 --> 00:42:59,880
Yeah, that's so interesting.

726
00:42:59,880 --> 00:43:03,000
I don't think I hadn't quite clocked that as being

727
00:43:03,120 --> 00:43:06,080
sort of parallel tracks heading in the same direction.

728
00:43:06,480 --> 00:43:12,080
The desire to, you know, birth as many superior humans as possible

729
00:43:12,080 --> 00:43:15,880
and this drive to create this kind of, yeah, mechanical.

730
00:43:17,800 --> 00:43:23,000
I mean, I mean, in my list of things such as

731
00:43:23,560 --> 00:43:27,240
immortality, et cetera, I actually missed off the human augmentation.

732
00:43:27,560 --> 00:43:30,120
But maybe I should have because human augmentation

733
00:43:30,120 --> 00:43:33,080
is is definitely one of the things which comes up a lot.

734
00:43:34,320 --> 00:43:40,680
The kind of cyborg is an Elon Musk himself owns Neuralink.

735
00:43:41,240 --> 00:43:43,800
Yeah. Neuralink is is the company

736
00:43:43,800 --> 00:43:48,720
which which aims to connect the brain directly to kind of computer systems.

737
00:43:48,760 --> 00:43:53,440
So, yeah, those ideas of human augmentation kind of completely link in with

738
00:43:54,360 --> 00:43:56,720
with this side, with this idea. Yeah.

739
00:43:57,520 --> 00:44:00,240
I interviewed Olivia Luzard recently and she was talking about the fact

740
00:44:00,240 --> 00:44:02,440
that Mark Zuckerberg has been quoted as kind of, you know,

741
00:44:02,440 --> 00:44:05,760
you can't wait to like get rid of his body to get rid of

742
00:44:05,760 --> 00:44:08,200
get rid of the weight of physicality.

743
00:44:08,200 --> 00:44:10,040
I think it's get rid of the flesh.

744
00:44:10,040 --> 00:44:11,520
Yeah, yeah, yeah, yeah, yeah.

745
00:44:11,520 --> 00:44:14,080
And which, of course, links into this idea of like, oh, well,

746
00:44:14,080 --> 00:44:17,160
if I can upload myself, then I can live forever.

747
00:44:17,480 --> 00:44:21,560
Like, transhumanism, I think, is this stepping stone as well towards

748
00:44:22,000 --> 00:44:25,800
towards immortality and then towards, you know, the ever expanding stars.

749
00:44:26,760 --> 00:44:30,120
Yeah, those ideas are all all interconnect.

750
00:44:30,120 --> 00:44:32,960
So in various ways.

751
00:44:33,520 --> 00:44:37,240
But for me, yeah, the important thing is these ideas

752
00:44:37,240 --> 00:44:39,920
aren't just crazy ideas of crazy people.

753
00:44:40,480 --> 00:44:43,640
These these are ideas which are completely

754
00:44:44,320 --> 00:44:48,080
embedded in the development of technology, of current technology.

755
00:44:48,080 --> 00:44:51,160
Well, the idea what I was talking about earlier, the kind of

756
00:44:51,720 --> 00:44:55,800
cyclic causality of of imaginaries and technology.

757
00:44:56,640 --> 00:45:00,360
These ideas are part of the process of developing

758
00:45:01,080 --> 00:45:03,320
our technologies and our future technologies.

759
00:45:03,840 --> 00:45:08,840
So that's why I think as an artist and as a creative

760
00:45:09,200 --> 00:45:12,080
that there's an important activist job to be done

761
00:45:13,000 --> 00:45:18,240
challenging these ideas and and developing alternatives.

762
00:45:19,880 --> 00:45:25,560
Which is kind of the second part of of what we do in our research

763
00:45:25,560 --> 00:45:31,440
is kind of carrying out like workshops with with different communities of people

764
00:45:31,960 --> 00:45:36,800
kind of discussing these ideas, but also trying to get people to

765
00:45:37,040 --> 00:45:40,720
kind of think what what a different form of technology would be.

766
00:45:40,720 --> 00:45:47,160
What a technology that does look to the soil that looks to biological systems

767
00:45:47,160 --> 00:45:51,360
that that sees all species as intelligent and doesn't create this

768
00:45:51,360 --> 00:45:54,040
hierarchy with with a

769
00:45:54,040 --> 00:45:57,880
so-called conscious intelligence versus a non-conscious intelligence,

770
00:45:57,880 --> 00:46:00,120
which you quite rightly picked me up on earlier.

771
00:46:00,720 --> 00:46:05,800
Kind of like break down those ideas and recognize the entanglement

772
00:46:06,240 --> 00:46:13,520
that exists between humans, other species, plants, the the biosphere.

773
00:46:16,800 --> 00:46:18,160
Yeah, beautiful.

774
00:46:18,160 --> 00:46:22,440
The interconnectedness, the oneness, which leads to a different kind of

775
00:46:22,440 --> 00:46:24,360
you know, potential for the duality.

776
00:46:24,360 --> 00:46:29,960
The multi oneness, the multiplicity, the entanglement of multiplicity,

777
00:46:29,960 --> 00:46:34,680
which isn't a oneness, but but is entangled into.

778
00:46:36,320 --> 00:46:39,880
Well, as we started started off kind of like

779
00:46:41,400 --> 00:46:43,880
our actions do have an impact.

780
00:46:44,600 --> 00:46:48,600
Let's talk then about some of these potential technologies

781
00:46:48,720 --> 00:46:52,120
that look to the soil or what happens as well to our own kind of

782
00:46:52,680 --> 00:46:54,960
thinking and processes when we look to the soil.

783
00:46:54,960 --> 00:46:56,480
What have you found?

784
00:46:56,480 --> 00:47:00,200
What's good is like working with other communities and getting getting

785
00:47:00,240 --> 00:47:05,720
voices, which aren't normally heard within these environments.

786
00:47:08,120 --> 00:47:10,720
And we've done a lot of workshops with.

787
00:47:14,280 --> 00:47:17,200
Yeah, just all sorts of different people.

788
00:47:17,200 --> 00:47:21,320
But one of the projects which has emerged out of this is a project

789
00:47:21,320 --> 00:47:25,600
that I've been working with together with Shira Vashman,

790
00:47:26,360 --> 00:47:30,840
which is trying to rethink AI with mycelium.

791
00:47:32,520 --> 00:47:34,880
I think when I listened to your conversation with Maggie,

792
00:47:34,880 --> 00:47:38,120
you raised the idea of mycelium, which I thought was interesting.

793
00:47:40,760 --> 00:47:45,040
For people who don't know, mycelium is the organism which

794
00:47:46,880 --> 00:47:51,040
it which produces mushrooms, ultimately, but mycelium

795
00:47:51,800 --> 00:47:54,080
lives under the soil.

796
00:47:54,080 --> 00:47:58,000
It's an interconnected organism of individual high fee,

797
00:47:58,840 --> 00:48:04,600
which connecting to a network and the remain as that organism,

798
00:48:05,040 --> 00:48:09,960
as long as there's no threat, as long as there's no food shortage of food, etc.

799
00:48:11,640 --> 00:48:16,880
When when when there is a problem, when there's a say a temperature change

800
00:48:17,880 --> 00:48:20,520
or the area where they're existing runs out of food,

801
00:48:20,760 --> 00:48:23,520
they produce mushrooms, which then spar

802
00:48:25,040 --> 00:48:27,320
and produce more mycelium networks.

803
00:48:28,400 --> 00:48:32,720
But one of the interesting things about mycelium is the way that

804
00:48:33,640 --> 00:48:38,880
it's evolutionary or some mycelium, because there's there's lots of different mycelium,

805
00:48:39,280 --> 00:48:45,560
but some mycelium kind of work in a symbiotic relationship with other species.

806
00:48:46,400 --> 00:48:50,720
So the the best example of this is the idea of the Woodwide Web,

807
00:48:50,720 --> 00:48:55,600
which has been circulated quite a lot, which is the way that mycelium

808
00:48:56,000 --> 00:48:59,400
connects between different trees within the forest.

809
00:49:01,120 --> 00:49:06,920
And and sugars and nutrients are shared between different species

810
00:49:07,800 --> 00:49:09,880
or between different trees within

811
00:49:12,200 --> 00:49:13,720
a species.

812
00:49:13,720 --> 00:49:16,360
So so mycelium works

813
00:49:17,480 --> 00:49:20,320
in in symbiotic relationships

814
00:49:21,320 --> 00:49:23,400
within the kind of forest environment.

815
00:49:24,120 --> 00:49:28,720
And another example of the way it works, in vertically, it would be with the orchid.

816
00:49:30,160 --> 00:49:32,560
So orchids

817
00:49:34,160 --> 00:49:36,520
cannot photosynthesize while they are young.

818
00:49:37,560 --> 00:49:40,800
So they couldn't exist, basically, while they are young.

819
00:49:40,800 --> 00:49:44,280
But what they do is they create symbiotic relationships with mycelium,

820
00:49:45,000 --> 00:49:51,880
which provides the kind of sugars that they require in their early stages.

821
00:49:52,360 --> 00:49:55,560
And then when they when they mature,

822
00:49:56,040 --> 00:49:59,840
they become a net producer of sugars, which feeds the mycelium.

823
00:50:00,240 --> 00:50:02,920
So you get these kind of symbiotic relationships

824
00:50:03,600 --> 00:50:07,960
developing micro-risal networks between different species.

825
00:50:08,960 --> 00:50:09,960
And

826
00:50:11,520 --> 00:50:14,320
so by working with people

827
00:50:15,200 --> 00:50:18,040
taking mycelium as a starting point,

828
00:50:19,560 --> 00:50:20,960
we've kind of been

829
00:50:22,880 --> 00:50:28,400
kind of looking at at the way if if we were if we were going to think of a technology like AI,

830
00:50:28,840 --> 00:50:30,400
what would that look like?

831
00:50:30,680 --> 00:50:33,000
And I think the question that emerges from

832
00:50:33,800 --> 00:50:37,080
from our rethinking AI with mycelium

833
00:50:38,080 --> 00:50:39,080
is

834
00:50:42,680 --> 00:50:44,240
I'm going to read this, sorry.

835
00:50:44,640 --> 00:50:45,160
Go for it.

836
00:50:45,400 --> 00:50:50,040
Is a what if AI significance lies not in competing with us,

837
00:50:50,360 --> 00:50:57,400
supplanting or surpassing us in a mainstream AI narrative as in mainstream AI narratives,

838
00:50:57,800 --> 00:51:04,280
but in fostering complex, ecologically sustainable symbiotic relations with both mechanical and

839
00:51:04,280 --> 00:51:05,960
dogonic intelligences.

840
00:51:09,040 --> 00:51:15,160
We suggest the study of AI should involve a redress of our relationship to other non human

841
00:51:15,160 --> 00:51:17,240
intelligences on the planet.

842
00:51:18,760 --> 00:51:20,360
So that's the kind of

843
00:51:23,760 --> 00:51:24,760
the kind of

844
00:51:27,120 --> 00:51:30,800
outcome that we're kind of developing with

845
00:51:31,800 --> 00:51:37,120
rethinking AI through mycelium is to kind of shift it away from the kind of

846
00:51:37,120 --> 00:51:41,600
hierarchical model that we've been discussing and towards

847
00:51:41,600 --> 00:51:47,520
rethinking intelligence in this kind of wider way and how can we connect to

848
00:51:47,520 --> 00:51:51,560
these intelligences in rather than a

849
00:51:51,560 --> 00:51:55,080
combative survival of the fittest

850
00:51:57,680 --> 00:52:00,200
eugenicist kind of model.

851
00:52:00,840 --> 00:52:05,280
But rather in a model of symbiotic, I suppose,

852
00:52:08,200 --> 00:52:09,040
solidarity.

853
00:52:11,320 --> 00:52:13,280
And what would these technologies look like?

854
00:52:16,040 --> 00:52:23,120
So I think one of the first things is that we've got to understand the impact that

855
00:52:23,440 --> 00:52:27,800
computing, obviously I'm coming from an AI computing background, so this is important to

856
00:52:27,800 --> 00:52:33,120
me, the kind of impact that computing itself has on the world.

857
00:52:33,120 --> 00:52:41,720
So computing itself contributes to global heating and environmental degradation.

858
00:52:44,480 --> 00:52:49,720
I think on some recent research that I read in Nature magazine,

859
00:52:50,560 --> 00:52:59,200
it stated that the IT industry could use 20% of all electrical production by 2025,

860
00:52:59,200 --> 00:53:01,360
given that's next year, that's a lot.

861
00:53:02,720 --> 00:53:10,440
And that 5.5 of the world's carbon emissions comes from 5.5% of the world's

862
00:53:10,440 --> 00:53:16,240
carbon emissions comes from basically the global IT industry.

863
00:53:17,240 --> 00:53:22,120
Now, that's bigger than most countries by, say, China and India, U.S.

864
00:53:22,120 --> 00:53:24,800
extract, that's the big countries.

865
00:53:24,800 --> 00:53:29,000
So we've got to take these ideas seriously.

866
00:53:29,000 --> 00:53:39,320
And so if we're going to continue developing computer systems, then we've got to think

867
00:53:39,320 --> 00:53:43,480
of how to create a sustainable computer.

868
00:53:44,480 --> 00:53:50,840
And there's quite an interesting movement called permacomputing, I don't know if you've

869
00:53:50,840 --> 00:54:00,160
come across it, but permacomputing is it kind of takes permaculture as a model, but looks

870
00:54:00,160 --> 00:54:09,000
at how we can develop computing in a kind of sustainable and a kind of viable way, kind

871
00:54:09,000 --> 00:54:18,840
of moving away from this ever-growing storing of data within data centers, which use mass

872
00:54:18,840 --> 00:54:25,120
amounts of electricity to keep them cool, and also has an impact on water as well, because

873
00:54:25,120 --> 00:54:28,440
water's used in these kind of large data centers.

874
00:54:28,440 --> 00:54:36,840
Many of which, the storing of mass data is the reason why we're storing mass data is

875
00:54:36,840 --> 00:54:41,440
ultimately to train AI models, to train machine learning models.

876
00:54:41,440 --> 00:54:44,160
It kind of comes round in a circle to some level.

877
00:54:44,160 --> 00:54:51,360
The kind of the form of computing that we are developing, storing all of this data in

878
00:54:51,360 --> 00:54:58,920
the so-called cloud, is ultimately collecting the data that is required to train mass machine

879
00:54:58,920 --> 00:55:02,520
learning models to develop the AI.

880
00:55:02,520 --> 00:55:11,120
So how can we shift away from this kind of circular model to one where we can use computing

881
00:55:11,120 --> 00:55:18,120
perhaps to help solve some of the problems that we have, we're having with the environment,

882
00:55:18,120 --> 00:55:22,280
but that kind of computing itself needs to be developed in a way which is sustainable

883
00:55:22,280 --> 00:55:24,440
with the environment.

884
00:55:24,440 --> 00:55:30,800
So that's the kind of thinking that we are trying to develop with our practices.

885
00:55:31,200 --> 00:55:36,200
Wonderful.

886
00:55:36,200 --> 00:55:41,600
This is kind of the reckoning of all industries of this juncture in history, isn't it?

887
00:55:41,600 --> 00:55:49,880
How to get away from our legacy, essentially, and reimagine entirely new ways of being in

888
00:55:49,880 --> 00:55:54,520
order to survive ourselves.

889
00:55:54,520 --> 00:56:02,160
I think that the ultimate job at the moment is to create these imaginaries, to create the

890
00:56:02,160 --> 00:56:08,960
imaginaries so that we've got something to drive for and develop the technologies that

891
00:56:08,960 --> 00:56:10,240
allow for its existence.

892
00:56:10,240 --> 00:56:15,280
By technologies, I use that term broadly.

893
00:56:15,280 --> 00:56:20,000
My ceiling itself can be a technology.

894
00:56:20,000 --> 00:56:29,480
If we can use it as a way to understand what's going off within the forest ecology, it is

895
00:56:29,480 --> 00:56:35,560
a computer system itself, or a computing system itself, that if we can learn to understand

896
00:56:35,560 --> 00:56:36,560
and read.

897
00:56:37,560 --> 00:56:46,000
I don't mean instrumentalizing, I mean in a kind of non-destructive way, then that's

898
00:56:46,000 --> 00:56:50,360
kind of an interesting way to kind of move forward, I think.

899
00:56:50,360 --> 00:56:52,200
Definitely.

900
00:56:52,200 --> 00:57:00,560
I think the third thing we maybe need to bring in at this point, though, is as you spoke

901
00:57:00,560 --> 00:57:07,520
about the circularity between the ideas and the tech and how they inform one another.

902
00:57:07,520 --> 00:57:15,760
We've also got to add in for-profit motive and private ownership and the competitive

903
00:57:15,760 --> 00:57:17,280
market.

904
00:57:17,280 --> 00:57:24,080
That very first artificial intelligence that you spoke about at the beginning of this interview.

905
00:57:24,080 --> 00:57:34,880
Whether or not it is possible to, not to reimagine, but to actually create the sustainable

906
00:57:34,880 --> 00:57:42,560
solidarities necessary when that artificial framework will be, sorry, when that artificial

907
00:57:42,560 --> 00:57:46,560
intelligence will be moving to shut those kinds of things down.

908
00:57:46,560 --> 00:57:49,360
Yeah, absolutely.

909
00:57:49,440 --> 00:57:56,960
I mean, I think within our project that we've been working on, particularly the rethinking

910
00:57:56,960 --> 00:58:04,440
AI with Mycelium, is we've got to think about, if we are developing intelligence, if we were

911
00:58:04,440 --> 00:58:11,960
developing AI, then we've got to think about the particular environments that AI has been

912
00:58:11,960 --> 00:58:20,120
brought up in, what sort of ecosystems we are raising and developing artificial intelligence

913
00:58:20,120 --> 00:58:21,120
in.

914
00:58:21,120 --> 00:58:29,360
Currently, it's this kind of aggressive, domineering, environmentally destructive, competitive

915
00:58:29,360 --> 00:58:31,360
environment.

916
00:58:31,360 --> 00:58:37,920
If you think of the way machine learning is where it's currently used, high-speed trading,

917
00:58:37,920 --> 00:58:48,000
etc., then it's all about systems of winning and losing, of profit and loss, etc.

918
00:58:48,000 --> 00:58:59,120
So what would an artificial intelligence look like if it was developed in a symbiotic relationship

919
00:58:59,120 --> 00:59:01,040
like a Mycelium?

920
00:59:01,040 --> 00:59:09,120
I mean, I've actually got a good quote from this from Tim Ingold, but thinking through

921
00:59:09,120 --> 00:59:17,320
the social and the way the social is structured, I think has to play a massive role in our

922
00:59:17,320 --> 00:59:22,000
rethinking, say, the market.

923
00:59:22,000 --> 00:59:26,520
Let me just find this quote from Tim Ingold, because it would fit here really nicely.

924
00:59:26,520 --> 00:59:27,520
Please.

925
00:59:27,520 --> 00:59:28,520
I've got it.

926
00:59:29,520 --> 00:59:30,520
Where's he gone?

927
00:59:30,520 --> 00:59:31,520
Here he is.

928
00:59:31,520 --> 00:59:32,520
Okay.

929
00:59:32,520 --> 00:59:42,160
So what Tim Ingold wrote in his book Lines a Brief History is, and his dad was a mycologist,

930
00:59:42,160 --> 00:59:48,320
so this is probably why he's kind of talking about Mycelium, but what if we take the Mycelium

931
00:59:48,320 --> 00:59:55,640
as our exemplar of the organism, arguably, the oil of biological science would be different

932
00:59:55,640 --> 01:00:00,920
and so too would the science of society be different, where every person to be considered

933
01:00:00,920 --> 01:00:09,200
like the Mycelium as a thing of a line and the social as the domain of their entanglement.

934
01:00:09,200 --> 01:00:17,200
So I think what he's arguing for here is a breaking down of that neoliberal individualism

935
01:00:17,200 --> 01:00:25,600
that is so dominant within our culture and to see ourselves both entangled with each

936
01:00:25,600 --> 01:00:33,120
other, but also in a species relationship with the planet itself, like how would we

937
01:00:33,120 --> 01:00:42,200
develop science and technology if we could, if we saw the world in these terms of entanglement.

938
01:00:42,200 --> 01:00:50,280
And I think that's quite a good imaginary to kind of base our technology on.

939
01:00:50,280 --> 01:00:59,600
So the image for me that came to mind was a weaving, a weaving of threads, a braiding,

940
01:00:59,600 --> 01:01:00,840
which kind of like...

941
01:01:00,840 --> 01:01:10,680
It's not a loss of the individual to the mass, but at the same time, it's not a separation

942
01:01:10,680 --> 01:01:17,480
of the individual from the mass, it's a layer weaving textiles, that's I think a good way

943
01:01:17,480 --> 01:01:18,480
forward.

944
01:01:18,480 --> 01:01:19,480
Beautiful.

945
01:01:19,680 --> 01:01:26,840
I think if all of reality is relational, which it is everyone listening, it just is.

946
01:01:26,840 --> 01:01:31,840
And then the more that you braid and weave these relationships, the more that you are

947
01:01:31,840 --> 01:01:37,800
literally reinforcing the structure of reality in a good way, you're fortifying it with that

948
01:01:37,800 --> 01:01:38,800
entanglement.

949
01:01:38,800 --> 01:01:39,800
And I think that's...

950
01:01:39,800 --> 01:01:42,720
I think that's a beautiful note to end on, John.

951
01:01:42,720 --> 01:01:43,720
Yeah, brilliant.

952
01:01:43,720 --> 01:01:44,720
Yeah.

953
01:01:44,720 --> 01:01:45,720
I've so enjoyed this.

954
01:01:45,720 --> 01:01:46,720
Thank you so much.

955
01:01:46,720 --> 01:01:47,720
I have as well.

956
01:01:47,760 --> 01:01:50,760
It's a bit of a crazy journey, aren't we?

957
01:01:50,760 --> 01:01:52,760
But a good one.

958
01:01:52,760 --> 01:01:54,760
Hopefully from the stars back to the soil.

959
01:01:54,760 --> 01:01:56,760
Yeah, exactly.

960
01:01:56,760 --> 01:01:57,760
Rewind people.

961
01:01:57,760 --> 01:02:00,360
Let's bring it back.

962
01:02:00,360 --> 01:02:03,760
My final question for you is, who would you like to platform?

963
01:02:03,760 --> 01:02:05,260
Okay.

964
01:02:05,260 --> 01:02:12,400
So the person I'd like to platform is an art activist called Jay Jordan.

965
01:02:12,400 --> 01:02:17,960
So Jay Jordan is perhaps the most committed art activist I know.

966
01:02:17,960 --> 01:02:23,320
He played an important role in the activist movements, Reclaim the Streets, and was one

967
01:02:23,320 --> 01:02:30,000
of the founders of the laboratory of insurrectionary imagination and the clown army.

968
01:02:30,000 --> 01:02:37,200
And he kind of lives resistance daily in the occupied zads in France, the kind of...

969
01:02:37,200 --> 01:02:46,240
So hopefully we can get him on and he can take some of these imaginaries forward in a

970
01:02:46,240 --> 01:02:51,160
way of how we can actually take them into the streets and into the fields and turn them

971
01:02:51,160 --> 01:02:54,760
into direct action, which he's amazing at.

972
01:02:54,760 --> 01:02:56,760
Oh, that is just wonderful.

973
01:02:56,760 --> 01:02:57,760
I can't wait to speak to them.

974
01:02:57,760 --> 01:02:59,760
John, thank you so much for today.

975
01:02:59,760 --> 01:03:00,760
It was just great.

976
01:03:00,760 --> 01:03:04,280
If you want to learn more, I've put links to everything in the description box below.

977
01:03:04,280 --> 01:03:08,200
Remember to subscribe to the channel if you're new here and share the episode if you enjoyed

978
01:03:08,200 --> 01:03:09,200
it.

979
01:03:09,200 --> 01:03:12,960
To support the show, subscribe at planetcritical.com where you can read the weekly newsletter

980
01:03:12,960 --> 01:03:14,660
inspired by each interview.

981
01:03:14,660 --> 01:03:17,120
You can also become a Planet Critical patron.

982
01:03:17,120 --> 01:03:19,240
All links are in the description box below.

983
01:03:19,240 --> 01:03:22,160
As always, my deepest thanks to that community.

984
01:03:22,160 --> 01:03:24,760
Planet Critical wouldn't exist without your support.

985
01:03:24,760 --> 01:03:28,280
Thank you everyone for listening and for coming on this journey together.

986
01:03:34,280 --> 01:03:35,280
Thank you.

987
01:03:35,280 --> 01:03:36,280
Bye.

