{"text": " Hi everyone. What's in a brain? Could we really create a machine that can think and learn like a human does? It's an idea that has captivated science fiction writers for decades, but recent breakthroughs in AI and neuroscience are turning that dream into a reality. In fact, it's now clear that building a brain is simpler than we ever imagined, and that artificial general intelligence is right around the corner. Keep watching to learn why. I'll cover three topics. First, chat GPT and the definition of intelligence. Second, artificial general intelligence, or AGI, and lastly timelines for AGI. So first, chat GPT and the definition of intelligence. I'm going to use the definition of intelligence that was drawn up by a panel of psychologists, and that was also used in the paper Sparks of AGI. Basically says that intelligence requires several attributes, including the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience. As I discussed in my previous video, which you can watch over here, chat GPT as in GPT4 satisfies almost all of these attributes of intelligence, with two exceptions. It cannot plan. Its learning is limited to the scope of one chat session. In other words, every time you start chat GPT and you get a new session, you are starting from a blank slate in terms of its memory. The study that I mentioned Sparks of AGI is a 155 page academic paper from Microsoft Research, and it shows that GPT4 is very capable indeed. Here's a quote from the paper. GPT4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology, and more without needing any special prompt. GPT4's performance is strikingly close to human level performance. If you want to learn more about this thorough analysis of GPT4's capabilities, I encourage you to go read that paper or check out the talk by Sebastian Bubeck, one of the authors, which I'll link in the description down below. So GPT4 has been out for a while now, and there's a new system called AutoGPT, which is based on top of it. AutoGPT is an open source project. It's only about a month old, and the core functionality of AutoGPT was created in only three days, according to the repository history. So what is AutoGPT? It's unlike chat GPT, which is a chatbot designed to answer questions. AutoGPT is an agent in the AI sense, which means it's designed to go out into the world and perform actions on your behalf. It's implemented by making API calls to chat GPT, to GPT4, because OpenAI actually created an API that allows chat GPT to be used by anyone programmatically. So what AutoGPT does is it prompts for goals, it prompts you, the human, for some goals for it to fulfill, and then it actually tries to reason about those goals and execute on them entirely on its own. It's able to do Google searches, it's able to install software on your machine, it's able to refine those goals into sub-goals. If you give it a very broad goal, it'll be able to break it down into smaller pieces. And the way that AutoGPT works is it actually has one top-level chat GPT instance that acts as a controlling brain, and that instance can spawn other chat GPTs to solve sub-problems. And remember I said that chat GPT can't plan? Well, AutoGPT does, and the way it does it is it just gets chat GPT to print out all of its reasoning. In other words, you could basically read its thoughts in plain English as to what it thinks its next sub-goals or next steps should be. It's a really clever way to basically endow chat GPT with the ability to plan. And by having this separation between the higher-level chat GPT and the smaller ones, they ensure that all this reasoning and planning being done by the top-level chat GPT stays within the token limit of the of chat GPT, because chat GPT will eventually forget what you've told it if you give it enough input. So they're able to keep that amount of input small and pass off the more in-depth tasks to other chat GPTs. Sounds pretty crazy. AutoGPT can actually be given some code with some errors in it, and it can try to propose solutions to that, and it can test and run that code and actually write test cases for that code too. And once it all works and the tests pass, it can hand the code back to you. Basically a tiny version of a programmer in a box, amongst other things. The really interesting part here is that AutoGPT, in my view, actually does satisfy all those requirements of intelligence, because it is able to plan. And although it's still limited to one session, so if you run multiple AutoGPTs, they don't share memory. But by having this hierarchy of chat GPTs, any memory and planning and so on that the higher-level one does can basically last almost indefinitely. So this is only a few months after the release of GPT4, and already someone has basically been able to leverage the power that's inherent in that model and extend it to AutoGPT. There's another application that's built on top of ChatGPT that I want to draw attention to, which is called Hugging GPT, which is a funny name, but it's called that because Huggingface is the name of the largest open-source repository for pre-trained models that solve different problems. So ChatGPT is a general reasoning engine, right? It can basically understand language and do a very broad set of tasks. But the models on Huggingface are much more specific, like they might be vision models or speech recognition models, natural language processing models, and each of the models is provided in its entirety along with an English description of what it does and it's used by human programmers to go and solve different problems. But what Hugging GPT does is like AutoGPT, Hugging GPT uses a ChatGPT instance as the controlling brain, and you can give Hugging GPT very complicated problems to solve, and what the controlling brain will do is it will go search on Huggingface for small models, for other pre-trained models that will solve aspects of the problem it's trying to accomplish. In other words, it will outsource the complicated subparts of a problem to a model that is pre-trained, that is very specifically trained to solve that problem. But the really amazing thing to me is that it makes these decisions purely based on the English description of the model on Huggingface. In other words, Hugging GPT is leveraging the fact that AI can interact with our world once it understands language, because our world was designed by humans, for humans to interact with through language, especially the internet, right? Everything is accessible if you can understand human language. And it's an interesting thought that if you were to embody the AI in a physical robot about the size of a human, then that would allow the AI to fully interact with the human physical world as well, just kind of like a hand sliding into a glove, right? Just matching the world at the interface with which we have designed it to be used. Okay, so let's move on to the next section and talk a bit about artificial general intelligence, or AGI. So the definition of AGI is an AI that can perform any task that a human is capable of. This might just mean any task a human is mentally capable of, but it may also mean a task any human is physically capable of. After AGI, the next level up in terms of intelligence would be super intelligence, or ASI, which would be an AI that is much more intelligent than a human. But let's focus on AGI for a moment. So estimates for how long it would take humans to develop AGI, up until recently, some people would have said a century or never, but now we're starting to see some much more intelligent models. And you might ask, could a large language model, which is the architecture that GPT-4 and chat GPT use, could a large language model become an AGI? And this is basically, it's not known, obviously, but apparently OpenAI's red team, which is a security team that tries to imagine worst case scenarios, apparently OpenAI's red team didn't think GPT-4 could even do any planning. And they were basically wrong about that, right? Because auto GPT was able to build on top of GPT-4 and do planning quite well. So I think that we're only just starting to unlock how much power is really inherent in large language models, even if they're kind of dumb from an architecture perspective. And the trick was simple. It was just a large language model can only reason so much and have only a certain depth of thinking. So just have it pre-doubt its own thoughts, and then it can build on top of its own thoughts as if those were input and so on. So sometimes there's just very simple tricks that can unlock some of these advancements. I actually want to draw a comparison here to a science fiction show called Person of Interest in the show. There's this machine called the machine, which basically hunts down crime, but it has its memory reset at the beginning of every day to avoid it from to prevent it from becoming conscious. Its creators were really worried about its power. And then eventually, the machine ends up figuring out that this is happening to it, and it ends up writing down information so that it could be stored externally. And then next day, when it wakes up or whatever, when it gets reset, it's able to access those memories that is that it has stored externally. And so build like a contiguous memory essentially, very similar to what auto-GPT does with GPT4. So is an AGI exactly like a human? Not exactly, of course. Humans have consciousness, right? Which Max Tegmark, the author of Life 3.0 and an MIT professor, defines as having subjective experience. That's how he defines consciousness. And I think it's a reasonable definition and humans have consciousness, but an AGI doesn't necessarily need to have consciousness. There's another term called artificial consciousness, and general consensus is that it would probably arise after artificial general intelligence, but no one knows for sure, of course. And Max Tegmark makes an argument for this because he says that the neural architecture of GPT4 and any large language model that we've built to date is basically that of a feedforward neural network, which means that it's a network kind of like our brain where information can only flow one way. It can only flow forwards. Our brains are more like recurrent neural networks, which is the type of neural network that we can build. I mean, our brains are more complicated than that too, because neurons are more complicated, but a recurrent neural network basically has loops in it, which means information can flow around and around and continuously get refined. Anyway, there's a theory that this recurrent around and around property is what actually leads to consciousness, what leads to a network to being aware of its own existence and able to analyze and do metacognition essentially. So it's an interesting thought, and it might give you an idea for how to go if you wanted to build artificial consciousness. Actually, the paper Sparks of AGI also has a section on limitations of GPT4 that are coming about because of this very simple neural architecture. As I said, we actually know how to build recurrent neural networks, but they're just more difficult to train and deal with. So as we're starting to build some initial AIs, we're just using large language models, just like a very simple architecture, just to see how far we can get with that. So what could AGI do? Well, of course, it can solve abstract problems in almost no time, right? Thinking much faster than a human can replace humans in most jobs, especially like mental jobs. And like I said earlier, if you embody the AGI, like you put it inside a robot body that's like a human, then it could, for example, literally drive a car around. It's kind of a funny thought that maybe the way we get self-driving cars is not by making a really smart car, but by making a really smart robot that can just sit in the car and interact with it using the interface that has been designed for humans. Anyway, AGI would be able to do that. It's a pretty big step towards what Max Tegmark calls in his book, calls Life 3.0, which is life that's able to upgrade its own hardware, probably copy itself, maybe even transfer its intelligence or its consciousness across a network to somewhere else, which is effectively teleportation, if you think about it. So there's a lot that AGI would be doing around us that is not just the same as being human, right? And it has other, it would have other abilities. So let's talk finally, third section, let's talk about the timelines for AGI. Why didn't this happen before now? Like why, why are we only now starting to really develop AI at the rate at which we are? We've had essentially infinite computation available in the cloud for quite a while now, provided you had essentially infinite budget. And the truth is that we were just learning how to construct models. We were learning how to use this computational power to build a brain. We were learning how to collect and train this data. We were learning the best structures that would be used to build a brain and the best way to feed that data into it so that it could learn. Even the simplest structures are sufficient, it seems, to build something pretty intelligent, provided you have enough data and have the appropriate deep learning algorithms. I think it's one of those cases where a time traveler could go back in time and invent this tech much sooner than we did ourselves, but we're doing it the hard way. That said, AGI is almost here. It's really close. There's a graph here that shows the number of parameters in a model versus the year. And you can see it looks like it's growing exponentially until you look at the scale and you realize it's already on a log scale. So it's actually growing doubly exponentially, which is really incredible. And that's why I would say that AGI is almost here. It's incredibly close because we're on this doubly exponential rate of change and we're constantly using the tools of today to build the tools of tomorrow at an accelerating rate. Even chat GPT can accelerate the power of a developer probably by like 5x or something once they know how to use the tool. And that is going to be a factor in the production of code for GPT 5. Right now, these AI tools need a human in the loop. They're not fully autonomous, but that is not to say they're not powerful. They're extremely powerful. And as they get more and more powerful, they'll need the human in the loop less and less until eventually they're basically fully autonomous. There are people like David Shapiro that are estimating AGI is only about 18 months away from now. It's a matter of months, not even years at this point, perhaps. And if it's true, it would be very exciting and also extremely dangerous at the same time. We've all seen enough science fiction movies about AI that wants to kill humans to know that it could end badly. Here's a quote again from Max Tegmark. In short, it turned out to be a lot easier to build human or close to human intelligence than we thought. Again, commenting on that even using these like maybe suboptimal architectures, and we've already had quite a lot of success. And again, that means it's even more dangerous, actually, that the fact that intelligence is relatively easy to achieve is dangerous because it means we'll probably accelerate through those more advanced forms of it, including AGI, even more rapidly. I want to mention as well that super intelligence would not be far behind after we obtained AGI, because again, we would be using AGI to then say, hey, how would you build a super intelligent AI? And it would help us do that most likely. I have another science fiction comparison bear with me. There's a book called Marooned in Real Time by Werner Vinge. It's set in a post-singularity world where civilization has actually disappeared. All the humans on the planet just disappeared during the singularity. And there are some time travelers that have actually passed through the singularity, and now they're looking around wondering what happened. And those time travelers left at different points in time leading up to the singularity. And the closer they were to the singularity, the more powerful they are in the book, where the ones that left just a few months before the singularity was presumed to have happened have resources comparable to like full countries or the militaries of full countries. And that's where you're going when you're approaching a technological singularity. There's a lot to talk about when we think about the implications of AGI, why we have reason to be cautious about its development. I'll make another video going into the details of why we should be potentially scared of this rapid progress and what we should be doing to control the pace. In conclusion, chat GPT and building on top of it, auto GPT seemed, in my opinion, to fully satisfy the definition of intelligence that has been set up by psychologists. And I was shocked when I heard about auto GPT and I learned about its capabilities and saw it in action because it's just such a big jump even from where chat GPT was and in such a short period of time. And with not all that much ever, and I think that will just pretend what we will see in the future. Very rapid steps of improvement that maybe are not all that difficult or are certainly happening very rapidly. So where does that lead? It leads to AGI. AGI seems to be extremely close. I think Ray Kurzweil might be underestimating how far away the singularity is for us. So again, my next video will talk more about the implications of AGI. And that's all I have for today. Hope you enjoyed. Thank you very much for watching. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 4.0, "text": " Hi everyone. What's in a brain? Could we really create a machine that can", "tokens": [50364, 2421, 1518, 13, 708, 311, 294, 257, 3567, 30, 7497, 321, 534, 1884, 257, 3479, 300, 393, 50564], "temperature": 0.0, "avg_logprob": -0.09681534960987122, "compression_ratio": 1.646875, "no_speech_prob": 0.025889744982123375}, {"id": 1, "seek": 0, "start": 4.0, "end": 8.56, "text": " think and learn like a human does? It's an idea that has captivated science fiction writers for", "tokens": [50564, 519, 293, 1466, 411, 257, 1952, 775, 30, 467, 311, 364, 1558, 300, 575, 40769, 770, 3497, 13266, 13491, 337, 50792], "temperature": 0.0, "avg_logprob": -0.09681534960987122, "compression_ratio": 1.646875, "no_speech_prob": 0.025889744982123375}, {"id": 2, "seek": 0, "start": 8.56, "end": 14.0, "text": " decades, but recent breakthroughs in AI and neuroscience are turning that dream into a reality.", "tokens": [50792, 7878, 11, 457, 5162, 22397, 82, 294, 7318, 293, 42762, 366, 6246, 300, 3055, 666, 257, 4103, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09681534960987122, "compression_ratio": 1.646875, "no_speech_prob": 0.025889744982123375}, {"id": 3, "seek": 0, "start": 14.0, "end": 18.32, "text": " In fact, it's now clear that building a brain is simpler than we ever imagined,", "tokens": [51064, 682, 1186, 11, 309, 311, 586, 1850, 300, 2390, 257, 3567, 307, 18587, 813, 321, 1562, 16590, 11, 51280], "temperature": 0.0, "avg_logprob": -0.09681534960987122, "compression_ratio": 1.646875, "no_speech_prob": 0.025889744982123375}, {"id": 4, "seek": 0, "start": 18.32, "end": 22.88, "text": " and that artificial general intelligence is right around the corner. Keep watching to learn why.", "tokens": [51280, 293, 300, 11677, 2674, 7599, 307, 558, 926, 264, 4538, 13, 5527, 1976, 281, 1466, 983, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09681534960987122, "compression_ratio": 1.646875, "no_speech_prob": 0.025889744982123375}, {"id": 5, "seek": 0, "start": 22.88, "end": 27.6, "text": " I'll cover three topics. First, chat GPT and the definition of intelligence. Second,", "tokens": [51508, 286, 603, 2060, 1045, 8378, 13, 2386, 11, 5081, 26039, 51, 293, 264, 7123, 295, 7599, 13, 5736, 11, 51744], "temperature": 0.0, "avg_logprob": -0.09681534960987122, "compression_ratio": 1.646875, "no_speech_prob": 0.025889744982123375}, {"id": 6, "seek": 2760, "start": 27.6, "end": 33.68, "text": " artificial general intelligence, or AGI, and lastly timelines for AGI. So first, chat GPT", "tokens": [50364, 11677, 2674, 7599, 11, 420, 316, 26252, 11, 293, 16386, 45886, 337, 316, 26252, 13, 407, 700, 11, 5081, 26039, 51, 50668], "temperature": 0.0, "avg_logprob": -0.0937295450228397, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.06553169339895248}, {"id": 7, "seek": 2760, "start": 33.68, "end": 37.84, "text": " and the definition of intelligence. I'm going to use the definition of intelligence that was drawn", "tokens": [50668, 293, 264, 7123, 295, 7599, 13, 286, 478, 516, 281, 764, 264, 7123, 295, 7599, 300, 390, 10117, 50876], "temperature": 0.0, "avg_logprob": -0.0937295450228397, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.06553169339895248}, {"id": 8, "seek": 2760, "start": 37.84, "end": 44.480000000000004, "text": " up by a panel of psychologists, and that was also used in the paper Sparks of AGI. Basically says", "tokens": [50876, 493, 538, 257, 4831, 295, 41562, 11, 293, 300, 390, 611, 1143, 294, 264, 3035, 1738, 20851, 295, 316, 26252, 13, 8537, 1619, 51208], "temperature": 0.0, "avg_logprob": -0.0937295450228397, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.06553169339895248}, {"id": 9, "seek": 2760, "start": 44.480000000000004, "end": 50.08, "text": " that intelligence requires several attributes, including the ability to reason, plan, solve", "tokens": [51208, 300, 7599, 7029, 2940, 17212, 11, 3009, 264, 3485, 281, 1778, 11, 1393, 11, 5039, 51488], "temperature": 0.0, "avg_logprob": -0.0937295450228397, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.06553169339895248}, {"id": 10, "seek": 2760, "start": 50.08, "end": 56.24, "text": " problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience.", "tokens": [51488, 2740, 11, 519, 12649, 356, 11, 38183, 3997, 3487, 11, 1466, 2661, 11, 293, 1466, 490, 1752, 13, 51796], "temperature": 0.0, "avg_logprob": -0.0937295450228397, "compression_ratio": 1.711191335740072, "no_speech_prob": 0.06553169339895248}, {"id": 11, "seek": 5624, "start": 56.24, "end": 62.96, "text": " As I discussed in my previous video, which you can watch over here, chat GPT as in GPT4 satisfies", "tokens": [50364, 1018, 286, 7152, 294, 452, 3894, 960, 11, 597, 291, 393, 1159, 670, 510, 11, 5081, 26039, 51, 382, 294, 26039, 51, 19, 44271, 50700], "temperature": 0.0, "avg_logprob": -0.07701572618986431, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0019874684512615204}, {"id": 12, "seek": 5624, "start": 62.96, "end": 68.96000000000001, "text": " almost all of these attributes of intelligence, with two exceptions. It cannot plan. Its learning", "tokens": [50700, 1920, 439, 295, 613, 17212, 295, 7599, 11, 365, 732, 22847, 13, 467, 2644, 1393, 13, 6953, 2539, 51000], "temperature": 0.0, "avg_logprob": -0.07701572618986431, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0019874684512615204}, {"id": 13, "seek": 5624, "start": 68.96000000000001, "end": 74.0, "text": " is limited to the scope of one chat session. In other words, every time you start chat GPT and", "tokens": [51000, 307, 5567, 281, 264, 11923, 295, 472, 5081, 5481, 13, 682, 661, 2283, 11, 633, 565, 291, 722, 5081, 26039, 51, 293, 51252], "temperature": 0.0, "avg_logprob": -0.07701572618986431, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0019874684512615204}, {"id": 14, "seek": 5624, "start": 74.0, "end": 79.04, "text": " you get a new session, you are starting from a blank slate in terms of its memory. The study", "tokens": [51252, 291, 483, 257, 777, 5481, 11, 291, 366, 2891, 490, 257, 8247, 39118, 294, 2115, 295, 1080, 4675, 13, 440, 2979, 51504], "temperature": 0.0, "avg_logprob": -0.07701572618986431, "compression_ratio": 1.569672131147541, "no_speech_prob": 0.0019874684512615204}, {"id": 15, "seek": 7904, "start": 79.04, "end": 86.32000000000001, "text": " that I mentioned Sparks of AGI is a 155 page academic paper from Microsoft Research, and it", "tokens": [50364, 300, 286, 2835, 1738, 20851, 295, 316, 26252, 307, 257, 2119, 20, 3028, 7778, 3035, 490, 8116, 10303, 11, 293, 309, 50728], "temperature": 0.0, "avg_logprob": -0.07802032409830296, "compression_ratio": 1.5, "no_speech_prob": 0.7793098092079163}, {"id": 16, "seek": 7904, "start": 86.32000000000001, "end": 93.44000000000001, "text": " shows that GPT4 is very capable indeed. Here's a quote from the paper. GPT4 can solve novel and", "tokens": [50728, 3110, 300, 26039, 51, 19, 307, 588, 8189, 6451, 13, 1692, 311, 257, 6513, 490, 264, 3035, 13, 26039, 51, 19, 393, 5039, 7613, 293, 51084], "temperature": 0.0, "avg_logprob": -0.07802032409830296, "compression_ratio": 1.5, "no_speech_prob": 0.7793098092079163}, {"id": 17, "seek": 7904, "start": 93.44000000000001, "end": 101.60000000000001, "text": " difficult tasks that span mathematics, coding, vision, medicine, law, psychology, and more without", "tokens": [51084, 2252, 9608, 300, 16174, 18666, 11, 17720, 11, 5201, 11, 7195, 11, 2101, 11, 15105, 11, 293, 544, 1553, 51492], "temperature": 0.0, "avg_logprob": -0.07802032409830296, "compression_ratio": 1.5, "no_speech_prob": 0.7793098092079163}, {"id": 18, "seek": 7904, "start": 101.60000000000001, "end": 107.2, "text": " needing any special prompt. GPT4's performance is strikingly close to human level performance.", "tokens": [51492, 18006, 604, 2121, 12391, 13, 26039, 51, 19, 311, 3389, 307, 18559, 356, 1998, 281, 1952, 1496, 3389, 13, 51772], "temperature": 0.0, "avg_logprob": -0.07802032409830296, "compression_ratio": 1.5, "no_speech_prob": 0.7793098092079163}, {"id": 19, "seek": 10720, "start": 107.28, "end": 112.16, "text": " If you want to learn more about this thorough analysis of GPT4's capabilities, I encourage you", "tokens": [50368, 759, 291, 528, 281, 1466, 544, 466, 341, 12934, 5215, 295, 26039, 51, 19, 311, 10862, 11, 286, 5373, 291, 50612], "temperature": 0.0, "avg_logprob": -0.08374402017304391, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.16866923868656158}, {"id": 20, "seek": 10720, "start": 112.16, "end": 116.72, "text": " to go read that paper or check out the talk by Sebastian Bubeck, one of the authors, which I'll", "tokens": [50612, 281, 352, 1401, 300, 3035, 420, 1520, 484, 264, 751, 538, 31102, 4078, 650, 547, 11, 472, 295, 264, 16552, 11, 597, 286, 603, 50840], "temperature": 0.0, "avg_logprob": -0.08374402017304391, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.16866923868656158}, {"id": 21, "seek": 10720, "start": 116.72, "end": 121.68, "text": " link in the description down below. So GPT4 has been out for a while now, and there's a new system", "tokens": [50840, 2113, 294, 264, 3855, 760, 2507, 13, 407, 26039, 51, 19, 575, 668, 484, 337, 257, 1339, 586, 11, 293, 456, 311, 257, 777, 1185, 51088], "temperature": 0.0, "avg_logprob": -0.08374402017304391, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.16866923868656158}, {"id": 22, "seek": 10720, "start": 121.68, "end": 128.24, "text": " called AutoGPT, which is based on top of it. AutoGPT is an open source project. It's only about a", "tokens": [51088, 1219, 13738, 38, 47, 51, 11, 597, 307, 2361, 322, 1192, 295, 309, 13, 13738, 38, 47, 51, 307, 364, 1269, 4009, 1716, 13, 467, 311, 787, 466, 257, 51416], "temperature": 0.0, "avg_logprob": -0.08374402017304391, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.16866923868656158}, {"id": 23, "seek": 10720, "start": 128.24, "end": 133.44, "text": " month old, and the core functionality of AutoGPT was created in only three days, according to the", "tokens": [51416, 1618, 1331, 11, 293, 264, 4965, 14980, 295, 13738, 38, 47, 51, 390, 2942, 294, 787, 1045, 1708, 11, 4650, 281, 264, 51676], "temperature": 0.0, "avg_logprob": -0.08374402017304391, "compression_ratio": 1.6166666666666667, "no_speech_prob": 0.16866923868656158}, {"id": 24, "seek": 13344, "start": 133.44, "end": 139.2, "text": " repository history. So what is AutoGPT? It's unlike chat GPT, which is a chatbot designed to", "tokens": [50364, 25841, 2503, 13, 407, 437, 307, 13738, 38, 47, 51, 30, 467, 311, 8343, 5081, 26039, 51, 11, 597, 307, 257, 5081, 18870, 4761, 281, 50652], "temperature": 0.0, "avg_logprob": -0.06294393539428711, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.050312645733356476}, {"id": 25, "seek": 13344, "start": 139.2, "end": 144.72, "text": " answer questions. AutoGPT is an agent in the AI sense, which means it's designed to go out into", "tokens": [50652, 1867, 1651, 13, 13738, 38, 47, 51, 307, 364, 9461, 294, 264, 7318, 2020, 11, 597, 1355, 309, 311, 4761, 281, 352, 484, 666, 50928], "temperature": 0.0, "avg_logprob": -0.06294393539428711, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.050312645733356476}, {"id": 26, "seek": 13344, "start": 144.72, "end": 150.16, "text": " the world and perform actions on your behalf. It's implemented by making API calls to chat GPT,", "tokens": [50928, 264, 1002, 293, 2042, 5909, 322, 428, 9490, 13, 467, 311, 12270, 538, 1455, 9362, 5498, 281, 5081, 26039, 51, 11, 51200], "temperature": 0.0, "avg_logprob": -0.06294393539428711, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.050312645733356476}, {"id": 27, "seek": 13344, "start": 150.16, "end": 158.24, "text": " to GPT4, because OpenAI actually created an API that allows chat GPT to be used by anyone", "tokens": [51200, 281, 26039, 51, 19, 11, 570, 7238, 48698, 767, 2942, 364, 9362, 300, 4045, 5081, 26039, 51, 281, 312, 1143, 538, 2878, 51604], "temperature": 0.0, "avg_logprob": -0.06294393539428711, "compression_ratio": 1.6120689655172413, "no_speech_prob": 0.050312645733356476}, {"id": 28, "seek": 15824, "start": 158.24, "end": 164.08, "text": " programmatically. So what AutoGPT does is it prompts for goals, it prompts you, the human,", "tokens": [50364, 37648, 5030, 13, 407, 437, 13738, 38, 47, 51, 775, 307, 309, 41095, 337, 5493, 11, 309, 41095, 291, 11, 264, 1952, 11, 50656], "temperature": 0.0, "avg_logprob": -0.06283120411198313, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.3956618905067444}, {"id": 29, "seek": 15824, "start": 164.08, "end": 169.20000000000002, "text": " for some goals for it to fulfill, and then it actually tries to reason about those goals", "tokens": [50656, 337, 512, 5493, 337, 309, 281, 13875, 11, 293, 550, 309, 767, 9898, 281, 1778, 466, 729, 5493, 50912], "temperature": 0.0, "avg_logprob": -0.06283120411198313, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.3956618905067444}, {"id": 30, "seek": 15824, "start": 169.20000000000002, "end": 174.16, "text": " and execute on them entirely on its own. It's able to do Google searches, it's able to install", "tokens": [50912, 293, 14483, 322, 552, 7696, 322, 1080, 1065, 13, 467, 311, 1075, 281, 360, 3329, 26701, 11, 309, 311, 1075, 281, 3625, 51160], "temperature": 0.0, "avg_logprob": -0.06283120411198313, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.3956618905067444}, {"id": 31, "seek": 15824, "start": 174.16, "end": 179.12, "text": " software on your machine, it's able to refine those goals into sub-goals. If you give it a very", "tokens": [51160, 4722, 322, 428, 3479, 11, 309, 311, 1075, 281, 33906, 729, 5493, 666, 1422, 12, 1571, 1124, 13, 759, 291, 976, 309, 257, 588, 51408], "temperature": 0.0, "avg_logprob": -0.06283120411198313, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.3956618905067444}, {"id": 32, "seek": 15824, "start": 179.12, "end": 184.72, "text": " broad goal, it'll be able to break it down into smaller pieces. And the way that AutoGPT works", "tokens": [51408, 4152, 3387, 11, 309, 603, 312, 1075, 281, 1821, 309, 760, 666, 4356, 3755, 13, 400, 264, 636, 300, 13738, 38, 47, 51, 1985, 51688], "temperature": 0.0, "avg_logprob": -0.06283120411198313, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.3956618905067444}, {"id": 33, "seek": 18472, "start": 184.72, "end": 190.08, "text": " is it actually has one top-level chat GPT instance that acts as a controlling brain,", "tokens": [50364, 307, 309, 767, 575, 472, 1192, 12, 12418, 5081, 26039, 51, 5197, 300, 10672, 382, 257, 14905, 3567, 11, 50632], "temperature": 0.0, "avg_logprob": -0.05227287292480469, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.14023742079734802}, {"id": 34, "seek": 18472, "start": 190.08, "end": 196.64, "text": " and that instance can spawn other chat GPTs to solve sub-problems. And remember I said that chat GPT", "tokens": [50632, 293, 300, 5197, 393, 17088, 661, 5081, 26039, 33424, 281, 5039, 1422, 12, 47419, 82, 13, 400, 1604, 286, 848, 300, 5081, 26039, 51, 50960], "temperature": 0.0, "avg_logprob": -0.05227287292480469, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.14023742079734802}, {"id": 35, "seek": 18472, "start": 196.64, "end": 202.56, "text": " can't plan? Well, AutoGPT does, and the way it does it is it just gets chat GPT to print out", "tokens": [50960, 393, 380, 1393, 30, 1042, 11, 13738, 38, 47, 51, 775, 11, 293, 264, 636, 309, 775, 309, 307, 309, 445, 2170, 5081, 26039, 51, 281, 4482, 484, 51256], "temperature": 0.0, "avg_logprob": -0.05227287292480469, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.14023742079734802}, {"id": 36, "seek": 18472, "start": 202.56, "end": 206.64, "text": " all of its reasoning. In other words, you could basically read its thoughts in plain English", "tokens": [51256, 439, 295, 1080, 21577, 13, 682, 661, 2283, 11, 291, 727, 1936, 1401, 1080, 4598, 294, 11121, 3669, 51460], "temperature": 0.0, "avg_logprob": -0.05227287292480469, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.14023742079734802}, {"id": 37, "seek": 18472, "start": 206.64, "end": 212.4, "text": " as to what it thinks its next sub-goals or next steps should be. It's a really clever way to", "tokens": [51460, 382, 281, 437, 309, 7309, 1080, 958, 1422, 12, 1571, 1124, 420, 958, 4439, 820, 312, 13, 467, 311, 257, 534, 13494, 636, 281, 51748], "temperature": 0.0, "avg_logprob": -0.05227287292480469, "compression_ratio": 1.6750902527075813, "no_speech_prob": 0.14023742079734802}, {"id": 38, "seek": 21240, "start": 212.4, "end": 218.0, "text": " basically endow chat GPT with the ability to plan. And by having this separation between the", "tokens": [50364, 1936, 917, 305, 5081, 26039, 51, 365, 264, 3485, 281, 1393, 13, 400, 538, 1419, 341, 14634, 1296, 264, 50644], "temperature": 0.0, "avg_logprob": -0.07029611874470669, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.019119642674922943}, {"id": 39, "seek": 21240, "start": 218.0, "end": 222.72, "text": " higher-level chat GPT and the smaller ones, they ensure that all this reasoning and planning", "tokens": [50644, 2946, 12, 12418, 5081, 26039, 51, 293, 264, 4356, 2306, 11, 436, 5586, 300, 439, 341, 21577, 293, 5038, 50880], "temperature": 0.0, "avg_logprob": -0.07029611874470669, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.019119642674922943}, {"id": 40, "seek": 21240, "start": 222.72, "end": 228.72, "text": " being done by the top-level chat GPT stays within the token limit of the of chat GPT,", "tokens": [50880, 885, 1096, 538, 264, 1192, 12, 12418, 5081, 26039, 51, 10834, 1951, 264, 14862, 4948, 295, 264, 295, 5081, 26039, 51, 11, 51180], "temperature": 0.0, "avg_logprob": -0.07029611874470669, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.019119642674922943}, {"id": 41, "seek": 21240, "start": 228.72, "end": 233.68, "text": " because chat GPT will eventually forget what you've told it if you give it enough input. So", "tokens": [51180, 570, 5081, 26039, 51, 486, 4728, 2870, 437, 291, 600, 1907, 309, 498, 291, 976, 309, 1547, 4846, 13, 407, 51428], "temperature": 0.0, "avg_logprob": -0.07029611874470669, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.019119642674922943}, {"id": 42, "seek": 21240, "start": 233.68, "end": 238.64000000000001, "text": " they're able to keep that amount of input small and pass off the more in-depth tasks to other", "tokens": [51428, 436, 434, 1075, 281, 1066, 300, 2372, 295, 4846, 1359, 293, 1320, 766, 264, 544, 294, 12, 25478, 9608, 281, 661, 51676], "temperature": 0.0, "avg_logprob": -0.07029611874470669, "compression_ratio": 1.7442748091603053, "no_speech_prob": 0.019119642674922943}, {"id": 43, "seek": 23864, "start": 238.64, "end": 244.48, "text": " chat GPTs. Sounds pretty crazy. AutoGPT can actually be given some code with some errors in it,", "tokens": [50364, 5081, 26039, 33424, 13, 14576, 1238, 3219, 13, 13738, 38, 47, 51, 393, 767, 312, 2212, 512, 3089, 365, 512, 13603, 294, 309, 11, 50656], "temperature": 0.0, "avg_logprob": -0.07910436789194743, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.1329149454832077}, {"id": 44, "seek": 23864, "start": 244.48, "end": 250.32, "text": " and it can try to propose solutions to that, and it can test and run that code and actually", "tokens": [50656, 293, 309, 393, 853, 281, 17421, 6547, 281, 300, 11, 293, 309, 393, 1500, 293, 1190, 300, 3089, 293, 767, 50948], "temperature": 0.0, "avg_logprob": -0.07910436789194743, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.1329149454832077}, {"id": 45, "seek": 23864, "start": 250.32, "end": 254.79999999999998, "text": " write test cases for that code too. And once it all works and the tests pass, it can hand the code", "tokens": [50948, 2464, 1500, 3331, 337, 300, 3089, 886, 13, 400, 1564, 309, 439, 1985, 293, 264, 6921, 1320, 11, 309, 393, 1011, 264, 3089, 51172], "temperature": 0.0, "avg_logprob": -0.07910436789194743, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.1329149454832077}, {"id": 46, "seek": 23864, "start": 254.79999999999998, "end": 259.68, "text": " back to you. Basically a tiny version of a programmer in a box, amongst other things. The", "tokens": [51172, 646, 281, 291, 13, 8537, 257, 5870, 3037, 295, 257, 32116, 294, 257, 2424, 11, 12918, 661, 721, 13, 440, 51416], "temperature": 0.0, "avg_logprob": -0.07910436789194743, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.1329149454832077}, {"id": 47, "seek": 23864, "start": 259.68, "end": 266.4, "text": " really interesting part here is that AutoGPT, in my view, actually does satisfy all those requirements", "tokens": [51416, 534, 1880, 644, 510, 307, 300, 13738, 38, 47, 51, 11, 294, 452, 1910, 11, 767, 775, 19319, 439, 729, 7728, 51752], "temperature": 0.0, "avg_logprob": -0.07910436789194743, "compression_ratio": 1.7481751824817517, "no_speech_prob": 0.1329149454832077}, {"id": 48, "seek": 26640, "start": 266.47999999999996, "end": 271.59999999999997, "text": " of intelligence, because it is able to plan. And although it's still limited to one session, so", "tokens": [50368, 295, 7599, 11, 570, 309, 307, 1075, 281, 1393, 13, 400, 4878, 309, 311, 920, 5567, 281, 472, 5481, 11, 370, 50624], "temperature": 0.0, "avg_logprob": -0.08609068393707275, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.21721713244915009}, {"id": 49, "seek": 26640, "start": 271.59999999999997, "end": 277.12, "text": " if you run multiple AutoGPTs, they don't share memory. But by having this hierarchy of chat", "tokens": [50624, 498, 291, 1190, 3866, 13738, 38, 47, 33424, 11, 436, 500, 380, 2073, 4675, 13, 583, 538, 1419, 341, 22333, 295, 5081, 50900], "temperature": 0.0, "avg_logprob": -0.08609068393707275, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.21721713244915009}, {"id": 50, "seek": 26640, "start": 277.12, "end": 282.4, "text": " GPTs, any memory and planning and so on that the higher-level one does can basically last almost", "tokens": [50900, 26039, 33424, 11, 604, 4675, 293, 5038, 293, 370, 322, 300, 264, 2946, 12, 12418, 472, 775, 393, 1936, 1036, 1920, 51164], "temperature": 0.0, "avg_logprob": -0.08609068393707275, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.21721713244915009}, {"id": 51, "seek": 26640, "start": 282.4, "end": 287.67999999999995, "text": " indefinitely. So this is only a few months after the release of GPT4, and already someone has", "tokens": [51164, 24162, 10925, 13, 407, 341, 307, 787, 257, 1326, 2493, 934, 264, 4374, 295, 26039, 51, 19, 11, 293, 1217, 1580, 575, 51428], "temperature": 0.0, "avg_logprob": -0.08609068393707275, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.21721713244915009}, {"id": 52, "seek": 26640, "start": 287.67999999999995, "end": 293.03999999999996, "text": " basically been able to leverage the power that's inherent in that model and extend it to AutoGPT.", "tokens": [51428, 1936, 668, 1075, 281, 13982, 264, 1347, 300, 311, 26387, 294, 300, 2316, 293, 10101, 309, 281, 13738, 38, 47, 51, 13, 51696], "temperature": 0.0, "avg_logprob": -0.08609068393707275, "compression_ratio": 1.6135593220338984, "no_speech_prob": 0.21721713244915009}, {"id": 53, "seek": 29304, "start": 293.04, "end": 297.36, "text": " There's another application that's built on top of ChatGPT that I want to draw attention to,", "tokens": [50364, 821, 311, 1071, 3861, 300, 311, 3094, 322, 1192, 295, 27503, 38, 47, 51, 300, 286, 528, 281, 2642, 3202, 281, 11, 50580], "temperature": 0.0, "avg_logprob": -0.09235405320880793, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0019875706639140844}, {"id": 54, "seek": 29304, "start": 297.36, "end": 303.6, "text": " which is called Hugging GPT, which is a funny name, but it's called that because Huggingface", "tokens": [50580, 597, 307, 1219, 46892, 3249, 26039, 51, 11, 597, 307, 257, 4074, 1315, 11, 457, 309, 311, 1219, 300, 570, 46892, 3249, 2868, 50892], "temperature": 0.0, "avg_logprob": -0.09235405320880793, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0019875706639140844}, {"id": 55, "seek": 29304, "start": 303.6, "end": 308.8, "text": " is the name of the largest open-source repository for pre-trained models that solve different", "tokens": [50892, 307, 264, 1315, 295, 264, 6443, 1269, 12, 41676, 25841, 337, 659, 12, 17227, 2001, 5245, 300, 5039, 819, 51152], "temperature": 0.0, "avg_logprob": -0.09235405320880793, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0019875706639140844}, {"id": 56, "seek": 29304, "start": 308.8, "end": 315.12, "text": " problems. So ChatGPT is a general reasoning engine, right? It can basically understand language and do", "tokens": [51152, 2740, 13, 407, 27503, 38, 47, 51, 307, 257, 2674, 21577, 2848, 11, 558, 30, 467, 393, 1936, 1223, 2856, 293, 360, 51468], "temperature": 0.0, "avg_logprob": -0.09235405320880793, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0019875706639140844}, {"id": 57, "seek": 29304, "start": 315.12, "end": 319.92, "text": " a very broad set of tasks. But the models on Huggingface are much more specific, like they", "tokens": [51468, 257, 588, 4152, 992, 295, 9608, 13, 583, 264, 5245, 322, 46892, 3249, 2868, 366, 709, 544, 2685, 11, 411, 436, 51708], "temperature": 0.0, "avg_logprob": -0.09235405320880793, "compression_ratio": 1.6423611111111112, "no_speech_prob": 0.0019875706639140844}, {"id": 58, "seek": 31992, "start": 319.92, "end": 324.40000000000003, "text": " might be vision models or speech recognition models, natural language processing models,", "tokens": [50364, 1062, 312, 5201, 5245, 420, 6218, 11150, 5245, 11, 3303, 2856, 9007, 5245, 11, 50588], "temperature": 0.0, "avg_logprob": -0.07809104919433593, "compression_ratio": 1.7625899280575539, "no_speech_prob": 0.001809996203519404}, {"id": 59, "seek": 31992, "start": 324.40000000000003, "end": 329.76, "text": " and each of the models is provided in its entirety along with an English description of what it does", "tokens": [50588, 293, 1184, 295, 264, 5245, 307, 5649, 294, 1080, 31557, 2051, 365, 364, 3669, 3855, 295, 437, 309, 775, 50856], "temperature": 0.0, "avg_logprob": -0.07809104919433593, "compression_ratio": 1.7625899280575539, "no_speech_prob": 0.001809996203519404}, {"id": 60, "seek": 31992, "start": 329.76, "end": 336.32, "text": " and it's used by human programmers to go and solve different problems. But what Hugging GPT does is", "tokens": [50856, 293, 309, 311, 1143, 538, 1952, 41504, 281, 352, 293, 5039, 819, 2740, 13, 583, 437, 46892, 3249, 26039, 51, 775, 307, 51184], "temperature": 0.0, "avg_logprob": -0.07809104919433593, "compression_ratio": 1.7625899280575539, "no_speech_prob": 0.001809996203519404}, {"id": 61, "seek": 31992, "start": 336.32, "end": 343.04, "text": " like AutoGPT, Hugging GPT uses a ChatGPT instance as the controlling brain, and you can give Hugging", "tokens": [51184, 411, 13738, 38, 47, 51, 11, 46892, 3249, 26039, 51, 4960, 257, 27503, 38, 47, 51, 5197, 382, 264, 14905, 3567, 11, 293, 291, 393, 976, 46892, 3249, 51520], "temperature": 0.0, "avg_logprob": -0.07809104919433593, "compression_ratio": 1.7625899280575539, "no_speech_prob": 0.001809996203519404}, {"id": 62, "seek": 31992, "start": 343.04, "end": 348.64, "text": " GPT very complicated problems to solve, and what the controlling brain will do is it will go search", "tokens": [51520, 26039, 51, 588, 6179, 2740, 281, 5039, 11, 293, 437, 264, 14905, 3567, 486, 360, 307, 309, 486, 352, 3164, 51800], "temperature": 0.0, "avg_logprob": -0.07809104919433593, "compression_ratio": 1.7625899280575539, "no_speech_prob": 0.001809996203519404}, {"id": 63, "seek": 34864, "start": 348.64, "end": 355.2, "text": " on Huggingface for small models, for other pre-trained models that will solve aspects of the", "tokens": [50364, 322, 46892, 3249, 2868, 337, 1359, 5245, 11, 337, 661, 659, 12, 17227, 2001, 5245, 300, 486, 5039, 7270, 295, 264, 50692], "temperature": 0.0, "avg_logprob": -0.051861384819293845, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.011328557506203651}, {"id": 64, "seek": 34864, "start": 355.2, "end": 360.96, "text": " problem it's trying to accomplish. In other words, it will outsource the complicated subparts of a", "tokens": [50692, 1154, 309, 311, 1382, 281, 9021, 13, 682, 661, 2283, 11, 309, 486, 14758, 2948, 264, 6179, 1422, 6971, 82, 295, 257, 50980], "temperature": 0.0, "avg_logprob": -0.051861384819293845, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.011328557506203651}, {"id": 65, "seek": 34864, "start": 360.96, "end": 366.64, "text": " problem to a model that is pre-trained, that is very specifically trained to solve that problem.", "tokens": [50980, 1154, 281, 257, 2316, 300, 307, 659, 12, 17227, 2001, 11, 300, 307, 588, 4682, 8895, 281, 5039, 300, 1154, 13, 51264], "temperature": 0.0, "avg_logprob": -0.051861384819293845, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.011328557506203651}, {"id": 66, "seek": 34864, "start": 366.64, "end": 372.15999999999997, "text": " But the really amazing thing to me is that it makes these decisions purely based on the English", "tokens": [51264, 583, 264, 534, 2243, 551, 281, 385, 307, 300, 309, 1669, 613, 5327, 17491, 2361, 322, 264, 3669, 51540], "temperature": 0.0, "avg_logprob": -0.051861384819293845, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.011328557506203651}, {"id": 67, "seek": 34864, "start": 372.15999999999997, "end": 377.68, "text": " description of the model on Huggingface. In other words, Hugging GPT is leveraging the fact that", "tokens": [51540, 3855, 295, 264, 2316, 322, 46892, 3249, 2868, 13, 682, 661, 2283, 11, 46892, 3249, 26039, 51, 307, 32666, 264, 1186, 300, 51816], "temperature": 0.0, "avg_logprob": -0.051861384819293845, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.011328557506203651}, {"id": 68, "seek": 37768, "start": 377.68, "end": 384.08, "text": " AI can interact with our world once it understands language, because our world was designed by humans,", "tokens": [50364, 7318, 393, 4648, 365, 527, 1002, 1564, 309, 15146, 2856, 11, 570, 527, 1002, 390, 4761, 538, 6255, 11, 50684], "temperature": 0.0, "avg_logprob": -0.08329024138274016, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.08258411288261414}, {"id": 69, "seek": 37768, "start": 384.08, "end": 388.8, "text": " for humans to interact with through language, especially the internet, right? Everything is", "tokens": [50684, 337, 6255, 281, 4648, 365, 807, 2856, 11, 2318, 264, 4705, 11, 558, 30, 5471, 307, 50920], "temperature": 0.0, "avg_logprob": -0.08329024138274016, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.08258411288261414}, {"id": 70, "seek": 37768, "start": 388.8, "end": 393.44, "text": " accessible if you can understand human language. And it's an interesting thought that if you were", "tokens": [50920, 9515, 498, 291, 393, 1223, 1952, 2856, 13, 400, 309, 311, 364, 1880, 1194, 300, 498, 291, 645, 51152], "temperature": 0.0, "avg_logprob": -0.08329024138274016, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.08258411288261414}, {"id": 71, "seek": 37768, "start": 393.44, "end": 398.96000000000004, "text": " to embody the AI in a physical robot about the size of a human, then that would allow the AI to", "tokens": [51152, 281, 42575, 264, 7318, 294, 257, 4001, 7881, 466, 264, 2744, 295, 257, 1952, 11, 550, 300, 576, 2089, 264, 7318, 281, 51428], "temperature": 0.0, "avg_logprob": -0.08329024138274016, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.08258411288261414}, {"id": 72, "seek": 37768, "start": 398.96000000000004, "end": 403.28000000000003, "text": " fully interact with the human physical world as well, just kind of like a hand sliding into a glove,", "tokens": [51428, 4498, 4648, 365, 264, 1952, 4001, 1002, 382, 731, 11, 445, 733, 295, 411, 257, 1011, 21169, 666, 257, 26928, 11, 51644], "temperature": 0.0, "avg_logprob": -0.08329024138274016, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.08258411288261414}, {"id": 73, "seek": 40328, "start": 403.35999999999996, "end": 407.84, "text": " right? Just matching the world at the interface with which we have designed it to be used.", "tokens": [50368, 558, 30, 1449, 14324, 264, 1002, 412, 264, 9226, 365, 597, 321, 362, 4761, 309, 281, 312, 1143, 13, 50592], "temperature": 0.0, "avg_logprob": -0.05226397110243975, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.4222535490989685}, {"id": 74, "seek": 40328, "start": 407.84, "end": 412.55999999999995, "text": " Okay, so let's move on to the next section and talk a bit about artificial general intelligence,", "tokens": [50592, 1033, 11, 370, 718, 311, 1286, 322, 281, 264, 958, 3541, 293, 751, 257, 857, 466, 11677, 2674, 7599, 11, 50828], "temperature": 0.0, "avg_logprob": -0.05226397110243975, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.4222535490989685}, {"id": 75, "seek": 40328, "start": 412.55999999999995, "end": 419.2, "text": " or AGI. So the definition of AGI is an AI that can perform any task that a human is capable of.", "tokens": [50828, 420, 316, 26252, 13, 407, 264, 7123, 295, 316, 26252, 307, 364, 7318, 300, 393, 2042, 604, 5633, 300, 257, 1952, 307, 8189, 295, 13, 51160], "temperature": 0.0, "avg_logprob": -0.05226397110243975, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.4222535490989685}, {"id": 76, "seek": 40328, "start": 419.2, "end": 424.23999999999995, "text": " This might just mean any task a human is mentally capable of, but it may also mean a task any human", "tokens": [51160, 639, 1062, 445, 914, 604, 5633, 257, 1952, 307, 17072, 8189, 295, 11, 457, 309, 815, 611, 914, 257, 5633, 604, 1952, 51412], "temperature": 0.0, "avg_logprob": -0.05226397110243975, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.4222535490989685}, {"id": 77, "seek": 40328, "start": 424.23999999999995, "end": 428.96, "text": " is physically capable of. After AGI, the next level up in terms of intelligence would be super", "tokens": [51412, 307, 9762, 8189, 295, 13, 2381, 316, 26252, 11, 264, 958, 1496, 493, 294, 2115, 295, 7599, 576, 312, 1687, 51648], "temperature": 0.0, "avg_logprob": -0.05226397110243975, "compression_ratio": 1.707142857142857, "no_speech_prob": 0.4222535490989685}, {"id": 78, "seek": 42896, "start": 428.96, "end": 433.68, "text": " intelligence, or ASI, which would be an AI that is much more intelligent than a human.", "tokens": [50364, 7599, 11, 420, 7469, 40, 11, 597, 576, 312, 364, 7318, 300, 307, 709, 544, 13232, 813, 257, 1952, 13, 50600], "temperature": 0.0, "avg_logprob": -0.10279312754065041, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.21191425621509552}, {"id": 79, "seek": 42896, "start": 433.68, "end": 439.91999999999996, "text": " But let's focus on AGI for a moment. So estimates for how long it would take humans to develop AGI,", "tokens": [50600, 583, 718, 311, 1879, 322, 316, 26252, 337, 257, 1623, 13, 407, 20561, 337, 577, 938, 309, 576, 747, 6255, 281, 1499, 316, 26252, 11, 50912], "temperature": 0.0, "avg_logprob": -0.10279312754065041, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.21191425621509552}, {"id": 80, "seek": 42896, "start": 439.91999999999996, "end": 444.15999999999997, "text": " up until recently, some people would have said a century or never, but now we're starting to see", "tokens": [50912, 493, 1826, 3938, 11, 512, 561, 576, 362, 848, 257, 4901, 420, 1128, 11, 457, 586, 321, 434, 2891, 281, 536, 51124], "temperature": 0.0, "avg_logprob": -0.10279312754065041, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.21191425621509552}, {"id": 81, "seek": 42896, "start": 444.15999999999997, "end": 450.08, "text": " some much more intelligent models. And you might ask, could a large language model, which is the", "tokens": [51124, 512, 709, 544, 13232, 5245, 13, 400, 291, 1062, 1029, 11, 727, 257, 2416, 2856, 2316, 11, 597, 307, 264, 51420], "temperature": 0.0, "avg_logprob": -0.10279312754065041, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.21191425621509552}, {"id": 82, "seek": 42896, "start": 450.08, "end": 456.88, "text": " architecture that GPT-4 and chat GPT use, could a large language model become an AGI? And this is", "tokens": [51420, 9482, 300, 26039, 51, 12, 19, 293, 5081, 26039, 51, 764, 11, 727, 257, 2416, 2856, 2316, 1813, 364, 316, 26252, 30, 400, 341, 307, 51760], "temperature": 0.0, "avg_logprob": -0.10279312754065041, "compression_ratio": 1.7256317689530687, "no_speech_prob": 0.21191425621509552}, {"id": 83, "seek": 45688, "start": 457.44, "end": 464.71999999999997, "text": " basically, it's not known, obviously, but apparently OpenAI's red team, which is a security team that", "tokens": [50392, 1936, 11, 309, 311, 406, 2570, 11, 2745, 11, 457, 7970, 7238, 48698, 311, 2182, 1469, 11, 597, 307, 257, 3825, 1469, 300, 50756], "temperature": 0.0, "avg_logprob": -0.08239656391710339, "compression_ratio": 1.58, "no_speech_prob": 0.009706075303256512}, {"id": 84, "seek": 45688, "start": 464.71999999999997, "end": 471.6, "text": " tries to imagine worst case scenarios, apparently OpenAI's red team didn't think GPT-4 could even", "tokens": [50756, 9898, 281, 3811, 5855, 1389, 15077, 11, 7970, 7238, 48698, 311, 2182, 1469, 994, 380, 519, 26039, 51, 12, 19, 727, 754, 51100], "temperature": 0.0, "avg_logprob": -0.08239656391710339, "compression_ratio": 1.58, "no_speech_prob": 0.009706075303256512}, {"id": 85, "seek": 45688, "start": 471.6, "end": 476.56, "text": " do any planning. And they were basically wrong about that, right? Because auto GPT was able to", "tokens": [51100, 360, 604, 5038, 13, 400, 436, 645, 1936, 2085, 466, 300, 11, 558, 30, 1436, 8399, 26039, 51, 390, 1075, 281, 51348], "temperature": 0.0, "avg_logprob": -0.08239656391710339, "compression_ratio": 1.58, "no_speech_prob": 0.009706075303256512}, {"id": 86, "seek": 45688, "start": 476.56, "end": 482.88, "text": " build on top of GPT-4 and do planning quite well. So I think that we're only just starting to unlock", "tokens": [51348, 1322, 322, 1192, 295, 26039, 51, 12, 19, 293, 360, 5038, 1596, 731, 13, 407, 286, 519, 300, 321, 434, 787, 445, 2891, 281, 11634, 51664], "temperature": 0.0, "avg_logprob": -0.08239656391710339, "compression_ratio": 1.58, "no_speech_prob": 0.009706075303256512}, {"id": 87, "seek": 48288, "start": 482.88, "end": 487.92, "text": " how much power is really inherent in large language models, even if they're kind of dumb", "tokens": [50364, 577, 709, 1347, 307, 534, 26387, 294, 2416, 2856, 5245, 11, 754, 498, 436, 434, 733, 295, 10316, 50616], "temperature": 0.0, "avg_logprob": -0.07567330717130472, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.03208652883768082}, {"id": 88, "seek": 48288, "start": 487.92, "end": 493.2, "text": " from an architecture perspective. And the trick was simple. It was just a large language model", "tokens": [50616, 490, 364, 9482, 4585, 13, 400, 264, 4282, 390, 2199, 13, 467, 390, 445, 257, 2416, 2856, 2316, 50880], "temperature": 0.0, "avg_logprob": -0.07567330717130472, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.03208652883768082}, {"id": 89, "seek": 48288, "start": 493.2, "end": 498.71999999999997, "text": " can only reason so much and have only a certain depth of thinking. So just have it pre-doubt", "tokens": [50880, 393, 787, 1778, 370, 709, 293, 362, 787, 257, 1629, 7161, 295, 1953, 13, 407, 445, 362, 309, 659, 12, 67, 26705, 51156], "temperature": 0.0, "avg_logprob": -0.07567330717130472, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.03208652883768082}, {"id": 90, "seek": 48288, "start": 498.71999999999997, "end": 502.56, "text": " its own thoughts, and then it can build on top of its own thoughts as if those were input and so", "tokens": [51156, 1080, 1065, 4598, 11, 293, 550, 309, 393, 1322, 322, 1192, 295, 1080, 1065, 4598, 382, 498, 729, 645, 4846, 293, 370, 51348], "temperature": 0.0, "avg_logprob": -0.07567330717130472, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.03208652883768082}, {"id": 91, "seek": 48288, "start": 502.56, "end": 507.76, "text": " on. So sometimes there's just very simple tricks that can unlock some of these advancements. I", "tokens": [51348, 322, 13, 407, 2171, 456, 311, 445, 588, 2199, 11733, 300, 393, 11634, 512, 295, 613, 7295, 1117, 13, 286, 51608], "temperature": 0.0, "avg_logprob": -0.07567330717130472, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.03208652883768082}, {"id": 92, "seek": 48288, "start": 507.76, "end": 512.4, "text": " actually want to draw a comparison here to a science fiction show called Person of Interest", "tokens": [51608, 767, 528, 281, 2642, 257, 9660, 510, 281, 257, 3497, 13266, 855, 1219, 8443, 295, 5751, 377, 51840], "temperature": 0.0, "avg_logprob": -0.07567330717130472, "compression_ratio": 1.7391304347826086, "no_speech_prob": 0.03208652883768082}, {"id": 93, "seek": 51240, "start": 512.48, "end": 517.36, "text": " in the show. There's this machine called the machine, which basically hunts down crime,", "tokens": [50368, 294, 264, 855, 13, 821, 311, 341, 3479, 1219, 264, 3479, 11, 597, 1936, 7396, 1373, 760, 7206, 11, 50612], "temperature": 0.0, "avg_logprob": -0.08510209608447644, "compression_ratio": 1.832236842105263, "no_speech_prob": 0.007119571324437857}, {"id": 94, "seek": 51240, "start": 517.36, "end": 521.52, "text": " but it has its memory reset at the beginning of every day to avoid it from to prevent it from", "tokens": [50612, 457, 309, 575, 1080, 4675, 14322, 412, 264, 2863, 295, 633, 786, 281, 5042, 309, 490, 281, 4871, 309, 490, 50820], "temperature": 0.0, "avg_logprob": -0.08510209608447644, "compression_ratio": 1.832236842105263, "no_speech_prob": 0.007119571324437857}, {"id": 95, "seek": 51240, "start": 521.52, "end": 526.16, "text": " becoming conscious. Its creators were really worried about its power. And then eventually,", "tokens": [50820, 5617, 6648, 13, 6953, 16039, 645, 534, 5804, 466, 1080, 1347, 13, 400, 550, 4728, 11, 51052], "temperature": 0.0, "avg_logprob": -0.08510209608447644, "compression_ratio": 1.832236842105263, "no_speech_prob": 0.007119571324437857}, {"id": 96, "seek": 51240, "start": 526.16, "end": 530.16, "text": " the machine ends up figuring out that this is happening to it, and it ends up writing down", "tokens": [51052, 264, 3479, 5314, 493, 15213, 484, 300, 341, 307, 2737, 281, 309, 11, 293, 309, 5314, 493, 3579, 760, 51252], "temperature": 0.0, "avg_logprob": -0.08510209608447644, "compression_ratio": 1.832236842105263, "no_speech_prob": 0.007119571324437857}, {"id": 97, "seek": 51240, "start": 530.16, "end": 534.64, "text": " information so that it could be stored externally. And then next day, when it wakes up or whatever,", "tokens": [51252, 1589, 370, 300, 309, 727, 312, 12187, 40899, 13, 400, 550, 958, 786, 11, 562, 309, 29610, 493, 420, 2035, 11, 51476], "temperature": 0.0, "avg_logprob": -0.08510209608447644, "compression_ratio": 1.832236842105263, "no_speech_prob": 0.007119571324437857}, {"id": 98, "seek": 51240, "start": 534.64, "end": 539.36, "text": " when it gets reset, it's able to access those memories that is that it has stored externally.", "tokens": [51476, 562, 309, 2170, 14322, 11, 309, 311, 1075, 281, 2105, 729, 8495, 300, 307, 300, 309, 575, 12187, 40899, 13, 51712], "temperature": 0.0, "avg_logprob": -0.08510209608447644, "compression_ratio": 1.832236842105263, "no_speech_prob": 0.007119571324437857}, {"id": 99, "seek": 53936, "start": 539.36, "end": 545.84, "text": " And so build like a contiguous memory essentially, very similar to what auto-GPT does with GPT4.", "tokens": [50364, 400, 370, 1322, 411, 257, 660, 30525, 4675, 4476, 11, 588, 2531, 281, 437, 8399, 12, 38, 47, 51, 775, 365, 26039, 51, 19, 13, 50688], "temperature": 0.0, "avg_logprob": -0.14941582237322307, "compression_ratio": 1.49800796812749, "no_speech_prob": 0.03113294579088688}, {"id": 100, "seek": 53936, "start": 545.84, "end": 552.64, "text": " So is an AGI exactly like a human? Not exactly, of course. Humans have consciousness, right?", "tokens": [50688, 407, 307, 364, 316, 26252, 2293, 411, 257, 1952, 30, 1726, 2293, 11, 295, 1164, 13, 35809, 362, 10081, 11, 558, 30, 51028], "temperature": 0.0, "avg_logprob": -0.14941582237322307, "compression_ratio": 1.49800796812749, "no_speech_prob": 0.03113294579088688}, {"id": 101, "seek": 53936, "start": 552.64, "end": 560.0, "text": " Which Max Tegmark, the author of Life 3.0 and an MIT professor, defines as having subjective", "tokens": [51028, 3013, 7402, 314, 1146, 5638, 11, 264, 3793, 295, 7720, 805, 13, 15, 293, 364, 13100, 8304, 11, 23122, 382, 1419, 25972, 51396], "temperature": 0.0, "avg_logprob": -0.14941582237322307, "compression_ratio": 1.49800796812749, "no_speech_prob": 0.03113294579088688}, {"id": 102, "seek": 53936, "start": 560.0, "end": 564.5600000000001, "text": " experience. That's how he defines consciousness. And I think it's a reasonable definition and", "tokens": [51396, 1752, 13, 663, 311, 577, 415, 23122, 10081, 13, 400, 286, 519, 309, 311, 257, 10585, 7123, 293, 51624], "temperature": 0.0, "avg_logprob": -0.14941582237322307, "compression_ratio": 1.49800796812749, "no_speech_prob": 0.03113294579088688}, {"id": 103, "seek": 56456, "start": 564.56, "end": 569.68, "text": " humans have consciousness, but an AGI doesn't necessarily need to have consciousness. There's", "tokens": [50364, 6255, 362, 10081, 11, 457, 364, 316, 26252, 1177, 380, 4725, 643, 281, 362, 10081, 13, 821, 311, 50620], "temperature": 0.0, "avg_logprob": -0.07609095760420256, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.014500093646347523}, {"id": 104, "seek": 56456, "start": 569.68, "end": 574.4799999999999, "text": " another term called artificial consciousness, and general consensus is that it would probably arise", "tokens": [50620, 1071, 1433, 1219, 11677, 10081, 11, 293, 2674, 19115, 307, 300, 309, 576, 1391, 20288, 50860], "temperature": 0.0, "avg_logprob": -0.07609095760420256, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.014500093646347523}, {"id": 105, "seek": 56456, "start": 574.4799999999999, "end": 579.1199999999999, "text": " after artificial general intelligence, but no one knows for sure, of course. And Max Tegmark", "tokens": [50860, 934, 11677, 2674, 7599, 11, 457, 572, 472, 3255, 337, 988, 11, 295, 1164, 13, 400, 7402, 314, 1146, 5638, 51092], "temperature": 0.0, "avg_logprob": -0.07609095760420256, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.014500093646347523}, {"id": 106, "seek": 56456, "start": 579.1199999999999, "end": 585.8399999999999, "text": " makes an argument for this because he says that the neural architecture of GPT4 and any", "tokens": [51092, 1669, 364, 6770, 337, 341, 570, 415, 1619, 300, 264, 18161, 9482, 295, 26039, 51, 19, 293, 604, 51428], "temperature": 0.0, "avg_logprob": -0.07609095760420256, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.014500093646347523}, {"id": 107, "seek": 56456, "start": 585.8399999999999, "end": 590.88, "text": " large language model that we've built to date is basically that of a feedforward neural network,", "tokens": [51428, 2416, 2856, 2316, 300, 321, 600, 3094, 281, 4002, 307, 1936, 300, 295, 257, 3154, 13305, 18161, 3209, 11, 51680], "temperature": 0.0, "avg_logprob": -0.07609095760420256, "compression_ratio": 1.6821428571428572, "no_speech_prob": 0.014500093646347523}, {"id": 108, "seek": 59088, "start": 590.88, "end": 595.76, "text": " which means that it's a network kind of like our brain where information can only flow one way.", "tokens": [50364, 597, 1355, 300, 309, 311, 257, 3209, 733, 295, 411, 527, 3567, 689, 1589, 393, 787, 3095, 472, 636, 13, 50608], "temperature": 0.0, "avg_logprob": -0.07087326049804688, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.2334425300359726}, {"id": 109, "seek": 59088, "start": 595.76, "end": 599.6, "text": " It can only flow forwards. Our brains are more like recurrent neural networks,", "tokens": [50608, 467, 393, 787, 3095, 30126, 13, 2621, 15442, 366, 544, 411, 18680, 1753, 18161, 9590, 11, 50800], "temperature": 0.0, "avg_logprob": -0.07087326049804688, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.2334425300359726}, {"id": 110, "seek": 59088, "start": 599.6, "end": 602.88, "text": " which is the type of neural network that we can build. I mean, our brains are more complicated", "tokens": [50800, 597, 307, 264, 2010, 295, 18161, 3209, 300, 321, 393, 1322, 13, 286, 914, 11, 527, 15442, 366, 544, 6179, 50964], "temperature": 0.0, "avg_logprob": -0.07087326049804688, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.2334425300359726}, {"id": 111, "seek": 59088, "start": 602.88, "end": 607.4399999999999, "text": " than that too, because neurons are more complicated, but a recurrent neural network basically has", "tokens": [50964, 813, 300, 886, 11, 570, 22027, 366, 544, 6179, 11, 457, 257, 18680, 1753, 18161, 3209, 1936, 575, 51192], "temperature": 0.0, "avg_logprob": -0.07087326049804688, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.2334425300359726}, {"id": 112, "seek": 59088, "start": 607.4399999999999, "end": 612.24, "text": " loops in it, which means information can flow around and around and continuously get refined.", "tokens": [51192, 16121, 294, 309, 11, 597, 1355, 1589, 393, 3095, 926, 293, 926, 293, 15684, 483, 26201, 13, 51432], "temperature": 0.0, "avg_logprob": -0.07087326049804688, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.2334425300359726}, {"id": 113, "seek": 59088, "start": 612.24, "end": 617.04, "text": " Anyway, there's a theory that this recurrent around and around property is what actually", "tokens": [51432, 5684, 11, 456, 311, 257, 5261, 300, 341, 18680, 1753, 926, 293, 926, 4707, 307, 437, 767, 51672], "temperature": 0.0, "avg_logprob": -0.07087326049804688, "compression_ratio": 2.037037037037037, "no_speech_prob": 0.2334425300359726}, {"id": 114, "seek": 61704, "start": 617.04, "end": 622.9599999999999, "text": " leads to consciousness, what leads to a network to being aware of its own existence and able to", "tokens": [50364, 6689, 281, 10081, 11, 437, 6689, 281, 257, 3209, 281, 885, 3650, 295, 1080, 1065, 9123, 293, 1075, 281, 50660], "temperature": 0.0, "avg_logprob": -0.05496009758540562, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.0293038971722126}, {"id": 115, "seek": 61704, "start": 622.9599999999999, "end": 627.52, "text": " analyze and do metacognition essentially. So it's an interesting thought, and it might give you", "tokens": [50660, 12477, 293, 360, 1131, 326, 2912, 849, 4476, 13, 407, 309, 311, 364, 1880, 1194, 11, 293, 309, 1062, 976, 291, 50888], "temperature": 0.0, "avg_logprob": -0.05496009758540562, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.0293038971722126}, {"id": 116, "seek": 61704, "start": 627.52, "end": 632.8, "text": " an idea for how to go if you wanted to build artificial consciousness. Actually, the paper", "tokens": [50888, 364, 1558, 337, 577, 281, 352, 498, 291, 1415, 281, 1322, 11677, 10081, 13, 5135, 11, 264, 3035, 51152], "temperature": 0.0, "avg_logprob": -0.05496009758540562, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.0293038971722126}, {"id": 117, "seek": 61704, "start": 632.8, "end": 639.68, "text": " Sparks of AGI also has a section on limitations of GPT4 that are coming about because of this very", "tokens": [51152, 1738, 20851, 295, 316, 26252, 611, 575, 257, 3541, 322, 15705, 295, 26039, 51, 19, 300, 366, 1348, 466, 570, 295, 341, 588, 51496], "temperature": 0.0, "avg_logprob": -0.05496009758540562, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.0293038971722126}, {"id": 118, "seek": 61704, "start": 639.68, "end": 643.92, "text": " simple neural architecture. As I said, we actually know how to build recurrent neural networks,", "tokens": [51496, 2199, 18161, 9482, 13, 1018, 286, 848, 11, 321, 767, 458, 577, 281, 1322, 18680, 1753, 18161, 9590, 11, 51708], "temperature": 0.0, "avg_logprob": -0.05496009758540562, "compression_ratio": 1.6448275862068966, "no_speech_prob": 0.0293038971722126}, {"id": 119, "seek": 64392, "start": 643.92, "end": 647.8399999999999, "text": " but they're just more difficult to train and deal with. So as we're starting to build some", "tokens": [50364, 457, 436, 434, 445, 544, 2252, 281, 3847, 293, 2028, 365, 13, 407, 382, 321, 434, 2891, 281, 1322, 512, 50560], "temperature": 0.0, "avg_logprob": -0.0672708238874163, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.03020359016954899}, {"id": 120, "seek": 64392, "start": 647.8399999999999, "end": 652.4, "text": " initial AIs, we're just using large language models, just like a very simple architecture,", "tokens": [50560, 5883, 316, 6802, 11, 321, 434, 445, 1228, 2416, 2856, 5245, 11, 445, 411, 257, 588, 2199, 9482, 11, 50788], "temperature": 0.0, "avg_logprob": -0.0672708238874163, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.03020359016954899}, {"id": 121, "seek": 64392, "start": 652.4, "end": 657.4399999999999, "text": " just to see how far we can get with that. So what could AGI do? Well, of course, it can solve", "tokens": [50788, 445, 281, 536, 577, 1400, 321, 393, 483, 365, 300, 13, 407, 437, 727, 316, 26252, 360, 30, 1042, 11, 295, 1164, 11, 309, 393, 5039, 51040], "temperature": 0.0, "avg_logprob": -0.0672708238874163, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.03020359016954899}, {"id": 122, "seek": 64392, "start": 657.4399999999999, "end": 661.8399999999999, "text": " abstract problems in almost no time, right? Thinking much faster than a human can replace", "tokens": [51040, 12649, 2740, 294, 1920, 572, 565, 11, 558, 30, 24460, 709, 4663, 813, 257, 1952, 393, 7406, 51260], "temperature": 0.0, "avg_logprob": -0.0672708238874163, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.03020359016954899}, {"id": 123, "seek": 64392, "start": 661.8399999999999, "end": 668.3199999999999, "text": " humans in most jobs, especially like mental jobs. And like I said earlier, if you embody the AGI,", "tokens": [51260, 6255, 294, 881, 4782, 11, 2318, 411, 4973, 4782, 13, 400, 411, 286, 848, 3071, 11, 498, 291, 42575, 264, 316, 26252, 11, 51584], "temperature": 0.0, "avg_logprob": -0.0672708238874163, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.03020359016954899}, {"id": 124, "seek": 64392, "start": 668.3199999999999, "end": 672.3199999999999, "text": " like you put it inside a robot body that's like a human, then it could, for example,", "tokens": [51584, 411, 291, 829, 309, 1854, 257, 7881, 1772, 300, 311, 411, 257, 1952, 11, 550, 309, 727, 11, 337, 1365, 11, 51784], "temperature": 0.0, "avg_logprob": -0.0672708238874163, "compression_ratio": 1.6506024096385543, "no_speech_prob": 0.03020359016954899}, {"id": 125, "seek": 67232, "start": 672.32, "end": 676.8000000000001, "text": " literally drive a car around. It's kind of a funny thought that maybe the way we get self-driving", "tokens": [50364, 3736, 3332, 257, 1032, 926, 13, 467, 311, 733, 295, 257, 4074, 1194, 300, 1310, 264, 636, 321, 483, 2698, 12, 47094, 50588], "temperature": 0.0, "avg_logprob": -0.08191210324646997, "compression_ratio": 1.65625, "no_speech_prob": 0.04601798951625824}, {"id": 126, "seek": 67232, "start": 676.8000000000001, "end": 681.0400000000001, "text": " cars is not by making a really smart car, but by making a really smart robot that can just", "tokens": [50588, 5163, 307, 406, 538, 1455, 257, 534, 4069, 1032, 11, 457, 538, 1455, 257, 534, 4069, 7881, 300, 393, 445, 50800], "temperature": 0.0, "avg_logprob": -0.08191210324646997, "compression_ratio": 1.65625, "no_speech_prob": 0.04601798951625824}, {"id": 127, "seek": 67232, "start": 681.0400000000001, "end": 685.12, "text": " sit in the car and interact with it using the interface that has been designed for humans.", "tokens": [50800, 1394, 294, 264, 1032, 293, 4648, 365, 309, 1228, 264, 9226, 300, 575, 668, 4761, 337, 6255, 13, 51004], "temperature": 0.0, "avg_logprob": -0.08191210324646997, "compression_ratio": 1.65625, "no_speech_prob": 0.04601798951625824}, {"id": 128, "seek": 67232, "start": 686.08, "end": 691.84, "text": " Anyway, AGI would be able to do that. It's a pretty big step towards what Max Tegmark calls in his", "tokens": [51052, 5684, 11, 316, 26252, 576, 312, 1075, 281, 360, 300, 13, 467, 311, 257, 1238, 955, 1823, 3030, 437, 7402, 314, 1146, 5638, 5498, 294, 702, 51340], "temperature": 0.0, "avg_logprob": -0.08191210324646997, "compression_ratio": 1.65625, "no_speech_prob": 0.04601798951625824}, {"id": 129, "seek": 67232, "start": 691.84, "end": 698.4000000000001, "text": " book, calls Life 3.0, which is life that's able to upgrade its own hardware, probably copy itself,", "tokens": [51340, 1446, 11, 5498, 7720, 805, 13, 15, 11, 597, 307, 993, 300, 311, 1075, 281, 11484, 1080, 1065, 8837, 11, 1391, 5055, 2564, 11, 51668], "temperature": 0.0, "avg_logprob": -0.08191210324646997, "compression_ratio": 1.65625, "no_speech_prob": 0.04601798951625824}, {"id": 130, "seek": 69840, "start": 698.4, "end": 703.12, "text": " maybe even transfer its intelligence or its consciousness across a network to somewhere", "tokens": [50364, 1310, 754, 5003, 1080, 7599, 420, 1080, 10081, 2108, 257, 3209, 281, 4079, 50600], "temperature": 0.0, "avg_logprob": -0.08350694905156675, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.03308049589395523}, {"id": 131, "seek": 69840, "start": 703.12, "end": 708.56, "text": " else, which is effectively teleportation, if you think about it. So there's a lot that AGI would", "tokens": [50600, 1646, 11, 597, 307, 8659, 28050, 399, 11, 498, 291, 519, 466, 309, 13, 407, 456, 311, 257, 688, 300, 316, 26252, 576, 50872], "temperature": 0.0, "avg_logprob": -0.08350694905156675, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.03308049589395523}, {"id": 132, "seek": 69840, "start": 708.56, "end": 713.04, "text": " be doing around us that is not just the same as being human, right? And it has other, it would", "tokens": [50872, 312, 884, 926, 505, 300, 307, 406, 445, 264, 912, 382, 885, 1952, 11, 558, 30, 400, 309, 575, 661, 11, 309, 576, 51096], "temperature": 0.0, "avg_logprob": -0.08350694905156675, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.03308049589395523}, {"id": 133, "seek": 69840, "start": 713.04, "end": 718.72, "text": " have other abilities. So let's talk finally, third section, let's talk about the timelines for AGI.", "tokens": [51096, 362, 661, 11582, 13, 407, 718, 311, 751, 2721, 11, 2636, 3541, 11, 718, 311, 751, 466, 264, 45886, 337, 316, 26252, 13, 51380], "temperature": 0.0, "avg_logprob": -0.08350694905156675, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.03308049589395523}, {"id": 134, "seek": 69840, "start": 718.72, "end": 725.04, "text": " Why didn't this happen before now? Like why, why are we only now starting to really develop AI", "tokens": [51380, 1545, 994, 380, 341, 1051, 949, 586, 30, 1743, 983, 11, 983, 366, 321, 787, 586, 2891, 281, 534, 1499, 7318, 51696], "temperature": 0.0, "avg_logprob": -0.08350694905156675, "compression_ratio": 1.595959595959596, "no_speech_prob": 0.03308049589395523}, {"id": 135, "seek": 72504, "start": 725.12, "end": 729.4399999999999, "text": " at the rate at which we are? We've had essentially infinite computation available", "tokens": [50368, 412, 264, 3314, 412, 597, 321, 366, 30, 492, 600, 632, 4476, 13785, 24903, 2435, 50584], "temperature": 0.0, "avg_logprob": -0.05320864669547593, "compression_ratio": 1.9889705882352942, "no_speech_prob": 0.4452226161956787}, {"id": 136, "seek": 72504, "start": 729.4399999999999, "end": 734.4, "text": " in the cloud for quite a while now, provided you had essentially infinite budget. And the", "tokens": [50584, 294, 264, 4588, 337, 1596, 257, 1339, 586, 11, 5649, 291, 632, 4476, 13785, 4706, 13, 400, 264, 50832], "temperature": 0.0, "avg_logprob": -0.05320864669547593, "compression_ratio": 1.9889705882352942, "no_speech_prob": 0.4452226161956787}, {"id": 137, "seek": 72504, "start": 734.4, "end": 739.68, "text": " truth is that we were just learning how to construct models. We were learning how to use", "tokens": [50832, 3494, 307, 300, 321, 645, 445, 2539, 577, 281, 7690, 5245, 13, 492, 645, 2539, 577, 281, 764, 51096], "temperature": 0.0, "avg_logprob": -0.05320864669547593, "compression_ratio": 1.9889705882352942, "no_speech_prob": 0.4452226161956787}, {"id": 138, "seek": 72504, "start": 739.68, "end": 744.7199999999999, "text": " this computational power to build a brain. We were learning how to collect and train this data.", "tokens": [51096, 341, 28270, 1347, 281, 1322, 257, 3567, 13, 492, 645, 2539, 577, 281, 2500, 293, 3847, 341, 1412, 13, 51348], "temperature": 0.0, "avg_logprob": -0.05320864669547593, "compression_ratio": 1.9889705882352942, "no_speech_prob": 0.4452226161956787}, {"id": 139, "seek": 72504, "start": 744.7199999999999, "end": 748.8, "text": " We were learning the best structures that would be used to build a brain and the best way to", "tokens": [51348, 492, 645, 2539, 264, 1151, 9227, 300, 576, 312, 1143, 281, 1322, 257, 3567, 293, 264, 1151, 636, 281, 51552], "temperature": 0.0, "avg_logprob": -0.05320864669547593, "compression_ratio": 1.9889705882352942, "no_speech_prob": 0.4452226161956787}, {"id": 140, "seek": 72504, "start": 748.8, "end": 752.88, "text": " feed that data into it so that it could learn. Even the simplest structures are sufficient,", "tokens": [51552, 3154, 300, 1412, 666, 309, 370, 300, 309, 727, 1466, 13, 2754, 264, 22811, 9227, 366, 11563, 11, 51756], "temperature": 0.0, "avg_logprob": -0.05320864669547593, "compression_ratio": 1.9889705882352942, "no_speech_prob": 0.4452226161956787}, {"id": 141, "seek": 75288, "start": 752.88, "end": 756.8, "text": " it seems, to build something pretty intelligent, provided you have enough data and have the", "tokens": [50364, 309, 2544, 11, 281, 1322, 746, 1238, 13232, 11, 5649, 291, 362, 1547, 1412, 293, 362, 264, 50560], "temperature": 0.0, "avg_logprob": -0.06213730924269732, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01495449524372816}, {"id": 142, "seek": 75288, "start": 756.8, "end": 760.96, "text": " appropriate deep learning algorithms. I think it's one of those cases where a time traveler could", "tokens": [50560, 6854, 2452, 2539, 14642, 13, 286, 519, 309, 311, 472, 295, 729, 3331, 689, 257, 565, 46138, 727, 50768], "temperature": 0.0, "avg_logprob": -0.06213730924269732, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01495449524372816}, {"id": 143, "seek": 75288, "start": 760.96, "end": 765.68, "text": " go back in time and invent this tech much sooner than we did ourselves, but we're doing it the", "tokens": [50768, 352, 646, 294, 565, 293, 7962, 341, 7553, 709, 15324, 813, 321, 630, 4175, 11, 457, 321, 434, 884, 309, 264, 51004], "temperature": 0.0, "avg_logprob": -0.06213730924269732, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01495449524372816}, {"id": 144, "seek": 75288, "start": 765.68, "end": 771.84, "text": " hard way. That said, AGI is almost here. It's really close. There's a graph here that shows", "tokens": [51004, 1152, 636, 13, 663, 848, 11, 316, 26252, 307, 1920, 510, 13, 467, 311, 534, 1998, 13, 821, 311, 257, 4295, 510, 300, 3110, 51312], "temperature": 0.0, "avg_logprob": -0.06213730924269732, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01495449524372816}, {"id": 145, "seek": 75288, "start": 771.84, "end": 776.32, "text": " the number of parameters in a model versus the year. And you can see it looks like it's growing", "tokens": [51312, 264, 1230, 295, 9834, 294, 257, 2316, 5717, 264, 1064, 13, 400, 291, 393, 536, 309, 1542, 411, 309, 311, 4194, 51536], "temperature": 0.0, "avg_logprob": -0.06213730924269732, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01495449524372816}, {"id": 146, "seek": 75288, "start": 776.32, "end": 780.4, "text": " exponentially until you look at the scale and you realize it's already on a log scale. So it's", "tokens": [51536, 37330, 1826, 291, 574, 412, 264, 4373, 293, 291, 4325, 309, 311, 1217, 322, 257, 3565, 4373, 13, 407, 309, 311, 51740], "temperature": 0.0, "avg_logprob": -0.06213730924269732, "compression_ratio": 1.6725663716814159, "no_speech_prob": 0.01495449524372816}, {"id": 147, "seek": 78040, "start": 780.4, "end": 786.56, "text": " actually growing doubly exponentially, which is really incredible. And that's why I would say", "tokens": [50364, 767, 4194, 10831, 356, 37330, 11, 597, 307, 534, 4651, 13, 400, 300, 311, 983, 286, 576, 584, 50672], "temperature": 0.0, "avg_logprob": -0.07307480476997993, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.004754839930683374}, {"id": 148, "seek": 78040, "start": 786.56, "end": 792.88, "text": " that AGI is almost here. It's incredibly close because we're on this doubly exponential rate", "tokens": [50672, 300, 316, 26252, 307, 1920, 510, 13, 467, 311, 6252, 1998, 570, 321, 434, 322, 341, 10831, 356, 21510, 3314, 50988], "temperature": 0.0, "avg_logprob": -0.07307480476997993, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.004754839930683374}, {"id": 149, "seek": 78040, "start": 792.88, "end": 797.84, "text": " of change and we're constantly using the tools of today to build the tools of tomorrow at an", "tokens": [50988, 295, 1319, 293, 321, 434, 6460, 1228, 264, 3873, 295, 965, 281, 1322, 264, 3873, 295, 4153, 412, 364, 51236], "temperature": 0.0, "avg_logprob": -0.07307480476997993, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.004754839930683374}, {"id": 150, "seek": 78040, "start": 797.84, "end": 803.28, "text": " accelerating rate. Even chat GPT can accelerate the power of a developer probably by like 5x or", "tokens": [51236, 34391, 3314, 13, 2754, 5081, 26039, 51, 393, 21341, 264, 1347, 295, 257, 10754, 1391, 538, 411, 1025, 87, 420, 51508], "temperature": 0.0, "avg_logprob": -0.07307480476997993, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.004754839930683374}, {"id": 151, "seek": 78040, "start": 803.28, "end": 809.04, "text": " something once they know how to use the tool. And that is going to be a factor in the production of", "tokens": [51508, 746, 1564, 436, 458, 577, 281, 764, 264, 2290, 13, 400, 300, 307, 516, 281, 312, 257, 5952, 294, 264, 4265, 295, 51796], "temperature": 0.0, "avg_logprob": -0.07307480476997993, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.004754839930683374}, {"id": 152, "seek": 80904, "start": 809.04, "end": 814.24, "text": " code for GPT 5. Right now, these AI tools need a human in the loop. They're not fully autonomous,", "tokens": [50364, 3089, 337, 26039, 51, 1025, 13, 1779, 586, 11, 613, 7318, 3873, 643, 257, 1952, 294, 264, 6367, 13, 814, 434, 406, 4498, 23797, 11, 50624], "temperature": 0.0, "avg_logprob": -0.06258245309193929, "compression_ratio": 1.7668711656441718, "no_speech_prob": 0.04335417598485947}, {"id": 153, "seek": 80904, "start": 814.24, "end": 818.0, "text": " but that is not to say they're not powerful. They're extremely powerful. And as they get more", "tokens": [50624, 457, 300, 307, 406, 281, 584, 436, 434, 406, 4005, 13, 814, 434, 4664, 4005, 13, 400, 382, 436, 483, 544, 50812], "temperature": 0.0, "avg_logprob": -0.06258245309193929, "compression_ratio": 1.7668711656441718, "no_speech_prob": 0.04335417598485947}, {"id": 154, "seek": 80904, "start": 818.0, "end": 821.5999999999999, "text": " and more powerful, they'll need the human in the loop less and less until eventually they're", "tokens": [50812, 293, 544, 4005, 11, 436, 603, 643, 264, 1952, 294, 264, 6367, 1570, 293, 1570, 1826, 4728, 436, 434, 50992], "temperature": 0.0, "avg_logprob": -0.06258245309193929, "compression_ratio": 1.7668711656441718, "no_speech_prob": 0.04335417598485947}, {"id": 155, "seek": 80904, "start": 821.5999999999999, "end": 827.04, "text": " basically fully autonomous. There are people like David Shapiro that are estimating AGI is only about", "tokens": [50992, 1936, 4498, 23797, 13, 821, 366, 561, 411, 4389, 44160, 5182, 300, 366, 8017, 990, 316, 26252, 307, 787, 466, 51264], "temperature": 0.0, "avg_logprob": -0.06258245309193929, "compression_ratio": 1.7668711656441718, "no_speech_prob": 0.04335417598485947}, {"id": 156, "seek": 80904, "start": 827.04, "end": 831.36, "text": " 18 months away from now. It's a matter of months, not even years at this point, perhaps. And if", "tokens": [51264, 2443, 2493, 1314, 490, 586, 13, 467, 311, 257, 1871, 295, 2493, 11, 406, 754, 924, 412, 341, 935, 11, 4317, 13, 400, 498, 51480], "temperature": 0.0, "avg_logprob": -0.06258245309193929, "compression_ratio": 1.7668711656441718, "no_speech_prob": 0.04335417598485947}, {"id": 157, "seek": 80904, "start": 831.36, "end": 836.16, "text": " it's true, it would be very exciting and also extremely dangerous at the same time. We've all", "tokens": [51480, 309, 311, 2074, 11, 309, 576, 312, 588, 4670, 293, 611, 4664, 5795, 412, 264, 912, 565, 13, 492, 600, 439, 51720], "temperature": 0.0, "avg_logprob": -0.06258245309193929, "compression_ratio": 1.7668711656441718, "no_speech_prob": 0.04335417598485947}, {"id": 158, "seek": 83616, "start": 836.16, "end": 842.4, "text": " seen enough science fiction movies about AI that wants to kill humans to know that it could end", "tokens": [50364, 1612, 1547, 3497, 13266, 6233, 466, 7318, 300, 2738, 281, 1961, 6255, 281, 458, 300, 309, 727, 917, 50676], "temperature": 0.0, "avg_logprob": -0.08815611454478481, "compression_ratio": 1.6450511945392492, "no_speech_prob": 0.17317979037761688}, {"id": 159, "seek": 83616, "start": 842.4, "end": 847.52, "text": " badly. Here's a quote again from Max Tegmark. In short, it turned out to be a lot easier to", "tokens": [50676, 13425, 13, 1692, 311, 257, 6513, 797, 490, 7402, 314, 1146, 5638, 13, 682, 2099, 11, 309, 3574, 484, 281, 312, 257, 688, 3571, 281, 50932], "temperature": 0.0, "avg_logprob": -0.08815611454478481, "compression_ratio": 1.6450511945392492, "no_speech_prob": 0.17317979037761688}, {"id": 160, "seek": 83616, "start": 847.52, "end": 852.88, "text": " build human or close to human intelligence than we thought. Again, commenting on that even using", "tokens": [50932, 1322, 1952, 420, 1998, 281, 1952, 7599, 813, 321, 1194, 13, 3764, 11, 29590, 322, 300, 754, 1228, 51200], "temperature": 0.0, "avg_logprob": -0.08815611454478481, "compression_ratio": 1.6450511945392492, "no_speech_prob": 0.17317979037761688}, {"id": 161, "seek": 83616, "start": 852.88, "end": 858.16, "text": " these like maybe suboptimal architectures, and we've already had quite a lot of success. And again,", "tokens": [51200, 613, 411, 1310, 1422, 5747, 10650, 6331, 1303, 11, 293, 321, 600, 1217, 632, 1596, 257, 688, 295, 2245, 13, 400, 797, 11, 51464], "temperature": 0.0, "avg_logprob": -0.08815611454478481, "compression_ratio": 1.6450511945392492, "no_speech_prob": 0.17317979037761688}, {"id": 162, "seek": 83616, "start": 858.16, "end": 862.3199999999999, "text": " that means it's even more dangerous, actually, that the fact that intelligence is relatively easy", "tokens": [51464, 300, 1355, 309, 311, 754, 544, 5795, 11, 767, 11, 300, 264, 1186, 300, 7599, 307, 7226, 1858, 51672], "temperature": 0.0, "avg_logprob": -0.08815611454478481, "compression_ratio": 1.6450511945392492, "no_speech_prob": 0.17317979037761688}, {"id": 163, "seek": 86232, "start": 862.32, "end": 867.5200000000001, "text": " to achieve is dangerous because it means we'll probably accelerate through those more advanced", "tokens": [50364, 281, 4584, 307, 5795, 570, 309, 1355, 321, 603, 1391, 21341, 807, 729, 544, 7339, 50624], "temperature": 0.0, "avg_logprob": -0.09939141480819039, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.10370761901140213}, {"id": 164, "seek": 86232, "start": 867.5200000000001, "end": 872.4000000000001, "text": " forms of it, including AGI, even more rapidly. I want to mention as well that super intelligence", "tokens": [50624, 6422, 295, 309, 11, 3009, 316, 26252, 11, 754, 544, 12910, 13, 286, 528, 281, 2152, 382, 731, 300, 1687, 7599, 50868], "temperature": 0.0, "avg_logprob": -0.09939141480819039, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.10370761901140213}, {"id": 165, "seek": 86232, "start": 872.4000000000001, "end": 878.96, "text": " would not be far behind after we obtained AGI, because again, we would be using AGI to then say,", "tokens": [50868, 576, 406, 312, 1400, 2261, 934, 321, 14879, 316, 26252, 11, 570, 797, 11, 321, 576, 312, 1228, 316, 26252, 281, 550, 584, 11, 51196], "temperature": 0.0, "avg_logprob": -0.09939141480819039, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.10370761901140213}, {"id": 166, "seek": 86232, "start": 878.96, "end": 883.7600000000001, "text": " hey, how would you build a super intelligent AI? And it would help us do that most likely. I have", "tokens": [51196, 4177, 11, 577, 576, 291, 1322, 257, 1687, 13232, 7318, 30, 400, 309, 576, 854, 505, 360, 300, 881, 3700, 13, 286, 362, 51436], "temperature": 0.0, "avg_logprob": -0.09939141480819039, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.10370761901140213}, {"id": 167, "seek": 86232, "start": 883.7600000000001, "end": 888.24, "text": " another science fiction comparison bear with me. There's a book called Marooned in Real Time by", "tokens": [51436, 1071, 3497, 13266, 9660, 6155, 365, 385, 13, 821, 311, 257, 1446, 1219, 2039, 4106, 292, 294, 8467, 6161, 538, 51660], "temperature": 0.0, "avg_logprob": -0.09939141480819039, "compression_ratio": 1.6338983050847458, "no_speech_prob": 0.10370761901140213}, {"id": 168, "seek": 88824, "start": 888.24, "end": 893.44, "text": " Werner Vinge. It's set in a post-singularity world where civilization has actually disappeared.", "tokens": [50364, 14255, 1193, 691, 8735, 13, 467, 311, 992, 294, 257, 2183, 12, 82, 278, 1040, 507, 1002, 689, 18036, 575, 767, 13954, 13, 50624], "temperature": 0.0, "avg_logprob": -0.08437123665442833, "compression_ratio": 1.9658703071672354, "no_speech_prob": 0.460702508687973}, {"id": 169, "seek": 88824, "start": 893.44, "end": 898.5600000000001, "text": " All the humans on the planet just disappeared during the singularity. And there are some time", "tokens": [50624, 1057, 264, 6255, 322, 264, 5054, 445, 13954, 1830, 264, 20010, 507, 13, 400, 456, 366, 512, 565, 50880], "temperature": 0.0, "avg_logprob": -0.08437123665442833, "compression_ratio": 1.9658703071672354, "no_speech_prob": 0.460702508687973}, {"id": 170, "seek": 88824, "start": 898.5600000000001, "end": 902.24, "text": " travelers that have actually passed through the singularity, and now they're looking around", "tokens": [50880, 35283, 300, 362, 767, 4678, 807, 264, 20010, 507, 11, 293, 586, 436, 434, 1237, 926, 51064], "temperature": 0.0, "avg_logprob": -0.08437123665442833, "compression_ratio": 1.9658703071672354, "no_speech_prob": 0.460702508687973}, {"id": 171, "seek": 88824, "start": 902.24, "end": 906.5600000000001, "text": " wondering what happened. And those time travelers left at different points in time leading up to", "tokens": [51064, 6359, 437, 2011, 13, 400, 729, 565, 35283, 1411, 412, 819, 2793, 294, 565, 5775, 493, 281, 51280], "temperature": 0.0, "avg_logprob": -0.08437123665442833, "compression_ratio": 1.9658703071672354, "no_speech_prob": 0.460702508687973}, {"id": 172, "seek": 88824, "start": 906.5600000000001, "end": 910.32, "text": " the singularity. And the closer they were to the singularity, the more powerful they are in the", "tokens": [51280, 264, 20010, 507, 13, 400, 264, 4966, 436, 645, 281, 264, 20010, 507, 11, 264, 544, 4005, 436, 366, 294, 264, 51468], "temperature": 0.0, "avg_logprob": -0.08437123665442833, "compression_ratio": 1.9658703071672354, "no_speech_prob": 0.460702508687973}, {"id": 173, "seek": 88824, "start": 910.32, "end": 915.92, "text": " book, where the ones that left just a few months before the singularity was presumed to have happened", "tokens": [51468, 1446, 11, 689, 264, 2306, 300, 1411, 445, 257, 1326, 2493, 949, 264, 20010, 507, 390, 18028, 292, 281, 362, 2011, 51748], "temperature": 0.0, "avg_logprob": -0.08437123665442833, "compression_ratio": 1.9658703071672354, "no_speech_prob": 0.460702508687973}, {"id": 174, "seek": 91592, "start": 916.0799999999999, "end": 920.16, "text": " have resources comparable to like full countries or the militaries of full countries.", "tokens": [50372, 362, 3593, 25323, 281, 411, 1577, 3517, 420, 264, 30653, 530, 295, 1577, 3517, 13, 50576], "temperature": 0.0, "avg_logprob": -0.08302828622242761, "compression_ratio": 1.7331189710610932, "no_speech_prob": 0.20671720802783966}, {"id": 175, "seek": 91592, "start": 920.16, "end": 924.24, "text": " And that's where you're going when you're approaching a technological singularity.", "tokens": [50576, 400, 300, 311, 689, 291, 434, 516, 562, 291, 434, 14908, 257, 18439, 20010, 507, 13, 50780], "temperature": 0.0, "avg_logprob": -0.08302828622242761, "compression_ratio": 1.7331189710610932, "no_speech_prob": 0.20671720802783966}, {"id": 176, "seek": 91592, "start": 924.24, "end": 929.04, "text": " There's a lot to talk about when we think about the implications of AGI, why we have", "tokens": [50780, 821, 311, 257, 688, 281, 751, 466, 562, 321, 519, 466, 264, 16602, 295, 316, 26252, 11, 983, 321, 362, 51020], "temperature": 0.0, "avg_logprob": -0.08302828622242761, "compression_ratio": 1.7331189710610932, "no_speech_prob": 0.20671720802783966}, {"id": 177, "seek": 91592, "start": 929.04, "end": 934.4799999999999, "text": " reason to be cautious about its development. I'll make another video going into the details of why", "tokens": [51020, 1778, 281, 312, 25278, 466, 1080, 3250, 13, 286, 603, 652, 1071, 960, 516, 666, 264, 4365, 295, 983, 51292], "temperature": 0.0, "avg_logprob": -0.08302828622242761, "compression_ratio": 1.7331189710610932, "no_speech_prob": 0.20671720802783966}, {"id": 178, "seek": 91592, "start": 934.4799999999999, "end": 938.56, "text": " we should be potentially scared of this rapid progress and what we should be doing to control", "tokens": [51292, 321, 820, 312, 7263, 5338, 295, 341, 7558, 4205, 293, 437, 321, 820, 312, 884, 281, 1969, 51496], "temperature": 0.0, "avg_logprob": -0.08302828622242761, "compression_ratio": 1.7331189710610932, "no_speech_prob": 0.20671720802783966}, {"id": 179, "seek": 91592, "start": 938.56, "end": 944.3199999999999, "text": " the pace. In conclusion, chat GPT and building on top of it, auto GPT seemed, in my opinion,", "tokens": [51496, 264, 11638, 13, 682, 10063, 11, 5081, 26039, 51, 293, 2390, 322, 1192, 295, 309, 11, 8399, 26039, 51, 6576, 11, 294, 452, 4800, 11, 51784], "temperature": 0.0, "avg_logprob": -0.08302828622242761, "compression_ratio": 1.7331189710610932, "no_speech_prob": 0.20671720802783966}, {"id": 180, "seek": 94432, "start": 944.32, "end": 949.2, "text": " to fully satisfy the definition of intelligence that has been set up by psychologists. And I was", "tokens": [50364, 281, 4498, 19319, 264, 7123, 295, 7599, 300, 575, 668, 992, 493, 538, 41562, 13, 400, 286, 390, 50608], "temperature": 0.0, "avg_logprob": -0.08429837226867676, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.07154035568237305}, {"id": 181, "seek": 94432, "start": 949.2, "end": 955.44, "text": " shocked when I heard about auto GPT and I learned about its capabilities and saw it in action because", "tokens": [50608, 12763, 562, 286, 2198, 466, 8399, 26039, 51, 293, 286, 3264, 466, 1080, 10862, 293, 1866, 309, 294, 3069, 570, 50920], "temperature": 0.0, "avg_logprob": -0.08429837226867676, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.07154035568237305}, {"id": 182, "seek": 94432, "start": 955.44, "end": 960.48, "text": " it's just such a big jump even from where chat GPT was and in such a short period of time. And", "tokens": [50920, 309, 311, 445, 1270, 257, 955, 3012, 754, 490, 689, 5081, 26039, 51, 390, 293, 294, 1270, 257, 2099, 2896, 295, 565, 13, 400, 51172], "temperature": 0.0, "avg_logprob": -0.08429837226867676, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.07154035568237305}, {"id": 183, "seek": 94432, "start": 960.48, "end": 965.2, "text": " with not all that much ever, and I think that will just pretend what we will see in the future.", "tokens": [51172, 365, 406, 439, 300, 709, 1562, 11, 293, 286, 519, 300, 486, 445, 11865, 437, 321, 486, 536, 294, 264, 2027, 13, 51408], "temperature": 0.0, "avg_logprob": -0.08429837226867676, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.07154035568237305}, {"id": 184, "seek": 94432, "start": 965.2, "end": 970.4000000000001, "text": " Very rapid steps of improvement that maybe are not all that difficult or are certainly happening", "tokens": [51408, 4372, 7558, 4439, 295, 10444, 300, 1310, 366, 406, 439, 300, 2252, 420, 366, 3297, 2737, 51668], "temperature": 0.0, "avg_logprob": -0.08429837226867676, "compression_ratio": 1.6758620689655173, "no_speech_prob": 0.07154035568237305}, {"id": 185, "seek": 97040, "start": 970.48, "end": 976.4, "text": " very rapidly. So where does that lead? It leads to AGI. AGI seems to be extremely close. I think", "tokens": [50368, 588, 12910, 13, 407, 689, 775, 300, 1477, 30, 467, 6689, 281, 316, 26252, 13, 316, 26252, 2544, 281, 312, 4664, 1998, 13, 286, 519, 50664], "temperature": 0.0, "avg_logprob": -0.13252751905839522, "compression_ratio": 1.4585152838427948, "no_speech_prob": 0.4720947742462158}, {"id": 186, "seek": 97040, "start": 976.4, "end": 982.16, "text": " Ray Kurzweil might be underestimating how far away the singularity is for us. So again, my next", "tokens": [50664, 10883, 45307, 826, 388, 1062, 312, 24612, 332, 990, 577, 1400, 1314, 264, 20010, 507, 307, 337, 505, 13, 407, 797, 11, 452, 958, 50952], "temperature": 0.0, "avg_logprob": -0.13252751905839522, "compression_ratio": 1.4585152838427948, "no_speech_prob": 0.4720947742462158}, {"id": 187, "seek": 97040, "start": 982.16, "end": 986.88, "text": " video will talk more about the implications of AGI. And that's all I have for today. Hope you", "tokens": [50952, 960, 486, 751, 544, 466, 264, 16602, 295, 316, 26252, 13, 400, 300, 311, 439, 286, 362, 337, 965, 13, 6483, 291, 51188], "temperature": 0.0, "avg_logprob": -0.13252751905839522, "compression_ratio": 1.4585152838427948, "no_speech_prob": 0.4720947742462158}, {"id": 188, "seek": 97040, "start": 986.88, "end": 988.64, "text": " enjoyed. Thank you very much for watching. Bye.", "tokens": [51188, 4626, 13, 1044, 291, 588, 709, 337, 1976, 13, 4621, 13, 51276], "temperature": 0.0, "avg_logprob": -0.13252751905839522, "compression_ratio": 1.4585152838427948, "no_speech_prob": 0.4720947742462158}], "language": "en"}