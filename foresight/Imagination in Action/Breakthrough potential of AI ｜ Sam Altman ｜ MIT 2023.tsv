start	end	text
0	23600	This particular room is maybe a little different from a lot of that.
23600	29480	Most of the people here in this room are either building companies or are working on plans
29480	35600	to build companies that are in the ecosystem really triggered by chat G.B.T.
35600	37200	My kind of people, I wish they were there.
37200	39120	Yeah, I wish you were here too.
39120	42640	Actually, they are exactly your kind of people.
42640	46000	And I know that part of the mission here is to make the world a better place, but also
46000	49680	to build on top of the platform that you've created and obviously you navigated to the
49680	53840	position you're in in life very deliberately and you're the perfect person to help advise
53840	54840	them.
54840	58440	So we're going to try and keep this on focus in a way that it helps this room as much
58440	59440	as possible.
59440	67320	A hundred people create successful companies.
67320	74320	And Sam, the first thing I'm going to ask you about is, you know, if AGI is in the near
74320	80960	term future, then we're right now at this inflection point where human history has a
80960	85880	period of time up till AGI and then obviously has a completely different history from here
85880	87040	forward.
87040	92280	So it seems to me that at this stage, you're going to be a centerpiece of the history books
92280	93280	no matter how this evolves.
93280	94760	Do you think it's the same?
94760	98400	So I think it's the same in terms of what?
98400	105520	In terms of the way history will describe this moment, this moment being this year of innovation
105520	108280	in this field.
108280	114480	I mean, I hope this will be like a page or a chapter in history books, but I think that
114600	120240	over the next several billion years, such unbelievable things are going to happen that
120240	125400	this will be just sort of like one small part and there will be new and bigger and more
125400	129040	exciting opportunities and challenges in front of us.
129040	133400	So I think one of the things that a lot of people are asking, you know, with prior iterations
133400	140120	of GPT, open source iterations, you had a whole variety of ways of taking that source code
140240	145040	and making a vertical company out of it or an adjacent company, something a federated
145040	147200	learning or something.
147200	152720	In the future iteration of these companies, you've got this highly tunable closed API
152720	153920	to start from.
153920	158120	Any quick advice on, okay, I'm starting a company, now I have to make some decisions
158120	159360	right out of the gate.
159360	160360	What do I start with?
160360	164640	How do I make it work in any given vertical use case?
164640	169680	You know, I think there's always more that stays the same about how to make a good company
169760	174240	than what changes the, and a lot of people, whenever there's like a new platform shift
174240	177120	like this thing, just because they're using the platform, like that's what's going to
177120	178800	guide business strategy.
178800	184320	It doesn't, nothing lets you off the hook for building a product that people love, for
184320	188560	being like very close to your users, fulfilling their needs, for thinking about a long-term
188560	190160	durable business strategy.
190160	195440	Like that's actually probably only more important during a platform shift, not less.
195440	199560	Like if we think back to the launch of the App Store, which is probably the most recent
199560	205600	similar example, there were a ton of companies that built very lightweight things with, I
205600	209760	don't want to call them like exploitative mechanics, but just like, you know, it was
209760	211760	not something durable.
211760	216680	And those companies had incredible meteoric rises and falls.
216680	219680	And then the companies that really like did all of the normal things you're supposed to
219680	223680	do to build a great business, endured for the last 15 years.
223680	226560	And so you definitely want to be in that latter category.
226560	231520	And the technology is just like, this is a new enabler.
231520	236080	But what you have to do as a company is like build a great company that has a long-term
236080	238280	compounding strategic advantage.
238280	241560	And then what about foundation models, just as a starting point?
241560	246000	You know, if I look back two years, one of the best ways to start was to take an existing
246000	252880	foundation model, maybe add some layers and retrain it in a vertical use case.
252880	257080	Now the foundation model, sort of the base model, is maybe a trillion parameters.
257080	259320	So it's much, much bigger.
259320	263680	But your ability to manipulate it without having to retrain it is also far, far more
263680	264680	flexible.
264680	268880	I think you have 50,000 tokens to play with right now in the basic model.
268880	269880	Is that right?
269880	270880	About a million?
270880	271880	32,000 in the biggest model.
271880	272880	32,000?
272880	273880	8,000 in the base model.
273880	274880	Okay.
274880	276800	And actually, so how's that going to evolve?
276800	280520	There are new iterations that are going to come out pretty quickly.
280520	288000	We're still trying to figure out exactly what developers want in terms of model customization.
288000	292280	We're open to doing a lot of things here, and we're, you know, we also hold our, like,
292280	293280	developers are users.
293280	298080	So our goal is to make developers super happy and figure out what they need.
298080	302120	We thought it was going to be much more of a fine tuning story, and we had been thinking
302120	304840	about how to offer that in different ways.
304840	309080	But people are doing pretty amazing things with the base model, and for a bunch of reasons
309080	310800	often seem to prefer that.
310800	317680	So we're, like, actively reconsidering what customization to prioritize, given what users
317680	320520	seem to want and seem to be making work.
320520	326120	As the models get better and better, it does seem like there is a trend towards less and
326120	330840	less of a need to fine tune, and you can do more and more in the context.
330840	333360	And when you say fine tune, you mean changing parameter weights?
333360	334360	Yeah.
334360	335360	Yeah.
335360	340960	And is there going to be an ability at all to change the parameter weights in the GPT
340960	341960	world?
341960	344200	Yeah, we'll definitely offer something there.
344200	349720	But it, like, right now it looks like maybe that will be less used than ability to offer
349720	353480	like super cheap context, like $1 million if we can ever figure that out on the base
353480	354480	model.
354480	355480	Yeah.
355480	359080	Let's drill in on that just a little bit, because it seems like regardless of the specifics,
359080	362600	the trend is toward, as the models are getting bigger and bigger and bigger, so you go from
362600	370120	$1 trillion to $10 trillion parameters, the amount you can achieve with just changing prompt
370120	377560	engineering or changing the tokens that are feeding into it is growing disproportionately
377560	378560	to the model size.
378560	380040	Does that sound right?
380040	385680	Um, disproportionately to the model size, yes, but I think we're like at the end of
385680	388800	the era where it's going to be these like giant, giant models and we'll make them better
388800	390600	in other ways.
390600	395000	But I would say it, like, it grows proportionate to the model capability.
395000	396080	Yep.
396080	401600	And then the investment in the creation of the foundation models is on the order of
401600	406760	$50 million, $100 million just in the training process.
406760	407760	So it seems like...
407760	408760	It's much more than that.
408760	409760	Is it?
409760	410760	What's the magnitude there?
410760	413600	We don't share, but it's much more than that.
413600	414600	Okay.
414600	418160	And rising, I assume, over time.
418160	419160	Yeah.
419160	423440	So then somebody trying to start from scratch, somebody trying to start from scratch, you
423440	425440	know, is trying to catch up to something that's in order.
425440	430000	And maybe, or maybe we're all being incredibly dumb and we're missing one big idea and all
430000	434080	of this is not as hard or expensive as we think and there will be a totally new paradigm
434080	437920	that absolutes us, which would be great, not great for us, but great for the world.
437920	438920	Yeah.
438920	439920	Yeah.
439920	440920	So let me get your take on something.
440920	446600	So Paul Graham calls you the greatest business strategist that he's ever encountered.
446600	450320	And of course, all these people are wrestling with their business strategy and what exactly
450320	451800	to build and where.
451800	456360	And so I've been asking you questions that are more or less vertical use cases that sit
456360	461640	on top of GPT-4 and soon GPT-5 and so on.
461640	464360	But there's also all these business models that are adjacent.
464360	472560	So things like federated learning or data conditioning or just deployment and so those
472560	473920	are interesting business models too.
473920	481120	If you were just investing in a class of company that's in the ecosystem, any thoughts on where
481120	485480	the greater returns are, where the faster growing more interesting business models are?
485480	487080	I don't think PG quite said that.
487080	494360	I know he said something like in that direction, but in any case, I don't think it'd be true.
494360	499400	I think there are people who are unbelievable business strategists and I'm not one of them.
499400	504400	So I hesitate to give advice here.
504400	508120	The only thing I know how to do, I think, is this one strategy again and again, which
508120	514360	is very long time horizon, capital intensive, difficult technology bets.
514360	515960	And I don't even think I'm particularly good at those.
515960	518120	I just think not many people try them.
518120	520400	So there's very little competition, which is nice.
520400	527280	I mean, I don't have a lot of competition, but the strategy that it takes to now like
527320	535520	take a platform like OpenAI and build a new fast growing defensible consumer enterprise
535520	539880	company, I know almost nothing about, like I know all of the theory, but none of the
539880	543200	practice and I would go find people who have done it and get the practice, get the advice
543200	544200	from them.
544200	545200	All right.
545200	546200	Good advice.
546200	548440	A couple of questions about the underlying tech platform here.
548440	554280	So I've been building neural networks myself since the parameter count was sub one million
554280	558600	and they're actually very useful for a bunch of commercial applications and then kind of
558600	562880	watch them tip into the billion and then the, you know, with GPT two, I think about one
562880	567800	and a half billion or so and then GPT three and now GPT four.
567800	568800	So you go up.
568800	573280	We don't know the current parameter count, but I think it was 175 billion in GPT three
573280	577520	and it was just mind-blowingly different from GPT two and then then GPT four is even more
577520	579680	mind-blowingly different.
579680	585960	So the raw underlying parameter count seems like it's on a trend just listening to Nvidia's
585960	594200	forecast where you can, you can go from a trillion to 10 trillion and then they're saying
594200	597160	up to 10 quadrillion in a decade.
597160	602320	So you've got four factors of 10 or 10,000 X in a decade.
602320	605840	Does that even sound like it's in the right ballpark?
605840	608080	I think it's way too much focus on parameter count.
608080	611800	I mean, parameter count will trend up for sure.
611800	617960	But this reminds me a lot of the gigahertz race in chips in the like nineties and two
617960	623360	thousands where everybody was trying to like point to a big number and then event like
623360	626400	you don't need probably most of you don't know how many gigahertz are on your iPhone,
626400	627400	but it's fast.
627400	631800	Like what we actually care about is capability and I think it's important that what we keep
631800	636080	the focus on is rapidly increasing capability.
636080	639840	And if there's some reason that parameter count should decrease over time or we should
639840	643480	have like multiple models working together, each of which are smaller, we would just do
643480	644480	that.
644480	648640	Like, well, we want to deliver to the world of the most capable, useful and safe models.
648640	652680	We are not here to like jerk ourselves off about parameter count.
652680	657480	Can we quote you on that?
657480	666040	It's going to get quoted no matter what, so, yeah, well, that's my, okay, well, thank you
666040	667680	for taking that away from me.
667680	674520	So, but one thing that's absolutely unique about this class of algorithm versus anything
674520	680320	I've ever seen before is that it surprises you with raw horsepower regardless of whether
680320	683960	you measure it in parameter count or some other way.
683960	689560	It does things that you didn't anticipate purely by putting more horsepower behind it.
689560	691160	And so it takes advantage of the scale.
691160	695560	The analogy I was making this morning is if you have a spreadsheet, you coded it up, you
695560	700320	run it on a computer that's 10,000 times faster, it doesn't really surprise you.
700320	704920	It's nice and responsive, it's still a spreadsheet, whereas this class of algorithm does things
704920	707120	that it just couldn't do before.
707120	711840	And so we actually, one of our partners in our venture fund wrote an entire book on GPT-2
711840	716800	and you can buy it on Amazon, it's called Start Here Romance.
716800	718160	I think about 10 copies of sold.
718160	721880	I bought one of them, so maybe nine copies of sold, but if you read the book, it's just
721880	724960	not a good book.
724960	730120	And here we are, that was four years ago, it's only been four years.
730120	737200	And now the quality of the book has gone from GPT-2, 3, 4, not a good book, it's a somewhat
737200	741480	reasonable book, to now it's possible to write a truly excellent book.
741480	745640	You have to give it the framework, you're still effectively writing the concept, but
745640	748400	it's filling in the words just beautifully.
748400	752680	And so as an author, that could be a force multiplier of something like 10, 100, it just
752680	755680	enables an author to be that much more powerful.
755680	760000	So this class of algorithm then, if the underlying substrate is getting faster and faster and
760000	766440	faster, it's going to do surprising things on a relatively short time scale.
766440	769320	And so I think one of the things that people in the room need to predict is, okay, what
769320	775920	is the next real world society benefiting use case that hits that tipping point on this
775920	776920	curve?
776920	780760	And I think one of the highlights you can give us into, what's going to be possible
780760	783880	that wasn't possible a year prior or two years prior?
783880	790200	Okay, I said I don't have business strategy advice, I just thought of something I do.
790200	796640	I think in new areas like this, one of the right approaches is to let tactics become
796640	799840	strategy instead of the other way around.
799840	805920	And I have my ideas, I'm sure you all have your ideas, maybe we'll be mostly right, we'll
805920	813160	be wrong in some ways, and even the details of how we're right will be wrong about it.
813160	818600	I think you never want to lose sight of vision and focus on the long term, but a very tight
818600	823800	feedback loop of paying attention to what is working and what is not working, and doing
823800	827080	more of the stuff that's working and less of the stuff that's not working.
827080	833440	And just very, very careful user observation can go super far.
833480	841040	So I can speculate on ideas, you all can speculate on ideas, none of that will be as valuable
841040	847440	as putting something out there and really deeply understanding what's happening and being
847440	850440	responsive to it.
850440	857560	For the next question, Sam, when did you know your baby, chat GPT was something really special,
857560	862000	and what was the special sauce that allowed you to pull off something that others haven't?
862120	864120	Dave will come back, but yeah.
864120	868120	Who likes Sam so far?
868120	871600	If Sam was hiring, would you consider being part of his team?
871600	874200	Okay, all right, we got a lot of hands.
874200	877920	Great, yeah, please come, we really need help and it's going to be a pretty exciting
877920	880280	next few years.
880280	885920	I mean, we've been working on it for so long that it's like you kind of know with gradually
885920	890280	increasing confidence that it's really going to work, but this is, you know, we've been
890280	892760	doing the company for seven years.
892760	895280	These things take a long, long time.
895280	900440	I would say by, and like in terms of why it worked when others happened, it's just because
900440	904920	we've like been on the grind sweating every detail for a long time and most people aren't
904920	906880	willing to do that.
906880	912240	In terms of when we knew that chat GPT in particular was going to like catch fire as
912240	916320	a consumer product, probably like 48 hours after launch.
916320	917320	Yeah.
917320	918320	All right.
918320	922320	So before Dave comes to one back, I asked Lex to ask a sexy question.
922320	923320	Hey, Lex.
923320	924320	Hey.
924320	925320	You want to use the communicator?
925320	926320	You're good.
926320	927320	What is it?
927320	928320	It's the Star Trek.
928320	929320	You're good.
929320	930320	I'm good.
930320	931320	Okay.
931320	934320	I grew up in the Soviet Union.
934320	936320	We didn't have Star Trek over there.
936320	937320	Check off.
937320	938320	Second, second season.
938320	939320	Yeah.
939320	940520	Let me ask some sexy controversial questions.
940520	947120	So you got legends in artificial intelligence, Ilya Suskeva and Andrew Kapathy over there.
947120	948120	Who's smarter?
948120	949120	Just kidding.
949120	950120	Just kidding.
950120	952120	You don't have to answer that.
952120	953120	That was a joke.
953120	954120	What?
954120	955120	He was about to.
955120	956120	He was thinking about it.
956120	957120	All right.
957120	958120	I like it.
958120	959120	No.
959120	963160	I just, so we're at MIT and from here with Max Tagmark and others, they put together
963160	968000	this open letter to halt AI development for six months.
968000	972800	What are your thoughts about this open letter?
972800	976440	There's parts of the thrust that I really agree with.
976440	981400	We spent more than six months after we finished training GPT-4 before we released it.
981400	988920	So taking the time to really study the safety of a model to get external audits, external
988920	994000	red teamers, to really try to understand what's going on and mitigate as much as you
994000	995000	can.
995000	996000	That's important.
996000	999120	It's been really nice since we have launched GPT-4.
999120	1003200	How many people have said, wow, this is not the most capable model open AI has put up,
1003200	1005680	but by far the safest and most aligned.
1005680	1009240	Unless I'm trying to get it to do something bad, it won't.
1009240	1013040	So that we totally, I totally agree with.
1013040	1020560	I also agree that as capabilities get more and more serious, that the safety bar has
1020560	1023000	got to increase.
1023000	1030520	But unfortunately, I think the letter is missing most technical nuance about where we need
1030520	1031520	the pause.
1031520	1035840	It's actually, an earlier version of the letter claimed the open AI is trained in GPT-5 right
1035840	1036840	now.
1036840	1038600	We are not and won't for some time.
1038600	1040320	So in that sense, it was sort of silly.
1040320	1045280	But we are doing other things on top of GPT-4 that I think have all sorts of safety issues
1045280	1048960	that are important to address and we're totally left out of the letter.
1048960	1057560	So I think moving with caution and an increasing rigor for safety issues is really important.
1057560	1061080	The letter I don't think is the optimal way to address it.
1061080	1062240	Just a quick question for me.
1062240	1069400	One more is, you have been extremely open, having a lot of conversations, being honest.
1069400	1070920	Others at open AI as well.
1070920	1072800	What's the philosophy behind that?
1072800	1077560	Because compared to other companies, there are much more clothes in that regard.
1077680	1080000	Do you plan to continue doing that?
1080000	1082200	We certainly plan to continue doing that.
1082200	1086440	The trade-off is like we say dumb stuff sometimes, stuff that turns out to be totally wrong.
1086440	1091560	I think a lot of other companies don't want to say something until they're sure it's right.
1091560	1096720	But I think this technology is going to so impact all of us that we believe that engaging
1096720	1102200	everyone in the discussion, putting these systems out into the world, deeply imperfect
1102200	1107240	though they are in their current state, so that people get to experience them, think
1107240	1109960	about them, understand the upsides and the downsides.
1109960	1114120	It's worth the trade-off, even though we do tend to embarrass ourselves in public and
1114120	1117960	have to change our minds with new data frequently.
1117960	1122120	So we're going to keep doing that because we think it's better than any alternative.
1122120	1127720	And a big part of our goal at Open AI is to get the world to engage with this and think
1127720	1134280	about it and gradually update and build new institutions or adapt our existing institutions
1134280	1138440	to be able to figure out what the future we all want is.
1138440	1140760	So that's kind of why we're here.
1140760	1144080	So we only have a few minutes left and I have to ask you a question that has been on my
1144080	1146120	mind since I was 13 years old.
1146120	1151320	So I think if you read Ray Kurzweil or any of the luminaries in this sector, the day
1151320	1158680	when the algorithms start writing the code that improves the algorithms is a pivotal
1158680	1159680	day.
1159680	1163000	It accelerates the process toward infinity or in the singularity view of the world to
1163000	1165600	absolute infinity.
1165600	1169680	And so now a lot of the companies that I'm an investor in or have been co-founder of
1169680	1173720	are starting to use LLMs for code generation.
1173720	1179000	And there's an interesting very wide range of lift or improvement in the performance
1179000	1184280	of an engineer ranging from about 5% to about 20x.
1184280	1187200	And it depends on what you're trying to do, what type of code, how much context it needs.
1187200	1190480	A lot of it is related to tuning in the system.
1190480	1191960	So there's two questions in there.
1192000	1197240	First, within OpenAI, how much of a force multiplier do you already see within the creation
1197240	1199320	of the next iteration of the code?
1199320	1203520	And then the follow-on question is, okay, what does it look like a few months from now,
1203520	1205200	a year from now, two years from now?
1205200	1210440	Are we getting close to that day where the thing is so rapidly self-improving that it
1210440	1211440	hits some...
1211440	1213680	Yeah, great question.
1213680	1221280	I think that it is going to be a much fuzzier boundary for getting to self-improvement or
1221280	1222440	not.
1222440	1227400	I think what will happen is that more and more of the improvement loop will be aided
1227400	1230880	by AIs, but humans will still be driving it.
1230880	1233640	And it's going to go like that for a long time.
1233640	1238160	And there's a whole bunch of other things that I have never believed in the one day
1238160	1245120	or one month takeoff for a bunch of reasons, but one of which is how incredibly long it
1245120	1249920	takes to build new data centers, bigger data centers, even if we knew how to do it right
1249920	1253560	now, just like waiting for the concrete to dry, getting the power into the building.
1253560	1256840	Stuff takes a while.
1256840	1260640	But I think what will happen is humans will be more and more augmented and be able to
1260640	1263680	do things in the world faster and faster.
1263680	1268560	And it will not work out like it will not somehow...
1268560	1273000	Like most of these things don't end up working out quite like the sci-fi books.
1273000	1274840	And neither will this one.
1274840	1279200	But the rate of change in the world will increase forever and more from here as humans get better
1279200	1279800	and better tools.
