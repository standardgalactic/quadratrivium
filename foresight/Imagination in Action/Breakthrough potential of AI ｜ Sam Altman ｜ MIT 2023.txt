This particular room is maybe a little different from a lot of that.
Most of the people here in this room are either building companies or are working on plans
to build companies that are in the ecosystem really triggered by chat G.B.T.
My kind of people, I wish they were there.
Yeah, I wish you were here too.
Actually, they are exactly your kind of people.
And I know that part of the mission here is to make the world a better place, but also
to build on top of the platform that you've created and obviously you navigated to the
position you're in in life very deliberately and you're the perfect person to help advise
them.
So we're going to try and keep this on focus in a way that it helps this room as much
as possible.
A hundred people create successful companies.
And Sam, the first thing I'm going to ask you about is, you know, if AGI is in the near
term future, then we're right now at this inflection point where human history has a
period of time up till AGI and then obviously has a completely different history from here
forward.
So it seems to me that at this stage, you're going to be a centerpiece of the history books
no matter how this evolves.
Do you think it's the same?
So I think it's the same in terms of what?
In terms of the way history will describe this moment, this moment being this year of innovation
in this field.
I mean, I hope this will be like a page or a chapter in history books, but I think that
over the next several billion years, such unbelievable things are going to happen that
this will be just sort of like one small part and there will be new and bigger and more
exciting opportunities and challenges in front of us.
So I think one of the things that a lot of people are asking, you know, with prior iterations
of GPT, open source iterations, you had a whole variety of ways of taking that source code
and making a vertical company out of it or an adjacent company, something a federated
learning or something.
In the future iteration of these companies, you've got this highly tunable closed API
to start from.
Any quick advice on, okay, I'm starting a company, now I have to make some decisions
right out of the gate.
What do I start with?
How do I make it work in any given vertical use case?
You know, I think there's always more that stays the same about how to make a good company
than what changes the, and a lot of people, whenever there's like a new platform shift
like this thing, just because they're using the platform, like that's what's going to
guide business strategy.
It doesn't, nothing lets you off the hook for building a product that people love, for
being like very close to your users, fulfilling their needs, for thinking about a long-term
durable business strategy.
Like that's actually probably only more important during a platform shift, not less.
Like if we think back to the launch of the App Store, which is probably the most recent
similar example, there were a ton of companies that built very lightweight things with, I
don't want to call them like exploitative mechanics, but just like, you know, it was
not something durable.
And those companies had incredible meteoric rises and falls.
And then the companies that really like did all of the normal things you're supposed to
do to build a great business, endured for the last 15 years.
And so you definitely want to be in that latter category.
And the technology is just like, this is a new enabler.
But what you have to do as a company is like build a great company that has a long-term
compounding strategic advantage.
And then what about foundation models, just as a starting point?
You know, if I look back two years, one of the best ways to start was to take an existing
foundation model, maybe add some layers and retrain it in a vertical use case.
Now the foundation model, sort of the base model, is maybe a trillion parameters.
So it's much, much bigger.
But your ability to manipulate it without having to retrain it is also far, far more
flexible.
I think you have 50,000 tokens to play with right now in the basic model.
Is that right?
About a million?
32,000 in the biggest model.
32,000?
8,000 in the base model.
Okay.
And actually, so how's that going to evolve?
There are new iterations that are going to come out pretty quickly.
We're still trying to figure out exactly what developers want in terms of model customization.
We're open to doing a lot of things here, and we're, you know, we also hold our, like,
developers are users.
So our goal is to make developers super happy and figure out what they need.
We thought it was going to be much more of a fine tuning story, and we had been thinking
about how to offer that in different ways.
But people are doing pretty amazing things with the base model, and for a bunch of reasons
often seem to prefer that.
So we're, like, actively reconsidering what customization to prioritize, given what users
seem to want and seem to be making work.
As the models get better and better, it does seem like there is a trend towards less and
less of a need to fine tune, and you can do more and more in the context.
And when you say fine tune, you mean changing parameter weights?
Yeah.
Yeah.
And is there going to be an ability at all to change the parameter weights in the GPT
world?
Yeah, we'll definitely offer something there.
But it, like, right now it looks like maybe that will be less used than ability to offer
like super cheap context, like $1 million if we can ever figure that out on the base
model.
Yeah.
Let's drill in on that just a little bit, because it seems like regardless of the specifics,
the trend is toward, as the models are getting bigger and bigger and bigger, so you go from
$1 trillion to $10 trillion parameters, the amount you can achieve with just changing prompt
engineering or changing the tokens that are feeding into it is growing disproportionately
to the model size.
Does that sound right?
Um, disproportionately to the model size, yes, but I think we're like at the end of
the era where it's going to be these like giant, giant models and we'll make them better
in other ways.
But I would say it, like, it grows proportionate to the model capability.
Yep.
And then the investment in the creation of the foundation models is on the order of
$50 million, $100 million just in the training process.
So it seems like...
It's much more than that.
Is it?
What's the magnitude there?
We don't share, but it's much more than that.
Okay.
And rising, I assume, over time.
Yeah.
So then somebody trying to start from scratch, somebody trying to start from scratch, you
know, is trying to catch up to something that's in order.
And maybe, or maybe we're all being incredibly dumb and we're missing one big idea and all
of this is not as hard or expensive as we think and there will be a totally new paradigm
that absolutes us, which would be great, not great for us, but great for the world.
Yeah.
Yeah.
So let me get your take on something.
So Paul Graham calls you the greatest business strategist that he's ever encountered.
And of course, all these people are wrestling with their business strategy and what exactly
to build and where.
And so I've been asking you questions that are more or less vertical use cases that sit
on top of GPT-4 and soon GPT-5 and so on.
But there's also all these business models that are adjacent.
So things like federated learning or data conditioning or just deployment and so those
are interesting business models too.
If you were just investing in a class of company that's in the ecosystem, any thoughts on where
the greater returns are, where the faster growing more interesting business models are?
I don't think PG quite said that.
I know he said something like in that direction, but in any case, I don't think it'd be true.
I think there are people who are unbelievable business strategists and I'm not one of them.
So I hesitate to give advice here.
The only thing I know how to do, I think, is this one strategy again and again, which
is very long time horizon, capital intensive, difficult technology bets.
And I don't even think I'm particularly good at those.
I just think not many people try them.
So there's very little competition, which is nice.
I mean, I don't have a lot of competition, but the strategy that it takes to now like
take a platform like OpenAI and build a new fast growing defensible consumer enterprise
company, I know almost nothing about, like I know all of the theory, but none of the
practice and I would go find people who have done it and get the practice, get the advice
from them.
All right.
Good advice.
A couple of questions about the underlying tech platform here.
So I've been building neural networks myself since the parameter count was sub one million
and they're actually very useful for a bunch of commercial applications and then kind of
watch them tip into the billion and then the, you know, with GPT two, I think about one
and a half billion or so and then GPT three and now GPT four.
So you go up.
We don't know the current parameter count, but I think it was 175 billion in GPT three
and it was just mind-blowingly different from GPT two and then then GPT four is even more
mind-blowingly different.
So the raw underlying parameter count seems like it's on a trend just listening to Nvidia's
forecast where you can, you can go from a trillion to 10 trillion and then they're saying
up to 10 quadrillion in a decade.
So you've got four factors of 10 or 10,000 X in a decade.
Does that even sound like it's in the right ballpark?
I think it's way too much focus on parameter count.
I mean, parameter count will trend up for sure.
But this reminds me a lot of the gigahertz race in chips in the like nineties and two
thousands where everybody was trying to like point to a big number and then event like
you don't need probably most of you don't know how many gigahertz are on your iPhone,
but it's fast.
Like what we actually care about is capability and I think it's important that what we keep
the focus on is rapidly increasing capability.
And if there's some reason that parameter count should decrease over time or we should
have like multiple models working together, each of which are smaller, we would just do
that.
Like, well, we want to deliver to the world of the most capable, useful and safe models.
We are not here to like jerk ourselves off about parameter count.
Can we quote you on that?
It's going to get quoted no matter what, so, yeah, well, that's my, okay, well, thank you
for taking that away from me.
So, but one thing that's absolutely unique about this class of algorithm versus anything
I've ever seen before is that it surprises you with raw horsepower regardless of whether
you measure it in parameter count or some other way.
It does things that you didn't anticipate purely by putting more horsepower behind it.
And so it takes advantage of the scale.
The analogy I was making this morning is if you have a spreadsheet, you coded it up, you
run it on a computer that's 10,000 times faster, it doesn't really surprise you.
It's nice and responsive, it's still a spreadsheet, whereas this class of algorithm does things
that it just couldn't do before.
And so we actually, one of our partners in our venture fund wrote an entire book on GPT-2
and you can buy it on Amazon, it's called Start Here Romance.
I think about 10 copies of sold.
I bought one of them, so maybe nine copies of sold, but if you read the book, it's just
not a good book.
And here we are, that was four years ago, it's only been four years.
And now the quality of the book has gone from GPT-2, 3, 4, not a good book, it's a somewhat
reasonable book, to now it's possible to write a truly excellent book.
You have to give it the framework, you're still effectively writing the concept, but
it's filling in the words just beautifully.
And so as an author, that could be a force multiplier of something like 10, 100, it just
enables an author to be that much more powerful.
So this class of algorithm then, if the underlying substrate is getting faster and faster and
faster, it's going to do surprising things on a relatively short time scale.
And so I think one of the things that people in the room need to predict is, okay, what
is the next real world society benefiting use case that hits that tipping point on this
curve?
And I think one of the highlights you can give us into, what's going to be possible
that wasn't possible a year prior or two years prior?
Okay, I said I don't have business strategy advice, I just thought of something I do.
I think in new areas like this, one of the right approaches is to let tactics become
strategy instead of the other way around.
And I have my ideas, I'm sure you all have your ideas, maybe we'll be mostly right, we'll
be wrong in some ways, and even the details of how we're right will be wrong about it.
I think you never want to lose sight of vision and focus on the long term, but a very tight
feedback loop of paying attention to what is working and what is not working, and doing
more of the stuff that's working and less of the stuff that's not working.
And just very, very careful user observation can go super far.
So I can speculate on ideas, you all can speculate on ideas, none of that will be as valuable
as putting something out there and really deeply understanding what's happening and being
responsive to it.
For the next question, Sam, when did you know your baby, chat GPT was something really special,
and what was the special sauce that allowed you to pull off something that others haven't?
Dave will come back, but yeah.
Who likes Sam so far?
If Sam was hiring, would you consider being part of his team?
Okay, all right, we got a lot of hands.
Great, yeah, please come, we really need help and it's going to be a pretty exciting
next few years.
I mean, we've been working on it for so long that it's like you kind of know with gradually
increasing confidence that it's really going to work, but this is, you know, we've been
doing the company for seven years.
These things take a long, long time.
I would say by, and like in terms of why it worked when others happened, it's just because
we've like been on the grind sweating every detail for a long time and most people aren't
willing to do that.
In terms of when we knew that chat GPT in particular was going to like catch fire as
a consumer product, probably like 48 hours after launch.
Yeah.
All right.
So before Dave comes to one back, I asked Lex to ask a sexy question.
Hey, Lex.
Hey.
You want to use the communicator?
You're good.
What is it?
It's the Star Trek.
You're good.
I'm good.
Okay.
I grew up in the Soviet Union.
We didn't have Star Trek over there.
Check off.
Second, second season.
Yeah.
Let me ask some sexy controversial questions.
So you got legends in artificial intelligence, Ilya Suskeva and Andrew Kapathy over there.
Who's smarter?
Just kidding.
Just kidding.
You don't have to answer that.
That was a joke.
What?
He was about to.
He was thinking about it.
All right.
I like it.
No.
I just, so we're at MIT and from here with Max Tagmark and others, they put together
this open letter to halt AI development for six months.
What are your thoughts about this open letter?
There's parts of the thrust that I really agree with.
We spent more than six months after we finished training GPT-4 before we released it.
So taking the time to really study the safety of a model to get external audits, external
red teamers, to really try to understand what's going on and mitigate as much as you
can.
That's important.
It's been really nice since we have launched GPT-4.
How many people have said, wow, this is not the most capable model open AI has put up,
but by far the safest and most aligned.
Unless I'm trying to get it to do something bad, it won't.
So that we totally, I totally agree with.
I also agree that as capabilities get more and more serious, that the safety bar has
got to increase.
But unfortunately, I think the letter is missing most technical nuance about where we need
the pause.
It's actually, an earlier version of the letter claimed the open AI is trained in GPT-5 right
now.
We are not and won't for some time.
So in that sense, it was sort of silly.
But we are doing other things on top of GPT-4 that I think have all sorts of safety issues
that are important to address and we're totally left out of the letter.
So I think moving with caution and an increasing rigor for safety issues is really important.
The letter I don't think is the optimal way to address it.
Just a quick question for me.
One more is, you have been extremely open, having a lot of conversations, being honest.
Others at open AI as well.
What's the philosophy behind that?
Because compared to other companies, there are much more clothes in that regard.
Do you plan to continue doing that?
We certainly plan to continue doing that.
The trade-off is like we say dumb stuff sometimes, stuff that turns out to be totally wrong.
I think a lot of other companies don't want to say something until they're sure it's right.
But I think this technology is going to so impact all of us that we believe that engaging
everyone in the discussion, putting these systems out into the world, deeply imperfect
though they are in their current state, so that people get to experience them, think
about them, understand the upsides and the downsides.
It's worth the trade-off, even though we do tend to embarrass ourselves in public and
have to change our minds with new data frequently.
So we're going to keep doing that because we think it's better than any alternative.
And a big part of our goal at Open AI is to get the world to engage with this and think
about it and gradually update and build new institutions or adapt our existing institutions
to be able to figure out what the future we all want is.
So that's kind of why we're here.
So we only have a few minutes left and I have to ask you a question that has been on my
mind since I was 13 years old.
So I think if you read Ray Kurzweil or any of the luminaries in this sector, the day
when the algorithms start writing the code that improves the algorithms is a pivotal
day.
It accelerates the process toward infinity or in the singularity view of the world to
absolute infinity.
And so now a lot of the companies that I'm an investor in or have been co-founder of
are starting to use LLMs for code generation.
And there's an interesting very wide range of lift or improvement in the performance
of an engineer ranging from about 5% to about 20x.
And it depends on what you're trying to do, what type of code, how much context it needs.
A lot of it is related to tuning in the system.
So there's two questions in there.
First, within OpenAI, how much of a force multiplier do you already see within the creation
of the next iteration of the code?
And then the follow-on question is, okay, what does it look like a few months from now,
a year from now, two years from now?
Are we getting close to that day where the thing is so rapidly self-improving that it
hits some...
Yeah, great question.
I think that it is going to be a much fuzzier boundary for getting to self-improvement or
not.
I think what will happen is that more and more of the improvement loop will be aided
by AIs, but humans will still be driving it.
And it's going to go like that for a long time.
And there's a whole bunch of other things that I have never believed in the one day
or one month takeoff for a bunch of reasons, but one of which is how incredibly long it
takes to build new data centers, bigger data centers, even if we knew how to do it right
now, just like waiting for the concrete to dry, getting the power into the building.
Stuff takes a while.
But I think what will happen is humans will be more and more augmented and be able to
do things in the world faster and faster.
And it will not work out like it will not somehow...
Like most of these things don't end up working out quite like the sci-fi books.
And neither will this one.
But the rate of change in the world will increase forever and more from here as humans get better
and better tools.
