WEBVTT

00:00.000 --> 00:23.600
This particular room is maybe a little different from a lot of that.

00:23.600 --> 00:29.480
Most of the people here in this room are either building companies or are working on plans

00:29.480 --> 00:35.600
to build companies that are in the ecosystem really triggered by chat G.B.T.

00:35.600 --> 00:37.200
My kind of people, I wish they were there.

00:37.200 --> 00:39.120
Yeah, I wish you were here too.

00:39.120 --> 00:42.640
Actually, they are exactly your kind of people.

00:42.640 --> 00:46.000
And I know that part of the mission here is to make the world a better place, but also

00:46.000 --> 00:49.680
to build on top of the platform that you've created and obviously you navigated to the

00:49.680 --> 00:53.840
position you're in in life very deliberately and you're the perfect person to help advise

00:53.840 --> 00:54.840
them.

00:54.840 --> 00:58.440
So we're going to try and keep this on focus in a way that it helps this room as much

00:58.440 --> 00:59.440
as possible.

00:59.440 --> 01:07.320
A hundred people create successful companies.

01:07.320 --> 01:14.320
And Sam, the first thing I'm going to ask you about is, you know, if AGI is in the near

01:14.320 --> 01:20.960
term future, then we're right now at this inflection point where human history has a

01:20.960 --> 01:25.880
period of time up till AGI and then obviously has a completely different history from here

01:25.880 --> 01:27.040
forward.

01:27.040 --> 01:32.280
So it seems to me that at this stage, you're going to be a centerpiece of the history books

01:32.280 --> 01:33.280
no matter how this evolves.

01:33.280 --> 01:34.760
Do you think it's the same?

01:34.760 --> 01:38.400
So I think it's the same in terms of what?

01:38.400 --> 01:45.520
In terms of the way history will describe this moment, this moment being this year of innovation

01:45.520 --> 01:48.280
in this field.

01:48.280 --> 01:54.480
I mean, I hope this will be like a page or a chapter in history books, but I think that

01:54.600 --> 02:00.240
over the next several billion years, such unbelievable things are going to happen that

02:00.240 --> 02:05.400
this will be just sort of like one small part and there will be new and bigger and more

02:05.400 --> 02:09.040
exciting opportunities and challenges in front of us.

02:09.040 --> 02:13.400
So I think one of the things that a lot of people are asking, you know, with prior iterations

02:13.400 --> 02:20.120
of GPT, open source iterations, you had a whole variety of ways of taking that source code

02:20.240 --> 02:25.040
and making a vertical company out of it or an adjacent company, something a federated

02:25.040 --> 02:27.200
learning or something.

02:27.200 --> 02:32.720
In the future iteration of these companies, you've got this highly tunable closed API

02:32.720 --> 02:33.920
to start from.

02:33.920 --> 02:38.120
Any quick advice on, okay, I'm starting a company, now I have to make some decisions

02:38.120 --> 02:39.360
right out of the gate.

02:39.360 --> 02:40.360
What do I start with?

02:40.360 --> 02:44.640
How do I make it work in any given vertical use case?

02:44.640 --> 02:49.680
You know, I think there's always more that stays the same about how to make a good company

02:49.760 --> 02:54.240
than what changes the, and a lot of people, whenever there's like a new platform shift

02:54.240 --> 02:57.120
like this thing, just because they're using the platform, like that's what's going to

02:57.120 --> 02:58.800
guide business strategy.

02:58.800 --> 03:04.320
It doesn't, nothing lets you off the hook for building a product that people love, for

03:04.320 --> 03:08.560
being like very close to your users, fulfilling their needs, for thinking about a long-term

03:08.560 --> 03:10.160
durable business strategy.

03:10.160 --> 03:15.440
Like that's actually probably only more important during a platform shift, not less.

03:15.440 --> 03:19.560
Like if we think back to the launch of the App Store, which is probably the most recent

03:19.560 --> 03:25.600
similar example, there were a ton of companies that built very lightweight things with, I

03:25.600 --> 03:29.760
don't want to call them like exploitative mechanics, but just like, you know, it was

03:29.760 --> 03:31.760
not something durable.

03:31.760 --> 03:36.680
And those companies had incredible meteoric rises and falls.

03:36.680 --> 03:39.680
And then the companies that really like did all of the normal things you're supposed to

03:39.680 --> 03:43.680
do to build a great business, endured for the last 15 years.

03:43.680 --> 03:46.560
And so you definitely want to be in that latter category.

03:46.560 --> 03:51.520
And the technology is just like, this is a new enabler.

03:51.520 --> 03:56.080
But what you have to do as a company is like build a great company that has a long-term

03:56.080 --> 03:58.280
compounding strategic advantage.

03:58.280 --> 04:01.560
And then what about foundation models, just as a starting point?

04:01.560 --> 04:06.000
You know, if I look back two years, one of the best ways to start was to take an existing

04:06.000 --> 04:12.880
foundation model, maybe add some layers and retrain it in a vertical use case.

04:12.880 --> 04:17.080
Now the foundation model, sort of the base model, is maybe a trillion parameters.

04:17.080 --> 04:19.320
So it's much, much bigger.

04:19.320 --> 04:23.680
But your ability to manipulate it without having to retrain it is also far, far more

04:23.680 --> 04:24.680
flexible.

04:24.680 --> 04:28.880
I think you have 50,000 tokens to play with right now in the basic model.

04:28.880 --> 04:29.880
Is that right?

04:29.880 --> 04:30.880
About a million?

04:30.880 --> 04:31.880
32,000 in the biggest model.

04:31.880 --> 04:32.880
32,000?

04:32.880 --> 04:33.880
8,000 in the base model.

04:33.880 --> 04:34.880
Okay.

04:34.880 --> 04:36.800
And actually, so how's that going to evolve?

04:36.800 --> 04:40.520
There are new iterations that are going to come out pretty quickly.

04:40.520 --> 04:48.000
We're still trying to figure out exactly what developers want in terms of model customization.

04:48.000 --> 04:52.280
We're open to doing a lot of things here, and we're, you know, we also hold our, like,

04:52.280 --> 04:53.280
developers are users.

04:53.280 --> 04:58.080
So our goal is to make developers super happy and figure out what they need.

04:58.080 --> 05:02.120
We thought it was going to be much more of a fine tuning story, and we had been thinking

05:02.120 --> 05:04.840
about how to offer that in different ways.

05:04.840 --> 05:09.080
But people are doing pretty amazing things with the base model, and for a bunch of reasons

05:09.080 --> 05:10.800
often seem to prefer that.

05:10.800 --> 05:17.680
So we're, like, actively reconsidering what customization to prioritize, given what users

05:17.680 --> 05:20.520
seem to want and seem to be making work.

05:20.520 --> 05:26.120
As the models get better and better, it does seem like there is a trend towards less and

05:26.120 --> 05:30.840
less of a need to fine tune, and you can do more and more in the context.

05:30.840 --> 05:33.360
And when you say fine tune, you mean changing parameter weights?

05:33.360 --> 05:34.360
Yeah.

05:34.360 --> 05:35.360
Yeah.

05:35.360 --> 05:40.960
And is there going to be an ability at all to change the parameter weights in the GPT

05:40.960 --> 05:41.960
world?

05:41.960 --> 05:44.200
Yeah, we'll definitely offer something there.

05:44.200 --> 05:49.720
But it, like, right now it looks like maybe that will be less used than ability to offer

05:49.720 --> 05:53.480
like super cheap context, like $1 million if we can ever figure that out on the base

05:53.480 --> 05:54.480
model.

05:54.480 --> 05:55.480
Yeah.

05:55.480 --> 05:59.080
Let's drill in on that just a little bit, because it seems like regardless of the specifics,

05:59.080 --> 06:02.600
the trend is toward, as the models are getting bigger and bigger and bigger, so you go from

06:02.600 --> 06:10.120
$1 trillion to $10 trillion parameters, the amount you can achieve with just changing prompt

06:10.120 --> 06:17.560
engineering or changing the tokens that are feeding into it is growing disproportionately

06:17.560 --> 06:18.560
to the model size.

06:18.560 --> 06:20.040
Does that sound right?

06:20.040 --> 06:25.680
Um, disproportionately to the model size, yes, but I think we're like at the end of

06:25.680 --> 06:28.800
the era where it's going to be these like giant, giant models and we'll make them better

06:28.800 --> 06:30.600
in other ways.

06:30.600 --> 06:35.000
But I would say it, like, it grows proportionate to the model capability.

06:35.000 --> 06:36.080
Yep.

06:36.080 --> 06:41.600
And then the investment in the creation of the foundation models is on the order of

06:41.600 --> 06:46.760
$50 million, $100 million just in the training process.

06:46.760 --> 06:47.760
So it seems like...

06:47.760 --> 06:48.760
It's much more than that.

06:48.760 --> 06:49.760
Is it?

06:49.760 --> 06:50.760
What's the magnitude there?

06:50.760 --> 06:53.600
We don't share, but it's much more than that.

06:53.600 --> 06:54.600
Okay.

06:54.600 --> 06:58.160
And rising, I assume, over time.

06:58.160 --> 06:59.160
Yeah.

06:59.160 --> 07:03.440
So then somebody trying to start from scratch, somebody trying to start from scratch, you

07:03.440 --> 07:05.440
know, is trying to catch up to something that's in order.

07:05.440 --> 07:10.000
And maybe, or maybe we're all being incredibly dumb and we're missing one big idea and all

07:10.000 --> 07:14.080
of this is not as hard or expensive as we think and there will be a totally new paradigm

07:14.080 --> 07:17.920
that absolutes us, which would be great, not great for us, but great for the world.

07:17.920 --> 07:18.920
Yeah.

07:18.920 --> 07:19.920
Yeah.

07:19.920 --> 07:20.920
So let me get your take on something.

07:20.920 --> 07:26.600
So Paul Graham calls you the greatest business strategist that he's ever encountered.

07:26.600 --> 07:30.320
And of course, all these people are wrestling with their business strategy and what exactly

07:30.320 --> 07:31.800
to build and where.

07:31.800 --> 07:36.360
And so I've been asking you questions that are more or less vertical use cases that sit

07:36.360 --> 07:41.640
on top of GPT-4 and soon GPT-5 and so on.

07:41.640 --> 07:44.360
But there's also all these business models that are adjacent.

07:44.360 --> 07:52.560
So things like federated learning or data conditioning or just deployment and so those

07:52.560 --> 07:53.920
are interesting business models too.

07:53.920 --> 08:01.120
If you were just investing in a class of company that's in the ecosystem, any thoughts on where

08:01.120 --> 08:05.480
the greater returns are, where the faster growing more interesting business models are?

08:05.480 --> 08:07.080
I don't think PG quite said that.

08:07.080 --> 08:14.360
I know he said something like in that direction, but in any case, I don't think it'd be true.

08:14.360 --> 08:19.400
I think there are people who are unbelievable business strategists and I'm not one of them.

08:19.400 --> 08:24.400
So I hesitate to give advice here.

08:24.400 --> 08:28.120
The only thing I know how to do, I think, is this one strategy again and again, which

08:28.120 --> 08:34.360
is very long time horizon, capital intensive, difficult technology bets.

08:34.360 --> 08:35.960
And I don't even think I'm particularly good at those.

08:35.960 --> 08:38.120
I just think not many people try them.

08:38.120 --> 08:40.400
So there's very little competition, which is nice.

08:40.400 --> 08:47.280
I mean, I don't have a lot of competition, but the strategy that it takes to now like

08:47.320 --> 08:55.520
take a platform like OpenAI and build a new fast growing defensible consumer enterprise

08:55.520 --> 08:59.880
company, I know almost nothing about, like I know all of the theory, but none of the

08:59.880 --> 09:03.200
practice and I would go find people who have done it and get the practice, get the advice

09:03.200 --> 09:04.200
from them.

09:04.200 --> 09:05.200
All right.

09:05.200 --> 09:06.200
Good advice.

09:06.200 --> 09:08.440
A couple of questions about the underlying tech platform here.

09:08.440 --> 09:14.280
So I've been building neural networks myself since the parameter count was sub one million

09:14.280 --> 09:18.600
and they're actually very useful for a bunch of commercial applications and then kind of

09:18.600 --> 09:22.880
watch them tip into the billion and then the, you know, with GPT two, I think about one

09:22.880 --> 09:27.800
and a half billion or so and then GPT three and now GPT four.

09:27.800 --> 09:28.800
So you go up.

09:28.800 --> 09:33.280
We don't know the current parameter count, but I think it was 175 billion in GPT three

09:33.280 --> 09:37.520
and it was just mind-blowingly different from GPT two and then then GPT four is even more

09:37.520 --> 09:39.680
mind-blowingly different.

09:39.680 --> 09:45.960
So the raw underlying parameter count seems like it's on a trend just listening to Nvidia's

09:45.960 --> 09:54.200
forecast where you can, you can go from a trillion to 10 trillion and then they're saying

09:54.200 --> 09:57.160
up to 10 quadrillion in a decade.

09:57.160 --> 10:02.320
So you've got four factors of 10 or 10,000 X in a decade.

10:02.320 --> 10:05.840
Does that even sound like it's in the right ballpark?

10:05.840 --> 10:08.080
I think it's way too much focus on parameter count.

10:08.080 --> 10:11.800
I mean, parameter count will trend up for sure.

10:11.800 --> 10:17.960
But this reminds me a lot of the gigahertz race in chips in the like nineties and two

10:17.960 --> 10:23.360
thousands where everybody was trying to like point to a big number and then event like

10:23.360 --> 10:26.400
you don't need probably most of you don't know how many gigahertz are on your iPhone,

10:26.400 --> 10:27.400
but it's fast.

10:27.400 --> 10:31.800
Like what we actually care about is capability and I think it's important that what we keep

10:31.800 --> 10:36.080
the focus on is rapidly increasing capability.

10:36.080 --> 10:39.840
And if there's some reason that parameter count should decrease over time or we should

10:39.840 --> 10:43.480
have like multiple models working together, each of which are smaller, we would just do

10:43.480 --> 10:44.480
that.

10:44.480 --> 10:48.640
Like, well, we want to deliver to the world of the most capable, useful and safe models.

10:48.640 --> 10:52.680
We are not here to like jerk ourselves off about parameter count.

10:52.680 --> 10:57.480
Can we quote you on that?

10:57.480 --> 11:06.040
It's going to get quoted no matter what, so, yeah, well, that's my, okay, well, thank you

11:06.040 --> 11:07.680
for taking that away from me.

11:07.680 --> 11:14.520
So, but one thing that's absolutely unique about this class of algorithm versus anything

11:14.520 --> 11:20.320
I've ever seen before is that it surprises you with raw horsepower regardless of whether

11:20.320 --> 11:23.960
you measure it in parameter count or some other way.

11:23.960 --> 11:29.560
It does things that you didn't anticipate purely by putting more horsepower behind it.

11:29.560 --> 11:31.160
And so it takes advantage of the scale.

11:31.160 --> 11:35.560
The analogy I was making this morning is if you have a spreadsheet, you coded it up, you

11:35.560 --> 11:40.320
run it on a computer that's 10,000 times faster, it doesn't really surprise you.

11:40.320 --> 11:44.920
It's nice and responsive, it's still a spreadsheet, whereas this class of algorithm does things

11:44.920 --> 11:47.120
that it just couldn't do before.

11:47.120 --> 11:51.840
And so we actually, one of our partners in our venture fund wrote an entire book on GPT-2

11:51.840 --> 11:56.800
and you can buy it on Amazon, it's called Start Here Romance.

11:56.800 --> 11:58.160
I think about 10 copies of sold.

11:58.160 --> 12:01.880
I bought one of them, so maybe nine copies of sold, but if you read the book, it's just

12:01.880 --> 12:04.960
not a good book.

12:04.960 --> 12:10.120
And here we are, that was four years ago, it's only been four years.

12:10.120 --> 12:17.200
And now the quality of the book has gone from GPT-2, 3, 4, not a good book, it's a somewhat

12:17.200 --> 12:21.480
reasonable book, to now it's possible to write a truly excellent book.

12:21.480 --> 12:25.640
You have to give it the framework, you're still effectively writing the concept, but

12:25.640 --> 12:28.400
it's filling in the words just beautifully.

12:28.400 --> 12:32.680
And so as an author, that could be a force multiplier of something like 10, 100, it just

12:32.680 --> 12:35.680
enables an author to be that much more powerful.

12:35.680 --> 12:40.000
So this class of algorithm then, if the underlying substrate is getting faster and faster and

12:40.000 --> 12:46.440
faster, it's going to do surprising things on a relatively short time scale.

12:46.440 --> 12:49.320
And so I think one of the things that people in the room need to predict is, okay, what

12:49.320 --> 12:55.920
is the next real world society benefiting use case that hits that tipping point on this

12:55.920 --> 12:56.920
curve?

12:56.920 --> 13:00.760
And I think one of the highlights you can give us into, what's going to be possible

13:00.760 --> 13:03.880
that wasn't possible a year prior or two years prior?

13:03.880 --> 13:10.200
Okay, I said I don't have business strategy advice, I just thought of something I do.

13:10.200 --> 13:16.640
I think in new areas like this, one of the right approaches is to let tactics become

13:16.640 --> 13:19.840
strategy instead of the other way around.

13:19.840 --> 13:25.920
And I have my ideas, I'm sure you all have your ideas, maybe we'll be mostly right, we'll

13:25.920 --> 13:33.160
be wrong in some ways, and even the details of how we're right will be wrong about it.

13:33.160 --> 13:38.600
I think you never want to lose sight of vision and focus on the long term, but a very tight

13:38.600 --> 13:43.800
feedback loop of paying attention to what is working and what is not working, and doing

13:43.800 --> 13:47.080
more of the stuff that's working and less of the stuff that's not working.

13:47.080 --> 13:53.440
And just very, very careful user observation can go super far.

13:53.480 --> 14:01.040
So I can speculate on ideas, you all can speculate on ideas, none of that will be as valuable

14:01.040 --> 14:07.440
as putting something out there and really deeply understanding what's happening and being

14:07.440 --> 14:10.440
responsive to it.

14:10.440 --> 14:17.560
For the next question, Sam, when did you know your baby, chat GPT was something really special,

14:17.560 --> 14:22.000
and what was the special sauce that allowed you to pull off something that others haven't?

14:22.120 --> 14:24.120
Dave will come back, but yeah.

14:24.120 --> 14:28.120
Who likes Sam so far?

14:28.120 --> 14:31.600
If Sam was hiring, would you consider being part of his team?

14:31.600 --> 14:34.200
Okay, all right, we got a lot of hands.

14:34.200 --> 14:37.920
Great, yeah, please come, we really need help and it's going to be a pretty exciting

14:37.920 --> 14:40.280
next few years.

14:40.280 --> 14:45.920
I mean, we've been working on it for so long that it's like you kind of know with gradually

14:45.920 --> 14:50.280
increasing confidence that it's really going to work, but this is, you know, we've been

14:50.280 --> 14:52.760
doing the company for seven years.

14:52.760 --> 14:55.280
These things take a long, long time.

14:55.280 --> 15:00.440
I would say by, and like in terms of why it worked when others happened, it's just because

15:00.440 --> 15:04.920
we've like been on the grind sweating every detail for a long time and most people aren't

15:04.920 --> 15:06.880
willing to do that.

15:06.880 --> 15:12.240
In terms of when we knew that chat GPT in particular was going to like catch fire as

15:12.240 --> 15:16.320
a consumer product, probably like 48 hours after launch.

15:16.320 --> 15:17.320
Yeah.

15:17.320 --> 15:18.320
All right.

15:18.320 --> 15:22.320
So before Dave comes to one back, I asked Lex to ask a sexy question.

15:22.320 --> 15:23.320
Hey, Lex.

15:23.320 --> 15:24.320
Hey.

15:24.320 --> 15:25.320
You want to use the communicator?

15:25.320 --> 15:26.320
You're good.

15:26.320 --> 15:27.320
What is it?

15:27.320 --> 15:28.320
It's the Star Trek.

15:28.320 --> 15:29.320
You're good.

15:29.320 --> 15:30.320
I'm good.

15:30.320 --> 15:31.320
Okay.

15:31.320 --> 15:34.320
I grew up in the Soviet Union.

15:34.320 --> 15:36.320
We didn't have Star Trek over there.

15:36.320 --> 15:37.320
Check off.

15:37.320 --> 15:38.320
Second, second season.

15:38.320 --> 15:39.320
Yeah.

15:39.320 --> 15:40.520
Let me ask some sexy controversial questions.

15:40.520 --> 15:47.120
So you got legends in artificial intelligence, Ilya Suskeva and Andrew Kapathy over there.

15:47.120 --> 15:48.120
Who's smarter?

15:48.120 --> 15:49.120
Just kidding.

15:49.120 --> 15:50.120
Just kidding.

15:50.120 --> 15:52.120
You don't have to answer that.

15:52.120 --> 15:53.120
That was a joke.

15:53.120 --> 15:54.120
What?

15:54.120 --> 15:55.120
He was about to.

15:55.120 --> 15:56.120
He was thinking about it.

15:56.120 --> 15:57.120
All right.

15:57.120 --> 15:58.120
I like it.

15:58.120 --> 15:59.120
No.

15:59.120 --> 16:03.160
I just, so we're at MIT and from here with Max Tagmark and others, they put together

16:03.160 --> 16:08.000
this open letter to halt AI development for six months.

16:08.000 --> 16:12.800
What are your thoughts about this open letter?

16:12.800 --> 16:16.440
There's parts of the thrust that I really agree with.

16:16.440 --> 16:21.400
We spent more than six months after we finished training GPT-4 before we released it.

16:21.400 --> 16:28.920
So taking the time to really study the safety of a model to get external audits, external

16:28.920 --> 16:34.000
red teamers, to really try to understand what's going on and mitigate as much as you

16:34.000 --> 16:35.000
can.

16:35.000 --> 16:36.000
That's important.

16:36.000 --> 16:39.120
It's been really nice since we have launched GPT-4.

16:39.120 --> 16:43.200
How many people have said, wow, this is not the most capable model open AI has put up,

16:43.200 --> 16:45.680
but by far the safest and most aligned.

16:45.680 --> 16:49.240
Unless I'm trying to get it to do something bad, it won't.

16:49.240 --> 16:53.040
So that we totally, I totally agree with.

16:53.040 --> 17:00.560
I also agree that as capabilities get more and more serious, that the safety bar has

17:00.560 --> 17:03.000
got to increase.

17:03.000 --> 17:10.520
But unfortunately, I think the letter is missing most technical nuance about where we need

17:10.520 --> 17:11.520
the pause.

17:11.520 --> 17:15.840
It's actually, an earlier version of the letter claimed the open AI is trained in GPT-5 right

17:15.840 --> 17:16.840
now.

17:16.840 --> 17:18.600
We are not and won't for some time.

17:18.600 --> 17:20.320
So in that sense, it was sort of silly.

17:20.320 --> 17:25.280
But we are doing other things on top of GPT-4 that I think have all sorts of safety issues

17:25.280 --> 17:28.960
that are important to address and we're totally left out of the letter.

17:28.960 --> 17:37.560
So I think moving with caution and an increasing rigor for safety issues is really important.

17:37.560 --> 17:41.080
The letter I don't think is the optimal way to address it.

17:41.080 --> 17:42.240
Just a quick question for me.

17:42.240 --> 17:49.400
One more is, you have been extremely open, having a lot of conversations, being honest.

17:49.400 --> 17:50.920
Others at open AI as well.

17:50.920 --> 17:52.800
What's the philosophy behind that?

17:52.800 --> 17:57.560
Because compared to other companies, there are much more clothes in that regard.

17:57.680 --> 18:00.000
Do you plan to continue doing that?

18:00.000 --> 18:02.200
We certainly plan to continue doing that.

18:02.200 --> 18:06.440
The trade-off is like we say dumb stuff sometimes, stuff that turns out to be totally wrong.

18:06.440 --> 18:11.560
I think a lot of other companies don't want to say something until they're sure it's right.

18:11.560 --> 18:16.720
But I think this technology is going to so impact all of us that we believe that engaging

18:16.720 --> 18:22.200
everyone in the discussion, putting these systems out into the world, deeply imperfect

18:22.200 --> 18:27.240
though they are in their current state, so that people get to experience them, think

18:27.240 --> 18:29.960
about them, understand the upsides and the downsides.

18:29.960 --> 18:34.120
It's worth the trade-off, even though we do tend to embarrass ourselves in public and

18:34.120 --> 18:37.960
have to change our minds with new data frequently.

18:37.960 --> 18:42.120
So we're going to keep doing that because we think it's better than any alternative.

18:42.120 --> 18:47.720
And a big part of our goal at Open AI is to get the world to engage with this and think

18:47.720 --> 18:54.280
about it and gradually update and build new institutions or adapt our existing institutions

18:54.280 --> 18:58.440
to be able to figure out what the future we all want is.

18:58.440 --> 19:00.760
So that's kind of why we're here.

19:00.760 --> 19:04.080
So we only have a few minutes left and I have to ask you a question that has been on my

19:04.080 --> 19:06.120
mind since I was 13 years old.

19:06.120 --> 19:11.320
So I think if you read Ray Kurzweil or any of the luminaries in this sector, the day

19:11.320 --> 19:18.680
when the algorithms start writing the code that improves the algorithms is a pivotal

19:18.680 --> 19:19.680
day.

19:19.680 --> 19:23.000
It accelerates the process toward infinity or in the singularity view of the world to

19:23.000 --> 19:25.600
absolute infinity.

19:25.600 --> 19:29.680
And so now a lot of the companies that I'm an investor in or have been co-founder of

19:29.680 --> 19:33.720
are starting to use LLMs for code generation.

19:33.720 --> 19:39.000
And there's an interesting very wide range of lift or improvement in the performance

19:39.000 --> 19:44.280
of an engineer ranging from about 5% to about 20x.

19:44.280 --> 19:47.200
And it depends on what you're trying to do, what type of code, how much context it needs.

19:47.200 --> 19:50.480
A lot of it is related to tuning in the system.

19:50.480 --> 19:51.960
So there's two questions in there.

19:52.000 --> 19:57.240
First, within OpenAI, how much of a force multiplier do you already see within the creation

19:57.240 --> 19:59.320
of the next iteration of the code?

19:59.320 --> 20:03.520
And then the follow-on question is, okay, what does it look like a few months from now,

20:03.520 --> 20:05.200
a year from now, two years from now?

20:05.200 --> 20:10.440
Are we getting close to that day where the thing is so rapidly self-improving that it

20:10.440 --> 20:11.440
hits some...

20:11.440 --> 20:13.680
Yeah, great question.

20:13.680 --> 20:21.280
I think that it is going to be a much fuzzier boundary for getting to self-improvement or

20:21.280 --> 20:22.440
not.

20:22.440 --> 20:27.400
I think what will happen is that more and more of the improvement loop will be aided

20:27.400 --> 20:30.880
by AIs, but humans will still be driving it.

20:30.880 --> 20:33.640
And it's going to go like that for a long time.

20:33.640 --> 20:38.160
And there's a whole bunch of other things that I have never believed in the one day

20:38.160 --> 20:45.120
or one month takeoff for a bunch of reasons, but one of which is how incredibly long it

20:45.120 --> 20:49.920
takes to build new data centers, bigger data centers, even if we knew how to do it right

20:49.920 --> 20:53.560
now, just like waiting for the concrete to dry, getting the power into the building.

20:53.560 --> 20:56.840
Stuff takes a while.

20:56.840 --> 21:00.640
But I think what will happen is humans will be more and more augmented and be able to

21:00.640 --> 21:03.680
do things in the world faster and faster.

21:03.680 --> 21:08.560
And it will not work out like it will not somehow...

21:08.560 --> 21:13.000
Like most of these things don't end up working out quite like the sci-fi books.

21:13.000 --> 21:14.840
And neither will this one.

21:14.840 --> 21:19.200
But the rate of change in the world will increase forever and more from here as humans get better

21:19.200 --> 21:19.800
and better tools.

