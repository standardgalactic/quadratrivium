1
00:00:00,000 --> 00:00:23,600
This particular room is maybe a little different from a lot of that.

2
00:00:23,600 --> 00:00:29,480
Most of the people here in this room are either building companies or are working on plans

3
00:00:29,480 --> 00:00:35,600
to build companies that are in the ecosystem really triggered by chat G.B.T.

4
00:00:35,600 --> 00:00:37,200
My kind of people, I wish they were there.

5
00:00:37,200 --> 00:00:39,120
Yeah, I wish you were here too.

6
00:00:39,120 --> 00:00:42,640
Actually, they are exactly your kind of people.

7
00:00:42,640 --> 00:00:46,000
And I know that part of the mission here is to make the world a better place, but also

8
00:00:46,000 --> 00:00:49,680
to build on top of the platform that you've created and obviously you navigated to the

9
00:00:49,680 --> 00:00:53,840
position you're in in life very deliberately and you're the perfect person to help advise

10
00:00:53,840 --> 00:00:54,840
them.

11
00:00:54,840 --> 00:00:58,440
So we're going to try and keep this on focus in a way that it helps this room as much

12
00:00:58,440 --> 00:00:59,440
as possible.

13
00:00:59,440 --> 00:01:07,320
A hundred people create successful companies.

14
00:01:07,320 --> 00:01:14,320
And Sam, the first thing I'm going to ask you about is, you know, if AGI is in the near

15
00:01:14,320 --> 00:01:20,960
term future, then we're right now at this inflection point where human history has a

16
00:01:20,960 --> 00:01:25,880
period of time up till AGI and then obviously has a completely different history from here

17
00:01:25,880 --> 00:01:27,040
forward.

18
00:01:27,040 --> 00:01:32,280
So it seems to me that at this stage, you're going to be a centerpiece of the history books

19
00:01:32,280 --> 00:01:33,280
no matter how this evolves.

20
00:01:33,280 --> 00:01:34,760
Do you think it's the same?

21
00:01:34,760 --> 00:01:38,400
So I think it's the same in terms of what?

22
00:01:38,400 --> 00:01:45,520
In terms of the way history will describe this moment, this moment being this year of innovation

23
00:01:45,520 --> 00:01:48,280
in this field.

24
00:01:48,280 --> 00:01:54,480
I mean, I hope this will be like a page or a chapter in history books, but I think that

25
00:01:54,600 --> 00:02:00,240
over the next several billion years, such unbelievable things are going to happen that

26
00:02:00,240 --> 00:02:05,400
this will be just sort of like one small part and there will be new and bigger and more

27
00:02:05,400 --> 00:02:09,040
exciting opportunities and challenges in front of us.

28
00:02:09,040 --> 00:02:13,400
So I think one of the things that a lot of people are asking, you know, with prior iterations

29
00:02:13,400 --> 00:02:20,120
of GPT, open source iterations, you had a whole variety of ways of taking that source code

30
00:02:20,240 --> 00:02:25,040
and making a vertical company out of it or an adjacent company, something a federated

31
00:02:25,040 --> 00:02:27,200
learning or something.

32
00:02:27,200 --> 00:02:32,720
In the future iteration of these companies, you've got this highly tunable closed API

33
00:02:32,720 --> 00:02:33,920
to start from.

34
00:02:33,920 --> 00:02:38,120
Any quick advice on, okay, I'm starting a company, now I have to make some decisions

35
00:02:38,120 --> 00:02:39,360
right out of the gate.

36
00:02:39,360 --> 00:02:40,360
What do I start with?

37
00:02:40,360 --> 00:02:44,640
How do I make it work in any given vertical use case?

38
00:02:44,640 --> 00:02:49,680
You know, I think there's always more that stays the same about how to make a good company

39
00:02:49,760 --> 00:02:54,240
than what changes the, and a lot of people, whenever there's like a new platform shift

40
00:02:54,240 --> 00:02:57,120
like this thing, just because they're using the platform, like that's what's going to

41
00:02:57,120 --> 00:02:58,800
guide business strategy.

42
00:02:58,800 --> 00:03:04,320
It doesn't, nothing lets you off the hook for building a product that people love, for

43
00:03:04,320 --> 00:03:08,560
being like very close to your users, fulfilling their needs, for thinking about a long-term

44
00:03:08,560 --> 00:03:10,160
durable business strategy.

45
00:03:10,160 --> 00:03:15,440
Like that's actually probably only more important during a platform shift, not less.

46
00:03:15,440 --> 00:03:19,560
Like if we think back to the launch of the App Store, which is probably the most recent

47
00:03:19,560 --> 00:03:25,600
similar example, there were a ton of companies that built very lightweight things with, I

48
00:03:25,600 --> 00:03:29,760
don't want to call them like exploitative mechanics, but just like, you know, it was

49
00:03:29,760 --> 00:03:31,760
not something durable.

50
00:03:31,760 --> 00:03:36,680
And those companies had incredible meteoric rises and falls.

51
00:03:36,680 --> 00:03:39,680
And then the companies that really like did all of the normal things you're supposed to

52
00:03:39,680 --> 00:03:43,680
do to build a great business, endured for the last 15 years.

53
00:03:43,680 --> 00:03:46,560
And so you definitely want to be in that latter category.

54
00:03:46,560 --> 00:03:51,520
And the technology is just like, this is a new enabler.

55
00:03:51,520 --> 00:03:56,080
But what you have to do as a company is like build a great company that has a long-term

56
00:03:56,080 --> 00:03:58,280
compounding strategic advantage.

57
00:03:58,280 --> 00:04:01,560
And then what about foundation models, just as a starting point?

58
00:04:01,560 --> 00:04:06,000
You know, if I look back two years, one of the best ways to start was to take an existing

59
00:04:06,000 --> 00:04:12,880
foundation model, maybe add some layers and retrain it in a vertical use case.

60
00:04:12,880 --> 00:04:17,080
Now the foundation model, sort of the base model, is maybe a trillion parameters.

61
00:04:17,080 --> 00:04:19,320
So it's much, much bigger.

62
00:04:19,320 --> 00:04:23,680
But your ability to manipulate it without having to retrain it is also far, far more

63
00:04:23,680 --> 00:04:24,680
flexible.

64
00:04:24,680 --> 00:04:28,880
I think you have 50,000 tokens to play with right now in the basic model.

65
00:04:28,880 --> 00:04:29,880
Is that right?

66
00:04:29,880 --> 00:04:30,880
About a million?

67
00:04:30,880 --> 00:04:31,880
32,000 in the biggest model.

68
00:04:31,880 --> 00:04:32,880
32,000?

69
00:04:32,880 --> 00:04:33,880
8,000 in the base model.

70
00:04:33,880 --> 00:04:34,880
Okay.

71
00:04:34,880 --> 00:04:36,800
And actually, so how's that going to evolve?

72
00:04:36,800 --> 00:04:40,520
There are new iterations that are going to come out pretty quickly.

73
00:04:40,520 --> 00:04:48,000
We're still trying to figure out exactly what developers want in terms of model customization.

74
00:04:48,000 --> 00:04:52,280
We're open to doing a lot of things here, and we're, you know, we also hold our, like,

75
00:04:52,280 --> 00:04:53,280
developers are users.

76
00:04:53,280 --> 00:04:58,080
So our goal is to make developers super happy and figure out what they need.

77
00:04:58,080 --> 00:05:02,120
We thought it was going to be much more of a fine tuning story, and we had been thinking

78
00:05:02,120 --> 00:05:04,840
about how to offer that in different ways.

79
00:05:04,840 --> 00:05:09,080
But people are doing pretty amazing things with the base model, and for a bunch of reasons

80
00:05:09,080 --> 00:05:10,800
often seem to prefer that.

81
00:05:10,800 --> 00:05:17,680
So we're, like, actively reconsidering what customization to prioritize, given what users

82
00:05:17,680 --> 00:05:20,520
seem to want and seem to be making work.

83
00:05:20,520 --> 00:05:26,120
As the models get better and better, it does seem like there is a trend towards less and

84
00:05:26,120 --> 00:05:30,840
less of a need to fine tune, and you can do more and more in the context.

85
00:05:30,840 --> 00:05:33,360
And when you say fine tune, you mean changing parameter weights?

86
00:05:33,360 --> 00:05:34,360
Yeah.

87
00:05:34,360 --> 00:05:35,360
Yeah.

88
00:05:35,360 --> 00:05:40,960
And is there going to be an ability at all to change the parameter weights in the GPT

89
00:05:40,960 --> 00:05:41,960
world?

90
00:05:41,960 --> 00:05:44,200
Yeah, we'll definitely offer something there.

91
00:05:44,200 --> 00:05:49,720
But it, like, right now it looks like maybe that will be less used than ability to offer

92
00:05:49,720 --> 00:05:53,480
like super cheap context, like $1 million if we can ever figure that out on the base

93
00:05:53,480 --> 00:05:54,480
model.

94
00:05:54,480 --> 00:05:55,480
Yeah.

95
00:05:55,480 --> 00:05:59,080
Let's drill in on that just a little bit, because it seems like regardless of the specifics,

96
00:05:59,080 --> 00:06:02,600
the trend is toward, as the models are getting bigger and bigger and bigger, so you go from

97
00:06:02,600 --> 00:06:10,120
$1 trillion to $10 trillion parameters, the amount you can achieve with just changing prompt

98
00:06:10,120 --> 00:06:17,560
engineering or changing the tokens that are feeding into it is growing disproportionately

99
00:06:17,560 --> 00:06:18,560
to the model size.

100
00:06:18,560 --> 00:06:20,040
Does that sound right?

101
00:06:20,040 --> 00:06:25,680
Um, disproportionately to the model size, yes, but I think we're like at the end of

102
00:06:25,680 --> 00:06:28,800
the era where it's going to be these like giant, giant models and we'll make them better

103
00:06:28,800 --> 00:06:30,600
in other ways.

104
00:06:30,600 --> 00:06:35,000
But I would say it, like, it grows proportionate to the model capability.

105
00:06:35,000 --> 00:06:36,080
Yep.

106
00:06:36,080 --> 00:06:41,600
And then the investment in the creation of the foundation models is on the order of

107
00:06:41,600 --> 00:06:46,760
$50 million, $100 million just in the training process.

108
00:06:46,760 --> 00:06:47,760
So it seems like...

109
00:06:47,760 --> 00:06:48,760
It's much more than that.

110
00:06:48,760 --> 00:06:49,760
Is it?

111
00:06:49,760 --> 00:06:50,760
What's the magnitude there?

112
00:06:50,760 --> 00:06:53,600
We don't share, but it's much more than that.

113
00:06:53,600 --> 00:06:54,600
Okay.

114
00:06:54,600 --> 00:06:58,160
And rising, I assume, over time.

115
00:06:58,160 --> 00:06:59,160
Yeah.

116
00:06:59,160 --> 00:07:03,440
So then somebody trying to start from scratch, somebody trying to start from scratch, you

117
00:07:03,440 --> 00:07:05,440
know, is trying to catch up to something that's in order.

118
00:07:05,440 --> 00:07:10,000
And maybe, or maybe we're all being incredibly dumb and we're missing one big idea and all

119
00:07:10,000 --> 00:07:14,080
of this is not as hard or expensive as we think and there will be a totally new paradigm

120
00:07:14,080 --> 00:07:17,920
that absolutes us, which would be great, not great for us, but great for the world.

121
00:07:17,920 --> 00:07:18,920
Yeah.

122
00:07:18,920 --> 00:07:19,920
Yeah.

123
00:07:19,920 --> 00:07:20,920
So let me get your take on something.

124
00:07:20,920 --> 00:07:26,600
So Paul Graham calls you the greatest business strategist that he's ever encountered.

125
00:07:26,600 --> 00:07:30,320
And of course, all these people are wrestling with their business strategy and what exactly

126
00:07:30,320 --> 00:07:31,800
to build and where.

127
00:07:31,800 --> 00:07:36,360
And so I've been asking you questions that are more or less vertical use cases that sit

128
00:07:36,360 --> 00:07:41,640
on top of GPT-4 and soon GPT-5 and so on.

129
00:07:41,640 --> 00:07:44,360
But there's also all these business models that are adjacent.

130
00:07:44,360 --> 00:07:52,560
So things like federated learning or data conditioning or just deployment and so those

131
00:07:52,560 --> 00:07:53,920
are interesting business models too.

132
00:07:53,920 --> 00:08:01,120
If you were just investing in a class of company that's in the ecosystem, any thoughts on where

133
00:08:01,120 --> 00:08:05,480
the greater returns are, where the faster growing more interesting business models are?

134
00:08:05,480 --> 00:08:07,080
I don't think PG quite said that.

135
00:08:07,080 --> 00:08:14,360
I know he said something like in that direction, but in any case, I don't think it'd be true.

136
00:08:14,360 --> 00:08:19,400
I think there are people who are unbelievable business strategists and I'm not one of them.

137
00:08:19,400 --> 00:08:24,400
So I hesitate to give advice here.

138
00:08:24,400 --> 00:08:28,120
The only thing I know how to do, I think, is this one strategy again and again, which

139
00:08:28,120 --> 00:08:34,360
is very long time horizon, capital intensive, difficult technology bets.

140
00:08:34,360 --> 00:08:35,960
And I don't even think I'm particularly good at those.

141
00:08:35,960 --> 00:08:38,120
I just think not many people try them.

142
00:08:38,120 --> 00:08:40,400
So there's very little competition, which is nice.

143
00:08:40,400 --> 00:08:47,280
I mean, I don't have a lot of competition, but the strategy that it takes to now like

144
00:08:47,320 --> 00:08:55,520
take a platform like OpenAI and build a new fast growing defensible consumer enterprise

145
00:08:55,520 --> 00:08:59,880
company, I know almost nothing about, like I know all of the theory, but none of the

146
00:08:59,880 --> 00:09:03,200
practice and I would go find people who have done it and get the practice, get the advice

147
00:09:03,200 --> 00:09:04,200
from them.

148
00:09:04,200 --> 00:09:05,200
All right.

149
00:09:05,200 --> 00:09:06,200
Good advice.

150
00:09:06,200 --> 00:09:08,440
A couple of questions about the underlying tech platform here.

151
00:09:08,440 --> 00:09:14,280
So I've been building neural networks myself since the parameter count was sub one million

152
00:09:14,280 --> 00:09:18,600
and they're actually very useful for a bunch of commercial applications and then kind of

153
00:09:18,600 --> 00:09:22,880
watch them tip into the billion and then the, you know, with GPT two, I think about one

154
00:09:22,880 --> 00:09:27,800
and a half billion or so and then GPT three and now GPT four.

155
00:09:27,800 --> 00:09:28,800
So you go up.

156
00:09:28,800 --> 00:09:33,280
We don't know the current parameter count, but I think it was 175 billion in GPT three

157
00:09:33,280 --> 00:09:37,520
and it was just mind-blowingly different from GPT two and then then GPT four is even more

158
00:09:37,520 --> 00:09:39,680
mind-blowingly different.

159
00:09:39,680 --> 00:09:45,960
So the raw underlying parameter count seems like it's on a trend just listening to Nvidia's

160
00:09:45,960 --> 00:09:54,200
forecast where you can, you can go from a trillion to 10 trillion and then they're saying

161
00:09:54,200 --> 00:09:57,160
up to 10 quadrillion in a decade.

162
00:09:57,160 --> 00:10:02,320
So you've got four factors of 10 or 10,000 X in a decade.

163
00:10:02,320 --> 00:10:05,840
Does that even sound like it's in the right ballpark?

164
00:10:05,840 --> 00:10:08,080
I think it's way too much focus on parameter count.

165
00:10:08,080 --> 00:10:11,800
I mean, parameter count will trend up for sure.

166
00:10:11,800 --> 00:10:17,960
But this reminds me a lot of the gigahertz race in chips in the like nineties and two

167
00:10:17,960 --> 00:10:23,360
thousands where everybody was trying to like point to a big number and then event like

168
00:10:23,360 --> 00:10:26,400
you don't need probably most of you don't know how many gigahertz are on your iPhone,

169
00:10:26,400 --> 00:10:27,400
but it's fast.

170
00:10:27,400 --> 00:10:31,800
Like what we actually care about is capability and I think it's important that what we keep

171
00:10:31,800 --> 00:10:36,080
the focus on is rapidly increasing capability.

172
00:10:36,080 --> 00:10:39,840
And if there's some reason that parameter count should decrease over time or we should

173
00:10:39,840 --> 00:10:43,480
have like multiple models working together, each of which are smaller, we would just do

174
00:10:43,480 --> 00:10:44,480
that.

175
00:10:44,480 --> 00:10:48,640
Like, well, we want to deliver to the world of the most capable, useful and safe models.

176
00:10:48,640 --> 00:10:52,680
We are not here to like jerk ourselves off about parameter count.

177
00:10:52,680 --> 00:10:57,480
Can we quote you on that?

178
00:10:57,480 --> 00:11:06,040
It's going to get quoted no matter what, so, yeah, well, that's my, okay, well, thank you

179
00:11:06,040 --> 00:11:07,680
for taking that away from me.

180
00:11:07,680 --> 00:11:14,520
So, but one thing that's absolutely unique about this class of algorithm versus anything

181
00:11:14,520 --> 00:11:20,320
I've ever seen before is that it surprises you with raw horsepower regardless of whether

182
00:11:20,320 --> 00:11:23,960
you measure it in parameter count or some other way.

183
00:11:23,960 --> 00:11:29,560
It does things that you didn't anticipate purely by putting more horsepower behind it.

184
00:11:29,560 --> 00:11:31,160
And so it takes advantage of the scale.

185
00:11:31,160 --> 00:11:35,560
The analogy I was making this morning is if you have a spreadsheet, you coded it up, you

186
00:11:35,560 --> 00:11:40,320
run it on a computer that's 10,000 times faster, it doesn't really surprise you.

187
00:11:40,320 --> 00:11:44,920
It's nice and responsive, it's still a spreadsheet, whereas this class of algorithm does things

188
00:11:44,920 --> 00:11:47,120
that it just couldn't do before.

189
00:11:47,120 --> 00:11:51,840
And so we actually, one of our partners in our venture fund wrote an entire book on GPT-2

190
00:11:51,840 --> 00:11:56,800
and you can buy it on Amazon, it's called Start Here Romance.

191
00:11:56,800 --> 00:11:58,160
I think about 10 copies of sold.

192
00:11:58,160 --> 00:12:01,880
I bought one of them, so maybe nine copies of sold, but if you read the book, it's just

193
00:12:01,880 --> 00:12:04,960
not a good book.

194
00:12:04,960 --> 00:12:10,120
And here we are, that was four years ago, it's only been four years.

195
00:12:10,120 --> 00:12:17,200
And now the quality of the book has gone from GPT-2, 3, 4, not a good book, it's a somewhat

196
00:12:17,200 --> 00:12:21,480
reasonable book, to now it's possible to write a truly excellent book.

197
00:12:21,480 --> 00:12:25,640
You have to give it the framework, you're still effectively writing the concept, but

198
00:12:25,640 --> 00:12:28,400
it's filling in the words just beautifully.

199
00:12:28,400 --> 00:12:32,680
And so as an author, that could be a force multiplier of something like 10, 100, it just

200
00:12:32,680 --> 00:12:35,680
enables an author to be that much more powerful.

201
00:12:35,680 --> 00:12:40,000
So this class of algorithm then, if the underlying substrate is getting faster and faster and

202
00:12:40,000 --> 00:12:46,440
faster, it's going to do surprising things on a relatively short time scale.

203
00:12:46,440 --> 00:12:49,320
And so I think one of the things that people in the room need to predict is, okay, what

204
00:12:49,320 --> 00:12:55,920
is the next real world society benefiting use case that hits that tipping point on this

205
00:12:55,920 --> 00:12:56,920
curve?

206
00:12:56,920 --> 00:13:00,760
And I think one of the highlights you can give us into, what's going to be possible

207
00:13:00,760 --> 00:13:03,880
that wasn't possible a year prior or two years prior?

208
00:13:03,880 --> 00:13:10,200
Okay, I said I don't have business strategy advice, I just thought of something I do.

209
00:13:10,200 --> 00:13:16,640
I think in new areas like this, one of the right approaches is to let tactics become

210
00:13:16,640 --> 00:13:19,840
strategy instead of the other way around.

211
00:13:19,840 --> 00:13:25,920
And I have my ideas, I'm sure you all have your ideas, maybe we'll be mostly right, we'll

212
00:13:25,920 --> 00:13:33,160
be wrong in some ways, and even the details of how we're right will be wrong about it.

213
00:13:33,160 --> 00:13:38,600
I think you never want to lose sight of vision and focus on the long term, but a very tight

214
00:13:38,600 --> 00:13:43,800
feedback loop of paying attention to what is working and what is not working, and doing

215
00:13:43,800 --> 00:13:47,080
more of the stuff that's working and less of the stuff that's not working.

216
00:13:47,080 --> 00:13:53,440
And just very, very careful user observation can go super far.

217
00:13:53,480 --> 00:14:01,040
So I can speculate on ideas, you all can speculate on ideas, none of that will be as valuable

218
00:14:01,040 --> 00:14:07,440
as putting something out there and really deeply understanding what's happening and being

219
00:14:07,440 --> 00:14:10,440
responsive to it.

220
00:14:10,440 --> 00:14:17,560
For the next question, Sam, when did you know your baby, chat GPT was something really special,

221
00:14:17,560 --> 00:14:22,000
and what was the special sauce that allowed you to pull off something that others haven't?

222
00:14:22,120 --> 00:14:24,120
Dave will come back, but yeah.

223
00:14:24,120 --> 00:14:28,120
Who likes Sam so far?

224
00:14:28,120 --> 00:14:31,600
If Sam was hiring, would you consider being part of his team?

225
00:14:31,600 --> 00:14:34,200
Okay, all right, we got a lot of hands.

226
00:14:34,200 --> 00:14:37,920
Great, yeah, please come, we really need help and it's going to be a pretty exciting

227
00:14:37,920 --> 00:14:40,280
next few years.

228
00:14:40,280 --> 00:14:45,920
I mean, we've been working on it for so long that it's like you kind of know with gradually

229
00:14:45,920 --> 00:14:50,280
increasing confidence that it's really going to work, but this is, you know, we've been

230
00:14:50,280 --> 00:14:52,760
doing the company for seven years.

231
00:14:52,760 --> 00:14:55,280
These things take a long, long time.

232
00:14:55,280 --> 00:15:00,440
I would say by, and like in terms of why it worked when others happened, it's just because

233
00:15:00,440 --> 00:15:04,920
we've like been on the grind sweating every detail for a long time and most people aren't

234
00:15:04,920 --> 00:15:06,880
willing to do that.

235
00:15:06,880 --> 00:15:12,240
In terms of when we knew that chat GPT in particular was going to like catch fire as

236
00:15:12,240 --> 00:15:16,320
a consumer product, probably like 48 hours after launch.

237
00:15:16,320 --> 00:15:17,320
Yeah.

238
00:15:17,320 --> 00:15:18,320
All right.

239
00:15:18,320 --> 00:15:22,320
So before Dave comes to one back, I asked Lex to ask a sexy question.

240
00:15:22,320 --> 00:15:23,320
Hey, Lex.

241
00:15:23,320 --> 00:15:24,320
Hey.

242
00:15:24,320 --> 00:15:25,320
You want to use the communicator?

243
00:15:25,320 --> 00:15:26,320
You're good.

244
00:15:26,320 --> 00:15:27,320
What is it?

245
00:15:27,320 --> 00:15:28,320
It's the Star Trek.

246
00:15:28,320 --> 00:15:29,320
You're good.

247
00:15:29,320 --> 00:15:30,320
I'm good.

248
00:15:30,320 --> 00:15:31,320
Okay.

249
00:15:31,320 --> 00:15:34,320
I grew up in the Soviet Union.

250
00:15:34,320 --> 00:15:36,320
We didn't have Star Trek over there.

251
00:15:36,320 --> 00:15:37,320
Check off.

252
00:15:37,320 --> 00:15:38,320
Second, second season.

253
00:15:38,320 --> 00:15:39,320
Yeah.

254
00:15:39,320 --> 00:15:40,520
Let me ask some sexy controversial questions.

255
00:15:40,520 --> 00:15:47,120
So you got legends in artificial intelligence, Ilya Suskeva and Andrew Kapathy over there.

256
00:15:47,120 --> 00:15:48,120
Who's smarter?

257
00:15:48,120 --> 00:15:49,120
Just kidding.

258
00:15:49,120 --> 00:15:50,120
Just kidding.

259
00:15:50,120 --> 00:15:52,120
You don't have to answer that.

260
00:15:52,120 --> 00:15:53,120
That was a joke.

261
00:15:53,120 --> 00:15:54,120
What?

262
00:15:54,120 --> 00:15:55,120
He was about to.

263
00:15:55,120 --> 00:15:56,120
He was thinking about it.

264
00:15:56,120 --> 00:15:57,120
All right.

265
00:15:57,120 --> 00:15:58,120
I like it.

266
00:15:58,120 --> 00:15:59,120
No.

267
00:15:59,120 --> 00:16:03,160
I just, so we're at MIT and from here with Max Tagmark and others, they put together

268
00:16:03,160 --> 00:16:08,000
this open letter to halt AI development for six months.

269
00:16:08,000 --> 00:16:12,800
What are your thoughts about this open letter?

270
00:16:12,800 --> 00:16:16,440
There's parts of the thrust that I really agree with.

271
00:16:16,440 --> 00:16:21,400
We spent more than six months after we finished training GPT-4 before we released it.

272
00:16:21,400 --> 00:16:28,920
So taking the time to really study the safety of a model to get external audits, external

273
00:16:28,920 --> 00:16:34,000
red teamers, to really try to understand what's going on and mitigate as much as you

274
00:16:34,000 --> 00:16:35,000
can.

275
00:16:35,000 --> 00:16:36,000
That's important.

276
00:16:36,000 --> 00:16:39,120
It's been really nice since we have launched GPT-4.

277
00:16:39,120 --> 00:16:43,200
How many people have said, wow, this is not the most capable model open AI has put up,

278
00:16:43,200 --> 00:16:45,680
but by far the safest and most aligned.

279
00:16:45,680 --> 00:16:49,240
Unless I'm trying to get it to do something bad, it won't.

280
00:16:49,240 --> 00:16:53,040
So that we totally, I totally agree with.

281
00:16:53,040 --> 00:17:00,560
I also agree that as capabilities get more and more serious, that the safety bar has

282
00:17:00,560 --> 00:17:03,000
got to increase.

283
00:17:03,000 --> 00:17:10,520
But unfortunately, I think the letter is missing most technical nuance about where we need

284
00:17:10,520 --> 00:17:11,520
the pause.

285
00:17:11,520 --> 00:17:15,840
It's actually, an earlier version of the letter claimed the open AI is trained in GPT-5 right

286
00:17:15,840 --> 00:17:16,840
now.

287
00:17:16,840 --> 00:17:18,600
We are not and won't for some time.

288
00:17:18,600 --> 00:17:20,320
So in that sense, it was sort of silly.

289
00:17:20,320 --> 00:17:25,280
But we are doing other things on top of GPT-4 that I think have all sorts of safety issues

290
00:17:25,280 --> 00:17:28,960
that are important to address and we're totally left out of the letter.

291
00:17:28,960 --> 00:17:37,560
So I think moving with caution and an increasing rigor for safety issues is really important.

292
00:17:37,560 --> 00:17:41,080
The letter I don't think is the optimal way to address it.

293
00:17:41,080 --> 00:17:42,240
Just a quick question for me.

294
00:17:42,240 --> 00:17:49,400
One more is, you have been extremely open, having a lot of conversations, being honest.

295
00:17:49,400 --> 00:17:50,920
Others at open AI as well.

296
00:17:50,920 --> 00:17:52,800
What's the philosophy behind that?

297
00:17:52,800 --> 00:17:57,560
Because compared to other companies, there are much more clothes in that regard.

298
00:17:57,680 --> 00:18:00,000
Do you plan to continue doing that?

299
00:18:00,000 --> 00:18:02,200
We certainly plan to continue doing that.

300
00:18:02,200 --> 00:18:06,440
The trade-off is like we say dumb stuff sometimes, stuff that turns out to be totally wrong.

301
00:18:06,440 --> 00:18:11,560
I think a lot of other companies don't want to say something until they're sure it's right.

302
00:18:11,560 --> 00:18:16,720
But I think this technology is going to so impact all of us that we believe that engaging

303
00:18:16,720 --> 00:18:22,200
everyone in the discussion, putting these systems out into the world, deeply imperfect

304
00:18:22,200 --> 00:18:27,240
though they are in their current state, so that people get to experience them, think

305
00:18:27,240 --> 00:18:29,960
about them, understand the upsides and the downsides.

306
00:18:29,960 --> 00:18:34,120
It's worth the trade-off, even though we do tend to embarrass ourselves in public and

307
00:18:34,120 --> 00:18:37,960
have to change our minds with new data frequently.

308
00:18:37,960 --> 00:18:42,120
So we're going to keep doing that because we think it's better than any alternative.

309
00:18:42,120 --> 00:18:47,720
And a big part of our goal at Open AI is to get the world to engage with this and think

310
00:18:47,720 --> 00:18:54,280
about it and gradually update and build new institutions or adapt our existing institutions

311
00:18:54,280 --> 00:18:58,440
to be able to figure out what the future we all want is.

312
00:18:58,440 --> 00:19:00,760
So that's kind of why we're here.

313
00:19:00,760 --> 00:19:04,080
So we only have a few minutes left and I have to ask you a question that has been on my

314
00:19:04,080 --> 00:19:06,120
mind since I was 13 years old.

315
00:19:06,120 --> 00:19:11,320
So I think if you read Ray Kurzweil or any of the luminaries in this sector, the day

316
00:19:11,320 --> 00:19:18,680
when the algorithms start writing the code that improves the algorithms is a pivotal

317
00:19:18,680 --> 00:19:19,680
day.

318
00:19:19,680 --> 00:19:23,000
It accelerates the process toward infinity or in the singularity view of the world to

319
00:19:23,000 --> 00:19:25,600
absolute infinity.

320
00:19:25,600 --> 00:19:29,680
And so now a lot of the companies that I'm an investor in or have been co-founder of

321
00:19:29,680 --> 00:19:33,720
are starting to use LLMs for code generation.

322
00:19:33,720 --> 00:19:39,000
And there's an interesting very wide range of lift or improvement in the performance

323
00:19:39,000 --> 00:19:44,280
of an engineer ranging from about 5% to about 20x.

324
00:19:44,280 --> 00:19:47,200
And it depends on what you're trying to do, what type of code, how much context it needs.

325
00:19:47,200 --> 00:19:50,480
A lot of it is related to tuning in the system.

326
00:19:50,480 --> 00:19:51,960
So there's two questions in there.

327
00:19:52,000 --> 00:19:57,240
First, within OpenAI, how much of a force multiplier do you already see within the creation

328
00:19:57,240 --> 00:19:59,320
of the next iteration of the code?

329
00:19:59,320 --> 00:20:03,520
And then the follow-on question is, okay, what does it look like a few months from now,

330
00:20:03,520 --> 00:20:05,200
a year from now, two years from now?

331
00:20:05,200 --> 00:20:10,440
Are we getting close to that day where the thing is so rapidly self-improving that it

332
00:20:10,440 --> 00:20:11,440
hits some...

333
00:20:11,440 --> 00:20:13,680
Yeah, great question.

334
00:20:13,680 --> 00:20:21,280
I think that it is going to be a much fuzzier boundary for getting to self-improvement or

335
00:20:21,280 --> 00:20:22,440
not.

336
00:20:22,440 --> 00:20:27,400
I think what will happen is that more and more of the improvement loop will be aided

337
00:20:27,400 --> 00:20:30,880
by AIs, but humans will still be driving it.

338
00:20:30,880 --> 00:20:33,640
And it's going to go like that for a long time.

339
00:20:33,640 --> 00:20:38,160
And there's a whole bunch of other things that I have never believed in the one day

340
00:20:38,160 --> 00:20:45,120
or one month takeoff for a bunch of reasons, but one of which is how incredibly long it

341
00:20:45,120 --> 00:20:49,920
takes to build new data centers, bigger data centers, even if we knew how to do it right

342
00:20:49,920 --> 00:20:53,560
now, just like waiting for the concrete to dry, getting the power into the building.

343
00:20:53,560 --> 00:20:56,840
Stuff takes a while.

344
00:20:56,840 --> 00:21:00,640
But I think what will happen is humans will be more and more augmented and be able to

345
00:21:00,640 --> 00:21:03,680
do things in the world faster and faster.

346
00:21:03,680 --> 00:21:08,560
And it will not work out like it will not somehow...

347
00:21:08,560 --> 00:21:13,000
Like most of these things don't end up working out quite like the sci-fi books.

348
00:21:13,000 --> 00:21:14,840
And neither will this one.

349
00:21:14,840 --> 00:21:19,200
But the rate of change in the world will increase forever and more from here as humans get better

350
00:21:19,200 --> 00:21:19,800
and better tools.

