WEBVTT

00:00.000 --> 00:03.680
I want to ask Sam some questions myself and then after that we'll open it up to a broader

00:03.680 --> 00:07.200
set of people so everyone can have some time.

00:07.200 --> 00:12.160
You know Sam, just before we start, I want everyone to understand your story because

00:12.160 --> 00:16.560
you have such an interesting background in the tech ecosystem, you know, help us walk

00:16.560 --> 00:21.960
us through from graduating, not graduating, joining Stanford, dropping out, running Y

00:21.960 --> 00:27.040
Combinator, running a different startup before that, and now running OpenAI and a number

00:27.040 --> 00:28.040
of things.

00:28.040 --> 00:30.720
Just help us understand how you came to where you are right now.

00:30.720 --> 00:39.240
Yeah, so I started at Stanford where we met and I was already in love with computer science

00:39.240 --> 00:42.080
but I really fell in love with it once I got there.

00:42.080 --> 00:46.000
I actually went to study AI but at the time AI was really not working at all.

00:46.000 --> 00:50.880
In fact, very memorably one of my professors said the only sure way to have a bad career

00:50.880 --> 00:53.000
in AI is to work on neural networks.

00:53.000 --> 00:56.200
We've decided those don't work.

00:56.200 --> 00:59.200
And so I got kind of discouraged and I started a company.

00:59.200 --> 01:00.200
That was a great experience.

01:00.200 --> 01:05.400
The company didn't work out that well but I kind of like learned about startups and

01:05.400 --> 01:08.600
thought they were a very powerful force and something I was very excited about.

01:08.600 --> 01:14.920
So I then ran YC for a while and while I was doing that, I got newly excited about the

01:14.920 --> 01:19.600
idea of startups that take on hard technical challenges.

01:19.600 --> 01:24.400
And I sort of thought it was curious to me more people weren't doing that.

01:24.400 --> 01:26.920
It seemed like a really valuable opportunity.

01:26.920 --> 01:30.000
With some other people, started OpenAI is one of those examples and many other things

01:30.000 --> 01:35.000
which have gone on to be pretty exciting but really fell in love with OpenAI.

01:35.000 --> 01:40.880
Once it seemed clear that we were really going to have a chance at making true general purpose

01:40.880 --> 01:45.960
AI like a system that could do what a human can do and contribute new knowledge to society,

01:45.960 --> 01:49.840
I got really excited and wanted to go work on that.

01:49.840 --> 01:54.360
And so stop being an investor and now I do that.

01:54.360 --> 01:56.920
So first of all, what is OpenAI?

01:56.920 --> 01:58.520
Is it just chat GPT?

01:58.520 --> 01:59.520
Are they the same thing?

01:59.520 --> 02:00.520
Are they different?

02:00.520 --> 02:03.200
Just help us understand what the company does.

02:03.200 --> 02:08.560
We are a company that is doing research and deployment to try to figure out how to build

02:08.560 --> 02:13.480
AGI and how to responsibly deploy that into the world for maximum benefit.

02:13.480 --> 02:16.840
So this is unlike other technologies.

02:16.840 --> 02:20.480
Other technologies are like this too but this is a strong case of a technology that

02:20.600 --> 02:24.800
on the one hand is the most exciting, most promising, coolest thing.

02:24.800 --> 02:27.680
I think that humanity will have yet built.

02:27.680 --> 02:32.680
We can cure all disease, we can give everybody a great education, better healthcare, massively

02:32.680 --> 02:37.320
increased productivity, huge scientific discovery, all of these wonderful things and we want

02:37.320 --> 02:41.880
to make sure that people get that benefit, that benefit is distributed equitably.

02:41.880 --> 02:47.280
And on the other hand, there are the obvious concerns about the power of this technology

02:47.280 --> 02:49.880
used in a negative direction.

02:50.000 --> 02:56.080
So we want to be a force to help manage those risks so that we all get to enjoy the benefits.

02:56.080 --> 02:59.960
Chat GPT is definitely what we are best known for so I guess there is sort of synonymous

02:59.960 --> 03:03.800
at this point but OpenAI is really about this quest for AGI.

03:03.800 --> 03:06.240
Interesting.

03:06.240 --> 03:09.360
So help us understand, I mean all of us have played with it, right?

03:09.360 --> 03:13.720
We have poems getting written by it, we have all asked it fun trivia questions, teller

03:13.720 --> 03:19.440
and answers but help us understand, you without a doubt have a better understanding of how

03:19.520 --> 03:24.080
it's getting used all around the world and all sorts of different industries, vocations,

03:24.080 --> 03:25.840
reasons.

03:25.840 --> 03:29.880
Talk to us a little bit about some of the most interesting things that you've seen.

03:29.880 --> 03:33.960
For example, what's the most surprising use case of some of the technologies that you

03:33.960 --> 03:37.440
guys have built that you've seen recently?

03:37.440 --> 03:41.840
So the main thing I would say that's interesting about it is its generality.

03:41.840 --> 03:45.080
There's a lot of other systems that can go do this thing well or that thing well or this

03:45.080 --> 03:48.120
thing and in many cases better than Chat GPT.

03:48.560 --> 03:53.880
There's probably not an AI that can write a better poem or whatever but other categories

03:53.880 --> 03:57.880
you could find something that's maybe better but the fact that this one system is truly

03:57.880 --> 04:02.600
general purpose and can do so many things means that people are integrating it into

04:02.600 --> 04:08.000
their workflow as a very powerful tool and so the same thing that can help you write

04:08.000 --> 04:13.600
computer code, one of the areas that we've seen the biggest impact is what coders are

04:13.600 --> 04:17.920
using this for, doubling, tripling their productivity.

04:17.920 --> 04:22.400
There was a paper that just came out that when Italy temporarily banned Chat GPT developer

04:22.400 --> 04:30.200
productivity fell in half on a fairly big study but it can do that, it can also help

04:30.200 --> 04:35.120
you find information, it can help you write a poem, it can help you summarize documents,

04:35.120 --> 04:40.440
it can translate things and people are using this which we hoped would happen as this sort

04:40.440 --> 04:44.960
of super assistant that just makes them more and more productive and it's that generality

04:44.960 --> 04:48.280
that I think is the coolest part.

04:48.280 --> 04:53.200
So with so much ability that maybe even you guys haven't even thought about how people

04:53.200 --> 04:58.360
are using it when you developed it and launched it, I'm sure you've seen a lot of interesting

04:58.360 --> 05:01.360
use cases right here out of India itself.

05:01.360 --> 05:04.280
Can you tell us something or just give us an example of something you've seen that's

05:04.280 --> 05:08.480
really inspired you that you've seen come out of the Indian market?

05:08.480 --> 05:14.000
So India has been a country that has really truly embraced Chat GPT in a way, maybe you

05:14.000 --> 05:17.080
can tell me why, I'm sort of curious, I'm hoping to learn while I'm here, we're very

05:17.080 --> 05:24.880
delighted but there has been a lot of early adoption and real enthusiasm from the users.

05:24.880 --> 05:30.920
One of the very earliest things like in the first weeks of launching Chat GPT, we heard

05:30.920 --> 05:37.120
about a farmer in India who wasn't able to access government services and via like Chat

05:37.120 --> 05:41.680
GPT hooked up to WhatsApp in some sort of complicated way, was then able to.

05:41.680 --> 05:44.600
And that was like one of the early things where we're like, huh, we did not think that

05:44.600 --> 05:46.800
was going to happen.

05:46.800 --> 05:52.640
And just to expand on it, so what I've understood about OpenAI is Chat GPT is one implementation

05:52.640 --> 05:59.720
of the things you've built but you have capabilities to real time translate, to transcribe audio

05:59.720 --> 06:05.360
and detects and are you seeing people use these in combination in ways that are surprising?

06:05.600 --> 06:10.240
Well, we recently launched an iPhone app that has speech recognition in it, which is that's

06:10.240 --> 06:12.640
hooking up two of our models together and people love that.

06:12.640 --> 06:19.440
But the main point that I would like to get across is none of the current systems really

06:19.440 --> 06:20.440
matter.

06:20.440 --> 06:24.680
Like, we're going to look back at GPT4 and you know, I don't know if any of you have

06:24.680 --> 06:30.320
like picked up an iPhone, original iPhone in recent years but it's like, wow, I cannot

06:30.320 --> 06:32.160
believe we're excited about this.

06:32.160 --> 06:38.960
Each pixel is like that big, it just feels like this incredibly antiquated thing.

06:38.960 --> 06:43.880
The curve here is going to be much, much steeper and what the systems are going to be capable

06:43.880 --> 06:48.800
of in the not distant future, we think, is going to be very dramatically different.

06:48.800 --> 06:54.760
So this is like a system that, I don't even know what the right, this is like the old

06:54.760 --> 06:59.960
first like grayscale Nokia phone that looked like a little candy bar and the iPhone 14

06:59.960 --> 07:02.560
is coming.

07:02.560 --> 07:08.360
So what I would say is, it's a mistake to get too focused on the current systems, their

07:08.360 --> 07:12.400
limitations, their capabilities, the impact they're having.

07:12.400 --> 07:17.920
The thing that matters here is we are on an exponential curve, truly, two big miracles

07:17.920 --> 07:19.480
I think in the field.

07:19.480 --> 07:25.720
Number one, we have an algorithm that can genuinely, truly like no tricks learn and number two,

07:25.720 --> 07:29.440
it gets predictably better with scale.

07:29.440 --> 07:35.400
And that, we're going to look back, I think, on those two realizations as a turning point

07:35.400 --> 07:39.640
in human history when you put them together, but what it means is that the rate of progress

07:39.640 --> 07:42.400
in the coming years, the capabilities are going to be significant.

07:42.400 --> 07:48.720
So it's totally cool that chatGVT can write that poem when a future system can like cure

07:48.720 --> 07:55.480
all disease or help us address climate change or radically improve education or make us

07:55.480 --> 07:58.520
all like 10 or 100 times more productive than what we do.

07:58.520 --> 07:59.840
That's quite impactful.

07:59.840 --> 08:02.040
It's amazing.

08:02.040 --> 08:07.080
Now let's flip to the other side of this because there's no doubt there's incredible power

08:07.080 --> 08:11.440
in this technology and with that comes challenges.

08:11.440 --> 08:15.280
I want to play a clip, maybe you guys can put on a clip of something I recently heard

08:15.280 --> 08:18.760
Sam speak somewhere and we can talk about it a bit.

08:18.760 --> 08:20.280
Could you play the clip please?

08:20.280 --> 08:23.880
Hi, my name is Sam and I'm happy to be here today.

08:23.880 --> 08:25.880
Thank you all for joining.

08:25.880 --> 08:30.400
I also wanted to say that the gentleman on stage with me is incredibly good looking and

08:30.400 --> 08:34.440
I also want to say that you should be very careful with videos generated with artificial

08:34.440 --> 08:36.400
intelligence technology.

08:36.400 --> 08:42.120
Okay, so you didn't say that recently, clearly that was just a ploy here and thank you by

08:42.120 --> 08:43.120
the way.

08:43.120 --> 08:46.120
Totally agree with that one.

08:46.120 --> 08:51.240
But nonetheless, I think it raises a real question, right, when this video, if you look

08:51.240 --> 08:54.560
closely you can see the lips aren't perfectly synced, but like you said this stuff is only

08:54.600 --> 08:58.280
going to get better and exponentially better.

08:58.280 --> 09:02.120
Fundamental questions are on authenticity, what's real and what's fake, how do we handle

09:02.120 --> 09:03.120
that?

09:03.120 --> 09:07.680
Yeah, so that was like deeply in the uncanny valley, it's very strange to watch, but we're

09:07.680 --> 09:14.600
not that far away from something that looks perfect and there's a lot of fear right now

09:14.600 --> 09:19.520
about the impact this is going to have on elections and on our society and how we ever

09:19.520 --> 09:22.280
trust media that we see.

09:22.320 --> 09:25.920
I have some fear there, but I think we're actually going to, when it comes to like a

09:25.920 --> 09:29.880
video like that, I think as a society we're going to rise to the occasion.

09:29.880 --> 09:35.360
We're going to learn very quickly that we don't trust videos unless we trust the sort

09:35.360 --> 09:36.800
of provenance.

09:36.800 --> 09:41.200
We'll have techniques like watermarking detectors, more than that I suspect at some point if people

09:41.200 --> 09:45.800
are saying something really important they'll cryptographically sign it and you know web

09:45.800 --> 09:52.240
browsers or phones or whatever will build in some ability to say okay this is authentic.

09:52.280 --> 09:58.080
But that part we can all adapt to, like we did this with Photoshop, there was a period

09:58.080 --> 10:02.040
of time where people thought if you see an image it's got to be real, we learned, we're

10:02.040 --> 10:07.600
like okay you know that thing is Photoshopped, it happened quickly, videos like that, society

10:07.600 --> 10:12.040
will build antibodies quickly, but there's a related thing that I think is getting discussed

10:12.040 --> 10:18.800
less which is not the ability to generate mass media like that, but customized one-on-one

10:18.840 --> 10:24.400
interactive persuasion and I think people are going to be able to create AIs that are

10:24.400 --> 10:29.880
very good at this, so I won't just be like you know I'm watching a video of you, but

10:29.880 --> 10:34.360
it'll be like I'm chatting with you back and forth and it's like the most interesting

10:34.360 --> 10:38.320
compelling conversation that I've ever had that's like affecting me in ways I don't know

10:38.320 --> 10:44.520
about and that's a new thing that's different than just generated media, again I think we'll

10:44.560 --> 10:49.120
find a way to build societal antibodies to it, but I don't think it's discussed as much

10:49.120 --> 10:51.040
and it's going to be a challenge.

10:51.040 --> 10:59.240
I also want to talk about jobs because the natural fears AI is going to make us redundant,

10:59.240 --> 11:03.880
particularly in markets like India where we have so much of a workforce and a lot of

11:03.880 --> 11:09.080
it is often times doing somewhat rote work, should we be worried about this, I mean does

11:09.080 --> 11:13.640
this affect societal disruption on employment and capitalism and all the things in how we've

11:13.640 --> 11:14.640
been running?

11:14.640 --> 11:20.760
I mean to some extent yes, every technological revolution leads to job change and this will

11:20.760 --> 11:28.760
be no exception, I guess three thoughts, number one job change itself is fine, you know if

11:28.760 --> 11:33.320
you kind of look at the history of this, in two generations we can kind of adapt to any

11:33.320 --> 11:38.040
amount of labor market change and there's new jobs and the new jobs are usually better

11:38.040 --> 11:43.720
and that's going to happen here too, some jobs will go away, there will be new better

11:43.720 --> 11:47.360
jobs, they're difficult to imagine as we sit here and dream about the future is going

11:47.360 --> 11:52.280
to look like, the thing that might be different about this is the speed with which it could

11:52.280 --> 11:57.800
happen and I think it will require a change to the socioeconomic contract and the way

11:57.800 --> 12:02.560
governments think about this if it happens at a very fast pace.

12:02.560 --> 12:08.400
The second thing is it's not going the way people predicted so far and I don't think

12:08.400 --> 12:14.240
it will in the future, so the current systems are actually not very good at all at doing

12:14.240 --> 12:19.840
whole jobs, they're very good at doing tasks and so the nature of the job, if you're say

12:19.840 --> 12:25.280
a computer programmer to stick with that example, shifts to you kind of like manage a team of

12:25.280 --> 12:31.280
extremely, extremely junior developers that can only do one minute task at a time and

12:31.280 --> 12:35.760
then they'll do 10 minute tasks and then they'll do an hour task but you'll still have to think

12:35.760 --> 12:39.520
of like how is this all going to fit together, what I want to build and you know maybe eventually

12:39.520 --> 12:45.760
it learns that too but this idea that instead of replacing jobs it's making people dramatically

12:45.760 --> 12:51.120
more efficient and there is such a demand overhang in most places, you know if we can

12:51.120 --> 12:55.760
overnight make the world create 3x more software because we make every software developer three

12:55.760 --> 13:00.600
times more efficient, that is not nearly enough, that does not nearly fulfill the demand the

13:00.600 --> 13:04.920
world has for software and I think we'll see that in many other places.

13:04.920 --> 13:12.080
So another example of this is that the consensus, not the consensus, the like absolute belief

13:12.080 --> 13:17.440
of experts around the world 10 years ago, first AI is going to come replace the physical

13:17.440 --> 13:22.840
labor jobs so truck drivers, farmers, factory workers, real trouble then it will come for

13:22.840 --> 13:29.640
the sort of easier kinds of cognitive labor then maybe eventually like computer programmers

13:29.680 --> 13:34.920
even a mathematician and then you know way in the future or maybe never because maybe

13:34.920 --> 13:40.160
it's like magical and human the creative jobs and of course we can look now and say it appears

13:40.160 --> 13:45.720
like it's going exactly the other direction but that was like really non-obvious certainly

13:45.720 --> 13:51.720
to us we started thinking we were going to build robots and it's still in some deep sense

13:51.720 --> 13:55.560
to me seems like it should be much easier to make robots than it is to make GPT-4 but

13:55.560 --> 14:02.560
here we are. I think with other job impacts it's just going to be surprising but I think

14:03.920 --> 14:08.360
the world will get way wealthier you'll have a productivity boom and we will find a lot

14:08.360 --> 14:10.760
of new things to do.

14:10.760 --> 14:17.760
You talked about robots and you know we've talked about sort of the real practical likely

14:18.560 --> 14:23.680
disruption that we're going to see because of AI but we also have to talk about that

14:23.720 --> 14:28.840
one percent like extinction risk or that robots are going to come and take over our

14:28.840 --> 14:33.040
lives how do you think about that I mean you have actually been probably more so than the

14:33.040 --> 14:39.540
average person cautious about this and for us we kind of think of it as sci-fi kind of

14:39.540 --> 14:44.880
like in the in the realm of not really realistic but interesting to talk about but I think

14:44.880 --> 14:48.480
you would say it's something real that we have to think about how we understand it.

14:48.480 --> 14:53.440
For sure like I want to be super clear I don't think current systems are dangerous I don't

14:53.480 --> 14:59.000
think there's any way that GPT-4 like causes an existential risk to the world but people

14:59.000 --> 15:06.000
are very bad at thinking about exponential curves and GPT-10 may be a extremely different

15:07.000 --> 15:13.200
thing. Given the importance of getting this right even if it's a one percent chance I

15:13.200 --> 15:20.200
think putting a lot of effort into thinking studying like how we align an AGI how we design

15:20.240 --> 15:25.800
safe systems at this kind of scale is super important and starting that early is really

15:25.800 --> 15:29.880
good I think we can totally manage through it I think we're developing techniques to

15:29.880 --> 15:34.080
mitigate it this is really why we started the company this was like our initial focus and

15:34.080 --> 15:39.360
still is our most important focus but yeah we need to address this.

15:39.360 --> 15:43.160
Is there like a power switch in the back of your office that nobody knows about where

15:43.160 --> 15:46.600
you can just pull like that thing in Jurassic Park that giant yeah it has to be big and

15:46.640 --> 15:50.520
dramatic but you pull this big thing and it shuts down all the systems if we need it exactly

15:50.520 --> 15:55.680
like that okay good I'm glad I feel better now okay and it works even if you're traveling

15:55.680 --> 16:01.360
right I mean yeah okay anyways so let's talk about regulation because again I think what's

16:01.360 --> 16:06.200
really unusual is this company is a few years old but really for the for the consumer it's

16:06.200 --> 16:11.960
like less than a year old because of chat GPT and yet here you are traveling the world

16:11.960 --> 16:16.200
meeting leaders globally to talk about the importance of regulation and not only are

16:16.200 --> 16:21.320
you doing that you are probably one of the most vocal people saying we need it and not

16:21.320 --> 16:25.040
one of those you know we'll regulate ourselves leave us alone type of things you are saying

16:25.040 --> 16:30.800
governments need to step up understand this and get involved this is very weird this is

16:30.800 --> 16:35.800
not like how most startups operate what's going on well again we started the company

16:35.800 --> 16:40.320
because we were nervous about AGI risk before you were really before people even talked

16:40.360 --> 16:45.360
about AGI and now I think part of the reason we deploy systems is so that people confront

16:48.480 --> 16:53.760
the technology feel it understand the risks the benefits and now a lot of other people

16:53.760 --> 16:59.280
are also very excited but sharing the concern I think this is a special moment where the

16:59.280 --> 17:02.360
globe can come together and kind of get this right and we certainly would like to try to

17:02.360 --> 17:07.360
do that let's talk a little bit more about AI in India because it's so unique for us

17:07.680 --> 17:11.680
and there are so many interesting use cases that are very India specific you know one of

17:11.680 --> 17:17.680
the obvious questions we think a lot about are languages right India has one of the largest

17:17.680 --> 17:23.160
depths of languages hundreds of languages in the country now AI is by and large trained

17:23.160 --> 17:27.880
on what's publicly available what's available on most of the internet which is you know

17:27.880 --> 17:32.560
inevitably going to be mostly English probably a lot more Western focused in terms of just

17:32.560 --> 17:37.120
the sheer quantity of stuff that goes into trade and that's what we're going to be doing

17:37.120 --> 17:42.440
to training how do you think about biases how do you think about inclusivity how do

17:42.440 --> 17:48.200
you think about multilingual countries like India and making a product that's relevant

17:48.200 --> 17:53.560
that's useful not just for all of us fancy people sitting in Bombay and Delhi but for

17:53.560 --> 17:59.120
you know everyone in the mass of the country yeah it's super important to us we had a big

17:59.120 --> 18:05.880
step forward from GPT 3.5 to 4 at non English languages so GPT 4 is pretty good at say the

18:05.880 --> 18:10.220
top 20 languages and okay it may be the top hundred we will be able to push this much

18:10.220 --> 18:17.200
further you know it's challenging for us for very small languages spoken by you know only

18:17.200 --> 18:21.640
a few tens of thousands or hundreds of thousands of people that that's difficult but the systems

18:21.640 --> 18:26.120
are fundamentally going to be very good at this I think and it's important for us to

18:26.120 --> 18:30.880
do now as you were saying it's not just the language it's also the history the culture

18:30.960 --> 18:37.520
the values and we want the entire world represented in here there will be some areas where the

18:37.520 --> 18:42.360
world's got to agree on like here the sort of global bounds of the system but mostly

18:42.360 --> 18:47.240
if you want to use it in the US or in India that can be under a different legal framework

18:47.240 --> 18:51.560
and then in different parts of the culture in each place it'll be very different and

18:51.560 --> 18:55.240
I think that should all be represented in there we recently launched a new program to

18:55.240 --> 18:59.840
give out grants for people that want to run experiments of the ways we can do this the

18:59.840 --> 19:06.840
way we can collect this but we really really want to you know India has been particularly

19:06.840 --> 19:13.280
unique and successful globally at building a lot of the underlying technology stacks

19:13.280 --> 19:20.080
to support new innovation and digital with India stack UPI other things like this do

19:20.080 --> 19:27.080
you think India should build its own LLM AGI AI engine you know in some sense should we

19:27.080 --> 19:31.120
think of this a little bit like nuclear technology where every country should be building its

19:31.120 --> 19:35.160
own capabilities and you know a little bit more nationalist in the way we think about

19:35.160 --> 19:40.000
this I mean how do you think as a country we should think about AI as something in a

19:40.000 --> 19:45.960
sovereign sense first of all it's super impressive to see what India has done I think in a way

19:45.960 --> 19:51.200
that really no other country has with these sort of saying we're going to do national

19:51.240 --> 19:58.240
technology really well and like make it a really like a national asset in terms of AI

19:58.520 --> 20:03.360
strategy I think there's like a lot of things that can work I think this question of sort

20:03.360 --> 20:08.520
of AI sovereignty none of us have an answer to yet feels like it's going to be at least

20:08.520 --> 20:14.440
somewhat important but the main thing that I think is important is figuring out how to

20:14.440 --> 20:21.440
integrate these technologies into other services and that is an area that I think governments

20:21.440 --> 20:28.440
are behind on and don't have the answers to yet but you know I think like hopefully we

20:29.720 --> 20:35.720
all start to use LLMs to make government services way better both from like how do I enroll

20:35.720 --> 20:39.720
in this program to like how do I get better healthcare but if you're in the Indian government

20:39.720 --> 20:45.080
should you be like we need to set up a team of crack engineers to build our own open AI

20:45.080 --> 20:49.440
I mean is there a concern for us to say are we depending on like for fundamental infrastructure

20:49.440 --> 20:54.560
are we depending on something that's not owned by our country yeah I think it is good to have

20:54.560 --> 20:58.960
certainly some sort of AI research effort what exactly that should do you know should

20:58.960 --> 21:04.720
that be training ground up LLMs should that be pursuing new research directions should

21:04.720 --> 21:08.800
that be focused on fine tuning open source projects I think there's a lot of options

21:08.800 --> 21:15.800
there and there's I don't yet like have conviction on the right answer but some you know nationally

21:16.400 --> 21:23.320
funded AI effort feels like a good idea. One of the things that I think is so interesting

21:23.320 --> 21:29.840
is that open AI straddles this line of being a non-profit and a for-profit and I don't know

21:29.840 --> 21:33.400
how I don't know if I fully understand it I don't know if many people do I know you've

21:33.400 --> 21:40.400
raised money from investors and Microsoft is definitely one of your shareholders when

21:40.440 --> 21:45.440
we think about it is it do we think of open AI as something that's here for society to

21:45.440 --> 21:50.480
do societal good is it here to make money for its shareholders is it both what happens

21:50.480 --> 21:54.320
if those conflict how do we think about that. If they conflict we're definitely here for

21:54.320 --> 21:59.040
the societal good like that's super clear and that's why we put up with all this complication

21:59.040 --> 22:01.920
in our structure. So what is the structure can you help us understand what exactly does

22:02.000 --> 22:05.560
it look like. Yeah so there's like a non-profit that has a board that governs this thing that

22:05.560 --> 22:11.760
we call a cap profit where our investors can make a certain return but if we ever need

22:11.760 --> 22:16.640
to make a decision that is in favor of societal good but not in favor of shareholders we're

22:16.640 --> 22:23.640
set up to do that. And one of the most controversial things I heard was that you don't own equity

22:23.640 --> 22:31.120
in open AI why is that what's going on. I mean it started just as like sort of this

22:31.240 --> 22:34.800
work of our structure where we needed non-conflicted people on the board who didn't have equity

22:34.800 --> 22:41.240
a certain number of them certain percentage. And then I kind of just like never got like

22:41.240 --> 22:46.240
I forget about it until it comes up and something like this but it's I don't think it's like

22:46.240 --> 22:51.520
a particularly noteworthy thing like I made a ton of money early in my career I actively

22:51.520 --> 22:56.760
invest so I expect to make a ton more. I get far more value from even like personally

22:56.760 --> 23:02.600
selfishly speaking I get far more value from like all of the other sort of benefits that

23:02.600 --> 23:07.840
come from running open AI a very interesting life than I would for more money. But most

23:07.840 --> 23:12.480
of all like I just believe that this is going to be the most important project of our time

23:12.480 --> 23:17.160
and I'm super grateful to work on it. If you need me to like send you reminders to

23:17.160 --> 23:22.000
keep up on it I'm happy to do that just let me know. So look a lot of people have flown

23:22.000 --> 23:25.840
in here from all around the country to come hear you and while understanding all this

23:25.920 --> 23:31.640
theory about AI is cool help us do our jobs better. I want to put you in a couple roles

23:31.640 --> 23:38.440
and tell me OK you are now the CEO of a hospital in India. What should you do and not theoretical

23:38.440 --> 23:43.560
go hire a couple people like tell like help me do my job better be my AI for a second.

23:43.560 --> 23:49.000
One of the things that we have heard from a lot of doctors is that they're using chat

23:49.000 --> 23:56.600
GPT with GPT for to help come up with new ideas for tricky cases. So you know input the symptoms

23:56.600 --> 24:00.840
maybe the test results say I can't figure this out. What are some ideas for the differential

24:00.840 --> 24:06.320
diagnosis and in many cases getting great results back. Now let's say you're running

24:06.320 --> 24:12.720
a bank. What do you do. This is like rapid fire. We're all taking notes to do our jobs

24:12.720 --> 24:17.160
better here like a sort of traditional like bank branch on the street that kind of bank

24:17.240 --> 24:21.680
not like an investment. Yeah like a bank like a traditional bank that issues credit cards

24:21.680 --> 24:33.080
and checking accounts and all that stuff. I think I would try to just like on a very brief

24:33.080 --> 24:38.280
little side journey of my career I once like helped build a mobile banking app and on the

24:38.280 --> 24:44.600
side. No no it was like OK yeah whatever. I still think the consumer experience of banking

24:44.600 --> 24:50.440
is terrible and could be a lot of it could be replaced by like chatting with an LLM. It's

24:50.440 --> 24:55.200
interesting. Let's say you're running a university. You're a you're a chancellor. I mean we've all

24:55.200 --> 24:59.320
seen how chat GPT can definitely affect the education experience. Now let's say you're

24:59.320 --> 25:02.680
running a university. Yeah that one I think is pretty clear. I would just like go redesign

25:02.680 --> 25:06.680
the education experience. I would have the equivalent of like personalized tutoring

25:06.680 --> 25:13.600
interactive textbooks. I would like I would integrate it into like all parts of the learning

25:13.600 --> 25:19.480
process. Now just totally theoretically let's say you're running like a large news media

25:19.480 --> 25:26.200
company in like a market like India like just as an example. What would you do. Let me just

25:26.200 --> 25:32.800
get my pen real quick. But what would you do. Just tell me. One of the things that you know

25:32.800 --> 25:36.600
there's been a lot of controversy about whether this is going to be good or bad for the publishing

25:36.600 --> 25:42.640
industry and news in particular. One of the things that we've heard from journalists and

25:42.640 --> 25:48.200
reporters who are actually using the the product is that it helps them do the boring parts

25:48.200 --> 25:53.040
of their jobs better and they get to spend more time reporting talking to sources thinking

25:53.040 --> 26:00.000
of ideas. And so I think I would just like encourage everyone to just start using it.

26:00.000 --> 26:06.400
And now let's say you are the ministry in India responsible for overseeing technology

26:06.560 --> 26:13.600
A.I. etc. You know what what would you do in that situation. Like what would you be doing

26:13.600 --> 26:19.080
today as a regulator. I would say you know we have the G 20 coming up. India can play

26:19.080 --> 26:25.120
like a huge role here in global conversation about what this sort of international regulatory

26:25.120 --> 26:28.880
thing might look like. And we are going to really focus on that between now and September

26:28.880 --> 26:35.800
and make sure we prioritize that. Can you tell us something that you haven't

26:35.840 --> 26:40.480
told other people about what's coming from open A.I. like maybe just some insider information

26:40.480 --> 26:44.280
that we could use in some form. You know we kind of tell people what we're working on

26:44.280 --> 26:48.520
like it's going to get smarter. It's going to get multimodal. We're going to try to like

26:48.520 --> 26:53.880
teach it to generate new ideas come up and help us like discover more new science. We're

26:53.880 --> 27:00.760
going to reduce hallucinations and give users like more control so no one feels like it's

27:00.760 --> 27:04.960
biased or at least it's biased in the way you want us to be biased. We don't have like

27:05.000 --> 27:11.000
a lot of secret plans here. I think always as a company to our strength and weakness we

27:11.000 --> 27:16.360
just sort of say what we think and what we're going to try to do. It's amazing. Now one

27:16.360 --> 27:21.120
of the most amazing things about you Sam is that you are running what is going to be one

27:21.120 --> 27:26.360
of the most impactful companies in history. Whenever people say impactful they leave out

27:26.360 --> 27:30.240
whether it's going to be a good or a bad impact. That is a very purposeful leave out because

27:30.280 --> 27:35.240
we don't know right. But you're going to shape the world with this. We know that. And this

27:35.240 --> 27:39.600
isn't your only job as I understand or maybe it's my only like operating. It's the only

27:39.600 --> 27:43.920
thing that I'm like in the trenches. Can you tell us what else you're doing that's like

27:43.920 --> 27:49.960
exciting you or motivating you outside of open AI. Your side hustles if we put it that way.

27:49.960 --> 27:55.960
I think we're going to get nuclear fusion to work in the next few years at and importantly

27:56.000 --> 28:02.000
not just as a scientific demonstration but as incredibly cheap energy and at global skill.

28:03.240 --> 28:09.040
So I think other than AI if you could do one thing that would like really help the world

28:09.040 --> 28:13.400
get richer increase the quality of life. It's very cheap energy. I think there's like a huge

28:13.400 --> 28:19.880
historical correlation there. And I think we've all like lost sight of the appropriate ambition

28:19.880 --> 28:25.040
level here of how how much of an impact we could make. But if we can get fusion to work and if

28:25.040 --> 28:30.040
we can make enough of it for the world then if it can like cut the energy cost 10 X plus that's

28:30.040 --> 28:37.480
pretty great. I'll pick that one. So your side gig is nuclear fusion. I don't. I am an investor and

28:37.480 --> 28:43.240
sort of like helper of that one. It's amazing. So just so I understand you're you're revolutionizing

28:43.240 --> 28:50.000
artificial intelligence and energy. Not you specifically. I think those are like those are

28:50.000 --> 28:56.560
the two. My basic model of the world is that the cost of intelligence and the cost of energy are

28:56.560 --> 29:03.000
kind of what compounded everything else. And if we want a radically better future those are the

29:03.000 --> 29:11.760
two things we should focus on trying to like make abundant. It's incredible. So my last question for

29:11.760 --> 29:19.480
you is what is the most exciting thing that you are seeing globally in your own company. Like what

29:19.520 --> 29:24.680
is the thing that outside of everything we are all talking about and seeing that excites you about

29:24.680 --> 29:30.000
where open AI or even not just AI in general. What's the most exciting thing ahead.

29:35.400 --> 29:42.120
I mean I think it's this generation of new scientific progress. If these systems can really

29:42.120 --> 29:49.560
contribute additional understanding of the world better technology better science that that is like

29:49.560 --> 29:53.880
the sustainable way that the world actually gets better and that the quality of life increases.

29:54.880 --> 29:58.920
We're not there yet. It might be soon it might take a while but I believe we are going to get there

29:59.600 --> 30:08.280
and that will I think we all underestimate that. It's amazing. Sam thank you so much. Thank you to be

30:08.280 --> 30:13.600
clear. Sam asked me to talk less and to open it up for you all to ask more.

