1
00:00:00,000 --> 00:00:03,680
I want to ask Sam some questions myself and then after that we'll open it up to a broader

2
00:00:03,680 --> 00:00:07,200
set of people so everyone can have some time.

3
00:00:07,200 --> 00:00:12,160
You know Sam, just before we start, I want everyone to understand your story because

4
00:00:12,160 --> 00:00:16,560
you have such an interesting background in the tech ecosystem, you know, help us walk

5
00:00:16,560 --> 00:00:21,960
us through from graduating, not graduating, joining Stanford, dropping out, running Y

6
00:00:21,960 --> 00:00:27,040
Combinator, running a different startup before that, and now running OpenAI and a number

7
00:00:27,040 --> 00:00:28,040
of things.

8
00:00:28,040 --> 00:00:30,720
Just help us understand how you came to where you are right now.

9
00:00:30,720 --> 00:00:39,240
Yeah, so I started at Stanford where we met and I was already in love with computer science

10
00:00:39,240 --> 00:00:42,080
but I really fell in love with it once I got there.

11
00:00:42,080 --> 00:00:46,000
I actually went to study AI but at the time AI was really not working at all.

12
00:00:46,000 --> 00:00:50,880
In fact, very memorably one of my professors said the only sure way to have a bad career

13
00:00:50,880 --> 00:00:53,000
in AI is to work on neural networks.

14
00:00:53,000 --> 00:00:56,200
We've decided those don't work.

15
00:00:56,200 --> 00:00:59,200
And so I got kind of discouraged and I started a company.

16
00:00:59,200 --> 00:01:00,200
That was a great experience.

17
00:01:00,200 --> 00:01:05,400
The company didn't work out that well but I kind of like learned about startups and

18
00:01:05,400 --> 00:01:08,600
thought they were a very powerful force and something I was very excited about.

19
00:01:08,600 --> 00:01:14,920
So I then ran YC for a while and while I was doing that, I got newly excited about the

20
00:01:14,920 --> 00:01:19,600
idea of startups that take on hard technical challenges.

21
00:01:19,600 --> 00:01:24,400
And I sort of thought it was curious to me more people weren't doing that.

22
00:01:24,400 --> 00:01:26,920
It seemed like a really valuable opportunity.

23
00:01:26,920 --> 00:01:30,000
With some other people, started OpenAI is one of those examples and many other things

24
00:01:30,000 --> 00:01:35,000
which have gone on to be pretty exciting but really fell in love with OpenAI.

25
00:01:35,000 --> 00:01:40,880
Once it seemed clear that we were really going to have a chance at making true general purpose

26
00:01:40,880 --> 00:01:45,960
AI like a system that could do what a human can do and contribute new knowledge to society,

27
00:01:45,960 --> 00:01:49,840
I got really excited and wanted to go work on that.

28
00:01:49,840 --> 00:01:54,360
And so stop being an investor and now I do that.

29
00:01:54,360 --> 00:01:56,920
So first of all, what is OpenAI?

30
00:01:56,920 --> 00:01:58,520
Is it just chat GPT?

31
00:01:58,520 --> 00:01:59,520
Are they the same thing?

32
00:01:59,520 --> 00:02:00,520
Are they different?

33
00:02:00,520 --> 00:02:03,200
Just help us understand what the company does.

34
00:02:03,200 --> 00:02:08,560
We are a company that is doing research and deployment to try to figure out how to build

35
00:02:08,560 --> 00:02:13,480
AGI and how to responsibly deploy that into the world for maximum benefit.

36
00:02:13,480 --> 00:02:16,840
So this is unlike other technologies.

37
00:02:16,840 --> 00:02:20,480
Other technologies are like this too but this is a strong case of a technology that

38
00:02:20,600 --> 00:02:24,800
on the one hand is the most exciting, most promising, coolest thing.

39
00:02:24,800 --> 00:02:27,680
I think that humanity will have yet built.

40
00:02:27,680 --> 00:02:32,680
We can cure all disease, we can give everybody a great education, better healthcare, massively

41
00:02:32,680 --> 00:02:37,320
increased productivity, huge scientific discovery, all of these wonderful things and we want

42
00:02:37,320 --> 00:02:41,880
to make sure that people get that benefit, that benefit is distributed equitably.

43
00:02:41,880 --> 00:02:47,280
And on the other hand, there are the obvious concerns about the power of this technology

44
00:02:47,280 --> 00:02:49,880
used in a negative direction.

45
00:02:50,000 --> 00:02:56,080
So we want to be a force to help manage those risks so that we all get to enjoy the benefits.

46
00:02:56,080 --> 00:02:59,960
Chat GPT is definitely what we are best known for so I guess there is sort of synonymous

47
00:02:59,960 --> 00:03:03,800
at this point but OpenAI is really about this quest for AGI.

48
00:03:03,800 --> 00:03:06,240
Interesting.

49
00:03:06,240 --> 00:03:09,360
So help us understand, I mean all of us have played with it, right?

50
00:03:09,360 --> 00:03:13,720
We have poems getting written by it, we have all asked it fun trivia questions, teller

51
00:03:13,720 --> 00:03:19,440
and answers but help us understand, you without a doubt have a better understanding of how

52
00:03:19,520 --> 00:03:24,080
it's getting used all around the world and all sorts of different industries, vocations,

53
00:03:24,080 --> 00:03:25,840
reasons.

54
00:03:25,840 --> 00:03:29,880
Talk to us a little bit about some of the most interesting things that you've seen.

55
00:03:29,880 --> 00:03:33,960
For example, what's the most surprising use case of some of the technologies that you

56
00:03:33,960 --> 00:03:37,440
guys have built that you've seen recently?

57
00:03:37,440 --> 00:03:41,840
So the main thing I would say that's interesting about it is its generality.

58
00:03:41,840 --> 00:03:45,080
There's a lot of other systems that can go do this thing well or that thing well or this

59
00:03:45,080 --> 00:03:48,120
thing and in many cases better than Chat GPT.

60
00:03:48,560 --> 00:03:53,880
There's probably not an AI that can write a better poem or whatever but other categories

61
00:03:53,880 --> 00:03:57,880
you could find something that's maybe better but the fact that this one system is truly

62
00:03:57,880 --> 00:04:02,600
general purpose and can do so many things means that people are integrating it into

63
00:04:02,600 --> 00:04:08,000
their workflow as a very powerful tool and so the same thing that can help you write

64
00:04:08,000 --> 00:04:13,600
computer code, one of the areas that we've seen the biggest impact is what coders are

65
00:04:13,600 --> 00:04:17,920
using this for, doubling, tripling their productivity.

66
00:04:17,920 --> 00:04:22,400
There was a paper that just came out that when Italy temporarily banned Chat GPT developer

67
00:04:22,400 --> 00:04:30,200
productivity fell in half on a fairly big study but it can do that, it can also help

68
00:04:30,200 --> 00:04:35,120
you find information, it can help you write a poem, it can help you summarize documents,

69
00:04:35,120 --> 00:04:40,440
it can translate things and people are using this which we hoped would happen as this sort

70
00:04:40,440 --> 00:04:44,960
of super assistant that just makes them more and more productive and it's that generality

71
00:04:44,960 --> 00:04:48,280
that I think is the coolest part.

72
00:04:48,280 --> 00:04:53,200
So with so much ability that maybe even you guys haven't even thought about how people

73
00:04:53,200 --> 00:04:58,360
are using it when you developed it and launched it, I'm sure you've seen a lot of interesting

74
00:04:58,360 --> 00:05:01,360
use cases right here out of India itself.

75
00:05:01,360 --> 00:05:04,280
Can you tell us something or just give us an example of something you've seen that's

76
00:05:04,280 --> 00:05:08,480
really inspired you that you've seen come out of the Indian market?

77
00:05:08,480 --> 00:05:14,000
So India has been a country that has really truly embraced Chat GPT in a way, maybe you

78
00:05:14,000 --> 00:05:17,080
can tell me why, I'm sort of curious, I'm hoping to learn while I'm here, we're very

79
00:05:17,080 --> 00:05:24,880
delighted but there has been a lot of early adoption and real enthusiasm from the users.

80
00:05:24,880 --> 00:05:30,920
One of the very earliest things like in the first weeks of launching Chat GPT, we heard

81
00:05:30,920 --> 00:05:37,120
about a farmer in India who wasn't able to access government services and via like Chat

82
00:05:37,120 --> 00:05:41,680
GPT hooked up to WhatsApp in some sort of complicated way, was then able to.

83
00:05:41,680 --> 00:05:44,600
And that was like one of the early things where we're like, huh, we did not think that

84
00:05:44,600 --> 00:05:46,800
was going to happen.

85
00:05:46,800 --> 00:05:52,640
And just to expand on it, so what I've understood about OpenAI is Chat GPT is one implementation

86
00:05:52,640 --> 00:05:59,720
of the things you've built but you have capabilities to real time translate, to transcribe audio

87
00:05:59,720 --> 00:06:05,360
and detects and are you seeing people use these in combination in ways that are surprising?

88
00:06:05,600 --> 00:06:10,240
Well, we recently launched an iPhone app that has speech recognition in it, which is that's

89
00:06:10,240 --> 00:06:12,640
hooking up two of our models together and people love that.

90
00:06:12,640 --> 00:06:19,440
But the main point that I would like to get across is none of the current systems really

91
00:06:19,440 --> 00:06:20,440
matter.

92
00:06:20,440 --> 00:06:24,680
Like, we're going to look back at GPT4 and you know, I don't know if any of you have

93
00:06:24,680 --> 00:06:30,320
like picked up an iPhone, original iPhone in recent years but it's like, wow, I cannot

94
00:06:30,320 --> 00:06:32,160
believe we're excited about this.

95
00:06:32,160 --> 00:06:38,960
Each pixel is like that big, it just feels like this incredibly antiquated thing.

96
00:06:38,960 --> 00:06:43,880
The curve here is going to be much, much steeper and what the systems are going to be capable

97
00:06:43,880 --> 00:06:48,800
of in the not distant future, we think, is going to be very dramatically different.

98
00:06:48,800 --> 00:06:54,760
So this is like a system that, I don't even know what the right, this is like the old

99
00:06:54,760 --> 00:06:59,960
first like grayscale Nokia phone that looked like a little candy bar and the iPhone 14

100
00:06:59,960 --> 00:07:02,560
is coming.

101
00:07:02,560 --> 00:07:08,360
So what I would say is, it's a mistake to get too focused on the current systems, their

102
00:07:08,360 --> 00:07:12,400
limitations, their capabilities, the impact they're having.

103
00:07:12,400 --> 00:07:17,920
The thing that matters here is we are on an exponential curve, truly, two big miracles

104
00:07:17,920 --> 00:07:19,480
I think in the field.

105
00:07:19,480 --> 00:07:25,720
Number one, we have an algorithm that can genuinely, truly like no tricks learn and number two,

106
00:07:25,720 --> 00:07:29,440
it gets predictably better with scale.

107
00:07:29,440 --> 00:07:35,400
And that, we're going to look back, I think, on those two realizations as a turning point

108
00:07:35,400 --> 00:07:39,640
in human history when you put them together, but what it means is that the rate of progress

109
00:07:39,640 --> 00:07:42,400
in the coming years, the capabilities are going to be significant.

110
00:07:42,400 --> 00:07:48,720
So it's totally cool that chatGVT can write that poem when a future system can like cure

111
00:07:48,720 --> 00:07:55,480
all disease or help us address climate change or radically improve education or make us

112
00:07:55,480 --> 00:07:58,520
all like 10 or 100 times more productive than what we do.

113
00:07:58,520 --> 00:07:59,840
That's quite impactful.

114
00:07:59,840 --> 00:08:02,040
It's amazing.

115
00:08:02,040 --> 00:08:07,080
Now let's flip to the other side of this because there's no doubt there's incredible power

116
00:08:07,080 --> 00:08:11,440
in this technology and with that comes challenges.

117
00:08:11,440 --> 00:08:15,280
I want to play a clip, maybe you guys can put on a clip of something I recently heard

118
00:08:15,280 --> 00:08:18,760
Sam speak somewhere and we can talk about it a bit.

119
00:08:18,760 --> 00:08:20,280
Could you play the clip please?

120
00:08:20,280 --> 00:08:23,880
Hi, my name is Sam and I'm happy to be here today.

121
00:08:23,880 --> 00:08:25,880
Thank you all for joining.

122
00:08:25,880 --> 00:08:30,400
I also wanted to say that the gentleman on stage with me is incredibly good looking and

123
00:08:30,400 --> 00:08:34,440
I also want to say that you should be very careful with videos generated with artificial

124
00:08:34,440 --> 00:08:36,400
intelligence technology.

125
00:08:36,400 --> 00:08:42,120
Okay, so you didn't say that recently, clearly that was just a ploy here and thank you by

126
00:08:42,120 --> 00:08:43,120
the way.

127
00:08:43,120 --> 00:08:46,120
Totally agree with that one.

128
00:08:46,120 --> 00:08:51,240
But nonetheless, I think it raises a real question, right, when this video, if you look

129
00:08:51,240 --> 00:08:54,560
closely you can see the lips aren't perfectly synced, but like you said this stuff is only

130
00:08:54,600 --> 00:08:58,280
going to get better and exponentially better.

131
00:08:58,280 --> 00:09:02,120
Fundamental questions are on authenticity, what's real and what's fake, how do we handle

132
00:09:02,120 --> 00:09:03,120
that?

133
00:09:03,120 --> 00:09:07,680
Yeah, so that was like deeply in the uncanny valley, it's very strange to watch, but we're

134
00:09:07,680 --> 00:09:14,600
not that far away from something that looks perfect and there's a lot of fear right now

135
00:09:14,600 --> 00:09:19,520
about the impact this is going to have on elections and on our society and how we ever

136
00:09:19,520 --> 00:09:22,280
trust media that we see.

137
00:09:22,320 --> 00:09:25,920
I have some fear there, but I think we're actually going to, when it comes to like a

138
00:09:25,920 --> 00:09:29,880
video like that, I think as a society we're going to rise to the occasion.

139
00:09:29,880 --> 00:09:35,360
We're going to learn very quickly that we don't trust videos unless we trust the sort

140
00:09:35,360 --> 00:09:36,800
of provenance.

141
00:09:36,800 --> 00:09:41,200
We'll have techniques like watermarking detectors, more than that I suspect at some point if people

142
00:09:41,200 --> 00:09:45,800
are saying something really important they'll cryptographically sign it and you know web

143
00:09:45,800 --> 00:09:52,240
browsers or phones or whatever will build in some ability to say okay this is authentic.

144
00:09:52,280 --> 00:09:58,080
But that part we can all adapt to, like we did this with Photoshop, there was a period

145
00:09:58,080 --> 00:10:02,040
of time where people thought if you see an image it's got to be real, we learned, we're

146
00:10:02,040 --> 00:10:07,600
like okay you know that thing is Photoshopped, it happened quickly, videos like that, society

147
00:10:07,600 --> 00:10:12,040
will build antibodies quickly, but there's a related thing that I think is getting discussed

148
00:10:12,040 --> 00:10:18,800
less which is not the ability to generate mass media like that, but customized one-on-one

149
00:10:18,840 --> 00:10:24,400
interactive persuasion and I think people are going to be able to create AIs that are

150
00:10:24,400 --> 00:10:29,880
very good at this, so I won't just be like you know I'm watching a video of you, but

151
00:10:29,880 --> 00:10:34,360
it'll be like I'm chatting with you back and forth and it's like the most interesting

152
00:10:34,360 --> 00:10:38,320
compelling conversation that I've ever had that's like affecting me in ways I don't know

153
00:10:38,320 --> 00:10:44,520
about and that's a new thing that's different than just generated media, again I think we'll

154
00:10:44,560 --> 00:10:49,120
find a way to build societal antibodies to it, but I don't think it's discussed as much

155
00:10:49,120 --> 00:10:51,040
and it's going to be a challenge.

156
00:10:51,040 --> 00:10:59,240
I also want to talk about jobs because the natural fears AI is going to make us redundant,

157
00:10:59,240 --> 00:11:03,880
particularly in markets like India where we have so much of a workforce and a lot of

158
00:11:03,880 --> 00:11:09,080
it is often times doing somewhat rote work, should we be worried about this, I mean does

159
00:11:09,080 --> 00:11:13,640
this affect societal disruption on employment and capitalism and all the things in how we've

160
00:11:13,640 --> 00:11:14,640
been running?

161
00:11:14,640 --> 00:11:20,760
I mean to some extent yes, every technological revolution leads to job change and this will

162
00:11:20,760 --> 00:11:28,760
be no exception, I guess three thoughts, number one job change itself is fine, you know if

163
00:11:28,760 --> 00:11:33,320
you kind of look at the history of this, in two generations we can kind of adapt to any

164
00:11:33,320 --> 00:11:38,040
amount of labor market change and there's new jobs and the new jobs are usually better

165
00:11:38,040 --> 00:11:43,720
and that's going to happen here too, some jobs will go away, there will be new better

166
00:11:43,720 --> 00:11:47,360
jobs, they're difficult to imagine as we sit here and dream about the future is going

167
00:11:47,360 --> 00:11:52,280
to look like, the thing that might be different about this is the speed with which it could

168
00:11:52,280 --> 00:11:57,800
happen and I think it will require a change to the socioeconomic contract and the way

169
00:11:57,800 --> 00:12:02,560
governments think about this if it happens at a very fast pace.

170
00:12:02,560 --> 00:12:08,400
The second thing is it's not going the way people predicted so far and I don't think

171
00:12:08,400 --> 00:12:14,240
it will in the future, so the current systems are actually not very good at all at doing

172
00:12:14,240 --> 00:12:19,840
whole jobs, they're very good at doing tasks and so the nature of the job, if you're say

173
00:12:19,840 --> 00:12:25,280
a computer programmer to stick with that example, shifts to you kind of like manage a team of

174
00:12:25,280 --> 00:12:31,280
extremely, extremely junior developers that can only do one minute task at a time and

175
00:12:31,280 --> 00:12:35,760
then they'll do 10 minute tasks and then they'll do an hour task but you'll still have to think

176
00:12:35,760 --> 00:12:39,520
of like how is this all going to fit together, what I want to build and you know maybe eventually

177
00:12:39,520 --> 00:12:45,760
it learns that too but this idea that instead of replacing jobs it's making people dramatically

178
00:12:45,760 --> 00:12:51,120
more efficient and there is such a demand overhang in most places, you know if we can

179
00:12:51,120 --> 00:12:55,760
overnight make the world create 3x more software because we make every software developer three

180
00:12:55,760 --> 00:13:00,600
times more efficient, that is not nearly enough, that does not nearly fulfill the demand the

181
00:13:00,600 --> 00:13:04,920
world has for software and I think we'll see that in many other places.

182
00:13:04,920 --> 00:13:12,080
So another example of this is that the consensus, not the consensus, the like absolute belief

183
00:13:12,080 --> 00:13:17,440
of experts around the world 10 years ago, first AI is going to come replace the physical

184
00:13:17,440 --> 00:13:22,840
labor jobs so truck drivers, farmers, factory workers, real trouble then it will come for

185
00:13:22,840 --> 00:13:29,640
the sort of easier kinds of cognitive labor then maybe eventually like computer programmers

186
00:13:29,680 --> 00:13:34,920
even a mathematician and then you know way in the future or maybe never because maybe

187
00:13:34,920 --> 00:13:40,160
it's like magical and human the creative jobs and of course we can look now and say it appears

188
00:13:40,160 --> 00:13:45,720
like it's going exactly the other direction but that was like really non-obvious certainly

189
00:13:45,720 --> 00:13:51,720
to us we started thinking we were going to build robots and it's still in some deep sense

190
00:13:51,720 --> 00:13:55,560
to me seems like it should be much easier to make robots than it is to make GPT-4 but

191
00:13:55,560 --> 00:14:02,560
here we are. I think with other job impacts it's just going to be surprising but I think

192
00:14:03,920 --> 00:14:08,360
the world will get way wealthier you'll have a productivity boom and we will find a lot

193
00:14:08,360 --> 00:14:10,760
of new things to do.

194
00:14:10,760 --> 00:14:17,760
You talked about robots and you know we've talked about sort of the real practical likely

195
00:14:18,560 --> 00:14:23,680
disruption that we're going to see because of AI but we also have to talk about that

196
00:14:23,720 --> 00:14:28,840
one percent like extinction risk or that robots are going to come and take over our

197
00:14:28,840 --> 00:14:33,040
lives how do you think about that I mean you have actually been probably more so than the

198
00:14:33,040 --> 00:14:39,540
average person cautious about this and for us we kind of think of it as sci-fi kind of

199
00:14:39,540 --> 00:14:44,880
like in the in the realm of not really realistic but interesting to talk about but I think

200
00:14:44,880 --> 00:14:48,480
you would say it's something real that we have to think about how we understand it.

201
00:14:48,480 --> 00:14:53,440
For sure like I want to be super clear I don't think current systems are dangerous I don't

202
00:14:53,480 --> 00:14:59,000
think there's any way that GPT-4 like causes an existential risk to the world but people

203
00:14:59,000 --> 00:15:06,000
are very bad at thinking about exponential curves and GPT-10 may be a extremely different

204
00:15:07,000 --> 00:15:13,200
thing. Given the importance of getting this right even if it's a one percent chance I

205
00:15:13,200 --> 00:15:20,200
think putting a lot of effort into thinking studying like how we align an AGI how we design

206
00:15:20,240 --> 00:15:25,800
safe systems at this kind of scale is super important and starting that early is really

207
00:15:25,800 --> 00:15:29,880
good I think we can totally manage through it I think we're developing techniques to

208
00:15:29,880 --> 00:15:34,080
mitigate it this is really why we started the company this was like our initial focus and

209
00:15:34,080 --> 00:15:39,360
still is our most important focus but yeah we need to address this.

210
00:15:39,360 --> 00:15:43,160
Is there like a power switch in the back of your office that nobody knows about where

211
00:15:43,160 --> 00:15:46,600
you can just pull like that thing in Jurassic Park that giant yeah it has to be big and

212
00:15:46,640 --> 00:15:50,520
dramatic but you pull this big thing and it shuts down all the systems if we need it exactly

213
00:15:50,520 --> 00:15:55,680
like that okay good I'm glad I feel better now okay and it works even if you're traveling

214
00:15:55,680 --> 00:16:01,360
right I mean yeah okay anyways so let's talk about regulation because again I think what's

215
00:16:01,360 --> 00:16:06,200
really unusual is this company is a few years old but really for the for the consumer it's

216
00:16:06,200 --> 00:16:11,960
like less than a year old because of chat GPT and yet here you are traveling the world

217
00:16:11,960 --> 00:16:16,200
meeting leaders globally to talk about the importance of regulation and not only are

218
00:16:16,200 --> 00:16:21,320
you doing that you are probably one of the most vocal people saying we need it and not

219
00:16:21,320 --> 00:16:25,040
one of those you know we'll regulate ourselves leave us alone type of things you are saying

220
00:16:25,040 --> 00:16:30,800
governments need to step up understand this and get involved this is very weird this is

221
00:16:30,800 --> 00:16:35,800
not like how most startups operate what's going on well again we started the company

222
00:16:35,800 --> 00:16:40,320
because we were nervous about AGI risk before you were really before people even talked

223
00:16:40,360 --> 00:16:45,360
about AGI and now I think part of the reason we deploy systems is so that people confront

224
00:16:48,480 --> 00:16:53,760
the technology feel it understand the risks the benefits and now a lot of other people

225
00:16:53,760 --> 00:16:59,280
are also very excited but sharing the concern I think this is a special moment where the

226
00:16:59,280 --> 00:17:02,360
globe can come together and kind of get this right and we certainly would like to try to

227
00:17:02,360 --> 00:17:07,360
do that let's talk a little bit more about AI in India because it's so unique for us

228
00:17:07,680 --> 00:17:11,680
and there are so many interesting use cases that are very India specific you know one of

229
00:17:11,680 --> 00:17:17,680
the obvious questions we think a lot about are languages right India has one of the largest

230
00:17:17,680 --> 00:17:23,160
depths of languages hundreds of languages in the country now AI is by and large trained

231
00:17:23,160 --> 00:17:27,880
on what's publicly available what's available on most of the internet which is you know

232
00:17:27,880 --> 00:17:32,560
inevitably going to be mostly English probably a lot more Western focused in terms of just

233
00:17:32,560 --> 00:17:37,120
the sheer quantity of stuff that goes into trade and that's what we're going to be doing

234
00:17:37,120 --> 00:17:42,440
to training how do you think about biases how do you think about inclusivity how do

235
00:17:42,440 --> 00:17:48,200
you think about multilingual countries like India and making a product that's relevant

236
00:17:48,200 --> 00:17:53,560
that's useful not just for all of us fancy people sitting in Bombay and Delhi but for

237
00:17:53,560 --> 00:17:59,120
you know everyone in the mass of the country yeah it's super important to us we had a big

238
00:17:59,120 --> 00:18:05,880
step forward from GPT 3.5 to 4 at non English languages so GPT 4 is pretty good at say the

239
00:18:05,880 --> 00:18:10,220
top 20 languages and okay it may be the top hundred we will be able to push this much

240
00:18:10,220 --> 00:18:17,200
further you know it's challenging for us for very small languages spoken by you know only

241
00:18:17,200 --> 00:18:21,640
a few tens of thousands or hundreds of thousands of people that that's difficult but the systems

242
00:18:21,640 --> 00:18:26,120
are fundamentally going to be very good at this I think and it's important for us to

243
00:18:26,120 --> 00:18:30,880
do now as you were saying it's not just the language it's also the history the culture

244
00:18:30,960 --> 00:18:37,520
the values and we want the entire world represented in here there will be some areas where the

245
00:18:37,520 --> 00:18:42,360
world's got to agree on like here the sort of global bounds of the system but mostly

246
00:18:42,360 --> 00:18:47,240
if you want to use it in the US or in India that can be under a different legal framework

247
00:18:47,240 --> 00:18:51,560
and then in different parts of the culture in each place it'll be very different and

248
00:18:51,560 --> 00:18:55,240
I think that should all be represented in there we recently launched a new program to

249
00:18:55,240 --> 00:18:59,840
give out grants for people that want to run experiments of the ways we can do this the

250
00:18:59,840 --> 00:19:06,840
way we can collect this but we really really want to you know India has been particularly

251
00:19:06,840 --> 00:19:13,280
unique and successful globally at building a lot of the underlying technology stacks

252
00:19:13,280 --> 00:19:20,080
to support new innovation and digital with India stack UPI other things like this do

253
00:19:20,080 --> 00:19:27,080
you think India should build its own LLM AGI AI engine you know in some sense should we

254
00:19:27,080 --> 00:19:31,120
think of this a little bit like nuclear technology where every country should be building its

255
00:19:31,120 --> 00:19:35,160
own capabilities and you know a little bit more nationalist in the way we think about

256
00:19:35,160 --> 00:19:40,000
this I mean how do you think as a country we should think about AI as something in a

257
00:19:40,000 --> 00:19:45,960
sovereign sense first of all it's super impressive to see what India has done I think in a way

258
00:19:45,960 --> 00:19:51,200
that really no other country has with these sort of saying we're going to do national

259
00:19:51,240 --> 00:19:58,240
technology really well and like make it a really like a national asset in terms of AI

260
00:19:58,520 --> 00:20:03,360
strategy I think there's like a lot of things that can work I think this question of sort

261
00:20:03,360 --> 00:20:08,520
of AI sovereignty none of us have an answer to yet feels like it's going to be at least

262
00:20:08,520 --> 00:20:14,440
somewhat important but the main thing that I think is important is figuring out how to

263
00:20:14,440 --> 00:20:21,440
integrate these technologies into other services and that is an area that I think governments

264
00:20:21,440 --> 00:20:28,440
are behind on and don't have the answers to yet but you know I think like hopefully we

265
00:20:29,720 --> 00:20:35,720
all start to use LLMs to make government services way better both from like how do I enroll

266
00:20:35,720 --> 00:20:39,720
in this program to like how do I get better healthcare but if you're in the Indian government

267
00:20:39,720 --> 00:20:45,080
should you be like we need to set up a team of crack engineers to build our own open AI

268
00:20:45,080 --> 00:20:49,440
I mean is there a concern for us to say are we depending on like for fundamental infrastructure

269
00:20:49,440 --> 00:20:54,560
are we depending on something that's not owned by our country yeah I think it is good to have

270
00:20:54,560 --> 00:20:58,960
certainly some sort of AI research effort what exactly that should do you know should

271
00:20:58,960 --> 00:21:04,720
that be training ground up LLMs should that be pursuing new research directions should

272
00:21:04,720 --> 00:21:08,800
that be focused on fine tuning open source projects I think there's a lot of options

273
00:21:08,800 --> 00:21:15,800
there and there's I don't yet like have conviction on the right answer but some you know nationally

274
00:21:16,400 --> 00:21:23,320
funded AI effort feels like a good idea. One of the things that I think is so interesting

275
00:21:23,320 --> 00:21:29,840
is that open AI straddles this line of being a non-profit and a for-profit and I don't know

276
00:21:29,840 --> 00:21:33,400
how I don't know if I fully understand it I don't know if many people do I know you've

277
00:21:33,400 --> 00:21:40,400
raised money from investors and Microsoft is definitely one of your shareholders when

278
00:21:40,440 --> 00:21:45,440
we think about it is it do we think of open AI as something that's here for society to

279
00:21:45,440 --> 00:21:50,480
do societal good is it here to make money for its shareholders is it both what happens

280
00:21:50,480 --> 00:21:54,320
if those conflict how do we think about that. If they conflict we're definitely here for

281
00:21:54,320 --> 00:21:59,040
the societal good like that's super clear and that's why we put up with all this complication

282
00:21:59,040 --> 00:22:01,920
in our structure. So what is the structure can you help us understand what exactly does

283
00:22:02,000 --> 00:22:05,560
it look like. Yeah so there's like a non-profit that has a board that governs this thing that

284
00:22:05,560 --> 00:22:11,760
we call a cap profit where our investors can make a certain return but if we ever need

285
00:22:11,760 --> 00:22:16,640
to make a decision that is in favor of societal good but not in favor of shareholders we're

286
00:22:16,640 --> 00:22:23,640
set up to do that. And one of the most controversial things I heard was that you don't own equity

287
00:22:23,640 --> 00:22:31,120
in open AI why is that what's going on. I mean it started just as like sort of this

288
00:22:31,240 --> 00:22:34,800
work of our structure where we needed non-conflicted people on the board who didn't have equity

289
00:22:34,800 --> 00:22:41,240
a certain number of them certain percentage. And then I kind of just like never got like

290
00:22:41,240 --> 00:22:46,240
I forget about it until it comes up and something like this but it's I don't think it's like

291
00:22:46,240 --> 00:22:51,520
a particularly noteworthy thing like I made a ton of money early in my career I actively

292
00:22:51,520 --> 00:22:56,760
invest so I expect to make a ton more. I get far more value from even like personally

293
00:22:56,760 --> 00:23:02,600
selfishly speaking I get far more value from like all of the other sort of benefits that

294
00:23:02,600 --> 00:23:07,840
come from running open AI a very interesting life than I would for more money. But most

295
00:23:07,840 --> 00:23:12,480
of all like I just believe that this is going to be the most important project of our time

296
00:23:12,480 --> 00:23:17,160
and I'm super grateful to work on it. If you need me to like send you reminders to

297
00:23:17,160 --> 00:23:22,000
keep up on it I'm happy to do that just let me know. So look a lot of people have flown

298
00:23:22,000 --> 00:23:25,840
in here from all around the country to come hear you and while understanding all this

299
00:23:25,920 --> 00:23:31,640
theory about AI is cool help us do our jobs better. I want to put you in a couple roles

300
00:23:31,640 --> 00:23:38,440
and tell me OK you are now the CEO of a hospital in India. What should you do and not theoretical

301
00:23:38,440 --> 00:23:43,560
go hire a couple people like tell like help me do my job better be my AI for a second.

302
00:23:43,560 --> 00:23:49,000
One of the things that we have heard from a lot of doctors is that they're using chat

303
00:23:49,000 --> 00:23:56,600
GPT with GPT for to help come up with new ideas for tricky cases. So you know input the symptoms

304
00:23:56,600 --> 00:24:00,840
maybe the test results say I can't figure this out. What are some ideas for the differential

305
00:24:00,840 --> 00:24:06,320
diagnosis and in many cases getting great results back. Now let's say you're running

306
00:24:06,320 --> 00:24:12,720
a bank. What do you do. This is like rapid fire. We're all taking notes to do our jobs

307
00:24:12,720 --> 00:24:17,160
better here like a sort of traditional like bank branch on the street that kind of bank

308
00:24:17,240 --> 00:24:21,680
not like an investment. Yeah like a bank like a traditional bank that issues credit cards

309
00:24:21,680 --> 00:24:33,080
and checking accounts and all that stuff. I think I would try to just like on a very brief

310
00:24:33,080 --> 00:24:38,280
little side journey of my career I once like helped build a mobile banking app and on the

311
00:24:38,280 --> 00:24:44,600
side. No no it was like OK yeah whatever. I still think the consumer experience of banking

312
00:24:44,600 --> 00:24:50,440
is terrible and could be a lot of it could be replaced by like chatting with an LLM. It's

313
00:24:50,440 --> 00:24:55,200
interesting. Let's say you're running a university. You're a you're a chancellor. I mean we've all

314
00:24:55,200 --> 00:24:59,320
seen how chat GPT can definitely affect the education experience. Now let's say you're

315
00:24:59,320 --> 00:25:02,680
running a university. Yeah that one I think is pretty clear. I would just like go redesign

316
00:25:02,680 --> 00:25:06,680
the education experience. I would have the equivalent of like personalized tutoring

317
00:25:06,680 --> 00:25:13,600
interactive textbooks. I would like I would integrate it into like all parts of the learning

318
00:25:13,600 --> 00:25:19,480
process. Now just totally theoretically let's say you're running like a large news media

319
00:25:19,480 --> 00:25:26,200
company in like a market like India like just as an example. What would you do. Let me just

320
00:25:26,200 --> 00:25:32,800
get my pen real quick. But what would you do. Just tell me. One of the things that you know

321
00:25:32,800 --> 00:25:36,600
there's been a lot of controversy about whether this is going to be good or bad for the publishing

322
00:25:36,600 --> 00:25:42,640
industry and news in particular. One of the things that we've heard from journalists and

323
00:25:42,640 --> 00:25:48,200
reporters who are actually using the the product is that it helps them do the boring parts

324
00:25:48,200 --> 00:25:53,040
of their jobs better and they get to spend more time reporting talking to sources thinking

325
00:25:53,040 --> 00:26:00,000
of ideas. And so I think I would just like encourage everyone to just start using it.

326
00:26:00,000 --> 00:26:06,400
And now let's say you are the ministry in India responsible for overseeing technology

327
00:26:06,560 --> 00:26:13,600
A.I. etc. You know what what would you do in that situation. Like what would you be doing

328
00:26:13,600 --> 00:26:19,080
today as a regulator. I would say you know we have the G 20 coming up. India can play

329
00:26:19,080 --> 00:26:25,120
like a huge role here in global conversation about what this sort of international regulatory

330
00:26:25,120 --> 00:26:28,880
thing might look like. And we are going to really focus on that between now and September

331
00:26:28,880 --> 00:26:35,800
and make sure we prioritize that. Can you tell us something that you haven't

332
00:26:35,840 --> 00:26:40,480
told other people about what's coming from open A.I. like maybe just some insider information

333
00:26:40,480 --> 00:26:44,280
that we could use in some form. You know we kind of tell people what we're working on

334
00:26:44,280 --> 00:26:48,520
like it's going to get smarter. It's going to get multimodal. We're going to try to like

335
00:26:48,520 --> 00:26:53,880
teach it to generate new ideas come up and help us like discover more new science. We're

336
00:26:53,880 --> 00:27:00,760
going to reduce hallucinations and give users like more control so no one feels like it's

337
00:27:00,760 --> 00:27:04,960
biased or at least it's biased in the way you want us to be biased. We don't have like

338
00:27:05,000 --> 00:27:11,000
a lot of secret plans here. I think always as a company to our strength and weakness we

339
00:27:11,000 --> 00:27:16,360
just sort of say what we think and what we're going to try to do. It's amazing. Now one

340
00:27:16,360 --> 00:27:21,120
of the most amazing things about you Sam is that you are running what is going to be one

341
00:27:21,120 --> 00:27:26,360
of the most impactful companies in history. Whenever people say impactful they leave out

342
00:27:26,360 --> 00:27:30,240
whether it's going to be a good or a bad impact. That is a very purposeful leave out because

343
00:27:30,280 --> 00:27:35,240
we don't know right. But you're going to shape the world with this. We know that. And this

344
00:27:35,240 --> 00:27:39,600
isn't your only job as I understand or maybe it's my only like operating. It's the only

345
00:27:39,600 --> 00:27:43,920
thing that I'm like in the trenches. Can you tell us what else you're doing that's like

346
00:27:43,920 --> 00:27:49,960
exciting you or motivating you outside of open AI. Your side hustles if we put it that way.

347
00:27:49,960 --> 00:27:55,960
I think we're going to get nuclear fusion to work in the next few years at and importantly

348
00:27:56,000 --> 00:28:02,000
not just as a scientific demonstration but as incredibly cheap energy and at global skill.

349
00:28:03,240 --> 00:28:09,040
So I think other than AI if you could do one thing that would like really help the world

350
00:28:09,040 --> 00:28:13,400
get richer increase the quality of life. It's very cheap energy. I think there's like a huge

351
00:28:13,400 --> 00:28:19,880
historical correlation there. And I think we've all like lost sight of the appropriate ambition

352
00:28:19,880 --> 00:28:25,040
level here of how how much of an impact we could make. But if we can get fusion to work and if

353
00:28:25,040 --> 00:28:30,040
we can make enough of it for the world then if it can like cut the energy cost 10 X plus that's

354
00:28:30,040 --> 00:28:37,480
pretty great. I'll pick that one. So your side gig is nuclear fusion. I don't. I am an investor and

355
00:28:37,480 --> 00:28:43,240
sort of like helper of that one. It's amazing. So just so I understand you're you're revolutionizing

356
00:28:43,240 --> 00:28:50,000
artificial intelligence and energy. Not you specifically. I think those are like those are

357
00:28:50,000 --> 00:28:56,560
the two. My basic model of the world is that the cost of intelligence and the cost of energy are

358
00:28:56,560 --> 00:29:03,000
kind of what compounded everything else. And if we want a radically better future those are the

359
00:29:03,000 --> 00:29:11,760
two things we should focus on trying to like make abundant. It's incredible. So my last question for

360
00:29:11,760 --> 00:29:19,480
you is what is the most exciting thing that you are seeing globally in your own company. Like what

361
00:29:19,520 --> 00:29:24,680
is the thing that outside of everything we are all talking about and seeing that excites you about

362
00:29:24,680 --> 00:29:30,000
where open AI or even not just AI in general. What's the most exciting thing ahead.

363
00:29:35,400 --> 00:29:42,120
I mean I think it's this generation of new scientific progress. If these systems can really

364
00:29:42,120 --> 00:29:49,560
contribute additional understanding of the world better technology better science that that is like

365
00:29:49,560 --> 00:29:53,880
the sustainable way that the world actually gets better and that the quality of life increases.

366
00:29:54,880 --> 00:29:58,920
We're not there yet. It might be soon it might take a while but I believe we are going to get there

367
00:29:59,600 --> 00:30:08,280
and that will I think we all underestimate that. It's amazing. Sam thank you so much. Thank you to be

368
00:30:08,280 --> 00:30:13,600
clear. Sam asked me to talk less and to open it up for you all to ask more.

