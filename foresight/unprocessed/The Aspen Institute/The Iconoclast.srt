1
00:00:00,000 --> 00:00:08,360
Good afternoon everybody, I'm Andrew Ross Sorkin and it is a privilege to have with

2
00:00:08,360 --> 00:00:14,640
me Peter Thiel this afternoon, one of the great legendary investors in Silicon Valley.

3
00:00:14,640 --> 00:00:19,720
He has been involved in just about everything that you touch and feel, including being the

4
00:00:19,720 --> 00:00:25,480
co-founder of PayPal, the co-founder of Palantir, he made his first outside investment, made

5
00:00:25,480 --> 00:00:30,240
the first outside investment, I'd say in Facebook, his firm Founders Fund is a big

6
00:00:30,240 --> 00:00:35,280
backer of Stripe and SpaceX, his firm backed numerous other startups through the Founders

7
00:00:35,280 --> 00:00:36,840
Fund into your capital.

8
00:00:36,840 --> 00:00:41,360
He also started the Thiel Fellowship to your program, that's an alternative to a college

9
00:00:41,360 --> 00:00:46,840
degree which I want to get to at one point and more importantly than all of it, he has

10
00:00:46,840 --> 00:00:52,800
touched some of the people and found the people who you read about in the headlines every

11
00:00:52,800 --> 00:00:58,320
day from Mark Zuckerberg to Elon Musk to Sam Altman and so many others and it is great

12
00:00:58,320 --> 00:00:59,320
to have you here.

13
00:00:59,320 --> 00:01:00,320
Thanks for having me.

14
00:01:00,320 --> 00:01:04,960
We're also going to talk a little politics as well, along with maybe some of the issues

15
00:01:04,960 --> 00:01:09,280
and culture conversations that are happening in Silicon Valley, but here's where I want

16
00:01:09,280 --> 00:01:12,640
to start the conversation because I want to start the conversation talking about people

17
00:01:12,640 --> 00:01:16,760
because I think there's something actually extraordinary when you think about your track

18
00:01:16,760 --> 00:01:22,760
record over the years of involving yourself in investing not just in companies but ultimately

19
00:01:23,040 --> 00:01:24,200
in people.

20
00:01:24,200 --> 00:01:28,520
You wrote a book which is coming on a 10-year anniversary and by the way I re-read it and

21
00:01:28,520 --> 00:01:34,440
it stands up in a very big way, it is called Zero to One and you wrote the following about

22
00:01:34,440 --> 00:01:37,280
founders, the idea of founders.

23
00:01:37,280 --> 00:01:40,480
You wrote that the lesson for business is that we need founders.

24
00:01:40,480 --> 00:01:48,200
If anything, we should be more tolerant of founders who seem strange or extreme.

25
00:01:48,200 --> 00:01:56,320
We need unusual individuals to lead companies beyond mere incrementalism and I mention that

26
00:01:56,320 --> 00:02:00,320
because I also just mentioned a number of individuals which we read about all the time

27
00:02:00,320 --> 00:02:05,760
and some of those people would be described as unusual perhaps or even strange.

28
00:02:05,760 --> 00:02:10,640
And I'm curious about how you think over the years you have found these individuals, what

29
00:02:10,640 --> 00:02:15,840
it is that has made these individuals as successful as they have become.

30
00:02:16,840 --> 00:02:22,960
Yeah, it's obviously, if there was some simple magic formula, this is what a founder looks

31
00:02:22,960 --> 00:02:29,760
like and you invest in this category of people who's a founder, it probably gets faked.

32
00:02:29,760 --> 00:02:35,160
It's like, I don't know, it's a 20-year-old with a t-shirt and jeans or something like

33
00:02:35,160 --> 00:02:42,320
this or you end up with all kinds of really fake ideas but yeah, I think a lot of the

34
00:02:42,320 --> 00:02:50,120
great companies that have been built over the last two decades were, they were founded

35
00:02:50,120 --> 00:02:56,520
by people where it was somehow deeply connected to their identity, their life project.

36
00:02:56,520 --> 00:03:03,560
They had some kind of idiosyncratic, somewhat different vision of what they were doing.

37
00:03:03,560 --> 00:03:10,680
They did something new and then they built something extraordinarily big over the years

38
00:03:10,680 --> 00:03:17,200
and of course they have these sort of extreme personalities, often have a lot of blind spots

39
00:03:17,200 --> 00:03:21,520
and there are all these ways in which it's a feature and there are ways in which it can

40
00:03:21,520 --> 00:03:29,120
be a little bit buggy but it's sort of a package deal and I net out to it being massively

41
00:03:29,120 --> 00:03:34,400
advantageous versus let's say a professional CEO being brought in.

42
00:03:34,400 --> 00:03:39,680
The prehistory of this I would say would be in the 1990s, the Silicon Valley formula was

43
00:03:39,680 --> 00:03:45,080
you had various people found the company and then you'd replace them as quickly as possible

44
00:03:45,080 --> 00:03:49,680
with professional CEOs, professional management and there are variations of this that happened

45
00:03:49,680 --> 00:03:56,840
with Netscape and Yahoo and even Google, all these companies and the Gen X people founded

46
00:03:56,840 --> 00:04:00,360
them, the baby boomers came along and sort of took over the companies and stole them

47
00:04:00,360 --> 00:04:03,320
from the Gen X founders in the 90s.

48
00:04:03,320 --> 00:04:08,960
In the 2000s when the millennials founded the companies they were given more of an opportunity

49
00:04:09,280 --> 00:04:10,960
and it made a big difference.

50
00:04:10,960 --> 00:04:18,760
The Facebook story I always tell is it was 2006, two years in, Zuckerberg was like 22

51
00:04:18,760 --> 00:04:27,240
years old and we got a $1 billion offer to sell the company to Yahoo and we had a board

52
00:04:27,240 --> 00:04:31,320
meeting, there were three of us and we thought we should at least talk about it.

53
00:04:31,320 --> 00:04:35,600
It was a lot of money, Zuckerberg would make $250 million and sort of an eight hour long

54
00:04:35,720 --> 00:04:40,160
discussion and he didn't know what he'd do with the money and he'd just start another

55
00:04:40,160 --> 00:04:43,480
social networking company and kind of like the one he had and he didn't know what else

56
00:04:43,480 --> 00:04:50,960
he would do and so he really didn't want to sell and if you had a professional CEO

57
00:04:50,960 --> 00:04:55,280
it would have just been, man I can't believe they're offering us a billion dollars and

58
00:04:55,280 --> 00:05:04,440
I'm going to try not to be too eager and we better take the money and run and that one

59
00:05:04,440 --> 00:05:05,440
thing right makes a big difference.

60
00:05:05,440 --> 00:05:09,080
Let me ask you a different question and all of these individuals had a huge impact on

61
00:05:09,080 --> 00:05:14,760
society and have an enormous individual power and I think one of the things that you've

62
00:05:14,760 --> 00:05:18,120
argued in this book and that you've argued over the years is that we need to give them

63
00:05:18,120 --> 00:05:25,840
that power, we need to offer them a latitude that in many ways we don't offer others.

64
00:05:25,840 --> 00:05:36,040
Well I think one of the frames I always have is that there are many ways in which the United

65
00:05:36,040 --> 00:05:43,040
States, the developed countries have been relatively stagnant for the last 50 years.

66
00:05:43,040 --> 00:05:44,800
Progress has slowed.

67
00:05:44,800 --> 00:05:50,160
We had progress in computers, internet, software and many other domains, things have kind of

68
00:05:50,160 --> 00:05:54,640
stalled out and it sort of manifests in low economic growth in the sense that the younger

69
00:05:54,640 --> 00:05:58,120
generation is going to have a tough time doing as well as their parents and there is

70
00:05:58,120 --> 00:06:06,280
sort of this way that there has been this broad stagnation for 40, 50 years and we need

71
00:06:06,280 --> 00:06:08,520
to find ways to do new things.

72
00:06:08,520 --> 00:06:14,120
I don't think, you know, startup, tech startup companies are the only ways to do them, that

73
00:06:14,120 --> 00:06:22,960
is a vehicle for doing it and if you don't allow these companies to have a certain latitude

74
00:06:23,560 --> 00:06:28,320
and flexibility to try to do new things, we shut it down right away, you know, the stagnation

75
00:06:28,320 --> 00:06:29,680
will be worse than ever.

76
00:06:29,680 --> 00:06:32,840
Okay, but here's a separate almost philosophical question, I'm going to read back something

77
00:06:32,840 --> 00:06:37,400
you said to the New Yorker, there was a piece about Sam Altman, this is right around actually

78
00:06:37,400 --> 00:06:44,280
when open AI began, 2016 and I think it actually might even be representative of how you might

79
00:06:44,280 --> 00:06:48,720
think about Mark Zuckerberg or Elon Musk or some of these other kinds of major players.

80
00:06:48,720 --> 00:06:56,280
This is what you said, you said, Sam's program for the world is anchored by ideas, not people

81
00:06:56,280 --> 00:07:01,440
and that's what makes it powerful because it doesn't immediately get derailed by questions

82
00:07:01,440 --> 00:07:05,720
of popularity and I thought that that was actually very indicative of most of the people

83
00:07:05,720 --> 00:07:10,240
that you have invested in, it's really been about ideas and in some ways you could even

84
00:07:10,240 --> 00:07:13,320
argue is disconnected from people.

85
00:07:13,920 --> 00:07:19,960
I think it is really about a whole wide, people are able to think about a wide spectrum

86
00:07:19,960 --> 00:07:24,640
of things, they're able to think about good founders have theories of how to hire people,

87
00:07:24,640 --> 00:07:28,240
how to manage them, how to build teams, they have theories about where the culture, the

88
00:07:28,240 --> 00:07:34,760
societies were going, they have technical things about the product, the design, they

89
00:07:34,760 --> 00:07:39,400
have ideas about how they should market their company, so they're sort of polymaths are

90
00:07:39,480 --> 00:07:45,720
able to think about a lot of these things, but yeah, I'm biased towards a lot of the

91
00:07:45,720 --> 00:07:52,200
ones where it's more intellectual and maybe, but I think that quote has held up pretty

92
00:07:52,200 --> 00:07:57,080
well with Sam Altman, maybe he needed to pay a little bit more attention to the board

93
00:07:57,080 --> 00:08:01,040
and things like that and there was probably a people dimension that he had ignored a little

94
00:08:01,040 --> 00:08:04,080
bit too much in November 2016.

95
00:08:04,080 --> 00:08:08,800
Since we're on the Sam Altman of it all and since Sam was here yesterday, I'm so curious,

96
00:08:08,840 --> 00:08:14,360
you were a mentor of his, what do you think of open AI and what do you think of AI more

97
00:08:14,360 --> 00:08:15,360
broadly right now?

98
00:08:15,360 --> 00:08:16,920
I mean, are we in a bubble?

99
00:08:16,920 --> 00:08:18,560
Is this the future?

100
00:08:18,560 --> 00:08:19,560
What is this?

101
00:08:19,560 --> 00:08:23,240
Wow, it's a broad question.

102
00:08:23,240 --> 00:08:27,920
I think I'm always hesitant to talk about it because I feel there's so many things I would

103
00:08:27,920 --> 00:08:32,080
have said about AI where I would have been very wrong two, three years ago, so maybe

104
00:08:32,080 --> 00:08:37,080
I'll start by just saying a little bit about the history of what people thought was going

105
00:08:37,120 --> 00:08:42,560
to happen and then the surprising thing that open AI achieved and that did happen and if

106
00:08:42,560 --> 00:08:48,560
you had this debate in the 2010s, there was sort of one, maybe frame in terms of two paradigms,

107
00:08:48,560 --> 00:08:55,560
two books, there was the Boston Book Superintelligence 2014, which is that AI was going to build

108
00:08:56,080 --> 00:09:01,800
this god-like superhuman intelligence and it was heading towards this god-like oracle

109
00:09:01,880 --> 00:09:07,880
and that was what AI was going to be and then there was the Kaifu Li, a rebuttal 2018 AI

110
00:09:07,880 --> 00:09:13,440
superpowers, which was sort of the CCP rebuttal to Silicon Valley, that no, AI is not about

111
00:09:13,440 --> 00:09:18,040
god-like intelligence, that's a science fiction fantasy Silicon Valley has, AI is going to

112
00:09:18,040 --> 00:09:25,040
be about machine learning, data collection, it's not conscious, it's not any of these

113
00:09:25,200 --> 00:09:30,560
weird things, it's surveillance tech and China is going to beat the U.S. in the race for

114
00:09:30,600 --> 00:09:36,080
AI because we have no qualms about sort of this totalitarian, not the word he used, collection

115
00:09:36,080 --> 00:09:42,080
of data in our society and that was sort of the way the AI debate got framed and then

116
00:09:42,080 --> 00:09:45,280
the thing I always said was, man, it's just such a weird word, it means all these different

117
00:09:45,280 --> 00:09:52,280
things, it's annoyingly undefined, but then the sort of surprising and strangely unexpected

118
00:09:52,520 --> 00:09:59,520
thing that happened is that in some sense, what open AI with chat GPT 3.5 for AI was

119
00:10:00,760 --> 00:10:07,760
achieved in late 22, early 23 was you passed the Turing test, which was not super-intelligence,

120
00:10:07,760 --> 00:10:14,760
it's not god-like, it's not low-tech surveillance, but that had been the holy grail of AI for

121
00:10:15,000 --> 00:10:21,680
60 or 70 years and it's a fuzzy line, the Turing test is you have a computer that can convince

122
00:10:21,680 --> 00:10:26,320
you that it's a human being and it's a somewhat fuzzy line because sometimes, but it pretty

123
00:10:26,320 --> 00:10:32,160
clearly hadn't been passed before, it pretty clearly is passed now and that's a really

124
00:10:32,160 --> 00:10:38,000
extraordinary achievement, it's extremely, it raises all sorts of interesting big picture

125
00:10:38,000 --> 00:10:45,000
questions, what does it mean to be a human being in 2024? The sort of placeholder answer

126
00:10:46,640 --> 00:10:50,000
I would have been tempted to give a couple of years ago would be something like the

127
00:10:50,000 --> 00:10:54,560
Noam Chomsky idea that something very important about language, this is what sets humans apart

128
00:10:54,600 --> 00:10:59,080
from all the other animals, we talk to each other and we have these rich semantic syntax

129
00:10:59,080 --> 00:11:06,080
things and so if a computer can replicate that, what does that mean for all of us in

130
00:11:06,080 --> 00:11:11,480
this room? It's an extraordinary development and it was also somehow, even though it had

131
00:11:11,480 --> 00:11:18,080
been the holy grail, somehow in the last decade before, it was not expected at all and so

132
00:11:18,080 --> 00:11:23,320
there's something very significant about it and very underrated and then of course you

133
00:11:23,360 --> 00:11:28,800
get all these questions about, is it going to, is it an econ one question, is it a compliment,

134
00:11:28,800 --> 00:11:32,280
is it going to make people more productive or is it a substitute, good, where it's going

135
00:11:32,280 --> 00:11:36,920
to replace? So what do you think of all of this and how bullish as an investor are you

136
00:11:36,920 --> 00:11:42,000
on this? What do you think our society, when you hear Sam Altman talk about this, you say

137
00:11:42,000 --> 00:11:46,160
he's right, that's what it's going to be, do you think it's going to be something else?

138
00:11:46,160 --> 00:11:50,480
You lived through 1999, there's some people who say this is a hype cycle, other people

139
00:11:50,560 --> 00:11:57,560
say this is the future. Well, I'm very anchored on the 99 history and I somehow always like

140
00:11:57,560 --> 00:12:05,560
to say that 99 was both. The peak of the bubble was also in a sense the peak of clarity, people

141
00:12:05,560 --> 00:12:09,000
who realized the new economy was going to replace the old economy, the internet was

142
00:12:09,000 --> 00:12:16,360
going to be the most important thing in the 21st century and people were right about that

143
00:12:16,400 --> 00:12:22,760
and then the specific investments were incredibly hard to make and even the no-brainer market

144
00:12:22,760 --> 00:12:27,440
leader, so if you said 1999, the no-brainer investment would have been Amazon stock,

145
00:12:27,440 --> 00:12:31,520
it's a leading e-commerce company and they're going to scale and they'll get bigger and

146
00:12:31,520 --> 00:12:41,520
it peaked in December 1999 at $113 a share, it was $5.5 in October 2001, 22 months later,

147
00:12:41,520 --> 00:12:45,680
you then had to wait until the end of 2009 to get back to the 99 highs and then if you'd

148
00:12:45,720 --> 00:12:48,560
waited until today, you would have made 25 times your money from 99, you'd have first

149
00:12:48,560 --> 00:12:55,600
lost, you'd have gone down 95% and then made 500X, so even the no-brainer investment from

150
00:12:55,600 --> 00:13:05,600
99 was wickedly tricky to pull off in retrospect and I sort of think that AI, the LLM form

151
00:13:05,600 --> 00:13:12,520
of AI. These are the large language models. The open AI's of the world. Again, passing

152
00:13:12,560 --> 00:13:19,080
the Turing test, I think it's roughly on the scale of the internet and so it's an incredibly

153
00:13:19,080 --> 00:13:23,880
important thing, it's going to be very important socially, politically, philosophically about

154
00:13:23,880 --> 00:13:30,480
all these questions about meaning and then the financial investment question I find unbelievably

155
00:13:30,480 --> 00:13:38,480
hard and confusing and yeah, it's probably quite tricky. If you had to sort of concretize

156
00:13:38,560 --> 00:13:45,560
one thing that's very strange about the, if you sort of just follow the money, at this

157
00:13:45,560 --> 00:13:51,720
point, 80 to 85% of the money in AI is being made by one company, it's NVIDIA and so it's

158
00:13:51,720 --> 00:13:56,440
all on this sort of very weird hardware layer which Silicon Valley doesn't even know very

159
00:13:56,440 --> 00:14:02,240
much about anymore. We don't really do hardware, we don't do silicon chips in Silicon Valley

160
00:14:02,240 --> 00:14:06,560
anymore. I get pitched on these companies once every three or four years and it's always

161
00:14:06,600 --> 00:14:11,120
I have no clue how to do this. It sounds like a pretty good idea but man, I have no clue

162
00:14:11,120 --> 00:14:17,840
and we never invest. Then there's sort of this theory that the hardware piece makes

163
00:14:17,840 --> 00:14:22,720
the money initially, then gets more commodified over time and it'll shift to software and

164
00:14:22,720 --> 00:14:27,400
the, I don't know, the multi-trillion dollar question, is that going to be true again this

165
00:14:27,400 --> 00:14:34,400
time or will NVIDIA sort of have this incredible monopoly at the moment?

166
00:14:36,800 --> 00:14:43,800
I suspect NVIDIA will, I think it will maintain its position for a while. I think the game

167
00:14:50,320 --> 00:14:55,480
theory on it is something like all the big tech companies are going to start to try to

168
00:14:55,480 --> 00:15:02,080
design their own AI chips so they don't have to do the 10x markup to NVIDIA and then how

169
00:15:02,160 --> 00:15:07,840
hard is it for them to do it, how long will it take? If they all do it, then the chips

170
00:15:07,840 --> 00:15:14,840
become a commodity and nobody makes money in chips. Then do you go into hardware and

171
00:15:15,560 --> 00:15:19,080
you should do it if nobody else is doing it, if everybody does it, you shouldn't do it,

172
00:15:19,080 --> 00:15:23,560
and then maybe, I'm not sure how that nets out, but probably people stay stuck for a

173
00:15:23,560 --> 00:15:27,200
while and NVIDIA goes from strength to strength for a while.

174
00:15:27,200 --> 00:15:30,720
I have a related but maybe personal question for you. You happen to have this very interesting

175
00:15:30,720 --> 00:15:34,840
relationship with Sam Altman and then also a very interesting relationship with Elon

176
00:15:34,840 --> 00:15:41,840
Musk. You both worked at PayPal. You famously were part of a coup effectively to push Elon

177
00:15:42,240 --> 00:15:46,360
Musk out of the company. You're now friends with him all over again and have a stake in

178
00:15:46,360 --> 00:15:49,600
SpaceX. You can maybe walk us through that friendship.

179
00:15:49,600 --> 00:15:56,600
We had some rough moments in 2000, 2001, but it's been beautiful.

180
00:15:57,320 --> 00:16:01,680
We're always going to go with this, actually, is one of the things that's been fascinating

181
00:16:01,680 --> 00:16:05,440
and fascinating to the valley and I think to the country has been the commentary we've

182
00:16:05,440 --> 00:16:09,720
heard from Elon Musk who helped build open AI with Sam and the break actually between

183
00:16:09,720 --> 00:16:16,640
the two of them as creating this not-for-profit and what's happened to it. In fact, Elon

184
00:16:16,640 --> 00:16:21,840
Musk originally sued Sam earlier this year and then dropped the suit recently.

185
00:16:21,840 --> 00:16:26,120
But how do you think about this idea of a company that was started as a not-for-profit

186
00:16:26,120 --> 00:16:30,440
and all of the safety concerns and things that you hear from Elon on one side and Sam

187
00:16:30,440 --> 00:16:32,120
on the other?

188
00:16:32,120 --> 00:16:39,120
Man, whichever person I talk to last, I find the most convincing probably. I talked to

189
00:16:40,360 --> 00:16:46,520
Elon about it and he made this argument. It's just completely illegal for a non-profit

190
00:16:46,520 --> 00:16:51,120
to become a for-profit company because otherwise everyone would set up companies as non-profits

191
00:16:51,120 --> 00:16:55,240
and take advantage of the tax laws and then you turn them into a for-profit and this is

192
00:16:56,240 --> 00:17:00,160
the most obvious arb and they just can't be allowed to do this. It's obviously just

193
00:17:00,160 --> 00:17:05,360
totally illegal with what Sam's trying to do at open AI. And then like half an hour

194
00:17:05,360 --> 00:17:11,680
after the conversation was over, at the moment it's like, oh, that's a really strong argument.

195
00:17:11,680 --> 00:17:17,040
And then half an hour later, it's like, but the whole history of open AI is that the biggest

196
00:17:17,040 --> 00:17:25,040
handicap they had was a non-profit and it led to all these crazy conflicting things culminating

197
00:17:25,120 --> 00:17:31,120
in this non-profit board that thought it was better to shut down the company or the whole

198
00:17:31,120 --> 00:17:36,240
venture, whatever you want to call it, rather than keep going. And nobody is ever going

199
00:17:36,240 --> 00:17:42,080
to take the lesson from open AI to start a non-profit and turn it into a for-profit later,

200
00:17:42,080 --> 00:17:49,080
given what a total disaster that was. But whoever I listen to last, I find the most compelling.

201
00:17:49,080 --> 00:17:53,200
Let me ask you a different question. You've left Silicon Valley. You have now moved to

202
00:17:53,200 --> 00:18:00,200
Los Angeles. That's your home. We left San Francisco specifically. It just felt it was

203
00:18:01,000 --> 00:18:04,600
time to get out. So tell us why it was time to get out, because I think a lot of the issues

204
00:18:04,600 --> 00:18:09,440
that actually we read about around open AI and some of the culture issues add a lot of

205
00:18:09,440 --> 00:18:16,200
these companies are the reason you decided you didn't want to live there anymore. It's

206
00:18:16,200 --> 00:18:23,200
hard to... It's a bunch of things that came together, but there was a sense that it was

207
00:18:27,600 --> 00:18:34,600
sort of the ground zero of the most unhinged place in the country. You had this catastrophic

208
00:18:34,960 --> 00:18:39,480
homeless problem, which maybe is not the most important problem, but it was never getting

209
00:18:39,480 --> 00:18:46,480
better. It was by 2018 when we moved to LA, it felt like it had become extraordinarily

210
00:18:47,240 --> 00:18:53,520
self-hating, where everybody who was not in tech hated the tech industry. This is very

211
00:18:53,520 --> 00:19:00,320
odd. It would be like the people in Houston hating oil or people in Detroit hating cars,

212
00:19:00,320 --> 00:19:07,320
people in New York hating finance. It had this unhinged, self-hating character in the

213
00:19:09,520 --> 00:19:16,360
city itself. There were all these things that seemed extraordinarily unhealthy. If you asked

214
00:19:16,360 --> 00:19:23,360
me in 2021, I would have said, man, they are finally... Yes, they're sitting on the biggest...

215
00:19:24,600 --> 00:19:29,360
They created all this wealth, and yet they are going to succeed in committing suicide.

216
00:19:29,360 --> 00:19:35,560
Three years later, I think the jury is a little bit more out, because maybe the AI revolution

217
00:19:35,640 --> 00:19:42,640
is big enough that it will save even the most, I don't know, the most ridiculously mismanaged

218
00:19:43,120 --> 00:19:44,720
city in the country.

219
00:19:44,720 --> 00:19:45,720
It seemed to me...

220
00:19:45,720 --> 00:19:51,920
Tell me if I'm wrong. I thought that part of the issue that you had with San Francisco

221
00:19:51,920 --> 00:19:57,640
was the politics of it. Not just the politics of it, but how politics had seeped into the

222
00:19:57,640 --> 00:20:04,640
culture of so many of the companies. I think that you thought that it had moved into the

223
00:20:05,560 --> 00:20:07,680
city in a very progressive way.

224
00:20:07,680 --> 00:20:14,680
Yeah, that's always a very clear dimension of it. That's the tip of the iceberg. That's

225
00:20:16,320 --> 00:20:22,120
the part that's above the surface that people always focus on. Then the part that's below

226
00:20:22,120 --> 00:20:29,120
the surface is just the deep corruption, the mismanagement of the schools, the buses, all

227
00:20:30,120 --> 00:20:37,120
the public services, the way things don't work, the way the zoning is the most absurd

228
00:20:37,120 --> 00:20:44,120
in the country. There was a house I was looking to buy where you couldn't build access into

229
00:20:44,360 --> 00:20:51,360
the garage, and Gavin Newsom, who is the Lieutenant Governor of California at the time, said he

230
00:20:51,360 --> 00:20:56,520
would help me get a garage access permit. Again, it's not clear that's what the Lieutenant

231
00:20:56,520 --> 00:21:00,200
Governor of the fifth largest economy in the world should be doing, but he said he knew

232
00:21:00,200 --> 00:21:07,200
how to do this in San Francisco and it was circa 2013. You needed to get it, you needed

233
00:21:08,080 --> 00:21:12,320
to get the neighbors to sign off, which was maybe doable. Then you needed to go to the

234
00:21:12,320 --> 00:21:17,560
Board of Supervisors because you had to build a staircase, and it was a public walkway,

235
00:21:17,560 --> 00:21:21,200
and the whole public had to comment. Nobody knew what happened then. Then even harder,

236
00:21:21,200 --> 00:21:26,480
a tree had grown where the driveway was supposed to be, and you needed a tree removal permit.

237
00:21:26,560 --> 00:21:32,280
This was the sort of thing that you would never get. You can describe all this as crazy

238
00:21:32,280 --> 00:21:39,280
left-wing ideology, but I think it's more like really, really deep corruption. This

239
00:21:39,280 --> 00:21:45,280
is in a way the San Francisco problem, it's the California problem. The analogy I have,

240
00:21:45,280 --> 00:21:50,560
if you want to think about the economy of California, in some ways it's analogous to

241
00:21:50,600 --> 00:21:57,600
Saudi Arabia. You have a very mismanaged state government, there's a lot of insane ideology

242
00:21:59,920 --> 00:22:05,640
that goes with it, but you have these incredible gushers called the big tech companies, and

243
00:22:05,640 --> 00:22:12,640
then there's a way the super insane governance is linked to the gold rush of the place. There's

244
00:22:15,960 --> 00:22:20,240
some point where it'll be too crazy even for California, but California can get away

245
00:22:20,320 --> 00:22:23,480
with a lot of stuff, you wouldn't get away with elsewhere. San Francisco, my judgment,

246
00:22:23,480 --> 00:22:29,480
had gone a little bit too far. Maybe the AI thing is they found one more giant gusher.

247
00:22:29,480 --> 00:22:32,320
You don't have any Saudi money in your fund, I hope.

248
00:22:32,320 --> 00:22:37,320
Virtually none. Just in case. Here's a different question though, because it gets to the politics

249
00:22:37,320 --> 00:22:43,820
of this, which is it seems like a shift inside Silicon Valley, and a shift in terms of even

250
00:22:43,820 --> 00:22:50,820
the way the companies are managed in a political dimension. You were very outspoken, obviously,

251
00:22:52,580 --> 00:22:58,580
you supported President Trump in the last go round. I want to get to that part too,

252
00:22:58,580 --> 00:23:03,220
but I want you to speak first to the shift in the valley, at least what seems like a

253
00:23:03,220 --> 00:23:09,060
shift perception wise from being a very progressive place to maybe less so. Maybe not, maybe it's

254
00:23:09,060 --> 00:23:13,780
just the large summers that I spoke this afternoon, and he said there's 10 people he thinks

255
00:23:13,780 --> 00:23:18,340
are very loud on Twitter, and that's why the world thinks that between David Sachs

256
00:23:18,340 --> 00:23:24,540
and a bunch of other people, and Elon Musk, that's not representative, but I think you

257
00:23:24,540 --> 00:23:26,420
may have a different view.

258
00:23:26,420 --> 00:23:33,420
Well, I don't think you'll get a majority of tech people to support Trump over Biden

259
00:23:35,740 --> 00:23:39,500
or anything like that. I think you'll get way more than you had four or eight years

260
00:23:39,540 --> 00:23:45,540
ago, so I don't know if you're measuring a relative shift or an absolute number. Those

261
00:23:45,540 --> 00:23:50,940
are probably two different measures on that, but I would say that if we ask a very different

262
00:23:50,940 --> 00:23:57,060
question about, let's say, extreme wokeness, or I don't even know what you're supposed

263
00:23:57,060 --> 00:24:04,060
to call it, there is probably a broad consensus among the good tech families in Silicon Valley,

264
00:24:09,660 --> 00:24:16,660
founders, startup CEOs, people across a pretty broad range that it's gone way too far. I

265
00:24:16,660 --> 00:24:22,340
talked to a lot of these people, a lot of them are, I'd say, more centrist Democrats,

266
00:24:22,340 --> 00:24:27,980
but it is just, we need to have a secret plan to fight this, and what they tell me behind

267
00:24:27,980 --> 00:24:34,300
closed doors is way, way tougher than what they dare say in public, and so it is like,

268
00:24:34,300 --> 00:24:38,460
we need to have a plan to hire fewer people from San Francisco, because that's where the

269
00:24:38,540 --> 00:24:42,540
employees are the crazy. So if you want to have a less woke workforce, we need to, we're

270
00:24:42,540 --> 00:24:47,540
going to have targets about how we steadily move our company out of San Francisco specifically,

271
00:24:47,540 --> 00:24:52,540
and yeah, these are the sort of conversations that I've...

272
00:24:52,540 --> 00:24:57,540
And do you agree with this? And by the way, let me just read, you probably know Alex,

273
00:24:57,540 --> 00:25:03,740
Alex Wang, Scale AI CEO, who said that he's put together what he calls a merit-based

274
00:25:03,740 --> 00:25:08,500
hiring program. He said he's getting rid of DEI, says hiring on merit will be a permanent

275
00:25:08,500 --> 00:25:11,820
policy at scale. It's a big deal whenever we invite someone to join our mission, and

276
00:25:11,820 --> 00:25:14,980
those decisions have never been swayed by orthodoxy or virtue signaling or whatever

277
00:25:14,980 --> 00:25:20,820
the current thing is. I think of our guiding principle as MEI, Merit Excellence and Intelligence.

278
00:25:20,820 --> 00:25:24,820
Bill Ackman went on to say that he thinks DEI is actually inherently a racist and illegal

279
00:25:24,820 --> 00:25:25,820
movement.

280
00:25:26,820 --> 00:25:33,820
Yeah, I, again, my feel for, there aren't that many people who are willing to say what

281
00:25:33,820 --> 00:25:38,820
Alex says, but I think there are an awful lot of people who are pretty close to thinking

282
00:25:38,820 --> 00:25:47,820
this, that there were ways they leaned into the DEI thing. It was like an anti-Trump thing.

283
00:25:47,820 --> 00:25:53,820
It was like a, everything was sort of polarized around Trump for the last four years of his

284
00:25:53,820 --> 00:25:58,820
campaign, so you have to demonstrate that you're anti-Trump by being even more pro-DEI.

285
00:25:58,820 --> 00:26:04,820
That's, of course, not necessarily a logical thing, but yes, people somehow ended up in

286
00:26:04,820 --> 00:26:13,820
this place that was very different. There always are questions what drove the DEI movement,

287
00:26:13,820 --> 00:26:22,820
the wokeness in these companies, and it probably is over-determined. There's a bottom-up,

288
00:26:22,820 --> 00:26:26,820
woke millennial people who were brainwashed into DEI in their colleges. That's sort of

289
00:26:26,820 --> 00:26:33,820
the bottom-up theory. There's sort of a, I don't know, there's sort of a cynical corporate

290
00:26:33,820 --> 00:26:38,820
version where this is, you know, the leadership of the company either believed it or used

291
00:26:38,820 --> 00:26:44,820
it as sort of a, as a way to manage and control their companies in certain ways. You know,

292
00:26:44,820 --> 00:26:49,820
the part that I always feel is a little bit underestimated is there was probably also

293
00:26:49,820 --> 00:26:57,820
a top-down level from a government regulatory point of view where, you know, if you don't

294
00:26:57,820 --> 00:27:05,820
do DEI, there is some point where you do get in trouble. You know, I don't know.

295
00:27:05,820 --> 00:27:09,820
This is part of the ESG movement now. I mean, look, we talked about ESG here for a long time.

296
00:27:09,820 --> 00:27:13,820
There was an ESG movement, and then there were probably all these governmental versions.

297
00:27:13,820 --> 00:27:17,820
And so, I don't know, this would be probably, if my candidate for the company in Silicon

298
00:27:17,820 --> 00:27:24,820
Valley is still probably the most woke, would be something like Google. And it's less woke

299
00:27:24,820 --> 00:27:28,820
than it was two, three years ago, but in some ways, you know, they have a total monopoly

300
00:27:28,820 --> 00:27:33,820
in search, and so there's sort of some way in which, you know, if wokeness is a luxury

301
00:27:33,820 --> 00:27:39,820
good, like, you can afford it more if you're a monopoly than if you're not. And then the

302
00:27:39,820 --> 00:27:45,820
problem for Google as a pretty big monopoly is that it's always going to be, you know,

303
00:27:45,820 --> 00:27:49,820
subject to a lot more regulatory pressure from the government. And so if you have something

304
00:27:49,820 --> 00:27:59,820
like the Gemini, the Gemini AI engine, you know, and it's sort of this comical, absurdist

305
00:27:59,820 --> 00:28:04,820
thing where it generates these black women Nazis, you know, and you're supposed to find

306
00:28:04,820 --> 00:28:09,820
famous Nazis, and then the diversity criterion gets applied across the board, and so it just

307
00:28:09,820 --> 00:28:13,820
generates fake black women who are Nazis, which is, you know, a little bit too progressive,

308
00:28:13,820 --> 00:28:20,820
I think. But then if you think of it in terms of this larger political context, Google will

309
00:28:20,820 --> 00:28:26,820
never get in trouble for that. The FDC will never sue them for misinformation or anything

310
00:28:26,820 --> 00:28:33,820
like that. That stuff does not get fact-checked. You don't really get in trouble, and the

311
00:28:33,820 --> 00:28:39,820
FDC will never get in trouble. And you probably even get some protection where, okay, you

312
00:28:39,820 --> 00:28:45,820
know, you are, you're going along with the woke directives from the ESG people or the

313
00:28:45,820 --> 00:28:49,820
government. Maybe you overdid it a little bit, but we trust you to be good at other

314
00:28:49,820 --> 00:28:54,820
things. So there may be a very different calculus if you're sort of a large quasi-regulated

315
00:28:54,820 --> 00:28:57,820
monopoly. Let me ask you about large quasi-regulated monopolies and also concentration, but I

316
00:28:57,820 --> 00:29:01,820
want to read you, this is something you actually wrote in your book 10 years ago about Google,

317
00:29:01,820 --> 00:29:05,820
and it being a monopoly. You said, since it doesn't have to worry about competing with

318
00:29:05,820 --> 00:29:11,820
anyone, it has wider latitude to care about its workers, its products, and its impact on

319
00:29:11,820 --> 00:29:15,820
the wider world. Google's motto, don't be evil, is in part a brand employee, but it's also

320
00:29:15,820 --> 00:29:20,820
a characteristic of a kind of business that's successful enough to take ethics seriously

321
00:29:20,820 --> 00:29:24,820
without jeopardizing its own existence. In business, money is either an important thing

322
00:29:24,820 --> 00:29:28,820
or it's everything. Monopolists can't afford to think about things other than making money.

323
00:29:28,820 --> 00:29:34,820
Non-monopolists can't. In a perfect competition, a business is so focused on today's margin that

324
00:29:34,820 --> 00:29:38,820
it can't possibly plan for a long-term future. Only one thing can allow a business to

325
00:29:38,820 --> 00:29:46,820
transcend the daily brute struggle for survival, monopoly profits. Were you writing in favor

326
00:29:46,820 --> 00:29:51,820
then of the monopoly idea or against?

327
00:29:52,820 --> 00:30:00,820
My book was giving you advice for what to do, and from the inside, you always want to do

328
00:30:00,820 --> 00:30:08,820
something like what Google did. If you're starting a company, competition is for losers.

329
00:30:08,820 --> 00:30:14,820
Capitalism and competition, people always say they're synonyms, I think they're antonyms,

330
00:30:14,820 --> 00:30:19,820
because if you have perfect competition you compete away all the capital. If you want to

331
00:30:19,820 --> 00:30:25,820
have Darwinian competition, beard, red and tooth and claw, you should open a restaurant.

332
00:30:25,820 --> 00:30:29,820
It's like an awful, awful business. You will never make any money. It's perfectly competitive

333
00:30:29,820 --> 00:30:37,820
and completely non-capitalist. From the inside, you want to always go for something like

334
00:30:37,820 --> 00:30:48,820
monopoly. In other parts of my book, I also qualify it that there are dynamic monopolies

335
00:30:48,820 --> 00:30:52,820
that invent something new, that create something new for the world, and we reward them with

336
00:30:52,820 --> 00:30:59,820
patents or things like that that they get. Then at some point, there's always a risk that

337
00:30:59,820 --> 00:31:05,820
these monopolies go bad, that they become like a troll collecting a toll at a bridge,

338
00:31:05,820 --> 00:31:09,820
that they're not dynamic, and that they become that fat and lazy.

339
00:31:09,820 --> 00:31:13,820
Are we there yet? Lena Kahn, if she was sitting here, would say we got there a long time ago.

340
00:31:13,820 --> 00:31:29,820
I think, man, all these ways I would, if I had to defend Google, and I would still say

341
00:31:29,820 --> 00:31:46,820
that it's still better run, even in its silly woke way, even in a slightly troll-like toll

342
00:31:46,820 --> 00:31:53,820
collecting way, than whatever a completely destructive path Lena Kahn would have for the

343
00:31:53,820 --> 00:31:57,820
company. We're still getting more good from Google as it is.

344
00:31:57,820 --> 00:32:01,820
Do you feel the way about all the big tech companies? You have lots of investments in smaller

345
00:32:01,820 --> 00:32:05,820
companies that need to access the app store on Apple's phone. Do you say to yourself that

346
00:32:05,820 --> 00:32:11,820
it should be opened up? Do you say they created the store, therefore they should control the

347
00:32:11,820 --> 00:32:14,820
store? How do you think about that kind of stuff?

348
00:32:14,820 --> 00:32:21,820
There are a lot of complicated questions on all these things. They're much bigger. We're

349
00:32:21,820 --> 00:32:26,820
in a very different place from where you were 10 years ago on these things. I still worry

350
00:32:26,820 --> 00:32:31,820
that in many cases the remedy is worse than the disease. A lot of these businesses are,

351
00:32:31,820 --> 00:32:38,820
if you have a natural monopoly, the remedy is not to break it up. It's like a utility company

352
00:32:38,820 --> 00:32:43,820
and then the remedy is to regulate it or tax it or do various things like that. If you

353
00:32:43,820 --> 00:32:51,820
could convince me that we are a static as a utility company, then maybe the remedy is to do

354
00:32:52,820 --> 00:33:01,820
something like that. To the extent that the real monopoly problems in our society are much

355
00:33:01,820 --> 00:33:09,820
more these old economy, racket-like companies. I spent three months during COVID in Maui and

356
00:33:09,820 --> 00:33:17,820
there's a single hospital in Maui. The Hawaiian Island, there's this line, if you have a pain,

357
00:33:17,820 --> 00:33:24,820
get on a plane because it's a local racket, it's completely mismanaged. That's probably

358
00:33:24,820 --> 00:33:31,820
the really dysfunctional monopolies in our society are these pretty big ones that control

359
00:33:31,820 --> 00:33:40,820
these local markets that are 100% troll collecting. I think even with all my misgivings about

360
00:33:41,820 --> 00:33:48,820
something like Google, it's a vastly morally superior place to your local hospital.

361
00:33:48,820 --> 00:33:53,820
How do you feel about it in the context of AI, which is to say that if you believe AI is this

362
00:33:53,820 --> 00:33:57,820
transformative product and that there's only going to be three or four players who are going to

363
00:33:57,820 --> 00:34:02,820
control all of these models, whether it be Google or Microsoft with open AI or maybe an

364
00:34:02,820 --> 00:34:08,820
Amazon along the way. I don't know where you think Apple is going to land in this conversation.

365
00:34:09,820 --> 00:34:13,820
Is that a good thing or a bad thing? Also, I would argue, even as an investor who looks at

366
00:34:13,820 --> 00:34:19,820
startups, how do you even look at startups down the line that could effectively get competed away

367
00:34:19,820 --> 00:34:24,820
because I'm going to build my app with AI and I'm just going to copy what you've made?

368
00:34:24,820 --> 00:34:31,820
Well, I think it's in a very different place from the consumer internet type businesses.

369
00:34:31,820 --> 00:34:37,820
There's been a history, they've been around for decades. If I had to make the anti-Google

370
00:34:37,820 --> 00:34:44,820
experiment, it would be, they won at Search in 2002 and there's been no serious competition

371
00:34:44,820 --> 00:34:55,820
for 21, 22 years. They beat Microsoft and Yahoo in 2002 and then it's somehow very hard to disrupt

372
00:34:55,820 --> 00:35:04,820
that. Then I think the AI piece is extremely fluid, it's extremely hard to know, it's very hard

373
00:35:04,820 --> 00:35:11,820
to know where the value is. As I said, it's like the obvious monopoly right now is Nvidia

374
00:35:11,820 --> 00:35:22,820
but it doesn't seem that durable. If you thought Nvidia is as durable as Google, the stock's really cheap.

375
00:35:22,820 --> 00:35:30,820
You should just buy it like crazy. The market pricing is telling you they have a temporary

376
00:35:30,820 --> 00:35:37,820
monopoly but it's not very robust and then on the level of the software companies, I worry that

377
00:35:37,820 --> 00:35:46,820
OpenAI has a lead, all sorts of other people are going to be able to catch up pretty quickly

378
00:35:46,820 --> 00:35:52,820
and if you have three or four doing the same thing, that's a lot more than one.

379
00:35:52,820 --> 00:35:55,820
Very, very different set of economics.

380
00:35:55,820 --> 00:35:58,820
I want to pivot the conversation again because another investment that you've made

381
00:35:58,820 --> 00:36:04,820
and been very public about is Bitcoin. You have remained a very big bull.

382
00:36:04,820 --> 00:36:10,820
You have come out publicly and you said that enemy number one to Bitcoin is the sociopathic

383
00:36:10,820 --> 00:36:15,820
grandpa from Omaha that you described as Warren Buffett.

384
00:36:15,820 --> 00:36:19,820
Can you tell me what you were thinking when you said that?

385
00:36:20,820 --> 00:36:37,820
It got a lot of laughs so somehow people, it probably had some kind of a nerve but it was in a 2022

386
00:36:37,820 --> 00:36:44,820
Bitcoin convention talk I gave and there were three separate enemies.

387
00:36:44,820 --> 00:36:49,820
There was Jamie Dimon, Larry Fink who is no longer an enemy by the way.

388
00:36:49,820 --> 00:36:54,820
He sort of shifted but maybe I can say that Larry Fink things too.

389
00:36:54,820 --> 00:37:05,820
Then there was Warren Buffett and the rough context was, my sort of political sociological analysis

390
00:37:05,820 --> 00:37:11,820
was the cryptocurrencies were, it was a revolutionary youth movement but for them to really take over,

391
00:37:11,820 --> 00:37:17,820
you needed, it couldn't just be a student uprising like 1968.

392
00:37:17,820 --> 00:37:24,820
You needed to get the rest of the society on board and as long as the old people were going to sit on their hands

393
00:37:24,820 --> 00:37:30,820
that was the big blocker for cryptocurrencies to go to the next level.

394
00:37:30,820 --> 00:37:32,820
Are you still convinced?

395
00:37:32,820 --> 00:37:48,820
I think it's gotten partially unlocked with the Bitcoin ETF but then probably the part where I'm less convinced of

396
00:37:48,820 --> 00:37:55,820
is this question of the sort of ideological founding vision of Bitcoin or these cryptocurrencies

397
00:37:55,820 --> 00:38:05,820
as sort of a cipherpunk crypto-anarchist libertarian anti-centralized government thing.

398
00:38:05,820 --> 00:38:08,820
Isn't that what got you interested in the first place?

399
00:38:08,820 --> 00:38:15,820
That's what I thought was terrific about it and then the question is does it really work that way

400
00:38:15,820 --> 00:38:18,820
or has that thread somehow gotten lost?

401
00:38:18,820 --> 00:38:25,820
And so when people in the FBI tell me that they'd much rather have criminals use Bitcoin than $100 bills

402
00:38:25,820 --> 00:38:30,820
it suggests that maybe it's not quite working the way it was supposed to.

403
00:38:30,820 --> 00:38:32,820
Have you sold any of your Bitcoin?

404
00:38:32,820 --> 00:38:34,820
I still hold some.

405
00:38:35,820 --> 00:38:49,820
There are all these ways. I didn't buy as much as I should have and I'm not sure it's going to go up that dramatically from here.

406
00:38:49,820 --> 00:38:50,820
From here?

407
00:38:50,820 --> 00:38:56,820
Yeah, I think we got the ETF edition and I don't know who else buys it quickly from here.

408
00:38:56,820 --> 00:38:59,820
It's an interesting investment advice.

409
00:39:00,820 --> 00:39:05,820
That actually surprised me because I thought you were still all in.

410
00:39:05,820 --> 00:39:08,820
I still have a small position.

411
00:39:08,820 --> 00:39:18,820
It probably still can go up some but it's going to be a volatile bumpy ride and I had a dual reason.

412
00:39:18,820 --> 00:39:27,820
One was the sort of ideological decentralized future of computing world that I really do believe and really believe would be better

413
00:39:27,820 --> 00:39:34,820
and it seemed like the perfect vehicle for that for such a long time and I am just much less convinced of that.

414
00:39:34,820 --> 00:39:35,820
Interesting.

415
00:39:35,820 --> 00:39:43,820
So maybe Larry Fink with the BlackRock ETF surrendered to the forces, the anti-ESG forces

416
00:39:43,820 --> 00:39:50,820
or maybe it's more like Bitcoin's been co-opted by them and I worry it was more the latter.

417
00:39:50,820 --> 00:39:54,820
Okay, different question. SpaceX, that's another big investment for you.

418
00:39:54,820 --> 00:39:59,820
After ousting Elon Musk you became friends with him again.

419
00:39:59,820 --> 00:40:02,820
What does that look like to you in the future?

420
00:40:02,820 --> 00:40:10,820
Is that going to be the biggest investment you've ever made when this is all said and done?

421
00:40:10,820 --> 00:40:24,820
Man, I'm always sort of hesitant to sort of pitch these companies too much.

422
00:40:24,820 --> 00:40:34,820
But I think there were sort of a lot of different things that came together.

423
00:40:35,820 --> 00:40:43,820
When Elon was building both Tesla and SpaceX in the 2000s, people thought he was just really, really crazy

424
00:40:43,820 --> 00:40:52,820
and I think even a lot of those of us who worked with him at PayPal, there was this PayPal book that David Sacks and I thought of writing

425
00:40:52,820 --> 00:41:01,820
and the Elon chapter was, I think, entitled something like the man who knew nothing about risk or something like this

426
00:41:01,820 --> 00:41:09,820
and there were all these crazy Elon stories I could tell and then if one of the two companies had succeeded

427
00:41:09,820 --> 00:41:15,820
you would say, well maybe he still got really lucky but when two out of two companies that people thought were completely

428
00:41:15,820 --> 00:41:24,820
hair-brained in the 2000s, when they both succeed, you have to somehow reassess it and somehow the rest of us

429
00:41:24,820 --> 00:41:30,820
somehow are too risk-averse or there's something about risk he knows that we don't or something like this

430
00:41:31,820 --> 00:41:34,820
and so yes, I think there's...

431
00:41:34,820 --> 00:41:36,820
You didn't invest in Tesla?

432
00:41:36,820 --> 00:41:44,820
We did not invest in Tesla. We should have invested in that one. It was public at a much earlier date

433
00:41:44,820 --> 00:41:50,820
and then there's always sort of a self-imposed limitation that we tend not to invest in public companies

434
00:41:50,820 --> 00:41:55,820
there's 20% of the venture fund you could but that was sort of the...

435
00:41:55,820 --> 00:42:01,820
I think they started Tesla in 2002, it went public in 2010

436
00:42:01,820 --> 00:42:10,820
I remember test driving the Model S in October 2012 and it was just, wow this is just a terrific car

437
00:42:10,820 --> 00:42:16,820
and I think the correct thing would have been to wait till they came out with it and then nobody liked it

438
00:42:16,820 --> 00:42:22,820
it was such a hated stock shorted by everybody and you could have just waited 10 years

439
00:42:22,820 --> 00:42:27,820
and just bought the shares in the public market and you would have made 10 times your money in 18 months

440
00:42:27,820 --> 00:42:32,820
and 100 times in the next 6, 7, 8 years

441
00:42:32,820 --> 00:42:42,820
and then there was something also about SpaceX that looked like it was a very crazy, hair-brained idea

442
00:42:42,820 --> 00:42:48,820
and yet it was very straightforward, it was the rocket launch business

443
00:42:49,820 --> 00:42:53,820
the government will pay or the customers pay for the vehicles before you build them

444
00:42:53,820 --> 00:42:58,820
so it's actually cash flow positive, there's some money they needed for expansion

445
00:42:58,820 --> 00:43:03,820
but it was basically a cash flow positive business, it was a weird investment in 2008

446
00:43:03,820 --> 00:43:09,820
they didn't need any of the money but there was some NASA or government rule where they needed outside investors

447
00:43:09,820 --> 00:43:15,820
and so they were forced to take investors and then we were on good enough terms that we did it

448
00:43:16,820 --> 00:43:19,820
if you had been a Tesla shareholder we've all been reading about it

449
00:43:19,820 --> 00:43:23,820
would have you paid him the big compensation package?

450
00:43:23,820 --> 00:43:33,820
I would have, well I think the nuanced answer is I would have voted in favor of the compensation package

451
00:43:33,820 --> 00:43:39,820
because you would know that if it failed the share price would have gone down a lot the next day

452
00:43:39,820 --> 00:43:43,820
because people would wonder whether Elon would quit and that would be bad for the company

453
00:43:43,820 --> 00:43:48,820
so whether you believe in the package or not the rational thing would be that you should vote for it

454
00:43:48,820 --> 00:43:54,820
and then if you think it's a bad idea maybe you sell your shares after you get a pop or something like this

455
00:43:54,820 --> 00:44:00,820
so that's the obvious game theory on why Elon was going to win that vote no matter what

456
00:44:00,820 --> 00:44:07,820
and it was really crazy that we listened to people in the media and I'm sure yourself

457
00:44:07,820 --> 00:44:11,820
but we're all saying it was a hair-brained thing and the shareholders were all going to vote against it

458
00:44:11,820 --> 00:44:16,820
and if you just did the basic analysis it was obvious that Elon was going to win the vote

459
00:44:16,820 --> 00:44:18,820
regardless of what the shareholders actually were

460
00:44:18,820 --> 00:44:20,820
What did you think of him investing in X?

461
00:44:20,820 --> 00:44:25,820
By the way X is what he wanted PayPal to be, did you give him money for that?

462
00:44:30,820 --> 00:44:38,820
We didn't do anything on the Twitter one, we didn't do anything on the current XAI company

463
00:44:38,820 --> 00:44:42,820
I guess there's a lot of different things that have X in the name with Elon

464
00:44:50,820 --> 00:45:02,820
I think it was an incredibly, I do think we need a broader surface area for debate in our society

465
00:45:02,820 --> 00:45:07,820
and so I think obviously there are all these very complicated trade-offs between

466
00:45:07,820 --> 00:45:14,820
how much speech do you suppress, how much good speech are we suppressing, how much bad speech are we allowing

467
00:45:14,820 --> 00:45:17,820
how do you get those trade-offs right, very, very hard to do

468
00:45:17,820 --> 00:45:22,820
My judgment is we should have just a lot more surface area for debate discussion

469
00:45:22,820 --> 00:45:29,820
and I think what Elon did with Twitter was I think extremely important

470
00:45:29,820 --> 00:45:32,820
and I support it as an ideological project

471
00:45:32,820 --> 00:45:40,820
I worry about it as a financial thing, I don't know if that works

472
00:45:40,820 --> 00:45:46,820
We've looked over the years, we've looked over and over again at starting some kind of media company

473
00:45:46,820 --> 00:45:53,820
and there's always sort of this thought, can't you do something else in the sort of right of center media space

474
00:45:53,820 --> 00:45:59,820
and does it have to all be as lame as Fox News, isn't there an opening to do something else

475
00:45:59,820 --> 00:46:06,820
and then the question you always have to ask is it the Murdoch family that keeps it lame

476
00:46:06,820 --> 00:46:10,820
Why do you think it's lame?

477
00:46:10,820 --> 00:46:13,820
I think it's lame because they're controlled by the advertisers

478
00:46:13,820 --> 00:46:19,820
and there's a very narrow limit on what they can do

479
00:46:19,820 --> 00:46:26,820
and then the Elon question with Twitter was are you really allowed to do this and keep the advertisers

480
00:46:26,820 --> 00:46:28,820
and so that's where...

481
00:46:28,820 --> 00:46:30,820
Would you make it harder?

482
00:46:30,820 --> 00:46:36,820
It's super important what Elon did as a non-profit but it's going to be tough as a best...

483
00:46:36,820 --> 00:46:41,820
What about Truth Social?

484
00:46:41,820 --> 00:46:45,820
They have a few other problems they have to solve first

485
00:46:45,820 --> 00:46:47,820
Now it's something you'd invest in

486
00:46:47,820 --> 00:46:50,820
You get your head around the $6 billion valuation

487
00:46:50,820 --> 00:46:57,820
If I wanted to secretly funnel money to the Trump campaign and get around the campaign limitations

488
00:46:57,820 --> 00:47:00,820
so the stock price goes up and you can sell some stock and fund this campaign

489
00:47:00,820 --> 00:47:02,820
that might be a reason to invest

490
00:47:02,820 --> 00:47:07,820
Do you think people are doing that?

491
00:47:07,820 --> 00:47:13,820
They probably don't think of it in quite that literal term but maybe that's what's going on

492
00:47:13,820 --> 00:47:17,820
Have you talked to people in your realm who have said hey?

493
00:47:17,820 --> 00:47:26,820
Nobody's said that but I suspect a lot of the investors are going to vote for Trump

494
00:47:26,820 --> 00:47:31,820
They're thinking about it at least on some subconscious, not articulated level

495
00:47:31,820 --> 00:47:34,820
I want to talk a little more about Trump in just one more second

496
00:47:34,820 --> 00:47:37,820
but I want to ask you one last related social media question

497
00:47:37,820 --> 00:47:41,820
which is the search in general was here in Aspen

498
00:47:41,820 --> 00:47:44,820
and I think you've probably seen in the last couple of weeks that he came out

499
00:47:44,820 --> 00:47:51,820
and genuinely believes that social media and the Facebooks of the world really

500
00:47:51,820 --> 00:47:58,820
have done a real disservice to young people in the country

501
00:47:58,820 --> 00:48:04,820
and I just wonder what you think of that as somebody who invested early in Facebook

502
00:48:04,820 --> 00:48:14,820
I can't say that he's 100% wrong

503
00:48:14,820 --> 00:48:22,820
The place where I always push back on is that I feel it's too easy to turn tech

504
00:48:22,820 --> 00:48:26,820
or the social media companies into the scapegoat for all of our problems

505
00:48:27,820 --> 00:48:34,820
There's some kind of an interesting critique one can make of the tech companies

506
00:48:34,820 --> 00:48:38,820
and if you ask how many of the executives in those companies

507
00:48:38,820 --> 00:48:41,820
how much screen time do they let their kids use

508
00:48:41,820 --> 00:48:44,820
and there's probably sort of an interesting critique one could make

509
00:48:44,820 --> 00:48:45,820
What do you do?

510
00:48:45,820 --> 00:48:47,820
Not very much and I think that's very...

511
00:48:47,820 --> 00:48:49,820
What's not very much?

512
00:48:49,820 --> 00:48:51,820
An hour and a half a week

513
00:48:51,820 --> 00:48:53,820
Something like that

514
00:48:53,820 --> 00:48:54,820
How old are your kids?

515
00:48:54,820 --> 00:48:56,820
Three and a half, five years

516
00:48:56,820 --> 00:48:58,820
Three and a half and five years old

517
00:49:02,820 --> 00:49:10,820
If I were to make the anti-tech argument it's that there are probably a lot of people in tech

518
00:49:10,820 --> 00:49:13,820
who do something quite similar for their own families

519
00:49:13,820 --> 00:49:16,820
and there's some questions that that might lead you to ask

520
00:49:16,820 --> 00:49:18,820
And then on the other hand

521
00:49:20,820 --> 00:49:29,820
I don't think this is the main cause for all the different types of social dysfunction we have

522
00:49:29,820 --> 00:49:33,820
and it's maybe it's a 15%, 20% cause

523
00:49:33,820 --> 00:49:39,820
There's sort of a lot of other things that have gone super haywire in our society

524
00:49:39,820 --> 00:49:46,820
and by putting all the blame onto this, onto tech or onto one company

525
00:49:46,820 --> 00:49:49,820
you are really ignoring a lot of other stuff

526
00:49:49,820 --> 00:49:51,820
We could do a whole panel on this but one related question

527
00:49:51,820 --> 00:49:53,820
because we haven't mentioned it, TikTok

528
00:49:53,820 --> 00:49:56,820
Do you think of TikTok as a national security threat?

529
00:49:59,820 --> 00:50:02,820
Yeah, it's a very...

530
00:50:02,820 --> 00:50:04,820
There's something very strange going on

531
00:50:04,820 --> 00:50:11,820
Obviously the TikTok algorithms for the US are very different from the ByteDance algorithms in China

532
00:50:11,820 --> 00:50:13,820
Would you shut it down in this country?

533
00:50:17,820 --> 00:50:22,820
I probably would lean towards a tougher response

534
00:50:22,820 --> 00:50:24,820
I think just to shift from the normative to the...

535
00:50:24,820 --> 00:50:26,820
I don't think we're going to do anything

536
00:50:26,820 --> 00:50:29,820
I met the TikTok CEO last summer

537
00:50:29,820 --> 00:50:33,820
and I was the Singaporean guy, the TikTok CEO

538
00:50:33,820 --> 00:50:37,820
and I told him he didn't need to worry about it being shut down in the US

539
00:50:37,820 --> 00:50:39,820
and maybe I'm wrong but I think...

540
00:50:39,820 --> 00:50:40,820
Because you know something?

541
00:50:40,820 --> 00:50:45,820
Because we are incompetent and slow and bureaucratic

542
00:50:45,820 --> 00:50:48,820
and we will never get our act together in dealing with the problems of China

543
00:50:48,820 --> 00:50:50,820
until the day they invade Taiwan

544
00:50:50,820 --> 00:50:53,820
and then it will be shut down within 24 hours

545
00:50:53,820 --> 00:50:58,820
and since I think there's a 50-50 chance that China will invade Taiwan in the next five years

546
00:50:58,820 --> 00:51:03,820
my advice to the TikTok CEO was you should take all your people and computers

547
00:51:03,820 --> 00:51:09,820
and get them out of China because once Taiwan gets invaded it'll be too late

548
00:51:09,820 --> 00:51:12,820
so that's my advice but you don't need to worry about us doing anything before then

549
00:51:12,820 --> 00:51:15,820
and then his somewhat...

550
00:51:15,820 --> 00:51:20,820
I'm not sure good or worrisome answer was that they had studied World War I and World War II very carefully

551
00:51:20,820 --> 00:51:25,820
and there were a bunch of companies that were able to trade with all sides in those wars

552
00:51:25,820 --> 00:51:30,820
By the way, that implies that he also thinks that China is going to invade Taiwan

553
00:51:30,820 --> 00:51:31,820
He did not...

554
00:51:31,820 --> 00:51:34,820
You know, again I didn't frame it deterministically

555
00:51:34,820 --> 00:51:37,820
I said 50% chance, five years

556
00:51:37,820 --> 00:51:40,820
We are over time but we're going to keep going just for a little bit

557
00:51:40,820 --> 00:51:43,820
because I promise you we're going to talk a little bit about politics

558
00:51:43,820 --> 00:51:46,820
and I want to talk about your own politics, your own personal politics

559
00:51:46,820 --> 00:51:54,820
You were very vocal and outspoken about supporting who is now the former president the last time

560
00:51:54,820 --> 00:51:59,820
You have been less outspoken this time

561
00:51:59,820 --> 00:52:01,820
We're all going to watch the debate tonight

562
00:52:01,820 --> 00:52:05,820
So before we can get into the lessons and everything that you've learned

563
00:52:05,820 --> 00:52:08,820
and all of your prior experience

564
00:52:08,820 --> 00:52:11,820
are you planning to support the president this time?

565
00:52:11,820 --> 00:52:13,820
You know, I'm...

566
00:52:13,820 --> 00:52:15,820
I'm the former president I should say

567
00:52:15,820 --> 00:52:17,820
You know, you hold a gun to my head

568
00:52:17,820 --> 00:52:21,820
I'll vote for Trump, I'll still...

569
00:52:22,820 --> 00:52:24,820
I'd rather have him vote than Biden

570
00:52:24,820 --> 00:52:27,820
I'm not going to give any money to his super PAC

571
00:52:27,820 --> 00:52:32,820
I'm going to be less involved in all these ways

572
00:52:32,820 --> 00:52:35,820
And man, look, it is...

573
00:52:35,820 --> 00:52:39,820
And then I don't know, I think Trump will win

574
00:52:39,820 --> 00:52:42,820
I think he will win quite solidly

575
00:52:42,820 --> 00:52:45,820
I don't think it's going to even be close

576
00:52:45,820 --> 00:52:49,820
And then my pessimistic look ahead function is

577
00:52:49,820 --> 00:52:53,820
after he wins there will be a lot of buyer's remorse

578
00:52:53,820 --> 00:52:55,820
because the elections are A-B tests

579
00:52:55,820 --> 00:52:58,820
You know, if you asked me to make a pro-Trump argument

580
00:52:58,820 --> 00:53:01,820
I wouldn't, but I can probably come up with anti-Biden arguments

581
00:53:01,820 --> 00:53:03,820
and Biden is not going to make a pro-Biden argument

582
00:53:03,820 --> 00:53:05,820
he's going to make anti-Trump arguments

583
00:53:05,820 --> 00:53:08,820
and it's these two different hate factories

584
00:53:08,820 --> 00:53:10,820
that we have targeted at each other

585
00:53:10,820 --> 00:53:12,820
and that's the way the politics work

586
00:53:12,820 --> 00:53:15,820
and my judgment is Trump will easily win that

587
00:53:15,820 --> 00:53:17,820
but yeah, the election is a relative choice

588
00:53:17,820 --> 00:53:19,820
the post-election is absolute

589
00:53:19,820 --> 00:53:22,820
and then it'll be like, you know, if Biden wins

590
00:53:22,820 --> 00:53:24,820
like, how did we get this senile old man

591
00:53:24,820 --> 00:53:27,820
and if Trump wins, it'll be...

592
00:53:27,820 --> 00:53:29,820
Can I just ask you this?

593
00:53:29,820 --> 00:53:32,820
It's still like this clown show, whatever people will say

594
00:53:32,820 --> 00:53:34,820
I'm not going to ask you to make the pro-Trump argument

595
00:53:34,820 --> 00:53:36,820
That's sort of...

596
00:53:36,820 --> 00:53:39,820
But let me ask you about what I imagine is your anti-Biden argument

597
00:53:39,820 --> 00:53:41,820
I look at the last four years and say to myself

598
00:53:41,820 --> 00:53:44,820
if you were in Silicon Valley and you owned stock

599
00:53:44,820 --> 00:53:46,820
and these tech companies over the last four years

600
00:53:46,820 --> 00:53:48,820
you unfortunately did nothing but go up

601
00:53:48,820 --> 00:53:50,820
and I know...

602
00:53:50,820 --> 00:53:53,820
And I just... I wonder if you can make the argument

603
00:53:53,820 --> 00:53:55,820
because we can talk about Lina Khan

604
00:53:55,820 --> 00:53:58,820
and we can talk about regulations and potential taxes

605
00:53:58,820 --> 00:54:00,820
and all sorts of things

606
00:54:00,820 --> 00:54:02,820
but it's hard for me to look at the last four years and say

607
00:54:02,820 --> 00:54:04,820
especially if I was sitting

608
00:54:04,820 --> 00:54:06,820
I would imagine in your seat and say

609
00:54:06,820 --> 00:54:08,820
this was a terrible travesty

610
00:54:08,820 --> 00:54:10,820
but maybe I don't understand

611
00:54:10,820 --> 00:54:12,820
Well, I mean, I don't know

612
00:54:12,820 --> 00:54:14,820
this may not be believable to you

613
00:54:14,820 --> 00:54:16,820
but I...

614
00:54:16,820 --> 00:54:18,820
I don't think it's...

615
00:54:18,820 --> 00:54:20,820
the only thing I care about

616
00:54:20,820 --> 00:54:23,820
is whether the country is good for the tech billionaires

617
00:54:23,820 --> 00:54:26,820
and I think there are

618
00:54:26,820 --> 00:54:28,820
a lot of people who have not experienced

619
00:54:28,820 --> 00:54:30,820
the last three or four years this way

620
00:54:30,820 --> 00:54:32,820
I think one of the things Carvel said in the earlier

621
00:54:32,820 --> 00:54:34,820
presentation just before

622
00:54:34,820 --> 00:54:36,820
this one that I thought was quite good

623
00:54:36,820 --> 00:54:38,820
was there's been a shocking loss of support

624
00:54:38,820 --> 00:54:41,820
by the Democrats in the 18 to 35 year voters

625
00:54:41,820 --> 00:54:43,820
and it's because you can't get on the housing ladder

626
00:54:43,820 --> 00:54:45,820
you can...

627
00:54:45,820 --> 00:54:47,820
you know, the college debt's overwhelming

628
00:54:47,820 --> 00:54:49,820
you can never get started

629
00:54:49,820 --> 00:54:51,820
and so there's sort of a sense that...

630
00:54:51,820 --> 00:54:53,820
You think he'll be able to fix that?

631
00:54:53,820 --> 00:54:55,820
I don't...

632
00:54:55,820 --> 00:54:57,820
I think it's...

633
00:54:57,820 --> 00:54:59,820
it's just an up-down referendum

634
00:54:59,820 --> 00:55:01,820
on the incumbent at this point

635
00:55:01,820 --> 00:55:03,820
and, you know,

636
00:55:03,820 --> 00:55:05,820
my guess is that the sense is

637
00:55:05,820 --> 00:55:07,820
Biden's definitely not going to fix it

638
00:55:07,820 --> 00:55:09,820
and his time will run out and that's...

639
00:55:09,820 --> 00:55:11,820
and then...

640
00:55:11,820 --> 00:55:13,820
this is where I'm not overly excited

641
00:55:13,820 --> 00:55:15,820
I don't think Trump will

642
00:55:15,820 --> 00:55:17,820
particularly fix it but look

643
00:55:17,820 --> 00:55:19,820
the place where people, you know

644
00:55:19,820 --> 00:55:21,820
at this...

645
00:55:21,820 --> 00:55:23,820
in the audience here I think are just maximally

646
00:55:23,820 --> 00:55:25,820
divergent

647
00:55:25,820 --> 00:55:27,820
yes, the stock market has been great for people here

648
00:55:27,820 --> 00:55:29,820
you're in this wonderful

649
00:55:29,820 --> 00:55:31,820
bubble in Aspen where it's like

650
00:55:31,820 --> 00:55:33,820
I know Clinton is still president

651
00:55:33,820 --> 00:55:35,820
and it's 1995

652
00:55:35,820 --> 00:55:37,820
and everything

653
00:55:37,820 --> 00:55:39,820
is just getting better every day

654
00:55:39,820 --> 00:55:41,820
in every way and it's like some new age

655
00:55:41,820 --> 00:55:43,820
chant if you just say that to yourselves

656
00:55:43,820 --> 00:55:45,820
it's true

657
00:55:45,820 --> 00:55:47,820
and then

658
00:55:47,820 --> 00:55:49,820
the...

659
00:55:49,820 --> 00:55:51,820
the part of the Trump

660
00:55:51,820 --> 00:55:53,820
statement that I think was

661
00:55:53,820 --> 00:55:55,820
the most offensive

662
00:55:55,820 --> 00:55:57,820
thing he said

663
00:55:57,820 --> 00:55:59,820
it was very offensive not just

664
00:55:59,820 --> 00:56:01,820
to Democrats and to Republicans

665
00:56:01,820 --> 00:56:03,820
and especially to Silicon Valley

666
00:56:03,820 --> 00:56:05,820
was make America great again

667
00:56:05,820 --> 00:56:07,820
because that was a pessimistic slogan

668
00:56:07,820 --> 00:56:09,820
it was the most pessimistic slogan

669
00:56:09,820 --> 00:56:11,820
a major presidential candidate

670
00:56:11,820 --> 00:56:13,820
ever had because

671
00:56:13,820 --> 00:56:15,820
what it says implicitly is this is no longer

672
00:56:15,820 --> 00:56:17,820
a great country

673
00:56:17,820 --> 00:56:19,820
and that's what you are never supposed to say

674
00:56:19,820 --> 00:56:21,820
especially if you're a Republican

675
00:56:21,820 --> 00:56:23,820
that's why the Bush people probably hate him

676
00:56:23,820 --> 00:56:25,820
more than anybody in this audience

677
00:56:25,820 --> 00:56:27,820
you know

678
00:56:27,820 --> 00:56:29,820
and then

679
00:56:29,820 --> 00:56:31,820
Silicon Valley was, you know

680
00:56:31,820 --> 00:56:33,820
it's somewhat offensive to people in New York City

681
00:56:33,820 --> 00:56:35,820
but the bankers on Wall Street don't really think

682
00:56:35,820 --> 00:56:37,820
they're making the country a great place

683
00:56:37,820 --> 00:56:39,820
so it's not personally offensive

684
00:56:39,820 --> 00:56:41,820
it was personally offensive to Silicon Valley

685
00:56:41,820 --> 00:56:43,820
and yet

686
00:56:43,820 --> 00:56:45,820
I always think

687
00:56:45,820 --> 00:56:47,820
there is this problem of stagnation

688
00:56:47,820 --> 00:56:49,820
there is this problem we're stuck

689
00:56:49,820 --> 00:56:51,820
there's a sense that

690
00:56:51,820 --> 00:56:53,820
we're not progressing in all these ways

691
00:56:53,820 --> 00:56:55,820
as a society as much as we have

692
00:56:55,820 --> 00:56:57,820
I don't think Trump has all the answers

693
00:56:57,820 --> 00:56:59,820
but I think

694
00:56:59,820 --> 00:57:01,820
what I said in 2016 is

695
00:57:01,820 --> 00:57:03,820
the first step

696
00:57:03,820 --> 00:57:05,820
towards solving problems is to at least talk about them

697
00:57:05,820 --> 00:57:07,820
what about the polarization part

698
00:57:07,820 --> 00:57:09,820
the polarization part the uncertainty part

699
00:57:09,820 --> 00:57:11,820
the questions about democracy

700
00:57:11,820 --> 00:57:13,820
and the rule of law and the future

701
00:57:13,820 --> 00:57:15,820
of a country and I think there's a lot of people

702
00:57:15,820 --> 00:57:17,820
who worry about those things

703
00:57:17,820 --> 00:57:19,820
sure those are all

704
00:57:19,820 --> 00:57:21,820
those are all still

705
00:57:21,820 --> 00:57:23,820
those are things you probably wouldn't worry about

706
00:57:23,820 --> 00:57:25,820
if Biden was the president right

707
00:57:25,820 --> 00:57:27,820
I feel the country is still very polarized

708
00:57:27,820 --> 00:57:29,820
it's been getting more polarized

709
00:57:29,820 --> 00:57:31,820
for decades

710
00:57:31,820 --> 00:57:33,820
it was polarized against

711
00:57:33,820 --> 00:57:35,820
Bork in the 80s

712
00:57:35,820 --> 00:57:37,820
that was sort of a new crescendo in polarization

713
00:57:37,820 --> 00:57:39,820
there was way Fox News was polarized

714
00:57:39,820 --> 00:57:41,820
against the Clintons

715
00:57:41,820 --> 00:57:43,820
and I don't know

716
00:57:43,820 --> 00:57:45,820
it's always what's cause and effect

717
00:57:45,820 --> 00:57:47,820
is the polarization causing the stagnation

718
00:57:47,820 --> 00:57:49,820
or does the stagnation lead to the polarization

719
00:57:49,820 --> 00:57:51,820
I don't think the polarization just happens

720
00:57:51,820 --> 00:57:53,820
in a country where everything is gross

721
00:57:53,820 --> 00:57:55,820
it's a tonal question

722
00:57:55,820 --> 00:57:57,820
you come across a very sensible reasonable person

723
00:57:57,820 --> 00:57:59,820
I think

724
00:57:59,820 --> 00:58:01,820
there are people who hear

725
00:58:01,820 --> 00:58:03,820
I'm sure disagree with you about

726
00:58:03,820 --> 00:58:05,820
lots of different issues but

727
00:58:05,820 --> 00:58:07,820
my question about tone

728
00:58:07,820 --> 00:58:09,820
about the president and the tone of the president

729
00:58:09,820 --> 00:58:11,820
by the way I should say

730
00:58:11,820 --> 00:58:13,820
and I'm not speaking out of school

731
00:58:13,820 --> 00:58:15,820
you also I would say by the way

732
00:58:15,820 --> 00:58:17,820
you've been a Republican

733
00:58:17,820 --> 00:58:19,820
for a long time now

734
00:58:19,820 --> 00:58:21,820
public about that

735
00:58:21,820 --> 00:58:23,820
you're also proudly gay

736
00:58:23,820 --> 00:58:25,820
openly so

737
00:58:25,820 --> 00:58:27,820
and I wonder if you can tie

738
00:58:27,820 --> 00:58:29,820
you know

739
00:58:29,820 --> 00:58:31,820
President Trump when he talks

740
00:58:31,820 --> 00:58:33,820
about some of the issues around LGBT issues

741
00:58:33,820 --> 00:58:35,820
in this country and other people

742
00:58:35,820 --> 00:58:37,820
there are people in those communities

743
00:58:37,820 --> 00:58:39,820
who say they don't feel safe about it

744
00:58:39,820 --> 00:58:41,820
yeah

745
00:58:41,820 --> 00:58:43,820
we can go through all these different

746
00:58:43,820 --> 00:58:45,820
different versions of that

747
00:58:45,820 --> 00:58:47,820
I think

748
00:58:47,820 --> 00:58:49,820
there was never any thought of reversing gay marriage

749
00:58:49,820 --> 00:58:51,820
or any of those things

750
00:58:51,820 --> 00:58:53,820
by Trump

751
00:58:53,820 --> 00:58:55,820
at least

752
00:58:55,820 --> 00:58:57,820
and look

753
00:58:57,820 --> 00:58:59,820
I think the

754
00:58:59,820 --> 00:59:01,820
yeah there are all these ways

755
00:59:01,820 --> 00:59:03,820
they're not the way

756
00:59:03,820 --> 00:59:05,820
I would articulate these things

757
00:59:05,820 --> 00:59:07,820
but

758
00:59:07,820 --> 00:59:09,820
these sort of polite tone

759
00:59:09,820 --> 00:59:11,820
there was

760
00:59:13,820 --> 00:59:15,820
people had attempted to say

761
00:59:15,820 --> 00:59:17,820
something's gone very wrong in our country

762
00:59:17,820 --> 00:59:19,820
the house is on fire, it's burning to the ground

763
00:59:19,820 --> 00:59:21,820
we are a society

764
00:59:21,820 --> 00:59:23,820
in decline, stagnation

765
00:59:23,820 --> 00:59:25,820
maybe

766
00:59:25,820 --> 00:59:27,820
AI will save us

767
00:59:27,820 --> 00:59:29,820
but this is the way people talk about AI

768
00:59:29,820 --> 00:59:31,820
if it doesn't lead to this cornucopian growth

769
00:59:31,820 --> 00:59:33,820
we're just completely going to be

770
00:59:33,820 --> 00:59:35,820
buried by budget deficits

771
00:59:35,820 --> 00:59:37,820
and debt

772
00:59:37,820 --> 00:59:39,820
for decades to come

773
00:59:39,820 --> 00:59:41,820
and I think

774
00:59:41,820 --> 00:59:43,820
AI is a big thing

775
00:59:43,820 --> 00:59:45,820
is it big enough to solve our budget deficit problem

776
00:59:45,820 --> 00:59:47,820
I don't believe it is

777
00:59:47,820 --> 00:59:49,820
we have a lot of these

778
00:59:49,820 --> 00:59:51,820
problems and

779
00:59:51,820 --> 00:59:53,820
at this point

780
00:59:53,820 --> 00:59:55,820
extra politeness

781
00:59:55,820 --> 00:59:57,820
is not quite

782
00:59:57,820 --> 00:59:59,820
the thing

783
00:59:59,820 --> 01:00:01,820
it was an inarticulate shriek for help

784
01:00:01,820 --> 01:00:03,820
and look my sort of fantasy

785
01:00:03,820 --> 01:00:05,820
in 2016 in supporting Trump

786
01:00:05,820 --> 01:00:07,820
this was where I was completely delusional

787
01:00:07,820 --> 01:00:09,820
this would be the way you start to have a conversation

788
01:00:09,820 --> 01:00:11,820
and

789
01:00:11,820 --> 01:00:13,820
and that's why

790
01:00:13,820 --> 01:00:15,820
another reason why I'm

791
01:00:15,820 --> 01:00:17,820
off-ramping I'd much rather have

792
01:00:17,820 --> 01:00:19,820
the sort of conversation we had here

793
01:00:19,820 --> 01:00:21,820
and if I lean in all the way to support Trump

794
01:00:21,820 --> 01:00:23,820
it'll be all about that

795
01:00:23,820 --> 01:00:25,820
and we can't talk about all these other things

796
01:00:25,820 --> 01:00:27,820
which is the way we are going to substantively solve the problems

797
01:00:27,820 --> 01:00:29,820
I want to thank you for this conversation

798
01:00:29,820 --> 01:00:31,820
and for addressing all these issues

799
01:00:31,820 --> 01:00:33,820
it really was a phenomenal discussion

800
01:00:33,820 --> 01:00:35,820
awesome thank you so so much

801
01:00:35,820 --> 01:00:37,820
thank you very much

802
01:00:49,820 --> 01:00:51,820
you

