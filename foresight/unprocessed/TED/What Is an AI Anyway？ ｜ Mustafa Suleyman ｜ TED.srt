1
00:00:00,000 --> 00:00:07,720
I want to tell you what I see coming.

2
00:00:07,740 --> 00:00:12,700
I've been lucky enough to be working on AI for almost 15 years now.

3
00:00:12,720 --> 00:00:17,820
Back when I started to describe it as fringe would be an understatement,

4
00:00:17,840 --> 00:00:19,280
researchers would say,

5
00:00:19,300 --> 00:00:21,920
no, we're only working on machine learning,

6
00:00:21,940 --> 00:00:25,060
because working on AI was seen as way too out there.

7
00:00:25,080 --> 00:00:29,460
In 2010, just a very mention of the phrase AGI,

8
00:00:29,480 --> 00:00:31,640
artificial general intelligence,

9
00:00:31,660 --> 00:00:34,540
would get you some seriously strange looks,

10
00:00:34,560 --> 00:00:36,400
and even a cold shoulder.

11
00:00:36,420 --> 00:00:40,280
You're actually building AGI, people would say.

12
00:00:40,300 --> 00:00:42,940
Isn't that something out of science fiction?

13
00:00:42,960 --> 00:00:45,400
People thought it was 50 years away or 100 years away,

14
00:00:45,420 --> 00:00:47,820
if it was even possible at all.

15
00:00:47,840 --> 00:00:51,680
Talk of AI was, I guess, kind of embarrassing.

16
00:00:51,700 --> 00:00:54,260
People generally thought we were weird,

17
00:00:54,280 --> 00:00:56,800
and I guess in some ways we kind of were.

18
00:00:56,860 --> 00:00:57,980
It wasn't long, though,

19
00:00:58,000 --> 00:01:01,580
before AI started beating humans at a whole range of tasks

20
00:01:01,600 --> 00:01:05,080
that people previously thought were way out of reach,

21
00:01:05,100 --> 00:01:07,200
understanding images,

22
00:01:07,220 --> 00:01:09,160
translating languages,

23
00:01:09,180 --> 00:01:10,740
transcribing speech,

24
00:01:10,760 --> 00:01:12,900
playing go and chess,

25
00:01:12,920 --> 00:01:14,780
and even diagnosing diseases.

26
00:01:15,980 --> 00:01:17,500
People started waking up to the fact

27
00:01:17,520 --> 00:01:21,200
that AI was going to have an enormous impact,

28
00:01:21,220 --> 00:01:24,020
and they were rightly asking technologists like me

29
00:01:24,040 --> 00:01:25,780
some pretty tough questions.

30
00:01:25,800 --> 00:01:29,140
Is it true that AI is going to solve the climate crisis?

31
00:01:29,160 --> 00:01:32,920
Will it make personalized education available to everyone?

32
00:01:32,940 --> 00:01:34,920
Does it mean we'll all get universal basic income

33
00:01:34,940 --> 00:01:37,240
and we won't have to work anymore?

34
00:01:37,260 --> 00:01:38,800
Should I be afraid?

35
00:01:38,820 --> 00:01:41,620
What does it mean for weapons and war?

36
00:01:41,640 --> 00:01:43,080
And of course, will China win?

37
00:01:43,100 --> 00:01:44,320
Are we in a race?

38
00:01:45,580 --> 00:01:49,180
Are we headed for a mass misinformation apocalypse?

39
00:01:49,200 --> 00:01:50,640
All good questions.

40
00:01:51,620 --> 00:01:55,560
But it was actually a simpler and much more kind of fundamental question

41
00:01:55,620 --> 00:01:56,960
that left me puzzled.

42
00:01:58,100 --> 00:02:01,800
One that actually gets to the very heart of my work every day.

43
00:02:03,720 --> 00:02:05,760
One morning over breakfast,

44
00:02:05,780 --> 00:02:09,160
my six-year-old nephew, Caspian, was playing with pie,

45
00:02:09,180 --> 00:02:11,940
the AI I created at my last company, Inflection.

46
00:02:12,580 --> 00:02:14,480
With a mouthful of scrambled eggs,

47
00:02:14,500 --> 00:02:17,960
he looked at me plain in the face and said,

48
00:02:17,980 --> 00:02:19,460
but Mustafa,

49
00:02:19,480 --> 00:02:21,140
what is an AI anyway?

50
00:02:22,000 --> 00:02:25,540
He's such a sincere and curious and optimistic little guy,

51
00:02:25,560 --> 00:02:28,080
he'd been talking to buy about how cool it would be

52
00:02:28,100 --> 00:02:32,220
if one day in the future he could visit dinosaurs at the zoo

53
00:02:32,240 --> 00:02:35,320
and how he could make infinite amounts of chocolate at home.

54
00:02:36,060 --> 00:02:38,640
And why pie couldn't yet play AI spy?

55
00:02:39,900 --> 00:02:42,900
Well, I said, it's a clever piece of software that's read

56
00:02:42,920 --> 00:02:44,620
most of the text on the open internet

57
00:02:44,640 --> 00:02:46,800
and it can talk to you about anything you want.

58
00:02:48,580 --> 00:02:49,720
Right.

59
00:02:49,740 --> 00:02:51,500
So like a person then,

60
00:02:52,000 --> 00:02:53,240
I was stumped,

61
00:02:54,960 --> 00:02:57,260
genuinely left scratching my head.

62
00:02:58,340 --> 00:03:02,120
All my boring stock answers came rushing through my mind.

63
00:03:02,620 --> 00:03:05,480
No, but AI is just another general-purpose technology,

64
00:03:05,500 --> 00:03:06,980
like printing or steam.

65
00:03:07,320 --> 00:03:09,780
It'll be a tool that will augment us

66
00:03:09,800 --> 00:03:12,080
and make us smarter and more productive.

67
00:03:12,580 --> 00:03:14,480
And when it gets better over time,

68
00:03:14,500 --> 00:03:16,440
it'll be like an all-knowing oracle

69
00:03:16,460 --> 00:03:19,560
that will help us solve grand scientific challenges.

70
00:03:19,860 --> 00:03:23,260
You know, all of these responses started to feel, I guess,

71
00:03:23,280 --> 00:03:25,040
a little bit defensive

72
00:03:26,040 --> 00:03:28,320
and actually better suited to a policy seminar

73
00:03:28,340 --> 00:03:30,480
than breakfast with a no-nonsense six-year-old.

74
00:03:30,500 --> 00:03:33,820
Why am I hesitating, I thought to myself?

75
00:03:35,660 --> 00:03:37,100
You know, let's be honest.

76
00:03:37,600 --> 00:03:40,600
My nephew was asking me a simple question

77
00:03:40,620 --> 00:03:44,580
that those of us in AI just don't confront often enough.

78
00:03:46,060 --> 00:03:48,520
What is it that we are not familiar with

79
00:03:48,540 --> 00:03:51,180
and what is it that we are actually creating?

80
00:03:52,040 --> 00:03:55,040
What does it mean to make something totally new,

81
00:03:55,820 --> 00:03:59,640
fundamentally different to any invention that we have known before?

82
00:04:00,800 --> 00:04:03,580
It is clear that we are at an inflection point

83
00:04:03,600 --> 00:04:05,320
in the history of humanity.

84
00:04:06,580 --> 00:04:08,500
On our current trajectory,

85
00:04:08,520 --> 00:04:10,540
we're headed towards the emergence of something

86
00:04:10,560 --> 00:04:13,000
that we are all struggling to describe.

87
00:04:13,960 --> 00:04:18,420
And yet, we cannot control what we don't understand.

88
00:04:19,520 --> 00:04:23,760
And so, the metaphors, the mental models, the names,

89
00:04:23,780 --> 00:04:26,900
these all matter if we are to get the most out of AI

90
00:04:26,920 --> 00:04:29,080
whilst limiting its potential downsides.

91
00:04:30,320 --> 00:04:33,720
As someone who embraces the possibilities of this technology,

92
00:04:33,740 --> 00:04:37,420
but who's also always cared deeply about its ethics,

93
00:04:37,440 --> 00:04:41,780
we should, I think, be able to easily describe what it is we are building,

94
00:04:41,800 --> 00:04:44,160
and that includes the six-year-olds.

95
00:04:44,180 --> 00:04:46,640
So it's in that spirit that I offer up today

96
00:04:46,780 --> 00:04:48,620
the following metaphor

97
00:04:48,640 --> 00:04:52,000
for helping us to try to grapple with what this moment really is.

98
00:04:52,620 --> 00:04:55,240
I think AI should best be understood

99
00:04:55,260 --> 00:04:59,280
as something like a new digital species.

100
00:05:00,360 --> 00:05:02,620
Now, don't take this too literally,

101
00:05:02,640 --> 00:05:07,320
but I predict that we'll come to see them as digital companions,

102
00:05:07,340 --> 00:05:10,600
new partners in the journeys of all our lives.

103
00:05:10,620 --> 00:05:14,720
Whether you think we're on a 10-, 20- or 30-year path here,

104
00:05:14,740 --> 00:05:17,280
this is, in my view, the most accurate

105
00:05:17,300 --> 00:05:22,640
and most fundamentally honest way of describing what's actually coming.

106
00:05:22,660 --> 00:05:26,640
And above all, it enables everybody to prepare for

107
00:05:26,660 --> 00:05:29,060
and shape what comes next.

108
00:05:29,920 --> 00:05:31,840
Now, I totally get this as a strong claim,

109
00:05:31,860 --> 00:05:36,320
and I'm going to explain to everyone as best I can why I'm making it.

110
00:05:36,340 --> 00:05:39,300
But first, let me just try to set the context.

111
00:05:39,320 --> 00:05:42,080
From the very first microscopic organisms,

112
00:05:42,100 --> 00:05:45,580
life on Earth stretches back billions of years.

113
00:05:45,600 --> 00:05:49,820
Over that time, life evolved and diversified.

114
00:05:49,840 --> 00:05:53,380
Then a few million years ago, something began to shift.

115
00:05:54,260 --> 00:05:57,880
After countless cycles of growth and adaptation,

116
00:05:57,900 --> 00:06:01,900
one of life's branches began using tools,

117
00:06:01,920 --> 00:06:04,840
and that branch grew into us.

118
00:06:04,860 --> 00:06:08,980
We went on to produce a mesmerizing variety of tools,

119
00:06:08,980 --> 00:06:12,660
at first slowly, and then with astonishing speed,

120
00:06:12,680 --> 00:06:16,320
we went from stone axes and fire

121
00:06:16,340 --> 00:06:21,080
to language, writing, and eventually, industrial technologies.

122
00:06:21,940 --> 00:06:25,240
One invention unleashed a thousand more,

123
00:06:25,260 --> 00:06:28,800
and in time, we became Homo Technologicus.

124
00:06:29,660 --> 00:06:34,000
Around 80 years ago, another new branch of technology began.

125
00:06:34,020 --> 00:06:35,780
With the invention of computers,

126
00:06:35,800 --> 00:06:39,120
we quickly jumped from the first mainframes and transistors

127
00:06:39,140 --> 00:06:42,620
to today's smartphones and virtual reality headsets.

128
00:06:42,640 --> 00:06:47,660
Information, knowledge, communication, computation.

129
00:06:47,680 --> 00:06:52,280
In this revolution, creation has exploded like never before.

130
00:06:53,320 --> 00:06:57,940
And now a new wave is upon us, artificial intelligence.

131
00:06:57,960 --> 00:07:00,560
These waves of history are clearly speeding up,

132
00:07:00,580 --> 00:07:05,180
as each one is amplified and accelerated by the last.

133
00:07:05,200 --> 00:07:06,320
And if you look back,

134
00:07:06,340 --> 00:07:10,900
it's clear that we are in the fastest and most consequential wave ever.

135
00:07:11,940 --> 00:07:16,580
The journeys of humanity and technology are now deeply intertwined.

136
00:07:16,600 --> 00:07:18,160
In just 18 months,

137
00:07:18,180 --> 00:07:21,820
over a billion people have used large language models.

138
00:07:21,840 --> 00:07:25,760
We've witnessed one landmark event after another.

139
00:07:25,780 --> 00:07:29,220
Just a few years ago, people said that AI would never be creative.

140
00:07:30,220 --> 00:07:34,280
And yet, AI now feels like an endless river of creativity,

141
00:07:34,300 --> 00:07:37,240
making poetry and images and music and video

142
00:07:37,260 --> 00:07:38,900
that stretch the imagination.

143
00:07:39,780 --> 00:07:42,500
People said it would never be empathetic.

144
00:07:42,520 --> 00:07:47,840
And yet today, millions of people enjoy meaningful conversations with AIs,

145
00:07:47,860 --> 00:07:49,840
talking about their hopes and dreams

146
00:07:49,860 --> 00:07:53,300
and helping them work through difficult emotional challenges.

147
00:07:53,320 --> 00:07:55,580
AIs can now drive cars,

148
00:07:55,600 --> 00:07:59,420
manage energy grids and even invent new molecules.

149
00:07:59,440 --> 00:08:03,000
Just a few years ago, each of these was impossible.

150
00:08:03,860 --> 00:08:05,980
And all of this is turbocharged

151
00:08:06,000 --> 00:08:11,040
by spiralling exponentials of data and computation.

152
00:08:11,060 --> 00:08:16,340
Last year, Inflation 2.5, our last model,

153
00:08:16,360 --> 00:08:20,480
used five billion times more computation

154
00:08:20,500 --> 00:08:24,120
than the DeepMind AI that beat the old-school Atari games

155
00:08:24,140 --> 00:08:26,180
just over 10 years ago.

156
00:08:26,200 --> 00:08:30,180
That's nine orders of magnitude more computation.

157
00:08:30,180 --> 00:08:35,060
10x per year, every year, for almost a decade.

158
00:08:35,080 --> 00:08:37,920
Over the same time, the size of these models has grown,

159
00:08:37,940 --> 00:08:41,880
from first tens of millions of parameters to then billions of parameters,

160
00:08:41,900 --> 00:08:45,360
and very soon tens of trillions of parameters.

161
00:08:45,380 --> 00:08:50,140
If someone did nothing but read 24 hours a day for their entire life,

162
00:08:50,160 --> 00:08:53,520
they'd consume eight billion words.

163
00:08:53,540 --> 00:08:55,400
And of course, that's a lot of words.

164
00:08:55,420 --> 00:09:01,200
But today, the most advanced AIs consume more than eight trillion words

165
00:09:01,220 --> 00:09:03,920
in a single month of training.

166
00:09:03,940 --> 00:09:05,920
And all of this is set to continue.

167
00:09:05,940 --> 00:09:09,060
The long arc of technological history

168
00:09:09,080 --> 00:09:12,320
is now in an extraordinary new phase.

169
00:09:12,340 --> 00:09:15,480
So what does this mean in practice?

170
00:09:15,500 --> 00:09:18,400
Well, just as the internet gave us the browser

171
00:09:18,420 --> 00:09:20,940
and the smartphone gave us apps,

172
00:09:20,960 --> 00:09:22,960
the cloud-based supercomputer

173
00:09:22,960 --> 00:09:27,500
is ushering in a new era of ubiquitous AIs.

174
00:09:27,520 --> 00:09:32,080
Everything will soon be represented by a conversational interface,

175
00:09:32,100 --> 00:09:34,820
or to put it another way, a personal AI.

176
00:09:35,820 --> 00:09:38,180
And these AIs will be infinitely knowledgeable,

177
00:09:38,200 --> 00:09:42,100
and soon they'll be factually accurate and reliable.

178
00:09:42,120 --> 00:09:44,000
They'll have near-perfect IQ.

179
00:09:44,980 --> 00:09:47,360
They'll also have exceptional EQ.

180
00:09:47,380 --> 00:09:51,320
They'll be kind, supportive, empathetic,

181
00:09:52,020 --> 00:09:53,980
and a kind of personalised tutor.

182
00:09:56,000 --> 00:09:58,700
These elements on their own would be transformational.

183
00:09:58,720 --> 00:10:02,380
Just imagine if everybody had a personalised tutor in their pocket

184
00:10:02,400 --> 00:10:04,900
and access to low-cost medical advice.

185
00:10:04,920 --> 00:10:08,820
A lawyer and a doctor, a business strategist and coach,

186
00:10:08,840 --> 00:10:11,260
all in your pocket 24 hours a day.

187
00:10:11,280 --> 00:10:13,760
But things really start to change

188
00:10:13,780 --> 00:10:18,120
when they develop what I call AQ, their actions quotient.

189
00:10:18,140 --> 00:10:21,000
This is their ability to actually get stuff done

190
00:10:21,000 --> 00:10:24,440
and, before long, it won't just be people that have AIs.

191
00:10:24,460 --> 00:10:26,120
Strangers, it may sound.

192
00:10:26,140 --> 00:10:30,660
Every organisation, from small business to nonprofit to national government,

193
00:10:30,680 --> 00:10:32,120
each will have their own.

194
00:10:32,900 --> 00:10:35,620
Every town, building and object

195
00:10:35,640 --> 00:10:39,360
will be represented by a unique interactive persona.

196
00:10:39,380 --> 00:10:42,300
And these won't just be mechanistic assistants.

197
00:10:42,320 --> 00:10:44,300
They'll be companions,

198
00:10:44,320 --> 00:10:46,120
confidants,

199
00:10:46,140 --> 00:10:47,500
colleagues,

200
00:10:47,540 --> 00:10:51,400
friends and partners, as varied and unique as we all are.

201
00:10:52,400 --> 00:10:53,560
At this point,

202
00:10:53,580 --> 00:10:56,880
AIs will convincingly imitate humans at most tasks.

203
00:10:57,840 --> 00:11:01,040
And we'll fill this at the most intimate of scales,

204
00:11:01,060 --> 00:11:04,700
an AI organising a community get-together for an elderly neighbour,

205
00:11:04,720 --> 00:11:06,240
a sympathetic expert

206
00:11:06,260 --> 00:11:09,320
helping you make sense of a difficult diagnosis.

207
00:11:09,340 --> 00:11:12,120
But we'll also feel it at the largest scales,

208
00:11:12,140 --> 00:11:14,700
accelerating scientific discovery,

209
00:11:14,720 --> 00:11:16,900
autonomous cars on the roads,

210
00:11:16,920 --> 00:11:19,080
drones in the skies.

211
00:11:19,100 --> 00:11:22,200
They'll both order the takeout and run the power station.

212
00:11:23,060 --> 00:11:26,440
They'll interact with us and, of course, with each other.

213
00:11:26,460 --> 00:11:28,380
They'll speak every language,

214
00:11:28,400 --> 00:11:31,240
take in every pattern of sensor data,

215
00:11:31,260 --> 00:11:33,580
sights, sounds,

216
00:11:33,600 --> 00:11:36,000
streams and streams of information,

217
00:11:36,020 --> 00:11:40,000
faster passing what any one of us could consume in a thousand lifetime.

218
00:11:40,880 --> 00:11:43,080
So what is this?

219
00:11:43,100 --> 00:11:44,860
What are these AIs?

220
00:11:45,860 --> 00:11:50,360
If we are to prioritise safety above all else,

221
00:11:50,380 --> 00:11:55,360
to ensure that this new wave always serves and amplifies humanity,

222
00:11:55,380 --> 00:12:00,800
then we need to find the right metaphors for what this might become.

223
00:12:00,820 --> 00:12:05,580
For years, we in the AI community and I specifically

224
00:12:05,600 --> 00:12:09,800
have had a tendency to refer to this as just tools.

225
00:12:09,820 --> 00:12:13,720
But that doesn't really capture what's actually happening here.

226
00:12:14,720 --> 00:12:17,260
AIs are clearly more dynamic,

227
00:12:17,280 --> 00:12:18,900
more ambiguous,

228
00:12:18,920 --> 00:12:22,640
more integrated and more emergent than mere tools,

229
00:12:22,660 --> 00:12:25,560
which are entirely subject to human control.

230
00:12:25,580 --> 00:12:28,100
So to contain this wave,

231
00:12:28,120 --> 00:12:31,180
to put human agency at its centre,

232
00:12:31,200 --> 00:12:33,900
and to mitigate the inevitable unintended consequences

233
00:12:33,920 --> 00:12:35,780
that are likely to arise,

234
00:12:35,800 --> 00:12:37,500
we should start to think about them

235
00:12:37,520 --> 00:12:41,140
as we might a new kind of digital species.

236
00:12:41,160 --> 00:12:42,940
Now, it's just an analogy.

237
00:12:43,020 --> 00:12:46,080
It's not a literal description, and it's not perfect.

238
00:12:46,100 --> 00:12:50,180
For a start, they clearly aren't biological in any traditional sense,

239
00:12:50,200 --> 00:12:52,580
but just pause for a moment

240
00:12:52,600 --> 00:12:55,580
and really think about what they already do.

241
00:12:55,600 --> 00:12:58,120
They communicate in our languages.

242
00:12:58,140 --> 00:13:00,320
They see what we see.

243
00:13:00,340 --> 00:13:03,720
They consume unimaginably large amounts of information.

244
00:13:04,860 --> 00:13:07,060
They have memory.

245
00:13:07,080 --> 00:13:08,640
They have personality.

246
00:13:09,620 --> 00:13:11,020
They have creativity.

247
00:13:12,020 --> 00:13:16,700
They can even reason to some extent and formulate rudimentary plans.

248
00:13:16,720 --> 00:13:20,520
They can act autonomously if we allow them.

249
00:13:20,540 --> 00:13:22,840
And they do all this at levels of sophistication

250
00:13:22,860 --> 00:13:26,420
that is far beyond anything that we've ever known from a mere tool.

251
00:13:27,740 --> 00:13:32,080
And so saying AI is mainly about the math or the code

252
00:13:32,100 --> 00:13:36,680
is like saying we humans are mainly about carbon and water.

253
00:13:37,900 --> 00:13:39,240
It's true,

254
00:13:39,320 --> 00:13:41,640
but it completely misses the point.

255
00:13:42,920 --> 00:13:46,540
And yes, I get it, this is a super-arresting thought,

256
00:13:46,560 --> 00:13:48,460
but I honestly think this frame

257
00:13:48,480 --> 00:13:51,760
helps sharpen our focus on the critical issues.

258
00:13:52,820 --> 00:13:54,260
What are the risks?

259
00:13:55,880 --> 00:13:58,360
What are the boundaries that we need to impose?

260
00:13:59,360 --> 00:14:03,520
What kind of AI do we want to build or allow to be built?

261
00:14:04,560 --> 00:14:06,960
This is a story that's still unfolding.

262
00:14:06,980 --> 00:14:09,080
Nothing should be accepted as a given.

263
00:14:09,480 --> 00:14:12,720
We all must choose what we create,

264
00:14:12,740 --> 00:14:14,760
what AIs we bring into the world,

265
00:14:15,760 --> 00:14:16,760
or not.

266
00:14:18,680 --> 00:14:21,480
These are the questions for all of us here today

267
00:14:21,500 --> 00:14:23,800
and all of us alive at this moment.

268
00:14:24,760 --> 00:14:29,000
For me, the benefits of this technology are stunningly obvious

269
00:14:29,020 --> 00:14:32,320
and they inspire my life's work every single day.

270
00:14:33,720 --> 00:14:36,560
But quite frankly, they'll speak for themselves.

271
00:14:37,560 --> 00:14:40,560
Over the years, I've never shied away from highlighting risks

272
00:14:40,580 --> 00:14:42,080
and talking about downsides.

273
00:14:43,000 --> 00:14:46,160
Thinking in this way helps us focus on the huge challenges

274
00:14:46,180 --> 00:14:47,640
that lie ahead for all of us.

275
00:14:48,680 --> 00:14:49,840
But let's be clear.

276
00:14:50,560 --> 00:14:54,480
There is no path to progress where we leave technology behind.

277
00:14:55,480 --> 00:14:59,120
The prize for all of civilization is immense.

278
00:14:59,880 --> 00:15:03,760
We need solutions in health care and education to our climate crisis,

279
00:15:04,000 --> 00:15:07,960
and if AI delivers just a fraction of its potential,

280
00:15:07,980 --> 00:15:12,000
the next decade is going to be the most productive in human history.

281
00:15:13,760 --> 00:15:15,800
Here's another way to think about it.

282
00:15:15,820 --> 00:15:20,760
In the past, unlocking economic growth often came with huge downsides.

283
00:15:21,560 --> 00:15:25,300
The economy expanded as people discovered new continents

284
00:15:25,320 --> 00:15:27,000
and opened up new frontiers.

285
00:15:28,720 --> 00:15:31,400
But they colonized populations at the same time.

286
00:15:32,400 --> 00:15:36,720
We built factories, but they were grim and dangerous places to work.

287
00:15:37,880 --> 00:15:41,040
We struck oil, but we polluted the planet.

288
00:15:41,960 --> 00:15:45,160
Now, because we are still designing and building AI,

289
00:15:45,200 --> 00:15:50,040
we have the potential and opportunity to do it better, radically better.

290
00:15:50,960 --> 00:15:53,560
And today, we're not discovering a new continent

291
00:15:53,600 --> 00:15:55,280
and plundering its resources.

292
00:15:56,120 --> 00:15:57,680
We're building one from scratch.

293
00:15:58,680 --> 00:16:02,880
Sometimes people say that data or chips are the 21st century's new oil.

294
00:16:04,000 --> 00:16:05,560
But that's totally the wrong image.

295
00:16:06,600 --> 00:16:11,000
AI is to the mind what nuclear fusion is to energy.

296
00:16:12,320 --> 00:16:16,040
Limitless, abundant, world-changing.

297
00:16:17,400 --> 00:16:19,680
And AI really is different.

298
00:16:20,400 --> 00:16:24,640
That means we have to think about it creatively and honestly.

299
00:16:24,720 --> 00:16:29,120
We have to push our analogies and our metaphors to the very limits

300
00:16:29,160 --> 00:16:31,400
to be able to grapple with what's coming,

301
00:16:31,440 --> 00:16:33,800
because this is not just another invention.

302
00:16:35,000 --> 00:16:38,240
AI is itself an infinite inventor.

303
00:16:38,960 --> 00:16:42,240
And yes, this is exciting and promising and concerning

304
00:16:42,280 --> 00:16:44,000
and intriguing all at once.

305
00:16:45,360 --> 00:16:47,160
To be quite honest, it's pretty surreal.

306
00:16:47,920 --> 00:16:52,720
But step back, see it on the long view of glacial time,

307
00:16:52,720 --> 00:16:57,520
and these really are the very most appropriate metaphors that we have today.

308
00:16:57,560 --> 00:16:59,400
Since the beginning of life on Earth,

309
00:17:00,560 --> 00:17:03,560
we've been evolving, changing,

310
00:17:03,600 --> 00:17:07,320
and then creating everything around us in our human world today.

311
00:17:08,320 --> 00:17:11,000
And AI isn't something outside of this story.

312
00:17:11,680 --> 00:17:14,320
In fact, it's the very opposite.

313
00:17:15,240 --> 00:17:18,440
It's the whole of everything that we have created

314
00:17:18,440 --> 00:17:23,040
distilled down into something that we can all interact with and benefit from.

315
00:17:23,960 --> 00:17:26,600
It's a reflection of humanity across time.

316
00:17:27,480 --> 00:17:30,520
And in this sense, it isn't a new species at all.

317
00:17:31,320 --> 00:17:33,000
This is where the metaphors end.

318
00:17:33,680 --> 00:17:36,040
Here's what Al-Talqaspian next time he asks.

319
00:17:37,320 --> 00:17:38,640
AI isn't separate.

320
00:17:39,480 --> 00:17:42,400
AI isn't even, in some senses, new.

321
00:17:43,600 --> 00:17:44,720
AI is us.

322
00:17:45,480 --> 00:17:46,480
It's all of us.

323
00:17:47,120 --> 00:17:50,920
And this is perhaps the most promising and vital thing of all

324
00:17:50,960 --> 00:17:53,160
that even a six-year-old can get a sense for.

325
00:17:54,640 --> 00:17:59,600
As we build out AI, we can and must reflect all that is good,

326
00:17:59,640 --> 00:18:03,560
all that we love, all that is special about humanity,

327
00:18:03,600 --> 00:18:07,840
our empathy, our kindness, our curiosity and our creativity.

328
00:18:09,440 --> 00:18:14,480
This, I would argue, is the greatest challenge of the 21st century,

329
00:18:14,520 --> 00:18:19,840
but also the most wonderful, inspiring and hopeful opportunity for all of us.

330
00:18:20,560 --> 00:18:21,560
Thank you.

331
00:18:21,600 --> 00:18:23,080
APPLAUSE

332
00:18:26,680 --> 00:18:28,080
Thank you. Thank you, Moustap.

333
00:18:28,120 --> 00:18:31,680
It's an amazing vision and a super powerful metaphor.

334
00:18:32,240 --> 00:18:34,000
You're in an amazing position right now.

335
00:18:34,040 --> 00:18:38,080
You were connected at the hip to the amazing work happening at OpenAI.

336
00:18:38,760 --> 00:18:40,760
You're going to have resources made available.

337
00:18:40,760 --> 00:18:44,960
There are reports of these giant new data centers,

338
00:18:45,000 --> 00:18:47,000
$100 billion invested and so forth.

339
00:18:48,200 --> 00:18:51,720
And a new species can emerge from it.

340
00:18:52,400 --> 00:18:54,280
I mean, you've done, in your book, you did,

341
00:18:54,320 --> 00:18:56,360
as well as painting an incredible optimistic vision.

342
00:18:56,400 --> 00:19:00,520
You were super eloquent on the dangers of AI.

343
00:19:00,560 --> 00:19:04,520
And I'm just curious where, from the view that you have now,

344
00:19:04,560 --> 00:19:06,680
what is it that most keeps you up at night?

345
00:19:07,200 --> 00:19:09,920
I think the great risk is that we get stuck in what I call

346
00:19:09,920 --> 00:19:11,560
the pessimism aversion trap.

347
00:19:11,600 --> 00:19:16,520
You know, we have to have the courage to confront the potential of dark scenarios

348
00:19:16,560 --> 00:19:19,640
in order to get the most out of all the benefits that we see, right?

349
00:19:19,680 --> 00:19:23,400
So the good news is that if you look at the last two or three years,

350
00:19:23,440 --> 00:19:26,360
there have been very, very few downsides, right?

351
00:19:26,400 --> 00:19:31,280
It's very hard to say explicitly what harm an LLM has caused.

352
00:19:31,320 --> 00:19:33,920
But that doesn't mean that that's what the trajectory is going to be

353
00:19:33,960 --> 00:19:35,120
over the next 10 years.

354
00:19:35,160 --> 00:19:39,240
So I think if you pay attention to a few specific capabilities,

355
00:19:39,280 --> 00:19:41,120
take, for example, autonomy.

356
00:19:41,160 --> 00:19:46,360
Autonomy is very obviously a threshold over which we increase risk in our society,

357
00:19:46,400 --> 00:19:49,280
and it's something that we should step towards very, very closely.

358
00:19:49,320 --> 00:19:52,920
The other would be something like recursive self-improvement.

359
00:19:52,960 --> 00:19:56,560
If you allow the model to independently self-improve,

360
00:19:56,600 --> 00:20:00,680
update its own code, explore an environment without oversight,

361
00:20:00,720 --> 00:20:04,760
and without a human in control to change how it operates,

362
00:20:04,800 --> 00:20:06,240
that would obviously be more dangerous.

363
00:20:06,280 --> 00:20:08,680
But I think that we're still some way away from that.

364
00:20:08,680 --> 00:20:12,120
I think it's still a good five to 10 years before we have to really confront that,

365
00:20:12,160 --> 00:20:13,960
but it's time to start talking about it now.

366
00:20:14,000 --> 00:20:17,000
A digital species, unlike any biological species,

367
00:20:17,040 --> 00:20:21,360
can replicate not in nine months, but in nine nanoseconds

368
00:20:21,400 --> 00:20:24,840
and produce an indefinite number of copies of itself,

369
00:20:24,880 --> 00:20:28,720
all of which have more power than we have in many ways.

370
00:20:28,760 --> 00:20:33,560
I mean, the possibility for unintended consequences seems pretty immense,

371
00:20:33,600 --> 00:20:37,680
and isn't it true that if a problem happens, it could happen in an hour?

372
00:20:37,720 --> 00:20:40,800
No, that is really not true.

373
00:20:40,840 --> 00:20:42,840
I think there's no evidence to suggest that,

374
00:20:42,880 --> 00:20:47,400
and I think that's often referred to as the intelligence explosion,

375
00:20:47,440 --> 00:20:51,120
and I think it is a theoretical, hypothetical maybe

376
00:20:51,160 --> 00:20:53,560
that we're all kind of curious to explore,

377
00:20:53,600 --> 00:20:56,520
but there's no evidence that we're anywhere near anything like that.

378
00:20:56,560 --> 00:20:59,320
And I think it's very important that we choose our words super carefully,

379
00:20:59,360 --> 00:21:03,840
because you're right, that's one of the weaknesses of the species framing.

380
00:21:03,920 --> 00:21:08,240
We will design the capability for self-replication into it

381
00:21:08,280 --> 00:21:11,920
if people choose to do that, and I would actually argue that we should not.

382
00:21:11,960 --> 00:21:14,240
That would be one of the dangerous capabilities

383
00:21:14,280 --> 00:21:16,000
that we should step back from.

384
00:21:16,040 --> 00:21:20,000
So there's no chance that this will emerge accidentally.

385
00:21:20,040 --> 00:21:22,560
I really think that's a very low probability.

386
00:21:22,600 --> 00:21:26,920
It will happen if engineers deliberately design those capabilities in,

387
00:21:26,960 --> 00:21:29,880
and if they don't take enough efforts to deliberately design them out.

388
00:21:29,920 --> 00:21:33,280
And so this is the point of being explicit and transparent

389
00:21:33,320 --> 00:21:37,920
about trying to introduce safety by design very early on.

390
00:21:39,160 --> 00:21:40,160
So thank you.

391
00:21:40,200 --> 00:21:45,120
I mean, your vision of humanity injecting into this new thing

392
00:21:45,160 --> 00:21:47,000
the best parts of ourselves,

393
00:21:47,040 --> 00:21:50,760
avoiding all those weird biological, freaky, horrible tendencies

394
00:21:50,800 --> 00:21:52,440
that we can have in certain circumstances.

395
00:21:52,480 --> 00:21:55,040
I mean, that is a very inspiring vision,

396
00:21:55,080 --> 00:21:58,400
and thank you so much for coming here and sharing it at TED.

397
00:21:58,440 --> 00:21:59,440
Thank you, good luck.

398
00:21:59,480 --> 00:22:00,480
Thanks a lot.

399
00:22:00,520 --> 00:22:01,520
I appreciate it.

