WEBVTT

00:00.000 --> 00:07.720
I want to tell you what I see coming.

00:07.740 --> 00:12.700
I've been lucky enough to be working on AI for almost 15 years now.

00:12.720 --> 00:17.820
Back when I started to describe it as fringe would be an understatement,

00:17.840 --> 00:19.280
researchers would say,

00:19.300 --> 00:21.920
no, we're only working on machine learning,

00:21.940 --> 00:25.060
because working on AI was seen as way too out there.

00:25.080 --> 00:29.460
In 2010, just a very mention of the phrase AGI,

00:29.480 --> 00:31.640
artificial general intelligence,

00:31.660 --> 00:34.540
would get you some seriously strange looks,

00:34.560 --> 00:36.400
and even a cold shoulder.

00:36.420 --> 00:40.280
You're actually building AGI, people would say.

00:40.300 --> 00:42.940
Isn't that something out of science fiction?

00:42.960 --> 00:45.400
People thought it was 50 years away or 100 years away,

00:45.420 --> 00:47.820
if it was even possible at all.

00:47.840 --> 00:51.680
Talk of AI was, I guess, kind of embarrassing.

00:51.700 --> 00:54.260
People generally thought we were weird,

00:54.280 --> 00:56.800
and I guess in some ways we kind of were.

00:56.860 --> 00:57.980
It wasn't long, though,

00:58.000 --> 01:01.580
before AI started beating humans at a whole range of tasks

01:01.600 --> 01:05.080
that people previously thought were way out of reach,

01:05.100 --> 01:07.200
understanding images,

01:07.220 --> 01:09.160
translating languages,

01:09.180 --> 01:10.740
transcribing speech,

01:10.760 --> 01:12.900
playing go and chess,

01:12.920 --> 01:14.780
and even diagnosing diseases.

01:15.980 --> 01:17.500
People started waking up to the fact

01:17.520 --> 01:21.200
that AI was going to have an enormous impact,

01:21.220 --> 01:24.020
and they were rightly asking technologists like me

01:24.040 --> 01:25.780
some pretty tough questions.

01:25.800 --> 01:29.140
Is it true that AI is going to solve the climate crisis?

01:29.160 --> 01:32.920
Will it make personalized education available to everyone?

01:32.940 --> 01:34.920
Does it mean we'll all get universal basic income

01:34.940 --> 01:37.240
and we won't have to work anymore?

01:37.260 --> 01:38.800
Should I be afraid?

01:38.820 --> 01:41.620
What does it mean for weapons and war?

01:41.640 --> 01:43.080
And of course, will China win?

01:43.100 --> 01:44.320
Are we in a race?

01:45.580 --> 01:49.180
Are we headed for a mass misinformation apocalypse?

01:49.200 --> 01:50.640
All good questions.

01:51.620 --> 01:55.560
But it was actually a simpler and much more kind of fundamental question

01:55.620 --> 01:56.960
that left me puzzled.

01:58.100 --> 02:01.800
One that actually gets to the very heart of my work every day.

02:03.720 --> 02:05.760
One morning over breakfast,

02:05.780 --> 02:09.160
my six-year-old nephew, Caspian, was playing with pie,

02:09.180 --> 02:11.940
the AI I created at my last company, Inflection.

02:12.580 --> 02:14.480
With a mouthful of scrambled eggs,

02:14.500 --> 02:17.960
he looked at me plain in the face and said,

02:17.980 --> 02:19.460
but Mustafa,

02:19.480 --> 02:21.140
what is an AI anyway?

02:22.000 --> 02:25.540
He's such a sincere and curious and optimistic little guy,

02:25.560 --> 02:28.080
he'd been talking to buy about how cool it would be

02:28.100 --> 02:32.220
if one day in the future he could visit dinosaurs at the zoo

02:32.240 --> 02:35.320
and how he could make infinite amounts of chocolate at home.

02:36.060 --> 02:38.640
And why pie couldn't yet play AI spy?

02:39.900 --> 02:42.900
Well, I said, it's a clever piece of software that's read

02:42.920 --> 02:44.620
most of the text on the open internet

02:44.640 --> 02:46.800
and it can talk to you about anything you want.

02:48.580 --> 02:49.720
Right.

02:49.740 --> 02:51.500
So like a person then,

02:52.000 --> 02:53.240
I was stumped,

02:54.960 --> 02:57.260
genuinely left scratching my head.

02:58.340 --> 03:02.120
All my boring stock answers came rushing through my mind.

03:02.620 --> 03:05.480
No, but AI is just another general-purpose technology,

03:05.500 --> 03:06.980
like printing or steam.

03:07.320 --> 03:09.780
It'll be a tool that will augment us

03:09.800 --> 03:12.080
and make us smarter and more productive.

03:12.580 --> 03:14.480
And when it gets better over time,

03:14.500 --> 03:16.440
it'll be like an all-knowing oracle

03:16.460 --> 03:19.560
that will help us solve grand scientific challenges.

03:19.860 --> 03:23.260
You know, all of these responses started to feel, I guess,

03:23.280 --> 03:25.040
a little bit defensive

03:26.040 --> 03:28.320
and actually better suited to a policy seminar

03:28.340 --> 03:30.480
than breakfast with a no-nonsense six-year-old.

03:30.500 --> 03:33.820
Why am I hesitating, I thought to myself?

03:35.660 --> 03:37.100
You know, let's be honest.

03:37.600 --> 03:40.600
My nephew was asking me a simple question

03:40.620 --> 03:44.580
that those of us in AI just don't confront often enough.

03:46.060 --> 03:48.520
What is it that we are not familiar with

03:48.540 --> 03:51.180
and what is it that we are actually creating?

03:52.040 --> 03:55.040
What does it mean to make something totally new,

03:55.820 --> 03:59.640
fundamentally different to any invention that we have known before?

04:00.800 --> 04:03.580
It is clear that we are at an inflection point

04:03.600 --> 04:05.320
in the history of humanity.

04:06.580 --> 04:08.500
On our current trajectory,

04:08.520 --> 04:10.540
we're headed towards the emergence of something

04:10.560 --> 04:13.000
that we are all struggling to describe.

04:13.960 --> 04:18.420
And yet, we cannot control what we don't understand.

04:19.520 --> 04:23.760
And so, the metaphors, the mental models, the names,

04:23.780 --> 04:26.900
these all matter if we are to get the most out of AI

04:26.920 --> 04:29.080
whilst limiting its potential downsides.

04:30.320 --> 04:33.720
As someone who embraces the possibilities of this technology,

04:33.740 --> 04:37.420
but who's also always cared deeply about its ethics,

04:37.440 --> 04:41.780
we should, I think, be able to easily describe what it is we are building,

04:41.800 --> 04:44.160
and that includes the six-year-olds.

04:44.180 --> 04:46.640
So it's in that spirit that I offer up today

04:46.780 --> 04:48.620
the following metaphor

04:48.640 --> 04:52.000
for helping us to try to grapple with what this moment really is.

04:52.620 --> 04:55.240
I think AI should best be understood

04:55.260 --> 04:59.280
as something like a new digital species.

05:00.360 --> 05:02.620
Now, don't take this too literally,

05:02.640 --> 05:07.320
but I predict that we'll come to see them as digital companions,

05:07.340 --> 05:10.600
new partners in the journeys of all our lives.

05:10.620 --> 05:14.720
Whether you think we're on a 10-, 20- or 30-year path here,

05:14.740 --> 05:17.280
this is, in my view, the most accurate

05:17.300 --> 05:22.640
and most fundamentally honest way of describing what's actually coming.

05:22.660 --> 05:26.640
And above all, it enables everybody to prepare for

05:26.660 --> 05:29.060
and shape what comes next.

05:29.920 --> 05:31.840
Now, I totally get this as a strong claim,

05:31.860 --> 05:36.320
and I'm going to explain to everyone as best I can why I'm making it.

05:36.340 --> 05:39.300
But first, let me just try to set the context.

05:39.320 --> 05:42.080
From the very first microscopic organisms,

05:42.100 --> 05:45.580
life on Earth stretches back billions of years.

05:45.600 --> 05:49.820
Over that time, life evolved and diversified.

05:49.840 --> 05:53.380
Then a few million years ago, something began to shift.

05:54.260 --> 05:57.880
After countless cycles of growth and adaptation,

05:57.900 --> 06:01.900
one of life's branches began using tools,

06:01.920 --> 06:04.840
and that branch grew into us.

06:04.860 --> 06:08.980
We went on to produce a mesmerizing variety of tools,

06:08.980 --> 06:12.660
at first slowly, and then with astonishing speed,

06:12.680 --> 06:16.320
we went from stone axes and fire

06:16.340 --> 06:21.080
to language, writing, and eventually, industrial technologies.

06:21.940 --> 06:25.240
One invention unleashed a thousand more,

06:25.260 --> 06:28.800
and in time, we became Homo Technologicus.

06:29.660 --> 06:34.000
Around 80 years ago, another new branch of technology began.

06:34.020 --> 06:35.780
With the invention of computers,

06:35.800 --> 06:39.120
we quickly jumped from the first mainframes and transistors

06:39.140 --> 06:42.620
to today's smartphones and virtual reality headsets.

06:42.640 --> 06:47.660
Information, knowledge, communication, computation.

06:47.680 --> 06:52.280
In this revolution, creation has exploded like never before.

06:53.320 --> 06:57.940
And now a new wave is upon us, artificial intelligence.

06:57.960 --> 07:00.560
These waves of history are clearly speeding up,

07:00.580 --> 07:05.180
as each one is amplified and accelerated by the last.

07:05.200 --> 07:06.320
And if you look back,

07:06.340 --> 07:10.900
it's clear that we are in the fastest and most consequential wave ever.

07:11.940 --> 07:16.580
The journeys of humanity and technology are now deeply intertwined.

07:16.600 --> 07:18.160
In just 18 months,

07:18.180 --> 07:21.820
over a billion people have used large language models.

07:21.840 --> 07:25.760
We've witnessed one landmark event after another.

07:25.780 --> 07:29.220
Just a few years ago, people said that AI would never be creative.

07:30.220 --> 07:34.280
And yet, AI now feels like an endless river of creativity,

07:34.300 --> 07:37.240
making poetry and images and music and video

07:37.260 --> 07:38.900
that stretch the imagination.

07:39.780 --> 07:42.500
People said it would never be empathetic.

07:42.520 --> 07:47.840
And yet today, millions of people enjoy meaningful conversations with AIs,

07:47.860 --> 07:49.840
talking about their hopes and dreams

07:49.860 --> 07:53.300
and helping them work through difficult emotional challenges.

07:53.320 --> 07:55.580
AIs can now drive cars,

07:55.600 --> 07:59.420
manage energy grids and even invent new molecules.

07:59.440 --> 08:03.000
Just a few years ago, each of these was impossible.

08:03.860 --> 08:05.980
And all of this is turbocharged

08:06.000 --> 08:11.040
by spiralling exponentials of data and computation.

08:11.060 --> 08:16.340
Last year, Inflation 2.5, our last model,

08:16.360 --> 08:20.480
used five billion times more computation

08:20.500 --> 08:24.120
than the DeepMind AI that beat the old-school Atari games

08:24.140 --> 08:26.180
just over 10 years ago.

08:26.200 --> 08:30.180
That's nine orders of magnitude more computation.

08:30.180 --> 08:35.060
10x per year, every year, for almost a decade.

08:35.080 --> 08:37.920
Over the same time, the size of these models has grown,

08:37.940 --> 08:41.880
from first tens of millions of parameters to then billions of parameters,

08:41.900 --> 08:45.360
and very soon tens of trillions of parameters.

08:45.380 --> 08:50.140
If someone did nothing but read 24 hours a day for their entire life,

08:50.160 --> 08:53.520
they'd consume eight billion words.

08:53.540 --> 08:55.400
And of course, that's a lot of words.

08:55.420 --> 09:01.200
But today, the most advanced AIs consume more than eight trillion words

09:01.220 --> 09:03.920
in a single month of training.

09:03.940 --> 09:05.920
And all of this is set to continue.

09:05.940 --> 09:09.060
The long arc of technological history

09:09.080 --> 09:12.320
is now in an extraordinary new phase.

09:12.340 --> 09:15.480
So what does this mean in practice?

09:15.500 --> 09:18.400
Well, just as the internet gave us the browser

09:18.420 --> 09:20.940
and the smartphone gave us apps,

09:20.960 --> 09:22.960
the cloud-based supercomputer

09:22.960 --> 09:27.500
is ushering in a new era of ubiquitous AIs.

09:27.520 --> 09:32.080
Everything will soon be represented by a conversational interface,

09:32.100 --> 09:34.820
or to put it another way, a personal AI.

09:35.820 --> 09:38.180
And these AIs will be infinitely knowledgeable,

09:38.200 --> 09:42.100
and soon they'll be factually accurate and reliable.

09:42.120 --> 09:44.000
They'll have near-perfect IQ.

09:44.980 --> 09:47.360
They'll also have exceptional EQ.

09:47.380 --> 09:51.320
They'll be kind, supportive, empathetic,

09:52.020 --> 09:53.980
and a kind of personalised tutor.

09:56.000 --> 09:58.700
These elements on their own would be transformational.

09:58.720 --> 10:02.380
Just imagine if everybody had a personalised tutor in their pocket

10:02.400 --> 10:04.900
and access to low-cost medical advice.

10:04.920 --> 10:08.820
A lawyer and a doctor, a business strategist and coach,

10:08.840 --> 10:11.260
all in your pocket 24 hours a day.

10:11.280 --> 10:13.760
But things really start to change

10:13.780 --> 10:18.120
when they develop what I call AQ, their actions quotient.

10:18.140 --> 10:21.000
This is their ability to actually get stuff done

10:21.000 --> 10:24.440
and, before long, it won't just be people that have AIs.

10:24.460 --> 10:26.120
Strangers, it may sound.

10:26.140 --> 10:30.660
Every organisation, from small business to nonprofit to national government,

10:30.680 --> 10:32.120
each will have their own.

10:32.900 --> 10:35.620
Every town, building and object

10:35.640 --> 10:39.360
will be represented by a unique interactive persona.

10:39.380 --> 10:42.300
And these won't just be mechanistic assistants.

10:42.320 --> 10:44.300
They'll be companions,

10:44.320 --> 10:46.120
confidants,

10:46.140 --> 10:47.500
colleagues,

10:47.540 --> 10:51.400
friends and partners, as varied and unique as we all are.

10:52.400 --> 10:53.560
At this point,

10:53.580 --> 10:56.880
AIs will convincingly imitate humans at most tasks.

10:57.840 --> 11:01.040
And we'll fill this at the most intimate of scales,

11:01.060 --> 11:04.700
an AI organising a community get-together for an elderly neighbour,

11:04.720 --> 11:06.240
a sympathetic expert

11:06.260 --> 11:09.320
helping you make sense of a difficult diagnosis.

11:09.340 --> 11:12.120
But we'll also feel it at the largest scales,

11:12.140 --> 11:14.700
accelerating scientific discovery,

11:14.720 --> 11:16.900
autonomous cars on the roads,

11:16.920 --> 11:19.080
drones in the skies.

11:19.100 --> 11:22.200
They'll both order the takeout and run the power station.

11:23.060 --> 11:26.440
They'll interact with us and, of course, with each other.

11:26.460 --> 11:28.380
They'll speak every language,

11:28.400 --> 11:31.240
take in every pattern of sensor data,

11:31.260 --> 11:33.580
sights, sounds,

11:33.600 --> 11:36.000
streams and streams of information,

11:36.020 --> 11:40.000
faster passing what any one of us could consume in a thousand lifetime.

11:40.880 --> 11:43.080
So what is this?

11:43.100 --> 11:44.860
What are these AIs?

11:45.860 --> 11:50.360
If we are to prioritise safety above all else,

11:50.380 --> 11:55.360
to ensure that this new wave always serves and amplifies humanity,

11:55.380 --> 12:00.800
then we need to find the right metaphors for what this might become.

12:00.820 --> 12:05.580
For years, we in the AI community and I specifically

12:05.600 --> 12:09.800
have had a tendency to refer to this as just tools.

12:09.820 --> 12:13.720
But that doesn't really capture what's actually happening here.

12:14.720 --> 12:17.260
AIs are clearly more dynamic,

12:17.280 --> 12:18.900
more ambiguous,

12:18.920 --> 12:22.640
more integrated and more emergent than mere tools,

12:22.660 --> 12:25.560
which are entirely subject to human control.

12:25.580 --> 12:28.100
So to contain this wave,

12:28.120 --> 12:31.180
to put human agency at its centre,

12:31.200 --> 12:33.900
and to mitigate the inevitable unintended consequences

12:33.920 --> 12:35.780
that are likely to arise,

12:35.800 --> 12:37.500
we should start to think about them

12:37.520 --> 12:41.140
as we might a new kind of digital species.

12:41.160 --> 12:42.940
Now, it's just an analogy.

12:43.020 --> 12:46.080
It's not a literal description, and it's not perfect.

12:46.100 --> 12:50.180
For a start, they clearly aren't biological in any traditional sense,

12:50.200 --> 12:52.580
but just pause for a moment

12:52.600 --> 12:55.580
and really think about what they already do.

12:55.600 --> 12:58.120
They communicate in our languages.

12:58.140 --> 13:00.320
They see what we see.

13:00.340 --> 13:03.720
They consume unimaginably large amounts of information.

13:04.860 --> 13:07.060
They have memory.

13:07.080 --> 13:08.640
They have personality.

13:09.620 --> 13:11.020
They have creativity.

13:12.020 --> 13:16.700
They can even reason to some extent and formulate rudimentary plans.

13:16.720 --> 13:20.520
They can act autonomously if we allow them.

13:20.540 --> 13:22.840
And they do all this at levels of sophistication

13:22.860 --> 13:26.420
that is far beyond anything that we've ever known from a mere tool.

13:27.740 --> 13:32.080
And so saying AI is mainly about the math or the code

13:32.100 --> 13:36.680
is like saying we humans are mainly about carbon and water.

13:37.900 --> 13:39.240
It's true,

13:39.320 --> 13:41.640
but it completely misses the point.

13:42.920 --> 13:46.540
And yes, I get it, this is a super-arresting thought,

13:46.560 --> 13:48.460
but I honestly think this frame

13:48.480 --> 13:51.760
helps sharpen our focus on the critical issues.

13:52.820 --> 13:54.260
What are the risks?

13:55.880 --> 13:58.360
What are the boundaries that we need to impose?

13:59.360 --> 14:03.520
What kind of AI do we want to build or allow to be built?

14:04.560 --> 14:06.960
This is a story that's still unfolding.

14:06.980 --> 14:09.080
Nothing should be accepted as a given.

14:09.480 --> 14:12.720
We all must choose what we create,

14:12.740 --> 14:14.760
what AIs we bring into the world,

14:15.760 --> 14:16.760
or not.

14:18.680 --> 14:21.480
These are the questions for all of us here today

14:21.500 --> 14:23.800
and all of us alive at this moment.

14:24.760 --> 14:29.000
For me, the benefits of this technology are stunningly obvious

14:29.020 --> 14:32.320
and they inspire my life's work every single day.

14:33.720 --> 14:36.560
But quite frankly, they'll speak for themselves.

14:37.560 --> 14:40.560
Over the years, I've never shied away from highlighting risks

14:40.580 --> 14:42.080
and talking about downsides.

14:43.000 --> 14:46.160
Thinking in this way helps us focus on the huge challenges

14:46.180 --> 14:47.640
that lie ahead for all of us.

14:48.680 --> 14:49.840
But let's be clear.

14:50.560 --> 14:54.480
There is no path to progress where we leave technology behind.

14:55.480 --> 14:59.120
The prize for all of civilization is immense.

14:59.880 --> 15:03.760
We need solutions in health care and education to our climate crisis,

15:04.000 --> 15:07.960
and if AI delivers just a fraction of its potential,

15:07.980 --> 15:12.000
the next decade is going to be the most productive in human history.

15:13.760 --> 15:15.800
Here's another way to think about it.

15:15.820 --> 15:20.760
In the past, unlocking economic growth often came with huge downsides.

15:21.560 --> 15:25.300
The economy expanded as people discovered new continents

15:25.320 --> 15:27.000
and opened up new frontiers.

15:28.720 --> 15:31.400
But they colonized populations at the same time.

15:32.400 --> 15:36.720
We built factories, but they were grim and dangerous places to work.

15:37.880 --> 15:41.040
We struck oil, but we polluted the planet.

15:41.960 --> 15:45.160
Now, because we are still designing and building AI,

15:45.200 --> 15:50.040
we have the potential and opportunity to do it better, radically better.

15:50.960 --> 15:53.560
And today, we're not discovering a new continent

15:53.600 --> 15:55.280
and plundering its resources.

15:56.120 --> 15:57.680
We're building one from scratch.

15:58.680 --> 16:02.880
Sometimes people say that data or chips are the 21st century's new oil.

16:04.000 --> 16:05.560
But that's totally the wrong image.

16:06.600 --> 16:11.000
AI is to the mind what nuclear fusion is to energy.

16:12.320 --> 16:16.040
Limitless, abundant, world-changing.

16:17.400 --> 16:19.680
And AI really is different.

16:20.400 --> 16:24.640
That means we have to think about it creatively and honestly.

16:24.720 --> 16:29.120
We have to push our analogies and our metaphors to the very limits

16:29.160 --> 16:31.400
to be able to grapple with what's coming,

16:31.440 --> 16:33.800
because this is not just another invention.

16:35.000 --> 16:38.240
AI is itself an infinite inventor.

16:38.960 --> 16:42.240
And yes, this is exciting and promising and concerning

16:42.280 --> 16:44.000
and intriguing all at once.

16:45.360 --> 16:47.160
To be quite honest, it's pretty surreal.

16:47.920 --> 16:52.720
But step back, see it on the long view of glacial time,

16:52.720 --> 16:57.520
and these really are the very most appropriate metaphors that we have today.

16:57.560 --> 16:59.400
Since the beginning of life on Earth,

17:00.560 --> 17:03.560
we've been evolving, changing,

17:03.600 --> 17:07.320
and then creating everything around us in our human world today.

17:08.320 --> 17:11.000
And AI isn't something outside of this story.

17:11.680 --> 17:14.320
In fact, it's the very opposite.

17:15.240 --> 17:18.440
It's the whole of everything that we have created

17:18.440 --> 17:23.040
distilled down into something that we can all interact with and benefit from.

17:23.960 --> 17:26.600
It's a reflection of humanity across time.

17:27.480 --> 17:30.520
And in this sense, it isn't a new species at all.

17:31.320 --> 17:33.000
This is where the metaphors end.

17:33.680 --> 17:36.040
Here's what Al-Talqaspian next time he asks.

17:37.320 --> 17:38.640
AI isn't separate.

17:39.480 --> 17:42.400
AI isn't even, in some senses, new.

17:43.600 --> 17:44.720
AI is us.

17:45.480 --> 17:46.480
It's all of us.

17:47.120 --> 17:50.920
And this is perhaps the most promising and vital thing of all

17:50.960 --> 17:53.160
that even a six-year-old can get a sense for.

17:54.640 --> 17:59.600
As we build out AI, we can and must reflect all that is good,

17:59.640 --> 18:03.560
all that we love, all that is special about humanity,

18:03.600 --> 18:07.840
our empathy, our kindness, our curiosity and our creativity.

18:09.440 --> 18:14.480
This, I would argue, is the greatest challenge of the 21st century,

18:14.520 --> 18:19.840
but also the most wonderful, inspiring and hopeful opportunity for all of us.

18:20.560 --> 18:21.560
Thank you.

18:21.600 --> 18:23.080
APPLAUSE

18:26.680 --> 18:28.080
Thank you. Thank you, Moustap.

18:28.120 --> 18:31.680
It's an amazing vision and a super powerful metaphor.

18:32.240 --> 18:34.000
You're in an amazing position right now.

18:34.040 --> 18:38.080
You were connected at the hip to the amazing work happening at OpenAI.

18:38.760 --> 18:40.760
You're going to have resources made available.

18:40.760 --> 18:44.960
There are reports of these giant new data centers,

18:45.000 --> 18:47.000
$100 billion invested and so forth.

18:48.200 --> 18:51.720
And a new species can emerge from it.

18:52.400 --> 18:54.280
I mean, you've done, in your book, you did,

18:54.320 --> 18:56.360
as well as painting an incredible optimistic vision.

18:56.400 --> 19:00.520
You were super eloquent on the dangers of AI.

19:00.560 --> 19:04.520
And I'm just curious where, from the view that you have now,

19:04.560 --> 19:06.680
what is it that most keeps you up at night?

19:07.200 --> 19:09.920
I think the great risk is that we get stuck in what I call

19:09.920 --> 19:11.560
the pessimism aversion trap.

19:11.600 --> 19:16.520
You know, we have to have the courage to confront the potential of dark scenarios

19:16.560 --> 19:19.640
in order to get the most out of all the benefits that we see, right?

19:19.680 --> 19:23.400
So the good news is that if you look at the last two or three years,

19:23.440 --> 19:26.360
there have been very, very few downsides, right?

19:26.400 --> 19:31.280
It's very hard to say explicitly what harm an LLM has caused.

19:31.320 --> 19:33.920
But that doesn't mean that that's what the trajectory is going to be

19:33.960 --> 19:35.120
over the next 10 years.

19:35.160 --> 19:39.240
So I think if you pay attention to a few specific capabilities,

19:39.280 --> 19:41.120
take, for example, autonomy.

19:41.160 --> 19:46.360
Autonomy is very obviously a threshold over which we increase risk in our society,

19:46.400 --> 19:49.280
and it's something that we should step towards very, very closely.

19:49.320 --> 19:52.920
The other would be something like recursive self-improvement.

19:52.960 --> 19:56.560
If you allow the model to independently self-improve,

19:56.600 --> 20:00.680
update its own code, explore an environment without oversight,

20:00.720 --> 20:04.760
and without a human in control to change how it operates,

20:04.800 --> 20:06.240
that would obviously be more dangerous.

20:06.280 --> 20:08.680
But I think that we're still some way away from that.

20:08.680 --> 20:12.120
I think it's still a good five to 10 years before we have to really confront that,

20:12.160 --> 20:13.960
but it's time to start talking about it now.

20:14.000 --> 20:17.000
A digital species, unlike any biological species,

20:17.040 --> 20:21.360
can replicate not in nine months, but in nine nanoseconds

20:21.400 --> 20:24.840
and produce an indefinite number of copies of itself,

20:24.880 --> 20:28.720
all of which have more power than we have in many ways.

20:28.760 --> 20:33.560
I mean, the possibility for unintended consequences seems pretty immense,

20:33.600 --> 20:37.680
and isn't it true that if a problem happens, it could happen in an hour?

20:37.720 --> 20:40.800
No, that is really not true.

20:40.840 --> 20:42.840
I think there's no evidence to suggest that,

20:42.880 --> 20:47.400
and I think that's often referred to as the intelligence explosion,

20:47.440 --> 20:51.120
and I think it is a theoretical, hypothetical maybe

20:51.160 --> 20:53.560
that we're all kind of curious to explore,

20:53.600 --> 20:56.520
but there's no evidence that we're anywhere near anything like that.

20:56.560 --> 20:59.320
And I think it's very important that we choose our words super carefully,

20:59.360 --> 21:03.840
because you're right, that's one of the weaknesses of the species framing.

21:03.920 --> 21:08.240
We will design the capability for self-replication into it

21:08.280 --> 21:11.920
if people choose to do that, and I would actually argue that we should not.

21:11.960 --> 21:14.240
That would be one of the dangerous capabilities

21:14.280 --> 21:16.000
that we should step back from.

21:16.040 --> 21:20.000
So there's no chance that this will emerge accidentally.

21:20.040 --> 21:22.560
I really think that's a very low probability.

21:22.600 --> 21:26.920
It will happen if engineers deliberately design those capabilities in,

21:26.960 --> 21:29.880
and if they don't take enough efforts to deliberately design them out.

21:29.920 --> 21:33.280
And so this is the point of being explicit and transparent

21:33.320 --> 21:37.920
about trying to introduce safety by design very early on.

21:39.160 --> 21:40.160
So thank you.

21:40.200 --> 21:45.120
I mean, your vision of humanity injecting into this new thing

21:45.160 --> 21:47.000
the best parts of ourselves,

21:47.040 --> 21:50.760
avoiding all those weird biological, freaky, horrible tendencies

21:50.800 --> 21:52.440
that we can have in certain circumstances.

21:52.480 --> 21:55.040
I mean, that is a very inspiring vision,

21:55.080 --> 21:58.400
and thank you so much for coming here and sharing it at TED.

21:58.440 --> 21:59.440
Thank you, good luck.

21:59.480 --> 22:00.480
Thanks a lot.

22:00.520 --> 22:01.520
I appreciate it.

