start	end	text
0	8760	Aaron, you're an ML PhD student at Georgia Tech, a research intent at Google, and a researcher
8760	11560	at OPAI.
11560	17080	At Eleuther, you've been working on a DAWI reproduction, which later became Lyon, and
17080	26080	you proposed GPTJ, a JAX-based GPT3-like model whose performance is on par with GPT3 6 billion
26080	29960	on various downstreaming tasks.
29960	35320	You're also well-known for being one of the two AKs where the legend says that if a deep
35320	41120	learning paper is important, one of the two AKs will have tweeted about it.
41120	46240	Your name was mentioned before on the podcast as one of the persons who have convinced Ethan
46240	50760	Caballero that scaling will turn out to be important.
50760	52840	Thanks Aaron for coming on the show.
52840	55880	Thanks for having me today.
55880	59080	So let's start with the legend of the two AKs.
59160	65000	Who is this other AK and why do people say that following the two of you is enough to
65000	67680	understand deep learning research?
67680	75320	Yeah, basically we scan all the archive papers, archive machine learning papers every day,
75320	80440	and we tweet about them with some summary and visualization.
80440	87680	And yeah, we've been doing it for several years and we became, we got more and more followers.
87680	98400	And that's probably why everyone follows us to get the grasp of the latest research.
98400	103480	How do you manage to read so many papers, writing all of the summaries on Twitter?
103480	106840	What's your process on a daily basis?
106840	116960	So I basically check archive CSP at around 6 p.m. every day and I scan all the titles
116960	122520	and if the title interests me, then I read the abstract.
122520	128360	Then if I'm still interested, then I would scan their tables and figures.
128360	136360	And if I think the paper is promising, then I would tweet the paper with short summary
136360	137920	and visualization.
137920	145960	He tweets much, many more than I do because he tries to tweet as many good papers as possible.
145960	150440	Well, I try to tweet the ones I think the best.
150440	154560	So yeah, he spends much more time on it than I do.
154560	161400	And yeah, I think that's amazing.
161400	167960	Yeah, so how much time do you spend every day, would you say?
167960	169480	About up to 30 minutes.
169480	171920	I wouldn't spend more than that.
171920	174560	You know, I'm a researcher.
174560	180360	So whether I tweeted about it or not, I have to scan all the archive papers anyway.
180360	184560	So I think this time investment is totally worth it.
184560	185560	Okay.
185560	192560	So yeah, how many abstracts do you end up reading every day, would you say like 10, 50, 100?
192560	195680	Yeah, it depends on the day.
195680	199120	Sometimes it's very, the field is very productive.
199120	206640	Like yesterday, I read like 10 abstracts, but I usually read only like three abstracts.
206640	213720	Yeah, I have probably because I have very specific tastes for papers.
213720	218680	I'm more biased towards NLP papers, for example.
218680	225480	Yeah, to go back to the reason why you're here today, you were tweeting as every day
225480	233520	and one of the tweets wrote was an answer to my AGI political compass, the latest one.
233520	239240	And you were saying that you were not on the compass because you were higher than some
239240	242400	ultimate or something more extreme than his position.
242400	248120	I know this is a joke and I'm sorry to bring something from Twitter on a podcast, but maybe
248120	253080	could you summarize your take for, you know, the listeners that are not on Twitter every
253080	254080	day?
254680	260360	Oh yeah, basically what I say is something like, so he said, is that I'm not worried
260360	262680	about AI alignment problems.
262680	269680	And I think I say, I don't consider this task of alignment to be any different from any
269680	272240	other half ML task.
272240	279000	But as we talked about it, I think I am wrong about this.
279000	281760	Yeah, so we can talk about it later.
281840	289160	And I think I also say we should focus on that long term, which we agree, I guess, and
289160	290840	scaling is important.
290840	294760	But unlike Ethan, I don't think it's all you need.
294760	302600	And I don't know about AGI, but I think superhuman level, recursively self-improving
302600	309920	language model, maybe possible somewhere around 2028 to 2038.
309920	310920	Yeah.
310920	315080	And I think there's a lot of disagreement on Twitter between focusing on the long term
315080	316920	or focusing on the near term.
316920	323040	And I think there's a bunch of confusion because for people who have short AI timelines, their
323040	325200	long term is kind of the near term.
325200	329840	So for you like 2028, 2038, it's kind of the long term future, right?
329840	334200	Because things are going to be a lot of different then.
334200	337720	So and yeah, people are also confused about definitions.
337720	344360	So AGI, artificial intelligence, or human level, or superpower intelligence.
344360	349800	So maybe just to ground the discussion a bit, could you give your definition of AGI?
349800	354320	So what's the definition you think about when you see that word on Twitter?
354320	355320	Yeah.
355320	362760	So I'm really interested in AGI, but to be honest, I'm not an expert on it.
362760	367480	So I'm just, I'm trying to learn about it.
367480	371440	So I don't know if my terminology or definition is correct.
371440	378120	But to me, AGI is something like a model that can recursively improve itself and can perform
378120	384840	any task, at least as well as humans do.
384840	389600	Personally, I'm, oh yeah, go ahead, sir.
389600	396760	I was just going to say that, you know, so if we think about like human level, not all
396840	401360	humans are capable of writing ML code or thinking about AI.
401360	406480	If you just take someone, some average human on the street, they will not be able to improve
406480	409520	an AI or self-improve.
409520	415040	So yeah, I think one of the definitions, sorry.
415040	422440	Yeah, one of the reasons people give for why an AI would be able to self-improve if it was
422560	430200	human level is that a human given like enough time and memory could be able to like read
430200	437200	all those archive papers and, you know, come up with another solution, assuming like our,
437200	443760	you know, like there's not like a problem in the architecture of humans that, you know,
443760	448760	would make it impossible to like improve or something.
448760	453360	So yeah, I guess that's kind of one of the reasons people might like not make a distinction
453360	460360	between those two, like human level and, you know, recursively self-improvement.
460360	465760	But yeah, I think that's like a reasonable guess to like put those very close.
465760	466760	Yeah.
466760	475760	So when I say human level, just like many other people, I think I'm saying that as well
476760	482760	doing each task as well as top human experts, so not like other humans.
482760	487760	You know, other humans, like you said, you cannot really write machine learning papers.
487760	492760	I don't remember whether you said that or yeah, but that's what I mean.
492760	497760	So yeah, that's basically what I meant.
497760	507760	So in particular, I'm interested in AGI's capability to do research because that's basically doing
507760	516760	doing research is basically or doing ML research is basically recursive self-improvement.
516760	526760	And also, you know, it can advance other areas of science or yeah.
526760	535760	So I think so as long as a machine learning model can do machine learning research as well as humans do,
535760	542760	I think that's that leads to AGI eventually without any human intervention.
542760	549760	So a human level machine learning research is AGI complete in some sense.
549760	555760	And I'm trying to make language model to machine learning research.
555760	560760	Yeah, I think that's a valid path to AGI, even though there are many other path.
560760	563760	Yeah, just to clarify the definitions.
563760	569760	When people say AGI complete, they usually mean you need AGI to reach that point.
569760	577760	What you're saying is doing ML research enables human level AI, right, or AGI.
577760	581760	So you're more saying like it implies, right?
581760	588760	It's being good at ML research and being able to do as much ML research as you want implies AGI.
588760	590760	Is that what you're saying?
590760	591760	Yeah.
591760	592760	Okay.
592760	601760	And I think so I agree with some version of that, but then it depends on what you consider to be ML research.
601760	620760	So for instance, is building like ML hardware and transistors and building factories that produce electricity that would power those ML hardware.
620760	622760	Is that all ML research?
622760	624760	Why do you draw the line?
624760	625760	Yeah.
625760	636760	For a human level machine learning research, I mean, to be able to replace pretty much every ML research staff, ML research project.
636760	640760	So including software.
640760	643760	Yeah.
643760	645760	That's what I meant.
645760	647760	Okay.
647760	652760	Yeah, I guess my take is that, you know, the economy is very complex.
652760	657760	And you need like energy to do things and robots to build things.
657760	676760	And so the kind of the task that would, so more like the intelligence level to be able to reproduce like all that branch of the economy that builds computers is kind of automating like 10% of what humans are able to do right now.
676760	690760	If you really wanted to like self-improve and build bigger and bigger computers without like having to call a human to do the task for you, then you would need to be pretty close to AGI already.
690760	692760	Yeah, that's true.
692760	711760	Given what we said about recursive self-improvement requiring a hardware and robots building the hardware, would you say like recursive self-improvement will arrive much later than, you know, ML research in archive papers?
711760	718760	You mean being able to write ML papers.
718760	726760	Right, or yeah, being able to write the papers and maybe like a run experiments by yourself without like actually like improving your hardware.
726760	727760	Yeah, I agree.
727760	732760	I think software improvement comes much earlier than hardware improvement.
732760	745760	But I think so, given if you look at the past ML research and scaling, most of the scaling comes from additional funding.
745760	751760	But and some of the scaling comes from hardware improvement.
751760	771760	But I think like a lot of improvement simply comes from software improvement, like improvement in design, like the transition from LSDM to transformer or like scaling, optimal scaling strategy.
771760	779760	So I think there's a lot of things AIs can do simply by improving the software.
779760	786760	And yeah, just to go back to what you said before about scaling is not all you need.
786760	791760	Yeah, why do you say you disagree with Ethan Caballero on that?
791760	793760	Obviously scaling is very important.
793760	803760	And if you look at the result of palm over, well, smaller palm or other models, then you can see that the improvement is huge.
803760	814760	And I think that's a very big indication that we still haven't exhausted all the space for improvement from coming from scaling.
814760	834760	But I don't say scaling is all you need because as I talked about, the improvement coming from performance improvement coming from model design improvement is huge.
835760	844760	The most important one is improvement in scaling strategy or called optimal scaling strategy.
844760	853760	For example, open AIs paper, that brings from 10 to 100 times of speed up.
853760	869760	So basically the model optimally scaled up performs as well as the model suboptimally scaled up using from 10 to 100 times of a compute.
869760	879760	This result was demonstrated, but technically this is possible.
879760	890760	So yeah, and also, you know, the improvement coming from VQVA, I mean, Dali 1 to Dali 2.
890760	900760	I think this huge leak is not possible simply by increasing the computational budget spent on Dali 1.
900760	912760	So this is probably because I think this is because there is a fundamental bottleneck with the design of VQVA or Dali 1.
912760	922760	So there are some of the things that you cannot simply overcome with additional by adding more copies.
923760	931760	Yeah, so what was the paper you mentioned where you go from a 10x improvement to a 100x?
931760	933760	Like you said, the open AIs paper?
933760	936760	Yeah, scaling goes for neural laggis models.
936760	942760	So that's like a way of scaling your models optimally.
942760	950760	And what you're saying is that you cannot just scale without thinking about the optimal scaling.
950760	953760	Is that what you're basically saying?
953760	963760	Yeah, basically not using this kind of principle way of scaling is a common practice before this paper.
963760	971760	But after this, basically most many of the big projects tried to follow this scaling now,
971760	982760	which I think is very important for saving the copies or maximizing the performance.
982760	993760	And yeah, one of the things that we mentioned in the episode with Ethan was that you kind of influence him to be more interested in scaling
993760	997760	because at the beginning it was not that much into scaling,
997760	1003760	but then maybe like a few years ago you told him that scaling was going to be huge.
1003760	1007760	So yeah, how did you get interested in scaling?
1007760	1012760	Yeah, what's here or what was the kind of thing that got you into it?
1012760	1015760	Yeah, so I first got into machine.
1015760	1021760	So I started reading machine learning papers on the summer of 2017.
1021760	1031760	Then Transformer paper was released and soon got into language modeling because I thought that would be the key for AGI.
1031760	1040760	Then yeah, I was almost immediately and talked about several months to be convinced of Transformer language model,
1040760	1044760	replacing all the LSDMs in every applications.
1044760	1055760	Then I read a paper titled Exploring the Limits of Language Modeling, which was released in 2016.
1055760	1064760	So this paper basically tries to scale up the size of LSDM and dataset size to improve that publicity.
1064760	1073760	And I did their generated text and it doesn't look so much better than any of the text I saw and their publicity is so much better.
1073760	1079760	So it's kind of like a GBT2 moment for me except it was like 2017.
1079760	1092760	So that's basically the first scaling thing I saw and there was also another paper titled Deep Learning Scaling is Predictable and Predictable.
1092760	1097760	I think Ethan mentioned that it was released in 2017 too.
1097760	1106760	It shows that there's a parallel between training cards, performance, model size and dataset size.
1106760	1111760	But that doesn't really tell you exactly how to scale up the models.
1111760	1119760	Then finally there's GPT2, which was in January of 2019.
1119760	1126760	By then I was already convinced that scaling is going to be the key.
1126760	1145760	Yeah, if you were convinced of scaling before Transformer or in 2017, then seeing GPT2 in beginning of 2019 must be enough or not a big update from 2017.
1145760	1155760	Or at least they showed that you could get generation of paragraphs and a bunch of benchmarks in NLP with just scaling.
1155760	1161760	So yeah, did it surprise you a little bit or were you not surprised at all from GPT2?
1161760	1170760	No, I was not surprised with GPT3 at all because I was kind of working on a similar project with small scale.
1170760	1180760	So like before GPT2, we language model researchers were working on better sampling techniques.
1180760	1184760	So GPT2 used, I think, top-K sampling and temperature.
1184760	1195760	So basically these two were rediscovered in 2018 and I was just playing with these new sampling methods.
1195760	1204760	And so the generated text was much better than anything I saw before.
1204760	1213760	So yeah, I was not surprised with the result at all, but I was very happy.
1213760	1217760	Do you want to tell us more about the project you were working on?
1217760	1224760	Oh, you mean the project I was working on in 2019?
1224760	1233760	Yeah, so you said it was a project that involved top-K sampling or other methods that were not the same as GPT2?
1233760	1238760	Oh yeah, it was not like a big project.
1238760	1251760	So I was just trying to use top-K sampling and temperature sampling as a small transformer language model I was trying in 2018.
1252760	1256760	Then the result was so much better.
1256760	1264760	So I guess that's all I can tell, but there's a project I did in 2019 about scaling.
1264760	1268760	Yeah, it's called one equal piece all you need.
1268760	1273760	So, okay, can I talk about this project?
1273760	1275760	Yeah, sure, go ahead.
1275760	1288760	So basically, okay, so nowadays we're trying a huge model on huge data set for one or a few epochs,
1288760	1298760	but back then we were training smaller models for many, many iterations with very, very small data set.
1298760	1304760	Even GPT2, GPT2 used 100 epochs, I think.
1304760	1305760	So bad.
1305760	1307760	Yeah, exactly.
1307760	1311760	The data set wasn't that big, like only 40 gigabytes.
1311760	1314760	Well, it was huge back then.
1314760	1324760	Yeah, so the model size was only one billion, even though AI can totally spend more money on that.
1324760	1341760	Okay, so basically this shift from old days to today, it was this open AI scaling paper,
1341760	1345760	and scaling laws for neural language model.
1345760	1350760	And we have many other papers like Tintera.
1350760	1366760	But actually, I wrote, I did a project in 2019 where I sort of formed all these nice scaling ideas by myself.
1366760	1371760	So, and I wrote a paper called one equal piece all you need.
1371760	1374760	Yeah, so, okay, let me talk about that.
1374760	1387760	Then, so the, so there I had a bunch of ideas and tried to verify these ideas with experiments using very small amount of computers.
1387760	1397760	And so first idea is that it is easy to analyze the pre-training data set so that one has to train only one or a few epochs.
1397760	1401760	Which dramatically improves the performance compute trade-off.
1401760	1410760	So, basically, yeah, I'm just advocating, let's just try for one epoch and use big data set.
1410760	1425760	Yeah, and the second idea is that so let's compute the optimal ratio of model size and number of tokens for given compute budget based on training curves.
1425760	1433760	So, back then, the models were too small and they used too many iterations.
1433760	1445760	So let's just, you know, adjust this ratio nicely so that we don't have to waste all the compute for like hundreds of epochs.
1445760	1449760	So is the ratio model size and data set size?
1449760	1451760	Yeah, that's right.
1451760	1460760	So, actually, Ginger also did measure this ratio.
1460760	1472760	So basically, they computed the scaling exponents for optimal data set size versus optimal model size.
1472760	1480760	And then they found that the scaling exponents for this to both 0.5.
1480760	1484760	So that means they both linearly increase.
1484760	1493760	So you can just measure the slope of this line, which gives you that optimal ratio.
1493760	1500760	Yeah, so basically, you would need to scale your data set size and model size the same amount.
1500760	1509760	So if you want to, you know, build GPT-4, you might just want to double the number of parameters and double the data set size.
1509760	1511760	Yeah, exactly.
1511760	1518760	And so, yeah, right now, so now we're not on 2019-2020 anymore, we're in 2022.
1518760	1528760	And I believe so you're working at Iluth AI and Google as an intern on some scaling work.
1528760	1533760	And I believe you might not want to talk about your private work on the podcast.
1533760	1538760	But yeah, what kind of work do you do publicly on scaling you won't be happy to talk about?
1538760	1540760	Oh, yeah, definitely.
1540760	1543760	So, okay, let me think.
1543760	1550760	So I'm currently very interested in Instruct GPT and T0.
1550760	1552760	So both of these.
1552760	1556760	Could you maybe summarize what's T0 for people who were not read the paper?
1556760	1557760	Yeah.
1557760	1564760	So T0 is basically a masked language model with multi-task point joining.
1564760	1576760	So first of all, masked language model is an easily encoder on the encoder-decoder model that is trying with a masked language model objective.
1576760	1585760	So this training is basically, so let's say you have some text, then you can randomly mask.
1586760	1591760	As some random spans of tokens.
1591760	1598760	So they, then you want your model to predict these masked tokens.
1598760	1599760	Yeah.
1599760	1605760	That's basically a masked language model or also called denoised audio encoding.
1605760	1606760	Yeah.
1606760	1609760	So that's what masked language model is about.
1609760	1614760	And T0 is a masked language model.
1614760	1626760	That is also fine tune from a bunch of many, many data sets, like maybe like Super Glue.
1626760	1630760	So what's Super Glue?
1630760	1635760	Super Glue is a standard, natural language understanding data set.
1635760	1636760	Yeah.
1636760	1641760	Maybe it has not trained on, maybe they did not use Super Glue for T0.
1641760	1643760	But basically, that's the idea.
1643760	1656760	So the reason why we want to fine tune T0 on a bunch of these data sets is that if you train like this, then it generalises,
1656760	1662760	obviously it performs very well on the task it was fine tuned on.
1662760	1669760	But it also performs very well on the tasks it was not fine tuned on.
1669760	1685760	So we know that GPT like models performs much worse than the GPT like model that is fine tuned on the task you are trying to deal with.
1685760	1686760	Right.
1686760	1701760	So yeah, basically this multi-task fine tuning allows your model to perform very well on the task, not only the tasks it was trained on,
1701760	1705760	but also the tasks it was not trained on.
1705760	1713760	So T0 actually can perform much better than GPT3 on many tasks.
1713760	1726760	Without fine tuning the model on this task specifically, while using like 10 times less complex than GPT3.
1726760	1736760	So is the idea that you train it on a bunch of different tasks, so not fine tuning, but you train it on a bunch of different tasks?
1736760	1740760	I think that's what you said, multi-task training or multi-task fine tuning?
1740760	1742760	Yeah, multi-task.
1742760	1750760	And then it's able to generalize well on held out tasks, it doesn't seem before like zero shots?
1750760	1751760	Exactly.
1751760	1763760	So you said you were interested in T0 and also in Struggle GPT, I believe in Struggle GPT was a model or a training procedure from OpenAI.
1763760	1766760	Can you tell us more about in Struggle GPT?
1766760	1767760	Yeah.
1767760	1782760	So in Struggle GPT is also functioned on many different tasks like T0, but it also uses human feedback.
1782760	1797760	So like basically they train the model to score the bunch of the generated results.
1797760	1813760	And this model tells this GPT3 to how to generate the text.
1813760	1820760	So this process is done by using reinforcement learning called PPO.
1820760	1829760	So this additional component improves GPT3 significant.
1829760	1848760	Yeah, so basically even though GPT3 doesn't perform as well as PPO, given the amount of compute it consumes, it performs very, very well on many different tasks.
1848760	1857760	So without having to, yeah, I think it performs very well even without using future samples.
1857760	1868760	So yeah, GPT3, sorry, Instruct GPT and T0 are some of the most compute efficient models out there.
1868760	1871760	So yeah, I'm very interested in these models.
1871760	1880760	And my project is basically trying to combine all these with scaling.
1880760	1894760	Right, so you kind of want to combine this URL from human feedback procedure from Instruct GPT with the pre-training from T0.
1894760	1903760	Yeah, and I'm thinking that we can do some interesting scaling analysis on this model for several reasons.
1903760	1908760	First of all, optimal scaling we do for GPT-like models.
1908760	1921760	We usually try to optimize the test application, and this model only has only decoded, unlike T0, which has encoded and decoded.
1921760	1932760	By the way, encoded decoded model performs much better than decoded on the model when it is fine-tuned or multi-task fine-tuned.
1932760	1936760	And that's why I'm thinking of this encoded decoded model.
1936760	1946760	And for people who are not into deep learning or NLP, so can you just give an example of decoder and encoder decoder?
1946760	1955760	So I think GPT2 is an decoder because it gets a prompt and then just generates a paragraph.
1955760	1957760	What's an encoder and decoder?
1957760	1981760	Yeah, so encoder decoder is basically encoder is like the model architecture used for Word, or like the first transformer paper architecture, and decoder is our usual decoder.
1981760	1992760	So basically you want to feed your prompt into encoder, and then you would feed the output to the decoder with self-attention.
1992760	2000760	That's what encoder decoder is, and it happens to perform very well in this situation.
2001760	2010760	Yeah, so basically we have encoder decoder and fine-tuning.
2010760	2020760	Yeah, so I think these elements make the scaling load very different from the local GPT models.
2020760	2028760	Oh yeah, and also we want to optimize for downstreaming performance instead of test public HD.
2028760	2056760	So I think these conditions force the model to be bigger and train shorter on free-tuning tasks because fine-tuning is so important that maybe we can make the model bigger while focusing less on the free-tuning.
2056760	2074760	And I think this is, so the current state-of-the-art model has like 100 billion amateurs and trains on trillions of tokens, which is very different from how human planes run.
2074760	2087760	Because our plane has like hundreds of billions of neurons, meaning like hundreds of trillions of synapses.
2087760	2105760	So yeah, I think that means it has more capacity than the former capacity than models do, and it is only trying on like a few billions of tokens because that's how many tokens we can process within our lifetime.
2105760	2114760	So yeah, basically I'm trying to make the models closer to how human planes learn.
2115760	2126760	So you're trying to write a scaling law that would be closer to the amount of data humans process throughout their lifetime?
2127760	2138760	Yeah, and I'm not like trying to make this forcibly similar to human planes constraints.
2138760	2158760	I'm just thinking that this new model based on this G0 instructivity, I think it will perform the best if we try and like how human planes learn.
2159760	2177760	I'm not sure I understand the methods to have this scaling law, so would you need to, you know, train T0 with like some kind of instructivity fine-tuning,
2177760	2194760	and then you test on a bunch of held out tasks, and then you would see like you would plot a curve of like optimal scaling with respect to those like done streaming tasks?
2194760	2213760	So basically what I'm saying is to train a bunch of different T0 with instructivity fine-tuning with different number of tokens pre-trying and different models.
2214760	2232760	I think I understood now, so you're trying to, the same as Ginchilla, like get the exponents for dataset size and for model size, and then you're trying to see if it's not like 0.5 and 0.5, but like maybe something else?
2232760	2234760	Yeah, something like that.
2234760	2257760	I think your most well-known for your scaling work at Eleuther AI, where you train, was it 6 billion parameters, or at least like you try to reproduce the results from GP3, 6.7 billion, and what was called GPTJ, I think for GPTJacks.
2258760	2269760	Yeah, can you just like give us a rough summary of this project, why you started this project, and what do you want to release it to the public?
2269760	2280760	Yeah, so around the beginning of 2020, I was trying to reproduce Dali 1 with some of the people in Eleuther AI.
2280760	2293760	So basically, Dali 1 consists of BQBAE encoder decoder and transformer language model for generating the discrete 11 variables.
2293760	2304760	So that way, this transformer language model is exactly, almost exactly the same as GPT3 except for the size.
2304760	2317760	So I thought we may be able to make the maximum impact if we reuse this model for our GPT3 production.
2317760	2326760	Yeah, and then at the same time, I thought, so Jaxx was becoming more and more popular.
2326760	2344760	Obviously, Jaxx is optimized for TB use, and we had a lot of TB use back then, and Jaxx is, so before that, we had our GPT3 replication, which is called GPT Neo.
2344760	2362760	It was implemented using mesh tensorflow, and this mesh tensorflow decoding speed is so slow, almost ridiculously slow, especially compared with Hytotes.
2362760	2368760	But Jaxx has no problem with that, it's almost as fast as Hytotes.
2368760	2373760	So we decided to use Jaxx for this project.
2373760	2397760	Yeah, so basically, I was supposed to work on the encoder decoder code, and I asked another guy in Eleuther AI, his name is Ben1, and I asked him to work on this language model site.
2397760	2403760	But admittedly, he spent far more time on this project than I did.
2403760	2413760	How much time would you say took you, maybe like six months between the beginning of 2021 and when you guys released the model?
2413760	2434760	I think it only took several months, so this project is kind of impressive because there are only two people in it, and we only spent like three months, and we basically open sourced the best language model.
2434760	2439760	So that is something I'm proud of.
2439760	2442760	You're right to be proud of this, it's pretty cool.
2442760	2444760	Yeah, thank you so much.
2444760	2451760	So, by the way, GPTJ is different.
2451760	2461760	Yeah, so basically GPT Neo actually could not really match the same performance with the GPT3 with similar size.
2461760	2474760	GPT Neo was like 2.7 billion model, but it performed worse than 1 billion GPT3, for example.
2474760	2489760	But GPTJ performs pretty much as well as 6 billion GPT3, so yeah, the model performs very well, I think.
2489760	2498760	So it was able to be more efficient than GPT Neo, which I believe is another model from Eleuther AI, but using TensorFlow Mesh, so a bit older, right?
2498760	2500760	Yeah.
2500760	2515760	And yeah, so to match that performance from GPT3, did you just like took the same API parameters and architecture from GPT3 paper, or did you like had to change stuff to, you know, match the performance?
2515760	2525760	Yeah, so I think we could be using the exact same architecture with GPT3, but we just wanted to do a bit more.
2525760	2535760	So first of all, yeah, one thing we tried is leisure with the depth ratio.
2535760	2542760	So basically with, I'm referring to the hidden dimensions of the model.
2542760	2551760	So we are trying to build a wider model rather than deeper model.
2551760	2565760	And this is important because generally speaking, wider models can utilize accelerators more efficiently, and latency is much better.
2565760	2576760	So yeah, and when we tried this wider model, we observed that we don't really lose much of performance from this.
2576760	2579760	So I think with this is worth it.
2579760	2590760	And another thing we tried is placing feedforward layer with attention layer in parallel.
2590760	2592760	Yeah.
2592760	2603760	Basically, this is, this also saves latency, and you can also make your accelerators utilize better.
2603760	2607760	And this was actually also adapted by Paul.
2607760	2616760	So I think this is something, I think this is a nice contribution from this project.
2616760	2624760	So you think palm researchers read GPTJ and thought like, oh yeah, this architecture change is very good. We're going to use it.
2624760	2633760	Yeah, exactly. I think they also sort of followed our wide model because their model is also very, very wide.
2633760	2638760	I think it's even wider than ours.
2638760	2646760	I read the blog post wrote about GPTJ. And so I kind of read about like all those tricks you did.
2646760	2649760	And you talk a lot about throughput.
2649760	2657760	So yeah, I'm curious like what's the throughput for people who are not like scaling models all the time.
2657760	2667760	And yeah, how does compare, you know, how does GPTJ come compared to like GPT3 or GPTNEO in terms of throughput?
2667760	2672760	Is it more efficient, less efficient, more throughput, less throughput?
2672760	2679760	Yeah, so throughput, I mean the number of tokens processed parameter per second.
2679760	2690760	So if you can prove this throughput, then we can try with a larger model or more tokens with the same amount of throughput.
2690760	2694760	So this way you can improve performance.
2694760	2700760	So it's per, so it's amount of token processed per parameter and per second?
2700760	2701760	Yeah.
2701760	2712760	If you have more, a lot of parameters, you will, I don't know, does the model size then change your throughput then?
2712760	2719760	Yeah, I guess you're only comparing like size models of the same size of GPT3 and GPTJ, so it's fine.
2719760	2728760	So that typically throughput is defined something like two lobs per second.
2728760	2744760	Oh yeah, yeah, so yeah, maybe a bit confusing, but let's say you have a one billion model, require, and you have TPU, one core.
2744760	2753760	If it spends one second per, let's say it spends one second per one token.
2753760	2771760	Then if you have two billion model and one core TPU, then it will take 0.5 seconds because they have the same throughput and something like that.
2771760	2784760	Yeah, so the throughput of the GPTJ six billion model for training is like 150 tokens per second.
2784760	2799760	On the other hand, GPTNEO with 2.7 billion parameters is also 150 tokens per second, but this model is half the size of GPTJ.
2799760	2809760	So basically, this means that we achieved twice improvement in efficiency for throughput.
2809760	2816760	So yeah, I think this different improvement is huge.
2816760	2824760	I think it's coming from this wider model using JAX instead of mesh TensorFlow.
2824760	2833760	Yeah, and not to mention that GPTJ has much better downstream performance than GPTNEO.
2833760	2839760	Yeah, how long did it take to complete like an entire training because I know it requires a lot of computes.
2839760	2848760	Yeah, so we spent five weeks using 256 cores of TPU V3.
2848760	2859760	Yeah, what do you do during those five weeks? Do you just look at the TensorFlow curve and the next answer boards and check it doesn't have any weird spikes?
2859760	2867760	Yeah, basically, that's what we did. There was no back because we already solved all the bugs.
2867760	2878760	And so basically, Ben, babysit this training for five weeks, he was complaining a bit, but he said it was not too bad.
2878760	2888760	And then at the end, you published this model on GitHub, people are very excited and start to use it to fine tune to a bunch of different cases, right?
2888760	2894760	Yeah, so we don't really fine tune ourselves, but many people try to fine tune.
2894760	2904760	It appears that GPTJ is more easier to deal with than other models like GPTNEO.
2904760	2914760	Maybe because the things we use like JAX is much easier to deal with than mesh TensorFlow.
2914760	2917760	So yeah, it became very popular.
2917760	2922760	Yeah, from reading the documentation in GitHub, there's a manual on how to fine tune it.
2922760	2928760	And it seems like the overall code is easier to read as well.
2928760	2939760	And even people on YouTube like Janine Kilsner use it for their projects like fine tuning on 4chan to generate more comments.
2939760	2944760	Have you seen this recent YouTube video and if so, what do you think about it?
2944760	2951760	Yeah, I actually didn't watch the video itself, but I saw the tweet and some people talking about it.
2951760	2953760	So I know it a little bit.
2953760	2962760	So my reaction to this entire controversy is kind of like that meme of like entering into a burning room with pizza.
2962760	2972760	So I think his project is kind of cringy for being too attention seeking and obviously not really ethically sound,
2972760	2982760	because you can just use this app for many bad things and agree with some of the critical reactions to his project,
2982760	2986760	even though some of them may be a bit too exaggerated.
2986760	3000760	But at the same time, I thought this case would lead to more attention spent on a language model that can detect outputs from other language models
3000760	3008760	so that you can filter out language model generated submissions on many websites like Reddit.
3009760	3020760	Yeah, like lotion or Chinese government can use this sort of process to influence social media and election results in the West.
3020760	3028760	But language models are very good at discriminating language model outputs from human outputs.
3028760	3033760	So I'm kind of optimistic about that, at least for short term.
3034760	3045760	What do you think of the fact that publishing GPTJ might have accelerated AI timelines where we might have less time to make AI safe
3045760	3050760	and align those models with human values?
3050760	3056760	Yeah, do you think releasing GPTJ wasn't that good for humanity?
3056760	3060760	Would you have done it before again if you had to?
3060760	3069760	Yeah, I think releasing GPTJ was a small net positive benefit for humanity.
3069760	3086760	And so basically, open sourcing language model, there's nobody who releases a language model that is substantially better than the previous state of the art.
3086760	3096760	Like in my case, in our case, our model performs only slightly better than GPTJ Neo or T5.
3096760	3104760	So I don't think there was actually like accelerating the timeline or anything.
3105760	3118760	Yeah, so basically, I don't think there's any one particular person who can make big negative impact by releasing a big language model with the current trend.
3118760	3131760	If there's anyone then who can make a big negative impact, it'll be someone who like fine tune the model to spread misinformation.
3132760	3137760	Yeah, just to be clear, the GPT Neo was also from A3AI.
3137760	3149760	So in some sense, if you remove GPT Neo and GPTJ, then you don't really have any open source implementation of the GPT3 that works on the internet.
3149760	3160760	And so maybe like you wouldn't have like all those other companies like Microsoft or Chinese or Korean companies using this implementation or even like the pile of the public data sets to train their models.
3160760	3165760	So in some sense, we're kind of helping the entire research community go faster on those topics.
3165760	3169760	And yeah, so maybe there are like other open source projects that would emerge.
3169760	3180760	But then the question is like how much, you know, releasing GPTJ in 2021 accelerates those timelines compared to the others.
3180760	3192760	And I would say like OpenAI releasing the neural scaling laws or GPT3 or even GPT2 kind of showed that like scaling was important to get to AGI.
3192760	3197760	So in some sense, they kind of released some secret cells and everyone started following them.
3197760	3205760	So, you know, if they didn't publish or publish a bit later, maybe the timelines would be a bit longer.
3205760	3207760	What do you make of that?
3207760	3215760	Yeah, so actually there was T5, open source T5.
3215760	3218760	That's not the same as GPT3, right?
3218760	3225760	That's right, but you can fine tune the model like GPT410.
3225760	3230760	So yeah, I think it was totally possible to do it.
3230760	3240760	Before GPT410, I think there was GPT2 reproduction from several people, several different groups.
3240760	3250760	Also, like Facebook and Google released some decoder on the model that performs well.
3250760	3265760	I don't think it affected on research because researchers, I think all these big companies like Google, they already had much bigger language models internally.
3265760	3270760	And people in academia, they cannot really affect.
3270760	3282760	And I don't think they had those big language models before, but I don't think they can really contribute to this large language model research because they don't have budget.
3282760	3288760	So I don't think my, our projects really accelerated the research timeline.
3288760	3297760	But I think, yeah, maybe slightly accelerated the open sourcing language model timeline.
3297760	3306760	Yeah, maybe you accelerate the open source timeline, but not the private research timeline.
3306760	3308760	So in some sense, you bring everyone on the same level.
3308760	3316760	So yeah, definitely our work is slightly accelerating the pace of open sourcing language models.
3316760	3323760	For example, Facebook recently released 100 full size GPT3 model.
3324760	3336760	Maybe that was, and yeah, maybe that was a response to our GPTJ or GPT NeoX model, which was released recently.
3336760	3339760	It has about 20 billion model parameters.
3339760	3351760	And I think this question of accelerating open source timelines, or at least AI research in general, is important in the context of differential progress.
3351760	3361760	So not only accelerating AI progress, but how does the speed of AI relate to the speed of AI alignment research?
3361760	3368760	And I'm not sure if you're very familiar with AI alignment, but in this podcast, we talk about this a lot.
3368760	3375760	And it might be worthwhile maybe defining alignment or at least like going with another definition you're familiar with.
3375760	3380760	So yeah, what do you understand of the concept of alignment?
3380760	3383760	How would you define it if you had to?
3383760	3389760	Yeah, well, as you know, at the meeting, I'm a beginner on alignment.
3389760	3400760	So all I can say, if I understand correctly, is alignment research is the research to harness advanced AI to do what we want to do.
3400760	3408760	And one big problem is considered is the existential risk due to advanced AI.
3408760	3410760	Yeah, that's pretty much it.
3410760	3439760	And I think the question is, if we have an AI that is much more than humans and doesn't really care about our values, it might end up optimizing for an objective and just change completely our planet without really caring about humans or stuff.
3439760	3460760	So we value and I guess this problem might be considered harder or easier depending on how you define the problem or how much time you think humans will have to think about those problems.
3460760	3483760	And one of those takes is that if you only consider alignment as like an AI problem, if we want to solve AI generally and have models that are able to generalize well, if we program them well, they will be able to like solve alignment.
3483760	3491760	And if we build them, if we build like good models, they will be aligned by default.
3491760	3500760	And I guess that's maybe like one of the, is that maybe one of your takes as well or something you think will happen?
3500760	3525760	Yeah, so for short term, I think we can more or less try to be careful with training like what I'm dropping for like open AI is doing, but in long term, which is what Ethan was referring to like when AI is trying to deceive humans.
3525760	3534760	I think that's when we can no longer like use the conventional machine learning approach to deal with.
3534760	3547760	Because, you know, if the AI is much more intelligent than humans, then we have no way to detect whether the model is deceiving or not.
3547760	3551760	Yeah, so that's something I would be worried about.
3551760	3559760	I guess the question is, when you have a benchmark, how do you know if the model is not pretending to be good at the benchmark?
3559760	3562760	Or is it like generally, generally good at the benchmark?
3562760	3571760	So if your benchmark is truthful Q&A, is it actually truthful or is just like pretending to be truthful for the moment?
3571760	3583760	And one of the things that Ethan was saying is that all alignment can be considered as inverse scaling problems.
3583760	3594760	So if you make your model too big at some point, it will, you know, have bad properties.
3594760	3604760	So if you're interested in scaling, you can see that as a scaling problem. So like a good behavior for scaling.
3604760	3608760	Yeah, I agree.
3608760	3621760	So in terms of benchmarks, you said that for models that can be deceptive, it can be hard to write down a good benchmark because they might be able to bypass it.
3621760	3633760	Do you think we might reach a point where we'll be able to have good evaluations and good ways to understand if our models behave correctly or not?
3633760	3649760	Yeah, this is going to be at some point. There's going to be no conventional machine learning benchmark that can detect AI that will be malicious or not.
3649760	3662760	I think more generally, like benchmarks are quite tricky, especially for language models and NLP where if you just like change the beginning of a word, like if you make the first letter capitalized or not in your benchmark,
3662760	3670760	or if you just like change the code base from one GitHub repo to another, you might get completely different results in your benchmark.
3670760	3682760	So do you think it's possible to have good NLP benchmarks to evaluate language models in the future and especially for alignment?
3682760	3694760	Yeah, obviously we need better benchmarks for NLP models, even outside of the alignment problem, even for short term.
3694760	3706760	Sometimes we use automatic methods to evaluate the quality of text, but they tend to be not very well-coordinated with human judgment.
3706760	3724760	So I'm advocating for the use of human judgment, but even human judgment sometimes is not very useful because humans do not really understand whether the generated text is factual or not,
3724760	3734760	because most of us is not really an expert on the field that the model is talking about.
3734760	3751760	So yeah, and for alignment, I think for short term we can probably manage to make up some next benchmark using human evaluations.
3751760	3773760	And if it doesn't work, then we will probably use some trained model to evaluate, because we can't understand what, like my example, humans are not good at judging the quality of models anyway.
3774760	3787760	But at some point, I think that even training models to evaluate model is not enough, because at that point the models are so much better than humans are.
3787760	3796760	And we don't understand the model and we cannot detect whether the model is being malicious or not.
3796760	3800760	So in that case, yeah, there's going to be a big problem, I think.
3801760	3808760	Yeah, I think you're right that it's hard to evaluate fully a model if you're not an expert on the domain.
3808760	3822760	And I think one of the things that we can do is what they did in struggle GPT where they had like a bunch of laborers and people giving evaluations for how much you prefer one output compared to the others.
3822760	3834760	And then you can roughly build a reward model that can say if an output is good or not, because you have like all those comparisons between different outputs.
3834760	3847760	And yeah, for alignment where you're basically saying that you would want like an AI or another language model to evaluate if the other one is answering correctly or not.
3847760	3861760	And at that point, you're kind of doing what people say is bringing a small Godzilla to check if the bigger Godzilla is doing good or not.
3861760	3867760	And so that was like from a blog post that was published recently.
3867760	3882760	And the problem with that is that at the moment when you're like bringing Godzilla to manage the other bigger one, there's already a problem because now you have two Godzilla's in your city.
3883760	3901760	So that's why I think it's a tricky problem to try to align or at least check for deception in those large models is because if you want to like have another model do it, then you need to check this smaller model.
3902760	3920760	And yeah, so yeah, what do you make of like AI's aligning AI's? Do you think us humans will be able to like, you know, build this like smaller model, it's providing the other big ones?
3920760	3935760	Yeah, I think that that is what's going to happen for a short time. Obviously, we cannot keep doing that for long term, because in the long run, AI is going to be so much smarter than we are.
3935760	3948760	So we can no longer make sure whether this small model is doing what we are thinking it's doing.
3948760	3970760	Yeah, so like at the point, we need an entirely different intervention. I'm talking in really long term, so it's probably well after AGI. And at the point, my guess is that something like we have to argument our own intellectual capacity with something new or something.
3970760	3978760	So we become the AI itself. Have you had of something like that?
3979760	3997760	So something like merging with AI's after maybe like some kind of rail curse rail scenario where you upload yourself or maybe like connect with brain competent interfaces to AI. I think that's more like the Elon Musk neural link scenario.
3997760	4000760	Yeah, which scenario you're talking about.
4004760	4008760	I'm not really familiar with your new link. Maybe the last one.
4009760	4029760	Okay. So when you were saying like long term, you were saying like after AGI, so at the beginning of the podcast, you were saying 2028 and 2038 were like lower bound and upper bound for AGI or at least like the rough guess for when it might appear. Is that still correct?
4029760	4038760	Yeah. So by that time, I meant like human level language model rather than AGI.
4039760	4048760	Human level language model. And yeah, how many years to go from human level language models to AGI?
4049760	4065760	I think it's going to take very little amount of time given that this human level language model can do machine learning research well so it can improve itself very rapidly.
4066760	4075760	Okay. So when you reach human level language models, they need to do research and then they achieve AGI. Is that your main timeline?
4075760	4076760	Yeah.
4076760	4081760	And so this might happen in days or weeks?
4082760	4084760	Maybe several months.
4084760	4086760	Several months, okay.
4087760	4111760	Yeah. What would be like the bottleneck at that moment? Like they just like keep improving and but then they need to like figure out how to convince humans to help them build more hardware or would they, I don't know, build the robots themselves?
4112760	4113760	Yeah. So how would it happen?
4113760	4116760	Hardware is going to be the bottleneck, I think.
4119760	4126760	Yeah. Also collaborating with humans, yeah, convincing. And that's going to be, yeah, the main bottleneck.
4126760	4148760	Okay. So if you're listening to this podcast in 2028 and you have language models convincing you to build more, you know, GPU centers and let them, you know, make post requests all over the web.
4148760	4159760	Please don't. It's a bad idea. They're trying to do more ML research. So you've heard it first here.
4161760	4171760	Yeah. So when we were talking about like connecting to neural networks and have our brains maybe like become AI or merge with AI.
4172760	4177760	We're talking about something closer to brain computer interfaces as well.
4178760	4179760	Yeah.
4181760	4192760	So then the question becomes like, well, kind of this timeline of the human slash human plus AI intelligence.
4193760	4206760	Like compared to like just pure AI intelligence. So will the humans be able to keep up a bit and, you know, understand what's going on or will they be left behind?
4207760	4208760	What do you think?
4208760	4222760	Yeah. So I think, yeah, we should control the pace of employment in a way so that we can catch up with the pure natural, pure AI.
4223760	4232760	So, yeah, we should absolutely make sure that we don't, we can keep the same pace, I think.
4233760	4241760	The problem is that we don't really control that as there are not that many brain computer interface researchers, there's not a lot of funding in there.
4242760	4244760	And the rate of progress in AI is accelerating a lot.
4245760	4258760	And if the problem is in, you know, neuroscience are much harder than, you know, scaling models, then even if we throw like a lot of money at it, it's going to be very hard to solve.
4259760	4271760	Yeah, that's true. But so my guess is that AGI is not going to be so, like, malicious for short land.
4272760	4278760	So I'm thinking of using AGI to improve this kind of research for the beginning.
4279760	4281760	At the beginning, what do you think?
4282760	4294760	So basically, you use a very smart language model, some kind of Oracle where you ask questions and gives like very good answers with like documentation and papers.
4295760	4298760	And you ask him like, hey, how do I build a good brain computer interface?
4299760	4302760	And it gives you good answers. And then you go on and you start building that.
4303760	4310760	The problem is that at the moment when you have this kind of model, you can also ask it like, hey, how do I build a better copilot?
4311760	4313760	How do I build a bigger language model?
4314760	4316760	And I guess people might want to do this first.
4317760	4335760	And you might also need to develop new robotics or actual hard tech microscope and neuroscience tools to do those.
4336760	4340760	And it's not going to be like only like a neural network architecture.
4341760	4353760	So I guess it's always like the hardware part is going to take longer than just like software or neural networks part.
4354760	4366760	Yeah, that's true. We may have to regulate the behavior of AGI so that we can keep the pace together.
4367760	4369760	How do you regulate that?
4370760	4384760	We need some physical access to AGI first, the AI, like physically preventing the models being scaled up too fast.
4385760	4398760	So you basically have an AI become the president and then implement some security measures to prevent people from building bigger AIs?
4399760	4400760	Yeah.
4401760	4403760	I'm not sure people would agree that.
4404760	4412760	Yeah, that's one of the scenarios. I guess there are different like bad or like nuanced scenarios that could happen.
4412760	4424760	One is like authoritarian good somehow where you have like one AI governing and then we do what we need to do to have a safe situation.
4425760	4433760	The bad situation is when the AI does whatever it wants and doesn't really care about humans, like a rogue AI self-improving.
4434760	4440760	And then there's another situation where humans are kind of free to do whatever they want.
4441760	4448760	And then things can be good or bad, but it will depend on like kind of how the market behaves.
4449760	4455760	But yeah, a lot of people have been talking about kind of the AI is a dictator scenario.
4456760	4463760	Which scenario do you find more likely or more, you know, you're more like optimistic about?
4463760	4473760	Like imagine you're in this like post-AGI world or human level language model world.
4474760	4480760	What do you kind of imagine the world will look like? Would humans still live like 80 years?
4481760	4486760	Would there be like a lot of crime? Would that be solved with like some kind of dictator AI?
4487760	4493760	I think we're going to try to prevent dictator AI from happening.
4497760	4501760	We probably just try to maintain the current democracy.
4502760	4507760	Yeah, because, you know, most people fare dear AI anyway.
4508760	4512760	I think we're going to do our best to prevent that, whether we like it or not.
4513760	4522760	So hopefully we can like, we can do the thing we propose.
4523760	4525760	I propose like the, I suggest it like this.
4526760	4529760	We can improve our intellectual capacity.
4530760	4535760	Then we can just continue our democratic process.
4536760	4538760	Yeah, that's my hope.
4538760	4548760	Yeah, I think that's a great note of positivity to end the podcast on.
4549760	4555760	So then people can decide for themselves what future they think could be a good one for them.
4556760	4564760	Yeah, it was a pleasure to have you on the show and hearing about your research on scaling
4564760	4569760	and your thoughts on the alignment, even though you're not an expert in this.
4570760	4576760	Do you want to quickly give your Twitter for people to follow one of the AKs?
4577760	4584760	Yeah, so I guess you can just post my Twitter username somewhere.
4585760	4594760	Yeah, I'm tweeting about the latest machine learning papers almost every day with some of these.
4595760	4596760	So please follow me on Twitter.
4597760	4601760	Thanks for listening to this podcast.
4602760	4606760	I think your Twitter ID is something like aran, kuma, tsuzaki, or is it different?
4607760	4608760	Oh yeah, it is.
4609760	4612760	Cool, yeah, I'll probably post it in the bio somewhere.
4612760	4619760	Thanks for coming on the show and I hope you keep on doing some cool research for other AI
4620760	4623760	and hopefully not accelerating too much the AI timelines.
4624760	4627760	Thanks, Aran, and see you maybe in the next episode.
