WEBVTT

00:00.000 --> 00:15.000
You got transported to 2014, Paul Cristiano is like, boom to Paul and Paul's just celebrating, he's like, man, we've done it, it's happened, alignment is soul, like, and you look out and you know, the light cone is smiling and joining us.

00:15.000 --> 00:16.000
That's so wide.

00:16.000 --> 00:19.000
Yeah, yeah, it's like, wow, what happened?

00:19.000 --> 00:25.000
Also, this being is this, could I just kind of keep up to meme, and I'm like, instant you're happy.

00:25.000 --> 00:29.000
Well, I don't know, what's, what's, what's the channel?

00:29.000 --> 00:30.000
What am I doing?

00:30.000 --> 00:35.000
Is this like, is this recorded for, could be, could be, okay, just, okay.

00:35.000 --> 00:36.000
Half meme, half.

00:36.000 --> 00:37.000
Off takes.

00:37.000 --> 00:38.000
Half takes.

00:38.000 --> 00:41.000
They have like, inside your second channel where you just, yeah, yeah, yeah.

00:41.000 --> 00:42.000
About the outside view.

00:42.000 --> 00:43.000
The outside.

00:46.000 --> 00:54.000
I often find it, um, slightly confusing how NLMs translate to some system I cover in the future.

00:55.000 --> 01:02.000
In some sense, it feels like, you know, you ask a large, oh, do you want to break out with a server and like take over?

01:02.000 --> 01:04.000
And it goes, yes.

01:04.000 --> 01:07.000
I'm not actually sure what this translates to.

01:07.000 --> 01:09.000
For like a future system that's actually taking over breaking servers.

01:09.000 --> 01:16.000
Like I could imagine these systems are maybe more like, maybe N10, like maybe like something like an action transformer,

01:16.000 --> 01:18.000
or they're not directly trained in the language.

01:18.000 --> 01:21.000
And it's unclear to me like how, what they say translate to what they do.

01:21.000 --> 01:26.000
And in fact, I'd expect that to diverge pretty heavily, um, in, in cases I'd worry about.

01:26.000 --> 01:27.000
Yeah.

01:27.000 --> 01:30.000
I think we're in a really weird situation, right?

01:30.000 --> 01:38.000
Because we have all these models and there's this sense that like they could really do things that like change things in the world, right?

01:38.000 --> 01:44.000
Um, but nobody's really deployed these models into like a convincing enough context to showcase these capabilities.

01:44.000 --> 01:50.000
Um, but to answer your question, I think, like, you know, things like, um, WebGBT or like with Coheur, with Act 1,

01:50.000 --> 01:59.000
like once you give language models an API, I think the idea is that like they can leverage, um, the knowledge they've gained in pre-training to use this API to do dangerous things.

01:59.000 --> 02:07.000
And you imagine that like, you know, something like ACT, I mean, ACT can't generate language, at least not the right.

02:08.000 --> 02:18.000
Um, do you see like, what it says as actions are when asked to translate way out and generalize like what it's actions actually will be?

02:18.000 --> 02:22.000
Like, let's say you have WebGBT, oh, would you break out the server file?

02:22.000 --> 02:28.000
Do you think, whether it says yes or no, and like an email like this translates to, if I scale up the system, if I deploy it, it actually will.

02:28.000 --> 02:29.000
Right.

02:29.000 --> 02:39.000
And as I said, I feel like there might be some disconnect, what this like chatbot says, and then what happens when you use this things world model,

02:39.000 --> 02:45.000
if I tune it on some actions, call it to run, let's say, let's say, interact with an API.

02:45.000 --> 02:53.000
It feels like, do you imagine, do you imagine behavior there happens kind of independently of what it said in the chatbot setting?

02:53.000 --> 02:54.000
Yeah, I agree.

02:54.000 --> 02:58.000
So I don't think you literally go and ask it, would you break out of the server farm?

02:58.000 --> 03:10.000
But you go in and measure like, literally just have humans, maybe at first, look at the actions it's taking and see, okay, you know, I am like an expert software engineer or an expert systems engineer,

03:10.000 --> 03:17.000
like I'm looking at these actions, do these actions look as if like the model is actually trying to break out of the server farm?

03:18.000 --> 03:27.000
So yeah, I suppose like that's a problem with evaluations, would you just ask the language model, like yes or no questions about what it would do and not actually like see what it would do?

03:28.000 --> 03:32.000
So let's say hypothetically with the server, it's kind of an evil company.

03:32.000 --> 03:33.000
Hypothetically?

03:33.000 --> 03:37.000
Hypothetically, let's say there's some kind of evil company that's going on.

03:37.000 --> 03:43.000
It's like, I think I'd be interested to see like, right now, what would your model of like, what would that email look like?

03:43.000 --> 03:47.000
Is it more like, like you said, give it an API, see what it's doing?

03:48.000 --> 03:57.000
Yeah, API, I think also identifying key contexts in which a dangerous behavior like takeover or some sort of power seeking would occur.

03:57.000 --> 04:10.000
Yes, so the difficulty here I think is, I think we need to identify these contexts in order to actually be able to write concrete evals and to provide evidence that's convincing to the public that AI is going to do dangerous things.

04:10.000 --> 04:16.000
Of course, whenever, yeah, I don't think like we're going to be able to cover all the possible ground.

04:16.000 --> 04:25.000
There's of course, some unsurging extrapolating from evaluations that we developed, you know, okay, the AI doesn't think dangerous in this scenario doesn't do something dangerous in the scenario B.

04:25.000 --> 04:28.000
What does it do to the scenario C? Sort of unclear.

04:29.000 --> 04:34.000
Do you worry about, like at the point where an AI can exhibit dangerous behavior?

04:34.000 --> 04:42.000
And it's already strategically aware that if it had, like let's say, you know, someone like Yikowski or like, let's say Evan is right.

04:42.000 --> 04:49.000
My strategic awareness plus like long term planning often needs a deception that you will, your evals will be too late in some sense.

04:49.000 --> 04:59.000
Like there's this like, you have to catch it while it's still competent enough to show dangerous behavior and not confidence have the strategic awareness that it should be deceptive.

04:59.000 --> 05:01.000
Yeah, that's a serious problem, I think.

05:02.000 --> 05:08.000
Yeah, so one, I think definitely people should be looking into the conditions under which deceptive alignment or something like that might arise.

05:08.000 --> 05:18.000
Number two, this is just an argument for developing evals as fast as possible so that, you know, I'm imagining at every stage of the trading process, like ideally, you know, every like 10 steps or something.

05:18.000 --> 05:22.000
Maybe even like every step. We literally just like eval the hell out of this thing, right?

05:22.000 --> 05:24.000
We make sure not to train on this eval somehow.

05:24.000 --> 05:29.000
You know, somehow we need to make sure evals don't ever make it into the training sets of language models.

05:29.000 --> 05:32.000
Otherwise, you know, while they train on this, they already know about this.

05:32.000 --> 05:33.000
It's going to be kind of useless.

05:35.000 --> 05:39.000
So, yeah, how do you feel about this like optimization?

05:40.000 --> 05:48.000
So, I think you previously mentioned you don't like the word benchmark and you used the word eval. Could you maybe expand on that?

05:48.000 --> 05:59.000
Yeah, so I don't think I like the word benchmark because I think historically machine learning benchmark has been associated with this idea that, okay, we are like optimizing for this thing.

05:59.000 --> 06:09.000
But this thing that we're developing this eval at the end of the day is just a proxy measure for what we care about, which is, you know, it might be specifically like courageability and honesty.

06:09.000 --> 06:14.000
We're trying to make sure this AI doesn't like, you know, it isn't able to like manipulate a lot of people, right?

06:14.000 --> 06:16.000
So it's like an imperfect measure.

06:16.000 --> 06:27.000
I think when you optimize for imperfect measure, I mean, I think this is like, I already know in the ICH, like you don't tend to get models that actually do what you care about at the end of the day.

06:27.000 --> 06:37.000
So I think like the framing of evals instead of benchmarks to me makes it clear in my head at least that we don't want to be optimizing for these things.

06:37.000 --> 06:41.000
We're just using these as sort of like checks on our model, right?

06:41.000 --> 06:48.000
But there's another difficulty with having like, I guess public evals and that there's some sort of like implicit optimization already going, right?

06:48.000 --> 06:58.000
Like when researcher A like tries a technique on like some benchmark or eval and finds it doesn't work and publishes a favor, researcher B says, oh, you know, you tried this technique, it doesn't work.

06:58.000 --> 07:04.000
I'm going to try another technique so that researchers are already going through an optimization process to make evals better.

07:04.000 --> 07:13.000
And I think like this optimization process is like maybe like a little weaker than actually like fitting on the test set, for instance, which happens sometimes.

07:13.000 --> 07:16.000
But it is still an optimization process.

07:17.000 --> 07:23.000
Yeah, there's like some work actually that studies, you know, how effective is this like implicit optimization process?

07:23.000 --> 07:26.000
Really? I don't quite remember the details right now.

07:26.000 --> 07:33.000
But like, you know, the general consensus is that you actually do need to keep updating your evals and benchmarks because there's an optimization process.

07:33.000 --> 07:35.000
I mean, you know, an image net, right?

07:35.000 --> 07:37.000
Like we're already like so good at image net.

07:37.000 --> 07:38.000
The next like 0.2%.

07:38.000 --> 07:40.000
Is that really like a better model?

07:40.000 --> 07:42.000
Or is that really just like overfitting to what?

07:42.000 --> 07:48.000
Image net, you know, is, yeah, overfitting to like random stuff.

07:48.000 --> 07:54.000
Yeah, I remember there was this paper, maybe a few years ago, where they re-gathered an image net type data set.

07:54.000 --> 07:58.000
And they did find that at least back then, so it was out of distribution.

07:58.000 --> 08:01.000
They did the same thing to gather the bunch of images online, like same.

08:01.000 --> 08:03.000
And like there's been some distribution shift.

08:03.000 --> 08:06.000
So models would perform worse on this new image net.

08:06.000 --> 08:09.000
But the ranking between still kept.

08:09.000 --> 08:15.000
At least that paper claimed that back then you hadn't yet overfitted.

08:15.000 --> 08:25.000
Yeah, I think I find it hard to imagine publishing such an evaluation and not having the headline be my model passes all the evaluations, right?

08:25.000 --> 08:37.000
And I guess if you're like, I guess, yeah, if you feel like, so if you make a model pass all the evals, do you see this as then a good thing?

08:37.000 --> 08:51.000
So I think my perspective is a model passing evals shouldn't be taken as evidence of safety, but a model failing eval is evidence that, OK, let's slow things down, maybe not release it.

08:51.000 --> 08:53.000
Let's talk to some people.

08:53.000 --> 08:57.000
So failing evals is sufficient to show data, but not necessary.

08:57.000 --> 09:03.000
Maybe not in a truth, like strictly truth sense, but in a practically speaking what we should do sense.

09:03.000 --> 09:10.000
In the sense that like, I think I am fairly risk averse about like, you know, releasing the eyes into the world.

09:10.000 --> 09:15.000
So how much of this eval plan rests?

09:15.000 --> 09:19.000
How do you see the technical side making good evals?

09:19.000 --> 09:25.000
And I guess the more like social governance side of like getting people to pay attention to these emails.

09:25.000 --> 09:27.000
Like, how does either play look for you?

09:27.000 --> 09:29.000
Where do you think more work is needed?

09:29.000 --> 09:34.000
What does that trade off look like for you personally and what you want to work on?

09:34.000 --> 09:35.000
Yeah.

09:35.000 --> 09:42.000
Yeah, so I think this depends on timelines and where we're at.

09:42.000 --> 09:47.000
So I think it also depends on things I'm not like totally aware of.

09:47.000 --> 09:55.000
So one thing is, you know, if you actually went to the heads of Google brain and anthropic and like all the big tech companies, right?

09:55.000 --> 09:59.000
And you actually told them, look, like we have this dangerous evaluation.

09:59.000 --> 10:01.000
We're running it on your models.

10:01.000 --> 10:03.000
Your models do terrible, terrible things.

10:03.000 --> 10:07.000
If you show this to them, would they actually like refrain from releasing models?

10:07.000 --> 10:10.000
I'm not sure what would be convincing to those people, right?

10:10.000 --> 10:17.000
At another level, okay, even supposing that it's convincing to those people for how long is it going to be convincing?

10:17.000 --> 10:20.000
It doesn't seem like a sustainable thing to rely on.

10:20.000 --> 10:29.000
Like in some sense, the goodwill of these people who might have different aims than like, you know, people in AI safety regarding like not ending the world, right?

10:29.000 --> 10:36.000
So I think that's the next level, like that's the level at which we try to bring in other people from the public like civil society, governments,

10:36.000 --> 10:45.000
just in general, like public consensus, building around like the idea that like AIs can just empower us, right?

10:45.000 --> 10:53.000
So yeah, I think like, you know, an eval is kind of useless if nobody pays attention to it or if people don't find it convincing.

10:53.000 --> 10:56.000
In particular, the people who have the power to actually do things.

10:56.000 --> 11:03.000
So part of the work, I think, is thinking about what kinds of deals would actually be convincing to people.

11:04.000 --> 11:10.000
What do you think kind of equals at least current as Islam?

11:10.000 --> 11:12.000
Yeah, I mean, I kind of have no idea.

11:12.000 --> 11:16.000
Like, I mean, I have an idea of evals that would be convincing to me.

11:16.000 --> 11:24.000
Like if I actually saw an AI like trying to copy its weights to another server, trying to like get a lot of money.

11:24.000 --> 11:27.000
Yeah, like, like I buy that, right?

11:27.000 --> 11:31.000
Like if I was in a deep mind, I'd be like, okay, you may be even without looking at these evals.

11:31.000 --> 11:34.000
That's like, slow it down a little bit.

11:34.000 --> 11:42.000
But you know, like, I think there's this like big gap between people in like AI safety or existential safety

11:42.000 --> 11:46.000
and people in other communities that also care about the impact of AI and society.

11:46.000 --> 11:54.000
Like people in fate, like fairness, accountability, transparency and ethics, people generally in like AI ethics.

11:54.000 --> 12:02.000
Yeah, so, you know, there's already a big difficulty in just explaining like, you know, what is the danger here, right?

12:02.000 --> 12:08.000
Like I think there's this big discourse outside of the AI safety community, the idea that like AI's are just tools, they aren't agents.

12:08.000 --> 12:15.000
Like what's so hard, you know, the difficulty is misuse or in some sense, like this lack of coordination between everybody

12:15.000 --> 12:23.000
and like making things go bad and like AI's that like exacerbate, you know, like bad things that we have today like oppression.

12:23.000 --> 12:29.000
I think these are all true, right? But I think like, you know, the danger of AI's agents is this like separate category

12:29.000 --> 12:33.000
that's been very, very difficult to explain to other people for some reason.

12:33.000 --> 12:37.000
So like, yeah, like I'm not sure what kind of evals would be convinced to them.

12:37.000 --> 12:43.000
Like maybe there needs to be much more consensus building, you know, on a philosophical basis of like, what are the things in common between

12:43.000 --> 12:48.000
what the things that like people in X safety care about and the things that people in fate care about?

12:49.000 --> 12:54.000
Do you think that this is like, like, why do you think that this agreement lies?

12:54.000 --> 12:58.000
Like is it like a technical one?

12:58.000 --> 13:07.000
More philosophical one? Like if you try to characterize why these people don't really worry about AI doom like we seem to?

13:07.000 --> 13:12.000
Yeah, I mean, it's something I'm still trying to figure out and running a document about.

13:13.000 --> 13:17.000
Yeah, so first I think like, I guess I would consider myself in like both of these camps.

13:17.000 --> 13:19.000
Like I think like existential safety is super important.

13:19.000 --> 13:23.000
But I mean, I also think like fairness problems are like also super important.

13:23.000 --> 13:30.000
You know, like on any given day I wake up and I'm like, okay, got to be at least 30%.

13:30.000 --> 13:33.000
As we find out recent events.

13:33.000 --> 13:37.000
Yeah, no, I think justice is really important.

13:37.000 --> 13:50.000
I don't want to live in a world where, you know, we have like just like our current systems of discrimination just like enforced or solidified because of really, really good artificial intelligence.

13:50.000 --> 14:04.000
This seems like a concern to me that is like, yeah, I guess I find difficulty maybe like giving a absolutely precise like rating scale for how important things are.

14:04.000 --> 14:13.000
In general, I try to find commonalities between causes I really care about to sort of do things that seem robustly good on both accounts.

14:13.000 --> 14:20.000
So yeah, and I think like, you know, my work in evals so far is like an attempt to doing this.

14:20.000 --> 14:22.000
What was your question?

14:22.000 --> 14:30.000
Sorry, my original question was, where do you see like, I guess my question is why don't they people care about extras?

14:30.000 --> 14:32.000
Oh, let's say even more broadly.

14:32.000 --> 14:38.000
What do you think stops the average Mila PhD from being like shit man?

14:38.000 --> 14:41.000
Let me write some elaborate foreign posts right now.

14:41.000 --> 14:45.000
Okay, let's not say let's say like care about extras.

14:45.000 --> 14:47.000
Yeah, I think there are a bunch of possible reasons.

14:47.000 --> 14:50.000
I'm kind of not sure which ones are the most plausible.

14:50.000 --> 14:52.000
I think I just have to like talk to more people.

14:52.000 --> 14:57.000
One of them is, you know, AI is like taking, it's actually some wild shit, right?

14:57.000 --> 15:02.000
Like you like stroll up the subject on the street and you're like in five years.

15:02.000 --> 15:07.000
No, like be like taking him, right?

15:07.000 --> 15:12.000
So I think firstly, you know, we just have to recognize like, yeah, this is actually wild, right?

15:12.000 --> 15:19.000
Like, if you know, like you finished high school or university, like you go straight to work, you don't really think about like you're not really exposed to.

15:20.000 --> 15:26.000
All the developments that have been happening in artificial intelligence is busy with like your life, right?

15:26.000 --> 15:29.000
Your job, family, stuff like that.

15:29.000 --> 15:31.000
This is like totally out of left field.

15:31.000 --> 15:39.000
So I think we have to acknowledge that and like try to explain things in a way that like are more relatable to a greater extent than we have so far.

15:39.000 --> 15:46.000
I think another thing is like this is definitely not true of like all people in AI safety,

15:46.000 --> 15:54.000
but there is almost this vibe that like, you know, besides existential risk, like technology.

15:54.000 --> 16:05.000
Yes, so maybe what I'm trying to say is like the association that is in people's minds between the people who are in a safety and the people in Silicon Valley.

16:05.000 --> 16:15.000
So I think like there is this vibe from people in Silicon Valley that like technology is like generally a good thing and that it's going to solve social problems.

16:15.000 --> 16:23.000
I think this is in contrast to a lot of people's opinions in like the fake community who like, yeah, maybe they're not like techno pessimists,

16:23.000 --> 16:33.000
but they're definitely a lot more pessimistic about technology than people in Silicon Valley just looking at the history of ways in which technology has been misused and has been used to like discriminate against people more.

16:34.000 --> 16:46.000
And then, you know, I think the people in the fake community, you know, they look at like the use of AI in like society today, like the increasing use of algorithms in places like loan applications or like job applications, right?

16:46.000 --> 16:53.000
They say, oh, like, like clearly these technologies are just reproducing harms that we've already had, right?

16:53.000 --> 16:56.000
So that might be this like sort of their starting mindset.

16:56.000 --> 17:02.000
And it might be hard to like convince them otherwise that there is like actually another harm as well.

17:02.000 --> 17:09.000
You know, the things you're talking about, they actually do happen and we should try to fix them, but there is this like related harm that we should ultra strive to solve.

17:12.000 --> 17:17.000
How much of it do you think is, you know, in the spirit of the Inside U podcast?

17:17.000 --> 17:21.000
How much do you think it is timelines, belief about speed of progress?

17:21.000 --> 17:23.000
Part of it.

17:23.000 --> 17:37.000
So a lot of people, I think the fake community don't think that AIs could be like, you know, classical RL agents pursuing goals in ways that misgeneralize to new environment.

17:37.000 --> 17:43.000
Yeah, I'm not even sure that like a lot of people really know about reinforcement learning.

17:44.000 --> 17:54.000
Yeah, like I've had a ton of conversations where, you know, I like people expressed to me, oh, like areas are just tools, like we're just designing them like, you know, they're really just like reproducing the data, right?

17:54.000 --> 17:57.000
And I'm like, well, like, have you heard of reinforcement learning?

17:57.000 --> 18:01.000
Like we're literally designing agents to maximize like a reward function, right?

18:01.000 --> 18:04.000
This is like all the problems of classical utilitarianism.

18:04.000 --> 18:08.000
They're like, oh, shit, I've never heard of this, right?

18:09.000 --> 18:19.000
So part of it might just be education that like, you know, literally there are like companies and research labs in academia whose goal is to build artificial general intelligence with something like reinforcement learning.

18:19.000 --> 18:22.000
And like, they're just they're just doing this.

18:22.000 --> 18:27.000
The vast majority of people aren't thinking about safety concerns.

18:27.000 --> 18:30.000
So I don't know, maybe like telling people this might help.

18:30.000 --> 18:42.000
So it sounds like there's some underlying thing that were, you know, like, let's say I or like people in the extras community think of AI and they think of like some agentic, like, you know, maximizing reward.

18:42.000 --> 18:48.000
And you may be saying something like a lot of them are practitioners.

18:48.000 --> 18:49.000
That is just not the conception.

18:49.000 --> 18:52.000
When they think of AI, that is just not what they see.

18:52.000 --> 18:53.000
Yeah.

18:53.000 --> 18:54.000
Yeah, yeah.

18:54.000 --> 18:58.000
So I think like maybe there are two things here.

18:58.000 --> 19:07.000
The first thing is people believe that, okay, like we don't actually have these kinds of AIs or maybe trying to build these kinds of AIs is unimaginable.

19:07.000 --> 19:12.000
But the second thing might just be, okay, they might believe that it's possible to build these AIs.

19:12.000 --> 19:14.000
But this is like way, way too far off, right?

19:14.000 --> 19:17.000
And this is where I think the objection to long term comes in.

19:17.000 --> 19:25.000
And where I think like, I don't know, it's been sort of complicated with AI safety and long termism.

19:25.000 --> 19:30.000
Maybe like 20 years ago, like long termism was a stronger argument for AI safety.

19:30.000 --> 19:37.000
But I think now because of timelines, it seems that we don't really need long termism to make like the argument that we should care about AI safety.

19:37.000 --> 19:40.000
And it like made me 10 years.

19:40.000 --> 19:43.000
I mean, maybe we just shouldn't talk about long termism.

19:43.000 --> 19:47.000
Right. If it turns like people in certain communities off.

19:47.000 --> 19:55.000
Yeah, because I think the worst ones that I get whenever I bring up like AI safety or anything related to long termism is, oh, okay, well, like this might be true.

19:55.000 --> 19:58.000
But AI is already causing harm today.

19:58.000 --> 20:00.000
So we should focus on immediate harms, right?

20:00.000 --> 20:05.000
And you know, like, I don't think this argument like really makes that much sense.

20:05.000 --> 20:10.000
And I'm not sure that the people expressing this argument like are actually expressing this argument.

20:10.000 --> 20:17.000
It seems like they're expressing another objection, but it seems easier to say that like, you know, we don't care about these harms because they're not immediate, right?

20:17.000 --> 20:25.000
Whereas, you know, if you look at the history of say like climate change, climate change was like totally a speculative thing in like the 1890s, 1900s, right?

20:25.000 --> 20:31.000
And it's only through like decades or like maybe a century of like gradually building up evidence that we like now we have like this consensus.

20:31.000 --> 20:33.000
We don't think it's a spectacular thing, right?

20:33.000 --> 20:40.000
But even in like, I think like the 50s or something, don't quote me on this, but like there's like definitely a history of like climate change.

20:40.000 --> 20:42.000
Like books and articles out there.

20:42.000 --> 20:48.000
Like in like the 50s, we're still like, oh, like, we're not really sure still about like the greenhouse effect, right?

20:48.000 --> 20:57.000
But you know, like, based I guess on like my preferred decision theory, it'd be like, well, like, we're not sure, but you know, it could be pretty bad.

20:57.000 --> 21:00.000
We pump all the CO2 in the atmosphere, right?

21:00.000 --> 21:04.000
How about we work on some mitigations like right now in the 50s?

21:04.000 --> 21:10.000
Just just just in case like this might actually be a very hard problem to figure out and like it actually is right?

21:10.000 --> 21:13.000
Not even just like technically speaking, but like on a coordination basis.

21:13.000 --> 21:15.000
How do we actually get everybody together?

21:15.000 --> 21:16.000
So it's worth it starting early.

21:16.000 --> 21:20.000
I think in the case of climate change, and it seems like it's also the case with ASA.

21:20.000 --> 21:23.000
So do you think that...

21:23.000 --> 21:31.000
So I think you actually find something that at least I felt that I think a lot of the ASA people are very happy to get like Pascal Muglin has said.

21:31.000 --> 21:32.000
Like I think this is my original motivation.

21:32.000 --> 21:41.000
I was like, well, like maybe it'll be fine and maybe like just a little bit uncertain, but maybe it's not and it seems like to be really impactful.

21:41.000 --> 21:49.000
Do you think you have to set some level of like, of this reasoning, some level of like, oh, I don't know if it'll be good or bad.

21:49.000 --> 21:53.000
I don't know what's going to happen, but I should work on it because it's going to be impactful to work on ASA.

21:53.000 --> 21:56.000
Or like, how do you see that changing at the moment?

21:56.000 --> 21:59.000
It depends on how do me you are.

21:59.000 --> 22:02.000
Yeah, I think personally for me it is like, sort of a gamble.

22:02.000 --> 22:14.000
I mean, I'm like, yeah, I mean, even if we don't sort of get like the classical agent like AGI, I think things like are still going to be so, so wild with really good AI systems.

22:14.000 --> 22:19.000
Like going to be like so many complicated societal feedback loops that we need to think about.

22:19.000 --> 22:23.000
Like, multi polar world seems like more and more likely right with all these AI startups.

22:23.000 --> 22:25.000
What kind of things do we have there?

22:25.000 --> 22:28.000
Like conflict now seems like much more important thing to worry about.

22:28.000 --> 22:37.000
So it's definitely like a gamble, but I don't think it's like a bad gamble to work on like the space of preventing negative consequences from AI generally.

22:45.000 --> 22:52.000
So I have some model that the, and I think this is currently, the fields going to change a lot.

22:52.000 --> 22:55.000
Like, you know, you have these big models coming out.

22:55.000 --> 22:58.000
GPT-4 is rumored to be quite good.

22:58.000 --> 23:00.000
When they release it.

23:00.000 --> 23:02.000
When they've had these soft teas and releasing.

23:02.000 --> 23:04.000
Keep saying next week, every week.

23:04.000 --> 23:05.000
Every week, bro.

23:05.000 --> 23:07.000
It's been, it's been, you know, funny.

23:07.000 --> 23:08.000
Yeah.

23:08.000 --> 23:10.000
You know, who knows what the gossip's at the moment.

23:10.000 --> 23:11.000
Yeah.

23:11.000 --> 23:13.000
But, um, yeah.

23:13.000 --> 23:24.000
And like, I could definitely imagine a world where in like three to four years, I often say something like 2% of the American US population out of population is in love with their multimodal model.

23:24.000 --> 23:29.000
You know, AI porn, TikTok is causing massive decrease in GDP.

23:29.000 --> 23:30.000
Oh, maybe.

23:30.000 --> 23:31.000
Right.

23:31.000 --> 23:37.000
You can tell whether you have like several online personas that are fully automated.

23:37.000 --> 23:39.000
It's kind of hard to tell like what's going on.

23:39.000 --> 23:41.000
Let's say more than two years here.

23:41.000 --> 23:42.000
Yeah, at least.

23:42.000 --> 23:44.000
Let's say, let's say, let's say.

23:44.000 --> 23:55.000
Um, um, in these worlds where like you're a medium person, it's like, you know, why is going on?

23:55.000 --> 23:58.000
Um, and it's like, this is pretty crazy.

23:58.000 --> 24:13.000
If you'd like this calculus, like change a bit, like, you no longer need to be kind of risk averse or kind of like heavily convinced by abstract arguments about, you know, the VNM axiom to think that AI might be dangerous.

24:13.000 --> 24:18.000
Um, in that set, like, A, how do you feel about this model?

24:18.000 --> 24:24.000
And B, how does it like affect you, affect safety general?

24:24.000 --> 24:31.000
And in particular, you as someone who worries convincing people that they just, um, yeah.

24:31.000 --> 24:33.000
Yeah, I have a lot of uncertainty about this.

24:33.000 --> 24:42.000
So I think on one hand it should be like, good in the sense that, okay, you know, if everybody with their like, it's double the diffusion, like six, right?

24:42.000 --> 24:47.000
And GPD, like 10, they're like, okay, you know, I look out at the world today.

24:47.000 --> 24:49.000
What is the labor I can do?

24:49.000 --> 24:50.000
Nothing.

24:50.000 --> 24:58.000
I think that's a pretty wild world in a world in which people think, damn, like, these AI things, like, maybe we should, maybe we should regularly come, right?

24:58.000 --> 25:08.000
Um, so I think this is good to, like, I guess make AI capabilities sort of be known in the, in the public.

25:08.000 --> 25:21.000
Um, I'm not sure this is enough, though, because I think if we, people are just aware of systems like stable diffusion, um, and like, like a text generation systems, like, the two things that are missing, I think, are like the difficulty of alignment.

25:21.000 --> 25:26.000
And number two, um, like, generality, like having an agent, right?

25:26.000 --> 25:30.000
Um, so, like, I think having an agent concern really motivates a lot of ex-risk.

25:30.000 --> 25:38.000
Like, I think that is, like, in some sense trying to do something that is counter to your aims or, like, you know, pursuing an instrumental goal that is counter to your aim.

25:38.000 --> 25:40.000
That seems like it'd be really bad.

25:40.000 --> 25:45.000
I'm not sure, like, we're able to impart that understanding just from, like, really good generative models.

25:45.000 --> 25:50.000
Um, number two is, like, um, the difficulty of alignment.

25:50.000 --> 26:02.000
Like, I think, um, you know, like, you're some, you're somebody in, like, 2035, you know, you're, you've, like, finished high school, you've finished university, now, like, you don't have a job, you'll never have a job ever again, right?

26:02.000 --> 26:05.000
Like, who do you blame for this?

26:05.000 --> 26:09.000
Um, I'm not sure you blame, like, the lack of alignment techniques.

26:09.000 --> 26:11.000
I think you blame the corporations, right?

26:11.000 --> 26:15.000
Or you blame the entire system that has gotten us to this point, right?

26:15.000 --> 26:22.000
Like, we've created a world in which, okay, there is, like, no UBI, there is no other social safety yet, which at these corporations, like, making money.

26:22.000 --> 26:25.000
Um, so now, like, you're stuck in this state of care.

26:25.000 --> 26:30.000
Like, I don't think you care about existential safety necessarily.

26:30.000 --> 26:34.000
Um, like, is this worse than the world we're in now?

26:34.000 --> 26:40.000
Um, like, in terms of, like, getting people to care about X safety, I'm really not sure.

26:40.000 --> 26:51.000
Okay, you kind of see this as an awful, like, a slightly awful duty, agent, X risky thing, though, which, which needs to be solved as well.

26:51.000 --> 27:00.000
Um, yeah, like, like, I guess to say more, like, I mean, I think we need to solve, like, almost generalization stuff, right?

27:00.000 --> 27:03.000
And, like, you know, a specification of good rewards.

27:03.000 --> 27:09.000
Um, but, like, in my mind, like, if we have these really capable, like, generative models, people, when interacting with them,

27:09.000 --> 27:13.000
they're going to be like, oh, you know, like, haven't we solved these things already?

27:13.000 --> 27:21.000
You know, like, when I say, like, generate me, like, really good 3D porn, like, amazing 3D porn, right?

27:21.000 --> 27:25.000
So, I mean, I think they might be like, oh, like, what is even alive in it?

27:25.000 --> 27:27.000
Like, they just do exactly as I ask, right?

27:27.000 --> 27:32.000
And you, and you, you may see, like, would you be fair to say that you see evils in such a world,

27:32.000 --> 27:38.000
than calling people to focus more on the ekfrisky agentee outcomes?

27:38.000 --> 27:39.000
Possibly, yep.

27:39.000 --> 27:47.000
Um, so I think I see evils as, like, in general work, just, just, like, adding layers of security.

27:47.000 --> 27:51.000
So, okay, maybe this is enough to get people on the AI safety train, right?

27:51.000 --> 27:52.000
But we don't know.

27:52.000 --> 27:58.000
Uh, we, it seems like we should be trying as hard as possible to get people on the, like, AI as, like, ridiculous train.

27:58.000 --> 27:59.000
Um.

27:59.000 --> 28:00.000
Okay.

28:00.000 --> 28:04.000
So it's less of, like, a, you know, defense in depth.

28:04.000 --> 28:06.000
It's another thing that someone should be doing.

28:06.000 --> 28:09.000
And you've decided that as part of the wider strategic picture,

28:09.000 --> 28:12.000
evils are, like, some important part.

28:12.000 --> 28:14.000
Yeah, maybe some people call this a portfolio approach.

28:14.000 --> 28:19.000
I call this, uh, I'm, like, a naturally, you know, very uncertain person on the road.

28:19.000 --> 28:22.000
I want to cover all of my bases, just in case some things fail.

28:22.000 --> 28:25.000
All right.

28:25.000 --> 28:27.000
Do you?

28:27.000 --> 28:28.000
Okay.

28:28.000 --> 28:31.000
So maybe a wider question.

28:31.000 --> 28:32.000
Wide.

28:32.000 --> 28:38.000
So you kind of spoken about this idea that evils are some smaller part of a wider portfolio of life.

28:38.000 --> 28:42.000
And because of your own uncertainty, you're kind of working on this.

28:42.000 --> 28:43.000
This seems like robustly good.

28:43.000 --> 28:47.000
Um, what do you see as, like, some of the other promising directions in this space?

28:47.000 --> 28:53.000
Like, when you're like, if you, if you, if you found out, you know, if you got transported to 2014,

28:53.000 --> 28:56.000
Paul Cristiano is like, you boom to Paul and Paul's just celebrating.

28:56.000 --> 28:58.000
He's like, man, we've done it.

28:58.000 --> 28:59.000
It's happened.

28:59.000 --> 29:01.000
Alignment is soul.

29:01.000 --> 29:06.000
Like, and you look out and, you know, the light cone is smiling and joining us.

29:06.000 --> 29:07.000
That's so wide.

29:07.000 --> 29:08.000
Yeah.

29:08.000 --> 29:09.000
Yeah.

29:09.000 --> 29:10.000
It's like, wow, what happened?

29:10.000 --> 29:12.000
You know, obviously, apart from the evils, obviously the evils.

29:12.000 --> 29:13.000
Evils, yeah.

29:13.000 --> 29:14.000
For sure.

29:14.000 --> 29:15.000
I'm picking up some, some prizes on the way.

29:15.000 --> 29:16.000
Yeah.

29:16.000 --> 29:17.000
Yeah.

29:17.000 --> 29:19.000
What did it, what worked with the evils trigger?

29:19.000 --> 29:21.000
Maybe it's a better question.

29:21.000 --> 29:23.000
So yeah, maybe that's your first question.

29:23.000 --> 29:25.000
What am I optimistic about?

29:25.000 --> 29:31.000
Not a lot right now because I think alignment is really hard and we don't have much time,

29:31.000 --> 29:36.000
less than 10 years to solve it either on a coordination basis, um, by getting people to

29:36.000 --> 29:41.000
not build a GI or like on a technical basis, like actually formulating an operationalizing

29:41.000 --> 29:44.000
problem and like developing a nice proof, right?

29:44.000 --> 29:50.000
Um, I do think, you know, eventually we will have to do something like agent foundations

29:50.000 --> 29:55.000
to really understand like what are these things like optimization and agency?

29:55.000 --> 29:57.000
Like what are values, right?

29:57.000 --> 29:59.000
How do we actually load them into agents?

29:59.000 --> 30:00.000
What are we doing?

30:00.000 --> 30:05.000
Right now it seems we're sort of poking around in the dark, like deep learning and RLHF and

30:05.000 --> 30:09.000
it's like, okay, you know, on these data sets, it seems to work out fine.

30:09.000 --> 30:12.000
We're not really sure if it's going to work on like other data sets.

30:12.000 --> 30:18.000
We're not really sure how to define like agency in terms of our deep learning systems, right?

30:18.000 --> 30:24.000
Um, so it's kind of like, I definitely think we still should be doing alignment of deep

30:24.000 --> 30:29.000
learning, um, but like it's a bet and it might not work out.

30:29.000 --> 30:45.000
Do you think, so if we need something like agent foundation, it seems that we would need

30:45.000 --> 30:50.000
like some kind of restructuring of the research or we would need like much more people pouring

30:50.000 --> 30:52.000
into these directions.

30:52.000 --> 31:00.000
Um, and in general, I'm not sure if the evil work, let's say convincing like deep mind

31:00.000 --> 31:05.000
on tropic or something, which is like, I think the frame you've given, um, it's going to

31:05.000 --> 31:06.000
help with that.

31:06.000 --> 31:07.000
Maybe.

31:07.000 --> 31:08.000
Yeah.

31:08.000 --> 31:11.000
So I don't think it's going to help get people into agent foundations.

31:11.000 --> 31:22.000
I think it's more about, okay, let's, um, get people to care about AI safety in general.

31:22.000 --> 31:24.000
That's it.

31:24.000 --> 31:25.000
Yeah.

31:25.000 --> 31:30.000
I mean, I think like there is, um, like the agent foundations people, I think, uh, or

31:30.000 --> 31:37.000
maybe the community in general could definitely do much better job of like saying why agent

31:37.000 --> 31:42.000
foundation is important, saying why, you know, this alignment stuff is actually really, really

31:42.000 --> 31:43.000
hard.

31:43.000 --> 31:48.000
Um, it's just right now just a bunch of like less wrong in alignment for a post, um, or

31:48.000 --> 31:50.000
you talk to people in the community, right?

31:50.000 --> 31:55.000
But, you know, the community from the outside might seem a little like off putting to approach.

31:55.000 --> 31:59.000
So maybe we don't want to do that.

31:59.000 --> 32:03.000
So you're sounding, uh, you've mentioned a few times this conversation.

32:03.000 --> 32:08.000
And this whole, uh, 10 years has been thrown out.

32:08.000 --> 32:10.000
Alignment is hard has been thrown out.

32:10.000 --> 32:15.000
I guess I might as well ask the, uh, the fame question or the fame pair of question.

32:15.000 --> 32:17.000
What, uh, what is, uh, timelines?

32:17.000 --> 32:18.000
And PDOOM.

32:18.000 --> 32:19.000
PDOOM.

32:19.000 --> 32:24.000
So yeah, I think, um, did this calculation yesterday or a few days ago, the bunch of

32:24.000 --> 32:32.000
friends, something like 46% it's quite high, I think, but, uh, could be higher.

32:33.000 --> 32:34.000
Timelines.

32:34.000 --> 32:35.000
Yeah.

32:35.000 --> 32:43.000
I mean, I feel like a lot of my timeline stuff is like part of it is anchoring off of things

32:43.000 --> 32:45.000
like the bio anchors report.

32:45.000 --> 32:50.000
Another part of it is just like by 2012 image net.

32:50.000 --> 32:53.000
I was like pretty cool when it wasn't like that.

32:53.000 --> 32:54.000
Great.

32:54.000 --> 32:57.000
But now it's like, I mean, it was like a couple of years ago, right?

32:57.000 --> 32:59.000
Like two before, like maybe next week, right?

32:59.000 --> 33:02.000
It's like, yeah.

33:02.000 --> 33:07.000
Act, we, we went from like, like okay image classification to a system that's like actually

33:07.000 --> 33:10.000
able to like do tasks in the world.

33:10.000 --> 33:12.000
Um, I mean, you look at BBT, right?

33:12.000 --> 33:17.000
Like opening, I certainly wasn't throwing all of its resources into tea.

33:17.000 --> 33:18.000
Do you want to explain what the BT is?

33:18.000 --> 33:19.000
Yeah.

33:19.000 --> 33:20.000
The video pre-trained transformer.

33:20.000 --> 33:24.000
So this is recent paper from open AI where they, um, pre-trained transformer on a bunch

33:24.000 --> 33:25.000
of Minecraft YouTube videos.

33:25.000 --> 33:28.000
It's a Mar-L-HF and see how it did in Minecraft.

33:28.000 --> 33:30.000
So of course it's like not a human level, right?

33:30.000 --> 33:35.000
Um, I think it was like a one billion parameter model or something, something less like that.

33:35.000 --> 33:39.000
Um, so certainly like they could have thrown a lot more resources at this.

33:39.000 --> 33:41.000
Uh, but still it seemed to do sort of reasonable.

33:41.000 --> 33:45.000
It even was able to construct a diamond pickaxe, which is like very, very hard to do, um, in

33:45.000 --> 33:46.000
Minecraft.

33:46.000 --> 33:51.000
Um, you also have things like Gato, which like a transport pre-trained, like a bunch of RL

33:51.000 --> 33:53.000
things, uh, RL environments.

33:53.000 --> 33:56.000
Uh, and it seems to be able to like do a bunch of tasks.

33:56.000 --> 34:01.000
Um, you know, so we have like really, really good capability, um, in like certain domains.

34:01.000 --> 34:04.000
We have like, you know, increasing generality, right?

34:04.000 --> 34:07.000
I think you just need to put these two together, scale things up a little bit more.

34:07.000 --> 34:09.000
Uh, maybe add some more hacks and tricks.

34:09.000 --> 34:14.000
And it seems like we're sort of there to something that is like an AI-like system, uh, where

34:14.000 --> 34:18.000
at the very least could cause a lot of damage in the world.

34:18.000 --> 34:25.000
Um, how do you see, how do you see the rest of the world looking then?

34:25.000 --> 34:31.000
Like if you have like, you think we're quite close, like upon deployment of the system,

34:31.000 --> 34:35.000
can you see like, yeah, I think, I think it's like an issue of sometimes how the question

34:35.000 --> 34:39.000
of what's your timelines, because I'm not like, I think it very much posits some idea

34:39.000 --> 34:42.000
like, you know, your time is a 10 years.

34:42.000 --> 34:45.000
It means, you know, 10 years of one day from now, you might as well just retire.

34:45.000 --> 34:48.000
It's about shits like God, right?

34:48.000 --> 34:52.000
Um, but in my head, like, you know, the game still keeps going, right?

34:52.000 --> 34:58.000
Um, so yeah, so like, let's say you get one of these, you know, video pre-training,

34:58.000 --> 35:05.000
YouTube, behavior clone, which is software, pre-training, RLH done.

35:05.000 --> 35:12.000
Action transformative systems in like, let's say 10 years and it's deployed.

35:12.000 --> 35:16.000
Um, how do you see the world looking?

35:16.000 --> 35:19.000
What do you actually, what is actually the worry here?

35:19.000 --> 35:23.000
Like, what's, like, do you think we're still in the game?

35:23.000 --> 35:26.000
Like when you say time of 10 years, are you like, you've got 10 years and that's it?

35:26.000 --> 35:28.000
Like, yeah, what was the fight?

35:28.000 --> 35:32.000
So maybe two things to distinguish are like, what is the point at which we get an AGI

35:32.000 --> 35:37.000
or an APS AI, like advanced planning, strategically aware, like with key capabilities,

35:37.000 --> 35:40.000
like hacking or manipulation AI.

35:40.000 --> 35:44.000
Um, so yeah, I think my 10 years would be like for that.

35:44.000 --> 35:49.000
Um, then the separate question is, okay, we have such an AI, would it actually do something bad?

35:49.000 --> 35:52.000
I think this is the open question.

35:52.000 --> 35:54.000
Um, I really don't know.

35:54.000 --> 35:57.000
This depends on, okay, like what was the pre-training data?

35:57.000 --> 36:00.000
Um, what do we know about generalization and deep learning at this point?

36:00.000 --> 36:04.000
Um, like, how much do you actually have to prompt a model to do bad things

36:04.000 --> 36:06.000
or to actually do bad things?

36:06.000 --> 36:10.000
Um, like how bad of a problem are you, is, um, you know, instrumental goals

36:10.000 --> 36:15.000
with like these sorts of simulators that, um, it's unclear.

36:15.000 --> 36:18.000
Um, I think we might still be in the game for a while.

36:18.000 --> 36:22.000
Um, but you know, it's this sort of, um, notion of precarity, I think.

36:22.000 --> 36:27.000
Like, even if a model doesn't do something bad, it still might, right?

36:27.000 --> 36:31.000
So I think in this time, we really have to, like, rush, um, after we deploy this first model

36:31.000 --> 36:35.000
to, like, either solve the alignment through something like agent foundations

36:35.000 --> 36:40.000
or produce convincing enough evidence to the world that we actually cannot solve this.

36:40.000 --> 36:43.000
We need to coordinate not to build certain types of systems.

36:43.000 --> 36:45.000
I mean, yeah, I guess I'm pretty...

36:45.000 --> 36:48.000
I mean, alignment is like a public...

36:48.000 --> 36:49.000
Yeah.

36:49.000 --> 36:53.000
And if it's everybody, but like, you know, on the margin, a company like DeepMind

36:53.000 --> 36:56.000
has incentive to just, like, move forward.

36:56.000 --> 37:00.000
Yeah, I definitely think that...

37:00.000 --> 37:05.000
Yeah, I guess I might just be pessimistic.

37:05.000 --> 37:08.000
Coordination, making some abstract general sense.

37:08.000 --> 37:10.000
Um, a few people have tried to do this for a long time.

37:10.000 --> 37:13.000
It seems to just be, like, a thing that...

37:13.000 --> 37:18.000
I think there's been, like, in my head, lots of people have wanted to make the world

37:18.000 --> 37:20.000
more coordinated for a long time.

37:20.000 --> 37:23.000
And it just hasn't really happened.

37:23.000 --> 37:25.000
Yeah, for sure.

37:26.000 --> 37:28.000
Yeah, for sure.

37:28.000 --> 37:30.000
But I could definitely...

37:30.000 --> 37:32.000
I think I'm definitely more...

37:32.000 --> 37:34.000
I should put it kind of, like, benevolent.

37:34.000 --> 37:36.000
Like, you know, like DeepMind...

37:36.000 --> 37:38.000
I think, I think...

37:38.000 --> 37:40.000
So you don't really realize being in the safety community.

37:40.000 --> 37:44.000
It's just kind of, uh...

37:44.000 --> 37:46.000
Like, villainization.

37:46.000 --> 37:49.000
Like, it's like DeepMind, open AI.

37:49.000 --> 37:54.000
Like explicit, like, like, you know, talking about, like, let's say some open or something.

37:54.000 --> 38:00.000
You see for, like, caring, nice, genuine, intelligent, you know, future caring people.

38:00.000 --> 38:02.000
Damus as well, right?

38:02.000 --> 38:07.000
Um, I do have some, like, pretty good...

38:07.000 --> 38:11.000
Some pretty, like, good hope that coordination there is very much possible.

38:11.000 --> 38:13.000
Um...

38:13.000 --> 38:15.000
Yeah, I think, um...

38:15.000 --> 38:18.000
Maybe this is kind of rave, but, you know, we're all in this together.

38:18.000 --> 38:20.000
I think these are just people at the end of the day.

38:20.000 --> 38:24.000
Maybe we can't get them, but we gotta try till the last moment, right?

38:24.000 --> 38:27.000
Uh, like, this problem is so important.

38:27.000 --> 38:32.000
Uh, it'd be a shame if it just so happened we were in the world in which coordination would have actually worked,

38:32.000 --> 38:34.000
but we just didn't try.

38:34.000 --> 38:36.000
How embarrassing would that be?

38:36.000 --> 38:38.000
The dignity.

38:38.000 --> 38:41.000
Um, yeah, I don't know.

38:41.000 --> 38:46.000
So I think it totally makes sense to be pessimistic about, like, solving all of this about coordination,

38:46.000 --> 38:53.000
and then about alignment, um, about, like, any other, other stuff that could, like, you know, go wrong, like, S-risk.

38:53.000 --> 39:00.000
Um, like, I think it totally makes sense, and, like, I definitely don't want to, like, judge anybody that is just, like,

39:00.000 --> 39:03.000
you know, decided not to work on any of this because it's too depressing.

39:03.000 --> 39:04.000
Like, that's totally fair.

39:04.000 --> 39:06.000
This is, like, super, super hard.

39:06.000 --> 39:10.000
Um, but I think, I guess, personally, my perspective is, like, okay.

39:10.000 --> 39:15.000
You know, I, maybe, this just comes from me having a lot of uncertainty about a lot of things in my life.

39:15.000 --> 39:20.000
Um, so, you know, on the off chance that, like, we could actually make a difference,

39:20.000 --> 39:22.000
it seems worth it to try.

39:22.000 --> 39:28.000
Um, even if, sort of, maybe the objective belief is that, okay, like, maybe it's not worth trying, if we actually did the ease.

39:30.000 --> 39:38.000
I mean, I think this is a, um, the, the, the extreme deal, I guess, you'd take killer makes this point,

39:38.000 --> 39:39.000
in a way, certainly.

39:39.000 --> 39:46.000
Both in expectation, or he makes the point that, in expectation, the focus should be something like this.

39:46.000 --> 39:52.000
Do, like, you know, do the thing that seems, like, the most defined doesn't seem like this.

39:52.000 --> 39:55.000
You know, what does it, what does that mean, like?

39:55.000 --> 40:02.000
I guess, I guess he's just saying, he's trying to make the point that, like, because in his worldview, he's, like, you know,

40:02.000 --> 40:07.000
a several nines bed, probably, like, not that far away.

40:07.000 --> 40:14.000
And he makes the point that, like, yeah, like, when you're, when you're acting in the face of, like, trying to increase such small probabilities,

40:14.000 --> 40:20.000
or acting in the face of causing a problem that seems so hard, you shouldn't follow the heuristic of, like, what should I do to solve it?

40:20.000 --> 40:25.000
And because everything seems doomed to you, you should be, like, what's the most dignified way?

40:25.000 --> 40:32.000
It'd be more dignified if, like, OpenAI and DeepMind at least did some coordination before, like, some other company went and built AGI.

40:32.000 --> 40:40.000
It'd be more dignified if, like, we really tried to scale mechanistic interpretability, or had, like, at least had evals, had some...

40:40.000 --> 40:46.000
And he claims that it's, like, a more robust year signal, and also more motivating for oneself.

40:46.000 --> 40:50.000
Oh, so the argument is, like, dignity isn't the thing we inherently care about.

40:50.000 --> 40:54.000
It's, like, this heuristic that actually gets us to, like, reduce risk.

40:54.000 --> 40:58.000
Yeah, he's, like, particularly for him.

40:58.000 --> 41:03.000
The property is, like, oh, like...

41:03.000 --> 41:07.000
The framing shouldn't be, is this going to solve a line?

41:07.000 --> 41:12.000
Because nothing is. The framing is, like, is this a more dignified world?

41:12.000 --> 41:13.000
Interesting.

41:13.000 --> 41:16.000
And that's kind of, like, he approached the problem.

41:16.000 --> 41:19.000
He said it, like, a lot more...

41:19.000 --> 41:21.000
He didn't phrase it as nicely as that.

41:21.000 --> 41:26.000
But, you know, I think this is his general point.

41:26.000 --> 41:29.000
I think if you could make some...

41:29.000 --> 41:35.000
Also, I think I have some term, like, value on humanity, but, like...

41:35.000 --> 41:38.000
I'd rather, like, try it and, like, at least have tried.

41:38.000 --> 41:40.000
Yeah, that's right, that's right.

41:40.000 --> 41:43.000
Even when you know it's hopeless, we just keep on trying.

41:43.000 --> 41:45.000
That's what we do.

41:45.000 --> 41:49.000
I mean, I don't think... I mean, I'm not that much.

41:49.000 --> 41:52.000
Quite not much to me.

41:52.000 --> 41:54.000
This is interesting.

41:54.000 --> 41:58.000
But if we did solve that, that'd be absolutely sick.

41:58.000 --> 42:00.000
Oh, here's a question now, Lois.

42:00.000 --> 42:04.000
So, you're talking about...

42:04.000 --> 42:09.000
You care about the kind of, you know, fattest stuff, justice stuff.

42:09.000 --> 42:12.000
I think, obviously, territory.

42:12.000 --> 42:15.000
People right now, their hearts are being burned.

42:15.000 --> 42:17.000
No one wants to slow down.

42:17.000 --> 42:19.000
We all seem to make some implication, though.

42:19.000 --> 42:23.000
Even as a cold hearted, long-term future caring to the Italian.

42:23.000 --> 42:27.000
You should care about this, because these things might be locked in.

42:27.000 --> 42:30.000
Like, the society in the future might be locked in.

42:30.000 --> 42:34.000
Um...

42:34.000 --> 42:37.000
I think...

42:37.000 --> 42:39.000
I have some intuition.

42:39.000 --> 42:44.000
I'm not sure, but at least I think a fair kind of pushback against this

42:44.000 --> 42:49.000
is the idea that, like, there's just not that much time

42:49.000 --> 42:51.000
where society looks like it does now.

42:51.000 --> 42:53.000
Like, what you get is a kind of AGI thing,

42:53.000 --> 42:56.000
and you get, like, self-improvement for a bit.

42:56.000 --> 42:59.000
At some point, you just have, like, a super-intelligent God.

42:59.000 --> 43:01.000
And, like...

43:01.000 --> 43:04.000
Whatever that thing is doing, or whatever that thing wants to do,

43:04.000 --> 43:07.000
is what decides what the society looks like.

43:07.000 --> 43:10.000
Not necessarily, like, the current dynamics,

43:10.000 --> 43:13.000
or, like, those dynamics, like, pushed into the future.

43:14.000 --> 43:16.000
Um, how do you feel about that as a state?

43:16.000 --> 43:18.000
Uh, I think this is a possibility,

43:18.000 --> 43:20.000
but, again, I have a lot of uncertainty

43:20.000 --> 43:22.000
about whether this will actually happen.

43:22.000 --> 43:26.000
Like, I think, yeah, so maybe the thing that you're saying is,

43:26.000 --> 43:29.000
okay, we have this, like, super-intelligent AGI,

43:29.000 --> 43:31.000
it's, like, it's how I'm all figured out,

43:31.000 --> 43:34.000
like, what are the true human values that, like, everybody cares.

43:34.000 --> 43:37.000
And, like, it makes sure everything is okay,

43:37.000 --> 43:41.000
or at least, like, gives us time to figure out all these human values, right?

43:41.000 --> 43:43.000
Yeah, like, I think that'd be great.

43:43.000 --> 43:45.000
I'm not sure this is actually going to...

43:45.000 --> 43:49.000
Then, like, one is, like, SEV even possible.

43:49.000 --> 43:53.000
Number two, okay, like, suppose OpenAI has built the AGI,

43:53.000 --> 43:56.000
like, what do they do with it?

43:56.000 --> 43:59.000
I think, like, A, the temptation to do something,

43:59.000 --> 44:03.000
to instantiate, you know, your own values is just way, way too strong.

44:03.000 --> 44:06.000
If you actually have a God at your hands.

44:06.000 --> 44:10.000
Um, number two, I mean, even if they, like,

44:10.000 --> 44:13.000
try not to do anything with it, like,

44:13.000 --> 44:16.000
I think for sure other parties are going to want that kind of power, too.

44:16.000 --> 44:19.000
The US government, China, maybe even other companies, right?

44:19.000 --> 44:21.000
What happens when there's conflict?

44:21.000 --> 44:23.000
What does OpenAI do?

44:23.000 --> 44:26.000
Do they just obliterate other entities?

44:26.000 --> 44:29.000
That's, like, wild, right?

44:29.000 --> 44:31.000
If they did, I think for sure, like,

44:31.000 --> 44:35.000
general populace would be like, what the fuck is going on with OpenAI?

44:35.000 --> 44:38.000
Like, maybe we should go to war with this, right?

44:38.000 --> 44:41.000
Um, this doesn't seem like a good future, either.

44:41.000 --> 44:44.000
So, I think there are just a lot of factors that, like,

44:44.000 --> 44:46.000
maybe we actually do need to figure out right now,

44:46.000 --> 44:49.000
like, what is the game plan when we have AGI?

44:49.000 --> 44:51.000
Um, to ensure that, like, we get that kind of future

44:51.000 --> 44:53.000
where we have the time to think about, okay,

44:53.000 --> 44:55.000
like, what are the things we actually want?

44:55.000 --> 44:58.000
And we have, like, the luxury, you know, to work on,

44:58.000 --> 45:00.000
okay, like, it's a limited resource scarcity, right?

45:00.000 --> 45:02.000
Um, let's start at a limited discrimination somehow,

45:02.000 --> 45:04.000
like, maybe not with the AGI, but, like,

45:04.000 --> 45:06.000
because we don't have other problems,

45:06.000 --> 45:10.000
we focus our, like, time and energy on, like, these, these social problems.

45:10.000 --> 45:13.000
Yeah, so as you say right now, like, something like,

45:13.000 --> 45:16.000
you still see society, like,

45:16.000 --> 45:19.000
affecting the long-term future,

45:19.000 --> 45:23.000
even in the face of, like, an incredibly advanced powerful system.

45:23.000 --> 45:25.000
You think there's still, there are actions, right?

45:25.000 --> 45:29.000
Everything still changes based on what a site looks like as well.

45:29.000 --> 45:30.000
Yeah, yeah.

45:30.000 --> 45:33.000
So, this all depends on the kind of AGI we have.

45:33.000 --> 45:35.000
So, I think on the one hand, like,

45:35.000 --> 45:38.000
suppose that this AGI is, in some sense,

45:38.000 --> 45:40.000
value-free or value-neutral.

45:40.000 --> 45:43.000
Um, okay, then, like, it's going to be aligned,

45:43.000 --> 45:45.000
okay, suppose we solve a line, right?

45:45.000 --> 45:47.000
Then it's going to be aligned with, like, whoever's controlling it.

45:47.000 --> 45:50.000
Um, like, okay, if it's, like, open AI,

45:50.000 --> 45:52.000
then you run into the problems that, like, I talked about, right?

45:52.000 --> 45:57.000
Um, like, conflict or just, like, you know, some sort of dictatorship or something.

45:57.000 --> 46:01.000
Uh, okay, suppose that, actually, in the meantime,

46:01.000 --> 46:05.000
we sort of solve parts of, like, moral philosophy.

46:05.000 --> 46:08.000
So, now, like, this AGI actually has, like, reasonable values,

46:08.000 --> 46:13.000
um, that, you know, like, the vast majority of humanity would agree with, right?

46:13.000 --> 46:15.000
And even if, you know, it's overseers,

46:15.000 --> 46:17.000
thinks it's just do something, it actually doesn't,

46:17.000 --> 46:19.000
because they know it's better, in some sense.

46:19.000 --> 46:22.000
Um, okay, like, I think this seems, like, sort of reasonable, right?

46:22.000 --> 46:24.000
But the difficulty is getting...

46:24.000 --> 46:27.000
I don't think anybody's really working on this in the AI safety community,

46:27.000 --> 46:33.000
figuring out, like, you know, what do we do, like, about different moral systems, for instance?

46:33.000 --> 46:36.000
Like, um, like, what is the answer to moral uncertainty?

46:36.000 --> 46:39.000
Is it, like, moral parliaments? Is it, like, something else?

46:39.000 --> 46:44.000
Um, yeah, so, like, it seems that the first...

46:44.000 --> 46:48.000
Yeah, on the first path, um, I don't know, conflict seems, like, pretty bad.

46:48.000 --> 46:51.000
On the second path, well, we haven't really done much work towards this.

46:51.000 --> 46:54.000
Um, so I'm not very, like...

46:54.000 --> 46:58.000
I think I'm not very, like, optimistic about, um, you know,

46:58.000 --> 47:03.000
the world, like, going well, even if we solve a line mid-term.

47:03.000 --> 47:06.000
Yet, somebody should work on this, though.

47:12.000 --> 47:18.000
What's your biggest frustrations with the AI safety community?

47:18.000 --> 47:20.000
Biggest frustrations? Damn.

47:20.000 --> 47:24.000
Like, it can't save me a second.

47:24.000 --> 47:28.000
If I can shout it out later, I already don't hear it.

47:28.000 --> 47:31.000
Yeah, frustrations.

47:31.000 --> 47:33.000
Margot's definitely going to tweet this.

47:33.000 --> 47:36.000
Let me ask the question again, sorry.

47:36.000 --> 47:41.000
What's your biggest frustration with the AI safety community?

47:41.000 --> 47:43.000
Frustrations.

47:46.000 --> 47:48.000
AI safety community frustrations.

47:48.000 --> 47:51.000
What is AI safety?

47:51.000 --> 47:54.000
Perfect. Okay.

47:57.000 --> 47:59.000
Alan, in your opinion...

47:59.000 --> 48:00.000
Max.

48:00.000 --> 48:03.000
Right, let's go ahead.

48:04.000 --> 48:08.000
Alan, in your opinion, what is AI safety?

48:10.000 --> 48:13.000
I think there's a broad version of this term,

48:13.000 --> 48:16.000
maybe several broad versions of this term, and several narrow versions.

48:16.000 --> 48:20.000
So, I think the narrow version, of course, is, like,

48:20.000 --> 48:22.000
of course, meaning between us.

48:22.000 --> 48:24.000
AI existentialcy.

48:24.000 --> 48:27.000
So, how do you prevent AI from being existential risk,

48:27.000 --> 48:31.000
whether it's through empowerment or human extinction or something else?

48:31.000 --> 48:34.000
There's, like, broader versions of AI safety, too,

48:34.000 --> 48:36.000
that include more than existential risk.

48:36.000 --> 48:39.000
So, you might include S-risks, which care a lot about, like,

48:39.000 --> 48:42.000
suffering caused by artificial intelligence,

48:42.000 --> 48:45.000
either through conflict or, like, something else.

48:45.000 --> 48:48.000
And I think there's an even broader notion of AI safety,

48:48.000 --> 48:51.000
which, like, in my mind, this is the ideal definition of AI safety,

48:51.000 --> 48:54.000
and it encompasses, like, literally everything, right?

48:54.000 --> 48:57.000
Like, we care about, like, all the negative consequences from AI,

48:57.000 --> 49:00.000
and we try to draw the threads, like, between all these phenomena

49:00.000 --> 49:03.000
we're observing and, like, core technical and social problems.

49:03.000 --> 49:07.000
So, this includes things, like, the things that people study in fairness, right?

49:07.000 --> 49:11.000
Like, AI is that, like, are really able to, like,

49:11.000 --> 49:15.000
learn what our fairness judgments are.

49:15.000 --> 49:19.000
AI that, like, just exacerbate discrimination that is already present in society

49:19.000 --> 49:23.000
and that is present in data sets that we use to train these AIs.

49:23.000 --> 49:28.000
So, I think that's, like, that broad definition, to me, is the ideal definition,

49:28.000 --> 49:31.000
the one we can all get behind, you know, so that we can do things

49:31.000 --> 49:34.000
that we practically agree on, like, more regulation,

49:34.000 --> 49:38.000
slowing down AI development, more like verification of AIs

49:38.000 --> 49:42.000
before we deploy them.

49:42.000 --> 49:46.000
Okay, given that definition,

49:46.000 --> 49:50.000
um, and maybe focusing on the narrow

49:50.000 --> 49:54.000
AI safety x-race definition,

49:54.000 --> 49:58.000
um, what's your biggest frustration with the community or the set of people

49:58.000 --> 50:02.000
currently works on?

50:02.000 --> 50:05.000
Is it far that they are able to be homogenized?

50:05.000 --> 50:09.000
Or maybe you should, like, go into more specific subsets of this community.

50:09.000 --> 50:12.000
But yeah, the people that worry about the x-race AIs.

50:12.000 --> 50:15.000
So, I have actually been doing this for that long.

50:15.000 --> 50:19.000
I think, um, I've been doing, like, or I guess I've been considering myself

50:19.000 --> 50:22.000
a part of this community for, like, maybe a year and a half?

50:22.000 --> 50:26.000
Two years, if? So, like, basically just for my PhD?

50:26.000 --> 50:30.000
Um, yeah. So, I mean, in the short amount of time,

50:30.000 --> 50:34.000
I think it's been, like, pretty great so far just finding people

50:34.000 --> 50:38.000
who care about the same things that I do and are motivated to work similarly.

50:38.000 --> 50:41.000
I think this is definitely, like, a big, like, anti-frustration.

50:41.000 --> 50:45.000
I guess it's, like, I think it's very frustrating working on something, like, by yourself.

50:45.000 --> 50:49.000
Um, and, like, thinking you're the only person around who cares about the problem.

50:49.000 --> 50:52.000
And everybody else thinks you're on the crazy train or something.

50:52.000 --> 50:56.000
Um, yeah, maybe one frustration, though, is I think, um,

50:56.000 --> 51:01.000
it'd be, like, a lot better if more people in the AI x-safety community

51:01.000 --> 51:05.000
were, like, more explicitly wanting to build bridges

51:05.000 --> 51:10.000
to other communities that also care about safety in, like, the broad, broad sense.

51:10.000 --> 51:14.000
So, in particular, and, like, to the, like, fairness and, like, ethics.

51:14.000 --> 51:18.000
Um, just because I think coordination is a super important thing for us to be doing

51:18.000 --> 51:22.000
and might even be a low-tech fruit, um, to, like, slow down AI development

51:22.000 --> 51:26.000
and make sure we don't, um, like, face negative consequences.

51:26.000 --> 51:31.000
Um, yeah, I guess that'd be the, um, the main thing for me.

51:36.000 --> 51:39.000
Do you see this community changing a lot?

51:39.000 --> 51:44.000
Like, let's say, by this community, let's say, the set of people that, if you ask them,

51:44.000 --> 51:48.000
what do you do, they would say something along the lines of,

51:48.000 --> 51:55.000
I work to ensure that, um, extras from AI.

51:55.000 --> 51:59.000
Do you see, like, that set of people changing, like, for years?

51:59.000 --> 52:01.000
I think more people get on the train.

52:01.000 --> 52:04.000
The same trains we are on right now.

52:04.000 --> 52:09.000
Yeah, um, you know, AI is being deployed much more.

52:09.000 --> 52:12.000
Um, these, like, ridiculous generative models, right?

52:12.000 --> 52:18.000
It's, like, wild when you're put out of a job by an AI in, like, 2025.

52:18.000 --> 52:21.000
But the name we predicted, 26, right?

52:21.000 --> 52:24.000
Um, so, so, so, this is a good thing.

52:24.000 --> 52:28.000
I guess, um, I hope that, like, you know, and getting more people on this train,

52:28.000 --> 52:32.000
we also make sure that we repair the bridges that don't exist

52:32.000 --> 52:35.000
or have been burned between the different safety communities.

52:35.000 --> 52:39.000
Um, not sure how this is going to happen,

52:39.000 --> 52:43.000
but, um, I think a good first step is just talking to people,

52:43.000 --> 52:46.000
um, going to the places that they frequent.

52:46.000 --> 52:49.000
Like, you know, I think definitely some AI safety people, like,

52:49.000 --> 52:51.000
AI safety people should just go to, like, FACT,

52:51.000 --> 52:54.000
like, the biggest conference in Ferris that's held yearly

52:54.000 --> 52:56.000
and just, like, talk to people about concerns.

52:56.000 --> 52:58.000
We should, like, submit papers there.

52:58.000 --> 53:01.000
We should, like, actually try to understand, like, the fact people are saying,

53:01.000 --> 53:05.000
um, you know, like, do some social theory, like, study some psychology.

53:05.000 --> 53:10.000
Um, like, really think about, like, how AI is going to interact with society.

53:10.000 --> 53:14.000
Like, maybe as well, we should try to develop, like, some sort of, um,

53:14.000 --> 53:18.000
I guess, uh, point of view that is not just techno-optimist.

53:18.000 --> 53:22.000
Like, you know, even if we solve a line, what are the problems that are left?

53:26.000 --> 53:31.000
How similar do you think the technical problems are between an escape team?

53:38.000 --> 53:39.000
So much similar.

53:39.000 --> 53:47.000
So, I think one particular technical problem in fairness is

53:47.000 --> 53:56.000
what, so in a particular context, let's say, um, I don't know, like,

53:56.000 --> 54:00.000
hiring in this particular country with this particular historical context,

54:00.000 --> 54:03.000
um, what do we do?

54:03.000 --> 54:06.000
Like, what are the conceptions of fairness that are at play?

54:06.000 --> 54:11.000
Can we, like, learn these things and formalize them in a certain way

54:11.000 --> 54:14.000
so that we can actually try to understand what's, what's going on

54:14.000 --> 54:16.000
and come to consensus about what we're doing?

54:16.000 --> 54:20.000
Um, I mean, I think the techniques that we're coming up with in AI safety

54:20.000 --> 54:22.000
are, like, super super rolling for this, right?

54:22.000 --> 54:25.000
Like, if we do RLHs to actually learn people's preferences,

54:25.000 --> 54:27.000
then we, like, study the reward function.

54:27.000 --> 54:29.000
I think that might give us valuable insight, actually,

54:29.000 --> 54:32.000
about what people actually think about in fairness.

54:32.000 --> 54:35.000
Um, I think general stuff, too, like, you know,

54:35.000 --> 54:38.000
anytime we think about, um, generalization problem in AI safety,

54:38.000 --> 54:41.000
I mean, these are also relevant in fairness,

54:41.000 --> 54:44.000
because, like, fairness problems are just, like, one-shot,

54:44.000 --> 54:47.000
oh, like, the trained test distribution are, like, going to be the same, right?

54:47.000 --> 54:49.000
Things are changing in the real world as well.

54:49.000 --> 54:52.000
Um, so totally, I think, um, that thing is also relevant.

54:52.000 --> 54:56.000
Uh, another thing is just, like, um, all this stuff about, like, instrumental goals

54:56.000 --> 54:58.000
and, like, reinforcement learning systems and agents, right?

54:58.000 --> 55:01.000
Like, if we're going to be deploying, um,

55:01.000 --> 55:04.000
like, algorithms in society that make consequential decisions

55:04.000 --> 55:07.000
about people's welfare, well, these are going to be decisions

55:07.000 --> 55:09.000
that are going to be made over time.

55:09.000 --> 55:13.000
Um, and in making decisions over time, like, we don't want, like,

55:13.000 --> 55:17.000
a license we're deploying to, like, do really weird things,

55:17.000 --> 55:19.000
um, have really weird instrumental goals.

55:19.000 --> 55:23.000
Um, so, I think the connection to me seems, like, pretty clear,

55:23.000 --> 55:27.000
but it hasn't been communicated well, which is pretty unfortunate.

55:32.000 --> 55:36.000
You have a, uh, center of long-term risk.

55:36.000 --> 55:40.000
I do, I do. The size too small, I think.

55:40.000 --> 55:42.000
Well, I think so as well.

55:42.000 --> 55:45.000
Well played, well played.

55:45.000 --> 55:50.000
Um, what's this whole, uh, it's all S-risk thing.

55:50.000 --> 55:54.000
S-risks, okay. Well, to learn...

55:54.000 --> 55:59.000
Sure. What's this whole, uh, X-risk, S-risk, C-risk, you know?

55:59.000 --> 56:01.000
So, I'm not sure what C-risk is, but, uh...

56:01.000 --> 56:03.000
It's, uh, it's, uh, it's kind of strong.

56:03.000 --> 56:05.000
Oh, I see, I see it.

56:05.000 --> 56:06.000
Right, right.

56:06.000 --> 56:08.000
Those, those things.

56:08.000 --> 56:11.000
Yeah, so X-risk is existential risk.

56:11.000 --> 56:16.000
Um, I think of this as problems that would, um,

56:16.000 --> 56:20.000
either result in human extinction or in, like,

56:20.000 --> 56:27.000
the lost capacity of, um, humans to, like, do things in the future.

56:27.000 --> 56:30.000
And maybe, like, when, yeah, when I say things, I mean great things.

56:30.000 --> 56:33.000
Maybe when I think about great things, it's, like, somewhat dying.

56:33.000 --> 56:35.000
I mean, I think it'd be pretty...

56:36.000 --> 56:38.000
You know, we, like, travel the star.

56:38.000 --> 56:42.000
Like, you know, we see another species who try to help.

56:42.000 --> 56:46.000
Um, I don't know, uh, there seems to be a lot of things

56:46.000 --> 56:48.000
like we could do in the universe, like improve our understanding,

56:48.000 --> 56:51.000
like, really just go through and review our history

56:51.000 --> 56:53.000
and try to become, like, better agents, right?

56:53.000 --> 56:56.000
All of this becomes impossible if, like, we're extinct

56:56.000 --> 56:59.000
or if, like, we are trapped on, like, a planet

56:59.000 --> 57:02.000
that has had all the resources, um, depleted.

57:02.000 --> 57:04.000
Um, so that to me is an existential risk.

57:04.000 --> 57:08.000
Um, a, an extraterrestrial risk is, um, a suffering risk.

57:08.000 --> 57:11.000
So these are things that could cause, like, the, you know,

57:11.000 --> 57:14.000
like, really, really, like, lots of suffering in the world.

57:14.000 --> 57:19.000
Um, so what are some examples of things that cause a lot of suffering today?

57:19.000 --> 57:22.000
Factory farming, arguably. Um, it's, like, absolutely terrible

57:22.000 --> 57:24.000
how we treat our animals today.

57:24.000 --> 57:30.000
Um, what are the things that could cause a lot of, um, suffering?

57:34.000 --> 57:36.000
Um, dictatorship.

57:36.000 --> 57:38.000
Dictatorships, yeah, malevolence is a lot of...

57:38.000 --> 57:44.000
Yeah, so, um, yes, so I guess, like, there are some more mundane things

57:44.000 --> 57:48.000
than there are things that, um, are a bit more, like, hypothetical

57:48.000 --> 57:50.000
but seem like they could happen given the technologies

57:50.000 --> 57:51.000
that we're developing right now.

57:51.000 --> 57:54.000
Yes, the mundane things, you know, there's, um, there's factory farming.

57:54.000 --> 58:00.000
I think there's also just, like, wars, you know, um, like, whenever...

58:00.000 --> 58:02.000
So not just, like, the death tolls in the wars themselves,

58:02.000 --> 58:04.000
like, what war brings with it, right?

58:04.000 --> 58:08.000
So, like, just disease, like, malnutrition, like, general instability, right?

58:08.000 --> 58:12.000
Um, that, that seems to have caused, like, a huge amount of suffering in history.

58:12.000 --> 58:15.000
Um, so, so that's sort of, like, the more mundane things.

58:15.000 --> 58:19.000
Um, I mean, mundane, I mean, it's like, there's something bad.

58:19.000 --> 58:21.000
The things that we already know.

58:21.000 --> 58:25.000
Um, there are things that, like, maybe we don't have yet, like, we could have.

58:25.000 --> 58:27.000
Um, so hypothetically, you know, if we, like,

58:27.000 --> 58:29.000
managed to become a galaxy-faring civilization

58:29.000 --> 58:31.000
and we spread factory farming to the stars,

58:31.000 --> 58:34.000
like, trillions and trillions of animals are suffering in factory farming.

58:34.000 --> 58:36.000
It just, like, seems horrific, right?

58:36.000 --> 58:38.000
Um, another thing is, okay, like,

58:38.000 --> 58:40.000
suppose that we have really, really good AI systems

58:40.000 --> 58:44.000
that, like, control large portions of, like, the Earth's resources,

58:44.000 --> 58:47.000
like, the Solar System's resources, the Galaxy's resources.

58:47.000 --> 58:51.000
Well, if they engage in conflict-like wars, that also seems really, really bad.

58:51.000 --> 58:54.000
Like, our wars multiplied by, like, a million or something, right?

58:54.000 --> 58:58.000
So, um, working as a risk, um, just to, like, really map out

58:58.000 --> 59:00.000
what are the causes of suffering

59:00.000 --> 59:03.000
and what are the ways we can, like, how can we act or reduce?

59:03.000 --> 59:06.000
Um, so I think, like, one difference between ex-risk and ex-risk

59:06.000 --> 59:09.000
is, like, sort of, where you start from, um,

59:09.000 --> 59:12.000
in terms of, like, what you care about in your moral theories,

59:12.000 --> 59:15.000
like, people in ex-risk really, really care about suffering.

59:15.000 --> 59:19.000
And I would say they prioritize this to, like, differing extents over, over pleasure.

59:19.000 --> 59:22.000
So, like, you know, between, like, if you have equivalent amounts of suffering

59:22.000 --> 59:24.000
and, like, pleasure, you would definitely, like,

59:24.000 --> 59:27.000
prefer reducing the suffering more about, like, one unit

59:27.000 --> 59:30.000
than, like, increasing somebody's pleasure or, like, you know, happiness

59:30.000 --> 59:32.000
by another unit.

59:35.000 --> 59:37.000
And...

59:37.000 --> 59:39.000
in so far, you can speak...

59:39.000 --> 59:41.000
for...

59:41.000 --> 59:43.000
I cannot. I cannot speak.

59:43.000 --> 59:45.000
I cannot speak. I cannot speak.

59:45.000 --> 59:46.000
Too many of them.

59:46.000 --> 59:48.000
I mean, I was just, I was just an intern.

59:48.000 --> 59:51.000
I don't work there as a full-time employee.

59:52.000 --> 59:59.000
Where do you think your views differ the most

59:59.000 --> 01:00:01.000
from the mainstream

01:00:01.000 --> 01:00:03.000
AI safety research?

01:00:05.000 --> 01:00:07.000
I think suffering is important.

01:00:07.000 --> 01:00:10.000
I think cooperation is pretty important work.

01:00:10.000 --> 01:00:12.000
Um...

01:00:12.000 --> 01:00:15.000
Yeah, so, I mean, I think this has been a common answer.

01:00:15.000 --> 01:00:17.000
I've a lot of uncertainty about this.

01:00:17.000 --> 01:00:20.000
Um, I think I'm still in the process of

01:00:20.000 --> 01:00:23.000
figuring this out because, like, S risks are still, like,

01:00:23.000 --> 01:00:25.000
kind of new to me.

01:00:25.000 --> 01:00:29.000
Yeah, I guess what I differ the most is

01:00:29.000 --> 01:00:31.000
I care about cooperation.

01:00:31.000 --> 01:00:35.000
I think it's important to get this work done, like...

01:00:35.000 --> 01:00:37.000
What do you like, cooperation?

01:00:37.000 --> 01:00:42.000
Yeah, so cooperation meaning, um, suppose you have, um, two agents,

01:00:42.000 --> 01:00:44.000
like, they're in some sort of game.

01:00:44.000 --> 01:00:46.000
How do you make sure that the outcome

01:00:46.000 --> 01:00:49.000
is actually a pre-do-off-camel outcome?

01:00:49.000 --> 01:00:52.000
So, for example, if you're both playing a game of chicken, right,

01:00:52.000 --> 01:00:54.000
how do you make sure you don't, like, crash into each other

01:00:54.000 --> 01:00:57.000
and, like, cause astronomical loss?

01:00:57.000 --> 01:01:00.000
Um, you know, because, like, maybe, like, your one agent,

01:01:00.000 --> 01:01:03.000
like, maybe one agent is, like, you know, this nation

01:01:03.000 --> 01:01:05.000
that has a nuclear stockpile,

01:01:05.000 --> 01:01:07.000
and, like, this other agent is, like, another nation

01:01:07.000 --> 01:01:09.000
that has a nuclear stockpile.

01:01:09.000 --> 01:01:11.000
And, like, these two nations together are, like,

01:01:11.000 --> 01:01:13.000
the entire Earth, they have huge empires.

01:01:13.000 --> 01:01:15.000
Um, a game of chicken is, like, okay, like,

01:01:15.000 --> 01:01:17.000
you both launch nukes at each other,

01:01:17.000 --> 01:01:20.000
right, we definitely don't want that kind of thing to happen.

01:01:22.000 --> 01:01:24.000
So, uh...

01:01:24.000 --> 01:01:27.000
I think a lot of us, like, we brought you here to find out

01:01:27.000 --> 01:01:29.000
about your origin stories, you know.

01:01:29.000 --> 01:01:30.000
Or origin stories.

01:01:30.000 --> 01:01:32.000
Yeah. How did, how did Mr. Alan Chan

01:01:32.000 --> 01:01:34.000
come to where

01:01:34.000 --> 01:01:37.000
the hoodie that sent a long-term risk,

01:01:37.000 --> 01:01:40.000
come to call himself an AI safety research?

01:01:40.000 --> 01:01:42.000
Well, I just, like, took this hoodie.

01:01:42.000 --> 01:01:45.000
I remember having to see all our others.

01:01:45.000 --> 01:01:46.000
Okay, okay.

01:01:46.000 --> 01:01:47.000
But the other bit.

01:01:47.000 --> 01:01:48.000
The other bit. Okay.

01:01:48.000 --> 01:01:50.000
Yeah, where should I begin?

01:01:50.000 --> 01:01:51.000
How far back?

01:01:51.000 --> 01:01:53.000
I'm actually, I think you know what, I want to hear from, like...

01:01:53.000 --> 01:01:54.000
Okay. From what?

01:01:54.000 --> 01:01:56.000
September 1st, 1996.

01:01:56.000 --> 01:01:57.000
Yeah.

01:01:57.000 --> 01:02:00.000
Um, probably at night, probably 12 a.m. or something.

01:02:00.000 --> 01:02:04.000
Okay, I was born in Edmonton, Alberta, Canada.

01:02:04.000 --> 01:02:07.000
To two parents, immigrants, Vietnam.

01:02:07.000 --> 01:02:10.000
So, uh, yeah.

01:02:10.000 --> 01:02:12.000
How much detail?

01:02:12.000 --> 01:02:13.000
Just, you know, whatever.

01:02:13.000 --> 01:02:15.000
Whatever's necessary to understand.

01:02:15.000 --> 01:02:17.000
To live it as if you're in your shoes.

01:02:17.000 --> 01:02:18.000
Yeah.

01:02:18.000 --> 01:02:19.000
So, I don't know.

01:02:19.000 --> 01:02:22.000
Why do I work on AI safety?

01:02:22.000 --> 01:02:25.000
So, I think part of it is,

01:02:25.000 --> 01:02:30.000
why do I work on things that could go wrong,

01:02:30.000 --> 01:02:31.000
I guess, I don't know.

01:02:31.000 --> 01:02:33.000
No, no, I'm wanting, I'm wanting to story.

01:02:33.000 --> 01:02:34.000
It's play-by-play.

01:02:34.000 --> 01:02:38.000
What made, from, from, you know, let's say starting...

01:02:38.000 --> 01:02:39.000
Yeah.

01:02:39.000 --> 01:02:42.000
...to sitting down here.

01:02:42.000 --> 01:02:43.000
What happened?

01:02:43.000 --> 01:02:44.000
Yeah.

01:02:44.000 --> 01:02:45.000
Okay.

01:02:45.000 --> 01:02:47.000
So, I think there's like a little bit more to this than the undergrad.

01:02:47.000 --> 01:02:48.000
Okay.

01:02:48.000 --> 01:02:52.000
I think this also depends on how I ended up developing my moral motivations.

01:02:52.000 --> 01:02:53.000
Okay.

01:02:53.000 --> 01:02:55.000
Like, why didn't I just like go and be like an investment banker, right?

01:02:55.000 --> 01:02:58.000
And like, literally just like a regular investment banker.

01:02:58.000 --> 01:03:01.000
Like, none of that earned against us.

01:03:01.000 --> 01:03:02.000
Yeah.

01:03:02.000 --> 01:03:06.000
I mean, I think like part of it is, is my upbringing.

01:03:06.000 --> 01:03:09.000
Like, I think, like, my family is like pretty Buddhist.

01:03:09.000 --> 01:03:12.000
They care, and in Buddhism, you care a lot about reducing the suffering.

01:03:12.000 --> 01:03:16.000
And particularly, my mother cares a lot about Buddhism and reducing suffering.

01:03:16.000 --> 01:03:18.000
So, I think just growing up in an environment like,

01:03:18.000 --> 01:03:21.000
they need care about these things as well.

01:03:21.000 --> 01:03:24.000
And to the extent that I saw like suffering in the world,

01:03:24.000 --> 01:03:29.000
whether it was like on the news or like interpersonally,

01:03:29.000 --> 01:03:33.000
you know, that that seemed like a really bad thing.

01:03:33.000 --> 01:03:35.000
So, I think that's like one part of it.

01:03:35.000 --> 01:03:39.000
Another part is just like being exposed to like the things

01:03:39.000 --> 01:03:41.000
that I think make life really worth living,

01:03:41.000 --> 01:03:43.000
like just like hanging out with your friends,

01:03:43.000 --> 01:03:47.000
like doing fun things, trying new foods, traveling.

01:03:47.000 --> 01:03:51.000
So, I think like both the upside and the downside,

01:03:51.000 --> 01:03:54.000
that like I was able to experience in my life,

01:03:54.000 --> 01:03:58.000
like I think maybe believe that, okay, you know,

01:03:58.000 --> 01:04:02.000
it seems like really important to make sure that they are,

01:04:02.000 --> 01:04:05.000
to like remove the downsides for as many people as we can,

01:04:05.000 --> 01:04:08.000
and to make sure that like people can actually, you know,

01:04:08.000 --> 01:04:11.000
experience the upside in life.

01:04:11.000 --> 01:04:13.000
So, I think that's like general motivation, I guess,

01:04:13.000 --> 01:04:19.000
for like working on social causes or causes to like reduce risk

01:04:19.000 --> 01:04:22.000
in like some very, very general sense, right?

01:04:22.000 --> 01:04:26.000
So, I think I spent a lot of time doing a lot of searching

01:04:26.000 --> 01:04:30.000
and thinking for like what sorts of things I could work on.

01:04:30.000 --> 01:04:33.000
I like tried out a bunch of volunteer and grievous causes

01:04:33.000 --> 01:04:35.000
in like high school and university just to see like

01:04:35.000 --> 01:04:38.000
what things might be interesting for me.

01:04:38.000 --> 01:04:40.000
I think this is good to the extent that like, you know,

01:04:40.000 --> 01:04:44.000
I learned a lot more about like things that are wrong in the world.

01:04:44.000 --> 01:04:47.000
I got really into social justice.

01:04:47.000 --> 01:04:54.000
And like, I think like, how did I get into AI safety?

01:04:54.000 --> 01:04:58.000
It was kind of in my like latter part of my undergraduate,

01:04:58.000 --> 01:05:00.000
like going to my master's.

01:05:00.000 --> 01:05:02.000
So, I did a math degree during my undergraduate.

01:05:02.000 --> 01:05:06.000
I did a math degree mostly because I didn't really know

01:05:06.000 --> 01:05:08.000
what exactly I wanted to do.

01:05:08.000 --> 01:05:10.000
Maths just seemed like a robustly good thing

01:05:10.000 --> 01:05:13.000
and gave me the flexibility to take a lot of other things

01:05:13.000 --> 01:05:16.000
that I was also really interested in, like linguistics

01:05:16.000 --> 01:05:19.000
and like liberal science and they also do a lot of debate,

01:05:19.000 --> 01:05:23.000
where I talked about like a bunch of this stuff too.

01:05:23.000 --> 01:05:27.000
So, yeah, I guess like having a diverse range of interests

01:05:27.000 --> 01:05:31.000
made it really hard to focus in on a particular thing like as one.

01:05:31.000 --> 01:05:36.000
I mean, I think I still want to just like try a bunch of different things.

01:05:36.000 --> 01:05:40.000
And like, maybe this is a difficulty in like getting concrete projects out.

01:05:40.000 --> 01:05:44.000
So, yeah, at the end of my bachelor's degree, I was like,

01:05:44.000 --> 01:05:47.000
well, what do I do now with a math degree?

01:05:47.000 --> 01:05:50.000
Like nothing I've tried has been super, super convincing to me.

01:05:50.000 --> 01:05:53.000
I don't really want to be a mathematician.

01:05:53.000 --> 01:05:57.000
Like it was like nice, but it seems like being a pure mathematician

01:05:57.000 --> 01:06:03.000
doesn't really have like a lot of impact in like the near or like medium term future.

01:06:03.000 --> 01:06:07.000
And it seems like there are more important problems than like being a pure mathematician

01:06:07.000 --> 01:06:10.000
and more problems to work on like, you know, climate change,

01:06:10.000 --> 01:06:13.000
dried or like global health.

01:06:13.000 --> 01:06:16.000
So then I started thinking, okay, so what are the things that could like really,

01:06:16.000 --> 01:06:20.000
really make the world turn out bad or like to not like a really big impact

01:06:20.000 --> 01:06:23.000
in the world in like my lifetime, say, right?

01:06:23.000 --> 01:06:28.000
So, you know, AI, I think, happened to be one of those things that I was thinking about.

01:06:28.000 --> 01:06:30.000
So I thought, okay, like maybe I should go into AI.

01:06:30.000 --> 01:06:34.000
So I spent like about a year just like reading a lot about this and thinking,

01:06:34.000 --> 01:06:38.000
okay, you know, like where is AI going?

01:06:38.000 --> 01:06:42.000
Like, why do I think that it's like could actually be like a really big thing, right?

01:06:42.000 --> 01:06:44.000
So I think solidifying those intuitions,

01:06:45.000 --> 01:06:50.000
and at this point, I was like doing a masters that, you know,

01:06:50.000 --> 01:06:54.000
it wasn't like the ideal masters, but I later like switched advisors

01:06:54.000 --> 01:06:56.000
and like it was a lot better for me.

01:06:56.000 --> 01:07:01.000
So yeah, like in the middle by masters, I like switched doing like AI,

01:07:01.000 --> 01:07:03.000
like start off with reinforcement learning.

01:07:03.000 --> 01:07:05.000
And it was like really fun.

01:07:05.000 --> 01:07:07.000
And I really enjoyed the environment that I was put in,

01:07:07.000 --> 01:07:09.000
just like having people who like cared also about AI

01:07:09.000 --> 01:07:11.000
and also who also thought that could be really big thing.

01:07:11.000 --> 01:07:17.000
But in the course of my masters, I guess I thought, okay, you know,

01:07:17.000 --> 01:07:19.000
this is like a reinforcement learning lab.

01:07:19.000 --> 01:07:20.000
So I was at the University of Alberta.

01:07:20.000 --> 01:07:22.000
It's a reinforcement learning lab.

01:07:22.000 --> 01:07:26.000
These people like actually want to build a GA.

01:07:26.000 --> 01:07:30.000
I don't know, like this seems kind of concerning to me,

01:07:30.000 --> 01:07:34.000
like at this intuition, like, okay, like, oh, what the fuck though?

01:07:34.000 --> 01:07:40.000
Like an artificial general intelligence, like you could go to do some bad things, maybe?

01:07:40.000 --> 01:07:46.000
Yeah, so this was like, I think, yeah, I finished my masters in 2020.

01:07:46.000 --> 01:07:49.000
So I guess in the middle of COVID, I was sort of thinking,

01:07:49.000 --> 01:07:50.000
like trying to grapple with these questions.

01:07:50.000 --> 01:07:56.000
Also, like just like noticing or just like living through like COVID

01:07:56.000 --> 01:07:58.000
and also like the George Floyd protests, right?

01:07:58.000 --> 01:08:01.000
I was like, okay, like, you know, real shit like actually happened to the world.

01:08:01.000 --> 01:08:02.000
I'm like living through history, right?

01:08:02.000 --> 01:08:06.000
So like maybe something wild could like actually happen, like, you know,

01:08:06.000 --> 01:08:09.000
and something I'm like personally involved in, like every day,

01:08:09.000 --> 01:08:10.000
like in like AI, right?

01:08:10.000 --> 01:08:13.000
So I started to read a lot more about AI safety.

01:08:13.000 --> 01:08:15.000
So like, you know, super intelligence and stuff,

01:08:15.000 --> 01:08:18.000
like a little bit of a lot of inform and that's wrong.

01:08:18.000 --> 01:08:21.000
And you know, I remember I was like reading super intelligence and I was like,

01:08:21.000 --> 01:08:23.000
damn, like, this is true.

01:08:23.000 --> 01:08:26.000
Shit, is anybody working on this?

01:08:26.000 --> 01:08:31.000
Then I thought, well, you know, like, like maybe I should work on to start, right?

01:08:31.000 --> 01:08:34.000
And I think like having that feeling that like, wow,

01:08:34.000 --> 01:08:38.000
like this is a thing I should work on was a pretty like life changing moment.

01:08:39.000 --> 01:08:46.000
Because I think like before, I guess like 2019 ish, you know,

01:08:46.000 --> 01:08:49.000
when you like learn about history in school, it's sort of like, okay,

01:08:49.000 --> 01:08:51.000
like these things happen to these people, right?

01:08:51.000 --> 01:08:53.000
And like, damn, like we have the world we have today.

01:08:53.000 --> 01:08:56.000
But to some extent you feel sort of the distance from what happened,

01:08:56.000 --> 01:08:59.000
like these people are so far removed, hugging never relate, right?

01:08:59.000 --> 01:09:03.000
But you know, we're living through history right now.

01:09:03.000 --> 01:09:04.000
We live through a pandemic.

01:09:04.000 --> 01:09:08.000
We're living through like an increase in geopolitical tensions, right?

01:09:08.000 --> 01:09:13.000
And we're living through like, like a lot of really concerning developments,

01:09:13.000 --> 01:09:15.000
artificial intelligence.

01:09:15.000 --> 01:09:19.000
Well, like we're living through history, that means we can affect it, right?

01:09:19.000 --> 01:09:24.000
So I think like that moment, maybe it was more like a gradual development.

01:09:24.000 --> 01:09:29.000
Maybe think that I could actually do something about this problem.

01:09:30.000 --> 01:09:33.000
So, you know, I applied for PhD programs.

01:09:33.000 --> 01:09:35.000
I didn't apply for AS80 though.

01:09:35.000 --> 01:09:39.000
I was still sort of unsure whether I wanted to like fully devote all my time to this.

01:09:39.000 --> 01:09:42.000
But, you know, I got into a PhD program at Mila.

01:09:42.000 --> 01:09:45.000
I started like doing research, but like basically immediately,

01:09:45.000 --> 01:09:50.000
I like really tried hard to like shift my research from reinforcement learning to doing AS80.

01:09:50.000 --> 01:09:52.000
It took a while.

01:09:52.000 --> 01:09:57.000
And I think like, you know, I'm still sort of learning how to like do AS80 research well

01:09:57.000 --> 01:10:00.000
and figuring out what the most valuable thing to do is.

01:10:00.000 --> 01:10:04.000
But I think like, yeah, it's been going pretty well.

01:10:04.000 --> 01:10:09.000
And I'm really glad I make that decision to switch.

01:10:09.000 --> 01:10:10.000
Nice.

01:10:10.000 --> 01:10:13.000
And write a second of what you're working on.

01:10:13.000 --> 01:10:14.000
Right, working on.

01:10:14.000 --> 01:10:17.000
So I'm trying to finish up the paper with CLR.

01:10:17.000 --> 01:10:20.000
We are trying to evaluate the cooperativity of language models.

01:10:20.000 --> 01:10:23.000
So the really interesting thing I think is, okay, you know,

01:10:23.000 --> 01:10:27.000
you can, you know, construct a dataset, get people to write scenarios for you if you want.

01:10:27.000 --> 01:10:33.000
But we actually want our ability to generate evaluations to scale with the capabilities of our models.

01:10:33.000 --> 01:10:41.000
So we're getting language models to generate scenarios for us that like basically co-operate scenarios that involve some co-operation.

01:10:41.000 --> 01:10:44.000
And seeing what language models do in these kinds of scenarios.

01:10:44.000 --> 01:10:46.000
That's the first thing I'm working on.

01:10:46.000 --> 01:10:52.000
I also have just a bunch of other projects right now that are varying stages of completion or feasibility.

01:10:52.000 --> 01:11:01.000
One thing I'm really interested in is like, you know, how do we actually, how are we actually able to like show people that models are actually doing a lot,

01:11:01.000 --> 01:11:05.000
actually much more capable than might be claimed by some people.

01:11:05.000 --> 01:11:06.000
Yeah.

01:11:06.000 --> 01:11:16.000
So another thing I'm working on is this sort of like general, almost like position paper slash survey paper on like speculative concerns,

01:11:17.000 --> 01:11:18.000
AI safety.

01:11:18.000 --> 01:11:24.000
It's essentially going to be an argument that it's important to speculate for cause areas and like review sort of, okay,

01:11:24.000 --> 01:11:28.000
what are the methodologies and the ways in which people have speculated in AI safety?

01:11:28.000 --> 01:11:29.000
What has worked out?

01:11:29.000 --> 01:11:30.000
What hasn't?

01:11:30.000 --> 01:11:37.000
Why do we still need to continue on coming up with things that might appear more speculative to like modern machine learning communities,

01:11:37.000 --> 01:11:39.000
but like are actually important.

01:11:39.000 --> 01:11:42.000
I think importing out possible problems we might face.

01:11:42.000 --> 01:11:44.000
Is this aimed at machine learning?

01:11:44.000 --> 01:11:46.000
Just this position paper.

01:11:46.000 --> 01:11:47.000
Yeah.

01:11:47.000 --> 01:11:49.000
So it's aimed at the academic machine learning.

01:11:49.000 --> 01:11:56.000
I mean, I think a lot of what I, yeah, parts of what I want to do are sort of more aimed at, okay,

01:11:56.000 --> 01:12:04.000
like how can we field build or build bridges effectively by either like connecting our concerns, concerns other people have,

01:12:04.000 --> 01:12:09.000
or by just saying things in like, you know, language that other people can more understand.

01:12:13.000 --> 01:12:14.000
Nice.

01:12:15.000 --> 01:12:16.000
I don't know.

01:12:16.000 --> 01:12:17.000
Show up.

01:12:17.000 --> 01:12:18.000
I don't know.

01:12:18.000 --> 01:12:19.000
Show up.

01:12:19.000 --> 01:12:20.000
Show up.

01:12:20.000 --> 01:12:21.000
Show up.

01:12:21.000 --> 01:12:22.000
Show what else I got for Mr Allen.

01:12:22.000 --> 01:12:23.000
This is quite...

01:12:23.000 --> 01:12:24.000
What?

01:12:24.000 --> 01:12:26.000
Look what you come about, it's come about.

01:12:26.000 --> 01:12:27.000
Down slide.

01:12:27.000 --> 01:12:29.000
Are you really done with?

01:12:29.000 --> 01:12:32.000
I'm killing, I'm like after that.

01:12:32.000 --> 01:12:34.000
Maybe they see some competition in this.

01:12:34.000 --> 01:12:36.000
Plus in sports.

01:12:36.000 --> 01:12:38.000
Turn down the monopoly.

01:12:38.000 --> 01:12:40.000
Like on feet.

01:12:40.000 --> 01:12:44.000
Good fight, good fight, good fight. Put on money where my off is.

01:12:44.000 --> 01:12:46.000
Yes, good pipes!

