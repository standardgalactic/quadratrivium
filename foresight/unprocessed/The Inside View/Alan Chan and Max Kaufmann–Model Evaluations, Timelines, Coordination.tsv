start	end	text
0	15000	You got transported to 2014, Paul Cristiano is like, boom to Paul and Paul's just celebrating, he's like, man, we've done it, it's happened, alignment is soul, like, and you look out and you know, the light cone is smiling and joining us.
15000	16000	That's so wide.
16000	19000	Yeah, yeah, it's like, wow, what happened?
19000	25000	Also, this being is this, could I just kind of keep up to meme, and I'm like, instant you're happy.
25000	29000	Well, I don't know, what's, what's, what's the channel?
29000	30000	What am I doing?
30000	35000	Is this like, is this recorded for, could be, could be, okay, just, okay.
35000	36000	Half meme, half.
36000	37000	Off takes.
37000	38000	Half takes.
38000	41000	They have like, inside your second channel where you just, yeah, yeah, yeah.
41000	42000	About the outside view.
42000	43000	The outside.
46000	54000	I often find it, um, slightly confusing how NLMs translate to some system I cover in the future.
55000	62000	In some sense, it feels like, you know, you ask a large, oh, do you want to break out with a server and like take over?
62000	64000	And it goes, yes.
64000	67000	I'm not actually sure what this translates to.
67000	69000	For like a future system that's actually taking over breaking servers.
69000	76000	Like I could imagine these systems are maybe more like, maybe N10, like maybe like something like an action transformer,
76000	78000	or they're not directly trained in the language.
78000	81000	And it's unclear to me like how, what they say translate to what they do.
81000	86000	And in fact, I'd expect that to diverge pretty heavily, um, in, in cases I'd worry about.
86000	87000	Yeah.
87000	90000	I think we're in a really weird situation, right?
90000	98000	Because we have all these models and there's this sense that like they could really do things that like change things in the world, right?
98000	104000	Um, but nobody's really deployed these models into like a convincing enough context to showcase these capabilities.
104000	110000	Um, but to answer your question, I think, like, you know, things like, um, WebGBT or like with Coheur, with Act 1,
110000	119000	like once you give language models an API, I think the idea is that like they can leverage, um, the knowledge they've gained in pre-training to use this API to do dangerous things.
119000	127000	And you imagine that like, you know, something like ACT, I mean, ACT can't generate language, at least not the right.
128000	138000	Um, do you see like, what it says as actions are when asked to translate way out and generalize like what it's actions actually will be?
138000	142000	Like, let's say you have WebGBT, oh, would you break out the server file?
142000	148000	Do you think, whether it says yes or no, and like an email like this translates to, if I scale up the system, if I deploy it, it actually will.
148000	149000	Right.
149000	159000	And as I said, I feel like there might be some disconnect, what this like chatbot says, and then what happens when you use this things world model,
159000	165000	if I tune it on some actions, call it to run, let's say, let's say, interact with an API.
165000	173000	It feels like, do you imagine, do you imagine behavior there happens kind of independently of what it said in the chatbot setting?
173000	174000	Yeah, I agree.
174000	178000	So I don't think you literally go and ask it, would you break out of the server farm?
178000	190000	But you go in and measure like, literally just have humans, maybe at first, look at the actions it's taking and see, okay, you know, I am like an expert software engineer or an expert systems engineer,
190000	197000	like I'm looking at these actions, do these actions look as if like the model is actually trying to break out of the server farm?
198000	207000	So yeah, I suppose like that's a problem with evaluations, would you just ask the language model, like yes or no questions about what it would do and not actually like see what it would do?
208000	212000	So let's say hypothetically with the server, it's kind of an evil company.
212000	213000	Hypothetically?
213000	217000	Hypothetically, let's say there's some kind of evil company that's going on.
217000	223000	It's like, I think I'd be interested to see like, right now, what would your model of like, what would that email look like?
223000	227000	Is it more like, like you said, give it an API, see what it's doing?
228000	237000	Yeah, API, I think also identifying key contexts in which a dangerous behavior like takeover or some sort of power seeking would occur.
237000	250000	Yes, so the difficulty here I think is, I think we need to identify these contexts in order to actually be able to write concrete evals and to provide evidence that's convincing to the public that AI is going to do dangerous things.
250000	256000	Of course, whenever, yeah, I don't think like we're going to be able to cover all the possible ground.
256000	265000	There's of course, some unsurging extrapolating from evaluations that we developed, you know, okay, the AI doesn't think dangerous in this scenario doesn't do something dangerous in the scenario B.
265000	268000	What does it do to the scenario C? Sort of unclear.
269000	274000	Do you worry about, like at the point where an AI can exhibit dangerous behavior?
274000	282000	And it's already strategically aware that if it had, like let's say, you know, someone like Yikowski or like, let's say Evan is right.
282000	289000	My strategic awareness plus like long term planning often needs a deception that you will, your evals will be too late in some sense.
289000	299000	Like there's this like, you have to catch it while it's still competent enough to show dangerous behavior and not confidence have the strategic awareness that it should be deceptive.
299000	301000	Yeah, that's a serious problem, I think.
302000	308000	Yeah, so one, I think definitely people should be looking into the conditions under which deceptive alignment or something like that might arise.
308000	318000	Number two, this is just an argument for developing evals as fast as possible so that, you know, I'm imagining at every stage of the trading process, like ideally, you know, every like 10 steps or something.
318000	322000	Maybe even like every step. We literally just like eval the hell out of this thing, right?
322000	324000	We make sure not to train on this eval somehow.
324000	329000	You know, somehow we need to make sure evals don't ever make it into the training sets of language models.
329000	332000	Otherwise, you know, while they train on this, they already know about this.
332000	333000	It's going to be kind of useless.
335000	339000	So, yeah, how do you feel about this like optimization?
340000	348000	So, I think you previously mentioned you don't like the word benchmark and you used the word eval. Could you maybe expand on that?
348000	359000	Yeah, so I don't think I like the word benchmark because I think historically machine learning benchmark has been associated with this idea that, okay, we are like optimizing for this thing.
359000	369000	But this thing that we're developing this eval at the end of the day is just a proxy measure for what we care about, which is, you know, it might be specifically like courageability and honesty.
369000	374000	We're trying to make sure this AI doesn't like, you know, it isn't able to like manipulate a lot of people, right?
374000	376000	So it's like an imperfect measure.
376000	387000	I think when you optimize for imperfect measure, I mean, I think this is like, I already know in the ICH, like you don't tend to get models that actually do what you care about at the end of the day.
387000	397000	So I think like the framing of evals instead of benchmarks to me makes it clear in my head at least that we don't want to be optimizing for these things.
397000	401000	We're just using these as sort of like checks on our model, right?
401000	408000	But there's another difficulty with having like, I guess public evals and that there's some sort of like implicit optimization already going, right?
408000	418000	Like when researcher A like tries a technique on like some benchmark or eval and finds it doesn't work and publishes a favor, researcher B says, oh, you know, you tried this technique, it doesn't work.
418000	424000	I'm going to try another technique so that researchers are already going through an optimization process to make evals better.
424000	433000	And I think like this optimization process is like maybe like a little weaker than actually like fitting on the test set, for instance, which happens sometimes.
433000	436000	But it is still an optimization process.
437000	443000	Yeah, there's like some work actually that studies, you know, how effective is this like implicit optimization process?
443000	446000	Really? I don't quite remember the details right now.
446000	453000	But like, you know, the general consensus is that you actually do need to keep updating your evals and benchmarks because there's an optimization process.
453000	455000	I mean, you know, an image net, right?
455000	457000	Like we're already like so good at image net.
457000	458000	The next like 0.2%.
458000	460000	Is that really like a better model?
460000	462000	Or is that really just like overfitting to what?
462000	468000	Image net, you know, is, yeah, overfitting to like random stuff.
468000	474000	Yeah, I remember there was this paper, maybe a few years ago, where they re-gathered an image net type data set.
474000	478000	And they did find that at least back then, so it was out of distribution.
478000	481000	They did the same thing to gather the bunch of images online, like same.
481000	483000	And like there's been some distribution shift.
483000	486000	So models would perform worse on this new image net.
486000	489000	But the ranking between still kept.
489000	495000	At least that paper claimed that back then you hadn't yet overfitted.
495000	505000	Yeah, I think I find it hard to imagine publishing such an evaluation and not having the headline be my model passes all the evaluations, right?
505000	517000	And I guess if you're like, I guess, yeah, if you feel like, so if you make a model pass all the evals, do you see this as then a good thing?
517000	531000	So I think my perspective is a model passing evals shouldn't be taken as evidence of safety, but a model failing eval is evidence that, OK, let's slow things down, maybe not release it.
531000	533000	Let's talk to some people.
533000	537000	So failing evals is sufficient to show data, but not necessary.
537000	543000	Maybe not in a truth, like strictly truth sense, but in a practically speaking what we should do sense.
543000	550000	In the sense that like, I think I am fairly risk averse about like, you know, releasing the eyes into the world.
550000	555000	So how much of this eval plan rests?
555000	559000	How do you see the technical side making good evals?
559000	565000	And I guess the more like social governance side of like getting people to pay attention to these emails.
565000	567000	Like, how does either play look for you?
567000	569000	Where do you think more work is needed?
569000	574000	What does that trade off look like for you personally and what you want to work on?
574000	575000	Yeah.
575000	582000	Yeah, so I think this depends on timelines and where we're at.
582000	587000	So I think it also depends on things I'm not like totally aware of.
587000	595000	So one thing is, you know, if you actually went to the heads of Google brain and anthropic and like all the big tech companies, right?
595000	599000	And you actually told them, look, like we have this dangerous evaluation.
599000	601000	We're running it on your models.
601000	603000	Your models do terrible, terrible things.
603000	607000	If you show this to them, would they actually like refrain from releasing models?
607000	610000	I'm not sure what would be convincing to those people, right?
610000	617000	At another level, okay, even supposing that it's convincing to those people for how long is it going to be convincing?
617000	620000	It doesn't seem like a sustainable thing to rely on.
620000	629000	Like in some sense, the goodwill of these people who might have different aims than like, you know, people in AI safety regarding like not ending the world, right?
629000	636000	So I think that's the next level, like that's the level at which we try to bring in other people from the public like civil society, governments,
636000	645000	just in general, like public consensus, building around like the idea that like AIs can just empower us, right?
645000	653000	So yeah, I think like, you know, an eval is kind of useless if nobody pays attention to it or if people don't find it convincing.
653000	656000	In particular, the people who have the power to actually do things.
656000	663000	So part of the work, I think, is thinking about what kinds of deals would actually be convincing to people.
664000	670000	What do you think kind of equals at least current as Islam?
670000	672000	Yeah, I mean, I kind of have no idea.
672000	676000	Like, I mean, I have an idea of evals that would be convincing to me.
676000	684000	Like if I actually saw an AI like trying to copy its weights to another server, trying to like get a lot of money.
684000	687000	Yeah, like, like I buy that, right?
687000	691000	Like if I was in a deep mind, I'd be like, okay, you may be even without looking at these evals.
691000	694000	That's like, slow it down a little bit.
694000	702000	But you know, like, I think there's this like big gap between people in like AI safety or existential safety
702000	706000	and people in other communities that also care about the impact of AI and society.
706000	714000	Like people in fate, like fairness, accountability, transparency and ethics, people generally in like AI ethics.
714000	722000	Yeah, so, you know, there's already a big difficulty in just explaining like, you know, what is the danger here, right?
722000	728000	Like I think there's this big discourse outside of the AI safety community, the idea that like AI's are just tools, they aren't agents.
728000	735000	Like what's so hard, you know, the difficulty is misuse or in some sense, like this lack of coordination between everybody
735000	743000	and like making things go bad and like AI's that like exacerbate, you know, like bad things that we have today like oppression.
743000	749000	I think these are all true, right? But I think like, you know, the danger of AI's agents is this like separate category
749000	753000	that's been very, very difficult to explain to other people for some reason.
753000	757000	So like, yeah, like I'm not sure what kind of evals would be convinced to them.
757000	763000	Like maybe there needs to be much more consensus building, you know, on a philosophical basis of like, what are the things in common between
763000	768000	what the things that like people in X safety care about and the things that people in fate care about?
769000	774000	Do you think that this is like, like, why do you think that this agreement lies?
774000	778000	Like is it like a technical one?
778000	787000	More philosophical one? Like if you try to characterize why these people don't really worry about AI doom like we seem to?
787000	792000	Yeah, I mean, it's something I'm still trying to figure out and running a document about.
793000	797000	Yeah, so first I think like, I guess I would consider myself in like both of these camps.
797000	799000	Like I think like existential safety is super important.
799000	803000	But I mean, I also think like fairness problems are like also super important.
803000	810000	You know, like on any given day I wake up and I'm like, okay, got to be at least 30%.
810000	813000	As we find out recent events.
813000	817000	Yeah, no, I think justice is really important.
817000	830000	I don't want to live in a world where, you know, we have like just like our current systems of discrimination just like enforced or solidified because of really, really good artificial intelligence.
830000	844000	This seems like a concern to me that is like, yeah, I guess I find difficulty maybe like giving a absolutely precise like rating scale for how important things are.
844000	853000	In general, I try to find commonalities between causes I really care about to sort of do things that seem robustly good on both accounts.
853000	860000	So yeah, and I think like, you know, my work in evals so far is like an attempt to doing this.
860000	862000	What was your question?
862000	870000	Sorry, my original question was, where do you see like, I guess my question is why don't they people care about extras?
870000	872000	Oh, let's say even more broadly.
872000	878000	What do you think stops the average Mila PhD from being like shit man?
878000	881000	Let me write some elaborate foreign posts right now.
881000	885000	Okay, let's not say let's say like care about extras.
885000	887000	Yeah, I think there are a bunch of possible reasons.
887000	890000	I'm kind of not sure which ones are the most plausible.
890000	892000	I think I just have to like talk to more people.
892000	897000	One of them is, you know, AI is like taking, it's actually some wild shit, right?
897000	902000	Like you like stroll up the subject on the street and you're like in five years.
902000	907000	No, like be like taking him, right?
907000	912000	So I think firstly, you know, we just have to recognize like, yeah, this is actually wild, right?
912000	919000	Like, if you know, like you finished high school or university, like you go straight to work, you don't really think about like you're not really exposed to.
920000	926000	All the developments that have been happening in artificial intelligence is busy with like your life, right?
926000	929000	Your job, family, stuff like that.
929000	931000	This is like totally out of left field.
931000	939000	So I think we have to acknowledge that and like try to explain things in a way that like are more relatable to a greater extent than we have so far.
939000	946000	I think another thing is like this is definitely not true of like all people in AI safety,
946000	954000	but there is almost this vibe that like, you know, besides existential risk, like technology.
954000	965000	Yes, so maybe what I'm trying to say is like the association that is in people's minds between the people who are in a safety and the people in Silicon Valley.
965000	975000	So I think like there is this vibe from people in Silicon Valley that like technology is like generally a good thing and that it's going to solve social problems.
975000	983000	I think this is in contrast to a lot of people's opinions in like the fake community who like, yeah, maybe they're not like techno pessimists,
983000	993000	but they're definitely a lot more pessimistic about technology than people in Silicon Valley just looking at the history of ways in which technology has been misused and has been used to like discriminate against people more.
994000	1006000	And then, you know, I think the people in the fake community, you know, they look at like the use of AI in like society today, like the increasing use of algorithms in places like loan applications or like job applications, right?
1006000	1013000	They say, oh, like, like clearly these technologies are just reproducing harms that we've already had, right?
1013000	1016000	So that might be this like sort of their starting mindset.
1016000	1022000	And it might be hard to like convince them otherwise that there is like actually another harm as well.
1022000	1029000	You know, the things you're talking about, they actually do happen and we should try to fix them, but there is this like related harm that we should ultra strive to solve.
1032000	1037000	How much of it do you think is, you know, in the spirit of the Inside U podcast?
1037000	1041000	How much do you think it is timelines, belief about speed of progress?
1041000	1043000	Part of it.
1043000	1057000	So a lot of people, I think the fake community don't think that AIs could be like, you know, classical RL agents pursuing goals in ways that misgeneralize to new environment.
1057000	1063000	Yeah, I'm not even sure that like a lot of people really know about reinforcement learning.
1064000	1074000	Yeah, like I've had a ton of conversations where, you know, I like people expressed to me, oh, like areas are just tools, like we're just designing them like, you know, they're really just like reproducing the data, right?
1074000	1077000	And I'm like, well, like, have you heard of reinforcement learning?
1077000	1081000	Like we're literally designing agents to maximize like a reward function, right?
1081000	1084000	This is like all the problems of classical utilitarianism.
1084000	1088000	They're like, oh, shit, I've never heard of this, right?
1089000	1099000	So part of it might just be education that like, you know, literally there are like companies and research labs in academia whose goal is to build artificial general intelligence with something like reinforcement learning.
1099000	1102000	And like, they're just they're just doing this.
1102000	1107000	The vast majority of people aren't thinking about safety concerns.
1107000	1110000	So I don't know, maybe like telling people this might help.
1110000	1122000	So it sounds like there's some underlying thing that were, you know, like, let's say I or like people in the extras community think of AI and they think of like some agentic, like, you know, maximizing reward.
1122000	1128000	And you may be saying something like a lot of them are practitioners.
1128000	1129000	That is just not the conception.
1129000	1132000	When they think of AI, that is just not what they see.
1132000	1133000	Yeah.
1133000	1134000	Yeah, yeah.
1134000	1138000	So I think like maybe there are two things here.
1138000	1147000	The first thing is people believe that, okay, like we don't actually have these kinds of AIs or maybe trying to build these kinds of AIs is unimaginable.
1147000	1152000	But the second thing might just be, okay, they might believe that it's possible to build these AIs.
1152000	1154000	But this is like way, way too far off, right?
1154000	1157000	And this is where I think the objection to long term comes in.
1157000	1165000	And where I think like, I don't know, it's been sort of complicated with AI safety and long termism.
1165000	1170000	Maybe like 20 years ago, like long termism was a stronger argument for AI safety.
1170000	1177000	But I think now because of timelines, it seems that we don't really need long termism to make like the argument that we should care about AI safety.
1177000	1180000	And it like made me 10 years.
1180000	1183000	I mean, maybe we just shouldn't talk about long termism.
1183000	1187000	Right. If it turns like people in certain communities off.
1187000	1195000	Yeah, because I think the worst ones that I get whenever I bring up like AI safety or anything related to long termism is, oh, okay, well, like this might be true.
1195000	1198000	But AI is already causing harm today.
1198000	1200000	So we should focus on immediate harms, right?
1200000	1205000	And you know, like, I don't think this argument like really makes that much sense.
1205000	1210000	And I'm not sure that the people expressing this argument like are actually expressing this argument.
1210000	1217000	It seems like they're expressing another objection, but it seems easier to say that like, you know, we don't care about these harms because they're not immediate, right?
1217000	1225000	Whereas, you know, if you look at the history of say like climate change, climate change was like totally a speculative thing in like the 1890s, 1900s, right?
1225000	1231000	And it's only through like decades or like maybe a century of like gradually building up evidence that we like now we have like this consensus.
1231000	1233000	We don't think it's a spectacular thing, right?
1233000	1240000	But even in like, I think like the 50s or something, don't quote me on this, but like there's like definitely a history of like climate change.
1240000	1242000	Like books and articles out there.
1242000	1248000	Like in like the 50s, we're still like, oh, like, we're not really sure still about like the greenhouse effect, right?
1248000	1257000	But you know, like, based I guess on like my preferred decision theory, it'd be like, well, like, we're not sure, but you know, it could be pretty bad.
1257000	1260000	We pump all the CO2 in the atmosphere, right?
1260000	1264000	How about we work on some mitigations like right now in the 50s?
1264000	1270000	Just just just in case like this might actually be a very hard problem to figure out and like it actually is right?
1270000	1273000	Not even just like technically speaking, but like on a coordination basis.
1273000	1275000	How do we actually get everybody together?
1275000	1276000	So it's worth it starting early.
1276000	1280000	I think in the case of climate change, and it seems like it's also the case with ASA.
1280000	1283000	So do you think that...
1283000	1291000	So I think you actually find something that at least I felt that I think a lot of the ASA people are very happy to get like Pascal Muglin has said.
1291000	1292000	Like I think this is my original motivation.
1292000	1301000	I was like, well, like maybe it'll be fine and maybe like just a little bit uncertain, but maybe it's not and it seems like to be really impactful.
1301000	1309000	Do you think you have to set some level of like, of this reasoning, some level of like, oh, I don't know if it'll be good or bad.
1309000	1313000	I don't know what's going to happen, but I should work on it because it's going to be impactful to work on ASA.
1313000	1316000	Or like, how do you see that changing at the moment?
1316000	1319000	It depends on how do me you are.
1319000	1322000	Yeah, I think personally for me it is like, sort of a gamble.
1322000	1334000	I mean, I'm like, yeah, I mean, even if we don't sort of get like the classical agent like AGI, I think things like are still going to be so, so wild with really good AI systems.
1334000	1339000	Like going to be like so many complicated societal feedback loops that we need to think about.
1339000	1343000	Like, multi polar world seems like more and more likely right with all these AI startups.
1343000	1345000	What kind of things do we have there?
1345000	1348000	Like conflict now seems like much more important thing to worry about.
1348000	1357000	So it's definitely like a gamble, but I don't think it's like a bad gamble to work on like the space of preventing negative consequences from AI generally.
1365000	1372000	So I have some model that the, and I think this is currently, the fields going to change a lot.
1372000	1375000	Like, you know, you have these big models coming out.
1375000	1378000	GPT-4 is rumored to be quite good.
1378000	1380000	When they release it.
1380000	1382000	When they've had these soft teas and releasing.
1382000	1384000	Keep saying next week, every week.
1384000	1385000	Every week, bro.
1385000	1387000	It's been, it's been, you know, funny.
1387000	1388000	Yeah.
1388000	1390000	You know, who knows what the gossip's at the moment.
1390000	1391000	Yeah.
1391000	1393000	But, um, yeah.
1393000	1404000	And like, I could definitely imagine a world where in like three to four years, I often say something like 2% of the American US population out of population is in love with their multimodal model.
1404000	1409000	You know, AI porn, TikTok is causing massive decrease in GDP.
1409000	1410000	Oh, maybe.
1410000	1411000	Right.
1411000	1417000	You can tell whether you have like several online personas that are fully automated.
1417000	1419000	It's kind of hard to tell like what's going on.
1419000	1421000	Let's say more than two years here.
1421000	1422000	Yeah, at least.
1422000	1424000	Let's say, let's say, let's say.
1424000	1435000	Um, um, in these worlds where like you're a medium person, it's like, you know, why is going on?
1435000	1438000	Um, and it's like, this is pretty crazy.
1438000	1453000	If you'd like this calculus, like change a bit, like, you no longer need to be kind of risk averse or kind of like heavily convinced by abstract arguments about, you know, the VNM axiom to think that AI might be dangerous.
1453000	1458000	Um, in that set, like, A, how do you feel about this model?
1458000	1464000	And B, how does it like affect you, affect safety general?
1464000	1471000	And in particular, you as someone who worries convincing people that they just, um, yeah.
1471000	1473000	Yeah, I have a lot of uncertainty about this.
1473000	1482000	So I think on one hand it should be like, good in the sense that, okay, you know, if everybody with their like, it's double the diffusion, like six, right?
1482000	1487000	And GPD, like 10, they're like, okay, you know, I look out at the world today.
1487000	1489000	What is the labor I can do?
1489000	1490000	Nothing.
1490000	1498000	I think that's a pretty wild world in a world in which people think, damn, like, these AI things, like, maybe we should, maybe we should regularly come, right?
1498000	1508000	Um, so I think this is good to, like, I guess make AI capabilities sort of be known in the, in the public.
1508000	1521000	Um, I'm not sure this is enough, though, because I think if we, people are just aware of systems like stable diffusion, um, and like, like a text generation systems, like, the two things that are missing, I think, are like the difficulty of alignment.
1521000	1526000	And number two, um, like, generality, like having an agent, right?
1526000	1530000	Um, so, like, I think having an agent concern really motivates a lot of ex-risk.
1530000	1538000	Like, I think that is, like, in some sense trying to do something that is counter to your aims or, like, you know, pursuing an instrumental goal that is counter to your aim.
1538000	1540000	That seems like it'd be really bad.
1540000	1545000	I'm not sure, like, we're able to impart that understanding just from, like, really good generative models.
1545000	1550000	Um, number two is, like, um, the difficulty of alignment.
1550000	1562000	Like, I think, um, you know, like, you're some, you're somebody in, like, 2035, you know, you're, you've, like, finished high school, you've finished university, now, like, you don't have a job, you'll never have a job ever again, right?
1562000	1565000	Like, who do you blame for this?
1565000	1569000	Um, I'm not sure you blame, like, the lack of alignment techniques.
1569000	1571000	I think you blame the corporations, right?
1571000	1575000	Or you blame the entire system that has gotten us to this point, right?
1575000	1582000	Like, we've created a world in which, okay, there is, like, no UBI, there is no other social safety yet, which at these corporations, like, making money.
1582000	1585000	Um, so now, like, you're stuck in this state of care.
1585000	1590000	Like, I don't think you care about existential safety necessarily.
1590000	1594000	Um, like, is this worse than the world we're in now?
1594000	1600000	Um, like, in terms of, like, getting people to care about X safety, I'm really not sure.
1600000	1611000	Okay, you kind of see this as an awful, like, a slightly awful duty, agent, X risky thing, though, which, which needs to be solved as well.
1611000	1620000	Um, yeah, like, like, I guess to say more, like, I mean, I think we need to solve, like, almost generalization stuff, right?
1620000	1623000	And, like, you know, a specification of good rewards.
1623000	1629000	Um, but, like, in my mind, like, if we have these really capable, like, generative models, people, when interacting with them,
1629000	1633000	they're going to be like, oh, you know, like, haven't we solved these things already?
1633000	1641000	You know, like, when I say, like, generate me, like, really good 3D porn, like, amazing 3D porn, right?
1641000	1645000	So, I mean, I think they might be like, oh, like, what is even alive in it?
1645000	1647000	Like, they just do exactly as I ask, right?
1647000	1652000	And you, and you, you may see, like, would you be fair to say that you see evils in such a world,
1652000	1658000	than calling people to focus more on the ekfrisky agentee outcomes?
1658000	1659000	Possibly, yep.
1659000	1667000	Um, so I think I see evils as, like, in general work, just, just, like, adding layers of security.
1667000	1671000	So, okay, maybe this is enough to get people on the AI safety train, right?
1671000	1672000	But we don't know.
1672000	1678000	Uh, we, it seems like we should be trying as hard as possible to get people on the, like, AI as, like, ridiculous train.
1678000	1679000	Um.
1679000	1680000	Okay.
1680000	1684000	So it's less of, like, a, you know, defense in depth.
1684000	1686000	It's another thing that someone should be doing.
1686000	1689000	And you've decided that as part of the wider strategic picture,
1689000	1692000	evils are, like, some important part.
1692000	1694000	Yeah, maybe some people call this a portfolio approach.
1694000	1699000	I call this, uh, I'm, like, a naturally, you know, very uncertain person on the road.
1699000	1702000	I want to cover all of my bases, just in case some things fail.
1702000	1705000	All right.
1705000	1707000	Do you?
1707000	1708000	Okay.
1708000	1711000	So maybe a wider question.
1711000	1712000	Wide.
1712000	1718000	So you kind of spoken about this idea that evils are some smaller part of a wider portfolio of life.
1718000	1722000	And because of your own uncertainty, you're kind of working on this.
1722000	1723000	This seems like robustly good.
1723000	1727000	Um, what do you see as, like, some of the other promising directions in this space?
1727000	1733000	Like, when you're like, if you, if you, if you found out, you know, if you got transported to 2014,
1733000	1736000	Paul Cristiano is like, you boom to Paul and Paul's just celebrating.
1736000	1738000	He's like, man, we've done it.
1738000	1739000	It's happened.
1739000	1741000	Alignment is soul.
1741000	1746000	Like, and you look out and, you know, the light cone is smiling and joining us.
1746000	1747000	That's so wide.
1747000	1748000	Yeah.
1748000	1749000	Yeah.
1749000	1750000	It's like, wow, what happened?
1750000	1752000	You know, obviously, apart from the evils, obviously the evils.
1752000	1753000	Evils, yeah.
1753000	1754000	For sure.
1754000	1755000	I'm picking up some, some prizes on the way.
1755000	1756000	Yeah.
1756000	1757000	Yeah.
1757000	1759000	What did it, what worked with the evils trigger?
1759000	1761000	Maybe it's a better question.
1761000	1763000	So yeah, maybe that's your first question.
1763000	1765000	What am I optimistic about?
1765000	1771000	Not a lot right now because I think alignment is really hard and we don't have much time,
1771000	1776000	less than 10 years to solve it either on a coordination basis, um, by getting people to
1776000	1781000	not build a GI or like on a technical basis, like actually formulating an operationalizing
1781000	1784000	problem and like developing a nice proof, right?
1784000	1790000	Um, I do think, you know, eventually we will have to do something like agent foundations
1790000	1795000	to really understand like what are these things like optimization and agency?
1795000	1797000	Like what are values, right?
1797000	1799000	How do we actually load them into agents?
1799000	1800000	What are we doing?
1800000	1805000	Right now it seems we're sort of poking around in the dark, like deep learning and RLHF and
1805000	1809000	it's like, okay, you know, on these data sets, it seems to work out fine.
1809000	1812000	We're not really sure if it's going to work on like other data sets.
1812000	1818000	We're not really sure how to define like agency in terms of our deep learning systems, right?
1818000	1824000	Um, so it's kind of like, I definitely think we still should be doing alignment of deep
1824000	1829000	learning, um, but like it's a bet and it might not work out.
1829000	1845000	Do you think, so if we need something like agent foundation, it seems that we would need
1845000	1850000	like some kind of restructuring of the research or we would need like much more people pouring
1850000	1852000	into these directions.
1852000	1860000	Um, and in general, I'm not sure if the evil work, let's say convincing like deep mind
1860000	1865000	on tropic or something, which is like, I think the frame you've given, um, it's going to
1865000	1866000	help with that.
1866000	1867000	Maybe.
1867000	1868000	Yeah.
1868000	1871000	So I don't think it's going to help get people into agent foundations.
1871000	1882000	I think it's more about, okay, let's, um, get people to care about AI safety in general.
1882000	1884000	That's it.
1884000	1885000	Yeah.
1885000	1890000	I mean, I think like there is, um, like the agent foundations people, I think, uh, or
1890000	1897000	maybe the community in general could definitely do much better job of like saying why agent
1897000	1902000	foundation is important, saying why, you know, this alignment stuff is actually really, really
1902000	1903000	hard.
1903000	1908000	Um, it's just right now just a bunch of like less wrong in alignment for a post, um, or
1908000	1910000	you talk to people in the community, right?
1910000	1915000	But, you know, the community from the outside might seem a little like off putting to approach.
1915000	1919000	So maybe we don't want to do that.
1919000	1923000	So you're sounding, uh, you've mentioned a few times this conversation.
1923000	1928000	And this whole, uh, 10 years has been thrown out.
1928000	1930000	Alignment is hard has been thrown out.
1930000	1935000	I guess I might as well ask the, uh, the fame question or the fame pair of question.
1935000	1937000	What, uh, what is, uh, timelines?
1937000	1938000	And PDOOM.
1938000	1939000	PDOOM.
1939000	1944000	So yeah, I think, um, did this calculation yesterday or a few days ago, the bunch of
1944000	1952000	friends, something like 46% it's quite high, I think, but, uh, could be higher.
1953000	1954000	Timelines.
1954000	1955000	Yeah.
1955000	1963000	I mean, I feel like a lot of my timeline stuff is like part of it is anchoring off of things
1963000	1965000	like the bio anchors report.
1965000	1970000	Another part of it is just like by 2012 image net.
1970000	1973000	I was like pretty cool when it wasn't like that.
1973000	1974000	Great.
1974000	1977000	But now it's like, I mean, it was like a couple of years ago, right?
1977000	1979000	Like two before, like maybe next week, right?
1979000	1982000	It's like, yeah.
1982000	1987000	Act, we, we went from like, like okay image classification to a system that's like actually
1987000	1990000	able to like do tasks in the world.
1990000	1992000	Um, I mean, you look at BBT, right?
1992000	1997000	Like opening, I certainly wasn't throwing all of its resources into tea.
1997000	1998000	Do you want to explain what the BT is?
1998000	1999000	Yeah.
1999000	2000000	The video pre-trained transformer.
2000000	2004000	So this is recent paper from open AI where they, um, pre-trained transformer on a bunch
2004000	2005000	of Minecraft YouTube videos.
2005000	2008000	It's a Mar-L-HF and see how it did in Minecraft.
2008000	2010000	So of course it's like not a human level, right?
2010000	2015000	Um, I think it was like a one billion parameter model or something, something less like that.
2015000	2019000	Um, so certainly like they could have thrown a lot more resources at this.
2019000	2021000	Uh, but still it seemed to do sort of reasonable.
2021000	2025000	It even was able to construct a diamond pickaxe, which is like very, very hard to do, um, in
2025000	2026000	Minecraft.
2026000	2031000	Um, you also have things like Gato, which like a transport pre-trained, like a bunch of RL
2031000	2033000	things, uh, RL environments.
2033000	2036000	Uh, and it seems to be able to like do a bunch of tasks.
2036000	2041000	Um, you know, so we have like really, really good capability, um, in like certain domains.
2041000	2044000	We have like, you know, increasing generality, right?
2044000	2047000	I think you just need to put these two together, scale things up a little bit more.
2047000	2049000	Uh, maybe add some more hacks and tricks.
2049000	2054000	And it seems like we're sort of there to something that is like an AI-like system, uh, where
2054000	2058000	at the very least could cause a lot of damage in the world.
2058000	2065000	Um, how do you see, how do you see the rest of the world looking then?
2065000	2071000	Like if you have like, you think we're quite close, like upon deployment of the system,
2071000	2075000	can you see like, yeah, I think, I think it's like an issue of sometimes how the question
2075000	2079000	of what's your timelines, because I'm not like, I think it very much posits some idea
2079000	2082000	like, you know, your time is a 10 years.
2082000	2085000	It means, you know, 10 years of one day from now, you might as well just retire.
2085000	2088000	It's about shits like God, right?
2088000	2092000	Um, but in my head, like, you know, the game still keeps going, right?
2092000	2098000	Um, so yeah, so like, let's say you get one of these, you know, video pre-training,
2098000	2105000	YouTube, behavior clone, which is software, pre-training, RLH done.
2105000	2112000	Action transformative systems in like, let's say 10 years and it's deployed.
2112000	2116000	Um, how do you see the world looking?
2116000	2119000	What do you actually, what is actually the worry here?
2119000	2123000	Like, what's, like, do you think we're still in the game?
2123000	2126000	Like when you say time of 10 years, are you like, you've got 10 years and that's it?
2126000	2128000	Like, yeah, what was the fight?
2128000	2132000	So maybe two things to distinguish are like, what is the point at which we get an AGI
2132000	2137000	or an APS AI, like advanced planning, strategically aware, like with key capabilities,
2137000	2140000	like hacking or manipulation AI.
2140000	2144000	Um, so yeah, I think my 10 years would be like for that.
2144000	2149000	Um, then the separate question is, okay, we have such an AI, would it actually do something bad?
2149000	2152000	I think this is the open question.
2152000	2154000	Um, I really don't know.
2154000	2157000	This depends on, okay, like what was the pre-training data?
2157000	2160000	Um, what do we know about generalization and deep learning at this point?
2160000	2164000	Um, like, how much do you actually have to prompt a model to do bad things
2164000	2166000	or to actually do bad things?
2166000	2170000	Um, like how bad of a problem are you, is, um, you know, instrumental goals
2170000	2175000	with like these sorts of simulators that, um, it's unclear.
2175000	2178000	Um, I think we might still be in the game for a while.
2178000	2182000	Um, but you know, it's this sort of, um, notion of precarity, I think.
2182000	2187000	Like, even if a model doesn't do something bad, it still might, right?
2187000	2191000	So I think in this time, we really have to, like, rush, um, after we deploy this first model
2191000	2195000	to, like, either solve the alignment through something like agent foundations
2195000	2200000	or produce convincing enough evidence to the world that we actually cannot solve this.
2200000	2203000	We need to coordinate not to build certain types of systems.
2203000	2205000	I mean, yeah, I guess I'm pretty...
2205000	2208000	I mean, alignment is like a public...
2208000	2209000	Yeah.
2209000	2213000	And if it's everybody, but like, you know, on the margin, a company like DeepMind
2213000	2216000	has incentive to just, like, move forward.
2216000	2220000	Yeah, I definitely think that...
2220000	2225000	Yeah, I guess I might just be pessimistic.
2225000	2228000	Coordination, making some abstract general sense.
2228000	2230000	Um, a few people have tried to do this for a long time.
2230000	2233000	It seems to just be, like, a thing that...
2233000	2238000	I think there's been, like, in my head, lots of people have wanted to make the world
2238000	2240000	more coordinated for a long time.
2240000	2243000	And it just hasn't really happened.
2243000	2245000	Yeah, for sure.
2246000	2248000	Yeah, for sure.
2248000	2250000	But I could definitely...
2250000	2252000	I think I'm definitely more...
2252000	2254000	I should put it kind of, like, benevolent.
2254000	2256000	Like, you know, like DeepMind...
2256000	2258000	I think, I think...
2258000	2260000	So you don't really realize being in the safety community.
2260000	2264000	It's just kind of, uh...
2264000	2266000	Like, villainization.
2266000	2269000	Like, it's like DeepMind, open AI.
2269000	2274000	Like explicit, like, like, you know, talking about, like, let's say some open or something.
2274000	2280000	You see for, like, caring, nice, genuine, intelligent, you know, future caring people.
2280000	2282000	Damus as well, right?
2282000	2287000	Um, I do have some, like, pretty good...
2287000	2291000	Some pretty, like, good hope that coordination there is very much possible.
2291000	2293000	Um...
2293000	2295000	Yeah, I think, um...
2295000	2298000	Maybe this is kind of rave, but, you know, we're all in this together.
2298000	2300000	I think these are just people at the end of the day.
2300000	2304000	Maybe we can't get them, but we gotta try till the last moment, right?
2304000	2307000	Uh, like, this problem is so important.
2307000	2312000	Uh, it'd be a shame if it just so happened we were in the world in which coordination would have actually worked,
2312000	2314000	but we just didn't try.
2314000	2316000	How embarrassing would that be?
2316000	2318000	The dignity.
2318000	2321000	Um, yeah, I don't know.
2321000	2326000	So I think it totally makes sense to be pessimistic about, like, solving all of this about coordination,
2326000	2333000	and then about alignment, um, about, like, any other, other stuff that could, like, you know, go wrong, like, S-risk.
2333000	2340000	Um, like, I think it totally makes sense, and, like, I definitely don't want to, like, judge anybody that is just, like,
2340000	2343000	you know, decided not to work on any of this because it's too depressing.
2343000	2344000	Like, that's totally fair.
2344000	2346000	This is, like, super, super hard.
2346000	2350000	Um, but I think, I guess, personally, my perspective is, like, okay.
2350000	2355000	You know, I, maybe, this just comes from me having a lot of uncertainty about a lot of things in my life.
2355000	2360000	Um, so, you know, on the off chance that, like, we could actually make a difference,
2360000	2362000	it seems worth it to try.
2362000	2368000	Um, even if, sort of, maybe the objective belief is that, okay, like, maybe it's not worth trying, if we actually did the ease.
2370000	2378000	I mean, I think this is a, um, the, the, the extreme deal, I guess, you'd take killer makes this point,
2378000	2379000	in a way, certainly.
2379000	2386000	Both in expectation, or he makes the point that, in expectation, the focus should be something like this.
2386000	2392000	Do, like, you know, do the thing that seems, like, the most defined doesn't seem like this.
2392000	2395000	You know, what does it, what does that mean, like?
2395000	2402000	I guess, I guess he's just saying, he's trying to make the point that, like, because in his worldview, he's, like, you know,
2402000	2407000	a several nines bed, probably, like, not that far away.
2407000	2414000	And he makes the point that, like, yeah, like, when you're, when you're acting in the face of, like, trying to increase such small probabilities,
2414000	2420000	or acting in the face of causing a problem that seems so hard, you shouldn't follow the heuristic of, like, what should I do to solve it?
2420000	2425000	And because everything seems doomed to you, you should be, like, what's the most dignified way?
2425000	2432000	It'd be more dignified if, like, OpenAI and DeepMind at least did some coordination before, like, some other company went and built AGI.
2432000	2440000	It'd be more dignified if, like, we really tried to scale mechanistic interpretability, or had, like, at least had evals, had some...
2440000	2446000	And he claims that it's, like, a more robust year signal, and also more motivating for oneself.
2446000	2450000	Oh, so the argument is, like, dignity isn't the thing we inherently care about.
2450000	2454000	It's, like, this heuristic that actually gets us to, like, reduce risk.
2454000	2458000	Yeah, he's, like, particularly for him.
2458000	2463000	The property is, like, oh, like...
2463000	2467000	The framing shouldn't be, is this going to solve a line?
2467000	2472000	Because nothing is. The framing is, like, is this a more dignified world?
2472000	2473000	Interesting.
2473000	2476000	And that's kind of, like, he approached the problem.
2476000	2479000	He said it, like, a lot more...
2479000	2481000	He didn't phrase it as nicely as that.
2481000	2486000	But, you know, I think this is his general point.
2486000	2489000	I think if you could make some...
2489000	2495000	Also, I think I have some term, like, value on humanity, but, like...
2495000	2498000	I'd rather, like, try it and, like, at least have tried.
2498000	2500000	Yeah, that's right, that's right.
2500000	2503000	Even when you know it's hopeless, we just keep on trying.
2503000	2505000	That's what we do.
2505000	2509000	I mean, I don't think... I mean, I'm not that much.
2509000	2512000	Quite not much to me.
2512000	2514000	This is interesting.
2514000	2518000	But if we did solve that, that'd be absolutely sick.
2518000	2520000	Oh, here's a question now, Lois.
2520000	2524000	So, you're talking about...
2524000	2529000	You care about the kind of, you know, fattest stuff, justice stuff.
2529000	2532000	I think, obviously, territory.
2532000	2535000	People right now, their hearts are being burned.
2535000	2537000	No one wants to slow down.
2537000	2539000	We all seem to make some implication, though.
2539000	2543000	Even as a cold hearted, long-term future caring to the Italian.
2543000	2547000	You should care about this, because these things might be locked in.
2547000	2550000	Like, the society in the future might be locked in.
2550000	2554000	Um...
2554000	2557000	I think...
2557000	2559000	I have some intuition.
2559000	2564000	I'm not sure, but at least I think a fair kind of pushback against this
2564000	2569000	is the idea that, like, there's just not that much time
2569000	2571000	where society looks like it does now.
2571000	2573000	Like, what you get is a kind of AGI thing,
2573000	2576000	and you get, like, self-improvement for a bit.
2576000	2579000	At some point, you just have, like, a super-intelligent God.
2579000	2581000	And, like...
2581000	2584000	Whatever that thing is doing, or whatever that thing wants to do,
2584000	2587000	is what decides what the society looks like.
2587000	2590000	Not necessarily, like, the current dynamics,
2590000	2593000	or, like, those dynamics, like, pushed into the future.
2594000	2596000	Um, how do you feel about that as a state?
2596000	2598000	Uh, I think this is a possibility,
2598000	2600000	but, again, I have a lot of uncertainty
2600000	2602000	about whether this will actually happen.
2602000	2606000	Like, I think, yeah, so maybe the thing that you're saying is,
2606000	2609000	okay, we have this, like, super-intelligent AGI,
2609000	2611000	it's, like, it's how I'm all figured out,
2611000	2614000	like, what are the true human values that, like, everybody cares.
2614000	2617000	And, like, it makes sure everything is okay,
2617000	2621000	or at least, like, gives us time to figure out all these human values, right?
2621000	2623000	Yeah, like, I think that'd be great.
2623000	2625000	I'm not sure this is actually going to...
2625000	2629000	Then, like, one is, like, SEV even possible.
2629000	2633000	Number two, okay, like, suppose OpenAI has built the AGI,
2633000	2636000	like, what do they do with it?
2636000	2639000	I think, like, A, the temptation to do something,
2639000	2643000	to instantiate, you know, your own values is just way, way too strong.
2643000	2646000	If you actually have a God at your hands.
2646000	2650000	Um, number two, I mean, even if they, like,
2650000	2653000	try not to do anything with it, like,
2653000	2656000	I think for sure other parties are going to want that kind of power, too.
2656000	2659000	The US government, China, maybe even other companies, right?
2659000	2661000	What happens when there's conflict?
2661000	2663000	What does OpenAI do?
2663000	2666000	Do they just obliterate other entities?
2666000	2669000	That's, like, wild, right?
2669000	2671000	If they did, I think for sure, like,
2671000	2675000	general populace would be like, what the fuck is going on with OpenAI?
2675000	2678000	Like, maybe we should go to war with this, right?
2678000	2681000	Um, this doesn't seem like a good future, either.
2681000	2684000	So, I think there are just a lot of factors that, like,
2684000	2686000	maybe we actually do need to figure out right now,
2686000	2689000	like, what is the game plan when we have AGI?
2689000	2691000	Um, to ensure that, like, we get that kind of future
2691000	2693000	where we have the time to think about, okay,
2693000	2695000	like, what are the things we actually want?
2695000	2698000	And we have, like, the luxury, you know, to work on,
2698000	2700000	okay, like, it's a limited resource scarcity, right?
2700000	2702000	Um, let's start at a limited discrimination somehow,
2702000	2704000	like, maybe not with the AGI, but, like,
2704000	2706000	because we don't have other problems,
2706000	2710000	we focus our, like, time and energy on, like, these, these social problems.
2710000	2713000	Yeah, so as you say right now, like, something like,
2713000	2716000	you still see society, like,
2716000	2719000	affecting the long-term future,
2719000	2723000	even in the face of, like, an incredibly advanced powerful system.
2723000	2725000	You think there's still, there are actions, right?
2725000	2729000	Everything still changes based on what a site looks like as well.
2729000	2730000	Yeah, yeah.
2730000	2733000	So, this all depends on the kind of AGI we have.
2733000	2735000	So, I think on the one hand, like,
2735000	2738000	suppose that this AGI is, in some sense,
2738000	2740000	value-free or value-neutral.
2740000	2743000	Um, okay, then, like, it's going to be aligned,
2743000	2745000	okay, suppose we solve a line, right?
2745000	2747000	Then it's going to be aligned with, like, whoever's controlling it.
2747000	2750000	Um, like, okay, if it's, like, open AI,
2750000	2752000	then you run into the problems that, like, I talked about, right?
2752000	2757000	Um, like, conflict or just, like, you know, some sort of dictatorship or something.
2757000	2761000	Uh, okay, suppose that, actually, in the meantime,
2761000	2765000	we sort of solve parts of, like, moral philosophy.
2765000	2768000	So, now, like, this AGI actually has, like, reasonable values,
2768000	2773000	um, that, you know, like, the vast majority of humanity would agree with, right?
2773000	2775000	And even if, you know, it's overseers,
2775000	2777000	thinks it's just do something, it actually doesn't,
2777000	2779000	because they know it's better, in some sense.
2779000	2782000	Um, okay, like, I think this seems, like, sort of reasonable, right?
2782000	2784000	But the difficulty is getting...
2784000	2787000	I don't think anybody's really working on this in the AI safety community,
2787000	2793000	figuring out, like, you know, what do we do, like, about different moral systems, for instance?
2793000	2796000	Like, um, like, what is the answer to moral uncertainty?
2796000	2799000	Is it, like, moral parliaments? Is it, like, something else?
2799000	2804000	Um, yeah, so, like, it seems that the first...
2804000	2808000	Yeah, on the first path, um, I don't know, conflict seems, like, pretty bad.
2808000	2811000	On the second path, well, we haven't really done much work towards this.
2811000	2814000	Um, so I'm not very, like...
2814000	2818000	I think I'm not very, like, optimistic about, um, you know,
2818000	2823000	the world, like, going well, even if we solve a line mid-term.
2823000	2826000	Yet, somebody should work on this, though.
2832000	2838000	What's your biggest frustrations with the AI safety community?
2838000	2840000	Biggest frustrations? Damn.
2840000	2844000	Like, it can't save me a second.
2844000	2848000	If I can shout it out later, I already don't hear it.
2848000	2851000	Yeah, frustrations.
2851000	2853000	Margot's definitely going to tweet this.
2853000	2856000	Let me ask the question again, sorry.
2856000	2861000	What's your biggest frustration with the AI safety community?
2861000	2863000	Frustrations.
2866000	2868000	AI safety community frustrations.
2868000	2871000	What is AI safety?
2871000	2874000	Perfect. Okay.
2877000	2879000	Alan, in your opinion...
2879000	2880000	Max.
2880000	2883000	Right, let's go ahead.
2884000	2888000	Alan, in your opinion, what is AI safety?
2890000	2893000	I think there's a broad version of this term,
2893000	2896000	maybe several broad versions of this term, and several narrow versions.
2896000	2900000	So, I think the narrow version, of course, is, like,
2900000	2902000	of course, meaning between us.
2902000	2904000	AI existentialcy.
2904000	2907000	So, how do you prevent AI from being existential risk,
2907000	2911000	whether it's through empowerment or human extinction or something else?
2911000	2914000	There's, like, broader versions of AI safety, too,
2914000	2916000	that include more than existential risk.
2916000	2919000	So, you might include S-risks, which care a lot about, like,
2919000	2922000	suffering caused by artificial intelligence,
2922000	2925000	either through conflict or, like, something else.
2925000	2928000	And I think there's an even broader notion of AI safety,
2928000	2931000	which, like, in my mind, this is the ideal definition of AI safety,
2931000	2934000	and it encompasses, like, literally everything, right?
2934000	2937000	Like, we care about, like, all the negative consequences from AI,
2937000	2940000	and we try to draw the threads, like, between all these phenomena
2940000	2943000	we're observing and, like, core technical and social problems.
2943000	2947000	So, this includes things, like, the things that people study in fairness, right?
2947000	2951000	Like, AI is that, like, are really able to, like,
2951000	2955000	learn what our fairness judgments are.
2955000	2959000	AI that, like, just exacerbate discrimination that is already present in society
2959000	2963000	and that is present in data sets that we use to train these AIs.
2963000	2968000	So, I think that's, like, that broad definition, to me, is the ideal definition,
2968000	2971000	the one we can all get behind, you know, so that we can do things
2971000	2974000	that we practically agree on, like, more regulation,
2974000	2978000	slowing down AI development, more like verification of AIs
2978000	2982000	before we deploy them.
2982000	2986000	Okay, given that definition,
2986000	2990000	um, and maybe focusing on the narrow
2990000	2994000	AI safety x-race definition,
2994000	2998000	um, what's your biggest frustration with the community or the set of people
2998000	3002000	currently works on?
3002000	3005000	Is it far that they are able to be homogenized?
3005000	3009000	Or maybe you should, like, go into more specific subsets of this community.
3009000	3012000	But yeah, the people that worry about the x-race AIs.
3012000	3015000	So, I have actually been doing this for that long.
3015000	3019000	I think, um, I've been doing, like, or I guess I've been considering myself
3019000	3022000	a part of this community for, like, maybe a year and a half?
3022000	3026000	Two years, if? So, like, basically just for my PhD?
3026000	3030000	Um, yeah. So, I mean, in the short amount of time,
3030000	3034000	I think it's been, like, pretty great so far just finding people
3034000	3038000	who care about the same things that I do and are motivated to work similarly.
3038000	3041000	I think this is definitely, like, a big, like, anti-frustration.
3041000	3045000	I guess it's, like, I think it's very frustrating working on something, like, by yourself.
3045000	3049000	Um, and, like, thinking you're the only person around who cares about the problem.
3049000	3052000	And everybody else thinks you're on the crazy train or something.
3052000	3056000	Um, yeah, maybe one frustration, though, is I think, um,
3056000	3061000	it'd be, like, a lot better if more people in the AI x-safety community
3061000	3065000	were, like, more explicitly wanting to build bridges
3065000	3070000	to other communities that also care about safety in, like, the broad, broad sense.
3070000	3074000	So, in particular, and, like, to the, like, fairness and, like, ethics.
3074000	3078000	Um, just because I think coordination is a super important thing for us to be doing
3078000	3082000	and might even be a low-tech fruit, um, to, like, slow down AI development
3082000	3086000	and make sure we don't, um, like, face negative consequences.
3086000	3091000	Um, yeah, I guess that'd be the, um, the main thing for me.
3096000	3099000	Do you see this community changing a lot?
3099000	3104000	Like, let's say, by this community, let's say, the set of people that, if you ask them,
3104000	3108000	what do you do, they would say something along the lines of,
3108000	3115000	I work to ensure that, um, extras from AI.
3115000	3119000	Do you see, like, that set of people changing, like, for years?
3119000	3121000	I think more people get on the train.
3121000	3124000	The same trains we are on right now.
3124000	3129000	Yeah, um, you know, AI is being deployed much more.
3129000	3132000	Um, these, like, ridiculous generative models, right?
3132000	3138000	It's, like, wild when you're put out of a job by an AI in, like, 2025.
3138000	3141000	But the name we predicted, 26, right?
3141000	3144000	Um, so, so, so, this is a good thing.
3144000	3148000	I guess, um, I hope that, like, you know, and getting more people on this train,
3148000	3152000	we also make sure that we repair the bridges that don't exist
3152000	3155000	or have been burned between the different safety communities.
3155000	3159000	Um, not sure how this is going to happen,
3159000	3163000	but, um, I think a good first step is just talking to people,
3163000	3166000	um, going to the places that they frequent.
3166000	3169000	Like, you know, I think definitely some AI safety people, like,
3169000	3171000	AI safety people should just go to, like, FACT,
3171000	3174000	like, the biggest conference in Ferris that's held yearly
3174000	3176000	and just, like, talk to people about concerns.
3176000	3178000	We should, like, submit papers there.
3178000	3181000	We should, like, actually try to understand, like, the fact people are saying,
3181000	3185000	um, you know, like, do some social theory, like, study some psychology.
3185000	3190000	Um, like, really think about, like, how AI is going to interact with society.
3190000	3194000	Like, maybe as well, we should try to develop, like, some sort of, um,
3194000	3198000	I guess, uh, point of view that is not just techno-optimist.
3198000	3202000	Like, you know, even if we solve a line, what are the problems that are left?
3206000	3211000	How similar do you think the technical problems are between an escape team?
3218000	3219000	So much similar.
3219000	3227000	So, I think one particular technical problem in fairness is
3227000	3236000	what, so in a particular context, let's say, um, I don't know, like,
3236000	3240000	hiring in this particular country with this particular historical context,
3240000	3243000	um, what do we do?
3243000	3246000	Like, what are the conceptions of fairness that are at play?
3246000	3251000	Can we, like, learn these things and formalize them in a certain way
3251000	3254000	so that we can actually try to understand what's, what's going on
3254000	3256000	and come to consensus about what we're doing?
3256000	3260000	Um, I mean, I think the techniques that we're coming up with in AI safety
3260000	3262000	are, like, super super rolling for this, right?
3262000	3265000	Like, if we do RLHs to actually learn people's preferences,
3265000	3267000	then we, like, study the reward function.
3267000	3269000	I think that might give us valuable insight, actually,
3269000	3272000	about what people actually think about in fairness.
3272000	3275000	Um, I think general stuff, too, like, you know,
3275000	3278000	anytime we think about, um, generalization problem in AI safety,
3278000	3281000	I mean, these are also relevant in fairness,
3281000	3284000	because, like, fairness problems are just, like, one-shot,
3284000	3287000	oh, like, the trained test distribution are, like, going to be the same, right?
3287000	3289000	Things are changing in the real world as well.
3289000	3292000	Um, so totally, I think, um, that thing is also relevant.
3292000	3296000	Uh, another thing is just, like, um, all this stuff about, like, instrumental goals
3296000	3298000	and, like, reinforcement learning systems and agents, right?
3298000	3301000	Like, if we're going to be deploying, um,
3301000	3304000	like, algorithms in society that make consequential decisions
3304000	3307000	about people's welfare, well, these are going to be decisions
3307000	3309000	that are going to be made over time.
3309000	3313000	Um, and in making decisions over time, like, we don't want, like,
3313000	3317000	a license we're deploying to, like, do really weird things,
3317000	3319000	um, have really weird instrumental goals.
3319000	3323000	Um, so, I think the connection to me seems, like, pretty clear,
3323000	3327000	but it hasn't been communicated well, which is pretty unfortunate.
3332000	3336000	You have a, uh, center of long-term risk.
3336000	3340000	I do, I do. The size too small, I think.
3340000	3342000	Well, I think so as well.
3342000	3345000	Well played, well played.
3345000	3350000	Um, what's this whole, uh, it's all S-risk thing.
3350000	3354000	S-risks, okay. Well, to learn...
3354000	3359000	Sure. What's this whole, uh, X-risk, S-risk, C-risk, you know?
3359000	3361000	So, I'm not sure what C-risk is, but, uh...
3361000	3363000	It's, uh, it's, uh, it's kind of strong.
3363000	3365000	Oh, I see, I see it.
3365000	3366000	Right, right.
3366000	3368000	Those, those things.
3368000	3371000	Yeah, so X-risk is existential risk.
3371000	3376000	Um, I think of this as problems that would, um,
3376000	3380000	either result in human extinction or in, like,
3380000	3387000	the lost capacity of, um, humans to, like, do things in the future.
3387000	3390000	And maybe, like, when, yeah, when I say things, I mean great things.
3390000	3393000	Maybe when I think about great things, it's, like, somewhat dying.
3393000	3395000	I mean, I think it'd be pretty...
3396000	3398000	You know, we, like, travel the star.
3398000	3402000	Like, you know, we see another species who try to help.
3402000	3406000	Um, I don't know, uh, there seems to be a lot of things
3406000	3408000	like we could do in the universe, like improve our understanding,
3408000	3411000	like, really just go through and review our history
3411000	3413000	and try to become, like, better agents, right?
3413000	3416000	All of this becomes impossible if, like, we're extinct
3416000	3419000	or if, like, we are trapped on, like, a planet
3419000	3422000	that has had all the resources, um, depleted.
3422000	3424000	Um, so that to me is an existential risk.
3424000	3428000	Um, a, an extraterrestrial risk is, um, a suffering risk.
3428000	3431000	So these are things that could cause, like, the, you know,
3431000	3434000	like, really, really, like, lots of suffering in the world.
3434000	3439000	Um, so what are some examples of things that cause a lot of suffering today?
3439000	3442000	Factory farming, arguably. Um, it's, like, absolutely terrible
3442000	3444000	how we treat our animals today.
3444000	3450000	Um, what are the things that could cause a lot of, um, suffering?
3454000	3456000	Um, dictatorship.
3456000	3458000	Dictatorships, yeah, malevolence is a lot of...
3458000	3464000	Yeah, so, um, yes, so I guess, like, there are some more mundane things
3464000	3468000	than there are things that, um, are a bit more, like, hypothetical
3468000	3470000	but seem like they could happen given the technologies
3470000	3471000	that we're developing right now.
3471000	3474000	Yes, the mundane things, you know, there's, um, there's factory farming.
3474000	3480000	I think there's also just, like, wars, you know, um, like, whenever...
3480000	3482000	So not just, like, the death tolls in the wars themselves,
3482000	3484000	like, what war brings with it, right?
3484000	3488000	So, like, just disease, like, malnutrition, like, general instability, right?
3488000	3492000	Um, that, that seems to have caused, like, a huge amount of suffering in history.
3492000	3495000	Um, so, so that's sort of, like, the more mundane things.
3495000	3499000	Um, I mean, mundane, I mean, it's like, there's something bad.
3499000	3501000	The things that we already know.
3501000	3505000	Um, there are things that, like, maybe we don't have yet, like, we could have.
3505000	3507000	Um, so hypothetically, you know, if we, like,
3507000	3509000	managed to become a galaxy-faring civilization
3509000	3511000	and we spread factory farming to the stars,
3511000	3514000	like, trillions and trillions of animals are suffering in factory farming.
3514000	3516000	It just, like, seems horrific, right?
3516000	3518000	Um, another thing is, okay, like,
3518000	3520000	suppose that we have really, really good AI systems
3520000	3524000	that, like, control large portions of, like, the Earth's resources,
3524000	3527000	like, the Solar System's resources, the Galaxy's resources.
3527000	3531000	Well, if they engage in conflict-like wars, that also seems really, really bad.
3531000	3534000	Like, our wars multiplied by, like, a million or something, right?
3534000	3538000	So, um, working as a risk, um, just to, like, really map out
3538000	3540000	what are the causes of suffering
3540000	3543000	and what are the ways we can, like, how can we act or reduce?
3543000	3546000	Um, so I think, like, one difference between ex-risk and ex-risk
3546000	3549000	is, like, sort of, where you start from, um,
3549000	3552000	in terms of, like, what you care about in your moral theories,
3552000	3555000	like, people in ex-risk really, really care about suffering.
3555000	3559000	And I would say they prioritize this to, like, differing extents over, over pleasure.
3559000	3562000	So, like, you know, between, like, if you have equivalent amounts of suffering
3562000	3564000	and, like, pleasure, you would definitely, like,
3564000	3567000	prefer reducing the suffering more about, like, one unit
3567000	3570000	than, like, increasing somebody's pleasure or, like, you know, happiness
3570000	3572000	by another unit.
3575000	3577000	And...
3577000	3579000	in so far, you can speak...
3579000	3581000	for...
3581000	3583000	I cannot. I cannot speak.
3583000	3585000	I cannot speak. I cannot speak.
3585000	3586000	Too many of them.
3586000	3588000	I mean, I was just, I was just an intern.
3588000	3591000	I don't work there as a full-time employee.
3592000	3599000	Where do you think your views differ the most
3599000	3601000	from the mainstream
3601000	3603000	AI safety research?
3605000	3607000	I think suffering is important.
3607000	3610000	I think cooperation is pretty important work.
3610000	3612000	Um...
3612000	3615000	Yeah, so, I mean, I think this has been a common answer.
3615000	3617000	I've a lot of uncertainty about this.
3617000	3620000	Um, I think I'm still in the process of
3620000	3623000	figuring this out because, like, S risks are still, like,
3623000	3625000	kind of new to me.
3625000	3629000	Yeah, I guess what I differ the most is
3629000	3631000	I care about cooperation.
3631000	3635000	I think it's important to get this work done, like...
3635000	3637000	What do you like, cooperation?
3637000	3642000	Yeah, so cooperation meaning, um, suppose you have, um, two agents,
3642000	3644000	like, they're in some sort of game.
3644000	3646000	How do you make sure that the outcome
3646000	3649000	is actually a pre-do-off-camel outcome?
3649000	3652000	So, for example, if you're both playing a game of chicken, right,
3652000	3654000	how do you make sure you don't, like, crash into each other
3654000	3657000	and, like, cause astronomical loss?
3657000	3660000	Um, you know, because, like, maybe, like, your one agent,
3660000	3663000	like, maybe one agent is, like, you know, this nation
3663000	3665000	that has a nuclear stockpile,
3665000	3667000	and, like, this other agent is, like, another nation
3667000	3669000	that has a nuclear stockpile.
3669000	3671000	And, like, these two nations together are, like,
3671000	3673000	the entire Earth, they have huge empires.
3673000	3675000	Um, a game of chicken is, like, okay, like,
3675000	3677000	you both launch nukes at each other,
3677000	3680000	right, we definitely don't want that kind of thing to happen.
3682000	3684000	So, uh...
3684000	3687000	I think a lot of us, like, we brought you here to find out
3687000	3689000	about your origin stories, you know.
3689000	3690000	Or origin stories.
3690000	3692000	Yeah. How did, how did Mr. Alan Chan
3692000	3694000	come to where
3694000	3697000	the hoodie that sent a long-term risk,
3697000	3700000	come to call himself an AI safety research?
3700000	3702000	Well, I just, like, took this hoodie.
3702000	3705000	I remember having to see all our others.
3705000	3706000	Okay, okay.
3706000	3707000	But the other bit.
3707000	3708000	The other bit. Okay.
3708000	3710000	Yeah, where should I begin?
3710000	3711000	How far back?
3711000	3713000	I'm actually, I think you know what, I want to hear from, like...
3713000	3714000	Okay. From what?
3714000	3716000	September 1st, 1996.
3716000	3717000	Yeah.
3717000	3720000	Um, probably at night, probably 12 a.m. or something.
3720000	3724000	Okay, I was born in Edmonton, Alberta, Canada.
3724000	3727000	To two parents, immigrants, Vietnam.
3727000	3730000	So, uh, yeah.
3730000	3732000	How much detail?
3732000	3733000	Just, you know, whatever.
3733000	3735000	Whatever's necessary to understand.
3735000	3737000	To live it as if you're in your shoes.
3737000	3738000	Yeah.
3738000	3739000	So, I don't know.
3739000	3742000	Why do I work on AI safety?
3742000	3745000	So, I think part of it is,
3745000	3750000	why do I work on things that could go wrong,
3750000	3751000	I guess, I don't know.
3751000	3753000	No, no, I'm wanting, I'm wanting to story.
3753000	3754000	It's play-by-play.
3754000	3758000	What made, from, from, you know, let's say starting...
3758000	3759000	Yeah.
3759000	3762000	...to sitting down here.
3762000	3763000	What happened?
3763000	3764000	Yeah.
3764000	3765000	Okay.
3765000	3767000	So, I think there's like a little bit more to this than the undergrad.
3767000	3768000	Okay.
3768000	3772000	I think this also depends on how I ended up developing my moral motivations.
3772000	3773000	Okay.
3773000	3775000	Like, why didn't I just like go and be like an investment banker, right?
3775000	3778000	And like, literally just like a regular investment banker.
3778000	3781000	Like, none of that earned against us.
3781000	3782000	Yeah.
3782000	3786000	I mean, I think like part of it is, is my upbringing.
3786000	3789000	Like, I think, like, my family is like pretty Buddhist.
3789000	3792000	They care, and in Buddhism, you care a lot about reducing the suffering.
3792000	3796000	And particularly, my mother cares a lot about Buddhism and reducing suffering.
3796000	3798000	So, I think just growing up in an environment like,
3798000	3801000	they need care about these things as well.
3801000	3804000	And to the extent that I saw like suffering in the world,
3804000	3809000	whether it was like on the news or like interpersonally,
3809000	3813000	you know, that that seemed like a really bad thing.
3813000	3815000	So, I think that's like one part of it.
3815000	3819000	Another part is just like being exposed to like the things
3819000	3821000	that I think make life really worth living,
3821000	3823000	like just like hanging out with your friends,
3823000	3827000	like doing fun things, trying new foods, traveling.
3827000	3831000	So, I think like both the upside and the downside,
3831000	3834000	that like I was able to experience in my life,
3834000	3838000	like I think maybe believe that, okay, you know,
3838000	3842000	it seems like really important to make sure that they are,
3842000	3845000	to like remove the downsides for as many people as we can,
3845000	3848000	and to make sure that like people can actually, you know,
3848000	3851000	experience the upside in life.
3851000	3853000	So, I think that's like general motivation, I guess,
3853000	3859000	for like working on social causes or causes to like reduce risk
3859000	3862000	in like some very, very general sense, right?
3862000	3866000	So, I think I spent a lot of time doing a lot of searching
3866000	3870000	and thinking for like what sorts of things I could work on.
3870000	3873000	I like tried out a bunch of volunteer and grievous causes
3873000	3875000	in like high school and university just to see like
3875000	3878000	what things might be interesting for me.
3878000	3880000	I think this is good to the extent that like, you know,
3880000	3884000	I learned a lot more about like things that are wrong in the world.
3884000	3887000	I got really into social justice.
3887000	3894000	And like, I think like, how did I get into AI safety?
3894000	3898000	It was kind of in my like latter part of my undergraduate,
3898000	3900000	like going to my master's.
3900000	3902000	So, I did a math degree during my undergraduate.
3902000	3906000	I did a math degree mostly because I didn't really know
3906000	3908000	what exactly I wanted to do.
3908000	3910000	Maths just seemed like a robustly good thing
3910000	3913000	and gave me the flexibility to take a lot of other things
3913000	3916000	that I was also really interested in, like linguistics
3916000	3919000	and like liberal science and they also do a lot of debate,
3919000	3923000	where I talked about like a bunch of this stuff too.
3923000	3927000	So, yeah, I guess like having a diverse range of interests
3927000	3931000	made it really hard to focus in on a particular thing like as one.
3931000	3936000	I mean, I think I still want to just like try a bunch of different things.
3936000	3940000	And like, maybe this is a difficulty in like getting concrete projects out.
3940000	3944000	So, yeah, at the end of my bachelor's degree, I was like,
3944000	3947000	well, what do I do now with a math degree?
3947000	3950000	Like nothing I've tried has been super, super convincing to me.
3950000	3953000	I don't really want to be a mathematician.
3953000	3957000	Like it was like nice, but it seems like being a pure mathematician
3957000	3963000	doesn't really have like a lot of impact in like the near or like medium term future.
3963000	3967000	And it seems like there are more important problems than like being a pure mathematician
3967000	3970000	and more problems to work on like, you know, climate change,
3970000	3973000	dried or like global health.
3973000	3976000	So then I started thinking, okay, so what are the things that could like really,
3976000	3980000	really make the world turn out bad or like to not like a really big impact
3980000	3983000	in the world in like my lifetime, say, right?
3983000	3988000	So, you know, AI, I think, happened to be one of those things that I was thinking about.
3988000	3990000	So I thought, okay, like maybe I should go into AI.
3990000	3994000	So I spent like about a year just like reading a lot about this and thinking,
3994000	3998000	okay, you know, like where is AI going?
3998000	4002000	Like, why do I think that it's like could actually be like a really big thing, right?
4002000	4004000	So I think solidifying those intuitions,
4005000	4010000	and at this point, I was like doing a masters that, you know,
4010000	4014000	it wasn't like the ideal masters, but I later like switched advisors
4014000	4016000	and like it was a lot better for me.
4016000	4021000	So yeah, like in the middle by masters, I like switched doing like AI,
4021000	4023000	like start off with reinforcement learning.
4023000	4025000	And it was like really fun.
4025000	4027000	And I really enjoyed the environment that I was put in,
4027000	4029000	just like having people who like cared also about AI
4029000	4031000	and also who also thought that could be really big thing.
4031000	4037000	But in the course of my masters, I guess I thought, okay, you know,
4037000	4039000	this is like a reinforcement learning lab.
4039000	4040000	So I was at the University of Alberta.
4040000	4042000	It's a reinforcement learning lab.
4042000	4046000	These people like actually want to build a GA.
4046000	4050000	I don't know, like this seems kind of concerning to me,
4050000	4054000	like at this intuition, like, okay, like, oh, what the fuck though?
4054000	4060000	Like an artificial general intelligence, like you could go to do some bad things, maybe?
4060000	4066000	Yeah, so this was like, I think, yeah, I finished my masters in 2020.
4066000	4069000	So I guess in the middle of COVID, I was sort of thinking,
4069000	4070000	like trying to grapple with these questions.
4070000	4076000	Also, like just like noticing or just like living through like COVID
4076000	4078000	and also like the George Floyd protests, right?
4078000	4081000	I was like, okay, like, you know, real shit like actually happened to the world.
4081000	4082000	I'm like living through history, right?
4082000	4086000	So like maybe something wild could like actually happen, like, you know,
4086000	4089000	and something I'm like personally involved in, like every day,
4089000	4090000	like in like AI, right?
4090000	4093000	So I started to read a lot more about AI safety.
4093000	4095000	So like, you know, super intelligence and stuff,
4095000	4098000	like a little bit of a lot of inform and that's wrong.
4098000	4101000	And you know, I remember I was like reading super intelligence and I was like,
4101000	4103000	damn, like, this is true.
4103000	4106000	Shit, is anybody working on this?
4106000	4111000	Then I thought, well, you know, like, like maybe I should work on to start, right?
4111000	4114000	And I think like having that feeling that like, wow,
4114000	4118000	like this is a thing I should work on was a pretty like life changing moment.
4119000	4126000	Because I think like before, I guess like 2019 ish, you know,
4126000	4129000	when you like learn about history in school, it's sort of like, okay,
4129000	4131000	like these things happen to these people, right?
4131000	4133000	And like, damn, like we have the world we have today.
4133000	4136000	But to some extent you feel sort of the distance from what happened,
4136000	4139000	like these people are so far removed, hugging never relate, right?
4139000	4143000	But you know, we're living through history right now.
4143000	4144000	We live through a pandemic.
4144000	4148000	We're living through like an increase in geopolitical tensions, right?
4148000	4153000	And we're living through like, like a lot of really concerning developments,
4153000	4155000	artificial intelligence.
4155000	4159000	Well, like we're living through history, that means we can affect it, right?
4159000	4164000	So I think like that moment, maybe it was more like a gradual development.
4164000	4169000	Maybe think that I could actually do something about this problem.
4170000	4173000	So, you know, I applied for PhD programs.
4173000	4175000	I didn't apply for AS80 though.
4175000	4179000	I was still sort of unsure whether I wanted to like fully devote all my time to this.
4179000	4182000	But, you know, I got into a PhD program at Mila.
4182000	4185000	I started like doing research, but like basically immediately,
4185000	4190000	I like really tried hard to like shift my research from reinforcement learning to doing AS80.
4190000	4192000	It took a while.
4192000	4197000	And I think like, you know, I'm still sort of learning how to like do AS80 research well
4197000	4200000	and figuring out what the most valuable thing to do is.
4200000	4204000	But I think like, yeah, it's been going pretty well.
4204000	4209000	And I'm really glad I make that decision to switch.
4209000	4210000	Nice.
4210000	4213000	And write a second of what you're working on.
4213000	4214000	Right, working on.
4214000	4217000	So I'm trying to finish up the paper with CLR.
4217000	4220000	We are trying to evaluate the cooperativity of language models.
4220000	4223000	So the really interesting thing I think is, okay, you know,
4223000	4227000	you can, you know, construct a dataset, get people to write scenarios for you if you want.
4227000	4233000	But we actually want our ability to generate evaluations to scale with the capabilities of our models.
4233000	4241000	So we're getting language models to generate scenarios for us that like basically co-operate scenarios that involve some co-operation.
4241000	4244000	And seeing what language models do in these kinds of scenarios.
4244000	4246000	That's the first thing I'm working on.
4246000	4252000	I also have just a bunch of other projects right now that are varying stages of completion or feasibility.
4252000	4261000	One thing I'm really interested in is like, you know, how do we actually, how are we actually able to like show people that models are actually doing a lot,
4261000	4265000	actually much more capable than might be claimed by some people.
4265000	4266000	Yeah.
4266000	4276000	So another thing I'm working on is this sort of like general, almost like position paper slash survey paper on like speculative concerns,
4277000	4278000	AI safety.
4278000	4284000	It's essentially going to be an argument that it's important to speculate for cause areas and like review sort of, okay,
4284000	4288000	what are the methodologies and the ways in which people have speculated in AI safety?
4288000	4289000	What has worked out?
4289000	4290000	What hasn't?
4290000	4297000	Why do we still need to continue on coming up with things that might appear more speculative to like modern machine learning communities,
4297000	4299000	but like are actually important.
4299000	4302000	I think importing out possible problems we might face.
4302000	4304000	Is this aimed at machine learning?
4304000	4306000	Just this position paper.
4306000	4307000	Yeah.
4307000	4309000	So it's aimed at the academic machine learning.
4309000	4316000	I mean, I think a lot of what I, yeah, parts of what I want to do are sort of more aimed at, okay,
4316000	4324000	like how can we field build or build bridges effectively by either like connecting our concerns, concerns other people have,
4324000	4329000	or by just saying things in like, you know, language that other people can more understand.
4333000	4334000	Nice.
4335000	4336000	I don't know.
4336000	4337000	Show up.
4337000	4338000	I don't know.
4338000	4339000	Show up.
4339000	4340000	Show up.
4340000	4341000	Show up.
4341000	4342000	Show what else I got for Mr Allen.
4342000	4343000	This is quite...
4343000	4344000	What?
4344000	4346000	Look what you come about, it's come about.
4346000	4347000	Down slide.
4347000	4349000	Are you really done with?
4349000	4352000	I'm killing, I'm like after that.
4352000	4354000	Maybe they see some competition in this.
4354000	4356000	Plus in sports.
4356000	4358000	Turn down the monopoly.
4358000	4360000	Like on feet.
4360000	4364000	Good fight, good fight, good fight. Put on money where my off is.
4364000	4366000	Yes, good pipes!
