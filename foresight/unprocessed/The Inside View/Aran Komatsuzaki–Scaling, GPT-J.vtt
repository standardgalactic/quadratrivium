WEBVTT

00:00.000 --> 00:08.760
Aaron, you're an ML PhD student at Georgia Tech, a research intent at Google, and a researcher

00:08.760 --> 00:11.560
at OPAI.

00:11.560 --> 00:17.080
At Eleuther, you've been working on a DAWI reproduction, which later became Lyon, and

00:17.080 --> 00:26.080
you proposed GPTJ, a JAX-based GPT3-like model whose performance is on par with GPT3 6 billion

00:26.080 --> 00:29.960
on various downstreaming tasks.

00:29.960 --> 00:35.320
You're also well-known for being one of the two AKs where the legend says that if a deep

00:35.320 --> 00:41.120
learning paper is important, one of the two AKs will have tweeted about it.

00:41.120 --> 00:46.240
Your name was mentioned before on the podcast as one of the persons who have convinced Ethan

00:46.240 --> 00:50.760
Caballero that scaling will turn out to be important.

00:50.760 --> 00:52.840
Thanks Aaron for coming on the show.

00:52.840 --> 00:55.880
Thanks for having me today.

00:55.880 --> 00:59.080
So let's start with the legend of the two AKs.

00:59.160 --> 01:05.000
Who is this other AK and why do people say that following the two of you is enough to

01:05.000 --> 01:07.680
understand deep learning research?

01:07.680 --> 01:15.320
Yeah, basically we scan all the archive papers, archive machine learning papers every day,

01:15.320 --> 01:20.440
and we tweet about them with some summary and visualization.

01:20.440 --> 01:27.680
And yeah, we've been doing it for several years and we became, we got more and more followers.

01:27.680 --> 01:38.400
And that's probably why everyone follows us to get the grasp of the latest research.

01:38.400 --> 01:43.480
How do you manage to read so many papers, writing all of the summaries on Twitter?

01:43.480 --> 01:46.840
What's your process on a daily basis?

01:46.840 --> 01:56.960
So I basically check archive CSP at around 6 p.m. every day and I scan all the titles

01:56.960 --> 02:02.520
and if the title interests me, then I read the abstract.

02:02.520 --> 02:08.360
Then if I'm still interested, then I would scan their tables and figures.

02:08.360 --> 02:16.360
And if I think the paper is promising, then I would tweet the paper with short summary

02:16.360 --> 02:17.920
and visualization.

02:17.920 --> 02:25.960
He tweets much, many more than I do because he tries to tweet as many good papers as possible.

02:25.960 --> 02:30.440
Well, I try to tweet the ones I think the best.

02:30.440 --> 02:34.560
So yeah, he spends much more time on it than I do.

02:34.560 --> 02:41.400
And yeah, I think that's amazing.

02:41.400 --> 02:47.960
Yeah, so how much time do you spend every day, would you say?

02:47.960 --> 02:49.480
About up to 30 minutes.

02:49.480 --> 02:51.920
I wouldn't spend more than that.

02:51.920 --> 02:54.560
You know, I'm a researcher.

02:54.560 --> 03:00.360
So whether I tweeted about it or not, I have to scan all the archive papers anyway.

03:00.360 --> 03:04.560
So I think this time investment is totally worth it.

03:04.560 --> 03:05.560
Okay.

03:05.560 --> 03:12.560
So yeah, how many abstracts do you end up reading every day, would you say like 10, 50, 100?

03:12.560 --> 03:15.680
Yeah, it depends on the day.

03:15.680 --> 03:19.120
Sometimes it's very, the field is very productive.

03:19.120 --> 03:26.640
Like yesterday, I read like 10 abstracts, but I usually read only like three abstracts.

03:26.640 --> 03:33.720
Yeah, I have probably because I have very specific tastes for papers.

03:33.720 --> 03:38.680
I'm more biased towards NLP papers, for example.

03:38.680 --> 03:45.480
Yeah, to go back to the reason why you're here today, you were tweeting as every day

03:45.480 --> 03:53.520
and one of the tweets wrote was an answer to my AGI political compass, the latest one.

03:53.520 --> 03:59.240
And you were saying that you were not on the compass because you were higher than some

03:59.240 --> 04:02.400
ultimate or something more extreme than his position.

04:02.400 --> 04:08.120
I know this is a joke and I'm sorry to bring something from Twitter on a podcast, but maybe

04:08.120 --> 04:13.080
could you summarize your take for, you know, the listeners that are not on Twitter every

04:13.080 --> 04:14.080
day?

04:14.680 --> 04:20.360
Oh yeah, basically what I say is something like, so he said, is that I'm not worried

04:20.360 --> 04:22.680
about AI alignment problems.

04:22.680 --> 04:29.680
And I think I say, I don't consider this task of alignment to be any different from any

04:29.680 --> 04:32.240
other half ML task.

04:32.240 --> 04:39.000
But as we talked about it, I think I am wrong about this.

04:39.000 --> 04:41.760
Yeah, so we can talk about it later.

04:41.840 --> 04:49.160
And I think I also say we should focus on that long term, which we agree, I guess, and

04:49.160 --> 04:50.840
scaling is important.

04:50.840 --> 04:54.760
But unlike Ethan, I don't think it's all you need.

04:54.760 --> 05:02.600
And I don't know about AGI, but I think superhuman level, recursively self-improving

05:02.600 --> 05:09.920
language model, maybe possible somewhere around 2028 to 2038.

05:09.920 --> 05:10.920
Yeah.

05:10.920 --> 05:15.080
And I think there's a lot of disagreement on Twitter between focusing on the long term

05:15.080 --> 05:16.920
or focusing on the near term.

05:16.920 --> 05:23.040
And I think there's a bunch of confusion because for people who have short AI timelines, their

05:23.040 --> 05:25.200
long term is kind of the near term.

05:25.200 --> 05:29.840
So for you like 2028, 2038, it's kind of the long term future, right?

05:29.840 --> 05:34.200
Because things are going to be a lot of different then.

05:34.200 --> 05:37.720
So and yeah, people are also confused about definitions.

05:37.720 --> 05:44.360
So AGI, artificial intelligence, or human level, or superpower intelligence.

05:44.360 --> 05:49.800
So maybe just to ground the discussion a bit, could you give your definition of AGI?

05:49.800 --> 05:54.320
So what's the definition you think about when you see that word on Twitter?

05:54.320 --> 05:55.320
Yeah.

05:55.320 --> 06:02.760
So I'm really interested in AGI, but to be honest, I'm not an expert on it.

06:02.760 --> 06:07.480
So I'm just, I'm trying to learn about it.

06:07.480 --> 06:11.440
So I don't know if my terminology or definition is correct.

06:11.440 --> 06:18.120
But to me, AGI is something like a model that can recursively improve itself and can perform

06:18.120 --> 06:24.840
any task, at least as well as humans do.

06:24.840 --> 06:29.600
Personally, I'm, oh yeah, go ahead, sir.

06:29.600 --> 06:36.760
I was just going to say that, you know, so if we think about like human level, not all

06:36.840 --> 06:41.360
humans are capable of writing ML code or thinking about AI.

06:41.360 --> 06:46.480
If you just take someone, some average human on the street, they will not be able to improve

06:46.480 --> 06:49.520
an AI or self-improve.

06:49.520 --> 06:55.040
So yeah, I think one of the definitions, sorry.

06:55.040 --> 07:02.440
Yeah, one of the reasons people give for why an AI would be able to self-improve if it was

07:02.560 --> 07:10.200
human level is that a human given like enough time and memory could be able to like read

07:10.200 --> 07:17.200
all those archive papers and, you know, come up with another solution, assuming like our,

07:17.200 --> 07:23.760
you know, like there's not like a problem in the architecture of humans that, you know,

07:23.760 --> 07:28.760
would make it impossible to like improve or something.

07:28.760 --> 07:33.360
So yeah, I guess that's kind of one of the reasons people might like not make a distinction

07:33.360 --> 07:40.360
between those two, like human level and, you know, recursively self-improvement.

07:40.360 --> 07:45.760
But yeah, I think that's like a reasonable guess to like put those very close.

07:45.760 --> 07:46.760
Yeah.

07:46.760 --> 07:55.760
So when I say human level, just like many other people, I think I'm saying that as well

07:56.760 --> 08:02.760
doing each task as well as top human experts, so not like other humans.

08:02.760 --> 08:07.760
You know, other humans, like you said, you cannot really write machine learning papers.

08:07.760 --> 08:12.760
I don't remember whether you said that or yeah, but that's what I mean.

08:12.760 --> 08:17.760
So yeah, that's basically what I meant.

08:17.760 --> 08:27.760
So in particular, I'm interested in AGI's capability to do research because that's basically doing

08:27.760 --> 08:36.760
doing research is basically or doing ML research is basically recursive self-improvement.

08:36.760 --> 08:46.760
And also, you know, it can advance other areas of science or yeah.

08:46.760 --> 08:55.760
So I think so as long as a machine learning model can do machine learning research as well as humans do,

08:55.760 --> 09:02.760
I think that's that leads to AGI eventually without any human intervention.

09:02.760 --> 09:09.760
So a human level machine learning research is AGI complete in some sense.

09:09.760 --> 09:15.760
And I'm trying to make language model to machine learning research.

09:15.760 --> 09:20.760
Yeah, I think that's a valid path to AGI, even though there are many other path.

09:20.760 --> 09:23.760
Yeah, just to clarify the definitions.

09:23.760 --> 09:29.760
When people say AGI complete, they usually mean you need AGI to reach that point.

09:29.760 --> 09:37.760
What you're saying is doing ML research enables human level AI, right, or AGI.

09:37.760 --> 09:41.760
So you're more saying like it implies, right?

09:41.760 --> 09:48.760
It's being good at ML research and being able to do as much ML research as you want implies AGI.

09:48.760 --> 09:50.760
Is that what you're saying?

09:50.760 --> 09:51.760
Yeah.

09:51.760 --> 09:52.760
Okay.

09:52.760 --> 10:01.760
And I think so I agree with some version of that, but then it depends on what you consider to be ML research.

10:01.760 --> 10:20.760
So for instance, is building like ML hardware and transistors and building factories that produce electricity that would power those ML hardware.

10:20.760 --> 10:22.760
Is that all ML research?

10:22.760 --> 10:24.760
Why do you draw the line?

10:24.760 --> 10:25.760
Yeah.

10:25.760 --> 10:36.760
For a human level machine learning research, I mean, to be able to replace pretty much every ML research staff, ML research project.

10:36.760 --> 10:40.760
So including software.

10:40.760 --> 10:43.760
Yeah.

10:43.760 --> 10:45.760
That's what I meant.

10:45.760 --> 10:47.760
Okay.

10:47.760 --> 10:52.760
Yeah, I guess my take is that, you know, the economy is very complex.

10:52.760 --> 10:57.760
And you need like energy to do things and robots to build things.

10:57.760 --> 11:16.760
And so the kind of the task that would, so more like the intelligence level to be able to reproduce like all that branch of the economy that builds computers is kind of automating like 10% of what humans are able to do right now.

11:16.760 --> 11:30.760
If you really wanted to like self-improve and build bigger and bigger computers without like having to call a human to do the task for you, then you would need to be pretty close to AGI already.

11:30.760 --> 11:32.760
Yeah, that's true.

11:32.760 --> 11:51.760
Given what we said about recursive self-improvement requiring a hardware and robots building the hardware, would you say like recursive self-improvement will arrive much later than, you know, ML research in archive papers?

11:51.760 --> 11:58.760
You mean being able to write ML papers.

11:58.760 --> 12:06.760
Right, or yeah, being able to write the papers and maybe like a run experiments by yourself without like actually like improving your hardware.

12:06.760 --> 12:07.760
Yeah, I agree.

12:07.760 --> 12:12.760
I think software improvement comes much earlier than hardware improvement.

12:12.760 --> 12:25.760
But I think so, given if you look at the past ML research and scaling, most of the scaling comes from additional funding.

12:25.760 --> 12:31.760
But and some of the scaling comes from hardware improvement.

12:31.760 --> 12:51.760
But I think like a lot of improvement simply comes from software improvement, like improvement in design, like the transition from LSDM to transformer or like scaling, optimal scaling strategy.

12:51.760 --> 12:59.760
So I think there's a lot of things AIs can do simply by improving the software.

12:59.760 --> 13:06.760
And yeah, just to go back to what you said before about scaling is not all you need.

13:06.760 --> 13:11.760
Yeah, why do you say you disagree with Ethan Caballero on that?

13:11.760 --> 13:13.760
Obviously scaling is very important.

13:13.760 --> 13:23.760
And if you look at the result of palm over, well, smaller palm or other models, then you can see that the improvement is huge.

13:23.760 --> 13:34.760
And I think that's a very big indication that we still haven't exhausted all the space for improvement from coming from scaling.

13:34.760 --> 13:54.760
But I don't say scaling is all you need because as I talked about, the improvement coming from performance improvement coming from model design improvement is huge.

13:55.760 --> 14:04.760
The most important one is improvement in scaling strategy or called optimal scaling strategy.

14:04.760 --> 14:13.760
For example, open AIs paper, that brings from 10 to 100 times of speed up.

14:13.760 --> 14:29.760
So basically the model optimally scaled up performs as well as the model suboptimally scaled up using from 10 to 100 times of a compute.

14:29.760 --> 14:39.760
This result was demonstrated, but technically this is possible.

14:39.760 --> 14:50.760
So yeah, and also, you know, the improvement coming from VQVA, I mean, Dali 1 to Dali 2.

14:50.760 --> 15:00.760
I think this huge leak is not possible simply by increasing the computational budget spent on Dali 1.

15:00.760 --> 15:12.760
So this is probably because I think this is because there is a fundamental bottleneck with the design of VQVA or Dali 1.

15:12.760 --> 15:22.760
So there are some of the things that you cannot simply overcome with additional by adding more copies.

15:23.760 --> 15:31.760
Yeah, so what was the paper you mentioned where you go from a 10x improvement to a 100x?

15:31.760 --> 15:33.760
Like you said, the open AIs paper?

15:33.760 --> 15:36.760
Yeah, scaling goes for neural laggis models.

15:36.760 --> 15:42.760
So that's like a way of scaling your models optimally.

15:42.760 --> 15:50.760
And what you're saying is that you cannot just scale without thinking about the optimal scaling.

15:50.760 --> 15:53.760
Is that what you're basically saying?

15:53.760 --> 16:03.760
Yeah, basically not using this kind of principle way of scaling is a common practice before this paper.

16:03.760 --> 16:11.760
But after this, basically most many of the big projects tried to follow this scaling now,

16:11.760 --> 16:22.760
which I think is very important for saving the copies or maximizing the performance.

16:22.760 --> 16:33.760
And yeah, one of the things that we mentioned in the episode with Ethan was that you kind of influence him to be more interested in scaling

16:33.760 --> 16:37.760
because at the beginning it was not that much into scaling,

16:37.760 --> 16:43.760
but then maybe like a few years ago you told him that scaling was going to be huge.

16:43.760 --> 16:47.760
So yeah, how did you get interested in scaling?

16:47.760 --> 16:52.760
Yeah, what's here or what was the kind of thing that got you into it?

16:52.760 --> 16:55.760
Yeah, so I first got into machine.

16:55.760 --> 17:01.760
So I started reading machine learning papers on the summer of 2017.

17:01.760 --> 17:11.760
Then Transformer paper was released and soon got into language modeling because I thought that would be the key for AGI.

17:11.760 --> 17:20.760
Then yeah, I was almost immediately and talked about several months to be convinced of Transformer language model,

17:20.760 --> 17:24.760
replacing all the LSDMs in every applications.

17:24.760 --> 17:35.760
Then I read a paper titled Exploring the Limits of Language Modeling, which was released in 2016.

17:35.760 --> 17:44.760
So this paper basically tries to scale up the size of LSDM and dataset size to improve that publicity.

17:44.760 --> 17:53.760
And I did their generated text and it doesn't look so much better than any of the text I saw and their publicity is so much better.

17:53.760 --> 17:59.760
So it's kind of like a GBT2 moment for me except it was like 2017.

17:59.760 --> 18:12.760
So that's basically the first scaling thing I saw and there was also another paper titled Deep Learning Scaling is Predictable and Predictable.

18:12.760 --> 18:17.760
I think Ethan mentioned that it was released in 2017 too.

18:17.760 --> 18:26.760
It shows that there's a parallel between training cards, performance, model size and dataset size.

18:26.760 --> 18:31.760
But that doesn't really tell you exactly how to scale up the models.

18:31.760 --> 18:39.760
Then finally there's GPT2, which was in January of 2019.

18:39.760 --> 18:46.760
By then I was already convinced that scaling is going to be the key.

18:46.760 --> 19:05.760
Yeah, if you were convinced of scaling before Transformer or in 2017, then seeing GPT2 in beginning of 2019 must be enough or not a big update from 2017.

19:05.760 --> 19:15.760
Or at least they showed that you could get generation of paragraphs and a bunch of benchmarks in NLP with just scaling.

19:15.760 --> 19:21.760
So yeah, did it surprise you a little bit or were you not surprised at all from GPT2?

19:21.760 --> 19:30.760
No, I was not surprised with GPT3 at all because I was kind of working on a similar project with small scale.

19:30.760 --> 19:40.760
So like before GPT2, we language model researchers were working on better sampling techniques.

19:40.760 --> 19:44.760
So GPT2 used, I think, top-K sampling and temperature.

19:44.760 --> 19:55.760
So basically these two were rediscovered in 2018 and I was just playing with these new sampling methods.

19:55.760 --> 20:04.760
And so the generated text was much better than anything I saw before.

20:04.760 --> 20:13.760
So yeah, I was not surprised with the result at all, but I was very happy.

20:13.760 --> 20:17.760
Do you want to tell us more about the project you were working on?

20:17.760 --> 20:24.760
Oh, you mean the project I was working on in 2019?

20:24.760 --> 20:33.760
Yeah, so you said it was a project that involved top-K sampling or other methods that were not the same as GPT2?

20:33.760 --> 20:38.760
Oh yeah, it was not like a big project.

20:38.760 --> 20:51.760
So I was just trying to use top-K sampling and temperature sampling as a small transformer language model I was trying in 2018.

20:52.760 --> 20:56.760
Then the result was so much better.

20:56.760 --> 21:04.760
So I guess that's all I can tell, but there's a project I did in 2019 about scaling.

21:04.760 --> 21:08.760
Yeah, it's called one equal piece all you need.

21:08.760 --> 21:13.760
So, okay, can I talk about this project?

21:13.760 --> 21:15.760
Yeah, sure, go ahead.

21:15.760 --> 21:28.760
So basically, okay, so nowadays we're trying a huge model on huge data set for one or a few epochs,

21:28.760 --> 21:38.760
but back then we were training smaller models for many, many iterations with very, very small data set.

21:38.760 --> 21:44.760
Even GPT2, GPT2 used 100 epochs, I think.

21:44.760 --> 21:45.760
So bad.

21:45.760 --> 21:47.760
Yeah, exactly.

21:47.760 --> 21:51.760
The data set wasn't that big, like only 40 gigabytes.

21:51.760 --> 21:54.760
Well, it was huge back then.

21:54.760 --> 22:04.760
Yeah, so the model size was only one billion, even though AI can totally spend more money on that.

22:04.760 --> 22:21.760
Okay, so basically this shift from old days to today, it was this open AI scaling paper,

22:21.760 --> 22:25.760
and scaling laws for neural language model.

22:25.760 --> 22:30.760
And we have many other papers like Tintera.

22:30.760 --> 22:46.760
But actually, I wrote, I did a project in 2019 where I sort of formed all these nice scaling ideas by myself.

22:46.760 --> 22:51.760
So, and I wrote a paper called one equal piece all you need.

22:51.760 --> 22:54.760
Yeah, so, okay, let me talk about that.

22:54.760 --> 23:07.760
Then, so the, so there I had a bunch of ideas and tried to verify these ideas with experiments using very small amount of computers.

23:07.760 --> 23:17.760
And so first idea is that it is easy to analyze the pre-training data set so that one has to train only one or a few epochs.

23:17.760 --> 23:21.760
Which dramatically improves the performance compute trade-off.

23:21.760 --> 23:30.760
So, basically, yeah, I'm just advocating, let's just try for one epoch and use big data set.

23:30.760 --> 23:45.760
Yeah, and the second idea is that so let's compute the optimal ratio of model size and number of tokens for given compute budget based on training curves.

23:45.760 --> 23:53.760
So, back then, the models were too small and they used too many iterations.

23:53.760 --> 24:05.760
So let's just, you know, adjust this ratio nicely so that we don't have to waste all the compute for like hundreds of epochs.

24:05.760 --> 24:09.760
So is the ratio model size and data set size?

24:09.760 --> 24:11.760
Yeah, that's right.

24:11.760 --> 24:20.760
So, actually, Ginger also did measure this ratio.

24:20.760 --> 24:32.760
So basically, they computed the scaling exponents for optimal data set size versus optimal model size.

24:32.760 --> 24:40.760
And then they found that the scaling exponents for this to both 0.5.

24:40.760 --> 24:44.760
So that means they both linearly increase.

24:44.760 --> 24:53.760
So you can just measure the slope of this line, which gives you that optimal ratio.

24:53.760 --> 25:00.760
Yeah, so basically, you would need to scale your data set size and model size the same amount.

25:00.760 --> 25:09.760
So if you want to, you know, build GPT-4, you might just want to double the number of parameters and double the data set size.

25:09.760 --> 25:11.760
Yeah, exactly.

25:11.760 --> 25:18.760
And so, yeah, right now, so now we're not on 2019-2020 anymore, we're in 2022.

25:18.760 --> 25:28.760
And I believe so you're working at Iluth AI and Google as an intern on some scaling work.

25:28.760 --> 25:33.760
And I believe you might not want to talk about your private work on the podcast.

25:33.760 --> 25:38.760
But yeah, what kind of work do you do publicly on scaling you won't be happy to talk about?

25:38.760 --> 25:40.760
Oh, yeah, definitely.

25:40.760 --> 25:43.760
So, okay, let me think.

25:43.760 --> 25:50.760
So I'm currently very interested in Instruct GPT and T0.

25:50.760 --> 25:52.760
So both of these.

25:52.760 --> 25:56.760
Could you maybe summarize what's T0 for people who were not read the paper?

25:56.760 --> 25:57.760
Yeah.

25:57.760 --> 26:04.760
So T0 is basically a masked language model with multi-task point joining.

26:04.760 --> 26:16.760
So first of all, masked language model is an easily encoder on the encoder-decoder model that is trying with a masked language model objective.

26:16.760 --> 26:25.760
So this training is basically, so let's say you have some text, then you can randomly mask.

26:26.760 --> 26:31.760
As some random spans of tokens.

26:31.760 --> 26:38.760
So they, then you want your model to predict these masked tokens.

26:38.760 --> 26:39.760
Yeah.

26:39.760 --> 26:45.760
That's basically a masked language model or also called denoised audio encoding.

26:45.760 --> 26:46.760
Yeah.

26:46.760 --> 26:49.760
So that's what masked language model is about.

26:49.760 --> 26:54.760
And T0 is a masked language model.

26:54.760 --> 27:06.760
That is also fine tune from a bunch of many, many data sets, like maybe like Super Glue.

27:06.760 --> 27:10.760
So what's Super Glue?

27:10.760 --> 27:15.760
Super Glue is a standard, natural language understanding data set.

27:15.760 --> 27:16.760
Yeah.

27:16.760 --> 27:21.760
Maybe it has not trained on, maybe they did not use Super Glue for T0.

27:21.760 --> 27:23.760
But basically, that's the idea.

27:23.760 --> 27:36.760
So the reason why we want to fine tune T0 on a bunch of these data sets is that if you train like this, then it generalises,

27:36.760 --> 27:42.760
obviously it performs very well on the task it was fine tuned on.

27:42.760 --> 27:49.760
But it also performs very well on the tasks it was not fine tuned on.

27:49.760 --> 28:05.760
So we know that GPT like models performs much worse than the GPT like model that is fine tuned on the task you are trying to deal with.

28:05.760 --> 28:06.760
Right.

28:06.760 --> 28:21.760
So yeah, basically this multi-task fine tuning allows your model to perform very well on the task, not only the tasks it was trained on,

28:21.760 --> 28:25.760
but also the tasks it was not trained on.

28:25.760 --> 28:33.760
So T0 actually can perform much better than GPT3 on many tasks.

28:33.760 --> 28:46.760
Without fine tuning the model on this task specifically, while using like 10 times less complex than GPT3.

28:46.760 --> 28:56.760
So is the idea that you train it on a bunch of different tasks, so not fine tuning, but you train it on a bunch of different tasks?

28:56.760 --> 29:00.760
I think that's what you said, multi-task training or multi-task fine tuning?

29:00.760 --> 29:02.760
Yeah, multi-task.

29:02.760 --> 29:10.760
And then it's able to generalize well on held out tasks, it doesn't seem before like zero shots?

29:10.760 --> 29:11.760
Exactly.

29:11.760 --> 29:23.760
So you said you were interested in T0 and also in Struggle GPT, I believe in Struggle GPT was a model or a training procedure from OpenAI.

29:23.760 --> 29:26.760
Can you tell us more about in Struggle GPT?

29:26.760 --> 29:27.760
Yeah.

29:27.760 --> 29:42.760
So in Struggle GPT is also functioned on many different tasks like T0, but it also uses human feedback.

29:42.760 --> 29:57.760
So like basically they train the model to score the bunch of the generated results.

29:57.760 --> 30:13.760
And this model tells this GPT3 to how to generate the text.

30:13.760 --> 30:20.760
So this process is done by using reinforcement learning called PPO.

30:20.760 --> 30:29.760
So this additional component improves GPT3 significant.

30:29.760 --> 30:48.760
Yeah, so basically even though GPT3 doesn't perform as well as PPO, given the amount of compute it consumes, it performs very, very well on many different tasks.

30:48.760 --> 30:57.760
So without having to, yeah, I think it performs very well even without using future samples.

30:57.760 --> 31:08.760
So yeah, GPT3, sorry, Instruct GPT and T0 are some of the most compute efficient models out there.

31:08.760 --> 31:11.760
So yeah, I'm very interested in these models.

31:11.760 --> 31:20.760
And my project is basically trying to combine all these with scaling.

31:20.760 --> 31:34.760
Right, so you kind of want to combine this URL from human feedback procedure from Instruct GPT with the pre-training from T0.

31:34.760 --> 31:43.760
Yeah, and I'm thinking that we can do some interesting scaling analysis on this model for several reasons.

31:43.760 --> 31:48.760
First of all, optimal scaling we do for GPT-like models.

31:48.760 --> 32:01.760
We usually try to optimize the test application, and this model only has only decoded, unlike T0, which has encoded and decoded.

32:01.760 --> 32:12.760
By the way, encoded decoded model performs much better than decoded on the model when it is fine-tuned or multi-task fine-tuned.

32:12.760 --> 32:16.760
And that's why I'm thinking of this encoded decoded model.

32:16.760 --> 32:26.760
And for people who are not into deep learning or NLP, so can you just give an example of decoder and encoder decoder?

32:26.760 --> 32:35.760
So I think GPT2 is an decoder because it gets a prompt and then just generates a paragraph.

32:35.760 --> 32:37.760
What's an encoder and decoder?

32:37.760 --> 33:01.760
Yeah, so encoder decoder is basically encoder is like the model architecture used for Word, or like the first transformer paper architecture, and decoder is our usual decoder.

33:01.760 --> 33:12.760
So basically you want to feed your prompt into encoder, and then you would feed the output to the decoder with self-attention.

33:12.760 --> 33:20.760
That's what encoder decoder is, and it happens to perform very well in this situation.

33:21.760 --> 33:30.760
Yeah, so basically we have encoder decoder and fine-tuning.

33:30.760 --> 33:40.760
Yeah, so I think these elements make the scaling load very different from the local GPT models.

33:40.760 --> 33:48.760
Oh yeah, and also we want to optimize for downstreaming performance instead of test public HD.

33:48.760 --> 34:16.760
So I think these conditions force the model to be bigger and train shorter on free-tuning tasks because fine-tuning is so important that maybe we can make the model bigger while focusing less on the free-tuning.

34:16.760 --> 34:34.760
And I think this is, so the current state-of-the-art model has like 100 billion amateurs and trains on trillions of tokens, which is very different from how human planes run.

34:34.760 --> 34:47.760
Because our plane has like hundreds of billions of neurons, meaning like hundreds of trillions of synapses.

34:47.760 --> 35:05.760
So yeah, I think that means it has more capacity than the former capacity than models do, and it is only trying on like a few billions of tokens because that's how many tokens we can process within our lifetime.

35:05.760 --> 35:14.760
So yeah, basically I'm trying to make the models closer to how human planes learn.

35:15.760 --> 35:26.760
So you're trying to write a scaling law that would be closer to the amount of data humans process throughout their lifetime?

35:27.760 --> 35:38.760
Yeah, and I'm not like trying to make this forcibly similar to human planes constraints.

35:38.760 --> 35:58.760
I'm just thinking that this new model based on this G0 instructivity, I think it will perform the best if we try and like how human planes learn.

35:59.760 --> 36:17.760
I'm not sure I understand the methods to have this scaling law, so would you need to, you know, train T0 with like some kind of instructivity fine-tuning,

36:17.760 --> 36:34.760
and then you test on a bunch of held out tasks, and then you would see like you would plot a curve of like optimal scaling with respect to those like done streaming tasks?

36:34.760 --> 36:53.760
So basically what I'm saying is to train a bunch of different T0 with instructivity fine-tuning with different number of tokens pre-trying and different models.

36:54.760 --> 37:12.760
I think I understood now, so you're trying to, the same as Ginchilla, like get the exponents for dataset size and for model size, and then you're trying to see if it's not like 0.5 and 0.5, but like maybe something else?

37:12.760 --> 37:14.760
Yeah, something like that.

37:14.760 --> 37:37.760
I think your most well-known for your scaling work at Eleuther AI, where you train, was it 6 billion parameters, or at least like you try to reproduce the results from GP3, 6.7 billion, and what was called GPTJ, I think for GPTJacks.

37:38.760 --> 37:49.760
Yeah, can you just like give us a rough summary of this project, why you started this project, and what do you want to release it to the public?

37:49.760 --> 38:00.760
Yeah, so around the beginning of 2020, I was trying to reproduce Dali 1 with some of the people in Eleuther AI.

38:00.760 --> 38:13.760
So basically, Dali 1 consists of BQBAE encoder decoder and transformer language model for generating the discrete 11 variables.

38:13.760 --> 38:24.760
So that way, this transformer language model is exactly, almost exactly the same as GPT3 except for the size.

38:24.760 --> 38:37.760
So I thought we may be able to make the maximum impact if we reuse this model for our GPT3 production.

38:37.760 --> 38:46.760
Yeah, and then at the same time, I thought, so Jaxx was becoming more and more popular.

38:46.760 --> 39:04.760
Obviously, Jaxx is optimized for TB use, and we had a lot of TB use back then, and Jaxx is, so before that, we had our GPT3 replication, which is called GPT Neo.

39:04.760 --> 39:22.760
It was implemented using mesh tensorflow, and this mesh tensorflow decoding speed is so slow, almost ridiculously slow, especially compared with Hytotes.

39:22.760 --> 39:28.760
But Jaxx has no problem with that, it's almost as fast as Hytotes.

39:28.760 --> 39:33.760
So we decided to use Jaxx for this project.

39:33.760 --> 39:57.760
Yeah, so basically, I was supposed to work on the encoder decoder code, and I asked another guy in Eleuther AI, his name is Ben1, and I asked him to work on this language model site.

39:57.760 --> 40:03.760
But admittedly, he spent far more time on this project than I did.

40:03.760 --> 40:13.760
How much time would you say took you, maybe like six months between the beginning of 2021 and when you guys released the model?

40:13.760 --> 40:34.760
I think it only took several months, so this project is kind of impressive because there are only two people in it, and we only spent like three months, and we basically open sourced the best language model.

40:34.760 --> 40:39.760
So that is something I'm proud of.

40:39.760 --> 40:42.760
You're right to be proud of this, it's pretty cool.

40:42.760 --> 40:44.760
Yeah, thank you so much.

40:44.760 --> 40:51.760
So, by the way, GPTJ is different.

40:51.760 --> 41:01.760
Yeah, so basically GPT Neo actually could not really match the same performance with the GPT3 with similar size.

41:01.760 --> 41:14.760
GPT Neo was like 2.7 billion model, but it performed worse than 1 billion GPT3, for example.

41:14.760 --> 41:29.760
But GPTJ performs pretty much as well as 6 billion GPT3, so yeah, the model performs very well, I think.

41:29.760 --> 41:38.760
So it was able to be more efficient than GPT Neo, which I believe is another model from Eleuther AI, but using TensorFlow Mesh, so a bit older, right?

41:38.760 --> 41:40.760
Yeah.

41:40.760 --> 41:55.760
And yeah, so to match that performance from GPT3, did you just like took the same API parameters and architecture from GPT3 paper, or did you like had to change stuff to, you know, match the performance?

41:55.760 --> 42:05.760
Yeah, so I think we could be using the exact same architecture with GPT3, but we just wanted to do a bit more.

42:05.760 --> 42:15.760
So first of all, yeah, one thing we tried is leisure with the depth ratio.

42:15.760 --> 42:22.760
So basically with, I'm referring to the hidden dimensions of the model.

42:22.760 --> 42:31.760
So we are trying to build a wider model rather than deeper model.

42:31.760 --> 42:45.760
And this is important because generally speaking, wider models can utilize accelerators more efficiently, and latency is much better.

42:45.760 --> 42:56.760
So yeah, and when we tried this wider model, we observed that we don't really lose much of performance from this.

42:56.760 --> 42:59.760
So I think with this is worth it.

42:59.760 --> 43:10.760
And another thing we tried is placing feedforward layer with attention layer in parallel.

43:10.760 --> 43:12.760
Yeah.

43:12.760 --> 43:23.760
Basically, this is, this also saves latency, and you can also make your accelerators utilize better.

43:23.760 --> 43:27.760
And this was actually also adapted by Paul.

43:27.760 --> 43:36.760
So I think this is something, I think this is a nice contribution from this project.

43:36.760 --> 43:44.760
So you think palm researchers read GPTJ and thought like, oh yeah, this architecture change is very good. We're going to use it.

43:44.760 --> 43:53.760
Yeah, exactly. I think they also sort of followed our wide model because their model is also very, very wide.

43:53.760 --> 43:58.760
I think it's even wider than ours.

43:58.760 --> 44:06.760
I read the blog post wrote about GPTJ. And so I kind of read about like all those tricks you did.

44:06.760 --> 44:09.760
And you talk a lot about throughput.

44:09.760 --> 44:17.760
So yeah, I'm curious like what's the throughput for people who are not like scaling models all the time.

44:17.760 --> 44:27.760
And yeah, how does compare, you know, how does GPTJ come compared to like GPT3 or GPTNEO in terms of throughput?

44:27.760 --> 44:32.760
Is it more efficient, less efficient, more throughput, less throughput?

44:32.760 --> 44:39.760
Yeah, so throughput, I mean the number of tokens processed parameter per second.

44:39.760 --> 44:50.760
So if you can prove this throughput, then we can try with a larger model or more tokens with the same amount of throughput.

44:50.760 --> 44:54.760
So this way you can improve performance.

44:54.760 --> 45:00.760
So it's per, so it's amount of token processed per parameter and per second?

45:00.760 --> 45:01.760
Yeah.

45:01.760 --> 45:12.760
If you have more, a lot of parameters, you will, I don't know, does the model size then change your throughput then?

45:12.760 --> 45:19.760
Yeah, I guess you're only comparing like size models of the same size of GPT3 and GPTJ, so it's fine.

45:19.760 --> 45:28.760
So that typically throughput is defined something like two lobs per second.

45:28.760 --> 45:44.760
Oh yeah, yeah, so yeah, maybe a bit confusing, but let's say you have a one billion model, require, and you have TPU, one core.

45:44.760 --> 45:53.760
If it spends one second per, let's say it spends one second per one token.

45:53.760 --> 46:11.760
Then if you have two billion model and one core TPU, then it will take 0.5 seconds because they have the same throughput and something like that.

46:11.760 --> 46:24.760
Yeah, so the throughput of the GPTJ six billion model for training is like 150 tokens per second.

46:24.760 --> 46:39.760
On the other hand, GPTNEO with 2.7 billion parameters is also 150 tokens per second, but this model is half the size of GPTJ.

46:39.760 --> 46:49.760
So basically, this means that we achieved twice improvement in efficiency for throughput.

46:49.760 --> 46:56.760
So yeah, I think this different improvement is huge.

46:56.760 --> 47:04.760
I think it's coming from this wider model using JAX instead of mesh TensorFlow.

47:04.760 --> 47:13.760
Yeah, and not to mention that GPTJ has much better downstream performance than GPTNEO.

47:13.760 --> 47:19.760
Yeah, how long did it take to complete like an entire training because I know it requires a lot of computes.

47:19.760 --> 47:28.760
Yeah, so we spent five weeks using 256 cores of TPU V3.

47:28.760 --> 47:39.760
Yeah, what do you do during those five weeks? Do you just look at the TensorFlow curve and the next answer boards and check it doesn't have any weird spikes?

47:39.760 --> 47:47.760
Yeah, basically, that's what we did. There was no back because we already solved all the bugs.

47:47.760 --> 47:58.760
And so basically, Ben, babysit this training for five weeks, he was complaining a bit, but he said it was not too bad.

47:58.760 --> 48:08.760
And then at the end, you published this model on GitHub, people are very excited and start to use it to fine tune to a bunch of different cases, right?

48:08.760 --> 48:14.760
Yeah, so we don't really fine tune ourselves, but many people try to fine tune.

48:14.760 --> 48:24.760
It appears that GPTJ is more easier to deal with than other models like GPTNEO.

48:24.760 --> 48:34.760
Maybe because the things we use like JAX is much easier to deal with than mesh TensorFlow.

48:34.760 --> 48:37.760
So yeah, it became very popular.

48:37.760 --> 48:42.760
Yeah, from reading the documentation in GitHub, there's a manual on how to fine tune it.

48:42.760 --> 48:48.760
And it seems like the overall code is easier to read as well.

48:48.760 --> 48:59.760
And even people on YouTube like Janine Kilsner use it for their projects like fine tuning on 4chan to generate more comments.

48:59.760 --> 49:04.760
Have you seen this recent YouTube video and if so, what do you think about it?

49:04.760 --> 49:11.760
Yeah, I actually didn't watch the video itself, but I saw the tweet and some people talking about it.

49:11.760 --> 49:13.760
So I know it a little bit.

49:13.760 --> 49:22.760
So my reaction to this entire controversy is kind of like that meme of like entering into a burning room with pizza.

49:22.760 --> 49:32.760
So I think his project is kind of cringy for being too attention seeking and obviously not really ethically sound,

49:32.760 --> 49:42.760
because you can just use this app for many bad things and agree with some of the critical reactions to his project,

49:42.760 --> 49:46.760
even though some of them may be a bit too exaggerated.

49:46.760 --> 50:00.760
But at the same time, I thought this case would lead to more attention spent on a language model that can detect outputs from other language models

50:00.760 --> 50:08.760
so that you can filter out language model generated submissions on many websites like Reddit.

50:09.760 --> 50:20.760
Yeah, like lotion or Chinese government can use this sort of process to influence social media and election results in the West.

50:20.760 --> 50:28.760
But language models are very good at discriminating language model outputs from human outputs.

50:28.760 --> 50:33.760
So I'm kind of optimistic about that, at least for short term.

50:34.760 --> 50:45.760
What do you think of the fact that publishing GPTJ might have accelerated AI timelines where we might have less time to make AI safe

50:45.760 --> 50:50.760
and align those models with human values?

50:50.760 --> 50:56.760
Yeah, do you think releasing GPTJ wasn't that good for humanity?

50:56.760 --> 51:00.760
Would you have done it before again if you had to?

51:00.760 --> 51:09.760
Yeah, I think releasing GPTJ was a small net positive benefit for humanity.

51:09.760 --> 51:26.760
And so basically, open sourcing language model, there's nobody who releases a language model that is substantially better than the previous state of the art.

51:26.760 --> 51:36.760
Like in my case, in our case, our model performs only slightly better than GPTJ Neo or T5.

51:36.760 --> 51:44.760
So I don't think there was actually like accelerating the timeline or anything.

51:45.760 --> 51:58.760
Yeah, so basically, I don't think there's any one particular person who can make big negative impact by releasing a big language model with the current trend.

51:58.760 --> 52:11.760
If there's anyone then who can make a big negative impact, it'll be someone who like fine tune the model to spread misinformation.

52:12.760 --> 52:17.760
Yeah, just to be clear, the GPT Neo was also from A3AI.

52:17.760 --> 52:29.760
So in some sense, if you remove GPT Neo and GPTJ, then you don't really have any open source implementation of the GPT3 that works on the internet.

52:29.760 --> 52:40.760
And so maybe like you wouldn't have like all those other companies like Microsoft or Chinese or Korean companies using this implementation or even like the pile of the public data sets to train their models.

52:40.760 --> 52:45.760
So in some sense, we're kind of helping the entire research community go faster on those topics.

52:45.760 --> 52:49.760
And yeah, so maybe there are like other open source projects that would emerge.

52:49.760 --> 53:00.760
But then the question is like how much, you know, releasing GPTJ in 2021 accelerates those timelines compared to the others.

53:00.760 --> 53:12.760
And I would say like OpenAI releasing the neural scaling laws or GPT3 or even GPT2 kind of showed that like scaling was important to get to AGI.

53:12.760 --> 53:17.760
So in some sense, they kind of released some secret cells and everyone started following them.

53:17.760 --> 53:25.760
So, you know, if they didn't publish or publish a bit later, maybe the timelines would be a bit longer.

53:25.760 --> 53:27.760
What do you make of that?

53:27.760 --> 53:35.760
Yeah, so actually there was T5, open source T5.

53:35.760 --> 53:38.760
That's not the same as GPT3, right?

53:38.760 --> 53:45.760
That's right, but you can fine tune the model like GPT410.

53:45.760 --> 53:50.760
So yeah, I think it was totally possible to do it.

53:50.760 --> 54:00.760
Before GPT410, I think there was GPT2 reproduction from several people, several different groups.

54:00.760 --> 54:10.760
Also, like Facebook and Google released some decoder on the model that performs well.

54:10.760 --> 54:25.760
I don't think it affected on research because researchers, I think all these big companies like Google, they already had much bigger language models internally.

54:25.760 --> 54:30.760
And people in academia, they cannot really affect.

54:30.760 --> 54:42.760
And I don't think they had those big language models before, but I don't think they can really contribute to this large language model research because they don't have budget.

54:42.760 --> 54:48.760
So I don't think my, our projects really accelerated the research timeline.

54:48.760 --> 54:57.760
But I think, yeah, maybe slightly accelerated the open sourcing language model timeline.

54:57.760 --> 55:06.760
Yeah, maybe you accelerate the open source timeline, but not the private research timeline.

55:06.760 --> 55:08.760
So in some sense, you bring everyone on the same level.

55:08.760 --> 55:16.760
So yeah, definitely our work is slightly accelerating the pace of open sourcing language models.

55:16.760 --> 55:23.760
For example, Facebook recently released 100 full size GPT3 model.

55:24.760 --> 55:36.760
Maybe that was, and yeah, maybe that was a response to our GPTJ or GPT NeoX model, which was released recently.

55:36.760 --> 55:39.760
It has about 20 billion model parameters.

55:39.760 --> 55:51.760
And I think this question of accelerating open source timelines, or at least AI research in general, is important in the context of differential progress.

55:51.760 --> 56:01.760
So not only accelerating AI progress, but how does the speed of AI relate to the speed of AI alignment research?

56:01.760 --> 56:08.760
And I'm not sure if you're very familiar with AI alignment, but in this podcast, we talk about this a lot.

56:08.760 --> 56:15.760
And it might be worthwhile maybe defining alignment or at least like going with another definition you're familiar with.

56:15.760 --> 56:20.760
So yeah, what do you understand of the concept of alignment?

56:20.760 --> 56:23.760
How would you define it if you had to?

56:23.760 --> 56:29.760
Yeah, well, as you know, at the meeting, I'm a beginner on alignment.

56:29.760 --> 56:40.760
So all I can say, if I understand correctly, is alignment research is the research to harness advanced AI to do what we want to do.

56:40.760 --> 56:48.760
And one big problem is considered is the existential risk due to advanced AI.

56:48.760 --> 56:50.760
Yeah, that's pretty much it.

56:50.760 --> 57:19.760
And I think the question is, if we have an AI that is much more than humans and doesn't really care about our values, it might end up optimizing for an objective and just change completely our planet without really caring about humans or stuff.

57:19.760 --> 57:40.760
So we value and I guess this problem might be considered harder or easier depending on how you define the problem or how much time you think humans will have to think about those problems.

57:40.760 --> 58:03.760
And one of those takes is that if you only consider alignment as like an AI problem, if we want to solve AI generally and have models that are able to generalize well, if we program them well, they will be able to like solve alignment.

58:03.760 --> 58:11.760
And if we build them, if we build like good models, they will be aligned by default.

58:11.760 --> 58:20.760
And I guess that's maybe like one of the, is that maybe one of your takes as well or something you think will happen?

58:20.760 --> 58:45.760
Yeah, so for short term, I think we can more or less try to be careful with training like what I'm dropping for like open AI is doing, but in long term, which is what Ethan was referring to like when AI is trying to deceive humans.

58:45.760 --> 58:54.760
I think that's when we can no longer like use the conventional machine learning approach to deal with.

58:54.760 --> 59:07.760
Because, you know, if the AI is much more intelligent than humans, then we have no way to detect whether the model is deceiving or not.

59:07.760 --> 59:11.760
Yeah, so that's something I would be worried about.

59:11.760 --> 59:19.760
I guess the question is, when you have a benchmark, how do you know if the model is not pretending to be good at the benchmark?

59:19.760 --> 59:22.760
Or is it like generally, generally good at the benchmark?

59:22.760 --> 59:31.760
So if your benchmark is truthful Q&A, is it actually truthful or is just like pretending to be truthful for the moment?

59:31.760 --> 59:43.760
And one of the things that Ethan was saying is that all alignment can be considered as inverse scaling problems.

59:43.760 --> 59:54.760
So if you make your model too big at some point, it will, you know, have bad properties.

59:54.760 --> 01:00:04.760
So if you're interested in scaling, you can see that as a scaling problem. So like a good behavior for scaling.

01:00:04.760 --> 01:00:08.760
Yeah, I agree.

01:00:08.760 --> 01:00:21.760
So in terms of benchmarks, you said that for models that can be deceptive, it can be hard to write down a good benchmark because they might be able to bypass it.

01:00:21.760 --> 01:00:33.760
Do you think we might reach a point where we'll be able to have good evaluations and good ways to understand if our models behave correctly or not?

01:00:33.760 --> 01:00:49.760
Yeah, this is going to be at some point. There's going to be no conventional machine learning benchmark that can detect AI that will be malicious or not.

01:00:49.760 --> 01:01:02.760
I think more generally, like benchmarks are quite tricky, especially for language models and NLP where if you just like change the beginning of a word, like if you make the first letter capitalized or not in your benchmark,

01:01:02.760 --> 01:01:10.760
or if you just like change the code base from one GitHub repo to another, you might get completely different results in your benchmark.

01:01:10.760 --> 01:01:22.760
So do you think it's possible to have good NLP benchmarks to evaluate language models in the future and especially for alignment?

01:01:22.760 --> 01:01:34.760
Yeah, obviously we need better benchmarks for NLP models, even outside of the alignment problem, even for short term.

01:01:34.760 --> 01:01:46.760
Sometimes we use automatic methods to evaluate the quality of text, but they tend to be not very well-coordinated with human judgment.

01:01:46.760 --> 01:02:04.760
So I'm advocating for the use of human judgment, but even human judgment sometimes is not very useful because humans do not really understand whether the generated text is factual or not,

01:02:04.760 --> 01:02:14.760
because most of us is not really an expert on the field that the model is talking about.

01:02:14.760 --> 01:02:31.760
So yeah, and for alignment, I think for short term we can probably manage to make up some next benchmark using human evaluations.

01:02:31.760 --> 01:02:53.760
And if it doesn't work, then we will probably use some trained model to evaluate, because we can't understand what, like my example, humans are not good at judging the quality of models anyway.

01:02:54.760 --> 01:03:07.760
But at some point, I think that even training models to evaluate model is not enough, because at that point the models are so much better than humans are.

01:03:07.760 --> 01:03:16.760
And we don't understand the model and we cannot detect whether the model is being malicious or not.

01:03:16.760 --> 01:03:20.760
So in that case, yeah, there's going to be a big problem, I think.

01:03:21.760 --> 01:03:28.760
Yeah, I think you're right that it's hard to evaluate fully a model if you're not an expert on the domain.

01:03:28.760 --> 01:03:42.760
And I think one of the things that we can do is what they did in struggle GPT where they had like a bunch of laborers and people giving evaluations for how much you prefer one output compared to the others.

01:03:42.760 --> 01:03:54.760
And then you can roughly build a reward model that can say if an output is good or not, because you have like all those comparisons between different outputs.

01:03:54.760 --> 01:04:07.760
And yeah, for alignment where you're basically saying that you would want like an AI or another language model to evaluate if the other one is answering correctly or not.

01:04:07.760 --> 01:04:21.760
And at that point, you're kind of doing what people say is bringing a small Godzilla to check if the bigger Godzilla is doing good or not.

01:04:21.760 --> 01:04:27.760
And so that was like from a blog post that was published recently.

01:04:27.760 --> 01:04:42.760
And the problem with that is that at the moment when you're like bringing Godzilla to manage the other bigger one, there's already a problem because now you have two Godzilla's in your city.

01:04:43.760 --> 01:05:01.760
So that's why I think it's a tricky problem to try to align or at least check for deception in those large models is because if you want to like have another model do it, then you need to check this smaller model.

01:05:02.760 --> 01:05:20.760
And yeah, so yeah, what do you make of like AI's aligning AI's? Do you think us humans will be able to like, you know, build this like smaller model, it's providing the other big ones?

01:05:20.760 --> 01:05:35.760
Yeah, I think that that is what's going to happen for a short time. Obviously, we cannot keep doing that for long term, because in the long run, AI is going to be so much smarter than we are.

01:05:35.760 --> 01:05:48.760
So we can no longer make sure whether this small model is doing what we are thinking it's doing.

01:05:48.760 --> 01:06:10.760
Yeah, so like at the point, we need an entirely different intervention. I'm talking in really long term, so it's probably well after AGI. And at the point, my guess is that something like we have to argument our own intellectual capacity with something new or something.

01:06:10.760 --> 01:06:18.760
So we become the AI itself. Have you had of something like that?

01:06:19.760 --> 01:06:37.760
So something like merging with AI's after maybe like some kind of rail curse rail scenario where you upload yourself or maybe like connect with brain competent interfaces to AI. I think that's more like the Elon Musk neural link scenario.

01:06:37.760 --> 01:06:40.760
Yeah, which scenario you're talking about.

01:06:44.760 --> 01:06:48.760
I'm not really familiar with your new link. Maybe the last one.

01:06:49.760 --> 01:07:09.760
Okay. So when you were saying like long term, you were saying like after AGI, so at the beginning of the podcast, you were saying 2028 and 2038 were like lower bound and upper bound for AGI or at least like the rough guess for when it might appear. Is that still correct?

01:07:09.760 --> 01:07:18.760
Yeah. So by that time, I meant like human level language model rather than AGI.

01:07:19.760 --> 01:07:28.760
Human level language model. And yeah, how many years to go from human level language models to AGI?

01:07:29.760 --> 01:07:45.760
I think it's going to take very little amount of time given that this human level language model can do machine learning research well so it can improve itself very rapidly.

01:07:46.760 --> 01:07:55.760
Okay. So when you reach human level language models, they need to do research and then they achieve AGI. Is that your main timeline?

01:07:55.760 --> 01:07:56.760
Yeah.

01:07:56.760 --> 01:08:01.760
And so this might happen in days or weeks?

01:08:02.760 --> 01:08:04.760
Maybe several months.

01:08:04.760 --> 01:08:06.760
Several months, okay.

01:08:07.760 --> 01:08:31.760
Yeah. What would be like the bottleneck at that moment? Like they just like keep improving and but then they need to like figure out how to convince humans to help them build more hardware or would they, I don't know, build the robots themselves?

01:08:32.760 --> 01:08:33.760
Yeah. So how would it happen?

01:08:33.760 --> 01:08:36.760
Hardware is going to be the bottleneck, I think.

01:08:39.760 --> 01:08:46.760
Yeah. Also collaborating with humans, yeah, convincing. And that's going to be, yeah, the main bottleneck.

01:08:46.760 --> 01:09:08.760
Okay. So if you're listening to this podcast in 2028 and you have language models convincing you to build more, you know, GPU centers and let them, you know, make post requests all over the web.

01:09:08.760 --> 01:09:19.760
Please don't. It's a bad idea. They're trying to do more ML research. So you've heard it first here.

01:09:21.760 --> 01:09:31.760
Yeah. So when we were talking about like connecting to neural networks and have our brains maybe like become AI or merge with AI.

01:09:32.760 --> 01:09:37.760
We're talking about something closer to brain computer interfaces as well.

01:09:38.760 --> 01:09:39.760
Yeah.

01:09:41.760 --> 01:09:52.760
So then the question becomes like, well, kind of this timeline of the human slash human plus AI intelligence.

01:09:53.760 --> 01:10:06.760
Like compared to like just pure AI intelligence. So will the humans be able to keep up a bit and, you know, understand what's going on or will they be left behind?

01:10:07.760 --> 01:10:08.760
What do you think?

01:10:08.760 --> 01:10:22.760
Yeah. So I think, yeah, we should control the pace of employment in a way so that we can catch up with the pure natural, pure AI.

01:10:23.760 --> 01:10:32.760
So, yeah, we should absolutely make sure that we don't, we can keep the same pace, I think.

01:10:33.760 --> 01:10:41.760
The problem is that we don't really control that as there are not that many brain computer interface researchers, there's not a lot of funding in there.

01:10:42.760 --> 01:10:44.760
And the rate of progress in AI is accelerating a lot.

01:10:45.760 --> 01:10:58.760
And if the problem is in, you know, neuroscience are much harder than, you know, scaling models, then even if we throw like a lot of money at it, it's going to be very hard to solve.

01:10:59.760 --> 01:11:11.760
Yeah, that's true. But so my guess is that AGI is not going to be so, like, malicious for short land.

01:11:12.760 --> 01:11:18.760
So I'm thinking of using AGI to improve this kind of research for the beginning.

01:11:19.760 --> 01:11:21.760
At the beginning, what do you think?

01:11:22.760 --> 01:11:34.760
So basically, you use a very smart language model, some kind of Oracle where you ask questions and gives like very good answers with like documentation and papers.

01:11:35.760 --> 01:11:38.760
And you ask him like, hey, how do I build a good brain computer interface?

01:11:39.760 --> 01:11:42.760
And it gives you good answers. And then you go on and you start building that.

01:11:43.760 --> 01:11:50.760
The problem is that at the moment when you have this kind of model, you can also ask it like, hey, how do I build a better copilot?

01:11:51.760 --> 01:11:53.760
How do I build a bigger language model?

01:11:54.760 --> 01:11:56.760
And I guess people might want to do this first.

01:11:57.760 --> 01:12:15.760
And you might also need to develop new robotics or actual hard tech microscope and neuroscience tools to do those.

01:12:16.760 --> 01:12:20.760
And it's not going to be like only like a neural network architecture.

01:12:21.760 --> 01:12:33.760
So I guess it's always like the hardware part is going to take longer than just like software or neural networks part.

01:12:34.760 --> 01:12:46.760
Yeah, that's true. We may have to regulate the behavior of AGI so that we can keep the pace together.

01:12:47.760 --> 01:12:49.760
How do you regulate that?

01:12:50.760 --> 01:13:04.760
We need some physical access to AGI first, the AI, like physically preventing the models being scaled up too fast.

01:13:05.760 --> 01:13:18.760
So you basically have an AI become the president and then implement some security measures to prevent people from building bigger AIs?

01:13:19.760 --> 01:13:20.760
Yeah.

01:13:21.760 --> 01:13:23.760
I'm not sure people would agree that.

01:13:24.760 --> 01:13:32.760
Yeah, that's one of the scenarios. I guess there are different like bad or like nuanced scenarios that could happen.

01:13:32.760 --> 01:13:44.760
One is like authoritarian good somehow where you have like one AI governing and then we do what we need to do to have a safe situation.

01:13:45.760 --> 01:13:53.760
The bad situation is when the AI does whatever it wants and doesn't really care about humans, like a rogue AI self-improving.

01:13:54.760 --> 01:14:00.760
And then there's another situation where humans are kind of free to do whatever they want.

01:14:01.760 --> 01:14:08.760
And then things can be good or bad, but it will depend on like kind of how the market behaves.

01:14:09.760 --> 01:14:15.760
But yeah, a lot of people have been talking about kind of the AI is a dictator scenario.

01:14:16.760 --> 01:14:23.760
Which scenario do you find more likely or more, you know, you're more like optimistic about?

01:14:23.760 --> 01:14:33.760
Like imagine you're in this like post-AGI world or human level language model world.

01:14:34.760 --> 01:14:40.760
What do you kind of imagine the world will look like? Would humans still live like 80 years?

01:14:41.760 --> 01:14:46.760
Would there be like a lot of crime? Would that be solved with like some kind of dictator AI?

01:14:47.760 --> 01:14:53.760
I think we're going to try to prevent dictator AI from happening.

01:14:57.760 --> 01:15:01.760
We probably just try to maintain the current democracy.

01:15:02.760 --> 01:15:07.760
Yeah, because, you know, most people fare dear AI anyway.

01:15:08.760 --> 01:15:12.760
I think we're going to do our best to prevent that, whether we like it or not.

01:15:13.760 --> 01:15:22.760
So hopefully we can like, we can do the thing we propose.

01:15:23.760 --> 01:15:25.760
I propose like the, I suggest it like this.

01:15:26.760 --> 01:15:29.760
We can improve our intellectual capacity.

01:15:30.760 --> 01:15:35.760
Then we can just continue our democratic process.

01:15:36.760 --> 01:15:38.760
Yeah, that's my hope.

01:15:38.760 --> 01:15:48.760
Yeah, I think that's a great note of positivity to end the podcast on.

01:15:49.760 --> 01:15:55.760
So then people can decide for themselves what future they think could be a good one for them.

01:15:56.760 --> 01:16:04.760
Yeah, it was a pleasure to have you on the show and hearing about your research on scaling

01:16:04.760 --> 01:16:09.760
and your thoughts on the alignment, even though you're not an expert in this.

01:16:10.760 --> 01:16:16.760
Do you want to quickly give your Twitter for people to follow one of the AKs?

01:16:17.760 --> 01:16:24.760
Yeah, so I guess you can just post my Twitter username somewhere.

01:16:25.760 --> 01:16:34.760
Yeah, I'm tweeting about the latest machine learning papers almost every day with some of these.

01:16:35.760 --> 01:16:36.760
So please follow me on Twitter.

01:16:37.760 --> 01:16:41.760
Thanks for listening to this podcast.

01:16:42.760 --> 01:16:46.760
I think your Twitter ID is something like aran, kuma, tsuzaki, or is it different?

01:16:47.760 --> 01:16:48.760
Oh yeah, it is.

01:16:49.760 --> 01:16:52.760
Cool, yeah, I'll probably post it in the bio somewhere.

01:16:52.760 --> 01:16:59.760
Thanks for coming on the show and I hope you keep on doing some cool research for other AI

01:17:00.760 --> 01:17:03.760
and hopefully not accelerating too much the AI timelines.

01:17:04.760 --> 01:17:07.760
Thanks, Aran, and see you maybe in the next episode.

