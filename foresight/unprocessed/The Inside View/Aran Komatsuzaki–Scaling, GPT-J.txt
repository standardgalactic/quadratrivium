Aaron, you're an ML PhD student at Georgia Tech, a research intent at Google, and a researcher
at OPAI.
At Eleuther, you've been working on a DAWI reproduction, which later became Lyon, and
you proposed GPTJ, a JAX-based GPT3-like model whose performance is on par with GPT3 6 billion
on various downstreaming tasks.
You're also well-known for being one of the two AKs where the legend says that if a deep
learning paper is important, one of the two AKs will have tweeted about it.
Your name was mentioned before on the podcast as one of the persons who have convinced Ethan
Caballero that scaling will turn out to be important.
Thanks Aaron for coming on the show.
Thanks for having me today.
So let's start with the legend of the two AKs.
Who is this other AK and why do people say that following the two of you is enough to
understand deep learning research?
Yeah, basically we scan all the archive papers, archive machine learning papers every day,
and we tweet about them with some summary and visualization.
And yeah, we've been doing it for several years and we became, we got more and more followers.
And that's probably why everyone follows us to get the grasp of the latest research.
How do you manage to read so many papers, writing all of the summaries on Twitter?
What's your process on a daily basis?
So I basically check archive CSP at around 6 p.m. every day and I scan all the titles
and if the title interests me, then I read the abstract.
Then if I'm still interested, then I would scan their tables and figures.
And if I think the paper is promising, then I would tweet the paper with short summary
and visualization.
He tweets much, many more than I do because he tries to tweet as many good papers as possible.
Well, I try to tweet the ones I think the best.
So yeah, he spends much more time on it than I do.
And yeah, I think that's amazing.
Yeah, so how much time do you spend every day, would you say?
About up to 30 minutes.
I wouldn't spend more than that.
You know, I'm a researcher.
So whether I tweeted about it or not, I have to scan all the archive papers anyway.
So I think this time investment is totally worth it.
Okay.
So yeah, how many abstracts do you end up reading every day, would you say like 10, 50, 100?
Yeah, it depends on the day.
Sometimes it's very, the field is very productive.
Like yesterday, I read like 10 abstracts, but I usually read only like three abstracts.
Yeah, I have probably because I have very specific tastes for papers.
I'm more biased towards NLP papers, for example.
Yeah, to go back to the reason why you're here today, you were tweeting as every day
and one of the tweets wrote was an answer to my AGI political compass, the latest one.
And you were saying that you were not on the compass because you were higher than some
ultimate or something more extreme than his position.
I know this is a joke and I'm sorry to bring something from Twitter on a podcast, but maybe
could you summarize your take for, you know, the listeners that are not on Twitter every
day?
Oh yeah, basically what I say is something like, so he said, is that I'm not worried
about AI alignment problems.
And I think I say, I don't consider this task of alignment to be any different from any
other half ML task.
But as we talked about it, I think I am wrong about this.
Yeah, so we can talk about it later.
And I think I also say we should focus on that long term, which we agree, I guess, and
scaling is important.
But unlike Ethan, I don't think it's all you need.
And I don't know about AGI, but I think superhuman level, recursively self-improving
language model, maybe possible somewhere around 2028 to 2038.
Yeah.
And I think there's a lot of disagreement on Twitter between focusing on the long term
or focusing on the near term.
And I think there's a bunch of confusion because for people who have short AI timelines, their
long term is kind of the near term.
So for you like 2028, 2038, it's kind of the long term future, right?
Because things are going to be a lot of different then.
So and yeah, people are also confused about definitions.
So AGI, artificial intelligence, or human level, or superpower intelligence.
So maybe just to ground the discussion a bit, could you give your definition of AGI?
So what's the definition you think about when you see that word on Twitter?
Yeah.
So I'm really interested in AGI, but to be honest, I'm not an expert on it.
So I'm just, I'm trying to learn about it.
So I don't know if my terminology or definition is correct.
But to me, AGI is something like a model that can recursively improve itself and can perform
any task, at least as well as humans do.
Personally, I'm, oh yeah, go ahead, sir.
I was just going to say that, you know, so if we think about like human level, not all
humans are capable of writing ML code or thinking about AI.
If you just take someone, some average human on the street, they will not be able to improve
an AI or self-improve.
So yeah, I think one of the definitions, sorry.
Yeah, one of the reasons people give for why an AI would be able to self-improve if it was
human level is that a human given like enough time and memory could be able to like read
all those archive papers and, you know, come up with another solution, assuming like our,
you know, like there's not like a problem in the architecture of humans that, you know,
would make it impossible to like improve or something.
So yeah, I guess that's kind of one of the reasons people might like not make a distinction
between those two, like human level and, you know, recursively self-improvement.
But yeah, I think that's like a reasonable guess to like put those very close.
Yeah.
So when I say human level, just like many other people, I think I'm saying that as well
doing each task as well as top human experts, so not like other humans.
You know, other humans, like you said, you cannot really write machine learning papers.
I don't remember whether you said that or yeah, but that's what I mean.
So yeah, that's basically what I meant.
So in particular, I'm interested in AGI's capability to do research because that's basically doing
doing research is basically or doing ML research is basically recursive self-improvement.
And also, you know, it can advance other areas of science or yeah.
So I think so as long as a machine learning model can do machine learning research as well as humans do,
I think that's that leads to AGI eventually without any human intervention.
So a human level machine learning research is AGI complete in some sense.
And I'm trying to make language model to machine learning research.
Yeah, I think that's a valid path to AGI, even though there are many other path.
Yeah, just to clarify the definitions.
When people say AGI complete, they usually mean you need AGI to reach that point.
What you're saying is doing ML research enables human level AI, right, or AGI.
So you're more saying like it implies, right?
It's being good at ML research and being able to do as much ML research as you want implies AGI.
Is that what you're saying?
Yeah.
Okay.
And I think so I agree with some version of that, but then it depends on what you consider to be ML research.
So for instance, is building like ML hardware and transistors and building factories that produce electricity that would power those ML hardware.
Is that all ML research?
Why do you draw the line?
Yeah.
For a human level machine learning research, I mean, to be able to replace pretty much every ML research staff, ML research project.
So including software.
Yeah.
That's what I meant.
Okay.
Yeah, I guess my take is that, you know, the economy is very complex.
And you need like energy to do things and robots to build things.
And so the kind of the task that would, so more like the intelligence level to be able to reproduce like all that branch of the economy that builds computers is kind of automating like 10% of what humans are able to do right now.
If you really wanted to like self-improve and build bigger and bigger computers without like having to call a human to do the task for you, then you would need to be pretty close to AGI already.
Yeah, that's true.
Given what we said about recursive self-improvement requiring a hardware and robots building the hardware, would you say like recursive self-improvement will arrive much later than, you know, ML research in archive papers?
You mean being able to write ML papers.
Right, or yeah, being able to write the papers and maybe like a run experiments by yourself without like actually like improving your hardware.
Yeah, I agree.
I think software improvement comes much earlier than hardware improvement.
But I think so, given if you look at the past ML research and scaling, most of the scaling comes from additional funding.
But and some of the scaling comes from hardware improvement.
But I think like a lot of improvement simply comes from software improvement, like improvement in design, like the transition from LSDM to transformer or like scaling, optimal scaling strategy.
So I think there's a lot of things AIs can do simply by improving the software.
And yeah, just to go back to what you said before about scaling is not all you need.
Yeah, why do you say you disagree with Ethan Caballero on that?
Obviously scaling is very important.
And if you look at the result of palm over, well, smaller palm or other models, then you can see that the improvement is huge.
And I think that's a very big indication that we still haven't exhausted all the space for improvement from coming from scaling.
But I don't say scaling is all you need because as I talked about, the improvement coming from performance improvement coming from model design improvement is huge.
The most important one is improvement in scaling strategy or called optimal scaling strategy.
For example, open AIs paper, that brings from 10 to 100 times of speed up.
So basically the model optimally scaled up performs as well as the model suboptimally scaled up using from 10 to 100 times of a compute.
This result was demonstrated, but technically this is possible.
So yeah, and also, you know, the improvement coming from VQVA, I mean, Dali 1 to Dali 2.
I think this huge leak is not possible simply by increasing the computational budget spent on Dali 1.
So this is probably because I think this is because there is a fundamental bottleneck with the design of VQVA or Dali 1.
So there are some of the things that you cannot simply overcome with additional by adding more copies.
Yeah, so what was the paper you mentioned where you go from a 10x improvement to a 100x?
Like you said, the open AIs paper?
Yeah, scaling goes for neural laggis models.
So that's like a way of scaling your models optimally.
And what you're saying is that you cannot just scale without thinking about the optimal scaling.
Is that what you're basically saying?
Yeah, basically not using this kind of principle way of scaling is a common practice before this paper.
But after this, basically most many of the big projects tried to follow this scaling now,
which I think is very important for saving the copies or maximizing the performance.
And yeah, one of the things that we mentioned in the episode with Ethan was that you kind of influence him to be more interested in scaling
because at the beginning it was not that much into scaling,
but then maybe like a few years ago you told him that scaling was going to be huge.
So yeah, how did you get interested in scaling?
Yeah, what's here or what was the kind of thing that got you into it?
Yeah, so I first got into machine.
So I started reading machine learning papers on the summer of 2017.
Then Transformer paper was released and soon got into language modeling because I thought that would be the key for AGI.
Then yeah, I was almost immediately and talked about several months to be convinced of Transformer language model,
replacing all the LSDMs in every applications.
Then I read a paper titled Exploring the Limits of Language Modeling, which was released in 2016.
So this paper basically tries to scale up the size of LSDM and dataset size to improve that publicity.
And I did their generated text and it doesn't look so much better than any of the text I saw and their publicity is so much better.
So it's kind of like a GBT2 moment for me except it was like 2017.
So that's basically the first scaling thing I saw and there was also another paper titled Deep Learning Scaling is Predictable and Predictable.
I think Ethan mentioned that it was released in 2017 too.
It shows that there's a parallel between training cards, performance, model size and dataset size.
But that doesn't really tell you exactly how to scale up the models.
Then finally there's GPT2, which was in January of 2019.
By then I was already convinced that scaling is going to be the key.
Yeah, if you were convinced of scaling before Transformer or in 2017, then seeing GPT2 in beginning of 2019 must be enough or not a big update from 2017.
Or at least they showed that you could get generation of paragraphs and a bunch of benchmarks in NLP with just scaling.
So yeah, did it surprise you a little bit or were you not surprised at all from GPT2?
No, I was not surprised with GPT3 at all because I was kind of working on a similar project with small scale.
So like before GPT2, we language model researchers were working on better sampling techniques.
So GPT2 used, I think, top-K sampling and temperature.
So basically these two were rediscovered in 2018 and I was just playing with these new sampling methods.
And so the generated text was much better than anything I saw before.
So yeah, I was not surprised with the result at all, but I was very happy.
Do you want to tell us more about the project you were working on?
Oh, you mean the project I was working on in 2019?
Yeah, so you said it was a project that involved top-K sampling or other methods that were not the same as GPT2?
Oh yeah, it was not like a big project.
So I was just trying to use top-K sampling and temperature sampling as a small transformer language model I was trying in 2018.
Then the result was so much better.
So I guess that's all I can tell, but there's a project I did in 2019 about scaling.
Yeah, it's called one equal piece all you need.
So, okay, can I talk about this project?
Yeah, sure, go ahead.
So basically, okay, so nowadays we're trying a huge model on huge data set for one or a few epochs,
but back then we were training smaller models for many, many iterations with very, very small data set.
Even GPT2, GPT2 used 100 epochs, I think.
So bad.
Yeah, exactly.
The data set wasn't that big, like only 40 gigabytes.
Well, it was huge back then.
Yeah, so the model size was only one billion, even though AI can totally spend more money on that.
Okay, so basically this shift from old days to today, it was this open AI scaling paper,
and scaling laws for neural language model.
And we have many other papers like Tintera.
But actually, I wrote, I did a project in 2019 where I sort of formed all these nice scaling ideas by myself.
So, and I wrote a paper called one equal piece all you need.
Yeah, so, okay, let me talk about that.
Then, so the, so there I had a bunch of ideas and tried to verify these ideas with experiments using very small amount of computers.
And so first idea is that it is easy to analyze the pre-training data set so that one has to train only one or a few epochs.
Which dramatically improves the performance compute trade-off.
So, basically, yeah, I'm just advocating, let's just try for one epoch and use big data set.
Yeah, and the second idea is that so let's compute the optimal ratio of model size and number of tokens for given compute budget based on training curves.
So, back then, the models were too small and they used too many iterations.
So let's just, you know, adjust this ratio nicely so that we don't have to waste all the compute for like hundreds of epochs.
So is the ratio model size and data set size?
Yeah, that's right.
So, actually, Ginger also did measure this ratio.
So basically, they computed the scaling exponents for optimal data set size versus optimal model size.
And then they found that the scaling exponents for this to both 0.5.
So that means they both linearly increase.
So you can just measure the slope of this line, which gives you that optimal ratio.
Yeah, so basically, you would need to scale your data set size and model size the same amount.
So if you want to, you know, build GPT-4, you might just want to double the number of parameters and double the data set size.
Yeah, exactly.
And so, yeah, right now, so now we're not on 2019-2020 anymore, we're in 2022.
And I believe so you're working at Iluth AI and Google as an intern on some scaling work.
And I believe you might not want to talk about your private work on the podcast.
But yeah, what kind of work do you do publicly on scaling you won't be happy to talk about?
Oh, yeah, definitely.
So, okay, let me think.
So I'm currently very interested in Instruct GPT and T0.
So both of these.
Could you maybe summarize what's T0 for people who were not read the paper?
Yeah.
So T0 is basically a masked language model with multi-task point joining.
So first of all, masked language model is an easily encoder on the encoder-decoder model that is trying with a masked language model objective.
So this training is basically, so let's say you have some text, then you can randomly mask.
As some random spans of tokens.
So they, then you want your model to predict these masked tokens.
Yeah.
That's basically a masked language model or also called denoised audio encoding.
Yeah.
So that's what masked language model is about.
And T0 is a masked language model.
That is also fine tune from a bunch of many, many data sets, like maybe like Super Glue.
So what's Super Glue?
Super Glue is a standard, natural language understanding data set.
Yeah.
Maybe it has not trained on, maybe they did not use Super Glue for T0.
But basically, that's the idea.
So the reason why we want to fine tune T0 on a bunch of these data sets is that if you train like this, then it generalises,
obviously it performs very well on the task it was fine tuned on.
But it also performs very well on the tasks it was not fine tuned on.
So we know that GPT like models performs much worse than the GPT like model that is fine tuned on the task you are trying to deal with.
Right.
So yeah, basically this multi-task fine tuning allows your model to perform very well on the task, not only the tasks it was trained on,
but also the tasks it was not trained on.
So T0 actually can perform much better than GPT3 on many tasks.
Without fine tuning the model on this task specifically, while using like 10 times less complex than GPT3.
So is the idea that you train it on a bunch of different tasks, so not fine tuning, but you train it on a bunch of different tasks?
I think that's what you said, multi-task training or multi-task fine tuning?
Yeah, multi-task.
And then it's able to generalize well on held out tasks, it doesn't seem before like zero shots?
Exactly.
So you said you were interested in T0 and also in Struggle GPT, I believe in Struggle GPT was a model or a training procedure from OpenAI.
Can you tell us more about in Struggle GPT?
Yeah.
So in Struggle GPT is also functioned on many different tasks like T0, but it also uses human feedback.
So like basically they train the model to score the bunch of the generated results.
And this model tells this GPT3 to how to generate the text.
So this process is done by using reinforcement learning called PPO.
So this additional component improves GPT3 significant.
Yeah, so basically even though GPT3 doesn't perform as well as PPO, given the amount of compute it consumes, it performs very, very well on many different tasks.
So without having to, yeah, I think it performs very well even without using future samples.
So yeah, GPT3, sorry, Instruct GPT and T0 are some of the most compute efficient models out there.
So yeah, I'm very interested in these models.
And my project is basically trying to combine all these with scaling.
Right, so you kind of want to combine this URL from human feedback procedure from Instruct GPT with the pre-training from T0.
Yeah, and I'm thinking that we can do some interesting scaling analysis on this model for several reasons.
First of all, optimal scaling we do for GPT-like models.
We usually try to optimize the test application, and this model only has only decoded, unlike T0, which has encoded and decoded.
By the way, encoded decoded model performs much better than decoded on the model when it is fine-tuned or multi-task fine-tuned.
And that's why I'm thinking of this encoded decoded model.
And for people who are not into deep learning or NLP, so can you just give an example of decoder and encoder decoder?
So I think GPT2 is an decoder because it gets a prompt and then just generates a paragraph.
What's an encoder and decoder?
Yeah, so encoder decoder is basically encoder is like the model architecture used for Word, or like the first transformer paper architecture, and decoder is our usual decoder.
So basically you want to feed your prompt into encoder, and then you would feed the output to the decoder with self-attention.
That's what encoder decoder is, and it happens to perform very well in this situation.
Yeah, so basically we have encoder decoder and fine-tuning.
Yeah, so I think these elements make the scaling load very different from the local GPT models.
Oh yeah, and also we want to optimize for downstreaming performance instead of test public HD.
So I think these conditions force the model to be bigger and train shorter on free-tuning tasks because fine-tuning is so important that maybe we can make the model bigger while focusing less on the free-tuning.
And I think this is, so the current state-of-the-art model has like 100 billion amateurs and trains on trillions of tokens, which is very different from how human planes run.
Because our plane has like hundreds of billions of neurons, meaning like hundreds of trillions of synapses.
So yeah, I think that means it has more capacity than the former capacity than models do, and it is only trying on like a few billions of tokens because that's how many tokens we can process within our lifetime.
So yeah, basically I'm trying to make the models closer to how human planes learn.
So you're trying to write a scaling law that would be closer to the amount of data humans process throughout their lifetime?
Yeah, and I'm not like trying to make this forcibly similar to human planes constraints.
I'm just thinking that this new model based on this G0 instructivity, I think it will perform the best if we try and like how human planes learn.
I'm not sure I understand the methods to have this scaling law, so would you need to, you know, train T0 with like some kind of instructivity fine-tuning,
and then you test on a bunch of held out tasks, and then you would see like you would plot a curve of like optimal scaling with respect to those like done streaming tasks?
So basically what I'm saying is to train a bunch of different T0 with instructivity fine-tuning with different number of tokens pre-trying and different models.
I think I understood now, so you're trying to, the same as Ginchilla, like get the exponents for dataset size and for model size, and then you're trying to see if it's not like 0.5 and 0.5, but like maybe something else?
Yeah, something like that.
I think your most well-known for your scaling work at Eleuther AI, where you train, was it 6 billion parameters, or at least like you try to reproduce the results from GP3, 6.7 billion, and what was called GPTJ, I think for GPTJacks.
Yeah, can you just like give us a rough summary of this project, why you started this project, and what do you want to release it to the public?
Yeah, so around the beginning of 2020, I was trying to reproduce Dali 1 with some of the people in Eleuther AI.
So basically, Dali 1 consists of BQBAE encoder decoder and transformer language model for generating the discrete 11 variables.
So that way, this transformer language model is exactly, almost exactly the same as GPT3 except for the size.
So I thought we may be able to make the maximum impact if we reuse this model for our GPT3 production.
Yeah, and then at the same time, I thought, so Jaxx was becoming more and more popular.
Obviously, Jaxx is optimized for TB use, and we had a lot of TB use back then, and Jaxx is, so before that, we had our GPT3 replication, which is called GPT Neo.
It was implemented using mesh tensorflow, and this mesh tensorflow decoding speed is so slow, almost ridiculously slow, especially compared with Hytotes.
But Jaxx has no problem with that, it's almost as fast as Hytotes.
So we decided to use Jaxx for this project.
Yeah, so basically, I was supposed to work on the encoder decoder code, and I asked another guy in Eleuther AI, his name is Ben1, and I asked him to work on this language model site.
But admittedly, he spent far more time on this project than I did.
How much time would you say took you, maybe like six months between the beginning of 2021 and when you guys released the model?
I think it only took several months, so this project is kind of impressive because there are only two people in it, and we only spent like three months, and we basically open sourced the best language model.
So that is something I'm proud of.
You're right to be proud of this, it's pretty cool.
Yeah, thank you so much.
So, by the way, GPTJ is different.
Yeah, so basically GPT Neo actually could not really match the same performance with the GPT3 with similar size.
GPT Neo was like 2.7 billion model, but it performed worse than 1 billion GPT3, for example.
But GPTJ performs pretty much as well as 6 billion GPT3, so yeah, the model performs very well, I think.
So it was able to be more efficient than GPT Neo, which I believe is another model from Eleuther AI, but using TensorFlow Mesh, so a bit older, right?
Yeah.
And yeah, so to match that performance from GPT3, did you just like took the same API parameters and architecture from GPT3 paper, or did you like had to change stuff to, you know, match the performance?
Yeah, so I think we could be using the exact same architecture with GPT3, but we just wanted to do a bit more.
So first of all, yeah, one thing we tried is leisure with the depth ratio.
So basically with, I'm referring to the hidden dimensions of the model.
So we are trying to build a wider model rather than deeper model.
And this is important because generally speaking, wider models can utilize accelerators more efficiently, and latency is much better.
So yeah, and when we tried this wider model, we observed that we don't really lose much of performance from this.
So I think with this is worth it.
And another thing we tried is placing feedforward layer with attention layer in parallel.
Yeah.
Basically, this is, this also saves latency, and you can also make your accelerators utilize better.
And this was actually also adapted by Paul.
So I think this is something, I think this is a nice contribution from this project.
So you think palm researchers read GPTJ and thought like, oh yeah, this architecture change is very good. We're going to use it.
Yeah, exactly. I think they also sort of followed our wide model because their model is also very, very wide.
I think it's even wider than ours.
I read the blog post wrote about GPTJ. And so I kind of read about like all those tricks you did.
And you talk a lot about throughput.
So yeah, I'm curious like what's the throughput for people who are not like scaling models all the time.
And yeah, how does compare, you know, how does GPTJ come compared to like GPT3 or GPTNEO in terms of throughput?
Is it more efficient, less efficient, more throughput, less throughput?
Yeah, so throughput, I mean the number of tokens processed parameter per second.
So if you can prove this throughput, then we can try with a larger model or more tokens with the same amount of throughput.
So this way you can improve performance.
So it's per, so it's amount of token processed per parameter and per second?
Yeah.
If you have more, a lot of parameters, you will, I don't know, does the model size then change your throughput then?
Yeah, I guess you're only comparing like size models of the same size of GPT3 and GPTJ, so it's fine.
So that typically throughput is defined something like two lobs per second.
Oh yeah, yeah, so yeah, maybe a bit confusing, but let's say you have a one billion model, require, and you have TPU, one core.
If it spends one second per, let's say it spends one second per one token.
Then if you have two billion model and one core TPU, then it will take 0.5 seconds because they have the same throughput and something like that.
Yeah, so the throughput of the GPTJ six billion model for training is like 150 tokens per second.
On the other hand, GPTNEO with 2.7 billion parameters is also 150 tokens per second, but this model is half the size of GPTJ.
So basically, this means that we achieved twice improvement in efficiency for throughput.
So yeah, I think this different improvement is huge.
I think it's coming from this wider model using JAX instead of mesh TensorFlow.
Yeah, and not to mention that GPTJ has much better downstream performance than GPTNEO.
Yeah, how long did it take to complete like an entire training because I know it requires a lot of computes.
Yeah, so we spent five weeks using 256 cores of TPU V3.
Yeah, what do you do during those five weeks? Do you just look at the TensorFlow curve and the next answer boards and check it doesn't have any weird spikes?
Yeah, basically, that's what we did. There was no back because we already solved all the bugs.
And so basically, Ben, babysit this training for five weeks, he was complaining a bit, but he said it was not too bad.
And then at the end, you published this model on GitHub, people are very excited and start to use it to fine tune to a bunch of different cases, right?
Yeah, so we don't really fine tune ourselves, but many people try to fine tune.
It appears that GPTJ is more easier to deal with than other models like GPTNEO.
Maybe because the things we use like JAX is much easier to deal with than mesh TensorFlow.
So yeah, it became very popular.
Yeah, from reading the documentation in GitHub, there's a manual on how to fine tune it.
And it seems like the overall code is easier to read as well.
And even people on YouTube like Janine Kilsner use it for their projects like fine tuning on 4chan to generate more comments.
Have you seen this recent YouTube video and if so, what do you think about it?
Yeah, I actually didn't watch the video itself, but I saw the tweet and some people talking about it.
So I know it a little bit.
So my reaction to this entire controversy is kind of like that meme of like entering into a burning room with pizza.
So I think his project is kind of cringy for being too attention seeking and obviously not really ethically sound,
because you can just use this app for many bad things and agree with some of the critical reactions to his project,
even though some of them may be a bit too exaggerated.
But at the same time, I thought this case would lead to more attention spent on a language model that can detect outputs from other language models
so that you can filter out language model generated submissions on many websites like Reddit.
Yeah, like lotion or Chinese government can use this sort of process to influence social media and election results in the West.
But language models are very good at discriminating language model outputs from human outputs.
So I'm kind of optimistic about that, at least for short term.
What do you think of the fact that publishing GPTJ might have accelerated AI timelines where we might have less time to make AI safe
and align those models with human values?
Yeah, do you think releasing GPTJ wasn't that good for humanity?
Would you have done it before again if you had to?
Yeah, I think releasing GPTJ was a small net positive benefit for humanity.
And so basically, open sourcing language model, there's nobody who releases a language model that is substantially better than the previous state of the art.
Like in my case, in our case, our model performs only slightly better than GPTJ Neo or T5.
So I don't think there was actually like accelerating the timeline or anything.
Yeah, so basically, I don't think there's any one particular person who can make big negative impact by releasing a big language model with the current trend.
If there's anyone then who can make a big negative impact, it'll be someone who like fine tune the model to spread misinformation.
Yeah, just to be clear, the GPT Neo was also from A3AI.
So in some sense, if you remove GPT Neo and GPTJ, then you don't really have any open source implementation of the GPT3 that works on the internet.
And so maybe like you wouldn't have like all those other companies like Microsoft or Chinese or Korean companies using this implementation or even like the pile of the public data sets to train their models.
So in some sense, we're kind of helping the entire research community go faster on those topics.
And yeah, so maybe there are like other open source projects that would emerge.
But then the question is like how much, you know, releasing GPTJ in 2021 accelerates those timelines compared to the others.
And I would say like OpenAI releasing the neural scaling laws or GPT3 or even GPT2 kind of showed that like scaling was important to get to AGI.
So in some sense, they kind of released some secret cells and everyone started following them.
So, you know, if they didn't publish or publish a bit later, maybe the timelines would be a bit longer.
What do you make of that?
Yeah, so actually there was T5, open source T5.
That's not the same as GPT3, right?
That's right, but you can fine tune the model like GPT410.
So yeah, I think it was totally possible to do it.
Before GPT410, I think there was GPT2 reproduction from several people, several different groups.
Also, like Facebook and Google released some decoder on the model that performs well.
I don't think it affected on research because researchers, I think all these big companies like Google, they already had much bigger language models internally.
And people in academia, they cannot really affect.
And I don't think they had those big language models before, but I don't think they can really contribute to this large language model research because they don't have budget.
So I don't think my, our projects really accelerated the research timeline.
But I think, yeah, maybe slightly accelerated the open sourcing language model timeline.
Yeah, maybe you accelerate the open source timeline, but not the private research timeline.
So in some sense, you bring everyone on the same level.
So yeah, definitely our work is slightly accelerating the pace of open sourcing language models.
For example, Facebook recently released 100 full size GPT3 model.
Maybe that was, and yeah, maybe that was a response to our GPTJ or GPT NeoX model, which was released recently.
It has about 20 billion model parameters.
And I think this question of accelerating open source timelines, or at least AI research in general, is important in the context of differential progress.
So not only accelerating AI progress, but how does the speed of AI relate to the speed of AI alignment research?
And I'm not sure if you're very familiar with AI alignment, but in this podcast, we talk about this a lot.
And it might be worthwhile maybe defining alignment or at least like going with another definition you're familiar with.
So yeah, what do you understand of the concept of alignment?
How would you define it if you had to?
Yeah, well, as you know, at the meeting, I'm a beginner on alignment.
So all I can say, if I understand correctly, is alignment research is the research to harness advanced AI to do what we want to do.
And one big problem is considered is the existential risk due to advanced AI.
Yeah, that's pretty much it.
And I think the question is, if we have an AI that is much more than humans and doesn't really care about our values, it might end up optimizing for an objective and just change completely our planet without really caring about humans or stuff.
So we value and I guess this problem might be considered harder or easier depending on how you define the problem or how much time you think humans will have to think about those problems.
And one of those takes is that if you only consider alignment as like an AI problem, if we want to solve AI generally and have models that are able to generalize well, if we program them well, they will be able to like solve alignment.
And if we build them, if we build like good models, they will be aligned by default.
And I guess that's maybe like one of the, is that maybe one of your takes as well or something you think will happen?
Yeah, so for short term, I think we can more or less try to be careful with training like what I'm dropping for like open AI is doing, but in long term, which is what Ethan was referring to like when AI is trying to deceive humans.
I think that's when we can no longer like use the conventional machine learning approach to deal with.
Because, you know, if the AI is much more intelligent than humans, then we have no way to detect whether the model is deceiving or not.
Yeah, so that's something I would be worried about.
I guess the question is, when you have a benchmark, how do you know if the model is not pretending to be good at the benchmark?
Or is it like generally, generally good at the benchmark?
So if your benchmark is truthful Q&A, is it actually truthful or is just like pretending to be truthful for the moment?
And one of the things that Ethan was saying is that all alignment can be considered as inverse scaling problems.
So if you make your model too big at some point, it will, you know, have bad properties.
So if you're interested in scaling, you can see that as a scaling problem. So like a good behavior for scaling.
Yeah, I agree.
So in terms of benchmarks, you said that for models that can be deceptive, it can be hard to write down a good benchmark because they might be able to bypass it.
Do you think we might reach a point where we'll be able to have good evaluations and good ways to understand if our models behave correctly or not?
Yeah, this is going to be at some point. There's going to be no conventional machine learning benchmark that can detect AI that will be malicious or not.
I think more generally, like benchmarks are quite tricky, especially for language models and NLP where if you just like change the beginning of a word, like if you make the first letter capitalized or not in your benchmark,
or if you just like change the code base from one GitHub repo to another, you might get completely different results in your benchmark.
So do you think it's possible to have good NLP benchmarks to evaluate language models in the future and especially for alignment?
Yeah, obviously we need better benchmarks for NLP models, even outside of the alignment problem, even for short term.
Sometimes we use automatic methods to evaluate the quality of text, but they tend to be not very well-coordinated with human judgment.
So I'm advocating for the use of human judgment, but even human judgment sometimes is not very useful because humans do not really understand whether the generated text is factual or not,
because most of us is not really an expert on the field that the model is talking about.
So yeah, and for alignment, I think for short term we can probably manage to make up some next benchmark using human evaluations.
And if it doesn't work, then we will probably use some trained model to evaluate, because we can't understand what, like my example, humans are not good at judging the quality of models anyway.
But at some point, I think that even training models to evaluate model is not enough, because at that point the models are so much better than humans are.
And we don't understand the model and we cannot detect whether the model is being malicious or not.
So in that case, yeah, there's going to be a big problem, I think.
Yeah, I think you're right that it's hard to evaluate fully a model if you're not an expert on the domain.
And I think one of the things that we can do is what they did in struggle GPT where they had like a bunch of laborers and people giving evaluations for how much you prefer one output compared to the others.
And then you can roughly build a reward model that can say if an output is good or not, because you have like all those comparisons between different outputs.
And yeah, for alignment where you're basically saying that you would want like an AI or another language model to evaluate if the other one is answering correctly or not.
And at that point, you're kind of doing what people say is bringing a small Godzilla to check if the bigger Godzilla is doing good or not.
And so that was like from a blog post that was published recently.
And the problem with that is that at the moment when you're like bringing Godzilla to manage the other bigger one, there's already a problem because now you have two Godzilla's in your city.
So that's why I think it's a tricky problem to try to align or at least check for deception in those large models is because if you want to like have another model do it, then you need to check this smaller model.
And yeah, so yeah, what do you make of like AI's aligning AI's? Do you think us humans will be able to like, you know, build this like smaller model, it's providing the other big ones?
Yeah, I think that that is what's going to happen for a short time. Obviously, we cannot keep doing that for long term, because in the long run, AI is going to be so much smarter than we are.
So we can no longer make sure whether this small model is doing what we are thinking it's doing.
Yeah, so like at the point, we need an entirely different intervention. I'm talking in really long term, so it's probably well after AGI. And at the point, my guess is that something like we have to argument our own intellectual capacity with something new or something.
So we become the AI itself. Have you had of something like that?
So something like merging with AI's after maybe like some kind of rail curse rail scenario where you upload yourself or maybe like connect with brain competent interfaces to AI. I think that's more like the Elon Musk neural link scenario.
Yeah, which scenario you're talking about.
I'm not really familiar with your new link. Maybe the last one.
Okay. So when you were saying like long term, you were saying like after AGI, so at the beginning of the podcast, you were saying 2028 and 2038 were like lower bound and upper bound for AGI or at least like the rough guess for when it might appear. Is that still correct?
Yeah. So by that time, I meant like human level language model rather than AGI.
Human level language model. And yeah, how many years to go from human level language models to AGI?
I think it's going to take very little amount of time given that this human level language model can do machine learning research well so it can improve itself very rapidly.
Okay. So when you reach human level language models, they need to do research and then they achieve AGI. Is that your main timeline?
Yeah.
And so this might happen in days or weeks?
Maybe several months.
Several months, okay.
Yeah. What would be like the bottleneck at that moment? Like they just like keep improving and but then they need to like figure out how to convince humans to help them build more hardware or would they, I don't know, build the robots themselves?
Yeah. So how would it happen?
Hardware is going to be the bottleneck, I think.
Yeah. Also collaborating with humans, yeah, convincing. And that's going to be, yeah, the main bottleneck.
Okay. So if you're listening to this podcast in 2028 and you have language models convincing you to build more, you know, GPU centers and let them, you know, make post requests all over the web.
Please don't. It's a bad idea. They're trying to do more ML research. So you've heard it first here.
Yeah. So when we were talking about like connecting to neural networks and have our brains maybe like become AI or merge with AI.
We're talking about something closer to brain computer interfaces as well.
Yeah.
So then the question becomes like, well, kind of this timeline of the human slash human plus AI intelligence.
Like compared to like just pure AI intelligence. So will the humans be able to keep up a bit and, you know, understand what's going on or will they be left behind?
What do you think?
Yeah. So I think, yeah, we should control the pace of employment in a way so that we can catch up with the pure natural, pure AI.
So, yeah, we should absolutely make sure that we don't, we can keep the same pace, I think.
The problem is that we don't really control that as there are not that many brain computer interface researchers, there's not a lot of funding in there.
And the rate of progress in AI is accelerating a lot.
And if the problem is in, you know, neuroscience are much harder than, you know, scaling models, then even if we throw like a lot of money at it, it's going to be very hard to solve.
Yeah, that's true. But so my guess is that AGI is not going to be so, like, malicious for short land.
So I'm thinking of using AGI to improve this kind of research for the beginning.
At the beginning, what do you think?
So basically, you use a very smart language model, some kind of Oracle where you ask questions and gives like very good answers with like documentation and papers.
And you ask him like, hey, how do I build a good brain computer interface?
And it gives you good answers. And then you go on and you start building that.
The problem is that at the moment when you have this kind of model, you can also ask it like, hey, how do I build a better copilot?
How do I build a bigger language model?
And I guess people might want to do this first.
And you might also need to develop new robotics or actual hard tech microscope and neuroscience tools to do those.
And it's not going to be like only like a neural network architecture.
So I guess it's always like the hardware part is going to take longer than just like software or neural networks part.
Yeah, that's true. We may have to regulate the behavior of AGI so that we can keep the pace together.
How do you regulate that?
We need some physical access to AGI first, the AI, like physically preventing the models being scaled up too fast.
So you basically have an AI become the president and then implement some security measures to prevent people from building bigger AIs?
Yeah.
I'm not sure people would agree that.
Yeah, that's one of the scenarios. I guess there are different like bad or like nuanced scenarios that could happen.
One is like authoritarian good somehow where you have like one AI governing and then we do what we need to do to have a safe situation.
The bad situation is when the AI does whatever it wants and doesn't really care about humans, like a rogue AI self-improving.
And then there's another situation where humans are kind of free to do whatever they want.
And then things can be good or bad, but it will depend on like kind of how the market behaves.
But yeah, a lot of people have been talking about kind of the AI is a dictator scenario.
Which scenario do you find more likely or more, you know, you're more like optimistic about?
Like imagine you're in this like post-AGI world or human level language model world.
What do you kind of imagine the world will look like? Would humans still live like 80 years?
Would there be like a lot of crime? Would that be solved with like some kind of dictator AI?
I think we're going to try to prevent dictator AI from happening.
We probably just try to maintain the current democracy.
Yeah, because, you know, most people fare dear AI anyway.
I think we're going to do our best to prevent that, whether we like it or not.
So hopefully we can like, we can do the thing we propose.
I propose like the, I suggest it like this.
We can improve our intellectual capacity.
Then we can just continue our democratic process.
Yeah, that's my hope.
Yeah, I think that's a great note of positivity to end the podcast on.
So then people can decide for themselves what future they think could be a good one for them.
Yeah, it was a pleasure to have you on the show and hearing about your research on scaling
and your thoughts on the alignment, even though you're not an expert in this.
Do you want to quickly give your Twitter for people to follow one of the AKs?
Yeah, so I guess you can just post my Twitter username somewhere.
Yeah, I'm tweeting about the latest machine learning papers almost every day with some of these.
So please follow me on Twitter.
Thanks for listening to this podcast.
I think your Twitter ID is something like aran, kuma, tsuzaki, or is it different?
Oh yeah, it is.
Cool, yeah, I'll probably post it in the bio somewhere.
Thanks for coming on the show and I hope you keep on doing some cool research for other AI
and hopefully not accelerating too much the AI timelines.
Thanks, Aran, and see you maybe in the next episode.
