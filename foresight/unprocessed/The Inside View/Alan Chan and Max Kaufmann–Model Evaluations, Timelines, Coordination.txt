You got transported to 2014, Paul Cristiano is like, boom to Paul and Paul's just celebrating, he's like, man, we've done it, it's happened, alignment is soul, like, and you look out and you know, the light cone is smiling and joining us.
That's so wide.
Yeah, yeah, it's like, wow, what happened?
Also, this being is this, could I just kind of keep up to meme, and I'm like, instant you're happy.
Well, I don't know, what's, what's, what's the channel?
What am I doing?
Is this like, is this recorded for, could be, could be, okay, just, okay.
Half meme, half.
Off takes.
Half takes.
They have like, inside your second channel where you just, yeah, yeah, yeah.
About the outside view.
The outside.
I often find it, um, slightly confusing how NLMs translate to some system I cover in the future.
In some sense, it feels like, you know, you ask a large, oh, do you want to break out with a server and like take over?
And it goes, yes.
I'm not actually sure what this translates to.
For like a future system that's actually taking over breaking servers.
Like I could imagine these systems are maybe more like, maybe N10, like maybe like something like an action transformer,
or they're not directly trained in the language.
And it's unclear to me like how, what they say translate to what they do.
And in fact, I'd expect that to diverge pretty heavily, um, in, in cases I'd worry about.
Yeah.
I think we're in a really weird situation, right?
Because we have all these models and there's this sense that like they could really do things that like change things in the world, right?
Um, but nobody's really deployed these models into like a convincing enough context to showcase these capabilities.
Um, but to answer your question, I think, like, you know, things like, um, WebGBT or like with Coheur, with Act 1,
like once you give language models an API, I think the idea is that like they can leverage, um, the knowledge they've gained in pre-training to use this API to do dangerous things.
And you imagine that like, you know, something like ACT, I mean, ACT can't generate language, at least not the right.
Um, do you see like, what it says as actions are when asked to translate way out and generalize like what it's actions actually will be?
Like, let's say you have WebGBT, oh, would you break out the server file?
Do you think, whether it says yes or no, and like an email like this translates to, if I scale up the system, if I deploy it, it actually will.
Right.
And as I said, I feel like there might be some disconnect, what this like chatbot says, and then what happens when you use this things world model,
if I tune it on some actions, call it to run, let's say, let's say, interact with an API.
It feels like, do you imagine, do you imagine behavior there happens kind of independently of what it said in the chatbot setting?
Yeah, I agree.
So I don't think you literally go and ask it, would you break out of the server farm?
But you go in and measure like, literally just have humans, maybe at first, look at the actions it's taking and see, okay, you know, I am like an expert software engineer or an expert systems engineer,
like I'm looking at these actions, do these actions look as if like the model is actually trying to break out of the server farm?
So yeah, I suppose like that's a problem with evaluations, would you just ask the language model, like yes or no questions about what it would do and not actually like see what it would do?
So let's say hypothetically with the server, it's kind of an evil company.
Hypothetically?
Hypothetically, let's say there's some kind of evil company that's going on.
It's like, I think I'd be interested to see like, right now, what would your model of like, what would that email look like?
Is it more like, like you said, give it an API, see what it's doing?
Yeah, API, I think also identifying key contexts in which a dangerous behavior like takeover or some sort of power seeking would occur.
Yes, so the difficulty here I think is, I think we need to identify these contexts in order to actually be able to write concrete evals and to provide evidence that's convincing to the public that AI is going to do dangerous things.
Of course, whenever, yeah, I don't think like we're going to be able to cover all the possible ground.
There's of course, some unsurging extrapolating from evaluations that we developed, you know, okay, the AI doesn't think dangerous in this scenario doesn't do something dangerous in the scenario B.
What does it do to the scenario C? Sort of unclear.
Do you worry about, like at the point where an AI can exhibit dangerous behavior?
And it's already strategically aware that if it had, like let's say, you know, someone like Yikowski or like, let's say Evan is right.
My strategic awareness plus like long term planning often needs a deception that you will, your evals will be too late in some sense.
Like there's this like, you have to catch it while it's still competent enough to show dangerous behavior and not confidence have the strategic awareness that it should be deceptive.
Yeah, that's a serious problem, I think.
Yeah, so one, I think definitely people should be looking into the conditions under which deceptive alignment or something like that might arise.
Number two, this is just an argument for developing evals as fast as possible so that, you know, I'm imagining at every stage of the trading process, like ideally, you know, every like 10 steps or something.
Maybe even like every step. We literally just like eval the hell out of this thing, right?
We make sure not to train on this eval somehow.
You know, somehow we need to make sure evals don't ever make it into the training sets of language models.
Otherwise, you know, while they train on this, they already know about this.
It's going to be kind of useless.
So, yeah, how do you feel about this like optimization?
So, I think you previously mentioned you don't like the word benchmark and you used the word eval. Could you maybe expand on that?
Yeah, so I don't think I like the word benchmark because I think historically machine learning benchmark has been associated with this idea that, okay, we are like optimizing for this thing.
But this thing that we're developing this eval at the end of the day is just a proxy measure for what we care about, which is, you know, it might be specifically like courageability and honesty.
We're trying to make sure this AI doesn't like, you know, it isn't able to like manipulate a lot of people, right?
So it's like an imperfect measure.
I think when you optimize for imperfect measure, I mean, I think this is like, I already know in the ICH, like you don't tend to get models that actually do what you care about at the end of the day.
So I think like the framing of evals instead of benchmarks to me makes it clear in my head at least that we don't want to be optimizing for these things.
We're just using these as sort of like checks on our model, right?
But there's another difficulty with having like, I guess public evals and that there's some sort of like implicit optimization already going, right?
Like when researcher A like tries a technique on like some benchmark or eval and finds it doesn't work and publishes a favor, researcher B says, oh, you know, you tried this technique, it doesn't work.
I'm going to try another technique so that researchers are already going through an optimization process to make evals better.
And I think like this optimization process is like maybe like a little weaker than actually like fitting on the test set, for instance, which happens sometimes.
But it is still an optimization process.
Yeah, there's like some work actually that studies, you know, how effective is this like implicit optimization process?
Really? I don't quite remember the details right now.
But like, you know, the general consensus is that you actually do need to keep updating your evals and benchmarks because there's an optimization process.
I mean, you know, an image net, right?
Like we're already like so good at image net.
The next like 0.2%.
Is that really like a better model?
Or is that really just like overfitting to what?
Image net, you know, is, yeah, overfitting to like random stuff.
Yeah, I remember there was this paper, maybe a few years ago, where they re-gathered an image net type data set.
And they did find that at least back then, so it was out of distribution.
They did the same thing to gather the bunch of images online, like same.
And like there's been some distribution shift.
So models would perform worse on this new image net.
But the ranking between still kept.
At least that paper claimed that back then you hadn't yet overfitted.
Yeah, I think I find it hard to imagine publishing such an evaluation and not having the headline be my model passes all the evaluations, right?
And I guess if you're like, I guess, yeah, if you feel like, so if you make a model pass all the evals, do you see this as then a good thing?
So I think my perspective is a model passing evals shouldn't be taken as evidence of safety, but a model failing eval is evidence that, OK, let's slow things down, maybe not release it.
Let's talk to some people.
So failing evals is sufficient to show data, but not necessary.
Maybe not in a truth, like strictly truth sense, but in a practically speaking what we should do sense.
In the sense that like, I think I am fairly risk averse about like, you know, releasing the eyes into the world.
So how much of this eval plan rests?
How do you see the technical side making good evals?
And I guess the more like social governance side of like getting people to pay attention to these emails.
Like, how does either play look for you?
Where do you think more work is needed?
What does that trade off look like for you personally and what you want to work on?
Yeah.
Yeah, so I think this depends on timelines and where we're at.
So I think it also depends on things I'm not like totally aware of.
So one thing is, you know, if you actually went to the heads of Google brain and anthropic and like all the big tech companies, right?
And you actually told them, look, like we have this dangerous evaluation.
We're running it on your models.
Your models do terrible, terrible things.
If you show this to them, would they actually like refrain from releasing models?
I'm not sure what would be convincing to those people, right?
At another level, okay, even supposing that it's convincing to those people for how long is it going to be convincing?
It doesn't seem like a sustainable thing to rely on.
Like in some sense, the goodwill of these people who might have different aims than like, you know, people in AI safety regarding like not ending the world, right?
So I think that's the next level, like that's the level at which we try to bring in other people from the public like civil society, governments,
just in general, like public consensus, building around like the idea that like AIs can just empower us, right?
So yeah, I think like, you know, an eval is kind of useless if nobody pays attention to it or if people don't find it convincing.
In particular, the people who have the power to actually do things.
So part of the work, I think, is thinking about what kinds of deals would actually be convincing to people.
What do you think kind of equals at least current as Islam?
Yeah, I mean, I kind of have no idea.
Like, I mean, I have an idea of evals that would be convincing to me.
Like if I actually saw an AI like trying to copy its weights to another server, trying to like get a lot of money.
Yeah, like, like I buy that, right?
Like if I was in a deep mind, I'd be like, okay, you may be even without looking at these evals.
That's like, slow it down a little bit.
But you know, like, I think there's this like big gap between people in like AI safety or existential safety
and people in other communities that also care about the impact of AI and society.
Like people in fate, like fairness, accountability, transparency and ethics, people generally in like AI ethics.
Yeah, so, you know, there's already a big difficulty in just explaining like, you know, what is the danger here, right?
Like I think there's this big discourse outside of the AI safety community, the idea that like AI's are just tools, they aren't agents.
Like what's so hard, you know, the difficulty is misuse or in some sense, like this lack of coordination between everybody
and like making things go bad and like AI's that like exacerbate, you know, like bad things that we have today like oppression.
I think these are all true, right? But I think like, you know, the danger of AI's agents is this like separate category
that's been very, very difficult to explain to other people for some reason.
So like, yeah, like I'm not sure what kind of evals would be convinced to them.
Like maybe there needs to be much more consensus building, you know, on a philosophical basis of like, what are the things in common between
what the things that like people in X safety care about and the things that people in fate care about?
Do you think that this is like, like, why do you think that this agreement lies?
Like is it like a technical one?
More philosophical one? Like if you try to characterize why these people don't really worry about AI doom like we seem to?
Yeah, I mean, it's something I'm still trying to figure out and running a document about.
Yeah, so first I think like, I guess I would consider myself in like both of these camps.
Like I think like existential safety is super important.
But I mean, I also think like fairness problems are like also super important.
You know, like on any given day I wake up and I'm like, okay, got to be at least 30%.
As we find out recent events.
Yeah, no, I think justice is really important.
I don't want to live in a world where, you know, we have like just like our current systems of discrimination just like enforced or solidified because of really, really good artificial intelligence.
This seems like a concern to me that is like, yeah, I guess I find difficulty maybe like giving a absolutely precise like rating scale for how important things are.
In general, I try to find commonalities between causes I really care about to sort of do things that seem robustly good on both accounts.
So yeah, and I think like, you know, my work in evals so far is like an attempt to doing this.
What was your question?
Sorry, my original question was, where do you see like, I guess my question is why don't they people care about extras?
Oh, let's say even more broadly.
What do you think stops the average Mila PhD from being like shit man?
Let me write some elaborate foreign posts right now.
Okay, let's not say let's say like care about extras.
Yeah, I think there are a bunch of possible reasons.
I'm kind of not sure which ones are the most plausible.
I think I just have to like talk to more people.
One of them is, you know, AI is like taking, it's actually some wild shit, right?
Like you like stroll up the subject on the street and you're like in five years.
No, like be like taking him, right?
So I think firstly, you know, we just have to recognize like, yeah, this is actually wild, right?
Like, if you know, like you finished high school or university, like you go straight to work, you don't really think about like you're not really exposed to.
All the developments that have been happening in artificial intelligence is busy with like your life, right?
Your job, family, stuff like that.
This is like totally out of left field.
So I think we have to acknowledge that and like try to explain things in a way that like are more relatable to a greater extent than we have so far.
I think another thing is like this is definitely not true of like all people in AI safety,
but there is almost this vibe that like, you know, besides existential risk, like technology.
Yes, so maybe what I'm trying to say is like the association that is in people's minds between the people who are in a safety and the people in Silicon Valley.
So I think like there is this vibe from people in Silicon Valley that like technology is like generally a good thing and that it's going to solve social problems.
I think this is in contrast to a lot of people's opinions in like the fake community who like, yeah, maybe they're not like techno pessimists,
but they're definitely a lot more pessimistic about technology than people in Silicon Valley just looking at the history of ways in which technology has been misused and has been used to like discriminate against people more.
And then, you know, I think the people in the fake community, you know, they look at like the use of AI in like society today, like the increasing use of algorithms in places like loan applications or like job applications, right?
They say, oh, like, like clearly these technologies are just reproducing harms that we've already had, right?
So that might be this like sort of their starting mindset.
And it might be hard to like convince them otherwise that there is like actually another harm as well.
You know, the things you're talking about, they actually do happen and we should try to fix them, but there is this like related harm that we should ultra strive to solve.
How much of it do you think is, you know, in the spirit of the Inside U podcast?
How much do you think it is timelines, belief about speed of progress?
Part of it.
So a lot of people, I think the fake community don't think that AIs could be like, you know, classical RL agents pursuing goals in ways that misgeneralize to new environment.
Yeah, I'm not even sure that like a lot of people really know about reinforcement learning.
Yeah, like I've had a ton of conversations where, you know, I like people expressed to me, oh, like areas are just tools, like we're just designing them like, you know, they're really just like reproducing the data, right?
And I'm like, well, like, have you heard of reinforcement learning?
Like we're literally designing agents to maximize like a reward function, right?
This is like all the problems of classical utilitarianism.
They're like, oh, shit, I've never heard of this, right?
So part of it might just be education that like, you know, literally there are like companies and research labs in academia whose goal is to build artificial general intelligence with something like reinforcement learning.
And like, they're just they're just doing this.
The vast majority of people aren't thinking about safety concerns.
So I don't know, maybe like telling people this might help.
So it sounds like there's some underlying thing that were, you know, like, let's say I or like people in the extras community think of AI and they think of like some agentic, like, you know, maximizing reward.
And you may be saying something like a lot of them are practitioners.
That is just not the conception.
When they think of AI, that is just not what they see.
Yeah.
Yeah, yeah.
So I think like maybe there are two things here.
The first thing is people believe that, okay, like we don't actually have these kinds of AIs or maybe trying to build these kinds of AIs is unimaginable.
But the second thing might just be, okay, they might believe that it's possible to build these AIs.
But this is like way, way too far off, right?
And this is where I think the objection to long term comes in.
And where I think like, I don't know, it's been sort of complicated with AI safety and long termism.
Maybe like 20 years ago, like long termism was a stronger argument for AI safety.
But I think now because of timelines, it seems that we don't really need long termism to make like the argument that we should care about AI safety.
And it like made me 10 years.
I mean, maybe we just shouldn't talk about long termism.
Right. If it turns like people in certain communities off.
Yeah, because I think the worst ones that I get whenever I bring up like AI safety or anything related to long termism is, oh, okay, well, like this might be true.
But AI is already causing harm today.
So we should focus on immediate harms, right?
And you know, like, I don't think this argument like really makes that much sense.
And I'm not sure that the people expressing this argument like are actually expressing this argument.
It seems like they're expressing another objection, but it seems easier to say that like, you know, we don't care about these harms because they're not immediate, right?
Whereas, you know, if you look at the history of say like climate change, climate change was like totally a speculative thing in like the 1890s, 1900s, right?
And it's only through like decades or like maybe a century of like gradually building up evidence that we like now we have like this consensus.
We don't think it's a spectacular thing, right?
But even in like, I think like the 50s or something, don't quote me on this, but like there's like definitely a history of like climate change.
Like books and articles out there.
Like in like the 50s, we're still like, oh, like, we're not really sure still about like the greenhouse effect, right?
But you know, like, based I guess on like my preferred decision theory, it'd be like, well, like, we're not sure, but you know, it could be pretty bad.
We pump all the CO2 in the atmosphere, right?
How about we work on some mitigations like right now in the 50s?
Just just just in case like this might actually be a very hard problem to figure out and like it actually is right?
Not even just like technically speaking, but like on a coordination basis.
How do we actually get everybody together?
So it's worth it starting early.
I think in the case of climate change, and it seems like it's also the case with ASA.
So do you think that...
So I think you actually find something that at least I felt that I think a lot of the ASA people are very happy to get like Pascal Muglin has said.
Like I think this is my original motivation.
I was like, well, like maybe it'll be fine and maybe like just a little bit uncertain, but maybe it's not and it seems like to be really impactful.
Do you think you have to set some level of like, of this reasoning, some level of like, oh, I don't know if it'll be good or bad.
I don't know what's going to happen, but I should work on it because it's going to be impactful to work on ASA.
Or like, how do you see that changing at the moment?
It depends on how do me you are.
Yeah, I think personally for me it is like, sort of a gamble.
I mean, I'm like, yeah, I mean, even if we don't sort of get like the classical agent like AGI, I think things like are still going to be so, so wild with really good AI systems.
Like going to be like so many complicated societal feedback loops that we need to think about.
Like, multi polar world seems like more and more likely right with all these AI startups.
What kind of things do we have there?
Like conflict now seems like much more important thing to worry about.
So it's definitely like a gamble, but I don't think it's like a bad gamble to work on like the space of preventing negative consequences from AI generally.
So I have some model that the, and I think this is currently, the fields going to change a lot.
Like, you know, you have these big models coming out.
GPT-4 is rumored to be quite good.
When they release it.
When they've had these soft teas and releasing.
Keep saying next week, every week.
Every week, bro.
It's been, it's been, you know, funny.
Yeah.
You know, who knows what the gossip's at the moment.
Yeah.
But, um, yeah.
And like, I could definitely imagine a world where in like three to four years, I often say something like 2% of the American US population out of population is in love with their multimodal model.
You know, AI porn, TikTok is causing massive decrease in GDP.
Oh, maybe.
Right.
You can tell whether you have like several online personas that are fully automated.
It's kind of hard to tell like what's going on.
Let's say more than two years here.
Yeah, at least.
Let's say, let's say, let's say.
Um, um, in these worlds where like you're a medium person, it's like, you know, why is going on?
Um, and it's like, this is pretty crazy.
If you'd like this calculus, like change a bit, like, you no longer need to be kind of risk averse or kind of like heavily convinced by abstract arguments about, you know, the VNM axiom to think that AI might be dangerous.
Um, in that set, like, A, how do you feel about this model?
And B, how does it like affect you, affect safety general?
And in particular, you as someone who worries convincing people that they just, um, yeah.
Yeah, I have a lot of uncertainty about this.
So I think on one hand it should be like, good in the sense that, okay, you know, if everybody with their like, it's double the diffusion, like six, right?
And GPD, like 10, they're like, okay, you know, I look out at the world today.
What is the labor I can do?
Nothing.
I think that's a pretty wild world in a world in which people think, damn, like, these AI things, like, maybe we should, maybe we should regularly come, right?
Um, so I think this is good to, like, I guess make AI capabilities sort of be known in the, in the public.
Um, I'm not sure this is enough, though, because I think if we, people are just aware of systems like stable diffusion, um, and like, like a text generation systems, like, the two things that are missing, I think, are like the difficulty of alignment.
And number two, um, like, generality, like having an agent, right?
Um, so, like, I think having an agent concern really motivates a lot of ex-risk.
Like, I think that is, like, in some sense trying to do something that is counter to your aims or, like, you know, pursuing an instrumental goal that is counter to your aim.
That seems like it'd be really bad.
I'm not sure, like, we're able to impart that understanding just from, like, really good generative models.
Um, number two is, like, um, the difficulty of alignment.
Like, I think, um, you know, like, you're some, you're somebody in, like, 2035, you know, you're, you've, like, finished high school, you've finished university, now, like, you don't have a job, you'll never have a job ever again, right?
Like, who do you blame for this?
Um, I'm not sure you blame, like, the lack of alignment techniques.
I think you blame the corporations, right?
Or you blame the entire system that has gotten us to this point, right?
Like, we've created a world in which, okay, there is, like, no UBI, there is no other social safety yet, which at these corporations, like, making money.
Um, so now, like, you're stuck in this state of care.
Like, I don't think you care about existential safety necessarily.
Um, like, is this worse than the world we're in now?
Um, like, in terms of, like, getting people to care about X safety, I'm really not sure.
Okay, you kind of see this as an awful, like, a slightly awful duty, agent, X risky thing, though, which, which needs to be solved as well.
Um, yeah, like, like, I guess to say more, like, I mean, I think we need to solve, like, almost generalization stuff, right?
And, like, you know, a specification of good rewards.
Um, but, like, in my mind, like, if we have these really capable, like, generative models, people, when interacting with them,
they're going to be like, oh, you know, like, haven't we solved these things already?
You know, like, when I say, like, generate me, like, really good 3D porn, like, amazing 3D porn, right?
So, I mean, I think they might be like, oh, like, what is even alive in it?
Like, they just do exactly as I ask, right?
And you, and you, you may see, like, would you be fair to say that you see evils in such a world,
than calling people to focus more on the ekfrisky agentee outcomes?
Possibly, yep.
Um, so I think I see evils as, like, in general work, just, just, like, adding layers of security.
So, okay, maybe this is enough to get people on the AI safety train, right?
But we don't know.
Uh, we, it seems like we should be trying as hard as possible to get people on the, like, AI as, like, ridiculous train.
Um.
Okay.
So it's less of, like, a, you know, defense in depth.
It's another thing that someone should be doing.
And you've decided that as part of the wider strategic picture,
evils are, like, some important part.
Yeah, maybe some people call this a portfolio approach.
I call this, uh, I'm, like, a naturally, you know, very uncertain person on the road.
I want to cover all of my bases, just in case some things fail.
All right.
Do you?
Okay.
So maybe a wider question.
Wide.
So you kind of spoken about this idea that evils are some smaller part of a wider portfolio of life.
And because of your own uncertainty, you're kind of working on this.
This seems like robustly good.
Um, what do you see as, like, some of the other promising directions in this space?
Like, when you're like, if you, if you, if you found out, you know, if you got transported to 2014,
Paul Cristiano is like, you boom to Paul and Paul's just celebrating.
He's like, man, we've done it.
It's happened.
Alignment is soul.
Like, and you look out and, you know, the light cone is smiling and joining us.
That's so wide.
Yeah.
Yeah.
It's like, wow, what happened?
You know, obviously, apart from the evils, obviously the evils.
Evils, yeah.
For sure.
I'm picking up some, some prizes on the way.
Yeah.
Yeah.
What did it, what worked with the evils trigger?
Maybe it's a better question.
So yeah, maybe that's your first question.
What am I optimistic about?
Not a lot right now because I think alignment is really hard and we don't have much time,
less than 10 years to solve it either on a coordination basis, um, by getting people to
not build a GI or like on a technical basis, like actually formulating an operationalizing
problem and like developing a nice proof, right?
Um, I do think, you know, eventually we will have to do something like agent foundations
to really understand like what are these things like optimization and agency?
Like what are values, right?
How do we actually load them into agents?
What are we doing?
Right now it seems we're sort of poking around in the dark, like deep learning and RLHF and
it's like, okay, you know, on these data sets, it seems to work out fine.
We're not really sure if it's going to work on like other data sets.
We're not really sure how to define like agency in terms of our deep learning systems, right?
Um, so it's kind of like, I definitely think we still should be doing alignment of deep
learning, um, but like it's a bet and it might not work out.
Do you think, so if we need something like agent foundation, it seems that we would need
like some kind of restructuring of the research or we would need like much more people pouring
into these directions.
Um, and in general, I'm not sure if the evil work, let's say convincing like deep mind
on tropic or something, which is like, I think the frame you've given, um, it's going to
help with that.
Maybe.
Yeah.
So I don't think it's going to help get people into agent foundations.
I think it's more about, okay, let's, um, get people to care about AI safety in general.
That's it.
Yeah.
I mean, I think like there is, um, like the agent foundations people, I think, uh, or
maybe the community in general could definitely do much better job of like saying why agent
foundation is important, saying why, you know, this alignment stuff is actually really, really
hard.
Um, it's just right now just a bunch of like less wrong in alignment for a post, um, or
you talk to people in the community, right?
But, you know, the community from the outside might seem a little like off putting to approach.
So maybe we don't want to do that.
So you're sounding, uh, you've mentioned a few times this conversation.
And this whole, uh, 10 years has been thrown out.
Alignment is hard has been thrown out.
I guess I might as well ask the, uh, the fame question or the fame pair of question.
What, uh, what is, uh, timelines?
And PDOOM.
PDOOM.
So yeah, I think, um, did this calculation yesterday or a few days ago, the bunch of
friends, something like 46% it's quite high, I think, but, uh, could be higher.
Timelines.
Yeah.
I mean, I feel like a lot of my timeline stuff is like part of it is anchoring off of things
like the bio anchors report.
Another part of it is just like by 2012 image net.
I was like pretty cool when it wasn't like that.
Great.
But now it's like, I mean, it was like a couple of years ago, right?
Like two before, like maybe next week, right?
It's like, yeah.
Act, we, we went from like, like okay image classification to a system that's like actually
able to like do tasks in the world.
Um, I mean, you look at BBT, right?
Like opening, I certainly wasn't throwing all of its resources into tea.
Do you want to explain what the BT is?
Yeah.
The video pre-trained transformer.
So this is recent paper from open AI where they, um, pre-trained transformer on a bunch
of Minecraft YouTube videos.
It's a Mar-L-HF and see how it did in Minecraft.
So of course it's like not a human level, right?
Um, I think it was like a one billion parameter model or something, something less like that.
Um, so certainly like they could have thrown a lot more resources at this.
Uh, but still it seemed to do sort of reasonable.
It even was able to construct a diamond pickaxe, which is like very, very hard to do, um, in
Minecraft.
Um, you also have things like Gato, which like a transport pre-trained, like a bunch of RL
things, uh, RL environments.
Uh, and it seems to be able to like do a bunch of tasks.
Um, you know, so we have like really, really good capability, um, in like certain domains.
We have like, you know, increasing generality, right?
I think you just need to put these two together, scale things up a little bit more.
Uh, maybe add some more hacks and tricks.
And it seems like we're sort of there to something that is like an AI-like system, uh, where
at the very least could cause a lot of damage in the world.
Um, how do you see, how do you see the rest of the world looking then?
Like if you have like, you think we're quite close, like upon deployment of the system,
can you see like, yeah, I think, I think it's like an issue of sometimes how the question
of what's your timelines, because I'm not like, I think it very much posits some idea
like, you know, your time is a 10 years.
It means, you know, 10 years of one day from now, you might as well just retire.
It's about shits like God, right?
Um, but in my head, like, you know, the game still keeps going, right?
Um, so yeah, so like, let's say you get one of these, you know, video pre-training,
YouTube, behavior clone, which is software, pre-training, RLH done.
Action transformative systems in like, let's say 10 years and it's deployed.
Um, how do you see the world looking?
What do you actually, what is actually the worry here?
Like, what's, like, do you think we're still in the game?
Like when you say time of 10 years, are you like, you've got 10 years and that's it?
Like, yeah, what was the fight?
So maybe two things to distinguish are like, what is the point at which we get an AGI
or an APS AI, like advanced planning, strategically aware, like with key capabilities,
like hacking or manipulation AI.
Um, so yeah, I think my 10 years would be like for that.
Um, then the separate question is, okay, we have such an AI, would it actually do something bad?
I think this is the open question.
Um, I really don't know.
This depends on, okay, like what was the pre-training data?
Um, what do we know about generalization and deep learning at this point?
Um, like, how much do you actually have to prompt a model to do bad things
or to actually do bad things?
Um, like how bad of a problem are you, is, um, you know, instrumental goals
with like these sorts of simulators that, um, it's unclear.
Um, I think we might still be in the game for a while.
Um, but you know, it's this sort of, um, notion of precarity, I think.
Like, even if a model doesn't do something bad, it still might, right?
So I think in this time, we really have to, like, rush, um, after we deploy this first model
to, like, either solve the alignment through something like agent foundations
or produce convincing enough evidence to the world that we actually cannot solve this.
We need to coordinate not to build certain types of systems.
I mean, yeah, I guess I'm pretty...
I mean, alignment is like a public...
Yeah.
And if it's everybody, but like, you know, on the margin, a company like DeepMind
has incentive to just, like, move forward.
Yeah, I definitely think that...
Yeah, I guess I might just be pessimistic.
Coordination, making some abstract general sense.
Um, a few people have tried to do this for a long time.
It seems to just be, like, a thing that...
I think there's been, like, in my head, lots of people have wanted to make the world
more coordinated for a long time.
And it just hasn't really happened.
Yeah, for sure.
Yeah, for sure.
But I could definitely...
I think I'm definitely more...
I should put it kind of, like, benevolent.
Like, you know, like DeepMind...
I think, I think...
So you don't really realize being in the safety community.
It's just kind of, uh...
Like, villainization.
Like, it's like DeepMind, open AI.
Like explicit, like, like, you know, talking about, like, let's say some open or something.
You see for, like, caring, nice, genuine, intelligent, you know, future caring people.
Damus as well, right?
Um, I do have some, like, pretty good...
Some pretty, like, good hope that coordination there is very much possible.
Um...
Yeah, I think, um...
Maybe this is kind of rave, but, you know, we're all in this together.
I think these are just people at the end of the day.
Maybe we can't get them, but we gotta try till the last moment, right?
Uh, like, this problem is so important.
Uh, it'd be a shame if it just so happened we were in the world in which coordination would have actually worked,
but we just didn't try.
How embarrassing would that be?
The dignity.
Um, yeah, I don't know.
So I think it totally makes sense to be pessimistic about, like, solving all of this about coordination,
and then about alignment, um, about, like, any other, other stuff that could, like, you know, go wrong, like, S-risk.
Um, like, I think it totally makes sense, and, like, I definitely don't want to, like, judge anybody that is just, like,
you know, decided not to work on any of this because it's too depressing.
Like, that's totally fair.
This is, like, super, super hard.
Um, but I think, I guess, personally, my perspective is, like, okay.
You know, I, maybe, this just comes from me having a lot of uncertainty about a lot of things in my life.
Um, so, you know, on the off chance that, like, we could actually make a difference,
it seems worth it to try.
Um, even if, sort of, maybe the objective belief is that, okay, like, maybe it's not worth trying, if we actually did the ease.
I mean, I think this is a, um, the, the, the extreme deal, I guess, you'd take killer makes this point,
in a way, certainly.
Both in expectation, or he makes the point that, in expectation, the focus should be something like this.
Do, like, you know, do the thing that seems, like, the most defined doesn't seem like this.
You know, what does it, what does that mean, like?
I guess, I guess he's just saying, he's trying to make the point that, like, because in his worldview, he's, like, you know,
a several nines bed, probably, like, not that far away.
And he makes the point that, like, yeah, like, when you're, when you're acting in the face of, like, trying to increase such small probabilities,
or acting in the face of causing a problem that seems so hard, you shouldn't follow the heuristic of, like, what should I do to solve it?
And because everything seems doomed to you, you should be, like, what's the most dignified way?
It'd be more dignified if, like, OpenAI and DeepMind at least did some coordination before, like, some other company went and built AGI.
It'd be more dignified if, like, we really tried to scale mechanistic interpretability, or had, like, at least had evals, had some...
And he claims that it's, like, a more robust year signal, and also more motivating for oneself.
Oh, so the argument is, like, dignity isn't the thing we inherently care about.
It's, like, this heuristic that actually gets us to, like, reduce risk.
Yeah, he's, like, particularly for him.
The property is, like, oh, like...
The framing shouldn't be, is this going to solve a line?
Because nothing is. The framing is, like, is this a more dignified world?
Interesting.
And that's kind of, like, he approached the problem.
He said it, like, a lot more...
He didn't phrase it as nicely as that.
But, you know, I think this is his general point.
I think if you could make some...
Also, I think I have some term, like, value on humanity, but, like...
I'd rather, like, try it and, like, at least have tried.
Yeah, that's right, that's right.
Even when you know it's hopeless, we just keep on trying.
That's what we do.
I mean, I don't think... I mean, I'm not that much.
Quite not much to me.
This is interesting.
But if we did solve that, that'd be absolutely sick.
Oh, here's a question now, Lois.
So, you're talking about...
You care about the kind of, you know, fattest stuff, justice stuff.
I think, obviously, territory.
People right now, their hearts are being burned.
No one wants to slow down.
We all seem to make some implication, though.
Even as a cold hearted, long-term future caring to the Italian.
You should care about this, because these things might be locked in.
Like, the society in the future might be locked in.
Um...
I think...
I have some intuition.
I'm not sure, but at least I think a fair kind of pushback against this
is the idea that, like, there's just not that much time
where society looks like it does now.
Like, what you get is a kind of AGI thing,
and you get, like, self-improvement for a bit.
At some point, you just have, like, a super-intelligent God.
And, like...
Whatever that thing is doing, or whatever that thing wants to do,
is what decides what the society looks like.
Not necessarily, like, the current dynamics,
or, like, those dynamics, like, pushed into the future.
Um, how do you feel about that as a state?
Uh, I think this is a possibility,
but, again, I have a lot of uncertainty
about whether this will actually happen.
Like, I think, yeah, so maybe the thing that you're saying is,
okay, we have this, like, super-intelligent AGI,
it's, like, it's how I'm all figured out,
like, what are the true human values that, like, everybody cares.
And, like, it makes sure everything is okay,
or at least, like, gives us time to figure out all these human values, right?
Yeah, like, I think that'd be great.
I'm not sure this is actually going to...
Then, like, one is, like, SEV even possible.
Number two, okay, like, suppose OpenAI has built the AGI,
like, what do they do with it?
I think, like, A, the temptation to do something,
to instantiate, you know, your own values is just way, way too strong.
If you actually have a God at your hands.
Um, number two, I mean, even if they, like,
try not to do anything with it, like,
I think for sure other parties are going to want that kind of power, too.
The US government, China, maybe even other companies, right?
What happens when there's conflict?
What does OpenAI do?
Do they just obliterate other entities?
That's, like, wild, right?
If they did, I think for sure, like,
general populace would be like, what the fuck is going on with OpenAI?
Like, maybe we should go to war with this, right?
Um, this doesn't seem like a good future, either.
So, I think there are just a lot of factors that, like,
maybe we actually do need to figure out right now,
like, what is the game plan when we have AGI?
Um, to ensure that, like, we get that kind of future
where we have the time to think about, okay,
like, what are the things we actually want?
And we have, like, the luxury, you know, to work on,
okay, like, it's a limited resource scarcity, right?
Um, let's start at a limited discrimination somehow,
like, maybe not with the AGI, but, like,
because we don't have other problems,
we focus our, like, time and energy on, like, these, these social problems.
Yeah, so as you say right now, like, something like,
you still see society, like,
affecting the long-term future,
even in the face of, like, an incredibly advanced powerful system.
You think there's still, there are actions, right?
Everything still changes based on what a site looks like as well.
Yeah, yeah.
So, this all depends on the kind of AGI we have.
So, I think on the one hand, like,
suppose that this AGI is, in some sense,
value-free or value-neutral.
Um, okay, then, like, it's going to be aligned,
okay, suppose we solve a line, right?
Then it's going to be aligned with, like, whoever's controlling it.
Um, like, okay, if it's, like, open AI,
then you run into the problems that, like, I talked about, right?
Um, like, conflict or just, like, you know, some sort of dictatorship or something.
Uh, okay, suppose that, actually, in the meantime,
we sort of solve parts of, like, moral philosophy.
So, now, like, this AGI actually has, like, reasonable values,
um, that, you know, like, the vast majority of humanity would agree with, right?
And even if, you know, it's overseers,
thinks it's just do something, it actually doesn't,
because they know it's better, in some sense.
Um, okay, like, I think this seems, like, sort of reasonable, right?
But the difficulty is getting...
I don't think anybody's really working on this in the AI safety community,
figuring out, like, you know, what do we do, like, about different moral systems, for instance?
Like, um, like, what is the answer to moral uncertainty?
Is it, like, moral parliaments? Is it, like, something else?
Um, yeah, so, like, it seems that the first...
Yeah, on the first path, um, I don't know, conflict seems, like, pretty bad.
On the second path, well, we haven't really done much work towards this.
Um, so I'm not very, like...
I think I'm not very, like, optimistic about, um, you know,
the world, like, going well, even if we solve a line mid-term.
Yet, somebody should work on this, though.
What's your biggest frustrations with the AI safety community?
Biggest frustrations? Damn.
Like, it can't save me a second.
If I can shout it out later, I already don't hear it.
Yeah, frustrations.
Margot's definitely going to tweet this.
Let me ask the question again, sorry.
What's your biggest frustration with the AI safety community?
Frustrations.
AI safety community frustrations.
What is AI safety?
Perfect. Okay.
Alan, in your opinion...
Max.
Right, let's go ahead.
Alan, in your opinion, what is AI safety?
I think there's a broad version of this term,
maybe several broad versions of this term, and several narrow versions.
So, I think the narrow version, of course, is, like,
of course, meaning between us.
AI existentialcy.
So, how do you prevent AI from being existential risk,
whether it's through empowerment or human extinction or something else?
There's, like, broader versions of AI safety, too,
that include more than existential risk.
So, you might include S-risks, which care a lot about, like,
suffering caused by artificial intelligence,
either through conflict or, like, something else.
And I think there's an even broader notion of AI safety,
which, like, in my mind, this is the ideal definition of AI safety,
and it encompasses, like, literally everything, right?
Like, we care about, like, all the negative consequences from AI,
and we try to draw the threads, like, between all these phenomena
we're observing and, like, core technical and social problems.
So, this includes things, like, the things that people study in fairness, right?
Like, AI is that, like, are really able to, like,
learn what our fairness judgments are.
AI that, like, just exacerbate discrimination that is already present in society
and that is present in data sets that we use to train these AIs.
So, I think that's, like, that broad definition, to me, is the ideal definition,
the one we can all get behind, you know, so that we can do things
that we practically agree on, like, more regulation,
slowing down AI development, more like verification of AIs
before we deploy them.
Okay, given that definition,
um, and maybe focusing on the narrow
AI safety x-race definition,
um, what's your biggest frustration with the community or the set of people
currently works on?
Is it far that they are able to be homogenized?
Or maybe you should, like, go into more specific subsets of this community.
But yeah, the people that worry about the x-race AIs.
So, I have actually been doing this for that long.
I think, um, I've been doing, like, or I guess I've been considering myself
a part of this community for, like, maybe a year and a half?
Two years, if? So, like, basically just for my PhD?
Um, yeah. So, I mean, in the short amount of time,
I think it's been, like, pretty great so far just finding people
who care about the same things that I do and are motivated to work similarly.
I think this is definitely, like, a big, like, anti-frustration.
I guess it's, like, I think it's very frustrating working on something, like, by yourself.
Um, and, like, thinking you're the only person around who cares about the problem.
And everybody else thinks you're on the crazy train or something.
Um, yeah, maybe one frustration, though, is I think, um,
it'd be, like, a lot better if more people in the AI x-safety community
were, like, more explicitly wanting to build bridges
to other communities that also care about safety in, like, the broad, broad sense.
So, in particular, and, like, to the, like, fairness and, like, ethics.
Um, just because I think coordination is a super important thing for us to be doing
and might even be a low-tech fruit, um, to, like, slow down AI development
and make sure we don't, um, like, face negative consequences.
Um, yeah, I guess that'd be the, um, the main thing for me.
Do you see this community changing a lot?
Like, let's say, by this community, let's say, the set of people that, if you ask them,
what do you do, they would say something along the lines of,
I work to ensure that, um, extras from AI.
Do you see, like, that set of people changing, like, for years?
I think more people get on the train.
The same trains we are on right now.
Yeah, um, you know, AI is being deployed much more.
Um, these, like, ridiculous generative models, right?
It's, like, wild when you're put out of a job by an AI in, like, 2025.
But the name we predicted, 26, right?
Um, so, so, so, this is a good thing.
I guess, um, I hope that, like, you know, and getting more people on this train,
we also make sure that we repair the bridges that don't exist
or have been burned between the different safety communities.
Um, not sure how this is going to happen,
but, um, I think a good first step is just talking to people,
um, going to the places that they frequent.
Like, you know, I think definitely some AI safety people, like,
AI safety people should just go to, like, FACT,
like, the biggest conference in Ferris that's held yearly
and just, like, talk to people about concerns.
We should, like, submit papers there.
We should, like, actually try to understand, like, the fact people are saying,
um, you know, like, do some social theory, like, study some psychology.
Um, like, really think about, like, how AI is going to interact with society.
Like, maybe as well, we should try to develop, like, some sort of, um,
I guess, uh, point of view that is not just techno-optimist.
Like, you know, even if we solve a line, what are the problems that are left?
How similar do you think the technical problems are between an escape team?
So much similar.
So, I think one particular technical problem in fairness is
what, so in a particular context, let's say, um, I don't know, like,
hiring in this particular country with this particular historical context,
um, what do we do?
Like, what are the conceptions of fairness that are at play?
Can we, like, learn these things and formalize them in a certain way
so that we can actually try to understand what's, what's going on
and come to consensus about what we're doing?
Um, I mean, I think the techniques that we're coming up with in AI safety
are, like, super super rolling for this, right?
Like, if we do RLHs to actually learn people's preferences,
then we, like, study the reward function.
I think that might give us valuable insight, actually,
about what people actually think about in fairness.
Um, I think general stuff, too, like, you know,
anytime we think about, um, generalization problem in AI safety,
I mean, these are also relevant in fairness,
because, like, fairness problems are just, like, one-shot,
oh, like, the trained test distribution are, like, going to be the same, right?
Things are changing in the real world as well.
Um, so totally, I think, um, that thing is also relevant.
Uh, another thing is just, like, um, all this stuff about, like, instrumental goals
and, like, reinforcement learning systems and agents, right?
Like, if we're going to be deploying, um,
like, algorithms in society that make consequential decisions
about people's welfare, well, these are going to be decisions
that are going to be made over time.
Um, and in making decisions over time, like, we don't want, like,
a license we're deploying to, like, do really weird things,
um, have really weird instrumental goals.
Um, so, I think the connection to me seems, like, pretty clear,
but it hasn't been communicated well, which is pretty unfortunate.
You have a, uh, center of long-term risk.
I do, I do. The size too small, I think.
Well, I think so as well.
Well played, well played.
Um, what's this whole, uh, it's all S-risk thing.
S-risks, okay. Well, to learn...
Sure. What's this whole, uh, X-risk, S-risk, C-risk, you know?
So, I'm not sure what C-risk is, but, uh...
It's, uh, it's, uh, it's kind of strong.
Oh, I see, I see it.
Right, right.
Those, those things.
Yeah, so X-risk is existential risk.
Um, I think of this as problems that would, um,
either result in human extinction or in, like,
the lost capacity of, um, humans to, like, do things in the future.
And maybe, like, when, yeah, when I say things, I mean great things.
Maybe when I think about great things, it's, like, somewhat dying.
I mean, I think it'd be pretty...
You know, we, like, travel the star.
Like, you know, we see another species who try to help.
Um, I don't know, uh, there seems to be a lot of things
like we could do in the universe, like improve our understanding,
like, really just go through and review our history
and try to become, like, better agents, right?
All of this becomes impossible if, like, we're extinct
or if, like, we are trapped on, like, a planet
that has had all the resources, um, depleted.
Um, so that to me is an existential risk.
Um, a, an extraterrestrial risk is, um, a suffering risk.
So these are things that could cause, like, the, you know,
like, really, really, like, lots of suffering in the world.
Um, so what are some examples of things that cause a lot of suffering today?
Factory farming, arguably. Um, it's, like, absolutely terrible
how we treat our animals today.
Um, what are the things that could cause a lot of, um, suffering?
Um, dictatorship.
Dictatorships, yeah, malevolence is a lot of...
Yeah, so, um, yes, so I guess, like, there are some more mundane things
than there are things that, um, are a bit more, like, hypothetical
but seem like they could happen given the technologies
that we're developing right now.
Yes, the mundane things, you know, there's, um, there's factory farming.
I think there's also just, like, wars, you know, um, like, whenever...
So not just, like, the death tolls in the wars themselves,
like, what war brings with it, right?
So, like, just disease, like, malnutrition, like, general instability, right?
Um, that, that seems to have caused, like, a huge amount of suffering in history.
Um, so, so that's sort of, like, the more mundane things.
Um, I mean, mundane, I mean, it's like, there's something bad.
The things that we already know.
Um, there are things that, like, maybe we don't have yet, like, we could have.
Um, so hypothetically, you know, if we, like,
managed to become a galaxy-faring civilization
and we spread factory farming to the stars,
like, trillions and trillions of animals are suffering in factory farming.
It just, like, seems horrific, right?
Um, another thing is, okay, like,
suppose that we have really, really good AI systems
that, like, control large portions of, like, the Earth's resources,
like, the Solar System's resources, the Galaxy's resources.
Well, if they engage in conflict-like wars, that also seems really, really bad.
Like, our wars multiplied by, like, a million or something, right?
So, um, working as a risk, um, just to, like, really map out
what are the causes of suffering
and what are the ways we can, like, how can we act or reduce?
Um, so I think, like, one difference between ex-risk and ex-risk
is, like, sort of, where you start from, um,
in terms of, like, what you care about in your moral theories,
like, people in ex-risk really, really care about suffering.
And I would say they prioritize this to, like, differing extents over, over pleasure.
So, like, you know, between, like, if you have equivalent amounts of suffering
and, like, pleasure, you would definitely, like,
prefer reducing the suffering more about, like, one unit
than, like, increasing somebody's pleasure or, like, you know, happiness
by another unit.
And...
in so far, you can speak...
for...
I cannot. I cannot speak.
I cannot speak. I cannot speak.
Too many of them.
I mean, I was just, I was just an intern.
I don't work there as a full-time employee.
Where do you think your views differ the most
from the mainstream
AI safety research?
I think suffering is important.
I think cooperation is pretty important work.
Um...
Yeah, so, I mean, I think this has been a common answer.
I've a lot of uncertainty about this.
Um, I think I'm still in the process of
figuring this out because, like, S risks are still, like,
kind of new to me.
Yeah, I guess what I differ the most is
I care about cooperation.
I think it's important to get this work done, like...
What do you like, cooperation?
Yeah, so cooperation meaning, um, suppose you have, um, two agents,
like, they're in some sort of game.
How do you make sure that the outcome
is actually a pre-do-off-camel outcome?
So, for example, if you're both playing a game of chicken, right,
how do you make sure you don't, like, crash into each other
and, like, cause astronomical loss?
Um, you know, because, like, maybe, like, your one agent,
like, maybe one agent is, like, you know, this nation
that has a nuclear stockpile,
and, like, this other agent is, like, another nation
that has a nuclear stockpile.
And, like, these two nations together are, like,
the entire Earth, they have huge empires.
Um, a game of chicken is, like, okay, like,
you both launch nukes at each other,
right, we definitely don't want that kind of thing to happen.
So, uh...
I think a lot of us, like, we brought you here to find out
about your origin stories, you know.
Or origin stories.
Yeah. How did, how did Mr. Alan Chan
come to where
the hoodie that sent a long-term risk,
come to call himself an AI safety research?
Well, I just, like, took this hoodie.
I remember having to see all our others.
Okay, okay.
But the other bit.
The other bit. Okay.
Yeah, where should I begin?
How far back?
I'm actually, I think you know what, I want to hear from, like...
Okay. From what?
September 1st, 1996.
Yeah.
Um, probably at night, probably 12 a.m. or something.
Okay, I was born in Edmonton, Alberta, Canada.
To two parents, immigrants, Vietnam.
So, uh, yeah.
How much detail?
Just, you know, whatever.
Whatever's necessary to understand.
To live it as if you're in your shoes.
Yeah.
So, I don't know.
Why do I work on AI safety?
So, I think part of it is,
why do I work on things that could go wrong,
I guess, I don't know.
No, no, I'm wanting, I'm wanting to story.
It's play-by-play.
What made, from, from, you know, let's say starting...
Yeah.
...to sitting down here.
What happened?
Yeah.
Okay.
So, I think there's like a little bit more to this than the undergrad.
Okay.
I think this also depends on how I ended up developing my moral motivations.
Okay.
Like, why didn't I just like go and be like an investment banker, right?
And like, literally just like a regular investment banker.
Like, none of that earned against us.
Yeah.
I mean, I think like part of it is, is my upbringing.
Like, I think, like, my family is like pretty Buddhist.
They care, and in Buddhism, you care a lot about reducing the suffering.
And particularly, my mother cares a lot about Buddhism and reducing suffering.
So, I think just growing up in an environment like,
they need care about these things as well.
And to the extent that I saw like suffering in the world,
whether it was like on the news or like interpersonally,
you know, that that seemed like a really bad thing.
So, I think that's like one part of it.
Another part is just like being exposed to like the things
that I think make life really worth living,
like just like hanging out with your friends,
like doing fun things, trying new foods, traveling.
So, I think like both the upside and the downside,
that like I was able to experience in my life,
like I think maybe believe that, okay, you know,
it seems like really important to make sure that they are,
to like remove the downsides for as many people as we can,
and to make sure that like people can actually, you know,
experience the upside in life.
So, I think that's like general motivation, I guess,
for like working on social causes or causes to like reduce risk
in like some very, very general sense, right?
So, I think I spent a lot of time doing a lot of searching
and thinking for like what sorts of things I could work on.
I like tried out a bunch of volunteer and grievous causes
in like high school and university just to see like
what things might be interesting for me.
I think this is good to the extent that like, you know,
I learned a lot more about like things that are wrong in the world.
I got really into social justice.
And like, I think like, how did I get into AI safety?
It was kind of in my like latter part of my undergraduate,
like going to my master's.
So, I did a math degree during my undergraduate.
I did a math degree mostly because I didn't really know
what exactly I wanted to do.
Maths just seemed like a robustly good thing
and gave me the flexibility to take a lot of other things
that I was also really interested in, like linguistics
and like liberal science and they also do a lot of debate,
where I talked about like a bunch of this stuff too.
So, yeah, I guess like having a diverse range of interests
made it really hard to focus in on a particular thing like as one.
I mean, I think I still want to just like try a bunch of different things.
And like, maybe this is a difficulty in like getting concrete projects out.
So, yeah, at the end of my bachelor's degree, I was like,
well, what do I do now with a math degree?
Like nothing I've tried has been super, super convincing to me.
I don't really want to be a mathematician.
Like it was like nice, but it seems like being a pure mathematician
doesn't really have like a lot of impact in like the near or like medium term future.
And it seems like there are more important problems than like being a pure mathematician
and more problems to work on like, you know, climate change,
dried or like global health.
So then I started thinking, okay, so what are the things that could like really,
really make the world turn out bad or like to not like a really big impact
in the world in like my lifetime, say, right?
So, you know, AI, I think, happened to be one of those things that I was thinking about.
So I thought, okay, like maybe I should go into AI.
So I spent like about a year just like reading a lot about this and thinking,
okay, you know, like where is AI going?
Like, why do I think that it's like could actually be like a really big thing, right?
So I think solidifying those intuitions,
and at this point, I was like doing a masters that, you know,
it wasn't like the ideal masters, but I later like switched advisors
and like it was a lot better for me.
So yeah, like in the middle by masters, I like switched doing like AI,
like start off with reinforcement learning.
And it was like really fun.
And I really enjoyed the environment that I was put in,
just like having people who like cared also about AI
and also who also thought that could be really big thing.
But in the course of my masters, I guess I thought, okay, you know,
this is like a reinforcement learning lab.
So I was at the University of Alberta.
It's a reinforcement learning lab.
These people like actually want to build a GA.
I don't know, like this seems kind of concerning to me,
like at this intuition, like, okay, like, oh, what the fuck though?
Like an artificial general intelligence, like you could go to do some bad things, maybe?
Yeah, so this was like, I think, yeah, I finished my masters in 2020.
So I guess in the middle of COVID, I was sort of thinking,
like trying to grapple with these questions.
Also, like just like noticing or just like living through like COVID
and also like the George Floyd protests, right?
I was like, okay, like, you know, real shit like actually happened to the world.
I'm like living through history, right?
So like maybe something wild could like actually happen, like, you know,
and something I'm like personally involved in, like every day,
like in like AI, right?
So I started to read a lot more about AI safety.
So like, you know, super intelligence and stuff,
like a little bit of a lot of inform and that's wrong.
And you know, I remember I was like reading super intelligence and I was like,
damn, like, this is true.
Shit, is anybody working on this?
Then I thought, well, you know, like, like maybe I should work on to start, right?
And I think like having that feeling that like, wow,
like this is a thing I should work on was a pretty like life changing moment.
Because I think like before, I guess like 2019 ish, you know,
when you like learn about history in school, it's sort of like, okay,
like these things happen to these people, right?
And like, damn, like we have the world we have today.
But to some extent you feel sort of the distance from what happened,
like these people are so far removed, hugging never relate, right?
But you know, we're living through history right now.
We live through a pandemic.
We're living through like an increase in geopolitical tensions, right?
And we're living through like, like a lot of really concerning developments,
artificial intelligence.
Well, like we're living through history, that means we can affect it, right?
So I think like that moment, maybe it was more like a gradual development.
Maybe think that I could actually do something about this problem.
So, you know, I applied for PhD programs.
I didn't apply for AS80 though.
I was still sort of unsure whether I wanted to like fully devote all my time to this.
But, you know, I got into a PhD program at Mila.
I started like doing research, but like basically immediately,
I like really tried hard to like shift my research from reinforcement learning to doing AS80.
It took a while.
And I think like, you know, I'm still sort of learning how to like do AS80 research well
and figuring out what the most valuable thing to do is.
But I think like, yeah, it's been going pretty well.
And I'm really glad I make that decision to switch.
Nice.
And write a second of what you're working on.
Right, working on.
So I'm trying to finish up the paper with CLR.
We are trying to evaluate the cooperativity of language models.
So the really interesting thing I think is, okay, you know,
you can, you know, construct a dataset, get people to write scenarios for you if you want.
But we actually want our ability to generate evaluations to scale with the capabilities of our models.
So we're getting language models to generate scenarios for us that like basically co-operate scenarios that involve some co-operation.
And seeing what language models do in these kinds of scenarios.
That's the first thing I'm working on.
I also have just a bunch of other projects right now that are varying stages of completion or feasibility.
One thing I'm really interested in is like, you know, how do we actually, how are we actually able to like show people that models are actually doing a lot,
actually much more capable than might be claimed by some people.
Yeah.
So another thing I'm working on is this sort of like general, almost like position paper slash survey paper on like speculative concerns,
AI safety.
It's essentially going to be an argument that it's important to speculate for cause areas and like review sort of, okay,
what are the methodologies and the ways in which people have speculated in AI safety?
What has worked out?
What hasn't?
Why do we still need to continue on coming up with things that might appear more speculative to like modern machine learning communities,
but like are actually important.
I think importing out possible problems we might face.
Is this aimed at machine learning?
Just this position paper.
Yeah.
So it's aimed at the academic machine learning.
I mean, I think a lot of what I, yeah, parts of what I want to do are sort of more aimed at, okay,
like how can we field build or build bridges effectively by either like connecting our concerns, concerns other people have,
or by just saying things in like, you know, language that other people can more understand.
Nice.
I don't know.
Show up.
I don't know.
Show up.
Show up.
Show up.
Show what else I got for Mr Allen.
This is quite...
What?
Look what you come about, it's come about.
Down slide.
Are you really done with?
I'm killing, I'm like after that.
Maybe they see some competition in this.
Plus in sports.
Turn down the monopoly.
Like on feet.
Good fight, good fight, good fight. Put on money where my off is.
Yes, good pipes!
