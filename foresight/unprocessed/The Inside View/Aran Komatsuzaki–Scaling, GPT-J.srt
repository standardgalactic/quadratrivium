1
00:00:00,000 --> 00:00:08,760
Aaron, you're an ML PhD student at Georgia Tech, a research intent at Google, and a researcher

2
00:00:08,760 --> 00:00:11,560
at OPAI.

3
00:00:11,560 --> 00:00:17,080
At Eleuther, you've been working on a DAWI reproduction, which later became Lyon, and

4
00:00:17,080 --> 00:00:26,080
you proposed GPTJ, a JAX-based GPT3-like model whose performance is on par with GPT3 6 billion

5
00:00:26,080 --> 00:00:29,960
on various downstreaming tasks.

6
00:00:29,960 --> 00:00:35,320
You're also well-known for being one of the two AKs where the legend says that if a deep

7
00:00:35,320 --> 00:00:41,120
learning paper is important, one of the two AKs will have tweeted about it.

8
00:00:41,120 --> 00:00:46,240
Your name was mentioned before on the podcast as one of the persons who have convinced Ethan

9
00:00:46,240 --> 00:00:50,760
Caballero that scaling will turn out to be important.

10
00:00:50,760 --> 00:00:52,840
Thanks Aaron for coming on the show.

11
00:00:52,840 --> 00:00:55,880
Thanks for having me today.

12
00:00:55,880 --> 00:00:59,080
So let's start with the legend of the two AKs.

13
00:00:59,160 --> 00:01:05,000
Who is this other AK and why do people say that following the two of you is enough to

14
00:01:05,000 --> 00:01:07,680
understand deep learning research?

15
00:01:07,680 --> 00:01:15,320
Yeah, basically we scan all the archive papers, archive machine learning papers every day,

16
00:01:15,320 --> 00:01:20,440
and we tweet about them with some summary and visualization.

17
00:01:20,440 --> 00:01:27,680
And yeah, we've been doing it for several years and we became, we got more and more followers.

18
00:01:27,680 --> 00:01:38,400
And that's probably why everyone follows us to get the grasp of the latest research.

19
00:01:38,400 --> 00:01:43,480
How do you manage to read so many papers, writing all of the summaries on Twitter?

20
00:01:43,480 --> 00:01:46,840
What's your process on a daily basis?

21
00:01:46,840 --> 00:01:56,960
So I basically check archive CSP at around 6 p.m. every day and I scan all the titles

22
00:01:56,960 --> 00:02:02,520
and if the title interests me, then I read the abstract.

23
00:02:02,520 --> 00:02:08,360
Then if I'm still interested, then I would scan their tables and figures.

24
00:02:08,360 --> 00:02:16,360
And if I think the paper is promising, then I would tweet the paper with short summary

25
00:02:16,360 --> 00:02:17,920
and visualization.

26
00:02:17,920 --> 00:02:25,960
He tweets much, many more than I do because he tries to tweet as many good papers as possible.

27
00:02:25,960 --> 00:02:30,440
Well, I try to tweet the ones I think the best.

28
00:02:30,440 --> 00:02:34,560
So yeah, he spends much more time on it than I do.

29
00:02:34,560 --> 00:02:41,400
And yeah, I think that's amazing.

30
00:02:41,400 --> 00:02:47,960
Yeah, so how much time do you spend every day, would you say?

31
00:02:47,960 --> 00:02:49,480
About up to 30 minutes.

32
00:02:49,480 --> 00:02:51,920
I wouldn't spend more than that.

33
00:02:51,920 --> 00:02:54,560
You know, I'm a researcher.

34
00:02:54,560 --> 00:03:00,360
So whether I tweeted about it or not, I have to scan all the archive papers anyway.

35
00:03:00,360 --> 00:03:04,560
So I think this time investment is totally worth it.

36
00:03:04,560 --> 00:03:05,560
Okay.

37
00:03:05,560 --> 00:03:12,560
So yeah, how many abstracts do you end up reading every day, would you say like 10, 50, 100?

38
00:03:12,560 --> 00:03:15,680
Yeah, it depends on the day.

39
00:03:15,680 --> 00:03:19,120
Sometimes it's very, the field is very productive.

40
00:03:19,120 --> 00:03:26,640
Like yesterday, I read like 10 abstracts, but I usually read only like three abstracts.

41
00:03:26,640 --> 00:03:33,720
Yeah, I have probably because I have very specific tastes for papers.

42
00:03:33,720 --> 00:03:38,680
I'm more biased towards NLP papers, for example.

43
00:03:38,680 --> 00:03:45,480
Yeah, to go back to the reason why you're here today, you were tweeting as every day

44
00:03:45,480 --> 00:03:53,520
and one of the tweets wrote was an answer to my AGI political compass, the latest one.

45
00:03:53,520 --> 00:03:59,240
And you were saying that you were not on the compass because you were higher than some

46
00:03:59,240 --> 00:04:02,400
ultimate or something more extreme than his position.

47
00:04:02,400 --> 00:04:08,120
I know this is a joke and I'm sorry to bring something from Twitter on a podcast, but maybe

48
00:04:08,120 --> 00:04:13,080
could you summarize your take for, you know, the listeners that are not on Twitter every

49
00:04:13,080 --> 00:04:14,080
day?

50
00:04:14,680 --> 00:04:20,360
Oh yeah, basically what I say is something like, so he said, is that I'm not worried

51
00:04:20,360 --> 00:04:22,680
about AI alignment problems.

52
00:04:22,680 --> 00:04:29,680
And I think I say, I don't consider this task of alignment to be any different from any

53
00:04:29,680 --> 00:04:32,240
other half ML task.

54
00:04:32,240 --> 00:04:39,000
But as we talked about it, I think I am wrong about this.

55
00:04:39,000 --> 00:04:41,760
Yeah, so we can talk about it later.

56
00:04:41,840 --> 00:04:49,160
And I think I also say we should focus on that long term, which we agree, I guess, and

57
00:04:49,160 --> 00:04:50,840
scaling is important.

58
00:04:50,840 --> 00:04:54,760
But unlike Ethan, I don't think it's all you need.

59
00:04:54,760 --> 00:05:02,600
And I don't know about AGI, but I think superhuman level, recursively self-improving

60
00:05:02,600 --> 00:05:09,920
language model, maybe possible somewhere around 2028 to 2038.

61
00:05:09,920 --> 00:05:10,920
Yeah.

62
00:05:10,920 --> 00:05:15,080
And I think there's a lot of disagreement on Twitter between focusing on the long term

63
00:05:15,080 --> 00:05:16,920
or focusing on the near term.

64
00:05:16,920 --> 00:05:23,040
And I think there's a bunch of confusion because for people who have short AI timelines, their

65
00:05:23,040 --> 00:05:25,200
long term is kind of the near term.

66
00:05:25,200 --> 00:05:29,840
So for you like 2028, 2038, it's kind of the long term future, right?

67
00:05:29,840 --> 00:05:34,200
Because things are going to be a lot of different then.

68
00:05:34,200 --> 00:05:37,720
So and yeah, people are also confused about definitions.

69
00:05:37,720 --> 00:05:44,360
So AGI, artificial intelligence, or human level, or superpower intelligence.

70
00:05:44,360 --> 00:05:49,800
So maybe just to ground the discussion a bit, could you give your definition of AGI?

71
00:05:49,800 --> 00:05:54,320
So what's the definition you think about when you see that word on Twitter?

72
00:05:54,320 --> 00:05:55,320
Yeah.

73
00:05:55,320 --> 00:06:02,760
So I'm really interested in AGI, but to be honest, I'm not an expert on it.

74
00:06:02,760 --> 00:06:07,480
So I'm just, I'm trying to learn about it.

75
00:06:07,480 --> 00:06:11,440
So I don't know if my terminology or definition is correct.

76
00:06:11,440 --> 00:06:18,120
But to me, AGI is something like a model that can recursively improve itself and can perform

77
00:06:18,120 --> 00:06:24,840
any task, at least as well as humans do.

78
00:06:24,840 --> 00:06:29,600
Personally, I'm, oh yeah, go ahead, sir.

79
00:06:29,600 --> 00:06:36,760
I was just going to say that, you know, so if we think about like human level, not all

80
00:06:36,840 --> 00:06:41,360
humans are capable of writing ML code or thinking about AI.

81
00:06:41,360 --> 00:06:46,480
If you just take someone, some average human on the street, they will not be able to improve

82
00:06:46,480 --> 00:06:49,520
an AI or self-improve.

83
00:06:49,520 --> 00:06:55,040
So yeah, I think one of the definitions, sorry.

84
00:06:55,040 --> 00:07:02,440
Yeah, one of the reasons people give for why an AI would be able to self-improve if it was

85
00:07:02,560 --> 00:07:10,200
human level is that a human given like enough time and memory could be able to like read

86
00:07:10,200 --> 00:07:17,200
all those archive papers and, you know, come up with another solution, assuming like our,

87
00:07:17,200 --> 00:07:23,760
you know, like there's not like a problem in the architecture of humans that, you know,

88
00:07:23,760 --> 00:07:28,760
would make it impossible to like improve or something.

89
00:07:28,760 --> 00:07:33,360
So yeah, I guess that's kind of one of the reasons people might like not make a distinction

90
00:07:33,360 --> 00:07:40,360
between those two, like human level and, you know, recursively self-improvement.

91
00:07:40,360 --> 00:07:45,760
But yeah, I think that's like a reasonable guess to like put those very close.

92
00:07:45,760 --> 00:07:46,760
Yeah.

93
00:07:46,760 --> 00:07:55,760
So when I say human level, just like many other people, I think I'm saying that as well

94
00:07:56,760 --> 00:08:02,760
doing each task as well as top human experts, so not like other humans.

95
00:08:02,760 --> 00:08:07,760
You know, other humans, like you said, you cannot really write machine learning papers.

96
00:08:07,760 --> 00:08:12,760
I don't remember whether you said that or yeah, but that's what I mean.

97
00:08:12,760 --> 00:08:17,760
So yeah, that's basically what I meant.

98
00:08:17,760 --> 00:08:27,760
So in particular, I'm interested in AGI's capability to do research because that's basically doing

99
00:08:27,760 --> 00:08:36,760
doing research is basically or doing ML research is basically recursive self-improvement.

100
00:08:36,760 --> 00:08:46,760
And also, you know, it can advance other areas of science or yeah.

101
00:08:46,760 --> 00:08:55,760
So I think so as long as a machine learning model can do machine learning research as well as humans do,

102
00:08:55,760 --> 00:09:02,760
I think that's that leads to AGI eventually without any human intervention.

103
00:09:02,760 --> 00:09:09,760
So a human level machine learning research is AGI complete in some sense.

104
00:09:09,760 --> 00:09:15,760
And I'm trying to make language model to machine learning research.

105
00:09:15,760 --> 00:09:20,760
Yeah, I think that's a valid path to AGI, even though there are many other path.

106
00:09:20,760 --> 00:09:23,760
Yeah, just to clarify the definitions.

107
00:09:23,760 --> 00:09:29,760
When people say AGI complete, they usually mean you need AGI to reach that point.

108
00:09:29,760 --> 00:09:37,760
What you're saying is doing ML research enables human level AI, right, or AGI.

109
00:09:37,760 --> 00:09:41,760
So you're more saying like it implies, right?

110
00:09:41,760 --> 00:09:48,760
It's being good at ML research and being able to do as much ML research as you want implies AGI.

111
00:09:48,760 --> 00:09:50,760
Is that what you're saying?

112
00:09:50,760 --> 00:09:51,760
Yeah.

113
00:09:51,760 --> 00:09:52,760
Okay.

114
00:09:52,760 --> 00:10:01,760
And I think so I agree with some version of that, but then it depends on what you consider to be ML research.

115
00:10:01,760 --> 00:10:20,760
So for instance, is building like ML hardware and transistors and building factories that produce electricity that would power those ML hardware.

116
00:10:20,760 --> 00:10:22,760
Is that all ML research?

117
00:10:22,760 --> 00:10:24,760
Why do you draw the line?

118
00:10:24,760 --> 00:10:25,760
Yeah.

119
00:10:25,760 --> 00:10:36,760
For a human level machine learning research, I mean, to be able to replace pretty much every ML research staff, ML research project.

120
00:10:36,760 --> 00:10:40,760
So including software.

121
00:10:40,760 --> 00:10:43,760
Yeah.

122
00:10:43,760 --> 00:10:45,760
That's what I meant.

123
00:10:45,760 --> 00:10:47,760
Okay.

124
00:10:47,760 --> 00:10:52,760
Yeah, I guess my take is that, you know, the economy is very complex.

125
00:10:52,760 --> 00:10:57,760
And you need like energy to do things and robots to build things.

126
00:10:57,760 --> 00:11:16,760
And so the kind of the task that would, so more like the intelligence level to be able to reproduce like all that branch of the economy that builds computers is kind of automating like 10% of what humans are able to do right now.

127
00:11:16,760 --> 00:11:30,760
If you really wanted to like self-improve and build bigger and bigger computers without like having to call a human to do the task for you, then you would need to be pretty close to AGI already.

128
00:11:30,760 --> 00:11:32,760
Yeah, that's true.

129
00:11:32,760 --> 00:11:51,760
Given what we said about recursive self-improvement requiring a hardware and robots building the hardware, would you say like recursive self-improvement will arrive much later than, you know, ML research in archive papers?

130
00:11:51,760 --> 00:11:58,760
You mean being able to write ML papers.

131
00:11:58,760 --> 00:12:06,760
Right, or yeah, being able to write the papers and maybe like a run experiments by yourself without like actually like improving your hardware.

132
00:12:06,760 --> 00:12:07,760
Yeah, I agree.

133
00:12:07,760 --> 00:12:12,760
I think software improvement comes much earlier than hardware improvement.

134
00:12:12,760 --> 00:12:25,760
But I think so, given if you look at the past ML research and scaling, most of the scaling comes from additional funding.

135
00:12:25,760 --> 00:12:31,760
But and some of the scaling comes from hardware improvement.

136
00:12:31,760 --> 00:12:51,760
But I think like a lot of improvement simply comes from software improvement, like improvement in design, like the transition from LSDM to transformer or like scaling, optimal scaling strategy.

137
00:12:51,760 --> 00:12:59,760
So I think there's a lot of things AIs can do simply by improving the software.

138
00:12:59,760 --> 00:13:06,760
And yeah, just to go back to what you said before about scaling is not all you need.

139
00:13:06,760 --> 00:13:11,760
Yeah, why do you say you disagree with Ethan Caballero on that?

140
00:13:11,760 --> 00:13:13,760
Obviously scaling is very important.

141
00:13:13,760 --> 00:13:23,760
And if you look at the result of palm over, well, smaller palm or other models, then you can see that the improvement is huge.

142
00:13:23,760 --> 00:13:34,760
And I think that's a very big indication that we still haven't exhausted all the space for improvement from coming from scaling.

143
00:13:34,760 --> 00:13:54,760
But I don't say scaling is all you need because as I talked about, the improvement coming from performance improvement coming from model design improvement is huge.

144
00:13:55,760 --> 00:14:04,760
The most important one is improvement in scaling strategy or called optimal scaling strategy.

145
00:14:04,760 --> 00:14:13,760
For example, open AIs paper, that brings from 10 to 100 times of speed up.

146
00:14:13,760 --> 00:14:29,760
So basically the model optimally scaled up performs as well as the model suboptimally scaled up using from 10 to 100 times of a compute.

147
00:14:29,760 --> 00:14:39,760
This result was demonstrated, but technically this is possible.

148
00:14:39,760 --> 00:14:50,760
So yeah, and also, you know, the improvement coming from VQVA, I mean, Dali 1 to Dali 2.

149
00:14:50,760 --> 00:15:00,760
I think this huge leak is not possible simply by increasing the computational budget spent on Dali 1.

150
00:15:00,760 --> 00:15:12,760
So this is probably because I think this is because there is a fundamental bottleneck with the design of VQVA or Dali 1.

151
00:15:12,760 --> 00:15:22,760
So there are some of the things that you cannot simply overcome with additional by adding more copies.

152
00:15:23,760 --> 00:15:31,760
Yeah, so what was the paper you mentioned where you go from a 10x improvement to a 100x?

153
00:15:31,760 --> 00:15:33,760
Like you said, the open AIs paper?

154
00:15:33,760 --> 00:15:36,760
Yeah, scaling goes for neural laggis models.

155
00:15:36,760 --> 00:15:42,760
So that's like a way of scaling your models optimally.

156
00:15:42,760 --> 00:15:50,760
And what you're saying is that you cannot just scale without thinking about the optimal scaling.

157
00:15:50,760 --> 00:15:53,760
Is that what you're basically saying?

158
00:15:53,760 --> 00:16:03,760
Yeah, basically not using this kind of principle way of scaling is a common practice before this paper.

159
00:16:03,760 --> 00:16:11,760
But after this, basically most many of the big projects tried to follow this scaling now,

160
00:16:11,760 --> 00:16:22,760
which I think is very important for saving the copies or maximizing the performance.

161
00:16:22,760 --> 00:16:33,760
And yeah, one of the things that we mentioned in the episode with Ethan was that you kind of influence him to be more interested in scaling

162
00:16:33,760 --> 00:16:37,760
because at the beginning it was not that much into scaling,

163
00:16:37,760 --> 00:16:43,760
but then maybe like a few years ago you told him that scaling was going to be huge.

164
00:16:43,760 --> 00:16:47,760
So yeah, how did you get interested in scaling?

165
00:16:47,760 --> 00:16:52,760
Yeah, what's here or what was the kind of thing that got you into it?

166
00:16:52,760 --> 00:16:55,760
Yeah, so I first got into machine.

167
00:16:55,760 --> 00:17:01,760
So I started reading machine learning papers on the summer of 2017.

168
00:17:01,760 --> 00:17:11,760
Then Transformer paper was released and soon got into language modeling because I thought that would be the key for AGI.

169
00:17:11,760 --> 00:17:20,760
Then yeah, I was almost immediately and talked about several months to be convinced of Transformer language model,

170
00:17:20,760 --> 00:17:24,760
replacing all the LSDMs in every applications.

171
00:17:24,760 --> 00:17:35,760
Then I read a paper titled Exploring the Limits of Language Modeling, which was released in 2016.

172
00:17:35,760 --> 00:17:44,760
So this paper basically tries to scale up the size of LSDM and dataset size to improve that publicity.

173
00:17:44,760 --> 00:17:53,760
And I did their generated text and it doesn't look so much better than any of the text I saw and their publicity is so much better.

174
00:17:53,760 --> 00:17:59,760
So it's kind of like a GBT2 moment for me except it was like 2017.

175
00:17:59,760 --> 00:18:12,760
So that's basically the first scaling thing I saw and there was also another paper titled Deep Learning Scaling is Predictable and Predictable.

176
00:18:12,760 --> 00:18:17,760
I think Ethan mentioned that it was released in 2017 too.

177
00:18:17,760 --> 00:18:26,760
It shows that there's a parallel between training cards, performance, model size and dataset size.

178
00:18:26,760 --> 00:18:31,760
But that doesn't really tell you exactly how to scale up the models.

179
00:18:31,760 --> 00:18:39,760
Then finally there's GPT2, which was in January of 2019.

180
00:18:39,760 --> 00:18:46,760
By then I was already convinced that scaling is going to be the key.

181
00:18:46,760 --> 00:19:05,760
Yeah, if you were convinced of scaling before Transformer or in 2017, then seeing GPT2 in beginning of 2019 must be enough or not a big update from 2017.

182
00:19:05,760 --> 00:19:15,760
Or at least they showed that you could get generation of paragraphs and a bunch of benchmarks in NLP with just scaling.

183
00:19:15,760 --> 00:19:21,760
So yeah, did it surprise you a little bit or were you not surprised at all from GPT2?

184
00:19:21,760 --> 00:19:30,760
No, I was not surprised with GPT3 at all because I was kind of working on a similar project with small scale.

185
00:19:30,760 --> 00:19:40,760
So like before GPT2, we language model researchers were working on better sampling techniques.

186
00:19:40,760 --> 00:19:44,760
So GPT2 used, I think, top-K sampling and temperature.

187
00:19:44,760 --> 00:19:55,760
So basically these two were rediscovered in 2018 and I was just playing with these new sampling methods.

188
00:19:55,760 --> 00:20:04,760
And so the generated text was much better than anything I saw before.

189
00:20:04,760 --> 00:20:13,760
So yeah, I was not surprised with the result at all, but I was very happy.

190
00:20:13,760 --> 00:20:17,760
Do you want to tell us more about the project you were working on?

191
00:20:17,760 --> 00:20:24,760
Oh, you mean the project I was working on in 2019?

192
00:20:24,760 --> 00:20:33,760
Yeah, so you said it was a project that involved top-K sampling or other methods that were not the same as GPT2?

193
00:20:33,760 --> 00:20:38,760
Oh yeah, it was not like a big project.

194
00:20:38,760 --> 00:20:51,760
So I was just trying to use top-K sampling and temperature sampling as a small transformer language model I was trying in 2018.

195
00:20:52,760 --> 00:20:56,760
Then the result was so much better.

196
00:20:56,760 --> 00:21:04,760
So I guess that's all I can tell, but there's a project I did in 2019 about scaling.

197
00:21:04,760 --> 00:21:08,760
Yeah, it's called one equal piece all you need.

198
00:21:08,760 --> 00:21:13,760
So, okay, can I talk about this project?

199
00:21:13,760 --> 00:21:15,760
Yeah, sure, go ahead.

200
00:21:15,760 --> 00:21:28,760
So basically, okay, so nowadays we're trying a huge model on huge data set for one or a few epochs,

201
00:21:28,760 --> 00:21:38,760
but back then we were training smaller models for many, many iterations with very, very small data set.

202
00:21:38,760 --> 00:21:44,760
Even GPT2, GPT2 used 100 epochs, I think.

203
00:21:44,760 --> 00:21:45,760
So bad.

204
00:21:45,760 --> 00:21:47,760
Yeah, exactly.

205
00:21:47,760 --> 00:21:51,760
The data set wasn't that big, like only 40 gigabytes.

206
00:21:51,760 --> 00:21:54,760
Well, it was huge back then.

207
00:21:54,760 --> 00:22:04,760
Yeah, so the model size was only one billion, even though AI can totally spend more money on that.

208
00:22:04,760 --> 00:22:21,760
Okay, so basically this shift from old days to today, it was this open AI scaling paper,

209
00:22:21,760 --> 00:22:25,760
and scaling laws for neural language model.

210
00:22:25,760 --> 00:22:30,760
And we have many other papers like Tintera.

211
00:22:30,760 --> 00:22:46,760
But actually, I wrote, I did a project in 2019 where I sort of formed all these nice scaling ideas by myself.

212
00:22:46,760 --> 00:22:51,760
So, and I wrote a paper called one equal piece all you need.

213
00:22:51,760 --> 00:22:54,760
Yeah, so, okay, let me talk about that.

214
00:22:54,760 --> 00:23:07,760
Then, so the, so there I had a bunch of ideas and tried to verify these ideas with experiments using very small amount of computers.

215
00:23:07,760 --> 00:23:17,760
And so first idea is that it is easy to analyze the pre-training data set so that one has to train only one or a few epochs.

216
00:23:17,760 --> 00:23:21,760
Which dramatically improves the performance compute trade-off.

217
00:23:21,760 --> 00:23:30,760
So, basically, yeah, I'm just advocating, let's just try for one epoch and use big data set.

218
00:23:30,760 --> 00:23:45,760
Yeah, and the second idea is that so let's compute the optimal ratio of model size and number of tokens for given compute budget based on training curves.

219
00:23:45,760 --> 00:23:53,760
So, back then, the models were too small and they used too many iterations.

220
00:23:53,760 --> 00:24:05,760
So let's just, you know, adjust this ratio nicely so that we don't have to waste all the compute for like hundreds of epochs.

221
00:24:05,760 --> 00:24:09,760
So is the ratio model size and data set size?

222
00:24:09,760 --> 00:24:11,760
Yeah, that's right.

223
00:24:11,760 --> 00:24:20,760
So, actually, Ginger also did measure this ratio.

224
00:24:20,760 --> 00:24:32,760
So basically, they computed the scaling exponents for optimal data set size versus optimal model size.

225
00:24:32,760 --> 00:24:40,760
And then they found that the scaling exponents for this to both 0.5.

226
00:24:40,760 --> 00:24:44,760
So that means they both linearly increase.

227
00:24:44,760 --> 00:24:53,760
So you can just measure the slope of this line, which gives you that optimal ratio.

228
00:24:53,760 --> 00:25:00,760
Yeah, so basically, you would need to scale your data set size and model size the same amount.

229
00:25:00,760 --> 00:25:09,760
So if you want to, you know, build GPT-4, you might just want to double the number of parameters and double the data set size.

230
00:25:09,760 --> 00:25:11,760
Yeah, exactly.

231
00:25:11,760 --> 00:25:18,760
And so, yeah, right now, so now we're not on 2019-2020 anymore, we're in 2022.

232
00:25:18,760 --> 00:25:28,760
And I believe so you're working at Iluth AI and Google as an intern on some scaling work.

233
00:25:28,760 --> 00:25:33,760
And I believe you might not want to talk about your private work on the podcast.

234
00:25:33,760 --> 00:25:38,760
But yeah, what kind of work do you do publicly on scaling you won't be happy to talk about?

235
00:25:38,760 --> 00:25:40,760
Oh, yeah, definitely.

236
00:25:40,760 --> 00:25:43,760
So, okay, let me think.

237
00:25:43,760 --> 00:25:50,760
So I'm currently very interested in Instruct GPT and T0.

238
00:25:50,760 --> 00:25:52,760
So both of these.

239
00:25:52,760 --> 00:25:56,760
Could you maybe summarize what's T0 for people who were not read the paper?

240
00:25:56,760 --> 00:25:57,760
Yeah.

241
00:25:57,760 --> 00:26:04,760
So T0 is basically a masked language model with multi-task point joining.

242
00:26:04,760 --> 00:26:16,760
So first of all, masked language model is an easily encoder on the encoder-decoder model that is trying with a masked language model objective.

243
00:26:16,760 --> 00:26:25,760
So this training is basically, so let's say you have some text, then you can randomly mask.

244
00:26:26,760 --> 00:26:31,760
As some random spans of tokens.

245
00:26:31,760 --> 00:26:38,760
So they, then you want your model to predict these masked tokens.

246
00:26:38,760 --> 00:26:39,760
Yeah.

247
00:26:39,760 --> 00:26:45,760
That's basically a masked language model or also called denoised audio encoding.

248
00:26:45,760 --> 00:26:46,760
Yeah.

249
00:26:46,760 --> 00:26:49,760
So that's what masked language model is about.

250
00:26:49,760 --> 00:26:54,760
And T0 is a masked language model.

251
00:26:54,760 --> 00:27:06,760
That is also fine tune from a bunch of many, many data sets, like maybe like Super Glue.

252
00:27:06,760 --> 00:27:10,760
So what's Super Glue?

253
00:27:10,760 --> 00:27:15,760
Super Glue is a standard, natural language understanding data set.

254
00:27:15,760 --> 00:27:16,760
Yeah.

255
00:27:16,760 --> 00:27:21,760
Maybe it has not trained on, maybe they did not use Super Glue for T0.

256
00:27:21,760 --> 00:27:23,760
But basically, that's the idea.

257
00:27:23,760 --> 00:27:36,760
So the reason why we want to fine tune T0 on a bunch of these data sets is that if you train like this, then it generalises,

258
00:27:36,760 --> 00:27:42,760
obviously it performs very well on the task it was fine tuned on.

259
00:27:42,760 --> 00:27:49,760
But it also performs very well on the tasks it was not fine tuned on.

260
00:27:49,760 --> 00:28:05,760
So we know that GPT like models performs much worse than the GPT like model that is fine tuned on the task you are trying to deal with.

261
00:28:05,760 --> 00:28:06,760
Right.

262
00:28:06,760 --> 00:28:21,760
So yeah, basically this multi-task fine tuning allows your model to perform very well on the task, not only the tasks it was trained on,

263
00:28:21,760 --> 00:28:25,760
but also the tasks it was not trained on.

264
00:28:25,760 --> 00:28:33,760
So T0 actually can perform much better than GPT3 on many tasks.

265
00:28:33,760 --> 00:28:46,760
Without fine tuning the model on this task specifically, while using like 10 times less complex than GPT3.

266
00:28:46,760 --> 00:28:56,760
So is the idea that you train it on a bunch of different tasks, so not fine tuning, but you train it on a bunch of different tasks?

267
00:28:56,760 --> 00:29:00,760
I think that's what you said, multi-task training or multi-task fine tuning?

268
00:29:00,760 --> 00:29:02,760
Yeah, multi-task.

269
00:29:02,760 --> 00:29:10,760
And then it's able to generalize well on held out tasks, it doesn't seem before like zero shots?

270
00:29:10,760 --> 00:29:11,760
Exactly.

271
00:29:11,760 --> 00:29:23,760
So you said you were interested in T0 and also in Struggle GPT, I believe in Struggle GPT was a model or a training procedure from OpenAI.

272
00:29:23,760 --> 00:29:26,760
Can you tell us more about in Struggle GPT?

273
00:29:26,760 --> 00:29:27,760
Yeah.

274
00:29:27,760 --> 00:29:42,760
So in Struggle GPT is also functioned on many different tasks like T0, but it also uses human feedback.

275
00:29:42,760 --> 00:29:57,760
So like basically they train the model to score the bunch of the generated results.

276
00:29:57,760 --> 00:30:13,760
And this model tells this GPT3 to how to generate the text.

277
00:30:13,760 --> 00:30:20,760
So this process is done by using reinforcement learning called PPO.

278
00:30:20,760 --> 00:30:29,760
So this additional component improves GPT3 significant.

279
00:30:29,760 --> 00:30:48,760
Yeah, so basically even though GPT3 doesn't perform as well as PPO, given the amount of compute it consumes, it performs very, very well on many different tasks.

280
00:30:48,760 --> 00:30:57,760
So without having to, yeah, I think it performs very well even without using future samples.

281
00:30:57,760 --> 00:31:08,760
So yeah, GPT3, sorry, Instruct GPT and T0 are some of the most compute efficient models out there.

282
00:31:08,760 --> 00:31:11,760
So yeah, I'm very interested in these models.

283
00:31:11,760 --> 00:31:20,760
And my project is basically trying to combine all these with scaling.

284
00:31:20,760 --> 00:31:34,760
Right, so you kind of want to combine this URL from human feedback procedure from Instruct GPT with the pre-training from T0.

285
00:31:34,760 --> 00:31:43,760
Yeah, and I'm thinking that we can do some interesting scaling analysis on this model for several reasons.

286
00:31:43,760 --> 00:31:48,760
First of all, optimal scaling we do for GPT-like models.

287
00:31:48,760 --> 00:32:01,760
We usually try to optimize the test application, and this model only has only decoded, unlike T0, which has encoded and decoded.

288
00:32:01,760 --> 00:32:12,760
By the way, encoded decoded model performs much better than decoded on the model when it is fine-tuned or multi-task fine-tuned.

289
00:32:12,760 --> 00:32:16,760
And that's why I'm thinking of this encoded decoded model.

290
00:32:16,760 --> 00:32:26,760
And for people who are not into deep learning or NLP, so can you just give an example of decoder and encoder decoder?

291
00:32:26,760 --> 00:32:35,760
So I think GPT2 is an decoder because it gets a prompt and then just generates a paragraph.

292
00:32:35,760 --> 00:32:37,760
What's an encoder and decoder?

293
00:32:37,760 --> 00:33:01,760
Yeah, so encoder decoder is basically encoder is like the model architecture used for Word, or like the first transformer paper architecture, and decoder is our usual decoder.

294
00:33:01,760 --> 00:33:12,760
So basically you want to feed your prompt into encoder, and then you would feed the output to the decoder with self-attention.

295
00:33:12,760 --> 00:33:20,760
That's what encoder decoder is, and it happens to perform very well in this situation.

296
00:33:21,760 --> 00:33:30,760
Yeah, so basically we have encoder decoder and fine-tuning.

297
00:33:30,760 --> 00:33:40,760
Yeah, so I think these elements make the scaling load very different from the local GPT models.

298
00:33:40,760 --> 00:33:48,760
Oh yeah, and also we want to optimize for downstreaming performance instead of test public HD.

299
00:33:48,760 --> 00:34:16,760
So I think these conditions force the model to be bigger and train shorter on free-tuning tasks because fine-tuning is so important that maybe we can make the model bigger while focusing less on the free-tuning.

300
00:34:16,760 --> 00:34:34,760
And I think this is, so the current state-of-the-art model has like 100 billion amateurs and trains on trillions of tokens, which is very different from how human planes run.

301
00:34:34,760 --> 00:34:47,760
Because our plane has like hundreds of billions of neurons, meaning like hundreds of trillions of synapses.

302
00:34:47,760 --> 00:35:05,760
So yeah, I think that means it has more capacity than the former capacity than models do, and it is only trying on like a few billions of tokens because that's how many tokens we can process within our lifetime.

303
00:35:05,760 --> 00:35:14,760
So yeah, basically I'm trying to make the models closer to how human planes learn.

304
00:35:15,760 --> 00:35:26,760
So you're trying to write a scaling law that would be closer to the amount of data humans process throughout their lifetime?

305
00:35:27,760 --> 00:35:38,760
Yeah, and I'm not like trying to make this forcibly similar to human planes constraints.

306
00:35:38,760 --> 00:35:58,760
I'm just thinking that this new model based on this G0 instructivity, I think it will perform the best if we try and like how human planes learn.

307
00:35:59,760 --> 00:36:17,760
I'm not sure I understand the methods to have this scaling law, so would you need to, you know, train T0 with like some kind of instructivity fine-tuning,

308
00:36:17,760 --> 00:36:34,760
and then you test on a bunch of held out tasks, and then you would see like you would plot a curve of like optimal scaling with respect to those like done streaming tasks?

309
00:36:34,760 --> 00:36:53,760
So basically what I'm saying is to train a bunch of different T0 with instructivity fine-tuning with different number of tokens pre-trying and different models.

310
00:36:54,760 --> 00:37:12,760
I think I understood now, so you're trying to, the same as Ginchilla, like get the exponents for dataset size and for model size, and then you're trying to see if it's not like 0.5 and 0.5, but like maybe something else?

311
00:37:12,760 --> 00:37:14,760
Yeah, something like that.

312
00:37:14,760 --> 00:37:37,760
I think your most well-known for your scaling work at Eleuther AI, where you train, was it 6 billion parameters, or at least like you try to reproduce the results from GP3, 6.7 billion, and what was called GPTJ, I think for GPTJacks.

313
00:37:38,760 --> 00:37:49,760
Yeah, can you just like give us a rough summary of this project, why you started this project, and what do you want to release it to the public?

314
00:37:49,760 --> 00:38:00,760
Yeah, so around the beginning of 2020, I was trying to reproduce Dali 1 with some of the people in Eleuther AI.

315
00:38:00,760 --> 00:38:13,760
So basically, Dali 1 consists of BQBAE encoder decoder and transformer language model for generating the discrete 11 variables.

316
00:38:13,760 --> 00:38:24,760
So that way, this transformer language model is exactly, almost exactly the same as GPT3 except for the size.

317
00:38:24,760 --> 00:38:37,760
So I thought we may be able to make the maximum impact if we reuse this model for our GPT3 production.

318
00:38:37,760 --> 00:38:46,760
Yeah, and then at the same time, I thought, so Jaxx was becoming more and more popular.

319
00:38:46,760 --> 00:39:04,760
Obviously, Jaxx is optimized for TB use, and we had a lot of TB use back then, and Jaxx is, so before that, we had our GPT3 replication, which is called GPT Neo.

320
00:39:04,760 --> 00:39:22,760
It was implemented using mesh tensorflow, and this mesh tensorflow decoding speed is so slow, almost ridiculously slow, especially compared with Hytotes.

321
00:39:22,760 --> 00:39:28,760
But Jaxx has no problem with that, it's almost as fast as Hytotes.

322
00:39:28,760 --> 00:39:33,760
So we decided to use Jaxx for this project.

323
00:39:33,760 --> 00:39:57,760
Yeah, so basically, I was supposed to work on the encoder decoder code, and I asked another guy in Eleuther AI, his name is Ben1, and I asked him to work on this language model site.

324
00:39:57,760 --> 00:40:03,760
But admittedly, he spent far more time on this project than I did.

325
00:40:03,760 --> 00:40:13,760
How much time would you say took you, maybe like six months between the beginning of 2021 and when you guys released the model?

326
00:40:13,760 --> 00:40:34,760
I think it only took several months, so this project is kind of impressive because there are only two people in it, and we only spent like three months, and we basically open sourced the best language model.

327
00:40:34,760 --> 00:40:39,760
So that is something I'm proud of.

328
00:40:39,760 --> 00:40:42,760
You're right to be proud of this, it's pretty cool.

329
00:40:42,760 --> 00:40:44,760
Yeah, thank you so much.

330
00:40:44,760 --> 00:40:51,760
So, by the way, GPTJ is different.

331
00:40:51,760 --> 00:41:01,760
Yeah, so basically GPT Neo actually could not really match the same performance with the GPT3 with similar size.

332
00:41:01,760 --> 00:41:14,760
GPT Neo was like 2.7 billion model, but it performed worse than 1 billion GPT3, for example.

333
00:41:14,760 --> 00:41:29,760
But GPTJ performs pretty much as well as 6 billion GPT3, so yeah, the model performs very well, I think.

334
00:41:29,760 --> 00:41:38,760
So it was able to be more efficient than GPT Neo, which I believe is another model from Eleuther AI, but using TensorFlow Mesh, so a bit older, right?

335
00:41:38,760 --> 00:41:40,760
Yeah.

336
00:41:40,760 --> 00:41:55,760
And yeah, so to match that performance from GPT3, did you just like took the same API parameters and architecture from GPT3 paper, or did you like had to change stuff to, you know, match the performance?

337
00:41:55,760 --> 00:42:05,760
Yeah, so I think we could be using the exact same architecture with GPT3, but we just wanted to do a bit more.

338
00:42:05,760 --> 00:42:15,760
So first of all, yeah, one thing we tried is leisure with the depth ratio.

339
00:42:15,760 --> 00:42:22,760
So basically with, I'm referring to the hidden dimensions of the model.

340
00:42:22,760 --> 00:42:31,760
So we are trying to build a wider model rather than deeper model.

341
00:42:31,760 --> 00:42:45,760
And this is important because generally speaking, wider models can utilize accelerators more efficiently, and latency is much better.

342
00:42:45,760 --> 00:42:56,760
So yeah, and when we tried this wider model, we observed that we don't really lose much of performance from this.

343
00:42:56,760 --> 00:42:59,760
So I think with this is worth it.

344
00:42:59,760 --> 00:43:10,760
And another thing we tried is placing feedforward layer with attention layer in parallel.

345
00:43:10,760 --> 00:43:12,760
Yeah.

346
00:43:12,760 --> 00:43:23,760
Basically, this is, this also saves latency, and you can also make your accelerators utilize better.

347
00:43:23,760 --> 00:43:27,760
And this was actually also adapted by Paul.

348
00:43:27,760 --> 00:43:36,760
So I think this is something, I think this is a nice contribution from this project.

349
00:43:36,760 --> 00:43:44,760
So you think palm researchers read GPTJ and thought like, oh yeah, this architecture change is very good. We're going to use it.

350
00:43:44,760 --> 00:43:53,760
Yeah, exactly. I think they also sort of followed our wide model because their model is also very, very wide.

351
00:43:53,760 --> 00:43:58,760
I think it's even wider than ours.

352
00:43:58,760 --> 00:44:06,760
I read the blog post wrote about GPTJ. And so I kind of read about like all those tricks you did.

353
00:44:06,760 --> 00:44:09,760
And you talk a lot about throughput.

354
00:44:09,760 --> 00:44:17,760
So yeah, I'm curious like what's the throughput for people who are not like scaling models all the time.

355
00:44:17,760 --> 00:44:27,760
And yeah, how does compare, you know, how does GPTJ come compared to like GPT3 or GPTNEO in terms of throughput?

356
00:44:27,760 --> 00:44:32,760
Is it more efficient, less efficient, more throughput, less throughput?

357
00:44:32,760 --> 00:44:39,760
Yeah, so throughput, I mean the number of tokens processed parameter per second.

358
00:44:39,760 --> 00:44:50,760
So if you can prove this throughput, then we can try with a larger model or more tokens with the same amount of throughput.

359
00:44:50,760 --> 00:44:54,760
So this way you can improve performance.

360
00:44:54,760 --> 00:45:00,760
So it's per, so it's amount of token processed per parameter and per second?

361
00:45:00,760 --> 00:45:01,760
Yeah.

362
00:45:01,760 --> 00:45:12,760
If you have more, a lot of parameters, you will, I don't know, does the model size then change your throughput then?

363
00:45:12,760 --> 00:45:19,760
Yeah, I guess you're only comparing like size models of the same size of GPT3 and GPTJ, so it's fine.

364
00:45:19,760 --> 00:45:28,760
So that typically throughput is defined something like two lobs per second.

365
00:45:28,760 --> 00:45:44,760
Oh yeah, yeah, so yeah, maybe a bit confusing, but let's say you have a one billion model, require, and you have TPU, one core.

366
00:45:44,760 --> 00:45:53,760
If it spends one second per, let's say it spends one second per one token.

367
00:45:53,760 --> 00:46:11,760
Then if you have two billion model and one core TPU, then it will take 0.5 seconds because they have the same throughput and something like that.

368
00:46:11,760 --> 00:46:24,760
Yeah, so the throughput of the GPTJ six billion model for training is like 150 tokens per second.

369
00:46:24,760 --> 00:46:39,760
On the other hand, GPTNEO with 2.7 billion parameters is also 150 tokens per second, but this model is half the size of GPTJ.

370
00:46:39,760 --> 00:46:49,760
So basically, this means that we achieved twice improvement in efficiency for throughput.

371
00:46:49,760 --> 00:46:56,760
So yeah, I think this different improvement is huge.

372
00:46:56,760 --> 00:47:04,760
I think it's coming from this wider model using JAX instead of mesh TensorFlow.

373
00:47:04,760 --> 00:47:13,760
Yeah, and not to mention that GPTJ has much better downstream performance than GPTNEO.

374
00:47:13,760 --> 00:47:19,760
Yeah, how long did it take to complete like an entire training because I know it requires a lot of computes.

375
00:47:19,760 --> 00:47:28,760
Yeah, so we spent five weeks using 256 cores of TPU V3.

376
00:47:28,760 --> 00:47:39,760
Yeah, what do you do during those five weeks? Do you just look at the TensorFlow curve and the next answer boards and check it doesn't have any weird spikes?

377
00:47:39,760 --> 00:47:47,760
Yeah, basically, that's what we did. There was no back because we already solved all the bugs.

378
00:47:47,760 --> 00:47:58,760
And so basically, Ben, babysit this training for five weeks, he was complaining a bit, but he said it was not too bad.

379
00:47:58,760 --> 00:48:08,760
And then at the end, you published this model on GitHub, people are very excited and start to use it to fine tune to a bunch of different cases, right?

380
00:48:08,760 --> 00:48:14,760
Yeah, so we don't really fine tune ourselves, but many people try to fine tune.

381
00:48:14,760 --> 00:48:24,760
It appears that GPTJ is more easier to deal with than other models like GPTNEO.

382
00:48:24,760 --> 00:48:34,760
Maybe because the things we use like JAX is much easier to deal with than mesh TensorFlow.

383
00:48:34,760 --> 00:48:37,760
So yeah, it became very popular.

384
00:48:37,760 --> 00:48:42,760
Yeah, from reading the documentation in GitHub, there's a manual on how to fine tune it.

385
00:48:42,760 --> 00:48:48,760
And it seems like the overall code is easier to read as well.

386
00:48:48,760 --> 00:48:59,760
And even people on YouTube like Janine Kilsner use it for their projects like fine tuning on 4chan to generate more comments.

387
00:48:59,760 --> 00:49:04,760
Have you seen this recent YouTube video and if so, what do you think about it?

388
00:49:04,760 --> 00:49:11,760
Yeah, I actually didn't watch the video itself, but I saw the tweet and some people talking about it.

389
00:49:11,760 --> 00:49:13,760
So I know it a little bit.

390
00:49:13,760 --> 00:49:22,760
So my reaction to this entire controversy is kind of like that meme of like entering into a burning room with pizza.

391
00:49:22,760 --> 00:49:32,760
So I think his project is kind of cringy for being too attention seeking and obviously not really ethically sound,

392
00:49:32,760 --> 00:49:42,760
because you can just use this app for many bad things and agree with some of the critical reactions to his project,

393
00:49:42,760 --> 00:49:46,760
even though some of them may be a bit too exaggerated.

394
00:49:46,760 --> 00:50:00,760
But at the same time, I thought this case would lead to more attention spent on a language model that can detect outputs from other language models

395
00:50:00,760 --> 00:50:08,760
so that you can filter out language model generated submissions on many websites like Reddit.

396
00:50:09,760 --> 00:50:20,760
Yeah, like lotion or Chinese government can use this sort of process to influence social media and election results in the West.

397
00:50:20,760 --> 00:50:28,760
But language models are very good at discriminating language model outputs from human outputs.

398
00:50:28,760 --> 00:50:33,760
So I'm kind of optimistic about that, at least for short term.

399
00:50:34,760 --> 00:50:45,760
What do you think of the fact that publishing GPTJ might have accelerated AI timelines where we might have less time to make AI safe

400
00:50:45,760 --> 00:50:50,760
and align those models with human values?

401
00:50:50,760 --> 00:50:56,760
Yeah, do you think releasing GPTJ wasn't that good for humanity?

402
00:50:56,760 --> 00:51:00,760
Would you have done it before again if you had to?

403
00:51:00,760 --> 00:51:09,760
Yeah, I think releasing GPTJ was a small net positive benefit for humanity.

404
00:51:09,760 --> 00:51:26,760
And so basically, open sourcing language model, there's nobody who releases a language model that is substantially better than the previous state of the art.

405
00:51:26,760 --> 00:51:36,760
Like in my case, in our case, our model performs only slightly better than GPTJ Neo or T5.

406
00:51:36,760 --> 00:51:44,760
So I don't think there was actually like accelerating the timeline or anything.

407
00:51:45,760 --> 00:51:58,760
Yeah, so basically, I don't think there's any one particular person who can make big negative impact by releasing a big language model with the current trend.

408
00:51:58,760 --> 00:52:11,760
If there's anyone then who can make a big negative impact, it'll be someone who like fine tune the model to spread misinformation.

409
00:52:12,760 --> 00:52:17,760
Yeah, just to be clear, the GPT Neo was also from A3AI.

410
00:52:17,760 --> 00:52:29,760
So in some sense, if you remove GPT Neo and GPTJ, then you don't really have any open source implementation of the GPT3 that works on the internet.

411
00:52:29,760 --> 00:52:40,760
And so maybe like you wouldn't have like all those other companies like Microsoft or Chinese or Korean companies using this implementation or even like the pile of the public data sets to train their models.

412
00:52:40,760 --> 00:52:45,760
So in some sense, we're kind of helping the entire research community go faster on those topics.

413
00:52:45,760 --> 00:52:49,760
And yeah, so maybe there are like other open source projects that would emerge.

414
00:52:49,760 --> 00:53:00,760
But then the question is like how much, you know, releasing GPTJ in 2021 accelerates those timelines compared to the others.

415
00:53:00,760 --> 00:53:12,760
And I would say like OpenAI releasing the neural scaling laws or GPT3 or even GPT2 kind of showed that like scaling was important to get to AGI.

416
00:53:12,760 --> 00:53:17,760
So in some sense, they kind of released some secret cells and everyone started following them.

417
00:53:17,760 --> 00:53:25,760
So, you know, if they didn't publish or publish a bit later, maybe the timelines would be a bit longer.

418
00:53:25,760 --> 00:53:27,760
What do you make of that?

419
00:53:27,760 --> 00:53:35,760
Yeah, so actually there was T5, open source T5.

420
00:53:35,760 --> 00:53:38,760
That's not the same as GPT3, right?

421
00:53:38,760 --> 00:53:45,760
That's right, but you can fine tune the model like GPT410.

422
00:53:45,760 --> 00:53:50,760
So yeah, I think it was totally possible to do it.

423
00:53:50,760 --> 00:54:00,760
Before GPT410, I think there was GPT2 reproduction from several people, several different groups.

424
00:54:00,760 --> 00:54:10,760
Also, like Facebook and Google released some decoder on the model that performs well.

425
00:54:10,760 --> 00:54:25,760
I don't think it affected on research because researchers, I think all these big companies like Google, they already had much bigger language models internally.

426
00:54:25,760 --> 00:54:30,760
And people in academia, they cannot really affect.

427
00:54:30,760 --> 00:54:42,760
And I don't think they had those big language models before, but I don't think they can really contribute to this large language model research because they don't have budget.

428
00:54:42,760 --> 00:54:48,760
So I don't think my, our projects really accelerated the research timeline.

429
00:54:48,760 --> 00:54:57,760
But I think, yeah, maybe slightly accelerated the open sourcing language model timeline.

430
00:54:57,760 --> 00:55:06,760
Yeah, maybe you accelerate the open source timeline, but not the private research timeline.

431
00:55:06,760 --> 00:55:08,760
So in some sense, you bring everyone on the same level.

432
00:55:08,760 --> 00:55:16,760
So yeah, definitely our work is slightly accelerating the pace of open sourcing language models.

433
00:55:16,760 --> 00:55:23,760
For example, Facebook recently released 100 full size GPT3 model.

434
00:55:24,760 --> 00:55:36,760
Maybe that was, and yeah, maybe that was a response to our GPTJ or GPT NeoX model, which was released recently.

435
00:55:36,760 --> 00:55:39,760
It has about 20 billion model parameters.

436
00:55:39,760 --> 00:55:51,760
And I think this question of accelerating open source timelines, or at least AI research in general, is important in the context of differential progress.

437
00:55:51,760 --> 00:56:01,760
So not only accelerating AI progress, but how does the speed of AI relate to the speed of AI alignment research?

438
00:56:01,760 --> 00:56:08,760
And I'm not sure if you're very familiar with AI alignment, but in this podcast, we talk about this a lot.

439
00:56:08,760 --> 00:56:15,760
And it might be worthwhile maybe defining alignment or at least like going with another definition you're familiar with.

440
00:56:15,760 --> 00:56:20,760
So yeah, what do you understand of the concept of alignment?

441
00:56:20,760 --> 00:56:23,760
How would you define it if you had to?

442
00:56:23,760 --> 00:56:29,760
Yeah, well, as you know, at the meeting, I'm a beginner on alignment.

443
00:56:29,760 --> 00:56:40,760
So all I can say, if I understand correctly, is alignment research is the research to harness advanced AI to do what we want to do.

444
00:56:40,760 --> 00:56:48,760
And one big problem is considered is the existential risk due to advanced AI.

445
00:56:48,760 --> 00:56:50,760
Yeah, that's pretty much it.

446
00:56:50,760 --> 00:57:19,760
And I think the question is, if we have an AI that is much more than humans and doesn't really care about our values, it might end up optimizing for an objective and just change completely our planet without really caring about humans or stuff.

447
00:57:19,760 --> 00:57:40,760
So we value and I guess this problem might be considered harder or easier depending on how you define the problem or how much time you think humans will have to think about those problems.

448
00:57:40,760 --> 00:58:03,760
And one of those takes is that if you only consider alignment as like an AI problem, if we want to solve AI generally and have models that are able to generalize well, if we program them well, they will be able to like solve alignment.

449
00:58:03,760 --> 00:58:11,760
And if we build them, if we build like good models, they will be aligned by default.

450
00:58:11,760 --> 00:58:20,760
And I guess that's maybe like one of the, is that maybe one of your takes as well or something you think will happen?

451
00:58:20,760 --> 00:58:45,760
Yeah, so for short term, I think we can more or less try to be careful with training like what I'm dropping for like open AI is doing, but in long term, which is what Ethan was referring to like when AI is trying to deceive humans.

452
00:58:45,760 --> 00:58:54,760
I think that's when we can no longer like use the conventional machine learning approach to deal with.

453
00:58:54,760 --> 00:59:07,760
Because, you know, if the AI is much more intelligent than humans, then we have no way to detect whether the model is deceiving or not.

454
00:59:07,760 --> 00:59:11,760
Yeah, so that's something I would be worried about.

455
00:59:11,760 --> 00:59:19,760
I guess the question is, when you have a benchmark, how do you know if the model is not pretending to be good at the benchmark?

456
00:59:19,760 --> 00:59:22,760
Or is it like generally, generally good at the benchmark?

457
00:59:22,760 --> 00:59:31,760
So if your benchmark is truthful Q&A, is it actually truthful or is just like pretending to be truthful for the moment?

458
00:59:31,760 --> 00:59:43,760
And one of the things that Ethan was saying is that all alignment can be considered as inverse scaling problems.

459
00:59:43,760 --> 00:59:54,760
So if you make your model too big at some point, it will, you know, have bad properties.

460
00:59:54,760 --> 01:00:04,760
So if you're interested in scaling, you can see that as a scaling problem. So like a good behavior for scaling.

461
01:00:04,760 --> 01:00:08,760
Yeah, I agree.

462
01:00:08,760 --> 01:00:21,760
So in terms of benchmarks, you said that for models that can be deceptive, it can be hard to write down a good benchmark because they might be able to bypass it.

463
01:00:21,760 --> 01:00:33,760
Do you think we might reach a point where we'll be able to have good evaluations and good ways to understand if our models behave correctly or not?

464
01:00:33,760 --> 01:00:49,760
Yeah, this is going to be at some point. There's going to be no conventional machine learning benchmark that can detect AI that will be malicious or not.

465
01:00:49,760 --> 01:01:02,760
I think more generally, like benchmarks are quite tricky, especially for language models and NLP where if you just like change the beginning of a word, like if you make the first letter capitalized or not in your benchmark,

466
01:01:02,760 --> 01:01:10,760
or if you just like change the code base from one GitHub repo to another, you might get completely different results in your benchmark.

467
01:01:10,760 --> 01:01:22,760
So do you think it's possible to have good NLP benchmarks to evaluate language models in the future and especially for alignment?

468
01:01:22,760 --> 01:01:34,760
Yeah, obviously we need better benchmarks for NLP models, even outside of the alignment problem, even for short term.

469
01:01:34,760 --> 01:01:46,760
Sometimes we use automatic methods to evaluate the quality of text, but they tend to be not very well-coordinated with human judgment.

470
01:01:46,760 --> 01:02:04,760
So I'm advocating for the use of human judgment, but even human judgment sometimes is not very useful because humans do not really understand whether the generated text is factual or not,

471
01:02:04,760 --> 01:02:14,760
because most of us is not really an expert on the field that the model is talking about.

472
01:02:14,760 --> 01:02:31,760
So yeah, and for alignment, I think for short term we can probably manage to make up some next benchmark using human evaluations.

473
01:02:31,760 --> 01:02:53,760
And if it doesn't work, then we will probably use some trained model to evaluate, because we can't understand what, like my example, humans are not good at judging the quality of models anyway.

474
01:02:54,760 --> 01:03:07,760
But at some point, I think that even training models to evaluate model is not enough, because at that point the models are so much better than humans are.

475
01:03:07,760 --> 01:03:16,760
And we don't understand the model and we cannot detect whether the model is being malicious or not.

476
01:03:16,760 --> 01:03:20,760
So in that case, yeah, there's going to be a big problem, I think.

477
01:03:21,760 --> 01:03:28,760
Yeah, I think you're right that it's hard to evaluate fully a model if you're not an expert on the domain.

478
01:03:28,760 --> 01:03:42,760
And I think one of the things that we can do is what they did in struggle GPT where they had like a bunch of laborers and people giving evaluations for how much you prefer one output compared to the others.

479
01:03:42,760 --> 01:03:54,760
And then you can roughly build a reward model that can say if an output is good or not, because you have like all those comparisons between different outputs.

480
01:03:54,760 --> 01:04:07,760
And yeah, for alignment where you're basically saying that you would want like an AI or another language model to evaluate if the other one is answering correctly or not.

481
01:04:07,760 --> 01:04:21,760
And at that point, you're kind of doing what people say is bringing a small Godzilla to check if the bigger Godzilla is doing good or not.

482
01:04:21,760 --> 01:04:27,760
And so that was like from a blog post that was published recently.

483
01:04:27,760 --> 01:04:42,760
And the problem with that is that at the moment when you're like bringing Godzilla to manage the other bigger one, there's already a problem because now you have two Godzilla's in your city.

484
01:04:43,760 --> 01:05:01,760
So that's why I think it's a tricky problem to try to align or at least check for deception in those large models is because if you want to like have another model do it, then you need to check this smaller model.

485
01:05:02,760 --> 01:05:20,760
And yeah, so yeah, what do you make of like AI's aligning AI's? Do you think us humans will be able to like, you know, build this like smaller model, it's providing the other big ones?

486
01:05:20,760 --> 01:05:35,760
Yeah, I think that that is what's going to happen for a short time. Obviously, we cannot keep doing that for long term, because in the long run, AI is going to be so much smarter than we are.

487
01:05:35,760 --> 01:05:48,760
So we can no longer make sure whether this small model is doing what we are thinking it's doing.

488
01:05:48,760 --> 01:06:10,760
Yeah, so like at the point, we need an entirely different intervention. I'm talking in really long term, so it's probably well after AGI. And at the point, my guess is that something like we have to argument our own intellectual capacity with something new or something.

489
01:06:10,760 --> 01:06:18,760
So we become the AI itself. Have you had of something like that?

490
01:06:19,760 --> 01:06:37,760
So something like merging with AI's after maybe like some kind of rail curse rail scenario where you upload yourself or maybe like connect with brain competent interfaces to AI. I think that's more like the Elon Musk neural link scenario.

491
01:06:37,760 --> 01:06:40,760
Yeah, which scenario you're talking about.

492
01:06:44,760 --> 01:06:48,760
I'm not really familiar with your new link. Maybe the last one.

493
01:06:49,760 --> 01:07:09,760
Okay. So when you were saying like long term, you were saying like after AGI, so at the beginning of the podcast, you were saying 2028 and 2038 were like lower bound and upper bound for AGI or at least like the rough guess for when it might appear. Is that still correct?

494
01:07:09,760 --> 01:07:18,760
Yeah. So by that time, I meant like human level language model rather than AGI.

495
01:07:19,760 --> 01:07:28,760
Human level language model. And yeah, how many years to go from human level language models to AGI?

496
01:07:29,760 --> 01:07:45,760
I think it's going to take very little amount of time given that this human level language model can do machine learning research well so it can improve itself very rapidly.

497
01:07:46,760 --> 01:07:55,760
Okay. So when you reach human level language models, they need to do research and then they achieve AGI. Is that your main timeline?

498
01:07:55,760 --> 01:07:56,760
Yeah.

499
01:07:56,760 --> 01:08:01,760
And so this might happen in days or weeks?

500
01:08:02,760 --> 01:08:04,760
Maybe several months.

501
01:08:04,760 --> 01:08:06,760
Several months, okay.

502
01:08:07,760 --> 01:08:31,760
Yeah. What would be like the bottleneck at that moment? Like they just like keep improving and but then they need to like figure out how to convince humans to help them build more hardware or would they, I don't know, build the robots themselves?

503
01:08:32,760 --> 01:08:33,760
Yeah. So how would it happen?

504
01:08:33,760 --> 01:08:36,760
Hardware is going to be the bottleneck, I think.

505
01:08:39,760 --> 01:08:46,760
Yeah. Also collaborating with humans, yeah, convincing. And that's going to be, yeah, the main bottleneck.

506
01:08:46,760 --> 01:09:08,760
Okay. So if you're listening to this podcast in 2028 and you have language models convincing you to build more, you know, GPU centers and let them, you know, make post requests all over the web.

507
01:09:08,760 --> 01:09:19,760
Please don't. It's a bad idea. They're trying to do more ML research. So you've heard it first here.

508
01:09:21,760 --> 01:09:31,760
Yeah. So when we were talking about like connecting to neural networks and have our brains maybe like become AI or merge with AI.

509
01:09:32,760 --> 01:09:37,760
We're talking about something closer to brain computer interfaces as well.

510
01:09:38,760 --> 01:09:39,760
Yeah.

511
01:09:41,760 --> 01:09:52,760
So then the question becomes like, well, kind of this timeline of the human slash human plus AI intelligence.

512
01:09:53,760 --> 01:10:06,760
Like compared to like just pure AI intelligence. So will the humans be able to keep up a bit and, you know, understand what's going on or will they be left behind?

513
01:10:07,760 --> 01:10:08,760
What do you think?

514
01:10:08,760 --> 01:10:22,760
Yeah. So I think, yeah, we should control the pace of employment in a way so that we can catch up with the pure natural, pure AI.

515
01:10:23,760 --> 01:10:32,760
So, yeah, we should absolutely make sure that we don't, we can keep the same pace, I think.

516
01:10:33,760 --> 01:10:41,760
The problem is that we don't really control that as there are not that many brain computer interface researchers, there's not a lot of funding in there.

517
01:10:42,760 --> 01:10:44,760
And the rate of progress in AI is accelerating a lot.

518
01:10:45,760 --> 01:10:58,760
And if the problem is in, you know, neuroscience are much harder than, you know, scaling models, then even if we throw like a lot of money at it, it's going to be very hard to solve.

519
01:10:59,760 --> 01:11:11,760
Yeah, that's true. But so my guess is that AGI is not going to be so, like, malicious for short land.

520
01:11:12,760 --> 01:11:18,760
So I'm thinking of using AGI to improve this kind of research for the beginning.

521
01:11:19,760 --> 01:11:21,760
At the beginning, what do you think?

522
01:11:22,760 --> 01:11:34,760
So basically, you use a very smart language model, some kind of Oracle where you ask questions and gives like very good answers with like documentation and papers.

523
01:11:35,760 --> 01:11:38,760
And you ask him like, hey, how do I build a good brain computer interface?

524
01:11:39,760 --> 01:11:42,760
And it gives you good answers. And then you go on and you start building that.

525
01:11:43,760 --> 01:11:50,760
The problem is that at the moment when you have this kind of model, you can also ask it like, hey, how do I build a better copilot?

526
01:11:51,760 --> 01:11:53,760
How do I build a bigger language model?

527
01:11:54,760 --> 01:11:56,760
And I guess people might want to do this first.

528
01:11:57,760 --> 01:12:15,760
And you might also need to develop new robotics or actual hard tech microscope and neuroscience tools to do those.

529
01:12:16,760 --> 01:12:20,760
And it's not going to be like only like a neural network architecture.

530
01:12:21,760 --> 01:12:33,760
So I guess it's always like the hardware part is going to take longer than just like software or neural networks part.

531
01:12:34,760 --> 01:12:46,760
Yeah, that's true. We may have to regulate the behavior of AGI so that we can keep the pace together.

532
01:12:47,760 --> 01:12:49,760
How do you regulate that?

533
01:12:50,760 --> 01:13:04,760
We need some physical access to AGI first, the AI, like physically preventing the models being scaled up too fast.

534
01:13:05,760 --> 01:13:18,760
So you basically have an AI become the president and then implement some security measures to prevent people from building bigger AIs?

535
01:13:19,760 --> 01:13:20,760
Yeah.

536
01:13:21,760 --> 01:13:23,760
I'm not sure people would agree that.

537
01:13:24,760 --> 01:13:32,760
Yeah, that's one of the scenarios. I guess there are different like bad or like nuanced scenarios that could happen.

538
01:13:32,760 --> 01:13:44,760
One is like authoritarian good somehow where you have like one AI governing and then we do what we need to do to have a safe situation.

539
01:13:45,760 --> 01:13:53,760
The bad situation is when the AI does whatever it wants and doesn't really care about humans, like a rogue AI self-improving.

540
01:13:54,760 --> 01:14:00,760
And then there's another situation where humans are kind of free to do whatever they want.

541
01:14:01,760 --> 01:14:08,760
And then things can be good or bad, but it will depend on like kind of how the market behaves.

542
01:14:09,760 --> 01:14:15,760
But yeah, a lot of people have been talking about kind of the AI is a dictator scenario.

543
01:14:16,760 --> 01:14:23,760
Which scenario do you find more likely or more, you know, you're more like optimistic about?

544
01:14:23,760 --> 01:14:33,760
Like imagine you're in this like post-AGI world or human level language model world.

545
01:14:34,760 --> 01:14:40,760
What do you kind of imagine the world will look like? Would humans still live like 80 years?

546
01:14:41,760 --> 01:14:46,760
Would there be like a lot of crime? Would that be solved with like some kind of dictator AI?

547
01:14:47,760 --> 01:14:53,760
I think we're going to try to prevent dictator AI from happening.

548
01:14:57,760 --> 01:15:01,760
We probably just try to maintain the current democracy.

549
01:15:02,760 --> 01:15:07,760
Yeah, because, you know, most people fare dear AI anyway.

550
01:15:08,760 --> 01:15:12,760
I think we're going to do our best to prevent that, whether we like it or not.

551
01:15:13,760 --> 01:15:22,760
So hopefully we can like, we can do the thing we propose.

552
01:15:23,760 --> 01:15:25,760
I propose like the, I suggest it like this.

553
01:15:26,760 --> 01:15:29,760
We can improve our intellectual capacity.

554
01:15:30,760 --> 01:15:35,760
Then we can just continue our democratic process.

555
01:15:36,760 --> 01:15:38,760
Yeah, that's my hope.

556
01:15:38,760 --> 01:15:48,760
Yeah, I think that's a great note of positivity to end the podcast on.

557
01:15:49,760 --> 01:15:55,760
So then people can decide for themselves what future they think could be a good one for them.

558
01:15:56,760 --> 01:16:04,760
Yeah, it was a pleasure to have you on the show and hearing about your research on scaling

559
01:16:04,760 --> 01:16:09,760
and your thoughts on the alignment, even though you're not an expert in this.

560
01:16:10,760 --> 01:16:16,760
Do you want to quickly give your Twitter for people to follow one of the AKs?

561
01:16:17,760 --> 01:16:24,760
Yeah, so I guess you can just post my Twitter username somewhere.

562
01:16:25,760 --> 01:16:34,760
Yeah, I'm tweeting about the latest machine learning papers almost every day with some of these.

563
01:16:35,760 --> 01:16:36,760
So please follow me on Twitter.

564
01:16:37,760 --> 01:16:41,760
Thanks for listening to this podcast.

565
01:16:42,760 --> 01:16:46,760
I think your Twitter ID is something like aran, kuma, tsuzaki, or is it different?

566
01:16:47,760 --> 01:16:48,760
Oh yeah, it is.

567
01:16:49,760 --> 01:16:52,760
Cool, yeah, I'll probably post it in the bio somewhere.

568
01:16:52,760 --> 01:16:59,760
Thanks for coming on the show and I hope you keep on doing some cool research for other AI

569
01:17:00,760 --> 01:17:03,760
and hopefully not accelerating too much the AI timelines.

570
01:17:04,760 --> 01:17:07,760
Thanks, Aran, and see you maybe in the next episode.

