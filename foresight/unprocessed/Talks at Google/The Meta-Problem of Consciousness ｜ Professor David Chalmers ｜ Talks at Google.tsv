start	end	text
0	13440	Thanks for coming out. It's good to be here. As Eric said, I'm a philosopher, thinking
13440	20160	about consciousness, coming from a background in the sciences and math that always struck
20160	28440	me that the most interesting and hardest unsolved problem in the sciences was the problem
28440	34480	of consciousness. Way back, 25 years ago, when I was in grad school, it seemed to me
34480	40600	the best way to come at this from a big picture perspective was to go into philosophy and
40600	44920	think about the foundational issues that arise and thinking about consciousness from any
44920	52200	number of different angles, including the angles of neuroscience and psychology and AI.
52200	55880	In this talk, I'm going to present a slightly different perspective on the problem after
55880	63800	laying out some background, the perspective of what I call the meta problem of consciousness.
63800	68680	I always like the idea that you approach a problem by stepping one level up, taking the
68680	76760	meta perspective. I love this quote, anything you can do, I can do meta. I have no idea
76760	81040	what the origins was. I like the fact this is attributed to Rudolf Kahnap, one of my
81040	85000	favorite philosophers, but anyone who knows Kahnap's work is completely implausible. He
85000	89120	would ever say anything so frivolous. It's also been attributed to my thesis advisor,
89120	94920	Doug Hofstadter, author of Goethe Lescher Bach and a big fan of the meta perspective,
94920	101000	but he assures me he never said it either. But the meta perspective on anything is stepping
101000	108620	up a level. The meta problem, as I think about it, is called the meta problem because it's
108620	114320	a problem about a problem. A meta theory is a theory about a theory. Meta problem is
114320	118980	a problem about a problem. In particular, it's the problem of explaining why we think
118980	124120	there is a problem about consciousness. So there's a first order problem, the problem
124120	129180	of consciousness. Today I'm going to focus on a problem about it, but I'll start by introducing
129180	135200	the first order problem itself. The first order problem is what we call the hard problem
135200	142000	of consciousness. It's the problem of explaining why and how physical processes should give
142040	149040	rise to conscious experience. You've got all this neurons firing in your brain, bringing
151520	157880	about all kinds of sophisticated behavior. We can get to be done explaining our various
157880	161680	responses, but there's this big question about how it feels from the first person point of
161680	166320	view. That's the subjective experience. I like this illustration of the hard problem
166320	171960	of consciousness. It seems to show someone's hair catching fire, but I guess it's a metaphorical
172400	178400	illustration of the subjective perspective. The hard problem is concerned with what philosophers
178400	184320	call phenomenal consciousness. The word consciousness is ambiguous a thousand ways, but phenomenal
184320	190360	consciousness is what it's like to be a subject from the first person point of view. A system
190360	195880	is phenomenally conscious. If there's something it's like to be it, a mental state is phenomenally
195880	200280	conscious if there's something it's like to be in it. The thought is there are some
200320	205200	systems, so there's something it's like to be that system. There's something it's like
205200	210320	to be me. I presume there's something it's like to be you, but presumably there's nothing
210320	215960	it's like to be this lectern as far as we know. The lectern does not have a first person
215960	222960	perspective. This phrase was made famous by my colleague Tom Nagel at NYU who back in 1974
223040	230040	wrote an article called What is it like to be a bat? The general idea was what's very
230960	236200	hard to know what it's like to be a bat from the third person point of view just looking
236200	240640	at it as a human who has different kinds of experience, but presumably very plausibly
240640	245000	there is something it's like to be a bat. The bat is conscious. It's having subjective
245000	252000	experiences just of a kind very different from ours. In human subjective experience
253920	260400	consciousness divides into any number of different kinds or aspects like different tracks of
260400	266400	the inner movie of consciousness. We have visual experiences like the experience of
266400	272920	say these colors blue and red and green from the first person point of view and of depth.
272920	279920	There are sensory experiences like the experience of my voice, experiences of taste and smell,
280360	287360	their experiences of your body, feeling pain or orgasms or hunger or a tickle or something.
287840	293080	They all have some distinctive first person quality, mental images like recalled visual
293080	300080	images, emotional experiences like experience of happiness or anger and indeed we all seem
300280	303840	to have this stream of a current thought or at the very least we're kind of chattering
303840	310840	away to ourselves and reflecting and deciding. All of these are aspects of subjective experience,
311200	317560	things we experience from the first person point of view and I think these subjective
317560	324560	experiences are at least on the face of it data, data for the science of consciousness
324760	329680	to explain. These are just facts about us that we're having these subjective experiences.
329680	334600	As we ignore them, we're ignoring the data. So if you catalog the data that say the science
334600	338360	of consciousness needs to explain, there are certainly facts about our behavior and how
338360	342480	we respond in situations. There are facts about how our brain, facts about how our brain
342480	346840	is working. There are also facts about how subjective experiences and on the face of
346840	353760	it their data. And it's these data that pose what I call the hard problem of consciousness.
353840	359680	This gets contrasted with the easy problems, the so-called easy problems of consciousness
359680	366680	which are the problems of explaining behavioral and cognitive functions. Objective things
367600	372560	you can measure from the third person point of view typically tied to behavior, perceptual
372560	378440	discrimination say of a stimulus. I can discriminate two different things in my environment. I
378440	383040	can say that's red and that's green. I can integrate the information say about the color
383040	387040	and the shape. I can use it to control my behavior, walk towards the red one rather
387040	394520	than the green one. I can report it, say that's red and so on. Those are all data too for
394520	400040	science to explain. But we've got a bead on how to explain those. They don't seem to
400040	407040	pose as big a problem. Why? We explain those easy problems by finding a mechanism, typically
408040	415040	a neural or computational mechanism that performs the relevant function to explain
415440	420440	how it is that I get to say there's a red thing over there or walk towards it. Will
420440	427440	you find the mechanisms involving perceptual processes and action processes in my brain
427800	432560	that leads to that behavior? Find the right mechanism that performs the function. You've
432560	438680	explained what needs to be explained with the easy problems of consciousness. But for
438680	443800	the hard problem, for subjective experience, it's just not clear that this standard method
443800	450800	works. It looks like explaining all that behavior still leaves open a further question. Why
451040	458040	does all that give you subjective experience? Explain the reacting, the responding, the
458800	465800	controlling, the reporting and so on. It still leaves open the question why is all that accompanied
466040	472680	by subjective experience? Why doesn't it go on in the dark without consciousness, so
472680	477600	to speak? There seems to be what the philosopher Joe Levine has called a gap here, an explanatory
477600	482600	gap between physical processes and subjective experience, at least our standard kinds of
482600	487840	explanation which work really well for the easy problems of behavior and so on. Don't
487880	494880	obviously give you a connection to the subjective aspects of experience. There's been a vast
495880	500760	amount of discussion of these things over, well for centuries really, but it's been particularly
500760	507760	active in recent decades, philosophers, scientists, all kinds of different views. Philosophically
508080	514600	you can divide approaches to the hard problem into at least two classes. One is an approach
514680	518920	on which consciousness is taken to be somehow irreducible and primitive. We can't explain
518920	524240	it in more basic physical terms, so take it as a kind of primitive and that might lead
524240	528080	to dualist theories of consciousness where consciousness is somehow separate from and
528080	533560	interacts with the brain. Recently very popular has been the class of panpsychist theories
533560	538880	of consciousness. I know Galen Strossom is here a while back talking, he very much favors
538880	543000	panpsychist theories where consciousness is something basic in the universe underlying
543040	549040	matter and indeed there are idealist theories where consciousness underlies the whole universe,
549040	555160	so these are all extremely speculative but interesting views that I've explored myself.
555160	560960	There are also reductionist theories of consciousness from functionalist approaches where consciousness
560960	567960	is just basically taken to be a giant algorithm or computation. Biological approaches to consciousness,
568000	574000	my colleague Ned Block was here I know talking about neurobiology based approaches where
574000	579440	it's not the algorithm that matters but the biology is implemented in and indeed the kind
579440	584720	of quantum approaches that people like Roger Penrose and Stuart Hemeroff have made famous.
584720	588960	I mean I think there's interesting things to say about all of these approaches. I think
588960	594400	that right now at least most of the reductionist approaches leave a gap but the non-reductionist
594440	601440	approaches have other problems in seeing how it all works. Today I'm going to take a different
601440	608440	kind of approach, this approach through the meta-problem. One way to motivate this is to
608520	614280	ask, I often get asked, okay you're a philosopher it's fine you get to think about these things
614280	620280	like the hard problem of consciousness, how can I as a scientist or an engineer or an
620280	627280	AI researcher, how can I do something to kind of contribute to help get at this hard problem
627440	631680	of consciousness? Is this just a problem for philosophy? I mean for me to work on it say
631680	637680	as an AI researcher I need something I can operationalize, something I can work with
637680	644180	and try to program and as it stands it's just not clear how to do that with the hard problem.
644180	648320	I mean if you're a neuroscientist there are some things you can do. You can say work with
648320	653520	humans and look at their brains and look for the neural correlates of consciousness, the
653520	656800	bits of the brain that go along with being conscious because at least with humans we
656800	661440	can take as a background assumption, a plausible background assumption that the system is conscious.
661440	665320	For AI we can't even do that, we don't know which AI systems we're working with or conscious,
665320	671320	we need some operational criteria. In AI we mostly work on modeling things like behavior
671320	676240	and objective functioning for consciousness, those are the easy problems. So how does someone
676300	682800	coming from this perspective make a connection to the hard problem of consciousness? Well
682800	689160	one approach is to work on certain problems among the easy problems of behavior that shed
689160	696160	particular light on the hard problem and that's going to be the approach that I look at today.
696920	702400	So the guiding, the key idea here is there are certain behavioral functions that seem
702440	709440	to have a particularly close relation to the hard problem of consciousness. In particular
709440	715080	we say things about consciousness. We make what philosophers call phenomenal reports,
715080	722080	verbal reports of conscious experiences. So I'll say things like I'm conscious, I'm
722160	728280	feeling pain right now and so on. Maybe the consciousness and the pain are subjective
728360	734360	experiences but the reports, the utterances, I am conscious, well that's a bit of behavior.
734360	740640	In principle explaining those is among the easy problems, objectively measurable response,
740640	747640	we can find a mechanism in the brain that produces it. And among our phenomenal reports
747640	753800	there's this special class we can call the problem reports, reports expressing our sense
753800	758720	that consciousness poses a problem. Now admittedly not everyone makes these reports but they
758720	763160	seem to be fairly widespread among, especially among philosophers and scientists thinking
763160	767920	about these things but furthermore it's a sense that it's fairly easy to find and a
767920	774080	very wide class of people who think about consciousness. People say things like there
774080	779920	is a problem of consciousness, a hard problem. On the face of it explaining behavior doesn't
779960	785320	explain consciousness, consciousness seems non-physical, how would you ever explain the
785320	790840	subjective experience of red and so on. It's an objective fact about us, at least about
790840	797840	some of us, that we make those reports and that's the fact about human behavior.
799440	805940	So the matter problem of consciousness then at a second approximation is roughly the problem
805940	812460	of explaining these problem reports, explaining you might say the conviction that we're conscious
812460	817220	and that consciousness is puzzling. And what's nice about this is that although the hard
817220	820820	problem is this, you know, airy-fairy problem about subjective experience that's hard to
820820	827020	pin down, this is a puzzle ultimately about behavior. So this is an easy problem, one
827020	831500	that ought to be open to those standard methods of explanation in the cognitive and brain
832460	839460	sciences. So there's a research program. There's a research program here. So I like
839460	842140	to think of the matter problem as something that can play that role. I talked about earlier
842140	846060	if you say an AI researcher thinking about this. The matter problem is an easy problem,
846060	850540	a problem about behavior that's closely tied to the hard problem. So it's something we
850540	854740	might be able to make some progress on using standard methods of thinking about algorithms
854740	859940	and computations or thinking about brain processes and behavior while still shedding
859940	863980	some light, at least indirectly on the hard problem. It's more tractable than the hard
863980	868220	problem but solving it ought to shed light on the hard problem. And today I'm just going
868220	872500	to kind of lay out the research program and talk about some ways in which it might potentially
872500	878820	shed some light. This is interesting to a philosopher because it looks like an instance
878820	882940	of what people sometimes call genealogical analysis. It goes back to Friedrich Nietzsche
882940	887780	on the genealogy of morals. Instead of thinking about what's good or bad, let's look at where
887780	894660	our sense of good or bad came from, the genealogy of it all in evolution or in culture or in
894660	900340	religion. And people take a genealogical approach to God. Instead of thinking about does God
900340	904860	exist or not, let's look at where our belief in God came from. Maybe there's some evolutionary
904860	910780	reason for why people believe in God. This often leads, not always, but often leads to
910780	916500	a kind of debunking of our beliefs about those domains. Explain why we believe in God in
916500	922500	evolutionary terms. No need for the God hypothesis anymore. Explain our moral beliefs in say
922500	928140	evolutionary terms. Maybe no need to take morality quite so seriously. So some people
928140	933020	at least are inclined to take an approach like this with consciousness too. If you think
933020	937660	about the meta problem, explaining our beliefs about consciousness, that might ultimately
937660	945220	debunk our beliefs about consciousness. This leads to a philosophical view which has recently
945260	951340	attracted a lot of interest, a philosophical view called illusionism, which is the view
951340	957780	that consciousness itself is an illusion or maybe that the problem of consciousness is
957780	965140	an illusion. Explain the illusion and we dissolve the problem. In terms of the meta
965140	970980	problem that view roughly comes to, solve the meta problem, it will dissolve the hard
971020	975420	problem. Explain why it is that we say all these things about consciousness. While we
975420	980940	say I am conscious, while we say consciousness is puzzling, if you can explain all that in
980940	987100	say algorithmic terms, then you'll remove the underlying problem because you'll have
987100	991500	explained why we're puzzled in the first place. Actually, walking over here today, I noticed
991500	996140	that just a couple of blocks away, we have the Museum of Illusions. So I'm going to check
996140	1000900	that out later on. But if illusionism is right and added to all those perceptual illusions,
1001260	1006220	it's going to be the problem of consciousness itself. It's roughly an illusion thrown up
1006220	1011700	by having a weird self-model with a certain kind of algorithm that attributes to ourselves
1011700	1018700	special properties that we don't have. So one line on the meta problem is the illusionist
1018700	1023820	line. Solve the meta problem, you'll get to treat consciousness as an illusion. That's
1023820	1029540	actually a view that has many antecedents in the history of philosophy one way or another.
1029580	1033740	Emmanuel Kahn, in his great critique of pure reason, had a section where he talked about
1033740	1041020	the self or the soul as a transcendental illusion. We seem to have this indivisible soul, but
1041020	1045740	that's a kind of illusion thrown up by our cognitive processes. The Australian philosophers
1045740	1051860	are on place, and David Armstrong had versions of this that I might touch on a bit later.
1051860	1058620	Daniel Dennett, leading reductionist thinker about consciousness, has been pushing for the
1058660	1063940	last couple of decades the idea that consciousness involves a certain kind of user illusion,
1063940	1070140	and most recently the British philosopher Keith Frankish has been really pushing illusionism
1070140	1077140	as a theory of consciousness. Here's a book centering around a paper by Keith Frankish
1078100	1083580	on illusionism as a theory of consciousness that I recommend to you. So one way to go
1083580	1087580	with the meta problem is the direction of illusionism, but one nice thing about many
1087620	1092340	people find illusionism completely unbelievable. They find you, how could it be that consciousness
1092340	1096900	is an illusion? Look, we just have the subjective experience, it's a datum about our nature,
1096900	1102460	and I confess I've got some sympathy with that reaction, so I'm not an illusionist myself.
1102460	1106780	I'm a realist about consciousness in the philosopher's sense where a realist about
1106780	1112260	something is someone who believes that thing is real. I think consciousness is real, I
1112260	1117260	think it's not an illusion, I think that solving the meta problem does not dissolve
1117260	1120540	the hard problem, but the nice thing about the meta problem is you can proceed on it
1120540	1126740	to some extent at least in initial neutrality on that question, is consciousness real or
1126740	1132620	is it an illusion? It's a basic problem about our objective functioning of these reports.
1132620	1139620	What explains those? There's a neutral research program here that both realists, illusionists,
1140020	1143200	people of all kinds of different views of consciousness can explain, and then we can
1143200	1149320	come back and look at the philosophical consequences. Well, I'm not an illusionist, I think consciousness
1149320	1154440	is real. I've got to say, I do feel the temptation of illusionism, I find it really intriguing
1154440	1160480	and in some ways attractive to you, just fundamentally unbelievable. Nevertheless, I think that the
1160480	1166960	meta problem should be a tractable problem. Solving it will shed, at the very least, will
1166960	1171720	shed much light on the hard problem of consciousness, even if it doesn't solve it. If you can explain
1171760	1176480	our conviction that we're conscious, somehow the source, the roots of our conviction that
1176480	1181280	we're conscious must have something to do with consciousness, especially if consciousness
1181280	1187720	is real. I think it's very much a good research program for people to explain. Then I'll move
1187720	1193880	on now to just outlining the research program a little bit more and then talk a bit about
1193880	1199120	potential solutions and on impact on theories of consciousness before wrapping up with just
1199200	1206200	a little bit more about illusionism. This meta problem, which I've been pushing recently,
1206920	1212920	opens up a tractable empirical research program for everyone, reductionists, non-reductionists,
1212920	1218420	illusionists, non-illusionists. We can try to solve it and then think about the philosophical
1218420	1225420	consequences. What is the meta problem? Well, the way I'm going to put it is it's the problem
1226420	1233420	of topic neutrally explaining problem intuitions or else explaining why that can't be done.
1234420	1241420	I'll unpack all the pieces of that right now, first starting with problem intuitions. What
1241820	1247260	are problem intuitions? Well, those are the things we say. There are things we think.
1247260	1251780	I say consciousness seems irreducible. I might think consciousness is irreducible. People
1251780	1256700	might be disposed, have a tendency to say or think those things. Problem intuitions all
1256700	1262820	take to be roughly that tendency. We have dispositions to say and think certain things
1262820	1268580	about consciousness. What are the core problem intuitions? Well, I think they break down into
1268580	1273100	a number of different kinds. There's the intuition that consciousness is non-physical. We might
1273100	1277660	think of that as a metaphysical intuition about the nature of consciousness. There are
1277700	1283300	intuitions about explanation. Consciousness is hard to explain. Explaining behavior doesn't
1283300	1288180	explain consciousness. There are intuitions about knowledge of consciousness. Some of
1288180	1292460	you may know the famous thought experiment of Mary in the black and white room who knows
1292460	1297380	all about the objective nature of color vision and so on, but still doesn't know what it's
1297380	1301700	like to see red. She sees red for the first time. She learns something new. That's an
1301700	1306140	intuition about knowledge of consciousness. There are what philosophers call modal intuitions
1306140	1312060	about what's possible or imaginable. One famous case is the case of a zombie, a creature
1312060	1317460	who's physically identical to you and me, but not conscious, or maybe an AI system which
1317460	1322460	is functionally identical to you and me, but not conscious. That at least seems conceivable
1322460	1328660	to many people. This is the philosophical zombie, unlike the zombies in movies which
1328660	1333380	have weird behaviors and go after brains and so on. The philosophical zombie is a creature
1333500	1338900	that seems, at least behaviorally, maybe physically like a normal human, but doesn't have any
1338900	1343980	conscious experiences. All the physical states, none of the mental states. It seems to many
1343980	1348260	people that's at least conceivable. We're not zombies. I don't think anyone here is
1348260	1353420	a zombie, I hope, but nonetheless, it seems that we can make sense of the idea and one
1353420	1358300	way to pose the hard problem is why are we not zombies. This imaginability of zombies
1358300	1363220	is one of the intuitions that gets the problem going. Then you can go on and catalog more
1363540	1368300	and more intuitions about the distribution of conscious, maybe the intuition that robots
1368300	1373580	won't be conscious. That's an optional one, I think, or consciousness matters morally
1373580	1380080	in certain ways and the list goes on. I think there's an interdisciplinary research program
1380080	1386180	here of working on those intuitions about consciousness and trying to explain them.
1386180	1391460	Experimental psychology and experimental philosophy and newly active area can study people's intuitions
1391500	1396540	about consciousness. We can work on models of these things, computational models or neurobiological
1396540	1400620	models of these intuitions and reports, and indeed, I think there's a lot of room for
1400620	1405460	philosophical analysis. There's just starting to be a program of people doing these things
1405460	1412460	in all these fields. It is an empirical question how widely these intuitions are shared. You
1412460	1415740	might be sitting there thinking, come on, I don't have any of these intuitions. Maybe
1415740	1420740	this is just you. My sense is from the psychological study to date, it seems that some of these
1420740	1425940	intuitions about consciousness are at least very widely shared, at least as dispositions
1425940	1431940	or intuitions, although they're often overridden on reflection. The current data on this is
1431940	1438940	somewhat limited. There is a lot of empirical work on intuitions about the mind concerning
1438980	1443020	things like belief, like when do kids get the idea that your belief's about the world,
1443020	1448340	can be false, concerning the way your self persists through time, could you exist after
1448340	1452180	the death of your body? Well, consciousness is concerned. There's work on the distribution
1452180	1455860	of consciousness. Could a robot be conscious? Could a group be conscious? Here's a book
1455860	1459980	by Paul Bloom, Descartes' Baby. The catalog's a lot of this interesting work, making the
1459980	1466340	case that many children are intuitive dualists. They're naturally inclined to think there's
1466340	1470980	something non-physical about the mind. So far, most of this work has not been so much
1470980	1477220	on these core problem intuitions about consciousness, but there's work developing in this direction.
1477260	1482060	Sarah Gottlieb and Tanya Lombrozo have a very recent article called Can Science Explain
1482060	1488340	the Human Mind on People's Judgements about when various mental phenomena are hard to
1488340	1492540	explain and they seem to find that, yes, subjective experience and things which have, to which
1492540	1498420	people have privileged first person access seem to pose the problem big time. So there's
1498420	1503660	the beginning of a research program here. I think there's room for a lot more.
1503660	1508700	The topic neutrality part, when I say we're looking for a topic neutral explanation of
1508700	1513740	problem intuitions, that's roughly to say an explanation that doesn't mention consciousness
1513740	1518700	itself. It's put in neutral terms. It's neutral on the existence of consciousness. The most
1518700	1523700	obvious one would be something like an algorithmic explanation. I can say, here is the algorithm.
1523700	1529260	The brain is executing. It generates our conviction that we're conscious and our reports about
1529300	1533620	consciousness. There may be some time between that algorithm and consciousness, but to specify
1533620	1539820	the algorithm, you don't need to make claims about consciousness. So the algorithmic version
1539820	1545340	of the metaproblem is roughly find the algorithm that generates our problem intuition. So that's
1545340	1554340	I think a principal research program that maybe AI researchers in combination would say psychologists,
1554900	1560500	the psychologist could help isolate data about the way that the human beings are doing it,
1560500	1564820	how these things are generated in humans and AI researchers can try and see about implementing
1564820	1569380	that algorithm in machines and see what results. And I'll talk about a little bit of research
1569380	1575340	in this direction in just a moment. But okay, now I want to say something about potential
1575340	1580100	solutions to the problem. Like I say, this is a big research program. I don't claim to
1580100	1584300	have the solution to the metaproblem. I've got some ideas, but I'm not going to try and
1584300	1590700	lay out a major solution or just so here are a few things which I think might be parts
1590700	1595780	of a solution to the problem, many of which have got antecedents here and there in scientific
1595780	1601220	and philosophical discussion. Some promising ideas include retrospective models, phenomenal
1601220	1606140	concepts, introspective opacity, the sense of acquaintance. Let me just say something
1606140	1612300	about a few of these. One starting idea that almost anyone's going to have here is somehow
1612300	1619100	models of ourselves are playing a central role here. Human beings have models of the
1619100	1626020	world, naive physics and naive psychology, models of other people and so on. We also
1626020	1630660	have models of ourselves. It makes sense for us to have models of ourselves and our own
1630660	1635660	mental processes. This is something that the psychologist Michael Graziano has written
1635660	1641620	a lot on. We have internal models of our own cognitive processes, including those tied
1641660	1647660	to consciousness. Somehow, something about our introspective models explains our sense,
1647660	1652980	A, that we are conscious and B, that this is distinctively problematic. I think anyone
1652980	1658180	thinking about the metaproblem, this has got to be at least the first step. We have these
1658180	1662260	introspective models. If you're an illusionist, they'll be false models. If you're a realist,
1662260	1668340	they needn't be false models, but at the very least, these introspective models are involved,
1668380	1673860	which is fine, but the devil's in the details. How do they work to generate this problem?
1673860	1678780	A number of philosophers have argued we have special concepts of consciousness, introspective
1678780	1684380	concepts of these special subjective states. People call these phenomenal concepts, concepts
1684380	1690460	of phenomenal consciousness. One thing that's special is these concepts are somehow independent
1690460	1697460	of our physical concepts. They explain, we've got one set of physical concepts for modeling
1697460	1701420	the external work world. We've got one set of introspective concepts for modeling our
1701420	1705220	own mind. These concepts, just by virtue of the way they're designed, are somewhat independent
1705220	1711340	of each other, and that partly explains why consciousness seems to be independent of
1711340	1716980	the physical world, intuitively. Maybe that independence of phenomenal concepts could
1716980	1721660	go some distance to explaining our problem report. I think there's got to be something
1721660	1727380	to this as well. At the same time, I don't think this goes nearly far enough because
1727860	1732460	we have concepts of many aspects of the mind, not just of the subjective experiential past,
1732460	1738180	but things we believe and things we desire. One, I believe that Paris is the capital of
1738180	1744300	France. That's part of my internal self-model, but that doesn't seem to generate the hard
1744300	1749060	problem in nearly the same way in which, say, the experience of red does. A lot more needs
1749060	1754380	to be said about what's going on in cases like having the experience of red and having
1754380	1759060	the sense that that generates a gap, so it doesn't generalize to everything about the
1759060	1765260	mind. Some people have thought that what we might call introspective opacity plays a
1765260	1771020	role, that when we introspect what's going on in our minds, we don't have access to the
1771020	1775540	underlying physical states. We don't see the neurons in our brains. We don't see that
1775540	1782300	consciousness as physical, so we see it as non-physical. Most recently, the physicist
1782340	1785740	Max Tegmark has argued in this direction, saying, somehow, consciousness is substrate
1785740	1790500	independent. We don't see the substrate, so then we think, ah, maybe it can float free
1790500	1799020	of the substrate. Armstrong made an analogy with the case of someone in a circus where
1799020	1804820	the headless person illusion where you don't see someone's there with a veil across their
1804820	1812140	head. You don't see their head, so you see them as having no head. Here is a 19th century
1812180	1816180	booth at a circus, a so-called headless woman with a veil over her head. You don't see the
1816180	1820300	head, so somehow it looks at least for a moment like the person doesn't have a head. Armstrong
1820300	1825100	says, maybe that's how it is with consciousness. You don't see that it's physical, so you
1825100	1832820	see it as non-physical. But still, the question comes up, how do we make this inference? There's
1832820	1838340	something that special goes on in cases like, say, color and taste and so on. Color experience
1838340	1844300	seems to attribute primitive properties to objects like redness, greenness and so on.
1844300	1848580	But in fact, in the external world, at the very least, they have complex, reducible
1848580	1855020	properties. Somehow, internal models of color treat colors like red and green as if they
1855020	1860580	are primitive things. It turns out to be useful to have these models of things where you treat
1860580	1864780	certain things as primitive even though they're reducible. And it sure seems that when we
1864780	1870500	experience colors, we experience greenness as a primitive quality, even though it may
1870500	1875860	be a very, very complex reducible property. That's something about our model of colors.
1875860	1881180	The philosopher Wolfgang Schwartz tried to make an analogy with sensor variables in,
1881180	1886660	say, image processing. You've got some visual sensors and a camera or something. You need
1886660	1892300	to process the image. Well, you've got some variables, some sensor variables to represent
1892300	1895620	the sensory inputs that the various sensors are getting. And you might treat them as a
1895620	1899100	primitive dimension because that's the most useful way to treat them. You don't treat
1899100	1904420	them as certain amounts of lights or photons firing. You don't need to know about that.
1904420	1909500	Use these sensor variables and treat them as a primitive dimension. And all that will
1909500	1914260	play into a model of these things as primitive. Maybe taking that idea and extending it to
1914260	1920820	introspection, you know, somehow these conscious states are somehow like sensor variables in
1920820	1927420	our model of the mind. And somehow these internal models give us the sense of being acquainted
1927420	1933180	with primitive concrete qualities and of our awareness of them. This is still just laying
1933180	1937140	out. I don't think this is still yet actually explaining a whole lot, but it's laying out.
1937140	1942220	It's narrowing down what it is that we need to explain to solve the meta problem. But
1942220	1945820	just to put the pieces together, here's a little summary. One thing I like about this
1945820	1952960	summary is you can read it in either an illusionist tone of voice as an account of the illusion
1952960	1958220	of consciousness. All this is how our false introspective models work or in a realist
1958220	1965540	tone of voice as an account of our true, correct models of consciousness. But we can set it
1965540	1969140	out in a way which is neutral on the two and then try and figure out later whether these
1969140	1975220	models are correct as the realist says or incorrect as the illusionist says. We have
1975220	1982620	introspective models deploying introspective concepts of our internal states that are largely
1982620	1989180	independent of our physical concepts. These concepts are introspectively opaque, not revealing
1989180	1995540	any of the underlying mechanisms, our perceptual models perceptually attribute primitive perceptual
1995540	2002700	qualities to the world, and our introspective models attribute primitive mental relations
2002780	2009980	to those qualities. These models produce the sense of acquaintance both with those qualities
2009980	2015700	and with our awareness of those qualities. Like I said, this is not a solution to the
2015700	2020580	meta problem, but it's trying at least to pin down some parts of the roots of those
2020580	2026060	intuitions and to narrow down what needs to be explained. To go further, you want, I think,
2026060	2031580	to test these explanations, both with psychological studies to see if this is plausibly what's
2031580	2036340	going on in humans. This is the kind of thing which is the basis of our intuitions and computational
2036340	2040500	models to see if, for example, you could program this kind of thing into an AI system and see
2040500	2047140	if it can generate somehow qualitatively similar reports and intuitions. You might think that
2047140	2052500	last thing is a bit far-fetched right now, but I know of at least one instance of this
2052500	2058660	research program which has been put into play by Luke Milhauser and Bach Schleggeres, two
2058700	2066860	researchers at Open Philanthropy, very interested in AI and consciousness. They actually built,
2066860	2071300	they took some ideas about the meta problem from something I'd written about it and from
2071300	2077900	something that the philosopher Francois Camero had written about it. A couple of basic ideas about
2077900	2083460	where problem intuitions might come from, and they tried to build them in to a computational
2084060	2092980	model. They built a little software agent which had certain axioms about colors and how they
2092980	2098500	work. There's red and there's green, and certain axioms about their own subjective experiences
2098500	2106180	of colors, and then they combined it with a little theorem prover, and they saw what did
2106180	2110140	this little software agent come up with, and it came up with claims like, hey, well, my
2111100	2116620	experiences of color are distinct from any physical state, and so on. They cut a few corners.
2118700	2125820	This is not yet a truly convincing, sophisticated model of everything going on in the human mind,
2125820	2131420	but it shows that there's a research program here of trying to find the algorithmic basis
2132140	2137260	of these states. I think as more sophisticated models develop, we might be able to use these
2137340	2141900	to provide a way in for AI researchers in thinking about this topic. Of course,
2141900	2146220	there is the question, if you model all this stuff better and better in a machine,
2146220	2151020	then is the machine actually going to be conscious, or is it just going to have found
2151020	2158460	self-models that replicate what's going on in humans? Some people have proposed an artificial
2158460	2166140	consciousness test. Aaron Sloman, Susan Snyder, and Turner have suggested somehow that if a machine
2166220	2170380	seems to be puzzled about consciousness in roughly the ways that we are,
2170380	2176860	maybe that's actually a sign that it's conscious. If a machine actually looks to ask
2176860	2182140	as if it's puzzled by consciousness, is that a sign of consciousness? These people,
2182140	2186620	this is suggested as a kind of Turing test for machine consciousness. Find machines which are
2186620	2190140	conscious like we are. Of course, the opposing point of view is going to be, no, the machine is
2190140	2195340	not actually conscious. It's just like the machine that studied up for the Turing test by reading
2195340	2201260	the talk like a human book. It's like, damn, do I really need to convince those humans that I'm
2201260	2207420	conscious by replicating all those ill-conceived confusions about consciousness? Well, I guess
2207420	2213340	I can do it if I need to. Anyway, I'm not going to settle this question here, but I do think that
2213340	2219260	if we somehow find machines being puzzled, it won't surprise me that once we actually have
2219260	2224300	serious AI systems which engage in natural language and modeling of themselves in the world,
2225420	2229020	they might well be natural to find themselves saying things like, yeah, I know in principle I'm
2229020	2236460	just a set of silicon circuits, but I feel like so much more. I think that might tell us something
2236460	2243580	about consciousness. Let me just say a little bit about theories of consciousness. I do think a
2243580	2249020	solution to the meta problem and a solution to the hard problem ought to be closely connected.
2249020	2252860	The illusionist says, solve the meta problem. You'll dissolve the hard problem. But even if
2252860	2259340	you're not an illusionist about consciousness, there ought to be some link. So here's a thesis.
2259340	2265660	Whatever explains consciousness should also partly explain our judgments and our reports
2266300	2270620	about consciousness. The rationale here is it would just be very strange if these things were
2271180	2278620	independent. If the basis of consciousness played no role in our judgments about consciousness.
2279340	2285820	So I think you can use this as a way of evaluating or testing theories of consciousness. For theory
2285820	2291900	of consciousness, there's mechanism M is the basis of consciousness, that M should also partly
2291900	2298700	explain our judgments about consciousness. Whatever the basis is ought to explain the reports.
2298700	2304380	And you can use this. You can bring this to bear on various extant theories of consciousness. Here's
2304460	2310060	one famous current theory of consciousness, integrated information theory developed by
2310060	2318700	Giulio Tononi and colleagues at the University of Wisconsin. Tononi says the basis of consciousness
2318700	2325260	is integrated information. A certain kind of integration of information for which Tononi
2325260	2330860	has a measure that he calls phi. Basically, when your phi is high enough, you get consciousness.
2330940	2336220	So consciousness is high phi. And there's a mathematical definition, but I won't go into
2336220	2342540	it here. But such a really interesting theory. So here's a, basically it analyzes a network
2342540	2348140	property of systems, of units, and it's got an informational measure called phi that's supposed
2348140	2354700	to go with consciousness. Question. How does, if integrated information is the basis of consciousness,
2354700	2360060	it ought to explain problem reports, at least in principle. Challenge. How does that work?
2360060	2365180	And it's at least far from obvious to me how integrated information will explain the problem
2365180	2371580	reports. It seems pretty dissociated from them. I mean, on Tononi's view, you can have simulations
2372220	2378460	of systems with high phi that have zero phi. They'll go about making exactly the same reports,
2378460	2383740	but without consciousness at all. So phi is at least somewhat dissociable. You get systems
2383820	2390460	very high phi, but no tendency to report. Maybe that's less worrying. Anyway, here's a challenge
2390460	2395020	for this theory, for other theories. Explain not just how high phi gives you consciousness,
2395020	2401100	but how it plays a central role in the algorithms that generate problem reports. Something similar
2401100	2407740	goes for many other theories, biological theories, quantum theories, global workspace, and so on.
2408540	2413500	But let me just wrap up by saying something about the issue of illusionism that I was
2413740	2419020	talking about near the start. Again, you might be inclined to think that this approach through
2419020	2423980	the meta problem tends, at least very naturally, to lead to illusionism. And I think it can be,
2424620	2429740	it certainly provides, I think, some motivation for illusionism. The view that consciousness
2429740	2436220	doesn't exist, we just think it does. On this view, again, a solution to the meta problem dissolves
2436780	2443580	the hard problem. So here's one way of putting the case for illusionism. If there's a solution
2443580	2448700	to the meta problem, then there's an explanation of our beliefs about consciousness that's
2448700	2452860	independent of consciousness. There's an algorithm that explains our beliefs about
2452860	2457260	consciousness, doesn't mention consciousness, arguably could be in place without consciousness.
2457820	2464300	Arguably, that kind of explanation could debunk our beliefs about consciousness the same way that,
2464300	2470540	perhaps, explaining beliefs about God in evolutionary terms might debunk belief in God.
2470540	2474220	It certainly doesn't prove that God doesn't exist. You might think that if you can explain
2474220	2480300	our beliefs in terms of evolution, it somehow removes the justification or the rational basis
2480300	2484780	for those beliefs. So something like that, I think, can be applied to consciousness, too. And
2484780	2490460	there's a lot to be said about analyzing the extent to which this might debunk the beliefs.
2490460	2496140	On the other hand, the case against illusionism is very, very strong for many people. And the
2496140	2500620	underlying worry is that some illusionism is completely unbelievable. It's just a manifest
2500620	2507420	fact about ourselves that we have conscious experience, we experience red, we feel pain,
2507420	2513740	and so on. To deny those things is to deny the data. Now, the dialectic here is complicated.
2513740	2518620	The illusionist will come back and say, yes, but I can explain why illusionism is unbelievable.
2518620	2523180	These models we have, these self-models of consciousness, are so strong that they're
2523260	2528380	just wired into us by evolution, and they're not models we can get rid of. So my view predicts
2528380	2534780	that my view is unbelievable. And the question is what, the dialectical situation is complex
2534780	2541340	and interesting. But maybe I could just wrap up with two expressions of absurdity on either side
2541340	2548860	of this question, the illusionist and the anti-illusionist, both finding absurdity
2548940	2556140	in the other person's views. Here's Galen Strawson, who is here. Galen's view is very much that
2556140	2561180	illusionism is totally absurd. In fact, he thinks it's the most absurd view that anyone has ever
2561180	2567180	held. There occurred in the 20th century, the most remarkable episode in the whole history of ideas,
2567180	2571980	the whole history of human thought, a number of thinkers denied the existence of something we
2571980	2577260	know with certainty to exist, consciousness. He thinks this is just a sign of incredible
2577260	2583260	philosophical pathology. Here's the rationalist philosopher Eliezer Yudkowski in something he
2583260	2590620	wrote a few years ago on zombies and consciousness, and the view, the epiphenomenalist view that
2590620	2595660	consciousness plays no causal role, where he was engaging some stuff I wrote a couple of decades
2595660	2599900	ago. He said, this is a zombie argument, the idea we can imagine zombies physically like us,
2599900	2605740	but without consciousness. Maybe a candidate for the most deranged idea in all of philosophy.
2606300	2613980	The causally closed cognitive system of charmer's internal narrative is malfunctioning in a way that
2613980	2620620	not by necessity, but just in our own universe miraculously happens to be correct. Here he's
2620620	2625100	expressing this debunking idea that on this view, there's an algorithm that generates these
2625100	2629500	intuitions about consciousness, and that's all physical. There's also this further layer of
2629500	2637260	non-physical stuff, and just by massive coincidence, the physical algorithm is a correct model of the
2637260	2644220	non-physical stuff. That's a form of debunking here. It would take a miracle for this view to be
2644220	2649900	correct. So I think both of these views are onto, these objections are onto something, and to make
2649900	2655260	progress on this, when I decide we need to find a way of getting past these absurdities. I mean,
2655260	2660780	you might say, well, there's middle ground between very strong illusionism and very strong epiphenomenalism.
2660780	2667100	It tends to slide back to the same problems. Other forms of illusionism, weaker forms don't
2667100	2672700	help much with the higher problem. Other forms of realism are still subject to this. It takes a
2672700	2680380	miracle for this view to be correct. Critique. So I think to get beyond absurdity here, both sides
2680380	2685100	need to do something more. The illusionist needs to do more to explain how having a mind could
2685100	2691260	be like this, somehow just like this, even though it's not at all the way that it seems. They need
2691260	2698300	to find some way to recapture the data. Realists need to explain how it is that these meta-problem
2698300	2703020	processes are not completely independent of consciousness. Realists need to explain how
2703020	2708940	meta-problem processes, the ones that generate these intuitions and reports and convictions
2708940	2713820	about consciousness are essentially grounded in consciousness, even if it's possible somehow for
2713820	2720540	them to occur or conceivable for them to occur without consciousness. Anyway, so that's just to
2720540	2727100	lay out a research program. I think a solution to the meta-problem that meets these ambitions
2727100	2732060	might just possibly solve the hard problem of consciousness or at the very least,
2732060	2737580	shed significant light on it. In the meantime, the meta-problem is a potentially tractable
2737580	2740940	research project for everyone, and mine I recommend to all of you. Thanks.
2748380	2754540	Yes, I just want to say I think it's very interesting this concept of we have these mental models,
2755100	2762540	collection of mental models, and that this collection of mental models is consciousness,
2762620	2767420	basically. Consciousness is defined as a collection of these mental models that we have,
2767420	2772540	and the problem of consciousness is that we don't understand the physical phenomenon that
2772540	2779740	causes these mental models or that stimulates these mental models. So we just have this belief
2779740	2788220	that it's ephemeral or not real or something like that. And if you take that view, then what's
2788220	2795100	interesting is that you could simulate these mental models, like robot could simulate these
2795100	2802780	mental models, and you could simulate consciousness as well. And even if the underlying physical
2802780	2807580	phenomenon that fuels these mental models is different, you know, robots have different
2807580	2813980	sensors, etc., you could still get the same consciousness effect in both cases.
2814780	2818620	Yeah, I think that's right. Or at the very least, you ought to be able to get,
2818620	2822380	it looks like you ought to be able to get the same models at least in a robot. If the models
2822380	2827900	themselves are something algorithmic, and ought to be, you ought to be able to design a robot
2827900	2833660	that has at the very least, let's say, isomorphic models in some sense that is conscious. Of course,
2833660	2837980	it's a further question, at least by my lights, but then the robot will be conscious. And that was
2837980	2841420	the question I alluded to in talking about the artificial consciousness test. But you might think
2841420	2845580	that would at least be very good evidence that the robot is conscious. If it's got a model of
2845580	2850140	consciousness just like ours, it seems very plausible there ought to be a very strong link
2850140	2855660	between having a model like that and being conscious. I mean, I think probably something
2855660	2859020	like Ned Block, who was here arguing against machine consciousness, would say, no, no,
2859020	2862700	the model is not enough. The model has to be built of the right stuff. Say it's got to be
2862700	2866860	built of biology and so on. But at least by my lights, I think if I have found the AI system
2866860	2872220	that had a very serious version of our model of consciousness, I take that as a very good reason
2872220	2880940	to believe it's conscious. In the IIT theory, is there a estimate or plausible estimate for what
2880940	2890780	the value of phi is for people and for other systems? Basically, no. It's extremely hard to measure
2891340	2897980	in systems of any size at all. I mean, because the way it's defined involves taking a sum over
2897980	2903340	every possible partition of a system. It turns out, I mean, A, it's hard to measure in the brain
2903340	2907820	because you've got to involve the causal dependencies set between different units on neurons. But even
2907820	2914380	for a pure algorithmic system, you've got like a neural network laid out in front of you,
2914380	2918220	it's computationally intractable to measure the phi of one of those once they get to bigger than
2918220	2923580	15 units or so. So, you know, today I'd like to say this is an empirical theory and in principle
2923580	2930140	empirically testable. But notice the in principle, it's extremely difficult to to to measure phi.
2930140	2939020	Some people, Scott Aronson, the computer scientist has argued, has tried to put forward counter-examples
2939020	2945180	to the theory, which are basically very, very simple systems like matrix multipliers that
2945260	2949900	multiply two large matrices turn out to have enormous phi. Phi is as big as you like if the
2949900	2954620	matrices are big enough. And therefore, by Tononi's theory, we'll not just be conscious, but as
2954620	2959980	conscious as a human being. And Aronson put this forward as a reductio ad absurdum of the IIT theory.
2959980	2964620	I think Tononi basically bit the bullet and said, yeah, yeah, those those matrix multipliers are
2964620	2970620	actually having some high degree of consciousness. So I think IIT is probably missing at least
2970620	2974540	missing a few pieces of what's going to be developed. But it's a research program too.
2976060	2981980	You mentioned belief as an example of something where, you know, this is another mental quality,
2981980	2987900	but people don't seem to have the same sense that it is very hard to explain. In fact,
2988940	2993900	it almost seems too easy where people like a belief about something sort of feels like just
2993900	2999580	how things are. You have to kind of reflect on a belief to notice it as a belief. Do you
3000220	3006700	think there's also or has there been research kind of related to this question into why is that
3006700	3011500	different? Like, it seems like another angle of attack on this problem is just like, why doesn't
3011500	3016460	this generate the same hard problem? Yeah, in terms, I'm not sure if there's been sort of
3017740	3020860	research from the perspective of the meta problem or a theory of mind. Certainly,
3020860	3025820	people have thought in their own right, what is the difference between belief and experience
3025820	3031340	that makes them so different? This goes way back to David Hume, a philosopher a few centuries ago,
3031340	3038220	who said, you know, basically, perception is vivid. Impressions and ideas. Impressions like
3038220	3044780	experiencing colors are vivid. They have force and vivacity and ideas are merely a faint copy
3044780	3048540	or something. But that's just the first order. And then there are contemporary versions of this
3048540	3052940	kind of thing, far more sophisticated ways of saying a similar thing. But yeah, you could,
3053020	3059500	in principle, explore that through the meta problem. Why does it seem to us that perception is so
3059500	3065260	much more vivid? What about our models of the mind makes perception seem so much more vivid
3065260	3070380	than belief? It makes belief seem kind of structural and empty, whereas
3071180	3075500	perception is so full of light. But no, I don't know of work on that from the meta problem
3075500	3080220	perspective. Like I said, there's not that much work on these introspective models directly. There
3080220	3083980	is work on theory of mind about beliefs tends to be about models of other people.
3085980	3089420	It may be there's something I could dig through a literature on belief that says something about
3089420	3095340	that. It's a good place to push. Thanks. I wanted to bring up Kurt Girdel. You mentioned your advisor
3095340	3101500	wrote Girdel Escher Bach. There's something that seems very like Girdel, Girdelian or whatever,
3101500	3108380	about this whole discussion in that. So Girdel showed that, given like a set of axioms and
3108380	3117820	mathematics, it would either be consistent or complete, but not both. And it seems like when
3117820	3124860	Daniel Dennett, Daniel Dennett seems to have like a set of axioms where he cannot construct
3124860	3129580	consciousness from them. He seems to be very much in this sort of consistent camp. Like he
3129580	3137020	wants to have a consistent framework, but is okay with the incompleteness. And I wonder if
3137100	3142860	similar approach could be taken with consciousness where we could in fact prove that consciousness
3142860	3149020	is independent of Daniel Dennett's set of axioms. The same way they proved after Girdel, they
3149020	3154220	proved like the continuum hypothesis was independent of ZF set theory. And then they added
3154860	3162140	the axiom of choice made at ZFC set theory. So I wonder if we could show that like in Daniel
3162140	3167260	Dennett's world we are essentially zombies or we are kind of either zombies or not. It doesn't
3167260	3173660	matter. Either statement could be true. And then find what is like the minimum axiom that has to be
3173660	3179500	added to Dennett's axioms in order to make consciousness true. Interesting. I thought for a
3179500	3183020	moment this was going to go in a different direction. And you're going to say Dennett is
3184460	3189100	consistent but incomplete. He doesn't have consciousness in this picture. I'm complete,
3189180	3192620	I've got consciousness, but inconsistent. That's why I say all these crazy things.
3194140	3198860	And you're faced with the choice of not having consciousness and being incomplete or having
3198860	3203180	consciousness and somehow getting this hard problem and being forced into at least puzzles
3203180	3205980	and paradoxes. But the way you put it was friendlier to me.
3210220	3215660	Yeah, I mean certainly Dark Hofstetter himself has written a lot on analogies between the
3215660	3221100	Gordelian paradoxes and the mind-body problem. And he thinks always our models, our self models
3221100	3225740	are always doomed to be incomplete in the Gordelian way. And he thinks that that might be somehow
3225740	3230940	part of the explanation of our puzzlement at least about consciousness. Someone like Roger Penrose,
3230940	3237740	of course, takes this much more seriously, literally. He thinks that the computational
3237740	3244060	aspects of computational systems are always going to be limited in the Gordel way. He thinks human
3244060	3248620	beings are not so limited. He thinks we've got mathematical capacities to prove theorems,
3249580	3255260	to see the truth of certain mathematical claims that no formal system could ever have.
3255820	3260380	So he thinks that we somehow go beyond that incomplete Gordelian. I don't know if he actually
3260380	3264860	thinks we're complete, but at least we're not incomplete in the way that finite computational
3264860	3270220	systems are incomplete. And furthermore, he thinks that extra thing that humans have is tied to
3270220	3275180	consciousness. I mean, I never quite saw how that last step goes, even if we did have these
3275180	3279500	special non-algorithmic capacities to see the truth of mathematical theorems. How would that be
3279500	3287100	tied to consciousness? But at the very least, there are structural analogies to be drawn between
3287100	3291420	those two cases about incompleteness of certain theories, how literally we should take the analogies
3291420	3298220	I'd have to think about. Has there been some consideration that the problem of understanding
3298220	3302860	consciousness sort of inherently must be difficult because we address the problem
3302860	3308940	using consciousness? I'm reminded of the halting problem in computer science where we say that
3308940	3313980	in the general case, a program cannot be written to tell whether another program will halt because
3313980	3319740	what if you ran it on itself? It can't sort of be broad enough to include its own execution. So I
3319740	3324700	wonder if there's a similar corollary in consciousness where we use consciousness to think about
3324700	3331500	consciousness. And so therefore, we may not have enough sort of equipment there to be able to unpack
3332700	3338220	Yeah, I mean, it's tricky. People say it's like a user ruler to measure a ruler. Well, I can use
3338220	3343580	this ruler to measure many other things, but it can't measure itself. On the other hand,
3343580	3348380	you can measure one ruler using another ruler. Maybe you can measure one consciousness using
3348380	3353820	another. The brain can't study the brain, but the brain actually has a pretty good job of studying.
3354700	3359500	The brain. So there are some self referential paradoxes there. And I think that again is at
3359500	3364860	the heart of Hofstadter's approach. But I think we'd have to look for very, very specific conditions
3364860	3370220	under which systems can't study themselves. I did always like the idea that the mind was simple
3370220	3377500	enough that we could understand it. We would be too simple to understand the mind. So maybe
3377500	3381180	something like that could be true of consciousness. On the other hand, I actually think that if you
3381180	3385260	start thinking that consciousness can go along with very simple systems, I think at the very
3385260	3390140	least we ought to be able to study consciousness in other systems simpler than ourselves. And boy,
3390140	3396700	if I could solve the hard problem, even in dogs, I'd be I'd be satisfied. Hey, so I have a question
3396700	3402540	about how the meta problem research program might proceed sort of related to the last question.
3403260	3409420	So certainly things we believe about our own consciousness, even if we all say them,
3409420	3414940	probably some of them are false. Our brain has a tendency to hide what reality is like.
3415820	3419900	If you look at like visual perception, you know, there's what's called lightness constancy, you
3419900	3424700	know, our brain subtracts out the lighting in the environment. So we actually see more reliably
3424700	3429980	what the colors of objects are. Like these viral examples of like the black and gold dress is an
3429980	3434540	example of this. And when you're kind of presented with an explanation of it, it's like, huh, my
3434620	3440380	brain does that. It's not something we have access to. Yeah. Or like the Yanni Laurel Laurel Yanni.
3440380	3444300	Yeah, illusion is like another one where like when you hear the explanation, you know, the scientists
3444300	3449900	that understand it, our own introspection doesn't include that. So how do you kind of proceed with
3451260	3457020	trying to get at what consciousness really is versus what our sort of whatever simplified or
3457020	3464300	distorted view might be? Yeah, well, one view here would be that we never have access to the
3464300	3469500	mechanisms that generate consciousness, but we still have access to the conscious states
3469500	3473980	themselves. Actually, the Colashley said this decades ago, he said, no process of the brain
3473980	3479340	is ever conscious. The processes that get you to the states are never conscious. The states they
3479340	3485340	get you to are conscious. So take your experience of the dress. For me, it was, it was what, white
3485420	3491100	and gold. So, you know, and I knew that, you know, each of us was certain that I am, I was
3491100	3495180	experiencing, I was certain that I was experiencing white and gold. Maybe you were certain that you
3495180	3501340	were experiencing blue and black. Which it was. I remember as I was right. You were sure that, yeah,
3502380	3508060	those idiots can't be, yeah, can't be looking at this right. But anyway, each of us, I think the
3508060	3511820	natural way to describe this at least is that each of us was certain what kind of conscious
3511820	3516700	experience we were having. But what we had no idea about was the mechanisms by which we got
3516700	3522220	there. So the mechanisms are completely opaque. But the states themselves were at least prima facie
3522220	3525580	transparent. So I think that would be the standard of view. And even a realist about consciousness
3526220	3530140	could go with that. They say, well, we know what conscious states, we know what those conscious
3530140	3534540	states are. We don't know the processes by which they're generated. The illusionist, I think,
3534540	3539500	wants to go much further and say, well, it seems to you that you know what conscious state you're
3539500	3544940	having. It seems to you that you're experiencing yellow and gold. Sorry, yellow and white, whatever
3544940	3550140	it was, golden, gold and white. Black and gold is whatever. Black and blue, I think. And blue.
3550140	3555900	Gold and white. Yeah. It seems to you that you're experiencing gold and white. But in fact, that
3555900	3561180	too is just something thrown up by another model. The yellow gold was a perceptual model. And then
3561180	3565660	there was an introspective model that said you're experiencing gold and white. When maybe, in fact,
3565660	3569100	you're just a zombie or who knows what's actually going on in your conscious state. So
3569100	3573340	the illusionist view, I think, has to somehow take this further and say not just the processes
3573340	3577260	that generate the conscious states, but maybe the conscious states themselves are somehow
3577260	3586940	opaque to us. All right. Thanks. It feels like some discussion of generality of a problem is
3586940	3593020	missing from this discussion. The matrix multiplier example of having high phi is still,
3593020	3597980	it's not a general thing. Is there someone exploring the space, the sort of intersection
3598060	3602300	of generality and complexity that leads to consciousness as an emergent behavior?
3603100	3607660	When you say generality, I mean, the idea that a theory should be general, that it should apply
3607660	3611900	to every system, you mean mechanisms of... No, generality of the agent, right? If I can write
3611900	3617180	an arbitrarily complex program to play tic-tac-toe, and all it will ever be able to do is play tic-tac-toe,
3617180	3623340	it has no outputs to express anything else. Yeah. As you said, general in the sense of AGI,
3623340	3628460	artificial general intelligence, I mean, some aspects of consciousness seem to be
3628460	3633740	domain general, like, for example, maybe as far as belief and reasoning is conscious,
3633740	3638140	those are domain general, but much of perception doesn't seem especially domain general, right?
3638140	3643900	Color is very domain... Taste is very domain specific, so it's still conscious. If my agent
3643900	3648860	can't express problem statements, like, if I don't give it an output by which it can express
3648860	3652060	problem statements, you can never come to a conclusion about its consciousness.
3653260	3657020	I like to distinguish intelligence and consciousness. I'm even able to... Even natural
3657020	3661980	language and, you know, being able to address a problem statement and analyze a problem,
3661980	3669740	that's already a very advanced form of intelligence. I think it's very plausible that, say, a mouse has
3669740	3675180	got some kind of consciousness, even it's got no ability to address problem statements in many
3675180	3679580	of its capacities, maybe very specialized. I mean, it's still much more general than, say,
3679580	3684620	a simple neural network that can only do one thing. A mouse can do many things, but I'm not
3684620	3688380	sure that I see an essential... I certainly see a connection between intelligence and
3688380	3693740	generality. We want to say, you know, somehow a high degree of generality is required for
3693740	3698220	high intelligence. I'm not sure there's the same connection for consciousness. I think
3698220	3704780	consciousness can be extremely domain specific, has, say, taste and maybe vision or it can be
3704780	3707660	domain general. So maybe those two cross cut each other a bit.
3711340	3718300	So it seems to me like the meta problem as it's formulated implies some amount of, like,
3718300	3722460	separation or epiphenomenalism between, like, consciousness and brain states.
3723100	3730060	And one thing that I think underlies a lot of people's motivation to do, say, science is that
3730620	3737500	it has causal import. Like, predicting behaviors is clearly a functionally useful thing to do.
3738140	3742460	And if you can predict all of behavior without having to explain consciousness,
3743020	3747740	their motivation for explaining consciousness sort of evaporates and it sort of feels like,
3747740	3752460	yeah, yeah, well, what's the point of even thinking about that because it's just not going
3752460	3757100	to do anything for me? What do you say to someone when they say that to you?
3757820	3759340	What is the thing that they say to me again?
3759340	3765020	That there's no, there's maybe consciousness exists, maybe it doesn't. But if I can explain
3765020	3768780	all of human behavior and all of the behavior of the world in general without
3768780	3774060	recourse to such concepts, then I've done everything that there is that's useful,
3774060	3780140	like explaining consciousness isn't a useful thing to do. And thus, I'm not interested in this and
3780140	3785740	it may not be real. I mean, I'm certainly, I'm not, I mean, I think epiphenomenalism could be
3785740	3789340	true. I certainly don't have any commitment to it though. It's quite possible that consciousness
3789340	3794940	has a role to play in generating behavior that we don't yet understand and maybe thinking hard
3794940	3800220	about the meta problem can help us get clearer on those roles. I think if you've got any sympathy
3800220	3804700	to panpsychism, maybe consciousness is intimately involved with how physical processes get going
3804700	3809660	in the, in the first place. And there are people who want to pursue interactionist ideas where
3809660	3813820	consciousness interacts with the brain. Or if you're a reductionist, consciousness may be just
3813820	3818860	a matter of the right algorithm. On all those views, consciousness may have some role to play,
3818860	3824220	but just say it turns out that you can explain all of behavior, including these problems without,
3825420	3829580	without bringing in consciousness. And does that mean that consciousness is not something we
3829580	3833020	should care about and not something that matters? I don't think that would follow. I mean, maybe it
3833020	3838300	wouldn't matter for certain engineering purposes, say you want to build a useful system. But,
3838860	3842860	you know, at least in my view, consciousness is really the only thing that matters. It's a thing
3842940	3849100	that makes life worth living. It's what gives our lives meaning and value and so on. So,
3849100	3853420	it might turn out that, okay, the point of consciousness is not that useful for explaining
3853420	3857980	other stuff. But it's, you know, if it's the source of intrinsic significance in the world,
3857980	3862780	then understanding consciousness will still be absolutely essential to understanding ourselves.
3862780	3868380	Furthermore, if it comes to developing other systems, like say AI systems or dealing with
3868380	3873740	non-human animals and so on, we absolutely want to know, we need to know whether they're conscious,
3873740	3877580	because, you know, if they're conscious, they presumably have moral status. If they can suffer,
3878220	3883420	then it's very bad to mistreat them. If they're not conscious, then you might, I think it's very
3883420	3887980	plausible, treat non-conscious systems. We can treat how we like, and it doesn't really matter
3888540	3892700	morally. So, the question of whether, say, an AI system is conscious or not, it's going to be
3892700	3897580	absolutely vital for how we interact with it and how we build our society. That's not a question
3897580	3901740	of engineering usefulness. That's a question of connecting with our most fundamental values.
3902380	3908060	Yeah, I completely agree. I just, I haven't found that formulation to be very convincing to others
3908060	3915660	necessarily. Hi, thank you so much for coming and chatting with us today. I'm really interested in
3915660	3921500	some of your earlier work, The Extended Mind Distributed Cognition. Yeah. And you're at a company
3921500	3925900	speaking with a bunch of people who do an incredibly cognitively demanding task. Yeah.
3925900	3931580	Most of the literature that I've read on this topic uses relatively simple examples of telling,
3931580	3938540	like, it's difficult to think just inside your head on these relatively simple things. And if
3938540	3942140	you take a look at the programs that we build, sort of like on a mundane, day-to-day basis,
3942140	3946700	there are millions of lines long. I've read people in the past say something like,
3946700	3952140	the Boeing 777 was the most complicated thing that human beings have ever made. And I think
3952140	3956540	most of us would look at that and say, we got that beat, you know, like the things that large
3956540	3961260	internet companies do, the size, the complexity of that is staggering. And yet if we close our
3961260	3965340	eyes, everyone in here is going to say, I'm going to have difficulty writing a 10-line program in
3965340	3971340	my head. Okay. So I'm just sort of as an open, like, I'd be very interested in hearing your thoughts
3971340	3976860	about how the activity of programming connects to the extended mind ideas.
3977500	3983100	Yeah. So this is a reference to something that I got started in about 20 years ago with
3983740	3989180	my colleague Andy Clarke. We wrote an article called The Extended Mind about how processes in
3989180	3994540	the mind can extend outside the brain when we become coupled to our tools. And actually,
3994540	3999820	our central example back then in the mid-90s was a notebook, someone writing stuff in a
3999820	4004780	notebook. I mean, even then, we knew about the internet and we had some internet examples.
4004780	4011500	I guess this company didn't exist yet in 95. But now, of course, our minds have just become
4011500	4017580	more and more extended and, you know, smartphones came along a few years later and everyone is
4017580	4022460	coupled very, very closely to their phones and their other devices that coupled them very,
4022460	4029820	very closely to the internet. Now, it's certainly the case that a whole lot of my memory is now
4029820	4035420	offloaded onto the servers of your company somewhere or other, whether it's in the
4037820	4044460	mail systems or navigation mapping systems or other systems. Most of my navigation has been
4045500	4051020	offloaded to maps and much of my memory has been offloaded. Well, maybe that's in my phone, but
4052460	4056220	other bits of my memory are offloaded into my file system on
4056620	4066140	some cloud service. So certainly, vast amounts of my mind are now
4067020	4071820	existing in the cloud. And if I was somehow to lose access to those completely, then I'd
4071820	4078780	lose an awful lot of my capacities. So I think we are now sort of extending
4078780	4084380	into the cloud, thanks to you guys and others. The question specifically about programming
4086940	4091420	programming is a kind of active interaction with our devices. I mean, I think of programming
4091420	4096460	as something that takes a little bit longer. It's a longer time scale. So the core cases of the
4096460	4101980	extended mind involve sort of automatic use of our devices, which are always ready to hand.
4101980	4108060	We can use them to get information, to act in the moment, which is the kind of thing that
4108940	4114220	the brain does. So insofar as programming is a slower process, you know, and I remember from
4114860	4123100	programming days, all the endless hours of debugging and so on. Then it's at least going
4123100	4127980	to be a slower time scale for the extended mind. But still, Feynman talked about writing
4128780	4136460	this way. Someone looked at Feynman's work and a bunch of notes he had about a physics problem
4136460	4141100	he was thinking about. And someone said to him, oh, it's nice you have this record of your work.
4142060	4147900	And Feynman said, that's not a record of my work. That's the work. That is the thinking.
4147900	4152300	And so I was writing it down and so on. I think, you know, at least my recollection from my programming
4152300	4156460	days was that, you know, when you're actually writing a program, that's not like you just
4157020	4162780	do a bunch of thinking and then code your thoughts. The programming is to some very
4162780	4168700	considerable extent your thinking. So is that the sort of thing you're thinking about here?
4168700	4175900	And the, if we, I think as people that program start to reflect on what we do,
4175900	4182300	and very few of us actually, like if you're the tech lead of a system, maybe you've got it in your
4182300	4186700	head, okay? But you would agree that most of the people on the team who've come more recently only
4186700	4190460	have a chunk of it in their head, and yet they're somehow still able to contribute.
4190460	4196140	Oh yeah, this is now distributed cognition. I mean, the extended mind that extended cognition
4196140	4202060	like starts with an individual and then extends out, extends their capacities out using their
4202060	4207100	tools or their devices or even other people. So maybe my partner serves as my memory, but it's
4207100	4212300	still centered on an individual. But then there's the closely related case of distributed cognition,
4212300	4217580	where you have a team of people who are doing something and are making joint decisions and
4217580	4221500	carrying out joint actions in an absolutely seamless way. And I take it as a company like this
4221500	4225180	that are going to be any number of instances of distributed cognition. I don't know whether the
4225180	4232220	company as a whole has one giant Google mind, or maybe there's just like a near infinite number
4232220	4239100	of separate Google minds for all the individual teams and divisions and so on. But I think yeah,
4239100	4243500	probably some anthropologist has already done a definitive analysis of distributed cognition
4243500	4254220	in this company, but if they haven't, they certainly need to. Thank you.
4255180	4258220	Thank you.
