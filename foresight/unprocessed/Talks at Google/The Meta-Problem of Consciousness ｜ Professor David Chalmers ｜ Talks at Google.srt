1
00:00:00,000 --> 00:00:13,440
Thanks for coming out. It's good to be here. As Eric said, I'm a philosopher, thinking

2
00:00:13,440 --> 00:00:20,160
about consciousness, coming from a background in the sciences and math that always struck

3
00:00:20,160 --> 00:00:28,440
me that the most interesting and hardest unsolved problem in the sciences was the problem

4
00:00:28,440 --> 00:00:34,480
of consciousness. Way back, 25 years ago, when I was in grad school, it seemed to me

5
00:00:34,480 --> 00:00:40,600
the best way to come at this from a big picture perspective was to go into philosophy and

6
00:00:40,600 --> 00:00:44,920
think about the foundational issues that arise and thinking about consciousness from any

7
00:00:44,920 --> 00:00:52,200
number of different angles, including the angles of neuroscience and psychology and AI.

8
00:00:52,200 --> 00:00:55,880
In this talk, I'm going to present a slightly different perspective on the problem after

9
00:00:55,880 --> 00:01:03,800
laying out some background, the perspective of what I call the meta problem of consciousness.

10
00:01:03,800 --> 00:01:08,680
I always like the idea that you approach a problem by stepping one level up, taking the

11
00:01:08,680 --> 00:01:16,760
meta perspective. I love this quote, anything you can do, I can do meta. I have no idea

12
00:01:16,760 --> 00:01:21,040
what the origins was. I like the fact this is attributed to Rudolf Kahnap, one of my

13
00:01:21,040 --> 00:01:25,000
favorite philosophers, but anyone who knows Kahnap's work is completely implausible. He

14
00:01:25,000 --> 00:01:29,120
would ever say anything so frivolous. It's also been attributed to my thesis advisor,

15
00:01:29,120 --> 00:01:34,920
Doug Hofstadter, author of Goethe Lescher Bach and a big fan of the meta perspective,

16
00:01:34,920 --> 00:01:41,000
but he assures me he never said it either. But the meta perspective on anything is stepping

17
00:01:41,000 --> 00:01:48,620
up a level. The meta problem, as I think about it, is called the meta problem because it's

18
00:01:48,620 --> 00:01:54,320
a problem about a problem. A meta theory is a theory about a theory. Meta problem is

19
00:01:54,320 --> 00:01:58,980
a problem about a problem. In particular, it's the problem of explaining why we think

20
00:01:58,980 --> 00:02:04,120
there is a problem about consciousness. So there's a first order problem, the problem

21
00:02:04,120 --> 00:02:09,180
of consciousness. Today I'm going to focus on a problem about it, but I'll start by introducing

22
00:02:09,180 --> 00:02:15,200
the first order problem itself. The first order problem is what we call the hard problem

23
00:02:15,200 --> 00:02:22,000
of consciousness. It's the problem of explaining why and how physical processes should give

24
00:02:22,040 --> 00:02:29,040
rise to conscious experience. You've got all this neurons firing in your brain, bringing

25
00:02:31,520 --> 00:02:37,880
about all kinds of sophisticated behavior. We can get to be done explaining our various

26
00:02:37,880 --> 00:02:41,680
responses, but there's this big question about how it feels from the first person point of

27
00:02:41,680 --> 00:02:46,320
view. That's the subjective experience. I like this illustration of the hard problem

28
00:02:46,320 --> 00:02:51,960
of consciousness. It seems to show someone's hair catching fire, but I guess it's a metaphorical

29
00:02:52,400 --> 00:02:58,400
illustration of the subjective perspective. The hard problem is concerned with what philosophers

30
00:02:58,400 --> 00:03:04,320
call phenomenal consciousness. The word consciousness is ambiguous a thousand ways, but phenomenal

31
00:03:04,320 --> 00:03:10,360
consciousness is what it's like to be a subject from the first person point of view. A system

32
00:03:10,360 --> 00:03:15,880
is phenomenally conscious. If there's something it's like to be it, a mental state is phenomenally

33
00:03:15,880 --> 00:03:20,280
conscious if there's something it's like to be in it. The thought is there are some

34
00:03:20,320 --> 00:03:25,200
systems, so there's something it's like to be that system. There's something it's like

35
00:03:25,200 --> 00:03:30,320
to be me. I presume there's something it's like to be you, but presumably there's nothing

36
00:03:30,320 --> 00:03:35,960
it's like to be this lectern as far as we know. The lectern does not have a first person

37
00:03:35,960 --> 00:03:42,960
perspective. This phrase was made famous by my colleague Tom Nagel at NYU who back in 1974

38
00:03:43,040 --> 00:03:50,040
wrote an article called What is it like to be a bat? The general idea was what's very

39
00:03:50,960 --> 00:03:56,200
hard to know what it's like to be a bat from the third person point of view just looking

40
00:03:56,200 --> 00:04:00,640
at it as a human who has different kinds of experience, but presumably very plausibly

41
00:04:00,640 --> 00:04:05,000
there is something it's like to be a bat. The bat is conscious. It's having subjective

42
00:04:05,000 --> 00:04:12,000
experiences just of a kind very different from ours. In human subjective experience

43
00:04:13,920 --> 00:04:20,400
consciousness divides into any number of different kinds or aspects like different tracks of

44
00:04:20,400 --> 00:04:26,400
the inner movie of consciousness. We have visual experiences like the experience of

45
00:04:26,400 --> 00:04:32,920
say these colors blue and red and green from the first person point of view and of depth.

46
00:04:32,920 --> 00:04:39,920
There are sensory experiences like the experience of my voice, experiences of taste and smell,

47
00:04:40,360 --> 00:04:47,360
their experiences of your body, feeling pain or orgasms or hunger or a tickle or something.

48
00:04:47,840 --> 00:04:53,080
They all have some distinctive first person quality, mental images like recalled visual

49
00:04:53,080 --> 00:05:00,080
images, emotional experiences like experience of happiness or anger and indeed we all seem

50
00:05:00,280 --> 00:05:03,840
to have this stream of a current thought or at the very least we're kind of chattering

51
00:05:03,840 --> 00:05:10,840
away to ourselves and reflecting and deciding. All of these are aspects of subjective experience,

52
00:05:11,200 --> 00:05:17,560
things we experience from the first person point of view and I think these subjective

53
00:05:17,560 --> 00:05:24,560
experiences are at least on the face of it data, data for the science of consciousness

54
00:05:24,760 --> 00:05:29,680
to explain. These are just facts about us that we're having these subjective experiences.

55
00:05:29,680 --> 00:05:34,600
As we ignore them, we're ignoring the data. So if you catalog the data that say the science

56
00:05:34,600 --> 00:05:38,360
of consciousness needs to explain, there are certainly facts about our behavior and how

57
00:05:38,360 --> 00:05:42,480
we respond in situations. There are facts about how our brain, facts about how our brain

58
00:05:42,480 --> 00:05:46,840
is working. There are also facts about how subjective experiences and on the face of

59
00:05:46,840 --> 00:05:53,760
it their data. And it's these data that pose what I call the hard problem of consciousness.

60
00:05:53,840 --> 00:05:59,680
This gets contrasted with the easy problems, the so-called easy problems of consciousness

61
00:05:59,680 --> 00:06:06,680
which are the problems of explaining behavioral and cognitive functions. Objective things

62
00:06:07,600 --> 00:06:12,560
you can measure from the third person point of view typically tied to behavior, perceptual

63
00:06:12,560 --> 00:06:18,440
discrimination say of a stimulus. I can discriminate two different things in my environment. I

64
00:06:18,440 --> 00:06:23,040
can say that's red and that's green. I can integrate the information say about the color

65
00:06:23,040 --> 00:06:27,040
and the shape. I can use it to control my behavior, walk towards the red one rather

66
00:06:27,040 --> 00:06:34,520
than the green one. I can report it, say that's red and so on. Those are all data too for

67
00:06:34,520 --> 00:06:40,040
science to explain. But we've got a bead on how to explain those. They don't seem to

68
00:06:40,040 --> 00:06:47,040
pose as big a problem. Why? We explain those easy problems by finding a mechanism, typically

69
00:06:48,040 --> 00:06:55,040
a neural or computational mechanism that performs the relevant function to explain

70
00:06:55,440 --> 00:07:00,440
how it is that I get to say there's a red thing over there or walk towards it. Will

71
00:07:00,440 --> 00:07:07,440
you find the mechanisms involving perceptual processes and action processes in my brain

72
00:07:07,800 --> 00:07:12,560
that leads to that behavior? Find the right mechanism that performs the function. You've

73
00:07:12,560 --> 00:07:18,680
explained what needs to be explained with the easy problems of consciousness. But for

74
00:07:18,680 --> 00:07:23,800
the hard problem, for subjective experience, it's just not clear that this standard method

75
00:07:23,800 --> 00:07:30,800
works. It looks like explaining all that behavior still leaves open a further question. Why

76
00:07:31,040 --> 00:07:38,040
does all that give you subjective experience? Explain the reacting, the responding, the

77
00:07:38,800 --> 00:07:45,800
controlling, the reporting and so on. It still leaves open the question why is all that accompanied

78
00:07:46,040 --> 00:07:52,680
by subjective experience? Why doesn't it go on in the dark without consciousness, so

79
00:07:52,680 --> 00:07:57,600
to speak? There seems to be what the philosopher Joe Levine has called a gap here, an explanatory

80
00:07:57,600 --> 00:08:02,600
gap between physical processes and subjective experience, at least our standard kinds of

81
00:08:02,600 --> 00:08:07,840
explanation which work really well for the easy problems of behavior and so on. Don't

82
00:08:07,880 --> 00:08:14,880
obviously give you a connection to the subjective aspects of experience. There's been a vast

83
00:08:15,880 --> 00:08:20,760
amount of discussion of these things over, well for centuries really, but it's been particularly

84
00:08:20,760 --> 00:08:27,760
active in recent decades, philosophers, scientists, all kinds of different views. Philosophically

85
00:08:28,080 --> 00:08:34,600
you can divide approaches to the hard problem into at least two classes. One is an approach

86
00:08:34,680 --> 00:08:38,920
on which consciousness is taken to be somehow irreducible and primitive. We can't explain

87
00:08:38,920 --> 00:08:44,240
it in more basic physical terms, so take it as a kind of primitive and that might lead

88
00:08:44,240 --> 00:08:48,080
to dualist theories of consciousness where consciousness is somehow separate from and

89
00:08:48,080 --> 00:08:53,560
interacts with the brain. Recently very popular has been the class of panpsychist theories

90
00:08:53,560 --> 00:08:58,880
of consciousness. I know Galen Strossom is here a while back talking, he very much favors

91
00:08:58,880 --> 00:09:03,000
panpsychist theories where consciousness is something basic in the universe underlying

92
00:09:03,040 --> 00:09:09,040
matter and indeed there are idealist theories where consciousness underlies the whole universe,

93
00:09:09,040 --> 00:09:15,160
so these are all extremely speculative but interesting views that I've explored myself.

94
00:09:15,160 --> 00:09:20,960
There are also reductionist theories of consciousness from functionalist approaches where consciousness

95
00:09:20,960 --> 00:09:27,960
is just basically taken to be a giant algorithm or computation. Biological approaches to consciousness,

96
00:09:28,000 --> 00:09:34,000
my colleague Ned Block was here I know talking about neurobiology based approaches where

97
00:09:34,000 --> 00:09:39,440
it's not the algorithm that matters but the biology is implemented in and indeed the kind

98
00:09:39,440 --> 00:09:44,720
of quantum approaches that people like Roger Penrose and Stuart Hemeroff have made famous.

99
00:09:44,720 --> 00:09:48,960
I mean I think there's interesting things to say about all of these approaches. I think

100
00:09:48,960 --> 00:09:54,400
that right now at least most of the reductionist approaches leave a gap but the non-reductionist

101
00:09:54,440 --> 00:10:01,440
approaches have other problems in seeing how it all works. Today I'm going to take a different

102
00:10:01,440 --> 00:10:08,440
kind of approach, this approach through the meta-problem. One way to motivate this is to

103
00:10:08,520 --> 00:10:14,280
ask, I often get asked, okay you're a philosopher it's fine you get to think about these things

104
00:10:14,280 --> 00:10:20,280
like the hard problem of consciousness, how can I as a scientist or an engineer or an

105
00:10:20,280 --> 00:10:27,280
AI researcher, how can I do something to kind of contribute to help get at this hard problem

106
00:10:27,440 --> 00:10:31,680
of consciousness? Is this just a problem for philosophy? I mean for me to work on it say

107
00:10:31,680 --> 00:10:37,680
as an AI researcher I need something I can operationalize, something I can work with

108
00:10:37,680 --> 00:10:44,180
and try to program and as it stands it's just not clear how to do that with the hard problem.

109
00:10:44,180 --> 00:10:48,320
I mean if you're a neuroscientist there are some things you can do. You can say work with

110
00:10:48,320 --> 00:10:53,520
humans and look at their brains and look for the neural correlates of consciousness, the

111
00:10:53,520 --> 00:10:56,800
bits of the brain that go along with being conscious because at least with humans we

112
00:10:56,800 --> 00:11:01,440
can take as a background assumption, a plausible background assumption that the system is conscious.

113
00:11:01,440 --> 00:11:05,320
For AI we can't even do that, we don't know which AI systems we're working with or conscious,

114
00:11:05,320 --> 00:11:11,320
we need some operational criteria. In AI we mostly work on modeling things like behavior

115
00:11:11,320 --> 00:11:16,240
and objective functioning for consciousness, those are the easy problems. So how does someone

116
00:11:16,300 --> 00:11:22,800
coming from this perspective make a connection to the hard problem of consciousness? Well

117
00:11:22,800 --> 00:11:29,160
one approach is to work on certain problems among the easy problems of behavior that shed

118
00:11:29,160 --> 00:11:36,160
particular light on the hard problem and that's going to be the approach that I look at today.

119
00:11:36,920 --> 00:11:42,400
So the guiding, the key idea here is there are certain behavioral functions that seem

120
00:11:42,440 --> 00:11:49,440
to have a particularly close relation to the hard problem of consciousness. In particular

121
00:11:49,440 --> 00:11:55,080
we say things about consciousness. We make what philosophers call phenomenal reports,

122
00:11:55,080 --> 00:12:02,080
verbal reports of conscious experiences. So I'll say things like I'm conscious, I'm

123
00:12:02,160 --> 00:12:08,280
feeling pain right now and so on. Maybe the consciousness and the pain are subjective

124
00:12:08,360 --> 00:12:14,360
experiences but the reports, the utterances, I am conscious, well that's a bit of behavior.

125
00:12:14,360 --> 00:12:20,640
In principle explaining those is among the easy problems, objectively measurable response,

126
00:12:20,640 --> 00:12:27,640
we can find a mechanism in the brain that produces it. And among our phenomenal reports

127
00:12:27,640 --> 00:12:33,800
there's this special class we can call the problem reports, reports expressing our sense

128
00:12:33,800 --> 00:12:38,720
that consciousness poses a problem. Now admittedly not everyone makes these reports but they

129
00:12:38,720 --> 00:12:43,160
seem to be fairly widespread among, especially among philosophers and scientists thinking

130
00:12:43,160 --> 00:12:47,920
about these things but furthermore it's a sense that it's fairly easy to find and a

131
00:12:47,920 --> 00:12:54,080
very wide class of people who think about consciousness. People say things like there

132
00:12:54,080 --> 00:12:59,920
is a problem of consciousness, a hard problem. On the face of it explaining behavior doesn't

133
00:12:59,960 --> 00:13:05,320
explain consciousness, consciousness seems non-physical, how would you ever explain the

134
00:13:05,320 --> 00:13:10,840
subjective experience of red and so on. It's an objective fact about us, at least about

135
00:13:10,840 --> 00:13:17,840
some of us, that we make those reports and that's the fact about human behavior.

136
00:13:19,440 --> 00:13:25,940
So the matter problem of consciousness then at a second approximation is roughly the problem

137
00:13:25,940 --> 00:13:32,460
of explaining these problem reports, explaining you might say the conviction that we're conscious

138
00:13:32,460 --> 00:13:37,220
and that consciousness is puzzling. And what's nice about this is that although the hard

139
00:13:37,220 --> 00:13:40,820
problem is this, you know, airy-fairy problem about subjective experience that's hard to

140
00:13:40,820 --> 00:13:47,020
pin down, this is a puzzle ultimately about behavior. So this is an easy problem, one

141
00:13:47,020 --> 00:13:51,500
that ought to be open to those standard methods of explanation in the cognitive and brain

142
00:13:52,460 --> 00:13:59,460
sciences. So there's a research program. There's a research program here. So I like

143
00:13:59,460 --> 00:14:02,140
to think of the matter problem as something that can play that role. I talked about earlier

144
00:14:02,140 --> 00:14:06,060
if you say an AI researcher thinking about this. The matter problem is an easy problem,

145
00:14:06,060 --> 00:14:10,540
a problem about behavior that's closely tied to the hard problem. So it's something we

146
00:14:10,540 --> 00:14:14,740
might be able to make some progress on using standard methods of thinking about algorithms

147
00:14:14,740 --> 00:14:19,940
and computations or thinking about brain processes and behavior while still shedding

148
00:14:19,940 --> 00:14:23,980
some light, at least indirectly on the hard problem. It's more tractable than the hard

149
00:14:23,980 --> 00:14:28,220
problem but solving it ought to shed light on the hard problem. And today I'm just going

150
00:14:28,220 --> 00:14:32,500
to kind of lay out the research program and talk about some ways in which it might potentially

151
00:14:32,500 --> 00:14:38,820
shed some light. This is interesting to a philosopher because it looks like an instance

152
00:14:38,820 --> 00:14:42,940
of what people sometimes call genealogical analysis. It goes back to Friedrich Nietzsche

153
00:14:42,940 --> 00:14:47,780
on the genealogy of morals. Instead of thinking about what's good or bad, let's look at where

154
00:14:47,780 --> 00:14:54,660
our sense of good or bad came from, the genealogy of it all in evolution or in culture or in

155
00:14:54,660 --> 00:15:00,340
religion. And people take a genealogical approach to God. Instead of thinking about does God

156
00:15:00,340 --> 00:15:04,860
exist or not, let's look at where our belief in God came from. Maybe there's some evolutionary

157
00:15:04,860 --> 00:15:10,780
reason for why people believe in God. This often leads, not always, but often leads to

158
00:15:10,780 --> 00:15:16,500
a kind of debunking of our beliefs about those domains. Explain why we believe in God in

159
00:15:16,500 --> 00:15:22,500
evolutionary terms. No need for the God hypothesis anymore. Explain our moral beliefs in say

160
00:15:22,500 --> 00:15:28,140
evolutionary terms. Maybe no need to take morality quite so seriously. So some people

161
00:15:28,140 --> 00:15:33,020
at least are inclined to take an approach like this with consciousness too. If you think

162
00:15:33,020 --> 00:15:37,660
about the meta problem, explaining our beliefs about consciousness, that might ultimately

163
00:15:37,660 --> 00:15:45,220
debunk our beliefs about consciousness. This leads to a philosophical view which has recently

164
00:15:45,260 --> 00:15:51,340
attracted a lot of interest, a philosophical view called illusionism, which is the view

165
00:15:51,340 --> 00:15:57,780
that consciousness itself is an illusion or maybe that the problem of consciousness is

166
00:15:57,780 --> 00:16:05,140
an illusion. Explain the illusion and we dissolve the problem. In terms of the meta

167
00:16:05,140 --> 00:16:10,980
problem that view roughly comes to, solve the meta problem, it will dissolve the hard

168
00:16:11,020 --> 00:16:15,420
problem. Explain why it is that we say all these things about consciousness. While we

169
00:16:15,420 --> 00:16:20,940
say I am conscious, while we say consciousness is puzzling, if you can explain all that in

170
00:16:20,940 --> 00:16:27,100
say algorithmic terms, then you'll remove the underlying problem because you'll have

171
00:16:27,100 --> 00:16:31,500
explained why we're puzzled in the first place. Actually, walking over here today, I noticed

172
00:16:31,500 --> 00:16:36,140
that just a couple of blocks away, we have the Museum of Illusions. So I'm going to check

173
00:16:36,140 --> 00:16:40,900
that out later on. But if illusionism is right and added to all those perceptual illusions,

174
00:16:41,260 --> 00:16:46,220
it's going to be the problem of consciousness itself. It's roughly an illusion thrown up

175
00:16:46,220 --> 00:16:51,700
by having a weird self-model with a certain kind of algorithm that attributes to ourselves

176
00:16:51,700 --> 00:16:58,700
special properties that we don't have. So one line on the meta problem is the illusionist

177
00:16:58,700 --> 00:17:03,820
line. Solve the meta problem, you'll get to treat consciousness as an illusion. That's

178
00:17:03,820 --> 00:17:09,540
actually a view that has many antecedents in the history of philosophy one way or another.

179
00:17:09,580 --> 00:17:13,740
Emmanuel Kahn, in his great critique of pure reason, had a section where he talked about

180
00:17:13,740 --> 00:17:21,020
the self or the soul as a transcendental illusion. We seem to have this indivisible soul, but

181
00:17:21,020 --> 00:17:25,740
that's a kind of illusion thrown up by our cognitive processes. The Australian philosophers

182
00:17:25,740 --> 00:17:31,860
are on place, and David Armstrong had versions of this that I might touch on a bit later.

183
00:17:31,860 --> 00:17:38,620
Daniel Dennett, leading reductionist thinker about consciousness, has been pushing for the

184
00:17:38,660 --> 00:17:43,940
last couple of decades the idea that consciousness involves a certain kind of user illusion,

185
00:17:43,940 --> 00:17:50,140
and most recently the British philosopher Keith Frankish has been really pushing illusionism

186
00:17:50,140 --> 00:17:57,140
as a theory of consciousness. Here's a book centering around a paper by Keith Frankish

187
00:17:58,100 --> 00:18:03,580
on illusionism as a theory of consciousness that I recommend to you. So one way to go

188
00:18:03,580 --> 00:18:07,580
with the meta problem is the direction of illusionism, but one nice thing about many

189
00:18:07,620 --> 00:18:12,340
people find illusionism completely unbelievable. They find you, how could it be that consciousness

190
00:18:12,340 --> 00:18:16,900
is an illusion? Look, we just have the subjective experience, it's a datum about our nature,

191
00:18:16,900 --> 00:18:22,460
and I confess I've got some sympathy with that reaction, so I'm not an illusionist myself.

192
00:18:22,460 --> 00:18:26,780
I'm a realist about consciousness in the philosopher's sense where a realist about

193
00:18:26,780 --> 00:18:32,260
something is someone who believes that thing is real. I think consciousness is real, I

194
00:18:32,260 --> 00:18:37,260
think it's not an illusion, I think that solving the meta problem does not dissolve

195
00:18:37,260 --> 00:18:40,540
the hard problem, but the nice thing about the meta problem is you can proceed on it

196
00:18:40,540 --> 00:18:46,740
to some extent at least in initial neutrality on that question, is consciousness real or

197
00:18:46,740 --> 00:18:52,620
is it an illusion? It's a basic problem about our objective functioning of these reports.

198
00:18:52,620 --> 00:18:59,620
What explains those? There's a neutral research program here that both realists, illusionists,

199
00:19:00,020 --> 00:19:03,200
people of all kinds of different views of consciousness can explain, and then we can

200
00:19:03,200 --> 00:19:09,320
come back and look at the philosophical consequences. Well, I'm not an illusionist, I think consciousness

201
00:19:09,320 --> 00:19:14,440
is real. I've got to say, I do feel the temptation of illusionism, I find it really intriguing

202
00:19:14,440 --> 00:19:20,480
and in some ways attractive to you, just fundamentally unbelievable. Nevertheless, I think that the

203
00:19:20,480 --> 00:19:26,960
meta problem should be a tractable problem. Solving it will shed, at the very least, will

204
00:19:26,960 --> 00:19:31,720
shed much light on the hard problem of consciousness, even if it doesn't solve it. If you can explain

205
00:19:31,760 --> 00:19:36,480
our conviction that we're conscious, somehow the source, the roots of our conviction that

206
00:19:36,480 --> 00:19:41,280
we're conscious must have something to do with consciousness, especially if consciousness

207
00:19:41,280 --> 00:19:47,720
is real. I think it's very much a good research program for people to explain. Then I'll move

208
00:19:47,720 --> 00:19:53,880
on now to just outlining the research program a little bit more and then talk a bit about

209
00:19:53,880 --> 00:19:59,120
potential solutions and on impact on theories of consciousness before wrapping up with just

210
00:19:59,200 --> 00:20:06,200
a little bit more about illusionism. This meta problem, which I've been pushing recently,

211
00:20:06,920 --> 00:20:12,920
opens up a tractable empirical research program for everyone, reductionists, non-reductionists,

212
00:20:12,920 --> 00:20:18,420
illusionists, non-illusionists. We can try to solve it and then think about the philosophical

213
00:20:18,420 --> 00:20:25,420
consequences. What is the meta problem? Well, the way I'm going to put it is it's the problem

214
00:20:26,420 --> 00:20:33,420
of topic neutrally explaining problem intuitions or else explaining why that can't be done.

215
00:20:34,420 --> 00:20:41,420
I'll unpack all the pieces of that right now, first starting with problem intuitions. What

216
00:20:41,820 --> 00:20:47,260
are problem intuitions? Well, those are the things we say. There are things we think.

217
00:20:47,260 --> 00:20:51,780
I say consciousness seems irreducible. I might think consciousness is irreducible. People

218
00:20:51,780 --> 00:20:56,700
might be disposed, have a tendency to say or think those things. Problem intuitions all

219
00:20:56,700 --> 00:21:02,820
take to be roughly that tendency. We have dispositions to say and think certain things

220
00:21:02,820 --> 00:21:08,580
about consciousness. What are the core problem intuitions? Well, I think they break down into

221
00:21:08,580 --> 00:21:13,100
a number of different kinds. There's the intuition that consciousness is non-physical. We might

222
00:21:13,100 --> 00:21:17,660
think of that as a metaphysical intuition about the nature of consciousness. There are

223
00:21:17,700 --> 00:21:23,300
intuitions about explanation. Consciousness is hard to explain. Explaining behavior doesn't

224
00:21:23,300 --> 00:21:28,180
explain consciousness. There are intuitions about knowledge of consciousness. Some of

225
00:21:28,180 --> 00:21:32,460
you may know the famous thought experiment of Mary in the black and white room who knows

226
00:21:32,460 --> 00:21:37,380
all about the objective nature of color vision and so on, but still doesn't know what it's

227
00:21:37,380 --> 00:21:41,700
like to see red. She sees red for the first time. She learns something new. That's an

228
00:21:41,700 --> 00:21:46,140
intuition about knowledge of consciousness. There are what philosophers call modal intuitions

229
00:21:46,140 --> 00:21:52,060
about what's possible or imaginable. One famous case is the case of a zombie, a creature

230
00:21:52,060 --> 00:21:57,460
who's physically identical to you and me, but not conscious, or maybe an AI system which

231
00:21:57,460 --> 00:22:02,460
is functionally identical to you and me, but not conscious. That at least seems conceivable

232
00:22:02,460 --> 00:22:08,660
to many people. This is the philosophical zombie, unlike the zombies in movies which

233
00:22:08,660 --> 00:22:13,380
have weird behaviors and go after brains and so on. The philosophical zombie is a creature

234
00:22:13,500 --> 00:22:18,900
that seems, at least behaviorally, maybe physically like a normal human, but doesn't have any

235
00:22:18,900 --> 00:22:23,980
conscious experiences. All the physical states, none of the mental states. It seems to many

236
00:22:23,980 --> 00:22:28,260
people that's at least conceivable. We're not zombies. I don't think anyone here is

237
00:22:28,260 --> 00:22:33,420
a zombie, I hope, but nonetheless, it seems that we can make sense of the idea and one

238
00:22:33,420 --> 00:22:38,300
way to pose the hard problem is why are we not zombies. This imaginability of zombies

239
00:22:38,300 --> 00:22:43,220
is one of the intuitions that gets the problem going. Then you can go on and catalog more

240
00:22:43,540 --> 00:22:48,300
and more intuitions about the distribution of conscious, maybe the intuition that robots

241
00:22:48,300 --> 00:22:53,580
won't be conscious. That's an optional one, I think, or consciousness matters morally

242
00:22:53,580 --> 00:23:00,080
in certain ways and the list goes on. I think there's an interdisciplinary research program

243
00:23:00,080 --> 00:23:06,180
here of working on those intuitions about consciousness and trying to explain them.

244
00:23:06,180 --> 00:23:11,460
Experimental psychology and experimental philosophy and newly active area can study people's intuitions

245
00:23:11,500 --> 00:23:16,540
about consciousness. We can work on models of these things, computational models or neurobiological

246
00:23:16,540 --> 00:23:20,620
models of these intuitions and reports, and indeed, I think there's a lot of room for

247
00:23:20,620 --> 00:23:25,460
philosophical analysis. There's just starting to be a program of people doing these things

248
00:23:25,460 --> 00:23:32,460
in all these fields. It is an empirical question how widely these intuitions are shared. You

249
00:23:32,460 --> 00:23:35,740
might be sitting there thinking, come on, I don't have any of these intuitions. Maybe

250
00:23:35,740 --> 00:23:40,740
this is just you. My sense is from the psychological study to date, it seems that some of these

251
00:23:40,740 --> 00:23:45,940
intuitions about consciousness are at least very widely shared, at least as dispositions

252
00:23:45,940 --> 00:23:51,940
or intuitions, although they're often overridden on reflection. The current data on this is

253
00:23:51,940 --> 00:23:58,940
somewhat limited. There is a lot of empirical work on intuitions about the mind concerning

254
00:23:58,980 --> 00:24:03,020
things like belief, like when do kids get the idea that your belief's about the world,

255
00:24:03,020 --> 00:24:08,340
can be false, concerning the way your self persists through time, could you exist after

256
00:24:08,340 --> 00:24:12,180
the death of your body? Well, consciousness is concerned. There's work on the distribution

257
00:24:12,180 --> 00:24:15,860
of consciousness. Could a robot be conscious? Could a group be conscious? Here's a book

258
00:24:15,860 --> 00:24:19,980
by Paul Bloom, Descartes' Baby. The catalog's a lot of this interesting work, making the

259
00:24:19,980 --> 00:24:26,340
case that many children are intuitive dualists. They're naturally inclined to think there's

260
00:24:26,340 --> 00:24:30,980
something non-physical about the mind. So far, most of this work has not been so much

261
00:24:30,980 --> 00:24:37,220
on these core problem intuitions about consciousness, but there's work developing in this direction.

262
00:24:37,260 --> 00:24:42,060
Sarah Gottlieb and Tanya Lombrozo have a very recent article called Can Science Explain

263
00:24:42,060 --> 00:24:48,340
the Human Mind on People's Judgements about when various mental phenomena are hard to

264
00:24:48,340 --> 00:24:52,540
explain and they seem to find that, yes, subjective experience and things which have, to which

265
00:24:52,540 --> 00:24:58,420
people have privileged first person access seem to pose the problem big time. So there's

266
00:24:58,420 --> 00:25:03,660
the beginning of a research program here. I think there's room for a lot more.

267
00:25:03,660 --> 00:25:08,700
The topic neutrality part, when I say we're looking for a topic neutral explanation of

268
00:25:08,700 --> 00:25:13,740
problem intuitions, that's roughly to say an explanation that doesn't mention consciousness

269
00:25:13,740 --> 00:25:18,700
itself. It's put in neutral terms. It's neutral on the existence of consciousness. The most

270
00:25:18,700 --> 00:25:23,700
obvious one would be something like an algorithmic explanation. I can say, here is the algorithm.

271
00:25:23,700 --> 00:25:29,260
The brain is executing. It generates our conviction that we're conscious and our reports about

272
00:25:29,300 --> 00:25:33,620
consciousness. There may be some time between that algorithm and consciousness, but to specify

273
00:25:33,620 --> 00:25:39,820
the algorithm, you don't need to make claims about consciousness. So the algorithmic version

274
00:25:39,820 --> 00:25:45,340
of the metaproblem is roughly find the algorithm that generates our problem intuition. So that's

275
00:25:45,340 --> 00:25:54,340
I think a principal research program that maybe AI researchers in combination would say psychologists,

276
00:25:54,900 --> 00:26:00,500
the psychologist could help isolate data about the way that the human beings are doing it,

277
00:26:00,500 --> 00:26:04,820
how these things are generated in humans and AI researchers can try and see about implementing

278
00:26:04,820 --> 00:26:09,380
that algorithm in machines and see what results. And I'll talk about a little bit of research

279
00:26:09,380 --> 00:26:15,340
in this direction in just a moment. But okay, now I want to say something about potential

280
00:26:15,340 --> 00:26:20,100
solutions to the problem. Like I say, this is a big research program. I don't claim to

281
00:26:20,100 --> 00:26:24,300
have the solution to the metaproblem. I've got some ideas, but I'm not going to try and

282
00:26:24,300 --> 00:26:30,700
lay out a major solution or just so here are a few things which I think might be parts

283
00:26:30,700 --> 00:26:35,780
of a solution to the problem, many of which have got antecedents here and there in scientific

284
00:26:35,780 --> 00:26:41,220
and philosophical discussion. Some promising ideas include retrospective models, phenomenal

285
00:26:41,220 --> 00:26:46,140
concepts, introspective opacity, the sense of acquaintance. Let me just say something

286
00:26:46,140 --> 00:26:52,300
about a few of these. One starting idea that almost anyone's going to have here is somehow

287
00:26:52,300 --> 00:26:59,100
models of ourselves are playing a central role here. Human beings have models of the

288
00:26:59,100 --> 00:27:06,020
world, naive physics and naive psychology, models of other people and so on. We also

289
00:27:06,020 --> 00:27:10,660
have models of ourselves. It makes sense for us to have models of ourselves and our own

290
00:27:10,660 --> 00:27:15,660
mental processes. This is something that the psychologist Michael Graziano has written

291
00:27:15,660 --> 00:27:21,620
a lot on. We have internal models of our own cognitive processes, including those tied

292
00:27:21,660 --> 00:27:27,660
to consciousness. Somehow, something about our introspective models explains our sense,

293
00:27:27,660 --> 00:27:32,980
A, that we are conscious and B, that this is distinctively problematic. I think anyone

294
00:27:32,980 --> 00:27:38,180
thinking about the metaproblem, this has got to be at least the first step. We have these

295
00:27:38,180 --> 00:27:42,260
introspective models. If you're an illusionist, they'll be false models. If you're a realist,

296
00:27:42,260 --> 00:27:48,340
they needn't be false models, but at the very least, these introspective models are involved,

297
00:27:48,380 --> 00:27:53,860
which is fine, but the devil's in the details. How do they work to generate this problem?

298
00:27:53,860 --> 00:27:58,780
A number of philosophers have argued we have special concepts of consciousness, introspective

299
00:27:58,780 --> 00:28:04,380
concepts of these special subjective states. People call these phenomenal concepts, concepts

300
00:28:04,380 --> 00:28:10,460
of phenomenal consciousness. One thing that's special is these concepts are somehow independent

301
00:28:10,460 --> 00:28:17,460
of our physical concepts. They explain, we've got one set of physical concepts for modeling

302
00:28:17,460 --> 00:28:21,420
the external work world. We've got one set of introspective concepts for modeling our

303
00:28:21,420 --> 00:28:25,220
own mind. These concepts, just by virtue of the way they're designed, are somewhat independent

304
00:28:25,220 --> 00:28:31,340
of each other, and that partly explains why consciousness seems to be independent of

305
00:28:31,340 --> 00:28:36,980
the physical world, intuitively. Maybe that independence of phenomenal concepts could

306
00:28:36,980 --> 00:28:41,660
go some distance to explaining our problem report. I think there's got to be something

307
00:28:41,660 --> 00:28:47,380
to this as well. At the same time, I don't think this goes nearly far enough because

308
00:28:47,860 --> 00:28:52,460
we have concepts of many aspects of the mind, not just of the subjective experiential past,

309
00:28:52,460 --> 00:28:58,180
but things we believe and things we desire. One, I believe that Paris is the capital of

310
00:28:58,180 --> 00:29:04,300
France. That's part of my internal self-model, but that doesn't seem to generate the hard

311
00:29:04,300 --> 00:29:09,060
problem in nearly the same way in which, say, the experience of red does. A lot more needs

312
00:29:09,060 --> 00:29:14,380
to be said about what's going on in cases like having the experience of red and having

313
00:29:14,380 --> 00:29:19,060
the sense that that generates a gap, so it doesn't generalize to everything about the

314
00:29:19,060 --> 00:29:25,260
mind. Some people have thought that what we might call introspective opacity plays a

315
00:29:25,260 --> 00:29:31,020
role, that when we introspect what's going on in our minds, we don't have access to the

316
00:29:31,020 --> 00:29:35,540
underlying physical states. We don't see the neurons in our brains. We don't see that

317
00:29:35,540 --> 00:29:42,300
consciousness as physical, so we see it as non-physical. Most recently, the physicist

318
00:29:42,340 --> 00:29:45,740
Max Tegmark has argued in this direction, saying, somehow, consciousness is substrate

319
00:29:45,740 --> 00:29:50,500
independent. We don't see the substrate, so then we think, ah, maybe it can float free

320
00:29:50,500 --> 00:29:59,020
of the substrate. Armstrong made an analogy with the case of someone in a circus where

321
00:29:59,020 --> 00:30:04,820
the headless person illusion where you don't see someone's there with a veil across their

322
00:30:04,820 --> 00:30:12,140
head. You don't see their head, so you see them as having no head. Here is a 19th century

323
00:30:12,180 --> 00:30:16,180
booth at a circus, a so-called headless woman with a veil over her head. You don't see the

324
00:30:16,180 --> 00:30:20,300
head, so somehow it looks at least for a moment like the person doesn't have a head. Armstrong

325
00:30:20,300 --> 00:30:25,100
says, maybe that's how it is with consciousness. You don't see that it's physical, so you

326
00:30:25,100 --> 00:30:32,820
see it as non-physical. But still, the question comes up, how do we make this inference? There's

327
00:30:32,820 --> 00:30:38,340
something that special goes on in cases like, say, color and taste and so on. Color experience

328
00:30:38,340 --> 00:30:44,300
seems to attribute primitive properties to objects like redness, greenness and so on.

329
00:30:44,300 --> 00:30:48,580
But in fact, in the external world, at the very least, they have complex, reducible

330
00:30:48,580 --> 00:30:55,020
properties. Somehow, internal models of color treat colors like red and green as if they

331
00:30:55,020 --> 00:31:00,580
are primitive things. It turns out to be useful to have these models of things where you treat

332
00:31:00,580 --> 00:31:04,780
certain things as primitive even though they're reducible. And it sure seems that when we

333
00:31:04,780 --> 00:31:10,500
experience colors, we experience greenness as a primitive quality, even though it may

334
00:31:10,500 --> 00:31:15,860
be a very, very complex reducible property. That's something about our model of colors.

335
00:31:15,860 --> 00:31:21,180
The philosopher Wolfgang Schwartz tried to make an analogy with sensor variables in,

336
00:31:21,180 --> 00:31:26,660
say, image processing. You've got some visual sensors and a camera or something. You need

337
00:31:26,660 --> 00:31:32,300
to process the image. Well, you've got some variables, some sensor variables to represent

338
00:31:32,300 --> 00:31:35,620
the sensory inputs that the various sensors are getting. And you might treat them as a

339
00:31:35,620 --> 00:31:39,100
primitive dimension because that's the most useful way to treat them. You don't treat

340
00:31:39,100 --> 00:31:44,420
them as certain amounts of lights or photons firing. You don't need to know about that.

341
00:31:44,420 --> 00:31:49,500
Use these sensor variables and treat them as a primitive dimension. And all that will

342
00:31:49,500 --> 00:31:54,260
play into a model of these things as primitive. Maybe taking that idea and extending it to

343
00:31:54,260 --> 00:32:00,820
introspection, you know, somehow these conscious states are somehow like sensor variables in

344
00:32:00,820 --> 00:32:07,420
our model of the mind. And somehow these internal models give us the sense of being acquainted

345
00:32:07,420 --> 00:32:13,180
with primitive concrete qualities and of our awareness of them. This is still just laying

346
00:32:13,180 --> 00:32:17,140
out. I don't think this is still yet actually explaining a whole lot, but it's laying out.

347
00:32:17,140 --> 00:32:22,220
It's narrowing down what it is that we need to explain to solve the meta problem. But

348
00:32:22,220 --> 00:32:25,820
just to put the pieces together, here's a little summary. One thing I like about this

349
00:32:25,820 --> 00:32:32,960
summary is you can read it in either an illusionist tone of voice as an account of the illusion

350
00:32:32,960 --> 00:32:38,220
of consciousness. All this is how our false introspective models work or in a realist

351
00:32:38,220 --> 00:32:45,540
tone of voice as an account of our true, correct models of consciousness. But we can set it

352
00:32:45,540 --> 00:32:49,140
out in a way which is neutral on the two and then try and figure out later whether these

353
00:32:49,140 --> 00:32:55,220
models are correct as the realist says or incorrect as the illusionist says. We have

354
00:32:55,220 --> 00:33:02,620
introspective models deploying introspective concepts of our internal states that are largely

355
00:33:02,620 --> 00:33:09,180
independent of our physical concepts. These concepts are introspectively opaque, not revealing

356
00:33:09,180 --> 00:33:15,540
any of the underlying mechanisms, our perceptual models perceptually attribute primitive perceptual

357
00:33:15,540 --> 00:33:22,700
qualities to the world, and our introspective models attribute primitive mental relations

358
00:33:22,780 --> 00:33:29,980
to those qualities. These models produce the sense of acquaintance both with those qualities

359
00:33:29,980 --> 00:33:35,700
and with our awareness of those qualities. Like I said, this is not a solution to the

360
00:33:35,700 --> 00:33:40,580
meta problem, but it's trying at least to pin down some parts of the roots of those

361
00:33:40,580 --> 00:33:46,060
intuitions and to narrow down what needs to be explained. To go further, you want, I think,

362
00:33:46,060 --> 00:33:51,580
to test these explanations, both with psychological studies to see if this is plausibly what's

363
00:33:51,580 --> 00:33:56,340
going on in humans. This is the kind of thing which is the basis of our intuitions and computational

364
00:33:56,340 --> 00:34:00,500
models to see if, for example, you could program this kind of thing into an AI system and see

365
00:34:00,500 --> 00:34:07,140
if it can generate somehow qualitatively similar reports and intuitions. You might think that

366
00:34:07,140 --> 00:34:12,500
last thing is a bit far-fetched right now, but I know of at least one instance of this

367
00:34:12,500 --> 00:34:18,660
research program which has been put into play by Luke Milhauser and Bach Schleggeres, two

368
00:34:18,700 --> 00:34:26,860
researchers at Open Philanthropy, very interested in AI and consciousness. They actually built,

369
00:34:26,860 --> 00:34:31,300
they took some ideas about the meta problem from something I'd written about it and from

370
00:34:31,300 --> 00:34:37,900
something that the philosopher Francois Camero had written about it. A couple of basic ideas about

371
00:34:37,900 --> 00:34:43,460
where problem intuitions might come from, and they tried to build them in to a computational

372
00:34:44,060 --> 00:34:52,980
model. They built a little software agent which had certain axioms about colors and how they

373
00:34:52,980 --> 00:34:58,500
work. There's red and there's green, and certain axioms about their own subjective experiences

374
00:34:58,500 --> 00:35:06,180
of colors, and then they combined it with a little theorem prover, and they saw what did

375
00:35:06,180 --> 00:35:10,140
this little software agent come up with, and it came up with claims like, hey, well, my

376
00:35:11,100 --> 00:35:16,620
experiences of color are distinct from any physical state, and so on. They cut a few corners.

377
00:35:18,700 --> 00:35:25,820
This is not yet a truly convincing, sophisticated model of everything going on in the human mind,

378
00:35:25,820 --> 00:35:31,420
but it shows that there's a research program here of trying to find the algorithmic basis

379
00:35:32,140 --> 00:35:37,260
of these states. I think as more sophisticated models develop, we might be able to use these

380
00:35:37,340 --> 00:35:41,900
to provide a way in for AI researchers in thinking about this topic. Of course,

381
00:35:41,900 --> 00:35:46,220
there is the question, if you model all this stuff better and better in a machine,

382
00:35:46,220 --> 00:35:51,020
then is the machine actually going to be conscious, or is it just going to have found

383
00:35:51,020 --> 00:35:58,460
self-models that replicate what's going on in humans? Some people have proposed an artificial

384
00:35:58,460 --> 00:36:06,140
consciousness test. Aaron Sloman, Susan Snyder, and Turner have suggested somehow that if a machine

385
00:36:06,220 --> 00:36:10,380
seems to be puzzled about consciousness in roughly the ways that we are,

386
00:36:10,380 --> 00:36:16,860
maybe that's actually a sign that it's conscious. If a machine actually looks to ask

387
00:36:16,860 --> 00:36:22,140
as if it's puzzled by consciousness, is that a sign of consciousness? These people,

388
00:36:22,140 --> 00:36:26,620
this is suggested as a kind of Turing test for machine consciousness. Find machines which are

389
00:36:26,620 --> 00:36:30,140
conscious like we are. Of course, the opposing point of view is going to be, no, the machine is

390
00:36:30,140 --> 00:36:35,340
not actually conscious. It's just like the machine that studied up for the Turing test by reading

391
00:36:35,340 --> 00:36:41,260
the talk like a human book. It's like, damn, do I really need to convince those humans that I'm

392
00:36:41,260 --> 00:36:47,420
conscious by replicating all those ill-conceived confusions about consciousness? Well, I guess

393
00:36:47,420 --> 00:36:53,340
I can do it if I need to. Anyway, I'm not going to settle this question here, but I do think that

394
00:36:53,340 --> 00:36:59,260
if we somehow find machines being puzzled, it won't surprise me that once we actually have

395
00:36:59,260 --> 00:37:04,300
serious AI systems which engage in natural language and modeling of themselves in the world,

396
00:37:05,420 --> 00:37:09,020
they might well be natural to find themselves saying things like, yeah, I know in principle I'm

397
00:37:09,020 --> 00:37:16,460
just a set of silicon circuits, but I feel like so much more. I think that might tell us something

398
00:37:16,460 --> 00:37:23,580
about consciousness. Let me just say a little bit about theories of consciousness. I do think a

399
00:37:23,580 --> 00:37:29,020
solution to the meta problem and a solution to the hard problem ought to be closely connected.

400
00:37:29,020 --> 00:37:32,860
The illusionist says, solve the meta problem. You'll dissolve the hard problem. But even if

401
00:37:32,860 --> 00:37:39,340
you're not an illusionist about consciousness, there ought to be some link. So here's a thesis.

402
00:37:39,340 --> 00:37:45,660
Whatever explains consciousness should also partly explain our judgments and our reports

403
00:37:46,300 --> 00:37:50,620
about consciousness. The rationale here is it would just be very strange if these things were

404
00:37:51,180 --> 00:37:58,620
independent. If the basis of consciousness played no role in our judgments about consciousness.

405
00:37:59,340 --> 00:38:05,820
So I think you can use this as a way of evaluating or testing theories of consciousness. For theory

406
00:38:05,820 --> 00:38:11,900
of consciousness, there's mechanism M is the basis of consciousness, that M should also partly

407
00:38:11,900 --> 00:38:18,700
explain our judgments about consciousness. Whatever the basis is ought to explain the reports.

408
00:38:18,700 --> 00:38:24,380
And you can use this. You can bring this to bear on various extant theories of consciousness. Here's

409
00:38:24,460 --> 00:38:30,060
one famous current theory of consciousness, integrated information theory developed by

410
00:38:30,060 --> 00:38:38,700
Giulio Tononi and colleagues at the University of Wisconsin. Tononi says the basis of consciousness

411
00:38:38,700 --> 00:38:45,260
is integrated information. A certain kind of integration of information for which Tononi

412
00:38:45,260 --> 00:38:50,860
has a measure that he calls phi. Basically, when your phi is high enough, you get consciousness.

413
00:38:50,940 --> 00:38:56,220
So consciousness is high phi. And there's a mathematical definition, but I won't go into

414
00:38:56,220 --> 00:39:02,540
it here. But such a really interesting theory. So here's a, basically it analyzes a network

415
00:39:02,540 --> 00:39:08,140
property of systems, of units, and it's got an informational measure called phi that's supposed

416
00:39:08,140 --> 00:39:14,700
to go with consciousness. Question. How does, if integrated information is the basis of consciousness,

417
00:39:14,700 --> 00:39:20,060
it ought to explain problem reports, at least in principle. Challenge. How does that work?

418
00:39:20,060 --> 00:39:25,180
And it's at least far from obvious to me how integrated information will explain the problem

419
00:39:25,180 --> 00:39:31,580
reports. It seems pretty dissociated from them. I mean, on Tononi's view, you can have simulations

420
00:39:32,220 --> 00:39:38,460
of systems with high phi that have zero phi. They'll go about making exactly the same reports,

421
00:39:38,460 --> 00:39:43,740
but without consciousness at all. So phi is at least somewhat dissociable. You get systems

422
00:39:43,820 --> 00:39:50,460
very high phi, but no tendency to report. Maybe that's less worrying. Anyway, here's a challenge

423
00:39:50,460 --> 00:39:55,020
for this theory, for other theories. Explain not just how high phi gives you consciousness,

424
00:39:55,020 --> 00:40:01,100
but how it plays a central role in the algorithms that generate problem reports. Something similar

425
00:40:01,100 --> 00:40:07,740
goes for many other theories, biological theories, quantum theories, global workspace, and so on.

426
00:40:08,540 --> 00:40:13,500
But let me just wrap up by saying something about the issue of illusionism that I was

427
00:40:13,740 --> 00:40:19,020
talking about near the start. Again, you might be inclined to think that this approach through

428
00:40:19,020 --> 00:40:23,980
the meta problem tends, at least very naturally, to lead to illusionism. And I think it can be,

429
00:40:24,620 --> 00:40:29,740
it certainly provides, I think, some motivation for illusionism. The view that consciousness

430
00:40:29,740 --> 00:40:36,220
doesn't exist, we just think it does. On this view, again, a solution to the meta problem dissolves

431
00:40:36,780 --> 00:40:43,580
the hard problem. So here's one way of putting the case for illusionism. If there's a solution

432
00:40:43,580 --> 00:40:48,700
to the meta problem, then there's an explanation of our beliefs about consciousness that's

433
00:40:48,700 --> 00:40:52,860
independent of consciousness. There's an algorithm that explains our beliefs about

434
00:40:52,860 --> 00:40:57,260
consciousness, doesn't mention consciousness, arguably could be in place without consciousness.

435
00:40:57,820 --> 00:41:04,300
Arguably, that kind of explanation could debunk our beliefs about consciousness the same way that,

436
00:41:04,300 --> 00:41:10,540
perhaps, explaining beliefs about God in evolutionary terms might debunk belief in God.

437
00:41:10,540 --> 00:41:14,220
It certainly doesn't prove that God doesn't exist. You might think that if you can explain

438
00:41:14,220 --> 00:41:20,300
our beliefs in terms of evolution, it somehow removes the justification or the rational basis

439
00:41:20,300 --> 00:41:24,780
for those beliefs. So something like that, I think, can be applied to consciousness, too. And

440
00:41:24,780 --> 00:41:30,460
there's a lot to be said about analyzing the extent to which this might debunk the beliefs.

441
00:41:30,460 --> 00:41:36,140
On the other hand, the case against illusionism is very, very strong for many people. And the

442
00:41:36,140 --> 00:41:40,620
underlying worry is that some illusionism is completely unbelievable. It's just a manifest

443
00:41:40,620 --> 00:41:47,420
fact about ourselves that we have conscious experience, we experience red, we feel pain,

444
00:41:47,420 --> 00:41:53,740
and so on. To deny those things is to deny the data. Now, the dialectic here is complicated.

445
00:41:53,740 --> 00:41:58,620
The illusionist will come back and say, yes, but I can explain why illusionism is unbelievable.

446
00:41:58,620 --> 00:42:03,180
These models we have, these self-models of consciousness, are so strong that they're

447
00:42:03,260 --> 00:42:08,380
just wired into us by evolution, and they're not models we can get rid of. So my view predicts

448
00:42:08,380 --> 00:42:14,780
that my view is unbelievable. And the question is what, the dialectical situation is complex

449
00:42:14,780 --> 00:42:21,340
and interesting. But maybe I could just wrap up with two expressions of absurdity on either side

450
00:42:21,340 --> 00:42:28,860
of this question, the illusionist and the anti-illusionist, both finding absurdity

451
00:42:28,940 --> 00:42:36,140
in the other person's views. Here's Galen Strawson, who is here. Galen's view is very much that

452
00:42:36,140 --> 00:42:41,180
illusionism is totally absurd. In fact, he thinks it's the most absurd view that anyone has ever

453
00:42:41,180 --> 00:42:47,180
held. There occurred in the 20th century, the most remarkable episode in the whole history of ideas,

454
00:42:47,180 --> 00:42:51,980
the whole history of human thought, a number of thinkers denied the existence of something we

455
00:42:51,980 --> 00:42:57,260
know with certainty to exist, consciousness. He thinks this is just a sign of incredible

456
00:42:57,260 --> 00:43:03,260
philosophical pathology. Here's the rationalist philosopher Eliezer Yudkowski in something he

457
00:43:03,260 --> 00:43:10,620
wrote a few years ago on zombies and consciousness, and the view, the epiphenomenalist view that

458
00:43:10,620 --> 00:43:15,660
consciousness plays no causal role, where he was engaging some stuff I wrote a couple of decades

459
00:43:15,660 --> 00:43:19,900
ago. He said, this is a zombie argument, the idea we can imagine zombies physically like us,

460
00:43:19,900 --> 00:43:25,740
but without consciousness. Maybe a candidate for the most deranged idea in all of philosophy.

461
00:43:26,300 --> 00:43:33,980
The causally closed cognitive system of charmer's internal narrative is malfunctioning in a way that

462
00:43:33,980 --> 00:43:40,620
not by necessity, but just in our own universe miraculously happens to be correct. Here he's

463
00:43:40,620 --> 00:43:45,100
expressing this debunking idea that on this view, there's an algorithm that generates these

464
00:43:45,100 --> 00:43:49,500
intuitions about consciousness, and that's all physical. There's also this further layer of

465
00:43:49,500 --> 00:43:57,260
non-physical stuff, and just by massive coincidence, the physical algorithm is a correct model of the

466
00:43:57,260 --> 00:44:04,220
non-physical stuff. That's a form of debunking here. It would take a miracle for this view to be

467
00:44:04,220 --> 00:44:09,900
correct. So I think both of these views are onto, these objections are onto something, and to make

468
00:44:09,900 --> 00:44:15,260
progress on this, when I decide we need to find a way of getting past these absurdities. I mean,

469
00:44:15,260 --> 00:44:20,780
you might say, well, there's middle ground between very strong illusionism and very strong epiphenomenalism.

470
00:44:20,780 --> 00:44:27,100
It tends to slide back to the same problems. Other forms of illusionism, weaker forms don't

471
00:44:27,100 --> 00:44:32,700
help much with the higher problem. Other forms of realism are still subject to this. It takes a

472
00:44:32,700 --> 00:44:40,380
miracle for this view to be correct. Critique. So I think to get beyond absurdity here, both sides

473
00:44:40,380 --> 00:44:45,100
need to do something more. The illusionist needs to do more to explain how having a mind could

474
00:44:45,100 --> 00:44:51,260
be like this, somehow just like this, even though it's not at all the way that it seems. They need

475
00:44:51,260 --> 00:44:58,300
to find some way to recapture the data. Realists need to explain how it is that these meta-problem

476
00:44:58,300 --> 00:45:03,020
processes are not completely independent of consciousness. Realists need to explain how

477
00:45:03,020 --> 00:45:08,940
meta-problem processes, the ones that generate these intuitions and reports and convictions

478
00:45:08,940 --> 00:45:13,820
about consciousness are essentially grounded in consciousness, even if it's possible somehow for

479
00:45:13,820 --> 00:45:20,540
them to occur or conceivable for them to occur without consciousness. Anyway, so that's just to

480
00:45:20,540 --> 00:45:27,100
lay out a research program. I think a solution to the meta-problem that meets these ambitions

481
00:45:27,100 --> 00:45:32,060
might just possibly solve the hard problem of consciousness or at the very least,

482
00:45:32,060 --> 00:45:37,580
shed significant light on it. In the meantime, the meta-problem is a potentially tractable

483
00:45:37,580 --> 00:45:40,940
research project for everyone, and mine I recommend to all of you. Thanks.

484
00:45:48,380 --> 00:45:54,540
Yes, I just want to say I think it's very interesting this concept of we have these mental models,

485
00:45:55,100 --> 00:46:02,540
collection of mental models, and that this collection of mental models is consciousness,

486
00:46:02,620 --> 00:46:07,420
basically. Consciousness is defined as a collection of these mental models that we have,

487
00:46:07,420 --> 00:46:12,540
and the problem of consciousness is that we don't understand the physical phenomenon that

488
00:46:12,540 --> 00:46:19,740
causes these mental models or that stimulates these mental models. So we just have this belief

489
00:46:19,740 --> 00:46:28,220
that it's ephemeral or not real or something like that. And if you take that view, then what's

490
00:46:28,220 --> 00:46:35,100
interesting is that you could simulate these mental models, like robot could simulate these

491
00:46:35,100 --> 00:46:42,780
mental models, and you could simulate consciousness as well. And even if the underlying physical

492
00:46:42,780 --> 00:46:47,580
phenomenon that fuels these mental models is different, you know, robots have different

493
00:46:47,580 --> 00:46:53,980
sensors, etc., you could still get the same consciousness effect in both cases.

494
00:46:54,780 --> 00:46:58,620
Yeah, I think that's right. Or at the very least, you ought to be able to get,

495
00:46:58,620 --> 00:47:02,380
it looks like you ought to be able to get the same models at least in a robot. If the models

496
00:47:02,380 --> 00:47:07,900
themselves are something algorithmic, and ought to be, you ought to be able to design a robot

497
00:47:07,900 --> 00:47:13,660
that has at the very least, let's say, isomorphic models in some sense that is conscious. Of course,

498
00:47:13,660 --> 00:47:17,980
it's a further question, at least by my lights, but then the robot will be conscious. And that was

499
00:47:17,980 --> 00:47:21,420
the question I alluded to in talking about the artificial consciousness test. But you might think

500
00:47:21,420 --> 00:47:25,580
that would at least be very good evidence that the robot is conscious. If it's got a model of

501
00:47:25,580 --> 00:47:30,140
consciousness just like ours, it seems very plausible there ought to be a very strong link

502
00:47:30,140 --> 00:47:35,660
between having a model like that and being conscious. I mean, I think probably something

503
00:47:35,660 --> 00:47:39,020
like Ned Block, who was here arguing against machine consciousness, would say, no, no,

504
00:47:39,020 --> 00:47:42,700
the model is not enough. The model has to be built of the right stuff. Say it's got to be

505
00:47:42,700 --> 00:47:46,860
built of biology and so on. But at least by my lights, I think if I have found the AI system

506
00:47:46,860 --> 00:47:52,220
that had a very serious version of our model of consciousness, I take that as a very good reason

507
00:47:52,220 --> 00:48:00,940
to believe it's conscious. In the IIT theory, is there a estimate or plausible estimate for what

508
00:48:00,940 --> 00:48:10,780
the value of phi is for people and for other systems? Basically, no. It's extremely hard to measure

509
00:48:11,340 --> 00:48:17,980
in systems of any size at all. I mean, because the way it's defined involves taking a sum over

510
00:48:17,980 --> 00:48:23,340
every possible partition of a system. It turns out, I mean, A, it's hard to measure in the brain

511
00:48:23,340 --> 00:48:27,820
because you've got to involve the causal dependencies set between different units on neurons. But even

512
00:48:27,820 --> 00:48:34,380
for a pure algorithmic system, you've got like a neural network laid out in front of you,

513
00:48:34,380 --> 00:48:38,220
it's computationally intractable to measure the phi of one of those once they get to bigger than

514
00:48:38,220 --> 00:48:43,580
15 units or so. So, you know, today I'd like to say this is an empirical theory and in principle

515
00:48:43,580 --> 00:48:50,140
empirically testable. But notice the in principle, it's extremely difficult to to to measure phi.

516
00:48:50,140 --> 00:48:59,020
Some people, Scott Aronson, the computer scientist has argued, has tried to put forward counter-examples

517
00:48:59,020 --> 00:49:05,180
to the theory, which are basically very, very simple systems like matrix multipliers that

518
00:49:05,260 --> 00:49:09,900
multiply two large matrices turn out to have enormous phi. Phi is as big as you like if the

519
00:49:09,900 --> 00:49:14,620
matrices are big enough. And therefore, by Tononi's theory, we'll not just be conscious, but as

520
00:49:14,620 --> 00:49:19,980
conscious as a human being. And Aronson put this forward as a reductio ad absurdum of the IIT theory.

521
00:49:19,980 --> 00:49:24,620
I think Tononi basically bit the bullet and said, yeah, yeah, those those matrix multipliers are

522
00:49:24,620 --> 00:49:30,620
actually having some high degree of consciousness. So I think IIT is probably missing at least

523
00:49:30,620 --> 00:49:34,540
missing a few pieces of what's going to be developed. But it's a research program too.

524
00:49:36,060 --> 00:49:41,980
You mentioned belief as an example of something where, you know, this is another mental quality,

525
00:49:41,980 --> 00:49:47,900
but people don't seem to have the same sense that it is very hard to explain. In fact,

526
00:49:48,940 --> 00:49:53,900
it almost seems too easy where people like a belief about something sort of feels like just

527
00:49:53,900 --> 00:49:59,580
how things are. You have to kind of reflect on a belief to notice it as a belief. Do you

528
00:50:00,220 --> 00:50:06,700
think there's also or has there been research kind of related to this question into why is that

529
00:50:06,700 --> 00:50:11,500
different? Like, it seems like another angle of attack on this problem is just like, why doesn't

530
00:50:11,500 --> 00:50:16,460
this generate the same hard problem? Yeah, in terms, I'm not sure if there's been sort of

531
00:50:17,740 --> 00:50:20,860
research from the perspective of the meta problem or a theory of mind. Certainly,

532
00:50:20,860 --> 00:50:25,820
people have thought in their own right, what is the difference between belief and experience

533
00:50:25,820 --> 00:50:31,340
that makes them so different? This goes way back to David Hume, a philosopher a few centuries ago,

534
00:50:31,340 --> 00:50:38,220
who said, you know, basically, perception is vivid. Impressions and ideas. Impressions like

535
00:50:38,220 --> 00:50:44,780
experiencing colors are vivid. They have force and vivacity and ideas are merely a faint copy

536
00:50:44,780 --> 00:50:48,540
or something. But that's just the first order. And then there are contemporary versions of this

537
00:50:48,540 --> 00:50:52,940
kind of thing, far more sophisticated ways of saying a similar thing. But yeah, you could,

538
00:50:53,020 --> 00:50:59,500
in principle, explore that through the meta problem. Why does it seem to us that perception is so

539
00:50:59,500 --> 00:51:05,260
much more vivid? What about our models of the mind makes perception seem so much more vivid

540
00:51:05,260 --> 00:51:10,380
than belief? It makes belief seem kind of structural and empty, whereas

541
00:51:11,180 --> 00:51:15,500
perception is so full of light. But no, I don't know of work on that from the meta problem

542
00:51:15,500 --> 00:51:20,220
perspective. Like I said, there's not that much work on these introspective models directly. There

543
00:51:20,220 --> 00:51:23,980
is work on theory of mind about beliefs tends to be about models of other people.

544
00:51:25,980 --> 00:51:29,420
It may be there's something I could dig through a literature on belief that says something about

545
00:51:29,420 --> 00:51:35,340
that. It's a good place to push. Thanks. I wanted to bring up Kurt Girdel. You mentioned your advisor

546
00:51:35,340 --> 00:51:41,500
wrote Girdel Escher Bach. There's something that seems very like Girdel, Girdelian or whatever,

547
00:51:41,500 --> 00:51:48,380
about this whole discussion in that. So Girdel showed that, given like a set of axioms and

548
00:51:48,380 --> 00:51:57,820
mathematics, it would either be consistent or complete, but not both. And it seems like when

549
00:51:57,820 --> 00:52:04,860
Daniel Dennett, Daniel Dennett seems to have like a set of axioms where he cannot construct

550
00:52:04,860 --> 00:52:09,580
consciousness from them. He seems to be very much in this sort of consistent camp. Like he

551
00:52:09,580 --> 00:52:17,020
wants to have a consistent framework, but is okay with the incompleteness. And I wonder if

552
00:52:17,100 --> 00:52:22,860
similar approach could be taken with consciousness where we could in fact prove that consciousness

553
00:52:22,860 --> 00:52:29,020
is independent of Daniel Dennett's set of axioms. The same way they proved after Girdel, they

554
00:52:29,020 --> 00:52:34,220
proved like the continuum hypothesis was independent of ZF set theory. And then they added

555
00:52:34,860 --> 00:52:42,140
the axiom of choice made at ZFC set theory. So I wonder if we could show that like in Daniel

556
00:52:42,140 --> 00:52:47,260
Dennett's world we are essentially zombies or we are kind of either zombies or not. It doesn't

557
00:52:47,260 --> 00:52:53,660
matter. Either statement could be true. And then find what is like the minimum axiom that has to be

558
00:52:53,660 --> 00:52:59,500
added to Dennett's axioms in order to make consciousness true. Interesting. I thought for a

559
00:52:59,500 --> 00:53:03,020
moment this was going to go in a different direction. And you're going to say Dennett is

560
00:53:04,460 --> 00:53:09,100
consistent but incomplete. He doesn't have consciousness in this picture. I'm complete,

561
00:53:09,180 --> 00:53:12,620
I've got consciousness, but inconsistent. That's why I say all these crazy things.

562
00:53:14,140 --> 00:53:18,860
And you're faced with the choice of not having consciousness and being incomplete or having

563
00:53:18,860 --> 00:53:23,180
consciousness and somehow getting this hard problem and being forced into at least puzzles

564
00:53:23,180 --> 00:53:25,980
and paradoxes. But the way you put it was friendlier to me.

565
00:53:30,220 --> 00:53:35,660
Yeah, I mean certainly Dark Hofstetter himself has written a lot on analogies between the

566
00:53:35,660 --> 00:53:41,100
Gordelian paradoxes and the mind-body problem. And he thinks always our models, our self models

567
00:53:41,100 --> 00:53:45,740
are always doomed to be incomplete in the Gordelian way. And he thinks that that might be somehow

568
00:53:45,740 --> 00:53:50,940
part of the explanation of our puzzlement at least about consciousness. Someone like Roger Penrose,

569
00:53:50,940 --> 00:53:57,740
of course, takes this much more seriously, literally. He thinks that the computational

570
00:53:57,740 --> 00:54:04,060
aspects of computational systems are always going to be limited in the Gordel way. He thinks human

571
00:54:04,060 --> 00:54:08,620
beings are not so limited. He thinks we've got mathematical capacities to prove theorems,

572
00:54:09,580 --> 00:54:15,260
to see the truth of certain mathematical claims that no formal system could ever have.

573
00:54:15,820 --> 00:54:20,380
So he thinks that we somehow go beyond that incomplete Gordelian. I don't know if he actually

574
00:54:20,380 --> 00:54:24,860
thinks we're complete, but at least we're not incomplete in the way that finite computational

575
00:54:24,860 --> 00:54:30,220
systems are incomplete. And furthermore, he thinks that extra thing that humans have is tied to

576
00:54:30,220 --> 00:54:35,180
consciousness. I mean, I never quite saw how that last step goes, even if we did have these

577
00:54:35,180 --> 00:54:39,500
special non-algorithmic capacities to see the truth of mathematical theorems. How would that be

578
00:54:39,500 --> 00:54:47,100
tied to consciousness? But at the very least, there are structural analogies to be drawn between

579
00:54:47,100 --> 00:54:51,420
those two cases about incompleteness of certain theories, how literally we should take the analogies

580
00:54:51,420 --> 00:54:58,220
I'd have to think about. Has there been some consideration that the problem of understanding

581
00:54:58,220 --> 00:55:02,860
consciousness sort of inherently must be difficult because we address the problem

582
00:55:02,860 --> 00:55:08,940
using consciousness? I'm reminded of the halting problem in computer science where we say that

583
00:55:08,940 --> 00:55:13,980
in the general case, a program cannot be written to tell whether another program will halt because

584
00:55:13,980 --> 00:55:19,740
what if you ran it on itself? It can't sort of be broad enough to include its own execution. So I

585
00:55:19,740 --> 00:55:24,700
wonder if there's a similar corollary in consciousness where we use consciousness to think about

586
00:55:24,700 --> 00:55:31,500
consciousness. And so therefore, we may not have enough sort of equipment there to be able to unpack

587
00:55:32,700 --> 00:55:38,220
Yeah, I mean, it's tricky. People say it's like a user ruler to measure a ruler. Well, I can use

588
00:55:38,220 --> 00:55:43,580
this ruler to measure many other things, but it can't measure itself. On the other hand,

589
00:55:43,580 --> 00:55:48,380
you can measure one ruler using another ruler. Maybe you can measure one consciousness using

590
00:55:48,380 --> 00:55:53,820
another. The brain can't study the brain, but the brain actually has a pretty good job of studying.

591
00:55:54,700 --> 00:55:59,500
The brain. So there are some self referential paradoxes there. And I think that again is at

592
00:55:59,500 --> 00:56:04,860
the heart of Hofstadter's approach. But I think we'd have to look for very, very specific conditions

593
00:56:04,860 --> 00:56:10,220
under which systems can't study themselves. I did always like the idea that the mind was simple

594
00:56:10,220 --> 00:56:17,500
enough that we could understand it. We would be too simple to understand the mind. So maybe

595
00:56:17,500 --> 00:56:21,180
something like that could be true of consciousness. On the other hand, I actually think that if you

596
00:56:21,180 --> 00:56:25,260
start thinking that consciousness can go along with very simple systems, I think at the very

597
00:56:25,260 --> 00:56:30,140
least we ought to be able to study consciousness in other systems simpler than ourselves. And boy,

598
00:56:30,140 --> 00:56:36,700
if I could solve the hard problem, even in dogs, I'd be I'd be satisfied. Hey, so I have a question

599
00:56:36,700 --> 00:56:42,540
about how the meta problem research program might proceed sort of related to the last question.

600
00:56:43,260 --> 00:56:49,420
So certainly things we believe about our own consciousness, even if we all say them,

601
00:56:49,420 --> 00:56:54,940
probably some of them are false. Our brain has a tendency to hide what reality is like.

602
00:56:55,820 --> 00:56:59,900
If you look at like visual perception, you know, there's what's called lightness constancy, you

603
00:56:59,900 --> 00:57:04,700
know, our brain subtracts out the lighting in the environment. So we actually see more reliably

604
00:57:04,700 --> 00:57:09,980
what the colors of objects are. Like these viral examples of like the black and gold dress is an

605
00:57:09,980 --> 00:57:14,540
example of this. And when you're kind of presented with an explanation of it, it's like, huh, my

606
00:57:14,620 --> 00:57:20,380
brain does that. It's not something we have access to. Yeah. Or like the Yanni Laurel Laurel Yanni.

607
00:57:20,380 --> 00:57:24,300
Yeah, illusion is like another one where like when you hear the explanation, you know, the scientists

608
00:57:24,300 --> 00:57:29,900
that understand it, our own introspection doesn't include that. So how do you kind of proceed with

609
00:57:31,260 --> 00:57:37,020
trying to get at what consciousness really is versus what our sort of whatever simplified or

610
00:57:37,020 --> 00:57:44,300
distorted view might be? Yeah, well, one view here would be that we never have access to the

611
00:57:44,300 --> 00:57:49,500
mechanisms that generate consciousness, but we still have access to the conscious states

612
00:57:49,500 --> 00:57:53,980
themselves. Actually, the Colashley said this decades ago, he said, no process of the brain

613
00:57:53,980 --> 00:57:59,340
is ever conscious. The processes that get you to the states are never conscious. The states they

614
00:57:59,340 --> 00:58:05,340
get you to are conscious. So take your experience of the dress. For me, it was, it was what, white

615
00:58:05,420 --> 00:58:11,100
and gold. So, you know, and I knew that, you know, each of us was certain that I am, I was

616
00:58:11,100 --> 00:58:15,180
experiencing, I was certain that I was experiencing white and gold. Maybe you were certain that you

617
00:58:15,180 --> 00:58:21,340
were experiencing blue and black. Which it was. I remember as I was right. You were sure that, yeah,

618
00:58:22,380 --> 00:58:28,060
those idiots can't be, yeah, can't be looking at this right. But anyway, each of us, I think the

619
00:58:28,060 --> 00:58:31,820
natural way to describe this at least is that each of us was certain what kind of conscious

620
00:58:31,820 --> 00:58:36,700
experience we were having. But what we had no idea about was the mechanisms by which we got

621
00:58:36,700 --> 00:58:42,220
there. So the mechanisms are completely opaque. But the states themselves were at least prima facie

622
00:58:42,220 --> 00:58:45,580
transparent. So I think that would be the standard of view. And even a realist about consciousness

623
00:58:46,220 --> 00:58:50,140
could go with that. They say, well, we know what conscious states, we know what those conscious

624
00:58:50,140 --> 00:58:54,540
states are. We don't know the processes by which they're generated. The illusionist, I think,

625
00:58:54,540 --> 00:58:59,500
wants to go much further and say, well, it seems to you that you know what conscious state you're

626
00:58:59,500 --> 00:59:04,940
having. It seems to you that you're experiencing yellow and gold. Sorry, yellow and white, whatever

627
00:59:04,940 --> 00:59:10,140
it was, golden, gold and white. Black and gold is whatever. Black and blue, I think. And blue.

628
00:59:10,140 --> 00:59:15,900
Gold and white. Yeah. It seems to you that you're experiencing gold and white. But in fact, that

629
00:59:15,900 --> 00:59:21,180
too is just something thrown up by another model. The yellow gold was a perceptual model. And then

630
00:59:21,180 --> 00:59:25,660
there was an introspective model that said you're experiencing gold and white. When maybe, in fact,

631
00:59:25,660 --> 00:59:29,100
you're just a zombie or who knows what's actually going on in your conscious state. So

632
00:59:29,100 --> 00:59:33,340
the illusionist view, I think, has to somehow take this further and say not just the processes

633
00:59:33,340 --> 00:59:37,260
that generate the conscious states, but maybe the conscious states themselves are somehow

634
00:59:37,260 --> 00:59:46,940
opaque to us. All right. Thanks. It feels like some discussion of generality of a problem is

635
00:59:46,940 --> 00:59:53,020
missing from this discussion. The matrix multiplier example of having high phi is still,

636
00:59:53,020 --> 00:59:57,980
it's not a general thing. Is there someone exploring the space, the sort of intersection

637
00:59:58,060 --> 01:00:02,300
of generality and complexity that leads to consciousness as an emergent behavior?

638
01:00:03,100 --> 01:00:07,660
When you say generality, I mean, the idea that a theory should be general, that it should apply

639
01:00:07,660 --> 01:00:11,900
to every system, you mean mechanisms of... No, generality of the agent, right? If I can write

640
01:00:11,900 --> 01:00:17,180
an arbitrarily complex program to play tic-tac-toe, and all it will ever be able to do is play tic-tac-toe,

641
01:00:17,180 --> 01:00:23,340
it has no outputs to express anything else. Yeah. As you said, general in the sense of AGI,

642
01:00:23,340 --> 01:00:28,460
artificial general intelligence, I mean, some aspects of consciousness seem to be

643
01:00:28,460 --> 01:00:33,740
domain general, like, for example, maybe as far as belief and reasoning is conscious,

644
01:00:33,740 --> 01:00:38,140
those are domain general, but much of perception doesn't seem especially domain general, right?

645
01:00:38,140 --> 01:00:43,900
Color is very domain... Taste is very domain specific, so it's still conscious. If my agent

646
01:00:43,900 --> 01:00:48,860
can't express problem statements, like, if I don't give it an output by which it can express

647
01:00:48,860 --> 01:00:52,060
problem statements, you can never come to a conclusion about its consciousness.

648
01:00:53,260 --> 01:00:57,020
I like to distinguish intelligence and consciousness. I'm even able to... Even natural

649
01:00:57,020 --> 01:01:01,980
language and, you know, being able to address a problem statement and analyze a problem,

650
01:01:01,980 --> 01:01:09,740
that's already a very advanced form of intelligence. I think it's very plausible that, say, a mouse has

651
01:01:09,740 --> 01:01:15,180
got some kind of consciousness, even it's got no ability to address problem statements in many

652
01:01:15,180 --> 01:01:19,580
of its capacities, maybe very specialized. I mean, it's still much more general than, say,

653
01:01:19,580 --> 01:01:24,620
a simple neural network that can only do one thing. A mouse can do many things, but I'm not

654
01:01:24,620 --> 01:01:28,380
sure that I see an essential... I certainly see a connection between intelligence and

655
01:01:28,380 --> 01:01:33,740
generality. We want to say, you know, somehow a high degree of generality is required for

656
01:01:33,740 --> 01:01:38,220
high intelligence. I'm not sure there's the same connection for consciousness. I think

657
01:01:38,220 --> 01:01:44,780
consciousness can be extremely domain specific, has, say, taste and maybe vision or it can be

658
01:01:44,780 --> 01:01:47,660
domain general. So maybe those two cross cut each other a bit.

659
01:01:51,340 --> 01:01:58,300
So it seems to me like the meta problem as it's formulated implies some amount of, like,

660
01:01:58,300 --> 01:02:02,460
separation or epiphenomenalism between, like, consciousness and brain states.

661
01:02:03,100 --> 01:02:10,060
And one thing that I think underlies a lot of people's motivation to do, say, science is that

662
01:02:10,620 --> 01:02:17,500
it has causal import. Like, predicting behaviors is clearly a functionally useful thing to do.

663
01:02:18,140 --> 01:02:22,460
And if you can predict all of behavior without having to explain consciousness,

664
01:02:23,020 --> 01:02:27,740
their motivation for explaining consciousness sort of evaporates and it sort of feels like,

665
01:02:27,740 --> 01:02:32,460
yeah, yeah, well, what's the point of even thinking about that because it's just not going

666
01:02:32,460 --> 01:02:37,100
to do anything for me? What do you say to someone when they say that to you?

667
01:02:37,820 --> 01:02:39,340
What is the thing that they say to me again?

668
01:02:39,340 --> 01:02:45,020
That there's no, there's maybe consciousness exists, maybe it doesn't. But if I can explain

669
01:02:45,020 --> 01:02:48,780
all of human behavior and all of the behavior of the world in general without

670
01:02:48,780 --> 01:02:54,060
recourse to such concepts, then I've done everything that there is that's useful,

671
01:02:54,060 --> 01:03:00,140
like explaining consciousness isn't a useful thing to do. And thus, I'm not interested in this and

672
01:03:00,140 --> 01:03:05,740
it may not be real. I mean, I'm certainly, I'm not, I mean, I think epiphenomenalism could be

673
01:03:05,740 --> 01:03:09,340
true. I certainly don't have any commitment to it though. It's quite possible that consciousness

674
01:03:09,340 --> 01:03:14,940
has a role to play in generating behavior that we don't yet understand and maybe thinking hard

675
01:03:14,940 --> 01:03:20,220
about the meta problem can help us get clearer on those roles. I think if you've got any sympathy

676
01:03:20,220 --> 01:03:24,700
to panpsychism, maybe consciousness is intimately involved with how physical processes get going

677
01:03:24,700 --> 01:03:29,660
in the, in the first place. And there are people who want to pursue interactionist ideas where

678
01:03:29,660 --> 01:03:33,820
consciousness interacts with the brain. Or if you're a reductionist, consciousness may be just

679
01:03:33,820 --> 01:03:38,860
a matter of the right algorithm. On all those views, consciousness may have some role to play,

680
01:03:38,860 --> 01:03:44,220
but just say it turns out that you can explain all of behavior, including these problems without,

681
01:03:45,420 --> 01:03:49,580
without bringing in consciousness. And does that mean that consciousness is not something we

682
01:03:49,580 --> 01:03:53,020
should care about and not something that matters? I don't think that would follow. I mean, maybe it

683
01:03:53,020 --> 01:03:58,300
wouldn't matter for certain engineering purposes, say you want to build a useful system. But,

684
01:03:58,860 --> 01:04:02,860
you know, at least in my view, consciousness is really the only thing that matters. It's a thing

685
01:04:02,940 --> 01:04:09,100
that makes life worth living. It's what gives our lives meaning and value and so on. So,

686
01:04:09,100 --> 01:04:13,420
it might turn out that, okay, the point of consciousness is not that useful for explaining

687
01:04:13,420 --> 01:04:17,980
other stuff. But it's, you know, if it's the source of intrinsic significance in the world,

688
01:04:17,980 --> 01:04:22,780
then understanding consciousness will still be absolutely essential to understanding ourselves.

689
01:04:22,780 --> 01:04:28,380
Furthermore, if it comes to developing other systems, like say AI systems or dealing with

690
01:04:28,380 --> 01:04:33,740
non-human animals and so on, we absolutely want to know, we need to know whether they're conscious,

691
01:04:33,740 --> 01:04:37,580
because, you know, if they're conscious, they presumably have moral status. If they can suffer,

692
01:04:38,220 --> 01:04:43,420
then it's very bad to mistreat them. If they're not conscious, then you might, I think it's very

693
01:04:43,420 --> 01:04:47,980
plausible, treat non-conscious systems. We can treat how we like, and it doesn't really matter

694
01:04:48,540 --> 01:04:52,700
morally. So, the question of whether, say, an AI system is conscious or not, it's going to be

695
01:04:52,700 --> 01:04:57,580
absolutely vital for how we interact with it and how we build our society. That's not a question

696
01:04:57,580 --> 01:05:01,740
of engineering usefulness. That's a question of connecting with our most fundamental values.

697
01:05:02,380 --> 01:05:08,060
Yeah, I completely agree. I just, I haven't found that formulation to be very convincing to others

698
01:05:08,060 --> 01:05:15,660
necessarily. Hi, thank you so much for coming and chatting with us today. I'm really interested in

699
01:05:15,660 --> 01:05:21,500
some of your earlier work, The Extended Mind Distributed Cognition. Yeah. And you're at a company

700
01:05:21,500 --> 01:05:25,900
speaking with a bunch of people who do an incredibly cognitively demanding task. Yeah.

701
01:05:25,900 --> 01:05:31,580
Most of the literature that I've read on this topic uses relatively simple examples of telling,

702
01:05:31,580 --> 01:05:38,540
like, it's difficult to think just inside your head on these relatively simple things. And if

703
01:05:38,540 --> 01:05:42,140
you take a look at the programs that we build, sort of like on a mundane, day-to-day basis,

704
01:05:42,140 --> 01:05:46,700
there are millions of lines long. I've read people in the past say something like,

705
01:05:46,700 --> 01:05:52,140
the Boeing 777 was the most complicated thing that human beings have ever made. And I think

706
01:05:52,140 --> 01:05:56,540
most of us would look at that and say, we got that beat, you know, like the things that large

707
01:05:56,540 --> 01:06:01,260
internet companies do, the size, the complexity of that is staggering. And yet if we close our

708
01:06:01,260 --> 01:06:05,340
eyes, everyone in here is going to say, I'm going to have difficulty writing a 10-line program in

709
01:06:05,340 --> 01:06:11,340
my head. Okay. So I'm just sort of as an open, like, I'd be very interested in hearing your thoughts

710
01:06:11,340 --> 01:06:16,860
about how the activity of programming connects to the extended mind ideas.

711
01:06:17,500 --> 01:06:23,100
Yeah. So this is a reference to something that I got started in about 20 years ago with

712
01:06:23,740 --> 01:06:29,180
my colleague Andy Clarke. We wrote an article called The Extended Mind about how processes in

713
01:06:29,180 --> 01:06:34,540
the mind can extend outside the brain when we become coupled to our tools. And actually,

714
01:06:34,540 --> 01:06:39,820
our central example back then in the mid-90s was a notebook, someone writing stuff in a

715
01:06:39,820 --> 01:06:44,780
notebook. I mean, even then, we knew about the internet and we had some internet examples.

716
01:06:44,780 --> 01:06:51,500
I guess this company didn't exist yet in 95. But now, of course, our minds have just become

717
01:06:51,500 --> 01:06:57,580
more and more extended and, you know, smartphones came along a few years later and everyone is

718
01:06:57,580 --> 01:07:02,460
coupled very, very closely to their phones and their other devices that coupled them very,

719
01:07:02,460 --> 01:07:09,820
very closely to the internet. Now, it's certainly the case that a whole lot of my memory is now

720
01:07:09,820 --> 01:07:15,420
offloaded onto the servers of your company somewhere or other, whether it's in the

721
01:07:17,820 --> 01:07:24,460
mail systems or navigation mapping systems or other systems. Most of my navigation has been

722
01:07:25,500 --> 01:07:31,020
offloaded to maps and much of my memory has been offloaded. Well, maybe that's in my phone, but

723
01:07:32,460 --> 01:07:36,220
other bits of my memory are offloaded into my file system on

724
01:07:36,620 --> 01:07:46,140
some cloud service. So certainly, vast amounts of my mind are now

725
01:07:47,020 --> 01:07:51,820
existing in the cloud. And if I was somehow to lose access to those completely, then I'd

726
01:07:51,820 --> 01:07:58,780
lose an awful lot of my capacities. So I think we are now sort of extending

727
01:07:58,780 --> 01:08:04,380
into the cloud, thanks to you guys and others. The question specifically about programming

728
01:08:06,940 --> 01:08:11,420
programming is a kind of active interaction with our devices. I mean, I think of programming

729
01:08:11,420 --> 01:08:16,460
as something that takes a little bit longer. It's a longer time scale. So the core cases of the

730
01:08:16,460 --> 01:08:21,980
extended mind involve sort of automatic use of our devices, which are always ready to hand.

731
01:08:21,980 --> 01:08:28,060
We can use them to get information, to act in the moment, which is the kind of thing that

732
01:08:28,940 --> 01:08:34,220
the brain does. So insofar as programming is a slower process, you know, and I remember from

733
01:08:34,860 --> 01:08:43,100
programming days, all the endless hours of debugging and so on. Then it's at least going

734
01:08:43,100 --> 01:08:47,980
to be a slower time scale for the extended mind. But still, Feynman talked about writing

735
01:08:48,780 --> 01:08:56,460
this way. Someone looked at Feynman's work and a bunch of notes he had about a physics problem

736
01:08:56,460 --> 01:09:01,100
he was thinking about. And someone said to him, oh, it's nice you have this record of your work.

737
01:09:02,060 --> 01:09:07,900
And Feynman said, that's not a record of my work. That's the work. That is the thinking.

738
01:09:07,900 --> 01:09:12,300
And so I was writing it down and so on. I think, you know, at least my recollection from my programming

739
01:09:12,300 --> 01:09:16,460
days was that, you know, when you're actually writing a program, that's not like you just

740
01:09:17,020 --> 01:09:22,780
do a bunch of thinking and then code your thoughts. The programming is to some very

741
01:09:22,780 --> 01:09:28,700
considerable extent your thinking. So is that the sort of thing you're thinking about here?

742
01:09:28,700 --> 01:09:35,900
And the, if we, I think as people that program start to reflect on what we do,

743
01:09:35,900 --> 01:09:42,300
and very few of us actually, like if you're the tech lead of a system, maybe you've got it in your

744
01:09:42,300 --> 01:09:46,700
head, okay? But you would agree that most of the people on the team who've come more recently only

745
01:09:46,700 --> 01:09:50,460
have a chunk of it in their head, and yet they're somehow still able to contribute.

746
01:09:50,460 --> 01:09:56,140
Oh yeah, this is now distributed cognition. I mean, the extended mind that extended cognition

747
01:09:56,140 --> 01:10:02,060
like starts with an individual and then extends out, extends their capacities out using their

748
01:10:02,060 --> 01:10:07,100
tools or their devices or even other people. So maybe my partner serves as my memory, but it's

749
01:10:07,100 --> 01:10:12,300
still centered on an individual. But then there's the closely related case of distributed cognition,

750
01:10:12,300 --> 01:10:17,580
where you have a team of people who are doing something and are making joint decisions and

751
01:10:17,580 --> 01:10:21,500
carrying out joint actions in an absolutely seamless way. And I take it as a company like this

752
01:10:21,500 --> 01:10:25,180
that are going to be any number of instances of distributed cognition. I don't know whether the

753
01:10:25,180 --> 01:10:32,220
company as a whole has one giant Google mind, or maybe there's just like a near infinite number

754
01:10:32,220 --> 01:10:39,100
of separate Google minds for all the individual teams and divisions and so on. But I think yeah,

755
01:10:39,100 --> 01:10:43,500
probably some anthropologist has already done a definitive analysis of distributed cognition

756
01:10:43,500 --> 01:10:54,220
in this company, but if they haven't, they certainly need to. Thank you.

757
01:10:55,180 --> 01:10:58,220
Thank you.

