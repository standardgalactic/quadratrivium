1
00:00:00,000 --> 00:00:05,200
Welcome back to Demystify Sci, where today we are exploring how to achieve the impossible.

2
00:00:05,200 --> 00:00:09,760
For this conversation we have with us Jim Keller, who is a microprocessor engineer who's worked at

3
00:00:09,760 --> 00:00:17,280
places like AMD, Apple, Tesla, and currently is working on a next generation of AI compatible chips

4
00:00:17,280 --> 00:00:21,680
that are going to be a competitor with Nvidia for all the stuff that they're doing for artificial

5
00:00:21,680 --> 00:00:28,000
intelligence and also has this wild idea for a startup which is already in progress for being

6
00:00:28,000 --> 00:00:35,760
able to create a semiconductor fab that is tabletop size. He's also got the idea of being able to

7
00:00:35,760 --> 00:00:43,120
3D print a car for $5,000 and so this is a guy who's worked for his entire life at these enormous

8
00:00:43,120 --> 00:00:48,080
organizations that are able to achieve things that across the board people say are not possible.

9
00:00:48,080 --> 00:00:52,400
And so we wanted to start with him to figure out what does that actually look like? What are the

10
00:00:52,400 --> 00:00:56,800
things that go into making a successful company? What are the pieces of culture that are internal

11
00:00:56,800 --> 00:01:01,520
to the company versus the pieces of culture that are internal to the humans within that company?

12
00:01:01,520 --> 00:01:08,960
And how do they play together in order to let people achieve what seems on paper absolutely

13
00:01:08,960 --> 00:01:15,040
not going to happen? Which is very improbable as a perspective considering he's actually

14
00:01:15,040 --> 00:01:20,320
so cynical about the lifespan of institutions and of these companies. He recognizes there's a

15
00:01:20,320 --> 00:01:26,880
cyclic nature to the boom and bust of the production in all of these different organizations.

16
00:01:27,840 --> 00:01:34,000
And so he's pulled out these really fascinating trends for what it means for a company to reach

17
00:01:34,000 --> 00:01:38,240
that pinnacle and why they can't stay there. And the same thing can be applied to our scientific

18
00:01:38,240 --> 00:01:44,080
institutions, our government, and we go into all of this. So it's a really, really refreshing,

19
00:01:44,080 --> 00:01:49,040
inspiring perspective that I don't think we've seen on this show so far.

20
00:01:49,040 --> 00:01:53,600
And honestly, it's also just really cool to sit down with somebody who has worked on the chips

21
00:01:53,600 --> 00:01:58,720
that have been in the technology that I've used my entire life and to discover that he's thinking

22
00:01:58,720 --> 00:02:07,040
about physics and science and human nature and the fractal arc of reality in this way that actually

23
00:02:07,040 --> 00:02:12,240
gives him the ability to come up with ideas that other people just seem to not have access to.

24
00:02:12,240 --> 00:02:16,560
And so we get into all of that. It's a really good conversation. Hopefully we'll be able to

25
00:02:16,560 --> 00:02:21,520
have him back to talk about some other stuff because he's been in contact with these titans of

26
00:02:21,520 --> 00:02:29,120
the last 50 years, Elon Musk, Steve Jobs, and just has a wealth of experience in the world

27
00:02:29,120 --> 00:02:33,600
that I think would be really interesting to talk more about. But in the meantime,

28
00:02:33,600 --> 00:02:39,440
I want you to consider coming over to our Patreon. So we are a patron-sponsored podcast. We take no

29
00:02:39,440 --> 00:02:45,520
ads, we have no commercial sponsors, and we really, really, really want to keep it that way

30
00:02:45,520 --> 00:02:50,800
because that aligns with the way that we see the world functioning. We make something that is useful

31
00:02:50,800 --> 00:02:55,920
to people and the people who like it support us and let us keep doing it. And so if you've watched

32
00:02:55,920 --> 00:03:00,560
a couple episodes of the podcast and you enjoy it, then consider coming over to patreon.com

33
00:03:00,560 --> 00:03:05,200
slash dmstify.com and joining us for just a couple dollars a month. A fistful of dollars if you like.

34
00:03:06,400 --> 00:03:13,680
Indeed. However, many dollars fit into the fistful that you would like to take, happy to accept.

35
00:03:13,680 --> 00:03:16,160
You can put different denominations in a fist, it turns out.

36
00:03:16,160 --> 00:03:22,400
It's true. And also, something else that you can really do is if you're watching the podcast on

37
00:03:22,400 --> 00:03:27,840
YouTube, leave a comment. If you're watching it on Spotify or any of the podcast stores,

38
00:03:27,840 --> 00:03:33,600
perhaps rate the podcast. These are all things that help us boost ourselves algorithmically,

39
00:03:33,600 --> 00:03:36,800
and they don't cost you anything except for a few minutes of your time.

40
00:03:36,800 --> 00:03:41,920
And it helps us get better guests too, which is ultimately going to serve you guys. So do it.

41
00:03:42,480 --> 00:03:47,360
All right. Hopefully, you will follow through. Hopefully, we will see you soon.

42
00:03:47,360 --> 00:03:49,520
And for now, enjoy the conversation with Jim Keller.

43
00:04:05,520 --> 00:04:09,680
I came across a quote that I think is an unfortunate article about you where they

44
00:04:09,680 --> 00:04:14,320
said that you're really a fan of the Steve Jobs aphorism, which is that once you know what to do,

45
00:04:14,320 --> 00:04:24,480
you shouldn't work on anything else. And I wonder about that in context of institutional longevity,

46
00:04:24,480 --> 00:04:29,600
because it's really easy for an institution to continuously come up with a new thing to do

47
00:04:29,600 --> 00:04:35,680
and keep trying to work on something different. And so in addition to knowing what to do and

48
00:04:35,680 --> 00:04:40,480
then working exclusively on it, do you think that there has to be a sense of an expiration date?

49
00:04:42,000 --> 00:04:48,960
Yes, probably definitely. So this is a, yeah, this is a really complicated question.

50
00:04:50,080 --> 00:05:00,320
And then I was part of, you know, the demise of digital equipment,

51
00:05:01,120 --> 00:05:07,520
which was a great company. And we went from growing and everybody there was excited.

52
00:05:08,720 --> 00:05:12,800
A friend of mine's wife said, what do they put in the water? All you guys do is work or talk about

53
00:05:12,800 --> 00:05:21,200
work. It's really fun. And then I've talked about this in a couple like financial analysts

54
00:05:21,200 --> 00:05:27,600
seminars recently that like when I joined digital, they were very proud of building low-cost open

55
00:05:27,600 --> 00:05:34,960
computers. And they were winning against IBM. And then IBM, and people said nobody went broke

56
00:05:34,960 --> 00:05:41,200
by in IBM, home digital was building this lower cost, more open computer that people could buy

57
00:05:41,200 --> 00:05:48,720
and do stuff with. And 10 years later, and only took 10 years, they're competing against sun and

58
00:05:48,720 --> 00:05:55,280
silicon graphics, who are building lower cost, easier to use computers. And the digital sales guys,

59
00:05:55,280 --> 00:06:02,400
some of the same people, I suspect, you know, poo pooed the sun systems as toys and cheap and

60
00:06:02,400 --> 00:06:08,000
little and non-professional. And, you know, digital went through like a literal collapse.

61
00:06:09,840 --> 00:06:18,240
Their best revenue year was on falling sales, rising prices. And then they just lost the

62
00:06:18,240 --> 00:06:26,560
market. And it was also interestingly enough, we were going bankrupt at the same time we were

63
00:06:26,560 --> 00:06:31,440
building the world's fastest computers. We were building a new generation of product that was

64
00:06:31,440 --> 00:06:37,600
demonstratively be better than anything we'd ever made, better than sun by a lot, and went bankrupt

65
00:06:37,600 --> 00:06:45,280
at the same time. And so. But seems impossible on some of them. Yeah, yeah, it's a miracle.

66
00:06:46,000 --> 00:06:54,000
But there was all kinds of things. So, so you have to, yeah, I tell people sometimes ask me,

67
00:06:54,000 --> 00:06:58,400
like, how do you manage a team? And how do you do something? And people simultaneously,

68
00:07:00,400 --> 00:07:06,880
let's say, underthink it and underdo it. Like, people will read one management book. A frequent

69
00:07:06,880 --> 00:07:12,080
question I get is, which book should I read? Because I say I read lots of books. I mostly

70
00:07:12,080 --> 00:07:18,160
say all of them. And, you know, and I've been trolling some of my internet friends by releasing

71
00:07:18,160 --> 00:07:24,000
lists of five books, but they're, they're relatively randomly arranged, you know, book on

72
00:07:24,000 --> 00:07:29,680
management, a book on science fiction and the book on, I really like Katie Byron Katie's book,

73
00:07:29,680 --> 00:07:39,280
Loving What Is, you know, and, you know, so. But now, as a book on management, that's not obvious,

74
00:07:39,280 --> 00:07:44,480
but it's a great book on management. Same with the Five Love Languages, which is, I think,

75
00:07:44,480 --> 00:07:51,200
a marital book. And, but it's complicated. You need lots of different frameworks to figure things

76
00:07:51,200 --> 00:07:55,600
out. So, so digital equipment is a great example of that. The company could literally

77
00:07:56,960 --> 00:08:03,120
revert almost everything that made it successful, then go bankrupt while building a great new product.

78
00:08:03,600 --> 00:08:10,800
And mostly, I think that made it successful. Yeah. Yeah, so we were building alpha computers,

79
00:08:10,800 --> 00:08:16,800
which had 64-bit addressing, and it was one of the better, better ones of the early 64-bit

80
00:08:16,800 --> 00:08:22,400
computers. And one of its charms was we could adjust very large memories from some server

81
00:08:22,400 --> 00:08:27,440
applications. That was important. But the memory group at digital had been making lots of money

82
00:08:27,440 --> 00:08:32,720
for years and they'd been raising prices. They literally created an industry around

83
00:08:32,720 --> 00:08:38,800
digital making plug-in memories that were cheaper when company EMC became very large and successful.

84
00:08:40,400 --> 00:08:44,160
So, as digital brought alpha to market, the memory guys were raising money,

85
00:08:44,160 --> 00:08:52,480
raising memory prices, and we didn't sell any memory. So, so that one of the virtues of the

86
00:08:52,480 --> 00:08:57,360
computer was addressing more memory and memory prices were so high that our customers

87
00:08:57,360 --> 00:09:02,640
were buying memory from a competitor. Like, like in the world of like, how dumb could it be?

88
00:09:03,360 --> 00:09:08,320
And as engineers, we knew this was happening. And the memory group was working with us to

89
00:09:08,320 --> 00:09:12,640
make it harder to plug in memory and the engineering team made it easier to plug in memory.

90
00:09:13,200 --> 00:09:17,760
And there was no intermediary that could be like, hey, we're working against each other?

91
00:09:18,720 --> 00:09:23,360
Well, so, so this is where you get it. So, Ken Olson was the founder and he was still there,

92
00:09:23,360 --> 00:09:30,720
mostly, although he was replaced with Bob Palmer. So, Ken had this kind of do the right thing attitude

93
00:09:30,720 --> 00:09:36,160
and, and, you know, let people go off and do their thing. But the problem is the memory business

94
00:09:36,160 --> 00:09:41,600
was a business and the server business was a business. And the memory business made more money

95
00:09:41,600 --> 00:09:47,840
by actually lowering the server business substantially. And that's where, you know,

96
00:09:48,400 --> 00:09:54,400
the uncoordinated action kill them because usually, you know, you, you would think somebody would say,

97
00:09:54,400 --> 00:09:57,920
hey, this doesn't make any sense. We're killing our business by not doing something.

98
00:09:58,480 --> 00:10:02,400
But it's really hard and mostly not companies when they're non founder run,

99
00:10:04,000 --> 00:10:06,000
you know, run the ground at some point, because they

100
00:10:07,200 --> 00:10:10,560
founders optimized for the longevity of their baby. And

101
00:10:11,440 --> 00:10:14,880
non founders frequently optimized for their own personal income.

102
00:10:15,440 --> 00:10:17,840
So how many people were digital equipment at that point?

103
00:10:18,640 --> 00:10:20,400
The peak was 110,000.

104
00:10:21,440 --> 00:10:23,280
That is enormous.

105
00:10:23,280 --> 00:10:27,840
Yeah, yeah. Yeah, back in the day, it was a Arctic cap was 14 billion.

106
00:10:29,120 --> 00:10:33,680
I worked on a computer. We, you know, back to the 800, we sold $5 billion worth of

107
00:10:34,480 --> 00:10:38,000
computers at a half a million a piece. It was, it was really amazing.

108
00:10:39,040 --> 00:10:44,400
So I like, like I said, so you need a couple things. So there's the life cycle story, which is

109
00:10:45,120 --> 00:10:50,000
you know, in human beings to the zero to 20 year, you're a kid learning stuff. And then 20 to 40,

110
00:10:50,000 --> 00:10:54,960
you find your place in the world. And then 40 to 60, you should, that's a mobile, exploit your

111
00:10:54,960 --> 00:11:01,200
expertise. And then 60 to, you know, death, you enjoy a retirement or your enlightenment or

112
00:11:01,840 --> 00:11:05,600
if you're a sociopath, you might continue to operate at some high level

113
00:11:06,640 --> 00:11:10,800
of manipulating reality, right? And so companies go through those cycles.

114
00:11:11,680 --> 00:11:15,040
And a lot of companies fail long run because they just get old.

115
00:11:16,880 --> 00:11:21,040
And some companies get restarted, like, you know, both Apple and Microsoft were

116
00:11:21,760 --> 00:11:27,520
essentially rebooted, you know, Apple by the founder and Microsoft by

117
00:11:28,800 --> 00:11:34,720
Satya Nardella, which is, which is amazing. Like nobody saw that coming because that company had

118
00:11:34,720 --> 00:11:40,640
seemed to have gotten into the, you know, they had cash cow products and they were soaking the

119
00:11:40,640 --> 00:11:49,360
customers for it. So it's very hard to, to reboot a company substantially, once you pass a certain

120
00:11:49,360 --> 00:11:55,600
point. But anyway, it's a framework. And there's another framework, which is organizations tend

121
00:11:55,600 --> 00:12:03,600
towards order and the order slowly, like a startup is very chaotic. And a friend of mine drew this

122
00:12:03,600 --> 00:12:10,480
graph of, you know, the X axis is chaos at the origin and then order, and then the Y axis is

123
00:12:10,480 --> 00:12:18,160
productivity. So for a while, as you increase organizational procedures and, and, and process,

124
00:12:18,160 --> 00:12:24,000
you get more productive, but at some point you get less productive. And the trick isn't figuring

125
00:12:24,000 --> 00:12:28,080
out where you should be on the curve, you should be at the top. It's like, you know, you curve,

126
00:12:28,960 --> 00:12:34,800
right? The trick is staying there. Because once you start organizing for productivity,

127
00:12:35,760 --> 00:12:42,080
more order always seems like the right answer. And then I know a lot of companies, the people who

128
00:12:42,080 --> 00:12:47,200
are very good at organizing things, sort of outmaneuver politically, the people are good at

129
00:12:47,200 --> 00:12:53,120
inventing things. And if there's nobody going, wait a minute, we need a new product. So it's

130
00:12:53,120 --> 00:12:58,880
very difficult to escape that trap. And does the size of the company influence that as well?

131
00:13:00,240 --> 00:13:05,840
Yes and no, you know, like there's many companies who go through, you know, never get very big,

132
00:13:05,840 --> 00:13:12,560
but still become bureaucratic. But it's, it probably gets harder to avoid it as you get bigger.

133
00:13:14,160 --> 00:13:17,680
Like, you know, a lot of the big tech companies that we think are, you know, great,

134
00:13:18,480 --> 00:13:24,400
are famously bureaucratic, you know, like Google and Facebook. And then there's some companies

135
00:13:24,400 --> 00:13:31,200
that are famously not like Amazon's run as a whole bunch of small silos that, you know,

136
00:13:31,200 --> 00:13:38,800
compete with each other. And I'm not sure what the current lay of the land is at Microsoft.

137
00:13:38,800 --> 00:13:46,160
Apple under Steve Jobs was mostly small teams and he didn't trust big teams. But since he

138
00:13:46,160 --> 00:13:50,560
passed away, the company has become unbelievably large and successful and all the teams are huge.

139
00:13:52,240 --> 00:13:55,840
What was the secret to the, to these reboots that actually worked out?

140
00:13:57,680 --> 00:14:05,440
Well, Steve Jobs was in a very strong belief in product. So we, I had a friend who worked for

141
00:14:05,440 --> 00:14:12,320
him at the time, and he said, we have 10 business groups, and they're all losing money. The company

142
00:14:12,320 --> 00:14:16,720
is losing money, but on paper, all the groups are making money. And that's because they were doing

143
00:14:16,720 --> 00:14:21,520
transfer costs, you know, bureaucratic shenanigans, like, I'll make something for you and sell it to

144
00:14:21,520 --> 00:14:26,320
you as I have a profit, but then you sell it to somebody, you know, so it was just a mess.

145
00:14:26,880 --> 00:14:31,920
And there's too many competing products. And if you have product A and product B,

146
00:14:31,920 --> 00:14:36,320
the marketing guy will say, make product between them. And, you know, you'll have a better product

147
00:14:36,320 --> 00:14:42,720
line, a better coverage. So Steve Jobs, he canceled apparently all the business units,

148
00:14:42,720 --> 00:14:47,760
he canceled the products, he canceled, fired most of the managers. And he created the same as

149
00:14:47,760 --> 00:14:54,960
two by two matrix, which was consumer pro mobile desktop. He said, we're going to make four products.

150
00:14:56,400 --> 00:15:01,600
And everybody wants one of those. Like if you're a professional and you sit at your desk, you want

151
00:15:01,600 --> 00:15:07,600
a desktop pro computer. If you're, you know, working, but you travel, you want a professional

152
00:15:07,600 --> 00:15:14,160
mobile computer. That was the MacBook Pro, the Mac Pro. And then the iMac was the famous

153
00:15:14,160 --> 00:15:21,200
translucent, you know, fun computer for the house. And so he created those. And then he said,

154
00:15:21,200 --> 00:15:27,200
we'll make each one of them the best we possibly can, which means we won't get all the customers

155
00:15:27,200 --> 00:15:32,880
because there'll be gaps and holes. But you could trust us that if you buy iMac, you'll be very happy.

156
00:15:34,400 --> 00:15:41,920
And, and then when I was going really well, then they did the iPhone. And then he really believed

157
00:15:41,920 --> 00:15:48,240
in the iPad. I was there during the, I guess the third iPhone chip in the start of the iPad.

158
00:15:49,440 --> 00:15:55,360
And then he really believed in TV and he told us he'd cracked TV. But he passed away before he

159
00:15:55,360 --> 00:16:00,160
built the product. And I don't know what he cracked because it wasn't the Apple TV product. He

160
00:16:00,160 --> 00:16:07,120
didn't like that very much. But he was very focused on the next thing. And then he's, Steve famously said,

161
00:16:11,360 --> 00:16:16,400
like, so technology is often what I call a cascade, instead of cascading and diminishing

162
00:16:16,400 --> 00:16:22,960
return curves. So you go up and it plateaus, and then there's a new invention, you go up into plateaus.

163
00:16:23,920 --> 00:16:29,760
So we went from rotary phones to button phones to touchscreen phones. And each one of them,

164
00:16:29,760 --> 00:16:35,440
when it first started. So the first, you know, touchscreen phones weren't as good as the buttons.

165
00:16:37,040 --> 00:16:44,320
Right. So black, blackberry users laughed at the touchscreen phone people. So the screen was

166
00:16:44,320 --> 00:16:49,600
bigger, but your touch wasn't accurate. It was slow sometime. I had friends that would prove to me

167
00:16:49,600 --> 00:16:55,280
they could type faster on blackberry than a touchscreen phone. And yet the blackberry died

168
00:16:55,280 --> 00:17:00,000
because they were married to the old technology. So Steve's point was when you go from one technology

169
00:17:00,000 --> 00:17:07,280
to the next one, you always go down that up. Right. You're, you're jump. You're, you're by

170
00:17:07,280 --> 00:17:12,720
definition jumping from a highly refined endpoint to an unrefined starting point.

171
00:17:13,600 --> 00:17:16,880
That depends on when you jump, right? Because the people who had

172
00:17:17,360 --> 00:17:24,320
and that's the thing. As he said, any idiot can show you could wait until it starts to be refined.

173
00:17:24,320 --> 00:17:31,840
Now you're behind. So the wind, if you wait for other people to do it, you're going to have a

174
00:17:31,840 --> 00:17:40,080
business strategy of being a fast follower. That's actually a business term. But if you always wait,

175
00:17:40,160 --> 00:17:46,240
you'll always be second. And when people create new markets, it's amazing.

176
00:17:47,120 --> 00:17:52,960
The digital and phones, I mean, Apple and phones, we did 64 bits before anybody else. We did

177
00:17:52,960 --> 00:17:58,640
high resolution displays before anybody else. We did get great cameras before anybody else.

178
00:17:58,640 --> 00:18:04,480
We did thin phones before anybody else. And each time it took a couple of years for people to catch

179
00:18:04,480 --> 00:18:10,880
up. So yeah, it's a, it's a funny thing. So was this, this was the reboot strategy?

180
00:18:10,880 --> 00:18:15,440
Yeah. So this was after like Steve's got thrown out a digit out of Apple,

181
00:18:16,640 --> 00:18:21,440
partly cause he couldn't work with people and partly he made a big back on the big bed on the

182
00:18:21,440 --> 00:18:28,080
Lisa and then the factory for the Lisa and the original Macintosh and it didn't go well.

183
00:18:28,400 --> 00:18:35,840
And he was famously hard to work with. And so when he came back, Apple bought next.

184
00:18:36,720 --> 00:18:39,760
And next was Steve's company with a new operating system.

185
00:18:40,800 --> 00:18:44,720
So like Apple at some point needed the new operating system they bought next. And then

186
00:18:45,440 --> 00:18:51,600
for a bunch of reasons, I wasn't there. He maneuvered his way back into being CEO and then

187
00:18:52,560 --> 00:18:59,120
you know, launched into a famous reboot of Apple, which, which by the way is relatively unprecedented.

188
00:19:00,320 --> 00:19:03,360
He said that Microsoft managed to do it as well. What was the story there?

189
00:19:03,360 --> 00:19:09,840
Yeah. And that's not a non-founder. Yeah. So Microsoft, everything was Windows. They had

190
00:19:09,840 --> 00:19:17,120
Windows, you know, Windows phones, Windows PCs, Windows mouse. And the world was sort of shifting

191
00:19:17,120 --> 00:19:20,880
away from Windows and Windows had a lot of problems. Like it was going mobile.

192
00:19:21,440 --> 00:19:25,600
And they tried to build something called .NET, but everything they did was proprietary

193
00:19:26,400 --> 00:19:32,800
early and the company was suffering. And Satya pivoted the company to being data first,

194
00:19:32,800 --> 00:19:40,720
essentially. He got rid of all the Windows groups. We have, I suspect they actually let go a lot

195
00:19:40,720 --> 00:19:46,240
of people, including senior management. So Microsoft built this beautiful tablet computer.

196
00:19:46,240 --> 00:19:51,200
I know the people who made it, but the group that did Office Suite wouldn't port their software to it.

197
00:19:53,120 --> 00:19:56,800
Right. And nobody could tell them to port. Like they ran a business and they said,

198
00:19:56,800 --> 00:20:03,520
it doesn't make any business sense for us to port the software. So under Satya, that all changed.

199
00:20:03,520 --> 00:20:07,200
What's that? Because it would be packaged with the tablet when it was sold and so it wasn't going

200
00:20:07,200 --> 00:20:12,640
to make money. So this is where you guys have probably read Shakespeare, right?

201
00:20:13,520 --> 00:20:15,680
Some, not with them.

202
00:20:16,480 --> 00:20:22,720
It's good to read Shakespeare. Right. So Shakespeare is always the drama between the king,

203
00:20:23,520 --> 00:20:27,440
the ministers, you know, the poor bastards and the hero.

204
00:20:29,360 --> 00:20:36,800
Right. And so big companies, when they become bureaucratic, you know, if they're run by somebody

205
00:20:36,800 --> 00:20:41,120
who then has some number of people running organizations, let's call them the conniving

206
00:20:41,120 --> 00:20:49,040
ministers. Right. And they are, let's say, vying for favor from the king and benefits

207
00:20:49,760 --> 00:20:55,840
by mostly messing with each other. And the king isn't king because he's stronger than all of them.

208
00:20:55,840 --> 00:21:00,960
He's stronger because he plays them against each other and they play against each other.

209
00:21:00,960 --> 00:21:09,040
But at some point there's, let's say, a real problem. Like Windows is going down or the

210
00:21:09,120 --> 00:21:13,520
Macintosh is broken. And then the drama is, is how does it play out?

211
00:21:14,800 --> 00:21:18,960
Because the king can't trust the hero because the hero will replace them. The poor bastards are all

212
00:21:18,960 --> 00:21:24,160
rooting for that. And the ministers are trying to get the hero on their side because then they can

213
00:21:24,160 --> 00:21:30,880
win. And it's very difficult in a lot of companies for anybody to, unless there's a really strong

214
00:21:30,880 --> 00:21:37,040
founder and a really strong culture. The picture that you're painting makes it seem

215
00:21:37,040 --> 00:21:42,320
almost miraculous that anything gets done. Oh yeah, that's actually true.

216
00:21:47,280 --> 00:21:53,280
Now I read this funny, here's a funny one. So there was a study about the differential

217
00:21:53,280 --> 00:22:00,480
growth between China and Vietnam. This is a 20, 25 years ago. And they basically said there was a

218
00:22:00,480 --> 00:22:06,960
10% difference in corruption between the two countries. I forget the exact numbers.

219
00:22:07,760 --> 00:22:12,400
And as a result, China was growing at 10% year over year and Vietnam was growing at zero.

220
00:22:13,920 --> 00:22:19,680
And the corruption number was big, like 50%. Which was what were corrupt?

221
00:22:24,240 --> 00:22:28,800
According to this study, I don't know if that, you know, it's true. But, you know, people look

222
00:22:28,800 --> 00:22:34,640
at spectacular growths and think, wow, that's amazing. Like they must be all working really

223
00:22:34,640 --> 00:22:41,920
hard and doing really good things. But it could be they're producing 4%, you know, real benefit

224
00:22:41,920 --> 00:22:47,520
year over year, but compound growth of 4% actually over, you know, 10 years is a lot.

225
00:22:49,200 --> 00:22:52,720
Right. And then in our big economy today with all the companies,

226
00:22:53,680 --> 00:22:58,720
like there's lots of strategies. So new technologies are mostly exploited by new companies.

227
00:22:59,680 --> 00:23:05,360
Like when Google was growing fast, some big companies had search engines, but they were all

228
00:23:05,360 --> 00:23:12,640
terrible. And Google grew and, you know, for a while, for a long time, they profess, you know,

229
00:23:12,640 --> 00:23:19,040
they do no evil and everything was the good of humanity. But the way they made money is

230
00:23:19,040 --> 00:23:23,680
serving ads. And then what happens is they get really good at it. And the more ads they serve,

231
00:23:24,640 --> 00:23:29,440
you know, the more money they make, and the more they get you to look at them, the better. And then

232
00:23:29,440 --> 00:23:36,160
they go back to their, the people want to pay for ads. And they say, we can manipulate the

233
00:23:36,160 --> 00:23:44,240
results to give you better responses. And now you have their company mission, which is to make

234
00:23:44,240 --> 00:23:48,400
all the data available to everybody. But their business says we're going to make a lot of money.

235
00:23:48,640 --> 00:23:55,600
But their advertisers, you know, would like, you know, differential benefits and the consumers

236
00:23:55,600 --> 00:24:02,560
have options. Right. So that's a complicated thing. And then you could cast that as a Shakespeare

237
00:24:02,560 --> 00:24:09,280
play and assign the, you know, the villains and heroes as you want. But it's really complicated.

238
00:24:10,560 --> 00:24:14,880
And this happens all the time, like every company goes through these cycles.

239
00:24:15,840 --> 00:24:20,160
Do you have any insight into the day that Google dropped its do no evil slogan?

240
00:24:21,280 --> 00:24:23,520
Yeah, it's long after that was true.

241
00:24:25,440 --> 00:24:30,640
You know, just so that there's cognizant. Well, companies go through this cognitive

242
00:24:30,640 --> 00:24:35,760
dissonance phase, you know, they have a corporate vision about who they are and what they want to

243
00:24:35,760 --> 00:24:40,960
be. And then they have a bottom line to attend to. And then the business practices that accomplish

244
00:24:40,960 --> 00:24:48,240
that. And then I'm sure some engineers said, hey, we can't do X, Y and Z because of our vision.

245
00:24:49,200 --> 00:24:53,920
And it was like, yeah, well, we haven't been doing that for years. And, you know, it's complicated.

246
00:24:54,880 --> 00:24:58,640
You know, most people at Google want to do the right thing. And most businesses want to,

247
00:24:58,640 --> 00:25:04,560
you know, be, you know, let's say as moral as they can be. And then the market

248
00:25:05,360 --> 00:25:11,680
kind of does this interesting thing, which is as companies become monopolies and start like

249
00:25:11,680 --> 00:25:18,480
squeezing their customers, essentially, you know, it's easy to cut your R&D and, you know, spend

250
00:25:18,480 --> 00:25:25,520
more on sales and make more money. But then there's a competitive thing. Like the US car industry,

251
00:25:25,520 --> 00:25:29,840
there was the big three, and they were all playing the same game and gaming each other.

252
00:25:29,920 --> 00:25:35,120
And when Toyota showed up, they laughed. It was a shitty little car. And Toyota made a better

253
00:25:35,120 --> 00:25:42,000
and better car. And all of a sudden, they're on the back foot, you know, trying to catch up with

254
00:25:42,000 --> 00:25:47,040
Toyota. Like, who saw that happen? They actually went sun Pro and went bankrupt. And then the

255
00:25:47,040 --> 00:25:51,440
government bailed them out because there's, you know, like the layers of complexity is they're

256
00:25:51,440 --> 00:25:57,920
so high. It's amazing. Like, they can some change depending on that at some point.

257
00:25:57,920 --> 00:26:04,560
Yeah, yeah, there's the politicians and the unions and the jobs and the states and distributed

258
00:26:04,560 --> 00:26:11,760
manufacturers and, but they all going through these cycles of their internal growth of technology

259
00:26:11,760 --> 00:26:17,360
cycles of, you know, competitive cycles. Yeah, it's quite amazing.

260
00:26:18,320 --> 00:26:22,960
Hey, folks, quick interruption. I really need you to come over and check out our Patreon page.

261
00:26:23,760 --> 00:26:27,600
You can give as little as a couple of dollars a month, or you can give us a fistful of dollars

262
00:26:27,680 --> 00:26:32,160
The point is this program is entirely supported by people like you who are enjoying this program.

263
00:26:32,160 --> 00:26:36,400
Don't have any sponsors. Don't have any ads. We want to keep it that way. So please consider

264
00:26:36,400 --> 00:26:40,720
coming over and checking out how you can just give the tiniest amount to support this project

265
00:26:40,720 --> 00:26:46,800
into the future. Thank you. I'll see you there. So do you think that you can tell when an organization

266
00:26:46,800 --> 00:26:53,920
is starting to get to the place of decline early? Or do you think that it's always something that

267
00:26:54,000 --> 00:27:00,400
you can only really tell once the decline is fulfilled and then you look back and kind of

268
00:27:00,400 --> 00:27:06,880
match things? Like is there a predictable time length? Even is there a time scale? Is it size

269
00:27:06,880 --> 00:27:11,440
dependent? I'm not even thinking necessarily about time scale. That would be interesting. I'm

270
00:27:11,440 --> 00:27:17,920
thinking more about it. In terms of things that happen, like patterns that you start to see emerging

271
00:27:17,920 --> 00:27:21,920
and people starting to behave in certain ways. Like you're talking about the drill equipment

272
00:27:21,920 --> 00:27:27,280
where you have the the branch that's dealing with memory and the branch that's dealing with

273
00:27:27,280 --> 00:27:32,880
servers completely operating at odds with each other. That serves as this harbinger of chaos

274
00:27:32,880 --> 00:27:42,560
to come unless somebody can come in and you know make the vessels behave. So in retrospect,

275
00:27:42,560 --> 00:27:49,920
it's way easier. Somebody said, you know, the past the banker, you know, they were going to

276
00:27:50,000 --> 00:27:54,960
bankruptcy and they said it happened slowly at first and then all at once. And that's often

277
00:27:56,160 --> 00:28:02,560
so digital went through this rising revenue on falling unit sales. That's a really good sign.

278
00:28:05,680 --> 00:28:11,760
You see companies, I worked at a company that was basically capitalizing cost.

279
00:28:12,720 --> 00:28:18,160
So if you build a product and you have to buy a component to put in that product, that's an

280
00:28:18,160 --> 00:28:22,960
expense. But if you build a factory to build that component, that's capital.

281
00:28:24,800 --> 00:28:31,840
Right. And so for financial accounting reasons, capital is deductible and viewed as a good thing.

282
00:28:31,840 --> 00:28:39,120
Capital is an asset versus expenses are a problem. And so as companies get bigger and

283
00:28:39,120 --> 00:28:43,680
mature, they get, let's say, sophisticated economic tools and some of those are really good

284
00:28:44,560 --> 00:28:48,400
because we have complicated tax rules and international business rules

285
00:28:49,280 --> 00:28:52,480
reporting roles and you have to know what they all are and you have to play the game.

286
00:28:53,440 --> 00:29:00,960
Otherwise, you won't report making any money. But you know, the nominal timeline looks something

287
00:29:00,960 --> 00:29:06,640
like the life cycle of a human being, which I said is 20 years, 20 years, 20 years, 20 years.

288
00:29:07,440 --> 00:29:14,400
But some of that's accelerated, like the lots of startups go through five to 10 years of

289
00:29:14,400 --> 00:29:19,680
figure finding their real place in the world and then ramping. Some companies ramp really quick,

290
00:29:19,680 --> 00:29:23,440
like Google probably would be wandering around for five years.

291
00:29:24,240 --> 00:29:27,600
Like they were making real progress and lots of people like them, but they weren't making any

292
00:29:27,600 --> 00:29:35,440
money. And then they ramped pretty hard for over 10 years and 15. They still make lots of money.

293
00:29:35,440 --> 00:29:41,360
To a lot of the AI companies right now, right? Like OpenAI and Anthropic, their ramps are

294
00:29:42,080 --> 00:29:46,720
super big. Yeah, that's a, so there's another phenomenon. So independent of the business

295
00:29:46,720 --> 00:29:54,320
cycle, there's kind of hype cycles. The AI is as a strong hype cycle, partly because there's a lot

296
00:29:54,320 --> 00:30:01,520
of revenue and market cap, you know, based on it. So there's a little FOMO going there, which is,

297
00:30:02,240 --> 00:30:05,760
you know, let's, let's get involved and do something.

298
00:30:06,480 --> 00:30:11,680
How much of that, can you tell how much of it is hype and how much of it is real? Because, I mean,

299
00:30:11,680 --> 00:30:17,680
you obviously can't really trust the representatives of a company to tell you accurately

300
00:30:19,200 --> 00:30:24,800
what's happening or what will happen. Promise is these massively intelligent,

301
00:30:26,160 --> 00:30:30,960
general intelligence machines that will one day outshine humans. Everybody's pointing to the

302
00:30:31,040 --> 00:30:37,280
exponential curve and like, we're, it's going to continue forever. But anybody who looks at

303
00:30:37,280 --> 00:30:42,960
exponential curves also knows that sometimes they're logarithmic and they just stable it.

304
00:30:42,960 --> 00:30:47,440
Yeah, yeah. Yeah, they're, they're S-curves, not exponentials. Yeah, it's pretty.

305
00:30:47,440 --> 00:30:54,080
Right. Yeah, it's hard to say. So the, in the internet boom, there was a whole bunch of companies

306
00:30:54,080 --> 00:31:00,080
who clearly had to achieve some network effect to be successful. So two good examples.

307
00:31:00,640 --> 00:31:06,320
One early was PayPal. So they were paying their customers to recommend them to other people.

308
00:31:07,280 --> 00:31:11,280
So they spent hundreds of millions of dollars in customer acquisition

309
00:31:12,000 --> 00:31:17,840
with actually no plan to make any money. But they got big enough and they started to make money.

310
00:31:17,840 --> 00:31:22,400
And then there was other companies who said, Hey, we're going to do the same thing, web band. And

311
00:31:23,200 --> 00:31:28,400
it was a pet company that was doing online stuff. So they were paid, they spent billions of dollars

312
00:31:28,400 --> 00:31:34,720
in customer acquisition and then didn't keep any of them and just earned the money at all.

313
00:31:34,720 --> 00:31:38,880
And Uber was the same way. Uber spent billions of dollars in customer acquisition.

314
00:31:40,160 --> 00:31:44,400
And, you know, Uber used to be a really good deal compared to a taxi on price.

315
00:31:45,040 --> 00:31:50,240
Like the convenience is a huge win, but the price was also better. But when they slowly

316
00:31:50,240 --> 00:31:55,040
started working on making money, when they had acquired enough customers that the argument of

317
00:31:55,040 --> 00:32:00,400
customer acquisition was kind of dumb, they started raising prices. And now it's not obvious

318
00:32:00,400 --> 00:32:05,360
that they're cheaper than taxis. They're more convenient. Now we got used to them. And then

319
00:32:05,360 --> 00:32:11,280
they're, you know, 10 years of spending money to get customers, you know, they put a lot of

320
00:32:11,280 --> 00:32:18,400
taxi companies out of business. But, you know, it's, it's not always obvious which way it's

321
00:32:18,400 --> 00:32:23,440
going to go. You know, there was a lot more web bands than there was PayPal's.

322
00:32:25,200 --> 00:32:32,400
But these are, you know, business cycles, which are really interesting. The internal

323
00:32:32,400 --> 00:32:38,800
dynamics. So humans are, you know, when we, when you build organizations, humans are very good

324
00:32:38,800 --> 00:32:44,480
in groups of five to 10, because that's essentially a family group. And we're very good in groups of

325
00:32:44,480 --> 00:32:50,160
like up to 100 or so. Because that's like a tribal group, like we can map them really well.

326
00:32:51,200 --> 00:32:57,280
But when you want to grow past 100, suddenly you need a whole different level of infrastructure.

327
00:32:58,640 --> 00:33:05,680
Because now you have one group working on something to deliver with another group where they don't

328
00:33:05,680 --> 00:33:13,520
know anybody. And, and this is another one of those dynamics when, when you have a strong vision

329
00:33:13,520 --> 00:33:19,360
and you're growing fast and there's too much to do, you know, sometimes those organizations

330
00:33:19,360 --> 00:33:25,360
grow naturally, sometimes you really have to work on it. But then when a company slows down and you

331
00:33:25,360 --> 00:33:30,640
have all these little groups of people who don't know each other very well, and they're exchanging,

332
00:33:30,640 --> 00:33:37,440
you know, work for a boss or some reward, that stuff can get gained pretty hard.

333
00:33:38,400 --> 00:33:44,400
And then companies become bureaucratic. And then there's a famous line about bureaucracy is like

334
00:33:44,400 --> 00:33:50,640
bureaucracy is inevitable with human beings, apparently. And, you know, it's, it's not whether

335
00:33:50,640 --> 00:33:55,520
you want one or not, it's how you manage it. Because at some point the bureaucracy will,

336
00:33:55,520 --> 00:34:00,560
like somebody said, you know, imagine that bureaucracy is always run by your worst enemy.

337
00:34:00,880 --> 00:34:07,120
And, and for a lot of companies, that's actually what ultimately does them in.

338
00:34:07,840 --> 00:34:11,680
So you have business cycle dynamics, which is really interesting. And then you have

339
00:34:12,320 --> 00:34:17,520
this organizational dynamic. And that's been studied pretty widely, you know, like there's a,

340
00:34:18,080 --> 00:34:23,360
there's one line that which is 20% of the people do 80% of the work, which is fairly common.

341
00:34:24,720 --> 00:34:28,800
And then there's another one, which is the output of an organization is the square root of the number

342
00:34:28,800 --> 00:34:36,000
of people, which has, you know, been studied understated behind that. And then you might say,

343
00:34:36,000 --> 00:34:40,240
well, then we should keep the group as a whole bunch of small teams. And some people do that,

344
00:34:40,240 --> 00:34:43,680
but sometimes that's impossible. Like you're doing something really big and hard, and you

345
00:34:43,680 --> 00:34:51,120
actually have to figure out, like, well, 1000 people isn't as efficient as 100 people.

346
00:34:52,400 --> 00:34:55,360
You can get a lot more done with 1000 people than 100.

347
00:34:56,320 --> 00:35:01,920
And is it just that the challenge with technology and large projects is that you can't break a

348
00:35:01,920 --> 00:35:06,320
project down into small enough chunks that you can get people to work in these small groups?

349
00:35:06,320 --> 00:35:09,120
Like I'm thinking for, you know, really,

350
00:35:09,120 --> 00:35:14,240
No, that's actually that you put your finger right on it. Like one of the fundamental things

351
00:35:14,240 --> 00:35:20,240
is humans aren't very smart. And we're not getting any smarter. So we don't solve harder problems

352
00:35:20,240 --> 00:35:25,280
because we're smarter period. Like, well, I've been working engineering for 40 years,

353
00:35:25,280 --> 00:35:31,040
I don't see any evidence that anybody's any smarter. Some of the tools we have are really good.

354
00:35:31,680 --> 00:35:33,280
Do you see other people are dumbers?

355
00:35:34,560 --> 00:35:37,600
No, not really. Okay, that's good. That's really no.

356
00:35:38,880 --> 00:35:42,000
I'm sure the answer is going to be yes, but that's good.

357
00:35:42,000 --> 00:35:46,000
I have an aunt who's very smart and very well read and she was complaining about young people.

358
00:35:46,000 --> 00:35:50,400
So I, I found this letter and I read it to her and she said, that's just it.

359
00:35:50,960 --> 00:35:54,960
People don't read enough anymore. They don't write, they don't do this, they don't do that.

360
00:35:54,960 --> 00:35:57,040
And I said, that was written by Ben Franklin.

361
00:35:59,120 --> 00:36:02,880
It's such a great, you can probably go Ben Franklin's letter about, you know,

362
00:36:02,880 --> 00:36:08,800
the inadequacies of young people. So, so when you think that that mostly just tells you your age.

363
00:36:10,080 --> 00:36:15,520
So all human beings around 50 years old or so starts to suspect that 20 year olds aren't very

364
00:36:15,520 --> 00:36:18,560
smart. I've always been an old soul some early at that arrival.

365
00:36:18,560 --> 00:36:22,720
So you might, yeah. So if you're, you're early, it really depends on where you go.

366
00:36:22,720 --> 00:36:28,640
Like we hired college kids at Tesla. They were so smart and hardworking and ready to go.

367
00:36:29,680 --> 00:36:33,440
And then I'm, you know, and I talked to friends at another big company and they're the people

368
00:36:33,440 --> 00:36:38,080
they hired from college, to be honest, weren't very good. But that's because the students were

369
00:36:38,080 --> 00:36:43,280
selecting based on the company. You know, the super smart ones wanted to go test their metal

370
00:36:43,280 --> 00:36:48,480
at a place where they knew they'd be stressed. And the students that were sort of average and

371
00:36:48,480 --> 00:36:53,360
wanted to go along with the get along and get a good paycheck were picking a company that was

372
00:36:53,360 --> 00:36:59,520
literally easy to work for and do nothing. And I mean, I'm just thinking I was looking at,

373
00:37:00,560 --> 00:37:07,360
yeah, that's a really funny drawing, hand drawing diagrams for, I think it was like

374
00:37:07,360 --> 00:37:11,680
rocket propulsion systems before they had computers. And so it's this photograph of this

375
00:37:11,680 --> 00:37:17,920
gigantic piece of paper. And there's like five engineers that are hand writing the calculations

376
00:37:17,920 --> 00:37:26,160
and drawing trajectories. And I look at that. And to me, it seems objectively like a harder thing

377
00:37:26,160 --> 00:37:33,280
to do. But if you have a computer that can do the same thing for you. And so it's like,

378
00:37:33,920 --> 00:37:38,800
if the computers are picking up the slack, then you wouldn't be able to notice the difference.

379
00:37:38,800 --> 00:37:43,840
But if you were to take the computers away, would we still be able to perform at that same level?

380
00:37:43,840 --> 00:37:47,920
And that's, that's the part where I'm not 100% sure.

381
00:37:47,920 --> 00:37:55,600
It really depends. So you could probably coast way, way further knowing a whole bunch of programs

382
00:37:55,600 --> 00:38:00,800
and tools. But the Tesla rocket engines are way better than the ones that built in the 70s.

383
00:38:00,800 --> 00:38:09,760
They're just way better. And you know, they produce them for 10x less money, 10x less hours,

384
00:38:09,760 --> 00:38:14,480
they're reusable, they're more efficient. Like everything about them is better.

385
00:38:14,480 --> 00:38:21,520
They're not even close. Modern cars are so good. Geez. You know, I was in the Hyundai factory

386
00:38:21,520 --> 00:38:30,000
recently, you know, the time to go from like basically what comes into the factory is these

387
00:38:30,000 --> 00:38:35,760
rolls of steel, you know, and they're, you know, random millimetres thick, depending on the parts.

388
00:38:36,960 --> 00:38:46,240
And, you know, four hours later, there's a car. All stamping is done. They have a machine that

389
00:38:46,240 --> 00:38:53,440
on rolls that cuts it, stamps it, knocks it out. They basically take every part and scan it with

390
00:38:53,440 --> 00:38:59,520
laser beams and they, you can mark them up and correct them and tweak them. And then the robots

391
00:38:59,520 --> 00:39:05,040
have put the pieces together and spot weld them all together. The damn things are near perfect.

392
00:39:06,320 --> 00:39:10,000
Stamping machines are great, by the way. So they've been using stamping machines forever.

393
00:39:10,640 --> 00:39:14,720
Like stamping machine is literally a car of a shape and a hard surface and then

394
00:39:14,800 --> 00:39:20,640
put a big weight behind it and drop it on a piece of flat sheet metal and sheet metal.

395
00:39:20,640 --> 00:39:22,640
Most of the number one cause of workplace accidents.

396
00:39:25,520 --> 00:39:30,080
Yeah, the modern stamping machines, they're all operated by robots. They don't let people near

397
00:39:30,080 --> 00:39:35,200
them. Yeah, stamping machines are tough. I got a funny question about donut factory and they're

398
00:39:35,200 --> 00:39:40,080
always losing fingers that were there. It's the worst. Yeah, yeah, stamping donut. That's amazing.

399
00:39:40,800 --> 00:39:45,920
Yeah, they've got to come from somewhere. Well, and then when you build a factory with robots,

400
00:39:45,920 --> 00:39:49,760
then you wear it because the robots are so strong and they move so fast that they'll take people

401
00:39:49,760 --> 00:39:55,680
apart. So then the, all the factories are like built with this is the people's zone and this is

402
00:39:55,680 --> 00:40:00,720
the robots on the pretty soon robots will be smart enough not to whack people. Like all the robots

403
00:40:00,720 --> 00:40:04,560
will see. But at that point, anyway, it's like the robot containment zone as well.

404
00:40:05,680 --> 00:40:09,520
What's that? I said, but at that point, when they're smart enough, they can escape the robot

405
00:40:09,520 --> 00:40:12,640
containment zone as well. So that's, that's a privilege sort of thing.

406
00:40:17,840 --> 00:40:21,360
Were you freaked out about these possibilities when you were working on that system?

407
00:40:24,400 --> 00:40:32,400
Yeah, so I ran the hardware, yeah, the autopilot ship hardware team. So we built a hardware

408
00:40:32,400 --> 00:40:37,520
three, it's called hardware three chip and started hardware four chip. And then also started Dojo,

409
00:40:37,520 --> 00:40:43,760
the Tesla super computer. And then for a while, they, the autopilot software team reported to me,

410
00:40:44,560 --> 00:40:50,720
but that was sort of, you know, a random thing because Andre Carpathi mostly worked for Elon and

411
00:40:51,760 --> 00:40:56,560
some of the software team worked for me and some of them worked for Andre and we missed around.

412
00:40:56,560 --> 00:40:59,680
So that's a different question. That's a different question that might freak out that

413
00:41:00,400 --> 00:41:04,640
robots or super intelligent computers will impact humanity.

414
00:41:04,640 --> 00:41:09,760
No, just these Tesla's, these self-driving cars, right? It seems like whenever you introduce a

415
00:41:09,760 --> 00:41:14,080
new technology, somebody's going to have to figure out what's wrong with it. Like airplanes are a

416
00:41:14,080 --> 00:41:17,920
good example, right? They're, when they first build these jetliners, they're falling out on the

417
00:41:17,920 --> 00:41:23,040
sky all the time, but now they're some of the safest ways to travel on the planet. And I remember

418
00:41:23,040 --> 00:41:27,920
there was a few, maybe actually, I don't remember one really bad incident with the Tesla car,

419
00:41:27,920 --> 00:41:33,600
but sticking an intersection, a truck pulling out for an intersection, an intersection for

420
00:41:34,240 --> 00:41:38,880
white or gray sky or something like that. There's been a couple of problems.

421
00:41:39,920 --> 00:41:46,320
No, we reviewed all the, all the fatals and many of the serious accidents every week,

422
00:41:46,320 --> 00:41:51,360
literally. Okay, so, so you're familiar with the trolley problem, I presume.

423
00:41:52,000 --> 00:41:56,480
Yeah, which is you have, you have a switch and then on one track, there's one person tied to

424
00:41:56,480 --> 00:41:59,680
the tracks on the other side, there's five people tied to the tracks, you pull the switch.

425
00:42:00,640 --> 00:42:07,040
Yeah, yeah. So Elon deeply believed you pulled the switch, right? Kill less people. So,

426
00:42:08,400 --> 00:42:14,240
so he said, oh, he said this publicly, you know, we're going to make cars safer. The only way to

427
00:42:14,240 --> 00:42:19,600
do that is autonomous driving and really good, you know, autonomous driving software and computers.

428
00:42:20,560 --> 00:42:28,640
And, but, but as it develops, some people will die because of defects in that software,

429
00:42:29,200 --> 00:42:37,600
but less people will die, I think. And then he also backed up that commitment by making Tesla,

430
00:42:37,600 --> 00:42:43,360
like Tesla had some of the crash analysis software in the world. And then the way they built Tesla

431
00:42:43,360 --> 00:42:48,320
was with the battery pack and the floor and the crush frames in the front and the back of the car

432
00:42:49,200 --> 00:42:52,640
and made it really safe. And so they were one of the first a five star,

433
00:42:53,440 --> 00:42:56,800
you know, crash certification, which is actually mostly a real thing, by the way,

434
00:42:57,440 --> 00:43:01,760
like some certifications that I'm skeptical of, but crash safety people are really good.

435
00:43:02,320 --> 00:43:07,760
And then the side impact crash and then, and then there's a really wild graph, which, you know, the

436
00:43:08,880 --> 00:43:14,480
fatalities versus speed graph. So over 40 miles an hour, you mostly die and under 40 miles an hour,

437
00:43:14,480 --> 00:43:20,480
you mostly don't. And so the mission was a, to raise the speed where you mostly don't die

438
00:43:21,200 --> 00:43:27,840
and be lower the speed of the car really fast in the event of a detection of an accident.

439
00:43:29,440 --> 00:43:37,680
Right. So, yeah, so was it, was it daunting building a safety product? Yes, definitely.

440
00:43:38,320 --> 00:43:43,760
Did we analyze it a lot and worry about it? Oh, yeah. So one of the guys who worked for me,

441
00:43:45,040 --> 00:43:54,400
there was a radar product we had that mistook a truck for a sign and rejected. So the problem

442
00:43:54,400 --> 00:44:00,160
was radar, it's a relatively low resolution and a small object with a right geometry can look

443
00:44:00,160 --> 00:44:07,520
like a very big object. And so the radar's problem is false positive. So they reject lots of false

444
00:44:07,600 --> 00:44:14,800
positives and it rejected a truck and the person dies. And then when we took the software apart,

445
00:44:14,800 --> 00:44:21,040
we figured out why, like the filters, the software using were very good. We rewrote the software so

446
00:44:21,040 --> 00:44:28,080
that we would detect that truck properly and some other situations where the radar was not doing

447
00:44:28,080 --> 00:44:33,680
that good. How safe are they at this point, these self-driving software as compared to the average

448
00:44:33,680 --> 00:44:42,880
driver? I don't know. I don't, I see Waymo and Waymo driving around San Francisco with no people

449
00:44:42,880 --> 00:44:49,440
in the cars. And I think they're relatively good. They overkill it really high on sensors and compute.

450
00:44:50,880 --> 00:44:56,720
Tesla, Tesla's are not there yet. I have the newest version of self-driving software. It's

451
00:44:56,720 --> 00:45:02,480
pretty good. I really like it, but I wouldn't let it, you know, drive to work. You wouldn't take

452
00:45:02,480 --> 00:45:08,080
a nap in the backseat. No, I definitely would not take a nap. But it's, it's coming along. And then

453
00:45:09,600 --> 00:45:15,120
like Elon's belief is it's going to go in every single car. So it can't cost 50 grand. So the

454
00:45:15,120 --> 00:45:20,960
Waymo solution is really expensive. It has many, many sensors and lots and lots of computers and

455
00:45:21,520 --> 00:45:28,640
looks like a fighter jet or something. It's got so much gear. But is it possible that the cost

456
00:45:28,640 --> 00:45:34,880
of processors is going to go down to the degree that the Waymo system isn't $50,000 anymore in like

457
00:45:34,880 --> 00:45:42,480
10 years? Yeah, yeah, 10 years, maybe. Yeah, so there's like two schools. So this is again one of

458
00:45:42,480 --> 00:45:48,800
those, you know, technology trends. So sometimes you say, well, I'll build the computer or system

459
00:45:48,800 --> 00:45:54,560
big enough to do what I want. And then I'll cost reduce it over time. And then the other is I'll

460
00:45:54,560 --> 00:46:03,280
build the solution and the size that I want. Right. And then work to make it, you know, the

461
00:46:03,280 --> 00:46:09,040
functionality fit the budget. Right. And if you look at computer technology over a long enough

462
00:46:09,040 --> 00:46:15,360
span of time, it definitely started with, we'll build it enormously expensive and enormously

463
00:46:15,360 --> 00:46:20,400
large, and then we'll scale it down. Is it just that we've gotten to be sufficiently

464
00:46:21,360 --> 00:46:27,600
geared out that we can start working on technological problems from the standpoint of now I have

465
00:46:27,600 --> 00:46:32,880
enough components that are small enough and advanced enough that I can build small from the

466
00:46:32,880 --> 00:46:42,320
gecko? Yeah, well, so yeah, there's a there's a funny trend line. So like every 10 years, we went

467
00:46:42,320 --> 00:46:49,520
from mainframe to mini computer to workstation to PC to mobile. So that was, you know, every 10 years

468
00:46:49,840 --> 00:46:57,040
is about 100 times more computing. So on the transistors per year, about 10 x every five years,

469
00:46:57,040 --> 00:47:05,280
it's pretty solid graph. And then what happened is, like, like a PC wasn't 100 times faster than

470
00:47:05,280 --> 00:47:11,600
mini computer, it was more like 10 times faster, it traded some of the transistor, you know,

471
00:47:11,600 --> 00:47:18,480
performance, or just having less of them in a different form factor. So, so if you look if you

472
00:47:18,480 --> 00:47:23,760
look at the trends of how many transistors you have versus how many transistors were in the

473
00:47:23,760 --> 00:47:32,160
new cheaper product, like the transistor count grew faster than you know, so the budget was slowly

474
00:47:32,160 --> 00:47:38,640
going down, which modulated the transistor improvement. The problem is AI, the AI you really

475
00:47:38,640 --> 00:47:46,080
want is like a million times more than we have. In terms of driving or just in general, what's that?

476
00:47:46,080 --> 00:47:53,120
In terms of self driving technology or just in general? Yeah, for a lot of things. Like if you

477
00:47:53,120 --> 00:48:00,720
look at, you know, the operations in a phone chip, like phone chips are 20 bucks, right? And, you know,

478
00:48:00,720 --> 00:48:07,680
Nvidia's new GPU is 25,000, right? And some of that's marked up because of, you know, the market

479
00:48:07,680 --> 00:48:13,280
position and some of it's a lot of transistors and a lot of heat and performance. So AI was an

480
00:48:13,280 --> 00:48:19,600
interesting step function. Like when you went from workstations to PCs to, you know, mobile,

481
00:48:20,960 --> 00:48:26,000
like, like the first PCs were obviously inadequate compared to workstations,

482
00:48:26,720 --> 00:48:32,080
but the transistor count quickly, you know, sped them up. And then the original phones were

483
00:48:32,080 --> 00:48:37,360
inadequate compared to PCs. But so then there's two funny things there. One is

484
00:48:37,920 --> 00:48:43,520
whenever you start with a big software on a big computer, the software never gets simpler.

485
00:48:44,240 --> 00:48:48,800
So when they went from workstation to software to PC software to mobile software,

486
00:48:49,680 --> 00:48:55,680
each time they were rewritten, like PC software is written from scratch and borrowed very little

487
00:48:55,680 --> 00:49:01,520
from workstation to your software, right? Now at some point, the PC got fast enough to run

488
00:49:01,520 --> 00:49:06,880
workstation software, but there was an evolutionary bottleneck that caused our big rewrite.

489
00:49:07,840 --> 00:49:13,280
And then same thing happened with mobile devices. Like the original mobile operating systems were

490
00:49:13,280 --> 00:49:19,120
very lean compared to PC operating systems that caused a really big cleanup. So the problem with

491
00:49:19,120 --> 00:49:23,520
the Waymo approach is once they write all that software, all those computers and everything,

492
00:49:24,800 --> 00:49:31,600
the odds of them, you know, making a 10x simpler by rewriting is kind of low. They're sort of

493
00:49:31,600 --> 00:49:38,800
stuck at a higher cost basis. So I'm a little skeptical of the build it big enough to do the

494
00:49:38,800 --> 00:49:45,840
job and make it cheap later or hope that or hope that that happens.

495
00:49:46,640 --> 00:49:52,640
Did you say that NVIDIA ship was $25,000? Yeah. Who are the customers for that?

496
00:49:53,440 --> 00:49:57,440
You know, Microsoft and Google and Bank of America and lots of people.

497
00:49:58,320 --> 00:50:06,320
What are they implementing it in? Just in their own software enterprise, like in their own

498
00:50:06,320 --> 00:50:11,120
computation back at home base? Oh, yeah, yeah. Right now, like all the big tech companies are

499
00:50:11,120 --> 00:50:15,440
in an arm race and they're all calling NVIDIA to get allocation for their new best,

500
00:50:16,000 --> 00:50:22,320
but they currently have the best processors by, you know, like functionality and, you know, obvious

501
00:50:22,320 --> 00:50:29,280
proof points. Again, Diddy is slowly becoming the IBM of the AI era, I think. And like, we'll see

502
00:50:29,280 --> 00:50:36,320
how that goes. Is it general? I run an AI tech company, so I have opinions about it, but I want

503
00:50:36,320 --> 00:50:41,840
to hear about that too. But is that generally the case that the first online technology is

504
00:50:41,840 --> 00:50:46,240
bought by these enormous corporations? Like you mentioned at the beginning,

505
00:50:46,240 --> 00:50:51,440
digitally you said was selling a half a million dollar computer. And I'm thinking who's buying that?

506
00:50:52,640 --> 00:50:57,520
Yeah. Oh, like a lot of people. So when digital started, they made logic boards. So they made

507
00:50:57,520 --> 00:51:02,400
little boards as big with transistors on them. And they had products called like AND gate and

508
00:51:02,400 --> 00:51:09,680
OR gate and right. And then they worked themselves up to making, I forget the original number, but

509
00:51:10,800 --> 00:51:16,880
you know, they were making computers in, you know, one to 10,000 zone that were,

510
00:51:17,840 --> 00:51:24,000
you know, slower than IBM's, but, you know, 10 times cheaper. And people loved them. And then

511
00:51:24,000 --> 00:51:32,320
they built the 1144, which is probably 10 to 50,000. And then the 1170, which is a little more

512
00:51:32,320 --> 00:51:40,800
expensive. And then the Vax 80, 100, or the Vax 780 was a one million instruction per second

513
00:51:40,800 --> 00:51:48,080
computer. It sold for like 100,000 to $200,000. And they sold, you know, for the time, a lot of them.

514
00:51:49,120 --> 00:51:55,920
And then the Vax 80, 100 was faster. And the cheapest one was over like 150,000. And then the

515
00:51:55,920 --> 00:52:03,920
high end was $5,600,000. And they sold them to banks, businesses, all kinds of people.

516
00:52:04,400 --> 00:52:10,320
Was the military a customer as well? Yeah, the military bottom, it was all over the place.

517
00:52:11,040 --> 00:52:16,160
But at the time, the half million, the high end Vax computer for half a million dollars was a great

518
00:52:16,160 --> 00:52:22,560
deal compared to the IBM solutions. And, you know, they were selling a lot of them, but

519
00:52:23,280 --> 00:52:27,200
then they were planning like a computer after that was even more expensive.

520
00:52:27,760 --> 00:52:33,440
And so this is another anomaly that happens is if you like the high end technology trends

521
00:52:34,000 --> 00:52:38,400
that you'll see in like Forbes or Business Review or something shows this falling price

522
00:52:38,400 --> 00:52:46,160
of computer. But generally speaking, in each kind of epoch of computing, the leaders slowly get more

523
00:52:46,160 --> 00:52:51,600
expensive. Like Sun started, you know, $3,000 workstation and when they went out of business,

524
00:52:51,600 --> 00:53:00,000
they were selling $100,000 servers. So there's a funny thing, like it's sort of like the Ford

525
00:53:00,000 --> 00:53:06,160
Mustang. When the Ford Mustang came out and was 65, it was this beautiful little sports car by 72,

526
00:53:06,240 --> 00:53:11,360
it was just bloated, you know, because everything everybody's idea maybe a little bit bigger,

527
00:53:11,360 --> 00:53:16,080
let's put a little bit bigger engine, a little bit bigger tire, we'll make the back seats a little

528
00:53:16,080 --> 00:53:21,360
bigger, we'll make it a little wider. So elbow room and what they, you know, they killed what

529
00:53:21,360 --> 00:53:27,600
they loved about it. And that's a very common phenomenon. So human beings are phenomenally

530
00:53:27,600 --> 00:53:33,520
good at this kind of stuff. So interesting, though, because on some level from the consumer's

531
00:53:33,520 --> 00:53:40,880
standpoint, the 65 Mustang might have been a perfect car. Yes, it didn't mean anything.

532
00:53:40,880 --> 00:53:46,480
And you could still be selling 65 Mustangs today, and they would be going like hotcakes because they

533
00:53:46,480 --> 00:53:54,640
have the aesthetics, the vibes, the feel, whatever. And so it seems like the process of technological

534
00:53:54,640 --> 00:54:02,320
development in this market system works directly against the person who just wants a thing

535
00:54:03,280 --> 00:54:09,040
that looks like the sweet spot that got hit. Because I'm like, is the iPhone 15

536
00:54:09,760 --> 00:54:16,720
significantly better than the iPhone 6? No. Right? And because Shiloh is still

537
00:54:16,720 --> 00:54:20,160
probably the owner of iPhone 6. So that's the division return curves. And then people,

538
00:54:21,120 --> 00:54:27,600
though the designers, the marketing people are all looking for something different. Because they,

539
00:54:27,600 --> 00:54:32,240
you know, somewhere in their pointy little heads are like, if we just keep making 65 Mustangs,

540
00:54:32,240 --> 00:54:37,040
people will maintain them and nobody will buy a new one. But if there's a new thing you have to

541
00:54:37,040 --> 00:54:42,800
have, so why do you buy a car, especially a sports car? It's partly for fun, partly transportation,

542
00:54:42,800 --> 00:54:49,600
partly for status, partly for new, partly you got bored, partly who knows, it's a complicated

543
00:54:50,480 --> 00:54:56,960
thing. So yeah, this is where I mean, like people like underanalyze everything. So

544
00:54:57,920 --> 00:55:05,040
so we just described, you know, at least a dozen frameworks, right? And so just all 12 equations

545
00:55:05,040 --> 00:55:11,840
with 12 unknowns, you need 12 equations. Like, you can't, you can't cherry pick a point and then

546
00:55:13,040 --> 00:55:17,440
get anywhere. So if you want to make new technology yet, it's good to be aware of all

547
00:55:17,440 --> 00:55:22,560
these different phenomena and then you have to think about it. But you can also overthink it

548
00:55:22,560 --> 00:55:27,440
and be over constrained by the whole thing. Okay, so yeah, I would, I would have loved

549
00:55:27,440 --> 00:55:33,200
to have much improved. Now, some of the Mustang growth was because they started rolling in new

550
00:55:33,200 --> 00:55:37,840
crash standards, which the old Mustang couldn't do. And then there was all kinds of wild emission

551
00:55:37,840 --> 00:55:43,760
stuff with which they tried to build emissions in the cars before they had the computer controls

552
00:55:43,760 --> 00:55:49,920
and sensors to do it. So nowadays, you could make a Mustang that big, that's way safer because we

553
00:55:49,920 --> 00:55:55,360
have the crash software and very efficient and relatively, you know, low on emissions because

554
00:55:55,360 --> 00:56:01,680
we have the sensors and the computer control systems and the high pressure pumps to do fuel

555
00:56:01,680 --> 00:56:07,920
injection properly. Like, it's a, you know, today we saw those problems with high end technology.

556
00:56:08,960 --> 00:56:13,760
Like modern computer controlled motors are amazing. What's that? But like Ford's not making,

557
00:56:13,760 --> 00:56:16,880
that's the thing that's really strange to me. Is that? Oh, yeah, yeah. So then the question is,

558
00:56:16,960 --> 00:56:20,160
why not to go back and make the great car with all the technology they have?

559
00:56:21,440 --> 00:56:22,000
I don't know.

560
00:56:22,000 --> 00:56:25,680
Like it's hard to actually just do it. It's really actually a really funny trend right now. It's

561
00:56:25,680 --> 00:56:30,000
like reissuing, you know, the 54 strad or something like that. And they're selling really well

562
00:56:30,000 --> 00:56:34,800
actually, because they just couldn't improve. And they're literally rewinding all the coils

563
00:56:34,800 --> 00:56:39,280
to spec and everything is like, exactly how it was. And people are just loving it.

564
00:56:39,280 --> 00:56:40,560
Is it all exactly because

565
00:56:41,440 --> 00:56:45,520
I mean, they're very, they're so expensive. They're high, like they're, you know,

566
00:56:45,520 --> 00:56:50,240
high end American products with, compared to some of their cheaper lines and stuff.

567
00:56:50,240 --> 00:56:54,480
I think that they really do go after the original as much as possible from what I can tell.

568
00:56:55,040 --> 00:56:58,800
And so it's an interesting strategy. I think the problem is the safety standards on those

569
00:56:58,800 --> 00:57:04,720
old Mustang searches. Like if you've ever driven an old 60s, 70s car, they're like the

570
00:57:04,720 --> 00:57:10,640
suspension's horrible. They have no pickup compared to what you're used to. They're just not

571
00:57:10,640 --> 00:57:16,560
actually good cars is the problem. Yeah, but you could make a car that looked and felt like

572
00:57:16,560 --> 00:57:22,960
the old Mustang that actually was safe, but it would be the opposite of like a reissued fender.

573
00:57:25,200 --> 00:57:31,040
You know, the reissued fender, you could make exactly the same guitar and somebody would love it.

574
00:57:31,120 --> 00:57:39,760
Whereas the reissued Mustang wouldn't look like even the metallurgy of the steel would be

575
00:57:39,760 --> 00:57:44,480
completely different and it would be so much better. And this is something that's really

576
00:57:44,480 --> 00:57:51,040
interesting to me because I'm like, okay, so Ford, for whatever reason, doesn't reissue the 65

577
00:57:51,040 --> 00:57:58,160
Mustang, like that's perplexing. But there are people that may smaller car manufacturers

578
00:57:58,160 --> 00:58:04,080
that would be happy to reissue a 65 Mustang with all of the updates, but they can't because of

579
00:58:04,080 --> 00:58:11,440
intellectual property. Like Ford in perpetuity owns the... No, they don't. It's mostly you could

580
00:58:11,440 --> 00:58:18,480
probably copy it. There's a difference in copyright law and patent law on this one.

581
00:58:18,480 --> 00:58:24,320
I don't know what's the intellectual property is. I was going to introduce a new car that's

582
00:58:24,320 --> 00:58:27,760
you were talking to Warren Mosler, he tried to do that, right? He built this car that was

583
00:58:28,480 --> 00:58:32,160
way better than all the competitors and it was just impossible to bring it to market.

584
00:58:32,960 --> 00:58:36,400
He seemed perplexed by even what had happened to him, but it seemed like it was

585
00:58:36,400 --> 00:58:41,600
really held down by a kind of old club. It's very difficult to edge into that.

586
00:58:41,600 --> 00:58:46,080
Okay, so fine. Like I think that the way that trademark law works is that if

587
00:58:47,680 --> 00:58:53,840
Ford owns the trademark to the Mustang and the design of the car is not functional,

588
00:58:53,840 --> 00:59:00,000
then nobody else can copy the look of the car because the only things that aren't protected by

589
00:59:00,000 --> 00:59:07,120
trademark are things that are not... That are functional. Yes, sorry, that's a double negative.

590
00:59:07,120 --> 00:59:14,880
Trademark protects non-functional appearance-based things. So the look of a car is protected by

591
00:59:14,880 --> 00:59:18,800
the trademark that they own over it unless somebody can prove that the look of the car is

592
00:59:18,800 --> 00:59:23,040
functional in which case trade dress doesn't cover it. But let's think about something

593
00:59:23,040 --> 00:59:29,360
that's more recent, like the iPhone, right? So Apple doesn't make iPhone 6 as big. So if you

594
00:59:29,360 --> 00:59:34,000
want an iPhone 6, you have to get one that's old and they're manufactured in such a way that

595
00:59:34,000 --> 00:59:38,480
they're hard to repair. And so by the time that you get an iPhone 6, like none of the operating

596
00:59:38,480 --> 00:59:43,600
system works on it anymore, it's very slow, like it's buggy, you can't really repair the components

597
00:59:43,600 --> 00:59:51,840
easily. Is there some space to separate out the companies that generate the ideas and then the

598
00:59:51,840 --> 00:59:57,680
people that implement them so that somebody who like Apple comes up with the idea for the iPhone,

599
00:59:59,280 --> 01:00:05,040
then allows other people to make iPhones because they've moved on from inventing the

600
01:00:05,040 --> 01:00:11,280
iPhone to inventing something else? Or is that just a complete socialist type of dream?

601
01:00:12,400 --> 01:00:17,280
Well, some of this just happens over time, but it's longer time frame. So

602
01:00:18,160 --> 01:00:26,080
so this is just my belief from history. 100% of the current companies will all become defunct.

603
01:00:27,120 --> 01:00:33,760
Somewhere in the next 10 to 50 years. Like there's like essentially zero companies left from 100

604
01:00:33,760 --> 01:00:40,400
years ago. I think General Electric is one of the few. So all the fortune one. Yeah.

605
01:00:40,880 --> 01:00:47,440
There's the biggest companies during the expansion westward were all windmill manufacturing

606
01:00:47,440 --> 01:00:57,120
companies. Yeah. Yeah. And then there was the oil and oil steel railroads. They were the high tech.

607
01:00:57,120 --> 01:01:04,160
They were the booming startups in the mid 1800s, early to mid 1800s. And then they were the

608
01:01:05,120 --> 01:01:12,320
you know, all the copies that ran the United States and literally lent money to the U.S.

609
01:01:12,320 --> 01:01:18,160
government to keep the float. And you can imagine how I was shifting that whole thing was, they

610
01:01:18,160 --> 01:01:22,560
literally called them the robber parents, you know, but then their descendants created all

611
01:01:22,560 --> 01:01:27,760
these foundations. So now people go to the Ford Foundation and Carnegie Mellon University. And

612
01:01:28,560 --> 01:01:33,280
you know, but those companies are 100% gone. And that cycle will repeat

613
01:01:34,080 --> 01:01:39,280
inevitably. And then some of that will create space for people to make new phones and stuff.

614
01:01:39,280 --> 01:01:44,400
But some of those devices, some of the devices, you know, like nobody really wants an iPhone 6

615
01:01:44,400 --> 01:01:50,000
because an iPhone, you know, 12 is good enough and cheap enough and serves the need. And, you know,

616
01:01:50,000 --> 01:01:56,640
the we're transitioning to like a different kind of computer in 10 years will be AI computers that

617
01:01:56,640 --> 01:02:02,720
we deal with. And the question is, you know, who owns and who controls it? How do you build them?

618
01:02:02,720 --> 01:02:06,960
How many companies are there? Who are the players? And it's going to be kind of wild.

619
01:02:09,520 --> 01:02:14,880
I don't think it'll be many of the current big players. And some of the current funded startups

620
01:02:14,880 --> 01:02:19,600
may be the ones, but some of the numbers that lead to next generation, you might not have heard

621
01:02:19,920 --> 01:02:27,840
yet. So this is again, this is another one. Sorry. I'm just curious if you think this is a

622
01:02:27,840 --> 01:02:32,480
fixed law of the universe, the lifespan of these companies, because it seems like bureaucracies

623
01:02:32,480 --> 01:02:36,560
can survive for a very long time. I know there's some Chinese bureaucracies that last almost a

624
01:02:36,560 --> 01:02:42,560
thousand years. I think it's the Chao dynasty. But it doesn't seem like necessarily the organizational

625
01:02:42,560 --> 01:02:48,960
structure is dated, right? It doesn't have an expiration date as much as something else about

626
01:02:48,960 --> 01:02:55,680
their inability to adapt to the needs of new customers. What is that? Is it possible?

627
01:02:57,440 --> 01:03:01,920
Well, I don't know if you want to, well, like the first step is understanding also, like,

628
01:03:02,720 --> 01:03:07,280
like, so business bureaucracies have self limits, because these companies become bureaucratic,

629
01:03:07,280 --> 01:03:12,880
they don't respond to the market, they consume all their revenue for, you know, nonsense. And

630
01:03:12,880 --> 01:03:18,640
they go bankrupt, right? So then there's these quasi businesses like utilities, which are horribly

631
01:03:18,640 --> 01:03:24,320
run, or big companies like Ford and Chrysler who have such big unions and political clout

632
01:03:25,040 --> 01:03:28,720
that they got bailed out by the government despite the fact that they probably should have been

633
01:03:28,720 --> 01:03:34,560
bankrupt. Now, they will tell you that they paid back the loans. And I'm again, I'm not an expert,

634
01:03:34,560 --> 01:03:40,960
maybe they did, maybe it was great. And but then government bureaucracies are a little funnier,

635
01:03:40,960 --> 01:03:47,120
because, you know, they're not funded by their success, they're funded by taxes and oddly enough,

636
01:03:47,200 --> 01:03:54,560
the worst they do, the more they spend and the more they tax. And so until it starts,

637
01:03:54,560 --> 01:03:58,000
it's having such a big impact that there's some either kind of internal political

638
01:03:58,720 --> 01:04:04,240
blush, or in fact, they failed so badly that, you know, they lose a war, like historically big

639
01:04:04,240 --> 01:04:10,880
corrupt countries, you know, didn't end well as it were. But those cycles can be longer. And then

640
01:04:10,880 --> 01:04:15,920
there's some bureaucracies, like there's some Japanese companies that are hundreds of years old.

641
01:04:17,360 --> 01:04:21,440
Some of them are successful. And but some of those are family businesses where the

642
01:04:22,560 --> 01:04:28,400
families escaped, you know, then the in the Western world, like family money only lasts like

643
01:04:28,400 --> 01:04:34,880
two or three generations. You know, like, like, you know, the up and comer guy who made the money,

644
01:04:34,880 --> 01:04:41,200
his his descendants do very well, if they keep growing the business, but at some point, they

645
01:04:41,200 --> 01:04:46,160
start dividing it up. And the children are raised in luxury, and they don't try very hard. And then

646
01:04:46,800 --> 01:04:51,120
some of those families, families, they go bankrupt quickly. Like is that just a cultural

647
01:04:51,120 --> 01:04:57,760
difference? East, West? Well, I suspect most rich families in Asia also go bankrupt the same way

648
01:04:57,760 --> 01:05:02,640
as over time. So but these ones, I'm not enough of an expert by just like, there's some famous

649
01:05:02,640 --> 01:05:06,800
families in Europe that are also hundreds of years old. That's true. That's true. And

650
01:05:07,600 --> 01:05:13,440
but if I remember correctly, these Japanese companies are not like big tech companies. They're

651
01:05:13,440 --> 01:05:21,920
like a pub that's been run by the same family for like 1000 years. Really? Yeah, like some of them

652
01:05:21,920 --> 01:05:27,520
are. Yeah. Yeah, there's I think that the oldest company in that I know of continuously independent,

653
01:05:27,520 --> 01:05:33,440
I think it's Nintendo. What's that? Nintendo? Yeah, I mean, yeah, yeah, Nintendo was definitely,

654
01:05:33,440 --> 01:05:40,240
it was not a handheld game company like 300 years ago. So yeah. So I think it became very famous

655
01:05:40,240 --> 01:05:52,080
for that. And then 1889. What's that? Nintendo was founded in 1889. Play cards. Yeah. And was it

656
01:05:54,240 --> 01:06:00,560
Nokia was a rubber company. You know, they became famous for phones. So sometimes they

657
01:06:00,560 --> 01:06:05,360
transition, but you know, it's so again, to the socialist point,

658
01:06:05,600 --> 01:06:12,640
like, I don't know, people work really hard when they see personal benefit and

659
01:06:14,480 --> 01:06:18,800
personal goals. Like most people don't work for money. I don't know anybody that works.

660
01:06:18,800 --> 01:06:23,360
Like once you're successful in high tech, you're not working for money. You're working for goals,

661
01:06:23,360 --> 01:06:29,600
for interest. You know, somebody said once you have $100 million, you know, you buy a couple

662
01:06:29,600 --> 01:06:35,280
houses in a ranch and a boat and a plane. And after that, like, one of the really rich people

663
01:06:35,280 --> 01:06:41,680
work for money. And most of them, you know, it's sort of like the money exists, but it's more

664
01:06:41,680 --> 01:06:49,520
like stewardship of an asset, right? Like people who have assets invested or they make decisions

665
01:06:49,520 --> 01:06:55,280
about it. They're, you know, it's a complicated thing. And then the question when you have assets,

666
01:06:55,280 --> 01:07:01,840
who should use them? And what does it even mean? Like, if you're a big shareholder in Tesla today,

667
01:07:01,840 --> 01:07:06,880
what do you own? Well, you own the production capacity of employees of 100,000 people around

668
01:07:06,880 --> 01:07:13,840
the world making millions of cars. And you mostly don't just spend that money because it's, if you

669
01:07:13,840 --> 01:07:17,840
sell your Tesla shares to spend the money, you don't own Tesla anymore. And if you own Tesla shares,

670
01:07:18,560 --> 01:07:24,400
your money is in a factory that's making cars, you know, so it's a, you know, like what does asset

671
01:07:24,400 --> 01:07:31,600
ownership mean? It's a complicated thing. And, you know, generally speaking, you know, there's

672
01:07:31,600 --> 01:07:37,120
producers in the world and consumers, and the consumers mostly don't produce anything.

673
01:07:38,400 --> 01:07:45,040
Seems like there's a flaw in the design of these corporations because the

674
01:07:46,560 --> 01:07:50,960
arm of the corporation that's invested in making money and selling stuff,

675
01:07:51,840 --> 01:07:58,080
sometimes works at odds with the department that is innovating and coming up with new ideas.

676
01:07:58,080 --> 01:08:03,760
A new idea is always less profitable than an old idea that you could just iterate and sell a little

677
01:08:03,760 --> 01:08:10,720
bit. Yeah, I know. And so it's called creative tension, I think that's flaw. Creative tension?

678
01:08:12,400 --> 01:08:16,240
Yeah, the creative tension between should you produce what's making money now,

679
01:08:17,040 --> 01:08:23,120
or should you invest in something new? And, and by the way, companies go bankrupt on both rails,

680
01:08:23,840 --> 01:08:30,000
so they don't make the new product and then nobody cares about them, right? You know,

681
01:08:30,000 --> 01:08:35,360
blockberry, like who cares about touchscreens? That didn't work out so well. And then other

682
01:08:35,360 --> 01:08:40,000
people invest in all kinds of new stuff and it doesn't work out. And you kind of look back and

683
01:08:40,000 --> 01:08:47,440
you think if they just kept making comfortable shoes, they'd been fine. And so the, you know,

684
01:08:47,440 --> 01:08:51,520
it's sort of like the same graph of like what percentage of humans have descendants after

685
01:08:51,520 --> 01:08:58,640
10 generations? The answer is not very many. What, what percentage of corporations are successful

686
01:08:58,640 --> 01:09:06,160
after 100 years, you know, essentially 1% or something. But that's because we live in a very

687
01:09:06,160 --> 01:09:11,440
experimental universe. Like the number of things that can go wrong is like essentially infinite.

688
01:09:11,440 --> 01:09:19,200
And the percentage of things that could go right is relatively low and often very dependent on

689
01:09:19,200 --> 01:09:26,320
luck in the current state. You know, so like the iPhone five years earlier have been done. As a

690
01:09:26,320 --> 01:09:30,960
matter of fact, they tried it a couple of times and the technology wasn't there. But when they

691
01:09:31,040 --> 01:09:38,000
finally did it, it was pretty good. And it seems like that might be what, and there's some inverse

692
01:09:38,000 --> 01:09:43,760
of that, which is that when the winds shift and you have a model that works, you're kind of screwed.

693
01:09:43,760 --> 01:09:49,200
Like we were talking earlier about Google being an ad serving business. At the end of the day,

694
01:09:49,200 --> 01:09:55,040
the goal is to get you to look at ads and those ads are having like their money. But with generative

695
01:09:55,120 --> 01:10:01,680
AI, you can go ask the same, the same question to chat with the tea or cloud or whoever it is

696
01:10:01,680 --> 01:10:07,120
that you're using. And there's no mechanism for serving you ads through that. Oh, there already

697
01:10:07,120 --> 01:10:12,800
is. There is. I have friends they're doing, they're sending me screenshots of they did a search and

698
01:10:13,360 --> 01:10:21,920
the search said, Oh, you might be in this article in the Atlantic or so the, the AI models are fine

699
01:10:21,920 --> 01:10:28,960
tuned with their advertisers data. So no, don't worry about that. Even worse, like movies and

700
01:10:28,960 --> 01:10:34,560
everything. So when you're watching a movie, even worse, even better, hard to say comes in your

701
01:10:34,560 --> 01:10:40,320
perspective. So you know how the, you know, Coca Cola famously paid, you know, Marvel Studios,

702
01:10:40,320 --> 01:10:47,680
all this money to put a coke can on a table. Well, that only works for a certain segment of

703
01:10:47,680 --> 01:10:52,080
the audience. Like some other segment, I guess, wants to see a can of, you know, pork and beans.

704
01:10:53,360 --> 01:11:01,520
So all the AI generated video will, will gain the environment to affect your perceptions.

705
01:11:02,160 --> 01:11:05,840
Also, like we'll be watching different movies, essentially. Yeah, yeah, everybody will be watching

706
01:11:05,840 --> 01:11:11,200
different movies. Taylor to my interests, might, might have all those fun areas she gets. Well,

707
01:11:11,200 --> 01:11:15,360
tell them some, some combination of your interest and what somebody wants you to think.

708
01:11:16,240 --> 01:11:20,880
Oh, it's really interesting because I'm trying to think of like Iron Man and having a can of pork

709
01:11:20,880 --> 01:11:26,240
and beans on his table instead of having a can of Coca Cola. And there's just something so strange

710
01:11:26,240 --> 01:11:31,520
about that. But I suppose, yeah, there's a 7.4% target market that would have loved that pork and

711
01:11:31,520 --> 01:11:38,880
beans. Like, you can't be, can't be cake, Coca Cola, you know, like bias, you have to,

712
01:11:39,600 --> 01:11:41,680
have to be open, the pork and beans experience.

713
01:11:45,200 --> 01:11:51,920
Okay, but so I have, I have like an overarching question about where this is all going.

714
01:11:52,480 --> 01:11:57,600
Yeah, I'm curious about still about the lifespan feature. Do, how much

715
01:11:58,640 --> 01:12:02,880
fallout is there in the death of a company? Like in the bankruptcy phase, how is that

716
01:12:02,880 --> 01:12:08,720
devastating beyond just the people who are working there? And is there anything like an end of life

717
01:12:09,040 --> 01:12:13,520
plan for companies? Or is this all sort of surprising when it happens, even though it's

718
01:12:13,520 --> 01:12:20,880
completely inevitable? Finder that people pull out, which is like in case of the end, do this.

719
01:12:20,880 --> 01:12:27,200
Yeah, yeah, for most companies, well, like I said, the, you know, the, the truism is they went

720
01:12:27,200 --> 01:12:32,800
bankrupt slowly at first, but then all at once. And, and lots of times people are very surprised.

721
01:12:33,520 --> 01:12:39,360
Now, so there's the question of what, what if anything should be done about that?

722
01:12:40,080 --> 01:12:46,800
So like most companies pivoted, that used to be you work for a company for 30 years and retired

723
01:12:46,800 --> 01:12:52,960
and got your pension from the company, right? And that like, now that's 100% over. All your

724
01:12:52,960 --> 01:13:00,080
pensions are what's called portable pensions and earned 401ks or IRAs. And so as you work there,

725
01:13:00,080 --> 01:13:05,840
you, you invest, and maybe the company is, but it's a great company invests in your retirement

726
01:13:05,840 --> 01:13:12,240
fund, which you own. So that's good. And then depending on, you know, dumb luck and everything

727
01:13:12,240 --> 01:13:18,720
else that may or may not grow at the stock market. So that's, that's a thing. Some companies get

728
01:13:18,720 --> 01:13:23,760
propped up because it's like too big to fail. Like, but then, then you have the opposite effect,

729
01:13:23,760 --> 01:13:29,200
which is what like, why do we prop up big banks and financial institutions? Because it didn't

730
01:13:29,200 --> 01:13:34,880
make them better companies basically taught them the valuable lesson that financial management was,

731
01:13:34,880 --> 01:13:42,320
you know, for babies and, and, you know, they can get away with anything. And so those, and that's

732
01:13:42,320 --> 01:13:47,520
where you get into the, you know, complex relationship between investors and companies and

733
01:13:47,520 --> 01:13:53,200
governments and regulators and revolving doors between regulators and companies. And, you know,

734
01:13:53,200 --> 01:14:00,720
my personal view is it would be better if companies failed more often. And, you know,

735
01:14:00,720 --> 01:14:05,600
you could imagine a regulation that say the life cycle of a government organization should be more

736
01:14:05,600 --> 01:14:11,840
like 30 years, because then you could go through a 10 year growth phase and a 20 year, you know,

737
01:14:11,840 --> 01:14:18,240
productivity phase and then get shut down before bureaucracy takes over 100% of your output.

738
01:14:18,240 --> 01:14:25,600
Because, you know, to your question was like, if you just said what percentage of our output is

739
01:14:25,600 --> 01:14:32,960
wasted on our bureaucracy, and it, it, it slowly grows. And then 100 years for most companies,

740
01:14:32,960 --> 01:14:41,200
it's over 100%. So the bureaucracy taxes, you know, a minimum of a percent a year. And at

741
01:14:41,200 --> 01:14:45,360
certain phases that goes up faster, and then occasionally gets reset, like Microsoft was

742
01:14:46,160 --> 01:14:51,600
clearly, you know, reinvented for a whole bunch of resuppos, clearly.

743
01:14:51,600 --> 01:14:56,320
Is that reset possible with the governments or with an academic institution? Because these

744
01:14:56,320 --> 01:15:01,520
are the real big bureaucracies today, as far as I can tell. Yeah. So universities used to be reset

745
01:15:01,520 --> 01:15:07,840
all the time because they were funded by their endowments, their patrons and their students,

746
01:15:07,840 --> 01:15:13,120
but now they're funded by the government. So they're, you know, so they've been

747
01:15:13,120 --> 01:15:19,520
detached from the market cycle at some level. Now some, some universities, I have a couple

748
01:15:19,520 --> 01:15:25,440
of friends I've been teaching for a long time, and they get cuts every year to the professors.

749
01:15:26,720 --> 01:15:32,400
And while the bureaucracy continues to grow, so they used to, their budgets used to be 10%

750
01:15:32,400 --> 01:15:40,240
bureaucracy, 10% facilities, and 80% professor salaries. And now it's like 40% bureaucracy

751
01:15:40,320 --> 01:15:48,960
and 30% facilities and 20% percent professor overhead, professor of overhead. And, you know,

752
01:15:48,960 --> 01:15:52,640
like it's amazing. They take half those grants, right? Half those government grants that

753
01:15:52,640 --> 01:15:59,040
I've been trying to set for this kind of grant, they get 50%. This kind of grant, they get 60%.

754
01:16:01,360 --> 01:16:02,880
And then they can, and then, yeah.

755
01:16:02,880 --> 01:16:07,760
So any hope there? I mean, you imagine a reset that's possible in such a

756
01:16:08,240 --> 01:16:11,280
there's, there's a whole bunch of people trying to invent something outside the current

757
01:16:11,280 --> 01:16:16,800
university structure. Alternative institutions. Yeah. So, but they still have an alternative

758
01:16:16,800 --> 01:16:23,200
government though. Yeah. Well, now governments have gone through a really big reset. You know,

759
01:16:23,200 --> 01:16:28,320
the United States government has gone through several major resets. Like the Civil War.

760
01:16:29,520 --> 01:16:33,120
Oh, so you guys are too young. So I have a friend who's very worried about the current

761
01:16:33,120 --> 01:16:40,240
government situation. So when I was a kid, you know, they were, they were taking young men

762
01:16:40,240 --> 01:16:45,040
at essentially gunpoint and sending them to Vietnam to kill people for, you know, no good reason.

763
01:16:45,920 --> 01:16:53,280
And then the National Guard shot students on campus. And then the police department beat up

764
01:16:53,280 --> 01:16:58,160
protesters at the Democratic National Convention on television. Like these things happen.

765
01:16:58,160 --> 01:17:04,400
And the Democratic Party lost brutally for multiple

766
01:17:05,680 --> 01:17:09,680
elections until Bill Clinton basically reinvented the Democratic Party.

767
01:17:10,560 --> 01:17:17,920
A really big reset. Like it happened. But then they became bureaucratic just the way,

768
01:17:19,040 --> 01:17:23,760
you know, the modern parties are basically in the same boat they were. And the Reagan

769
01:17:23,760 --> 01:17:29,760
revolution was somewhat real. You know, the, you know, Nixon, Nixon got taken out for a bunch

770
01:17:29,760 --> 01:17:36,960
of complicated reasons. And Ford was a loser. And, you know, like it was quite a turnover in the 60s

771
01:17:36,960 --> 01:17:43,520
and 70s. And then things have to get bad enough. And then the pendulum swing. Yeah. Yeah. And

772
01:17:43,520 --> 01:17:49,200
there's lots of people who study like the 20 year cycles and 60 and 80, 100 and 200.

773
01:17:49,920 --> 01:17:52,720
They were clearly going through one of those cycles. Like,

774
01:17:53,840 --> 01:17:56,320
like it's hard to believe like who's running for president now.

775
01:17:57,600 --> 01:17:58,880
The year without an election.

776
01:17:59,760 --> 01:18:04,800
Yeah. Yeah. Something that, and that's probably what people will say. And then people look back

777
01:18:04,800 --> 01:18:11,040
and go, well, how did they let that happen? But when I was kid, they taught us about yellow

778
01:18:11,040 --> 01:18:16,480
journalism and the robber barons and the Great Depression. And, you know,

779
01:18:17,280 --> 01:18:24,320
like we went through multiple cycles, like, literally since 1850, that I was taught in detail

780
01:18:24,320 --> 01:18:29,760
about. And, you know, like we're in another one of those cycles, which will be in the history books.

781
01:18:29,760 --> 01:18:37,280
And there'll be, you know, the great deficit and the great election, something or other,

782
01:18:37,280 --> 01:18:41,280
who knows what they'll call it. And like these cycles keep happening.

783
01:18:41,520 --> 01:18:45,520
So, so the big difference, and this, this took me a while.

784
01:18:46,960 --> 01:18:49,600
I was talking to my friend, because he's very anxious, you know, he's younger,

785
01:18:50,640 --> 01:18:54,640
you know, 30 something. And I think everybody has that, by the way.

786
01:18:55,200 --> 01:18:59,200
What's that? I think every single person listening probably has at least one friend that's

787
01:18:59,200 --> 01:19:02,960
in a similar position with respect to the government situation right now.

788
01:19:02,960 --> 01:19:09,520
Yeah. Yeah. So it took me a lot of like, when, when I was going through that and watching television,

789
01:19:09,520 --> 01:19:12,880
I was a young person. And, but, you know, there was a threat that I could be sent to

790
01:19:12,880 --> 01:19:18,240
Vietnam. And my parents were anti-war protesters. And there was literally, you know,

791
01:19:18,240 --> 01:19:23,360
also civil rights activists, which is why, because my father was a Reagan Republican,

792
01:19:23,360 --> 01:19:29,520
but, but it was a moral statement, not a left wing statement. And he had friends that literally

793
01:19:29,520 --> 01:19:35,280
got beat by police in Philadelphia. So, like, like there was a real threat. And we didn't know how

794
01:19:35,280 --> 01:19:40,560
it was going to go. Like we sat around the kitchen table, and my parents would pray

795
01:19:41,200 --> 01:19:46,560
that we would live through this. Like that was my childhood. But we did.

796
01:19:48,480 --> 01:19:53,200
Now it could be, now I have a, well, you can live through almost anything attitude,

797
01:19:53,200 --> 01:19:58,480
which would be wrong. It was, you know, that was pretty threatening. And, you know, it was,

798
01:19:59,680 --> 01:20:03,840
you know, there was like, when we were kids, we used to train for atomic fallout by

799
01:20:03,840 --> 01:20:09,120
halving under the desks. I was six years old, and we're, you know, we're seeing the six-year-old

800
01:20:09,120 --> 01:20:12,720
desk at the top of the desk is about this big. And we would get underneath it to protect us

801
01:20:12,720 --> 01:20:19,360
from fallout, which we had no idea what that was. But there was air raid sirens and, you know,

802
01:20:20,480 --> 01:20:23,840
you know, running the classroom and hide under these little tiny desks.

803
01:20:25,040 --> 01:20:26,880
That was fairly, fairly daunting.

804
01:20:28,160 --> 01:20:33,200
But people have lived through so many hard things. Like my mom will tell me stories of

805
01:20:33,840 --> 01:20:41,520
the arc of our family in the Soviet Union. And it's just insane, right? Like Russian Jews

806
01:20:41,520 --> 01:20:50,800
in 1920 that are, that are running from the Red Army that are, you know, and then like her entire

807
01:20:50,800 --> 01:20:57,040
branch of the family left St. Petersburg and they went into exile in Siberia. And then they ended

808
01:20:57,040 --> 01:21:02,240
up joining one of these communes and the communes, they took all your passports so you could never

809
01:21:02,240 --> 01:21:10,160
leave. And so like the husband of the family was either killed or arrested. And it was this

810
01:21:10,160 --> 01:21:15,600
woman with eight children, no food. She can't leave because she doesn't have any of her papers.

811
01:21:15,600 --> 01:21:21,600
And she manages like her dad would tell her stories of they would gather wild hemp seeds

812
01:21:21,600 --> 01:21:25,840
and squeeze them for oil. And that was the only fat that they would be able to get.

813
01:21:26,800 --> 01:21:31,760
And yet they made it, right? Like my parents live in California now. And the memories of

814
01:21:31,760 --> 01:21:35,920
everything that they went through have kind of faded into the background. And so I developed

815
01:21:35,920 --> 01:21:41,600
this feeling about the political shifts, which is like, yeah, they happen, but life exists

816
01:21:43,600 --> 01:21:50,240
on the foreground. While all of that stuff happens sort of in some ways beyond our control,

817
01:21:50,240 --> 01:21:56,080
like these are systems that run. And our task is to figure out how to do something meaningful

818
01:21:56,080 --> 01:21:59,360
and worthwhile, despite the fact that the background is so insane.

819
01:22:00,000 --> 01:22:06,240
So this gets to the philosophical point, which is, you know, these cycles exist.

820
01:22:07,280 --> 01:22:15,680
So the, you know, the Nassim Talib is like, the harder you work to keep it frozen, the worse the

821
01:22:15,680 --> 01:22:22,880
end is, you know, you know, the long tail event, like in the financial world, trying to regulate

822
01:22:22,880 --> 01:22:29,760
the system to avoid crashes makes the crash worse. Now, may in fact put it off. And then there's,

823
01:22:29,760 --> 01:22:35,680
you know, the creative tension between, should you crash early enough and, and then deal with it

824
01:22:35,680 --> 01:22:42,560
all the time, but then you have smaller ones, or should you try to build a robust financial system

825
01:22:43,440 --> 01:22:47,120
that, you know, avoids that. And the same with political systems, like

826
01:22:48,000 --> 01:22:51,760
United States for the most part has had a pretty dynamic political system, but

827
01:22:52,720 --> 01:23:00,160
you know, the current alternative seem, you know, there's a funny phenomena, like

828
01:23:01,040 --> 01:23:05,840
in a negotiation between multiple parties with the stake in the outcome, you compromise towards

829
01:23:05,840 --> 01:23:11,440
the middle, but in bubbles of, you know, like-minded people, it tends to, let's say,

830
01:23:11,520 --> 01:23:18,000
negotiate towards the edges, you know, I'm the most virtuous, no, I'm the most virtuous, no, you

831
01:23:18,000 --> 01:23:24,480
know, and, and so that's a, that's an interesting dynamic, which also plays out organizationally,

832
01:23:24,480 --> 01:23:32,160
like when, when there's a proper negotiation of the parties, something interesting happens.

833
01:23:32,960 --> 01:23:38,640
But when there's a, you know, autocratic ruler makes stupid decisions, or I got to describe

834
01:23:39,600 --> 01:23:43,600
this business group, they were making so much money, nobody wanted to call them on it,

835
01:23:43,600 --> 01:23:46,320
but their decisions were actually killing the company as a whole.

836
01:23:47,360 --> 01:23:53,360
And, you know, these, these, these systems happen at multiple, like I have a belief that

837
01:23:53,360 --> 01:23:58,800
these systems play out at multiple levels. So, you know, like how family dynamics work

838
01:23:58,800 --> 01:24:04,080
with a small number of people, looks a whole lot like a small team dynamics work, which looks a

839
01:24:04,080 --> 01:24:10,000
whole lot like extended family dynamics work. And that's because we're humans. Now, if we weren't

840
01:24:10,000 --> 01:24:16,400
humans, say we were beings with a thousand life cycle and we didn't have families and,

841
01:24:17,120 --> 01:24:21,840
you know, we, we could keep track of like a thousand numbers in our head instead of seven.

842
01:24:22,800 --> 01:24:26,080
Would our dynamics play out differently? I think they'd be a lot different.

843
01:24:27,360 --> 01:24:32,080
But our dynamics that we live and play out the way they do because of our biological,

844
01:24:32,080 --> 01:24:39,280
you know, grounding and our evolutionary process. And we co-evolved with our culture,

845
01:24:39,280 --> 01:24:44,880
which gave us mental capacities for a whole bunch of things. We co-evolved with our lifespan,

846
01:24:44,880 --> 01:24:52,400
which gives us, you know, let's say expectations and pretty deeply ground ideas about what to do.

847
01:24:56,080 --> 01:24:59,520
And you have to understand like 10 levels of that to make any sense out of anything.

848
01:25:00,240 --> 01:25:03,520
And so there's something that's really crazy that's happening right now,

849
01:25:03,520 --> 01:25:08,640
which I think has been a continuous project since the invention of the first computers,

850
01:25:09,360 --> 01:25:17,600
which is to create a fundamental shift in the biology of humans. Like I don't necessarily

851
01:25:17,600 --> 01:25:22,800
think that the first people who sat around and were asking how to build a computer had this in

852
01:25:22,800 --> 01:25:32,640
line. But I see it as this emergent step in evolution, where we had the dream of being able

853
01:25:32,640 --> 01:25:38,800
to make a machine that could think. And the first generations of those machines were only able to

854
01:25:38,800 --> 01:25:45,760
think in very basic ways, according to the way that we programmed them. And then progressively,

855
01:25:45,760 --> 01:25:51,840
we increase the speed with which they can think the number of complex processes that they can run.

856
01:25:51,840 --> 01:25:54,880
But still, you're limited by the fact that it is, you know,

857
01:25:56,800 --> 01:26:02,960
garbage in, garbage out. If you don't know how to work the machine, you can't make it go.

858
01:26:04,560 --> 01:26:14,640
But as we develop tools that are intelligent, and we start to gradually mount the computational

859
01:26:14,640 --> 01:26:21,280
power of computers with the computational power of humans, and then we also start to

860
01:26:21,280 --> 01:26:29,680
manipulate the lifespan of humans in order to extend it, what effect is that going to have?

861
01:26:29,680 --> 01:26:36,160
Because it seems like it's this project that we're running full throttle into. And obviously,

862
01:26:36,160 --> 01:26:41,200
you do, right? We build tools, that's what humans do. But I feel like there's something

863
01:26:41,200 --> 01:26:47,200
that's fundamentally different about the tool that is the steam engine and the tool that is AI

864
01:26:48,080 --> 01:26:51,360
and bioengineering combined with AI.

865
01:26:51,360 --> 01:26:58,080
Yeah. Let me try to take, yeah, there's about a couple of different ideas there that I think

866
01:26:58,960 --> 01:27:06,080
it's interesting to take it apart. So starting with your last one. So we made a whole bunch of

867
01:27:06,080 --> 01:27:14,400
tools which didn't really, or did only slowly re-engineer our biology, but they re-engineered

868
01:27:14,400 --> 01:27:22,080
our culture. So all over the world, human beings live in, you know, aboriginal humans live in tribes

869
01:27:22,080 --> 01:27:29,600
of 100 to 200 adults. And that's a human constant. As best I can tell, I've read a number of books

870
01:27:29,600 --> 01:27:37,040
about it. And so our ape relatives Gibbons are famously monogamous, and orangutans are solitary

871
01:27:37,040 --> 01:27:42,720
territorial with some really interesting sideshows, and gorillas are a dominant male with a harem,

872
01:27:43,360 --> 01:27:49,280
and chimpanzees are a dominant group of adults, you know, three to five males and three to five

873
01:27:49,280 --> 01:27:57,200
females in a group of 30 to 40. They can go from a solitary animal to a pair-bonded animal to a

874
01:27:57,200 --> 01:28:06,320
harem animal to 30 to 40 to 100 to 200. So that's our organic basis. But with tools, you know,

875
01:28:07,280 --> 01:28:14,480
you know, and who knows which one drove it? Was it farming? Was it herding? Was it bow and arrows?

876
01:28:14,480 --> 01:28:23,840
Was it whatever? We used our tools to engineer culture and society. And then essentially,

877
01:28:23,840 --> 01:28:27,760
the person that could keep the biggest village together could win against the other villages.

878
01:28:27,760 --> 01:28:34,720
And then at some point, that also caused, let's say, a massive escalation or political abilities,

879
01:28:34,720 --> 01:28:38,960
because if you want to trade with that village and get rich or go to war with them and get killed,

880
01:28:38,960 --> 01:28:46,960
like there's, you know, so our tools, what we co-evolve with our tools and our culture, right?

881
01:28:46,960 --> 01:28:52,160
And some cultures are way more effective than other ones that are making new tools. And but,

882
01:28:52,160 --> 01:28:58,240
you know, that's also a very chaotic function. And so we're making new tools, which is going to

883
01:28:58,240 --> 01:29:04,560
cause some additional cultural change. And it's already doing it in some of that good ways and

884
01:29:04,560 --> 01:29:11,120
bad ways. Like the internet is full of people speculating about it. But, you know, we've gone

885
01:29:11,120 --> 01:29:17,520
through a lot of evolution. Now, in computing, computers don't think, right? And there's a

886
01:29:17,520 --> 01:29:26,000
funny thing. So the first computers, you know, executed sets of statements, which, you know,

887
01:29:26,000 --> 01:29:30,400
you could do with a pencil and paper is just, you know, when they do a billion instructions a

888
01:29:30,400 --> 01:29:37,840
second, it's a little faster than, you know, one a second. But that's a matter of quantity, right?

889
01:29:39,280 --> 01:29:46,880
And then, like, so the statement kind of computer, and then we went into, you know,

890
01:29:46,880 --> 01:29:52,560
there was the big data revolution and analysis, where you could run so many programs that cost

891
01:29:52,560 --> 01:29:58,000
data, you could find signal where nobody could. And it's almost like we got into pattern recognition.

892
01:29:58,000 --> 01:30:03,600
So you went from executing statements to something that seemed to do something different,

893
01:30:04,560 --> 01:30:10,480
like pattern recognition. And then the early AI programs, you know, like the famous Alexnet,

894
01:30:10,480 --> 01:30:15,200
which could recognize a cat in a photo. Like that's kind of a wild thing. But,

895
01:30:16,160 --> 01:30:22,000
and it was way better than what's called classic vision, which attempted to find cats by correlation.

896
01:30:22,000 --> 01:30:25,760
They would say a cat is a collection of pointy ears and round eyes and fuzzy hair,

897
01:30:26,480 --> 01:30:32,880
and cute smile or something. And then you could have a database of when you see these parameters,

898
01:30:32,880 --> 01:30:38,080
these features with these ratios, it's a cat versus these features, it's a dog versus,

899
01:30:38,720 --> 01:30:44,640
you know, the problem is or something. So, so that, you know, the statements to, you know,

900
01:30:44,640 --> 01:30:49,120
like analysis, and then recognition. And then the language model,

901
01:30:50,320 --> 01:30:55,120
you know, people are very puzzled by this because they're sort of stupidly good at chit chat.

902
01:30:56,080 --> 01:31:02,080
And summarizing things and predicting the next word. And they do something that seems almost

903
01:31:02,080 --> 01:31:10,640
amazing. Like you can think of having multiple equations like, you know, x plus y plus 3z equals

904
01:31:10,640 --> 01:31:17,840
five and two x plus seven y plus z squared equals seven. So those are equations. And then you can

905
01:31:17,840 --> 01:31:24,000
do algebraic processes that to do something with, but you can't add the word cat and dog.

906
01:31:24,800 --> 01:31:31,440
Like cat plus dog doesn't mean anything. Does that make sense? But you can translate

907
01:31:31,440 --> 01:31:35,600
cat and dog into a space where you could add them up.

908
01:31:37,920 --> 01:31:45,120
Right. And then you could query a statement against the words. And it turns out with enough

909
01:31:45,120 --> 01:31:52,320
computation, you can create really called embedding. And then there's all kinds of transformations

910
01:31:52,320 --> 01:31:57,520
you do on that data, where you essentially you've turned ideas into mathematical things. And I know

911
01:31:57,520 --> 01:32:03,120
they're mathematical, but I can actually look inside the computer chip and look at the register

912
01:32:03,120 --> 01:32:08,080
that has the bits in it that I'm adding. But there's no magic in AI computers.

913
01:32:09,600 --> 01:32:17,280
They're just numbers being added up. But like the representation of the word cat and dog isn't

914
01:32:17,280 --> 01:32:24,720
a very good representation for manipulation, let's say. So some modern computers, we can do

915
01:32:24,720 --> 01:32:31,360
something really interesting, which is with a set of words, predict the next good word.

916
01:32:32,320 --> 01:32:38,080
Or with two sets of ideas, put them together and then summarize that. It's kind of amazing.

917
01:32:38,720 --> 01:32:45,040
And it seems relatively intelligent. Because in fact, the current state of play is

918
01:32:46,000 --> 01:32:51,760
it's ability to write, summarize, analyze and talk to you is better than the average person.

919
01:32:52,960 --> 01:32:54,240
But it's not clear it's thinking.

920
01:32:55,680 --> 01:32:58,240
Yeah, I was going to say, how do you define intelligence and thought?

921
01:32:59,040 --> 01:33:04,320
Yeah. So you know the joke, every time intelligence can do something, it stops being AI.

922
01:33:04,960 --> 01:33:10,320
So like, surely it's AI when you can play chess. But then we wrote a program that clearly wasn't

923
01:33:10,400 --> 01:33:16,960
intelligent play chess, but go. That's so hard. You have to be intelligent. No, that was just a

924
01:33:16,960 --> 01:33:23,360
program to like to recognize generally any object in a picture, you must be intelligent.

925
01:33:24,080 --> 01:33:29,360
No, that was easy. That happened 10 years ago. Right. To complete this sentence, that must be

926
01:33:29,360 --> 01:33:36,080
intelligence. Nope. The turning test. I talked to you for 10 minutes and I can't tell it's

927
01:33:36,720 --> 01:33:42,240
not a person. That's been passed. The bar exam. That must be artificial intelligence.

928
01:33:42,960 --> 01:33:49,520
Now that was easy. Like so. So we don't, so what's happening is intelligence is being defined by

929
01:33:49,520 --> 01:33:56,960
what it is, which is crazy. Right. And then today people say, well, the AI models are no smarter

930
01:33:56,960 --> 01:34:04,800
than the input that they we've had it. Well, human beings aren't data and intelligence.

931
01:34:05,600 --> 01:34:10,800
Like Christ, the average person barely gets 100 megabytes of data. And by the time they can talk

932
01:34:10,800 --> 01:34:16,960
and do smart things, like work where I would call computational intelligence. So, and that

933
01:34:16,960 --> 01:34:23,840
that means some of it's trained in a way that like builds these simple spaces and representational

934
01:34:23,840 --> 01:34:31,040
things. But then we can experiment experimental, right? That's yeah, yeah, we can do variations on

935
01:34:31,040 --> 01:34:35,680
that and combine them over and over. But it turns out the variations that are being done,

936
01:34:37,040 --> 01:34:43,920
large magical, their variations in the representational space now turns out those

937
01:34:43,920 --> 01:34:49,520
representational spaces have infinite variation possibilities or very large variation possibilities.

938
01:34:50,400 --> 01:34:57,680
And we do a funny thing where we're very good at making small variations and then filtering

939
01:34:57,680 --> 01:35:04,640
for some sensibility and then combining filters of those variations for something bigger. And

940
01:35:04,640 --> 01:35:09,680
depending on how smart we are and what we learned and what, you know, let's say sense, you know,

941
01:35:11,040 --> 01:35:15,920
make sense of the constructs we have, you could be more or less successful at this.

942
01:35:16,800 --> 01:35:19,440
Like, have you ever watched like Richard Feynman lectures on math?

943
01:35:20,480 --> 01:35:24,720
Like, you're super fun because he goes, well, this is obvious, this is obvious. And he's just

944
01:35:24,720 --> 01:35:32,800
hopping across the the trail like 100 yards at a time because his ability to produce variations

945
01:35:32,800 --> 01:35:40,080
were really good. And his ability to make sense out of the possibilities were amazing and really

946
01:35:40,080 --> 01:35:46,080
well embedded in thinking. Right. And I go, well, I can spend an hour on each one of those,

947
01:35:46,720 --> 01:35:50,880
you know, cognitive beliefs he made, but I can do the same thing that appears.

948
01:35:52,640 --> 01:35:54,560
I can describe how come what's that?

949
01:35:55,440 --> 01:35:58,640
I'm trying to separate out what humans do that's different than these artificial

950
01:35:58,640 --> 01:36:02,320
intelligence programs. And it seems like something about our ability to compound

951
01:36:02,320 --> 01:36:08,000
abstractions into new layers of abstraction that I'm not sure that computers are pulling off.

952
01:36:08,720 --> 01:36:15,360
I actually think they're pretty good at that side. The programs are written yet to do the

953
01:36:15,360 --> 01:36:22,800
variation. That's not just noise. So if you just do noise, like, like there's, I think the problem,

954
01:36:22,800 --> 01:36:30,640
the crack is how to do variations that are interesting without just being noise. And I

955
01:36:30,640 --> 01:36:35,840
don't have a good idea. Like chat GPT is, I don't know, I've been chit chatting with it about, you

956
01:36:35,840 --> 01:36:42,080
know, physics and stuff. It's interesting how it puts it together. And then you can see places

957
01:36:42,080 --> 01:36:45,680
where it just falls flat and other things is like, well, that's kind of interesting.

958
01:36:46,880 --> 01:36:51,200
It's synthesis is me, I would say medium. So to pass the bar, because that's mostly factual

959
01:36:51,840 --> 01:36:57,120
associations, they can play all the games. Those are mostly role based kind of things.

960
01:36:57,200 --> 01:37:07,520
And you can see somewhere it starts to fall apart on idea synthesis. But it's not, it's not sitting

961
01:37:07,520 --> 01:37:12,960
there cranking 24 seven doing variations. Like when you think about something hard,

962
01:37:13,600 --> 01:37:19,040
like you guys are working on your physics problem, that's partly running as a background

963
01:37:19,040 --> 01:37:22,400
program in your mind for what a year, two years, some of the years.

964
01:37:25,520 --> 01:37:29,280
And well, 10 years, all right, so you have 10 years of background processing

965
01:37:30,080 --> 01:37:36,480
with a pretty well refined sense of what makes sense. And which may in fact be your biggest

966
01:37:36,480 --> 01:37:40,560
problem, you may be filtering out all the best ideas. Right? That's a, that's a real problem

967
01:37:40,560 --> 01:37:48,080
for smart people. So it's kind of complicated. So I don't, I think the synthesis is getting

968
01:37:48,080 --> 01:37:52,800
better pretty fast. And people keep saying, we see at the current level, it's terrible,

969
01:37:52,800 --> 01:37:56,160
they'll never get better. And I think, well, compared to a year ago, it's unbelievable.

970
01:37:56,720 --> 01:38:02,880
Like we're on some pretty steep curves here. But this, and then this mixture of experts thing

971
01:38:02,880 --> 01:38:10,400
matches my personal model of, like there's a workspace, and many, many agents that we apply

972
01:38:10,400 --> 01:38:17,680
to it. And then we have many tools of variation and combination or when we think, and they're

973
01:38:17,680 --> 01:38:24,480
starting to build that platform of multiple experts and a common workspace. And then the

974
01:38:24,480 --> 01:38:30,800
the ability to make variations and then test the variations for say some useful just before we

975
01:38:30,800 --> 01:38:36,160
waste too much time thinking about it. Like you don't spend very much time thinking, well, what

976
01:38:36,160 --> 01:38:40,480
if I could just invert gravity? Like that's not worth the years worth of thinking, it's worth

977
01:38:40,480 --> 01:38:46,800
like a half a second worth of thinking. But what if there's a different way to look at, you know,

978
01:38:46,800 --> 01:38:53,680
these field equations, and you're, you're good at a bunch of mathematics, then you can test

979
01:38:53,680 --> 01:38:58,320
pretty fast those things. Now the problem is there might be five more variations of field

980
01:38:58,320 --> 01:39:02,160
equations that you don't know. So one of those might have been good. So you have a filtering

981
01:39:02,160 --> 01:39:05,920
problem. So you may have in fact had a good idea that you filtered it out, because you

982
01:39:05,920 --> 01:39:13,120
didn't have an analysis, a good enough thing. Like I had a friend who's better at me than a

983
01:39:13,200 --> 01:39:18,960
computer designer smarter, but I was more creative. He felt he he would run into a

984
01:39:18,960 --> 01:39:22,320
problem's idea, put it down, and I would kind of go, well, this is pretty good, but it's got

985
01:39:22,320 --> 01:39:25,920
these three problems. And then this is pretty good, but it's got these two problems. And then

986
01:39:25,920 --> 01:39:29,760
this is pretty good, but it's got this problem. And then one day I would go, if I take this and

987
01:39:29,760 --> 01:39:32,960
this and this and put it together, and I got something that works, and I would tell them,

988
01:39:32,960 --> 01:39:35,680
and he would go, how do you stick that out? And I explained it to him, he says,

989
01:39:35,680 --> 01:39:39,120
but we rejected all this ideas. And I said, you stop thinking about it.

990
01:39:40,080 --> 01:39:41,680
And I didn't stop thinking about them.

991
01:39:42,240 --> 01:39:45,680
That's the layer on the AI that I don't see yet, which is the ability to take

992
01:39:45,680 --> 01:39:50,480
disparate concepts and smash them together in a way that's unlikely, but productive.

993
01:39:50,480 --> 01:39:53,840
I just explained the whole program, we can write that in 10 minutes.

994
01:39:56,320 --> 01:40:01,440
That program wasn't hard to write. And I'm not even going to write as the AI programs

995
01:40:01,440 --> 01:40:08,560
go to write it. I think the sensibility filters are a bigger problem than the

996
01:40:08,560 --> 01:40:12,960
combination of things that is pretty good. Synthesis is medium.

997
01:40:13,600 --> 01:40:15,680
But we just started breaking that into problems.

998
01:40:17,840 --> 01:40:23,680
Yeah, so I'm fairly, well, I don't think thinking is magical. I don't think it's quantum.

999
01:40:23,840 --> 01:40:31,520
I think it's like we already built computers with more operations in a second.

1000
01:40:32,960 --> 01:40:38,880
They're like, biology is insanely efficient for computation compared to transistors.

1001
01:40:38,880 --> 01:40:42,000
But there's reasons for that. And I think we'll solve that problem, by the way.

1002
01:40:44,000 --> 01:40:47,200
Like, where do you think thoughts come from? Like, where do you have a thought?

1003
01:40:47,200 --> 01:40:48,960
You're like, oh, I thought I just thought of something.

1004
01:40:49,520 --> 01:40:52,880
Where does that come from inside of the human architecture?

1005
01:40:54,160 --> 01:41:00,480
Oh, we're, you know, like the base of, you know, what we think of thinking is, you know,

1006
01:41:00,480 --> 01:41:04,480
advanced planning systems. Well, I think so your brain grew in a bunch of layers,

1007
01:41:05,280 --> 01:41:10,000
like, you know, in a sensory cortex and a motor cortex. And, you know, we sense both the

1008
01:41:10,000 --> 01:41:16,480
external world and ourselves. At some point, you know, low level animals got good at that.

1009
01:41:17,120 --> 01:41:21,280
And then they started moving around. And then they started doing basic planning, which is,

1010
01:41:21,360 --> 01:41:25,760
you know, run towards food and run away from teas and, you know, all kinds of things.

1011
01:41:26,320 --> 01:41:31,520
And then we've been elaborating planning more and more and more. So when you're just sitting

1012
01:41:31,520 --> 01:41:36,880
there doing nothing, the various parts of your brain will be like sort of newling on what should

1013
01:41:36,880 --> 01:41:43,280
I do? What should I think? How should I get ahead? And it turns out we've elaborated that so far.

1014
01:41:43,280 --> 01:41:48,800
We can think, how do I plan ahead in mathematics? How do I plan ahead in art? How do I plan ahead

1015
01:41:48,800 --> 01:41:54,240
in engineering? How do I plan ahead in four dimensional political chaff? How do I plan ahead?

1016
01:41:55,120 --> 01:41:59,760
Like, we're really good at it. Just very strange how thoughts come at the most unlikely time. We

1017
01:41:59,760 --> 01:42:04,880
know you talk to people who have been had inventions or brilliant mathematical physicists,

1018
01:42:04,880 --> 01:42:09,680
and they'll just mention these strange occasions. You know, I was having dinner with my wife talking

1019
01:42:09,680 --> 01:42:14,560
about the opera, or I was in the shower. It seems to be a very common feature that people

1020
01:42:14,560 --> 01:42:22,240
think. Yeah, so yeah, as a computer architect, I don't think that's odd at all. So the voice in

1021
01:42:22,240 --> 01:42:29,200
your head is a postdoc narrative. This part of your planning review system. And your actual

1022
01:42:29,200 --> 01:42:36,720
thinking is in many, many, many layers of computation. And not very many of those are visible to you.

1023
01:42:38,720 --> 01:42:44,240
All right, so some of those layers are computing and being rejected and computing and being rejected

1024
01:42:44,320 --> 01:42:48,640
when you don't even know it. I have the sense I can feel myself thinking sometimes and I can't

1025
01:42:49,440 --> 01:42:54,000
access. And then sometimes that kind of breaks through to higher level things where you start

1026
01:42:54,000 --> 01:42:59,120
to label it like I'm having these thoughts and they have labels which are words which I can say,

1027
01:43:00,000 --> 01:43:06,400
or enough of it, go somewhere and then you can kind of go tell the story about your thinking.

1028
01:43:07,600 --> 01:43:12,960
Just the feeling of thinking before you said that sometimes you have the feeling that you're

1029
01:43:12,960 --> 01:43:19,200
thinking that you can't access. What does that feel like? There was a really good book about

1030
01:43:19,200 --> 01:43:25,520
this which they did a whole bunch of experiments which showed that your verbal thoughts are a half

1031
01:43:25,520 --> 01:43:30,240
second behind your actions, which at the time shook me up. And then I thought, well, of course,

1032
01:43:30,240 --> 01:43:41,120
that's how it always feels. And then there's advice I've given people which is if you want to

1033
01:43:41,120 --> 01:43:47,280
solve some hard problem, you have to spend a lot of time and effort putting the problem in your head.

1034
01:43:48,800 --> 01:43:57,280
And I also noticed as a college student, I was relatively bad at learning things quickly and

1035
01:43:57,280 --> 01:44:02,880
getting an A on the test. And I had some friends that did that. Like if I took an engineering

1036
01:44:02,880 --> 01:44:08,880
class, I started studying day one and I did all the problems. And I mostly didn't study for the

1037
01:44:08,880 --> 01:44:14,800
last week of the course before finals because I found it was better for me to stew on what I knew

1038
01:44:14,800 --> 01:44:20,080
than to inject some new thoughts so it would screw it all up. I've always been that kind of person

1039
01:44:20,880 --> 01:44:28,160
and I trust the thinking. But yeah, it's strange when you're thinking hard about something that

1040
01:44:28,160 --> 01:44:33,920
it's kind of boiling around. But again, so my model of it is there's many layers of it.

1041
01:44:34,000 --> 01:44:40,000
Might not be true, I couldn't tell you, I haven't put any probes in. And that much of your thinking

1042
01:44:40,000 --> 01:44:46,160
and computation isn't visible to what you think of your current working set of thoughts.

1043
01:44:47,120 --> 01:44:54,240
And so the fact that occasionally things work out and get signal to some simpler level of thinking,

1044
01:44:54,960 --> 01:44:58,720
and then you're surprised by it isn't surprising to me, I think that's obvious.

1045
01:44:59,680 --> 01:45:04,080
But I had a friend who was very freaked out about the fact that he could never figure out

1046
01:45:04,080 --> 01:45:10,560
where his good ideas came from. He literally dropped out of college for a year because he never

1047
01:45:10,560 --> 01:45:15,600
knew like halfway through a course if he was going to get it. He just felt really anxious

1048
01:45:15,600 --> 01:45:19,920
about it all the time and it's like, well, what did you do about it? Basically nothing. He said,

1049
01:45:19,920 --> 01:45:26,320
I just live with it. And I was like, well, you could spend more time meditating on that. Like,

1050
01:45:26,320 --> 01:45:31,280
I think you can have some more access to it. But you could also say that's part of our architecture.

1051
01:45:32,640 --> 01:45:36,400
I think that he was just freaked out that it would fail him at some point if he didn't understand.

1052
01:45:37,120 --> 01:45:39,920
I don't know where it comes from. So I don't know why it will happen again tomorrow.

1053
01:45:40,960 --> 01:45:46,400
Interesting. Yeah, it's a funny thing, but I'm not surprised about it because, well, like,

1054
01:45:46,400 --> 01:45:50,800
if you go look in neural networks, like, when they first started doing like the cat

1055
01:45:50,800 --> 01:45:54,240
recognizers, they would see as you went through these layers of convolution,

1056
01:45:54,960 --> 01:45:59,600
that, you know, you went from an image to components of an image. And then, you know,

1057
01:45:59,600 --> 01:46:03,920
essentially you would have a layer which would do size and variance and orientation and variance

1058
01:46:03,920 --> 01:46:10,240
and then associations of different things. And sometimes in layers, you could see what it was

1059
01:46:10,240 --> 01:46:14,240
doing. And sometimes in layers, you couldn't. So they would pull the layer out and say, well,

1060
01:46:14,240 --> 01:46:17,600
if I don't know what it is, what's it doing there, they pull it out and it wouldn't work anymore.

1061
01:46:18,160 --> 01:46:26,000
And that's probably because that was some computed association map that had no relevance to what you

1062
01:46:26,000 --> 01:46:35,280
think. Like, and there's also a thing called overfitting, which is kind of funny, where you can

1063
01:46:35,280 --> 01:46:40,960
train a network so good it can only recognize what you trained it with, right? Or there's another

1064
01:46:41,040 --> 01:46:49,520
when you have way more, say, parameters in the network and enough layers, you can

1065
01:46:51,680 --> 01:46:56,400
compute the associations in a way that they're not overfit, so it can recognize essentially

1066
01:46:56,400 --> 01:47:00,640
more than you've trained it with, which is not mathematically right description, but

1067
01:47:01,920 --> 01:47:09,520
like, that's how it feels. And then I think humans have very deep layers, because we do,

1068
01:47:10,080 --> 01:47:14,640
you can see them in your brain, like cortical columns or six layers of neuron stick with a

1069
01:47:14,640 --> 01:47:19,840
spectacular number of connections. And then the cortical columns talk to each other, and then

1070
01:47:19,840 --> 01:47:25,840
they fire each other, and then they run many, many times. And when you're thinking hard for 10 years,

1071
01:47:25,840 --> 01:47:33,600
like, the number of passes through your brain is unbelievable. And your brain is even coordinating

1072
01:47:33,600 --> 01:47:36,880
those passes, it's not your whole brain, you're doing all kinds of different things.

1073
01:47:37,840 --> 01:47:40,800
They send a beginning of this that computers don't think.

1074
01:47:41,680 --> 01:47:47,840
Yeah, so the computers we have today, so the AI we have today is not what I would call thinking.

1075
01:47:48,960 --> 01:47:51,440
But it's always to build one that does.

1076
01:47:51,440 --> 01:47:53,360
But yeah, it's going to happen pretty soon.

1077
01:47:53,360 --> 01:47:55,200
Oh, so I mean, it's like, yeah, they don't think, yeah.

1078
01:47:55,200 --> 01:47:56,720
What's that distinction look like?

1079
01:47:58,400 --> 01:48:02,160
Well, it's, you know, it's like the read, the endlessly redefined scoring test.

1080
01:48:02,720 --> 01:48:03,200
Yeah.

1081
01:48:03,200 --> 01:48:09,360
So Elon's version of his couldn't solve a novel physics problem was the high end of human thinking

1082
01:48:09,360 --> 01:48:14,480
is solving hard novel problems. And I think that's the differentiated from,

1083
01:48:16,320 --> 01:48:20,960
you know, producing lots of word salad. Like, like this is a interesting thing.

1084
01:48:21,520 --> 01:48:27,280
I have a friend who's a writer, and I threatened him with writing all his possible books and

1085
01:48:27,280 --> 01:48:33,520
copyrighting them before he was, he wrote them because he's a very good writer, but he has a style.

1086
01:48:34,800 --> 01:48:40,880
And, you know, and, and if he told me that 20 ideas of the book, we could, we can generate

1087
01:48:40,880 --> 01:48:48,160
much of the writing. Now, obviously, a really good book has a, as a mental journey to it that

1088
01:48:48,720 --> 01:48:54,560
solves some problem, like some, you know, your, like some people are interested in, you know,

1089
01:48:54,640 --> 01:48:59,600
like trip reports and, and, you know, travel dialogues and whatnot, which are predicted,

1090
01:48:59,600 --> 01:49:06,160
right? If you go to Thailand and then degrees and then Iceland, and you look around, you will see

1091
01:49:06,160 --> 01:49:12,000
things and you can literally take a camera and point it at those places and then give that chat

1092
01:49:12,000 --> 01:49:17,920
GPT, which would write you a nice travel book about what happened. Like that's not intelligence.

1093
01:49:17,920 --> 01:49:26,480
That's a scriby. Right. But if you went there and the cultural cons, you know, cultural differences

1094
01:49:26,480 --> 01:49:31,120
between Thailand and Greece, maybe you understand something new about humanity, that could be a

1095
01:49:31,120 --> 01:49:37,040
really interesting book because that's a novel idea. And today chat GPT wouldn't write that book,

1096
01:49:37,040 --> 01:49:44,800
but you might. Right. But at some point that GPT will write that book.

1097
01:49:45,760 --> 01:49:50,800
So with the, the solving the novel problem, wasn't Alpha fold solving a novel problem?

1098
01:49:52,640 --> 01:49:57,840
Right. If they basically figured out a way using AI to solve the protein folding problem,

1099
01:49:57,840 --> 01:50:04,240
which had, yeah, so this is a really good boundary. So that's still a computational problem.

1100
01:50:05,920 --> 01:50:09,440
Now, I don't, maybe human culture is a computational problem.

1101
01:50:10,240 --> 01:50:13,200
Yeah. So there was something.

1102
01:50:15,200 --> 01:50:22,720
So I talked to these guys that just, just cracked me up. So when you do finite element analysis

1103
01:50:22,720 --> 01:50:29,920
on an airplane, imagine have an airplane and you're blowing wind across and then the wind

1104
01:50:29,920 --> 01:50:35,360
is turbulent, of course, the air is turbulent. And then there's, there's both an analysis of

1105
01:50:35,360 --> 01:50:39,520
structure, of temperature, different nations. There's a whole bunch of really interesting

1106
01:50:39,520 --> 01:50:45,360
things. And so it turns out they have reasonable finite element analysis programs written in

1107
01:50:45,360 --> 01:50:53,120
Fortran, which is a little rusty. And, and they're computationally limited to analyze, you know,

1108
01:50:53,120 --> 01:50:57,680
airflow over planes and structural failure, all kinds of stuff. So then they were trained,

1109
01:50:57,680 --> 01:51:03,120
they were using the Fortran programs to train AI models. And now at one level,

1110
01:51:03,120 --> 01:51:08,960
if you want to analyze one element, you know, and it's a billion operations of elements,

1111
01:51:09,920 --> 01:51:17,280
you know, that's a lot. And to run the, that program on a variety of elements to train an AI

1112
01:51:17,280 --> 01:51:24,160
model that turns around and for the AI steps to do anything, it's a, you know, it's a, it's a

1113
01:51:24,160 --> 01:51:30,960
million trillion operations, which is a lot more than a billion. But here's the amazing thing is

1114
01:51:30,960 --> 01:51:38,000
when they train multiple very large AI models with these Fortran programs, the AI models are

1115
01:51:38,000 --> 01:51:41,920
really good at analyzing things and ignoring the things that don't matter and sending more

1116
01:51:41,920 --> 01:51:47,120
computation on things that did. And they analyze the whole airplane, it was less total computational

1117
01:51:47,120 --> 01:51:54,640
steps than the finite element analysis, which couldn't differentiate any element. They all

1118
01:51:54,640 --> 01:52:00,720
look the same. The Fortran program doesn't care that AI model had a deeper level of analysis.

1119
01:52:01,360 --> 01:52:05,280
Which could do something to look at the whole plane. Yeah. Yeah, well,

1120
01:52:06,160 --> 01:52:11,920
yeah, it did something interesting. I don't know exactly what it did. But people are using programs

1121
01:52:11,920 --> 01:52:18,080
to train AI models and then getting better results. Because the AI models wouldn't analyze it

1122
01:52:18,880 --> 01:52:28,000
as sort of this log, rhythmic, or maybe linear computational bed burden for scale,

1123
01:52:28,080 --> 01:52:34,320
whereas the finite elements just have some kind of exponential burden for scale. So the protein

1124
01:52:34,320 --> 01:52:38,800
folding thing is really interesting. It says, we're like really small atoms, they probably have

1125
01:52:38,800 --> 01:52:45,120
programs that can do stuff, but they scale it up and computationally explodes. And the AI models

1126
01:52:45,120 --> 01:52:51,840
probably don't computationally explode. So when the AI start thinking, do we have an ethical

1127
01:52:51,840 --> 01:52:56,240
issue on our hands? Like, are they alive essentially at that point? Do they have self-awareness?

1128
01:52:56,480 --> 01:53:04,000
Is that our real life? Yeah, I think you already have that problem. So this will make you think

1129
01:53:04,000 --> 01:53:10,240
about what thinking is. So AI is clearly making everybody think about what thinking is. Because

1130
01:53:10,240 --> 01:53:14,240
we've had 10 definitions of thinking, they all fell apart. Because as soon as AI did it, everybody

1131
01:53:14,240 --> 01:53:23,840
declared it's not thinking. Because it's obviously not thinking. And now we have ethics built around

1132
01:53:23,840 --> 01:53:31,440
the sanctity of the human life, which has the narrow definition of embodiment in a person

1133
01:53:31,440 --> 01:53:44,000
between two and eight feet tall and 500 pounds. So we have this definitional state in a person.

1134
01:53:44,880 --> 01:53:50,240
And we've mostly given most of those people at various points in time some kind of ethical standing.

1135
01:53:51,200 --> 01:53:59,280
So not what happens when we enable a trillion AIs who are smarter than us. Do they have ethical

1136
01:53:59,280 --> 01:54:06,400
standing? Do we have ethical standing in their mind? I don't know. It makes

1137
01:54:06,400 --> 01:54:12,640
conclude that we're paperclips or something. It's a curious phenomena.

1138
01:54:12,720 --> 01:54:18,080
This leads back to the question that...

1139
01:54:19,360 --> 01:54:23,280
I'm just curious how we can rush towards something like that before we figure out the answer to

1140
01:54:23,280 --> 01:54:33,120
that problem. Are you familiar with Ian Banks, the sci-fi writer? Yeah, that was one of his.

1141
01:54:36,160 --> 01:54:39,680
I think he's a pretty good writer because he imagined the universe where there was

1142
01:54:40,560 --> 01:54:44,400
a wide range from robots that were obviously not very smart to humans,

1143
01:54:45,200 --> 01:54:52,320
including some very smart humans to AIs which were human level intelligence to extremely past

1144
01:54:52,320 --> 01:54:57,200
that. And he created a world where it was sort of all interesting.

1145
01:54:58,160 --> 01:55:06,560
So 99.9% already live in that world.

1146
01:55:08,560 --> 01:55:13,440
You guys are pretty smart. You're already enough smarter than most people that

1147
01:55:14,160 --> 01:55:22,320
you're unintelligible to them when you actually talk seriously. 99% of people already live in

1148
01:55:22,320 --> 01:55:29,120
the world where they're not very smart. It's already a fact. We already live in the world

1149
01:55:29,120 --> 01:55:33,440
where there are cats and dogs and ants and mosquitoes and birds and trees and all kinds

1150
01:55:33,440 --> 01:55:39,920
of things and nobody wakes up in the morning trying to stomp out all the cats. It's not a thing.

1151
01:55:40,720 --> 01:55:46,400
And anything any smarter than us is seriously not resource bound. This thing where the robots

1152
01:55:46,400 --> 01:55:52,240
destroy their Earth because they need energy or something is just whack. The Sun is putting

1153
01:55:52,240 --> 01:55:57,520
not so much energy. It's unbelievable that energy and the material around us is startling.

1154
01:55:58,800 --> 01:56:07,360
Like our local consumption of energy is 0.0000001% of the available energy.

1155
01:56:08,480 --> 01:56:12,640
So this part doesn't necessarily guarantee that your ideas will take off either, right?

1156
01:56:12,640 --> 01:56:16,880
There's this whole marketing charisma element to having your idea. It's to be a kind of a

1157
01:56:16,880 --> 01:56:21,360
beloved person, at least in the realm of scientific ideas that we play in. It's like,

1158
01:56:21,360 --> 01:56:24,240
you can have a great idea, but that's just the start of your battle.

1159
01:56:25,600 --> 01:56:32,960
I actually think that it's... What's the battle? Like humans are very conflict-oriented. So we

1160
01:56:32,960 --> 01:56:38,880
have all these interesting priors. One of our biggest problems is humans for a million years

1161
01:56:38,880 --> 01:56:45,840
of evolution, whatever you believe, are essentially zero-sum game because it was a zero-sum game.

1162
01:56:46,560 --> 01:56:49,760
Like if you eat all the local deer and stuff, there aren't any deer.

1163
01:56:50,800 --> 01:56:54,000
But now we can produce an unlimited amount of food with energy,

1164
01:56:54,000 --> 01:56:59,040
probably, and there's an unlimited amount of energy. So we don't live in a zero-sum world.

1165
01:57:01,200 --> 01:57:07,360
Only by political... Yeah, yeah. There's only a couple kinds of shortages. There's something that's

1166
01:57:07,360 --> 01:57:12,800
in fact rare, and then there's another kind of shortage, which is it's new and they haven't made

1167
01:57:12,800 --> 01:57:17,360
enough for it. They're too expensive for everybody, but that's usually a transient.

1168
01:57:18,240 --> 01:57:24,560
And then there's shortages due to monopolies and there's shortages due to, let's say,

1169
01:57:24,560 --> 01:57:30,800
refreshing of production, right? And there's literally nothing rare. Like we can't run out

1170
01:57:30,800 --> 01:57:37,120
of aluminum because aluminum is a metal. Like you use it and you melt it down. Like there's no

1171
01:57:37,120 --> 01:57:41,840
shortage. I always love these things. They won't have enough copper to make the solar cells.

1172
01:57:43,760 --> 01:57:46,880
Oh, there's so much copper in there. It's very expensive at some point, right? It ends up in

1173
01:57:46,880 --> 01:57:53,840
landfills and you've got to process it. No, there's so much in the top 100 kilometers of the earth.

1174
01:57:53,840 --> 01:57:58,960
It's not funny. Oh, here's a funny story. It's like, so computers are made out of essentially

1175
01:57:58,960 --> 01:58:07,200
three atoms. Oxygen, silicon, aluminum, a little bit of copper. Some gold in there as well, right?

1176
01:58:07,760 --> 01:58:14,320
Yeah, it's really trace. So they use like 32 elements as trace elements, but you know, by bulk,

1177
01:58:15,120 --> 01:58:21,600
it's so it's by bulk, it's silicon and oxygen. So 70% of the earth's thrust down like 100 miles

1178
01:58:22,240 --> 01:58:27,920
is silicon, oxygen and aluminum, which looks like a setup to me, like

1179
01:58:30,240 --> 01:58:36,720
like carbon and hydrogen, like these are all trace elements. There's a lot of carbon and

1180
01:58:36,720 --> 01:58:44,560
hydrogen, but the big hitters are, for whatever reason, is silicon dioxide and aluminum oxides

1181
01:58:44,560 --> 01:58:50,880
and, you know, all kinds of iron. But iron, like the percentage of it goes up as you go further down

1182
01:58:50,880 --> 01:58:56,720
on the earth because of the, you know, melting or something. But there's like so much of everything

1183
01:58:56,720 --> 01:59:03,040
that's not funny. Like they did, there's a couple of processes. Just look up by weight how much like

1184
01:59:03,040 --> 01:59:09,120
copper and magnesium titanium is in like the ocean. Like there's really good processes they're

1185
01:59:09,120 --> 01:59:17,440
developing to like just in sea water. Yeah, just sea water. So there's a lot. I don't miss it. I

1186
01:59:17,440 --> 01:59:22,400
think that the idea that AI are going to show up and they're going to destroy the humans and

1187
01:59:23,120 --> 01:59:31,360
the paperclip maximizer means are, I think that they're deeply informed by dystopian science fiction.

1188
01:59:31,360 --> 01:59:32,880
And my

1189
01:59:34,880 --> 01:59:43,920
greater worry is that they'll will become stuck at some computed optimum.

1190
01:59:46,160 --> 01:59:51,760
And what I mean by that is if you have an, if you have a super intelligence and the super

1191
01:59:51,760 --> 01:59:58,640
intelligence is like, this is what should be done in this circumstance. And it has the ability

1192
01:59:58,640 --> 02:00:04,400
to inform what you do at every circumstance. That's kind of the ultimate centrally planned

1193
02:00:05,040 --> 02:00:11,120
economy in some ways. The most centrally planned culture.

1194
02:00:12,400 --> 02:00:21,040
Well, let's, well, I like Elon Musk's line about very important to train these things about the

1195
02:00:21,040 --> 02:00:26,720
truth. And you know, one of the truths are there's endless cycles and there's endless chaos and

1196
02:00:26,720 --> 02:00:35,200
chaos is very important to development as is randomization and maximizing the alternative frames.

1197
02:00:36,400 --> 02:00:45,920
Like, so I think that's true personally. But, you know, I think, and, and I have a theory that

1198
02:00:45,920 --> 02:00:52,640
like thinking and ideas are essentially infinite. I'm not sure why there's a limit to it. And so

1199
02:00:52,720 --> 02:00:59,040
if it was smart at all, you know, my, you know, my, my guess, my preference maybe or my hope

1200
02:00:59,760 --> 02:01:07,760
is that, you know, it goes in some interesting direction of more, more variation, more possibilities,

1201
02:01:07,760 --> 02:01:17,840
more explorations. And then you said, like, like humans, because of our biological grounding and

1202
02:01:17,840 --> 02:01:22,720
our competitive, like humans are hyper competitive, even when the act like they're not that's just a

1203
02:01:22,720 --> 02:01:28,320
good competitive strategy. Like you, you're both very nice. That's a winning strategy with a lot

1204
02:01:28,320 --> 02:01:32,800
of people who are going to make friends that way. Like, but, but we're hype no matter what we're

1205
02:01:32,800 --> 02:01:40,000
hyper competitive. We have a built in zero sum game, we're very oriented towards the world about

1206
02:01:40,000 --> 02:01:47,600
whether it's like one of the basis of company cycles of is it uncertain that enables

1207
02:01:47,600 --> 02:01:53,760
exploratory behaviors and growing that enables us to participate in growth? Is it steady state?

1208
02:01:54,560 --> 02:01:58,480
And then we start reacting with steady state means we're going to run out, by the way,

1209
02:01:59,200 --> 02:02:03,120
like, then you start exploiting what you have and trying to get your share.

1210
02:02:04,160 --> 02:02:11,520
And then there's inevitable collapse. And then, you know, we reset. And so, so we're very aware of

1211
02:02:11,520 --> 02:02:21,280
those states. And so AI that triggers fears of competition and running out in scarcity is very

1212
02:02:21,280 --> 02:02:27,760
different from technology. So like when technology has been a lot of fun, it's when this is like

1213
02:02:27,760 --> 02:02:33,520
the iPhone created a new possibility, the internet's a new possibility. You know, the thing I'm not

1214
02:02:33,520 --> 02:02:39,120
super happy about AI right now is, you know, only large companies heavily funded,

1215
02:02:40,080 --> 02:02:45,600
you know, big guys are doing it. And they're, they're using the language of establishment and

1216
02:02:45,600 --> 02:02:52,880
scarcity, which triggers people to fear it. As opposed to the language of the PC, the iPhone,

1217
02:02:53,440 --> 02:02:57,200
like some of those technology literally came out of small growing companies.

1218
02:02:57,840 --> 02:03:02,640
And they had the energy of growth, which stimulated the people around them to think,

1219
02:03:03,840 --> 02:03:07,520
by the way, I just, this just occurred to me. So it's not something I was doing on.

1220
02:03:08,320 --> 02:03:10,240
But anyway, it seems kind of obvious that

1221
02:03:11,360 --> 02:03:16,160
like if the environment says, Hey, there's only so much of stuff and we might run out,

1222
02:03:16,160 --> 02:03:20,240
which is the endless drama about resources, we're going to run out of resources.

1223
02:03:20,800 --> 02:03:24,480
You guys somewhere in you believe we're going to run out of copper or something.

1224
02:03:24,480 --> 02:03:28,880
I can prove to you that we won't run out of copper or aluminum or silicon dioxide.

1225
02:03:28,880 --> 02:03:33,120
It's literally what the planets made out of like, we can't run out of materials to make chips.

1226
02:03:33,120 --> 02:03:40,640
Right. It's actually suspicious how much computer technology is embedded in the near part of the

1227
02:03:40,640 --> 02:03:46,720
earth. Like I have a theory that this is the earth is the remnants of the previous supercomputer that

1228
02:03:46,720 --> 02:03:54,400
was, you know, somehow, in some sense for a planet like this, too. I often just think about us being

1229
02:03:54,400 --> 02:03:59,360
of the earth and just some sort of organ that the earth has produced. Yeah, there's just enough

1230
02:03:59,440 --> 02:04:05,280
carbon, carbon to give the book program is biologically, but the computational substrate

1231
02:04:05,280 --> 02:04:12,640
is silicon dioxide. So like, and there's a bunch of reasons like carbon is super good low temperature

1232
02:04:12,640 --> 02:04:19,840
chemistry and carbon chains and all kinds of stuff are silicon saster. It's 300 degrees C,

1233
02:04:19,840 --> 02:04:26,560
you know, get a melted 1000 degrees C. It's it's really sticky. You put an atom on a silicon

1234
02:04:26,800 --> 02:04:32,560
atom. It's stuck there. It's, you know, you can only manipulate the high enough temperatures

1235
02:04:32,560 --> 02:04:40,320
that you really don't want to. Whereas like carbon is fantastic, low temperature, relatively low

1236
02:04:40,320 --> 02:04:47,840
vibrational mode, you know, you know, chemistry is super good. But anyway, that idea about the

1237
02:04:47,840 --> 02:04:55,840
competition and the scarcity. Yeah, so so the the the the advertising guys would like you to think

1238
02:04:56,640 --> 02:05:03,760
there isn't enough to stop so FOMO and it's valuable. Like you may charge more about convincing

1239
02:05:03,760 --> 02:05:09,040
people there aren't very many of them and you have the good one. So the current, you know,

1240
02:05:09,040 --> 02:05:17,520
marketing and the consumer capitalism is all scarcity, you know, demonizing, like just scaring

1241
02:05:17,520 --> 02:05:22,240
the shit out of people. We're going to run out of oil. That's crazy. There's lots of oil. We're

1242
02:05:22,240 --> 02:05:27,840
going to run out of oxygen. We're going to run out of food. We're going to run out of carbon or

1243
02:05:27,840 --> 02:05:34,720
aluminum. Jesus Christ, like, there's 7% aluminum or something. We can't run out of aluminum.

1244
02:05:36,640 --> 02:05:43,200
But that puts you in the mindset of fear and running out. And so the technologies that were

1245
02:05:43,200 --> 02:05:48,560
exciting, when film first came out and airplanes came out, there was 40 airplane companies,

1246
02:05:48,560 --> 02:05:55,520
maybe 100 and cars, there was 100 car companies and PCs used to go there and there'd be 50 guys

1247
02:05:55,520 --> 02:06:04,560
making PCs, right? So today, the big AI is coming out, Nvidia makes the chip and Microsoft

1248
02:06:04,560 --> 02:06:10,320
open AI and Google make the AI models. And it's a there's a scarcity mindset to it and

1249
02:06:11,200 --> 02:06:16,800
unobtainium something to it, which is not good for technology and good for people. We're not

1250
02:06:16,800 --> 02:06:20,560
running out of air. We're not running out of aluminum. We're not running out of compute. We're

1251
02:06:20,560 --> 02:06:29,120
not running out of like we should be in a growth mindset. And humans are very fun to be with when

1252
02:06:29,120 --> 02:06:35,440
you're in a growth mindset. And we're very boring and let's say not great to be with when you're

1253
02:06:35,440 --> 02:06:41,120
in a scarcity mindset. Scarcity mindset means if you eat that apple, I don't get an apple and I'm

1254
02:06:41,120 --> 02:06:46,080
going to fight you for it. Gross mindset of there's so many apples that are falling from the trees

1255
02:06:46,160 --> 02:06:50,800
and we're planning different kinds of trees and you talk to friends and they've invented four

1256
02:06:50,800 --> 02:06:56,560
kinds of apple trees last week and you know, you can't wait to see what the next apple is,

1257
02:06:56,560 --> 02:07:00,000
you know, that that's that would be fantastic world to be in.

1258
02:07:01,600 --> 02:07:07,200
And what freaks people out about AI beyond just the scarcity is something that you mentioned

1259
02:07:07,200 --> 02:07:12,560
earlier, which is that you had product placements back in the day where everything was all the

1260
02:07:12,560 --> 02:07:18,880
Marvel movies had Coca-Cola cans and now you're going to have Marvel movies that have a unique

1261
02:07:18,880 --> 02:07:23,040
product based on all of the data that they've collected off of you off your cell phone or

1262
02:07:23,040 --> 02:07:29,440
whatever else. And that fractures reality just a little bit. And then you have all of these

1263
02:07:29,440 --> 02:07:38,960
subsequent fractures of reality where that shares if a shared reality is a limited commodity all of

1264
02:07:38,960 --> 02:07:44,880
the sudden, that's kind of the fundamental. That's the scariest thing at all, where you can go talk

1265
02:07:44,880 --> 02:07:50,240
to somebody across the street and they no longer live in the same universe as you because they have

1266
02:07:50,240 --> 02:08:00,000
consumed video and media that has nothing to do with what you've seen. And so that scarcity on the

1267
02:08:00,000 --> 02:08:06,800
back of AI is kind of baked into the way that the tools are going to be used because I can imagine

1268
02:08:07,520 --> 02:08:12,560
more AI startups. If the chip isn't $25,000

1269
02:08:14,800 --> 02:08:17,840
and you have people that are interested in building stuff on the back of it, then

1270
02:08:20,080 --> 02:08:26,640
it seems plausible that you could get that kind of ecosystem. But that ecosystem seems to feed

1271
02:08:26,640 --> 02:08:29,760
into a scarcity of shared reality. And that's really interesting.

1272
02:08:29,760 --> 02:08:37,360
Yeah. So, I mean, this cycle has happened, you know, with newspapers, with movies, with television,

1273
02:08:37,360 --> 02:08:42,400
with the internet, where when it's in the growth mindset, lots of people participate,

1274
02:08:43,360 --> 02:08:50,000
we develop a sense of trust with it. Like I remember when television broke thrust

1275
02:08:50,960 --> 02:08:54,640
in the 60s because they were lying about the war and lying about the peace march.

1276
02:08:55,280 --> 02:09:00,800
And then in the 70s, there was lots of cynical TV shows. And so that,

1277
02:09:02,080 --> 02:09:07,120
and I know that got gained as well, but there was kind of an era of cynicism about

1278
02:09:07,120 --> 02:09:12,880
something that had been trusted. Walter Conkret was the news. The New York Times was the news.

1279
02:09:12,880 --> 02:09:19,120
And these are institutions you can trust. And Walter Conkret died. And I don't think anybody

1280
02:09:19,200 --> 02:09:26,960
believes the New York Times anymore. Well, you know, some writers, yeah, so that, but here's the

1281
02:09:26,960 --> 02:09:33,040
thing is that healthy human beings form relationships with things and evaluate them with should they

1282
02:09:33,040 --> 02:09:39,440
trust them, right? And then periodically, like sometimes it's individuals stop trusting this.

1283
02:09:40,400 --> 02:09:46,800
Like, I don't know if I mentioned this, scientific American, like I used to read it a lot, but I

1284
02:09:46,800 --> 02:09:52,080
noticed the computer articles were lousy. But I thought it was just a computer articles. But

1285
02:09:52,080 --> 02:09:56,720
then I talked to a friend who was a mathematician and he said the math articles were lousy. And

1286
02:09:56,720 --> 02:10:00,400
then he called up a friend who was a chemist. He said, no, the chemistry. Oh, well, so shit.

1287
02:10:01,120 --> 02:10:06,480
You know, I thought this was trustworthy because now it's, it continued to be fun because the

1288
02:10:06,480 --> 02:10:12,720
scientific American was essentially entertainment at that point. But it wasn't, it wasn't a trusted

1289
02:10:12,720 --> 02:10:16,720
source as it were. Now, whether they know they're a trusted source, you're not idle.

1290
02:10:16,720 --> 02:10:21,680
Like these are complicated things. And then trusted at what level and to what impact to you.

1291
02:10:23,200 --> 02:10:28,080
So like, like the Marvel movies were a really interesting thing because the original stories

1292
02:10:28,080 --> 02:10:33,760
were really well grounded stories, like they were good archetypal stories. But then as they tried

1293
02:10:33,760 --> 02:10:38,480
to elaborate it and the bean counters took over, they started just making a whole bunch of blah,

1294
02:10:38,560 --> 02:10:44,480
blah, blah, you know, including whatever message of the week they were into. And then nobody

1295
02:10:44,480 --> 02:10:52,000
watches them. They're boring and repetitive. And so there's that kind of thing happens in like

1296
02:10:52,000 --> 02:10:58,080
the same thing happened on the internet. You know, like there was everybody was there,

1297
02:10:58,080 --> 02:11:02,560
there's all these forums and people talking and then they got into better technology,

1298
02:11:02,560 --> 02:11:07,520
but then they got curated and you know, most people don't believe that much about that.

1299
02:11:08,160 --> 02:11:12,720
But it's, but the relentless in this office is interesting. And I think the same thing will

1300
02:11:12,720 --> 02:11:19,760
happen with AI, like, like people get gained by it. But at the same point, people start to go,

1301
02:11:19,760 --> 02:11:25,200
let's say I generated, it's probably, you know, there's marginal utility in it. And then

1302
02:11:26,240 --> 02:11:31,840
there's been many attempts to have curated news and internet to media, like the internet you

1303
02:11:31,840 --> 02:11:35,440
can trust, but it's really hard because soon as they make any money, they got bought by somebody

1304
02:11:35,520 --> 02:11:43,440
doesn't feel that way. And so it's, it's pretty complicated, but most humans grow up in this world.

1305
02:11:45,120 --> 02:11:48,960
And then humans, humans sort their lives out depending on what they need to do.

1306
02:11:50,480 --> 02:11:54,480
You know, people who are trying to do geopolitical operations need to have

1307
02:11:55,360 --> 02:12:01,760
a way difference filter for, you know, let's say, you know, shenanigans and somebody who's running

1308
02:12:01,760 --> 02:12:07,200
a, you know, simple store and technologists is the same way. So

1309
02:12:10,960 --> 02:12:15,680
I think that there's something in there about an eternal

1310
02:12:16,960 --> 02:12:24,080
untrustworthiness that's been with us for a long time. Because if you go back to a model where

1311
02:12:24,080 --> 02:12:29,520
we're all just 100 person tribes, it's not like the tribes are living in harmony. There's probably

1312
02:12:29,520 --> 02:12:34,960
warfare. There's conflict. And the leader of your tribe might be telling you things about the other

1313
02:12:34,960 --> 02:12:41,280
tribe that aren't true, but are functional and necessary in order for you to be willing to go

1314
02:12:41,280 --> 02:12:47,440
on a raid or that. And so I think that we've, sometimes I think about this in the sense that

1315
02:12:47,440 --> 02:12:53,360
we've loaned ourselves into a false belief that there is something outside of our direct experience

1316
02:12:53,360 --> 02:12:59,200
with people that can be trusted, because we want so badly for that to be the case. Because if you

1317
02:12:59,200 --> 02:13:02,640
think about your own personal relationships, the people that you have relationships with

1318
02:13:02,640 --> 02:13:08,240
that are closest to the ones that you can trust them, right? If you have somebody that you look

1319
02:13:08,240 --> 02:13:11,840
at and you're like, well, I can't trust you, you're probably not going to be very good friends with

1320
02:13:11,840 --> 02:13:17,360
that person. And so we want to be able to generalize that outwards to systems. But then as you're

1321
02:13:17,360 --> 02:13:22,400
speaking, I'm sort of thinking maybe that's a false, maybe that's been an eternal false hope

1322
02:13:22,400 --> 02:13:31,200
where even Walter Cronkite, like he, he was the news, but the CIA had tons of people at all

1323
02:13:31,200 --> 02:13:35,280
these news organizations. Yeah, yeah, he was just reading the paper. So here's another way to think

1324
02:13:35,280 --> 02:13:42,640
about it is, you know, when, when you start to go into a scarcity system, the, the, you know,

1325
02:13:42,640 --> 02:13:48,080
everybody you deal with, you know, has some cost benefit with, you know, if they lie to you and

1326
02:13:48,080 --> 02:13:56,080
you catch them, that's a cost to them. But, and if the benefit, the line is really low,

1327
02:13:56,080 --> 02:14:01,280
they're probably not going to. So in a, again, in this frame of in a growth mindset,

1328
02:14:01,920 --> 02:14:06,240
where there's lots to do, the benefit of lying is relatively low because you can,

1329
02:14:06,240 --> 02:14:12,880
you can get what you need. But if you're convinced that, you know, things are tough,

1330
02:14:12,880 --> 02:14:17,520
the company has layoffs coming, you know, a company that's growing 10% a year,

1331
02:14:17,520 --> 02:14:21,600
it's pretty healthy. People generally help each other to get the job done.

1332
02:14:22,320 --> 02:14:28,480
There's an interesting book called stretch, which says if, if you have about 10% more to do than

1333
02:14:28,480 --> 02:14:35,440
you can, you stretch and if somebody offers to help you, you accept it, you appreciate it, right?

1334
02:14:35,440 --> 02:14:39,280
If it's 50% more, you get burnout because you can't achieve your goals.

1335
02:14:40,400 --> 02:14:45,680
If you have 10% less than you need to do and somebody offers to help you say, stay off my tariff.

1336
02:14:46,640 --> 02:14:50,720
Because if they do your work and there's a cut coming, the boss is going to go,

1337
02:14:51,360 --> 02:14:57,360
well, Jim only ever does 80% and, you know, Bob over here picked up the slack. And so

1338
02:14:58,160 --> 02:15:04,400
the stretch of the organization sets the trust level. It's almost sad how simple it is,

1339
02:15:04,400 --> 02:15:10,000
but it's actually true. Once you weed out them, the various characters, if your organization

1340
02:15:10,080 --> 02:15:16,160
stretch, they are incented to cooperate. And if, and if there's not enough to do and there's cuts

1341
02:15:16,160 --> 02:15:23,600
coming, they're incented to undermine each other. It's the same thing with product placement and

1342
02:15:23,600 --> 02:15:29,360
stuff. Like, you know, Coke is a weird thing because it's like a zero value product because,

1343
02:15:29,360 --> 02:15:35,280
you know, zero to make. It's entirely, you know, manufactured and sold on brand.

1344
02:15:35,920 --> 02:15:40,160
Nobody really cares what it tastes like. I don't think so. I sure don't.

1345
02:15:42,080 --> 02:15:47,920
But the brand has established something and humans have, you know, habits and stuff. So they

1346
02:15:47,920 --> 02:15:53,360
have a positive association with the brand, the image, the consumption, there's a flywheel of

1347
02:15:53,360 --> 02:16:05,360
that kind of thing. So I think, yeah, he's generically afraid of AI. Like, it's not helpful

1348
02:16:05,360 --> 02:16:10,480
because he can't solve any problem by being worried about it. Having some understanding of how

1349
02:16:10,480 --> 02:16:17,440
technology impacts society, how we use it to change our culture, what stage we're in is really

1350
02:16:17,440 --> 02:16:23,760
interesting. And then what's going on with the players? Like, because I think that's,

1351
02:16:25,280 --> 02:16:33,040
that's helpful. And then there's the, you know, the odds that humans with this technology aren't

1352
02:16:33,040 --> 02:16:39,600
going to develop intelligence is zero. Right? Like, we're either going to keep going up or,

1353
02:16:39,600 --> 02:16:44,560
you know, like, like, we're very dynamic, you know, we accumulate knowledge like mad,

1354
02:16:44,560 --> 02:16:52,640
there's eight billion of us. We're not super good with, you know, low stress environments,

1355
02:16:52,640 --> 02:16:58,560
humans do really well with some stress and some restrictions and stretch, I think,

1356
02:16:59,520 --> 02:17:04,800
the companies, you know, with real missions, you know, do real well because people work together

1357
02:17:04,800 --> 02:17:17,280
to solve the mission. And, you know, it's a very curious time because we're developing something

1358
02:17:17,280 --> 02:17:23,600
very new in a time when many, many people were think we're in a zero thumb game and possibly

1359
02:17:24,560 --> 02:17:28,800
going down, but, and the advertising to that effect is relentless.

1360
02:17:29,600 --> 02:17:35,840
You know, we're running out of like literally everything. Now, there are certain things that

1361
02:17:35,840 --> 02:17:40,160
are getting worse, like our ability to build roads, that's bureaucratically captured this poor

1362
02:17:40,800 --> 02:17:44,640
following the building to build airplanes, they laid off all the engineers apparently,

1363
02:17:44,640 --> 02:17:49,120
and it's run by, you know, the accountants like those things actually have.

1364
02:17:50,240 --> 02:17:55,680
So airplanes got unbelievably reliable. And then people took it for granted and then,

1365
02:17:56,640 --> 02:18:02,800
you know, but that just means it had something to do with their growth, too, because there's

1366
02:18:02,800 --> 02:18:08,880
a merger with the fellow Douglas and they laid off all these QA people. And yeah, yeah, it's

1367
02:18:08,880 --> 02:18:14,800
quite the shenanigans. They financial engineered it. Yeah, it's, you know, that's the kind of thing

1368
02:18:14,800 --> 02:18:20,000
it was like, if I had a magic wand, I wouldn't allow mergers of large companies because it'd be

1369
02:18:20,000 --> 02:18:25,200
better for them to grow or fail by themselves and create more small companies because I think more

1370
02:18:25,200 --> 02:18:31,200
small things is better than small numbers of large things. But I don't have that magic wand and

1371
02:18:31,200 --> 02:18:35,200
big companies lobby heavily a lot of those mergers because that's how they make money.

1372
02:18:36,320 --> 02:18:40,080
Like a lot of big companies essentially never have any new ideas. They just keep buying small

1373
02:18:40,080 --> 02:18:49,520
companies with ideas. You know, like Google, for example, has bought a large number of companies,

1374
02:18:49,520 --> 02:18:55,040
but there are only two money making assets are basically search and YouTube. And they

1375
02:18:55,040 --> 02:19:04,240
bought YouTube. Facebook famously did better. So they bought WhatsApp and Snapchat. I always

1376
02:19:04,240 --> 02:19:10,000
forget the list of them, but they bought a relative. They bought companies that actually had real

1377
02:19:10,000 --> 02:19:14,720
growth and value and continued to grow them. But it's not clear the world would have been better

1378
02:19:14,720 --> 02:19:22,080
off of those companies have been separate companies. And it does seem to be this tendency

1379
02:19:22,160 --> 02:19:28,400
towards agglomeration. Like there is, there doesn't appear to be an upper level of size,

1380
02:19:28,400 --> 02:19:36,400
like there is for something like a cell, right? Well, yeah, yeah, that's pretty funny. Well,

1381
02:19:38,560 --> 02:19:46,240
yeah, the cell, like in a company, essentially the cell is a small team of 10 to 20 people.

1382
02:19:46,320 --> 02:19:55,280
So all big companies like are made of cells, right? And so, yeah, but one company can acquire

1383
02:19:55,280 --> 02:20:00,480
another whole animal and then merge the cells together. And, you know, they'll do it for

1384
02:20:00,480 --> 02:20:05,440
some reason about synergy or efficiency or something. But it may in fact, because they

1385
02:20:05,440 --> 02:20:09,600
don't have any new products. So they have a lot of money, but they don't have any new products.

1386
02:20:09,600 --> 02:20:14,880
So some companies like, like big pharmaceutical companies, they invest piles of money in R&D,

1387
02:20:14,880 --> 02:20:18,800
but almost all their new products come from acquisitions. So

1388
02:20:20,320 --> 02:20:28,080
I wonder if there's the same sort of cycle like there is with the megalodons, right? So there's

1389
02:20:28,080 --> 02:20:33,680
megafauna. There's a period on earth where megafauna dominate, and then the conditions change,

1390
02:20:33,680 --> 02:20:38,160
and then all of a sudden, they're no longer the dominant biological paradigm. Do you think that

1391
02:20:38,160 --> 02:20:43,840
the same thing happens inside of corporate structures where there's just an era of huge

1392
02:20:43,840 --> 02:20:48,240
companies, and then the winds shift, and then it goes back towards smaller companies?

1393
02:20:49,360 --> 02:20:53,360
No, I think things just tend to get bigger. So when I was a kid, I was taught that

1394
02:20:53,360 --> 02:20:56,640
animals are really big either because it's really hot or because it's really cold.

1395
02:20:57,440 --> 02:21:03,120
You know, mammoths are big at the poles because it's cold, and dinosaurs are big because it was

1396
02:21:03,120 --> 02:21:08,800
hot. And I thought, actually, they probably just get bigger. And then something happens,

1397
02:21:08,800 --> 02:21:13,840
like a big rock falls on the earth and resets it, and then it kills all the big animals because

1398
02:21:13,840 --> 02:21:19,280
the infrastructure for big animals is really complicated. So my guess is companies just get

1399
02:21:19,280 --> 02:21:26,560
bigger, and then they occasionally fail hard. You know, some companies have actually failed

1400
02:21:27,680 --> 02:21:31,920
and gone bankrupt after they've been big. And some companies, especially with the

1401
02:21:32,640 --> 02:21:38,320
way government bureaucracies and companies work, they can continue to acquire companies.

1402
02:21:40,960 --> 02:21:46,480
But there will be some financial reset. Like during the PC boom, all the companies involved were

1403
02:21:46,480 --> 02:21:54,000
small. Like IBM messed around with Windows and PCs, but they gave the operating system to Microsoft,

1404
02:21:54,000 --> 02:21:57,600
and they had their own chip, but they decided to buy the chip from Intel. Both of those companies

1405
02:21:57,600 --> 02:22:04,480
were small. And then Dell and Gateway, all these companies were all small. And they were all actually

1406
02:22:04,480 --> 02:22:11,120
too small for the big guys to even care about. But then they grew so fast, you know, they avoided

1407
02:22:11,120 --> 02:22:19,440
getting acquired. Like that's an interesting phenomena that they grew. Whereas once, you know,

1408
02:22:19,440 --> 02:22:25,760
Google and Facebook and some other companies got big, they had grown so fast that they grew fast

1409
02:22:25,840 --> 02:22:31,600
enough not to be acquired. And Zuckerberg famously turned down a billion dollars for his toy startup.

1410
02:22:32,480 --> 02:22:37,120
But but then they turned around and they bought a lot of the small companies.

1411
02:22:37,120 --> 02:22:41,120
Well, we can business model at some point, I knew lots of people that were starting startups

1412
02:22:41,120 --> 02:22:45,760
exclusively with the goal of getting a profit Google. Yeah, like the network thing happened

1413
02:22:45,760 --> 02:22:50,640
the same way. There was like hundreds of small networking companies and the big ones got a

1414
02:22:50,640 --> 02:22:55,760
certain point. And then Cisco's business model was literally acquiring successful startups and

1415
02:22:56,320 --> 02:23:01,040
ramping their revenue. And sometimes that was real, like a small company couldn't build a

1416
02:23:01,040 --> 02:23:05,680
billion dollars worth of network switches. But Cisco could, they had the supply chain and

1417
02:23:05,680 --> 02:23:10,880
manufacturing and operations to do it. So sometimes the action creates value like Cisco,

1418
02:23:11,840 --> 02:23:14,320
like I think often did, but sometimes they just,

1419
02:23:15,040 --> 02:23:20,640
yeah, it's, it's hard to say. It makes somebody use some money. Hardware might be different than

1420
02:23:20,640 --> 02:23:28,320
software. Well, so there was a long time when hardware wasn't like that. There was in fact,

1421
02:23:28,320 --> 02:23:33,920
you know, 50 or 100 airplane companies and miles of motor companies. And then the big

1422
02:23:33,920 --> 02:23:39,360
companies in the US, they outsourced. There was a period of time when Ford made everything and

1423
02:23:39,360 --> 02:23:45,840
they slowly outsourced the seats and the transmissions and the tires and the shock absorbers

1424
02:23:45,840 --> 02:23:51,760
and all kinds of stuff. And so there's a, there's a pendulum swing they call vertical to horizontal

1425
02:23:51,760 --> 02:23:57,200
integration. Like you make everything in the stack or do you make your piece like PC world was

1426
02:23:57,200 --> 02:24:02,000
what's called horizontal structure. So Microsoft made the operating system until made the chip,

1427
02:24:02,000 --> 02:24:06,640
Dell made the computer, somebody else distributed the thing or somebody else made the network.

1428
02:24:06,880 --> 02:24:11,680
But now Apple is a vertically integrated company. They make every single thing.

1429
02:24:13,760 --> 02:24:18,320
So you said that one of the things that you would do if you could waive a wand was to prevent

1430
02:24:18,320 --> 02:24:25,680
large companies from buying little ones. What are the other things that you would do?

1431
02:24:27,120 --> 02:24:29,840
I'll govern the arc receipts that have time limits.

1432
02:24:30,240 --> 02:24:36,720
You know, obviously, you know, our represent representative should have time limits or

1433
02:24:36,720 --> 02:24:43,440
term limits. Like, imagine you wanted the most independent experiments happening

1434
02:24:44,160 --> 02:24:48,320
so that none of them that they failed would take out lots of people.

1435
02:24:50,880 --> 02:24:54,800
Like we have a, you know, the, you know, the, the, the NIMBY problem, right? So,

1436
02:24:55,760 --> 02:25:00,240
so people want to build new houses and they can't come to regulation. So they go somewhere and

1437
02:25:00,240 --> 02:25:06,160
start a town or some town allocates a whole bunch of land, which great. So they can build houses

1438
02:25:06,160 --> 02:25:10,880
and new roads and the new school, the new shopping center. And as everybody moves in,

1439
02:25:10,880 --> 02:25:15,120
they all join the town council and they pass regulations so they can't build any more houses.

1440
02:25:15,120 --> 02:25:22,880
Right. So like this, and again, this is, this is, this is biological. Like,

1441
02:25:23,600 --> 02:25:29,200
like we operate from zero sum games and limits to growth and in competitive nature.

1442
02:25:30,640 --> 02:25:38,320
So if you want to architect a world that creates, keeps creating possibility. And by the way,

1443
02:25:38,320 --> 02:25:43,040
I don't think it's bad. So, you know, you bought your house when you're 30 years old and, you know,

1444
02:25:43,040 --> 02:25:46,880
you raise your family or retire there and you don't want to build a new shopping center and

1445
02:25:46,880 --> 02:25:52,960
screw up the neighborhood. Like, sure, go ahead and do it. But then you can't complain that there's

1446
02:25:52,960 --> 02:25:58,960
no housing in the area. Like, like the United States is literally three, you know, three,

1447
02:25:58,960 --> 02:26:05,120
three million square miles of land. Most of it's empty. You know, there's like a good

1448
02:26:05,120 --> 02:26:09,200
support population of the trillion probably. Most of it's really bad. And I don't know if you've

1449
02:26:09,200 --> 02:26:13,360
spent a lot of time in the inner mountain west, but that's a harsh, harsh landscape.

1450
02:26:13,360 --> 02:26:20,560
Yeah, it's an energy problem. But the, my premise is more small independent things are better.

1451
02:26:22,240 --> 02:26:30,400
Like, and one reason we met was you had some scientists on that, you know, I quite like

1452
02:26:30,960 --> 02:26:34,800
and some of whom are literally actively suppressed by the scientific community.

1453
02:26:35,760 --> 02:26:42,880
And again, I don't know if they're right around at some level. Like, I'm interested in it. And I was,

1454
02:26:43,520 --> 02:26:48,400
you know, surprised. You know, borderline shocked, you know, when I first discovered that there were

1455
02:26:48,400 --> 02:26:53,120
scientists who couldn't get their papers published, not because they seemed obviously stupid, but

1456
02:26:53,120 --> 02:26:59,600
because they didn't go to the narrative. And, you know, at some level, you could think,

1457
02:26:59,840 --> 02:27:08,080
well, I know governments could craft and big companies get bureaucratic and stodgy. But at

1458
02:27:08,080 --> 02:27:14,240
least, you know, the scientists, you know, Einstein, Shirley Einstein is a good guy. And,

1459
02:27:15,520 --> 02:27:20,560
and, you know, the people who studied his work, and then you read these articles, it was Peter

1460
02:27:20,560 --> 02:27:25,280
Voigt, Peter Voigt, he wrote the book, not even wrong. That's what a few hours in his office one

1461
02:27:25,280 --> 02:27:29,280
day. I actually he was the reason I even went down this path in the first place. When I was a

1462
02:27:29,280 --> 02:27:33,840
little kid, he had one of the first blogs that was critical string theory. I was so excited to

1463
02:27:33,840 --> 02:27:38,880
meet him and everything. He just turns out to have a lot of crazy mathematical ideas too. So,

1464
02:27:40,320 --> 02:27:44,720
yeah, but he also is a very strong believer in some parts of particle physics, so other people

1465
02:27:44,720 --> 02:27:50,560
are wrong. And he's very dismissive of them. And to be in hospitals, or her book, Lost in the

1466
02:27:50,560 --> 02:27:58,400
Maths is great. But then she's very much a true believer in other things. So, you know,

1467
02:27:59,120 --> 02:28:04,960
but you run into this interesting phenomena that even people who, who seem outside the

1468
02:28:04,960 --> 02:28:11,600
canon in some places could still be true believers in other places. And I have a theory,

1469
02:28:11,600 --> 02:28:17,680
I have a theory that you can only have one out of the canon idea at a time. Like,

1470
02:28:18,240 --> 02:28:24,640
if you have an idea that you're championing, unless you're like Thomas Gold or something.

1471
02:28:25,200 --> 02:28:29,760
You mean you're only allowed to have one idea out of the canon? That's a strategy or you think

1472
02:28:29,760 --> 02:28:35,120
that's a human nature problem? I think that's a strategy. I think that people are basically like,

1473
02:28:35,120 --> 02:28:41,200
look, I know this is crazy. Yeah, yeah. I'm not crazy because I'm buying to all of these. Yeah,

1474
02:28:41,760 --> 02:28:50,720
I'll tell you, I believe it. And I kind of get that. But I don't know,

1475
02:28:51,920 --> 02:28:56,640
for some people, they probably have three crazes. And there's probably humans have a limit.

1476
02:28:57,440 --> 02:29:05,040
Our computational substrate has limits on how many non-conformist ideas we can have at a time.

1477
02:29:05,120 --> 02:29:12,880
And that's another kind of problem. It's really painful, right? Like, if you don't have any common

1478
02:29:12,880 --> 02:29:18,880
ground to stand on with other people, it gets really painful, I think. You have to surrender

1479
02:29:18,880 --> 02:29:24,720
to some common knowledge at some point. Yeah, and it's in this surprise. I told you guys, I

1480
02:29:26,000 --> 02:29:29,440
found this list of, you know, crazy things about the sun, which I thought was fun,

1481
02:29:30,080 --> 02:29:37,520
but I talked to a, you know, a sun scientist and he was quite mad about it. And I was a little

1482
02:29:37,520 --> 02:29:42,080
taken aback because I was like, man, this is amazing. Like, you know, like, there it is right

1483
02:29:42,080 --> 02:29:50,560
there. It's so hard and complicated. And so the polarity of ideas is, I think, really important.

1484
02:29:50,720 --> 02:29:59,840
And then, so the recognition of tendencies, like I told you, there's this, you know, chaos versus

1485
02:29:59,840 --> 02:30:08,640
order productivity graph. Everybody can tell you where you should be, but nobody knows how to stay

1486
02:30:08,640 --> 02:30:17,120
there. It's like, when they built the interstate highway system, they invented the road system,

1487
02:30:17,120 --> 02:30:23,360
the equipment to build the companies were created. Like everywhere they went, they had to build new

1488
02:30:23,360 --> 02:30:28,960
companies to build these new roads. And it was sort of like they were so excited about building new

1489
02:30:28,960 --> 02:30:34,240
roads, they didn't have time to think like we could literally steal half this money. And nowadays,

1490
02:30:34,240 --> 02:30:39,840
it costs literally 10 times as much money per mile to build a road. They're better engineered,

1491
02:30:39,840 --> 02:30:46,160
quote unquote, and better planned. And, you know, but the money disappears in bureaucracy. And so

1492
02:30:47,840 --> 02:30:52,400
you know, like the best thing would be a way to like really reinvent all kinds of things.

1493
02:30:53,280 --> 02:30:58,560
So like young people, everyone's like, people say stuff like, oh, you know, there's so much debt

1494
02:30:58,560 --> 02:31:02,560
and you screwed all this up. It's like, yeah, but we built all that. Why don't you build your own

1495
02:31:02,560 --> 02:31:09,600
stuff? Like, like the current cartists, companies are defunct. You know, and then Elon builds a

1496
02:31:09,600 --> 02:31:13,680
new car company, you can't build a car company, you definitely can't build a rocket ship company,

1497
02:31:13,680 --> 02:31:18,640
and you definitely can't build a New Orleans company. He's probably got a list of 100 things,

1498
02:31:18,640 --> 02:31:27,440
those things you can't build. Like, build new stuff. I'm involved with Atomic Semi to help

1499
02:31:27,440 --> 02:31:31,840
pound the company and we're building a semiconductor fab that's really small. And

1500
02:31:32,560 --> 02:31:37,120
like you explain it to people and they'll say, well, half that might work and half like it'll

1501
02:31:37,120 --> 02:31:41,680
never work. Obviously semiconductor companies cost $10 billion. Where are you going to get

1502
02:31:41,680 --> 02:31:47,440
$10 billion? It was like, well, I was going to build it for $10,000, like not $10 billion.

1503
02:31:49,120 --> 02:31:55,600
And we've developed so much science as that. So humans are,

1504
02:31:58,480 --> 02:32:04,000
Jordan Peterson told me this funny line is like, like, there's a high incentive to get used to

1505
02:32:04,000 --> 02:32:10,560
things. So the first time you drive somewhere, it takes a while, right? But if you drive there

1506
02:32:10,640 --> 02:32:14,400
10 times, you drive back and forth every day, you don't even notice the drive anymore.

1507
02:32:14,960 --> 02:32:20,880
And I've noticed even on like a hike, when you walk, like if you walk a hike and you walk there

1508
02:32:20,880 --> 02:32:27,360
and then you walk back, the walk there takes twice as long as the walk back. Because you've

1509
02:32:27,360 --> 02:32:35,840
already, you know, because it's your perception of time as novelty and bias, which is really curious.

1510
02:32:36,800 --> 02:32:42,080
But there's a couple of exceptions to this. Like you don't get used to your kids. Well,

1511
02:32:42,080 --> 02:32:50,960
if you do, they'll die. So, or it could be your perception of your children is highly influenced

1512
02:32:50,960 --> 02:32:57,120
by how important they are to you. So even small deltas are novel. Like there might be some,

1513
02:32:57,760 --> 02:33:03,120
like the nice way of saying this, if you love your children, they're always new.

1514
02:33:04,080 --> 02:33:10,880
But the other way that the economy argument of it is, we're not descended for people who didn't

1515
02:33:10,880 --> 02:33:15,280
pay attention to their children. So we pay attention to them. And we notice pretty small

1516
02:33:15,280 --> 02:33:20,000
differences where as you, you walk down the street, you don't really care very much whether your

1517
02:33:20,560 --> 02:33:26,240
neighbor mowed their grass or moved a car around this year, an idiot, right? So do we have this

1518
02:33:26,240 --> 02:33:31,280
kind of funny? So, but the problem with that is we get used to everything. We're used to the science

1519
02:33:31,360 --> 02:33:38,000
we have. We're used to how they make cars, you know, we're used to the things like when I first

1520
02:33:38,000 --> 02:33:44,560
went to the Tesla factory, like it was all new to me, right? And it would really warm me out. I

1521
02:33:44,560 --> 02:33:50,080
spend hours there, I go home, like, you know, flashing lights and moving equipment. And after

1522
02:33:50,080 --> 02:33:55,840
a year, I've been there so many times, like, I would, I would be walking to a meeting and

1523
02:33:55,840 --> 02:34:01,280
annoyed that I couldn't park my car. And I was five minutes late. So I was walking fast through

1524
02:34:01,280 --> 02:34:09,040
this factory of wonders. I was walking to a place that literally made me sleep an extra hour a day

1525
02:34:09,040 --> 02:34:15,280
for three months, because it was so novel and new. Right. And now I was walking by it all annoyed

1526
02:34:15,280 --> 02:34:22,240
that I was late. Like, and this happens to everything scientists, physicists, you know,

1527
02:34:22,240 --> 02:34:27,360
they're so used to string theory, everything is bullshit compared to that. And we're so good

1528
02:34:27,360 --> 02:34:34,800
at something like, I really love the line, the unreasonable effectiveness of mathematics.

1529
02:34:36,640 --> 02:34:39,440
Right. But there's a counterline to that, which is something like,

1530
02:34:40,400 --> 02:34:45,200
like physics isn't mathematical, because we know so much, it's mathematical, because we know so

1531
02:34:45,200 --> 02:34:58,080
little. Like, it's so wild. So, yes. So the people had a better understanding of, you know,

1532
02:34:58,080 --> 02:35:03,760
why should there be limits to size? Why should there be limits to duration? Why is it hard to

1533
02:35:03,760 --> 02:35:10,800
stay on the high point of the productivity curve? Why do we get so used to things that we ignore

1534
02:35:10,800 --> 02:35:16,560
them even when they're really important? Why do we accept that you can't build a better X, Y, or Z?

1535
02:35:18,400 --> 02:35:24,480
Why are you condensed by advertisers who have only their own interests in mind that we're running

1536
02:35:24,480 --> 02:35:30,400
out of everything, create scarcity, you know, like, like, these are hard things to get out.

1537
02:35:30,400 --> 02:35:36,560
These are mental traps. There's a lot of them. And some of them are like, we earned them. Like,

1538
02:35:37,280 --> 02:35:43,280
being a zero-sum person as a person member of a tribe after a million years of evolution,

1539
02:35:44,000 --> 02:35:51,440
that pretty smart strategy. Right. Now, being oriented towards spring, summer, or fall,

1540
02:35:52,400 --> 02:35:56,640
you know, that's also pretty smart, too. You know, like in the winter, you know,

1541
02:35:56,640 --> 02:36:03,520
you fight over the last apple in the summer, you know, that's great. So, but knowing these things

1542
02:36:03,520 --> 02:36:08,720
is important because we build politics around them. We build structures around people, build their

1543
02:36:08,720 --> 02:36:13,040
lives around them. I know people are going to spend their entire lives worried, you know,

1544
02:36:13,040 --> 02:36:18,720
it was peak oil and then peak this and peak that, you know, and none of it happened. Like, literally,

1545
02:36:18,720 --> 02:36:24,800
none of it happened. I think that that's so vital and it's really interesting because it

1546
02:36:24,800 --> 02:36:29,760
keeps coming up. It translates into your personal life, too, right, in terms of where you put your

1547
02:36:29,760 --> 02:36:36,560
attention. I had this mind-bending experience as an artist, as a musical artist, where I heard

1548
02:36:36,560 --> 02:36:42,960
something that Nigel Godrich, he's a producer, he worked with Radiohead for many years. He said

1549
02:36:42,960 --> 02:36:46,480
something like, in an interview one time, this blew my mind, where he was like, you need to stop

1550
02:36:46,480 --> 02:36:50,880
trying to make it sound good and start trying to make it sound interesting. And I was like, wow,

1551
02:36:50,880 --> 02:36:55,920
that's really profound because that's exactly as a musician who's been playing an instrument for 40

1552
02:36:55,920 --> 02:37:02,000
years or something. You're so obsessed with optimizing your technical ability and it's like,

1553
02:37:02,000 --> 02:37:07,520
nobody cares. Like, literally, everyone only hears technically perfect music all day, every day,

1554
02:37:07,520 --> 02:37:11,840
but it doesn't stand out because it's technically perfect. It actually blends into the background

1555
02:37:11,840 --> 02:37:17,600
and these artists just kind of decay into fade away into nothing. And so there's just something

1556
02:37:17,600 --> 02:37:23,440
really, really interesting there about following what's surprising and what actually grabs you as

1557
02:37:23,440 --> 02:37:27,840
opposed to what's perfect or what's, you know, the best version of what came before.

1558
02:37:29,360 --> 02:37:37,040
Yeah, I saw Crosby Stills and Nash, I guess, play and they played their top hits just exactly

1559
02:37:37,040 --> 02:37:43,120
like they played them 30 years before, which I thought was terrible. And then I, because they

1560
02:37:43,120 --> 02:37:49,440
were such a wild band, like they did like five albums in two years that were all knockouts and

1561
02:37:49,440 --> 02:37:57,280
then they, and then Jeff Beck did this live album. That was so much better and more interesting than

1562
02:37:57,280 --> 02:38:04,320
his early stuff. Like he never stopped experimenting with a guitar, he could make it sing and dance.

1563
02:38:04,320 --> 02:38:12,720
Like, like some of it you can clearly hear in his earlier stuff, but that was just amazing. I was

1564
02:38:12,720 --> 02:38:19,760
like the Jerry Garcia line and we asked him what his limitations were. He said, everything I learned,

1565
02:38:19,760 --> 02:38:27,040
everything I know, everything I've played, like that, like that's an amazing thing.

1566
02:38:28,640 --> 02:38:33,280
And he was like, I really like his guitar playing too. But he also had a style and then he killed

1567
02:38:33,280 --> 02:38:39,360
himself with Harrow and Jesus. I don't know.

1568
02:38:39,360 --> 02:38:44,240
Well, I wonder if like this, the drug side of things too, it's just some reaction to that

1569
02:38:44,240 --> 02:38:48,000
inability, that trying to get outside, to get out of your own way kind of thing.

1570
02:38:50,000 --> 02:38:55,440
Yeah, it could be, some of what made him great was also what he was playing with, was with

1571
02:38:55,520 --> 02:39:01,200
Lewis and Jennings and Harrow and God knows what. That's a tough road to hoe though.

1572
02:39:02,160 --> 02:39:06,880
I think technically it was diabetes. They killed him, but he apparently would live on ice cream for

1573
02:39:06,880 --> 02:39:13,680
a month at a time. Okay. Jerry Garcia. Yeah. Yeah, he sued him over that too. That was pretty

1574
02:39:13,680 --> 02:39:19,040
funny. He named it after him. He literally killed him. That's right, because there's a big Ben and

1575
02:39:19,120 --> 02:39:24,880
Jerry is at the corner of Hayden-Nashbury right now. Oh, you can't even make it up.

1576
02:39:25,920 --> 02:39:31,200
But you know, that kind of combinational complexity and insanity is just a beautiful thing.

1577
02:39:32,640 --> 02:39:39,360
Like you couldn't, that's as weird and deep as you could possibly get at some level.

1578
02:39:40,720 --> 02:39:47,200
I mean, it's been really, really fun talking to you. You know so much about the arc of this

1579
02:39:47,200 --> 02:39:53,200
technology that's shaping everything about the world right now, and you're working so deeply on

1580
02:39:53,200 --> 02:40:02,960
it. And you have a really subdued optimism about it, which I really like. Subdued optimism. My nephew

1581
02:40:02,960 --> 02:40:08,400
said I'm a cynical optimist. I haven't looked at too much cynicism up here. I think that you

1582
02:40:08,400 --> 02:40:14,400
call things as they are, but it's just, I want to see a world where people are optimistic again.

1583
02:40:15,280 --> 02:40:24,640
Yeah, that's a great goal. And humans are good at being optimistic, but we are so attuned to our

1584
02:40:24,640 --> 02:40:32,720
environment. You know, like it sounds dumb, but there's winter, spring, summer, and fall in winter

1585
02:40:32,720 --> 02:40:41,280
again. And you know, we were very optimistic towards the length of the day and the growing of

1586
02:40:41,280 --> 02:40:49,040
everything. And you know, logically pessimistic, you know, at the end of the year, where you start

1587
02:40:49,040 --> 02:40:54,400
to run under here and work run out of stuff, and our environment and our culture and everything

1588
02:40:54,400 --> 02:41:01,680
that we create influences people's attitudes and stuff. And getting like, like even like a company

1589
02:41:01,680 --> 02:41:09,040
culture, like how do you get to growth mindset? And one reason I love working Freelon is I watch

1590
02:41:09,120 --> 02:41:14,320
them create it. Because it's like, we're going to save our so we're going to go to get a backup

1591
02:41:14,320 --> 02:41:19,520
playing it and we're going to do 10 of possible things. And we're going to work our asses off

1592
02:41:19,520 --> 02:41:25,280
to do it. And there's so much space now that's all this space. Are we going to make a rocket

1593
02:41:25,280 --> 02:41:31,600
better? Well, one set of people say you have to write a stupid or requirements document and outsource

1594
02:41:31,600 --> 02:41:36,560
it through government contractors and, you know, get a rocket that sucks for a lot of money. And

1595
02:41:36,560 --> 02:41:40,560
the other one says we're going to build a rocket and we're going to be able to do it. We're going

1596
02:41:40,560 --> 02:41:45,920
to keep doing it every day and we're going to make the machines. So they quickly ran out of machines

1597
02:41:45,920 --> 02:41:51,200
to make the rocket. So they had to make some machines to make the rocket and, you know, and

1598
02:41:51,200 --> 02:42:00,560
then, you know, something magical happened because now they can land rockets that take off. It never

1599
02:42:00,560 --> 02:42:08,000
looks really watching them take off and land. It just feels like the future. We don't have flying

1600
02:42:08,000 --> 02:42:14,080
cars, but we have reusable rockets and I feel like that's a decent. We're going to build flying cars.

1601
02:42:14,080 --> 02:42:21,600
Don't worry about it. It's coming. Be optimistic. I was in a like, I was at Tesla the first time

1602
02:42:21,600 --> 02:42:26,080
they landed two rockets next to each other and they set up big TV screen with a couple hundred

1603
02:42:26,080 --> 02:42:31,760
people went there and everybody's milling around and they, we watched it take off and then, you

1604
02:42:31,760 --> 02:42:35,760
know, the cameras are following them up and then they separate and then they come back

1605
02:42:36,720 --> 02:42:42,960
and then they landed. The whole place jumped up and down, cheered people hugged each other.

1606
02:42:42,960 --> 02:42:49,680
There was people, there was grown people crying. It was so fantastic and I felt like it was so

1607
02:42:49,680 --> 02:42:54,960
great to be part of that. And here's the crazy thing. We could all be part of that every day.

1608
02:42:56,880 --> 02:43:02,400
And I have to ask yourself, why aren't you part of that every day? Like how many,

1609
02:43:03,920 --> 02:43:11,280
how many things have impacted you in a way that, you know, we're all getting negged by society

1610
02:43:11,280 --> 02:43:16,960
and people are interested in, you know, negging us for their advantage. It's a, it's a crazy thing.

1611
02:43:18,320 --> 02:43:24,560
I think I'm less cynical because maybe I'm more accepting like the solution to

1612
02:43:26,160 --> 02:43:33,360
cynicism isn't to ignore it to be bitter. So you figure things out and then, you know,

1613
02:43:33,360 --> 02:43:39,760
to things like humans go through lots of cycles. They're often absolutely horrible,

1614
02:43:39,760 --> 02:43:45,440
but they create wonders and people aiming at wonders are way more likely to do something

1615
02:43:45,440 --> 02:43:50,240
good than people aiming at, you know, terrible things or restrictions or

1616
02:43:50,240 --> 02:43:57,680
the whole degrowth thing is crazy. Like, I know where it comes from. If you think it's November,

1617
02:43:57,680 --> 02:44:00,480
it's going to be cold for six months. It's a pretty good mindset.

1618
02:44:01,760 --> 02:44:05,120
There's, I think that there's something about California, like you're talking about

1619
02:44:05,120 --> 02:44:11,760
environment, having a huge influence on mindset. And California is a place that doesn't really

1620
02:44:11,760 --> 02:44:20,000
have winter. It has, you know, like, it has a sunny and it has a slightly wet period,

1621
02:44:20,000 --> 02:44:24,480
but it's just this kind of flat experience. And it was really interesting moving to a

1622
02:44:24,480 --> 02:44:29,440
place like New York, where all of a sudden you're confronted viscerally with the fact

1623
02:44:29,440 --> 02:44:34,000
that the seasons come and they change. And winters are always around the corner,

1624
02:44:34,000 --> 02:44:38,080
even when it's peak of summer, you're already thinking about that biting wind

1625
02:44:38,080 --> 02:44:44,320
in the, in the canyons. And so I, I don't think that it's incidental.

1626
02:44:44,320 --> 02:44:45,760
I was worried about the swampy summer.

1627
02:44:47,520 --> 02:44:51,920
Summer a lot more for me. It's just, there were times where it would be so cold and windy in

1628
02:44:51,920 --> 02:44:56,480
Manhattan that you would round and the wind would blow along the avenues and you would

1629
02:44:56,480 --> 02:45:00,160
round the building and it would just literally take your breath away, it would hit you in the

1630
02:45:00,160 --> 02:45:07,120
face and you're just like, and then you reenter it and you keep going. But the optimism that

1631
02:45:07,120 --> 02:45:11,680
California offers in this place where there's sun and there's grass and there's ocean,

1632
02:45:12,640 --> 02:45:18,320
I think that it isn't an accident that so much of the technologies that shaped the future is

1633
02:45:18,320 --> 02:45:22,880
coming from California, because it is a place where it's easier to be optimistic than perhaps

1634
02:45:22,880 --> 02:45:28,320
anywhere else in the country. Yeah, but on the flip side, then it also creates the comfort

1635
02:45:28,320 --> 02:45:36,720
stuff that humans are also not very good at. So not like, it's a, we're, we're curious,

1636
02:45:36,720 --> 02:45:42,800
we're a curious species, you know, and, and you have to be knowledgeable about what that is.

1637
02:45:44,480 --> 02:45:50,000
You know, the, when I was in Intel, there had been a 10% layoff, which by, you know,

1638
02:45:50,000 --> 02:45:55,200
Silicon Valley standards and companies with troubles with nothing. And like four years

1639
02:45:55,200 --> 02:46:01,040
after it happened, they all still talked about it. And, and that's partly because it was viewed as

1640
02:46:01,040 --> 02:46:08,720
unfair and poorly executed, which might have been true. But there was also a presumption of

1641
02:46:09,840 --> 02:46:18,320
stability and comfort, right, that had grown into the culture. And, and, and that kind of thing

1642
02:46:18,320 --> 02:46:23,840
could have been, you know, a reset and a positive journey, but it caused people to kind of double

1643
02:46:23,840 --> 02:46:30,400
down on protecting their turf and security and really dysregulated quite a few people.

1644
02:46:31,840 --> 02:46:40,240
And so that seems weird, but like even big organizations need proper spiritual leadership to

1645
02:46:42,000 --> 02:46:45,760
to navigate, you know, stuff like that. You know, there was a need for

1646
02:46:46,560 --> 02:46:52,320
change and reorganization, but it had like almost everything needs to be done skillfully.

1647
02:46:53,840 --> 02:46:57,360
Now, sometimes you can't do it skillfully because you just don't know. And, you know,

1648
02:46:57,360 --> 02:47:01,360
the famous World War II study where they evaluated like the quality of decisions

1649
02:47:01,360 --> 02:47:09,120
based on the time contemplating the decision. No, no, they, and they found there was no correlation.

1650
02:47:10,160 --> 02:47:16,400
So there was decisions made in weeks, decisions make months, decision making years, but the decision

1651
02:47:16,400 --> 02:47:22,160
making time was not correlated with the quality of the outcome. And the big difference was the

1652
02:47:22,160 --> 02:47:28,640
faster you made decisions, if it was bad, you found out faster. And so again, this is great

1653
02:47:28,640 --> 02:47:36,480
attention. So you need to have some framework and some like, you know, sensible guidance when

1654
02:47:36,480 --> 02:47:42,160
you're doing change on the flip side, you know, change needs to move at a pace such that you

1655
02:47:42,160 --> 02:47:48,880
can evaluate the failures of it to move on. And again, that's like, and they're both true.

1656
02:47:49,440 --> 02:47:57,360
And so, like, how do you be, you know, and human beings need to be able to live in a world

1657
02:47:57,360 --> 02:48:04,000
where many, many, many things are true. And then we're navigating a line along some set of these

1658
02:48:04,000 --> 02:48:11,200
constraints. But, but some things, okay, have more positive outcomes than other ones.

1659
02:48:12,640 --> 02:48:16,480
I mean, like maintaining a clear line for how those cause and effects

1660
02:48:17,280 --> 02:48:21,600
folds together is also really important, right? Because the worst outcome is to make a decision

1661
02:48:21,600 --> 02:48:25,680
and then to not really understand what the effect of it was, so you can never evaluate if you made

1662
02:48:25,680 --> 02:48:31,920
the right decision. Yeah, you see that kind of stuff happen all the time. Yeah, yeah, yeah.

1663
02:48:32,720 --> 02:48:36,960
When you want to learn stuff and make decisions and do experiments and sometimes fail,

1664
02:48:38,000 --> 02:48:42,640
then you really have to be willing to look at what really happened and why. And some companies

1665
02:48:43,200 --> 02:48:46,880
are going, like, we're going to make more decisions and fail when we need to. And then

1666
02:48:46,880 --> 02:48:52,560
they all cover up what the failures were, assign blame to the political parties out of power,

1667
02:48:52,560 --> 02:48:58,800
and, you know, it doesn't work at all. Oh, yeah. I like companies where like,

1668
02:48:59,600 --> 02:49:04,880
like Tesla had this idea, like doing the normal thing was failure. Thinking a risk was good.

1669
02:49:04,880 --> 02:49:08,560
And if it worked out, that's really good. And if it didn't work out, if you learn something,

1670
02:49:08,560 --> 02:49:14,400
that's good. As opposed to most places, you judged on the outcome. You know, if it worked,

1671
02:49:14,400 --> 02:49:21,120
it's good. If it didn't work, it's a failure. And which makes a lot of sense, unless you're

1672
02:49:21,120 --> 02:49:27,840
trying to do new things, in which case doing the same thing has doomed you to not improving.

1673
02:49:28,560 --> 02:49:32,000
I heard that culture really broke a lot of people though.

1674
02:49:32,960 --> 02:49:34,560
Yeah. Oh, yeah, it's really hard on people.

1675
02:49:37,760 --> 02:49:41,680
Yeah, no, I saw people at Tesla, like after four years, a lot of people be burned out,

1676
02:49:41,680 --> 02:49:43,920
but I tell you, they're way better engineers when they started.

1677
02:49:45,200 --> 02:49:49,920
So they kind of give questions, what's your goal? To have a happy, comfortable life or to have a,

1678
02:49:50,720 --> 02:49:54,000
you know, population of people that can do things, pick stuff up and get to work?

1679
02:49:55,040 --> 02:49:55,520
It's your goal.

1680
02:49:56,560 --> 02:49:57,200
What's that?

1681
02:49:57,200 --> 02:49:58,800
I said, what's your goal?

1682
02:49:58,880 --> 02:50:02,160
Oh, I like to do stuff. Like, I'm really curious about things. Like,

1683
02:50:02,160 --> 02:50:05,760
I'm curious what's going to happen. I'm curious about what people are going to do.

1684
02:50:07,120 --> 02:50:10,560
Like, currently, I'm running a science project that my company about,

1685
02:50:11,360 --> 02:50:18,720
can we achieve a high level of creative and productive output with, you know,

1686
02:50:18,720 --> 02:50:24,880
independent autonomous teams doing stuff? Oh, I'm into it. It's fun to watch people.

1687
02:50:25,840 --> 02:50:29,360
It's just in every possible way, I think I could, I could jump in there and help out

1688
02:50:29,920 --> 02:50:34,480
what if they fail, they'll learn a lot more and then, you know, we'll see what happens.

1689
02:50:36,720 --> 02:50:41,120
Yeah, I played the fuck around to find out video for the whole company in all hands.

1690
02:50:41,680 --> 02:50:42,320
Super fun.

1691
02:50:44,320 --> 02:50:44,960
What's that?

1692
02:50:44,960 --> 02:50:46,640
I said, I don't know what that video is.

1693
02:50:46,640 --> 02:50:53,520
Oh, I'll send you the video. It's a little walkthrough of a graph about, you know,

1694
02:50:54,960 --> 02:50:57,680
what do you have to do to find out? It's pretty important.

1695
02:50:58,160 --> 02:50:59,280
All right, we'll keep it in mind.

1696
02:51:00,000 --> 02:51:03,200
Well, then, then some group did, one of my teams did something,

1697
02:51:03,760 --> 02:51:07,680
and then it was a complete mess. And they were like, well, Jim, you told us to do this.

1698
02:51:07,680 --> 02:51:11,520
Like, you played the video. Like, what did you expect would happen? Well, I was

1699
02:51:12,960 --> 02:51:16,240
hoping you'd iterate a couple of times faster and figure it out a little.

1700
02:51:17,440 --> 02:51:18,960
Are you, are you still hiring?

1701
02:51:19,920 --> 02:51:20,160
Yeah.

1702
02:51:21,360 --> 02:51:22,160
Who are you hiring?

1703
02:51:22,720 --> 02:51:29,440
Oh, yeah, over the last year and a half, we had some like, let's say leadership

1704
02:51:30,000 --> 02:51:33,760
voids and we did a bunch of reorganization and smaller teams.

1705
02:51:33,760 --> 02:51:42,240
And right now we're, we're mostly hiring for like skill sets. We're hiring programmers and,

1706
02:51:42,240 --> 02:51:45,840
you know, like computer design is complicated. There's so many different skill sets.

1707
02:51:45,840 --> 02:51:48,800
Like people don't realize how many disciplines there are.

1708
02:51:49,360 --> 02:51:52,480
Like, even if you're an electrical engineer, a computer engineer, a programmer,

1709
02:51:53,280 --> 02:51:55,760
underneath there's a lot of differentiation.

1710
02:51:58,480 --> 02:52:01,520
So yeah, we're going to probably hire 50 people relatively short order.

1711
02:52:03,040 --> 02:52:07,920
And, and then when you hire people, it's really the coolest thing is everybody hire

1712
02:52:07,920 --> 02:52:13,040
changes your group a little bit. So you make a plan and then hire some people.

1713
02:52:13,040 --> 02:52:18,240
And then the new people change what you're doing and why and there's new ideas.

1714
02:52:18,320 --> 02:52:21,200
And sometimes it causes a reshuffling of how the org works.

1715
02:52:22,400 --> 02:52:23,280
How many people do you have right now?

1716
02:52:25,280 --> 02:52:29,040
Um, yeah, 10 students, 450 people.

1717
02:52:29,040 --> 02:52:31,760
Oh, so if you're hiring 50, it's like a pretty significant.

1718
02:52:31,760 --> 02:52:33,360
Yeah, it's a big chunk. Yeah.

1719
02:52:33,360 --> 02:52:35,360
Tom Xeminy, we're close to 30.

1720
02:52:36,800 --> 02:52:37,680
Yeah, it's super fun.

1721
02:52:38,240 --> 02:52:38,720
Very cool.

1722
02:52:40,000 --> 02:52:41,440
Well, I think we should let you go.

1723
02:52:41,440 --> 02:52:43,440
There's a lot more left on the table to talk about,

1724
02:52:43,440 --> 02:52:46,640
but I'd prefer to just have you back then to keep rolling right now.

1725
02:52:46,720 --> 02:52:49,760
All right. Hey, great to chat.

1726
02:52:49,760 --> 02:52:51,280
I was very curious how this would go.

1727
02:52:51,920 --> 02:52:53,680
Yeah, thanks, Jim. You're a fascinating dude.

1728
02:52:54,560 --> 02:52:56,320
Oh yeah, you guys too.

1729
02:52:57,200 --> 02:52:59,840
Keep it up. I, uh, I enjoy your guys content too.

1730
02:52:59,840 --> 02:53:00,800
Appreciate it.

1731
02:53:00,800 --> 02:53:01,840
Thank you so much.

1732
02:53:01,840 --> 02:53:03,440
Have a great rest of your day.

1733
02:53:03,440 --> 02:53:04,240
All right.

1734
02:53:04,240 --> 02:53:04,560
Thanks.

1735
02:53:04,560 --> 02:53:05,040
Take care.

