start	end	text
0	5200	Welcome back to Demystify Sci, where today we are exploring how to achieve the impossible.
5200	9760	For this conversation we have with us Jim Keller, who is a microprocessor engineer who's worked at
9760	17280	places like AMD, Apple, Tesla, and currently is working on a next generation of AI compatible chips
17280	21680	that are going to be a competitor with Nvidia for all the stuff that they're doing for artificial
21680	28000	intelligence and also has this wild idea for a startup which is already in progress for being
28000	35760	able to create a semiconductor fab that is tabletop size. He's also got the idea of being able to
35760	43120	3D print a car for $5,000 and so this is a guy who's worked for his entire life at these enormous
43120	48080	organizations that are able to achieve things that across the board people say are not possible.
48080	52400	And so we wanted to start with him to figure out what does that actually look like? What are the
52400	56800	things that go into making a successful company? What are the pieces of culture that are internal
56800	61520	to the company versus the pieces of culture that are internal to the humans within that company?
61520	68960	And how do they play together in order to let people achieve what seems on paper absolutely
68960	75040	not going to happen? Which is very improbable as a perspective considering he's actually
75040	80320	so cynical about the lifespan of institutions and of these companies. He recognizes there's a
80320	86880	cyclic nature to the boom and bust of the production in all of these different organizations.
87840	94000	And so he's pulled out these really fascinating trends for what it means for a company to reach
94000	98240	that pinnacle and why they can't stay there. And the same thing can be applied to our scientific
98240	104080	institutions, our government, and we go into all of this. So it's a really, really refreshing,
104080	109040	inspiring perspective that I don't think we've seen on this show so far.
109040	113600	And honestly, it's also just really cool to sit down with somebody who has worked on the chips
113600	118720	that have been in the technology that I've used my entire life and to discover that he's thinking
118720	127040	about physics and science and human nature and the fractal arc of reality in this way that actually
127040	132240	gives him the ability to come up with ideas that other people just seem to not have access to.
132240	136560	And so we get into all of that. It's a really good conversation. Hopefully we'll be able to
136560	141520	have him back to talk about some other stuff because he's been in contact with these titans of
141520	149120	the last 50 years, Elon Musk, Steve Jobs, and just has a wealth of experience in the world
149120	153600	that I think would be really interesting to talk more about. But in the meantime,
153600	159440	I want you to consider coming over to our Patreon. So we are a patron-sponsored podcast. We take no
159440	165520	ads, we have no commercial sponsors, and we really, really, really want to keep it that way
165520	170800	because that aligns with the way that we see the world functioning. We make something that is useful
170800	175920	to people and the people who like it support us and let us keep doing it. And so if you've watched
175920	180560	a couple episodes of the podcast and you enjoy it, then consider coming over to patreon.com
180560	185200	slash dmstify.com and joining us for just a couple dollars a month. A fistful of dollars if you like.
186400	193680	Indeed. However, many dollars fit into the fistful that you would like to take, happy to accept.
193680	196160	You can put different denominations in a fist, it turns out.
196160	202400	It's true. And also, something else that you can really do is if you're watching the podcast on
202400	207840	YouTube, leave a comment. If you're watching it on Spotify or any of the podcast stores,
207840	213600	perhaps rate the podcast. These are all things that help us boost ourselves algorithmically,
213600	216800	and they don't cost you anything except for a few minutes of your time.
216800	221920	And it helps us get better guests too, which is ultimately going to serve you guys. So do it.
222480	227360	All right. Hopefully, you will follow through. Hopefully, we will see you soon.
227360	229520	And for now, enjoy the conversation with Jim Keller.
245520	249680	I came across a quote that I think is an unfortunate article about you where they
249680	254320	said that you're really a fan of the Steve Jobs aphorism, which is that once you know what to do,
254320	264480	you shouldn't work on anything else. And I wonder about that in context of institutional longevity,
264480	269600	because it's really easy for an institution to continuously come up with a new thing to do
269600	275680	and keep trying to work on something different. And so in addition to knowing what to do and
275680	280480	then working exclusively on it, do you think that there has to be a sense of an expiration date?
282000	288960	Yes, probably definitely. So this is a, yeah, this is a really complicated question.
290080	300320	And then I was part of, you know, the demise of digital equipment,
301120	307520	which was a great company. And we went from growing and everybody there was excited.
308720	312800	A friend of mine's wife said, what do they put in the water? All you guys do is work or talk about
312800	321200	work. It's really fun. And then I've talked about this in a couple like financial analysts
321200	327600	seminars recently that like when I joined digital, they were very proud of building low-cost open
327600	334960	computers. And they were winning against IBM. And then IBM, and people said nobody went broke
334960	341200	by in IBM, home digital was building this lower cost, more open computer that people could buy
341200	348720	and do stuff with. And 10 years later, and only took 10 years, they're competing against sun and
348720	355280	silicon graphics, who are building lower cost, easier to use computers. And the digital sales guys,
355280	362400	some of the same people, I suspect, you know, poo pooed the sun systems as toys and cheap and
362400	368000	little and non-professional. And, you know, digital went through like a literal collapse.
369840	378240	Their best revenue year was on falling sales, rising prices. And then they just lost the
378240	386560	market. And it was also interestingly enough, we were going bankrupt at the same time we were
386560	391440	building the world's fastest computers. We were building a new generation of product that was
391440	397600	demonstratively be better than anything we'd ever made, better than sun by a lot, and went bankrupt
397600	405280	at the same time. And so. But seems impossible on some of them. Yeah, yeah, it's a miracle.
406000	414000	But there was all kinds of things. So, so you have to, yeah, I tell people sometimes ask me,
414000	418400	like, how do you manage a team? And how do you do something? And people simultaneously,
420400	426880	let's say, underthink it and underdo it. Like, people will read one management book. A frequent
426880	432080	question I get is, which book should I read? Because I say I read lots of books. I mostly
432080	438160	say all of them. And, you know, and I've been trolling some of my internet friends by releasing
438160	444000	lists of five books, but they're, they're relatively randomly arranged, you know, book on
444000	449680	management, a book on science fiction and the book on, I really like Katie Byron Katie's book,
449680	459280	Loving What Is, you know, and, you know, so. But now, as a book on management, that's not obvious,
459280	464480	but it's a great book on management. Same with the Five Love Languages, which is, I think,
464480	471200	a marital book. And, but it's complicated. You need lots of different frameworks to figure things
471200	475600	out. So, so digital equipment is a great example of that. The company could literally
476960	483120	revert almost everything that made it successful, then go bankrupt while building a great new product.
483600	490800	And mostly, I think that made it successful. Yeah. Yeah, so we were building alpha computers,
490800	496800	which had 64-bit addressing, and it was one of the better, better ones of the early 64-bit
496800	502400	computers. And one of its charms was we could adjust very large memories from some server
502400	507440	applications. That was important. But the memory group at digital had been making lots of money
507440	512720	for years and they'd been raising prices. They literally created an industry around
512720	518800	digital making plug-in memories that were cheaper when company EMC became very large and successful.
520400	524160	So, as digital brought alpha to market, the memory guys were raising money,
524160	532480	raising memory prices, and we didn't sell any memory. So, so that one of the virtues of the
532480	537360	computer was addressing more memory and memory prices were so high that our customers
537360	542640	were buying memory from a competitor. Like, like in the world of like, how dumb could it be?
543360	548320	And as engineers, we knew this was happening. And the memory group was working with us to
548320	552640	make it harder to plug in memory and the engineering team made it easier to plug in memory.
553200	557760	And there was no intermediary that could be like, hey, we're working against each other?
558720	563360	Well, so, so this is where you get it. So, Ken Olson was the founder and he was still there,
563360	570720	mostly, although he was replaced with Bob Palmer. So, Ken had this kind of do the right thing attitude
570720	576160	and, and, you know, let people go off and do their thing. But the problem is the memory business
576160	581600	was a business and the server business was a business. And the memory business made more money
581600	587840	by actually lowering the server business substantially. And that's where, you know,
588400	594400	the uncoordinated action kill them because usually, you know, you, you would think somebody would say,
594400	597920	hey, this doesn't make any sense. We're killing our business by not doing something.
598480	602400	But it's really hard and mostly not companies when they're non founder run,
604000	606000	you know, run the ground at some point, because they
607200	610560	founders optimized for the longevity of their baby. And
611440	614880	non founders frequently optimized for their own personal income.
615440	617840	So how many people were digital equipment at that point?
618640	620400	The peak was 110,000.
621440	623280	That is enormous.
623280	627840	Yeah, yeah. Yeah, back in the day, it was a Arctic cap was 14 billion.
629120	633680	I worked on a computer. We, you know, back to the 800, we sold $5 billion worth of
634480	638000	computers at a half a million a piece. It was, it was really amazing.
639040	644400	So I like, like I said, so you need a couple things. So there's the life cycle story, which is
645120	650000	you know, in human beings to the zero to 20 year, you're a kid learning stuff. And then 20 to 40,
650000	654960	you find your place in the world. And then 40 to 60, you should, that's a mobile, exploit your
654960	661200	expertise. And then 60 to, you know, death, you enjoy a retirement or your enlightenment or
661840	665600	if you're a sociopath, you might continue to operate at some high level
666640	670800	of manipulating reality, right? And so companies go through those cycles.
671680	675040	And a lot of companies fail long run because they just get old.
676880	681040	And some companies get restarted, like, you know, both Apple and Microsoft were
681760	687520	essentially rebooted, you know, Apple by the founder and Microsoft by
688800	694720	Satya Nardella, which is, which is amazing. Like nobody saw that coming because that company had
694720	700640	seemed to have gotten into the, you know, they had cash cow products and they were soaking the
700640	709360	customers for it. So it's very hard to, to reboot a company substantially, once you pass a certain
709360	715600	point. But anyway, it's a framework. And there's another framework, which is organizations tend
715600	723600	towards order and the order slowly, like a startup is very chaotic. And a friend of mine drew this
723600	730480	graph of, you know, the X axis is chaos at the origin and then order, and then the Y axis is
730480	738160	productivity. So for a while, as you increase organizational procedures and, and, and process,
738160	744000	you get more productive, but at some point you get less productive. And the trick isn't figuring
744000	748080	out where you should be on the curve, you should be at the top. It's like, you know, you curve,
748960	754800	right? The trick is staying there. Because once you start organizing for productivity,
755760	762080	more order always seems like the right answer. And then I know a lot of companies, the people who
762080	767200	are very good at organizing things, sort of outmaneuver politically, the people are good at
767200	773120	inventing things. And if there's nobody going, wait a minute, we need a new product. So it's
773120	778880	very difficult to escape that trap. And does the size of the company influence that as well?
780240	785840	Yes and no, you know, like there's many companies who go through, you know, never get very big,
785840	792560	but still become bureaucratic. But it's, it probably gets harder to avoid it as you get bigger.
794160	797680	Like, you know, a lot of the big tech companies that we think are, you know, great,
798480	804400	are famously bureaucratic, you know, like Google and Facebook. And then there's some companies
804400	811200	that are famously not like Amazon's run as a whole bunch of small silos that, you know,
811200	818800	compete with each other. And I'm not sure what the current lay of the land is at Microsoft.
818800	826160	Apple under Steve Jobs was mostly small teams and he didn't trust big teams. But since he
826160	830560	passed away, the company has become unbelievably large and successful and all the teams are huge.
832240	835840	What was the secret to the, to these reboots that actually worked out?
837680	845440	Well, Steve Jobs was in a very strong belief in product. So we, I had a friend who worked for
845440	852320	him at the time, and he said, we have 10 business groups, and they're all losing money. The company
852320	856720	is losing money, but on paper, all the groups are making money. And that's because they were doing
856720	861520	transfer costs, you know, bureaucratic shenanigans, like, I'll make something for you and sell it to
861520	866320	you as I have a profit, but then you sell it to somebody, you know, so it was just a mess.
866880	871920	And there's too many competing products. And if you have product A and product B,
871920	876320	the marketing guy will say, make product between them. And, you know, you'll have a better product
876320	882720	line, a better coverage. So Steve Jobs, he canceled apparently all the business units,
882720	887760	he canceled the products, he canceled, fired most of the managers. And he created the same as
887760	894960	two by two matrix, which was consumer pro mobile desktop. He said, we're going to make four products.
896400	901600	And everybody wants one of those. Like if you're a professional and you sit at your desk, you want
901600	907600	a desktop pro computer. If you're, you know, working, but you travel, you want a professional
907600	914160	mobile computer. That was the MacBook Pro, the Mac Pro. And then the iMac was the famous
914160	921200	translucent, you know, fun computer for the house. And so he created those. And then he said,
921200	927200	we'll make each one of them the best we possibly can, which means we won't get all the customers
927200	932880	because there'll be gaps and holes. But you could trust us that if you buy iMac, you'll be very happy.
934400	941920	And, and then when I was going really well, then they did the iPhone. And then he really believed
941920	948240	in the iPad. I was there during the, I guess the third iPhone chip in the start of the iPad.
949440	955360	And then he really believed in TV and he told us he'd cracked TV. But he passed away before he
955360	960160	built the product. And I don't know what he cracked because it wasn't the Apple TV product. He
960160	967120	didn't like that very much. But he was very focused on the next thing. And then he's, Steve famously said,
971360	976400	like, so technology is often what I call a cascade, instead of cascading and diminishing
976400	982960	return curves. So you go up and it plateaus, and then there's a new invention, you go up into plateaus.
983920	989760	So we went from rotary phones to button phones to touchscreen phones. And each one of them,
989760	995440	when it first started. So the first, you know, touchscreen phones weren't as good as the buttons.
997040	1004320	Right. So black, blackberry users laughed at the touchscreen phone people. So the screen was
1004320	1009600	bigger, but your touch wasn't accurate. It was slow sometime. I had friends that would prove to me
1009600	1015280	they could type faster on blackberry than a touchscreen phone. And yet the blackberry died
1015280	1020000	because they were married to the old technology. So Steve's point was when you go from one technology
1020000	1027280	to the next one, you always go down that up. Right. You're, you're jump. You're, you're by
1027280	1032720	definition jumping from a highly refined endpoint to an unrefined starting point.
1033600	1036880	That depends on when you jump, right? Because the people who had
1037360	1044320	and that's the thing. As he said, any idiot can show you could wait until it starts to be refined.
1044320	1051840	Now you're behind. So the wind, if you wait for other people to do it, you're going to have a
1051840	1060080	business strategy of being a fast follower. That's actually a business term. But if you always wait,
1060160	1066240	you'll always be second. And when people create new markets, it's amazing.
1067120	1072960	The digital and phones, I mean, Apple and phones, we did 64 bits before anybody else. We did
1072960	1078640	high resolution displays before anybody else. We did get great cameras before anybody else.
1078640	1084480	We did thin phones before anybody else. And each time it took a couple of years for people to catch
1084480	1090880	up. So yeah, it's a, it's a funny thing. So was this, this was the reboot strategy?
1090880	1095440	Yeah. So this was after like Steve's got thrown out a digit out of Apple,
1096640	1101440	partly cause he couldn't work with people and partly he made a big back on the big bed on the
1101440	1108080	Lisa and then the factory for the Lisa and the original Macintosh and it didn't go well.
1108400	1115840	And he was famously hard to work with. And so when he came back, Apple bought next.
1116720	1119760	And next was Steve's company with a new operating system.
1120800	1124720	So like Apple at some point needed the new operating system they bought next. And then
1125440	1131600	for a bunch of reasons, I wasn't there. He maneuvered his way back into being CEO and then
1132560	1139120	you know, launched into a famous reboot of Apple, which, which by the way is relatively unprecedented.
1140320	1143360	He said that Microsoft managed to do it as well. What was the story there?
1143360	1149840	Yeah. And that's not a non-founder. Yeah. So Microsoft, everything was Windows. They had
1149840	1157120	Windows, you know, Windows phones, Windows PCs, Windows mouse. And the world was sort of shifting
1157120	1160880	away from Windows and Windows had a lot of problems. Like it was going mobile.
1161440	1165600	And they tried to build something called .NET, but everything they did was proprietary
1166400	1172800	early and the company was suffering. And Satya pivoted the company to being data first,
1172800	1180720	essentially. He got rid of all the Windows groups. We have, I suspect they actually let go a lot
1180720	1186240	of people, including senior management. So Microsoft built this beautiful tablet computer.
1186240	1191200	I know the people who made it, but the group that did Office Suite wouldn't port their software to it.
1193120	1196800	Right. And nobody could tell them to port. Like they ran a business and they said,
1196800	1203520	it doesn't make any business sense for us to port the software. So under Satya, that all changed.
1203520	1207200	What's that? Because it would be packaged with the tablet when it was sold and so it wasn't going
1207200	1212640	to make money. So this is where you guys have probably read Shakespeare, right?
1213520	1215680	Some, not with them.
1216480	1222720	It's good to read Shakespeare. Right. So Shakespeare is always the drama between the king,
1223520	1227440	the ministers, you know, the poor bastards and the hero.
1229360	1236800	Right. And so big companies, when they become bureaucratic, you know, if they're run by somebody
1236800	1241120	who then has some number of people running organizations, let's call them the conniving
1241120	1249040	ministers. Right. And they are, let's say, vying for favor from the king and benefits
1249760	1255840	by mostly messing with each other. And the king isn't king because he's stronger than all of them.
1255840	1260960	He's stronger because he plays them against each other and they play against each other.
1260960	1269040	But at some point there's, let's say, a real problem. Like Windows is going down or the
1269120	1273520	Macintosh is broken. And then the drama is, is how does it play out?
1274800	1278960	Because the king can't trust the hero because the hero will replace them. The poor bastards are all
1278960	1284160	rooting for that. And the ministers are trying to get the hero on their side because then they can
1284160	1290880	win. And it's very difficult in a lot of companies for anybody to, unless there's a really strong
1290880	1297040	founder and a really strong culture. The picture that you're painting makes it seem
1297040	1302320	almost miraculous that anything gets done. Oh yeah, that's actually true.
1307280	1313280	Now I read this funny, here's a funny one. So there was a study about the differential
1313280	1320480	growth between China and Vietnam. This is a 20, 25 years ago. And they basically said there was a
1320480	1326960	10% difference in corruption between the two countries. I forget the exact numbers.
1327760	1332400	And as a result, China was growing at 10% year over year and Vietnam was growing at zero.
1333920	1339680	And the corruption number was big, like 50%. Which was what were corrupt?
1344240	1348800	According to this study, I don't know if that, you know, it's true. But, you know, people look
1348800	1354640	at spectacular growths and think, wow, that's amazing. Like they must be all working really
1354640	1361920	hard and doing really good things. But it could be they're producing 4%, you know, real benefit
1361920	1367520	year over year, but compound growth of 4% actually over, you know, 10 years is a lot.
1369200	1372720	Right. And then in our big economy today with all the companies,
1373680	1378720	like there's lots of strategies. So new technologies are mostly exploited by new companies.
1379680	1385360	Like when Google was growing fast, some big companies had search engines, but they were all
1385360	1392640	terrible. And Google grew and, you know, for a while, for a long time, they profess, you know,
1392640	1399040	they do no evil and everything was the good of humanity. But the way they made money is
1399040	1403680	serving ads. And then what happens is they get really good at it. And the more ads they serve,
1404640	1409440	you know, the more money they make, and the more they get you to look at them, the better. And then
1409440	1416160	they go back to their, the people want to pay for ads. And they say, we can manipulate the
1416160	1424240	results to give you better responses. And now you have their company mission, which is to make
1424240	1428400	all the data available to everybody. But their business says we're going to make a lot of money.
1428640	1435600	But their advertisers, you know, would like, you know, differential benefits and the consumers
1435600	1442560	have options. Right. So that's a complicated thing. And then you could cast that as a Shakespeare
1442560	1449280	play and assign the, you know, the villains and heroes as you want. But it's really complicated.
1450560	1454880	And this happens all the time, like every company goes through these cycles.
1455840	1460160	Do you have any insight into the day that Google dropped its do no evil slogan?
1461280	1463520	Yeah, it's long after that was true.
1465440	1470640	You know, just so that there's cognizant. Well, companies go through this cognitive
1470640	1475760	dissonance phase, you know, they have a corporate vision about who they are and what they want to
1475760	1480960	be. And then they have a bottom line to attend to. And then the business practices that accomplish
1480960	1488240	that. And then I'm sure some engineers said, hey, we can't do X, Y and Z because of our vision.
1489200	1493920	And it was like, yeah, well, we haven't been doing that for years. And, you know, it's complicated.
1494880	1498640	You know, most people at Google want to do the right thing. And most businesses want to,
1498640	1504560	you know, be, you know, let's say as moral as they can be. And then the market
1505360	1511680	kind of does this interesting thing, which is as companies become monopolies and start like
1511680	1518480	squeezing their customers, essentially, you know, it's easy to cut your R&D and, you know, spend
1518480	1525520	more on sales and make more money. But then there's a competitive thing. Like the US car industry,
1525520	1529840	there was the big three, and they were all playing the same game and gaming each other.
1529920	1535120	And when Toyota showed up, they laughed. It was a shitty little car. And Toyota made a better
1535120	1542000	and better car. And all of a sudden, they're on the back foot, you know, trying to catch up with
1542000	1547040	Toyota. Like, who saw that happen? They actually went sun Pro and went bankrupt. And then the
1547040	1551440	government bailed them out because there's, you know, like the layers of complexity is they're
1551440	1557920	so high. It's amazing. Like, they can some change depending on that at some point.
1557920	1564560	Yeah, yeah, there's the politicians and the unions and the jobs and the states and distributed
1564560	1571760	manufacturers and, but they all going through these cycles of their internal growth of technology
1571760	1577360	cycles of, you know, competitive cycles. Yeah, it's quite amazing.
1578320	1582960	Hey, folks, quick interruption. I really need you to come over and check out our Patreon page.
1583760	1587600	You can give as little as a couple of dollars a month, or you can give us a fistful of dollars
1587680	1592160	The point is this program is entirely supported by people like you who are enjoying this program.
1592160	1596400	Don't have any sponsors. Don't have any ads. We want to keep it that way. So please consider
1596400	1600720	coming over and checking out how you can just give the tiniest amount to support this project
1600720	1606800	into the future. Thank you. I'll see you there. So do you think that you can tell when an organization
1606800	1613920	is starting to get to the place of decline early? Or do you think that it's always something that
1614000	1620400	you can only really tell once the decline is fulfilled and then you look back and kind of
1620400	1626880	match things? Like is there a predictable time length? Even is there a time scale? Is it size
1626880	1631440	dependent? I'm not even thinking necessarily about time scale. That would be interesting. I'm
1631440	1637920	thinking more about it. In terms of things that happen, like patterns that you start to see emerging
1637920	1641920	and people starting to behave in certain ways. Like you're talking about the drill equipment
1641920	1647280	where you have the the branch that's dealing with memory and the branch that's dealing with
1647280	1652880	servers completely operating at odds with each other. That serves as this harbinger of chaos
1652880	1662560	to come unless somebody can come in and you know make the vessels behave. So in retrospect,
1662560	1669920	it's way easier. Somebody said, you know, the past the banker, you know, they were going to
1670000	1674960	bankruptcy and they said it happened slowly at first and then all at once. And that's often
1676160	1682560	so digital went through this rising revenue on falling unit sales. That's a really good sign.
1685680	1691760	You see companies, I worked at a company that was basically capitalizing cost.
1692720	1698160	So if you build a product and you have to buy a component to put in that product, that's an
1698160	1702960	expense. But if you build a factory to build that component, that's capital.
1704800	1711840	Right. And so for financial accounting reasons, capital is deductible and viewed as a good thing.
1711840	1719120	Capital is an asset versus expenses are a problem. And so as companies get bigger and
1719120	1723680	mature, they get, let's say, sophisticated economic tools and some of those are really good
1724560	1728400	because we have complicated tax rules and international business rules
1729280	1732480	reporting roles and you have to know what they all are and you have to play the game.
1733440	1740960	Otherwise, you won't report making any money. But you know, the nominal timeline looks something
1740960	1746640	like the life cycle of a human being, which I said is 20 years, 20 years, 20 years, 20 years.
1747440	1754400	But some of that's accelerated, like the lots of startups go through five to 10 years of
1754400	1759680	figure finding their real place in the world and then ramping. Some companies ramp really quick,
1759680	1763440	like Google probably would be wandering around for five years.
1764240	1767600	Like they were making real progress and lots of people like them, but they weren't making any
1767600	1775440	money. And then they ramped pretty hard for over 10 years and 15. They still make lots of money.
1775440	1781360	To a lot of the AI companies right now, right? Like OpenAI and Anthropic, their ramps are
1782080	1786720	super big. Yeah, that's a, so there's another phenomenon. So independent of the business
1786720	1794320	cycle, there's kind of hype cycles. The AI is as a strong hype cycle, partly because there's a lot
1794320	1801520	of revenue and market cap, you know, based on it. So there's a little FOMO going there, which is,
1802240	1805760	you know, let's, let's get involved and do something.
1806480	1811680	How much of that, can you tell how much of it is hype and how much of it is real? Because, I mean,
1811680	1817680	you obviously can't really trust the representatives of a company to tell you accurately
1819200	1824800	what's happening or what will happen. Promise is these massively intelligent,
1826160	1830960	general intelligence machines that will one day outshine humans. Everybody's pointing to the
1831040	1837280	exponential curve and like, we're, it's going to continue forever. But anybody who looks at
1837280	1842960	exponential curves also knows that sometimes they're logarithmic and they just stable it.
1842960	1847440	Yeah, yeah. Yeah, they're, they're S-curves, not exponentials. Yeah, it's pretty.
1847440	1854080	Right. Yeah, it's hard to say. So the, in the internet boom, there was a whole bunch of companies
1854080	1860080	who clearly had to achieve some network effect to be successful. So two good examples.
1860640	1866320	One early was PayPal. So they were paying their customers to recommend them to other people.
1867280	1871280	So they spent hundreds of millions of dollars in customer acquisition
1872000	1877840	with actually no plan to make any money. But they got big enough and they started to make money.
1877840	1882400	And then there was other companies who said, Hey, we're going to do the same thing, web band. And
1883200	1888400	it was a pet company that was doing online stuff. So they were paid, they spent billions of dollars
1888400	1894720	in customer acquisition and then didn't keep any of them and just earned the money at all.
1894720	1898880	And Uber was the same way. Uber spent billions of dollars in customer acquisition.
1900160	1904400	And, you know, Uber used to be a really good deal compared to a taxi on price.
1905040	1910240	Like the convenience is a huge win, but the price was also better. But when they slowly
1910240	1915040	started working on making money, when they had acquired enough customers that the argument of
1915040	1920400	customer acquisition was kind of dumb, they started raising prices. And now it's not obvious
1920400	1925360	that they're cheaper than taxis. They're more convenient. Now we got used to them. And then
1925360	1931280	they're, you know, 10 years of spending money to get customers, you know, they put a lot of
1931280	1938400	taxi companies out of business. But, you know, it's, it's not always obvious which way it's
1938400	1943440	going to go. You know, there was a lot more web bands than there was PayPal's.
1945200	1952400	But these are, you know, business cycles, which are really interesting. The internal
1952400	1958800	dynamics. So humans are, you know, when we, when you build organizations, humans are very good
1958800	1964480	in groups of five to 10, because that's essentially a family group. And we're very good in groups of
1964480	1970160	like up to 100 or so. Because that's like a tribal group, like we can map them really well.
1971200	1977280	But when you want to grow past 100, suddenly you need a whole different level of infrastructure.
1978640	1985680	Because now you have one group working on something to deliver with another group where they don't
1985680	1993520	know anybody. And, and this is another one of those dynamics when, when you have a strong vision
1993520	1999360	and you're growing fast and there's too much to do, you know, sometimes those organizations
1999360	2005360	grow naturally, sometimes you really have to work on it. But then when a company slows down and you
2005360	2010640	have all these little groups of people who don't know each other very well, and they're exchanging,
2010640	2017440	you know, work for a boss or some reward, that stuff can get gained pretty hard.
2018400	2024400	And then companies become bureaucratic. And then there's a famous line about bureaucracy is like
2024400	2030640	bureaucracy is inevitable with human beings, apparently. And, you know, it's, it's not whether
2030640	2035520	you want one or not, it's how you manage it. Because at some point the bureaucracy will,
2035520	2040560	like somebody said, you know, imagine that bureaucracy is always run by your worst enemy.
2040880	2047120	And, and for a lot of companies, that's actually what ultimately does them in.
2047840	2051680	So you have business cycle dynamics, which is really interesting. And then you have
2052320	2057520	this organizational dynamic. And that's been studied pretty widely, you know, like there's a,
2058080	2063360	there's one line that which is 20% of the people do 80% of the work, which is fairly common.
2064720	2068800	And then there's another one, which is the output of an organization is the square root of the number
2068800	2076000	of people, which has, you know, been studied understated behind that. And then you might say,
2076000	2080240	well, then we should keep the group as a whole bunch of small teams. And some people do that,
2080240	2083680	but sometimes that's impossible. Like you're doing something really big and hard, and you
2083680	2091120	actually have to figure out, like, well, 1000 people isn't as efficient as 100 people.
2092400	2095360	You can get a lot more done with 1000 people than 100.
2096320	2101920	And is it just that the challenge with technology and large projects is that you can't break a
2101920	2106320	project down into small enough chunks that you can get people to work in these small groups?
2106320	2109120	Like I'm thinking for, you know, really,
2109120	2114240	No, that's actually that you put your finger right on it. Like one of the fundamental things
2114240	2120240	is humans aren't very smart. And we're not getting any smarter. So we don't solve harder problems
2120240	2125280	because we're smarter period. Like, well, I've been working engineering for 40 years,
2125280	2131040	I don't see any evidence that anybody's any smarter. Some of the tools we have are really good.
2131680	2133280	Do you see other people are dumbers?
2134560	2137600	No, not really. Okay, that's good. That's really no.
2138880	2142000	I'm sure the answer is going to be yes, but that's good.
2142000	2146000	I have an aunt who's very smart and very well read and she was complaining about young people.
2146000	2150400	So I, I found this letter and I read it to her and she said, that's just it.
2150960	2154960	People don't read enough anymore. They don't write, they don't do this, they don't do that.
2154960	2157040	And I said, that was written by Ben Franklin.
2159120	2162880	It's such a great, you can probably go Ben Franklin's letter about, you know,
2162880	2168800	the inadequacies of young people. So, so when you think that that mostly just tells you your age.
2170080	2175520	So all human beings around 50 years old or so starts to suspect that 20 year olds aren't very
2175520	2178560	smart. I've always been an old soul some early at that arrival.
2178560	2182720	So you might, yeah. So if you're, you're early, it really depends on where you go.
2182720	2188640	Like we hired college kids at Tesla. They were so smart and hardworking and ready to go.
2189680	2193440	And then I'm, you know, and I talked to friends at another big company and they're the people
2193440	2198080	they hired from college, to be honest, weren't very good. But that's because the students were
2198080	2203280	selecting based on the company. You know, the super smart ones wanted to go test their metal
2203280	2208480	at a place where they knew they'd be stressed. And the students that were sort of average and
2208480	2213360	wanted to go along with the get along and get a good paycheck were picking a company that was
2213360	2219520	literally easy to work for and do nothing. And I mean, I'm just thinking I was looking at,
2220560	2227360	yeah, that's a really funny drawing, hand drawing diagrams for, I think it was like
2227360	2231680	rocket propulsion systems before they had computers. And so it's this photograph of this
2231680	2237920	gigantic piece of paper. And there's like five engineers that are hand writing the calculations
2237920	2246160	and drawing trajectories. And I look at that. And to me, it seems objectively like a harder thing
2246160	2253280	to do. But if you have a computer that can do the same thing for you. And so it's like,
2253920	2258800	if the computers are picking up the slack, then you wouldn't be able to notice the difference.
2258800	2263840	But if you were to take the computers away, would we still be able to perform at that same level?
2263840	2267920	And that's, that's the part where I'm not 100% sure.
2267920	2275600	It really depends. So you could probably coast way, way further knowing a whole bunch of programs
2275600	2280800	and tools. But the Tesla rocket engines are way better than the ones that built in the 70s.
2280800	2289760	They're just way better. And you know, they produce them for 10x less money, 10x less hours,
2289760	2294480	they're reusable, they're more efficient. Like everything about them is better.
2294480	2301520	They're not even close. Modern cars are so good. Geez. You know, I was in the Hyundai factory
2301520	2310000	recently, you know, the time to go from like basically what comes into the factory is these
2310000	2315760	rolls of steel, you know, and they're, you know, random millimetres thick, depending on the parts.
2316960	2326240	And, you know, four hours later, there's a car. All stamping is done. They have a machine that
2326240	2333440	on rolls that cuts it, stamps it, knocks it out. They basically take every part and scan it with
2333440	2339520	laser beams and they, you can mark them up and correct them and tweak them. And then the robots
2339520	2345040	have put the pieces together and spot weld them all together. The damn things are near perfect.
2346320	2350000	Stamping machines are great, by the way. So they've been using stamping machines forever.
2350640	2354720	Like stamping machine is literally a car of a shape and a hard surface and then
2354800	2360640	put a big weight behind it and drop it on a piece of flat sheet metal and sheet metal.
2360640	2362640	Most of the number one cause of workplace accidents.
2365520	2370080	Yeah, the modern stamping machines, they're all operated by robots. They don't let people near
2370080	2375200	them. Yeah, stamping machines are tough. I got a funny question about donut factory and they're
2375200	2380080	always losing fingers that were there. It's the worst. Yeah, yeah, stamping donut. That's amazing.
2380800	2385920	Yeah, they've got to come from somewhere. Well, and then when you build a factory with robots,
2385920	2389760	then you wear it because the robots are so strong and they move so fast that they'll take people
2389760	2395680	apart. So then the, all the factories are like built with this is the people's zone and this is
2395680	2400720	the robots on the pretty soon robots will be smart enough not to whack people. Like all the robots
2400720	2404560	will see. But at that point, anyway, it's like the robot containment zone as well.
2405680	2409520	What's that? I said, but at that point, when they're smart enough, they can escape the robot
2409520	2412640	containment zone as well. So that's, that's a privilege sort of thing.
2417840	2421360	Were you freaked out about these possibilities when you were working on that system?
2424400	2432400	Yeah, so I ran the hardware, yeah, the autopilot ship hardware team. So we built a hardware
2432400	2437520	three, it's called hardware three chip and started hardware four chip. And then also started Dojo,
2437520	2443760	the Tesla super computer. And then for a while, they, the autopilot software team reported to me,
2444560	2450720	but that was sort of, you know, a random thing because Andre Carpathi mostly worked for Elon and
2451760	2456560	some of the software team worked for me and some of them worked for Andre and we missed around.
2456560	2459680	So that's a different question. That's a different question that might freak out that
2460400	2464640	robots or super intelligent computers will impact humanity.
2464640	2469760	No, just these Tesla's, these self-driving cars, right? It seems like whenever you introduce a
2469760	2474080	new technology, somebody's going to have to figure out what's wrong with it. Like airplanes are a
2474080	2477920	good example, right? They're, when they first build these jetliners, they're falling out on the
2477920	2483040	sky all the time, but now they're some of the safest ways to travel on the planet. And I remember
2483040	2487920	there was a few, maybe actually, I don't remember one really bad incident with the Tesla car,
2487920	2493600	but sticking an intersection, a truck pulling out for an intersection, an intersection for
2494240	2498880	white or gray sky or something like that. There's been a couple of problems.
2499920	2506320	No, we reviewed all the, all the fatals and many of the serious accidents every week,
2506320	2511360	literally. Okay, so, so you're familiar with the trolley problem, I presume.
2512000	2516480	Yeah, which is you have, you have a switch and then on one track, there's one person tied to
2516480	2519680	the tracks on the other side, there's five people tied to the tracks, you pull the switch.
2520640	2527040	Yeah, yeah. So Elon deeply believed you pulled the switch, right? Kill less people. So,
2528400	2534240	so he said, oh, he said this publicly, you know, we're going to make cars safer. The only way to
2534240	2539600	do that is autonomous driving and really good, you know, autonomous driving software and computers.
2540560	2548640	And, but, but as it develops, some people will die because of defects in that software,
2549200	2557600	but less people will die, I think. And then he also backed up that commitment by making Tesla,
2557600	2563360	like Tesla had some of the crash analysis software in the world. And then the way they built Tesla
2563360	2568320	was with the battery pack and the floor and the crush frames in the front and the back of the car
2569200	2572640	and made it really safe. And so they were one of the first a five star,
2573440	2576800	you know, crash certification, which is actually mostly a real thing, by the way,
2577440	2581760	like some certifications that I'm skeptical of, but crash safety people are really good.
2582320	2587760	And then the side impact crash and then, and then there's a really wild graph, which, you know, the
2588880	2594480	fatalities versus speed graph. So over 40 miles an hour, you mostly die and under 40 miles an hour,
2594480	2600480	you mostly don't. And so the mission was a, to raise the speed where you mostly don't die
2601200	2607840	and be lower the speed of the car really fast in the event of a detection of an accident.
2609440	2617680	Right. So, yeah, so was it, was it daunting building a safety product? Yes, definitely.
2618320	2623760	Did we analyze it a lot and worry about it? Oh, yeah. So one of the guys who worked for me,
2625040	2634400	there was a radar product we had that mistook a truck for a sign and rejected. So the problem
2634400	2640160	was radar, it's a relatively low resolution and a small object with a right geometry can look
2640160	2647520	like a very big object. And so the radar's problem is false positive. So they reject lots of false
2647600	2654800	positives and it rejected a truck and the person dies. And then when we took the software apart,
2654800	2661040	we figured out why, like the filters, the software using were very good. We rewrote the software so
2661040	2668080	that we would detect that truck properly and some other situations where the radar was not doing
2668080	2673680	that good. How safe are they at this point, these self-driving software as compared to the average
2673680	2682880	driver? I don't know. I don't, I see Waymo and Waymo driving around San Francisco with no people
2682880	2689440	in the cars. And I think they're relatively good. They overkill it really high on sensors and compute.
2690880	2696720	Tesla, Tesla's are not there yet. I have the newest version of self-driving software. It's
2696720	2702480	pretty good. I really like it, but I wouldn't let it, you know, drive to work. You wouldn't take
2702480	2708080	a nap in the backseat. No, I definitely would not take a nap. But it's, it's coming along. And then
2709600	2715120	like Elon's belief is it's going to go in every single car. So it can't cost 50 grand. So the
2715120	2720960	Waymo solution is really expensive. It has many, many sensors and lots and lots of computers and
2721520	2728640	looks like a fighter jet or something. It's got so much gear. But is it possible that the cost
2728640	2734880	of processors is going to go down to the degree that the Waymo system isn't $50,000 anymore in like
2734880	2742480	10 years? Yeah, yeah, 10 years, maybe. Yeah, so there's like two schools. So this is again one of
2742480	2748800	those, you know, technology trends. So sometimes you say, well, I'll build the computer or system
2748800	2754560	big enough to do what I want. And then I'll cost reduce it over time. And then the other is I'll
2754560	2763280	build the solution and the size that I want. Right. And then work to make it, you know, the
2763280	2769040	functionality fit the budget. Right. And if you look at computer technology over a long enough
2769040	2775360	span of time, it definitely started with, we'll build it enormously expensive and enormously
2775360	2780400	large, and then we'll scale it down. Is it just that we've gotten to be sufficiently
2781360	2787600	geared out that we can start working on technological problems from the standpoint of now I have
2787600	2792880	enough components that are small enough and advanced enough that I can build small from the
2792880	2802320	gecko? Yeah, well, so yeah, there's a there's a funny trend line. So like every 10 years, we went
2802320	2809520	from mainframe to mini computer to workstation to PC to mobile. So that was, you know, every 10 years
2809840	2817040	is about 100 times more computing. So on the transistors per year, about 10 x every five years,
2817040	2825280	it's pretty solid graph. And then what happened is, like, like a PC wasn't 100 times faster than
2825280	2831600	mini computer, it was more like 10 times faster, it traded some of the transistor, you know,
2831600	2838480	performance, or just having less of them in a different form factor. So, so if you look if you
2838480	2843760	look at the trends of how many transistors you have versus how many transistors were in the
2843760	2852160	new cheaper product, like the transistor count grew faster than you know, so the budget was slowly
2852160	2858640	going down, which modulated the transistor improvement. The problem is AI, the AI you really
2858640	2866080	want is like a million times more than we have. In terms of driving or just in general, what's that?
2866080	2873120	In terms of self driving technology or just in general? Yeah, for a lot of things. Like if you
2873120	2880720	look at, you know, the operations in a phone chip, like phone chips are 20 bucks, right? And, you know,
2880720	2887680	Nvidia's new GPU is 25,000, right? And some of that's marked up because of, you know, the market
2887680	2893280	position and some of it's a lot of transistors and a lot of heat and performance. So AI was an
2893280	2899600	interesting step function. Like when you went from workstations to PCs to, you know, mobile,
2900960	2906000	like, like the first PCs were obviously inadequate compared to workstations,
2906720	2912080	but the transistor count quickly, you know, sped them up. And then the original phones were
2912080	2917360	inadequate compared to PCs. But so then there's two funny things there. One is
2917920	2923520	whenever you start with a big software on a big computer, the software never gets simpler.
2924240	2928800	So when they went from workstation to software to PC software to mobile software,
2929680	2935680	each time they were rewritten, like PC software is written from scratch and borrowed very little
2935680	2941520	from workstation to your software, right? Now at some point, the PC got fast enough to run
2941520	2946880	workstation software, but there was an evolutionary bottleneck that caused our big rewrite.
2947840	2953280	And then same thing happened with mobile devices. Like the original mobile operating systems were
2953280	2959120	very lean compared to PC operating systems that caused a really big cleanup. So the problem with
2959120	2963520	the Waymo approach is once they write all that software, all those computers and everything,
2964800	2971600	the odds of them, you know, making a 10x simpler by rewriting is kind of low. They're sort of
2971600	2978800	stuck at a higher cost basis. So I'm a little skeptical of the build it big enough to do the
2978800	2985840	job and make it cheap later or hope that or hope that that happens.
2986640	2992640	Did you say that NVIDIA ship was $25,000? Yeah. Who are the customers for that?
2993440	2997440	You know, Microsoft and Google and Bank of America and lots of people.
2998320	3006320	What are they implementing it in? Just in their own software enterprise, like in their own
3006320	3011120	computation back at home base? Oh, yeah, yeah. Right now, like all the big tech companies are
3011120	3015440	in an arm race and they're all calling NVIDIA to get allocation for their new best,
3016000	3022320	but they currently have the best processors by, you know, like functionality and, you know, obvious
3022320	3029280	proof points. Again, Diddy is slowly becoming the IBM of the AI era, I think. And like, we'll see
3029280	3036320	how that goes. Is it general? I run an AI tech company, so I have opinions about it, but I want
3036320	3041840	to hear about that too. But is that generally the case that the first online technology is
3041840	3046240	bought by these enormous corporations? Like you mentioned at the beginning,
3046240	3051440	digitally you said was selling a half a million dollar computer. And I'm thinking who's buying that?
3052640	3057520	Yeah. Oh, like a lot of people. So when digital started, they made logic boards. So they made
3057520	3062400	little boards as big with transistors on them. And they had products called like AND gate and
3062400	3069680	OR gate and right. And then they worked themselves up to making, I forget the original number, but
3070800	3076880	you know, they were making computers in, you know, one to 10,000 zone that were,
3077840	3084000	you know, slower than IBM's, but, you know, 10 times cheaper. And people loved them. And then
3084000	3092320	they built the 1144, which is probably 10 to 50,000. And then the 1170, which is a little more
3092320	3100800	expensive. And then the Vax 80, 100, or the Vax 780 was a one million instruction per second
3100800	3108080	computer. It sold for like 100,000 to $200,000. And they sold, you know, for the time, a lot of them.
3109120	3115920	And then the Vax 80, 100 was faster. And the cheapest one was over like 150,000. And then the
3115920	3123920	high end was $5,600,000. And they sold them to banks, businesses, all kinds of people.
3124400	3130320	Was the military a customer as well? Yeah, the military bottom, it was all over the place.
3131040	3136160	But at the time, the half million, the high end Vax computer for half a million dollars was a great
3136160	3142560	deal compared to the IBM solutions. And, you know, they were selling a lot of them, but
3143280	3147200	then they were planning like a computer after that was even more expensive.
3147760	3153440	And so this is another anomaly that happens is if you like the high end technology trends
3154000	3158400	that you'll see in like Forbes or Business Review or something shows this falling price
3158400	3166160	of computer. But generally speaking, in each kind of epoch of computing, the leaders slowly get more
3166160	3171600	expensive. Like Sun started, you know, $3,000 workstation and when they went out of business,
3171600	3180000	they were selling $100,000 servers. So there's a funny thing, like it's sort of like the Ford
3180000	3186160	Mustang. When the Ford Mustang came out and was 65, it was this beautiful little sports car by 72,
3186240	3191360	it was just bloated, you know, because everything everybody's idea maybe a little bit bigger,
3191360	3196080	let's put a little bit bigger engine, a little bit bigger tire, we'll make the back seats a little
3196080	3201360	bigger, we'll make it a little wider. So elbow room and what they, you know, they killed what
3201360	3207600	they loved about it. And that's a very common phenomenon. So human beings are phenomenally
3207600	3213520	good at this kind of stuff. So interesting, though, because on some level from the consumer's
3213520	3220880	standpoint, the 65 Mustang might have been a perfect car. Yes, it didn't mean anything.
3220880	3226480	And you could still be selling 65 Mustangs today, and they would be going like hotcakes because they
3226480	3234640	have the aesthetics, the vibes, the feel, whatever. And so it seems like the process of technological
3234640	3242320	development in this market system works directly against the person who just wants a thing
3243280	3249040	that looks like the sweet spot that got hit. Because I'm like, is the iPhone 15
3249760	3256720	significantly better than the iPhone 6? No. Right? And because Shiloh is still
3256720	3260160	probably the owner of iPhone 6. So that's the division return curves. And then people,
3261120	3267600	though the designers, the marketing people are all looking for something different. Because they,
3267600	3272240	you know, somewhere in their pointy little heads are like, if we just keep making 65 Mustangs,
3272240	3277040	people will maintain them and nobody will buy a new one. But if there's a new thing you have to
3277040	3282800	have, so why do you buy a car, especially a sports car? It's partly for fun, partly transportation,
3282800	3289600	partly for status, partly for new, partly you got bored, partly who knows, it's a complicated
3290480	3296960	thing. So yeah, this is where I mean, like people like underanalyze everything. So
3297920	3305040	so we just described, you know, at least a dozen frameworks, right? And so just all 12 equations
3305040	3311840	with 12 unknowns, you need 12 equations. Like, you can't, you can't cherry pick a point and then
3313040	3317440	get anywhere. So if you want to make new technology yet, it's good to be aware of all
3317440	3322560	these different phenomena and then you have to think about it. But you can also overthink it
3322560	3327440	and be over constrained by the whole thing. Okay, so yeah, I would, I would have loved
3327440	3333200	to have much improved. Now, some of the Mustang growth was because they started rolling in new
3333200	3337840	crash standards, which the old Mustang couldn't do. And then there was all kinds of wild emission
3337840	3343760	stuff with which they tried to build emissions in the cars before they had the computer controls
3343760	3349920	and sensors to do it. So nowadays, you could make a Mustang that big, that's way safer because we
3349920	3355360	have the crash software and very efficient and relatively, you know, low on emissions because
3355360	3361680	we have the sensors and the computer control systems and the high pressure pumps to do fuel
3361680	3367920	injection properly. Like, it's a, you know, today we saw those problems with high end technology.
3368960	3373760	Like modern computer controlled motors are amazing. What's that? But like Ford's not making,
3373760	3376880	that's the thing that's really strange to me. Is that? Oh, yeah, yeah. So then the question is,
3376960	3380160	why not to go back and make the great car with all the technology they have?
3381440	3382000	I don't know.
3382000	3385680	Like it's hard to actually just do it. It's really actually a really funny trend right now. It's
3385680	3390000	like reissuing, you know, the 54 strad or something like that. And they're selling really well
3390000	3394800	actually, because they just couldn't improve. And they're literally rewinding all the coils
3394800	3399280	to spec and everything is like, exactly how it was. And people are just loving it.
3399280	3400560	Is it all exactly because
3401440	3405520	I mean, they're very, they're so expensive. They're high, like they're, you know,
3405520	3410240	high end American products with, compared to some of their cheaper lines and stuff.
3410240	3414480	I think that they really do go after the original as much as possible from what I can tell.
3415040	3418800	And so it's an interesting strategy. I think the problem is the safety standards on those
3418800	3424720	old Mustang searches. Like if you've ever driven an old 60s, 70s car, they're like the
3424720	3430640	suspension's horrible. They have no pickup compared to what you're used to. They're just not
3430640	3436560	actually good cars is the problem. Yeah, but you could make a car that looked and felt like
3436560	3442960	the old Mustang that actually was safe, but it would be the opposite of like a reissued fender.
3445200	3451040	You know, the reissued fender, you could make exactly the same guitar and somebody would love it.
3451120	3459760	Whereas the reissued Mustang wouldn't look like even the metallurgy of the steel would be
3459760	3464480	completely different and it would be so much better. And this is something that's really
3464480	3471040	interesting to me because I'm like, okay, so Ford, for whatever reason, doesn't reissue the 65
3471040	3478160	Mustang, like that's perplexing. But there are people that may smaller car manufacturers
3478160	3484080	that would be happy to reissue a 65 Mustang with all of the updates, but they can't because of
3484080	3491440	intellectual property. Like Ford in perpetuity owns the... No, they don't. It's mostly you could
3491440	3498480	probably copy it. There's a difference in copyright law and patent law on this one.
3498480	3504320	I don't know what's the intellectual property is. I was going to introduce a new car that's
3504320	3507760	you were talking to Warren Mosler, he tried to do that, right? He built this car that was
3508480	3512160	way better than all the competitors and it was just impossible to bring it to market.
3512960	3516400	He seemed perplexed by even what had happened to him, but it seemed like it was
3516400	3521600	really held down by a kind of old club. It's very difficult to edge into that.
3521600	3526080	Okay, so fine. Like I think that the way that trademark law works is that if
3527680	3533840	Ford owns the trademark to the Mustang and the design of the car is not functional,
3533840	3540000	then nobody else can copy the look of the car because the only things that aren't protected by
3540000	3547120	trademark are things that are not... That are functional. Yes, sorry, that's a double negative.
3547120	3554880	Trademark protects non-functional appearance-based things. So the look of a car is protected by
3554880	3558800	the trademark that they own over it unless somebody can prove that the look of the car is
3558800	3563040	functional in which case trade dress doesn't cover it. But let's think about something
3563040	3569360	that's more recent, like the iPhone, right? So Apple doesn't make iPhone 6 as big. So if you
3569360	3574000	want an iPhone 6, you have to get one that's old and they're manufactured in such a way that
3574000	3578480	they're hard to repair. And so by the time that you get an iPhone 6, like none of the operating
3578480	3583600	system works on it anymore, it's very slow, like it's buggy, you can't really repair the components
3583600	3591840	easily. Is there some space to separate out the companies that generate the ideas and then the
3591840	3597680	people that implement them so that somebody who like Apple comes up with the idea for the iPhone,
3599280	3605040	then allows other people to make iPhones because they've moved on from inventing the
3605040	3611280	iPhone to inventing something else? Or is that just a complete socialist type of dream?
3612400	3617280	Well, some of this just happens over time, but it's longer time frame. So
3618160	3626080	so this is just my belief from history. 100% of the current companies will all become defunct.
3627120	3633760	Somewhere in the next 10 to 50 years. Like there's like essentially zero companies left from 100
3633760	3640400	years ago. I think General Electric is one of the few. So all the fortune one. Yeah.
3640880	3647440	There's the biggest companies during the expansion westward were all windmill manufacturing
3647440	3657120	companies. Yeah. Yeah. And then there was the oil and oil steel railroads. They were the high tech.
3657120	3664160	They were the booming startups in the mid 1800s, early to mid 1800s. And then they were the
3665120	3672320	you know, all the copies that ran the United States and literally lent money to the U.S.
3672320	3678160	government to keep the float. And you can imagine how I was shifting that whole thing was, they
3678160	3682560	literally called them the robber parents, you know, but then their descendants created all
3682560	3687760	these foundations. So now people go to the Ford Foundation and Carnegie Mellon University. And
3688560	3693280	you know, but those companies are 100% gone. And that cycle will repeat
3694080	3699280	inevitably. And then some of that will create space for people to make new phones and stuff.
3699280	3704400	But some of those devices, some of the devices, you know, like nobody really wants an iPhone 6
3704400	3710000	because an iPhone, you know, 12 is good enough and cheap enough and serves the need. And, you know,
3710000	3716640	the we're transitioning to like a different kind of computer in 10 years will be AI computers that
3716640	3722720	we deal with. And the question is, you know, who owns and who controls it? How do you build them?
3722720	3726960	How many companies are there? Who are the players? And it's going to be kind of wild.
3729520	3734880	I don't think it'll be many of the current big players. And some of the current funded startups
3734880	3739600	may be the ones, but some of the numbers that lead to next generation, you might not have heard
3739920	3747840	yet. So this is again, this is another one. Sorry. I'm just curious if you think this is a
3747840	3752480	fixed law of the universe, the lifespan of these companies, because it seems like bureaucracies
3752480	3756560	can survive for a very long time. I know there's some Chinese bureaucracies that last almost a
3756560	3762560	thousand years. I think it's the Chao dynasty. But it doesn't seem like necessarily the organizational
3762560	3768960	structure is dated, right? It doesn't have an expiration date as much as something else about
3768960	3775680	their inability to adapt to the needs of new customers. What is that? Is it possible?
3777440	3781920	Well, I don't know if you want to, well, like the first step is understanding also, like,
3782720	3787280	like, so business bureaucracies have self limits, because these companies become bureaucratic,
3787280	3792880	they don't respond to the market, they consume all their revenue for, you know, nonsense. And
3792880	3798640	they go bankrupt, right? So then there's these quasi businesses like utilities, which are horribly
3798640	3804320	run, or big companies like Ford and Chrysler who have such big unions and political clout
3805040	3808720	that they got bailed out by the government despite the fact that they probably should have been
3808720	3814560	bankrupt. Now, they will tell you that they paid back the loans. And I'm again, I'm not an expert,
3814560	3820960	maybe they did, maybe it was great. And but then government bureaucracies are a little funnier,
3820960	3827120	because, you know, they're not funded by their success, they're funded by taxes and oddly enough,
3827200	3834560	the worst they do, the more they spend and the more they tax. And so until it starts,
3834560	3838000	it's having such a big impact that there's some either kind of internal political
3838720	3844240	blush, or in fact, they failed so badly that, you know, they lose a war, like historically big
3844240	3850880	corrupt countries, you know, didn't end well as it were. But those cycles can be longer. And then
3850880	3855920	there's some bureaucracies, like there's some Japanese companies that are hundreds of years old.
3857360	3861440	Some of them are successful. And but some of those are family businesses where the
3862560	3868400	families escaped, you know, then the in the Western world, like family money only lasts like
3868400	3874880	two or three generations. You know, like, like, you know, the up and comer guy who made the money,
3874880	3881200	his his descendants do very well, if they keep growing the business, but at some point, they
3881200	3886160	start dividing it up. And the children are raised in luxury, and they don't try very hard. And then
3886800	3891120	some of those families, families, they go bankrupt quickly. Like is that just a cultural
3891120	3897760	difference? East, West? Well, I suspect most rich families in Asia also go bankrupt the same way
3897760	3902640	as over time. So but these ones, I'm not enough of an expert by just like, there's some famous
3902640	3906800	families in Europe that are also hundreds of years old. That's true. That's true. And
3907600	3913440	but if I remember correctly, these Japanese companies are not like big tech companies. They're
3913440	3921920	like a pub that's been run by the same family for like 1000 years. Really? Yeah, like some of them
3921920	3927520	are. Yeah. Yeah, there's I think that the oldest company in that I know of continuously independent,
3927520	3933440	I think it's Nintendo. What's that? Nintendo? Yeah, I mean, yeah, yeah, Nintendo was definitely,
3933440	3940240	it was not a handheld game company like 300 years ago. So yeah. So I think it became very famous
3940240	3952080	for that. And then 1889. What's that? Nintendo was founded in 1889. Play cards. Yeah. And was it
3954240	3960560	Nokia was a rubber company. You know, they became famous for phones. So sometimes they
3960560	3965360	transition, but you know, it's so again, to the socialist point,
3965600	3972640	like, I don't know, people work really hard when they see personal benefit and
3974480	3978800	personal goals. Like most people don't work for money. I don't know anybody that works.
3978800	3983360	Like once you're successful in high tech, you're not working for money. You're working for goals,
3983360	3989600	for interest. You know, somebody said once you have $100 million, you know, you buy a couple
3989600	3995280	houses in a ranch and a boat and a plane. And after that, like, one of the really rich people
3995280	4001680	work for money. And most of them, you know, it's sort of like the money exists, but it's more
4001680	4009520	like stewardship of an asset, right? Like people who have assets invested or they make decisions
4009520	4015280	about it. They're, you know, it's a complicated thing. And then the question when you have assets,
4015280	4021840	who should use them? And what does it even mean? Like, if you're a big shareholder in Tesla today,
4021840	4026880	what do you own? Well, you own the production capacity of employees of 100,000 people around
4026880	4033840	the world making millions of cars. And you mostly don't just spend that money because it's, if you
4033840	4037840	sell your Tesla shares to spend the money, you don't own Tesla anymore. And if you own Tesla shares,
4038560	4044400	your money is in a factory that's making cars, you know, so it's a, you know, like what does asset
4044400	4051600	ownership mean? It's a complicated thing. And, you know, generally speaking, you know, there's
4051600	4057120	producers in the world and consumers, and the consumers mostly don't produce anything.
4058400	4065040	Seems like there's a flaw in the design of these corporations because the
4066560	4070960	arm of the corporation that's invested in making money and selling stuff,
4071840	4078080	sometimes works at odds with the department that is innovating and coming up with new ideas.
4078080	4083760	A new idea is always less profitable than an old idea that you could just iterate and sell a little
4083760	4090720	bit. Yeah, I know. And so it's called creative tension, I think that's flaw. Creative tension?
4092400	4096240	Yeah, the creative tension between should you produce what's making money now,
4097040	4103120	or should you invest in something new? And, and by the way, companies go bankrupt on both rails,
4103840	4110000	so they don't make the new product and then nobody cares about them, right? You know,
4110000	4115360	blockberry, like who cares about touchscreens? That didn't work out so well. And then other
4115360	4120000	people invest in all kinds of new stuff and it doesn't work out. And you kind of look back and
4120000	4127440	you think if they just kept making comfortable shoes, they'd been fine. And so the, you know,
4127440	4131520	it's sort of like the same graph of like what percentage of humans have descendants after
4131520	4138640	10 generations? The answer is not very many. What, what percentage of corporations are successful
4138640	4146160	after 100 years, you know, essentially 1% or something. But that's because we live in a very
4146160	4151440	experimental universe. Like the number of things that can go wrong is like essentially infinite.
4151440	4159200	And the percentage of things that could go right is relatively low and often very dependent on
4159200	4166320	luck in the current state. You know, so like the iPhone five years earlier have been done. As a
4166320	4170960	matter of fact, they tried it a couple of times and the technology wasn't there. But when they
4171040	4178000	finally did it, it was pretty good. And it seems like that might be what, and there's some inverse
4178000	4183760	of that, which is that when the winds shift and you have a model that works, you're kind of screwed.
4183760	4189200	Like we were talking earlier about Google being an ad serving business. At the end of the day,
4189200	4195040	the goal is to get you to look at ads and those ads are having like their money. But with generative
4195120	4201680	AI, you can go ask the same, the same question to chat with the tea or cloud or whoever it is
4201680	4207120	that you're using. And there's no mechanism for serving you ads through that. Oh, there already
4207120	4212800	is. There is. I have friends they're doing, they're sending me screenshots of they did a search and
4213360	4221920	the search said, Oh, you might be in this article in the Atlantic or so the, the AI models are fine
4221920	4228960	tuned with their advertisers data. So no, don't worry about that. Even worse, like movies and
4228960	4234560	everything. So when you're watching a movie, even worse, even better, hard to say comes in your
4234560	4240320	perspective. So you know how the, you know, Coca Cola famously paid, you know, Marvel Studios,
4240320	4247680	all this money to put a coke can on a table. Well, that only works for a certain segment of
4247680	4252080	the audience. Like some other segment, I guess, wants to see a can of, you know, pork and beans.
4253360	4261520	So all the AI generated video will, will gain the environment to affect your perceptions.
4262160	4265840	Also, like we'll be watching different movies, essentially. Yeah, yeah, everybody will be watching
4265840	4271200	different movies. Taylor to my interests, might, might have all those fun areas she gets. Well,
4271200	4275360	tell them some, some combination of your interest and what somebody wants you to think.
4276240	4280880	Oh, it's really interesting because I'm trying to think of like Iron Man and having a can of pork
4280880	4286240	and beans on his table instead of having a can of Coca Cola. And there's just something so strange
4286240	4291520	about that. But I suppose, yeah, there's a 7.4% target market that would have loved that pork and
4291520	4298880	beans. Like, you can't be, can't be cake, Coca Cola, you know, like bias, you have to,
4299600	4301680	have to be open, the pork and beans experience.
4305200	4311920	Okay, but so I have, I have like an overarching question about where this is all going.
4312480	4317600	Yeah, I'm curious about still about the lifespan feature. Do, how much
4318640	4322880	fallout is there in the death of a company? Like in the bankruptcy phase, how is that
4322880	4328720	devastating beyond just the people who are working there? And is there anything like an end of life
4329040	4333520	plan for companies? Or is this all sort of surprising when it happens, even though it's
4333520	4340880	completely inevitable? Finder that people pull out, which is like in case of the end, do this.
4340880	4347200	Yeah, yeah, for most companies, well, like I said, the, you know, the, the truism is they went
4347200	4352800	bankrupt slowly at first, but then all at once. And, and lots of times people are very surprised.
4353520	4359360	Now, so there's the question of what, what if anything should be done about that?
4360080	4366800	So like most companies pivoted, that used to be you work for a company for 30 years and retired
4366800	4372960	and got your pension from the company, right? And that like, now that's 100% over. All your
4372960	4380080	pensions are what's called portable pensions and earned 401ks or IRAs. And so as you work there,
4380080	4385840	you, you invest, and maybe the company is, but it's a great company invests in your retirement
4385840	4392240	fund, which you own. So that's good. And then depending on, you know, dumb luck and everything
4392240	4398720	else that may or may not grow at the stock market. So that's, that's a thing. Some companies get
4398720	4403760	propped up because it's like too big to fail. Like, but then, then you have the opposite effect,
4403760	4409200	which is what like, why do we prop up big banks and financial institutions? Because it didn't
4409200	4414880	make them better companies basically taught them the valuable lesson that financial management was,
4414880	4422320	you know, for babies and, and, you know, they can get away with anything. And so those, and that's
4422320	4427520	where you get into the, you know, complex relationship between investors and companies and
4427520	4433200	governments and regulators and revolving doors between regulators and companies. And, you know,
4433200	4440720	my personal view is it would be better if companies failed more often. And, you know,
4440720	4445600	you could imagine a regulation that say the life cycle of a government organization should be more
4445600	4451840	like 30 years, because then you could go through a 10 year growth phase and a 20 year, you know,
4451840	4458240	productivity phase and then get shut down before bureaucracy takes over 100% of your output.
4458240	4465600	Because, you know, to your question was like, if you just said what percentage of our output is
4465600	4472960	wasted on our bureaucracy, and it, it, it slowly grows. And then 100 years for most companies,
4472960	4481200	it's over 100%. So the bureaucracy taxes, you know, a minimum of a percent a year. And at
4481200	4485360	certain phases that goes up faster, and then occasionally gets reset, like Microsoft was
4486160	4491600	clearly, you know, reinvented for a whole bunch of resuppos, clearly.
4491600	4496320	Is that reset possible with the governments or with an academic institution? Because these
4496320	4501520	are the real big bureaucracies today, as far as I can tell. Yeah. So universities used to be reset
4501520	4507840	all the time because they were funded by their endowments, their patrons and their students,
4507840	4513120	but now they're funded by the government. So they're, you know, so they've been
4513120	4519520	detached from the market cycle at some level. Now some, some universities, I have a couple
4519520	4525440	of friends I've been teaching for a long time, and they get cuts every year to the professors.
4526720	4532400	And while the bureaucracy continues to grow, so they used to, their budgets used to be 10%
4532400	4540240	bureaucracy, 10% facilities, and 80% professor salaries. And now it's like 40% bureaucracy
4540320	4548960	and 30% facilities and 20% percent professor overhead, professor of overhead. And, you know,
4548960	4552640	like it's amazing. They take half those grants, right? Half those government grants that
4552640	4559040	I've been trying to set for this kind of grant, they get 50%. This kind of grant, they get 60%.
4561360	4562880	And then they can, and then, yeah.
4562880	4567760	So any hope there? I mean, you imagine a reset that's possible in such a
4568240	4571280	there's, there's a whole bunch of people trying to invent something outside the current
4571280	4576800	university structure. Alternative institutions. Yeah. So, but they still have an alternative
4576800	4583200	government though. Yeah. Well, now governments have gone through a really big reset. You know,
4583200	4588320	the United States government has gone through several major resets. Like the Civil War.
4589520	4593120	Oh, so you guys are too young. So I have a friend who's very worried about the current
4593120	4600240	government situation. So when I was a kid, you know, they were, they were taking young men
4600240	4605040	at essentially gunpoint and sending them to Vietnam to kill people for, you know, no good reason.
4605920	4613280	And then the National Guard shot students on campus. And then the police department beat up
4613280	4618160	protesters at the Democratic National Convention on television. Like these things happen.
4618160	4624400	And the Democratic Party lost brutally for multiple
4625680	4629680	elections until Bill Clinton basically reinvented the Democratic Party.
4630560	4637920	A really big reset. Like it happened. But then they became bureaucratic just the way,
4639040	4643760	you know, the modern parties are basically in the same boat they were. And the Reagan
4643760	4649760	revolution was somewhat real. You know, the, you know, Nixon, Nixon got taken out for a bunch
4649760	4656960	of complicated reasons. And Ford was a loser. And, you know, like it was quite a turnover in the 60s
4656960	4663520	and 70s. And then things have to get bad enough. And then the pendulum swing. Yeah. Yeah. And
4663520	4669200	there's lots of people who study like the 20 year cycles and 60 and 80, 100 and 200.
4669920	4672720	They were clearly going through one of those cycles. Like,
4673840	4676320	like it's hard to believe like who's running for president now.
4677600	4678880	The year without an election.
4679760	4684800	Yeah. Yeah. Something that, and that's probably what people will say. And then people look back
4684800	4691040	and go, well, how did they let that happen? But when I was kid, they taught us about yellow
4691040	4696480	journalism and the robber barons and the Great Depression. And, you know,
4697280	4704320	like we went through multiple cycles, like, literally since 1850, that I was taught in detail
4704320	4709760	about. And, you know, like we're in another one of those cycles, which will be in the history books.
4709760	4717280	And there'll be, you know, the great deficit and the great election, something or other,
4717280	4721280	who knows what they'll call it. And like these cycles keep happening.
4721520	4725520	So, so the big difference, and this, this took me a while.
4726960	4729600	I was talking to my friend, because he's very anxious, you know, he's younger,
4730640	4734640	you know, 30 something. And I think everybody has that, by the way.
4735200	4739200	What's that? I think every single person listening probably has at least one friend that's
4739200	4742960	in a similar position with respect to the government situation right now.
4742960	4749520	Yeah. Yeah. So it took me a lot of like, when, when I was going through that and watching television,
4749520	4752880	I was a young person. And, but, you know, there was a threat that I could be sent to
4752880	4758240	Vietnam. And my parents were anti-war protesters. And there was literally, you know,
4758240	4763360	also civil rights activists, which is why, because my father was a Reagan Republican,
4763360	4769520	but, but it was a moral statement, not a left wing statement. And he had friends that literally
4769520	4775280	got beat by police in Philadelphia. So, like, like there was a real threat. And we didn't know how
4775280	4780560	it was going to go. Like we sat around the kitchen table, and my parents would pray
4781200	4786560	that we would live through this. Like that was my childhood. But we did.
4788480	4793200	Now it could be, now I have a, well, you can live through almost anything attitude,
4793200	4798480	which would be wrong. It was, you know, that was pretty threatening. And, you know, it was,
4799680	4803840	you know, there was like, when we were kids, we used to train for atomic fallout by
4803840	4809120	halving under the desks. I was six years old, and we're, you know, we're seeing the six-year-old
4809120	4812720	desk at the top of the desk is about this big. And we would get underneath it to protect us
4812720	4819360	from fallout, which we had no idea what that was. But there was air raid sirens and, you know,
4820480	4823840	you know, running the classroom and hide under these little tiny desks.
4825040	4826880	That was fairly, fairly daunting.
4828160	4833200	But people have lived through so many hard things. Like my mom will tell me stories of
4833840	4841520	the arc of our family in the Soviet Union. And it's just insane, right? Like Russian Jews
4841520	4850800	in 1920 that are, that are running from the Red Army that are, you know, and then like her entire
4850800	4857040	branch of the family left St. Petersburg and they went into exile in Siberia. And then they ended
4857040	4862240	up joining one of these communes and the communes, they took all your passports so you could never
4862240	4870160	leave. And so like the husband of the family was either killed or arrested. And it was this
4870160	4875600	woman with eight children, no food. She can't leave because she doesn't have any of her papers.
4875600	4881600	And she manages like her dad would tell her stories of they would gather wild hemp seeds
4881600	4885840	and squeeze them for oil. And that was the only fat that they would be able to get.
4886800	4891760	And yet they made it, right? Like my parents live in California now. And the memories of
4891760	4895920	everything that they went through have kind of faded into the background. And so I developed
4895920	4901600	this feeling about the political shifts, which is like, yeah, they happen, but life exists
4903600	4910240	on the foreground. While all of that stuff happens sort of in some ways beyond our control,
4910240	4916080	like these are systems that run. And our task is to figure out how to do something meaningful
4916080	4919360	and worthwhile, despite the fact that the background is so insane.
4920000	4926240	So this gets to the philosophical point, which is, you know, these cycles exist.
4927280	4935680	So the, you know, the Nassim Talib is like, the harder you work to keep it frozen, the worse the
4935680	4942880	end is, you know, you know, the long tail event, like in the financial world, trying to regulate
4942880	4949760	the system to avoid crashes makes the crash worse. Now, may in fact put it off. And then there's,
4949760	4955680	you know, the creative tension between, should you crash early enough and, and then deal with it
4955680	4962560	all the time, but then you have smaller ones, or should you try to build a robust financial system
4963440	4967120	that, you know, avoids that. And the same with political systems, like
4968000	4971760	United States for the most part has had a pretty dynamic political system, but
4972720	4980160	you know, the current alternative seem, you know, there's a funny phenomena, like
4981040	4985840	in a negotiation between multiple parties with the stake in the outcome, you compromise towards
4985840	4991440	the middle, but in bubbles of, you know, like-minded people, it tends to, let's say,
4991520	4998000	negotiate towards the edges, you know, I'm the most virtuous, no, I'm the most virtuous, no, you
4998000	5004480	know, and, and so that's a, that's an interesting dynamic, which also plays out organizationally,
5004480	5012160	like when, when there's a proper negotiation of the parties, something interesting happens.
5012960	5018640	But when there's a, you know, autocratic ruler makes stupid decisions, or I got to describe
5019600	5023600	this business group, they were making so much money, nobody wanted to call them on it,
5023600	5026320	but their decisions were actually killing the company as a whole.
5027360	5033360	And, you know, these, these, these systems happen at multiple, like I have a belief that
5033360	5038800	these systems play out at multiple levels. So, you know, like how family dynamics work
5038800	5044080	with a small number of people, looks a whole lot like a small team dynamics work, which looks a
5044080	5050000	whole lot like extended family dynamics work. And that's because we're humans. Now, if we weren't
5050000	5056400	humans, say we were beings with a thousand life cycle and we didn't have families and,
5057120	5061840	you know, we, we could keep track of like a thousand numbers in our head instead of seven.
5062800	5066080	Would our dynamics play out differently? I think they'd be a lot different.
5067360	5072080	But our dynamics that we live and play out the way they do because of our biological,
5072080	5079280	you know, grounding and our evolutionary process. And we co-evolved with our culture,
5079280	5084880	which gave us mental capacities for a whole bunch of things. We co-evolved with our lifespan,
5084880	5092400	which gives us, you know, let's say expectations and pretty deeply ground ideas about what to do.
5096080	5099520	And you have to understand like 10 levels of that to make any sense out of anything.
5100240	5103520	And so there's something that's really crazy that's happening right now,
5103520	5108640	which I think has been a continuous project since the invention of the first computers,
5109360	5117600	which is to create a fundamental shift in the biology of humans. Like I don't necessarily
5117600	5122800	think that the first people who sat around and were asking how to build a computer had this in
5122800	5132640	line. But I see it as this emergent step in evolution, where we had the dream of being able
5132640	5138800	to make a machine that could think. And the first generations of those machines were only able to
5138800	5145760	think in very basic ways, according to the way that we programmed them. And then progressively,
5145760	5151840	we increase the speed with which they can think the number of complex processes that they can run.
5151840	5154880	But still, you're limited by the fact that it is, you know,
5156800	5162960	garbage in, garbage out. If you don't know how to work the machine, you can't make it go.
5164560	5174640	But as we develop tools that are intelligent, and we start to gradually mount the computational
5174640	5181280	power of computers with the computational power of humans, and then we also start to
5181280	5189680	manipulate the lifespan of humans in order to extend it, what effect is that going to have?
5189680	5196160	Because it seems like it's this project that we're running full throttle into. And obviously,
5196160	5201200	you do, right? We build tools, that's what humans do. But I feel like there's something
5201200	5207200	that's fundamentally different about the tool that is the steam engine and the tool that is AI
5208080	5211360	and bioengineering combined with AI.
5211360	5218080	Yeah. Let me try to take, yeah, there's about a couple of different ideas there that I think
5218960	5226080	it's interesting to take it apart. So starting with your last one. So we made a whole bunch of
5226080	5234400	tools which didn't really, or did only slowly re-engineer our biology, but they re-engineered
5234400	5242080	our culture. So all over the world, human beings live in, you know, aboriginal humans live in tribes
5242080	5249600	of 100 to 200 adults. And that's a human constant. As best I can tell, I've read a number of books
5249600	5257040	about it. And so our ape relatives Gibbons are famously monogamous, and orangutans are solitary
5257040	5262720	territorial with some really interesting sideshows, and gorillas are a dominant male with a harem,
5263360	5269280	and chimpanzees are a dominant group of adults, you know, three to five males and three to five
5269280	5277200	females in a group of 30 to 40. They can go from a solitary animal to a pair-bonded animal to a
5277200	5286320	harem animal to 30 to 40 to 100 to 200. So that's our organic basis. But with tools, you know,
5287280	5294480	you know, and who knows which one drove it? Was it farming? Was it herding? Was it bow and arrows?
5294480	5303840	Was it whatever? We used our tools to engineer culture and society. And then essentially,
5303840	5307760	the person that could keep the biggest village together could win against the other villages.
5307760	5314720	And then at some point, that also caused, let's say, a massive escalation or political abilities,
5314720	5318960	because if you want to trade with that village and get rich or go to war with them and get killed,
5318960	5326960	like there's, you know, so our tools, what we co-evolve with our tools and our culture, right?
5326960	5332160	And some cultures are way more effective than other ones that are making new tools. And but,
5332160	5338240	you know, that's also a very chaotic function. And so we're making new tools, which is going to
5338240	5344560	cause some additional cultural change. And it's already doing it in some of that good ways and
5344560	5351120	bad ways. Like the internet is full of people speculating about it. But, you know, we've gone
5351120	5357520	through a lot of evolution. Now, in computing, computers don't think, right? And there's a
5357520	5366000	funny thing. So the first computers, you know, executed sets of statements, which, you know,
5366000	5370400	you could do with a pencil and paper is just, you know, when they do a billion instructions a
5370400	5377840	second, it's a little faster than, you know, one a second. But that's a matter of quantity, right?
5379280	5386880	And then, like, so the statement kind of computer, and then we went into, you know,
5386880	5392560	there was the big data revolution and analysis, where you could run so many programs that cost
5392560	5398000	data, you could find signal where nobody could. And it's almost like we got into pattern recognition.
5398000	5403600	So you went from executing statements to something that seemed to do something different,
5404560	5410480	like pattern recognition. And then the early AI programs, you know, like the famous Alexnet,
5410480	5415200	which could recognize a cat in a photo. Like that's kind of a wild thing. But,
5416160	5422000	and it was way better than what's called classic vision, which attempted to find cats by correlation.
5422000	5425760	They would say a cat is a collection of pointy ears and round eyes and fuzzy hair,
5426480	5432880	and cute smile or something. And then you could have a database of when you see these parameters,
5432880	5438080	these features with these ratios, it's a cat versus these features, it's a dog versus,
5438720	5444640	you know, the problem is or something. So, so that, you know, the statements to, you know,
5444640	5449120	like analysis, and then recognition. And then the language model,
5450320	5455120	you know, people are very puzzled by this because they're sort of stupidly good at chit chat.
5456080	5462080	And summarizing things and predicting the next word. And they do something that seems almost
5462080	5470640	amazing. Like you can think of having multiple equations like, you know, x plus y plus 3z equals
5470640	5477840	five and two x plus seven y plus z squared equals seven. So those are equations. And then you can
5477840	5484000	do algebraic processes that to do something with, but you can't add the word cat and dog.
5484800	5491440	Like cat plus dog doesn't mean anything. Does that make sense? But you can translate
5491440	5495600	cat and dog into a space where you could add them up.
5497920	5505120	Right. And then you could query a statement against the words. And it turns out with enough
5505120	5512320	computation, you can create really called embedding. And then there's all kinds of transformations
5512320	5517520	you do on that data, where you essentially you've turned ideas into mathematical things. And I know
5517520	5523120	they're mathematical, but I can actually look inside the computer chip and look at the register
5523120	5528080	that has the bits in it that I'm adding. But there's no magic in AI computers.
5529600	5537280	They're just numbers being added up. But like the representation of the word cat and dog isn't
5537280	5544720	a very good representation for manipulation, let's say. So some modern computers, we can do
5544720	5551360	something really interesting, which is with a set of words, predict the next good word.
5552320	5558080	Or with two sets of ideas, put them together and then summarize that. It's kind of amazing.
5558720	5565040	And it seems relatively intelligent. Because in fact, the current state of play is
5566000	5571760	it's ability to write, summarize, analyze and talk to you is better than the average person.
5572960	5574240	But it's not clear it's thinking.
5575680	5578240	Yeah, I was going to say, how do you define intelligence and thought?
5579040	5584320	Yeah. So you know the joke, every time intelligence can do something, it stops being AI.
5584960	5590320	So like, surely it's AI when you can play chess. But then we wrote a program that clearly wasn't
5590400	5596960	intelligent play chess, but go. That's so hard. You have to be intelligent. No, that was just a
5596960	5603360	program to like to recognize generally any object in a picture, you must be intelligent.
5604080	5609360	No, that was easy. That happened 10 years ago. Right. To complete this sentence, that must be
5609360	5616080	intelligence. Nope. The turning test. I talked to you for 10 minutes and I can't tell it's
5616720	5622240	not a person. That's been passed. The bar exam. That must be artificial intelligence.
5622960	5629520	Now that was easy. Like so. So we don't, so what's happening is intelligence is being defined by
5629520	5636960	what it is, which is crazy. Right. And then today people say, well, the AI models are no smarter
5636960	5644800	than the input that they we've had it. Well, human beings aren't data and intelligence.
5645600	5650800	Like Christ, the average person barely gets 100 megabytes of data. And by the time they can talk
5650800	5656960	and do smart things, like work where I would call computational intelligence. So, and that
5656960	5663840	that means some of it's trained in a way that like builds these simple spaces and representational
5663840	5671040	things. But then we can experiment experimental, right? That's yeah, yeah, we can do variations on
5671040	5675680	that and combine them over and over. But it turns out the variations that are being done,
5677040	5683920	large magical, their variations in the representational space now turns out those
5683920	5689520	representational spaces have infinite variation possibilities or very large variation possibilities.
5690400	5697680	And we do a funny thing where we're very good at making small variations and then filtering
5697680	5704640	for some sensibility and then combining filters of those variations for something bigger. And
5704640	5709680	depending on how smart we are and what we learned and what, you know, let's say sense, you know,
5711040	5715920	make sense of the constructs we have, you could be more or less successful at this.
5716800	5719440	Like, have you ever watched like Richard Feynman lectures on math?
5720480	5724720	Like, you're super fun because he goes, well, this is obvious, this is obvious. And he's just
5724720	5732800	hopping across the the trail like 100 yards at a time because his ability to produce variations
5732800	5740080	were really good. And his ability to make sense out of the possibilities were amazing and really
5740080	5746080	well embedded in thinking. Right. And I go, well, I can spend an hour on each one of those,
5746720	5750880	you know, cognitive beliefs he made, but I can do the same thing that appears.
5752640	5754560	I can describe how come what's that?
5755440	5758640	I'm trying to separate out what humans do that's different than these artificial
5758640	5762320	intelligence programs. And it seems like something about our ability to compound
5762320	5768000	abstractions into new layers of abstraction that I'm not sure that computers are pulling off.
5768720	5775360	I actually think they're pretty good at that side. The programs are written yet to do the
5775360	5782800	variation. That's not just noise. So if you just do noise, like, like there's, I think the problem,
5782800	5790640	the crack is how to do variations that are interesting without just being noise. And I
5790640	5795840	don't have a good idea. Like chat GPT is, I don't know, I've been chit chatting with it about, you
5795840	5802080	know, physics and stuff. It's interesting how it puts it together. And then you can see places
5802080	5805680	where it just falls flat and other things is like, well, that's kind of interesting.
5806880	5811200	It's synthesis is me, I would say medium. So to pass the bar, because that's mostly factual
5811840	5817120	associations, they can play all the games. Those are mostly role based kind of things.
5817200	5827520	And you can see somewhere it starts to fall apart on idea synthesis. But it's not, it's not sitting
5827520	5832960	there cranking 24 seven doing variations. Like when you think about something hard,
5833600	5839040	like you guys are working on your physics problem, that's partly running as a background
5839040	5842400	program in your mind for what a year, two years, some of the years.
5845520	5849280	And well, 10 years, all right, so you have 10 years of background processing
5850080	5856480	with a pretty well refined sense of what makes sense. And which may in fact be your biggest
5856480	5860560	problem, you may be filtering out all the best ideas. Right? That's a, that's a real problem
5860560	5868080	for smart people. So it's kind of complicated. So I don't, I think the synthesis is getting
5868080	5872800	better pretty fast. And people keep saying, we see at the current level, it's terrible,
5872800	5876160	they'll never get better. And I think, well, compared to a year ago, it's unbelievable.
5876720	5882880	Like we're on some pretty steep curves here. But this, and then this mixture of experts thing
5882880	5890400	matches my personal model of, like there's a workspace, and many, many agents that we apply
5890400	5897680	to it. And then we have many tools of variation and combination or when we think, and they're
5897680	5904480	starting to build that platform of multiple experts and a common workspace. And then the
5904480	5910800	the ability to make variations and then test the variations for say some useful just before we
5910800	5916160	waste too much time thinking about it. Like you don't spend very much time thinking, well, what
5916160	5920480	if I could just invert gravity? Like that's not worth the years worth of thinking, it's worth
5920480	5926800	like a half a second worth of thinking. But what if there's a different way to look at, you know,
5926800	5933680	these field equations, and you're, you're good at a bunch of mathematics, then you can test
5933680	5938320	pretty fast those things. Now the problem is there might be five more variations of field
5938320	5942160	equations that you don't know. So one of those might have been good. So you have a filtering
5942160	5945920	problem. So you may have in fact had a good idea that you filtered it out, because you
5945920	5953120	didn't have an analysis, a good enough thing. Like I had a friend who's better at me than a
5953200	5958960	computer designer smarter, but I was more creative. He felt he he would run into a
5958960	5962320	problem's idea, put it down, and I would kind of go, well, this is pretty good, but it's got
5962320	5965920	these three problems. And then this is pretty good, but it's got these two problems. And then
5965920	5969760	this is pretty good, but it's got this problem. And then one day I would go, if I take this and
5969760	5972960	this and this and put it together, and I got something that works, and I would tell them,
5972960	5975680	and he would go, how do you stick that out? And I explained it to him, he says,
5975680	5979120	but we rejected all this ideas. And I said, you stop thinking about it.
5980080	5981680	And I didn't stop thinking about them.
5982240	5985680	That's the layer on the AI that I don't see yet, which is the ability to take
5985680	5990480	disparate concepts and smash them together in a way that's unlikely, but productive.
5990480	5993840	I just explained the whole program, we can write that in 10 minutes.
5996320	6001440	That program wasn't hard to write. And I'm not even going to write as the AI programs
6001440	6008560	go to write it. I think the sensibility filters are a bigger problem than the
6008560	6012960	combination of things that is pretty good. Synthesis is medium.
6013600	6015680	But we just started breaking that into problems.
6017840	6023680	Yeah, so I'm fairly, well, I don't think thinking is magical. I don't think it's quantum.
6023840	6031520	I think it's like we already built computers with more operations in a second.
6032960	6038880	They're like, biology is insanely efficient for computation compared to transistors.
6038880	6042000	But there's reasons for that. And I think we'll solve that problem, by the way.
6044000	6047200	Like, where do you think thoughts come from? Like, where do you have a thought?
6047200	6048960	You're like, oh, I thought I just thought of something.
6049520	6052880	Where does that come from inside of the human architecture?
6054160	6060480	Oh, we're, you know, like the base of, you know, what we think of thinking is, you know,
6060480	6064480	advanced planning systems. Well, I think so your brain grew in a bunch of layers,
6065280	6070000	like, you know, in a sensory cortex and a motor cortex. And, you know, we sense both the
6070000	6076480	external world and ourselves. At some point, you know, low level animals got good at that.
6077120	6081280	And then they started moving around. And then they started doing basic planning, which is,
6081360	6085760	you know, run towards food and run away from teas and, you know, all kinds of things.
6086320	6091520	And then we've been elaborating planning more and more and more. So when you're just sitting
6091520	6096880	there doing nothing, the various parts of your brain will be like sort of newling on what should
6096880	6103280	I do? What should I think? How should I get ahead? And it turns out we've elaborated that so far.
6103280	6108800	We can think, how do I plan ahead in mathematics? How do I plan ahead in art? How do I plan ahead
6108800	6114240	in engineering? How do I plan ahead in four dimensional political chaff? How do I plan ahead?
6115120	6119760	Like, we're really good at it. Just very strange how thoughts come at the most unlikely time. We
6119760	6124880	know you talk to people who have been had inventions or brilliant mathematical physicists,
6124880	6129680	and they'll just mention these strange occasions. You know, I was having dinner with my wife talking
6129680	6134560	about the opera, or I was in the shower. It seems to be a very common feature that people
6134560	6142240	think. Yeah, so yeah, as a computer architect, I don't think that's odd at all. So the voice in
6142240	6149200	your head is a postdoc narrative. This part of your planning review system. And your actual
6149200	6156720	thinking is in many, many, many layers of computation. And not very many of those are visible to you.
6158720	6164240	All right, so some of those layers are computing and being rejected and computing and being rejected
6164320	6168640	when you don't even know it. I have the sense I can feel myself thinking sometimes and I can't
6169440	6174000	access. And then sometimes that kind of breaks through to higher level things where you start
6174000	6179120	to label it like I'm having these thoughts and they have labels which are words which I can say,
6180000	6186400	or enough of it, go somewhere and then you can kind of go tell the story about your thinking.
6187600	6192960	Just the feeling of thinking before you said that sometimes you have the feeling that you're
6192960	6199200	thinking that you can't access. What does that feel like? There was a really good book about
6199200	6205520	this which they did a whole bunch of experiments which showed that your verbal thoughts are a half
6205520	6210240	second behind your actions, which at the time shook me up. And then I thought, well, of course,
6210240	6221120	that's how it always feels. And then there's advice I've given people which is if you want to
6221120	6227280	solve some hard problem, you have to spend a lot of time and effort putting the problem in your head.
6228800	6237280	And I also noticed as a college student, I was relatively bad at learning things quickly and
6237280	6242880	getting an A on the test. And I had some friends that did that. Like if I took an engineering
6242880	6248880	class, I started studying day one and I did all the problems. And I mostly didn't study for the
6248880	6254800	last week of the course before finals because I found it was better for me to stew on what I knew
6254800	6260080	than to inject some new thoughts so it would screw it all up. I've always been that kind of person
6260880	6268160	and I trust the thinking. But yeah, it's strange when you're thinking hard about something that
6268160	6273920	it's kind of boiling around. But again, so my model of it is there's many layers of it.
6274000	6280000	Might not be true, I couldn't tell you, I haven't put any probes in. And that much of your thinking
6280000	6286160	and computation isn't visible to what you think of your current working set of thoughts.
6287120	6294240	And so the fact that occasionally things work out and get signal to some simpler level of thinking,
6294960	6298720	and then you're surprised by it isn't surprising to me, I think that's obvious.
6299680	6304080	But I had a friend who was very freaked out about the fact that he could never figure out
6304080	6310560	where his good ideas came from. He literally dropped out of college for a year because he never
6310560	6315600	knew like halfway through a course if he was going to get it. He just felt really anxious
6315600	6319920	about it all the time and it's like, well, what did you do about it? Basically nothing. He said,
6319920	6326320	I just live with it. And I was like, well, you could spend more time meditating on that. Like,
6326320	6331280	I think you can have some more access to it. But you could also say that's part of our architecture.
6332640	6336400	I think that he was just freaked out that it would fail him at some point if he didn't understand.
6337120	6339920	I don't know where it comes from. So I don't know why it will happen again tomorrow.
6340960	6346400	Interesting. Yeah, it's a funny thing, but I'm not surprised about it because, well, like,
6346400	6350800	if you go look in neural networks, like, when they first started doing like the cat
6350800	6354240	recognizers, they would see as you went through these layers of convolution,
6354960	6359600	that, you know, you went from an image to components of an image. And then, you know,
6359600	6363920	essentially you would have a layer which would do size and variance and orientation and variance
6363920	6370240	and then associations of different things. And sometimes in layers, you could see what it was
6370240	6374240	doing. And sometimes in layers, you couldn't. So they would pull the layer out and say, well,
6374240	6377600	if I don't know what it is, what's it doing there, they pull it out and it wouldn't work anymore.
6378160	6386000	And that's probably because that was some computed association map that had no relevance to what you
6386000	6395280	think. Like, and there's also a thing called overfitting, which is kind of funny, where you can
6395280	6400960	train a network so good it can only recognize what you trained it with, right? Or there's another
6401040	6409520	when you have way more, say, parameters in the network and enough layers, you can
6411680	6416400	compute the associations in a way that they're not overfit, so it can recognize essentially
6416400	6420640	more than you've trained it with, which is not mathematically right description, but
6421920	6429520	like, that's how it feels. And then I think humans have very deep layers, because we do,
6430080	6434640	you can see them in your brain, like cortical columns or six layers of neuron stick with a
6434640	6439840	spectacular number of connections. And then the cortical columns talk to each other, and then
6439840	6445840	they fire each other, and then they run many, many times. And when you're thinking hard for 10 years,
6445840	6453600	like, the number of passes through your brain is unbelievable. And your brain is even coordinating
6453600	6456880	those passes, it's not your whole brain, you're doing all kinds of different things.
6457840	6460800	They send a beginning of this that computers don't think.
6461680	6467840	Yeah, so the computers we have today, so the AI we have today is not what I would call thinking.
6468960	6471440	But it's always to build one that does.
6471440	6473360	But yeah, it's going to happen pretty soon.
6473360	6475200	Oh, so I mean, it's like, yeah, they don't think, yeah.
6475200	6476720	What's that distinction look like?
6478400	6482160	Well, it's, you know, it's like the read, the endlessly redefined scoring test.
6482720	6483200	Yeah.
6483200	6489360	So Elon's version of his couldn't solve a novel physics problem was the high end of human thinking
6489360	6494480	is solving hard novel problems. And I think that's the differentiated from,
6496320	6500960	you know, producing lots of word salad. Like, like this is a interesting thing.
6501520	6507280	I have a friend who's a writer, and I threatened him with writing all his possible books and
6507280	6513520	copyrighting them before he was, he wrote them because he's a very good writer, but he has a style.
6514800	6520880	And, you know, and, and if he told me that 20 ideas of the book, we could, we can generate
6520880	6528160	much of the writing. Now, obviously, a really good book has a, as a mental journey to it that
6528720	6534560	solves some problem, like some, you know, your, like some people are interested in, you know,
6534640	6539600	like trip reports and, and, you know, travel dialogues and whatnot, which are predicted,
6539600	6546160	right? If you go to Thailand and then degrees and then Iceland, and you look around, you will see
6546160	6552000	things and you can literally take a camera and point it at those places and then give that chat
6552000	6557920	GPT, which would write you a nice travel book about what happened. Like that's not intelligence.
6557920	6566480	That's a scriby. Right. But if you went there and the cultural cons, you know, cultural differences
6566480	6571120	between Thailand and Greece, maybe you understand something new about humanity, that could be a
6571120	6577040	really interesting book because that's a novel idea. And today chat GPT wouldn't write that book,
6577040	6584800	but you might. Right. But at some point that GPT will write that book.
6585760	6590800	So with the, the solving the novel problem, wasn't Alpha fold solving a novel problem?
6592640	6597840	Right. If they basically figured out a way using AI to solve the protein folding problem,
6597840	6604240	which had, yeah, so this is a really good boundary. So that's still a computational problem.
6605920	6609440	Now, I don't, maybe human culture is a computational problem.
6610240	6613200	Yeah. So there was something.
6615200	6622720	So I talked to these guys that just, just cracked me up. So when you do finite element analysis
6622720	6629920	on an airplane, imagine have an airplane and you're blowing wind across and then the wind
6629920	6635360	is turbulent, of course, the air is turbulent. And then there's, there's both an analysis of
6635360	6639520	structure, of temperature, different nations. There's a whole bunch of really interesting
6639520	6645360	things. And so it turns out they have reasonable finite element analysis programs written in
6645360	6653120	Fortran, which is a little rusty. And, and they're computationally limited to analyze, you know,
6653120	6657680	airflow over planes and structural failure, all kinds of stuff. So then they were trained,
6657680	6663120	they were using the Fortran programs to train AI models. And now at one level,
6663120	6668960	if you want to analyze one element, you know, and it's a billion operations of elements,
6669920	6677280	you know, that's a lot. And to run the, that program on a variety of elements to train an AI
6677280	6684160	model that turns around and for the AI steps to do anything, it's a, you know, it's a, it's a
6684160	6690960	million trillion operations, which is a lot more than a billion. But here's the amazing thing is
6690960	6698000	when they train multiple very large AI models with these Fortran programs, the AI models are
6698000	6701920	really good at analyzing things and ignoring the things that don't matter and sending more
6701920	6707120	computation on things that did. And they analyze the whole airplane, it was less total computational
6707120	6714640	steps than the finite element analysis, which couldn't differentiate any element. They all
6714640	6720720	look the same. The Fortran program doesn't care that AI model had a deeper level of analysis.
6721360	6725280	Which could do something to look at the whole plane. Yeah. Yeah, well,
6726160	6731920	yeah, it did something interesting. I don't know exactly what it did. But people are using programs
6731920	6738080	to train AI models and then getting better results. Because the AI models wouldn't analyze it
6738880	6748000	as sort of this log, rhythmic, or maybe linear computational bed burden for scale,
6748080	6754320	whereas the finite elements just have some kind of exponential burden for scale. So the protein
6754320	6758800	folding thing is really interesting. It says, we're like really small atoms, they probably have
6758800	6765120	programs that can do stuff, but they scale it up and computationally explodes. And the AI models
6765120	6771840	probably don't computationally explode. So when the AI start thinking, do we have an ethical
6771840	6776240	issue on our hands? Like, are they alive essentially at that point? Do they have self-awareness?
6776480	6784000	Is that our real life? Yeah, I think you already have that problem. So this will make you think
6784000	6790240	about what thinking is. So AI is clearly making everybody think about what thinking is. Because
6790240	6794240	we've had 10 definitions of thinking, they all fell apart. Because as soon as AI did it, everybody
6794240	6803840	declared it's not thinking. Because it's obviously not thinking. And now we have ethics built around
6803840	6811440	the sanctity of the human life, which has the narrow definition of embodiment in a person
6811440	6824000	between two and eight feet tall and 500 pounds. So we have this definitional state in a person.
6824880	6830240	And we've mostly given most of those people at various points in time some kind of ethical standing.
6831200	6839280	So not what happens when we enable a trillion AIs who are smarter than us. Do they have ethical
6839280	6846400	standing? Do we have ethical standing in their mind? I don't know. It makes
6846400	6852640	conclude that we're paperclips or something. It's a curious phenomena.
6852720	6858080	This leads back to the question that...
6859360	6863280	I'm just curious how we can rush towards something like that before we figure out the answer to
6863280	6873120	that problem. Are you familiar with Ian Banks, the sci-fi writer? Yeah, that was one of his.
6876160	6879680	I think he's a pretty good writer because he imagined the universe where there was
6880560	6884400	a wide range from robots that were obviously not very smart to humans,
6885200	6892320	including some very smart humans to AIs which were human level intelligence to extremely past
6892320	6897200	that. And he created a world where it was sort of all interesting.
6898160	6906560	So 99.9% already live in that world.
6908560	6913440	You guys are pretty smart. You're already enough smarter than most people that
6914160	6922320	you're unintelligible to them when you actually talk seriously. 99% of people already live in
6922320	6929120	the world where they're not very smart. It's already a fact. We already live in the world
6929120	6933440	where there are cats and dogs and ants and mosquitoes and birds and trees and all kinds
6933440	6939920	of things and nobody wakes up in the morning trying to stomp out all the cats. It's not a thing.
6940720	6946400	And anything any smarter than us is seriously not resource bound. This thing where the robots
6946400	6952240	destroy their Earth because they need energy or something is just whack. The Sun is putting
6952240	6957520	not so much energy. It's unbelievable that energy and the material around us is startling.
6958800	6967360	Like our local consumption of energy is 0.0000001% of the available energy.
6968480	6972640	So this part doesn't necessarily guarantee that your ideas will take off either, right?
6972640	6976880	There's this whole marketing charisma element to having your idea. It's to be a kind of a
6976880	6981360	beloved person, at least in the realm of scientific ideas that we play in. It's like,
6981360	6984240	you can have a great idea, but that's just the start of your battle.
6985600	6992960	I actually think that it's... What's the battle? Like humans are very conflict-oriented. So we
6992960	6998880	have all these interesting priors. One of our biggest problems is humans for a million years
6998880	7005840	of evolution, whatever you believe, are essentially zero-sum game because it was a zero-sum game.
7006560	7009760	Like if you eat all the local deer and stuff, there aren't any deer.
7010800	7014000	But now we can produce an unlimited amount of food with energy,
7014000	7019040	probably, and there's an unlimited amount of energy. So we don't live in a zero-sum world.
7021200	7027360	Only by political... Yeah, yeah. There's only a couple kinds of shortages. There's something that's
7027360	7032800	in fact rare, and then there's another kind of shortage, which is it's new and they haven't made
7032800	7037360	enough for it. They're too expensive for everybody, but that's usually a transient.
7038240	7044560	And then there's shortages due to monopolies and there's shortages due to, let's say,
7044560	7050800	refreshing of production, right? And there's literally nothing rare. Like we can't run out
7050800	7057120	of aluminum because aluminum is a metal. Like you use it and you melt it down. Like there's no
7057120	7061840	shortage. I always love these things. They won't have enough copper to make the solar cells.
7063760	7066880	Oh, there's so much copper in there. It's very expensive at some point, right? It ends up in
7066880	7073840	landfills and you've got to process it. No, there's so much in the top 100 kilometers of the earth.
7073840	7078960	It's not funny. Oh, here's a funny story. It's like, so computers are made out of essentially
7078960	7087200	three atoms. Oxygen, silicon, aluminum, a little bit of copper. Some gold in there as well, right?
7087760	7094320	Yeah, it's really trace. So they use like 32 elements as trace elements, but you know, by bulk,
7095120	7101600	it's so it's by bulk, it's silicon and oxygen. So 70% of the earth's thrust down like 100 miles
7102240	7107920	is silicon, oxygen and aluminum, which looks like a setup to me, like
7110240	7116720	like carbon and hydrogen, like these are all trace elements. There's a lot of carbon and
7116720	7124560	hydrogen, but the big hitters are, for whatever reason, is silicon dioxide and aluminum oxides
7124560	7130880	and, you know, all kinds of iron. But iron, like the percentage of it goes up as you go further down
7130880	7136720	on the earth because of the, you know, melting or something. But there's like so much of everything
7136720	7143040	that's not funny. Like they did, there's a couple of processes. Just look up by weight how much like
7143040	7149120	copper and magnesium titanium is in like the ocean. Like there's really good processes they're
7149120	7157440	developing to like just in sea water. Yeah, just sea water. So there's a lot. I don't miss it. I
7157440	7162400	think that the idea that AI are going to show up and they're going to destroy the humans and
7163120	7171360	the paperclip maximizer means are, I think that they're deeply informed by dystopian science fiction.
7171360	7172880	And my
7174880	7183920	greater worry is that they'll will become stuck at some computed optimum.
7186160	7191760	And what I mean by that is if you have an, if you have a super intelligence and the super
7191760	7198640	intelligence is like, this is what should be done in this circumstance. And it has the ability
7198640	7204400	to inform what you do at every circumstance. That's kind of the ultimate centrally planned
7205040	7211120	economy in some ways. The most centrally planned culture.
7212400	7221040	Well, let's, well, I like Elon Musk's line about very important to train these things about the
7221040	7226720	truth. And you know, one of the truths are there's endless cycles and there's endless chaos and
7226720	7235200	chaos is very important to development as is randomization and maximizing the alternative frames.
7236400	7245920	Like, so I think that's true personally. But, you know, I think, and, and I have a theory that
7245920	7252640	like thinking and ideas are essentially infinite. I'm not sure why there's a limit to it. And so
7252720	7259040	if it was smart at all, you know, my, you know, my, my guess, my preference maybe or my hope
7259760	7267760	is that, you know, it goes in some interesting direction of more, more variation, more possibilities,
7267760	7277840	more explorations. And then you said, like, like humans, because of our biological grounding and
7277840	7282720	our competitive, like humans are hyper competitive, even when the act like they're not that's just a
7282720	7288320	good competitive strategy. Like you, you're both very nice. That's a winning strategy with a lot
7288320	7292800	of people who are going to make friends that way. Like, but, but we're hype no matter what we're
7292800	7300000	hyper competitive. We have a built in zero sum game, we're very oriented towards the world about
7300000	7307600	whether it's like one of the basis of company cycles of is it uncertain that enables
7307600	7313760	exploratory behaviors and growing that enables us to participate in growth? Is it steady state?
7314560	7318480	And then we start reacting with steady state means we're going to run out, by the way,
7319200	7323120	like, then you start exploiting what you have and trying to get your share.
7324160	7331520	And then there's inevitable collapse. And then, you know, we reset. And so, so we're very aware of
7331520	7341280	those states. And so AI that triggers fears of competition and running out in scarcity is very
7341280	7347760	different from technology. So like when technology has been a lot of fun, it's when this is like
7347760	7353520	the iPhone created a new possibility, the internet's a new possibility. You know, the thing I'm not
7353520	7359120	super happy about AI right now is, you know, only large companies heavily funded,
7360080	7365600	you know, big guys are doing it. And they're, they're using the language of establishment and
7365600	7372880	scarcity, which triggers people to fear it. As opposed to the language of the PC, the iPhone,
7373440	7377200	like some of those technology literally came out of small growing companies.
7377840	7382640	And they had the energy of growth, which stimulated the people around them to think,
7383840	7387520	by the way, I just, this just occurred to me. So it's not something I was doing on.
7388320	7390240	But anyway, it seems kind of obvious that
7391360	7396160	like if the environment says, Hey, there's only so much of stuff and we might run out,
7396160	7400240	which is the endless drama about resources, we're going to run out of resources.
7400800	7404480	You guys somewhere in you believe we're going to run out of copper or something.
7404480	7408880	I can prove to you that we won't run out of copper or aluminum or silicon dioxide.
7408880	7413120	It's literally what the planets made out of like, we can't run out of materials to make chips.
7413120	7420640	Right. It's actually suspicious how much computer technology is embedded in the near part of the
7420640	7426720	earth. Like I have a theory that this is the earth is the remnants of the previous supercomputer that
7426720	7434400	was, you know, somehow, in some sense for a planet like this, too. I often just think about us being
7434400	7439360	of the earth and just some sort of organ that the earth has produced. Yeah, there's just enough
7439440	7445280	carbon, carbon to give the book program is biologically, but the computational substrate
7445280	7452640	is silicon dioxide. So like, and there's a bunch of reasons like carbon is super good low temperature
7452640	7459840	chemistry and carbon chains and all kinds of stuff are silicon saster. It's 300 degrees C,
7459840	7466560	you know, get a melted 1000 degrees C. It's it's really sticky. You put an atom on a silicon
7466800	7472560	atom. It's stuck there. It's, you know, you can only manipulate the high enough temperatures
7472560	7480320	that you really don't want to. Whereas like carbon is fantastic, low temperature, relatively low
7480320	7487840	vibrational mode, you know, you know, chemistry is super good. But anyway, that idea about the
7487840	7495840	competition and the scarcity. Yeah, so so the the the the advertising guys would like you to think
7496640	7503760	there isn't enough to stop so FOMO and it's valuable. Like you may charge more about convincing
7503760	7509040	people there aren't very many of them and you have the good one. So the current, you know,
7509040	7517520	marketing and the consumer capitalism is all scarcity, you know, demonizing, like just scaring
7517520	7522240	the shit out of people. We're going to run out of oil. That's crazy. There's lots of oil. We're
7522240	7527840	going to run out of oxygen. We're going to run out of food. We're going to run out of carbon or
7527840	7534720	aluminum. Jesus Christ, like, there's 7% aluminum or something. We can't run out of aluminum.
7536640	7543200	But that puts you in the mindset of fear and running out. And so the technologies that were
7543200	7548560	exciting, when film first came out and airplanes came out, there was 40 airplane companies,
7548560	7555520	maybe 100 and cars, there was 100 car companies and PCs used to go there and there'd be 50 guys
7555520	7564560	making PCs, right? So today, the big AI is coming out, Nvidia makes the chip and Microsoft
7564560	7570320	open AI and Google make the AI models. And it's a there's a scarcity mindset to it and
7571200	7576800	unobtainium something to it, which is not good for technology and good for people. We're not
7576800	7580560	running out of air. We're not running out of aluminum. We're not running out of compute. We're
7580560	7589120	not running out of like we should be in a growth mindset. And humans are very fun to be with when
7589120	7595440	you're in a growth mindset. And we're very boring and let's say not great to be with when you're
7595440	7601120	in a scarcity mindset. Scarcity mindset means if you eat that apple, I don't get an apple and I'm
7601120	7606080	going to fight you for it. Gross mindset of there's so many apples that are falling from the trees
7606160	7610800	and we're planning different kinds of trees and you talk to friends and they've invented four
7610800	7616560	kinds of apple trees last week and you know, you can't wait to see what the next apple is,
7616560	7620000	you know, that that's that would be fantastic world to be in.
7621600	7627200	And what freaks people out about AI beyond just the scarcity is something that you mentioned
7627200	7632560	earlier, which is that you had product placements back in the day where everything was all the
7632560	7638880	Marvel movies had Coca-Cola cans and now you're going to have Marvel movies that have a unique
7638880	7643040	product based on all of the data that they've collected off of you off your cell phone or
7643040	7649440	whatever else. And that fractures reality just a little bit. And then you have all of these
7649440	7658960	subsequent fractures of reality where that shares if a shared reality is a limited commodity all of
7658960	7664880	the sudden, that's kind of the fundamental. That's the scariest thing at all, where you can go talk
7664880	7670240	to somebody across the street and they no longer live in the same universe as you because they have
7670240	7680000	consumed video and media that has nothing to do with what you've seen. And so that scarcity on the
7680000	7686800	back of AI is kind of baked into the way that the tools are going to be used because I can imagine
7687520	7692560	more AI startups. If the chip isn't $25,000
7694800	7697840	and you have people that are interested in building stuff on the back of it, then
7700080	7706640	it seems plausible that you could get that kind of ecosystem. But that ecosystem seems to feed
7706640	7709760	into a scarcity of shared reality. And that's really interesting.
7709760	7717360	Yeah. So, I mean, this cycle has happened, you know, with newspapers, with movies, with television,
7717360	7722400	with the internet, where when it's in the growth mindset, lots of people participate,
7723360	7730000	we develop a sense of trust with it. Like I remember when television broke thrust
7730960	7734640	in the 60s because they were lying about the war and lying about the peace march.
7735280	7740800	And then in the 70s, there was lots of cynical TV shows. And so that,
7742080	7747120	and I know that got gained as well, but there was kind of an era of cynicism about
7747120	7752880	something that had been trusted. Walter Conkret was the news. The New York Times was the news.
7752880	7759120	And these are institutions you can trust. And Walter Conkret died. And I don't think anybody
7759200	7766960	believes the New York Times anymore. Well, you know, some writers, yeah, so that, but here's the
7766960	7773040	thing is that healthy human beings form relationships with things and evaluate them with should they
7773040	7779440	trust them, right? And then periodically, like sometimes it's individuals stop trusting this.
7780400	7786800	Like, I don't know if I mentioned this, scientific American, like I used to read it a lot, but I
7786800	7792080	noticed the computer articles were lousy. But I thought it was just a computer articles. But
7792080	7796720	then I talked to a friend who was a mathematician and he said the math articles were lousy. And
7796720	7800400	then he called up a friend who was a chemist. He said, no, the chemistry. Oh, well, so shit.
7801120	7806480	You know, I thought this was trustworthy because now it's, it continued to be fun because the
7806480	7812720	scientific American was essentially entertainment at that point. But it wasn't, it wasn't a trusted
7812720	7816720	source as it were. Now, whether they know they're a trusted source, you're not idle.
7816720	7821680	Like these are complicated things. And then trusted at what level and to what impact to you.
7823200	7828080	So like, like the Marvel movies were a really interesting thing because the original stories
7828080	7833760	were really well grounded stories, like they were good archetypal stories. But then as they tried
7833760	7838480	to elaborate it and the bean counters took over, they started just making a whole bunch of blah,
7838560	7844480	blah, blah, you know, including whatever message of the week they were into. And then nobody
7844480	7852000	watches them. They're boring and repetitive. And so there's that kind of thing happens in like
7852000	7858080	the same thing happened on the internet. You know, like there was everybody was there,
7858080	7862560	there's all these forums and people talking and then they got into better technology,
7862560	7867520	but then they got curated and you know, most people don't believe that much about that.
7868160	7872720	But it's, but the relentless in this office is interesting. And I think the same thing will
7872720	7879760	happen with AI, like, like people get gained by it. But at the same point, people start to go,
7879760	7885200	let's say I generated, it's probably, you know, there's marginal utility in it. And then
7886240	7891840	there's been many attempts to have curated news and internet to media, like the internet you
7891840	7895440	can trust, but it's really hard because soon as they make any money, they got bought by somebody
7895520	7903440	doesn't feel that way. And so it's, it's pretty complicated, but most humans grow up in this world.
7905120	7908960	And then humans, humans sort their lives out depending on what they need to do.
7910480	7914480	You know, people who are trying to do geopolitical operations need to have
7915360	7921760	a way difference filter for, you know, let's say, you know, shenanigans and somebody who's running
7921760	7927200	a, you know, simple store and technologists is the same way. So
7930960	7935680	I think that there's something in there about an eternal
7936960	7944080	untrustworthiness that's been with us for a long time. Because if you go back to a model where
7944080	7949520	we're all just 100 person tribes, it's not like the tribes are living in harmony. There's probably
7949520	7954960	warfare. There's conflict. And the leader of your tribe might be telling you things about the other
7954960	7961280	tribe that aren't true, but are functional and necessary in order for you to be willing to go
7961280	7967440	on a raid or that. And so I think that we've, sometimes I think about this in the sense that
7967440	7973360	we've loaned ourselves into a false belief that there is something outside of our direct experience
7973360	7979200	with people that can be trusted, because we want so badly for that to be the case. Because if you
7979200	7982640	think about your own personal relationships, the people that you have relationships with
7982640	7988240	that are closest to the ones that you can trust them, right? If you have somebody that you look
7988240	7991840	at and you're like, well, I can't trust you, you're probably not going to be very good friends with
7991840	7997360	that person. And so we want to be able to generalize that outwards to systems. But then as you're
7997360	8002400	speaking, I'm sort of thinking maybe that's a false, maybe that's been an eternal false hope
8002400	8011200	where even Walter Cronkite, like he, he was the news, but the CIA had tons of people at all
8011200	8015280	these news organizations. Yeah, yeah, he was just reading the paper. So here's another way to think
8015280	8022640	about it is, you know, when, when you start to go into a scarcity system, the, the, you know,
8022640	8028080	everybody you deal with, you know, has some cost benefit with, you know, if they lie to you and
8028080	8036080	you catch them, that's a cost to them. But, and if the benefit, the line is really low,
8036080	8041280	they're probably not going to. So in a, again, in this frame of in a growth mindset,
8041920	8046240	where there's lots to do, the benefit of lying is relatively low because you can,
8046240	8052880	you can get what you need. But if you're convinced that, you know, things are tough,
8052880	8057520	the company has layoffs coming, you know, a company that's growing 10% a year,
8057520	8061600	it's pretty healthy. People generally help each other to get the job done.
8062320	8068480	There's an interesting book called stretch, which says if, if you have about 10% more to do than
8068480	8075440	you can, you stretch and if somebody offers to help you, you accept it, you appreciate it, right?
8075440	8079280	If it's 50% more, you get burnout because you can't achieve your goals.
8080400	8085680	If you have 10% less than you need to do and somebody offers to help you say, stay off my tariff.
8086640	8090720	Because if they do your work and there's a cut coming, the boss is going to go,
8091360	8097360	well, Jim only ever does 80% and, you know, Bob over here picked up the slack. And so
8098160	8104400	the stretch of the organization sets the trust level. It's almost sad how simple it is,
8104400	8110000	but it's actually true. Once you weed out them, the various characters, if your organization
8110080	8116160	stretch, they are incented to cooperate. And if, and if there's not enough to do and there's cuts
8116160	8123600	coming, they're incented to undermine each other. It's the same thing with product placement and
8123600	8129360	stuff. Like, you know, Coke is a weird thing because it's like a zero value product because,
8129360	8135280	you know, zero to make. It's entirely, you know, manufactured and sold on brand.
8135920	8140160	Nobody really cares what it tastes like. I don't think so. I sure don't.
8142080	8147920	But the brand has established something and humans have, you know, habits and stuff. So they
8147920	8153360	have a positive association with the brand, the image, the consumption, there's a flywheel of
8153360	8165360	that kind of thing. So I think, yeah, he's generically afraid of AI. Like, it's not helpful
8165360	8170480	because he can't solve any problem by being worried about it. Having some understanding of how
8170480	8177440	technology impacts society, how we use it to change our culture, what stage we're in is really
8177440	8183760	interesting. And then what's going on with the players? Like, because I think that's,
8185280	8193040	that's helpful. And then there's the, you know, the odds that humans with this technology aren't
8193040	8199600	going to develop intelligence is zero. Right? Like, we're either going to keep going up or,
8199600	8204560	you know, like, like, we're very dynamic, you know, we accumulate knowledge like mad,
8204560	8212640	there's eight billion of us. We're not super good with, you know, low stress environments,
8212640	8218560	humans do really well with some stress and some restrictions and stretch, I think,
8219520	8224800	the companies, you know, with real missions, you know, do real well because people work together
8224800	8237280	to solve the mission. And, you know, it's a very curious time because we're developing something
8237280	8243600	very new in a time when many, many people were think we're in a zero thumb game and possibly
8244560	8248800	going down, but, and the advertising to that effect is relentless.
8249600	8255840	You know, we're running out of like literally everything. Now, there are certain things that
8255840	8260160	are getting worse, like our ability to build roads, that's bureaucratically captured this poor
8260800	8264640	following the building to build airplanes, they laid off all the engineers apparently,
8264640	8269120	and it's run by, you know, the accountants like those things actually have.
8270240	8275680	So airplanes got unbelievably reliable. And then people took it for granted and then,
8276640	8282800	you know, but that just means it had something to do with their growth, too, because there's
8282800	8288880	a merger with the fellow Douglas and they laid off all these QA people. And yeah, yeah, it's
8288880	8294800	quite the shenanigans. They financial engineered it. Yeah, it's, you know, that's the kind of thing
8294800	8300000	it was like, if I had a magic wand, I wouldn't allow mergers of large companies because it'd be
8300000	8305200	better for them to grow or fail by themselves and create more small companies because I think more
8305200	8311200	small things is better than small numbers of large things. But I don't have that magic wand and
8311200	8315200	big companies lobby heavily a lot of those mergers because that's how they make money.
8316320	8320080	Like a lot of big companies essentially never have any new ideas. They just keep buying small
8320080	8329520	companies with ideas. You know, like Google, for example, has bought a large number of companies,
8329520	8335040	but there are only two money making assets are basically search and YouTube. And they
8335040	8344240	bought YouTube. Facebook famously did better. So they bought WhatsApp and Snapchat. I always
8344240	8350000	forget the list of them, but they bought a relative. They bought companies that actually had real
8350000	8354720	growth and value and continued to grow them. But it's not clear the world would have been better
8354720	8362080	off of those companies have been separate companies. And it does seem to be this tendency
8362160	8368400	towards agglomeration. Like there is, there doesn't appear to be an upper level of size,
8368400	8376400	like there is for something like a cell, right? Well, yeah, yeah, that's pretty funny. Well,
8378560	8386240	yeah, the cell, like in a company, essentially the cell is a small team of 10 to 20 people.
8386320	8395280	So all big companies like are made of cells, right? And so, yeah, but one company can acquire
8395280	8400480	another whole animal and then merge the cells together. And, you know, they'll do it for
8400480	8405440	some reason about synergy or efficiency or something. But it may in fact, because they
8405440	8409600	don't have any new products. So they have a lot of money, but they don't have any new products.
8409600	8414880	So some companies like, like big pharmaceutical companies, they invest piles of money in R&D,
8414880	8418800	but almost all their new products come from acquisitions. So
8420320	8428080	I wonder if there's the same sort of cycle like there is with the megalodons, right? So there's
8428080	8433680	megafauna. There's a period on earth where megafauna dominate, and then the conditions change,
8433680	8438160	and then all of a sudden, they're no longer the dominant biological paradigm. Do you think that
8438160	8443840	the same thing happens inside of corporate structures where there's just an era of huge
8443840	8448240	companies, and then the winds shift, and then it goes back towards smaller companies?
8449360	8453360	No, I think things just tend to get bigger. So when I was a kid, I was taught that
8453360	8456640	animals are really big either because it's really hot or because it's really cold.
8457440	8463120	You know, mammoths are big at the poles because it's cold, and dinosaurs are big because it was
8463120	8468800	hot. And I thought, actually, they probably just get bigger. And then something happens,
8468800	8473840	like a big rock falls on the earth and resets it, and then it kills all the big animals because
8473840	8479280	the infrastructure for big animals is really complicated. So my guess is companies just get
8479280	8486560	bigger, and then they occasionally fail hard. You know, some companies have actually failed
8487680	8491920	and gone bankrupt after they've been big. And some companies, especially with the
8492640	8498320	way government bureaucracies and companies work, they can continue to acquire companies.
8500960	8506480	But there will be some financial reset. Like during the PC boom, all the companies involved were
8506480	8514000	small. Like IBM messed around with Windows and PCs, but they gave the operating system to Microsoft,
8514000	8517600	and they had their own chip, but they decided to buy the chip from Intel. Both of those companies
8517600	8524480	were small. And then Dell and Gateway, all these companies were all small. And they were all actually
8524480	8531120	too small for the big guys to even care about. But then they grew so fast, you know, they avoided
8531120	8539440	getting acquired. Like that's an interesting phenomena that they grew. Whereas once, you know,
8539440	8545760	Google and Facebook and some other companies got big, they had grown so fast that they grew fast
8545840	8551600	enough not to be acquired. And Zuckerberg famously turned down a billion dollars for his toy startup.
8552480	8557120	But but then they turned around and they bought a lot of the small companies.
8557120	8561120	Well, we can business model at some point, I knew lots of people that were starting startups
8561120	8565760	exclusively with the goal of getting a profit Google. Yeah, like the network thing happened
8565760	8570640	the same way. There was like hundreds of small networking companies and the big ones got a
8570640	8575760	certain point. And then Cisco's business model was literally acquiring successful startups and
8576320	8581040	ramping their revenue. And sometimes that was real, like a small company couldn't build a
8581040	8585680	billion dollars worth of network switches. But Cisco could, they had the supply chain and
8585680	8590880	manufacturing and operations to do it. So sometimes the action creates value like Cisco,
8591840	8594320	like I think often did, but sometimes they just,
8595040	8600640	yeah, it's, it's hard to say. It makes somebody use some money. Hardware might be different than
8600640	8608320	software. Well, so there was a long time when hardware wasn't like that. There was in fact,
8608320	8613920	you know, 50 or 100 airplane companies and miles of motor companies. And then the big
8613920	8619360	companies in the US, they outsourced. There was a period of time when Ford made everything and
8619360	8625840	they slowly outsourced the seats and the transmissions and the tires and the shock absorbers
8625840	8631760	and all kinds of stuff. And so there's a, there's a pendulum swing they call vertical to horizontal
8631760	8637200	integration. Like you make everything in the stack or do you make your piece like PC world was
8637200	8642000	what's called horizontal structure. So Microsoft made the operating system until made the chip,
8642000	8646640	Dell made the computer, somebody else distributed the thing or somebody else made the network.
8646880	8651680	But now Apple is a vertically integrated company. They make every single thing.
8653760	8658320	So you said that one of the things that you would do if you could waive a wand was to prevent
8658320	8665680	large companies from buying little ones. What are the other things that you would do?
8667120	8669840	I'll govern the arc receipts that have time limits.
8670240	8676720	You know, obviously, you know, our represent representative should have time limits or
8676720	8683440	term limits. Like, imagine you wanted the most independent experiments happening
8684160	8688320	so that none of them that they failed would take out lots of people.
8690880	8694800	Like we have a, you know, the, you know, the, the, the NIMBY problem, right? So,
8695760	8700240	so people want to build new houses and they can't come to regulation. So they go somewhere and
8700240	8706160	start a town or some town allocates a whole bunch of land, which great. So they can build houses
8706160	8710880	and new roads and the new school, the new shopping center. And as everybody moves in,
8710880	8715120	they all join the town council and they pass regulations so they can't build any more houses.
8715120	8722880	Right. So like this, and again, this is, this is, this is biological. Like,
8723600	8729200	like we operate from zero sum games and limits to growth and in competitive nature.
8730640	8738320	So if you want to architect a world that creates, keeps creating possibility. And by the way,
8738320	8743040	I don't think it's bad. So, you know, you bought your house when you're 30 years old and, you know,
8743040	8746880	you raise your family or retire there and you don't want to build a new shopping center and
8746880	8752960	screw up the neighborhood. Like, sure, go ahead and do it. But then you can't complain that there's
8752960	8758960	no housing in the area. Like, like the United States is literally three, you know, three,
8758960	8765120	three million square miles of land. Most of it's empty. You know, there's like a good
8765120	8769200	support population of the trillion probably. Most of it's really bad. And I don't know if you've
8769200	8773360	spent a lot of time in the inner mountain west, but that's a harsh, harsh landscape.
8773360	8780560	Yeah, it's an energy problem. But the, my premise is more small independent things are better.
8782240	8790400	Like, and one reason we met was you had some scientists on that, you know, I quite like
8790960	8794800	and some of whom are literally actively suppressed by the scientific community.
8795760	8802880	And again, I don't know if they're right around at some level. Like, I'm interested in it. And I was,
8803520	8808400	you know, surprised. You know, borderline shocked, you know, when I first discovered that there were
8808400	8813120	scientists who couldn't get their papers published, not because they seemed obviously stupid, but
8813120	8819600	because they didn't go to the narrative. And, you know, at some level, you could think,
8819840	8828080	well, I know governments could craft and big companies get bureaucratic and stodgy. But at
8828080	8834240	least, you know, the scientists, you know, Einstein, Shirley Einstein is a good guy. And,
8835520	8840560	and, you know, the people who studied his work, and then you read these articles, it was Peter
8840560	8845280	Voigt, Peter Voigt, he wrote the book, not even wrong. That's what a few hours in his office one
8845280	8849280	day. I actually he was the reason I even went down this path in the first place. When I was a
8849280	8853840	little kid, he had one of the first blogs that was critical string theory. I was so excited to
8853840	8858880	meet him and everything. He just turns out to have a lot of crazy mathematical ideas too. So,
8860320	8864720	yeah, but he also is a very strong believer in some parts of particle physics, so other people
8864720	8870560	are wrong. And he's very dismissive of them. And to be in hospitals, or her book, Lost in the
8870560	8878400	Maths is great. But then she's very much a true believer in other things. So, you know,
8879120	8884960	but you run into this interesting phenomena that even people who, who seem outside the
8884960	8891600	canon in some places could still be true believers in other places. And I have a theory,
8891600	8897680	I have a theory that you can only have one out of the canon idea at a time. Like,
8898240	8904640	if you have an idea that you're championing, unless you're like Thomas Gold or something.
8905200	8909760	You mean you're only allowed to have one idea out of the canon? That's a strategy or you think
8909760	8915120	that's a human nature problem? I think that's a strategy. I think that people are basically like,
8915120	8921200	look, I know this is crazy. Yeah, yeah. I'm not crazy because I'm buying to all of these. Yeah,
8921760	8930720	I'll tell you, I believe it. And I kind of get that. But I don't know,
8931920	8936640	for some people, they probably have three crazes. And there's probably humans have a limit.
8937440	8945040	Our computational substrate has limits on how many non-conformist ideas we can have at a time.
8945120	8952880	And that's another kind of problem. It's really painful, right? Like, if you don't have any common
8952880	8958880	ground to stand on with other people, it gets really painful, I think. You have to surrender
8958880	8964720	to some common knowledge at some point. Yeah, and it's in this surprise. I told you guys, I
8966000	8969440	found this list of, you know, crazy things about the sun, which I thought was fun,
8970080	8977520	but I talked to a, you know, a sun scientist and he was quite mad about it. And I was a little
8977520	8982080	taken aback because I was like, man, this is amazing. Like, you know, like, there it is right
8982080	8990560	there. It's so hard and complicated. And so the polarity of ideas is, I think, really important.
8990720	8999840	And then, so the recognition of tendencies, like I told you, there's this, you know, chaos versus
8999840	9008640	order productivity graph. Everybody can tell you where you should be, but nobody knows how to stay
9008640	9017120	there. It's like, when they built the interstate highway system, they invented the road system,
9017120	9023360	the equipment to build the companies were created. Like everywhere they went, they had to build new
9023360	9028960	companies to build these new roads. And it was sort of like they were so excited about building new
9028960	9034240	roads, they didn't have time to think like we could literally steal half this money. And nowadays,
9034240	9039840	it costs literally 10 times as much money per mile to build a road. They're better engineered,
9039840	9046160	quote unquote, and better planned. And, you know, but the money disappears in bureaucracy. And so
9047840	9052400	you know, like the best thing would be a way to like really reinvent all kinds of things.
9053280	9058560	So like young people, everyone's like, people say stuff like, oh, you know, there's so much debt
9058560	9062560	and you screwed all this up. It's like, yeah, but we built all that. Why don't you build your own
9062560	9069600	stuff? Like, like the current cartists, companies are defunct. You know, and then Elon builds a
9069600	9073680	new car company, you can't build a car company, you definitely can't build a rocket ship company,
9073680	9078640	and you definitely can't build a New Orleans company. He's probably got a list of 100 things,
9078640	9087440	those things you can't build. Like, build new stuff. I'm involved with Atomic Semi to help
9087440	9091840	pound the company and we're building a semiconductor fab that's really small. And
9092560	9097120	like you explain it to people and they'll say, well, half that might work and half like it'll
9097120	9101680	never work. Obviously semiconductor companies cost $10 billion. Where are you going to get
9101680	9107440	$10 billion? It was like, well, I was going to build it for $10,000, like not $10 billion.
9109120	9115600	And we've developed so much science as that. So humans are,
9118480	9124000	Jordan Peterson told me this funny line is like, like, there's a high incentive to get used to
9124000	9130560	things. So the first time you drive somewhere, it takes a while, right? But if you drive there
9130640	9134400	10 times, you drive back and forth every day, you don't even notice the drive anymore.
9134960	9140880	And I've noticed even on like a hike, when you walk, like if you walk a hike and you walk there
9140880	9147360	and then you walk back, the walk there takes twice as long as the walk back. Because you've
9147360	9155840	already, you know, because it's your perception of time as novelty and bias, which is really curious.
9156800	9162080	But there's a couple of exceptions to this. Like you don't get used to your kids. Well,
9162080	9170960	if you do, they'll die. So, or it could be your perception of your children is highly influenced
9170960	9177120	by how important they are to you. So even small deltas are novel. Like there might be some,
9177760	9183120	like the nice way of saying this, if you love your children, they're always new.
9184080	9190880	But the other way that the economy argument of it is, we're not descended for people who didn't
9190880	9195280	pay attention to their children. So we pay attention to them. And we notice pretty small
9195280	9200000	differences where as you, you walk down the street, you don't really care very much whether your
9200560	9206240	neighbor mowed their grass or moved a car around this year, an idiot, right? So do we have this
9206240	9211280	kind of funny? So, but the problem with that is we get used to everything. We're used to the science
9211360	9218000	we have. We're used to how they make cars, you know, we're used to the things like when I first
9218000	9224560	went to the Tesla factory, like it was all new to me, right? And it would really warm me out. I
9224560	9230080	spend hours there, I go home, like, you know, flashing lights and moving equipment. And after
9230080	9235840	a year, I've been there so many times, like, I would, I would be walking to a meeting and
9235840	9241280	annoyed that I couldn't park my car. And I was five minutes late. So I was walking fast through
9241280	9249040	this factory of wonders. I was walking to a place that literally made me sleep an extra hour a day
9249040	9255280	for three months, because it was so novel and new. Right. And now I was walking by it all annoyed
9255280	9262240	that I was late. Like, and this happens to everything scientists, physicists, you know,
9262240	9267360	they're so used to string theory, everything is bullshit compared to that. And we're so good
9267360	9274800	at something like, I really love the line, the unreasonable effectiveness of mathematics.
9276640	9279440	Right. But there's a counterline to that, which is something like,
9280400	9285200	like physics isn't mathematical, because we know so much, it's mathematical, because we know so
9285200	9298080	little. Like, it's so wild. So, yes. So the people had a better understanding of, you know,
9298080	9303760	why should there be limits to size? Why should there be limits to duration? Why is it hard to
9303760	9310800	stay on the high point of the productivity curve? Why do we get so used to things that we ignore
9310800	9316560	them even when they're really important? Why do we accept that you can't build a better X, Y, or Z?
9318400	9324480	Why are you condensed by advertisers who have only their own interests in mind that we're running
9324480	9330400	out of everything, create scarcity, you know, like, like, these are hard things to get out.
9330400	9336560	These are mental traps. There's a lot of them. And some of them are like, we earned them. Like,
9337280	9343280	being a zero-sum person as a person member of a tribe after a million years of evolution,
9344000	9351440	that pretty smart strategy. Right. Now, being oriented towards spring, summer, or fall,
9352400	9356640	you know, that's also pretty smart, too. You know, like in the winter, you know,
9356640	9363520	you fight over the last apple in the summer, you know, that's great. So, but knowing these things
9363520	9368720	is important because we build politics around them. We build structures around people, build their
9368720	9373040	lives around them. I know people are going to spend their entire lives worried, you know,
9373040	9378720	it was peak oil and then peak this and peak that, you know, and none of it happened. Like, literally,
9378720	9384800	none of it happened. I think that that's so vital and it's really interesting because it
9384800	9389760	keeps coming up. It translates into your personal life, too, right, in terms of where you put your
9389760	9396560	attention. I had this mind-bending experience as an artist, as a musical artist, where I heard
9396560	9402960	something that Nigel Godrich, he's a producer, he worked with Radiohead for many years. He said
9402960	9406480	something like, in an interview one time, this blew my mind, where he was like, you need to stop
9406480	9410880	trying to make it sound good and start trying to make it sound interesting. And I was like, wow,
9410880	9415920	that's really profound because that's exactly as a musician who's been playing an instrument for 40
9415920	9422000	years or something. You're so obsessed with optimizing your technical ability and it's like,
9422000	9427520	nobody cares. Like, literally, everyone only hears technically perfect music all day, every day,
9427520	9431840	but it doesn't stand out because it's technically perfect. It actually blends into the background
9431840	9437600	and these artists just kind of decay into fade away into nothing. And so there's just something
9437600	9443440	really, really interesting there about following what's surprising and what actually grabs you as
9443440	9447840	opposed to what's perfect or what's, you know, the best version of what came before.
9449360	9457040	Yeah, I saw Crosby Stills and Nash, I guess, play and they played their top hits just exactly
9457040	9463120	like they played them 30 years before, which I thought was terrible. And then I, because they
9463120	9469440	were such a wild band, like they did like five albums in two years that were all knockouts and
9469440	9477280	then they, and then Jeff Beck did this live album. That was so much better and more interesting than
9477280	9484320	his early stuff. Like he never stopped experimenting with a guitar, he could make it sing and dance.
9484320	9492720	Like, like some of it you can clearly hear in his earlier stuff, but that was just amazing. I was
9492720	9499760	like the Jerry Garcia line and we asked him what his limitations were. He said, everything I learned,
9499760	9507040	everything I know, everything I've played, like that, like that's an amazing thing.
9508640	9513280	And he was like, I really like his guitar playing too. But he also had a style and then he killed
9513280	9519360	himself with Harrow and Jesus. I don't know.
9519360	9524240	Well, I wonder if like this, the drug side of things too, it's just some reaction to that
9524240	9528000	inability, that trying to get outside, to get out of your own way kind of thing.
9530000	9535440	Yeah, it could be, some of what made him great was also what he was playing with, was with
9535520	9541200	Lewis and Jennings and Harrow and God knows what. That's a tough road to hoe though.
9542160	9546880	I think technically it was diabetes. They killed him, but he apparently would live on ice cream for
9546880	9553680	a month at a time. Okay. Jerry Garcia. Yeah. Yeah, he sued him over that too. That was pretty
9553680	9559040	funny. He named it after him. He literally killed him. That's right, because there's a big Ben and
9559120	9564880	Jerry is at the corner of Hayden-Nashbury right now. Oh, you can't even make it up.
9565920	9571200	But you know, that kind of combinational complexity and insanity is just a beautiful thing.
9572640	9579360	Like you couldn't, that's as weird and deep as you could possibly get at some level.
9580720	9587200	I mean, it's been really, really fun talking to you. You know so much about the arc of this
9587200	9593200	technology that's shaping everything about the world right now, and you're working so deeply on
9593200	9602960	it. And you have a really subdued optimism about it, which I really like. Subdued optimism. My nephew
9602960	9608400	said I'm a cynical optimist. I haven't looked at too much cynicism up here. I think that you
9608400	9614400	call things as they are, but it's just, I want to see a world where people are optimistic again.
9615280	9624640	Yeah, that's a great goal. And humans are good at being optimistic, but we are so attuned to our
9624640	9632720	environment. You know, like it sounds dumb, but there's winter, spring, summer, and fall in winter
9632720	9641280	again. And you know, we were very optimistic towards the length of the day and the growing of
9641280	9649040	everything. And you know, logically pessimistic, you know, at the end of the year, where you start
9649040	9654400	to run under here and work run out of stuff, and our environment and our culture and everything
9654400	9661680	that we create influences people's attitudes and stuff. And getting like, like even like a company
9661680	9669040	culture, like how do you get to growth mindset? And one reason I love working Freelon is I watch
9669120	9674320	them create it. Because it's like, we're going to save our so we're going to go to get a backup
9674320	9679520	playing it and we're going to do 10 of possible things. And we're going to work our asses off
9679520	9685280	to do it. And there's so much space now that's all this space. Are we going to make a rocket
9685280	9691600	better? Well, one set of people say you have to write a stupid or requirements document and outsource
9691600	9696560	it through government contractors and, you know, get a rocket that sucks for a lot of money. And
9696560	9700560	the other one says we're going to build a rocket and we're going to be able to do it. We're going
9700560	9705920	to keep doing it every day and we're going to make the machines. So they quickly ran out of machines
9705920	9711200	to make the rocket. So they had to make some machines to make the rocket and, you know, and
9711200	9720560	then, you know, something magical happened because now they can land rockets that take off. It never
9720560	9728000	looks really watching them take off and land. It just feels like the future. We don't have flying
9728000	9734080	cars, but we have reusable rockets and I feel like that's a decent. We're going to build flying cars.
9734080	9741600	Don't worry about it. It's coming. Be optimistic. I was in a like, I was at Tesla the first time
9741600	9746080	they landed two rockets next to each other and they set up big TV screen with a couple hundred
9746080	9751760	people went there and everybody's milling around and they, we watched it take off and then, you
9751760	9755760	know, the cameras are following them up and then they separate and then they come back
9756720	9762960	and then they landed. The whole place jumped up and down, cheered people hugged each other.
9762960	9769680	There was people, there was grown people crying. It was so fantastic and I felt like it was so
9769680	9774960	great to be part of that. And here's the crazy thing. We could all be part of that every day.
9776880	9782400	And I have to ask yourself, why aren't you part of that every day? Like how many,
9783920	9791280	how many things have impacted you in a way that, you know, we're all getting negged by society
9791280	9796960	and people are interested in, you know, negging us for their advantage. It's a, it's a crazy thing.
9798320	9804560	I think I'm less cynical because maybe I'm more accepting like the solution to
9806160	9813360	cynicism isn't to ignore it to be bitter. So you figure things out and then, you know,
9813360	9819760	to things like humans go through lots of cycles. They're often absolutely horrible,
9819760	9825440	but they create wonders and people aiming at wonders are way more likely to do something
9825440	9830240	good than people aiming at, you know, terrible things or restrictions or
9830240	9837680	the whole degrowth thing is crazy. Like, I know where it comes from. If you think it's November,
9837680	9840480	it's going to be cold for six months. It's a pretty good mindset.
9841760	9845120	There's, I think that there's something about California, like you're talking about
9845120	9851760	environment, having a huge influence on mindset. And California is a place that doesn't really
9851760	9860000	have winter. It has, you know, like, it has a sunny and it has a slightly wet period,
9860000	9864480	but it's just this kind of flat experience. And it was really interesting moving to a
9864480	9869440	place like New York, where all of a sudden you're confronted viscerally with the fact
9869440	9874000	that the seasons come and they change. And winters are always around the corner,
9874000	9878080	even when it's peak of summer, you're already thinking about that biting wind
9878080	9884320	in the, in the canyons. And so I, I don't think that it's incidental.
9884320	9885760	I was worried about the swampy summer.
9887520	9891920	Summer a lot more for me. It's just, there were times where it would be so cold and windy in
9891920	9896480	Manhattan that you would round and the wind would blow along the avenues and you would
9896480	9900160	round the building and it would just literally take your breath away, it would hit you in the
9900160	9907120	face and you're just like, and then you reenter it and you keep going. But the optimism that
9907120	9911680	California offers in this place where there's sun and there's grass and there's ocean,
9912640	9918320	I think that it isn't an accident that so much of the technologies that shaped the future is
9918320	9922880	coming from California, because it is a place where it's easier to be optimistic than perhaps
9922880	9928320	anywhere else in the country. Yeah, but on the flip side, then it also creates the comfort
9928320	9936720	stuff that humans are also not very good at. So not like, it's a, we're, we're curious,
9936720	9942800	we're a curious species, you know, and, and you have to be knowledgeable about what that is.
9944480	9950000	You know, the, when I was in Intel, there had been a 10% layoff, which by, you know,
9950000	9955200	Silicon Valley standards and companies with troubles with nothing. And like four years
9955200	9961040	after it happened, they all still talked about it. And, and that's partly because it was viewed as
9961040	9968720	unfair and poorly executed, which might have been true. But there was also a presumption of
9969840	9978320	stability and comfort, right, that had grown into the culture. And, and, and that kind of thing
9978320	9983840	could have been, you know, a reset and a positive journey, but it caused people to kind of double
9983840	9990400	down on protecting their turf and security and really dysregulated quite a few people.
9991840	10000240	And so that seems weird, but like even big organizations need proper spiritual leadership to
10002000	10005760	to navigate, you know, stuff like that. You know, there was a need for
10006560	10012320	change and reorganization, but it had like almost everything needs to be done skillfully.
10013840	10017360	Now, sometimes you can't do it skillfully because you just don't know. And, you know,
10017360	10021360	the famous World War II study where they evaluated like the quality of decisions
10021360	10029120	based on the time contemplating the decision. No, no, they, and they found there was no correlation.
10030160	10036400	So there was decisions made in weeks, decisions make months, decision making years, but the decision
10036400	10042160	making time was not correlated with the quality of the outcome. And the big difference was the
10042160	10048640	faster you made decisions, if it was bad, you found out faster. And so again, this is great
10048640	10056480	attention. So you need to have some framework and some like, you know, sensible guidance when
10056480	10062160	you're doing change on the flip side, you know, change needs to move at a pace such that you
10062160	10068880	can evaluate the failures of it to move on. And again, that's like, and they're both true.
10069440	10077360	And so, like, how do you be, you know, and human beings need to be able to live in a world
10077360	10084000	where many, many, many things are true. And then we're navigating a line along some set of these
10084000	10091200	constraints. But, but some things, okay, have more positive outcomes than other ones.
10092640	10096480	I mean, like maintaining a clear line for how those cause and effects
10097280	10101600	folds together is also really important, right? Because the worst outcome is to make a decision
10101600	10105680	and then to not really understand what the effect of it was, so you can never evaluate if you made
10105680	10111920	the right decision. Yeah, you see that kind of stuff happen all the time. Yeah, yeah, yeah.
10112720	10116960	When you want to learn stuff and make decisions and do experiments and sometimes fail,
10118000	10122640	then you really have to be willing to look at what really happened and why. And some companies
10123200	10126880	are going, like, we're going to make more decisions and fail when we need to. And then
10126880	10132560	they all cover up what the failures were, assign blame to the political parties out of power,
10132560	10138800	and, you know, it doesn't work at all. Oh, yeah. I like companies where like,
10139600	10144880	like Tesla had this idea, like doing the normal thing was failure. Thinking a risk was good.
10144880	10148560	And if it worked out, that's really good. And if it didn't work out, if you learn something,
10148560	10154400	that's good. As opposed to most places, you judged on the outcome. You know, if it worked,
10154400	10161120	it's good. If it didn't work, it's a failure. And which makes a lot of sense, unless you're
10161120	10167840	trying to do new things, in which case doing the same thing has doomed you to not improving.
10168560	10172000	I heard that culture really broke a lot of people though.
10172960	10174560	Yeah. Oh, yeah, it's really hard on people.
10177760	10181680	Yeah, no, I saw people at Tesla, like after four years, a lot of people be burned out,
10181680	10183920	but I tell you, they're way better engineers when they started.
10185200	10189920	So they kind of give questions, what's your goal? To have a happy, comfortable life or to have a,
10190720	10194000	you know, population of people that can do things, pick stuff up and get to work?
10195040	10195520	It's your goal.
10196560	10197200	What's that?
10197200	10198800	I said, what's your goal?
10198880	10202160	Oh, I like to do stuff. Like, I'm really curious about things. Like,
10202160	10205760	I'm curious what's going to happen. I'm curious about what people are going to do.
10207120	10210560	Like, currently, I'm running a science project that my company about,
10211360	10218720	can we achieve a high level of creative and productive output with, you know,
10218720	10224880	independent autonomous teams doing stuff? Oh, I'm into it. It's fun to watch people.
10225840	10229360	It's just in every possible way, I think I could, I could jump in there and help out
10229920	10234480	what if they fail, they'll learn a lot more and then, you know, we'll see what happens.
10236720	10241120	Yeah, I played the fuck around to find out video for the whole company in all hands.
10241680	10242320	Super fun.
10244320	10244960	What's that?
10244960	10246640	I said, I don't know what that video is.
10246640	10253520	Oh, I'll send you the video. It's a little walkthrough of a graph about, you know,
10254960	10257680	what do you have to do to find out? It's pretty important.
10258160	10259280	All right, we'll keep it in mind.
10260000	10263200	Well, then, then some group did, one of my teams did something,
10263760	10267680	and then it was a complete mess. And they were like, well, Jim, you told us to do this.
10267680	10271520	Like, you played the video. Like, what did you expect would happen? Well, I was
10272960	10276240	hoping you'd iterate a couple of times faster and figure it out a little.
10277440	10278960	Are you, are you still hiring?
10279920	10280160	Yeah.
10281360	10282160	Who are you hiring?
10282720	10289440	Oh, yeah, over the last year and a half, we had some like, let's say leadership
10290000	10293760	voids and we did a bunch of reorganization and smaller teams.
10293760	10302240	And right now we're, we're mostly hiring for like skill sets. We're hiring programmers and,
10302240	10305840	you know, like computer design is complicated. There's so many different skill sets.
10305840	10308800	Like people don't realize how many disciplines there are.
10309360	10312480	Like, even if you're an electrical engineer, a computer engineer, a programmer,
10313280	10315760	underneath there's a lot of differentiation.
10318480	10321520	So yeah, we're going to probably hire 50 people relatively short order.
10323040	10327920	And, and then when you hire people, it's really the coolest thing is everybody hire
10327920	10333040	changes your group a little bit. So you make a plan and then hire some people.
10333040	10338240	And then the new people change what you're doing and why and there's new ideas.
10338320	10341200	And sometimes it causes a reshuffling of how the org works.
10342400	10343280	How many people do you have right now?
10345280	10349040	Um, yeah, 10 students, 450 people.
10349040	10351760	Oh, so if you're hiring 50, it's like a pretty significant.
10351760	10353360	Yeah, it's a big chunk. Yeah.
10353360	10355360	Tom Xeminy, we're close to 30.
10356800	10357680	Yeah, it's super fun.
10358240	10358720	Very cool.
10360000	10361440	Well, I think we should let you go.
10361440	10363440	There's a lot more left on the table to talk about,
10363440	10366640	but I'd prefer to just have you back then to keep rolling right now.
10366720	10369760	All right. Hey, great to chat.
10369760	10371280	I was very curious how this would go.
10371920	10373680	Yeah, thanks, Jim. You're a fascinating dude.
10374560	10376320	Oh yeah, you guys too.
10377200	10379840	Keep it up. I, uh, I enjoy your guys content too.
10379840	10380800	Appreciate it.
10380800	10381840	Thank you so much.
10381840	10383440	Have a great rest of your day.
10383440	10384240	All right.
10384240	10384560	Thanks.
10384560	10385040	Take care.
