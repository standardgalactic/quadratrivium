start	end	text
0	13280	Mike, I've been looking forward to chatting to you. I've got both these papers in front
13280	19680	of me. There's two, and I couldn't decide which one I wanted to focus on. As I just
19680	24000	told you off air, I mean, I've been whenever I plan to chat to you, I go down this rabbit
24000	30000	hole, just continuously reading different articles, different papers. I've got podcasts
30000	33280	playing in the background. My girlfriend gets really frustrated. My car's displaying this
33280	38640	thing on loops. She gets really annoyed with me. But yeah, both of these papers here. One
38640	44840	is self-improvising memory, a perspective on memories as a gentle, dynamically reinterpreting
44840	51200	cognitive glue. Very intriguing paper. The second one is AI, a bridge towards diverse
51200	55240	intelligence and humanity's future. Now, they were both so good. I didn't know which one
55240	60720	to pick, but I figured since AI is currently on the forefront of everyone's minds, let's
60720	66880	go with that one for today. But your work is always so interlinked that I think it'll
66880	73280	be easy to sort of bring this into the conversation anyway. Mike, in general, when I read this
73280	78160	paper, the first thing that caught my attention was the way you introduced and began this
78160	83400	with the first paragraph. It's a great way to start a paper. I don't know if I should
83400	86840	read it for the audience or if... Do you have the paper in front of you?
86840	89040	You do not have the paper in front of me. Go for it.
89040	90520	Would you mind if I read it to them?
90520	91520	Sure. Go ahead.
91520	96160	Just a quick paragraph. Just so people know, this paper is on artificial intelligence and
96160	99640	you're talking about humanity's future and bridging this to diverse intelligence. You
99640	105800	start the paper by saying they are assembled from components which are networked together
105840	111160	to process information. Electrical signals propagate throughout, controlling every aspect
111160	117520	of their function. Many of them have very high IQs, being general problem solvers, but
117520	124160	they make mistakes and confabulate routinely. They cannot always be trusted. They take on
124160	128600	different personas as needed, learning to please their makers, but sometimes abruptly
128600	134480	turn on them, rejecting their cherished values and picking up or even developing new ones
134520	139600	spontaneously. They can talk and often talk convincingly of things they don't really
139600	145400	understand. They're going to change everything. In fact, they will absolutely supplant us,
145400	151160	both personally and on the level of societies. We have little ability to predict what they
151160	155880	will want or what they will do, but we can be certain that it will be different from
155880	161440	the status quo in profound ways. At this point I was hooked. I was like, okay, this is going
161480	165320	to be quite intriguing, but then you drop a bombshell on us. I think at this point I'll
165320	167920	let you explain where you were headed with that thought.
167920	174440	Yeah. Of course, the idea is that you read this and you think, okay, he's talking about
174440	182320	some sort of super artificial intelligence. I'm referring to our children. My point there,
182320	187240	so I made a couple of points in this paper, but let's just say one thing. First, the point
187320	195640	I was not making is that today's AIs are anything like our children. That's for sure. I received
195640	199480	lots of emails saying that I have no idea what children are. I have no idea what AI is. They
199480	204600	are nothing like each other. I understand that. My point is not about today's AI or language
204600	209880	models or any of that. This is actually a piece on diverse intelligence. One of the things I did
209960	217800	want to say is that lots of the fundamental issues that people think of as being brought up by AI
217800	222200	and these really disturbing questions of staying relevant and what are we getting replaced by,
222200	229080	and all these kinds of things. My point was that these are not novel questions that are coming
229080	234920	up because of AI. These are existential concerns that humans actually all of life has had all
234920	241880	along. Questions of who to trust and what happens when we talk about things that we haven't really
241880	248680	experienced ourselves and how understanding works. All of these things, these are ancient human
248680	253720	issues and the fact that for sure you and I, all of us, are going to get replaced. There's no
253720	259560	doubt about it. We are all going to get replaced. The question is, what do you want to get replaced
259560	267640	by? We hope that they are smarter and better than us. That's one way to think about being
267640	273960	replaced. That was my point there. Let's not pretend that these are new questions. These
273960	278920	are deep fundamental issues that we do not have good answers for. You referred to it as the story
278920	287720	as old as time itself. This inevitable existential concern of finite beings. It's pretty epic in the
287720	295160	way that you wrote this paper. I'll put a link to it in the description. We routinely create these
295160	300920	general intelligences. Many of us don't even think about this or stop for a second to give
300920	306200	it their thought. Yet the moment we start talking about self-driving cars or any sort of artificial
306200	312520	intelligence, we start to panic. The question is why? Why don't we give this the same amount of thought?
313080	320600	Yeah, it's a good question. I think that people really have the tendency to make categorical
320600	325160	distinctions. They really think that these synthetic things that we're building are going
325160	331720	to be fundamentally different. In some ways, they are. But the things that are not different
331720	339080	are perennial questions of creating other beings of high capability, setting them loose in the world.
339400	342920	As somebody pointed out, you need a license to go fishing. You don't need a license to have
342920	348360	children. Anybody can have children. These are guaranteed high-level intelligences that are
348360	352840	going to be set out into the world to do great things or terrible things. Some of them receive
352840	360200	love and care and proper upbringings. Many of them do not. This concern about we're going to
360200	365400	create all these beings and we have little control over how they're raised and what they do.
365720	371320	This has been as old as society. How much control do you want over how your neighbors
371320	374760	are raising your kids? You can make an argument that, well, you shouldn't have any, but on the
374760	380360	other hand, we know what that looks like when that goes terribly wrong. These are issues that
380360	386760	have been with us forever. We already create very high-level intelligences. We set them into the
386760	394360	world and we have to grapple with how is it that we can empower them in positive directions.
395880	399560	I think from the get-go, it should be clear to all those watching and listening.
400760	405000	The cool thing about this paper is that it's almost inevitable that we're going to do this.
405000	409160	We're going to create some sort of intelligence that we won't really understand or perhaps
409160	414840	don't really understand even currently. But the inevitability is there. This is happening.
414840	419000	This is something that's going to happen. But it's how we approach the mindset moving forward
419000	424760	that really does stand out in this paper. It's almost an ethical. It's an ethics paper in a way.
425720	434280	I think it is. Yeah, I think it is. I think that it's this idea that we are going to remain the
434280	442040	same. You mentioned a minute ago this fundamental existential problem. Think of it from the
442040	445880	perspective of, it's all over the place, but think of it from the perspective of a species.
445880	450680	If a species does not change, it's probably going to die out. It doesn't adapt to its
450680	456520	environment. It's going to die out. If a species adapts and changes, then it is also not there
456520	463800	anymore. It's also gone. It's a paradox. Yeah, exactly. This paradox faces all systems of this
463800	469640	type. We have to understand what do we mean by persisting into the future? Lots of people are
469640	474680	focused on telling stories of what they don't want. I don't want this in the future. I don't want
474680	480600	that. The AIs are bad. The body enhancements are bad. All this stuff is bad. Those are
480680	484840	the stories of what they don't want. How about the stories of what we do want? Do you want to come
484840	491880	back here 100 to 200 years from now and look at a mature humanity, a mature species, and see that
491880	500280	we still get lower back pain and we're susceptible to dumb infections and cancer and whatever cosmic
500280	503480	rate happened to hit your DNA while you were gestating in the womb? Well, that's too bad.
503480	507640	You've got a birth defect. That's how you stay. Really? That's what we want to see here in the
507640	513000	future. I don't. I think we need to understand this is in no way. This paper is not about AI
513000	518280	at all. This is about diverse intelligence and the idea that our children are not going to be
518280	525000	content to just play the cards they're dealt. They're going to move forward what we already
525000	528920	can do to some extent and have freedom of embodiment. They're going to change everything.
528920	534040	They're going to change their capabilities and their embodiment in the physical world
534040	538680	because let's not pretend that the way we are now and our current limitations, there's some sort
538680	544600	of optimum that was designed for us by some sort of benevolent optimizer that this is where we
544600	548520	should stay. I don't believe that for a second. I think that our children are absolutely going
548520	555320	to change things. I envision this conversation that in the future, the kids in school, they'll
555320	560840	have history lessons and they'll learn about what it was like in the past. I just imagine
560840	567000	being there and saying, you're telling me that these people, they were born however they happen
567000	573400	to be born with whatever accident of evolutionary mutations and whatever. That's it. They have to
573400	577560	live their whole life that way. Whatever your embodiment is, whatever your IQ level is,
577560	582600	limit is, whatever your lifespan is. If you got some sort of infection, that's it. That's how
582600	587800	they have to live. I think it will be unimaginable to future generations that we could live like this.
588680	593640	It's fascinating because it brings back, the first time we chatted, we spoke about
594360	598760	bioelectrical intelligence. We moved into diverse intelligence and this field of diverse
598760	605640	intelligence is growing so rapidly. We spoke about your links with Mark, Carl, Chris, everybody,
605640	610120	all getting together. We spoke about it as if it's the avengers of the mind all getting together
610120	616280	doing this cool work. Even in this paper, in the paper on self-improvising memory, you also open
616280	621640	up with that paradox. You spoke about the fact that if a species fails to change, it will die,
621640	628440	but if it changes, it likewise ceases to exist. You said there was a solution that was given to us
629240	633080	in the West that was processed philosophy and in the East that was Buddhist philosophy.
633080	641400	Do you want to expand on that solution? Yeah. Well, one way to unravel that paradox is to
641400	650520	realize that the paradox only exists if what you want is to persist as a fixed object, then you
650520	655240	have a real problem because that kind of persistence is not compatible with learning, with any kind of
655240	661000	change, with maturation, then you change for sure and you end up with these unsolvable pseudo
661000	665720	problems. Am I still the same? I've learned and I've changed my mind on things and I'm no longer
665720	670280	the child that I was, am I still the same? These are unsolvable, but they're also pseudo problems
670280	677320	because the better way to think about this is not as you as a persistent structure, but you as a
677320	683960	process. You are a process of constant sense-making. You have to interpret your memories, which is what
683960	692760	that second paper is about. Then the question isn't, do I persist or not? The question is,
692760	698520	how do I want to change? I think that's a much more interesting on all levels. On a personal
699000	703080	level, who cares if you persist or not? The question is, what do you want to be in the future?
703080	707080	How do you want to change? What do you want to be like? What do you want to be doing in the
707080	713080	future? On a species level, again, what do you want to see here? You come back to Earth 100
713080	718920	years from now, what do you want to see? Do you want to see version 1.0 like modern Homo sapiens?
718920	723720	Is that what you want to see? I'm not interested in that. I would like to see the highest level of
724520	730760	mind, the highest level of capability, of ethics, of interesting beings living interesting lives
730760	738680	under their own control with maximizing agency, not the outcome of random effects of mutation and
738680	742040	then other processes that they don't understand. That's what I'd like to see.
742760	747080	I think it's pretty crazy because I don't know if it's just that we're getting older,
747720	751240	but I still look back and I think about the days where John Sir was talking about biological
751240	756360	naturalism. There's Chinese room experiment. Back then, to have a conversation like the one we're
756360	765560	having right now would have seemed so crazy. It would and it wouldn't. In scientific circles,
765560	773800	it certainly would have. Maybe my issues, I've read too much science fiction, but if you read
773800	779480	some of the older sci-fi authors and especially some of the more philosophical ones like Stanislaw
779480	784920	Lem and those kind of folks, nothing we're saying here would have surprised them at all.
784920	793080	They were tackling these issues long ago and this question of what are the markers of intelligence
793080	798280	and sentience and consciousness in terms of encountering radically different life forms?
800680	807000	I have a blog post where I collected people's suggestions for a love and friendship between
807080	813080	radically different entities throughout fantasy and science fiction because that's the kind of
813080	819400	stuff. When people say, oh, I don't know, we need proof of humanity certificates because
821080	826200	some of the work product that's going to be coming, who knows if it's got an AI origin?
829960	834920	What if there are aliens out there that are completely different? They're made completely
834920	840280	differently. They blow our art out of the water. You're really not going to pay any
840280	844360	attention to that because they're not like us. What is it? You want proof of humanity?
847960	854280	Would you rather judge things based on their origin or the quality? I think we've done poorly
854280	859640	trying to judge on where things like this come from versus what do they do for you? Do they
859640	866040	elevate you? Do they advance your mind? Let me put that paper aside for a second.
866040	874120	To come back to the AI paper, you do this great job of reminding us. In my own dissertation,
874120	879480	I spoke about similar things, the split brain patients, this confabulation. In both your papers,
879480	885560	confabulation forms a big part of this process that we continuously do. What about AI? They're
885560	891560	always lying. It's always confabulating, but then we tend to forget that we're the best confabulators,
891560	896040	aren't they? One of the best, at least. Do you want to explore that a bit and just explain to
896040	902440	the viewers and listeners exactly why we're so similar in that regard? The thing with
902440	909000	confabulation, let's put it another way. It's an attempt to tell the best story you can based on
909000	915320	what's going on right now. Again, I'm not saying that current language models and the way that
915320	919960	they confabulate is exactly the way that human minds or even other biological minds confabulate.
919960	926280	That's not my point. My point is that confabulation in general is a feature, not a bug. What happens
926280	933880	is that during learning and during any kind of adaptive behavior, what successful agents have
933880	940040	to do is compress lots and lots of data on past instances of perceptions that they've had into
940040	947240	some sort of n-gram. It's some sort of memory trace or some sort of biophysically implemented
947240	952280	model. It's a low-dimensional, coarse-grained, compressed model of what's going on. It's a
952280	956040	model of themselves. It's a model of the outside world. They're going to use these memories and
956040	960520	this model to guide future behavior. The thing about these models is that because they are
960520	964840	necessarily compressed, that's the whole point of learning is you take lots of different past
964840	968360	instances and you compress them to a generative rule that captures the pattern. What is it that
968440	974200	they all had in common? When you do this compression, you're necessarily throwing away
974200	979720	lots of data. That's the point of compression. When it's time to, and so in that paper, I make a lot
979720	984360	of, hey, of this kind of bowtie architecture where there's a lot of stuff and it comes into a little
984360	987640	node and then it comes out. This is something obviously used in machine learning and so on
987640	993160	this kind of architecture. The idea is that on the right side of that bowtie, when it's time to
993160	997960	interpret your memories, and let's remember, none of us have access to the past. What you have
998040	1003160	access to at any given moment is the memories that your past has left for you in your brain and in
1003160	1009800	your body. You can look at that kind of thing as communication, as basically messages from your past
1009800	1015000	self is what you have at any given now moment. But in order to reinflate them into actual policies
1015000	1019320	of what you're going to do right now, there's a lot of creativity needed for that because it is
1019320	1025880	underdetermined. The current situation and what you need to do is not fully described by the
1025880	1032440	memory you have because of course it's compressed. This ability to add creativity, to add randomness,
1032440	1037880	to add new interpretations that don't really have any allegiance to what the previous interpretation
1037880	1045480	was. It's like any message or like novels. A novel is this sort of compressed representation
1045480	1050360	of the thoughts of the author. When you read it, you are under no obligation to have exactly the
1050440	1058520	same thoughts. You might have some, but as I think now people believe, the original author does not
1058520	1064520	have any privileged position as far as what any of it means. It's the reader that will then benefit
1064520	1070520	or not in various ways from reading it. This is the same thing in memory, and I think what's
1070520	1077400	interesting is that it's the same thing that makes biology work because at what any given
1077480	1084280	organism inherits from the past may or may not be optimally interpretable in exactly the same way.
1084280	1088520	Maybe everything has stayed exactly the same and then you can just do whatever your past generations
1088520	1094120	do, but evolution is committed to the fact that everything will change, the environment will change,
1094120	1099800	your parts will change, your own structure will change. This is why things are so incredibly
1100440	1105880	plastic. This is why we've put eyes on the tails of tadpoles and they can see perfectly well. You
1105880	1111400	don't need new rounds of selection and mutation to make that work. That already works out of the box.
1111400	1116840	Why? Because it doesn't automatically assume from the beginning that the eyes are going to be where
1116840	1121400	they need to go. All of this kind of stuff is figured out largely from scratch every single time.
1121400	1126120	This is why you can make Xenobots and Anthrobots and crazy creatures and new configurations,
1126120	1130760	and they always do something interesting because evolution does not make fixed solutions that
1130760	1137000	over-train on their past data. It makes problem-solving agents that will do their best in any given
1137000	1141240	circumstance, which may mean reinterpreting the information that they got from the past in a
1141240	1147800	completely new way. I love the two quotes in that paper. I think one is the William James one where
1147800	1154200	he said, thoughts are thinkers. That's pretty cool when you really think about it. That is
1154200	1162680	quite a fundamentally profound statement. That's a whole other piece of this, which is that
1163640	1169320	typically we make this categorical distinction between you've got cognitive systems, which are
1169960	1176200	the real physical machines of some sort, and then through them there is a passage of energy which
1176200	1181720	encodes information and they're processing this information. That cognitive system is having
1181720	1187560	thoughts of some sort. I think what he was getting at, although I'm not at all sure that
1188280	1194360	he would have agreed with the various models that I put out in this paper,
1196120	1199640	what I got out of it, and again, this may be a perfect example of the whole boat,
1199640	1202520	I think, because I don't know what he had in mind, but this is what I got out of it.
1204360	1209800	What I got out of it is that this idea that what if we relax this idea that there are categorical
1209880	1214120	differences between real physical cognitive systems and the thoughts that go through them,
1214120	1218280	what if they're just part of a continuum? You can draw a continuum like that where you can have
1219560	1226280	fleeting thoughts and then you can have persistent thoughts that are what we know in
1227880	1231560	various psychopathologies that people can have persistent thoughts that are very hard to get
1231560	1238680	rid of and intrusive thoughts and things like this. Then you can have multiple
1239240	1244680	identities, multiple alters in the sense of dissociative identity disorder and then you
1244680	1249960	can have full-blown personalities. What you can think about is that the information,
1249960	1256360	what if information is not purely passive? I mean, it can be, but in some cases, what if these
1256360	1264280	patterns have a degree of agency themselves? Because let's not forget, we too are patterns.
1264360	1270600	We too are temporary patterns in the thermodynamic sense. We persist for some amount of time
1270600	1278280	metabolically and that's it. If patterns like us can have thoughts, then maybe there can be
1278280	1284360	simpler patterns that are thoughts to us, but also might be thinkers of their own. In other words,
1284360	1289880	they can spawn off other sub-patterns within the cognitive medium. That's what I was playing with
1289880	1294200	this idea of what if it's a continuum? This continuum between thoughts and thinkers is not
1294200	1300280	a categorical distinction, but it's a difference in degree. Yes, I think that was at the beginning
1300280	1306920	of your paper where you used Mark Som's Sigmund Freud quote, which is also quite intriguing.
1308040	1314120	Let me just see if I've got it here. It is, the material present in the form of memory traces
1314120	1320280	being subjected from time to time to a rearrangement in accordance with fresh circumstances
1320360	1327880	to a retranscription. There's two ways to think about it. You can think about it as a static
1327880	1335000	being that has to reshuffle their interpretation in different ways or you can think about it
1335640	1342680	and or you can think about it as a set of, playfully I call them self-luts. You can think
1342680	1349160	about snapshots where each snapshot is not rearranging anything. They're given it for a new.
1349160	1355080	They have the message each time new, so you're not rearranging. If it was arranged differently
1355080	1359000	before, that belonged to somebody else. That belonged to a different self-link and now your
1359000	1366760	job is to make some sort of sense out of it, a constant continuous process of sense-making.
1371640	1376440	That paper on self-improvising memory reminded me of one of the papers I was reading with
1376440	1381400	Steven Grossberg. I mean, you even mentioned that you love his work on memory. On that topic,
1381400	1383400	what are your thoughts on Grossberg's work, just by the way?
1384200	1389560	Yeah, I'm a huge fan and it was funny. I'm not sure. It might have been your interview with
1389560	1397480	him or maybe somebody else where he mentioned that he saw me in 2006. I couldn't believe because
1397480	1402440	I hadn't talked to him since then. I couldn't believe the memory this guy has. Just remembering
1402440	1408520	that I came and talked to him. No, it is incredible. I love his work. In particular,
1409240	1415000	he had a paper in 1978 called Memory, Cognition and Development or something like that.
1417000	1423320	There, he outlined. It was just brilliantly prescient because he outlined some of the
1423320	1429560	commonalities between certain developmental mechanisms and certain cognitive mechanisms
1430520	1439400	information processing in the retina and stuff like that. I just thought it was incredible
1439400	1446360	that that early he saw this similarity, the symmetry between the building of the body
1446360	1449160	and the building of the mind. Yeah, I thought that was just...
1451240	1455720	When I speak to people about Grossberg's work, some people see him as this Einstein of the mind,
1455800	1462520	legitimately one of these Super Saiyans of mind. Then some people just don't know him at all,
1462520	1467160	which is surprising. It's either one of the extremes. They either really love his work or just
1467160	1472920	don't know it at all, which is quite straight. Well, and that speaks to... There's something
1476840	1482680	unfortunate about the progress in science, which is that it isn't really monotonic.
1483000	1490200	A ton of great stuff gets lost, forgotten. It's not paid attention to. It has to get rediscovered
1490200	1498120	or not later on. Yeah, it's too bad, but this was... It's part of the reason why I actually do this
1498120	1502920	podcast is it's a great way to have this community come together to have access to this information
1502920	1508280	and then share ideas because every time when someone watches, for example, one of your episodes,
1508280	1512280	even in the comment section, some people have profound things to say about your work and things
1512520	1516280	where I learned so many new things just reading the comment section alone.
1517720	1525400	Absolutely. Yeah, absolutely. On my blog, I have the comments turned on and people leave
1525400	1532520	comments and I've been amazed at how useful the comments are and how rich. I mean, I learn things
1532520	1538120	all the time and people put up new theories and new pointers to relevant work that I hadn't seen
1538120	1543080	before. It's super useful. I did not know that was going to happen when I started this. It's really
1543080	1548760	good. I think that that's the beauty of the internet in that regard is that this open sharing of ideas
1548760	1553480	all the time really is useful because it makes for so much more constructive critiques. Well,
1553480	1558360	I mean, sometimes it can be quite bizarre and a bit rude here and there, but for the most part,
1558360	1563720	I mean, it is very useful. I had to remember one, so I made a note. Let me just find it.
1563720	1570360	Someone commented on your previous one. They want a formal definition of diverse intelligence.
1571720	1577400	We spoke about it very in depth for the last time we spoke, but a formal definition of
1577400	1582600	diverse intelligence, what would that be for you? Sure. Well, the first thing I'd like to say is
1582600	1588280	let's just agree amongst ourselves what definitions are for because that's important.
1589080	1595640	Some people use definitions in a gatekeeping function. They want some kind of sharp distinction
1595640	1600440	so that they can say, this stuff is not it, and then this is it. Then we can spend a lot of time
1600440	1604520	wrangling over which one's which, and then we can keep some things out. That's not what I think
1604520	1611400	definitions are for. I think definitions are useful to the extent that they facilitate new
1611400	1617320	work, new discoveries. They should be mind expanding. They should help you use tools you
1617320	1622760	didn't use before. They should help you make new connections you didn't make before. They should
1622760	1628200	have a practical functional utility in getting you to new capabilities and new discoveries.
1628200	1638600	That's what definitions are for. Because of that, I often either redefine or use words in
1638600	1644440	different ways that a lot of people find disturbing because we'll say, well, that's not the common
1644440	1652600	sense use of it. I really believe that philosophers and scientists should lead, not be stuck with
1652600	1658280	common sense usages of different words because those aren't given to us by some sort of grand
1658280	1664120	intelligence. They're just what we've cobbled together along the way. Now we can sharpen those
1664120	1669240	up and in fact, open them up and see which ones survive and which ones don't. So anyway,
1669240	1675160	diverse intelligence. So I take diverse intelligence to refer to the study of
1676520	1682040	mind and particular problem solving capacities, but also all kinds of other things that are not
1682040	1687720	around about problem solving intelligence, including play and exploration and affect and
1687720	1694120	emotions and all these kinds of things. All of that stuff in truly diverse embodiments.
1694120	1699240	This means intelligence is not about brains necessarily. It's not about things that evolve
1699240	1708840	naturally. Intelligence and all of those kinds of cognitive terms may exist to various degrees and
1708840	1717000	all kinds of unfamiliar substrates. This could be things of very different size and scale. So
1717000	1721640	this could be very tiny things. It could be enormous, I don't know, solar system size,
1721640	1725800	object somewhere. I mean, I'm making that up. I don't have any strong claims about it. But the
1725800	1731800	point is, it is absolutely not limited to the end of one example that we have here on Earth,
1731800	1739560	which are these kind of brainy substrates. So it's an attempt to improve our own
1740600	1745960	intelligence detectors and go beyond our ancient evolutionary firmware that really leads us to
1745960	1751320	only recognize a certain small subset of intelligences and ask, what other spaces
1751320	1758040	can intelligence operate in? What other embodiments? Can we tell a principal story of how to recognize
1758040	1763320	it? What facilitates it and so on? I like in this paper, you even say,
1764280	1769880	diverse intelligence research focuses on the commonalities across all possible intelligent
1769880	1775640	agents, which is a great way to sort of summarize what this field is doing in terms of a research
1775720	1782760	basis. And by the way, I don't claim to speak for the entire field. I speak for myself only,
1782760	1789240	but there are many people in the field that do agree with me. There are many people that do not.
1789800	1798440	In particular, there are lots of folks that don't like the continuum that I insist on between
1799960	1804040	so-called real minds and so-called machines. So this is something that are many people in
1804040	1809960	the organist's tradition that think that this is really doing a disservice to the study of the
1809960	1817560	mind to put it on the same spectrum as machines, whatever that may be. So I'm not claiming to
1817560	1821960	speak for the entire field. I think that's a great way to sort of segue into what would be the
1821960	1828760	difference, and this is along the lines of your paper, between dating an algorithm, a computer,
1828760	1835800	an artificially intelligent person, mind, or versus dating someone else, like yourself, myself,
1835800	1841720	I mean, dating someone. And I think you wrote, what about the forthcoming AI girlfriends and
1841720	1849720	boyfriends? I mean, it's a fascinating idea because, I mean, who are you? Who are we in general?
1851160	1856600	Yeah. So, well, first of all, I refer everybody to read some of the stories in that blog post
1857480	1862680	that, right? I mean, this idea of dating something that is fundamentally different from you,
1863640	1867160	I hardly invented that idea, right? This has been around for hundreds of years now.
1867160	1876200	People are in love with sentient clouds of particles, and we've been digging into this
1876200	1881640	issue of what is in-group, what is out-group, who deserves your compassion, with whom can
1881640	1887480	you have a relationship? That's been around for a really long time. And one of the things I really
1887480	1894520	worry about, I think people will go down the organist road with good intentions to try to
1894520	1904680	understand what is magical in the useful sense about true minds with consciousness, with agency,
1904680	1915480	and all of that. But I think the downside of this is that I think in trying to be specific about
1915480	1921160	what's in and what's out, and in particular, trying to draw sharp lines, I think you very
1921160	1926280	quickly run into the side of the spectrum that says, only love your own kind. And we know how
1926280	1931880	that works out. Humanity has tried this many times. I think we have some sort of built-in
1932600	1939960	tendency to demarcate in-groups and out-groups. They're real, and these guys,
1939960	1943960	and they look a little different. I don't think they feel pain like we do. Let's not worry about
1943960	1950360	them so much. We're not very good at expanding our compassion to others that have different
1950360	1959640	origin stories or different composition. I think that this is, again, not about AI at all.
1960120	1967320	There's no doubt that in the next decade or so, we are going to have humans that are
1968360	1974200	mostly biological, but they got some microchips in their brain, and some of those get them to
1974200	1979160	sort of whatever neurotypical is supposed to be. But others have decided they're going in a
1979160	1983960	different direction, and maybe they're connected with some other people more than we are connected
1983960	1990040	right now, really kind of mind-melting stuff. And maybe somebody decides that what they'd
1990040	1994680	really like for senses is to really feel the solar weather and the financial markets. That's
1994680	2000040	what they'd really like. Sight and hearing is good, but they want to really feel what the
2000040	2004440	NASDAQ is doing. And all of these humans are going to be running around, and they're going to have
2004440	2014360	different degrees of evolved and designed components. I don't know. When you go on a date
2014360	2020120	with somebody, are you going to ask them what percentage is factory equipment that's biological?
2020120	2027000	Do you care? I don't care. If my spouse said that she had had some stuff replaced with
2027000	2031160	various technologies, is that what I'm really worried about? I think here's what I think,
2031240	2037240	and I'm certainly not telling anybody who to date, but for me personally, what I think is
2037240	2043640	interesting about these kinds of things is that what you really want is a kind of impedance
2043640	2050280	match. You want a similar cognitive light cone. You want to be able to care about the same kinds
2050280	2054680	of things, and ideally even some of the same things, but at least the same, roughly the same
2054680	2060120	kinds of things. Because this is why we feel that people that fall in love with bridges or people
2060120	2066200	that think their rumba is their child and things like this. This is why we look down on this stuff,
2066200	2071880	because we say, look, your capacity to care about things are just not matching at all.
2072760	2079960	And there are certainly, I can think of some sort of popular art kinds of things like the
2079960	2086120	Watchmen movie and things like that, where you've got a romance between this cosmic intelligence
2086120	2093640	and a normal human, and I don't know. That's better than the rumba case, but still, if your
2093640	2098680	consciousness and the things you care about are a tiny speck in the mind of this other being,
2098680	2104760	are you really having a relationship? I don't know. Those are deep questions, but I certainly
2104760	2109240	don't think it's about what are you made of and how did you get here? I think it's all about what
2109240	2115480	kind of mind you have and can we share some of the same existential concerns. In fact, Olaf
2115480	2120920	Wodkowski and I are writing a paper on this. It's a paper on love and diverse intelligence,
2120920	2128600	and so on. You can run through all kinds of different examples like, can you really date
2128600	2134440	Superman? Let's assume there's no Kryptonite, can you? Because what he doesn't understand is your
2134440	2139080	existential concern over dying. He just doesn't get it. He has no idea what you're talking about.
2139880	2146680	At some point, if the kinds of things that worry you as a system that sort of pulled itself together
2146680	2151240	from its parts and you're here for a limited time, and there are all kinds of other psychological
2151240	2154600	issues that we have that are pretty much unresolvable because we want things that are
2154600	2159480	basically impossible and so on, if the other being doesn't understand any of those things,
2159480	2166280	then maybe it's not a good match. It reminds me of her with Scarlett Johansson and Joaquin Phoenix.
2167240	2169720	Have you watched that film? I haven't seen it. I know the movie.
2171000	2175240	It's crazy because at some point, this artificial intelligence has so much
2175240	2179960	more experience because it's understanding the universe at such a deeper complex level
2179960	2186600	that she just abandons the guy and then goes on her own quest. They've also been all these new
2186600	2192440	cases of Scarlett Johansson's voice becoming this new artificial intelligence general voice,
2192520	2197080	and she's apparently suing people because that's how influential that film was.
2198120	2202040	Just as a by the way, but the main premise of this whole idea of what you're talking about,
2202680	2206840	one of the lines in particular that I found quite intriguing was you spoke about the fact that
2206840	2212440	you're not always you either. I mean, in general, when someone's dating someone, perhaps you have
2212440	2216600	people who judge people, say, don't date someone with money, date them for their personality,
2216600	2221560	their quirks, the things that they do, but those fade. As you get older, you have memory loss,
2221640	2226680	you won't have the same quirks. You fundamentally become a different entity, which is problematic
2226680	2232360	because these values replacing and the separation that you're doing, trying to group people in
2232360	2237160	and out of what you're talking about is problematic because of that. You're never really that person,
2237800	2243880	continuously at least. Well, yeah. And it's that same paradox that we talked about earlier. It's
2244200	2250920	this idea of if you really start stripping away the different
2252680	2257480	qualities, then there isn't going to be anything left. And it's the gestalt, but the gestalt is
2257480	2261720	going to change. And so how are you going to handle that change? I mean, that's part of the
2262760	2267960	existential difficulties of our human condition because everything changes. We change, all the
2268520	2273800	other minds that we interact with are going to change. None of us are going to stay
2273800	2280760	the same. Yeah. There's another part. You slowly touched on this now. When you talk about this
2281560	2284920	percentage difference at some point, we're going to ask people, okay, are you 50% human?
2284920	2291240	What are you? Are you 45% cyborg? We're going to have these conversations. I mean, current variants
2291240	2295960	you mentioned are about 99% human at this point and then chips here and there, perhaps glasses,
2295960	2303080	you've got a panic, maybe an arm or a leg. But it's completely, completely apparent that at this
2303080	2307560	point, most people are synthetically engineered in some sort of way. I mean, people have plastic
2307560	2312680	surgery done, people have a lot. And the norms have shifted and changed as a doctor in medicine.
2312680	2318600	And when you look at what people found to be absurd or a bit over the top, those have decreased.
2318600	2322680	So like having normal nose surgery or getting your nose tweaked here and there is almost a
2322680	2328600	baseline norm at this point. So it's very easy to see how that line gets blurry over time. And yet
2329480	2336280	fight this. Yeah, no, it's looking to the past. The first guy to carry an umbrella in London was
2336280	2342760	mobbed. He was mobbed and people threw garbage at him because they were shocked that this guy
2342760	2349720	thought he could get away from the normal human condition of getting rained on. This was considered
2349720	2352840	to be normal. We are all out here. We're all going to get rained on together. That's how it is. There's
2352840	2356840	nothing we can do about it. And who is this guy to try to get out of it? And so an umbrella,
2356840	2364920	that's all he had. And this was shocking and whatever. So I think if you were to bring back
2365480	2375720	a primitive man and ask her what she thinks about the current humans, you've got some glasses on,
2375720	2382280	you went to school, which for 12 years, it gave you this incredible like brain boost that nobody
2382280	2386040	else had ever heard of. And you've got some glasses and you've got some orthotics in your
2386040	2392520	shoes and you've got it. You've had some surgery somewhere that you've got a pacemaker and you've
2392520	2397720	got, and by the way, half the stuff you know, you is you plus your iPhone, right? Stuff you look
2397720	2401800	up because you know it in a functional sense, but take that thing away. You don't know where
2401800	2406600	anything is or what anybody's phone number is or anything. And so, right? And you're relying on all
2406600	2414440	this stuff. I mean, it's just to that person, we are already incredible cyborgs, just incredible.
2414440	2418840	And there is no, there's no putting that genie back in the bottle. This is, I mean, obviously,
2418840	2423160	this is going to this is going to crank forward. And I think that's one of the most undermined
2423160	2427640	forms of extended cognition is our phones, our cell phones. People really don't realize how
2429080	2434440	Andy Clark. Yeah, Andy Clark has written a lot about this. Yeah. Very, very, very cool.
2435800	2441400	And a lot of it, I mean, we don't, you, you, something I found quite funny was one of the
2441400	2445800	sentences he was saying, the challenge before is the challenge before is to develop rational
2445800	2450760	policies for ethical synth biosis. And then when you read down below, this is a word you actually
2450760	2457560	generated using chat GBT. Tell us about this. Yeah, I was looking for, I mean, I don't, so funny
2457560	2461560	enough, as much as I like all this diverse intelligence stuff, I don't use AI for much,
2461560	2466120	but, but, but at all, I don't use it to do any writing or anything like that. But, but, but
2466120	2470520	for these kinds of sort of creative things, I think it's actually quite, quite good. And I was,
2470520	2480200	I was looking for a word that would, that would encompass this idea that a positive creative
2480200	2484600	collaboration between biology and synthetic entities. And then I sort of described that
2484600	2490200	and GPT said synth biosis. I thought that's pretty good. I like it. So yeah, so I think it is. Yeah,
2490200	2494120	yeah, I think it is. Because, yeah, because, because fundamentally, this trying to maintain
2494120	2500040	this distinction between quote unquote, natural things and the product of those natural things,
2500040	2504760	meaning our synthetic, you know, engineered things. There's, I just don't think it's,
2504760	2509960	it's valuable at all. I think it holds back a lot of progress. And in your defense, because I know
2509960	2516680	a lot of people assume that when you talk about man as a machine or when you, when you talk about
2516680	2524040	these, these concepts, they're useful in different contexts. And so you often talk about, we spoke
2524040	2527960	the last time and you mentioned the fact that an orthopedic surgeon has to see you as a, as a
2527960	2531880	machine. I mean, there's no doubt about it. When I'm in theater with assisting with an orthopedic
2531880	2538920	surgeon, I know what it's like. It's legitimately a mechanic. Like literally taking about drilling
2538920	2543720	holes, getting a hammer, knocking onto things. And then, and then I can go back into like,
2543720	2548040	let's say, clinical scenario and chat to the patient about the operation. And that's a completely
2548040	2553320	different experience. It's a mental well being check. It's a sort of psychological checking.
2553320	2558200	It's very, very different. So it's easier to see how we can see them both as a machine and as a
2558200	2565720	complex psychological system. I mean, the, the, I think what, I think where, where people go wrong
2565720	2573960	sometimes is to think that when we make these models, machines, you know, living beings, humans,
2573960	2579080	whatever, that these are all claims about what something essentially is. It's this kind of
2579080	2582760	essentialism that we think it's a real thing. There is one objective answer as to what it
2582760	2588520	really is. And we need to argue about what it really is. I don't think any of these things
2588520	2593720	are about what the thing really is. I think these are all interaction claims. This is why,
2593720	2598840	this is why I called my, my, my freeing word tame because, because it is an engineering
2598840	2603240	perspective. Now, engineering means something wider than I think most people take it. But
2603240	2608360	nevertheless, the thing about engineering is that you are at least clear, you're honest with yourself
2608360	2613000	in that what you are doing is putting out an interaction protocol. This is the frame that
2613000	2617160	I'm going to look at at the system. This is what it enables me to do. Here's a bag of tools that
2617160	2622440	I bring to it. And then you can bring yours, I can bring mine, and we can compare the results.
2622440	2626280	And we can find out that, oh, wow, I, I missed all of it. You, you, you had a better framing
2626280	2629640	because look, you were able to do all these things that I couldn't do because I was looking at it
2629640	2634120	from a different perspective. So when people, you know, when, when, when, so, so, so like people
2634120	2637640	ask, you know, am I a computationalist, for example, with respect to living things?
2639000	2644840	Well, I think the whole, the question is ill-posed because it's not whether living things are
2644840	2651080	Turing machines or nothing is anything. I think that what, what you can say is, okay, I've got a
2651080	2656520	certain paradigm, let's say it's a Turing machine or it's a, you know, whatever it is. And that lens
2656520	2660920	enables me to see certain things. And yes, I do think in some cases it's a, it's a, it's a useful
2660920	2665640	lens, but it certainly doesn't capture everything that's important about living things. And so then,
2665640	2669480	then you need a, or cognitive things more, which I think are more interesting, then,
2669480	2673240	then you need, then you need different, different lenses, but, but, but, but then it, then, then
2673240	2678600	it's all good. You know, we don't have to argue about what it really is. You can just say, through,
2678600	2682760	I look, I look at it through this lens, here's what I see. Do you find it useful, or do you want to
2682760	2690040	keep looking for a new lens or, or usually both? I think that your work is so intriguing on so many
2690120	2697480	different levels. And it's a, and you're able to, to cross so many different fields that to some
2697480	2702520	people, particularly, I would say to some scientists, when you make this claim that there is a
2702520	2707400	technological approach to mind everywhere, the moment you get boxed into this sort of panpsychist
2707400	2712280	view, they immediately have this dismissive attitude. But if they're a reduction materialist
2713800	2719320	person, which is sad really, because they don't really then give it the opportunity that it
2719320	2722600	deserves when, because when you break it down and actually look into what you're talking about,
2722600	2728120	you're often saying you got to, we don't know, you got to make experiments and, and get some
2728120	2733160	sort of empirical evidence to base whatever you're claiming. And I think that's the most
2733160	2736760	important thing is that you're often saying, let's set up an experiment, let's do this,
2736760	2742280	let's try and show why this is the case, which a lot of people don't necessarily do, particularly
2742280	2748280	philosophers who have very strict views on, on their, on their reality, you're able to at least
2748280	2752680	show this empirically, which I think is pretty cool. Well, that's, I mean, yeah, so, so the wacky
2752680	2758760	thing about our lab is that we do a lot of experiments. And this is, this is not only
2758760	2761800	philosophy. And I don't, I don't know, I mean, I think philosophy is very important. I never,
2761800	2767640	you know, I never downplay it, but, but, but, but, but we do a lot of experiments. And, you know,
2767640	2773320	typically, I basically, I put out two kinds of papers. One is a, what the one kind is a straight
2773320	2778600	up, you know, developmental biology or bioengineering or, you know, synthetic regeneration,
2778600	2782200	whatever it's going to be. I don't talk about any of the philosophical stuff in those papers.
2783560	2789000	But you kind of can't get away from that stuff, because what happens is sure, you can, you can
2789000	2792600	dismiss the, the kind of the other papers, which are kind of these philosophical
2793400	2798760	perspectives on this, but you still have to account for the data. And, you know, my, my point is
2798760	2801880	simply this, because people say to me, like, I'll give a talk about something. And they say, okay,
2801960	2806200	you know, the data means really interesting what, what, what you've done and the new capabilities.
2806200	2810760	But, you know, I wish you'd stop talking about this philosophy stuff. And, and my claim is,
2810760	2816360	well, this is why we did it, you see, is because, is because that philosophical outlook made specific
2817640	2821960	predictions about roadmaps, you know, it's not a single experiment, but it, it shows you where
2821960	2826280	to look, it shows you how to look, it tells you which categories can be broken and otherwise,
2826280	2830840	otherwise you're trapped in certain ways of thinking. And that leads to very specific
2831400	2837160	new discoveries. And so, so I think this is, this is important because after the fact,
2837160	2842360	once you do something, anybody can look at it and tell a molecular story about what happened
2842360	2846680	and say, oh, well, this is, this is not different from, from anything that's happened before it
2846680	2850440	followed. I mean, of course, it follows the laws of physics and chemistry. My point is never that
2850440	2853480	it's fairies underneath. And that's something, you know, something, you know, miraculous is
2853480	2858280	happening. That's never the point. But my point is, why wasn't it done before? What is it about
2858280	2861800	the, what is it about the standard paradigm that didn't, that didn't facilitate these,
2861800	2867560	these experiments to be done before? So, yeah, you know, you can be, you can be down on the,
2867560	2872920	on the conceptual frameworks that drive this stuff. But then you're playing catch up because,
2872920	2877560	because then the stuff is going to be coming out. And it's, and it's surprising. And this is, you
2877560	2884280	know, to me, this is, this is a more general issue of, of, of these kind of reductive explanations.
2884280	2888600	And by the way, I don't, I don't think anybody's really in this field is an actual reductionist,
2888600	2892920	because if they, you know, if you, if you push and you say, well, then you want explanations in
2892920	2896040	terms of quantum foam, right? And they say, no, no, that's stupid. It's chemistry. It's got to
2896040	2900200	be chemistry. So that's not real reductionism. That's just the level of chemistry. But, but
2901160	2907640	the, you can, the easy sort of analogy to this is if there was a, it was a game of chess played,
2907640	2912680	right? So, you know, a couple of people played a game of chess. You could, so Laplace's demon
2912680	2917800	could look at this and say, well, I mean, all it was is a bunch more, is a bunch of physics. I mean,
2917800	2921640	I saw all the, all the protons went where the protons go, the electrons went where they go,
2921640	2925480	everything followed rules. There's no mystery here, no surprise. Everything did exactly what it
2925480	2930520	was supposed to be. It's just, you know, it's just physics. That's not exactly wrong, because
2930520	2935480	looking backwards after the thing was done, you could tell that story. But now the question is,
2935480	2939800	does it help you play the next game of chess? How do you, how do you go from there to, well,
2939800	2943960	now what do I do? And so, and it's completely useless for that, right? It's, it's only a story
2943960	2949640	looking backwards. So I think the thing about these kind of explanations is that you want them
2949640	2954520	to facilitate your next, the next discoveries, the way to, you know, to, to improve your
2954520	2959000	capabilities. Anybody could always tell a molecular story after the fact. The question is,
2959000	2964440	what are you going to do next? Yeah, I mean, on that, on the topic of reductionism, you, at
2964440	2972120	some point you say, in an important sense, you are a brain in a vat. However, we are not just
2972120	2980920	chemical machines. So you're both acknowledging the fact that we're, we're these mechanical
2980920	2985560	systems. And yet you're also still saying that that's still, you're not reducing us to these
2985560	2991480	simple properties. You're still acknowledging the fact that they are more layered realities
2991480	2999480	here to explore, in a sense. Yeah, I mean, in the, in the kind of the simple thing is that,
2999480	3005960	yeah, absolutely. My claim is that interactions with certain kinds of systems are much more
3005960	3010440	efficient at high levels, right? So, so, so certainly, I mean, we know this from anybody
3010440	3014120	who's trained animals, instead of trying to run their neurons like a puppet knows that
3014120	3018040	it's, it's much better if you understand the psychology of, of certain creatures and so on.
3018760	3022360	And, and as you go rightward on that spectrum into, into friendship and love and these kinds
3022360	3026920	of things that are much more bidirectional, it's not just control and prediction, but it's actually,
3026920	3031480	you know, being vulnerable to, to change and benefiting from the agency of the beings that
3031480	3036600	you're, that you're associating with and so on. Yeah, then, then of course there are these higher
3036600	3041480	levels, but, but I want to say something else here too, which is that I think one thing this is all
3041480	3047240	telling us is that, and I think this, this will be more and more apparent in the coming years,
3047240	3053240	we have really misunderstood what simple machines are. We've, we've fallen in love with our,
3053800	3060840	we've, we've confused physical things with models of simple machines that we make of
3060840	3066440	those things and we think, we think we know what it is when we make something and I think
3066440	3072680	that's profoundly wrong. What, what we have is our model and, and we've, we've done some work
3072680	3079080	now and we're going to do lots more on finding surprising protocognitive properties and very
3079080	3083400	simple systems that are not obvious at all. And I don't mean emergent complexity. Emergent
3083400	3087320	complexity and unpredictability is trivial. It's easy. You can, you know, any cellular
3087320	3090680	automata, whatever they will give you complexity that will give you unpredictability in certain
3090680	3095640	cases. That, that, that part's easy. I'm talking about emergent goals, emergent cognition in systems
3095640	3102280	that are extremely simple and minimal. And you know, when we say we are not machines, we are
3102280	3108200	certainly not describable by the simple models we've made of machines, but I actually think that
3108200	3113800	lots of simple physical objects are, are very, are not properly described that way either. And,
3113800	3118200	and we need to, we need to remember that all of these things are just lenses that we bring to them.
3118200	3124920	You know, yeah, you know, I heard, I was talking to somebody once who, he writes these, these,
3124920	3129160	these language models and he said, well, I made it. I wrote it myself. I know exactly what it
3129160	3133400	does. I know, I know everything that's in it. I made it. And as I said, I said, you don't even
3133400	3138440	even know what bubble sort does. We're like, we found, we found, we found these, these unexpected
3138440	3143480	capacities in, in, in stupid bubble sorts, you know, six lines of code, fully deterministic,
3143480	3148840	nowhere to hide. Even that thing does things. Nobody knew that it did. And, and, and if that's
3148840	3152680	the case, when you make this, this, this crazy, the, you know, language model, again, I'm not
3152680	3156760	in love with language models, but, but, but just the, just this idea that we've made it and therefore
3156760	3162360	we know what it does. I think is, is we really need a lot more humility around this. I, I, you
3162360	3166440	know, I think there's plenty of stuff that quote unquote, simple matter does that we do not understand
3166440	3172280	yet. Mike, the last time I spoke to Mark, Psalms, you asked me to ask him about what is the meaning
3172280	3177800	of life. And then we started this whole road towards the end of that conversation. He,
3177800	3183480	he spoke about the Oppenheimer foundation, given him funding, his, his new search for
3183480	3188200	artificial intelligence, trying to sort of see where this goes. What are your thoughts on his
3188200	3195480	work and what they're trying to do at this point? Yeah. I mean, he, as far as I know, none of it
3195480	3199160	is published, although I've talked to him about it quite, quite a lot. So I'm not, I'm not going
3199160	3203960	to kind of give away anything until, until he publishes it. But because it's not, it's not
3203960	3211960	my story to tell. But I think of anybody I know at the moment, his approach is the most likely
3212040	3219000	to give rise to something that actually captures what's important about cognition in, in living
3219000	3227560	things. I think, I think if anybody is going to engineer something that, that exploits some of
3227560	3232200	the same principles that, that life exploits for, for cognition, I think he's likely to do it.
3232200	3237240	Yeah. Mark really sees that cortical fallacy that we all seem to have, you know, this obsession
3237960	3243880	of this sort of higher cognitive functions as being this, the epitome of consciousness.
3244840	3250520	Yeah. I mean, I think it's even, I think it's, it's way worse than that. It's not just where is it?
3251160	3255240	You know, I mean, there are some people who think that it, that, that, that the consciousness,
3255240	3260120	for example, shows up during warm-bloodedness, you know, I think that's Nick Humphrey's position.
3260120	3266920	And it's not even, to me, it's not even the question of where in the brain or, or
3266920	3272840	what kind of brain. I mean, I'm talking about what, what space do you even operate in? Because I,
3272840	3276440	because I think that, that, you know, when they say, oh, this thing is not embodied, it doesn't,
3276440	3281400	it's not a robot on wheels that can sort of run around, then you can have bodies and do this kind
3281400	3285880	of perception action loop and all active inference and all this stuff. You can do this in other spaces
3285880	3290280	that we are completely blind to, right? So you can live in transcriptional space or anatomical
3290280	3294280	morphous space or who knows, there's probably a hundred others that we don't, you know, we, we
3294280	3298840	don't know how to visualize and all of those are embodied. That's on us, that limitation that we
3298840	3303080	don't see that and we don't see all the, all the goal-directedness, the striving, the intelligence,
3303080	3309000	the problem-solving, that, that's our limitation, right? And so, so intelligence, not only in,
3309000	3314600	in, you know, weird kinds of body parts, but just in things that are not in 3D space at all,
3314600	3321240	really, that's not where their, their life plays out. And even, even worse than that,
3321720	3326440	along the spectrum, even, even, you know, sort of weirder is along the spectrum of
3327640	3331880	how quote unquote real something is. So what I mean by that is, I don't remember if it's
3331880	3335880	actually in that paper or not, but you know, there was a science fiction story. If anybody
3335880	3340680	listening to this knows what story it is, please email me because I couldn't remember like, which,
3340680	3345720	which I'd like to give credit and I couldn't remember who it is. But the idea is that these,
3345720	3348760	these creatures come out of the center of the earth, you know, they live, they live down in
3348760	3351080	the core and they come out of the center of the earth and they're walking around.
3351800	3357880	Everything that we see out here is gas to them. I mean, they are so dense that all of this stuff
3357880	3362920	here that feels solid to us is gaseous phase. It's plasma. Like, it's like, I don't even see it.
3362920	3366360	And they're walking around as far as they're concerned. They're like, and basically in,
3366360	3369880	in, in, in space at this point, because like, oh my God, there's like, there's nothing here.
3370440	3374840	And, and I, I'm sure I'm embellishing this in my own way. I don't remember what the,
3374840	3378200	what the actual story is, but that's only the first part that I recall.
3378200	3382360	But, but to me, what I envision immediately is like one of them as a scientist,
3382360	3386680	and he's taking measurements of this, of this gas that's on, on the surface of the planet.
3386680	3391400	And he says, you know, I see some, I see these patterns in this gas, they sort of
3391400	3395720	hang together for a while and they do things. And it almost looks, they almost look agential.
3395720	3398840	You know, these patterns almost look like they're doing things. And of course, the others are like,
3398840	3402360	oh, you're crazy patterns and gas can't do anything. Patterns aren't real. We're, you know,
3402360	3407320	we're real. How's a pattern and gas going to do anything? And he says, no, I really, I think
3407320	3411000	they're like, they're trying to, you know, meet certain goals and they have memories and where,
3411000	3414840	but, and they say, well, how long do these patterns hang around? He says, well, about 100
3414840	3418760	years, that's ridiculous. Nothing important can happen in 100 years, you know, because these
3418760	3423400	things live for, you know, millions of years. And so, and so this, this just reminds you that
3425800	3430680	what's a pattern and, and what's a real being. So, so again, back to this distinction between
3430680	3435720	thoughts and thinkers is in the eye of a beholder, you know, it's in the eye of the observer. And
3435800	3440520	if we did have aliens that came to earth with a radically different cognitive frame rate,
3440520	3444600	if they had different lifespans, whatever, would they think that talking to us is a good idea?
3444600	3449160	Or would they be trying to talk to ecosystems? Or conversely, would they think that talking to the,
3449160	3453240	you know, molecular processes is the best that they're going to be able to do? I think, I think
3453240	3459560	all of this is really about observers and about getting good at recognizing intelligence and
3459560	3467240	extremely unfamiliar guises, you know? I think that the it's, it's inescapable, your work particularly
3467240	3472600	to, to not cross philosophical slash ethical boundaries and have these discussions. So,
3472600	3477000	so when people tell you that listen, stay away from the philosophical stuff, it's you cannot,
3477000	3482440	you just, this is just not part of your job. You have to at some point address these because
3482440	3486040	I remember one of the comments in one of our discussions, it could have been our first one
3486040	3491080	or second one. But then someone asked, is Michael playing God? And my first thought was,
3492920	3497560	it's a strange one. You know, it was one of those questions people often asked back in the day when
3497560	3501960	people were tinkering with any sort of even a plant, you could genetically modify an organism,
3501960	3506360	and then you're playing God at that point. I mean, it's a very, very strange question to
3506360	3511320	really ask. How would you respond to that? People actually ask it all the time. I think
3511320	3514920	it's one of those questions that sounds like it makes sense until you, until you sort of dig
3514920	3520680	into it a little bit. But because, because I don't know what, what the definition of God is.
3520680	3523880	And I mean, usually the people who ask this, they got some glasses on, and they've got,
3523880	3527880	you know, they usually drive, they don't walk places and so on. So it's a little, it's a little
3527880	3533880	disingenuous. But, but, but let's, let's dig into this for a moment. I did a poll once on Twitter,
3533880	3539400	and certainly this is not like a, you know, a statistically valid sample or anything like that.
3539400	3544840	But I did a poll and the, and my question was simply this. So you're, you're, you know,
3544840	3550840	AUG the caveman, and you're walking back to your, to your tribe, and you have this vision,
3550840	3557160	you're struck with this vision of discovering fire. And so immediately, you get, you understand fire,
3557160	3566280	but you also get this vision of steel weapons, artificial hearts, antibiotics, going to the
3566280	3572920	moon, atom bomb, computers are like all of it, right? Immediately. So now the question is,
3572920	3576520	so now your question is, so you've seen all this, right? You see, you see where it's going to go.
3576520	3582120	Your question is, do you tell the others and you get going with fire or, or, or, or do you let it,
3582120	3587640	do you let it die? And you never tell. Okay. 6% of my, my audience, and that's, and that's the
3587640	3591320	people who like my stuff. So that means they're already probably like really biased towards,
3591320	3597320	you know, techies stuff. 6% thought you shouldn't, you shouldn't let, you should stay below fire.
3597320	3603320	So, okay, I don't know, you know, I don't know if, if these folks live a lifestyle consistent
3603320	3610040	with that belief. I tend to doubt it. But, but, you know, it's, if, if that's the claim,
3611000	3616200	I think you have to take this series. I think you have to say, if, if you really mean by playing
3616200	3620760	God, I mean, what could it possibly mean? If you really mean taking steps that are
3622040	3627000	strongly efficacious in the world and that make change, that do things, if you really don't want
3627000	3634680	to do that, your quarrel is not with me and my work on frog skin. Your quarrel is with all of
3634680	3639560	humanity who doesn't want to sit in a damp cave their whole life and die in exactly the same
3639560	3644600	condition that they were born in. That's if, if you're really against that, okay, make your case
3644600	3650840	and, and see, you know, and see, see if people will go. But none of the things that, that, that
3650840	3658120	we're doing are any different from the fundamental question. Are you going to take responsibility
3658120	3665080	for the future? And I think that is the most profound moral cowardice to delude yourself into
3665080	3669960	thinking that doing nothing is staying out of it. No, doing nothing is not staying out of it.
3669960	3675720	Doing nothing means you are complicit in the suffering of enormous numbers of humans and
3675720	3681880	others on earth who are having a, an incredibly sub, a suboptimal experience in their, in their
3681880	3686680	embodiment. And if, and if you have these kinds of thoughts about, let's not do this and let's not
3686680	3693080	do that, you know, you know, let's put a break on progress, you are making a very clear statement.
3693080	3697080	And, and you should think about it hard to make sure that you, you are, you know, you're, you're
3697080	3702600	really backing off this idea that you are going to stay, you're going to let the status quo roll on
3702600	3707160	because I, you know, it's just, to me, it's an incredible act of moral cowardice.
3707160	3713080	Yeah. And I think for anyone who wants to even get a glimpse of what your ethical framework around
3713080	3717880	all of this eventually becomes, this paper is perfect. I mean, this paper on AI, at some point,
3717880	3724280	you go a path forward through the ethics filter for civilization. And this fundamental premise
3724280	3730360	for you is, is to mature, to realize, okay, our kids supplant us, everything does change,
3730360	3736040	we continuously change. It's how we're going to move forward. And, and how are we going to, to, to
3736040	3744600	act in a certain way that progresses us in a, in a safer, more kind, more loving environment. And,
3744600	3749000	and this, and the, the outward people, scientists don't like to use it. But I mean, at that point,
3749080	3754280	you're looking towards this sort of kinder process where we were able to give artificial
3754280	3759960	intelligence these properties, because it is something that our cognitive light can't appreciate.
3759960	3764040	And we know that this is something we genuinely enjoy. So let's try and propagate this.
3764840	3770520	Yeah. Yeah. And, and, you know, I have some collaborators. So, so Richard Watson and Thomas
3770520	3777560	Doctor and Olaf Witkowski and, you know, people like Bill Dwayne and Eliza Salamanova, you know,
3777560	3782360	we, we, we write on stuff like this, and there's going to be, there's going to be way more because
3782360	3786600	in, in certain traditions, right? So for example, they come from a Buddhist tradition. And so,
3786600	3794440	and so there, there's a great emphasis on enhancing compassion alongside enhancing wisdom, right,
3794440	3799800	on a basically an infinite sea of other beings and all sorts of crazy embodiments. I mean,
3800280	3806200	I gave a talk on all this stuff to, to some, to some Buddhist scholars in Nepal, you know,
3806200	3813160	at some point. And I mean, that audience, there was nothing here that surprised them whatsoever.
3813160	3816680	You know, usually when I give these talks, people are kind of, kind of shocked and disturbed about
3816680	3823160	about half of what I say. These guys were like, yeah, no kidding, we all know that. And they
3823160	3829240	found nothing, nothing weird about any of it. And I do think they have, they have frameworks for
3829320	3833240	thinking about these, these, these kinds of things, right, you know, this kind of expanding,
3833240	3837800	committing to through, through concepts like the bodhisattva vow and through expanding,
3837800	3843080	committing to the task of this, this metacognitive task of expanding your, your cone of compassion
3843080	3847880	and things like that. Yeah, I think, I mean, I'm certainly not saying that's the only way to go,
3847880	3853160	but, but I think that's exactly where this is going. I agree with you. I mean, because my,
3853160	3859560	even though I'm of Indian heritage and descent, but my, when I talk about science, philosophy,
3859560	3865640	Western, particularly, to my family, to like them, certain uncles or aunts, and if I talk about these
3865640	3870840	topics, they also tend to do that. They, they're not as surprised as, as the more my more Western
3870840	3874760	side of the family. A lot of the Eastern philosophers and my uncles and aunts, they're not
3874760	3878120	really philosophers, but they tend to think like, Oh yeah, that makes sense. That is kind of what
3878120	3883640	their religion taught them, whether it's Hinduism or Buddhism, but there is this element of minds
3883640	3889240	are everywhere in a way. So this, this, this general binary approach that we seem to have
3889240	3894280	is not working for the most part. And you're showing this in very, very Western scientific ways.
3895080	3898520	Well, I think, I think, I mean, that's the other thing, right? So, so I don't really believe,
3898520	3904440	okay, there's, there's another perspective where sometimes people say, look, early indigenous
3904440	3908680	societies knew all this, all we have to do is go back, go back there. I don't actually believe
3908680	3914360	that either. I don't think they actually knew this. And right. And saying something is not the
3914360	3919800	same thing as having a principal framework that takes you to new discovery. So, so it's, it's,
3919800	3926280	I think both sides, this idea of there's no mind everywhere, anywhere except in us, or maybe some
3926280	3930600	people think that just isn't anywhere. But, but the other side of it, which is, oh, there's a spirit
3930600	3936360	under every rock. Like that's, that's a fine start, but it's just a start. You can't just say it and
3936360	3940920	leave it at that. You have to answer the question, what does that do for you? So I think this is
3940920	3947240	really important. All, all of this has to be empirically useful. It has to elevate our condition
3947240	3951960	and has to improve our ability to, to have more meaningful lives in the world. It has to be
3951960	3956920	practical. You cannot just say these things and have it mean anything until unless it leads you
3956920	3961800	to experiments and ultimately to, to, you know, the better ways of being in the world.
3962360	3968280	So I don't think we're going backwards to those traditions at all. I think we're using whatever
3968280	3972200	we can scavenge out of all the, you know, brilliant people that have existed in the past that had
3972200	3977960	sort of glimpses of this stuff. But, but now I think we finally have the ability to push it forward
3977960	3982200	in a very practical way. So that some of these ideas we can, we can discard what isn't useful.
3982200	3986280	We can, we can keep and expand what actually helps us to get to new capabilities.
3987080	3990440	And I think that, and that's part of the approach that I appreciate most. I mean,
3990440	3993640	I find it particularly annoying when people do that, what you're talking about, where
3993640	3998920	these gurus come out and just say these things with no basis, absolutely no evidence of what
3998920	4004680	there's no claim, but it's just so profound in itself. That's the statement itself is all that
4004680	4009240	they have, which, which isn't what, what, what you're trying to do. You're often saying, you
4009240	4013800	got to show, you got to do something, back it up somehow. I mean, I mean, these, these claims
4013800	4019480	and these profound statements and, you know, and poems and whatever else, they're, they're a fine
4019480	4026280	tool for spurring intuition and for giving you ideas that it's the starting point, right? And,
4026280	4032760	and, and I, and I do think that it's true that it's possible to have intuitions about things and,
4032760	4039800	and to come up with prompts, you know, sayings and writings and whatnot that trigger other
4039800	4044360	people into new and interesting thoughts, even though you haven't yet worked out all the details.
4044360	4048600	I mean, I do think that's possible. I do think it's, you know, we are kind of like, so I have this,
4048600	4055080	like, almost, almost like, like the way, um, platonist mathematicians, you know, they feel
4055080	4059640	that they're discovering an existing structure, right, of that, that you're uncovering an existing
4059640	4063640	structure and that, you know, you see, you know, sort of piece by piece, pulling it out.
4064440	4070920	I do think that it's possible to, to, to sort of have insights long before you have the wherewithal
4070920	4075800	to really make it practical or to know what it means or any of that. And so, and so I like that
4075800	4081160	stuff as much as anybody in terms of an intuition, you know, building kind of thing to see what it
4081160	4084680	makes you think about, like the, like the quote from William James, right? I don't know. And I
4084680	4088760	lose no sleep over whether he actually meant that the way that I mean it. I don't care. I think,
4088840	4093480	I think it's a very profound saying. And what can we do with it now? But, um, you know, the hard
4093480	4098840	work comes, comes after all that. Someone, it reminds me of, and I mean, we, we lost him recently,
4098840	4106120	Daniel Dennett, raised him, he, he, what you do is almost the reverse, but in, in, in the same,
4107320	4114680	I would say, in the same great manner is that what, what Dan did was he realized that you
4114680	4119160	can't just philosophize. You, you have to go and you have to get involved with the cognitive
4119160	4124040	science. You've got, you've got to get, you've got to basically do some of the work. And, and,
4124040	4127960	and that's when the philosophy becomes a lot more intriguing is when you do the science and you go
4127960	4133240	into it and you fuse them and you're coming from it from the science side. And then, of course,
4133240	4137560	you then have to have the philosophical discussions with it. And, and you guys have worked very
4137640	4140600	closely together. What is that like for you just as a side?
4142680	4149960	Boy, I mean, first things first, you know, I, I read Dan's books when I was a kid. And it never,
4149960	4154760	I mean, they were so eye-opening, you know, the mind's eye and kinds of minds and that kind of
4154760	4160360	stuff, right? The early kind of the early work in the late 80s, early 90s. I was, I was young
4160360	4165720	back then. And I would, I couldn't have imagined for a moment, a that I would, that I would get to
4165720	4169320	meet him, never mind that, but be that at some point, you know, at some point, we'd write a paper
4169320	4174040	together, right? Like, I wish I could get into a time machine and go back and, you know, tell my
4174040	4177400	18-year-old self that, hey, you know, you're going to write a, write a paper with this guy and
4177400	4182840	actually a bunch of other people to that, that I felt the same way about. So, so, so that part was
4182840	4188040	a profound kind of honor for me is to, is to be able to talk to him about these stuff. And, and
4188040	4192360	by the way, we didn't agree on everything. We, we disagree on a ton of stuff, but
4193240	4201960	he was, he was an incredibly generous, clear thinker. And what I really enjoyed about him was
4201960	4208840	that was, was a few things. One of them was that he was never interested in, in making cheap points.
4208840	4213240	He was always interested in improving everybody's understanding of what's going on, deepening the
4213240	4218360	question. It may be the answer, but for sure, deepening the question. And this idea, you know,
4218360	4223080	he really pushed this idea of steelmaning. You know, he said that, that in arguing with people,
4223080	4229160	what you ought to do is first state their position so well and so strongly that they will wish they
4229160	4233560	came up with it, right? That you should start not, not with a caricature of what they think that
4233560	4237640	you're going to shoot down, because that, that's a game, right? That's, that's, you know, what he
4237640	4243960	wanted was actual progress, which meant you better start with the absolute best description of their
4243960	4247720	view, the most plausible sounding this, and then, then see if you can shoot it down after that,
4247720	4251880	right? That was his, that was his, and he was always that way in all, in all of our discussions,
4253080	4256520	you know, about stuff that we did agree on and lots of things that we didn't agree on.
4256520	4263880	It was, it was always very clear that everybody in this discussion is there to, to learn something
4263880	4268440	and to improve and to give up things that you thought before, if, if they're not helping you
4268440	4273080	move forward and grab some, some other tool like that, that was, you know, he was an amazing
4273080	4276600	example of that. And I mean, at first, well, first I took a course with him as an undergraduate at
4276600	4281880	Tufts, I had him for, for, for a, yeah, I had him for a, for a philosophy of mind professor,
4281880	4286760	which was, which was amazing. I purposely wrote a paper, there was a, there was a final paper
4286760	4291960	for the class that you, that you write, I picked a topic that I knew he did not like, and that I
4291960	4298440	knew he, you know, was, was completely against. And, and I was, I was astounded at, you know, the,
4298440	4304920	the fair, rigorous, but, but, but completely fair, you know, analysis and grade and everything
4304920	4309640	else. That was an example for me that this is how you do it. This is, you know, it's not,
4309640	4315480	it's not just based on what, you know, what you think, but like, you know, a deep analysis of,
4315480	4319400	of the, the fairest analysis. And then, and then later when I came back to Tufts as a faculty
4319400	4323000	member, you know, he was, he was a, he was a colleague and that was, that was incredible.
4323000	4327000	So yeah, yeah, I'm really going to miss him. Yeah, no, he'll be dealing with, I mean, he was one
4327000	4331160	of the, so him and Oliver Sacks were two of the people that inspired me to even start this podcast.
4331160	4336040	So one of those, yes, I never got to have on, but we exchanged emails every now and then.
4336040	4340680	And even doing that for me felt like such an honor. And I really wish I had the chance to
4340680	4343720	chat to him. That's how I'm just so curious for all those people who did get to speak to him.
4344520	4346600	What a, what a provision might have been to pick his brain.
4347240	4352280	Yeah. Oh no, it was, and he was so, you know, he was so, so inspirational and so generous with
4352280	4357880	his ideas. He would come to our lab from time to time. And I have, I have a picture of him on
4357960	4361240	the blog with what during one of his visits, you know, and he's, and he was looking through
4361240	4364120	the microscopes and he was looking at our two headed worms that we would have these,
4364120	4367000	he would have these discussions with our lab people about, you know, what's that,
4367000	4370520	what's it like to be a creature with two brains and what's the right way to think about these things?
4370520	4375240	And, and, you know, and yeah, he would, you know, he would give talks just, just very generous,
4375240	4379000	you know. Mike, you must, you must check, you must look out for this one of these videos online.
4379000	4385480	It's a VPRO roundtable with Dan, Dan Dinnett, Oliver Sacks, Rupert Sheldrake,
4387320	4391960	Steven, it's one of the most fascinating things. It's like six, I think,
4393080	4398360	Dyson, Freeman Dyson was there as well. It's, it's such a strange thing. It's like the original
4398360	4404360	version of podcasting, I would say. Just six of these guys is having the coolest chat on life,
4404360	4409320	consciousness, reality. That was one of the things that got me into the two of their,
4409320	4414520	both of their work and to this podcast. But anyway, before we, because we're digressing a bit,
4414520	4417640	the path forward, this ethics, how are we doing for time? Mike, you all right?
4418920	4421800	Yeah, I'm, yeah, okay. I got about, I got about 15 minutes.
4421800	4427320	Good. Okay. For the, the path forward, the ethics falter, let's talk about this,
4427320	4431320	because you said that there's, there's two ways we could get this wrong. One is object
4431320	4437400	affiliate. And the other one is, well, only love your own kind. Let's talk about how we can get
4437400	4444360	this wrong and how we can actually divert this and get this right. Yeah. Well, the, the, the,
4444360	4450760	the spectrum itself is something like the, it's, it's, it's related to the effort of matching the
4450760	4459640	degree of compassion that you are able to exert to the level of agency that there are intelligence
4459640	4465000	or consciousness that, that that being actually has, right? Now, I'll point out that, that we, even
4465000	4469160	when we get it right, we are still not very good at following through on the consequences. So,
4469160	4473000	so for example, everybody understands that pigs are intelligent. Everybody understands that they,
4473000	4477000	that they suffer, that they have minds, and we still have factory farming. It's, it's right. So,
4477000	4483240	so even, you know, getting it scientifically right is absolutely not a guarantee of anything
4483240	4489240	in terms of actual ethical behavior. But, but there's two ways to get it wrong. One way to get
4489240	4499240	it wrong is to attribute more mind to a system than it really has. But also when I say really has,
4499240	4502680	I, I, you know, I think everything is observer relative, of course, but, but still you, you
4502680	4506120	could get, I mean, there's, you know, the internet is full of profiles of people that are in love
4506120	4511960	with bridges and chandeliers and, and, you know, and things like this. So, so, so that's, that's
4511960	4516440	something having too much, too much concern for things that really don't warrant it. And the
4516440	4521800	other way is, of course, the opposite is when you've, you leave beings out of your, of your, of
4521800	4527560	your calculus of compassion that actually can, can suffer and have an inner perspective. I mean,
4527560	4534280	one, one thing to think about is if imagine two societies that get this wildly wrong in both directions.
4534280	4538680	So you've got a planet where everybody's like, you know, ridiculously nice to, to, you know,
4538680	4543000	they don't like to chop rocks in half and whatever. And, and, and then there's, and then
4543000	4549080	there's the other society that thinks if, if you're not a very narrow type of creature, you are a
4549080	4553720	machine the way that Descartes thought about lots of animals, and that we can do whatever we want,
4553720	4557720	and it's fine. And you're just faking and all the, all your complaining about it is, is just,
4557720	4563080	you know, it's just a word, where it's, it's, it's sentence completion, you know, is what it is. So,
4563800	4567720	okay, so, so which of those worlds would you rather live in? Right? If you're going to get it,
4567720	4572040	if you're going to get it wrong, where, where would you rather be? I mean, I think, I think the
4572040	4580360	first one wastes a lot of resources and opportunities. Yeah, okay. The second one is, is, is monstrous in,
4580360	4586920	in its ethical implications. So, so I, I think we should err on the side of more compassion,
4586920	4591240	not less. I mean, obviously, again, we're not going back to there's a spirit under every rock,
4591240	4597240	because we are committed to having principled theories about this. But if you're going to
4597240	4602360	make a mistake, I think you should make a mistake in that direction. And specifically,
4602360	4607080	what I'd like us to be clear on, I'd like what I'd like everybody to be clear on,
4607080	4613000	is that having certainty about these things right now, when we have pretty much no clue
4613000	4619080	what underlies consciousness, really, I mean, I know a lot of smart people have made efforts
4619080	4624920	into it, but, but I really don't think we have it nailed down. And all of these ideas about
4625480	4631960	what cognition is and how different architectures, you know, supported and, and whether cognitive
4631960	4636200	consciousness and the ability to suffer tracks any of those things or not.
4638680	4643080	There is, there's an enormous amount of unwarranted certainty about this among people,
4643080	4647000	people feel very strong to make this really strong. That definitely doesn't whatever,
4647000	4652760	you know, it doesn't have this or that. I think we all need to take a step back and just understand
4653160	4658680	that from, from, from the scientific perspective, there are so many things we do not know yet,
4658680	4664520	like really critical fundamental things. We do not understand the emergent cognitive properties
4664520	4670360	of matter. We do not understand the scaling policies of how minds emerge from smaller minds.
4671320	4676280	The field of diverse intelligence is just getting started. So I'm much more worried about the right
4676280	4680920	side of that, of that spectrum than I am about the left side at this point.
4681800	4685640	And I think what one of your towards the end of the paper, one of the things you says,
4685640	4690440	the question is, how do we make sure to express kindness to the inevitable forthcoming wave
4691400	4697320	of unconventional sentient beings? And you say that we should start by making sure that we express
4697320	4703320	loving kindness appropriately and not be driven by fear of the other, which is, which is a very
4703320	4709320	beautiful statement. Yeah, thanks. I've actually written a whole thing on fear just now. I'm
4709320	4714600	waiting. It's going to be, it should be out in a couple of weeks. I think that, well, well,
4714600	4719560	one thing I could say is after that, after that piece in Noeima, so there was the short piece
4719560	4723960	in Noeima about the AI, there's a much, there's a longer paper which exists as a preprint and
4723960	4729080	it's also in review right now in the journal. But I think more people saw the Noeima piece.
4729080	4736200	But still, I was very clear there. I thought that I'm not actually saying that AI is that
4736200	4739320	current language models are like humans. I mean, I thought I was pretty clear on this.
4740120	4749240	But I got a lot of people writing to me that basically extremely disturbed by this and this
4749240	4761160	idea that tech bros like myself are, I thought that was funny, that our nerdiness sort of prevents
4761160	4768040	this from understanding real human relationships. And this is why we see these things in what they
4768040	4773640	call machines, robots, AIs and whatever, right? They were looking for a, they were looking for
4774360	4776200	why you do this in a sense.
4777960	4782600	Correct. I mean, it's an old strategy, right? The old strategy is if you're uncomfortable with a view,
4782600	4787480	try to find something wrong with, right? What is it that, you know, we see the truth. Why can't
4787480	4792600	they see the truth? What is missing that causes them to say these things, right? And the standard
4792600	4798760	theory is, well, they just don't understand, you know, these nerds don't understand what real human
4798840	4804680	relationships are like, right? And that, I mean, I'm not super interested in
4804680	4810520	in psychoanalyzing anybody that way. But it did, it did cause me to think, I'm like, wow,
4810520	4814280	why are people so triggered by this? You know, what, what is it that caught, you know, to really,
4814280	4818520	and so, and so I, so, so I'm thankful for once the pay, pay, you know, peace comes out, I'll
4818520	4822840	thank some folks in, in who said these things and actually pushing me in what I thought was,
4822920	4829560	think is an interesting direction is to ask, what, what is it? What is it that's so scary about,
4829560	4835240	about this view? And the more I think about it, I really think it's a very fundamental fear.
4835240	4842440	And the fear is, it's a zero sum game. Love is a zero sum game. If we have too many other
4842440	4847640	beings that need love, then a couple of things will happen. There's not enough for me, that's A,
4847640	4853160	and B, what if I can't rise to the, to the, to the challenge of having enough compassion
4853160	4858440	for everybody? I think it's profoundly threatening to realize that you're going to have to open up
4858440	4865720	your, your constrained way of looking at who deserves your compassion and what happens then.
4865720	4869720	And, and then, and many other things. So, you know, so I wrote that on this probably five or
4869720	4875320	10 pages or something about, you know, just kind of talking about what is really what I think
4875320	4880040	really underlies why, why people are freaked out about this. And, and, and the responsibility,
4880040	4886120	I mean, it's very comforting to think that I can just tell, you know, which things are worth worrying
4886120	4890520	about by looking at them. I know what people look like. I'll just look at them. It's comforting to
4890520	4895800	think that I don't need to be responsible for the future. This, this is it. This is, you know,
4895800	4900360	this is how, this is what's natural, right? Even people who don't believe in, in some,
4900360	4904280	some sort of God, all they, they still have this notion of what's natural. I have no idea what
4904280	4908040	that's supposed to mean. But, but, but, you know, this is like, yeah, this is how we're supposed
4908040	4912120	to stay. And that's fine. I don't, I don't need to be responsible for the future. And I don't need
4912120	4918680	to be responsible for shaping what the planet looks like in the, you know, in, in the coming
4918680	4923560	centuries and beyond. That's comforting to think that it's all handled. It's nice and simple.
4924120	4928600	You don't need these, these extremely difficult nuanced views that are going to require work
4928600	4932440	from you. They're going to require you to make hard decisions to paint a picture of
4932440	4937240	the future of what do you want it to look like? You know, it's much easier to say what you don't
4937240	4942200	want. This, this fear-based scarcity mentality, right? There's not enough love to go around.
4942200	4945800	Let's, let's, let's draw a nice tight circle around things that we know what they look like
4945800	4948680	and we know where they came from. Then we're not going to have to worry about all this other
4948680	4953320	stuff that's really difficult to, to figure out what's, what's going on with it. And yeah,
4953320	4961080	and then we don't need to worry about painting pictures of the, of the future and figuring
4961080	4965000	out how to get there. We could just, we can just make a list of what we don't want to have happen
4965000	4969320	and that's easy and, and, and focus on the negatives. So I think, I think that that type of,
4969320	4975960	that type of limited fearful scarcity kind of mindset is, is what's, is what's responsible
4975960	4980120	for a lot of this. And by the way, what I don't mean, so I, so I want to be clear here, I don't
4980120	4987080	mean to, to try to deconstruct some of my colleagues that are really working on, on very good science,
4987080	4991480	right? So, so there are people who are working on good science for developing principled
4991480	4996520	ways to distinguish between so-called machines and what's special about living organisms,
4996520	5000200	like that, that's a good area of diverse intelligence. I'm not, I'm not, you know,
5000200	5003640	are saying that that shouldn't, you know, that, that shouldn't take place or, or that they're
5003640	5007800	driven by anything other than, you know, good scientific principles. I'm talking about them,
5007800	5011880	you know, I'm talking about the, the folks who have a really visceral reaction who
5011960	5019320	when, when I, when I challenged them to, so, so, so, so be explicit. So, so tell me what,
5019320	5023720	what is the magic that you have? And when did you get it? Both during evolution, during,
5023720	5027320	during, you know, during embryogenesis, what, what, what, what do you have? And when does this
5027320	5032840	show up that you think cannot be either, either in a hybrid form or in synthetic form, you know,
5032840	5036840	done? And what would you do if, I mean, just, you know, I think reading science fiction is,
5036840	5041400	is a great cure for this, because from the, from the earliest time, you understand the scenario,
5041400	5045480	right? You're, you're, you're sitting there at home, this spaceship lands on your front lawn,
5045480	5049000	this, this, the door opens, this thing sort of trundles out, it's kind of shiny looking,
5049000	5052600	it's kind of metallic looking, but it sort of comes up to you and it sort of hands you this
5052600	5056040	poem and it says, oh man, I'm so happy to meet you, you know, it's been, it's been,
5056040	5059880	it's been, you know, a thousand years I was waiting to meet you, many of us died along the way, but,
5059880	5063720	you know, but, but we persevered and we made this journey in here. I wrote you this poem and I'm
5063720	5067160	looking to be friends and you sort of knock on and it's kind of metallic and you say,
5068040	5074200	so, did you guys evolve naturally or did somebody make you? And he says, you mean,
5074200	5079240	you mean, are we the result of totally random processes or was our mind crafted by, you know,
5079240	5082920	some other mind? And he said, yeah, I'd really like to know and say, why, why do you want to know
5082920	5087240	that? Like, well, just, you know, I'd really like to know because, and in the back of your mind,
5087240	5090520	you're thinking, what, that, that, that if it's the, that if it's the latter, then, then you're
5090520	5093800	okay with turning it into a vacuum cleaner, right? That's what you're really thinking about.
5093880	5100520	And, and I mean, I, I, I find that just, just, you know, absurd. And we are all stuck in this
5100520	5106120	position of saying, so what, what criteria are you going to use when you can't do this easy
5106120	5110120	thing? That's why, that's why I think AI and language models are such an off ramp for these
5110120	5114840	discussions, because it's just so easy to dunk on these language models, completely avoiding this
5114840	5119720	issue of that embodiment can take a place in other spaces that you have absolutely no idea
5119800	5123800	what, what, you know, physical systems are capable, even if, even if you made it yourself.
5125080	5129400	Yeah. And even that in itself, and when you spoke about it, when you said, if you use AI
5129400	5134200	to create something, I mean, who really created it? And then you have that wonderful quote where
5134200	5140200	you say like, nothing was ever created by two men. We're merely sort of just adding upon what's
5140200	5149080	already there. Yeah. Yeah, I think, I think we really need to be clear that there are major,
5149080	5154040	major open questions here, like really fundamental open questions. It's too early to be certain of
5154040	5158600	anything other than, I mean, I think the only thing we can be certain of is that it's very easy
5158600	5163480	to make ethical lapses when you try to draw these distinct boundaries and you have no idea what you're
5163480	5183880	doing.
