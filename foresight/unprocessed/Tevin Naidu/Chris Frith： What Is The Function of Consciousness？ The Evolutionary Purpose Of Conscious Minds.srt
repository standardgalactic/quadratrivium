1
00:00:00,000 --> 00:00:16,800
Chris, talk to me about how you define consciousness. I think I'm going to use your paper, The

2
00:00:16,800 --> 00:00:22,000
Neutral Basis of Consciousness, as the foundation for how I'm going to approach the questions

3
00:00:22,000 --> 00:00:27,280
for this interview. How would you define consciousness and why is it important to make

4
00:00:27,280 --> 00:00:33,840
certain differentiations, for example, contents of consciousness versus level of consciousness?

5
00:00:35,040 --> 00:00:42,080
Yeah, let's run through that. Well, I follow most people in defining consciousness as

6
00:00:42,800 --> 00:00:50,400
having subjective experiences. And I think that the distinction between level and content is

7
00:00:50,400 --> 00:00:56,800
important because it probably involves different ways of doing the experiments and thinking about

8
00:00:56,800 --> 00:01:03,840
the brain systems involved. So level of consciousness at one extreme, a high level of

9
00:01:03,840 --> 00:01:11,440
consciousness, you are having subjective experiences. And then of course, you can go into a low level,

10
00:01:11,440 --> 00:01:17,680
sometimes, for example, people who are in this minimally conscious state after brain injury,

11
00:01:17,680 --> 00:01:23,440
they will sometimes have contents of consciousness, but maybe not. And then when you're unconscious

12
00:01:23,520 --> 00:01:30,720
or in coma, you're not having any subjective experiences. Now, the contents of consciousness

13
00:01:30,720 --> 00:01:37,680
is, of course, is talking about what are the subjective experiences. And in particular,

14
00:01:38,640 --> 00:01:47,200
I and many others in the brain imaging world have been fascinated by the observation that when we

15
00:01:47,200 --> 00:01:54,400
are conscious, we are aware or conscious of some things that are impinging on us and not aware

16
00:01:55,360 --> 00:02:02,400
and other things that are impinging on us. And perhaps the most famous example of that is the,

17
00:02:03,840 --> 00:02:11,360
which I'm sure you have seen the video of the gorilla. So this is where you're instructed to

18
00:02:11,360 --> 00:02:15,760
watch this video. And there are people playing basketball or something, and they're in red shirts

19
00:02:15,840 --> 00:02:19,920
and white shirts, and you have to count how many times the people in the white shirts pass the

20
00:02:19,920 --> 00:02:26,480
ball to each other. And at the end, you ask them, did you see the gorilla? And they say no.

21
00:02:28,720 --> 00:02:32,160
Even though if you hadn't been given that instruction, I'd been looking for other things,

22
00:02:32,160 --> 00:02:36,080
you would have obviously noticed the gorilla. So there are many situations where you can show that

23
00:02:37,120 --> 00:02:44,400
even though your senses are being stimulated, this doesn't actually reach awareness. It goes into

24
00:02:44,400 --> 00:02:49,200
the brain, but it doesn't become conscious. And that's exciting, because you can then ask the

25
00:02:49,200 --> 00:02:53,280
question, what's the difference between the brain activity associated with things that you're aware

26
00:02:53,280 --> 00:02:59,600
of, and the brain activity associated with things that you respond to, but are not aware of. Now,

27
00:02:59,600 --> 00:03:04,960
in that case, that's not about the level of consciousness, because in both, this is the

28
00:03:04,960 --> 00:03:11,360
level is the same. But then you're aware of some things and not of others. And that's why I think

29
00:03:11,360 --> 00:03:16,000
that's important. I mean, it's also important for us clinically, because you're working,

30
00:03:16,000 --> 00:03:20,400
obviously, from a clinical perspective, I mean, even myself as a doctor, we have to know,

31
00:03:20,400 --> 00:03:25,520
we use the Glasgow Coma Scale or whatever sort of steps you try and differentiate between how

32
00:03:25,520 --> 00:03:31,200
conscious someone is versus how conscious they're not. But it's very different from subjectivity.

33
00:03:31,840 --> 00:03:37,280
When it comes to the neural basis of consciousness, Chris, how often do you get,

34
00:03:37,360 --> 00:03:43,840
let's say, feedback from people in a negative manner, trying to tell you that there's no way to

35
00:03:43,840 --> 00:03:50,800
find a neural basis of something that is subject. The neural correlates of consciousness do not exist,

36
00:03:50,800 --> 00:03:56,320
and nearly correlations. How often do you get this? And what are your responses to that?

37
00:03:58,480 --> 00:04:04,480
Well, I mean, since they are correlations, that's why we call them the neural correlates of

38
00:04:04,480 --> 00:04:11,280
consciousness. So they don't, as on their own, explain how consciousness emerges, or why

39
00:04:11,280 --> 00:04:18,720
consciousness emerges. So one often gets this question. And I guess I would want to say the

40
00:04:18,720 --> 00:04:25,840
way I would try to get around this problem is to start asking questions about what is the function

41
00:04:25,840 --> 00:04:36,240
of consciousness? What is it for? And as I say in the paper, I think that consciousness is a sort

42
00:04:36,240 --> 00:04:42,720
of biological phenomenon. It's rather than something that physicists can tell us about.

43
00:04:43,840 --> 00:04:51,920
And that means that I think it evolved so that in early organisms, we're not conscious.

44
00:04:52,880 --> 00:04:58,880
And it's only more recently, in evolutionary time, that consciousness has emerged. And then

45
00:04:58,880 --> 00:05:02,800
from that point of view, you have to say, it must have had some advantage. It must have given some

46
00:05:02,800 --> 00:05:10,000
advantage in order for it to emerge. And then you can, you can then ask very difficult questions,

47
00:05:10,000 --> 00:05:15,200
like at what point in evolution did it emerge? What sort of creatures of consciousness are

48
00:05:15,520 --> 00:05:21,200
conscious? And what sort of creatures are not conscious? And that's beginning to approach a

49
00:05:21,200 --> 00:05:27,600
sort of mechanistic idea, because if you, if you can work out what consciousness is for,

50
00:05:27,600 --> 00:05:33,040
what advantages it gives you, and then you can look at the neural correlates of that,

51
00:05:33,760 --> 00:05:39,200
you might start to think about how you could actually make something conscious,

52
00:05:40,160 --> 00:05:43,520
which I'm talking about AI now. I mean, what would you have to,

53
00:05:44,240 --> 00:05:48,720
how would you program a computer or a robot so that it actually was conscious?

54
00:05:51,040 --> 00:05:52,400
I say I have no idea, but

55
00:05:53,120 --> 00:05:57,920
I was going to ask you, like, what do you think? Let's, there was something I was going to ask

56
00:05:57,920 --> 00:06:03,520
you about later on. I mean, if this is something that has evolved, it must have other ways of

57
00:06:03,520 --> 00:06:09,280
evolving. I mean, if the virtual selection has done this, possibly artificial selection is going

58
00:06:09,280 --> 00:06:14,480
to do the same with us. Do you think it's possible for us to do this? Considering that the framework

59
00:06:14,480 --> 00:06:19,360
we use today, I mean, you using the Bayesian brain framework, I mean, for the listeners,

60
00:06:19,360 --> 00:06:23,920
just so that I can give them some context, I've quoted you in my dissertation a couple of times,

61
00:06:23,920 --> 00:06:29,440
and I mean, I used your work to help, help my work. And so thank you for that. First of all,

62
00:06:29,440 --> 00:06:34,640
your contributions to the field. But the way you guys approach it, I mean, Friston,

63
00:06:34,640 --> 00:06:41,280
yourself, I've read so many of you guys' papers. It's very much part of this Bayesian framework.

64
00:06:43,040 --> 00:06:48,320
And we'll talk about its implications with schizophrenia, etc. later on. But do you think

65
00:06:48,320 --> 00:06:59,120
we're going to do this with AI? Yes, I think so. I mean, I remember, I mean, this has been

66
00:06:59,120 --> 00:07:03,200
sort of floating around for a long time, even before we got DeepMind and all these people. I

67
00:07:03,200 --> 00:07:09,840
remember about 30 years ago, down to a meeting where an engineer was presenting this computer,

68
00:07:09,840 --> 00:07:18,720
which he had programmed to show attention, it could attend to some things and not to other

69
00:07:18,720 --> 00:07:22,400
things. And then he was saying, isn't this amazing? And this is probably the basis of

70
00:07:22,400 --> 00:07:28,480
consciousness. And then some elderly philosopher in the back of the lecture theater got up and said,

71
00:07:28,480 --> 00:07:30,640
have you got ethical permission to turn it off?

72
00:07:34,800 --> 00:07:35,840
That's a very good question.

73
00:07:38,240 --> 00:07:43,360
So that's sort of been hanging around. But there are all sorts of ethical implications

74
00:07:43,360 --> 00:07:51,520
of this. I think, I mean, I don't know what the timeline is. I suspect not in my lifetime will

75
00:07:51,520 --> 00:07:59,120
we have a conscious robotic type device. But one of the questions that I have asked the

76
00:07:59,120 --> 00:08:04,480
roboticists who are trying to do these sort of things is it's all very well developing this

77
00:08:04,480 --> 00:08:10,000
in this way. But I would the question I would ask, how would you know when you had achieved

78
00:08:12,000 --> 00:08:17,200
your conscious machine? And I think that's one of the most difficult questions to answer.

79
00:08:18,080 --> 00:08:22,000
Because it's so blurry, the lines. I mean, where do we draw these lines? It was the same

80
00:08:22,000 --> 00:08:28,800
with humans. We made the Ian Vitale. What is life? What is not life? I mean, blurry questions.

81
00:08:30,640 --> 00:08:34,160
That's very good. I mean, I'm very interested in that because I think in the olden days,

82
00:08:35,840 --> 00:08:40,800
like in Mary Shelley's Frankenstein, for example, there was no distinction between life and

83
00:08:40,800 --> 00:08:45,920
consciousness. If you gave it life, you also gave it consciousness. And of course,

84
00:08:45,920 --> 00:08:51,360
the monster in the original is hyper conscious. It doesn't bear any relation to the thing in the

85
00:08:51,360 --> 00:08:58,080
film version. And what's changed as we were indicating is we now have a very good idea of

86
00:08:58,640 --> 00:09:04,800
what life is. And in some sense, can make it without genetic manipulation and so on.

87
00:09:05,360 --> 00:09:11,360
And it's very much become divorced now from consciousness. So we would probably be happy

88
00:09:11,360 --> 00:09:17,520
to say that viruses are not conscious and amoeba are probably not conscious.

89
00:09:18,480 --> 00:09:24,400
So this distinction has now been made. But the big problem now is, as you say, is where does,

90
00:09:25,200 --> 00:09:27,840
at what point does consciousness emerge? And how do we know that?

91
00:09:28,480 --> 00:09:34,480
Exactly. Do you think that every time science makes progress in a certain field,

92
00:09:35,200 --> 00:09:40,240
doubters and skeptics will find another way of adding a layer of importance to us?

93
00:09:41,680 --> 00:09:48,720
Yes, well, I may be responsible for doing that myself, as we will see later on, probably.

94
00:09:48,720 --> 00:09:56,640
I mean, I tend to show thought disorder you'll discover. So there's an example of this, I think,

95
00:09:56,640 --> 00:10:02,880
in psychiatry. There's a very nice book that I had to review for Brain, which is about the

96
00:10:02,880 --> 00:10:08,720
sort of history of psychiatry. And what was fascinating is that as soon as some psychiatric

97
00:10:08,720 --> 00:10:16,880
disorder is resolved, it ceases to be part of psychiatry. So you had what was it tertiary

98
00:10:16,880 --> 00:10:23,200
syphilis, which at one point, most of the mental hospitals had patients with this.

99
00:10:23,920 --> 00:10:27,680
And then it was resolved and it was discovered precisely what it was. And then it ceased to

100
00:10:27,680 --> 00:10:32,400
be psychiatry and also was more or less cured. And the same with epilepsy, that used to be part

101
00:10:32,400 --> 00:10:36,960
of psychiatry, but it's now part of neurology, hunting is career now part of neurology and so on.

102
00:10:37,200 --> 00:10:41,680
So psychiatry is always left with the things that no one can explain.

103
00:10:44,240 --> 00:10:49,040
And I think that's the same phrase as you're suggesting, even if we explain consciousness,

104
00:10:49,040 --> 00:10:53,280
there'll be something else. Or I think what will happen is there are all sorts of different

105
00:10:53,280 --> 00:10:56,960
aspects of consciousness, and it will probably divide up into different things and people who

106
00:10:56,960 --> 00:11:02,240
have already had different definitions of it. And there'll always be parts of it that we

107
00:11:02,240 --> 00:11:06,720
haven't resolved yet. And the ones that we have resolved will say, well, that wasn't really a

108
00:11:06,720 --> 00:11:11,760
part of, that's not what I meant by consciousness. So Chris, what do you think then about those

109
00:11:11,760 --> 00:11:17,760
people who say that, I mean, subjective experience is merely just a form of introspection. I mean,

110
00:11:17,760 --> 00:11:22,800
Daniel Dennett, Keith Frankish, I mean, this is just something we think we have. And it's just

111
00:11:22,800 --> 00:11:28,560
because we're articulating it to ourselves and self-sufficiency, but what are your thoughts on

112
00:11:28,640 --> 00:11:30,720
illusionism then as a theory of consciousness?

113
00:11:32,320 --> 00:11:36,000
Yeah, I think there are two aspects that I don't really understand the idea that

114
00:11:36,000 --> 00:11:43,280
consciousness is an illusion, because we all experience it. I'm more sympathetic, although

115
00:11:43,280 --> 00:11:50,800
the idea is wrong, the idea is it has no function. And this goes back actually, at least as far as

116
00:11:51,520 --> 00:11:59,280
Thomas Huxley, who had this nice metaphor, he says consciousness is like the whistle of a steam

117
00:11:59,280 --> 00:12:05,680
engine, has absolutely no effect on the functioning of the steam engine. And my response to that is

118
00:12:05,680 --> 00:12:15,600
it has a big effect on the functioning of other steam engines. And that's exactly why at some

119
00:12:15,600 --> 00:12:21,680
point, I mean, when you're talking about other species interacting and humans, how important

120
00:12:21,680 --> 00:12:25,920
it is for social interaction, social interaction, that's where you go about with this.

121
00:12:26,480 --> 00:12:27,520
Yeah, that's right.

122
00:12:27,520 --> 00:12:30,720
You want to touch on that? Let's talk about that. Let's get to that topic for a while.

123
00:12:32,080 --> 00:12:40,640
Well, I'm trying to remember. One of the problems in this neuro-based studying neuro-based

124
00:12:40,640 --> 00:12:45,680
of consciousness, and indeed studying subjective experience in general, is that we know about

125
00:12:45,680 --> 00:12:51,440
our own subjective experience, but we don't know about anybody else's. And we rely on them telling

126
00:12:51,440 --> 00:12:57,600
us about it, which is, as you say, is to do with introspection. And this, in fact, has a long

127
00:12:57,600 --> 00:13:03,440
will, and for psychological terms, has a long history, because this is what Fechner was doing

128
00:13:03,440 --> 00:13:09,360
in 1860, which is called psychophysics, which you basically make a noise and you say how loud

129
00:13:09,360 --> 00:13:18,240
was that, which is a subjective report. And you get very, very robust, reliable results using

130
00:13:18,240 --> 00:13:24,320
this. You can show that loudness, you know, follows an exponential scale in relation to the

131
00:13:24,320 --> 00:13:31,520
actual physical intensity of the stimulus. It's very robust. So it really works. And then it

132
00:13:31,520 --> 00:13:36,560
went into dispute because of behaviorism coming in and saying, you can't trust these reports.

133
00:13:36,560 --> 00:13:41,760
And I think that's because people just didn't do the reporting properly. But certainly in the

134
00:13:41,760 --> 00:13:46,080
studies of the neuro-based of consciousness, you have to say, for example, were you aware of the

135
00:13:46,080 --> 00:13:54,000
gorilla or not, for example. And you rely on, or you can sometimes slightly more sophisticated ones,

136
00:13:54,000 --> 00:13:59,040
where you say how aware of the gorilla, were you on a four point scale or something. And then some

137
00:13:59,040 --> 00:14:04,000
people say, well, this is all very well. But when you see these neural activity, is it because of

138
00:14:04,000 --> 00:14:11,360
making the report rather than just the experience? And my answer to that is that the ability to

139
00:14:11,360 --> 00:14:16,960
make the report is actually a very important part of the experience, because this is this rather

140
00:14:16,960 --> 00:14:23,920
wonderful thing that humans especially can do is we can tell each other about our subjective

141
00:14:23,920 --> 00:14:32,640
experiences. And I think, and we can get onto that, but I think this actually creates all sorts of

142
00:14:32,720 --> 00:14:42,000
advantages, which we can measure. And that gives you one reason why this ability might have evolved.

143
00:14:42,800 --> 00:14:48,480
But I'm, I used to believe, I couldn't really quite understand how you could have conscious

144
00:14:48,480 --> 00:14:54,480
experience without being able to, without knowing about it or being able to report it. But I'm now

145
00:14:54,480 --> 00:15:01,040
persuaded that there is an earlier stage where you have the experience, but in a sense, you're not

146
00:15:01,040 --> 00:15:06,240
aware of it. You're aware of the stimulus, but you're not aware of being aware of the stimulus.

147
00:15:06,240 --> 00:15:11,440
This is this higher level, sometimes called meta consciousness. But I think for the one we're

148
00:15:11,440 --> 00:15:17,520
studying human consciousness, the fact that we can report it is actually a very important aspect

149
00:15:17,520 --> 00:15:22,640
of the experience. But then how important do you think linguistics plays a role here? Because we

150
00:15:22,640 --> 00:15:29,520
obviously can't express ourselves as as great as we think we do. I mean, there's so much more

151
00:15:29,520 --> 00:15:34,400
going on inside here, compared to what I can articulate to you. So do you think that's actually

152
00:15:34,400 --> 00:15:38,160
could be a hindrance to the way we try and express ourselves to each other?

153
00:15:41,600 --> 00:15:49,520
Well, I know, I think, in a sense, there's too much. If we try to express everything, we collapse.

154
00:15:49,600 --> 00:15:58,960
And there's some work called coarse grainy or something you can actually, by reducing,

155
00:15:59,840 --> 00:16:05,280
by using these clever ways of reducing what we're talking about to some simplified system,

156
00:16:05,280 --> 00:16:10,400
we can actually get a better account of it than if we tried to put the whole thing across. So

157
00:16:10,400 --> 00:16:15,440
there is that aspect of it. But I think also by talking to each other, we can discover first

158
00:16:15,520 --> 00:16:20,960
what are the important things to tell each other about. And secondly, we can discover ways of

159
00:16:21,520 --> 00:16:27,760
talking about them. In a sense, that's what art is all about, this new ways of

160
00:16:28,480 --> 00:16:34,320
approaching experience and talking about experience and demonstrating experience.

161
00:16:34,320 --> 00:16:38,720
And we're getting better and better at that. And this has become as complicated, because this is

162
00:16:38,720 --> 00:16:46,160
all very cultural. And we would, there's, I don't know whether people really believe this,

163
00:16:46,160 --> 00:16:53,200
but there's some idea that in ancient Greece, there was a big switch between the Iliad and the

164
00:16:53,200 --> 00:16:58,960
Odyssey. And this was to do with the way they thought. So there's an earlier stage where

165
00:17:00,000 --> 00:17:07,280
introspection consisted of hearing the gods telling you what to do. And it's only later on that this

166
00:17:08,240 --> 00:17:15,600
became awareness of this is what you wanted to do. So there was actually a fundamental change in how

167
00:17:15,600 --> 00:17:22,160
you understood yourself, which would obviously influence very much how you explained your

168
00:17:22,160 --> 00:17:26,880
behavior to other people. So we, and you have this with children, for example,

169
00:17:28,000 --> 00:17:33,840
when my grandson hits his twin sister, he says, you know, that was a mistake, I didn't intend to do it.

170
00:17:34,400 --> 00:17:38,560
Whereas presuming an ancient Greek child would have said Zeus told me to or something.

171
00:17:40,560 --> 00:17:45,520
So there are these very interesting cultural changes, which mean although our brains are

172
00:17:45,520 --> 00:17:51,600
essentially the same as they were, whether it was 100, I can't remember the numbers 100,000

173
00:17:51,600 --> 00:17:56,960
years ago, the way we use them and the way we talk to each other are dramatically different.

174
00:17:58,320 --> 00:18:02,800
That's because of consciousness. What are the differences between the self and consciousness?

175
00:18:03,520 --> 00:18:08,320
Or do you think that it was just one thing? Self consciousness is just its own thing?

176
00:18:12,880 --> 00:18:17,520
No, I'm not sure about that. I mean, I think the self is a very important aspect of consciousness,

177
00:18:17,520 --> 00:18:23,280
but there are all sorts of things that I am aware of that are not just about me, by hope.

178
00:18:23,680 --> 00:18:33,840
Yeah. And certainly when we interact with other people, we become aware of what they are like.

179
00:18:35,440 --> 00:18:39,120
So I think there's more to, I mean, I think the self is an important aspect of consciousness,

180
00:18:39,120 --> 00:18:42,240
but it's not the whole of consciousness. Do you think it's just a model we've built?

181
00:18:42,960 --> 00:18:47,760
Do you think? Yes. I mean, this is getting back around slowly to the Bayesian idea. Yes,

182
00:18:47,760 --> 00:18:53,840
there are all these models, and we have a model of ourselves, which is very much determined,

183
00:18:54,640 --> 00:19:00,480
in fact, by other people. So my model of myself is probably largely what I think other people think

184
00:19:00,480 --> 00:19:11,200
of me. With this Bayesian idea, I know that when you look at, let's say, for example,

185
00:19:11,200 --> 00:19:15,440
schizophrenia, it shows such a wide variety of experience of conscious experience. You can see

186
00:19:15,440 --> 00:19:20,720
it as sort of a spectrum. I know Fletcher also does work on this, where he shows you how inherently

187
00:19:20,720 --> 00:19:29,360
all perception can be seen as hallucination or all belief can be seen as delusion. And it's based

188
00:19:29,360 --> 00:19:34,640
on these prior expectations and how we experience that. What are your thoughts on that? I mean,

189
00:19:34,640 --> 00:19:39,840
because clearly, then everything is technically hallucinatory or delusional.

190
00:19:40,000 --> 00:19:48,000
Well, yes, I partly imagine how I use the words, but this goes back along at least to Helmholtz,

191
00:19:48,640 --> 00:19:55,200
is the idea that perception is not like being a camera. It's not the world coming into my eye,

192
00:19:55,200 --> 00:20:00,160
and then I somehow make sense of it. I can only make sense of it if I have some prior expectation

193
00:20:00,880 --> 00:20:06,800
of what is out there. This is what Bayesians call the prize, but it's really past experience,

194
00:20:06,800 --> 00:20:17,120
prior expectations. An example of this is that story, I think it's maybe from India, I'm not

195
00:20:17,120 --> 00:20:24,720
sure. It's about the five blind men who come across an elephant. And one of them feels it's

196
00:20:24,720 --> 00:20:28,880
trunk and says this is a snake, and then another one feels its leg and says this is a tree, and

197
00:20:28,880 --> 00:20:37,920
another one feels its tail and says this is a lion or something. So because in every case,

198
00:20:37,920 --> 00:20:42,560
they're using their past experience, what this ought to be given what I'm experiencing now.

199
00:20:46,320 --> 00:20:52,240
And similar to the whole bunny duck situation, if you've grown up in a place where you've never

200
00:20:52,240 --> 00:20:58,080
really seen that rabbit duck illusion, where it's one of you duck, if you grew up in a place where

201
00:20:58,080 --> 00:21:02,560
you've never seen a rabbit before, you're automatically going to see the duck. Yes, exactly.

202
00:21:03,920 --> 00:21:08,880
And there's this very strong illusion that you must know about the inverse mask illusion. Yes.

203
00:21:10,240 --> 00:21:14,800
So if you have a mask of a face and you see it from the inside, where it's effectively an inside

204
00:21:14,800 --> 00:21:20,240
out face, you cannot see it as an inside out face, because inside out faces don't exist.

205
00:21:20,320 --> 00:21:26,000
So yes, that's an example of the prior experience taking over.

206
00:21:29,440 --> 00:21:33,840
And there's lots of data supporting this idea, I think.

207
00:21:36,640 --> 00:21:44,320
I mean, basically, our sensations are extremely crude and minimal. I mean, vision,

208
00:21:44,960 --> 00:21:49,600
we have this, we don't realize this, but there's a very small area in the middle of our field of

209
00:21:49,600 --> 00:21:55,360
view where we see things in detail. And beyond that is all blurred, but we're not aware of that.

210
00:21:55,360 --> 00:22:02,000
And we fill things in. And there's the famous example of the, what's it called, the blind spot.

211
00:22:02,960 --> 00:22:07,280
So there's this bizarre situation that the retina is the wrong way around. So the blood vessels

212
00:22:07,280 --> 00:22:12,800
are in front of the receptors. And then they all have to go through a small hole to get back to the

213
00:22:12,800 --> 00:22:18,800
rest of the body. But we're not aware of this hole. We fill it in on the basis of what's immediately

214
00:22:18,880 --> 00:22:24,240
around it. And you can fool the system to fill it in wrong and so on like that.

215
00:22:24,240 --> 00:22:28,640
With that blind spot, even if you do, because there's that experiment, I mean, you close an eye

216
00:22:28,640 --> 00:22:31,680
and you slowly move your thumb away and then suddenly your thumb disappears.

217
00:22:32,480 --> 00:22:38,400
And still we know it's there and yet your brain, it's gone, can't help.

218
00:22:39,840 --> 00:22:43,680
And there are many, many optical illusions like that. I mean, there's the Muller-Lauer

219
00:22:43,680 --> 00:22:49,520
illusion where you see these two lines of different lengths, even though you know they're not different.

220
00:22:50,480 --> 00:22:53,760
With the arrows at the edges, once you're close.

221
00:22:55,760 --> 00:22:59,600
And the neck of cube, because it's in two dimensions, it can have three,

222
00:23:00,320 --> 00:23:04,960
in three dimensions, it has two possible versions and you just switch between them.

223
00:23:06,080 --> 00:23:12,240
But then how do we meet our beliefs and our perceptions, Chris? How do we, as scientists,

224
00:23:12,240 --> 00:23:16,720
as people trying to figure out this neural basis, how can we trust anything we see?

225
00:23:18,320 --> 00:23:24,160
Well, I mean, I think I would say one version of it, which is what Arnold said, I think, is that

226
00:23:24,960 --> 00:23:32,560
our perception is a hallucination constrained by reality, which I also said in my book.

227
00:23:35,200 --> 00:23:38,080
And as long as the constraints are good enough,

228
00:23:38,480 --> 00:23:44,880
we're fine. I mean, that's the sense that we have a model of the world. What we perceive is our model

229
00:23:44,880 --> 00:23:50,320
of the world, not the world. And as long as our model is good enough for what we're trying to do,

230
00:23:51,440 --> 00:23:57,120
that is sufficient. And science operates the same way. So a nuclear model of how the world

231
00:23:57,120 --> 00:24:03,840
works, which marks it works incredibly well. And then Einstein came along and showed us a little

232
00:24:03,840 --> 00:24:10,160
bit at the edge of this model, didn't actually work and produced general relativity or whatever it was.

233
00:24:10,720 --> 00:24:16,800
It was like Mercury. It was like one of the few things that Newton's theory just couldn't solve.

234
00:24:20,160 --> 00:24:24,880
And so I think our perception is very much the same. It works most of the time. And occasionally

235
00:24:24,880 --> 00:24:30,880
with these optical illusions, you find where it's gone wrong. And we can now understand them.

236
00:24:31,440 --> 00:24:36,960
And what I mean, of course, my being so interested in the social aspect is that, say, we have our

237
00:24:36,960 --> 00:24:44,240
own models of the world. And not only can we test it by seeing what happens, making predictions and

238
00:24:44,240 --> 00:24:51,600
so on, but we can also test it by comparing it with other people's models. And there's a nice

239
00:24:51,600 --> 00:24:56,480
cartoon I found somewhere, you know, which says that reality is a shared hallucination.

240
00:24:57,440 --> 00:25:04,560
And that's interesting, because, in a sense, what goes wrong, perhaps in schizophrenia,

241
00:25:04,560 --> 00:25:10,960
is they stop sharing. And then they can go off in these weird directions, because they're no

242
00:25:10,960 --> 00:25:19,440
longer constrained by how other people see things. Yes, that's a very speculative idea.

243
00:25:20,400 --> 00:25:28,080
Yeah. But so in other words, we're all having these hallucinations. And the only reason why

244
00:25:28,080 --> 00:25:32,960
they work is because we're all sharing the same hallucination. And it's once someone diverges

245
00:25:32,960 --> 00:25:38,160
on the spectrum, that they're no longer in touch with reality. And that's what obviously

246
00:25:38,160 --> 00:25:45,120
something like schizophrenia or psychosis is. Yeah. And occasionally, we discover that we're

247
00:25:45,120 --> 00:25:52,720
not sharing it. So, for example, people who are colorblind often don't discover that they're

248
00:25:52,720 --> 00:25:59,280
colorblind until they're adults. So a friend of ours didn't discover he was colorblind until he was

249
00:25:59,280 --> 00:26:03,600
a doctor and they complained that he was feeling the form was wrong. And that's because he didn't

250
00:26:03,600 --> 00:26:11,040
realize they were different colors for different things. And perhaps the most famous example of

251
00:26:11,040 --> 00:26:16,240
this is the dress. Yes. Where suddenly people wear that some were seeing it this way and other

252
00:26:16,240 --> 00:26:22,000
people were seeing it the other way. Yeah, the blue and black and white and gold. Yeah. And it

253
00:26:22,000 --> 00:26:27,600
turned out blue and black because they then went to Photoshop, right clicked on it, checked exactly

254
00:26:27,600 --> 00:26:32,320
what color was in the dress is actually blue and black. When I told some of my friends that

255
00:26:32,320 --> 00:26:36,800
they still they still think I'm lying to them. And this is not a story. And yet,

256
00:26:36,800 --> 00:26:43,680
you know that we perceived it differently. One theory is I'm sure you know this is to do with,

257
00:26:44,720 --> 00:26:53,520
of course, our perception of color is very depends on what we think is illuminating the scene.

258
00:26:55,680 --> 00:26:59,680
And one theory about the dress is if you think it's being illuminated in daylight,

259
00:26:59,680 --> 00:27:03,280
it looks one color if you think it's being illuminated in artificial light, it's another color.

260
00:27:03,920 --> 00:27:10,640
So I mean, and again, this is an example. That's nice for me because of course, that's the prior.

261
00:27:10,640 --> 00:27:14,640
Your prior is what you think it's been illuminating in which you have to take into account to

262
00:27:16,000 --> 00:27:20,640
actually work out what the colors really are. Basically like the checkerboard illusion where

263
00:27:20,640 --> 00:27:25,040
you see the shadow and the two grays. Oh, yes. And they look completely different. Yeah,

264
00:27:25,040 --> 00:27:29,440
I think we think the different color. And it's not people tend to think that we're lying to them

265
00:27:29,440 --> 00:27:33,680
when you show them that this is actually the same color. You have to legitimately prove it.

266
00:27:37,120 --> 00:27:41,200
What did you think about a fixed firm belief? Because that's obviously what some of us have

267
00:27:41,200 --> 00:27:45,600
and there's this cognitive dissonance when you show them that our perceptions are not really

268
00:27:46,560 --> 00:27:51,920
always that accurate. Well, that's it. I mean, I would follow Carl

269
00:27:51,920 --> 00:28:00,080
Friston here and say there's a sort of hierarchy. And at the bottom of the hierarchy, you have what

270
00:28:00,080 --> 00:28:08,240
is the real color of this square. And you have priors and about what is being illuminated and

271
00:28:08,240 --> 00:28:12,160
you could be persuaded that actually it's a different kind of illumination. But at the

272
00:28:12,160 --> 00:28:19,360
top of the hierarchy, you have belief like perception is accurate. And that's very,

273
00:28:19,360 --> 00:28:25,040
that could be very firmly fixed. And in the sense in schizophrenia, I think you have

274
00:28:27,200 --> 00:28:33,280
something eventually goes wrong at the top. So the idea that everybody is trying to deceive me

275
00:28:33,280 --> 00:28:38,560
because this high level prior that cannot really be changed. And it's very difficult to come up

276
00:28:38,560 --> 00:28:43,040
with evidence against that idea that everybody is trying to deceive you, because of course,

277
00:28:43,040 --> 00:28:48,880
you're one of the people who's trying to deceive them anyway. And it's funny because in those

278
00:28:48,880 --> 00:28:54,080
schizophrenic patients, some of the psychotic patients, these optical, these perceptual illusions

279
00:28:54,080 --> 00:28:59,920
don't work on them. Yes, that's right. Which shows that their priors have changed.

280
00:29:03,520 --> 00:29:09,440
But that's very confusing. And I think that's more research is needed, as they say, because

281
00:29:10,160 --> 00:29:14,720
in some cases, their priors are too strong. In other cases, their priors are too weak.

282
00:29:15,280 --> 00:29:20,720
So there has to be some solution to this observation has to be made.

283
00:29:21,600 --> 00:29:26,640
So Chris, tell me, what do you think then about because part of when I worked, I mean,

284
00:29:26,640 --> 00:29:30,960
we did the philosophy and ethics of mental health, you see how we've had three biological sort of

285
00:29:30,960 --> 00:29:36,080
revolutions in psychiatry, for example, when Prozac was invented. I mean, that was one of them.

286
00:29:36,080 --> 00:29:43,440
People went crazy. This was the new craze. And it's functional MRIs, better neuroimaging.

287
00:29:44,320 --> 00:29:51,200
When is psychiatry going to actually get a hold of you that they really want to do?

288
00:29:52,000 --> 00:29:56,480
Is this going to happen in your opinion? Or do you think we're going to continuously fail at this?

289
00:29:56,480 --> 00:30:04,560
Because I guess like what I was saying earlier, I think, for example, there's these very rare,

290
00:30:04,560 --> 00:30:09,760
I can't remember what is it, 1% of people who get a diagnosis of schizophrenia actually have an

291
00:30:09,760 --> 00:30:18,640
autoimmune disease. Okay, I didn't know that. There's a book. Someday there's a journalist who

292
00:30:18,640 --> 00:30:27,760
this happened to. And she describes it very well. And I think what will happen is that there are

293
00:30:27,760 --> 00:30:36,480
multiple causes of schizophrenia and psychosis. And they will gradually be discovered. But not all

294
00:30:36,480 --> 00:30:41,600
of them. I was a bit like in the olden days, I think there used to be something called mental

295
00:30:41,600 --> 00:30:52,320
deficiency. And currently, you know, every week, another 1% is explained by some unique genetic

296
00:30:53,600 --> 00:31:01,840
alteration, which is gradually explaining all these things. And I think something analogous

297
00:31:01,840 --> 00:31:07,360
might happen with schizophrenia, though, they'll be found, you know, there'll be a genetic version

298
00:31:07,360 --> 00:31:14,480
and there'll be an autoimmune version and there'll be various other versions. And each time that will

299
00:31:14,480 --> 00:31:19,680
then become part of the neurology and what's left will be unexplained and left to the psychiatry

300
00:31:19,680 --> 00:31:26,160
so somehow. I want to try to figure out the sort of evolutionary basis for psychosis and

301
00:31:26,880 --> 00:31:31,520
schizophrenia. And I remember, I think I was reading something in evolutionary psychiatry,

302
00:31:31,520 --> 00:31:35,760
it was a journal, I can't remember the name, but we were talking about how a lot of foreigners,

303
00:31:35,760 --> 00:31:40,640
people who move from one country to another, into develop schizophrenia at a higher rate

304
00:31:40,640 --> 00:31:46,080
than those who stay in their own place. And if you have to think about the evolutionary advantage

305
00:31:46,080 --> 00:31:50,720
of that, it would be trust people around you a little less, because you don't know them and

306
00:31:50,720 --> 00:31:54,880
they might not have your best interest at heart. So it's fascinating to think that there might be a

307
00:31:54,880 --> 00:32:01,200
lot of evolutionary basis behind some of these mental illnesses. That's right. I mean, I'm very

308
00:32:01,200 --> 00:32:08,720
interested in this game, that's the Bayesian story. There's this idea of volatility. So if you, if

309
00:32:08,720 --> 00:32:15,040
you're, so this is a new way of looking at learning. So you can have fast learning and have slow

310
00:32:15,040 --> 00:32:21,520
learning. Normally we think of slow learning surely bad, but this is, it entirely depends on

311
00:32:21,600 --> 00:32:29,760
the environment you're in. If you're in a stable environment, then it pays to learn slowly,

312
00:32:29,760 --> 00:32:34,240
because what you've learned in the past probably still applies. So you don't need to change very

313
00:32:34,240 --> 00:32:41,760
much. But if you're in a volatile environment, you have to learn quickly, because the situation

314
00:32:41,760 --> 00:32:47,520
keeps changing. And I guess if you move to a foreign country, you're precisely in a more volatile

315
00:32:47,520 --> 00:32:52,480
environment. And it's interesting that this does seem to have an effect on the dopamine system.

316
00:32:53,760 --> 00:33:01,120
So that in the, yes, I think as you increase the amount of dopamine, the learning rate will

317
00:33:01,120 --> 00:33:06,160
increase or learning faster, which is good for a volatile environment. And of course, if you have

318
00:33:06,160 --> 00:33:12,240
a high level prior that says I'm an involuntary environment, when you're not, this could be

319
00:33:12,240 --> 00:33:19,360
produced all sorts of peculiarities. While we're on the on neuro anatomy, I mean, when you talk

320
00:33:19,360 --> 00:33:24,480
about how we might have emerged or evolved consciousness, and which parts of the brain

321
00:33:24,480 --> 00:33:31,840
do you think are most responsible for this? This is very controversial, as you probably know.

322
00:33:33,440 --> 00:33:40,480
I think there are two aspects to this. I think first of all, I think it's pretty well agreed

323
00:33:41,200 --> 00:33:47,680
that consciousness requires long range connectivity between different brain regions.

324
00:33:47,680 --> 00:33:55,360
And if this breaks down, you become unconscious. Now, certainly in the case of self conscious or

325
00:33:55,360 --> 00:34:03,440
meta consciousness, I think the frontal cortex is critical. Because in a sense, it's at the top

326
00:34:03,440 --> 00:34:12,000
of this hierarchy. And it is where we don't know much about it yet, where you're modeling how the

327
00:34:12,000 --> 00:34:18,480
rest of the brain works. And I think that's necessary for this, say this high level of

328
00:34:18,480 --> 00:34:25,600
consciousness for sentience, which is what I like to call this lower level of consciousness,

329
00:34:25,600 --> 00:34:34,480
which we probably share with many, many animals. That probably doesn't involve the frontal cortex

330
00:34:34,480 --> 00:34:40,960
to the same degree and is probably much more dependent on the long range connectivity.

331
00:34:41,840 --> 00:34:48,720
So one of the things, if we think about sentience, for example, one of the things that has fascinated

332
00:34:48,720 --> 00:34:57,200
me for a long time is what's it called corollary discharge and the reafference principle.

333
00:34:58,320 --> 00:35:05,680
So when we move, and again, Helmholtz was involved in this as part of the reason, when we move our eyes,

334
00:35:08,640 --> 00:35:12,400
obviously the image on our retina is jumping about all over the place.

335
00:35:12,720 --> 00:35:18,800
But we don't see the world jumping about we perceive it stable. And this goes right back to the very

336
00:35:18,800 --> 00:35:25,760
beginning of evolution, when organisms were able for the first time to move under their own power.

337
00:35:28,320 --> 00:35:33,120
Almost immediately, they had to be a mechanism which enabled them to recognize the difference

338
00:35:33,120 --> 00:35:37,840
between movement out there due to something else moving and movement out there due to them moving.

339
00:35:38,000 --> 00:35:42,160
And this is where the reafference or corollary discharge comes in, because you basically,

340
00:35:42,160 --> 00:35:48,000
when you move yourself, you have to say in the signal to the receptive parts of the brain to say,

341
00:35:48,000 --> 00:35:55,360
this is not the world moving, this is me moving. And I think that's perhaps that sort of interaction

342
00:35:55,360 --> 00:36:00,720
between brain areas is the start of something like sentience. And if that's the case,

343
00:36:01,520 --> 00:36:06,800
and if that's the case, it's certainly present in fish and insects and

344
00:36:08,960 --> 00:36:17,120
all sorts of much lower organisms and us. I was speaking to Ian McGillchrist, I'm not sure if

345
00:36:17,120 --> 00:36:20,880
you're familiar with him. We were speaking just the other day, and we're talking about like

346
00:36:20,880 --> 00:36:27,120
Kazanica's work, for example, his work, how important that copus callosum is in terms of

347
00:36:27,120 --> 00:36:32,480
that long range connection you're talking about, because you split that and a lot can happen.

348
00:36:32,480 --> 00:36:36,160
Well, what comes to your mind when you think about split brain patients?

349
00:36:36,160 --> 00:36:39,360
How does it affect anything in terms of your view of consciousness?

350
00:36:41,120 --> 00:36:43,840
Well, I'm a little bit dubious about this.

351
00:36:51,040 --> 00:36:54,000
I mean, one problem is that they don't do the operation anymore.

352
00:36:54,960 --> 00:37:02,800
So we can't sort of replicate the results. The other problem is that occasionally people

353
00:37:02,800 --> 00:37:08,640
turn up who never had a copus callosum, and they seem to be remarkably normal,

354
00:37:11,280 --> 00:37:19,280
compared to the late cases where they are split. So I mean, clearly,

355
00:37:19,680 --> 00:37:26,800
so I guess I'm not really sure what I would predict ought to happen. I mean, a lot of the

356
00:37:26,800 --> 00:37:34,720
brain imaging work, in a sense, went against this idea of distinct properties for the left

357
00:37:34,720 --> 00:37:40,560
and the right brain. So for example, language was always used to be very much in the left brain,

358
00:37:40,560 --> 00:37:44,880
but the brain imaging show there's all sorts of aspects of languages in the right brain as well.

359
00:37:45,120 --> 00:37:54,160
There's this interesting phenomenon of left sided neglect, where there does seem to be some

360
00:37:54,800 --> 00:37:59,440
lateralities of attention so that one side of the brain seems to attention to both fields

361
00:37:59,440 --> 00:38:08,560
and the other side to any one, which is slightly odd. So I'm very confused about the role of

362
00:38:08,560 --> 00:38:17,280
laterality. In terms of that, that it's sort of a hemispatial neglect, Professor Michael

363
00:38:17,280 --> 00:38:23,920
Graziano at Princeton, he has ancient schema theory. So he sort of says that it's more about what

364
00:38:23,920 --> 00:38:29,600
we're attending to, and then we build models to tell us what we're attending to, and that's what

365
00:38:29,600 --> 00:38:36,320
we sort of call consciousness. It's more of a caricature. We're not really seeing reality as it

366
00:38:36,320 --> 00:38:42,400
is. We're just everything we see. Yes, I mean, that's another version of what we were talking

367
00:38:42,400 --> 00:38:46,240
about earlier. We have a model of the world, and that's what we see, not the world itself.

368
00:38:47,200 --> 00:38:51,600
But I think, yes, his ideas are very interesting in relation to how attention works, and particularly

369
00:38:51,600 --> 00:38:57,120
the idea that we, once we, you know, we can model our own attention, we can perhaps model

370
00:38:57,120 --> 00:39:01,280
other people's attention, and I would even go as far as to say, maybe it's the other way around,

371
00:39:02,160 --> 00:39:06,720
that we start off by having to note, it's very important to us to work out where other people

372
00:39:06,720 --> 00:39:13,200
are attending to. I mean, this is not just for us. I mean, there's always nice work with Thomas

373
00:39:13,200 --> 00:39:19,840
Sello on chimpanzee. So the low ranking chimpanzee has to make sure the high ranking chimpanzee

374
00:39:19,840 --> 00:39:27,120
can't see the food, so they can then go and get it. So knowing where other people are attending to

375
00:39:27,120 --> 00:39:31,520
is very important. And maybe once we have worked out how to do that, we suddenly realize we can

376
00:39:31,520 --> 00:39:38,960
apply it to ourselves. There's a similar version in relation to theory of mind. So the sort of

377
00:39:39,520 --> 00:39:43,920
standard idea was we first of all learn how to think about our own minds, our intentions,

378
00:39:43,920 --> 00:39:48,320
our beliefs and desires, and then we apply it to other people. But some people have suggested

379
00:39:48,320 --> 00:39:53,600
it's actually the other way around. We find it so useful to work out what other people are wanting

380
00:39:53,600 --> 00:39:59,200
and knowing and desiring that there's a pressure to find that out. And then we find we can apply it

381
00:39:59,200 --> 00:40:05,680
to ourselves as well. Fascinating. So what I'm going to do is, Chris, the podcast is obviously

382
00:40:05,680 --> 00:40:11,440
called Mind Body Solutions. So we're trying to figure out the infamous mind body problem. So

383
00:40:11,440 --> 00:40:15,920
I'm going to run through a couple theories with you or a couple of philosophical views. And I just

384
00:40:15,920 --> 00:40:19,520
want to hear your thoughts on them. Let's start off with panpsychism. What are your thoughts on

385
00:40:19,520 --> 00:40:26,320
panpsychism? Yes, I don't really understand this. I mean, as I said at the beginning, I think

386
00:40:26,320 --> 00:40:32,480
consciousness is a biological thing. So I expect to find it in biological entities or

387
00:40:34,240 --> 00:40:40,720
machines that are built to represent sort of copies of biological entities. So it could

388
00:40:40,720 --> 00:40:47,120
be in silicon. And I don't really, I mean, the philosophers say that this doesn't solve the

389
00:40:47,120 --> 00:40:54,320
hard problem of how you get subjected to experience out of a physical entity. And I guess I would say

390
00:40:55,840 --> 00:41:01,040
I don't really worry about that at this stage. But the idea that spoons are conscious in some

391
00:41:01,040 --> 00:41:07,280
sense doesn't really help me with what I'm trying to do. Do you think it's a problem with semantics

392
00:41:07,280 --> 00:41:12,720
here? We're just all just saying the same word in a very different way? I think that's partly,

393
00:41:12,720 --> 00:41:19,280
yes, I mean, what it depends on. I mean, does it make any sense to say that spoons are having

394
00:41:19,280 --> 00:41:25,680
subjective experience? But if you then think about something like IIT integrated information theory,

395
00:41:26,480 --> 00:41:31,520
do you think that's following a similar path? Or because they're also trying to search for a neural

396
00:41:31,520 --> 00:41:37,040
basis of consciousness, of course. I mean, they're clearly searching for a neural basis of consciousness.

397
00:41:37,120 --> 00:41:42,960
And I think I would agree that long range connectivity in the brain is very important.

398
00:41:44,080 --> 00:41:49,520
What I don't quite get with integrated information theory is that they start from,

399
00:41:50,240 --> 00:41:55,520
I can't remember five principles of how what consciousness is. And I don't agree with them.

400
00:41:57,680 --> 00:42:02,000
I can't at this moment, because I don't agree with them, of course, I can't remember what they are.

401
00:42:02,000 --> 00:42:08,720
But I think, for example, that it's integrated. And for example, semi-executive was always saying

402
00:42:08,720 --> 00:42:15,360
that consciousness wasn't integrated at all. And it was very messy and so on. So I think their starting

403
00:42:15,360 --> 00:42:23,200
point of what consciousness is, is not the same as mine. Do you think global workspace theory

404
00:42:23,200 --> 00:42:28,880
does a better job at explaining it? Yes, I think that's one of the best ones around at the moment.

405
00:42:29,840 --> 00:42:31,440
Why do you think that?

406
00:42:32,480 --> 00:42:40,560
Why do I? I mean, lots of partly because I stand the heart of a friend of mine. But I think

407
00:42:42,480 --> 00:42:49,760
but it also I think it fits the data very well. In my paper, I point that out so that

408
00:42:50,800 --> 00:42:54,400
if you think it's to do with working, it goes back to the idea of working memory.

409
00:42:55,360 --> 00:43:01,120
Working memory very much involves Prato and frontal cortex. You find that Prato and frontal cortex

410
00:43:01,120 --> 00:43:08,080
are the bits of the brain that are less active and disconnected in coma and vegetative state and

411
00:43:08,080 --> 00:43:12,960
things like that. In the experiments, I actually did when I was still doing brain imaging, where we

412
00:43:12,960 --> 00:43:18,800
were showing looking at the difference between things that you were conscious of and things

413
00:43:18,800 --> 00:43:23,760
that were affecting the brain that you were not conscious of. When you were not conscious of a

414
00:43:23,760 --> 00:43:29,680
face, for example, the face area lit up. But when you were conscious of that face, then Prato

415
00:43:29,680 --> 00:43:36,880
and frontal cortex lit up as well. So that sort of fits with this sort of story. I guess my

416
00:43:37,680 --> 00:43:43,680
worry about global workspace theory is that I don't really quite understand the

417
00:43:44,320 --> 00:43:50,000
ignition story. And of course, it doesn't bring in other people, which

418
00:43:50,320 --> 00:43:57,200
is a question. And how the reporting works and all that sort of thing. But I think it's the best

419
00:43:57,200 --> 00:44:08,000
one around at the moment. Though I also like, of course, Hakuenlau and David Rosenthal's

420
00:44:08,000 --> 00:44:14,800
higher order theory. Can you give me some reasons for that one as well?

421
00:44:15,760 --> 00:44:20,320
Well, that one, I think what is that may be talking about a different thing. So I think

422
00:44:20,320 --> 00:44:25,360
higher order thought theory, I think, is actually talking about this metaconsciousness,

423
00:44:25,360 --> 00:44:30,640
which is this higher level where you're aware that you're aware. And there's lots of evidence that,

424
00:44:31,600 --> 00:44:39,040
for example, we talk about explicit metacognition, which metacognition being means thinking about

425
00:44:39,120 --> 00:44:44,640
thinking. So I'm particularly interested in our ability. When we make decisions,

426
00:44:45,680 --> 00:44:51,840
we can actually think about how likely is this decision to be right? How much confidence do I

427
00:44:51,840 --> 00:44:57,440
have in this decision? And we have this feeling of confidence, we can actually describe how confident

428
00:44:57,440 --> 00:45:03,280
we are to other people. This is actually useful in when we work with other people

429
00:45:04,240 --> 00:45:09,440
to compare our levels of confidence. And again, there's lots of brain imaging work,

430
00:45:09,440 --> 00:45:15,440
particularly from Stephen Fleming, showing that when people have to use confidence,

431
00:45:15,440 --> 00:45:20,640
they're actually using a particular part of the frontal cortex, that is the frontal polar area,

432
00:45:20,640 --> 00:45:27,360
BA10, which seems to have a special role in thinking about thinking. And it seems to be

433
00:45:27,360 --> 00:45:33,520
thinking about thinking is very closely related to this higher order thought theory of consciousness.

434
00:45:36,080 --> 00:45:42,480
So I like that idea. Yeah, I think it seems like a lot of these theories have so many good

435
00:45:42,480 --> 00:45:48,480
ideas that somehow just going to accumulate and provide us with one at some point. It's almost

436
00:45:48,480 --> 00:45:53,680
like they all kind of need each other a little bit. Yes, I think that's right. And then, of course,

437
00:45:53,760 --> 00:45:59,200
there's an interesting paper recently, which looked at all these different theories of consciousness

438
00:45:59,200 --> 00:46:03,360
and the way that they test themselves. I mean, the experiments they do to test them,

439
00:46:03,920 --> 00:46:09,360
which quite nicely showed, as you might expect, that they test their own theory, but it has no

440
00:46:09,360 --> 00:46:13,760
impact whatever on any of the other theories. So the tests that you use for global workspace

441
00:46:13,760 --> 00:46:19,520
theories, Jolly Good for global workspace theory, but has no relevance for IIT or higher

442
00:46:19,520 --> 00:46:24,400
order thought. So in a sense, they have to somehow come together, which is beginning to happen.

443
00:46:24,960 --> 00:46:29,120
I remember when I was chatting to Michael Graziano, he was telling me how you can take

444
00:46:29,120 --> 00:46:34,800
attention schema theory and actually plunk it onto global workspace theory. And then you have

445
00:46:34,800 --> 00:46:40,880
a much better theory of consciousness as a global workspace theory is one of the better ones in

446
00:46:40,880 --> 00:46:49,040
terms of explaining. I think that's right. But I'm reminded, I don't know whether you read these

447
00:46:49,040 --> 00:46:55,440
novels by Sebastian folks. But one of them, I can't remember what it was called, but it's in fact,

448
00:46:55,440 --> 00:47:02,640
it's a series of five different stories. And one of them is about this neuroscientist in Italy who

449
00:47:02,640 --> 00:47:09,200
discovers the basis of neural basis of consciousness. And I was involved because he asked me to comment

450
00:47:09,200 --> 00:47:14,640
on this before everyone knew whether it was plausible and so on. And the hippocampus had a

451
00:47:15,440 --> 00:47:21,200
major role. But what I found quite nice about this story was that when she does define in the

452
00:47:21,200 --> 00:47:24,960
story, she finds the neural basis of consciousness and everybody agrees it's neural basis of

453
00:47:24,960 --> 00:47:35,680
consciousness. And then everybody loses interest. This podcast becomes completely relevant. Let's

454
00:47:35,680 --> 00:47:43,760
hope no one can forget it. That's exactly what happened with the Yarn Vettel. I mean, going back

455
00:47:43,840 --> 00:47:52,080
to what you said earlier. Once it's done, once we figure it out, it's like magic. Nicholas Humphrey

456
00:47:52,080 --> 00:47:58,160
and I were chatting about it, because he calls it phenomenal surrealism. No longer considers it

457
00:47:58,160 --> 00:48:04,800
an illusion. He thinks it's more real than real, like red, even redder than red, because we perceive

458
00:48:04,800 --> 00:48:12,800
it as red. But yeah, he sees it as magic. That's a good point. Actually, it says consciousness is

459
00:48:12,800 --> 00:48:19,440
more real. I think his paper is redder than red. I can't remember exactly the name.

460
00:48:20,080 --> 00:48:27,120
Yes, but just on the same topic, though, once the magician tells you the trick, I mean, you're

461
00:48:27,120 --> 00:48:35,120
done. The search is over this whole magical experience. It's finished. Yes, I think so. Some

462
00:48:35,120 --> 00:48:40,240
people, I won't name names, like to keep their theories, has a little bit of magic. It has a

463
00:48:40,240 --> 00:48:47,920
little bit that no one can quite understand. Do you think when people make these definitions,

464
00:48:47,920 --> 00:48:53,680
let's say, like when we claim that it's a hard problem of consciousness, that we're adding the

465
00:48:53,680 --> 00:48:58,320
element that we don't need? I mean, Daniel Dennis is trying to quine qualia, a quining qualia or

466
00:48:58,320 --> 00:49:02,960
a way to make it easier for us to actually find a neural basis of consciousness. You think that's

467
00:49:02,960 --> 00:49:08,400
a better approach? Not necessarily an illusion, but to at least make... I think it is a better

468
00:49:08,400 --> 00:49:13,920
approach. Yes, I mean, the other extreme is, is it McGinn? Is it the people who call themselves

469
00:49:13,920 --> 00:49:25,520
mysterious, who say we won't ever solve it? And I think that that's clearly a, what's the word,

470
00:49:25,520 --> 00:49:28,880
a challenge. If you say it's impossible to solve, then it's like, right, we're going to do it.

471
00:49:30,320 --> 00:49:34,160
You think this is a way for philosophers just to keep their jobs in certain...

472
00:49:38,720 --> 00:49:44,400
Tell me, Chris, when it comes to theory of mind, I mean, you and I, if you've done such great work

473
00:49:44,400 --> 00:49:52,000
in terms of working together, humans, why are we meant to work together? Over time, how do you think

474
00:49:52,000 --> 00:49:58,400
that this has led to such crazy amounts of technological revolutions? I mean, in terms of

475
00:49:58,400 --> 00:50:03,600
what we've done as a species, why do you think humans have done so much more than, let's say,

476
00:50:03,600 --> 00:50:12,080
our cousins, chimpanzees, bonobos, etc.? Well, I think, yes, I think the basic reason is that,

477
00:50:12,080 --> 00:50:16,880
as we were saying earlier, that we are able actually to share our subjective experiences.

478
00:50:18,560 --> 00:50:23,440
We can, as a result of this, we can actually alter our subjective experiences and make them,

479
00:50:23,440 --> 00:50:29,040
in some sense, more accurate, better models of what the world is like, because we can talk to

480
00:50:29,040 --> 00:50:33,360
each other about what the world is like, and we can get better theories. I mean, this is what

481
00:50:33,360 --> 00:50:38,000
science is all about. You have the better and better model of the world. It never gets there,

482
00:50:38,000 --> 00:50:48,560
but it gets better each time. And we can... I mean, there's this lovely book by Celia Hayes,

483
00:50:48,560 --> 00:50:54,960
which is all about cognitive gadgets, which is about cultural, has given us ways of...

484
00:50:55,920 --> 00:51:04,960
new ways of thinking. And these may be more important than... these cognitive tools may

485
00:51:04,960 --> 00:51:11,440
be more important than, say, stone tools or iron tools or whatever. I mean, mathematics is an example

486
00:51:11,440 --> 00:51:19,840
of a tool which has been created by humans interacting together. So, I mean, and writing

487
00:51:19,840 --> 00:51:27,920
is another example. So, I mean, universal literacy has only been around for the last

488
00:51:29,440 --> 00:51:34,240
hundred years or so. So, these are the things that have made us...

489
00:51:37,280 --> 00:51:41,120
given us all these advantages, which means that we're now dominating the world and probably

490
00:51:41,120 --> 00:51:47,280
about to destroy it, but that's another matter. And the other thing, it's very frightening,

491
00:51:47,280 --> 00:51:53,440
it could all be lost. Exactly. I mean, that's the... I mean, I often ask this, I mean,

492
00:51:54,080 --> 00:51:59,120
I don't believe there is any sort of teleology. I don't... I'm very agnostic regarding anything

493
00:51:59,120 --> 00:52:04,960
regarding the origins of the universe or why we hear what we are to do. But do you think,

494
00:52:04,960 --> 00:52:10,640
for example, AI would be the next evolutionary step? And do you think it's meant to be? Do you

495
00:52:10,640 --> 00:52:16,480
think that's what humans are actually here to do, in a sense? Well, in some sense, yes. I mean,

496
00:52:17,120 --> 00:52:22,480
you could... Yes, we wouldn't want to say it was meant to happen, but we would say the way the

497
00:52:22,480 --> 00:52:28,960
system works means it will happen. It's the next logical step. That's obviously something that will

498
00:52:28,960 --> 00:52:35,600
survive, let's say something a bit more catastrophic on the planet. Should we send it into space or

499
00:52:36,720 --> 00:52:44,800
whatever with it? I mean, that's very interesting. And I'm sad that I probably won't live to see this

500
00:52:44,880 --> 00:52:52,160
happening. But I'm certainly very interested in, yes, what's happening in AI and DeepMind and all

501
00:52:52,160 --> 00:52:57,120
this sort of thing. Sort of quantum computing. I mean, they're doing a lot of intriguing stuff,

502
00:52:57,120 --> 00:53:02,480
but I also don't think I'll be alive, to be honest. But I still don't think most of us would be alive

503
00:53:02,480 --> 00:53:08,480
at that point. Things are going down south very quickly. But I think even DeepMind, the amazing

504
00:53:08,480 --> 00:53:15,280
things they're doing is all happening at this lowest level from a biological point of view.

505
00:53:15,280 --> 00:53:20,720
I mean, in my paper, I show this diagram at three levels. And the bottom level, we have

506
00:53:22,000 --> 00:53:26,240
model-free learning, as it's called, which depends on having huge amounts of evidence

507
00:53:26,240 --> 00:53:32,320
and doing statistical learning. And I think that's what AI can do at the moment.

508
00:53:33,200 --> 00:53:38,080
So, sorry, continue, Chris, what were you saying?

509
00:53:38,640 --> 00:53:42,800
No, I was just going to say the next step up is to start making models and they're beginning

510
00:53:42,800 --> 00:53:46,640
to do that. But I'm not sure whether that's how far that's gone yet.

511
00:53:48,720 --> 00:53:54,560
Michael was telling me that they are applying that theory to certain AI. And it's starting to

512
00:53:54,560 --> 00:54:00,880
work quite well. I mean, they're making some progress, but they're just very far away to

513
00:54:01,120 --> 00:54:05,840
do it with obviously to the complexity that we've reached over billions of years. It takes a long

514
00:54:05,840 --> 00:54:11,760
time. But I think we'll get there. I don't know. I mean, certainly, in relation to theory of mind,

515
00:54:11,760 --> 00:54:17,680
there are, I mean, my worries about our ideas about how theory of mind works is we don't really

516
00:54:17,680 --> 00:54:24,080
have a good computational account of how that works. And people are beginning to develop it

517
00:54:24,080 --> 00:54:32,480
like, what's his name, 10 and down? But the, I think it's not quite there yet. And they typically

518
00:54:32,480 --> 00:54:35,920
say, yes, we've solved this with that. Well, that's not what I meant by theory of mind.

519
00:54:37,840 --> 00:54:43,040
Do you feel that, I mean, we're obviously following this computational approach right now, but

520
00:54:43,040 --> 00:54:46,880
it's because this is the current framework we live in. I mean, people often have this argument

521
00:54:46,880 --> 00:54:52,000
that when we understood the way pumps work, that's how we thought the brain worked when the train

522
00:54:52,000 --> 00:54:56,800
station, we thought it works the same way. Do you feel that perhaps there might be something

523
00:54:56,800 --> 00:55:00,160
we'll figure out in the future that we'll go beyond this computational model? Or you think

524
00:55:00,160 --> 00:55:06,880
we got it this time that we're almost there? No, I was thinking that there were certainly

525
00:55:06,880 --> 00:55:10,800
better things. I guess that's very interesting point, because that's absolutely right. I mean,

526
00:55:10,800 --> 00:55:14,880
what was wrong with Freud's approach? Because he started off by having this model of the mind

527
00:55:14,880 --> 00:55:20,640
that it was basically hydraulic. And he didn't know about information flow and such like, so I

528
00:55:20,640 --> 00:55:27,280
think that was a big step forward is to go. So he was just talking about energy. And the switch

529
00:55:27,280 --> 00:55:34,560
to be able to talk about information, I think was a crucial step in this direction. And there may

530
00:55:34,560 --> 00:55:38,800
well be another crucial step, of course, because we haven't made it yet, we have absolutely no idea

531
00:55:40,240 --> 00:55:46,240
what it is. But in some sense, I mean, this is a sort of Turing story that you can model anything

532
00:55:46,240 --> 00:55:52,400
on a general computer. So you could model your hydraulic system, you could model your steam

533
00:55:52,400 --> 00:56:00,240
engine type system. What we don't know is what we should be modeling, I guess. But I mean,

534
00:56:00,240 --> 00:56:06,640
I agree entirely that we're always using the technology we currently have. But I think we're

535
00:56:06,640 --> 00:56:13,680
beginning to get a sort of meta technology. So we have a technology of technology, which may be

536
00:56:13,680 --> 00:56:25,600
the critical step to the next stage on idealism. Idealism. I guess I'm not quite sure what that

537
00:56:25,600 --> 00:56:31,760
means. Do you mean in the sort of platonic sense? So basically that it's all mind, because there's

538
00:56:31,760 --> 00:56:37,280
a lot of theories now. I mean, for example, Donald Hoffman, Bernardo Kerstrup, there's a lot of people

539
00:56:37,280 --> 00:56:44,400
who are saying that it's all in the mind, everything is mind. And making some very good

540
00:56:44,400 --> 00:56:49,040
physical theories like physics, using a lot of physics to actually prove that space time, for

541
00:56:49,040 --> 00:56:54,320
example, isn't as real as we thought it was, according to the mathematics, and then trying

542
00:56:54,320 --> 00:57:00,400
to figure out how consciousness could be fundamental to that. Yes, I haven't really thought about that.

543
00:57:00,400 --> 00:57:05,600
Is this a bit solipsistic in the sense that say we are creating the world out there?

544
00:57:07,040 --> 00:57:13,440
I think, yes, I'm not so happy with that. I mean, I'm very keen to believe that there is a real world

545
00:57:13,440 --> 00:57:24,640
out there that we're trying to find out about. But I agree that the way what we impose upon it is

546
00:57:24,640 --> 00:57:30,960
obviously must be, to some extent, created by our minds. I mean, this is almost a sort of Bayesian

547
00:57:30,960 --> 00:57:38,560
thing that we, the priors come from within. So in that sense. A sort of naive realism. We know

548
00:57:38,560 --> 00:57:43,200
that there's something there, but we're just not seeing it as accurately as we might. Yes, I'm a

549
00:57:43,200 --> 00:57:47,520
naive realist, I think. So I think, yes, we're getting more accurate. But I believe that there is,

550
00:57:48,160 --> 00:57:52,480
I mean, if we don't believe there's anything out there, there's no way we can improve our accuracy.

551
00:57:52,480 --> 00:57:58,080
We have to be something, I mean, I'm very keen on prediction errors. I mean, that's a sort of

552
00:57:58,080 --> 00:58:03,360
Bayesian story. Errors are a very good thing because they tell us about what we don't know.

553
00:58:05,600 --> 00:58:10,720
So I'm keen on the idea that there are things that we don't know that we will be able to find out

554
00:58:10,720 --> 00:58:17,840
a bit about. Tell me, Chris, when you started out with this, because there's a book by John

555
00:58:17,840 --> 00:58:27,120
Hogan or Hogan or Hogan called Mind Body Problems. And what the author does is look at different

556
00:58:27,120 --> 00:58:33,120
scientists, philosophers, Christoph Koch, for example, each one, and it almost goes into a

557
00:58:33,120 --> 00:58:39,040
personal deep dive into their life and how their personal experiences might have led them to their

558
00:58:39,040 --> 00:58:45,280
current belief in consciousness. So for example, if someone was very spiritual or very religious,

559
00:58:45,840 --> 00:58:51,040
somehow things consciousness will be fundamental. Someone who's not, grows up very atheist,

560
00:58:51,920 --> 00:58:57,840
tends to think consciousness is an illusion. So how do you think your spiritual views and beliefs

561
00:58:57,840 --> 00:59:05,120
have shaped the way you think about consciousness today? Well, I was certainly brought up in a

562
00:59:05,120 --> 00:59:17,840
very religious family, Anglican, of course. But I still get a great deal of pleasure from

563
00:59:18,880 --> 00:59:28,160
services and singing psalms and peculiar rituals of this sort. So I quite like to be an Anglican

564
00:59:28,160 --> 00:59:31,760
on the grounds that these are the Christians who don't actually believe in God. So that's

565
00:59:32,560 --> 00:59:40,800
so from that, I mean, so there's that side of it. I guess my main interesting consciousness

566
00:59:40,800 --> 00:59:48,160
probably was because I very early on trained as a clinical psychologist. And I actually

567
00:59:48,160 --> 00:59:54,480
interacted with people with psychosis, who had hallucinations and delusions. And I became

568
00:59:54,480 --> 01:00:00,400
completely fascinated by what an earth hallucination, what it meant to be having a hallucination.

569
01:00:01,360 --> 01:00:06,960
And of course, I was also reading Philip K. Dick, which is a very strong experience.

570
01:00:08,320 --> 01:00:12,480
And his theme, of course, is that, you know, reality is not what we think it is.

571
01:00:13,600 --> 01:00:19,360
And what we, you know, we have been, it's an illusion that's been created by evil

572
01:00:20,560 --> 01:00:26,000
capitalists of some kind to keep us down. So I mean, so those two sides may be extremely

573
01:00:26,000 --> 01:00:31,120
interesting hallucinations. Why should my belief, my beliefs about the world be any better than

574
01:00:31,120 --> 01:00:37,600
these patients that I was interacting with? And I guess that was my start in thinking about.

575
01:00:38,640 --> 01:00:43,840
So I was thinking about hallucinations. And I realized very rapidly that this is actually

576
01:00:43,840 --> 01:00:49,920
all about consciousness. So hallucination is the extreme subjective experience, because it

577
01:00:49,920 --> 01:00:57,120
doesn't actually, it's not based on what's coming into the senses. So it's almost subjective

578
01:00:57,120 --> 01:01:02,640
experience without reality. So that was, and I guess that was my main starting point.

579
01:01:03,920 --> 01:01:08,400
And then at some point, I mean, in your book, Making at the Mind, I remember reading it a

580
01:01:08,400 --> 01:01:14,560
long time ago, for the first time. And I mean, it blew my mind as well. I mean, you talk about

581
01:01:14,560 --> 01:01:17,920
various different things in your papers, in your books, altered states of consciousness,

582
01:01:18,000 --> 01:01:22,720
so one of them. How do you think altered states of consciousness give us a better idea of

583
01:01:22,720 --> 01:01:29,680
understanding of consciousness in itself? Well, there's some very interesting, I mean,

584
01:01:29,680 --> 01:01:36,480
I talk about this a bit in my paper. But I think the altered states of consciousness make us realize

585
01:01:36,480 --> 01:01:47,120
that, you know, there's more to it than we typically experience. And I mean, that certainly

586
01:01:47,120 --> 01:01:51,120
it's a very basis for studying levels of consciousness when you're talking about altered

587
01:01:51,120 --> 01:01:59,840
states due to lack of various things. But also the use of psychedelic drugs reveal that you can

588
01:01:59,840 --> 01:02:04,880
have states of consciousness that are not like what we are used to. And again, I think that's

589
01:02:04,880 --> 01:02:10,800
very important. It's rather like the dress to realize that there's more to consciousness than

590
01:02:10,880 --> 01:02:20,160
perhaps we realize. I'm not quite sure in what direction this is going to go in. But one of the

591
01:02:20,160 --> 01:02:25,520
things it's maybe important is going right back to us why and how we started talking about how do

592
01:02:25,520 --> 01:02:32,880
you study subjective experience. And there is always this problem of psychology that introspection

593
01:02:32,880 --> 01:02:40,400
is difficult. And we need better tools for doing introspection. And I think studying altered

594
01:02:40,480 --> 01:02:47,760
states is one of the ways of improving the tools because it in a sense, defines different dimensions

595
01:02:47,760 --> 01:02:53,520
within which we might describe our conscious experience. So it's giving us a better sort of

596
01:02:53,520 --> 01:02:57,440
framework for beginning to talk about our subjective experience, which is what we

597
01:02:58,080 --> 01:02:59,920
desperately need. So that's

598
01:03:01,120 --> 01:03:05,280
There are a lot of universities nowadays studying like the effects of psilocybin,

599
01:03:06,240 --> 01:03:12,320
as the certain drugs on patients with depression, etc. And it seems to be very promising.

600
01:03:14,320 --> 01:03:22,320
Yes, I'm still a little bit suspicious. But it's certainly worth looking at further.

601
01:03:22,960 --> 01:03:28,400
I think the one I read, I remember there was a five or six month follow up on patients who

602
01:03:28,400 --> 01:03:37,680
had taken a high dose psilocybin trip programs at very high dose. And most of the patients'

603
01:03:37,680 --> 01:03:42,960
experiences seem to be back to normal at their baseline, which obviously needs to still be peer

604
01:03:42,960 --> 01:03:48,560
reviewed a lot more and tested. But that does seem to be quite promising. Because currently,

605
01:03:48,560 --> 01:03:53,360
if you look at our current approach, treating depression, I mean, anti-depressants are not

606
01:03:54,240 --> 01:03:58,240
not up to standard. It's a point, actually, how effective they are.

607
01:03:59,600 --> 01:04:04,400
Well, no, that's a big problem. I mean, also in the study of psychosis, because

608
01:04:05,200 --> 01:04:10,160
basically, big pharma has stopped doing any research. Because there's no

609
01:04:11,520 --> 01:04:17,440
there's no obvious direction to take. So we certainly need to somehow get back into that.

610
01:04:18,400 --> 01:04:24,880
Although, I'm not quite sure where that would take us. But no, I think this is interesting. But

611
01:04:24,880 --> 01:04:31,360
as I say, bigger trials are needed. And there's also the work on ketamine.

612
01:04:34,480 --> 01:04:36,160
Yes, there's been a lot of studies on that as well.

613
01:04:38,320 --> 01:04:46,240
I think it's Joanna Moncriff, who wrote a book on big pharma and how a lot of these drugs are

614
01:04:46,240 --> 01:04:51,760
just completely controlled by the pharmaceutical industry. And now it's taken over, it's taken

615
01:04:51,760 --> 01:04:56,240
a life on its own that the way the science no longer really matters at this point.

616
01:04:58,800 --> 01:05:02,320
Do you feel the same way as a clinical psychologist that look in psychiatry? I mean,

617
01:05:02,320 --> 01:05:07,760
these patients are all no longer sorting out the psychological elements and the social elements.

618
01:05:09,360 --> 01:05:12,880
I mean, it's partly to do with a matter of expense. But I mean, the drugs,

619
01:05:13,520 --> 01:05:18,800
they're still giving the same drugs that were discovered based in the 50s. And as I said,

620
01:05:18,800 --> 01:05:23,120
the sum of useful companies are no longer interested in trying to develop better drugs,

621
01:05:23,120 --> 01:05:29,920
because they don't know how to. There's a lot of work with what's it called cognitive

622
01:05:29,920 --> 01:05:36,640
behavior therapy. Although this is not quite as good as we were hoping.

623
01:05:37,120 --> 01:05:46,080
Sorry, that was something I was going to say. But it's partly,

624
01:05:48,080 --> 01:05:53,760
when I was studying schizophrenia a long ago, there were these huge mental hospitals,

625
01:05:54,400 --> 01:05:59,680
which were full of patients and they were all closed down. And it was believed that this would

626
01:05:59,680 --> 01:06:04,640
help them and they would no longer be institutionalized. Now, this has not worked. It was effectively

627
01:06:04,640 --> 01:06:13,600
a money saving enterprise. And they're now treated in the community, which is a euphemism for not

628
01:06:13,600 --> 01:06:24,960
being treated at all. I'm sure that much better procedures could be applied if there was the money

629
01:06:24,960 --> 01:06:32,560
to do so. But we still don't, I mean, whether this is not about cure, this is about management.

630
01:06:33,280 --> 01:06:40,640
And apart from these odd things like the very rare cases that have autoimmune

631
01:06:40,640 --> 01:06:45,200
problems, which can actually be cured, the rest of it, we still don't know.

632
01:06:48,320 --> 01:06:52,720
When you look at autistic patients, I mean, mines are clearly better than one. I mean,

633
01:06:54,560 --> 01:06:59,520
you speak about this, you've got a graphic novel, you've got, you've done a lot of lectures on it.

634
01:07:00,400 --> 01:07:04,640
Yeah. How do you think that patients with on the autistic spectrum

635
01:07:06,160 --> 01:07:12,000
differ from the average person in terms of how they perceive the world from a theory of mind

636
01:07:12,000 --> 01:07:15,280
perspective? Well,

637
01:07:18,720 --> 01:07:24,960
certainly at the more extreme ends of the spectrum, I think they generally have difficulty in

638
01:07:25,920 --> 01:07:35,760
recognising that behaviour is driven by beliefs, rather than reality. And therefore,

639
01:07:35,760 --> 01:07:41,760
the difficulty in understanding that people can have false beliefs, difficulty in understanding

640
01:07:41,760 --> 01:07:47,360
that people can, I mean, this is the dangerous part, people can be deliberately deceiving you

641
01:07:47,680 --> 01:07:53,840
by trying to impart a false belief in you. So they can be over trusting.

642
01:07:56,640 --> 01:08:03,680
So there are these sorts of problems. They, the other area where they may have difficulty

643
01:08:03,680 --> 01:08:08,880
is recognising, you know, which is part of the same thing. Things like white lies that you

644
01:08:08,880 --> 01:08:15,760
can deceive people for good reasons. And you don't always have to obey the rules. So they

645
01:08:15,760 --> 01:08:21,360
tend to be very ruled down because of not recognising these sort of grey areas, which

646
01:08:21,360 --> 01:08:28,320
can make them difficult to work with because they object to people doing minor infringements,

647
01:08:28,320 --> 01:08:36,480
which I'm sure our Prime Minister would suffer from. I think that's one of the things that I

648
01:08:36,480 --> 01:08:42,720
do like most about the Bayesian brain approach is that you try and explain these with using

649
01:08:42,720 --> 01:08:48,000
Bayes theorem and using these priors. I mean, it does explain quite a lot of psychiatric disorders.

650
01:08:48,000 --> 01:08:55,440
I mean, too many incorrect priors fix more. Yes, I think the dangerous it may be explains too much.

651
01:08:57,520 --> 01:09:01,680
And I mean, I think it explains things on a low level, but we need much more

652
01:09:04,080 --> 01:09:10,640
testable predictions. So it's perhaps too good at explaining things, but we need to actually have

653
01:09:10,640 --> 01:09:18,640
some new hypotheses that we can demonstrate. And if you okay, so we've done we've spoken about

654
01:09:18,640 --> 01:09:25,040
panpsychism, idealism, so people just thinking all in the mind. What about those who believe in

655
01:09:25,040 --> 01:09:29,120
quantum consciousness? I mean, this isn't the microtubules you've got to hammer off.

656
01:09:31,440 --> 01:09:39,040
This is something more. I don't think we need it. I mean, I think the microtubules

657
01:09:39,040 --> 01:09:45,120
is largely discounted as far as I understand it. But I'm not convinced that we need

658
01:09:48,400 --> 01:09:53,840
a special as yet, we don't need to bring in these rather

659
01:09:54,320 --> 01:10:00,640
abstruse physical mechanisms to explain

660
01:10:02,560 --> 01:10:10,560
how the brain works. I had I'm what I had once had an interaction with Penfield about

661
01:10:11,760 --> 01:10:19,920
I mean, and one of his arguments is that the brain cannot be a computer because

662
01:10:20,880 --> 01:10:27,360
you know, people can solve non computable problems or some other details. And I was objecting to

663
01:10:27,360 --> 01:10:32,960
this because I was saying was, I mean, in fact, when you look at the computation involved,

664
01:10:34,000 --> 01:10:38,400
at an unconscious level, we do actually much better than we do at the conscious level. Our

665
01:10:38,400 --> 01:10:44,000
conscious processing is actually much more primitive than the unconscious processing. So

666
01:10:44,640 --> 01:10:51,120
if we really need quantum type explanations, it may apply at the unconscious level of how

667
01:10:51,120 --> 01:10:55,920
well the brain works, but we don't really need it for the conscious level. It seems to me that's

668
01:10:55,920 --> 01:11:01,520
a different problem. In a few weeks, I'm chatting to Mark Swalms. I'm not sure if you're familiar

669
01:11:01,520 --> 01:11:09,920
with his work. Mark Swalms. Oh, yes, psychoanalysis. Yes. So he sort of thinks it's a bit more

670
01:11:10,480 --> 01:11:17,680
a lot more primitive, actually, that it's not so much to do with the prefrontal cortex and that

671
01:11:17,680 --> 01:11:23,120
sort of communication out downward, it's more from the bottom up that we consciousness sort of

672
01:11:23,120 --> 01:11:28,640
evolves. Do you have thoughts on that? Yes, I mean, I think that goes back to this idea that the

673
01:11:28,640 --> 01:11:34,000
frontal cortex is particularly involved with self consciousness and meta consciousness, and that

674
01:11:34,000 --> 01:11:41,360
sentence may well be much more bottom up. I'm not myself too keen on psychoanalytic approaches

675
01:11:41,360 --> 01:11:48,080
to this, because I don't quite know how they fit, or what it's all about. But I would certainly

676
01:11:48,080 --> 01:11:55,200
agree that there's a lot of, I mean, the bottom end of sentence, which as I say, is probably

677
01:11:55,200 --> 01:12:02,800
shared with fish and bumblebees and so on. The frontal cortex may not be so relevant, and this

678
01:12:02,800 --> 01:12:11,600
is certainly, in that sense, very primitive. When you think of other species like, let's say dolphins,

679
01:12:13,440 --> 01:12:18,720
what else, let's just octopi, I mean, is it octopuses or octopi? I'm not sure what the correct

680
01:12:18,720 --> 01:12:23,840
term there is, but a lot of people have done studies now with octopuses, and they seem to think

681
01:12:23,840 --> 01:12:30,720
that they're a lot more complicated than we thought. Yes, I think that's absolutely true.

682
01:12:30,720 --> 01:12:36,880
I mean, one task, which I mean, there's this chapter on the virtues written a great deal about

683
01:12:36,880 --> 01:12:42,720
consciousness is a philosopher about invertible consciousness, and I think makes a very good

684
01:12:42,720 --> 01:12:51,200
case for octopuses, which I think, anyway, to the extent that the, it's now, I think the government

685
01:12:51,200 --> 01:13:00,400
has recognized that octopuses and squids should be protected in certain ways. But there's a very

686
01:13:00,400 --> 01:13:08,080
interesting task, which I talk about in my paper, which I, which is this reversal learning, you know

687
01:13:08,080 --> 01:13:17,840
about this, this is, so it's a very simple task. For example, you can have two food wells, which

688
01:13:17,840 --> 01:13:24,320
have, you know, one's red and one's blue, and food is hidden under one of them. And most creatures

689
01:13:24,320 --> 01:13:32,240
can learn with trial and error that it's hidden under the red one. And so they do that for some

690
01:13:32,240 --> 01:13:38,720
time, and then you switch without telling them you switch the reward to the blue one. And then

691
01:13:38,720 --> 01:13:42,560
after a few trials, they realize they were wrong, and they realize it's down to the blue one, and

692
01:13:42,560 --> 01:13:49,840
you can go on doing this, this is very boring, and a very simple model free learning device

693
01:13:49,840 --> 01:13:55,440
can learn this. But every time you switch, it takes a certain number of trials until it gets

694
01:13:55,440 --> 01:14:02,800
the right answer. But most creatures show something which is called learning to learn. So after a few

695
01:14:02,800 --> 01:14:10,400
switches, they can switch much more quickly. And this involves actually a higher level,

696
01:14:10,400 --> 01:14:15,680
you now have to go up to a level which recognizes that the world has two states, it's either the

697
01:14:16,320 --> 01:14:22,720
the red's good or the blue's good. And you have to recognize when the state changes.

698
01:14:24,160 --> 01:14:27,680
And this is also very interesting from a Bayesian point of view, because when it's in the,

699
01:14:27,680 --> 01:14:32,960
this is all probabilistic. So even when it's, when it's usually under the red one,

700
01:14:33,680 --> 01:14:39,520
it's only under the red one, say 80% of the time. So very occasionally you'll find it's

701
01:14:39,520 --> 01:14:45,120
under the other one, you'll be wrong. But because you realize it's a state, you say this

702
01:14:45,680 --> 01:14:50,400
I know this is just noise. So I ignore these prediction errors. This is very becoming very

703
01:14:50,400 --> 01:14:58,560
sophisticated. You say prediction errors are not, not always important. I can, in some situations,

704
01:14:58,560 --> 01:15:03,680
I ignore them. But you've also learned that after a certain amount of time, it's going to switch.

705
01:15:06,000 --> 01:15:09,520
And therefore we start saying, well, I'm reaching the point where I suspect that these

706
01:15:09,520 --> 01:15:14,720
prediction errors are becoming important. Because they may be telling me there's going to be a

707
01:15:14,720 --> 01:15:19,440
switch in the state of the world. So I'll start attending to them. And that's how you can switch

708
01:15:19,440 --> 01:15:23,520
very rapidly, because you roughly know where it's going to be. And as soon as their prediction

709
01:15:23,520 --> 01:15:29,120
error, at this point, then you say, I'm going to switch to the other one. So this is highly

710
01:15:29,120 --> 01:15:35,360
sophisticated and enables you to switch quickly. Now, what is interesting is that

711
01:15:37,680 --> 01:15:39,360
most creatures can do this.

712
01:15:40,000 --> 01:15:47,840
This is what Jonathan Birch points out, this rapid learning to learn this

713
01:15:49,200 --> 01:15:55,600
reversal learning switch can be found in many animals, you know, go fish, probably I haven't

714
01:15:55,600 --> 01:16:01,920
done bumble bees yet, but they probably will. And I think this is a marker of sentence. You

715
01:16:01,920 --> 01:16:07,600
now have a model of the world that it can be in two different states, rather than just learning

716
01:16:07,600 --> 01:16:13,600
this response is good, or this response is good. That's the difference between the model free learner

717
01:16:13,600 --> 01:16:17,840
who just knows whether the response is good or not, and has to learn it. And the model

718
01:16:19,200 --> 01:16:23,200
determined learner who realizes there are two different states of the world. And this is what

719
01:16:23,200 --> 01:16:28,000
I have to learn about. And I think that's where sentence is. And if this really is a marker

720
01:16:28,000 --> 01:16:33,840
of sentence, then many creatures have it. That's quite intriguing. So even goldfish are able to

721
01:16:33,840 --> 01:16:41,200
do this. That's pretty cool. And let's see, that's fascinating. When you think about

722
01:16:43,200 --> 01:16:48,160
virtual realities, I mean, we're reaching a point where meta, we're talking about things like meta

723
01:16:48,160 --> 01:16:54,000
and metaverse, people are thinking about how we're all at some point going to be computerized,

724
01:16:54,000 --> 01:16:59,760
we're going to plug ourselves in, become part of a virtual world. Do you think that it's possible

725
01:16:59,760 --> 01:17:06,400
that this is actually a virtual? Do you ever have those sort of philosophical thoughts as a

726
01:17:07,520 --> 01:17:11,040
very prominent figure in the neuroscience and psychology field?

727
01:17:11,840 --> 01:17:15,840
No, I have to say, I mean, I've certainly been interested in the idea of a brain and a fat.

728
01:17:17,680 --> 01:17:21,040
And this really goes right back to Descartes, who was worried about, you know,

729
01:17:21,040 --> 01:17:24,800
there's an evil demon that's determining what I'm perceiving.

730
01:17:24,960 --> 01:17:33,680
And I guess, but I've never really taken that on board. And I guess it's, again, it's to do with

731
01:17:34,640 --> 01:17:41,600
interacting with other people who have slightly different views of the world, who have slightly,

732
01:17:41,600 --> 01:17:47,360
you know, they have different past experiences. And it doesn't, if we were all brains in vats,

733
01:17:47,360 --> 01:17:53,200
I just thought that similarity would be much greater. Or maybe it depends on how

734
01:17:53,840 --> 01:18:01,920
sophisticated the programmer is. But I think, so I'm not really taken with that. I mean,

735
01:18:01,920 --> 01:18:06,720
so I'm certainly very interested in the idea that, you know, with virtual reality, we do become

736
01:18:06,720 --> 01:18:12,800
brains in vats. But interestingly, we can choose which that we're in, which is quite

737
01:18:14,880 --> 01:18:20,400
interesting. But so far, I have not been very impressed with the virtual reality.

738
01:18:20,400 --> 01:18:27,040
I have tried, but I'm not very good at it. I mean, who was once asked to be on the jury of the,

739
01:18:28,400 --> 01:18:34,000
you know, every year they have some competition for the best new game or something. And so we got

740
01:18:34,000 --> 01:18:38,000
lots of these games to play. And we found that we were completely incompetent at playing them.

741
01:18:42,080 --> 01:18:44,880
So that was probably the reason why I'm not impressed by the

742
01:18:45,920 --> 01:18:49,600
virtual world, because I'm just not very good at getting into them properly.

743
01:18:49,920 --> 01:18:50,960
If you had to...

744
01:18:50,960 --> 01:18:56,080
Trinity, Chris, to actually put your brain, download your consciousness and keep it alive

745
01:18:56,080 --> 01:18:57,440
and keep it going, would you do that?

746
01:18:58,800 --> 01:19:00,160
That's a very interesting question.

747
01:19:03,760 --> 01:19:09,520
Probably not. Unless the virtual worlds are much better than they are now.

748
01:19:10,720 --> 01:19:15,360
And I'm very, I'm very mystified about this idea that we'll soon be able to download our brain

749
01:19:15,360 --> 01:19:19,680
into a computer or something, because I am becoming more and more complicated. I mean,

750
01:19:19,680 --> 01:19:23,680
the recent work says that you would have to take into account, you know, every synapse,

751
01:19:23,680 --> 01:19:24,960
not just every neuron.

752
01:19:26,800 --> 01:19:27,280
Exactly.

753
01:19:27,920 --> 01:19:30,160
And I think that's not enough.

754
01:19:30,160 --> 01:19:35,360
And you lose a lot of competition in the sense that we are very much embodied beings functioning.

755
01:19:35,360 --> 01:19:39,120
I mean, a lot of our perceptual experiences come from the fact that we have hands,

756
01:19:40,000 --> 01:19:42,560
and we can't have 10s because we've got these 10 fingers.

757
01:19:43,440 --> 01:19:43,920
Yeah.

758
01:19:43,920 --> 01:19:47,440
So there's a lot that we'd lose out in terms of information.

759
01:19:47,440 --> 01:19:51,680
Yeah. Yeah. Yeah, I think that's very important. I mean, the other thing I've been very fascinated

760
01:19:51,680 --> 01:19:58,400
by is that the people have started working on theirs. Occasionally, people have six fingers

761
01:19:58,400 --> 01:20:04,400
instead of five. And it's to some extent genetic. And I think there's a family in Brazil where

762
01:20:04,400 --> 01:20:08,400
they all have six fingers or something. And there's some very interesting work on what they do with

763
01:20:08,400 --> 01:20:11,360
their mobile phones, and they're actually able to do more things.

764
01:20:15,200 --> 01:20:16,720
I'm going to read up on that after this.

765
01:20:18,080 --> 01:20:22,080
And that's another that's called augmented reality, I think. But I mean, we're going to get

766
01:20:24,320 --> 01:20:30,320
things, I mean, machines or whatever, extra bits so that we can actually have more fingers or

767
01:20:30,960 --> 01:20:35,920
more arms or whatever it is. So that would be quite, I mean, I guess I'm more interested in

768
01:20:35,920 --> 01:20:41,200
that aspect of interacting with reality than the virtual reality.

769
01:20:41,920 --> 01:20:45,600
It's fine because now David Chalmers has completely left consciousness. Well,

770
01:20:45,600 --> 01:20:51,200
he hasn't left it, but he's now very much focused on virtual reality and the philosophy of virtual.

771
01:20:52,560 --> 01:20:56,960
How do we behave? What are the moral, what are the ethical principles here, etc.

772
01:20:56,960 --> 01:21:01,200
But I think that's interesting because I would be more interested in what he thinks about, you know,

773
01:21:01,200 --> 01:21:09,440
having more hands on. And there's also work on having new senses. So for example, you can wear a

774
01:21:09,440 --> 01:21:15,440
belt through which you can feel where Magnetic North is. Is that useful?

775
01:21:17,920 --> 01:21:23,040
It's quite interesting because if you grow up learning how to use it, it definitely takes your

776
01:21:23,040 --> 01:21:27,120
priors. Yeah. And that's again, goes back to consciousness because

777
01:21:29,440 --> 01:21:35,280
and our subjective experience depends a great deal on what sensory inputs we have.

778
01:21:36,480 --> 01:21:40,080
And presumably someone who is blind from birth has a very different

779
01:21:40,720 --> 01:21:46,240
conscious experience from other people. And likewise, if we can have these extra senses,

780
01:21:47,200 --> 01:21:53,520
what will that do to us? I mean, again, I've jumped around a bit, but you know,

781
01:21:53,520 --> 01:21:57,680
this recent discovery that some women have four color receptors rather than three.

782
01:22:01,040 --> 01:22:10,480
Does it make any difference? Can you make use of it? So I guess I'm more interested in these

783
01:22:10,480 --> 01:22:17,360
developments. David Eagleman does a lot of work on sensory substitution. And how you like,

784
01:22:17,360 --> 01:22:23,120
he can wear a you can wear a sort of vest that will vibrate in a certain way. If you are blind,

785
01:22:23,120 --> 01:22:28,400
then you've got glasses and I need to like take in information via a camera and then

786
01:22:28,400 --> 01:22:33,120
vibrate in a certain way on your vest and then you know what reality is. You paint a new picture

787
01:22:33,120 --> 01:22:38,960
for yourself. But then that leads to the question, how much more could we augment this real experience?

788
01:22:39,920 --> 01:22:43,200
I agree, which is a lot more intriguing to think about.

789
01:22:45,120 --> 01:22:53,920
Do you believe in free will, Chris? Yes, I've been, I've took a long time. But again,

790
01:22:53,920 --> 01:22:58,880
it's very social thing. I think the experience of having free will is to some extent a social

791
01:22:58,880 --> 01:23:03,760
phenomenon. I mean, like I was talking about the ancient Greeks, perhaps didn't have,

792
01:23:03,760 --> 01:23:06,800
didn't think they had free will because the gods were telling them what to do.

793
01:23:06,800 --> 01:23:13,360
But if you then re-attribute that to me telling me yourself what to do, then

794
01:23:14,000 --> 01:23:18,640
you do have free will. But I'm very interested in, I mean, there's this interesting relationship

795
01:23:18,640 --> 01:23:24,400
between free will and responsibility. And we're all brought up to feel responsible for our actions.

796
01:23:24,400 --> 01:23:33,120
And this is why I was said very early on, my grandson would, if he hits his twin sister will say,

797
01:23:34,000 --> 01:23:38,480
I'm not responsible because it was an accident. So he's learned very early on to make the distinction

798
01:23:38,480 --> 01:23:43,120
between things you do deliberately and things that you do by accident. And this has become very

799
01:23:43,120 --> 01:23:49,200
important, at least in Western cultures, about, you know, it's a basis of legal things. You have to

800
01:23:49,200 --> 01:23:57,920
be, you have to have the intention to do something, not just, that didn't just happen. And I think

801
01:23:57,920 --> 01:24:02,640
this is something we learn on very early on. And we have this feeling that we're responsible for our

802
01:24:02,640 --> 01:24:09,920
actions. We have particularly a strong feeling of regret. And the mere fact that we can feel

803
01:24:09,920 --> 01:24:13,760
regret that I should have done something else implies that at the time we could have done

804
01:24:13,760 --> 01:24:19,520
something else. Otherwise, this doesn't make any sense. So I think, again, the feeling of regret,

805
01:24:19,520 --> 01:24:23,600
the feeling of being in control of our actions, these are very important aspects of our conscious

806
01:24:23,600 --> 01:24:30,320
experience. And they are the basis of our belief in free will. And they're very important for

807
01:24:30,320 --> 01:24:36,800
social cohesion and the rule of law, and understanding people, because we believe that people do

808
01:24:36,800 --> 01:24:44,560
things for reasons. And in that sense, we expect people to have, to some extent, free will.

809
01:24:46,000 --> 01:24:50,480
So I think that's very important. It's a cultural phenomena, but to some extent,

810
01:24:50,480 --> 01:24:54,800
is created by culture. And it's also very important for making culture work.

811
01:24:55,280 --> 01:25:01,120
So even though we have these experiments by Benjamin Libet, let's say, or

812
01:25:02,000 --> 01:25:07,920
Uri Maus does certain work on it as well, you still believe just because it's because of

813
01:25:07,920 --> 01:25:11,600
this fundamental link with moral responsibility, you have to take it into account.

814
01:25:12,800 --> 01:25:18,960
Patrick Haggard has followed up the Libet experiments in many studies. And what he has

815
01:25:18,960 --> 01:25:24,320
shown is that the experience of action is just like other perceptions. It's very Bayesian. So

816
01:25:24,400 --> 01:25:29,760
you have a prior, and you have an outcome. So your experience of performing an act is you have a prior

817
01:25:29,760 --> 01:25:35,600
intention, and you have the outcome, and they're bound together to give the experience of the action.

818
01:25:36,240 --> 01:25:43,600
And I think what Libet, the reason that Libet finds that, you know, say when I had the decision

819
01:25:43,600 --> 01:25:50,640
to do the action is later, is because of this binding phenomena. And you can move it around.

820
01:25:50,880 --> 01:25:55,040
That's what Patrick Haggard has shown. You can move the timing around by what happens afterwards.

821
01:25:55,760 --> 01:25:58,560
So in a sense, what happens afterwards is bound into your

822
01:26:02,480 --> 01:26:09,040
subjective experience of the act. So I think the timing is not the fact that it happens after

823
01:26:09,040 --> 01:26:14,400
the brain starts working, is not relevant to the problem of whether we have free will or not.

824
01:26:14,400 --> 01:26:17,520
It's part of it. It's the way the experience is created.

825
01:26:19,920 --> 01:26:27,360
To close, Chris, there's listeners they often love to when some of their favorite neuroscientists,

826
01:26:27,360 --> 01:26:32,720
psychologists, philosophers tell us some book recommendations or author recommendations in

827
01:26:32,720 --> 01:26:35,760
general with your favorite philosophers. If you had to give me five, at least.

828
01:26:36,080 --> 01:26:45,760
Within the field with two, I mean, relevant books. I mean, the most the book on consciousness

829
01:26:45,760 --> 01:26:52,880
that people should now read is Hacquan Lau's new book. In consciousness, we trust is that I think

830
01:26:52,880 --> 01:26:58,720
it's called it. Who's it? Sorry, repeat the author's name. Hacquan Lau. I haven't read it.

831
01:26:59,520 --> 01:27:04,400
Oh, I mean, he's worked with. Yes, it's very new. It only came out a month ago.

832
01:27:05,520 --> 01:27:14,320
So that tells you great. It's tough. It's not like analysis. It goes into the nitty gritty of

833
01:27:14,320 --> 01:27:19,200
how you do experiments and so on. Okay, so it's not like being you. I think Anil's is being you.

834
01:27:20,080 --> 01:27:31,680
Yeah, yeah. I'm basically recommending all my friends. Nick Shea wrote a few years and what's

835
01:27:31,680 --> 01:27:38,960
good about this Hacquan's book is it's I think it's often in the university press, but it's free.

836
01:27:38,960 --> 01:27:45,600
You can download it for free. Nick Shea is the philosopher who wrote a couple of years ago a

837
01:27:45,600 --> 01:27:53,440
book about representation, which is very important in neuroscience. And I can't remember what it's

838
01:27:53,440 --> 01:28:04,000
called, but that's also you can download it for free. Of course, the best, best philosophy book,

839
01:28:05,200 --> 01:28:10,320
if you're interested in Wittgenstein and people like that is Lodgy Comics.

840
01:28:10,560 --> 01:28:16,080
Have you, you know that one, which is the graphic novel on the life of Bertrand Russell?

841
01:28:21,120 --> 01:28:24,560
And obviously you have to read a Philip K. Dick book if you want to understand.

842
01:28:25,840 --> 01:28:29,200
I've got a really cool Philip K. Dick box set from Folius.

843
01:28:31,200 --> 01:28:35,600
But yes, do do androids dream of electric sheep is the sort of classic.

844
01:28:36,320 --> 01:28:41,120
But there's another one which has the lovely title of the penultimate truth.

845
01:28:46,080 --> 01:28:51,120
Gosh, it's very difficult stuff. I put you in the spot there. I know a lot of my guests

846
01:28:51,120 --> 01:28:54,000
can't get caught in the spot at that moment when I asked them that question.

847
01:28:56,320 --> 01:28:59,280
But I haven't, I'm not very good on books.

848
01:29:00,240 --> 01:29:04,880
If you think about the philosophers who or psychologists who really influenced your view

849
01:29:04,880 --> 01:29:08,080
and made made you the person you are today in terms of the thinker you are,

850
01:29:08,640 --> 01:29:11,040
would you think played some big roles?

851
01:29:11,040 --> 01:29:13,600
Well, I mean, there's Tim Shannis's book,

852
01:29:17,200 --> 01:29:21,040
which is called Neuropsychology and Mental Structure. I mean, that's quite an old book,

853
01:29:21,040 --> 01:29:29,120
but that that was very influential on my work. And obviously, I read, I mean,

854
01:29:29,120 --> 01:29:34,240
I read all the old stuff like Donald Hibbs, what was it called?

855
01:29:35,360 --> 01:29:38,400
Principles of behavior? I'm not sure.

856
01:29:44,400 --> 01:29:47,120
And going by even further, because I had to give the

857
01:29:47,440 --> 01:29:52,560
Bartlett Lecture recently, I read Frederick Bartlett's book called Remembering,

858
01:29:53,760 --> 01:29:58,640
which is from 1932. But that has all this sort of Bayesian approach

859
01:29:59,440 --> 01:30:05,840
that the memory is constructive and so on, and has a second section on social psychology,

860
01:30:05,840 --> 01:30:07,680
even the effects of culture.

861
01:30:12,000 --> 01:30:13,920
Probably anything on Helmholtz as well.

862
01:30:14,720 --> 01:30:19,120
Oh, yes. Well, Helmholtz is very difficult. There is a little book on Helmholtz,

863
01:30:19,840 --> 01:30:25,040
where I have to confess my German is not good enough to have read the original.

864
01:30:26,480 --> 01:30:32,960
I mean, the book that very much influenced me in early stages was this one,

865
01:30:32,960 --> 01:30:37,760
or his. Oh, nice. Okay.

866
01:30:39,520 --> 01:30:47,680
And what is it? He has labyrinths, it's called. But he has, I mean, they're all very short stories,

867
01:30:47,680 --> 01:30:53,920
but one of them is called Funes the Memorias, which is about someone who has some sort of brain

868
01:30:53,920 --> 01:30:59,200
damage, which means they can, their memory is completely altered, and they can remember everything,

869
01:30:59,680 --> 01:31:04,800
their episodic memory is perfect as a result of which they abandon concepts.

870
01:31:09,360 --> 01:31:13,040
Because they don't need to simplify. That's fascinating.

871
01:31:15,120 --> 01:31:20,720
Chris, thanks. I mean, everybody, when people look, read it, when people read your work,

872
01:31:20,720 --> 01:31:23,920
when people read like people like Friston's work, I mean, they're always curious,

873
01:31:23,920 --> 01:31:27,840
like, how do you guys get to the point where you are? Who are the thinkers who make you,

874
01:31:27,920 --> 01:31:32,880
who you are? It's fascinating to see the journey you guys take to get, to get to that point.

875
01:31:34,640 --> 01:31:36,000
Well, it's random, basically.

876
01:31:38,000 --> 01:31:41,520
You guys are there for us. I mean, like one day, if somebody asks me these questions,

877
01:31:41,520 --> 01:31:48,800
I'm going to say, like Chris, I mean, so it's great to know who influenced you and who played

878
01:31:48,800 --> 01:31:55,760
that big role for you guys. Do you think so, Chris? I mean, of course, it's mind, body, solution.

879
01:31:56,720 --> 01:31:59,280
Do you think we're going to find the mind, body, solution?

880
01:32:00,480 --> 01:32:02,960
We're going to solve this mind, body, problem anytime soon?

881
01:32:04,720 --> 01:32:05,440
I hope not.

882
01:32:06,960 --> 01:32:08,080
Is it the magic is gone?

883
01:32:08,800 --> 01:32:09,840
Where's the magic is gone?

884
01:32:09,840 --> 01:32:10,320
Yeah.

885
01:32:10,320 --> 01:32:15,360
But honestly, thanks to you at least listeners. I've taken one step closer to the mind, body,

886
01:32:15,360 --> 01:32:20,960
solution. And I really appreciate you accommodating me, knowing that we're having some

887
01:32:20,960 --> 01:32:23,360
horrible weather conditions here in South Africa.

888
01:32:23,360 --> 01:32:24,400
I gather, yeah.

889
01:32:24,400 --> 01:32:29,520
It's been crazy, like lots of flooding, lots of death. It's been quite intense.

890
01:32:30,800 --> 01:32:34,560
Right. But you're getting, it's getting back to normal.

891
01:32:35,760 --> 01:32:40,800
Guys, I appreciate your time and for rescheduling the meeting with me.

892
01:32:41,760 --> 01:32:45,600
Yeah, that was fine. Yeah. No, thanks. I enjoyed this chat very much.

893
01:32:46,560 --> 01:32:47,120
Thanks, Chris.

