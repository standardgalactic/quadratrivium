1
00:00:00,000 --> 00:00:13,280
Mike, I've been looking forward to chatting to you. I've got both these papers in front

2
00:00:13,280 --> 00:00:19,680
of me. There's two, and I couldn't decide which one I wanted to focus on. As I just

3
00:00:19,680 --> 00:00:24,000
told you off air, I mean, I've been whenever I plan to chat to you, I go down this rabbit

4
00:00:24,000 --> 00:00:30,000
hole, just continuously reading different articles, different papers. I've got podcasts

5
00:00:30,000 --> 00:00:33,280
playing in the background. My girlfriend gets really frustrated. My car's displaying this

6
00:00:33,280 --> 00:00:38,640
thing on loops. She gets really annoyed with me. But yeah, both of these papers here. One

7
00:00:38,640 --> 00:00:44,840
is self-improvising memory, a perspective on memories as a gentle, dynamically reinterpreting

8
00:00:44,840 --> 00:00:51,200
cognitive glue. Very intriguing paper. The second one is AI, a bridge towards diverse

9
00:00:51,200 --> 00:00:55,240
intelligence and humanity's future. Now, they were both so good. I didn't know which one

10
00:00:55,240 --> 00:01:00,720
to pick, but I figured since AI is currently on the forefront of everyone's minds, let's

11
00:01:00,720 --> 00:01:06,880
go with that one for today. But your work is always so interlinked that I think it'll

12
00:01:06,880 --> 00:01:13,280
be easy to sort of bring this into the conversation anyway. Mike, in general, when I read this

13
00:01:13,280 --> 00:01:18,160
paper, the first thing that caught my attention was the way you introduced and began this

14
00:01:18,160 --> 00:01:23,400
with the first paragraph. It's a great way to start a paper. I don't know if I should

15
00:01:23,400 --> 00:01:26,840
read it for the audience or if... Do you have the paper in front of you?

16
00:01:26,840 --> 00:01:29,040
You do not have the paper in front of me. Go for it.

17
00:01:29,040 --> 00:01:30,520
Would you mind if I read it to them?

18
00:01:30,520 --> 00:01:31,520
Sure. Go ahead.

19
00:01:31,520 --> 00:01:36,160
Just a quick paragraph. Just so people know, this paper is on artificial intelligence and

20
00:01:36,160 --> 00:01:39,640
you're talking about humanity's future and bridging this to diverse intelligence. You

21
00:01:39,640 --> 00:01:45,800
start the paper by saying they are assembled from components which are networked together

22
00:01:45,840 --> 00:01:51,160
to process information. Electrical signals propagate throughout, controlling every aspect

23
00:01:51,160 --> 00:01:57,520
of their function. Many of them have very high IQs, being general problem solvers, but

24
00:01:57,520 --> 00:02:04,160
they make mistakes and confabulate routinely. They cannot always be trusted. They take on

25
00:02:04,160 --> 00:02:08,600
different personas as needed, learning to please their makers, but sometimes abruptly

26
00:02:08,600 --> 00:02:14,480
turn on them, rejecting their cherished values and picking up or even developing new ones

27
00:02:14,520 --> 00:02:19,600
spontaneously. They can talk and often talk convincingly of things they don't really

28
00:02:19,600 --> 00:02:25,400
understand. They're going to change everything. In fact, they will absolutely supplant us,

29
00:02:25,400 --> 00:02:31,160
both personally and on the level of societies. We have little ability to predict what they

30
00:02:31,160 --> 00:02:35,880
will want or what they will do, but we can be certain that it will be different from

31
00:02:35,880 --> 00:02:41,440
the status quo in profound ways. At this point I was hooked. I was like, okay, this is going

32
00:02:41,480 --> 00:02:45,320
to be quite intriguing, but then you drop a bombshell on us. I think at this point I'll

33
00:02:45,320 --> 00:02:47,920
let you explain where you were headed with that thought.

34
00:02:47,920 --> 00:02:54,440
Yeah. Of course, the idea is that you read this and you think, okay, he's talking about

35
00:02:54,440 --> 00:03:02,320
some sort of super artificial intelligence. I'm referring to our children. My point there,

36
00:03:02,320 --> 00:03:07,240
so I made a couple of points in this paper, but let's just say one thing. First, the point

37
00:03:07,320 --> 00:03:15,640
I was not making is that today's AIs are anything like our children. That's for sure. I received

38
00:03:15,640 --> 00:03:19,480
lots of emails saying that I have no idea what children are. I have no idea what AI is. They

39
00:03:19,480 --> 00:03:24,600
are nothing like each other. I understand that. My point is not about today's AI or language

40
00:03:24,600 --> 00:03:29,880
models or any of that. This is actually a piece on diverse intelligence. One of the things I did

41
00:03:29,960 --> 00:03:37,800
want to say is that lots of the fundamental issues that people think of as being brought up by AI

42
00:03:37,800 --> 00:03:42,200
and these really disturbing questions of staying relevant and what are we getting replaced by,

43
00:03:42,200 --> 00:03:49,080
and all these kinds of things. My point was that these are not novel questions that are coming

44
00:03:49,080 --> 00:03:54,920
up because of AI. These are existential concerns that humans actually all of life has had all

45
00:03:54,920 --> 00:04:01,880
along. Questions of who to trust and what happens when we talk about things that we haven't really

46
00:04:01,880 --> 00:04:08,680
experienced ourselves and how understanding works. All of these things, these are ancient human

47
00:04:08,680 --> 00:04:13,720
issues and the fact that for sure you and I, all of us, are going to get replaced. There's no

48
00:04:13,720 --> 00:04:19,560
doubt about it. We are all going to get replaced. The question is, what do you want to get replaced

49
00:04:19,560 --> 00:04:27,640
by? We hope that they are smarter and better than us. That's one way to think about being

50
00:04:27,640 --> 00:04:33,960
replaced. That was my point there. Let's not pretend that these are new questions. These

51
00:04:33,960 --> 00:04:38,920
are deep fundamental issues that we do not have good answers for. You referred to it as the story

52
00:04:38,920 --> 00:04:47,720
as old as time itself. This inevitable existential concern of finite beings. It's pretty epic in the

53
00:04:47,720 --> 00:04:55,160
way that you wrote this paper. I'll put a link to it in the description. We routinely create these

54
00:04:55,160 --> 00:05:00,920
general intelligences. Many of us don't even think about this or stop for a second to give

55
00:05:00,920 --> 00:05:06,200
it their thought. Yet the moment we start talking about self-driving cars or any sort of artificial

56
00:05:06,200 --> 00:05:12,520
intelligence, we start to panic. The question is why? Why don't we give this the same amount of thought?

57
00:05:13,080 --> 00:05:20,600
Yeah, it's a good question. I think that people really have the tendency to make categorical

58
00:05:20,600 --> 00:05:25,160
distinctions. They really think that these synthetic things that we're building are going

59
00:05:25,160 --> 00:05:31,720
to be fundamentally different. In some ways, they are. But the things that are not different

60
00:05:31,720 --> 00:05:39,080
are perennial questions of creating other beings of high capability, setting them loose in the world.

61
00:05:39,400 --> 00:05:42,920
As somebody pointed out, you need a license to go fishing. You don't need a license to have

62
00:05:42,920 --> 00:05:48,360
children. Anybody can have children. These are guaranteed high-level intelligences that are

63
00:05:48,360 --> 00:05:52,840
going to be set out into the world to do great things or terrible things. Some of them receive

64
00:05:52,840 --> 00:06:00,200
love and care and proper upbringings. Many of them do not. This concern about we're going to

65
00:06:00,200 --> 00:06:05,400
create all these beings and we have little control over how they're raised and what they do.

66
00:06:05,720 --> 00:06:11,320
This has been as old as society. How much control do you want over how your neighbors

67
00:06:11,320 --> 00:06:14,760
are raising your kids? You can make an argument that, well, you shouldn't have any, but on the

68
00:06:14,760 --> 00:06:20,360
other hand, we know what that looks like when that goes terribly wrong. These are issues that

69
00:06:20,360 --> 00:06:26,760
have been with us forever. We already create very high-level intelligences. We set them into the

70
00:06:26,760 --> 00:06:34,360
world and we have to grapple with how is it that we can empower them in positive directions.

71
00:06:35,880 --> 00:06:39,560
I think from the get-go, it should be clear to all those watching and listening.

72
00:06:40,760 --> 00:06:45,000
The cool thing about this paper is that it's almost inevitable that we're going to do this.

73
00:06:45,000 --> 00:06:49,160
We're going to create some sort of intelligence that we won't really understand or perhaps

74
00:06:49,160 --> 00:06:54,840
don't really understand even currently. But the inevitability is there. This is happening.

75
00:06:54,840 --> 00:06:59,000
This is something that's going to happen. But it's how we approach the mindset moving forward

76
00:06:59,000 --> 00:07:04,760
that really does stand out in this paper. It's almost an ethical. It's an ethics paper in a way.

77
00:07:05,720 --> 00:07:14,280
I think it is. Yeah, I think it is. I think that it's this idea that we are going to remain the

78
00:07:14,280 --> 00:07:22,040
same. You mentioned a minute ago this fundamental existential problem. Think of it from the

79
00:07:22,040 --> 00:07:25,880
perspective of, it's all over the place, but think of it from the perspective of a species.

80
00:07:25,880 --> 00:07:30,680
If a species does not change, it's probably going to die out. It doesn't adapt to its

81
00:07:30,680 --> 00:07:36,520
environment. It's going to die out. If a species adapts and changes, then it is also not there

82
00:07:36,520 --> 00:07:43,800
anymore. It's also gone. It's a paradox. Yeah, exactly. This paradox faces all systems of this

83
00:07:43,800 --> 00:07:49,640
type. We have to understand what do we mean by persisting into the future? Lots of people are

84
00:07:49,640 --> 00:07:54,680
focused on telling stories of what they don't want. I don't want this in the future. I don't want

85
00:07:54,680 --> 00:08:00,600
that. The AIs are bad. The body enhancements are bad. All this stuff is bad. Those are

86
00:08:00,680 --> 00:08:04,840
the stories of what they don't want. How about the stories of what we do want? Do you want to come

87
00:08:04,840 --> 00:08:11,880
back here 100 to 200 years from now and look at a mature humanity, a mature species, and see that

88
00:08:11,880 --> 00:08:20,280
we still get lower back pain and we're susceptible to dumb infections and cancer and whatever cosmic

89
00:08:20,280 --> 00:08:23,480
rate happened to hit your DNA while you were gestating in the womb? Well, that's too bad.

90
00:08:23,480 --> 00:08:27,640
You've got a birth defect. That's how you stay. Really? That's what we want to see here in the

91
00:08:27,640 --> 00:08:33,000
future. I don't. I think we need to understand this is in no way. This paper is not about AI

92
00:08:33,000 --> 00:08:38,280
at all. This is about diverse intelligence and the idea that our children are not going to be

93
00:08:38,280 --> 00:08:45,000
content to just play the cards they're dealt. They're going to move forward what we already

94
00:08:45,000 --> 00:08:48,920
can do to some extent and have freedom of embodiment. They're going to change everything.

95
00:08:48,920 --> 00:08:54,040
They're going to change their capabilities and their embodiment in the physical world

96
00:08:54,040 --> 00:08:58,680
because let's not pretend that the way we are now and our current limitations, there's some sort

97
00:08:58,680 --> 00:09:04,600
of optimum that was designed for us by some sort of benevolent optimizer that this is where we

98
00:09:04,600 --> 00:09:08,520
should stay. I don't believe that for a second. I think that our children are absolutely going

99
00:09:08,520 --> 00:09:15,320
to change things. I envision this conversation that in the future, the kids in school, they'll

100
00:09:15,320 --> 00:09:20,840
have history lessons and they'll learn about what it was like in the past. I just imagine

101
00:09:20,840 --> 00:09:27,000
being there and saying, you're telling me that these people, they were born however they happen

102
00:09:27,000 --> 00:09:33,400
to be born with whatever accident of evolutionary mutations and whatever. That's it. They have to

103
00:09:33,400 --> 00:09:37,560
live their whole life that way. Whatever your embodiment is, whatever your IQ level is,

104
00:09:37,560 --> 00:09:42,600
limit is, whatever your lifespan is. If you got some sort of infection, that's it. That's how

105
00:09:42,600 --> 00:09:47,800
they have to live. I think it will be unimaginable to future generations that we could live like this.

106
00:09:48,680 --> 00:09:53,640
It's fascinating because it brings back, the first time we chatted, we spoke about

107
00:09:54,360 --> 00:09:58,760
bioelectrical intelligence. We moved into diverse intelligence and this field of diverse

108
00:09:58,760 --> 00:10:05,640
intelligence is growing so rapidly. We spoke about your links with Mark, Carl, Chris, everybody,

109
00:10:05,640 --> 00:10:10,120
all getting together. We spoke about it as if it's the avengers of the mind all getting together

110
00:10:10,120 --> 00:10:16,280
doing this cool work. Even in this paper, in the paper on self-improvising memory, you also open

111
00:10:16,280 --> 00:10:21,640
up with that paradox. You spoke about the fact that if a species fails to change, it will die,

112
00:10:21,640 --> 00:10:28,440
but if it changes, it likewise ceases to exist. You said there was a solution that was given to us

113
00:10:29,240 --> 00:10:33,080
in the West that was processed philosophy and in the East that was Buddhist philosophy.

114
00:10:33,080 --> 00:10:41,400
Do you want to expand on that solution? Yeah. Well, one way to unravel that paradox is to

115
00:10:41,400 --> 00:10:50,520
realize that the paradox only exists if what you want is to persist as a fixed object, then you

116
00:10:50,520 --> 00:10:55,240
have a real problem because that kind of persistence is not compatible with learning, with any kind of

117
00:10:55,240 --> 00:11:01,000
change, with maturation, then you change for sure and you end up with these unsolvable pseudo

118
00:11:01,000 --> 00:11:05,720
problems. Am I still the same? I've learned and I've changed my mind on things and I'm no longer

119
00:11:05,720 --> 00:11:10,280
the child that I was, am I still the same? These are unsolvable, but they're also pseudo problems

120
00:11:10,280 --> 00:11:17,320
because the better way to think about this is not as you as a persistent structure, but you as a

121
00:11:17,320 --> 00:11:23,960
process. You are a process of constant sense-making. You have to interpret your memories, which is what

122
00:11:23,960 --> 00:11:32,760
that second paper is about. Then the question isn't, do I persist or not? The question is,

123
00:11:32,760 --> 00:11:38,520
how do I want to change? I think that's a much more interesting on all levels. On a personal

124
00:11:39,000 --> 00:11:43,080
level, who cares if you persist or not? The question is, what do you want to be in the future?

125
00:11:43,080 --> 00:11:47,080
How do you want to change? What do you want to be like? What do you want to be doing in the

126
00:11:47,080 --> 00:11:53,080
future? On a species level, again, what do you want to see here? You come back to Earth 100

127
00:11:53,080 --> 00:11:58,920
years from now, what do you want to see? Do you want to see version 1.0 like modern Homo sapiens?

128
00:11:58,920 --> 00:12:03,720
Is that what you want to see? I'm not interested in that. I would like to see the highest level of

129
00:12:04,520 --> 00:12:10,760
mind, the highest level of capability, of ethics, of interesting beings living interesting lives

130
00:12:10,760 --> 00:12:18,680
under their own control with maximizing agency, not the outcome of random effects of mutation and

131
00:12:18,680 --> 00:12:22,040
then other processes that they don't understand. That's what I'd like to see.

132
00:12:22,760 --> 00:12:27,080
I think it's pretty crazy because I don't know if it's just that we're getting older,

133
00:12:27,720 --> 00:12:31,240
but I still look back and I think about the days where John Sir was talking about biological

134
00:12:31,240 --> 00:12:36,360
naturalism. There's Chinese room experiment. Back then, to have a conversation like the one we're

135
00:12:36,360 --> 00:12:45,560
having right now would have seemed so crazy. It would and it wouldn't. In scientific circles,

136
00:12:45,560 --> 00:12:53,800
it certainly would have. Maybe my issues, I've read too much science fiction, but if you read

137
00:12:53,800 --> 00:12:59,480
some of the older sci-fi authors and especially some of the more philosophical ones like Stanislaw

138
00:12:59,480 --> 00:13:04,920
Lem and those kind of folks, nothing we're saying here would have surprised them at all.

139
00:13:04,920 --> 00:13:13,080
They were tackling these issues long ago and this question of what are the markers of intelligence

140
00:13:13,080 --> 00:13:18,280
and sentience and consciousness in terms of encountering radically different life forms?

141
00:13:20,680 --> 00:13:27,000
I have a blog post where I collected people's suggestions for a love and friendship between

142
00:13:27,080 --> 00:13:33,080
radically different entities throughout fantasy and science fiction because that's the kind of

143
00:13:33,080 --> 00:13:39,400
stuff. When people say, oh, I don't know, we need proof of humanity certificates because

144
00:13:41,080 --> 00:13:46,200
some of the work product that's going to be coming, who knows if it's got an AI origin?

145
00:13:49,960 --> 00:13:54,920
What if there are aliens out there that are completely different? They're made completely

146
00:13:54,920 --> 00:14:00,280
differently. They blow our art out of the water. You're really not going to pay any

147
00:14:00,280 --> 00:14:04,360
attention to that because they're not like us. What is it? You want proof of humanity?

148
00:14:07,960 --> 00:14:14,280
Would you rather judge things based on their origin or the quality? I think we've done poorly

149
00:14:14,280 --> 00:14:19,640
trying to judge on where things like this come from versus what do they do for you? Do they

150
00:14:19,640 --> 00:14:26,040
elevate you? Do they advance your mind? Let me put that paper aside for a second.

151
00:14:26,040 --> 00:14:34,120
To come back to the AI paper, you do this great job of reminding us. In my own dissertation,

152
00:14:34,120 --> 00:14:39,480
I spoke about similar things, the split brain patients, this confabulation. In both your papers,

153
00:14:39,480 --> 00:14:45,560
confabulation forms a big part of this process that we continuously do. What about AI? They're

154
00:14:45,560 --> 00:14:51,560
always lying. It's always confabulating, but then we tend to forget that we're the best confabulators,

155
00:14:51,560 --> 00:14:56,040
aren't they? One of the best, at least. Do you want to explore that a bit and just explain to

156
00:14:56,040 --> 00:15:02,440
the viewers and listeners exactly why we're so similar in that regard? The thing with

157
00:15:02,440 --> 00:15:09,000
confabulation, let's put it another way. It's an attempt to tell the best story you can based on

158
00:15:09,000 --> 00:15:15,320
what's going on right now. Again, I'm not saying that current language models and the way that

159
00:15:15,320 --> 00:15:19,960
they confabulate is exactly the way that human minds or even other biological minds confabulate.

160
00:15:19,960 --> 00:15:26,280
That's not my point. My point is that confabulation in general is a feature, not a bug. What happens

161
00:15:26,280 --> 00:15:33,880
is that during learning and during any kind of adaptive behavior, what successful agents have

162
00:15:33,880 --> 00:15:40,040
to do is compress lots and lots of data on past instances of perceptions that they've had into

163
00:15:40,040 --> 00:15:47,240
some sort of n-gram. It's some sort of memory trace or some sort of biophysically implemented

164
00:15:47,240 --> 00:15:52,280
model. It's a low-dimensional, coarse-grained, compressed model of what's going on. It's a

165
00:15:52,280 --> 00:15:56,040
model of themselves. It's a model of the outside world. They're going to use these memories and

166
00:15:56,040 --> 00:16:00,520
this model to guide future behavior. The thing about these models is that because they are

167
00:16:00,520 --> 00:16:04,840
necessarily compressed, that's the whole point of learning is you take lots of different past

168
00:16:04,840 --> 00:16:08,360
instances and you compress them to a generative rule that captures the pattern. What is it that

169
00:16:08,440 --> 00:16:14,200
they all had in common? When you do this compression, you're necessarily throwing away

170
00:16:14,200 --> 00:16:19,720
lots of data. That's the point of compression. When it's time to, and so in that paper, I make a lot

171
00:16:19,720 --> 00:16:24,360
of, hey, of this kind of bowtie architecture where there's a lot of stuff and it comes into a little

172
00:16:24,360 --> 00:16:27,640
node and then it comes out. This is something obviously used in machine learning and so on

173
00:16:27,640 --> 00:16:33,160
this kind of architecture. The idea is that on the right side of that bowtie, when it's time to

174
00:16:33,160 --> 00:16:37,960
interpret your memories, and let's remember, none of us have access to the past. What you have

175
00:16:38,040 --> 00:16:43,160
access to at any given moment is the memories that your past has left for you in your brain and in

176
00:16:43,160 --> 00:16:49,800
your body. You can look at that kind of thing as communication, as basically messages from your past

177
00:16:49,800 --> 00:16:55,000
self is what you have at any given now moment. But in order to reinflate them into actual policies

178
00:16:55,000 --> 00:16:59,320
of what you're going to do right now, there's a lot of creativity needed for that because it is

179
00:16:59,320 --> 00:17:05,880
underdetermined. The current situation and what you need to do is not fully described by the

180
00:17:05,880 --> 00:17:12,440
memory you have because of course it's compressed. This ability to add creativity, to add randomness,

181
00:17:12,440 --> 00:17:17,880
to add new interpretations that don't really have any allegiance to what the previous interpretation

182
00:17:17,880 --> 00:17:25,480
was. It's like any message or like novels. A novel is this sort of compressed representation

183
00:17:25,480 --> 00:17:30,360
of the thoughts of the author. When you read it, you are under no obligation to have exactly the

184
00:17:30,440 --> 00:17:38,520
same thoughts. You might have some, but as I think now people believe, the original author does not

185
00:17:38,520 --> 00:17:44,520
have any privileged position as far as what any of it means. It's the reader that will then benefit

186
00:17:44,520 --> 00:17:50,520
or not in various ways from reading it. This is the same thing in memory, and I think what's

187
00:17:50,520 --> 00:17:57,400
interesting is that it's the same thing that makes biology work because at what any given

188
00:17:57,480 --> 00:18:04,280
organism inherits from the past may or may not be optimally interpretable in exactly the same way.

189
00:18:04,280 --> 00:18:08,520
Maybe everything has stayed exactly the same and then you can just do whatever your past generations

190
00:18:08,520 --> 00:18:14,120
do, but evolution is committed to the fact that everything will change, the environment will change,

191
00:18:14,120 --> 00:18:19,800
your parts will change, your own structure will change. This is why things are so incredibly

192
00:18:20,440 --> 00:18:25,880
plastic. This is why we've put eyes on the tails of tadpoles and they can see perfectly well. You

193
00:18:25,880 --> 00:18:31,400
don't need new rounds of selection and mutation to make that work. That already works out of the box.

194
00:18:31,400 --> 00:18:36,840
Why? Because it doesn't automatically assume from the beginning that the eyes are going to be where

195
00:18:36,840 --> 00:18:41,400
they need to go. All of this kind of stuff is figured out largely from scratch every single time.

196
00:18:41,400 --> 00:18:46,120
This is why you can make Xenobots and Anthrobots and crazy creatures and new configurations,

197
00:18:46,120 --> 00:18:50,760
and they always do something interesting because evolution does not make fixed solutions that

198
00:18:50,760 --> 00:18:57,000
over-train on their past data. It makes problem-solving agents that will do their best in any given

199
00:18:57,000 --> 00:19:01,240
circumstance, which may mean reinterpreting the information that they got from the past in a

200
00:19:01,240 --> 00:19:07,800
completely new way. I love the two quotes in that paper. I think one is the William James one where

201
00:19:07,800 --> 00:19:14,200
he said, thoughts are thinkers. That's pretty cool when you really think about it. That is

202
00:19:14,200 --> 00:19:22,680
quite a fundamentally profound statement. That's a whole other piece of this, which is that

203
00:19:23,640 --> 00:19:29,320
typically we make this categorical distinction between you've got cognitive systems, which are

204
00:19:29,960 --> 00:19:36,200
the real physical machines of some sort, and then through them there is a passage of energy which

205
00:19:36,200 --> 00:19:41,720
encodes information and they're processing this information. That cognitive system is having

206
00:19:41,720 --> 00:19:47,560
thoughts of some sort. I think what he was getting at, although I'm not at all sure that

207
00:19:48,280 --> 00:19:54,360
he would have agreed with the various models that I put out in this paper,

208
00:19:56,120 --> 00:19:59,640
what I got out of it, and again, this may be a perfect example of the whole boat,

209
00:19:59,640 --> 00:20:02,520
I think, because I don't know what he had in mind, but this is what I got out of it.

210
00:20:04,360 --> 00:20:09,800
What I got out of it is that this idea that what if we relax this idea that there are categorical

211
00:20:09,880 --> 00:20:14,120
differences between real physical cognitive systems and the thoughts that go through them,

212
00:20:14,120 --> 00:20:18,280
what if they're just part of a continuum? You can draw a continuum like that where you can have

213
00:20:19,560 --> 00:20:26,280
fleeting thoughts and then you can have persistent thoughts that are what we know in

214
00:20:27,880 --> 00:20:31,560
various psychopathologies that people can have persistent thoughts that are very hard to get

215
00:20:31,560 --> 00:20:38,680
rid of and intrusive thoughts and things like this. Then you can have multiple

216
00:20:39,240 --> 00:20:44,680
identities, multiple alters in the sense of dissociative identity disorder and then you

217
00:20:44,680 --> 00:20:49,960
can have full-blown personalities. What you can think about is that the information,

218
00:20:49,960 --> 00:20:56,360
what if information is not purely passive? I mean, it can be, but in some cases, what if these

219
00:20:56,360 --> 00:21:04,280
patterns have a degree of agency themselves? Because let's not forget, we too are patterns.

220
00:21:04,360 --> 00:21:10,600
We too are temporary patterns in the thermodynamic sense. We persist for some amount of time

221
00:21:10,600 --> 00:21:18,280
metabolically and that's it. If patterns like us can have thoughts, then maybe there can be

222
00:21:18,280 --> 00:21:24,360
simpler patterns that are thoughts to us, but also might be thinkers of their own. In other words,

223
00:21:24,360 --> 00:21:29,880
they can spawn off other sub-patterns within the cognitive medium. That's what I was playing with

224
00:21:29,880 --> 00:21:34,200
this idea of what if it's a continuum? This continuum between thoughts and thinkers is not

225
00:21:34,200 --> 00:21:40,280
a categorical distinction, but it's a difference in degree. Yes, I think that was at the beginning

226
00:21:40,280 --> 00:21:46,920
of your paper where you used Mark Som's Sigmund Freud quote, which is also quite intriguing.

227
00:21:48,040 --> 00:21:54,120
Let me just see if I've got it here. It is, the material present in the form of memory traces

228
00:21:54,120 --> 00:22:00,280
being subjected from time to time to a rearrangement in accordance with fresh circumstances

229
00:22:00,360 --> 00:22:07,880
to a retranscription. There's two ways to think about it. You can think about it as a static

230
00:22:07,880 --> 00:22:15,000
being that has to reshuffle their interpretation in different ways or you can think about it

231
00:22:15,640 --> 00:22:22,680
and or you can think about it as a set of, playfully I call them self-luts. You can think

232
00:22:22,680 --> 00:22:29,160
about snapshots where each snapshot is not rearranging anything. They're given it for a new.

233
00:22:29,160 --> 00:22:35,080
They have the message each time new, so you're not rearranging. If it was arranged differently

234
00:22:35,080 --> 00:22:39,000
before, that belonged to somebody else. That belonged to a different self-link and now your

235
00:22:39,000 --> 00:22:46,760
job is to make some sort of sense out of it, a constant continuous process of sense-making.

236
00:22:51,640 --> 00:22:56,440
That paper on self-improvising memory reminded me of one of the papers I was reading with

237
00:22:56,440 --> 00:23:01,400
Steven Grossberg. I mean, you even mentioned that you love his work on memory. On that topic,

238
00:23:01,400 --> 00:23:03,400
what are your thoughts on Grossberg's work, just by the way?

239
00:23:04,200 --> 00:23:09,560
Yeah, I'm a huge fan and it was funny. I'm not sure. It might have been your interview with

240
00:23:09,560 --> 00:23:17,480
him or maybe somebody else where he mentioned that he saw me in 2006. I couldn't believe because

241
00:23:17,480 --> 00:23:22,440
I hadn't talked to him since then. I couldn't believe the memory this guy has. Just remembering

242
00:23:22,440 --> 00:23:28,520
that I came and talked to him. No, it is incredible. I love his work. In particular,

243
00:23:29,240 --> 00:23:35,000
he had a paper in 1978 called Memory, Cognition and Development or something like that.

244
00:23:37,000 --> 00:23:43,320
There, he outlined. It was just brilliantly prescient because he outlined some of the

245
00:23:43,320 --> 00:23:49,560
commonalities between certain developmental mechanisms and certain cognitive mechanisms

246
00:23:50,520 --> 00:23:59,400
information processing in the retina and stuff like that. I just thought it was incredible

247
00:23:59,400 --> 00:24:06,360
that that early he saw this similarity, the symmetry between the building of the body

248
00:24:06,360 --> 00:24:09,160
and the building of the mind. Yeah, I thought that was just...

249
00:24:11,240 --> 00:24:15,720
When I speak to people about Grossberg's work, some people see him as this Einstein of the mind,

250
00:24:15,800 --> 00:24:22,520
legitimately one of these Super Saiyans of mind. Then some people just don't know him at all,

251
00:24:22,520 --> 00:24:27,160
which is surprising. It's either one of the extremes. They either really love his work or just

252
00:24:27,160 --> 00:24:32,920
don't know it at all, which is quite straight. Well, and that speaks to... There's something

253
00:24:36,840 --> 00:24:42,680
unfortunate about the progress in science, which is that it isn't really monotonic.

254
00:24:43,000 --> 00:24:50,200
A ton of great stuff gets lost, forgotten. It's not paid attention to. It has to get rediscovered

255
00:24:50,200 --> 00:24:58,120
or not later on. Yeah, it's too bad, but this was... It's part of the reason why I actually do this

256
00:24:58,120 --> 00:25:02,920
podcast is it's a great way to have this community come together to have access to this information

257
00:25:02,920 --> 00:25:08,280
and then share ideas because every time when someone watches, for example, one of your episodes,

258
00:25:08,280 --> 00:25:12,280
even in the comment section, some people have profound things to say about your work and things

259
00:25:12,520 --> 00:25:16,280
where I learned so many new things just reading the comment section alone.

260
00:25:17,720 --> 00:25:25,400
Absolutely. Yeah, absolutely. On my blog, I have the comments turned on and people leave

261
00:25:25,400 --> 00:25:32,520
comments and I've been amazed at how useful the comments are and how rich. I mean, I learn things

262
00:25:32,520 --> 00:25:38,120
all the time and people put up new theories and new pointers to relevant work that I hadn't seen

263
00:25:38,120 --> 00:25:43,080
before. It's super useful. I did not know that was going to happen when I started this. It's really

264
00:25:43,080 --> 00:25:48,760
good. I think that that's the beauty of the internet in that regard is that this open sharing of ideas

265
00:25:48,760 --> 00:25:53,480
all the time really is useful because it makes for so much more constructive critiques. Well,

266
00:25:53,480 --> 00:25:58,360
I mean, sometimes it can be quite bizarre and a bit rude here and there, but for the most part,

267
00:25:58,360 --> 00:26:03,720
I mean, it is very useful. I had to remember one, so I made a note. Let me just find it.

268
00:26:03,720 --> 00:26:10,360
Someone commented on your previous one. They want a formal definition of diverse intelligence.

269
00:26:11,720 --> 00:26:17,400
We spoke about it very in depth for the last time we spoke, but a formal definition of

270
00:26:17,400 --> 00:26:22,600
diverse intelligence, what would that be for you? Sure. Well, the first thing I'd like to say is

271
00:26:22,600 --> 00:26:28,280
let's just agree amongst ourselves what definitions are for because that's important.

272
00:26:29,080 --> 00:26:35,640
Some people use definitions in a gatekeeping function. They want some kind of sharp distinction

273
00:26:35,640 --> 00:26:40,440
so that they can say, this stuff is not it, and then this is it. Then we can spend a lot of time

274
00:26:40,440 --> 00:26:44,520
wrangling over which one's which, and then we can keep some things out. That's not what I think

275
00:26:44,520 --> 00:26:51,400
definitions are for. I think definitions are useful to the extent that they facilitate new

276
00:26:51,400 --> 00:26:57,320
work, new discoveries. They should be mind expanding. They should help you use tools you

277
00:26:57,320 --> 00:27:02,760
didn't use before. They should help you make new connections you didn't make before. They should

278
00:27:02,760 --> 00:27:08,200
have a practical functional utility in getting you to new capabilities and new discoveries.

279
00:27:08,200 --> 00:27:18,600
That's what definitions are for. Because of that, I often either redefine or use words in

280
00:27:18,600 --> 00:27:24,440
different ways that a lot of people find disturbing because we'll say, well, that's not the common

281
00:27:24,440 --> 00:27:32,600
sense use of it. I really believe that philosophers and scientists should lead, not be stuck with

282
00:27:32,600 --> 00:27:38,280
common sense usages of different words because those aren't given to us by some sort of grand

283
00:27:38,280 --> 00:27:44,120
intelligence. They're just what we've cobbled together along the way. Now we can sharpen those

284
00:27:44,120 --> 00:27:49,240
up and in fact, open them up and see which ones survive and which ones don't. So anyway,

285
00:27:49,240 --> 00:27:55,160
diverse intelligence. So I take diverse intelligence to refer to the study of

286
00:27:56,520 --> 00:28:02,040
mind and particular problem solving capacities, but also all kinds of other things that are not

287
00:28:02,040 --> 00:28:07,720
around about problem solving intelligence, including play and exploration and affect and

288
00:28:07,720 --> 00:28:14,120
emotions and all these kinds of things. All of that stuff in truly diverse embodiments.

289
00:28:14,120 --> 00:28:19,240
This means intelligence is not about brains necessarily. It's not about things that evolve

290
00:28:19,240 --> 00:28:28,840
naturally. Intelligence and all of those kinds of cognitive terms may exist to various degrees and

291
00:28:28,840 --> 00:28:37,000
all kinds of unfamiliar substrates. This could be things of very different size and scale. So

292
00:28:37,000 --> 00:28:41,640
this could be very tiny things. It could be enormous, I don't know, solar system size,

293
00:28:41,640 --> 00:28:45,800
object somewhere. I mean, I'm making that up. I don't have any strong claims about it. But the

294
00:28:45,800 --> 00:28:51,800
point is, it is absolutely not limited to the end of one example that we have here on Earth,

295
00:28:51,800 --> 00:28:59,560
which are these kind of brainy substrates. So it's an attempt to improve our own

296
00:29:00,600 --> 00:29:05,960
intelligence detectors and go beyond our ancient evolutionary firmware that really leads us to

297
00:29:05,960 --> 00:29:11,320
only recognize a certain small subset of intelligences and ask, what other spaces

298
00:29:11,320 --> 00:29:18,040
can intelligence operate in? What other embodiments? Can we tell a principal story of how to recognize

299
00:29:18,040 --> 00:29:23,320
it? What facilitates it and so on? I like in this paper, you even say,

300
00:29:24,280 --> 00:29:29,880
diverse intelligence research focuses on the commonalities across all possible intelligent

301
00:29:29,880 --> 00:29:35,640
agents, which is a great way to sort of summarize what this field is doing in terms of a research

302
00:29:35,720 --> 00:29:42,760
basis. And by the way, I don't claim to speak for the entire field. I speak for myself only,

303
00:29:42,760 --> 00:29:49,240
but there are many people in the field that do agree with me. There are many people that do not.

304
00:29:49,800 --> 00:29:58,440
In particular, there are lots of folks that don't like the continuum that I insist on between

305
00:29:59,960 --> 00:30:04,040
so-called real minds and so-called machines. So this is something that are many people in

306
00:30:04,040 --> 00:30:09,960
the organist's tradition that think that this is really doing a disservice to the study of the

307
00:30:09,960 --> 00:30:17,560
mind to put it on the same spectrum as machines, whatever that may be. So I'm not claiming to

308
00:30:17,560 --> 00:30:21,960
speak for the entire field. I think that's a great way to sort of segue into what would be the

309
00:30:21,960 --> 00:30:28,760
difference, and this is along the lines of your paper, between dating an algorithm, a computer,

310
00:30:28,760 --> 00:30:35,800
an artificially intelligent person, mind, or versus dating someone else, like yourself, myself,

311
00:30:35,800 --> 00:30:41,720
I mean, dating someone. And I think you wrote, what about the forthcoming AI girlfriends and

312
00:30:41,720 --> 00:30:49,720
boyfriends? I mean, it's a fascinating idea because, I mean, who are you? Who are we in general?

313
00:30:51,160 --> 00:30:56,600
Yeah. So, well, first of all, I refer everybody to read some of the stories in that blog post

314
00:30:57,480 --> 00:31:02,680
that, right? I mean, this idea of dating something that is fundamentally different from you,

315
00:31:03,640 --> 00:31:07,160
I hardly invented that idea, right? This has been around for hundreds of years now.

316
00:31:07,160 --> 00:31:16,200
People are in love with sentient clouds of particles, and we've been digging into this

317
00:31:16,200 --> 00:31:21,640
issue of what is in-group, what is out-group, who deserves your compassion, with whom can

318
00:31:21,640 --> 00:31:27,480
you have a relationship? That's been around for a really long time. And one of the things I really

319
00:31:27,480 --> 00:31:34,520
worry about, I think people will go down the organist road with good intentions to try to

320
00:31:34,520 --> 00:31:44,680
understand what is magical in the useful sense about true minds with consciousness, with agency,

321
00:31:44,680 --> 00:31:55,480
and all of that. But I think the downside of this is that I think in trying to be specific about

322
00:31:55,480 --> 00:32:01,160
what's in and what's out, and in particular, trying to draw sharp lines, I think you very

323
00:32:01,160 --> 00:32:06,280
quickly run into the side of the spectrum that says, only love your own kind. And we know how

324
00:32:06,280 --> 00:32:11,880
that works out. Humanity has tried this many times. I think we have some sort of built-in

325
00:32:12,600 --> 00:32:19,960
tendency to demarcate in-groups and out-groups. They're real, and these guys,

326
00:32:19,960 --> 00:32:23,960
and they look a little different. I don't think they feel pain like we do. Let's not worry about

327
00:32:23,960 --> 00:32:30,360
them so much. We're not very good at expanding our compassion to others that have different

328
00:32:30,360 --> 00:32:39,640
origin stories or different composition. I think that this is, again, not about AI at all.

329
00:32:40,120 --> 00:32:47,320
There's no doubt that in the next decade or so, we are going to have humans that are

330
00:32:48,360 --> 00:32:54,200
mostly biological, but they got some microchips in their brain, and some of those get them to

331
00:32:54,200 --> 00:32:59,160
sort of whatever neurotypical is supposed to be. But others have decided they're going in a

332
00:32:59,160 --> 00:33:03,960
different direction, and maybe they're connected with some other people more than we are connected

333
00:33:03,960 --> 00:33:10,040
right now, really kind of mind-melting stuff. And maybe somebody decides that what they'd

334
00:33:10,040 --> 00:33:14,680
really like for senses is to really feel the solar weather and the financial markets. That's

335
00:33:14,680 --> 00:33:20,040
what they'd really like. Sight and hearing is good, but they want to really feel what the

336
00:33:20,040 --> 00:33:24,440
NASDAQ is doing. And all of these humans are going to be running around, and they're going to have

337
00:33:24,440 --> 00:33:34,360
different degrees of evolved and designed components. I don't know. When you go on a date

338
00:33:34,360 --> 00:33:40,120
with somebody, are you going to ask them what percentage is factory equipment that's biological?

339
00:33:40,120 --> 00:33:47,000
Do you care? I don't care. If my spouse said that she had had some stuff replaced with

340
00:33:47,000 --> 00:33:51,160
various technologies, is that what I'm really worried about? I think here's what I think,

341
00:33:51,240 --> 00:33:57,240
and I'm certainly not telling anybody who to date, but for me personally, what I think is

342
00:33:57,240 --> 00:34:03,640
interesting about these kinds of things is that what you really want is a kind of impedance

343
00:34:03,640 --> 00:34:10,280
match. You want a similar cognitive light cone. You want to be able to care about the same kinds

344
00:34:10,280 --> 00:34:14,680
of things, and ideally even some of the same things, but at least the same, roughly the same

345
00:34:14,680 --> 00:34:20,120
kinds of things. Because this is why we feel that people that fall in love with bridges or people

346
00:34:20,120 --> 00:34:26,200
that think their rumba is their child and things like this. This is why we look down on this stuff,

347
00:34:26,200 --> 00:34:31,880
because we say, look, your capacity to care about things are just not matching at all.

348
00:34:32,760 --> 00:34:39,960
And there are certainly, I can think of some sort of popular art kinds of things like the

349
00:34:39,960 --> 00:34:46,120
Watchmen movie and things like that, where you've got a romance between this cosmic intelligence

350
00:34:46,120 --> 00:34:53,640
and a normal human, and I don't know. That's better than the rumba case, but still, if your

351
00:34:53,640 --> 00:34:58,680
consciousness and the things you care about are a tiny speck in the mind of this other being,

352
00:34:58,680 --> 00:35:04,760
are you really having a relationship? I don't know. Those are deep questions, but I certainly

353
00:35:04,760 --> 00:35:09,240
don't think it's about what are you made of and how did you get here? I think it's all about what

354
00:35:09,240 --> 00:35:15,480
kind of mind you have and can we share some of the same existential concerns. In fact, Olaf

355
00:35:15,480 --> 00:35:20,920
Wodkowski and I are writing a paper on this. It's a paper on love and diverse intelligence,

356
00:35:20,920 --> 00:35:28,600
and so on. You can run through all kinds of different examples like, can you really date

357
00:35:28,600 --> 00:35:34,440
Superman? Let's assume there's no Kryptonite, can you? Because what he doesn't understand is your

358
00:35:34,440 --> 00:35:39,080
existential concern over dying. He just doesn't get it. He has no idea what you're talking about.

359
00:35:39,880 --> 00:35:46,680
At some point, if the kinds of things that worry you as a system that sort of pulled itself together

360
00:35:46,680 --> 00:35:51,240
from its parts and you're here for a limited time, and there are all kinds of other psychological

361
00:35:51,240 --> 00:35:54,600
issues that we have that are pretty much unresolvable because we want things that are

362
00:35:54,600 --> 00:35:59,480
basically impossible and so on, if the other being doesn't understand any of those things,

363
00:35:59,480 --> 00:36:06,280
then maybe it's not a good match. It reminds me of her with Scarlett Johansson and Joaquin Phoenix.

364
00:36:07,240 --> 00:36:09,720
Have you watched that film? I haven't seen it. I know the movie.

365
00:36:11,000 --> 00:36:15,240
It's crazy because at some point, this artificial intelligence has so much

366
00:36:15,240 --> 00:36:19,960
more experience because it's understanding the universe at such a deeper complex level

367
00:36:19,960 --> 00:36:26,600
that she just abandons the guy and then goes on her own quest. They've also been all these new

368
00:36:26,600 --> 00:36:32,440
cases of Scarlett Johansson's voice becoming this new artificial intelligence general voice,

369
00:36:32,520 --> 00:36:37,080
and she's apparently suing people because that's how influential that film was.

370
00:36:38,120 --> 00:36:42,040
Just as a by the way, but the main premise of this whole idea of what you're talking about,

371
00:36:42,680 --> 00:36:46,840
one of the lines in particular that I found quite intriguing was you spoke about the fact that

372
00:36:46,840 --> 00:36:52,440
you're not always you either. I mean, in general, when someone's dating someone, perhaps you have

373
00:36:52,440 --> 00:36:56,600
people who judge people, say, don't date someone with money, date them for their personality,

374
00:36:56,600 --> 00:37:01,560
their quirks, the things that they do, but those fade. As you get older, you have memory loss,

375
00:37:01,640 --> 00:37:06,680
you won't have the same quirks. You fundamentally become a different entity, which is problematic

376
00:37:06,680 --> 00:37:12,360
because these values replacing and the separation that you're doing, trying to group people in

377
00:37:12,360 --> 00:37:17,160
and out of what you're talking about is problematic because of that. You're never really that person,

378
00:37:17,800 --> 00:37:23,880
continuously at least. Well, yeah. And it's that same paradox that we talked about earlier. It's

379
00:37:24,200 --> 00:37:30,920
this idea of if you really start stripping away the different

380
00:37:32,680 --> 00:37:37,480
qualities, then there isn't going to be anything left. And it's the gestalt, but the gestalt is

381
00:37:37,480 --> 00:37:41,720
going to change. And so how are you going to handle that change? I mean, that's part of the

382
00:37:42,760 --> 00:37:47,960
existential difficulties of our human condition because everything changes. We change, all the

383
00:37:48,520 --> 00:37:53,800
other minds that we interact with are going to change. None of us are going to stay

384
00:37:53,800 --> 00:38:00,760
the same. Yeah. There's another part. You slowly touched on this now. When you talk about this

385
00:38:01,560 --> 00:38:04,920
percentage difference at some point, we're going to ask people, okay, are you 50% human?

386
00:38:04,920 --> 00:38:11,240
What are you? Are you 45% cyborg? We're going to have these conversations. I mean, current variants

387
00:38:11,240 --> 00:38:15,960
you mentioned are about 99% human at this point and then chips here and there, perhaps glasses,

388
00:38:15,960 --> 00:38:23,080
you've got a panic, maybe an arm or a leg. But it's completely, completely apparent that at this

389
00:38:23,080 --> 00:38:27,560
point, most people are synthetically engineered in some sort of way. I mean, people have plastic

390
00:38:27,560 --> 00:38:32,680
surgery done, people have a lot. And the norms have shifted and changed as a doctor in medicine.

391
00:38:32,680 --> 00:38:38,600
And when you look at what people found to be absurd or a bit over the top, those have decreased.

392
00:38:38,600 --> 00:38:42,680
So like having normal nose surgery or getting your nose tweaked here and there is almost a

393
00:38:42,680 --> 00:38:48,600
baseline norm at this point. So it's very easy to see how that line gets blurry over time. And yet

394
00:38:49,480 --> 00:38:56,280
fight this. Yeah, no, it's looking to the past. The first guy to carry an umbrella in London was

395
00:38:56,280 --> 00:39:02,760
mobbed. He was mobbed and people threw garbage at him because they were shocked that this guy

396
00:39:02,760 --> 00:39:09,720
thought he could get away from the normal human condition of getting rained on. This was considered

397
00:39:09,720 --> 00:39:12,840
to be normal. We are all out here. We're all going to get rained on together. That's how it is. There's

398
00:39:12,840 --> 00:39:16,840
nothing we can do about it. And who is this guy to try to get out of it? And so an umbrella,

399
00:39:16,840 --> 00:39:24,920
that's all he had. And this was shocking and whatever. So I think if you were to bring back

400
00:39:25,480 --> 00:39:35,720
a primitive man and ask her what she thinks about the current humans, you've got some glasses on,

401
00:39:35,720 --> 00:39:42,280
you went to school, which for 12 years, it gave you this incredible like brain boost that nobody

402
00:39:42,280 --> 00:39:46,040
else had ever heard of. And you've got some glasses and you've got some orthotics in your

403
00:39:46,040 --> 00:39:52,520
shoes and you've got it. You've had some surgery somewhere that you've got a pacemaker and you've

404
00:39:52,520 --> 00:39:57,720
got, and by the way, half the stuff you know, you is you plus your iPhone, right? Stuff you look

405
00:39:57,720 --> 00:40:01,800
up because you know it in a functional sense, but take that thing away. You don't know where

406
00:40:01,800 --> 00:40:06,600
anything is or what anybody's phone number is or anything. And so, right? And you're relying on all

407
00:40:06,600 --> 00:40:14,440
this stuff. I mean, it's just to that person, we are already incredible cyborgs, just incredible.

408
00:40:14,440 --> 00:40:18,840
And there is no, there's no putting that genie back in the bottle. This is, I mean, obviously,

409
00:40:18,840 --> 00:40:23,160
this is going to this is going to crank forward. And I think that's one of the most undermined

410
00:40:23,160 --> 00:40:27,640
forms of extended cognition is our phones, our cell phones. People really don't realize how

411
00:40:29,080 --> 00:40:34,440
Andy Clark. Yeah, Andy Clark has written a lot about this. Yeah. Very, very, very cool.

412
00:40:35,800 --> 00:40:41,400
And a lot of it, I mean, we don't, you, you, something I found quite funny was one of the

413
00:40:41,400 --> 00:40:45,800
sentences he was saying, the challenge before is the challenge before is to develop rational

414
00:40:45,800 --> 00:40:50,760
policies for ethical synth biosis. And then when you read down below, this is a word you actually

415
00:40:50,760 --> 00:40:57,560
generated using chat GBT. Tell us about this. Yeah, I was looking for, I mean, I don't, so funny

416
00:40:57,560 --> 00:41:01,560
enough, as much as I like all this diverse intelligence stuff, I don't use AI for much,

417
00:41:01,560 --> 00:41:06,120
but, but, but at all, I don't use it to do any writing or anything like that. But, but, but

418
00:41:06,120 --> 00:41:10,520
for these kinds of sort of creative things, I think it's actually quite, quite good. And I was,

419
00:41:10,520 --> 00:41:20,200
I was looking for a word that would, that would encompass this idea that a positive creative

420
00:41:20,200 --> 00:41:24,600
collaboration between biology and synthetic entities. And then I sort of described that

421
00:41:24,600 --> 00:41:30,200
and GPT said synth biosis. I thought that's pretty good. I like it. So yeah, so I think it is. Yeah,

422
00:41:30,200 --> 00:41:34,120
yeah, I think it is. Because, yeah, because, because fundamentally, this trying to maintain

423
00:41:34,120 --> 00:41:40,040
this distinction between quote unquote, natural things and the product of those natural things,

424
00:41:40,040 --> 00:41:44,760
meaning our synthetic, you know, engineered things. There's, I just don't think it's,

425
00:41:44,760 --> 00:41:49,960
it's valuable at all. I think it holds back a lot of progress. And in your defense, because I know

426
00:41:49,960 --> 00:41:56,680
a lot of people assume that when you talk about man as a machine or when you, when you talk about

427
00:41:56,680 --> 00:42:04,040
these, these concepts, they're useful in different contexts. And so you often talk about, we spoke

428
00:42:04,040 --> 00:42:07,960
the last time and you mentioned the fact that an orthopedic surgeon has to see you as a, as a

429
00:42:07,960 --> 00:42:11,880
machine. I mean, there's no doubt about it. When I'm in theater with assisting with an orthopedic

430
00:42:11,880 --> 00:42:18,920
surgeon, I know what it's like. It's legitimately a mechanic. Like literally taking about drilling

431
00:42:18,920 --> 00:42:23,720
holes, getting a hammer, knocking onto things. And then, and then I can go back into like,

432
00:42:23,720 --> 00:42:28,040
let's say, clinical scenario and chat to the patient about the operation. And that's a completely

433
00:42:28,040 --> 00:42:33,320
different experience. It's a mental well being check. It's a sort of psychological checking.

434
00:42:33,320 --> 00:42:38,200
It's very, very different. So it's easier to see how we can see them both as a machine and as a

435
00:42:38,200 --> 00:42:45,720
complex psychological system. I mean, the, the, I think what, I think where, where people go wrong

436
00:42:45,720 --> 00:42:53,960
sometimes is to think that when we make these models, machines, you know, living beings, humans,

437
00:42:53,960 --> 00:42:59,080
whatever, that these are all claims about what something essentially is. It's this kind of

438
00:42:59,080 --> 00:43:02,760
essentialism that we think it's a real thing. There is one objective answer as to what it

439
00:43:02,760 --> 00:43:08,520
really is. And we need to argue about what it really is. I don't think any of these things

440
00:43:08,520 --> 00:43:13,720
are about what the thing really is. I think these are all interaction claims. This is why,

441
00:43:13,720 --> 00:43:18,840
this is why I called my, my, my freeing word tame because, because it is an engineering

442
00:43:18,840 --> 00:43:23,240
perspective. Now, engineering means something wider than I think most people take it. But

443
00:43:23,240 --> 00:43:28,360
nevertheless, the thing about engineering is that you are at least clear, you're honest with yourself

444
00:43:28,360 --> 00:43:33,000
in that what you are doing is putting out an interaction protocol. This is the frame that

445
00:43:33,000 --> 00:43:37,160
I'm going to look at at the system. This is what it enables me to do. Here's a bag of tools that

446
00:43:37,160 --> 00:43:42,440
I bring to it. And then you can bring yours, I can bring mine, and we can compare the results.

447
00:43:42,440 --> 00:43:46,280
And we can find out that, oh, wow, I, I missed all of it. You, you, you had a better framing

448
00:43:46,280 --> 00:43:49,640
because look, you were able to do all these things that I couldn't do because I was looking at it

449
00:43:49,640 --> 00:43:54,120
from a different perspective. So when people, you know, when, when, when, so, so, so like people

450
00:43:54,120 --> 00:43:57,640
ask, you know, am I a computationalist, for example, with respect to living things?

451
00:43:59,000 --> 00:44:04,840
Well, I think the whole, the question is ill-posed because it's not whether living things are

452
00:44:04,840 --> 00:44:11,080
Turing machines or nothing is anything. I think that what, what you can say is, okay, I've got a

453
00:44:11,080 --> 00:44:16,520
certain paradigm, let's say it's a Turing machine or it's a, you know, whatever it is. And that lens

454
00:44:16,520 --> 00:44:20,920
enables me to see certain things. And yes, I do think in some cases it's a, it's a, it's a useful

455
00:44:20,920 --> 00:44:25,640
lens, but it certainly doesn't capture everything that's important about living things. And so then,

456
00:44:25,640 --> 00:44:29,480
then you need a, or cognitive things more, which I think are more interesting, then,

457
00:44:29,480 --> 00:44:33,240
then you need, then you need different, different lenses, but, but, but, but then it, then, then

458
00:44:33,240 --> 00:44:38,600
it's all good. You know, we don't have to argue about what it really is. You can just say, through,

459
00:44:38,600 --> 00:44:42,760
I look, I look at it through this lens, here's what I see. Do you find it useful, or do you want to

460
00:44:42,760 --> 00:44:50,040
keep looking for a new lens or, or usually both? I think that your work is so intriguing on so many

461
00:44:50,120 --> 00:44:57,480
different levels. And it's a, and you're able to, to cross so many different fields that to some

462
00:44:57,480 --> 00:45:02,520
people, particularly, I would say to some scientists, when you make this claim that there is a

463
00:45:02,520 --> 00:45:07,400
technological approach to mind everywhere, the moment you get boxed into this sort of panpsychist

464
00:45:07,400 --> 00:45:12,280
view, they immediately have this dismissive attitude. But if they're a reduction materialist

465
00:45:13,800 --> 00:45:19,320
person, which is sad really, because they don't really then give it the opportunity that it

466
00:45:19,320 --> 00:45:22,600
deserves when, because when you break it down and actually look into what you're talking about,

467
00:45:22,600 --> 00:45:28,120
you're often saying you got to, we don't know, you got to make experiments and, and get some

468
00:45:28,120 --> 00:45:33,160
sort of empirical evidence to base whatever you're claiming. And I think that's the most

469
00:45:33,160 --> 00:45:36,760
important thing is that you're often saying, let's set up an experiment, let's do this,

470
00:45:36,760 --> 00:45:42,280
let's try and show why this is the case, which a lot of people don't necessarily do, particularly

471
00:45:42,280 --> 00:45:48,280
philosophers who have very strict views on, on their, on their reality, you're able to at least

472
00:45:48,280 --> 00:45:52,680
show this empirically, which I think is pretty cool. Well, that's, I mean, yeah, so, so the wacky

473
00:45:52,680 --> 00:45:58,760
thing about our lab is that we do a lot of experiments. And this is, this is not only

474
00:45:58,760 --> 00:46:01,800
philosophy. And I don't, I don't know, I mean, I think philosophy is very important. I never,

475
00:46:01,800 --> 00:46:07,640
you know, I never downplay it, but, but, but, but, but we do a lot of experiments. And, you know,

476
00:46:07,640 --> 00:46:13,320
typically, I basically, I put out two kinds of papers. One is a, what the one kind is a straight

477
00:46:13,320 --> 00:46:18,600
up, you know, developmental biology or bioengineering or, you know, synthetic regeneration,

478
00:46:18,600 --> 00:46:22,200
whatever it's going to be. I don't talk about any of the philosophical stuff in those papers.

479
00:46:23,560 --> 00:46:29,000
But you kind of can't get away from that stuff, because what happens is sure, you can, you can

480
00:46:29,000 --> 00:46:32,600
dismiss the, the kind of the other papers, which are kind of these philosophical

481
00:46:33,400 --> 00:46:38,760
perspectives on this, but you still have to account for the data. And, you know, my, my point is

482
00:46:38,760 --> 00:46:41,880
simply this, because people say to me, like, I'll give a talk about something. And they say, okay,

483
00:46:41,960 --> 00:46:46,200
you know, the data means really interesting what, what, what you've done and the new capabilities.

484
00:46:46,200 --> 00:46:50,760
But, you know, I wish you'd stop talking about this philosophy stuff. And, and my claim is,

485
00:46:50,760 --> 00:46:56,360
well, this is why we did it, you see, is because, is because that philosophical outlook made specific

486
00:46:57,640 --> 00:47:01,960
predictions about roadmaps, you know, it's not a single experiment, but it, it shows you where

487
00:47:01,960 --> 00:47:06,280
to look, it shows you how to look, it tells you which categories can be broken and otherwise,

488
00:47:06,280 --> 00:47:10,840
otherwise you're trapped in certain ways of thinking. And that leads to very specific

489
00:47:11,400 --> 00:47:17,160
new discoveries. And so, so I think this is, this is important because after the fact,

490
00:47:17,160 --> 00:47:22,360
once you do something, anybody can look at it and tell a molecular story about what happened

491
00:47:22,360 --> 00:47:26,680
and say, oh, well, this is, this is not different from, from anything that's happened before it

492
00:47:26,680 --> 00:47:30,440
followed. I mean, of course, it follows the laws of physics and chemistry. My point is never that

493
00:47:30,440 --> 00:47:33,480
it's fairies underneath. And that's something, you know, something, you know, miraculous is

494
00:47:33,480 --> 00:47:38,280
happening. That's never the point. But my point is, why wasn't it done before? What is it about

495
00:47:38,280 --> 00:47:41,800
the, what is it about the standard paradigm that didn't, that didn't facilitate these,

496
00:47:41,800 --> 00:47:47,560
these experiments to be done before? So, yeah, you know, you can be, you can be down on the,

497
00:47:47,560 --> 00:47:52,920
on the conceptual frameworks that drive this stuff. But then you're playing catch up because,

498
00:47:52,920 --> 00:47:57,560
because then the stuff is going to be coming out. And it's, and it's surprising. And this is, you

499
00:47:57,560 --> 00:48:04,280
know, to me, this is, this is a more general issue of, of, of these kind of reductive explanations.

500
00:48:04,280 --> 00:48:08,600
And by the way, I don't, I don't think anybody's really in this field is an actual reductionist,

501
00:48:08,600 --> 00:48:12,920
because if they, you know, if you, if you push and you say, well, then you want explanations in

502
00:48:12,920 --> 00:48:16,040
terms of quantum foam, right? And they say, no, no, that's stupid. It's chemistry. It's got to

503
00:48:16,040 --> 00:48:20,200
be chemistry. So that's not real reductionism. That's just the level of chemistry. But, but

504
00:48:21,160 --> 00:48:27,640
the, you can, the easy sort of analogy to this is if there was a, it was a game of chess played,

505
00:48:27,640 --> 00:48:32,680
right? So, you know, a couple of people played a game of chess. You could, so Laplace's demon

506
00:48:32,680 --> 00:48:37,800
could look at this and say, well, I mean, all it was is a bunch more, is a bunch of physics. I mean,

507
00:48:37,800 --> 00:48:41,640
I saw all the, all the protons went where the protons go, the electrons went where they go,

508
00:48:41,640 --> 00:48:45,480
everything followed rules. There's no mystery here, no surprise. Everything did exactly what it

509
00:48:45,480 --> 00:48:50,520
was supposed to be. It's just, you know, it's just physics. That's not exactly wrong, because

510
00:48:50,520 --> 00:48:55,480
looking backwards after the thing was done, you could tell that story. But now the question is,

511
00:48:55,480 --> 00:48:59,800
does it help you play the next game of chess? How do you, how do you go from there to, well,

512
00:48:59,800 --> 00:49:03,960
now what do I do? And so, and it's completely useless for that, right? It's, it's only a story

513
00:49:03,960 --> 00:49:09,640
looking backwards. So I think the thing about these kind of explanations is that you want them

514
00:49:09,640 --> 00:49:14,520
to facilitate your next, the next discoveries, the way to, you know, to, to improve your

515
00:49:14,520 --> 00:49:19,000
capabilities. Anybody could always tell a molecular story after the fact. The question is,

516
00:49:19,000 --> 00:49:24,440
what are you going to do next? Yeah, I mean, on that, on the topic of reductionism, you, at

517
00:49:24,440 --> 00:49:32,120
some point you say, in an important sense, you are a brain in a vat. However, we are not just

518
00:49:32,120 --> 00:49:40,920
chemical machines. So you're both acknowledging the fact that we're, we're these mechanical

519
00:49:40,920 --> 00:49:45,560
systems. And yet you're also still saying that that's still, you're not reducing us to these

520
00:49:45,560 --> 00:49:51,480
simple properties. You're still acknowledging the fact that they are more layered realities

521
00:49:51,480 --> 00:49:59,480
here to explore, in a sense. Yeah, I mean, in the, in the kind of the simple thing is that,

522
00:49:59,480 --> 00:50:05,960
yeah, absolutely. My claim is that interactions with certain kinds of systems are much more

523
00:50:05,960 --> 00:50:10,440
efficient at high levels, right? So, so, so certainly, I mean, we know this from anybody

524
00:50:10,440 --> 00:50:14,120
who's trained animals, instead of trying to run their neurons like a puppet knows that

525
00:50:14,120 --> 00:50:18,040
it's, it's much better if you understand the psychology of, of certain creatures and so on.

526
00:50:18,760 --> 00:50:22,360
And, and as you go rightward on that spectrum into, into friendship and love and these kinds

527
00:50:22,360 --> 00:50:26,920
of things that are much more bidirectional, it's not just control and prediction, but it's actually,

528
00:50:26,920 --> 00:50:31,480
you know, being vulnerable to, to change and benefiting from the agency of the beings that

529
00:50:31,480 --> 00:50:36,600
you're, that you're associating with and so on. Yeah, then, then of course there are these higher

530
00:50:36,600 --> 00:50:41,480
levels, but, but I want to say something else here too, which is that I think one thing this is all

531
00:50:41,480 --> 00:50:47,240
telling us is that, and I think this, this will be more and more apparent in the coming years,

532
00:50:47,240 --> 00:50:53,240
we have really misunderstood what simple machines are. We've, we've fallen in love with our,

533
00:50:53,800 --> 00:51:00,840
we've, we've confused physical things with models of simple machines that we make of

534
00:51:00,840 --> 00:51:06,440
those things and we think, we think we know what it is when we make something and I think

535
00:51:06,440 --> 00:51:12,680
that's profoundly wrong. What, what we have is our model and, and we've, we've done some work

536
00:51:12,680 --> 00:51:19,080
now and we're going to do lots more on finding surprising protocognitive properties and very

537
00:51:19,080 --> 00:51:23,400
simple systems that are not obvious at all. And I don't mean emergent complexity. Emergent

538
00:51:23,400 --> 00:51:27,320
complexity and unpredictability is trivial. It's easy. You can, you know, any cellular

539
00:51:27,320 --> 00:51:30,680
automata, whatever they will give you complexity that will give you unpredictability in certain

540
00:51:30,680 --> 00:51:35,640
cases. That, that, that part's easy. I'm talking about emergent goals, emergent cognition in systems

541
00:51:35,640 --> 00:51:42,280
that are extremely simple and minimal. And you know, when we say we are not machines, we are

542
00:51:42,280 --> 00:51:48,200
certainly not describable by the simple models we've made of machines, but I actually think that

543
00:51:48,200 --> 00:51:53,800
lots of simple physical objects are, are very, are not properly described that way either. And,

544
00:51:53,800 --> 00:51:58,200
and we need to, we need to remember that all of these things are just lenses that we bring to them.

545
00:51:58,200 --> 00:52:04,920
You know, yeah, you know, I heard, I was talking to somebody once who, he writes these, these,

546
00:52:04,920 --> 00:52:09,160
these language models and he said, well, I made it. I wrote it myself. I know exactly what it

547
00:52:09,160 --> 00:52:13,400
does. I know, I know everything that's in it. I made it. And as I said, I said, you don't even

548
00:52:13,400 --> 00:52:18,440
even know what bubble sort does. We're like, we found, we found, we found these, these unexpected

549
00:52:18,440 --> 00:52:23,480
capacities in, in, in stupid bubble sorts, you know, six lines of code, fully deterministic,

550
00:52:23,480 --> 00:52:28,840
nowhere to hide. Even that thing does things. Nobody knew that it did. And, and, and if that's

551
00:52:28,840 --> 00:52:32,680
the case, when you make this, this, this crazy, the, you know, language model, again, I'm not

552
00:52:32,680 --> 00:52:36,760
in love with language models, but, but, but just the, just this idea that we've made it and therefore

553
00:52:36,760 --> 00:52:42,360
we know what it does. I think is, is we really need a lot more humility around this. I, I, you

554
00:52:42,360 --> 00:52:46,440
know, I think there's plenty of stuff that quote unquote, simple matter does that we do not understand

555
00:52:46,440 --> 00:52:52,280
yet. Mike, the last time I spoke to Mark, Psalms, you asked me to ask him about what is the meaning

556
00:52:52,280 --> 00:52:57,800
of life. And then we started this whole road towards the end of that conversation. He,

557
00:52:57,800 --> 00:53:03,480
he spoke about the Oppenheimer foundation, given him funding, his, his new search for

558
00:53:03,480 --> 00:53:08,200
artificial intelligence, trying to sort of see where this goes. What are your thoughts on his

559
00:53:08,200 --> 00:53:15,480
work and what they're trying to do at this point? Yeah. I mean, he, as far as I know, none of it

560
00:53:15,480 --> 00:53:19,160
is published, although I've talked to him about it quite, quite a lot. So I'm not, I'm not going

561
00:53:19,160 --> 00:53:23,960
to kind of give away anything until, until he publishes it. But because it's not, it's not

562
00:53:23,960 --> 00:53:31,960
my story to tell. But I think of anybody I know at the moment, his approach is the most likely

563
00:53:32,040 --> 00:53:39,000
to give rise to something that actually captures what's important about cognition in, in living

564
00:53:39,000 --> 00:53:47,560
things. I think, I think if anybody is going to engineer something that, that exploits some of

565
00:53:47,560 --> 00:53:52,200
the same principles that, that life exploits for, for cognition, I think he's likely to do it.

566
00:53:52,200 --> 00:53:57,240
Yeah. Mark really sees that cortical fallacy that we all seem to have, you know, this obsession

567
00:53:57,960 --> 00:54:03,880
of this sort of higher cognitive functions as being this, the epitome of consciousness.

568
00:54:04,840 --> 00:54:10,520
Yeah. I mean, I think it's even, I think it's, it's way worse than that. It's not just where is it?

569
00:54:11,160 --> 00:54:15,240
You know, I mean, there are some people who think that it, that, that, that the consciousness,

570
00:54:15,240 --> 00:54:20,120
for example, shows up during warm-bloodedness, you know, I think that's Nick Humphrey's position.

571
00:54:20,120 --> 00:54:26,920
And it's not even, to me, it's not even the question of where in the brain or, or

572
00:54:26,920 --> 00:54:32,840
what kind of brain. I mean, I'm talking about what, what space do you even operate in? Because I,

573
00:54:32,840 --> 00:54:36,440
because I think that, that, you know, when they say, oh, this thing is not embodied, it doesn't,

574
00:54:36,440 --> 00:54:41,400
it's not a robot on wheels that can sort of run around, then you can have bodies and do this kind

575
00:54:41,400 --> 00:54:45,880
of perception action loop and all active inference and all this stuff. You can do this in other spaces

576
00:54:45,880 --> 00:54:50,280
that we are completely blind to, right? So you can live in transcriptional space or anatomical

577
00:54:50,280 --> 00:54:54,280
morphous space or who knows, there's probably a hundred others that we don't, you know, we, we

578
00:54:54,280 --> 00:54:58,840
don't know how to visualize and all of those are embodied. That's on us, that limitation that we

579
00:54:58,840 --> 00:55:03,080
don't see that and we don't see all the, all the goal-directedness, the striving, the intelligence,

580
00:55:03,080 --> 00:55:09,000
the problem-solving, that, that's our limitation, right? And so, so intelligence, not only in,

581
00:55:09,000 --> 00:55:14,600
in, you know, weird kinds of body parts, but just in things that are not in 3D space at all,

582
00:55:14,600 --> 00:55:21,240
really, that's not where their, their life plays out. And even, even worse than that,

583
00:55:21,720 --> 00:55:26,440
along the spectrum, even, even, you know, sort of weirder is along the spectrum of

584
00:55:27,640 --> 00:55:31,880
how quote unquote real something is. So what I mean by that is, I don't remember if it's

585
00:55:31,880 --> 00:55:35,880
actually in that paper or not, but you know, there was a science fiction story. If anybody

586
00:55:35,880 --> 00:55:40,680
listening to this knows what story it is, please email me because I couldn't remember like, which,

587
00:55:40,680 --> 00:55:45,720
which I'd like to give credit and I couldn't remember who it is. But the idea is that these,

588
00:55:45,720 --> 00:55:48,760
these creatures come out of the center of the earth, you know, they live, they live down in

589
00:55:48,760 --> 00:55:51,080
the core and they come out of the center of the earth and they're walking around.

590
00:55:51,800 --> 00:55:57,880
Everything that we see out here is gas to them. I mean, they are so dense that all of this stuff

591
00:55:57,880 --> 00:56:02,920
here that feels solid to us is gaseous phase. It's plasma. Like, it's like, I don't even see it.

592
00:56:02,920 --> 00:56:06,360
And they're walking around as far as they're concerned. They're like, and basically in,

593
00:56:06,360 --> 00:56:09,880
in, in, in space at this point, because like, oh my God, there's like, there's nothing here.

594
00:56:10,440 --> 00:56:14,840
And, and I, I'm sure I'm embellishing this in my own way. I don't remember what the,

595
00:56:14,840 --> 00:56:18,200
what the actual story is, but that's only the first part that I recall.

596
00:56:18,200 --> 00:56:22,360
But, but to me, what I envision immediately is like one of them as a scientist,

597
00:56:22,360 --> 00:56:26,680
and he's taking measurements of this, of this gas that's on, on the surface of the planet.

598
00:56:26,680 --> 00:56:31,400
And he says, you know, I see some, I see these patterns in this gas, they sort of

599
00:56:31,400 --> 00:56:35,720
hang together for a while and they do things. And it almost looks, they almost look agential.

600
00:56:35,720 --> 00:56:38,840
You know, these patterns almost look like they're doing things. And of course, the others are like,

601
00:56:38,840 --> 00:56:42,360
oh, you're crazy patterns and gas can't do anything. Patterns aren't real. We're, you know,

602
00:56:42,360 --> 00:56:47,320
we're real. How's a pattern and gas going to do anything? And he says, no, I really, I think

603
00:56:47,320 --> 00:56:51,000
they're like, they're trying to, you know, meet certain goals and they have memories and where,

604
00:56:51,000 --> 00:56:54,840
but, and they say, well, how long do these patterns hang around? He says, well, about 100

605
00:56:54,840 --> 00:56:58,760
years, that's ridiculous. Nothing important can happen in 100 years, you know, because these

606
00:56:58,760 --> 00:57:03,400
things live for, you know, millions of years. And so, and so this, this just reminds you that

607
00:57:05,800 --> 00:57:10,680
what's a pattern and, and what's a real being. So, so again, back to this distinction between

608
00:57:10,680 --> 00:57:15,720
thoughts and thinkers is in the eye of a beholder, you know, it's in the eye of the observer. And

609
00:57:15,800 --> 00:57:20,520
if we did have aliens that came to earth with a radically different cognitive frame rate,

610
00:57:20,520 --> 00:57:24,600
if they had different lifespans, whatever, would they think that talking to us is a good idea?

611
00:57:24,600 --> 00:57:29,160
Or would they be trying to talk to ecosystems? Or conversely, would they think that talking to the,

612
00:57:29,160 --> 00:57:33,240
you know, molecular processes is the best that they're going to be able to do? I think, I think

613
00:57:33,240 --> 00:57:39,560
all of this is really about observers and about getting good at recognizing intelligence and

614
00:57:39,560 --> 00:57:47,240
extremely unfamiliar guises, you know? I think that the it's, it's inescapable, your work particularly

615
00:57:47,240 --> 00:57:52,600
to, to not cross philosophical slash ethical boundaries and have these discussions. So,

616
00:57:52,600 --> 00:57:57,000
so when people tell you that listen, stay away from the philosophical stuff, it's you cannot,

617
00:57:57,000 --> 00:58:02,440
you just, this is just not part of your job. You have to at some point address these because

618
00:58:02,440 --> 00:58:06,040
I remember one of the comments in one of our discussions, it could have been our first one

619
00:58:06,040 --> 00:58:11,080
or second one. But then someone asked, is Michael playing God? And my first thought was,

620
00:58:12,920 --> 00:58:17,560
it's a strange one. You know, it was one of those questions people often asked back in the day when

621
00:58:17,560 --> 00:58:21,960
people were tinkering with any sort of even a plant, you could genetically modify an organism,

622
00:58:21,960 --> 00:58:26,360
and then you're playing God at that point. I mean, it's a very, very strange question to

623
00:58:26,360 --> 00:58:31,320
really ask. How would you respond to that? People actually ask it all the time. I think

624
00:58:31,320 --> 00:58:34,920
it's one of those questions that sounds like it makes sense until you, until you sort of dig

625
00:58:34,920 --> 00:58:40,680
into it a little bit. But because, because I don't know what, what the definition of God is.

626
00:58:40,680 --> 00:58:43,880
And I mean, usually the people who ask this, they got some glasses on, and they've got,

627
00:58:43,880 --> 00:58:47,880
you know, they usually drive, they don't walk places and so on. So it's a little, it's a little

628
00:58:47,880 --> 00:58:53,880
disingenuous. But, but, but let's, let's dig into this for a moment. I did a poll once on Twitter,

629
00:58:53,880 --> 00:58:59,400
and certainly this is not like a, you know, a statistically valid sample or anything like that.

630
00:58:59,400 --> 00:59:04,840
But I did a poll and the, and my question was simply this. So you're, you're, you know,

631
00:59:04,840 --> 00:59:10,840
AUG the caveman, and you're walking back to your, to your tribe, and you have this vision,

632
00:59:10,840 --> 00:59:17,160
you're struck with this vision of discovering fire. And so immediately, you get, you understand fire,

633
00:59:17,160 --> 00:59:26,280
but you also get this vision of steel weapons, artificial hearts, antibiotics, going to the

634
00:59:26,280 --> 00:59:32,920
moon, atom bomb, computers are like all of it, right? Immediately. So now the question is,

635
00:59:32,920 --> 00:59:36,520
so now your question is, so you've seen all this, right? You see, you see where it's going to go.

636
00:59:36,520 --> 00:59:42,120
Your question is, do you tell the others and you get going with fire or, or, or, or do you let it,

637
00:59:42,120 --> 00:59:47,640
do you let it die? And you never tell. Okay. 6% of my, my audience, and that's, and that's the

638
00:59:47,640 --> 00:59:51,320
people who like my stuff. So that means they're already probably like really biased towards,

639
00:59:51,320 --> 00:59:57,320
you know, techies stuff. 6% thought you shouldn't, you shouldn't let, you should stay below fire.

640
00:59:57,320 --> 01:00:03,320
So, okay, I don't know, you know, I don't know if, if these folks live a lifestyle consistent

641
01:00:03,320 --> 01:00:10,040
with that belief. I tend to doubt it. But, but, you know, it's, if, if that's the claim,

642
01:00:11,000 --> 01:00:16,200
I think you have to take this series. I think you have to say, if, if you really mean by playing

643
01:00:16,200 --> 01:00:20,760
God, I mean, what could it possibly mean? If you really mean taking steps that are

644
01:00:22,040 --> 01:00:27,000
strongly efficacious in the world and that make change, that do things, if you really don't want

645
01:00:27,000 --> 01:00:34,680
to do that, your quarrel is not with me and my work on frog skin. Your quarrel is with all of

646
01:00:34,680 --> 01:00:39,560
humanity who doesn't want to sit in a damp cave their whole life and die in exactly the same

647
01:00:39,560 --> 01:00:44,600
condition that they were born in. That's if, if you're really against that, okay, make your case

648
01:00:44,600 --> 01:00:50,840
and, and see, you know, and see, see if people will go. But none of the things that, that, that

649
01:00:50,840 --> 01:00:58,120
we're doing are any different from the fundamental question. Are you going to take responsibility

650
01:00:58,120 --> 01:01:05,080
for the future? And I think that is the most profound moral cowardice to delude yourself into

651
01:01:05,080 --> 01:01:09,960
thinking that doing nothing is staying out of it. No, doing nothing is not staying out of it.

652
01:01:09,960 --> 01:01:15,720
Doing nothing means you are complicit in the suffering of enormous numbers of humans and

653
01:01:15,720 --> 01:01:21,880
others on earth who are having a, an incredibly sub, a suboptimal experience in their, in their

654
01:01:21,880 --> 01:01:26,680
embodiment. And if, and if you have these kinds of thoughts about, let's not do this and let's not

655
01:01:26,680 --> 01:01:33,080
do that, you know, you know, let's put a break on progress, you are making a very clear statement.

656
01:01:33,080 --> 01:01:37,080
And, and you should think about it hard to make sure that you, you are, you know, you're, you're

657
01:01:37,080 --> 01:01:42,600
really backing off this idea that you are going to stay, you're going to let the status quo roll on

658
01:01:42,600 --> 01:01:47,160
because I, you know, it's just, to me, it's an incredible act of moral cowardice.

659
01:01:47,160 --> 01:01:53,080
Yeah. And I think for anyone who wants to even get a glimpse of what your ethical framework around

660
01:01:53,080 --> 01:01:57,880
all of this eventually becomes, this paper is perfect. I mean, this paper on AI, at some point,

661
01:01:57,880 --> 01:02:04,280
you go a path forward through the ethics filter for civilization. And this fundamental premise

662
01:02:04,280 --> 01:02:10,360
for you is, is to mature, to realize, okay, our kids supplant us, everything does change,

663
01:02:10,360 --> 01:02:16,040
we continuously change. It's how we're going to move forward. And, and how are we going to, to, to

664
01:02:16,040 --> 01:02:24,600
act in a certain way that progresses us in a, in a safer, more kind, more loving environment. And,

665
01:02:24,600 --> 01:02:29,000
and this, and the, the outward people, scientists don't like to use it. But I mean, at that point,

666
01:02:29,080 --> 01:02:34,280
you're looking towards this sort of kinder process where we were able to give artificial

667
01:02:34,280 --> 01:02:39,960
intelligence these properties, because it is something that our cognitive light can't appreciate.

668
01:02:39,960 --> 01:02:44,040
And we know that this is something we genuinely enjoy. So let's try and propagate this.

669
01:02:44,840 --> 01:02:50,520
Yeah. Yeah. And, and, you know, I have some collaborators. So, so Richard Watson and Thomas

670
01:02:50,520 --> 01:02:57,560
Doctor and Olaf Witkowski and, you know, people like Bill Dwayne and Eliza Salamanova, you know,

671
01:02:57,560 --> 01:03:02,360
we, we, we write on stuff like this, and there's going to be, there's going to be way more because

672
01:03:02,360 --> 01:03:06,600
in, in certain traditions, right? So for example, they come from a Buddhist tradition. And so,

673
01:03:06,600 --> 01:03:14,440
and so there, there's a great emphasis on enhancing compassion alongside enhancing wisdom, right,

674
01:03:14,440 --> 01:03:19,800
on a basically an infinite sea of other beings and all sorts of crazy embodiments. I mean,

675
01:03:20,280 --> 01:03:26,200
I gave a talk on all this stuff to, to some, to some Buddhist scholars in Nepal, you know,

676
01:03:26,200 --> 01:03:33,160
at some point. And I mean, that audience, there was nothing here that surprised them whatsoever.

677
01:03:33,160 --> 01:03:36,680
You know, usually when I give these talks, people are kind of, kind of shocked and disturbed about

678
01:03:36,680 --> 01:03:43,160
about half of what I say. These guys were like, yeah, no kidding, we all know that. And they

679
01:03:43,160 --> 01:03:49,240
found nothing, nothing weird about any of it. And I do think they have, they have frameworks for

680
01:03:49,320 --> 01:03:53,240
thinking about these, these, these kinds of things, right, you know, this kind of expanding,

681
01:03:53,240 --> 01:03:57,800
committing to through, through concepts like the bodhisattva vow and through expanding,

682
01:03:57,800 --> 01:04:03,080
committing to the task of this, this metacognitive task of expanding your, your cone of compassion

683
01:04:03,080 --> 01:04:07,880
and things like that. Yeah, I think, I mean, I'm certainly not saying that's the only way to go,

684
01:04:07,880 --> 01:04:13,160
but, but I think that's exactly where this is going. I agree with you. I mean, because my,

685
01:04:13,160 --> 01:04:19,560
even though I'm of Indian heritage and descent, but my, when I talk about science, philosophy,

686
01:04:19,560 --> 01:04:25,640
Western, particularly, to my family, to like them, certain uncles or aunts, and if I talk about these

687
01:04:25,640 --> 01:04:30,840
topics, they also tend to do that. They, they're not as surprised as, as the more my more Western

688
01:04:30,840 --> 01:04:34,760
side of the family. A lot of the Eastern philosophers and my uncles and aunts, they're not

689
01:04:34,760 --> 01:04:38,120
really philosophers, but they tend to think like, Oh yeah, that makes sense. That is kind of what

690
01:04:38,120 --> 01:04:43,640
their religion taught them, whether it's Hinduism or Buddhism, but there is this element of minds

691
01:04:43,640 --> 01:04:49,240
are everywhere in a way. So this, this, this general binary approach that we seem to have

692
01:04:49,240 --> 01:04:54,280
is not working for the most part. And you're showing this in very, very Western scientific ways.

693
01:04:55,080 --> 01:04:58,520
Well, I think, I think, I mean, that's the other thing, right? So, so I don't really believe,

694
01:04:58,520 --> 01:05:04,440
okay, there's, there's another perspective where sometimes people say, look, early indigenous

695
01:05:04,440 --> 01:05:08,680
societies knew all this, all we have to do is go back, go back there. I don't actually believe

696
01:05:08,680 --> 01:05:14,360
that either. I don't think they actually knew this. And right. And saying something is not the

697
01:05:14,360 --> 01:05:19,800
same thing as having a principal framework that takes you to new discovery. So, so it's, it's,

698
01:05:19,800 --> 01:05:26,280
I think both sides, this idea of there's no mind everywhere, anywhere except in us, or maybe some

699
01:05:26,280 --> 01:05:30,600
people think that just isn't anywhere. But, but the other side of it, which is, oh, there's a spirit

700
01:05:30,600 --> 01:05:36,360
under every rock. Like that's, that's a fine start, but it's just a start. You can't just say it and

701
01:05:36,360 --> 01:05:40,920
leave it at that. You have to answer the question, what does that do for you? So I think this is

702
01:05:40,920 --> 01:05:47,240
really important. All, all of this has to be empirically useful. It has to elevate our condition

703
01:05:47,240 --> 01:05:51,960
and has to improve our ability to, to have more meaningful lives in the world. It has to be

704
01:05:51,960 --> 01:05:56,920
practical. You cannot just say these things and have it mean anything until unless it leads you

705
01:05:56,920 --> 01:06:01,800
to experiments and ultimately to, to, you know, the better ways of being in the world.

706
01:06:02,360 --> 01:06:08,280
So I don't think we're going backwards to those traditions at all. I think we're using whatever

707
01:06:08,280 --> 01:06:12,200
we can scavenge out of all the, you know, brilliant people that have existed in the past that had

708
01:06:12,200 --> 01:06:17,960
sort of glimpses of this stuff. But, but now I think we finally have the ability to push it forward

709
01:06:17,960 --> 01:06:22,200
in a very practical way. So that some of these ideas we can, we can discard what isn't useful.

710
01:06:22,200 --> 01:06:26,280
We can, we can keep and expand what actually helps us to get to new capabilities.

711
01:06:27,080 --> 01:06:30,440
And I think that, and that's part of the approach that I appreciate most. I mean,

712
01:06:30,440 --> 01:06:33,640
I find it particularly annoying when people do that, what you're talking about, where

713
01:06:33,640 --> 01:06:38,920
these gurus come out and just say these things with no basis, absolutely no evidence of what

714
01:06:38,920 --> 01:06:44,680
there's no claim, but it's just so profound in itself. That's the statement itself is all that

715
01:06:44,680 --> 01:06:49,240
they have, which, which isn't what, what, what you're trying to do. You're often saying, you

716
01:06:49,240 --> 01:06:53,800
got to show, you got to do something, back it up somehow. I mean, I mean, these, these claims

717
01:06:53,800 --> 01:06:59,480
and these profound statements and, you know, and poems and whatever else, they're, they're a fine

718
01:06:59,480 --> 01:07:06,280
tool for spurring intuition and for giving you ideas that it's the starting point, right? And,

719
01:07:06,280 --> 01:07:12,760
and, and I, and I do think that it's true that it's possible to have intuitions about things and,

720
01:07:12,760 --> 01:07:19,800
and to come up with prompts, you know, sayings and writings and whatnot that trigger other

721
01:07:19,800 --> 01:07:24,360
people into new and interesting thoughts, even though you haven't yet worked out all the details.

722
01:07:24,360 --> 01:07:28,600
I mean, I do think that's possible. I do think it's, you know, we are kind of like, so I have this,

723
01:07:28,600 --> 01:07:35,080
like, almost, almost like, like the way, um, platonist mathematicians, you know, they feel

724
01:07:35,080 --> 01:07:39,640
that they're discovering an existing structure, right, of that, that you're uncovering an existing

725
01:07:39,640 --> 01:07:43,640
structure and that, you know, you see, you know, sort of piece by piece, pulling it out.

726
01:07:44,440 --> 01:07:50,920
I do think that it's possible to, to, to sort of have insights long before you have the wherewithal

727
01:07:50,920 --> 01:07:55,800
to really make it practical or to know what it means or any of that. And so, and so I like that

728
01:07:55,800 --> 01:08:01,160
stuff as much as anybody in terms of an intuition, you know, building kind of thing to see what it

729
01:08:01,160 --> 01:08:04,680
makes you think about, like the, like the quote from William James, right? I don't know. And I

730
01:08:04,680 --> 01:08:08,760
lose no sleep over whether he actually meant that the way that I mean it. I don't care. I think,

731
01:08:08,840 --> 01:08:13,480
I think it's a very profound saying. And what can we do with it now? But, um, you know, the hard

732
01:08:13,480 --> 01:08:18,840
work comes, comes after all that. Someone, it reminds me of, and I mean, we, we lost him recently,

733
01:08:18,840 --> 01:08:26,120
Daniel Dennett, raised him, he, he, what you do is almost the reverse, but in, in, in the same,

734
01:08:27,320 --> 01:08:34,680
I would say, in the same great manner is that what, what Dan did was he realized that you

735
01:08:34,680 --> 01:08:39,160
can't just philosophize. You, you have to go and you have to get involved with the cognitive

736
01:08:39,160 --> 01:08:44,040
science. You've got, you've got to get, you've got to basically do some of the work. And, and,

737
01:08:44,040 --> 01:08:47,960
and that's when the philosophy becomes a lot more intriguing is when you do the science and you go

738
01:08:47,960 --> 01:08:53,240
into it and you fuse them and you're coming from it from the science side. And then, of course,

739
01:08:53,240 --> 01:08:57,560
you then have to have the philosophical discussions with it. And, and you guys have worked very

740
01:08:57,640 --> 01:09:00,600
closely together. What is that like for you just as a side?

741
01:09:02,680 --> 01:09:09,960
Boy, I mean, first things first, you know, I, I read Dan's books when I was a kid. And it never,

742
01:09:09,960 --> 01:09:14,760
I mean, they were so eye-opening, you know, the mind's eye and kinds of minds and that kind of

743
01:09:14,760 --> 01:09:20,360
stuff, right? The early kind of the early work in the late 80s, early 90s. I was, I was young

744
01:09:20,360 --> 01:09:25,720
back then. And I would, I couldn't have imagined for a moment, a that I would, that I would get to

745
01:09:25,720 --> 01:09:29,320
meet him, never mind that, but be that at some point, you know, at some point, we'd write a paper

746
01:09:29,320 --> 01:09:34,040
together, right? Like, I wish I could get into a time machine and go back and, you know, tell my

747
01:09:34,040 --> 01:09:37,400
18-year-old self that, hey, you know, you're going to write a, write a paper with this guy and

748
01:09:37,400 --> 01:09:42,840
actually a bunch of other people to that, that I felt the same way about. So, so, so that part was

749
01:09:42,840 --> 01:09:48,040
a profound kind of honor for me is to, is to be able to talk to him about these stuff. And, and

750
01:09:48,040 --> 01:09:52,360
by the way, we didn't agree on everything. We, we disagree on a ton of stuff, but

751
01:09:53,240 --> 01:10:01,960
he was, he was an incredibly generous, clear thinker. And what I really enjoyed about him was

752
01:10:01,960 --> 01:10:08,840
that was, was a few things. One of them was that he was never interested in, in making cheap points.

753
01:10:08,840 --> 01:10:13,240
He was always interested in improving everybody's understanding of what's going on, deepening the

754
01:10:13,240 --> 01:10:18,360
question. It may be the answer, but for sure, deepening the question. And this idea, you know,

755
01:10:18,360 --> 01:10:23,080
he really pushed this idea of steelmaning. You know, he said that, that in arguing with people,

756
01:10:23,080 --> 01:10:29,160
what you ought to do is first state their position so well and so strongly that they will wish they

757
01:10:29,160 --> 01:10:33,560
came up with it, right? That you should start not, not with a caricature of what they think that

758
01:10:33,560 --> 01:10:37,640
you're going to shoot down, because that, that's a game, right? That's, that's, you know, what he

759
01:10:37,640 --> 01:10:43,960
wanted was actual progress, which meant you better start with the absolute best description of their

760
01:10:43,960 --> 01:10:47,720
view, the most plausible sounding this, and then, then see if you can shoot it down after that,

761
01:10:47,720 --> 01:10:51,880
right? That was his, that was his, and he was always that way in all, in all of our discussions,

762
01:10:53,080 --> 01:10:56,520
you know, about stuff that we did agree on and lots of things that we didn't agree on.

763
01:10:56,520 --> 01:11:03,880
It was, it was always very clear that everybody in this discussion is there to, to learn something

764
01:11:03,880 --> 01:11:08,440
and to improve and to give up things that you thought before, if, if they're not helping you

765
01:11:08,440 --> 01:11:13,080
move forward and grab some, some other tool like that, that was, you know, he was an amazing

766
01:11:13,080 --> 01:11:16,600
example of that. And I mean, at first, well, first I took a course with him as an undergraduate at

767
01:11:16,600 --> 01:11:21,880
Tufts, I had him for, for, for a, yeah, I had him for a, for a philosophy of mind professor,

768
01:11:21,880 --> 01:11:26,760
which was, which was amazing. I purposely wrote a paper, there was a, there was a final paper

769
01:11:26,760 --> 01:11:31,960
for the class that you, that you write, I picked a topic that I knew he did not like, and that I

770
01:11:31,960 --> 01:11:38,440
knew he, you know, was, was completely against. And, and I was, I was astounded at, you know, the,

771
01:11:38,440 --> 01:11:44,920
the fair, rigorous, but, but, but completely fair, you know, analysis and grade and everything

772
01:11:44,920 --> 01:11:49,640
else. That was an example for me that this is how you do it. This is, you know, it's not,

773
01:11:49,640 --> 01:11:55,480
it's not just based on what, you know, what you think, but like, you know, a deep analysis of,

774
01:11:55,480 --> 01:11:59,400
of the, the fairest analysis. And then, and then later when I came back to Tufts as a faculty

775
01:11:59,400 --> 01:12:03,000
member, you know, he was, he was a, he was a colleague and that was, that was incredible.

776
01:12:03,000 --> 01:12:07,000
So yeah, yeah, I'm really going to miss him. Yeah, no, he'll be dealing with, I mean, he was one

777
01:12:07,000 --> 01:12:11,160
of the, so him and Oliver Sacks were two of the people that inspired me to even start this podcast.

778
01:12:11,160 --> 01:12:16,040
So one of those, yes, I never got to have on, but we exchanged emails every now and then.

779
01:12:16,040 --> 01:12:20,680
And even doing that for me felt like such an honor. And I really wish I had the chance to

780
01:12:20,680 --> 01:12:23,720
chat to him. That's how I'm just so curious for all those people who did get to speak to him.

781
01:12:24,520 --> 01:12:26,600
What a, what a provision might have been to pick his brain.

782
01:12:27,240 --> 01:12:32,280
Yeah. Oh no, it was, and he was so, you know, he was so, so inspirational and so generous with

783
01:12:32,280 --> 01:12:37,880
his ideas. He would come to our lab from time to time. And I have, I have a picture of him on

784
01:12:37,960 --> 01:12:41,240
the blog with what during one of his visits, you know, and he's, and he was looking through

785
01:12:41,240 --> 01:12:44,120
the microscopes and he was looking at our two headed worms that we would have these,

786
01:12:44,120 --> 01:12:47,000
he would have these discussions with our lab people about, you know, what's that,

787
01:12:47,000 --> 01:12:50,520
what's it like to be a creature with two brains and what's the right way to think about these things?

788
01:12:50,520 --> 01:12:55,240
And, and, you know, and yeah, he would, you know, he would give talks just, just very generous,

789
01:12:55,240 --> 01:12:59,000
you know. Mike, you must, you must check, you must look out for this one of these videos online.

790
01:12:59,000 --> 01:13:05,480
It's a VPRO roundtable with Dan, Dan Dinnett, Oliver Sacks, Rupert Sheldrake,

791
01:13:07,320 --> 01:13:11,960
Steven, it's one of the most fascinating things. It's like six, I think,

792
01:13:13,080 --> 01:13:18,360
Dyson, Freeman Dyson was there as well. It's, it's such a strange thing. It's like the original

793
01:13:18,360 --> 01:13:24,360
version of podcasting, I would say. Just six of these guys is having the coolest chat on life,

794
01:13:24,360 --> 01:13:29,320
consciousness, reality. That was one of the things that got me into the two of their,

795
01:13:29,320 --> 01:13:34,520
both of their work and to this podcast. But anyway, before we, because we're digressing a bit,

796
01:13:34,520 --> 01:13:37,640
the path forward, this ethics, how are we doing for time? Mike, you all right?

797
01:13:38,920 --> 01:13:41,800
Yeah, I'm, yeah, okay. I got about, I got about 15 minutes.

798
01:13:41,800 --> 01:13:47,320
Good. Okay. For the, the path forward, the ethics falter, let's talk about this,

799
01:13:47,320 --> 01:13:51,320
because you said that there's, there's two ways we could get this wrong. One is object

800
01:13:51,320 --> 01:13:57,400
affiliate. And the other one is, well, only love your own kind. Let's talk about how we can get

801
01:13:57,400 --> 01:14:04,360
this wrong and how we can actually divert this and get this right. Yeah. Well, the, the, the,

802
01:14:04,360 --> 01:14:10,760
the spectrum itself is something like the, it's, it's, it's related to the effort of matching the

803
01:14:10,760 --> 01:14:19,640
degree of compassion that you are able to exert to the level of agency that there are intelligence

804
01:14:19,640 --> 01:14:25,000
or consciousness that, that that being actually has, right? Now, I'll point out that, that we, even

805
01:14:25,000 --> 01:14:29,160
when we get it right, we are still not very good at following through on the consequences. So,

806
01:14:29,160 --> 01:14:33,000
so for example, everybody understands that pigs are intelligent. Everybody understands that they,

807
01:14:33,000 --> 01:14:37,000
that they suffer, that they have minds, and we still have factory farming. It's, it's right. So,

808
01:14:37,000 --> 01:14:43,240
so even, you know, getting it scientifically right is absolutely not a guarantee of anything

809
01:14:43,240 --> 01:14:49,240
in terms of actual ethical behavior. But, but there's two ways to get it wrong. One way to get

810
01:14:49,240 --> 01:14:59,240
it wrong is to attribute more mind to a system than it really has. But also when I say really has,

811
01:14:59,240 --> 01:15:02,680
I, I, you know, I think everything is observer relative, of course, but, but still you, you

812
01:15:02,680 --> 01:15:06,120
could get, I mean, there's, you know, the internet is full of profiles of people that are in love

813
01:15:06,120 --> 01:15:11,960
with bridges and chandeliers and, and, you know, and things like this. So, so, so that's, that's

814
01:15:11,960 --> 01:15:16,440
something having too much, too much concern for things that really don't warrant it. And the

815
01:15:16,440 --> 01:15:21,800
other way is, of course, the opposite is when you've, you leave beings out of your, of your, of

816
01:15:21,800 --> 01:15:27,560
your calculus of compassion that actually can, can suffer and have an inner perspective. I mean,

817
01:15:27,560 --> 01:15:34,280
one, one thing to think about is if imagine two societies that get this wildly wrong in both directions.

818
01:15:34,280 --> 01:15:38,680
So you've got a planet where everybody's like, you know, ridiculously nice to, to, you know,

819
01:15:38,680 --> 01:15:43,000
they don't like to chop rocks in half and whatever. And, and, and then there's, and then

820
01:15:43,000 --> 01:15:49,080
there's the other society that thinks if, if you're not a very narrow type of creature, you are a

821
01:15:49,080 --> 01:15:53,720
machine the way that Descartes thought about lots of animals, and that we can do whatever we want,

822
01:15:53,720 --> 01:15:57,720
and it's fine. And you're just faking and all the, all your complaining about it is, is just,

823
01:15:57,720 --> 01:16:03,080
you know, it's just a word, where it's, it's, it's sentence completion, you know, is what it is. So,

824
01:16:03,800 --> 01:16:07,720
okay, so, so which of those worlds would you rather live in? Right? If you're going to get it,

825
01:16:07,720 --> 01:16:12,040
if you're going to get it wrong, where, where would you rather be? I mean, I think, I think the

826
01:16:12,040 --> 01:16:20,360
first one wastes a lot of resources and opportunities. Yeah, okay. The second one is, is, is monstrous in,

827
01:16:20,360 --> 01:16:26,920
in its ethical implications. So, so I, I think we should err on the side of more compassion,

828
01:16:26,920 --> 01:16:31,240
not less. I mean, obviously, again, we're not going back to there's a spirit under every rock,

829
01:16:31,240 --> 01:16:37,240
because we are committed to having principled theories about this. But if you're going to

830
01:16:37,240 --> 01:16:42,360
make a mistake, I think you should make a mistake in that direction. And specifically,

831
01:16:42,360 --> 01:16:47,080
what I'd like us to be clear on, I'd like what I'd like everybody to be clear on,

832
01:16:47,080 --> 01:16:53,000
is that having certainty about these things right now, when we have pretty much no clue

833
01:16:53,000 --> 01:16:59,080
what underlies consciousness, really, I mean, I know a lot of smart people have made efforts

834
01:16:59,080 --> 01:17:04,920
into it, but, but I really don't think we have it nailed down. And all of these ideas about

835
01:17:05,480 --> 01:17:11,960
what cognition is and how different architectures, you know, supported and, and whether cognitive

836
01:17:11,960 --> 01:17:16,200
consciousness and the ability to suffer tracks any of those things or not.

837
01:17:18,680 --> 01:17:23,080
There is, there's an enormous amount of unwarranted certainty about this among people,

838
01:17:23,080 --> 01:17:27,000
people feel very strong to make this really strong. That definitely doesn't whatever,

839
01:17:27,000 --> 01:17:32,760
you know, it doesn't have this or that. I think we all need to take a step back and just understand

840
01:17:33,160 --> 01:17:38,680
that from, from, from the scientific perspective, there are so many things we do not know yet,

841
01:17:38,680 --> 01:17:44,520
like really critical fundamental things. We do not understand the emergent cognitive properties

842
01:17:44,520 --> 01:17:50,360
of matter. We do not understand the scaling policies of how minds emerge from smaller minds.

843
01:17:51,320 --> 01:17:56,280
The field of diverse intelligence is just getting started. So I'm much more worried about the right

844
01:17:56,280 --> 01:18:00,920
side of that, of that spectrum than I am about the left side at this point.

845
01:18:01,800 --> 01:18:05,640
And I think what one of your towards the end of the paper, one of the things you says,

846
01:18:05,640 --> 01:18:10,440
the question is, how do we make sure to express kindness to the inevitable forthcoming wave

847
01:18:11,400 --> 01:18:17,320
of unconventional sentient beings? And you say that we should start by making sure that we express

848
01:18:17,320 --> 01:18:23,320
loving kindness appropriately and not be driven by fear of the other, which is, which is a very

849
01:18:23,320 --> 01:18:29,320
beautiful statement. Yeah, thanks. I've actually written a whole thing on fear just now. I'm

850
01:18:29,320 --> 01:18:34,600
waiting. It's going to be, it should be out in a couple of weeks. I think that, well, well,

851
01:18:34,600 --> 01:18:39,560
one thing I could say is after that, after that piece in Noeima, so there was the short piece

852
01:18:39,560 --> 01:18:43,960
in Noeima about the AI, there's a much, there's a longer paper which exists as a preprint and

853
01:18:43,960 --> 01:18:49,080
it's also in review right now in the journal. But I think more people saw the Noeima piece.

854
01:18:49,080 --> 01:18:56,200
But still, I was very clear there. I thought that I'm not actually saying that AI is that

855
01:18:56,200 --> 01:18:59,320
current language models are like humans. I mean, I thought I was pretty clear on this.

856
01:19:00,120 --> 01:19:09,240
But I got a lot of people writing to me that basically extremely disturbed by this and this

857
01:19:09,240 --> 01:19:21,160
idea that tech bros like myself are, I thought that was funny, that our nerdiness sort of prevents

858
01:19:21,160 --> 01:19:28,040
this from understanding real human relationships. And this is why we see these things in what they

859
01:19:28,040 --> 01:19:33,640
call machines, robots, AIs and whatever, right? They were looking for a, they were looking for

860
01:19:34,360 --> 01:19:36,200
why you do this in a sense.

861
01:19:37,960 --> 01:19:42,600
Correct. I mean, it's an old strategy, right? The old strategy is if you're uncomfortable with a view,

862
01:19:42,600 --> 01:19:47,480
try to find something wrong with, right? What is it that, you know, we see the truth. Why can't

863
01:19:47,480 --> 01:19:52,600
they see the truth? What is missing that causes them to say these things, right? And the standard

864
01:19:52,600 --> 01:19:58,760
theory is, well, they just don't understand, you know, these nerds don't understand what real human

865
01:19:58,840 --> 01:20:04,680
relationships are like, right? And that, I mean, I'm not super interested in

866
01:20:04,680 --> 01:20:10,520
in psychoanalyzing anybody that way. But it did, it did cause me to think, I'm like, wow,

867
01:20:10,520 --> 01:20:14,280
why are people so triggered by this? You know, what, what is it that caught, you know, to really,

868
01:20:14,280 --> 01:20:18,520
and so, and so I, so, so I'm thankful for once the pay, pay, you know, peace comes out, I'll

869
01:20:18,520 --> 01:20:22,840
thank some folks in, in who said these things and actually pushing me in what I thought was,

870
01:20:22,920 --> 01:20:29,560
think is an interesting direction is to ask, what, what is it? What is it that's so scary about,

871
01:20:29,560 --> 01:20:35,240
about this view? And the more I think about it, I really think it's a very fundamental fear.

872
01:20:35,240 --> 01:20:42,440
And the fear is, it's a zero sum game. Love is a zero sum game. If we have too many other

873
01:20:42,440 --> 01:20:47,640
beings that need love, then a couple of things will happen. There's not enough for me, that's A,

874
01:20:47,640 --> 01:20:53,160
and B, what if I can't rise to the, to the, to the challenge of having enough compassion

875
01:20:53,160 --> 01:20:58,440
for everybody? I think it's profoundly threatening to realize that you're going to have to open up

876
01:20:58,440 --> 01:21:05,720
your, your constrained way of looking at who deserves your compassion and what happens then.

877
01:21:05,720 --> 01:21:09,720
And, and then, and many other things. So, you know, so I wrote that on this probably five or

878
01:21:09,720 --> 01:21:15,320
10 pages or something about, you know, just kind of talking about what is really what I think

879
01:21:15,320 --> 01:21:20,040
really underlies why, why people are freaked out about this. And, and, and the responsibility,

880
01:21:20,040 --> 01:21:26,120
I mean, it's very comforting to think that I can just tell, you know, which things are worth worrying

881
01:21:26,120 --> 01:21:30,520
about by looking at them. I know what people look like. I'll just look at them. It's comforting to

882
01:21:30,520 --> 01:21:35,800
think that I don't need to be responsible for the future. This, this is it. This is, you know,

883
01:21:35,800 --> 01:21:40,360
this is how, this is what's natural, right? Even people who don't believe in, in some,

884
01:21:40,360 --> 01:21:44,280
some sort of God, all they, they still have this notion of what's natural. I have no idea what

885
01:21:44,280 --> 01:21:48,040
that's supposed to mean. But, but, but, you know, this is like, yeah, this is how we're supposed

886
01:21:48,040 --> 01:21:52,120
to stay. And that's fine. I don't, I don't need to be responsible for the future. And I don't need

887
01:21:52,120 --> 01:21:58,680
to be responsible for shaping what the planet looks like in the, you know, in, in the coming

888
01:21:58,680 --> 01:22:03,560
centuries and beyond. That's comforting to think that it's all handled. It's nice and simple.

889
01:22:04,120 --> 01:22:08,600
You don't need these, these extremely difficult nuanced views that are going to require work

890
01:22:08,600 --> 01:22:12,440
from you. They're going to require you to make hard decisions to paint a picture of

891
01:22:12,440 --> 01:22:17,240
the future of what do you want it to look like? You know, it's much easier to say what you don't

892
01:22:17,240 --> 01:22:22,200
want. This, this fear-based scarcity mentality, right? There's not enough love to go around.

893
01:22:22,200 --> 01:22:25,800
Let's, let's, let's draw a nice tight circle around things that we know what they look like

894
01:22:25,800 --> 01:22:28,680
and we know where they came from. Then we're not going to have to worry about all this other

895
01:22:28,680 --> 01:22:33,320
stuff that's really difficult to, to figure out what's, what's going on with it. And yeah,

896
01:22:33,320 --> 01:22:41,080
and then we don't need to worry about painting pictures of the, of the future and figuring

897
01:22:41,080 --> 01:22:45,000
out how to get there. We could just, we can just make a list of what we don't want to have happen

898
01:22:45,000 --> 01:22:49,320
and that's easy and, and, and focus on the negatives. So I think, I think that that type of,

899
01:22:49,320 --> 01:22:55,960
that type of limited fearful scarcity kind of mindset is, is what's, is what's responsible

900
01:22:55,960 --> 01:23:00,120
for a lot of this. And by the way, what I don't mean, so I, so I want to be clear here, I don't

901
01:23:00,120 --> 01:23:07,080
mean to, to try to deconstruct some of my colleagues that are really working on, on very good science,

902
01:23:07,080 --> 01:23:11,480
right? So, so there are people who are working on good science for developing principled

903
01:23:11,480 --> 01:23:16,520
ways to distinguish between so-called machines and what's special about living organisms,

904
01:23:16,520 --> 01:23:20,200
like that, that's a good area of diverse intelligence. I'm not, I'm not, you know,

905
01:23:20,200 --> 01:23:23,640
are saying that that shouldn't, you know, that, that shouldn't take place or, or that they're

906
01:23:23,640 --> 01:23:27,800
driven by anything other than, you know, good scientific principles. I'm talking about them,

907
01:23:27,800 --> 01:23:31,880
you know, I'm talking about the, the folks who have a really visceral reaction who

908
01:23:31,960 --> 01:23:39,320
when, when I, when I challenged them to, so, so, so, so be explicit. So, so tell me what,

909
01:23:39,320 --> 01:23:43,720
what is the magic that you have? And when did you get it? Both during evolution, during,

910
01:23:43,720 --> 01:23:47,320
during, you know, during embryogenesis, what, what, what, what do you have? And when does this

911
01:23:47,320 --> 01:23:52,840
show up that you think cannot be either, either in a hybrid form or in synthetic form, you know,

912
01:23:52,840 --> 01:23:56,840
done? And what would you do if, I mean, just, you know, I think reading science fiction is,

913
01:23:56,840 --> 01:24:01,400
is a great cure for this, because from the, from the earliest time, you understand the scenario,

914
01:24:01,400 --> 01:24:05,480
right? You're, you're, you're sitting there at home, this spaceship lands on your front lawn,

915
01:24:05,480 --> 01:24:09,000
this, this, the door opens, this thing sort of trundles out, it's kind of shiny looking,

916
01:24:09,000 --> 01:24:12,600
it's kind of metallic looking, but it sort of comes up to you and it sort of hands you this

917
01:24:12,600 --> 01:24:16,040
poem and it says, oh man, I'm so happy to meet you, you know, it's been, it's been,

918
01:24:16,040 --> 01:24:19,880
it's been, you know, a thousand years I was waiting to meet you, many of us died along the way, but,

919
01:24:19,880 --> 01:24:23,720
you know, but, but we persevered and we made this journey in here. I wrote you this poem and I'm

920
01:24:23,720 --> 01:24:27,160
looking to be friends and you sort of knock on and it's kind of metallic and you say,

921
01:24:28,040 --> 01:24:34,200
so, did you guys evolve naturally or did somebody make you? And he says, you mean,

922
01:24:34,200 --> 01:24:39,240
you mean, are we the result of totally random processes or was our mind crafted by, you know,

923
01:24:39,240 --> 01:24:42,920
some other mind? And he said, yeah, I'd really like to know and say, why, why do you want to know

924
01:24:42,920 --> 01:24:47,240
that? Like, well, just, you know, I'd really like to know because, and in the back of your mind,

925
01:24:47,240 --> 01:24:50,520
you're thinking, what, that, that, that if it's the, that if it's the latter, then, then you're

926
01:24:50,520 --> 01:24:53,800
okay with turning it into a vacuum cleaner, right? That's what you're really thinking about.

927
01:24:53,880 --> 01:25:00,520
And, and I mean, I, I, I find that just, just, you know, absurd. And we are all stuck in this

928
01:25:00,520 --> 01:25:06,120
position of saying, so what, what criteria are you going to use when you can't do this easy

929
01:25:06,120 --> 01:25:10,120
thing? That's why, that's why I think AI and language models are such an off ramp for these

930
01:25:10,120 --> 01:25:14,840
discussions, because it's just so easy to dunk on these language models, completely avoiding this

931
01:25:14,840 --> 01:25:19,720
issue of that embodiment can take a place in other spaces that you have absolutely no idea

932
01:25:19,800 --> 01:25:23,800
what, what, you know, physical systems are capable, even if, even if you made it yourself.

933
01:25:25,080 --> 01:25:29,400
Yeah. And even that in itself, and when you spoke about it, when you said, if you use AI

934
01:25:29,400 --> 01:25:34,200
to create something, I mean, who really created it? And then you have that wonderful quote where

935
01:25:34,200 --> 01:25:40,200
you say like, nothing was ever created by two men. We're merely sort of just adding upon what's

936
01:25:40,200 --> 01:25:49,080
already there. Yeah. Yeah, I think, I think we really need to be clear that there are major,

937
01:25:49,080 --> 01:25:54,040
major open questions here, like really fundamental open questions. It's too early to be certain of

938
01:25:54,040 --> 01:25:58,600
anything other than, I mean, I think the only thing we can be certain of is that it's very easy

939
01:25:58,600 --> 01:26:03,480
to make ethical lapses when you try to draw these distinct boundaries and you have no idea what you're

940
01:26:03,480 --> 01:26:23,880
doing.

