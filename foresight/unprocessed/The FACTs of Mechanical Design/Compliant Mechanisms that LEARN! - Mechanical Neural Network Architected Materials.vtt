WEBVTT

00:00.000 --> 00:08.820
This is a compliant machine called a mechanical neural network.

00:08.820 --> 00:13.880
It's the first of its kind to successfully demonstrate the mysterious ability to learn

00:13.880 --> 00:16.300
similar to biological brains.

00:16.300 --> 00:22.040
Inspired by the mathematics of artificial neural networks, which enable most artificial intelligent

00:22.040 --> 00:27.440
machine learning technologies today, this mechanical neural network paves the way for

00:27.440 --> 00:33.880
a new kind of material that can physically learn its mechanical behaviors and properties.

00:33.880 --> 00:38.860
Whereas the properties of most materials remain largely fixed and are a function of their

00:38.860 --> 00:44.080
composition and microstructure, this new kind of learning material could get better and

00:44.080 --> 00:49.740
better at exhibiting desired mechanical properties and behaviors, such as this shape morphing

00:49.740 --> 00:55.520
behavior, when exposed to ever increasing amounts of loading experiences.

00:55.760 --> 01:00.920
If such materials are ever damaged or cut to form new shapes or sizes, they could not

01:00.920 --> 01:07.080
only relearn their original behaviors, but they could also learn new behaviors as desired.

01:07.080 --> 01:10.720
The applications of such learning materials are endless.

01:10.720 --> 01:15.640
Imagine training the wings of an aircraft so that they learn to optimally warp their

01:15.640 --> 01:21.480
airfoil shape when subjected to unanticipated and changing wind loading conditions so that

01:21.480 --> 01:26.880
the aircraft improves its fuel efficiency and maneuverability with every flight.

01:26.880 --> 01:31.440
Or imagine training the structural members of a building by shaking them in different

01:31.440 --> 01:36.480
ways so that the building learns to remain stationary regardless of the kind of seismic

01:36.480 --> 01:40.040
waves induced by an actual earthquake when it strikes.

01:40.040 --> 01:45.360
Or imagine training body armor by repeatedly shooting it at different locations and from

01:45.360 --> 01:50.880
different orientations so that the armor gets better and better at redirecting any projectiles

01:50.920 --> 01:54.360
impacting shock waves away from vital organs.

01:54.360 --> 01:58.920
Instead of expending the immense amount of time and cost it currently takes to develop

01:58.920 --> 02:03.800
a new material that achieves specific combinations of desired properties which are currently

02:03.800 --> 02:09.400
not possible, such learning materials could simply be deployed without human understanding

02:09.400 --> 02:14.680
and the material would autonomously learn to achieve those properties while also acquiring

02:14.680 --> 02:18.880
that understanding for future designers to learn from.

02:18.880 --> 02:21.880
So how would this kind of learning material work?

02:21.880 --> 02:26.200
Well, it's important to first understand the source of its inspiration.

02:26.200 --> 02:32.360
The idea for a mechanical neural network was inspired by a physical version of an artificial

02:32.360 --> 02:33.560
neural network.

02:33.560 --> 02:38.840
The mathematics underlying artificial neural networks are diagrammed using interconnected

02:38.840 --> 02:44.280
lines that represent scalar weight values which are multiplied by input numbers that

02:44.280 --> 02:46.880
are fed into multiple layers of neurons.

02:46.880 --> 02:52.400
These neurons consist of activation functions that ultimately produce output numbers.

02:52.400 --> 02:57.360
If the artificial neural network is provided with a set of known input and output numbers,

02:57.360 --> 03:03.480
the network can be trained by tuning its weights over time so that it accurately predicts previously

03:03.480 --> 03:07.360
unknown output numbers that result for any input numbers.

03:07.360 --> 03:11.560
Here, different shades of blue represent different scalar weight values.

03:11.560 --> 03:17.280
In this way, artificial neural networks can mathematically learn to model complex systems

03:17.280 --> 03:20.160
that map many inputs to many outputs.

03:20.160 --> 03:25.640
Similarly, mechanical neural networks possess physical interconnected tunable beams, shown

03:25.640 --> 03:30.680
here as blue lines, which are mechanical analogues to the weight lines within artificial neural

03:30.680 --> 03:32.040
network diagrams.

03:32.040 --> 03:36.640
The beams connected nodes, shown as white circles outline black, which are analogous

03:36.720 --> 03:39.800
to the neurons within artificial neural networks.

03:39.800 --> 03:44.920
Whereas artificial neural networks tune their weights to match input numbers to output numbers,

03:44.920 --> 03:50.280
the mechanical neural network proposed here tunes the axial stiffness values of its beams

03:50.280 --> 03:54.720
to match input loads to desired output behaviors.

03:54.720 --> 03:59.240
To demonstrate how the envisioned mechanical learning would work, suppose a shape morphing

03:59.240 --> 04:05.080
behavior is desired for the following eight-layer deep triangular lattice consisting of eight

04:05.080 --> 04:08.120
input nodes and eight output nodes.

04:08.120 --> 04:13.360
Note that the black bars along the top and bottom edges of this lattice represent a grounded

04:13.360 --> 04:16.880
body on which the nodes that touch it are pinned.

04:16.880 --> 04:21.520
Suppose that when the input nodes along the left side are loaded by equivalent horizontal

04:21.520 --> 04:27.160
forces, it is desired that the output nodes along the right side respond by displacing

04:27.160 --> 04:32.720
to target locations along the contour of an undulating sinusoidal shape, shown as the

04:32.720 --> 04:33.800
red curve.

04:33.800 --> 04:38.160
To learn the shape morphing behavior, each tunable beam in the lattice would start the

04:38.160 --> 04:44.760
learning process by setting their axial stiffness to a random value depicted here as a specific

04:44.760 --> 04:47.800
shade of blue according to this color scale.

04:47.800 --> 04:52.120
When the input nodes on the left side of the lattice are loaded with the horizontal input

04:52.120 --> 04:57.560
forces of the desired behavior, the resulting displacements of the output nodes would then

04:57.560 --> 05:04.520
be measured and a mean-squared error, i.e. MSE, would be calculated by finding the average

05:04.520 --> 05:09.600
of the scalar difference between the target output node displacements and these measured

05:09.600 --> 05:16.080
output node displacements, i.e. their final location error, EI, for all n nodes on the

05:16.080 --> 05:22.040
right side of the lattice squared, for this lattice, n equals eight output nodes.

05:22.040 --> 05:27.120
The tunable beam elements would then change their axial stiffness values according to

05:27.160 --> 05:33.640
an optimization algorithm such that when the process of loading, measuring, and calculating

05:33.640 --> 05:39.360
the mean-squared error is repeated, the mean-squared error would be minimized until a working

05:39.360 --> 05:45.120
combination of beam stiffness values is identified that achieves the desired behavior.

05:45.120 --> 05:49.960
One possible combination of beam stiffness values that enabled this mechanical neural

05:49.960 --> 05:55.120
network to achieve the desired sinusoidal shape morphing behavior is shown here.

05:55.120 --> 06:00.520
Suppose it is desired that the neural network then learns another new behavior in addition

06:00.520 --> 06:01.880
to the first behavior.

06:01.880 --> 06:06.880
Specifically, suppose it is desired that in addition to the lattice's output nodes displacing

06:06.880 --> 06:11.960
to this sinusoidal shape in response to its input nodes being loaded with equivalent horizontal

06:11.960 --> 06:17.720
forces, the same lattice's output nodes also displace to an inverted sinusoidal shape in

06:17.720 --> 06:23.480
response to its input nodes being loaded by equivalent vertical input forces instead.

06:23.480 --> 06:28.800
To learn the new behavior, shown green, while maintaining the ability to simultaneously achieve

06:28.800 --> 06:33.640
the first behavior, shown red, the lattice of tunable beam elements could either start

06:33.640 --> 06:39.160
with another random combination of stiffness values as shown here, or they could start

06:39.160 --> 06:43.840
with the same combination of stiffness values that were found to successfully achieve the

06:43.840 --> 06:45.480
first behavior only.

06:45.480 --> 06:50.280
Parenthetically, the latter choice becomes increasingly favorable as the mechanical neural

06:50.280 --> 06:54.920
network acquires more and more behaviors because the working combination of beam stiffness

06:54.920 --> 06:59.720
values acts as a sort of muscle memory for previously learned behaviors.

06:59.720 --> 07:04.280
Regardless of what starting combination of beam stiffness values are selected, however,

07:04.280 --> 07:09.120
the input nodes would then be loaded with both the horizontal and then vertical forces

07:09.120 --> 07:15.160
of the first and second behaviors respectively, and a single mean squared error would be calculated

07:15.160 --> 07:21.800
that simultaneously considers the square of the output node final location errors of both

07:21.800 --> 07:24.400
loading scenarios averaged together.

07:24.400 --> 07:29.080
The tunable beam elements would then change their axial stiffness values according to

07:29.080 --> 07:36.920
the same optimization algorithm such that when the process of loading, measuring, and calculating

07:36.920 --> 07:42.400
the cumulative mean squared error of both behaviors is repeated, this new mean squared

07:42.400 --> 07:48.360
error would be minimized until a working combination of beam stiffness values is identified

07:48.360 --> 07:53.400
that successfully achieves the first and second behaviors simultaneously.

07:53.400 --> 07:58.200
Note that the tunable beams all remain the same shade of blue regardless of whether the

07:58.200 --> 08:02.680
lattice is actuated with the loads of the first or the second behaviors because the

08:02.680 --> 08:07.600
same combination of beam stiffness values successfully achieves both behaviors.

08:07.600 --> 08:12.520
It's also important to recognize that mechanical neural networks can achieve the same desired

08:12.520 --> 08:17.640
set of behaviors using many different combinations of beam stiffness values.

08:17.640 --> 08:22.680
Note for instance that although this second solution exhibits the same desired behaviors

08:22.680 --> 08:27.680
as our first solution, it does so with an entirely different combination of beam stiffness

08:27.680 --> 08:28.680
values.

08:28.680 --> 08:32.560
The fact that many different combinations of beam stiffness values can achieve the same

08:32.560 --> 08:37.360
behaviors allows mechanical neural networks to learn more and more new behaviors while

08:37.360 --> 08:40.520
retaining memory of previously learned behaviors.

08:40.520 --> 08:45.200
Finally, it's also worth noting that mechanical neural networks are not limited to learning

08:45.200 --> 08:51.120
shape morphing behaviors only but can learn almost any combination of quasi-static, thermal,

08:51.120 --> 08:56.560
and even dynamic mechanical behaviors including the control of wave propagation within their

08:56.560 --> 08:59.620
lattice.

08:59.620 --> 09:04.640
To experimentally demonstrate the concept of a mechanical neural network, it was important

09:04.640 --> 09:09.600
to first design a tunable beam that could achieve adjustable stiffness along its axis.

09:09.600 --> 09:14.080
After comparing multiple concepts, we settled on this compliant design.

09:14.080 --> 09:18.720
It consists of two parallel blade flexors that deform to guide the translational extension

09:18.720 --> 09:23.920
and contraction of the beam along its axis while rigidly constraining all other directions.

09:23.920 --> 09:29.040
A bracket is attached to the beam's housing in part to provide a hard stop so that the

09:29.040 --> 09:33.240
parallel blade flexors are not allowed to deform to a point where they would yield,

09:33.240 --> 09:36.520
i.e. be permanently damaged beyond their elastic limit.

09:36.520 --> 09:42.120
The bracket is also attached to the magnet end of a voice coil actuator which is aligned

09:42.120 --> 09:44.040
with the beam's central axis.

09:44.040 --> 09:48.720
The actuator's other mating end, which consists of a coil of copper wire wrapped around a

09:48.720 --> 09:53.560
drum, is attached to another bracket that is attached to the other side of the beam's

09:53.560 --> 09:54.560
housing.

09:54.560 --> 09:58.520
Depending on the direction and magnitude of the current flowing through the wrapped wire,

09:58.520 --> 10:03.280
the magnetic field can be induced by the coil that pushes or pulls on the voice coil's

10:03.280 --> 10:08.800
magnet end, thus actuating the beam along its axis in either direction.

10:08.800 --> 10:13.280
Two strain gauge sensors are mounted on either side of one of the parallel blade flexors

10:13.280 --> 10:18.660
at its base to accurately measure the resulting displacement of the beam along its axis by

10:18.660 --> 10:23.560
transforming the flexor's deformation strain into a proportional voltage signal.

10:23.560 --> 10:29.080
In this way, closed loop control can be applied to actively tune the beam's axial stiffness

10:29.080 --> 10:32.320
to achieve any value between an upper and a lower limit.

10:32.320 --> 10:37.320
You could imagine that the highest axial stiffness would be achieved if when the beam is loaded,

10:37.320 --> 10:42.880
the voice coil responds by resisting the load with the largest actuated force possible in

10:42.880 --> 10:45.480
the opposite direction as the applied load.

10:45.480 --> 10:49.920
Likewise, the lowest axial stiffness would be achieved if when the beam is loaded, the

10:49.920 --> 10:55.160
voice coil responds by assisting the load with the largest actuated force possible along

10:55.160 --> 10:57.760
the same direction as the applied load.

10:57.760 --> 11:03.760
In this way, the tunable beams could be made to achieve zero or even negative stiffness.

11:03.760 --> 11:08.880
The beam's housing inflectures were cut from an aluminum sheet using wire EDM and its brackets

11:08.880 --> 11:11.760
were machined from aluminum L brackets.

11:11.760 --> 11:16.760
We applied proportional and derivative closed loop control as detailed by this diagram to

11:16.760 --> 11:20.240
achieve the desired stiffness control of the tunable beams.

11:20.240 --> 11:25.760
An instant testing machine was used to individually calibrate each beam by generating these four

11:25.760 --> 11:28.800
plots to inform the controller as shown.

11:28.800 --> 11:34.640
If this function is set to EK in the control loop and the proportional gain Kp is set to

11:34.640 --> 11:40.000
a desired value, the resulting forced displacement response of the actively controlled beam will

11:40.000 --> 11:45.120
be linear and will possess an unchanging slope, i.e. stiffness, that is equal to the

11:45.120 --> 11:48.280
proportional gain Kp value set.

11:48.280 --> 11:52.960
This plot, measured using an instant testing machine, shows the linear forced displacement

11:52.960 --> 11:58.480
responses of a tunable beam being controlled with different Kp values to achieve corresponding

11:58.480 --> 12:01.520
positive and negative axial stiffness values.

12:01.520 --> 12:05.840
The maximum and minimum stiffness values that the beam could be controlled to achieve without

12:05.840 --> 12:13.280
becoming unstable was measured to be 2.3 Nm and negative 2 Nm respectively.

12:13.280 --> 12:18.280
With a working tunable beam that could be controlled to achieve any desired axial stiffness

12:18.280 --> 12:24.000
between its maximum and minimum stiffness values, 21 such tunable beams were fabricated

12:24.000 --> 12:29.320
and assembled within a triangular configuration as shown by these blue lines to demonstrate

12:29.320 --> 12:32.000
learning within a mechanical neural network.

12:32.000 --> 12:36.720
Four additional voice coil actuators were used in conjunction with decoupling flexures

12:36.720 --> 12:40.720
to drive the two input nodes on the left side of the lattice with forces that can be made

12:40.720 --> 12:43.800
to point in any in-plane direction desired.

12:43.800 --> 12:49.800
Two cameras mounted on a frame directly measure the displacement of pins inserted at the center

12:49.800 --> 12:55.360
of both output nodes and black felt is used to contrast the white color of the pin heads

12:55.360 --> 12:57.080
so that they stand out.

12:57.080 --> 13:01.760
This colored computer generated image helps clarify other important features within the

13:01.760 --> 13:03.440
mechanical neural network.

13:03.440 --> 13:08.400
Note the purple colored rotational flexures centered around each of the network's nodes.

13:08.400 --> 13:13.760
These flexures passively deform to accommodate the expansions and contractions of the tunable

13:13.760 --> 13:16.840
beams as the network is loaded during learning.

13:16.840 --> 13:22.120
Also note the green colored flexures that decouple the input actuators due to their cleverly

13:22.120 --> 13:23.640
stacked arrangement.

13:23.640 --> 13:28.480
Hard stops are built around all the flexures in the system to prevent them from yielding.

13:28.480 --> 13:33.000
Although the machine's two mounted cameras can directly measure the lattice's output

13:33.000 --> 13:38.560
node displacements, note that the strain gauge sensors on each beam can directly measure

13:38.560 --> 13:43.360
the beam's extension and contraction and that information can be used to indirectly

13:43.360 --> 13:47.600
calculate the displacements of all the nodes in the mechanical neural network including

13:47.600 --> 13:49.880
the displacements of its output nodes.

13:49.880 --> 13:54.560
This strain gauge approach to indirectly sensing the output node displacements can predict

13:54.560 --> 13:59.680
the displacements with a much higher sampling rate compared to the frame rate of the cameras.

13:59.680 --> 14:04.440
These plots show how accurately the strain gauge approach tracked the cameras measured

14:04.440 --> 14:09.000
output node displacements when the lattice was loaded with a random combination of axial

14:09.000 --> 14:12.720
stiffness values uploaded to each tunable beam in the lattice.

14:12.720 --> 14:16.680
The strain gauge approach is also important to the functionality of mechanical neural

14:16.680 --> 14:21.400
networks because without the approach such networks cannot learn without being placed

14:21.400 --> 14:26.680
in a testing rig which is not practical for most applications that require in-field learning.

14:26.680 --> 14:31.360
Moreover, the ability to accurately measure the displacements of all the nodes in the

14:31.360 --> 14:36.560
network when it is subjected to unanticipated and changing ambient loading scenarios is

14:36.560 --> 14:41.800
necessary for mechanical neural networks to be able to identify when those loads correspond

14:41.800 --> 14:46.480
to the input forces of their desired behaviors being learned so that the network can then

14:46.480 --> 14:51.080
calculate its mean squared error and minimize it as described previously.

14:51.080 --> 14:56.480
Note that the input node forces can be indirectly calculated at any given time using the current

14:56.480 --> 15:01.800
combination of beam stiffness values uploaded to the network at that time and the corresponding

15:01.800 --> 15:06.240
strain gauge measured displacements of all the network's nodes that resulted from these

15:06.240 --> 15:08.400
loading forces.

15:08.400 --> 15:13.600
Our 21 beam mechanical neural network first demonstrated its ability to learn by learning

15:13.600 --> 15:18.160
two behaviors simultaneously using the approach described previously.

15:18.160 --> 15:23.640
For the first behavior shown exaggerated in red here, output node 1 should displace outward

15:23.640 --> 15:29.080
0.5 millimeters while output node 2 should displace inward 0.5 millimeters when the input

15:29.080 --> 15:32.000
nodes are loaded with 1 Newton horizontal forces.

15:32.000 --> 15:37.200
For the second behavior shown exaggerated in green here, the output node 1 should displace

15:37.200 --> 15:42.720
inward 0.5 millimeters while output node 2 should displace outward 0.5 millimeters when

15:42.720 --> 15:46.360
the input nodes are loaded with 1 Newton vertical forces.

15:46.360 --> 15:50.920
The first optimization algorithm that we use to determine what combination of axial stiffness

15:50.920 --> 15:55.720
values should be uploaded to each tunable beam in lattice during each step of the learning

15:55.720 --> 15:58.560
approach was a genetic algorithm.

15:58.560 --> 16:03.280
The algorithm samples 1000 random beam stiffness combinations.

16:03.280 --> 16:08.280
It then identifies and plots the combination that achieved the lowest resulting output

16:08.280 --> 16:10.520
node displacement mean squared error.

16:10.520 --> 16:16.320
A new, more promising group of 1000 beam stiffness combinations is then generated by crossing

16:16.320 --> 16:20.240
the most successful combinations attempted in the previous group.

16:20.240 --> 16:25.640
The process is repeated until the mean squared error calculated stops changing from one group

16:25.640 --> 16:26.640
to the next.

16:26.640 --> 16:31.840
A plot showing how the algorithm reduced the mean squared error over time is shown here,

16:31.840 --> 16:37.160
along with a video showing the mechanical neural network learning in real time.

16:37.160 --> 16:42.280
This animation shows how both output nodes displaced progressively closer to their target

16:42.280 --> 16:48.320
locations as improved beam stiffness combinations were identified from one group to the next.

16:48.320 --> 16:53.280
The initial starting and ending locations of those output nodes are shown here without

16:53.280 --> 16:55.720
the visual clutter of the path taken.

16:55.720 --> 17:01.520
You can see that their final locations are almost directly on top of the target locations.

17:01.520 --> 17:06.720
Once learning was successfully demonstrated in this way, using the genetic algorithm described

17:06.720 --> 17:12.640
previously, we then conducted a study to compare the performance of five other optimization

17:12.640 --> 17:17.440
algorithms to determine which algorithm is best suited for mechanical neural network

17:17.440 --> 17:19.040
learning in general.

17:19.040 --> 17:24.400
The five additional algorithms studied were full pattern search, partial pattern search,

17:24.400 --> 17:28.880
interior point, sequential quadratic programming, and Nelder mean.

17:28.880 --> 17:33.200
We compared how low the final mean squared error could be made using each algorithm,

17:33.200 --> 17:38.160
i.e. how accurately the mechanical neural network could successfully learn its behaviors,

17:38.160 --> 17:43.480
and how many iterations the algorithm required to achieve that final mean squared error, i.e.

17:43.480 --> 17:47.160
how fast the mechanical neural network could learn its behaviors.

17:47.160 --> 17:51.400
It was determined that Nelder mean was the best suited algorithm for mechanical neural

17:51.400 --> 17:56.840
networks due to the algorithm's practical learning speed, impressive learning accuracy,

17:56.840 --> 17:59.280
and its insensitivity to system noise.

17:59.280 --> 18:03.520
The details of that study were published in the Journal of Mechanical Design and a link

18:03.520 --> 18:06.480
to the paper is provided in the description below.

18:06.480 --> 18:11.280
We were also interested to use the mechanical neural network to determine whether beams

18:11.280 --> 18:16.040
that are tuned to exhibit non-linear stiffness, i.e. stiffness that changes as the beams

18:16.040 --> 18:20.800
deform, are favorable for learning compared to beams that are tuned to exhibit linear

18:20.800 --> 18:21.800
stiffness.

18:21.800 --> 18:25.440
Our closed loop controller was designed to test this hypothesis.

18:25.440 --> 18:31.600
If this f of ek function is changed from ek to a different function, like tangent ek,

18:31.600 --> 18:36.120
then the resulting force displacement plot exhibited by the actively controlled beam

18:36.120 --> 18:38.680
would be a non-linear tangent function.

18:38.680 --> 18:43.600
This plot shows the tunable beam's force displacement response measured using an instrument

18:43.600 --> 18:50.200
testing machine with f of ek equaling ek and tangent ek for different proportional gain

18:50.200 --> 18:54.240
values, i.e. a kp of 1, 0, and negative 1.

18:54.240 --> 18:59.280
We then trained the mechanical neural network to learn random shape morphing behaviors using

18:59.280 --> 19:05.240
both linear and non-linear tangent force displacement responses and compared their mean squared error

19:05.240 --> 19:07.960
versus time plots as shown here.

19:07.960 --> 19:13.400
Much to our surprise, the plots suggest that tunable beams that achieve linear stiffness

19:13.400 --> 19:18.360
can learn behaviors with greater accuracy, i.e. lower mean squared error, than tunable

19:18.360 --> 19:21.360
beams that achieve non-linear stiffness.

19:21.360 --> 19:26.640
We then created a computational tool to simulate the behavior of our mechanical neural network

19:26.640 --> 19:32.200
design so that we could use the tool to predict how well larger versions of the same design

19:32.200 --> 19:37.320
would learn if we had the time and resources to build and incorporate many more tunable

19:37.320 --> 19:42.960
beams within its lattice as depicted by this photoshopped image of a much larger lattice.

19:42.960 --> 19:48.240
Our computational tool models the tunable beams as linear beams, which are depicted

19:48.240 --> 19:53.160
as blue lines, and their lengths are set to be the length of the beams in our fabricated

19:53.160 --> 19:57.360
mechanical neural network, i.e. 6 inches from node to node.

19:57.360 --> 20:02.240
We restricted each beam in our simulation to only achieve axial stiffness values between

20:02.240 --> 20:07.920
the maximum and minimum stiffness values measured from our fabricated beam, i.e. 2.3 Newtons

20:07.920 --> 20:12.440
per millimeter and negative 2 Newtons per millimeter respectively, and we set their passive

20:12.440 --> 20:18.280
non-axial stiffness values equal to the values calculated using finite element analysis as

20:18.280 --> 20:19.280
shown here.

20:19.280 --> 20:24.400
We also restricted the simulated beams from extending or contracting more than plus and

20:24.400 --> 20:29.280
minus 2.5 millimeters, which is the limit of our fabricated beams as governed by their

20:29.280 --> 20:30.640
hard stops.

20:30.640 --> 20:35.760
Finite element analysis was used to validate the computational tool's accuracy by loading

20:35.760 --> 20:41.080
a 21-beam version of the design in its passive state, i.e. without any closed loop stiffness

20:41.080 --> 20:47.120
control activated, with 25 random force combinations imparted on its two input nodes.

20:47.120 --> 20:52.560
The X and Y components of the lattice's resulting output node displacements, calculated using

20:52.560 --> 20:59.000
both finite element analysis and our computational tool, are plotted here showing good correspondence

20:59.000 --> 21:05.240
between each of the 25 force combinations, once configured to mimic our fabricated mechanical

21:05.240 --> 21:06.240
neural network.

21:06.240 --> 21:11.260
The computational tool was then used to simulate the effect that the number of layers would

21:11.260 --> 21:16.920
have on the ability for a triangularly configured mechanical neural network consisting of eight

21:16.920 --> 21:22.440
input and output nodes to learn different numbers of random shape morphing behaviors.

21:22.440 --> 21:27.360
The results of the study indicate, one, that mechanical learning improves with more layers,

21:27.360 --> 21:31.920
likely because there are more tunable beams with which to learn, and two, the more random

21:31.920 --> 21:36.420
behaviors that are required to be learned, the less accurately all the behaviors can

21:36.420 --> 21:38.440
be learned simultaneously.

21:38.440 --> 21:43.280
This plot was similarly generated, but for only two, four, and eight layers, and for

21:43.280 --> 21:48.360
both triangular and square lattice configurations, shown green and red respectively.

21:48.360 --> 21:53.680
It is clear from these results that triangular lattice configurations can, in general, learn

21:53.680 --> 21:59.120
different numbers of shape morphing behaviors more effectively than square lattice configurations.

21:59.120 --> 22:04.240
The reason is likely because triangular lattices have more beams for the same number of layers,

22:04.240 --> 22:09.200
and they can propagate displacements in all directions, rather than just along orthogonal

22:09.200 --> 22:12.240
directions, as is the case with square lattices.

22:12.240 --> 22:17.200
To learn the effect that the number of layers and output nodes have on mechanical learning,

22:17.200 --> 22:22.680
we used our computational tool to generate the following plot for triangular lattices

22:22.680 --> 22:26.280
that learn the two sinusoidal behaviors described previously.

22:26.280 --> 22:31.480
The plot indicates that once the lattice possesses two or more layers, the number of output nodes

22:31.480 --> 22:33.120
does not seem to matter.

22:33.120 --> 22:38.040
It's true that the more output nodes a lattice has, the more output node displacement requirements

22:38.040 --> 22:42.880
the output nodes must satisfy, but it's also true that the more output nodes a lattice has,

22:42.880 --> 22:47.760
the more beams the lattice can employ to satisfy those requirements during learning, so both

22:47.760 --> 22:49.760
effects seem to negate each other.

22:49.760 --> 22:54.040
If you'd like to learn more about the details presented in this video, I encourage you to

22:54.040 --> 22:58.320
read our first published journal article on the topic of mechanical neural networks

22:58.320 --> 23:02.760
in Science Robotics, where our work was featured on the journal's front cover.

23:02.760 --> 23:07.680
A link to the paper is provided in the description below, along with a link to my Thingiverse

23:07.680 --> 23:12.360
account where you can download the part files necessary to fabricate our mechanical neural

23:12.360 --> 23:13.360
network.

23:13.360 --> 23:18.000
Finally, I want to thank my students Ryan Lee, who built and tested the mechanical neural

23:18.000 --> 23:24.160
network, Erwin Mulder, who developed our computational tool, P. H. R. Sainaghi, who helped perform

23:24.160 --> 23:28.960
the optimization algorithm comparison study, and all the other students who contributed

23:28.960 --> 23:31.960
in smaller ways to the success of this project.

23:31.960 --> 23:37.680
I am especially grateful to my AFOSR program manager, Les Lee, for making this research

23:37.680 --> 23:42.160
possible through his continued funding and generous support of my group.

23:42.160 --> 23:46.240
If you'd like to support my channel, I've provided instructions in the description

23:46.240 --> 23:47.240
below.

23:47.240 --> 23:50.000
Thanks for watching the Facts of Mechanical Design.

