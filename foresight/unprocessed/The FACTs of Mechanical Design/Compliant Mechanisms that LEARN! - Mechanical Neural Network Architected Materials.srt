1
00:00:00,000 --> 00:00:08,820
This is a compliant machine called a mechanical neural network.

2
00:00:08,820 --> 00:00:13,880
It's the first of its kind to successfully demonstrate the mysterious ability to learn

3
00:00:13,880 --> 00:00:16,300
similar to biological brains.

4
00:00:16,300 --> 00:00:22,040
Inspired by the mathematics of artificial neural networks, which enable most artificial intelligent

5
00:00:22,040 --> 00:00:27,440
machine learning technologies today, this mechanical neural network paves the way for

6
00:00:27,440 --> 00:00:33,880
a new kind of material that can physically learn its mechanical behaviors and properties.

7
00:00:33,880 --> 00:00:38,860
Whereas the properties of most materials remain largely fixed and are a function of their

8
00:00:38,860 --> 00:00:44,080
composition and microstructure, this new kind of learning material could get better and

9
00:00:44,080 --> 00:00:49,740
better at exhibiting desired mechanical properties and behaviors, such as this shape morphing

10
00:00:49,740 --> 00:00:55,520
behavior, when exposed to ever increasing amounts of loading experiences.

11
00:00:55,760 --> 00:01:00,920
If such materials are ever damaged or cut to form new shapes or sizes, they could not

12
00:01:00,920 --> 00:01:07,080
only relearn their original behaviors, but they could also learn new behaviors as desired.

13
00:01:07,080 --> 00:01:10,720
The applications of such learning materials are endless.

14
00:01:10,720 --> 00:01:15,640
Imagine training the wings of an aircraft so that they learn to optimally warp their

15
00:01:15,640 --> 00:01:21,480
airfoil shape when subjected to unanticipated and changing wind loading conditions so that

16
00:01:21,480 --> 00:01:26,880
the aircraft improves its fuel efficiency and maneuverability with every flight.

17
00:01:26,880 --> 00:01:31,440
Or imagine training the structural members of a building by shaking them in different

18
00:01:31,440 --> 00:01:36,480
ways so that the building learns to remain stationary regardless of the kind of seismic

19
00:01:36,480 --> 00:01:40,040
waves induced by an actual earthquake when it strikes.

20
00:01:40,040 --> 00:01:45,360
Or imagine training body armor by repeatedly shooting it at different locations and from

21
00:01:45,360 --> 00:01:50,880
different orientations so that the armor gets better and better at redirecting any projectiles

22
00:01:50,920 --> 00:01:54,360
impacting shock waves away from vital organs.

23
00:01:54,360 --> 00:01:58,920
Instead of expending the immense amount of time and cost it currently takes to develop

24
00:01:58,920 --> 00:02:03,800
a new material that achieves specific combinations of desired properties which are currently

25
00:02:03,800 --> 00:02:09,400
not possible, such learning materials could simply be deployed without human understanding

26
00:02:09,400 --> 00:02:14,680
and the material would autonomously learn to achieve those properties while also acquiring

27
00:02:14,680 --> 00:02:18,880
that understanding for future designers to learn from.

28
00:02:18,880 --> 00:02:21,880
So how would this kind of learning material work?

29
00:02:21,880 --> 00:02:26,200
Well, it's important to first understand the source of its inspiration.

30
00:02:26,200 --> 00:02:32,360
The idea for a mechanical neural network was inspired by a physical version of an artificial

31
00:02:32,360 --> 00:02:33,560
neural network.

32
00:02:33,560 --> 00:02:38,840
The mathematics underlying artificial neural networks are diagrammed using interconnected

33
00:02:38,840 --> 00:02:44,280
lines that represent scalar weight values which are multiplied by input numbers that

34
00:02:44,280 --> 00:02:46,880
are fed into multiple layers of neurons.

35
00:02:46,880 --> 00:02:52,400
These neurons consist of activation functions that ultimately produce output numbers.

36
00:02:52,400 --> 00:02:57,360
If the artificial neural network is provided with a set of known input and output numbers,

37
00:02:57,360 --> 00:03:03,480
the network can be trained by tuning its weights over time so that it accurately predicts previously

38
00:03:03,480 --> 00:03:07,360
unknown output numbers that result for any input numbers.

39
00:03:07,360 --> 00:03:11,560
Here, different shades of blue represent different scalar weight values.

40
00:03:11,560 --> 00:03:17,280
In this way, artificial neural networks can mathematically learn to model complex systems

41
00:03:17,280 --> 00:03:20,160
that map many inputs to many outputs.

42
00:03:20,160 --> 00:03:25,640
Similarly, mechanical neural networks possess physical interconnected tunable beams, shown

43
00:03:25,640 --> 00:03:30,680
here as blue lines, which are mechanical analogues to the weight lines within artificial neural

44
00:03:30,680 --> 00:03:32,040
network diagrams.

45
00:03:32,040 --> 00:03:36,640
The beams connected nodes, shown as white circles outline black, which are analogous

46
00:03:36,720 --> 00:03:39,800
to the neurons within artificial neural networks.

47
00:03:39,800 --> 00:03:44,920
Whereas artificial neural networks tune their weights to match input numbers to output numbers,

48
00:03:44,920 --> 00:03:50,280
the mechanical neural network proposed here tunes the axial stiffness values of its beams

49
00:03:50,280 --> 00:03:54,720
to match input loads to desired output behaviors.

50
00:03:54,720 --> 00:03:59,240
To demonstrate how the envisioned mechanical learning would work, suppose a shape morphing

51
00:03:59,240 --> 00:04:05,080
behavior is desired for the following eight-layer deep triangular lattice consisting of eight

52
00:04:05,080 --> 00:04:08,120
input nodes and eight output nodes.

53
00:04:08,120 --> 00:04:13,360
Note that the black bars along the top and bottom edges of this lattice represent a grounded

54
00:04:13,360 --> 00:04:16,880
body on which the nodes that touch it are pinned.

55
00:04:16,880 --> 00:04:21,520
Suppose that when the input nodes along the left side are loaded by equivalent horizontal

56
00:04:21,520 --> 00:04:27,160
forces, it is desired that the output nodes along the right side respond by displacing

57
00:04:27,160 --> 00:04:32,720
to target locations along the contour of an undulating sinusoidal shape, shown as the

58
00:04:32,720 --> 00:04:33,800
red curve.

59
00:04:33,800 --> 00:04:38,160
To learn the shape morphing behavior, each tunable beam in the lattice would start the

60
00:04:38,160 --> 00:04:44,760
learning process by setting their axial stiffness to a random value depicted here as a specific

61
00:04:44,760 --> 00:04:47,800
shade of blue according to this color scale.

62
00:04:47,800 --> 00:04:52,120
When the input nodes on the left side of the lattice are loaded with the horizontal input

63
00:04:52,120 --> 00:04:57,560
forces of the desired behavior, the resulting displacements of the output nodes would then

64
00:04:57,560 --> 00:05:04,520
be measured and a mean-squared error, i.e. MSE, would be calculated by finding the average

65
00:05:04,520 --> 00:05:09,600
of the scalar difference between the target output node displacements and these measured

66
00:05:09,600 --> 00:05:16,080
output node displacements, i.e. their final location error, EI, for all n nodes on the

67
00:05:16,080 --> 00:05:22,040
right side of the lattice squared, for this lattice, n equals eight output nodes.

68
00:05:22,040 --> 00:05:27,120
The tunable beam elements would then change their axial stiffness values according to

69
00:05:27,160 --> 00:05:33,640
an optimization algorithm such that when the process of loading, measuring, and calculating

70
00:05:33,640 --> 00:05:39,360
the mean-squared error is repeated, the mean-squared error would be minimized until a working

71
00:05:39,360 --> 00:05:45,120
combination of beam stiffness values is identified that achieves the desired behavior.

72
00:05:45,120 --> 00:05:49,960
One possible combination of beam stiffness values that enabled this mechanical neural

73
00:05:49,960 --> 00:05:55,120
network to achieve the desired sinusoidal shape morphing behavior is shown here.

74
00:05:55,120 --> 00:06:00,520
Suppose it is desired that the neural network then learns another new behavior in addition

75
00:06:00,520 --> 00:06:01,880
to the first behavior.

76
00:06:01,880 --> 00:06:06,880
Specifically, suppose it is desired that in addition to the lattice's output nodes displacing

77
00:06:06,880 --> 00:06:11,960
to this sinusoidal shape in response to its input nodes being loaded with equivalent horizontal

78
00:06:11,960 --> 00:06:17,720
forces, the same lattice's output nodes also displace to an inverted sinusoidal shape in

79
00:06:17,720 --> 00:06:23,480
response to its input nodes being loaded by equivalent vertical input forces instead.

80
00:06:23,480 --> 00:06:28,800
To learn the new behavior, shown green, while maintaining the ability to simultaneously achieve

81
00:06:28,800 --> 00:06:33,640
the first behavior, shown red, the lattice of tunable beam elements could either start

82
00:06:33,640 --> 00:06:39,160
with another random combination of stiffness values as shown here, or they could start

83
00:06:39,160 --> 00:06:43,840
with the same combination of stiffness values that were found to successfully achieve the

84
00:06:43,840 --> 00:06:45,480
first behavior only.

85
00:06:45,480 --> 00:06:50,280
Parenthetically, the latter choice becomes increasingly favorable as the mechanical neural

86
00:06:50,280 --> 00:06:54,920
network acquires more and more behaviors because the working combination of beam stiffness

87
00:06:54,920 --> 00:06:59,720
values acts as a sort of muscle memory for previously learned behaviors.

88
00:06:59,720 --> 00:07:04,280
Regardless of what starting combination of beam stiffness values are selected, however,

89
00:07:04,280 --> 00:07:09,120
the input nodes would then be loaded with both the horizontal and then vertical forces

90
00:07:09,120 --> 00:07:15,160
of the first and second behaviors respectively, and a single mean squared error would be calculated

91
00:07:15,160 --> 00:07:21,800
that simultaneously considers the square of the output node final location errors of both

92
00:07:21,800 --> 00:07:24,400
loading scenarios averaged together.

93
00:07:24,400 --> 00:07:29,080
The tunable beam elements would then change their axial stiffness values according to

94
00:07:29,080 --> 00:07:36,920
the same optimization algorithm such that when the process of loading, measuring, and calculating

95
00:07:36,920 --> 00:07:42,400
the cumulative mean squared error of both behaviors is repeated, this new mean squared

96
00:07:42,400 --> 00:07:48,360
error would be minimized until a working combination of beam stiffness values is identified

97
00:07:48,360 --> 00:07:53,400
that successfully achieves the first and second behaviors simultaneously.

98
00:07:53,400 --> 00:07:58,200
Note that the tunable beams all remain the same shade of blue regardless of whether the

99
00:07:58,200 --> 00:08:02,680
lattice is actuated with the loads of the first or the second behaviors because the

100
00:08:02,680 --> 00:08:07,600
same combination of beam stiffness values successfully achieves both behaviors.

101
00:08:07,600 --> 00:08:12,520
It's also important to recognize that mechanical neural networks can achieve the same desired

102
00:08:12,520 --> 00:08:17,640
set of behaviors using many different combinations of beam stiffness values.

103
00:08:17,640 --> 00:08:22,680
Note for instance that although this second solution exhibits the same desired behaviors

104
00:08:22,680 --> 00:08:27,680
as our first solution, it does so with an entirely different combination of beam stiffness

105
00:08:27,680 --> 00:08:28,680
values.

106
00:08:28,680 --> 00:08:32,560
The fact that many different combinations of beam stiffness values can achieve the same

107
00:08:32,560 --> 00:08:37,360
behaviors allows mechanical neural networks to learn more and more new behaviors while

108
00:08:37,360 --> 00:08:40,520
retaining memory of previously learned behaviors.

109
00:08:40,520 --> 00:08:45,200
Finally, it's also worth noting that mechanical neural networks are not limited to learning

110
00:08:45,200 --> 00:08:51,120
shape morphing behaviors only but can learn almost any combination of quasi-static, thermal,

111
00:08:51,120 --> 00:08:56,560
and even dynamic mechanical behaviors including the control of wave propagation within their

112
00:08:56,560 --> 00:08:59,620
lattice.

113
00:08:59,620 --> 00:09:04,640
To experimentally demonstrate the concept of a mechanical neural network, it was important

114
00:09:04,640 --> 00:09:09,600
to first design a tunable beam that could achieve adjustable stiffness along its axis.

115
00:09:09,600 --> 00:09:14,080
After comparing multiple concepts, we settled on this compliant design.

116
00:09:14,080 --> 00:09:18,720
It consists of two parallel blade flexors that deform to guide the translational extension

117
00:09:18,720 --> 00:09:23,920
and contraction of the beam along its axis while rigidly constraining all other directions.

118
00:09:23,920 --> 00:09:29,040
A bracket is attached to the beam's housing in part to provide a hard stop so that the

119
00:09:29,040 --> 00:09:33,240
parallel blade flexors are not allowed to deform to a point where they would yield,

120
00:09:33,240 --> 00:09:36,520
i.e. be permanently damaged beyond their elastic limit.

121
00:09:36,520 --> 00:09:42,120
The bracket is also attached to the magnet end of a voice coil actuator which is aligned

122
00:09:42,120 --> 00:09:44,040
with the beam's central axis.

123
00:09:44,040 --> 00:09:48,720
The actuator's other mating end, which consists of a coil of copper wire wrapped around a

124
00:09:48,720 --> 00:09:53,560
drum, is attached to another bracket that is attached to the other side of the beam's

125
00:09:53,560 --> 00:09:54,560
housing.

126
00:09:54,560 --> 00:09:58,520
Depending on the direction and magnitude of the current flowing through the wrapped wire,

127
00:09:58,520 --> 00:10:03,280
the magnetic field can be induced by the coil that pushes or pulls on the voice coil's

128
00:10:03,280 --> 00:10:08,800
magnet end, thus actuating the beam along its axis in either direction.

129
00:10:08,800 --> 00:10:13,280
Two strain gauge sensors are mounted on either side of one of the parallel blade flexors

130
00:10:13,280 --> 00:10:18,660
at its base to accurately measure the resulting displacement of the beam along its axis by

131
00:10:18,660 --> 00:10:23,560
transforming the flexor's deformation strain into a proportional voltage signal.

132
00:10:23,560 --> 00:10:29,080
In this way, closed loop control can be applied to actively tune the beam's axial stiffness

133
00:10:29,080 --> 00:10:32,320
to achieve any value between an upper and a lower limit.

134
00:10:32,320 --> 00:10:37,320
You could imagine that the highest axial stiffness would be achieved if when the beam is loaded,

135
00:10:37,320 --> 00:10:42,880
the voice coil responds by resisting the load with the largest actuated force possible in

136
00:10:42,880 --> 00:10:45,480
the opposite direction as the applied load.

137
00:10:45,480 --> 00:10:49,920
Likewise, the lowest axial stiffness would be achieved if when the beam is loaded, the

138
00:10:49,920 --> 00:10:55,160
voice coil responds by assisting the load with the largest actuated force possible along

139
00:10:55,160 --> 00:10:57,760
the same direction as the applied load.

140
00:10:57,760 --> 00:11:03,760
In this way, the tunable beams could be made to achieve zero or even negative stiffness.

141
00:11:03,760 --> 00:11:08,880
The beam's housing inflectures were cut from an aluminum sheet using wire EDM and its brackets

142
00:11:08,880 --> 00:11:11,760
were machined from aluminum L brackets.

143
00:11:11,760 --> 00:11:16,760
We applied proportional and derivative closed loop control as detailed by this diagram to

144
00:11:16,760 --> 00:11:20,240
achieve the desired stiffness control of the tunable beams.

145
00:11:20,240 --> 00:11:25,760
An instant testing machine was used to individually calibrate each beam by generating these four

146
00:11:25,760 --> 00:11:28,800
plots to inform the controller as shown.

147
00:11:28,800 --> 00:11:34,640
If this function is set to EK in the control loop and the proportional gain Kp is set to

148
00:11:34,640 --> 00:11:40,000
a desired value, the resulting forced displacement response of the actively controlled beam will

149
00:11:40,000 --> 00:11:45,120
be linear and will possess an unchanging slope, i.e. stiffness, that is equal to the

150
00:11:45,120 --> 00:11:48,280
proportional gain Kp value set.

151
00:11:48,280 --> 00:11:52,960
This plot, measured using an instant testing machine, shows the linear forced displacement

152
00:11:52,960 --> 00:11:58,480
responses of a tunable beam being controlled with different Kp values to achieve corresponding

153
00:11:58,480 --> 00:12:01,520
positive and negative axial stiffness values.

154
00:12:01,520 --> 00:12:05,840
The maximum and minimum stiffness values that the beam could be controlled to achieve without

155
00:12:05,840 --> 00:12:13,280
becoming unstable was measured to be 2.3 Nm and negative 2 Nm respectively.

156
00:12:13,280 --> 00:12:18,280
With a working tunable beam that could be controlled to achieve any desired axial stiffness

157
00:12:18,280 --> 00:12:24,000
between its maximum and minimum stiffness values, 21 such tunable beams were fabricated

158
00:12:24,000 --> 00:12:29,320
and assembled within a triangular configuration as shown by these blue lines to demonstrate

159
00:12:29,320 --> 00:12:32,000
learning within a mechanical neural network.

160
00:12:32,000 --> 00:12:36,720
Four additional voice coil actuators were used in conjunction with decoupling flexures

161
00:12:36,720 --> 00:12:40,720
to drive the two input nodes on the left side of the lattice with forces that can be made

162
00:12:40,720 --> 00:12:43,800
to point in any in-plane direction desired.

163
00:12:43,800 --> 00:12:49,800
Two cameras mounted on a frame directly measure the displacement of pins inserted at the center

164
00:12:49,800 --> 00:12:55,360
of both output nodes and black felt is used to contrast the white color of the pin heads

165
00:12:55,360 --> 00:12:57,080
so that they stand out.

166
00:12:57,080 --> 00:13:01,760
This colored computer generated image helps clarify other important features within the

167
00:13:01,760 --> 00:13:03,440
mechanical neural network.

168
00:13:03,440 --> 00:13:08,400
Note the purple colored rotational flexures centered around each of the network's nodes.

169
00:13:08,400 --> 00:13:13,760
These flexures passively deform to accommodate the expansions and contractions of the tunable

170
00:13:13,760 --> 00:13:16,840
beams as the network is loaded during learning.

171
00:13:16,840 --> 00:13:22,120
Also note the green colored flexures that decouple the input actuators due to their cleverly

172
00:13:22,120 --> 00:13:23,640
stacked arrangement.

173
00:13:23,640 --> 00:13:28,480
Hard stops are built around all the flexures in the system to prevent them from yielding.

174
00:13:28,480 --> 00:13:33,000
Although the machine's two mounted cameras can directly measure the lattice's output

175
00:13:33,000 --> 00:13:38,560
node displacements, note that the strain gauge sensors on each beam can directly measure

176
00:13:38,560 --> 00:13:43,360
the beam's extension and contraction and that information can be used to indirectly

177
00:13:43,360 --> 00:13:47,600
calculate the displacements of all the nodes in the mechanical neural network including

178
00:13:47,600 --> 00:13:49,880
the displacements of its output nodes.

179
00:13:49,880 --> 00:13:54,560
This strain gauge approach to indirectly sensing the output node displacements can predict

180
00:13:54,560 --> 00:13:59,680
the displacements with a much higher sampling rate compared to the frame rate of the cameras.

181
00:13:59,680 --> 00:14:04,440
These plots show how accurately the strain gauge approach tracked the cameras measured

182
00:14:04,440 --> 00:14:09,000
output node displacements when the lattice was loaded with a random combination of axial

183
00:14:09,000 --> 00:14:12,720
stiffness values uploaded to each tunable beam in the lattice.

184
00:14:12,720 --> 00:14:16,680
The strain gauge approach is also important to the functionality of mechanical neural

185
00:14:16,680 --> 00:14:21,400
networks because without the approach such networks cannot learn without being placed

186
00:14:21,400 --> 00:14:26,680
in a testing rig which is not practical for most applications that require in-field learning.

187
00:14:26,680 --> 00:14:31,360
Moreover, the ability to accurately measure the displacements of all the nodes in the

188
00:14:31,360 --> 00:14:36,560
network when it is subjected to unanticipated and changing ambient loading scenarios is

189
00:14:36,560 --> 00:14:41,800
necessary for mechanical neural networks to be able to identify when those loads correspond

190
00:14:41,800 --> 00:14:46,480
to the input forces of their desired behaviors being learned so that the network can then

191
00:14:46,480 --> 00:14:51,080
calculate its mean squared error and minimize it as described previously.

192
00:14:51,080 --> 00:14:56,480
Note that the input node forces can be indirectly calculated at any given time using the current

193
00:14:56,480 --> 00:15:01,800
combination of beam stiffness values uploaded to the network at that time and the corresponding

194
00:15:01,800 --> 00:15:06,240
strain gauge measured displacements of all the network's nodes that resulted from these

195
00:15:06,240 --> 00:15:08,400
loading forces.

196
00:15:08,400 --> 00:15:13,600
Our 21 beam mechanical neural network first demonstrated its ability to learn by learning

197
00:15:13,600 --> 00:15:18,160
two behaviors simultaneously using the approach described previously.

198
00:15:18,160 --> 00:15:23,640
For the first behavior shown exaggerated in red here, output node 1 should displace outward

199
00:15:23,640 --> 00:15:29,080
0.5 millimeters while output node 2 should displace inward 0.5 millimeters when the input

200
00:15:29,080 --> 00:15:32,000
nodes are loaded with 1 Newton horizontal forces.

201
00:15:32,000 --> 00:15:37,200
For the second behavior shown exaggerated in green here, the output node 1 should displace

202
00:15:37,200 --> 00:15:42,720
inward 0.5 millimeters while output node 2 should displace outward 0.5 millimeters when

203
00:15:42,720 --> 00:15:46,360
the input nodes are loaded with 1 Newton vertical forces.

204
00:15:46,360 --> 00:15:50,920
The first optimization algorithm that we use to determine what combination of axial stiffness

205
00:15:50,920 --> 00:15:55,720
values should be uploaded to each tunable beam in lattice during each step of the learning

206
00:15:55,720 --> 00:15:58,560
approach was a genetic algorithm.

207
00:15:58,560 --> 00:16:03,280
The algorithm samples 1000 random beam stiffness combinations.

208
00:16:03,280 --> 00:16:08,280
It then identifies and plots the combination that achieved the lowest resulting output

209
00:16:08,280 --> 00:16:10,520
node displacement mean squared error.

210
00:16:10,520 --> 00:16:16,320
A new, more promising group of 1000 beam stiffness combinations is then generated by crossing

211
00:16:16,320 --> 00:16:20,240
the most successful combinations attempted in the previous group.

212
00:16:20,240 --> 00:16:25,640
The process is repeated until the mean squared error calculated stops changing from one group

213
00:16:25,640 --> 00:16:26,640
to the next.

214
00:16:26,640 --> 00:16:31,840
A plot showing how the algorithm reduced the mean squared error over time is shown here,

215
00:16:31,840 --> 00:16:37,160
along with a video showing the mechanical neural network learning in real time.

216
00:16:37,160 --> 00:16:42,280
This animation shows how both output nodes displaced progressively closer to their target

217
00:16:42,280 --> 00:16:48,320
locations as improved beam stiffness combinations were identified from one group to the next.

218
00:16:48,320 --> 00:16:53,280
The initial starting and ending locations of those output nodes are shown here without

219
00:16:53,280 --> 00:16:55,720
the visual clutter of the path taken.

220
00:16:55,720 --> 00:17:01,520
You can see that their final locations are almost directly on top of the target locations.

221
00:17:01,520 --> 00:17:06,720
Once learning was successfully demonstrated in this way, using the genetic algorithm described

222
00:17:06,720 --> 00:17:12,640
previously, we then conducted a study to compare the performance of five other optimization

223
00:17:12,640 --> 00:17:17,440
algorithms to determine which algorithm is best suited for mechanical neural network

224
00:17:17,440 --> 00:17:19,040
learning in general.

225
00:17:19,040 --> 00:17:24,400
The five additional algorithms studied were full pattern search, partial pattern search,

226
00:17:24,400 --> 00:17:28,880
interior point, sequential quadratic programming, and Nelder mean.

227
00:17:28,880 --> 00:17:33,200
We compared how low the final mean squared error could be made using each algorithm,

228
00:17:33,200 --> 00:17:38,160
i.e. how accurately the mechanical neural network could successfully learn its behaviors,

229
00:17:38,160 --> 00:17:43,480
and how many iterations the algorithm required to achieve that final mean squared error, i.e.

230
00:17:43,480 --> 00:17:47,160
how fast the mechanical neural network could learn its behaviors.

231
00:17:47,160 --> 00:17:51,400
It was determined that Nelder mean was the best suited algorithm for mechanical neural

232
00:17:51,400 --> 00:17:56,840
networks due to the algorithm's practical learning speed, impressive learning accuracy,

233
00:17:56,840 --> 00:17:59,280
and its insensitivity to system noise.

234
00:17:59,280 --> 00:18:03,520
The details of that study were published in the Journal of Mechanical Design and a link

235
00:18:03,520 --> 00:18:06,480
to the paper is provided in the description below.

236
00:18:06,480 --> 00:18:11,280
We were also interested to use the mechanical neural network to determine whether beams

237
00:18:11,280 --> 00:18:16,040
that are tuned to exhibit non-linear stiffness, i.e. stiffness that changes as the beams

238
00:18:16,040 --> 00:18:20,800
deform, are favorable for learning compared to beams that are tuned to exhibit linear

239
00:18:20,800 --> 00:18:21,800
stiffness.

240
00:18:21,800 --> 00:18:25,440
Our closed loop controller was designed to test this hypothesis.

241
00:18:25,440 --> 00:18:31,600
If this f of ek function is changed from ek to a different function, like tangent ek,

242
00:18:31,600 --> 00:18:36,120
then the resulting force displacement plot exhibited by the actively controlled beam

243
00:18:36,120 --> 00:18:38,680
would be a non-linear tangent function.

244
00:18:38,680 --> 00:18:43,600
This plot shows the tunable beam's force displacement response measured using an instrument

245
00:18:43,600 --> 00:18:50,200
testing machine with f of ek equaling ek and tangent ek for different proportional gain

246
00:18:50,200 --> 00:18:54,240
values, i.e. a kp of 1, 0, and negative 1.

247
00:18:54,240 --> 00:18:59,280
We then trained the mechanical neural network to learn random shape morphing behaviors using

248
00:18:59,280 --> 00:19:05,240
both linear and non-linear tangent force displacement responses and compared their mean squared error

249
00:19:05,240 --> 00:19:07,960
versus time plots as shown here.

250
00:19:07,960 --> 00:19:13,400
Much to our surprise, the plots suggest that tunable beams that achieve linear stiffness

251
00:19:13,400 --> 00:19:18,360
can learn behaviors with greater accuracy, i.e. lower mean squared error, than tunable

252
00:19:18,360 --> 00:19:21,360
beams that achieve non-linear stiffness.

253
00:19:21,360 --> 00:19:26,640
We then created a computational tool to simulate the behavior of our mechanical neural network

254
00:19:26,640 --> 00:19:32,200
design so that we could use the tool to predict how well larger versions of the same design

255
00:19:32,200 --> 00:19:37,320
would learn if we had the time and resources to build and incorporate many more tunable

256
00:19:37,320 --> 00:19:42,960
beams within its lattice as depicted by this photoshopped image of a much larger lattice.

257
00:19:42,960 --> 00:19:48,240
Our computational tool models the tunable beams as linear beams, which are depicted

258
00:19:48,240 --> 00:19:53,160
as blue lines, and their lengths are set to be the length of the beams in our fabricated

259
00:19:53,160 --> 00:19:57,360
mechanical neural network, i.e. 6 inches from node to node.

260
00:19:57,360 --> 00:20:02,240
We restricted each beam in our simulation to only achieve axial stiffness values between

261
00:20:02,240 --> 00:20:07,920
the maximum and minimum stiffness values measured from our fabricated beam, i.e. 2.3 Newtons

262
00:20:07,920 --> 00:20:12,440
per millimeter and negative 2 Newtons per millimeter respectively, and we set their passive

263
00:20:12,440 --> 00:20:18,280
non-axial stiffness values equal to the values calculated using finite element analysis as

264
00:20:18,280 --> 00:20:19,280
shown here.

265
00:20:19,280 --> 00:20:24,400
We also restricted the simulated beams from extending or contracting more than plus and

266
00:20:24,400 --> 00:20:29,280
minus 2.5 millimeters, which is the limit of our fabricated beams as governed by their

267
00:20:29,280 --> 00:20:30,640
hard stops.

268
00:20:30,640 --> 00:20:35,760
Finite element analysis was used to validate the computational tool's accuracy by loading

269
00:20:35,760 --> 00:20:41,080
a 21-beam version of the design in its passive state, i.e. without any closed loop stiffness

270
00:20:41,080 --> 00:20:47,120
control activated, with 25 random force combinations imparted on its two input nodes.

271
00:20:47,120 --> 00:20:52,560
The X and Y components of the lattice's resulting output node displacements, calculated using

272
00:20:52,560 --> 00:20:59,000
both finite element analysis and our computational tool, are plotted here showing good correspondence

273
00:20:59,000 --> 00:21:05,240
between each of the 25 force combinations, once configured to mimic our fabricated mechanical

274
00:21:05,240 --> 00:21:06,240
neural network.

275
00:21:06,240 --> 00:21:11,260
The computational tool was then used to simulate the effect that the number of layers would

276
00:21:11,260 --> 00:21:16,920
have on the ability for a triangularly configured mechanical neural network consisting of eight

277
00:21:16,920 --> 00:21:22,440
input and output nodes to learn different numbers of random shape morphing behaviors.

278
00:21:22,440 --> 00:21:27,360
The results of the study indicate, one, that mechanical learning improves with more layers,

279
00:21:27,360 --> 00:21:31,920
likely because there are more tunable beams with which to learn, and two, the more random

280
00:21:31,920 --> 00:21:36,420
behaviors that are required to be learned, the less accurately all the behaviors can

281
00:21:36,420 --> 00:21:38,440
be learned simultaneously.

282
00:21:38,440 --> 00:21:43,280
This plot was similarly generated, but for only two, four, and eight layers, and for

283
00:21:43,280 --> 00:21:48,360
both triangular and square lattice configurations, shown green and red respectively.

284
00:21:48,360 --> 00:21:53,680
It is clear from these results that triangular lattice configurations can, in general, learn

285
00:21:53,680 --> 00:21:59,120
different numbers of shape morphing behaviors more effectively than square lattice configurations.

286
00:21:59,120 --> 00:22:04,240
The reason is likely because triangular lattices have more beams for the same number of layers,

287
00:22:04,240 --> 00:22:09,200
and they can propagate displacements in all directions, rather than just along orthogonal

288
00:22:09,200 --> 00:22:12,240
directions, as is the case with square lattices.

289
00:22:12,240 --> 00:22:17,200
To learn the effect that the number of layers and output nodes have on mechanical learning,

290
00:22:17,200 --> 00:22:22,680
we used our computational tool to generate the following plot for triangular lattices

291
00:22:22,680 --> 00:22:26,280
that learn the two sinusoidal behaviors described previously.

292
00:22:26,280 --> 00:22:31,480
The plot indicates that once the lattice possesses two or more layers, the number of output nodes

293
00:22:31,480 --> 00:22:33,120
does not seem to matter.

294
00:22:33,120 --> 00:22:38,040
It's true that the more output nodes a lattice has, the more output node displacement requirements

295
00:22:38,040 --> 00:22:42,880
the output nodes must satisfy, but it's also true that the more output nodes a lattice has,

296
00:22:42,880 --> 00:22:47,760
the more beams the lattice can employ to satisfy those requirements during learning, so both

297
00:22:47,760 --> 00:22:49,760
effects seem to negate each other.

298
00:22:49,760 --> 00:22:54,040
If you'd like to learn more about the details presented in this video, I encourage you to

299
00:22:54,040 --> 00:22:58,320
read our first published journal article on the topic of mechanical neural networks

300
00:22:58,320 --> 00:23:02,760
in Science Robotics, where our work was featured on the journal's front cover.

301
00:23:02,760 --> 00:23:07,680
A link to the paper is provided in the description below, along with a link to my Thingiverse

302
00:23:07,680 --> 00:23:12,360
account where you can download the part files necessary to fabricate our mechanical neural

303
00:23:12,360 --> 00:23:13,360
network.

304
00:23:13,360 --> 00:23:18,000
Finally, I want to thank my students Ryan Lee, who built and tested the mechanical neural

305
00:23:18,000 --> 00:23:24,160
network, Erwin Mulder, who developed our computational tool, P. H. R. Sainaghi, who helped perform

306
00:23:24,160 --> 00:23:28,960
the optimization algorithm comparison study, and all the other students who contributed

307
00:23:28,960 --> 00:23:31,960
in smaller ways to the success of this project.

308
00:23:31,960 --> 00:23:37,680
I am especially grateful to my AFOSR program manager, Les Lee, for making this research

309
00:23:37,680 --> 00:23:42,160
possible through his continued funding and generous support of my group.

310
00:23:42,160 --> 00:23:46,240
If you'd like to support my channel, I've provided instructions in the description

311
00:23:46,240 --> 00:23:47,240
below.

312
00:23:47,240 --> 00:23:50,000
Thanks for watching the Facts of Mechanical Design.

