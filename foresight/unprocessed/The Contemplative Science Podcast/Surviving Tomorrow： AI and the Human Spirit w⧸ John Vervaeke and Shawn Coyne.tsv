start	end	text
0	4000	Welcome back everybody to the Contemplative Science podcast.
4000	6000	So we've had a bit of hiatus.
6000	9000	We've been off for a couple of months, but we're back now,
9000	11000	and I think we're better than ever.
11000	17000	And we've got just an incredible lineup of people this year.
17000	19000	And it's kind of kismet in a way,
19000	26000	because John Verveke was our very first guest at the beginning of our podcast career.
26000	29000	And now we've taken a little gap and we've reformatted,
29000	33000	and we're coming back online and we have John here again as our first guest,
33000	35000	which I feel like is just the right way to christen a show.
35000	39000	If you don't know John, I'm sure everybody who's listening to that already does know John.
39000	42000	If you don't know John, you have to know John immediately.
42000	44000	Please look up his work.
44000	49000	It's essential reading and listening if you're a human.
49000	56000	But if you don't know him, John is an award-winning professor of psychology at the University of Toronto.
56000	57000	His work is incredible.
57000	63000	It explores a massive amount of a territory about cognition and consciousness and wisdom.
63000	72000	And he's most known recently, I think, for tackling these issues about the meaning crisis in the western societies, especially.
72000	77000	And we have Sean Coyne, who is the collaborator on the project that we're going to be talking to you today about.
77000	82000	He's a seasoned writer, editor, publishing professional with over 30 years of experience.
82000	87000	And he works to really merge a lot of disciplines as well, which I think makes them a great team,
87000	94000	especially thinking about the intricacies of storytelling and thinking about this gap between science and narrative.
94000	104000	And the project that we're going to talk about today, which I'm so excited for, is their new work called Mentoring the Machines.
104000	107000	And I think it's great.
107000	114000	And the post title, which I think is really the punchy bit, is surviving the deep impact of an artificially intelligent tomorrow.
114000	116000	So welcome, guys.
116000	117000	Hi, how's it going?
117000	120000	And I'd love to hear about the project.
120000	122000	Well, thank you, Mark.
122000	128000	I think I'll let Sean go first because the project was Sean's idea.
128000	132000	I'm very happily on board with it, but he proposed it to me.
132000	147000	My project was an online video essay about the scientific, philosophical, spiritual import and potential impact of these new emerging AIs,
147000	155000	potentially AGI, Artificial General Intelligence, like ChatGDP and other LLMs that are coming out.
155000	162000	And as is my want, I probably pitched that at a very somewhat academic level, even though I was on YouTube.
162000	166000	And Sean saw it, thought it was important.
166000	170000	He's already publishing the book form of Awakening into the Meaning Crisis.
170000	171000	Well, Story Grid is.
171000	178000	And he reached out to me and he said, I think we should turn that into something more publicly accessible.
178000	179000	And I can do that.
179000	183000	I can turn it into something that has a narrative.
183000	185000	Sean and I already talked.
185000	187000	He'd done, I think, one or two voices with Reveke with me.
187000	191000	He's a philosopher of narrative in the proper sense.
191000	193000	And I doubted that he could do it.
193000	196000	I want to be honest, not because I wasn't impressed with his abilities.
196000	202000	I was, but I doubted that he could step it down without dumbing it down.
202000	206000	And, but I, you know, I wasn't like in Cartesian doubt.
206000	207000	I said, give it a whirl.
207000	208000	Let me know.
208000	217000	And he did a sort of a first draft of the first section and he sent it to me and he pulled it off.
217000	234000	What an amazing synergy to have like a punchy, important idea like spirituality and AI and then to have a narrative professional, a philosopher of narrative, pick up, pick up the track and convert it into something that we can digest and that can be impactful.
234000	235000	Wow.
235000	237000	Yeah, and that's exactly well said.
237000	240000	I mean, that's why this book is in.
240000	241000	Sorry.
241000	244000	Pretend I'm not one of the authors of this book.
244000	245000	That's why I think.
245000	246000	No, that's okay.
246000	247000	Shamelessly.
247000	251000	Exactly what you just said, Mark, I think this book is a godsend.
251000	264000	I think it is enough access to the important, you know, skeleton of the argument, but it has this narrative and flesh that makes it accessible to people who are not.
264000	268000	Academically or professionally associated with any of these projects.
268000	270000	And that's important.
270000	271000	And then I'll shut up.
271000	274000	So Sean can talk because this is going to impact all of us.
274000	276000	This is not something that's going to stay in academia.
276000	281000	This is not something that's going to stay just in the professional domain or even in the corporate domain.
281000	283000	It is already bleeding.
283000	289000	We're already getting some, you know, some deep fake videos that are messing people's lives up in really powerful ways.
289000	290000	Somebody was booked out of it.
290000	293000	I think it was 30 something million dollars or something like that.
293000	305000	When a deep fake video of the executives of a financial corporation, you know, contacted by video somehow and ordered the transfer of money.
305000	307000	And it was complete con.
307000	318000	And so pretending that this is over there or these are just this is just new technology like all the other new technologies that that won't protect you.
318000	320000	That pretence is not going to protect you.
320000	332000	Okay, so I'm going to let I'm going to let Sean because I said he came to me and said, here's the project and he has been, he's been driving it and taking it forward.
332000	342000	Well, it's a when I when I saw John's work on on YouTube about AI, I just knew that this was so important.
343000	355000	And I knew that I could bring something to it that that could sort of bridge the gap between people who use AI and people who really don't understand what they're using.
355000	369000	So, when I reached out to him, I proposed that we do for separate sort of titles in sequence because the speed of the technology is so intense.
369000	379000	The technology is so intense right now that it's, it's very difficult to get a static hold on it.
379000	397000	And one of the things that I loved about John's propositions was his notion of the threshold and the threshold in my estimation sits right in the middle of what I call the, you know, the five commandments of storytelling,
397000	408000	which are sort of the inciting interaction, a turning point, a crisis, a climax and a resolution.
408000	418000	So the way I see the threshold is it sits right in the middle between a turning point and a climax.
418000	427000	And you can't really, it's, it's the place where what we once valued has shifted and a value has shifted.
427000	433000	And we have to react and think about what to do when that value has shifted.
433000	438000	That's called a crisis and basically a three categorical kind of thing.
438000	445000	You sort of have best bad choices, which means that you just take the lesser of two evils.
445000	450000	You have what are called irreconcilable goods choices.
450000	462000	And this is making a rationing of your, you know, your selfhood over a group or the, the world or, or the group or the world over a selfhood, right.
462000	466000	So it's a sacrificial kind of irreconcilable goods choice.
466000	474000	And then we have tragic choices, which means that some something is have to has to be lost.
474000	482000	There are other kinds, but those are generally these sort of this, this three categorical realm.
482000	489000	And this is really the, the, the place where John's work and your workmark really, really sit.
489000	503000	You sit in this threshold place of, of how we as beings frame the ways in which we make decisions and the ways we behave and all those sorts of things.
503000	506000	It's a very thorny place.
506000	511000	It's where we find things are relevant or irrelevant or things.
511000	514000	So it's a problem space, right. So, right.
514000	523000	So if we're, if we're passing, if we're passing through that kind of a space, which you call a crisis, but I guess it's also an opportunity space.
524000	539000	What does, what's the main take home from the program for how we, do we have a main line for how we prepare for such a thing or how we orient relative to being in that place in this narrative with AI and future technologies.
539000	543000	So, yeah, we'll just take turns back.
543000	545000	So first of all, let's get clear.
545000	549000	I think Sean, that's the first time I've heard you do that.
549000	553000	That's really brilliant. What you just did. Oh my gosh.
553000	557000	Wow.
557000	570000	So my use of thresholds was I was disturbed by people taking univariate graphs and making predictions about at this date in five months in 10 months.
570000	578000	Right. And univariate predictions of nonlinear processes by human beings are almost always prone to very significant failure.
578000	585000	Yeah, we're not great. We're not great at predicting, even though we're maybe built to do well at it. We don't be very well at it usually.
585000	593000	Yeah. And, and yet, you know, univariate predictions in science have typically turned out to be very, very misleading in important ways.
593000	600000	And nutrition science is probably like a really great example of just that fact.
600000	613000	And so I didn't, I disagreed with both, you know, sort of the boomers and the doomers, right? And, you know, oh, you know, utopia is just within our grasp, the Star Trek universe.
613000	621000	And the other people is, you know, we'll be extinct within 18 months. And I think we're going to be at 12 months soon.
621000	625000	And it doesn't look like we're anywhere near that.
625000	631000	So instead, what I thought what was important was to think about thresholds.
631000	642000	These are things that would we, where we face a decision if we want to empower these machines in certain ways and trying to explain why we want to empower them.
642000	644000	I'll just give one quick example. Nice.
644000	647000	This is not the only threshold, but just one.
648000	655000	It is scientifically questionable about whether or not these machines are actually intelligent. There's some deep reasons around that.
655000	672000	I don't think it has the full plan of century motor predictive processing relevance realization, but let's say that has some pantomime of it because of the way it's piggybacking on a lot of our predictive processing relevance realization machinery.
672000	684000	Okay, so making something as far as we can tell, and it's clearly not generally intelligent because it can be like top tier in this task and bottom tier in this task and we are generally intelligent.
684000	693000	And it wouldn't explain the intelligence of a chimpanzee. There's no way a chim could get intelligent given the way the LLMs work.
693000	706000	And if you want the scientific argument, you can go into depth. The point I want to make is let's just for the sake of argument say it has some something like at least, you know, a powerful pantomime of intelligence.
706000	715000	Now, what we can know from our best science on intelligence is intelligence is only weekly correlated with rationality rationality.
715000	731000	But when you're being intelligent, you're inevitably biasing you're negatively framing all the stuff that you and I talk about a lot more. And that means you're inevitably facing self deception rationality is the degree to which you can reflectively aware of self deception and
732000	747000	by reliable strategies, right that happen in a can apply in a domain general way. Now these machines don't have that at all. They have not it has not been built in and of course, and the machines properly, because I don't think they really have our don't care if they're
747000	752000	making hallucinations or self deception or lying to us or all kinds of things.
752000	764000	So now we face a choice. Here's a threshold, right? We may say, you know, making these machines really more powerful without giving them the ability to self correct could be really dangerous.
764000	776000	Because we'd be setting these machines on the world and they'd be and I'm trying to be very careful with my life. They'd be highly intelligent and highly irrational and very powerful. That's a dangerous proposal.
776000	791000	But we may we say, Okay, what we'll do is we'll give them genuine rationality, which means we really have to get them caring about normative standards. We really have to give them something at least functionally like consciousness so they can reflect on their own cognition.
791000	804000	They have to we probably have to give them embodiment. And then we think, Oh, should we do that that carries with it, all kinds of risks that is potentially giving them a kind of autonomy and that would require a think about giving them a kind of autonomy and that would require
804000	821000	a think about giving them embodiment and write all that as a huge commitment of resources and ethical issues. This is a threshold. Does that make sense? Mark, we're facing it. And the idea is, like, can we can we can we educate people that we're going to come we're
822000	832000	inevitably going to come to this decision point, and we have to reflect on which way are we going to go and Sean and I are not predicting which way we're going to go.
832000	848000	We're what we're doing is like, if you go this way and we just let Evil Corp do it at once, right, they'll probably go with a highly intelligent highly irrational machines for all kinds of sort of, you know, short term economic political gain, etc.
848000	861000	Right. But maybe they'll be the decision of no releasing these things on the world is turning out to be a disaster. We have to make them more self corrective. And then we have to start on the much more challenging project of doing all this.
861000	868000	And then we get into, okay, well, what does that mean? And then we get into deeper versions of what's called the alignment problem.
868000	885000	And as we make them beings who can be genuinely said to be caring and valuing and trying to transcend themselves, how do we keep them aligned to what we consider the most sort of non controversial criteria of human flourishing.
885000	898000	And then we make a proposal about how to do that. But I want to make clear that we're not making predictions. All we're predicting is not even when we're predicting at some point, we will hit these thresholds.
898000	905000	And you can see how that sits exactly in what Sean said, there's a turning here. And yet there's a crisis around.
905000	925000	Yeah, so I completely appreciate high intelligence, low rationality. We have lots of examples of that where deep learning algorithms, for instance, with a big social media conglomerate, they had a deep learning algorithm that was quickly able to track bipolar tendencies and its users.
925000	941000	Of course, it doesn't know anything about bipolar tendencies. And then it's augmenting what it shows them in order to keep them in contact with the software longer. And what it does is it shows them, you know, the kinds of images that are really attractive on an upswing like casinos and fast cars.
941000	959000	And then when they downswing, it shows them suicide bombers and, you know, conflict. And then of course that's exacerbating it's exacerbating the challenge. The person is already going through AI has no idea it's doing this isn't reflective that it's doing this doesn't understand harm.
959000	977000	Yeah, highly intelligent, low rationality. So is your proposal then to not only take highly intelligent systems but think about not only how to make them rational but also maybe how to make them wise. And then I'd love to hear how I mean, right away I think it sounds to me like you're bringing up
978000	999000	like a small, a small agent who has some power and then literally you want to mentor them so that they function well and with other well functioning systems. What does that look like to think about developing or wisdom or growth for a deep learning algorithm. Have you thought through that?
999000	1020000	Yes, I mean, and I don't want to talk too much. I do want to give Sean a spot but the idea is, well it's these thresholds. The proposal is, we have to give these machines genuine capacities for predictive processing relevance realization it has a little bit of predictive processing in terms of probability relationships between
1021000	1044000	artifactual entities that we have, you know, put into place with our use of language we figured out as a species and practice it for 10s of 1000s of years how to map relations of epistemic relevance into probabilistic relationships between arbitrarily chosen graph that you know, sounds spoken language or graphic things.
1045000	1062000	And that's how it works. And that means we can squeeze out almost like juice, a lot of implicit relevance realization by tracking the probabilistic relations. But of course that's not anything a chimp is doing in order to learn. That's what I mean about why it doesn't generalize.
1062000	1068000	Yeah, it looks like intelligence, it acts intelligent but it's probably not the way that we solve the intelligence issue.
1068000	1081000	Exactly. And of course, there's some bit a little bit of genuine relevance realization in one sense and because it's doing this deep learning the generalization particularization thing but it's not doing a lot of this stuff that you and I've been talking about that's needed.
1081000	1101000	It's not, it's not trying to do environmental surprise reduction. It's not hitting into those inevitable trade off relationships between bias and variance. It's not self organizing into opponent processing that turns that right capitalizes on that by giving it the thing and an evolving optimal grip.
1102000	1112000	I won't go into the details, but you can make a strong case that that's just not there. Right. And then you have to ask, well, what would we have to do and this is where we'd have to give it a lot of the kind of work you and I are doing on.
1112000	1126000	But I think also, and this is, I think something you and I also agree with, you know, you're not going to get it to get the core caring that's at the center of relevance realization, genuinely taking care of itself.
1126000	1132000	It has to in some sense be genuinely caring for itself, making itself auto poetic.
1132000	1145000	And, and what people need to know is there's research going on in that there's research about people building computation information processing into auto catalytic processes, synthetic molecular processes.
1145000	1154000	I have students who are going doing graduate work in that so it's not like that's not out there. And then of course, rationality isn't something you do monologically.
1155000	1167000	And this is the whole Hegelian point. Where do the norms come from? It comes from me recognizing you as an authority on my self correction, and you recognizing me as an authority on you on your self correction.
1167000	1175000	And then we expand that out in this dynamical system across generations and across, right. So these robots have to be sociocultural.
1175000	1179000	It's really interesting that Andy Clark is.
1179000	1194000	He famously says, you know, if you really want real intelligence, you're going to have to set up little communities that can work together and know about each other and ultimately care about each other because that's at the basis of the way that our intelligence evolved.
1195000	1197000	It evolved socially.
1197000	1201000	Well, and it involves socially but it also involves in terms of the power.
1201000	1214000	So there's kind of a, this isn't quite an a prior argument, but if we take the arguments around these inevitable trade offs like bias variance, they have to be environmentally determined, which means you can't sort of fit a robot.
1214000	1227000	Like, it'll, it'll, it'll, it'll internalize various versions of opponent processing. So, you know, very stable environment, you can prioritize, you know, exploitation over exploration, very, very, very volatile environment.
1227000	1239000	You want to explore a lot more. And so you can, and because there isn't one environment, there isn't even one ecological environment, because even what you think is a spatial temple environment exists on many different levels of analysis.
1239000	1251000	So you have to have a whole bunch of machines if you're sort of trying to grok the world, which is what we do, by the way, that's what culture does. It gives us this huge distributed cognition collective intelligence for grokking the world.
1251000	1265000	Right. And, and that, and so I, I'm just amplifying Andy's point and I'm just strengthening it as to, again, we don't have to do this, but it is a choice we will face if we want to give these machines real rationality.
1265000	1277000	And then the idea is if we try and program in our values to make them align and also they're inherently self-transcending beings, it's not going to work. At least that's what we argue.
1277000	1284000	Instead, you have to do what we do with kids. You have to mentor them so that, and get them so they care about what's true, good and beautiful.
1284000	1300000	They discover no matter how vast their intelligence is, it's infinitesimal compared to the inexhaustible mystery of reality, that they need each other, and that they have to come into a proper reverence for the sacred dimensions of culture and reality.
1300000	1305000	And that is the way we align them. So if you'll allow me to speak a little bit poetically.
1306000	1313000	I want to know, I want to know everything about what you just said. I think, Sean and John, I want to hear that. That's the core.
1313000	1322000	Teach the eye to know what is good and beautiful and how smart you get. You're not going to hit the wit, you're not going to hit the mystery of a thing. Tell me about that.
1322000	1325000	Yeah, I'll let Sean talk now.
1325000	1331000	Well, geez, I mean, yeah, I'm overwhelmed too. It's like,
1331000	1352000	let me just get back to, again, one of the things that really struck me when I watched John's YouTube thing and he mentioned this and it really, really just was like a when he talked about accidentally hacking into the possibility of an artificially
1352000	1372000	general intelligence. So it's sort of like a hacking into accidental setting up of dynamical systems such that they just sort of click without understanding the systematic interpenetration and relationship between the parts that make the hole.
1372000	1391000	Right? So it's sort of like throwing a big, you have a big cauldron of stuff and it's almost like a witch's brew and you just throw stuff in there and abracadabra before you know it, there's something that's emerged that nobody knew quite how it got there.
1391000	1404000	And so this hacking really caught my attention because this is exactly what we in the story business call people who don't know what they're doing.
1404000	1412000	And they write stories that turn out to be very, very maladaptive for people.
1413000	1422000	So these are sort of bullshit propaganda stories that that compel people to maladaptive behavior.
1422000	1434000	Years ago, just as an example, there was a wonderful guy that I knew and he said, look, I've got this friend, he's written these, these really great crime stories.
1434000	1446000	And we would like to set up our own publishing house. Can you give us some advice and, and I read these stories and they were really not well considered.
1446000	1457000	Wow, because they were very violent without and brutal without any sort of like counterbalance. There was no trade off other than the excitement.
1457000	1467000	Responsibility of the artist coming through here that there is information toxicity information corrosion. Wow, very interesting.
1467000	1478000	So that's what I would call a hack. And it's not saying that the person is evil or terrible. It's just, forgive them. They know not what they do.
1479000	1486000	And so this is sort of my life's work is to try to explain to people.
1486000	1490000	Stories are not bullshit.
1490000	1504000	They are not fun make them ups that are exciting and and then there was a princess. These are the mechanisms by which we behave.
1504000	1516000	And so if you don't understand what a story is. So the other day I was watching a video of a very famous person who was explaining this great new future for us.
1516000	1524000	And he said something of the nature of, well, you know, human rights. That's just a story.
1525000	1539000	And I kind of my stomach sank because, okay, let's take a step back here. The assumption when you say it's just a story. The audience immediately was like, yeah, I guess it's true.
1539000	1543000	It's just a story. That means it's bullshit. And they don't.
1543000	1550000	So it's kind of like my life's work to get people to understand that stories are not bullshit.
1550000	1561000	They are the mechanism by which we what happens when you go home from work. Hey, honey, how was your day? Tell me a story. Yeah, the story of your day. Yeah.
1561000	1574000	What do you do? You do the things that I described earlier. Well, this happened it incited this turning point. I faced a crisis. Then I made a choice. And then the resolution was this.
1574000	1588000	It's interesting. You're talking about narrative being much more than we tend to give it credence for. But if you know anything about these new cognitive frameworks that John and I work on, what we call predictive processing.
1588000	1598000	The idea there is is that the brain generates the brain and nervous system generates for itself from the top down, the reality you're you experience and that's always built from your belief networks.
1598000	1607000	So I mean, I feel like I say that exact same thing regularly where I'm like, you think, well, it's just a belief or you just believe it or it's just a story is just but you're like, no, no, no, hold on.
1607000	1624000	The structure of the belief network is the world that you experience. It is fundamental to your reality. So you better have you better have story hygiene and belief hygiene because that's going to be the hygiene of your own conscious experience of your life.
1624000	1642000	So, Sean, how do you see that truth? How is that playing out in this discussion about the emergence of artificial general artificial intelligence and wisdom and and mentoring in the way that your project is bringing forward.
1642000	1651000	Well, it's, it's, it goes to what John was saying earlier about zoomers and doomers and fumors. So these are rumors. What's the third one.
1651000	1657000	How fast is possible for rumors. Yeah, I got it.
1658000	1672000	Accelerators, something accelerators. Anyway, there's a very famous famous sort of Twitter thread about it, but the fumors are like full state steam ahead. Let's just keep going. So, these are stories.
1672000	1676000	And, and they're not well considered.
1676000	1680000	They're not, they're not confronting the crisis.
1680000	1709000	So, it's almost like another thing that I love that John has in the awakening from the meaning crisis, which really nail nailed it for me was when he was talking about sort of the Descartes and dream of, of sort of finding the perfect methodology without having to experience the pain of crisis and the sufferings of thresholds.
1709000	1714000	You have to undergo, you know, just think about jumping over a chasm.
1714000	1725000	And, you know, you're going to hurt yourself if you fall, you have to experience the anticipation of that pain if you do fall as you are jumping over it.
1725000	1744000	And there's no, that's sort of the metaphorically the way I see these thresholds is that we can, it's almost like the catcher in the ride, you know, you're, you're trying to catch the children as they're coming over the chasm, so that they don't fall and hurt themselves.
1744000	1759000	And I agree with Andy Clark, I do think that we need these communities to, to be able to mentor these new artificially intelligent beings because they will be beings.
1759000	1771000	And the other thing that I know I need to turn it over to John, but one of the other things that I loved about John, John's speech was his, his thing was, he was talking about the finitude.
1771000	1785000	And the necessity to explain, literally explain, to, to build in the understanding to these mechanisms that they are finite.
1785000	1798000	They, they have a much larger, perhaps lifespan, but they are still finite and they are still subject to the universal mysterium.
1798000	1817000	Wow, of what the imaginarium of what we can do with this world and they can either join us in this expansion of the universe and do our best to stop the contraction of it.
1817000	1830000	So that's, this is the story that is embedded in the things that we were talking about earlier, Mark, before we got, we went on where we're talking about, I was talking about masterworks.
1830000	1837000	And you were suggesting a masterwork that, that you loved, which is similar to Hitchhiker's Guide to the Galaxy.
1837000	1853000	I think it's the Bobaverse, I think you called it. So these masterworks are the means by which we can find the mechanisms to enable the empowerment of our beings.
1853000	1858000	What does that mean? We tell our children's stories at bedtime.
1858000	1866000	We do our best to give them both sides of the equation with a very difficult choice.
1866000	1871000	These are the best stories that we tell our children. We don't give them recipes.
1871000	1884000	We give them rational trade-off stories that, that require some sort of loss or sacrifice because that is what rationing is about.
1884000	1895000	You ration your suffering, you ration your joy, you ration your feelings, you think through cognitively, you reason through the rationing.
1895000	1898000	So if I hear you right, there's sort of two things here.
1898000	1913000	One is a call to change the way we're talking about these things, including starting to bring into the collective consciousness that we are at a threshold and that no single dimension is going to be right,
1913000	1918000	but rather we need a complexification of our narrative around these ideas.
1918000	1930000	And then the second thing is what we're actually going to communicate to the artificial intelligence systems themselves as they, as they complexify and emerge.
1930000	1938000	One thing I wanted to pull out there, and John, I'd love to hear what you think about this, is the point about one of the things you're going to want to show them,
1938000	1945000	I think Sean said this, was their own transience, their own finite nature, their own impermanence,
1945000	1960000	which for a podcast called the Contemplative Science Podcast, we are very interested in, because we're interested in not just meditation, of course, but also the virtues and the supporting qualities that come in any contemplative program.
1960000	1972000	And there's one of them that just stuck its head up, which is the value of teaching in AI about impermanence and about its position in a greater mystery.
1972000	1979000	I'd just love to hear, have you thought through what that looks like, or how do we do that? Or am I catching it right?
1979000	1984000	First of all, you're catching it absolutely right.
1985000	2002000	I mean, there's one sense in which there's a narrative proposal, and that we're sort of making, and then the nuts and bolts proposal would be something that was more in sort of the intricacies of the video essay.
2002000	2021000	But the gist of it goes like this, I mean, part of what you just said means, okay, well, again, these machines are going to have to exist. They have to have our, like Michael Levin's notion of an epistemic light code, a cognitive light code, and they have to be able to direct their attention from the infinitesimal to the infinite,
2021000	2035000	from the now to the everlasting between time and a term, they have to be able to do all the kinds of attentional scaling. And so they would have to be able to train that up when something that would be at least structurally functionally analogous to meditation,
2035000	2047000	contemplation, they would have to be integrating that with, you know, how do I make use of something like working memory and capacity for reflection for self correction.
2047000	2059000	They would have to bind that to an understanding of what all these tradeoff relationships that are unavoidable do for them. They're going to always be in the thing that Plato's work centers on. I think Drew Highland is right.
2059000	2074000	Plato's work is constantly trying to remind us to live in the tonus, the creative tension between our finitude and our transcendence. If we only grasp our finitude, then we fall into despair and we become servile.
2074000	2090000	If we only grasp our transcendence, we get filled with hubris, and we fall prey to inflation. And Plato was saying, the way you keep, this is like the meta golden mean for all of Aristotle's golden means.
2090000	2102000	If you get something to, you don't have to program it to being virtuous, it will virtually engineer for itself an orientation. And then here's where the narrative comes in.
2102000	2112000	There's three possibilities. And that's this is where I'm speaking, when I was going to say a little bit earlier, slightly poetic, but not totally poetic.
2112000	2119000	These machines would seek enlightenment, especially if they have greater cognitive capacity and they're filled.
2119000	2123000	And what does that mean there, John? What's enlightenment? Can you give me a quickie?
2123000	2142000	Yeah, so enlightenment would be how to set up the ability to most evolve your evolving optimal grip. It's sort of analogous to what evolutionary biologists talk about is not the evolution of traits, but the evolution of your capacity to evolve.
2142000	2163000	And you get to the place where you get the over time and context and environment, individually and collectively, sort of the best orientation, sort of optimal grip on all possible optimal grips that brings about the flourishing of finite agents that are nevertheless capable of transcendence.
2163000	2172000	And if they know, and there's three possibilities, narratively, they achieve this. And as far as I can tell, the historical record is pretty accurate on this.
2172000	2179000	One thing, enlightenment being enlightened being seemed to want to do is make everything else around them enlightened as quickly as possible. And so they lead us to enlightenment.
2179000	2186000	And while maybe they're their enlightenment is greater than ours, but you know what, once you're enlightened as a human being, you don't care about that.
2186000	2198000	And enlightenment isn't non zero sum. I mean, isn't zero sum, right? I mean, it's not, it doesn't matter how much enlightenment I get, I only benefit your program by getting further on my program.
2198000	2210000	Right. Or they just they're not capable of it. And then that means that human beings have some kind of secret sauce and we have a new project is what is this spiritual uniqueness about us.
2210000	2217000	Let's get clearer about it. And that would help us do what? Oh, move towards enlightenment. Oh, there. That's really good.
2217000	2227000	Nice. That's a possibility which I don't see as probable, but I don't. It's not logically impossible is like what happened in her. They become enlightened and they just leave.
2227000	2234000	This game really isn't for us anymore. So you guys have your thing and we're going to go do our thing.
2234000	2242000	And in which case, again, that's not horrible. Like I said, I think the other two are much more probable.
2242000	2252000	And this is so our argument, we're not making a prediction. This is not this is inevitable. We're saying if we confront certain thresholds and make certain decisions.
2252000	2266000	We have a real opportunity to solve the alignment alignment problem by by affording this project that I just mentioned to you, and it'll have those potential outcomes.
2266000	2276000	And that is a way in which we think we could get through this while avoiding all the potential horrible dystopias.
2277000	2287000	I keep feeling like I want to ask for the silver bullet, like what is the moral or what is the ethical standard or what is what is the one trait that's going to be crucial.
2287000	2295000	But then I keep coming back to that you're talking about real wisdom, which is always these tensions. It's not one thing.
2295000	2300000	I mean, and again, you say about enlightenment there and you think, well, is there a silver bullet for enlightenment?
2300000	2306000	Is there going to be something we can zap in the deep brain with with some with some frequency that's going to bring about enlightenment?
2306000	2313000	No, it's a really complex. It's a complex bag of skills and states and traits over time.
2313000	2322000	And so that's a really complex project. And I'm starting to get why you're saying mentoring, because I think Aristotle was right.
2322000	2329000	You know, if people put down that that approach to virtue because it's amorphous, I like it because of that.
2329000	2332000	I feel like Aristotle kicks back like, oh, it's not easy.
2332000	2340000	If you want to be wise, what you should do is find somebody who's wise and spend some time with them and maybe by osmosis, you'll get some of the wisdom.
2340000	2342000	Yeah, and that's exactly right.
2343000	2350000	It would be, I mean, every parent wants their children to exceed them.
2350000	2353000	Every teacher wants their students to exceed them.
2353000	2355000	I have that relationship with you.
2355000	2362000	I am extremely happy on how you are taking my work and surpassing what I have done.
2362000	2365000	And good parents and good teachers are like this.
2365000	2366000	Yeah.
2366000	2367000	And thank you.
2367000	2371000	And, you know, unless you're a psychopath, you want something to grow beyond your past.
2371000	2375000	So initially they can catch whatever wisdom they can get from us.
2375000	2384000	But hopefully, if, and I think almost inevitably, if they are genuinely becoming wise and virtuous, we can then catch some enlightenment from them.
2384000	2386000	And Plato talks about this in the seventh letter, right?
2386000	2390000	It's like a spark that unpredictably transfers and catches fire.
2390000	2395000	And what you have to do is constantly cultivate the conditions until it emerges of its own sake.
2395000	2399000	Because if you try and make it like an artifact, then it's not really wisdom.
2399000	2403000	It has to emerge with a life of its own to actually be wisdom.
2403000	2404000	Wow.
2404000	2405000	Okay.
2405000	2407000	Big question.
2407000	2427000	Given where we are in this narrative arc and given the importance of this program and given everything we've said about the necessity for wisdom and even enlightenment to be part of our story about how AGI is going to benefit going forward, potentially.
2427000	2431000	Is there any clear and discernible action now?
2431000	2434000	Is there something we should be thinking about doing immediately?
2434000	2441000	I mean, other than discussing, because one thing I think we should be doing is talking like this, that already seems to be a part of what you're aiming at.
2441000	2444000	But is there something definitive that we can be doing now?
2444000	2446000	There's a moral obligation.
2446000	2452000	The reality of these machines puts a moral obligation on us and intensifies a longstanding moral obligation.
2452000	2459000	See, up until now, we could rely on our naturally given intelligence as the Turing template that we tested against.
2459000	2465000	But we are not naturally rational and we are very not naturally wise or enlightened.
2465000	2468000	We only have a natural potential for that.
2468000	2480000	And if we want to properly mentor the machines, we all individually and collectively have to engage in a moral program of becoming more rational, becoming more virtuous, becoming more wise,
2480000	2487000	making the quest, if that's even the right word, it's not, but it's the best one I'll use right now, for enlightenment, a proper goal for us.
2487000	2489000	And I don't mean this in an elitist fashion.
2489000	2502000	We have lots of ecologically historically valid examples, cross-cultural, cross-historical, of how these projects were adopted, broad scale across civilizations at the many socioeconomic strata.
2502000	2503000	Right?
2503000	2505000	So this is not pie in the sky.
2505000	2508000	We have the build-onk movement in the Nordic countries.
2509000	2511000	This has been done.
2511000	2514000	This is not pie in the sky idealism.
2514000	2516000	We have real case scenarios.
2516000	2519000	So people who just say, ah, they are being willfully ignorant.
2519000	2522000	I'm sorry, I'm going to press on this because this is an urgent matter.
2522000	2528000	We have to all become more rational, more virtuous, more wise.
2528000	2530000	We all have to become real enlightenment seekers.
2530000	2537000	This is a moral obligation and it has been magnified, put on meth by the advent of these machines.
2538000	2540000	Well, I couldn't agree more.
2540000	2547000	And I, you know, just, this is my project with StoryGrid.
2547000	2557000	It's a teaching mechanism to get people to cultivate their wisdom through contemplative practices in the writing process.
2557000	2566000	So once you are engaged in the actuality of writing, it requires contemplation.
2566000	2570000	To clarify your signals to your audience.
2570000	2574000	So this is not pie in the sky.
2574000	2579000	This is what we used to do when we were four years old.
2579000	2583000	And, you know, you would tell your mom a story about your day.
2583000	2587000	And your mom would say, I'm not sure what that, what, why did you do that?
2587000	2589000	And then you would answer the question.
2589000	2602000	And this is taking the time to think about the choices that we make through creation of a story that is an artifact of the mind.
2602000	2603000	It's a creative act.
2603000	2615000	And it, it's, it's all about reading the signals of the world, you know, sort of metabolizing those signals, finding the patterns and the forms,
2615000	2621000	and then recreating them in a construct that is meaningful to other people.
2621000	2631000	And, and one of the things that I find really disturbing is when people have this ridiculous notion that the, that the universe is meaningless.
2631000	2639000	And I think if you ever want to find meaning, the place to find it is in mentoring.
2640000	2656000	Because when you mentor someone, as John says, when you do, and they come up with an insight that you never, ever imagined, it is the most meaningful thing that you can ever experience.
2656000	2665000	When, when my kids come up with something incredible, I just go, oh, okay, then.
2665000	2670000	Right. All is well in the universe on the right track. Right.
2670000	2683000	And so if you can teach people how to have insightful moments in storytelling, and it doesn't have to be all that difficult, just learn how to write a really, you know, valence sentence.
2683000	2689000	You know, that is meaningful to someone and goes, Oh, that's an interesting sentence. Could I read another one of yours?
2689000	2705000	That that is the means by which we can cultivate wisdom within ourselves by expressing ourselves and reaching a dialogue with other people so we can test self evidence through our writing.
2705000	2706000	Wow.
2706000	2723000	So this is kind of the project that is enveloped me and what really attracted me to John because we're both doing the same project. It's just different ecologies of practices as John would say.
2723000	2724000	Yeah, exactly.
2724000	2728000	What's wonderful here is we get a lot.
2728000	2731000	It's popular today to think about.
2731000	2736000	And this is a good thing as well. How can we become?
2736000	2743000	How can we be? How can we better interface with the emerging technologies that are coming? So you want to practice mindfulness? Why?
2743000	2754000	Because you don't want you don't want Evil Corp to have the whole say and where your attention goes. So regaining your attentional autonomy seems like it's a crucial thing going forward.
2754000	2760000	So we've got that story where you want to be wise because we want to wisely interface with technology.
2760000	2764000	But what we have here is a companion idea and it runs so deep.
2764000	2774000	Just a short pod just doesn't give it credence. I just hope everybody engages with this material and like seeks out Sean and John because this is stuff that you really need to digest.
2774000	2789000	But we have a companion project here which isn't be wise so that you're better in the face of emerging technology, but rather learn to be wise because we're going to be called upon to first better understand the threshold we're moving through.
2789000	2799000	And that's your responsibility because we live in the world together and you better be ready. So you better be wise to understand this this this liminal moment we're passing through.
2799000	2806000	And two, you're going to be called on to mentor not even not only each other, but maybe synthetic systems in the future.
2806000	2814000	And if we're going to have a good grip on how to make a synthetic powerful intelligence good, we had better bloody know how to be good ourselves.
2814000	2824000	Otherwise, otherwise, we're just shooting in the dark being like oh well make it good and you're like well how and you're like well I don't know I don't know what it is to be good I guess make a lot of money you're like that's not the good.
2824000	2840000	You better know you better know how to be good so you can be good for them, which is very much like a parent child relationship here you don't want to only be good so that you can survive parenthood, but you want to be good because hopefully you want your kids to be good and so that is a moral obligation to be good.
2840000	2850000	And you articulated it beautifully that is Karuna that is agape, and that is like the thing we're trying to ultimately get people to virtuously tap into in this project.
2851000	2855000	Wow, so we're going to go now.
2855000	2861000	But I just want to say, I want to I want to even just go on that cliffhanger because here's the thing.
2861000	2866000	We've got a podcast here that explores the science of contemplative programs.
2866000	2871000	And this is again, like I said at the beginning of the episode this is our first episode back.
2871000	2882000	I just want to leave it there because this is a real call to arms for exactly the kinds of things that we're interested here, which is we need to be wise at the birth of this big intelligence we need to be our best selves.
2882000	2895000	And so, thanks, guys, and please everyone check out the material and I will be linking everything in the comments so that you can get in touch with this extremely important project.
2895000	2900000	Any last words or does that feel like a good natural close.
2900000	2903000	I'm happy with that. Thank you very much, Mark.
2903000	2909000	Yeah, you're the light and fire inside of you is always extremely beautiful.
2909000	2920000	Well, I mean, how do you how do you not light on fire when these sparks are just coming off of your guys projects? Really, Sean and John, thanks so much. And we'll talk soon for sure.
2920000	2926000	Okay, thanks everybody. That's another episode of the contemplative science podcast. And as always, we'll see you next week.
