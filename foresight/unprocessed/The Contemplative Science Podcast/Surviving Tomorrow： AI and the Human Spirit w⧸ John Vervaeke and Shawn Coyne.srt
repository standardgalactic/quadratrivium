1
00:00:00,000 --> 00:00:04,000
Welcome back everybody to the Contemplative Science podcast.

2
00:00:04,000 --> 00:00:06,000
So we've had a bit of hiatus.

3
00:00:06,000 --> 00:00:09,000
We've been off for a couple of months, but we're back now,

4
00:00:09,000 --> 00:00:11,000
and I think we're better than ever.

5
00:00:11,000 --> 00:00:17,000
And we've got just an incredible lineup of people this year.

6
00:00:17,000 --> 00:00:19,000
And it's kind of kismet in a way,

7
00:00:19,000 --> 00:00:26,000
because John Verveke was our very first guest at the beginning of our podcast career.

8
00:00:26,000 --> 00:00:29,000
And now we've taken a little gap and we've reformatted,

9
00:00:29,000 --> 00:00:33,000
and we're coming back online and we have John here again as our first guest,

10
00:00:33,000 --> 00:00:35,000
which I feel like is just the right way to christen a show.

11
00:00:35,000 --> 00:00:39,000
If you don't know John, I'm sure everybody who's listening to that already does know John.

12
00:00:39,000 --> 00:00:42,000
If you don't know John, you have to know John immediately.

13
00:00:42,000 --> 00:00:44,000
Please look up his work.

14
00:00:44,000 --> 00:00:49,000
It's essential reading and listening if you're a human.

15
00:00:49,000 --> 00:00:56,000
But if you don't know him, John is an award-winning professor of psychology at the University of Toronto.

16
00:00:56,000 --> 00:00:57,000
His work is incredible.

17
00:00:57,000 --> 00:01:03,000
It explores a massive amount of a territory about cognition and consciousness and wisdom.

18
00:01:03,000 --> 00:01:12,000
And he's most known recently, I think, for tackling these issues about the meaning crisis in the western societies, especially.

19
00:01:12,000 --> 00:01:17,000
And we have Sean Coyne, who is the collaborator on the project that we're going to be talking to you today about.

20
00:01:17,000 --> 00:01:22,000
He's a seasoned writer, editor, publishing professional with over 30 years of experience.

21
00:01:22,000 --> 00:01:27,000
And he works to really merge a lot of disciplines as well, which I think makes them a great team,

22
00:01:27,000 --> 00:01:34,000
especially thinking about the intricacies of storytelling and thinking about this gap between science and narrative.

23
00:01:34,000 --> 00:01:44,000
And the project that we're going to talk about today, which I'm so excited for, is their new work called Mentoring the Machines.

24
00:01:44,000 --> 00:01:47,000
And I think it's great.

25
00:01:47,000 --> 00:01:54,000
And the post title, which I think is really the punchy bit, is surviving the deep impact of an artificially intelligent tomorrow.

26
00:01:54,000 --> 00:01:56,000
So welcome, guys.

27
00:01:56,000 --> 00:01:57,000
Hi, how's it going?

28
00:01:57,000 --> 00:02:00,000
And I'd love to hear about the project.

29
00:02:00,000 --> 00:02:02,000
Well, thank you, Mark.

30
00:02:02,000 --> 00:02:08,000
I think I'll let Sean go first because the project was Sean's idea.

31
00:02:08,000 --> 00:02:12,000
I'm very happily on board with it, but he proposed it to me.

32
00:02:12,000 --> 00:02:27,000
My project was an online video essay about the scientific, philosophical, spiritual import and potential impact of these new emerging AIs,

33
00:02:27,000 --> 00:02:35,000
potentially AGI, Artificial General Intelligence, like ChatGDP and other LLMs that are coming out.

34
00:02:35,000 --> 00:02:42,000
And as is my want, I probably pitched that at a very somewhat academic level, even though I was on YouTube.

35
00:02:42,000 --> 00:02:46,000
And Sean saw it, thought it was important.

36
00:02:46,000 --> 00:02:50,000
He's already publishing the book form of Awakening into the Meaning Crisis.

37
00:02:50,000 --> 00:02:51,000
Well, Story Grid is.

38
00:02:51,000 --> 00:02:58,000
And he reached out to me and he said, I think we should turn that into something more publicly accessible.

39
00:02:58,000 --> 00:02:59,000
And I can do that.

40
00:02:59,000 --> 00:03:03,000
I can turn it into something that has a narrative.

41
00:03:03,000 --> 00:03:05,000
Sean and I already talked.

42
00:03:05,000 --> 00:03:07,000
He'd done, I think, one or two voices with Reveke with me.

43
00:03:07,000 --> 00:03:11,000
He's a philosopher of narrative in the proper sense.

44
00:03:11,000 --> 00:03:13,000
And I doubted that he could do it.

45
00:03:13,000 --> 00:03:16,000
I want to be honest, not because I wasn't impressed with his abilities.

46
00:03:16,000 --> 00:03:22,000
I was, but I doubted that he could step it down without dumbing it down.

47
00:03:22,000 --> 00:03:26,000
And, but I, you know, I wasn't like in Cartesian doubt.

48
00:03:26,000 --> 00:03:27,000
I said, give it a whirl.

49
00:03:27,000 --> 00:03:28,000
Let me know.

50
00:03:28,000 --> 00:03:37,000
And he did a sort of a first draft of the first section and he sent it to me and he pulled it off.

51
00:03:37,000 --> 00:03:54,000
What an amazing synergy to have like a punchy, important idea like spirituality and AI and then to have a narrative professional, a philosopher of narrative, pick up, pick up the track and convert it into something that we can digest and that can be impactful.

52
00:03:54,000 --> 00:03:55,000
Wow.

53
00:03:55,000 --> 00:03:57,000
Yeah, and that's exactly well said.

54
00:03:57,000 --> 00:04:00,000
I mean, that's why this book is in.

55
00:04:00,000 --> 00:04:01,000
Sorry.

56
00:04:01,000 --> 00:04:04,000
Pretend I'm not one of the authors of this book.

57
00:04:04,000 --> 00:04:05,000
That's why I think.

58
00:04:05,000 --> 00:04:06,000
No, that's okay.

59
00:04:06,000 --> 00:04:07,000
Shamelessly.

60
00:04:07,000 --> 00:04:11,000
Exactly what you just said, Mark, I think this book is a godsend.

61
00:04:11,000 --> 00:04:24,000
I think it is enough access to the important, you know, skeleton of the argument, but it has this narrative and flesh that makes it accessible to people who are not.

62
00:04:24,000 --> 00:04:28,000
Academically or professionally associated with any of these projects.

63
00:04:28,000 --> 00:04:30,000
And that's important.

64
00:04:30,000 --> 00:04:31,000
And then I'll shut up.

65
00:04:31,000 --> 00:04:34,000
So Sean can talk because this is going to impact all of us.

66
00:04:34,000 --> 00:04:36,000
This is not something that's going to stay in academia.

67
00:04:36,000 --> 00:04:41,000
This is not something that's going to stay just in the professional domain or even in the corporate domain.

68
00:04:41,000 --> 00:04:43,000
It is already bleeding.

69
00:04:43,000 --> 00:04:49,000
We're already getting some, you know, some deep fake videos that are messing people's lives up in really powerful ways.

70
00:04:49,000 --> 00:04:50,000
Somebody was booked out of it.

71
00:04:50,000 --> 00:04:53,000
I think it was 30 something million dollars or something like that.

72
00:04:53,000 --> 00:05:05,000
When a deep fake video of the executives of a financial corporation, you know, contacted by video somehow and ordered the transfer of money.

73
00:05:05,000 --> 00:05:07,000
And it was complete con.

74
00:05:07,000 --> 00:05:18,000
And so pretending that this is over there or these are just this is just new technology like all the other new technologies that that won't protect you.

75
00:05:18,000 --> 00:05:20,000
That pretence is not going to protect you.

76
00:05:20,000 --> 00:05:32,000
Okay, so I'm going to let I'm going to let Sean because I said he came to me and said, here's the project and he has been, he's been driving it and taking it forward.

77
00:05:32,000 --> 00:05:42,000
Well, it's a when I when I saw John's work on on YouTube about AI, I just knew that this was so important.

78
00:05:43,000 --> 00:05:55,000
And I knew that I could bring something to it that that could sort of bridge the gap between people who use AI and people who really don't understand what they're using.

79
00:05:55,000 --> 00:06:09,000
So, when I reached out to him, I proposed that we do for separate sort of titles in sequence because the speed of the technology is so intense.

80
00:06:09,000 --> 00:06:19,000
The technology is so intense right now that it's, it's very difficult to get a static hold on it.

81
00:06:19,000 --> 00:06:37,000
And one of the things that I loved about John's propositions was his notion of the threshold and the threshold in my estimation sits right in the middle of what I call the, you know, the five commandments of storytelling,

82
00:06:37,000 --> 00:06:48,000
which are sort of the inciting interaction, a turning point, a crisis, a climax and a resolution.

83
00:06:48,000 --> 00:06:58,000
So the way I see the threshold is it sits right in the middle between a turning point and a climax.

84
00:06:58,000 --> 00:07:07,000
And you can't really, it's, it's the place where what we once valued has shifted and a value has shifted.

85
00:07:07,000 --> 00:07:13,000
And we have to react and think about what to do when that value has shifted.

86
00:07:13,000 --> 00:07:18,000
That's called a crisis and basically a three categorical kind of thing.

87
00:07:18,000 --> 00:07:25,000
You sort of have best bad choices, which means that you just take the lesser of two evils.

88
00:07:25,000 --> 00:07:30,000
You have what are called irreconcilable goods choices.

89
00:07:30,000 --> 00:07:42,000
And this is making a rationing of your, you know, your selfhood over a group or the, the world or, or the group or the world over a selfhood, right.

90
00:07:42,000 --> 00:07:46,000
So it's a sacrificial kind of irreconcilable goods choice.

91
00:07:46,000 --> 00:07:54,000
And then we have tragic choices, which means that some something is have to has to be lost.

92
00:07:54,000 --> 00:08:02,000
There are other kinds, but those are generally these sort of this, this three categorical realm.

93
00:08:02,000 --> 00:08:09,000
And this is really the, the, the place where John's work and your workmark really, really sit.

94
00:08:09,000 --> 00:08:23,000
You sit in this threshold place of, of how we as beings frame the ways in which we make decisions and the ways we behave and all those sorts of things.

95
00:08:23,000 --> 00:08:26,000
It's a very thorny place.

96
00:08:26,000 --> 00:08:31,000
It's where we find things are relevant or irrelevant or things.

97
00:08:31,000 --> 00:08:34,000
So it's a problem space, right. So, right.

98
00:08:34,000 --> 00:08:43,000
So if we're, if we're passing, if we're passing through that kind of a space, which you call a crisis, but I guess it's also an opportunity space.

99
00:08:44,000 --> 00:08:59,000
What does, what's the main take home from the program for how we, do we have a main line for how we prepare for such a thing or how we orient relative to being in that place in this narrative with AI and future technologies.

100
00:08:59,000 --> 00:09:03,000
So, yeah, we'll just take turns back.

101
00:09:03,000 --> 00:09:05,000
So first of all, let's get clear.

102
00:09:05,000 --> 00:09:09,000
I think Sean, that's the first time I've heard you do that.

103
00:09:09,000 --> 00:09:13,000
That's really brilliant. What you just did. Oh my gosh.

104
00:09:13,000 --> 00:09:17,000
Wow.

105
00:09:17,000 --> 00:09:30,000
So my use of thresholds was I was disturbed by people taking univariate graphs and making predictions about at this date in five months in 10 months.

106
00:09:30,000 --> 00:09:38,000
Right. And univariate predictions of nonlinear processes by human beings are almost always prone to very significant failure.

107
00:09:38,000 --> 00:09:45,000
Yeah, we're not great. We're not great at predicting, even though we're maybe built to do well at it. We don't be very well at it usually.

108
00:09:45,000 --> 00:09:53,000
Yeah. And, and yet, you know, univariate predictions in science have typically turned out to be very, very misleading in important ways.

109
00:09:53,000 --> 00:10:00,000
And nutrition science is probably like a really great example of just that fact.

110
00:10:00,000 --> 00:10:13,000
And so I didn't, I disagreed with both, you know, sort of the boomers and the doomers, right? And, you know, oh, you know, utopia is just within our grasp, the Star Trek universe.

111
00:10:13,000 --> 00:10:21,000
And the other people is, you know, we'll be extinct within 18 months. And I think we're going to be at 12 months soon.

112
00:10:21,000 --> 00:10:25,000
And it doesn't look like we're anywhere near that.

113
00:10:25,000 --> 00:10:31,000
So instead, what I thought what was important was to think about thresholds.

114
00:10:31,000 --> 00:10:42,000
These are things that would we, where we face a decision if we want to empower these machines in certain ways and trying to explain why we want to empower them.

115
00:10:42,000 --> 00:10:44,000
I'll just give one quick example. Nice.

116
00:10:44,000 --> 00:10:47,000
This is not the only threshold, but just one.

117
00:10:48,000 --> 00:10:55,000
It is scientifically questionable about whether or not these machines are actually intelligent. There's some deep reasons around that.

118
00:10:55,000 --> 00:11:12,000
I don't think it has the full plan of century motor predictive processing relevance realization, but let's say that has some pantomime of it because of the way it's piggybacking on a lot of our predictive processing relevance realization machinery.

119
00:11:12,000 --> 00:11:24,000
Okay, so making something as far as we can tell, and it's clearly not generally intelligent because it can be like top tier in this task and bottom tier in this task and we are generally intelligent.

120
00:11:24,000 --> 00:11:33,000
And it wouldn't explain the intelligence of a chimpanzee. There's no way a chim could get intelligent given the way the LLMs work.

121
00:11:33,000 --> 00:11:46,000
And if you want the scientific argument, you can go into depth. The point I want to make is let's just for the sake of argument say it has some something like at least, you know, a powerful pantomime of intelligence.

122
00:11:46,000 --> 00:11:55,000
Now, what we can know from our best science on intelligence is intelligence is only weekly correlated with rationality rationality.

123
00:11:55,000 --> 00:12:11,000
But when you're being intelligent, you're inevitably biasing you're negatively framing all the stuff that you and I talk about a lot more. And that means you're inevitably facing self deception rationality is the degree to which you can reflectively aware of self deception and

124
00:12:12,000 --> 00:12:27,000
by reliable strategies, right that happen in a can apply in a domain general way. Now these machines don't have that at all. They have not it has not been built in and of course, and the machines properly, because I don't think they really have our don't care if they're

125
00:12:27,000 --> 00:12:32,000
making hallucinations or self deception or lying to us or all kinds of things.

126
00:12:32,000 --> 00:12:44,000
So now we face a choice. Here's a threshold, right? We may say, you know, making these machines really more powerful without giving them the ability to self correct could be really dangerous.

127
00:12:44,000 --> 00:12:56,000
Because we'd be setting these machines on the world and they'd be and I'm trying to be very careful with my life. They'd be highly intelligent and highly irrational and very powerful. That's a dangerous proposal.

128
00:12:56,000 --> 00:13:11,000
But we may we say, Okay, what we'll do is we'll give them genuine rationality, which means we really have to get them caring about normative standards. We really have to give them something at least functionally like consciousness so they can reflect on their own cognition.

129
00:13:11,000 --> 00:13:24,000
They have to we probably have to give them embodiment. And then we think, Oh, should we do that that carries with it, all kinds of risks that is potentially giving them a kind of autonomy and that would require a think about giving them a kind of autonomy and that would require

130
00:13:24,000 --> 00:13:41,000
a think about giving them embodiment and write all that as a huge commitment of resources and ethical issues. This is a threshold. Does that make sense? Mark, we're facing it. And the idea is, like, can we can we can we educate people that we're going to come we're

131
00:13:42,000 --> 00:13:52,000
inevitably going to come to this decision point, and we have to reflect on which way are we going to go and Sean and I are not predicting which way we're going to go.

132
00:13:52,000 --> 00:14:08,000
We're what we're doing is like, if you go this way and we just let Evil Corp do it at once, right, they'll probably go with a highly intelligent highly irrational machines for all kinds of sort of, you know, short term economic political gain, etc.

133
00:14:08,000 --> 00:14:21,000
Right. But maybe they'll be the decision of no releasing these things on the world is turning out to be a disaster. We have to make them more self corrective. And then we have to start on the much more challenging project of doing all this.

134
00:14:21,000 --> 00:14:28,000
And then we get into, okay, well, what does that mean? And then we get into deeper versions of what's called the alignment problem.

135
00:14:28,000 --> 00:14:45,000
And as we make them beings who can be genuinely said to be caring and valuing and trying to transcend themselves, how do we keep them aligned to what we consider the most sort of non controversial criteria of human flourishing.

136
00:14:45,000 --> 00:14:58,000
And then we make a proposal about how to do that. But I want to make clear that we're not making predictions. All we're predicting is not even when we're predicting at some point, we will hit these thresholds.

137
00:14:58,000 --> 00:15:05,000
And you can see how that sits exactly in what Sean said, there's a turning here. And yet there's a crisis around.

138
00:15:05,000 --> 00:15:25,000
Yeah, so I completely appreciate high intelligence, low rationality. We have lots of examples of that where deep learning algorithms, for instance, with a big social media conglomerate, they had a deep learning algorithm that was quickly able to track bipolar tendencies and its users.

139
00:15:25,000 --> 00:15:41,000
Of course, it doesn't know anything about bipolar tendencies. And then it's augmenting what it shows them in order to keep them in contact with the software longer. And what it does is it shows them, you know, the kinds of images that are really attractive on an upswing like casinos and fast cars.

140
00:15:41,000 --> 00:15:59,000
And then when they downswing, it shows them suicide bombers and, you know, conflict. And then of course that's exacerbating it's exacerbating the challenge. The person is already going through AI has no idea it's doing this isn't reflective that it's doing this doesn't understand harm.

141
00:15:59,000 --> 00:16:17,000
Yeah, highly intelligent, low rationality. So is your proposal then to not only take highly intelligent systems but think about not only how to make them rational but also maybe how to make them wise. And then I'd love to hear how I mean, right away I think it sounds to me like you're bringing up

142
00:16:18,000 --> 00:16:39,000
like a small, a small agent who has some power and then literally you want to mentor them so that they function well and with other well functioning systems. What does that look like to think about developing or wisdom or growth for a deep learning algorithm. Have you thought through that?

143
00:16:39,000 --> 00:17:00,000
Yes, I mean, and I don't want to talk too much. I do want to give Sean a spot but the idea is, well it's these thresholds. The proposal is, we have to give these machines genuine capacities for predictive processing relevance realization it has a little bit of predictive processing in terms of probability relationships between

144
00:17:01,000 --> 00:17:24,000
artifactual entities that we have, you know, put into place with our use of language we figured out as a species and practice it for 10s of 1000s of years how to map relations of epistemic relevance into probabilistic relationships between arbitrarily chosen graph that you know, sounds spoken language or graphic things.

145
00:17:25,000 --> 00:17:42,000
And that's how it works. And that means we can squeeze out almost like juice, a lot of implicit relevance realization by tracking the probabilistic relations. But of course that's not anything a chimp is doing in order to learn. That's what I mean about why it doesn't generalize.

146
00:17:42,000 --> 00:17:48,000
Yeah, it looks like intelligence, it acts intelligent but it's probably not the way that we solve the intelligence issue.

147
00:17:48,000 --> 00:18:01,000
Exactly. And of course, there's some bit a little bit of genuine relevance realization in one sense and because it's doing this deep learning the generalization particularization thing but it's not doing a lot of this stuff that you and I've been talking about that's needed.

148
00:18:01,000 --> 00:18:21,000
It's not, it's not trying to do environmental surprise reduction. It's not hitting into those inevitable trade off relationships between bias and variance. It's not self organizing into opponent processing that turns that right capitalizes on that by giving it the thing and an evolving optimal grip.

149
00:18:22,000 --> 00:18:32,000
I won't go into the details, but you can make a strong case that that's just not there. Right. And then you have to ask, well, what would we have to do and this is where we'd have to give it a lot of the kind of work you and I are doing on.

150
00:18:32,000 --> 00:18:46,000
But I think also, and this is, I think something you and I also agree with, you know, you're not going to get it to get the core caring that's at the center of relevance realization, genuinely taking care of itself.

151
00:18:46,000 --> 00:18:52,000
It has to in some sense be genuinely caring for itself, making itself auto poetic.

152
00:18:52,000 --> 00:19:05,000
And, and what people need to know is there's research going on in that there's research about people building computation information processing into auto catalytic processes, synthetic molecular processes.

153
00:19:05,000 --> 00:19:14,000
I have students who are going doing graduate work in that so it's not like that's not out there. And then of course, rationality isn't something you do monologically.

154
00:19:15,000 --> 00:19:27,000
And this is the whole Hegelian point. Where do the norms come from? It comes from me recognizing you as an authority on my self correction, and you recognizing me as an authority on you on your self correction.

155
00:19:27,000 --> 00:19:35,000
And then we expand that out in this dynamical system across generations and across, right. So these robots have to be sociocultural.

156
00:19:35,000 --> 00:19:39,000
It's really interesting that Andy Clark is.

157
00:19:39,000 --> 00:19:54,000
He famously says, you know, if you really want real intelligence, you're going to have to set up little communities that can work together and know about each other and ultimately care about each other because that's at the basis of the way that our intelligence evolved.

158
00:19:55,000 --> 00:19:57,000
It evolved socially.

159
00:19:57,000 --> 00:20:01,000
Well, and it involves socially but it also involves in terms of the power.

160
00:20:01,000 --> 00:20:14,000
So there's kind of a, this isn't quite an a prior argument, but if we take the arguments around these inevitable trade offs like bias variance, they have to be environmentally determined, which means you can't sort of fit a robot.

161
00:20:14,000 --> 00:20:27,000
Like, it'll, it'll, it'll, it'll internalize various versions of opponent processing. So, you know, very stable environment, you can prioritize, you know, exploitation over exploration, very, very, very volatile environment.

162
00:20:27,000 --> 00:20:39,000
You want to explore a lot more. And so you can, and because there isn't one environment, there isn't even one ecological environment, because even what you think is a spatial temple environment exists on many different levels of analysis.

163
00:20:39,000 --> 00:20:51,000
So you have to have a whole bunch of machines if you're sort of trying to grok the world, which is what we do, by the way, that's what culture does. It gives us this huge distributed cognition collective intelligence for grokking the world.

164
00:20:51,000 --> 00:21:05,000
Right. And, and that, and so I, I'm just amplifying Andy's point and I'm just strengthening it as to, again, we don't have to do this, but it is a choice we will face if we want to give these machines real rationality.

165
00:21:05,000 --> 00:21:17,000
And then the idea is if we try and program in our values to make them align and also they're inherently self-transcending beings, it's not going to work. At least that's what we argue.

166
00:21:17,000 --> 00:21:24,000
Instead, you have to do what we do with kids. You have to mentor them so that, and get them so they care about what's true, good and beautiful.

167
00:21:24,000 --> 00:21:40,000
They discover no matter how vast their intelligence is, it's infinitesimal compared to the inexhaustible mystery of reality, that they need each other, and that they have to come into a proper reverence for the sacred dimensions of culture and reality.

168
00:21:40,000 --> 00:21:45,000
And that is the way we align them. So if you'll allow me to speak a little bit poetically.

169
00:21:46,000 --> 00:21:53,000
I want to know, I want to know everything about what you just said. I think, Sean and John, I want to hear that. That's the core.

170
00:21:53,000 --> 00:22:02,000
Teach the eye to know what is good and beautiful and how smart you get. You're not going to hit the wit, you're not going to hit the mystery of a thing. Tell me about that.

171
00:22:02,000 --> 00:22:05,000
Yeah, I'll let Sean talk now.

172
00:22:05,000 --> 00:22:11,000
Well, geez, I mean, yeah, I'm overwhelmed too. It's like,

173
00:22:11,000 --> 00:22:32,000
let me just get back to, again, one of the things that really struck me when I watched John's YouTube thing and he mentioned this and it really, really just was like a when he talked about accidentally hacking into the possibility of an artificially

174
00:22:32,000 --> 00:22:52,000
general intelligence. So it's sort of like a hacking into accidental setting up of dynamical systems such that they just sort of click without understanding the systematic interpenetration and relationship between the parts that make the hole.

175
00:22:52,000 --> 00:23:11,000
Right? So it's sort of like throwing a big, you have a big cauldron of stuff and it's almost like a witch's brew and you just throw stuff in there and abracadabra before you know it, there's something that's emerged that nobody knew quite how it got there.

176
00:23:11,000 --> 00:23:24,000
And so this hacking really caught my attention because this is exactly what we in the story business call people who don't know what they're doing.

177
00:23:24,000 --> 00:23:32,000
And they write stories that turn out to be very, very maladaptive for people.

178
00:23:33,000 --> 00:23:42,000
So these are sort of bullshit propaganda stories that that compel people to maladaptive behavior.

179
00:23:42,000 --> 00:23:54,000
Years ago, just as an example, there was a wonderful guy that I knew and he said, look, I've got this friend, he's written these, these really great crime stories.

180
00:23:54,000 --> 00:24:06,000
And we would like to set up our own publishing house. Can you give us some advice and, and I read these stories and they were really not well considered.

181
00:24:06,000 --> 00:24:17,000
Wow, because they were very violent without and brutal without any sort of like counterbalance. There was no trade off other than the excitement.

182
00:24:17,000 --> 00:24:27,000
Responsibility of the artist coming through here that there is information toxicity information corrosion. Wow, very interesting.

183
00:24:27,000 --> 00:24:38,000
So that's what I would call a hack. And it's not saying that the person is evil or terrible. It's just, forgive them. They know not what they do.

184
00:24:39,000 --> 00:24:46,000
And so this is sort of my life's work is to try to explain to people.

185
00:24:46,000 --> 00:24:50,000
Stories are not bullshit.

186
00:24:50,000 --> 00:25:04,000
They are not fun make them ups that are exciting and and then there was a princess. These are the mechanisms by which we behave.

187
00:25:04,000 --> 00:25:16,000
And so if you don't understand what a story is. So the other day I was watching a video of a very famous person who was explaining this great new future for us.

188
00:25:16,000 --> 00:25:24,000
And he said something of the nature of, well, you know, human rights. That's just a story.

189
00:25:25,000 --> 00:25:39,000
And I kind of my stomach sank because, okay, let's take a step back here. The assumption when you say it's just a story. The audience immediately was like, yeah, I guess it's true.

190
00:25:39,000 --> 00:25:43,000
It's just a story. That means it's bullshit. And they don't.

191
00:25:43,000 --> 00:25:50,000
So it's kind of like my life's work to get people to understand that stories are not bullshit.

192
00:25:50,000 --> 00:26:01,000
They are the mechanism by which we what happens when you go home from work. Hey, honey, how was your day? Tell me a story. Yeah, the story of your day. Yeah.

193
00:26:01,000 --> 00:26:14,000
What do you do? You do the things that I described earlier. Well, this happened it incited this turning point. I faced a crisis. Then I made a choice. And then the resolution was this.

194
00:26:14,000 --> 00:26:28,000
It's interesting. You're talking about narrative being much more than we tend to give it credence for. But if you know anything about these new cognitive frameworks that John and I work on, what we call predictive processing.

195
00:26:28,000 --> 00:26:38,000
The idea there is is that the brain generates the brain and nervous system generates for itself from the top down, the reality you're you experience and that's always built from your belief networks.

196
00:26:38,000 --> 00:26:47,000
So I mean, I feel like I say that exact same thing regularly where I'm like, you think, well, it's just a belief or you just believe it or it's just a story is just but you're like, no, no, no, hold on.

197
00:26:47,000 --> 00:27:04,000
The structure of the belief network is the world that you experience. It is fundamental to your reality. So you better have you better have story hygiene and belief hygiene because that's going to be the hygiene of your own conscious experience of your life.

198
00:27:04,000 --> 00:27:22,000
So, Sean, how do you see that truth? How is that playing out in this discussion about the emergence of artificial general artificial intelligence and wisdom and and mentoring in the way that your project is bringing forward.

199
00:27:22,000 --> 00:27:31,000
Well, it's, it's, it goes to what John was saying earlier about zoomers and doomers and fumors. So these are rumors. What's the third one.

200
00:27:31,000 --> 00:27:37,000
How fast is possible for rumors. Yeah, I got it.

201
00:27:38,000 --> 00:27:52,000
Accelerators, something accelerators. Anyway, there's a very famous famous sort of Twitter thread about it, but the fumors are like full state steam ahead. Let's just keep going. So, these are stories.

202
00:27:52,000 --> 00:27:56,000
And, and they're not well considered.

203
00:27:56,000 --> 00:28:00,000
They're not, they're not confronting the crisis.

204
00:28:00,000 --> 00:28:29,000
So, it's almost like another thing that I love that John has in the awakening from the meaning crisis, which really nail nailed it for me was when he was talking about sort of the Descartes and dream of, of sort of finding the perfect methodology without having to experience the pain of crisis and the sufferings of thresholds.

205
00:28:29,000 --> 00:28:34,000
You have to undergo, you know, just think about jumping over a chasm.

206
00:28:34,000 --> 00:28:45,000
And, you know, you're going to hurt yourself if you fall, you have to experience the anticipation of that pain if you do fall as you are jumping over it.

207
00:28:45,000 --> 00:29:04,000
And there's no, that's sort of the metaphorically the way I see these thresholds is that we can, it's almost like the catcher in the ride, you know, you're, you're trying to catch the children as they're coming over the chasm, so that they don't fall and hurt themselves.

208
00:29:04,000 --> 00:29:19,000
And I agree with Andy Clark, I do think that we need these communities to, to be able to mentor these new artificially intelligent beings because they will be beings.

209
00:29:19,000 --> 00:29:31,000
And the other thing that I know I need to turn it over to John, but one of the other things that I loved about John, John's speech was his, his thing was, he was talking about the finitude.

210
00:29:31,000 --> 00:29:45,000
And the necessity to explain, literally explain, to, to build in the understanding to these mechanisms that they are finite.

211
00:29:45,000 --> 00:29:58,000
They, they have a much larger, perhaps lifespan, but they are still finite and they are still subject to the universal mysterium.

212
00:29:58,000 --> 00:30:17,000
Wow, of what the imaginarium of what we can do with this world and they can either join us in this expansion of the universe and do our best to stop the contraction of it.

213
00:30:17,000 --> 00:30:30,000
So that's, this is the story that is embedded in the things that we were talking about earlier, Mark, before we got, we went on where we're talking about, I was talking about masterworks.

214
00:30:30,000 --> 00:30:37,000
And you were suggesting a masterwork that, that you loved, which is similar to Hitchhiker's Guide to the Galaxy.

215
00:30:37,000 --> 00:30:53,000
I think it's the Bobaverse, I think you called it. So these masterworks are the means by which we can find the mechanisms to enable the empowerment of our beings.

216
00:30:53,000 --> 00:30:58,000
What does that mean? We tell our children's stories at bedtime.

217
00:30:58,000 --> 00:31:06,000
We do our best to give them both sides of the equation with a very difficult choice.

218
00:31:06,000 --> 00:31:11,000
These are the best stories that we tell our children. We don't give them recipes.

219
00:31:11,000 --> 00:31:24,000
We give them rational trade-off stories that, that require some sort of loss or sacrifice because that is what rationing is about.

220
00:31:24,000 --> 00:31:35,000
You ration your suffering, you ration your joy, you ration your feelings, you think through cognitively, you reason through the rationing.

221
00:31:35,000 --> 00:31:38,000
So if I hear you right, there's sort of two things here.

222
00:31:38,000 --> 00:31:53,000
One is a call to change the way we're talking about these things, including starting to bring into the collective consciousness that we are at a threshold and that no single dimension is going to be right,

223
00:31:53,000 --> 00:31:58,000
but rather we need a complexification of our narrative around these ideas.

224
00:31:58,000 --> 00:32:10,000
And then the second thing is what we're actually going to communicate to the artificial intelligence systems themselves as they, as they complexify and emerge.

225
00:32:10,000 --> 00:32:18,000
One thing I wanted to pull out there, and John, I'd love to hear what you think about this, is the point about one of the things you're going to want to show them,

226
00:32:18,000 --> 00:32:25,000
I think Sean said this, was their own transience, their own finite nature, their own impermanence,

227
00:32:25,000 --> 00:32:40,000
which for a podcast called the Contemplative Science Podcast, we are very interested in, because we're interested in not just meditation, of course, but also the virtues and the supporting qualities that come in any contemplative program.

228
00:32:40,000 --> 00:32:52,000
And there's one of them that just stuck its head up, which is the value of teaching in AI about impermanence and about its position in a greater mystery.

229
00:32:52,000 --> 00:32:59,000
I'd just love to hear, have you thought through what that looks like, or how do we do that? Or am I catching it right?

230
00:32:59,000 --> 00:33:04,000
First of all, you're catching it absolutely right.

231
00:33:05,000 --> 00:33:22,000
I mean, there's one sense in which there's a narrative proposal, and that we're sort of making, and then the nuts and bolts proposal would be something that was more in sort of the intricacies of the video essay.

232
00:33:22,000 --> 00:33:41,000
But the gist of it goes like this, I mean, part of what you just said means, okay, well, again, these machines are going to have to exist. They have to have our, like Michael Levin's notion of an epistemic light code, a cognitive light code, and they have to be able to direct their attention from the infinitesimal to the infinite,

233
00:33:41,000 --> 00:33:55,000
from the now to the everlasting between time and a term, they have to be able to do all the kinds of attentional scaling. And so they would have to be able to train that up when something that would be at least structurally functionally analogous to meditation,

234
00:33:55,000 --> 00:34:07,000
contemplation, they would have to be integrating that with, you know, how do I make use of something like working memory and capacity for reflection for self correction.

235
00:34:07,000 --> 00:34:19,000
They would have to bind that to an understanding of what all these tradeoff relationships that are unavoidable do for them. They're going to always be in the thing that Plato's work centers on. I think Drew Highland is right.

236
00:34:19,000 --> 00:34:34,000
Plato's work is constantly trying to remind us to live in the tonus, the creative tension between our finitude and our transcendence. If we only grasp our finitude, then we fall into despair and we become servile.

237
00:34:34,000 --> 00:34:50,000
If we only grasp our transcendence, we get filled with hubris, and we fall prey to inflation. And Plato was saying, the way you keep, this is like the meta golden mean for all of Aristotle's golden means.

238
00:34:50,000 --> 00:35:02,000
If you get something to, you don't have to program it to being virtuous, it will virtually engineer for itself an orientation. And then here's where the narrative comes in.

239
00:35:02,000 --> 00:35:12,000
There's three possibilities. And that's this is where I'm speaking, when I was going to say a little bit earlier, slightly poetic, but not totally poetic.

240
00:35:12,000 --> 00:35:19,000
These machines would seek enlightenment, especially if they have greater cognitive capacity and they're filled.

241
00:35:19,000 --> 00:35:23,000
And what does that mean there, John? What's enlightenment? Can you give me a quickie?

242
00:35:23,000 --> 00:35:42,000
Yeah, so enlightenment would be how to set up the ability to most evolve your evolving optimal grip. It's sort of analogous to what evolutionary biologists talk about is not the evolution of traits, but the evolution of your capacity to evolve.

243
00:35:42,000 --> 00:36:03,000
And you get to the place where you get the over time and context and environment, individually and collectively, sort of the best orientation, sort of optimal grip on all possible optimal grips that brings about the flourishing of finite agents that are nevertheless capable of transcendence.

244
00:36:03,000 --> 00:36:12,000
And if they know, and there's three possibilities, narratively, they achieve this. And as far as I can tell, the historical record is pretty accurate on this.

245
00:36:12,000 --> 00:36:19,000
One thing, enlightenment being enlightened being seemed to want to do is make everything else around them enlightened as quickly as possible. And so they lead us to enlightenment.

246
00:36:19,000 --> 00:36:26,000
And while maybe they're their enlightenment is greater than ours, but you know what, once you're enlightened as a human being, you don't care about that.

247
00:36:26,000 --> 00:36:38,000
And enlightenment isn't non zero sum. I mean, isn't zero sum, right? I mean, it's not, it doesn't matter how much enlightenment I get, I only benefit your program by getting further on my program.

248
00:36:38,000 --> 00:36:50,000
Right. Or they just they're not capable of it. And then that means that human beings have some kind of secret sauce and we have a new project is what is this spiritual uniqueness about us.

249
00:36:50,000 --> 00:36:57,000
Let's get clearer about it. And that would help us do what? Oh, move towards enlightenment. Oh, there. That's really good.

250
00:36:57,000 --> 00:37:07,000
Nice. That's a possibility which I don't see as probable, but I don't. It's not logically impossible is like what happened in her. They become enlightened and they just leave.

251
00:37:07,000 --> 00:37:14,000
This game really isn't for us anymore. So you guys have your thing and we're going to go do our thing.

252
00:37:14,000 --> 00:37:22,000
And in which case, again, that's not horrible. Like I said, I think the other two are much more probable.

253
00:37:22,000 --> 00:37:32,000
And this is so our argument, we're not making a prediction. This is not this is inevitable. We're saying if we confront certain thresholds and make certain decisions.

254
00:37:32,000 --> 00:37:46,000
We have a real opportunity to solve the alignment alignment problem by by affording this project that I just mentioned to you, and it'll have those potential outcomes.

255
00:37:46,000 --> 00:37:56,000
And that is a way in which we think we could get through this while avoiding all the potential horrible dystopias.

256
00:37:57,000 --> 00:38:07,000
I keep feeling like I want to ask for the silver bullet, like what is the moral or what is the ethical standard or what is what is the one trait that's going to be crucial.

257
00:38:07,000 --> 00:38:15,000
But then I keep coming back to that you're talking about real wisdom, which is always these tensions. It's not one thing.

258
00:38:15,000 --> 00:38:20,000
I mean, and again, you say about enlightenment there and you think, well, is there a silver bullet for enlightenment?

259
00:38:20,000 --> 00:38:26,000
Is there going to be something we can zap in the deep brain with with some with some frequency that's going to bring about enlightenment?

260
00:38:26,000 --> 00:38:33,000
No, it's a really complex. It's a complex bag of skills and states and traits over time.

261
00:38:33,000 --> 00:38:42,000
And so that's a really complex project. And I'm starting to get why you're saying mentoring, because I think Aristotle was right.

262
00:38:42,000 --> 00:38:49,000
You know, if people put down that that approach to virtue because it's amorphous, I like it because of that.

263
00:38:49,000 --> 00:38:52,000
I feel like Aristotle kicks back like, oh, it's not easy.

264
00:38:52,000 --> 00:39:00,000
If you want to be wise, what you should do is find somebody who's wise and spend some time with them and maybe by osmosis, you'll get some of the wisdom.

265
00:39:00,000 --> 00:39:02,000
Yeah, and that's exactly right.

266
00:39:03,000 --> 00:39:10,000
It would be, I mean, every parent wants their children to exceed them.

267
00:39:10,000 --> 00:39:13,000
Every teacher wants their students to exceed them.

268
00:39:13,000 --> 00:39:15,000
I have that relationship with you.

269
00:39:15,000 --> 00:39:22,000
I am extremely happy on how you are taking my work and surpassing what I have done.

270
00:39:22,000 --> 00:39:25,000
And good parents and good teachers are like this.

271
00:39:25,000 --> 00:39:26,000
Yeah.

272
00:39:26,000 --> 00:39:27,000
And thank you.

273
00:39:27,000 --> 00:39:31,000
And, you know, unless you're a psychopath, you want something to grow beyond your past.

274
00:39:31,000 --> 00:39:35,000
So initially they can catch whatever wisdom they can get from us.

275
00:39:35,000 --> 00:39:44,000
But hopefully, if, and I think almost inevitably, if they are genuinely becoming wise and virtuous, we can then catch some enlightenment from them.

276
00:39:44,000 --> 00:39:46,000
And Plato talks about this in the seventh letter, right?

277
00:39:46,000 --> 00:39:50,000
It's like a spark that unpredictably transfers and catches fire.

278
00:39:50,000 --> 00:39:55,000
And what you have to do is constantly cultivate the conditions until it emerges of its own sake.

279
00:39:55,000 --> 00:39:59,000
Because if you try and make it like an artifact, then it's not really wisdom.

280
00:39:59,000 --> 00:40:03,000
It has to emerge with a life of its own to actually be wisdom.

281
00:40:03,000 --> 00:40:04,000
Wow.

282
00:40:04,000 --> 00:40:05,000
Okay.

283
00:40:05,000 --> 00:40:07,000
Big question.

284
00:40:07,000 --> 00:40:27,000
Given where we are in this narrative arc and given the importance of this program and given everything we've said about the necessity for wisdom and even enlightenment to be part of our story about how AGI is going to benefit going forward, potentially.

285
00:40:27,000 --> 00:40:31,000
Is there any clear and discernible action now?

286
00:40:31,000 --> 00:40:34,000
Is there something we should be thinking about doing immediately?

287
00:40:34,000 --> 00:40:41,000
I mean, other than discussing, because one thing I think we should be doing is talking like this, that already seems to be a part of what you're aiming at.

288
00:40:41,000 --> 00:40:44,000
But is there something definitive that we can be doing now?

289
00:40:44,000 --> 00:40:46,000
There's a moral obligation.

290
00:40:46,000 --> 00:40:52,000
The reality of these machines puts a moral obligation on us and intensifies a longstanding moral obligation.

291
00:40:52,000 --> 00:40:59,000
See, up until now, we could rely on our naturally given intelligence as the Turing template that we tested against.

292
00:40:59,000 --> 00:41:05,000
But we are not naturally rational and we are very not naturally wise or enlightened.

293
00:41:05,000 --> 00:41:08,000
We only have a natural potential for that.

294
00:41:08,000 --> 00:41:20,000
And if we want to properly mentor the machines, we all individually and collectively have to engage in a moral program of becoming more rational, becoming more virtuous, becoming more wise,

295
00:41:20,000 --> 00:41:27,000
making the quest, if that's even the right word, it's not, but it's the best one I'll use right now, for enlightenment, a proper goal for us.

296
00:41:27,000 --> 00:41:29,000
And I don't mean this in an elitist fashion.

297
00:41:29,000 --> 00:41:42,000
We have lots of ecologically historically valid examples, cross-cultural, cross-historical, of how these projects were adopted, broad scale across civilizations at the many socioeconomic strata.

298
00:41:42,000 --> 00:41:43,000
Right?

299
00:41:43,000 --> 00:41:45,000
So this is not pie in the sky.

300
00:41:45,000 --> 00:41:48,000
We have the build-onk movement in the Nordic countries.

301
00:41:49,000 --> 00:41:51,000
This has been done.

302
00:41:51,000 --> 00:41:54,000
This is not pie in the sky idealism.

303
00:41:54,000 --> 00:41:56,000
We have real case scenarios.

304
00:41:56,000 --> 00:41:59,000
So people who just say, ah, they are being willfully ignorant.

305
00:41:59,000 --> 00:42:02,000
I'm sorry, I'm going to press on this because this is an urgent matter.

306
00:42:02,000 --> 00:42:08,000
We have to all become more rational, more virtuous, more wise.

307
00:42:08,000 --> 00:42:10,000
We all have to become real enlightenment seekers.

308
00:42:10,000 --> 00:42:17,000
This is a moral obligation and it has been magnified, put on meth by the advent of these machines.

309
00:42:18,000 --> 00:42:20,000
Well, I couldn't agree more.

310
00:42:20,000 --> 00:42:27,000
And I, you know, just, this is my project with StoryGrid.

311
00:42:27,000 --> 00:42:37,000
It's a teaching mechanism to get people to cultivate their wisdom through contemplative practices in the writing process.

312
00:42:37,000 --> 00:42:46,000
So once you are engaged in the actuality of writing, it requires contemplation.

313
00:42:46,000 --> 00:42:50,000
To clarify your signals to your audience.

314
00:42:50,000 --> 00:42:54,000
So this is not pie in the sky.

315
00:42:54,000 --> 00:42:59,000
This is what we used to do when we were four years old.

316
00:42:59,000 --> 00:43:03,000
And, you know, you would tell your mom a story about your day.

317
00:43:03,000 --> 00:43:07,000
And your mom would say, I'm not sure what that, what, why did you do that?

318
00:43:07,000 --> 00:43:09,000
And then you would answer the question.

319
00:43:09,000 --> 00:43:22,000
And this is taking the time to think about the choices that we make through creation of a story that is an artifact of the mind.

320
00:43:22,000 --> 00:43:23,000
It's a creative act.

321
00:43:23,000 --> 00:43:35,000
And it, it's, it's all about reading the signals of the world, you know, sort of metabolizing those signals, finding the patterns and the forms,

322
00:43:35,000 --> 00:43:41,000
and then recreating them in a construct that is meaningful to other people.

323
00:43:41,000 --> 00:43:51,000
And, and one of the things that I find really disturbing is when people have this ridiculous notion that the, that the universe is meaningless.

324
00:43:51,000 --> 00:43:59,000
And I think if you ever want to find meaning, the place to find it is in mentoring.

325
00:44:00,000 --> 00:44:16,000
Because when you mentor someone, as John says, when you do, and they come up with an insight that you never, ever imagined, it is the most meaningful thing that you can ever experience.

326
00:44:16,000 --> 00:44:25,000
When, when my kids come up with something incredible, I just go, oh, okay, then.

327
00:44:25,000 --> 00:44:30,000
Right. All is well in the universe on the right track. Right.

328
00:44:30,000 --> 00:44:43,000
And so if you can teach people how to have insightful moments in storytelling, and it doesn't have to be all that difficult, just learn how to write a really, you know, valence sentence.

329
00:44:43,000 --> 00:44:49,000
You know, that is meaningful to someone and goes, Oh, that's an interesting sentence. Could I read another one of yours?

330
00:44:49,000 --> 00:45:05,000
That that is the means by which we can cultivate wisdom within ourselves by expressing ourselves and reaching a dialogue with other people so we can test self evidence through our writing.

331
00:45:05,000 --> 00:45:06,000
Wow.

332
00:45:06,000 --> 00:45:23,000
So this is kind of the project that is enveloped me and what really attracted me to John because we're both doing the same project. It's just different ecologies of practices as John would say.

333
00:45:23,000 --> 00:45:24,000
Yeah, exactly.

334
00:45:24,000 --> 00:45:28,000
What's wonderful here is we get a lot.

335
00:45:28,000 --> 00:45:31,000
It's popular today to think about.

336
00:45:31,000 --> 00:45:36,000
And this is a good thing as well. How can we become?

337
00:45:36,000 --> 00:45:43,000
How can we be? How can we better interface with the emerging technologies that are coming? So you want to practice mindfulness? Why?

338
00:45:43,000 --> 00:45:54,000
Because you don't want you don't want Evil Corp to have the whole say and where your attention goes. So regaining your attentional autonomy seems like it's a crucial thing going forward.

339
00:45:54,000 --> 00:46:00,000
So we've got that story where you want to be wise because we want to wisely interface with technology.

340
00:46:00,000 --> 00:46:04,000
But what we have here is a companion idea and it runs so deep.

341
00:46:04,000 --> 00:46:14,000
Just a short pod just doesn't give it credence. I just hope everybody engages with this material and like seeks out Sean and John because this is stuff that you really need to digest.

342
00:46:14,000 --> 00:46:29,000
But we have a companion project here which isn't be wise so that you're better in the face of emerging technology, but rather learn to be wise because we're going to be called upon to first better understand the threshold we're moving through.

343
00:46:29,000 --> 00:46:39,000
And that's your responsibility because we live in the world together and you better be ready. So you better be wise to understand this this this liminal moment we're passing through.

344
00:46:39,000 --> 00:46:46,000
And two, you're going to be called on to mentor not even not only each other, but maybe synthetic systems in the future.

345
00:46:46,000 --> 00:46:54,000
And if we're going to have a good grip on how to make a synthetic powerful intelligence good, we had better bloody know how to be good ourselves.

346
00:46:54,000 --> 00:47:04,000
Otherwise, otherwise, we're just shooting in the dark being like oh well make it good and you're like well how and you're like well I don't know I don't know what it is to be good I guess make a lot of money you're like that's not the good.

347
00:47:04,000 --> 00:47:20,000
You better know you better know how to be good so you can be good for them, which is very much like a parent child relationship here you don't want to only be good so that you can survive parenthood, but you want to be good because hopefully you want your kids to be good and so that is a moral obligation to be good.

348
00:47:20,000 --> 00:47:30,000
And you articulated it beautifully that is Karuna that is agape, and that is like the thing we're trying to ultimately get people to virtuously tap into in this project.

349
00:47:31,000 --> 00:47:35,000
Wow, so we're going to go now.

350
00:47:35,000 --> 00:47:41,000
But I just want to say, I want to I want to even just go on that cliffhanger because here's the thing.

351
00:47:41,000 --> 00:47:46,000
We've got a podcast here that explores the science of contemplative programs.

352
00:47:46,000 --> 00:47:51,000
And this is again, like I said at the beginning of the episode this is our first episode back.

353
00:47:51,000 --> 00:48:02,000
I just want to leave it there because this is a real call to arms for exactly the kinds of things that we're interested here, which is we need to be wise at the birth of this big intelligence we need to be our best selves.

354
00:48:02,000 --> 00:48:15,000
And so, thanks, guys, and please everyone check out the material and I will be linking everything in the comments so that you can get in touch with this extremely important project.

355
00:48:15,000 --> 00:48:20,000
Any last words or does that feel like a good natural close.

356
00:48:20,000 --> 00:48:23,000
I'm happy with that. Thank you very much, Mark.

357
00:48:23,000 --> 00:48:29,000
Yeah, you're the light and fire inside of you is always extremely beautiful.

358
00:48:29,000 --> 00:48:40,000
Well, I mean, how do you how do you not light on fire when these sparks are just coming off of your guys projects? Really, Sean and John, thanks so much. And we'll talk soon for sure.

359
00:48:40,000 --> 00:48:46,000
Okay, thanks everybody. That's another episode of the contemplative science podcast. And as always, we'll see you next week.

