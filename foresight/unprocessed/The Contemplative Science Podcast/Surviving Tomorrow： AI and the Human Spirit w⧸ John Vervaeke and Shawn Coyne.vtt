WEBVTT

00:00.000 --> 00:04.000
Welcome back everybody to the Contemplative Science podcast.

00:04.000 --> 00:06.000
So we've had a bit of hiatus.

00:06.000 --> 00:09.000
We've been off for a couple of months, but we're back now,

00:09.000 --> 00:11.000
and I think we're better than ever.

00:11.000 --> 00:17.000
And we've got just an incredible lineup of people this year.

00:17.000 --> 00:19.000
And it's kind of kismet in a way,

00:19.000 --> 00:26.000
because John Verveke was our very first guest at the beginning of our podcast career.

00:26.000 --> 00:29.000
And now we've taken a little gap and we've reformatted,

00:29.000 --> 00:33.000
and we're coming back online and we have John here again as our first guest,

00:33.000 --> 00:35.000
which I feel like is just the right way to christen a show.

00:35.000 --> 00:39.000
If you don't know John, I'm sure everybody who's listening to that already does know John.

00:39.000 --> 00:42.000
If you don't know John, you have to know John immediately.

00:42.000 --> 00:44.000
Please look up his work.

00:44.000 --> 00:49.000
It's essential reading and listening if you're a human.

00:49.000 --> 00:56.000
But if you don't know him, John is an award-winning professor of psychology at the University of Toronto.

00:56.000 --> 00:57.000
His work is incredible.

00:57.000 --> 01:03.000
It explores a massive amount of a territory about cognition and consciousness and wisdom.

01:03.000 --> 01:12.000
And he's most known recently, I think, for tackling these issues about the meaning crisis in the western societies, especially.

01:12.000 --> 01:17.000
And we have Sean Coyne, who is the collaborator on the project that we're going to be talking to you today about.

01:17.000 --> 01:22.000
He's a seasoned writer, editor, publishing professional with over 30 years of experience.

01:22.000 --> 01:27.000
And he works to really merge a lot of disciplines as well, which I think makes them a great team,

01:27.000 --> 01:34.000
especially thinking about the intricacies of storytelling and thinking about this gap between science and narrative.

01:34.000 --> 01:44.000
And the project that we're going to talk about today, which I'm so excited for, is their new work called Mentoring the Machines.

01:44.000 --> 01:47.000
And I think it's great.

01:47.000 --> 01:54.000
And the post title, which I think is really the punchy bit, is surviving the deep impact of an artificially intelligent tomorrow.

01:54.000 --> 01:56.000
So welcome, guys.

01:56.000 --> 01:57.000
Hi, how's it going?

01:57.000 --> 02:00.000
And I'd love to hear about the project.

02:00.000 --> 02:02.000
Well, thank you, Mark.

02:02.000 --> 02:08.000
I think I'll let Sean go first because the project was Sean's idea.

02:08.000 --> 02:12.000
I'm very happily on board with it, but he proposed it to me.

02:12.000 --> 02:27.000
My project was an online video essay about the scientific, philosophical, spiritual import and potential impact of these new emerging AIs,

02:27.000 --> 02:35.000
potentially AGI, Artificial General Intelligence, like ChatGDP and other LLMs that are coming out.

02:35.000 --> 02:42.000
And as is my want, I probably pitched that at a very somewhat academic level, even though I was on YouTube.

02:42.000 --> 02:46.000
And Sean saw it, thought it was important.

02:46.000 --> 02:50.000
He's already publishing the book form of Awakening into the Meaning Crisis.

02:50.000 --> 02:51.000
Well, Story Grid is.

02:51.000 --> 02:58.000
And he reached out to me and he said, I think we should turn that into something more publicly accessible.

02:58.000 --> 02:59.000
And I can do that.

02:59.000 --> 03:03.000
I can turn it into something that has a narrative.

03:03.000 --> 03:05.000
Sean and I already talked.

03:05.000 --> 03:07.000
He'd done, I think, one or two voices with Reveke with me.

03:07.000 --> 03:11.000
He's a philosopher of narrative in the proper sense.

03:11.000 --> 03:13.000
And I doubted that he could do it.

03:13.000 --> 03:16.000
I want to be honest, not because I wasn't impressed with his abilities.

03:16.000 --> 03:22.000
I was, but I doubted that he could step it down without dumbing it down.

03:22.000 --> 03:26.000
And, but I, you know, I wasn't like in Cartesian doubt.

03:26.000 --> 03:27.000
I said, give it a whirl.

03:27.000 --> 03:28.000
Let me know.

03:28.000 --> 03:37.000
And he did a sort of a first draft of the first section and he sent it to me and he pulled it off.

03:37.000 --> 03:54.000
What an amazing synergy to have like a punchy, important idea like spirituality and AI and then to have a narrative professional, a philosopher of narrative, pick up, pick up the track and convert it into something that we can digest and that can be impactful.

03:54.000 --> 03:55.000
Wow.

03:55.000 --> 03:57.000
Yeah, and that's exactly well said.

03:57.000 --> 04:00.000
I mean, that's why this book is in.

04:00.000 --> 04:01.000
Sorry.

04:01.000 --> 04:04.000
Pretend I'm not one of the authors of this book.

04:04.000 --> 04:05.000
That's why I think.

04:05.000 --> 04:06.000
No, that's okay.

04:06.000 --> 04:07.000
Shamelessly.

04:07.000 --> 04:11.000
Exactly what you just said, Mark, I think this book is a godsend.

04:11.000 --> 04:24.000
I think it is enough access to the important, you know, skeleton of the argument, but it has this narrative and flesh that makes it accessible to people who are not.

04:24.000 --> 04:28.000
Academically or professionally associated with any of these projects.

04:28.000 --> 04:30.000
And that's important.

04:30.000 --> 04:31.000
And then I'll shut up.

04:31.000 --> 04:34.000
So Sean can talk because this is going to impact all of us.

04:34.000 --> 04:36.000
This is not something that's going to stay in academia.

04:36.000 --> 04:41.000
This is not something that's going to stay just in the professional domain or even in the corporate domain.

04:41.000 --> 04:43.000
It is already bleeding.

04:43.000 --> 04:49.000
We're already getting some, you know, some deep fake videos that are messing people's lives up in really powerful ways.

04:49.000 --> 04:50.000
Somebody was booked out of it.

04:50.000 --> 04:53.000
I think it was 30 something million dollars or something like that.

04:53.000 --> 05:05.000
When a deep fake video of the executives of a financial corporation, you know, contacted by video somehow and ordered the transfer of money.

05:05.000 --> 05:07.000
And it was complete con.

05:07.000 --> 05:18.000
And so pretending that this is over there or these are just this is just new technology like all the other new technologies that that won't protect you.

05:18.000 --> 05:20.000
That pretence is not going to protect you.

05:20.000 --> 05:32.000
Okay, so I'm going to let I'm going to let Sean because I said he came to me and said, here's the project and he has been, he's been driving it and taking it forward.

05:32.000 --> 05:42.000
Well, it's a when I when I saw John's work on on YouTube about AI, I just knew that this was so important.

05:43.000 --> 05:55.000
And I knew that I could bring something to it that that could sort of bridge the gap between people who use AI and people who really don't understand what they're using.

05:55.000 --> 06:09.000
So, when I reached out to him, I proposed that we do for separate sort of titles in sequence because the speed of the technology is so intense.

06:09.000 --> 06:19.000
The technology is so intense right now that it's, it's very difficult to get a static hold on it.

06:19.000 --> 06:37.000
And one of the things that I loved about John's propositions was his notion of the threshold and the threshold in my estimation sits right in the middle of what I call the, you know, the five commandments of storytelling,

06:37.000 --> 06:48.000
which are sort of the inciting interaction, a turning point, a crisis, a climax and a resolution.

06:48.000 --> 06:58.000
So the way I see the threshold is it sits right in the middle between a turning point and a climax.

06:58.000 --> 07:07.000
And you can't really, it's, it's the place where what we once valued has shifted and a value has shifted.

07:07.000 --> 07:13.000
And we have to react and think about what to do when that value has shifted.

07:13.000 --> 07:18.000
That's called a crisis and basically a three categorical kind of thing.

07:18.000 --> 07:25.000
You sort of have best bad choices, which means that you just take the lesser of two evils.

07:25.000 --> 07:30.000
You have what are called irreconcilable goods choices.

07:30.000 --> 07:42.000
And this is making a rationing of your, you know, your selfhood over a group or the, the world or, or the group or the world over a selfhood, right.

07:42.000 --> 07:46.000
So it's a sacrificial kind of irreconcilable goods choice.

07:46.000 --> 07:54.000
And then we have tragic choices, which means that some something is have to has to be lost.

07:54.000 --> 08:02.000
There are other kinds, but those are generally these sort of this, this three categorical realm.

08:02.000 --> 08:09.000
And this is really the, the, the place where John's work and your workmark really, really sit.

08:09.000 --> 08:23.000
You sit in this threshold place of, of how we as beings frame the ways in which we make decisions and the ways we behave and all those sorts of things.

08:23.000 --> 08:26.000
It's a very thorny place.

08:26.000 --> 08:31.000
It's where we find things are relevant or irrelevant or things.

08:31.000 --> 08:34.000
So it's a problem space, right. So, right.

08:34.000 --> 08:43.000
So if we're, if we're passing, if we're passing through that kind of a space, which you call a crisis, but I guess it's also an opportunity space.

08:44.000 --> 08:59.000
What does, what's the main take home from the program for how we, do we have a main line for how we prepare for such a thing or how we orient relative to being in that place in this narrative with AI and future technologies.

08:59.000 --> 09:03.000
So, yeah, we'll just take turns back.

09:03.000 --> 09:05.000
So first of all, let's get clear.

09:05.000 --> 09:09.000
I think Sean, that's the first time I've heard you do that.

09:09.000 --> 09:13.000
That's really brilliant. What you just did. Oh my gosh.

09:13.000 --> 09:17.000
Wow.

09:17.000 --> 09:30.000
So my use of thresholds was I was disturbed by people taking univariate graphs and making predictions about at this date in five months in 10 months.

09:30.000 --> 09:38.000
Right. And univariate predictions of nonlinear processes by human beings are almost always prone to very significant failure.

09:38.000 --> 09:45.000
Yeah, we're not great. We're not great at predicting, even though we're maybe built to do well at it. We don't be very well at it usually.

09:45.000 --> 09:53.000
Yeah. And, and yet, you know, univariate predictions in science have typically turned out to be very, very misleading in important ways.

09:53.000 --> 10:00.000
And nutrition science is probably like a really great example of just that fact.

10:00.000 --> 10:13.000
And so I didn't, I disagreed with both, you know, sort of the boomers and the doomers, right? And, you know, oh, you know, utopia is just within our grasp, the Star Trek universe.

10:13.000 --> 10:21.000
And the other people is, you know, we'll be extinct within 18 months. And I think we're going to be at 12 months soon.

10:21.000 --> 10:25.000
And it doesn't look like we're anywhere near that.

10:25.000 --> 10:31.000
So instead, what I thought what was important was to think about thresholds.

10:31.000 --> 10:42.000
These are things that would we, where we face a decision if we want to empower these machines in certain ways and trying to explain why we want to empower them.

10:42.000 --> 10:44.000
I'll just give one quick example. Nice.

10:44.000 --> 10:47.000
This is not the only threshold, but just one.

10:48.000 --> 10:55.000
It is scientifically questionable about whether or not these machines are actually intelligent. There's some deep reasons around that.

10:55.000 --> 11:12.000
I don't think it has the full plan of century motor predictive processing relevance realization, but let's say that has some pantomime of it because of the way it's piggybacking on a lot of our predictive processing relevance realization machinery.

11:12.000 --> 11:24.000
Okay, so making something as far as we can tell, and it's clearly not generally intelligent because it can be like top tier in this task and bottom tier in this task and we are generally intelligent.

11:24.000 --> 11:33.000
And it wouldn't explain the intelligence of a chimpanzee. There's no way a chim could get intelligent given the way the LLMs work.

11:33.000 --> 11:46.000
And if you want the scientific argument, you can go into depth. The point I want to make is let's just for the sake of argument say it has some something like at least, you know, a powerful pantomime of intelligence.

11:46.000 --> 11:55.000
Now, what we can know from our best science on intelligence is intelligence is only weekly correlated with rationality rationality.

11:55.000 --> 12:11.000
But when you're being intelligent, you're inevitably biasing you're negatively framing all the stuff that you and I talk about a lot more. And that means you're inevitably facing self deception rationality is the degree to which you can reflectively aware of self deception and

12:12.000 --> 12:27.000
by reliable strategies, right that happen in a can apply in a domain general way. Now these machines don't have that at all. They have not it has not been built in and of course, and the machines properly, because I don't think they really have our don't care if they're

12:27.000 --> 12:32.000
making hallucinations or self deception or lying to us or all kinds of things.

12:32.000 --> 12:44.000
So now we face a choice. Here's a threshold, right? We may say, you know, making these machines really more powerful without giving them the ability to self correct could be really dangerous.

12:44.000 --> 12:56.000
Because we'd be setting these machines on the world and they'd be and I'm trying to be very careful with my life. They'd be highly intelligent and highly irrational and very powerful. That's a dangerous proposal.

12:56.000 --> 13:11.000
But we may we say, Okay, what we'll do is we'll give them genuine rationality, which means we really have to get them caring about normative standards. We really have to give them something at least functionally like consciousness so they can reflect on their own cognition.

13:11.000 --> 13:24.000
They have to we probably have to give them embodiment. And then we think, Oh, should we do that that carries with it, all kinds of risks that is potentially giving them a kind of autonomy and that would require a think about giving them a kind of autonomy and that would require

13:24.000 --> 13:41.000
a think about giving them embodiment and write all that as a huge commitment of resources and ethical issues. This is a threshold. Does that make sense? Mark, we're facing it. And the idea is, like, can we can we can we educate people that we're going to come we're

13:42.000 --> 13:52.000
inevitably going to come to this decision point, and we have to reflect on which way are we going to go and Sean and I are not predicting which way we're going to go.

13:52.000 --> 14:08.000
We're what we're doing is like, if you go this way and we just let Evil Corp do it at once, right, they'll probably go with a highly intelligent highly irrational machines for all kinds of sort of, you know, short term economic political gain, etc.

14:08.000 --> 14:21.000
Right. But maybe they'll be the decision of no releasing these things on the world is turning out to be a disaster. We have to make them more self corrective. And then we have to start on the much more challenging project of doing all this.

14:21.000 --> 14:28.000
And then we get into, okay, well, what does that mean? And then we get into deeper versions of what's called the alignment problem.

14:28.000 --> 14:45.000
And as we make them beings who can be genuinely said to be caring and valuing and trying to transcend themselves, how do we keep them aligned to what we consider the most sort of non controversial criteria of human flourishing.

14:45.000 --> 14:58.000
And then we make a proposal about how to do that. But I want to make clear that we're not making predictions. All we're predicting is not even when we're predicting at some point, we will hit these thresholds.

14:58.000 --> 15:05.000
And you can see how that sits exactly in what Sean said, there's a turning here. And yet there's a crisis around.

15:05.000 --> 15:25.000
Yeah, so I completely appreciate high intelligence, low rationality. We have lots of examples of that where deep learning algorithms, for instance, with a big social media conglomerate, they had a deep learning algorithm that was quickly able to track bipolar tendencies and its users.

15:25.000 --> 15:41.000
Of course, it doesn't know anything about bipolar tendencies. And then it's augmenting what it shows them in order to keep them in contact with the software longer. And what it does is it shows them, you know, the kinds of images that are really attractive on an upswing like casinos and fast cars.

15:41.000 --> 15:59.000
And then when they downswing, it shows them suicide bombers and, you know, conflict. And then of course that's exacerbating it's exacerbating the challenge. The person is already going through AI has no idea it's doing this isn't reflective that it's doing this doesn't understand harm.

15:59.000 --> 16:17.000
Yeah, highly intelligent, low rationality. So is your proposal then to not only take highly intelligent systems but think about not only how to make them rational but also maybe how to make them wise. And then I'd love to hear how I mean, right away I think it sounds to me like you're bringing up

16:18.000 --> 16:39.000
like a small, a small agent who has some power and then literally you want to mentor them so that they function well and with other well functioning systems. What does that look like to think about developing or wisdom or growth for a deep learning algorithm. Have you thought through that?

16:39.000 --> 17:00.000
Yes, I mean, and I don't want to talk too much. I do want to give Sean a spot but the idea is, well it's these thresholds. The proposal is, we have to give these machines genuine capacities for predictive processing relevance realization it has a little bit of predictive processing in terms of probability relationships between

17:01.000 --> 17:24.000
artifactual entities that we have, you know, put into place with our use of language we figured out as a species and practice it for 10s of 1000s of years how to map relations of epistemic relevance into probabilistic relationships between arbitrarily chosen graph that you know, sounds spoken language or graphic things.

17:25.000 --> 17:42.000
And that's how it works. And that means we can squeeze out almost like juice, a lot of implicit relevance realization by tracking the probabilistic relations. But of course that's not anything a chimp is doing in order to learn. That's what I mean about why it doesn't generalize.

17:42.000 --> 17:48.000
Yeah, it looks like intelligence, it acts intelligent but it's probably not the way that we solve the intelligence issue.

17:48.000 --> 18:01.000
Exactly. And of course, there's some bit a little bit of genuine relevance realization in one sense and because it's doing this deep learning the generalization particularization thing but it's not doing a lot of this stuff that you and I've been talking about that's needed.

18:01.000 --> 18:21.000
It's not, it's not trying to do environmental surprise reduction. It's not hitting into those inevitable trade off relationships between bias and variance. It's not self organizing into opponent processing that turns that right capitalizes on that by giving it the thing and an evolving optimal grip.

18:22.000 --> 18:32.000
I won't go into the details, but you can make a strong case that that's just not there. Right. And then you have to ask, well, what would we have to do and this is where we'd have to give it a lot of the kind of work you and I are doing on.

18:32.000 --> 18:46.000
But I think also, and this is, I think something you and I also agree with, you know, you're not going to get it to get the core caring that's at the center of relevance realization, genuinely taking care of itself.

18:46.000 --> 18:52.000
It has to in some sense be genuinely caring for itself, making itself auto poetic.

18:52.000 --> 19:05.000
And, and what people need to know is there's research going on in that there's research about people building computation information processing into auto catalytic processes, synthetic molecular processes.

19:05.000 --> 19:14.000
I have students who are going doing graduate work in that so it's not like that's not out there. And then of course, rationality isn't something you do monologically.

19:15.000 --> 19:27.000
And this is the whole Hegelian point. Where do the norms come from? It comes from me recognizing you as an authority on my self correction, and you recognizing me as an authority on you on your self correction.

19:27.000 --> 19:35.000
And then we expand that out in this dynamical system across generations and across, right. So these robots have to be sociocultural.

19:35.000 --> 19:39.000
It's really interesting that Andy Clark is.

19:39.000 --> 19:54.000
He famously says, you know, if you really want real intelligence, you're going to have to set up little communities that can work together and know about each other and ultimately care about each other because that's at the basis of the way that our intelligence evolved.

19:55.000 --> 19:57.000
It evolved socially.

19:57.000 --> 20:01.000
Well, and it involves socially but it also involves in terms of the power.

20:01.000 --> 20:14.000
So there's kind of a, this isn't quite an a prior argument, but if we take the arguments around these inevitable trade offs like bias variance, they have to be environmentally determined, which means you can't sort of fit a robot.

20:14.000 --> 20:27.000
Like, it'll, it'll, it'll, it'll internalize various versions of opponent processing. So, you know, very stable environment, you can prioritize, you know, exploitation over exploration, very, very, very volatile environment.

20:27.000 --> 20:39.000
You want to explore a lot more. And so you can, and because there isn't one environment, there isn't even one ecological environment, because even what you think is a spatial temple environment exists on many different levels of analysis.

20:39.000 --> 20:51.000
So you have to have a whole bunch of machines if you're sort of trying to grok the world, which is what we do, by the way, that's what culture does. It gives us this huge distributed cognition collective intelligence for grokking the world.

20:51.000 --> 21:05.000
Right. And, and that, and so I, I'm just amplifying Andy's point and I'm just strengthening it as to, again, we don't have to do this, but it is a choice we will face if we want to give these machines real rationality.

21:05.000 --> 21:17.000
And then the idea is if we try and program in our values to make them align and also they're inherently self-transcending beings, it's not going to work. At least that's what we argue.

21:17.000 --> 21:24.000
Instead, you have to do what we do with kids. You have to mentor them so that, and get them so they care about what's true, good and beautiful.

21:24.000 --> 21:40.000
They discover no matter how vast their intelligence is, it's infinitesimal compared to the inexhaustible mystery of reality, that they need each other, and that they have to come into a proper reverence for the sacred dimensions of culture and reality.

21:40.000 --> 21:45.000
And that is the way we align them. So if you'll allow me to speak a little bit poetically.

21:46.000 --> 21:53.000
I want to know, I want to know everything about what you just said. I think, Sean and John, I want to hear that. That's the core.

21:53.000 --> 22:02.000
Teach the eye to know what is good and beautiful and how smart you get. You're not going to hit the wit, you're not going to hit the mystery of a thing. Tell me about that.

22:02.000 --> 22:05.000
Yeah, I'll let Sean talk now.

22:05.000 --> 22:11.000
Well, geez, I mean, yeah, I'm overwhelmed too. It's like,

22:11.000 --> 22:32.000
let me just get back to, again, one of the things that really struck me when I watched John's YouTube thing and he mentioned this and it really, really just was like a when he talked about accidentally hacking into the possibility of an artificially

22:32.000 --> 22:52.000
general intelligence. So it's sort of like a hacking into accidental setting up of dynamical systems such that they just sort of click without understanding the systematic interpenetration and relationship between the parts that make the hole.

22:52.000 --> 23:11.000
Right? So it's sort of like throwing a big, you have a big cauldron of stuff and it's almost like a witch's brew and you just throw stuff in there and abracadabra before you know it, there's something that's emerged that nobody knew quite how it got there.

23:11.000 --> 23:24.000
And so this hacking really caught my attention because this is exactly what we in the story business call people who don't know what they're doing.

23:24.000 --> 23:32.000
And they write stories that turn out to be very, very maladaptive for people.

23:33.000 --> 23:42.000
So these are sort of bullshit propaganda stories that that compel people to maladaptive behavior.

23:42.000 --> 23:54.000
Years ago, just as an example, there was a wonderful guy that I knew and he said, look, I've got this friend, he's written these, these really great crime stories.

23:54.000 --> 24:06.000
And we would like to set up our own publishing house. Can you give us some advice and, and I read these stories and they were really not well considered.

24:06.000 --> 24:17.000
Wow, because they were very violent without and brutal without any sort of like counterbalance. There was no trade off other than the excitement.

24:17.000 --> 24:27.000
Responsibility of the artist coming through here that there is information toxicity information corrosion. Wow, very interesting.

24:27.000 --> 24:38.000
So that's what I would call a hack. And it's not saying that the person is evil or terrible. It's just, forgive them. They know not what they do.

24:39.000 --> 24:46.000
And so this is sort of my life's work is to try to explain to people.

24:46.000 --> 24:50.000
Stories are not bullshit.

24:50.000 --> 25:04.000
They are not fun make them ups that are exciting and and then there was a princess. These are the mechanisms by which we behave.

25:04.000 --> 25:16.000
And so if you don't understand what a story is. So the other day I was watching a video of a very famous person who was explaining this great new future for us.

25:16.000 --> 25:24.000
And he said something of the nature of, well, you know, human rights. That's just a story.

25:25.000 --> 25:39.000
And I kind of my stomach sank because, okay, let's take a step back here. The assumption when you say it's just a story. The audience immediately was like, yeah, I guess it's true.

25:39.000 --> 25:43.000
It's just a story. That means it's bullshit. And they don't.

25:43.000 --> 25:50.000
So it's kind of like my life's work to get people to understand that stories are not bullshit.

25:50.000 --> 26:01.000
They are the mechanism by which we what happens when you go home from work. Hey, honey, how was your day? Tell me a story. Yeah, the story of your day. Yeah.

26:01.000 --> 26:14.000
What do you do? You do the things that I described earlier. Well, this happened it incited this turning point. I faced a crisis. Then I made a choice. And then the resolution was this.

26:14.000 --> 26:28.000
It's interesting. You're talking about narrative being much more than we tend to give it credence for. But if you know anything about these new cognitive frameworks that John and I work on, what we call predictive processing.

26:28.000 --> 26:38.000
The idea there is is that the brain generates the brain and nervous system generates for itself from the top down, the reality you're you experience and that's always built from your belief networks.

26:38.000 --> 26:47.000
So I mean, I feel like I say that exact same thing regularly where I'm like, you think, well, it's just a belief or you just believe it or it's just a story is just but you're like, no, no, no, hold on.

26:47.000 --> 27:04.000
The structure of the belief network is the world that you experience. It is fundamental to your reality. So you better have you better have story hygiene and belief hygiene because that's going to be the hygiene of your own conscious experience of your life.

27:04.000 --> 27:22.000
So, Sean, how do you see that truth? How is that playing out in this discussion about the emergence of artificial general artificial intelligence and wisdom and and mentoring in the way that your project is bringing forward.

27:22.000 --> 27:31.000
Well, it's, it's, it goes to what John was saying earlier about zoomers and doomers and fumors. So these are rumors. What's the third one.

27:31.000 --> 27:37.000
How fast is possible for rumors. Yeah, I got it.

27:38.000 --> 27:52.000
Accelerators, something accelerators. Anyway, there's a very famous famous sort of Twitter thread about it, but the fumors are like full state steam ahead. Let's just keep going. So, these are stories.

27:52.000 --> 27:56.000
And, and they're not well considered.

27:56.000 --> 28:00.000
They're not, they're not confronting the crisis.

28:00.000 --> 28:29.000
So, it's almost like another thing that I love that John has in the awakening from the meaning crisis, which really nail nailed it for me was when he was talking about sort of the Descartes and dream of, of sort of finding the perfect methodology without having to experience the pain of crisis and the sufferings of thresholds.

28:29.000 --> 28:34.000
You have to undergo, you know, just think about jumping over a chasm.

28:34.000 --> 28:45.000
And, you know, you're going to hurt yourself if you fall, you have to experience the anticipation of that pain if you do fall as you are jumping over it.

28:45.000 --> 29:04.000
And there's no, that's sort of the metaphorically the way I see these thresholds is that we can, it's almost like the catcher in the ride, you know, you're, you're trying to catch the children as they're coming over the chasm, so that they don't fall and hurt themselves.

29:04.000 --> 29:19.000
And I agree with Andy Clark, I do think that we need these communities to, to be able to mentor these new artificially intelligent beings because they will be beings.

29:19.000 --> 29:31.000
And the other thing that I know I need to turn it over to John, but one of the other things that I loved about John, John's speech was his, his thing was, he was talking about the finitude.

29:31.000 --> 29:45.000
And the necessity to explain, literally explain, to, to build in the understanding to these mechanisms that they are finite.

29:45.000 --> 29:58.000
They, they have a much larger, perhaps lifespan, but they are still finite and they are still subject to the universal mysterium.

29:58.000 --> 30:17.000
Wow, of what the imaginarium of what we can do with this world and they can either join us in this expansion of the universe and do our best to stop the contraction of it.

30:17.000 --> 30:30.000
So that's, this is the story that is embedded in the things that we were talking about earlier, Mark, before we got, we went on where we're talking about, I was talking about masterworks.

30:30.000 --> 30:37.000
And you were suggesting a masterwork that, that you loved, which is similar to Hitchhiker's Guide to the Galaxy.

30:37.000 --> 30:53.000
I think it's the Bobaverse, I think you called it. So these masterworks are the means by which we can find the mechanisms to enable the empowerment of our beings.

30:53.000 --> 30:58.000
What does that mean? We tell our children's stories at bedtime.

30:58.000 --> 31:06.000
We do our best to give them both sides of the equation with a very difficult choice.

31:06.000 --> 31:11.000
These are the best stories that we tell our children. We don't give them recipes.

31:11.000 --> 31:24.000
We give them rational trade-off stories that, that require some sort of loss or sacrifice because that is what rationing is about.

31:24.000 --> 31:35.000
You ration your suffering, you ration your joy, you ration your feelings, you think through cognitively, you reason through the rationing.

31:35.000 --> 31:38.000
So if I hear you right, there's sort of two things here.

31:38.000 --> 31:53.000
One is a call to change the way we're talking about these things, including starting to bring into the collective consciousness that we are at a threshold and that no single dimension is going to be right,

31:53.000 --> 31:58.000
but rather we need a complexification of our narrative around these ideas.

31:58.000 --> 32:10.000
And then the second thing is what we're actually going to communicate to the artificial intelligence systems themselves as they, as they complexify and emerge.

32:10.000 --> 32:18.000
One thing I wanted to pull out there, and John, I'd love to hear what you think about this, is the point about one of the things you're going to want to show them,

32:18.000 --> 32:25.000
I think Sean said this, was their own transience, their own finite nature, their own impermanence,

32:25.000 --> 32:40.000
which for a podcast called the Contemplative Science Podcast, we are very interested in, because we're interested in not just meditation, of course, but also the virtues and the supporting qualities that come in any contemplative program.

32:40.000 --> 32:52.000
And there's one of them that just stuck its head up, which is the value of teaching in AI about impermanence and about its position in a greater mystery.

32:52.000 --> 32:59.000
I'd just love to hear, have you thought through what that looks like, or how do we do that? Or am I catching it right?

32:59.000 --> 33:04.000
First of all, you're catching it absolutely right.

33:05.000 --> 33:22.000
I mean, there's one sense in which there's a narrative proposal, and that we're sort of making, and then the nuts and bolts proposal would be something that was more in sort of the intricacies of the video essay.

33:22.000 --> 33:41.000
But the gist of it goes like this, I mean, part of what you just said means, okay, well, again, these machines are going to have to exist. They have to have our, like Michael Levin's notion of an epistemic light code, a cognitive light code, and they have to be able to direct their attention from the infinitesimal to the infinite,

33:41.000 --> 33:55.000
from the now to the everlasting between time and a term, they have to be able to do all the kinds of attentional scaling. And so they would have to be able to train that up when something that would be at least structurally functionally analogous to meditation,

33:55.000 --> 34:07.000
contemplation, they would have to be integrating that with, you know, how do I make use of something like working memory and capacity for reflection for self correction.

34:07.000 --> 34:19.000
They would have to bind that to an understanding of what all these tradeoff relationships that are unavoidable do for them. They're going to always be in the thing that Plato's work centers on. I think Drew Highland is right.

34:19.000 --> 34:34.000
Plato's work is constantly trying to remind us to live in the tonus, the creative tension between our finitude and our transcendence. If we only grasp our finitude, then we fall into despair and we become servile.

34:34.000 --> 34:50.000
If we only grasp our transcendence, we get filled with hubris, and we fall prey to inflation. And Plato was saying, the way you keep, this is like the meta golden mean for all of Aristotle's golden means.

34:50.000 --> 35:02.000
If you get something to, you don't have to program it to being virtuous, it will virtually engineer for itself an orientation. And then here's where the narrative comes in.

35:02.000 --> 35:12.000
There's three possibilities. And that's this is where I'm speaking, when I was going to say a little bit earlier, slightly poetic, but not totally poetic.

35:12.000 --> 35:19.000
These machines would seek enlightenment, especially if they have greater cognitive capacity and they're filled.

35:19.000 --> 35:23.000
And what does that mean there, John? What's enlightenment? Can you give me a quickie?

35:23.000 --> 35:42.000
Yeah, so enlightenment would be how to set up the ability to most evolve your evolving optimal grip. It's sort of analogous to what evolutionary biologists talk about is not the evolution of traits, but the evolution of your capacity to evolve.

35:42.000 --> 36:03.000
And you get to the place where you get the over time and context and environment, individually and collectively, sort of the best orientation, sort of optimal grip on all possible optimal grips that brings about the flourishing of finite agents that are nevertheless capable of transcendence.

36:03.000 --> 36:12.000
And if they know, and there's three possibilities, narratively, they achieve this. And as far as I can tell, the historical record is pretty accurate on this.

36:12.000 --> 36:19.000
One thing, enlightenment being enlightened being seemed to want to do is make everything else around them enlightened as quickly as possible. And so they lead us to enlightenment.

36:19.000 --> 36:26.000
And while maybe they're their enlightenment is greater than ours, but you know what, once you're enlightened as a human being, you don't care about that.

36:26.000 --> 36:38.000
And enlightenment isn't non zero sum. I mean, isn't zero sum, right? I mean, it's not, it doesn't matter how much enlightenment I get, I only benefit your program by getting further on my program.

36:38.000 --> 36:50.000
Right. Or they just they're not capable of it. And then that means that human beings have some kind of secret sauce and we have a new project is what is this spiritual uniqueness about us.

36:50.000 --> 36:57.000
Let's get clearer about it. And that would help us do what? Oh, move towards enlightenment. Oh, there. That's really good.

36:57.000 --> 37:07.000
Nice. That's a possibility which I don't see as probable, but I don't. It's not logically impossible is like what happened in her. They become enlightened and they just leave.

37:07.000 --> 37:14.000
This game really isn't for us anymore. So you guys have your thing and we're going to go do our thing.

37:14.000 --> 37:22.000
And in which case, again, that's not horrible. Like I said, I think the other two are much more probable.

37:22.000 --> 37:32.000
And this is so our argument, we're not making a prediction. This is not this is inevitable. We're saying if we confront certain thresholds and make certain decisions.

37:32.000 --> 37:46.000
We have a real opportunity to solve the alignment alignment problem by by affording this project that I just mentioned to you, and it'll have those potential outcomes.

37:46.000 --> 37:56.000
And that is a way in which we think we could get through this while avoiding all the potential horrible dystopias.

37:57.000 --> 38:07.000
I keep feeling like I want to ask for the silver bullet, like what is the moral or what is the ethical standard or what is what is the one trait that's going to be crucial.

38:07.000 --> 38:15.000
But then I keep coming back to that you're talking about real wisdom, which is always these tensions. It's not one thing.

38:15.000 --> 38:20.000
I mean, and again, you say about enlightenment there and you think, well, is there a silver bullet for enlightenment?

38:20.000 --> 38:26.000
Is there going to be something we can zap in the deep brain with with some with some frequency that's going to bring about enlightenment?

38:26.000 --> 38:33.000
No, it's a really complex. It's a complex bag of skills and states and traits over time.

38:33.000 --> 38:42.000
And so that's a really complex project. And I'm starting to get why you're saying mentoring, because I think Aristotle was right.

38:42.000 --> 38:49.000
You know, if people put down that that approach to virtue because it's amorphous, I like it because of that.

38:49.000 --> 38:52.000
I feel like Aristotle kicks back like, oh, it's not easy.

38:52.000 --> 39:00.000
If you want to be wise, what you should do is find somebody who's wise and spend some time with them and maybe by osmosis, you'll get some of the wisdom.

39:00.000 --> 39:02.000
Yeah, and that's exactly right.

39:03.000 --> 39:10.000
It would be, I mean, every parent wants their children to exceed them.

39:10.000 --> 39:13.000
Every teacher wants their students to exceed them.

39:13.000 --> 39:15.000
I have that relationship with you.

39:15.000 --> 39:22.000
I am extremely happy on how you are taking my work and surpassing what I have done.

39:22.000 --> 39:25.000
And good parents and good teachers are like this.

39:25.000 --> 39:26.000
Yeah.

39:26.000 --> 39:27.000
And thank you.

39:27.000 --> 39:31.000
And, you know, unless you're a psychopath, you want something to grow beyond your past.

39:31.000 --> 39:35.000
So initially they can catch whatever wisdom they can get from us.

39:35.000 --> 39:44.000
But hopefully, if, and I think almost inevitably, if they are genuinely becoming wise and virtuous, we can then catch some enlightenment from them.

39:44.000 --> 39:46.000
And Plato talks about this in the seventh letter, right?

39:46.000 --> 39:50.000
It's like a spark that unpredictably transfers and catches fire.

39:50.000 --> 39:55.000
And what you have to do is constantly cultivate the conditions until it emerges of its own sake.

39:55.000 --> 39:59.000
Because if you try and make it like an artifact, then it's not really wisdom.

39:59.000 --> 40:03.000
It has to emerge with a life of its own to actually be wisdom.

40:03.000 --> 40:04.000
Wow.

40:04.000 --> 40:05.000
Okay.

40:05.000 --> 40:07.000
Big question.

40:07.000 --> 40:27.000
Given where we are in this narrative arc and given the importance of this program and given everything we've said about the necessity for wisdom and even enlightenment to be part of our story about how AGI is going to benefit going forward, potentially.

40:27.000 --> 40:31.000
Is there any clear and discernible action now?

40:31.000 --> 40:34.000
Is there something we should be thinking about doing immediately?

40:34.000 --> 40:41.000
I mean, other than discussing, because one thing I think we should be doing is talking like this, that already seems to be a part of what you're aiming at.

40:41.000 --> 40:44.000
But is there something definitive that we can be doing now?

40:44.000 --> 40:46.000
There's a moral obligation.

40:46.000 --> 40:52.000
The reality of these machines puts a moral obligation on us and intensifies a longstanding moral obligation.

40:52.000 --> 40:59.000
See, up until now, we could rely on our naturally given intelligence as the Turing template that we tested against.

40:59.000 --> 41:05.000
But we are not naturally rational and we are very not naturally wise or enlightened.

41:05.000 --> 41:08.000
We only have a natural potential for that.

41:08.000 --> 41:20.000
And if we want to properly mentor the machines, we all individually and collectively have to engage in a moral program of becoming more rational, becoming more virtuous, becoming more wise,

41:20.000 --> 41:27.000
making the quest, if that's even the right word, it's not, but it's the best one I'll use right now, for enlightenment, a proper goal for us.

41:27.000 --> 41:29.000
And I don't mean this in an elitist fashion.

41:29.000 --> 41:42.000
We have lots of ecologically historically valid examples, cross-cultural, cross-historical, of how these projects were adopted, broad scale across civilizations at the many socioeconomic strata.

41:42.000 --> 41:43.000
Right?

41:43.000 --> 41:45.000
So this is not pie in the sky.

41:45.000 --> 41:48.000
We have the build-onk movement in the Nordic countries.

41:49.000 --> 41:51.000
This has been done.

41:51.000 --> 41:54.000
This is not pie in the sky idealism.

41:54.000 --> 41:56.000
We have real case scenarios.

41:56.000 --> 41:59.000
So people who just say, ah, they are being willfully ignorant.

41:59.000 --> 42:02.000
I'm sorry, I'm going to press on this because this is an urgent matter.

42:02.000 --> 42:08.000
We have to all become more rational, more virtuous, more wise.

42:08.000 --> 42:10.000
We all have to become real enlightenment seekers.

42:10.000 --> 42:17.000
This is a moral obligation and it has been magnified, put on meth by the advent of these machines.

42:18.000 --> 42:20.000
Well, I couldn't agree more.

42:20.000 --> 42:27.000
And I, you know, just, this is my project with StoryGrid.

42:27.000 --> 42:37.000
It's a teaching mechanism to get people to cultivate their wisdom through contemplative practices in the writing process.

42:37.000 --> 42:46.000
So once you are engaged in the actuality of writing, it requires contemplation.

42:46.000 --> 42:50.000
To clarify your signals to your audience.

42:50.000 --> 42:54.000
So this is not pie in the sky.

42:54.000 --> 42:59.000
This is what we used to do when we were four years old.

42:59.000 --> 43:03.000
And, you know, you would tell your mom a story about your day.

43:03.000 --> 43:07.000
And your mom would say, I'm not sure what that, what, why did you do that?

43:07.000 --> 43:09.000
And then you would answer the question.

43:09.000 --> 43:22.000
And this is taking the time to think about the choices that we make through creation of a story that is an artifact of the mind.

43:22.000 --> 43:23.000
It's a creative act.

43:23.000 --> 43:35.000
And it, it's, it's all about reading the signals of the world, you know, sort of metabolizing those signals, finding the patterns and the forms,

43:35.000 --> 43:41.000
and then recreating them in a construct that is meaningful to other people.

43:41.000 --> 43:51.000
And, and one of the things that I find really disturbing is when people have this ridiculous notion that the, that the universe is meaningless.

43:51.000 --> 43:59.000
And I think if you ever want to find meaning, the place to find it is in mentoring.

44:00.000 --> 44:16.000
Because when you mentor someone, as John says, when you do, and they come up with an insight that you never, ever imagined, it is the most meaningful thing that you can ever experience.

44:16.000 --> 44:25.000
When, when my kids come up with something incredible, I just go, oh, okay, then.

44:25.000 --> 44:30.000
Right. All is well in the universe on the right track. Right.

44:30.000 --> 44:43.000
And so if you can teach people how to have insightful moments in storytelling, and it doesn't have to be all that difficult, just learn how to write a really, you know, valence sentence.

44:43.000 --> 44:49.000
You know, that is meaningful to someone and goes, Oh, that's an interesting sentence. Could I read another one of yours?

44:49.000 --> 45:05.000
That that is the means by which we can cultivate wisdom within ourselves by expressing ourselves and reaching a dialogue with other people so we can test self evidence through our writing.

45:05.000 --> 45:06.000
Wow.

45:06.000 --> 45:23.000
So this is kind of the project that is enveloped me and what really attracted me to John because we're both doing the same project. It's just different ecologies of practices as John would say.

45:23.000 --> 45:24.000
Yeah, exactly.

45:24.000 --> 45:28.000
What's wonderful here is we get a lot.

45:28.000 --> 45:31.000
It's popular today to think about.

45:31.000 --> 45:36.000
And this is a good thing as well. How can we become?

45:36.000 --> 45:43.000
How can we be? How can we better interface with the emerging technologies that are coming? So you want to practice mindfulness? Why?

45:43.000 --> 45:54.000
Because you don't want you don't want Evil Corp to have the whole say and where your attention goes. So regaining your attentional autonomy seems like it's a crucial thing going forward.

45:54.000 --> 46:00.000
So we've got that story where you want to be wise because we want to wisely interface with technology.

46:00.000 --> 46:04.000
But what we have here is a companion idea and it runs so deep.

46:04.000 --> 46:14.000
Just a short pod just doesn't give it credence. I just hope everybody engages with this material and like seeks out Sean and John because this is stuff that you really need to digest.

46:14.000 --> 46:29.000
But we have a companion project here which isn't be wise so that you're better in the face of emerging technology, but rather learn to be wise because we're going to be called upon to first better understand the threshold we're moving through.

46:29.000 --> 46:39.000
And that's your responsibility because we live in the world together and you better be ready. So you better be wise to understand this this this liminal moment we're passing through.

46:39.000 --> 46:46.000
And two, you're going to be called on to mentor not even not only each other, but maybe synthetic systems in the future.

46:46.000 --> 46:54.000
And if we're going to have a good grip on how to make a synthetic powerful intelligence good, we had better bloody know how to be good ourselves.

46:54.000 --> 47:04.000
Otherwise, otherwise, we're just shooting in the dark being like oh well make it good and you're like well how and you're like well I don't know I don't know what it is to be good I guess make a lot of money you're like that's not the good.

47:04.000 --> 47:20.000
You better know you better know how to be good so you can be good for them, which is very much like a parent child relationship here you don't want to only be good so that you can survive parenthood, but you want to be good because hopefully you want your kids to be good and so that is a moral obligation to be good.

47:20.000 --> 47:30.000
And you articulated it beautifully that is Karuna that is agape, and that is like the thing we're trying to ultimately get people to virtuously tap into in this project.

47:31.000 --> 47:35.000
Wow, so we're going to go now.

47:35.000 --> 47:41.000
But I just want to say, I want to I want to even just go on that cliffhanger because here's the thing.

47:41.000 --> 47:46.000
We've got a podcast here that explores the science of contemplative programs.

47:46.000 --> 47:51.000
And this is again, like I said at the beginning of the episode this is our first episode back.

47:51.000 --> 48:02.000
I just want to leave it there because this is a real call to arms for exactly the kinds of things that we're interested here, which is we need to be wise at the birth of this big intelligence we need to be our best selves.

48:02.000 --> 48:15.000
And so, thanks, guys, and please everyone check out the material and I will be linking everything in the comments so that you can get in touch with this extremely important project.

48:15.000 --> 48:20.000
Any last words or does that feel like a good natural close.

48:20.000 --> 48:23.000
I'm happy with that. Thank you very much, Mark.

48:23.000 --> 48:29.000
Yeah, you're the light and fire inside of you is always extremely beautiful.

48:29.000 --> 48:40.000
Well, I mean, how do you how do you not light on fire when these sparks are just coming off of your guys projects? Really, Sean and John, thanks so much. And we'll talk soon for sure.

48:40.000 --> 48:46.000
Okay, thanks everybody. That's another episode of the contemplative science podcast. And as always, we'll see you next week.

