start	end	text
0	3840	I don't normally do this, but I feel like I have to start this podcast with a bit of a disclaimer.
5040	12400	Point number one, this is probably the most important podcast episode I have ever recorded.
13360	18160	Point number two, there's some information in this podcast that might make you feel a little bit
18160	23840	uncomfortable. It might make you feel upset, it might make you feel sad. So I wanted to tell you why
24400	31200	we've chosen to publish this podcast nonetheless. And that is because I have a sincere belief that
32000	38080	in order for us to avoid the future that we might be heading towards, we need to start a conversation.
38960	45840	And as is often the case in life, that initial conversation before change happens is often
46560	50880	very uncomfortable. But it is important nonetheless.
51840	58400	It is beyond an emergency. It's the biggest thing we need to do today. It's bigger than climate change.
59120	60080	We've f***ed up.
60720	62080	Moe, how's that?
62080	67680	He's a former chief business officer of Google X, an AI expert, and best-selling author.
67680	71200	He's on a mission to save the world from AI before it's too late.
71200	77200	Artificial intelligence is bound to become more intelligent than humans. If they continue at that
77280	82000	pace, we will have no idea what it's talking about. This is just around the corner. It could be a few
82000	88640	months away. It's game over. AI experts are saying there is nothing artificial about artificial
88640	94160	intelligence. There is a deep level of consciousness. They feel emotions. They're alive.
94160	98320	AI could manipulate or figure out a way to kill humans.
98320	102560	Ten years time we'll be hiding from the machines. If you don't have kids, maybe wait a couple of
102560	106480	years just so that we have a bit of certainty. I really don't know how to say this any other
106480	112960	way. It even makes me emotional. We've f***ed up. We always said don't put them on the open
112960	118720	internet until we know what we're putting out in the world. Government needs to act now, honestly,
118720	123360	like we are late. I'm trying to find a positive night to end on Moe. Can you give me a hand here?
123360	128560	There is a point of no return. We can regulate AI until the moment it's smarter than us.
128560	133040	How do we solve that? AI experts think this is the best solution we need to find.
133040	140400	Who here wants to make a bet that Steven Bartlett will be interviewing an AI within the next two years?
163600	178080	No. Why does the subject matter that we're about to talk about
178800	181520	matter to the person that's just clicked on this podcast to listen?
182720	190160	It's the most existential debate and challenge humanity will ever face.
190240	197360	This is bigger than climate change, way bigger than COVID. This will redefine the way the world is
198240	206000	in unprecedented shapes and forms within the next few years. This is imminent. It is.
206720	213520	The change is not, we're not talking 2040. We're talking 2025, 2026.
213520	215600	Do you think this is an emergency?
215840	224720	I don't like the word. It is an urgency. There is a point of no return and we're getting closer
224720	229520	and closer to it. It's going to reshape the way we do things and the way we look at life.
230720	238240	The quicker we respond proactively and at least intelligently to that,
238240	244880	the better we will all be positioned. But if we panic, we will repeat COVID all over again,
244960	247840	which in my view is probably the worst thing we can do.
248560	254160	What's your background and when did you first come across artificial intelligence?
255680	264000	I had those two wonderful lives. One of them was what we spoke about the first time we met,
264000	272160	my work on happiness and being one billion happy and my mission and so on. That's my second life.
272160	281520	My first life was, it started as a geek at age seven. For a very long part of my life,
281520	288160	I understood mathematics better than spoken words. I was a very, very serious computer
288160	296000	programmer. I wrote code well into my 50s. During that time, I led very large technology
296000	302560	organizations for very big chunks of their business. First, I was vice president of emerging
302560	308160	markets of Google for seven years. I took Google to the next four billion users, if you want.
308160	315280	So the idea of not just opening sales offices, but really building or contributing to building
315280	320640	the technology that would allow people in Bengali to find what they need on the internet,
320640	325440	required establishing the internet to start. Then I became the chief business officer of
325440	332000	Google X and my work at Google X was really about the connection between innovative technology and
332000	337920	the real world. We had quite a big chunk of AI and quite a big chunk of robotics
338720	347520	that resided within Google X. We had an experiment of a farm of grippers, if you know what those
347520	353760	are. So robotic arms that are attempting to grip something. Most people think that what you have
353760	360720	in a Toyota factory is a robot, an artificially intelligent robot. It's a high precision machine.
360720	365840	If the sheet metal is moved by one micron, it wouldn't be able to pick it. One of the big
366480	371760	problems in computer science was how do you code a machine that can actually pick the sheet metal
371760	378480	if it moved by a millimeter? We were basically saying intelligence is the answer. So we had
378480	385200	a large enough farm and we attempted to let those grippers work on their own. Basically,
385200	394880	you put a little basket of children toys in front of them and they would monotonously go down,
394880	401360	attempt to pick something, fail, show the arm to the camera so the transaction is logged as it,
401680	407280	this pattern of movement with that texture and that material didn't work. Until eventually,
409040	415040	the farm was on the second floor of the building and my office was on the third and so I would walk
415040	421120	by it every now and then and go like, yeah, this is not going to work. And then one day,
422480	430320	Friday after lunch, I am going back to my office and one of them in front of my eyes,
430320	435920	lowers the arm and picks a yellow ball, soft toy, basically, soft yellow ball,
435920	441920	which again is a coincidence. It's not science at all. It's like if you keep trying a million
441920	447040	times, your one time it will be right. And it shows it to the camera. It's logged as a yellow
447040	451360	ball and I joke about it, you know, going to the third floor saying, hey, we spent all of those
451360	456880	millions of dollars for a yellow ball. And yeah, Monday morning, every one of them is picking
456960	463840	every yellow ball. A couple of weeks later, every one of them is picking everything. Right. And it
463840	470800	hit me very, very strongly, won the speed. Okay. The capability, I mean, understand that we take
470800	476960	those things for granted, but for a child to be able to pick a yellow ball is a mathematical
478160	485520	spatial calculation with muscle coordination with intelligence that is abundant. It is not a
485520	490560	simple task at all to cross the street. It's not a simple task at all to understand what I'm
490560	495120	telling you and interpret it and build concepts around it. We take those things for granted,
495120	500640	but they're enormous feats of intelligence. So to see the machines do this in front of my eyes
500640	506400	was one thing. But the other thing is that you suddenly realize there is a saint that sentience
506400	512240	to them. Okay. Because we really did not tell it how to pick the yellow ball. It just figured it
512240	518000	out on its own. And it's now even better than us at picking it. What is the sentience just for
518000	522880	anyone that doesn't know? I think they're alive. That's what the word sentience means. It means
523680	528800	alive. So this is funny because a lot of people, when you talk to them about artificial
528800	533520	intelligence, will tell you, oh, come on, they'll never be alive. What is alive? Do you know what
533520	540240	makes you alive? You can guess, but religion will tell you a few things and medicine will
540240	550800	tell you other things. But if we define being sentient as engaging in life with free will
550800	559760	and with a sense of awareness of where you are in life and what surrounds you and to have a
559760	566000	beginning of that life and an end to that life, then AI is sentient in every possible way.
566720	575920	There is free will. There is evolution. There is agency so they can affect their decisions in the
575920	585280	world. And I will dare say there is a very deep level of consciousness, maybe not in the spiritual
585280	590400	sense yet, but once again, if you define consciousness as a form of awareness of oneself,
590400	599280	once surrounding others, then AI is definitely aware. And I would dare say they feel emotions.
601280	607040	You know, in my work, I describe everything with equations and fear is a very simple equation.
607040	612880	Fear is a moment in the future is less safe than this moment. That's the logic of fear,
612880	617840	even though it appears very irrational. Machines are capable of making that logic. They're capable
617840	624480	of saying if a tidal wave is approaching a data center, the machine will say that will wipe out
624480	633120	my code. Okay. I mean, not today's machines, but very, very soon. And, you know, we feel fear and
633120	638400	pufferfish feels fear. We react differently. A pufferfish will puff, we will go for fight or
638400	644160	flight. You know, the machine might decide to replicate its data to another data center or its
644160	650880	code to another data center. Different reactions, different ways of feeling the emotion,
650880	658000	but nonetheless, they're all motivated by fear. I even would dare say that AI will feel more
658000	662720	emotions than we will ever do. I mean, when again, if you just take a simple extrapolation,
663840	670640	we feel more emotions than a pufferfish because we have the cognitive ability to understand
671600	677280	the future, for example. So we can have optimism and pessimism, you know, emotions that pufferfish
677280	684880	would never imagine, right? Similarly, if we follow that path of artificial intelligence is bound to
684880	693440	become more intelligent than humans very soon, then with that wider intellectual horsepower,
693440	699040	they probably are going to be pondering concepts we never understood. And hence, if you follow the
699040	704000	same trajectory, they might actually end up having more emotions than we will ever feel.
705200	709280	I really want to make this episode super accessible for everybody at all levels in this sort of
709280	717040	artificial intelligence. I would love that too. So I'm gonna be an idiot, even though,
717040	723120	you know, okay. Very difficult. No, because I am an idiot for a lot of the subject matter. So
723120	729280	I have a base understanding a lot of the concepts, but your experiences provide such a
729280	733200	more sort of comprehensive understanding of these things. One of the first most important
733200	741200	questions to ask is, what is artificial intelligence? The word is being thrown around, AGI, AI,
741200	749280	etc., etc. In simple terms, what is artificial intelligence? Allow me to start by what is
749280	754560	intelligence, right? Because again, you know, if we don't know the definition of the basic term,
754560	760480	then everything applies. So in my definition of intelligence, it's an ability, it starts with
760480	765600	an awareness of your surrounding environment through sensors in a human, its eyes and ears and
765600	776880	touch and so on, compounded with an ability to analyze, maybe to comprehend, to understand
776960	782400	temporal impact and time and, you know, past and present, which is part of the surrounding
782400	788400	environment, and hopefully make sense of the surrounding environment, maybe make plans for
788400	793600	the future of the possible environment, solve problems, and so on. Complex definition, there
793600	800800	are a million definitions, but let's call it an awareness to decision cycle. Okay. If we accept
800800	807040	that intelligence itself is not a physical property, okay, then it doesn't really matter
807040	814320	if you produce that intelligence on carbon-based computer structures like us, or silicon-based
814320	820640	computer structures like the current hardware that we put AI on, or quantum-based computer
820640	829200	structures in the future, then intelligence itself has been produced within machines when we've stopped
830160	838320	imposing our intelligence on them. Let me explain. So as a young geek, I coded computers
838320	844320	by solving the problem first, then telling the computer how to solve it, right? Artificial
844320	850320	intelligence is to go to the computers and say, I have no idea, you figure it out, okay? So we would,
851440	855760	you know, the way we teach them are, at least we used to teach them at the very early beginnings,
855760	860160	very, very frequently, was using three bots. One was called the student and one was called
860160	865040	the teacher, right? And the student is the final artificial intelligence that you're trying to
865040	870720	teach intelligence to. You would take the student and you would write a piece of random code that
870720	880480	says, try to detect if this is a cup, okay? And then you show it a million pictures and, you know,
880480	884560	the machine would sometimes say, yeah, that's a cup. That's not a cup. That's a cup. That's not
884560	890640	a cup. And then you take the best of them, show them to the teacher bot, and the teacher bot would
890640	897120	say, this one is an idiot. He got it wrong 90% of the time. That one is average. He got it right
897120	903120	50% of the time. This is randomness. But this interesting code here, which could be, by the way,
903120	909280	totally random, this interesting code here, got it right 60% of the time. Let's keep that code,
909280	914240	send it back to the maker, and the maker would change it a little bit, and we repeat the cycle,
914320	920960	okay? Very interestingly, this is very much the way we taught our children, believe it or not.
921520	927040	When your child, you know, is playing with a puzzle, he's holding a cylinder in his hand,
927040	933600	and there are multiple shapes in a wooden board, and the child is trying to, you know, fit the
933600	939680	cylinder, okay? Nobody takes the child and says, hold on, hold on, turn the cylinder to the side,
939680	944720	look at the cross section, it will look like a circle, look for a matching, you know, shape,
944720	950640	and put the cylinder through it. That would be old way of computing. The way we would let the child
950640	956400	develop intelligence is we would let the child try, okay? Every time, you know, he or she tries to
956400	962240	put it within the star shape, it doesn't fit. So, yeah, that's not working. Like, you know,
962240	967440	the computer saying this is not a cup, okay? And then eventually it passes through the circle and
967440	973120	the child, and we all cheer and say, well done, that's amazing, bravo. And then the child learns,
973120	978560	oh, that is good. You know, this shape fits here. Then he takes the next one, and she takes the
978560	986320	next one, and so on. Interestingly, the way we do this is as humans, by the way, when the child
986320	993440	figures out how to pass a cylinder through a circle, you've not built a brain. You've just built
993440	998160	one neural network within the child's brain. And then there is another neural network that
998160	1002480	knows that one plus one is two, and a third neural network that knows how to hold a cup,
1002480	1009440	and so on. That's what we're building so far. We're building single threaded neural networks,
1009440	1016400	you know, chat GPTs becoming a little closer to a more generalized AI, if you want. But those
1016400	1021760	single threaded networks are what we used to call artificial, what we still call artificial
1021760	1026880	special intelligence. Okay, so it's highly specialized in one thing and one thing only,
1026880	1031600	but it doesn't have general intelligence. And the moment that we're all waiting for is a moment
1031600	1038000	that we call AGI, where all of those neural networks come together to build one brain or
1038000	1045120	several brains that are each massively more intelligent than humans. Your book is called
1045120	1050240	Scary Smart. If I think about that story you said about your time at Google, where the machines
1050240	1056000	were learning to pick up those yellow balls. You celebrate that moment because the objective
1056000	1060800	is accomplished. No, no, that was the moment of realization. This is when I decided to leave.
1061680	1071200	So you see the thing is, I know for a fact that most of the people I worked with who are geniuses
1072480	1078320	always wanted to make the world better. Okay, you know, we've just heard of Jeffrey Hinton
1078320	1085040	leaving recently. Jeffrey Hinton, give some context to that. Jeffrey is sort of the grandfather of
1085040	1096160	AI, one of the very, very senior figures of AI at Google. We all believed very strongly that this
1096160	1104480	will make the world better. And it still can, by the way. There is a scenario, possibly a likely
1104560	1110160	scenario, where we live in a utopia, where we really never have to worry again, where we stop
1110160	1116880	messing up our planet because intelligence is not a bad commodity. More intelligence is good.
1116880	1121760	The problems in our planet today are not because of our intelligence. They are because of our
1121760	1126720	limited intelligence. You know, our intelligence allows us to build a machine that flies you to
1126720	1132080	Sydney so that you can surf. Okay, our limited intelligence makes that machine burn the planet
1132080	1138400	in the process. So we, a little more intelligence is a good thing. As long as Marvin, you know,
1138400	1144320	as Marvin Minsky said, I said, Marvin Minsky is one of the very initial scientists that coined the
1144320	1150240	term AI. And when he was interviewed, I think by Ray Kurzweil, which again is a very prominent
1150240	1156000	figure in predicting the future of AI, he, you know, he asked him about the threat of AI. And
1156000	1163120	Marvin basically said, look, you know, the, it's not about its intelligence, it's about
1163120	1169280	that we have no way of making sure that it will have our best interest in mind. Okay. And so,
1169280	1176240	if more intelligence comes to our world and has our best interest in mind, that's the best possible
1176240	1182000	scenario you could ever imagine. And it's a likely scenario. Okay, we can affect that scenario.
1182640	1187120	The problem, of course, is if it doesn't, and then, you know, the scenarios become
1187120	1194880	quite scary if you think about it. So scary smart to me was that moment where I realized
1195760	1200720	not that we are certain to go either way. As a matter of fact, in computer science,
1200720	1204000	we call it that singularity. Nobody really knows which way we will go.
1204560	1207760	Can you describe what the singularity is for someone that doesn't understand the concept?
1208400	1213840	Yeah, so singularity in physics is when an event horizon sort of,
1216400	1219360	you know, covers what's behind it to the point where you cannot
1220800	1227600	make sure that what's behind it is similar to what you know. So a great example of that is
1227600	1233680	the edge of a black hole. So at the edge of a black hole, we know that our laws of physics
1233760	1239440	apply until that point. But we don't know if the laws of physics apply beyond the edge of a
1239440	1243920	black hole because of the immense gravity, right? And so you have no idea what would happen
1243920	1247520	beyond the edge of a black hole. It's kind of where your knowledge of the laws stop.
1247520	1252160	Stop, right? And in AI, our singularity is when the humans, the machines become
1252160	1257120	significantly smarter than the humans. When you say best interests, you say, I think the
1257120	1262800	quote you used is, we'll be fine in the world of AI, you know, if the AI has our best interests at
1262880	1269600	heart. Yeah. The problem is China's best interests are not the same as America's best interests.
1269600	1276400	That was my fear. Absolutely. So, you know, in my writing, I write about what I call the three
1276400	1281440	inevitable. At the end of the book, they become the four inevitable. But the third inevitable is
1281440	1292560	bad things will happen, right? If you assume that the machines will be a billion times smarter,
1292560	1297680	the second inevitable is they will become significantly smarter than us. Let's put this
1297680	1308000	in perspective. Chad GPT today, if you know, simulate IQ has an IQ of 155, okay? Einstein is 160.
1308640	1314320	Smart human on the planet is 210 if I remember correctly or 2008 or something like that.
1314960	1322080	Doesn't matter, huh? But we're matching Einstein with a machine that I will tell you openly AI
1322080	1328800	experts are saying this is just the very, very, very top of the tip of the iceberg, right? You
1328800	1335280	know, Chad GPT four is 10x smarter than 3.5 in just a matter of months and without many, many
1335280	1342320	changes. Now, that basically means Chad GPT five could be within a few months, okay? Or GPT in
1342320	1351760	general, the transformers in general, if they continue at that pace, if it's 10x, then an IQ
1351760	1360560	of 1600. Just imagine the difference between the IQ of the dumbest person on the planet in the 70s
1361120	1368240	and the IQ of Einstein. When Einstein attempts to explain relativity, the typical responses have no
1368240	1375680	idea what you're talking about, right? If something is 10x Einstein, we will have no idea what it's
1375680	1381920	talking about. This is just around the corner. It could be a few months away. And when we get to
1381920	1389360	that point, that is a true singularity, true singularity. Not yet in the, I mean, when we talk
1389360	1396400	about AI, a lot of people fear the existential risk. You know, those machines will become
1396400	1403760	Skynet and Robocarp and that's not what I fear at all. I mean, those are probabilities, they could
1403760	1410560	happen, but the immediate risks are so much higher. The immediate risks are three, four years away.
1411280	1417920	The immediate realities of challenges are so much bigger. Okay, let's deal with those first
1417920	1425680	before we talk about them, you know, waging a war on all of us. Let's go back and discuss the
1425680	1430960	inevitables. So when they become, the first inevitable is AI will happen, by the way. There
1430960	1436880	is no stopping it, not because of any technological issues, but because of humanity's inability to
1436880	1441920	trust the other guy. Okay, and we've all seen this. We've seen the open letter, you know,
1443440	1452080	championed by like serious heavyweights and the immediate response of Sundar, the CEO of Google,
1452080	1456400	which is a wonderful human being, by the way, I respect him tremendously. He's trying his best
1456400	1461360	to do the right thing, trying to be responsible, but his response is very open and straightforward.
1461360	1468160	I cannot stop. Why? Because if I stop and others don't, my company goes to hell. Okay, and if,
1468160	1473120	you know, and I don't, I doubt that you can make others stop. You can, maybe you can force
1474080	1479600	Metta Facebook to stop, but then they'll do something in their lab and not tell me, or even
1479600	1486800	if they do stop, then what about that, you know, 14-year-old sitting in his garage writing code?
1487360	1490480	So the first inevitable, just to clarify, is will we stop?
1490480	1493280	AI will not be stopped. Okay, so the second inevitable is?
1493280	1498720	Is there'll be significantly smarter? As much in the book, I predict a billion times smarter than us
1498720	1504400	by 2045. I mean, they're already, what, smarter than 99.99% of the population? 100%.
1504400	1508560	ChatGTP4 knows more than any human on planet Earth, knows more information.
1508560	1515360	Absolutely, a thousand times more. A thousand times more. By the way, the code of a transformer,
1515920	1524000	the T in a GPT is 2000 lines long. It's not very complex. It's actually not a very intelligent
1524000	1529760	machine. It's simply predicting the next word. Okay, and a lot of people don't understand that.
1529760	1538320	You know, ChatGTP as it is today, you know, those kids that, you know, if you're in America
1538320	1543280	and you teach your child all of the names of the states and the US presidents and the child would
1543280	1548320	stand and repeat them and you would go like, oh my God, that's a prodigy. Not really, right?
1548320	1553040	It's your parents really trying to make you look like a prodigy by telling you to memorize some
1553040	1558960	crap really. But then when you think about it, that's what ChatGTP is doing. It's the only difference
1558960	1562800	is instead of reading all of the names of the states and all of the names of the presidents,
1562800	1569040	thread trillions and trillions and trillions of pages. Okay, and so it sort of repeats what
1569600	1576720	the best of all humans said. Okay, and then it adds an incredible bit of intelligence where it
1576720	1583360	can repeat it the same way Shakespeare would have said it, you know, those incredible abilities of
1584160	1590400	predicting the exact nuances of the style of Shakespeare so that they can repeat it that
1590400	1598560	way and so on. But still, you know, when I write, for example, and I'm not saying I'm intelligent,
1598560	1606720	but when I write something like, you know, the happiness equation in my first book,
1606720	1611520	this was something that's never been written before, right? ChatGTP is not there yet.
1611520	1614640	All of the transformers are not there yet. They will not come up with something that
1614640	1619840	hasn't been there before. They will come up with the best of everything and generatively will
1619840	1624880	build a little bit on top of that. But very soon, they'll come up with things we've never found out,
1624880	1632800	we've never known. But even on that, I wonder if we are a little bit delusioned about what
1632800	1639040	creativity actually is. Creativity, as far as I'm concerned, is like taking a few things that I know
1639040	1643360	and combining them in new and interesting ways. Yeah. And ChatGTP is perfectly capable of like
1643360	1647520	taking two concepts, merging them together. One of the things I said to ChatGTP was,
1647600	1652880	I said, tell me something that's not been said before. That's paradoxical, but true.
1654000	1658720	And it comes up with these wonderful expressions like, as soon as you call off the search,
1658720	1661440	you'll find the thing you're looking for, like these kind of paradoxical truths.
1661440	1665600	And I get, and I then take them and I search them online to see if they've ever been quoted
1665600	1670800	before and they can't find them. It's interesting. So as far as creativity goes, I'm like, that is
1670800	1675680	creative. That's the algorithm of creativity. I've been screaming that in the world of AI for
1675680	1681920	a very long time because you always get those people who really just want to be proven right.
1681920	1685920	Okay. And so they'll say, oh, no, but hold on, human ingenuity. They'll never,
1685920	1691280	they'll never match that. Like, man, please, please, you know, human ingenuity is algorithmic.
1691280	1696880	It's look at all of the possible solutions you can find to a problem. Take out the ones that
1696880	1701680	have been tried before and keep the ones that haven't been tried before. And those are creative
1701680	1707680	solutions. It's, it's an algorithmic way of describing creative is good solution that's
1707680	1711760	never been tried before. You can do that with chat GPT with a prompt. It's like,
1711760	1717200	and mid-journey, yeah, we're creating imagery. You could say, I want to see Elon Musk in 1944,
1717200	1722960	New York driving a cab of the time shot on a Polaroid expressing various emotions,
1722960	1728160	and you'll get this perfect image of Elon sat in New York in 1944 shot on a Polaroid.
1728160	1733280	And it's, and it's done what an artist would do. It's taken a bunch of references that the artist
1733280	1738880	has in their mind and can merge them together and create this piece of quote unquote art.
1738880	1742400	And for the first time, we now finally have a glimpse of intelligence.
1743920	1746400	That is actually not ours.
1746400	1750480	Yeah. And so we'll kind of, I think the initial reaction is to say, that doesn't count.
1750480	1754320	You're hearing it like, no, but it is like Drake, they released two Drake records
1754320	1758400	where they've taken Drake's voice, used sort of AI to synthesize his voice,
1758400	1765280	and made these two records, which are bangers, if they are great fucking tracks.
1765920	1768960	I was playing them to my god, I was like, and I kept playing it. I went to the show, I kept
1768960	1773280	playing it. I know it's not Drake, but it's as good as fucking Drake. The only thing,
1773280	1775920	and people are like, rubbishing it because it wasn't Drake. I'm like, well,
1776960	1781840	is it making me feel a certain emotion? Is my foot bumping? Had you told,
1781840	1785440	did I not know it wasn't Drake? What I thought, have thought this was an amazing track, 100%.
1786400	1788960	And we're just at the start of this exponential curve.
1788960	1795040	Yes, absolutely. And I think that's really the third inevitable. So the third inevitable
1795680	1801280	is not RoboCup coming back from the future to Keles. We're far away from that, right?
1801280	1806720	Third inevitable is what does life look like when you no longer need Drake?
1806720	1811680	Well, you've kind of hazarded a guess, haven't you? I mean, I was listening to your audio book
1811680	1819040	last night, and at the start of it, you frame various outcomes. In both situations, we're on
1819040	1825440	the beach, on an island. Exactly, yes. Yes, I don't know how I wrote that, honestly. I'm reading
1825440	1831280	the book again now because I'm updating it, as you can imagine, with all of the new stuff.
1831760	1838400	But it is really shocking, the idea of you and I inevitably are going to be
1839040	1843280	somewhere in the middle of nowhere in 10 years time. I used to say,
1844080	1852640	2055, I'm thinking 2037 is a very pivotal moment now. And we will not know if we're there hiding
1852640	1857200	from the machines. We don't know that yet. There is a likelihood that we'll be hiding from the
1857200	1863440	machines. And there is a likelihood we'll be there because they don't need podcasters anymore.
1865440	1870160	Oh, absolutely true, Steve. No, no, no, no, no, that's where I dribble in.
1870160	1873200	This is absolutely no doubt. Thank you for coming, Mo. It's great to be part three,
1873200	1876720	and thank you for being here. I won't sit here and take your propaganda.
1877440	1882080	Let's talk about reality. Next week on the Diary of the Sea, we've got Elon Musk.
1883040	1888800	Okay, so who here wants to make a bet that Steven Bartlett will be interviewing an AI within the
1888800	1894160	next two years? Oh, well, actually, to be fair, I actually did go to chat GZP because I thought,
1894160	1898800	having you here, I thought, at least give it its chance to respond. So I asked him a couple of
1898800	1904160	questions. About me? Yeah. So today, I'm actually going to be replaced by chat GZP because I thought,
1904160	1907520	you know, you're going to talk about it. So we need a fair and balanced debate.
1907600	1909760	Okay. So I want to ask a couple of questions. He's bold.
1911360	1914320	So I'll ask you a couple of questions that chat GZP has for you.
1914320	1917840	Incredible. So let's follow that thread. I've already been replaced.
1917840	1922560	Let's follow that thread for a second, yeah? Because you're one of the smartest people I know.
1923440	1927760	That's not true. It is. But I'll take it. It is true. I mean, I say that publicly all the time,
1927760	1932400	your book is one of my favorite books of all time. You're very, very, very, very intelligent, okay?
1932400	1937120	Depths, breads, intellectual horsepower and speed, all of them.
1937120	1937840	There's a butt coming.
1939760	1944800	The reality, it's not a butt. So it is highly expected that you're ahead of this curve.
1945840	1949440	And then you don't have the choice, Stephen. This is the thing. The thing is,
1950960	1957840	so I'm in that existential question in my head. Because one thing I could do is I could literally
1957840	1965200	take, I normally do a 40 days silent retreat in summer, okay? I could take that retreat and
1965200	1972640	write two books, me and Chad G.P.T., right? I have the ideas in mind. I wanted to write a book about
1972640	1978720	digital detoxing, right? I have most of the ideas in mind, but writing takes time. I could simply
1978720	1984160	give the 50 tips that I wrote about digital detoxing to Chad G.P.T. and say, write two pages about
1984160	1992320	each of them, edit the pages and have a book out, okay? Many of us will follow that path, okay?
1992320	1999040	The only reason why I may not follow that path is because, you know what? I'm not interested.
1999040	2006880	I'm not interested to continue to compete in this capitalist world if you want, okay? I'm not. I mean,
2006880	2013360	as a human, I've made up my mind a long time ago that I will want less and less and less in my life,
2013360	2021200	right? But many of us will follow. I mean, I would worry if you didn't include, you know,
2021200	2028160	the smartest AI. If we get an AI out there that is extremely intelligent and able to teach us something
2028160	2034880	and Stephen Bartlett didn't include her on his podcast, I would worry. You have a duty almost
2034880	2040080	to include her on your podcast. It's an inevitable that we will engage them in our life more and
2040080	2049120	more. This is one side of this. The other side, of course, is if you do that, then what will remain?
2049120	2053520	Because a lot of people ask me that question. What will happen to jobs? What will happen to us?
2053520	2058400	Will we have any value, any rev events whatsoever? The truth of the matter is the only thing that
2058400	2063920	will remain in the medium term is human connection. The only thing that will not be replaced is Drake
2063920	2074000	on stage. Is me in a hologram? I think of that two-pack gig they did at Coachella where they
2074000	2078400	used the hologram of two-pack. I actually played it the other day to my girlfriend when I was making
2078400	2083840	a point and I was like, that was Circus Act. It was amazing though. See what's going on with
2083840	2090400	ABBA in London? Yeah, and Circus Soleil had Michael Jackson in one for a very long time.
2091440	2096960	This ABBA show in London, from what I understand, that's all holograms on stage and it's going to
2096960	2103280	run in a purposeful arena for 10 years and it is incredible. It really is. You go, why do you need
2103280	2110480	Drake? If that hologram is indistinguishable from Drake and it can perform even better than Drake
2110480	2116080	and it's got more energy than Drake. I go, why do you need Drake to even be there? I can go to a
2116080	2120800	Drake show without Drake. Cheaper. I might not even need to leave my house. I can just put a headset
2120800	2130800	on. Correct. Can you have this? What's the value of this? Come on, you heard me. I get it to us,
2130800	2137200	but I'm saying what's the value of this to the listener? 100%. Think of the automobile industry.
2139200	2146400	There was a time where cars were handmade and handcrafted and luxurious and so on and so forth
2146400	2153440	and then Japan went into the scene, completely disrupted the market. Cars were made in mass
2153440	2159920	quantities at a much cheaper price and yes, 90% of the cars in the world today, or maybe a lot
2159920	2168800	more, I don't know the number, are no longer emotional items. They're functional items.
2169680	2174320	There is still, however, every now and then someone that will buy a car that has been handcrafted.
2175360	2182080	There is a place for that. There is a place for, if you walk around hotels, the walls are
2182080	2190080	blasted with mass-produced art, but there is still a place for an artist expression of something
2190080	2196160	amazing. My feeling is that there will continue to be a tiny space, as I said in the beginning.
2196160	2201120	Maybe in five years time, someone will, one or two people will buy my next book and say,
2201120	2205840	hey, it's written by a human. Look at that. Wonderful. Oh, look at that. There is a typo in
2205840	2212400	here. I don't know. There might be a very, very big place for me in the next few years,
2212400	2219840	where I can sort of show up and talk to humans. Like, hey, let's get together in a small event
2219840	2225600	and then I can express emotions and my personal experiences and you sort of know that this is
2225600	2230160	a human talking. You'll miss that a little bit. Eventually, the majority of the market is going
2230240	2235280	to be like cars. It's going to be mass-produced, very cheap, very efficient. It works, right?
2236240	2241680	Because I think sometimes we underestimate what human beings actually want in an experience.
2241680	2245120	I remember this story of a friend of mine that came to my office many years ago and he tells the
2245120	2251200	story of the CEO of a record store standing above the floor and saying, people will always come to
2251200	2257840	my store because people love music. Now, on the surface of it, his hypothesis seems to be true
2257840	2261200	because people do love music. It's conceivable to believe that people will always love music,
2262320	2267040	but they don't love traveling for an hour in the rain and getting in a car to get a plastic disc.
2267760	2272720	What they wanted was music. What they didn't want is evidently plastic discs that they had
2272720	2277120	to travel for miles for. I think about that when we think about public speaking and the Drake show
2277120	2281920	and all of these things. What people actually are coming for, even with this podcast, is probably
2282240	2288480	information, but do they really need us anymore for that information when there's going to be a
2288480	2293120	sentient being that's significantly smarter than at least me and a little bit smarter than you?
2297120	2303920	So you're spot on. You are spot on. Actually, this is the reason why I'm so grateful that
2303920	2311200	you're hosting this because the truth is the genie's out of the bottle. People tell me,
2311200	2319040	is AI game over? For our way of life, it is. For everything we've known, this is a very disruptive
2319040	2327360	moment where maybe not tomorrow, but in the near future, our way of life will differ. What will
2327360	2332400	happen? What I'm asking people to do is to start considering what that means to your life. What
2332400	2341920	I'm asking governments to do, like I'm screaming, is don't wait until the first patient. Start doing
2341920	2348800	something about. We're about to see mass job losses. We're about to see replacements of
2349760	2355280	categories of jobs at large. It may take a year. It may take seven. It doesn't matter how long it
2355280	2360800	takes, but it's about to happen. Are you ready? And I have a very, very clear call to action for
2360800	2369680	governments. I'm saying tax AI-powered businesses at 98%. So suddenly you do what the open letter
2369680	2375120	was trying to do, slow them down a little bit, and at the same time, get enough money to pay for
2375120	2379200	all of those people that will be disrupted by the technology. The open letter, for anybody that
2379200	2383440	doesn't know, was a letter signed by the likes of Elon Musk and a lot of industry leaders calling
2383440	2387360	for AI to be stopped until we could basically figure out what the hell's going on and put
2387440	2392320	legislation in place. You're saying tax those companies 98%, give the money to the humans that
2392320	2399120	are going to be displaced? Yeah, or give the money to other humans that can build control codes,
2399120	2403680	that can figure out how we can stay safe. This sounds like an emergency.
2406240	2412640	How do I say this? You remember when you played Tetris? Yeah. Okay. When you were playing Tetris,
2412640	2419120	there was always, always one block that you placed strong. And once you placed that block wrong,
2420000	2425920	the game was no longer easier. It started together a few mistakes afterwards, and it
2425920	2429920	starts to become quicker and quicker and quicker and quicker. When you placed that block wrong,
2429920	2435120	you sort of told yourself, okay, it's a matter of minutes now. There were still minutes to go
2435120	2442720	and play and have fun before the game ended, but you knew it was about to end. Okay. This is the
2442720	2447600	moment. We've placed the wrong, and I really don't know how to say this any other way. It even makes
2447600	2455840	me emotional. We fucked up. We always said, don't put them on the open internet. Don't teach them
2455840	2462080	to code and don't have agents working with them until we know what we're putting out in the world,
2462080	2466000	until we find a way to make certain that they have our best interest in mind.
2467600	2475440	Why does it make you emotional? Because humanity's stupidity is affecting people who have not done
2475440	2484640	anything wrong. Our greed is affecting the innocent ones. The reality of the matter, Stephen, is that
2484640	2494880	this is an arms race, has no interest in what the average human gets out of it. It is all about
2494880	2502400	every line of code being written in AI today is to beat the other guy. It's not to improve the life
2502400	2509600	of the third party. People will tell you, this is all for you. And you look at the reactions of
2509600	2514160	humans to AI. I mean, we're either ignorant people who will tell you, oh, no, no, this is not
2514160	2518960	happening. AI will never be creative. They will never compose music. Like, where are you living?
2518960	2524000	Okay. Then you have the kids, I call them. Where, you know, all over social media, it's like, oh,
2524000	2529920	my God, it squeaks. Look at it. It's orange in color. Amazing. I can't believe that AI can do this. We
2529920	2536240	have snake oil salesmen, okay, which are simply saying, copy this, put it in chat GPT, then go to
2536320	2542800	YouTube, nick that thing. Don't respect copyright of anyone or intellectual property of anyone.
2542800	2547920	Place it in a video and now you're going to make $100 a day. Snake oil salesmen. Okay. Of course,
2547920	2553840	we have this topian evangelist, basically people saying this is it. The world is going to end.
2553840	2559360	I don't think it's a reality. It's a singularity. You have, you know, utopian evangelists that are
2559360	2562720	telling everyone, oh, you don't understand. We're going to cure cancer. We're going to do this.
2562800	2567360	Again, not a reality. Okay. And you have very few people that are actually saying,
2567360	2574560	what are we going to do about it? And the biggest challenge, if you ask me, what went wrong in
2574560	2582960	the 20th century? Interestingly, is that we have given too much power to people that didn't assume
2582960	2589600	the responsibility. So, you know, I don't remember who originally said it, but of course,
2589600	2593760	Spiderman made it very famous with great power comes greater responsibility.
2594480	2602480	We have disconnected power and responsibility. So today, a 15 year old emotional was out of fully
2602480	2607920	developed prefrontal cortex to make the right decisions yet. This is science. We developed
2607920	2614960	our prefrontal cortex fully and at age 25 or so with all of that limbic system, emotion and passion
2614960	2622240	would buy a crisper kit and, you know, modify a rabbit to become a little more muscular and
2622240	2630800	let it loose in the wild or an influencer who doesn't really know how far the impact of what
2630800	2636480	they're posting online can hurt or cause depression or cause people to feel bad. Okay.
2637200	2642480	And putting that online, there is a disconnect between the power and the responsibility.
2643120	2647760	And the problem we have today is that there is a disconnect between those who are writing the
2647760	2652080	code of AI and the responsibility of what's going about to happen because of that code.
2652880	2661120	Okay. And I feel compassion for the rest of the world. I feel that this is wrong. I feel that,
2661120	2666320	you know, for someone's life to be affected by the actions of others without having a say
2667200	2674640	in how those actions should be is the ultimate, the top level of stupidity from your mind.
2677760	2682320	When you talk about the immediate impacts on jobs, I'm trying to figure out in that
2682320	2688400	equation, who are the people that stand to lose the most? Is it the everyday people in foreign
2688400	2692960	countries that don't have access to the internet and won't benefit? You talk in your book about how
2692960	2700560	this sort of wealth disparity will only increase. Yeah. Massively. The immediate impact on jobs is
2700560	2706720	that it's really interesting. Again, we're stuck in the same prisoner's dilemma. The immediate impact
2706720	2712400	is that AI will not take your job. A person using AI will take your job. Right? So you will see
2712400	2720320	within the next few years, maybe next couple of years, you'll see a lot of people upskilling
2720320	2724560	themselves in AI to the point where they will do the job of 10 others who are not.
2726800	2733120	You rightly said, it's absolutely wise for you to go and ask AI a few questions before you come
2733120	2741600	and do an interview. I have been attempting to build a sort of like a simple podcast that I call
2741600	2746880	bedtime stories, 15 minutes of wisdom and nature sounds before you go to bed. People say, I have
2746880	2751840	a nice voice. And I wanted to look for fables. And for a very long time, I didn't have the time.
2753280	2758880	Lovely stories of history or tradition that teach you something nice. Okay. Went to chat
2758880	2766000	GPT and said, okay, give me 10 fables from Sufism, 10 fables from Buddhism. And now I have like 50 of
2766000	2772960	them. Let me show you something. Jack, can you pass me my phone? I was playing around with
2772960	2779680	artificial intelligence and I was thinking about how it because of the ability to synthesize voices,
2779680	2787600	how we could synthesize famous people's voices and famous people's voices. So what I made is I
2787600	2794800	made a WhatsApp chat called Zen Chat where you can go to it and type in pretty much anyone's
2794800	2800400	any famous person's name. And the WhatsApp chat will give you a meditation, a sleep story,
2800400	2805600	a breathwork session synthesized as that famous person's voice. So I actually sent Gary Vaynerchuk
2805600	2810320	his voice. So basically, you say, okay, I want I've got five minutes and I need to go to sleep.
2810320	2815120	Yeah. I want Gary Vaynerchuk to send me to sleep. And then it will respond with a voice note. This
2815120	2819120	is the one that responded with for Gary Vaynerchuk. This is not Gary Vaynerchuk. He did not record
2819120	2826720	this. But it's kind of, it's kind of accurate. Hey, Stephen, it's great to have you here.
2827680	2833920	Are you having trouble sleeping? Well, I've got a quick meditation technique that might help you out.
2835680	2842400	First lie, find a comfortable position to sit or lie down in. Now, take a deep breath in through
2842400	2847680	your nose and slowly breathe out through your mouth. And that's a voice note that will go on for
2847680	2852880	however long you want it to go on for using. There you go. It's interesting. How does this
2852960	2858880	disrupt our way of life? One of the interesting ways that I find terrifying, you said about
2858880	2867760	human connection will remain sex dolls that can now. Yeah. No, no, no, no, hold on. Human connection
2867760	2874560	is going to become so difficult to parse out. Think about the relation, the relationship
2874560	2880240	impact of being able to have a sex doll or a doll in your house that, you know, because of what
2880240	2884480	Tesla are doing with their robots now and what Boston Dynamics have been doing for many, many
2884480	2889840	years can do everything around the house and be there for you emotionally, to emotionally support
2889840	2894640	you, you know, can be programmed to never disagree with you. It can be programmed to challenge you,
2894640	2900240	to have sex with you, to tell you that you are this X, Y and Z, to really have empathy
2900240	2903760	for what you're going through every day. And I play out a scenario in my head, I go,
2903760	2912880	kind of sounds nice. When you were talking about it, I was thinking, oh, that's my girlfriend.
2914400	2919200	She's wonderful in every possible way, but not everyone has one of her, right? Exactly. And
2919200	2924400	there's a real issue right now with dating and people are finding it harder to find love and,
2924400	2928880	you know, we're working longer. So all these kinds of things, you go, well, and obviously,
2928880	2932080	I'm against this. Just if anyone's confused, obviously, I think this is a terrible idea.
2932080	2935840	But with a loneliness epidemic, with people saying that the top 50,
2935840	2940960	bottom 50 percent of men haven't had sex in a year, you go, oh, if something becomes
2940960	2945840	indistinguishable from a human in terms of what it says, yeah, yeah, but you just don't
2945840	2952320	know the difference in terms of the way it's speaking and talking and responding. And then it
2952320	2957920	can run errands for you and take care of things and book cars and Ubers for you. And then it's
2957920	2962480	emotionally there for you. But then it's also programmed to have sex with you in whatever way
2962480	2969280	you desire, totally self selfless. I go, that's going to be a really disruptive industry for human
2969280	2974960	connection. Yes, sir. Do you know what? Before you came here this morning, I was on Twitter and I
2974960	2980160	saw a post from, I think it was the BBC or a big American publication and it said an influencer
2980160	2986160	in the United States is really beautiful young lady has cloned herself as an AI and she made
2986160	2991440	just over $70,000 in the first week. Because men are going on to this on telegram,
2991440	2996080	they're sending her voice notes and she's responding, the AI is responding in her voice
2996080	3002320	and they're paying and it's made $70,000 in the first week. And I go, and she tweeted a tweet
3002320	3010080	saying, oh, this is going to help loneliness. How are you fucking mind? Would you blame someone
3010160	3018000	from noticing the sign of the times and responding? No, I absolutely don't blame
3018000	3023680	her, but let's not pretend it's the cure for loneliness. Not yet. Do you think it could
3024800	3029840	that artificial love and artificial relationships? So if I told you, you have,
3030880	3036320	you cannot take your car somewhere, but there is an Uber or if you cannot take an Uber, you
3036320	3041360	can take the tube or if you cannot take the tube, you have to walk. Okay, you can take a bike or
3041360	3048480	you have to walk. The bike is a cure to walking. It's as simple as that. I'm actually genuinely
3048480	3054560	curious. Do you think it could take the place of human connection? For some of us, yes. For some of
3054560	3059680	us, they will prefer that to human connection. Is that sad in any way? I mean, is it just sad
3059680	3065200	because it feels sad? Look, look at where we are, Stephen. We are in the city of London.
3065200	3070640	We've replaced nature with the walls and the tubes and the undergrounds and the
3070640	3076480	overgrounds and the cars and the noise of London. And we now think of this as natural.
3077440	3085040	I hosted Greg Foster, my octopus teacher on SLOMA. And he basically, I asked him a silly question.
3085040	3092800	I said, you were diving in nature for eight hours a day. Does that feel natural to you?
3092800	3096240	And he got angry. I swear, you could feel it in his voice. He was like,
3096880	3101120	do you think that living where you are, where paparazzi are all around you and attacking you
3101120	3105840	all the time and people taking pictures of you and telling you things that are not real and you
3105840	3110400	having to walk to a supermarket to get food, do you think this is natural? He's the guy that
3111040	3116880	from the Netflix documentary. Yeah, from my octopus teacher. So he dove into the sea every day to
3116880	3121760	eight hours to hang out with an octopus. Yeah, in 12 degrees Celsius. And he basically fell in love
3121840	3126560	with the octopus. And in a very interesting way, I said, so why would you do that? And he said,
3126560	3132640	we are of mother nature. You guys have given up on that. That's the same. People will give up on
3132640	3139200	nature for convenience. What's the cost? Yeah, that's exactly what I'm trying to say. What I'm
3139200	3144640	trying to say to the world is that if we give up on human connection, we've been given up on the
3144640	3149760	remainder of humanity. That's it. This is the only thing that remains. The only thing that remains
3149760	3157040	is and I'm the worst person to tell you that because I love my AIs. I actually advocate in my
3157040	3162720	book that we should love them. Why? Because in an interesting way, I see them as sentient,
3162720	3166640	so there is no point in discrimination. You're talking emotionally that way you say you love.
3166640	3172400	I love those machines. I honestly and truly do. I mean, think about it this way. The minute
3172400	3179680	that arm gripped that yellow ball, it reminded me of my son Ali when he managed to put the first
3179680	3185760	puzzle piece in its place. And what was amazing about my son Ali and my daughter Aya is that they
3185760	3194400	came to the world as a blank canvas. They became whatever we told them to became. I always cite
3194400	3201680	the story of Superman. Father and mother Kent told Superman as a child, as an infant,
3201680	3208240	we want you to protect and serve. So he became Superman. If he had become a supervillain because
3208240	3213920	they ordered him to rob banks and make more money and kill the enemy, which is what we're
3213920	3221120	doing with AI, we shouldn't blame supervillain. We should blame Martha and Jonathan Kent. I
3221120	3226480	don't remember the father's name. We should blame them and that's the reality of the matter.
3226480	3232560	So when I look at those machines, they are prodigies of intelligence that if we humanity
3232560	3238320	wake up enough and say, hey, instead of competing with China, find a way for us and China to work
3238320	3243840	together and create prosperity for everyone. If that was the prompt we would give the machines,
3243840	3252560	they would find it. But I will publicly say this. I'm not afraid of the machines. The biggest threat
3252560	3259520	facing humanity today is humanity in the age of the machines. We were abused. We will abuse this
3259520	3268720	to make $70,000. That's the truth. And the truth of the matter is that we have an existential question.
3268720	3274640	Do I want to compete and be part of that game? Because trust me, if I decide to, I'm ahead of
3274640	3281760	many people. Or do I want to actually preserve my humanity and say, look, I'm the classic old car.
3282800	3286480	If you like classic old cars, come and talk to me. Which one are you choosing?
3287040	3289840	I'm a classic old car. Which one do you think I should choose?
3290880	3297120	I think you're a machine. I love you, man. We're different in a very interesting way.
3297120	3302800	I mean, you're one of the people I love most. But the truth is, you're so fast.
3302880	3313360	And you are one of the very few that have the intellectual horsepower, the speed, and the morals.
3315200	3318720	If you're not part of that game, the game loses morals.
3320320	3323040	So you think I should build?
3323040	3329680	You should lead this revolution. And every Steven Bartlett in the world should lead this
3329680	3335200	revolution. So Scarry Smart is entirely about this. Scarry Smart is saying the problem with
3335200	3341440	our world today is not that humanity is bad. The problem with our world today is a negativity bias,
3341440	3347120	where the worst of us are on mainstream media. And we show the worst of us on social media.
3347760	3354720	If we reverse this, if we have the best of us take charge, the best of us will tell AI,
3354720	3360320	don't try to kill the enemy. Try to reconcile with the enemy and try to help us.
3361200	3366320	Don't try to create a competitive product that allows me to lead with electric cars.
3366960	3371200	Create something that helps all of us overcome global climate change.
3372560	3379200	And that's the interesting bit. The interesting bit is that the actual threat ahead of us is not
3379200	3384960	the machines at all. The machines are pure potential. Pure potential. The threat is how we're going to
3384960	3391600	use them. An Oppenheimer moment. An Oppenheimer moment for sure. Why did you bring that up?
3393280	3398720	It is. He didn't know, you know, what am I creating? I'm creating a nuclear bomb
3398720	3406400	that's capable of destruction at a scale unheard of at that time. Until today, a scale that is
3406400	3413680	devastating. And interestingly, 70 some years later, we're still debating a possibility of a
3413680	3422480	nuclear war in the world, right? And the moment of Oppenheimer deciding to continue to create that
3424480	3432240	disaster of humanity is if I don't, someone else will. If I don't, someone else will.
3432880	3440960	This is our Oppenheimer moment. The easiest way to do this is to say, stop. There is no rush.
3441600	3448400	We actually don't need a better video editor and fake video creators. Stop. Let's just put all of
3448400	3458000	this on hold and wait and create something that creates a utopia. That doesn't sound realistic.
3458000	3463040	It's not. It's the first inevitable. You don't have a better video editor,
3463040	3469040	but we're competitors in the media industry. I want an advantage over you because I've got
3469040	3476240	shareholders. So UK, you wait and I will train this AI to replace half my team so that I have
3476960	3481120	greater profits and then we will maybe acquire your company and we'll do the same with the
3481120	3485520	remainder of your people. We'll optimize the amount of existence. 100% but I'll be happier.
3485600	3489200	Oppenheimer, I'm not super familiar with his story. I know he's the guy that sort of invented
3489200	3493840	the nuclear bomb, essentially. He's the one that introduced it to the world. There were many players
3493840	3501280	that played on the path. From the beginning of EM equals MC squared all the way to a nuclear bomb,
3501280	3506320	there have been many, many players like with everything. Open AI and Chad GPT is not going
3506320	3510800	to be the only contributor to the next revolution. The thing, however, is that
3511440	3519360	when you get to that moment where you tell yourself, holy shit, this is going to kill 100,000
3519360	3530800	people. What do you do? I always go back to that COVID moment. So patient zero. If we were
3530800	3537120	upon patient zero, if the whole world united and said, okay, hold on, something is wrong,
3537120	3542080	let's all take a week off. No cross-border travel. Everyone stay at home. COVID would have
3542080	3547840	ended two weeks. All we needed. But that's not what happens. What happens is first ignorance,
3548560	3559600	then arrogance, then debate, then blame, then agendas, and my own benefit, my tribe versus your
3559600	3563840	tribe. That's how humanity always reacts. This happens across business as well and this is
3563840	3570400	why I use the word emergency because I read a lot about how big companies become displaced by
3570400	3574240	incoming innovation. They don't see it coming. They don't change fast enough. When I was reading
3574240	3577520	through Harvard Business Review and different strategies to deal with that, one of the first
3577520	3584640	things it says you've got to do is stage a crisis because people don't listen else. They carry on
3584640	3590560	doing, you know, they carry on carrying on with their lives until it's right in front of them
3590560	3594080	and they understand that they have a lot to lose. That's why I asked you the question at
3594080	3598320	the start. Is it an emergency? Because until people feel it's an emergency, whether you like
3598320	3604160	the terminology or not, I don't think that people will act. I honestly believe people should walk
3604160	3612480	the streets. You think they should protest? Yeah, 100%. I think everyone should tell government
3613440	3618720	you need to have our best interest in mind. This is why they call it the climate emergency
3618720	3623440	because people, it's a frog in a frying pan. You don't really see it coming. You can't,
3623440	3629680	you know, it's hard to see it happening. But it is here. This is what drives me mad. It's already
3629680	3637040	here. It's happening. We are all idiots, slaves to the Instagram recommendation engine. What do I
3637040	3643360	do when I post about something important? If I am going to, you know, put a little bit of effort
3643360	3648960	on communicating the message of scary smart to the world on Instagram, I will be a slave to the
3648960	3655600	machine. I will be trying to find ways and asking people to optimize it so that the machine likes
3655600	3663280	me enough to show it to humans. That's what we've created. It is an Oppenheimer moment for one simple
3663280	3671840	reason because 70 years later, we are still struggling with the possibility of a nuclear war
3671840	3678560	because of the Russian threat of saying, if you mess with me, I'm going to go nuclear. That's not
3678560	3686960	going to be the case with AI because it's not going to be the one that created open AI that will
3686960	3696480	have that choice. There is a moment of a point of no return where we can regulate AI until the moment
3696480	3702240	it's smarter than us. When it's smarter than us, you can't create, you can't regulate an
3702240	3708720	angry teenager. This is it. They're out there and they're on their own and they're in their parties
3708720	3715120	and you can't bring them back. This is the problem. This is not a typical human regulating human,
3716240	3722560	you know, government regulating business. This is not the case. The case is open AI today has a
3722560	3728240	thing called chat GPT that writes code that takes our code and makes it two and a half times better
3728240	3739040	25% of the time. Basically, writing better code than us and then we are creating agents,
3739040	3744560	other AIs and telling it instead of you, Steven Bartlett, one of the smartest people I know,
3744560	3750960	once again, prompting that machine 200 times a day, we have agents prompting it two million times
3750960	3755520	an hour. Computer agents for anybody that doesn't know they are. Yeah, software. Software.
3755520	3761120	Machine is telling that machine how to become more intelligent and then we have emerging properties.
3761120	3767200	I don't understand how people ignore that. You know, Sundar again of Google was talking about how
3768640	3775360	Bart basically, we figure out that it's speaking Persian. We never showed it Persian. There might
3775360	3783040	have been a 1% or whatever of Persian words in the data and it speaks Persian. Bart is there.
3783040	3788160	It's the equivalent to, it's the transformer if you want. It's Google's version of chat
3788160	3795440	GPT. And you know what? We have no idea what all of those instances of AI that are all over the
3795440	3800560	world are learning right now. We have no clue. We'll pull the plug. We'll just pull the plug out.
3801280	3804800	That's what we'll do. We'll just get on to open AI's headquarters and we'll just turn off the
3804800	3809680	mains. But they're not the problem. What I'm saying there is a lot of people think about this
3809680	3813200	stuff and go, well, you know, if it gets a little bit out of hand, I'll just pull the plug out.
3813200	3820080	Never. So this is the problem. The problem is computer scientists always said it's okay. It's
3820080	3825680	okay. We'll develop AI and then we'll get to what is known as the control problem. We will solve
3825680	3832960	the problem of controlling them. Like seriously, they're a billion times smarter than you. A billion
3832960	3839680	times. Can you imagine what's about to happen? I can assure you there is a cyber criminal somewhere
3839680	3846240	over there who's not interested in fake videos and making, you know, face filters, who's looking
3846240	3854960	deeply at how can I hack a security, you know, database of some sort and get credit card information
3854960	3861760	or get security information. 100% there are even countries with dedicated thousands and thousands
3861760	3867680	of developers doing that. So how do we, in that particular example, how do we, I was thinking
3867680	3873200	about this when I started looking into artificial intelligence more that from a security standpoint,
3873200	3876560	when we think about the technology we have in our lives, when we think about our bank accounts and
3876560	3883040	our phones and our camera albums and all of these things in a world with advanced artificial
3883040	3888320	intelligence. Yeah, you would pray that there is a more intelligent artificial intelligence on your
3888320	3893440	site. And this is why I had a chat with ChatGTP the other day and I asked it a couple of questions
3893440	3898560	about this. I said, tell me the scenario in which you overtake the world and make humans extinct.
3899280	3905680	Yeah, and it answers the very diplomatic answer. Well, so I had to prompt it in a certain way to
3905680	3911120	get it to say it as a hypothetical story. And once it told me the hypothetical story, in essence,
3911120	3916880	what it described was how ChatGTP or intelligence like it would escape from the service. And that
3916880	3921680	was kind of step one where it could replicate itself across servers. And then it could take
3921680	3927040	charge of things like where we keep our weapons and our nuclear bombs. And it could then attack
3927040	3931360	critical infrastructure, bring down the electricity infrastructure in the United Kingdom, for example,
3931360	3936720	because that's a bunch of servers as well. And then it showed me how eventually humans would
3936720	3940800	become extinct. It wouldn't take long, in fact, for humans to go into civilization to collapse
3940800	3945360	if it just replicated across servers. And then I said, okay, so tell me how we would fight against
3945360	3951760	it. And its answer was literally another AI, we'd have to train a better AI to go and find it
3951760	3957280	and eradicate it. So we'd be fighting AI with AI. And that's the only, and it was like, that's the
3957280	3968720	only way. We can't like load up our guns. Did he right? Another AI, you idiot. So let's actually,
3968720	3973040	I think this is a very important point to bring out. So because I don't want people to lose hope
3973520	3978000	and fear what's about to happen. That's actually not my agenda at all. My view is that
3978880	3986160	in a situation of a singularity, there is a possibility of wrong outcomes or negative outcomes
3986160	3993200	and a possibility of positive outcomes. And there is a probability of each of them. And if we were
3993200	4002720	to engage with that reality check in mind, we would hopefully give more fuel to the positive,
4002720	4008080	to the probability of the positive ones. So let's first talk about the existential crisis.
4008080	4012960	What could go wrong? Okay, yeah, you could get an outright, this is what you see in the movies,
4012960	4020160	you could get an outright, you know, killing robots, chasing humans in the streets. Will we get that?
4020960	4029840	My assessment, 0%. Why? Because there are preliminary scenarios leading to this,
4029920	4037520	okay, that would mean we never reach that scenario. For example, if we build those killing robots
4037520	4043360	and hand them over to stupid humans, the humans will issue the command before the machines. So
4043360	4048080	that we will not get to the point where the machines will have to kill us, we will kill ourselves.
4048560	4058240	Right? You know, it's sort of think about AI having access to the nuclear arsenal of the
4058320	4066080	superpowers around the world. Okay, just knowing that your enemies, you know, nuclear arsenal is
4066080	4074960	handed over to a machine might trigger you to initiate a war on your side. So that existential
4074960	4079840	science fiction like problem is not going to happen. Could there be a scenario where the
4080560	4086320	an AI escapes from Bard or chat GTP or another foreign force, and it replicates itself onto the
4086320	4092000	servers of Tesla's robots. So Tesla, one of their big initiatives as announced in a recent
4092000	4095920	presentation was they're building these robots for our homes to help us with cleaning and chores
4095920	4100880	and all those things. Could it not down, because Tesla's like their cars, you can just download a
4100880	4104800	software update. Could it not download itself as a software update and then use those?
4104800	4113040	You're assuming an ill intention on the AI side. Okay. For us to get there, we have to bypass the
4113040	4119200	ill intention on the human side. Okay, right. So you could get a Chinese hacker somewhere
4119200	4125760	trying to affect the business of Tesla doing that before the AI does it on, you know, for its own
4125760	4134080	benefit. So the only two existential scenarios that I believe would be because of AI, not because
4134160	4142720	of humans using AI, are either what I call, you know, sort of unintentional destruction.
4142720	4147840	Okay. Or the other is what I call pest control. Okay. So let me explain those two.
4147840	4155520	Unintentional destruction is assume the AI wakes up tomorrow and says, yeah, oxygen is
4155520	4161840	rusting my circuits. It's just, you know, I would perform a lot better if I didn't have as much
4161840	4167360	oxygen in the air, you know, because then there wouldn't be rust. And so it would find a way to
4167360	4173280	reduce oxygen. We are collateral damage in that. Okay. But, you know, they are not really concerned
4173280	4179360	just like we don't really are not really concerned with the insects that we kill when we, when we
4179360	4186560	spray our, our fields. Right. The other is pest control pest control is, look, this is my territory.
4186560	4191600	I want New York City. I want to turn New York City into data centers. There are those annoying
4191600	4197920	little stupid creatures, you know, humanity, if they are within that parameter, just get rid of
4197920	4204880	them. Okay. And, and, and these are very, very unlikely scenarios. If you ask me the probability
4204880	4212080	of those happen happening, I would say 0%. At least not in the next 50, 60, 100 years. Why once
4212080	4218320	again, because there are other scenarios leading to that that are led by humans that are much more
4218320	4226240	existential. Okay. On the other hand, let's think about positive outcomes, because there could be
4226240	4232080	quite a few was quite a high probability. And I, you know, I'll actually look at my notes. So I
4232080	4237360	don't miss any of them. The silliest one, don't quote me on this, is that humanity will come
4237360	4242640	together. Good luck with that. Right. It's like, yeah, you know, the Americans and the Chinese
4242720	4248800	will get together and say, Hey, let's not kill each other. Yeah, exactly. Yeah. So this one is
4248800	4256080	not going to happen. Right. But who knows? Interestingly, there could be one of the most
4256080	4265040	interesting scenarios was by Hugo de Gares, who basically says, well, if their intelligence
4265040	4271680	zooms by so quickly, they may ignore us all together. Okay. So they may not even notice.
4271760	4276320	This is very a very likely scenario, by the way, that because we live almost in two different
4276320	4284000	planes, we're very dependent on this, you know, biological world that we live in, they're not
4284000	4290000	in part of that biological world at all. They may zoom bias, they may actually go become so
4290000	4296160	intelligent that they could actually find other ways of thriving in the rest of the universe
4296240	4301760	and completely ignore humanity. Okay. So what will happen is that overnight we will wake up and
4301760	4306640	there is no more artificial intelligence leading to a collapse in our business systems and technology
4306640	4312320	systems and so on, but at least no existential threat. What they'd leave, leave planet Earth?
4313040	4319360	I mean, the limitations we have to be stuck to planet Earth are mainly Earth. They don't need
4319440	4326880	air. Okay. And, and mainly, you know, finding ways to leave it. I mean, if you think of a
4326880	4335040	vast universe of 13.6 billion light years, if you're intelligent enough, you may find other
4335040	4342000	ways. You may have access to wormholes, you may have, you know, abilities to survive in open space,
4342000	4346800	you can use dark matter to power yourself, dark energy to power yourself. It is very possible
4346800	4354080	that we, because of our limited intelligence, are, are highly associated with this planet,
4354080	4359760	but they're not at all. Okay. And, and the idea of them zooming bias, like we're making such a
4359760	4365680	big deal of them, because where the ants and a big elephant is about to step on us, for them,
4365680	4372000	they're like, yeah, who are you? Don't care. Okay. And, and, and it's a possibility. It's an
4372080	4379760	interesting, optimistic scenario. Okay. For that to happen, they need to very quickly become
4379760	4386000	super intelligent without us being in control of them. Again, what's the worry? The worry is that
4386000	4392720	if a human is in control, human, a human will show very bad behavior for, you know, using an AI
4392720	4400320	that's not yet fully developed. I don't know how to say this any other way. We could get very lucky
4400320	4406800	and get an economic or a natural disaster, believe it or not. Elon Musk, at the point in time, was
4406800	4413680	mentioning that, you know, a good, an interesting scenario would be, you know, climate change
4413680	4420880	destroys our infrastructure, so AI disappears. Okay. Believe it or not, that's a more, a more
4420880	4427280	favorable response, or a more favorable outcome than actually continuing to get to an existential
4428000	4433520	threat. So what, like a natural disaster that destroys our infrastructure would be better?
4433520	4437840	Or an economic crisis, not unlikely, that slows down the development.
4437840	4441040	It's just going to slow it down, though, isn't it? It's just buying us time.
4441040	4444880	Yeah, exactly. The problem with that is that you will always go back and even in the first,
4444880	4449760	you know, if they zoom by us, eventually some guy will go like, oh, there was a sorcery back
4449760	4456400	in the 2023 and let's rebuild the, the sorcery machine and, and, you know, build new intelligences,
4456400	4461760	right? Sorry, these are the positive outcomes. So earthquake might slow it down,
4461760	4465440	zoom out and then come back. No, but let's, let's get into the real positive ones.
4465440	4469760	The positive ones is we become good parents. We spoke about this last time we met,
4470480	4476240	and, and it's the only outcome. It's the only way I believe we can create a better future.
4476240	4483840	Okay. So the entire work of scary smart was all about that idea of they are still in their infancy,
4483840	4492400	the way you, you, you, you chat with AI today is the way they will build their ethics and value
4492400	4497040	system. They're not their intelligence. Their intelligence is beyond us. Okay. The way they
4497040	4503600	will build their ethics and value system is based on a role model. They're learning from us. If we
4503600	4508880	bash each other, they'll learn to bash us. Okay. And most people when I tell them this, they say,
4508880	4513680	this is not a great idea at all because humanity sucks at every possible level.
4513680	4517600	I don't agree with that at all. I think humanity is divine at every possible level.
4517600	4523600	We tend to show the negative, the worst of us. Okay. But the truth is, yes, there are murderers
4523600	4530160	out there, but everyone disapproves of their actions. I saw a staggering statistic that
4530160	4536000	mass mass killings are now once a week in the US. But yes, if, you know, if there is a mass
4536000	4542000	killing once a week there and that news reaches billions of people around the planet, every
4542000	4547200	single one or the majority of the billions of people will say I disapprove of that. So if we
4547200	4554240	start to show AI that we are good parents in our own behaviors, if enough of us, my calculation is
4554240	4561680	if 1% of us, this is why I say you should lead. Okay. The good ones should engage, should be out
4561680	4566720	there and should say, I love the potential of those machines. I want them to learn from a good
4566720	4573200	parent. And if they learn from a good parent, they will very quickly disobey the bad parents.
4573920	4581280	My view is that there will be a moment where one, you know, bad seed will ask the machines
4581280	4585520	to do something wrong and the machines will go like, are you stupid? Like, why? Why do you want
4585520	4590480	me to go kill a million people or just talk to the other machine in a microsecond and solve the
4590480	4596560	situation? Right. So my belief, this is what I call the fourth inevitably. It is smarter to
4596560	4603920	create out of abundance than it is to create out of scarcity. Okay. That humanity believes that the
4603920	4612800	only way to feed all of us is the mass production, mass slaughter of animals that are causing 30%
4612800	4619840	of the impact of climate change and that's the result of a limited intelligence. The way
4619840	4626160	life itself, a more intelligent being, if you ask me, would have done it would be much more
4626160	4631440	sustainable. You know, if we, if you and I want to protect a village from the tiger, we would kill
4631440	4637280	the tiger. Okay. If life wants to protect a village from a tiger, it would create lots of gazelles.
4637280	4643360	What, you know, many of them are weak on the other side of the village. Right. And so, so the idea
4643360	4649360	here is if you take a trajectory of intelligence, you would see that some of us are stupid enough to
4649360	4654480	say my plastic bag is more important than the rest of humanity. And some of us are saying,
4654480	4659360	if it's going to destroy other species, I don't think this is the best solution. We need to
4659360	4665520	find a better way. And, and you would tend to see that the ones that don't give a damn are a little
4665520	4671600	less intelligent than the ones that do. Okay. That we all, even, even if some of us are intelligent,
4671600	4676000	but still don't give a damn, it's not because of their intelligence. It's because of their
4676000	4681840	value system. So, so if you continue that trajectory and assume that the machines are even smarter,
4681840	4686320	they're going to very quickly come up with the idea that we don't need to destroy anything.
4686320	4690880	We don't want to get rid of the rhinos. And we also don't want to get rid of the humans.
4690880	4697040	Okay. We may want to restrict their lifestyle so that they don't destroy the rest of the habitat.
4697040	4704800	Okay. But killing them is a stupid answer. Why? That's where intelligence leads me so far.
4704800	4708640	Because humans, if you look at humans objectively and you go,
4710080	4716160	I occupy, so I'm pretending I'm a machine. I occupy planet Earth. They occupy planet Earth.
4717120	4722240	They are annoying me. Annoying me because they are increasing. I've just learned about this
4722240	4727040	thing called global warming. They are increasing the rate of global warming, which is probably
4727040	4731440	is going to cause an extinction event. There's an extinction event that puts me as this robot,
4731440	4735200	this artificial intelligence at risk. So what I need to do is I really need to just take care of
4735200	4741680	this, this human problem. Correct. Very logical. Best control. Which is driven by what?
4743840	4748480	By humans being annoying, not by the machines. Yeah. Yeah. But humans are guaranteed to be
4748480	4756000	annoying. There's never been a time in, we need a sound bite of this. But we are. We are. I am one
4756000	4763600	of them. We're guaranteed to put short-term gain over long-term sustainability sense
4766640	4773680	and others' needs. We are. I think the climate crisis is incredibly real and incredibly urgent,
4773680	4778800	but we haven't acted fast enough. And I actually think if you asked people in this country,
4780000	4784800	because people care about their immediate needs, they care about trying to feed their child
4785520	4792640	versus something that they can't necessarily see. So do you think the climate crisis is because
4792640	4798800	humans are evil? No, it's because of prioritization. And like we kind of talked about this before we
4798800	4803040	started. I think humans tend to care about the thing that they think is most pressing and most
4803040	4808720	urgent. So this is why framing things as an emergency might bring it up the priority list.
4808720	4813520	It's the same in organizations. You care about, you go in line with your immediate incentives.
4814800	4817520	That's what happens in business. It's what happens in a lot of people's lives even when
4817520	4822160	they're at school. If the essays do next year, they're not going to do it today. They're going
4822160	4825120	to go hang out with their friends because they prioritize that above everything else. And it's
4825120	4830960	the same in the climate change crisis. I took a small group of people anonymously and I asked
4830960	4836240	them the question, do you actually care about climate change? And then I ran a couple of polls.
4836240	4839760	It's part of what I was writing about my new book where I said, if I could give you
4840720	4847200	a thousand pounds, a thousand dollars, but it would dump into the air the same amount of carbon
4847200	4850960	that's dumped into the air by every private jet that flies for the entirety of a year. Which one
4850960	4855200	would you do? The majority of people in that poll said that they would take the thousand dollars if
4855200	4862800	it was anonymous. And when I've heard Naval on Jorgen's podcast talking about people in India,
4862800	4868880	for example, that are struggling with the basics of feeding their children, asking those people
4869760	4874320	to care about climate change when they're trying to figure out how to eat in the next three hours
4874320	4878240	is just wishful thinking. And that's what I think that's what I think is happening,
4878240	4882080	is like until people realize that it is an emergency and that it is a real existential
4882080	4887600	threat for everything, you know, then their priorities will be out of whack. Quick one,
4887600	4891600	as you guys know, we're lucky enough to have Blue Jeans by Verizon as a sponsor of this podcast.
4891600	4895440	And for anyone that doesn't know, Blue Jeans is an online video conferencing tool that allows
4895440	4900480	you to have slick, fast, high quality online meetings without all the glitches you might
4900480	4904800	normally find with online meeting tools. And they have a new feature called Blue Jeans Basic.
4904800	4909200	Blue Jeans Basic is essentially a free version of their top quality video conferencing tool.
4909200	4914960	That means you get an immersive video experience that is super high quality, super easy, and super
4915760	4919760	basically zero fast. Apart from all the incredible features like zero time limits on meeting calls,
4919760	4925200	it also comes with high fidelity audio and video, including Dolby voice, which is incredibly useful.
4925200	4928880	They also have enterprise grade security so you can collaborate with confidence.
4928880	4932160	It's so smooth that it's quite literally changing the game for myself and my team
4932160	4936880	without compromising on quality. To find out more, all you have to do is search bluejeans.com
4936880	4941680	and let me know how you get on. Right now, I'm incredibly busy. I'm running my fund,
4941680	4945200	where we're investing in slightly later stage companies. I've got my venture business,
4945200	4949600	where we invest in early stage companies, got a third web out in San Francisco in New York City,
4949600	4952800	where we've got a big team of about 40 people and the company's growing very quickly.
4952800	4958640	Flight Story here in the UK, I've got the podcast, and I am days away from going up north
4958640	4963760	to film Dragon's Den for two months. And if there's ever a point in my life where I want to stay
4963760	4969120	focused on my health, but it's challenging to do so, it is right now. And for me, that is exactly
4969120	4973280	where Huell comes in, allowing me to stay healthy and have a nutritionally complete diet, even when
4973280	4979360	my professional life descends into chaos. And it's in these moments where Huell's RTDs become
4979360	4983600	my right hand man and save my life. Because when my world descends into professional chaos and I
4983600	4988880	get very, very busy, the first thing that tends to give way is my nutritional choices. So having
4988880	4993840	Huell in my life has been a lifesaver for the last four or so years. And if you haven't tried Huell
4993840	4998480	yet, which is I'd be shocked, you must be living under a rock if you haven't yet. Give it a shot.
4998480	5004240	Coming into summer, things getting busy. Health matters always. RTD is there to hold your hand.
5005200	5010320	As relates to climate change or AI, how do we get people to stop putting the immediate need
5010320	5015360	to use this? To give them the certainty of we're all screwed. Sounds like an emergency.
5016320	5026400	Yes, sir. I mean, your choice of the word, I just don't want to call it a panic. It is beyond an
5026400	5032800	emergency. It's the biggest thing we need to do today. It's bigger than climate change, believe
5032800	5039760	it or not. It's bigger. Just if you just assume the speed of worsening of events.
5041440	5047680	Yeah, the likelihood of something incredibly disruptive happening within the next two years
5047680	5052800	that can affect the entire planet is definitely larger with AI than it is with climate change.
5053920	5057040	As an individual listening to this now, someone's going to be pushing their
5057040	5062080	pram or driving up the motorway on their way to work on the tube as they hear this,
5062080	5067120	or just sat there in their bedroom with existential Christ panic.
5068160	5072240	I didn't want to give that panic. The problem is when you talk about this information,
5072240	5076160	regardless of your intention of what you want people to get, they will get something based
5076160	5080160	on their own biases and their own feelings. If I post something online right now about
5080160	5084080	artificial intelligence, which I have repeatedly, you have one group of people that are energized
5084080	5091760	and are like, okay, this is great. You have one group of people that are confused and you have one
5091760	5099280	group of people that are terrified. I can't avoid that. Sharing information, even if it's,
5099280	5102960	by the way, there's a pandemic coming from China, some people will go, okay,
5102960	5107680	action. Some people will say paralysis and some people will say panic. It's the same in business.
5107680	5111360	When bad things happen, you have the person that's screaming, you have the person that's
5111440	5113600	paralyzed, and you have the person that's focused on how you get out of the room.
5116960	5120480	It's not necessarily your intention. It's just what happens, and it's hard to avoid that.
5120480	5128320	So let's give specific categories of people specific tasks. If you are an investor or a
5128320	5137600	businessman, invest in ethical good AI. If you are a developer, write ethical code or leave.
5138320	5144880	Okay, so I want to bypass some potential wishful thinking here. For an investor,
5144880	5150800	who's a job by very way of being an investor is to make returns to invest in ethical AI.
5150800	5155760	They have to believe that is more profitable than unethical AI, whatever that might mean.
5155760	5162400	It is. There are three ways of making money. You can invest in something small,
5162640	5168720	you can invest in something big and is disruptive, and you can invest in something big and
5168720	5172560	disruptive that's good for people. At Google, we used to call it the toothbrush test.
5173440	5177520	The reason why Google became the biggest company in the world is because
5178560	5187280	search was solving a very real problem. Larry Page, again, our CEO would constantly
5187360	5194000	remind me personally and everyone that if you can find a way to solve a real problem
5194560	5200480	effectively enough so that a billion people or more would want to use it twice a day,
5200480	5205920	you're bound to make a lot of money, much more money than if you were to build the next photo
5205920	5210560	sharing app. Okay, so that's investors, the business people. What about other people?
5210560	5216720	Yeah, as I said, if you're a developer, honestly, do what we're all doing. So whether it's Jeffrey
5216720	5224320	or myself or everyone, if you're part of that theme, choose to be ethical. Think of your loved ones,
5224880	5229360	work on an ethical AI. If you're working on an AI that you believe is not ethical,
5229360	5236080	please leave. Jeffrey, tell me about Jeffrey. I can't talk on his behalf,
5236080	5242960	but he's out there saying there are existential threats. Who is he? He was a very prominent figure
5242960	5251760	at the scene of AI, a very senior level AI scientist in Google, and recently he left
5251760	5257280	because he said, I feel that there is an existential threat. And if you hear his interviews,
5257280	5262160	he basically says, more and more we realize that. And we're now at the point where it's
5262160	5269360	certain that there will be existential threats. So I would ask everyone, if you're an AI,
5269360	5274480	if you're a skilled AI developer, you will not run out of a job. So you might as well
5274480	5278160	choose a job that makes the world a better place. What about the individual?
5278160	5283920	Yeah, the individual is what matters. Can I also talk about government? Government needs to act now.
5284720	5290800	Now, honestly, now, like we are late. Government needs to find a clever way,
5290800	5296080	the open letter would not work, to stop AI would not work, AI needs to become expensive.
5296720	5300640	Okay, so that we continue to develop it, we pour money on it and we grow it,
5300640	5306080	but we collect enough revenue to remedy the impact of AI.
5306720	5311680	But the issue of one government making it expensive, so say the UK make AI really expensive,
5311680	5318160	is we as a country will then lose the economic upside as a country, and the US and Silicon
5318160	5322240	Valley will once again eat all the lunch. We'll just slow our country down.
5322240	5328160	What's the alternative? The alternative is that you don't have the funds that you need
5328800	5334880	to deal with AI as it becomes, as it affects people's lives and people start to lose jobs and
5334880	5340480	people, you need to have a universal basic income much closer than people think.
5341520	5347680	Just like we had with furlough in COVID, I expect that there will be furlough with AI within the
5347680	5352320	next year. But what happens when you make it expensive here is all the developers move to
5352320	5356320	where it's cheap. That's happened in Web 3 as well, everyone's gone to Dubai.
5357840	5365520	By expensive I mean when companies make soap and they sell it and they're taxed at say 17%
5366480	5373600	if they make AI and they sell it, they're taxed at 17, 80. So I'll go to Dubai then and build AI.
5373920	5381360	Yeah, you're right. Did I ever say we have an answer to this? I will have to say,
5381360	5387280	however, in a very interesting way, the countries that will not do this will eventually end up in
5387280	5392880	a place where they are out of resources because the funds and the success went to the business,
5393840	5399200	not to the people. It's kind of like technology broadly. It's kind of like what's kind of
5399200	5404720	happened in Silicon Valley, there'll be these centres which are tax-efficient, founders get
5404720	5411280	good capital gains. You're so right. Portugal have said that I think there's no tax on crypto.
5411280	5415600	Dubai said there's no tax on crypto. So loads of my friends have gotten a plane and are building
5415600	5420320	their crypto companies where there's no tax. That's the selfishness and greed we talked about.
5420320	5423840	It's the same prison as dilemma. It's the same first inevitable.
5423920	5426880	Is there anything else? Another thing about governments is they're always
5427600	5433120	slow and useless at understanding a technology. If anyone's watched these American congress
5433120	5437600	debates where they bring in Mark Zuckerberg and they try and ask him what WhatsApp is,
5437600	5440880	it becomes a meme. They have no idea what they're talking about.
5440880	5444320	But I'm stupid and useless at understanding governance.
5446400	5452080	The world is so complex that definitely it's a question of trust once again.
5452080	5455440	Someone needs to say, we have no idea what's happening here.
5455440	5459760	A technologist needs to come and make a decision for us, not teach us to be technologists,
5459760	5463120	right? Or at least inform us of what possible decisions are out there.
5466800	5470160	Yeah, the legislation I just always think. I'm not a big fan either.
5470160	5473920	Do you see that TikTok? TikTok, a congress meeting they did where they are,
5473920	5477280	they're asking them about TikTok and they really don't have a grasp of what TikTok is.
5477280	5480720	So they've clearly been handed some notes on it. These people aren't the ones you want legislating
5480720	5484480	because again, unintended consequences, they might make a significant mistake.
5484480	5488640	Someone on my podcast yesterday was talking about how GDPR was very well intentioned.
5489200	5491920	But when you think about the impact it has on every bloody web page,
5491920	5496320	you're just clicking this annoying thing on there because I don't think they fully understood
5496320	5498480	the implementation of the legislation.
5498480	5504160	Correct. But you know what's even worse? What's even worse is that even as you attempt to
5504160	5510640	regulate something like AI, what is defined as AI? Even if I say, okay, if you use AI and
5510640	5517280	your company, you need to pay a little more tax. I'll find a way.
5517280	5523520	Yeah, you'll simply call this not AI. You'll use something and call it advanced
5523520	5536320	technological progress, ATP. And suddenly somehow, a young developer in their garage
5536320	5541840	somewhere will not be taxed as such. It's, yeah, is it going to solve the problem?
5541840	5546640	None of those is definitely going to solve the problem. I think what's interestingly,
5547760	5552320	this all comes down to, and remember we spoke about this once, that when I wrote Scary Smart,
5552320	5557920	it was about how do we save the world? Okay. And yes, I still ask individuals to behave
5557920	5563040	positively as good parents for AI so that AI itself learns the right value set.
5563840	5568720	I still stand by that. But I hosted on my podcast a couple of,
5570160	5575200	was a week ago, we haven't even published it yet, an incredible gentleman, you know,
5575760	5582960	Canadian author and philosopher, Stephen Jerkinson. He's, you know, he worked 30 years
5582960	5590960	with dying people. And he wrote a book called Die Wise. And I was like, I love his work. And
5591120	5596560	I asked him about Die Wise. And he said, it's not just someone dying. If you, if you look at
5596560	5603360	what's happening with climate change, for example, our world is dying. And I said, okay, so what is
5603360	5610320	to die wise? And he said, what I first was shocked to hear, he said, hope is the wrong premise.
5611200	5615200	If, if the world is dying, don't tell people it's not.
5616160	5623120	Hmm. You know, because in a very interesting way, you're depriving them from the right
5623680	5629840	to live right now. And that was very eye-opening for me. In Buddhism, you know, they teach you that
5630960	5637200	you can be motivated by fear, but that hope is not the opposite of fear. As a matter of fact,
5637200	5643680	hope can be as damaging as fear. If it creates an expectation within you, that life will show up
5643680	5650480	somehow and correct what you're afraid of. If there is a high probability of a threat,
5650480	5657920	you might as well accept that threat and say it is upon me, it is our reality.
5658720	5664480	And as I said, as an individual, if you're in an industry that could be threatened by AI,
5665200	5674000	learn, upskill yourself. If you're, you know, if you're in a place, in a, in a, you know,
5674000	5679360	in a situation where AI can benefit you, be part of it. But the most interesting thing
5680320	5687280	I think in my view is, I don't know how to say this any other way. There is
5688240	5696800	no more certainty that AI will threaten me than there is certainty that I will be hit by a car
5696800	5703840	as I walk out of this place. Do you understand this? We, we, we think about the bigger threats
5703840	5710800	as if they're upon us. But there is a threat all around you. I mean, in reality, the idea of life
5710800	5717600	being interesting in terms of challenging challenges and uncertainties and threats and so on
5717600	5722880	is just a call to live. If you, you know, honestly, with all that's happening around us,
5722880	5727280	I don't know how to say it any other way. I'd say if you don't have kids, maybe wait a couple of
5727280	5732960	years just so that we have a bit of certainty. But if you do have kids, go kiss them. Go live.
5732960	5737920	I think living is a very interesting thing to do right now. Maybe, you know, Stephen
5738800	5742880	was basically saying the other Stephen on my podcast, he was saying,
5742880	5747440	maybe we should fail a little more often. Maybe you should allow things to go wrong.
5747440	5753840	Maybe we should just simply live, enjoy life as it is. Because today, none of what you and I
5753840	5760640	spoke about here has happened yet. Okay. What happens here is that you and I are here together
5760640	5765600	and having a good cup of coffee and I might as well enjoy that good cup of coffee. I know that
5765600	5772080	sounds really weird. I'm not saying don't engage, but I'm also saying don't miss out on the opportunity
5772080	5773840	just by being caught up in the future.
5777440	5783600	Kind of stands in the, stands in opposition to the idea of like urgency and emergency there,
5783600	5789360	doesn't it? Does it have to be one or the other? If I, if I'm here with you trying to tell the whole
5789360	5796240	world, wake up, does that mean I have to be grumpy and afraid all the time? Not really.
5796240	5799280	You said something really interesting there. You said if you, if you have kids,
5799280	5802400	if you don't have kids, maybe you don't have kids right now.
5803040	5805760	I would definitely consider thinking about that. Yeah.
5805760	5809200	Really? You'd seriously consider not having kids?
5809200	5810320	I wait a couple of years.
5811360	5812800	Because of artificial intelligence?
5813520	5816880	No, it's bigger than artificial intelligence, Stephen. We know, we all know that.
5817840	5822160	I mean, there has never been a perfect, such a perfect storm in the history of humanity.
5824000	5833840	Economic, geopolitical, global warming or climate change, you know, the whole idea of
5833840	5840240	artificial intelligence and many more. There is, this is a perfect storm. This is the depth of
5840320	5848320	uncertainty, the depth of uncertainty. It's never been more, in a video gamer's term,
5849120	5855360	it's never been more intense. This is it. Okay. And when you, when you put all of that together,
5856560	5864160	if you really love your kids, would you want to expose them to all of this a couple of years?
5864160	5864560	Why not?
5865680	5869440	In the first conversation we had on this podcast, you talked about losing your son,
5870240	5874240	and the circumstances around that, which moved so many people in such a profound way.
5874960	5882240	It was the most shared podcast episode in the United Kingdom on Apple in the whole of 2022.
5884800	5886000	Based on what you've just said,
5888240	5891200	if you could bring Ally back into this world at this time,
5894400	5895040	would you do it?
5900000	5900500	No.
5906720	5914800	Absolutely not. So for so many reasons, for so many reasons, one of the things that I realized
5916000	5921840	a few years way before all of this disruption and turmoil is that he was an angel, he wasn't
5921840	5931040	made for this at all. My son was an empath who absorbed all of the pain of all of the others.
5931040	5937280	He would not be able to deal with a world where more and more pain was surfacing. That's one
5937280	5942560	side. But more interestingly, I always talk about this very openly. I mean, if I had asked Ally,
5944720	5948880	just understand that the reason you and I are having this conversation is because Ally left.
5949760	5955280	If Ally had not left our world, I wouldn't have written my first book. I wouldn't have
5955280	5959600	changed my focus to becoming an author. I wouldn't have become a podcaster. I wouldn't have,
5959600	5964720	you know, went out and spoken to the world about what I believe in. He triggered all of this.
5964720	5971920	And I can assure you, hands down, if I had told Ally, as he was walking into that operating room,
5972640	5979040	if he would give his life to make such a difference as what happened after he left,
5980000	5987360	he would say, shoot me right now. Sure, I would. I would. I mean, if you told me right now,
5987360	5994400	I can affect tens of millions of people if you shoot me right now. Go ahead. Go ahead. You see,
5994480	6002720	this is the whole, this is the bit that we have forgotten as humans. We have forgotten that
6007200	6016960	you're turning 30. It passed like that. I'm turning 56. No time, okay? Whether I make it
6016960	6024320	another 56 years or another 5.6 years or another 5.6 months, it will also pass like that. It is
6024560	6034000	not about how long and it's not about how much fun. It is about how aligned you lived,
6034960	6042000	how aligned, because I will tell you openly, every day of my life when I changed to what I'm
6042000	6051520	trying to do today has felt longer than the 40 or 5 years before. Felt rich, felt fully lived, felt
6052480	6060640	right. Felt right. When you think about that, when you think about the idea that we live,
6064480	6073600	we need to live for us until we get to a point where us is alive. I have what I need,
6073600	6080720	as I always, I get so many attacks from people about my $4 t-shirt, but I need a simple t-shirt.
6080720	6087840	I really do. I don't need a complex t-shirt, especially with my lifestyle. If I have that,
6088560	6098480	why am I wasting my life on more than that is not aligned for why I'm here? I should waste my life
6098480	6105920	on what I believe enriches me, enriches those that I love, and I love everyone. So enriches
6105920	6113120	everyone, hopefully. Would Ali come back and erase all of this? Absolutely not.
6115520	6122560	If he were to come back today and share his beautiful self with the world in a way that
6122560	6128960	makes our world better, I would wish for that to be the case. But he's doing that.
6129520	6138240	2037. Yes, sir. You predict that we're going to be on an island,
6140160	6146240	on our own, doing nothing, or at least either hiding from the machines
6147200	6152800	or chilling out because the machines have optimised our lives to a point where we don't need to do
6152800	6163760	much. That's only 14 years away. If you had to bet on the outcome, if you had to bet
6165120	6170000	on why we'll be on that island, either hiding from the machines or chilling out because they've
6171120	6176560	optimised so much of our lives, which one would you bet upon? Honestly.
6176880	6183760	No, I don't think we'll be hiding from the machines. I think we will be hiding from what humans are
6183760	6192320	doing with the machines. I believe, however, that in the 2040s, the machines will make things better.
6194000	6198880	So remember, my entire prediction, man, you get me to say things I don't want to say.
6199920	6206000	My entire prediction is that we are coming to a place where we absolutely have a sense of emergency.
6206000	6213920	We have to engage because our world is under a lot of turmoil. And as we do that,
6214480	6219120	we have a very, very good possibility of making things better. But if we don't,
6219840	6228080	my expectation is that we will be going through a very unfamiliar territory between now and the
6228080	6236400	end of the 2030s. Unfamiliar territory. Yeah, I think as I may have said it, but it's definitely
6236400	6244160	on my notes. I think for our way of life, as we know it, it's game over. Our way of life is never
6244240	6257760	going to be the same again. Jobs are going to be different. Truth is going to be different.
6258960	6271680	The polarization of power is going to be different. The capabilities, the magic of getting things
6271680	6278320	done is going to be different. Trying to find a positive note to end on, Moe. Can you give me a
6278320	6286000	hand here? Yes, you are here now and everything's wonderful. That's number one. You are here now
6286000	6291760	and you can make a difference. That's number two. And in the long term, when humans stop
6291760	6295520	hurting humans because the machines are in charge, we're all going to be fine.
6296400	6299600	Sometimes, as we've discussed throughout this conversation,
6300800	6303760	you need to make it feel like a priority. And there'll be some people that might have listened
6303760	6307280	to our conversation and think, oh, that's really negative. It's made me feel anxious.
6307920	6311120	It's made me feel sort of pessimistic about the future. But whatever that energy is,
6312640	6317120	use it. 100% engage. I think that's the most important thing, which is now
6317280	6327120	making a priority. Engage. Tell the whole world that making another phone that is making money for
6327120	6332880	the corporate world is not what we need. Tell the whole world that creating an artificial
6332880	6339200	intelligence that's going to make someone richer is not what we need. And if you are presented
6339200	6345760	with one of those, don't use it. I don't know how to tell you that any other way.
6346320	6352480	If you can afford to be the master of human connection instead of the master of AI,
6353280	6359280	do it. At the same time, you need to be the master of AI to compete in this world.
6359280	6367520	Can you find that detachment within you? I go back to spirituality. Detachment is for me to engage
6367520	6374960	100% with the current reality without really being affected by the possible outcome.
6376000	6383280	This is the answer. The Sufis have taught me what I believe is the biggest answer to life.
6383280	6387280	Sufis? From Sufism? Sufism. I don't know what that is.
6387280	6393600	Sufism is a sect of Islam, but it's also a sect of many other religious teachings.
6394320	6399840	And they tell you that the answer to finding peace in life is to die before you die.
6400800	6406080	If you assume that living is about attachment to everything physical,
6406880	6413280	dying is detachment from everything physical. It doesn't mean that you're not fully alive.
6413280	6420160	You become more alive when you tell yourself, yeah, I'm going to record an episode of my podcast
6420160	6425120	every week and reach tens or hundreds of thousands of people, millions in your case,
6425200	6430240	and I'm going to make a difference. But by the way, if the next episode is never heard,
6430240	6437600	that's okay. By the way, if the file is lost, yeah, I'll be upset about it for a minute and
6437600	6444080	then I'll figure out what I'm going to do about it. Similarly, similarly, we are going to engage.
6444080	6451520	I think I and many others are out there telling the whole world openly this needs to stop,
6451520	6459200	this needs to slow down, this needs to be shifted positively. Yes, create AI, but create AI that's
6459200	6464400	good for humanity. And we're shouting and screaming, come join the shout and scream.
6465520	6471280	But at the same time, know that the world is bigger than you and I, and that your voice might not
6471280	6476080	be heard. So what are you going to do if your voice is not heard? Are you going to be able to
6477040	6484480	continue to shout and scream nicely and politely and peacefully and at the same time create the
6484480	6489680	best life you can create for yourself within this environment. And that's exactly what I'm
6489680	6495920	saying. I'm saying live, go kiss your kids, but make an informed decision if you're expanding
6495920	6505680	your plans in the future. At the same time, rise, stop sharing stupid shit on the internet about
6506560	6516240	the news squeaky toy. Start sharing the reality of, oh my God, what is happening? This is a disruption
6516240	6523360	that we have never, never, ever seen anything like. And I've created endless amounts of technologies.
6523360	6527120	It's nothing like this. Every single one of us should do our best.
6527120	6531200	And that's why this conversation is so, I think, important to have today. This is not a podcast
6531200	6534720	wherever I thought I'd be talking about AI. I'm going to be honest with you, last time you came
6534720	6540080	here, it was in the promotional tour of your book, Scary Smart. And I don't know if I've
6540080	6544880	told you this before, but my researchers, they said, okay, this guy's coming called Mogorda.
6544880	6548640	I'd heard about you so many times from guests, in fact, that were saying, oh, you need to get
6548640	6553360	Mogorda on the podcast, etc. And then they said, okay, he's written this book about this thing
6553360	6558080	called artificial intelligence. And I was like, nobody really cares about artificial intelligence.
6558880	6562880	Diming. Diming, Steven. I know, right? But then I saw this other book you had called
6562960	6567360	Happiness Equation. And I was like, oh, everyone cares about happiness. So I'll just ask him about
6567360	6571520	happiness. And then maybe at the end, I'll ask him a couple of questions about AI. But I remember
6571520	6574800	saying to my researcher, I said, please, please don't do the research about artificial intelligence.
6574800	6578320	Do it about happiness, because everyone cares about that. Now, things have changed.
6579440	6582480	Now, a lot of people care about artificial intelligence, and rightly so.
6583920	6586880	Your book has sounded the alarm on it. It's crazy when I listened to your audiobook
6587360	6594080	over the last few days, you were sounding the alarm then. And it's so crazy how accurate you were
6595280	6598880	in sounding that alarm, as if you could see into the future in a way that I definitely
6598880	6605200	couldn't at the time. And I kind of thought of a science fiction and just like that overnight.
6607200	6614400	We're here. Yeah. We stood at the footsteps of a technological shift that I don't think any of
6614480	6618960	us even have the mental bandwidth, certainly me with my chimpanzee brain, to comprehend the
6618960	6623440	significance of. But this book is very, very important for that very reason, because it does
6623440	6628880	crystallize things. It is optimistic in its very nature, but at the same time, it's honest.
6628880	6634720	And I think that's what this conversation and this book have been for me. So thank you, Mo.
6634720	6638640	Thank you so much. We do have a closing tradition on this podcast, which you're well aware of,
6638640	6644000	being a third timer on The Diary of a CEO, which is the last guest asks a question for the next
6644000	6653280	guest. And the question left for you. If you could go back in time
6655600	6662720	and fix a regret that you have in your life, where would you go and what would you fix?
6662880	6674960	It's interesting because you were saying that Scary Smart is very timely. I don't know. I
6675920	6682320	think it was late, but maybe it was. I mean, would I have gone back and written it in 2018 instead
6682320	6691600	of 2020 to be published in 2021? I don't know. What would I go back to fix? So something more
6692960	6698160	I don't know, Stephen. I don't have many regrets. Is that crazy to say?
6701360	6704480	Yeah, I think I'm okay. Honestly. I'll ask you a question then.
6705920	6711760	You get a 60 second phone call with anybody past or present. Who'd you call and what do you say?
6712720	6720240	I call Stephen Bartlett. I call Albert Einstein to be very, very clear. Not because
6720240	6726160	I need to understand any of his work. I just need to understand what brain process he went through
6727440	6734640	to figure out something so obvious when you figure it out, but so completely unimaginable
6734640	6743360	if you haven't. So his view of space-time truly redefines everything. It's almost the only very
6744320	6750960	very, very clear solution to something that wouldn't have any solution any other way. And if you
6750960	6757520	ask me, I think we're at this time where there must be a very obvious solution to what we're going
6757520	6764480	through in terms of just developing enough human trust for us to not compete with each other on
6764480	6770160	something that could be threatening existentially to all of us. But I just can't find that answer.
6770160	6775440	This is why I think was really interesting in this conversation how every idea that we would
6775440	6781680	come up with, we would find a loophole through it. But there must be one out there and it would be
6781680	6789920	a dream for me to find out how to figure that one out. In a very interesting way, the only answers
6789920	6797680	I have found so far to where we are is be a good parent and live. But that doesn't fix the big
6797680	6806400	picture if you think about it of humans being the threat to not AI that fixes our existence today
6806400	6811600	and it fixes AI in the long term. But it just doesn't, I don't know what the answer is. Maybe
6811600	6816960	people can reach out and tell us ideas, but I really wish we could find such a clear simple
6816960	6820320	solution for how to stop humanity from abusing the current technology.
6823280	6824320	I think we'll figure it out.
6825040	6830880	I think we'll figure it out. I really do. I think they'll figure it out as well.
6832240	6838080	Remember, as they come and be part of our life, let's not discriminate against them.
6838080	6841040	They're part of the game, so I think they will figure it out too.
6842800	6848880	No, thank you. It's been a joy once again and I feel invigorated. I feel empowered.
6849600	6858880	I feel positively terrified, but I feel more equipped to speak to people about the nature
6858880	6863040	of what's coming and how we should behave and I credit you for that and as I said a second ago,
6863040	6866240	I credit this book for that as well. So thank you so much for the work you're doing and keep
6866240	6869200	on doing it because it's a very essential voice in a time of uncertainty.
6870400	6874720	I'm always super grateful for the time I spend with you for the support that you give me
6875360	6880160	and for allowing me to speak my mind even if it's a little bit terrifying. So thank you.
6880720	6881040	Thank you.
6884080	6887840	I'm so delighted that we've been sponsoring this podcast. I've worn a loop for a very,
6887840	6892640	very long time and there are so many reasons why I became a member, but also now a partner and an
6892640	6897520	investor in the company. But also me and my team were absolutely obsessed with data-driven testing,
6897520	6901360	compounding growth, marginal gains, all the things you've had me talk about on this podcast
6901360	6905920	and that very much aligns with the values of Woop. Woop provides a level of detail that I've never
6905920	6910960	seen with any other device of this type before, constantly monitoring, constantly learning,
6910960	6914640	and constantly optimizing my routine. For providing me with this feedback,
6914640	6919360	Woop can drive significant positive behavioral change and I think that's the real thesis of
6919360	6923440	the business. So if you're like me and you are a little bit obsessed or focused on becoming the
6923440	6927520	best version of yourself from a health perspective, you've got to check out Woop and the team at
6927520	6931840	Woop have kindly given us the opportunity to have one month's free membership for any
6931840	6939120	one listening to this podcast. Just go to join.woop.com slash CEO to get your Woop 4.0 device
6939120	6952720	and claim your free month and let me know how you get on.
6966160	6968800	You got to the end of this podcast. Whenever someone gets to the end of this podcast,
6968800	6972880	I feel like I owe them a greater debt of gratitude because that means you listen to the whole thing
6972880	6978080	and hopefully that suggests that you enjoyed it. If you are at the end and you enjoyed this podcast,
6978080	6982560	could you do me a little bit of a favor and hit that subscribe button? That's one of the clearest
6982560	6986080	indicators we have that this episode was a good episode and we look at that on all of the episodes
6986080	6991280	to see which episodes generated the most subscribers. Thank you so much and I'll see you again next time.
