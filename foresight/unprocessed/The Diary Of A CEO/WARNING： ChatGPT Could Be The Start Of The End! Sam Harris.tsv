start	end	text
0	2400	Artificial intelligence is superhuman.
2400	6800	It is smarter than you are and there's something inherently dangerous for the
6800	9800	dumber party in that relationship.
9800	12200	You just can't put the genie back in the bottle.
12200	13000	Damn Harris!
13000	15800	Neuroscientist, philosopher, author, podcaster.
15800	20600	He goes into intellectual territory where few others dare tread.
20600	22400	Six years ago you dared talk.
22400	26600	The gains we make in artificial intelligence could ultimately destroy us.
26600	31200	If your objective was to make humanity happy and there was a button placed in front of you
31200	35000	and it would end artificial intelligence, what would you do?
35000	37600	Well, I would definitely pause it.
37600	43000	The idea that we've lost the moment to decide whether to hook our most powerful AI to everything
43000	48600	is just, oh, it's already connected to the internet, got millions of people using it.
48600	52600	And the idea that these things will stay aligned with us because we have built them,
52600	55200	yet we gave them a capacity to rewrite their code.
55200	56600	There's just no reason to believe that.
56600	62200	And I worry about the near-term problem of what humans do with increasingly powerful AI,
62200	64600	how it amplifies misinformation.
64600	68000	Most of what's online could soon be fake.
68000	73400	Can we hold a presidential election 18 months from now that we recognize as valid?
73400	74600	Like, is it safe?
74600	76600	And it just gets scarier and scarier.
76600	80400	I worry we're just going to have to declare bankruptcy to the internet.
80400	82000	The internet, the internet.
82000	87200	If your intuition is correct, are you optimistic about our chances of survival?
112200	116200	Sam, six years ago, you did a TED Talk.
116200	119200	I watched that TED Talk a few times over the last week.
119200	124200	And the TED Talk was called, Can We Build AI Without Losing Control Over It?
124200	128200	In that TED Talk, you really discussed the idea of whether AI,
128200	132200	when it gets to a certain point of sentience in the internet,
132200	135200	can build AI without losing control over it.
135200	139200	In that TED Talk, you really discussed the idea of whether AI,
139200	143200	when it gets to a certain point of sentience and intelligence,
143200	148200	will wreak havoc on humanity.
148200	152200	Six years later, where do you stand on it today?
152200	160200	Do you think, are you optimistic about our chances of survival?
160200	162200	Yeah, I can't say I'm optimistic.
162200	171200	I'm worried about two species of problem here that are related.
171200	178200	There's the near-term problem of just what humans do with increasingly powerful AI
178200	185200	and how it amplifies the problem of misinformation and disinformation
185200	189200	and just makes it harder and harder to make sense of reality together.
193200	198200	And then there's just the longer-term concern about what's called alignment
198200	204200	with artificial general intelligence, where we build AI that is truly general
204200	210200	and by definition superhuman in its competence and power.
210200	215200	And then the question is, have we built it in such a way that is aligned
215200	218200	in a durable way with our interests?
218200	224200	And there's some people who just don't see this problem.
224200	227200	They're kind of blind to it.
227200	231200	When I'm in the presence of someone who doesn't share this intuition,
231200	236200	they don't resonate to it, I just don't understand what they're doing
236200	239200	or not doing with their minds in that moment.
239200	241200	Let's say I'm wrong about that.
241200	243200	Well, then it's just the other person's right.
243200	248200	We just have fundamentally different intuitions about this particular point.
248200	254200	And the point is this, if you're imagining building true artificial general intelligence
254200	258200	that is superhuman, and that is what everyone, whatever their intuitions,
258200	260200	purports to be imagining here.
260200	263200	There are people on both sides of the alignment debate.
263200	268200	There are people who think alignment is a real problem and people think it's a total fiction.
268200	272200	But everyone, firstly everyone who's party to this conversation agrees
272200	280200	that we will ultimately build artificial general intelligence that will be superhuman in its capacities.
280200	285200	And there's very little you have to assume to be confident that we're going to do that.
285200	287200	There's really just two assumptions.
287200	291200	One is that intelligence is substrate independent.
291200	293200	It doesn't have to be made of meat.
293200	295200	It can be made in silico.
295200	298200	And we've already proven that with narrow AI.
298200	301200	We obviously have intelligent machines.
301200	305200	And your calculator in your phone is better than you are at arithmetic.
305200	309200	And that's some very narrow band of intelligence.
309200	314200	So as we keep building intelligent machines on the assumption
314200	318200	that there's nothing magical about having a computer made of meat,
318200	322200	the only other thing you have to assume is that we will keep doing this.
322200	324200	We will keep making progress.
324200	331200	And eventually we will be in the presence of something more intelligent than we are.
331200	333200	And that's not assuming Moore's law.
333200	335200	It's not assuming exponential progress.
335200	337200	We just have to keep going.
337200	342200	And when you look at the reasons why we wouldn't keep going, those are all just terrifying.
342200	347200	Because intelligence is so valuable and we're so incentivized to have more of it.
347200	349200	And every increment of it is valuable.
349200	354200	It's not like it only gets valuable when you double it or 10X it.
354200	360200	No, no, if you just get three more percent, that pays for itself.
360200	364200	So we're going to keep doing this.
364200	369200	Our failure to do it suggests that something terrible has happened in the meantime.
369200	370200	We've had a world war.
370200	373200	We've had a global pandemic far worse than COVID.
373200	375200	We got hit by an asteroid.
375200	381200	Something happened that prevented us as a species from continuing to make progress
381200	383200	in building intelligent machines.
383200	386200	So absent that, we're going to keep going.
386200	391200	We will eventually be in the presence of something smarter than we are.
391200	394200	And this is where intuitions divide.
394200	402200	My intuition, and it's shared by many people, I know at least one who you've spoken to,
402200	413200	my intuition is that there is something inherently dangerous for the dumber party in that relationship.
413200	420200	There's something inherently dangerous for the dumber species to be in the presence of the smarter species.
420200	427200	And we have seen this based on our entanglement with all other species, dumber than we are.
427200	432200	Or certainly less competent than we are.
432200	442200	And so reasoning by analogy, it would be true of something smarter than we are.
442200	448200	People imagine that because we have built these machines, that is no longer true.
448200	453200	But here's where my intuition goes from there.
453200	461200	That imagination is born of not taking intelligence seriously.
461200	469200	Because what intelligence is, is a mismatch in intelligence in particular,
469200	480200	is a fundamental lack of insight into what the smarter party is doing and why it's doing it
480200	484200	and what it will do next on the part of the dumber party.
484200	496200	You just could imagine that by analogy, just imagine that the dogs had invented us as their super intelligent AIs.
496200	501200	For the purpose of making their lives better, just securing resources for them,
501200	507200	securing comfort for them, getting them medical attention.
508200	512200	It's been working out pretty well for the dogs for about 10,000 years.
512200	515200	There's some exceptions. We mistreat certain dogs.
515200	522200	But generally speaking, for most dogs, most of the time, humans have been a great invention.
522200	536200	Now, it's true that the mismatch in our intelligence dictates a fundamental blindness with respect to what we've become in the meantime.
536200	542200	We have all these instrumental goals and things we care about that they cannot possibly conceive.
542200	548200	They know that when we go get the leash and say, it's time for a walk, they understand that particular part of the language game.
548200	554200	But everything else we do when we're talking to each other, when we're on our computers or on our phones,
554200	558200	they don't have the dimmest idea of what we're up to.
559200	566200	The truth is, we love our dogs. We make irrational sacrifices for our dogs.
566200	572200	We prioritize their health over all kinds of things that it's just amazing to consider.
572200	583200	And yet, if there was a new global pandemic kicking off and some Xenovirus was jumping from dogs to humans,
583200	588200	and it was just super Ebola, it was 90% lethal.
588200	596200	And this was just a forced choice between what do you value more, the lives of your dogs or the lives of your kids?
596200	601200	If that's a situation we were in, it's totally conceivable.
601200	608200	By no means impossible. We would just kill all the dogs, and they would never know why.
608200	618200	And it's because we have this layer of mind and culture and just the new sphere.
618200	629200	There's this realm of mind that requires a requisite level of intelligence to even be partied to, to even know exists,
629200	633200	that they have no idea it exists.
633200	642200	And this is a fanciful analogy because the dogs did not invent us, but evolution invented us.
642200	649200	Evolution has coded us, as I said, to survive and spawn, and that's it.
649200	657200	So evolution can't see everything else we've done with our time and attention and all the values we've formed in the meantime.
657200	663200	And all the ways in which we have explicitly disavowed the program we've been given.
663200	671200	So evolution gave us a program, but if we were really going to live by the lights of that program, what would we be doing?
671200	674200	We would be having as many kids as possible.
674200	682200	Guys would be going to sperm banks and donating their sperm and finding that the best use of their time and attention.
682200	688200	Like the idea that you could have hundreds of kids for which you have no financial responsibility.
688200	696200	That would be the, that should be the most rewarding thing that you could possibly do with your time as a man.
696200	700200	And yet that's obviously not what we do.
700200	702200	And there are people who decide not to have kids.
703200	712200	And yet, and everything else we do from having podcast conversations like this to curing diseases,
712200	725200	literally everything we're doing with science, with culture is, yes, there are points of contact between those products and our evolved capacities.
726200	727200	It's not magic.
727200	734200	We are social primates that have leveraged certain ancient hardware to do new things.
734200	739200	But the code that we've been given doesn't see any of that.
739200	744200	And we've not been optimized to build democracies.
744200	746200	Evolution knows nothing.
746200	747200	It can know nothing.
747200	755200	If evolution were a coder, there's just no, there's no democracy maximization in that code, right?
755200	758200	It's just, it's not, it's just not there.
758200	764200	So the idea that these things will stay aligned with us because we have built them,
764200	767200	because if we have this origin story that we gave them their initial code,
767200	775200	and yet we gave them a capacity to rewrite their code and build future generations of themselves, right?
775200	777200	There's just no reason to believe that.
777200	783200	I see no, and the mismatch in intelligence is intrinsically dangerous.
783200	788200	And you could see this by, I mean, Stuart Russell, I don't know if you had him on the podcast.
788200	792200	He's a great professor of computer science at Berkeley.
792200	799200	And he wrote, literally co-wrote one of the most popular textbooks on AI.
799200	804200	I mean, he has some arresting analogies, which I think are good intuition pumps here.
804200	813200	And one is, just think of how you would feel if you knew, like, let's say we got a communication from elsewhere in the galaxy.
813200	820200	And it was a message that we decoded and it said, people of Earth, we will arrive on your lowly planet in 50 years.
820200	824200	Get ready, right?
824200	833200	That, anyone who thinks that we're going to get super intelligent AI in, let's say, 50 years,
833200	840200	thinks we're essentially in that situation and yet we're not responding emotionally to it in the same way.
840200	850200	If we received a communication from a species that we knew just by the sheer fact that they were communicating with us in this way,
850200	854200	we knew they're more competent and more powerful and more intelligent than we are, right?
854200	856200	And they're going to arrive, right?
856200	867200	We would feel that we were on the threshold of the most momentous change in the history of our species.
867200	878200	And we would feel, but most importantly, we would feel that it's because this is a relationship, an unavoidable relationship that's being foisted upon us, right?
879200	888200	A new creature is coming into the room with its own capacities and now you're in relationship.
888200	893200	And one thing is absolutely certain, it is smarter than you are, right?
893200	906200	By what factor, I mean, ultimately we're talking about, by factors, just by so many orders of magnitude, our intuitions completely fail.
906200	917200	I mean, even if it was just a difference in the time of processing, let's say there was no difference in the actual native intelligence,
917200	929200	but it's just processing speed, a million-fold difference in processing speed is just a phantasmagorical difference in capacity.
930200	939200	Just imagine we had 10 smart guys in a room over there and they were working and thinking and talking a million times faster than we are, right?
939200	942200	Well, so they're no smarter than we are, but they're just faster.
942200	951200	And we talk to them once every two weeks just to catch up on what they're up to and what they want to do and whether they still want to collaborate with us.
951200	957200	Well, two weeks for us is 20,000 years of analogous progress for them.
957200	973200	So how could we possibly hope to constrain the opinions and collaborate with and negotiate with people no smarter than ourselves who are making 20,000 years of progress every time we make two weeks of progress, right?
973200	976200	It's just, it's unimaginable.
976200	980200	And yet there are many people who don't just think this is just fiction.
980200	990200	Everything I, all the noises I've made in the last five minutes are just like a new religion of fear, right?
990200	996200	And it's just there's no reason to think that alignment is even a potential problem.
996200	1010200	If your intuition is correct and that analogy of us getting a signal from outer space that someone is coming in 30 years, which by the way, a lot of people that speak on this subject matter, don't believe it's even going to be 30 years until we reach that sort of singularity moment.
1010200	1012200	I think they speak of artificial general intelligence.
1012200	1020200	I've heard people like Elon say, you know, many fewer decades, 10, 10 years, 15 years, 20 years, etc.
1020200	1028200	If that is correct, then surely this is the most pressing challenge, conversation issue of our time.
1028200	1036200	And there's no logical reason that I can see to refute your intuition there.
1036200	1038200	I can't see a logical reason.
1038200	1040200	The rate of progress will continue.
1040200	1046200	Don't necessarily see anything that will wipe out or pause our rate of progress.
1046200	1050200	Let me just be charitable to the other side here.
1050200	1058200	There are other assumptions that they smuggle in that some do it without being aware of it, but some actually believe these assumptions.
1058200	1064200	And this spells the difference on this particular intuition.
1064200	1073200	So it's possible to assume that the more intelligent you get, the more ethical you become by definition right now.
1073200	1083200	And we might draw a somewhat more equivocal picture from just the human case where we see that, oh, there's some very smart people who aren't that ethical.
1083200	1099200	But I believe there are people, and I've talked to at least a few people who believe this, there are people who assume they're kind of in the limit as you push out into just far beyond human levels of intelligence.
1099200	1111200	There's every reason to believe that all of the provincial, creaturely failures of human ethics will be left behind as well.
1111200	1116200	It's like you're not like the selfishness and the basis for conflict.
1117200	1128200	The apish urges of status-seeking monkeys is just not going to be in the code.
1128200	1142200	And as you push out into the omnibus genius of the coming AI, there's a kind of a sainthood that's going to come along with it and a wisdom that will come along with it.
1142200	1146200	Now, I just think that's quite a gamble.
1146200	1150200	I would take the other side of that bet and I would frame it this way.
1150200	1159200	There have to be ways in the space of all possible intelligences that are beyond the human, right?
1159200	1161200	There's got to be more than one possible.
1161200	1168200	It's just like there's many different ways to have a chess engine that's better than I am at chess.
1168200	1173200	They're different from each other, but they're all better than me, right?
1173200	1180200	There's got to be more than one way to have a superhuman artificial intelligence.
1180200	1190200	And I would imagine there are not an infinite number of ways, but just a vast number of...
1190200	1200200	In the space of all possible minds, there are many locations in that space beyond the human that are not aligned with human well-being.
1200200	1205200	There's got to be more ways to build this unaligned than aligned, right?
1205200	1212200	And what other people are smuggling into this conversation is the intuition that, no, no, once you get beyond the human,
1213200	1220200	you're going to be in the presence of just the Buddha who understands quantum mechanics and oncology and everything else, right?
1220200	1223200	I just see no reason to think that that's so.
1223200	1229200	And we could build something that is, again, taken intelligence seriously.
1229200	1232200	We're going to build something that we're in relationship to.
1232200	1235200	It's really intelligent in all the ways that we're intelligent.
1235200	1238200	It's just better at all of those things than we are.
1238200	1242200	It's, by definition, superhuman because the only way it wouldn't be superhuman,
1242200	1248200	the only way it would be human level, even for 15 minutes, is if we didn't let it improve itself,
1248200	1252200	if we wanted to just keep it stuck at a...
1252200	1256200	We built a college undergraduate and we wanted just to keep it stuck there,
1256200	1261200	but we would have to dumb down all of the specific capacities we've already built, right?
1261200	1266200	Just like every AI we have, narrow AI, is superhuman for the thing it does.
1267200	1271200	It has access to all the information on the Internet, right?
1271200	1273200	It's got perfect memories.
1273200	1275200	It can perfectly copy itself.
1275200	1281200	When one part of the system learns something, the rest of the system learns it because it just can swap files, right?
1281200	1286200	It's, again, your phone is a superhuman calculator.
1286200	1290200	There's no reason to make it a calculator that is human level.
1290200	1292200	And so we're never going to do that.
1292200	1295200	We're never going to be in the presence of human AGI.
1295200	1299200	We will be immediately in the presence of superhuman AGI.
1299200	1307200	And then the question is how quickly it improves and how much headroom is there to improve into.
1307200	1312200	On the assumption that you can get quite a bit more intelligent than we are, right,
1312200	1317200	that we're nowhere near the summit of possible intelligence,
1317200	1323200	you have to imagine that you're going to be in the presence of something that is, again,
1323200	1325200	it could be completely unconscious, right?
1325200	1330200	I'm not saying that there's something that's like to be this thing, although there might be.
1330200	1334200	And that's a totally different problem that's worth worrying about.
1334200	1341200	But whether conscious or not, it is solving problems, detecting problems,
1341200	1348200	improving its capacity to do all of that in ways that we can't possibly understand.
1348200	1355200	And the products of its increasing competence are always being surfaced, right?
1355200	1360200	So it's like we've been using it to change the world.
1360200	1362200	We've become reliant upon it.
1362200	1364200	We built this thing for a reason.
1364200	1369200	I mean, one thing that's been amazing about developments in recent months is that
1369200	1374200	those of us who have been at all cognizant of the AI safety space
1375200	1378200	now going on a decade or more for some people,
1378200	1384200	always assumed that as we got closer to the end zone,
1384200	1390200	that the labs would become more circumspect, we'd be building this stuff air-gapped from the internet.
1390200	1393200	It's like we have this phrase air-gapped from the internet.
1393200	1394200	We thought this was a thing.
1394200	1396200	This thing would be in a box.
1396200	1401200	And then the question would be, well, do we let it out of the box and let it do something?
1401200	1402200	Like, is it safe?
1402200	1404200	And how do we know if it's safe?
1404200	1406200	And we thought we would have that moment.
1406200	1410200	We thought it would happen in a lab at Google or at Facebook or somewhere.
1410200	1413200	We thought we would hear, OK, we've got something really impressive,
1413200	1416200	and now we just want it to touch the stock market,
1416200	1419200	or we want it to touch our medical data,
1419200	1422200	or we just want to see if we can use it.
1422200	1424200	We're way past that.
1424200	1426200	We've built this stuff already in the wild.
1426200	1429200	It's already connected to the internet.
1429200	1432200	It's already got millions of people using it.
1432200	1434200	It already has APIs.
1434200	1437200	It's already doing work.
1437200	1439200	From an AI safety point of view, that's amazing.
1439200	1445200	We didn't even have the choice point we thought was going to be so fraught.
1445200	1450200	Of course we didn't, because there was such pressing incentives
1450200	1453200	for people to press forward regardless of that conversation.
1453200	1461200	But everyone thought, I mean, I don't believe I was ever in conversation with someone,
1461200	1467200	someone like L.A.'s or Yudikowski or Nick Bostrom or Stuart Russell,
1467200	1472200	who assumed we would be in this spot.
1472200	1478200	I'd have to go back and look at those conversations,
1478200	1481200	but there was so much time spent.
1481200	1489200	It seems quite unnecessarily on this idea that we'd make a certain amount of progress,
1489200	1492200	and circumspection would kick in.
1492200	1497200	Even the people who were doubters would become worried.
1497200	1502200	In the final yards, as we go across into the end zone,
1502200	1506200	there'd be some mode where we could slow down and figure it out
1506200	1509200	and try to deal with the arms race dynamics.
1509200	1514200	We could place a phone call to China and just talk about this,
1514200	1519200	we got something interesting, but the stuff is already being built in connection to everything.
1519200	1528200	There's already just endless businesses being devised on the back of this thing,
1528200	1532200	and all the improvements are going to get plowed into it.
1532200	1535200	Just imagine what this looks like even in success.
1535200	1538200	I'll say it just starts working wonders for us,
1538200	1545200	and we get these great productivity gains.
1545200	1550200	Then we cross into whatever the singularity is,
1550200	1554200	at whatever speed we find ourselves in the presence of something that is truly general.
1554200	1560200	After all of this narrow stuff, albeit superhuman narrow stuff,
1560200	1565200	is something that we totally depend on.
1565200	1569200	Every hospital requires it, and every airplane requires it,
1569200	1572200	and all of our missile systems require it.
1572200	1577200	This is the way we do business.
1577200	1582200	There's nothing to turn off at that point.
1582200	1585200	I put this to Mark Andreessen on my podcast, and he said,
1585200	1589200	if you can turn off the Internet, I can't believe he was quite serious.
1589200	1593200	If you're North Korea, I guess you can turn off the Internet for North Korea,
1593200	1596200	and that's why North Korea is like North Korea.
1596200	1611200	The cost of turning off the Internet now would be unimaginable.
1611200	1617200	The atomic cost alone, it just would be...
1617200	1628200	The idea that we've lost the moment to decide whether to hook our most powerful AI to everything,
1628200	1633200	because it's already being built more or less in contact with, if not everything,
1633200	1638200	so many things that you just can't put the genie back in the bottle,
1638200	1643200	that is genuinely surprising to me, and yeah, I mean, incentives.
1643200	1647200	Is this not the most pressing problem, though?
1647200	1652200	I was going to ask this conversation by asking you the question about the thing that occupies your mind the most,
1652200	1654200	and the most important thing we should be talking about,
1654200	1658200	and I in part assumed the answer would be artificial intelligence,
1658200	1661200	because the way that you talk about your intuition on this subject matter,
1661200	1666200	you've got children, you think about the future a lot.
1666200	1672200	If you can see this species coming to Earth, even if it's in the next 100 years,
1672200	1676200	it strikes me to be the most pressing problem for humanity.
1676200	1685200	Well, as interesting as I think that problem is, and consequential as it is,
1685200	1691200	I'm worried that life could become unlivable in the near term before we even get there.
1691200	1695200	I'm just worried about the misuses of narrow AI in the meantime.
1695200	1699200	I'm worried about, just take the current level of AI we have.
1699200	1705200	We have GPT-4.
1705200	1712200	I think within the next 12 months or two years, let's say whatever GPT-5 is,
1712200	1718200	we're going to be in the presence of something where most of what's online
1718200	1722200	that purports to be information could soon be fake.
1723200	1729200	Most of the text you find on any topic is just fake.
1729200	1737200	Someone has just decided, write me a thousand journal articles on why mRNA vaccines cause cancer,
1737200	1741200	and give me 150 citations, write them in the style of nature,
1741200	1746200	and nature genetics, and Lancet, and JAMA, and just put them out there.
1746200	1751200	One teenager could do that in five minutes with the right AI.
1752200	1758200	GPT-4 is not quite that, but GPT-5 possibly will be that.
1758200	1761200	That is such a near-term advance.
1761200	1768200	When you imagine knitting together the visual stuff like mid-journey, and dolly,
1768200	1774200	and stable diffusion with a large language model, just imagine the tool.
1775200	1780200	Maybe this is 18 months away, maybe it's three years away, but it's not 30 years away.
1780200	1787200	The tool where you can just say, give me a 45-minute documentary on how the Holocaust never happened,
1787200	1794200	filled with archival imagery, give me Hitler speaking in German with the appropriate translations,
1794200	1801200	and give it in the style of Alex Gibney or Ken Burns.
1802200	1807200	Give me 10,000 of those.
1807200	1813200	All the friction for misinformation has been taken out of the system.
1813200	1819200	I worry we're just going to have to declare bankruptcy with respect to the internet.
1819200	1824200	We just are not going to be able to figure out what's real.
1824200	1834200	When you look at how hard that is now with social media in the aftermath of COVID and Trump,
1834200	1841200	just the challenge of holding an election that most of the population agrees was valid.
1841200	1851200	That challenge already is on the verge of being insurmountable in the US.
1851200	1856200	It's easy to see us failing at that, AI aside.
1856200	1862200	When you add large language models to that and the more competent future version of it,
1862200	1871200	where it's just the most compelling deep fakes are indistinguishable from real data.
1871200	1878200	Everyone is siloed into their tribes where they're stigmatizing the information that comes from any other tribe.
1878200	1884200	The internet is now so big a place that there really isn't the ordinary selection pressures
1884200	1888200	where bad information gets successfully debunked so that it goes away.
1888200	1894200	You can live in a conspiracy cult for the rest of your life if you want to.
1894200	1898200	You can be queuing on all day long if you want to.
1898200	1908200	Now we've got deep fakes shoring all that up and just spurious scientific articles shoring all that up.
1908200	1914200	All of this just becomes a more compelling form of psychosis and culturally speaking.
1914200	1923200	I'm just worried that it's going to get harder and harder for us to cooperate with one another and collaborate
1923200	1933200	and that our politics will just completely break and that'll offer an opportunity for lots of bad actors.
1933200	1946200	Leaving aside, there's cyber terrorism and there's synthetic biology that the moment you turn AI loose on the prospect of engineering viruses
1947200	1960200	The asymmetry here is that it seems like it's always easier to break things than to fix them or to categorically prevent people from breaking them.
1960200	1971200	What we have with increasingly powerful technology is the ability for one person to create more and more damage or one small group of people.
1972200	1979200	It just turns out it's hard enough to build a nuclear bomb that one person can't really do it no matter how smart.
1979200	1990200	You need a team and traditionally you've needed state actors and you need access to resources and you have to get the fissile material and it's hard enough.
1991200	2001200	This is being fully democratized this tech and so I worry about the near term chaos.
2001200	2008200	I've never found the narrow term consequences of artificial intelligence to be that interesting until now.
2008200	2012200	That image of the internet becoming unusable.
2012200	2016200	So that was a real eureka moment for me because I've not been thinking about that.
2016200	2045200	Yeah, me too. I was just concerned about the AGI risk and now really in the aftermath of Trump and COVID, I see the risk of, if not losing everything, losing a lot that matters just based on our interacting with these very simple tools
2045200	2048200	that are reliably misleading us.
2048200	2054200	I'm amazed at what social media, I'm amazed at what Twitter did to me.
2054200	2071200	Even with all of my training and with my head screwed on reasonably straight, it's amazing to say it, but almost all of the truly bad things that have happened to me in the last decade
2071200	2080200	that just destabilized relationships and priorities and really kind of got plowed back into me.
2080200	2087200	It became a kind of professional emergency, stuff I had to respond to in writing or on podcasts.
2087200	2089200	It was all Twitter.
2089200	2097200	My engagement with Twitter was the thing that produced the chaos and it was completely unnecessary.
2097200	2107200	It was amplifying a kind of signal for me that I felt compelled to pay attention to because I was on it and I was trying to communicate with people on it.
2107200	2117200	I was getting certain communication back and it was giving me a picture of the rest of humanity, which I now think was fundamentally misleading, but it was still consequential.
2118200	2129200	A certain point believing that it was misleading wasn't enough to inoculate me against the delusion of the opinion change that was being forced upon me.
2129200	2134200	I was feeling like these people are becoming unrecognizable.
2134200	2135200	I know some of these people.
2135200	2145200	I've had dinner with some of these people and their behavior on Twitter is appearing so deranged to me in such bad faith.
2146200	2154200	People who I know to be non-psychopaths are starting to behave like psychopaths, at least on Twitter.
2154200	2159200	I'm becoming similarly unrecognizable to them.
2159200	2171200	It all felt like a psychological experiment to which I hadn't consented and which I enrolled myself somehow because it was what everyone was doing in 2009.
2171200	2177200	I spent 12 years there getting some signal and responding to it.
2177200	2180200	It's not to say that it was all bad.
2180200	2194200	I read a bunch of good articles that got linked there and I discovered some interesting people, but the change in my life after I deleted my Twitter account was so enormous.
2194200	2197200	It's embarrassing to admit it.
2198200	2201200	It's like getting out of a bad relationship.
2201200	2218200	It was a fundamental freedom from this chaos monster that was always there ready to disrupt something based on its own dynamics.
2218200	2220200	When did you delete it?
2220200	2223200	I think it was December.
2224200	2226200	I'm not someone that really takes sides on things.
2226200	2228200	I like to try and remain in the middle.
2228200	2231200	So you must have a very different Twitter experience than I was having?
2231200	2233200	No.
2233200	2237200	So I don't tweet anything other than this podcast trailer.
2237200	2239200	I don't tweet anything else.
2239200	2241200	The only thing you'll see on my Twitter is the podcast trailer.
2241200	2243200	That's it.
2243200	2251200	For all the reasons you've described, and more interestingly, I wanted to say in the last eight months, as someone that doesn't get caught up too much in the media,
2251200	2255200	oh, Elon bought this, it's 100% gone in that direction.
2255200	2263200	As in my timeline now is, I say it to my friends all the time, and some of my friends who are again, I think are nuanced and balanced have said to me,
2263200	2271200	there's something that's been turned up in the algorithm to increase engagement that has planted me in an unpleasant echo chamber that I didn't desire to be in.
2271200	2276200	And if I wasn't somewhat conscious, I would 100% be in there.
2276200	2284200	My timeline, my friend tweeted the other day, my friend Cahill tweeted, he's never seen more people die on his Twitter timeline than he has in the last six months.
2284200	2289200	They're prioritizing video, so you're seeing a lot of like death and CCTV footage that I've never seen before.
2289200	2300200	And then the debate around gender, politics, right leaning subject matter has never been more right down your throat.
2300200	2307200	Because it's almost like something in the algorithm has been switched, where it's now, people have been let out of the asylum.
2307200	2311200	That's the only way I can describe it, and it's made me retract even more.
2311200	2320200	So when Zuckerberg announced threads the other couple of weeks ago, it was kind of like a life raft out of the Titanic.
2320200	2327200	And I really, really mean that, and I'm not someone to get easily caught up in narrative as it relates to social media platforms.
2327200	2329200	It's been my industry for a decade.
2329200	2335200	What I've seen on Twitter, and it's actually made me believe this hypothesis I had five years ago where I thought there would be,
2335200	2341200	I thought the route, the journey of social networking would have way more social networks and there'd be more siloed.
2341200	2346200	I thought we'd have one for our neighborhood, our football club, and now I believe that even more than ever.
2346200	2348200	Yeah, that seems right.
2348200	2355200	And I think it's, I mean, whether it's possible to have a truly healthy social network that people want to be in,
2355200	2360200	and it's a good reason to be there, and it's, I don't know if it's possible.
2360200	2372200	I'd like to think it is, but it's, I think there are certain things you have to clean up at the outset that is supposed to make it possible.
2372200	2374200	And I think, I think anonymity is a bad thing.
2374200	2378200	I think probably being free is a bad thing.
2378200	2381200	I think, you know, you sort of get what you pay for online.
2381200	2387200	And if it's, I just think there might be ways to set it up that would be better, but.
2387200	2389200	I don't think it'd be popular.
2389200	2390200	What was that?
2390200	2392200	I think with the thing that makes it popular makes it toxic.
2392200	2393200	Right.
2393200	2394200	Right.
2394200	2397200	And even the anonymity piece, I've played this out a couple of times in my mind.
2397200	2404200	The problem I always get is while there's people in Syria who have news to break important news to break and they'd be hung if they.
2404200	2408200	So we need a anonymous version of the social internet.
2408200	2409200	Right.
2409200	2410200	Yeah.
2410200	2416200	Well, I guess there could be some exception there, but I don't know.
2416200	2426200	It just doesn't, it actually doesn't interest me because I just feel such a different sense of.
2426200	2438200	My being in the world as a result of not paying attention to the, my online, the simulacrum of myself, it's a.
2438200	2440200	Because Twitter was the only one I use.
2440200	2442200	Like I was on, I've been on Facebook this whole time.
2442200	2448200	I've been on, I think, I guess I'm on Instagram too, but it's like my team just uses those as marketing channels.
2448200	2451200	You know, it's just like, it sounds like that's the way you use Twitter now.
2451200	2455200	But Twitter was the one that I decided, okay, this is going to be me.
2455200	2456200	I'm going to be posting here.
2456200	2460200	I'm going to, you know, if I've made a mistake, I want to hear about it.
2460200	2466200	You know, it's like, and I just wanted to use it as, as actual, actual basis for communication.
2466200	2473200	And for the longest time, it actually felt like a valid tool in that respect.
2473200	2475200	You know, it reached a crisis point.
2475200	2477200	I decided this is just pure toxicity.
2477200	2482200	There's just no reason, even the good stuff can't possibly make a dent in the bad stuff.
2482200	2484200	So I just deleted it.
2484200	2488200	And then I was, I was returned to the real world, right?
2488200	2500200	Where I've, where I actually live and to books and to, I mean, I'm online all the time anyway, but it's not having the, it's the time course of reactivity.
2500200	2511200	When you don't have social media, when you don't, and you don't have a place to put this, this instantaneous hot take that you're tempted to put out into the world,
2511200	2513200	because there's literally no place to put it.
2513200	2525200	Like for me, if I have some reaction to something in the news, I have to decide whether it's worth talking about it in my next podcast that I might be recording, you know, four days from now.
2525200	2533200	And rather often people have been just bloviating about this thing for four solid days before I ever get to the microphone.
2533200	2537200	And then I get to think, well, is it still worth talking about it?
2537200	2540200	And most, almost nothing survives that test anymore, right?
2540200	2543200	So I get the conversations moved on.
2543200	2560200	So there's actually no place for me to just type this thing that either takes me 10 seconds and then rolls out there to get, to detonate in the minds of, you know, my friends and enemies to opposite effect.
2560200	2571200	And then I see the result of all that, you know, on a, again, on a, this sort of reinforcement loop of every 15 minutes.
2571200	2575200	Not having that is such a relief that I just don't even know why I would.
2575200	2580200	So like when Threads was announced, I wasn't, I think I'm on Threads too, but it's not me.
2580200	2583200	It's just, you know, just get another marketing channel.
2584200	2591200	But yeah, I haven't, I feel such relief not exercising that muscle anymore.
2591200	2603200	Where it's like, you know, I don't know how often I was checking Twitter, but it was, I was, you know, I was not checking it just to see what was happening to me or the response to my last thing I tweeted.
2603200	2606200	I was checking it a lot because it was my newsfeed.
2606200	2609200	It's like I'm following, you know, 200 smart people.
2609200	2611200	They're telling me what they're paying attention to.
2611200	2612200	And so I'm fascinated.
2612200	2615200	So yeah, well, yeah, I want to see that next article or that next video.
2615200	2624200	Just that engagement and the endless opportunity to comment and to put my foot in my mouth or put my foot in someone else's mouth or have someone put their foot.
2624200	2638200	It's just not having that has been such relief that I would be, I mean, it's not impossible, but I would be very cautious in reactivating that because it was, it was so much noise.
2638200	2655200	And again, it created, there's so much, it became a, it became an opportunity cost, but it became a just this endless opportunity for misunderstanding.
2656200	2662200	Misunderstanding of me and, you know, everything I've been putting out into the world and then my sense that I had to react to it.
2662200	2673200	And then you just kind of plow that back into the, you know, that becomes the basis for further misunderstanding.
2673200	2684200	And it just constantly was giving me the sense that there's something, there's something I need to react to on my podcast, in an article, on Twitter, that it's just, this is a valid signal.
2684200	2687200	Like this is, this is, this is like, this is a five alarm fire.
2687200	2688200	This is like, you got to stop everything.
2688200	2693200	Like you're by the pool on the one vacation you're taking with your family that summer.
2693200	2699200	And this thing just happened on your phone that it can't wait, right?
2699200	2702200	Like you actually have to pay attention because it's like the conversation is happening right now.
2702200	2720200	And so it was a kind of addiction to information and, you know, at some level, reputation management or, or, or, and it was just, I mean, just to just be free of it is such a relief.
2720200	2734200	And apart from like, you know, health issues with certain family members, virtually the only bad things that have happened to me have been a result of my engagement with Twitter over the last 10 years.
2734200	2743200	So it's just, it's just, you know, I, you know, I guess I'm, if I'm a masochist, I would be back on Twitter, but like that would be the only reason to do it.
2743200	2744200	Narrow AI.
2744200	2749200	I asked you the question a second ago, which we, I really wanted to get a solution to it because I'm mildly terrified.
2750200	2760200	I completely believe your, believe your the logic underneath your opinion that Narrow AI will cause this destabilization and unusability of the internet.
2760200	2773200	So just focusing on Narrow AI, what would you consider to be a solution to prevent us getting to that world where misinformation is rife to the point that it can destabilize society, politics and culture?
2774200	2784200	Well, I think it's something I've been asking people about on my podcast because it's not actually my wheelhouse and I would just need to hear from experts about what's possible technically here.
2784200	2802200	But I'm imagining that paradoxically or ironically, this could usher in a new kind of gatekeeping that we're going to rely on because like the provenance of information is going to be so important.
2802200	2812200	I mean, the assurance that a video has not been manipulated or there's not a, just a pure confection of deep fakery.
2812200	2831200	Right, so you get, so it could be that we're meandering into a new period where you're not going to trust a photo unless it's coming from, you know, Getty images or, you know, the New York Times has some story about how they have verified every photo.
2831200	2834200	In there that they put in their newspaper, they have a process.
2834200	2850200	And, you know, so if you see a video of Vladimir Putin seeming to say that he's declaring war on the US, right, I think most people are going to assume that's fake until proven otherwise.
2850200	2854200	It's like, it's just, it's just going to be too much fake stuff.
2854200	2867200	And it's going to be, it's going to all going to look so good that the New York Times and every other, you know, organ of media that we have relied upon as imperfect as they've been of late.
2867200	2875200	They're going to have to figure out what the tools are whereby they can say, OK, this is actually a video of Putin, right.
2875200	2878200	And if the new, I mean, I'm not going to be able to figure it out on my own, right.
2878200	2886200	The New York Times doesn't have a process or CNN doesn't have a process that they go through before they say, OK, Putin really said this.
2886200	2892200	And so this is, we have to now react to this because this is real.
2893200	2900200	Whatever that process is and whether it's whether there's some kind of digital watermark that, you know, that's connected to the blockchain.
2900200	2914200	There's some tech implementation of it that can be fully democratized where you by just being in the latest version of the Chrome browser can know that you can differentiate real and fake videos.
2914200	2920200	I don't know what the implementation will be, but I just know we're going to get to some spot where it's going to be.
2920200	2925200	Right. We have to declare epistemological bankruptcy.
2925200	2927200	We don't know what's real.
2927200	2934200	We have to assume anything, especially lured or agitating is fake until proven otherwise.
2934200	2936200	So prove otherwise.
2936200	2939200	And that's, you know, that that'll be a resetting of something.
2939200	2949200	I don't know what we do with that in a world where we really don't have that much time to react to certain things that are a video of Putin saying he's launched his big missiles.
2951200	2956200	Is something that, you know, 30 minutes from now we would we would understand whether it's real or not.
2956200	2962200	We forget about again, forget about everything we just said about AI.
2962200	2964200	Look at all of our legacy risks.
2964200	2974200	Look at the risk of nuclear war, the risk of stumbling into a nuclear war by accident has been hanging over our head for 70 years.
2974200	2976200	I mean, we've got this old tech.
2976200	2980200	We've got these wonky radar systems that throw up errors.
2980200	2997200	We have moments in history where, you know, one Soviet sub commander decided based on his just gut feeling his common sense that the data was almost certainly an error.
2997200	3009200	And he decided not to pass the obvious evidence of an American ICBM launch up the chain of command, knowing that the chain of command would say, OK, you have to fire.
3009200	3010200	Right.
3010200	3019200	And he reasoned that if the U.S. was going to attack the Soviet Union, they would launch more than I think in this case it looked like there were four missiles.
3019200	3021200	That was the radar signature.
3021200	3032200	If the U.S. is going to launch a first strike against the Soviet Union in this like the mid 80s, they're going to launch more than four missiles.
3032200	3034200	This has to be this has to be bad data.
3034200	3035200	Right.
3035200	3041200	But, you know, so if we automate all this, will we automate it to systems that have that kind of common sense?
3041200	3042200	Right.
3043200	3051200	But we've been perched on the on the edge of the abyss based on this.
3051200	3056200	The possible forget about malevolent actors, you know, who might decide to have a nuclear war on purpose.
3056200	3060200	We have the possibility of accidental nuclear war.
3060200	3066200	You add this cacophony of misinformation and deep fake to all of that.
3066200	3069200	And it just gets scarier and scarier.
3069200	3071200	And this is not even AI.
3071200	3076200	This is just, you know, narrow AI amplified misinformation.
3076200	3078200	How do you think about it?
3078200	3080200	Well, this is the thing that worries me.
3080200	3082200	I worry about the next election.
3082200	3095200	I think the next president, if we can run the 2024 election in a way that most of America acknowledges was valid, that will be an amazing victory.
3095200	3101200	You know, whatever the outcome, I mean, obviously, I would not be looking forward to a Trump presidency.
3101200	3114200	But I think even more fundamental than that is, can we hold a presidential election 18 months from now that is that we recognize as valid?
3114200	3115200	Right.
3115200	3116200	Like that.
3116200	3117200	I don't know.
3117200	3122200	I don't know what kind of resources are being spent on on that particular performance.
3122200	3125200	But that is hugely important.
3125200	3133200	And I don't think our near term experiments with AI is going to make that easier.
3133200	3135200	Why is it so important?
3135200	3147200	Well, it's just, I mean, if you think the maintenance of a valid democracy in the world's lone superpower is of minor importance.
3147200	3151200	I'd like to drink the tea you're drinking.
3151200	3152200	Is that optimistic?
3152200	3155200	I mean, I can't say I'm optimistic.
3155200	3160200	You know, it's a paradoxical state, I mean, because I definitely have.
3160200	3164200	I tend to focus on what's wrong or might be wrong.
3164200	3170200	I tend to, I think, have a pessimistic bias, right?
3170200	3173200	Like I tend to notice what's wrong as opposed to what's right.
3173200	3178200	You know, that's my, that's my bias.
3178200	3182200	But I'm actually very happy, right?
3182200	3184200	Like I have a very, a very good life.
3184200	3188200	I'm just like everything is just I'm incredibly lucky.
3188200	3189200	I'm surrounded by great people.
3189200	3192200	It's like it's all great.
3192200	3196200	And yet I see all of these risks on the horizon.
3196200	3205200	So I'm not, I just have a very high degree of well-being at this moment in my life.
3205200	3210200	And yet what's on the television is scary.
3210200	3214200	And so it's a very interesting juxtaposition.
3214200	3221200	You know, I'll be, I'll be very relieved if we have a, I feel like we're in a very weird spot.
3221200	3229200	I mean, like the, I haven't seen a full post-mortem on the COVID pandemic that has fully encapsulated
3229200	3233200	what I think we, what I think happened to us there.
3233200	3239200	But my vague sense is that we didn't learn a whole hell of a lot.
3239200	3244200	I mean, basically what we learned is we're really bad at responding to this kind of thing.
3244200	3248200	This was a challenge that, that just fragmented us as a society.
3248200	3250200	It could have brought us together.
3250200	3252200	It didn't.
3252200	3264200	And it, it amplified all of the divisions in our society politically and economically and tribally and all kinds of ways.
3264200	3269200	The role of misinformation and disinformation and all of that was, was all too clear.
3269200	3271200	And I think just getting worse.
3271200	3280200	So I think, you know, as a dress rehearsal for some future pandemic that's, that is inevitably going to come and is, you know, could well be worse.
3280200	3282200	I think we failed this dress rehearsal.
3282200	3296200	And, you know, I have to hope that at some point our institutions will reconstitute themselves so as to be obviously trustworthy and engender the kind of trust we actually need to have on our institutions.
3296200	3304200	Like, we need a CDC, then not only that we trust, but that is trustworthy, that we, that we, that we're right to trust, right?
3305200	3310200	And so it is with an FDA and every other, you know, institution that, that is relevant here.
3310200	3314200	And we don't quite have that.
3314200	3317200	And half of our society thinks we don't have that at all.
3317200	3318200	Right.
3318200	3323200	And so it's, we have to rebuild trust and institutions somehow.
3323200	3333200	And I just think, you know, we have a lot of work to do to even figure out how to make an increment of progress on that score.
3333200	3346200	Because we're, again, the siloing of large constituents into alternate information universes is just not functional.
3346200	3350200	And that's so much of what social media has done to us and alternative media.
3350200	3355200	I mean, like, you know, I call it, you know, you and I are podcasters, but I call it podcast to stand, right?
3355200	3371200	I mean, we have this, this landscape of, I mean, there's now whatever million plus podcasts and there's, you know, email newsletters and everyone has now just decided to curate their information diet in a way that's just bespoke to them.
3371200	3384200	And you can stay there forever and you're getting, you're getting one slice of, and it could be, you know, a completely fictional slice of, of reality.
3384200	3391200	And we're losing the ability to converge on a common picture of what's going on.
3391200	3393200	And you.
3393200	3395200	So that's not optimistic.
3395200	3397200	I didn't hear the optimism in there.
3397200	3398200	You tell me.
3398200	3402200	No, I, but I kind of can't refute anything you said on a like a logical basis.
3402200	3406200	It all sounds like that is the direction of travel that we're going in.
3406200	3412200	Unfortunately, I have faith that there'll be surprising positives.
3412200	3416200	There always tends to be surprising positives that we also didn't factor in.
3416200	3419200	Well, yeah, I mean, it's easy to see.
3419200	3432200	I mean, if there's anything, if there's any significant low hanging fruit technologically or or scientifically that could be AI enabled for us.
3432200	3437200	I mean, just take like, you know, a cure for cancer, a cure for Alzheimer's, right?
3437200	3445200	I mean, just having one thing like that, right, that would be such an enormous good.
3445200	3449200	And that is, that is, that's what, that's why we can't get off this ride.
3449200	3451200	And that's why there is no break to pull.
3451200	3455200	Because the value of intelligence is so enormous.
3455200	3458200	I mean, it is, it is just, it's not everything.
3458200	3463200	I mean, it's not, you know, there's other things we care about and a right to care about beyond intelligence.
3463200	3466200	I mean, love is not the same thing as intelligence, right?
3466200	3471200	But intelligence is the thing that can safeguard everything you love.
3471200	3482200	I mean, even if you think the whole point in life is to just get on a beach with your friends and your family and just hang out and enjoy the sunset.
3482200	3486200	Okay, you don't have to augment.
3486200	3489200	You don't need superhuman intelligence to do any of that, right?
3489200	3492200	You're fit to do it exactly as you are.
3492200	3497200	You could have done that in the 70s and it would just be just as good a beach and it'd be just as good friends.
3497200	3506200	But every gain we make in intelligence is the thing that safeguards that opportunity for you and everyone else.
3506200	3510200	How would you, I feel like we've not defined the term artificial general intelligence.
3510200	3515200	From my understanding of it, it's when the intelligence can think and make decisions almost like a human.
3515200	3520200	Yeah, I mean, loosely, I mean, this, this is kind of just a semantic problem.
3520200	3540200	But intelligence can mean many things, but, you know, loosely speaking, it is the ability to solve problems and meet goals, make decisions in response to a changing environment, in response to data.
3541200	3564200	And the general aspect of that is an ability to do that in many different situations, all the sort of situations we encounter as people, and to have one's capacity in one area not, you know, as I get better at deciding whether or not this is a cup,
3564200	3570200	I don't magically get worse at deciding whether, you know, you just said a word, right?
3570200	3575200	It's like, I can do multiple things in multiple channels.
3575200	3582200	That's not something we had in our artificial systems for the longest time because everything was bespoke to the task.
3582200	3585200	We'd build a chess engine and it couldn't even play tic-tac-toe.
3585200	3593200	All it could do is play chess and we just would get better and better in these piecemeal, narrow ways.
3593200	3607200	And then things began to change a few years ago where you'd get, you know, like deep mind would have its algorithms that were, you know, the same algorithm with slightly different tuning could play go, right?
3607200	3613200	Or it could, you know, it could solve a protein folding problem as opposed to just playing chess, right?
3613200	3616200	And it became the best in the world at chess and it became the best in the world at go.
3617200	3636200	And amazingly, I mean, to take, you know, what AlphaZero did, it, you know, before AlphaZero, all the chess algorithms were, they just had all of our chess knowledge plowed into them.
3636200	3642200	They had studied every human game of chess and they just, it was just, you know, it was a bespoke chess engine.
3642200	3648200	AlphaZero just played itself, I think for like four hours, right?
3648200	3658200	It just, it just had the rules of chess and then it played itself and it became better not merely than every other, every person who's ever played the game.
3658200	3664200	It became better than all the chess engines that had all of the, the, all of our chess knowledge plowed into them.
3664200	3674200	So it's a fundamentally new moment in, in how you build an intelligent system and it promises this, this possibility.
3674200	3680200	Again, this inevitability, the moment you admit that we will eventually get there.
3680200	3691200	The moment, the moment you admit that it's, it can be done in silico and the moment that you admit that we will just keep going unless a catastrophe happens.
3692200	3698200	And those two things are so easy to admit that I don't, at this point, I don't see any place to stand where you're not forced to admit them, right?
3698200	3710200	I don't see any neuroscientific or cognitive scientific argument for substrate dependence for intelligence, given what we've already built.
3710200	3715200	And again, we're, we're going to keep going until something stops us.
3715200	3722200	We'll hit some immovable object that prevents us from releasing the next iPhone, but otherwise we're going to keep going.
3722200	3738200	And then, yeah, so then it, then whatever general will mean in that first case, there'll be a case where we've built a system that is so good at everything we care about that is functionally general.
3738200	3743200	Now, maybe it's missing something, maybe it's not, you know, maybe it's missing something that we don't even have a name for.
3743200	3750200	You know, we're missing all kinds of, there are possible intelligences that we haven't even thought about because we just haven't thought about them.
3750200	3762200	There are things that, there are ways to section the universe undoubtedly that we can't even conceive of because we are just, we have the minds we have.
3762200	3764200	Elon was asked a question on this by a journalist.
3764200	3770200	The journalist said to him, in a world where you believe that to be true, that artificial general intelligence is around the corner.
3770200	3777200	When your kids come to you and say, Daddy, what should I do with my life to find purpose and meaning?
3777200	3781200	What advice do you now give them if you hold that intuition to be true?
3781200	3783200	That it's around the corner.
3783200	3788200	What do you say to your children when they say, what should I do with my life to create purpose and meaning?
3788200	3790200	Did you say that Elon answered this question?
3790200	3791200	Yeah.
3791200	3792200	What did he say?
3792200	3798200	It's one of the most chilling moments in an interview I think I've seen in recent times because he stutters.
3798200	3801200	He goes silent for about 15 seconds, which is very un-Elon.
3801200	3802200	He stutters.
3802200	3804200	He stutters.
3804200	3806200	He stutters a bit more.
3806200	3817200	And then he says, he thinks he's living in suspended disbelief because if he really thought about it too much, what's the point?
3817200	3819200	He says, what's the point of me building all these cars?
3819200	3820200	He was in his Tesla factory.
3820200	3822200	What's the point of me building all these cars and what's the point?
3822200	3823200	I do think that sometimes.
3823200	3827200	So I think I have to live in, as his words were, suspended disbelief.
3827200	3834200	I would encourage him to ask what's the point of spending so much time on Twitter because he could clearly benefit from rethinking that.
3834200	3847200	But that aside, my answer to that is, and I think other people have echoed this of late, sort of surprising to me.
3847200	3861200	My answer is that this begins to privilege a return to the humanities as a kind of a core, like the center of mass intellectually for us.
3861200	3875200	Because when you look at what we're really good at and it's among the last things that can be plausibly automated.
3875200	3879200	And if we automate it, we may cease to care about it.
3879200	3886200	So it's like learning to write good code is something that is being automated now.
3886200	3900200	I'm not a programmer, but I have it on good authority that already these large language models are improving code and something like half the time they're writing better code than people.
3900200	3902200	That's all going to become like chess, right?
3902200	3907200	It's just it's going to be better than people ultimately.
3907200	3914200	So being a software engineer is something that, you know, and being a radiologist and being like those things.
3914200	3924200	It's easy to see how AI just cancels those professions or at least makes one person, you know, so effective at using AI tools that one person can do the work of 100 people.
3924200	3931200	So you got 99 people who don't have to be doing that job.
3931200	3947200	But creating art and, you know, writing novels and being a philosopher and talking about what it means to live a good life and how to do it.
3947200	3968200	That's something that if we have to look at where we're going to care that we're actually in relationship to and in dialogue with another person who we know to be conscious.
3968200	3974200	Where we don't care about that, we're not going to care, we're going to want just the best version of it.
3974200	3980200	You know, if the cure for cancer comes from an AI, an incentive in AI, I do not give a shit.
3980200	3982200	I just want the cure for cancer, right?
3982200	3990200	Like there's no added value that where I find out, okay, the person who gave me this cure really felt good about it.
3990200	3993200	And he's, you know, he had tears in his eyes when he figured out the cure.
3993200	3995200	Every engineering problem is like that.
3995200	3997200	We want safer planes.
3997200	3999200	We want, you know, we just want things to work.
3999200	4006200	We're not sentimental about the artistry that went into all of that.
4006200	4014200	And when the difference, when the gulf between the best and the mediocre gets big and consequential, we're just going to want the best.
4014200	4016200	We're just going to want the best all the way down the line.
4016200	4020200	But what is the best novel, right?
4020200	4024200	What is the best podcast conversation?
4024200	4032200	And can you subtract out the conscious person from that and still think it's the best?
4032200	4040200	And so like someone once sent me what purported to be, I didn't even listen to it, so I'm not even sure what it was.
4040200	4046200	But it looked like it was an AI-generated conversation between Alan Watts and Terrence McKenna, right?
4046200	4050200	Both guys who I love, I remember, I didn't know either of them.
4050200	4055200	But some fans of both have listened to hundreds of hours of both talk.
4055200	4057200	As far as I know, they never met each other.
4057200	4059200	It would have been a fascinating conversation.
4059200	4066200	I realized when I looked at this YouTube video, I realized I simply don't care how good this is.
4066200	4072200	Because I only care if it was actually Alan Watts and Terrence McKenna talking.
4072200	4078200	I got a simulacrum of Alan Watts and Terrence McKenna in this context I don't care about.
4078200	4088200	So in another use case, I stumbled upon, I was playing with ChatGPT and I asked the causes of World War II.
4088200	4091200	Give me 500 words on the cause of World War II.
4091200	4096200	It gives you this perfect little bullet-pointed essay on the cause of World War II.
4096200	4099200	That's exactly what I want from it.
4099200	4100200	That's fine.
4100200	4104200	I don't care that there was no person behind that typing.
4104200	4114200	But when I think, well, do I want to read Churchill's history of World War II?
4114200	4115200	It's on my shelf to read.
4115200	4119200	It's one of these aspirational sets of books.
4119200	4120200	I haven't read it yet.
4120200	4124200	I actually want to read it because Churchill wrote it.
4124200	4130200	And if you could give me an AI version of Churchill saying this is in the style of Churchill,
4131200	4134200	even Churchill scholars say this sounds like Churchill.
4134200	4136200	I actually don't care about it.
4136200	4137200	That's not the use.
4137200	4143200	I'll take the generic use of give me the cause of World War II.
4143200	4146200	The fake Churchill is profoundly uninteresting to me.
4146200	4151200	The real Churchill, even though he's dead, is interesting to me.
4151200	4158200	So the rebuttal I give here, and this is what my mind is doing, is saying the distinction you're presenting,
4158200	4161200	the difference I see is that in the case of the conversation between two people,
4161200	4166200	your respect that has been generated by AI, someone has signaled to you that it is fake.
4166200	4172200	If you remove that because say Churchill thought, why would I write a book when I could just click a button
4172200	4180200	and this thing will write it in my voice, in my tone of voice, with the entire back catalogue of things I've written before.
4180200	4184200	And it will produce my account and it will save me time.
4184200	4187200	So I'll just click a button, my publisher maybe will do it for me.
4187200	4195200	And then I'll sell that to Sam on the basis that it is my thoughts, which I can imagine a very near future.
4195200	4200200	If we just do it by percentage, how many books are going to be increasingly written by artificial intelligence?
4200200	4204200	To the point that when you look at a shelf, I imagine at some point in the future,
4204200	4208200	if the intelligence does increase by any measure,
4208200	4214200	that most of it would be words strung together by artificial intelligence
4214200	4218200	and it will be selling potentially better than the words written by humans.
4218200	4224200	So again, when we go back to the conversation with your children, there might not be a career there either
4224200	4230200	because artificial intelligence is faster, can produce more, can test and iterate on whether it sells better,
4230200	4236200	clicks gets more clicks, it can write the headline, create the picture, write the content.
4236200	4240200	And then I can just take the check because I put my name to it.
4240200	4244200	So even in that regard, what remains?
4244200	4251200	Well, so in the limit, what I think we're imagining is a world where,
4251200	4256200	and so none of the terrifyingly bad things have happened, so it's just all working.
4256200	4262200	We're just producing a ton of great stuff that is better than the human stuff and people are losing their jobs.
4262200	4268200	We've got a labor disruption, but we're not talking about any other kind of political catastrophe
4268200	4278200	or cyber apocalypse, much less AGI destroying everything.
4278200	4286200	Then I think we just need a different economic assumption and ethical intuition around the value of work.
4286200	4296200	Our default norm now in a capitalist society is you have to figure out something to do with most of your time
4296200	4299200	that other people are willing to pay you for.
4299200	4305200	You have to figure out how to add value to other people's lives such that you reliably get paid.
4305200	4308200	Otherwise, you might die.
4308200	4313200	We've got a social safety net, but it's pretty meager.
4314200	4316200	There are cracks you can fall through.
4316200	4325200	You can wind up homeless, and we're not going to figure out what to do about that all too well.
4325200	4335200	Your claim upon your existence among us is you finding something to do with your time that other people will pay you for.
4336200	4343200	Now we've got artificial intelligence removing some of those opportunities, creating others, but in the limit,
4343200	4352200	and I do think it is different, I think analogies to other moments in technological history are fundamentally flawed,
4352200	4362200	I think this is a technology which in the limit will replace jobs and not create better new jobs in their wake.
4362200	4368200	This just cancels the need for human labor ultimately.
4368200	4375200	Strangely, it replaces some of the highest status most cognitively intensive jobs first.
4375200	4385200	It replaces Elon Musk before it replaces your electrician or your plumber or your masseuse way before.
4385200	4389200	We have to internalize the reality of that.
4389200	4395200	Again, this is in success. This is all good things happening.
4395200	4397200	We have to have a new ethic.
4397200	4405200	We have to have a new economics based on that ethic, which is UBI is one solution to this.
4405200	4407200	You shouldn't have to work to survive.
4407200	4409200	Universal basic income.
4409200	4412200	There's so much abundance now being created.
4412200	4415200	We have to figure out how to spread this wealth around.
4415200	4417200	We've got a cure for cancer over here.
4417200	4425200	We've got perfect photovoltaic driven economies over here.
4425200	4428200	We've solved the climate change issue.
4428200	4433200	We're just pulling wealth out of the ether, essentially.
4433200	4440200	We've got nanotechnology that is just birthing whole new industries yet, but it's all being driven by AI.
4440200	4442200	There's no room in this.
4442200	4450200	Whenever you put a person in the decision chain, you're just adding noise.
4450200	4454200	This should be the best thing that's ever happened to us.
4454200	4459200	This is just like God handing us the perfect labor saving device.
4459200	4464200	The machine that can build every other machine that can do anything you could possibly want.
4464200	4468200	We should figure out how to spread the wealth around in that case.
4468200	4474200	This is just powered by sunlight, no more wars over resource extraction.
4474200	4477200	It can build anything.
4477200	4481200	We can all be on the beach just hanging out with our friends and family.
4481200	4486200	Did you believe we should do universal basic income where everybody's given a monthly check?
4486200	4488200	We have to break this connection.
4488200	4496200	Again, this is what will have to happen in the presence of this kind of labor force dislocation
4496200	4500200	enabled by all of this going perfectly well.
4500200	4502200	This is pure success.
4502200	4504200	AI is just producing good things.
4504200	4508200	The only bad thing is putting all these people out of work.
4508200	4510200	It's coming for your job eventually.
4510200	4511200	I've heard this.
4511200	4515200	My issue with it and my rebuttal when I talked to my friends about this idea of universal basic income
4515200	4519200	where we hand out enough cash or resources to people so that they're stable,
4519200	4523200	which I'm not necessarily against but just want to play with it a little bit,
4523200	4528200	is humans seem to have an innate desire for purpose and meaning
4528200	4534200	and we seem to be designed and built psychologically for labor and for discomfort.
4534200	4539200	But it doesn't have to be labor that's tied to money.
4539200	4544200	We will get our status in other ways and we'll get our meaning in other ways.
4544200	4547200	These are all just stories we tell ourselves.
4547200	4553200	You're talking to a person who knows it's possible to be happy actually doing nothing,
4553200	4557200	like just sitting in a room for a month and just staring at the wall.
4557200	4558200	Because you've done it.
4558200	4560200	Like that's possible.
4560200	4563200	Yet that's most people's worst nightmare.
4563200	4566200	It's solitary confinement in a prison is considered a torture.
4566200	4569200	I know people who spent 20 years in a cave.
4569200	4573200	There are capacities here that are worth talking about.
4573200	4582200	But just more commonly, I think we want to be entertained.
4582200	4584200	We want to have fun.
4584200	4586200	We want to be with the people we love.
4586200	4591200	We want to be useful in relationship.
4591200	4601200	And in so far as that gets uncoupled from the necessity of working to survive.
4601200	4602200	It doesn't all just go away.
4602200	4610200	We just need new norms and new ethics and new conversations around what we do on vacation.
4610200	4615200	What you're imagining is that if you put everyone on vacation, on the best vacation,
4615200	4622200	you can make the vacation as good as possible, a majority of people will eventually be miserable
4622200	4626200	because they're not back at work.
4626200	4629200	And yet most of these people are working so that they have enough money
4629200	4631200	so that they could finally take that vacation.
4631200	4635200	We will figure out a new way to be happy on the beach.
4635200	4641200	If you get bored with Frisbee, we will figure something else out that is fun.
4641200	4647200	I'll be able to read the Churchill history of World War II on the beach
4647200	4653200	and not be rushed by any other imperative because I'm happily retired
4653200	4660200	because my AI is creating the thing that is solving all my economic problems.
4660200	4664200	We should be so lucky as to have that be our problem.
4664200	4670200	How to be happy in conditions of no economic imperative,
4670200	4675200	no basis for political strife on the basis of scarce resources
4675200	4688200	and the question of survival is off the table with respect to what one does with one's time and attention.
4688200	4691200	You can be as lazy as you want and you'll still survive.
4691200	4696200	You can be as unlucky as you want and you'll still survive.
4696200	4703200	The awful situation we're in now is that differences in luck mean everything.
4703200	4710200	Someone is born without any of the advantages that we have.
4710200	4715200	We don't have an economic system that reliably gives them
4715200	4719200	every advantage and opportunity they could have.
4719200	4728200	We've convinced ourselves we either don't have the resources
4728200	4730200	or we've convinced ourselves we don't have the resources.
4730200	4733200	We don't have the incentive such that we access the resources
4733200	4739200	so as to actually come to the help of people we could help.
4739200	4744200	The idea that people starve to death is just unimaginable and yet it still happens.
4744200	4749200	That's not a scarcity problem, it's a political problem wherever it happens.
4749200	4754200	Yet all of this is tied to a system where everyone has convinced themselves
4754200	4764200	that it's normal to really have one's survival be in question if one doesn't work.
4765200	4773200	By choice or by accident, I think it's still true that at least in the U.S.
4773200	4775200	this is almost certainly not true in the U.K.
4775200	4783200	but in the U.S. the most common reason for a personal bankruptcy is overwhelming medical expense
4783200	4785200	that just comes upon you for whatever reason.
4785200	4791200	Your wife gets cancer, you guys go bankrupt solving the cancer problem
4791200	4796200	failing to solve the cancer problem and now everything else unravels.
4796200	4802200	We have a society which thinks, yeah, well, I'm lucky you.
4802200	4805200	If you wind up homeless just don't sleep in front of my store
4805200	4809200	because you're going to hurt my business.
4809200	4819200	Successful AI that cancels lots of jobs would only be cancelling those jobs
4819200	4824200	by virtue of producing so many good things, so much value for everybody
4824200	4828200	that we would have to figure out how to spread that wealth around.
4828200	4839200	Otherwise we would have an amazingly dystopian bottleneck for a few short years
4839200	4841200	and then we would just have a revolution.
4841200	4847200	Then the guys in their integrated communities making trillions of dollars
4847200	4857200	based on them having gotten close enough to the GPUs that some of it rubbed off on them.
4857200	4861200	Yeah, they'd be dragged out of their houses and off their Gulf Streams
4861200	4867200	and we would have a fundamental reset, we'd have a hard reset of the political system.
4867200	4872200	If I had to put you in a yes or no situation and ask your intuition the question now
4872200	4878200	that if your objective was to, which I'm sure it is, is to encourage the betterment of humanity
4878200	4882200	and to increase our odds of happiness and well-being 100 years from now
4882200	4885200	and there was a button placed in front of you
4885200	4891200	and it would either end the development of artificial intelligence as we've seen it over the last decade
4891200	4897200	so we'd never proceed with developing intelligent machines or not.
4897200	4901200	So you could press a button and stop it right now.
4901200	4904200	And stop it permanently such that we never then do that thing?
4904200	4909200	We just never figure out how to build intelligent machines?
4909200	4913200	Pause it indefinitely.
4913200	4922200	Well, I would definitely pause it to a point where we would get our heads around the alignment problem.
4922200	4928200	Permanently. If the button was a permanent pause that you couldn't undo.
4928200	4931200	Well, the question is how deep does that go?
4931200	4934200	We have everything we have now but we just never get better than now.
4934200	4937200	Yeah, we never make progress from here.
4937200	4941200	And your objective is to make humanity happy and prosperous?
4941200	4952200	It's hard because when you begin imagining all of the good stuff that we could get with aligned superhuman AI
4952200	4957200	then it's just cornucopia upon cornucopia.
4957200	4961200	It's just everything is potentially within reach.
4961200	4969200	Yeah, I mean I take the existential risk scenario seriously enough that I would pause it.
4969200	4973200	I think we will eventually get to it.
4973200	4980200	If curing cancer is a biomedical engineering problem that admits of a solution
4980200	4984200	I think there's every reason to believe it ultimately would be.
4984200	4990200	We will eventually get there based on our own muddling along with our current level of tech.
4990200	4993200	Currently information tech.
4993200	5000200	I'm reasonably confident of that.
5001200	5012200	Our intelligence shows every sign of being general is just it's not as fast as we would want it to be.
5012200	5023200	The thing that AI is going to give us is it's going to give us speed that is...
5023200	5026200	There's speed and then there's the access, there's memory.
5026200	5036200	We can't integrate, we don't have the ability, no person or team of people can integrate all of the data we already have.
5036200	5045200	The real promise here is that these systems will be able to find patterns that we wouldn't even know how to look for
5045200	5048200	and then do something on the basis of those patterns.
5048200	5059200	I think an intelligent search within the data space by apes like ourselves will eventually do most of the great things we want done.
5059200	5077200	The problems we need to solve so as to safeguard the career of our species
5077200	5087200	to make civilization durable and sane and to remove this sort of Damocles that is over our heads at every moment
5087200	5092200	that at any moment we could just decide to have a nuclear war that ruins everything
5092200	5098200	or create an engineered pandemic that ruins everything.
5098200	5101200	We don't need superhuman intelligence to solve all those problems.
5101200	5108200	We need an appropriate emotional response to the untenability of the status quo
5108200	5115200	and we need a political dialogue that eventually transcends our tribalism.
5115200	5119200	For those of you that don't know, this podcast is sponsored by Woop, a company that I'm a shareholder in
5119200	5122200	and I'm obsessed with my Woop, it's glued to my wrist 24x7
5122200	5129200	and for those of you that don't know, it's essentially a personalized wearable health and fitness coach that helps me to have the best possible health.
5129200	5131200	My Woop has literally changed my life.
5131200	5134200	Woop is doing something this month which I'd highly suggest checking out.
5134200	5138200	It's a global community challenge called the Core 4 Challenge.
5138200	5144200	Essentially, they guide you through a set of four activities throughout the month of August that are scientifically proven to improve your overall health.
5144200	5149200	I'm giving it a go and I can't wait to see the impact it has on me and I highly recommend you to join me with that.
5149200	5152200	So if you're not on Woop here, there is no better time to start.
5152200	5156200	If you're a friend of mine, there's a high probability that I've already given you a Woop because I'm that obsessed with it.
5156200	5158200	It is the thing that I check when I wake up in the morning.
5158200	5159200	It's the first thing that I look at.
5159200	5162200	I want the information on my sleep to then plan my day around.
5162200	5172200	So if you haven't joined Woop yet, head to join.woop.com.co to get your free Woop device and your first month free.
5172200	5176200	Try it for free and if you don't like it after 29 days, they're going to give you your money back.
5176200	5178200	But I have a suspicion that you're going to keep it.
5178200	5180200	Check it out now and let me know how you get on.
5180200	5181200	Send me a DM.
5181200	5182200	Quick one.
5182200	5187200	If you've been listening to this podcast for some time, one of the recurring messages you've heard over and over and over again,
5187200	5194200	especially when we first had that conversation with Tim Spector, is about the importance of greens in our diet.
5194200	5201200	And a while ago, I started pressing my friends at Hewlett to come out with a product that did exactly that.
5201200	5205200	Allowed you to have all those greens, the vitamins and minerals you need in a drink.
5205200	5212200	And after several, several, several months of iterations and processes, they released this product called Hewlett Daily Greens,
5212200	5219200	which is now one of my favorite products from Hewlett because it tastes great and it fills that very important nutritional gap that I had in my diet.
5219200	5228200	The problem is, it launched in the US and it sold out straight away and became a smash hit for Hewlett for the very reasons I've described.
5228200	5232200	It's now back in stock in the United States, but it's not here in the UK yet.
5232200	5236200	So if you're a UK listener, which I know a lot of you are, it's not yet available.
5236200	5239200	So let's all attack Hewlett.
5239200	5244200	Let's DM them everywhere we can and tell them to bring Hewlett Daily Greens to the UK.
5244200	5246200	This is the product.
5246200	5251200	When it is available in the UK, I'm going to let you know first, but until then, let's spam their DMs.
5251200	5259200	You and, I'd say a few others, maybe two or three others, helped change my mind about one of the most profound things I think anyone could believe,
5259200	5266200	which was when I was 18, I believed in Christianity and then there was a couple of moments that shook my belief.
5266200	5278200	Nothing on a personal level, just a couple of ideas that managed to sort of infect my operating system that led my curiosity towards your work.
5278200	5280200	And I changed my mind profoundly.
5280200	5283200	It's such a profound change that I had.
5283200	5285200	How do we change our minds?
5285200	5292200	And I really want to focus that question on the individual's mind.
5292200	5294200	I want to change my mind.
5294200	5303200	I want better beliefs, better ideas in my head that are going to allow me to get out of my own way because I'm not a cheat.
5303200	5304200	I'm miserable.
5304200	5307200	I'm not living the life that I...
5307200	5311200	I would say I know I can live, but some people don't even know they can live a better life.
5311200	5312200	I'm not happy.
5312200	5314200	That's the signal.
5314200	5317200	And I want to rectify this in some way.
5317200	5318200	Yeah.
5318200	5322200	Well, there are a few right lines for me.
5322200	5329200	We take our ethical lives and our relationships to other people.
5329200	5337200	There's the problem of individual well-being that's still real even if you're in a moral solitude.
5337200	5342200	If you're on a desert island by yourself, you really don't have ethical questions that are emerging
5342200	5346200	because you're not in a relationship with anybody else, but you still have the problem of how to be happy.
5346200	5351200	But so much of our unhappiness is in collaboration with others.
5351200	5353200	We're unhappy in our relationships.
5353200	5357200	We're unhappy professionally.
5357200	5362200	And it's worth looking at how we're behaving with other people.
5362200	5375200	For me, the highest leverage change I ever made, and it's, again, it's very easy to spell out and it's very clear,
5375200	5383200	and ultimately it's pretty easy, is just to decide that you're not going to lie about anything, really.
5383200	5388200	I mean, there might be some situations in extremis where you'll feel forced to lie,
5388200	5398200	but those, in my view, are analogous to acts of violence that you may be forced to use in self-defense.
5398200	5402200	A line is sort of the first stage on the continuum of violence for me.
5402200	5409200	I'm not going to lie to someone unless I recognize that this is not a rational actor who I can possibly collaborate with.
5409200	5419200	This is someone I have to avoid or defeat or otherwise contain their propensity to do me harm.
5419200	5425200	So, yes, if the Nazis come to the door and ask if you've got Anne Frank and the Attic, yes, you can lie,
5425200	5430200	or you can shoot them, these are not normal circumstances.
5431200	5445200	But that aside, every other moment in life where people are tempted to lie is one that I think you can categorically rule out as being unethical
5445200	5458200	and beyond unethical, it's just not, it's creating a life you don't, when you examine it, you don't want to live.
5458200	5465200	The moment you know that you're not going to lie to people and they know that about you,
5465200	5473200	it's like all of the social dials get recalibrated on both sides,
5473200	5484200	and then you find yourself in the presence of people who don't ask you for your opinion unless they really want it.
5485200	5495200	And then when you're honest, then it's a night and day difference when you're giving people feedback, critical feedback,
5495200	5505200	and they know you're honest, they know their bullshit detector is not going off because they just know you're,
5505200	5513200	even when it's not convenient, you're being honest, or even when it's not comfortable, you're being honest.
5514200	5521200	One that's incredibly valuable because basically you're giving them the information that you would want if you were in their shoes,
5521200	5524200	because we have this sort of delusion that takes over us.
5524200	5531200	Whenever we're tempted to tell a white lie, we imagine, okay, this person doesn't want,
5531200	5539200	it'd be much better for me to just tell them the kind fiction than tell them the uncomfortable truth,
5539200	5545200	but we don't calculate for the golden rule there most of the time,
5545200	5549200	and if you just took a moment, you'd realize, wait a minute,
5549200	5558200	does someone who is actually doing a bad job want me to tell them that they're doing a good job
5558200	5564200	and then just send them out into the world to bounce around other people who are going to be recognizing,
5564200	5569200	as I just did, that the thing they're doing isn't so great.
5569200	5571200	You're just not doing them a favor.
5571200	5573200	This is part of the nature of belief change, isn't it?
5573200	5580200	When we believe that someone is on our side, or we believe from a political standpoint that they represent,
5580200	5584200	99% of the views that we represent, we're much more likely to change our beliefs.
5584200	5591200	I spoke to Tali Sharra about this, the neuroscientist, and I wrote about this in a chapter in my upcoming book about how you change people's minds.
5591200	5596200	They showed in the elections that if a flat earther says something to a flat earther about the nature of the earth,
5596200	5600200	they'll believe it, but if NASA says something to a flat earther, they will just dismiss it on site
5600200	5606200	because the source of that information is not one that they believe, or trust, or like, or believe is well-intentioned.
5606200	5608200	This is a bug, not a feature.
5608200	5615200	It's understandable, but this is something we have to grow beyond because the truth is the truth.
5616200	5619200	It goes in both directions.
5619200	5628200	The person on your team who you love and respect is capable in their very next sentence of speaking of falsehood,
5628200	5631200	and you need to be able to detect that.
5631200	5640200	Conversely, the person you least respect is capable of saying something that's quite incisive and worth taking on board.
5641200	5654200	We have to have this sort of metacognitive layer where we're noticing how we're getting played by our social alliances
5654200	5664200	and recognize that the truth, and rather often important truths, are evaluated by different principles.
5664200	5670200	I mean, it's not a matter of the messenger. You shouldn't shoot the messenger and you shouldn't worship him.
5670200	5680200	You mentioned lying as being a significant step change in your own happiness. Is that accurate?
5680200	5682200	In my happiness.
5684200	5686200	Immensely so.
5686200	5690200	Practically and specifically how?
5690200	5699200	When you look at how people ruin their reputations and their relationships and their businesses, their careers,
5699200	5705200	the gateway to all of the misbehavior that accomplishes that is lying.
5705200	5708200	I mean, look at some of that Lance Armstrong or Tiger Woods.
5708200	5713200	These guys are the absolute apogee of sport.
5713200	5719200	Everyone loves them. Everyone's just amazed at what they've accomplished.
5720200	5727200	Yet the dysfunction in their lives just gets vomited up for all to see at a certain point.
5727200	5735200	It was just enabled at every stage along the way by lying.
5735200	5742200	If either of them had early in their career before they became famous, before they became rich,
5743200	5750200	before they became tempted to do anything that was going to derail their lives later on,
5750200	5754200	if they had decided they weren't going to lie, right?
5754200	5760200	They would have found all, everything else they did to screw up their success impossible.
5760200	5767200	So when I decided, and this was in the book, this was a course I took at Stanford.
5767200	5772200	It was a seminar with this brilliant professor, Ron Howard, who many people,
5772200	5777200	I think some people in Silicon Valley have taken this course as well.
5777200	5781200	I mean, this course was just like a machine.
5781200	5785200	Undergraduates and graduate students would come in on one side,
5785200	5792200	and then 12 weeks later would come out convinced that basically lying was no longer on the menu, right?
5793200	5800200	The whole seminar was an analysis of the question, is it ever right to lie?
5800200	5809200	And really we focused on white lies, truly tempting lies as opposed to the obvious lies that screw up people's lives and relationships.
5811200	5818200	It's just so corrosive, and it's corrosive of relationships in ways that you,
5819200	5822200	unless you're a student of this kind of thing, you don't necessarily notice.
5822200	5826200	I mean, one example I believe that's in that book is that,
5826200	5832200	I remember my wife was with a friend, and the two of them were out,
5832200	5838200	and the friend had something she had to do with another friend later that night,
5838200	5841200	but she didn't really feel like doing it,
5841200	5844200	and she got a call from that friend in the presence of my wife,
5844200	5849200	and she just lied to the friends to get out of the plan.
5849200	5853200	She said, oh, I'm so sorry, but my daughter's got this thing,
5853200	5861200	and it was just an utterly facile use of dishonesty to get,
5861200	5866200	or she could have just been honest, but it was just too awkward to be honest,
5866200	5870200	so she just got out of it with a lie, but now it's in the presence of my wife,
5870200	5873200	and my wife is now, the immediate question is,
5873200	5876200	how many times have I been on the other side of that conversation?
5876200	5883200	How many times has she lied to me in an equally compelling way about something so trivial?
5883200	5890200	And so it just eroded trust in that relationship in a way that the liar would never have known about,
5890200	5894200	would never have detected it, because it just went right back to having a good time with,
5894200	5897200	they would just have to lunch, and they continued having their lunch,
5897200	5900200	and they're still having a good time, and it's all smiles,
5900200	5906200	but my wife has just logged something about kind of the ethical limitations of this person,
5906200	5909200	and the person doesn't know it, right?
5909200	5914200	And so once you sort of pull on this thread,
5914200	5921200	basically your entire life becomes, at least for the transition period,
5921200	5928200	until this just becomes a habit that you no longer have to consider,
5928200	5934200	suddenly the world becomes a kind of mirror thrown up to your mind,
5934200	5939200	and you meet yourself in all these situations where you were avoiding yourself before.
5939200	5945200	So like someone will say, do you want to have plans,
5945200	5948200	or do you want to collaborate with me on this project?
5949200	5959200	And if previously you always had recourse to some kind of white lie that just got you out of the awkward truth,
5959200	5966200	which is the answer is no, and there are actually reasons why not, right?
5966200	5969200	You never have to confront the awkwardness of that,
5969200	5972200	you're this kind of person who has these kinds of commitments,
5973200	5979200	I mean the most awkward one would be someone declares a romantic interest in you,
5979200	5990200	and the answer is no, and it's no for a totally superficial reason, right?
5990200	5995200	Like this person is, they're not attractive enough for you, right?
5995200	6000200	Or they're overweight, it's just like you have your reason why not,
6000200	6003200	and this is something you feel you cannot say, right?
6003200	6008200	Now I'm not saying that you should always go out of your way,
6008200	6013200	like someone with Tourette's who just helplessly blurts out the truth,
6013200	6018200	like there's a scope for kindness and compassion and tact,
6018200	6023200	but if someone is going to really drill down on the reasons why not,
6023200	6027200	if the person says no, I want to know exactly why you don't want to go out with me,
6028200	6033200	there's something to discover on either side of that true disclosure, right?
6033200	6037200	Like either you are cast back on yourself and you have to realize,
6037200	6045200	okay, I'm such a superficial person that it doesn't matter who anyone is,
6045200	6049200	if they're 10 pounds overweight, I'm not interested, right?
6049200	6053200	That's the mirror held up to your mind, it's like, okay, all right,
6053200	6056200	so you're that kind of person, do you want to still be that kind of person?
6056200	6061200	Do you really want to just decide that everyone, no matter what their virtues, right,
6061200	6065200	and no matter what chaos is going on in their life,
6065200	6069200	actually this person might actually lose those 10 pounds next month
6069200	6072200	and you would have a very different situation,
6072200	6078200	but are you really not available, are you really filtering by weight in this way?
6078200	6081200	And are you really comfortable with that?
6081200	6084200	And are you comfortable saying that?
6084200	6088200	If somebody forces you to actually be honest?
6088200	6090200	We have a closing tradition on this podcast
6090200	6092200	where the last guest leaves a question for the next guest,
6092200	6094200	not knowing who they're going to leave it for.
6094200	6097200	The question that's been left for you, peckable handwriting.
6097200	6101200	Where do you want to be when you die?
6101200	6108200	Describe the place, time, people, smell, and feeling.
6109200	6117200	Well, this actually connects with an idea I've had.
6117200	6121200	I mean, I think what we need, we haven't talked about psychedelics here,
6121200	6125200	but there's been this renaissance in research in psychedelics,
6125200	6127200	and it's hard to know.
6127200	6132200	I'm worried that we could recapitulate some of the errors of the 60s
6132200	6137200	and roll this all out in a way that's less than wise,
6137200	6142200	but the wise version would be, I think we need to recapitulate
6142200	6145200	something like the Mysteries of Elusis,
6145200	6150200	where we have rites of passage that are enabled by,
6150200	6156200	in many people's case, psychedelics and the practice of meditation.
6156200	6163200	I just think these are just fundamental tools of insight that are...
6163200	6166200	I mean, for most people, it's hard to see how they would get them any other way.
6167200	6172200	There's a longer conversation about which molecule and how and all that,
6172200	6181200	but another component of this is a hospice situation where the experience of dying
6181200	6188200	is as wisely embraced and facilitated as is possible,
6188200	6192200	and I think psychedelics could certainly play a role for many people there.
6192200	6197200	I imagine something like, we need places that are truly beautiful
6197200	6203200	where people have gone to die and their families can visit them there,
6203200	6217200	and it is just a final rite of passage that is embraced with all the wisdom we can muster there.
6218200	6225200	In my case, currently, I'd be happy to be home,
6225200	6232200	but wherever home is at that point, I would want a view of the sky.
6232200	6235200	It could be an ocean beneath the sky, that would be ideal.
6239200	6244200	There's basically nothing that makes me happier than just looking at a blue sky
6244200	6249200	with just watching cumulus clouds move across a blue sky.
6249200	6254200	I can extract so much mental pleasure just looking at that.
6258200	6263200	If I'm going to spend my last hours of life looking at anything,
6263200	6266200	if my eyes are going to be open, looking at the sky and having...
6266200	6269200	The stars with the sky, the daytime sky.
6270200	6277200	Light pollution is enough of a thing in my world that I feel like I go for years
6277200	6283200	without seeing a good night sky, so I've kind of given up hope there,
6283200	6285200	but I do love that.
6287200	6292200	But yeah, just a view of the sky and with the people I love at that point
6292200	6295200	who are still alive at that point.
6296200	6301200	I'm not worried about death in that sense.
6306200	6308200	The death part is not a problem.
6311200	6315200	I can imagine there could be sort of medical chaos and uncertainty
6315200	6319200	and all of the weirdness that happens around the dying process,
6319200	6321200	depending on...
6321200	6324200	There are all kinds of ways to die that I wouldn't choose,
6325200	6330200	but having a nice place to do that with a view of the sky
6330200	6334200	would be the only solution I think I would require.
6334200	6337200	The question asks the smell.
6337200	6338200	Give me the smell.
6338200	6340200	Give me an ocean breeze.
6340200	6343200	I have put an ocean there, so yeah, an ocean breeze would be perfect.
6345200	6346200	Sam, thank you so much.
6346200	6348200	Thank you for not just this conversation.
6348200	6350200	As I said to you before you sat down,
6350200	6354200	you were pivotal in really helping me to unpack some problems on our jungle,
6354200	6359200	some conflicts I should describe them as with my view on religious belief
6359200	6364200	and the nature of the world, but I think more importantly,
6364200	6368200	you didn't rob me of my religious beliefs and leave me with nothing.
6368200	6371200	You left me with something else, which is something that was really important to me,
6371200	6374200	which was the idea that there can still be great meaning
6374200	6378200	and there can be what you describe as spirituality in the absence
6378200	6381200	or in the place of that religious belief.
6381200	6384200	Religious belief gives people a lot of things,
6384200	6386200	and it's funny because when I was religious
6386200	6389200	and I went on the journey to becoming agnostic, let's say,
6389200	6393200	I was in conflict with people, as in I would want to have a debate with everybody
6393200	6397200	and I spent those two years watching everything that you and Richard Dawkins
6397200	6402200	and Hitchens had all done, and then I came out of the side and it was peaceful.
6402200	6405200	You believe what you want, I'll believe what I want.
6405200	6408200	As long as we're not causing any conflicts with each other
6408200	6410200	and doing any harm, it's okay.
6410200	6413200	And then I discovered what I would call my own spirituality,
6413200	6416200	which is the meaning that I see in the world around me
6416200	6419200	and the self and things like psychedelics.
6419200	6421200	And it's a better place to be.
6421200	6425200	And it removed my fear of death, which I had as a religious person.
6425200	6428200	So thank you. Thank you for that.
6428200	6430200	And all your subsequent work, incredible books,
6430200	6432200	you've written so many of them that are absolutely incredible.
6432200	6438200	You've got an unbelievable podcast, which I was gorging on before you came here as well in an app,
6438200	6442200	which, I mean, if you could speak just a few sentences about the meaning of the app
6442200	6444200	and what you do, I know it's much more than meditation now,
6444200	6448200	but I think people listening to this might be compelled to check it out and download it.
6448200	6452200	Yeah, well, so I had that book, which you're holding, Waking Up,
6452200	6456200	which is where I talk about my experience in meditation
6456200	6463200	and just how I fit it into a scientific, secular worldview.
6463200	6468200	And it just turns out that an app is a much better delivery system for that kind of information.
6468200	6471200	I mean, it's just hearing audio. You don't even need video.
6471200	6475200	I think audio is the perfect medium for it.
6475200	6479200	So when that technology came about or when I discovered it,
6479200	6483200	I just felt incredibly lucky to be able to build it.
6483200	6486200	And so it's kind of outgrown me now. There are many, many teachers on it
6486200	6489200	and many other topics beyond meditation that are touched.
6489200	6500200	But it really subverts all of the problems that some of which we touched upon here with the smartphone.
6500200	6505200	I mean, the smartphone has become this tool of fragmentation for us.
6505200	6509200	It fragments our attention. It continually interrupts our experience.
6509200	6513200	It's depending on how you use it.
6513200	6516200	But most of what we do with it, you know, you're checking Slack,
6516200	6518200	you're checking your email, you're checking your social media,
6518200	6522200	you're just, it's punctuating your life with all this, you know,
6522200	6526200	at this point, seemingly necessary interruptions.
6526200	6532200	But this app or, you know, really any app like it that is delivering this kind of content
6532200	6536200	subverts all that because it's just, this is, it's just a platform
6536200	6541200	where you're getting audio that is guiding you in a specific,
6541200	6545200	very specific use of attention and a sort of reordering of your priorities
6545200	6553200	and getting you to recognize things about your experience that you wouldn't otherwise see.
6553200	6558200	And yeah, an app is just a sheer good luck.
6558200	6562200	It turns out it's just the perfect delivery system for that information.
6562200	6566200	So yeah, I just felt very lucky to have stumbled upon it because again, you know,
6566200	6570200	10 years ago, there were no apps and, you know, there's just,
6570200	6572200	all I could do is write a book.
6572200	6573200	Sam, thank you.
6573200	6574200	Yeah, thank you.
6574200	6576200	Thank you so much for your generosity.
6576200	6577200	Yeah, a pleasure to meet you as well.
6577200	6578200	Congratulations with everything.
6578200	6582200	I mean, it's really, I was catching up on your podcast in anticipation of this
6582200	6584200	and it's amazing the reach you've got now.
6584200	6586200	So it's wonderful.
6586200	6590200	No, it's still trying to catch up with it, but it's a credit to all of the team.
6590200	6593200	And I really want to say from the bottom of my heart, thank you.
6593200	6596200	Because the work you do is really, really important.
6596200	6599200	It's been important in my life, as I've said, but it's just really important.
6599200	6603200	And I feel like we're living in a world where like nuance and all the things you've talked about
6603200	6608200	and openness to debate and honest dialogue asks, we're getting further and further away from there.
6608200	6611200	So if there's anyone left in this world that's still willing to engage on that level,
6611200	6613200	I feel like they must be protected at all costs.
6613200	6614200	And I see you as one of those people.
6614200	6615200	So thank you.
6615200	6616200	Nice.
6616200	6617200	Well, to be continued.
6620200	6621200	Thank you.
