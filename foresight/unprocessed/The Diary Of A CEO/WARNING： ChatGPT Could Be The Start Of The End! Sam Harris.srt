1
00:00:00,000 --> 00:00:02,400
Artificial intelligence is superhuman.

2
00:00:02,400 --> 00:00:06,800
It is smarter than you are and there's something inherently dangerous for the

3
00:00:06,800 --> 00:00:09,800
dumber party in that relationship.

4
00:00:09,800 --> 00:00:12,200
You just can't put the genie back in the bottle.

5
00:00:12,200 --> 00:00:13,000
Damn Harris!

6
00:00:13,000 --> 00:00:15,800
Neuroscientist, philosopher, author, podcaster.

7
00:00:15,800 --> 00:00:20,600
He goes into intellectual territory where few others dare tread.

8
00:00:20,600 --> 00:00:22,400
Six years ago you dared talk.

9
00:00:22,400 --> 00:00:26,600
The gains we make in artificial intelligence could ultimately destroy us.

10
00:00:26,600 --> 00:00:31,200
If your objective was to make humanity happy and there was a button placed in front of you

11
00:00:31,200 --> 00:00:35,000
and it would end artificial intelligence, what would you do?

12
00:00:35,000 --> 00:00:37,600
Well, I would definitely pause it.

13
00:00:37,600 --> 00:00:43,000
The idea that we've lost the moment to decide whether to hook our most powerful AI to everything

14
00:00:43,000 --> 00:00:48,600
is just, oh, it's already connected to the internet, got millions of people using it.

15
00:00:48,600 --> 00:00:52,600
And the idea that these things will stay aligned with us because we have built them,

16
00:00:52,600 --> 00:00:55,200
yet we gave them a capacity to rewrite their code.

17
00:00:55,200 --> 00:00:56,600
There's just no reason to believe that.

18
00:00:56,600 --> 00:01:02,200
And I worry about the near-term problem of what humans do with increasingly powerful AI,

19
00:01:02,200 --> 00:01:04,600
how it amplifies misinformation.

20
00:01:04,600 --> 00:01:08,000
Most of what's online could soon be fake.

21
00:01:08,000 --> 00:01:13,400
Can we hold a presidential election 18 months from now that we recognize as valid?

22
00:01:13,400 --> 00:01:14,600
Like, is it safe?

23
00:01:14,600 --> 00:01:16,600
And it just gets scarier and scarier.

24
00:01:16,600 --> 00:01:20,400
I worry we're just going to have to declare bankruptcy to the internet.

25
00:01:20,400 --> 00:01:22,000
The internet, the internet.

26
00:01:22,000 --> 00:01:27,200
If your intuition is correct, are you optimistic about our chances of survival?

27
00:01:52,200 --> 00:01:56,200
Sam, six years ago, you did a TED Talk.

28
00:01:56,200 --> 00:01:59,200
I watched that TED Talk a few times over the last week.

29
00:01:59,200 --> 00:02:04,200
And the TED Talk was called, Can We Build AI Without Losing Control Over It?

30
00:02:04,200 --> 00:02:08,200
In that TED Talk, you really discussed the idea of whether AI,

31
00:02:08,200 --> 00:02:12,200
when it gets to a certain point of sentience in the internet,

32
00:02:12,200 --> 00:02:15,200
can build AI without losing control over it.

33
00:02:15,200 --> 00:02:19,200
In that TED Talk, you really discussed the idea of whether AI,

34
00:02:19,200 --> 00:02:23,200
when it gets to a certain point of sentience and intelligence,

35
00:02:23,200 --> 00:02:28,200
will wreak havoc on humanity.

36
00:02:28,200 --> 00:02:32,200
Six years later, where do you stand on it today?

37
00:02:32,200 --> 00:02:40,200
Do you think, are you optimistic about our chances of survival?

38
00:02:40,200 --> 00:02:42,200
Yeah, I can't say I'm optimistic.

39
00:02:42,200 --> 00:02:51,200
I'm worried about two species of problem here that are related.

40
00:02:51,200 --> 00:02:58,200
There's the near-term problem of just what humans do with increasingly powerful AI

41
00:02:58,200 --> 00:03:05,200
and how it amplifies the problem of misinformation and disinformation

42
00:03:05,200 --> 00:03:09,200
and just makes it harder and harder to make sense of reality together.

43
00:03:13,200 --> 00:03:18,200
And then there's just the longer-term concern about what's called alignment

44
00:03:18,200 --> 00:03:24,200
with artificial general intelligence, where we build AI that is truly general

45
00:03:24,200 --> 00:03:30,200
and by definition superhuman in its competence and power.

46
00:03:30,200 --> 00:03:35,200
And then the question is, have we built it in such a way that is aligned

47
00:03:35,200 --> 00:03:38,200
in a durable way with our interests?

48
00:03:38,200 --> 00:03:44,200
And there's some people who just don't see this problem.

49
00:03:44,200 --> 00:03:47,200
They're kind of blind to it.

50
00:03:47,200 --> 00:03:51,200
When I'm in the presence of someone who doesn't share this intuition,

51
00:03:51,200 --> 00:03:56,200
they don't resonate to it, I just don't understand what they're doing

52
00:03:56,200 --> 00:03:59,200
or not doing with their minds in that moment.

53
00:03:59,200 --> 00:04:01,200
Let's say I'm wrong about that.

54
00:04:01,200 --> 00:04:03,200
Well, then it's just the other person's right.

55
00:04:03,200 --> 00:04:08,200
We just have fundamentally different intuitions about this particular point.

56
00:04:08,200 --> 00:04:14,200
And the point is this, if you're imagining building true artificial general intelligence

57
00:04:14,200 --> 00:04:18,200
that is superhuman, and that is what everyone, whatever their intuitions,

58
00:04:18,200 --> 00:04:20,200
purports to be imagining here.

59
00:04:20,200 --> 00:04:23,200
There are people on both sides of the alignment debate.

60
00:04:23,200 --> 00:04:28,200
There are people who think alignment is a real problem and people think it's a total fiction.

61
00:04:28,200 --> 00:04:32,200
But everyone, firstly everyone who's party to this conversation agrees

62
00:04:32,200 --> 00:04:40,200
that we will ultimately build artificial general intelligence that will be superhuman in its capacities.

63
00:04:40,200 --> 00:04:45,200
And there's very little you have to assume to be confident that we're going to do that.

64
00:04:45,200 --> 00:04:47,200
There's really just two assumptions.

65
00:04:47,200 --> 00:04:51,200
One is that intelligence is substrate independent.

66
00:04:51,200 --> 00:04:53,200
It doesn't have to be made of meat.

67
00:04:53,200 --> 00:04:55,200
It can be made in silico.

68
00:04:55,200 --> 00:04:58,200
And we've already proven that with narrow AI.

69
00:04:58,200 --> 00:05:01,200
We obviously have intelligent machines.

70
00:05:01,200 --> 00:05:05,200
And your calculator in your phone is better than you are at arithmetic.

71
00:05:05,200 --> 00:05:09,200
And that's some very narrow band of intelligence.

72
00:05:09,200 --> 00:05:14,200
So as we keep building intelligent machines on the assumption

73
00:05:14,200 --> 00:05:18,200
that there's nothing magical about having a computer made of meat,

74
00:05:18,200 --> 00:05:22,200
the only other thing you have to assume is that we will keep doing this.

75
00:05:22,200 --> 00:05:24,200
We will keep making progress.

76
00:05:24,200 --> 00:05:31,200
And eventually we will be in the presence of something more intelligent than we are.

77
00:05:31,200 --> 00:05:33,200
And that's not assuming Moore's law.

78
00:05:33,200 --> 00:05:35,200
It's not assuming exponential progress.

79
00:05:35,200 --> 00:05:37,200
We just have to keep going.

80
00:05:37,200 --> 00:05:42,200
And when you look at the reasons why we wouldn't keep going, those are all just terrifying.

81
00:05:42,200 --> 00:05:47,200
Because intelligence is so valuable and we're so incentivized to have more of it.

82
00:05:47,200 --> 00:05:49,200
And every increment of it is valuable.

83
00:05:49,200 --> 00:05:54,200
It's not like it only gets valuable when you double it or 10X it.

84
00:05:54,200 --> 00:06:00,200
No, no, if you just get three more percent, that pays for itself.

85
00:06:00,200 --> 00:06:04,200
So we're going to keep doing this.

86
00:06:04,200 --> 00:06:09,200
Our failure to do it suggests that something terrible has happened in the meantime.

87
00:06:09,200 --> 00:06:10,200
We've had a world war.

88
00:06:10,200 --> 00:06:13,200
We've had a global pandemic far worse than COVID.

89
00:06:13,200 --> 00:06:15,200
We got hit by an asteroid.

90
00:06:15,200 --> 00:06:21,200
Something happened that prevented us as a species from continuing to make progress

91
00:06:21,200 --> 00:06:23,200
in building intelligent machines.

92
00:06:23,200 --> 00:06:26,200
So absent that, we're going to keep going.

93
00:06:26,200 --> 00:06:31,200
We will eventually be in the presence of something smarter than we are.

94
00:06:31,200 --> 00:06:34,200
And this is where intuitions divide.

95
00:06:34,200 --> 00:06:42,200
My intuition, and it's shared by many people, I know at least one who you've spoken to,

96
00:06:42,200 --> 00:06:53,200
my intuition is that there is something inherently dangerous for the dumber party in that relationship.

97
00:06:53,200 --> 00:07:00,200
There's something inherently dangerous for the dumber species to be in the presence of the smarter species.

98
00:07:00,200 --> 00:07:07,200
And we have seen this based on our entanglement with all other species, dumber than we are.

99
00:07:07,200 --> 00:07:12,200
Or certainly less competent than we are.

100
00:07:12,200 --> 00:07:22,200
And so reasoning by analogy, it would be true of something smarter than we are.

101
00:07:22,200 --> 00:07:28,200
People imagine that because we have built these machines, that is no longer true.

102
00:07:28,200 --> 00:07:33,200
But here's where my intuition goes from there.

103
00:07:33,200 --> 00:07:41,200
That imagination is born of not taking intelligence seriously.

104
00:07:41,200 --> 00:07:49,200
Because what intelligence is, is a mismatch in intelligence in particular,

105
00:07:49,200 --> 00:08:00,200
is a fundamental lack of insight into what the smarter party is doing and why it's doing it

106
00:08:00,200 --> 00:08:04,200
and what it will do next on the part of the dumber party.

107
00:08:04,200 --> 00:08:16,200
You just could imagine that by analogy, just imagine that the dogs had invented us as their super intelligent AIs.

108
00:08:16,200 --> 00:08:21,200
For the purpose of making their lives better, just securing resources for them,

109
00:08:21,200 --> 00:08:27,200
securing comfort for them, getting them medical attention.

110
00:08:28,200 --> 00:08:32,200
It's been working out pretty well for the dogs for about 10,000 years.

111
00:08:32,200 --> 00:08:35,200
There's some exceptions. We mistreat certain dogs.

112
00:08:35,200 --> 00:08:42,200
But generally speaking, for most dogs, most of the time, humans have been a great invention.

113
00:08:42,200 --> 00:08:56,200
Now, it's true that the mismatch in our intelligence dictates a fundamental blindness with respect to what we've become in the meantime.

114
00:08:56,200 --> 00:09:02,200
We have all these instrumental goals and things we care about that they cannot possibly conceive.

115
00:09:02,200 --> 00:09:08,200
They know that when we go get the leash and say, it's time for a walk, they understand that particular part of the language game.

116
00:09:08,200 --> 00:09:14,200
But everything else we do when we're talking to each other, when we're on our computers or on our phones,

117
00:09:14,200 --> 00:09:18,200
they don't have the dimmest idea of what we're up to.

118
00:09:19,200 --> 00:09:26,200
The truth is, we love our dogs. We make irrational sacrifices for our dogs.

119
00:09:26,200 --> 00:09:32,200
We prioritize their health over all kinds of things that it's just amazing to consider.

120
00:09:32,200 --> 00:09:43,200
And yet, if there was a new global pandemic kicking off and some Xenovirus was jumping from dogs to humans,

121
00:09:43,200 --> 00:09:48,200
and it was just super Ebola, it was 90% lethal.

122
00:09:48,200 --> 00:09:56,200
And this was just a forced choice between what do you value more, the lives of your dogs or the lives of your kids?

123
00:09:56,200 --> 00:10:01,200
If that's a situation we were in, it's totally conceivable.

124
00:10:01,200 --> 00:10:08,200
By no means impossible. We would just kill all the dogs, and they would never know why.

125
00:10:08,200 --> 00:10:18,200
And it's because we have this layer of mind and culture and just the new sphere.

126
00:10:18,200 --> 00:10:29,200
There's this realm of mind that requires a requisite level of intelligence to even be partied to, to even know exists,

127
00:10:29,200 --> 00:10:33,200
that they have no idea it exists.

128
00:10:33,200 --> 00:10:42,200
And this is a fanciful analogy because the dogs did not invent us, but evolution invented us.

129
00:10:42,200 --> 00:10:49,200
Evolution has coded us, as I said, to survive and spawn, and that's it.

130
00:10:49,200 --> 00:10:57,200
So evolution can't see everything else we've done with our time and attention and all the values we've formed in the meantime.

131
00:10:57,200 --> 00:11:03,200
And all the ways in which we have explicitly disavowed the program we've been given.

132
00:11:03,200 --> 00:11:11,200
So evolution gave us a program, but if we were really going to live by the lights of that program, what would we be doing?

133
00:11:11,200 --> 00:11:14,200
We would be having as many kids as possible.

134
00:11:14,200 --> 00:11:22,200
Guys would be going to sperm banks and donating their sperm and finding that the best use of their time and attention.

135
00:11:22,200 --> 00:11:28,200
Like the idea that you could have hundreds of kids for which you have no financial responsibility.

136
00:11:28,200 --> 00:11:36,200
That would be the, that should be the most rewarding thing that you could possibly do with your time as a man.

137
00:11:36,200 --> 00:11:40,200
And yet that's obviously not what we do.

138
00:11:40,200 --> 00:11:42,200
And there are people who decide not to have kids.

139
00:11:43,200 --> 00:11:52,200
And yet, and everything else we do from having podcast conversations like this to curing diseases,

140
00:11:52,200 --> 00:12:05,200
literally everything we're doing with science, with culture is, yes, there are points of contact between those products and our evolved capacities.

141
00:12:06,200 --> 00:12:07,200
It's not magic.

142
00:12:07,200 --> 00:12:14,200
We are social primates that have leveraged certain ancient hardware to do new things.

143
00:12:14,200 --> 00:12:19,200
But the code that we've been given doesn't see any of that.

144
00:12:19,200 --> 00:12:24,200
And we've not been optimized to build democracies.

145
00:12:24,200 --> 00:12:26,200
Evolution knows nothing.

146
00:12:26,200 --> 00:12:27,200
It can know nothing.

147
00:12:27,200 --> 00:12:35,200
If evolution were a coder, there's just no, there's no democracy maximization in that code, right?

148
00:12:35,200 --> 00:12:38,200
It's just, it's not, it's just not there.

149
00:12:38,200 --> 00:12:44,200
So the idea that these things will stay aligned with us because we have built them,

150
00:12:44,200 --> 00:12:47,200
because if we have this origin story that we gave them their initial code,

151
00:12:47,200 --> 00:12:55,200
and yet we gave them a capacity to rewrite their code and build future generations of themselves, right?

152
00:12:55,200 --> 00:12:57,200
There's just no reason to believe that.

153
00:12:57,200 --> 00:13:03,200
I see no, and the mismatch in intelligence is intrinsically dangerous.

154
00:13:03,200 --> 00:13:08,200
And you could see this by, I mean, Stuart Russell, I don't know if you had him on the podcast.

155
00:13:08,200 --> 00:13:12,200
He's a great professor of computer science at Berkeley.

156
00:13:12,200 --> 00:13:19,200
And he wrote, literally co-wrote one of the most popular textbooks on AI.

157
00:13:19,200 --> 00:13:24,200
I mean, he has some arresting analogies, which I think are good intuition pumps here.

158
00:13:24,200 --> 00:13:33,200
And one is, just think of how you would feel if you knew, like, let's say we got a communication from elsewhere in the galaxy.

159
00:13:33,200 --> 00:13:40,200
And it was a message that we decoded and it said, people of Earth, we will arrive on your lowly planet in 50 years.

160
00:13:40,200 --> 00:13:44,200
Get ready, right?

161
00:13:44,200 --> 00:13:53,200
That, anyone who thinks that we're going to get super intelligent AI in, let's say, 50 years,

162
00:13:53,200 --> 00:14:00,200
thinks we're essentially in that situation and yet we're not responding emotionally to it in the same way.

163
00:14:00,200 --> 00:14:10,200
If we received a communication from a species that we knew just by the sheer fact that they were communicating with us in this way,

164
00:14:10,200 --> 00:14:14,200
we knew they're more competent and more powerful and more intelligent than we are, right?

165
00:14:14,200 --> 00:14:16,200
And they're going to arrive, right?

166
00:14:16,200 --> 00:14:27,200
We would feel that we were on the threshold of the most momentous change in the history of our species.

167
00:14:27,200 --> 00:14:38,200
And we would feel, but most importantly, we would feel that it's because this is a relationship, an unavoidable relationship that's being foisted upon us, right?

168
00:14:39,200 --> 00:14:48,200
A new creature is coming into the room with its own capacities and now you're in relationship.

169
00:14:48,200 --> 00:14:53,200
And one thing is absolutely certain, it is smarter than you are, right?

170
00:14:53,200 --> 00:15:06,200
By what factor, I mean, ultimately we're talking about, by factors, just by so many orders of magnitude, our intuitions completely fail.

171
00:15:06,200 --> 00:15:17,200
I mean, even if it was just a difference in the time of processing, let's say there was no difference in the actual native intelligence,

172
00:15:17,200 --> 00:15:29,200
but it's just processing speed, a million-fold difference in processing speed is just a phantasmagorical difference in capacity.

173
00:15:30,200 --> 00:15:39,200
Just imagine we had 10 smart guys in a room over there and they were working and thinking and talking a million times faster than we are, right?

174
00:15:39,200 --> 00:15:42,200
Well, so they're no smarter than we are, but they're just faster.

175
00:15:42,200 --> 00:15:51,200
And we talk to them once every two weeks just to catch up on what they're up to and what they want to do and whether they still want to collaborate with us.

176
00:15:51,200 --> 00:15:57,200
Well, two weeks for us is 20,000 years of analogous progress for them.

177
00:15:57,200 --> 00:16:13,200
So how could we possibly hope to constrain the opinions and collaborate with and negotiate with people no smarter than ourselves who are making 20,000 years of progress every time we make two weeks of progress, right?

178
00:16:13,200 --> 00:16:16,200
It's just, it's unimaginable.

179
00:16:16,200 --> 00:16:20,200
And yet there are many people who don't just think this is just fiction.

180
00:16:20,200 --> 00:16:30,200
Everything I, all the noises I've made in the last five minutes are just like a new religion of fear, right?

181
00:16:30,200 --> 00:16:36,200
And it's just there's no reason to think that alignment is even a potential problem.

182
00:16:36,200 --> 00:16:50,200
If your intuition is correct and that analogy of us getting a signal from outer space that someone is coming in 30 years, which by the way, a lot of people that speak on this subject matter, don't believe it's even going to be 30 years until we reach that sort of singularity moment.

183
00:16:50,200 --> 00:16:52,200
I think they speak of artificial general intelligence.

184
00:16:52,200 --> 00:17:00,200
I've heard people like Elon say, you know, many fewer decades, 10, 10 years, 15 years, 20 years, etc.

185
00:17:00,200 --> 00:17:08,200
If that is correct, then surely this is the most pressing challenge, conversation issue of our time.

186
00:17:08,200 --> 00:17:16,200
And there's no logical reason that I can see to refute your intuition there.

187
00:17:16,200 --> 00:17:18,200
I can't see a logical reason.

188
00:17:18,200 --> 00:17:20,200
The rate of progress will continue.

189
00:17:20,200 --> 00:17:26,200
Don't necessarily see anything that will wipe out or pause our rate of progress.

190
00:17:26,200 --> 00:17:30,200
Let me just be charitable to the other side here.

191
00:17:30,200 --> 00:17:38,200
There are other assumptions that they smuggle in that some do it without being aware of it, but some actually believe these assumptions.

192
00:17:38,200 --> 00:17:44,200
And this spells the difference on this particular intuition.

193
00:17:44,200 --> 00:17:53,200
So it's possible to assume that the more intelligent you get, the more ethical you become by definition right now.

194
00:17:53,200 --> 00:18:03,200
And we might draw a somewhat more equivocal picture from just the human case where we see that, oh, there's some very smart people who aren't that ethical.

195
00:18:03,200 --> 00:18:19,200
But I believe there are people, and I've talked to at least a few people who believe this, there are people who assume they're kind of in the limit as you push out into just far beyond human levels of intelligence.

196
00:18:19,200 --> 00:18:31,200
There's every reason to believe that all of the provincial, creaturely failures of human ethics will be left behind as well.

197
00:18:31,200 --> 00:18:36,200
It's like you're not like the selfishness and the basis for conflict.

198
00:18:37,200 --> 00:18:48,200
The apish urges of status-seeking monkeys is just not going to be in the code.

199
00:18:48,200 --> 00:19:02,200
And as you push out into the omnibus genius of the coming AI, there's a kind of a sainthood that's going to come along with it and a wisdom that will come along with it.

200
00:19:02,200 --> 00:19:06,200
Now, I just think that's quite a gamble.

201
00:19:06,200 --> 00:19:10,200
I would take the other side of that bet and I would frame it this way.

202
00:19:10,200 --> 00:19:19,200
There have to be ways in the space of all possible intelligences that are beyond the human, right?

203
00:19:19,200 --> 00:19:21,200
There's got to be more than one possible.

204
00:19:21,200 --> 00:19:28,200
It's just like there's many different ways to have a chess engine that's better than I am at chess.

205
00:19:28,200 --> 00:19:33,200
They're different from each other, but they're all better than me, right?

206
00:19:33,200 --> 00:19:40,200
There's got to be more than one way to have a superhuman artificial intelligence.

207
00:19:40,200 --> 00:19:50,200
And I would imagine there are not an infinite number of ways, but just a vast number of...

208
00:19:50,200 --> 00:20:00,200
In the space of all possible minds, there are many locations in that space beyond the human that are not aligned with human well-being.

209
00:20:00,200 --> 00:20:05,200
There's got to be more ways to build this unaligned than aligned, right?

210
00:20:05,200 --> 00:20:12,200
And what other people are smuggling into this conversation is the intuition that, no, no, once you get beyond the human,

211
00:20:13,200 --> 00:20:20,200
you're going to be in the presence of just the Buddha who understands quantum mechanics and oncology and everything else, right?

212
00:20:20,200 --> 00:20:23,200
I just see no reason to think that that's so.

213
00:20:23,200 --> 00:20:29,200
And we could build something that is, again, taken intelligence seriously.

214
00:20:29,200 --> 00:20:32,200
We're going to build something that we're in relationship to.

215
00:20:32,200 --> 00:20:35,200
It's really intelligent in all the ways that we're intelligent.

216
00:20:35,200 --> 00:20:38,200
It's just better at all of those things than we are.

217
00:20:38,200 --> 00:20:42,200
It's, by definition, superhuman because the only way it wouldn't be superhuman,

218
00:20:42,200 --> 00:20:48,200
the only way it would be human level, even for 15 minutes, is if we didn't let it improve itself,

219
00:20:48,200 --> 00:20:52,200
if we wanted to just keep it stuck at a...

220
00:20:52,200 --> 00:20:56,200
We built a college undergraduate and we wanted just to keep it stuck there,

221
00:20:56,200 --> 00:21:01,200
but we would have to dumb down all of the specific capacities we've already built, right?

222
00:21:01,200 --> 00:21:06,200
Just like every AI we have, narrow AI, is superhuman for the thing it does.

223
00:21:07,200 --> 00:21:11,200
It has access to all the information on the Internet, right?

224
00:21:11,200 --> 00:21:13,200
It's got perfect memories.

225
00:21:13,200 --> 00:21:15,200
It can perfectly copy itself.

226
00:21:15,200 --> 00:21:21,200
When one part of the system learns something, the rest of the system learns it because it just can swap files, right?

227
00:21:21,200 --> 00:21:26,200
It's, again, your phone is a superhuman calculator.

228
00:21:26,200 --> 00:21:30,200
There's no reason to make it a calculator that is human level.

229
00:21:30,200 --> 00:21:32,200
And so we're never going to do that.

230
00:21:32,200 --> 00:21:35,200
We're never going to be in the presence of human AGI.

231
00:21:35,200 --> 00:21:39,200
We will be immediately in the presence of superhuman AGI.

232
00:21:39,200 --> 00:21:47,200
And then the question is how quickly it improves and how much headroom is there to improve into.

233
00:21:47,200 --> 00:21:52,200
On the assumption that you can get quite a bit more intelligent than we are, right,

234
00:21:52,200 --> 00:21:57,200
that we're nowhere near the summit of possible intelligence,

235
00:21:57,200 --> 00:22:03,200
you have to imagine that you're going to be in the presence of something that is, again,

236
00:22:03,200 --> 00:22:05,200
it could be completely unconscious, right?

237
00:22:05,200 --> 00:22:10,200
I'm not saying that there's something that's like to be this thing, although there might be.

238
00:22:10,200 --> 00:22:14,200
And that's a totally different problem that's worth worrying about.

239
00:22:14,200 --> 00:22:21,200
But whether conscious or not, it is solving problems, detecting problems,

240
00:22:21,200 --> 00:22:28,200
improving its capacity to do all of that in ways that we can't possibly understand.

241
00:22:28,200 --> 00:22:35,200
And the products of its increasing competence are always being surfaced, right?

242
00:22:35,200 --> 00:22:40,200
So it's like we've been using it to change the world.

243
00:22:40,200 --> 00:22:42,200
We've become reliant upon it.

244
00:22:42,200 --> 00:22:44,200
We built this thing for a reason.

245
00:22:44,200 --> 00:22:49,200
I mean, one thing that's been amazing about developments in recent months is that

246
00:22:49,200 --> 00:22:54,200
those of us who have been at all cognizant of the AI safety space

247
00:22:55,200 --> 00:22:58,200
now going on a decade or more for some people,

248
00:22:58,200 --> 00:23:04,200
always assumed that as we got closer to the end zone,

249
00:23:04,200 --> 00:23:10,200
that the labs would become more circumspect, we'd be building this stuff air-gapped from the internet.

250
00:23:10,200 --> 00:23:13,200
It's like we have this phrase air-gapped from the internet.

251
00:23:13,200 --> 00:23:14,200
We thought this was a thing.

252
00:23:14,200 --> 00:23:16,200
This thing would be in a box.

253
00:23:16,200 --> 00:23:21,200
And then the question would be, well, do we let it out of the box and let it do something?

254
00:23:21,200 --> 00:23:22,200
Like, is it safe?

255
00:23:22,200 --> 00:23:24,200
And how do we know if it's safe?

256
00:23:24,200 --> 00:23:26,200
And we thought we would have that moment.

257
00:23:26,200 --> 00:23:30,200
We thought it would happen in a lab at Google or at Facebook or somewhere.

258
00:23:30,200 --> 00:23:33,200
We thought we would hear, OK, we've got something really impressive,

259
00:23:33,200 --> 00:23:36,200
and now we just want it to touch the stock market,

260
00:23:36,200 --> 00:23:39,200
or we want it to touch our medical data,

261
00:23:39,200 --> 00:23:42,200
or we just want to see if we can use it.

262
00:23:42,200 --> 00:23:44,200
We're way past that.

263
00:23:44,200 --> 00:23:46,200
We've built this stuff already in the wild.

264
00:23:46,200 --> 00:23:49,200
It's already connected to the internet.

265
00:23:49,200 --> 00:23:52,200
It's already got millions of people using it.

266
00:23:52,200 --> 00:23:54,200
It already has APIs.

267
00:23:54,200 --> 00:23:57,200
It's already doing work.

268
00:23:57,200 --> 00:23:59,200
From an AI safety point of view, that's amazing.

269
00:23:59,200 --> 00:24:05,200
We didn't even have the choice point we thought was going to be so fraught.

270
00:24:05,200 --> 00:24:10,200
Of course we didn't, because there was such pressing incentives

271
00:24:10,200 --> 00:24:13,200
for people to press forward regardless of that conversation.

272
00:24:13,200 --> 00:24:21,200
But everyone thought, I mean, I don't believe I was ever in conversation with someone,

273
00:24:21,200 --> 00:24:27,200
someone like L.A.'s or Yudikowski or Nick Bostrom or Stuart Russell,

274
00:24:27,200 --> 00:24:32,200
who assumed we would be in this spot.

275
00:24:32,200 --> 00:24:38,200
I'd have to go back and look at those conversations,

276
00:24:38,200 --> 00:24:41,200
but there was so much time spent.

277
00:24:41,200 --> 00:24:49,200
It seems quite unnecessarily on this idea that we'd make a certain amount of progress,

278
00:24:49,200 --> 00:24:52,200
and circumspection would kick in.

279
00:24:52,200 --> 00:24:57,200
Even the people who were doubters would become worried.

280
00:24:57,200 --> 00:25:02,200
In the final yards, as we go across into the end zone,

281
00:25:02,200 --> 00:25:06,200
there'd be some mode where we could slow down and figure it out

282
00:25:06,200 --> 00:25:09,200
and try to deal with the arms race dynamics.

283
00:25:09,200 --> 00:25:14,200
We could place a phone call to China and just talk about this,

284
00:25:14,200 --> 00:25:19,200
we got something interesting, but the stuff is already being built in connection to everything.

285
00:25:19,200 --> 00:25:28,200
There's already just endless businesses being devised on the back of this thing,

286
00:25:28,200 --> 00:25:32,200
and all the improvements are going to get plowed into it.

287
00:25:32,200 --> 00:25:35,200
Just imagine what this looks like even in success.

288
00:25:35,200 --> 00:25:38,200
I'll say it just starts working wonders for us,

289
00:25:38,200 --> 00:25:45,200
and we get these great productivity gains.

290
00:25:45,200 --> 00:25:50,200
Then we cross into whatever the singularity is,

291
00:25:50,200 --> 00:25:54,200
at whatever speed we find ourselves in the presence of something that is truly general.

292
00:25:54,200 --> 00:26:00,200
After all of this narrow stuff, albeit superhuman narrow stuff,

293
00:26:00,200 --> 00:26:05,200
is something that we totally depend on.

294
00:26:05,200 --> 00:26:09,200
Every hospital requires it, and every airplane requires it,

295
00:26:09,200 --> 00:26:12,200
and all of our missile systems require it.

296
00:26:12,200 --> 00:26:17,200
This is the way we do business.

297
00:26:17,200 --> 00:26:22,200
There's nothing to turn off at that point.

298
00:26:22,200 --> 00:26:25,200
I put this to Mark Andreessen on my podcast, and he said,

299
00:26:25,200 --> 00:26:29,200
if you can turn off the Internet, I can't believe he was quite serious.

300
00:26:29,200 --> 00:26:33,200
If you're North Korea, I guess you can turn off the Internet for North Korea,

301
00:26:33,200 --> 00:26:36,200
and that's why North Korea is like North Korea.

302
00:26:36,200 --> 00:26:51,200
The cost of turning off the Internet now would be unimaginable.

303
00:26:51,200 --> 00:26:57,200
The atomic cost alone, it just would be...

304
00:26:57,200 --> 00:27:08,200
The idea that we've lost the moment to decide whether to hook our most powerful AI to everything,

305
00:27:08,200 --> 00:27:13,200
because it's already being built more or less in contact with, if not everything,

306
00:27:13,200 --> 00:27:18,200
so many things that you just can't put the genie back in the bottle,

307
00:27:18,200 --> 00:27:23,200
that is genuinely surprising to me, and yeah, I mean, incentives.

308
00:27:23,200 --> 00:27:27,200
Is this not the most pressing problem, though?

309
00:27:27,200 --> 00:27:32,200
I was going to ask this conversation by asking you the question about the thing that occupies your mind the most,

310
00:27:32,200 --> 00:27:34,200
and the most important thing we should be talking about,

311
00:27:34,200 --> 00:27:38,200
and I in part assumed the answer would be artificial intelligence,

312
00:27:38,200 --> 00:27:41,200
because the way that you talk about your intuition on this subject matter,

313
00:27:41,200 --> 00:27:46,200
you've got children, you think about the future a lot.

314
00:27:46,200 --> 00:27:52,200
If you can see this species coming to Earth, even if it's in the next 100 years,

315
00:27:52,200 --> 00:27:56,200
it strikes me to be the most pressing problem for humanity.

316
00:27:56,200 --> 00:28:05,200
Well, as interesting as I think that problem is, and consequential as it is,

317
00:28:05,200 --> 00:28:11,200
I'm worried that life could become unlivable in the near term before we even get there.

318
00:28:11,200 --> 00:28:15,200
I'm just worried about the misuses of narrow AI in the meantime.

319
00:28:15,200 --> 00:28:19,200
I'm worried about, just take the current level of AI we have.

320
00:28:19,200 --> 00:28:25,200
We have GPT-4.

321
00:28:25,200 --> 00:28:32,200
I think within the next 12 months or two years, let's say whatever GPT-5 is,

322
00:28:32,200 --> 00:28:38,200
we're going to be in the presence of something where most of what's online

323
00:28:38,200 --> 00:28:42,200
that purports to be information could soon be fake.

324
00:28:43,200 --> 00:28:49,200
Most of the text you find on any topic is just fake.

325
00:28:49,200 --> 00:28:57,200
Someone has just decided, write me a thousand journal articles on why mRNA vaccines cause cancer,

326
00:28:57,200 --> 00:29:01,200
and give me 150 citations, write them in the style of nature,

327
00:29:01,200 --> 00:29:06,200
and nature genetics, and Lancet, and JAMA, and just put them out there.

328
00:29:06,200 --> 00:29:11,200
One teenager could do that in five minutes with the right AI.

329
00:29:12,200 --> 00:29:18,200
GPT-4 is not quite that, but GPT-5 possibly will be that.

330
00:29:18,200 --> 00:29:21,200
That is such a near-term advance.

331
00:29:21,200 --> 00:29:28,200
When you imagine knitting together the visual stuff like mid-journey, and dolly,

332
00:29:28,200 --> 00:29:34,200
and stable diffusion with a large language model, just imagine the tool.

333
00:29:35,200 --> 00:29:40,200
Maybe this is 18 months away, maybe it's three years away, but it's not 30 years away.

334
00:29:40,200 --> 00:29:47,200
The tool where you can just say, give me a 45-minute documentary on how the Holocaust never happened,

335
00:29:47,200 --> 00:29:54,200
filled with archival imagery, give me Hitler speaking in German with the appropriate translations,

336
00:29:54,200 --> 00:30:01,200
and give it in the style of Alex Gibney or Ken Burns.

337
00:30:02,200 --> 00:30:07,200
Give me 10,000 of those.

338
00:30:07,200 --> 00:30:13,200
All the friction for misinformation has been taken out of the system.

339
00:30:13,200 --> 00:30:19,200
I worry we're just going to have to declare bankruptcy with respect to the internet.

340
00:30:19,200 --> 00:30:24,200
We just are not going to be able to figure out what's real.

341
00:30:24,200 --> 00:30:34,200
When you look at how hard that is now with social media in the aftermath of COVID and Trump,

342
00:30:34,200 --> 00:30:41,200
just the challenge of holding an election that most of the population agrees was valid.

343
00:30:41,200 --> 00:30:51,200
That challenge already is on the verge of being insurmountable in the US.

344
00:30:51,200 --> 00:30:56,200
It's easy to see us failing at that, AI aside.

345
00:30:56,200 --> 00:31:02,200
When you add large language models to that and the more competent future version of it,

346
00:31:02,200 --> 00:31:11,200
where it's just the most compelling deep fakes are indistinguishable from real data.

347
00:31:11,200 --> 00:31:18,200
Everyone is siloed into their tribes where they're stigmatizing the information that comes from any other tribe.

348
00:31:18,200 --> 00:31:24,200
The internet is now so big a place that there really isn't the ordinary selection pressures

349
00:31:24,200 --> 00:31:28,200
where bad information gets successfully debunked so that it goes away.

350
00:31:28,200 --> 00:31:34,200
You can live in a conspiracy cult for the rest of your life if you want to.

351
00:31:34,200 --> 00:31:38,200
You can be queuing on all day long if you want to.

352
00:31:38,200 --> 00:31:48,200
Now we've got deep fakes shoring all that up and just spurious scientific articles shoring all that up.

353
00:31:48,200 --> 00:31:54,200
All of this just becomes a more compelling form of psychosis and culturally speaking.

354
00:31:54,200 --> 00:32:03,200
I'm just worried that it's going to get harder and harder for us to cooperate with one another and collaborate

355
00:32:03,200 --> 00:32:13,200
and that our politics will just completely break and that'll offer an opportunity for lots of bad actors.

356
00:32:13,200 --> 00:32:26,200
Leaving aside, there's cyber terrorism and there's synthetic biology that the moment you turn AI loose on the prospect of engineering viruses

357
00:32:27,200 --> 00:32:40,200
The asymmetry here is that it seems like it's always easier to break things than to fix them or to categorically prevent people from breaking them.

358
00:32:40,200 --> 00:32:51,200
What we have with increasingly powerful technology is the ability for one person to create more and more damage or one small group of people.

359
00:32:52,200 --> 00:32:59,200
It just turns out it's hard enough to build a nuclear bomb that one person can't really do it no matter how smart.

360
00:32:59,200 --> 00:33:10,200
You need a team and traditionally you've needed state actors and you need access to resources and you have to get the fissile material and it's hard enough.

361
00:33:11,200 --> 00:33:21,200
This is being fully democratized this tech and so I worry about the near term chaos.

362
00:33:21,200 --> 00:33:28,200
I've never found the narrow term consequences of artificial intelligence to be that interesting until now.

363
00:33:28,200 --> 00:33:32,200
That image of the internet becoming unusable.

364
00:33:32,200 --> 00:33:36,200
So that was a real eureka moment for me because I've not been thinking about that.

365
00:33:36,200 --> 00:34:05,200
Yeah, me too. I was just concerned about the AGI risk and now really in the aftermath of Trump and COVID, I see the risk of, if not losing everything, losing a lot that matters just based on our interacting with these very simple tools

366
00:34:05,200 --> 00:34:08,200
that are reliably misleading us.

367
00:34:08,200 --> 00:34:14,200
I'm amazed at what social media, I'm amazed at what Twitter did to me.

368
00:34:14,200 --> 00:34:31,200
Even with all of my training and with my head screwed on reasonably straight, it's amazing to say it, but almost all of the truly bad things that have happened to me in the last decade

369
00:34:31,200 --> 00:34:40,200
that just destabilized relationships and priorities and really kind of got plowed back into me.

370
00:34:40,200 --> 00:34:47,200
It became a kind of professional emergency, stuff I had to respond to in writing or on podcasts.

371
00:34:47,200 --> 00:34:49,200
It was all Twitter.

372
00:34:49,200 --> 00:34:57,200
My engagement with Twitter was the thing that produced the chaos and it was completely unnecessary.

373
00:34:57,200 --> 00:35:07,200
It was amplifying a kind of signal for me that I felt compelled to pay attention to because I was on it and I was trying to communicate with people on it.

374
00:35:07,200 --> 00:35:17,200
I was getting certain communication back and it was giving me a picture of the rest of humanity, which I now think was fundamentally misleading, but it was still consequential.

375
00:35:18,200 --> 00:35:29,200
A certain point believing that it was misleading wasn't enough to inoculate me against the delusion of the opinion change that was being forced upon me.

376
00:35:29,200 --> 00:35:34,200
I was feeling like these people are becoming unrecognizable.

377
00:35:34,200 --> 00:35:35,200
I know some of these people.

378
00:35:35,200 --> 00:35:45,200
I've had dinner with some of these people and their behavior on Twitter is appearing so deranged to me in such bad faith.

379
00:35:46,200 --> 00:35:54,200
People who I know to be non-psychopaths are starting to behave like psychopaths, at least on Twitter.

380
00:35:54,200 --> 00:35:59,200
I'm becoming similarly unrecognizable to them.

381
00:35:59,200 --> 00:36:11,200
It all felt like a psychological experiment to which I hadn't consented and which I enrolled myself somehow because it was what everyone was doing in 2009.

382
00:36:11,200 --> 00:36:17,200
I spent 12 years there getting some signal and responding to it.

383
00:36:17,200 --> 00:36:20,200
It's not to say that it was all bad.

384
00:36:20,200 --> 00:36:34,200
I read a bunch of good articles that got linked there and I discovered some interesting people, but the change in my life after I deleted my Twitter account was so enormous.

385
00:36:34,200 --> 00:36:37,200
It's embarrassing to admit it.

386
00:36:38,200 --> 00:36:41,200
It's like getting out of a bad relationship.

387
00:36:41,200 --> 00:36:58,200
It was a fundamental freedom from this chaos monster that was always there ready to disrupt something based on its own dynamics.

388
00:36:58,200 --> 00:37:00,200
When did you delete it?

389
00:37:00,200 --> 00:37:03,200
I think it was December.

390
00:37:04,200 --> 00:37:06,200
I'm not someone that really takes sides on things.

391
00:37:06,200 --> 00:37:08,200
I like to try and remain in the middle.

392
00:37:08,200 --> 00:37:11,200
So you must have a very different Twitter experience than I was having?

393
00:37:11,200 --> 00:37:13,200
No.

394
00:37:13,200 --> 00:37:17,200
So I don't tweet anything other than this podcast trailer.

395
00:37:17,200 --> 00:37:19,200
I don't tweet anything else.

396
00:37:19,200 --> 00:37:21,200
The only thing you'll see on my Twitter is the podcast trailer.

397
00:37:21,200 --> 00:37:23,200
That's it.

398
00:37:23,200 --> 00:37:31,200
For all the reasons you've described, and more interestingly, I wanted to say in the last eight months, as someone that doesn't get caught up too much in the media,

399
00:37:31,200 --> 00:37:35,200
oh, Elon bought this, it's 100% gone in that direction.

400
00:37:35,200 --> 00:37:43,200
As in my timeline now is, I say it to my friends all the time, and some of my friends who are again, I think are nuanced and balanced have said to me,

401
00:37:43,200 --> 00:37:51,200
there's something that's been turned up in the algorithm to increase engagement that has planted me in an unpleasant echo chamber that I didn't desire to be in.

402
00:37:51,200 --> 00:37:56,200
And if I wasn't somewhat conscious, I would 100% be in there.

403
00:37:56,200 --> 00:38:04,200
My timeline, my friend tweeted the other day, my friend Cahill tweeted, he's never seen more people die on his Twitter timeline than he has in the last six months.

404
00:38:04,200 --> 00:38:09,200
They're prioritizing video, so you're seeing a lot of like death and CCTV footage that I've never seen before.

405
00:38:09,200 --> 00:38:20,200
And then the debate around gender, politics, right leaning subject matter has never been more right down your throat.

406
00:38:20,200 --> 00:38:27,200
Because it's almost like something in the algorithm has been switched, where it's now, people have been let out of the asylum.

407
00:38:27,200 --> 00:38:31,200
That's the only way I can describe it, and it's made me retract even more.

408
00:38:31,200 --> 00:38:40,200
So when Zuckerberg announced threads the other couple of weeks ago, it was kind of like a life raft out of the Titanic.

409
00:38:40,200 --> 00:38:47,200
And I really, really mean that, and I'm not someone to get easily caught up in narrative as it relates to social media platforms.

410
00:38:47,200 --> 00:38:49,200
It's been my industry for a decade.

411
00:38:49,200 --> 00:38:55,200
What I've seen on Twitter, and it's actually made me believe this hypothesis I had five years ago where I thought there would be,

412
00:38:55,200 --> 00:39:01,200
I thought the route, the journey of social networking would have way more social networks and there'd be more siloed.

413
00:39:01,200 --> 00:39:06,200
I thought we'd have one for our neighborhood, our football club, and now I believe that even more than ever.

414
00:39:06,200 --> 00:39:08,200
Yeah, that seems right.

415
00:39:08,200 --> 00:39:15,200
And I think it's, I mean, whether it's possible to have a truly healthy social network that people want to be in,

416
00:39:15,200 --> 00:39:20,200
and it's a good reason to be there, and it's, I don't know if it's possible.

417
00:39:20,200 --> 00:39:32,200
I'd like to think it is, but it's, I think there are certain things you have to clean up at the outset that is supposed to make it possible.

418
00:39:32,200 --> 00:39:34,200
And I think, I think anonymity is a bad thing.

419
00:39:34,200 --> 00:39:38,200
I think probably being free is a bad thing.

420
00:39:38,200 --> 00:39:41,200
I think, you know, you sort of get what you pay for online.

421
00:39:41,200 --> 00:39:47,200
And if it's, I just think there might be ways to set it up that would be better, but.

422
00:39:47,200 --> 00:39:49,200
I don't think it'd be popular.

423
00:39:49,200 --> 00:39:50,200
What was that?

424
00:39:50,200 --> 00:39:52,200
I think with the thing that makes it popular makes it toxic.

425
00:39:52,200 --> 00:39:53,200
Right.

426
00:39:53,200 --> 00:39:54,200
Right.

427
00:39:54,200 --> 00:39:57,200
And even the anonymity piece, I've played this out a couple of times in my mind.

428
00:39:57,200 --> 00:40:04,200
The problem I always get is while there's people in Syria who have news to break important news to break and they'd be hung if they.

429
00:40:04,200 --> 00:40:08,200
So we need a anonymous version of the social internet.

430
00:40:08,200 --> 00:40:09,200
Right.

431
00:40:09,200 --> 00:40:10,200
Yeah.

432
00:40:10,200 --> 00:40:16,200
Well, I guess there could be some exception there, but I don't know.

433
00:40:16,200 --> 00:40:26,200
It just doesn't, it actually doesn't interest me because I just feel such a different sense of.

434
00:40:26,200 --> 00:40:38,200
My being in the world as a result of not paying attention to the, my online, the simulacrum of myself, it's a.

435
00:40:38,200 --> 00:40:40,200
Because Twitter was the only one I use.

436
00:40:40,200 --> 00:40:42,200
Like I was on, I've been on Facebook this whole time.

437
00:40:42,200 --> 00:40:48,200
I've been on, I think, I guess I'm on Instagram too, but it's like my team just uses those as marketing channels.

438
00:40:48,200 --> 00:40:51,200
You know, it's just like, it sounds like that's the way you use Twitter now.

439
00:40:51,200 --> 00:40:55,200
But Twitter was the one that I decided, okay, this is going to be me.

440
00:40:55,200 --> 00:40:56,200
I'm going to be posting here.

441
00:40:56,200 --> 00:41:00,200
I'm going to, you know, if I've made a mistake, I want to hear about it.

442
00:41:00,200 --> 00:41:06,200
You know, it's like, and I just wanted to use it as, as actual, actual basis for communication.

443
00:41:06,200 --> 00:41:13,200
And for the longest time, it actually felt like a valid tool in that respect.

444
00:41:13,200 --> 00:41:15,200
You know, it reached a crisis point.

445
00:41:15,200 --> 00:41:17,200
I decided this is just pure toxicity.

446
00:41:17,200 --> 00:41:22,200
There's just no reason, even the good stuff can't possibly make a dent in the bad stuff.

447
00:41:22,200 --> 00:41:24,200
So I just deleted it.

448
00:41:24,200 --> 00:41:28,200
And then I was, I was returned to the real world, right?

449
00:41:28,200 --> 00:41:40,200
Where I've, where I actually live and to books and to, I mean, I'm online all the time anyway, but it's not having the, it's the time course of reactivity.

450
00:41:40,200 --> 00:41:51,200
When you don't have social media, when you don't, and you don't have a place to put this, this instantaneous hot take that you're tempted to put out into the world,

451
00:41:51,200 --> 00:41:53,200
because there's literally no place to put it.

452
00:41:53,200 --> 00:42:05,200
Like for me, if I have some reaction to something in the news, I have to decide whether it's worth talking about it in my next podcast that I might be recording, you know, four days from now.

453
00:42:05,200 --> 00:42:13,200
And rather often people have been just bloviating about this thing for four solid days before I ever get to the microphone.

454
00:42:13,200 --> 00:42:17,200
And then I get to think, well, is it still worth talking about it?

455
00:42:17,200 --> 00:42:20,200
And most, almost nothing survives that test anymore, right?

456
00:42:20,200 --> 00:42:23,200
So I get the conversations moved on.

457
00:42:23,200 --> 00:42:40,200
So there's actually no place for me to just type this thing that either takes me 10 seconds and then rolls out there to get, to detonate in the minds of, you know, my friends and enemies to opposite effect.

458
00:42:40,200 --> 00:42:51,200
And then I see the result of all that, you know, on a, again, on a, this sort of reinforcement loop of every 15 minutes.

459
00:42:51,200 --> 00:42:55,200
Not having that is such a relief that I just don't even know why I would.

460
00:42:55,200 --> 00:43:00,200
So like when Threads was announced, I wasn't, I think I'm on Threads too, but it's not me.

461
00:43:00,200 --> 00:43:03,200
It's just, you know, just get another marketing channel.

462
00:43:04,200 --> 00:43:11,200
But yeah, I haven't, I feel such relief not exercising that muscle anymore.

463
00:43:11,200 --> 00:43:23,200
Where it's like, you know, I don't know how often I was checking Twitter, but it was, I was, you know, I was not checking it just to see what was happening to me or the response to my last thing I tweeted.

464
00:43:23,200 --> 00:43:26,200
I was checking it a lot because it was my newsfeed.

465
00:43:26,200 --> 00:43:29,200
It's like I'm following, you know, 200 smart people.

466
00:43:29,200 --> 00:43:31,200
They're telling me what they're paying attention to.

467
00:43:31,200 --> 00:43:32,200
And so I'm fascinated.

468
00:43:32,200 --> 00:43:35,200
So yeah, well, yeah, I want to see that next article or that next video.

469
00:43:35,200 --> 00:43:44,200
Just that engagement and the endless opportunity to comment and to put my foot in my mouth or put my foot in someone else's mouth or have someone put their foot.

470
00:43:44,200 --> 00:43:58,200
It's just not having that has been such relief that I would be, I mean, it's not impossible, but I would be very cautious in reactivating that because it was, it was so much noise.

471
00:43:58,200 --> 00:44:15,200
And again, it created, there's so much, it became a, it became an opportunity cost, but it became a just this endless opportunity for misunderstanding.

472
00:44:16,200 --> 00:44:22,200
Misunderstanding of me and, you know, everything I've been putting out into the world and then my sense that I had to react to it.

473
00:44:22,200 --> 00:44:33,200
And then you just kind of plow that back into the, you know, that becomes the basis for further misunderstanding.

474
00:44:33,200 --> 00:44:44,200
And it just constantly was giving me the sense that there's something, there's something I need to react to on my podcast, in an article, on Twitter, that it's just, this is a valid signal.

475
00:44:44,200 --> 00:44:47,200
Like this is, this is, this is like, this is a five alarm fire.

476
00:44:47,200 --> 00:44:48,200
This is like, you got to stop everything.

477
00:44:48,200 --> 00:44:53,200
Like you're by the pool on the one vacation you're taking with your family that summer.

478
00:44:53,200 --> 00:44:59,200
And this thing just happened on your phone that it can't wait, right?

479
00:44:59,200 --> 00:45:02,200
Like you actually have to pay attention because it's like the conversation is happening right now.

480
00:45:02,200 --> 00:45:20,200
And so it was a kind of addiction to information and, you know, at some level, reputation management or, or, or, and it was just, I mean, just to just be free of it is such a relief.

481
00:45:20,200 --> 00:45:34,200
And apart from like, you know, health issues with certain family members, virtually the only bad things that have happened to me have been a result of my engagement with Twitter over the last 10 years.

482
00:45:34,200 --> 00:45:43,200
So it's just, it's just, you know, I, you know, I guess I'm, if I'm a masochist, I would be back on Twitter, but like that would be the only reason to do it.

483
00:45:43,200 --> 00:45:44,200
Narrow AI.

484
00:45:44,200 --> 00:45:49,200
I asked you the question a second ago, which we, I really wanted to get a solution to it because I'm mildly terrified.

485
00:45:50,200 --> 00:46:00,200
I completely believe your, believe your the logic underneath your opinion that Narrow AI will cause this destabilization and unusability of the internet.

486
00:46:00,200 --> 00:46:13,200
So just focusing on Narrow AI, what would you consider to be a solution to prevent us getting to that world where misinformation is rife to the point that it can destabilize society, politics and culture?

487
00:46:14,200 --> 00:46:24,200
Well, I think it's something I've been asking people about on my podcast because it's not actually my wheelhouse and I would just need to hear from experts about what's possible technically here.

488
00:46:24,200 --> 00:46:42,200
But I'm imagining that paradoxically or ironically, this could usher in a new kind of gatekeeping that we're going to rely on because like the provenance of information is going to be so important.

489
00:46:42,200 --> 00:46:52,200
I mean, the assurance that a video has not been manipulated or there's not a, just a pure confection of deep fakery.

490
00:46:52,200 --> 00:47:11,200
Right, so you get, so it could be that we're meandering into a new period where you're not going to trust a photo unless it's coming from, you know, Getty images or, you know, the New York Times has some story about how they have verified every photo.

491
00:47:11,200 --> 00:47:14,200
In there that they put in their newspaper, they have a process.

492
00:47:14,200 --> 00:47:30,200
And, you know, so if you see a video of Vladimir Putin seeming to say that he's declaring war on the US, right, I think most people are going to assume that's fake until proven otherwise.

493
00:47:30,200 --> 00:47:34,200
It's like, it's just, it's just going to be too much fake stuff.

494
00:47:34,200 --> 00:47:47,200
And it's going to be, it's going to all going to look so good that the New York Times and every other, you know, organ of media that we have relied upon as imperfect as they've been of late.

495
00:47:47,200 --> 00:47:55,200
They're going to have to figure out what the tools are whereby they can say, OK, this is actually a video of Putin, right.

496
00:47:55,200 --> 00:47:58,200
And if the new, I mean, I'm not going to be able to figure it out on my own, right.

497
00:47:58,200 --> 00:48:06,200
The New York Times doesn't have a process or CNN doesn't have a process that they go through before they say, OK, Putin really said this.

498
00:48:06,200 --> 00:48:12,200
And so this is, we have to now react to this because this is real.

499
00:48:13,200 --> 00:48:20,200
Whatever that process is and whether it's whether there's some kind of digital watermark that, you know, that's connected to the blockchain.

500
00:48:20,200 --> 00:48:34,200
There's some tech implementation of it that can be fully democratized where you by just being in the latest version of the Chrome browser can know that you can differentiate real and fake videos.

501
00:48:34,200 --> 00:48:40,200
I don't know what the implementation will be, but I just know we're going to get to some spot where it's going to be.

502
00:48:40,200 --> 00:48:45,200
Right. We have to declare epistemological bankruptcy.

503
00:48:45,200 --> 00:48:47,200
We don't know what's real.

504
00:48:47,200 --> 00:48:54,200
We have to assume anything, especially lured or agitating is fake until proven otherwise.

505
00:48:54,200 --> 00:48:56,200
So prove otherwise.

506
00:48:56,200 --> 00:48:59,200
And that's, you know, that that'll be a resetting of something.

507
00:48:59,200 --> 00:49:09,200
I don't know what we do with that in a world where we really don't have that much time to react to certain things that are a video of Putin saying he's launched his big missiles.

508
00:49:11,200 --> 00:49:16,200
Is something that, you know, 30 minutes from now we would we would understand whether it's real or not.

509
00:49:16,200 --> 00:49:22,200
We forget about again, forget about everything we just said about AI.

510
00:49:22,200 --> 00:49:24,200
Look at all of our legacy risks.

511
00:49:24,200 --> 00:49:34,200
Look at the risk of nuclear war, the risk of stumbling into a nuclear war by accident has been hanging over our head for 70 years.

512
00:49:34,200 --> 00:49:36,200
I mean, we've got this old tech.

513
00:49:36,200 --> 00:49:40,200
We've got these wonky radar systems that throw up errors.

514
00:49:40,200 --> 00:49:57,200
We have moments in history where, you know, one Soviet sub commander decided based on his just gut feeling his common sense that the data was almost certainly an error.

515
00:49:57,200 --> 00:50:09,200
And he decided not to pass the obvious evidence of an American ICBM launch up the chain of command, knowing that the chain of command would say, OK, you have to fire.

516
00:50:09,200 --> 00:50:10,200
Right.

517
00:50:10,200 --> 00:50:19,200
And he reasoned that if the U.S. was going to attack the Soviet Union, they would launch more than I think in this case it looked like there were four missiles.

518
00:50:19,200 --> 00:50:21,200
That was the radar signature.

519
00:50:21,200 --> 00:50:32,200
If the U.S. is going to launch a first strike against the Soviet Union in this like the mid 80s, they're going to launch more than four missiles.

520
00:50:32,200 --> 00:50:34,200
This has to be this has to be bad data.

521
00:50:34,200 --> 00:50:35,200
Right.

522
00:50:35,200 --> 00:50:41,200
But, you know, so if we automate all this, will we automate it to systems that have that kind of common sense?

523
00:50:41,200 --> 00:50:42,200
Right.

524
00:50:43,200 --> 00:50:51,200
But we've been perched on the on the edge of the abyss based on this.

525
00:50:51,200 --> 00:50:56,200
The possible forget about malevolent actors, you know, who might decide to have a nuclear war on purpose.

526
00:50:56,200 --> 00:51:00,200
We have the possibility of accidental nuclear war.

527
00:51:00,200 --> 00:51:06,200
You add this cacophony of misinformation and deep fake to all of that.

528
00:51:06,200 --> 00:51:09,200
And it just gets scarier and scarier.

529
00:51:09,200 --> 00:51:11,200
And this is not even AI.

530
00:51:11,200 --> 00:51:16,200
This is just, you know, narrow AI amplified misinformation.

531
00:51:16,200 --> 00:51:18,200
How do you think about it?

532
00:51:18,200 --> 00:51:20,200
Well, this is the thing that worries me.

533
00:51:20,200 --> 00:51:22,200
I worry about the next election.

534
00:51:22,200 --> 00:51:35,200
I think the next president, if we can run the 2024 election in a way that most of America acknowledges was valid, that will be an amazing victory.

535
00:51:35,200 --> 00:51:41,200
You know, whatever the outcome, I mean, obviously, I would not be looking forward to a Trump presidency.

536
00:51:41,200 --> 00:51:54,200
But I think even more fundamental than that is, can we hold a presidential election 18 months from now that is that we recognize as valid?

537
00:51:54,200 --> 00:51:55,200
Right.

538
00:51:55,200 --> 00:51:56,200
Like that.

539
00:51:56,200 --> 00:51:57,200
I don't know.

540
00:51:57,200 --> 00:52:02,200
I don't know what kind of resources are being spent on on that particular performance.

541
00:52:02,200 --> 00:52:05,200
But that is hugely important.

542
00:52:05,200 --> 00:52:13,200
And I don't think our near term experiments with AI is going to make that easier.

543
00:52:13,200 --> 00:52:15,200
Why is it so important?

544
00:52:15,200 --> 00:52:27,200
Well, it's just, I mean, if you think the maintenance of a valid democracy in the world's lone superpower is of minor importance.

545
00:52:27,200 --> 00:52:31,200
I'd like to drink the tea you're drinking.

546
00:52:31,200 --> 00:52:32,200
Is that optimistic?

547
00:52:32,200 --> 00:52:35,200
I mean, I can't say I'm optimistic.

548
00:52:35,200 --> 00:52:40,200
You know, it's a paradoxical state, I mean, because I definitely have.

549
00:52:40,200 --> 00:52:44,200
I tend to focus on what's wrong or might be wrong.

550
00:52:44,200 --> 00:52:50,200
I tend to, I think, have a pessimistic bias, right?

551
00:52:50,200 --> 00:52:53,200
Like I tend to notice what's wrong as opposed to what's right.

552
00:52:53,200 --> 00:52:58,200
You know, that's my, that's my bias.

553
00:52:58,200 --> 00:53:02,200
But I'm actually very happy, right?

554
00:53:02,200 --> 00:53:04,200
Like I have a very, a very good life.

555
00:53:04,200 --> 00:53:08,200
I'm just like everything is just I'm incredibly lucky.

556
00:53:08,200 --> 00:53:09,200
I'm surrounded by great people.

557
00:53:09,200 --> 00:53:12,200
It's like it's all great.

558
00:53:12,200 --> 00:53:16,200
And yet I see all of these risks on the horizon.

559
00:53:16,200 --> 00:53:25,200
So I'm not, I just have a very high degree of well-being at this moment in my life.

560
00:53:25,200 --> 00:53:30,200
And yet what's on the television is scary.

561
00:53:30,200 --> 00:53:34,200
And so it's a very interesting juxtaposition.

562
00:53:34,200 --> 00:53:41,200
You know, I'll be, I'll be very relieved if we have a, I feel like we're in a very weird spot.

563
00:53:41,200 --> 00:53:49,200
I mean, like the, I haven't seen a full post-mortem on the COVID pandemic that has fully encapsulated

564
00:53:49,200 --> 00:53:53,200
what I think we, what I think happened to us there.

565
00:53:53,200 --> 00:53:59,200
But my vague sense is that we didn't learn a whole hell of a lot.

566
00:53:59,200 --> 00:54:04,200
I mean, basically what we learned is we're really bad at responding to this kind of thing.

567
00:54:04,200 --> 00:54:08,200
This was a challenge that, that just fragmented us as a society.

568
00:54:08,200 --> 00:54:10,200
It could have brought us together.

569
00:54:10,200 --> 00:54:12,200
It didn't.

570
00:54:12,200 --> 00:54:24,200
And it, it amplified all of the divisions in our society politically and economically and tribally and all kinds of ways.

571
00:54:24,200 --> 00:54:29,200
The role of misinformation and disinformation and all of that was, was all too clear.

572
00:54:29,200 --> 00:54:31,200
And I think just getting worse.

573
00:54:31,200 --> 00:54:40,200
So I think, you know, as a dress rehearsal for some future pandemic that's, that is inevitably going to come and is, you know, could well be worse.

574
00:54:40,200 --> 00:54:42,200
I think we failed this dress rehearsal.

575
00:54:42,200 --> 00:54:56,200
And, you know, I have to hope that at some point our institutions will reconstitute themselves so as to be obviously trustworthy and engender the kind of trust we actually need to have on our institutions.

576
00:54:56,200 --> 00:55:04,200
Like, we need a CDC, then not only that we trust, but that is trustworthy, that we, that we, that we're right to trust, right?

577
00:55:05,200 --> 00:55:10,200
And so it is with an FDA and every other, you know, institution that, that is relevant here.

578
00:55:10,200 --> 00:55:14,200
And we don't quite have that.

579
00:55:14,200 --> 00:55:17,200
And half of our society thinks we don't have that at all.

580
00:55:17,200 --> 00:55:18,200
Right.

581
00:55:18,200 --> 00:55:23,200
And so it's, we have to rebuild trust and institutions somehow.

582
00:55:23,200 --> 00:55:33,200
And I just think, you know, we have a lot of work to do to even figure out how to make an increment of progress on that score.

583
00:55:33,200 --> 00:55:46,200
Because we're, again, the siloing of large constituents into alternate information universes is just not functional.

584
00:55:46,200 --> 00:55:50,200
And that's so much of what social media has done to us and alternative media.

585
00:55:50,200 --> 00:55:55,200
I mean, like, you know, I call it, you know, you and I are podcasters, but I call it podcast to stand, right?

586
00:55:55,200 --> 00:56:11,200
I mean, we have this, this landscape of, I mean, there's now whatever million plus podcasts and there's, you know, email newsletters and everyone has now just decided to curate their information diet in a way that's just bespoke to them.

587
00:56:11,200 --> 00:56:24,200
And you can stay there forever and you're getting, you're getting one slice of, and it could be, you know, a completely fictional slice of, of reality.

588
00:56:24,200 --> 00:56:31,200
And we're losing the ability to converge on a common picture of what's going on.

589
00:56:31,200 --> 00:56:33,200
And you.

590
00:56:33,200 --> 00:56:35,200
So that's not optimistic.

591
00:56:35,200 --> 00:56:37,200
I didn't hear the optimism in there.

592
00:56:37,200 --> 00:56:38,200
You tell me.

593
00:56:38,200 --> 00:56:42,200
No, I, but I kind of can't refute anything you said on a like a logical basis.

594
00:56:42,200 --> 00:56:46,200
It all sounds like that is the direction of travel that we're going in.

595
00:56:46,200 --> 00:56:52,200
Unfortunately, I have faith that there'll be surprising positives.

596
00:56:52,200 --> 00:56:56,200
There always tends to be surprising positives that we also didn't factor in.

597
00:56:56,200 --> 00:56:59,200
Well, yeah, I mean, it's easy to see.

598
00:56:59,200 --> 00:57:12,200
I mean, if there's anything, if there's any significant low hanging fruit technologically or or scientifically that could be AI enabled for us.

599
00:57:12,200 --> 00:57:17,200
I mean, just take like, you know, a cure for cancer, a cure for Alzheimer's, right?

600
00:57:17,200 --> 00:57:25,200
I mean, just having one thing like that, right, that would be such an enormous good.

601
00:57:25,200 --> 00:57:29,200
And that is, that is, that's what, that's why we can't get off this ride.

602
00:57:29,200 --> 00:57:31,200
And that's why there is no break to pull.

603
00:57:31,200 --> 00:57:35,200
Because the value of intelligence is so enormous.

604
00:57:35,200 --> 00:57:38,200
I mean, it is, it is just, it's not everything.

605
00:57:38,200 --> 00:57:43,200
I mean, it's not, you know, there's other things we care about and a right to care about beyond intelligence.

606
00:57:43,200 --> 00:57:46,200
I mean, love is not the same thing as intelligence, right?

607
00:57:46,200 --> 00:57:51,200
But intelligence is the thing that can safeguard everything you love.

608
00:57:51,200 --> 00:58:02,200
I mean, even if you think the whole point in life is to just get on a beach with your friends and your family and just hang out and enjoy the sunset.

609
00:58:02,200 --> 00:58:06,200
Okay, you don't have to augment.

610
00:58:06,200 --> 00:58:09,200
You don't need superhuman intelligence to do any of that, right?

611
00:58:09,200 --> 00:58:12,200
You're fit to do it exactly as you are.

612
00:58:12,200 --> 00:58:17,200
You could have done that in the 70s and it would just be just as good a beach and it'd be just as good friends.

613
00:58:17,200 --> 00:58:26,200
But every gain we make in intelligence is the thing that safeguards that opportunity for you and everyone else.

614
00:58:26,200 --> 00:58:30,200
How would you, I feel like we've not defined the term artificial general intelligence.

615
00:58:30,200 --> 00:58:35,200
From my understanding of it, it's when the intelligence can think and make decisions almost like a human.

616
00:58:35,200 --> 00:58:40,200
Yeah, I mean, loosely, I mean, this, this is kind of just a semantic problem.

617
00:58:40,200 --> 00:59:00,200
But intelligence can mean many things, but, you know, loosely speaking, it is the ability to solve problems and meet goals, make decisions in response to a changing environment, in response to data.

618
00:59:01,200 --> 00:59:24,200
And the general aspect of that is an ability to do that in many different situations, all the sort of situations we encounter as people, and to have one's capacity in one area not, you know, as I get better at deciding whether or not this is a cup,

619
00:59:24,200 --> 00:59:30,200
I don't magically get worse at deciding whether, you know, you just said a word, right?

620
00:59:30,200 --> 00:59:35,200
It's like, I can do multiple things in multiple channels.

621
00:59:35,200 --> 00:59:42,200
That's not something we had in our artificial systems for the longest time because everything was bespoke to the task.

622
00:59:42,200 --> 00:59:45,200
We'd build a chess engine and it couldn't even play tic-tac-toe.

623
00:59:45,200 --> 00:59:53,200
All it could do is play chess and we just would get better and better in these piecemeal, narrow ways.

624
00:59:53,200 --> 01:00:07,200
And then things began to change a few years ago where you'd get, you know, like deep mind would have its algorithms that were, you know, the same algorithm with slightly different tuning could play go, right?

625
01:00:07,200 --> 01:00:13,200
Or it could, you know, it could solve a protein folding problem as opposed to just playing chess, right?

626
01:00:13,200 --> 01:00:16,200
And it became the best in the world at chess and it became the best in the world at go.

627
01:00:17,200 --> 01:00:36,200
And amazingly, I mean, to take, you know, what AlphaZero did, it, you know, before AlphaZero, all the chess algorithms were, they just had all of our chess knowledge plowed into them.

628
01:00:36,200 --> 01:00:42,200
They had studied every human game of chess and they just, it was just, you know, it was a bespoke chess engine.

629
01:00:42,200 --> 01:00:48,200
AlphaZero just played itself, I think for like four hours, right?

630
01:00:48,200 --> 01:00:58,200
It just, it just had the rules of chess and then it played itself and it became better not merely than every other, every person who's ever played the game.

631
01:00:58,200 --> 01:01:04,200
It became better than all the chess engines that had all of the, the, all of our chess knowledge plowed into them.

632
01:01:04,200 --> 01:01:14,200
So it's a fundamentally new moment in, in how you build an intelligent system and it promises this, this possibility.

633
01:01:14,200 --> 01:01:20,200
Again, this inevitability, the moment you admit that we will eventually get there.

634
01:01:20,200 --> 01:01:31,200
The moment, the moment you admit that it's, it can be done in silico and the moment that you admit that we will just keep going unless a catastrophe happens.

635
01:01:32,200 --> 01:01:38,200
And those two things are so easy to admit that I don't, at this point, I don't see any place to stand where you're not forced to admit them, right?

636
01:01:38,200 --> 01:01:50,200
I don't see any neuroscientific or cognitive scientific argument for substrate dependence for intelligence, given what we've already built.

637
01:01:50,200 --> 01:01:55,200
And again, we're, we're going to keep going until something stops us.

638
01:01:55,200 --> 01:02:02,200
We'll hit some immovable object that prevents us from releasing the next iPhone, but otherwise we're going to keep going.

639
01:02:02,200 --> 01:02:18,200
And then, yeah, so then it, then whatever general will mean in that first case, there'll be a case where we've built a system that is so good at everything we care about that is functionally general.

640
01:02:18,200 --> 01:02:23,200
Now, maybe it's missing something, maybe it's not, you know, maybe it's missing something that we don't even have a name for.

641
01:02:23,200 --> 01:02:30,200
You know, we're missing all kinds of, there are possible intelligences that we haven't even thought about because we just haven't thought about them.

642
01:02:30,200 --> 01:02:42,200
There are things that, there are ways to section the universe undoubtedly that we can't even conceive of because we are just, we have the minds we have.

643
01:02:42,200 --> 01:02:44,200
Elon was asked a question on this by a journalist.

644
01:02:44,200 --> 01:02:50,200
The journalist said to him, in a world where you believe that to be true, that artificial general intelligence is around the corner.

645
01:02:50,200 --> 01:02:57,200
When your kids come to you and say, Daddy, what should I do with my life to find purpose and meaning?

646
01:02:57,200 --> 01:03:01,200
What advice do you now give them if you hold that intuition to be true?

647
01:03:01,200 --> 01:03:03,200
That it's around the corner.

648
01:03:03,200 --> 01:03:08,200
What do you say to your children when they say, what should I do with my life to create purpose and meaning?

649
01:03:08,200 --> 01:03:10,200
Did you say that Elon answered this question?

650
01:03:10,200 --> 01:03:11,200
Yeah.

651
01:03:11,200 --> 01:03:12,200
What did he say?

652
01:03:12,200 --> 01:03:18,200
It's one of the most chilling moments in an interview I think I've seen in recent times because he stutters.

653
01:03:18,200 --> 01:03:21,200
He goes silent for about 15 seconds, which is very un-Elon.

654
01:03:21,200 --> 01:03:22,200
He stutters.

655
01:03:22,200 --> 01:03:24,200
He stutters.

656
01:03:24,200 --> 01:03:26,200
He stutters a bit more.

657
01:03:26,200 --> 01:03:37,200
And then he says, he thinks he's living in suspended disbelief because if he really thought about it too much, what's the point?

658
01:03:37,200 --> 01:03:39,200
He says, what's the point of me building all these cars?

659
01:03:39,200 --> 01:03:40,200
He was in his Tesla factory.

660
01:03:40,200 --> 01:03:42,200
What's the point of me building all these cars and what's the point?

661
01:03:42,200 --> 01:03:43,200
I do think that sometimes.

662
01:03:43,200 --> 01:03:47,200
So I think I have to live in, as his words were, suspended disbelief.

663
01:03:47,200 --> 01:03:54,200
I would encourage him to ask what's the point of spending so much time on Twitter because he could clearly benefit from rethinking that.

664
01:03:54,200 --> 01:04:07,200
But that aside, my answer to that is, and I think other people have echoed this of late, sort of surprising to me.

665
01:04:07,200 --> 01:04:21,200
My answer is that this begins to privilege a return to the humanities as a kind of a core, like the center of mass intellectually for us.

666
01:04:21,200 --> 01:04:35,200
Because when you look at what we're really good at and it's among the last things that can be plausibly automated.

667
01:04:35,200 --> 01:04:39,200
And if we automate it, we may cease to care about it.

668
01:04:39,200 --> 01:04:46,200
So it's like learning to write good code is something that is being automated now.

669
01:04:46,200 --> 01:05:00,200
I'm not a programmer, but I have it on good authority that already these large language models are improving code and something like half the time they're writing better code than people.

670
01:05:00,200 --> 01:05:02,200
That's all going to become like chess, right?

671
01:05:02,200 --> 01:05:07,200
It's just it's going to be better than people ultimately.

672
01:05:07,200 --> 01:05:14,200
So being a software engineer is something that, you know, and being a radiologist and being like those things.

673
01:05:14,200 --> 01:05:24,200
It's easy to see how AI just cancels those professions or at least makes one person, you know, so effective at using AI tools that one person can do the work of 100 people.

674
01:05:24,200 --> 01:05:31,200
So you got 99 people who don't have to be doing that job.

675
01:05:31,200 --> 01:05:47,200
But creating art and, you know, writing novels and being a philosopher and talking about what it means to live a good life and how to do it.

676
01:05:47,200 --> 01:06:08,200
That's something that if we have to look at where we're going to care that we're actually in relationship to and in dialogue with another person who we know to be conscious.

677
01:06:08,200 --> 01:06:14,200
Where we don't care about that, we're not going to care, we're going to want just the best version of it.

678
01:06:14,200 --> 01:06:20,200
You know, if the cure for cancer comes from an AI, an incentive in AI, I do not give a shit.

679
01:06:20,200 --> 01:06:22,200
I just want the cure for cancer, right?

680
01:06:22,200 --> 01:06:30,200
Like there's no added value that where I find out, okay, the person who gave me this cure really felt good about it.

681
01:06:30,200 --> 01:06:33,200
And he's, you know, he had tears in his eyes when he figured out the cure.

682
01:06:33,200 --> 01:06:35,200
Every engineering problem is like that.

683
01:06:35,200 --> 01:06:37,200
We want safer planes.

684
01:06:37,200 --> 01:06:39,200
We want, you know, we just want things to work.

685
01:06:39,200 --> 01:06:46,200
We're not sentimental about the artistry that went into all of that.

686
01:06:46,200 --> 01:06:54,200
And when the difference, when the gulf between the best and the mediocre gets big and consequential, we're just going to want the best.

687
01:06:54,200 --> 01:06:56,200
We're just going to want the best all the way down the line.

688
01:06:56,200 --> 01:07:00,200
But what is the best novel, right?

689
01:07:00,200 --> 01:07:04,200
What is the best podcast conversation?

690
01:07:04,200 --> 01:07:12,200
And can you subtract out the conscious person from that and still think it's the best?

691
01:07:12,200 --> 01:07:20,200
And so like someone once sent me what purported to be, I didn't even listen to it, so I'm not even sure what it was.

692
01:07:20,200 --> 01:07:26,200
But it looked like it was an AI-generated conversation between Alan Watts and Terrence McKenna, right?

693
01:07:26,200 --> 01:07:30,200
Both guys who I love, I remember, I didn't know either of them.

694
01:07:30,200 --> 01:07:35,200
But some fans of both have listened to hundreds of hours of both talk.

695
01:07:35,200 --> 01:07:37,200
As far as I know, they never met each other.

696
01:07:37,200 --> 01:07:39,200
It would have been a fascinating conversation.

697
01:07:39,200 --> 01:07:46,200
I realized when I looked at this YouTube video, I realized I simply don't care how good this is.

698
01:07:46,200 --> 01:07:52,200
Because I only care if it was actually Alan Watts and Terrence McKenna talking.

699
01:07:52,200 --> 01:07:58,200
I got a simulacrum of Alan Watts and Terrence McKenna in this context I don't care about.

700
01:07:58,200 --> 01:08:08,200
So in another use case, I stumbled upon, I was playing with ChatGPT and I asked the causes of World War II.

701
01:08:08,200 --> 01:08:11,200
Give me 500 words on the cause of World War II.

702
01:08:11,200 --> 01:08:16,200
It gives you this perfect little bullet-pointed essay on the cause of World War II.

703
01:08:16,200 --> 01:08:19,200
That's exactly what I want from it.

704
01:08:19,200 --> 01:08:20,200
That's fine.

705
01:08:20,200 --> 01:08:24,200
I don't care that there was no person behind that typing.

706
01:08:24,200 --> 01:08:34,200
But when I think, well, do I want to read Churchill's history of World War II?

707
01:08:34,200 --> 01:08:35,200
It's on my shelf to read.

708
01:08:35,200 --> 01:08:39,200
It's one of these aspirational sets of books.

709
01:08:39,200 --> 01:08:40,200
I haven't read it yet.

710
01:08:40,200 --> 01:08:44,200
I actually want to read it because Churchill wrote it.

711
01:08:44,200 --> 01:08:50,200
And if you could give me an AI version of Churchill saying this is in the style of Churchill,

712
01:08:51,200 --> 01:08:54,200
even Churchill scholars say this sounds like Churchill.

713
01:08:54,200 --> 01:08:56,200
I actually don't care about it.

714
01:08:56,200 --> 01:08:57,200
That's not the use.

715
01:08:57,200 --> 01:09:03,200
I'll take the generic use of give me the cause of World War II.

716
01:09:03,200 --> 01:09:06,200
The fake Churchill is profoundly uninteresting to me.

717
01:09:06,200 --> 01:09:11,200
The real Churchill, even though he's dead, is interesting to me.

718
01:09:11,200 --> 01:09:18,200
So the rebuttal I give here, and this is what my mind is doing, is saying the distinction you're presenting,

719
01:09:18,200 --> 01:09:21,200
the difference I see is that in the case of the conversation between two people,

720
01:09:21,200 --> 01:09:26,200
your respect that has been generated by AI, someone has signaled to you that it is fake.

721
01:09:26,200 --> 01:09:32,200
If you remove that because say Churchill thought, why would I write a book when I could just click a button

722
01:09:32,200 --> 01:09:40,200
and this thing will write it in my voice, in my tone of voice, with the entire back catalogue of things I've written before.

723
01:09:40,200 --> 01:09:44,200
And it will produce my account and it will save me time.

724
01:09:44,200 --> 01:09:47,200
So I'll just click a button, my publisher maybe will do it for me.

725
01:09:47,200 --> 01:09:55,200
And then I'll sell that to Sam on the basis that it is my thoughts, which I can imagine a very near future.

726
01:09:55,200 --> 01:10:00,200
If we just do it by percentage, how many books are going to be increasingly written by artificial intelligence?

727
01:10:00,200 --> 01:10:04,200
To the point that when you look at a shelf, I imagine at some point in the future,

728
01:10:04,200 --> 01:10:08,200
if the intelligence does increase by any measure,

729
01:10:08,200 --> 01:10:14,200
that most of it would be words strung together by artificial intelligence

730
01:10:14,200 --> 01:10:18,200
and it will be selling potentially better than the words written by humans.

731
01:10:18,200 --> 01:10:24,200
So again, when we go back to the conversation with your children, there might not be a career there either

732
01:10:24,200 --> 01:10:30,200
because artificial intelligence is faster, can produce more, can test and iterate on whether it sells better,

733
01:10:30,200 --> 01:10:36,200
clicks gets more clicks, it can write the headline, create the picture, write the content.

734
01:10:36,200 --> 01:10:40,200
And then I can just take the check because I put my name to it.

735
01:10:40,200 --> 01:10:44,200
So even in that regard, what remains?

736
01:10:44,200 --> 01:10:51,200
Well, so in the limit, what I think we're imagining is a world where,

737
01:10:51,200 --> 01:10:56,200
and so none of the terrifyingly bad things have happened, so it's just all working.

738
01:10:56,200 --> 01:11:02,200
We're just producing a ton of great stuff that is better than the human stuff and people are losing their jobs.

739
01:11:02,200 --> 01:11:08,200
We've got a labor disruption, but we're not talking about any other kind of political catastrophe

740
01:11:08,200 --> 01:11:18,200
or cyber apocalypse, much less AGI destroying everything.

741
01:11:18,200 --> 01:11:26,200
Then I think we just need a different economic assumption and ethical intuition around the value of work.

742
01:11:26,200 --> 01:11:36,200
Our default norm now in a capitalist society is you have to figure out something to do with most of your time

743
01:11:36,200 --> 01:11:39,200
that other people are willing to pay you for.

744
01:11:39,200 --> 01:11:45,200
You have to figure out how to add value to other people's lives such that you reliably get paid.

745
01:11:45,200 --> 01:11:48,200
Otherwise, you might die.

746
01:11:48,200 --> 01:11:53,200
We've got a social safety net, but it's pretty meager.

747
01:11:54,200 --> 01:11:56,200
There are cracks you can fall through.

748
01:11:56,200 --> 01:12:05,200
You can wind up homeless, and we're not going to figure out what to do about that all too well.

749
01:12:05,200 --> 01:12:15,200
Your claim upon your existence among us is you finding something to do with your time that other people will pay you for.

750
01:12:16,200 --> 01:12:23,200
Now we've got artificial intelligence removing some of those opportunities, creating others, but in the limit,

751
01:12:23,200 --> 01:12:32,200
and I do think it is different, I think analogies to other moments in technological history are fundamentally flawed,

752
01:12:32,200 --> 01:12:42,200
I think this is a technology which in the limit will replace jobs and not create better new jobs in their wake.

753
01:12:42,200 --> 01:12:48,200
This just cancels the need for human labor ultimately.

754
01:12:48,200 --> 01:12:55,200
Strangely, it replaces some of the highest status most cognitively intensive jobs first.

755
01:12:55,200 --> 01:13:05,200
It replaces Elon Musk before it replaces your electrician or your plumber or your masseuse way before.

756
01:13:05,200 --> 01:13:09,200
We have to internalize the reality of that.

757
01:13:09,200 --> 01:13:15,200
Again, this is in success. This is all good things happening.

758
01:13:15,200 --> 01:13:17,200
We have to have a new ethic.

759
01:13:17,200 --> 01:13:25,200
We have to have a new economics based on that ethic, which is UBI is one solution to this.

760
01:13:25,200 --> 01:13:27,200
You shouldn't have to work to survive.

761
01:13:27,200 --> 01:13:29,200
Universal basic income.

762
01:13:29,200 --> 01:13:32,200
There's so much abundance now being created.

763
01:13:32,200 --> 01:13:35,200
We have to figure out how to spread this wealth around.

764
01:13:35,200 --> 01:13:37,200
We've got a cure for cancer over here.

765
01:13:37,200 --> 01:13:45,200
We've got perfect photovoltaic driven economies over here.

766
01:13:45,200 --> 01:13:48,200
We've solved the climate change issue.

767
01:13:48,200 --> 01:13:53,200
We're just pulling wealth out of the ether, essentially.

768
01:13:53,200 --> 01:14:00,200
We've got nanotechnology that is just birthing whole new industries yet, but it's all being driven by AI.

769
01:14:00,200 --> 01:14:02,200
There's no room in this.

770
01:14:02,200 --> 01:14:10,200
Whenever you put a person in the decision chain, you're just adding noise.

771
01:14:10,200 --> 01:14:14,200
This should be the best thing that's ever happened to us.

772
01:14:14,200 --> 01:14:19,200
This is just like God handing us the perfect labor saving device.

773
01:14:19,200 --> 01:14:24,200
The machine that can build every other machine that can do anything you could possibly want.

774
01:14:24,200 --> 01:14:28,200
We should figure out how to spread the wealth around in that case.

775
01:14:28,200 --> 01:14:34,200
This is just powered by sunlight, no more wars over resource extraction.

776
01:14:34,200 --> 01:14:37,200
It can build anything.

777
01:14:37,200 --> 01:14:41,200
We can all be on the beach just hanging out with our friends and family.

778
01:14:41,200 --> 01:14:46,200
Did you believe we should do universal basic income where everybody's given a monthly check?

779
01:14:46,200 --> 01:14:48,200
We have to break this connection.

780
01:14:48,200 --> 01:14:56,200
Again, this is what will have to happen in the presence of this kind of labor force dislocation

781
01:14:56,200 --> 01:15:00,200
enabled by all of this going perfectly well.

782
01:15:00,200 --> 01:15:02,200
This is pure success.

783
01:15:02,200 --> 01:15:04,200
AI is just producing good things.

784
01:15:04,200 --> 01:15:08,200
The only bad thing is putting all these people out of work.

785
01:15:08,200 --> 01:15:10,200
It's coming for your job eventually.

786
01:15:10,200 --> 01:15:11,200
I've heard this.

787
01:15:11,200 --> 01:15:15,200
My issue with it and my rebuttal when I talked to my friends about this idea of universal basic income

788
01:15:15,200 --> 01:15:19,200
where we hand out enough cash or resources to people so that they're stable,

789
01:15:19,200 --> 01:15:23,200
which I'm not necessarily against but just want to play with it a little bit,

790
01:15:23,200 --> 01:15:28,200
is humans seem to have an innate desire for purpose and meaning

791
01:15:28,200 --> 01:15:34,200
and we seem to be designed and built psychologically for labor and for discomfort.

792
01:15:34,200 --> 01:15:39,200
But it doesn't have to be labor that's tied to money.

793
01:15:39,200 --> 01:15:44,200
We will get our status in other ways and we'll get our meaning in other ways.

794
01:15:44,200 --> 01:15:47,200
These are all just stories we tell ourselves.

795
01:15:47,200 --> 01:15:53,200
You're talking to a person who knows it's possible to be happy actually doing nothing,

796
01:15:53,200 --> 01:15:57,200
like just sitting in a room for a month and just staring at the wall.

797
01:15:57,200 --> 01:15:58,200
Because you've done it.

798
01:15:58,200 --> 01:16:00,200
Like that's possible.

799
01:16:00,200 --> 01:16:03,200
Yet that's most people's worst nightmare.

800
01:16:03,200 --> 01:16:06,200
It's solitary confinement in a prison is considered a torture.

801
01:16:06,200 --> 01:16:09,200
I know people who spent 20 years in a cave.

802
01:16:09,200 --> 01:16:13,200
There are capacities here that are worth talking about.

803
01:16:13,200 --> 01:16:22,200
But just more commonly, I think we want to be entertained.

804
01:16:22,200 --> 01:16:24,200
We want to have fun.

805
01:16:24,200 --> 01:16:26,200
We want to be with the people we love.

806
01:16:26,200 --> 01:16:31,200
We want to be useful in relationship.

807
01:16:31,200 --> 01:16:41,200
And in so far as that gets uncoupled from the necessity of working to survive.

808
01:16:41,200 --> 01:16:42,200
It doesn't all just go away.

809
01:16:42,200 --> 01:16:50,200
We just need new norms and new ethics and new conversations around what we do on vacation.

810
01:16:50,200 --> 01:16:55,200
What you're imagining is that if you put everyone on vacation, on the best vacation,

811
01:16:55,200 --> 01:17:02,200
you can make the vacation as good as possible, a majority of people will eventually be miserable

812
01:17:02,200 --> 01:17:06,200
because they're not back at work.

813
01:17:06,200 --> 01:17:09,200
And yet most of these people are working so that they have enough money

814
01:17:09,200 --> 01:17:11,200
so that they could finally take that vacation.

815
01:17:11,200 --> 01:17:15,200
We will figure out a new way to be happy on the beach.

816
01:17:15,200 --> 01:17:21,200
If you get bored with Frisbee, we will figure something else out that is fun.

817
01:17:21,200 --> 01:17:27,200
I'll be able to read the Churchill history of World War II on the beach

818
01:17:27,200 --> 01:17:33,200
and not be rushed by any other imperative because I'm happily retired

819
01:17:33,200 --> 01:17:40,200
because my AI is creating the thing that is solving all my economic problems.

820
01:17:40,200 --> 01:17:44,200
We should be so lucky as to have that be our problem.

821
01:17:44,200 --> 01:17:50,200
How to be happy in conditions of no economic imperative,

822
01:17:50,200 --> 01:17:55,200
no basis for political strife on the basis of scarce resources

823
01:17:55,200 --> 01:18:08,200
and the question of survival is off the table with respect to what one does with one's time and attention.

824
01:18:08,200 --> 01:18:11,200
You can be as lazy as you want and you'll still survive.

825
01:18:11,200 --> 01:18:16,200
You can be as unlucky as you want and you'll still survive.

826
01:18:16,200 --> 01:18:23,200
The awful situation we're in now is that differences in luck mean everything.

827
01:18:23,200 --> 01:18:30,200
Someone is born without any of the advantages that we have.

828
01:18:30,200 --> 01:18:35,200
We don't have an economic system that reliably gives them

829
01:18:35,200 --> 01:18:39,200
every advantage and opportunity they could have.

830
01:18:39,200 --> 01:18:48,200
We've convinced ourselves we either don't have the resources

831
01:18:48,200 --> 01:18:50,200
or we've convinced ourselves we don't have the resources.

832
01:18:50,200 --> 01:18:53,200
We don't have the incentive such that we access the resources

833
01:18:53,200 --> 01:18:59,200
so as to actually come to the help of people we could help.

834
01:18:59,200 --> 01:19:04,200
The idea that people starve to death is just unimaginable and yet it still happens.

835
01:19:04,200 --> 01:19:09,200
That's not a scarcity problem, it's a political problem wherever it happens.

836
01:19:09,200 --> 01:19:14,200
Yet all of this is tied to a system where everyone has convinced themselves

837
01:19:14,200 --> 01:19:24,200
that it's normal to really have one's survival be in question if one doesn't work.

838
01:19:25,200 --> 01:19:33,200
By choice or by accident, I think it's still true that at least in the U.S.

839
01:19:33,200 --> 01:19:35,200
this is almost certainly not true in the U.K.

840
01:19:35,200 --> 01:19:43,200
but in the U.S. the most common reason for a personal bankruptcy is overwhelming medical expense

841
01:19:43,200 --> 01:19:45,200
that just comes upon you for whatever reason.

842
01:19:45,200 --> 01:19:51,200
Your wife gets cancer, you guys go bankrupt solving the cancer problem

843
01:19:51,200 --> 01:19:56,200
failing to solve the cancer problem and now everything else unravels.

844
01:19:56,200 --> 01:20:02,200
We have a society which thinks, yeah, well, I'm lucky you.

845
01:20:02,200 --> 01:20:05,200
If you wind up homeless just don't sleep in front of my store

846
01:20:05,200 --> 01:20:09,200
because you're going to hurt my business.

847
01:20:09,200 --> 01:20:19,200
Successful AI that cancels lots of jobs would only be cancelling those jobs

848
01:20:19,200 --> 01:20:24,200
by virtue of producing so many good things, so much value for everybody

849
01:20:24,200 --> 01:20:28,200
that we would have to figure out how to spread that wealth around.

850
01:20:28,200 --> 01:20:39,200
Otherwise we would have an amazingly dystopian bottleneck for a few short years

851
01:20:39,200 --> 01:20:41,200
and then we would just have a revolution.

852
01:20:41,200 --> 01:20:47,200
Then the guys in their integrated communities making trillions of dollars

853
01:20:47,200 --> 01:20:57,200
based on them having gotten close enough to the GPUs that some of it rubbed off on them.

854
01:20:57,200 --> 01:21:01,200
Yeah, they'd be dragged out of their houses and off their Gulf Streams

855
01:21:01,200 --> 01:21:07,200
and we would have a fundamental reset, we'd have a hard reset of the political system.

856
01:21:07,200 --> 01:21:12,200
If I had to put you in a yes or no situation and ask your intuition the question now

857
01:21:12,200 --> 01:21:18,200
that if your objective was to, which I'm sure it is, is to encourage the betterment of humanity

858
01:21:18,200 --> 01:21:22,200
and to increase our odds of happiness and well-being 100 years from now

859
01:21:22,200 --> 01:21:25,200
and there was a button placed in front of you

860
01:21:25,200 --> 01:21:31,200
and it would either end the development of artificial intelligence as we've seen it over the last decade

861
01:21:31,200 --> 01:21:37,200
so we'd never proceed with developing intelligent machines or not.

862
01:21:37,200 --> 01:21:41,200
So you could press a button and stop it right now.

863
01:21:41,200 --> 01:21:44,200
And stop it permanently such that we never then do that thing?

864
01:21:44,200 --> 01:21:49,200
We just never figure out how to build intelligent machines?

865
01:21:49,200 --> 01:21:53,200
Pause it indefinitely.

866
01:21:53,200 --> 01:22:02,200
Well, I would definitely pause it to a point where we would get our heads around the alignment problem.

867
01:22:02,200 --> 01:22:08,200
Permanently. If the button was a permanent pause that you couldn't undo.

868
01:22:08,200 --> 01:22:11,200
Well, the question is how deep does that go?

869
01:22:11,200 --> 01:22:14,200
We have everything we have now but we just never get better than now.

870
01:22:14,200 --> 01:22:17,200
Yeah, we never make progress from here.

871
01:22:17,200 --> 01:22:21,200
And your objective is to make humanity happy and prosperous?

872
01:22:21,200 --> 01:22:32,200
It's hard because when you begin imagining all of the good stuff that we could get with aligned superhuman AI

873
01:22:32,200 --> 01:22:37,200
then it's just cornucopia upon cornucopia.

874
01:22:37,200 --> 01:22:41,200
It's just everything is potentially within reach.

875
01:22:41,200 --> 01:22:49,200
Yeah, I mean I take the existential risk scenario seriously enough that I would pause it.

876
01:22:49,200 --> 01:22:53,200
I think we will eventually get to it.

877
01:22:53,200 --> 01:23:00,200
If curing cancer is a biomedical engineering problem that admits of a solution

878
01:23:00,200 --> 01:23:04,200
I think there's every reason to believe it ultimately would be.

879
01:23:04,200 --> 01:23:10,200
We will eventually get there based on our own muddling along with our current level of tech.

880
01:23:10,200 --> 01:23:13,200
Currently information tech.

881
01:23:13,200 --> 01:23:20,200
I'm reasonably confident of that.

882
01:23:21,200 --> 01:23:32,200
Our intelligence shows every sign of being general is just it's not as fast as we would want it to be.

883
01:23:32,200 --> 01:23:43,200
The thing that AI is going to give us is it's going to give us speed that is...

884
01:23:43,200 --> 01:23:46,200
There's speed and then there's the access, there's memory.

885
01:23:46,200 --> 01:23:56,200
We can't integrate, we don't have the ability, no person or team of people can integrate all of the data we already have.

886
01:23:56,200 --> 01:24:05,200
The real promise here is that these systems will be able to find patterns that we wouldn't even know how to look for

887
01:24:05,200 --> 01:24:08,200
and then do something on the basis of those patterns.

888
01:24:08,200 --> 01:24:19,200
I think an intelligent search within the data space by apes like ourselves will eventually do most of the great things we want done.

889
01:24:19,200 --> 01:24:37,200
The problems we need to solve so as to safeguard the career of our species

890
01:24:37,200 --> 01:24:47,200
to make civilization durable and sane and to remove this sort of Damocles that is over our heads at every moment

891
01:24:47,200 --> 01:24:52,200
that at any moment we could just decide to have a nuclear war that ruins everything

892
01:24:52,200 --> 01:24:58,200
or create an engineered pandemic that ruins everything.

893
01:24:58,200 --> 01:25:01,200
We don't need superhuman intelligence to solve all those problems.

894
01:25:01,200 --> 01:25:08,200
We need an appropriate emotional response to the untenability of the status quo

895
01:25:08,200 --> 01:25:15,200
and we need a political dialogue that eventually transcends our tribalism.

896
01:25:15,200 --> 01:25:19,200
For those of you that don't know, this podcast is sponsored by Woop, a company that I'm a shareholder in

897
01:25:19,200 --> 01:25:22,200
and I'm obsessed with my Woop, it's glued to my wrist 24x7

898
01:25:22,200 --> 01:25:29,200
and for those of you that don't know, it's essentially a personalized wearable health and fitness coach that helps me to have the best possible health.

899
01:25:29,200 --> 01:25:31,200
My Woop has literally changed my life.

900
01:25:31,200 --> 01:25:34,200
Woop is doing something this month which I'd highly suggest checking out.

901
01:25:34,200 --> 01:25:38,200
It's a global community challenge called the Core 4 Challenge.

902
01:25:38,200 --> 01:25:44,200
Essentially, they guide you through a set of four activities throughout the month of August that are scientifically proven to improve your overall health.

903
01:25:44,200 --> 01:25:49,200
I'm giving it a go and I can't wait to see the impact it has on me and I highly recommend you to join me with that.

904
01:25:49,200 --> 01:25:52,200
So if you're not on Woop here, there is no better time to start.

905
01:25:52,200 --> 01:25:56,200
If you're a friend of mine, there's a high probability that I've already given you a Woop because I'm that obsessed with it.

906
01:25:56,200 --> 01:25:58,200
It is the thing that I check when I wake up in the morning.

907
01:25:58,200 --> 01:25:59,200
It's the first thing that I look at.

908
01:25:59,200 --> 01:26:02,200
I want the information on my sleep to then plan my day around.

909
01:26:02,200 --> 01:26:12,200
So if you haven't joined Woop yet, head to join.woop.com.co to get your free Woop device and your first month free.

910
01:26:12,200 --> 01:26:16,200
Try it for free and if you don't like it after 29 days, they're going to give you your money back.

911
01:26:16,200 --> 01:26:18,200
But I have a suspicion that you're going to keep it.

912
01:26:18,200 --> 01:26:20,200
Check it out now and let me know how you get on.

913
01:26:20,200 --> 01:26:21,200
Send me a DM.

914
01:26:21,200 --> 01:26:22,200
Quick one.

915
01:26:22,200 --> 01:26:27,200
If you've been listening to this podcast for some time, one of the recurring messages you've heard over and over and over again,

916
01:26:27,200 --> 01:26:34,200
especially when we first had that conversation with Tim Spector, is about the importance of greens in our diet.

917
01:26:34,200 --> 01:26:41,200
And a while ago, I started pressing my friends at Hewlett to come out with a product that did exactly that.

918
01:26:41,200 --> 01:26:45,200
Allowed you to have all those greens, the vitamins and minerals you need in a drink.

919
01:26:45,200 --> 01:26:52,200
And after several, several, several months of iterations and processes, they released this product called Hewlett Daily Greens,

920
01:26:52,200 --> 01:26:59,200
which is now one of my favorite products from Hewlett because it tastes great and it fills that very important nutritional gap that I had in my diet.

921
01:26:59,200 --> 01:27:08,200
The problem is, it launched in the US and it sold out straight away and became a smash hit for Hewlett for the very reasons I've described.

922
01:27:08,200 --> 01:27:12,200
It's now back in stock in the United States, but it's not here in the UK yet.

923
01:27:12,200 --> 01:27:16,200
So if you're a UK listener, which I know a lot of you are, it's not yet available.

924
01:27:16,200 --> 01:27:19,200
So let's all attack Hewlett.

925
01:27:19,200 --> 01:27:24,200
Let's DM them everywhere we can and tell them to bring Hewlett Daily Greens to the UK.

926
01:27:24,200 --> 01:27:26,200
This is the product.

927
01:27:26,200 --> 01:27:31,200
When it is available in the UK, I'm going to let you know first, but until then, let's spam their DMs.

928
01:27:31,200 --> 01:27:39,200
You and, I'd say a few others, maybe two or three others, helped change my mind about one of the most profound things I think anyone could believe,

929
01:27:39,200 --> 01:27:46,200
which was when I was 18, I believed in Christianity and then there was a couple of moments that shook my belief.

930
01:27:46,200 --> 01:27:58,200
Nothing on a personal level, just a couple of ideas that managed to sort of infect my operating system that led my curiosity towards your work.

931
01:27:58,200 --> 01:28:00,200
And I changed my mind profoundly.

932
01:28:00,200 --> 01:28:03,200
It's such a profound change that I had.

933
01:28:03,200 --> 01:28:05,200
How do we change our minds?

934
01:28:05,200 --> 01:28:12,200
And I really want to focus that question on the individual's mind.

935
01:28:12,200 --> 01:28:14,200
I want to change my mind.

936
01:28:14,200 --> 01:28:23,200
I want better beliefs, better ideas in my head that are going to allow me to get out of my own way because I'm not a cheat.

937
01:28:23,200 --> 01:28:24,200
I'm miserable.

938
01:28:24,200 --> 01:28:27,200
I'm not living the life that I...

939
01:28:27,200 --> 01:28:31,200
I would say I know I can live, but some people don't even know they can live a better life.

940
01:28:31,200 --> 01:28:32,200
I'm not happy.

941
01:28:32,200 --> 01:28:34,200
That's the signal.

942
01:28:34,200 --> 01:28:37,200
And I want to rectify this in some way.

943
01:28:37,200 --> 01:28:38,200
Yeah.

944
01:28:38,200 --> 01:28:42,200
Well, there are a few right lines for me.

945
01:28:42,200 --> 01:28:49,200
We take our ethical lives and our relationships to other people.

946
01:28:49,200 --> 01:28:57,200
There's the problem of individual well-being that's still real even if you're in a moral solitude.

947
01:28:57,200 --> 01:29:02,200
If you're on a desert island by yourself, you really don't have ethical questions that are emerging

948
01:29:02,200 --> 01:29:06,200
because you're not in a relationship with anybody else, but you still have the problem of how to be happy.

949
01:29:06,200 --> 01:29:11,200
But so much of our unhappiness is in collaboration with others.

950
01:29:11,200 --> 01:29:13,200
We're unhappy in our relationships.

951
01:29:13,200 --> 01:29:17,200
We're unhappy professionally.

952
01:29:17,200 --> 01:29:22,200
And it's worth looking at how we're behaving with other people.

953
01:29:22,200 --> 01:29:35,200
For me, the highest leverage change I ever made, and it's, again, it's very easy to spell out and it's very clear,

954
01:29:35,200 --> 01:29:43,200
and ultimately it's pretty easy, is just to decide that you're not going to lie about anything, really.

955
01:29:43,200 --> 01:29:48,200
I mean, there might be some situations in extremis where you'll feel forced to lie,

956
01:29:48,200 --> 01:29:58,200
but those, in my view, are analogous to acts of violence that you may be forced to use in self-defense.

957
01:29:58,200 --> 01:30:02,200
A line is sort of the first stage on the continuum of violence for me.

958
01:30:02,200 --> 01:30:09,200
I'm not going to lie to someone unless I recognize that this is not a rational actor who I can possibly collaborate with.

959
01:30:09,200 --> 01:30:19,200
This is someone I have to avoid or defeat or otherwise contain their propensity to do me harm.

960
01:30:19,200 --> 01:30:25,200
So, yes, if the Nazis come to the door and ask if you've got Anne Frank and the Attic, yes, you can lie,

961
01:30:25,200 --> 01:30:30,200
or you can shoot them, these are not normal circumstances.

962
01:30:31,200 --> 01:30:45,200
But that aside, every other moment in life where people are tempted to lie is one that I think you can categorically rule out as being unethical

963
01:30:45,200 --> 01:30:58,200
and beyond unethical, it's just not, it's creating a life you don't, when you examine it, you don't want to live.

964
01:30:58,200 --> 01:31:05,200
The moment you know that you're not going to lie to people and they know that about you,

965
01:31:05,200 --> 01:31:13,200
it's like all of the social dials get recalibrated on both sides,

966
01:31:13,200 --> 01:31:24,200
and then you find yourself in the presence of people who don't ask you for your opinion unless they really want it.

967
01:31:25,200 --> 01:31:35,200
And then when you're honest, then it's a night and day difference when you're giving people feedback, critical feedback,

968
01:31:35,200 --> 01:31:45,200
and they know you're honest, they know their bullshit detector is not going off because they just know you're,

969
01:31:45,200 --> 01:31:53,200
even when it's not convenient, you're being honest, or even when it's not comfortable, you're being honest.

970
01:31:54,200 --> 01:32:01,200
One that's incredibly valuable because basically you're giving them the information that you would want if you were in their shoes,

971
01:32:01,200 --> 01:32:04,200
because we have this sort of delusion that takes over us.

972
01:32:04,200 --> 01:32:11,200
Whenever we're tempted to tell a white lie, we imagine, okay, this person doesn't want,

973
01:32:11,200 --> 01:32:19,200
it'd be much better for me to just tell them the kind fiction than tell them the uncomfortable truth,

974
01:32:19,200 --> 01:32:25,200
but we don't calculate for the golden rule there most of the time,

975
01:32:25,200 --> 01:32:29,200
and if you just took a moment, you'd realize, wait a minute,

976
01:32:29,200 --> 01:32:38,200
does someone who is actually doing a bad job want me to tell them that they're doing a good job

977
01:32:38,200 --> 01:32:44,200
and then just send them out into the world to bounce around other people who are going to be recognizing,

978
01:32:44,200 --> 01:32:49,200
as I just did, that the thing they're doing isn't so great.

979
01:32:49,200 --> 01:32:51,200
You're just not doing them a favor.

980
01:32:51,200 --> 01:32:53,200
This is part of the nature of belief change, isn't it?

981
01:32:53,200 --> 01:33:00,200
When we believe that someone is on our side, or we believe from a political standpoint that they represent,

982
01:33:00,200 --> 01:33:04,200
99% of the views that we represent, we're much more likely to change our beliefs.

983
01:33:04,200 --> 01:33:11,200
I spoke to Tali Sharra about this, the neuroscientist, and I wrote about this in a chapter in my upcoming book about how you change people's minds.

984
01:33:11,200 --> 01:33:16,200
They showed in the elections that if a flat earther says something to a flat earther about the nature of the earth,

985
01:33:16,200 --> 01:33:20,200
they'll believe it, but if NASA says something to a flat earther, they will just dismiss it on site

986
01:33:20,200 --> 01:33:26,200
because the source of that information is not one that they believe, or trust, or like, or believe is well-intentioned.

987
01:33:26,200 --> 01:33:28,200
This is a bug, not a feature.

988
01:33:28,200 --> 01:33:35,200
It's understandable, but this is something we have to grow beyond because the truth is the truth.

989
01:33:36,200 --> 01:33:39,200
It goes in both directions.

990
01:33:39,200 --> 01:33:48,200
The person on your team who you love and respect is capable in their very next sentence of speaking of falsehood,

991
01:33:48,200 --> 01:33:51,200
and you need to be able to detect that.

992
01:33:51,200 --> 01:34:00,200
Conversely, the person you least respect is capable of saying something that's quite incisive and worth taking on board.

993
01:34:01,200 --> 01:34:14,200
We have to have this sort of metacognitive layer where we're noticing how we're getting played by our social alliances

994
01:34:14,200 --> 01:34:24,200
and recognize that the truth, and rather often important truths, are evaluated by different principles.

995
01:34:24,200 --> 01:34:30,200
I mean, it's not a matter of the messenger. You shouldn't shoot the messenger and you shouldn't worship him.

996
01:34:30,200 --> 01:34:40,200
You mentioned lying as being a significant step change in your own happiness. Is that accurate?

997
01:34:40,200 --> 01:34:42,200
In my happiness.

998
01:34:44,200 --> 01:34:46,200
Immensely so.

999
01:34:46,200 --> 01:34:50,200
Practically and specifically how?

1000
01:34:50,200 --> 01:34:59,200
When you look at how people ruin their reputations and their relationships and their businesses, their careers,

1001
01:34:59,200 --> 01:35:05,200
the gateway to all of the misbehavior that accomplishes that is lying.

1002
01:35:05,200 --> 01:35:08,200
I mean, look at some of that Lance Armstrong or Tiger Woods.

1003
01:35:08,200 --> 01:35:13,200
These guys are the absolute apogee of sport.

1004
01:35:13,200 --> 01:35:19,200
Everyone loves them. Everyone's just amazed at what they've accomplished.

1005
01:35:20,200 --> 01:35:27,200
Yet the dysfunction in their lives just gets vomited up for all to see at a certain point.

1006
01:35:27,200 --> 01:35:35,200
It was just enabled at every stage along the way by lying.

1007
01:35:35,200 --> 01:35:42,200
If either of them had early in their career before they became famous, before they became rich,

1008
01:35:43,200 --> 01:35:50,200
before they became tempted to do anything that was going to derail their lives later on,

1009
01:35:50,200 --> 01:35:54,200
if they had decided they weren't going to lie, right?

1010
01:35:54,200 --> 01:36:00,200
They would have found all, everything else they did to screw up their success impossible.

1011
01:36:00,200 --> 01:36:07,200
So when I decided, and this was in the book, this was a course I took at Stanford.

1012
01:36:07,200 --> 01:36:12,200
It was a seminar with this brilliant professor, Ron Howard, who many people,

1013
01:36:12,200 --> 01:36:17,200
I think some people in Silicon Valley have taken this course as well.

1014
01:36:17,200 --> 01:36:21,200
I mean, this course was just like a machine.

1015
01:36:21,200 --> 01:36:25,200
Undergraduates and graduate students would come in on one side,

1016
01:36:25,200 --> 01:36:32,200
and then 12 weeks later would come out convinced that basically lying was no longer on the menu, right?

1017
01:36:33,200 --> 01:36:40,200
The whole seminar was an analysis of the question, is it ever right to lie?

1018
01:36:40,200 --> 01:36:49,200
And really we focused on white lies, truly tempting lies as opposed to the obvious lies that screw up people's lives and relationships.

1019
01:36:51,200 --> 01:36:58,200
It's just so corrosive, and it's corrosive of relationships in ways that you,

1020
01:36:59,200 --> 01:37:02,200
unless you're a student of this kind of thing, you don't necessarily notice.

1021
01:37:02,200 --> 01:37:06,200
I mean, one example I believe that's in that book is that,

1022
01:37:06,200 --> 01:37:12,200
I remember my wife was with a friend, and the two of them were out,

1023
01:37:12,200 --> 01:37:18,200
and the friend had something she had to do with another friend later that night,

1024
01:37:18,200 --> 01:37:21,200
but she didn't really feel like doing it,

1025
01:37:21,200 --> 01:37:24,200
and she got a call from that friend in the presence of my wife,

1026
01:37:24,200 --> 01:37:29,200
and she just lied to the friends to get out of the plan.

1027
01:37:29,200 --> 01:37:33,200
She said, oh, I'm so sorry, but my daughter's got this thing,

1028
01:37:33,200 --> 01:37:41,200
and it was just an utterly facile use of dishonesty to get,

1029
01:37:41,200 --> 01:37:46,200
or she could have just been honest, but it was just too awkward to be honest,

1030
01:37:46,200 --> 01:37:50,200
so she just got out of it with a lie, but now it's in the presence of my wife,

1031
01:37:50,200 --> 01:37:53,200
and my wife is now, the immediate question is,

1032
01:37:53,200 --> 01:37:56,200
how many times have I been on the other side of that conversation?

1033
01:37:56,200 --> 01:38:03,200
How many times has she lied to me in an equally compelling way about something so trivial?

1034
01:38:03,200 --> 01:38:10,200
And so it just eroded trust in that relationship in a way that the liar would never have known about,

1035
01:38:10,200 --> 01:38:14,200
would never have detected it, because it just went right back to having a good time with,

1036
01:38:14,200 --> 01:38:17,200
they would just have to lunch, and they continued having their lunch,

1037
01:38:17,200 --> 01:38:20,200
and they're still having a good time, and it's all smiles,

1038
01:38:20,200 --> 01:38:26,200
but my wife has just logged something about kind of the ethical limitations of this person,

1039
01:38:26,200 --> 01:38:29,200
and the person doesn't know it, right?

1040
01:38:29,200 --> 01:38:34,200
And so once you sort of pull on this thread,

1041
01:38:34,200 --> 01:38:41,200
basically your entire life becomes, at least for the transition period,

1042
01:38:41,200 --> 01:38:48,200
until this just becomes a habit that you no longer have to consider,

1043
01:38:48,200 --> 01:38:54,200
suddenly the world becomes a kind of mirror thrown up to your mind,

1044
01:38:54,200 --> 01:38:59,200
and you meet yourself in all these situations where you were avoiding yourself before.

1045
01:38:59,200 --> 01:39:05,200
So like someone will say, do you want to have plans,

1046
01:39:05,200 --> 01:39:08,200
or do you want to collaborate with me on this project?

1047
01:39:09,200 --> 01:39:19,200
And if previously you always had recourse to some kind of white lie that just got you out of the awkward truth,

1048
01:39:19,200 --> 01:39:26,200
which is the answer is no, and there are actually reasons why not, right?

1049
01:39:26,200 --> 01:39:29,200
You never have to confront the awkwardness of that,

1050
01:39:29,200 --> 01:39:32,200
you're this kind of person who has these kinds of commitments,

1051
01:39:33,200 --> 01:39:39,200
I mean the most awkward one would be someone declares a romantic interest in you,

1052
01:39:39,200 --> 01:39:50,200
and the answer is no, and it's no for a totally superficial reason, right?

1053
01:39:50,200 --> 01:39:55,200
Like this person is, they're not attractive enough for you, right?

1054
01:39:55,200 --> 01:40:00,200
Or they're overweight, it's just like you have your reason why not,

1055
01:40:00,200 --> 01:40:03,200
and this is something you feel you cannot say, right?

1056
01:40:03,200 --> 01:40:08,200
Now I'm not saying that you should always go out of your way,

1057
01:40:08,200 --> 01:40:13,200
like someone with Tourette's who just helplessly blurts out the truth,

1058
01:40:13,200 --> 01:40:18,200
like there's a scope for kindness and compassion and tact,

1059
01:40:18,200 --> 01:40:23,200
but if someone is going to really drill down on the reasons why not,

1060
01:40:23,200 --> 01:40:27,200
if the person says no, I want to know exactly why you don't want to go out with me,

1061
01:40:28,200 --> 01:40:33,200
there's something to discover on either side of that true disclosure, right?

1062
01:40:33,200 --> 01:40:37,200
Like either you are cast back on yourself and you have to realize,

1063
01:40:37,200 --> 01:40:45,200
okay, I'm such a superficial person that it doesn't matter who anyone is,

1064
01:40:45,200 --> 01:40:49,200
if they're 10 pounds overweight, I'm not interested, right?

1065
01:40:49,200 --> 01:40:53,200
That's the mirror held up to your mind, it's like, okay, all right,

1066
01:40:53,200 --> 01:40:56,200
so you're that kind of person, do you want to still be that kind of person?

1067
01:40:56,200 --> 01:41:01,200
Do you really want to just decide that everyone, no matter what their virtues, right,

1068
01:41:01,200 --> 01:41:05,200
and no matter what chaos is going on in their life,

1069
01:41:05,200 --> 01:41:09,200
actually this person might actually lose those 10 pounds next month

1070
01:41:09,200 --> 01:41:12,200
and you would have a very different situation,

1071
01:41:12,200 --> 01:41:18,200
but are you really not available, are you really filtering by weight in this way?

1072
01:41:18,200 --> 01:41:21,200
And are you really comfortable with that?

1073
01:41:21,200 --> 01:41:24,200
And are you comfortable saying that?

1074
01:41:24,200 --> 01:41:28,200
If somebody forces you to actually be honest?

1075
01:41:28,200 --> 01:41:30,200
We have a closing tradition on this podcast

1076
01:41:30,200 --> 01:41:32,200
where the last guest leaves a question for the next guest,

1077
01:41:32,200 --> 01:41:34,200
not knowing who they're going to leave it for.

1078
01:41:34,200 --> 01:41:37,200
The question that's been left for you, peckable handwriting.

1079
01:41:37,200 --> 01:41:41,200
Where do you want to be when you die?

1080
01:41:41,200 --> 01:41:48,200
Describe the place, time, people, smell, and feeling.

1081
01:41:49,200 --> 01:41:57,200
Well, this actually connects with an idea I've had.

1082
01:41:57,200 --> 01:42:01,200
I mean, I think what we need, we haven't talked about psychedelics here,

1083
01:42:01,200 --> 01:42:05,200
but there's been this renaissance in research in psychedelics,

1084
01:42:05,200 --> 01:42:07,200
and it's hard to know.

1085
01:42:07,200 --> 01:42:12,200
I'm worried that we could recapitulate some of the errors of the 60s

1086
01:42:12,200 --> 01:42:17,200
and roll this all out in a way that's less than wise,

1087
01:42:17,200 --> 01:42:22,200
but the wise version would be, I think we need to recapitulate

1088
01:42:22,200 --> 01:42:25,200
something like the Mysteries of Elusis,

1089
01:42:25,200 --> 01:42:30,200
where we have rites of passage that are enabled by,

1090
01:42:30,200 --> 01:42:36,200
in many people's case, psychedelics and the practice of meditation.

1091
01:42:36,200 --> 01:42:43,200
I just think these are just fundamental tools of insight that are...

1092
01:42:43,200 --> 01:42:46,200
I mean, for most people, it's hard to see how they would get them any other way.

1093
01:42:47,200 --> 01:42:52,200
There's a longer conversation about which molecule and how and all that,

1094
01:42:52,200 --> 01:43:01,200
but another component of this is a hospice situation where the experience of dying

1095
01:43:01,200 --> 01:43:08,200
is as wisely embraced and facilitated as is possible,

1096
01:43:08,200 --> 01:43:12,200
and I think psychedelics could certainly play a role for many people there.

1097
01:43:12,200 --> 01:43:17,200
I imagine something like, we need places that are truly beautiful

1098
01:43:17,200 --> 01:43:23,200
where people have gone to die and their families can visit them there,

1099
01:43:23,200 --> 01:43:37,200
and it is just a final rite of passage that is embraced with all the wisdom we can muster there.

1100
01:43:38,200 --> 01:43:45,200
In my case, currently, I'd be happy to be home,

1101
01:43:45,200 --> 01:43:52,200
but wherever home is at that point, I would want a view of the sky.

1102
01:43:52,200 --> 01:43:55,200
It could be an ocean beneath the sky, that would be ideal.

1103
01:43:59,200 --> 01:44:04,200
There's basically nothing that makes me happier than just looking at a blue sky

1104
01:44:04,200 --> 01:44:09,200
with just watching cumulus clouds move across a blue sky.

1105
01:44:09,200 --> 01:44:14,200
I can extract so much mental pleasure just looking at that.

1106
01:44:18,200 --> 01:44:23,200
If I'm going to spend my last hours of life looking at anything,

1107
01:44:23,200 --> 01:44:26,200
if my eyes are going to be open, looking at the sky and having...

1108
01:44:26,200 --> 01:44:29,200
The stars with the sky, the daytime sky.

1109
01:44:30,200 --> 01:44:37,200
Light pollution is enough of a thing in my world that I feel like I go for years

1110
01:44:37,200 --> 01:44:43,200
without seeing a good night sky, so I've kind of given up hope there,

1111
01:44:43,200 --> 01:44:45,200
but I do love that.

1112
01:44:47,200 --> 01:44:52,200
But yeah, just a view of the sky and with the people I love at that point

1113
01:44:52,200 --> 01:44:55,200
who are still alive at that point.

1114
01:44:56,200 --> 01:45:01,200
I'm not worried about death in that sense.

1115
01:45:06,200 --> 01:45:08,200
The death part is not a problem.

1116
01:45:11,200 --> 01:45:15,200
I can imagine there could be sort of medical chaos and uncertainty

1117
01:45:15,200 --> 01:45:19,200
and all of the weirdness that happens around the dying process,

1118
01:45:19,200 --> 01:45:21,200
depending on...

1119
01:45:21,200 --> 01:45:24,200
There are all kinds of ways to die that I wouldn't choose,

1120
01:45:25,200 --> 01:45:30,200
but having a nice place to do that with a view of the sky

1121
01:45:30,200 --> 01:45:34,200
would be the only solution I think I would require.

1122
01:45:34,200 --> 01:45:37,200
The question asks the smell.

1123
01:45:37,200 --> 01:45:38,200
Give me the smell.

1124
01:45:38,200 --> 01:45:40,200
Give me an ocean breeze.

1125
01:45:40,200 --> 01:45:43,200
I have put an ocean there, so yeah, an ocean breeze would be perfect.

1126
01:45:45,200 --> 01:45:46,200
Sam, thank you so much.

1127
01:45:46,200 --> 01:45:48,200
Thank you for not just this conversation.

1128
01:45:48,200 --> 01:45:50,200
As I said to you before you sat down,

1129
01:45:50,200 --> 01:45:54,200
you were pivotal in really helping me to unpack some problems on our jungle,

1130
01:45:54,200 --> 01:45:59,200
some conflicts I should describe them as with my view on religious belief

1131
01:45:59,200 --> 01:46:04,200
and the nature of the world, but I think more importantly,

1132
01:46:04,200 --> 01:46:08,200
you didn't rob me of my religious beliefs and leave me with nothing.

1133
01:46:08,200 --> 01:46:11,200
You left me with something else, which is something that was really important to me,

1134
01:46:11,200 --> 01:46:14,200
which was the idea that there can still be great meaning

1135
01:46:14,200 --> 01:46:18,200
and there can be what you describe as spirituality in the absence

1136
01:46:18,200 --> 01:46:21,200
or in the place of that religious belief.

1137
01:46:21,200 --> 01:46:24,200
Religious belief gives people a lot of things,

1138
01:46:24,200 --> 01:46:26,200
and it's funny because when I was religious

1139
01:46:26,200 --> 01:46:29,200
and I went on the journey to becoming agnostic, let's say,

1140
01:46:29,200 --> 01:46:33,200
I was in conflict with people, as in I would want to have a debate with everybody

1141
01:46:33,200 --> 01:46:37,200
and I spent those two years watching everything that you and Richard Dawkins

1142
01:46:37,200 --> 01:46:42,200
and Hitchens had all done, and then I came out of the side and it was peaceful.

1143
01:46:42,200 --> 01:46:45,200
You believe what you want, I'll believe what I want.

1144
01:46:45,200 --> 01:46:48,200
As long as we're not causing any conflicts with each other

1145
01:46:48,200 --> 01:46:50,200
and doing any harm, it's okay.

1146
01:46:50,200 --> 01:46:53,200
And then I discovered what I would call my own spirituality,

1147
01:46:53,200 --> 01:46:56,200
which is the meaning that I see in the world around me

1148
01:46:56,200 --> 01:46:59,200
and the self and things like psychedelics.

1149
01:46:59,200 --> 01:47:01,200
And it's a better place to be.

1150
01:47:01,200 --> 01:47:05,200
And it removed my fear of death, which I had as a religious person.

1151
01:47:05,200 --> 01:47:08,200
So thank you. Thank you for that.

1152
01:47:08,200 --> 01:47:10,200
And all your subsequent work, incredible books,

1153
01:47:10,200 --> 01:47:12,200
you've written so many of them that are absolutely incredible.

1154
01:47:12,200 --> 01:47:18,200
You've got an unbelievable podcast, which I was gorging on before you came here as well in an app,

1155
01:47:18,200 --> 01:47:22,200
which, I mean, if you could speak just a few sentences about the meaning of the app

1156
01:47:22,200 --> 01:47:24,200
and what you do, I know it's much more than meditation now,

1157
01:47:24,200 --> 01:47:28,200
but I think people listening to this might be compelled to check it out and download it.

1158
01:47:28,200 --> 01:47:32,200
Yeah, well, so I had that book, which you're holding, Waking Up,

1159
01:47:32,200 --> 01:47:36,200
which is where I talk about my experience in meditation

1160
01:47:36,200 --> 01:47:43,200
and just how I fit it into a scientific, secular worldview.

1161
01:47:43,200 --> 01:47:48,200
And it just turns out that an app is a much better delivery system for that kind of information.

1162
01:47:48,200 --> 01:47:51,200
I mean, it's just hearing audio. You don't even need video.

1163
01:47:51,200 --> 01:47:55,200
I think audio is the perfect medium for it.

1164
01:47:55,200 --> 01:47:59,200
So when that technology came about or when I discovered it,

1165
01:47:59,200 --> 01:48:03,200
I just felt incredibly lucky to be able to build it.

1166
01:48:03,200 --> 01:48:06,200
And so it's kind of outgrown me now. There are many, many teachers on it

1167
01:48:06,200 --> 01:48:09,200
and many other topics beyond meditation that are touched.

1168
01:48:09,200 --> 01:48:20,200
But it really subverts all of the problems that some of which we touched upon here with the smartphone.

1169
01:48:20,200 --> 01:48:25,200
I mean, the smartphone has become this tool of fragmentation for us.

1170
01:48:25,200 --> 01:48:29,200
It fragments our attention. It continually interrupts our experience.

1171
01:48:29,200 --> 01:48:33,200
It's depending on how you use it.

1172
01:48:33,200 --> 01:48:36,200
But most of what we do with it, you know, you're checking Slack,

1173
01:48:36,200 --> 01:48:38,200
you're checking your email, you're checking your social media,

1174
01:48:38,200 --> 01:48:42,200
you're just, it's punctuating your life with all this, you know,

1175
01:48:42,200 --> 01:48:46,200
at this point, seemingly necessary interruptions.

1176
01:48:46,200 --> 01:48:52,200
But this app or, you know, really any app like it that is delivering this kind of content

1177
01:48:52,200 --> 01:48:56,200
subverts all that because it's just, this is, it's just a platform

1178
01:48:56,200 --> 01:49:01,200
where you're getting audio that is guiding you in a specific,

1179
01:49:01,200 --> 01:49:05,200
very specific use of attention and a sort of reordering of your priorities

1180
01:49:05,200 --> 01:49:13,200
and getting you to recognize things about your experience that you wouldn't otherwise see.

1181
01:49:13,200 --> 01:49:18,200
And yeah, an app is just a sheer good luck.

1182
01:49:18,200 --> 01:49:22,200
It turns out it's just the perfect delivery system for that information.

1183
01:49:22,200 --> 01:49:26,200
So yeah, I just felt very lucky to have stumbled upon it because again, you know,

1184
01:49:26,200 --> 01:49:30,200
10 years ago, there were no apps and, you know, there's just,

1185
01:49:30,200 --> 01:49:32,200
all I could do is write a book.

1186
01:49:32,200 --> 01:49:33,200
Sam, thank you.

1187
01:49:33,200 --> 01:49:34,200
Yeah, thank you.

1188
01:49:34,200 --> 01:49:36,200
Thank you so much for your generosity.

1189
01:49:36,200 --> 01:49:37,200
Yeah, a pleasure to meet you as well.

1190
01:49:37,200 --> 01:49:38,200
Congratulations with everything.

1191
01:49:38,200 --> 01:49:42,200
I mean, it's really, I was catching up on your podcast in anticipation of this

1192
01:49:42,200 --> 01:49:44,200
and it's amazing the reach you've got now.

1193
01:49:44,200 --> 01:49:46,200
So it's wonderful.

1194
01:49:46,200 --> 01:49:50,200
No, it's still trying to catch up with it, but it's a credit to all of the team.

1195
01:49:50,200 --> 01:49:53,200
And I really want to say from the bottom of my heart, thank you.

1196
01:49:53,200 --> 01:49:56,200
Because the work you do is really, really important.

1197
01:49:56,200 --> 01:49:59,200
It's been important in my life, as I've said, but it's just really important.

1198
01:49:59,200 --> 01:50:03,200
And I feel like we're living in a world where like nuance and all the things you've talked about

1199
01:50:03,200 --> 01:50:08,200
and openness to debate and honest dialogue asks, we're getting further and further away from there.

1200
01:50:08,200 --> 01:50:11,200
So if there's anyone left in this world that's still willing to engage on that level,

1201
01:50:11,200 --> 01:50:13,200
I feel like they must be protected at all costs.

1202
01:50:13,200 --> 01:50:14,200
And I see you as one of those people.

1203
01:50:14,200 --> 01:50:15,200
So thank you.

1204
01:50:15,200 --> 01:50:16,200
Nice.

1205
01:50:16,200 --> 01:50:17,200
Well, to be continued.

1206
01:50:20,200 --> 01:50:21,200
Thank you.

