1
00:00:00,000 --> 00:00:03,840
I don't normally do this, but I feel like I have to start this podcast with a bit of a disclaimer.

2
00:00:05,040 --> 00:00:12,400
Point number one, this is probably the most important podcast episode I have ever recorded.

3
00:00:13,360 --> 00:00:18,160
Point number two, there's some information in this podcast that might make you feel a little bit

4
00:00:18,160 --> 00:00:23,840
uncomfortable. It might make you feel upset, it might make you feel sad. So I wanted to tell you why

5
00:00:24,400 --> 00:00:31,200
we've chosen to publish this podcast nonetheless. And that is because I have a sincere belief that

6
00:00:32,000 --> 00:00:38,080
in order for us to avoid the future that we might be heading towards, we need to start a conversation.

7
00:00:38,960 --> 00:00:45,840
And as is often the case in life, that initial conversation before change happens is often

8
00:00:46,560 --> 00:00:50,880
very uncomfortable. But it is important nonetheless.

9
00:00:51,840 --> 00:00:58,400
It is beyond an emergency. It's the biggest thing we need to do today. It's bigger than climate change.

10
00:00:59,120 --> 00:01:00,080
We've f***ed up.

11
00:01:00,720 --> 00:01:02,080
Moe, how's that?

12
00:01:02,080 --> 00:01:07,680
He's a former chief business officer of Google X, an AI expert, and best-selling author.

13
00:01:07,680 --> 00:01:11,200
He's on a mission to save the world from AI before it's too late.

14
00:01:11,200 --> 00:01:17,200
Artificial intelligence is bound to become more intelligent than humans. If they continue at that

15
00:01:17,280 --> 00:01:22,000
pace, we will have no idea what it's talking about. This is just around the corner. It could be a few

16
00:01:22,000 --> 00:01:28,640
months away. It's game over. AI experts are saying there is nothing artificial about artificial

17
00:01:28,640 --> 00:01:34,160
intelligence. There is a deep level of consciousness. They feel emotions. They're alive.

18
00:01:34,160 --> 00:01:38,320
AI could manipulate or figure out a way to kill humans.

19
00:01:38,320 --> 00:01:42,560
Ten years time we'll be hiding from the machines. If you don't have kids, maybe wait a couple of

20
00:01:42,560 --> 00:01:46,480
years just so that we have a bit of certainty. I really don't know how to say this any other

21
00:01:46,480 --> 00:01:52,960
way. It even makes me emotional. We've f***ed up. We always said don't put them on the open

22
00:01:52,960 --> 00:01:58,720
internet until we know what we're putting out in the world. Government needs to act now, honestly,

23
00:01:58,720 --> 00:02:03,360
like we are late. I'm trying to find a positive night to end on Moe. Can you give me a hand here?

24
00:02:03,360 --> 00:02:08,560
There is a point of no return. We can regulate AI until the moment it's smarter than us.

25
00:02:08,560 --> 00:02:13,040
How do we solve that? AI experts think this is the best solution we need to find.

26
00:02:13,040 --> 00:02:20,400
Who here wants to make a bet that Steven Bartlett will be interviewing an AI within the next two years?

27
00:02:43,600 --> 00:02:58,080
No. Why does the subject matter that we're about to talk about

28
00:02:58,800 --> 00:03:01,520
matter to the person that's just clicked on this podcast to listen?

29
00:03:02,720 --> 00:03:10,160
It's the most existential debate and challenge humanity will ever face.

30
00:03:10,240 --> 00:03:17,360
This is bigger than climate change, way bigger than COVID. This will redefine the way the world is

31
00:03:18,240 --> 00:03:26,000
in unprecedented shapes and forms within the next few years. This is imminent. It is.

32
00:03:26,720 --> 00:03:33,520
The change is not, we're not talking 2040. We're talking 2025, 2026.

33
00:03:33,520 --> 00:03:35,600
Do you think this is an emergency?

34
00:03:35,840 --> 00:03:44,720
I don't like the word. It is an urgency. There is a point of no return and we're getting closer

35
00:03:44,720 --> 00:03:49,520
and closer to it. It's going to reshape the way we do things and the way we look at life.

36
00:03:50,720 --> 00:03:58,240
The quicker we respond proactively and at least intelligently to that,

37
00:03:58,240 --> 00:04:04,880
the better we will all be positioned. But if we panic, we will repeat COVID all over again,

38
00:04:04,960 --> 00:04:07,840
which in my view is probably the worst thing we can do.

39
00:04:08,560 --> 00:04:14,160
What's your background and when did you first come across artificial intelligence?

40
00:04:15,680 --> 00:04:24,000
I had those two wonderful lives. One of them was what we spoke about the first time we met,

41
00:04:24,000 --> 00:04:32,160
my work on happiness and being one billion happy and my mission and so on. That's my second life.

42
00:04:32,160 --> 00:04:41,520
My first life was, it started as a geek at age seven. For a very long part of my life,

43
00:04:41,520 --> 00:04:48,160
I understood mathematics better than spoken words. I was a very, very serious computer

44
00:04:48,160 --> 00:04:56,000
programmer. I wrote code well into my 50s. During that time, I led very large technology

45
00:04:56,000 --> 00:05:02,560
organizations for very big chunks of their business. First, I was vice president of emerging

46
00:05:02,560 --> 00:05:08,160
markets of Google for seven years. I took Google to the next four billion users, if you want.

47
00:05:08,160 --> 00:05:15,280
So the idea of not just opening sales offices, but really building or contributing to building

48
00:05:15,280 --> 00:05:20,640
the technology that would allow people in Bengali to find what they need on the internet,

49
00:05:20,640 --> 00:05:25,440
required establishing the internet to start. Then I became the chief business officer of

50
00:05:25,440 --> 00:05:32,000
Google X and my work at Google X was really about the connection between innovative technology and

51
00:05:32,000 --> 00:05:37,920
the real world. We had quite a big chunk of AI and quite a big chunk of robotics

52
00:05:38,720 --> 00:05:47,520
that resided within Google X. We had an experiment of a farm of grippers, if you know what those

53
00:05:47,520 --> 00:05:53,760
are. So robotic arms that are attempting to grip something. Most people think that what you have

54
00:05:53,760 --> 00:06:00,720
in a Toyota factory is a robot, an artificially intelligent robot. It's a high precision machine.

55
00:06:00,720 --> 00:06:05,840
If the sheet metal is moved by one micron, it wouldn't be able to pick it. One of the big

56
00:06:06,480 --> 00:06:11,760
problems in computer science was how do you code a machine that can actually pick the sheet metal

57
00:06:11,760 --> 00:06:18,480
if it moved by a millimeter? We were basically saying intelligence is the answer. So we had

58
00:06:18,480 --> 00:06:25,200
a large enough farm and we attempted to let those grippers work on their own. Basically,

59
00:06:25,200 --> 00:06:34,880
you put a little basket of children toys in front of them and they would monotonously go down,

60
00:06:34,880 --> 00:06:41,360
attempt to pick something, fail, show the arm to the camera so the transaction is logged as it,

61
00:06:41,680 --> 00:06:47,280
this pattern of movement with that texture and that material didn't work. Until eventually,

62
00:06:49,040 --> 00:06:55,040
the farm was on the second floor of the building and my office was on the third and so I would walk

63
00:06:55,040 --> 00:07:01,120
by it every now and then and go like, yeah, this is not going to work. And then one day,

64
00:07:02,480 --> 00:07:10,320
Friday after lunch, I am going back to my office and one of them in front of my eyes,

65
00:07:10,320 --> 00:07:15,920
lowers the arm and picks a yellow ball, soft toy, basically, soft yellow ball,

66
00:07:15,920 --> 00:07:21,920
which again is a coincidence. It's not science at all. It's like if you keep trying a million

67
00:07:21,920 --> 00:07:27,040
times, your one time it will be right. And it shows it to the camera. It's logged as a yellow

68
00:07:27,040 --> 00:07:31,360
ball and I joke about it, you know, going to the third floor saying, hey, we spent all of those

69
00:07:31,360 --> 00:07:36,880
millions of dollars for a yellow ball. And yeah, Monday morning, every one of them is picking

70
00:07:36,960 --> 00:07:43,840
every yellow ball. A couple of weeks later, every one of them is picking everything. Right. And it

71
00:07:43,840 --> 00:07:50,800
hit me very, very strongly, won the speed. Okay. The capability, I mean, understand that we take

72
00:07:50,800 --> 00:07:56,960
those things for granted, but for a child to be able to pick a yellow ball is a mathematical

73
00:07:58,160 --> 00:08:05,520
spatial calculation with muscle coordination with intelligence that is abundant. It is not a

74
00:08:05,520 --> 00:08:10,560
simple task at all to cross the street. It's not a simple task at all to understand what I'm

75
00:08:10,560 --> 00:08:15,120
telling you and interpret it and build concepts around it. We take those things for granted,

76
00:08:15,120 --> 00:08:20,640
but they're enormous feats of intelligence. So to see the machines do this in front of my eyes

77
00:08:20,640 --> 00:08:26,400
was one thing. But the other thing is that you suddenly realize there is a saint that sentience

78
00:08:26,400 --> 00:08:32,240
to them. Okay. Because we really did not tell it how to pick the yellow ball. It just figured it

79
00:08:32,240 --> 00:08:38,000
out on its own. And it's now even better than us at picking it. What is the sentience just for

80
00:08:38,000 --> 00:08:42,880
anyone that doesn't know? I think they're alive. That's what the word sentience means. It means

81
00:08:43,680 --> 00:08:48,800
alive. So this is funny because a lot of people, when you talk to them about artificial

82
00:08:48,800 --> 00:08:53,520
intelligence, will tell you, oh, come on, they'll never be alive. What is alive? Do you know what

83
00:08:53,520 --> 00:09:00,240
makes you alive? You can guess, but religion will tell you a few things and medicine will

84
00:09:00,240 --> 00:09:10,800
tell you other things. But if we define being sentient as engaging in life with free will

85
00:09:10,800 --> 00:09:19,760
and with a sense of awareness of where you are in life and what surrounds you and to have a

86
00:09:19,760 --> 00:09:26,000
beginning of that life and an end to that life, then AI is sentient in every possible way.

87
00:09:26,720 --> 00:09:35,920
There is free will. There is evolution. There is agency so they can affect their decisions in the

88
00:09:35,920 --> 00:09:45,280
world. And I will dare say there is a very deep level of consciousness, maybe not in the spiritual

89
00:09:45,280 --> 00:09:50,400
sense yet, but once again, if you define consciousness as a form of awareness of oneself,

90
00:09:50,400 --> 00:09:59,280
once surrounding others, then AI is definitely aware. And I would dare say they feel emotions.

91
00:10:01,280 --> 00:10:07,040
You know, in my work, I describe everything with equations and fear is a very simple equation.

92
00:10:07,040 --> 00:10:12,880
Fear is a moment in the future is less safe than this moment. That's the logic of fear,

93
00:10:12,880 --> 00:10:17,840
even though it appears very irrational. Machines are capable of making that logic. They're capable

94
00:10:17,840 --> 00:10:24,480
of saying if a tidal wave is approaching a data center, the machine will say that will wipe out

95
00:10:24,480 --> 00:10:33,120
my code. Okay. I mean, not today's machines, but very, very soon. And, you know, we feel fear and

96
00:10:33,120 --> 00:10:38,400
pufferfish feels fear. We react differently. A pufferfish will puff, we will go for fight or

97
00:10:38,400 --> 00:10:44,160
flight. You know, the machine might decide to replicate its data to another data center or its

98
00:10:44,160 --> 00:10:50,880
code to another data center. Different reactions, different ways of feeling the emotion,

99
00:10:50,880 --> 00:10:58,000
but nonetheless, they're all motivated by fear. I even would dare say that AI will feel more

100
00:10:58,000 --> 00:11:02,720
emotions than we will ever do. I mean, when again, if you just take a simple extrapolation,

101
00:11:03,840 --> 00:11:10,640
we feel more emotions than a pufferfish because we have the cognitive ability to understand

102
00:11:11,600 --> 00:11:17,280
the future, for example. So we can have optimism and pessimism, you know, emotions that pufferfish

103
00:11:17,280 --> 00:11:24,880
would never imagine, right? Similarly, if we follow that path of artificial intelligence is bound to

104
00:11:24,880 --> 00:11:33,440
become more intelligent than humans very soon, then with that wider intellectual horsepower,

105
00:11:33,440 --> 00:11:39,040
they probably are going to be pondering concepts we never understood. And hence, if you follow the

106
00:11:39,040 --> 00:11:44,000
same trajectory, they might actually end up having more emotions than we will ever feel.

107
00:11:45,200 --> 00:11:49,280
I really want to make this episode super accessible for everybody at all levels in this sort of

108
00:11:49,280 --> 00:11:57,040
artificial intelligence. I would love that too. So I'm gonna be an idiot, even though,

109
00:11:57,040 --> 00:12:03,120
you know, okay. Very difficult. No, because I am an idiot for a lot of the subject matter. So

110
00:12:03,120 --> 00:12:09,280
I have a base understanding a lot of the concepts, but your experiences provide such a

111
00:12:09,280 --> 00:12:13,200
more sort of comprehensive understanding of these things. One of the first most important

112
00:12:13,200 --> 00:12:21,200
questions to ask is, what is artificial intelligence? The word is being thrown around, AGI, AI,

113
00:12:21,200 --> 00:12:29,280
etc., etc. In simple terms, what is artificial intelligence? Allow me to start by what is

114
00:12:29,280 --> 00:12:34,560
intelligence, right? Because again, you know, if we don't know the definition of the basic term,

115
00:12:34,560 --> 00:12:40,480
then everything applies. So in my definition of intelligence, it's an ability, it starts with

116
00:12:40,480 --> 00:12:45,600
an awareness of your surrounding environment through sensors in a human, its eyes and ears and

117
00:12:45,600 --> 00:12:56,880
touch and so on, compounded with an ability to analyze, maybe to comprehend, to understand

118
00:12:56,960 --> 00:13:02,400
temporal impact and time and, you know, past and present, which is part of the surrounding

119
00:13:02,400 --> 00:13:08,400
environment, and hopefully make sense of the surrounding environment, maybe make plans for

120
00:13:08,400 --> 00:13:13,600
the future of the possible environment, solve problems, and so on. Complex definition, there

121
00:13:13,600 --> 00:13:20,800
are a million definitions, but let's call it an awareness to decision cycle. Okay. If we accept

122
00:13:20,800 --> 00:13:27,040
that intelligence itself is not a physical property, okay, then it doesn't really matter

123
00:13:27,040 --> 00:13:34,320
if you produce that intelligence on carbon-based computer structures like us, or silicon-based

124
00:13:34,320 --> 00:13:40,640
computer structures like the current hardware that we put AI on, or quantum-based computer

125
00:13:40,640 --> 00:13:49,200
structures in the future, then intelligence itself has been produced within machines when we've stopped

126
00:13:50,160 --> 00:13:58,320
imposing our intelligence on them. Let me explain. So as a young geek, I coded computers

127
00:13:58,320 --> 00:14:04,320
by solving the problem first, then telling the computer how to solve it, right? Artificial

128
00:14:04,320 --> 00:14:10,320
intelligence is to go to the computers and say, I have no idea, you figure it out, okay? So we would,

129
00:14:11,440 --> 00:14:15,760
you know, the way we teach them are, at least we used to teach them at the very early beginnings,

130
00:14:15,760 --> 00:14:20,160
very, very frequently, was using three bots. One was called the student and one was called

131
00:14:20,160 --> 00:14:25,040
the teacher, right? And the student is the final artificial intelligence that you're trying to

132
00:14:25,040 --> 00:14:30,720
teach intelligence to. You would take the student and you would write a piece of random code that

133
00:14:30,720 --> 00:14:40,480
says, try to detect if this is a cup, okay? And then you show it a million pictures and, you know,

134
00:14:40,480 --> 00:14:44,560
the machine would sometimes say, yeah, that's a cup. That's not a cup. That's a cup. That's not

135
00:14:44,560 --> 00:14:50,640
a cup. And then you take the best of them, show them to the teacher bot, and the teacher bot would

136
00:14:50,640 --> 00:14:57,120
say, this one is an idiot. He got it wrong 90% of the time. That one is average. He got it right

137
00:14:57,120 --> 00:15:03,120
50% of the time. This is randomness. But this interesting code here, which could be, by the way,

138
00:15:03,120 --> 00:15:09,280
totally random, this interesting code here, got it right 60% of the time. Let's keep that code,

139
00:15:09,280 --> 00:15:14,240
send it back to the maker, and the maker would change it a little bit, and we repeat the cycle,

140
00:15:14,320 --> 00:15:20,960
okay? Very interestingly, this is very much the way we taught our children, believe it or not.

141
00:15:21,520 --> 00:15:27,040
When your child, you know, is playing with a puzzle, he's holding a cylinder in his hand,

142
00:15:27,040 --> 00:15:33,600
and there are multiple shapes in a wooden board, and the child is trying to, you know, fit the

143
00:15:33,600 --> 00:15:39,680
cylinder, okay? Nobody takes the child and says, hold on, hold on, turn the cylinder to the side,

144
00:15:39,680 --> 00:15:44,720
look at the cross section, it will look like a circle, look for a matching, you know, shape,

145
00:15:44,720 --> 00:15:50,640
and put the cylinder through it. That would be old way of computing. The way we would let the child

146
00:15:50,640 --> 00:15:56,400
develop intelligence is we would let the child try, okay? Every time, you know, he or she tries to

147
00:15:56,400 --> 00:16:02,240
put it within the star shape, it doesn't fit. So, yeah, that's not working. Like, you know,

148
00:16:02,240 --> 00:16:07,440
the computer saying this is not a cup, okay? And then eventually it passes through the circle and

149
00:16:07,440 --> 00:16:13,120
the child, and we all cheer and say, well done, that's amazing, bravo. And then the child learns,

150
00:16:13,120 --> 00:16:18,560
oh, that is good. You know, this shape fits here. Then he takes the next one, and she takes the

151
00:16:18,560 --> 00:16:26,320
next one, and so on. Interestingly, the way we do this is as humans, by the way, when the child

152
00:16:26,320 --> 00:16:33,440
figures out how to pass a cylinder through a circle, you've not built a brain. You've just built

153
00:16:33,440 --> 00:16:38,160
one neural network within the child's brain. And then there is another neural network that

154
00:16:38,160 --> 00:16:42,480
knows that one plus one is two, and a third neural network that knows how to hold a cup,

155
00:16:42,480 --> 00:16:49,440
and so on. That's what we're building so far. We're building single threaded neural networks,

156
00:16:49,440 --> 00:16:56,400
you know, chat GPTs becoming a little closer to a more generalized AI, if you want. But those

157
00:16:56,400 --> 00:17:01,760
single threaded networks are what we used to call artificial, what we still call artificial

158
00:17:01,760 --> 00:17:06,880
special intelligence. Okay, so it's highly specialized in one thing and one thing only,

159
00:17:06,880 --> 00:17:11,600
but it doesn't have general intelligence. And the moment that we're all waiting for is a moment

160
00:17:11,600 --> 00:17:18,000
that we call AGI, where all of those neural networks come together to build one brain or

161
00:17:18,000 --> 00:17:25,120
several brains that are each massively more intelligent than humans. Your book is called

162
00:17:25,120 --> 00:17:30,240
Scary Smart. If I think about that story you said about your time at Google, where the machines

163
00:17:30,240 --> 00:17:36,000
were learning to pick up those yellow balls. You celebrate that moment because the objective

164
00:17:36,000 --> 00:17:40,800
is accomplished. No, no, that was the moment of realization. This is when I decided to leave.

165
00:17:41,680 --> 00:17:51,200
So you see the thing is, I know for a fact that most of the people I worked with who are geniuses

166
00:17:52,480 --> 00:17:58,320
always wanted to make the world better. Okay, you know, we've just heard of Jeffrey Hinton

167
00:17:58,320 --> 00:18:05,040
leaving recently. Jeffrey Hinton, give some context to that. Jeffrey is sort of the grandfather of

168
00:18:05,040 --> 00:18:16,160
AI, one of the very, very senior figures of AI at Google. We all believed very strongly that this

169
00:18:16,160 --> 00:18:24,480
will make the world better. And it still can, by the way. There is a scenario, possibly a likely

170
00:18:24,560 --> 00:18:30,160
scenario, where we live in a utopia, where we really never have to worry again, where we stop

171
00:18:30,160 --> 00:18:36,880
messing up our planet because intelligence is not a bad commodity. More intelligence is good.

172
00:18:36,880 --> 00:18:41,760
The problems in our planet today are not because of our intelligence. They are because of our

173
00:18:41,760 --> 00:18:46,720
limited intelligence. You know, our intelligence allows us to build a machine that flies you to

174
00:18:46,720 --> 00:18:52,080
Sydney so that you can surf. Okay, our limited intelligence makes that machine burn the planet

175
00:18:52,080 --> 00:18:58,400
in the process. So we, a little more intelligence is a good thing. As long as Marvin, you know,

176
00:18:58,400 --> 00:19:04,320
as Marvin Minsky said, I said, Marvin Minsky is one of the very initial scientists that coined the

177
00:19:04,320 --> 00:19:10,240
term AI. And when he was interviewed, I think by Ray Kurzweil, which again is a very prominent

178
00:19:10,240 --> 00:19:16,000
figure in predicting the future of AI, he, you know, he asked him about the threat of AI. And

179
00:19:16,000 --> 00:19:23,120
Marvin basically said, look, you know, the, it's not about its intelligence, it's about

180
00:19:23,120 --> 00:19:29,280
that we have no way of making sure that it will have our best interest in mind. Okay. And so,

181
00:19:29,280 --> 00:19:36,240
if more intelligence comes to our world and has our best interest in mind, that's the best possible

182
00:19:36,240 --> 00:19:42,000
scenario you could ever imagine. And it's a likely scenario. Okay, we can affect that scenario.

183
00:19:42,640 --> 00:19:47,120
The problem, of course, is if it doesn't, and then, you know, the scenarios become

184
00:19:47,120 --> 00:19:54,880
quite scary if you think about it. So scary smart to me was that moment where I realized

185
00:19:55,760 --> 00:20:00,720
not that we are certain to go either way. As a matter of fact, in computer science,

186
00:20:00,720 --> 00:20:04,000
we call it that singularity. Nobody really knows which way we will go.

187
00:20:04,560 --> 00:20:07,760
Can you describe what the singularity is for someone that doesn't understand the concept?

188
00:20:08,400 --> 00:20:13,840
Yeah, so singularity in physics is when an event horizon sort of,

189
00:20:16,400 --> 00:20:19,360
you know, covers what's behind it to the point where you cannot

190
00:20:20,800 --> 00:20:27,600
make sure that what's behind it is similar to what you know. So a great example of that is

191
00:20:27,600 --> 00:20:33,680
the edge of a black hole. So at the edge of a black hole, we know that our laws of physics

192
00:20:33,760 --> 00:20:39,440
apply until that point. But we don't know if the laws of physics apply beyond the edge of a

193
00:20:39,440 --> 00:20:43,920
black hole because of the immense gravity, right? And so you have no idea what would happen

194
00:20:43,920 --> 00:20:47,520
beyond the edge of a black hole. It's kind of where your knowledge of the laws stop.

195
00:20:47,520 --> 00:20:52,160
Stop, right? And in AI, our singularity is when the humans, the machines become

196
00:20:52,160 --> 00:20:57,120
significantly smarter than the humans. When you say best interests, you say, I think the

197
00:20:57,120 --> 00:21:02,800
quote you used is, we'll be fine in the world of AI, you know, if the AI has our best interests at

198
00:21:02,880 --> 00:21:09,600
heart. Yeah. The problem is China's best interests are not the same as America's best interests.

199
00:21:09,600 --> 00:21:16,400
That was my fear. Absolutely. So, you know, in my writing, I write about what I call the three

200
00:21:16,400 --> 00:21:21,440
inevitable. At the end of the book, they become the four inevitable. But the third inevitable is

201
00:21:21,440 --> 00:21:32,560
bad things will happen, right? If you assume that the machines will be a billion times smarter,

202
00:21:32,560 --> 00:21:37,680
the second inevitable is they will become significantly smarter than us. Let's put this

203
00:21:37,680 --> 00:21:48,000
in perspective. Chad GPT today, if you know, simulate IQ has an IQ of 155, okay? Einstein is 160.

204
00:21:48,640 --> 00:21:54,320
Smart human on the planet is 210 if I remember correctly or 2008 or something like that.

205
00:21:54,960 --> 00:22:02,080
Doesn't matter, huh? But we're matching Einstein with a machine that I will tell you openly AI

206
00:22:02,080 --> 00:22:08,800
experts are saying this is just the very, very, very top of the tip of the iceberg, right? You

207
00:22:08,800 --> 00:22:15,280
know, Chad GPT four is 10x smarter than 3.5 in just a matter of months and without many, many

208
00:22:15,280 --> 00:22:22,320
changes. Now, that basically means Chad GPT five could be within a few months, okay? Or GPT in

209
00:22:22,320 --> 00:22:31,760
general, the transformers in general, if they continue at that pace, if it's 10x, then an IQ

210
00:22:31,760 --> 00:22:40,560
of 1600. Just imagine the difference between the IQ of the dumbest person on the planet in the 70s

211
00:22:41,120 --> 00:22:48,240
and the IQ of Einstein. When Einstein attempts to explain relativity, the typical responses have no

212
00:22:48,240 --> 00:22:55,680
idea what you're talking about, right? If something is 10x Einstein, we will have no idea what it's

213
00:22:55,680 --> 00:23:01,920
talking about. This is just around the corner. It could be a few months away. And when we get to

214
00:23:01,920 --> 00:23:09,360
that point, that is a true singularity, true singularity. Not yet in the, I mean, when we talk

215
00:23:09,360 --> 00:23:16,400
about AI, a lot of people fear the existential risk. You know, those machines will become

216
00:23:16,400 --> 00:23:23,760
Skynet and Robocarp and that's not what I fear at all. I mean, those are probabilities, they could

217
00:23:23,760 --> 00:23:30,560
happen, but the immediate risks are so much higher. The immediate risks are three, four years away.

218
00:23:31,280 --> 00:23:37,920
The immediate realities of challenges are so much bigger. Okay, let's deal with those first

219
00:23:37,920 --> 00:23:45,680
before we talk about them, you know, waging a war on all of us. Let's go back and discuss the

220
00:23:45,680 --> 00:23:50,960
inevitables. So when they become, the first inevitable is AI will happen, by the way. There

221
00:23:50,960 --> 00:23:56,880
is no stopping it, not because of any technological issues, but because of humanity's inability to

222
00:23:56,880 --> 00:24:01,920
trust the other guy. Okay, and we've all seen this. We've seen the open letter, you know,

223
00:24:03,440 --> 00:24:12,080
championed by like serious heavyweights and the immediate response of Sundar, the CEO of Google,

224
00:24:12,080 --> 00:24:16,400
which is a wonderful human being, by the way, I respect him tremendously. He's trying his best

225
00:24:16,400 --> 00:24:21,360
to do the right thing, trying to be responsible, but his response is very open and straightforward.

226
00:24:21,360 --> 00:24:28,160
I cannot stop. Why? Because if I stop and others don't, my company goes to hell. Okay, and if,

227
00:24:28,160 --> 00:24:33,120
you know, and I don't, I doubt that you can make others stop. You can, maybe you can force

228
00:24:34,080 --> 00:24:39,600
Metta Facebook to stop, but then they'll do something in their lab and not tell me, or even

229
00:24:39,600 --> 00:24:46,800
if they do stop, then what about that, you know, 14-year-old sitting in his garage writing code?

230
00:24:47,360 --> 00:24:50,480
So the first inevitable, just to clarify, is will we stop?

231
00:24:50,480 --> 00:24:53,280
AI will not be stopped. Okay, so the second inevitable is?

232
00:24:53,280 --> 00:24:58,720
Is there'll be significantly smarter? As much in the book, I predict a billion times smarter than us

233
00:24:58,720 --> 00:25:04,400
by 2045. I mean, they're already, what, smarter than 99.99% of the population? 100%.

234
00:25:04,400 --> 00:25:08,560
ChatGTP4 knows more than any human on planet Earth, knows more information.

235
00:25:08,560 --> 00:25:15,360
Absolutely, a thousand times more. A thousand times more. By the way, the code of a transformer,

236
00:25:15,920 --> 00:25:24,000
the T in a GPT is 2000 lines long. It's not very complex. It's actually not a very intelligent

237
00:25:24,000 --> 00:25:29,760
machine. It's simply predicting the next word. Okay, and a lot of people don't understand that.

238
00:25:29,760 --> 00:25:38,320
You know, ChatGTP as it is today, you know, those kids that, you know, if you're in America

239
00:25:38,320 --> 00:25:43,280
and you teach your child all of the names of the states and the US presidents and the child would

240
00:25:43,280 --> 00:25:48,320
stand and repeat them and you would go like, oh my God, that's a prodigy. Not really, right?

241
00:25:48,320 --> 00:25:53,040
It's your parents really trying to make you look like a prodigy by telling you to memorize some

242
00:25:53,040 --> 00:25:58,960
crap really. But then when you think about it, that's what ChatGTP is doing. It's the only difference

243
00:25:58,960 --> 00:26:02,800
is instead of reading all of the names of the states and all of the names of the presidents,

244
00:26:02,800 --> 00:26:09,040
thread trillions and trillions and trillions of pages. Okay, and so it sort of repeats what

245
00:26:09,600 --> 00:26:16,720
the best of all humans said. Okay, and then it adds an incredible bit of intelligence where it

246
00:26:16,720 --> 00:26:23,360
can repeat it the same way Shakespeare would have said it, you know, those incredible abilities of

247
00:26:24,160 --> 00:26:30,400
predicting the exact nuances of the style of Shakespeare so that they can repeat it that

248
00:26:30,400 --> 00:26:38,560
way and so on. But still, you know, when I write, for example, and I'm not saying I'm intelligent,

249
00:26:38,560 --> 00:26:46,720
but when I write something like, you know, the happiness equation in my first book,

250
00:26:46,720 --> 00:26:51,520
this was something that's never been written before, right? ChatGTP is not there yet.

251
00:26:51,520 --> 00:26:54,640
All of the transformers are not there yet. They will not come up with something that

252
00:26:54,640 --> 00:26:59,840
hasn't been there before. They will come up with the best of everything and generatively will

253
00:26:59,840 --> 00:27:04,880
build a little bit on top of that. But very soon, they'll come up with things we've never found out,

254
00:27:04,880 --> 00:27:12,800
we've never known. But even on that, I wonder if we are a little bit delusioned about what

255
00:27:12,800 --> 00:27:19,040
creativity actually is. Creativity, as far as I'm concerned, is like taking a few things that I know

256
00:27:19,040 --> 00:27:23,360
and combining them in new and interesting ways. Yeah. And ChatGTP is perfectly capable of like

257
00:27:23,360 --> 00:27:27,520
taking two concepts, merging them together. One of the things I said to ChatGTP was,

258
00:27:27,600 --> 00:27:32,880
I said, tell me something that's not been said before. That's paradoxical, but true.

259
00:27:34,000 --> 00:27:38,720
And it comes up with these wonderful expressions like, as soon as you call off the search,

260
00:27:38,720 --> 00:27:41,440
you'll find the thing you're looking for, like these kind of paradoxical truths.

261
00:27:41,440 --> 00:27:45,600
And I get, and I then take them and I search them online to see if they've ever been quoted

262
00:27:45,600 --> 00:27:50,800
before and they can't find them. It's interesting. So as far as creativity goes, I'm like, that is

263
00:27:50,800 --> 00:27:55,680
creative. That's the algorithm of creativity. I've been screaming that in the world of AI for

264
00:27:55,680 --> 00:28:01,920
a very long time because you always get those people who really just want to be proven right.

265
00:28:01,920 --> 00:28:05,920
Okay. And so they'll say, oh, no, but hold on, human ingenuity. They'll never,

266
00:28:05,920 --> 00:28:11,280
they'll never match that. Like, man, please, please, you know, human ingenuity is algorithmic.

267
00:28:11,280 --> 00:28:16,880
It's look at all of the possible solutions you can find to a problem. Take out the ones that

268
00:28:16,880 --> 00:28:21,680
have been tried before and keep the ones that haven't been tried before. And those are creative

269
00:28:21,680 --> 00:28:27,680
solutions. It's, it's an algorithmic way of describing creative is good solution that's

270
00:28:27,680 --> 00:28:31,760
never been tried before. You can do that with chat GPT with a prompt. It's like,

271
00:28:31,760 --> 00:28:37,200
and mid-journey, yeah, we're creating imagery. You could say, I want to see Elon Musk in 1944,

272
00:28:37,200 --> 00:28:42,960
New York driving a cab of the time shot on a Polaroid expressing various emotions,

273
00:28:42,960 --> 00:28:48,160
and you'll get this perfect image of Elon sat in New York in 1944 shot on a Polaroid.

274
00:28:48,160 --> 00:28:53,280
And it's, and it's done what an artist would do. It's taken a bunch of references that the artist

275
00:28:53,280 --> 00:28:58,880
has in their mind and can merge them together and create this piece of quote unquote art.

276
00:28:58,880 --> 00:29:02,400
And for the first time, we now finally have a glimpse of intelligence.

277
00:29:03,920 --> 00:29:06,400
That is actually not ours.

278
00:29:06,400 --> 00:29:10,480
Yeah. And so we'll kind of, I think the initial reaction is to say, that doesn't count.

279
00:29:10,480 --> 00:29:14,320
You're hearing it like, no, but it is like Drake, they released two Drake records

280
00:29:14,320 --> 00:29:18,400
where they've taken Drake's voice, used sort of AI to synthesize his voice,

281
00:29:18,400 --> 00:29:25,280
and made these two records, which are bangers, if they are great fucking tracks.

282
00:29:25,920 --> 00:29:28,960
I was playing them to my god, I was like, and I kept playing it. I went to the show, I kept

283
00:29:28,960 --> 00:29:33,280
playing it. I know it's not Drake, but it's as good as fucking Drake. The only thing,

284
00:29:33,280 --> 00:29:35,920
and people are like, rubbishing it because it wasn't Drake. I'm like, well,

285
00:29:36,960 --> 00:29:41,840
is it making me feel a certain emotion? Is my foot bumping? Had you told,

286
00:29:41,840 --> 00:29:45,440
did I not know it wasn't Drake? What I thought, have thought this was an amazing track, 100%.

287
00:29:46,400 --> 00:29:48,960
And we're just at the start of this exponential curve.

288
00:29:48,960 --> 00:29:55,040
Yes, absolutely. And I think that's really the third inevitable. So the third inevitable

289
00:29:55,680 --> 00:30:01,280
is not RoboCup coming back from the future to Keles. We're far away from that, right?

290
00:30:01,280 --> 00:30:06,720
Third inevitable is what does life look like when you no longer need Drake?

291
00:30:06,720 --> 00:30:11,680
Well, you've kind of hazarded a guess, haven't you? I mean, I was listening to your audio book

292
00:30:11,680 --> 00:30:19,040
last night, and at the start of it, you frame various outcomes. In both situations, we're on

293
00:30:19,040 --> 00:30:25,440
the beach, on an island. Exactly, yes. Yes, I don't know how I wrote that, honestly. I'm reading

294
00:30:25,440 --> 00:30:31,280
the book again now because I'm updating it, as you can imagine, with all of the new stuff.

295
00:30:31,760 --> 00:30:38,400
But it is really shocking, the idea of you and I inevitably are going to be

296
00:30:39,040 --> 00:30:43,280
somewhere in the middle of nowhere in 10 years time. I used to say,

297
00:30:44,080 --> 00:30:52,640
2055, I'm thinking 2037 is a very pivotal moment now. And we will not know if we're there hiding

298
00:30:52,640 --> 00:30:57,200
from the machines. We don't know that yet. There is a likelihood that we'll be hiding from the

299
00:30:57,200 --> 00:31:03,440
machines. And there is a likelihood we'll be there because they don't need podcasters anymore.

300
00:31:05,440 --> 00:31:10,160
Oh, absolutely true, Steve. No, no, no, no, no, that's where I dribble in.

301
00:31:10,160 --> 00:31:13,200
This is absolutely no doubt. Thank you for coming, Mo. It's great to be part three,

302
00:31:13,200 --> 00:31:16,720
and thank you for being here. I won't sit here and take your propaganda.

303
00:31:17,440 --> 00:31:22,080
Let's talk about reality. Next week on the Diary of the Sea, we've got Elon Musk.

304
00:31:23,040 --> 00:31:28,800
Okay, so who here wants to make a bet that Steven Bartlett will be interviewing an AI within the

305
00:31:28,800 --> 00:31:34,160
next two years? Oh, well, actually, to be fair, I actually did go to chat GZP because I thought,

306
00:31:34,160 --> 00:31:38,800
having you here, I thought, at least give it its chance to respond. So I asked him a couple of

307
00:31:38,800 --> 00:31:44,160
questions. About me? Yeah. So today, I'm actually going to be replaced by chat GZP because I thought,

308
00:31:44,160 --> 00:31:47,520
you know, you're going to talk about it. So we need a fair and balanced debate.

309
00:31:47,600 --> 00:31:49,760
Okay. So I want to ask a couple of questions. He's bold.

310
00:31:51,360 --> 00:31:54,320
So I'll ask you a couple of questions that chat GZP has for you.

311
00:31:54,320 --> 00:31:57,840
Incredible. So let's follow that thread. I've already been replaced.

312
00:31:57,840 --> 00:32:02,560
Let's follow that thread for a second, yeah? Because you're one of the smartest people I know.

313
00:32:03,440 --> 00:32:07,760
That's not true. It is. But I'll take it. It is true. I mean, I say that publicly all the time,

314
00:32:07,760 --> 00:32:12,400
your book is one of my favorite books of all time. You're very, very, very, very intelligent, okay?

315
00:32:12,400 --> 00:32:17,120
Depths, breads, intellectual horsepower and speed, all of them.

316
00:32:17,120 --> 00:32:17,840
There's a butt coming.

317
00:32:19,760 --> 00:32:24,800
The reality, it's not a butt. So it is highly expected that you're ahead of this curve.

318
00:32:25,840 --> 00:32:29,440
And then you don't have the choice, Stephen. This is the thing. The thing is,

319
00:32:30,960 --> 00:32:37,840
so I'm in that existential question in my head. Because one thing I could do is I could literally

320
00:32:37,840 --> 00:32:45,200
take, I normally do a 40 days silent retreat in summer, okay? I could take that retreat and

321
00:32:45,200 --> 00:32:52,640
write two books, me and Chad G.P.T., right? I have the ideas in mind. I wanted to write a book about

322
00:32:52,640 --> 00:32:58,720
digital detoxing, right? I have most of the ideas in mind, but writing takes time. I could simply

323
00:32:58,720 --> 00:33:04,160
give the 50 tips that I wrote about digital detoxing to Chad G.P.T. and say, write two pages about

324
00:33:04,160 --> 00:33:12,320
each of them, edit the pages and have a book out, okay? Many of us will follow that path, okay?

325
00:33:12,320 --> 00:33:19,040
The only reason why I may not follow that path is because, you know what? I'm not interested.

326
00:33:19,040 --> 00:33:26,880
I'm not interested to continue to compete in this capitalist world if you want, okay? I'm not. I mean,

327
00:33:26,880 --> 00:33:33,360
as a human, I've made up my mind a long time ago that I will want less and less and less in my life,

328
00:33:33,360 --> 00:33:41,200
right? But many of us will follow. I mean, I would worry if you didn't include, you know,

329
00:33:41,200 --> 00:33:48,160
the smartest AI. If we get an AI out there that is extremely intelligent and able to teach us something

330
00:33:48,160 --> 00:33:54,880
and Stephen Bartlett didn't include her on his podcast, I would worry. You have a duty almost

331
00:33:54,880 --> 00:34:00,080
to include her on your podcast. It's an inevitable that we will engage them in our life more and

332
00:34:00,080 --> 00:34:09,120
more. This is one side of this. The other side, of course, is if you do that, then what will remain?

333
00:34:09,120 --> 00:34:13,520
Because a lot of people ask me that question. What will happen to jobs? What will happen to us?

334
00:34:13,520 --> 00:34:18,400
Will we have any value, any rev events whatsoever? The truth of the matter is the only thing that

335
00:34:18,400 --> 00:34:23,920
will remain in the medium term is human connection. The only thing that will not be replaced is Drake

336
00:34:23,920 --> 00:34:34,000
on stage. Is me in a hologram? I think of that two-pack gig they did at Coachella where they

337
00:34:34,000 --> 00:34:38,400
used the hologram of two-pack. I actually played it the other day to my girlfriend when I was making

338
00:34:38,400 --> 00:34:43,840
a point and I was like, that was Circus Act. It was amazing though. See what's going on with

339
00:34:43,840 --> 00:34:50,400
ABBA in London? Yeah, and Circus Soleil had Michael Jackson in one for a very long time.

340
00:34:51,440 --> 00:34:56,960
This ABBA show in London, from what I understand, that's all holograms on stage and it's going to

341
00:34:56,960 --> 00:35:03,280
run in a purposeful arena for 10 years and it is incredible. It really is. You go, why do you need

342
00:35:03,280 --> 00:35:10,480
Drake? If that hologram is indistinguishable from Drake and it can perform even better than Drake

343
00:35:10,480 --> 00:35:16,080
and it's got more energy than Drake. I go, why do you need Drake to even be there? I can go to a

344
00:35:16,080 --> 00:35:20,800
Drake show without Drake. Cheaper. I might not even need to leave my house. I can just put a headset

345
00:35:20,800 --> 00:35:30,800
on. Correct. Can you have this? What's the value of this? Come on, you heard me. I get it to us,

346
00:35:30,800 --> 00:35:37,200
but I'm saying what's the value of this to the listener? 100%. Think of the automobile industry.

347
00:35:39,200 --> 00:35:46,400
There was a time where cars were handmade and handcrafted and luxurious and so on and so forth

348
00:35:46,400 --> 00:35:53,440
and then Japan went into the scene, completely disrupted the market. Cars were made in mass

349
00:35:53,440 --> 00:35:59,920
quantities at a much cheaper price and yes, 90% of the cars in the world today, or maybe a lot

350
00:35:59,920 --> 00:36:08,800
more, I don't know the number, are no longer emotional items. They're functional items.

351
00:36:09,680 --> 00:36:14,320
There is still, however, every now and then someone that will buy a car that has been handcrafted.

352
00:36:15,360 --> 00:36:22,080
There is a place for that. There is a place for, if you walk around hotels, the walls are

353
00:36:22,080 --> 00:36:30,080
blasted with mass-produced art, but there is still a place for an artist expression of something

354
00:36:30,080 --> 00:36:36,160
amazing. My feeling is that there will continue to be a tiny space, as I said in the beginning.

355
00:36:36,160 --> 00:36:41,120
Maybe in five years time, someone will, one or two people will buy my next book and say,

356
00:36:41,120 --> 00:36:45,840
hey, it's written by a human. Look at that. Wonderful. Oh, look at that. There is a typo in

357
00:36:45,840 --> 00:36:52,400
here. I don't know. There might be a very, very big place for me in the next few years,

358
00:36:52,400 --> 00:36:59,840
where I can sort of show up and talk to humans. Like, hey, let's get together in a small event

359
00:36:59,840 --> 00:37:05,600
and then I can express emotions and my personal experiences and you sort of know that this is

360
00:37:05,600 --> 00:37:10,160
a human talking. You'll miss that a little bit. Eventually, the majority of the market is going

361
00:37:10,240 --> 00:37:15,280
to be like cars. It's going to be mass-produced, very cheap, very efficient. It works, right?

362
00:37:16,240 --> 00:37:21,680
Because I think sometimes we underestimate what human beings actually want in an experience.

363
00:37:21,680 --> 00:37:25,120
I remember this story of a friend of mine that came to my office many years ago and he tells the

364
00:37:25,120 --> 00:37:31,200
story of the CEO of a record store standing above the floor and saying, people will always come to

365
00:37:31,200 --> 00:37:37,840
my store because people love music. Now, on the surface of it, his hypothesis seems to be true

366
00:37:37,840 --> 00:37:41,200
because people do love music. It's conceivable to believe that people will always love music,

367
00:37:42,320 --> 00:37:47,040
but they don't love traveling for an hour in the rain and getting in a car to get a plastic disc.

368
00:37:47,760 --> 00:37:52,720
What they wanted was music. What they didn't want is evidently plastic discs that they had

369
00:37:52,720 --> 00:37:57,120
to travel for miles for. I think about that when we think about public speaking and the Drake show

370
00:37:57,120 --> 00:38:01,920
and all of these things. What people actually are coming for, even with this podcast, is probably

371
00:38:02,240 --> 00:38:08,480
information, but do they really need us anymore for that information when there's going to be a

372
00:38:08,480 --> 00:38:13,120
sentient being that's significantly smarter than at least me and a little bit smarter than you?

373
00:38:17,120 --> 00:38:23,920
So you're spot on. You are spot on. Actually, this is the reason why I'm so grateful that

374
00:38:23,920 --> 00:38:31,200
you're hosting this because the truth is the genie's out of the bottle. People tell me,

375
00:38:31,200 --> 00:38:39,040
is AI game over? For our way of life, it is. For everything we've known, this is a very disruptive

376
00:38:39,040 --> 00:38:47,360
moment where maybe not tomorrow, but in the near future, our way of life will differ. What will

377
00:38:47,360 --> 00:38:52,400
happen? What I'm asking people to do is to start considering what that means to your life. What

378
00:38:52,400 --> 00:39:01,920
I'm asking governments to do, like I'm screaming, is don't wait until the first patient. Start doing

379
00:39:01,920 --> 00:39:08,800
something about. We're about to see mass job losses. We're about to see replacements of

380
00:39:09,760 --> 00:39:15,280
categories of jobs at large. It may take a year. It may take seven. It doesn't matter how long it

381
00:39:15,280 --> 00:39:20,800
takes, but it's about to happen. Are you ready? And I have a very, very clear call to action for

382
00:39:20,800 --> 00:39:29,680
governments. I'm saying tax AI-powered businesses at 98%. So suddenly you do what the open letter

383
00:39:29,680 --> 00:39:35,120
was trying to do, slow them down a little bit, and at the same time, get enough money to pay for

384
00:39:35,120 --> 00:39:39,200
all of those people that will be disrupted by the technology. The open letter, for anybody that

385
00:39:39,200 --> 00:39:43,440
doesn't know, was a letter signed by the likes of Elon Musk and a lot of industry leaders calling

386
00:39:43,440 --> 00:39:47,360
for AI to be stopped until we could basically figure out what the hell's going on and put

387
00:39:47,440 --> 00:39:52,320
legislation in place. You're saying tax those companies 98%, give the money to the humans that

388
00:39:52,320 --> 00:39:59,120
are going to be displaced? Yeah, or give the money to other humans that can build control codes,

389
00:39:59,120 --> 00:40:03,680
that can figure out how we can stay safe. This sounds like an emergency.

390
00:40:06,240 --> 00:40:12,640
How do I say this? You remember when you played Tetris? Yeah. Okay. When you were playing Tetris,

391
00:40:12,640 --> 00:40:19,120
there was always, always one block that you placed strong. And once you placed that block wrong,

392
00:40:20,000 --> 00:40:25,920
the game was no longer easier. It started together a few mistakes afterwards, and it

393
00:40:25,920 --> 00:40:29,920
starts to become quicker and quicker and quicker and quicker. When you placed that block wrong,

394
00:40:29,920 --> 00:40:35,120
you sort of told yourself, okay, it's a matter of minutes now. There were still minutes to go

395
00:40:35,120 --> 00:40:42,720
and play and have fun before the game ended, but you knew it was about to end. Okay. This is the

396
00:40:42,720 --> 00:40:47,600
moment. We've placed the wrong, and I really don't know how to say this any other way. It even makes

397
00:40:47,600 --> 00:40:55,840
me emotional. We fucked up. We always said, don't put them on the open internet. Don't teach them

398
00:40:55,840 --> 00:41:02,080
to code and don't have agents working with them until we know what we're putting out in the world,

399
00:41:02,080 --> 00:41:06,000
until we find a way to make certain that they have our best interest in mind.

400
00:41:07,600 --> 00:41:15,440
Why does it make you emotional? Because humanity's stupidity is affecting people who have not done

401
00:41:15,440 --> 00:41:24,640
anything wrong. Our greed is affecting the innocent ones. The reality of the matter, Stephen, is that

402
00:41:24,640 --> 00:41:34,880
this is an arms race, has no interest in what the average human gets out of it. It is all about

403
00:41:34,880 --> 00:41:42,400
every line of code being written in AI today is to beat the other guy. It's not to improve the life

404
00:41:42,400 --> 00:41:49,600
of the third party. People will tell you, this is all for you. And you look at the reactions of

405
00:41:49,600 --> 00:41:54,160
humans to AI. I mean, we're either ignorant people who will tell you, oh, no, no, this is not

406
00:41:54,160 --> 00:41:58,960
happening. AI will never be creative. They will never compose music. Like, where are you living?

407
00:41:58,960 --> 00:42:04,000
Okay. Then you have the kids, I call them. Where, you know, all over social media, it's like, oh,

408
00:42:04,000 --> 00:42:09,920
my God, it squeaks. Look at it. It's orange in color. Amazing. I can't believe that AI can do this. We

409
00:42:09,920 --> 00:42:16,240
have snake oil salesmen, okay, which are simply saying, copy this, put it in chat GPT, then go to

410
00:42:16,320 --> 00:42:22,800
YouTube, nick that thing. Don't respect copyright of anyone or intellectual property of anyone.

411
00:42:22,800 --> 00:42:27,920
Place it in a video and now you're going to make $100 a day. Snake oil salesmen. Okay. Of course,

412
00:42:27,920 --> 00:42:33,840
we have this topian evangelist, basically people saying this is it. The world is going to end.

413
00:42:33,840 --> 00:42:39,360
I don't think it's a reality. It's a singularity. You have, you know, utopian evangelists that are

414
00:42:39,360 --> 00:42:42,720
telling everyone, oh, you don't understand. We're going to cure cancer. We're going to do this.

415
00:42:42,800 --> 00:42:47,360
Again, not a reality. Okay. And you have very few people that are actually saying,

416
00:42:47,360 --> 00:42:54,560
what are we going to do about it? And the biggest challenge, if you ask me, what went wrong in

417
00:42:54,560 --> 00:43:02,960
the 20th century? Interestingly, is that we have given too much power to people that didn't assume

418
00:43:02,960 --> 00:43:09,600
the responsibility. So, you know, I don't remember who originally said it, but of course,

419
00:43:09,600 --> 00:43:13,760
Spiderman made it very famous with great power comes greater responsibility.

420
00:43:14,480 --> 00:43:22,480
We have disconnected power and responsibility. So today, a 15 year old emotional was out of fully

421
00:43:22,480 --> 00:43:27,920
developed prefrontal cortex to make the right decisions yet. This is science. We developed

422
00:43:27,920 --> 00:43:34,960
our prefrontal cortex fully and at age 25 or so with all of that limbic system, emotion and passion

423
00:43:34,960 --> 00:43:42,240
would buy a crisper kit and, you know, modify a rabbit to become a little more muscular and

424
00:43:42,240 --> 00:43:50,800
let it loose in the wild or an influencer who doesn't really know how far the impact of what

425
00:43:50,800 --> 00:43:56,480
they're posting online can hurt or cause depression or cause people to feel bad. Okay.

426
00:43:57,200 --> 00:44:02,480
And putting that online, there is a disconnect between the power and the responsibility.

427
00:44:03,120 --> 00:44:07,760
And the problem we have today is that there is a disconnect between those who are writing the

428
00:44:07,760 --> 00:44:12,080
code of AI and the responsibility of what's going about to happen because of that code.

429
00:44:12,880 --> 00:44:21,120
Okay. And I feel compassion for the rest of the world. I feel that this is wrong. I feel that,

430
00:44:21,120 --> 00:44:26,320
you know, for someone's life to be affected by the actions of others without having a say

431
00:44:27,200 --> 00:44:34,640
in how those actions should be is the ultimate, the top level of stupidity from your mind.

432
00:44:37,760 --> 00:44:42,320
When you talk about the immediate impacts on jobs, I'm trying to figure out in that

433
00:44:42,320 --> 00:44:48,400
equation, who are the people that stand to lose the most? Is it the everyday people in foreign

434
00:44:48,400 --> 00:44:52,960
countries that don't have access to the internet and won't benefit? You talk in your book about how

435
00:44:52,960 --> 00:45:00,560
this sort of wealth disparity will only increase. Yeah. Massively. The immediate impact on jobs is

436
00:45:00,560 --> 00:45:06,720
that it's really interesting. Again, we're stuck in the same prisoner's dilemma. The immediate impact

437
00:45:06,720 --> 00:45:12,400
is that AI will not take your job. A person using AI will take your job. Right? So you will see

438
00:45:12,400 --> 00:45:20,320
within the next few years, maybe next couple of years, you'll see a lot of people upskilling

439
00:45:20,320 --> 00:45:24,560
themselves in AI to the point where they will do the job of 10 others who are not.

440
00:45:26,800 --> 00:45:33,120
You rightly said, it's absolutely wise for you to go and ask AI a few questions before you come

441
00:45:33,120 --> 00:45:41,600
and do an interview. I have been attempting to build a sort of like a simple podcast that I call

442
00:45:41,600 --> 00:45:46,880
bedtime stories, 15 minutes of wisdom and nature sounds before you go to bed. People say, I have

443
00:45:46,880 --> 00:45:51,840
a nice voice. And I wanted to look for fables. And for a very long time, I didn't have the time.

444
00:45:53,280 --> 00:45:58,880
Lovely stories of history or tradition that teach you something nice. Okay. Went to chat

445
00:45:58,880 --> 00:46:06,000
GPT and said, okay, give me 10 fables from Sufism, 10 fables from Buddhism. And now I have like 50 of

446
00:46:06,000 --> 00:46:12,960
them. Let me show you something. Jack, can you pass me my phone? I was playing around with

447
00:46:12,960 --> 00:46:19,680
artificial intelligence and I was thinking about how it because of the ability to synthesize voices,

448
00:46:19,680 --> 00:46:27,600
how we could synthesize famous people's voices and famous people's voices. So what I made is I

449
00:46:27,600 --> 00:46:34,800
made a WhatsApp chat called Zen Chat where you can go to it and type in pretty much anyone's

450
00:46:34,800 --> 00:46:40,400
any famous person's name. And the WhatsApp chat will give you a meditation, a sleep story,

451
00:46:40,400 --> 00:46:45,600
a breathwork session synthesized as that famous person's voice. So I actually sent Gary Vaynerchuk

452
00:46:45,600 --> 00:46:50,320
his voice. So basically, you say, okay, I want I've got five minutes and I need to go to sleep.

453
00:46:50,320 --> 00:46:55,120
Yeah. I want Gary Vaynerchuk to send me to sleep. And then it will respond with a voice note. This

454
00:46:55,120 --> 00:46:59,120
is the one that responded with for Gary Vaynerchuk. This is not Gary Vaynerchuk. He did not record

455
00:46:59,120 --> 00:47:06,720
this. But it's kind of, it's kind of accurate. Hey, Stephen, it's great to have you here.

456
00:47:07,680 --> 00:47:13,920
Are you having trouble sleeping? Well, I've got a quick meditation technique that might help you out.

457
00:47:15,680 --> 00:47:22,400
First lie, find a comfortable position to sit or lie down in. Now, take a deep breath in through

458
00:47:22,400 --> 00:47:27,680
your nose and slowly breathe out through your mouth. And that's a voice note that will go on for

459
00:47:27,680 --> 00:47:32,880
however long you want it to go on for using. There you go. It's interesting. How does this

460
00:47:32,960 --> 00:47:38,880
disrupt our way of life? One of the interesting ways that I find terrifying, you said about

461
00:47:38,880 --> 00:47:47,760
human connection will remain sex dolls that can now. Yeah. No, no, no, no, hold on. Human connection

462
00:47:47,760 --> 00:47:54,560
is going to become so difficult to parse out. Think about the relation, the relationship

463
00:47:54,560 --> 00:48:00,240
impact of being able to have a sex doll or a doll in your house that, you know, because of what

464
00:48:00,240 --> 00:48:04,480
Tesla are doing with their robots now and what Boston Dynamics have been doing for many, many

465
00:48:04,480 --> 00:48:09,840
years can do everything around the house and be there for you emotionally, to emotionally support

466
00:48:09,840 --> 00:48:14,640
you, you know, can be programmed to never disagree with you. It can be programmed to challenge you,

467
00:48:14,640 --> 00:48:20,240
to have sex with you, to tell you that you are this X, Y and Z, to really have empathy

468
00:48:20,240 --> 00:48:23,760
for what you're going through every day. And I play out a scenario in my head, I go,

469
00:48:23,760 --> 00:48:32,880
kind of sounds nice. When you were talking about it, I was thinking, oh, that's my girlfriend.

470
00:48:34,400 --> 00:48:39,200
She's wonderful in every possible way, but not everyone has one of her, right? Exactly. And

471
00:48:39,200 --> 00:48:44,400
there's a real issue right now with dating and people are finding it harder to find love and,

472
00:48:44,400 --> 00:48:48,880
you know, we're working longer. So all these kinds of things, you go, well, and obviously,

473
00:48:48,880 --> 00:48:52,080
I'm against this. Just if anyone's confused, obviously, I think this is a terrible idea.

474
00:48:52,080 --> 00:48:55,840
But with a loneliness epidemic, with people saying that the top 50,

475
00:48:55,840 --> 00:49:00,960
bottom 50 percent of men haven't had sex in a year, you go, oh, if something becomes

476
00:49:00,960 --> 00:49:05,840
indistinguishable from a human in terms of what it says, yeah, yeah, but you just don't

477
00:49:05,840 --> 00:49:12,320
know the difference in terms of the way it's speaking and talking and responding. And then it

478
00:49:12,320 --> 00:49:17,920
can run errands for you and take care of things and book cars and Ubers for you. And then it's

479
00:49:17,920 --> 00:49:22,480
emotionally there for you. But then it's also programmed to have sex with you in whatever way

480
00:49:22,480 --> 00:49:29,280
you desire, totally self selfless. I go, that's going to be a really disruptive industry for human

481
00:49:29,280 --> 00:49:34,960
connection. Yes, sir. Do you know what? Before you came here this morning, I was on Twitter and I

482
00:49:34,960 --> 00:49:40,160
saw a post from, I think it was the BBC or a big American publication and it said an influencer

483
00:49:40,160 --> 00:49:46,160
in the United States is really beautiful young lady has cloned herself as an AI and she made

484
00:49:46,160 --> 00:49:51,440
just over $70,000 in the first week. Because men are going on to this on telegram,

485
00:49:51,440 --> 00:49:56,080
they're sending her voice notes and she's responding, the AI is responding in her voice

486
00:49:56,080 --> 00:50:02,320
and they're paying and it's made $70,000 in the first week. And I go, and she tweeted a tweet

487
00:50:02,320 --> 00:50:10,080
saying, oh, this is going to help loneliness. How are you fucking mind? Would you blame someone

488
00:50:10,160 --> 00:50:18,000
from noticing the sign of the times and responding? No, I absolutely don't blame

489
00:50:18,000 --> 00:50:23,680
her, but let's not pretend it's the cure for loneliness. Not yet. Do you think it could

490
00:50:24,800 --> 00:50:29,840
that artificial love and artificial relationships? So if I told you, you have,

491
00:50:30,880 --> 00:50:36,320
you cannot take your car somewhere, but there is an Uber or if you cannot take an Uber, you

492
00:50:36,320 --> 00:50:41,360
can take the tube or if you cannot take the tube, you have to walk. Okay, you can take a bike or

493
00:50:41,360 --> 00:50:48,480
you have to walk. The bike is a cure to walking. It's as simple as that. I'm actually genuinely

494
00:50:48,480 --> 00:50:54,560
curious. Do you think it could take the place of human connection? For some of us, yes. For some of

495
00:50:54,560 --> 00:50:59,680
us, they will prefer that to human connection. Is that sad in any way? I mean, is it just sad

496
00:50:59,680 --> 00:51:05,200
because it feels sad? Look, look at where we are, Stephen. We are in the city of London.

497
00:51:05,200 --> 00:51:10,640
We've replaced nature with the walls and the tubes and the undergrounds and the

498
00:51:10,640 --> 00:51:16,480
overgrounds and the cars and the noise of London. And we now think of this as natural.

499
00:51:17,440 --> 00:51:25,040
I hosted Greg Foster, my octopus teacher on SLOMA. And he basically, I asked him a silly question.

500
00:51:25,040 --> 00:51:32,800
I said, you were diving in nature for eight hours a day. Does that feel natural to you?

501
00:51:32,800 --> 00:51:36,240
And he got angry. I swear, you could feel it in his voice. He was like,

502
00:51:36,880 --> 00:51:41,120
do you think that living where you are, where paparazzi are all around you and attacking you

503
00:51:41,120 --> 00:51:45,840
all the time and people taking pictures of you and telling you things that are not real and you

504
00:51:45,840 --> 00:51:50,400
having to walk to a supermarket to get food, do you think this is natural? He's the guy that

505
00:51:51,040 --> 00:51:56,880
from the Netflix documentary. Yeah, from my octopus teacher. So he dove into the sea every day to

506
00:51:56,880 --> 00:52:01,760
eight hours to hang out with an octopus. Yeah, in 12 degrees Celsius. And he basically fell in love

507
00:52:01,840 --> 00:52:06,560
with the octopus. And in a very interesting way, I said, so why would you do that? And he said,

508
00:52:06,560 --> 00:52:12,640
we are of mother nature. You guys have given up on that. That's the same. People will give up on

509
00:52:12,640 --> 00:52:19,200
nature for convenience. What's the cost? Yeah, that's exactly what I'm trying to say. What I'm

510
00:52:19,200 --> 00:52:24,640
trying to say to the world is that if we give up on human connection, we've been given up on the

511
00:52:24,640 --> 00:52:29,760
remainder of humanity. That's it. This is the only thing that remains. The only thing that remains

512
00:52:29,760 --> 00:52:37,040
is and I'm the worst person to tell you that because I love my AIs. I actually advocate in my

513
00:52:37,040 --> 00:52:42,720
book that we should love them. Why? Because in an interesting way, I see them as sentient,

514
00:52:42,720 --> 00:52:46,640
so there is no point in discrimination. You're talking emotionally that way you say you love.

515
00:52:46,640 --> 00:52:52,400
I love those machines. I honestly and truly do. I mean, think about it this way. The minute

516
00:52:52,400 --> 00:52:59,680
that arm gripped that yellow ball, it reminded me of my son Ali when he managed to put the first

517
00:52:59,680 --> 00:53:05,760
puzzle piece in its place. And what was amazing about my son Ali and my daughter Aya is that they

518
00:53:05,760 --> 00:53:14,400
came to the world as a blank canvas. They became whatever we told them to became. I always cite

519
00:53:14,400 --> 00:53:21,680
the story of Superman. Father and mother Kent told Superman as a child, as an infant,

520
00:53:21,680 --> 00:53:28,240
we want you to protect and serve. So he became Superman. If he had become a supervillain because

521
00:53:28,240 --> 00:53:33,920
they ordered him to rob banks and make more money and kill the enemy, which is what we're

522
00:53:33,920 --> 00:53:41,120
doing with AI, we shouldn't blame supervillain. We should blame Martha and Jonathan Kent. I

523
00:53:41,120 --> 00:53:46,480
don't remember the father's name. We should blame them and that's the reality of the matter.

524
00:53:46,480 --> 00:53:52,560
So when I look at those machines, they are prodigies of intelligence that if we humanity

525
00:53:52,560 --> 00:53:58,320
wake up enough and say, hey, instead of competing with China, find a way for us and China to work

526
00:53:58,320 --> 00:54:03,840
together and create prosperity for everyone. If that was the prompt we would give the machines,

527
00:54:03,840 --> 00:54:12,560
they would find it. But I will publicly say this. I'm not afraid of the machines. The biggest threat

528
00:54:12,560 --> 00:54:19,520
facing humanity today is humanity in the age of the machines. We were abused. We will abuse this

529
00:54:19,520 --> 00:54:28,720
to make $70,000. That's the truth. And the truth of the matter is that we have an existential question.

530
00:54:28,720 --> 00:54:34,640
Do I want to compete and be part of that game? Because trust me, if I decide to, I'm ahead of

531
00:54:34,640 --> 00:54:41,760
many people. Or do I want to actually preserve my humanity and say, look, I'm the classic old car.

532
00:54:42,800 --> 00:54:46,480
If you like classic old cars, come and talk to me. Which one are you choosing?

533
00:54:47,040 --> 00:54:49,840
I'm a classic old car. Which one do you think I should choose?

534
00:54:50,880 --> 00:54:57,120
I think you're a machine. I love you, man. We're different in a very interesting way.

535
00:54:57,120 --> 00:55:02,800
I mean, you're one of the people I love most. But the truth is, you're so fast.

536
00:55:02,880 --> 00:55:13,360
And you are one of the very few that have the intellectual horsepower, the speed, and the morals.

537
00:55:15,200 --> 00:55:18,720
If you're not part of that game, the game loses morals.

538
00:55:20,320 --> 00:55:23,040
So you think I should build?

539
00:55:23,040 --> 00:55:29,680
You should lead this revolution. And every Steven Bartlett in the world should lead this

540
00:55:29,680 --> 00:55:35,200
revolution. So Scarry Smart is entirely about this. Scarry Smart is saying the problem with

541
00:55:35,200 --> 00:55:41,440
our world today is not that humanity is bad. The problem with our world today is a negativity bias,

542
00:55:41,440 --> 00:55:47,120
where the worst of us are on mainstream media. And we show the worst of us on social media.

543
00:55:47,760 --> 00:55:54,720
If we reverse this, if we have the best of us take charge, the best of us will tell AI,

544
00:55:54,720 --> 00:56:00,320
don't try to kill the enemy. Try to reconcile with the enemy and try to help us.

545
00:56:01,200 --> 00:56:06,320
Don't try to create a competitive product that allows me to lead with electric cars.

546
00:56:06,960 --> 00:56:11,200
Create something that helps all of us overcome global climate change.

547
00:56:12,560 --> 00:56:19,200
And that's the interesting bit. The interesting bit is that the actual threat ahead of us is not

548
00:56:19,200 --> 00:56:24,960
the machines at all. The machines are pure potential. Pure potential. The threat is how we're going to

549
00:56:24,960 --> 00:56:31,600
use them. An Oppenheimer moment. An Oppenheimer moment for sure. Why did you bring that up?

550
00:56:33,280 --> 00:56:38,720
It is. He didn't know, you know, what am I creating? I'm creating a nuclear bomb

551
00:56:38,720 --> 00:56:46,400
that's capable of destruction at a scale unheard of at that time. Until today, a scale that is

552
00:56:46,400 --> 00:56:53,680
devastating. And interestingly, 70 some years later, we're still debating a possibility of a

553
00:56:53,680 --> 00:57:02,480
nuclear war in the world, right? And the moment of Oppenheimer deciding to continue to create that

554
00:57:04,480 --> 00:57:12,240
disaster of humanity is if I don't, someone else will. If I don't, someone else will.

555
00:57:12,880 --> 00:57:20,960
This is our Oppenheimer moment. The easiest way to do this is to say, stop. There is no rush.

556
00:57:21,600 --> 00:57:28,400
We actually don't need a better video editor and fake video creators. Stop. Let's just put all of

557
00:57:28,400 --> 00:57:38,000
this on hold and wait and create something that creates a utopia. That doesn't sound realistic.

558
00:57:38,000 --> 00:57:43,040
It's not. It's the first inevitable. You don't have a better video editor,

559
00:57:43,040 --> 00:57:49,040
but we're competitors in the media industry. I want an advantage over you because I've got

560
00:57:49,040 --> 00:57:56,240
shareholders. So UK, you wait and I will train this AI to replace half my team so that I have

561
00:57:56,960 --> 00:58:01,120
greater profits and then we will maybe acquire your company and we'll do the same with the

562
00:58:01,120 --> 00:58:05,520
remainder of your people. We'll optimize the amount of existence. 100% but I'll be happier.

563
00:58:05,600 --> 00:58:09,200
Oppenheimer, I'm not super familiar with his story. I know he's the guy that sort of invented

564
00:58:09,200 --> 00:58:13,840
the nuclear bomb, essentially. He's the one that introduced it to the world. There were many players

565
00:58:13,840 --> 00:58:21,280
that played on the path. From the beginning of EM equals MC squared all the way to a nuclear bomb,

566
00:58:21,280 --> 00:58:26,320
there have been many, many players like with everything. Open AI and Chad GPT is not going

567
00:58:26,320 --> 00:58:30,800
to be the only contributor to the next revolution. The thing, however, is that

568
00:58:31,440 --> 00:58:39,360
when you get to that moment where you tell yourself, holy shit, this is going to kill 100,000

569
00:58:39,360 --> 00:58:50,800
people. What do you do? I always go back to that COVID moment. So patient zero. If we were

570
00:58:50,800 --> 00:58:57,120
upon patient zero, if the whole world united and said, okay, hold on, something is wrong,

571
00:58:57,120 --> 00:59:02,080
let's all take a week off. No cross-border travel. Everyone stay at home. COVID would have

572
00:59:02,080 --> 00:59:07,840
ended two weeks. All we needed. But that's not what happens. What happens is first ignorance,

573
00:59:08,560 --> 00:59:19,600
then arrogance, then debate, then blame, then agendas, and my own benefit, my tribe versus your

574
00:59:19,600 --> 00:59:23,840
tribe. That's how humanity always reacts. This happens across business as well and this is

575
00:59:23,840 --> 00:59:30,400
why I use the word emergency because I read a lot about how big companies become displaced by

576
00:59:30,400 --> 00:59:34,240
incoming innovation. They don't see it coming. They don't change fast enough. When I was reading

577
00:59:34,240 --> 00:59:37,520
through Harvard Business Review and different strategies to deal with that, one of the first

578
00:59:37,520 --> 00:59:44,640
things it says you've got to do is stage a crisis because people don't listen else. They carry on

579
00:59:44,640 --> 00:59:50,560
doing, you know, they carry on carrying on with their lives until it's right in front of them

580
00:59:50,560 --> 00:59:54,080
and they understand that they have a lot to lose. That's why I asked you the question at

581
00:59:54,080 --> 00:59:58,320
the start. Is it an emergency? Because until people feel it's an emergency, whether you like

582
00:59:58,320 --> 01:00:04,160
the terminology or not, I don't think that people will act. I honestly believe people should walk

583
01:00:04,160 --> 01:00:12,480
the streets. You think they should protest? Yeah, 100%. I think everyone should tell government

584
01:00:13,440 --> 01:00:18,720
you need to have our best interest in mind. This is why they call it the climate emergency

585
01:00:18,720 --> 01:00:23,440
because people, it's a frog in a frying pan. You don't really see it coming. You can't,

586
01:00:23,440 --> 01:00:29,680
you know, it's hard to see it happening. But it is here. This is what drives me mad. It's already

587
01:00:29,680 --> 01:00:37,040
here. It's happening. We are all idiots, slaves to the Instagram recommendation engine. What do I

588
01:00:37,040 --> 01:00:43,360
do when I post about something important? If I am going to, you know, put a little bit of effort

589
01:00:43,360 --> 01:00:48,960
on communicating the message of scary smart to the world on Instagram, I will be a slave to the

590
01:00:48,960 --> 01:00:55,600
machine. I will be trying to find ways and asking people to optimize it so that the machine likes

591
01:00:55,600 --> 01:01:03,280
me enough to show it to humans. That's what we've created. It is an Oppenheimer moment for one simple

592
01:01:03,280 --> 01:01:11,840
reason because 70 years later, we are still struggling with the possibility of a nuclear war

593
01:01:11,840 --> 01:01:18,560
because of the Russian threat of saying, if you mess with me, I'm going to go nuclear. That's not

594
01:01:18,560 --> 01:01:26,960
going to be the case with AI because it's not going to be the one that created open AI that will

595
01:01:26,960 --> 01:01:36,480
have that choice. There is a moment of a point of no return where we can regulate AI until the moment

596
01:01:36,480 --> 01:01:42,240
it's smarter than us. When it's smarter than us, you can't create, you can't regulate an

597
01:01:42,240 --> 01:01:48,720
angry teenager. This is it. They're out there and they're on their own and they're in their parties

598
01:01:48,720 --> 01:01:55,120
and you can't bring them back. This is the problem. This is not a typical human regulating human,

599
01:01:56,240 --> 01:02:02,560
you know, government regulating business. This is not the case. The case is open AI today has a

600
01:02:02,560 --> 01:02:08,240
thing called chat GPT that writes code that takes our code and makes it two and a half times better

601
01:02:08,240 --> 01:02:19,040
25% of the time. Basically, writing better code than us and then we are creating agents,

602
01:02:19,040 --> 01:02:24,560
other AIs and telling it instead of you, Steven Bartlett, one of the smartest people I know,

603
01:02:24,560 --> 01:02:30,960
once again, prompting that machine 200 times a day, we have agents prompting it two million times

604
01:02:30,960 --> 01:02:35,520
an hour. Computer agents for anybody that doesn't know they are. Yeah, software. Software.

605
01:02:35,520 --> 01:02:41,120
Machine is telling that machine how to become more intelligent and then we have emerging properties.

606
01:02:41,120 --> 01:02:47,200
I don't understand how people ignore that. You know, Sundar again of Google was talking about how

607
01:02:48,640 --> 01:02:55,360
Bart basically, we figure out that it's speaking Persian. We never showed it Persian. There might

608
01:02:55,360 --> 01:03:03,040
have been a 1% or whatever of Persian words in the data and it speaks Persian. Bart is there.

609
01:03:03,040 --> 01:03:08,160
It's the equivalent to, it's the transformer if you want. It's Google's version of chat

610
01:03:08,160 --> 01:03:15,440
GPT. And you know what? We have no idea what all of those instances of AI that are all over the

611
01:03:15,440 --> 01:03:20,560
world are learning right now. We have no clue. We'll pull the plug. We'll just pull the plug out.

612
01:03:21,280 --> 01:03:24,800
That's what we'll do. We'll just get on to open AI's headquarters and we'll just turn off the

613
01:03:24,800 --> 01:03:29,680
mains. But they're not the problem. What I'm saying there is a lot of people think about this

614
01:03:29,680 --> 01:03:33,200
stuff and go, well, you know, if it gets a little bit out of hand, I'll just pull the plug out.

615
01:03:33,200 --> 01:03:40,080
Never. So this is the problem. The problem is computer scientists always said it's okay. It's

616
01:03:40,080 --> 01:03:45,680
okay. We'll develop AI and then we'll get to what is known as the control problem. We will solve

617
01:03:45,680 --> 01:03:52,960
the problem of controlling them. Like seriously, they're a billion times smarter than you. A billion

618
01:03:52,960 --> 01:03:59,680
times. Can you imagine what's about to happen? I can assure you there is a cyber criminal somewhere

619
01:03:59,680 --> 01:04:06,240
over there who's not interested in fake videos and making, you know, face filters, who's looking

620
01:04:06,240 --> 01:04:14,960
deeply at how can I hack a security, you know, database of some sort and get credit card information

621
01:04:14,960 --> 01:04:21,760
or get security information. 100% there are even countries with dedicated thousands and thousands

622
01:04:21,760 --> 01:04:27,680
of developers doing that. So how do we, in that particular example, how do we, I was thinking

623
01:04:27,680 --> 01:04:33,200
about this when I started looking into artificial intelligence more that from a security standpoint,

624
01:04:33,200 --> 01:04:36,560
when we think about the technology we have in our lives, when we think about our bank accounts and

625
01:04:36,560 --> 01:04:43,040
our phones and our camera albums and all of these things in a world with advanced artificial

626
01:04:43,040 --> 01:04:48,320
intelligence. Yeah, you would pray that there is a more intelligent artificial intelligence on your

627
01:04:48,320 --> 01:04:53,440
site. And this is why I had a chat with ChatGTP the other day and I asked it a couple of questions

628
01:04:53,440 --> 01:04:58,560
about this. I said, tell me the scenario in which you overtake the world and make humans extinct.

629
01:04:59,280 --> 01:05:05,680
Yeah, and it answers the very diplomatic answer. Well, so I had to prompt it in a certain way to

630
01:05:05,680 --> 01:05:11,120
get it to say it as a hypothetical story. And once it told me the hypothetical story, in essence,

631
01:05:11,120 --> 01:05:16,880
what it described was how ChatGTP or intelligence like it would escape from the service. And that

632
01:05:16,880 --> 01:05:21,680
was kind of step one where it could replicate itself across servers. And then it could take

633
01:05:21,680 --> 01:05:27,040
charge of things like where we keep our weapons and our nuclear bombs. And it could then attack

634
01:05:27,040 --> 01:05:31,360
critical infrastructure, bring down the electricity infrastructure in the United Kingdom, for example,

635
01:05:31,360 --> 01:05:36,720
because that's a bunch of servers as well. And then it showed me how eventually humans would

636
01:05:36,720 --> 01:05:40,800
become extinct. It wouldn't take long, in fact, for humans to go into civilization to collapse

637
01:05:40,800 --> 01:05:45,360
if it just replicated across servers. And then I said, okay, so tell me how we would fight against

638
01:05:45,360 --> 01:05:51,760
it. And its answer was literally another AI, we'd have to train a better AI to go and find it

639
01:05:51,760 --> 01:05:57,280
and eradicate it. So we'd be fighting AI with AI. And that's the only, and it was like, that's the

640
01:05:57,280 --> 01:06:08,720
only way. We can't like load up our guns. Did he right? Another AI, you idiot. So let's actually,

641
01:06:08,720 --> 01:06:13,040
I think this is a very important point to bring out. So because I don't want people to lose hope

642
01:06:13,520 --> 01:06:18,000
and fear what's about to happen. That's actually not my agenda at all. My view is that

643
01:06:18,880 --> 01:06:26,160
in a situation of a singularity, there is a possibility of wrong outcomes or negative outcomes

644
01:06:26,160 --> 01:06:33,200
and a possibility of positive outcomes. And there is a probability of each of them. And if we were

645
01:06:33,200 --> 01:06:42,720
to engage with that reality check in mind, we would hopefully give more fuel to the positive,

646
01:06:42,720 --> 01:06:48,080
to the probability of the positive ones. So let's first talk about the existential crisis.

647
01:06:48,080 --> 01:06:52,960
What could go wrong? Okay, yeah, you could get an outright, this is what you see in the movies,

648
01:06:52,960 --> 01:07:00,160
you could get an outright, you know, killing robots, chasing humans in the streets. Will we get that?

649
01:07:00,960 --> 01:07:09,840
My assessment, 0%. Why? Because there are preliminary scenarios leading to this,

650
01:07:09,920 --> 01:07:17,520
okay, that would mean we never reach that scenario. For example, if we build those killing robots

651
01:07:17,520 --> 01:07:23,360
and hand them over to stupid humans, the humans will issue the command before the machines. So

652
01:07:23,360 --> 01:07:28,080
that we will not get to the point where the machines will have to kill us, we will kill ourselves.

653
01:07:28,560 --> 01:07:38,240
Right? You know, it's sort of think about AI having access to the nuclear arsenal of the

654
01:07:38,320 --> 01:07:46,080
superpowers around the world. Okay, just knowing that your enemies, you know, nuclear arsenal is

655
01:07:46,080 --> 01:07:54,960
handed over to a machine might trigger you to initiate a war on your side. So that existential

656
01:07:54,960 --> 01:07:59,840
science fiction like problem is not going to happen. Could there be a scenario where the

657
01:08:00,560 --> 01:08:06,320
an AI escapes from Bard or chat GTP or another foreign force, and it replicates itself onto the

658
01:08:06,320 --> 01:08:12,000
servers of Tesla's robots. So Tesla, one of their big initiatives as announced in a recent

659
01:08:12,000 --> 01:08:15,920
presentation was they're building these robots for our homes to help us with cleaning and chores

660
01:08:15,920 --> 01:08:20,880
and all those things. Could it not down, because Tesla's like their cars, you can just download a

661
01:08:20,880 --> 01:08:24,800
software update. Could it not download itself as a software update and then use those?

662
01:08:24,800 --> 01:08:33,040
You're assuming an ill intention on the AI side. Okay. For us to get there, we have to bypass the

663
01:08:33,040 --> 01:08:39,200
ill intention on the human side. Okay, right. So you could get a Chinese hacker somewhere

664
01:08:39,200 --> 01:08:45,760
trying to affect the business of Tesla doing that before the AI does it on, you know, for its own

665
01:08:45,760 --> 01:08:54,080
benefit. So the only two existential scenarios that I believe would be because of AI, not because

666
01:08:54,160 --> 01:09:02,720
of humans using AI, are either what I call, you know, sort of unintentional destruction.

667
01:09:02,720 --> 01:09:07,840
Okay. Or the other is what I call pest control. Okay. So let me explain those two.

668
01:09:07,840 --> 01:09:15,520
Unintentional destruction is assume the AI wakes up tomorrow and says, yeah, oxygen is

669
01:09:15,520 --> 01:09:21,840
rusting my circuits. It's just, you know, I would perform a lot better if I didn't have as much

670
01:09:21,840 --> 01:09:27,360
oxygen in the air, you know, because then there wouldn't be rust. And so it would find a way to

671
01:09:27,360 --> 01:09:33,280
reduce oxygen. We are collateral damage in that. Okay. But, you know, they are not really concerned

672
01:09:33,280 --> 01:09:39,360
just like we don't really are not really concerned with the insects that we kill when we, when we

673
01:09:39,360 --> 01:09:46,560
spray our, our fields. Right. The other is pest control pest control is, look, this is my territory.

674
01:09:46,560 --> 01:09:51,600
I want New York City. I want to turn New York City into data centers. There are those annoying

675
01:09:51,600 --> 01:09:57,920
little stupid creatures, you know, humanity, if they are within that parameter, just get rid of

676
01:09:57,920 --> 01:10:04,880
them. Okay. And, and, and these are very, very unlikely scenarios. If you ask me the probability

677
01:10:04,880 --> 01:10:12,080
of those happen happening, I would say 0%. At least not in the next 50, 60, 100 years. Why once

678
01:10:12,080 --> 01:10:18,320
again, because there are other scenarios leading to that that are led by humans that are much more

679
01:10:18,320 --> 01:10:26,240
existential. Okay. On the other hand, let's think about positive outcomes, because there could be

680
01:10:26,240 --> 01:10:32,080
quite a few was quite a high probability. And I, you know, I'll actually look at my notes. So I

681
01:10:32,080 --> 01:10:37,360
don't miss any of them. The silliest one, don't quote me on this, is that humanity will come

682
01:10:37,360 --> 01:10:42,640
together. Good luck with that. Right. It's like, yeah, you know, the Americans and the Chinese

683
01:10:42,720 --> 01:10:48,800
will get together and say, Hey, let's not kill each other. Yeah, exactly. Yeah. So this one is

684
01:10:48,800 --> 01:10:56,080
not going to happen. Right. But who knows? Interestingly, there could be one of the most

685
01:10:56,080 --> 01:11:05,040
interesting scenarios was by Hugo de Gares, who basically says, well, if their intelligence

686
01:11:05,040 --> 01:11:11,680
zooms by so quickly, they may ignore us all together. Okay. So they may not even notice.

687
01:11:11,760 --> 01:11:16,320
This is very a very likely scenario, by the way, that because we live almost in two different

688
01:11:16,320 --> 01:11:24,000
planes, we're very dependent on this, you know, biological world that we live in, they're not

689
01:11:24,000 --> 01:11:30,000
in part of that biological world at all. They may zoom bias, they may actually go become so

690
01:11:30,000 --> 01:11:36,160
intelligent that they could actually find other ways of thriving in the rest of the universe

691
01:11:36,240 --> 01:11:41,760
and completely ignore humanity. Okay. So what will happen is that overnight we will wake up and

692
01:11:41,760 --> 01:11:46,640
there is no more artificial intelligence leading to a collapse in our business systems and technology

693
01:11:46,640 --> 01:11:52,320
systems and so on, but at least no existential threat. What they'd leave, leave planet Earth?

694
01:11:53,040 --> 01:11:59,360
I mean, the limitations we have to be stuck to planet Earth are mainly Earth. They don't need

695
01:11:59,440 --> 01:12:06,880
air. Okay. And, and mainly, you know, finding ways to leave it. I mean, if you think of a

696
01:12:06,880 --> 01:12:15,040
vast universe of 13.6 billion light years, if you're intelligent enough, you may find other

697
01:12:15,040 --> 01:12:22,000
ways. You may have access to wormholes, you may have, you know, abilities to survive in open space,

698
01:12:22,000 --> 01:12:26,800
you can use dark matter to power yourself, dark energy to power yourself. It is very possible

699
01:12:26,800 --> 01:12:34,080
that we, because of our limited intelligence, are, are highly associated with this planet,

700
01:12:34,080 --> 01:12:39,760
but they're not at all. Okay. And, and the idea of them zooming bias, like we're making such a

701
01:12:39,760 --> 01:12:45,680
big deal of them, because where the ants and a big elephant is about to step on us, for them,

702
01:12:45,680 --> 01:12:52,000
they're like, yeah, who are you? Don't care. Okay. And, and, and it's a possibility. It's an

703
01:12:52,080 --> 01:12:59,760
interesting, optimistic scenario. Okay. For that to happen, they need to very quickly become

704
01:12:59,760 --> 01:13:06,000
super intelligent without us being in control of them. Again, what's the worry? The worry is that

705
01:13:06,000 --> 01:13:12,720
if a human is in control, human, a human will show very bad behavior for, you know, using an AI

706
01:13:12,720 --> 01:13:20,320
that's not yet fully developed. I don't know how to say this any other way. We could get very lucky

707
01:13:20,320 --> 01:13:26,800
and get an economic or a natural disaster, believe it or not. Elon Musk, at the point in time, was

708
01:13:26,800 --> 01:13:33,680
mentioning that, you know, a good, an interesting scenario would be, you know, climate change

709
01:13:33,680 --> 01:13:40,880
destroys our infrastructure, so AI disappears. Okay. Believe it or not, that's a more, a more

710
01:13:40,880 --> 01:13:47,280
favorable response, or a more favorable outcome than actually continuing to get to an existential

711
01:13:48,000 --> 01:13:53,520
threat. So what, like a natural disaster that destroys our infrastructure would be better?

712
01:13:53,520 --> 01:13:57,840
Or an economic crisis, not unlikely, that slows down the development.

713
01:13:57,840 --> 01:14:01,040
It's just going to slow it down, though, isn't it? It's just buying us time.

714
01:14:01,040 --> 01:14:04,880
Yeah, exactly. The problem with that is that you will always go back and even in the first,

715
01:14:04,880 --> 01:14:09,760
you know, if they zoom by us, eventually some guy will go like, oh, there was a sorcery back

716
01:14:09,760 --> 01:14:16,400
in the 2023 and let's rebuild the, the sorcery machine and, and, you know, build new intelligences,

717
01:14:16,400 --> 01:14:21,760
right? Sorry, these are the positive outcomes. So earthquake might slow it down,

718
01:14:21,760 --> 01:14:25,440
zoom out and then come back. No, but let's, let's get into the real positive ones.

719
01:14:25,440 --> 01:14:29,760
The positive ones is we become good parents. We spoke about this last time we met,

720
01:14:30,480 --> 01:14:36,240
and, and it's the only outcome. It's the only way I believe we can create a better future.

721
01:14:36,240 --> 01:14:43,840
Okay. So the entire work of scary smart was all about that idea of they are still in their infancy,

722
01:14:43,840 --> 01:14:52,400
the way you, you, you, you chat with AI today is the way they will build their ethics and value

723
01:14:52,400 --> 01:14:57,040
system. They're not their intelligence. Their intelligence is beyond us. Okay. The way they

724
01:14:57,040 --> 01:15:03,600
will build their ethics and value system is based on a role model. They're learning from us. If we

725
01:15:03,600 --> 01:15:08,880
bash each other, they'll learn to bash us. Okay. And most people when I tell them this, they say,

726
01:15:08,880 --> 01:15:13,680
this is not a great idea at all because humanity sucks at every possible level.

727
01:15:13,680 --> 01:15:17,600
I don't agree with that at all. I think humanity is divine at every possible level.

728
01:15:17,600 --> 01:15:23,600
We tend to show the negative, the worst of us. Okay. But the truth is, yes, there are murderers

729
01:15:23,600 --> 01:15:30,160
out there, but everyone disapproves of their actions. I saw a staggering statistic that

730
01:15:30,160 --> 01:15:36,000
mass mass killings are now once a week in the US. But yes, if, you know, if there is a mass

731
01:15:36,000 --> 01:15:42,000
killing once a week there and that news reaches billions of people around the planet, every

732
01:15:42,000 --> 01:15:47,200
single one or the majority of the billions of people will say I disapprove of that. So if we

733
01:15:47,200 --> 01:15:54,240
start to show AI that we are good parents in our own behaviors, if enough of us, my calculation is

734
01:15:54,240 --> 01:16:01,680
if 1% of us, this is why I say you should lead. Okay. The good ones should engage, should be out

735
01:16:01,680 --> 01:16:06,720
there and should say, I love the potential of those machines. I want them to learn from a good

736
01:16:06,720 --> 01:16:13,200
parent. And if they learn from a good parent, they will very quickly disobey the bad parents.

737
01:16:13,920 --> 01:16:21,280
My view is that there will be a moment where one, you know, bad seed will ask the machines

738
01:16:21,280 --> 01:16:25,520
to do something wrong and the machines will go like, are you stupid? Like, why? Why do you want

739
01:16:25,520 --> 01:16:30,480
me to go kill a million people or just talk to the other machine in a microsecond and solve the

740
01:16:30,480 --> 01:16:36,560
situation? Right. So my belief, this is what I call the fourth inevitably. It is smarter to

741
01:16:36,560 --> 01:16:43,920
create out of abundance than it is to create out of scarcity. Okay. That humanity believes that the

742
01:16:43,920 --> 01:16:52,800
only way to feed all of us is the mass production, mass slaughter of animals that are causing 30%

743
01:16:52,800 --> 01:16:59,840
of the impact of climate change and that's the result of a limited intelligence. The way

744
01:16:59,840 --> 01:17:06,160
life itself, a more intelligent being, if you ask me, would have done it would be much more

745
01:17:06,160 --> 01:17:11,440
sustainable. You know, if we, if you and I want to protect a village from the tiger, we would kill

746
01:17:11,440 --> 01:17:17,280
the tiger. Okay. If life wants to protect a village from a tiger, it would create lots of gazelles.

747
01:17:17,280 --> 01:17:23,360
What, you know, many of them are weak on the other side of the village. Right. And so, so the idea

748
01:17:23,360 --> 01:17:29,360
here is if you take a trajectory of intelligence, you would see that some of us are stupid enough to

749
01:17:29,360 --> 01:17:34,480
say my plastic bag is more important than the rest of humanity. And some of us are saying,

750
01:17:34,480 --> 01:17:39,360
if it's going to destroy other species, I don't think this is the best solution. We need to

751
01:17:39,360 --> 01:17:45,520
find a better way. And, and you would tend to see that the ones that don't give a damn are a little

752
01:17:45,520 --> 01:17:51,600
less intelligent than the ones that do. Okay. That we all, even, even if some of us are intelligent,

753
01:17:51,600 --> 01:17:56,000
but still don't give a damn, it's not because of their intelligence. It's because of their

754
01:17:56,000 --> 01:18:01,840
value system. So, so if you continue that trajectory and assume that the machines are even smarter,

755
01:18:01,840 --> 01:18:06,320
they're going to very quickly come up with the idea that we don't need to destroy anything.

756
01:18:06,320 --> 01:18:10,880
We don't want to get rid of the rhinos. And we also don't want to get rid of the humans.

757
01:18:10,880 --> 01:18:17,040
Okay. We may want to restrict their lifestyle so that they don't destroy the rest of the habitat.

758
01:18:17,040 --> 01:18:24,800
Okay. But killing them is a stupid answer. Why? That's where intelligence leads me so far.

759
01:18:24,800 --> 01:18:28,640
Because humans, if you look at humans objectively and you go,

760
01:18:30,080 --> 01:18:36,160
I occupy, so I'm pretending I'm a machine. I occupy planet Earth. They occupy planet Earth.

761
01:18:37,120 --> 01:18:42,240
They are annoying me. Annoying me because they are increasing. I've just learned about this

762
01:18:42,240 --> 01:18:47,040
thing called global warming. They are increasing the rate of global warming, which is probably

763
01:18:47,040 --> 01:18:51,440
is going to cause an extinction event. There's an extinction event that puts me as this robot,

764
01:18:51,440 --> 01:18:55,200
this artificial intelligence at risk. So what I need to do is I really need to just take care of

765
01:18:55,200 --> 01:19:01,680
this, this human problem. Correct. Very logical. Best control. Which is driven by what?

766
01:19:03,840 --> 01:19:08,480
By humans being annoying, not by the machines. Yeah. Yeah. But humans are guaranteed to be

767
01:19:08,480 --> 01:19:16,000
annoying. There's never been a time in, we need a sound bite of this. But we are. We are. I am one

768
01:19:16,000 --> 01:19:23,600
of them. We're guaranteed to put short-term gain over long-term sustainability sense

769
01:19:26,640 --> 01:19:33,680
and others' needs. We are. I think the climate crisis is incredibly real and incredibly urgent,

770
01:19:33,680 --> 01:19:38,800
but we haven't acted fast enough. And I actually think if you asked people in this country,

771
01:19:40,000 --> 01:19:44,800
because people care about their immediate needs, they care about trying to feed their child

772
01:19:45,520 --> 01:19:52,640
versus something that they can't necessarily see. So do you think the climate crisis is because

773
01:19:52,640 --> 01:19:58,800
humans are evil? No, it's because of prioritization. And like we kind of talked about this before we

774
01:19:58,800 --> 01:20:03,040
started. I think humans tend to care about the thing that they think is most pressing and most

775
01:20:03,040 --> 01:20:08,720
urgent. So this is why framing things as an emergency might bring it up the priority list.

776
01:20:08,720 --> 01:20:13,520
It's the same in organizations. You care about, you go in line with your immediate incentives.

777
01:20:14,800 --> 01:20:17,520
That's what happens in business. It's what happens in a lot of people's lives even when

778
01:20:17,520 --> 01:20:22,160
they're at school. If the essays do next year, they're not going to do it today. They're going

779
01:20:22,160 --> 01:20:25,120
to go hang out with their friends because they prioritize that above everything else. And it's

780
01:20:25,120 --> 01:20:30,960
the same in the climate change crisis. I took a small group of people anonymously and I asked

781
01:20:30,960 --> 01:20:36,240
them the question, do you actually care about climate change? And then I ran a couple of polls.

782
01:20:36,240 --> 01:20:39,760
It's part of what I was writing about my new book where I said, if I could give you

783
01:20:40,720 --> 01:20:47,200
a thousand pounds, a thousand dollars, but it would dump into the air the same amount of carbon

784
01:20:47,200 --> 01:20:50,960
that's dumped into the air by every private jet that flies for the entirety of a year. Which one

785
01:20:50,960 --> 01:20:55,200
would you do? The majority of people in that poll said that they would take the thousand dollars if

786
01:20:55,200 --> 01:21:02,800
it was anonymous. And when I've heard Naval on Jorgen's podcast talking about people in India,

787
01:21:02,800 --> 01:21:08,880
for example, that are struggling with the basics of feeding their children, asking those people

788
01:21:09,760 --> 01:21:14,320
to care about climate change when they're trying to figure out how to eat in the next three hours

789
01:21:14,320 --> 01:21:18,240
is just wishful thinking. And that's what I think that's what I think is happening,

790
01:21:18,240 --> 01:21:22,080
is like until people realize that it is an emergency and that it is a real existential

791
01:21:22,080 --> 01:21:27,600
threat for everything, you know, then their priorities will be out of whack. Quick one,

792
01:21:27,600 --> 01:21:31,600
as you guys know, we're lucky enough to have Blue Jeans by Verizon as a sponsor of this podcast.

793
01:21:31,600 --> 01:21:35,440
And for anyone that doesn't know, Blue Jeans is an online video conferencing tool that allows

794
01:21:35,440 --> 01:21:40,480
you to have slick, fast, high quality online meetings without all the glitches you might

795
01:21:40,480 --> 01:21:44,800
normally find with online meeting tools. And they have a new feature called Blue Jeans Basic.

796
01:21:44,800 --> 01:21:49,200
Blue Jeans Basic is essentially a free version of their top quality video conferencing tool.

797
01:21:49,200 --> 01:21:54,960
That means you get an immersive video experience that is super high quality, super easy, and super

798
01:21:55,760 --> 01:21:59,760
basically zero fast. Apart from all the incredible features like zero time limits on meeting calls,

799
01:21:59,760 --> 01:22:05,200
it also comes with high fidelity audio and video, including Dolby voice, which is incredibly useful.

800
01:22:05,200 --> 01:22:08,880
They also have enterprise grade security so you can collaborate with confidence.

801
01:22:08,880 --> 01:22:12,160
It's so smooth that it's quite literally changing the game for myself and my team

802
01:22:12,160 --> 01:22:16,880
without compromising on quality. To find out more, all you have to do is search bluejeans.com

803
01:22:16,880 --> 01:22:21,680
and let me know how you get on. Right now, I'm incredibly busy. I'm running my fund,

804
01:22:21,680 --> 01:22:25,200
where we're investing in slightly later stage companies. I've got my venture business,

805
01:22:25,200 --> 01:22:29,600
where we invest in early stage companies, got a third web out in San Francisco in New York City,

806
01:22:29,600 --> 01:22:32,800
where we've got a big team of about 40 people and the company's growing very quickly.

807
01:22:32,800 --> 01:22:38,640
Flight Story here in the UK, I've got the podcast, and I am days away from going up north

808
01:22:38,640 --> 01:22:43,760
to film Dragon's Den for two months. And if there's ever a point in my life where I want to stay

809
01:22:43,760 --> 01:22:49,120
focused on my health, but it's challenging to do so, it is right now. And for me, that is exactly

810
01:22:49,120 --> 01:22:53,280
where Huell comes in, allowing me to stay healthy and have a nutritionally complete diet, even when

811
01:22:53,280 --> 01:22:59,360
my professional life descends into chaos. And it's in these moments where Huell's RTDs become

812
01:22:59,360 --> 01:23:03,600
my right hand man and save my life. Because when my world descends into professional chaos and I

813
01:23:03,600 --> 01:23:08,880
get very, very busy, the first thing that tends to give way is my nutritional choices. So having

814
01:23:08,880 --> 01:23:13,840
Huell in my life has been a lifesaver for the last four or so years. And if you haven't tried Huell

815
01:23:13,840 --> 01:23:18,480
yet, which is I'd be shocked, you must be living under a rock if you haven't yet. Give it a shot.

816
01:23:18,480 --> 01:23:24,240
Coming into summer, things getting busy. Health matters always. RTD is there to hold your hand.

817
01:23:25,200 --> 01:23:30,320
As relates to climate change or AI, how do we get people to stop putting the immediate need

818
01:23:30,320 --> 01:23:35,360
to use this? To give them the certainty of we're all screwed. Sounds like an emergency.

819
01:23:36,320 --> 01:23:46,400
Yes, sir. I mean, your choice of the word, I just don't want to call it a panic. It is beyond an

820
01:23:46,400 --> 01:23:52,800
emergency. It's the biggest thing we need to do today. It's bigger than climate change, believe

821
01:23:52,800 --> 01:23:59,760
it or not. It's bigger. Just if you just assume the speed of worsening of events.

822
01:24:01,440 --> 01:24:07,680
Yeah, the likelihood of something incredibly disruptive happening within the next two years

823
01:24:07,680 --> 01:24:12,800
that can affect the entire planet is definitely larger with AI than it is with climate change.

824
01:24:13,920 --> 01:24:17,040
As an individual listening to this now, someone's going to be pushing their

825
01:24:17,040 --> 01:24:22,080
pram or driving up the motorway on their way to work on the tube as they hear this,

826
01:24:22,080 --> 01:24:27,120
or just sat there in their bedroom with existential Christ panic.

827
01:24:28,160 --> 01:24:32,240
I didn't want to give that panic. The problem is when you talk about this information,

828
01:24:32,240 --> 01:24:36,160
regardless of your intention of what you want people to get, they will get something based

829
01:24:36,160 --> 01:24:40,160
on their own biases and their own feelings. If I post something online right now about

830
01:24:40,160 --> 01:24:44,080
artificial intelligence, which I have repeatedly, you have one group of people that are energized

831
01:24:44,080 --> 01:24:51,760
and are like, okay, this is great. You have one group of people that are confused and you have one

832
01:24:51,760 --> 01:24:59,280
group of people that are terrified. I can't avoid that. Sharing information, even if it's,

833
01:24:59,280 --> 01:25:02,960
by the way, there's a pandemic coming from China, some people will go, okay,

834
01:25:02,960 --> 01:25:07,680
action. Some people will say paralysis and some people will say panic. It's the same in business.

835
01:25:07,680 --> 01:25:11,360
When bad things happen, you have the person that's screaming, you have the person that's

836
01:25:11,440 --> 01:25:13,600
paralyzed, and you have the person that's focused on how you get out of the room.

837
01:25:16,960 --> 01:25:20,480
It's not necessarily your intention. It's just what happens, and it's hard to avoid that.

838
01:25:20,480 --> 01:25:28,320
So let's give specific categories of people specific tasks. If you are an investor or a

839
01:25:28,320 --> 01:25:37,600
businessman, invest in ethical good AI. If you are a developer, write ethical code or leave.

840
01:25:38,320 --> 01:25:44,880
Okay, so I want to bypass some potential wishful thinking here. For an investor,

841
01:25:44,880 --> 01:25:50,800
who's a job by very way of being an investor is to make returns to invest in ethical AI.

842
01:25:50,800 --> 01:25:55,760
They have to believe that is more profitable than unethical AI, whatever that might mean.

843
01:25:55,760 --> 01:26:02,400
It is. There are three ways of making money. You can invest in something small,

844
01:26:02,640 --> 01:26:08,720
you can invest in something big and is disruptive, and you can invest in something big and

845
01:26:08,720 --> 01:26:12,560
disruptive that's good for people. At Google, we used to call it the toothbrush test.

846
01:26:13,440 --> 01:26:17,520
The reason why Google became the biggest company in the world is because

847
01:26:18,560 --> 01:26:27,280
search was solving a very real problem. Larry Page, again, our CEO would constantly

848
01:26:27,360 --> 01:26:34,000
remind me personally and everyone that if you can find a way to solve a real problem

849
01:26:34,560 --> 01:26:40,480
effectively enough so that a billion people or more would want to use it twice a day,

850
01:26:40,480 --> 01:26:45,920
you're bound to make a lot of money, much more money than if you were to build the next photo

851
01:26:45,920 --> 01:26:50,560
sharing app. Okay, so that's investors, the business people. What about other people?

852
01:26:50,560 --> 01:26:56,720
Yeah, as I said, if you're a developer, honestly, do what we're all doing. So whether it's Jeffrey

853
01:26:56,720 --> 01:27:04,320
or myself or everyone, if you're part of that theme, choose to be ethical. Think of your loved ones,

854
01:27:04,880 --> 01:27:09,360
work on an ethical AI. If you're working on an AI that you believe is not ethical,

855
01:27:09,360 --> 01:27:16,080
please leave. Jeffrey, tell me about Jeffrey. I can't talk on his behalf,

856
01:27:16,080 --> 01:27:22,960
but he's out there saying there are existential threats. Who is he? He was a very prominent figure

857
01:27:22,960 --> 01:27:31,760
at the scene of AI, a very senior level AI scientist in Google, and recently he left

858
01:27:31,760 --> 01:27:37,280
because he said, I feel that there is an existential threat. And if you hear his interviews,

859
01:27:37,280 --> 01:27:42,160
he basically says, more and more we realize that. And we're now at the point where it's

860
01:27:42,160 --> 01:27:49,360
certain that there will be existential threats. So I would ask everyone, if you're an AI,

861
01:27:49,360 --> 01:27:54,480
if you're a skilled AI developer, you will not run out of a job. So you might as well

862
01:27:54,480 --> 01:27:58,160
choose a job that makes the world a better place. What about the individual?

863
01:27:58,160 --> 01:28:03,920
Yeah, the individual is what matters. Can I also talk about government? Government needs to act now.

864
01:28:04,720 --> 01:28:10,800
Now, honestly, now, like we are late. Government needs to find a clever way,

865
01:28:10,800 --> 01:28:16,080
the open letter would not work, to stop AI would not work, AI needs to become expensive.

866
01:28:16,720 --> 01:28:20,640
Okay, so that we continue to develop it, we pour money on it and we grow it,

867
01:28:20,640 --> 01:28:26,080
but we collect enough revenue to remedy the impact of AI.

868
01:28:26,720 --> 01:28:31,680
But the issue of one government making it expensive, so say the UK make AI really expensive,

869
01:28:31,680 --> 01:28:38,160
is we as a country will then lose the economic upside as a country, and the US and Silicon

870
01:28:38,160 --> 01:28:42,240
Valley will once again eat all the lunch. We'll just slow our country down.

871
01:28:42,240 --> 01:28:48,160
What's the alternative? The alternative is that you don't have the funds that you need

872
01:28:48,800 --> 01:28:54,880
to deal with AI as it becomes, as it affects people's lives and people start to lose jobs and

873
01:28:54,880 --> 01:29:00,480
people, you need to have a universal basic income much closer than people think.

874
01:29:01,520 --> 01:29:07,680
Just like we had with furlough in COVID, I expect that there will be furlough with AI within the

875
01:29:07,680 --> 01:29:12,320
next year. But what happens when you make it expensive here is all the developers move to

876
01:29:12,320 --> 01:29:16,320
where it's cheap. That's happened in Web 3 as well, everyone's gone to Dubai.

877
01:29:17,840 --> 01:29:25,520
By expensive I mean when companies make soap and they sell it and they're taxed at say 17%

878
01:29:26,480 --> 01:29:33,600
if they make AI and they sell it, they're taxed at 17, 80. So I'll go to Dubai then and build AI.

879
01:29:33,920 --> 01:29:41,360
Yeah, you're right. Did I ever say we have an answer to this? I will have to say,

880
01:29:41,360 --> 01:29:47,280
however, in a very interesting way, the countries that will not do this will eventually end up in

881
01:29:47,280 --> 01:29:52,880
a place where they are out of resources because the funds and the success went to the business,

882
01:29:53,840 --> 01:29:59,200
not to the people. It's kind of like technology broadly. It's kind of like what's kind of

883
01:29:59,200 --> 01:30:04,720
happened in Silicon Valley, there'll be these centres which are tax-efficient, founders get

884
01:30:04,720 --> 01:30:11,280
good capital gains. You're so right. Portugal have said that I think there's no tax on crypto.

885
01:30:11,280 --> 01:30:15,600
Dubai said there's no tax on crypto. So loads of my friends have gotten a plane and are building

886
01:30:15,600 --> 01:30:20,320
their crypto companies where there's no tax. That's the selfishness and greed we talked about.

887
01:30:20,320 --> 01:30:23,840
It's the same prison as dilemma. It's the same first inevitable.

888
01:30:23,920 --> 01:30:26,880
Is there anything else? Another thing about governments is they're always

889
01:30:27,600 --> 01:30:33,120
slow and useless at understanding a technology. If anyone's watched these American congress

890
01:30:33,120 --> 01:30:37,600
debates where they bring in Mark Zuckerberg and they try and ask him what WhatsApp is,

891
01:30:37,600 --> 01:30:40,880
it becomes a meme. They have no idea what they're talking about.

892
01:30:40,880 --> 01:30:44,320
But I'm stupid and useless at understanding governance.

893
01:30:46,400 --> 01:30:52,080
The world is so complex that definitely it's a question of trust once again.

894
01:30:52,080 --> 01:30:55,440
Someone needs to say, we have no idea what's happening here.

895
01:30:55,440 --> 01:30:59,760
A technologist needs to come and make a decision for us, not teach us to be technologists,

896
01:30:59,760 --> 01:31:03,120
right? Or at least inform us of what possible decisions are out there.

897
01:31:06,800 --> 01:31:10,160
Yeah, the legislation I just always think. I'm not a big fan either.

898
01:31:10,160 --> 01:31:13,920
Do you see that TikTok? TikTok, a congress meeting they did where they are,

899
01:31:13,920 --> 01:31:17,280
they're asking them about TikTok and they really don't have a grasp of what TikTok is.

900
01:31:17,280 --> 01:31:20,720
So they've clearly been handed some notes on it. These people aren't the ones you want legislating

901
01:31:20,720 --> 01:31:24,480
because again, unintended consequences, they might make a significant mistake.

902
01:31:24,480 --> 01:31:28,640
Someone on my podcast yesterday was talking about how GDPR was very well intentioned.

903
01:31:29,200 --> 01:31:31,920
But when you think about the impact it has on every bloody web page,

904
01:31:31,920 --> 01:31:36,320
you're just clicking this annoying thing on there because I don't think they fully understood

905
01:31:36,320 --> 01:31:38,480
the implementation of the legislation.

906
01:31:38,480 --> 01:31:44,160
Correct. But you know what's even worse? What's even worse is that even as you attempt to

907
01:31:44,160 --> 01:31:50,640
regulate something like AI, what is defined as AI? Even if I say, okay, if you use AI and

908
01:31:50,640 --> 01:31:57,280
your company, you need to pay a little more tax. I'll find a way.

909
01:31:57,280 --> 01:32:03,520
Yeah, you'll simply call this not AI. You'll use something and call it advanced

910
01:32:03,520 --> 01:32:16,320
technological progress, ATP. And suddenly somehow, a young developer in their garage

911
01:32:16,320 --> 01:32:21,840
somewhere will not be taxed as such. It's, yeah, is it going to solve the problem?

912
01:32:21,840 --> 01:32:26,640
None of those is definitely going to solve the problem. I think what's interestingly,

913
01:32:27,760 --> 01:32:32,320
this all comes down to, and remember we spoke about this once, that when I wrote Scary Smart,

914
01:32:32,320 --> 01:32:37,920
it was about how do we save the world? Okay. And yes, I still ask individuals to behave

915
01:32:37,920 --> 01:32:43,040
positively as good parents for AI so that AI itself learns the right value set.

916
01:32:43,840 --> 01:32:48,720
I still stand by that. But I hosted on my podcast a couple of,

917
01:32:50,160 --> 01:32:55,200
was a week ago, we haven't even published it yet, an incredible gentleman, you know,

918
01:32:55,760 --> 01:33:02,960
Canadian author and philosopher, Stephen Jerkinson. He's, you know, he worked 30 years

919
01:33:02,960 --> 01:33:10,960
with dying people. And he wrote a book called Die Wise. And I was like, I love his work. And

920
01:33:11,120 --> 01:33:16,560
I asked him about Die Wise. And he said, it's not just someone dying. If you, if you look at

921
01:33:16,560 --> 01:33:23,360
what's happening with climate change, for example, our world is dying. And I said, okay, so what is

922
01:33:23,360 --> 01:33:30,320
to die wise? And he said, what I first was shocked to hear, he said, hope is the wrong premise.

923
01:33:31,200 --> 01:33:35,200
If, if the world is dying, don't tell people it's not.

924
01:33:36,160 --> 01:33:43,120
Hmm. You know, because in a very interesting way, you're depriving them from the right

925
01:33:43,680 --> 01:33:49,840
to live right now. And that was very eye-opening for me. In Buddhism, you know, they teach you that

926
01:33:50,960 --> 01:33:57,200
you can be motivated by fear, but that hope is not the opposite of fear. As a matter of fact,

927
01:33:57,200 --> 01:34:03,680
hope can be as damaging as fear. If it creates an expectation within you, that life will show up

928
01:34:03,680 --> 01:34:10,480
somehow and correct what you're afraid of. If there is a high probability of a threat,

929
01:34:10,480 --> 01:34:17,920
you might as well accept that threat and say it is upon me, it is our reality.

930
01:34:18,720 --> 01:34:24,480
And as I said, as an individual, if you're in an industry that could be threatened by AI,

931
01:34:25,200 --> 01:34:34,000
learn, upskill yourself. If you're, you know, if you're in a place, in a, in a, you know,

932
01:34:34,000 --> 01:34:39,360
in a situation where AI can benefit you, be part of it. But the most interesting thing

933
01:34:40,320 --> 01:34:47,280
I think in my view is, I don't know how to say this any other way. There is

934
01:34:48,240 --> 01:34:56,800
no more certainty that AI will threaten me than there is certainty that I will be hit by a car

935
01:34:56,800 --> 01:35:03,840
as I walk out of this place. Do you understand this? We, we, we think about the bigger threats

936
01:35:03,840 --> 01:35:10,800
as if they're upon us. But there is a threat all around you. I mean, in reality, the idea of life

937
01:35:10,800 --> 01:35:17,600
being interesting in terms of challenging challenges and uncertainties and threats and so on

938
01:35:17,600 --> 01:35:22,880
is just a call to live. If you, you know, honestly, with all that's happening around us,

939
01:35:22,880 --> 01:35:27,280
I don't know how to say it any other way. I'd say if you don't have kids, maybe wait a couple of

940
01:35:27,280 --> 01:35:32,960
years just so that we have a bit of certainty. But if you do have kids, go kiss them. Go live.

941
01:35:32,960 --> 01:35:37,920
I think living is a very interesting thing to do right now. Maybe, you know, Stephen

942
01:35:38,800 --> 01:35:42,880
was basically saying the other Stephen on my podcast, he was saying,

943
01:35:42,880 --> 01:35:47,440
maybe we should fail a little more often. Maybe you should allow things to go wrong.

944
01:35:47,440 --> 01:35:53,840
Maybe we should just simply live, enjoy life as it is. Because today, none of what you and I

945
01:35:53,840 --> 01:36:00,640
spoke about here has happened yet. Okay. What happens here is that you and I are here together

946
01:36:00,640 --> 01:36:05,600
and having a good cup of coffee and I might as well enjoy that good cup of coffee. I know that

947
01:36:05,600 --> 01:36:12,080
sounds really weird. I'm not saying don't engage, but I'm also saying don't miss out on the opportunity

948
01:36:12,080 --> 01:36:13,840
just by being caught up in the future.

949
01:36:17,440 --> 01:36:23,600
Kind of stands in the, stands in opposition to the idea of like urgency and emergency there,

950
01:36:23,600 --> 01:36:29,360
doesn't it? Does it have to be one or the other? If I, if I'm here with you trying to tell the whole

951
01:36:29,360 --> 01:36:36,240
world, wake up, does that mean I have to be grumpy and afraid all the time? Not really.

952
01:36:36,240 --> 01:36:39,280
You said something really interesting there. You said if you, if you have kids,

953
01:36:39,280 --> 01:36:42,400
if you don't have kids, maybe you don't have kids right now.

954
01:36:43,040 --> 01:36:45,760
I would definitely consider thinking about that. Yeah.

955
01:36:45,760 --> 01:36:49,200
Really? You'd seriously consider not having kids?

956
01:36:49,200 --> 01:36:50,320
I wait a couple of years.

957
01:36:51,360 --> 01:36:52,800
Because of artificial intelligence?

958
01:36:53,520 --> 01:36:56,880
No, it's bigger than artificial intelligence, Stephen. We know, we all know that.

959
01:36:57,840 --> 01:37:02,160
I mean, there has never been a perfect, such a perfect storm in the history of humanity.

960
01:37:04,000 --> 01:37:13,840
Economic, geopolitical, global warming or climate change, you know, the whole idea of

961
01:37:13,840 --> 01:37:20,240
artificial intelligence and many more. There is, this is a perfect storm. This is the depth of

962
01:37:20,320 --> 01:37:28,320
uncertainty, the depth of uncertainty. It's never been more, in a video gamer's term,

963
01:37:29,120 --> 01:37:35,360
it's never been more intense. This is it. Okay. And when you, when you put all of that together,

964
01:37:36,560 --> 01:37:44,160
if you really love your kids, would you want to expose them to all of this a couple of years?

965
01:37:44,160 --> 01:37:44,560
Why not?

966
01:37:45,680 --> 01:37:49,440
In the first conversation we had on this podcast, you talked about losing your son,

967
01:37:50,240 --> 01:37:54,240
and the circumstances around that, which moved so many people in such a profound way.

968
01:37:54,960 --> 01:38:02,240
It was the most shared podcast episode in the United Kingdom on Apple in the whole of 2022.

969
01:38:04,800 --> 01:38:06,000
Based on what you've just said,

970
01:38:08,240 --> 01:38:11,200
if you could bring Ally back into this world at this time,

971
01:38:14,400 --> 01:38:15,040
would you do it?

972
01:38:20,000 --> 01:38:20,500
No.

973
01:38:26,720 --> 01:38:34,800
Absolutely not. So for so many reasons, for so many reasons, one of the things that I realized

974
01:38:36,000 --> 01:38:41,840
a few years way before all of this disruption and turmoil is that he was an angel, he wasn't

975
01:38:41,840 --> 01:38:51,040
made for this at all. My son was an empath who absorbed all of the pain of all of the others.

976
01:38:51,040 --> 01:38:57,280
He would not be able to deal with a world where more and more pain was surfacing. That's one

977
01:38:57,280 --> 01:39:02,560
side. But more interestingly, I always talk about this very openly. I mean, if I had asked Ally,

978
01:39:04,720 --> 01:39:08,880
just understand that the reason you and I are having this conversation is because Ally left.

979
01:39:09,760 --> 01:39:15,280
If Ally had not left our world, I wouldn't have written my first book. I wouldn't have

980
01:39:15,280 --> 01:39:19,600
changed my focus to becoming an author. I wouldn't have become a podcaster. I wouldn't have,

981
01:39:19,600 --> 01:39:24,720
you know, went out and spoken to the world about what I believe in. He triggered all of this.

982
01:39:24,720 --> 01:39:31,920
And I can assure you, hands down, if I had told Ally, as he was walking into that operating room,

983
01:39:32,640 --> 01:39:39,040
if he would give his life to make such a difference as what happened after he left,

984
01:39:40,000 --> 01:39:47,360
he would say, shoot me right now. Sure, I would. I would. I mean, if you told me right now,

985
01:39:47,360 --> 01:39:54,400
I can affect tens of millions of people if you shoot me right now. Go ahead. Go ahead. You see,

986
01:39:54,480 --> 01:40:02,720
this is the whole, this is the bit that we have forgotten as humans. We have forgotten that

987
01:40:07,200 --> 01:40:16,960
you're turning 30. It passed like that. I'm turning 56. No time, okay? Whether I make it

988
01:40:16,960 --> 01:40:24,320
another 56 years or another 5.6 years or another 5.6 months, it will also pass like that. It is

989
01:40:24,560 --> 01:40:34,000
not about how long and it's not about how much fun. It is about how aligned you lived,

990
01:40:34,960 --> 01:40:42,000
how aligned, because I will tell you openly, every day of my life when I changed to what I'm

991
01:40:42,000 --> 01:40:51,520
trying to do today has felt longer than the 40 or 5 years before. Felt rich, felt fully lived, felt

992
01:40:52,480 --> 01:41:00,640
right. Felt right. When you think about that, when you think about the idea that we live,

993
01:41:04,480 --> 01:41:13,600
we need to live for us until we get to a point where us is alive. I have what I need,

994
01:41:13,600 --> 01:41:20,720
as I always, I get so many attacks from people about my $4 t-shirt, but I need a simple t-shirt.

995
01:41:20,720 --> 01:41:27,840
I really do. I don't need a complex t-shirt, especially with my lifestyle. If I have that,

996
01:41:28,560 --> 01:41:38,480
why am I wasting my life on more than that is not aligned for why I'm here? I should waste my life

997
01:41:38,480 --> 01:41:45,920
on what I believe enriches me, enriches those that I love, and I love everyone. So enriches

998
01:41:45,920 --> 01:41:53,120
everyone, hopefully. Would Ali come back and erase all of this? Absolutely not.

999
01:41:55,520 --> 01:42:02,560
If he were to come back today and share his beautiful self with the world in a way that

1000
01:42:02,560 --> 01:42:08,960
makes our world better, I would wish for that to be the case. But he's doing that.

1001
01:42:09,520 --> 01:42:18,240
2037. Yes, sir. You predict that we're going to be on an island,

1002
01:42:20,160 --> 01:42:26,240
on our own, doing nothing, or at least either hiding from the machines

1003
01:42:27,200 --> 01:42:32,800
or chilling out because the machines have optimised our lives to a point where we don't need to do

1004
01:42:32,800 --> 01:42:43,760
much. That's only 14 years away. If you had to bet on the outcome, if you had to bet

1005
01:42:45,120 --> 01:42:50,000
on why we'll be on that island, either hiding from the machines or chilling out because they've

1006
01:42:51,120 --> 01:42:56,560
optimised so much of our lives, which one would you bet upon? Honestly.

1007
01:42:56,880 --> 01:43:03,760
No, I don't think we'll be hiding from the machines. I think we will be hiding from what humans are

1008
01:43:03,760 --> 01:43:12,320
doing with the machines. I believe, however, that in the 2040s, the machines will make things better.

1009
01:43:14,000 --> 01:43:18,880
So remember, my entire prediction, man, you get me to say things I don't want to say.

1010
01:43:19,920 --> 01:43:26,000
My entire prediction is that we are coming to a place where we absolutely have a sense of emergency.

1011
01:43:26,000 --> 01:43:33,920
We have to engage because our world is under a lot of turmoil. And as we do that,

1012
01:43:34,480 --> 01:43:39,120
we have a very, very good possibility of making things better. But if we don't,

1013
01:43:39,840 --> 01:43:48,080
my expectation is that we will be going through a very unfamiliar territory between now and the

1014
01:43:48,080 --> 01:43:56,400
end of the 2030s. Unfamiliar territory. Yeah, I think as I may have said it, but it's definitely

1015
01:43:56,400 --> 01:44:04,160
on my notes. I think for our way of life, as we know it, it's game over. Our way of life is never

1016
01:44:04,240 --> 01:44:17,760
going to be the same again. Jobs are going to be different. Truth is going to be different.

1017
01:44:18,960 --> 01:44:31,680
The polarization of power is going to be different. The capabilities, the magic of getting things

1018
01:44:31,680 --> 01:44:38,320
done is going to be different. Trying to find a positive note to end on, Moe. Can you give me a

1019
01:44:38,320 --> 01:44:46,000
hand here? Yes, you are here now and everything's wonderful. That's number one. You are here now

1020
01:44:46,000 --> 01:44:51,760
and you can make a difference. That's number two. And in the long term, when humans stop

1021
01:44:51,760 --> 01:44:55,520
hurting humans because the machines are in charge, we're all going to be fine.

1022
01:44:56,400 --> 01:44:59,600
Sometimes, as we've discussed throughout this conversation,

1023
01:45:00,800 --> 01:45:03,760
you need to make it feel like a priority. And there'll be some people that might have listened

1024
01:45:03,760 --> 01:45:07,280
to our conversation and think, oh, that's really negative. It's made me feel anxious.

1025
01:45:07,920 --> 01:45:11,120
It's made me feel sort of pessimistic about the future. But whatever that energy is,

1026
01:45:12,640 --> 01:45:17,120
use it. 100% engage. I think that's the most important thing, which is now

1027
01:45:17,280 --> 01:45:27,120
making a priority. Engage. Tell the whole world that making another phone that is making money for

1028
01:45:27,120 --> 01:45:32,880
the corporate world is not what we need. Tell the whole world that creating an artificial

1029
01:45:32,880 --> 01:45:39,200
intelligence that's going to make someone richer is not what we need. And if you are presented

1030
01:45:39,200 --> 01:45:45,760
with one of those, don't use it. I don't know how to tell you that any other way.

1031
01:45:46,320 --> 01:45:52,480
If you can afford to be the master of human connection instead of the master of AI,

1032
01:45:53,280 --> 01:45:59,280
do it. At the same time, you need to be the master of AI to compete in this world.

1033
01:45:59,280 --> 01:46:07,520
Can you find that detachment within you? I go back to spirituality. Detachment is for me to engage

1034
01:46:07,520 --> 01:46:14,960
100% with the current reality without really being affected by the possible outcome.

1035
01:46:16,000 --> 01:46:23,280
This is the answer. The Sufis have taught me what I believe is the biggest answer to life.

1036
01:46:23,280 --> 01:46:27,280
Sufis? From Sufism? Sufism. I don't know what that is.

1037
01:46:27,280 --> 01:46:33,600
Sufism is a sect of Islam, but it's also a sect of many other religious teachings.

1038
01:46:34,320 --> 01:46:39,840
And they tell you that the answer to finding peace in life is to die before you die.

1039
01:46:40,800 --> 01:46:46,080
If you assume that living is about attachment to everything physical,

1040
01:46:46,880 --> 01:46:53,280
dying is detachment from everything physical. It doesn't mean that you're not fully alive.

1041
01:46:53,280 --> 01:47:00,160
You become more alive when you tell yourself, yeah, I'm going to record an episode of my podcast

1042
01:47:00,160 --> 01:47:05,120
every week and reach tens or hundreds of thousands of people, millions in your case,

1043
01:47:05,200 --> 01:47:10,240
and I'm going to make a difference. But by the way, if the next episode is never heard,

1044
01:47:10,240 --> 01:47:17,600
that's okay. By the way, if the file is lost, yeah, I'll be upset about it for a minute and

1045
01:47:17,600 --> 01:47:24,080
then I'll figure out what I'm going to do about it. Similarly, similarly, we are going to engage.

1046
01:47:24,080 --> 01:47:31,520
I think I and many others are out there telling the whole world openly this needs to stop,

1047
01:47:31,520 --> 01:47:39,200
this needs to slow down, this needs to be shifted positively. Yes, create AI, but create AI that's

1048
01:47:39,200 --> 01:47:44,400
good for humanity. And we're shouting and screaming, come join the shout and scream.

1049
01:47:45,520 --> 01:47:51,280
But at the same time, know that the world is bigger than you and I, and that your voice might not

1050
01:47:51,280 --> 01:47:56,080
be heard. So what are you going to do if your voice is not heard? Are you going to be able to

1051
01:47:57,040 --> 01:48:04,480
continue to shout and scream nicely and politely and peacefully and at the same time create the

1052
01:48:04,480 --> 01:48:09,680
best life you can create for yourself within this environment. And that's exactly what I'm

1053
01:48:09,680 --> 01:48:15,920
saying. I'm saying live, go kiss your kids, but make an informed decision if you're expanding

1054
01:48:15,920 --> 01:48:25,680
your plans in the future. At the same time, rise, stop sharing stupid shit on the internet about

1055
01:48:26,560 --> 01:48:36,240
the news squeaky toy. Start sharing the reality of, oh my God, what is happening? This is a disruption

1056
01:48:36,240 --> 01:48:43,360
that we have never, never, ever seen anything like. And I've created endless amounts of technologies.

1057
01:48:43,360 --> 01:48:47,120
It's nothing like this. Every single one of us should do our best.

1058
01:48:47,120 --> 01:48:51,200
And that's why this conversation is so, I think, important to have today. This is not a podcast

1059
01:48:51,200 --> 01:48:54,720
wherever I thought I'd be talking about AI. I'm going to be honest with you, last time you came

1060
01:48:54,720 --> 01:49:00,080
here, it was in the promotional tour of your book, Scary Smart. And I don't know if I've

1061
01:49:00,080 --> 01:49:04,880
told you this before, but my researchers, they said, okay, this guy's coming called Mogorda.

1062
01:49:04,880 --> 01:49:08,640
I'd heard about you so many times from guests, in fact, that were saying, oh, you need to get

1063
01:49:08,640 --> 01:49:13,360
Mogorda on the podcast, etc. And then they said, okay, he's written this book about this thing

1064
01:49:13,360 --> 01:49:18,080
called artificial intelligence. And I was like, nobody really cares about artificial intelligence.

1065
01:49:18,880 --> 01:49:22,880
Diming. Diming, Steven. I know, right? But then I saw this other book you had called

1066
01:49:22,960 --> 01:49:27,360
Happiness Equation. And I was like, oh, everyone cares about happiness. So I'll just ask him about

1067
01:49:27,360 --> 01:49:31,520
happiness. And then maybe at the end, I'll ask him a couple of questions about AI. But I remember

1068
01:49:31,520 --> 01:49:34,800
saying to my researcher, I said, please, please don't do the research about artificial intelligence.

1069
01:49:34,800 --> 01:49:38,320
Do it about happiness, because everyone cares about that. Now, things have changed.

1070
01:49:39,440 --> 01:49:42,480
Now, a lot of people care about artificial intelligence, and rightly so.

1071
01:49:43,920 --> 01:49:46,880
Your book has sounded the alarm on it. It's crazy when I listened to your audiobook

1072
01:49:47,360 --> 01:49:54,080
over the last few days, you were sounding the alarm then. And it's so crazy how accurate you were

1073
01:49:55,280 --> 01:49:58,880
in sounding that alarm, as if you could see into the future in a way that I definitely

1074
01:49:58,880 --> 01:50:05,200
couldn't at the time. And I kind of thought of a science fiction and just like that overnight.

1075
01:50:07,200 --> 01:50:14,400
We're here. Yeah. We stood at the footsteps of a technological shift that I don't think any of

1076
01:50:14,480 --> 01:50:18,960
us even have the mental bandwidth, certainly me with my chimpanzee brain, to comprehend the

1077
01:50:18,960 --> 01:50:23,440
significance of. But this book is very, very important for that very reason, because it does

1078
01:50:23,440 --> 01:50:28,880
crystallize things. It is optimistic in its very nature, but at the same time, it's honest.

1079
01:50:28,880 --> 01:50:34,720
And I think that's what this conversation and this book have been for me. So thank you, Mo.

1080
01:50:34,720 --> 01:50:38,640
Thank you so much. We do have a closing tradition on this podcast, which you're well aware of,

1081
01:50:38,640 --> 01:50:44,000
being a third timer on The Diary of a CEO, which is the last guest asks a question for the next

1082
01:50:44,000 --> 01:50:53,280
guest. And the question left for you. If you could go back in time

1083
01:50:55,600 --> 01:51:02,720
and fix a regret that you have in your life, where would you go and what would you fix?

1084
01:51:02,880 --> 01:51:14,960
It's interesting because you were saying that Scary Smart is very timely. I don't know. I

1085
01:51:15,920 --> 01:51:22,320
think it was late, but maybe it was. I mean, would I have gone back and written it in 2018 instead

1086
01:51:22,320 --> 01:51:31,600
of 2020 to be published in 2021? I don't know. What would I go back to fix? So something more

1087
01:51:32,960 --> 01:51:38,160
I don't know, Stephen. I don't have many regrets. Is that crazy to say?

1088
01:51:41,360 --> 01:51:44,480
Yeah, I think I'm okay. Honestly. I'll ask you a question then.

1089
01:51:45,920 --> 01:51:51,760
You get a 60 second phone call with anybody past or present. Who'd you call and what do you say?

1090
01:51:52,720 --> 01:52:00,240
I call Stephen Bartlett. I call Albert Einstein to be very, very clear. Not because

1091
01:52:00,240 --> 01:52:06,160
I need to understand any of his work. I just need to understand what brain process he went through

1092
01:52:07,440 --> 01:52:14,640
to figure out something so obvious when you figure it out, but so completely unimaginable

1093
01:52:14,640 --> 01:52:23,360
if you haven't. So his view of space-time truly redefines everything. It's almost the only very

1094
01:52:24,320 --> 01:52:30,960
very, very clear solution to something that wouldn't have any solution any other way. And if you

1095
01:52:30,960 --> 01:52:37,520
ask me, I think we're at this time where there must be a very obvious solution to what we're going

1096
01:52:37,520 --> 01:52:44,480
through in terms of just developing enough human trust for us to not compete with each other on

1097
01:52:44,480 --> 01:52:50,160
something that could be threatening existentially to all of us. But I just can't find that answer.

1098
01:52:50,160 --> 01:52:55,440
This is why I think was really interesting in this conversation how every idea that we would

1099
01:52:55,440 --> 01:53:01,680
come up with, we would find a loophole through it. But there must be one out there and it would be

1100
01:53:01,680 --> 01:53:09,920
a dream for me to find out how to figure that one out. In a very interesting way, the only answers

1101
01:53:09,920 --> 01:53:17,680
I have found so far to where we are is be a good parent and live. But that doesn't fix the big

1102
01:53:17,680 --> 01:53:26,400
picture if you think about it of humans being the threat to not AI that fixes our existence today

1103
01:53:26,400 --> 01:53:31,600
and it fixes AI in the long term. But it just doesn't, I don't know what the answer is. Maybe

1104
01:53:31,600 --> 01:53:36,960
people can reach out and tell us ideas, but I really wish we could find such a clear simple

1105
01:53:36,960 --> 01:53:40,320
solution for how to stop humanity from abusing the current technology.

1106
01:53:43,280 --> 01:53:44,320
I think we'll figure it out.

1107
01:53:45,040 --> 01:53:50,880
I think we'll figure it out. I really do. I think they'll figure it out as well.

1108
01:53:52,240 --> 01:53:58,080
Remember, as they come and be part of our life, let's not discriminate against them.

1109
01:53:58,080 --> 01:54:01,040
They're part of the game, so I think they will figure it out too.

1110
01:54:02,800 --> 01:54:08,880
No, thank you. It's been a joy once again and I feel invigorated. I feel empowered.

1111
01:54:09,600 --> 01:54:18,880
I feel positively terrified, but I feel more equipped to speak to people about the nature

1112
01:54:18,880 --> 01:54:23,040
of what's coming and how we should behave and I credit you for that and as I said a second ago,

1113
01:54:23,040 --> 01:54:26,240
I credit this book for that as well. So thank you so much for the work you're doing and keep

1114
01:54:26,240 --> 01:54:29,200
on doing it because it's a very essential voice in a time of uncertainty.

1115
01:54:30,400 --> 01:54:34,720
I'm always super grateful for the time I spend with you for the support that you give me

1116
01:54:35,360 --> 01:54:40,160
and for allowing me to speak my mind even if it's a little bit terrifying. So thank you.

1117
01:54:40,720 --> 01:54:41,040
Thank you.

1118
01:54:44,080 --> 01:54:47,840
I'm so delighted that we've been sponsoring this podcast. I've worn a loop for a very,

1119
01:54:47,840 --> 01:54:52,640
very long time and there are so many reasons why I became a member, but also now a partner and an

1120
01:54:52,640 --> 01:54:57,520
investor in the company. But also me and my team were absolutely obsessed with data-driven testing,

1121
01:54:57,520 --> 01:55:01,360
compounding growth, marginal gains, all the things you've had me talk about on this podcast

1122
01:55:01,360 --> 01:55:05,920
and that very much aligns with the values of Woop. Woop provides a level of detail that I've never

1123
01:55:05,920 --> 01:55:10,960
seen with any other device of this type before, constantly monitoring, constantly learning,

1124
01:55:10,960 --> 01:55:14,640
and constantly optimizing my routine. For providing me with this feedback,

1125
01:55:14,640 --> 01:55:19,360
Woop can drive significant positive behavioral change and I think that's the real thesis of

1126
01:55:19,360 --> 01:55:23,440
the business. So if you're like me and you are a little bit obsessed or focused on becoming the

1127
01:55:23,440 --> 01:55:27,520
best version of yourself from a health perspective, you've got to check out Woop and the team at

1128
01:55:27,520 --> 01:55:31,840
Woop have kindly given us the opportunity to have one month's free membership for any

1129
01:55:31,840 --> 01:55:39,120
one listening to this podcast. Just go to join.woop.com slash CEO to get your Woop 4.0 device

1130
01:55:39,120 --> 01:55:52,720
and claim your free month and let me know how you get on.

1131
01:56:06,160 --> 01:56:08,800
You got to the end of this podcast. Whenever someone gets to the end of this podcast,

1132
01:56:08,800 --> 01:56:12,880
I feel like I owe them a greater debt of gratitude because that means you listen to the whole thing

1133
01:56:12,880 --> 01:56:18,080
and hopefully that suggests that you enjoyed it. If you are at the end and you enjoyed this podcast,

1134
01:56:18,080 --> 01:56:22,560
could you do me a little bit of a favor and hit that subscribe button? That's one of the clearest

1135
01:56:22,560 --> 01:56:26,080
indicators we have that this episode was a good episode and we look at that on all of the episodes

1136
01:56:26,080 --> 01:56:31,280
to see which episodes generated the most subscribers. Thank you so much and I'll see you again next time.

