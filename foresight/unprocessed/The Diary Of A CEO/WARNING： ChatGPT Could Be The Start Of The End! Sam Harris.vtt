WEBVTT

00:00.000 --> 00:02.400
Artificial intelligence is superhuman.

00:02.400 --> 00:06.800
It is smarter than you are and there's something inherently dangerous for the

00:06.800 --> 00:09.800
dumber party in that relationship.

00:09.800 --> 00:12.200
You just can't put the genie back in the bottle.

00:12.200 --> 00:13.000
Damn Harris!

00:13.000 --> 00:15.800
Neuroscientist, philosopher, author, podcaster.

00:15.800 --> 00:20.600
He goes into intellectual territory where few others dare tread.

00:20.600 --> 00:22.400
Six years ago you dared talk.

00:22.400 --> 00:26.600
The gains we make in artificial intelligence could ultimately destroy us.

00:26.600 --> 00:31.200
If your objective was to make humanity happy and there was a button placed in front of you

00:31.200 --> 00:35.000
and it would end artificial intelligence, what would you do?

00:35.000 --> 00:37.600
Well, I would definitely pause it.

00:37.600 --> 00:43.000
The idea that we've lost the moment to decide whether to hook our most powerful AI to everything

00:43.000 --> 00:48.600
is just, oh, it's already connected to the internet, got millions of people using it.

00:48.600 --> 00:52.600
And the idea that these things will stay aligned with us because we have built them,

00:52.600 --> 00:55.200
yet we gave them a capacity to rewrite their code.

00:55.200 --> 00:56.600
There's just no reason to believe that.

00:56.600 --> 01:02.200
And I worry about the near-term problem of what humans do with increasingly powerful AI,

01:02.200 --> 01:04.600
how it amplifies misinformation.

01:04.600 --> 01:08.000
Most of what's online could soon be fake.

01:08.000 --> 01:13.400
Can we hold a presidential election 18 months from now that we recognize as valid?

01:13.400 --> 01:14.600
Like, is it safe?

01:14.600 --> 01:16.600
And it just gets scarier and scarier.

01:16.600 --> 01:20.400
I worry we're just going to have to declare bankruptcy to the internet.

01:20.400 --> 01:22.000
The internet, the internet.

01:22.000 --> 01:27.200
If your intuition is correct, are you optimistic about our chances of survival?

01:52.200 --> 01:56.200
Sam, six years ago, you did a TED Talk.

01:56.200 --> 01:59.200
I watched that TED Talk a few times over the last week.

01:59.200 --> 02:04.200
And the TED Talk was called, Can We Build AI Without Losing Control Over It?

02:04.200 --> 02:08.200
In that TED Talk, you really discussed the idea of whether AI,

02:08.200 --> 02:12.200
when it gets to a certain point of sentience in the internet,

02:12.200 --> 02:15.200
can build AI without losing control over it.

02:15.200 --> 02:19.200
In that TED Talk, you really discussed the idea of whether AI,

02:19.200 --> 02:23.200
when it gets to a certain point of sentience and intelligence,

02:23.200 --> 02:28.200
will wreak havoc on humanity.

02:28.200 --> 02:32.200
Six years later, where do you stand on it today?

02:32.200 --> 02:40.200
Do you think, are you optimistic about our chances of survival?

02:40.200 --> 02:42.200
Yeah, I can't say I'm optimistic.

02:42.200 --> 02:51.200
I'm worried about two species of problem here that are related.

02:51.200 --> 02:58.200
There's the near-term problem of just what humans do with increasingly powerful AI

02:58.200 --> 03:05.200
and how it amplifies the problem of misinformation and disinformation

03:05.200 --> 03:09.200
and just makes it harder and harder to make sense of reality together.

03:13.200 --> 03:18.200
And then there's just the longer-term concern about what's called alignment

03:18.200 --> 03:24.200
with artificial general intelligence, where we build AI that is truly general

03:24.200 --> 03:30.200
and by definition superhuman in its competence and power.

03:30.200 --> 03:35.200
And then the question is, have we built it in such a way that is aligned

03:35.200 --> 03:38.200
in a durable way with our interests?

03:38.200 --> 03:44.200
And there's some people who just don't see this problem.

03:44.200 --> 03:47.200
They're kind of blind to it.

03:47.200 --> 03:51.200
When I'm in the presence of someone who doesn't share this intuition,

03:51.200 --> 03:56.200
they don't resonate to it, I just don't understand what they're doing

03:56.200 --> 03:59.200
or not doing with their minds in that moment.

03:59.200 --> 04:01.200
Let's say I'm wrong about that.

04:01.200 --> 04:03.200
Well, then it's just the other person's right.

04:03.200 --> 04:08.200
We just have fundamentally different intuitions about this particular point.

04:08.200 --> 04:14.200
And the point is this, if you're imagining building true artificial general intelligence

04:14.200 --> 04:18.200
that is superhuman, and that is what everyone, whatever their intuitions,

04:18.200 --> 04:20.200
purports to be imagining here.

04:20.200 --> 04:23.200
There are people on both sides of the alignment debate.

04:23.200 --> 04:28.200
There are people who think alignment is a real problem and people think it's a total fiction.

04:28.200 --> 04:32.200
But everyone, firstly everyone who's party to this conversation agrees

04:32.200 --> 04:40.200
that we will ultimately build artificial general intelligence that will be superhuman in its capacities.

04:40.200 --> 04:45.200
And there's very little you have to assume to be confident that we're going to do that.

04:45.200 --> 04:47.200
There's really just two assumptions.

04:47.200 --> 04:51.200
One is that intelligence is substrate independent.

04:51.200 --> 04:53.200
It doesn't have to be made of meat.

04:53.200 --> 04:55.200
It can be made in silico.

04:55.200 --> 04:58.200
And we've already proven that with narrow AI.

04:58.200 --> 05:01.200
We obviously have intelligent machines.

05:01.200 --> 05:05.200
And your calculator in your phone is better than you are at arithmetic.

05:05.200 --> 05:09.200
And that's some very narrow band of intelligence.

05:09.200 --> 05:14.200
So as we keep building intelligent machines on the assumption

05:14.200 --> 05:18.200
that there's nothing magical about having a computer made of meat,

05:18.200 --> 05:22.200
the only other thing you have to assume is that we will keep doing this.

05:22.200 --> 05:24.200
We will keep making progress.

05:24.200 --> 05:31.200
And eventually we will be in the presence of something more intelligent than we are.

05:31.200 --> 05:33.200
And that's not assuming Moore's law.

05:33.200 --> 05:35.200
It's not assuming exponential progress.

05:35.200 --> 05:37.200
We just have to keep going.

05:37.200 --> 05:42.200
And when you look at the reasons why we wouldn't keep going, those are all just terrifying.

05:42.200 --> 05:47.200
Because intelligence is so valuable and we're so incentivized to have more of it.

05:47.200 --> 05:49.200
And every increment of it is valuable.

05:49.200 --> 05:54.200
It's not like it only gets valuable when you double it or 10X it.

05:54.200 --> 06:00.200
No, no, if you just get three more percent, that pays for itself.

06:00.200 --> 06:04.200
So we're going to keep doing this.

06:04.200 --> 06:09.200
Our failure to do it suggests that something terrible has happened in the meantime.

06:09.200 --> 06:10.200
We've had a world war.

06:10.200 --> 06:13.200
We've had a global pandemic far worse than COVID.

06:13.200 --> 06:15.200
We got hit by an asteroid.

06:15.200 --> 06:21.200
Something happened that prevented us as a species from continuing to make progress

06:21.200 --> 06:23.200
in building intelligent machines.

06:23.200 --> 06:26.200
So absent that, we're going to keep going.

06:26.200 --> 06:31.200
We will eventually be in the presence of something smarter than we are.

06:31.200 --> 06:34.200
And this is where intuitions divide.

06:34.200 --> 06:42.200
My intuition, and it's shared by many people, I know at least one who you've spoken to,

06:42.200 --> 06:53.200
my intuition is that there is something inherently dangerous for the dumber party in that relationship.

06:53.200 --> 07:00.200
There's something inherently dangerous for the dumber species to be in the presence of the smarter species.

07:00.200 --> 07:07.200
And we have seen this based on our entanglement with all other species, dumber than we are.

07:07.200 --> 07:12.200
Or certainly less competent than we are.

07:12.200 --> 07:22.200
And so reasoning by analogy, it would be true of something smarter than we are.

07:22.200 --> 07:28.200
People imagine that because we have built these machines, that is no longer true.

07:28.200 --> 07:33.200
But here's where my intuition goes from there.

07:33.200 --> 07:41.200
That imagination is born of not taking intelligence seriously.

07:41.200 --> 07:49.200
Because what intelligence is, is a mismatch in intelligence in particular,

07:49.200 --> 08:00.200
is a fundamental lack of insight into what the smarter party is doing and why it's doing it

08:00.200 --> 08:04.200
and what it will do next on the part of the dumber party.

08:04.200 --> 08:16.200
You just could imagine that by analogy, just imagine that the dogs had invented us as their super intelligent AIs.

08:16.200 --> 08:21.200
For the purpose of making their lives better, just securing resources for them,

08:21.200 --> 08:27.200
securing comfort for them, getting them medical attention.

08:28.200 --> 08:32.200
It's been working out pretty well for the dogs for about 10,000 years.

08:32.200 --> 08:35.200
There's some exceptions. We mistreat certain dogs.

08:35.200 --> 08:42.200
But generally speaking, for most dogs, most of the time, humans have been a great invention.

08:42.200 --> 08:56.200
Now, it's true that the mismatch in our intelligence dictates a fundamental blindness with respect to what we've become in the meantime.

08:56.200 --> 09:02.200
We have all these instrumental goals and things we care about that they cannot possibly conceive.

09:02.200 --> 09:08.200
They know that when we go get the leash and say, it's time for a walk, they understand that particular part of the language game.

09:08.200 --> 09:14.200
But everything else we do when we're talking to each other, when we're on our computers or on our phones,

09:14.200 --> 09:18.200
they don't have the dimmest idea of what we're up to.

09:19.200 --> 09:26.200
The truth is, we love our dogs. We make irrational sacrifices for our dogs.

09:26.200 --> 09:32.200
We prioritize their health over all kinds of things that it's just amazing to consider.

09:32.200 --> 09:43.200
And yet, if there was a new global pandemic kicking off and some Xenovirus was jumping from dogs to humans,

09:43.200 --> 09:48.200
and it was just super Ebola, it was 90% lethal.

09:48.200 --> 09:56.200
And this was just a forced choice between what do you value more, the lives of your dogs or the lives of your kids?

09:56.200 --> 10:01.200
If that's a situation we were in, it's totally conceivable.

10:01.200 --> 10:08.200
By no means impossible. We would just kill all the dogs, and they would never know why.

10:08.200 --> 10:18.200
And it's because we have this layer of mind and culture and just the new sphere.

10:18.200 --> 10:29.200
There's this realm of mind that requires a requisite level of intelligence to even be partied to, to even know exists,

10:29.200 --> 10:33.200
that they have no idea it exists.

10:33.200 --> 10:42.200
And this is a fanciful analogy because the dogs did not invent us, but evolution invented us.

10:42.200 --> 10:49.200
Evolution has coded us, as I said, to survive and spawn, and that's it.

10:49.200 --> 10:57.200
So evolution can't see everything else we've done with our time and attention and all the values we've formed in the meantime.

10:57.200 --> 11:03.200
And all the ways in which we have explicitly disavowed the program we've been given.

11:03.200 --> 11:11.200
So evolution gave us a program, but if we were really going to live by the lights of that program, what would we be doing?

11:11.200 --> 11:14.200
We would be having as many kids as possible.

11:14.200 --> 11:22.200
Guys would be going to sperm banks and donating their sperm and finding that the best use of their time and attention.

11:22.200 --> 11:28.200
Like the idea that you could have hundreds of kids for which you have no financial responsibility.

11:28.200 --> 11:36.200
That would be the, that should be the most rewarding thing that you could possibly do with your time as a man.

11:36.200 --> 11:40.200
And yet that's obviously not what we do.

11:40.200 --> 11:42.200
And there are people who decide not to have kids.

11:43.200 --> 11:52.200
And yet, and everything else we do from having podcast conversations like this to curing diseases,

11:52.200 --> 12:05.200
literally everything we're doing with science, with culture is, yes, there are points of contact between those products and our evolved capacities.

12:06.200 --> 12:07.200
It's not magic.

12:07.200 --> 12:14.200
We are social primates that have leveraged certain ancient hardware to do new things.

12:14.200 --> 12:19.200
But the code that we've been given doesn't see any of that.

12:19.200 --> 12:24.200
And we've not been optimized to build democracies.

12:24.200 --> 12:26.200
Evolution knows nothing.

12:26.200 --> 12:27.200
It can know nothing.

12:27.200 --> 12:35.200
If evolution were a coder, there's just no, there's no democracy maximization in that code, right?

12:35.200 --> 12:38.200
It's just, it's not, it's just not there.

12:38.200 --> 12:44.200
So the idea that these things will stay aligned with us because we have built them,

12:44.200 --> 12:47.200
because if we have this origin story that we gave them their initial code,

12:47.200 --> 12:55.200
and yet we gave them a capacity to rewrite their code and build future generations of themselves, right?

12:55.200 --> 12:57.200
There's just no reason to believe that.

12:57.200 --> 13:03.200
I see no, and the mismatch in intelligence is intrinsically dangerous.

13:03.200 --> 13:08.200
And you could see this by, I mean, Stuart Russell, I don't know if you had him on the podcast.

13:08.200 --> 13:12.200
He's a great professor of computer science at Berkeley.

13:12.200 --> 13:19.200
And he wrote, literally co-wrote one of the most popular textbooks on AI.

13:19.200 --> 13:24.200
I mean, he has some arresting analogies, which I think are good intuition pumps here.

13:24.200 --> 13:33.200
And one is, just think of how you would feel if you knew, like, let's say we got a communication from elsewhere in the galaxy.

13:33.200 --> 13:40.200
And it was a message that we decoded and it said, people of Earth, we will arrive on your lowly planet in 50 years.

13:40.200 --> 13:44.200
Get ready, right?

13:44.200 --> 13:53.200
That, anyone who thinks that we're going to get super intelligent AI in, let's say, 50 years,

13:53.200 --> 14:00.200
thinks we're essentially in that situation and yet we're not responding emotionally to it in the same way.

14:00.200 --> 14:10.200
If we received a communication from a species that we knew just by the sheer fact that they were communicating with us in this way,

14:10.200 --> 14:14.200
we knew they're more competent and more powerful and more intelligent than we are, right?

14:14.200 --> 14:16.200
And they're going to arrive, right?

14:16.200 --> 14:27.200
We would feel that we were on the threshold of the most momentous change in the history of our species.

14:27.200 --> 14:38.200
And we would feel, but most importantly, we would feel that it's because this is a relationship, an unavoidable relationship that's being foisted upon us, right?

14:39.200 --> 14:48.200
A new creature is coming into the room with its own capacities and now you're in relationship.

14:48.200 --> 14:53.200
And one thing is absolutely certain, it is smarter than you are, right?

14:53.200 --> 15:06.200
By what factor, I mean, ultimately we're talking about, by factors, just by so many orders of magnitude, our intuitions completely fail.

15:06.200 --> 15:17.200
I mean, even if it was just a difference in the time of processing, let's say there was no difference in the actual native intelligence,

15:17.200 --> 15:29.200
but it's just processing speed, a million-fold difference in processing speed is just a phantasmagorical difference in capacity.

15:30.200 --> 15:39.200
Just imagine we had 10 smart guys in a room over there and they were working and thinking and talking a million times faster than we are, right?

15:39.200 --> 15:42.200
Well, so they're no smarter than we are, but they're just faster.

15:42.200 --> 15:51.200
And we talk to them once every two weeks just to catch up on what they're up to and what they want to do and whether they still want to collaborate with us.

15:51.200 --> 15:57.200
Well, two weeks for us is 20,000 years of analogous progress for them.

15:57.200 --> 16:13.200
So how could we possibly hope to constrain the opinions and collaborate with and negotiate with people no smarter than ourselves who are making 20,000 years of progress every time we make two weeks of progress, right?

16:13.200 --> 16:16.200
It's just, it's unimaginable.

16:16.200 --> 16:20.200
And yet there are many people who don't just think this is just fiction.

16:20.200 --> 16:30.200
Everything I, all the noises I've made in the last five minutes are just like a new religion of fear, right?

16:30.200 --> 16:36.200
And it's just there's no reason to think that alignment is even a potential problem.

16:36.200 --> 16:50.200
If your intuition is correct and that analogy of us getting a signal from outer space that someone is coming in 30 years, which by the way, a lot of people that speak on this subject matter, don't believe it's even going to be 30 years until we reach that sort of singularity moment.

16:50.200 --> 16:52.200
I think they speak of artificial general intelligence.

16:52.200 --> 17:00.200
I've heard people like Elon say, you know, many fewer decades, 10, 10 years, 15 years, 20 years, etc.

17:00.200 --> 17:08.200
If that is correct, then surely this is the most pressing challenge, conversation issue of our time.

17:08.200 --> 17:16.200
And there's no logical reason that I can see to refute your intuition there.

17:16.200 --> 17:18.200
I can't see a logical reason.

17:18.200 --> 17:20.200
The rate of progress will continue.

17:20.200 --> 17:26.200
Don't necessarily see anything that will wipe out or pause our rate of progress.

17:26.200 --> 17:30.200
Let me just be charitable to the other side here.

17:30.200 --> 17:38.200
There are other assumptions that they smuggle in that some do it without being aware of it, but some actually believe these assumptions.

17:38.200 --> 17:44.200
And this spells the difference on this particular intuition.

17:44.200 --> 17:53.200
So it's possible to assume that the more intelligent you get, the more ethical you become by definition right now.

17:53.200 --> 18:03.200
And we might draw a somewhat more equivocal picture from just the human case where we see that, oh, there's some very smart people who aren't that ethical.

18:03.200 --> 18:19.200
But I believe there are people, and I've talked to at least a few people who believe this, there are people who assume they're kind of in the limit as you push out into just far beyond human levels of intelligence.

18:19.200 --> 18:31.200
There's every reason to believe that all of the provincial, creaturely failures of human ethics will be left behind as well.

18:31.200 --> 18:36.200
It's like you're not like the selfishness and the basis for conflict.

18:37.200 --> 18:48.200
The apish urges of status-seeking monkeys is just not going to be in the code.

18:48.200 --> 19:02.200
And as you push out into the omnibus genius of the coming AI, there's a kind of a sainthood that's going to come along with it and a wisdom that will come along with it.

19:02.200 --> 19:06.200
Now, I just think that's quite a gamble.

19:06.200 --> 19:10.200
I would take the other side of that bet and I would frame it this way.

19:10.200 --> 19:19.200
There have to be ways in the space of all possible intelligences that are beyond the human, right?

19:19.200 --> 19:21.200
There's got to be more than one possible.

19:21.200 --> 19:28.200
It's just like there's many different ways to have a chess engine that's better than I am at chess.

19:28.200 --> 19:33.200
They're different from each other, but they're all better than me, right?

19:33.200 --> 19:40.200
There's got to be more than one way to have a superhuman artificial intelligence.

19:40.200 --> 19:50.200
And I would imagine there are not an infinite number of ways, but just a vast number of...

19:50.200 --> 20:00.200
In the space of all possible minds, there are many locations in that space beyond the human that are not aligned with human well-being.

20:00.200 --> 20:05.200
There's got to be more ways to build this unaligned than aligned, right?

20:05.200 --> 20:12.200
And what other people are smuggling into this conversation is the intuition that, no, no, once you get beyond the human,

20:13.200 --> 20:20.200
you're going to be in the presence of just the Buddha who understands quantum mechanics and oncology and everything else, right?

20:20.200 --> 20:23.200
I just see no reason to think that that's so.

20:23.200 --> 20:29.200
And we could build something that is, again, taken intelligence seriously.

20:29.200 --> 20:32.200
We're going to build something that we're in relationship to.

20:32.200 --> 20:35.200
It's really intelligent in all the ways that we're intelligent.

20:35.200 --> 20:38.200
It's just better at all of those things than we are.

20:38.200 --> 20:42.200
It's, by definition, superhuman because the only way it wouldn't be superhuman,

20:42.200 --> 20:48.200
the only way it would be human level, even for 15 minutes, is if we didn't let it improve itself,

20:48.200 --> 20:52.200
if we wanted to just keep it stuck at a...

20:52.200 --> 20:56.200
We built a college undergraduate and we wanted just to keep it stuck there,

20:56.200 --> 21:01.200
but we would have to dumb down all of the specific capacities we've already built, right?

21:01.200 --> 21:06.200
Just like every AI we have, narrow AI, is superhuman for the thing it does.

21:07.200 --> 21:11.200
It has access to all the information on the Internet, right?

21:11.200 --> 21:13.200
It's got perfect memories.

21:13.200 --> 21:15.200
It can perfectly copy itself.

21:15.200 --> 21:21.200
When one part of the system learns something, the rest of the system learns it because it just can swap files, right?

21:21.200 --> 21:26.200
It's, again, your phone is a superhuman calculator.

21:26.200 --> 21:30.200
There's no reason to make it a calculator that is human level.

21:30.200 --> 21:32.200
And so we're never going to do that.

21:32.200 --> 21:35.200
We're never going to be in the presence of human AGI.

21:35.200 --> 21:39.200
We will be immediately in the presence of superhuman AGI.

21:39.200 --> 21:47.200
And then the question is how quickly it improves and how much headroom is there to improve into.

21:47.200 --> 21:52.200
On the assumption that you can get quite a bit more intelligent than we are, right,

21:52.200 --> 21:57.200
that we're nowhere near the summit of possible intelligence,

21:57.200 --> 22:03.200
you have to imagine that you're going to be in the presence of something that is, again,

22:03.200 --> 22:05.200
it could be completely unconscious, right?

22:05.200 --> 22:10.200
I'm not saying that there's something that's like to be this thing, although there might be.

22:10.200 --> 22:14.200
And that's a totally different problem that's worth worrying about.

22:14.200 --> 22:21.200
But whether conscious or not, it is solving problems, detecting problems,

22:21.200 --> 22:28.200
improving its capacity to do all of that in ways that we can't possibly understand.

22:28.200 --> 22:35.200
And the products of its increasing competence are always being surfaced, right?

22:35.200 --> 22:40.200
So it's like we've been using it to change the world.

22:40.200 --> 22:42.200
We've become reliant upon it.

22:42.200 --> 22:44.200
We built this thing for a reason.

22:44.200 --> 22:49.200
I mean, one thing that's been amazing about developments in recent months is that

22:49.200 --> 22:54.200
those of us who have been at all cognizant of the AI safety space

22:55.200 --> 22:58.200
now going on a decade or more for some people,

22:58.200 --> 23:04.200
always assumed that as we got closer to the end zone,

23:04.200 --> 23:10.200
that the labs would become more circumspect, we'd be building this stuff air-gapped from the internet.

23:10.200 --> 23:13.200
It's like we have this phrase air-gapped from the internet.

23:13.200 --> 23:14.200
We thought this was a thing.

23:14.200 --> 23:16.200
This thing would be in a box.

23:16.200 --> 23:21.200
And then the question would be, well, do we let it out of the box and let it do something?

23:21.200 --> 23:22.200
Like, is it safe?

23:22.200 --> 23:24.200
And how do we know if it's safe?

23:24.200 --> 23:26.200
And we thought we would have that moment.

23:26.200 --> 23:30.200
We thought it would happen in a lab at Google or at Facebook or somewhere.

23:30.200 --> 23:33.200
We thought we would hear, OK, we've got something really impressive,

23:33.200 --> 23:36.200
and now we just want it to touch the stock market,

23:36.200 --> 23:39.200
or we want it to touch our medical data,

23:39.200 --> 23:42.200
or we just want to see if we can use it.

23:42.200 --> 23:44.200
We're way past that.

23:44.200 --> 23:46.200
We've built this stuff already in the wild.

23:46.200 --> 23:49.200
It's already connected to the internet.

23:49.200 --> 23:52.200
It's already got millions of people using it.

23:52.200 --> 23:54.200
It already has APIs.

23:54.200 --> 23:57.200
It's already doing work.

23:57.200 --> 23:59.200
From an AI safety point of view, that's amazing.

23:59.200 --> 24:05.200
We didn't even have the choice point we thought was going to be so fraught.

24:05.200 --> 24:10.200
Of course we didn't, because there was such pressing incentives

24:10.200 --> 24:13.200
for people to press forward regardless of that conversation.

24:13.200 --> 24:21.200
But everyone thought, I mean, I don't believe I was ever in conversation with someone,

24:21.200 --> 24:27.200
someone like L.A.'s or Yudikowski or Nick Bostrom or Stuart Russell,

24:27.200 --> 24:32.200
who assumed we would be in this spot.

24:32.200 --> 24:38.200
I'd have to go back and look at those conversations,

24:38.200 --> 24:41.200
but there was so much time spent.

24:41.200 --> 24:49.200
It seems quite unnecessarily on this idea that we'd make a certain amount of progress,

24:49.200 --> 24:52.200
and circumspection would kick in.

24:52.200 --> 24:57.200
Even the people who were doubters would become worried.

24:57.200 --> 25:02.200
In the final yards, as we go across into the end zone,

25:02.200 --> 25:06.200
there'd be some mode where we could slow down and figure it out

25:06.200 --> 25:09.200
and try to deal with the arms race dynamics.

25:09.200 --> 25:14.200
We could place a phone call to China and just talk about this,

25:14.200 --> 25:19.200
we got something interesting, but the stuff is already being built in connection to everything.

25:19.200 --> 25:28.200
There's already just endless businesses being devised on the back of this thing,

25:28.200 --> 25:32.200
and all the improvements are going to get plowed into it.

25:32.200 --> 25:35.200
Just imagine what this looks like even in success.

25:35.200 --> 25:38.200
I'll say it just starts working wonders for us,

25:38.200 --> 25:45.200
and we get these great productivity gains.

25:45.200 --> 25:50.200
Then we cross into whatever the singularity is,

25:50.200 --> 25:54.200
at whatever speed we find ourselves in the presence of something that is truly general.

25:54.200 --> 26:00.200
After all of this narrow stuff, albeit superhuman narrow stuff,

26:00.200 --> 26:05.200
is something that we totally depend on.

26:05.200 --> 26:09.200
Every hospital requires it, and every airplane requires it,

26:09.200 --> 26:12.200
and all of our missile systems require it.

26:12.200 --> 26:17.200
This is the way we do business.

26:17.200 --> 26:22.200
There's nothing to turn off at that point.

26:22.200 --> 26:25.200
I put this to Mark Andreessen on my podcast, and he said,

26:25.200 --> 26:29.200
if you can turn off the Internet, I can't believe he was quite serious.

26:29.200 --> 26:33.200
If you're North Korea, I guess you can turn off the Internet for North Korea,

26:33.200 --> 26:36.200
and that's why North Korea is like North Korea.

26:36.200 --> 26:51.200
The cost of turning off the Internet now would be unimaginable.

26:51.200 --> 26:57.200
The atomic cost alone, it just would be...

26:57.200 --> 27:08.200
The idea that we've lost the moment to decide whether to hook our most powerful AI to everything,

27:08.200 --> 27:13.200
because it's already being built more or less in contact with, if not everything,

27:13.200 --> 27:18.200
so many things that you just can't put the genie back in the bottle,

27:18.200 --> 27:23.200
that is genuinely surprising to me, and yeah, I mean, incentives.

27:23.200 --> 27:27.200
Is this not the most pressing problem, though?

27:27.200 --> 27:32.200
I was going to ask this conversation by asking you the question about the thing that occupies your mind the most,

27:32.200 --> 27:34.200
and the most important thing we should be talking about,

27:34.200 --> 27:38.200
and I in part assumed the answer would be artificial intelligence,

27:38.200 --> 27:41.200
because the way that you talk about your intuition on this subject matter,

27:41.200 --> 27:46.200
you've got children, you think about the future a lot.

27:46.200 --> 27:52.200
If you can see this species coming to Earth, even if it's in the next 100 years,

27:52.200 --> 27:56.200
it strikes me to be the most pressing problem for humanity.

27:56.200 --> 28:05.200
Well, as interesting as I think that problem is, and consequential as it is,

28:05.200 --> 28:11.200
I'm worried that life could become unlivable in the near term before we even get there.

28:11.200 --> 28:15.200
I'm just worried about the misuses of narrow AI in the meantime.

28:15.200 --> 28:19.200
I'm worried about, just take the current level of AI we have.

28:19.200 --> 28:25.200
We have GPT-4.

28:25.200 --> 28:32.200
I think within the next 12 months or two years, let's say whatever GPT-5 is,

28:32.200 --> 28:38.200
we're going to be in the presence of something where most of what's online

28:38.200 --> 28:42.200
that purports to be information could soon be fake.

28:43.200 --> 28:49.200
Most of the text you find on any topic is just fake.

28:49.200 --> 28:57.200
Someone has just decided, write me a thousand journal articles on why mRNA vaccines cause cancer,

28:57.200 --> 29:01.200
and give me 150 citations, write them in the style of nature,

29:01.200 --> 29:06.200
and nature genetics, and Lancet, and JAMA, and just put them out there.

29:06.200 --> 29:11.200
One teenager could do that in five minutes with the right AI.

29:12.200 --> 29:18.200
GPT-4 is not quite that, but GPT-5 possibly will be that.

29:18.200 --> 29:21.200
That is such a near-term advance.

29:21.200 --> 29:28.200
When you imagine knitting together the visual stuff like mid-journey, and dolly,

29:28.200 --> 29:34.200
and stable diffusion with a large language model, just imagine the tool.

29:35.200 --> 29:40.200
Maybe this is 18 months away, maybe it's three years away, but it's not 30 years away.

29:40.200 --> 29:47.200
The tool where you can just say, give me a 45-minute documentary on how the Holocaust never happened,

29:47.200 --> 29:54.200
filled with archival imagery, give me Hitler speaking in German with the appropriate translations,

29:54.200 --> 30:01.200
and give it in the style of Alex Gibney or Ken Burns.

30:02.200 --> 30:07.200
Give me 10,000 of those.

30:07.200 --> 30:13.200
All the friction for misinformation has been taken out of the system.

30:13.200 --> 30:19.200
I worry we're just going to have to declare bankruptcy with respect to the internet.

30:19.200 --> 30:24.200
We just are not going to be able to figure out what's real.

30:24.200 --> 30:34.200
When you look at how hard that is now with social media in the aftermath of COVID and Trump,

30:34.200 --> 30:41.200
just the challenge of holding an election that most of the population agrees was valid.

30:41.200 --> 30:51.200
That challenge already is on the verge of being insurmountable in the US.

30:51.200 --> 30:56.200
It's easy to see us failing at that, AI aside.

30:56.200 --> 31:02.200
When you add large language models to that and the more competent future version of it,

31:02.200 --> 31:11.200
where it's just the most compelling deep fakes are indistinguishable from real data.

31:11.200 --> 31:18.200
Everyone is siloed into their tribes where they're stigmatizing the information that comes from any other tribe.

31:18.200 --> 31:24.200
The internet is now so big a place that there really isn't the ordinary selection pressures

31:24.200 --> 31:28.200
where bad information gets successfully debunked so that it goes away.

31:28.200 --> 31:34.200
You can live in a conspiracy cult for the rest of your life if you want to.

31:34.200 --> 31:38.200
You can be queuing on all day long if you want to.

31:38.200 --> 31:48.200
Now we've got deep fakes shoring all that up and just spurious scientific articles shoring all that up.

31:48.200 --> 31:54.200
All of this just becomes a more compelling form of psychosis and culturally speaking.

31:54.200 --> 32:03.200
I'm just worried that it's going to get harder and harder for us to cooperate with one another and collaborate

32:03.200 --> 32:13.200
and that our politics will just completely break and that'll offer an opportunity for lots of bad actors.

32:13.200 --> 32:26.200
Leaving aside, there's cyber terrorism and there's synthetic biology that the moment you turn AI loose on the prospect of engineering viruses

32:27.200 --> 32:40.200
The asymmetry here is that it seems like it's always easier to break things than to fix them or to categorically prevent people from breaking them.

32:40.200 --> 32:51.200
What we have with increasingly powerful technology is the ability for one person to create more and more damage or one small group of people.

32:52.200 --> 32:59.200
It just turns out it's hard enough to build a nuclear bomb that one person can't really do it no matter how smart.

32:59.200 --> 33:10.200
You need a team and traditionally you've needed state actors and you need access to resources and you have to get the fissile material and it's hard enough.

33:11.200 --> 33:21.200
This is being fully democratized this tech and so I worry about the near term chaos.

33:21.200 --> 33:28.200
I've never found the narrow term consequences of artificial intelligence to be that interesting until now.

33:28.200 --> 33:32.200
That image of the internet becoming unusable.

33:32.200 --> 33:36.200
So that was a real eureka moment for me because I've not been thinking about that.

33:36.200 --> 34:05.200
Yeah, me too. I was just concerned about the AGI risk and now really in the aftermath of Trump and COVID, I see the risk of, if not losing everything, losing a lot that matters just based on our interacting with these very simple tools

34:05.200 --> 34:08.200
that are reliably misleading us.

34:08.200 --> 34:14.200
I'm amazed at what social media, I'm amazed at what Twitter did to me.

34:14.200 --> 34:31.200
Even with all of my training and with my head screwed on reasonably straight, it's amazing to say it, but almost all of the truly bad things that have happened to me in the last decade

34:31.200 --> 34:40.200
that just destabilized relationships and priorities and really kind of got plowed back into me.

34:40.200 --> 34:47.200
It became a kind of professional emergency, stuff I had to respond to in writing or on podcasts.

34:47.200 --> 34:49.200
It was all Twitter.

34:49.200 --> 34:57.200
My engagement with Twitter was the thing that produced the chaos and it was completely unnecessary.

34:57.200 --> 35:07.200
It was amplifying a kind of signal for me that I felt compelled to pay attention to because I was on it and I was trying to communicate with people on it.

35:07.200 --> 35:17.200
I was getting certain communication back and it was giving me a picture of the rest of humanity, which I now think was fundamentally misleading, but it was still consequential.

35:18.200 --> 35:29.200
A certain point believing that it was misleading wasn't enough to inoculate me against the delusion of the opinion change that was being forced upon me.

35:29.200 --> 35:34.200
I was feeling like these people are becoming unrecognizable.

35:34.200 --> 35:35.200
I know some of these people.

35:35.200 --> 35:45.200
I've had dinner with some of these people and their behavior on Twitter is appearing so deranged to me in such bad faith.

35:46.200 --> 35:54.200
People who I know to be non-psychopaths are starting to behave like psychopaths, at least on Twitter.

35:54.200 --> 35:59.200
I'm becoming similarly unrecognizable to them.

35:59.200 --> 36:11.200
It all felt like a psychological experiment to which I hadn't consented and which I enrolled myself somehow because it was what everyone was doing in 2009.

36:11.200 --> 36:17.200
I spent 12 years there getting some signal and responding to it.

36:17.200 --> 36:20.200
It's not to say that it was all bad.

36:20.200 --> 36:34.200
I read a bunch of good articles that got linked there and I discovered some interesting people, but the change in my life after I deleted my Twitter account was so enormous.

36:34.200 --> 36:37.200
It's embarrassing to admit it.

36:38.200 --> 36:41.200
It's like getting out of a bad relationship.

36:41.200 --> 36:58.200
It was a fundamental freedom from this chaos monster that was always there ready to disrupt something based on its own dynamics.

36:58.200 --> 37:00.200
When did you delete it?

37:00.200 --> 37:03.200
I think it was December.

37:04.200 --> 37:06.200
I'm not someone that really takes sides on things.

37:06.200 --> 37:08.200
I like to try and remain in the middle.

37:08.200 --> 37:11.200
So you must have a very different Twitter experience than I was having?

37:11.200 --> 37:13.200
No.

37:13.200 --> 37:17.200
So I don't tweet anything other than this podcast trailer.

37:17.200 --> 37:19.200
I don't tweet anything else.

37:19.200 --> 37:21.200
The only thing you'll see on my Twitter is the podcast trailer.

37:21.200 --> 37:23.200
That's it.

37:23.200 --> 37:31.200
For all the reasons you've described, and more interestingly, I wanted to say in the last eight months, as someone that doesn't get caught up too much in the media,

37:31.200 --> 37:35.200
oh, Elon bought this, it's 100% gone in that direction.

37:35.200 --> 37:43.200
As in my timeline now is, I say it to my friends all the time, and some of my friends who are again, I think are nuanced and balanced have said to me,

37:43.200 --> 37:51.200
there's something that's been turned up in the algorithm to increase engagement that has planted me in an unpleasant echo chamber that I didn't desire to be in.

37:51.200 --> 37:56.200
And if I wasn't somewhat conscious, I would 100% be in there.

37:56.200 --> 38:04.200
My timeline, my friend tweeted the other day, my friend Cahill tweeted, he's never seen more people die on his Twitter timeline than he has in the last six months.

38:04.200 --> 38:09.200
They're prioritizing video, so you're seeing a lot of like death and CCTV footage that I've never seen before.

38:09.200 --> 38:20.200
And then the debate around gender, politics, right leaning subject matter has never been more right down your throat.

38:20.200 --> 38:27.200
Because it's almost like something in the algorithm has been switched, where it's now, people have been let out of the asylum.

38:27.200 --> 38:31.200
That's the only way I can describe it, and it's made me retract even more.

38:31.200 --> 38:40.200
So when Zuckerberg announced threads the other couple of weeks ago, it was kind of like a life raft out of the Titanic.

38:40.200 --> 38:47.200
And I really, really mean that, and I'm not someone to get easily caught up in narrative as it relates to social media platforms.

38:47.200 --> 38:49.200
It's been my industry for a decade.

38:49.200 --> 38:55.200
What I've seen on Twitter, and it's actually made me believe this hypothesis I had five years ago where I thought there would be,

38:55.200 --> 39:01.200
I thought the route, the journey of social networking would have way more social networks and there'd be more siloed.

39:01.200 --> 39:06.200
I thought we'd have one for our neighborhood, our football club, and now I believe that even more than ever.

39:06.200 --> 39:08.200
Yeah, that seems right.

39:08.200 --> 39:15.200
And I think it's, I mean, whether it's possible to have a truly healthy social network that people want to be in,

39:15.200 --> 39:20.200
and it's a good reason to be there, and it's, I don't know if it's possible.

39:20.200 --> 39:32.200
I'd like to think it is, but it's, I think there are certain things you have to clean up at the outset that is supposed to make it possible.

39:32.200 --> 39:34.200
And I think, I think anonymity is a bad thing.

39:34.200 --> 39:38.200
I think probably being free is a bad thing.

39:38.200 --> 39:41.200
I think, you know, you sort of get what you pay for online.

39:41.200 --> 39:47.200
And if it's, I just think there might be ways to set it up that would be better, but.

39:47.200 --> 39:49.200
I don't think it'd be popular.

39:49.200 --> 39:50.200
What was that?

39:50.200 --> 39:52.200
I think with the thing that makes it popular makes it toxic.

39:52.200 --> 39:53.200
Right.

39:53.200 --> 39:54.200
Right.

39:54.200 --> 39:57.200
And even the anonymity piece, I've played this out a couple of times in my mind.

39:57.200 --> 40:04.200
The problem I always get is while there's people in Syria who have news to break important news to break and they'd be hung if they.

40:04.200 --> 40:08.200
So we need a anonymous version of the social internet.

40:08.200 --> 40:09.200
Right.

40:09.200 --> 40:10.200
Yeah.

40:10.200 --> 40:16.200
Well, I guess there could be some exception there, but I don't know.

40:16.200 --> 40:26.200
It just doesn't, it actually doesn't interest me because I just feel such a different sense of.

40:26.200 --> 40:38.200
My being in the world as a result of not paying attention to the, my online, the simulacrum of myself, it's a.

40:38.200 --> 40:40.200
Because Twitter was the only one I use.

40:40.200 --> 40:42.200
Like I was on, I've been on Facebook this whole time.

40:42.200 --> 40:48.200
I've been on, I think, I guess I'm on Instagram too, but it's like my team just uses those as marketing channels.

40:48.200 --> 40:51.200
You know, it's just like, it sounds like that's the way you use Twitter now.

40:51.200 --> 40:55.200
But Twitter was the one that I decided, okay, this is going to be me.

40:55.200 --> 40:56.200
I'm going to be posting here.

40:56.200 --> 41:00.200
I'm going to, you know, if I've made a mistake, I want to hear about it.

41:00.200 --> 41:06.200
You know, it's like, and I just wanted to use it as, as actual, actual basis for communication.

41:06.200 --> 41:13.200
And for the longest time, it actually felt like a valid tool in that respect.

41:13.200 --> 41:15.200
You know, it reached a crisis point.

41:15.200 --> 41:17.200
I decided this is just pure toxicity.

41:17.200 --> 41:22.200
There's just no reason, even the good stuff can't possibly make a dent in the bad stuff.

41:22.200 --> 41:24.200
So I just deleted it.

41:24.200 --> 41:28.200
And then I was, I was returned to the real world, right?

41:28.200 --> 41:40.200
Where I've, where I actually live and to books and to, I mean, I'm online all the time anyway, but it's not having the, it's the time course of reactivity.

41:40.200 --> 41:51.200
When you don't have social media, when you don't, and you don't have a place to put this, this instantaneous hot take that you're tempted to put out into the world,

41:51.200 --> 41:53.200
because there's literally no place to put it.

41:53.200 --> 42:05.200
Like for me, if I have some reaction to something in the news, I have to decide whether it's worth talking about it in my next podcast that I might be recording, you know, four days from now.

42:05.200 --> 42:13.200
And rather often people have been just bloviating about this thing for four solid days before I ever get to the microphone.

42:13.200 --> 42:17.200
And then I get to think, well, is it still worth talking about it?

42:17.200 --> 42:20.200
And most, almost nothing survives that test anymore, right?

42:20.200 --> 42:23.200
So I get the conversations moved on.

42:23.200 --> 42:40.200
So there's actually no place for me to just type this thing that either takes me 10 seconds and then rolls out there to get, to detonate in the minds of, you know, my friends and enemies to opposite effect.

42:40.200 --> 42:51.200
And then I see the result of all that, you know, on a, again, on a, this sort of reinforcement loop of every 15 minutes.

42:51.200 --> 42:55.200
Not having that is such a relief that I just don't even know why I would.

42:55.200 --> 43:00.200
So like when Threads was announced, I wasn't, I think I'm on Threads too, but it's not me.

43:00.200 --> 43:03.200
It's just, you know, just get another marketing channel.

43:04.200 --> 43:11.200
But yeah, I haven't, I feel such relief not exercising that muscle anymore.

43:11.200 --> 43:23.200
Where it's like, you know, I don't know how often I was checking Twitter, but it was, I was, you know, I was not checking it just to see what was happening to me or the response to my last thing I tweeted.

43:23.200 --> 43:26.200
I was checking it a lot because it was my newsfeed.

43:26.200 --> 43:29.200
It's like I'm following, you know, 200 smart people.

43:29.200 --> 43:31.200
They're telling me what they're paying attention to.

43:31.200 --> 43:32.200
And so I'm fascinated.

43:32.200 --> 43:35.200
So yeah, well, yeah, I want to see that next article or that next video.

43:35.200 --> 43:44.200
Just that engagement and the endless opportunity to comment and to put my foot in my mouth or put my foot in someone else's mouth or have someone put their foot.

43:44.200 --> 43:58.200
It's just not having that has been such relief that I would be, I mean, it's not impossible, but I would be very cautious in reactivating that because it was, it was so much noise.

43:58.200 --> 44:15.200
And again, it created, there's so much, it became a, it became an opportunity cost, but it became a just this endless opportunity for misunderstanding.

44:16.200 --> 44:22.200
Misunderstanding of me and, you know, everything I've been putting out into the world and then my sense that I had to react to it.

44:22.200 --> 44:33.200
And then you just kind of plow that back into the, you know, that becomes the basis for further misunderstanding.

44:33.200 --> 44:44.200
And it just constantly was giving me the sense that there's something, there's something I need to react to on my podcast, in an article, on Twitter, that it's just, this is a valid signal.

44:44.200 --> 44:47.200
Like this is, this is, this is like, this is a five alarm fire.

44:47.200 --> 44:48.200
This is like, you got to stop everything.

44:48.200 --> 44:53.200
Like you're by the pool on the one vacation you're taking with your family that summer.

44:53.200 --> 44:59.200
And this thing just happened on your phone that it can't wait, right?

44:59.200 --> 45:02.200
Like you actually have to pay attention because it's like the conversation is happening right now.

45:02.200 --> 45:20.200
And so it was a kind of addiction to information and, you know, at some level, reputation management or, or, or, and it was just, I mean, just to just be free of it is such a relief.

45:20.200 --> 45:34.200
And apart from like, you know, health issues with certain family members, virtually the only bad things that have happened to me have been a result of my engagement with Twitter over the last 10 years.

45:34.200 --> 45:43.200
So it's just, it's just, you know, I, you know, I guess I'm, if I'm a masochist, I would be back on Twitter, but like that would be the only reason to do it.

45:43.200 --> 45:44.200
Narrow AI.

45:44.200 --> 45:49.200
I asked you the question a second ago, which we, I really wanted to get a solution to it because I'm mildly terrified.

45:50.200 --> 46:00.200
I completely believe your, believe your the logic underneath your opinion that Narrow AI will cause this destabilization and unusability of the internet.

46:00.200 --> 46:13.200
So just focusing on Narrow AI, what would you consider to be a solution to prevent us getting to that world where misinformation is rife to the point that it can destabilize society, politics and culture?

46:14.200 --> 46:24.200
Well, I think it's something I've been asking people about on my podcast because it's not actually my wheelhouse and I would just need to hear from experts about what's possible technically here.

46:24.200 --> 46:42.200
But I'm imagining that paradoxically or ironically, this could usher in a new kind of gatekeeping that we're going to rely on because like the provenance of information is going to be so important.

46:42.200 --> 46:52.200
I mean, the assurance that a video has not been manipulated or there's not a, just a pure confection of deep fakery.

46:52.200 --> 47:11.200
Right, so you get, so it could be that we're meandering into a new period where you're not going to trust a photo unless it's coming from, you know, Getty images or, you know, the New York Times has some story about how they have verified every photo.

47:11.200 --> 47:14.200
In there that they put in their newspaper, they have a process.

47:14.200 --> 47:30.200
And, you know, so if you see a video of Vladimir Putin seeming to say that he's declaring war on the US, right, I think most people are going to assume that's fake until proven otherwise.

47:30.200 --> 47:34.200
It's like, it's just, it's just going to be too much fake stuff.

47:34.200 --> 47:47.200
And it's going to be, it's going to all going to look so good that the New York Times and every other, you know, organ of media that we have relied upon as imperfect as they've been of late.

47:47.200 --> 47:55.200
They're going to have to figure out what the tools are whereby they can say, OK, this is actually a video of Putin, right.

47:55.200 --> 47:58.200
And if the new, I mean, I'm not going to be able to figure it out on my own, right.

47:58.200 --> 48:06.200
The New York Times doesn't have a process or CNN doesn't have a process that they go through before they say, OK, Putin really said this.

48:06.200 --> 48:12.200
And so this is, we have to now react to this because this is real.

48:13.200 --> 48:20.200
Whatever that process is and whether it's whether there's some kind of digital watermark that, you know, that's connected to the blockchain.

48:20.200 --> 48:34.200
There's some tech implementation of it that can be fully democratized where you by just being in the latest version of the Chrome browser can know that you can differentiate real and fake videos.

48:34.200 --> 48:40.200
I don't know what the implementation will be, but I just know we're going to get to some spot where it's going to be.

48:40.200 --> 48:45.200
Right. We have to declare epistemological bankruptcy.

48:45.200 --> 48:47.200
We don't know what's real.

48:47.200 --> 48:54.200
We have to assume anything, especially lured or agitating is fake until proven otherwise.

48:54.200 --> 48:56.200
So prove otherwise.

48:56.200 --> 48:59.200
And that's, you know, that that'll be a resetting of something.

48:59.200 --> 49:09.200
I don't know what we do with that in a world where we really don't have that much time to react to certain things that are a video of Putin saying he's launched his big missiles.

49:11.200 --> 49:16.200
Is something that, you know, 30 minutes from now we would we would understand whether it's real or not.

49:16.200 --> 49:22.200
We forget about again, forget about everything we just said about AI.

49:22.200 --> 49:24.200
Look at all of our legacy risks.

49:24.200 --> 49:34.200
Look at the risk of nuclear war, the risk of stumbling into a nuclear war by accident has been hanging over our head for 70 years.

49:34.200 --> 49:36.200
I mean, we've got this old tech.

49:36.200 --> 49:40.200
We've got these wonky radar systems that throw up errors.

49:40.200 --> 49:57.200
We have moments in history where, you know, one Soviet sub commander decided based on his just gut feeling his common sense that the data was almost certainly an error.

49:57.200 --> 50:09.200
And he decided not to pass the obvious evidence of an American ICBM launch up the chain of command, knowing that the chain of command would say, OK, you have to fire.

50:09.200 --> 50:10.200
Right.

50:10.200 --> 50:19.200
And he reasoned that if the U.S. was going to attack the Soviet Union, they would launch more than I think in this case it looked like there were four missiles.

50:19.200 --> 50:21.200
That was the radar signature.

50:21.200 --> 50:32.200
If the U.S. is going to launch a first strike against the Soviet Union in this like the mid 80s, they're going to launch more than four missiles.

50:32.200 --> 50:34.200
This has to be this has to be bad data.

50:34.200 --> 50:35.200
Right.

50:35.200 --> 50:41.200
But, you know, so if we automate all this, will we automate it to systems that have that kind of common sense?

50:41.200 --> 50:42.200
Right.

50:43.200 --> 50:51.200
But we've been perched on the on the edge of the abyss based on this.

50:51.200 --> 50:56.200
The possible forget about malevolent actors, you know, who might decide to have a nuclear war on purpose.

50:56.200 --> 51:00.200
We have the possibility of accidental nuclear war.

51:00.200 --> 51:06.200
You add this cacophony of misinformation and deep fake to all of that.

51:06.200 --> 51:09.200
And it just gets scarier and scarier.

51:09.200 --> 51:11.200
And this is not even AI.

51:11.200 --> 51:16.200
This is just, you know, narrow AI amplified misinformation.

51:16.200 --> 51:18.200
How do you think about it?

51:18.200 --> 51:20.200
Well, this is the thing that worries me.

51:20.200 --> 51:22.200
I worry about the next election.

51:22.200 --> 51:35.200
I think the next president, if we can run the 2024 election in a way that most of America acknowledges was valid, that will be an amazing victory.

51:35.200 --> 51:41.200
You know, whatever the outcome, I mean, obviously, I would not be looking forward to a Trump presidency.

51:41.200 --> 51:54.200
But I think even more fundamental than that is, can we hold a presidential election 18 months from now that is that we recognize as valid?

51:54.200 --> 51:55.200
Right.

51:55.200 --> 51:56.200
Like that.

51:56.200 --> 51:57.200
I don't know.

51:57.200 --> 52:02.200
I don't know what kind of resources are being spent on on that particular performance.

52:02.200 --> 52:05.200
But that is hugely important.

52:05.200 --> 52:13.200
And I don't think our near term experiments with AI is going to make that easier.

52:13.200 --> 52:15.200
Why is it so important?

52:15.200 --> 52:27.200
Well, it's just, I mean, if you think the maintenance of a valid democracy in the world's lone superpower is of minor importance.

52:27.200 --> 52:31.200
I'd like to drink the tea you're drinking.

52:31.200 --> 52:32.200
Is that optimistic?

52:32.200 --> 52:35.200
I mean, I can't say I'm optimistic.

52:35.200 --> 52:40.200
You know, it's a paradoxical state, I mean, because I definitely have.

52:40.200 --> 52:44.200
I tend to focus on what's wrong or might be wrong.

52:44.200 --> 52:50.200
I tend to, I think, have a pessimistic bias, right?

52:50.200 --> 52:53.200
Like I tend to notice what's wrong as opposed to what's right.

52:53.200 --> 52:58.200
You know, that's my, that's my bias.

52:58.200 --> 53:02.200
But I'm actually very happy, right?

53:02.200 --> 53:04.200
Like I have a very, a very good life.

53:04.200 --> 53:08.200
I'm just like everything is just I'm incredibly lucky.

53:08.200 --> 53:09.200
I'm surrounded by great people.

53:09.200 --> 53:12.200
It's like it's all great.

53:12.200 --> 53:16.200
And yet I see all of these risks on the horizon.

53:16.200 --> 53:25.200
So I'm not, I just have a very high degree of well-being at this moment in my life.

53:25.200 --> 53:30.200
And yet what's on the television is scary.

53:30.200 --> 53:34.200
And so it's a very interesting juxtaposition.

53:34.200 --> 53:41.200
You know, I'll be, I'll be very relieved if we have a, I feel like we're in a very weird spot.

53:41.200 --> 53:49.200
I mean, like the, I haven't seen a full post-mortem on the COVID pandemic that has fully encapsulated

53:49.200 --> 53:53.200
what I think we, what I think happened to us there.

53:53.200 --> 53:59.200
But my vague sense is that we didn't learn a whole hell of a lot.

53:59.200 --> 54:04.200
I mean, basically what we learned is we're really bad at responding to this kind of thing.

54:04.200 --> 54:08.200
This was a challenge that, that just fragmented us as a society.

54:08.200 --> 54:10.200
It could have brought us together.

54:10.200 --> 54:12.200
It didn't.

54:12.200 --> 54:24.200
And it, it amplified all of the divisions in our society politically and economically and tribally and all kinds of ways.

54:24.200 --> 54:29.200
The role of misinformation and disinformation and all of that was, was all too clear.

54:29.200 --> 54:31.200
And I think just getting worse.

54:31.200 --> 54:40.200
So I think, you know, as a dress rehearsal for some future pandemic that's, that is inevitably going to come and is, you know, could well be worse.

54:40.200 --> 54:42.200
I think we failed this dress rehearsal.

54:42.200 --> 54:56.200
And, you know, I have to hope that at some point our institutions will reconstitute themselves so as to be obviously trustworthy and engender the kind of trust we actually need to have on our institutions.

54:56.200 --> 55:04.200
Like, we need a CDC, then not only that we trust, but that is trustworthy, that we, that we, that we're right to trust, right?

55:05.200 --> 55:10.200
And so it is with an FDA and every other, you know, institution that, that is relevant here.

55:10.200 --> 55:14.200
And we don't quite have that.

55:14.200 --> 55:17.200
And half of our society thinks we don't have that at all.

55:17.200 --> 55:18.200
Right.

55:18.200 --> 55:23.200
And so it's, we have to rebuild trust and institutions somehow.

55:23.200 --> 55:33.200
And I just think, you know, we have a lot of work to do to even figure out how to make an increment of progress on that score.

55:33.200 --> 55:46.200
Because we're, again, the siloing of large constituents into alternate information universes is just not functional.

55:46.200 --> 55:50.200
And that's so much of what social media has done to us and alternative media.

55:50.200 --> 55:55.200
I mean, like, you know, I call it, you know, you and I are podcasters, but I call it podcast to stand, right?

55:55.200 --> 56:11.200
I mean, we have this, this landscape of, I mean, there's now whatever million plus podcasts and there's, you know, email newsletters and everyone has now just decided to curate their information diet in a way that's just bespoke to them.

56:11.200 --> 56:24.200
And you can stay there forever and you're getting, you're getting one slice of, and it could be, you know, a completely fictional slice of, of reality.

56:24.200 --> 56:31.200
And we're losing the ability to converge on a common picture of what's going on.

56:31.200 --> 56:33.200
And you.

56:33.200 --> 56:35.200
So that's not optimistic.

56:35.200 --> 56:37.200
I didn't hear the optimism in there.

56:37.200 --> 56:38.200
You tell me.

56:38.200 --> 56:42.200
No, I, but I kind of can't refute anything you said on a like a logical basis.

56:42.200 --> 56:46.200
It all sounds like that is the direction of travel that we're going in.

56:46.200 --> 56:52.200
Unfortunately, I have faith that there'll be surprising positives.

56:52.200 --> 56:56.200
There always tends to be surprising positives that we also didn't factor in.

56:56.200 --> 56:59.200
Well, yeah, I mean, it's easy to see.

56:59.200 --> 57:12.200
I mean, if there's anything, if there's any significant low hanging fruit technologically or or scientifically that could be AI enabled for us.

57:12.200 --> 57:17.200
I mean, just take like, you know, a cure for cancer, a cure for Alzheimer's, right?

57:17.200 --> 57:25.200
I mean, just having one thing like that, right, that would be such an enormous good.

57:25.200 --> 57:29.200
And that is, that is, that's what, that's why we can't get off this ride.

57:29.200 --> 57:31.200
And that's why there is no break to pull.

57:31.200 --> 57:35.200
Because the value of intelligence is so enormous.

57:35.200 --> 57:38.200
I mean, it is, it is just, it's not everything.

57:38.200 --> 57:43.200
I mean, it's not, you know, there's other things we care about and a right to care about beyond intelligence.

57:43.200 --> 57:46.200
I mean, love is not the same thing as intelligence, right?

57:46.200 --> 57:51.200
But intelligence is the thing that can safeguard everything you love.

57:51.200 --> 58:02.200
I mean, even if you think the whole point in life is to just get on a beach with your friends and your family and just hang out and enjoy the sunset.

58:02.200 --> 58:06.200
Okay, you don't have to augment.

58:06.200 --> 58:09.200
You don't need superhuman intelligence to do any of that, right?

58:09.200 --> 58:12.200
You're fit to do it exactly as you are.

58:12.200 --> 58:17.200
You could have done that in the 70s and it would just be just as good a beach and it'd be just as good friends.

58:17.200 --> 58:26.200
But every gain we make in intelligence is the thing that safeguards that opportunity for you and everyone else.

58:26.200 --> 58:30.200
How would you, I feel like we've not defined the term artificial general intelligence.

58:30.200 --> 58:35.200
From my understanding of it, it's when the intelligence can think and make decisions almost like a human.

58:35.200 --> 58:40.200
Yeah, I mean, loosely, I mean, this, this is kind of just a semantic problem.

58:40.200 --> 59:00.200
But intelligence can mean many things, but, you know, loosely speaking, it is the ability to solve problems and meet goals, make decisions in response to a changing environment, in response to data.

59:01.200 --> 59:24.200
And the general aspect of that is an ability to do that in many different situations, all the sort of situations we encounter as people, and to have one's capacity in one area not, you know, as I get better at deciding whether or not this is a cup,

59:24.200 --> 59:30.200
I don't magically get worse at deciding whether, you know, you just said a word, right?

59:30.200 --> 59:35.200
It's like, I can do multiple things in multiple channels.

59:35.200 --> 59:42.200
That's not something we had in our artificial systems for the longest time because everything was bespoke to the task.

59:42.200 --> 59:45.200
We'd build a chess engine and it couldn't even play tic-tac-toe.

59:45.200 --> 59:53.200
All it could do is play chess and we just would get better and better in these piecemeal, narrow ways.

59:53.200 --> 01:00:07.200
And then things began to change a few years ago where you'd get, you know, like deep mind would have its algorithms that were, you know, the same algorithm with slightly different tuning could play go, right?

01:00:07.200 --> 01:00:13.200
Or it could, you know, it could solve a protein folding problem as opposed to just playing chess, right?

01:00:13.200 --> 01:00:16.200
And it became the best in the world at chess and it became the best in the world at go.

01:00:17.200 --> 01:00:36.200
And amazingly, I mean, to take, you know, what AlphaZero did, it, you know, before AlphaZero, all the chess algorithms were, they just had all of our chess knowledge plowed into them.

01:00:36.200 --> 01:00:42.200
They had studied every human game of chess and they just, it was just, you know, it was a bespoke chess engine.

01:00:42.200 --> 01:00:48.200
AlphaZero just played itself, I think for like four hours, right?

01:00:48.200 --> 01:00:58.200
It just, it just had the rules of chess and then it played itself and it became better not merely than every other, every person who's ever played the game.

01:00:58.200 --> 01:01:04.200
It became better than all the chess engines that had all of the, the, all of our chess knowledge plowed into them.

01:01:04.200 --> 01:01:14.200
So it's a fundamentally new moment in, in how you build an intelligent system and it promises this, this possibility.

01:01:14.200 --> 01:01:20.200
Again, this inevitability, the moment you admit that we will eventually get there.

01:01:20.200 --> 01:01:31.200
The moment, the moment you admit that it's, it can be done in silico and the moment that you admit that we will just keep going unless a catastrophe happens.

01:01:32.200 --> 01:01:38.200
And those two things are so easy to admit that I don't, at this point, I don't see any place to stand where you're not forced to admit them, right?

01:01:38.200 --> 01:01:50.200
I don't see any neuroscientific or cognitive scientific argument for substrate dependence for intelligence, given what we've already built.

01:01:50.200 --> 01:01:55.200
And again, we're, we're going to keep going until something stops us.

01:01:55.200 --> 01:02:02.200
We'll hit some immovable object that prevents us from releasing the next iPhone, but otherwise we're going to keep going.

01:02:02.200 --> 01:02:18.200
And then, yeah, so then it, then whatever general will mean in that first case, there'll be a case where we've built a system that is so good at everything we care about that is functionally general.

01:02:18.200 --> 01:02:23.200
Now, maybe it's missing something, maybe it's not, you know, maybe it's missing something that we don't even have a name for.

01:02:23.200 --> 01:02:30.200
You know, we're missing all kinds of, there are possible intelligences that we haven't even thought about because we just haven't thought about them.

01:02:30.200 --> 01:02:42.200
There are things that, there are ways to section the universe undoubtedly that we can't even conceive of because we are just, we have the minds we have.

01:02:42.200 --> 01:02:44.200
Elon was asked a question on this by a journalist.

01:02:44.200 --> 01:02:50.200
The journalist said to him, in a world where you believe that to be true, that artificial general intelligence is around the corner.

01:02:50.200 --> 01:02:57.200
When your kids come to you and say, Daddy, what should I do with my life to find purpose and meaning?

01:02:57.200 --> 01:03:01.200
What advice do you now give them if you hold that intuition to be true?

01:03:01.200 --> 01:03:03.200
That it's around the corner.

01:03:03.200 --> 01:03:08.200
What do you say to your children when they say, what should I do with my life to create purpose and meaning?

01:03:08.200 --> 01:03:10.200
Did you say that Elon answered this question?

01:03:10.200 --> 01:03:11.200
Yeah.

01:03:11.200 --> 01:03:12.200
What did he say?

01:03:12.200 --> 01:03:18.200
It's one of the most chilling moments in an interview I think I've seen in recent times because he stutters.

01:03:18.200 --> 01:03:21.200
He goes silent for about 15 seconds, which is very un-Elon.

01:03:21.200 --> 01:03:22.200
He stutters.

01:03:22.200 --> 01:03:24.200
He stutters.

01:03:24.200 --> 01:03:26.200
He stutters a bit more.

01:03:26.200 --> 01:03:37.200
And then he says, he thinks he's living in suspended disbelief because if he really thought about it too much, what's the point?

01:03:37.200 --> 01:03:39.200
He says, what's the point of me building all these cars?

01:03:39.200 --> 01:03:40.200
He was in his Tesla factory.

01:03:40.200 --> 01:03:42.200
What's the point of me building all these cars and what's the point?

01:03:42.200 --> 01:03:43.200
I do think that sometimes.

01:03:43.200 --> 01:03:47.200
So I think I have to live in, as his words were, suspended disbelief.

01:03:47.200 --> 01:03:54.200
I would encourage him to ask what's the point of spending so much time on Twitter because he could clearly benefit from rethinking that.

01:03:54.200 --> 01:04:07.200
But that aside, my answer to that is, and I think other people have echoed this of late, sort of surprising to me.

01:04:07.200 --> 01:04:21.200
My answer is that this begins to privilege a return to the humanities as a kind of a core, like the center of mass intellectually for us.

01:04:21.200 --> 01:04:35.200
Because when you look at what we're really good at and it's among the last things that can be plausibly automated.

01:04:35.200 --> 01:04:39.200
And if we automate it, we may cease to care about it.

01:04:39.200 --> 01:04:46.200
So it's like learning to write good code is something that is being automated now.

01:04:46.200 --> 01:05:00.200
I'm not a programmer, but I have it on good authority that already these large language models are improving code and something like half the time they're writing better code than people.

01:05:00.200 --> 01:05:02.200
That's all going to become like chess, right?

01:05:02.200 --> 01:05:07.200
It's just it's going to be better than people ultimately.

01:05:07.200 --> 01:05:14.200
So being a software engineer is something that, you know, and being a radiologist and being like those things.

01:05:14.200 --> 01:05:24.200
It's easy to see how AI just cancels those professions or at least makes one person, you know, so effective at using AI tools that one person can do the work of 100 people.

01:05:24.200 --> 01:05:31.200
So you got 99 people who don't have to be doing that job.

01:05:31.200 --> 01:05:47.200
But creating art and, you know, writing novels and being a philosopher and talking about what it means to live a good life and how to do it.

01:05:47.200 --> 01:06:08.200
That's something that if we have to look at where we're going to care that we're actually in relationship to and in dialogue with another person who we know to be conscious.

01:06:08.200 --> 01:06:14.200
Where we don't care about that, we're not going to care, we're going to want just the best version of it.

01:06:14.200 --> 01:06:20.200
You know, if the cure for cancer comes from an AI, an incentive in AI, I do not give a shit.

01:06:20.200 --> 01:06:22.200
I just want the cure for cancer, right?

01:06:22.200 --> 01:06:30.200
Like there's no added value that where I find out, okay, the person who gave me this cure really felt good about it.

01:06:30.200 --> 01:06:33.200
And he's, you know, he had tears in his eyes when he figured out the cure.

01:06:33.200 --> 01:06:35.200
Every engineering problem is like that.

01:06:35.200 --> 01:06:37.200
We want safer planes.

01:06:37.200 --> 01:06:39.200
We want, you know, we just want things to work.

01:06:39.200 --> 01:06:46.200
We're not sentimental about the artistry that went into all of that.

01:06:46.200 --> 01:06:54.200
And when the difference, when the gulf between the best and the mediocre gets big and consequential, we're just going to want the best.

01:06:54.200 --> 01:06:56.200
We're just going to want the best all the way down the line.

01:06:56.200 --> 01:07:00.200
But what is the best novel, right?

01:07:00.200 --> 01:07:04.200
What is the best podcast conversation?

01:07:04.200 --> 01:07:12.200
And can you subtract out the conscious person from that and still think it's the best?

01:07:12.200 --> 01:07:20.200
And so like someone once sent me what purported to be, I didn't even listen to it, so I'm not even sure what it was.

01:07:20.200 --> 01:07:26.200
But it looked like it was an AI-generated conversation between Alan Watts and Terrence McKenna, right?

01:07:26.200 --> 01:07:30.200
Both guys who I love, I remember, I didn't know either of them.

01:07:30.200 --> 01:07:35.200
But some fans of both have listened to hundreds of hours of both talk.

01:07:35.200 --> 01:07:37.200
As far as I know, they never met each other.

01:07:37.200 --> 01:07:39.200
It would have been a fascinating conversation.

01:07:39.200 --> 01:07:46.200
I realized when I looked at this YouTube video, I realized I simply don't care how good this is.

01:07:46.200 --> 01:07:52.200
Because I only care if it was actually Alan Watts and Terrence McKenna talking.

01:07:52.200 --> 01:07:58.200
I got a simulacrum of Alan Watts and Terrence McKenna in this context I don't care about.

01:07:58.200 --> 01:08:08.200
So in another use case, I stumbled upon, I was playing with ChatGPT and I asked the causes of World War II.

01:08:08.200 --> 01:08:11.200
Give me 500 words on the cause of World War II.

01:08:11.200 --> 01:08:16.200
It gives you this perfect little bullet-pointed essay on the cause of World War II.

01:08:16.200 --> 01:08:19.200
That's exactly what I want from it.

01:08:19.200 --> 01:08:20.200
That's fine.

01:08:20.200 --> 01:08:24.200
I don't care that there was no person behind that typing.

01:08:24.200 --> 01:08:34.200
But when I think, well, do I want to read Churchill's history of World War II?

01:08:34.200 --> 01:08:35.200
It's on my shelf to read.

01:08:35.200 --> 01:08:39.200
It's one of these aspirational sets of books.

01:08:39.200 --> 01:08:40.200
I haven't read it yet.

01:08:40.200 --> 01:08:44.200
I actually want to read it because Churchill wrote it.

01:08:44.200 --> 01:08:50.200
And if you could give me an AI version of Churchill saying this is in the style of Churchill,

01:08:51.200 --> 01:08:54.200
even Churchill scholars say this sounds like Churchill.

01:08:54.200 --> 01:08:56.200
I actually don't care about it.

01:08:56.200 --> 01:08:57.200
That's not the use.

01:08:57.200 --> 01:09:03.200
I'll take the generic use of give me the cause of World War II.

01:09:03.200 --> 01:09:06.200
The fake Churchill is profoundly uninteresting to me.

01:09:06.200 --> 01:09:11.200
The real Churchill, even though he's dead, is interesting to me.

01:09:11.200 --> 01:09:18.200
So the rebuttal I give here, and this is what my mind is doing, is saying the distinction you're presenting,

01:09:18.200 --> 01:09:21.200
the difference I see is that in the case of the conversation between two people,

01:09:21.200 --> 01:09:26.200
your respect that has been generated by AI, someone has signaled to you that it is fake.

01:09:26.200 --> 01:09:32.200
If you remove that because say Churchill thought, why would I write a book when I could just click a button

01:09:32.200 --> 01:09:40.200
and this thing will write it in my voice, in my tone of voice, with the entire back catalogue of things I've written before.

01:09:40.200 --> 01:09:44.200
And it will produce my account and it will save me time.

01:09:44.200 --> 01:09:47.200
So I'll just click a button, my publisher maybe will do it for me.

01:09:47.200 --> 01:09:55.200
And then I'll sell that to Sam on the basis that it is my thoughts, which I can imagine a very near future.

01:09:55.200 --> 01:10:00.200
If we just do it by percentage, how many books are going to be increasingly written by artificial intelligence?

01:10:00.200 --> 01:10:04.200
To the point that when you look at a shelf, I imagine at some point in the future,

01:10:04.200 --> 01:10:08.200
if the intelligence does increase by any measure,

01:10:08.200 --> 01:10:14.200
that most of it would be words strung together by artificial intelligence

01:10:14.200 --> 01:10:18.200
and it will be selling potentially better than the words written by humans.

01:10:18.200 --> 01:10:24.200
So again, when we go back to the conversation with your children, there might not be a career there either

01:10:24.200 --> 01:10:30.200
because artificial intelligence is faster, can produce more, can test and iterate on whether it sells better,

01:10:30.200 --> 01:10:36.200
clicks gets more clicks, it can write the headline, create the picture, write the content.

01:10:36.200 --> 01:10:40.200
And then I can just take the check because I put my name to it.

01:10:40.200 --> 01:10:44.200
So even in that regard, what remains?

01:10:44.200 --> 01:10:51.200
Well, so in the limit, what I think we're imagining is a world where,

01:10:51.200 --> 01:10:56.200
and so none of the terrifyingly bad things have happened, so it's just all working.

01:10:56.200 --> 01:11:02.200
We're just producing a ton of great stuff that is better than the human stuff and people are losing their jobs.

01:11:02.200 --> 01:11:08.200
We've got a labor disruption, but we're not talking about any other kind of political catastrophe

01:11:08.200 --> 01:11:18.200
or cyber apocalypse, much less AGI destroying everything.

01:11:18.200 --> 01:11:26.200
Then I think we just need a different economic assumption and ethical intuition around the value of work.

01:11:26.200 --> 01:11:36.200
Our default norm now in a capitalist society is you have to figure out something to do with most of your time

01:11:36.200 --> 01:11:39.200
that other people are willing to pay you for.

01:11:39.200 --> 01:11:45.200
You have to figure out how to add value to other people's lives such that you reliably get paid.

01:11:45.200 --> 01:11:48.200
Otherwise, you might die.

01:11:48.200 --> 01:11:53.200
We've got a social safety net, but it's pretty meager.

01:11:54.200 --> 01:11:56.200
There are cracks you can fall through.

01:11:56.200 --> 01:12:05.200
You can wind up homeless, and we're not going to figure out what to do about that all too well.

01:12:05.200 --> 01:12:15.200
Your claim upon your existence among us is you finding something to do with your time that other people will pay you for.

01:12:16.200 --> 01:12:23.200
Now we've got artificial intelligence removing some of those opportunities, creating others, but in the limit,

01:12:23.200 --> 01:12:32.200
and I do think it is different, I think analogies to other moments in technological history are fundamentally flawed,

01:12:32.200 --> 01:12:42.200
I think this is a technology which in the limit will replace jobs and not create better new jobs in their wake.

01:12:42.200 --> 01:12:48.200
This just cancels the need for human labor ultimately.

01:12:48.200 --> 01:12:55.200
Strangely, it replaces some of the highest status most cognitively intensive jobs first.

01:12:55.200 --> 01:13:05.200
It replaces Elon Musk before it replaces your electrician or your plumber or your masseuse way before.

01:13:05.200 --> 01:13:09.200
We have to internalize the reality of that.

01:13:09.200 --> 01:13:15.200
Again, this is in success. This is all good things happening.

01:13:15.200 --> 01:13:17.200
We have to have a new ethic.

01:13:17.200 --> 01:13:25.200
We have to have a new economics based on that ethic, which is UBI is one solution to this.

01:13:25.200 --> 01:13:27.200
You shouldn't have to work to survive.

01:13:27.200 --> 01:13:29.200
Universal basic income.

01:13:29.200 --> 01:13:32.200
There's so much abundance now being created.

01:13:32.200 --> 01:13:35.200
We have to figure out how to spread this wealth around.

01:13:35.200 --> 01:13:37.200
We've got a cure for cancer over here.

01:13:37.200 --> 01:13:45.200
We've got perfect photovoltaic driven economies over here.

01:13:45.200 --> 01:13:48.200
We've solved the climate change issue.

01:13:48.200 --> 01:13:53.200
We're just pulling wealth out of the ether, essentially.

01:13:53.200 --> 01:14:00.200
We've got nanotechnology that is just birthing whole new industries yet, but it's all being driven by AI.

01:14:00.200 --> 01:14:02.200
There's no room in this.

01:14:02.200 --> 01:14:10.200
Whenever you put a person in the decision chain, you're just adding noise.

01:14:10.200 --> 01:14:14.200
This should be the best thing that's ever happened to us.

01:14:14.200 --> 01:14:19.200
This is just like God handing us the perfect labor saving device.

01:14:19.200 --> 01:14:24.200
The machine that can build every other machine that can do anything you could possibly want.

01:14:24.200 --> 01:14:28.200
We should figure out how to spread the wealth around in that case.

01:14:28.200 --> 01:14:34.200
This is just powered by sunlight, no more wars over resource extraction.

01:14:34.200 --> 01:14:37.200
It can build anything.

01:14:37.200 --> 01:14:41.200
We can all be on the beach just hanging out with our friends and family.

01:14:41.200 --> 01:14:46.200
Did you believe we should do universal basic income where everybody's given a monthly check?

01:14:46.200 --> 01:14:48.200
We have to break this connection.

01:14:48.200 --> 01:14:56.200
Again, this is what will have to happen in the presence of this kind of labor force dislocation

01:14:56.200 --> 01:15:00.200
enabled by all of this going perfectly well.

01:15:00.200 --> 01:15:02.200
This is pure success.

01:15:02.200 --> 01:15:04.200
AI is just producing good things.

01:15:04.200 --> 01:15:08.200
The only bad thing is putting all these people out of work.

01:15:08.200 --> 01:15:10.200
It's coming for your job eventually.

01:15:10.200 --> 01:15:11.200
I've heard this.

01:15:11.200 --> 01:15:15.200
My issue with it and my rebuttal when I talked to my friends about this idea of universal basic income

01:15:15.200 --> 01:15:19.200
where we hand out enough cash or resources to people so that they're stable,

01:15:19.200 --> 01:15:23.200
which I'm not necessarily against but just want to play with it a little bit,

01:15:23.200 --> 01:15:28.200
is humans seem to have an innate desire for purpose and meaning

01:15:28.200 --> 01:15:34.200
and we seem to be designed and built psychologically for labor and for discomfort.

01:15:34.200 --> 01:15:39.200
But it doesn't have to be labor that's tied to money.

01:15:39.200 --> 01:15:44.200
We will get our status in other ways and we'll get our meaning in other ways.

01:15:44.200 --> 01:15:47.200
These are all just stories we tell ourselves.

01:15:47.200 --> 01:15:53.200
You're talking to a person who knows it's possible to be happy actually doing nothing,

01:15:53.200 --> 01:15:57.200
like just sitting in a room for a month and just staring at the wall.

01:15:57.200 --> 01:15:58.200
Because you've done it.

01:15:58.200 --> 01:16:00.200
Like that's possible.

01:16:00.200 --> 01:16:03.200
Yet that's most people's worst nightmare.

01:16:03.200 --> 01:16:06.200
It's solitary confinement in a prison is considered a torture.

01:16:06.200 --> 01:16:09.200
I know people who spent 20 years in a cave.

01:16:09.200 --> 01:16:13.200
There are capacities here that are worth talking about.

01:16:13.200 --> 01:16:22.200
But just more commonly, I think we want to be entertained.

01:16:22.200 --> 01:16:24.200
We want to have fun.

01:16:24.200 --> 01:16:26.200
We want to be with the people we love.

01:16:26.200 --> 01:16:31.200
We want to be useful in relationship.

01:16:31.200 --> 01:16:41.200
And in so far as that gets uncoupled from the necessity of working to survive.

01:16:41.200 --> 01:16:42.200
It doesn't all just go away.

01:16:42.200 --> 01:16:50.200
We just need new norms and new ethics and new conversations around what we do on vacation.

01:16:50.200 --> 01:16:55.200
What you're imagining is that if you put everyone on vacation, on the best vacation,

01:16:55.200 --> 01:17:02.200
you can make the vacation as good as possible, a majority of people will eventually be miserable

01:17:02.200 --> 01:17:06.200
because they're not back at work.

01:17:06.200 --> 01:17:09.200
And yet most of these people are working so that they have enough money

01:17:09.200 --> 01:17:11.200
so that they could finally take that vacation.

01:17:11.200 --> 01:17:15.200
We will figure out a new way to be happy on the beach.

01:17:15.200 --> 01:17:21.200
If you get bored with Frisbee, we will figure something else out that is fun.

01:17:21.200 --> 01:17:27.200
I'll be able to read the Churchill history of World War II on the beach

01:17:27.200 --> 01:17:33.200
and not be rushed by any other imperative because I'm happily retired

01:17:33.200 --> 01:17:40.200
because my AI is creating the thing that is solving all my economic problems.

01:17:40.200 --> 01:17:44.200
We should be so lucky as to have that be our problem.

01:17:44.200 --> 01:17:50.200
How to be happy in conditions of no economic imperative,

01:17:50.200 --> 01:17:55.200
no basis for political strife on the basis of scarce resources

01:17:55.200 --> 01:18:08.200
and the question of survival is off the table with respect to what one does with one's time and attention.

01:18:08.200 --> 01:18:11.200
You can be as lazy as you want and you'll still survive.

01:18:11.200 --> 01:18:16.200
You can be as unlucky as you want and you'll still survive.

01:18:16.200 --> 01:18:23.200
The awful situation we're in now is that differences in luck mean everything.

01:18:23.200 --> 01:18:30.200
Someone is born without any of the advantages that we have.

01:18:30.200 --> 01:18:35.200
We don't have an economic system that reliably gives them

01:18:35.200 --> 01:18:39.200
every advantage and opportunity they could have.

01:18:39.200 --> 01:18:48.200
We've convinced ourselves we either don't have the resources

01:18:48.200 --> 01:18:50.200
or we've convinced ourselves we don't have the resources.

01:18:50.200 --> 01:18:53.200
We don't have the incentive such that we access the resources

01:18:53.200 --> 01:18:59.200
so as to actually come to the help of people we could help.

01:18:59.200 --> 01:19:04.200
The idea that people starve to death is just unimaginable and yet it still happens.

01:19:04.200 --> 01:19:09.200
That's not a scarcity problem, it's a political problem wherever it happens.

01:19:09.200 --> 01:19:14.200
Yet all of this is tied to a system where everyone has convinced themselves

01:19:14.200 --> 01:19:24.200
that it's normal to really have one's survival be in question if one doesn't work.

01:19:25.200 --> 01:19:33.200
By choice or by accident, I think it's still true that at least in the U.S.

01:19:33.200 --> 01:19:35.200
this is almost certainly not true in the U.K.

01:19:35.200 --> 01:19:43.200
but in the U.S. the most common reason for a personal bankruptcy is overwhelming medical expense

01:19:43.200 --> 01:19:45.200
that just comes upon you for whatever reason.

01:19:45.200 --> 01:19:51.200
Your wife gets cancer, you guys go bankrupt solving the cancer problem

01:19:51.200 --> 01:19:56.200
failing to solve the cancer problem and now everything else unravels.

01:19:56.200 --> 01:20:02.200
We have a society which thinks, yeah, well, I'm lucky you.

01:20:02.200 --> 01:20:05.200
If you wind up homeless just don't sleep in front of my store

01:20:05.200 --> 01:20:09.200
because you're going to hurt my business.

01:20:09.200 --> 01:20:19.200
Successful AI that cancels lots of jobs would only be cancelling those jobs

01:20:19.200 --> 01:20:24.200
by virtue of producing so many good things, so much value for everybody

01:20:24.200 --> 01:20:28.200
that we would have to figure out how to spread that wealth around.

01:20:28.200 --> 01:20:39.200
Otherwise we would have an amazingly dystopian bottleneck for a few short years

01:20:39.200 --> 01:20:41.200
and then we would just have a revolution.

01:20:41.200 --> 01:20:47.200
Then the guys in their integrated communities making trillions of dollars

01:20:47.200 --> 01:20:57.200
based on them having gotten close enough to the GPUs that some of it rubbed off on them.

01:20:57.200 --> 01:21:01.200
Yeah, they'd be dragged out of their houses and off their Gulf Streams

01:21:01.200 --> 01:21:07.200
and we would have a fundamental reset, we'd have a hard reset of the political system.

01:21:07.200 --> 01:21:12.200
If I had to put you in a yes or no situation and ask your intuition the question now

01:21:12.200 --> 01:21:18.200
that if your objective was to, which I'm sure it is, is to encourage the betterment of humanity

01:21:18.200 --> 01:21:22.200
and to increase our odds of happiness and well-being 100 years from now

01:21:22.200 --> 01:21:25.200
and there was a button placed in front of you

01:21:25.200 --> 01:21:31.200
and it would either end the development of artificial intelligence as we've seen it over the last decade

01:21:31.200 --> 01:21:37.200
so we'd never proceed with developing intelligent machines or not.

01:21:37.200 --> 01:21:41.200
So you could press a button and stop it right now.

01:21:41.200 --> 01:21:44.200
And stop it permanently such that we never then do that thing?

01:21:44.200 --> 01:21:49.200
We just never figure out how to build intelligent machines?

01:21:49.200 --> 01:21:53.200
Pause it indefinitely.

01:21:53.200 --> 01:22:02.200
Well, I would definitely pause it to a point where we would get our heads around the alignment problem.

01:22:02.200 --> 01:22:08.200
Permanently. If the button was a permanent pause that you couldn't undo.

01:22:08.200 --> 01:22:11.200
Well, the question is how deep does that go?

01:22:11.200 --> 01:22:14.200
We have everything we have now but we just never get better than now.

01:22:14.200 --> 01:22:17.200
Yeah, we never make progress from here.

01:22:17.200 --> 01:22:21.200
And your objective is to make humanity happy and prosperous?

01:22:21.200 --> 01:22:32.200
It's hard because when you begin imagining all of the good stuff that we could get with aligned superhuman AI

01:22:32.200 --> 01:22:37.200
then it's just cornucopia upon cornucopia.

01:22:37.200 --> 01:22:41.200
It's just everything is potentially within reach.

01:22:41.200 --> 01:22:49.200
Yeah, I mean I take the existential risk scenario seriously enough that I would pause it.

01:22:49.200 --> 01:22:53.200
I think we will eventually get to it.

01:22:53.200 --> 01:23:00.200
If curing cancer is a biomedical engineering problem that admits of a solution

01:23:00.200 --> 01:23:04.200
I think there's every reason to believe it ultimately would be.

01:23:04.200 --> 01:23:10.200
We will eventually get there based on our own muddling along with our current level of tech.

01:23:10.200 --> 01:23:13.200
Currently information tech.

01:23:13.200 --> 01:23:20.200
I'm reasonably confident of that.

01:23:21.200 --> 01:23:32.200
Our intelligence shows every sign of being general is just it's not as fast as we would want it to be.

01:23:32.200 --> 01:23:43.200
The thing that AI is going to give us is it's going to give us speed that is...

01:23:43.200 --> 01:23:46.200
There's speed and then there's the access, there's memory.

01:23:46.200 --> 01:23:56.200
We can't integrate, we don't have the ability, no person or team of people can integrate all of the data we already have.

01:23:56.200 --> 01:24:05.200
The real promise here is that these systems will be able to find patterns that we wouldn't even know how to look for

01:24:05.200 --> 01:24:08.200
and then do something on the basis of those patterns.

01:24:08.200 --> 01:24:19.200
I think an intelligent search within the data space by apes like ourselves will eventually do most of the great things we want done.

01:24:19.200 --> 01:24:37.200
The problems we need to solve so as to safeguard the career of our species

01:24:37.200 --> 01:24:47.200
to make civilization durable and sane and to remove this sort of Damocles that is over our heads at every moment

01:24:47.200 --> 01:24:52.200
that at any moment we could just decide to have a nuclear war that ruins everything

01:24:52.200 --> 01:24:58.200
or create an engineered pandemic that ruins everything.

01:24:58.200 --> 01:25:01.200
We don't need superhuman intelligence to solve all those problems.

01:25:01.200 --> 01:25:08.200
We need an appropriate emotional response to the untenability of the status quo

01:25:08.200 --> 01:25:15.200
and we need a political dialogue that eventually transcends our tribalism.

01:25:15.200 --> 01:25:19.200
For those of you that don't know, this podcast is sponsored by Woop, a company that I'm a shareholder in

01:25:19.200 --> 01:25:22.200
and I'm obsessed with my Woop, it's glued to my wrist 24x7

01:25:22.200 --> 01:25:29.200
and for those of you that don't know, it's essentially a personalized wearable health and fitness coach that helps me to have the best possible health.

01:25:29.200 --> 01:25:31.200
My Woop has literally changed my life.

01:25:31.200 --> 01:25:34.200
Woop is doing something this month which I'd highly suggest checking out.

01:25:34.200 --> 01:25:38.200
It's a global community challenge called the Core 4 Challenge.

01:25:38.200 --> 01:25:44.200
Essentially, they guide you through a set of four activities throughout the month of August that are scientifically proven to improve your overall health.

01:25:44.200 --> 01:25:49.200
I'm giving it a go and I can't wait to see the impact it has on me and I highly recommend you to join me with that.

01:25:49.200 --> 01:25:52.200
So if you're not on Woop here, there is no better time to start.

01:25:52.200 --> 01:25:56.200
If you're a friend of mine, there's a high probability that I've already given you a Woop because I'm that obsessed with it.

01:25:56.200 --> 01:25:58.200
It is the thing that I check when I wake up in the morning.

01:25:58.200 --> 01:25:59.200
It's the first thing that I look at.

01:25:59.200 --> 01:26:02.200
I want the information on my sleep to then plan my day around.

01:26:02.200 --> 01:26:12.200
So if you haven't joined Woop yet, head to join.woop.com.co to get your free Woop device and your first month free.

01:26:12.200 --> 01:26:16.200
Try it for free and if you don't like it after 29 days, they're going to give you your money back.

01:26:16.200 --> 01:26:18.200
But I have a suspicion that you're going to keep it.

01:26:18.200 --> 01:26:20.200
Check it out now and let me know how you get on.

01:26:20.200 --> 01:26:21.200
Send me a DM.

01:26:21.200 --> 01:26:22.200
Quick one.

01:26:22.200 --> 01:26:27.200
If you've been listening to this podcast for some time, one of the recurring messages you've heard over and over and over again,

01:26:27.200 --> 01:26:34.200
especially when we first had that conversation with Tim Spector, is about the importance of greens in our diet.

01:26:34.200 --> 01:26:41.200
And a while ago, I started pressing my friends at Hewlett to come out with a product that did exactly that.

01:26:41.200 --> 01:26:45.200
Allowed you to have all those greens, the vitamins and minerals you need in a drink.

01:26:45.200 --> 01:26:52.200
And after several, several, several months of iterations and processes, they released this product called Hewlett Daily Greens,

01:26:52.200 --> 01:26:59.200
which is now one of my favorite products from Hewlett because it tastes great and it fills that very important nutritional gap that I had in my diet.

01:26:59.200 --> 01:27:08.200
The problem is, it launched in the US and it sold out straight away and became a smash hit for Hewlett for the very reasons I've described.

01:27:08.200 --> 01:27:12.200
It's now back in stock in the United States, but it's not here in the UK yet.

01:27:12.200 --> 01:27:16.200
So if you're a UK listener, which I know a lot of you are, it's not yet available.

01:27:16.200 --> 01:27:19.200
So let's all attack Hewlett.

01:27:19.200 --> 01:27:24.200
Let's DM them everywhere we can and tell them to bring Hewlett Daily Greens to the UK.

01:27:24.200 --> 01:27:26.200
This is the product.

01:27:26.200 --> 01:27:31.200
When it is available in the UK, I'm going to let you know first, but until then, let's spam their DMs.

01:27:31.200 --> 01:27:39.200
You and, I'd say a few others, maybe two or three others, helped change my mind about one of the most profound things I think anyone could believe,

01:27:39.200 --> 01:27:46.200
which was when I was 18, I believed in Christianity and then there was a couple of moments that shook my belief.

01:27:46.200 --> 01:27:58.200
Nothing on a personal level, just a couple of ideas that managed to sort of infect my operating system that led my curiosity towards your work.

01:27:58.200 --> 01:28:00.200
And I changed my mind profoundly.

01:28:00.200 --> 01:28:03.200
It's such a profound change that I had.

01:28:03.200 --> 01:28:05.200
How do we change our minds?

01:28:05.200 --> 01:28:12.200
And I really want to focus that question on the individual's mind.

01:28:12.200 --> 01:28:14.200
I want to change my mind.

01:28:14.200 --> 01:28:23.200
I want better beliefs, better ideas in my head that are going to allow me to get out of my own way because I'm not a cheat.

01:28:23.200 --> 01:28:24.200
I'm miserable.

01:28:24.200 --> 01:28:27.200
I'm not living the life that I...

01:28:27.200 --> 01:28:31.200
I would say I know I can live, but some people don't even know they can live a better life.

01:28:31.200 --> 01:28:32.200
I'm not happy.

01:28:32.200 --> 01:28:34.200
That's the signal.

01:28:34.200 --> 01:28:37.200
And I want to rectify this in some way.

01:28:37.200 --> 01:28:38.200
Yeah.

01:28:38.200 --> 01:28:42.200
Well, there are a few right lines for me.

01:28:42.200 --> 01:28:49.200
We take our ethical lives and our relationships to other people.

01:28:49.200 --> 01:28:57.200
There's the problem of individual well-being that's still real even if you're in a moral solitude.

01:28:57.200 --> 01:29:02.200
If you're on a desert island by yourself, you really don't have ethical questions that are emerging

01:29:02.200 --> 01:29:06.200
because you're not in a relationship with anybody else, but you still have the problem of how to be happy.

01:29:06.200 --> 01:29:11.200
But so much of our unhappiness is in collaboration with others.

01:29:11.200 --> 01:29:13.200
We're unhappy in our relationships.

01:29:13.200 --> 01:29:17.200
We're unhappy professionally.

01:29:17.200 --> 01:29:22.200
And it's worth looking at how we're behaving with other people.

01:29:22.200 --> 01:29:35.200
For me, the highest leverage change I ever made, and it's, again, it's very easy to spell out and it's very clear,

01:29:35.200 --> 01:29:43.200
and ultimately it's pretty easy, is just to decide that you're not going to lie about anything, really.

01:29:43.200 --> 01:29:48.200
I mean, there might be some situations in extremis where you'll feel forced to lie,

01:29:48.200 --> 01:29:58.200
but those, in my view, are analogous to acts of violence that you may be forced to use in self-defense.

01:29:58.200 --> 01:30:02.200
A line is sort of the first stage on the continuum of violence for me.

01:30:02.200 --> 01:30:09.200
I'm not going to lie to someone unless I recognize that this is not a rational actor who I can possibly collaborate with.

01:30:09.200 --> 01:30:19.200
This is someone I have to avoid or defeat or otherwise contain their propensity to do me harm.

01:30:19.200 --> 01:30:25.200
So, yes, if the Nazis come to the door and ask if you've got Anne Frank and the Attic, yes, you can lie,

01:30:25.200 --> 01:30:30.200
or you can shoot them, these are not normal circumstances.

01:30:31.200 --> 01:30:45.200
But that aside, every other moment in life where people are tempted to lie is one that I think you can categorically rule out as being unethical

01:30:45.200 --> 01:30:58.200
and beyond unethical, it's just not, it's creating a life you don't, when you examine it, you don't want to live.

01:30:58.200 --> 01:31:05.200
The moment you know that you're not going to lie to people and they know that about you,

01:31:05.200 --> 01:31:13.200
it's like all of the social dials get recalibrated on both sides,

01:31:13.200 --> 01:31:24.200
and then you find yourself in the presence of people who don't ask you for your opinion unless they really want it.

01:31:25.200 --> 01:31:35.200
And then when you're honest, then it's a night and day difference when you're giving people feedback, critical feedback,

01:31:35.200 --> 01:31:45.200
and they know you're honest, they know their bullshit detector is not going off because they just know you're,

01:31:45.200 --> 01:31:53.200
even when it's not convenient, you're being honest, or even when it's not comfortable, you're being honest.

01:31:54.200 --> 01:32:01.200
One that's incredibly valuable because basically you're giving them the information that you would want if you were in their shoes,

01:32:01.200 --> 01:32:04.200
because we have this sort of delusion that takes over us.

01:32:04.200 --> 01:32:11.200
Whenever we're tempted to tell a white lie, we imagine, okay, this person doesn't want,

01:32:11.200 --> 01:32:19.200
it'd be much better for me to just tell them the kind fiction than tell them the uncomfortable truth,

01:32:19.200 --> 01:32:25.200
but we don't calculate for the golden rule there most of the time,

01:32:25.200 --> 01:32:29.200
and if you just took a moment, you'd realize, wait a minute,

01:32:29.200 --> 01:32:38.200
does someone who is actually doing a bad job want me to tell them that they're doing a good job

01:32:38.200 --> 01:32:44.200
and then just send them out into the world to bounce around other people who are going to be recognizing,

01:32:44.200 --> 01:32:49.200
as I just did, that the thing they're doing isn't so great.

01:32:49.200 --> 01:32:51.200
You're just not doing them a favor.

01:32:51.200 --> 01:32:53.200
This is part of the nature of belief change, isn't it?

01:32:53.200 --> 01:33:00.200
When we believe that someone is on our side, or we believe from a political standpoint that they represent,

01:33:00.200 --> 01:33:04.200
99% of the views that we represent, we're much more likely to change our beliefs.

01:33:04.200 --> 01:33:11.200
I spoke to Tali Sharra about this, the neuroscientist, and I wrote about this in a chapter in my upcoming book about how you change people's minds.

01:33:11.200 --> 01:33:16.200
They showed in the elections that if a flat earther says something to a flat earther about the nature of the earth,

01:33:16.200 --> 01:33:20.200
they'll believe it, but if NASA says something to a flat earther, they will just dismiss it on site

01:33:20.200 --> 01:33:26.200
because the source of that information is not one that they believe, or trust, or like, or believe is well-intentioned.

01:33:26.200 --> 01:33:28.200
This is a bug, not a feature.

01:33:28.200 --> 01:33:35.200
It's understandable, but this is something we have to grow beyond because the truth is the truth.

01:33:36.200 --> 01:33:39.200
It goes in both directions.

01:33:39.200 --> 01:33:48.200
The person on your team who you love and respect is capable in their very next sentence of speaking of falsehood,

01:33:48.200 --> 01:33:51.200
and you need to be able to detect that.

01:33:51.200 --> 01:34:00.200
Conversely, the person you least respect is capable of saying something that's quite incisive and worth taking on board.

01:34:01.200 --> 01:34:14.200
We have to have this sort of metacognitive layer where we're noticing how we're getting played by our social alliances

01:34:14.200 --> 01:34:24.200
and recognize that the truth, and rather often important truths, are evaluated by different principles.

01:34:24.200 --> 01:34:30.200
I mean, it's not a matter of the messenger. You shouldn't shoot the messenger and you shouldn't worship him.

01:34:30.200 --> 01:34:40.200
You mentioned lying as being a significant step change in your own happiness. Is that accurate?

01:34:40.200 --> 01:34:42.200
In my happiness.

01:34:44.200 --> 01:34:46.200
Immensely so.

01:34:46.200 --> 01:34:50.200
Practically and specifically how?

01:34:50.200 --> 01:34:59.200
When you look at how people ruin their reputations and their relationships and their businesses, their careers,

01:34:59.200 --> 01:35:05.200
the gateway to all of the misbehavior that accomplishes that is lying.

01:35:05.200 --> 01:35:08.200
I mean, look at some of that Lance Armstrong or Tiger Woods.

01:35:08.200 --> 01:35:13.200
These guys are the absolute apogee of sport.

01:35:13.200 --> 01:35:19.200
Everyone loves them. Everyone's just amazed at what they've accomplished.

01:35:20.200 --> 01:35:27.200
Yet the dysfunction in their lives just gets vomited up for all to see at a certain point.

01:35:27.200 --> 01:35:35.200
It was just enabled at every stage along the way by lying.

01:35:35.200 --> 01:35:42.200
If either of them had early in their career before they became famous, before they became rich,

01:35:43.200 --> 01:35:50.200
before they became tempted to do anything that was going to derail their lives later on,

01:35:50.200 --> 01:35:54.200
if they had decided they weren't going to lie, right?

01:35:54.200 --> 01:36:00.200
They would have found all, everything else they did to screw up their success impossible.

01:36:00.200 --> 01:36:07.200
So when I decided, and this was in the book, this was a course I took at Stanford.

01:36:07.200 --> 01:36:12.200
It was a seminar with this brilliant professor, Ron Howard, who many people,

01:36:12.200 --> 01:36:17.200
I think some people in Silicon Valley have taken this course as well.

01:36:17.200 --> 01:36:21.200
I mean, this course was just like a machine.

01:36:21.200 --> 01:36:25.200
Undergraduates and graduate students would come in on one side,

01:36:25.200 --> 01:36:32.200
and then 12 weeks later would come out convinced that basically lying was no longer on the menu, right?

01:36:33.200 --> 01:36:40.200
The whole seminar was an analysis of the question, is it ever right to lie?

01:36:40.200 --> 01:36:49.200
And really we focused on white lies, truly tempting lies as opposed to the obvious lies that screw up people's lives and relationships.

01:36:51.200 --> 01:36:58.200
It's just so corrosive, and it's corrosive of relationships in ways that you,

01:36:59.200 --> 01:37:02.200
unless you're a student of this kind of thing, you don't necessarily notice.

01:37:02.200 --> 01:37:06.200
I mean, one example I believe that's in that book is that,

01:37:06.200 --> 01:37:12.200
I remember my wife was with a friend, and the two of them were out,

01:37:12.200 --> 01:37:18.200
and the friend had something she had to do with another friend later that night,

01:37:18.200 --> 01:37:21.200
but she didn't really feel like doing it,

01:37:21.200 --> 01:37:24.200
and she got a call from that friend in the presence of my wife,

01:37:24.200 --> 01:37:29.200
and she just lied to the friends to get out of the plan.

01:37:29.200 --> 01:37:33.200
She said, oh, I'm so sorry, but my daughter's got this thing,

01:37:33.200 --> 01:37:41.200
and it was just an utterly facile use of dishonesty to get,

01:37:41.200 --> 01:37:46.200
or she could have just been honest, but it was just too awkward to be honest,

01:37:46.200 --> 01:37:50.200
so she just got out of it with a lie, but now it's in the presence of my wife,

01:37:50.200 --> 01:37:53.200
and my wife is now, the immediate question is,

01:37:53.200 --> 01:37:56.200
how many times have I been on the other side of that conversation?

01:37:56.200 --> 01:38:03.200
How many times has she lied to me in an equally compelling way about something so trivial?

01:38:03.200 --> 01:38:10.200
And so it just eroded trust in that relationship in a way that the liar would never have known about,

01:38:10.200 --> 01:38:14.200
would never have detected it, because it just went right back to having a good time with,

01:38:14.200 --> 01:38:17.200
they would just have to lunch, and they continued having their lunch,

01:38:17.200 --> 01:38:20.200
and they're still having a good time, and it's all smiles,

01:38:20.200 --> 01:38:26.200
but my wife has just logged something about kind of the ethical limitations of this person,

01:38:26.200 --> 01:38:29.200
and the person doesn't know it, right?

01:38:29.200 --> 01:38:34.200
And so once you sort of pull on this thread,

01:38:34.200 --> 01:38:41.200
basically your entire life becomes, at least for the transition period,

01:38:41.200 --> 01:38:48.200
until this just becomes a habit that you no longer have to consider,

01:38:48.200 --> 01:38:54.200
suddenly the world becomes a kind of mirror thrown up to your mind,

01:38:54.200 --> 01:38:59.200
and you meet yourself in all these situations where you were avoiding yourself before.

01:38:59.200 --> 01:39:05.200
So like someone will say, do you want to have plans,

01:39:05.200 --> 01:39:08.200
or do you want to collaborate with me on this project?

01:39:09.200 --> 01:39:19.200
And if previously you always had recourse to some kind of white lie that just got you out of the awkward truth,

01:39:19.200 --> 01:39:26.200
which is the answer is no, and there are actually reasons why not, right?

01:39:26.200 --> 01:39:29.200
You never have to confront the awkwardness of that,

01:39:29.200 --> 01:39:32.200
you're this kind of person who has these kinds of commitments,

01:39:33.200 --> 01:39:39.200
I mean the most awkward one would be someone declares a romantic interest in you,

01:39:39.200 --> 01:39:50.200
and the answer is no, and it's no for a totally superficial reason, right?

01:39:50.200 --> 01:39:55.200
Like this person is, they're not attractive enough for you, right?

01:39:55.200 --> 01:40:00.200
Or they're overweight, it's just like you have your reason why not,

01:40:00.200 --> 01:40:03.200
and this is something you feel you cannot say, right?

01:40:03.200 --> 01:40:08.200
Now I'm not saying that you should always go out of your way,

01:40:08.200 --> 01:40:13.200
like someone with Tourette's who just helplessly blurts out the truth,

01:40:13.200 --> 01:40:18.200
like there's a scope for kindness and compassion and tact,

01:40:18.200 --> 01:40:23.200
but if someone is going to really drill down on the reasons why not,

01:40:23.200 --> 01:40:27.200
if the person says no, I want to know exactly why you don't want to go out with me,

01:40:28.200 --> 01:40:33.200
there's something to discover on either side of that true disclosure, right?

01:40:33.200 --> 01:40:37.200
Like either you are cast back on yourself and you have to realize,

01:40:37.200 --> 01:40:45.200
okay, I'm such a superficial person that it doesn't matter who anyone is,

01:40:45.200 --> 01:40:49.200
if they're 10 pounds overweight, I'm not interested, right?

01:40:49.200 --> 01:40:53.200
That's the mirror held up to your mind, it's like, okay, all right,

01:40:53.200 --> 01:40:56.200
so you're that kind of person, do you want to still be that kind of person?

01:40:56.200 --> 01:41:01.200
Do you really want to just decide that everyone, no matter what their virtues, right,

01:41:01.200 --> 01:41:05.200
and no matter what chaos is going on in their life,

01:41:05.200 --> 01:41:09.200
actually this person might actually lose those 10 pounds next month

01:41:09.200 --> 01:41:12.200
and you would have a very different situation,

01:41:12.200 --> 01:41:18.200
but are you really not available, are you really filtering by weight in this way?

01:41:18.200 --> 01:41:21.200
And are you really comfortable with that?

01:41:21.200 --> 01:41:24.200
And are you comfortable saying that?

01:41:24.200 --> 01:41:28.200
If somebody forces you to actually be honest?

01:41:28.200 --> 01:41:30.200
We have a closing tradition on this podcast

01:41:30.200 --> 01:41:32.200
where the last guest leaves a question for the next guest,

01:41:32.200 --> 01:41:34.200
not knowing who they're going to leave it for.

01:41:34.200 --> 01:41:37.200
The question that's been left for you, peckable handwriting.

01:41:37.200 --> 01:41:41.200
Where do you want to be when you die?

01:41:41.200 --> 01:41:48.200
Describe the place, time, people, smell, and feeling.

01:41:49.200 --> 01:41:57.200
Well, this actually connects with an idea I've had.

01:41:57.200 --> 01:42:01.200
I mean, I think what we need, we haven't talked about psychedelics here,

01:42:01.200 --> 01:42:05.200
but there's been this renaissance in research in psychedelics,

01:42:05.200 --> 01:42:07.200
and it's hard to know.

01:42:07.200 --> 01:42:12.200
I'm worried that we could recapitulate some of the errors of the 60s

01:42:12.200 --> 01:42:17.200
and roll this all out in a way that's less than wise,

01:42:17.200 --> 01:42:22.200
but the wise version would be, I think we need to recapitulate

01:42:22.200 --> 01:42:25.200
something like the Mysteries of Elusis,

01:42:25.200 --> 01:42:30.200
where we have rites of passage that are enabled by,

01:42:30.200 --> 01:42:36.200
in many people's case, psychedelics and the practice of meditation.

01:42:36.200 --> 01:42:43.200
I just think these are just fundamental tools of insight that are...

01:42:43.200 --> 01:42:46.200
I mean, for most people, it's hard to see how they would get them any other way.

01:42:47.200 --> 01:42:52.200
There's a longer conversation about which molecule and how and all that,

01:42:52.200 --> 01:43:01.200
but another component of this is a hospice situation where the experience of dying

01:43:01.200 --> 01:43:08.200
is as wisely embraced and facilitated as is possible,

01:43:08.200 --> 01:43:12.200
and I think psychedelics could certainly play a role for many people there.

01:43:12.200 --> 01:43:17.200
I imagine something like, we need places that are truly beautiful

01:43:17.200 --> 01:43:23.200
where people have gone to die and their families can visit them there,

01:43:23.200 --> 01:43:37.200
and it is just a final rite of passage that is embraced with all the wisdom we can muster there.

01:43:38.200 --> 01:43:45.200
In my case, currently, I'd be happy to be home,

01:43:45.200 --> 01:43:52.200
but wherever home is at that point, I would want a view of the sky.

01:43:52.200 --> 01:43:55.200
It could be an ocean beneath the sky, that would be ideal.

01:43:59.200 --> 01:44:04.200
There's basically nothing that makes me happier than just looking at a blue sky

01:44:04.200 --> 01:44:09.200
with just watching cumulus clouds move across a blue sky.

01:44:09.200 --> 01:44:14.200
I can extract so much mental pleasure just looking at that.

01:44:18.200 --> 01:44:23.200
If I'm going to spend my last hours of life looking at anything,

01:44:23.200 --> 01:44:26.200
if my eyes are going to be open, looking at the sky and having...

01:44:26.200 --> 01:44:29.200
The stars with the sky, the daytime sky.

01:44:30.200 --> 01:44:37.200
Light pollution is enough of a thing in my world that I feel like I go for years

01:44:37.200 --> 01:44:43.200
without seeing a good night sky, so I've kind of given up hope there,

01:44:43.200 --> 01:44:45.200
but I do love that.

01:44:47.200 --> 01:44:52.200
But yeah, just a view of the sky and with the people I love at that point

01:44:52.200 --> 01:44:55.200
who are still alive at that point.

01:44:56.200 --> 01:45:01.200
I'm not worried about death in that sense.

01:45:06.200 --> 01:45:08.200
The death part is not a problem.

01:45:11.200 --> 01:45:15.200
I can imagine there could be sort of medical chaos and uncertainty

01:45:15.200 --> 01:45:19.200
and all of the weirdness that happens around the dying process,

01:45:19.200 --> 01:45:21.200
depending on...

01:45:21.200 --> 01:45:24.200
There are all kinds of ways to die that I wouldn't choose,

01:45:25.200 --> 01:45:30.200
but having a nice place to do that with a view of the sky

01:45:30.200 --> 01:45:34.200
would be the only solution I think I would require.

01:45:34.200 --> 01:45:37.200
The question asks the smell.

01:45:37.200 --> 01:45:38.200
Give me the smell.

01:45:38.200 --> 01:45:40.200
Give me an ocean breeze.

01:45:40.200 --> 01:45:43.200
I have put an ocean there, so yeah, an ocean breeze would be perfect.

01:45:45.200 --> 01:45:46.200
Sam, thank you so much.

01:45:46.200 --> 01:45:48.200
Thank you for not just this conversation.

01:45:48.200 --> 01:45:50.200
As I said to you before you sat down,

01:45:50.200 --> 01:45:54.200
you were pivotal in really helping me to unpack some problems on our jungle,

01:45:54.200 --> 01:45:59.200
some conflicts I should describe them as with my view on religious belief

01:45:59.200 --> 01:46:04.200
and the nature of the world, but I think more importantly,

01:46:04.200 --> 01:46:08.200
you didn't rob me of my religious beliefs and leave me with nothing.

01:46:08.200 --> 01:46:11.200
You left me with something else, which is something that was really important to me,

01:46:11.200 --> 01:46:14.200
which was the idea that there can still be great meaning

01:46:14.200 --> 01:46:18.200
and there can be what you describe as spirituality in the absence

01:46:18.200 --> 01:46:21.200
or in the place of that religious belief.

01:46:21.200 --> 01:46:24.200
Religious belief gives people a lot of things,

01:46:24.200 --> 01:46:26.200
and it's funny because when I was religious

01:46:26.200 --> 01:46:29.200
and I went on the journey to becoming agnostic, let's say,

01:46:29.200 --> 01:46:33.200
I was in conflict with people, as in I would want to have a debate with everybody

01:46:33.200 --> 01:46:37.200
and I spent those two years watching everything that you and Richard Dawkins

01:46:37.200 --> 01:46:42.200
and Hitchens had all done, and then I came out of the side and it was peaceful.

01:46:42.200 --> 01:46:45.200
You believe what you want, I'll believe what I want.

01:46:45.200 --> 01:46:48.200
As long as we're not causing any conflicts with each other

01:46:48.200 --> 01:46:50.200
and doing any harm, it's okay.

01:46:50.200 --> 01:46:53.200
And then I discovered what I would call my own spirituality,

01:46:53.200 --> 01:46:56.200
which is the meaning that I see in the world around me

01:46:56.200 --> 01:46:59.200
and the self and things like psychedelics.

01:46:59.200 --> 01:47:01.200
And it's a better place to be.

01:47:01.200 --> 01:47:05.200
And it removed my fear of death, which I had as a religious person.

01:47:05.200 --> 01:47:08.200
So thank you. Thank you for that.

01:47:08.200 --> 01:47:10.200
And all your subsequent work, incredible books,

01:47:10.200 --> 01:47:12.200
you've written so many of them that are absolutely incredible.

01:47:12.200 --> 01:47:18.200
You've got an unbelievable podcast, which I was gorging on before you came here as well in an app,

01:47:18.200 --> 01:47:22.200
which, I mean, if you could speak just a few sentences about the meaning of the app

01:47:22.200 --> 01:47:24.200
and what you do, I know it's much more than meditation now,

01:47:24.200 --> 01:47:28.200
but I think people listening to this might be compelled to check it out and download it.

01:47:28.200 --> 01:47:32.200
Yeah, well, so I had that book, which you're holding, Waking Up,

01:47:32.200 --> 01:47:36.200
which is where I talk about my experience in meditation

01:47:36.200 --> 01:47:43.200
and just how I fit it into a scientific, secular worldview.

01:47:43.200 --> 01:47:48.200
And it just turns out that an app is a much better delivery system for that kind of information.

01:47:48.200 --> 01:47:51.200
I mean, it's just hearing audio. You don't even need video.

01:47:51.200 --> 01:47:55.200
I think audio is the perfect medium for it.

01:47:55.200 --> 01:47:59.200
So when that technology came about or when I discovered it,

01:47:59.200 --> 01:48:03.200
I just felt incredibly lucky to be able to build it.

01:48:03.200 --> 01:48:06.200
And so it's kind of outgrown me now. There are many, many teachers on it

01:48:06.200 --> 01:48:09.200
and many other topics beyond meditation that are touched.

01:48:09.200 --> 01:48:20.200
But it really subverts all of the problems that some of which we touched upon here with the smartphone.

01:48:20.200 --> 01:48:25.200
I mean, the smartphone has become this tool of fragmentation for us.

01:48:25.200 --> 01:48:29.200
It fragments our attention. It continually interrupts our experience.

01:48:29.200 --> 01:48:33.200
It's depending on how you use it.

01:48:33.200 --> 01:48:36.200
But most of what we do with it, you know, you're checking Slack,

01:48:36.200 --> 01:48:38.200
you're checking your email, you're checking your social media,

01:48:38.200 --> 01:48:42.200
you're just, it's punctuating your life with all this, you know,

01:48:42.200 --> 01:48:46.200
at this point, seemingly necessary interruptions.

01:48:46.200 --> 01:48:52.200
But this app or, you know, really any app like it that is delivering this kind of content

01:48:52.200 --> 01:48:56.200
subverts all that because it's just, this is, it's just a platform

01:48:56.200 --> 01:49:01.200
where you're getting audio that is guiding you in a specific,

01:49:01.200 --> 01:49:05.200
very specific use of attention and a sort of reordering of your priorities

01:49:05.200 --> 01:49:13.200
and getting you to recognize things about your experience that you wouldn't otherwise see.

01:49:13.200 --> 01:49:18.200
And yeah, an app is just a sheer good luck.

01:49:18.200 --> 01:49:22.200
It turns out it's just the perfect delivery system for that information.

01:49:22.200 --> 01:49:26.200
So yeah, I just felt very lucky to have stumbled upon it because again, you know,

01:49:26.200 --> 01:49:30.200
10 years ago, there were no apps and, you know, there's just,

01:49:30.200 --> 01:49:32.200
all I could do is write a book.

01:49:32.200 --> 01:49:33.200
Sam, thank you.

01:49:33.200 --> 01:49:34.200
Yeah, thank you.

01:49:34.200 --> 01:49:36.200
Thank you so much for your generosity.

01:49:36.200 --> 01:49:37.200
Yeah, a pleasure to meet you as well.

01:49:37.200 --> 01:49:38.200
Congratulations with everything.

01:49:38.200 --> 01:49:42.200
I mean, it's really, I was catching up on your podcast in anticipation of this

01:49:42.200 --> 01:49:44.200
and it's amazing the reach you've got now.

01:49:44.200 --> 01:49:46.200
So it's wonderful.

01:49:46.200 --> 01:49:50.200
No, it's still trying to catch up with it, but it's a credit to all of the team.

01:49:50.200 --> 01:49:53.200
And I really want to say from the bottom of my heart, thank you.

01:49:53.200 --> 01:49:56.200
Because the work you do is really, really important.

01:49:56.200 --> 01:49:59.200
It's been important in my life, as I've said, but it's just really important.

01:49:59.200 --> 01:50:03.200
And I feel like we're living in a world where like nuance and all the things you've talked about

01:50:03.200 --> 01:50:08.200
and openness to debate and honest dialogue asks, we're getting further and further away from there.

01:50:08.200 --> 01:50:11.200
So if there's anyone left in this world that's still willing to engage on that level,

01:50:11.200 --> 01:50:13.200
I feel like they must be protected at all costs.

01:50:13.200 --> 01:50:14.200
And I see you as one of those people.

01:50:14.200 --> 01:50:15.200
So thank you.

01:50:15.200 --> 01:50:16.200
Nice.

01:50:16.200 --> 01:50:17.200
Well, to be continued.

01:50:20.200 --> 01:50:21.200
Thank you.

