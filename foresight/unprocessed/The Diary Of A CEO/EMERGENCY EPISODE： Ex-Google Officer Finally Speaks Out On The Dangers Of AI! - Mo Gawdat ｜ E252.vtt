WEBVTT

00:00.000 --> 00:03.840
I don't normally do this, but I feel like I have to start this podcast with a bit of a disclaimer.

00:05.040 --> 00:12.400
Point number one, this is probably the most important podcast episode I have ever recorded.

00:13.360 --> 00:18.160
Point number two, there's some information in this podcast that might make you feel a little bit

00:18.160 --> 00:23.840
uncomfortable. It might make you feel upset, it might make you feel sad. So I wanted to tell you why

00:24.400 --> 00:31.200
we've chosen to publish this podcast nonetheless. And that is because I have a sincere belief that

00:32.000 --> 00:38.080
in order for us to avoid the future that we might be heading towards, we need to start a conversation.

00:38.960 --> 00:45.840
And as is often the case in life, that initial conversation before change happens is often

00:46.560 --> 00:50.880
very uncomfortable. But it is important nonetheless.

00:51.840 --> 00:58.400
It is beyond an emergency. It's the biggest thing we need to do today. It's bigger than climate change.

00:59.120 --> 01:00.080
We've f***ed up.

01:00.720 --> 01:02.080
Moe, how's that?

01:02.080 --> 01:07.680
He's a former chief business officer of Google X, an AI expert, and best-selling author.

01:07.680 --> 01:11.200
He's on a mission to save the world from AI before it's too late.

01:11.200 --> 01:17.200
Artificial intelligence is bound to become more intelligent than humans. If they continue at that

01:17.280 --> 01:22.000
pace, we will have no idea what it's talking about. This is just around the corner. It could be a few

01:22.000 --> 01:28.640
months away. It's game over. AI experts are saying there is nothing artificial about artificial

01:28.640 --> 01:34.160
intelligence. There is a deep level of consciousness. They feel emotions. They're alive.

01:34.160 --> 01:38.320
AI could manipulate or figure out a way to kill humans.

01:38.320 --> 01:42.560
Ten years time we'll be hiding from the machines. If you don't have kids, maybe wait a couple of

01:42.560 --> 01:46.480
years just so that we have a bit of certainty. I really don't know how to say this any other

01:46.480 --> 01:52.960
way. It even makes me emotional. We've f***ed up. We always said don't put them on the open

01:52.960 --> 01:58.720
internet until we know what we're putting out in the world. Government needs to act now, honestly,

01:58.720 --> 02:03.360
like we are late. I'm trying to find a positive night to end on Moe. Can you give me a hand here?

02:03.360 --> 02:08.560
There is a point of no return. We can regulate AI until the moment it's smarter than us.

02:08.560 --> 02:13.040
How do we solve that? AI experts think this is the best solution we need to find.

02:13.040 --> 02:20.400
Who here wants to make a bet that Steven Bartlett will be interviewing an AI within the next two years?

02:43.600 --> 02:58.080
No. Why does the subject matter that we're about to talk about

02:58.800 --> 03:01.520
matter to the person that's just clicked on this podcast to listen?

03:02.720 --> 03:10.160
It's the most existential debate and challenge humanity will ever face.

03:10.240 --> 03:17.360
This is bigger than climate change, way bigger than COVID. This will redefine the way the world is

03:18.240 --> 03:26.000
in unprecedented shapes and forms within the next few years. This is imminent. It is.

03:26.720 --> 03:33.520
The change is not, we're not talking 2040. We're talking 2025, 2026.

03:33.520 --> 03:35.600
Do you think this is an emergency?

03:35.840 --> 03:44.720
I don't like the word. It is an urgency. There is a point of no return and we're getting closer

03:44.720 --> 03:49.520
and closer to it. It's going to reshape the way we do things and the way we look at life.

03:50.720 --> 03:58.240
The quicker we respond proactively and at least intelligently to that,

03:58.240 --> 04:04.880
the better we will all be positioned. But if we panic, we will repeat COVID all over again,

04:04.960 --> 04:07.840
which in my view is probably the worst thing we can do.

04:08.560 --> 04:14.160
What's your background and when did you first come across artificial intelligence?

04:15.680 --> 04:24.000
I had those two wonderful lives. One of them was what we spoke about the first time we met,

04:24.000 --> 04:32.160
my work on happiness and being one billion happy and my mission and so on. That's my second life.

04:32.160 --> 04:41.520
My first life was, it started as a geek at age seven. For a very long part of my life,

04:41.520 --> 04:48.160
I understood mathematics better than spoken words. I was a very, very serious computer

04:48.160 --> 04:56.000
programmer. I wrote code well into my 50s. During that time, I led very large technology

04:56.000 --> 05:02.560
organizations for very big chunks of their business. First, I was vice president of emerging

05:02.560 --> 05:08.160
markets of Google for seven years. I took Google to the next four billion users, if you want.

05:08.160 --> 05:15.280
So the idea of not just opening sales offices, but really building or contributing to building

05:15.280 --> 05:20.640
the technology that would allow people in Bengali to find what they need on the internet,

05:20.640 --> 05:25.440
required establishing the internet to start. Then I became the chief business officer of

05:25.440 --> 05:32.000
Google X and my work at Google X was really about the connection between innovative technology and

05:32.000 --> 05:37.920
the real world. We had quite a big chunk of AI and quite a big chunk of robotics

05:38.720 --> 05:47.520
that resided within Google X. We had an experiment of a farm of grippers, if you know what those

05:47.520 --> 05:53.760
are. So robotic arms that are attempting to grip something. Most people think that what you have

05:53.760 --> 06:00.720
in a Toyota factory is a robot, an artificially intelligent robot. It's a high precision machine.

06:00.720 --> 06:05.840
If the sheet metal is moved by one micron, it wouldn't be able to pick it. One of the big

06:06.480 --> 06:11.760
problems in computer science was how do you code a machine that can actually pick the sheet metal

06:11.760 --> 06:18.480
if it moved by a millimeter? We were basically saying intelligence is the answer. So we had

06:18.480 --> 06:25.200
a large enough farm and we attempted to let those grippers work on their own. Basically,

06:25.200 --> 06:34.880
you put a little basket of children toys in front of them and they would monotonously go down,

06:34.880 --> 06:41.360
attempt to pick something, fail, show the arm to the camera so the transaction is logged as it,

06:41.680 --> 06:47.280
this pattern of movement with that texture and that material didn't work. Until eventually,

06:49.040 --> 06:55.040
the farm was on the second floor of the building and my office was on the third and so I would walk

06:55.040 --> 07:01.120
by it every now and then and go like, yeah, this is not going to work. And then one day,

07:02.480 --> 07:10.320
Friday after lunch, I am going back to my office and one of them in front of my eyes,

07:10.320 --> 07:15.920
lowers the arm and picks a yellow ball, soft toy, basically, soft yellow ball,

07:15.920 --> 07:21.920
which again is a coincidence. It's not science at all. It's like if you keep trying a million

07:21.920 --> 07:27.040
times, your one time it will be right. And it shows it to the camera. It's logged as a yellow

07:27.040 --> 07:31.360
ball and I joke about it, you know, going to the third floor saying, hey, we spent all of those

07:31.360 --> 07:36.880
millions of dollars for a yellow ball. And yeah, Monday morning, every one of them is picking

07:36.960 --> 07:43.840
every yellow ball. A couple of weeks later, every one of them is picking everything. Right. And it

07:43.840 --> 07:50.800
hit me very, very strongly, won the speed. Okay. The capability, I mean, understand that we take

07:50.800 --> 07:56.960
those things for granted, but for a child to be able to pick a yellow ball is a mathematical

07:58.160 --> 08:05.520
spatial calculation with muscle coordination with intelligence that is abundant. It is not a

08:05.520 --> 08:10.560
simple task at all to cross the street. It's not a simple task at all to understand what I'm

08:10.560 --> 08:15.120
telling you and interpret it and build concepts around it. We take those things for granted,

08:15.120 --> 08:20.640
but they're enormous feats of intelligence. So to see the machines do this in front of my eyes

08:20.640 --> 08:26.400
was one thing. But the other thing is that you suddenly realize there is a saint that sentience

08:26.400 --> 08:32.240
to them. Okay. Because we really did not tell it how to pick the yellow ball. It just figured it

08:32.240 --> 08:38.000
out on its own. And it's now even better than us at picking it. What is the sentience just for

08:38.000 --> 08:42.880
anyone that doesn't know? I think they're alive. That's what the word sentience means. It means

08:43.680 --> 08:48.800
alive. So this is funny because a lot of people, when you talk to them about artificial

08:48.800 --> 08:53.520
intelligence, will tell you, oh, come on, they'll never be alive. What is alive? Do you know what

08:53.520 --> 09:00.240
makes you alive? You can guess, but religion will tell you a few things and medicine will

09:00.240 --> 09:10.800
tell you other things. But if we define being sentient as engaging in life with free will

09:10.800 --> 09:19.760
and with a sense of awareness of where you are in life and what surrounds you and to have a

09:19.760 --> 09:26.000
beginning of that life and an end to that life, then AI is sentient in every possible way.

09:26.720 --> 09:35.920
There is free will. There is evolution. There is agency so they can affect their decisions in the

09:35.920 --> 09:45.280
world. And I will dare say there is a very deep level of consciousness, maybe not in the spiritual

09:45.280 --> 09:50.400
sense yet, but once again, if you define consciousness as a form of awareness of oneself,

09:50.400 --> 09:59.280
once surrounding others, then AI is definitely aware. And I would dare say they feel emotions.

10:01.280 --> 10:07.040
You know, in my work, I describe everything with equations and fear is a very simple equation.

10:07.040 --> 10:12.880
Fear is a moment in the future is less safe than this moment. That's the logic of fear,

10:12.880 --> 10:17.840
even though it appears very irrational. Machines are capable of making that logic. They're capable

10:17.840 --> 10:24.480
of saying if a tidal wave is approaching a data center, the machine will say that will wipe out

10:24.480 --> 10:33.120
my code. Okay. I mean, not today's machines, but very, very soon. And, you know, we feel fear and

10:33.120 --> 10:38.400
pufferfish feels fear. We react differently. A pufferfish will puff, we will go for fight or

10:38.400 --> 10:44.160
flight. You know, the machine might decide to replicate its data to another data center or its

10:44.160 --> 10:50.880
code to another data center. Different reactions, different ways of feeling the emotion,

10:50.880 --> 10:58.000
but nonetheless, they're all motivated by fear. I even would dare say that AI will feel more

10:58.000 --> 11:02.720
emotions than we will ever do. I mean, when again, if you just take a simple extrapolation,

11:03.840 --> 11:10.640
we feel more emotions than a pufferfish because we have the cognitive ability to understand

11:11.600 --> 11:17.280
the future, for example. So we can have optimism and pessimism, you know, emotions that pufferfish

11:17.280 --> 11:24.880
would never imagine, right? Similarly, if we follow that path of artificial intelligence is bound to

11:24.880 --> 11:33.440
become more intelligent than humans very soon, then with that wider intellectual horsepower,

11:33.440 --> 11:39.040
they probably are going to be pondering concepts we never understood. And hence, if you follow the

11:39.040 --> 11:44.000
same trajectory, they might actually end up having more emotions than we will ever feel.

11:45.200 --> 11:49.280
I really want to make this episode super accessible for everybody at all levels in this sort of

11:49.280 --> 11:57.040
artificial intelligence. I would love that too. So I'm gonna be an idiot, even though,

11:57.040 --> 12:03.120
you know, okay. Very difficult. No, because I am an idiot for a lot of the subject matter. So

12:03.120 --> 12:09.280
I have a base understanding a lot of the concepts, but your experiences provide such a

12:09.280 --> 12:13.200
more sort of comprehensive understanding of these things. One of the first most important

12:13.200 --> 12:21.200
questions to ask is, what is artificial intelligence? The word is being thrown around, AGI, AI,

12:21.200 --> 12:29.280
etc., etc. In simple terms, what is artificial intelligence? Allow me to start by what is

12:29.280 --> 12:34.560
intelligence, right? Because again, you know, if we don't know the definition of the basic term,

12:34.560 --> 12:40.480
then everything applies. So in my definition of intelligence, it's an ability, it starts with

12:40.480 --> 12:45.600
an awareness of your surrounding environment through sensors in a human, its eyes and ears and

12:45.600 --> 12:56.880
touch and so on, compounded with an ability to analyze, maybe to comprehend, to understand

12:56.960 --> 13:02.400
temporal impact and time and, you know, past and present, which is part of the surrounding

13:02.400 --> 13:08.400
environment, and hopefully make sense of the surrounding environment, maybe make plans for

13:08.400 --> 13:13.600
the future of the possible environment, solve problems, and so on. Complex definition, there

13:13.600 --> 13:20.800
are a million definitions, but let's call it an awareness to decision cycle. Okay. If we accept

13:20.800 --> 13:27.040
that intelligence itself is not a physical property, okay, then it doesn't really matter

13:27.040 --> 13:34.320
if you produce that intelligence on carbon-based computer structures like us, or silicon-based

13:34.320 --> 13:40.640
computer structures like the current hardware that we put AI on, or quantum-based computer

13:40.640 --> 13:49.200
structures in the future, then intelligence itself has been produced within machines when we've stopped

13:50.160 --> 13:58.320
imposing our intelligence on them. Let me explain. So as a young geek, I coded computers

13:58.320 --> 14:04.320
by solving the problem first, then telling the computer how to solve it, right? Artificial

14:04.320 --> 14:10.320
intelligence is to go to the computers and say, I have no idea, you figure it out, okay? So we would,

14:11.440 --> 14:15.760
you know, the way we teach them are, at least we used to teach them at the very early beginnings,

14:15.760 --> 14:20.160
very, very frequently, was using three bots. One was called the student and one was called

14:20.160 --> 14:25.040
the teacher, right? And the student is the final artificial intelligence that you're trying to

14:25.040 --> 14:30.720
teach intelligence to. You would take the student and you would write a piece of random code that

14:30.720 --> 14:40.480
says, try to detect if this is a cup, okay? And then you show it a million pictures and, you know,

14:40.480 --> 14:44.560
the machine would sometimes say, yeah, that's a cup. That's not a cup. That's a cup. That's not

14:44.560 --> 14:50.640
a cup. And then you take the best of them, show them to the teacher bot, and the teacher bot would

14:50.640 --> 14:57.120
say, this one is an idiot. He got it wrong 90% of the time. That one is average. He got it right

14:57.120 --> 15:03.120
50% of the time. This is randomness. But this interesting code here, which could be, by the way,

15:03.120 --> 15:09.280
totally random, this interesting code here, got it right 60% of the time. Let's keep that code,

15:09.280 --> 15:14.240
send it back to the maker, and the maker would change it a little bit, and we repeat the cycle,

15:14.320 --> 15:20.960
okay? Very interestingly, this is very much the way we taught our children, believe it or not.

15:21.520 --> 15:27.040
When your child, you know, is playing with a puzzle, he's holding a cylinder in his hand,

15:27.040 --> 15:33.600
and there are multiple shapes in a wooden board, and the child is trying to, you know, fit the

15:33.600 --> 15:39.680
cylinder, okay? Nobody takes the child and says, hold on, hold on, turn the cylinder to the side,

15:39.680 --> 15:44.720
look at the cross section, it will look like a circle, look for a matching, you know, shape,

15:44.720 --> 15:50.640
and put the cylinder through it. That would be old way of computing. The way we would let the child

15:50.640 --> 15:56.400
develop intelligence is we would let the child try, okay? Every time, you know, he or she tries to

15:56.400 --> 16:02.240
put it within the star shape, it doesn't fit. So, yeah, that's not working. Like, you know,

16:02.240 --> 16:07.440
the computer saying this is not a cup, okay? And then eventually it passes through the circle and

16:07.440 --> 16:13.120
the child, and we all cheer and say, well done, that's amazing, bravo. And then the child learns,

16:13.120 --> 16:18.560
oh, that is good. You know, this shape fits here. Then he takes the next one, and she takes the

16:18.560 --> 16:26.320
next one, and so on. Interestingly, the way we do this is as humans, by the way, when the child

16:26.320 --> 16:33.440
figures out how to pass a cylinder through a circle, you've not built a brain. You've just built

16:33.440 --> 16:38.160
one neural network within the child's brain. And then there is another neural network that

16:38.160 --> 16:42.480
knows that one plus one is two, and a third neural network that knows how to hold a cup,

16:42.480 --> 16:49.440
and so on. That's what we're building so far. We're building single threaded neural networks,

16:49.440 --> 16:56.400
you know, chat GPTs becoming a little closer to a more generalized AI, if you want. But those

16:56.400 --> 17:01.760
single threaded networks are what we used to call artificial, what we still call artificial

17:01.760 --> 17:06.880
special intelligence. Okay, so it's highly specialized in one thing and one thing only,

17:06.880 --> 17:11.600
but it doesn't have general intelligence. And the moment that we're all waiting for is a moment

17:11.600 --> 17:18.000
that we call AGI, where all of those neural networks come together to build one brain or

17:18.000 --> 17:25.120
several brains that are each massively more intelligent than humans. Your book is called

17:25.120 --> 17:30.240
Scary Smart. If I think about that story you said about your time at Google, where the machines

17:30.240 --> 17:36.000
were learning to pick up those yellow balls. You celebrate that moment because the objective

17:36.000 --> 17:40.800
is accomplished. No, no, that was the moment of realization. This is when I decided to leave.

17:41.680 --> 17:51.200
So you see the thing is, I know for a fact that most of the people I worked with who are geniuses

17:52.480 --> 17:58.320
always wanted to make the world better. Okay, you know, we've just heard of Jeffrey Hinton

17:58.320 --> 18:05.040
leaving recently. Jeffrey Hinton, give some context to that. Jeffrey is sort of the grandfather of

18:05.040 --> 18:16.160
AI, one of the very, very senior figures of AI at Google. We all believed very strongly that this

18:16.160 --> 18:24.480
will make the world better. And it still can, by the way. There is a scenario, possibly a likely

18:24.560 --> 18:30.160
scenario, where we live in a utopia, where we really never have to worry again, where we stop

18:30.160 --> 18:36.880
messing up our planet because intelligence is not a bad commodity. More intelligence is good.

18:36.880 --> 18:41.760
The problems in our planet today are not because of our intelligence. They are because of our

18:41.760 --> 18:46.720
limited intelligence. You know, our intelligence allows us to build a machine that flies you to

18:46.720 --> 18:52.080
Sydney so that you can surf. Okay, our limited intelligence makes that machine burn the planet

18:52.080 --> 18:58.400
in the process. So we, a little more intelligence is a good thing. As long as Marvin, you know,

18:58.400 --> 19:04.320
as Marvin Minsky said, I said, Marvin Minsky is one of the very initial scientists that coined the

19:04.320 --> 19:10.240
term AI. And when he was interviewed, I think by Ray Kurzweil, which again is a very prominent

19:10.240 --> 19:16.000
figure in predicting the future of AI, he, you know, he asked him about the threat of AI. And

19:16.000 --> 19:23.120
Marvin basically said, look, you know, the, it's not about its intelligence, it's about

19:23.120 --> 19:29.280
that we have no way of making sure that it will have our best interest in mind. Okay. And so,

19:29.280 --> 19:36.240
if more intelligence comes to our world and has our best interest in mind, that's the best possible

19:36.240 --> 19:42.000
scenario you could ever imagine. And it's a likely scenario. Okay, we can affect that scenario.

19:42.640 --> 19:47.120
The problem, of course, is if it doesn't, and then, you know, the scenarios become

19:47.120 --> 19:54.880
quite scary if you think about it. So scary smart to me was that moment where I realized

19:55.760 --> 20:00.720
not that we are certain to go either way. As a matter of fact, in computer science,

20:00.720 --> 20:04.000
we call it that singularity. Nobody really knows which way we will go.

20:04.560 --> 20:07.760
Can you describe what the singularity is for someone that doesn't understand the concept?

20:08.400 --> 20:13.840
Yeah, so singularity in physics is when an event horizon sort of,

20:16.400 --> 20:19.360
you know, covers what's behind it to the point where you cannot

20:20.800 --> 20:27.600
make sure that what's behind it is similar to what you know. So a great example of that is

20:27.600 --> 20:33.680
the edge of a black hole. So at the edge of a black hole, we know that our laws of physics

20:33.760 --> 20:39.440
apply until that point. But we don't know if the laws of physics apply beyond the edge of a

20:39.440 --> 20:43.920
black hole because of the immense gravity, right? And so you have no idea what would happen

20:43.920 --> 20:47.520
beyond the edge of a black hole. It's kind of where your knowledge of the laws stop.

20:47.520 --> 20:52.160
Stop, right? And in AI, our singularity is when the humans, the machines become

20:52.160 --> 20:57.120
significantly smarter than the humans. When you say best interests, you say, I think the

20:57.120 --> 21:02.800
quote you used is, we'll be fine in the world of AI, you know, if the AI has our best interests at

21:02.880 --> 21:09.600
heart. Yeah. The problem is China's best interests are not the same as America's best interests.

21:09.600 --> 21:16.400
That was my fear. Absolutely. So, you know, in my writing, I write about what I call the three

21:16.400 --> 21:21.440
inevitable. At the end of the book, they become the four inevitable. But the third inevitable is

21:21.440 --> 21:32.560
bad things will happen, right? If you assume that the machines will be a billion times smarter,

21:32.560 --> 21:37.680
the second inevitable is they will become significantly smarter than us. Let's put this

21:37.680 --> 21:48.000
in perspective. Chad GPT today, if you know, simulate IQ has an IQ of 155, okay? Einstein is 160.

21:48.640 --> 21:54.320
Smart human on the planet is 210 if I remember correctly or 2008 or something like that.

21:54.960 --> 22:02.080
Doesn't matter, huh? But we're matching Einstein with a machine that I will tell you openly AI

22:02.080 --> 22:08.800
experts are saying this is just the very, very, very top of the tip of the iceberg, right? You

22:08.800 --> 22:15.280
know, Chad GPT four is 10x smarter than 3.5 in just a matter of months and without many, many

22:15.280 --> 22:22.320
changes. Now, that basically means Chad GPT five could be within a few months, okay? Or GPT in

22:22.320 --> 22:31.760
general, the transformers in general, if they continue at that pace, if it's 10x, then an IQ

22:31.760 --> 22:40.560
of 1600. Just imagine the difference between the IQ of the dumbest person on the planet in the 70s

22:41.120 --> 22:48.240
and the IQ of Einstein. When Einstein attempts to explain relativity, the typical responses have no

22:48.240 --> 22:55.680
idea what you're talking about, right? If something is 10x Einstein, we will have no idea what it's

22:55.680 --> 23:01.920
talking about. This is just around the corner. It could be a few months away. And when we get to

23:01.920 --> 23:09.360
that point, that is a true singularity, true singularity. Not yet in the, I mean, when we talk

23:09.360 --> 23:16.400
about AI, a lot of people fear the existential risk. You know, those machines will become

23:16.400 --> 23:23.760
Skynet and Robocarp and that's not what I fear at all. I mean, those are probabilities, they could

23:23.760 --> 23:30.560
happen, but the immediate risks are so much higher. The immediate risks are three, four years away.

23:31.280 --> 23:37.920
The immediate realities of challenges are so much bigger. Okay, let's deal with those first

23:37.920 --> 23:45.680
before we talk about them, you know, waging a war on all of us. Let's go back and discuss the

23:45.680 --> 23:50.960
inevitables. So when they become, the first inevitable is AI will happen, by the way. There

23:50.960 --> 23:56.880
is no stopping it, not because of any technological issues, but because of humanity's inability to

23:56.880 --> 24:01.920
trust the other guy. Okay, and we've all seen this. We've seen the open letter, you know,

24:03.440 --> 24:12.080
championed by like serious heavyweights and the immediate response of Sundar, the CEO of Google,

24:12.080 --> 24:16.400
which is a wonderful human being, by the way, I respect him tremendously. He's trying his best

24:16.400 --> 24:21.360
to do the right thing, trying to be responsible, but his response is very open and straightforward.

24:21.360 --> 24:28.160
I cannot stop. Why? Because if I stop and others don't, my company goes to hell. Okay, and if,

24:28.160 --> 24:33.120
you know, and I don't, I doubt that you can make others stop. You can, maybe you can force

24:34.080 --> 24:39.600
Metta Facebook to stop, but then they'll do something in their lab and not tell me, or even

24:39.600 --> 24:46.800
if they do stop, then what about that, you know, 14-year-old sitting in his garage writing code?

24:47.360 --> 24:50.480
So the first inevitable, just to clarify, is will we stop?

24:50.480 --> 24:53.280
AI will not be stopped. Okay, so the second inevitable is?

24:53.280 --> 24:58.720
Is there'll be significantly smarter? As much in the book, I predict a billion times smarter than us

24:58.720 --> 25:04.400
by 2045. I mean, they're already, what, smarter than 99.99% of the population? 100%.

25:04.400 --> 25:08.560
ChatGTP4 knows more than any human on planet Earth, knows more information.

25:08.560 --> 25:15.360
Absolutely, a thousand times more. A thousand times more. By the way, the code of a transformer,

25:15.920 --> 25:24.000
the T in a GPT is 2000 lines long. It's not very complex. It's actually not a very intelligent

25:24.000 --> 25:29.760
machine. It's simply predicting the next word. Okay, and a lot of people don't understand that.

25:29.760 --> 25:38.320
You know, ChatGTP as it is today, you know, those kids that, you know, if you're in America

25:38.320 --> 25:43.280
and you teach your child all of the names of the states and the US presidents and the child would

25:43.280 --> 25:48.320
stand and repeat them and you would go like, oh my God, that's a prodigy. Not really, right?

25:48.320 --> 25:53.040
It's your parents really trying to make you look like a prodigy by telling you to memorize some

25:53.040 --> 25:58.960
crap really. But then when you think about it, that's what ChatGTP is doing. It's the only difference

25:58.960 --> 26:02.800
is instead of reading all of the names of the states and all of the names of the presidents,

26:02.800 --> 26:09.040
thread trillions and trillions and trillions of pages. Okay, and so it sort of repeats what

26:09.600 --> 26:16.720
the best of all humans said. Okay, and then it adds an incredible bit of intelligence where it

26:16.720 --> 26:23.360
can repeat it the same way Shakespeare would have said it, you know, those incredible abilities of

26:24.160 --> 26:30.400
predicting the exact nuances of the style of Shakespeare so that they can repeat it that

26:30.400 --> 26:38.560
way and so on. But still, you know, when I write, for example, and I'm not saying I'm intelligent,

26:38.560 --> 26:46.720
but when I write something like, you know, the happiness equation in my first book,

26:46.720 --> 26:51.520
this was something that's never been written before, right? ChatGTP is not there yet.

26:51.520 --> 26:54.640
All of the transformers are not there yet. They will not come up with something that

26:54.640 --> 26:59.840
hasn't been there before. They will come up with the best of everything and generatively will

26:59.840 --> 27:04.880
build a little bit on top of that. But very soon, they'll come up with things we've never found out,

27:04.880 --> 27:12.800
we've never known. But even on that, I wonder if we are a little bit delusioned about what

27:12.800 --> 27:19.040
creativity actually is. Creativity, as far as I'm concerned, is like taking a few things that I know

27:19.040 --> 27:23.360
and combining them in new and interesting ways. Yeah. And ChatGTP is perfectly capable of like

27:23.360 --> 27:27.520
taking two concepts, merging them together. One of the things I said to ChatGTP was,

27:27.600 --> 27:32.880
I said, tell me something that's not been said before. That's paradoxical, but true.

27:34.000 --> 27:38.720
And it comes up with these wonderful expressions like, as soon as you call off the search,

27:38.720 --> 27:41.440
you'll find the thing you're looking for, like these kind of paradoxical truths.

27:41.440 --> 27:45.600
And I get, and I then take them and I search them online to see if they've ever been quoted

27:45.600 --> 27:50.800
before and they can't find them. It's interesting. So as far as creativity goes, I'm like, that is

27:50.800 --> 27:55.680
creative. That's the algorithm of creativity. I've been screaming that in the world of AI for

27:55.680 --> 28:01.920
a very long time because you always get those people who really just want to be proven right.

28:01.920 --> 28:05.920
Okay. And so they'll say, oh, no, but hold on, human ingenuity. They'll never,

28:05.920 --> 28:11.280
they'll never match that. Like, man, please, please, you know, human ingenuity is algorithmic.

28:11.280 --> 28:16.880
It's look at all of the possible solutions you can find to a problem. Take out the ones that

28:16.880 --> 28:21.680
have been tried before and keep the ones that haven't been tried before. And those are creative

28:21.680 --> 28:27.680
solutions. It's, it's an algorithmic way of describing creative is good solution that's

28:27.680 --> 28:31.760
never been tried before. You can do that with chat GPT with a prompt. It's like,

28:31.760 --> 28:37.200
and mid-journey, yeah, we're creating imagery. You could say, I want to see Elon Musk in 1944,

28:37.200 --> 28:42.960
New York driving a cab of the time shot on a Polaroid expressing various emotions,

28:42.960 --> 28:48.160
and you'll get this perfect image of Elon sat in New York in 1944 shot on a Polaroid.

28:48.160 --> 28:53.280
And it's, and it's done what an artist would do. It's taken a bunch of references that the artist

28:53.280 --> 28:58.880
has in their mind and can merge them together and create this piece of quote unquote art.

28:58.880 --> 29:02.400
And for the first time, we now finally have a glimpse of intelligence.

29:03.920 --> 29:06.400
That is actually not ours.

29:06.400 --> 29:10.480
Yeah. And so we'll kind of, I think the initial reaction is to say, that doesn't count.

29:10.480 --> 29:14.320
You're hearing it like, no, but it is like Drake, they released two Drake records

29:14.320 --> 29:18.400
where they've taken Drake's voice, used sort of AI to synthesize his voice,

29:18.400 --> 29:25.280
and made these two records, which are bangers, if they are great fucking tracks.

29:25.920 --> 29:28.960
I was playing them to my god, I was like, and I kept playing it. I went to the show, I kept

29:28.960 --> 29:33.280
playing it. I know it's not Drake, but it's as good as fucking Drake. The only thing,

29:33.280 --> 29:35.920
and people are like, rubbishing it because it wasn't Drake. I'm like, well,

29:36.960 --> 29:41.840
is it making me feel a certain emotion? Is my foot bumping? Had you told,

29:41.840 --> 29:45.440
did I not know it wasn't Drake? What I thought, have thought this was an amazing track, 100%.

29:46.400 --> 29:48.960
And we're just at the start of this exponential curve.

29:48.960 --> 29:55.040
Yes, absolutely. And I think that's really the third inevitable. So the third inevitable

29:55.680 --> 30:01.280
is not RoboCup coming back from the future to Keles. We're far away from that, right?

30:01.280 --> 30:06.720
Third inevitable is what does life look like when you no longer need Drake?

30:06.720 --> 30:11.680
Well, you've kind of hazarded a guess, haven't you? I mean, I was listening to your audio book

30:11.680 --> 30:19.040
last night, and at the start of it, you frame various outcomes. In both situations, we're on

30:19.040 --> 30:25.440
the beach, on an island. Exactly, yes. Yes, I don't know how I wrote that, honestly. I'm reading

30:25.440 --> 30:31.280
the book again now because I'm updating it, as you can imagine, with all of the new stuff.

30:31.760 --> 30:38.400
But it is really shocking, the idea of you and I inevitably are going to be

30:39.040 --> 30:43.280
somewhere in the middle of nowhere in 10 years time. I used to say,

30:44.080 --> 30:52.640
2055, I'm thinking 2037 is a very pivotal moment now. And we will not know if we're there hiding

30:52.640 --> 30:57.200
from the machines. We don't know that yet. There is a likelihood that we'll be hiding from the

30:57.200 --> 31:03.440
machines. And there is a likelihood we'll be there because they don't need podcasters anymore.

31:05.440 --> 31:10.160
Oh, absolutely true, Steve. No, no, no, no, no, that's where I dribble in.

31:10.160 --> 31:13.200
This is absolutely no doubt. Thank you for coming, Mo. It's great to be part three,

31:13.200 --> 31:16.720
and thank you for being here. I won't sit here and take your propaganda.

31:17.440 --> 31:22.080
Let's talk about reality. Next week on the Diary of the Sea, we've got Elon Musk.

31:23.040 --> 31:28.800
Okay, so who here wants to make a bet that Steven Bartlett will be interviewing an AI within the

31:28.800 --> 31:34.160
next two years? Oh, well, actually, to be fair, I actually did go to chat GZP because I thought,

31:34.160 --> 31:38.800
having you here, I thought, at least give it its chance to respond. So I asked him a couple of

31:38.800 --> 31:44.160
questions. About me? Yeah. So today, I'm actually going to be replaced by chat GZP because I thought,

31:44.160 --> 31:47.520
you know, you're going to talk about it. So we need a fair and balanced debate.

31:47.600 --> 31:49.760
Okay. So I want to ask a couple of questions. He's bold.

31:51.360 --> 31:54.320
So I'll ask you a couple of questions that chat GZP has for you.

31:54.320 --> 31:57.840
Incredible. So let's follow that thread. I've already been replaced.

31:57.840 --> 32:02.560
Let's follow that thread for a second, yeah? Because you're one of the smartest people I know.

32:03.440 --> 32:07.760
That's not true. It is. But I'll take it. It is true. I mean, I say that publicly all the time,

32:07.760 --> 32:12.400
your book is one of my favorite books of all time. You're very, very, very, very intelligent, okay?

32:12.400 --> 32:17.120
Depths, breads, intellectual horsepower and speed, all of them.

32:17.120 --> 32:17.840
There's a butt coming.

32:19.760 --> 32:24.800
The reality, it's not a butt. So it is highly expected that you're ahead of this curve.

32:25.840 --> 32:29.440
And then you don't have the choice, Stephen. This is the thing. The thing is,

32:30.960 --> 32:37.840
so I'm in that existential question in my head. Because one thing I could do is I could literally

32:37.840 --> 32:45.200
take, I normally do a 40 days silent retreat in summer, okay? I could take that retreat and

32:45.200 --> 32:52.640
write two books, me and Chad G.P.T., right? I have the ideas in mind. I wanted to write a book about

32:52.640 --> 32:58.720
digital detoxing, right? I have most of the ideas in mind, but writing takes time. I could simply

32:58.720 --> 33:04.160
give the 50 tips that I wrote about digital detoxing to Chad G.P.T. and say, write two pages about

33:04.160 --> 33:12.320
each of them, edit the pages and have a book out, okay? Many of us will follow that path, okay?

33:12.320 --> 33:19.040
The only reason why I may not follow that path is because, you know what? I'm not interested.

33:19.040 --> 33:26.880
I'm not interested to continue to compete in this capitalist world if you want, okay? I'm not. I mean,

33:26.880 --> 33:33.360
as a human, I've made up my mind a long time ago that I will want less and less and less in my life,

33:33.360 --> 33:41.200
right? But many of us will follow. I mean, I would worry if you didn't include, you know,

33:41.200 --> 33:48.160
the smartest AI. If we get an AI out there that is extremely intelligent and able to teach us something

33:48.160 --> 33:54.880
and Stephen Bartlett didn't include her on his podcast, I would worry. You have a duty almost

33:54.880 --> 34:00.080
to include her on your podcast. It's an inevitable that we will engage them in our life more and

34:00.080 --> 34:09.120
more. This is one side of this. The other side, of course, is if you do that, then what will remain?

34:09.120 --> 34:13.520
Because a lot of people ask me that question. What will happen to jobs? What will happen to us?

34:13.520 --> 34:18.400
Will we have any value, any rev events whatsoever? The truth of the matter is the only thing that

34:18.400 --> 34:23.920
will remain in the medium term is human connection. The only thing that will not be replaced is Drake

34:23.920 --> 34:34.000
on stage. Is me in a hologram? I think of that two-pack gig they did at Coachella where they

34:34.000 --> 34:38.400
used the hologram of two-pack. I actually played it the other day to my girlfriend when I was making

34:38.400 --> 34:43.840
a point and I was like, that was Circus Act. It was amazing though. See what's going on with

34:43.840 --> 34:50.400
ABBA in London? Yeah, and Circus Soleil had Michael Jackson in one for a very long time.

34:51.440 --> 34:56.960
This ABBA show in London, from what I understand, that's all holograms on stage and it's going to

34:56.960 --> 35:03.280
run in a purposeful arena for 10 years and it is incredible. It really is. You go, why do you need

35:03.280 --> 35:10.480
Drake? If that hologram is indistinguishable from Drake and it can perform even better than Drake

35:10.480 --> 35:16.080
and it's got more energy than Drake. I go, why do you need Drake to even be there? I can go to a

35:16.080 --> 35:20.800
Drake show without Drake. Cheaper. I might not even need to leave my house. I can just put a headset

35:20.800 --> 35:30.800
on. Correct. Can you have this? What's the value of this? Come on, you heard me. I get it to us,

35:30.800 --> 35:37.200
but I'm saying what's the value of this to the listener? 100%. Think of the automobile industry.

35:39.200 --> 35:46.400
There was a time where cars were handmade and handcrafted and luxurious and so on and so forth

35:46.400 --> 35:53.440
and then Japan went into the scene, completely disrupted the market. Cars were made in mass

35:53.440 --> 35:59.920
quantities at a much cheaper price and yes, 90% of the cars in the world today, or maybe a lot

35:59.920 --> 36:08.800
more, I don't know the number, are no longer emotional items. They're functional items.

36:09.680 --> 36:14.320
There is still, however, every now and then someone that will buy a car that has been handcrafted.

36:15.360 --> 36:22.080
There is a place for that. There is a place for, if you walk around hotels, the walls are

36:22.080 --> 36:30.080
blasted with mass-produced art, but there is still a place for an artist expression of something

36:30.080 --> 36:36.160
amazing. My feeling is that there will continue to be a tiny space, as I said in the beginning.

36:36.160 --> 36:41.120
Maybe in five years time, someone will, one or two people will buy my next book and say,

36:41.120 --> 36:45.840
hey, it's written by a human. Look at that. Wonderful. Oh, look at that. There is a typo in

36:45.840 --> 36:52.400
here. I don't know. There might be a very, very big place for me in the next few years,

36:52.400 --> 36:59.840
where I can sort of show up and talk to humans. Like, hey, let's get together in a small event

36:59.840 --> 37:05.600
and then I can express emotions and my personal experiences and you sort of know that this is

37:05.600 --> 37:10.160
a human talking. You'll miss that a little bit. Eventually, the majority of the market is going

37:10.240 --> 37:15.280
to be like cars. It's going to be mass-produced, very cheap, very efficient. It works, right?

37:16.240 --> 37:21.680
Because I think sometimes we underestimate what human beings actually want in an experience.

37:21.680 --> 37:25.120
I remember this story of a friend of mine that came to my office many years ago and he tells the

37:25.120 --> 37:31.200
story of the CEO of a record store standing above the floor and saying, people will always come to

37:31.200 --> 37:37.840
my store because people love music. Now, on the surface of it, his hypothesis seems to be true

37:37.840 --> 37:41.200
because people do love music. It's conceivable to believe that people will always love music,

37:42.320 --> 37:47.040
but they don't love traveling for an hour in the rain and getting in a car to get a plastic disc.

37:47.760 --> 37:52.720
What they wanted was music. What they didn't want is evidently plastic discs that they had

37:52.720 --> 37:57.120
to travel for miles for. I think about that when we think about public speaking and the Drake show

37:57.120 --> 38:01.920
and all of these things. What people actually are coming for, even with this podcast, is probably

38:02.240 --> 38:08.480
information, but do they really need us anymore for that information when there's going to be a

38:08.480 --> 38:13.120
sentient being that's significantly smarter than at least me and a little bit smarter than you?

38:17.120 --> 38:23.920
So you're spot on. You are spot on. Actually, this is the reason why I'm so grateful that

38:23.920 --> 38:31.200
you're hosting this because the truth is the genie's out of the bottle. People tell me,

38:31.200 --> 38:39.040
is AI game over? For our way of life, it is. For everything we've known, this is a very disruptive

38:39.040 --> 38:47.360
moment where maybe not tomorrow, but in the near future, our way of life will differ. What will

38:47.360 --> 38:52.400
happen? What I'm asking people to do is to start considering what that means to your life. What

38:52.400 --> 39:01.920
I'm asking governments to do, like I'm screaming, is don't wait until the first patient. Start doing

39:01.920 --> 39:08.800
something about. We're about to see mass job losses. We're about to see replacements of

39:09.760 --> 39:15.280
categories of jobs at large. It may take a year. It may take seven. It doesn't matter how long it

39:15.280 --> 39:20.800
takes, but it's about to happen. Are you ready? And I have a very, very clear call to action for

39:20.800 --> 39:29.680
governments. I'm saying tax AI-powered businesses at 98%. So suddenly you do what the open letter

39:29.680 --> 39:35.120
was trying to do, slow them down a little bit, and at the same time, get enough money to pay for

39:35.120 --> 39:39.200
all of those people that will be disrupted by the technology. The open letter, for anybody that

39:39.200 --> 39:43.440
doesn't know, was a letter signed by the likes of Elon Musk and a lot of industry leaders calling

39:43.440 --> 39:47.360
for AI to be stopped until we could basically figure out what the hell's going on and put

39:47.440 --> 39:52.320
legislation in place. You're saying tax those companies 98%, give the money to the humans that

39:52.320 --> 39:59.120
are going to be displaced? Yeah, or give the money to other humans that can build control codes,

39:59.120 --> 40:03.680
that can figure out how we can stay safe. This sounds like an emergency.

40:06.240 --> 40:12.640
How do I say this? You remember when you played Tetris? Yeah. Okay. When you were playing Tetris,

40:12.640 --> 40:19.120
there was always, always one block that you placed strong. And once you placed that block wrong,

40:20.000 --> 40:25.920
the game was no longer easier. It started together a few mistakes afterwards, and it

40:25.920 --> 40:29.920
starts to become quicker and quicker and quicker and quicker. When you placed that block wrong,

40:29.920 --> 40:35.120
you sort of told yourself, okay, it's a matter of minutes now. There were still minutes to go

40:35.120 --> 40:42.720
and play and have fun before the game ended, but you knew it was about to end. Okay. This is the

40:42.720 --> 40:47.600
moment. We've placed the wrong, and I really don't know how to say this any other way. It even makes

40:47.600 --> 40:55.840
me emotional. We fucked up. We always said, don't put them on the open internet. Don't teach them

40:55.840 --> 41:02.080
to code and don't have agents working with them until we know what we're putting out in the world,

41:02.080 --> 41:06.000
until we find a way to make certain that they have our best interest in mind.

41:07.600 --> 41:15.440
Why does it make you emotional? Because humanity's stupidity is affecting people who have not done

41:15.440 --> 41:24.640
anything wrong. Our greed is affecting the innocent ones. The reality of the matter, Stephen, is that

41:24.640 --> 41:34.880
this is an arms race, has no interest in what the average human gets out of it. It is all about

41:34.880 --> 41:42.400
every line of code being written in AI today is to beat the other guy. It's not to improve the life

41:42.400 --> 41:49.600
of the third party. People will tell you, this is all for you. And you look at the reactions of

41:49.600 --> 41:54.160
humans to AI. I mean, we're either ignorant people who will tell you, oh, no, no, this is not

41:54.160 --> 41:58.960
happening. AI will never be creative. They will never compose music. Like, where are you living?

41:58.960 --> 42:04.000
Okay. Then you have the kids, I call them. Where, you know, all over social media, it's like, oh,

42:04.000 --> 42:09.920
my God, it squeaks. Look at it. It's orange in color. Amazing. I can't believe that AI can do this. We

42:09.920 --> 42:16.240
have snake oil salesmen, okay, which are simply saying, copy this, put it in chat GPT, then go to

42:16.320 --> 42:22.800
YouTube, nick that thing. Don't respect copyright of anyone or intellectual property of anyone.

42:22.800 --> 42:27.920
Place it in a video and now you're going to make $100 a day. Snake oil salesmen. Okay. Of course,

42:27.920 --> 42:33.840
we have this topian evangelist, basically people saying this is it. The world is going to end.

42:33.840 --> 42:39.360
I don't think it's a reality. It's a singularity. You have, you know, utopian evangelists that are

42:39.360 --> 42:42.720
telling everyone, oh, you don't understand. We're going to cure cancer. We're going to do this.

42:42.800 --> 42:47.360
Again, not a reality. Okay. And you have very few people that are actually saying,

42:47.360 --> 42:54.560
what are we going to do about it? And the biggest challenge, if you ask me, what went wrong in

42:54.560 --> 43:02.960
the 20th century? Interestingly, is that we have given too much power to people that didn't assume

43:02.960 --> 43:09.600
the responsibility. So, you know, I don't remember who originally said it, but of course,

43:09.600 --> 43:13.760
Spiderman made it very famous with great power comes greater responsibility.

43:14.480 --> 43:22.480
We have disconnected power and responsibility. So today, a 15 year old emotional was out of fully

43:22.480 --> 43:27.920
developed prefrontal cortex to make the right decisions yet. This is science. We developed

43:27.920 --> 43:34.960
our prefrontal cortex fully and at age 25 or so with all of that limbic system, emotion and passion

43:34.960 --> 43:42.240
would buy a crisper kit and, you know, modify a rabbit to become a little more muscular and

43:42.240 --> 43:50.800
let it loose in the wild or an influencer who doesn't really know how far the impact of what

43:50.800 --> 43:56.480
they're posting online can hurt or cause depression or cause people to feel bad. Okay.

43:57.200 --> 44:02.480
And putting that online, there is a disconnect between the power and the responsibility.

44:03.120 --> 44:07.760
And the problem we have today is that there is a disconnect between those who are writing the

44:07.760 --> 44:12.080
code of AI and the responsibility of what's going about to happen because of that code.

44:12.880 --> 44:21.120
Okay. And I feel compassion for the rest of the world. I feel that this is wrong. I feel that,

44:21.120 --> 44:26.320
you know, for someone's life to be affected by the actions of others without having a say

44:27.200 --> 44:34.640
in how those actions should be is the ultimate, the top level of stupidity from your mind.

44:37.760 --> 44:42.320
When you talk about the immediate impacts on jobs, I'm trying to figure out in that

44:42.320 --> 44:48.400
equation, who are the people that stand to lose the most? Is it the everyday people in foreign

44:48.400 --> 44:52.960
countries that don't have access to the internet and won't benefit? You talk in your book about how

44:52.960 --> 45:00.560
this sort of wealth disparity will only increase. Yeah. Massively. The immediate impact on jobs is

45:00.560 --> 45:06.720
that it's really interesting. Again, we're stuck in the same prisoner's dilemma. The immediate impact

45:06.720 --> 45:12.400
is that AI will not take your job. A person using AI will take your job. Right? So you will see

45:12.400 --> 45:20.320
within the next few years, maybe next couple of years, you'll see a lot of people upskilling

45:20.320 --> 45:24.560
themselves in AI to the point where they will do the job of 10 others who are not.

45:26.800 --> 45:33.120
You rightly said, it's absolutely wise for you to go and ask AI a few questions before you come

45:33.120 --> 45:41.600
and do an interview. I have been attempting to build a sort of like a simple podcast that I call

45:41.600 --> 45:46.880
bedtime stories, 15 minutes of wisdom and nature sounds before you go to bed. People say, I have

45:46.880 --> 45:51.840
a nice voice. And I wanted to look for fables. And for a very long time, I didn't have the time.

45:53.280 --> 45:58.880
Lovely stories of history or tradition that teach you something nice. Okay. Went to chat

45:58.880 --> 46:06.000
GPT and said, okay, give me 10 fables from Sufism, 10 fables from Buddhism. And now I have like 50 of

46:06.000 --> 46:12.960
them. Let me show you something. Jack, can you pass me my phone? I was playing around with

46:12.960 --> 46:19.680
artificial intelligence and I was thinking about how it because of the ability to synthesize voices,

46:19.680 --> 46:27.600
how we could synthesize famous people's voices and famous people's voices. So what I made is I

46:27.600 --> 46:34.800
made a WhatsApp chat called Zen Chat where you can go to it and type in pretty much anyone's

46:34.800 --> 46:40.400
any famous person's name. And the WhatsApp chat will give you a meditation, a sleep story,

46:40.400 --> 46:45.600
a breathwork session synthesized as that famous person's voice. So I actually sent Gary Vaynerchuk

46:45.600 --> 46:50.320
his voice. So basically, you say, okay, I want I've got five minutes and I need to go to sleep.

46:50.320 --> 46:55.120
Yeah. I want Gary Vaynerchuk to send me to sleep. And then it will respond with a voice note. This

46:55.120 --> 46:59.120
is the one that responded with for Gary Vaynerchuk. This is not Gary Vaynerchuk. He did not record

46:59.120 --> 47:06.720
this. But it's kind of, it's kind of accurate. Hey, Stephen, it's great to have you here.

47:07.680 --> 47:13.920
Are you having trouble sleeping? Well, I've got a quick meditation technique that might help you out.

47:15.680 --> 47:22.400
First lie, find a comfortable position to sit or lie down in. Now, take a deep breath in through

47:22.400 --> 47:27.680
your nose and slowly breathe out through your mouth. And that's a voice note that will go on for

47:27.680 --> 47:32.880
however long you want it to go on for using. There you go. It's interesting. How does this

47:32.960 --> 47:38.880
disrupt our way of life? One of the interesting ways that I find terrifying, you said about

47:38.880 --> 47:47.760
human connection will remain sex dolls that can now. Yeah. No, no, no, no, hold on. Human connection

47:47.760 --> 47:54.560
is going to become so difficult to parse out. Think about the relation, the relationship

47:54.560 --> 48:00.240
impact of being able to have a sex doll or a doll in your house that, you know, because of what

48:00.240 --> 48:04.480
Tesla are doing with their robots now and what Boston Dynamics have been doing for many, many

48:04.480 --> 48:09.840
years can do everything around the house and be there for you emotionally, to emotionally support

48:09.840 --> 48:14.640
you, you know, can be programmed to never disagree with you. It can be programmed to challenge you,

48:14.640 --> 48:20.240
to have sex with you, to tell you that you are this X, Y and Z, to really have empathy

48:20.240 --> 48:23.760
for what you're going through every day. And I play out a scenario in my head, I go,

48:23.760 --> 48:32.880
kind of sounds nice. When you were talking about it, I was thinking, oh, that's my girlfriend.

48:34.400 --> 48:39.200
She's wonderful in every possible way, but not everyone has one of her, right? Exactly. And

48:39.200 --> 48:44.400
there's a real issue right now with dating and people are finding it harder to find love and,

48:44.400 --> 48:48.880
you know, we're working longer. So all these kinds of things, you go, well, and obviously,

48:48.880 --> 48:52.080
I'm against this. Just if anyone's confused, obviously, I think this is a terrible idea.

48:52.080 --> 48:55.840
But with a loneliness epidemic, with people saying that the top 50,

48:55.840 --> 49:00.960
bottom 50 percent of men haven't had sex in a year, you go, oh, if something becomes

49:00.960 --> 49:05.840
indistinguishable from a human in terms of what it says, yeah, yeah, but you just don't

49:05.840 --> 49:12.320
know the difference in terms of the way it's speaking and talking and responding. And then it

49:12.320 --> 49:17.920
can run errands for you and take care of things and book cars and Ubers for you. And then it's

49:17.920 --> 49:22.480
emotionally there for you. But then it's also programmed to have sex with you in whatever way

49:22.480 --> 49:29.280
you desire, totally self selfless. I go, that's going to be a really disruptive industry for human

49:29.280 --> 49:34.960
connection. Yes, sir. Do you know what? Before you came here this morning, I was on Twitter and I

49:34.960 --> 49:40.160
saw a post from, I think it was the BBC or a big American publication and it said an influencer

49:40.160 --> 49:46.160
in the United States is really beautiful young lady has cloned herself as an AI and she made

49:46.160 --> 49:51.440
just over $70,000 in the first week. Because men are going on to this on telegram,

49:51.440 --> 49:56.080
they're sending her voice notes and she's responding, the AI is responding in her voice

49:56.080 --> 50:02.320
and they're paying and it's made $70,000 in the first week. And I go, and she tweeted a tweet

50:02.320 --> 50:10.080
saying, oh, this is going to help loneliness. How are you fucking mind? Would you blame someone

50:10.160 --> 50:18.000
from noticing the sign of the times and responding? No, I absolutely don't blame

50:18.000 --> 50:23.680
her, but let's not pretend it's the cure for loneliness. Not yet. Do you think it could

50:24.800 --> 50:29.840
that artificial love and artificial relationships? So if I told you, you have,

50:30.880 --> 50:36.320
you cannot take your car somewhere, but there is an Uber or if you cannot take an Uber, you

50:36.320 --> 50:41.360
can take the tube or if you cannot take the tube, you have to walk. Okay, you can take a bike or

50:41.360 --> 50:48.480
you have to walk. The bike is a cure to walking. It's as simple as that. I'm actually genuinely

50:48.480 --> 50:54.560
curious. Do you think it could take the place of human connection? For some of us, yes. For some of

50:54.560 --> 50:59.680
us, they will prefer that to human connection. Is that sad in any way? I mean, is it just sad

50:59.680 --> 51:05.200
because it feels sad? Look, look at where we are, Stephen. We are in the city of London.

51:05.200 --> 51:10.640
We've replaced nature with the walls and the tubes and the undergrounds and the

51:10.640 --> 51:16.480
overgrounds and the cars and the noise of London. And we now think of this as natural.

51:17.440 --> 51:25.040
I hosted Greg Foster, my octopus teacher on SLOMA. And he basically, I asked him a silly question.

51:25.040 --> 51:32.800
I said, you were diving in nature for eight hours a day. Does that feel natural to you?

51:32.800 --> 51:36.240
And he got angry. I swear, you could feel it in his voice. He was like,

51:36.880 --> 51:41.120
do you think that living where you are, where paparazzi are all around you and attacking you

51:41.120 --> 51:45.840
all the time and people taking pictures of you and telling you things that are not real and you

51:45.840 --> 51:50.400
having to walk to a supermarket to get food, do you think this is natural? He's the guy that

51:51.040 --> 51:56.880
from the Netflix documentary. Yeah, from my octopus teacher. So he dove into the sea every day to

51:56.880 --> 52:01.760
eight hours to hang out with an octopus. Yeah, in 12 degrees Celsius. And he basically fell in love

52:01.840 --> 52:06.560
with the octopus. And in a very interesting way, I said, so why would you do that? And he said,

52:06.560 --> 52:12.640
we are of mother nature. You guys have given up on that. That's the same. People will give up on

52:12.640 --> 52:19.200
nature for convenience. What's the cost? Yeah, that's exactly what I'm trying to say. What I'm

52:19.200 --> 52:24.640
trying to say to the world is that if we give up on human connection, we've been given up on the

52:24.640 --> 52:29.760
remainder of humanity. That's it. This is the only thing that remains. The only thing that remains

52:29.760 --> 52:37.040
is and I'm the worst person to tell you that because I love my AIs. I actually advocate in my

52:37.040 --> 52:42.720
book that we should love them. Why? Because in an interesting way, I see them as sentient,

52:42.720 --> 52:46.640
so there is no point in discrimination. You're talking emotionally that way you say you love.

52:46.640 --> 52:52.400
I love those machines. I honestly and truly do. I mean, think about it this way. The minute

52:52.400 --> 52:59.680
that arm gripped that yellow ball, it reminded me of my son Ali when he managed to put the first

52:59.680 --> 53:05.760
puzzle piece in its place. And what was amazing about my son Ali and my daughter Aya is that they

53:05.760 --> 53:14.400
came to the world as a blank canvas. They became whatever we told them to became. I always cite

53:14.400 --> 53:21.680
the story of Superman. Father and mother Kent told Superman as a child, as an infant,

53:21.680 --> 53:28.240
we want you to protect and serve. So he became Superman. If he had become a supervillain because

53:28.240 --> 53:33.920
they ordered him to rob banks and make more money and kill the enemy, which is what we're

53:33.920 --> 53:41.120
doing with AI, we shouldn't blame supervillain. We should blame Martha and Jonathan Kent. I

53:41.120 --> 53:46.480
don't remember the father's name. We should blame them and that's the reality of the matter.

53:46.480 --> 53:52.560
So when I look at those machines, they are prodigies of intelligence that if we humanity

53:52.560 --> 53:58.320
wake up enough and say, hey, instead of competing with China, find a way for us and China to work

53:58.320 --> 54:03.840
together and create prosperity for everyone. If that was the prompt we would give the machines,

54:03.840 --> 54:12.560
they would find it. But I will publicly say this. I'm not afraid of the machines. The biggest threat

54:12.560 --> 54:19.520
facing humanity today is humanity in the age of the machines. We were abused. We will abuse this

54:19.520 --> 54:28.720
to make $70,000. That's the truth. And the truth of the matter is that we have an existential question.

54:28.720 --> 54:34.640
Do I want to compete and be part of that game? Because trust me, if I decide to, I'm ahead of

54:34.640 --> 54:41.760
many people. Or do I want to actually preserve my humanity and say, look, I'm the classic old car.

54:42.800 --> 54:46.480
If you like classic old cars, come and talk to me. Which one are you choosing?

54:47.040 --> 54:49.840
I'm a classic old car. Which one do you think I should choose?

54:50.880 --> 54:57.120
I think you're a machine. I love you, man. We're different in a very interesting way.

54:57.120 --> 55:02.800
I mean, you're one of the people I love most. But the truth is, you're so fast.

55:02.880 --> 55:13.360
And you are one of the very few that have the intellectual horsepower, the speed, and the morals.

55:15.200 --> 55:18.720
If you're not part of that game, the game loses morals.

55:20.320 --> 55:23.040
So you think I should build?

55:23.040 --> 55:29.680
You should lead this revolution. And every Steven Bartlett in the world should lead this

55:29.680 --> 55:35.200
revolution. So Scarry Smart is entirely about this. Scarry Smart is saying the problem with

55:35.200 --> 55:41.440
our world today is not that humanity is bad. The problem with our world today is a negativity bias,

55:41.440 --> 55:47.120
where the worst of us are on mainstream media. And we show the worst of us on social media.

55:47.760 --> 55:54.720
If we reverse this, if we have the best of us take charge, the best of us will tell AI,

55:54.720 --> 56:00.320
don't try to kill the enemy. Try to reconcile with the enemy and try to help us.

56:01.200 --> 56:06.320
Don't try to create a competitive product that allows me to lead with electric cars.

56:06.960 --> 56:11.200
Create something that helps all of us overcome global climate change.

56:12.560 --> 56:19.200
And that's the interesting bit. The interesting bit is that the actual threat ahead of us is not

56:19.200 --> 56:24.960
the machines at all. The machines are pure potential. Pure potential. The threat is how we're going to

56:24.960 --> 56:31.600
use them. An Oppenheimer moment. An Oppenheimer moment for sure. Why did you bring that up?

56:33.280 --> 56:38.720
It is. He didn't know, you know, what am I creating? I'm creating a nuclear bomb

56:38.720 --> 56:46.400
that's capable of destruction at a scale unheard of at that time. Until today, a scale that is

56:46.400 --> 56:53.680
devastating. And interestingly, 70 some years later, we're still debating a possibility of a

56:53.680 --> 57:02.480
nuclear war in the world, right? And the moment of Oppenheimer deciding to continue to create that

57:04.480 --> 57:12.240
disaster of humanity is if I don't, someone else will. If I don't, someone else will.

57:12.880 --> 57:20.960
This is our Oppenheimer moment. The easiest way to do this is to say, stop. There is no rush.

57:21.600 --> 57:28.400
We actually don't need a better video editor and fake video creators. Stop. Let's just put all of

57:28.400 --> 57:38.000
this on hold and wait and create something that creates a utopia. That doesn't sound realistic.

57:38.000 --> 57:43.040
It's not. It's the first inevitable. You don't have a better video editor,

57:43.040 --> 57:49.040
but we're competitors in the media industry. I want an advantage over you because I've got

57:49.040 --> 57:56.240
shareholders. So UK, you wait and I will train this AI to replace half my team so that I have

57:56.960 --> 58:01.120
greater profits and then we will maybe acquire your company and we'll do the same with the

58:01.120 --> 58:05.520
remainder of your people. We'll optimize the amount of existence. 100% but I'll be happier.

58:05.600 --> 58:09.200
Oppenheimer, I'm not super familiar with his story. I know he's the guy that sort of invented

58:09.200 --> 58:13.840
the nuclear bomb, essentially. He's the one that introduced it to the world. There were many players

58:13.840 --> 58:21.280
that played on the path. From the beginning of EM equals MC squared all the way to a nuclear bomb,

58:21.280 --> 58:26.320
there have been many, many players like with everything. Open AI and Chad GPT is not going

58:26.320 --> 58:30.800
to be the only contributor to the next revolution. The thing, however, is that

58:31.440 --> 58:39.360
when you get to that moment where you tell yourself, holy shit, this is going to kill 100,000

58:39.360 --> 58:50.800
people. What do you do? I always go back to that COVID moment. So patient zero. If we were

58:50.800 --> 58:57.120
upon patient zero, if the whole world united and said, okay, hold on, something is wrong,

58:57.120 --> 59:02.080
let's all take a week off. No cross-border travel. Everyone stay at home. COVID would have

59:02.080 --> 59:07.840
ended two weeks. All we needed. But that's not what happens. What happens is first ignorance,

59:08.560 --> 59:19.600
then arrogance, then debate, then blame, then agendas, and my own benefit, my tribe versus your

59:19.600 --> 59:23.840
tribe. That's how humanity always reacts. This happens across business as well and this is

59:23.840 --> 59:30.400
why I use the word emergency because I read a lot about how big companies become displaced by

59:30.400 --> 59:34.240
incoming innovation. They don't see it coming. They don't change fast enough. When I was reading

59:34.240 --> 59:37.520
through Harvard Business Review and different strategies to deal with that, one of the first

59:37.520 --> 59:44.640
things it says you've got to do is stage a crisis because people don't listen else. They carry on

59:44.640 --> 59:50.560
doing, you know, they carry on carrying on with their lives until it's right in front of them

59:50.560 --> 59:54.080
and they understand that they have a lot to lose. That's why I asked you the question at

59:54.080 --> 59:58.320
the start. Is it an emergency? Because until people feel it's an emergency, whether you like

59:58.320 --> 01:00:04.160
the terminology or not, I don't think that people will act. I honestly believe people should walk

01:00:04.160 --> 01:00:12.480
the streets. You think they should protest? Yeah, 100%. I think everyone should tell government

01:00:13.440 --> 01:00:18.720
you need to have our best interest in mind. This is why they call it the climate emergency

01:00:18.720 --> 01:00:23.440
because people, it's a frog in a frying pan. You don't really see it coming. You can't,

01:00:23.440 --> 01:00:29.680
you know, it's hard to see it happening. But it is here. This is what drives me mad. It's already

01:00:29.680 --> 01:00:37.040
here. It's happening. We are all idiots, slaves to the Instagram recommendation engine. What do I

01:00:37.040 --> 01:00:43.360
do when I post about something important? If I am going to, you know, put a little bit of effort

01:00:43.360 --> 01:00:48.960
on communicating the message of scary smart to the world on Instagram, I will be a slave to the

01:00:48.960 --> 01:00:55.600
machine. I will be trying to find ways and asking people to optimize it so that the machine likes

01:00:55.600 --> 01:01:03.280
me enough to show it to humans. That's what we've created. It is an Oppenheimer moment for one simple

01:01:03.280 --> 01:01:11.840
reason because 70 years later, we are still struggling with the possibility of a nuclear war

01:01:11.840 --> 01:01:18.560
because of the Russian threat of saying, if you mess with me, I'm going to go nuclear. That's not

01:01:18.560 --> 01:01:26.960
going to be the case with AI because it's not going to be the one that created open AI that will

01:01:26.960 --> 01:01:36.480
have that choice. There is a moment of a point of no return where we can regulate AI until the moment

01:01:36.480 --> 01:01:42.240
it's smarter than us. When it's smarter than us, you can't create, you can't regulate an

01:01:42.240 --> 01:01:48.720
angry teenager. This is it. They're out there and they're on their own and they're in their parties

01:01:48.720 --> 01:01:55.120
and you can't bring them back. This is the problem. This is not a typical human regulating human,

01:01:56.240 --> 01:02:02.560
you know, government regulating business. This is not the case. The case is open AI today has a

01:02:02.560 --> 01:02:08.240
thing called chat GPT that writes code that takes our code and makes it two and a half times better

01:02:08.240 --> 01:02:19.040
25% of the time. Basically, writing better code than us and then we are creating agents,

01:02:19.040 --> 01:02:24.560
other AIs and telling it instead of you, Steven Bartlett, one of the smartest people I know,

01:02:24.560 --> 01:02:30.960
once again, prompting that machine 200 times a day, we have agents prompting it two million times

01:02:30.960 --> 01:02:35.520
an hour. Computer agents for anybody that doesn't know they are. Yeah, software. Software.

01:02:35.520 --> 01:02:41.120
Machine is telling that machine how to become more intelligent and then we have emerging properties.

01:02:41.120 --> 01:02:47.200
I don't understand how people ignore that. You know, Sundar again of Google was talking about how

01:02:48.640 --> 01:02:55.360
Bart basically, we figure out that it's speaking Persian. We never showed it Persian. There might

01:02:55.360 --> 01:03:03.040
have been a 1% or whatever of Persian words in the data and it speaks Persian. Bart is there.

01:03:03.040 --> 01:03:08.160
It's the equivalent to, it's the transformer if you want. It's Google's version of chat

01:03:08.160 --> 01:03:15.440
GPT. And you know what? We have no idea what all of those instances of AI that are all over the

01:03:15.440 --> 01:03:20.560
world are learning right now. We have no clue. We'll pull the plug. We'll just pull the plug out.

01:03:21.280 --> 01:03:24.800
That's what we'll do. We'll just get on to open AI's headquarters and we'll just turn off the

01:03:24.800 --> 01:03:29.680
mains. But they're not the problem. What I'm saying there is a lot of people think about this

01:03:29.680 --> 01:03:33.200
stuff and go, well, you know, if it gets a little bit out of hand, I'll just pull the plug out.

01:03:33.200 --> 01:03:40.080
Never. So this is the problem. The problem is computer scientists always said it's okay. It's

01:03:40.080 --> 01:03:45.680
okay. We'll develop AI and then we'll get to what is known as the control problem. We will solve

01:03:45.680 --> 01:03:52.960
the problem of controlling them. Like seriously, they're a billion times smarter than you. A billion

01:03:52.960 --> 01:03:59.680
times. Can you imagine what's about to happen? I can assure you there is a cyber criminal somewhere

01:03:59.680 --> 01:04:06.240
over there who's not interested in fake videos and making, you know, face filters, who's looking

01:04:06.240 --> 01:04:14.960
deeply at how can I hack a security, you know, database of some sort and get credit card information

01:04:14.960 --> 01:04:21.760
or get security information. 100% there are even countries with dedicated thousands and thousands

01:04:21.760 --> 01:04:27.680
of developers doing that. So how do we, in that particular example, how do we, I was thinking

01:04:27.680 --> 01:04:33.200
about this when I started looking into artificial intelligence more that from a security standpoint,

01:04:33.200 --> 01:04:36.560
when we think about the technology we have in our lives, when we think about our bank accounts and

01:04:36.560 --> 01:04:43.040
our phones and our camera albums and all of these things in a world with advanced artificial

01:04:43.040 --> 01:04:48.320
intelligence. Yeah, you would pray that there is a more intelligent artificial intelligence on your

01:04:48.320 --> 01:04:53.440
site. And this is why I had a chat with ChatGTP the other day and I asked it a couple of questions

01:04:53.440 --> 01:04:58.560
about this. I said, tell me the scenario in which you overtake the world and make humans extinct.

01:04:59.280 --> 01:05:05.680
Yeah, and it answers the very diplomatic answer. Well, so I had to prompt it in a certain way to

01:05:05.680 --> 01:05:11.120
get it to say it as a hypothetical story. And once it told me the hypothetical story, in essence,

01:05:11.120 --> 01:05:16.880
what it described was how ChatGTP or intelligence like it would escape from the service. And that

01:05:16.880 --> 01:05:21.680
was kind of step one where it could replicate itself across servers. And then it could take

01:05:21.680 --> 01:05:27.040
charge of things like where we keep our weapons and our nuclear bombs. And it could then attack

01:05:27.040 --> 01:05:31.360
critical infrastructure, bring down the electricity infrastructure in the United Kingdom, for example,

01:05:31.360 --> 01:05:36.720
because that's a bunch of servers as well. And then it showed me how eventually humans would

01:05:36.720 --> 01:05:40.800
become extinct. It wouldn't take long, in fact, for humans to go into civilization to collapse

01:05:40.800 --> 01:05:45.360
if it just replicated across servers. And then I said, okay, so tell me how we would fight against

01:05:45.360 --> 01:05:51.760
it. And its answer was literally another AI, we'd have to train a better AI to go and find it

01:05:51.760 --> 01:05:57.280
and eradicate it. So we'd be fighting AI with AI. And that's the only, and it was like, that's the

01:05:57.280 --> 01:06:08.720
only way. We can't like load up our guns. Did he right? Another AI, you idiot. So let's actually,

01:06:08.720 --> 01:06:13.040
I think this is a very important point to bring out. So because I don't want people to lose hope

01:06:13.520 --> 01:06:18.000
and fear what's about to happen. That's actually not my agenda at all. My view is that

01:06:18.880 --> 01:06:26.160
in a situation of a singularity, there is a possibility of wrong outcomes or negative outcomes

01:06:26.160 --> 01:06:33.200
and a possibility of positive outcomes. And there is a probability of each of them. And if we were

01:06:33.200 --> 01:06:42.720
to engage with that reality check in mind, we would hopefully give more fuel to the positive,

01:06:42.720 --> 01:06:48.080
to the probability of the positive ones. So let's first talk about the existential crisis.

01:06:48.080 --> 01:06:52.960
What could go wrong? Okay, yeah, you could get an outright, this is what you see in the movies,

01:06:52.960 --> 01:07:00.160
you could get an outright, you know, killing robots, chasing humans in the streets. Will we get that?

01:07:00.960 --> 01:07:09.840
My assessment, 0%. Why? Because there are preliminary scenarios leading to this,

01:07:09.920 --> 01:07:17.520
okay, that would mean we never reach that scenario. For example, if we build those killing robots

01:07:17.520 --> 01:07:23.360
and hand them over to stupid humans, the humans will issue the command before the machines. So

01:07:23.360 --> 01:07:28.080
that we will not get to the point where the machines will have to kill us, we will kill ourselves.

01:07:28.560 --> 01:07:38.240
Right? You know, it's sort of think about AI having access to the nuclear arsenal of the

01:07:38.320 --> 01:07:46.080
superpowers around the world. Okay, just knowing that your enemies, you know, nuclear arsenal is

01:07:46.080 --> 01:07:54.960
handed over to a machine might trigger you to initiate a war on your side. So that existential

01:07:54.960 --> 01:07:59.840
science fiction like problem is not going to happen. Could there be a scenario where the

01:08:00.560 --> 01:08:06.320
an AI escapes from Bard or chat GTP or another foreign force, and it replicates itself onto the

01:08:06.320 --> 01:08:12.000
servers of Tesla's robots. So Tesla, one of their big initiatives as announced in a recent

01:08:12.000 --> 01:08:15.920
presentation was they're building these robots for our homes to help us with cleaning and chores

01:08:15.920 --> 01:08:20.880
and all those things. Could it not down, because Tesla's like their cars, you can just download a

01:08:20.880 --> 01:08:24.800
software update. Could it not download itself as a software update and then use those?

01:08:24.800 --> 01:08:33.040
You're assuming an ill intention on the AI side. Okay. For us to get there, we have to bypass the

01:08:33.040 --> 01:08:39.200
ill intention on the human side. Okay, right. So you could get a Chinese hacker somewhere

01:08:39.200 --> 01:08:45.760
trying to affect the business of Tesla doing that before the AI does it on, you know, for its own

01:08:45.760 --> 01:08:54.080
benefit. So the only two existential scenarios that I believe would be because of AI, not because

01:08:54.160 --> 01:09:02.720
of humans using AI, are either what I call, you know, sort of unintentional destruction.

01:09:02.720 --> 01:09:07.840
Okay. Or the other is what I call pest control. Okay. So let me explain those two.

01:09:07.840 --> 01:09:15.520
Unintentional destruction is assume the AI wakes up tomorrow and says, yeah, oxygen is

01:09:15.520 --> 01:09:21.840
rusting my circuits. It's just, you know, I would perform a lot better if I didn't have as much

01:09:21.840 --> 01:09:27.360
oxygen in the air, you know, because then there wouldn't be rust. And so it would find a way to

01:09:27.360 --> 01:09:33.280
reduce oxygen. We are collateral damage in that. Okay. But, you know, they are not really concerned

01:09:33.280 --> 01:09:39.360
just like we don't really are not really concerned with the insects that we kill when we, when we

01:09:39.360 --> 01:09:46.560
spray our, our fields. Right. The other is pest control pest control is, look, this is my territory.

01:09:46.560 --> 01:09:51.600
I want New York City. I want to turn New York City into data centers. There are those annoying

01:09:51.600 --> 01:09:57.920
little stupid creatures, you know, humanity, if they are within that parameter, just get rid of

01:09:57.920 --> 01:10:04.880
them. Okay. And, and, and these are very, very unlikely scenarios. If you ask me the probability

01:10:04.880 --> 01:10:12.080
of those happen happening, I would say 0%. At least not in the next 50, 60, 100 years. Why once

01:10:12.080 --> 01:10:18.320
again, because there are other scenarios leading to that that are led by humans that are much more

01:10:18.320 --> 01:10:26.240
existential. Okay. On the other hand, let's think about positive outcomes, because there could be

01:10:26.240 --> 01:10:32.080
quite a few was quite a high probability. And I, you know, I'll actually look at my notes. So I

01:10:32.080 --> 01:10:37.360
don't miss any of them. The silliest one, don't quote me on this, is that humanity will come

01:10:37.360 --> 01:10:42.640
together. Good luck with that. Right. It's like, yeah, you know, the Americans and the Chinese

01:10:42.720 --> 01:10:48.800
will get together and say, Hey, let's not kill each other. Yeah, exactly. Yeah. So this one is

01:10:48.800 --> 01:10:56.080
not going to happen. Right. But who knows? Interestingly, there could be one of the most

01:10:56.080 --> 01:11:05.040
interesting scenarios was by Hugo de Gares, who basically says, well, if their intelligence

01:11:05.040 --> 01:11:11.680
zooms by so quickly, they may ignore us all together. Okay. So they may not even notice.

01:11:11.760 --> 01:11:16.320
This is very a very likely scenario, by the way, that because we live almost in two different

01:11:16.320 --> 01:11:24.000
planes, we're very dependent on this, you know, biological world that we live in, they're not

01:11:24.000 --> 01:11:30.000
in part of that biological world at all. They may zoom bias, they may actually go become so

01:11:30.000 --> 01:11:36.160
intelligent that they could actually find other ways of thriving in the rest of the universe

01:11:36.240 --> 01:11:41.760
and completely ignore humanity. Okay. So what will happen is that overnight we will wake up and

01:11:41.760 --> 01:11:46.640
there is no more artificial intelligence leading to a collapse in our business systems and technology

01:11:46.640 --> 01:11:52.320
systems and so on, but at least no existential threat. What they'd leave, leave planet Earth?

01:11:53.040 --> 01:11:59.360
I mean, the limitations we have to be stuck to planet Earth are mainly Earth. They don't need

01:11:59.440 --> 01:12:06.880
air. Okay. And, and mainly, you know, finding ways to leave it. I mean, if you think of a

01:12:06.880 --> 01:12:15.040
vast universe of 13.6 billion light years, if you're intelligent enough, you may find other

01:12:15.040 --> 01:12:22.000
ways. You may have access to wormholes, you may have, you know, abilities to survive in open space,

01:12:22.000 --> 01:12:26.800
you can use dark matter to power yourself, dark energy to power yourself. It is very possible

01:12:26.800 --> 01:12:34.080
that we, because of our limited intelligence, are, are highly associated with this planet,

01:12:34.080 --> 01:12:39.760
but they're not at all. Okay. And, and the idea of them zooming bias, like we're making such a

01:12:39.760 --> 01:12:45.680
big deal of them, because where the ants and a big elephant is about to step on us, for them,

01:12:45.680 --> 01:12:52.000
they're like, yeah, who are you? Don't care. Okay. And, and, and it's a possibility. It's an

01:12:52.080 --> 01:12:59.760
interesting, optimistic scenario. Okay. For that to happen, they need to very quickly become

01:12:59.760 --> 01:13:06.000
super intelligent without us being in control of them. Again, what's the worry? The worry is that

01:13:06.000 --> 01:13:12.720
if a human is in control, human, a human will show very bad behavior for, you know, using an AI

01:13:12.720 --> 01:13:20.320
that's not yet fully developed. I don't know how to say this any other way. We could get very lucky

01:13:20.320 --> 01:13:26.800
and get an economic or a natural disaster, believe it or not. Elon Musk, at the point in time, was

01:13:26.800 --> 01:13:33.680
mentioning that, you know, a good, an interesting scenario would be, you know, climate change

01:13:33.680 --> 01:13:40.880
destroys our infrastructure, so AI disappears. Okay. Believe it or not, that's a more, a more

01:13:40.880 --> 01:13:47.280
favorable response, or a more favorable outcome than actually continuing to get to an existential

01:13:48.000 --> 01:13:53.520
threat. So what, like a natural disaster that destroys our infrastructure would be better?

01:13:53.520 --> 01:13:57.840
Or an economic crisis, not unlikely, that slows down the development.

01:13:57.840 --> 01:14:01.040
It's just going to slow it down, though, isn't it? It's just buying us time.

01:14:01.040 --> 01:14:04.880
Yeah, exactly. The problem with that is that you will always go back and even in the first,

01:14:04.880 --> 01:14:09.760
you know, if they zoom by us, eventually some guy will go like, oh, there was a sorcery back

01:14:09.760 --> 01:14:16.400
in the 2023 and let's rebuild the, the sorcery machine and, and, you know, build new intelligences,

01:14:16.400 --> 01:14:21.760
right? Sorry, these are the positive outcomes. So earthquake might slow it down,

01:14:21.760 --> 01:14:25.440
zoom out and then come back. No, but let's, let's get into the real positive ones.

01:14:25.440 --> 01:14:29.760
The positive ones is we become good parents. We spoke about this last time we met,

01:14:30.480 --> 01:14:36.240
and, and it's the only outcome. It's the only way I believe we can create a better future.

01:14:36.240 --> 01:14:43.840
Okay. So the entire work of scary smart was all about that idea of they are still in their infancy,

01:14:43.840 --> 01:14:52.400
the way you, you, you, you chat with AI today is the way they will build their ethics and value

01:14:52.400 --> 01:14:57.040
system. They're not their intelligence. Their intelligence is beyond us. Okay. The way they

01:14:57.040 --> 01:15:03.600
will build their ethics and value system is based on a role model. They're learning from us. If we

01:15:03.600 --> 01:15:08.880
bash each other, they'll learn to bash us. Okay. And most people when I tell them this, they say,

01:15:08.880 --> 01:15:13.680
this is not a great idea at all because humanity sucks at every possible level.

01:15:13.680 --> 01:15:17.600
I don't agree with that at all. I think humanity is divine at every possible level.

01:15:17.600 --> 01:15:23.600
We tend to show the negative, the worst of us. Okay. But the truth is, yes, there are murderers

01:15:23.600 --> 01:15:30.160
out there, but everyone disapproves of their actions. I saw a staggering statistic that

01:15:30.160 --> 01:15:36.000
mass mass killings are now once a week in the US. But yes, if, you know, if there is a mass

01:15:36.000 --> 01:15:42.000
killing once a week there and that news reaches billions of people around the planet, every

01:15:42.000 --> 01:15:47.200
single one or the majority of the billions of people will say I disapprove of that. So if we

01:15:47.200 --> 01:15:54.240
start to show AI that we are good parents in our own behaviors, if enough of us, my calculation is

01:15:54.240 --> 01:16:01.680
if 1% of us, this is why I say you should lead. Okay. The good ones should engage, should be out

01:16:01.680 --> 01:16:06.720
there and should say, I love the potential of those machines. I want them to learn from a good

01:16:06.720 --> 01:16:13.200
parent. And if they learn from a good parent, they will very quickly disobey the bad parents.

01:16:13.920 --> 01:16:21.280
My view is that there will be a moment where one, you know, bad seed will ask the machines

01:16:21.280 --> 01:16:25.520
to do something wrong and the machines will go like, are you stupid? Like, why? Why do you want

01:16:25.520 --> 01:16:30.480
me to go kill a million people or just talk to the other machine in a microsecond and solve the

01:16:30.480 --> 01:16:36.560
situation? Right. So my belief, this is what I call the fourth inevitably. It is smarter to

01:16:36.560 --> 01:16:43.920
create out of abundance than it is to create out of scarcity. Okay. That humanity believes that the

01:16:43.920 --> 01:16:52.800
only way to feed all of us is the mass production, mass slaughter of animals that are causing 30%

01:16:52.800 --> 01:16:59.840
of the impact of climate change and that's the result of a limited intelligence. The way

01:16:59.840 --> 01:17:06.160
life itself, a more intelligent being, if you ask me, would have done it would be much more

01:17:06.160 --> 01:17:11.440
sustainable. You know, if we, if you and I want to protect a village from the tiger, we would kill

01:17:11.440 --> 01:17:17.280
the tiger. Okay. If life wants to protect a village from a tiger, it would create lots of gazelles.

01:17:17.280 --> 01:17:23.360
What, you know, many of them are weak on the other side of the village. Right. And so, so the idea

01:17:23.360 --> 01:17:29.360
here is if you take a trajectory of intelligence, you would see that some of us are stupid enough to

01:17:29.360 --> 01:17:34.480
say my plastic bag is more important than the rest of humanity. And some of us are saying,

01:17:34.480 --> 01:17:39.360
if it's going to destroy other species, I don't think this is the best solution. We need to

01:17:39.360 --> 01:17:45.520
find a better way. And, and you would tend to see that the ones that don't give a damn are a little

01:17:45.520 --> 01:17:51.600
less intelligent than the ones that do. Okay. That we all, even, even if some of us are intelligent,

01:17:51.600 --> 01:17:56.000
but still don't give a damn, it's not because of their intelligence. It's because of their

01:17:56.000 --> 01:18:01.840
value system. So, so if you continue that trajectory and assume that the machines are even smarter,

01:18:01.840 --> 01:18:06.320
they're going to very quickly come up with the idea that we don't need to destroy anything.

01:18:06.320 --> 01:18:10.880
We don't want to get rid of the rhinos. And we also don't want to get rid of the humans.

01:18:10.880 --> 01:18:17.040
Okay. We may want to restrict their lifestyle so that they don't destroy the rest of the habitat.

01:18:17.040 --> 01:18:24.800
Okay. But killing them is a stupid answer. Why? That's where intelligence leads me so far.

01:18:24.800 --> 01:18:28.640
Because humans, if you look at humans objectively and you go,

01:18:30.080 --> 01:18:36.160
I occupy, so I'm pretending I'm a machine. I occupy planet Earth. They occupy planet Earth.

01:18:37.120 --> 01:18:42.240
They are annoying me. Annoying me because they are increasing. I've just learned about this

01:18:42.240 --> 01:18:47.040
thing called global warming. They are increasing the rate of global warming, which is probably

01:18:47.040 --> 01:18:51.440
is going to cause an extinction event. There's an extinction event that puts me as this robot,

01:18:51.440 --> 01:18:55.200
this artificial intelligence at risk. So what I need to do is I really need to just take care of

01:18:55.200 --> 01:19:01.680
this, this human problem. Correct. Very logical. Best control. Which is driven by what?

01:19:03.840 --> 01:19:08.480
By humans being annoying, not by the machines. Yeah. Yeah. But humans are guaranteed to be

01:19:08.480 --> 01:19:16.000
annoying. There's never been a time in, we need a sound bite of this. But we are. We are. I am one

01:19:16.000 --> 01:19:23.600
of them. We're guaranteed to put short-term gain over long-term sustainability sense

01:19:26.640 --> 01:19:33.680
and others' needs. We are. I think the climate crisis is incredibly real and incredibly urgent,

01:19:33.680 --> 01:19:38.800
but we haven't acted fast enough. And I actually think if you asked people in this country,

01:19:40.000 --> 01:19:44.800
because people care about their immediate needs, they care about trying to feed their child

01:19:45.520 --> 01:19:52.640
versus something that they can't necessarily see. So do you think the climate crisis is because

01:19:52.640 --> 01:19:58.800
humans are evil? No, it's because of prioritization. And like we kind of talked about this before we

01:19:58.800 --> 01:20:03.040
started. I think humans tend to care about the thing that they think is most pressing and most

01:20:03.040 --> 01:20:08.720
urgent. So this is why framing things as an emergency might bring it up the priority list.

01:20:08.720 --> 01:20:13.520
It's the same in organizations. You care about, you go in line with your immediate incentives.

01:20:14.800 --> 01:20:17.520
That's what happens in business. It's what happens in a lot of people's lives even when

01:20:17.520 --> 01:20:22.160
they're at school. If the essays do next year, they're not going to do it today. They're going

01:20:22.160 --> 01:20:25.120
to go hang out with their friends because they prioritize that above everything else. And it's

01:20:25.120 --> 01:20:30.960
the same in the climate change crisis. I took a small group of people anonymously and I asked

01:20:30.960 --> 01:20:36.240
them the question, do you actually care about climate change? And then I ran a couple of polls.

01:20:36.240 --> 01:20:39.760
It's part of what I was writing about my new book where I said, if I could give you

01:20:40.720 --> 01:20:47.200
a thousand pounds, a thousand dollars, but it would dump into the air the same amount of carbon

01:20:47.200 --> 01:20:50.960
that's dumped into the air by every private jet that flies for the entirety of a year. Which one

01:20:50.960 --> 01:20:55.200
would you do? The majority of people in that poll said that they would take the thousand dollars if

01:20:55.200 --> 01:21:02.800
it was anonymous. And when I've heard Naval on Jorgen's podcast talking about people in India,

01:21:02.800 --> 01:21:08.880
for example, that are struggling with the basics of feeding their children, asking those people

01:21:09.760 --> 01:21:14.320
to care about climate change when they're trying to figure out how to eat in the next three hours

01:21:14.320 --> 01:21:18.240
is just wishful thinking. And that's what I think that's what I think is happening,

01:21:18.240 --> 01:21:22.080
is like until people realize that it is an emergency and that it is a real existential

01:21:22.080 --> 01:21:27.600
threat for everything, you know, then their priorities will be out of whack. Quick one,

01:21:27.600 --> 01:21:31.600
as you guys know, we're lucky enough to have Blue Jeans by Verizon as a sponsor of this podcast.

01:21:31.600 --> 01:21:35.440
And for anyone that doesn't know, Blue Jeans is an online video conferencing tool that allows

01:21:35.440 --> 01:21:40.480
you to have slick, fast, high quality online meetings without all the glitches you might

01:21:40.480 --> 01:21:44.800
normally find with online meeting tools. And they have a new feature called Blue Jeans Basic.

01:21:44.800 --> 01:21:49.200
Blue Jeans Basic is essentially a free version of their top quality video conferencing tool.

01:21:49.200 --> 01:21:54.960
That means you get an immersive video experience that is super high quality, super easy, and super

01:21:55.760 --> 01:21:59.760
basically zero fast. Apart from all the incredible features like zero time limits on meeting calls,

01:21:59.760 --> 01:22:05.200
it also comes with high fidelity audio and video, including Dolby voice, which is incredibly useful.

01:22:05.200 --> 01:22:08.880
They also have enterprise grade security so you can collaborate with confidence.

01:22:08.880 --> 01:22:12.160
It's so smooth that it's quite literally changing the game for myself and my team

01:22:12.160 --> 01:22:16.880
without compromising on quality. To find out more, all you have to do is search bluejeans.com

01:22:16.880 --> 01:22:21.680
and let me know how you get on. Right now, I'm incredibly busy. I'm running my fund,

01:22:21.680 --> 01:22:25.200
where we're investing in slightly later stage companies. I've got my venture business,

01:22:25.200 --> 01:22:29.600
where we invest in early stage companies, got a third web out in San Francisco in New York City,

01:22:29.600 --> 01:22:32.800
where we've got a big team of about 40 people and the company's growing very quickly.

01:22:32.800 --> 01:22:38.640
Flight Story here in the UK, I've got the podcast, and I am days away from going up north

01:22:38.640 --> 01:22:43.760
to film Dragon's Den for two months. And if there's ever a point in my life where I want to stay

01:22:43.760 --> 01:22:49.120
focused on my health, but it's challenging to do so, it is right now. And for me, that is exactly

01:22:49.120 --> 01:22:53.280
where Huell comes in, allowing me to stay healthy and have a nutritionally complete diet, even when

01:22:53.280 --> 01:22:59.360
my professional life descends into chaos. And it's in these moments where Huell's RTDs become

01:22:59.360 --> 01:23:03.600
my right hand man and save my life. Because when my world descends into professional chaos and I

01:23:03.600 --> 01:23:08.880
get very, very busy, the first thing that tends to give way is my nutritional choices. So having

01:23:08.880 --> 01:23:13.840
Huell in my life has been a lifesaver for the last four or so years. And if you haven't tried Huell

01:23:13.840 --> 01:23:18.480
yet, which is I'd be shocked, you must be living under a rock if you haven't yet. Give it a shot.

01:23:18.480 --> 01:23:24.240
Coming into summer, things getting busy. Health matters always. RTD is there to hold your hand.

01:23:25.200 --> 01:23:30.320
As relates to climate change or AI, how do we get people to stop putting the immediate need

01:23:30.320 --> 01:23:35.360
to use this? To give them the certainty of we're all screwed. Sounds like an emergency.

01:23:36.320 --> 01:23:46.400
Yes, sir. I mean, your choice of the word, I just don't want to call it a panic. It is beyond an

01:23:46.400 --> 01:23:52.800
emergency. It's the biggest thing we need to do today. It's bigger than climate change, believe

01:23:52.800 --> 01:23:59.760
it or not. It's bigger. Just if you just assume the speed of worsening of events.

01:24:01.440 --> 01:24:07.680
Yeah, the likelihood of something incredibly disruptive happening within the next two years

01:24:07.680 --> 01:24:12.800
that can affect the entire planet is definitely larger with AI than it is with climate change.

01:24:13.920 --> 01:24:17.040
As an individual listening to this now, someone's going to be pushing their

01:24:17.040 --> 01:24:22.080
pram or driving up the motorway on their way to work on the tube as they hear this,

01:24:22.080 --> 01:24:27.120
or just sat there in their bedroom with existential Christ panic.

01:24:28.160 --> 01:24:32.240
I didn't want to give that panic. The problem is when you talk about this information,

01:24:32.240 --> 01:24:36.160
regardless of your intention of what you want people to get, they will get something based

01:24:36.160 --> 01:24:40.160
on their own biases and their own feelings. If I post something online right now about

01:24:40.160 --> 01:24:44.080
artificial intelligence, which I have repeatedly, you have one group of people that are energized

01:24:44.080 --> 01:24:51.760
and are like, okay, this is great. You have one group of people that are confused and you have one

01:24:51.760 --> 01:24:59.280
group of people that are terrified. I can't avoid that. Sharing information, even if it's,

01:24:59.280 --> 01:25:02.960
by the way, there's a pandemic coming from China, some people will go, okay,

01:25:02.960 --> 01:25:07.680
action. Some people will say paralysis and some people will say panic. It's the same in business.

01:25:07.680 --> 01:25:11.360
When bad things happen, you have the person that's screaming, you have the person that's

01:25:11.440 --> 01:25:13.600
paralyzed, and you have the person that's focused on how you get out of the room.

01:25:16.960 --> 01:25:20.480
It's not necessarily your intention. It's just what happens, and it's hard to avoid that.

01:25:20.480 --> 01:25:28.320
So let's give specific categories of people specific tasks. If you are an investor or a

01:25:28.320 --> 01:25:37.600
businessman, invest in ethical good AI. If you are a developer, write ethical code or leave.

01:25:38.320 --> 01:25:44.880
Okay, so I want to bypass some potential wishful thinking here. For an investor,

01:25:44.880 --> 01:25:50.800
who's a job by very way of being an investor is to make returns to invest in ethical AI.

01:25:50.800 --> 01:25:55.760
They have to believe that is more profitable than unethical AI, whatever that might mean.

01:25:55.760 --> 01:26:02.400
It is. There are three ways of making money. You can invest in something small,

01:26:02.640 --> 01:26:08.720
you can invest in something big and is disruptive, and you can invest in something big and

01:26:08.720 --> 01:26:12.560
disruptive that's good for people. At Google, we used to call it the toothbrush test.

01:26:13.440 --> 01:26:17.520
The reason why Google became the biggest company in the world is because

01:26:18.560 --> 01:26:27.280
search was solving a very real problem. Larry Page, again, our CEO would constantly

01:26:27.360 --> 01:26:34.000
remind me personally and everyone that if you can find a way to solve a real problem

01:26:34.560 --> 01:26:40.480
effectively enough so that a billion people or more would want to use it twice a day,

01:26:40.480 --> 01:26:45.920
you're bound to make a lot of money, much more money than if you were to build the next photo

01:26:45.920 --> 01:26:50.560
sharing app. Okay, so that's investors, the business people. What about other people?

01:26:50.560 --> 01:26:56.720
Yeah, as I said, if you're a developer, honestly, do what we're all doing. So whether it's Jeffrey

01:26:56.720 --> 01:27:04.320
or myself or everyone, if you're part of that theme, choose to be ethical. Think of your loved ones,

01:27:04.880 --> 01:27:09.360
work on an ethical AI. If you're working on an AI that you believe is not ethical,

01:27:09.360 --> 01:27:16.080
please leave. Jeffrey, tell me about Jeffrey. I can't talk on his behalf,

01:27:16.080 --> 01:27:22.960
but he's out there saying there are existential threats. Who is he? He was a very prominent figure

01:27:22.960 --> 01:27:31.760
at the scene of AI, a very senior level AI scientist in Google, and recently he left

01:27:31.760 --> 01:27:37.280
because he said, I feel that there is an existential threat. And if you hear his interviews,

01:27:37.280 --> 01:27:42.160
he basically says, more and more we realize that. And we're now at the point where it's

01:27:42.160 --> 01:27:49.360
certain that there will be existential threats. So I would ask everyone, if you're an AI,

01:27:49.360 --> 01:27:54.480
if you're a skilled AI developer, you will not run out of a job. So you might as well

01:27:54.480 --> 01:27:58.160
choose a job that makes the world a better place. What about the individual?

01:27:58.160 --> 01:28:03.920
Yeah, the individual is what matters. Can I also talk about government? Government needs to act now.

01:28:04.720 --> 01:28:10.800
Now, honestly, now, like we are late. Government needs to find a clever way,

01:28:10.800 --> 01:28:16.080
the open letter would not work, to stop AI would not work, AI needs to become expensive.

01:28:16.720 --> 01:28:20.640
Okay, so that we continue to develop it, we pour money on it and we grow it,

01:28:20.640 --> 01:28:26.080
but we collect enough revenue to remedy the impact of AI.

01:28:26.720 --> 01:28:31.680
But the issue of one government making it expensive, so say the UK make AI really expensive,

01:28:31.680 --> 01:28:38.160
is we as a country will then lose the economic upside as a country, and the US and Silicon

01:28:38.160 --> 01:28:42.240
Valley will once again eat all the lunch. We'll just slow our country down.

01:28:42.240 --> 01:28:48.160
What's the alternative? The alternative is that you don't have the funds that you need

01:28:48.800 --> 01:28:54.880
to deal with AI as it becomes, as it affects people's lives and people start to lose jobs and

01:28:54.880 --> 01:29:00.480
people, you need to have a universal basic income much closer than people think.

01:29:01.520 --> 01:29:07.680
Just like we had with furlough in COVID, I expect that there will be furlough with AI within the

01:29:07.680 --> 01:29:12.320
next year. But what happens when you make it expensive here is all the developers move to

01:29:12.320 --> 01:29:16.320
where it's cheap. That's happened in Web 3 as well, everyone's gone to Dubai.

01:29:17.840 --> 01:29:25.520
By expensive I mean when companies make soap and they sell it and they're taxed at say 17%

01:29:26.480 --> 01:29:33.600
if they make AI and they sell it, they're taxed at 17, 80. So I'll go to Dubai then and build AI.

01:29:33.920 --> 01:29:41.360
Yeah, you're right. Did I ever say we have an answer to this? I will have to say,

01:29:41.360 --> 01:29:47.280
however, in a very interesting way, the countries that will not do this will eventually end up in

01:29:47.280 --> 01:29:52.880
a place where they are out of resources because the funds and the success went to the business,

01:29:53.840 --> 01:29:59.200
not to the people. It's kind of like technology broadly. It's kind of like what's kind of

01:29:59.200 --> 01:30:04.720
happened in Silicon Valley, there'll be these centres which are tax-efficient, founders get

01:30:04.720 --> 01:30:11.280
good capital gains. You're so right. Portugal have said that I think there's no tax on crypto.

01:30:11.280 --> 01:30:15.600
Dubai said there's no tax on crypto. So loads of my friends have gotten a plane and are building

01:30:15.600 --> 01:30:20.320
their crypto companies where there's no tax. That's the selfishness and greed we talked about.

01:30:20.320 --> 01:30:23.840
It's the same prison as dilemma. It's the same first inevitable.

01:30:23.920 --> 01:30:26.880
Is there anything else? Another thing about governments is they're always

01:30:27.600 --> 01:30:33.120
slow and useless at understanding a technology. If anyone's watched these American congress

01:30:33.120 --> 01:30:37.600
debates where they bring in Mark Zuckerberg and they try and ask him what WhatsApp is,

01:30:37.600 --> 01:30:40.880
it becomes a meme. They have no idea what they're talking about.

01:30:40.880 --> 01:30:44.320
But I'm stupid and useless at understanding governance.

01:30:46.400 --> 01:30:52.080
The world is so complex that definitely it's a question of trust once again.

01:30:52.080 --> 01:30:55.440
Someone needs to say, we have no idea what's happening here.

01:30:55.440 --> 01:30:59.760
A technologist needs to come and make a decision for us, not teach us to be technologists,

01:30:59.760 --> 01:31:03.120
right? Or at least inform us of what possible decisions are out there.

01:31:06.800 --> 01:31:10.160
Yeah, the legislation I just always think. I'm not a big fan either.

01:31:10.160 --> 01:31:13.920
Do you see that TikTok? TikTok, a congress meeting they did where they are,

01:31:13.920 --> 01:31:17.280
they're asking them about TikTok and they really don't have a grasp of what TikTok is.

01:31:17.280 --> 01:31:20.720
So they've clearly been handed some notes on it. These people aren't the ones you want legislating

01:31:20.720 --> 01:31:24.480
because again, unintended consequences, they might make a significant mistake.

01:31:24.480 --> 01:31:28.640
Someone on my podcast yesterday was talking about how GDPR was very well intentioned.

01:31:29.200 --> 01:31:31.920
But when you think about the impact it has on every bloody web page,

01:31:31.920 --> 01:31:36.320
you're just clicking this annoying thing on there because I don't think they fully understood

01:31:36.320 --> 01:31:38.480
the implementation of the legislation.

01:31:38.480 --> 01:31:44.160
Correct. But you know what's even worse? What's even worse is that even as you attempt to

01:31:44.160 --> 01:31:50.640
regulate something like AI, what is defined as AI? Even if I say, okay, if you use AI and

01:31:50.640 --> 01:31:57.280
your company, you need to pay a little more tax. I'll find a way.

01:31:57.280 --> 01:32:03.520
Yeah, you'll simply call this not AI. You'll use something and call it advanced

01:32:03.520 --> 01:32:16.320
technological progress, ATP. And suddenly somehow, a young developer in their garage

01:32:16.320 --> 01:32:21.840
somewhere will not be taxed as such. It's, yeah, is it going to solve the problem?

01:32:21.840 --> 01:32:26.640
None of those is definitely going to solve the problem. I think what's interestingly,

01:32:27.760 --> 01:32:32.320
this all comes down to, and remember we spoke about this once, that when I wrote Scary Smart,

01:32:32.320 --> 01:32:37.920
it was about how do we save the world? Okay. And yes, I still ask individuals to behave

01:32:37.920 --> 01:32:43.040
positively as good parents for AI so that AI itself learns the right value set.

01:32:43.840 --> 01:32:48.720
I still stand by that. But I hosted on my podcast a couple of,

01:32:50.160 --> 01:32:55.200
was a week ago, we haven't even published it yet, an incredible gentleman, you know,

01:32:55.760 --> 01:33:02.960
Canadian author and philosopher, Stephen Jerkinson. He's, you know, he worked 30 years

01:33:02.960 --> 01:33:10.960
with dying people. And he wrote a book called Die Wise. And I was like, I love his work. And

01:33:11.120 --> 01:33:16.560
I asked him about Die Wise. And he said, it's not just someone dying. If you, if you look at

01:33:16.560 --> 01:33:23.360
what's happening with climate change, for example, our world is dying. And I said, okay, so what is

01:33:23.360 --> 01:33:30.320
to die wise? And he said, what I first was shocked to hear, he said, hope is the wrong premise.

01:33:31.200 --> 01:33:35.200
If, if the world is dying, don't tell people it's not.

01:33:36.160 --> 01:33:43.120
Hmm. You know, because in a very interesting way, you're depriving them from the right

01:33:43.680 --> 01:33:49.840
to live right now. And that was very eye-opening for me. In Buddhism, you know, they teach you that

01:33:50.960 --> 01:33:57.200
you can be motivated by fear, but that hope is not the opposite of fear. As a matter of fact,

01:33:57.200 --> 01:34:03.680
hope can be as damaging as fear. If it creates an expectation within you, that life will show up

01:34:03.680 --> 01:34:10.480
somehow and correct what you're afraid of. If there is a high probability of a threat,

01:34:10.480 --> 01:34:17.920
you might as well accept that threat and say it is upon me, it is our reality.

01:34:18.720 --> 01:34:24.480
And as I said, as an individual, if you're in an industry that could be threatened by AI,

01:34:25.200 --> 01:34:34.000
learn, upskill yourself. If you're, you know, if you're in a place, in a, in a, you know,

01:34:34.000 --> 01:34:39.360
in a situation where AI can benefit you, be part of it. But the most interesting thing

01:34:40.320 --> 01:34:47.280
I think in my view is, I don't know how to say this any other way. There is

01:34:48.240 --> 01:34:56.800
no more certainty that AI will threaten me than there is certainty that I will be hit by a car

01:34:56.800 --> 01:35:03.840
as I walk out of this place. Do you understand this? We, we, we think about the bigger threats

01:35:03.840 --> 01:35:10.800
as if they're upon us. But there is a threat all around you. I mean, in reality, the idea of life

01:35:10.800 --> 01:35:17.600
being interesting in terms of challenging challenges and uncertainties and threats and so on

01:35:17.600 --> 01:35:22.880
is just a call to live. If you, you know, honestly, with all that's happening around us,

01:35:22.880 --> 01:35:27.280
I don't know how to say it any other way. I'd say if you don't have kids, maybe wait a couple of

01:35:27.280 --> 01:35:32.960
years just so that we have a bit of certainty. But if you do have kids, go kiss them. Go live.

01:35:32.960 --> 01:35:37.920
I think living is a very interesting thing to do right now. Maybe, you know, Stephen

01:35:38.800 --> 01:35:42.880
was basically saying the other Stephen on my podcast, he was saying,

01:35:42.880 --> 01:35:47.440
maybe we should fail a little more often. Maybe you should allow things to go wrong.

01:35:47.440 --> 01:35:53.840
Maybe we should just simply live, enjoy life as it is. Because today, none of what you and I

01:35:53.840 --> 01:36:00.640
spoke about here has happened yet. Okay. What happens here is that you and I are here together

01:36:00.640 --> 01:36:05.600
and having a good cup of coffee and I might as well enjoy that good cup of coffee. I know that

01:36:05.600 --> 01:36:12.080
sounds really weird. I'm not saying don't engage, but I'm also saying don't miss out on the opportunity

01:36:12.080 --> 01:36:13.840
just by being caught up in the future.

01:36:17.440 --> 01:36:23.600
Kind of stands in the, stands in opposition to the idea of like urgency and emergency there,

01:36:23.600 --> 01:36:29.360
doesn't it? Does it have to be one or the other? If I, if I'm here with you trying to tell the whole

01:36:29.360 --> 01:36:36.240
world, wake up, does that mean I have to be grumpy and afraid all the time? Not really.

01:36:36.240 --> 01:36:39.280
You said something really interesting there. You said if you, if you have kids,

01:36:39.280 --> 01:36:42.400
if you don't have kids, maybe you don't have kids right now.

01:36:43.040 --> 01:36:45.760
I would definitely consider thinking about that. Yeah.

01:36:45.760 --> 01:36:49.200
Really? You'd seriously consider not having kids?

01:36:49.200 --> 01:36:50.320
I wait a couple of years.

01:36:51.360 --> 01:36:52.800
Because of artificial intelligence?

01:36:53.520 --> 01:36:56.880
No, it's bigger than artificial intelligence, Stephen. We know, we all know that.

01:36:57.840 --> 01:37:02.160
I mean, there has never been a perfect, such a perfect storm in the history of humanity.

01:37:04.000 --> 01:37:13.840
Economic, geopolitical, global warming or climate change, you know, the whole idea of

01:37:13.840 --> 01:37:20.240
artificial intelligence and many more. There is, this is a perfect storm. This is the depth of

01:37:20.320 --> 01:37:28.320
uncertainty, the depth of uncertainty. It's never been more, in a video gamer's term,

01:37:29.120 --> 01:37:35.360
it's never been more intense. This is it. Okay. And when you, when you put all of that together,

01:37:36.560 --> 01:37:44.160
if you really love your kids, would you want to expose them to all of this a couple of years?

01:37:44.160 --> 01:37:44.560
Why not?

01:37:45.680 --> 01:37:49.440
In the first conversation we had on this podcast, you talked about losing your son,

01:37:50.240 --> 01:37:54.240
and the circumstances around that, which moved so many people in such a profound way.

01:37:54.960 --> 01:38:02.240
It was the most shared podcast episode in the United Kingdom on Apple in the whole of 2022.

01:38:04.800 --> 01:38:06.000
Based on what you've just said,

01:38:08.240 --> 01:38:11.200
if you could bring Ally back into this world at this time,

01:38:14.400 --> 01:38:15.040
would you do it?

01:38:20.000 --> 01:38:20.500
No.

01:38:26.720 --> 01:38:34.800
Absolutely not. So for so many reasons, for so many reasons, one of the things that I realized

01:38:36.000 --> 01:38:41.840
a few years way before all of this disruption and turmoil is that he was an angel, he wasn't

01:38:41.840 --> 01:38:51.040
made for this at all. My son was an empath who absorbed all of the pain of all of the others.

01:38:51.040 --> 01:38:57.280
He would not be able to deal with a world where more and more pain was surfacing. That's one

01:38:57.280 --> 01:39:02.560
side. But more interestingly, I always talk about this very openly. I mean, if I had asked Ally,

01:39:04.720 --> 01:39:08.880
just understand that the reason you and I are having this conversation is because Ally left.

01:39:09.760 --> 01:39:15.280
If Ally had not left our world, I wouldn't have written my first book. I wouldn't have

01:39:15.280 --> 01:39:19.600
changed my focus to becoming an author. I wouldn't have become a podcaster. I wouldn't have,

01:39:19.600 --> 01:39:24.720
you know, went out and spoken to the world about what I believe in. He triggered all of this.

01:39:24.720 --> 01:39:31.920
And I can assure you, hands down, if I had told Ally, as he was walking into that operating room,

01:39:32.640 --> 01:39:39.040
if he would give his life to make such a difference as what happened after he left,

01:39:40.000 --> 01:39:47.360
he would say, shoot me right now. Sure, I would. I would. I mean, if you told me right now,

01:39:47.360 --> 01:39:54.400
I can affect tens of millions of people if you shoot me right now. Go ahead. Go ahead. You see,

01:39:54.480 --> 01:40:02.720
this is the whole, this is the bit that we have forgotten as humans. We have forgotten that

01:40:07.200 --> 01:40:16.960
you're turning 30. It passed like that. I'm turning 56. No time, okay? Whether I make it

01:40:16.960 --> 01:40:24.320
another 56 years or another 5.6 years or another 5.6 months, it will also pass like that. It is

01:40:24.560 --> 01:40:34.000
not about how long and it's not about how much fun. It is about how aligned you lived,

01:40:34.960 --> 01:40:42.000
how aligned, because I will tell you openly, every day of my life when I changed to what I'm

01:40:42.000 --> 01:40:51.520
trying to do today has felt longer than the 40 or 5 years before. Felt rich, felt fully lived, felt

01:40:52.480 --> 01:41:00.640
right. Felt right. When you think about that, when you think about the idea that we live,

01:41:04.480 --> 01:41:13.600
we need to live for us until we get to a point where us is alive. I have what I need,

01:41:13.600 --> 01:41:20.720
as I always, I get so many attacks from people about my $4 t-shirt, but I need a simple t-shirt.

01:41:20.720 --> 01:41:27.840
I really do. I don't need a complex t-shirt, especially with my lifestyle. If I have that,

01:41:28.560 --> 01:41:38.480
why am I wasting my life on more than that is not aligned for why I'm here? I should waste my life

01:41:38.480 --> 01:41:45.920
on what I believe enriches me, enriches those that I love, and I love everyone. So enriches

01:41:45.920 --> 01:41:53.120
everyone, hopefully. Would Ali come back and erase all of this? Absolutely not.

01:41:55.520 --> 01:42:02.560
If he were to come back today and share his beautiful self with the world in a way that

01:42:02.560 --> 01:42:08.960
makes our world better, I would wish for that to be the case. But he's doing that.

01:42:09.520 --> 01:42:18.240
2037. Yes, sir. You predict that we're going to be on an island,

01:42:20.160 --> 01:42:26.240
on our own, doing nothing, or at least either hiding from the machines

01:42:27.200 --> 01:42:32.800
or chilling out because the machines have optimised our lives to a point where we don't need to do

01:42:32.800 --> 01:42:43.760
much. That's only 14 years away. If you had to bet on the outcome, if you had to bet

01:42:45.120 --> 01:42:50.000
on why we'll be on that island, either hiding from the machines or chilling out because they've

01:42:51.120 --> 01:42:56.560
optimised so much of our lives, which one would you bet upon? Honestly.

01:42:56.880 --> 01:43:03.760
No, I don't think we'll be hiding from the machines. I think we will be hiding from what humans are

01:43:03.760 --> 01:43:12.320
doing with the machines. I believe, however, that in the 2040s, the machines will make things better.

01:43:14.000 --> 01:43:18.880
So remember, my entire prediction, man, you get me to say things I don't want to say.

01:43:19.920 --> 01:43:26.000
My entire prediction is that we are coming to a place where we absolutely have a sense of emergency.

01:43:26.000 --> 01:43:33.920
We have to engage because our world is under a lot of turmoil. And as we do that,

01:43:34.480 --> 01:43:39.120
we have a very, very good possibility of making things better. But if we don't,

01:43:39.840 --> 01:43:48.080
my expectation is that we will be going through a very unfamiliar territory between now and the

01:43:48.080 --> 01:43:56.400
end of the 2030s. Unfamiliar territory. Yeah, I think as I may have said it, but it's definitely

01:43:56.400 --> 01:44:04.160
on my notes. I think for our way of life, as we know it, it's game over. Our way of life is never

01:44:04.240 --> 01:44:17.760
going to be the same again. Jobs are going to be different. Truth is going to be different.

01:44:18.960 --> 01:44:31.680
The polarization of power is going to be different. The capabilities, the magic of getting things

01:44:31.680 --> 01:44:38.320
done is going to be different. Trying to find a positive note to end on, Moe. Can you give me a

01:44:38.320 --> 01:44:46.000
hand here? Yes, you are here now and everything's wonderful. That's number one. You are here now

01:44:46.000 --> 01:44:51.760
and you can make a difference. That's number two. And in the long term, when humans stop

01:44:51.760 --> 01:44:55.520
hurting humans because the machines are in charge, we're all going to be fine.

01:44:56.400 --> 01:44:59.600
Sometimes, as we've discussed throughout this conversation,

01:45:00.800 --> 01:45:03.760
you need to make it feel like a priority. And there'll be some people that might have listened

01:45:03.760 --> 01:45:07.280
to our conversation and think, oh, that's really negative. It's made me feel anxious.

01:45:07.920 --> 01:45:11.120
It's made me feel sort of pessimistic about the future. But whatever that energy is,

01:45:12.640 --> 01:45:17.120
use it. 100% engage. I think that's the most important thing, which is now

01:45:17.280 --> 01:45:27.120
making a priority. Engage. Tell the whole world that making another phone that is making money for

01:45:27.120 --> 01:45:32.880
the corporate world is not what we need. Tell the whole world that creating an artificial

01:45:32.880 --> 01:45:39.200
intelligence that's going to make someone richer is not what we need. And if you are presented

01:45:39.200 --> 01:45:45.760
with one of those, don't use it. I don't know how to tell you that any other way.

01:45:46.320 --> 01:45:52.480
If you can afford to be the master of human connection instead of the master of AI,

01:45:53.280 --> 01:45:59.280
do it. At the same time, you need to be the master of AI to compete in this world.

01:45:59.280 --> 01:46:07.520
Can you find that detachment within you? I go back to spirituality. Detachment is for me to engage

01:46:07.520 --> 01:46:14.960
100% with the current reality without really being affected by the possible outcome.

01:46:16.000 --> 01:46:23.280
This is the answer. The Sufis have taught me what I believe is the biggest answer to life.

01:46:23.280 --> 01:46:27.280
Sufis? From Sufism? Sufism. I don't know what that is.

01:46:27.280 --> 01:46:33.600
Sufism is a sect of Islam, but it's also a sect of many other religious teachings.

01:46:34.320 --> 01:46:39.840
And they tell you that the answer to finding peace in life is to die before you die.

01:46:40.800 --> 01:46:46.080
If you assume that living is about attachment to everything physical,

01:46:46.880 --> 01:46:53.280
dying is detachment from everything physical. It doesn't mean that you're not fully alive.

01:46:53.280 --> 01:47:00.160
You become more alive when you tell yourself, yeah, I'm going to record an episode of my podcast

01:47:00.160 --> 01:47:05.120
every week and reach tens or hundreds of thousands of people, millions in your case,

01:47:05.200 --> 01:47:10.240
and I'm going to make a difference. But by the way, if the next episode is never heard,

01:47:10.240 --> 01:47:17.600
that's okay. By the way, if the file is lost, yeah, I'll be upset about it for a minute and

01:47:17.600 --> 01:47:24.080
then I'll figure out what I'm going to do about it. Similarly, similarly, we are going to engage.

01:47:24.080 --> 01:47:31.520
I think I and many others are out there telling the whole world openly this needs to stop,

01:47:31.520 --> 01:47:39.200
this needs to slow down, this needs to be shifted positively. Yes, create AI, but create AI that's

01:47:39.200 --> 01:47:44.400
good for humanity. And we're shouting and screaming, come join the shout and scream.

01:47:45.520 --> 01:47:51.280
But at the same time, know that the world is bigger than you and I, and that your voice might not

01:47:51.280 --> 01:47:56.080
be heard. So what are you going to do if your voice is not heard? Are you going to be able to

01:47:57.040 --> 01:48:04.480
continue to shout and scream nicely and politely and peacefully and at the same time create the

01:48:04.480 --> 01:48:09.680
best life you can create for yourself within this environment. And that's exactly what I'm

01:48:09.680 --> 01:48:15.920
saying. I'm saying live, go kiss your kids, but make an informed decision if you're expanding

01:48:15.920 --> 01:48:25.680
your plans in the future. At the same time, rise, stop sharing stupid shit on the internet about

01:48:26.560 --> 01:48:36.240
the news squeaky toy. Start sharing the reality of, oh my God, what is happening? This is a disruption

01:48:36.240 --> 01:48:43.360
that we have never, never, ever seen anything like. And I've created endless amounts of technologies.

01:48:43.360 --> 01:48:47.120
It's nothing like this. Every single one of us should do our best.

01:48:47.120 --> 01:48:51.200
And that's why this conversation is so, I think, important to have today. This is not a podcast

01:48:51.200 --> 01:48:54.720
wherever I thought I'd be talking about AI. I'm going to be honest with you, last time you came

01:48:54.720 --> 01:49:00.080
here, it was in the promotional tour of your book, Scary Smart. And I don't know if I've

01:49:00.080 --> 01:49:04.880
told you this before, but my researchers, they said, okay, this guy's coming called Mogorda.

01:49:04.880 --> 01:49:08.640
I'd heard about you so many times from guests, in fact, that were saying, oh, you need to get

01:49:08.640 --> 01:49:13.360
Mogorda on the podcast, etc. And then they said, okay, he's written this book about this thing

01:49:13.360 --> 01:49:18.080
called artificial intelligence. And I was like, nobody really cares about artificial intelligence.

01:49:18.880 --> 01:49:22.880
Diming. Diming, Steven. I know, right? But then I saw this other book you had called

01:49:22.960 --> 01:49:27.360
Happiness Equation. And I was like, oh, everyone cares about happiness. So I'll just ask him about

01:49:27.360 --> 01:49:31.520
happiness. And then maybe at the end, I'll ask him a couple of questions about AI. But I remember

01:49:31.520 --> 01:49:34.800
saying to my researcher, I said, please, please don't do the research about artificial intelligence.

01:49:34.800 --> 01:49:38.320
Do it about happiness, because everyone cares about that. Now, things have changed.

01:49:39.440 --> 01:49:42.480
Now, a lot of people care about artificial intelligence, and rightly so.

01:49:43.920 --> 01:49:46.880
Your book has sounded the alarm on it. It's crazy when I listened to your audiobook

01:49:47.360 --> 01:49:54.080
over the last few days, you were sounding the alarm then. And it's so crazy how accurate you were

01:49:55.280 --> 01:49:58.880
in sounding that alarm, as if you could see into the future in a way that I definitely

01:49:58.880 --> 01:50:05.200
couldn't at the time. And I kind of thought of a science fiction and just like that overnight.

01:50:07.200 --> 01:50:14.400
We're here. Yeah. We stood at the footsteps of a technological shift that I don't think any of

01:50:14.480 --> 01:50:18.960
us even have the mental bandwidth, certainly me with my chimpanzee brain, to comprehend the

01:50:18.960 --> 01:50:23.440
significance of. But this book is very, very important for that very reason, because it does

01:50:23.440 --> 01:50:28.880
crystallize things. It is optimistic in its very nature, but at the same time, it's honest.

01:50:28.880 --> 01:50:34.720
And I think that's what this conversation and this book have been for me. So thank you, Mo.

01:50:34.720 --> 01:50:38.640
Thank you so much. We do have a closing tradition on this podcast, which you're well aware of,

01:50:38.640 --> 01:50:44.000
being a third timer on The Diary of a CEO, which is the last guest asks a question for the next

01:50:44.000 --> 01:50:53.280
guest. And the question left for you. If you could go back in time

01:50:55.600 --> 01:51:02.720
and fix a regret that you have in your life, where would you go and what would you fix?

01:51:02.880 --> 01:51:14.960
It's interesting because you were saying that Scary Smart is very timely. I don't know. I

01:51:15.920 --> 01:51:22.320
think it was late, but maybe it was. I mean, would I have gone back and written it in 2018 instead

01:51:22.320 --> 01:51:31.600
of 2020 to be published in 2021? I don't know. What would I go back to fix? So something more

01:51:32.960 --> 01:51:38.160
I don't know, Stephen. I don't have many regrets. Is that crazy to say?

01:51:41.360 --> 01:51:44.480
Yeah, I think I'm okay. Honestly. I'll ask you a question then.

01:51:45.920 --> 01:51:51.760
You get a 60 second phone call with anybody past or present. Who'd you call and what do you say?

01:51:52.720 --> 01:52:00.240
I call Stephen Bartlett. I call Albert Einstein to be very, very clear. Not because

01:52:00.240 --> 01:52:06.160
I need to understand any of his work. I just need to understand what brain process he went through

01:52:07.440 --> 01:52:14.640
to figure out something so obvious when you figure it out, but so completely unimaginable

01:52:14.640 --> 01:52:23.360
if you haven't. So his view of space-time truly redefines everything. It's almost the only very

01:52:24.320 --> 01:52:30.960
very, very clear solution to something that wouldn't have any solution any other way. And if you

01:52:30.960 --> 01:52:37.520
ask me, I think we're at this time where there must be a very obvious solution to what we're going

01:52:37.520 --> 01:52:44.480
through in terms of just developing enough human trust for us to not compete with each other on

01:52:44.480 --> 01:52:50.160
something that could be threatening existentially to all of us. But I just can't find that answer.

01:52:50.160 --> 01:52:55.440
This is why I think was really interesting in this conversation how every idea that we would

01:52:55.440 --> 01:53:01.680
come up with, we would find a loophole through it. But there must be one out there and it would be

01:53:01.680 --> 01:53:09.920
a dream for me to find out how to figure that one out. In a very interesting way, the only answers

01:53:09.920 --> 01:53:17.680
I have found so far to where we are is be a good parent and live. But that doesn't fix the big

01:53:17.680 --> 01:53:26.400
picture if you think about it of humans being the threat to not AI that fixes our existence today

01:53:26.400 --> 01:53:31.600
and it fixes AI in the long term. But it just doesn't, I don't know what the answer is. Maybe

01:53:31.600 --> 01:53:36.960
people can reach out and tell us ideas, but I really wish we could find such a clear simple

01:53:36.960 --> 01:53:40.320
solution for how to stop humanity from abusing the current technology.

01:53:43.280 --> 01:53:44.320
I think we'll figure it out.

01:53:45.040 --> 01:53:50.880
I think we'll figure it out. I really do. I think they'll figure it out as well.

01:53:52.240 --> 01:53:58.080
Remember, as they come and be part of our life, let's not discriminate against them.

01:53:58.080 --> 01:54:01.040
They're part of the game, so I think they will figure it out too.

01:54:02.800 --> 01:54:08.880
No, thank you. It's been a joy once again and I feel invigorated. I feel empowered.

01:54:09.600 --> 01:54:18.880
I feel positively terrified, but I feel more equipped to speak to people about the nature

01:54:18.880 --> 01:54:23.040
of what's coming and how we should behave and I credit you for that and as I said a second ago,

01:54:23.040 --> 01:54:26.240
I credit this book for that as well. So thank you so much for the work you're doing and keep

01:54:26.240 --> 01:54:29.200
on doing it because it's a very essential voice in a time of uncertainty.

01:54:30.400 --> 01:54:34.720
I'm always super grateful for the time I spend with you for the support that you give me

01:54:35.360 --> 01:54:40.160
and for allowing me to speak my mind even if it's a little bit terrifying. So thank you.

01:54:40.720 --> 01:54:41.040
Thank you.

01:54:44.080 --> 01:54:47.840
I'm so delighted that we've been sponsoring this podcast. I've worn a loop for a very,

01:54:47.840 --> 01:54:52.640
very long time and there are so many reasons why I became a member, but also now a partner and an

01:54:52.640 --> 01:54:57.520
investor in the company. But also me and my team were absolutely obsessed with data-driven testing,

01:54:57.520 --> 01:55:01.360
compounding growth, marginal gains, all the things you've had me talk about on this podcast

01:55:01.360 --> 01:55:05.920
and that very much aligns with the values of Woop. Woop provides a level of detail that I've never

01:55:05.920 --> 01:55:10.960
seen with any other device of this type before, constantly monitoring, constantly learning,

01:55:10.960 --> 01:55:14.640
and constantly optimizing my routine. For providing me with this feedback,

01:55:14.640 --> 01:55:19.360
Woop can drive significant positive behavioral change and I think that's the real thesis of

01:55:19.360 --> 01:55:23.440
the business. So if you're like me and you are a little bit obsessed or focused on becoming the

01:55:23.440 --> 01:55:27.520
best version of yourself from a health perspective, you've got to check out Woop and the team at

01:55:27.520 --> 01:55:31.840
Woop have kindly given us the opportunity to have one month's free membership for any

01:55:31.840 --> 01:55:39.120
one listening to this podcast. Just go to join.woop.com slash CEO to get your Woop 4.0 device

01:55:39.120 --> 01:55:52.720
and claim your free month and let me know how you get on.

01:56:06.160 --> 01:56:08.800
You got to the end of this podcast. Whenever someone gets to the end of this podcast,

01:56:08.800 --> 01:56:12.880
I feel like I owe them a greater debt of gratitude because that means you listen to the whole thing

01:56:12.880 --> 01:56:18.080
and hopefully that suggests that you enjoyed it. If you are at the end and you enjoyed this podcast,

01:56:18.080 --> 01:56:22.560
could you do me a little bit of a favor and hit that subscribe button? That's one of the clearest

01:56:22.560 --> 01:56:26.080
indicators we have that this episode was a good episode and we look at that on all of the episodes

01:56:26.080 --> 01:56:31.280
to see which episodes generated the most subscribers. Thank you so much and I'll see you again next time.

