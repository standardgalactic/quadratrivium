{"text": " So, welcome to this week's lecture. So, seminar structure. Next week there'll be some linear combination of Gayog and Geordi, and the constant is yet to be determined. The week after, so after next week we move into the talks from experts. So, we've been trying to give you background and then the experts will tell us what's really going on. So, I'm really excited by the lineup. So, Adam Saltbaugner has done some amazing work using reinforcement learning to construct counter examples in graph theory. If you want to prepare for his talk, the paper is really beautifully written. Bandad Hosseini works on graph methods. So, we'll see a bit of graph neural nets today and we'll see more in his talk. Carlos Simpson is talking on a kind of archetypal garden of how to use machine learning and mathematical proof. And I think that that's extremely interesting. And then there's another three talks lined up after that, taking us to the end of semester. And the other thing I wanted to point out is Joel has been doing an amazing job with the toe labs. If you've been taking part in the tutorials, you'll be aware of this. But if you're watching online and you just want to muck around with something, they're really great and I'm hoping that they provide a good resource. So, we'll hope to keep these going for the expert talks so you can play with some of Adam's ideas, play with some of Bumdud's ideas, et cetera. So, again, the seminar principles. So, today we'll be about geometric deep learning and geometric deep learning is very much the question of how do you incorporate symmetry into a learning problem. But before we go into that, I just want to recall the notion of an inductive bias, which is a very important notion in machine learning. So what's going on here? So it's very common situation in maths that you have some problem and you know or suspect something about the solution. So a basic example would be something like we expect the solution to be smooth or we hope the solution is smooth. The solution should satisfy conservation of energy, for example, there'll be some differential equation that the solution should satisfy. The solution should be invariant under a group, so this occurs all the time in physics. The solution might be locally determined, the counter example, so also we might be looking for a counter example, we might suspect something about the counter example. So the fancy names for this are inductive bias or prior. So inductive bias just means something that you know about the problem a priori before starting to solve it. And it's very important to remember what inductive biases there are in a problem. So how does one imagine that one has some inductive bias? So for example, the solution might be smooth. So this is related very much to regularization. So in this was Gayog's talk last week. So for example, he talked about this Gaussian kernel, which very much encourages functions that have you know Fourier modes that aren't wiggling around too quickly, that are smooth in a very strong sense. Solution should satisfy conservation of energy, this might be another example of regularization. There's also these fascinating things, if you want to have a Google called physically informed neural nets. So here you don't require the solution to satisfy some differential equation, but you add the differential equation to the cost function. And so it encourages the solution to satisfy a differential equation. Problem should be the solution should be invariant under. So this is today. That's the subject of today. The solution might be locally determined. So this is like examples might be CNN's or LSTM. Okay we expect for example small part areas of small parts of an image to play an important role initially. And the counter example is probably highly connected. I included this example because this is an example where I've got no idea how to put this kind of information into a network. And this was definitely my experience with DeepMind in that often, so the movement from an inductive bias to the neural net design is art rather than science. So several times with the DeepMind team I said oh you know I know this about the solution and they said we have absolutely no idea how to incorporate this into the model. And I think that one should be aware that this is often a big issue. So yeah one should be just be aware that it's very important to have some kind of inductive bias and be aware of it but you might not know what to do with it. And this is just the same slide that very similar to things that Georg has been saying. You have the capacity of a model roughly speaking how many parameters it has and there is this playoff between simplicity and expressivity. So a lot of parameters means you can express for example any function but training may be infeasible you might have hundreds of billions of parameters and it's less interpretable and so there's often sweet spots but in this trade off. Okay so today what I'm talking about is symmetry in neural networks and I'm really extremely happy to give this talk because it's been kind of a revelation for me. So I began with this book Geometric Deep Learning by Bronstein Bruner Cohen and Petar Velichkovich who I worked with on the DeepMind project and Gayog pointed out this group here with very convolutional networks and Petar recently just pointed out this steerable CNNs and it's very interesting subject and I think it's a really remarkable example. So in physics we often see this phenomenon where just knowing some kind of symmetry is present has enormous effects on your ability to solve a problem or formulate a model or something like that so there's this extraordinary paper I think it's by gross called symmetry and physical laws which I found really inspiring and I find this equivariance in convolutional neural nets to be a kind of similar story it's like it seems so innocent to require some kind of invariance and yet it essentially determines your entire architecture it's really remarkable. So never underestimate symmetry I had this on my web page for about 10 years as the first thing that you read. So I want to review what kind of vanilla CNN is and then so I'll first just remember what a CNN is and then we'll view a CNN through the language of group theory and I want to try to convince you that three basic principles already basically determine CNN architecture. So here's my image in the top right so this is a grayscale image I'm assuming that it is on a square so I have a fixed width and a fixed height a fixed number of pixels wide a fixed number of pixels high and I have my pixel value is given by a real valued function so for example this pixel value might be between zero and 255 and what I'm looking for is some function from this is called a so I'm calling this a periodic image because I'm kind of wrapping the top you know the physicists would say I'm compactifying on the torus and you know I sound very fancy when I say that but we're just simplifying our situation by imagining that our image is on the torus and there's it just simplifies the group theoretic discussion in a second and there's no genuine need to do this. So we're seeking some function from periodic images i.e functions on this grid to the real numbers which are for example positive on tigers and negative on non-tigers is a classic and a machine learning problem and also the problem on which machine learning has been wildly successful and we do this in the following way so we have several layers and typically these layers will consist of other periodic images perhaps at lower scales so when in the first two layers of this neural net I'm assuming that my periodic image is the same scale and then in the third layer I'm assuming that the periodic image has dropped in scale somewhat so I'm this h is assumed to h prime is assumed to divide h. So what one should think about this problem so this problem this seek so we're seeking a function from here to here and this function is going to be highly non-linear so it's a non-linear function on this big vector space and I guess one of the points of machine learning is that learning a highly non-linear function on a high dimensional vector space is a very difficult task and we get enormous amount of mileage out of viewing it as composed of out of rather simple functions so the simple functions that we use are convolutions so this might be like we might have some kind of filter here the filter and this might be for example 8-1-1-1-1-1-1-1-1-1-1-1-1-1 and this particular filter we point wise multiply with our image and then sum up the result and so this particular filter would have the effect that on areas of blank color we would get a very low value but on out on edges we would get a high value so it would be have a kind of um outlining effect so this is an example of something that we might uh convolve with so that's applying this filter can be rephrased as a convolution and what we do so we convolve and then we apply apply a value and then we convolve and we apply a value and then we do an operation called pooling that I'll basically ignore here which might be you look at a grid of pixels and take the maximum element so this will take you down to a smaller image and then finally at the end you might do some fully connected layers and the point of all this is that we don't we specify this architecture uh at the beginning and we specify all the values at the beginning but we learn the convolutions that's the important point we learn these filters okay so that was meant to just be a review now I want to look at this through the language of group here so this is just a direct copy of the previous architecture so z mod h squared sorry z mod hz squared is a group so it's z mod hz times z mod hz very simple abelian group and as with any group it acts on functions on that group so if I have a function on my group what I can do is translate my group around and I get a new function okay and convolving by a single so when I translate around by my group this is the same thing as convolving with a delta function on my group so on my group I can consider the function that's just one at a particular group element and zero elsewhere and convolving with that element is the same thing as translating by that group element and uh so any any convolution is a linear combination of these convolution with a single with a delta function and so this is gamma equivalent so another way so in the abelian case kind of nothing nothing matters in terms of orders you know when I wrote g dot f of x equals f of g plus x it doesn't matter whether I whether I write x plus g or x minus g but in the non-abelian case I'd have to have an inverse and in the non-elabelian case I would kind of think about convolutions as maybe acting from the right or something like that but it's a general fact that the if we look at functions on a group then the equivalent maps from functions on a group to functions on a group are the same thing as functions on that group acting by a convolution and that's what I that's what I say here so the equivalent maps from functions on the group to functions on the group are simply functions on the group okay and this is true for any gamma this is very basic representation theory if you will so remember that I hate questions and any question will involve um a horror show so please don't ask any questions so what are the basic observations about this so we want often in these problems we want a gamma invariant answer so if we move our picture of a cat around if we translate around the answer should still be cat okay this is one of the reasons that I assumed a periodic periodic image that's very important another point is that so there's another classic machine learning task which is called image segmentation which asks us to say where are the two eyes in this picture or you know when your phone for example tells you where the head of the people this is an example of image segmentation now if you think about what this task is this is not invariant it's equivariant this means that if I move the image my my prediction should move in the same way so we often want a gamma invariant answer or a gamma equivariant answer okay convolution and value are gamma equivariant and for simplicity I'm ignoring pooling layers but we can definitely add them into the discussion but I feel like the the guts of this business really is exposed when we ignore pooling so I'm going to do that from now on so so convolution is equivariant and and value is equivariant if we so we have our image we have a whole bunch of real numbers if we translate and then set some of those numbers to zero that's the same thing as setting some of those numbers to zero and then translate locality yeah no any activation function would be equivariant but as we'll discuss in a second it's really essential that these are permutation representations for an active function activation function to be equivariant so so we have requirement one we want a gamma invariant answer requirement two is that everything going on in this network should be gamma equivariant requirement three is locality so what this says often in you know if you look apparently at the early layers of the brain in the visual cortex what happens is local and so and it's very natural to also impose this in a cnn so we our our filters are supported around the identity initially and then later on we let them grow out through pooling and potentially fully connected layers so an actual cnn we wouldn't require we wouldn't look at periodic images and what we would do is pat around the edge um to make all our images the same size for example yeah and we would only have we the same principles would be there but we wouldn't have a kind of full symmetry that only makes sense to shift pictures a little bit but what I find remarkable here and it's kind of a simple simple thing is that gamma invariance gamma equivariance and locality basically tell me what I what I have to do in my neural net so if you assume that you should compose it out of simple functions and if you fix relu then everything else is basically specified which is remark which seems to me to be remarkable and what I want to explain soon is that there's kind of nothing special about this particular group that this would actually tell us how to make predictions on any space on which a group acts transitively so let me just emphasize one extremely important point from a implementation point of view okay so just for completeness gaug asked here whether relu was specific for it being equivariant and the response was no any non-linear activation function would be fine at this point as long as our representation is a permutation representation and what Stefan asked was in this particular picture here um you know for completeness if we translated this little cat we'd have half the cat's head over here and half the cat's head over here and the question was you know how does an how does an actual cnn in real life do this and basically you know we don't we don't enforce that full equivariance we only allow kind of small translations within some bounded region so kind of partial symmetry thank you very much for the reminder so this is very important from a basic implementation point of view imagine on the left hand side we have two we this is a layer of our neural net and so we have l1 inputs and l2 outputs single layer so now if you think about the number of parameters it's l1 times l2 so if it's easy to find a picture for example with 10 million uh 10 million pixels in it and if we do one layer then that's whatever 10 million squared is you know i'm not a physicist i don't know what 10 million squared is okay so some enormous number that i can probably never actually train on a computer and but if we're doing so here here i have a first layer of a convolutional neural net in one dimensional in one dimension so here are my inputs and locality says that my filter only affects neighboring points so that's why i have these three parameters x y and z and equivariance says that these x y and z are the same across the whole thing so no matter how big this layer is it just depends on three size um three parameters okay so i should emphasize this is one piece of the first layer so up here this would be one of these pieces so i could expect to have potentially 20 of these pieces or something but still some you know 20 times 3 is a lot smaller than 10 million squared i'm enough of a physicist to know that inequality okay so now i want to explain a blueprint for learning on a general homogeneous space for a group so we have our group and i'm typically thinking about a finite group or a league group like so three or something like that so gamma was z mod h z squared before and x is a transitive gamma set so this just means that um gamma acts transitively on x but in the category of leap in the category of differential manifold this would mean that i have a manifold with a continuous action of my or a smooth action of my league and in any situation in which um this makes sense we have that x is just the same thing as gamma mod a single stabilizer and what we want to do is learn an invariant so i'll stick to the invariant case but notice that we might also want to make an equivariant prediction function from functions on x to r now very basic representation theoretic observation or maybe so basic but it's not yet representation theory is that because our action is transitive there is only one linear uh or at most one linear map from functions on x to r that is invariant namely like summing over my finite set or integrating or so i found this kind of illustrative because this tells you for example in the image classification task you're definitely looking for a nonlinear function because a linear function would be like averaging over pixel values and this is the kind of silliest thing that one could imagine so we're looking for some um invariant function and here's the blueprint so we fix a transitive gamma set this is where we want to make the prediction and then we just our architecture consists of a whole lot of choices of transitive gamma sets and basically i think one way to think about these transitive gamma sets is so the the invariant prediction says that you want to take um so any gamma has an important transitive gamma set namely one point and this is where we want our prediction to end up and then if you look at um classical c and n's you want your sets to kind of slowly get smaller in some sense until you reach the prediction so you can think about these transitive gamma sets as slowly decreasing in size if that makes sense and this is all we do so we um consider some equivariant maps so convolution so this should be a gamma a gamma equivariant linear map and then we never do a relu and then we do another one and then we do a relu and then we do another one and we want to train across the parameters of gamma equivariant linear maps yeah and probably this is a point of it so this whole space is r so if gamma has a metric or similar we might want convolution supported near the identity now this is the most important point and i think that this has kind of been missed in the in the machine learning literature there's something very basic in representation theory which i call the double coset formula people might call it hecar algebras there's many different names for it so we're asking what is such a map so because any transitive set is simply gamma mod h or gamma mod h prime we want to know what this home space is and the formula says that homomorphisms from such a function space to such a function space are simply functions on double cosets now there's many different ways to understand this formula if you if you're in the world of finite groups this is a very nice exercise if you're in the world of compact league groups it's a significantly more difficult exercise but i just want you to accept this formula as a kind of beautiful thing in the world and we'll see it's very useful okay and if you want to know more about it i'm very happy to talk more about this formula okay but this is a kind of i don't know very useful formula in many different situations so this is telling us what the possible space of convolutions is so here's an example imagine that we're learning on a sphere so we have a nice sphere here and we have so three so this is um orthogonal three by three matrices of determinant one so these are these transformations so it acts on the sphere and s2 is a transitive space i can move any point on the sphere to another point on the sphere of iron orthogonal transformation what is the stabilizer of single point it's those rotations in the axis that point determines through the origin so s2 is s03 mod s1 now imagine that we want to learn on the sphere so we want to have some image on the sphere some function on the sphere and we want to say it's a cat or something you might ask i don't generally see pictures of cats on spheres this is my answer to that okay this is a beautiful article in quanta so this is the cosmic background radiation no absolutely extraordinary thing from around about 2003 where we see the early what the universe looked at early on and we do this by basically going around the world and looking out into space and so it's an archetypal example of an image on a sphere so sam has a question um we'll just i just want to admire this picture for 10 more seconds so i'm told that you can see the fluctuations of quantum field theory in this picture so this is a very early universe so it's when the universe was very small and you expect the behavior to be given by the laws of the very small and i'm told that you can see evidence of quantum field theory in this picture that totally blows my mind okay so sam's question for discrete finitely generated gamma could support near the identity be regarded as having sort some sort of choice of generator that's a really good question i was thinking about what the support nearly up near the identity kind of means so in the example of the cnn we have this discrete group which has no no really convincing metric on it but it is embedded in s1 times s1 that does have a good metric on it and so for groups that come with some embedding we can put a metric on them but i also think that that's a good suggestion if you have a some kind of uh what's what's that distance you're talking about it's um like kind of distance in the kaillie graph might also be a a decent measure of locality i also want to try to explain in a second that for a non-Abelian group locality is less important so so the building blocks so what are the homogeneous spaces for so three so this is in a league group so i can ask what are the dimensions of the subgroups of s03 so there's a whole lot of interesting finite subgroups of s03 for example the symmetries of the icosahedron form a very interesting subgroup of s03 and um and then you have the two sphere and rp2 and then you have a point and that's it okay so our building blocks are rather restricted which is interesting and also i would say that we if you're employing some kind of practicality in building your model you don't want complicated things like s03 modified subgroup so and rp2 and s2 are very very similar you know one is just a two-fold cover of the other and so i would advocate building a building a network which just involves functions on spheres and functions on a point so this is the proposal for a blueprint for learning on the sphere and also i am aware that it's very difficult for a computer to understand a function on s2 okay but this is meant to be some kind of blueprint that you then try to interpret and sometimes you know to have the idea of what you're doing very clearly in your head is very useful when you come to implementing something here we have h equals h prime exactly so i'll go through the double coset formula in two examples now so the point the the problem here which geyog is pointing out so geyog was asking which which subgroup are we using the double coset formula in here i just want to first say why we're using the double coset formula we want to know what are these maps what are our possible so three every variant convolutions here so what are the so three every rank convolutions so this is the double coset formula again this is our friend okay i'm just specializing the double coset formula for so three so that made the formula much less easy to read so i'll delete it again okay so what does this say let's first do a silly example what are the homomorphisms from s03 mod s1 i.e s2 to s03 mod s03 namely a point so i said as an exercise in very great generality the only such function is given essentially by integrating over your space up to a scalar but let's see it pop out of the double coset formula so s03 mod s03 is a point yeah so we've got functions on a point what's more interesting so this is the kind of silly example what's more interesting is what are the rest of the layers you know so just to emphasize here this is telling us that even though this is an enormous vector space with this only one scalar possibility of of maps here so this belongs to r this belongs to r this belongs to r so now ignore the integral bit at the moment just look at this so what are the s03 equivariant homomorphisms from functions on s2 to functions on s2 they're functions on by our double coset formula s1 mod s03 mod s1 so that's the same thing as s s1 mod s2 so i take s2 and i have s1 rotating it around and then i quotient that out and our representatives for that quotient space are just an interval stretching from one pole to the other so functions on that interval so what the hell are these intertwiners so i should say that such an element inside here is called an intertwiner so intertwiner is synonymous with s03 equivariant linear map so i can look at one way to understand these things is to try to look at delta function so a delta function at the identity at this base point is just the identity a delta function at this end is the antipode but what the hell is going on in the middle you know you're you're seeking a continuous family of operators which interpolates between the identity and the antipode okay looks like a tough ask but there's a really beautiful thing you can do which is you consider the following operator on function so i have a function on the on the two sphere so this is in on s2 and now and i have a gamma and i can consider a new function which at a point x is given by the integral around a loop of my original function at distance gamma from my point there's a way of producing a new function so i've told you how to take a function s2 get a new function on s2 and it's a beautiful thought exercise that this is invariant this this is equivariant okay so if i move my function and then do this operation that's the same thing is doing this operation and then moving my function okay so these are these intertwiners so so that's the answer for what all these maps are and of course like that's still a infinite dimensional vector space but compared to you know functions on like if you're just thinking about linear maps here that's something like functions on s2 times s2 so it's like a four dimensional roughly speaking and it's kind of remarkable that just in employing this equivariance massively cuts down the number of parameters and of course you can make this whole picture even richer using spherical harmonics and there's like incredibly nice functions to put in here projecting to the irreducible representations inside functions on s2 etc i should have said very very much earlier like for me fun is just firstly it's to remind us that we're having fun secondly um it's just some kind of class of function so when i'm talking about the sphere i'm probably talking about l2 functions when i'm talking about a um discrete set i'm just talking about any function etc so yeah the point is that when we have a non-Abelian gamma there's a big reduction in parameters so um in the cnn slide there was this equivariance plus locality drastic reduces the number of parameters and here i'm kind of saying that equivariance plus non-Abelian drastically reduces the number of parameters which i think is very interesting so my task for myself and if you have any ideas i'd love to hear it is find an interesting learning problem where the symmetries are an interesting non-Abelian group what's a learning problem where the symmetries are naturally sl2 fq or something like that or you know some interesting groups so a lot of the groups that show up in machine learning are very much related to um three-dimensional space or two-dimensional space or so like p4 which consists of um all translations and 90-degree rotation shows up a lot and stuff like that but it would be lovely to inject some really interesting groups into this oh gl2 yeah gl2 would be great yeah that's it so stefan just suggested learning on hyperbolic space and that's a great great suggestion yeah i don't know why i didn't think of that i had fl2 uh sorry yeah so learning on okay there's enormous um possibilities here that i think are very interesting so let let's just go back to cnn's so the the question is basically like what the hell's going on so let's imagine so i'll try to explain um what the hell's going on and then we can have a short break so let's say that we're trying to do image processing so we're z mod hz squared we have functions on this and then we we want to have a layer of our neural net so typically one layer of our neural net might be like this you know one piece of one layer of our neural net might look like this so now what's the dimension of this space the dimension is h squared namely the number of points in the set and what's the dimension of this space the dimension whoops the dimension is h square okay so now if i were doing a fully connected neural net i would have h squared basis vectors here h squared basis vectors here and then i would have an h squared times h squared matrix so i'd have an enormous matrix of size h to the four that's and each of those parameters i have to train okay what do i do in cnn's i say i want this matrix to be invariant that already cuts down the number of parameters from h to the four back down to h squared and then i say i want this matrix to satisfy locality and that cuts down my number of parameters from h squared to nine so harini is asking uh what basically like why do you restrict the parameters to this extent so i would say that there's two reasons for this so these are inductive priors so they're not there's something that i believe is true about the solution they're not something that like is definitely true about the solution there's some category of practicality i want to build a model that works i can't train a hundred billion parameters but i can train you know a hundred or a thousand fine yeah and the inductive priors in this are invariants namely i can see you i can still see you yeah that's you know like you're still there like that's invariance yeah um and the other thing is locality and i think locality makes a lot of sense like when i look at this room i don't think the first thing that happens in my brain is i think oh i'm in kaslo 273 what happens first is i go oh edge corner chair person stairs light looks like a lecture hall sydney probably kaslo 273 yeah and so that's invariant that's um locality and these are our inductive priors and these inductive priors massively cut down the parameters and then from then we're cooking on gas and we can get these models that actually work no but it's again like changing the group is a gain inductive priors so um for example like you know have you been upside down and you look and it's actually much harder to recognize stuff so the idea that we satisfy this invariance is much less well established than the idea that we satisfy this invariance and then we want to bake in that symmetry exactly yeah i don't know if you really want rotations so like a classic pre-training task in image recognition is to recognize whether your image is upside down or not so you know that's a classically non-invariant thing under rotation by 180 but there's there's situations like you know in an MRI scan or something where it really you know you really want that invariance that's when you should bake it into the model if you go back to our friend l2 of s2 there's a great exercise so the laplacian okay so the casimir gives you the laplacian on the sphere and the eigenspaces for the laplacian are the spherical harmonics so the the casimir is acting everywhere in this whole big diagram commuting with everything and splitting it up into irreducible it's not so much locality it's the fact that so three is maybe i can write so you know l2 of s2 is a topological direct sum of l gamma where gamma is is a spherical harmonics and the the casimir is providing this decomposition so it's breaking breaking up this space so the casimir on each one of these acts by a different scalar and it's the the casimir aka the laplacian acts on each of these you know this is something like restriction of of degree gamma or gamma over two polynomial polynomials and so you have this totally canonical decomposition of this space of functions and everything wherever i had this picture everything here is respecting this decomposition all the linear maps respecting this decomposition and roughly speaking you can kind of think about like you know if you take your function here and do a Fourier expansion of it then you get a whole lot of quantities and those quantities are giving you the you get a whole lot of scalars and those scalars are basically telling you how it acts on each one of these okay so this is just super interesting for me but maybe more technical uh so i want to go in the second half i want to go over why permutation representations i want to go over something called deep sets and then i want to go over graph neural nets so one question that is very natural to ask as a representation theorist is why so if you take functions on a set on a gamma set this is called a permutation representation and it's called a permutation representation because if you look at the matrices that represent your elements they're permutation matrices and in our first class in representation theory the first representations we see a permutation but then we quickly convince our students that we should break up permutation representations into irreducible representations and that's really interesting so why am i insisting that we have permutation representations everywhere so i i found that this charming little lemma which i i didn't find in the literature which is if you have a representation of gamma which is assumed finite i'm not quite sure what the analog of this for a league group is then we so once we have v we can choose a basis for it and once we have a basis we can ask is relu gamma equivalent yep so remember relu takes our vector which now that we have a basis is just a sequence of real numbers and the ones that are negative it sets to zero and the ones that are positive it keeps so is this gamma equivalent if our representation is just permuting our coordinates around it's easier to see that it's gamma equivalent but uh that's if and only if so in order to have a gamma equivalent relu with respect to some basis you have to be permutation with respect to that basis now this is something that has been exciting me a lot over the last few days and is having a hell of a lot of fun with is basically like piecewise linear representation theory um so what you could imagine is you you have these layers and they all break up into irreducible representations and if you include an irreducible into a permutation representation do a relu and then project back you get a piecewise linear endomorphism equivariant endomorphism of your representation and so what you could imagine is networks in which you have irreducible representations together with equivariant non-linearities and my impression is that this is extremely interesting and I've already learned like basic things about representations that I didn't know from thinking about this so if you're interested in this ask me but it's kind of much more specialized so I'm not going to talk about it today so let's I want to do another example of our blueprint so one way of seeing this is just first think about this so imagine that we have a whole lot of points and they're unordered and they have a whole lot of information attached to them for example you could imagine all of the citizens of Sydney and they're labeled by their age and how much tax they paid last year yeah so I've got a two dimensional vector associated to every person in Sydney now one way of viewing this data set is as a point cloud so what I have is an enormous in this you know age and tax example I just have R2 and I have an enormous number of points in R2 and I want to make some qualitative statement so a basic statement that I could make is some kind of like center of mass statement like the average age of people in Sydney is blah that would be a kind of boring measurement but a much more interesting measurement would be there's this kind of weird hole in this data for example you know a particular age in a particular tax is you know not paid in some region or something like that that would be a much more interesting statement you can make about this data analysis and there's about this data and this is one of the one of the this is related to this very interesting subject called persistent homology which I know next to nothing about uh okay but we have a point cloud so we want to make a prediction based on this point cloud and so this is an equivariant prediction task we have so uh so here we have rd to the n so here are our n different points uh and we want to learn some function you know like is there a big hole in this data or something like that um and it's it's convenient to swap the indices so if you think about how s n acts here it acts like I have a whole packet of numbers um and then it permutes them around but it's more useful to view this from the from the point of equivariance as one packet of numbers like age that's being permitted around and one packet of numbers like how much tax was paid being commuted around and so I'll do this innocent rewriting um but I'm just pointing this out so it doesn't confuse the hell out of you on the next slide uh so we want to make an s n invariant prediction so basically we have in the language of representation theory we have a whole lot of copies of the most basic permutation representation of s n namely s n acts acting on functions on an n set and we want to make an s n invariant prediction so let's do some basic representation theory which I almost certainly learned from andrew at some point in about 30 or something so we take functions from one up to n functions on the set one up to n so this is a permutation representation of s n and we take the trivial representation so this is where every permutation just acts by the identity so here's a whole lot of homomorphism formulas so this is before I was saying what are the arrows in my neural net here I'm working them out explicitly what's what parameters there are so home from the trivial representation of the trivial representation this is the same thing as home from r to r this is r times the identity home from n to one this is another instance of this statement that on a permutation representation the only invariant measurement you can make is essentially summing up your entries that's that home back the other way here if we look at the image of one we want some vector we want some function which is invariant under s n i.e. we want this function to take the same value everywhere which is alpha now a little bit trickier a little bit like you know this would be a second week exercise in representations of the symmetric group or something is that home from the trivial from this permutation representation to itself is two-dimensional and it's spanned by the identity and the map that sums up the coordinates and and takes that sum and multiplies it by the constant function now exercise deduce this from the double coset formula all of these formulas are very easy concept consequences of the double coset formula why is that the case do it if you're a student you should do this and if you're a student it's not already obvious to you you should do this so i this is deep set architecture so you can look at so here's here's our paper from 2017 and to me as a non-machine learning person it looks a bit mystifying but this is just another instance of our blueprint so here's our input so this would be our three three-dimensional point cloud input yep so we have um three parameters per point now we do s in equilibrium maps and we uh sprinkle around ends and ones these are our building blocks now note how crazily this reduces the number of parameters for a large n so here if we had no assumption of s in equilibrium this would be an n squared space of parameters you know and n could easily be a million or something but now because we want this to be s in equilibrium we've just got two two possibilities here we've got one possibility here we've got one possibility here we've got one possibility etc so this allows us to make enormous um networks involving you know hundreds of billion you know billion dimensional things um with few parameters okay and that is um deep set architecture and it's i think it's um state of the art in terms of point cloud prediction okay graph neural nets if there's no questions uh graphs are everywhere in mathematics they come in many forms and variants so i just want you to keep in mind that graph here is a very um loose term it might be a directed graph it might be a digraph a directed graph the edges might be colored the vertices might be colored the edges might be weighted the vertices might be weighted the vertices might have 10 parameters associated to them we might be talking about hyper graphs so that's graphs where we have like an edge need can connect more than what more than two two vertices etc so it's whole plethora of things called graphs and just want to emphasize that there's many ways so graphs are everywhere in mathematics but once you start thinking about them they're even more everywhere because there's a whole lot of stuff that wasn't obviously a graph initially and then you can make it a graph so examples of this uh you might say well you know graph theory is one dimensional topology and i'm a sophisticated eight dimensional topologist and i only care about eight dimensional manifolds yeah but if you take a compact eight dimensional manifold you can choose a point cloud on it and you'll get a graph and that graph tells you enormous amounts about that eight manifold if you have a simplicial complex you know for me like graphs are just one dimensional simplicial complexes and so i said to petah oh we should be sophisticated and learn on simplicial complexes and he said well a simplicial complex is just a graph truly you know here's my triangle here are my edges and here are my vertices it's a colored a simplicial complex is a colored vertex colored graph okay of a special form so this is another example this is from gaorg so if we have a a data set and it's somehow embedded in a space then we can get a natural metric graph out of it by looking at distances between vertices we might include the coordinates here we might do some funny function applied to this these lengths etc okay so graphs are everywhere and graph neural nets seem to be an incredibly powerful flexible way of dealing with um data so i think the graph neural nets have kind of really genuine like c and n's the thing that's that we stare at as mathematicians and think how could we make something like this that would help us in mathematics but i think that graphs are actually a thing that will help us in mathematics all the time so that's very worthwhile thinking about so what the graph neural nets do an example we might want to learn a function on graphs so an example would be a function which is learn planarity okay so this output's a positive number if it's planar a negative number if it's not planar so that would be a prediction task on graphs um we also might want to know for example the Euler characteristic of the graph that would be another example of a prediction task another important example kind of more like image like generalized image recognition is producing um some learning some function from functions on graphs to r so you might think that so no you you might repackage c and n's as being a grid and then an image is the same thing as a function on the vertices of this grid another very important thing is that um like there's many many incredibly interesting for example embedding problems of graphs so you give me a graph and I want to put it in some space in an interesting way um and one way of doing that would be to provide coordinates of where I want to put the vertices of that graph and so that would be an example of learning a function from graphs to functions on the vertices of a graph uh so I guess the takeaway from this is that anything to do with graphs graph neural nets uh useful for as long as it's not like an NP hard problem on graphs of which there are plenty yeah so graph neural nets aren't going to help you solve something like is there a Hamiltonian circuit or something like that so what's the basic idea so imagine that I give you a graph and you want to learn on it it's enormously difficult as far as I can work out to work out the automorphism of a group of a graph so this is something that people spend many many years thinking about from an algorithmic point of view and so I might not know what global symmetries are present so what I was talking about before does not apply um or that just like most graphs have no symmetry whatsoever um or you might so Gaston is asking what do you mean by hard um so that I mean yeah maybe maybe like NP or something like that but I just want I like in my mind there's there's stuff on graphs which is useful and maybe not so crazily difficult like um like embedding your graph in a nice way or something like that and then there's a whole lot of like seemingly innocent problems on graphs that are extremely hard um like embedding a graph improvably the nice the best way or something like that or finding a Hamiltonian circuit or stuff like that okay so in graph land it's easy to wander into an intractable problem um but there's also a whole lot of useful stuff that can be done so so there's plenty of local symmetry in graphs so around every vertex we have a symmetric group of symmetry and also we have a metric so you can imagine processes which are symmetric and kind of diffuse on the graph and that's what a graph neural net is so I'll quickly go over the architecture so this is an important slide so here's my graph and as part of my architecture I fix n1 n2 n3 and of course I'm just telling you one possible variant of like a thousand different possibilities of building graph neural net but once you've seen one of them then the other ones make a lot more sense so we fix these n1 and then what we do is each of our layers is a sum over the vertices of that particular rn1 okay so you know in a vanilla neural net we just fix dimensions here we fix dimensions at every vertex so that's this and so in this particular case my my neural net looks like this so I have three layers so here I have some linear map here I have a relu here I have a linear map a relu and then a fully connected layer okay so what do I do basically I train self and neighbor maps so here's the formula down here so I'm telling you what phi x of v so here's my my layer which is phi and I wanted to find you this map and in order to do that I can tell you this map evaluated at a particular vertex so that this might be vertex v and what I'm saying in order to get that answer what you do is you take this self map times whatever I've got here plus all of these neighbor maps so roughly speaking in my second layer something here has a term that comes from here together with terms that come from the three neighbors so it's a very natural it's called a message pass and as usual si is something like is affine linear okay so s1 is an n1 times n2 matrix plus an n2 vector okay so each of these so this is yes thank you Brian this should be an s2 yeah so that's very important that so this is an another example of an inductive prior sorry inductive bias or a prior so what we're saying is that we want these n1 these n1s sorry Stefan asked should all these n1s be the same and the answer is in general yes so we want for example the n1s that talk to this guy from here and from here to be the same n1s that talk to this guy from here here okay so the n1s are the same so if you imagine this matrix here it looks like something like s1 s1 s1 s1 down the diagonal and then n1s in off diagonal places given by the adjacency matrix you know something like this so inside this space of like n1 times vertices times n2 times vertices so this is what my big matrix would look like I'm saying like it should be block diagonal and a whole lot of blocks should be the same so it's a very strong inductive bias to assume now if I'm honest you know we might have seven like let's say two different colors on our vertices and then we would train um neighbor maps that preserve the color of vertices neighbor maps that change the color from red to blue neighbor maps that change the color from blue to red etc so there's a zillion variants but in the basic vanilla version of a neural net we assume all the n1s are the same and all the s1s are the same so this one's so I'll give the diagonal term of this s1 so it's a very complicated slide but it's a simple idea I think so we do that for a number of times and then we evaluate or in the situation where we're trying to learn for example and embedding or something like that we wouldn't do the final layer we've got some coordinates on our vertices and we're happy another classic example of a task might be you want to divide your vertices into two classes and so then you would you'd do all your layers and then you would say at the end uh this is a real number and then you would softmax that and then that would be the probability that your vertex is in or out of this class okay and also there's you know a million variants often the neighbor term is weighted by one over the degree of the vertex okay that's what I set up there it's affine maps okay so that's the architecture and we'll have some fun playing with this architecture in the colab tomorrow so many variants are possible for digraphs digraphs you might think that you only train a forward map but generally you don't you train a forward map and a back with map so back forward and backwards neighbor maps if graph has edge coloring you can train message passing for every color of the edge vertex coloring similarly you might also have a couple of global parameters hanging around and in every step your global parameters speak to the parameters on the graph and are spoken to by parameters from the graph in some way which is probably nothing to do with the n ones on the s ones so these are some nice examples imagine that I consider this digraph here if you look at what a graph neural net does on this digraph it exactly mimics cnn's without falling layer so you know it's it's it's not it's not an exaggeration to say that cnn's are a subset of giant graph neural nets and if you kind of unpack this definition for uh deeps for a complete graph you uh basically get deepsets and that is all for today so thank you very much and yeah if there's any questions please ask so gay i'll ask a question i'll just ask uh answer the question of dr bouts you first I wonder if by adding linear maps between vertices of the graph and making the whole architecture commutative kind of quirl net has any ml interpretation yeah so I totally agree that when you look at neural net architectures it looks very much like river varieties and things that we study in representation theory however one really can't underestimate the effect of this relu of like so if you think about a like just a classic feed forward vanilla neural net um if you don't have the relu's we understand totally what happens by basically near algebra but when we add the relu's we can suddenly approximate any function on a compact set so and we we have absolutely no idea of what happens inside the the neural net so yeah I find the quiver language like very useful to think to think about but um one shouldn't underestimate relu's so gaug asked um what have graph neural nets been used for um and my understanding is that um like I don't know like let's say half of facebook and 75 percent of twitter is graph neural nets um because you've got all these like connection graphs and social networks and stuff like that um graph neural nets were like the absolute center thing that we used with deep mind too on this work on cash analysis polynomials um my understanding is that graph neural nets are kind of um taking over neural net world in terms of they're very flexible um they're very powerful um and a lot of tasks where for example c and n would involve like a drastic change of architecture um in graph neural nets you can just like add a vertex or something like that so they're very flexible powerful framework for machine learning as far as I can make out sam yates asked for discrete valued problem could we pick other group rings say with some choice of analogous relu like function perhaps uh that's an intimidating prospect for me because I wouldn't know how to train and things like that so my very vague understanding so there's this kind of revolution in the last kind of three four years given by transformers and my understanding of that is like basically like a a graph neural net um hooked on to an lstm so like the graph neural net kind of decides where to look back in the sentence and things like that so um but I'm not directly aware of any recurrent technologies used directly with graph neural nets what another thing that graph neural nets are very useful for which is kind of counterintuitive is like predicting graph structure so you have data and you want to um and you want to predict which edges exist so like you might want to predict social relationships or something like that and um my understanding is that you you start off with a complete graph and run a graph neural net and your graph kind of learns a probability of discarding an edge and that's very very powerful so it it's intuitive and intuitive but graph neural nets can learn graphs which I think is awesome I think I think like learning graphs is like learning graphs is a really difficult problem in machine learning um and yeah of course so it's going to be yeah it's an intimidating problem that or n choose tuna yeah so that's a really good question so what kind of problems and why can be addressed with graph neural nets my understanding is that very recently there's been kind of benchmarking so there's been like a list of 50 problems and some some attempt to decide you know what what can various classes of graph neural net algorithms do and what they can't do um my very rough picture as as a graph neural net greenhorn is um that anything that you can kind of decide via a finite like a small amount of walking around your graph and if you if if you're extremely smart and you're allowed to walk around your graph a little bit and you can already make a decision then that's something that you could solve so something like detecting planarity or detecting a three cycle or something this is something that you can solve but if it involves exploring the whole graph particularly potentially in many many different iterations for example finding a Hamiltonian cycle or something um yeah it's not going to work she'll have said the piecewise linear representation can you say a bit more about it sure so how about I will ask answer Joel's question and then maybe I can say something about it but I'll give other people the chance to leave because it's somewhat specialised so a part of Adam salt Wagner's work was learning graphs learning good people to a particularly conjecture for instance so we might hear about learning graphs in a few weeks yes exactly so salt Wagner's work is using reinforcement learning to produce interesting graphs okay so now we'll declare it over and anybody that's interested in this piecewise linear business can stick around I think this is super beautiful and um I will just explain it briefly so let's just consider the following silly examples so we're looking at the sn acting on s3 acting on r3 or permutation representation now we know that r3 decomposes canonically as nat plus triv and that is the set of vectors in r3 such that the sum of the lambda i is zero and triv is the set of r times 111 and this decomposition is completely canonical so now you can ask the following like just kind of totally naive question if we go from nat into r3 and then apply value go to r3 back down to nat the composition is an s3 equivariant pl endomorphism i.e in the category of piecewise linear maps from this vector space to itself that a s3 equivariant this is a pl endomorphism and what is it it's super beautiful so basically what you do is inside so here's nat so we divide up nat so just for people that don't do this every day nat is the um symmetries of the triangle embedded inside r3 r2 okay so we just take an equilateral triangle and we take the symmetries of the equilateral triangle so now um there's three regions here and what happens so there's six regions there's the blue regions and the red regions and the blue regions get squashed and the red regions get kind of expanded out so these get squashed and these get expanded and i don't know this is just like a very beautiful basic um like pl endomorphism of a representation that i've never encountered before in my life okay and you can start having fun like what is this or nat inside r in and you know this is a nice exercise and yeah so that one of the things that i find really interesting is that home pl from any representation to r um is interesting okay so example is that home from pl from the sign representation to r contains the absolute value map um like this is for this is the sign representation of s2 oh no sign representation in general in fact um but home from the trivial representation pl to any irrep not equal to the trivial zero and i kind of feel like this is telling us something remarkable that about kind of how equivalence kind of flows through a neural net so this is still very speculative but what i feel like is that you have some kind of measure of complexity so it like at the start you have all all irreps and then at the end you have the trivial and then you have the maps in the in the neural net so you have linear and then you have pl and then you have linear pl and these pl maps have a definite sense of direction like you know once we get through the trivial representation we can never get out of it again and like i don't know this seems to explain some some very interesting aspects of neural nets but it's just exciting stuff that i've been thinking about last week so very unbaked um so maybe when it's baked i can uh talk more about it so thanks are the blue rays there inside the reflecting hyperplanes this is alpha one and this is alpha two so no oh yeah the so the reflecting hyperplanes would be like this and this and this thanks everyone i think we'll stop now you", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.200000000000001, "text": " So, welcome to this week's lecture.", "tokens": [50364, 407, 11, 2928, 281, 341, 1243, 311, 7991, 13, 51024], "temperature": 0.0, "avg_logprob": -0.28359413146972656, "compression_ratio": 1.31496062992126, "no_speech_prob": 0.06148482859134674}, {"id": 1, "seek": 0, "start": 13.200000000000001, "end": 16.8, "text": " So, seminar structure.", "tokens": [51024, 407, 11, 29235, 3877, 13, 51204], "temperature": 0.0, "avg_logprob": -0.28359413146972656, "compression_ratio": 1.31496062992126, "no_speech_prob": 0.06148482859134674}, {"id": 2, "seek": 0, "start": 16.8, "end": 22.240000000000002, "text": " Next week there'll be some linear combination of Gayog and Geordi, and the constant is yet", "tokens": [51204, 3087, 1243, 456, 603, 312, 512, 8213, 6562, 295, 23081, 664, 293, 2876, 765, 72, 11, 293, 264, 5754, 307, 1939, 51476], "temperature": 0.0, "avg_logprob": -0.28359413146972656, "compression_ratio": 1.31496062992126, "no_speech_prob": 0.06148482859134674}, {"id": 3, "seek": 0, "start": 22.240000000000002, "end": 24.04, "text": " to be determined.", "tokens": [51476, 281, 312, 9540, 13, 51566], "temperature": 0.0, "avg_logprob": -0.28359413146972656, "compression_ratio": 1.31496062992126, "no_speech_prob": 0.06148482859134674}, {"id": 4, "seek": 2404, "start": 24.04, "end": 29.2, "text": " The week after, so after next week we move into the talks from experts.", "tokens": [50364, 440, 1243, 934, 11, 370, 934, 958, 1243, 321, 1286, 666, 264, 6686, 490, 8572, 13, 50622], "temperature": 0.0, "avg_logprob": -0.2504392224688863, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.028782673180103302}, {"id": 5, "seek": 2404, "start": 29.2, "end": 33.82, "text": " So, we've been trying to give you background and then the experts will tell us what's really", "tokens": [50622, 407, 11, 321, 600, 668, 1382, 281, 976, 291, 3678, 293, 550, 264, 8572, 486, 980, 505, 437, 311, 534, 50853], "temperature": 0.0, "avg_logprob": -0.2504392224688863, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.028782673180103302}, {"id": 6, "seek": 2404, "start": 33.82, "end": 34.82, "text": " going on.", "tokens": [50853, 516, 322, 13, 50903], "temperature": 0.0, "avg_logprob": -0.2504392224688863, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.028782673180103302}, {"id": 7, "seek": 2404, "start": 34.82, "end": 38.44, "text": " So, I'm really excited by the lineup.", "tokens": [50903, 407, 11, 286, 478, 534, 2919, 538, 264, 26461, 13, 51084], "temperature": 0.0, "avg_logprob": -0.2504392224688863, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.028782673180103302}, {"id": 8, "seek": 2404, "start": 38.44, "end": 45.68, "text": " So, Adam Saltbaugner has done some amazing work using reinforcement learning to construct", "tokens": [51084, 407, 11, 7938, 19503, 4231, 697, 1193, 575, 1096, 512, 2243, 589, 1228, 29280, 2539, 281, 7690, 51446], "temperature": 0.0, "avg_logprob": -0.2504392224688863, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.028782673180103302}, {"id": 9, "seek": 2404, "start": 45.68, "end": 48.879999999999995, "text": " counter examples in graph theory.", "tokens": [51446, 5682, 5110, 294, 4295, 5261, 13, 51606], "temperature": 0.0, "avg_logprob": -0.2504392224688863, "compression_ratio": 1.5342465753424657, "no_speech_prob": 0.028782673180103302}, {"id": 10, "seek": 4888, "start": 48.88, "end": 54.72, "text": " If you want to prepare for his talk, the paper is really beautifully written.", "tokens": [50364, 759, 291, 528, 281, 5940, 337, 702, 751, 11, 264, 3035, 307, 534, 16525, 3720, 13, 50656], "temperature": 0.0, "avg_logprob": -0.23171443729610233, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.060720257461071014}, {"id": 11, "seek": 4888, "start": 54.72, "end": 57.64, "text": " Bandad Hosseini works on graph methods.", "tokens": [50656, 15462, 345, 389, 14353, 3812, 1985, 322, 4295, 7150, 13, 50802], "temperature": 0.0, "avg_logprob": -0.23171443729610233, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.060720257461071014}, {"id": 12, "seek": 4888, "start": 57.64, "end": 63.120000000000005, "text": " So, we'll see a bit of graph neural nets today and we'll see more in his talk.", "tokens": [50802, 407, 11, 321, 603, 536, 257, 857, 295, 4295, 18161, 36170, 965, 293, 321, 603, 536, 544, 294, 702, 751, 13, 51076], "temperature": 0.0, "avg_logprob": -0.23171443729610233, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.060720257461071014}, {"id": 13, "seek": 4888, "start": 63.120000000000005, "end": 70.04, "text": " Carlos Simpson is talking on a kind of archetypal garden of how to use machine learning and", "tokens": [51076, 19646, 38184, 307, 1417, 322, 257, 733, 295, 41852, 31862, 7431, 295, 577, 281, 764, 3479, 2539, 293, 51422], "temperature": 0.0, "avg_logprob": -0.23171443729610233, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.060720257461071014}, {"id": 14, "seek": 4888, "start": 70.04, "end": 72.76, "text": " mathematical proof.", "tokens": [51422, 18894, 8177, 13, 51558], "temperature": 0.0, "avg_logprob": -0.23171443729610233, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.060720257461071014}, {"id": 15, "seek": 4888, "start": 72.76, "end": 76.36, "text": " And I think that that's extremely interesting.", "tokens": [51558, 400, 286, 519, 300, 300, 311, 4664, 1880, 13, 51738], "temperature": 0.0, "avg_logprob": -0.23171443729610233, "compression_ratio": 1.5502183406113537, "no_speech_prob": 0.060720257461071014}, {"id": 16, "seek": 7636, "start": 76.36, "end": 82.96, "text": " And then there's another three talks lined up after that, taking us to the end of semester.", "tokens": [50364, 400, 550, 456, 311, 1071, 1045, 6686, 17189, 493, 934, 300, 11, 1940, 505, 281, 264, 917, 295, 11894, 13, 50694], "temperature": 0.0, "avg_logprob": -0.1339525988721472, "compression_ratio": 1.71875, "no_speech_prob": 0.0066687921062111855}, {"id": 17, "seek": 7636, "start": 82.96, "end": 86.76, "text": " And the other thing I wanted to point out is Joel has been doing an amazing job with", "tokens": [50694, 400, 264, 661, 551, 286, 1415, 281, 935, 484, 307, 21522, 575, 668, 884, 364, 2243, 1691, 365, 50884], "temperature": 0.0, "avg_logprob": -0.1339525988721472, "compression_ratio": 1.71875, "no_speech_prob": 0.0066687921062111855}, {"id": 18, "seek": 7636, "start": 86.76, "end": 87.76, "text": " the toe labs.", "tokens": [50884, 264, 13976, 20339, 13, 50934], "temperature": 0.0, "avg_logprob": -0.1339525988721472, "compression_ratio": 1.71875, "no_speech_prob": 0.0066687921062111855}, {"id": 19, "seek": 7636, "start": 87.76, "end": 91.44, "text": " If you've been taking part in the tutorials, you'll be aware of this.", "tokens": [50934, 759, 291, 600, 668, 1940, 644, 294, 264, 17616, 11, 291, 603, 312, 3650, 295, 341, 13, 51118], "temperature": 0.0, "avg_logprob": -0.1339525988721472, "compression_ratio": 1.71875, "no_speech_prob": 0.0066687921062111855}, {"id": 20, "seek": 7636, "start": 91.44, "end": 95.8, "text": " But if you're watching online and you just want to muck around with something, they're", "tokens": [51118, 583, 498, 291, 434, 1976, 2950, 293, 291, 445, 528, 281, 275, 1134, 926, 365, 746, 11, 436, 434, 51336], "temperature": 0.0, "avg_logprob": -0.1339525988721472, "compression_ratio": 1.71875, "no_speech_prob": 0.0066687921062111855}, {"id": 21, "seek": 7636, "start": 95.8, "end": 98.36, "text": " really great and I'm hoping that they provide a good resource.", "tokens": [51336, 534, 869, 293, 286, 478, 7159, 300, 436, 2893, 257, 665, 7684, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1339525988721472, "compression_ratio": 1.71875, "no_speech_prob": 0.0066687921062111855}, {"id": 22, "seek": 7636, "start": 98.36, "end": 103.16, "text": " So, we'll hope to keep these going for the expert talks so you can play with some of", "tokens": [51464, 407, 11, 321, 603, 1454, 281, 1066, 613, 516, 337, 264, 5844, 6686, 370, 291, 393, 862, 365, 512, 295, 51704], "temperature": 0.0, "avg_logprob": -0.1339525988721472, "compression_ratio": 1.71875, "no_speech_prob": 0.0066687921062111855}, {"id": 23, "seek": 10316, "start": 103.16, "end": 106.75999999999999, "text": " Adam's ideas, play with some of Bumdud's ideas, et cetera.", "tokens": [50364, 7938, 311, 3487, 11, 862, 365, 512, 295, 363, 449, 67, 532, 311, 3487, 11, 1030, 11458, 13, 50544], "temperature": 0.0, "avg_logprob": -0.19666401723797403, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.012804120779037476}, {"id": 24, "seek": 10316, "start": 106.75999999999999, "end": 111.2, "text": " So, again, the seminar principles.", "tokens": [50544, 407, 11, 797, 11, 264, 29235, 9156, 13, 50766], "temperature": 0.0, "avg_logprob": -0.19666401723797403, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.012804120779037476}, {"id": 25, "seek": 10316, "start": 111.2, "end": 117.44, "text": " So, today we'll be about geometric deep learning and geometric deep learning is very much the", "tokens": [50766, 407, 11, 965, 321, 603, 312, 466, 33246, 2452, 2539, 293, 33246, 2452, 2539, 307, 588, 709, 264, 51078], "temperature": 0.0, "avg_logprob": -0.19666401723797403, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.012804120779037476}, {"id": 26, "seek": 10316, "start": 117.44, "end": 124.52, "text": " question of how do you incorporate symmetry into a learning problem.", "tokens": [51078, 1168, 295, 577, 360, 291, 16091, 25440, 666, 257, 2539, 1154, 13, 51432], "temperature": 0.0, "avg_logprob": -0.19666401723797403, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.012804120779037476}, {"id": 27, "seek": 10316, "start": 124.52, "end": 132.56, "text": " But before we go into that, I just want to recall the notion of an inductive bias, which", "tokens": [51432, 583, 949, 321, 352, 666, 300, 11, 286, 445, 528, 281, 9901, 264, 10710, 295, 364, 31612, 488, 12577, 11, 597, 51834], "temperature": 0.0, "avg_logprob": -0.19666401723797403, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.012804120779037476}, {"id": 28, "seek": 13256, "start": 132.64000000000001, "end": 135.36, "text": " is a very important notion in machine learning.", "tokens": [50368, 307, 257, 588, 1021, 10710, 294, 3479, 2539, 13, 50504], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 29, "seek": 13256, "start": 135.36, "end": 138.52, "text": " So what's going on here?", "tokens": [50504, 407, 437, 311, 516, 322, 510, 30, 50662], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 30, "seek": 13256, "start": 138.52, "end": 145.04, "text": " So it's very common situation in maths that you have some problem and you know or suspect", "tokens": [50662, 407, 309, 311, 588, 2689, 2590, 294, 36287, 300, 291, 362, 512, 1154, 293, 291, 458, 420, 9091, 50988], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 31, "seek": 13256, "start": 145.04, "end": 147.68, "text": " something about the solution.", "tokens": [50988, 746, 466, 264, 3827, 13, 51120], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 32, "seek": 13256, "start": 147.68, "end": 151.84, "text": " So a basic example would be something like we expect the solution to be smooth or we", "tokens": [51120, 407, 257, 3875, 1365, 576, 312, 746, 411, 321, 2066, 264, 3827, 281, 312, 5508, 420, 321, 51328], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 33, "seek": 13256, "start": 151.84, "end": 154.52, "text": " hope the solution is smooth.", "tokens": [51328, 1454, 264, 3827, 307, 5508, 13, 51462], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 34, "seek": 13256, "start": 154.52, "end": 159.56, "text": " The solution should satisfy conservation of energy, for example, there'll be some differential", "tokens": [51462, 440, 3827, 820, 19319, 16185, 295, 2281, 11, 337, 1365, 11, 456, 603, 312, 512, 15756, 51714], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 35, "seek": 13256, "start": 159.56, "end": 162.16, "text": " equation that the solution should satisfy.", "tokens": [51714, 5367, 300, 264, 3827, 820, 19319, 13, 51844], "temperature": 0.0, "avg_logprob": -0.19551851217029165, "compression_ratio": 1.865546218487395, "no_speech_prob": 0.008171058259904385}, {"id": 36, "seek": 16216, "start": 162.16, "end": 167.32, "text": " The solution should be invariant under a group, so this occurs all the time in physics.", "tokens": [50364, 440, 3827, 820, 312, 33270, 394, 833, 257, 1594, 11, 370, 341, 11843, 439, 264, 565, 294, 10649, 13, 50622], "temperature": 0.0, "avg_logprob": -0.14908428814100183, "compression_ratio": 1.8558139534883722, "no_speech_prob": 0.0002909996546804905}, {"id": 37, "seek": 16216, "start": 167.32, "end": 172.8, "text": " The solution might be locally determined, the counter example, so also we might be looking", "tokens": [50622, 440, 3827, 1062, 312, 16143, 9540, 11, 264, 5682, 1365, 11, 370, 611, 321, 1062, 312, 1237, 50896], "temperature": 0.0, "avg_logprob": -0.14908428814100183, "compression_ratio": 1.8558139534883722, "no_speech_prob": 0.0002909996546804905}, {"id": 38, "seek": 16216, "start": 172.8, "end": 177.48, "text": " for a counter example, we might suspect something about the counter example.", "tokens": [50896, 337, 257, 5682, 1365, 11, 321, 1062, 9091, 746, 466, 264, 5682, 1365, 13, 51130], "temperature": 0.0, "avg_logprob": -0.14908428814100183, "compression_ratio": 1.8558139534883722, "no_speech_prob": 0.0002909996546804905}, {"id": 39, "seek": 16216, "start": 177.48, "end": 183.51999999999998, "text": " So the fancy names for this are inductive bias or prior.", "tokens": [51130, 407, 264, 10247, 5288, 337, 341, 366, 31612, 488, 12577, 420, 4059, 13, 51432], "temperature": 0.0, "avg_logprob": -0.14908428814100183, "compression_ratio": 1.8558139534883722, "no_speech_prob": 0.0002909996546804905}, {"id": 40, "seek": 16216, "start": 183.51999999999998, "end": 187.51999999999998, "text": " So inductive bias just means something that you know about the problem a priori before", "tokens": [51432, 407, 31612, 488, 12577, 445, 1355, 746, 300, 291, 458, 466, 264, 1154, 257, 4059, 72, 949, 51632], "temperature": 0.0, "avg_logprob": -0.14908428814100183, "compression_ratio": 1.8558139534883722, "no_speech_prob": 0.0002909996546804905}, {"id": 41, "seek": 18752, "start": 187.52, "end": 188.84, "text": " starting to solve it.", "tokens": [50364, 2891, 281, 5039, 309, 13, 50430], "temperature": 0.0, "avg_logprob": -0.22417431447043348, "compression_ratio": 1.5, "no_speech_prob": 0.0037500427570194006}, {"id": 42, "seek": 18752, "start": 188.84, "end": 196.48000000000002, "text": " And it's very important to remember what inductive biases there are in a problem.", "tokens": [50430, 400, 309, 311, 588, 1021, 281, 1604, 437, 31612, 488, 32152, 456, 366, 294, 257, 1154, 13, 50812], "temperature": 0.0, "avg_logprob": -0.22417431447043348, "compression_ratio": 1.5, "no_speech_prob": 0.0037500427570194006}, {"id": 43, "seek": 18752, "start": 196.48000000000002, "end": 200.8, "text": " So how does one imagine that one has some inductive bias?", "tokens": [50812, 407, 577, 775, 472, 3811, 300, 472, 575, 512, 31612, 488, 12577, 30, 51028], "temperature": 0.0, "avg_logprob": -0.22417431447043348, "compression_ratio": 1.5, "no_speech_prob": 0.0037500427570194006}, {"id": 44, "seek": 18752, "start": 200.8, "end": 203.28, "text": " So for example, the solution might be smooth.", "tokens": [51028, 407, 337, 1365, 11, 264, 3827, 1062, 312, 5508, 13, 51152], "temperature": 0.0, "avg_logprob": -0.22417431447043348, "compression_ratio": 1.5, "no_speech_prob": 0.0037500427570194006}, {"id": 45, "seek": 18752, "start": 203.28, "end": 210.88, "text": " So this is related very much to regularization.", "tokens": [51152, 407, 341, 307, 4077, 588, 709, 281, 3890, 2144, 13, 51532], "temperature": 0.0, "avg_logprob": -0.22417431447043348, "compression_ratio": 1.5, "no_speech_prob": 0.0037500427570194006}, {"id": 46, "seek": 21088, "start": 210.88, "end": 218.4, "text": " So in this was Gayog's talk last week.", "tokens": [50364, 407, 294, 341, 390, 23081, 664, 311, 751, 1036, 1243, 13, 50740], "temperature": 0.0, "avg_logprob": -0.20562448501586914, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.001753578893840313}, {"id": 47, "seek": 21088, "start": 218.4, "end": 224.28, "text": " So for example, he talked about this Gaussian kernel, which very much encourages functions", "tokens": [50740, 407, 337, 1365, 11, 415, 2825, 466, 341, 39148, 28256, 11, 597, 588, 709, 28071, 6828, 51034], "temperature": 0.0, "avg_logprob": -0.20562448501586914, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.001753578893840313}, {"id": 48, "seek": 21088, "start": 224.28, "end": 232.28, "text": " that have you know Fourier modes that aren't wiggling around too quickly, that are smooth", "tokens": [51034, 300, 362, 291, 458, 36810, 14068, 300, 3212, 380, 261, 24542, 926, 886, 2661, 11, 300, 366, 5508, 51434], "temperature": 0.0, "avg_logprob": -0.20562448501586914, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.001753578893840313}, {"id": 49, "seek": 21088, "start": 232.28, "end": 234.88, "text": " in a very strong sense.", "tokens": [51434, 294, 257, 588, 2068, 2020, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20562448501586914, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.001753578893840313}, {"id": 50, "seek": 21088, "start": 234.88, "end": 238.84, "text": " Solution should satisfy conservation of energy, this might be another example of regularization.", "tokens": [51564, 318, 3386, 820, 19319, 16185, 295, 2281, 11, 341, 1062, 312, 1071, 1365, 295, 3890, 2144, 13, 51762], "temperature": 0.0, "avg_logprob": -0.20562448501586914, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.001753578893840313}, {"id": 51, "seek": 23884, "start": 238.84, "end": 242.88, "text": " There's also these fascinating things, if you want to have a Google called physically", "tokens": [50364, 821, 311, 611, 613, 10343, 721, 11, 498, 291, 528, 281, 362, 257, 3329, 1219, 9762, 50566], "temperature": 0.0, "avg_logprob": -0.1635709623011147, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.06930334866046906}, {"id": 52, "seek": 23884, "start": 242.88, "end": 250.36, "text": " informed neural nets.", "tokens": [50566, 11740, 18161, 36170, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1635709623011147, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.06930334866046906}, {"id": 53, "seek": 23884, "start": 250.36, "end": 255.92000000000002, "text": " So here you don't require the solution to satisfy some differential equation, but you", "tokens": [50940, 407, 510, 291, 500, 380, 3651, 264, 3827, 281, 19319, 512, 15756, 5367, 11, 457, 291, 51218], "temperature": 0.0, "avg_logprob": -0.1635709623011147, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.06930334866046906}, {"id": 54, "seek": 23884, "start": 255.92000000000002, "end": 258.56, "text": " add the differential equation to the cost function.", "tokens": [51218, 909, 264, 15756, 5367, 281, 264, 2063, 2445, 13, 51350], "temperature": 0.0, "avg_logprob": -0.1635709623011147, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.06930334866046906}, {"id": 55, "seek": 23884, "start": 258.56, "end": 263.72, "text": " And so it encourages the solution to satisfy a differential equation.", "tokens": [51350, 400, 370, 309, 28071, 264, 3827, 281, 19319, 257, 15756, 5367, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1635709623011147, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.06930334866046906}, {"id": 56, "seek": 23884, "start": 263.72, "end": 266.4, "text": " Problem should be the solution should be invariant under.", "tokens": [51608, 11676, 820, 312, 264, 3827, 820, 312, 33270, 394, 833, 13, 51742], "temperature": 0.0, "avg_logprob": -0.1635709623011147, "compression_ratio": 1.7932692307692308, "no_speech_prob": 0.06930334866046906}, {"id": 57, "seek": 26640, "start": 266.4, "end": 269.64, "text": " So this is today.", "tokens": [50364, 407, 341, 307, 965, 13, 50526], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 58, "seek": 26640, "start": 269.64, "end": 271.59999999999997, "text": " That's the subject of today.", "tokens": [50526, 663, 311, 264, 3983, 295, 965, 13, 50624], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 59, "seek": 26640, "start": 271.59999999999997, "end": 273.71999999999997, "text": " The solution might be locally determined.", "tokens": [50624, 440, 3827, 1062, 312, 16143, 9540, 13, 50730], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 60, "seek": 26640, "start": 273.71999999999997, "end": 279.71999999999997, "text": " So this is like examples might be CNN's or LSTM.", "tokens": [50730, 407, 341, 307, 411, 5110, 1062, 312, 24859, 311, 420, 441, 6840, 44, 13, 51030], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 61, "seek": 26640, "start": 279.71999999999997, "end": 284.23999999999995, "text": " Okay we expect for example small part areas of small parts of an image to play an important", "tokens": [51030, 1033, 321, 2066, 337, 1365, 1359, 644, 3179, 295, 1359, 3166, 295, 364, 3256, 281, 862, 364, 1021, 51256], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 62, "seek": 26640, "start": 284.23999999999995, "end": 287.0, "text": " role initially.", "tokens": [51256, 3090, 9105, 13, 51394], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 63, "seek": 26640, "start": 287.0, "end": 289.23999999999995, "text": " And the counter example is probably highly connected.", "tokens": [51394, 400, 264, 5682, 1365, 307, 1391, 5405, 4582, 13, 51506], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 64, "seek": 26640, "start": 289.23999999999995, "end": 293.59999999999997, "text": " I included this example because this is an example where I've got no idea how to put", "tokens": [51506, 286, 5556, 341, 1365, 570, 341, 307, 364, 1365, 689, 286, 600, 658, 572, 1558, 577, 281, 829, 51724], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 65, "seek": 26640, "start": 293.59999999999997, "end": 295.59999999999997, "text": " this kind of information into a network.", "tokens": [51724, 341, 733, 295, 1589, 666, 257, 3209, 13, 51824], "temperature": 0.0, "avg_logprob": -0.18502011430372886, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06943626701831818}, {"id": 66, "seek": 29560, "start": 295.6, "end": 303.8, "text": " And this was definitely my experience with DeepMind in that often, so the movement from", "tokens": [50364, 400, 341, 390, 2138, 452, 1752, 365, 14895, 44, 471, 294, 300, 2049, 11, 370, 264, 3963, 490, 50774], "temperature": 0.0, "avg_logprob": -0.16142024269586877, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.012584284879267216}, {"id": 67, "seek": 29560, "start": 303.8, "end": 310.48, "text": " an inductive bias to the neural net design is art rather than science.", "tokens": [50774, 364, 31612, 488, 12577, 281, 264, 18161, 2533, 1715, 307, 1523, 2831, 813, 3497, 13, 51108], "temperature": 0.0, "avg_logprob": -0.16142024269586877, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.012584284879267216}, {"id": 68, "seek": 29560, "start": 310.48, "end": 316.8, "text": " So several times with the DeepMind team I said oh you know I know this about the solution", "tokens": [51108, 407, 2940, 1413, 365, 264, 14895, 44, 471, 1469, 286, 848, 1954, 291, 458, 286, 458, 341, 466, 264, 3827, 51424], "temperature": 0.0, "avg_logprob": -0.16142024269586877, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.012584284879267216}, {"id": 69, "seek": 29560, "start": 316.8, "end": 322.04, "text": " and they said we have absolutely no idea how to incorporate this into the model.", "tokens": [51424, 293, 436, 848, 321, 362, 3122, 572, 1558, 577, 281, 16091, 341, 666, 264, 2316, 13, 51686], "temperature": 0.0, "avg_logprob": -0.16142024269586877, "compression_ratio": 1.5893719806763285, "no_speech_prob": 0.012584284879267216}, {"id": 70, "seek": 32204, "start": 322.04, "end": 329.52000000000004, "text": " And I think that one should be aware that this is often a big issue.", "tokens": [50364, 400, 286, 519, 300, 472, 820, 312, 3650, 300, 341, 307, 2049, 257, 955, 2734, 13, 50738], "temperature": 0.0, "avg_logprob": -0.13093285501739124, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.018513578921556473}, {"id": 71, "seek": 32204, "start": 329.52000000000004, "end": 335.52000000000004, "text": " So yeah one should be just be aware that it's very important to have some kind of inductive", "tokens": [50738, 407, 1338, 472, 820, 312, 445, 312, 3650, 300, 309, 311, 588, 1021, 281, 362, 512, 733, 295, 31612, 488, 51038], "temperature": 0.0, "avg_logprob": -0.13093285501739124, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.018513578921556473}, {"id": 72, "seek": 32204, "start": 335.52000000000004, "end": 343.76, "text": " bias and be aware of it but you might not know what to do with it.", "tokens": [51038, 12577, 293, 312, 3650, 295, 309, 457, 291, 1062, 406, 458, 437, 281, 360, 365, 309, 13, 51450], "temperature": 0.0, "avg_logprob": -0.13093285501739124, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.018513578921556473}, {"id": 73, "seek": 32204, "start": 343.76, "end": 350.56, "text": " And this is just the same slide that very similar to things that Georg has been saying.", "tokens": [51450, 400, 341, 307, 445, 264, 912, 4137, 300, 588, 2531, 281, 721, 300, 10114, 575, 668, 1566, 13, 51790], "temperature": 0.0, "avg_logprob": -0.13093285501739124, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.018513578921556473}, {"id": 74, "seek": 35056, "start": 350.56, "end": 357.36, "text": " You have the capacity of a model roughly speaking how many parameters it has and there is this", "tokens": [50364, 509, 362, 264, 6042, 295, 257, 2316, 9810, 4124, 577, 867, 9834, 309, 575, 293, 456, 307, 341, 50704], "temperature": 0.0, "avg_logprob": -0.14294426877733687, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.03403022885322571}, {"id": 75, "seek": 35056, "start": 357.36, "end": 360.24, "text": " playoff between simplicity and expressivity.", "tokens": [50704, 862, 4506, 1296, 25632, 293, 5109, 4253, 13, 50848], "temperature": 0.0, "avg_logprob": -0.14294426877733687, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.03403022885322571}, {"id": 76, "seek": 35056, "start": 360.24, "end": 367.96, "text": " So a lot of parameters means you can express for example any function but training may", "tokens": [50848, 407, 257, 688, 295, 9834, 1355, 291, 393, 5109, 337, 1365, 604, 2445, 457, 3097, 815, 51234], "temperature": 0.0, "avg_logprob": -0.14294426877733687, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.03403022885322571}, {"id": 77, "seek": 35056, "start": 367.96, "end": 373.68, "text": " be infeasible you might have hundreds of billions of parameters and it's less interpretable", "tokens": [51234, 312, 1536, 68, 296, 964, 291, 1062, 362, 6779, 295, 17375, 295, 9834, 293, 309, 311, 1570, 7302, 712, 51520], "temperature": 0.0, "avg_logprob": -0.14294426877733687, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.03403022885322571}, {"id": 78, "seek": 37368, "start": 373.68, "end": 381.28000000000003, "text": " and so there's often sweet spots but in this trade off.", "tokens": [50364, 293, 370, 456, 311, 2049, 3844, 10681, 457, 294, 341, 4923, 766, 13, 50744], "temperature": 0.0, "avg_logprob": -0.19700818247609325, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.03445572033524513}, {"id": 79, "seek": 37368, "start": 381.28000000000003, "end": 388.64, "text": " Okay so today what I'm talking about is symmetry in neural networks and I'm really extremely", "tokens": [50744, 1033, 370, 965, 437, 286, 478, 1417, 466, 307, 25440, 294, 18161, 9590, 293, 286, 478, 534, 4664, 51112], "temperature": 0.0, "avg_logprob": -0.19700818247609325, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.03445572033524513}, {"id": 80, "seek": 37368, "start": 388.64, "end": 393.24, "text": " happy to give this talk because it's been kind of a revelation for me.", "tokens": [51112, 2055, 281, 976, 341, 751, 570, 309, 311, 668, 733, 295, 257, 23456, 337, 385, 13, 51342], "temperature": 0.0, "avg_logprob": -0.19700818247609325, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.03445572033524513}, {"id": 81, "seek": 37368, "start": 393.24, "end": 399.04, "text": " So I began with this book Geometric Deep Learning by Bronstein Bruner Cohen and Petar", "tokens": [51342, 407, 286, 4283, 365, 341, 1446, 2876, 29470, 14895, 15205, 538, 19544, 9089, 1603, 409, 260, 32968, 293, 10472, 289, 51632], "temperature": 0.0, "avg_logprob": -0.19700818247609325, "compression_ratio": 1.4523809523809523, "no_speech_prob": 0.03445572033524513}, {"id": 82, "seek": 39904, "start": 399.04, "end": 405.16, "text": " Velichkovich who I worked with on the DeepMind project and Gayog pointed out this group here", "tokens": [50364, 17814, 480, 33516, 480, 567, 286, 2732, 365, 322, 264, 14895, 44, 471, 1716, 293, 23081, 664, 10932, 484, 341, 1594, 510, 50670], "temperature": 0.0, "avg_logprob": -0.23414843010179925, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.1908133625984192}, {"id": 83, "seek": 39904, "start": 405.16, "end": 410.96000000000004, "text": " with very convolutional networks and Petar recently just pointed out this steerable CNNs", "tokens": [50670, 365, 588, 45216, 304, 9590, 293, 10472, 289, 3938, 445, 10932, 484, 341, 30814, 712, 24859, 82, 50960], "temperature": 0.0, "avg_logprob": -0.23414843010179925, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.1908133625984192}, {"id": 84, "seek": 39904, "start": 410.96000000000004, "end": 417.6, "text": " and it's very interesting subject and I think it's a really remarkable example.", "tokens": [50960, 293, 309, 311, 588, 1880, 3983, 293, 286, 519, 309, 311, 257, 534, 12802, 1365, 13, 51292], "temperature": 0.0, "avg_logprob": -0.23414843010179925, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.1908133625984192}, {"id": 85, "seek": 39904, "start": 417.6, "end": 423.72, "text": " So in physics we often see this phenomenon where just knowing some kind of symmetry is", "tokens": [51292, 407, 294, 10649, 321, 2049, 536, 341, 14029, 689, 445, 5276, 512, 733, 295, 25440, 307, 51598], "temperature": 0.0, "avg_logprob": -0.23414843010179925, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.1908133625984192}, {"id": 86, "seek": 39904, "start": 423.72, "end": 428.28000000000003, "text": " present has enormous effects on your ability to solve a problem or formulate a model or", "tokens": [51598, 1974, 575, 11322, 5065, 322, 428, 3485, 281, 5039, 257, 1154, 420, 47881, 257, 2316, 420, 51826], "temperature": 0.0, "avg_logprob": -0.23414843010179925, "compression_ratio": 1.614814814814815, "no_speech_prob": 0.1908133625984192}, {"id": 87, "seek": 42828, "start": 428.28, "end": 432.84, "text": " something like that so there's this extraordinary paper I think it's by gross called symmetry", "tokens": [50364, 746, 411, 300, 370, 456, 311, 341, 10581, 3035, 286, 519, 309, 311, 538, 11367, 1219, 25440, 50592], "temperature": 0.0, "avg_logprob": -0.2097781804891733, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.009244046173989773}, {"id": 88, "seek": 42828, "start": 432.84, "end": 439.47999999999996, "text": " and physical laws which I found really inspiring and I find this", "tokens": [50592, 293, 4001, 6064, 597, 286, 1352, 534, 15883, 293, 286, 915, 341, 50924], "temperature": 0.0, "avg_logprob": -0.2097781804891733, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.009244046173989773}, {"id": 89, "seek": 42828, "start": 439.47999999999996, "end": 444.08, "text": " equivariance in convolutional neural nets to be a kind of similar story it's like it", "tokens": [50924, 1267, 592, 3504, 719, 294, 45216, 304, 18161, 36170, 281, 312, 257, 733, 295, 2531, 1657, 309, 311, 411, 309, 51154], "temperature": 0.0, "avg_logprob": -0.2097781804891733, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.009244046173989773}, {"id": 90, "seek": 42828, "start": 444.08, "end": 448.71999999999997, "text": " seems so innocent to require some kind of invariance and yet it essentially determines", "tokens": [51154, 2544, 370, 13171, 281, 3651, 512, 733, 295, 33270, 719, 293, 1939, 309, 4476, 24799, 51386], "temperature": 0.0, "avg_logprob": -0.2097781804891733, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.009244046173989773}, {"id": 91, "seek": 42828, "start": 448.71999999999997, "end": 452.79999999999995, "text": " your entire architecture it's really remarkable.", "tokens": [51386, 428, 2302, 9482, 309, 311, 534, 12802, 13, 51590], "temperature": 0.0, "avg_logprob": -0.2097781804891733, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.009244046173989773}, {"id": 92, "seek": 42828, "start": 452.79999999999995, "end": 458.15999999999997, "text": " So never underestimate symmetry I had this on my web page for about 10 years as the first", "tokens": [51590, 407, 1128, 35826, 25440, 286, 632, 341, 322, 452, 3670, 3028, 337, 466, 1266, 924, 382, 264, 700, 51858], "temperature": 0.0, "avg_logprob": -0.2097781804891733, "compression_ratio": 1.6931407942238268, "no_speech_prob": 0.009244046173989773}, {"id": 93, "seek": 45816, "start": 458.16, "end": 460.92, "text": " thing that you read.", "tokens": [50364, 551, 300, 291, 1401, 13, 50502], "temperature": 0.0, "avg_logprob": -0.12696603415668875, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.02288120798766613}, {"id": 94, "seek": 45816, "start": 460.92, "end": 470.12, "text": " So I want to review what kind of vanilla CNN is and then so I'll first just remember", "tokens": [50502, 407, 286, 528, 281, 3131, 437, 733, 295, 17528, 24859, 307, 293, 550, 370, 286, 603, 700, 445, 1604, 50962], "temperature": 0.0, "avg_logprob": -0.12696603415668875, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.02288120798766613}, {"id": 95, "seek": 45816, "start": 470.12, "end": 477.04, "text": " what a CNN is and then we'll view a CNN through the language of group theory and I want to", "tokens": [50962, 437, 257, 24859, 307, 293, 550, 321, 603, 1910, 257, 24859, 807, 264, 2856, 295, 1594, 5261, 293, 286, 528, 281, 51308], "temperature": 0.0, "avg_logprob": -0.12696603415668875, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.02288120798766613}, {"id": 96, "seek": 45816, "start": 477.04, "end": 486.44000000000005, "text": " try to convince you that three basic principles already basically determine CNN architecture.", "tokens": [51308, 853, 281, 13447, 291, 300, 1045, 3875, 9156, 1217, 1936, 6997, 24859, 9482, 13, 51778], "temperature": 0.0, "avg_logprob": -0.12696603415668875, "compression_ratio": 1.576086956521739, "no_speech_prob": 0.02288120798766613}, {"id": 97, "seek": 48644, "start": 486.44, "end": 494.36, "text": " So here's my image in the top right so this is a grayscale image I'm assuming that it", "tokens": [50364, 407, 510, 311, 452, 3256, 294, 264, 1192, 558, 370, 341, 307, 257, 677, 3772, 37088, 3256, 286, 478, 11926, 300, 309, 50760], "temperature": 0.0, "avg_logprob": -0.12763258685236392, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.01853564754128456}, {"id": 98, "seek": 48644, "start": 494.36, "end": 499.56, "text": " is on a square so I have a fixed width and a fixed height a fixed number of pixels wide", "tokens": [50760, 307, 322, 257, 3732, 370, 286, 362, 257, 6806, 11402, 293, 257, 6806, 6681, 257, 6806, 1230, 295, 18668, 4874, 51020], "temperature": 0.0, "avg_logprob": -0.12763258685236392, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.01853564754128456}, {"id": 99, "seek": 48644, "start": 499.56, "end": 507.84, "text": " a fixed number of pixels high and I have my pixel value is given by a real valued function", "tokens": [51020, 257, 6806, 1230, 295, 18668, 1090, 293, 286, 362, 452, 19261, 2158, 307, 2212, 538, 257, 957, 22608, 2445, 51434], "temperature": 0.0, "avg_logprob": -0.12763258685236392, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.01853564754128456}, {"id": 100, "seek": 48644, "start": 507.84, "end": 515.12, "text": " so for example this pixel value might be between zero and 255 and what I'm looking for is some", "tokens": [51434, 370, 337, 1365, 341, 19261, 2158, 1062, 312, 1296, 4018, 293, 3552, 20, 293, 437, 286, 478, 1237, 337, 307, 512, 51798], "temperature": 0.0, "avg_logprob": -0.12763258685236392, "compression_ratio": 1.7259615384615385, "no_speech_prob": 0.01853564754128456}, {"id": 101, "seek": 51512, "start": 515.24, "end": 521.16, "text": " function from this is called a so I'm calling this a periodic image because I'm kind of", "tokens": [50370, 2445, 490, 341, 307, 1219, 257, 370, 286, 478, 5141, 341, 257, 27790, 3256, 570, 286, 478, 733, 295, 50666], "temperature": 0.0, "avg_logprob": -0.164032823899213, "compression_ratio": 1.7702127659574467, "no_speech_prob": 0.07449840009212494}, {"id": 102, "seek": 51512, "start": 521.16, "end": 529.64, "text": " wrapping the top you know the physicists would say I'm compactifying on the torus and you", "tokens": [50666, 21993, 264, 1192, 291, 458, 264, 48716, 576, 584, 286, 478, 14679, 5489, 322, 264, 3930, 301, 293, 291, 51090], "temperature": 0.0, "avg_logprob": -0.164032823899213, "compression_ratio": 1.7702127659574467, "no_speech_prob": 0.07449840009212494}, {"id": 103, "seek": 51512, "start": 529.64, "end": 534.32, "text": " know I sound very fancy when I say that but we're just simplifying our situation by imagining", "tokens": [51090, 458, 286, 1626, 588, 10247, 562, 286, 584, 300, 457, 321, 434, 445, 6883, 5489, 527, 2590, 538, 27798, 51324], "temperature": 0.0, "avg_logprob": -0.164032823899213, "compression_ratio": 1.7702127659574467, "no_speech_prob": 0.07449840009212494}, {"id": 104, "seek": 51512, "start": 534.32, "end": 540.16, "text": " that our image is on the torus and there's it just simplifies the group theoretic discussion", "tokens": [51324, 300, 527, 3256, 307, 322, 264, 3930, 301, 293, 456, 311, 309, 445, 6883, 11221, 264, 1594, 14308, 299, 5017, 51616], "temperature": 0.0, "avg_logprob": -0.164032823899213, "compression_ratio": 1.7702127659574467, "no_speech_prob": 0.07449840009212494}, {"id": 105, "seek": 51512, "start": 540.16, "end": 543.4, "text": " in a second and there's no genuine need to do this.", "tokens": [51616, 294, 257, 1150, 293, 456, 311, 572, 16699, 643, 281, 360, 341, 13, 51778], "temperature": 0.0, "avg_logprob": -0.164032823899213, "compression_ratio": 1.7702127659574467, "no_speech_prob": 0.07449840009212494}, {"id": 106, "seek": 54340, "start": 543.4, "end": 551.28, "text": " So we're seeking some function from periodic images i.e functions on this grid to the real", "tokens": [50364, 407, 321, 434, 11670, 512, 2445, 490, 27790, 5267, 741, 13, 68, 6828, 322, 341, 10748, 281, 264, 957, 50758], "temperature": 0.0, "avg_logprob": -0.16065964698791504, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0004370139504317194}, {"id": 107, "seek": 54340, "start": 551.28, "end": 558.76, "text": " numbers which are for example positive on tigers and negative on non-tigers is a classic", "tokens": [50758, 3547, 597, 366, 337, 1365, 3353, 322, 47949, 293, 3671, 322, 2107, 12, 83, 24756, 307, 257, 7230, 51132], "temperature": 0.0, "avg_logprob": -0.16065964698791504, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0004370139504317194}, {"id": 108, "seek": 54340, "start": 558.76, "end": 562.68, "text": " and a machine learning problem and also the problem on which machine learning has been", "tokens": [51132, 293, 257, 3479, 2539, 1154, 293, 611, 264, 1154, 322, 597, 3479, 2539, 575, 668, 51328], "temperature": 0.0, "avg_logprob": -0.16065964698791504, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0004370139504317194}, {"id": 109, "seek": 56268, "start": 562.68, "end": 576.28, "text": " wildly successful and we do this in the following way so we have several layers and typically", "tokens": [50364, 34731, 4406, 293, 321, 360, 341, 294, 264, 3480, 636, 370, 321, 362, 2940, 7914, 293, 5850, 51044], "temperature": 0.0, "avg_logprob": -0.13317104043631717, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.15790246427059174}, {"id": 110, "seek": 56268, "start": 576.28, "end": 587.1999999999999, "text": " these layers will consist of other periodic images perhaps at lower scales so when in", "tokens": [51044, 613, 7914, 486, 4603, 295, 661, 27790, 5267, 4317, 412, 3126, 17408, 370, 562, 294, 51590], "temperature": 0.0, "avg_logprob": -0.13317104043631717, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.15790246427059174}, {"id": 111, "seek": 56268, "start": 587.1999999999999, "end": 591.88, "text": " the first two layers of this neural net I'm assuming that my periodic image is the same", "tokens": [51590, 264, 700, 732, 7914, 295, 341, 18161, 2533, 286, 478, 11926, 300, 452, 27790, 3256, 307, 264, 912, 51824], "temperature": 0.0, "avg_logprob": -0.13317104043631717, "compression_ratio": 1.6280487804878048, "no_speech_prob": 0.15790246427059174}, {"id": 112, "seek": 59188, "start": 591.96, "end": 598.36, "text": " scale and then in the third layer I'm assuming that the periodic image has dropped in scale", "tokens": [50368, 4373, 293, 550, 294, 264, 2636, 4583, 286, 478, 11926, 300, 264, 27790, 3256, 575, 8119, 294, 4373, 50688], "temperature": 0.0, "avg_logprob": -0.1523111172211476, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0005180608131922781}, {"id": 113, "seek": 59188, "start": 598.36, "end": 603.4, "text": " somewhat so I'm this h is assumed to h prime is assumed to divide h.", "tokens": [50688, 8344, 370, 286, 478, 341, 276, 307, 15895, 281, 276, 5835, 307, 15895, 281, 9845, 276, 13, 50940], "temperature": 0.0, "avg_logprob": -0.1523111172211476, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0005180608131922781}, {"id": 114, "seek": 59188, "start": 609.08, "end": 614.68, "text": " So what one should think about this problem so this problem this seek so we're seeking a", "tokens": [51224, 407, 437, 472, 820, 519, 466, 341, 1154, 370, 341, 1154, 341, 8075, 370, 321, 434, 11670, 257, 51504], "temperature": 0.0, "avg_logprob": -0.1523111172211476, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0005180608131922781}, {"id": 115, "seek": 59188, "start": 614.68, "end": 620.12, "text": " function from here to here and this function is going to be highly non-linear", "tokens": [51504, 2445, 490, 510, 281, 510, 293, 341, 2445, 307, 516, 281, 312, 5405, 2107, 12, 28263, 51776], "temperature": 0.0, "avg_logprob": -0.1523111172211476, "compression_ratio": 1.6855670103092784, "no_speech_prob": 0.0005180608131922781}, {"id": 116, "seek": 62188, "start": 621.96, "end": 628.52, "text": " so it's a non-linear function on this big vector space and I guess one of the points", "tokens": [50368, 370, 309, 311, 257, 2107, 12, 28263, 2445, 322, 341, 955, 8062, 1901, 293, 286, 2041, 472, 295, 264, 2793, 50696], "temperature": 0.0, "avg_logprob": -0.07538107546364389, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.0006357522797770798}, {"id": 117, "seek": 62188, "start": 628.52, "end": 634.04, "text": " of machine learning is that learning a highly non-linear function on a high dimensional vector", "tokens": [50696, 295, 3479, 2539, 307, 300, 2539, 257, 5405, 2107, 12, 28263, 2445, 322, 257, 1090, 18795, 8062, 50972], "temperature": 0.0, "avg_logprob": -0.07538107546364389, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.0006357522797770798}, {"id": 118, "seek": 62188, "start": 634.04, "end": 641.24, "text": " space is a very difficult task and we get enormous amount of mileage out of viewing it", "tokens": [50972, 1901, 307, 257, 588, 2252, 5633, 293, 321, 483, 11322, 2372, 295, 43121, 484, 295, 17480, 309, 51332], "temperature": 0.0, "avg_logprob": -0.07538107546364389, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.0006357522797770798}, {"id": 119, "seek": 62188, "start": 641.24, "end": 648.84, "text": " as composed of out of rather simple functions so the simple functions that we use are convolutions", "tokens": [51332, 382, 18204, 295, 484, 295, 2831, 2199, 6828, 370, 264, 2199, 6828, 300, 321, 764, 366, 3754, 15892, 51712], "temperature": 0.0, "avg_logprob": -0.07538107546364389, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.0006357522797770798}, {"id": 120, "seek": 64884, "start": 648.84, "end": 652.12, "text": " so this might be like we might have some kind of filter here", "tokens": [50364, 370, 341, 1062, 312, 411, 321, 1062, 362, 512, 733, 295, 6608, 510, 50528], "temperature": 0.0, "avg_logprob": -0.10761873538677509, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.0012239710194990039}, {"id": 121, "seek": 64884, "start": 655.72, "end": 666.9200000000001, "text": " the filter and this might be for example 8-1-1-1-1-1-1-1-1-1-1-1-1-1 and this particular filter", "tokens": [50708, 264, 6608, 293, 341, 1062, 312, 337, 1365, 1649, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 12, 16, 293, 341, 1729, 6608, 51268], "temperature": 0.0, "avg_logprob": -0.10761873538677509, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.0012239710194990039}, {"id": 122, "seek": 64884, "start": 667.72, "end": 674.84, "text": " we point wise multiply with our image and then sum up the result and so this particular filter", "tokens": [51308, 321, 935, 10829, 12972, 365, 527, 3256, 293, 550, 2408, 493, 264, 1874, 293, 370, 341, 1729, 6608, 51664], "temperature": 0.0, "avg_logprob": -0.10761873538677509, "compression_ratio": 1.832116788321168, "no_speech_prob": 0.0012239710194990039}, {"id": 123, "seek": 67484, "start": 674.84, "end": 680.9200000000001, "text": " would have the effect that on areas of blank color we would get a very low value but on", "tokens": [50364, 576, 362, 264, 1802, 300, 322, 3179, 295, 8247, 2017, 321, 576, 483, 257, 588, 2295, 2158, 457, 322, 50668], "temperature": 0.0, "avg_logprob": -0.09831121232774523, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0019852297846227884}, {"id": 124, "seek": 67484, "start": 680.9200000000001, "end": 685.88, "text": " out on edges we would get a high value so it would be have a kind of um outlining effect", "tokens": [50668, 484, 322, 8819, 321, 576, 483, 257, 1090, 2158, 370, 309, 576, 312, 362, 257, 733, 295, 1105, 484, 31079, 1802, 50916], "temperature": 0.0, "avg_logprob": -0.09831121232774523, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0019852297846227884}, {"id": 125, "seek": 67484, "start": 687.1600000000001, "end": 693.72, "text": " so this is an example of something that we might uh convolve with so that's applying this", "tokens": [50980, 370, 341, 307, 364, 1365, 295, 746, 300, 321, 1062, 2232, 3754, 37361, 365, 370, 300, 311, 9275, 341, 51308], "temperature": 0.0, "avg_logprob": -0.09831121232774523, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0019852297846227884}, {"id": 126, "seek": 67484, "start": 693.72, "end": 700.12, "text": " filter can be rephrased as a convolution and what we do so we convolve and then we apply", "tokens": [51308, 6608, 393, 312, 319, 44598, 1937, 382, 257, 45216, 293, 437, 321, 360, 370, 321, 3754, 37361, 293, 550, 321, 3079, 51628], "temperature": 0.0, "avg_logprob": -0.09831121232774523, "compression_ratio": 1.7661691542288558, "no_speech_prob": 0.0019852297846227884}, {"id": 127, "seek": 70012, "start": 700.92, "end": 709.72, "text": " apply a value and then we convolve and we apply a value and then we do an operation called pooling", "tokens": [50404, 3079, 257, 2158, 293, 550, 321, 3754, 37361, 293, 321, 3079, 257, 2158, 293, 550, 321, 360, 364, 6916, 1219, 7005, 278, 50844], "temperature": 0.0, "avg_logprob": -0.0821752076620584, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0012057945132255554}, {"id": 128, "seek": 70012, "start": 709.72, "end": 714.36, "text": " that I'll basically ignore here which might be you look at a grid of pixels and take the maximum", "tokens": [50844, 300, 286, 603, 1936, 11200, 510, 597, 1062, 312, 291, 574, 412, 257, 10748, 295, 18668, 293, 747, 264, 6674, 51076], "temperature": 0.0, "avg_logprob": -0.0821752076620584, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0012057945132255554}, {"id": 129, "seek": 70012, "start": 714.36, "end": 720.6, "text": " element so this will take you down to a smaller image and then finally at the end you might do", "tokens": [51076, 4478, 370, 341, 486, 747, 291, 760, 281, 257, 4356, 3256, 293, 550, 2721, 412, 264, 917, 291, 1062, 360, 51388], "temperature": 0.0, "avg_logprob": -0.0821752076620584, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0012057945132255554}, {"id": 130, "seek": 70012, "start": 720.6, "end": 727.96, "text": " some fully connected layers and the point of all this is that we don't we specify this architecture", "tokens": [51388, 512, 4498, 4582, 7914, 293, 264, 935, 295, 439, 341, 307, 300, 321, 500, 380, 321, 16500, 341, 9482, 51756], "temperature": 0.0, "avg_logprob": -0.0821752076620584, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0012057945132255554}, {"id": 131, "seek": 72796, "start": 728.6800000000001, "end": 734.6800000000001, "text": " uh at the beginning and we specify all the values at the beginning but we learn the convolutions", "tokens": [50400, 2232, 412, 264, 2863, 293, 321, 16500, 439, 264, 4190, 412, 264, 2863, 457, 321, 1466, 264, 3754, 15892, 50700], "temperature": 0.0, "avg_logprob": -0.07645534806781346, "compression_ratio": 1.6939890710382515, "no_speech_prob": 0.00046514792484231293}, {"id": 132, "seek": 72796, "start": 734.6800000000001, "end": 737.24, "text": " that's the important point we learn these filters", "tokens": [50700, 300, 311, 264, 1021, 935, 321, 1466, 613, 15995, 50828], "temperature": 0.0, "avg_logprob": -0.07645534806781346, "compression_ratio": 1.6939890710382515, "no_speech_prob": 0.00046514792484231293}, {"id": 133, "seek": 72796, "start": 739.5600000000001, "end": 745.24, "text": " okay so that was meant to just be a review now I want to look at this through the language of group here", "tokens": [50944, 1392, 370, 300, 390, 4140, 281, 445, 312, 257, 3131, 586, 286, 528, 281, 574, 412, 341, 807, 264, 2856, 295, 1594, 510, 51228], "temperature": 0.0, "avg_logprob": -0.07645534806781346, "compression_ratio": 1.6939890710382515, "no_speech_prob": 0.00046514792484231293}, {"id": 134, "seek": 72796, "start": 749.0, "end": 752.6, "text": " so this is just a direct copy of the previous architecture", "tokens": [51416, 370, 341, 307, 445, 257, 2047, 5055, 295, 264, 3894, 9482, 51596], "temperature": 0.0, "avg_logprob": -0.07645534806781346, "compression_ratio": 1.6939890710382515, "no_speech_prob": 0.00046514792484231293}, {"id": 135, "seek": 75260, "start": 753.32, "end": 764.44, "text": " so z mod h squared sorry z mod hz squared is a group so it's z mod hz times z mod hz", "tokens": [50400, 370, 710, 1072, 276, 8889, 2597, 710, 1072, 276, 89, 8889, 307, 257, 1594, 370, 309, 311, 710, 1072, 276, 89, 1413, 710, 1072, 276, 89, 50956], "temperature": 0.0, "avg_logprob": -0.12967130771050087, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.002114938572049141}, {"id": 136, "seek": 75260, "start": 766.36, "end": 767.96, "text": " very simple abelian group", "tokens": [51052, 588, 2199, 410, 338, 952, 1594, 51132], "temperature": 0.0, "avg_logprob": -0.12967130771050087, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.002114938572049141}, {"id": 137, "seek": 75260, "start": 773.72, "end": 779.48, "text": " and as with any group it acts on functions on that group", "tokens": [51420, 293, 382, 365, 604, 1594, 309, 10672, 322, 6828, 322, 300, 1594, 51708], "temperature": 0.0, "avg_logprob": -0.12967130771050087, "compression_ratio": 1.5754716981132075, "no_speech_prob": 0.002114938572049141}, {"id": 138, "seek": 77948, "start": 780.36, "end": 787.8000000000001, "text": " so if I have a function on my group what I can do is translate my group around and I get a new", "tokens": [50408, 370, 498, 286, 362, 257, 2445, 322, 452, 1594, 437, 286, 393, 360, 307, 13799, 452, 1594, 926, 293, 286, 483, 257, 777, 50780], "temperature": 0.0, "avg_logprob": -0.11356660764511317, "compression_ratio": 1.9072847682119205, "no_speech_prob": 0.0008686240762472153}, {"id": 139, "seek": 77948, "start": 787.8000000000001, "end": 795.08, "text": " function okay and convolving by a single so when I translate around by my group this is the same", "tokens": [50780, 2445, 1392, 293, 3754, 401, 798, 538, 257, 2167, 370, 562, 286, 13799, 926, 538, 452, 1594, 341, 307, 264, 912, 51144], "temperature": 0.0, "avg_logprob": -0.11356660764511317, "compression_ratio": 1.9072847682119205, "no_speech_prob": 0.0008686240762472153}, {"id": 140, "seek": 77948, "start": 795.08, "end": 803.72, "text": " thing as convolving with a delta function on my group so on my group I can consider the function", "tokens": [51144, 551, 382, 3754, 401, 798, 365, 257, 8289, 2445, 322, 452, 1594, 370, 322, 452, 1594, 286, 393, 1949, 264, 2445, 51576], "temperature": 0.0, "avg_logprob": -0.11356660764511317, "compression_ratio": 1.9072847682119205, "no_speech_prob": 0.0008686240762472153}, {"id": 141, "seek": 80372, "start": 804.36, "end": 812.6, "text": " that's just one at a particular group element and zero elsewhere and convolving with that element", "tokens": [50396, 300, 311, 445, 472, 412, 257, 1729, 1594, 4478, 293, 4018, 14517, 293, 3754, 401, 798, 365, 300, 4478, 50808], "temperature": 0.0, "avg_logprob": -0.09218883514404297, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.005218059290200472}, {"id": 142, "seek": 80372, "start": 812.6, "end": 816.12, "text": " is the same thing as translating by that group element", "tokens": [50808, 307, 264, 912, 551, 382, 35030, 538, 300, 1594, 4478, 50984], "temperature": 0.0, "avg_logprob": -0.09218883514404297, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.005218059290200472}, {"id": 143, "seek": 80372, "start": 818.9200000000001, "end": 827.48, "text": " and uh so any any convolution is a linear combination of these convolution with a single", "tokens": [51124, 293, 2232, 370, 604, 604, 45216, 307, 257, 8213, 6562, 295, 613, 45216, 365, 257, 2167, 51552], "temperature": 0.0, "avg_logprob": -0.09218883514404297, "compression_ratio": 1.6736111111111112, "no_speech_prob": 0.005218059290200472}, {"id": 144, "seek": 82748, "start": 827.48, "end": 834.28, "text": " with a delta function and so this is gamma equivalent so another way so in the abelian case", "tokens": [50364, 365, 257, 8289, 2445, 293, 370, 341, 307, 15546, 10344, 370, 1071, 636, 370, 294, 264, 410, 338, 952, 1389, 50704], "temperature": 0.0, "avg_logprob": -0.10749847109955137, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005725565366446972}, {"id": 145, "seek": 82748, "start": 834.28, "end": 841.96, "text": " kind of nothing nothing matters in terms of orders you know when I wrote g dot f of x equals f of", "tokens": [50704, 733, 295, 1825, 1825, 7001, 294, 2115, 295, 9470, 291, 458, 562, 286, 4114, 290, 5893, 283, 295, 2031, 6915, 283, 295, 51088], "temperature": 0.0, "avg_logprob": -0.10749847109955137, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005725565366446972}, {"id": 146, "seek": 82748, "start": 842.6, "end": 848.6800000000001, "text": " g plus x it doesn't matter whether I whether I write x plus g or x minus g but in the non-abelian", "tokens": [51120, 290, 1804, 2031, 309, 1177, 380, 1871, 1968, 286, 1968, 286, 2464, 2031, 1804, 290, 420, 2031, 3175, 290, 457, 294, 264, 2107, 12, 18657, 952, 51424], "temperature": 0.0, "avg_logprob": -0.10749847109955137, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005725565366446972}, {"id": 147, "seek": 82748, "start": 848.6800000000001, "end": 854.52, "text": " case I'd have to have an inverse and in the non-elabelian case I would kind of think about", "tokens": [51424, 1389, 286, 1116, 362, 281, 362, 364, 17340, 293, 294, 264, 2107, 12, 338, 18657, 952, 1389, 286, 576, 733, 295, 519, 466, 51716], "temperature": 0.0, "avg_logprob": -0.10749847109955137, "compression_ratio": 1.8086124401913874, "no_speech_prob": 0.005725565366446972}, {"id": 148, "seek": 85452, "start": 854.76, "end": 860.52, "text": " convolutions as maybe acting from the right or something like that but it's a general fact that", "tokens": [50376, 3754, 15892, 382, 1310, 6577, 490, 264, 558, 420, 746, 411, 300, 457, 309, 311, 257, 2674, 1186, 300, 50664], "temperature": 0.0, "avg_logprob": -0.08832275035769441, "compression_ratio": 2.125, "no_speech_prob": 0.002630074741318822}, {"id": 149, "seek": 85452, "start": 862.36, "end": 870.52, "text": " the if we look at functions on a group then the equivalent maps from functions on a group to", "tokens": [50756, 264, 498, 321, 574, 412, 6828, 322, 257, 1594, 550, 264, 10344, 11317, 490, 6828, 322, 257, 1594, 281, 51164], "temperature": 0.0, "avg_logprob": -0.08832275035769441, "compression_ratio": 2.125, "no_speech_prob": 0.002630074741318822}, {"id": 150, "seek": 85452, "start": 870.52, "end": 875.3199999999999, "text": " functions on a group are the same thing as functions on that group acting by a convolution", "tokens": [51164, 6828, 322, 257, 1594, 366, 264, 912, 551, 382, 6828, 322, 300, 1594, 6577, 538, 257, 45216, 51404], "temperature": 0.0, "avg_logprob": -0.08832275035769441, "compression_ratio": 2.125, "no_speech_prob": 0.002630074741318822}, {"id": 151, "seek": 85452, "start": 876.36, "end": 882.04, "text": " and that's what I that's what I say here so the equivalent maps from functions on the group to", "tokens": [51456, 293, 300, 311, 437, 286, 300, 311, 437, 286, 584, 510, 370, 264, 10344, 11317, 490, 6828, 322, 264, 1594, 281, 51740], "temperature": 0.0, "avg_logprob": -0.08832275035769441, "compression_ratio": 2.125, "no_speech_prob": 0.002630074741318822}, {"id": 152, "seek": 88204, "start": 882.04, "end": 889.24, "text": " functions on the group are simply functions on the group okay and this is true for any gamma", "tokens": [50364, 6828, 322, 264, 1594, 366, 2935, 6828, 322, 264, 1594, 1392, 293, 341, 307, 2074, 337, 604, 15546, 50724], "temperature": 0.0, "avg_logprob": -0.14881836891174316, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.00027784862322732806}, {"id": 153, "seek": 88204, "start": 891.0, "end": 894.4399999999999, "text": " this is very basic representation theory if you will", "tokens": [50812, 341, 307, 588, 3875, 10290, 5261, 498, 291, 486, 50984], "temperature": 0.0, "avg_logprob": -0.14881836891174316, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.00027784862322732806}, {"id": 154, "seek": 88204, "start": 899.8, "end": 907.9599999999999, "text": " so remember that I hate questions and any question will involve um a horror show", "tokens": [51252, 370, 1604, 300, 286, 4700, 1651, 293, 604, 1168, 486, 9494, 1105, 257, 11501, 855, 51660], "temperature": 0.0, "avg_logprob": -0.14881836891174316, "compression_ratio": 1.6258992805755397, "no_speech_prob": 0.00027784862322732806}, {"id": 155, "seek": 90796, "start": 908.9200000000001, "end": 910.2800000000001, "text": " so please don't ask any questions", "tokens": [50412, 370, 1767, 500, 380, 1029, 604, 1651, 50480], "temperature": 0.0, "avg_logprob": -0.08694190710363253, "compression_ratio": 1.644808743169399, "no_speech_prob": 0.0008291490958072245}, {"id": 156, "seek": 90796, "start": 915.1600000000001, "end": 920.36, "text": " so what are the basic observations about this so we want often in these problems we want a", "tokens": [50724, 370, 437, 366, 264, 3875, 18163, 466, 341, 370, 321, 528, 2049, 294, 613, 2740, 321, 528, 257, 50984], "temperature": 0.0, "avg_logprob": -0.08694190710363253, "compression_ratio": 1.644808743169399, "no_speech_prob": 0.0008291490958072245}, {"id": 157, "seek": 90796, "start": 920.36, "end": 926.6800000000001, "text": " gamma invariant answer so if we move our picture of a cat around if we translate around the answer", "tokens": [50984, 15546, 33270, 394, 1867, 370, 498, 321, 1286, 527, 3036, 295, 257, 3857, 926, 498, 321, 13799, 926, 264, 1867, 51300], "temperature": 0.0, "avg_logprob": -0.08694190710363253, "compression_ratio": 1.644808743169399, "no_speech_prob": 0.0008291490958072245}, {"id": 158, "seek": 90796, "start": 926.6800000000001, "end": 931.1600000000001, "text": " should still be cat okay this is one of the reasons that I assumed a periodic", "tokens": [51300, 820, 920, 312, 3857, 1392, 341, 307, 472, 295, 264, 4112, 300, 286, 15895, 257, 27790, 51524], "temperature": 0.0, "avg_logprob": -0.08694190710363253, "compression_ratio": 1.644808743169399, "no_speech_prob": 0.0008291490958072245}, {"id": 159, "seek": 93116, "start": 931.48, "end": 940.92, "text": " periodic image that's very important another point is that so there's another classic machine", "tokens": [50380, 27790, 3256, 300, 311, 588, 1021, 1071, 935, 307, 300, 370, 456, 311, 1071, 7230, 3479, 50852], "temperature": 0.0, "avg_logprob": -0.07249330878257751, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.009262735955417156}, {"id": 160, "seek": 93116, "start": 940.92, "end": 947.9599999999999, "text": " learning task which is called image segmentation which asks us to say where are the two eyes in", "tokens": [50852, 2539, 5633, 597, 307, 1219, 3256, 9469, 399, 597, 8962, 505, 281, 584, 689, 366, 264, 732, 2575, 294, 51204], "temperature": 0.0, "avg_logprob": -0.07249330878257751, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.009262735955417156}, {"id": 161, "seek": 93116, "start": 947.9599999999999, "end": 953.7199999999999, "text": " this picture or you know when your phone for example tells you where the head of the people", "tokens": [51204, 341, 3036, 420, 291, 458, 562, 428, 2593, 337, 1365, 5112, 291, 689, 264, 1378, 295, 264, 561, 51492], "temperature": 0.0, "avg_logprob": -0.07249330878257751, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.009262735955417156}, {"id": 162, "seek": 93116, "start": 953.7199999999999, "end": 959.16, "text": " this is an example of image segmentation now if you think about what this task is", "tokens": [51492, 341, 307, 364, 1365, 295, 3256, 9469, 399, 586, 498, 291, 519, 466, 437, 341, 5633, 307, 51764], "temperature": 0.0, "avg_logprob": -0.07249330878257751, "compression_ratio": 1.7707317073170732, "no_speech_prob": 0.009262735955417156}, {"id": 163, "seek": 95916, "start": 959.16, "end": 964.4399999999999, "text": " this is not invariant it's equivariant this means that if I move the image", "tokens": [50364, 341, 307, 406, 33270, 394, 309, 311, 1267, 592, 3504, 394, 341, 1355, 300, 498, 286, 1286, 264, 3256, 50628], "temperature": 0.0, "avg_logprob": -0.11160557166389797, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.0009242936503142118}, {"id": 164, "seek": 95916, "start": 965.56, "end": 972.28, "text": " my my prediction should move in the same way so we often want a gamma invariant answer or a gamma", "tokens": [50684, 452, 452, 17630, 820, 1286, 294, 264, 912, 636, 370, 321, 2049, 528, 257, 15546, 33270, 394, 1867, 420, 257, 15546, 51020], "temperature": 0.0, "avg_logprob": -0.11160557166389797, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.0009242936503142118}, {"id": 165, "seek": 97228, "start": 972.28, "end": 983.4, "text": " equivariant answer", "tokens": [50364, 1267, 592, 3504, 394, 1867, 50920], "temperature": 0.0, "avg_logprob": -0.44674963421291775, "compression_ratio": 0.6923076923076923, "no_speech_prob": 0.012214832939207554}, {"id": 166, "seek": 98340, "start": 983.4, "end": 1005.3199999999999, "text": " okay convolution and value are gamma equivariant and for simplicity I'm ignoring pooling layers", "tokens": [50364, 1392, 45216, 293, 2158, 366, 15546, 1267, 592, 3504, 394, 293, 337, 25632, 286, 478, 26258, 7005, 278, 7914, 51460], "temperature": 0.0, "avg_logprob": -0.17735622145912863, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.004188165999948978}, {"id": 167, "seek": 98340, "start": 1005.3199999999999, "end": 1009.8, "text": " but we can definitely add them into the discussion but I feel like the the guts of this business", "tokens": [51460, 457, 321, 393, 2138, 909, 552, 666, 264, 5017, 457, 286, 841, 411, 264, 264, 28560, 295, 341, 1606, 51684], "temperature": 0.0, "avg_logprob": -0.17735622145912863, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.004188165999948978}, {"id": 168, "seek": 100980, "start": 1010.76, "end": 1013.8, "text": " really is exposed when we ignore pooling so I'm going to do that from now on", "tokens": [50412, 534, 307, 9495, 562, 321, 11200, 7005, 278, 370, 286, 478, 516, 281, 360, 300, 490, 586, 322, 50564], "temperature": 0.0, "avg_logprob": -0.06346022914832747, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.00453444616869092}, {"id": 169, "seek": 100980, "start": 1015.7199999999999, "end": 1026.2, "text": " so so convolution is equivariant and and value is equivariant if we so we have our image we", "tokens": [50660, 370, 370, 45216, 307, 1267, 592, 3504, 394, 293, 293, 2158, 307, 1267, 592, 3504, 394, 498, 321, 370, 321, 362, 527, 3256, 321, 51184], "temperature": 0.0, "avg_logprob": -0.06346022914832747, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.00453444616869092}, {"id": 170, "seek": 100980, "start": 1026.2, "end": 1031.48, "text": " have a whole bunch of real numbers if we translate and then set some of those numbers to zero that's", "tokens": [51184, 362, 257, 1379, 3840, 295, 957, 3547, 498, 321, 13799, 293, 550, 992, 512, 295, 729, 3547, 281, 4018, 300, 311, 51448], "temperature": 0.0, "avg_logprob": -0.06346022914832747, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.00453444616869092}, {"id": 171, "seek": 103148, "start": 1031.56, "end": 1033.88, "text": " the same thing as setting some of those numbers to zero and then translate", "tokens": [50368, 264, 912, 551, 382, 3287, 512, 295, 729, 3547, 281, 4018, 293, 550, 13799, 50484], "temperature": 0.0, "avg_logprob": -0.0860576331615448, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.003761880798265338}, {"id": 172, "seek": 103148, "start": 1038.44, "end": 1045.32, "text": " locality yeah no any activation function would be equivariant but as we'll discuss in a second", "tokens": [50712, 1628, 1860, 1338, 572, 604, 24433, 2445, 576, 312, 1267, 592, 3504, 394, 457, 382, 321, 603, 2248, 294, 257, 1150, 51056], "temperature": 0.0, "avg_logprob": -0.0860576331615448, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.003761880798265338}, {"id": 173, "seek": 103148, "start": 1045.32, "end": 1050.28, "text": " it's really essential that these are permutation representations for an active function activation", "tokens": [51056, 309, 311, 534, 7115, 300, 613, 366, 4784, 11380, 33358, 337, 364, 4967, 2445, 24433, 51304], "temperature": 0.0, "avg_logprob": -0.0860576331615448, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.003761880798265338}, {"id": 174, "seek": 103148, "start": 1050.28, "end": 1057.96, "text": " function to be equivariant so so we have requirement one we want a gamma invariant answer", "tokens": [51304, 2445, 281, 312, 1267, 592, 3504, 394, 370, 370, 321, 362, 11695, 472, 321, 528, 257, 15546, 33270, 394, 1867, 51688], "temperature": 0.0, "avg_logprob": -0.0860576331615448, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.003761880798265338}, {"id": 175, "seek": 105796, "start": 1058.28, "end": 1065.64, "text": " requirement two is that everything going on in this network should be gamma equivariant", "tokens": [50380, 11695, 732, 307, 300, 1203, 516, 322, 294, 341, 3209, 820, 312, 15546, 1267, 592, 3504, 394, 50748], "temperature": 0.0, "avg_logprob": -0.1151171326637268, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.002047017915174365}, {"id": 176, "seek": 105796, "start": 1066.8400000000001, "end": 1074.3600000000001, "text": " requirement three is locality so what this says often in you know if you look apparently at the", "tokens": [50808, 11695, 1045, 307, 1628, 1860, 370, 437, 341, 1619, 2049, 294, 291, 458, 498, 291, 574, 7970, 412, 264, 51184], "temperature": 0.0, "avg_logprob": -0.1151171326637268, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.002047017915174365}, {"id": 177, "seek": 105796, "start": 1074.3600000000001, "end": 1080.68, "text": " early layers of the brain in the visual cortex what happens is local and so and it's very natural", "tokens": [51184, 2440, 7914, 295, 264, 3567, 294, 264, 5056, 33312, 437, 2314, 307, 2654, 293, 370, 293, 309, 311, 588, 3303, 51500], "temperature": 0.0, "avg_logprob": -0.1151171326637268, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.002047017915174365}, {"id": 178, "seek": 108068, "start": 1080.68, "end": 1088.44, "text": " to also impose this in a cnn so we our our filters are supported around the identity initially", "tokens": [50364, 281, 611, 26952, 341, 294, 257, 269, 26384, 370, 321, 527, 527, 15995, 366, 8104, 926, 264, 6575, 9105, 50752], "temperature": 0.0, "avg_logprob": -0.09976436875083229, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.009697486646473408}, {"id": 179, "seek": 108068, "start": 1088.44, "end": 1093.72, "text": " and then later on we let them grow out through pooling and potentially fully connected layers", "tokens": [50752, 293, 550, 1780, 322, 321, 718, 552, 1852, 484, 807, 7005, 278, 293, 7263, 4498, 4582, 7914, 51016], "temperature": 0.0, "avg_logprob": -0.09976436875083229, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.009697486646473408}, {"id": 180, "seek": 108068, "start": 1095.0, "end": 1101.64, "text": " so an actual cnn we wouldn't require we wouldn't look at periodic images and what we would do", "tokens": [51080, 370, 364, 3539, 269, 26384, 321, 2759, 380, 3651, 321, 2759, 380, 574, 412, 27790, 5267, 293, 437, 321, 576, 360, 51412], "temperature": 0.0, "avg_logprob": -0.09976436875083229, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.009697486646473408}, {"id": 181, "seek": 108068, "start": 1101.64, "end": 1109.0800000000002, "text": " is pat around the edge um to make all our images the same size for example yeah and we would only", "tokens": [51412, 307, 1947, 926, 264, 4691, 1105, 281, 652, 439, 527, 5267, 264, 912, 2744, 337, 1365, 1338, 293, 321, 576, 787, 51784], "temperature": 0.0, "avg_logprob": -0.09976436875083229, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.009697486646473408}, {"id": 182, "seek": 110908, "start": 1109.08, "end": 1113.96, "text": " have we the same principles would be there but we wouldn't have a kind of full symmetry that only", "tokens": [50364, 362, 321, 264, 912, 9156, 576, 312, 456, 457, 321, 2759, 380, 362, 257, 733, 295, 1577, 25440, 300, 787, 50608], "temperature": 0.0, "avg_logprob": -0.09990668296813965, "compression_ratio": 1.8141263940520447, "no_speech_prob": 0.0015726127894595265}, {"id": 183, "seek": 110908, "start": 1113.96, "end": 1119.6399999999999, "text": " makes sense to shift pictures a little bit but what I find remarkable here and it's kind of a", "tokens": [50608, 1669, 2020, 281, 5513, 5242, 257, 707, 857, 457, 437, 286, 915, 12802, 510, 293, 309, 311, 733, 295, 257, 50892], "temperature": 0.0, "avg_logprob": -0.09990668296813965, "compression_ratio": 1.8141263940520447, "no_speech_prob": 0.0015726127894595265}, {"id": 184, "seek": 110908, "start": 1119.6399999999999, "end": 1125.32, "text": " simple simple thing is that gamma invariance gamma equivariance and locality basically tell me what", "tokens": [50892, 2199, 2199, 551, 307, 300, 15546, 33270, 719, 15546, 1267, 592, 3504, 719, 293, 1628, 1860, 1936, 980, 385, 437, 51176], "temperature": 0.0, "avg_logprob": -0.09990668296813965, "compression_ratio": 1.8141263940520447, "no_speech_prob": 0.0015726127894595265}, {"id": 185, "seek": 110908, "start": 1125.32, "end": 1130.9199999999998, "text": " I what I have to do in my neural net so if you assume that you should compose it out of simple", "tokens": [51176, 286, 437, 286, 362, 281, 360, 294, 452, 18161, 2533, 370, 498, 291, 6552, 300, 291, 820, 35925, 309, 484, 295, 2199, 51456], "temperature": 0.0, "avg_logprob": -0.09990668296813965, "compression_ratio": 1.8141263940520447, "no_speech_prob": 0.0015726127894595265}, {"id": 186, "seek": 110908, "start": 1130.9199999999998, "end": 1137.96, "text": " functions and if you fix relu then everything else is basically specified which is remark which seems", "tokens": [51456, 6828, 293, 498, 291, 3191, 1039, 84, 550, 1203, 1646, 307, 1936, 22206, 597, 307, 7942, 597, 2544, 51808], "temperature": 0.0, "avg_logprob": -0.09990668296813965, "compression_ratio": 1.8141263940520447, "no_speech_prob": 0.0015726127894595265}, {"id": 187, "seek": 113796, "start": 1137.96, "end": 1145.4, "text": " to me to be remarkable and what I want to explain soon is that there's kind of nothing special about", "tokens": [50364, 281, 385, 281, 312, 12802, 293, 437, 286, 528, 281, 2903, 2321, 307, 300, 456, 311, 733, 295, 1825, 2121, 466, 50736], "temperature": 0.0, "avg_logprob": -0.08931474387645721, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0012241625227034092}, {"id": 188, "seek": 113796, "start": 1145.4, "end": 1151.8, "text": " this particular group that this would actually tell us how to make predictions on any space on", "tokens": [50736, 341, 1729, 1594, 300, 341, 576, 767, 980, 505, 577, 281, 652, 21264, 322, 604, 1901, 322, 51056], "temperature": 0.0, "avg_logprob": -0.08931474387645721, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0012241625227034092}, {"id": 189, "seek": 113796, "start": 1151.8, "end": 1158.28, "text": " which a group acts transitively so let me just emphasize one extremely important point from a", "tokens": [51056, 597, 257, 1594, 10672, 1145, 2187, 356, 370, 718, 385, 445, 16078, 472, 4664, 1021, 935, 490, 257, 51380], "temperature": 0.0, "avg_logprob": -0.08931474387645721, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.0012241625227034092}, {"id": 190, "seek": 115828, "start": 1159.24, "end": 1168.76, "text": " implementation point of view okay so just for completeness gaug asked here whether", "tokens": [50412, 11420, 935, 295, 1910, 1392, 370, 445, 337, 1557, 15264, 5959, 697, 2351, 510, 1968, 50888], "temperature": 0.0, "avg_logprob": -0.11653541613228713, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.026305384933948517}, {"id": 191, "seek": 115828, "start": 1170.12, "end": 1176.04, "text": " relu was specific for it being equivariant and the response was no any non-linear activation", "tokens": [50956, 1039, 84, 390, 2685, 337, 309, 885, 1267, 592, 3504, 394, 293, 264, 4134, 390, 572, 604, 2107, 12, 28263, 24433, 51252], "temperature": 0.0, "avg_logprob": -0.11653541613228713, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.026305384933948517}, {"id": 192, "seek": 115828, "start": 1176.04, "end": 1180.36, "text": " function would be fine at this point as long as our representation is a permutation representation", "tokens": [51252, 2445, 576, 312, 2489, 412, 341, 935, 382, 938, 382, 527, 10290, 307, 257, 4784, 11380, 10290, 51468], "temperature": 0.0, "avg_logprob": -0.11653541613228713, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.026305384933948517}, {"id": 193, "seek": 115828, "start": 1181.3999999999999, "end": 1188.04, "text": " and what Stefan asked was in this particular picture here um you know for completeness", "tokens": [51520, 293, 437, 32158, 2351, 390, 294, 341, 1729, 3036, 510, 1105, 291, 458, 337, 1557, 15264, 51852], "temperature": 0.0, "avg_logprob": -0.11653541613228713, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.026305384933948517}, {"id": 194, "seek": 118804, "start": 1188.04, "end": 1191.6399999999999, "text": " if we translated this little cat we'd have half the cat's head over here and half the cat's head", "tokens": [50364, 498, 321, 16805, 341, 707, 3857, 321, 1116, 362, 1922, 264, 3857, 311, 1378, 670, 510, 293, 1922, 264, 3857, 311, 1378, 50544], "temperature": 0.0, "avg_logprob": -0.0698269482316642, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.0018079793080687523}, {"id": 195, "seek": 118804, "start": 1191.6399999999999, "end": 1196.44, "text": " over here and the question was you know how does an how does an actual cnn in real life", "tokens": [50544, 670, 510, 293, 264, 1168, 390, 291, 458, 577, 775, 364, 577, 775, 364, 3539, 269, 26384, 294, 957, 993, 50784], "temperature": 0.0, "avg_logprob": -0.0698269482316642, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.0018079793080687523}, {"id": 196, "seek": 118804, "start": 1198.44, "end": 1204.6, "text": " do this and basically you know we don't we don't enforce that full equivariance we only", "tokens": [50884, 360, 341, 293, 1936, 291, 458, 321, 500, 380, 321, 500, 380, 24825, 300, 1577, 1267, 592, 3504, 719, 321, 787, 51192], "temperature": 0.0, "avg_logprob": -0.0698269482316642, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.0018079793080687523}, {"id": 197, "seek": 118804, "start": 1204.6, "end": 1209.48, "text": " allow kind of small translations within some bounded region so kind of partial symmetry", "tokens": [51192, 2089, 733, 295, 1359, 37578, 1951, 512, 37498, 4458, 370, 733, 295, 14641, 25440, 51436], "temperature": 0.0, "avg_logprob": -0.0698269482316642, "compression_ratio": 1.8274111675126903, "no_speech_prob": 0.0018079793080687523}, {"id": 198, "seek": 120948, "start": 1210.28, "end": 1211.48, "text": " thank you very much for the reminder", "tokens": [50404, 1309, 291, 588, 709, 337, 264, 13548, 50464], "temperature": 0.0, "avg_logprob": -0.08302988944115577, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.023665422573685646}, {"id": 199, "seek": 120948, "start": 1214.04, "end": 1222.28, "text": " so this is very important from a basic implementation point of view imagine on the left hand side we", "tokens": [50592, 370, 341, 307, 588, 1021, 490, 257, 3875, 11420, 935, 295, 1910, 3811, 322, 264, 1411, 1011, 1252, 321, 51004], "temperature": 0.0, "avg_logprob": -0.08302988944115577, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.023665422573685646}, {"id": 200, "seek": 120948, "start": 1222.28, "end": 1229.72, "text": " have two we this is a layer of our neural net and so we have l1 inputs and l2 outputs", "tokens": [51004, 362, 732, 321, 341, 307, 257, 4583, 295, 527, 18161, 2533, 293, 370, 321, 362, 287, 16, 15743, 293, 287, 17, 23930, 51376], "temperature": 0.0, "avg_logprob": -0.08302988944115577, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.023665422573685646}, {"id": 201, "seek": 120948, "start": 1230.84, "end": 1236.04, "text": " single layer so now if you think about the number of parameters it's l1 times l2", "tokens": [51432, 2167, 4583, 370, 586, 498, 291, 519, 466, 264, 1230, 295, 9834, 309, 311, 287, 16, 1413, 287, 17, 51692], "temperature": 0.0, "avg_logprob": -0.08302988944115577, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.023665422573685646}, {"id": 202, "seek": 123604, "start": 1236.2, "end": 1245.32, "text": " so if it's easy to find a picture for example with 10 million uh 10 million pixels in it", "tokens": [50372, 370, 498, 309, 311, 1858, 281, 915, 257, 3036, 337, 1365, 365, 1266, 2459, 2232, 1266, 2459, 18668, 294, 309, 50828], "temperature": 0.0, "avg_logprob": -0.12968820684096394, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0008283390779979527}, {"id": 203, "seek": 123604, "start": 1245.8799999999999, "end": 1254.44, "text": " and if we do one layer then that's whatever 10 million squared is you know i'm not a physicist", "tokens": [50856, 293, 498, 321, 360, 472, 4583, 550, 300, 311, 2035, 1266, 2459, 8889, 307, 291, 458, 741, 478, 406, 257, 42466, 51284], "temperature": 0.0, "avg_logprob": -0.12968820684096394, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0008283390779979527}, {"id": 204, "seek": 123604, "start": 1254.44, "end": 1259.72, "text": " i don't know what 10 million squared is okay so some enormous number that i can probably never", "tokens": [51284, 741, 500, 380, 458, 437, 1266, 2459, 8889, 307, 1392, 370, 512, 11322, 1230, 300, 741, 393, 1391, 1128, 51548], "temperature": 0.0, "avg_logprob": -0.12968820684096394, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0008283390779979527}, {"id": 205, "seek": 125972, "start": 1259.72, "end": 1270.68, "text": " actually train on a computer and but if we're doing so here here i have a first layer of a", "tokens": [50364, 767, 3847, 322, 257, 3820, 293, 457, 498, 321, 434, 884, 370, 510, 510, 741, 362, 257, 700, 4583, 295, 257, 50912], "temperature": 0.0, "avg_logprob": -0.09317704169980941, "compression_ratio": 1.6257668711656441, "no_speech_prob": 0.006793866865336895}, {"id": 206, "seek": 125972, "start": 1271.4, "end": 1277.4, "text": " convolutional neural net in one dimensional in one dimension so here are my inputs", "tokens": [50948, 45216, 304, 18161, 2533, 294, 472, 18795, 294, 472, 10139, 370, 510, 366, 452, 15743, 51248], "temperature": 0.0, "avg_logprob": -0.09317704169980941, "compression_ratio": 1.6257668711656441, "no_speech_prob": 0.006793866865336895}, {"id": 207, "seek": 125972, "start": 1279.72, "end": 1286.6000000000001, "text": " and locality says that my filter only affects neighboring points so that's why i have these", "tokens": [51364, 293, 1628, 1860, 1619, 300, 452, 6608, 787, 11807, 31521, 2793, 370, 300, 311, 983, 741, 362, 613, 51708], "temperature": 0.0, "avg_logprob": -0.09317704169980941, "compression_ratio": 1.6257668711656441, "no_speech_prob": 0.006793866865336895}, {"id": 208, "seek": 128660, "start": 1286.6, "end": 1295.8799999999999, "text": " three parameters x y and z and equivariance says that these x y and z are the same across the whole", "tokens": [50364, 1045, 9834, 2031, 288, 293, 710, 293, 1267, 592, 3504, 719, 1619, 300, 613, 2031, 288, 293, 710, 366, 264, 912, 2108, 264, 1379, 50828], "temperature": 0.0, "avg_logprob": -0.08975787295235528, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0021819444373250008}, {"id": 209, "seek": 128660, "start": 1295.8799999999999, "end": 1304.9199999999998, "text": " thing so no matter how big this layer is it just depends on three size um three parameters okay", "tokens": [50828, 551, 370, 572, 1871, 577, 955, 341, 4583, 307, 309, 445, 5946, 322, 1045, 2744, 1105, 1045, 9834, 1392, 51280], "temperature": 0.0, "avg_logprob": -0.08975787295235528, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0021819444373250008}, {"id": 210, "seek": 128660, "start": 1304.9199999999998, "end": 1311.8, "text": " so i should emphasize this is one piece of the first layer so up here this would be one of these", "tokens": [51280, 370, 741, 820, 16078, 341, 307, 472, 2522, 295, 264, 700, 4583, 370, 493, 510, 341, 576, 312, 472, 295, 613, 51624], "temperature": 0.0, "avg_logprob": -0.08975787295235528, "compression_ratio": 1.7380952380952381, "no_speech_prob": 0.0021819444373250008}, {"id": 211, "seek": 131180, "start": 1311.8, "end": 1318.52, "text": " pieces so i could expect to have potentially 20 of these pieces or something but still some you", "tokens": [50364, 3755, 370, 741, 727, 2066, 281, 362, 7263, 945, 295, 613, 3755, 420, 746, 457, 920, 512, 291, 50700], "temperature": 0.0, "avg_logprob": -0.06590584346226283, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.0017536840168759227}, {"id": 212, "seek": 131180, "start": 1318.52, "end": 1326.52, "text": " know 20 times 3 is a lot smaller than 10 million squared i'm enough of a physicist to know that", "tokens": [50700, 458, 945, 1413, 805, 307, 257, 688, 4356, 813, 1266, 2459, 8889, 741, 478, 1547, 295, 257, 42466, 281, 458, 300, 51100], "temperature": 0.0, "avg_logprob": -0.06590584346226283, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.0017536840168759227}, {"id": 213, "seek": 131180, "start": 1326.52, "end": 1338.76, "text": " inequality okay so now i want to explain a blueprint for learning on a general homogeneous", "tokens": [51100, 16970, 1392, 370, 586, 741, 528, 281, 2903, 257, 35868, 337, 2539, 322, 257, 2674, 42632, 51712], "temperature": 0.0, "avg_logprob": -0.06590584346226283, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.0017536840168759227}, {"id": 214, "seek": 133876, "start": 1338.76, "end": 1348.12, "text": " space for a group so we have our group and i'm typically thinking about a finite group or a", "tokens": [50364, 1901, 337, 257, 1594, 370, 321, 362, 527, 1594, 293, 741, 478, 5850, 1953, 466, 257, 19362, 1594, 420, 257, 50832], "temperature": 0.0, "avg_logprob": -0.09629639457253848, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.0016225529834628105}, {"id": 215, "seek": 133876, "start": 1348.12, "end": 1356.04, "text": " league group like so three or something like that so gamma was z mod h z squared before", "tokens": [50832, 14957, 1594, 411, 370, 1045, 420, 746, 411, 300, 370, 15546, 390, 710, 1072, 276, 710, 8889, 949, 51228], "temperature": 0.0, "avg_logprob": -0.09629639457253848, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.0016225529834628105}, {"id": 216, "seek": 133876, "start": 1356.92, "end": 1364.36, "text": " and x is a transitive gamma set so this just means that um gamma acts transitively on x but", "tokens": [51272, 293, 2031, 307, 257, 1145, 2187, 15546, 992, 370, 341, 445, 1355, 300, 1105, 15546, 10672, 1145, 2187, 356, 322, 2031, 457, 51644], "temperature": 0.0, "avg_logprob": -0.09629639457253848, "compression_ratio": 1.6424242424242423, "no_speech_prob": 0.0016225529834628105}, {"id": 217, "seek": 136436, "start": 1364.36, "end": 1368.6799999999998, "text": " in the category of leap in the category of differential manifold this would mean that", "tokens": [50364, 294, 264, 7719, 295, 19438, 294, 264, 7719, 295, 15756, 47138, 341, 576, 914, 300, 50580], "temperature": 0.0, "avg_logprob": -0.08679825067520142, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.0013880723854526877}, {"id": 218, "seek": 136436, "start": 1368.6799999999998, "end": 1376.6799999999998, "text": " i have a manifold with a continuous action of my or a smooth action of my league and in any", "tokens": [50580, 741, 362, 257, 47138, 365, 257, 10957, 3069, 295, 452, 420, 257, 5508, 3069, 295, 452, 14957, 293, 294, 604, 50980], "temperature": 0.0, "avg_logprob": -0.08679825067520142, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.0013880723854526877}, {"id": 219, "seek": 136436, "start": 1376.6799999999998, "end": 1385.24, "text": " situation in which um this makes sense we have that x is just the same thing as gamma mod a", "tokens": [50980, 2590, 294, 597, 1105, 341, 1669, 2020, 321, 362, 300, 2031, 307, 445, 264, 912, 551, 382, 15546, 1072, 257, 51408], "temperature": 0.0, "avg_logprob": -0.08679825067520142, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.0013880723854526877}, {"id": 220, "seek": 136436, "start": 1385.24, "end": 1391.8799999999999, "text": " single stabilizer and what we want to do is learn an invariant so i'll stick to the invariant case", "tokens": [51408, 2167, 11652, 6545, 293, 437, 321, 528, 281, 360, 307, 1466, 364, 33270, 394, 370, 741, 603, 2897, 281, 264, 33270, 394, 1389, 51740], "temperature": 0.0, "avg_logprob": -0.08679825067520142, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.0013880723854526877}, {"id": 221, "seek": 139188, "start": 1391.88, "end": 1398.5200000000002, "text": " but notice that we might also want to make an equivariant prediction function from functions on", "tokens": [50364, 457, 3449, 300, 321, 1062, 611, 528, 281, 652, 364, 1267, 592, 3504, 394, 17630, 2445, 490, 6828, 322, 50696], "temperature": 0.0, "avg_logprob": -0.0678546233255355, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.002550120698288083}, {"id": 222, "seek": 139188, "start": 1398.5200000000002, "end": 1405.96, "text": " x to r now very basic representation theoretic observation or maybe so basic but it's not yet", "tokens": [50696, 2031, 281, 367, 586, 588, 3875, 10290, 14308, 299, 14816, 420, 1310, 370, 3875, 457, 309, 311, 406, 1939, 51068], "temperature": 0.0, "avg_logprob": -0.0678546233255355, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.002550120698288083}, {"id": 223, "seek": 139188, "start": 1405.96, "end": 1414.3600000000001, "text": " representation theory is that because our action is transitive there is only one linear", "tokens": [51068, 10290, 5261, 307, 300, 570, 527, 3069, 307, 1145, 2187, 456, 307, 787, 472, 8213, 51488], "temperature": 0.0, "avg_logprob": -0.0678546233255355, "compression_ratio": 1.6890243902439024, "no_speech_prob": 0.002550120698288083}, {"id": 224, "seek": 141436, "start": 1414.36, "end": 1424.36, "text": " uh or at most one linear map from functions on x to r that is invariant namely", "tokens": [50364, 2232, 420, 412, 881, 472, 8213, 4471, 490, 6828, 322, 2031, 281, 367, 300, 307, 33270, 394, 20926, 50864], "temperature": 0.0, "avg_logprob": -0.09713802640400236, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.020953450351953506}, {"id": 225, "seek": 141436, "start": 1425.8, "end": 1434.28, "text": " like summing over my finite set or integrating or so i found this kind of illustrative because", "tokens": [50936, 411, 2408, 2810, 670, 452, 19362, 992, 420, 26889, 420, 370, 741, 1352, 341, 733, 295, 8490, 30457, 570, 51360], "temperature": 0.0, "avg_logprob": -0.09713802640400236, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.020953450351953506}, {"id": 226, "seek": 141436, "start": 1434.28, "end": 1439.08, "text": " this tells you for example in the image classification task you're definitely looking for a nonlinear", "tokens": [51360, 341, 5112, 291, 337, 1365, 294, 264, 3256, 21538, 5633, 291, 434, 2138, 1237, 337, 257, 2107, 28263, 51600], "temperature": 0.0, "avg_logprob": -0.09713802640400236, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.020953450351953506}, {"id": 227, "seek": 143908, "start": 1439.08, "end": 1445.8, "text": " function because a linear function would be like averaging over pixel values and this is the kind", "tokens": [50364, 2445, 570, 257, 8213, 2445, 576, 312, 411, 47308, 670, 19261, 4190, 293, 341, 307, 264, 733, 50700], "temperature": 0.0, "avg_logprob": -0.0819321796298027, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0015720968367531896}, {"id": 228, "seek": 143908, "start": 1445.8, "end": 1455.24, "text": " of silliest thing that one could imagine so we're looking for some um invariant function", "tokens": [50700, 295, 37160, 6495, 551, 300, 472, 727, 3811, 370, 321, 434, 1237, 337, 512, 1105, 33270, 394, 2445, 51172], "temperature": 0.0, "avg_logprob": -0.0819321796298027, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0015720968367531896}, {"id": 229, "seek": 143908, "start": 1457.0, "end": 1467.72, "text": " and here's the blueprint so we fix a transitive gamma set this is where we want to make the", "tokens": [51260, 293, 510, 311, 264, 35868, 370, 321, 3191, 257, 1145, 2187, 15546, 992, 341, 307, 689, 321, 528, 281, 652, 264, 51796], "temperature": 0.0, "avg_logprob": -0.0819321796298027, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.0015720968367531896}, {"id": 230, "seek": 146772, "start": 1467.8, "end": 1472.6000000000001, "text": " prediction and then we just our architecture consists of a whole lot of choices of transitive", "tokens": [50368, 17630, 293, 550, 321, 445, 527, 9482, 14689, 295, 257, 1379, 688, 295, 7994, 295, 1145, 2187, 50608], "temperature": 0.0, "avg_logprob": -0.04172046213264925, "compression_ratio": 1.8781725888324874, "no_speech_prob": 0.0009245151304639876}, {"id": 231, "seek": 146772, "start": 1472.6000000000001, "end": 1479.72, "text": " gamma sets and basically i think one way to think about these transitive gamma sets is", "tokens": [50608, 15546, 6352, 293, 1936, 741, 519, 472, 636, 281, 519, 466, 613, 1145, 2187, 15546, 6352, 307, 50964], "temperature": 0.0, "avg_logprob": -0.04172046213264925, "compression_ratio": 1.8781725888324874, "no_speech_prob": 0.0009245151304639876}, {"id": 232, "seek": 146772, "start": 1479.72, "end": 1486.6000000000001, "text": " so the the invariant prediction says that you want to take um so any gamma has an important", "tokens": [50964, 370, 264, 264, 33270, 394, 17630, 1619, 300, 291, 528, 281, 747, 1105, 370, 604, 15546, 575, 364, 1021, 51308], "temperature": 0.0, "avg_logprob": -0.04172046213264925, "compression_ratio": 1.8781725888324874, "no_speech_prob": 0.0009245151304639876}, {"id": 233, "seek": 146772, "start": 1486.6000000000001, "end": 1492.84, "text": " transitive gamma set namely one point and this is where we want our prediction to end up and then", "tokens": [51308, 1145, 2187, 15546, 992, 20926, 472, 935, 293, 341, 307, 689, 321, 528, 527, 17630, 281, 917, 493, 293, 550, 51620], "temperature": 0.0, "avg_logprob": -0.04172046213264925, "compression_ratio": 1.8781725888324874, "no_speech_prob": 0.0009245151304639876}, {"id": 234, "seek": 149284, "start": 1492.84, "end": 1499.72, "text": " if you look at um classical c and n's you want your sets to kind of slowly get smaller in some", "tokens": [50364, 498, 291, 574, 412, 1105, 13735, 269, 293, 297, 311, 291, 528, 428, 6352, 281, 733, 295, 5692, 483, 4356, 294, 512, 50708], "temperature": 0.0, "avg_logprob": -0.06625846660498416, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0005111487698741257}, {"id": 235, "seek": 149284, "start": 1499.72, "end": 1503.32, "text": " sense until you reach the prediction so you can think about these transitive gamma sets", "tokens": [50708, 2020, 1826, 291, 2524, 264, 17630, 370, 291, 393, 519, 466, 613, 1145, 2187, 15546, 6352, 50888], "temperature": 0.0, "avg_logprob": -0.06625846660498416, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0005111487698741257}, {"id": 236, "seek": 149284, "start": 1503.32, "end": 1514.28, "text": " as slowly decreasing in size if that makes sense and this is all we do so we um consider some", "tokens": [50888, 382, 5692, 23223, 294, 2744, 498, 300, 1669, 2020, 293, 341, 307, 439, 321, 360, 370, 321, 1105, 1949, 512, 51436], "temperature": 0.0, "avg_logprob": -0.06625846660498416, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.0005111487698741257}, {"id": 237, "seek": 151428, "start": 1514.36, "end": 1521.0, "text": " equivariant maps so convolution so this should be a gamma a gamma equivariant", "tokens": [50368, 1267, 592, 3504, 394, 11317, 370, 45216, 370, 341, 820, 312, 257, 15546, 257, 15546, 1267, 592, 3504, 394, 50700], "temperature": 0.0, "avg_logprob": -0.11755105611440297, "compression_ratio": 2.126984126984127, "no_speech_prob": 0.012226072140038013}, {"id": 238, "seek": 151428, "start": 1524.52, "end": 1527.8799999999999, "text": " linear map", "tokens": [50876, 8213, 4471, 51044], "temperature": 0.0, "avg_logprob": -0.11755105611440297, "compression_ratio": 2.126984126984127, "no_speech_prob": 0.012226072140038013}, {"id": 239, "seek": 151428, "start": 1530.28, "end": 1533.08, "text": " and then we never do a relu and then we do another one and then we do a relu and then", "tokens": [51164, 293, 550, 321, 1128, 360, 257, 1039, 84, 293, 550, 321, 360, 1071, 472, 293, 550, 321, 360, 257, 1039, 84, 293, 550, 51304], "temperature": 0.0, "avg_logprob": -0.11755105611440297, "compression_ratio": 2.126984126984127, "no_speech_prob": 0.012226072140038013}, {"id": 240, "seek": 151428, "start": 1533.08, "end": 1539.8799999999999, "text": " we do another one and we want to train across the parameters of gamma equivariant linear maps", "tokens": [51304, 321, 360, 1071, 472, 293, 321, 528, 281, 3847, 2108, 264, 9834, 295, 15546, 1267, 592, 3504, 394, 8213, 11317, 51644], "temperature": 0.0, "avg_logprob": -0.11755105611440297, "compression_ratio": 2.126984126984127, "no_speech_prob": 0.012226072140038013}, {"id": 241, "seek": 153988, "start": 1540.6000000000001, "end": 1546.1200000000001, "text": " yeah and probably this is a point of it so this whole space is r", "tokens": [50400, 1338, 293, 1391, 341, 307, 257, 935, 295, 309, 370, 341, 1379, 1901, 307, 367, 50676], "temperature": 0.0, "avg_logprob": -0.1351199702212685, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0006458362913690507}, {"id": 242, "seek": 153988, "start": 1550.2, "end": 1554.2800000000002, "text": " so if gamma has a metric or similar we might want convolution supported near the identity", "tokens": [50880, 370, 498, 15546, 575, 257, 20678, 420, 2531, 321, 1062, 528, 45216, 8104, 2651, 264, 6575, 51084], "temperature": 0.0, "avg_logprob": -0.1351199702212685, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0006458362913690507}, {"id": 243, "seek": 153988, "start": 1554.92, "end": 1558.68, "text": " now this is the most important point and i think that this has kind of been missed in the", "tokens": [51116, 586, 341, 307, 264, 881, 1021, 935, 293, 741, 519, 300, 341, 575, 733, 295, 668, 6721, 294, 264, 51304], "temperature": 0.0, "avg_logprob": -0.1351199702212685, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0006458362913690507}, {"id": 244, "seek": 153988, "start": 1559.4, "end": 1564.68, "text": " in the machine learning literature there's something very basic in representation theory", "tokens": [51340, 294, 264, 3479, 2539, 10394, 456, 311, 746, 588, 3875, 294, 10290, 5261, 51604], "temperature": 0.0, "avg_logprob": -0.1351199702212685, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0006458362913690507}, {"id": 245, "seek": 153988, "start": 1564.68, "end": 1569.16, "text": " which i call the double coset formula people might call it hecar algebras there's many", "tokens": [51604, 597, 741, 818, 264, 3834, 3792, 302, 8513, 561, 1062, 818, 309, 415, 6166, 419, 432, 38182, 456, 311, 867, 51828], "temperature": 0.0, "avg_logprob": -0.1351199702212685, "compression_ratio": 1.728395061728395, "no_speech_prob": 0.0006458362913690507}, {"id": 246, "seek": 156916, "start": 1569.16, "end": 1580.76, "text": " different names for it so we're asking what is such a map so because any transitive set is", "tokens": [50364, 819, 5288, 337, 309, 370, 321, 434, 3365, 437, 307, 1270, 257, 4471, 370, 570, 604, 1145, 2187, 992, 307, 50944], "temperature": 0.0, "avg_logprob": -0.051537156105041504, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.0009540976025164127}, {"id": 247, "seek": 156916, "start": 1580.76, "end": 1586.3600000000001, "text": " simply gamma mod h or gamma mod h prime we want to know what this home space is", "tokens": [50944, 2935, 15546, 1072, 276, 420, 15546, 1072, 276, 5835, 321, 528, 281, 458, 437, 341, 1280, 1901, 307, 51224], "temperature": 0.0, "avg_logprob": -0.051537156105041504, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.0009540976025164127}, {"id": 248, "seek": 156916, "start": 1587.96, "end": 1594.8400000000001, "text": " and the formula says that homomorphisms from such a function space to such a function space", "tokens": [51304, 293, 264, 8513, 1619, 300, 3655, 32702, 13539, 490, 1270, 257, 2445, 1901, 281, 1270, 257, 2445, 1901, 51648], "temperature": 0.0, "avg_logprob": -0.051537156105041504, "compression_ratio": 1.6582278481012658, "no_speech_prob": 0.0009540976025164127}, {"id": 249, "seek": 159484, "start": 1594.84, "end": 1597.0, "text": " are simply functions on double cosets", "tokens": [50364, 366, 2935, 6828, 322, 3834, 3792, 1385, 50472], "temperature": 0.0, "avg_logprob": -0.09314739581235905, "compression_ratio": 1.8526785714285714, "no_speech_prob": 0.0031215199269354343}, {"id": 250, "seek": 159484, "start": 1600.04, "end": 1604.84, "text": " now there's many different ways to understand this formula if you if you're in the world of", "tokens": [50624, 586, 456, 311, 867, 819, 2098, 281, 1223, 341, 8513, 498, 291, 498, 291, 434, 294, 264, 1002, 295, 50864], "temperature": 0.0, "avg_logprob": -0.09314739581235905, "compression_ratio": 1.8526785714285714, "no_speech_prob": 0.0031215199269354343}, {"id": 251, "seek": 159484, "start": 1604.84, "end": 1609.56, "text": " finite groups this is a very nice exercise if you're in the world of compact league groups it's a", "tokens": [50864, 19362, 3935, 341, 307, 257, 588, 1481, 5380, 498, 291, 434, 294, 264, 1002, 295, 14679, 14957, 3935, 309, 311, 257, 51100], "temperature": 0.0, "avg_logprob": -0.09314739581235905, "compression_ratio": 1.8526785714285714, "no_speech_prob": 0.0031215199269354343}, {"id": 252, "seek": 159484, "start": 1610.28, "end": 1617.1599999999999, "text": " significantly more difficult exercise but i just want you to accept this formula as a kind of", "tokens": [51136, 10591, 544, 2252, 5380, 457, 741, 445, 528, 291, 281, 3241, 341, 8513, 382, 257, 733, 295, 51480], "temperature": 0.0, "avg_logprob": -0.09314739581235905, "compression_ratio": 1.8526785714285714, "no_speech_prob": 0.0031215199269354343}, {"id": 253, "seek": 159484, "start": 1618.28, "end": 1624.36, "text": " beautiful thing in the world and we'll see it's very useful okay and if you want to know more", "tokens": [51536, 2238, 551, 294, 264, 1002, 293, 321, 603, 536, 309, 311, 588, 4420, 1392, 293, 498, 291, 528, 281, 458, 544, 51840], "temperature": 0.0, "avg_logprob": -0.09314739581235905, "compression_ratio": 1.8526785714285714, "no_speech_prob": 0.0031215199269354343}, {"id": 254, "seek": 162436, "start": 1624.36, "end": 1631.08, "text": " about it i'm very happy to talk more about this formula okay but this is a kind of i don't know", "tokens": [50364, 466, 309, 741, 478, 588, 2055, 281, 751, 544, 466, 341, 8513, 1392, 457, 341, 307, 257, 733, 295, 741, 500, 380, 458, 50700], "temperature": 0.0, "avg_logprob": -0.037462401390075686, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.0007910338463261724}, {"id": 255, "seek": 162436, "start": 1631.08, "end": 1637.4799999999998, "text": " very useful formula in many different situations so this is telling us what the possible space of", "tokens": [50700, 588, 4420, 8513, 294, 867, 819, 6851, 370, 341, 307, 3585, 505, 437, 264, 1944, 1901, 295, 51020], "temperature": 0.0, "avg_logprob": -0.037462401390075686, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.0007910338463261724}, {"id": 256, "seek": 162436, "start": 1637.4799999999998, "end": 1649.4799999999998, "text": " convolutions is so here's an example imagine that we're learning on a sphere so we have a nice sphere", "tokens": [51020, 3754, 15892, 307, 370, 510, 311, 364, 1365, 3811, 300, 321, 434, 2539, 322, 257, 16687, 370, 321, 362, 257, 1481, 16687, 51620], "temperature": 0.0, "avg_logprob": -0.037462401390075686, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.0007910338463261724}, {"id": 257, "seek": 164948, "start": 1649.48, "end": 1658.92, "text": " here and we have so three so this is um orthogonal three by three matrices of determinant one so", "tokens": [50364, 510, 293, 321, 362, 370, 1045, 370, 341, 307, 1105, 41488, 1045, 538, 1045, 32284, 295, 41296, 472, 370, 50836], "temperature": 0.0, "avg_logprob": -0.09827936263311476, "compression_ratio": 1.9793814432989691, "no_speech_prob": 0.007006114348769188}, {"id": 258, "seek": 164948, "start": 1658.92, "end": 1667.4, "text": " these are these transformations so it acts on the sphere and s2 is a transitive space i can move", "tokens": [50836, 613, 366, 613, 34852, 370, 309, 10672, 322, 264, 16687, 293, 262, 17, 307, 257, 1145, 2187, 1901, 741, 393, 1286, 51260], "temperature": 0.0, "avg_logprob": -0.09827936263311476, "compression_ratio": 1.9793814432989691, "no_speech_prob": 0.007006114348769188}, {"id": 259, "seek": 164948, "start": 1667.4, "end": 1671.48, "text": " any point on the sphere to another point on the sphere of iron orthogonal transformation", "tokens": [51260, 604, 935, 322, 264, 16687, 281, 1071, 935, 322, 264, 16687, 295, 6497, 41488, 9887, 51464], "temperature": 0.0, "avg_logprob": -0.09827936263311476, "compression_ratio": 1.9793814432989691, "no_speech_prob": 0.007006114348769188}, {"id": 260, "seek": 164948, "start": 1671.48, "end": 1679.08, "text": " what is the stabilizer of single point it's those rotations in the axis that point determines through", "tokens": [51464, 437, 307, 264, 11652, 6545, 295, 2167, 935, 309, 311, 729, 44796, 294, 264, 10298, 300, 935, 24799, 807, 51844], "temperature": 0.0, "avg_logprob": -0.09827936263311476, "compression_ratio": 1.9793814432989691, "no_speech_prob": 0.007006114348769188}, {"id": 261, "seek": 167908, "start": 1679.08, "end": 1689.8, "text": " the origin so s2 is s03 mod s1 now imagine that we want to learn on the sphere so we want to have", "tokens": [50364, 264, 4957, 370, 262, 17, 307, 262, 11592, 1072, 262, 16, 586, 3811, 300, 321, 528, 281, 1466, 322, 264, 16687, 370, 321, 528, 281, 362, 50900], "temperature": 0.0, "avg_logprob": -0.051096558570861816, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0021142978221178055}, {"id": 262, "seek": 167908, "start": 1689.8, "end": 1695.3999999999999, "text": " some image on the sphere some function on the sphere and we want to say it's a cat or something", "tokens": [50900, 512, 3256, 322, 264, 16687, 512, 2445, 322, 264, 16687, 293, 321, 528, 281, 584, 309, 311, 257, 3857, 420, 746, 51180], "temperature": 0.0, "avg_logprob": -0.051096558570861816, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0021142978221178055}, {"id": 263, "seek": 167908, "start": 1696.12, "end": 1701.96, "text": " you might ask i don't generally see pictures of cats on spheres this is my answer to that", "tokens": [51216, 291, 1062, 1029, 741, 500, 380, 5101, 536, 5242, 295, 11111, 322, 41225, 341, 307, 452, 1867, 281, 300, 51508], "temperature": 0.0, "avg_logprob": -0.051096558570861816, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0021142978221178055}, {"id": 264, "seek": 167908, "start": 1702.84, "end": 1707.72, "text": " okay this is a beautiful article in quanta so this is the cosmic background radiation", "tokens": [51552, 1392, 341, 307, 257, 2238, 7222, 294, 4426, 64, 370, 341, 307, 264, 27614, 3678, 12420, 51796], "temperature": 0.0, "avg_logprob": -0.051096558570861816, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0021142978221178055}, {"id": 265, "seek": 170772, "start": 1708.68, "end": 1716.2, "text": " no absolutely extraordinary thing from around about 2003 where we see the early what the universe", "tokens": [50412, 572, 3122, 10581, 551, 490, 926, 466, 16416, 689, 321, 536, 264, 2440, 437, 264, 6445, 50788], "temperature": 0.0, "avg_logprob": -0.11408135860781127, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.001476743957027793}, {"id": 266, "seek": 170772, "start": 1716.2, "end": 1720.6000000000001, "text": " looked at early on and we do this by basically going around the world and looking out into space", "tokens": [50788, 2956, 412, 2440, 322, 293, 321, 360, 341, 538, 1936, 516, 926, 264, 1002, 293, 1237, 484, 666, 1901, 51008], "temperature": 0.0, "avg_logprob": -0.11408135860781127, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.001476743957027793}, {"id": 267, "seek": 170772, "start": 1720.6000000000001, "end": 1723.08, "text": " and so it's an archetypal example of an image on a sphere", "tokens": [51008, 293, 370, 309, 311, 364, 41852, 31862, 1365, 295, 364, 3256, 322, 257, 16687, 51132], "temperature": 0.0, "avg_logprob": -0.11408135860781127, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.001476743957027793}, {"id": 268, "seek": 170772, "start": 1727.8, "end": 1734.3600000000001, "text": " so sam has a question um we'll just i just want to admire this picture for 10 more seconds", "tokens": [51368, 370, 3247, 575, 257, 1168, 1105, 321, 603, 445, 741, 445, 528, 281, 21951, 341, 3036, 337, 1266, 544, 3949, 51696], "temperature": 0.0, "avg_logprob": -0.11408135860781127, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.001476743957027793}, {"id": 269, "seek": 173436, "start": 1734.4399999999998, "end": 1742.4399999999998, "text": " so i'm told that you can see the fluctuations of quantum field theory in this picture so", "tokens": [50368, 370, 741, 478, 1907, 300, 291, 393, 536, 264, 45276, 295, 13018, 2519, 5261, 294, 341, 3036, 370, 50768], "temperature": 0.0, "avg_logprob": -0.089732028470181, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.0024305132683366537}, {"id": 270, "seek": 173436, "start": 1742.4399999999998, "end": 1746.76, "text": " this is a very early universe so it's when the universe was very small and you expect", "tokens": [50768, 341, 307, 257, 588, 2440, 6445, 370, 309, 311, 562, 264, 6445, 390, 588, 1359, 293, 291, 2066, 50984], "temperature": 0.0, "avg_logprob": -0.089732028470181, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.0024305132683366537}, {"id": 271, "seek": 173436, "start": 1748.1999999999998, "end": 1752.52, "text": " the behavior to be given by the laws of the very small and i'm told that you can see", "tokens": [51056, 264, 5223, 281, 312, 2212, 538, 264, 6064, 295, 264, 588, 1359, 293, 741, 478, 1907, 300, 291, 393, 536, 51272], "temperature": 0.0, "avg_logprob": -0.089732028470181, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.0024305132683366537}, {"id": 272, "seek": 173436, "start": 1753.08, "end": 1758.52, "text": " evidence of quantum field theory in this picture that totally blows my mind okay so sam's question", "tokens": [51300, 4467, 295, 13018, 2519, 5261, 294, 341, 3036, 300, 3879, 18458, 452, 1575, 1392, 370, 3247, 311, 1168, 51572], "temperature": 0.0, "avg_logprob": -0.089732028470181, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.0024305132683366537}, {"id": 273, "seek": 173436, "start": 1758.52, "end": 1763.1599999999999, "text": " for discrete finitely generated gamma could support near the identity be regarded as having", "tokens": [51572, 337, 27706, 962, 1959, 10833, 15546, 727, 1406, 2651, 264, 6575, 312, 26047, 382, 1419, 51804], "temperature": 0.0, "avg_logprob": -0.089732028470181, "compression_ratio": 1.9230769230769231, "no_speech_prob": 0.0024305132683366537}, {"id": 274, "seek": 176316, "start": 1763.16, "end": 1768.68, "text": " sort some sort of choice of generator that's a really good question i was thinking about", "tokens": [50364, 1333, 512, 1333, 295, 3922, 295, 19265, 300, 311, 257, 534, 665, 1168, 741, 390, 1953, 466, 50640], "temperature": 0.0, "avg_logprob": -0.09669483667132499, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0008288136450573802}, {"id": 275, "seek": 176316, "start": 1768.68, "end": 1776.1200000000001, "text": " what the support nearly up near the identity kind of means so in the example of the cnn", "tokens": [50640, 437, 264, 1406, 6217, 493, 2651, 264, 6575, 733, 295, 1355, 370, 294, 264, 1365, 295, 264, 269, 26384, 51012], "temperature": 0.0, "avg_logprob": -0.09669483667132499, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0008288136450573802}, {"id": 276, "seek": 176316, "start": 1777.24, "end": 1785.48, "text": " we have this discrete group which has no no really convincing metric on it but it is embedded in", "tokens": [51068, 321, 362, 341, 27706, 1594, 597, 575, 572, 572, 534, 24823, 20678, 322, 309, 457, 309, 307, 16741, 294, 51480], "temperature": 0.0, "avg_logprob": -0.09669483667132499, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0008288136450573802}, {"id": 277, "seek": 176316, "start": 1786.0400000000002, "end": 1791.8000000000002, "text": " s1 times s1 that does have a good metric on it and so for groups that come with some embedding", "tokens": [51508, 262, 16, 1413, 262, 16, 300, 775, 362, 257, 665, 20678, 322, 309, 293, 370, 337, 3935, 300, 808, 365, 512, 12240, 3584, 51796], "temperature": 0.0, "avg_logprob": -0.09669483667132499, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0008288136450573802}, {"id": 278, "seek": 179316, "start": 1793.16, "end": 1797.16, "text": " we can put a metric on them but i also think that that's a good suggestion if you have a", "tokens": [50364, 321, 393, 829, 257, 20678, 322, 552, 457, 741, 611, 519, 300, 300, 311, 257, 665, 16541, 498, 291, 362, 257, 50564], "temperature": 0.0, "avg_logprob": -0.13959078061378608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.001672971062362194}, {"id": 279, "seek": 179316, "start": 1798.52, "end": 1802.44, "text": " some kind of uh what's what's that distance you're talking about it's um like kind of", "tokens": [50632, 512, 733, 295, 2232, 437, 311, 437, 311, 300, 4560, 291, 434, 1417, 466, 309, 311, 1105, 411, 733, 295, 50828], "temperature": 0.0, "avg_logprob": -0.13959078061378608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.001672971062362194}, {"id": 280, "seek": 179316, "start": 1802.44, "end": 1807.5600000000002, "text": " distance in the kaillie graph might also be a a decent measure of locality i also want to try", "tokens": [50828, 4560, 294, 264, 6799, 373, 414, 4295, 1062, 611, 312, 257, 257, 8681, 3481, 295, 1628, 1860, 741, 611, 528, 281, 853, 51084], "temperature": 0.0, "avg_logprob": -0.13959078061378608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.001672971062362194}, {"id": 281, "seek": 179316, "start": 1807.5600000000002, "end": 1814.28, "text": " to explain in a second that for a non-Abelian group locality is less important so so the building", "tokens": [51084, 281, 2903, 294, 257, 1150, 300, 337, 257, 2107, 12, 32, 5390, 952, 1594, 1628, 1860, 307, 1570, 1021, 370, 370, 264, 2390, 51420], "temperature": 0.0, "avg_logprob": -0.13959078061378608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.001672971062362194}, {"id": 282, "seek": 179316, "start": 1814.28, "end": 1820.52, "text": " blocks so what are the homogeneous spaces for so three so this is in a league group so i can", "tokens": [51420, 8474, 370, 437, 366, 264, 42632, 7673, 337, 370, 1045, 370, 341, 307, 294, 257, 14957, 1594, 370, 741, 393, 51732], "temperature": 0.0, "avg_logprob": -0.13959078061378608, "compression_ratio": 1.751908396946565, "no_speech_prob": 0.001672971062362194}, {"id": 283, "seek": 182052, "start": 1820.52, "end": 1825.16, "text": " ask what are the dimensions of the subgroups of s03 so there's a whole lot of interesting finite", "tokens": [50364, 1029, 437, 366, 264, 12819, 295, 264, 1422, 17377, 82, 295, 262, 11592, 370, 456, 311, 257, 1379, 688, 295, 1880, 19362, 50596], "temperature": 0.0, "avg_logprob": -0.06292753523968636, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.004606697242707014}, {"id": 284, "seek": 182052, "start": 1825.16, "end": 1830.04, "text": " subgroups of s03 for example the symmetries of the icosahedron form a very interesting", "tokens": [50596, 1422, 17377, 82, 295, 262, 11592, 337, 1365, 264, 14232, 302, 2244, 295, 264, 4376, 329, 545, 292, 2044, 1254, 257, 588, 1880, 50840], "temperature": 0.0, "avg_logprob": -0.06292753523968636, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.004606697242707014}, {"id": 285, "seek": 182052, "start": 1830.68, "end": 1840.92, "text": " subgroup of s03 and um and then you have the two sphere and rp2 and then you have a point", "tokens": [50872, 1422, 17377, 295, 262, 11592, 293, 1105, 293, 550, 291, 362, 264, 732, 16687, 293, 367, 79, 17, 293, 550, 291, 362, 257, 935, 51384], "temperature": 0.0, "avg_logprob": -0.06292753523968636, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.004606697242707014}, {"id": 286, "seek": 182052, "start": 1840.92, "end": 1846.76, "text": " and that's it okay so our building blocks are rather restricted which is interesting", "tokens": [51384, 293, 300, 311, 309, 1392, 370, 527, 2390, 8474, 366, 2831, 20608, 597, 307, 1880, 51676], "temperature": 0.0, "avg_logprob": -0.06292753523968636, "compression_ratio": 1.8358974358974358, "no_speech_prob": 0.004606697242707014}, {"id": 287, "seek": 184676, "start": 1846.76, "end": 1853.96, "text": " and also i would say that we if you're employing some kind of practicality in building your model", "tokens": [50364, 293, 611, 741, 576, 584, 300, 321, 498, 291, 434, 3188, 278, 512, 733, 295, 8496, 507, 294, 2390, 428, 2316, 50724], "temperature": 0.0, "avg_logprob": -0.1130328695458102, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.002114321803674102}, {"id": 288, "seek": 184676, "start": 1854.52, "end": 1860.2, "text": " you don't want complicated things like s03 modified subgroup so", "tokens": [50752, 291, 500, 380, 528, 6179, 721, 411, 262, 11592, 15873, 1422, 17377, 370, 51036], "temperature": 0.0, "avg_logprob": -0.1130328695458102, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.002114321803674102}, {"id": 289, "seek": 184676, "start": 1862.2, "end": 1867.72, "text": " and rp2 and s2 are very very similar you know one is just a two-fold cover of the other", "tokens": [51136, 293, 367, 79, 17, 293, 262, 17, 366, 588, 588, 2531, 291, 458, 472, 307, 445, 257, 732, 12, 18353, 2060, 295, 264, 661, 51412], "temperature": 0.0, "avg_logprob": -0.1130328695458102, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.002114321803674102}, {"id": 290, "seek": 184676, "start": 1868.92, "end": 1874.52, "text": " and so i would advocate building a building a network which just involves functions on", "tokens": [51472, 293, 370, 741, 576, 14608, 2390, 257, 2390, 257, 3209, 597, 445, 11626, 6828, 322, 51752], "temperature": 0.0, "avg_logprob": -0.1130328695458102, "compression_ratio": 1.6310679611650485, "no_speech_prob": 0.002114321803674102}, {"id": 291, "seek": 187452, "start": 1874.6, "end": 1881.16, "text": " spheres and functions on a point so this is the proposal for a blueprint for learning on the sphere", "tokens": [50368, 41225, 293, 6828, 322, 257, 935, 370, 341, 307, 264, 11494, 337, 257, 35868, 337, 2539, 322, 264, 16687, 50696], "temperature": 0.0, "avg_logprob": -0.06177132084684552, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.007337833754718304}, {"id": 292, "seek": 187452, "start": 1881.16, "end": 1885.72, "text": " and also i am aware that it's very difficult for a computer to understand a function on s2", "tokens": [50696, 293, 611, 741, 669, 3650, 300, 309, 311, 588, 2252, 337, 257, 3820, 281, 1223, 257, 2445, 322, 262, 17, 50924], "temperature": 0.0, "avg_logprob": -0.06177132084684552, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.007337833754718304}, {"id": 293, "seek": 187452, "start": 1885.72, "end": 1890.6, "text": " okay but this is meant to be some kind of blueprint that you then try to interpret", "tokens": [50924, 1392, 457, 341, 307, 4140, 281, 312, 512, 733, 295, 35868, 300, 291, 550, 853, 281, 7302, 51168], "temperature": 0.0, "avg_logprob": -0.06177132084684552, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.007337833754718304}, {"id": 294, "seek": 187452, "start": 1891.8799999999999, "end": 1896.12, "text": " and sometimes you know to have the idea of what you're doing very clearly in your head is very", "tokens": [51232, 293, 2171, 291, 458, 281, 362, 264, 1558, 295, 437, 291, 434, 884, 588, 4448, 294, 428, 1378, 307, 588, 51444], "temperature": 0.0, "avg_logprob": -0.06177132084684552, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.007337833754718304}, {"id": 295, "seek": 187452, "start": 1896.12, "end": 1901.56, "text": " useful when you come to implementing something here we have h equals h prime exactly so i'll", "tokens": [51444, 4420, 562, 291, 808, 281, 18114, 746, 510, 321, 362, 276, 6915, 276, 5835, 2293, 370, 741, 603, 51716], "temperature": 0.0, "avg_logprob": -0.06177132084684552, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.007337833754718304}, {"id": 296, "seek": 190156, "start": 1901.56, "end": 1907.8, "text": " go through the double coset formula in two examples now so the point the the problem here", "tokens": [50364, 352, 807, 264, 3834, 3792, 302, 8513, 294, 732, 5110, 586, 370, 264, 935, 264, 264, 1154, 510, 50676], "temperature": 0.0, "avg_logprob": -0.11779317962989379, "compression_ratio": 1.9421052631578948, "no_speech_prob": 0.003121364861726761}, {"id": 297, "seek": 190156, "start": 1907.8, "end": 1913.6399999999999, "text": " which geyog is pointing out so geyog was asking which which subgroup are we using the double", "tokens": [50676, 597, 1519, 88, 664, 307, 12166, 484, 370, 1519, 88, 664, 390, 3365, 597, 597, 1422, 17377, 366, 321, 1228, 264, 3834, 50968], "temperature": 0.0, "avg_logprob": -0.11779317962989379, "compression_ratio": 1.9421052631578948, "no_speech_prob": 0.003121364861726761}, {"id": 298, "seek": 190156, "start": 1913.6399999999999, "end": 1917.96, "text": " coset formula in here i just want to first say why we're using the double coset formula", "tokens": [50968, 3792, 302, 8513, 294, 510, 741, 445, 528, 281, 700, 584, 983, 321, 434, 1228, 264, 3834, 3792, 302, 8513, 51184], "temperature": 0.0, "avg_logprob": -0.11779317962989379, "compression_ratio": 1.9421052631578948, "no_speech_prob": 0.003121364861726761}, {"id": 299, "seek": 190156, "start": 1917.96, "end": 1924.6, "text": " we want to know what are these maps what are our possible so three every variant convolutions here", "tokens": [51184, 321, 528, 281, 458, 437, 366, 613, 11317, 437, 366, 527, 1944, 370, 1045, 633, 17501, 3754, 15892, 510, 51516], "temperature": 0.0, "avg_logprob": -0.11779317962989379, "compression_ratio": 1.9421052631578948, "no_speech_prob": 0.003121364861726761}, {"id": 300, "seek": 192460, "start": 1924.6, "end": 1931.9599999999998, "text": " so what are the so three every rank convolutions so this is the double coset formula again this is", "tokens": [50364, 370, 437, 366, 264, 370, 1045, 633, 6181, 3754, 15892, 370, 341, 307, 264, 3834, 3792, 302, 8513, 797, 341, 307, 50732], "temperature": 0.0, "avg_logprob": -0.13625109685610418, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0025501614436507225}, {"id": 301, "seek": 192460, "start": 1931.9599999999998, "end": 1941.9599999999998, "text": " our friend okay i'm just specializing the double coset formula for so three so that made the formula", "tokens": [50732, 527, 1277, 1392, 741, 478, 445, 2121, 3319, 264, 3834, 3792, 302, 8513, 337, 370, 1045, 370, 300, 1027, 264, 8513, 51232], "temperature": 0.0, "avg_logprob": -0.13625109685610418, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0025501614436507225}, {"id": 302, "seek": 192460, "start": 1941.9599999999998, "end": 1946.4399999999998, "text": " much less easy to read so i'll delete it again okay so what does this say let's first do a silly", "tokens": [51232, 709, 1570, 1858, 281, 1401, 370, 741, 603, 12097, 309, 797, 1392, 370, 437, 775, 341, 584, 718, 311, 700, 360, 257, 11774, 51456], "temperature": 0.0, "avg_logprob": -0.13625109685610418, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0025501614436507225}, {"id": 303, "seek": 194644, "start": 1946.44, "end": 1955.24, "text": " example what are the homomorphisms from s03 mod s1 i.e s2 to s03 mod s03 namely a point", "tokens": [50364, 1365, 437, 366, 264, 3655, 32702, 13539, 490, 262, 11592, 1072, 262, 16, 741, 13, 68, 262, 17, 281, 262, 11592, 1072, 262, 11592, 20926, 257, 935, 50804], "temperature": 0.0, "avg_logprob": -0.09353429620916193, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.030647531151771545}, {"id": 304, "seek": 194644, "start": 1956.1200000000001, "end": 1960.8400000000001, "text": " so i said as an exercise in very great generality the only such function is given essentially by", "tokens": [50848, 370, 741, 848, 382, 364, 5380, 294, 588, 869, 1337, 1860, 264, 787, 1270, 2445, 307, 2212, 4476, 538, 51084], "temperature": 0.0, "avg_logprob": -0.09353429620916193, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.030647531151771545}, {"id": 305, "seek": 194644, "start": 1960.8400000000001, "end": 1965.16, "text": " integrating over your space up to a scalar but let's see it pop out of the double coset formula", "tokens": [51084, 26889, 670, 428, 1901, 493, 281, 257, 39684, 457, 718, 311, 536, 309, 1665, 484, 295, 264, 3834, 3792, 302, 8513, 51300], "temperature": 0.0, "avg_logprob": -0.09353429620916193, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.030647531151771545}, {"id": 306, "seek": 194644, "start": 1965.16, "end": 1971.16, "text": " so s03 mod s03 is a point yeah so we've got functions on a point what's more interesting", "tokens": [51300, 370, 262, 11592, 1072, 262, 11592, 307, 257, 935, 1338, 370, 321, 600, 658, 6828, 322, 257, 935, 437, 311, 544, 1880, 51600], "temperature": 0.0, "avg_logprob": -0.09353429620916193, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.030647531151771545}, {"id": 307, "seek": 197116, "start": 1971.16, "end": 1977.0, "text": " so this is the kind of silly example what's more interesting is what are the rest of the layers", "tokens": [50364, 370, 341, 307, 264, 733, 295, 11774, 1365, 437, 311, 544, 1880, 307, 437, 366, 264, 1472, 295, 264, 7914, 50656], "temperature": 0.0, "avg_logprob": -0.08598442600197988, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.005461231339722872}, {"id": 308, "seek": 197116, "start": 1977.72, "end": 1982.0400000000002, "text": " you know so just to emphasize here this is telling us that even though this is an enormous vector", "tokens": [50692, 291, 458, 370, 445, 281, 16078, 510, 341, 307, 3585, 505, 300, 754, 1673, 341, 307, 364, 11322, 8062, 50908], "temperature": 0.0, "avg_logprob": -0.08598442600197988, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.005461231339722872}, {"id": 309, "seek": 197116, "start": 1982.0400000000002, "end": 1990.68, "text": " space with this only one scalar possibility of of maps here so this belongs to r this belongs to r", "tokens": [50908, 1901, 365, 341, 787, 472, 39684, 7959, 295, 295, 11317, 510, 370, 341, 12953, 281, 367, 341, 12953, 281, 367, 51340], "temperature": 0.0, "avg_logprob": -0.08598442600197988, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.005461231339722872}, {"id": 310, "seek": 197116, "start": 1991.4, "end": 1992.2, "text": " this belongs to r", "tokens": [51376, 341, 12953, 281, 367, 51416], "temperature": 0.0, "avg_logprob": -0.08598442600197988, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.005461231339722872}, {"id": 311, "seek": 199220, "start": 1993.16, "end": 2003.48, "text": " so now ignore the integral bit at the moment just look at this so what are the s03", "tokens": [50412, 370, 586, 11200, 264, 11573, 857, 412, 264, 1623, 445, 574, 412, 341, 370, 437, 366, 264, 262, 11592, 50928], "temperature": 0.0, "avg_logprob": -0.13415742501979921, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.0010480574565008283}, {"id": 312, "seek": 199220, "start": 2003.48, "end": 2010.04, "text": " equivariant homomorphisms from functions on s2 to functions on s2 they're functions on by our double", "tokens": [50928, 1267, 592, 3504, 394, 3655, 32702, 13539, 490, 6828, 322, 262, 17, 281, 6828, 322, 262, 17, 436, 434, 6828, 322, 538, 527, 3834, 51256], "temperature": 0.0, "avg_logprob": -0.13415742501979921, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.0010480574565008283}, {"id": 313, "seek": 199220, "start": 2010.04, "end": 2018.52, "text": " coset formula s1 mod s03 mod s1 so that's the same thing as s s1 mod s2 so i take s2 and i", "tokens": [51256, 3792, 302, 8513, 262, 16, 1072, 262, 11592, 1072, 262, 16, 370, 300, 311, 264, 912, 551, 382, 262, 262, 16, 1072, 262, 17, 370, 741, 747, 262, 17, 293, 741, 51680], "temperature": 0.0, "avg_logprob": -0.13415742501979921, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.0010480574565008283}, {"id": 314, "seek": 201852, "start": 2018.68, "end": 2025.16, "text": " have s1 rotating it around and then i quotient that out and our representatives for that quotient", "tokens": [50372, 362, 262, 16, 19627, 309, 926, 293, 550, 741, 9641, 1196, 300, 484, 293, 527, 18628, 337, 300, 9641, 1196, 50696], "temperature": 0.0, "avg_logprob": -0.08719140039363378, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.00031495976145379245}, {"id": 315, "seek": 201852, "start": 2025.16, "end": 2031.72, "text": " space are just an interval stretching from one pole to the other so functions on that interval", "tokens": [50696, 1901, 366, 445, 364, 15035, 19632, 490, 472, 13208, 281, 264, 661, 370, 6828, 322, 300, 15035, 51024], "temperature": 0.0, "avg_logprob": -0.08719140039363378, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.00031495976145379245}, {"id": 316, "seek": 201852, "start": 2033.56, "end": 2040.76, "text": " so what the hell are these intertwiners so i should say that such an element inside here", "tokens": [51116, 370, 437, 264, 4921, 366, 613, 44400, 259, 433, 370, 741, 820, 584, 300, 1270, 364, 4478, 1854, 510, 51476], "temperature": 0.0, "avg_logprob": -0.08719140039363378, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.00031495976145379245}, {"id": 317, "seek": 201852, "start": 2041.4, "end": 2042.6, "text": " is called an intertwiner", "tokens": [51508, 307, 1219, 364, 44400, 4564, 51568], "temperature": 0.0, "avg_logprob": -0.08719140039363378, "compression_ratio": 1.7191011235955056, "no_speech_prob": 0.00031495976145379245}, {"id": 318, "seek": 204260, "start": 2042.9199999999998, "end": 2053.88, "text": " so intertwiner is synonymous with s03 equivariant linear map", "tokens": [50380, 370, 44400, 4564, 307, 5451, 18092, 365, 262, 11592, 1267, 592, 3504, 394, 8213, 4471, 50928], "temperature": 0.0, "avg_logprob": -0.11145120196872288, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.0004304292378947139}, {"id": 319, "seek": 204260, "start": 2056.36, "end": 2062.6, "text": " so i can look at one way to understand these things is to try to look at delta function so a delta", "tokens": [51052, 370, 741, 393, 574, 412, 472, 636, 281, 1223, 613, 721, 307, 281, 853, 281, 574, 412, 8289, 2445, 370, 257, 8289, 51364], "temperature": 0.0, "avg_logprob": -0.11145120196872288, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.0004304292378947139}, {"id": 320, "seek": 204260, "start": 2062.6, "end": 2070.2, "text": " function at the identity at this base point is just the identity a delta function at this end", "tokens": [51364, 2445, 412, 264, 6575, 412, 341, 3096, 935, 307, 445, 264, 6575, 257, 8289, 2445, 412, 341, 917, 51744], "temperature": 0.0, "avg_logprob": -0.11145120196872288, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.0004304292378947139}, {"id": 321, "seek": 207020, "start": 2070.2, "end": 2078.04, "text": " is the antipode but what the hell is going on in the middle you know you're you're seeking a", "tokens": [50364, 307, 264, 2511, 647, 1429, 457, 437, 264, 4921, 307, 516, 322, 294, 264, 2808, 291, 458, 291, 434, 291, 434, 11670, 257, 50756], "temperature": 0.0, "avg_logprob": -0.0767262036150152, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.0006767638842575252}, {"id": 322, "seek": 207020, "start": 2078.04, "end": 2084.7599999999998, "text": " continuous family of operators which interpolates between the identity and the antipode okay looks", "tokens": [50756, 10957, 1605, 295, 19077, 597, 44902, 1024, 1296, 264, 6575, 293, 264, 2511, 647, 1429, 1392, 1542, 51092], "temperature": 0.0, "avg_logprob": -0.0767262036150152, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.0006767638842575252}, {"id": 323, "seek": 207020, "start": 2084.7599999999998, "end": 2090.52, "text": " like a tough ask but there's a really beautiful thing you can do which is you consider the", "tokens": [51092, 411, 257, 4930, 1029, 457, 456, 311, 257, 534, 2238, 551, 291, 393, 360, 597, 307, 291, 1949, 264, 51380], "temperature": 0.0, "avg_logprob": -0.0767262036150152, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.0006767638842575252}, {"id": 324, "seek": 207020, "start": 2090.52, "end": 2097.3999999999996, "text": " following operator on function so i have a function on the on the two sphere so this is in", "tokens": [51380, 3480, 12973, 322, 2445, 370, 741, 362, 257, 2445, 322, 264, 322, 264, 732, 16687, 370, 341, 307, 294, 51724], "temperature": 0.0, "avg_logprob": -0.0767262036150152, "compression_ratio": 1.759433962264151, "no_speech_prob": 0.0006767638842575252}, {"id": 325, "seek": 209740, "start": 2098.36, "end": 2108.04, "text": " on s2 and now and i have a gamma and i can consider a new function which at a point x is", "tokens": [50412, 322, 262, 17, 293, 586, 293, 741, 362, 257, 15546, 293, 741, 393, 1949, 257, 777, 2445, 597, 412, 257, 935, 2031, 307, 50896], "temperature": 0.0, "avg_logprob": -0.0869827555186713, "compression_ratio": 1.607361963190184, "no_speech_prob": 0.0015973524423316121}, {"id": 326, "seek": 209740, "start": 2108.04, "end": 2114.28, "text": " given by the integral around a loop of my original function at distance gamma", "tokens": [50896, 2212, 538, 264, 11573, 926, 257, 6367, 295, 452, 3380, 2445, 412, 4560, 15546, 51208], "temperature": 0.0, "avg_logprob": -0.0869827555186713, "compression_ratio": 1.607361963190184, "no_speech_prob": 0.0015973524423316121}, {"id": 327, "seek": 209740, "start": 2120.2000000000003, "end": 2126.28, "text": " from my point there's a way of producing a new function so i've told you how to take a function", "tokens": [51504, 490, 452, 935, 456, 311, 257, 636, 295, 10501, 257, 777, 2445, 370, 741, 600, 1907, 291, 577, 281, 747, 257, 2445, 51808], "temperature": 0.0, "avg_logprob": -0.0869827555186713, "compression_ratio": 1.607361963190184, "no_speech_prob": 0.0015973524423316121}, {"id": 328, "seek": 212628, "start": 2126.6000000000004, "end": 2133.0800000000004, "text": " s2 get a new function on s2 and it's a beautiful thought exercise that this is invariant this", "tokens": [50380, 262, 17, 483, 257, 777, 2445, 322, 262, 17, 293, 309, 311, 257, 2238, 1194, 5380, 300, 341, 307, 33270, 394, 341, 50704], "temperature": 0.0, "avg_logprob": -0.09301700592041015, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.0032704416662454605}, {"id": 329, "seek": 212628, "start": 2133.0800000000004, "end": 2138.92, "text": " this is equivariant okay so if i move my function and then do this operation that's the same thing", "tokens": [50704, 341, 307, 1267, 592, 3504, 394, 1392, 370, 498, 741, 1286, 452, 2445, 293, 550, 360, 341, 6916, 300, 311, 264, 912, 551, 50996], "temperature": 0.0, "avg_logprob": -0.09301700592041015, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.0032704416662454605}, {"id": 330, "seek": 212628, "start": 2138.92, "end": 2143.48, "text": " is doing this operation and then moving my function okay so these are these", "tokens": [50996, 307, 884, 341, 6916, 293, 550, 2684, 452, 2445, 1392, 370, 613, 366, 613, 51224], "temperature": 0.0, "avg_logprob": -0.09301700592041015, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.0032704416662454605}, {"id": 331, "seek": 212628, "start": 2145.96, "end": 2147.4, "text": " intertwiners so", "tokens": [51348, 44400, 259, 433, 370, 51420], "temperature": 0.0, "avg_logprob": -0.09301700592041015, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.0032704416662454605}, {"id": 332, "seek": 212628, "start": 2150.0400000000004, "end": 2155.8, "text": " so that's the answer for what all these maps are and of course like that's still a infinite", "tokens": [51552, 370, 300, 311, 264, 1867, 337, 437, 439, 613, 11317, 366, 293, 295, 1164, 411, 300, 311, 920, 257, 13785, 51840], "temperature": 0.0, "avg_logprob": -0.09301700592041015, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.0032704416662454605}, {"id": 333, "seek": 215580, "start": 2155.8, "end": 2165.0800000000004, "text": " dimensional vector space but compared to you know functions on like if you're just thinking", "tokens": [50364, 18795, 8062, 1901, 457, 5347, 281, 291, 458, 6828, 322, 411, 498, 291, 434, 445, 1953, 50828], "temperature": 0.0, "avg_logprob": -0.10687259391502098, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.0009541194885969162}, {"id": 334, "seek": 215580, "start": 2165.0800000000004, "end": 2170.84, "text": " about linear maps here that's something like functions on s2 times s2 so it's like a four", "tokens": [50828, 466, 8213, 11317, 510, 300, 311, 746, 411, 6828, 322, 262, 17, 1413, 262, 17, 370, 309, 311, 411, 257, 1451, 51116], "temperature": 0.0, "avg_logprob": -0.10687259391502098, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.0009541194885969162}, {"id": 335, "seek": 215580, "start": 2170.84, "end": 2177.8, "text": " dimensional roughly speaking and it's kind of remarkable that just in employing this", "tokens": [51116, 18795, 9810, 4124, 293, 309, 311, 733, 295, 12802, 300, 445, 294, 3188, 278, 341, 51464], "temperature": 0.0, "avg_logprob": -0.10687259391502098, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.0009541194885969162}, {"id": 336, "seek": 215580, "start": 2177.8, "end": 2183.7200000000003, "text": " equivariance massively cuts down the number of parameters and of course you can make this whole", "tokens": [51464, 1267, 592, 3504, 719, 29379, 9992, 760, 264, 1230, 295, 9834, 293, 295, 1164, 291, 393, 652, 341, 1379, 51760], "temperature": 0.0, "avg_logprob": -0.10687259391502098, "compression_ratio": 1.6759259259259258, "no_speech_prob": 0.0009541194885969162}, {"id": 337, "seek": 218372, "start": 2183.72, "end": 2189.56, "text": " picture even richer using spherical harmonics and there's like incredibly nice functions to put in", "tokens": [50364, 3036, 754, 29021, 1228, 37300, 14750, 1167, 293, 456, 311, 411, 6252, 1481, 6828, 281, 829, 294, 50656], "temperature": 0.0, "avg_logprob": -0.05596520282604076, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.001283517456613481}, {"id": 338, "seek": 218372, "start": 2189.56, "end": 2197.56, "text": " here projecting to the irreducible representations inside functions on s2 etc i should have said", "tokens": [50656, 510, 43001, 281, 264, 16014, 769, 32128, 33358, 1854, 6828, 322, 262, 17, 5183, 741, 820, 362, 848, 51056], "temperature": 0.0, "avg_logprob": -0.05596520282604076, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.001283517456613481}, {"id": 339, "seek": 218372, "start": 2197.56, "end": 2202.7599999999998, "text": " very very much earlier like for me fun is just firstly it's to remind us that we're having fun", "tokens": [51056, 588, 588, 709, 3071, 411, 337, 385, 1019, 307, 445, 27376, 309, 311, 281, 4160, 505, 300, 321, 434, 1419, 1019, 51316], "temperature": 0.0, "avg_logprob": -0.05596520282604076, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.001283517456613481}, {"id": 340, "seek": 218372, "start": 2203.7999999999997, "end": 2208.9199999999996, "text": " secondly um it's just some kind of class of function so when i'm talking about the sphere", "tokens": [51368, 26246, 1105, 309, 311, 445, 512, 733, 295, 1508, 295, 2445, 370, 562, 741, 478, 1417, 466, 264, 16687, 51624], "temperature": 0.0, "avg_logprob": -0.05596520282604076, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.001283517456613481}, {"id": 341, "seek": 218372, "start": 2208.9199999999996, "end": 2212.9199999999996, "text": " i'm probably talking about l2 functions when i'm talking about a um discrete set i'm just talking", "tokens": [51624, 741, 478, 1391, 1417, 466, 287, 17, 6828, 562, 741, 478, 1417, 466, 257, 1105, 27706, 992, 741, 478, 445, 1417, 51824], "temperature": 0.0, "avg_logprob": -0.05596520282604076, "compression_ratio": 1.8384615384615384, "no_speech_prob": 0.001283517456613481}, {"id": 342, "seek": 221292, "start": 2212.92, "end": 2220.04, "text": " about any function etc so yeah the point is that when we have a non-Abelian gamma there's a big", "tokens": [50364, 466, 604, 2445, 5183, 370, 1338, 264, 935, 307, 300, 562, 321, 362, 257, 2107, 12, 32, 5390, 952, 15546, 456, 311, 257, 955, 50720], "temperature": 0.0, "avg_logprob": -0.1075509322317023, "compression_ratio": 1.8349056603773586, "no_speech_prob": 0.00020010776643175632}, {"id": 343, "seek": 221292, "start": 2220.04, "end": 2228.84, "text": " reduction in parameters so um in the cnn slide there was this equivariance plus locality drastic", "tokens": [50720, 11004, 294, 9834, 370, 1105, 294, 264, 269, 26384, 4137, 456, 390, 341, 1267, 592, 3504, 719, 1804, 1628, 1860, 36821, 51160], "temperature": 0.0, "avg_logprob": -0.1075509322317023, "compression_ratio": 1.8349056603773586, "no_speech_prob": 0.00020010776643175632}, {"id": 344, "seek": 221292, "start": 2228.84, "end": 2234.12, "text": " reduces the number of parameters and here i'm kind of saying that equivariance plus non-Abelian", "tokens": [51160, 18081, 264, 1230, 295, 9834, 293, 510, 741, 478, 733, 295, 1566, 300, 1267, 592, 3504, 719, 1804, 2107, 12, 32, 5390, 952, 51424], "temperature": 0.0, "avg_logprob": -0.1075509322317023, "compression_ratio": 1.8349056603773586, "no_speech_prob": 0.00020010776643175632}, {"id": 345, "seek": 221292, "start": 2234.12, "end": 2240.28, "text": " drastically reduces the number of parameters which i think is very interesting so my task for myself", "tokens": [51424, 29673, 18081, 264, 1230, 295, 9834, 597, 741, 519, 307, 588, 1880, 370, 452, 5633, 337, 2059, 51732], "temperature": 0.0, "avg_logprob": -0.1075509322317023, "compression_ratio": 1.8349056603773586, "no_speech_prob": 0.00020010776643175632}, {"id": 346, "seek": 224028, "start": 2240.28, "end": 2245.2400000000002, "text": " and if you have any ideas i'd love to hear it is find an interesting learning problem", "tokens": [50364, 293, 498, 291, 362, 604, 3487, 741, 1116, 959, 281, 1568, 309, 307, 915, 364, 1880, 2539, 1154, 50612], "temperature": 0.0, "avg_logprob": -0.07645992173088921, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.004392449744045734}, {"id": 347, "seek": 224028, "start": 2245.2400000000002, "end": 2251.1600000000003, "text": " where the symmetries are an interesting non-Abelian group what's a learning problem where the", "tokens": [50612, 689, 264, 14232, 302, 2244, 366, 364, 1880, 2107, 12, 32, 5390, 952, 1594, 437, 311, 257, 2539, 1154, 689, 264, 50908], "temperature": 0.0, "avg_logprob": -0.07645992173088921, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.004392449744045734}, {"id": 348, "seek": 224028, "start": 2251.1600000000003, "end": 2257.96, "text": " symmetries are naturally sl2 fq or something like that or you know some interesting groups so a lot", "tokens": [50908, 14232, 302, 2244, 366, 8195, 1061, 17, 283, 80, 420, 746, 411, 300, 420, 291, 458, 512, 1880, 3935, 370, 257, 688, 51248], "temperature": 0.0, "avg_logprob": -0.07645992173088921, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.004392449744045734}, {"id": 349, "seek": 224028, "start": 2257.96, "end": 2263.96, "text": " of the groups that show up in machine learning are very much related to um three-dimensional space", "tokens": [51248, 295, 264, 3935, 300, 855, 493, 294, 3479, 2539, 366, 588, 709, 4077, 281, 1105, 1045, 12, 18759, 1901, 51548], "temperature": 0.0, "avg_logprob": -0.07645992173088921, "compression_ratio": 1.8349514563106797, "no_speech_prob": 0.004392449744045734}, {"id": 350, "seek": 226396, "start": 2263.96, "end": 2272.12, "text": " or two-dimensional space or so like p4 which consists of um all translations and 90-degree", "tokens": [50364, 420, 732, 12, 18759, 1901, 420, 370, 411, 280, 19, 597, 14689, 295, 1105, 439, 37578, 293, 4289, 12, 34368, 50772], "temperature": 0.0, "avg_logprob": -0.11596870422363281, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005621474236249924}, {"id": 351, "seek": 226396, "start": 2272.12, "end": 2276.44, "text": " rotation shows up a lot and stuff like that but it would be lovely to inject some really", "tokens": [50772, 12447, 3110, 493, 257, 688, 293, 1507, 411, 300, 457, 309, 576, 312, 7496, 281, 10711, 512, 534, 50988], "temperature": 0.0, "avg_logprob": -0.11596870422363281, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005621474236249924}, {"id": 352, "seek": 226396, "start": 2276.44, "end": 2280.2, "text": " interesting groups into this oh gl2 yeah gl2 would be great", "tokens": [50988, 1880, 3935, 666, 341, 1954, 1563, 17, 1338, 1563, 17, 576, 312, 869, 51176], "temperature": 0.0, "avg_logprob": -0.11596870422363281, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005621474236249924}, {"id": 353, "seek": 226396, "start": 2285.32, "end": 2290.04, "text": " yeah that's it so stefan just suggested learning on hyperbolic space and that's a great", "tokens": [51432, 1338, 300, 311, 309, 370, 2126, 20361, 445, 10945, 2539, 322, 9848, 65, 7940, 1901, 293, 300, 311, 257, 869, 51668], "temperature": 0.0, "avg_logprob": -0.11596870422363281, "compression_ratio": 1.5721153846153846, "no_speech_prob": 0.005621474236249924}, {"id": 354, "seek": 229004, "start": 2290.12, "end": 2297.48, "text": " great suggestion yeah i don't know why i didn't think of that i had fl2 uh sorry yeah so learning on", "tokens": [50368, 869, 16541, 1338, 741, 500, 380, 458, 983, 741, 994, 380, 519, 295, 300, 741, 632, 932, 17, 2232, 2597, 1338, 370, 2539, 322, 50736], "temperature": 0.0, "avg_logprob": -0.1330251831939255, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0010738198179751635}, {"id": 355, "seek": 229004, "start": 2304.12, "end": 2308.36, "text": " okay there's enormous um possibilities here that i think are very interesting", "tokens": [51068, 1392, 456, 311, 11322, 1105, 12178, 510, 300, 741, 519, 366, 588, 1880, 51280], "temperature": 0.0, "avg_logprob": -0.1330251831939255, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0010738198179751635}, {"id": 356, "seek": 229004, "start": 2309.32, "end": 2314.36, "text": " so let let's just go back to cnn's so the the question is basically like what the hell's going on", "tokens": [51328, 370, 718, 718, 311, 445, 352, 646, 281, 269, 26384, 311, 370, 264, 264, 1168, 307, 1936, 411, 437, 264, 4921, 311, 516, 322, 51580], "temperature": 0.0, "avg_logprob": -0.1330251831939255, "compression_ratio": 1.5771428571428572, "no_speech_prob": 0.0010738198179751635}, {"id": 357, "seek": 231436, "start": 2314.92, "end": 2324.52, "text": " so let's imagine so i'll try to explain um what the hell's going on and then we can have a short break", "tokens": [50392, 370, 718, 311, 3811, 370, 741, 603, 853, 281, 2903, 1105, 437, 264, 4921, 311, 516, 322, 293, 550, 321, 393, 362, 257, 2099, 1821, 50872], "temperature": 0.0, "avg_logprob": -0.16098431178501674, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.0012635568855330348}, {"id": 358, "seek": 231436, "start": 2325.1600000000003, "end": 2333.48, "text": " so let's say that we're trying to do image processing so we're z mod hz squared we have functions on this", "tokens": [50904, 370, 718, 311, 584, 300, 321, 434, 1382, 281, 360, 3256, 9007, 370, 321, 434, 710, 1072, 276, 89, 8889, 321, 362, 6828, 322, 341, 51320], "temperature": 0.0, "avg_logprob": -0.16098431178501674, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.0012635568855330348}, {"id": 359, "seek": 233348, "start": 2333.64, "end": 2340.92, "text": " and then we we want to have a layer of our neural net", "tokens": [50372, 293, 550, 321, 321, 528, 281, 362, 257, 4583, 295, 527, 18161, 2533, 50736], "temperature": 0.0, "avg_logprob": -0.08826504723500397, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.020300764590501785}, {"id": 360, "seek": 233348, "start": 2342.92, "end": 2344.28, "text": " so typically", "tokens": [50836, 370, 5850, 50904], "temperature": 0.0, "avg_logprob": -0.08826504723500397, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.020300764590501785}, {"id": 361, "seek": 233348, "start": 2347.2400000000002, "end": 2352.2, "text": " one layer of our neural net might be like this you know one piece of one layer of our", "tokens": [51052, 472, 4583, 295, 527, 18161, 2533, 1062, 312, 411, 341, 291, 458, 472, 2522, 295, 472, 4583, 295, 527, 51300], "temperature": 0.0, "avg_logprob": -0.08826504723500397, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.020300764590501785}, {"id": 362, "seek": 233348, "start": 2354.84, "end": 2360.04, "text": " neural net might look like this so now what's the dimension of this space", "tokens": [51432, 18161, 2533, 1062, 574, 411, 341, 370, 586, 437, 311, 264, 10139, 295, 341, 1901, 51692], "temperature": 0.0, "avg_logprob": -0.08826504723500397, "compression_ratio": 1.8225806451612903, "no_speech_prob": 0.020300764590501785}, {"id": 363, "seek": 236004, "start": 2361.0, "end": 2366.84, "text": " the dimension is h squared namely the number of", "tokens": [50412, 264, 10139, 307, 276, 8889, 20926, 264, 1230, 295, 50704], "temperature": 0.0, "avg_logprob": -0.18013660430908204, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.0007092378218658268}, {"id": 364, "seek": 236004, "start": 2368.84, "end": 2376.2, "text": " points in the set and what's the dimension of this space the dimension whoops the dimension is h square", "tokens": [50804, 2793, 294, 264, 992, 293, 437, 311, 264, 10139, 295, 341, 1901, 264, 10139, 567, 3370, 264, 10139, 307, 276, 3732, 51172], "temperature": 0.0, "avg_logprob": -0.18013660430908204, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.0007092378218658268}, {"id": 365, "seek": 236004, "start": 2378.68, "end": 2383.8, "text": " okay so now if i were doing a fully connected neural net", "tokens": [51296, 1392, 370, 586, 498, 741, 645, 884, 257, 4498, 4582, 18161, 2533, 51552], "temperature": 0.0, "avg_logprob": -0.18013660430908204, "compression_ratio": 1.6507936507936507, "no_speech_prob": 0.0007092378218658268}, {"id": 366, "seek": 238380, "start": 2384.76, "end": 2392.76, "text": " i would have h squared basis vectors here h squared basis vectors here and then i would have an h squared times h squared matrix", "tokens": [50412, 741, 576, 362, 276, 8889, 5143, 18875, 510, 276, 8889, 5143, 18875, 510, 293, 550, 741, 576, 362, 364, 276, 8889, 1413, 276, 8889, 8141, 50812], "temperature": 0.0, "avg_logprob": -0.23546228910747327, "compression_ratio": 1.9171974522292994, "no_speech_prob": 0.005553386174142361}, {"id": 367, "seek": 238380, "start": 2393.8, "end": 2400.76, "text": " so i'd have an enormous matrix of size h to the four that's and each of those parameters i have to train", "tokens": [50864, 370, 741, 1116, 362, 364, 11322, 8141, 295, 2744, 276, 281, 264, 1451, 300, 311, 293, 1184, 295, 729, 9834, 741, 362, 281, 3847, 51212], "temperature": 0.0, "avg_logprob": -0.23546228910747327, "compression_ratio": 1.9171974522292994, "no_speech_prob": 0.005553386174142361}, {"id": 368, "seek": 238380, "start": 2401.96, "end": 2408.6000000000004, "text": " okay what do i do in cnn's i say i want this matrix to be invariant", "tokens": [51272, 1392, 437, 360, 741, 360, 294, 269, 26384, 311, 741, 584, 741, 528, 341, 8141, 281, 312, 33270, 394, 51604], "temperature": 0.0, "avg_logprob": -0.23546228910747327, "compression_ratio": 1.9171974522292994, "no_speech_prob": 0.005553386174142361}, {"id": 369, "seek": 240860, "start": 2408.7599999999998, "end": 2415.96, "text": " that already cuts down the number of parameters from h to the four back down to h squared", "tokens": [50372, 300, 1217, 9992, 760, 264, 1230, 295, 9834, 490, 276, 281, 264, 1451, 646, 760, 281, 276, 8889, 50732], "temperature": 0.0, "avg_logprob": -0.1310276644570487, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.0018382994458079338}, {"id": 370, "seek": 240860, "start": 2417.24, "end": 2421.0, "text": " and then i say i want this matrix to satisfy locality", "tokens": [50796, 293, 550, 741, 584, 741, 528, 341, 8141, 281, 19319, 1628, 1860, 50984], "temperature": 0.0, "avg_logprob": -0.1310276644570487, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.0018382994458079338}, {"id": 371, "seek": 240860, "start": 2422.12, "end": 2425.56, "text": " and that cuts down my number of parameters from h squared to nine", "tokens": [51040, 293, 300, 9992, 760, 452, 1230, 295, 9834, 490, 276, 8889, 281, 4949, 51212], "temperature": 0.0, "avg_logprob": -0.1310276644570487, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.0018382994458079338}, {"id": 372, "seek": 240860, "start": 2426.68, "end": 2432.52, "text": " so harini is asking uh what basically like why do you restrict the parameters to this extent", "tokens": [51268, 370, 2233, 3812, 307, 3365, 2232, 437, 1936, 411, 983, 360, 291, 7694, 264, 9834, 281, 341, 8396, 51560], "temperature": 0.0, "avg_logprob": -0.1310276644570487, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.0018382994458079338}, {"id": 373, "seek": 240860, "start": 2433.4, "end": 2435.48, "text": " so i would say that there's two reasons for this", "tokens": [51604, 370, 741, 576, 584, 300, 456, 311, 732, 4112, 337, 341, 51708], "temperature": 0.0, "avg_logprob": -0.1310276644570487, "compression_ratio": 1.7908163265306123, "no_speech_prob": 0.0018382994458079338}, {"id": 374, "seek": 243548, "start": 2436.44, "end": 2442.92, "text": " so these are inductive priors so they're not there's something that i believe is true about the solution", "tokens": [50412, 370, 613, 366, 31612, 488, 1790, 830, 370, 436, 434, 406, 456, 311, 746, 300, 741, 1697, 307, 2074, 466, 264, 3827, 50736], "temperature": 0.0, "avg_logprob": -0.10438874003651379, "compression_ratio": 1.9086538461538463, "no_speech_prob": 0.0018086471827700734}, {"id": 375, "seek": 243548, "start": 2443.48, "end": 2448.92, "text": " they're not something that like is definitely true about the solution there's some category of practicality", "tokens": [50764, 436, 434, 406, 746, 300, 411, 307, 2138, 2074, 466, 264, 3827, 456, 311, 512, 7719, 295, 8496, 507, 51036], "temperature": 0.0, "avg_logprob": -0.10438874003651379, "compression_ratio": 1.9086538461538463, "no_speech_prob": 0.0018086471827700734}, {"id": 376, "seek": 243548, "start": 2448.92, "end": 2453.88, "text": " i want to build a model that works i can't train a hundred billion parameters but i can train", "tokens": [51036, 741, 528, 281, 1322, 257, 2316, 300, 1985, 741, 393, 380, 3847, 257, 3262, 5218, 9834, 457, 741, 393, 3847, 51284], "temperature": 0.0, "avg_logprob": -0.10438874003651379, "compression_ratio": 1.9086538461538463, "no_speech_prob": 0.0018086471827700734}, {"id": 377, "seek": 243548, "start": 2454.68, "end": 2459.88, "text": " you know a hundred or a thousand fine yeah and the inductive priors in this are invariants", "tokens": [51324, 291, 458, 257, 3262, 420, 257, 4714, 2489, 1338, 293, 264, 31612, 488, 1790, 830, 294, 341, 366, 33270, 1719, 51584], "temperature": 0.0, "avg_logprob": -0.10438874003651379, "compression_ratio": 1.9086538461538463, "no_speech_prob": 0.0018086471827700734}, {"id": 378, "seek": 245988, "start": 2460.6, "end": 2467.32, "text": " namely i can see you i can still see you yeah that's you know like you're still there like that's", "tokens": [50400, 20926, 741, 393, 536, 291, 741, 393, 920, 536, 291, 1338, 300, 311, 291, 458, 411, 291, 434, 920, 456, 411, 300, 311, 50736], "temperature": 0.0, "avg_logprob": -0.07380422567709899, "compression_ratio": 1.793939393939394, "no_speech_prob": 0.05736217647790909}, {"id": 379, "seek": 245988, "start": 2467.32, "end": 2474.6, "text": " invariance yeah um and the other thing is locality and i think locality makes a lot of sense like when", "tokens": [50736, 33270, 719, 1338, 1105, 293, 264, 661, 551, 307, 1628, 1860, 293, 741, 519, 1628, 1860, 1669, 257, 688, 295, 2020, 411, 562, 51100], "temperature": 0.0, "avg_logprob": -0.07380422567709899, "compression_ratio": 1.793939393939394, "no_speech_prob": 0.05736217647790909}, {"id": 380, "seek": 245988, "start": 2474.6, "end": 2480.36, "text": " i look at this room i don't think the first thing that happens in my brain is i think oh i'm in", "tokens": [51100, 741, 574, 412, 341, 1808, 741, 500, 380, 519, 264, 700, 551, 300, 2314, 294, 452, 3567, 307, 741, 519, 1954, 741, 478, 294, 51388], "temperature": 0.0, "avg_logprob": -0.07380422567709899, "compression_ratio": 1.793939393939394, "no_speech_prob": 0.05736217647790909}, {"id": 381, "seek": 248036, "start": 2480.36, "end": 2490.44, "text": " kaslo 273 what happens first is i go oh edge corner chair person stairs light looks like a lecture hall", "tokens": [50364, 19173, 752, 7634, 18, 437, 2314, 700, 307, 741, 352, 1954, 4691, 4538, 6090, 954, 13471, 1442, 1542, 411, 257, 7991, 6500, 50868], "temperature": 0.0, "avg_logprob": -0.11697272459665935, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.061742156744003296}, {"id": 382, "seek": 248036, "start": 2491.32, "end": 2501.4, "text": " sydney probably kaslo 273 yeah and so that's invariant that's um locality and these are our", "tokens": [50912, 943, 67, 2397, 1391, 19173, 752, 7634, 18, 1338, 293, 370, 300, 311, 33270, 394, 300, 311, 1105, 1628, 1860, 293, 613, 366, 527, 51416], "temperature": 0.0, "avg_logprob": -0.11697272459665935, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.061742156744003296}, {"id": 383, "seek": 248036, "start": 2501.4, "end": 2506.6, "text": " inductive priors and these inductive priors massively cut down the parameters and then from", "tokens": [51416, 31612, 488, 1790, 830, 293, 613, 31612, 488, 1790, 830, 29379, 1723, 760, 264, 9834, 293, 550, 490, 51676], "temperature": 0.0, "avg_logprob": -0.11697272459665935, "compression_ratio": 1.6214689265536724, "no_speech_prob": 0.061742156744003296}, {"id": 384, "seek": 250660, "start": 2506.6, "end": 2511.7999999999997, "text": " then we're cooking on gas and we can get these models that actually work no but it's again like", "tokens": [50364, 550, 321, 434, 6361, 322, 4211, 293, 321, 393, 483, 613, 5245, 300, 767, 589, 572, 457, 309, 311, 797, 411, 50624], "temperature": 0.0, "avg_logprob": -0.059381029108068445, "compression_ratio": 1.8027522935779816, "no_speech_prob": 0.003823470789939165}, {"id": 385, "seek": 250660, "start": 2511.7999999999997, "end": 2519.24, "text": " changing the group is a gain inductive priors so um for example like you know have you been upside", "tokens": [50624, 4473, 264, 1594, 307, 257, 6052, 31612, 488, 1790, 830, 370, 1105, 337, 1365, 411, 291, 458, 362, 291, 668, 14119, 50996], "temperature": 0.0, "avg_logprob": -0.059381029108068445, "compression_ratio": 1.8027522935779816, "no_speech_prob": 0.003823470789939165}, {"id": 386, "seek": 250660, "start": 2519.24, "end": 2526.52, "text": " down and you look and it's actually much harder to recognize stuff so the idea that we satisfy this", "tokens": [50996, 760, 293, 291, 574, 293, 309, 311, 767, 709, 6081, 281, 5521, 1507, 370, 264, 1558, 300, 321, 19319, 341, 51360], "temperature": 0.0, "avg_logprob": -0.059381029108068445, "compression_ratio": 1.8027522935779816, "no_speech_prob": 0.003823470789939165}, {"id": 387, "seek": 250660, "start": 2526.52, "end": 2531.72, "text": " invariance is much less well established than the idea that we satisfy this invariance and then we", "tokens": [51360, 33270, 719, 307, 709, 1570, 731, 7545, 813, 264, 1558, 300, 321, 19319, 341, 33270, 719, 293, 550, 321, 51620], "temperature": 0.0, "avg_logprob": -0.059381029108068445, "compression_ratio": 1.8027522935779816, "no_speech_prob": 0.003823470789939165}, {"id": 388, "seek": 253172, "start": 2531.72, "end": 2539.56, "text": " want to bake in that symmetry exactly yeah i don't know if you really want rotations so like a classic", "tokens": [50364, 528, 281, 16562, 294, 300, 25440, 2293, 1338, 741, 500, 380, 458, 498, 291, 534, 528, 44796, 370, 411, 257, 7230, 50756], "temperature": 0.0, "avg_logprob": -0.06950414037129965, "compression_ratio": 1.6168224299065421, "no_speech_prob": 0.002549753524363041}, {"id": 389, "seek": 253172, "start": 2539.56, "end": 2545.08, "text": " pre-training task in image recognition is to recognize whether your image is upside down or not", "tokens": [50756, 659, 12, 17227, 1760, 5633, 294, 3256, 11150, 307, 281, 5521, 1968, 428, 3256, 307, 14119, 760, 420, 406, 51032], "temperature": 0.0, "avg_logprob": -0.06950414037129965, "compression_ratio": 1.6168224299065421, "no_speech_prob": 0.002549753524363041}, {"id": 390, "seek": 253172, "start": 2546.4399999999996, "end": 2551.56, "text": " so you know that's a classically non-invariant thing under rotation by 180", "tokens": [51100, 370, 291, 458, 300, 311, 257, 1508, 984, 2107, 12, 259, 34033, 394, 551, 833, 12447, 538, 11971, 51356], "temperature": 0.0, "avg_logprob": -0.06950414037129965, "compression_ratio": 1.6168224299065421, "no_speech_prob": 0.002549753524363041}, {"id": 391, "seek": 253172, "start": 2554.2, "end": 2558.04, "text": " but there's there's situations like you know in an MRI scan or something", "tokens": [51488, 457, 456, 311, 456, 311, 6851, 411, 291, 458, 294, 364, 32812, 11049, 420, 746, 51680], "temperature": 0.0, "avg_logprob": -0.06950414037129965, "compression_ratio": 1.6168224299065421, "no_speech_prob": 0.002549753524363041}, {"id": 392, "seek": 255804, "start": 2558.04, "end": 2562.7599999999998, "text": " where it really you know you really want that invariance that's when you should bake it into the model", "tokens": [50364, 689, 309, 534, 291, 458, 291, 534, 528, 300, 33270, 719, 300, 311, 562, 291, 820, 16562, 309, 666, 264, 2316, 50600], "temperature": 0.0, "avg_logprob": -0.10486253877965415, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0002778121561277658}, {"id": 393, "seek": 255804, "start": 2566.7599999999998, "end": 2572.2799999999997, "text": " if you go back to our friend l2 of s2 there's a great exercise so the laplacian", "tokens": [50800, 498, 291, 352, 646, 281, 527, 1277, 287, 17, 295, 262, 17, 456, 311, 257, 869, 5380, 370, 264, 635, 564, 326, 952, 51076], "temperature": 0.0, "avg_logprob": -0.10486253877965415, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0002778121561277658}, {"id": 394, "seek": 255804, "start": 2572.2799999999997, "end": 2577.16, "text": " okay so the casimir gives you the laplacian on the sphere and the eigenspaces for the laplacian", "tokens": [51076, 1392, 370, 264, 3058, 26935, 2709, 291, 264, 635, 564, 326, 952, 322, 264, 16687, 293, 264, 9728, 694, 79, 2116, 337, 264, 635, 564, 326, 952, 51320], "temperature": 0.0, "avg_logprob": -0.10486253877965415, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0002778121561277658}, {"id": 395, "seek": 255804, "start": 2577.16, "end": 2582.36, "text": " are the spherical harmonics so the the casimir is acting everywhere in this whole big diagram", "tokens": [51320, 366, 264, 37300, 14750, 1167, 370, 264, 264, 3058, 26935, 307, 6577, 5315, 294, 341, 1379, 955, 10686, 51580], "temperature": 0.0, "avg_logprob": -0.10486253877965415, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0002778121561277658}, {"id": 396, "seek": 255804, "start": 2582.36, "end": 2587.24, "text": " commuting with everything and splitting it up into irreducible it's not so much locality it's the", "tokens": [51580, 800, 10861, 365, 1203, 293, 30348, 309, 493, 666, 16014, 769, 32128, 309, 311, 406, 370, 709, 1628, 1860, 309, 311, 264, 51824], "temperature": 0.0, "avg_logprob": -0.10486253877965415, "compression_ratio": 1.8503937007874016, "no_speech_prob": 0.0002778121561277658}, {"id": 397, "seek": 258724, "start": 2587.24, "end": 2599.64, "text": " fact that so three is maybe i can write so you know l2 of s2 is a topological direct sum of l gamma", "tokens": [50364, 1186, 300, 370, 1045, 307, 1310, 741, 393, 2464, 370, 291, 458, 287, 17, 295, 262, 17, 307, 257, 1192, 4383, 2047, 2408, 295, 287, 15546, 50984], "temperature": 0.0, "avg_logprob": -0.2658813290479707, "compression_ratio": 1.3365384615384615, "no_speech_prob": 0.001029480597935617}, {"id": 398, "seek": 258724, "start": 2600.68, "end": 2602.9199999999996, "text": " where gamma is", "tokens": [51036, 689, 15546, 307, 51148], "temperature": 0.0, "avg_logprob": -0.2658813290479707, "compression_ratio": 1.3365384615384615, "no_speech_prob": 0.001029480597935617}, {"id": 399, "seek": 258724, "start": 2608.68, "end": 2609.8799999999997, "text": " is a spherical harmonics", "tokens": [51436, 307, 257, 37300, 14750, 1167, 51496], "temperature": 0.0, "avg_logprob": -0.2658813290479707, "compression_ratio": 1.3365384615384615, "no_speech_prob": 0.001029480597935617}, {"id": 400, "seek": 261724, "start": 2617.72, "end": 2623.24, "text": " and the the casimir is providing this decomposition so it's breaking", "tokens": [50388, 293, 264, 264, 3058, 26935, 307, 6530, 341, 48356, 370, 309, 311, 7697, 50664], "temperature": 0.0, "avg_logprob": -0.15721050898234049, "compression_ratio": 1.8248587570621468, "no_speech_prob": 0.003119515022262931}, {"id": 401, "seek": 261724, "start": 2624.52, "end": 2628.12, "text": " breaking up this space so the casimir on each one of these acts by a different scalar", "tokens": [50728, 7697, 493, 341, 1901, 370, 264, 3058, 26935, 322, 1184, 472, 295, 613, 10672, 538, 257, 819, 39684, 50908], "temperature": 0.0, "avg_logprob": -0.15721050898234049, "compression_ratio": 1.8248587570621468, "no_speech_prob": 0.003119515022262931}, {"id": 402, "seek": 261724, "start": 2629.24, "end": 2635.8799999999997, "text": " and it's the the casimir aka the laplacian acts on each of these you know this is something like", "tokens": [50964, 293, 309, 311, 264, 264, 3058, 26935, 28042, 264, 635, 564, 326, 952, 10672, 322, 1184, 295, 613, 291, 458, 341, 307, 746, 411, 51296], "temperature": 0.0, "avg_logprob": -0.15721050898234049, "compression_ratio": 1.8248587570621468, "no_speech_prob": 0.003119515022262931}, {"id": 403, "seek": 261724, "start": 2636.68, "end": 2639.64, "text": " restriction of of degree", "tokens": [51336, 29529, 295, 295, 4314, 51484], "temperature": 0.0, "avg_logprob": -0.15721050898234049, "compression_ratio": 1.8248587570621468, "no_speech_prob": 0.003119515022262931}, {"id": 404, "seek": 261724, "start": 2641.8799999999997, "end": 2644.9199999999996, "text": " gamma or gamma over two polynomial polynomials", "tokens": [51596, 15546, 420, 15546, 670, 732, 26110, 22560, 12356, 51748], "temperature": 0.0, "avg_logprob": -0.15721050898234049, "compression_ratio": 1.8248587570621468, "no_speech_prob": 0.003119515022262931}, {"id": 405, "seek": 264724, "start": 2647.7999999999997, "end": 2655.16, "text": " and so you have this totally canonical decomposition of this space of functions", "tokens": [50392, 293, 370, 291, 362, 341, 3879, 46491, 48356, 295, 341, 1901, 295, 6828, 50760], "temperature": 0.0, "avg_logprob": -0.10134684453245069, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.0006871750811114907}, {"id": 406, "seek": 264724, "start": 2655.8799999999997, "end": 2664.52, "text": " and everything wherever i had this picture everything here is respecting this decomposition", "tokens": [50796, 293, 1203, 8660, 741, 632, 341, 3036, 1203, 510, 307, 41968, 341, 48356, 51228], "temperature": 0.0, "avg_logprob": -0.10134684453245069, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.0006871750811114907}, {"id": 407, "seek": 264724, "start": 2664.52, "end": 2666.68, "text": " all the linear maps respecting this decomposition", "tokens": [51228, 439, 264, 8213, 11317, 41968, 341, 48356, 51336], "temperature": 0.0, "avg_logprob": -0.10134684453245069, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.0006871750811114907}, {"id": 408, "seek": 264724, "start": 2670.04, "end": 2675.3999999999996, "text": " and roughly speaking you can kind of think about like you know if you take your function here", "tokens": [51504, 293, 9810, 4124, 291, 393, 733, 295, 519, 466, 411, 291, 458, 498, 291, 747, 428, 2445, 510, 51772], "temperature": 0.0, "avg_logprob": -0.10134684453245069, "compression_ratio": 1.8975903614457832, "no_speech_prob": 0.0006871750811114907}, {"id": 409, "seek": 267540, "start": 2676.04, "end": 2680.6, "text": " and do a Fourier expansion of it then you get a whole lot of quantities", "tokens": [50396, 293, 360, 257, 36810, 11260, 295, 309, 550, 291, 483, 257, 1379, 688, 295, 22927, 50624], "temperature": 0.0, "avg_logprob": -0.11120828683825507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.0018941337475553155}, {"id": 410, "seek": 267540, "start": 2681.7200000000003, "end": 2684.52, "text": " and those quantities are giving you the", "tokens": [50680, 293, 729, 22927, 366, 2902, 291, 264, 50820], "temperature": 0.0, "avg_logprob": -0.11120828683825507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.0018941337475553155}, {"id": 411, "seek": 267540, "start": 2688.6, "end": 2692.44, "text": " you get a whole lot of scalars and those scalars are basically telling you how it acts on each one of these", "tokens": [51024, 291, 483, 257, 1379, 688, 295, 15664, 685, 293, 729, 15664, 685, 366, 1936, 3585, 291, 577, 309, 10672, 322, 1184, 472, 295, 613, 51216], "temperature": 0.0, "avg_logprob": -0.11120828683825507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.0018941337475553155}, {"id": 412, "seek": 267540, "start": 2698.6800000000003, "end": 2704.04, "text": " okay so this is just super interesting for me but maybe more technical", "tokens": [51528, 1392, 370, 341, 307, 445, 1687, 1880, 337, 385, 457, 1310, 544, 6191, 51796], "temperature": 0.0, "avg_logprob": -0.11120828683825507, "compression_ratio": 1.7365269461077844, "no_speech_prob": 0.0018941337475553155}, {"id": 413, "seek": 270404, "start": 2704.92, "end": 2711.4, "text": " uh so i want to go in the second half i want to go over why permutation representations i want to go over", "tokens": [50408, 2232, 370, 741, 528, 281, 352, 294, 264, 1150, 1922, 741, 528, 281, 352, 670, 983, 4784, 11380, 33358, 741, 528, 281, 352, 670, 50732], "temperature": 0.0, "avg_logprob": -0.15221915245056153, "compression_ratio": 1.8248175182481752, "no_speech_prob": 0.0023581204004585743}, {"id": 414, "seek": 270404, "start": 2711.4, "end": 2717.88, "text": " something called deep sets and then i want to go over graph neural nets", "tokens": [50732, 746, 1219, 2452, 6352, 293, 550, 741, 528, 281, 352, 670, 4295, 18161, 36170, 51056], "temperature": 0.0, "avg_logprob": -0.15221915245056153, "compression_ratio": 1.8248175182481752, "no_speech_prob": 0.0023581204004585743}, {"id": 415, "seek": 270404, "start": 2719.24, "end": 2725.08, "text": " so one question that is very natural to ask as a representation theorist", "tokens": [51124, 370, 472, 1168, 300, 307, 588, 3303, 281, 1029, 382, 257, 10290, 27423, 468, 51416], "temperature": 0.0, "avg_logprob": -0.15221915245056153, "compression_ratio": 1.8248175182481752, "no_speech_prob": 0.0023581204004585743}, {"id": 416, "seek": 272508, "start": 2725.4, "end": 2735.56, "text": " is why so if you take functions on a set on a gamma set this is called a permutation representation", "tokens": [50380, 307, 983, 370, 498, 291, 747, 6828, 322, 257, 992, 322, 257, 15546, 992, 341, 307, 1219, 257, 4784, 11380, 10290, 50888], "temperature": 0.0, "avg_logprob": -0.08665582198130933, "compression_ratio": 2.087912087912088, "no_speech_prob": 0.018537679687142372}, {"id": 417, "seek": 272508, "start": 2736.36, "end": 2740.36, "text": " and it's called a permutation representation because if you look at the matrices that represent", "tokens": [50928, 293, 309, 311, 1219, 257, 4784, 11380, 10290, 570, 498, 291, 574, 412, 264, 32284, 300, 2906, 51128], "temperature": 0.0, "avg_logprob": -0.08665582198130933, "compression_ratio": 2.087912087912088, "no_speech_prob": 0.018537679687142372}, {"id": 418, "seek": 272508, "start": 2740.36, "end": 2748.2, "text": " your elements they're permutation matrices and in our first class in representation theory the", "tokens": [51128, 428, 4959, 436, 434, 4784, 11380, 32284, 293, 294, 527, 700, 1508, 294, 10290, 5261, 264, 51520], "temperature": 0.0, "avg_logprob": -0.08665582198130933, "compression_ratio": 2.087912087912088, "no_speech_prob": 0.018537679687142372}, {"id": 419, "seek": 272508, "start": 2748.2, "end": 2752.84, "text": " first representations we see a permutation but then we quickly convince our students that", "tokens": [51520, 700, 33358, 321, 536, 257, 4784, 11380, 457, 550, 321, 2661, 13447, 527, 1731, 300, 51752], "temperature": 0.0, "avg_logprob": -0.08665582198130933, "compression_ratio": 2.087912087912088, "no_speech_prob": 0.018537679687142372}, {"id": 420, "seek": 275284, "start": 2753.8, "end": 2758.1200000000003, "text": " we should break up permutation representations into irreducible representations and that's", "tokens": [50412, 321, 820, 1821, 493, 4784, 11380, 33358, 666, 16014, 769, 32128, 33358, 293, 300, 311, 50628], "temperature": 0.0, "avg_logprob": -0.0651244322458903, "compression_ratio": 1.8711340206185567, "no_speech_prob": 0.00026912870816886425}, {"id": 421, "seek": 275284, "start": 2758.1200000000003, "end": 2766.44, "text": " really interesting so why am i insisting that we have permutation representations everywhere", "tokens": [50628, 534, 1880, 370, 983, 669, 741, 13466, 278, 300, 321, 362, 4784, 11380, 33358, 5315, 51044], "temperature": 0.0, "avg_logprob": -0.0651244322458903, "compression_ratio": 1.8711340206185567, "no_speech_prob": 0.00026912870816886425}, {"id": 422, "seek": 275284, "start": 2768.76, "end": 2773.6400000000003, "text": " so i i found that this charming little lemma which i i didn't find in the literature", "tokens": [51160, 370, 741, 741, 1352, 300, 341, 23387, 707, 7495, 1696, 597, 741, 741, 994, 380, 915, 294, 264, 10394, 51404], "temperature": 0.0, "avg_logprob": -0.0651244322458903, "compression_ratio": 1.8711340206185567, "no_speech_prob": 0.00026912870816886425}, {"id": 423, "seek": 275284, "start": 2774.36, "end": 2779.88, "text": " which is if you have a representation of gamma which is assumed finite i'm not quite sure what", "tokens": [51440, 597, 307, 498, 291, 362, 257, 10290, 295, 15546, 597, 307, 15895, 19362, 741, 478, 406, 1596, 988, 437, 51716], "temperature": 0.0, "avg_logprob": -0.0651244322458903, "compression_ratio": 1.8711340206185567, "no_speech_prob": 0.00026912870816886425}, {"id": 424, "seek": 277988, "start": 2779.88, "end": 2786.76, "text": " the analog of this for a league group is then we so once we have v we can choose a basis for it", "tokens": [50364, 264, 16660, 295, 341, 337, 257, 14957, 1594, 307, 550, 321, 370, 1564, 321, 362, 371, 321, 393, 2826, 257, 5143, 337, 309, 50708], "temperature": 0.0, "avg_logprob": -0.08467232097278941, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.006093826610594988}, {"id": 425, "seek": 277988, "start": 2787.6400000000003, "end": 2794.6, "text": " and once we have a basis we can ask is relu gamma equivalent yep so remember relu takes our", "tokens": [50752, 293, 1564, 321, 362, 257, 5143, 321, 393, 1029, 307, 1039, 84, 15546, 10344, 18633, 370, 1604, 1039, 84, 2516, 527, 51100], "temperature": 0.0, "avg_logprob": -0.08467232097278941, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.006093826610594988}, {"id": 426, "seek": 277988, "start": 2795.48, "end": 2799.2400000000002, "text": " vector which now that we have a basis is just a sequence of real numbers and the ones that are", "tokens": [51144, 8062, 597, 586, 300, 321, 362, 257, 5143, 307, 445, 257, 8310, 295, 957, 3547, 293, 264, 2306, 300, 366, 51332], "temperature": 0.0, "avg_logprob": -0.08467232097278941, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.006093826610594988}, {"id": 427, "seek": 277988, "start": 2799.2400000000002, "end": 2803.4, "text": " negative it sets to zero and the ones that are positive it keeps so is this gamma equivalent", "tokens": [51332, 3671, 309, 6352, 281, 4018, 293, 264, 2306, 300, 366, 3353, 309, 5965, 370, 307, 341, 15546, 10344, 51540], "temperature": 0.0, "avg_logprob": -0.08467232097278941, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.006093826610594988}, {"id": 428, "seek": 277988, "start": 2804.12, "end": 2809.4, "text": " if our representation is just permuting our coordinates around it's easier to see that", "tokens": [51576, 498, 527, 10290, 307, 445, 4784, 10861, 527, 21056, 926, 309, 311, 3571, 281, 536, 300, 51840], "temperature": 0.0, "avg_logprob": -0.08467232097278941, "compression_ratio": 1.9012345679012346, "no_speech_prob": 0.006093826610594988}, {"id": 429, "seek": 280940, "start": 2809.4, "end": 2817.88, "text": " it's gamma equivalent but uh that's if and only if so in order to have a gamma equivalent", "tokens": [50364, 309, 311, 15546, 10344, 457, 2232, 300, 311, 498, 293, 787, 498, 370, 294, 1668, 281, 362, 257, 15546, 10344, 50788], "temperature": 0.0, "avg_logprob": -0.10562434828425028, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.000209738384000957}, {"id": 430, "seek": 280940, "start": 2817.88, "end": 2821.48, "text": " relu with respect to some basis you have to be permutation with respect to that basis", "tokens": [50788, 1039, 84, 365, 3104, 281, 512, 5143, 291, 362, 281, 312, 4784, 11380, 365, 3104, 281, 300, 5143, 50968], "temperature": 0.0, "avg_logprob": -0.10562434828425028, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.000209738384000957}, {"id": 431, "seek": 280940, "start": 2824.76, "end": 2830.12, "text": " now this is something that has been exciting me a lot over the last few days and is having", "tokens": [51132, 586, 341, 307, 746, 300, 575, 668, 4670, 385, 257, 688, 670, 264, 1036, 1326, 1708, 293, 307, 1419, 51400], "temperature": 0.0, "avg_logprob": -0.10562434828425028, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.000209738384000957}, {"id": 432, "seek": 280940, "start": 2830.12, "end": 2835.4, "text": " a hell of a lot of fun with is basically like piecewise linear representation theory", "tokens": [51400, 257, 4921, 295, 257, 688, 295, 1019, 365, 307, 1936, 411, 2522, 3711, 8213, 10290, 5261, 51664], "temperature": 0.0, "avg_logprob": -0.10562434828425028, "compression_ratio": 1.763819095477387, "no_speech_prob": 0.000209738384000957}, {"id": 433, "seek": 283540, "start": 2836.2000000000003, "end": 2842.2000000000003, "text": " um so what you could imagine is you you have these layers and they all break up into irreducible", "tokens": [50404, 1105, 370, 437, 291, 727, 3811, 307, 291, 291, 362, 613, 7914, 293, 436, 439, 1821, 493, 666, 16014, 769, 32128, 50704], "temperature": 0.0, "avg_logprob": -0.07637311124253547, "compression_ratio": 1.9893048128342246, "no_speech_prob": 0.0007910720305517316}, {"id": 434, "seek": 283540, "start": 2842.2000000000003, "end": 2849.7200000000003, "text": " representations and if you include an irreducible into a permutation representation do a relu and", "tokens": [50704, 33358, 293, 498, 291, 4090, 364, 16014, 769, 32128, 666, 257, 4784, 11380, 10290, 360, 257, 1039, 84, 293, 51080], "temperature": 0.0, "avg_logprob": -0.07637311124253547, "compression_ratio": 1.9893048128342246, "no_speech_prob": 0.0007910720305517316}, {"id": 435, "seek": 283540, "start": 2849.7200000000003, "end": 2856.52, "text": " then project back you get a piecewise linear endomorphism equivariant endomorphism of your", "tokens": [51080, 550, 1716, 646, 291, 483, 257, 2522, 3711, 8213, 917, 32702, 1434, 1267, 592, 3504, 394, 917, 32702, 1434, 295, 428, 51420], "temperature": 0.0, "avg_logprob": -0.07637311124253547, "compression_ratio": 1.9893048128342246, "no_speech_prob": 0.0007910720305517316}, {"id": 436, "seek": 283540, "start": 2856.52, "end": 2860.76, "text": " representation and so what you could imagine is networks in which you have irreducible", "tokens": [51420, 10290, 293, 370, 437, 291, 727, 3811, 307, 9590, 294, 597, 291, 362, 16014, 769, 32128, 51632], "temperature": 0.0, "avg_logprob": -0.07637311124253547, "compression_ratio": 1.9893048128342246, "no_speech_prob": 0.0007910720305517316}, {"id": 437, "seek": 286076, "start": 2860.76, "end": 2870.28, "text": " representations together with equivariant non-linearities and my impression is that this", "tokens": [50364, 33358, 1214, 365, 1267, 592, 3504, 394, 2107, 12, 28263, 1088, 293, 452, 9995, 307, 300, 341, 50840], "temperature": 0.0, "avg_logprob": -0.08762539695290958, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.005213166121393442}, {"id": 438, "seek": 286076, "start": 2870.28, "end": 2875.8, "text": " is extremely interesting and I've already learned like basic things about representations that I", "tokens": [50840, 307, 4664, 1880, 293, 286, 600, 1217, 3264, 411, 3875, 721, 466, 33358, 300, 286, 51116], "temperature": 0.0, "avg_logprob": -0.08762539695290958, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.005213166121393442}, {"id": 439, "seek": 286076, "start": 2875.8, "end": 2881.0800000000004, "text": " didn't know from thinking about this so if you're interested in this ask me but it's kind of much", "tokens": [51116, 994, 380, 458, 490, 1953, 466, 341, 370, 498, 291, 434, 3102, 294, 341, 1029, 385, 457, 309, 311, 733, 295, 709, 51380], "temperature": 0.0, "avg_logprob": -0.08762539695290958, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.005213166121393442}, {"id": 440, "seek": 286076, "start": 2881.0800000000004, "end": 2887.5600000000004, "text": " more specialized so I'm not going to talk about it today so let's I want to do another example", "tokens": [51380, 544, 19813, 370, 286, 478, 406, 516, 281, 751, 466, 309, 965, 370, 718, 311, 286, 528, 281, 360, 1071, 1365, 51704], "temperature": 0.0, "avg_logprob": -0.08762539695290958, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.005213166121393442}, {"id": 441, "seek": 288756, "start": 2887.56, "end": 2894.04, "text": " of our blueprint so one way of seeing this is just first think about this", "tokens": [50364, 295, 527, 35868, 370, 472, 636, 295, 2577, 341, 307, 445, 700, 519, 466, 341, 50688], "temperature": 0.0, "avg_logprob": -0.07284099354463465, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.005994238890707493}, {"id": 442, "seek": 288756, "start": 2895.72, "end": 2901.32, "text": " so imagine that we have a whole lot of points and they're unordered and they have a whole", "tokens": [50772, 370, 3811, 300, 321, 362, 257, 1379, 688, 295, 2793, 293, 436, 434, 517, 765, 4073, 293, 436, 362, 257, 1379, 51052], "temperature": 0.0, "avg_logprob": -0.07284099354463465, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.005994238890707493}, {"id": 443, "seek": 288756, "start": 2901.32, "end": 2907.32, "text": " lot of information attached to them for example you could imagine all of the citizens of Sydney", "tokens": [51052, 688, 295, 1589, 8570, 281, 552, 337, 1365, 291, 727, 3811, 439, 295, 264, 7180, 295, 21065, 51352], "temperature": 0.0, "avg_logprob": -0.07284099354463465, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.005994238890707493}, {"id": 444, "seek": 288756, "start": 2907.32, "end": 2913.08, "text": " and they're labeled by their age and how much tax they paid last year yeah so I've got a two", "tokens": [51352, 293, 436, 434, 21335, 538, 641, 3205, 293, 577, 709, 3366, 436, 4835, 1036, 1064, 1338, 370, 286, 600, 658, 257, 732, 51640], "temperature": 0.0, "avg_logprob": -0.07284099354463465, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.005994238890707493}, {"id": 445, "seek": 291308, "start": 2913.08, "end": 2919.56, "text": " dimensional vector associated to every person in Sydney now one way of viewing this data set is", "tokens": [50364, 18795, 8062, 6615, 281, 633, 954, 294, 21065, 586, 472, 636, 295, 17480, 341, 1412, 992, 307, 50688], "temperature": 0.0, "avg_logprob": -0.05985503111566816, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0027995433192700148}, {"id": 446, "seek": 291308, "start": 2919.56, "end": 2926.36, "text": " as a point cloud so what I have is an enormous in this you know age and tax example I just have R2", "tokens": [50688, 382, 257, 935, 4588, 370, 437, 286, 362, 307, 364, 11322, 294, 341, 291, 458, 3205, 293, 3366, 1365, 286, 445, 362, 497, 17, 51028], "temperature": 0.0, "avg_logprob": -0.05985503111566816, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0027995433192700148}, {"id": 447, "seek": 291308, "start": 2926.36, "end": 2932.04, "text": " and I have an enormous number of points in R2 and I want to make some qualitative statement so a basic", "tokens": [51028, 293, 286, 362, 364, 11322, 1230, 295, 2793, 294, 497, 17, 293, 286, 528, 281, 652, 512, 31312, 5629, 370, 257, 3875, 51312], "temperature": 0.0, "avg_logprob": -0.05985503111566816, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0027995433192700148}, {"id": 448, "seek": 291308, "start": 2932.04, "end": 2936.44, "text": " statement that I could make is some kind of like center of mass statement like the average age of", "tokens": [51312, 5629, 300, 286, 727, 652, 307, 512, 733, 295, 411, 3056, 295, 2758, 5629, 411, 264, 4274, 3205, 295, 51532], "temperature": 0.0, "avg_logprob": -0.05985503111566816, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0027995433192700148}, {"id": 449, "seek": 291308, "start": 2936.44, "end": 2941.3199999999997, "text": " people in Sydney is blah that would be a kind of boring measurement but a much more interesting", "tokens": [51532, 561, 294, 21065, 307, 12288, 300, 576, 312, 257, 733, 295, 9989, 13160, 457, 257, 709, 544, 1880, 51776], "temperature": 0.0, "avg_logprob": -0.05985503111566816, "compression_ratio": 1.8185185185185184, "no_speech_prob": 0.0027995433192700148}, {"id": 450, "seek": 294132, "start": 2941.32, "end": 2947.32, "text": " measurement would be there's this kind of weird hole in this data for example you know a particular", "tokens": [50364, 13160, 576, 312, 456, 311, 341, 733, 295, 3657, 5458, 294, 341, 1412, 337, 1365, 291, 458, 257, 1729, 50664], "temperature": 0.0, "avg_logprob": -0.07379589080810547, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0034800663124769926}, {"id": 451, "seek": 294132, "start": 2947.32, "end": 2952.6800000000003, "text": " age in a particular tax is you know not paid in some region or something like that that would be", "tokens": [50664, 3205, 294, 257, 1729, 3366, 307, 291, 458, 406, 4835, 294, 512, 4458, 420, 746, 411, 300, 300, 576, 312, 50932], "temperature": 0.0, "avg_logprob": -0.07379589080810547, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0034800663124769926}, {"id": 452, "seek": 294132, "start": 2952.6800000000003, "end": 2956.84, "text": " a much more interesting statement you can make about this data analysis and there's about this", "tokens": [50932, 257, 709, 544, 1880, 5629, 291, 393, 652, 466, 341, 1412, 5215, 293, 456, 311, 466, 341, 51140], "temperature": 0.0, "avg_logprob": -0.07379589080810547, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0034800663124769926}, {"id": 453, "seek": 294132, "start": 2956.84, "end": 2963.1600000000003, "text": " data and this is one of the one of the this is related to this very interesting subject called", "tokens": [51140, 1412, 293, 341, 307, 472, 295, 264, 472, 295, 264, 341, 307, 4077, 281, 341, 588, 1880, 3983, 1219, 51456], "temperature": 0.0, "avg_logprob": -0.07379589080810547, "compression_ratio": 1.9203980099502487, "no_speech_prob": 0.0034800663124769926}, {"id": 454, "seek": 296316, "start": 2963.16, "end": 2971.7999999999997, "text": " persistent homology which I know next to nothing about uh okay but we have a point cloud so we", "tokens": [50364, 24315, 3655, 1793, 597, 286, 458, 958, 281, 1825, 466, 2232, 1392, 457, 321, 362, 257, 935, 4588, 370, 321, 50796], "temperature": 0.0, "avg_logprob": -0.11034836976424507, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.016394535079598427}, {"id": 455, "seek": 296316, "start": 2971.7999999999997, "end": 2977.16, "text": " want to make a prediction based on this point cloud and so this is an equivariant prediction task", "tokens": [50796, 528, 281, 652, 257, 17630, 2361, 322, 341, 935, 4588, 293, 370, 341, 307, 364, 1267, 592, 3504, 394, 17630, 5633, 51064], "temperature": 0.0, "avg_logprob": -0.11034836976424507, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.016394535079598427}, {"id": 456, "seek": 296316, "start": 2977.8799999999997, "end": 2986.92, "text": " we have so uh so here we have rd to the n so here are our n different points", "tokens": [51100, 321, 362, 370, 2232, 370, 510, 321, 362, 367, 67, 281, 264, 297, 370, 510, 366, 527, 297, 819, 2793, 51552], "temperature": 0.0, "avg_logprob": -0.11034836976424507, "compression_ratio": 1.7025316455696202, "no_speech_prob": 0.016394535079598427}, {"id": 457, "seek": 298692, "start": 2987.2400000000002, "end": 2994.52, "text": " uh and we want to learn some function you know like is there a big hole in this data or something", "tokens": [50380, 2232, 293, 321, 528, 281, 1466, 512, 2445, 291, 458, 411, 307, 456, 257, 955, 5458, 294, 341, 1412, 420, 746, 50744], "temperature": 0.0, "avg_logprob": -0.0785590700201086, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0037031141109764576}, {"id": 458, "seek": 298692, "start": 2994.52, "end": 3003.32, "text": " like that um and it's it's convenient to swap the indices so if you think about how s n acts here", "tokens": [50744, 411, 300, 1105, 293, 309, 311, 309, 311, 10851, 281, 18135, 264, 43840, 370, 498, 291, 519, 466, 577, 262, 297, 10672, 510, 51184], "temperature": 0.0, "avg_logprob": -0.0785590700201086, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0037031141109764576}, {"id": 459, "seek": 298692, "start": 3004.28, "end": 3011.48, "text": " it acts like I have a whole packet of numbers um and then it permutes them around but it's more", "tokens": [51232, 309, 10672, 411, 286, 362, 257, 1379, 20300, 295, 3547, 1105, 293, 550, 309, 4784, 1819, 552, 926, 457, 309, 311, 544, 51592], "temperature": 0.0, "avg_logprob": -0.0785590700201086, "compression_ratio": 1.6440677966101696, "no_speech_prob": 0.0037031141109764576}, {"id": 460, "seek": 301148, "start": 3012.28, "end": 3018.12, "text": " useful to view this from the from the point of equivariance as one packet of numbers like age", "tokens": [50404, 4420, 281, 1910, 341, 490, 264, 490, 264, 935, 295, 1267, 592, 3504, 719, 382, 472, 20300, 295, 3547, 411, 3205, 50696], "temperature": 0.0, "avg_logprob": -0.06285669548170907, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0037041816394776106}, {"id": 461, "seek": 301148, "start": 3018.12, "end": 3022.52, "text": " that's being permitted around and one packet of numbers like how much tax was paid being", "tokens": [50696, 300, 311, 885, 28658, 926, 293, 472, 20300, 295, 3547, 411, 577, 709, 3366, 390, 4835, 885, 50916], "temperature": 0.0, "avg_logprob": -0.06285669548170907, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0037041816394776106}, {"id": 462, "seek": 301148, "start": 3022.52, "end": 3029.48, "text": " commuted around and so I'll do this innocent rewriting um but I'm just pointing this out so", "tokens": [50916, 800, 4866, 926, 293, 370, 286, 603, 360, 341, 13171, 319, 19868, 1105, 457, 286, 478, 445, 12166, 341, 484, 370, 51264], "temperature": 0.0, "avg_logprob": -0.06285669548170907, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0037041816394776106}, {"id": 463, "seek": 301148, "start": 3029.48, "end": 3035.16, "text": " it doesn't confuse the hell out of you on the next slide uh so we want to make an s n invariant", "tokens": [51264, 309, 1177, 380, 28584, 264, 4921, 484, 295, 291, 322, 264, 958, 4137, 2232, 370, 321, 528, 281, 652, 364, 262, 297, 33270, 394, 51548], "temperature": 0.0, "avg_logprob": -0.06285669548170907, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0037041816394776106}, {"id": 464, "seek": 301148, "start": 3035.16, "end": 3039.64, "text": " prediction so basically we have in the language of representation theory we have a whole lot of", "tokens": [51548, 17630, 370, 1936, 321, 362, 294, 264, 2856, 295, 10290, 5261, 321, 362, 257, 1379, 688, 295, 51772], "temperature": 0.0, "avg_logprob": -0.06285669548170907, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0037041816394776106}, {"id": 465, "seek": 303964, "start": 3039.64, "end": 3045.7999999999997, "text": " copies of the most basic permutation representation of s n namely s n acts acting on functions on", "tokens": [50364, 14341, 295, 264, 881, 3875, 4784, 11380, 10290, 295, 262, 297, 20926, 262, 297, 10672, 6577, 322, 6828, 322, 50672], "temperature": 0.0, "avg_logprob": -0.05770407252841526, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0005271975533105433}, {"id": 466, "seek": 303964, "start": 3045.7999999999997, "end": 3053.0, "text": " an n set and we want to make an s n invariant prediction so let's do some basic representation", "tokens": [50672, 364, 297, 992, 293, 321, 528, 281, 652, 364, 262, 297, 33270, 394, 17630, 370, 718, 311, 360, 512, 3875, 10290, 51032], "temperature": 0.0, "avg_logprob": -0.05770407252841526, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0005271975533105433}, {"id": 467, "seek": 303964, "start": 3053.0, "end": 3059.96, "text": " theory which I almost certainly learned from andrew at some point in about 30 or something", "tokens": [51032, 5261, 597, 286, 1920, 3297, 3264, 490, 293, 2236, 412, 512, 935, 294, 466, 2217, 420, 746, 51380], "temperature": 0.0, "avg_logprob": -0.05770407252841526, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0005271975533105433}, {"id": 468, "seek": 303964, "start": 3060.68, "end": 3066.52, "text": " so we take functions from one up to n functions on the set one up to n so this is a permutation", "tokens": [51416, 370, 321, 747, 6828, 490, 472, 493, 281, 297, 6828, 322, 264, 992, 472, 493, 281, 297, 370, 341, 307, 257, 4784, 11380, 51708], "temperature": 0.0, "avg_logprob": -0.05770407252841526, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0005271975533105433}, {"id": 469, "seek": 306652, "start": 3066.52, "end": 3072.2, "text": " representation of s n and we take the trivial representation so this is where every permutation", "tokens": [50364, 10290, 295, 262, 297, 293, 321, 747, 264, 26703, 10290, 370, 341, 307, 689, 633, 4784, 11380, 50648], "temperature": 0.0, "avg_logprob": -0.046836521890428334, "compression_ratio": 2.0466101694915255, "no_speech_prob": 0.010162366554141045}, {"id": 470, "seek": 306652, "start": 3072.2, "end": 3080.44, "text": " just acts by the identity so here's a whole lot of homomorphism formulas so this is before I was", "tokens": [50648, 445, 10672, 538, 264, 6575, 370, 510, 311, 257, 1379, 688, 295, 3655, 32702, 1434, 30546, 370, 341, 307, 949, 286, 390, 51060], "temperature": 0.0, "avg_logprob": -0.046836521890428334, "compression_ratio": 2.0466101694915255, "no_speech_prob": 0.010162366554141045}, {"id": 471, "seek": 306652, "start": 3080.44, "end": 3085.88, "text": " saying what are the arrows in my neural net here I'm working them out explicitly what's what parameters", "tokens": [51060, 1566, 437, 366, 264, 19669, 294, 452, 18161, 2533, 510, 286, 478, 1364, 552, 484, 20803, 437, 311, 437, 9834, 51332], "temperature": 0.0, "avg_logprob": -0.046836521890428334, "compression_ratio": 2.0466101694915255, "no_speech_prob": 0.010162366554141045}, {"id": 472, "seek": 306652, "start": 3085.88, "end": 3089.64, "text": " there are so home from the trivial representation of the trivial representation this is the same", "tokens": [51332, 456, 366, 370, 1280, 490, 264, 26703, 10290, 295, 264, 26703, 10290, 341, 307, 264, 912, 51520], "temperature": 0.0, "avg_logprob": -0.046836521890428334, "compression_ratio": 2.0466101694915255, "no_speech_prob": 0.010162366554141045}, {"id": 473, "seek": 306652, "start": 3089.64, "end": 3096.36, "text": " thing as home from r to r this is r times the identity home from n to one this is another", "tokens": [51520, 551, 382, 1280, 490, 367, 281, 367, 341, 307, 367, 1413, 264, 6575, 1280, 490, 297, 281, 472, 341, 307, 1071, 51856], "temperature": 0.0, "avg_logprob": -0.046836521890428334, "compression_ratio": 2.0466101694915255, "no_speech_prob": 0.010162366554141045}, {"id": 474, "seek": 309636, "start": 3096.36, "end": 3102.04, "text": " instance of this statement that on a permutation representation the only invariant measurement", "tokens": [50364, 5197, 295, 341, 5629, 300, 322, 257, 4784, 11380, 10290, 264, 787, 33270, 394, 13160, 50648], "temperature": 0.0, "avg_logprob": -0.04103537968226841, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.0010640588589012623}, {"id": 475, "seek": 309636, "start": 3102.04, "end": 3108.52, "text": " you can make is essentially summing up your entries that's that home back the other way", "tokens": [50648, 291, 393, 652, 307, 4476, 2408, 2810, 493, 428, 23041, 300, 311, 300, 1280, 646, 264, 661, 636, 50972], "temperature": 0.0, "avg_logprob": -0.04103537968226841, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.0010640588589012623}, {"id": 476, "seek": 309636, "start": 3109.4, "end": 3114.92, "text": " here if we look at the image of one we want some vector we want some function which is", "tokens": [51016, 510, 498, 321, 574, 412, 264, 3256, 295, 472, 321, 528, 512, 8062, 321, 528, 512, 2445, 597, 307, 51292], "temperature": 0.0, "avg_logprob": -0.04103537968226841, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.0010640588589012623}, {"id": 477, "seek": 309636, "start": 3114.92, "end": 3120.2000000000003, "text": " invariant under s n i.e. we want this function to take the same value everywhere which is alpha", "tokens": [51292, 33270, 394, 833, 262, 297, 741, 13, 68, 13, 321, 528, 341, 2445, 281, 747, 264, 912, 2158, 5315, 597, 307, 8961, 51556], "temperature": 0.0, "avg_logprob": -0.04103537968226841, "compression_ratio": 1.7892156862745099, "no_speech_prob": 0.0010640588589012623}, {"id": 478, "seek": 312020, "start": 3120.8399999999997, "end": 3128.12, "text": " now a little bit trickier a little bit like you know this would be a second week exercise", "tokens": [50396, 586, 257, 707, 857, 4282, 811, 257, 707, 857, 411, 291, 458, 341, 576, 312, 257, 1150, 1243, 5380, 50760], "temperature": 0.0, "avg_logprob": -0.11439261436462403, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0008290536352433264}, {"id": 479, "seek": 312020, "start": 3128.12, "end": 3132.2, "text": " in representations of the symmetric group or something is that home from the trivial from", "tokens": [50760, 294, 33358, 295, 264, 32330, 1594, 420, 746, 307, 300, 1280, 490, 264, 26703, 490, 50964], "temperature": 0.0, "avg_logprob": -0.11439261436462403, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0008290536352433264}, {"id": 480, "seek": 312020, "start": 3132.2, "end": 3137.96, "text": " this permutation representation to itself is two-dimensional and it's spanned by the identity", "tokens": [50964, 341, 4784, 11380, 10290, 281, 2564, 307, 732, 12, 18759, 293, 309, 311, 637, 5943, 538, 264, 6575, 51252], "temperature": 0.0, "avg_logprob": -0.11439261436462403, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0008290536352433264}, {"id": 481, "seek": 312020, "start": 3137.96, "end": 3143.56, "text": " and the map that sums up the coordinates and and takes that sum and multiplies it by", "tokens": [51252, 293, 264, 4471, 300, 34499, 493, 264, 21056, 293, 293, 2516, 300, 2408, 293, 12788, 530, 309, 538, 51532], "temperature": 0.0, "avg_logprob": -0.11439261436462403, "compression_ratio": 1.7463414634146341, "no_speech_prob": 0.0008290536352433264}, {"id": 482, "seek": 314356, "start": 3144.52, "end": 3154.7599999999998, "text": " the constant function now exercise deduce this from the double coset formula all of these formulas", "tokens": [50412, 264, 5754, 2445, 586, 5380, 4172, 4176, 341, 490, 264, 3834, 3792, 302, 8513, 439, 295, 613, 30546, 50924], "temperature": 0.0, "avg_logprob": -0.1801781803369522, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007686383090913296}, {"id": 483, "seek": 314356, "start": 3154.7599999999998, "end": 3158.84, "text": " are very easy concept consequences of the double coset formula why is that the case", "tokens": [50924, 366, 588, 1858, 3410, 10098, 295, 264, 3834, 3792, 302, 8513, 983, 307, 300, 264, 1389, 51128], "temperature": 0.0, "avg_logprob": -0.1801781803369522, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007686383090913296}, {"id": 484, "seek": 314356, "start": 3160.7599999999998, "end": 3166.04, "text": " do it if you're a student you should do this and if you're a student it's not already obvious", "tokens": [51224, 360, 309, 498, 291, 434, 257, 3107, 291, 820, 360, 341, 293, 498, 291, 434, 257, 3107, 309, 311, 406, 1217, 6322, 51488], "temperature": 0.0, "avg_logprob": -0.1801781803369522, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.007686383090913296}, {"id": 485, "seek": 316604, "start": 3166.04, "end": 3171.96, "text": " to you you should do this so i this is deep set architecture so you can look at so here's", "tokens": [50364, 281, 291, 291, 820, 360, 341, 370, 741, 341, 307, 2452, 992, 9482, 370, 291, 393, 574, 412, 370, 510, 311, 50660], "temperature": 0.0, "avg_logprob": -0.18128985252933225, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.01969582587480545}, {"id": 486, "seek": 316604, "start": 3172.7599999999998, "end": 3179.16, "text": " here's our paper from 2017 and to me as a non-machine learning person it looks a bit", "tokens": [50700, 510, 311, 527, 3035, 490, 6591, 293, 281, 385, 382, 257, 2107, 12, 46061, 2539, 954, 309, 1542, 257, 857, 51020], "temperature": 0.0, "avg_logprob": -0.18128985252933225, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.01969582587480545}, {"id": 487, "seek": 316604, "start": 3179.16, "end": 3188.12, "text": " mystifying but this is just another instance of our blueprint so here's our input so this would be", "tokens": [51020, 9111, 5489, 457, 341, 307, 445, 1071, 5197, 295, 527, 35868, 370, 510, 311, 527, 4846, 370, 341, 576, 312, 51468], "temperature": 0.0, "avg_logprob": -0.18128985252933225, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.01969582587480545}, {"id": 488, "seek": 318812, "start": 3188.2, "end": 3195.64, "text": " our three three-dimensional point cloud input yep so we have um three parameters per point", "tokens": [50368, 527, 1045, 1045, 12, 18759, 935, 4588, 4846, 18633, 370, 321, 362, 1105, 1045, 9834, 680, 935, 50740], "temperature": 0.0, "avg_logprob": -0.15092352353609526, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.02227122336626053}, {"id": 489, "seek": 318812, "start": 3198.2799999999997, "end": 3206.68, "text": " now we do s in equilibrium maps and we uh sprinkle around ends and ones these are our building blocks", "tokens": [50872, 586, 321, 360, 262, 294, 15625, 11317, 293, 321, 2232, 24745, 926, 5314, 293, 2306, 613, 366, 527, 2390, 8474, 51292], "temperature": 0.0, "avg_logprob": -0.15092352353609526, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.02227122336626053}, {"id": 490, "seek": 318812, "start": 3208.44, "end": 3217.88, "text": " now note how crazily this reduces the number of parameters for a large n so here if we had", "tokens": [51380, 586, 3637, 577, 46348, 953, 341, 18081, 264, 1230, 295, 9834, 337, 257, 2416, 297, 370, 510, 498, 321, 632, 51852], "temperature": 0.0, "avg_logprob": -0.15092352353609526, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.02227122336626053}, {"id": 491, "seek": 321788, "start": 3217.88, "end": 3224.04, "text": " no assumption of s in equilibrium this would be an n squared space of parameters you know and n", "tokens": [50364, 572, 15302, 295, 262, 294, 15625, 341, 576, 312, 364, 297, 8889, 1901, 295, 9834, 291, 458, 293, 297, 50672], "temperature": 0.0, "avg_logprob": -0.09045832577873679, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0013234216021373868}, {"id": 492, "seek": 321788, "start": 3224.04, "end": 3230.44, "text": " could easily be a million or something but now because we want this to be s in equilibrium we've", "tokens": [50672, 727, 3612, 312, 257, 2459, 420, 746, 457, 586, 570, 321, 528, 341, 281, 312, 262, 294, 15625, 321, 600, 50992], "temperature": 0.0, "avg_logprob": -0.09045832577873679, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0013234216021373868}, {"id": 493, "seek": 321788, "start": 3230.44, "end": 3237.1600000000003, "text": " just got two two possibilities here we've got one possibility here we've got one possibility here", "tokens": [50992, 445, 658, 732, 732, 12178, 510, 321, 600, 658, 472, 7959, 510, 321, 600, 658, 472, 7959, 510, 51328], "temperature": 0.0, "avg_logprob": -0.09045832577873679, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0013234216021373868}, {"id": 494, "seek": 321788, "start": 3237.1600000000003, "end": 3245.2400000000002, "text": " we've got one possibility etc so this allows us to make enormous um networks involving you know", "tokens": [51328, 321, 600, 658, 472, 7959, 5183, 370, 341, 4045, 505, 281, 652, 11322, 1105, 9590, 17030, 291, 458, 51732], "temperature": 0.0, "avg_logprob": -0.09045832577873679, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0013234216021373868}, {"id": 495, "seek": 324524, "start": 3246.2, "end": 3251.72, "text": " hundreds of billion you know billion dimensional things um with few parameters", "tokens": [50412, 6779, 295, 5218, 291, 458, 5218, 18795, 721, 1105, 365, 1326, 9834, 50688], "temperature": 0.0, "avg_logprob": -0.16241878162730825, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0001940347719937563}, {"id": 496, "seek": 324524, "start": 3253.72, "end": 3261.72, "text": " okay and that is um deep set architecture and it's i think it's um state of the art in terms of", "tokens": [50788, 1392, 293, 300, 307, 1105, 2452, 992, 9482, 293, 309, 311, 741, 519, 309, 311, 1105, 1785, 295, 264, 1523, 294, 2115, 295, 51188], "temperature": 0.0, "avg_logprob": -0.16241878162730825, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0001940347719937563}, {"id": 497, "seek": 324524, "start": 3262.9199999999996, "end": 3269.9599999999996, "text": " point cloud prediction okay graph neural nets if there's no questions", "tokens": [51248, 935, 4588, 17630, 1392, 4295, 18161, 36170, 498, 456, 311, 572, 1651, 51600], "temperature": 0.0, "avg_logprob": -0.16241878162730825, "compression_ratio": 1.5741935483870968, "no_speech_prob": 0.0001940347719937563}, {"id": 498, "seek": 326996, "start": 3270.04, "end": 3278.12, "text": " uh graphs are everywhere in mathematics they come in many forms and variants so", "tokens": [50368, 2232, 24877, 366, 5315, 294, 18666, 436, 808, 294, 867, 6422, 293, 21669, 370, 50772], "temperature": 0.0, "avg_logprob": -0.12758861780166625, "compression_ratio": 2.24375, "no_speech_prob": 0.0019562833476811647}, {"id": 499, "seek": 326996, "start": 3280.76, "end": 3285.56, "text": " i just want you to keep in mind that graph here is a very um loose term it might be a", "tokens": [50904, 741, 445, 528, 291, 281, 1066, 294, 1575, 300, 4295, 510, 307, 257, 588, 1105, 9612, 1433, 309, 1062, 312, 257, 51144], "temperature": 0.0, "avg_logprob": -0.12758861780166625, "compression_ratio": 2.24375, "no_speech_prob": 0.0019562833476811647}, {"id": 500, "seek": 326996, "start": 3285.56, "end": 3290.76, "text": " directed graph it might be a digraph a directed graph the edges might be colored the vertices", "tokens": [51144, 12898, 4295, 309, 1062, 312, 257, 2528, 2662, 257, 12898, 4295, 264, 8819, 1062, 312, 14332, 264, 32053, 51404], "temperature": 0.0, "avg_logprob": -0.12758861780166625, "compression_ratio": 2.24375, "no_speech_prob": 0.0019562833476811647}, {"id": 501, "seek": 326996, "start": 3290.76, "end": 3295.7200000000003, "text": " might be colored the edges might be weighted the vertices might be weighted the vertices might have", "tokens": [51404, 1062, 312, 14332, 264, 8819, 1062, 312, 32807, 264, 32053, 1062, 312, 32807, 264, 32053, 1062, 362, 51652], "temperature": 0.0, "avg_logprob": -0.12758861780166625, "compression_ratio": 2.24375, "no_speech_prob": 0.0019562833476811647}, {"id": 502, "seek": 329572, "start": 3295.72, "end": 3300.6, "text": " 10 parameters associated to them we might be talking about hyper graphs so that's graphs where we", "tokens": [50364, 1266, 9834, 6615, 281, 552, 321, 1062, 312, 1417, 466, 9848, 24877, 370, 300, 311, 24877, 689, 321, 50608], "temperature": 0.0, "avg_logprob": -0.07159719280168123, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.003122167196124792}, {"id": 503, "seek": 329572, "start": 3300.6, "end": 3307.3199999999997, "text": " have like an edge need can connect more than what more than two two vertices etc so it's", "tokens": [50608, 362, 411, 364, 4691, 643, 393, 1745, 544, 813, 437, 544, 813, 732, 732, 32053, 5183, 370, 309, 311, 50944], "temperature": 0.0, "avg_logprob": -0.07159719280168123, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.003122167196124792}, {"id": 504, "seek": 329572, "start": 3307.3199999999997, "end": 3313.8799999999997, "text": " whole plethora of things called graphs and just want to emphasize that there's many ways so", "tokens": [50944, 1379, 499, 302, 7013, 295, 721, 1219, 24877, 293, 445, 528, 281, 16078, 300, 456, 311, 867, 2098, 370, 51272], "temperature": 0.0, "avg_logprob": -0.07159719280168123, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.003122167196124792}, {"id": 505, "seek": 329572, "start": 3313.8799999999997, "end": 3317.8799999999997, "text": " graphs are everywhere in mathematics but once you start thinking about them they're even more", "tokens": [51272, 24877, 366, 5315, 294, 18666, 457, 1564, 291, 722, 1953, 466, 552, 436, 434, 754, 544, 51472], "temperature": 0.0, "avg_logprob": -0.07159719280168123, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.003122167196124792}, {"id": 506, "seek": 329572, "start": 3317.8799999999997, "end": 3322.2799999999997, "text": " everywhere because there's a whole lot of stuff that wasn't obviously a graph initially and then", "tokens": [51472, 5315, 570, 456, 311, 257, 1379, 688, 295, 1507, 300, 2067, 380, 2745, 257, 4295, 9105, 293, 550, 51692], "temperature": 0.0, "avg_logprob": -0.07159719280168123, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.003122167196124792}, {"id": 507, "seek": 332228, "start": 3322.28, "end": 3327.5600000000004, "text": " you can make it a graph so examples of this uh you might say well you know graph theory is one", "tokens": [50364, 291, 393, 652, 309, 257, 4295, 370, 5110, 295, 341, 2232, 291, 1062, 584, 731, 291, 458, 4295, 5261, 307, 472, 50628], "temperature": 0.0, "avg_logprob": -0.05699274918743383, "compression_ratio": 1.9918367346938775, "no_speech_prob": 0.0035358243621885777}, {"id": 508, "seek": 332228, "start": 3327.5600000000004, "end": 3331.7200000000003, "text": " dimensional topology and i'm a sophisticated eight dimensional topologist and i only care about", "tokens": [50628, 18795, 1192, 1793, 293, 741, 478, 257, 16950, 3180, 18795, 1192, 9201, 293, 741, 787, 1127, 466, 50836], "temperature": 0.0, "avg_logprob": -0.05699274918743383, "compression_ratio": 1.9918367346938775, "no_speech_prob": 0.0035358243621885777}, {"id": 509, "seek": 332228, "start": 3331.7200000000003, "end": 3336.36, "text": " eight dimensional manifolds yeah but if you take a compact eight dimensional manifold you can choose", "tokens": [50836, 3180, 18795, 8173, 31518, 1338, 457, 498, 291, 747, 257, 14679, 3180, 18795, 47138, 291, 393, 2826, 51068], "temperature": 0.0, "avg_logprob": -0.05699274918743383, "compression_ratio": 1.9918367346938775, "no_speech_prob": 0.0035358243621885777}, {"id": 510, "seek": 332228, "start": 3336.36, "end": 3341.32, "text": " a point cloud on it and you'll get a graph and that graph tells you enormous amounts about that", "tokens": [51068, 257, 935, 4588, 322, 309, 293, 291, 603, 483, 257, 4295, 293, 300, 4295, 5112, 291, 11322, 11663, 466, 300, 51316], "temperature": 0.0, "avg_logprob": -0.05699274918743383, "compression_ratio": 1.9918367346938775, "no_speech_prob": 0.0035358243621885777}, {"id": 511, "seek": 332228, "start": 3341.32, "end": 3347.0, "text": " eight manifold if you have a simplicial complex you know for me like graphs are just one dimensional", "tokens": [51316, 3180, 47138, 498, 291, 362, 257, 1034, 4770, 831, 3997, 291, 458, 337, 385, 411, 24877, 366, 445, 472, 18795, 51600], "temperature": 0.0, "avg_logprob": -0.05699274918743383, "compression_ratio": 1.9918367346938775, "no_speech_prob": 0.0035358243621885777}, {"id": 512, "seek": 334700, "start": 3347.0, "end": 3352.44, "text": " simplicial complexes and so i said to petah oh we should be sophisticated and learn on", "tokens": [50364, 1034, 4770, 831, 43676, 293, 370, 741, 848, 281, 3817, 545, 1954, 321, 820, 312, 16950, 293, 1466, 322, 50636], "temperature": 0.0, "avg_logprob": -0.09728082021077473, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.0036453518550843}, {"id": 513, "seek": 334700, "start": 3352.44, "end": 3358.2, "text": " simplicial complexes and he said well a simplicial complex is just a graph truly you know here's", "tokens": [50636, 1034, 4770, 831, 43676, 293, 415, 848, 731, 257, 1034, 4770, 831, 3997, 307, 445, 257, 4295, 4908, 291, 458, 510, 311, 50924], "temperature": 0.0, "avg_logprob": -0.09728082021077473, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.0036453518550843}, {"id": 514, "seek": 334700, "start": 3358.2, "end": 3364.2, "text": " my triangle here are my edges and here are my vertices it's a colored a simplicial complex is", "tokens": [50924, 452, 13369, 510, 366, 452, 8819, 293, 510, 366, 452, 32053, 309, 311, 257, 14332, 257, 1034, 4770, 831, 3997, 307, 51224], "temperature": 0.0, "avg_logprob": -0.09728082021077473, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.0036453518550843}, {"id": 515, "seek": 334700, "start": 3364.2, "end": 3371.64, "text": " a colored vertex colored graph okay of a special form so this is another example this is from", "tokens": [51224, 257, 14332, 28162, 14332, 4295, 1392, 295, 257, 2121, 1254, 370, 341, 307, 1071, 1365, 341, 307, 490, 51596], "temperature": 0.0, "avg_logprob": -0.09728082021077473, "compression_ratio": 1.9734042553191489, "no_speech_prob": 0.0036453518550843}, {"id": 516, "seek": 337164, "start": 3371.64, "end": 3378.44, "text": " gaorg so if we have a a data set and it's somehow embedded in a space then we can get a natural", "tokens": [50364, 5959, 4646, 370, 498, 321, 362, 257, 257, 1412, 992, 293, 309, 311, 6063, 16741, 294, 257, 1901, 550, 321, 393, 483, 257, 3303, 50704], "temperature": 0.0, "avg_logprob": -0.09352497774011949, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.005218069534748793}, {"id": 517, "seek": 337164, "start": 3378.44, "end": 3385.72, "text": " metric graph out of it by looking at distances between vertices we might include the coordinates here", "tokens": [50704, 20678, 4295, 484, 295, 309, 538, 1237, 412, 22182, 1296, 32053, 321, 1062, 4090, 264, 21056, 510, 51068], "temperature": 0.0, "avg_logprob": -0.09352497774011949, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.005218069534748793}, {"id": 518, "seek": 337164, "start": 3385.72, "end": 3393.0, "text": " we might do some funny function applied to this these lengths etc okay so graphs are everywhere", "tokens": [51068, 321, 1062, 360, 512, 4074, 2445, 6456, 281, 341, 613, 26329, 5183, 1392, 370, 24877, 366, 5315, 51432], "temperature": 0.0, "avg_logprob": -0.09352497774011949, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.005218069534748793}, {"id": 519, "seek": 337164, "start": 3393.7999999999997, "end": 3401.08, "text": " and graph neural nets seem to be an incredibly powerful flexible way of dealing with um data so", "tokens": [51472, 293, 4295, 18161, 36170, 1643, 281, 312, 364, 6252, 4005, 11358, 636, 295, 6260, 365, 1105, 1412, 370, 51836], "temperature": 0.0, "avg_logprob": -0.09352497774011949, "compression_ratio": 1.6553191489361703, "no_speech_prob": 0.005218069534748793}, {"id": 520, "seek": 340164, "start": 3402.04, "end": 3406.68, "text": " i think the graph neural nets have kind of really genuine like c and n's the thing that's that we", "tokens": [50384, 741, 519, 264, 4295, 18161, 36170, 362, 733, 295, 534, 16699, 411, 269, 293, 297, 311, 264, 551, 300, 311, 300, 321, 50616], "temperature": 0.0, "avg_logprob": -0.10675491227044, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.00026935842470265925}, {"id": 521, "seek": 340164, "start": 3406.68, "end": 3411.64, "text": " stare at as mathematicians and think how could we make something like this that would help us in", "tokens": [50616, 22432, 412, 382, 32811, 2567, 293, 519, 577, 727, 321, 652, 746, 411, 341, 300, 576, 854, 505, 294, 50864], "temperature": 0.0, "avg_logprob": -0.10675491227044, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.00026935842470265925}, {"id": 522, "seek": 340164, "start": 3411.64, "end": 3415.8799999999997, "text": " mathematics but i think that graphs are actually a thing that will help us in mathematics all the", "tokens": [50864, 18666, 457, 741, 519, 300, 24877, 366, 767, 257, 551, 300, 486, 854, 505, 294, 18666, 439, 264, 51076], "temperature": 0.0, "avg_logprob": -0.10675491227044, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.00026935842470265925}, {"id": 523, "seek": 340164, "start": 3415.8799999999997, "end": 3425.0, "text": " time so that's very worthwhile thinking about so what the graph neural nets do an example we might", "tokens": [51076, 565, 370, 300, 311, 588, 28159, 1953, 466, 370, 437, 264, 4295, 18161, 36170, 360, 364, 1365, 321, 1062, 51532], "temperature": 0.0, "avg_logprob": -0.10675491227044, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.00026935842470265925}, {"id": 524, "seek": 340164, "start": 3425.72, "end": 3431.0, "text": " want to learn a function on graphs so an example would be a function which is learn planarity", "tokens": [51568, 528, 281, 1466, 257, 2445, 322, 24877, 370, 364, 1365, 576, 312, 257, 2445, 597, 307, 1466, 1393, 17409, 51832], "temperature": 0.0, "avg_logprob": -0.10675491227044, "compression_ratio": 2.0208333333333335, "no_speech_prob": 0.00026935842470265925}, {"id": 525, "seek": 343164, "start": 3431.96, "end": 3437.3199999999997, "text": " okay so this output's a positive number if it's planar a negative number if it's not planar so", "tokens": [50380, 1392, 370, 341, 5598, 311, 257, 3353, 1230, 498, 309, 311, 1393, 289, 257, 3671, 1230, 498, 309, 311, 406, 1393, 289, 370, 50648], "temperature": 0.0, "avg_logprob": -0.06742143034934997, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0002097863471135497}, {"id": 526, "seek": 343164, "start": 3437.3199999999997, "end": 3442.04, "text": " that would be a prediction task on graphs um we also might want to know for example the", "tokens": [50648, 300, 576, 312, 257, 17630, 5633, 322, 24877, 1105, 321, 611, 1062, 528, 281, 458, 337, 1365, 264, 50884], "temperature": 0.0, "avg_logprob": -0.06742143034934997, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0002097863471135497}, {"id": 527, "seek": 343164, "start": 3442.04, "end": 3448.8399999999997, "text": " Euler characteristic of the graph that would be another example of a prediction task another", "tokens": [50884, 462, 26318, 16282, 295, 264, 4295, 300, 576, 312, 1071, 1365, 295, 257, 17630, 5633, 1071, 51224], "temperature": 0.0, "avg_logprob": -0.06742143034934997, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0002097863471135497}, {"id": 528, "seek": 343164, "start": 3448.8399999999997, "end": 3454.92, "text": " important example kind of more like image like generalized image recognition is producing", "tokens": [51224, 1021, 1365, 733, 295, 544, 411, 3256, 411, 44498, 3256, 11150, 307, 10501, 51528], "temperature": 0.0, "avg_logprob": -0.06742143034934997, "compression_ratio": 1.8341708542713568, "no_speech_prob": 0.0002097863471135497}, {"id": 529, "seek": 345492, "start": 3455.88, "end": 3462.28, "text": " um some learning some function from functions on graphs to r so you might think that so", "tokens": [50412, 1105, 512, 2539, 512, 2445, 490, 6828, 322, 24877, 281, 367, 370, 291, 1062, 519, 300, 370, 50732], "temperature": 0.0, "avg_logprob": -0.11359098824587735, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.00689897732809186}, {"id": 530, "seek": 345492, "start": 3463.4, "end": 3469.88, "text": " no you you might repackage c and n's as being a grid and then an image is the same thing as a", "tokens": [50788, 572, 291, 291, 1062, 1085, 501, 609, 269, 293, 297, 311, 382, 885, 257, 10748, 293, 550, 364, 3256, 307, 264, 912, 551, 382, 257, 51112], "temperature": 0.0, "avg_logprob": -0.11359098824587735, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.00689897732809186}, {"id": 531, "seek": 345492, "start": 3469.88, "end": 3477.56, "text": " function on the vertices of this grid another very important thing is that um like there's many", "tokens": [51112, 2445, 322, 264, 32053, 295, 341, 10748, 1071, 588, 1021, 551, 307, 300, 1105, 411, 456, 311, 867, 51496], "temperature": 0.0, "avg_logprob": -0.11359098824587735, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.00689897732809186}, {"id": 532, "seek": 345492, "start": 3477.56, "end": 3482.44, "text": " many incredibly interesting for example embedding problems of graphs so you give me a graph and I", "tokens": [51496, 867, 6252, 1880, 337, 1365, 12240, 3584, 2740, 295, 24877, 370, 291, 976, 385, 257, 4295, 293, 286, 51740], "temperature": 0.0, "avg_logprob": -0.11359098824587735, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.00689897732809186}, {"id": 533, "seek": 348244, "start": 3482.44, "end": 3488.44, "text": " want to put it in some space in an interesting way um and one way of doing that would be to provide", "tokens": [50364, 528, 281, 829, 309, 294, 512, 1901, 294, 364, 1880, 636, 1105, 293, 472, 636, 295, 884, 300, 576, 312, 281, 2893, 50664], "temperature": 0.0, "avg_logprob": -0.05290534703627876, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0010159043595194817}, {"id": 534, "seek": 348244, "start": 3488.44, "end": 3493.8, "text": " coordinates of where I want to put the vertices of that graph and so that would be an example of", "tokens": [50664, 21056, 295, 689, 286, 528, 281, 829, 264, 32053, 295, 300, 4295, 293, 370, 300, 576, 312, 364, 1365, 295, 50932], "temperature": 0.0, "avg_logprob": -0.05290534703627876, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0010159043595194817}, {"id": 535, "seek": 348244, "start": 3493.8, "end": 3503.0, "text": " learning a function from graphs to functions on the vertices of a graph uh so I guess the takeaway", "tokens": [50932, 2539, 257, 2445, 490, 24877, 281, 6828, 322, 264, 32053, 295, 257, 4295, 2232, 370, 286, 2041, 264, 30681, 51392], "temperature": 0.0, "avg_logprob": -0.05290534703627876, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0010159043595194817}, {"id": 536, "seek": 348244, "start": 3503.0, "end": 3508.84, "text": " from this is that anything to do with graphs graph neural nets uh useful for as long as it's not", "tokens": [51392, 490, 341, 307, 300, 1340, 281, 360, 365, 24877, 4295, 18161, 36170, 2232, 4420, 337, 382, 938, 382, 309, 311, 406, 51684], "temperature": 0.0, "avg_logprob": -0.05290534703627876, "compression_ratio": 1.8148148148148149, "no_speech_prob": 0.0010159043595194817}, {"id": 537, "seek": 350884, "start": 3508.84, "end": 3513.1600000000003, "text": " like an NP hard problem on graphs of which there are plenty yeah so graph neural nets aren't going", "tokens": [50364, 411, 364, 38611, 1152, 1154, 322, 24877, 295, 597, 456, 366, 7140, 1338, 370, 4295, 18161, 36170, 3212, 380, 516, 50580], "temperature": 0.0, "avg_logprob": -0.04923065430527433, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.003072339342907071}, {"id": 538, "seek": 350884, "start": 3513.1600000000003, "end": 3516.44, "text": " to help you solve something like is there a Hamiltonian circuit or something like that", "tokens": [50580, 281, 854, 291, 5039, 746, 411, 307, 456, 257, 18484, 952, 9048, 420, 746, 411, 300, 50744], "temperature": 0.0, "avg_logprob": -0.04923065430527433, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.003072339342907071}, {"id": 539, "seek": 350884, "start": 3518.2000000000003, "end": 3523.32, "text": " so what's the basic idea so imagine that I give you a graph and you want to learn on it", "tokens": [50832, 370, 437, 311, 264, 3875, 1558, 370, 3811, 300, 286, 976, 291, 257, 4295, 293, 291, 528, 281, 1466, 322, 309, 51088], "temperature": 0.0, "avg_logprob": -0.04923065430527433, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.003072339342907071}, {"id": 540, "seek": 350884, "start": 3525.0, "end": 3529.56, "text": " it's enormously difficult as far as I can work out to work out the automorphism of a group of a", "tokens": [51172, 309, 311, 39669, 2252, 382, 1400, 382, 286, 393, 589, 484, 281, 589, 484, 264, 3553, 18191, 1434, 295, 257, 1594, 295, 257, 51400], "temperature": 0.0, "avg_logprob": -0.04923065430527433, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.003072339342907071}, {"id": 541, "seek": 350884, "start": 3529.56, "end": 3534.6000000000004, "text": " graph so this is something that people spend many many years thinking about from an algorithmic", "tokens": [51400, 4295, 370, 341, 307, 746, 300, 561, 3496, 867, 867, 924, 1953, 466, 490, 364, 9284, 299, 51652], "temperature": 0.0, "avg_logprob": -0.04923065430527433, "compression_ratio": 1.7481203007518797, "no_speech_prob": 0.003072339342907071}, {"id": 542, "seek": 353460, "start": 3534.6, "end": 3540.12, "text": " point of view and so I might not know what global symmetries are present so what I was talking about", "tokens": [50364, 935, 295, 1910, 293, 370, 286, 1062, 406, 458, 437, 4338, 14232, 302, 2244, 366, 1974, 370, 437, 286, 390, 1417, 466, 50640], "temperature": 0.0, "avg_logprob": -0.08228279698279596, "compression_ratio": 1.5625, "no_speech_prob": 0.006784921977669001}, {"id": 543, "seek": 353460, "start": 3540.12, "end": 3549.08, "text": " before does not apply um or that just like most graphs have no symmetry whatsoever um or you might", "tokens": [50640, 949, 775, 406, 3079, 1105, 420, 300, 445, 411, 881, 24877, 362, 572, 25440, 17076, 1105, 420, 291, 1062, 51088], "temperature": 0.0, "avg_logprob": -0.08228279698279596, "compression_ratio": 1.5625, "no_speech_prob": 0.006784921977669001}, {"id": 544, "seek": 353460, "start": 3553.3199999999997, "end": 3557.64, "text": " so Gaston is asking what do you mean by hard um so", "tokens": [51300, 370, 31988, 266, 307, 3365, 437, 360, 291, 914, 538, 1152, 1105, 370, 51516], "temperature": 0.0, "avg_logprob": -0.08228279698279596, "compression_ratio": 1.5625, "no_speech_prob": 0.006784921977669001}, {"id": 545, "seek": 355764, "start": 3558.2, "end": 3568.2, "text": " that I mean yeah maybe maybe like NP or something like that but I just want I like in my mind there's", "tokens": [50392, 300, 286, 914, 1338, 1310, 1310, 411, 38611, 420, 746, 411, 300, 457, 286, 445, 528, 286, 411, 294, 452, 1575, 456, 311, 50892], "temperature": 0.0, "avg_logprob": -0.0912844888095198, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0012248574057593942}, {"id": 546, "seek": 355764, "start": 3568.2, "end": 3575.56, "text": " there's stuff on graphs which is useful and maybe not so crazily difficult like um like", "tokens": [50892, 456, 311, 1507, 322, 24877, 597, 307, 4420, 293, 1310, 406, 370, 46348, 953, 2252, 411, 1105, 411, 51260], "temperature": 0.0, "avg_logprob": -0.0912844888095198, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0012248574057593942}, {"id": 547, "seek": 355764, "start": 3575.56, "end": 3580.3599999999997, "text": " embedding your graph in a nice way or something like that and then there's a whole lot of like", "tokens": [51260, 12240, 3584, 428, 4295, 294, 257, 1481, 636, 420, 746, 411, 300, 293, 550, 456, 311, 257, 1379, 688, 295, 411, 51500], "temperature": 0.0, "avg_logprob": -0.0912844888095198, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0012248574057593942}, {"id": 548, "seek": 355764, "start": 3580.3599999999997, "end": 3585.56, "text": " seemingly innocent problems on graphs that are extremely hard um like embedding a graph", "tokens": [51500, 18709, 13171, 2740, 322, 24877, 300, 366, 4664, 1152, 1105, 411, 12240, 3584, 257, 4295, 51760], "temperature": 0.0, "avg_logprob": -0.0912844888095198, "compression_ratio": 1.832512315270936, "no_speech_prob": 0.0012248574057593942}, {"id": 549, "seek": 358556, "start": 3585.56, "end": 3590.12, "text": " improvably the nice the best way or something like that or finding a Hamiltonian circuit or", "tokens": [50364, 2530, 85, 1188, 264, 1481, 264, 1151, 636, 420, 746, 411, 300, 420, 5006, 257, 18484, 952, 9048, 420, 50592], "temperature": 0.0, "avg_logprob": -0.10282293598303634, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.00030032882932573557}, {"id": 550, "seek": 358556, "start": 3590.12, "end": 3596.6, "text": " stuff like that okay so in graph land it's easy to wander into an intractable problem", "tokens": [50592, 1507, 411, 300, 1392, 370, 294, 4295, 2117, 309, 311, 1858, 281, 27541, 666, 364, 560, 1897, 712, 1154, 50916], "temperature": 0.0, "avg_logprob": -0.10282293598303634, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.00030032882932573557}, {"id": 551, "seek": 358556, "start": 3598.2, "end": 3604.92, "text": " um but there's also a whole lot of useful stuff that can be done so so there's plenty of local", "tokens": [50996, 1105, 457, 456, 311, 611, 257, 1379, 688, 295, 4420, 1507, 300, 393, 312, 1096, 370, 370, 456, 311, 7140, 295, 2654, 51332], "temperature": 0.0, "avg_logprob": -0.10282293598303634, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.00030032882932573557}, {"id": 552, "seek": 358556, "start": 3604.92, "end": 3612.12, "text": " symmetry in graphs so around every vertex we have a symmetric group of symmetry and also we have a", "tokens": [51332, 25440, 294, 24877, 370, 926, 633, 28162, 321, 362, 257, 32330, 1594, 295, 25440, 293, 611, 321, 362, 257, 51692], "temperature": 0.0, "avg_logprob": -0.10282293598303634, "compression_ratio": 1.7417840375586855, "no_speech_prob": 0.00030032882932573557}, {"id": 553, "seek": 361212, "start": 3612.12, "end": 3618.6, "text": " metric so you can imagine processes which are symmetric and kind of diffuse on the graph and", "tokens": [50364, 20678, 370, 291, 393, 3811, 7555, 597, 366, 32330, 293, 733, 295, 42165, 322, 264, 4295, 293, 50688], "temperature": 0.0, "avg_logprob": -0.11944163762606107, "compression_ratio": 1.4896551724137932, "no_speech_prob": 0.0029332181438803673}, {"id": 554, "seek": 361212, "start": 3618.6, "end": 3621.88, "text": " that's what a graph neural net is so I'll quickly go over the architecture", "tokens": [50688, 300, 311, 437, 257, 4295, 18161, 2533, 307, 370, 286, 603, 2661, 352, 670, 264, 9482, 50852], "temperature": 0.0, "avg_logprob": -0.11944163762606107, "compression_ratio": 1.4896551724137932, "no_speech_prob": 0.0029332181438803673}, {"id": 555, "seek": 361212, "start": 3624.8399999999997, "end": 3630.04, "text": " so this is an important slide so here's my graph", "tokens": [51000, 370, 341, 307, 364, 1021, 4137, 370, 510, 311, 452, 4295, 51260], "temperature": 0.0, "avg_logprob": -0.11944163762606107, "compression_ratio": 1.4896551724137932, "no_speech_prob": 0.0029332181438803673}, {"id": 556, "seek": 363004, "start": 3630.84, "end": 3641.08, "text": " and as part of my architecture I fix n1", "tokens": [50404, 293, 382, 644, 295, 452, 9482, 286, 3191, 297, 16, 50916], "temperature": 0.0, "avg_logprob": -0.1068557585988726, "compression_ratio": 1.477124183006536, "no_speech_prob": 0.004535963758826256}, {"id": 557, "seek": 363004, "start": 3642.36, "end": 3649.88, "text": " n2 n3 and of course I'm just telling you one possible variant of like a thousand different", "tokens": [50980, 297, 17, 297, 18, 293, 295, 1164, 286, 478, 445, 3585, 291, 472, 1944, 17501, 295, 411, 257, 4714, 819, 51356], "temperature": 0.0, "avg_logprob": -0.1068557585988726, "compression_ratio": 1.477124183006536, "no_speech_prob": 0.004535963758826256}, {"id": 558, "seek": 363004, "start": 3649.88, "end": 3655.88, "text": " possibilities of building graph neural net but once you've seen one of them then the other ones", "tokens": [51356, 12178, 295, 2390, 4295, 18161, 2533, 457, 1564, 291, 600, 1612, 472, 295, 552, 550, 264, 661, 2306, 51656], "temperature": 0.0, "avg_logprob": -0.1068557585988726, "compression_ratio": 1.477124183006536, "no_speech_prob": 0.004535963758826256}, {"id": 559, "seek": 365588, "start": 3655.96, "end": 3663.1600000000003, "text": " make a lot more sense so we fix these n1 and then what we do is each of our layers is a sum", "tokens": [50368, 652, 257, 688, 544, 2020, 370, 321, 3191, 613, 297, 16, 293, 550, 437, 321, 360, 307, 1184, 295, 527, 7914, 307, 257, 2408, 50728], "temperature": 0.0, "avg_logprob": -0.06642951567967732, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015710339648649096}, {"id": 560, "seek": 365588, "start": 3663.1600000000003, "end": 3670.52, "text": " over the vertices of that particular rn1 okay so you know in a vanilla neural net we just fix", "tokens": [50728, 670, 264, 32053, 295, 300, 1729, 367, 77, 16, 1392, 370, 291, 458, 294, 257, 17528, 18161, 2533, 321, 445, 3191, 51096], "temperature": 0.0, "avg_logprob": -0.06642951567967732, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015710339648649096}, {"id": 561, "seek": 365588, "start": 3670.52, "end": 3679.48, "text": " dimensions here we fix dimensions at every vertex so that's this and so in this particular case my", "tokens": [51096, 12819, 510, 321, 3191, 12819, 412, 633, 28162, 370, 300, 311, 341, 293, 370, 294, 341, 1729, 1389, 452, 51544], "temperature": 0.0, "avg_logprob": -0.06642951567967732, "compression_ratio": 1.6705882352941177, "no_speech_prob": 0.0015710339648649096}, {"id": 562, "seek": 367948, "start": 3679.8, "end": 3683.64, "text": " my neural net looks like this so I have", "tokens": [50380, 452, 18161, 2533, 1542, 411, 341, 370, 286, 362, 50572], "temperature": 0.0, "avg_logprob": -0.13710312332425797, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0017534950748085976}, {"id": 563, "seek": 367948, "start": 3687.2400000000002, "end": 3691.96, "text": " three layers so here I have some linear map here I have a relu here I have a linear map a relu", "tokens": [50752, 1045, 7914, 370, 510, 286, 362, 512, 8213, 4471, 510, 286, 362, 257, 1039, 84, 510, 286, 362, 257, 8213, 4471, 257, 1039, 84, 50988], "temperature": 0.0, "avg_logprob": -0.13710312332425797, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0017534950748085976}, {"id": 564, "seek": 367948, "start": 3693.88, "end": 3702.6, "text": " and then a fully connected layer okay so what do I do basically I train", "tokens": [51084, 293, 550, 257, 4498, 4582, 4583, 1392, 370, 437, 360, 286, 360, 1936, 286, 3847, 51520], "temperature": 0.0, "avg_logprob": -0.13710312332425797, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.0017534950748085976}, {"id": 565, "seek": 370260, "start": 3703.56, "end": 3717.96, "text": " self and neighbor maps so here's the formula down here so I'm telling you what phi x of v so here's", "tokens": [50412, 2698, 293, 5987, 11317, 370, 510, 311, 264, 8513, 760, 510, 370, 286, 478, 3585, 291, 437, 13107, 2031, 295, 371, 370, 510, 311, 51132], "temperature": 0.0, "avg_logprob": -0.1012965131689001, "compression_ratio": 1.4701492537313432, "no_speech_prob": 0.005638094153255224}, {"id": 566, "seek": 370260, "start": 3717.96, "end": 3728.92, "text": " my my layer which is phi and I wanted to find you this map and in order to do that I can tell you", "tokens": [51132, 452, 452, 4583, 597, 307, 13107, 293, 286, 1415, 281, 915, 291, 341, 4471, 293, 294, 1668, 281, 360, 300, 286, 393, 980, 291, 51680], "temperature": 0.0, "avg_logprob": -0.1012965131689001, "compression_ratio": 1.4701492537313432, "no_speech_prob": 0.005638094153255224}, {"id": 567, "seek": 372892, "start": 3728.92, "end": 3736.52, "text": " this map evaluated at a particular vertex so that this might be vertex v and what I'm saying in", "tokens": [50364, 341, 4471, 25509, 412, 257, 1729, 28162, 370, 300, 341, 1062, 312, 28162, 371, 293, 437, 286, 478, 1566, 294, 50744], "temperature": 0.0, "avg_logprob": -0.05080716289691071, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.004535905085504055}, {"id": 568, "seek": 372892, "start": 3736.52, "end": 3745.0, "text": " order to get that answer what you do is you take this self map times whatever I've got here plus", "tokens": [50744, 1668, 281, 483, 300, 1867, 437, 291, 360, 307, 291, 747, 341, 2698, 4471, 1413, 2035, 286, 600, 658, 510, 1804, 51168], "temperature": 0.0, "avg_logprob": -0.05080716289691071, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.004535905085504055}, {"id": 569, "seek": 372892, "start": 3745.0, "end": 3752.76, "text": " all of these neighbor maps so roughly speaking in my second layer something here has a term that", "tokens": [51168, 439, 295, 613, 5987, 11317, 370, 9810, 4124, 294, 452, 1150, 4583, 746, 510, 575, 257, 1433, 300, 51556], "temperature": 0.0, "avg_logprob": -0.05080716289691071, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.004535905085504055}, {"id": 570, "seek": 375276, "start": 3752.76, "end": 3759.7200000000003, "text": " comes from here together with terms that come from the three neighbors so it's a very natural", "tokens": [50364, 1487, 490, 510, 1214, 365, 2115, 300, 808, 490, 264, 1045, 12512, 370, 309, 311, 257, 588, 3303, 50712], "temperature": 0.0, "avg_logprob": -0.12050707747296589, "compression_ratio": 1.425, "no_speech_prob": 0.009989170357584953}, {"id": 571, "seek": 375276, "start": 3760.44, "end": 3773.4, "text": " it's called a message pass and as usual si is something like is affine linear", "tokens": [50748, 309, 311, 1219, 257, 3636, 1320, 293, 382, 7713, 1511, 307, 746, 411, 307, 2096, 533, 8213, 51396], "temperature": 0.0, "avg_logprob": -0.12050707747296589, "compression_ratio": 1.425, "no_speech_prob": 0.009989170357584953}, {"id": 572, "seek": 377340, "start": 3774.2000000000003, "end": 3783.4, "text": " okay so s1 is an n1 times n2 matrix", "tokens": [50404, 1392, 370, 262, 16, 307, 364, 297, 16, 1413, 297, 17, 8141, 50864], "temperature": 0.0, "avg_logprob": -0.14841230525526888, "compression_ratio": 1.2828282828282829, "no_speech_prob": 0.004817872773855925}, {"id": 573, "seek": 377340, "start": 3786.44, "end": 3787.7200000000003, "text": " plus an n2 vector", "tokens": [51016, 1804, 364, 297, 17, 8062, 51080], "temperature": 0.0, "avg_logprob": -0.14841230525526888, "compression_ratio": 1.2828282828282829, "no_speech_prob": 0.004817872773855925}, {"id": 574, "seek": 377340, "start": 3790.44, "end": 3796.6, "text": " okay so each of these so this is yes thank you Brian this should be an s2", "tokens": [51216, 1392, 370, 1184, 295, 613, 370, 341, 307, 2086, 1309, 291, 10765, 341, 820, 312, 364, 262, 17, 51524], "temperature": 0.0, "avg_logprob": -0.14841230525526888, "compression_ratio": 1.2828282828282829, "no_speech_prob": 0.004817872773855925}, {"id": 575, "seek": 379660, "start": 3796.6, "end": 3807.48, "text": " yeah so that's very important that so this is an another example of an inductive prior", "tokens": [50364, 1338, 370, 300, 311, 588, 1021, 300, 370, 341, 307, 364, 1071, 1365, 295, 364, 31612, 488, 4059, 50908], "temperature": 0.0, "avg_logprob": -0.16050063862520106, "compression_ratio": 1.6496815286624205, "no_speech_prob": 0.004065266344696283}, {"id": 576, "seek": 379660, "start": 3808.12, "end": 3814.04, "text": " sorry inductive bias or a prior so what we're saying is that we want these n1", "tokens": [50940, 2597, 31612, 488, 12577, 420, 257, 4059, 370, 437, 321, 434, 1566, 307, 300, 321, 528, 613, 297, 16, 51236], "temperature": 0.0, "avg_logprob": -0.16050063862520106, "compression_ratio": 1.6496815286624205, "no_speech_prob": 0.004065266344696283}, {"id": 577, "seek": 379660, "start": 3815.48, "end": 3822.6, "text": " these n1s sorry Stefan asked should all these n1s be the same and the answer is in general yes", "tokens": [51308, 613, 297, 16, 82, 2597, 32158, 2351, 820, 439, 613, 297, 16, 82, 312, 264, 912, 293, 264, 1867, 307, 294, 2674, 2086, 51664], "temperature": 0.0, "avg_logprob": -0.16050063862520106, "compression_ratio": 1.6496815286624205, "no_speech_prob": 0.004065266344696283}, {"id": 578, "seek": 382260, "start": 3823.48, "end": 3830.36, "text": " so we want for example the n1s that talk to this guy from here and from here", "tokens": [50408, 370, 321, 528, 337, 1365, 264, 297, 16, 82, 300, 751, 281, 341, 2146, 490, 510, 293, 490, 510, 50752], "temperature": 0.0, "avg_logprob": -0.06238287466543692, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.004748619627207518}, {"id": 579, "seek": 382260, "start": 3831.16, "end": 3838.2799999999997, "text": " to be the same n1s that talk to this guy from here here okay so the n1s are the same so", "tokens": [50792, 281, 312, 264, 912, 297, 16, 82, 300, 751, 281, 341, 2146, 490, 510, 510, 1392, 370, 264, 297, 16, 82, 366, 264, 912, 370, 51148], "temperature": 0.0, "avg_logprob": -0.06238287466543692, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.004748619627207518}, {"id": 580, "seek": 382260, "start": 3838.2799999999997, "end": 3840.04, "text": " if you imagine this matrix here", "tokens": [51148, 498, 291, 3811, 341, 8141, 510, 51236], "temperature": 0.0, "avg_logprob": -0.06238287466543692, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.004748619627207518}, {"id": 581, "seek": 382260, "start": 3843.08, "end": 3849.72, "text": " it looks like something like s1 s1 s1 s1 down the diagonal and then n1s", "tokens": [51388, 309, 1542, 411, 746, 411, 262, 16, 262, 16, 262, 16, 262, 16, 760, 264, 21539, 293, 550, 297, 16, 82, 51720], "temperature": 0.0, "avg_logprob": -0.06238287466543692, "compression_ratio": 1.9142857142857144, "no_speech_prob": 0.004748619627207518}, {"id": 582, "seek": 384972, "start": 3849.9599999999996, "end": 3855.3199999999997, "text": " in off diagonal places given by the adjacency matrix", "tokens": [50376, 294, 766, 21539, 3190, 2212, 538, 264, 22940, 3020, 8141, 50644], "temperature": 0.0, "avg_logprob": -0.08542163372039795, "compression_ratio": 1.703125, "no_speech_prob": 0.0008680403116159141}, {"id": 583, "seek": 384972, "start": 3857.3999999999996, "end": 3861.48, "text": " you know something like this so inside this space of like", "tokens": [50748, 291, 458, 746, 411, 341, 370, 1854, 341, 1901, 295, 411, 50952], "temperature": 0.0, "avg_logprob": -0.08542163372039795, "compression_ratio": 1.703125, "no_speech_prob": 0.0008680403116159141}, {"id": 584, "seek": 384972, "start": 3862.2799999999997, "end": 3869.3999999999996, "text": " n1 times vertices times n2 times vertices so this is what my big matrix would look like", "tokens": [50992, 297, 16, 1413, 32053, 1413, 297, 17, 1413, 32053, 370, 341, 307, 437, 452, 955, 8141, 576, 574, 411, 51348], "temperature": 0.0, "avg_logprob": -0.08542163372039795, "compression_ratio": 1.703125, "no_speech_prob": 0.0008680403116159141}, {"id": 585, "seek": 384972, "start": 3869.3999999999996, "end": 3873.8799999999997, "text": " I'm saying like it should be block diagonal and a whole lot of blocks should be the same", "tokens": [51348, 286, 478, 1566, 411, 309, 820, 312, 3461, 21539, 293, 257, 1379, 688, 295, 8474, 820, 312, 264, 912, 51572], "temperature": 0.0, "avg_logprob": -0.08542163372039795, "compression_ratio": 1.703125, "no_speech_prob": 0.0008680403116159141}, {"id": 586, "seek": 384972, "start": 3873.8799999999997, "end": 3877.24, "text": " so it's a very strong inductive bias to", "tokens": [51572, 370, 309, 311, 257, 588, 2068, 31612, 488, 12577, 281, 51740], "temperature": 0.0, "avg_logprob": -0.08542163372039795, "compression_ratio": 1.703125, "no_speech_prob": 0.0008680403116159141}, {"id": 587, "seek": 387972, "start": 3879.8799999999997, "end": 3888.9199999999996, "text": " assume now if I'm honest you know we might have seven like let's say two different colors on", "tokens": [50372, 6552, 586, 498, 286, 478, 3245, 291, 458, 321, 1062, 362, 3407, 411, 718, 311, 584, 732, 819, 4577, 322, 50824], "temperature": 0.0, "avg_logprob": -0.0870340381349836, "compression_ratio": 1.8366336633663367, "no_speech_prob": 0.0018086389172822237}, {"id": 588, "seek": 387972, "start": 3888.9199999999996, "end": 3896.12, "text": " our vertices and then we would train um neighbor maps that preserve the color of vertices neighbor", "tokens": [50824, 527, 32053, 293, 550, 321, 576, 3847, 1105, 5987, 11317, 300, 15665, 264, 2017, 295, 32053, 5987, 51184], "temperature": 0.0, "avg_logprob": -0.0870340381349836, "compression_ratio": 1.8366336633663367, "no_speech_prob": 0.0018086389172822237}, {"id": 589, "seek": 387972, "start": 3896.12, "end": 3899.8799999999997, "text": " maps that change the color from red to blue neighbor maps that change the color from blue", "tokens": [51184, 11317, 300, 1319, 264, 2017, 490, 2182, 281, 3344, 5987, 11317, 300, 1319, 264, 2017, 490, 3344, 51372], "temperature": 0.0, "avg_logprob": -0.0870340381349836, "compression_ratio": 1.8366336633663367, "no_speech_prob": 0.0018086389172822237}, {"id": 590, "seek": 387972, "start": 3899.8799999999997, "end": 3905.9599999999996, "text": " to red etc so there's a zillion variants but in the basic vanilla version of a neural net", "tokens": [51372, 281, 2182, 5183, 370, 456, 311, 257, 710, 11836, 21669, 457, 294, 264, 3875, 17528, 3037, 295, 257, 18161, 2533, 51676], "temperature": 0.0, "avg_logprob": -0.0870340381349836, "compression_ratio": 1.8366336633663367, "no_speech_prob": 0.0018086389172822237}, {"id": 591, "seek": 390596, "start": 3905.96, "end": 3910.68, "text": " we assume all the n1s are the same and all the s1s are the same so this one's", "tokens": [50364, 321, 6552, 439, 264, 297, 16, 82, 366, 264, 912, 293, 439, 264, 262, 16, 82, 366, 264, 912, 370, 341, 472, 311, 50600], "temperature": 0.0, "avg_logprob": -0.13606730593910701, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0008290670812129974}, {"id": 592, "seek": 390596, "start": 3911.32, "end": 3913.8, "text": " so I'll give the diagonal term of this s1", "tokens": [50632, 370, 286, 603, 976, 264, 21539, 1433, 295, 341, 262, 16, 50756], "temperature": 0.0, "avg_logprob": -0.13606730593910701, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0008290670812129974}, {"id": 593, "seek": 390596, "start": 3923.4, "end": 3926.36, "text": " so it's a very complicated slide but it's a simple idea I think", "tokens": [51236, 370, 309, 311, 257, 588, 6179, 4137, 457, 309, 311, 257, 2199, 1558, 286, 519, 51384], "temperature": 0.0, "avg_logprob": -0.13606730593910701, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0008290670812129974}, {"id": 594, "seek": 390596, "start": 3927.48, "end": 3934.2, "text": " so we do that for a number of times and then we evaluate or in the situation where we're", "tokens": [51440, 370, 321, 360, 300, 337, 257, 1230, 295, 1413, 293, 550, 321, 13059, 420, 294, 264, 2590, 689, 321, 434, 51776], "temperature": 0.0, "avg_logprob": -0.13606730593910701, "compression_ratio": 1.6585365853658536, "no_speech_prob": 0.0008290670812129974}, {"id": 595, "seek": 393420, "start": 3934.2, "end": 3937.8799999999997, "text": " trying to learn for example and embedding or something like that we wouldn't do the final", "tokens": [50364, 1382, 281, 1466, 337, 1365, 293, 12240, 3584, 420, 746, 411, 300, 321, 2759, 380, 360, 264, 2572, 50548], "temperature": 0.0, "avg_logprob": -0.076486417225429, "compression_ratio": 1.928, "no_speech_prob": 0.0011509922333061695}, {"id": 596, "seek": 393420, "start": 3937.8799999999997, "end": 3944.4399999999996, "text": " layer we've got some coordinates on our vertices and we're happy another classic example of a task", "tokens": [50548, 4583, 321, 600, 658, 512, 21056, 322, 527, 32053, 293, 321, 434, 2055, 1071, 7230, 1365, 295, 257, 5633, 50876], "temperature": 0.0, "avg_logprob": -0.076486417225429, "compression_ratio": 1.928, "no_speech_prob": 0.0011509922333061695}, {"id": 597, "seek": 393420, "start": 3944.4399999999996, "end": 3949.72, "text": " might be you want to divide your vertices into two classes and so then you would you'd do all your", "tokens": [50876, 1062, 312, 291, 528, 281, 9845, 428, 32053, 666, 732, 5359, 293, 370, 550, 291, 576, 291, 1116, 360, 439, 428, 51140], "temperature": 0.0, "avg_logprob": -0.076486417225429, "compression_ratio": 1.928, "no_speech_prob": 0.0011509922333061695}, {"id": 598, "seek": 393420, "start": 3949.72, "end": 3953.7999999999997, "text": " layers and then you would say at the end uh this is a real number and then you would softmax that", "tokens": [51140, 7914, 293, 550, 291, 576, 584, 412, 264, 917, 2232, 341, 307, 257, 957, 1230, 293, 550, 291, 576, 2787, 41167, 300, 51344], "temperature": 0.0, "avg_logprob": -0.076486417225429, "compression_ratio": 1.928, "no_speech_prob": 0.0011509922333061695}, {"id": 599, "seek": 393420, "start": 3953.7999999999997, "end": 3960.12, "text": " and then that would be the probability that your vertex is in or out of this class okay and also", "tokens": [51344, 293, 550, 300, 576, 312, 264, 8482, 300, 428, 28162, 307, 294, 420, 484, 295, 341, 1508, 1392, 293, 611, 51660], "temperature": 0.0, "avg_logprob": -0.076486417225429, "compression_ratio": 1.928, "no_speech_prob": 0.0011509922333061695}, {"id": 600, "seek": 396012, "start": 3960.12, "end": 3965.72, "text": " there's you know a million variants often the neighbor term is weighted by one over the degree", "tokens": [50364, 456, 311, 291, 458, 257, 2459, 21669, 2049, 264, 5987, 1433, 307, 32807, 538, 472, 670, 264, 4314, 50644], "temperature": 0.0, "avg_logprob": -0.0619603157043457, "compression_ratio": 1.539877300613497, "no_speech_prob": 6.603389192605391e-05}, {"id": 601, "seek": 396012, "start": 3965.72, "end": 3972.44, "text": " of the vertex okay that's what I set up there it's affine maps", "tokens": [50644, 295, 264, 28162, 1392, 300, 311, 437, 286, 992, 493, 456, 309, 311, 2096, 533, 11317, 50980], "temperature": 0.0, "avg_logprob": -0.0619603157043457, "compression_ratio": 1.539877300613497, "no_speech_prob": 6.603389192605391e-05}, {"id": 602, "seek": 396012, "start": 3977.24, "end": 3981.72, "text": " okay so that's the architecture and we'll have some fun playing with this architecture in the", "tokens": [51220, 1392, 370, 300, 311, 264, 9482, 293, 321, 603, 362, 512, 1019, 2433, 365, 341, 9482, 294, 264, 51444], "temperature": 0.0, "avg_logprob": -0.0619603157043457, "compression_ratio": 1.539877300613497, "no_speech_prob": 6.603389192605391e-05}, {"id": 603, "seek": 398172, "start": 3981.72, "end": 3991.16, "text": " colab tomorrow so many variants are possible for digraphs digraphs you might think that you only", "tokens": [50364, 1173, 455, 4153, 370, 867, 21669, 366, 1944, 337, 2528, 2662, 82, 2528, 2662, 82, 291, 1062, 519, 300, 291, 787, 50836], "temperature": 0.0, "avg_logprob": -0.08206351210431355, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005909123457968235}, {"id": 604, "seek": 398172, "start": 3991.16, "end": 3995.48, "text": " train a forward map but generally you don't you train a forward map and a back with map", "tokens": [50836, 3847, 257, 2128, 4471, 457, 5101, 291, 500, 380, 291, 3847, 257, 2128, 4471, 293, 257, 646, 365, 4471, 51052], "temperature": 0.0, "avg_logprob": -0.08206351210431355, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005909123457968235}, {"id": 605, "seek": 398172, "start": 3996.68, "end": 4003.16, "text": " so back forward and backwards neighbor maps if graph has edge coloring you can train message", "tokens": [51112, 370, 646, 2128, 293, 12204, 5987, 11317, 498, 4295, 575, 4691, 23198, 291, 393, 3847, 3636, 51436], "temperature": 0.0, "avg_logprob": -0.08206351210431355, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005909123457968235}, {"id": 606, "seek": 398172, "start": 4003.16, "end": 4009.48, "text": " passing for every color of the edge vertex coloring similarly you might also have a couple", "tokens": [51436, 8437, 337, 633, 2017, 295, 264, 4691, 28162, 23198, 14138, 291, 1062, 611, 362, 257, 1916, 51752], "temperature": 0.0, "avg_logprob": -0.08206351210431355, "compression_ratio": 1.7951219512195122, "no_speech_prob": 0.005909123457968235}, {"id": 607, "seek": 400948, "start": 4009.48, "end": 4014.76, "text": " of global parameters hanging around and in every step your global parameters speak to the parameters", "tokens": [50364, 295, 4338, 9834, 8345, 926, 293, 294, 633, 1823, 428, 4338, 9834, 1710, 281, 264, 9834, 50628], "temperature": 0.0, "avg_logprob": -0.0954130596584744, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.004465275444090366}, {"id": 608, "seek": 400948, "start": 4014.76, "end": 4020.44, "text": " on the graph and are spoken to by parameters from the graph in some way which is probably nothing to", "tokens": [50628, 322, 264, 4295, 293, 366, 10759, 281, 538, 9834, 490, 264, 4295, 294, 512, 636, 597, 307, 1391, 1825, 281, 50912], "temperature": 0.0, "avg_logprob": -0.0954130596584744, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.004465275444090366}, {"id": 609, "seek": 400948, "start": 4020.44, "end": 4028.44, "text": " do with the n ones on the s ones so these are some nice examples imagine that I consider this", "tokens": [50912, 360, 365, 264, 297, 2306, 322, 264, 262, 2306, 370, 613, 366, 512, 1481, 5110, 3811, 300, 286, 1949, 341, 51312], "temperature": 0.0, "avg_logprob": -0.0954130596584744, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.004465275444090366}, {"id": 610, "seek": 400948, "start": 4028.44, "end": 4037.72, "text": " digraph here if you look at what a graph neural net does on this digraph it exactly mimics cnn's", "tokens": [51312, 2528, 2662, 510, 498, 291, 574, 412, 437, 257, 4295, 18161, 2533, 775, 322, 341, 2528, 2662, 309, 2293, 12247, 1167, 269, 26384, 311, 51776], "temperature": 0.0, "avg_logprob": -0.0954130596584744, "compression_ratio": 1.8317757009345794, "no_speech_prob": 0.004465275444090366}, {"id": 611, "seek": 403772, "start": 4038.2799999999997, "end": 4046.4399999999996, "text": " without falling layer so you know it's it's it's not it's not an exaggeration to say that", "tokens": [50392, 1553, 7440, 4583, 370, 291, 458, 309, 311, 309, 311, 309, 311, 406, 309, 311, 406, 364, 19123, 399, 281, 584, 300, 50800], "temperature": 0.0, "avg_logprob": -0.13487467272528286, "compression_ratio": 1.5579710144927537, "no_speech_prob": 0.0023577508982270956}, {"id": 612, "seek": 403772, "start": 4047.7999999999997, "end": 4050.2799999999997, "text": " cnn's are a subset of giant graph neural nets", "tokens": [50868, 269, 26384, 311, 366, 257, 25993, 295, 7410, 4295, 18161, 36170, 50992], "temperature": 0.0, "avg_logprob": -0.13487467272528286, "compression_ratio": 1.5579710144927537, "no_speech_prob": 0.0023577508982270956}, {"id": 613, "seek": 403772, "start": 4056.9199999999996, "end": 4064.4399999999996, "text": " and if you kind of unpack this definition for uh deeps for a complete graph you", "tokens": [51324, 293, 498, 291, 733, 295, 26699, 341, 7123, 337, 2232, 2452, 82, 337, 257, 3566, 4295, 291, 51700], "temperature": 0.0, "avg_logprob": -0.13487467272528286, "compression_ratio": 1.5579710144927537, "no_speech_prob": 0.0023577508982270956}, {"id": 614, "seek": 406444, "start": 4065.4, "end": 4067.32, "text": " uh basically get deepsets", "tokens": [50412, 2232, 1936, 483, 2452, 82, 1385, 50508], "temperature": 0.0, "avg_logprob": -0.18779674653084047, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.004327493254095316}, {"id": 615, "seek": 406444, "start": 4069.8, "end": 4075.7200000000003, "text": " and that is all for today so thank you very much and yeah if there's any questions please ask", "tokens": [50632, 293, 300, 307, 439, 337, 965, 370, 1309, 291, 588, 709, 293, 1338, 498, 456, 311, 604, 1651, 1767, 1029, 50928], "temperature": 0.0, "avg_logprob": -0.18779674653084047, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.004327493254095316}, {"id": 616, "seek": 406444, "start": 4075.7200000000003, "end": 4079.48, "text": " so gay i'll ask a question i'll just ask uh answer the question of dr bouts you first", "tokens": [50928, 370, 9049, 741, 603, 1029, 257, 1168, 741, 603, 445, 1029, 2232, 1867, 264, 1168, 295, 1224, 272, 7711, 291, 700, 51116], "temperature": 0.0, "avg_logprob": -0.18779674653084047, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.004327493254095316}, {"id": 617, "seek": 406444, "start": 4081.0, "end": 4084.68, "text": " I wonder if by adding linear maps between vertices of the graph and making the whole", "tokens": [51192, 286, 2441, 498, 538, 5127, 8213, 11317, 1296, 32053, 295, 264, 4295, 293, 1455, 264, 1379, 51376], "temperature": 0.0, "avg_logprob": -0.18779674653084047, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.004327493254095316}, {"id": 618, "seek": 406444, "start": 4084.68, "end": 4092.92, "text": " architecture commutative kind of quirl net has any ml interpretation yeah so I totally agree", "tokens": [51376, 9482, 800, 325, 1166, 733, 295, 421, 1648, 2533, 575, 604, 23271, 14174, 1338, 370, 286, 3879, 3986, 51788], "temperature": 0.0, "avg_logprob": -0.18779674653084047, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.004327493254095316}, {"id": 619, "seek": 409292, "start": 4092.92, "end": 4097.56, "text": " that when you look at neural net architectures it looks very much like river varieties and", "tokens": [50364, 300, 562, 291, 574, 412, 18161, 2533, 6331, 1303, 309, 1542, 588, 709, 411, 6810, 22092, 293, 50596], "temperature": 0.0, "avg_logprob": -0.12007642373806093, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.007564406841993332}, {"id": 620, "seek": 409292, "start": 4097.56, "end": 4102.92, "text": " things that we study in representation theory however one really can't underestimate the", "tokens": [50596, 721, 300, 321, 2979, 294, 10290, 5261, 4461, 472, 534, 393, 380, 35826, 264, 50864], "temperature": 0.0, "avg_logprob": -0.12007642373806093, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.007564406841993332}, {"id": 621, "seek": 409292, "start": 4102.92, "end": 4111.4, "text": " effect of this relu of like so if you think about a like just a classic feed forward vanilla neural", "tokens": [50864, 1802, 295, 341, 1039, 84, 295, 411, 370, 498, 291, 519, 466, 257, 411, 445, 257, 7230, 3154, 2128, 17528, 18161, 51288], "temperature": 0.0, "avg_logprob": -0.12007642373806093, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.007564406841993332}, {"id": 622, "seek": 409292, "start": 4111.4, "end": 4118.2, "text": " net um if you don't have the relu's we understand totally what happens by basically near algebra", "tokens": [51288, 2533, 1105, 498, 291, 500, 380, 362, 264, 1039, 84, 311, 321, 1223, 3879, 437, 2314, 538, 1936, 2651, 21989, 51628], "temperature": 0.0, "avg_logprob": -0.12007642373806093, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.007564406841993332}, {"id": 623, "seek": 411820, "start": 4119.16, "end": 4123.8, "text": " but when we add the relu's we can suddenly approximate any function on a compact set", "tokens": [50412, 457, 562, 321, 909, 264, 1039, 84, 311, 321, 393, 5800, 30874, 604, 2445, 322, 257, 14679, 992, 50644], "temperature": 0.0, "avg_logprob": -0.08855697512626648, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.001954819308593869}, {"id": 624, "seek": 411820, "start": 4124.44, "end": 4131.72, "text": " so and we we have absolutely no idea of what happens inside the the neural net so yeah I find", "tokens": [50676, 370, 293, 321, 321, 362, 3122, 572, 1558, 295, 437, 2314, 1854, 264, 264, 18161, 2533, 370, 1338, 286, 915, 51040], "temperature": 0.0, "avg_logprob": -0.08855697512626648, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.001954819308593869}, {"id": 625, "seek": 411820, "start": 4131.72, "end": 4136.84, "text": " the quiver language like very useful to think to think about but um one shouldn't underestimate", "tokens": [51040, 264, 421, 1837, 2856, 411, 588, 4420, 281, 519, 281, 519, 466, 457, 1105, 472, 4659, 380, 35826, 51296], "temperature": 0.0, "avg_logprob": -0.08855697512626648, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.001954819308593869}, {"id": 626, "seek": 413684, "start": 4137.8, "end": 4144.84, "text": " relu's so gaug asked um what have graph neural nets been used for", "tokens": [50412, 1039, 84, 311, 370, 5959, 697, 2351, 1105, 437, 362, 4295, 18161, 36170, 668, 1143, 337, 50764], "temperature": 0.0, "avg_logprob": -0.20320917665958405, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.05652419105172157}, {"id": 627, "seek": 413684, "start": 4146.2, "end": 4155.56, "text": " um and my understanding is that um like I don't know like let's say half of facebook and 75 percent", "tokens": [50832, 1105, 293, 452, 3701, 307, 300, 1105, 411, 286, 500, 380, 458, 411, 718, 311, 584, 1922, 295, 23372, 293, 9562, 3043, 51300], "temperature": 0.0, "avg_logprob": -0.20320917665958405, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.05652419105172157}, {"id": 628, "seek": 413684, "start": 4155.56, "end": 4161.88, "text": " of twitter is graph neural nets um because you've got all these like connection graphs and social", "tokens": [51300, 295, 21439, 307, 4295, 18161, 36170, 1105, 570, 291, 600, 658, 439, 613, 411, 4984, 24877, 293, 2093, 51616], "temperature": 0.0, "avg_logprob": -0.20320917665958405, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.05652419105172157}, {"id": 629, "seek": 416188, "start": 4161.88, "end": 4170.12, "text": " networks and stuff like that um graph neural nets were like the absolute center thing that we used", "tokens": [50364, 9590, 293, 1507, 411, 300, 1105, 4295, 18161, 36170, 645, 411, 264, 8236, 3056, 551, 300, 321, 1143, 50776], "temperature": 0.0, "avg_logprob": -0.1191311316056685, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.009369486942887306}, {"id": 630, "seek": 416188, "start": 4170.12, "end": 4177.400000000001, "text": " with deep mind too on this work on cash analysis polynomials um my understanding is that graph neural", "tokens": [50776, 365, 2452, 1575, 886, 322, 341, 589, 322, 6388, 5215, 22560, 12356, 1105, 452, 3701, 307, 300, 4295, 18161, 51140], "temperature": 0.0, "avg_logprob": -0.1191311316056685, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.009369486942887306}, {"id": 631, "seek": 416188, "start": 4177.400000000001, "end": 4185.24, "text": " nets are kind of um taking over neural net world in terms of they're very flexible um they're very", "tokens": [51140, 36170, 366, 733, 295, 1105, 1940, 670, 18161, 2533, 1002, 294, 2115, 295, 436, 434, 588, 11358, 1105, 436, 434, 588, 51532], "temperature": 0.0, "avg_logprob": -0.1191311316056685, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.009369486942887306}, {"id": 632, "seek": 416188, "start": 4185.24, "end": 4191.72, "text": " powerful um and a lot of tasks where for example c and n would involve like a drastic change of", "tokens": [51532, 4005, 1105, 293, 257, 688, 295, 9608, 689, 337, 1365, 269, 293, 297, 576, 9494, 411, 257, 36821, 1319, 295, 51856], "temperature": 0.0, "avg_logprob": -0.1191311316056685, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.009369486942887306}, {"id": 633, "seek": 419172, "start": 4191.72, "end": 4198.360000000001, "text": " architecture um in graph neural nets you can just like add a vertex or something like that so", "tokens": [50364, 9482, 1105, 294, 4295, 18161, 36170, 291, 393, 445, 411, 909, 257, 28162, 420, 746, 411, 300, 370, 50696], "temperature": 0.0, "avg_logprob": -0.08734089211572575, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0002164145844290033}, {"id": 634, "seek": 419172, "start": 4198.360000000001, "end": 4203.08, "text": " they're very flexible powerful framework for machine learning as far as I can make out", "tokens": [50696, 436, 434, 588, 11358, 4005, 8388, 337, 3479, 2539, 382, 1400, 382, 286, 393, 652, 484, 50932], "temperature": 0.0, "avg_logprob": -0.08734089211572575, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0002164145844290033}, {"id": 635, "seek": 419172, "start": 4204.52, "end": 4209.240000000001, "text": " sam yates asked for discrete valued problem could we pick other group rings say with some", "tokens": [51004, 3247, 288, 1024, 2351, 337, 27706, 22608, 1154, 727, 321, 1888, 661, 1594, 11136, 584, 365, 512, 51240], "temperature": 0.0, "avg_logprob": -0.08734089211572575, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0002164145844290033}, {"id": 636, "seek": 419172, "start": 4209.240000000001, "end": 4217.0, "text": " choice of analogous relu like function perhaps uh that's an intimidating prospect for me because", "tokens": [51240, 3922, 295, 16660, 563, 1039, 84, 411, 2445, 4317, 2232, 300, 311, 364, 29714, 15005, 337, 385, 570, 51628], "temperature": 0.0, "avg_logprob": -0.08734089211572575, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0002164145844290033}, {"id": 637, "seek": 421700, "start": 4217.0, "end": 4223.08, "text": " I wouldn't know how to train and things like that so my very vague understanding so there's this", "tokens": [50364, 286, 2759, 380, 458, 577, 281, 3847, 293, 721, 411, 300, 370, 452, 588, 24247, 3701, 370, 456, 311, 341, 50668], "temperature": 0.0, "avg_logprob": -0.04600724353585192, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.001621672767214477}, {"id": 638, "seek": 421700, "start": 4223.08, "end": 4230.36, "text": " kind of revolution in the last kind of three four years given by transformers and my understanding", "tokens": [50668, 733, 295, 8894, 294, 264, 1036, 733, 295, 1045, 1451, 924, 2212, 538, 4088, 433, 293, 452, 3701, 51032], "temperature": 0.0, "avg_logprob": -0.04600724353585192, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.001621672767214477}, {"id": 639, "seek": 421700, "start": 4230.36, "end": 4239.88, "text": " of that is like basically like a a graph neural net um hooked on to an lstm so like the graph neural", "tokens": [51032, 295, 300, 307, 411, 1936, 411, 257, 257, 4295, 18161, 2533, 1105, 20410, 322, 281, 364, 287, 372, 76, 370, 411, 264, 4295, 18161, 51508], "temperature": 0.0, "avg_logprob": -0.04600724353585192, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.001621672767214477}, {"id": 640, "seek": 421700, "start": 4239.88, "end": 4246.04, "text": " net kind of decides where to look back in the sentence and things like that so um but I'm not", "tokens": [51508, 2533, 733, 295, 14898, 689, 281, 574, 646, 294, 264, 8174, 293, 721, 411, 300, 370, 1105, 457, 286, 478, 406, 51816], "temperature": 0.0, "avg_logprob": -0.04600724353585192, "compression_ratio": 1.8224299065420562, "no_speech_prob": 0.001621672767214477}, {"id": 641, "seek": 424604, "start": 4246.04, "end": 4251.8, "text": " directly aware of any recurrent technologies used directly with graph neural nets what another", "tokens": [50364, 3838, 3650, 295, 604, 18680, 1753, 7943, 1143, 3838, 365, 4295, 18161, 36170, 437, 1071, 50652], "temperature": 0.0, "avg_logprob": -0.055456716187146246, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.0014084847643971443}, {"id": 642, "seek": 424604, "start": 4251.8, "end": 4256.28, "text": " thing that graph neural nets are very useful for which is kind of counterintuitive is like predicting", "tokens": [50652, 551, 300, 4295, 18161, 36170, 366, 588, 4420, 337, 597, 307, 733, 295, 5682, 686, 48314, 307, 411, 32884, 50876], "temperature": 0.0, "avg_logprob": -0.055456716187146246, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.0014084847643971443}, {"id": 643, "seek": 424604, "start": 4256.28, "end": 4262.5199999999995, "text": " graph structure so you have data and you want to um and you want to predict which edges exist", "tokens": [50876, 4295, 3877, 370, 291, 362, 1412, 293, 291, 528, 281, 1105, 293, 291, 528, 281, 6069, 597, 8819, 2514, 51188], "temperature": 0.0, "avg_logprob": -0.055456716187146246, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.0014084847643971443}, {"id": 644, "seek": 424604, "start": 4263.08, "end": 4267.08, "text": " so like you might want to predict social relationships or something like that and um my", "tokens": [51216, 370, 411, 291, 1062, 528, 281, 6069, 2093, 6159, 420, 746, 411, 300, 293, 1105, 452, 51416], "temperature": 0.0, "avg_logprob": -0.055456716187146246, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.0014084847643971443}, {"id": 645, "seek": 424604, "start": 4267.08, "end": 4271.88, "text": " understanding is that you you start off with a complete graph and run a graph neural net", "tokens": [51416, 3701, 307, 300, 291, 291, 722, 766, 365, 257, 3566, 4295, 293, 1190, 257, 4295, 18161, 2533, 51656], "temperature": 0.0, "avg_logprob": -0.055456716187146246, "compression_ratio": 1.9458333333333333, "no_speech_prob": 0.0014084847643971443}, {"id": 646, "seek": 427188, "start": 4271.88, "end": 4277.56, "text": " and your graph kind of learns a probability of discarding an edge and that's very very powerful", "tokens": [50364, 293, 428, 4295, 733, 295, 27152, 257, 8482, 295, 31597, 278, 364, 4691, 293, 300, 311, 588, 588, 4005, 50648], "temperature": 0.0, "avg_logprob": -0.08605775488428323, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.001323177246376872}, {"id": 647, "seek": 427188, "start": 4279.32, "end": 4283.88, "text": " so it it's intuitive and intuitive but graph neural nets can learn graphs which I think is", "tokens": [50736, 370, 309, 309, 311, 21769, 293, 21769, 457, 4295, 18161, 36170, 393, 1466, 24877, 597, 286, 519, 307, 50964], "temperature": 0.0, "avg_logprob": -0.08605775488428323, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.001323177246376872}, {"id": 648, "seek": 427188, "start": 4283.88, "end": 4288.92, "text": " awesome I think I think like learning graphs is like learning graphs is a really difficult", "tokens": [50964, 3476, 286, 519, 286, 519, 411, 2539, 24877, 307, 411, 2539, 24877, 307, 257, 534, 2252, 51216], "temperature": 0.0, "avg_logprob": -0.08605775488428323, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.001323177246376872}, {"id": 649, "seek": 427188, "start": 4288.92, "end": 4297.24, "text": " problem in machine learning um and yeah of course so it's going to be yeah it's an intimidating", "tokens": [51216, 1154, 294, 3479, 2539, 1105, 293, 1338, 295, 1164, 370, 309, 311, 516, 281, 312, 1338, 309, 311, 364, 29714, 51632], "temperature": 0.0, "avg_logprob": -0.08605775488428323, "compression_ratio": 1.8284313725490196, "no_speech_prob": 0.001323177246376872}, {"id": 650, "seek": 429724, "start": 4297.24, "end": 4303.96, "text": " problem that or n choose tuna yeah so that's a really good question so what kind of problems", "tokens": [50364, 1154, 300, 420, 297, 2826, 26670, 1338, 370, 300, 311, 257, 534, 665, 1168, 370, 437, 733, 295, 2740, 50700], "temperature": 0.0, "avg_logprob": -0.09513242854628452, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0021446594037115574}, {"id": 651, "seek": 429724, "start": 4303.96, "end": 4310.04, "text": " and why can be addressed with graph neural nets my understanding is that very recently there's been", "tokens": [50700, 293, 983, 393, 312, 13847, 365, 4295, 18161, 36170, 452, 3701, 307, 300, 588, 3938, 456, 311, 668, 51004], "temperature": 0.0, "avg_logprob": -0.09513242854628452, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0021446594037115574}, {"id": 652, "seek": 429724, "start": 4310.04, "end": 4316.12, "text": " kind of benchmarking so there's been like a list of 50 problems and some some attempt to decide", "tokens": [51004, 733, 295, 18927, 278, 370, 456, 311, 668, 411, 257, 1329, 295, 2625, 2740, 293, 512, 512, 5217, 281, 4536, 51308], "temperature": 0.0, "avg_logprob": -0.09513242854628452, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0021446594037115574}, {"id": 653, "seek": 429724, "start": 4316.12, "end": 4320.679999999999, "text": " you know what what can various classes of graph neural net algorithms do and what they can't do", "tokens": [51308, 291, 458, 437, 437, 393, 3683, 5359, 295, 4295, 18161, 2533, 14642, 360, 293, 437, 436, 393, 380, 360, 51536], "temperature": 0.0, "avg_logprob": -0.09513242854628452, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0021446594037115574}, {"id": 654, "seek": 432068, "start": 4321.4800000000005, "end": 4331.320000000001, "text": " um my very rough picture as as a graph neural net greenhorn is um that anything that you can kind", "tokens": [50404, 1105, 452, 588, 5903, 3036, 382, 382, 257, 4295, 18161, 2533, 3092, 31990, 307, 1105, 300, 1340, 300, 291, 393, 733, 50896], "temperature": 0.0, "avg_logprob": -0.09292556192273292, "compression_ratio": 1.9672131147540983, "no_speech_prob": 0.008573366329073906}, {"id": 655, "seek": 432068, "start": 4331.320000000001, "end": 4337.8, "text": " of decide via a finite like a small amount of walking around your graph and if you if if you're", "tokens": [50896, 295, 4536, 5766, 257, 19362, 411, 257, 1359, 2372, 295, 4494, 926, 428, 4295, 293, 498, 291, 498, 498, 291, 434, 51220], "temperature": 0.0, "avg_logprob": -0.09292556192273292, "compression_ratio": 1.9672131147540983, "no_speech_prob": 0.008573366329073906}, {"id": 656, "seek": 432068, "start": 4337.8, "end": 4341.320000000001, "text": " extremely smart and you're allowed to walk around your graph a little bit and you can already make", "tokens": [51220, 4664, 4069, 293, 291, 434, 4350, 281, 1792, 926, 428, 4295, 257, 707, 857, 293, 291, 393, 1217, 652, 51396], "temperature": 0.0, "avg_logprob": -0.09292556192273292, "compression_ratio": 1.9672131147540983, "no_speech_prob": 0.008573366329073906}, {"id": 657, "seek": 432068, "start": 4341.320000000001, "end": 4346.280000000001, "text": " a decision then that's something that you could solve so something like detecting planarity or", "tokens": [51396, 257, 3537, 550, 300, 311, 746, 300, 291, 727, 5039, 370, 746, 411, 40237, 1393, 17409, 420, 51644], "temperature": 0.0, "avg_logprob": -0.09292556192273292, "compression_ratio": 1.9672131147540983, "no_speech_prob": 0.008573366329073906}, {"id": 658, "seek": 432068, "start": 4346.280000000001, "end": 4350.4400000000005, "text": " detecting a three cycle or something this is something that you can solve but if it involves", "tokens": [51644, 40237, 257, 1045, 6586, 420, 746, 341, 307, 746, 300, 291, 393, 5039, 457, 498, 309, 11626, 51852], "temperature": 0.0, "avg_logprob": -0.09292556192273292, "compression_ratio": 1.9672131147540983, "no_speech_prob": 0.008573366329073906}, {"id": 659, "seek": 435044, "start": 4350.5199999999995, "end": 4355.96, "text": " exploring the whole graph particularly potentially in many many different iterations for example", "tokens": [50368, 12736, 264, 1379, 4295, 4098, 7263, 294, 867, 867, 819, 36540, 337, 1365, 50640], "temperature": 0.0, "avg_logprob": -0.14892515572168494, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0004105975094716996}, {"id": 660, "seek": 435044, "start": 4355.96, "end": 4361.24, "text": " finding a Hamiltonian cycle or something um yeah it's not going to work", "tokens": [50640, 5006, 257, 18484, 952, 6586, 420, 746, 1105, 1338, 309, 311, 406, 516, 281, 589, 50904], "temperature": 0.0, "avg_logprob": -0.14892515572168494, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0004105975094716996}, {"id": 661, "seek": 435044, "start": 4363.16, "end": 4366.759999999999, "text": " she'll have said the piecewise linear representation can you say a bit more about it", "tokens": [51000, 750, 603, 362, 848, 264, 2522, 3711, 8213, 10290, 393, 291, 584, 257, 857, 544, 466, 309, 51180], "temperature": 0.0, "avg_logprob": -0.14892515572168494, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0004105975094716996}, {"id": 662, "seek": 435044, "start": 4367.719999999999, "end": 4371.719999999999, "text": " sure so how about I will ask answer Joel's question and then maybe I can say something", "tokens": [51228, 988, 370, 577, 466, 286, 486, 1029, 1867, 21522, 311, 1168, 293, 550, 1310, 286, 393, 584, 746, 51428], "temperature": 0.0, "avg_logprob": -0.14892515572168494, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0004105975094716996}, {"id": 663, "seek": 435044, "start": 4371.719999999999, "end": 4375.639999999999, "text": " about it but I'll give other people the chance to leave because it's somewhat specialised", "tokens": [51428, 466, 309, 457, 286, 603, 976, 661, 561, 264, 2931, 281, 1856, 570, 309, 311, 8344, 2121, 2640, 51624], "temperature": 0.0, "avg_logprob": -0.14892515572168494, "compression_ratio": 1.6602316602316602, "no_speech_prob": 0.0004105975094716996}, {"id": 664, "seek": 437564, "start": 4376.360000000001, "end": 4381.64, "text": " so a part of Adam salt Wagner's work was learning graphs learning good people", "tokens": [50400, 370, 257, 644, 295, 7938, 5139, 38146, 311, 589, 390, 2539, 24877, 2539, 665, 561, 50664], "temperature": 0.0, "avg_logprob": -0.10230603814125061, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0015407288447022438}, {"id": 665, "seek": 437564, "start": 4383.160000000001, "end": 4387.0, "text": " to a particularly conjecture for instance so we might hear about learning graphs in a few weeks", "tokens": [50740, 281, 257, 4098, 416, 1020, 540, 337, 5197, 370, 321, 1062, 1568, 466, 2539, 24877, 294, 257, 1326, 3259, 50932], "temperature": 0.0, "avg_logprob": -0.10230603814125061, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0015407288447022438}, {"id": 666, "seek": 437564, "start": 4387.0, "end": 4391.64, "text": " yes exactly so salt Wagner's work is using reinforcement learning to produce interesting", "tokens": [50932, 2086, 2293, 370, 5139, 38146, 311, 589, 307, 1228, 29280, 2539, 281, 5258, 1880, 51164], "temperature": 0.0, "avg_logprob": -0.10230603814125061, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0015407288447022438}, {"id": 667, "seek": 437564, "start": 4391.64, "end": 4395.88, "text": " graphs okay so now we'll declare it over and anybody that's interested in this piecewise", "tokens": [51164, 24877, 1392, 370, 586, 321, 603, 19710, 309, 670, 293, 4472, 300, 311, 3102, 294, 341, 2522, 3711, 51376], "temperature": 0.0, "avg_logprob": -0.10230603814125061, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0015407288447022438}, {"id": 668, "seek": 437564, "start": 4395.88, "end": 4403.96, "text": " linear business can stick around I think this is super beautiful and um I will just explain it", "tokens": [51376, 8213, 1606, 393, 2897, 926, 286, 519, 341, 307, 1687, 2238, 293, 1105, 286, 486, 445, 2903, 309, 51780], "temperature": 0.0, "avg_logprob": -0.10230603814125061, "compression_ratio": 1.7559055118110236, "no_speech_prob": 0.0015407288447022438}, {"id": 669, "seek": 440396, "start": 4403.96, "end": 4409.72, "text": " briefly so let's just consider the following silly examples so we're looking at the", "tokens": [50364, 10515, 370, 718, 311, 445, 1949, 264, 3480, 11774, 5110, 370, 321, 434, 1237, 412, 264, 50652], "temperature": 0.0, "avg_logprob": -0.160155595735062, "compression_ratio": 1.4180327868852458, "no_speech_prob": 0.0016462308121845126}, {"id": 670, "seek": 440396, "start": 4411.16, "end": 4423.08, "text": " sn acting on s3 acting on r3 or permutation representation now we know that r3 decomposes", "tokens": [50724, 2406, 6577, 322, 262, 18, 6577, 322, 367, 18, 420, 4784, 11380, 10290, 586, 321, 458, 300, 367, 18, 22867, 4201, 51320], "temperature": 0.0, "avg_logprob": -0.160155595735062, "compression_ratio": 1.4180327868852458, "no_speech_prob": 0.0016462308121845126}, {"id": 671, "seek": 442308, "start": 4423.16, "end": 4434.5199999999995, "text": " canonically as nat plus triv and that is the set of vectors in r3 such that the sum of the lambda", "tokens": [50368, 21985, 984, 382, 2249, 1804, 1376, 85, 293, 300, 307, 264, 992, 295, 18875, 294, 367, 18, 1270, 300, 264, 2408, 295, 264, 13607, 50936], "temperature": 0.0, "avg_logprob": -0.16188513502782706, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.015870120376348495}, {"id": 672, "seek": 442308, "start": 4434.5199999999995, "end": 4451.16, "text": " i is zero and triv is the set of r times 111 and this decomposition is completely canonical", "tokens": [50936, 741, 307, 4018, 293, 1376, 85, 307, 264, 992, 295, 367, 1413, 2975, 16, 293, 341, 48356, 307, 2584, 46491, 51768], "temperature": 0.0, "avg_logprob": -0.16188513502782706, "compression_ratio": 1.5365853658536586, "no_speech_prob": 0.015870120376348495}, {"id": 673, "seek": 445308, "start": 4453.4, "end": 4461.08, "text": " so now you can ask the following like just kind of totally naive question if we go from nat", "tokens": [50380, 370, 586, 291, 393, 1029, 264, 3480, 411, 445, 733, 295, 3879, 29052, 1168, 498, 321, 352, 490, 2249, 50764], "temperature": 0.0, "avg_logprob": -0.17132843241972082, "compression_ratio": 1.4296875, "no_speech_prob": 0.000458052905742079}, {"id": 674, "seek": 445308, "start": 4462.5199999999995, "end": 4466.68, "text": " into r3 and then apply value", "tokens": [50836, 666, 367, 18, 293, 550, 3079, 2158, 51044], "temperature": 0.0, "avg_logprob": -0.17132843241972082, "compression_ratio": 1.4296875, "no_speech_prob": 0.000458052905742079}, {"id": 675, "seek": 445308, "start": 4469.64, "end": 4478.28, "text": " go to r3 back down to nat the composition is an s3 equivariant", "tokens": [51192, 352, 281, 367, 18, 646, 760, 281, 2249, 264, 12686, 307, 364, 262, 18, 1267, 592, 3504, 394, 51624], "temperature": 0.0, "avg_logprob": -0.17132843241972082, "compression_ratio": 1.4296875, "no_speech_prob": 0.000458052905742079}, {"id": 676, "seek": 448308, "start": 4483.72, "end": 4488.68, "text": " pl endomorphism", "tokens": [50396, 499, 917, 32702, 1434, 50644], "temperature": 0.0, "avg_logprob": -0.14786641494087552, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.0012234966270625591}, {"id": 677, "seek": 448308, "start": 4489.64, "end": 4495.8, "text": " i.e in the category of piecewise linear maps from this vector space to itself that a s3", "tokens": [50692, 741, 13, 68, 294, 264, 7719, 295, 2522, 3711, 8213, 11317, 490, 341, 8062, 1901, 281, 2564, 300, 257, 262, 18, 51000], "temperature": 0.0, "avg_logprob": -0.14786641494087552, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.0012234966270625591}, {"id": 678, "seek": 448308, "start": 4495.8, "end": 4499.08, "text": " equivariant this is a pl endomorphism and what is it", "tokens": [51000, 1267, 592, 3504, 394, 341, 307, 257, 499, 917, 32702, 1434, 293, 437, 307, 309, 51164], "temperature": 0.0, "avg_logprob": -0.14786641494087552, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.0012234966270625591}, {"id": 679, "seek": 448308, "start": 4502.12, "end": 4510.04, "text": " it's super beautiful so basically what you do is inside so here's nat so we divide up", "tokens": [51316, 309, 311, 1687, 2238, 370, 1936, 437, 291, 360, 307, 1854, 370, 510, 311, 2249, 370, 321, 9845, 493, 51712], "temperature": 0.0, "avg_logprob": -0.14786641494087552, "compression_ratio": 1.5414012738853504, "no_speech_prob": 0.0012234966270625591}, {"id": 680, "seek": 451004, "start": 4510.04, "end": 4521.64, "text": " nat so just for people that don't do this every day nat is the um symmetries of the triangle", "tokens": [50364, 2249, 370, 445, 337, 561, 300, 500, 380, 360, 341, 633, 786, 2249, 307, 264, 1105, 14232, 302, 2244, 295, 264, 13369, 50944], "temperature": 0.0, "avg_logprob": -0.14011082649230958, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0009104047785513103}, {"id": 681, "seek": 451004, "start": 4524.44, "end": 4529.88, "text": " embedded inside r3 r2 okay so we just take an equilateral triangle and we take the symmetries", "tokens": [51084, 16741, 1854, 367, 18, 367, 17, 1392, 370, 321, 445, 747, 364, 1267, 37751, 13369, 293, 321, 747, 264, 14232, 302, 2244, 51356], "temperature": 0.0, "avg_logprob": -0.14011082649230958, "compression_ratio": 1.4761904761904763, "no_speech_prob": 0.0009104047785513103}, {"id": 682, "seek": 452988, "start": 4529.88, "end": 4543.56, "text": " of the equilateral triangle so now um there's three regions here and what happens so there's six", "tokens": [50364, 295, 264, 1267, 37751, 13369, 370, 586, 1105, 456, 311, 1045, 10682, 510, 293, 437, 2314, 370, 456, 311, 2309, 51048], "temperature": 0.0, "avg_logprob": -0.11689208802722749, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004461492411792278}, {"id": 683, "seek": 452988, "start": 4543.56, "end": 4555.4800000000005, "text": " regions there's the blue regions and the red regions and the blue regions get squashed", "tokens": [51048, 10682, 456, 311, 264, 3344, 10682, 293, 264, 2182, 10682, 293, 264, 3344, 10682, 483, 2339, 12219, 51644], "temperature": 0.0, "avg_logprob": -0.11689208802722749, "compression_ratio": 1.7264150943396226, "no_speech_prob": 0.004461492411792278}, {"id": 684, "seek": 455988, "start": 4560.84, "end": 4563.88, "text": " and the red regions get kind of expanded out", "tokens": [50412, 293, 264, 2182, 10682, 483, 733, 295, 14342, 484, 50564], "temperature": 0.0, "avg_logprob": -0.13826796473289021, "compression_ratio": 1.4545454545454546, "no_speech_prob": 9.759428940014914e-05}, {"id": 685, "seek": 455988, "start": 4569.16, "end": 4570.2, "text": " so these get squashed", "tokens": [50828, 370, 613, 483, 2339, 12219, 50880], "temperature": 0.0, "avg_logprob": -0.13826796473289021, "compression_ratio": 1.4545454545454546, "no_speech_prob": 9.759428940014914e-05}, {"id": 686, "seek": 455988, "start": 4572.76, "end": 4573.8, "text": " and these get expanded", "tokens": [51008, 293, 613, 483, 14342, 51060], "temperature": 0.0, "avg_logprob": -0.13826796473289021, "compression_ratio": 1.4545454545454546, "no_speech_prob": 9.759428940014914e-05}, {"id": 687, "seek": 455988, "start": 4579.08, "end": 4585.96, "text": " and i don't know this is just like a very beautiful basic um like pl endomorphism of a", "tokens": [51324, 293, 741, 500, 380, 458, 341, 307, 445, 411, 257, 588, 2238, 3875, 1105, 411, 499, 917, 32702, 1434, 295, 257, 51668], "temperature": 0.0, "avg_logprob": -0.13826796473289021, "compression_ratio": 1.4545454545454546, "no_speech_prob": 9.759428940014914e-05}, {"id": 688, "seek": 458596, "start": 4586.04, "end": 4592.36, "text": " representation that i've never encountered before in my life okay and you can start having fun like", "tokens": [50368, 10290, 300, 741, 600, 1128, 20381, 949, 294, 452, 993, 1392, 293, 291, 393, 722, 1419, 1019, 411, 50684], "temperature": 0.0, "avg_logprob": -0.10750711531866164, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.001223467756062746}, {"id": 689, "seek": 458596, "start": 4592.36, "end": 4592.92, "text": " what is this", "tokens": [50684, 437, 307, 341, 50712], "temperature": 0.0, "avg_logprob": -0.10750711531866164, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.001223467756062746}, {"id": 690, "seek": 458596, "start": 4598.44, "end": 4604.76, "text": " or nat inside r in and you know this is a nice exercise", "tokens": [50988, 420, 2249, 1854, 367, 294, 293, 291, 458, 341, 307, 257, 1481, 5380, 51304], "temperature": 0.0, "avg_logprob": -0.10750711531866164, "compression_ratio": 1.423728813559322, "no_speech_prob": 0.001223467756062746}, {"id": 691, "seek": 460476, "start": 4605.320000000001, "end": 4616.68, "text": " and yeah so that one of the things that i find really interesting is that home", "tokens": [50392, 293, 1338, 370, 300, 472, 295, 264, 721, 300, 741, 915, 534, 1880, 307, 300, 1280, 50960], "temperature": 0.0, "avg_logprob": -0.21325175992904172, "compression_ratio": 1.4065934065934067, "no_speech_prob": 0.0062822275795042515}, {"id": 692, "seek": 460476, "start": 4619.400000000001, "end": 4625.320000000001, "text": " pl from any representation to r um is interesting", "tokens": [51096, 499, 490, 604, 10290, 281, 367, 1105, 307, 1880, 51392], "temperature": 0.0, "avg_logprob": -0.21325175992904172, "compression_ratio": 1.4065934065934067, "no_speech_prob": 0.0062822275795042515}, {"id": 693, "seek": 462532, "start": 4625.799999999999, "end": 4638.12, "text": " okay so example is that home from pl from the sign representation to r", "tokens": [50388, 1392, 370, 1365, 307, 300, 1280, 490, 499, 490, 264, 1465, 10290, 281, 367, 51004], "temperature": 0.0, "avg_logprob": -0.18994737708050272, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.004677504301071167}, {"id": 694, "seek": 462532, "start": 4639.16, "end": 4641.639999999999, "text": " contains the absolute value map", "tokens": [51056, 8306, 264, 8236, 2158, 4471, 51180], "temperature": 0.0, "avg_logprob": -0.18994737708050272, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.004677504301071167}, {"id": 695, "seek": 462532, "start": 4645.96, "end": 4651.08, "text": " um like this is for this is the sign representation of s2 oh no sign representation in general", "tokens": [51396, 1105, 411, 341, 307, 337, 341, 307, 264, 1465, 10290, 295, 262, 17, 1954, 572, 1465, 10290, 294, 2674, 51652], "temperature": 0.0, "avg_logprob": -0.18994737708050272, "compression_ratio": 1.7130434782608697, "no_speech_prob": 0.004677504301071167}, {"id": 696, "seek": 465108, "start": 4651.08, "end": 4660.68, "text": " in fact um but home from the trivial representation pl to any irrep", "tokens": [50364, 294, 1186, 1105, 457, 1280, 490, 264, 26703, 10290, 499, 281, 604, 16014, 79, 50844], "temperature": 0.0, "avg_logprob": -0.18761800130208334, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.001224642270244658}, {"id": 697, "seek": 465108, "start": 4662.68, "end": 4666.84, "text": " not equal to the trivial zero", "tokens": [50944, 406, 2681, 281, 264, 26703, 4018, 51152], "temperature": 0.0, "avg_logprob": -0.18761800130208334, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.001224642270244658}, {"id": 698, "seek": 465108, "start": 4669.64, "end": 4675.16, "text": " and i kind of feel like this is telling us something remarkable that about kind of how", "tokens": [51292, 293, 741, 733, 295, 841, 411, 341, 307, 3585, 505, 746, 12802, 300, 466, 733, 295, 577, 51568], "temperature": 0.0, "avg_logprob": -0.18761800130208334, "compression_ratio": 1.4838709677419355, "no_speech_prob": 0.001224642270244658}, {"id": 699, "seek": 467516, "start": 4675.24, "end": 4680.84, "text": " equivalence kind of flows through a neural net so this is still very speculative but what", "tokens": [50368, 9052, 655, 733, 295, 12867, 807, 257, 18161, 2533, 370, 341, 307, 920, 588, 49415, 457, 437, 50648], "temperature": 0.0, "avg_logprob": -0.08226678848266601, "compression_ratio": 1.8353658536585367, "no_speech_prob": 0.00342408730648458}, {"id": 700, "seek": 467516, "start": 4680.84, "end": 4688.68, "text": " i feel like is that you have some kind of measure of complexity so it like at the start you have all", "tokens": [50648, 741, 841, 411, 307, 300, 291, 362, 512, 733, 295, 3481, 295, 14024, 370, 309, 411, 412, 264, 722, 291, 362, 439, 51040], "temperature": 0.0, "avg_logprob": -0.08226678848266601, "compression_ratio": 1.8353658536585367, "no_speech_prob": 0.00342408730648458}, {"id": 701, "seek": 467516, "start": 4688.68, "end": 4689.32, "text": " all irreps", "tokens": [51040, 439, 16014, 1878, 51072], "temperature": 0.0, "avg_logprob": -0.08226678848266601, "compression_ratio": 1.8353658536585367, "no_speech_prob": 0.00342408730648458}, {"id": 702, "seek": 467516, "start": 4694.12, "end": 4700.84, "text": " and then at the end you have the trivial and then you have the maps in the in the neural net so you", "tokens": [51312, 293, 550, 412, 264, 917, 291, 362, 264, 26703, 293, 550, 291, 362, 264, 11317, 294, 264, 294, 264, 18161, 2533, 370, 291, 51648], "temperature": 0.0, "avg_logprob": -0.08226678848266601, "compression_ratio": 1.8353658536585367, "no_speech_prob": 0.00342408730648458}, {"id": 703, "seek": 470084, "start": 4700.84, "end": 4710.92, "text": " have linear and then you have pl and then you have linear pl and these pl maps have a definite", "tokens": [50364, 362, 8213, 293, 550, 291, 362, 499, 293, 550, 291, 362, 8213, 499, 293, 613, 499, 11317, 362, 257, 25131, 50868], "temperature": 0.0, "avg_logprob": -0.04873959223429362, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0024324192199856043}, {"id": 704, "seek": 470084, "start": 4710.92, "end": 4715.4800000000005, "text": " sense of direction like you know once we get through the trivial representation we can never", "tokens": [50868, 2020, 295, 3513, 411, 291, 458, 1564, 321, 483, 807, 264, 26703, 10290, 321, 393, 1128, 51096], "temperature": 0.0, "avg_logprob": -0.04873959223429362, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0024324192199856043}, {"id": 705, "seek": 470084, "start": 4715.4800000000005, "end": 4722.76, "text": " get out of it again and like i don't know this seems to explain some some very interesting", "tokens": [51096, 483, 484, 295, 309, 797, 293, 411, 741, 500, 380, 458, 341, 2544, 281, 2903, 512, 512, 588, 1880, 51460], "temperature": 0.0, "avg_logprob": -0.04873959223429362, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0024324192199856043}, {"id": 706, "seek": 470084, "start": 4722.76, "end": 4727.56, "text": " aspects of neural nets but it's just exciting stuff that i've been thinking about last week so", "tokens": [51460, 7270, 295, 18161, 36170, 457, 309, 311, 445, 4670, 1507, 300, 741, 600, 668, 1953, 466, 1036, 1243, 370, 51700], "temperature": 0.0, "avg_logprob": -0.04873959223429362, "compression_ratio": 1.7511737089201878, "no_speech_prob": 0.0024324192199856043}, {"id": 707, "seek": 472756, "start": 4727.64, "end": 4732.92, "text": " very unbaked um so maybe when it's baked i can uh talk more about it", "tokens": [50368, 588, 517, 65, 7301, 1105, 370, 1310, 562, 309, 311, 19453, 741, 393, 2232, 751, 544, 466, 309, 50632], "temperature": 0.0, "avg_logprob": -0.09702085226010053, "compression_ratio": 1.7751479289940828, "no_speech_prob": 0.0025447614025324583}, {"id": 708, "seek": 472756, "start": 4735.56, "end": 4739.64, "text": " so thanks are the blue rays there inside the reflecting hyperplanes", "tokens": [50764, 370, 3231, 366, 264, 3344, 24417, 456, 1854, 264, 23543, 9848, 564, 12779, 50968], "temperature": 0.0, "avg_logprob": -0.09702085226010053, "compression_ratio": 1.7751479289940828, "no_speech_prob": 0.0025447614025324583}, {"id": 709, "seek": 472756, "start": 4741.4800000000005, "end": 4743.240000000001, "text": " this is alpha one and this is alpha two", "tokens": [51060, 341, 307, 8961, 472, 293, 341, 307, 8961, 732, 51148], "temperature": 0.0, "avg_logprob": -0.09702085226010053, "compression_ratio": 1.7751479289940828, "no_speech_prob": 0.0025447614025324583}, {"id": 710, "seek": 472756, "start": 4745.88, "end": 4752.120000000001, "text": " so no oh yeah the so the reflecting hyperplanes would be like this and this and this", "tokens": [51280, 370, 572, 1954, 1338, 264, 370, 264, 23543, 9848, 564, 12779, 576, 312, 411, 341, 293, 341, 293, 341, 51592], "temperature": 0.0, "avg_logprob": -0.09702085226010053, "compression_ratio": 1.7751479289940828, "no_speech_prob": 0.0025447614025324583}, {"id": 711, "seek": 472756, "start": 4753.88, "end": 4755.400000000001, "text": " thanks everyone i think we'll stop now", "tokens": [51680, 3231, 1518, 741, 519, 321, 603, 1590, 586, 51756], "temperature": 0.0, "avg_logprob": -0.09702085226010053, "compression_ratio": 1.7751479289940828, "no_speech_prob": 0.0025447614025324583}, {"id": 712, "seek": 475756, "start": 4757.56, "end": 4758.06, "text": " you", "tokens": [50364, 291, 50389], "temperature": 0.0, "avg_logprob": -0.8184937238693237, "compression_ratio": 0.2727272727272727, "no_speech_prob": 0.43908819556236267}], "language": "en"}