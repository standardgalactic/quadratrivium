So, welcome to this week's lecture.
So, seminar structure.
Next week there'll be some linear combination of Gayog and Geordi, and the constant is yet
to be determined.
The week after, so after next week we move into the talks from experts.
So, we've been trying to give you background and then the experts will tell us what's really
going on.
So, I'm really excited by the lineup.
So, Adam Saltbaugner has done some amazing work using reinforcement learning to construct
counter examples in graph theory.
If you want to prepare for his talk, the paper is really beautifully written.
Bandad Hosseini works on graph methods.
So, we'll see a bit of graph neural nets today and we'll see more in his talk.
Carlos Simpson is talking on a kind of archetypal garden of how to use machine learning and
mathematical proof.
And I think that that's extremely interesting.
And then there's another three talks lined up after that, taking us to the end of semester.
And the other thing I wanted to point out is Joel has been doing an amazing job with
the toe labs.
If you've been taking part in the tutorials, you'll be aware of this.
But if you're watching online and you just want to muck around with something, they're
really great and I'm hoping that they provide a good resource.
So, we'll hope to keep these going for the expert talks so you can play with some of
Adam's ideas, play with some of Bumdud's ideas, et cetera.
So, again, the seminar principles.
So, today we'll be about geometric deep learning and geometric deep learning is very much the
question of how do you incorporate symmetry into a learning problem.
But before we go into that, I just want to recall the notion of an inductive bias, which
is a very important notion in machine learning.
So what's going on here?
So it's very common situation in maths that you have some problem and you know or suspect
something about the solution.
So a basic example would be something like we expect the solution to be smooth or we
hope the solution is smooth.
The solution should satisfy conservation of energy, for example, there'll be some differential
equation that the solution should satisfy.
The solution should be invariant under a group, so this occurs all the time in physics.
The solution might be locally determined, the counter example, so also we might be looking
for a counter example, we might suspect something about the counter example.
So the fancy names for this are inductive bias or prior.
So inductive bias just means something that you know about the problem a priori before
starting to solve it.
And it's very important to remember what inductive biases there are in a problem.
So how does one imagine that one has some inductive bias?
So for example, the solution might be smooth.
So this is related very much to regularization.
So in this was Gayog's talk last week.
So for example, he talked about this Gaussian kernel, which very much encourages functions
that have you know Fourier modes that aren't wiggling around too quickly, that are smooth
in a very strong sense.
Solution should satisfy conservation of energy, this might be another example of regularization.
There's also these fascinating things, if you want to have a Google called physically
informed neural nets.
So here you don't require the solution to satisfy some differential equation, but you
add the differential equation to the cost function.
And so it encourages the solution to satisfy a differential equation.
Problem should be the solution should be invariant under.
So this is today.
That's the subject of today.
The solution might be locally determined.
So this is like examples might be CNN's or LSTM.
Okay we expect for example small part areas of small parts of an image to play an important
role initially.
And the counter example is probably highly connected.
I included this example because this is an example where I've got no idea how to put
this kind of information into a network.
And this was definitely my experience with DeepMind in that often, so the movement from
an inductive bias to the neural net design is art rather than science.
So several times with the DeepMind team I said oh you know I know this about the solution
and they said we have absolutely no idea how to incorporate this into the model.
And I think that one should be aware that this is often a big issue.
So yeah one should be just be aware that it's very important to have some kind of inductive
bias and be aware of it but you might not know what to do with it.
And this is just the same slide that very similar to things that Georg has been saying.
You have the capacity of a model roughly speaking how many parameters it has and there is this
playoff between simplicity and expressivity.
So a lot of parameters means you can express for example any function but training may
be infeasible you might have hundreds of billions of parameters and it's less interpretable
and so there's often sweet spots but in this trade off.
Okay so today what I'm talking about is symmetry in neural networks and I'm really extremely
happy to give this talk because it's been kind of a revelation for me.
So I began with this book Geometric Deep Learning by Bronstein Bruner Cohen and Petar
Velichkovich who I worked with on the DeepMind project and Gayog pointed out this group here
with very convolutional networks and Petar recently just pointed out this steerable CNNs
and it's very interesting subject and I think it's a really remarkable example.
So in physics we often see this phenomenon where just knowing some kind of symmetry is
present has enormous effects on your ability to solve a problem or formulate a model or
something like that so there's this extraordinary paper I think it's by gross called symmetry
and physical laws which I found really inspiring and I find this
equivariance in convolutional neural nets to be a kind of similar story it's like it
seems so innocent to require some kind of invariance and yet it essentially determines
your entire architecture it's really remarkable.
So never underestimate symmetry I had this on my web page for about 10 years as the first
thing that you read.
So I want to review what kind of vanilla CNN is and then so I'll first just remember
what a CNN is and then we'll view a CNN through the language of group theory and I want to
try to convince you that three basic principles already basically determine CNN architecture.
So here's my image in the top right so this is a grayscale image I'm assuming that it
is on a square so I have a fixed width and a fixed height a fixed number of pixels wide
a fixed number of pixels high and I have my pixel value is given by a real valued function
so for example this pixel value might be between zero and 255 and what I'm looking for is some
function from this is called a so I'm calling this a periodic image because I'm kind of
wrapping the top you know the physicists would say I'm compactifying on the torus and you
know I sound very fancy when I say that but we're just simplifying our situation by imagining
that our image is on the torus and there's it just simplifies the group theoretic discussion
in a second and there's no genuine need to do this.
So we're seeking some function from periodic images i.e functions on this grid to the real
numbers which are for example positive on tigers and negative on non-tigers is a classic
and a machine learning problem and also the problem on which machine learning has been
wildly successful and we do this in the following way so we have several layers and typically
these layers will consist of other periodic images perhaps at lower scales so when in
the first two layers of this neural net I'm assuming that my periodic image is the same
scale and then in the third layer I'm assuming that the periodic image has dropped in scale
somewhat so I'm this h is assumed to h prime is assumed to divide h.
So what one should think about this problem so this problem this seek so we're seeking a
function from here to here and this function is going to be highly non-linear
so it's a non-linear function on this big vector space and I guess one of the points
of machine learning is that learning a highly non-linear function on a high dimensional vector
space is a very difficult task and we get enormous amount of mileage out of viewing it
as composed of out of rather simple functions so the simple functions that we use are convolutions
so this might be like we might have some kind of filter here
the filter and this might be for example 8-1-1-1-1-1-1-1-1-1-1-1-1-1 and this particular filter
we point wise multiply with our image and then sum up the result and so this particular filter
would have the effect that on areas of blank color we would get a very low value but on
out on edges we would get a high value so it would be have a kind of um outlining effect
so this is an example of something that we might uh convolve with so that's applying this
filter can be rephrased as a convolution and what we do so we convolve and then we apply
apply a value and then we convolve and we apply a value and then we do an operation called pooling
that I'll basically ignore here which might be you look at a grid of pixels and take the maximum
element so this will take you down to a smaller image and then finally at the end you might do
some fully connected layers and the point of all this is that we don't we specify this architecture
uh at the beginning and we specify all the values at the beginning but we learn the convolutions
that's the important point we learn these filters
okay so that was meant to just be a review now I want to look at this through the language of group here
so this is just a direct copy of the previous architecture
so z mod h squared sorry z mod hz squared is a group so it's z mod hz times z mod hz
very simple abelian group
and as with any group it acts on functions on that group
so if I have a function on my group what I can do is translate my group around and I get a new
function okay and convolving by a single so when I translate around by my group this is the same
thing as convolving with a delta function on my group so on my group I can consider the function
that's just one at a particular group element and zero elsewhere and convolving with that element
is the same thing as translating by that group element
and uh so any any convolution is a linear combination of these convolution with a single
with a delta function and so this is gamma equivalent so another way so in the abelian case
kind of nothing nothing matters in terms of orders you know when I wrote g dot f of x equals f of
g plus x it doesn't matter whether I whether I write x plus g or x minus g but in the non-abelian
case I'd have to have an inverse and in the non-elabelian case I would kind of think about
convolutions as maybe acting from the right or something like that but it's a general fact that
the if we look at functions on a group then the equivalent maps from functions on a group to
functions on a group are the same thing as functions on that group acting by a convolution
and that's what I that's what I say here so the equivalent maps from functions on the group to
functions on the group are simply functions on the group okay and this is true for any gamma
this is very basic representation theory if you will
so remember that I hate questions and any question will involve um a horror show
so please don't ask any questions
so what are the basic observations about this so we want often in these problems we want a
gamma invariant answer so if we move our picture of a cat around if we translate around the answer
should still be cat okay this is one of the reasons that I assumed a periodic
periodic image that's very important another point is that so there's another classic machine
learning task which is called image segmentation which asks us to say where are the two eyes in
this picture or you know when your phone for example tells you where the head of the people
this is an example of image segmentation now if you think about what this task is
this is not invariant it's equivariant this means that if I move the image
my my prediction should move in the same way so we often want a gamma invariant answer or a gamma
equivariant answer
okay convolution and value are gamma equivariant and for simplicity I'm ignoring pooling layers
but we can definitely add them into the discussion but I feel like the the guts of this business
really is exposed when we ignore pooling so I'm going to do that from now on
so so convolution is equivariant and and value is equivariant if we so we have our image we
have a whole bunch of real numbers if we translate and then set some of those numbers to zero that's
the same thing as setting some of those numbers to zero and then translate
locality yeah no any activation function would be equivariant but as we'll discuss in a second
it's really essential that these are permutation representations for an active function activation
function to be equivariant so so we have requirement one we want a gamma invariant answer
requirement two is that everything going on in this network should be gamma equivariant
requirement three is locality so what this says often in you know if you look apparently at the
early layers of the brain in the visual cortex what happens is local and so and it's very natural
to also impose this in a cnn so we our our filters are supported around the identity initially
and then later on we let them grow out through pooling and potentially fully connected layers
so an actual cnn we wouldn't require we wouldn't look at periodic images and what we would do
is pat around the edge um to make all our images the same size for example yeah and we would only
have we the same principles would be there but we wouldn't have a kind of full symmetry that only
makes sense to shift pictures a little bit but what I find remarkable here and it's kind of a
simple simple thing is that gamma invariance gamma equivariance and locality basically tell me what
I what I have to do in my neural net so if you assume that you should compose it out of simple
functions and if you fix relu then everything else is basically specified which is remark which seems
to me to be remarkable and what I want to explain soon is that there's kind of nothing special about
this particular group that this would actually tell us how to make predictions on any space on
which a group acts transitively so let me just emphasize one extremely important point from a
implementation point of view okay so just for completeness gaug asked here whether
relu was specific for it being equivariant and the response was no any non-linear activation
function would be fine at this point as long as our representation is a permutation representation
and what Stefan asked was in this particular picture here um you know for completeness
if we translated this little cat we'd have half the cat's head over here and half the cat's head
over here and the question was you know how does an how does an actual cnn in real life
do this and basically you know we don't we don't enforce that full equivariance we only
allow kind of small translations within some bounded region so kind of partial symmetry
thank you very much for the reminder
so this is very important from a basic implementation point of view imagine on the left hand side we
have two we this is a layer of our neural net and so we have l1 inputs and l2 outputs
single layer so now if you think about the number of parameters it's l1 times l2
so if it's easy to find a picture for example with 10 million uh 10 million pixels in it
and if we do one layer then that's whatever 10 million squared is you know i'm not a physicist
i don't know what 10 million squared is okay so some enormous number that i can probably never
actually train on a computer and but if we're doing so here here i have a first layer of a
convolutional neural net in one dimensional in one dimension so here are my inputs
and locality says that my filter only affects neighboring points so that's why i have these
three parameters x y and z and equivariance says that these x y and z are the same across the whole
thing so no matter how big this layer is it just depends on three size um three parameters okay
so i should emphasize this is one piece of the first layer so up here this would be one of these
pieces so i could expect to have potentially 20 of these pieces or something but still some you
know 20 times 3 is a lot smaller than 10 million squared i'm enough of a physicist to know that
inequality okay so now i want to explain a blueprint for learning on a general homogeneous
space for a group so we have our group and i'm typically thinking about a finite group or a
league group like so three or something like that so gamma was z mod h z squared before
and x is a transitive gamma set so this just means that um gamma acts transitively on x but
in the category of leap in the category of differential manifold this would mean that
i have a manifold with a continuous action of my or a smooth action of my league and in any
situation in which um this makes sense we have that x is just the same thing as gamma mod a
single stabilizer and what we want to do is learn an invariant so i'll stick to the invariant case
but notice that we might also want to make an equivariant prediction function from functions on
x to r now very basic representation theoretic observation or maybe so basic but it's not yet
representation theory is that because our action is transitive there is only one linear
uh or at most one linear map from functions on x to r that is invariant namely
like summing over my finite set or integrating or so i found this kind of illustrative because
this tells you for example in the image classification task you're definitely looking for a nonlinear
function because a linear function would be like averaging over pixel values and this is the kind
of silliest thing that one could imagine so we're looking for some um invariant function
and here's the blueprint so we fix a transitive gamma set this is where we want to make the
prediction and then we just our architecture consists of a whole lot of choices of transitive
gamma sets and basically i think one way to think about these transitive gamma sets is
so the the invariant prediction says that you want to take um so any gamma has an important
transitive gamma set namely one point and this is where we want our prediction to end up and then
if you look at um classical c and n's you want your sets to kind of slowly get smaller in some
sense until you reach the prediction so you can think about these transitive gamma sets
as slowly decreasing in size if that makes sense and this is all we do so we um consider some
equivariant maps so convolution so this should be a gamma a gamma equivariant
linear map
and then we never do a relu and then we do another one and then we do a relu and then
we do another one and we want to train across the parameters of gamma equivariant linear maps
yeah and probably this is a point of it so this whole space is r
so if gamma has a metric or similar we might want convolution supported near the identity
now this is the most important point and i think that this has kind of been missed in the
in the machine learning literature there's something very basic in representation theory
which i call the double coset formula people might call it hecar algebras there's many
different names for it so we're asking what is such a map so because any transitive set is
simply gamma mod h or gamma mod h prime we want to know what this home space is
and the formula says that homomorphisms from such a function space to such a function space
are simply functions on double cosets
now there's many different ways to understand this formula if you if you're in the world of
finite groups this is a very nice exercise if you're in the world of compact league groups it's a
significantly more difficult exercise but i just want you to accept this formula as a kind of
beautiful thing in the world and we'll see it's very useful okay and if you want to know more
about it i'm very happy to talk more about this formula okay but this is a kind of i don't know
very useful formula in many different situations so this is telling us what the possible space of
convolutions is so here's an example imagine that we're learning on a sphere so we have a nice sphere
here and we have so three so this is um orthogonal three by three matrices of determinant one so
these are these transformations so it acts on the sphere and s2 is a transitive space i can move
any point on the sphere to another point on the sphere of iron orthogonal transformation
what is the stabilizer of single point it's those rotations in the axis that point determines through
the origin so s2 is s03 mod s1 now imagine that we want to learn on the sphere so we want to have
some image on the sphere some function on the sphere and we want to say it's a cat or something
you might ask i don't generally see pictures of cats on spheres this is my answer to that
okay this is a beautiful article in quanta so this is the cosmic background radiation
no absolutely extraordinary thing from around about 2003 where we see the early what the universe
looked at early on and we do this by basically going around the world and looking out into space
and so it's an archetypal example of an image on a sphere
so sam has a question um we'll just i just want to admire this picture for 10 more seconds
so i'm told that you can see the fluctuations of quantum field theory in this picture so
this is a very early universe so it's when the universe was very small and you expect
the behavior to be given by the laws of the very small and i'm told that you can see
evidence of quantum field theory in this picture that totally blows my mind okay so sam's question
for discrete finitely generated gamma could support near the identity be regarded as having
sort some sort of choice of generator that's a really good question i was thinking about
what the support nearly up near the identity kind of means so in the example of the cnn
we have this discrete group which has no no really convincing metric on it but it is embedded in
s1 times s1 that does have a good metric on it and so for groups that come with some embedding
we can put a metric on them but i also think that that's a good suggestion if you have a
some kind of uh what's what's that distance you're talking about it's um like kind of
distance in the kaillie graph might also be a a decent measure of locality i also want to try
to explain in a second that for a non-Abelian group locality is less important so so the building
blocks so what are the homogeneous spaces for so three so this is in a league group so i can
ask what are the dimensions of the subgroups of s03 so there's a whole lot of interesting finite
subgroups of s03 for example the symmetries of the icosahedron form a very interesting
subgroup of s03 and um and then you have the two sphere and rp2 and then you have a point
and that's it okay so our building blocks are rather restricted which is interesting
and also i would say that we if you're employing some kind of practicality in building your model
you don't want complicated things like s03 modified subgroup so
and rp2 and s2 are very very similar you know one is just a two-fold cover of the other
and so i would advocate building a building a network which just involves functions on
spheres and functions on a point so this is the proposal for a blueprint for learning on the sphere
and also i am aware that it's very difficult for a computer to understand a function on s2
okay but this is meant to be some kind of blueprint that you then try to interpret
and sometimes you know to have the idea of what you're doing very clearly in your head is very
useful when you come to implementing something here we have h equals h prime exactly so i'll
go through the double coset formula in two examples now so the point the the problem here
which geyog is pointing out so geyog was asking which which subgroup are we using the double
coset formula in here i just want to first say why we're using the double coset formula
we want to know what are these maps what are our possible so three every variant convolutions here
so what are the so three every rank convolutions so this is the double coset formula again this is
our friend okay i'm just specializing the double coset formula for so three so that made the formula
much less easy to read so i'll delete it again okay so what does this say let's first do a silly
example what are the homomorphisms from s03 mod s1 i.e s2 to s03 mod s03 namely a point
so i said as an exercise in very great generality the only such function is given essentially by
integrating over your space up to a scalar but let's see it pop out of the double coset formula
so s03 mod s03 is a point yeah so we've got functions on a point what's more interesting
so this is the kind of silly example what's more interesting is what are the rest of the layers
you know so just to emphasize here this is telling us that even though this is an enormous vector
space with this only one scalar possibility of of maps here so this belongs to r this belongs to r
this belongs to r
so now ignore the integral bit at the moment just look at this so what are the s03
equivariant homomorphisms from functions on s2 to functions on s2 they're functions on by our double
coset formula s1 mod s03 mod s1 so that's the same thing as s s1 mod s2 so i take s2 and i
have s1 rotating it around and then i quotient that out and our representatives for that quotient
space are just an interval stretching from one pole to the other so functions on that interval
so what the hell are these intertwiners so i should say that such an element inside here
is called an intertwiner
so intertwiner is synonymous with s03 equivariant linear map
so i can look at one way to understand these things is to try to look at delta function so a delta
function at the identity at this base point is just the identity a delta function at this end
is the antipode but what the hell is going on in the middle you know you're you're seeking a
continuous family of operators which interpolates between the identity and the antipode okay looks
like a tough ask but there's a really beautiful thing you can do which is you consider the
following operator on function so i have a function on the on the two sphere so this is in
on s2 and now and i have a gamma and i can consider a new function which at a point x is
given by the integral around a loop of my original function at distance gamma
from my point there's a way of producing a new function so i've told you how to take a function
s2 get a new function on s2 and it's a beautiful thought exercise that this is invariant this
this is equivariant okay so if i move my function and then do this operation that's the same thing
is doing this operation and then moving my function okay so these are these
intertwiners so
so that's the answer for what all these maps are and of course like that's still a infinite
dimensional vector space but compared to you know functions on like if you're just thinking
about linear maps here that's something like functions on s2 times s2 so it's like a four
dimensional roughly speaking and it's kind of remarkable that just in employing this
equivariance massively cuts down the number of parameters and of course you can make this whole
picture even richer using spherical harmonics and there's like incredibly nice functions to put in
here projecting to the irreducible representations inside functions on s2 etc i should have said
very very much earlier like for me fun is just firstly it's to remind us that we're having fun
secondly um it's just some kind of class of function so when i'm talking about the sphere
i'm probably talking about l2 functions when i'm talking about a um discrete set i'm just talking
about any function etc so yeah the point is that when we have a non-Abelian gamma there's a big
reduction in parameters so um in the cnn slide there was this equivariance plus locality drastic
reduces the number of parameters and here i'm kind of saying that equivariance plus non-Abelian
drastically reduces the number of parameters which i think is very interesting so my task for myself
and if you have any ideas i'd love to hear it is find an interesting learning problem
where the symmetries are an interesting non-Abelian group what's a learning problem where the
symmetries are naturally sl2 fq or something like that or you know some interesting groups so a lot
of the groups that show up in machine learning are very much related to um three-dimensional space
or two-dimensional space or so like p4 which consists of um all translations and 90-degree
rotation shows up a lot and stuff like that but it would be lovely to inject some really
interesting groups into this oh gl2 yeah gl2 would be great
yeah that's it so stefan just suggested learning on hyperbolic space and that's a great
great suggestion yeah i don't know why i didn't think of that i had fl2 uh sorry yeah so learning on
okay there's enormous um possibilities here that i think are very interesting
so let let's just go back to cnn's so the the question is basically like what the hell's going on
so let's imagine so i'll try to explain um what the hell's going on and then we can have a short break
so let's say that we're trying to do image processing so we're z mod hz squared we have functions on this
and then we we want to have a layer of our neural net
so typically
one layer of our neural net might be like this you know one piece of one layer of our
neural net might look like this so now what's the dimension of this space
the dimension is h squared namely the number of
points in the set and what's the dimension of this space the dimension whoops the dimension is h square
okay so now if i were doing a fully connected neural net
i would have h squared basis vectors here h squared basis vectors here and then i would have an h squared times h squared matrix
so i'd have an enormous matrix of size h to the four that's and each of those parameters i have to train
okay what do i do in cnn's i say i want this matrix to be invariant
that already cuts down the number of parameters from h to the four back down to h squared
and then i say i want this matrix to satisfy locality
and that cuts down my number of parameters from h squared to nine
so harini is asking uh what basically like why do you restrict the parameters to this extent
so i would say that there's two reasons for this
so these are inductive priors so they're not there's something that i believe is true about the solution
they're not something that like is definitely true about the solution there's some category of practicality
i want to build a model that works i can't train a hundred billion parameters but i can train
you know a hundred or a thousand fine yeah and the inductive priors in this are invariants
namely i can see you i can still see you yeah that's you know like you're still there like that's
invariance yeah um and the other thing is locality and i think locality makes a lot of sense like when
i look at this room i don't think the first thing that happens in my brain is i think oh i'm in
kaslo 273 what happens first is i go oh edge corner chair person stairs light looks like a lecture hall
sydney probably kaslo 273 yeah and so that's invariant that's um locality and these are our
inductive priors and these inductive priors massively cut down the parameters and then from
then we're cooking on gas and we can get these models that actually work no but it's again like
changing the group is a gain inductive priors so um for example like you know have you been upside
down and you look and it's actually much harder to recognize stuff so the idea that we satisfy this
invariance is much less well established than the idea that we satisfy this invariance and then we
want to bake in that symmetry exactly yeah i don't know if you really want rotations so like a classic
pre-training task in image recognition is to recognize whether your image is upside down or not
so you know that's a classically non-invariant thing under rotation by 180
but there's there's situations like you know in an MRI scan or something
where it really you know you really want that invariance that's when you should bake it into the model
if you go back to our friend l2 of s2 there's a great exercise so the laplacian
okay so the casimir gives you the laplacian on the sphere and the eigenspaces for the laplacian
are the spherical harmonics so the the casimir is acting everywhere in this whole big diagram
commuting with everything and splitting it up into irreducible it's not so much locality it's the
fact that so three is maybe i can write so you know l2 of s2 is a topological direct sum of l gamma
where gamma is
is a spherical harmonics
and the the casimir is providing this decomposition so it's breaking
breaking up this space so the casimir on each one of these acts by a different scalar
and it's the the casimir aka the laplacian acts on each of these you know this is something like
restriction of of degree
gamma or gamma over two polynomial polynomials
and so you have this totally canonical decomposition of this space of functions
and everything wherever i had this picture everything here is respecting this decomposition
all the linear maps respecting this decomposition
and roughly speaking you can kind of think about like you know if you take your function here
and do a Fourier expansion of it then you get a whole lot of quantities
and those quantities are giving you the
you get a whole lot of scalars and those scalars are basically telling you how it acts on each one of these
okay so this is just super interesting for me but maybe more technical
uh so i want to go in the second half i want to go over why permutation representations i want to go over
something called deep sets and then i want to go over graph neural nets
so one question that is very natural to ask as a representation theorist
is why so if you take functions on a set on a gamma set this is called a permutation representation
and it's called a permutation representation because if you look at the matrices that represent
your elements they're permutation matrices and in our first class in representation theory the
first representations we see a permutation but then we quickly convince our students that
we should break up permutation representations into irreducible representations and that's
really interesting so why am i insisting that we have permutation representations everywhere
so i i found that this charming little lemma which i i didn't find in the literature
which is if you have a representation of gamma which is assumed finite i'm not quite sure what
the analog of this for a league group is then we so once we have v we can choose a basis for it
and once we have a basis we can ask is relu gamma equivalent yep so remember relu takes our
vector which now that we have a basis is just a sequence of real numbers and the ones that are
negative it sets to zero and the ones that are positive it keeps so is this gamma equivalent
if our representation is just permuting our coordinates around it's easier to see that
it's gamma equivalent but uh that's if and only if so in order to have a gamma equivalent
relu with respect to some basis you have to be permutation with respect to that basis
now this is something that has been exciting me a lot over the last few days and is having
a hell of a lot of fun with is basically like piecewise linear representation theory
um so what you could imagine is you you have these layers and they all break up into irreducible
representations and if you include an irreducible into a permutation representation do a relu and
then project back you get a piecewise linear endomorphism equivariant endomorphism of your
representation and so what you could imagine is networks in which you have irreducible
representations together with equivariant non-linearities and my impression is that this
is extremely interesting and I've already learned like basic things about representations that I
didn't know from thinking about this so if you're interested in this ask me but it's kind of much
more specialized so I'm not going to talk about it today so let's I want to do another example
of our blueprint so one way of seeing this is just first think about this
so imagine that we have a whole lot of points and they're unordered and they have a whole
lot of information attached to them for example you could imagine all of the citizens of Sydney
and they're labeled by their age and how much tax they paid last year yeah so I've got a two
dimensional vector associated to every person in Sydney now one way of viewing this data set is
as a point cloud so what I have is an enormous in this you know age and tax example I just have R2
and I have an enormous number of points in R2 and I want to make some qualitative statement so a basic
statement that I could make is some kind of like center of mass statement like the average age of
people in Sydney is blah that would be a kind of boring measurement but a much more interesting
measurement would be there's this kind of weird hole in this data for example you know a particular
age in a particular tax is you know not paid in some region or something like that that would be
a much more interesting statement you can make about this data analysis and there's about this
data and this is one of the one of the this is related to this very interesting subject called
persistent homology which I know next to nothing about uh okay but we have a point cloud so we
want to make a prediction based on this point cloud and so this is an equivariant prediction task
we have so uh so here we have rd to the n so here are our n different points
uh and we want to learn some function you know like is there a big hole in this data or something
like that um and it's it's convenient to swap the indices so if you think about how s n acts here
it acts like I have a whole packet of numbers um and then it permutes them around but it's more
useful to view this from the from the point of equivariance as one packet of numbers like age
that's being permitted around and one packet of numbers like how much tax was paid being
commuted around and so I'll do this innocent rewriting um but I'm just pointing this out so
it doesn't confuse the hell out of you on the next slide uh so we want to make an s n invariant
prediction so basically we have in the language of representation theory we have a whole lot of
copies of the most basic permutation representation of s n namely s n acts acting on functions on
an n set and we want to make an s n invariant prediction so let's do some basic representation
theory which I almost certainly learned from andrew at some point in about 30 or something
so we take functions from one up to n functions on the set one up to n so this is a permutation
representation of s n and we take the trivial representation so this is where every permutation
just acts by the identity so here's a whole lot of homomorphism formulas so this is before I was
saying what are the arrows in my neural net here I'm working them out explicitly what's what parameters
there are so home from the trivial representation of the trivial representation this is the same
thing as home from r to r this is r times the identity home from n to one this is another
instance of this statement that on a permutation representation the only invariant measurement
you can make is essentially summing up your entries that's that home back the other way
here if we look at the image of one we want some vector we want some function which is
invariant under s n i.e. we want this function to take the same value everywhere which is alpha
now a little bit trickier a little bit like you know this would be a second week exercise
in representations of the symmetric group or something is that home from the trivial from
this permutation representation to itself is two-dimensional and it's spanned by the identity
and the map that sums up the coordinates and and takes that sum and multiplies it by
the constant function now exercise deduce this from the double coset formula all of these formulas
are very easy concept consequences of the double coset formula why is that the case
do it if you're a student you should do this and if you're a student it's not already obvious
to you you should do this so i this is deep set architecture so you can look at so here's
here's our paper from 2017 and to me as a non-machine learning person it looks a bit
mystifying but this is just another instance of our blueprint so here's our input so this would be
our three three-dimensional point cloud input yep so we have um three parameters per point
now we do s in equilibrium maps and we uh sprinkle around ends and ones these are our building blocks
now note how crazily this reduces the number of parameters for a large n so here if we had
no assumption of s in equilibrium this would be an n squared space of parameters you know and n
could easily be a million or something but now because we want this to be s in equilibrium we've
just got two two possibilities here we've got one possibility here we've got one possibility here
we've got one possibility etc so this allows us to make enormous um networks involving you know
hundreds of billion you know billion dimensional things um with few parameters
okay and that is um deep set architecture and it's i think it's um state of the art in terms of
point cloud prediction okay graph neural nets if there's no questions
uh graphs are everywhere in mathematics they come in many forms and variants so
i just want you to keep in mind that graph here is a very um loose term it might be a
directed graph it might be a digraph a directed graph the edges might be colored the vertices
might be colored the edges might be weighted the vertices might be weighted the vertices might have
10 parameters associated to them we might be talking about hyper graphs so that's graphs where we
have like an edge need can connect more than what more than two two vertices etc so it's
whole plethora of things called graphs and just want to emphasize that there's many ways so
graphs are everywhere in mathematics but once you start thinking about them they're even more
everywhere because there's a whole lot of stuff that wasn't obviously a graph initially and then
you can make it a graph so examples of this uh you might say well you know graph theory is one
dimensional topology and i'm a sophisticated eight dimensional topologist and i only care about
eight dimensional manifolds yeah but if you take a compact eight dimensional manifold you can choose
a point cloud on it and you'll get a graph and that graph tells you enormous amounts about that
eight manifold if you have a simplicial complex you know for me like graphs are just one dimensional
simplicial complexes and so i said to petah oh we should be sophisticated and learn on
simplicial complexes and he said well a simplicial complex is just a graph truly you know here's
my triangle here are my edges and here are my vertices it's a colored a simplicial complex is
a colored vertex colored graph okay of a special form so this is another example this is from
gaorg so if we have a a data set and it's somehow embedded in a space then we can get a natural
metric graph out of it by looking at distances between vertices we might include the coordinates here
we might do some funny function applied to this these lengths etc okay so graphs are everywhere
and graph neural nets seem to be an incredibly powerful flexible way of dealing with um data so
i think the graph neural nets have kind of really genuine like c and n's the thing that's that we
stare at as mathematicians and think how could we make something like this that would help us in
mathematics but i think that graphs are actually a thing that will help us in mathematics all the
time so that's very worthwhile thinking about so what the graph neural nets do an example we might
want to learn a function on graphs so an example would be a function which is learn planarity
okay so this output's a positive number if it's planar a negative number if it's not planar so
that would be a prediction task on graphs um we also might want to know for example the
Euler characteristic of the graph that would be another example of a prediction task another
important example kind of more like image like generalized image recognition is producing
um some learning some function from functions on graphs to r so you might think that so
no you you might repackage c and n's as being a grid and then an image is the same thing as a
function on the vertices of this grid another very important thing is that um like there's many
many incredibly interesting for example embedding problems of graphs so you give me a graph and I
want to put it in some space in an interesting way um and one way of doing that would be to provide
coordinates of where I want to put the vertices of that graph and so that would be an example of
learning a function from graphs to functions on the vertices of a graph uh so I guess the takeaway
from this is that anything to do with graphs graph neural nets uh useful for as long as it's not
like an NP hard problem on graphs of which there are plenty yeah so graph neural nets aren't going
to help you solve something like is there a Hamiltonian circuit or something like that
so what's the basic idea so imagine that I give you a graph and you want to learn on it
it's enormously difficult as far as I can work out to work out the automorphism of a group of a
graph so this is something that people spend many many years thinking about from an algorithmic
point of view and so I might not know what global symmetries are present so what I was talking about
before does not apply um or that just like most graphs have no symmetry whatsoever um or you might
so Gaston is asking what do you mean by hard um so
that I mean yeah maybe maybe like NP or something like that but I just want I like in my mind there's
there's stuff on graphs which is useful and maybe not so crazily difficult like um like
embedding your graph in a nice way or something like that and then there's a whole lot of like
seemingly innocent problems on graphs that are extremely hard um like embedding a graph
improvably the nice the best way or something like that or finding a Hamiltonian circuit or
stuff like that okay so in graph land it's easy to wander into an intractable problem
um but there's also a whole lot of useful stuff that can be done so so there's plenty of local
symmetry in graphs so around every vertex we have a symmetric group of symmetry and also we have a
metric so you can imagine processes which are symmetric and kind of diffuse on the graph and
that's what a graph neural net is so I'll quickly go over the architecture
so this is an important slide so here's my graph
and as part of my architecture I fix n1
n2 n3 and of course I'm just telling you one possible variant of like a thousand different
possibilities of building graph neural net but once you've seen one of them then the other ones
make a lot more sense so we fix these n1 and then what we do is each of our layers is a sum
over the vertices of that particular rn1 okay so you know in a vanilla neural net we just fix
dimensions here we fix dimensions at every vertex so that's this and so in this particular case my
my neural net looks like this so I have
three layers so here I have some linear map here I have a relu here I have a linear map a relu
and then a fully connected layer okay so what do I do basically I train
self and neighbor maps so here's the formula down here so I'm telling you what phi x of v so here's
my my layer which is phi and I wanted to find you this map and in order to do that I can tell you
this map evaluated at a particular vertex so that this might be vertex v and what I'm saying in
order to get that answer what you do is you take this self map times whatever I've got here plus
all of these neighbor maps so roughly speaking in my second layer something here has a term that
comes from here together with terms that come from the three neighbors so it's a very natural
it's called a message pass and as usual si is something like is affine linear
okay so s1 is an n1 times n2 matrix
plus an n2 vector
okay so each of these so this is yes thank you Brian this should be an s2
yeah so that's very important that so this is an another example of an inductive prior
sorry inductive bias or a prior so what we're saying is that we want these n1
these n1s sorry Stefan asked should all these n1s be the same and the answer is in general yes
so we want for example the n1s that talk to this guy from here and from here
to be the same n1s that talk to this guy from here here okay so the n1s are the same so
if you imagine this matrix here
it looks like something like s1 s1 s1 s1 down the diagonal and then n1s
in off diagonal places given by the adjacency matrix
you know something like this so inside this space of like
n1 times vertices times n2 times vertices so this is what my big matrix would look like
I'm saying like it should be block diagonal and a whole lot of blocks should be the same
so it's a very strong inductive bias to
assume now if I'm honest you know we might have seven like let's say two different colors on
our vertices and then we would train um neighbor maps that preserve the color of vertices neighbor
maps that change the color from red to blue neighbor maps that change the color from blue
to red etc so there's a zillion variants but in the basic vanilla version of a neural net
we assume all the n1s are the same and all the s1s are the same so this one's
so I'll give the diagonal term of this s1
so it's a very complicated slide but it's a simple idea I think
so we do that for a number of times and then we evaluate or in the situation where we're
trying to learn for example and embedding or something like that we wouldn't do the final
layer we've got some coordinates on our vertices and we're happy another classic example of a task
might be you want to divide your vertices into two classes and so then you would you'd do all your
layers and then you would say at the end uh this is a real number and then you would softmax that
and then that would be the probability that your vertex is in or out of this class okay and also
there's you know a million variants often the neighbor term is weighted by one over the degree
of the vertex okay that's what I set up there it's affine maps
okay so that's the architecture and we'll have some fun playing with this architecture in the
colab tomorrow so many variants are possible for digraphs digraphs you might think that you only
train a forward map but generally you don't you train a forward map and a back with map
so back forward and backwards neighbor maps if graph has edge coloring you can train message
passing for every color of the edge vertex coloring similarly you might also have a couple
of global parameters hanging around and in every step your global parameters speak to the parameters
on the graph and are spoken to by parameters from the graph in some way which is probably nothing to
do with the n ones on the s ones so these are some nice examples imagine that I consider this
digraph here if you look at what a graph neural net does on this digraph it exactly mimics cnn's
without falling layer so you know it's it's it's not it's not an exaggeration to say that
cnn's are a subset of giant graph neural nets
and if you kind of unpack this definition for uh deeps for a complete graph you
uh basically get deepsets
and that is all for today so thank you very much and yeah if there's any questions please ask
so gay i'll ask a question i'll just ask uh answer the question of dr bouts you first
I wonder if by adding linear maps between vertices of the graph and making the whole
architecture commutative kind of quirl net has any ml interpretation yeah so I totally agree
that when you look at neural net architectures it looks very much like river varieties and
things that we study in representation theory however one really can't underestimate the
effect of this relu of like so if you think about a like just a classic feed forward vanilla neural
net um if you don't have the relu's we understand totally what happens by basically near algebra
but when we add the relu's we can suddenly approximate any function on a compact set
so and we we have absolutely no idea of what happens inside the the neural net so yeah I find
the quiver language like very useful to think to think about but um one shouldn't underestimate
relu's so gaug asked um what have graph neural nets been used for
um and my understanding is that um like I don't know like let's say half of facebook and 75 percent
of twitter is graph neural nets um because you've got all these like connection graphs and social
networks and stuff like that um graph neural nets were like the absolute center thing that we used
with deep mind too on this work on cash analysis polynomials um my understanding is that graph neural
nets are kind of um taking over neural net world in terms of they're very flexible um they're very
powerful um and a lot of tasks where for example c and n would involve like a drastic change of
architecture um in graph neural nets you can just like add a vertex or something like that so
they're very flexible powerful framework for machine learning as far as I can make out
sam yates asked for discrete valued problem could we pick other group rings say with some
choice of analogous relu like function perhaps uh that's an intimidating prospect for me because
I wouldn't know how to train and things like that so my very vague understanding so there's this
kind of revolution in the last kind of three four years given by transformers and my understanding
of that is like basically like a a graph neural net um hooked on to an lstm so like the graph neural
net kind of decides where to look back in the sentence and things like that so um but I'm not
directly aware of any recurrent technologies used directly with graph neural nets what another
thing that graph neural nets are very useful for which is kind of counterintuitive is like predicting
graph structure so you have data and you want to um and you want to predict which edges exist
so like you might want to predict social relationships or something like that and um my
understanding is that you you start off with a complete graph and run a graph neural net
and your graph kind of learns a probability of discarding an edge and that's very very powerful
so it it's intuitive and intuitive but graph neural nets can learn graphs which I think is
awesome I think I think like learning graphs is like learning graphs is a really difficult
problem in machine learning um and yeah of course so it's going to be yeah it's an intimidating
problem that or n choose tuna yeah so that's a really good question so what kind of problems
and why can be addressed with graph neural nets my understanding is that very recently there's been
kind of benchmarking so there's been like a list of 50 problems and some some attempt to decide
you know what what can various classes of graph neural net algorithms do and what they can't do
um my very rough picture as as a graph neural net greenhorn is um that anything that you can kind
of decide via a finite like a small amount of walking around your graph and if you if if you're
extremely smart and you're allowed to walk around your graph a little bit and you can already make
a decision then that's something that you could solve so something like detecting planarity or
detecting a three cycle or something this is something that you can solve but if it involves
exploring the whole graph particularly potentially in many many different iterations for example
finding a Hamiltonian cycle or something um yeah it's not going to work
she'll have said the piecewise linear representation can you say a bit more about it
sure so how about I will ask answer Joel's question and then maybe I can say something
about it but I'll give other people the chance to leave because it's somewhat specialised
so a part of Adam salt Wagner's work was learning graphs learning good people
to a particularly conjecture for instance so we might hear about learning graphs in a few weeks
yes exactly so salt Wagner's work is using reinforcement learning to produce interesting
graphs okay so now we'll declare it over and anybody that's interested in this piecewise
linear business can stick around I think this is super beautiful and um I will just explain it
briefly so let's just consider the following silly examples so we're looking at the
sn acting on s3 acting on r3 or permutation representation now we know that r3 decomposes
canonically as nat plus triv and that is the set of vectors in r3 such that the sum of the lambda
i is zero and triv is the set of r times 111 and this decomposition is completely canonical
so now you can ask the following like just kind of totally naive question if we go from nat
into r3 and then apply value
go to r3 back down to nat the composition is an s3 equivariant
pl endomorphism
i.e in the category of piecewise linear maps from this vector space to itself that a s3
equivariant this is a pl endomorphism and what is it
it's super beautiful so basically what you do is inside so here's nat so we divide up
nat so just for people that don't do this every day nat is the um symmetries of the triangle
embedded inside r3 r2 okay so we just take an equilateral triangle and we take the symmetries
of the equilateral triangle so now um there's three regions here and what happens so there's six
regions there's the blue regions and the red regions and the blue regions get squashed
and the red regions get kind of expanded out
so these get squashed
and these get expanded
and i don't know this is just like a very beautiful basic um like pl endomorphism of a
representation that i've never encountered before in my life okay and you can start having fun like
what is this
or nat inside r in and you know this is a nice exercise
and yeah so that one of the things that i find really interesting is that home
pl from any representation to r um is interesting
okay so example is that home from pl from the sign representation to r
contains the absolute value map
um like this is for this is the sign representation of s2 oh no sign representation in general
in fact um but home from the trivial representation pl to any irrep
not equal to the trivial zero
and i kind of feel like this is telling us something remarkable that about kind of how
equivalence kind of flows through a neural net so this is still very speculative but what
i feel like is that you have some kind of measure of complexity so it like at the start you have all
all irreps
and then at the end you have the trivial and then you have the maps in the in the neural net so you
have linear and then you have pl and then you have linear pl and these pl maps have a definite
sense of direction like you know once we get through the trivial representation we can never
get out of it again and like i don't know this seems to explain some some very interesting
aspects of neural nets but it's just exciting stuff that i've been thinking about last week so
very unbaked um so maybe when it's baked i can uh talk more about it
so thanks are the blue rays there inside the reflecting hyperplanes
this is alpha one and this is alpha two
so no oh yeah the so the reflecting hyperplanes would be like this and this and this
thanks everyone i think we'll stop now
you
