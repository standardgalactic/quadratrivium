start	end	text
0	13200	So, welcome to this week's lecture.
13200	16800	So, seminar structure.
16800	22240	Next week there'll be some linear combination of Gayog and Geordi, and the constant is yet
22240	24040	to be determined.
24040	29200	The week after, so after next week we move into the talks from experts.
29200	33820	So, we've been trying to give you background and then the experts will tell us what's really
33820	34820	going on.
34820	38440	So, I'm really excited by the lineup.
38440	45680	So, Adam Saltbaugner has done some amazing work using reinforcement learning to construct
45680	48880	counter examples in graph theory.
48880	54720	If you want to prepare for his talk, the paper is really beautifully written.
54720	57640	Bandad Hosseini works on graph methods.
57640	63120	So, we'll see a bit of graph neural nets today and we'll see more in his talk.
63120	70040	Carlos Simpson is talking on a kind of archetypal garden of how to use machine learning and
70040	72760	mathematical proof.
72760	76360	And I think that that's extremely interesting.
76360	82960	And then there's another three talks lined up after that, taking us to the end of semester.
82960	86760	And the other thing I wanted to point out is Joel has been doing an amazing job with
86760	87760	the toe labs.
87760	91440	If you've been taking part in the tutorials, you'll be aware of this.
91440	95800	But if you're watching online and you just want to muck around with something, they're
95800	98360	really great and I'm hoping that they provide a good resource.
98360	103160	So, we'll hope to keep these going for the expert talks so you can play with some of
103160	106760	Adam's ideas, play with some of Bumdud's ideas, et cetera.
106760	111200	So, again, the seminar principles.
111200	117440	So, today we'll be about geometric deep learning and geometric deep learning is very much the
117440	124520	question of how do you incorporate symmetry into a learning problem.
124520	132560	But before we go into that, I just want to recall the notion of an inductive bias, which
132640	135360	is a very important notion in machine learning.
135360	138520	So what's going on here?
138520	145040	So it's very common situation in maths that you have some problem and you know or suspect
145040	147680	something about the solution.
147680	151840	So a basic example would be something like we expect the solution to be smooth or we
151840	154520	hope the solution is smooth.
154520	159560	The solution should satisfy conservation of energy, for example, there'll be some differential
159560	162160	equation that the solution should satisfy.
162160	167320	The solution should be invariant under a group, so this occurs all the time in physics.
167320	172800	The solution might be locally determined, the counter example, so also we might be looking
172800	177480	for a counter example, we might suspect something about the counter example.
177480	183520	So the fancy names for this are inductive bias or prior.
183520	187520	So inductive bias just means something that you know about the problem a priori before
187520	188840	starting to solve it.
188840	196480	And it's very important to remember what inductive biases there are in a problem.
196480	200800	So how does one imagine that one has some inductive bias?
200800	203280	So for example, the solution might be smooth.
203280	210880	So this is related very much to regularization.
210880	218400	So in this was Gayog's talk last week.
218400	224280	So for example, he talked about this Gaussian kernel, which very much encourages functions
224280	232280	that have you know Fourier modes that aren't wiggling around too quickly, that are smooth
232280	234880	in a very strong sense.
234880	238840	Solution should satisfy conservation of energy, this might be another example of regularization.
238840	242880	There's also these fascinating things, if you want to have a Google called physically
242880	250360	informed neural nets.
250360	255920	So here you don't require the solution to satisfy some differential equation, but you
255920	258560	add the differential equation to the cost function.
258560	263720	And so it encourages the solution to satisfy a differential equation.
263720	266400	Problem should be the solution should be invariant under.
266400	269640	So this is today.
269640	271600	That's the subject of today.
271600	273720	The solution might be locally determined.
273720	279720	So this is like examples might be CNN's or LSTM.
279720	284240	Okay we expect for example small part areas of small parts of an image to play an important
284240	287000	role initially.
287000	289240	And the counter example is probably highly connected.
289240	293600	I included this example because this is an example where I've got no idea how to put
293600	295600	this kind of information into a network.
295600	303800	And this was definitely my experience with DeepMind in that often, so the movement from
303800	310480	an inductive bias to the neural net design is art rather than science.
310480	316800	So several times with the DeepMind team I said oh you know I know this about the solution
316800	322040	and they said we have absolutely no idea how to incorporate this into the model.
322040	329520	And I think that one should be aware that this is often a big issue.
329520	335520	So yeah one should be just be aware that it's very important to have some kind of inductive
335520	343760	bias and be aware of it but you might not know what to do with it.
343760	350560	And this is just the same slide that very similar to things that Georg has been saying.
350560	357360	You have the capacity of a model roughly speaking how many parameters it has and there is this
357360	360240	playoff between simplicity and expressivity.
360240	367960	So a lot of parameters means you can express for example any function but training may
367960	373680	be infeasible you might have hundreds of billions of parameters and it's less interpretable
373680	381280	and so there's often sweet spots but in this trade off.
381280	388640	Okay so today what I'm talking about is symmetry in neural networks and I'm really extremely
388640	393240	happy to give this talk because it's been kind of a revelation for me.
393240	399040	So I began with this book Geometric Deep Learning by Bronstein Bruner Cohen and Petar
399040	405160	Velichkovich who I worked with on the DeepMind project and Gayog pointed out this group here
405160	410960	with very convolutional networks and Petar recently just pointed out this steerable CNNs
410960	417600	and it's very interesting subject and I think it's a really remarkable example.
417600	423720	So in physics we often see this phenomenon where just knowing some kind of symmetry is
423720	428280	present has enormous effects on your ability to solve a problem or formulate a model or
428280	432840	something like that so there's this extraordinary paper I think it's by gross called symmetry
432840	439480	and physical laws which I found really inspiring and I find this
439480	444080	equivariance in convolutional neural nets to be a kind of similar story it's like it
444080	448720	seems so innocent to require some kind of invariance and yet it essentially determines
448720	452800	your entire architecture it's really remarkable.
452800	458160	So never underestimate symmetry I had this on my web page for about 10 years as the first
458160	460920	thing that you read.
460920	470120	So I want to review what kind of vanilla CNN is and then so I'll first just remember
470120	477040	what a CNN is and then we'll view a CNN through the language of group theory and I want to
477040	486440	try to convince you that three basic principles already basically determine CNN architecture.
486440	494360	So here's my image in the top right so this is a grayscale image I'm assuming that it
494360	499560	is on a square so I have a fixed width and a fixed height a fixed number of pixels wide
499560	507840	a fixed number of pixels high and I have my pixel value is given by a real valued function
507840	515120	so for example this pixel value might be between zero and 255 and what I'm looking for is some
515240	521160	function from this is called a so I'm calling this a periodic image because I'm kind of
521160	529640	wrapping the top you know the physicists would say I'm compactifying on the torus and you
529640	534320	know I sound very fancy when I say that but we're just simplifying our situation by imagining
534320	540160	that our image is on the torus and there's it just simplifies the group theoretic discussion
540160	543400	in a second and there's no genuine need to do this.
543400	551280	So we're seeking some function from periodic images i.e functions on this grid to the real
551280	558760	numbers which are for example positive on tigers and negative on non-tigers is a classic
558760	562680	and a machine learning problem and also the problem on which machine learning has been
562680	576280	wildly successful and we do this in the following way so we have several layers and typically
576280	587200	these layers will consist of other periodic images perhaps at lower scales so when in
587200	591880	the first two layers of this neural net I'm assuming that my periodic image is the same
591960	598360	scale and then in the third layer I'm assuming that the periodic image has dropped in scale
598360	603400	somewhat so I'm this h is assumed to h prime is assumed to divide h.
609080	614680	So what one should think about this problem so this problem this seek so we're seeking a
614680	620120	function from here to here and this function is going to be highly non-linear
621960	628520	so it's a non-linear function on this big vector space and I guess one of the points
628520	634040	of machine learning is that learning a highly non-linear function on a high dimensional vector
634040	641240	space is a very difficult task and we get enormous amount of mileage out of viewing it
641240	648840	as composed of out of rather simple functions so the simple functions that we use are convolutions
648840	652120	so this might be like we might have some kind of filter here
655720	666920	the filter and this might be for example 8-1-1-1-1-1-1-1-1-1-1-1-1-1 and this particular filter
667720	674840	we point wise multiply with our image and then sum up the result and so this particular filter
674840	680920	would have the effect that on areas of blank color we would get a very low value but on
680920	685880	out on edges we would get a high value so it would be have a kind of um outlining effect
687160	693720	so this is an example of something that we might uh convolve with so that's applying this
693720	700120	filter can be rephrased as a convolution and what we do so we convolve and then we apply
700920	709720	apply a value and then we convolve and we apply a value and then we do an operation called pooling
709720	714360	that I'll basically ignore here which might be you look at a grid of pixels and take the maximum
714360	720600	element so this will take you down to a smaller image and then finally at the end you might do
720600	727960	some fully connected layers and the point of all this is that we don't we specify this architecture
728680	734680	uh at the beginning and we specify all the values at the beginning but we learn the convolutions
734680	737240	that's the important point we learn these filters
739560	745240	okay so that was meant to just be a review now I want to look at this through the language of group here
749000	752600	so this is just a direct copy of the previous architecture
753320	764440	so z mod h squared sorry z mod hz squared is a group so it's z mod hz times z mod hz
766360	767960	very simple abelian group
773720	779480	and as with any group it acts on functions on that group
780360	787800	so if I have a function on my group what I can do is translate my group around and I get a new
787800	795080	function okay and convolving by a single so when I translate around by my group this is the same
795080	803720	thing as convolving with a delta function on my group so on my group I can consider the function
804360	812600	that's just one at a particular group element and zero elsewhere and convolving with that element
812600	816120	is the same thing as translating by that group element
818920	827480	and uh so any any convolution is a linear combination of these convolution with a single
827480	834280	with a delta function and so this is gamma equivalent so another way so in the abelian case
834280	841960	kind of nothing nothing matters in terms of orders you know when I wrote g dot f of x equals f of
842600	848680	g plus x it doesn't matter whether I whether I write x plus g or x minus g but in the non-abelian
848680	854520	case I'd have to have an inverse and in the non-elabelian case I would kind of think about
854760	860520	convolutions as maybe acting from the right or something like that but it's a general fact that
862360	870520	the if we look at functions on a group then the equivalent maps from functions on a group to
870520	875320	functions on a group are the same thing as functions on that group acting by a convolution
876360	882040	and that's what I that's what I say here so the equivalent maps from functions on the group to
882040	889240	functions on the group are simply functions on the group okay and this is true for any gamma
891000	894440	this is very basic representation theory if you will
899800	907960	so remember that I hate questions and any question will involve um a horror show
908920	910280	so please don't ask any questions
915160	920360	so what are the basic observations about this so we want often in these problems we want a
920360	926680	gamma invariant answer so if we move our picture of a cat around if we translate around the answer
926680	931160	should still be cat okay this is one of the reasons that I assumed a periodic
931480	940920	periodic image that's very important another point is that so there's another classic machine
940920	947960	learning task which is called image segmentation which asks us to say where are the two eyes in
947960	953720	this picture or you know when your phone for example tells you where the head of the people
953720	959160	this is an example of image segmentation now if you think about what this task is
959160	964440	this is not invariant it's equivariant this means that if I move the image
965560	972280	my my prediction should move in the same way so we often want a gamma invariant answer or a gamma
972280	983400	equivariant answer
983400	1005320	okay convolution and value are gamma equivariant and for simplicity I'm ignoring pooling layers
1005320	1009800	but we can definitely add them into the discussion but I feel like the the guts of this business
1010760	1013800	really is exposed when we ignore pooling so I'm going to do that from now on
1015720	1026200	so so convolution is equivariant and and value is equivariant if we so we have our image we
1026200	1031480	have a whole bunch of real numbers if we translate and then set some of those numbers to zero that's
1031560	1033880	the same thing as setting some of those numbers to zero and then translate
1038440	1045320	locality yeah no any activation function would be equivariant but as we'll discuss in a second
1045320	1050280	it's really essential that these are permutation representations for an active function activation
1050280	1057960	function to be equivariant so so we have requirement one we want a gamma invariant answer
1058280	1065640	requirement two is that everything going on in this network should be gamma equivariant
1066840	1074360	requirement three is locality so what this says often in you know if you look apparently at the
1074360	1080680	early layers of the brain in the visual cortex what happens is local and so and it's very natural
1080680	1088440	to also impose this in a cnn so we our our filters are supported around the identity initially
1088440	1093720	and then later on we let them grow out through pooling and potentially fully connected layers
1095000	1101640	so an actual cnn we wouldn't require we wouldn't look at periodic images and what we would do
1101640	1109080	is pat around the edge um to make all our images the same size for example yeah and we would only
1109080	1113960	have we the same principles would be there but we wouldn't have a kind of full symmetry that only
1113960	1119640	makes sense to shift pictures a little bit but what I find remarkable here and it's kind of a
1119640	1125320	simple simple thing is that gamma invariance gamma equivariance and locality basically tell me what
1125320	1130920	I what I have to do in my neural net so if you assume that you should compose it out of simple
1130920	1137960	functions and if you fix relu then everything else is basically specified which is remark which seems
1137960	1145400	to me to be remarkable and what I want to explain soon is that there's kind of nothing special about
1145400	1151800	this particular group that this would actually tell us how to make predictions on any space on
1151800	1158280	which a group acts transitively so let me just emphasize one extremely important point from a
1159240	1168760	implementation point of view okay so just for completeness gaug asked here whether
1170120	1176040	relu was specific for it being equivariant and the response was no any non-linear activation
1176040	1180360	function would be fine at this point as long as our representation is a permutation representation
1181400	1188040	and what Stefan asked was in this particular picture here um you know for completeness
1188040	1191640	if we translated this little cat we'd have half the cat's head over here and half the cat's head
1191640	1196440	over here and the question was you know how does an how does an actual cnn in real life
1198440	1204600	do this and basically you know we don't we don't enforce that full equivariance we only
1204600	1209480	allow kind of small translations within some bounded region so kind of partial symmetry
1210280	1211480	thank you very much for the reminder
1214040	1222280	so this is very important from a basic implementation point of view imagine on the left hand side we
1222280	1229720	have two we this is a layer of our neural net and so we have l1 inputs and l2 outputs
1230840	1236040	single layer so now if you think about the number of parameters it's l1 times l2
1236200	1245320	so if it's easy to find a picture for example with 10 million uh 10 million pixels in it
1245880	1254440	and if we do one layer then that's whatever 10 million squared is you know i'm not a physicist
1254440	1259720	i don't know what 10 million squared is okay so some enormous number that i can probably never
1259720	1270680	actually train on a computer and but if we're doing so here here i have a first layer of a
1271400	1277400	convolutional neural net in one dimensional in one dimension so here are my inputs
1279720	1286600	and locality says that my filter only affects neighboring points so that's why i have these
1286600	1295880	three parameters x y and z and equivariance says that these x y and z are the same across the whole
1295880	1304920	thing so no matter how big this layer is it just depends on three size um three parameters okay
1304920	1311800	so i should emphasize this is one piece of the first layer so up here this would be one of these
1311800	1318520	pieces so i could expect to have potentially 20 of these pieces or something but still some you
1318520	1326520	know 20 times 3 is a lot smaller than 10 million squared i'm enough of a physicist to know that
1326520	1338760	inequality okay so now i want to explain a blueprint for learning on a general homogeneous
1338760	1348120	space for a group so we have our group and i'm typically thinking about a finite group or a
1348120	1356040	league group like so three or something like that so gamma was z mod h z squared before
1356920	1364360	and x is a transitive gamma set so this just means that um gamma acts transitively on x but
1364360	1368680	in the category of leap in the category of differential manifold this would mean that
1368680	1376680	i have a manifold with a continuous action of my or a smooth action of my league and in any
1376680	1385240	situation in which um this makes sense we have that x is just the same thing as gamma mod a
1385240	1391880	single stabilizer and what we want to do is learn an invariant so i'll stick to the invariant case
1391880	1398520	but notice that we might also want to make an equivariant prediction function from functions on
1398520	1405960	x to r now very basic representation theoretic observation or maybe so basic but it's not yet
1405960	1414360	representation theory is that because our action is transitive there is only one linear
1414360	1424360	uh or at most one linear map from functions on x to r that is invariant namely
1425800	1434280	like summing over my finite set or integrating or so i found this kind of illustrative because
1434280	1439080	this tells you for example in the image classification task you're definitely looking for a nonlinear
1439080	1445800	function because a linear function would be like averaging over pixel values and this is the kind
1445800	1455240	of silliest thing that one could imagine so we're looking for some um invariant function
1457000	1467720	and here's the blueprint so we fix a transitive gamma set this is where we want to make the
1467800	1472600	prediction and then we just our architecture consists of a whole lot of choices of transitive
1472600	1479720	gamma sets and basically i think one way to think about these transitive gamma sets is
1479720	1486600	so the the invariant prediction says that you want to take um so any gamma has an important
1486600	1492840	transitive gamma set namely one point and this is where we want our prediction to end up and then
1492840	1499720	if you look at um classical c and n's you want your sets to kind of slowly get smaller in some
1499720	1503320	sense until you reach the prediction so you can think about these transitive gamma sets
1503320	1514280	as slowly decreasing in size if that makes sense and this is all we do so we um consider some
1514360	1521000	equivariant maps so convolution so this should be a gamma a gamma equivariant
1524520	1527880	linear map
1530280	1533080	and then we never do a relu and then we do another one and then we do a relu and then
1533080	1539880	we do another one and we want to train across the parameters of gamma equivariant linear maps
1540600	1546120	yeah and probably this is a point of it so this whole space is r
1550200	1554280	so if gamma has a metric or similar we might want convolution supported near the identity
1554920	1558680	now this is the most important point and i think that this has kind of been missed in the
1559400	1564680	in the machine learning literature there's something very basic in representation theory
1564680	1569160	which i call the double coset formula people might call it hecar algebras there's many
1569160	1580760	different names for it so we're asking what is such a map so because any transitive set is
1580760	1586360	simply gamma mod h or gamma mod h prime we want to know what this home space is
1587960	1594840	and the formula says that homomorphisms from such a function space to such a function space
1594840	1597000	are simply functions on double cosets
1600040	1604840	now there's many different ways to understand this formula if you if you're in the world of
1604840	1609560	finite groups this is a very nice exercise if you're in the world of compact league groups it's a
1610280	1617160	significantly more difficult exercise but i just want you to accept this formula as a kind of
1618280	1624360	beautiful thing in the world and we'll see it's very useful okay and if you want to know more
1624360	1631080	about it i'm very happy to talk more about this formula okay but this is a kind of i don't know
1631080	1637480	very useful formula in many different situations so this is telling us what the possible space of
1637480	1649480	convolutions is so here's an example imagine that we're learning on a sphere so we have a nice sphere
1649480	1658920	here and we have so three so this is um orthogonal three by three matrices of determinant one so
1658920	1667400	these are these transformations so it acts on the sphere and s2 is a transitive space i can move
1667400	1671480	any point on the sphere to another point on the sphere of iron orthogonal transformation
1671480	1679080	what is the stabilizer of single point it's those rotations in the axis that point determines through
1679080	1689800	the origin so s2 is s03 mod s1 now imagine that we want to learn on the sphere so we want to have
1689800	1695400	some image on the sphere some function on the sphere and we want to say it's a cat or something
1696120	1701960	you might ask i don't generally see pictures of cats on spheres this is my answer to that
1702840	1707720	okay this is a beautiful article in quanta so this is the cosmic background radiation
1708680	1716200	no absolutely extraordinary thing from around about 2003 where we see the early what the universe
1716200	1720600	looked at early on and we do this by basically going around the world and looking out into space
1720600	1723080	and so it's an archetypal example of an image on a sphere
1727800	1734360	so sam has a question um we'll just i just want to admire this picture for 10 more seconds
1734440	1742440	so i'm told that you can see the fluctuations of quantum field theory in this picture so
1742440	1746760	this is a very early universe so it's when the universe was very small and you expect
1748200	1752520	the behavior to be given by the laws of the very small and i'm told that you can see
1753080	1758520	evidence of quantum field theory in this picture that totally blows my mind okay so sam's question
1758520	1763160	for discrete finitely generated gamma could support near the identity be regarded as having
1763160	1768680	sort some sort of choice of generator that's a really good question i was thinking about
1768680	1776120	what the support nearly up near the identity kind of means so in the example of the cnn
1777240	1785480	we have this discrete group which has no no really convincing metric on it but it is embedded in
1786040	1791800	s1 times s1 that does have a good metric on it and so for groups that come with some embedding
1793160	1797160	we can put a metric on them but i also think that that's a good suggestion if you have a
1798520	1802440	some kind of uh what's what's that distance you're talking about it's um like kind of
1802440	1807560	distance in the kaillie graph might also be a a decent measure of locality i also want to try
1807560	1814280	to explain in a second that for a non-Abelian group locality is less important so so the building
1814280	1820520	blocks so what are the homogeneous spaces for so three so this is in a league group so i can
1820520	1825160	ask what are the dimensions of the subgroups of s03 so there's a whole lot of interesting finite
1825160	1830040	subgroups of s03 for example the symmetries of the icosahedron form a very interesting
1830680	1840920	subgroup of s03 and um and then you have the two sphere and rp2 and then you have a point
1840920	1846760	and that's it okay so our building blocks are rather restricted which is interesting
1846760	1853960	and also i would say that we if you're employing some kind of practicality in building your model
1854520	1860200	you don't want complicated things like s03 modified subgroup so
1862200	1867720	and rp2 and s2 are very very similar you know one is just a two-fold cover of the other
1868920	1874520	and so i would advocate building a building a network which just involves functions on
1874600	1881160	spheres and functions on a point so this is the proposal for a blueprint for learning on the sphere
1881160	1885720	and also i am aware that it's very difficult for a computer to understand a function on s2
1885720	1890600	okay but this is meant to be some kind of blueprint that you then try to interpret
1891880	1896120	and sometimes you know to have the idea of what you're doing very clearly in your head is very
1896120	1901560	useful when you come to implementing something here we have h equals h prime exactly so i'll
1901560	1907800	go through the double coset formula in two examples now so the point the the problem here
1907800	1913640	which geyog is pointing out so geyog was asking which which subgroup are we using the double
1913640	1917960	coset formula in here i just want to first say why we're using the double coset formula
1917960	1924600	we want to know what are these maps what are our possible so three every variant convolutions here
1924600	1931960	so what are the so three every rank convolutions so this is the double coset formula again this is
1931960	1941960	our friend okay i'm just specializing the double coset formula for so three so that made the formula
1941960	1946440	much less easy to read so i'll delete it again okay so what does this say let's first do a silly
1946440	1955240	example what are the homomorphisms from s03 mod s1 i.e s2 to s03 mod s03 namely a point
1956120	1960840	so i said as an exercise in very great generality the only such function is given essentially by
1960840	1965160	integrating over your space up to a scalar but let's see it pop out of the double coset formula
1965160	1971160	so s03 mod s03 is a point yeah so we've got functions on a point what's more interesting
1971160	1977000	so this is the kind of silly example what's more interesting is what are the rest of the layers
1977720	1982040	you know so just to emphasize here this is telling us that even though this is an enormous vector
1982040	1990680	space with this only one scalar possibility of of maps here so this belongs to r this belongs to r
1991400	1992200	this belongs to r
1993160	2003480	so now ignore the integral bit at the moment just look at this so what are the s03
2003480	2010040	equivariant homomorphisms from functions on s2 to functions on s2 they're functions on by our double
2010040	2018520	coset formula s1 mod s03 mod s1 so that's the same thing as s s1 mod s2 so i take s2 and i
2018680	2025160	have s1 rotating it around and then i quotient that out and our representatives for that quotient
2025160	2031720	space are just an interval stretching from one pole to the other so functions on that interval
2033560	2040760	so what the hell are these intertwiners so i should say that such an element inside here
2041400	2042600	is called an intertwiner
2042920	2053880	so intertwiner is synonymous with s03 equivariant linear map
2056360	2062600	so i can look at one way to understand these things is to try to look at delta function so a delta
2062600	2070200	function at the identity at this base point is just the identity a delta function at this end
2070200	2078040	is the antipode but what the hell is going on in the middle you know you're you're seeking a
2078040	2084760	continuous family of operators which interpolates between the identity and the antipode okay looks
2084760	2090520	like a tough ask but there's a really beautiful thing you can do which is you consider the
2090520	2097400	following operator on function so i have a function on the on the two sphere so this is in
2098360	2108040	on s2 and now and i have a gamma and i can consider a new function which at a point x is
2108040	2114280	given by the integral around a loop of my original function at distance gamma
2120200	2126280	from my point there's a way of producing a new function so i've told you how to take a function
2126600	2133080	s2 get a new function on s2 and it's a beautiful thought exercise that this is invariant this
2133080	2138920	this is equivariant okay so if i move my function and then do this operation that's the same thing
2138920	2143480	is doing this operation and then moving my function okay so these are these
2145960	2147400	intertwiners so
2150040	2155800	so that's the answer for what all these maps are and of course like that's still a infinite
2155800	2165080	dimensional vector space but compared to you know functions on like if you're just thinking
2165080	2170840	about linear maps here that's something like functions on s2 times s2 so it's like a four
2170840	2177800	dimensional roughly speaking and it's kind of remarkable that just in employing this
2177800	2183720	equivariance massively cuts down the number of parameters and of course you can make this whole
2183720	2189560	picture even richer using spherical harmonics and there's like incredibly nice functions to put in
2189560	2197560	here projecting to the irreducible representations inside functions on s2 etc i should have said
2197560	2202760	very very much earlier like for me fun is just firstly it's to remind us that we're having fun
2203800	2208920	secondly um it's just some kind of class of function so when i'm talking about the sphere
2208920	2212920	i'm probably talking about l2 functions when i'm talking about a um discrete set i'm just talking
2212920	2220040	about any function etc so yeah the point is that when we have a non-Abelian gamma there's a big
2220040	2228840	reduction in parameters so um in the cnn slide there was this equivariance plus locality drastic
2228840	2234120	reduces the number of parameters and here i'm kind of saying that equivariance plus non-Abelian
2234120	2240280	drastically reduces the number of parameters which i think is very interesting so my task for myself
2240280	2245240	and if you have any ideas i'd love to hear it is find an interesting learning problem
2245240	2251160	where the symmetries are an interesting non-Abelian group what's a learning problem where the
2251160	2257960	symmetries are naturally sl2 fq or something like that or you know some interesting groups so a lot
2257960	2263960	of the groups that show up in machine learning are very much related to um three-dimensional space
2263960	2272120	or two-dimensional space or so like p4 which consists of um all translations and 90-degree
2272120	2276440	rotation shows up a lot and stuff like that but it would be lovely to inject some really
2276440	2280200	interesting groups into this oh gl2 yeah gl2 would be great
2285320	2290040	yeah that's it so stefan just suggested learning on hyperbolic space and that's a great
2290120	2297480	great suggestion yeah i don't know why i didn't think of that i had fl2 uh sorry yeah so learning on
2304120	2308360	okay there's enormous um possibilities here that i think are very interesting
2309320	2314360	so let let's just go back to cnn's so the the question is basically like what the hell's going on
2314920	2324520	so let's imagine so i'll try to explain um what the hell's going on and then we can have a short break
2325160	2333480	so let's say that we're trying to do image processing so we're z mod hz squared we have functions on this
2333640	2340920	and then we we want to have a layer of our neural net
2342920	2344280	so typically
2347240	2352200	one layer of our neural net might be like this you know one piece of one layer of our
2354840	2360040	neural net might look like this so now what's the dimension of this space
2361000	2366840	the dimension is h squared namely the number of
2368840	2376200	points in the set and what's the dimension of this space the dimension whoops the dimension is h square
2378680	2383800	okay so now if i were doing a fully connected neural net
2384760	2392760	i would have h squared basis vectors here h squared basis vectors here and then i would have an h squared times h squared matrix
2393800	2400760	so i'd have an enormous matrix of size h to the four that's and each of those parameters i have to train
2401960	2408600	okay what do i do in cnn's i say i want this matrix to be invariant
2408760	2415960	that already cuts down the number of parameters from h to the four back down to h squared
2417240	2421000	and then i say i want this matrix to satisfy locality
2422120	2425560	and that cuts down my number of parameters from h squared to nine
2426680	2432520	so harini is asking uh what basically like why do you restrict the parameters to this extent
2433400	2435480	so i would say that there's two reasons for this
2436440	2442920	so these are inductive priors so they're not there's something that i believe is true about the solution
2443480	2448920	they're not something that like is definitely true about the solution there's some category of practicality
2448920	2453880	i want to build a model that works i can't train a hundred billion parameters but i can train
2454680	2459880	you know a hundred or a thousand fine yeah and the inductive priors in this are invariants
2460600	2467320	namely i can see you i can still see you yeah that's you know like you're still there like that's
2467320	2474600	invariance yeah um and the other thing is locality and i think locality makes a lot of sense like when
2474600	2480360	i look at this room i don't think the first thing that happens in my brain is i think oh i'm in
2480360	2490440	kaslo 273 what happens first is i go oh edge corner chair person stairs light looks like a lecture hall
2491320	2501400	sydney probably kaslo 273 yeah and so that's invariant that's um locality and these are our
2501400	2506600	inductive priors and these inductive priors massively cut down the parameters and then from
2506600	2511800	then we're cooking on gas and we can get these models that actually work no but it's again like
2511800	2519240	changing the group is a gain inductive priors so um for example like you know have you been upside
2519240	2526520	down and you look and it's actually much harder to recognize stuff so the idea that we satisfy this
2526520	2531720	invariance is much less well established than the idea that we satisfy this invariance and then we
2531720	2539560	want to bake in that symmetry exactly yeah i don't know if you really want rotations so like a classic
2539560	2545080	pre-training task in image recognition is to recognize whether your image is upside down or not
2546440	2551560	so you know that's a classically non-invariant thing under rotation by 180
2554200	2558040	but there's there's situations like you know in an MRI scan or something
2558040	2562760	where it really you know you really want that invariance that's when you should bake it into the model
2566760	2572280	if you go back to our friend l2 of s2 there's a great exercise so the laplacian
2572280	2577160	okay so the casimir gives you the laplacian on the sphere and the eigenspaces for the laplacian
2577160	2582360	are the spherical harmonics so the the casimir is acting everywhere in this whole big diagram
2582360	2587240	commuting with everything and splitting it up into irreducible it's not so much locality it's the
2587240	2599640	fact that so three is maybe i can write so you know l2 of s2 is a topological direct sum of l gamma
2600680	2602920	where gamma is
2608680	2609880	is a spherical harmonics
2617720	2623240	and the the casimir is providing this decomposition so it's breaking
2624520	2628120	breaking up this space so the casimir on each one of these acts by a different scalar
2629240	2635880	and it's the the casimir aka the laplacian acts on each of these you know this is something like
2636680	2639640	restriction of of degree
2641880	2644920	gamma or gamma over two polynomial polynomials
2647800	2655160	and so you have this totally canonical decomposition of this space of functions
2655880	2664520	and everything wherever i had this picture everything here is respecting this decomposition
2664520	2666680	all the linear maps respecting this decomposition
2670040	2675400	and roughly speaking you can kind of think about like you know if you take your function here
2676040	2680600	and do a Fourier expansion of it then you get a whole lot of quantities
2681720	2684520	and those quantities are giving you the
2688600	2692440	you get a whole lot of scalars and those scalars are basically telling you how it acts on each one of these
2698680	2704040	okay so this is just super interesting for me but maybe more technical
2704920	2711400	uh so i want to go in the second half i want to go over why permutation representations i want to go over
2711400	2717880	something called deep sets and then i want to go over graph neural nets
2719240	2725080	so one question that is very natural to ask as a representation theorist
2725400	2735560	is why so if you take functions on a set on a gamma set this is called a permutation representation
2736360	2740360	and it's called a permutation representation because if you look at the matrices that represent
2740360	2748200	your elements they're permutation matrices and in our first class in representation theory the
2748200	2752840	first representations we see a permutation but then we quickly convince our students that
2753800	2758120	we should break up permutation representations into irreducible representations and that's
2758120	2766440	really interesting so why am i insisting that we have permutation representations everywhere
2768760	2773640	so i i found that this charming little lemma which i i didn't find in the literature
2774360	2779880	which is if you have a representation of gamma which is assumed finite i'm not quite sure what
2779880	2786760	the analog of this for a league group is then we so once we have v we can choose a basis for it
2787640	2794600	and once we have a basis we can ask is relu gamma equivalent yep so remember relu takes our
2795480	2799240	vector which now that we have a basis is just a sequence of real numbers and the ones that are
2799240	2803400	negative it sets to zero and the ones that are positive it keeps so is this gamma equivalent
2804120	2809400	if our representation is just permuting our coordinates around it's easier to see that
2809400	2817880	it's gamma equivalent but uh that's if and only if so in order to have a gamma equivalent
2817880	2821480	relu with respect to some basis you have to be permutation with respect to that basis
2824760	2830120	now this is something that has been exciting me a lot over the last few days and is having
2830120	2835400	a hell of a lot of fun with is basically like piecewise linear representation theory
2836200	2842200	um so what you could imagine is you you have these layers and they all break up into irreducible
2842200	2849720	representations and if you include an irreducible into a permutation representation do a relu and
2849720	2856520	then project back you get a piecewise linear endomorphism equivariant endomorphism of your
2856520	2860760	representation and so what you could imagine is networks in which you have irreducible
2860760	2870280	representations together with equivariant non-linearities and my impression is that this
2870280	2875800	is extremely interesting and I've already learned like basic things about representations that I
2875800	2881080	didn't know from thinking about this so if you're interested in this ask me but it's kind of much
2881080	2887560	more specialized so I'm not going to talk about it today so let's I want to do another example
2887560	2894040	of our blueprint so one way of seeing this is just first think about this
2895720	2901320	so imagine that we have a whole lot of points and they're unordered and they have a whole
2901320	2907320	lot of information attached to them for example you could imagine all of the citizens of Sydney
2907320	2913080	and they're labeled by their age and how much tax they paid last year yeah so I've got a two
2913080	2919560	dimensional vector associated to every person in Sydney now one way of viewing this data set is
2919560	2926360	as a point cloud so what I have is an enormous in this you know age and tax example I just have R2
2926360	2932040	and I have an enormous number of points in R2 and I want to make some qualitative statement so a basic
2932040	2936440	statement that I could make is some kind of like center of mass statement like the average age of
2936440	2941320	people in Sydney is blah that would be a kind of boring measurement but a much more interesting
2941320	2947320	measurement would be there's this kind of weird hole in this data for example you know a particular
2947320	2952680	age in a particular tax is you know not paid in some region or something like that that would be
2952680	2956840	a much more interesting statement you can make about this data analysis and there's about this
2956840	2963160	data and this is one of the one of the this is related to this very interesting subject called
2963160	2971800	persistent homology which I know next to nothing about uh okay but we have a point cloud so we
2971800	2977160	want to make a prediction based on this point cloud and so this is an equivariant prediction task
2977880	2986920	we have so uh so here we have rd to the n so here are our n different points
2987240	2994520	uh and we want to learn some function you know like is there a big hole in this data or something
2994520	3003320	like that um and it's it's convenient to swap the indices so if you think about how s n acts here
3004280	3011480	it acts like I have a whole packet of numbers um and then it permutes them around but it's more
3012280	3018120	useful to view this from the from the point of equivariance as one packet of numbers like age
3018120	3022520	that's being permitted around and one packet of numbers like how much tax was paid being
3022520	3029480	commuted around and so I'll do this innocent rewriting um but I'm just pointing this out so
3029480	3035160	it doesn't confuse the hell out of you on the next slide uh so we want to make an s n invariant
3035160	3039640	prediction so basically we have in the language of representation theory we have a whole lot of
3039640	3045800	copies of the most basic permutation representation of s n namely s n acts acting on functions on
3045800	3053000	an n set and we want to make an s n invariant prediction so let's do some basic representation
3053000	3059960	theory which I almost certainly learned from andrew at some point in about 30 or something
3060680	3066520	so we take functions from one up to n functions on the set one up to n so this is a permutation
3066520	3072200	representation of s n and we take the trivial representation so this is where every permutation
3072200	3080440	just acts by the identity so here's a whole lot of homomorphism formulas so this is before I was
3080440	3085880	saying what are the arrows in my neural net here I'm working them out explicitly what's what parameters
3085880	3089640	there are so home from the trivial representation of the trivial representation this is the same
3089640	3096360	thing as home from r to r this is r times the identity home from n to one this is another
3096360	3102040	instance of this statement that on a permutation representation the only invariant measurement
3102040	3108520	you can make is essentially summing up your entries that's that home back the other way
3109400	3114920	here if we look at the image of one we want some vector we want some function which is
3114920	3120200	invariant under s n i.e. we want this function to take the same value everywhere which is alpha
3120840	3128120	now a little bit trickier a little bit like you know this would be a second week exercise
3128120	3132200	in representations of the symmetric group or something is that home from the trivial from
3132200	3137960	this permutation representation to itself is two-dimensional and it's spanned by the identity
3137960	3143560	and the map that sums up the coordinates and and takes that sum and multiplies it by
3144520	3154760	the constant function now exercise deduce this from the double coset formula all of these formulas
3154760	3158840	are very easy concept consequences of the double coset formula why is that the case
3160760	3166040	do it if you're a student you should do this and if you're a student it's not already obvious
3166040	3171960	to you you should do this so i this is deep set architecture so you can look at so here's
3172760	3179160	here's our paper from 2017 and to me as a non-machine learning person it looks a bit
3179160	3188120	mystifying but this is just another instance of our blueprint so here's our input so this would be
3188200	3195640	our three three-dimensional point cloud input yep so we have um three parameters per point
3198280	3206680	now we do s in equilibrium maps and we uh sprinkle around ends and ones these are our building blocks
3208440	3217880	now note how crazily this reduces the number of parameters for a large n so here if we had
3217880	3224040	no assumption of s in equilibrium this would be an n squared space of parameters you know and n
3224040	3230440	could easily be a million or something but now because we want this to be s in equilibrium we've
3230440	3237160	just got two two possibilities here we've got one possibility here we've got one possibility here
3237160	3245240	we've got one possibility etc so this allows us to make enormous um networks involving you know
3246200	3251720	hundreds of billion you know billion dimensional things um with few parameters
3253720	3261720	okay and that is um deep set architecture and it's i think it's um state of the art in terms of
3262920	3269960	point cloud prediction okay graph neural nets if there's no questions
3270040	3278120	uh graphs are everywhere in mathematics they come in many forms and variants so
3280760	3285560	i just want you to keep in mind that graph here is a very um loose term it might be a
3285560	3290760	directed graph it might be a digraph a directed graph the edges might be colored the vertices
3290760	3295720	might be colored the edges might be weighted the vertices might be weighted the vertices might have
3295720	3300600	10 parameters associated to them we might be talking about hyper graphs so that's graphs where we
3300600	3307320	have like an edge need can connect more than what more than two two vertices etc so it's
3307320	3313880	whole plethora of things called graphs and just want to emphasize that there's many ways so
3313880	3317880	graphs are everywhere in mathematics but once you start thinking about them they're even more
3317880	3322280	everywhere because there's a whole lot of stuff that wasn't obviously a graph initially and then
3322280	3327560	you can make it a graph so examples of this uh you might say well you know graph theory is one
3327560	3331720	dimensional topology and i'm a sophisticated eight dimensional topologist and i only care about
3331720	3336360	eight dimensional manifolds yeah but if you take a compact eight dimensional manifold you can choose
3336360	3341320	a point cloud on it and you'll get a graph and that graph tells you enormous amounts about that
3341320	3347000	eight manifold if you have a simplicial complex you know for me like graphs are just one dimensional
3347000	3352440	simplicial complexes and so i said to petah oh we should be sophisticated and learn on
3352440	3358200	simplicial complexes and he said well a simplicial complex is just a graph truly you know here's
3358200	3364200	my triangle here are my edges and here are my vertices it's a colored a simplicial complex is
3364200	3371640	a colored vertex colored graph okay of a special form so this is another example this is from
3371640	3378440	gaorg so if we have a a data set and it's somehow embedded in a space then we can get a natural
3378440	3385720	metric graph out of it by looking at distances between vertices we might include the coordinates here
3385720	3393000	we might do some funny function applied to this these lengths etc okay so graphs are everywhere
3393800	3401080	and graph neural nets seem to be an incredibly powerful flexible way of dealing with um data so
3402040	3406680	i think the graph neural nets have kind of really genuine like c and n's the thing that's that we
3406680	3411640	stare at as mathematicians and think how could we make something like this that would help us in
3411640	3415880	mathematics but i think that graphs are actually a thing that will help us in mathematics all the
3415880	3425000	time so that's very worthwhile thinking about so what the graph neural nets do an example we might
3425720	3431000	want to learn a function on graphs so an example would be a function which is learn planarity
3431960	3437320	okay so this output's a positive number if it's planar a negative number if it's not planar so
3437320	3442040	that would be a prediction task on graphs um we also might want to know for example the
3442040	3448840	Euler characteristic of the graph that would be another example of a prediction task another
3448840	3454920	important example kind of more like image like generalized image recognition is producing
3455880	3462280	um some learning some function from functions on graphs to r so you might think that so
3463400	3469880	no you you might repackage c and n's as being a grid and then an image is the same thing as a
3469880	3477560	function on the vertices of this grid another very important thing is that um like there's many
3477560	3482440	many incredibly interesting for example embedding problems of graphs so you give me a graph and I
3482440	3488440	want to put it in some space in an interesting way um and one way of doing that would be to provide
3488440	3493800	coordinates of where I want to put the vertices of that graph and so that would be an example of
3493800	3503000	learning a function from graphs to functions on the vertices of a graph uh so I guess the takeaway
3503000	3508840	from this is that anything to do with graphs graph neural nets uh useful for as long as it's not
3508840	3513160	like an NP hard problem on graphs of which there are plenty yeah so graph neural nets aren't going
3513160	3516440	to help you solve something like is there a Hamiltonian circuit or something like that
3518200	3523320	so what's the basic idea so imagine that I give you a graph and you want to learn on it
3525000	3529560	it's enormously difficult as far as I can work out to work out the automorphism of a group of a
3529560	3534600	graph so this is something that people spend many many years thinking about from an algorithmic
3534600	3540120	point of view and so I might not know what global symmetries are present so what I was talking about
3540120	3549080	before does not apply um or that just like most graphs have no symmetry whatsoever um or you might
3553320	3557640	so Gaston is asking what do you mean by hard um so
3558200	3568200	that I mean yeah maybe maybe like NP or something like that but I just want I like in my mind there's
3568200	3575560	there's stuff on graphs which is useful and maybe not so crazily difficult like um like
3575560	3580360	embedding your graph in a nice way or something like that and then there's a whole lot of like
3580360	3585560	seemingly innocent problems on graphs that are extremely hard um like embedding a graph
3585560	3590120	improvably the nice the best way or something like that or finding a Hamiltonian circuit or
3590120	3596600	stuff like that okay so in graph land it's easy to wander into an intractable problem
3598200	3604920	um but there's also a whole lot of useful stuff that can be done so so there's plenty of local
3604920	3612120	symmetry in graphs so around every vertex we have a symmetric group of symmetry and also we have a
3612120	3618600	metric so you can imagine processes which are symmetric and kind of diffuse on the graph and
3618600	3621880	that's what a graph neural net is so I'll quickly go over the architecture
3624840	3630040	so this is an important slide so here's my graph
3630840	3641080	and as part of my architecture I fix n1
3642360	3649880	n2 n3 and of course I'm just telling you one possible variant of like a thousand different
3649880	3655880	possibilities of building graph neural net but once you've seen one of them then the other ones
3655960	3663160	make a lot more sense so we fix these n1 and then what we do is each of our layers is a sum
3663160	3670520	over the vertices of that particular rn1 okay so you know in a vanilla neural net we just fix
3670520	3679480	dimensions here we fix dimensions at every vertex so that's this and so in this particular case my
3679800	3683640	my neural net looks like this so I have
3687240	3691960	three layers so here I have some linear map here I have a relu here I have a linear map a relu
3693880	3702600	and then a fully connected layer okay so what do I do basically I train
3703560	3717960	self and neighbor maps so here's the formula down here so I'm telling you what phi x of v so here's
3717960	3728920	my my layer which is phi and I wanted to find you this map and in order to do that I can tell you
3728920	3736520	this map evaluated at a particular vertex so that this might be vertex v and what I'm saying in
3736520	3745000	order to get that answer what you do is you take this self map times whatever I've got here plus
3745000	3752760	all of these neighbor maps so roughly speaking in my second layer something here has a term that
3752760	3759720	comes from here together with terms that come from the three neighbors so it's a very natural
3760440	3773400	it's called a message pass and as usual si is something like is affine linear
3774200	3783400	okay so s1 is an n1 times n2 matrix
3786440	3787720	plus an n2 vector
3790440	3796600	okay so each of these so this is yes thank you Brian this should be an s2
3796600	3807480	yeah so that's very important that so this is an another example of an inductive prior
3808120	3814040	sorry inductive bias or a prior so what we're saying is that we want these n1
3815480	3822600	these n1s sorry Stefan asked should all these n1s be the same and the answer is in general yes
3823480	3830360	so we want for example the n1s that talk to this guy from here and from here
3831160	3838280	to be the same n1s that talk to this guy from here here okay so the n1s are the same so
3838280	3840040	if you imagine this matrix here
3843080	3849720	it looks like something like s1 s1 s1 s1 down the diagonal and then n1s
3849960	3855320	in off diagonal places given by the adjacency matrix
3857400	3861480	you know something like this so inside this space of like
3862280	3869400	n1 times vertices times n2 times vertices so this is what my big matrix would look like
3869400	3873880	I'm saying like it should be block diagonal and a whole lot of blocks should be the same
3873880	3877240	so it's a very strong inductive bias to
3879880	3888920	assume now if I'm honest you know we might have seven like let's say two different colors on
3888920	3896120	our vertices and then we would train um neighbor maps that preserve the color of vertices neighbor
3896120	3899880	maps that change the color from red to blue neighbor maps that change the color from blue
3899880	3905960	to red etc so there's a zillion variants but in the basic vanilla version of a neural net
3905960	3910680	we assume all the n1s are the same and all the s1s are the same so this one's
3911320	3913800	so I'll give the diagonal term of this s1
3923400	3926360	so it's a very complicated slide but it's a simple idea I think
3927480	3934200	so we do that for a number of times and then we evaluate or in the situation where we're
3934200	3937880	trying to learn for example and embedding or something like that we wouldn't do the final
3937880	3944440	layer we've got some coordinates on our vertices and we're happy another classic example of a task
3944440	3949720	might be you want to divide your vertices into two classes and so then you would you'd do all your
3949720	3953800	layers and then you would say at the end uh this is a real number and then you would softmax that
3953800	3960120	and then that would be the probability that your vertex is in or out of this class okay and also
3960120	3965720	there's you know a million variants often the neighbor term is weighted by one over the degree
3965720	3972440	of the vertex okay that's what I set up there it's affine maps
3977240	3981720	okay so that's the architecture and we'll have some fun playing with this architecture in the
3981720	3991160	colab tomorrow so many variants are possible for digraphs digraphs you might think that you only
3991160	3995480	train a forward map but generally you don't you train a forward map and a back with map
3996680	4003160	so back forward and backwards neighbor maps if graph has edge coloring you can train message
4003160	4009480	passing for every color of the edge vertex coloring similarly you might also have a couple
4009480	4014760	of global parameters hanging around and in every step your global parameters speak to the parameters
4014760	4020440	on the graph and are spoken to by parameters from the graph in some way which is probably nothing to
4020440	4028440	do with the n ones on the s ones so these are some nice examples imagine that I consider this
4028440	4037720	digraph here if you look at what a graph neural net does on this digraph it exactly mimics cnn's
4038280	4046440	without falling layer so you know it's it's it's not it's not an exaggeration to say that
4047800	4050280	cnn's are a subset of giant graph neural nets
4056920	4064440	and if you kind of unpack this definition for uh deeps for a complete graph you
4065400	4067320	uh basically get deepsets
4069800	4075720	and that is all for today so thank you very much and yeah if there's any questions please ask
4075720	4079480	so gay i'll ask a question i'll just ask uh answer the question of dr bouts you first
4081000	4084680	I wonder if by adding linear maps between vertices of the graph and making the whole
4084680	4092920	architecture commutative kind of quirl net has any ml interpretation yeah so I totally agree
4092920	4097560	that when you look at neural net architectures it looks very much like river varieties and
4097560	4102920	things that we study in representation theory however one really can't underestimate the
4102920	4111400	effect of this relu of like so if you think about a like just a classic feed forward vanilla neural
4111400	4118200	net um if you don't have the relu's we understand totally what happens by basically near algebra
4119160	4123800	but when we add the relu's we can suddenly approximate any function on a compact set
4124440	4131720	so and we we have absolutely no idea of what happens inside the the neural net so yeah I find
4131720	4136840	the quiver language like very useful to think to think about but um one shouldn't underestimate
4137800	4144840	relu's so gaug asked um what have graph neural nets been used for
4146200	4155560	um and my understanding is that um like I don't know like let's say half of facebook and 75 percent
4155560	4161880	of twitter is graph neural nets um because you've got all these like connection graphs and social
4161880	4170120	networks and stuff like that um graph neural nets were like the absolute center thing that we used
4170120	4177400	with deep mind too on this work on cash analysis polynomials um my understanding is that graph neural
4177400	4185240	nets are kind of um taking over neural net world in terms of they're very flexible um they're very
4185240	4191720	powerful um and a lot of tasks where for example c and n would involve like a drastic change of
4191720	4198360	architecture um in graph neural nets you can just like add a vertex or something like that so
4198360	4203080	they're very flexible powerful framework for machine learning as far as I can make out
4204520	4209240	sam yates asked for discrete valued problem could we pick other group rings say with some
4209240	4217000	choice of analogous relu like function perhaps uh that's an intimidating prospect for me because
4217000	4223080	I wouldn't know how to train and things like that so my very vague understanding so there's this
4223080	4230360	kind of revolution in the last kind of three four years given by transformers and my understanding
4230360	4239880	of that is like basically like a a graph neural net um hooked on to an lstm so like the graph neural
4239880	4246040	net kind of decides where to look back in the sentence and things like that so um but I'm not
4246040	4251800	directly aware of any recurrent technologies used directly with graph neural nets what another
4251800	4256280	thing that graph neural nets are very useful for which is kind of counterintuitive is like predicting
4256280	4262520	graph structure so you have data and you want to um and you want to predict which edges exist
4263080	4267080	so like you might want to predict social relationships or something like that and um my
4267080	4271880	understanding is that you you start off with a complete graph and run a graph neural net
4271880	4277560	and your graph kind of learns a probability of discarding an edge and that's very very powerful
4279320	4283880	so it it's intuitive and intuitive but graph neural nets can learn graphs which I think is
4283880	4288920	awesome I think I think like learning graphs is like learning graphs is a really difficult
4288920	4297240	problem in machine learning um and yeah of course so it's going to be yeah it's an intimidating
4297240	4303960	problem that or n choose tuna yeah so that's a really good question so what kind of problems
4303960	4310040	and why can be addressed with graph neural nets my understanding is that very recently there's been
4310040	4316120	kind of benchmarking so there's been like a list of 50 problems and some some attempt to decide
4316120	4320680	you know what what can various classes of graph neural net algorithms do and what they can't do
4321480	4331320	um my very rough picture as as a graph neural net greenhorn is um that anything that you can kind
4331320	4337800	of decide via a finite like a small amount of walking around your graph and if you if if you're
4337800	4341320	extremely smart and you're allowed to walk around your graph a little bit and you can already make
4341320	4346280	a decision then that's something that you could solve so something like detecting planarity or
4346280	4350440	detecting a three cycle or something this is something that you can solve but if it involves
4350520	4355960	exploring the whole graph particularly potentially in many many different iterations for example
4355960	4361240	finding a Hamiltonian cycle or something um yeah it's not going to work
4363160	4366760	she'll have said the piecewise linear representation can you say a bit more about it
4367720	4371720	sure so how about I will ask answer Joel's question and then maybe I can say something
4371720	4375640	about it but I'll give other people the chance to leave because it's somewhat specialised
4376360	4381640	so a part of Adam salt Wagner's work was learning graphs learning good people
4383160	4387000	to a particularly conjecture for instance so we might hear about learning graphs in a few weeks
4387000	4391640	yes exactly so salt Wagner's work is using reinforcement learning to produce interesting
4391640	4395880	graphs okay so now we'll declare it over and anybody that's interested in this piecewise
4395880	4403960	linear business can stick around I think this is super beautiful and um I will just explain it
4403960	4409720	briefly so let's just consider the following silly examples so we're looking at the
4411160	4423080	sn acting on s3 acting on r3 or permutation representation now we know that r3 decomposes
4423160	4434520	canonically as nat plus triv and that is the set of vectors in r3 such that the sum of the lambda
4434520	4451160	i is zero and triv is the set of r times 111 and this decomposition is completely canonical
4453400	4461080	so now you can ask the following like just kind of totally naive question if we go from nat
4462520	4466680	into r3 and then apply value
4469640	4478280	go to r3 back down to nat the composition is an s3 equivariant
4483720	4488680	pl endomorphism
4489640	4495800	i.e in the category of piecewise linear maps from this vector space to itself that a s3
4495800	4499080	equivariant this is a pl endomorphism and what is it
4502120	4510040	it's super beautiful so basically what you do is inside so here's nat so we divide up
4510040	4521640	nat so just for people that don't do this every day nat is the um symmetries of the triangle
4524440	4529880	embedded inside r3 r2 okay so we just take an equilateral triangle and we take the symmetries
4529880	4543560	of the equilateral triangle so now um there's three regions here and what happens so there's six
4543560	4555480	regions there's the blue regions and the red regions and the blue regions get squashed
4560840	4563880	and the red regions get kind of expanded out
4569160	4570200	so these get squashed
4572760	4573800	and these get expanded
4579080	4585960	and i don't know this is just like a very beautiful basic um like pl endomorphism of a
4586040	4592360	representation that i've never encountered before in my life okay and you can start having fun like
4592360	4592920	what is this
4598440	4604760	or nat inside r in and you know this is a nice exercise
4605320	4616680	and yeah so that one of the things that i find really interesting is that home
4619400	4625320	pl from any representation to r um is interesting
4625800	4638120	okay so example is that home from pl from the sign representation to r
4639160	4641640	contains the absolute value map
4645960	4651080	um like this is for this is the sign representation of s2 oh no sign representation in general
4651080	4660680	in fact um but home from the trivial representation pl to any irrep
4662680	4666840	not equal to the trivial zero
4669640	4675160	and i kind of feel like this is telling us something remarkable that about kind of how
4675240	4680840	equivalence kind of flows through a neural net so this is still very speculative but what
4680840	4688680	i feel like is that you have some kind of measure of complexity so it like at the start you have all
4688680	4689320	all irreps
4694120	4700840	and then at the end you have the trivial and then you have the maps in the in the neural net so you
4700840	4710920	have linear and then you have pl and then you have linear pl and these pl maps have a definite
4710920	4715480	sense of direction like you know once we get through the trivial representation we can never
4715480	4722760	get out of it again and like i don't know this seems to explain some some very interesting
4722760	4727560	aspects of neural nets but it's just exciting stuff that i've been thinking about last week so
4727640	4732920	very unbaked um so maybe when it's baked i can uh talk more about it
4735560	4739640	so thanks are the blue rays there inside the reflecting hyperplanes
4741480	4743240	this is alpha one and this is alpha two
4745880	4752120	so no oh yeah the so the reflecting hyperplanes would be like this and this and this
4753880	4755400	thanks everyone i think we'll stop now
4757560	4758060	you
