WEBVTT

00:00.000 --> 00:13.200
So, welcome to this week's lecture.

00:13.200 --> 00:16.800
So, seminar structure.

00:16.800 --> 00:22.240
Next week there'll be some linear combination of Gayog and Geordi, and the constant is yet

00:22.240 --> 00:24.040
to be determined.

00:24.040 --> 00:29.200
The week after, so after next week we move into the talks from experts.

00:29.200 --> 00:33.820
So, we've been trying to give you background and then the experts will tell us what's really

00:33.820 --> 00:34.820
going on.

00:34.820 --> 00:38.440
So, I'm really excited by the lineup.

00:38.440 --> 00:45.680
So, Adam Saltbaugner has done some amazing work using reinforcement learning to construct

00:45.680 --> 00:48.880
counter examples in graph theory.

00:48.880 --> 00:54.720
If you want to prepare for his talk, the paper is really beautifully written.

00:54.720 --> 00:57.640
Bandad Hosseini works on graph methods.

00:57.640 --> 01:03.120
So, we'll see a bit of graph neural nets today and we'll see more in his talk.

01:03.120 --> 01:10.040
Carlos Simpson is talking on a kind of archetypal garden of how to use machine learning and

01:10.040 --> 01:12.760
mathematical proof.

01:12.760 --> 01:16.360
And I think that that's extremely interesting.

01:16.360 --> 01:22.960
And then there's another three talks lined up after that, taking us to the end of semester.

01:22.960 --> 01:26.760
And the other thing I wanted to point out is Joel has been doing an amazing job with

01:26.760 --> 01:27.760
the toe labs.

01:27.760 --> 01:31.440
If you've been taking part in the tutorials, you'll be aware of this.

01:31.440 --> 01:35.800
But if you're watching online and you just want to muck around with something, they're

01:35.800 --> 01:38.360
really great and I'm hoping that they provide a good resource.

01:38.360 --> 01:43.160
So, we'll hope to keep these going for the expert talks so you can play with some of

01:43.160 --> 01:46.760
Adam's ideas, play with some of Bumdud's ideas, et cetera.

01:46.760 --> 01:51.200
So, again, the seminar principles.

01:51.200 --> 01:57.440
So, today we'll be about geometric deep learning and geometric deep learning is very much the

01:57.440 --> 02:04.520
question of how do you incorporate symmetry into a learning problem.

02:04.520 --> 02:12.560
But before we go into that, I just want to recall the notion of an inductive bias, which

02:12.640 --> 02:15.360
is a very important notion in machine learning.

02:15.360 --> 02:18.520
So what's going on here?

02:18.520 --> 02:25.040
So it's very common situation in maths that you have some problem and you know or suspect

02:25.040 --> 02:27.680
something about the solution.

02:27.680 --> 02:31.840
So a basic example would be something like we expect the solution to be smooth or we

02:31.840 --> 02:34.520
hope the solution is smooth.

02:34.520 --> 02:39.560
The solution should satisfy conservation of energy, for example, there'll be some differential

02:39.560 --> 02:42.160
equation that the solution should satisfy.

02:42.160 --> 02:47.320
The solution should be invariant under a group, so this occurs all the time in physics.

02:47.320 --> 02:52.800
The solution might be locally determined, the counter example, so also we might be looking

02:52.800 --> 02:57.480
for a counter example, we might suspect something about the counter example.

02:57.480 --> 03:03.520
So the fancy names for this are inductive bias or prior.

03:03.520 --> 03:07.520
So inductive bias just means something that you know about the problem a priori before

03:07.520 --> 03:08.840
starting to solve it.

03:08.840 --> 03:16.480
And it's very important to remember what inductive biases there are in a problem.

03:16.480 --> 03:20.800
So how does one imagine that one has some inductive bias?

03:20.800 --> 03:23.280
So for example, the solution might be smooth.

03:23.280 --> 03:30.880
So this is related very much to regularization.

03:30.880 --> 03:38.400
So in this was Gayog's talk last week.

03:38.400 --> 03:44.280
So for example, he talked about this Gaussian kernel, which very much encourages functions

03:44.280 --> 03:52.280
that have you know Fourier modes that aren't wiggling around too quickly, that are smooth

03:52.280 --> 03:54.880
in a very strong sense.

03:54.880 --> 03:58.840
Solution should satisfy conservation of energy, this might be another example of regularization.

03:58.840 --> 04:02.880
There's also these fascinating things, if you want to have a Google called physically

04:02.880 --> 04:10.360
informed neural nets.

04:10.360 --> 04:15.920
So here you don't require the solution to satisfy some differential equation, but you

04:15.920 --> 04:18.560
add the differential equation to the cost function.

04:18.560 --> 04:23.720
And so it encourages the solution to satisfy a differential equation.

04:23.720 --> 04:26.400
Problem should be the solution should be invariant under.

04:26.400 --> 04:29.640
So this is today.

04:29.640 --> 04:31.600
That's the subject of today.

04:31.600 --> 04:33.720
The solution might be locally determined.

04:33.720 --> 04:39.720
So this is like examples might be CNN's or LSTM.

04:39.720 --> 04:44.240
Okay we expect for example small part areas of small parts of an image to play an important

04:44.240 --> 04:47.000
role initially.

04:47.000 --> 04:49.240
And the counter example is probably highly connected.

04:49.240 --> 04:53.600
I included this example because this is an example where I've got no idea how to put

04:53.600 --> 04:55.600
this kind of information into a network.

04:55.600 --> 05:03.800
And this was definitely my experience with DeepMind in that often, so the movement from

05:03.800 --> 05:10.480
an inductive bias to the neural net design is art rather than science.

05:10.480 --> 05:16.800
So several times with the DeepMind team I said oh you know I know this about the solution

05:16.800 --> 05:22.040
and they said we have absolutely no idea how to incorporate this into the model.

05:22.040 --> 05:29.520
And I think that one should be aware that this is often a big issue.

05:29.520 --> 05:35.520
So yeah one should be just be aware that it's very important to have some kind of inductive

05:35.520 --> 05:43.760
bias and be aware of it but you might not know what to do with it.

05:43.760 --> 05:50.560
And this is just the same slide that very similar to things that Georg has been saying.

05:50.560 --> 05:57.360
You have the capacity of a model roughly speaking how many parameters it has and there is this

05:57.360 --> 06:00.240
playoff between simplicity and expressivity.

06:00.240 --> 06:07.960
So a lot of parameters means you can express for example any function but training may

06:07.960 --> 06:13.680
be infeasible you might have hundreds of billions of parameters and it's less interpretable

06:13.680 --> 06:21.280
and so there's often sweet spots but in this trade off.

06:21.280 --> 06:28.640
Okay so today what I'm talking about is symmetry in neural networks and I'm really extremely

06:28.640 --> 06:33.240
happy to give this talk because it's been kind of a revelation for me.

06:33.240 --> 06:39.040
So I began with this book Geometric Deep Learning by Bronstein Bruner Cohen and Petar

06:39.040 --> 06:45.160
Velichkovich who I worked with on the DeepMind project and Gayog pointed out this group here

06:45.160 --> 06:50.960
with very convolutional networks and Petar recently just pointed out this steerable CNNs

06:50.960 --> 06:57.600
and it's very interesting subject and I think it's a really remarkable example.

06:57.600 --> 07:03.720
So in physics we often see this phenomenon where just knowing some kind of symmetry is

07:03.720 --> 07:08.280
present has enormous effects on your ability to solve a problem or formulate a model or

07:08.280 --> 07:12.840
something like that so there's this extraordinary paper I think it's by gross called symmetry

07:12.840 --> 07:19.480
and physical laws which I found really inspiring and I find this

07:19.480 --> 07:24.080
equivariance in convolutional neural nets to be a kind of similar story it's like it

07:24.080 --> 07:28.720
seems so innocent to require some kind of invariance and yet it essentially determines

07:28.720 --> 07:32.800
your entire architecture it's really remarkable.

07:32.800 --> 07:38.160
So never underestimate symmetry I had this on my web page for about 10 years as the first

07:38.160 --> 07:40.920
thing that you read.

07:40.920 --> 07:50.120
So I want to review what kind of vanilla CNN is and then so I'll first just remember

07:50.120 --> 07:57.040
what a CNN is and then we'll view a CNN through the language of group theory and I want to

07:57.040 --> 08:06.440
try to convince you that three basic principles already basically determine CNN architecture.

08:06.440 --> 08:14.360
So here's my image in the top right so this is a grayscale image I'm assuming that it

08:14.360 --> 08:19.560
is on a square so I have a fixed width and a fixed height a fixed number of pixels wide

08:19.560 --> 08:27.840
a fixed number of pixels high and I have my pixel value is given by a real valued function

08:27.840 --> 08:35.120
so for example this pixel value might be between zero and 255 and what I'm looking for is some

08:35.240 --> 08:41.160
function from this is called a so I'm calling this a periodic image because I'm kind of

08:41.160 --> 08:49.640
wrapping the top you know the physicists would say I'm compactifying on the torus and you

08:49.640 --> 08:54.320
know I sound very fancy when I say that but we're just simplifying our situation by imagining

08:54.320 --> 09:00.160
that our image is on the torus and there's it just simplifies the group theoretic discussion

09:00.160 --> 09:03.400
in a second and there's no genuine need to do this.

09:03.400 --> 09:11.280
So we're seeking some function from periodic images i.e functions on this grid to the real

09:11.280 --> 09:18.760
numbers which are for example positive on tigers and negative on non-tigers is a classic

09:18.760 --> 09:22.680
and a machine learning problem and also the problem on which machine learning has been

09:22.680 --> 09:36.280
wildly successful and we do this in the following way so we have several layers and typically

09:36.280 --> 09:47.200
these layers will consist of other periodic images perhaps at lower scales so when in

09:47.200 --> 09:51.880
the first two layers of this neural net I'm assuming that my periodic image is the same

09:51.960 --> 09:58.360
scale and then in the third layer I'm assuming that the periodic image has dropped in scale

09:58.360 --> 10:03.400
somewhat so I'm this h is assumed to h prime is assumed to divide h.

10:09.080 --> 10:14.680
So what one should think about this problem so this problem this seek so we're seeking a

10:14.680 --> 10:20.120
function from here to here and this function is going to be highly non-linear

10:21.960 --> 10:28.520
so it's a non-linear function on this big vector space and I guess one of the points

10:28.520 --> 10:34.040
of machine learning is that learning a highly non-linear function on a high dimensional vector

10:34.040 --> 10:41.240
space is a very difficult task and we get enormous amount of mileage out of viewing it

10:41.240 --> 10:48.840
as composed of out of rather simple functions so the simple functions that we use are convolutions

10:48.840 --> 10:52.120
so this might be like we might have some kind of filter here

10:55.720 --> 11:06.920
the filter and this might be for example 8-1-1-1-1-1-1-1-1-1-1-1-1-1 and this particular filter

11:07.720 --> 11:14.840
we point wise multiply with our image and then sum up the result and so this particular filter

11:14.840 --> 11:20.920
would have the effect that on areas of blank color we would get a very low value but on

11:20.920 --> 11:25.880
out on edges we would get a high value so it would be have a kind of um outlining effect

11:27.160 --> 11:33.720
so this is an example of something that we might uh convolve with so that's applying this

11:33.720 --> 11:40.120
filter can be rephrased as a convolution and what we do so we convolve and then we apply

11:40.920 --> 11:49.720
apply a value and then we convolve and we apply a value and then we do an operation called pooling

11:49.720 --> 11:54.360
that I'll basically ignore here which might be you look at a grid of pixels and take the maximum

11:54.360 --> 12:00.600
element so this will take you down to a smaller image and then finally at the end you might do

12:00.600 --> 12:07.960
some fully connected layers and the point of all this is that we don't we specify this architecture

12:08.680 --> 12:14.680
uh at the beginning and we specify all the values at the beginning but we learn the convolutions

12:14.680 --> 12:17.240
that's the important point we learn these filters

12:19.560 --> 12:25.240
okay so that was meant to just be a review now I want to look at this through the language of group here

12:29.000 --> 12:32.600
so this is just a direct copy of the previous architecture

12:33.320 --> 12:44.440
so z mod h squared sorry z mod hz squared is a group so it's z mod hz times z mod hz

12:46.360 --> 12:47.960
very simple abelian group

12:53.720 --> 12:59.480
and as with any group it acts on functions on that group

13:00.360 --> 13:07.800
so if I have a function on my group what I can do is translate my group around and I get a new

13:07.800 --> 13:15.080
function okay and convolving by a single so when I translate around by my group this is the same

13:15.080 --> 13:23.720
thing as convolving with a delta function on my group so on my group I can consider the function

13:24.360 --> 13:32.600
that's just one at a particular group element and zero elsewhere and convolving with that element

13:32.600 --> 13:36.120
is the same thing as translating by that group element

13:38.920 --> 13:47.480
and uh so any any convolution is a linear combination of these convolution with a single

13:47.480 --> 13:54.280
with a delta function and so this is gamma equivalent so another way so in the abelian case

13:54.280 --> 14:01.960
kind of nothing nothing matters in terms of orders you know when I wrote g dot f of x equals f of

14:02.600 --> 14:08.680
g plus x it doesn't matter whether I whether I write x plus g or x minus g but in the non-abelian

14:08.680 --> 14:14.520
case I'd have to have an inverse and in the non-elabelian case I would kind of think about

14:14.760 --> 14:20.520
convolutions as maybe acting from the right or something like that but it's a general fact that

14:22.360 --> 14:30.520
the if we look at functions on a group then the equivalent maps from functions on a group to

14:30.520 --> 14:35.320
functions on a group are the same thing as functions on that group acting by a convolution

14:36.360 --> 14:42.040
and that's what I that's what I say here so the equivalent maps from functions on the group to

14:42.040 --> 14:49.240
functions on the group are simply functions on the group okay and this is true for any gamma

14:51.000 --> 14:54.440
this is very basic representation theory if you will

14:59.800 --> 15:07.960
so remember that I hate questions and any question will involve um a horror show

15:08.920 --> 15:10.280
so please don't ask any questions

15:15.160 --> 15:20.360
so what are the basic observations about this so we want often in these problems we want a

15:20.360 --> 15:26.680
gamma invariant answer so if we move our picture of a cat around if we translate around the answer

15:26.680 --> 15:31.160
should still be cat okay this is one of the reasons that I assumed a periodic

15:31.480 --> 15:40.920
periodic image that's very important another point is that so there's another classic machine

15:40.920 --> 15:47.960
learning task which is called image segmentation which asks us to say where are the two eyes in

15:47.960 --> 15:53.720
this picture or you know when your phone for example tells you where the head of the people

15:53.720 --> 15:59.160
this is an example of image segmentation now if you think about what this task is

15:59.160 --> 16:04.440
this is not invariant it's equivariant this means that if I move the image

16:05.560 --> 16:12.280
my my prediction should move in the same way so we often want a gamma invariant answer or a gamma

16:12.280 --> 16:23.400
equivariant answer

16:23.400 --> 16:45.320
okay convolution and value are gamma equivariant and for simplicity I'm ignoring pooling layers

16:45.320 --> 16:49.800
but we can definitely add them into the discussion but I feel like the the guts of this business

16:50.760 --> 16:53.800
really is exposed when we ignore pooling so I'm going to do that from now on

16:55.720 --> 17:06.200
so so convolution is equivariant and and value is equivariant if we so we have our image we

17:06.200 --> 17:11.480
have a whole bunch of real numbers if we translate and then set some of those numbers to zero that's

17:11.560 --> 17:13.880
the same thing as setting some of those numbers to zero and then translate

17:18.440 --> 17:25.320
locality yeah no any activation function would be equivariant but as we'll discuss in a second

17:25.320 --> 17:30.280
it's really essential that these are permutation representations for an active function activation

17:30.280 --> 17:37.960
function to be equivariant so so we have requirement one we want a gamma invariant answer

17:38.280 --> 17:45.640
requirement two is that everything going on in this network should be gamma equivariant

17:46.840 --> 17:54.360
requirement three is locality so what this says often in you know if you look apparently at the

17:54.360 --> 18:00.680
early layers of the brain in the visual cortex what happens is local and so and it's very natural

18:00.680 --> 18:08.440
to also impose this in a cnn so we our our filters are supported around the identity initially

18:08.440 --> 18:13.720
and then later on we let them grow out through pooling and potentially fully connected layers

18:15.000 --> 18:21.640
so an actual cnn we wouldn't require we wouldn't look at periodic images and what we would do

18:21.640 --> 18:29.080
is pat around the edge um to make all our images the same size for example yeah and we would only

18:29.080 --> 18:33.960
have we the same principles would be there but we wouldn't have a kind of full symmetry that only

18:33.960 --> 18:39.640
makes sense to shift pictures a little bit but what I find remarkable here and it's kind of a

18:39.640 --> 18:45.320
simple simple thing is that gamma invariance gamma equivariance and locality basically tell me what

18:45.320 --> 18:50.920
I what I have to do in my neural net so if you assume that you should compose it out of simple

18:50.920 --> 18:57.960
functions and if you fix relu then everything else is basically specified which is remark which seems

18:57.960 --> 19:05.400
to me to be remarkable and what I want to explain soon is that there's kind of nothing special about

19:05.400 --> 19:11.800
this particular group that this would actually tell us how to make predictions on any space on

19:11.800 --> 19:18.280
which a group acts transitively so let me just emphasize one extremely important point from a

19:19.240 --> 19:28.760
implementation point of view okay so just for completeness gaug asked here whether

19:30.120 --> 19:36.040
relu was specific for it being equivariant and the response was no any non-linear activation

19:36.040 --> 19:40.360
function would be fine at this point as long as our representation is a permutation representation

19:41.400 --> 19:48.040
and what Stefan asked was in this particular picture here um you know for completeness

19:48.040 --> 19:51.640
if we translated this little cat we'd have half the cat's head over here and half the cat's head

19:51.640 --> 19:56.440
over here and the question was you know how does an how does an actual cnn in real life

19:58.440 --> 20:04.600
do this and basically you know we don't we don't enforce that full equivariance we only

20:04.600 --> 20:09.480
allow kind of small translations within some bounded region so kind of partial symmetry

20:10.280 --> 20:11.480
thank you very much for the reminder

20:14.040 --> 20:22.280
so this is very important from a basic implementation point of view imagine on the left hand side we

20:22.280 --> 20:29.720
have two we this is a layer of our neural net and so we have l1 inputs and l2 outputs

20:30.840 --> 20:36.040
single layer so now if you think about the number of parameters it's l1 times l2

20:36.200 --> 20:45.320
so if it's easy to find a picture for example with 10 million uh 10 million pixels in it

20:45.880 --> 20:54.440
and if we do one layer then that's whatever 10 million squared is you know i'm not a physicist

20:54.440 --> 20:59.720
i don't know what 10 million squared is okay so some enormous number that i can probably never

20:59.720 --> 21:10.680
actually train on a computer and but if we're doing so here here i have a first layer of a

21:11.400 --> 21:17.400
convolutional neural net in one dimensional in one dimension so here are my inputs

21:19.720 --> 21:26.600
and locality says that my filter only affects neighboring points so that's why i have these

21:26.600 --> 21:35.880
three parameters x y and z and equivariance says that these x y and z are the same across the whole

21:35.880 --> 21:44.920
thing so no matter how big this layer is it just depends on three size um three parameters okay

21:44.920 --> 21:51.800
so i should emphasize this is one piece of the first layer so up here this would be one of these

21:51.800 --> 21:58.520
pieces so i could expect to have potentially 20 of these pieces or something but still some you

21:58.520 --> 22:06.520
know 20 times 3 is a lot smaller than 10 million squared i'm enough of a physicist to know that

22:06.520 --> 22:18.760
inequality okay so now i want to explain a blueprint for learning on a general homogeneous

22:18.760 --> 22:28.120
space for a group so we have our group and i'm typically thinking about a finite group or a

22:28.120 --> 22:36.040
league group like so three or something like that so gamma was z mod h z squared before

22:36.920 --> 22:44.360
and x is a transitive gamma set so this just means that um gamma acts transitively on x but

22:44.360 --> 22:48.680
in the category of leap in the category of differential manifold this would mean that

22:48.680 --> 22:56.680
i have a manifold with a continuous action of my or a smooth action of my league and in any

22:56.680 --> 23:05.240
situation in which um this makes sense we have that x is just the same thing as gamma mod a

23:05.240 --> 23:11.880
single stabilizer and what we want to do is learn an invariant so i'll stick to the invariant case

23:11.880 --> 23:18.520
but notice that we might also want to make an equivariant prediction function from functions on

23:18.520 --> 23:25.960
x to r now very basic representation theoretic observation or maybe so basic but it's not yet

23:25.960 --> 23:34.360
representation theory is that because our action is transitive there is only one linear

23:34.360 --> 23:44.360
uh or at most one linear map from functions on x to r that is invariant namely

23:45.800 --> 23:54.280
like summing over my finite set or integrating or so i found this kind of illustrative because

23:54.280 --> 23:59.080
this tells you for example in the image classification task you're definitely looking for a nonlinear

23:59.080 --> 24:05.800
function because a linear function would be like averaging over pixel values and this is the kind

24:05.800 --> 24:15.240
of silliest thing that one could imagine so we're looking for some um invariant function

24:17.000 --> 24:27.720
and here's the blueprint so we fix a transitive gamma set this is where we want to make the

24:27.800 --> 24:32.600
prediction and then we just our architecture consists of a whole lot of choices of transitive

24:32.600 --> 24:39.720
gamma sets and basically i think one way to think about these transitive gamma sets is

24:39.720 --> 24:46.600
so the the invariant prediction says that you want to take um so any gamma has an important

24:46.600 --> 24:52.840
transitive gamma set namely one point and this is where we want our prediction to end up and then

24:52.840 --> 24:59.720
if you look at um classical c and n's you want your sets to kind of slowly get smaller in some

24:59.720 --> 25:03.320
sense until you reach the prediction so you can think about these transitive gamma sets

25:03.320 --> 25:14.280
as slowly decreasing in size if that makes sense and this is all we do so we um consider some

25:14.360 --> 25:21.000
equivariant maps so convolution so this should be a gamma a gamma equivariant

25:24.520 --> 25:27.880
linear map

25:30.280 --> 25:33.080
and then we never do a relu and then we do another one and then we do a relu and then

25:33.080 --> 25:39.880
we do another one and we want to train across the parameters of gamma equivariant linear maps

25:40.600 --> 25:46.120
yeah and probably this is a point of it so this whole space is r

25:50.200 --> 25:54.280
so if gamma has a metric or similar we might want convolution supported near the identity

25:54.920 --> 25:58.680
now this is the most important point and i think that this has kind of been missed in the

25:59.400 --> 26:04.680
in the machine learning literature there's something very basic in representation theory

26:04.680 --> 26:09.160
which i call the double coset formula people might call it hecar algebras there's many

26:09.160 --> 26:20.760
different names for it so we're asking what is such a map so because any transitive set is

26:20.760 --> 26:26.360
simply gamma mod h or gamma mod h prime we want to know what this home space is

26:27.960 --> 26:34.840
and the formula says that homomorphisms from such a function space to such a function space

26:34.840 --> 26:37.000
are simply functions on double cosets

26:40.040 --> 26:44.840
now there's many different ways to understand this formula if you if you're in the world of

26:44.840 --> 26:49.560
finite groups this is a very nice exercise if you're in the world of compact league groups it's a

26:50.280 --> 26:57.160
significantly more difficult exercise but i just want you to accept this formula as a kind of

26:58.280 --> 27:04.360
beautiful thing in the world and we'll see it's very useful okay and if you want to know more

27:04.360 --> 27:11.080
about it i'm very happy to talk more about this formula okay but this is a kind of i don't know

27:11.080 --> 27:17.480
very useful formula in many different situations so this is telling us what the possible space of

27:17.480 --> 27:29.480
convolutions is so here's an example imagine that we're learning on a sphere so we have a nice sphere

27:29.480 --> 27:38.920
here and we have so three so this is um orthogonal three by three matrices of determinant one so

27:38.920 --> 27:47.400
these are these transformations so it acts on the sphere and s2 is a transitive space i can move

27:47.400 --> 27:51.480
any point on the sphere to another point on the sphere of iron orthogonal transformation

27:51.480 --> 27:59.080
what is the stabilizer of single point it's those rotations in the axis that point determines through

27:59.080 --> 28:09.800
the origin so s2 is s03 mod s1 now imagine that we want to learn on the sphere so we want to have

28:09.800 --> 28:15.400
some image on the sphere some function on the sphere and we want to say it's a cat or something

28:16.120 --> 28:21.960
you might ask i don't generally see pictures of cats on spheres this is my answer to that

28:22.840 --> 28:27.720
okay this is a beautiful article in quanta so this is the cosmic background radiation

28:28.680 --> 28:36.200
no absolutely extraordinary thing from around about 2003 where we see the early what the universe

28:36.200 --> 28:40.600
looked at early on and we do this by basically going around the world and looking out into space

28:40.600 --> 28:43.080
and so it's an archetypal example of an image on a sphere

28:47.800 --> 28:54.360
so sam has a question um we'll just i just want to admire this picture for 10 more seconds

28:54.440 --> 29:02.440
so i'm told that you can see the fluctuations of quantum field theory in this picture so

29:02.440 --> 29:06.760
this is a very early universe so it's when the universe was very small and you expect

29:08.200 --> 29:12.520
the behavior to be given by the laws of the very small and i'm told that you can see

29:13.080 --> 29:18.520
evidence of quantum field theory in this picture that totally blows my mind okay so sam's question

29:18.520 --> 29:23.160
for discrete finitely generated gamma could support near the identity be regarded as having

29:23.160 --> 29:28.680
sort some sort of choice of generator that's a really good question i was thinking about

29:28.680 --> 29:36.120
what the support nearly up near the identity kind of means so in the example of the cnn

29:37.240 --> 29:45.480
we have this discrete group which has no no really convincing metric on it but it is embedded in

29:46.040 --> 29:51.800
s1 times s1 that does have a good metric on it and so for groups that come with some embedding

29:53.160 --> 29:57.160
we can put a metric on them but i also think that that's a good suggestion if you have a

29:58.520 --> 30:02.440
some kind of uh what's what's that distance you're talking about it's um like kind of

30:02.440 --> 30:07.560
distance in the kaillie graph might also be a a decent measure of locality i also want to try

30:07.560 --> 30:14.280
to explain in a second that for a non-Abelian group locality is less important so so the building

30:14.280 --> 30:20.520
blocks so what are the homogeneous spaces for so three so this is in a league group so i can

30:20.520 --> 30:25.160
ask what are the dimensions of the subgroups of s03 so there's a whole lot of interesting finite

30:25.160 --> 30:30.040
subgroups of s03 for example the symmetries of the icosahedron form a very interesting

30:30.680 --> 30:40.920
subgroup of s03 and um and then you have the two sphere and rp2 and then you have a point

30:40.920 --> 30:46.760
and that's it okay so our building blocks are rather restricted which is interesting

30:46.760 --> 30:53.960
and also i would say that we if you're employing some kind of practicality in building your model

30:54.520 --> 31:00.200
you don't want complicated things like s03 modified subgroup so

31:02.200 --> 31:07.720
and rp2 and s2 are very very similar you know one is just a two-fold cover of the other

31:08.920 --> 31:14.520
and so i would advocate building a building a network which just involves functions on

31:14.600 --> 31:21.160
spheres and functions on a point so this is the proposal for a blueprint for learning on the sphere

31:21.160 --> 31:25.720
and also i am aware that it's very difficult for a computer to understand a function on s2

31:25.720 --> 31:30.600
okay but this is meant to be some kind of blueprint that you then try to interpret

31:31.880 --> 31:36.120
and sometimes you know to have the idea of what you're doing very clearly in your head is very

31:36.120 --> 31:41.560
useful when you come to implementing something here we have h equals h prime exactly so i'll

31:41.560 --> 31:47.800
go through the double coset formula in two examples now so the point the the problem here

31:47.800 --> 31:53.640
which geyog is pointing out so geyog was asking which which subgroup are we using the double

31:53.640 --> 31:57.960
coset formula in here i just want to first say why we're using the double coset formula

31:57.960 --> 32:04.600
we want to know what are these maps what are our possible so three every variant convolutions here

32:04.600 --> 32:11.960
so what are the so three every rank convolutions so this is the double coset formula again this is

32:11.960 --> 32:21.960
our friend okay i'm just specializing the double coset formula for so three so that made the formula

32:21.960 --> 32:26.440
much less easy to read so i'll delete it again okay so what does this say let's first do a silly

32:26.440 --> 32:35.240
example what are the homomorphisms from s03 mod s1 i.e s2 to s03 mod s03 namely a point

32:36.120 --> 32:40.840
so i said as an exercise in very great generality the only such function is given essentially by

32:40.840 --> 32:45.160
integrating over your space up to a scalar but let's see it pop out of the double coset formula

32:45.160 --> 32:51.160
so s03 mod s03 is a point yeah so we've got functions on a point what's more interesting

32:51.160 --> 32:57.000
so this is the kind of silly example what's more interesting is what are the rest of the layers

32:57.720 --> 33:02.040
you know so just to emphasize here this is telling us that even though this is an enormous vector

33:02.040 --> 33:10.680
space with this only one scalar possibility of of maps here so this belongs to r this belongs to r

33:11.400 --> 33:12.200
this belongs to r

33:13.160 --> 33:23.480
so now ignore the integral bit at the moment just look at this so what are the s03

33:23.480 --> 33:30.040
equivariant homomorphisms from functions on s2 to functions on s2 they're functions on by our double

33:30.040 --> 33:38.520
coset formula s1 mod s03 mod s1 so that's the same thing as s s1 mod s2 so i take s2 and i

33:38.680 --> 33:45.160
have s1 rotating it around and then i quotient that out and our representatives for that quotient

33:45.160 --> 33:51.720
space are just an interval stretching from one pole to the other so functions on that interval

33:53.560 --> 34:00.760
so what the hell are these intertwiners so i should say that such an element inside here

34:01.400 --> 34:02.600
is called an intertwiner

34:02.920 --> 34:13.880
so intertwiner is synonymous with s03 equivariant linear map

34:16.360 --> 34:22.600
so i can look at one way to understand these things is to try to look at delta function so a delta

34:22.600 --> 34:30.200
function at the identity at this base point is just the identity a delta function at this end

34:30.200 --> 34:38.040
is the antipode but what the hell is going on in the middle you know you're you're seeking a

34:38.040 --> 34:44.760
continuous family of operators which interpolates between the identity and the antipode okay looks

34:44.760 --> 34:50.520
like a tough ask but there's a really beautiful thing you can do which is you consider the

34:50.520 --> 34:57.400
following operator on function so i have a function on the on the two sphere so this is in

34:58.360 --> 35:08.040
on s2 and now and i have a gamma and i can consider a new function which at a point x is

35:08.040 --> 35:14.280
given by the integral around a loop of my original function at distance gamma

35:20.200 --> 35:26.280
from my point there's a way of producing a new function so i've told you how to take a function

35:26.600 --> 35:33.080
s2 get a new function on s2 and it's a beautiful thought exercise that this is invariant this

35:33.080 --> 35:38.920
this is equivariant okay so if i move my function and then do this operation that's the same thing

35:38.920 --> 35:43.480
is doing this operation and then moving my function okay so these are these

35:45.960 --> 35:47.400
intertwiners so

35:50.040 --> 35:55.800
so that's the answer for what all these maps are and of course like that's still a infinite

35:55.800 --> 36:05.080
dimensional vector space but compared to you know functions on like if you're just thinking

36:05.080 --> 36:10.840
about linear maps here that's something like functions on s2 times s2 so it's like a four

36:10.840 --> 36:17.800
dimensional roughly speaking and it's kind of remarkable that just in employing this

36:17.800 --> 36:23.720
equivariance massively cuts down the number of parameters and of course you can make this whole

36:23.720 --> 36:29.560
picture even richer using spherical harmonics and there's like incredibly nice functions to put in

36:29.560 --> 36:37.560
here projecting to the irreducible representations inside functions on s2 etc i should have said

36:37.560 --> 36:42.760
very very much earlier like for me fun is just firstly it's to remind us that we're having fun

36:43.800 --> 36:48.920
secondly um it's just some kind of class of function so when i'm talking about the sphere

36:48.920 --> 36:52.920
i'm probably talking about l2 functions when i'm talking about a um discrete set i'm just talking

36:52.920 --> 37:00.040
about any function etc so yeah the point is that when we have a non-Abelian gamma there's a big

37:00.040 --> 37:08.840
reduction in parameters so um in the cnn slide there was this equivariance plus locality drastic

37:08.840 --> 37:14.120
reduces the number of parameters and here i'm kind of saying that equivariance plus non-Abelian

37:14.120 --> 37:20.280
drastically reduces the number of parameters which i think is very interesting so my task for myself

37:20.280 --> 37:25.240
and if you have any ideas i'd love to hear it is find an interesting learning problem

37:25.240 --> 37:31.160
where the symmetries are an interesting non-Abelian group what's a learning problem where the

37:31.160 --> 37:37.960
symmetries are naturally sl2 fq or something like that or you know some interesting groups so a lot

37:37.960 --> 37:43.960
of the groups that show up in machine learning are very much related to um three-dimensional space

37:43.960 --> 37:52.120
or two-dimensional space or so like p4 which consists of um all translations and 90-degree

37:52.120 --> 37:56.440
rotation shows up a lot and stuff like that but it would be lovely to inject some really

37:56.440 --> 38:00.200
interesting groups into this oh gl2 yeah gl2 would be great

38:05.320 --> 38:10.040
yeah that's it so stefan just suggested learning on hyperbolic space and that's a great

38:10.120 --> 38:17.480
great suggestion yeah i don't know why i didn't think of that i had fl2 uh sorry yeah so learning on

38:24.120 --> 38:28.360
okay there's enormous um possibilities here that i think are very interesting

38:29.320 --> 38:34.360
so let let's just go back to cnn's so the the question is basically like what the hell's going on

38:34.920 --> 38:44.520
so let's imagine so i'll try to explain um what the hell's going on and then we can have a short break

38:45.160 --> 38:53.480
so let's say that we're trying to do image processing so we're z mod hz squared we have functions on this

38:53.640 --> 39:00.920
and then we we want to have a layer of our neural net

39:02.920 --> 39:04.280
so typically

39:07.240 --> 39:12.200
one layer of our neural net might be like this you know one piece of one layer of our

39:14.840 --> 39:20.040
neural net might look like this so now what's the dimension of this space

39:21.000 --> 39:26.840
the dimension is h squared namely the number of

39:28.840 --> 39:36.200
points in the set and what's the dimension of this space the dimension whoops the dimension is h square

39:38.680 --> 39:43.800
okay so now if i were doing a fully connected neural net

39:44.760 --> 39:52.760
i would have h squared basis vectors here h squared basis vectors here and then i would have an h squared times h squared matrix

39:53.800 --> 40:00.760
so i'd have an enormous matrix of size h to the four that's and each of those parameters i have to train

40:01.960 --> 40:08.600
okay what do i do in cnn's i say i want this matrix to be invariant

40:08.760 --> 40:15.960
that already cuts down the number of parameters from h to the four back down to h squared

40:17.240 --> 40:21.000
and then i say i want this matrix to satisfy locality

40:22.120 --> 40:25.560
and that cuts down my number of parameters from h squared to nine

40:26.680 --> 40:32.520
so harini is asking uh what basically like why do you restrict the parameters to this extent

40:33.400 --> 40:35.480
so i would say that there's two reasons for this

40:36.440 --> 40:42.920
so these are inductive priors so they're not there's something that i believe is true about the solution

40:43.480 --> 40:48.920
they're not something that like is definitely true about the solution there's some category of practicality

40:48.920 --> 40:53.880
i want to build a model that works i can't train a hundred billion parameters but i can train

40:54.680 --> 40:59.880
you know a hundred or a thousand fine yeah and the inductive priors in this are invariants

41:00.600 --> 41:07.320
namely i can see you i can still see you yeah that's you know like you're still there like that's

41:07.320 --> 41:14.600
invariance yeah um and the other thing is locality and i think locality makes a lot of sense like when

41:14.600 --> 41:20.360
i look at this room i don't think the first thing that happens in my brain is i think oh i'm in

41:20.360 --> 41:30.440
kaslo 273 what happens first is i go oh edge corner chair person stairs light looks like a lecture hall

41:31.320 --> 41:41.400
sydney probably kaslo 273 yeah and so that's invariant that's um locality and these are our

41:41.400 --> 41:46.600
inductive priors and these inductive priors massively cut down the parameters and then from

41:46.600 --> 41:51.800
then we're cooking on gas and we can get these models that actually work no but it's again like

41:51.800 --> 41:59.240
changing the group is a gain inductive priors so um for example like you know have you been upside

41:59.240 --> 42:06.520
down and you look and it's actually much harder to recognize stuff so the idea that we satisfy this

42:06.520 --> 42:11.720
invariance is much less well established than the idea that we satisfy this invariance and then we

42:11.720 --> 42:19.560
want to bake in that symmetry exactly yeah i don't know if you really want rotations so like a classic

42:19.560 --> 42:25.080
pre-training task in image recognition is to recognize whether your image is upside down or not

42:26.440 --> 42:31.560
so you know that's a classically non-invariant thing under rotation by 180

42:34.200 --> 42:38.040
but there's there's situations like you know in an MRI scan or something

42:38.040 --> 42:42.760
where it really you know you really want that invariance that's when you should bake it into the model

42:46.760 --> 42:52.280
if you go back to our friend l2 of s2 there's a great exercise so the laplacian

42:52.280 --> 42:57.160
okay so the casimir gives you the laplacian on the sphere and the eigenspaces for the laplacian

42:57.160 --> 43:02.360
are the spherical harmonics so the the casimir is acting everywhere in this whole big diagram

43:02.360 --> 43:07.240
commuting with everything and splitting it up into irreducible it's not so much locality it's the

43:07.240 --> 43:19.640
fact that so three is maybe i can write so you know l2 of s2 is a topological direct sum of l gamma

43:20.680 --> 43:22.920
where gamma is

43:28.680 --> 43:29.880
is a spherical harmonics

43:37.720 --> 43:43.240
and the the casimir is providing this decomposition so it's breaking

43:44.520 --> 43:48.120
breaking up this space so the casimir on each one of these acts by a different scalar

43:49.240 --> 43:55.880
and it's the the casimir aka the laplacian acts on each of these you know this is something like

43:56.680 --> 43:59.640
restriction of of degree

44:01.880 --> 44:04.920
gamma or gamma over two polynomial polynomials

44:07.800 --> 44:15.160
and so you have this totally canonical decomposition of this space of functions

44:15.880 --> 44:24.520
and everything wherever i had this picture everything here is respecting this decomposition

44:24.520 --> 44:26.680
all the linear maps respecting this decomposition

44:30.040 --> 44:35.400
and roughly speaking you can kind of think about like you know if you take your function here

44:36.040 --> 44:40.600
and do a Fourier expansion of it then you get a whole lot of quantities

44:41.720 --> 44:44.520
and those quantities are giving you the

44:48.600 --> 44:52.440
you get a whole lot of scalars and those scalars are basically telling you how it acts on each one of these

44:58.680 --> 45:04.040
okay so this is just super interesting for me but maybe more technical

45:04.920 --> 45:11.400
uh so i want to go in the second half i want to go over why permutation representations i want to go over

45:11.400 --> 45:17.880
something called deep sets and then i want to go over graph neural nets

45:19.240 --> 45:25.080
so one question that is very natural to ask as a representation theorist

45:25.400 --> 45:35.560
is why so if you take functions on a set on a gamma set this is called a permutation representation

45:36.360 --> 45:40.360
and it's called a permutation representation because if you look at the matrices that represent

45:40.360 --> 45:48.200
your elements they're permutation matrices and in our first class in representation theory the

45:48.200 --> 45:52.840
first representations we see a permutation but then we quickly convince our students that

45:53.800 --> 45:58.120
we should break up permutation representations into irreducible representations and that's

45:58.120 --> 46:06.440
really interesting so why am i insisting that we have permutation representations everywhere

46:08.760 --> 46:13.640
so i i found that this charming little lemma which i i didn't find in the literature

46:14.360 --> 46:19.880
which is if you have a representation of gamma which is assumed finite i'm not quite sure what

46:19.880 --> 46:26.760
the analog of this for a league group is then we so once we have v we can choose a basis for it

46:27.640 --> 46:34.600
and once we have a basis we can ask is relu gamma equivalent yep so remember relu takes our

46:35.480 --> 46:39.240
vector which now that we have a basis is just a sequence of real numbers and the ones that are

46:39.240 --> 46:43.400
negative it sets to zero and the ones that are positive it keeps so is this gamma equivalent

46:44.120 --> 46:49.400
if our representation is just permuting our coordinates around it's easier to see that

46:49.400 --> 46:57.880
it's gamma equivalent but uh that's if and only if so in order to have a gamma equivalent

46:57.880 --> 47:01.480
relu with respect to some basis you have to be permutation with respect to that basis

47:04.760 --> 47:10.120
now this is something that has been exciting me a lot over the last few days and is having

47:10.120 --> 47:15.400
a hell of a lot of fun with is basically like piecewise linear representation theory

47:16.200 --> 47:22.200
um so what you could imagine is you you have these layers and they all break up into irreducible

47:22.200 --> 47:29.720
representations and if you include an irreducible into a permutation representation do a relu and

47:29.720 --> 47:36.520
then project back you get a piecewise linear endomorphism equivariant endomorphism of your

47:36.520 --> 47:40.760
representation and so what you could imagine is networks in which you have irreducible

47:40.760 --> 47:50.280
representations together with equivariant non-linearities and my impression is that this

47:50.280 --> 47:55.800
is extremely interesting and I've already learned like basic things about representations that I

47:55.800 --> 48:01.080
didn't know from thinking about this so if you're interested in this ask me but it's kind of much

48:01.080 --> 48:07.560
more specialized so I'm not going to talk about it today so let's I want to do another example

48:07.560 --> 48:14.040
of our blueprint so one way of seeing this is just first think about this

48:15.720 --> 48:21.320
so imagine that we have a whole lot of points and they're unordered and they have a whole

48:21.320 --> 48:27.320
lot of information attached to them for example you could imagine all of the citizens of Sydney

48:27.320 --> 48:33.080
and they're labeled by their age and how much tax they paid last year yeah so I've got a two

48:33.080 --> 48:39.560
dimensional vector associated to every person in Sydney now one way of viewing this data set is

48:39.560 --> 48:46.360
as a point cloud so what I have is an enormous in this you know age and tax example I just have R2

48:46.360 --> 48:52.040
and I have an enormous number of points in R2 and I want to make some qualitative statement so a basic

48:52.040 --> 48:56.440
statement that I could make is some kind of like center of mass statement like the average age of

48:56.440 --> 49:01.320
people in Sydney is blah that would be a kind of boring measurement but a much more interesting

49:01.320 --> 49:07.320
measurement would be there's this kind of weird hole in this data for example you know a particular

49:07.320 --> 49:12.680
age in a particular tax is you know not paid in some region or something like that that would be

49:12.680 --> 49:16.840
a much more interesting statement you can make about this data analysis and there's about this

49:16.840 --> 49:23.160
data and this is one of the one of the this is related to this very interesting subject called

49:23.160 --> 49:31.800
persistent homology which I know next to nothing about uh okay but we have a point cloud so we

49:31.800 --> 49:37.160
want to make a prediction based on this point cloud and so this is an equivariant prediction task

49:37.880 --> 49:46.920
we have so uh so here we have rd to the n so here are our n different points

49:47.240 --> 49:54.520
uh and we want to learn some function you know like is there a big hole in this data or something

49:54.520 --> 50:03.320
like that um and it's it's convenient to swap the indices so if you think about how s n acts here

50:04.280 --> 50:11.480
it acts like I have a whole packet of numbers um and then it permutes them around but it's more

50:12.280 --> 50:18.120
useful to view this from the from the point of equivariance as one packet of numbers like age

50:18.120 --> 50:22.520
that's being permitted around and one packet of numbers like how much tax was paid being

50:22.520 --> 50:29.480
commuted around and so I'll do this innocent rewriting um but I'm just pointing this out so

50:29.480 --> 50:35.160
it doesn't confuse the hell out of you on the next slide uh so we want to make an s n invariant

50:35.160 --> 50:39.640
prediction so basically we have in the language of representation theory we have a whole lot of

50:39.640 --> 50:45.800
copies of the most basic permutation representation of s n namely s n acts acting on functions on

50:45.800 --> 50:53.000
an n set and we want to make an s n invariant prediction so let's do some basic representation

50:53.000 --> 50:59.960
theory which I almost certainly learned from andrew at some point in about 30 or something

51:00.680 --> 51:06.520
so we take functions from one up to n functions on the set one up to n so this is a permutation

51:06.520 --> 51:12.200
representation of s n and we take the trivial representation so this is where every permutation

51:12.200 --> 51:20.440
just acts by the identity so here's a whole lot of homomorphism formulas so this is before I was

51:20.440 --> 51:25.880
saying what are the arrows in my neural net here I'm working them out explicitly what's what parameters

51:25.880 --> 51:29.640
there are so home from the trivial representation of the trivial representation this is the same

51:29.640 --> 51:36.360
thing as home from r to r this is r times the identity home from n to one this is another

51:36.360 --> 51:42.040
instance of this statement that on a permutation representation the only invariant measurement

51:42.040 --> 51:48.520
you can make is essentially summing up your entries that's that home back the other way

51:49.400 --> 51:54.920
here if we look at the image of one we want some vector we want some function which is

51:54.920 --> 52:00.200
invariant under s n i.e. we want this function to take the same value everywhere which is alpha

52:00.840 --> 52:08.120
now a little bit trickier a little bit like you know this would be a second week exercise

52:08.120 --> 52:12.200
in representations of the symmetric group or something is that home from the trivial from

52:12.200 --> 52:17.960
this permutation representation to itself is two-dimensional and it's spanned by the identity

52:17.960 --> 52:23.560
and the map that sums up the coordinates and and takes that sum and multiplies it by

52:24.520 --> 52:34.760
the constant function now exercise deduce this from the double coset formula all of these formulas

52:34.760 --> 52:38.840
are very easy concept consequences of the double coset formula why is that the case

52:40.760 --> 52:46.040
do it if you're a student you should do this and if you're a student it's not already obvious

52:46.040 --> 52:51.960
to you you should do this so i this is deep set architecture so you can look at so here's

52:52.760 --> 52:59.160
here's our paper from 2017 and to me as a non-machine learning person it looks a bit

52:59.160 --> 53:08.120
mystifying but this is just another instance of our blueprint so here's our input so this would be

53:08.200 --> 53:15.640
our three three-dimensional point cloud input yep so we have um three parameters per point

53:18.280 --> 53:26.680
now we do s in equilibrium maps and we uh sprinkle around ends and ones these are our building blocks

53:28.440 --> 53:37.880
now note how crazily this reduces the number of parameters for a large n so here if we had

53:37.880 --> 53:44.040
no assumption of s in equilibrium this would be an n squared space of parameters you know and n

53:44.040 --> 53:50.440
could easily be a million or something but now because we want this to be s in equilibrium we've

53:50.440 --> 53:57.160
just got two two possibilities here we've got one possibility here we've got one possibility here

53:57.160 --> 54:05.240
we've got one possibility etc so this allows us to make enormous um networks involving you know

54:06.200 --> 54:11.720
hundreds of billion you know billion dimensional things um with few parameters

54:13.720 --> 54:21.720
okay and that is um deep set architecture and it's i think it's um state of the art in terms of

54:22.920 --> 54:29.960
point cloud prediction okay graph neural nets if there's no questions

54:30.040 --> 54:38.120
uh graphs are everywhere in mathematics they come in many forms and variants so

54:40.760 --> 54:45.560
i just want you to keep in mind that graph here is a very um loose term it might be a

54:45.560 --> 54:50.760
directed graph it might be a digraph a directed graph the edges might be colored the vertices

54:50.760 --> 54:55.720
might be colored the edges might be weighted the vertices might be weighted the vertices might have

54:55.720 --> 55:00.600
10 parameters associated to them we might be talking about hyper graphs so that's graphs where we

55:00.600 --> 55:07.320
have like an edge need can connect more than what more than two two vertices etc so it's

55:07.320 --> 55:13.880
whole plethora of things called graphs and just want to emphasize that there's many ways so

55:13.880 --> 55:17.880
graphs are everywhere in mathematics but once you start thinking about them they're even more

55:17.880 --> 55:22.280
everywhere because there's a whole lot of stuff that wasn't obviously a graph initially and then

55:22.280 --> 55:27.560
you can make it a graph so examples of this uh you might say well you know graph theory is one

55:27.560 --> 55:31.720
dimensional topology and i'm a sophisticated eight dimensional topologist and i only care about

55:31.720 --> 55:36.360
eight dimensional manifolds yeah but if you take a compact eight dimensional manifold you can choose

55:36.360 --> 55:41.320
a point cloud on it and you'll get a graph and that graph tells you enormous amounts about that

55:41.320 --> 55:47.000
eight manifold if you have a simplicial complex you know for me like graphs are just one dimensional

55:47.000 --> 55:52.440
simplicial complexes and so i said to petah oh we should be sophisticated and learn on

55:52.440 --> 55:58.200
simplicial complexes and he said well a simplicial complex is just a graph truly you know here's

55:58.200 --> 56:04.200
my triangle here are my edges and here are my vertices it's a colored a simplicial complex is

56:04.200 --> 56:11.640
a colored vertex colored graph okay of a special form so this is another example this is from

56:11.640 --> 56:18.440
gaorg so if we have a a data set and it's somehow embedded in a space then we can get a natural

56:18.440 --> 56:25.720
metric graph out of it by looking at distances between vertices we might include the coordinates here

56:25.720 --> 56:33.000
we might do some funny function applied to this these lengths etc okay so graphs are everywhere

56:33.800 --> 56:41.080
and graph neural nets seem to be an incredibly powerful flexible way of dealing with um data so

56:42.040 --> 56:46.680
i think the graph neural nets have kind of really genuine like c and n's the thing that's that we

56:46.680 --> 56:51.640
stare at as mathematicians and think how could we make something like this that would help us in

56:51.640 --> 56:55.880
mathematics but i think that graphs are actually a thing that will help us in mathematics all the

56:55.880 --> 57:05.000
time so that's very worthwhile thinking about so what the graph neural nets do an example we might

57:05.720 --> 57:11.000
want to learn a function on graphs so an example would be a function which is learn planarity

57:11.960 --> 57:17.320
okay so this output's a positive number if it's planar a negative number if it's not planar so

57:17.320 --> 57:22.040
that would be a prediction task on graphs um we also might want to know for example the

57:22.040 --> 57:28.840
Euler characteristic of the graph that would be another example of a prediction task another

57:28.840 --> 57:34.920
important example kind of more like image like generalized image recognition is producing

57:35.880 --> 57:42.280
um some learning some function from functions on graphs to r so you might think that so

57:43.400 --> 57:49.880
no you you might repackage c and n's as being a grid and then an image is the same thing as a

57:49.880 --> 57:57.560
function on the vertices of this grid another very important thing is that um like there's many

57:57.560 --> 58:02.440
many incredibly interesting for example embedding problems of graphs so you give me a graph and I

58:02.440 --> 58:08.440
want to put it in some space in an interesting way um and one way of doing that would be to provide

58:08.440 --> 58:13.800
coordinates of where I want to put the vertices of that graph and so that would be an example of

58:13.800 --> 58:23.000
learning a function from graphs to functions on the vertices of a graph uh so I guess the takeaway

58:23.000 --> 58:28.840
from this is that anything to do with graphs graph neural nets uh useful for as long as it's not

58:28.840 --> 58:33.160
like an NP hard problem on graphs of which there are plenty yeah so graph neural nets aren't going

58:33.160 --> 58:36.440
to help you solve something like is there a Hamiltonian circuit or something like that

58:38.200 --> 58:43.320
so what's the basic idea so imagine that I give you a graph and you want to learn on it

58:45.000 --> 58:49.560
it's enormously difficult as far as I can work out to work out the automorphism of a group of a

58:49.560 --> 58:54.600
graph so this is something that people spend many many years thinking about from an algorithmic

58:54.600 --> 59:00.120
point of view and so I might not know what global symmetries are present so what I was talking about

59:00.120 --> 59:09.080
before does not apply um or that just like most graphs have no symmetry whatsoever um or you might

59:13.320 --> 59:17.640
so Gaston is asking what do you mean by hard um so

59:18.200 --> 59:28.200
that I mean yeah maybe maybe like NP or something like that but I just want I like in my mind there's

59:28.200 --> 59:35.560
there's stuff on graphs which is useful and maybe not so crazily difficult like um like

59:35.560 --> 59:40.360
embedding your graph in a nice way or something like that and then there's a whole lot of like

59:40.360 --> 59:45.560
seemingly innocent problems on graphs that are extremely hard um like embedding a graph

59:45.560 --> 59:50.120
improvably the nice the best way or something like that or finding a Hamiltonian circuit or

59:50.120 --> 59:56.600
stuff like that okay so in graph land it's easy to wander into an intractable problem

59:58.200 --> 01:00:04.920
um but there's also a whole lot of useful stuff that can be done so so there's plenty of local

01:00:04.920 --> 01:00:12.120
symmetry in graphs so around every vertex we have a symmetric group of symmetry and also we have a

01:00:12.120 --> 01:00:18.600
metric so you can imagine processes which are symmetric and kind of diffuse on the graph and

01:00:18.600 --> 01:00:21.880
that's what a graph neural net is so I'll quickly go over the architecture

01:00:24.840 --> 01:00:30.040
so this is an important slide so here's my graph

01:00:30.840 --> 01:00:41.080
and as part of my architecture I fix n1

01:00:42.360 --> 01:00:49.880
n2 n3 and of course I'm just telling you one possible variant of like a thousand different

01:00:49.880 --> 01:00:55.880
possibilities of building graph neural net but once you've seen one of them then the other ones

01:00:55.960 --> 01:01:03.160
make a lot more sense so we fix these n1 and then what we do is each of our layers is a sum

01:01:03.160 --> 01:01:10.520
over the vertices of that particular rn1 okay so you know in a vanilla neural net we just fix

01:01:10.520 --> 01:01:19.480
dimensions here we fix dimensions at every vertex so that's this and so in this particular case my

01:01:19.800 --> 01:01:23.640
my neural net looks like this so I have

01:01:27.240 --> 01:01:31.960
three layers so here I have some linear map here I have a relu here I have a linear map a relu

01:01:33.880 --> 01:01:42.600
and then a fully connected layer okay so what do I do basically I train

01:01:43.560 --> 01:01:57.960
self and neighbor maps so here's the formula down here so I'm telling you what phi x of v so here's

01:01:57.960 --> 01:02:08.920
my my layer which is phi and I wanted to find you this map and in order to do that I can tell you

01:02:08.920 --> 01:02:16.520
this map evaluated at a particular vertex so that this might be vertex v and what I'm saying in

01:02:16.520 --> 01:02:25.000
order to get that answer what you do is you take this self map times whatever I've got here plus

01:02:25.000 --> 01:02:32.760
all of these neighbor maps so roughly speaking in my second layer something here has a term that

01:02:32.760 --> 01:02:39.720
comes from here together with terms that come from the three neighbors so it's a very natural

01:02:40.440 --> 01:02:53.400
it's called a message pass and as usual si is something like is affine linear

01:02:54.200 --> 01:03:03.400
okay so s1 is an n1 times n2 matrix

01:03:06.440 --> 01:03:07.720
plus an n2 vector

01:03:10.440 --> 01:03:16.600
okay so each of these so this is yes thank you Brian this should be an s2

01:03:16.600 --> 01:03:27.480
yeah so that's very important that so this is an another example of an inductive prior

01:03:28.120 --> 01:03:34.040
sorry inductive bias or a prior so what we're saying is that we want these n1

01:03:35.480 --> 01:03:42.600
these n1s sorry Stefan asked should all these n1s be the same and the answer is in general yes

01:03:43.480 --> 01:03:50.360
so we want for example the n1s that talk to this guy from here and from here

01:03:51.160 --> 01:03:58.280
to be the same n1s that talk to this guy from here here okay so the n1s are the same so

01:03:58.280 --> 01:04:00.040
if you imagine this matrix here

01:04:03.080 --> 01:04:09.720
it looks like something like s1 s1 s1 s1 down the diagonal and then n1s

01:04:09.960 --> 01:04:15.320
in off diagonal places given by the adjacency matrix

01:04:17.400 --> 01:04:21.480
you know something like this so inside this space of like

01:04:22.280 --> 01:04:29.400
n1 times vertices times n2 times vertices so this is what my big matrix would look like

01:04:29.400 --> 01:04:33.880
I'm saying like it should be block diagonal and a whole lot of blocks should be the same

01:04:33.880 --> 01:04:37.240
so it's a very strong inductive bias to

01:04:39.880 --> 01:04:48.920
assume now if I'm honest you know we might have seven like let's say two different colors on

01:04:48.920 --> 01:04:56.120
our vertices and then we would train um neighbor maps that preserve the color of vertices neighbor

01:04:56.120 --> 01:04:59.880
maps that change the color from red to blue neighbor maps that change the color from blue

01:04:59.880 --> 01:05:05.960
to red etc so there's a zillion variants but in the basic vanilla version of a neural net

01:05:05.960 --> 01:05:10.680
we assume all the n1s are the same and all the s1s are the same so this one's

01:05:11.320 --> 01:05:13.800
so I'll give the diagonal term of this s1

01:05:23.400 --> 01:05:26.360
so it's a very complicated slide but it's a simple idea I think

01:05:27.480 --> 01:05:34.200
so we do that for a number of times and then we evaluate or in the situation where we're

01:05:34.200 --> 01:05:37.880
trying to learn for example and embedding or something like that we wouldn't do the final

01:05:37.880 --> 01:05:44.440
layer we've got some coordinates on our vertices and we're happy another classic example of a task

01:05:44.440 --> 01:05:49.720
might be you want to divide your vertices into two classes and so then you would you'd do all your

01:05:49.720 --> 01:05:53.800
layers and then you would say at the end uh this is a real number and then you would softmax that

01:05:53.800 --> 01:06:00.120
and then that would be the probability that your vertex is in or out of this class okay and also

01:06:00.120 --> 01:06:05.720
there's you know a million variants often the neighbor term is weighted by one over the degree

01:06:05.720 --> 01:06:12.440
of the vertex okay that's what I set up there it's affine maps

01:06:17.240 --> 01:06:21.720
okay so that's the architecture and we'll have some fun playing with this architecture in the

01:06:21.720 --> 01:06:31.160
colab tomorrow so many variants are possible for digraphs digraphs you might think that you only

01:06:31.160 --> 01:06:35.480
train a forward map but generally you don't you train a forward map and a back with map

01:06:36.680 --> 01:06:43.160
so back forward and backwards neighbor maps if graph has edge coloring you can train message

01:06:43.160 --> 01:06:49.480
passing for every color of the edge vertex coloring similarly you might also have a couple

01:06:49.480 --> 01:06:54.760
of global parameters hanging around and in every step your global parameters speak to the parameters

01:06:54.760 --> 01:07:00.440
on the graph and are spoken to by parameters from the graph in some way which is probably nothing to

01:07:00.440 --> 01:07:08.440
do with the n ones on the s ones so these are some nice examples imagine that I consider this

01:07:08.440 --> 01:07:17.720
digraph here if you look at what a graph neural net does on this digraph it exactly mimics cnn's

01:07:18.280 --> 01:07:26.440
without falling layer so you know it's it's it's not it's not an exaggeration to say that

01:07:27.800 --> 01:07:30.280
cnn's are a subset of giant graph neural nets

01:07:36.920 --> 01:07:44.440
and if you kind of unpack this definition for uh deeps for a complete graph you

01:07:45.400 --> 01:07:47.320
uh basically get deepsets

01:07:49.800 --> 01:07:55.720
and that is all for today so thank you very much and yeah if there's any questions please ask

01:07:55.720 --> 01:07:59.480
so gay i'll ask a question i'll just ask uh answer the question of dr bouts you first

01:08:01.000 --> 01:08:04.680
I wonder if by adding linear maps between vertices of the graph and making the whole

01:08:04.680 --> 01:08:12.920
architecture commutative kind of quirl net has any ml interpretation yeah so I totally agree

01:08:12.920 --> 01:08:17.560
that when you look at neural net architectures it looks very much like river varieties and

01:08:17.560 --> 01:08:22.920
things that we study in representation theory however one really can't underestimate the

01:08:22.920 --> 01:08:31.400
effect of this relu of like so if you think about a like just a classic feed forward vanilla neural

01:08:31.400 --> 01:08:38.200
net um if you don't have the relu's we understand totally what happens by basically near algebra

01:08:39.160 --> 01:08:43.800
but when we add the relu's we can suddenly approximate any function on a compact set

01:08:44.440 --> 01:08:51.720
so and we we have absolutely no idea of what happens inside the the neural net so yeah I find

01:08:51.720 --> 01:08:56.840
the quiver language like very useful to think to think about but um one shouldn't underestimate

01:08:57.800 --> 01:09:04.840
relu's so gaug asked um what have graph neural nets been used for

01:09:06.200 --> 01:09:15.560
um and my understanding is that um like I don't know like let's say half of facebook and 75 percent

01:09:15.560 --> 01:09:21.880
of twitter is graph neural nets um because you've got all these like connection graphs and social

01:09:21.880 --> 01:09:30.120
networks and stuff like that um graph neural nets were like the absolute center thing that we used

01:09:30.120 --> 01:09:37.400
with deep mind too on this work on cash analysis polynomials um my understanding is that graph neural

01:09:37.400 --> 01:09:45.240
nets are kind of um taking over neural net world in terms of they're very flexible um they're very

01:09:45.240 --> 01:09:51.720
powerful um and a lot of tasks where for example c and n would involve like a drastic change of

01:09:51.720 --> 01:09:58.360
architecture um in graph neural nets you can just like add a vertex or something like that so

01:09:58.360 --> 01:10:03.080
they're very flexible powerful framework for machine learning as far as I can make out

01:10:04.520 --> 01:10:09.240
sam yates asked for discrete valued problem could we pick other group rings say with some

01:10:09.240 --> 01:10:17.000
choice of analogous relu like function perhaps uh that's an intimidating prospect for me because

01:10:17.000 --> 01:10:23.080
I wouldn't know how to train and things like that so my very vague understanding so there's this

01:10:23.080 --> 01:10:30.360
kind of revolution in the last kind of three four years given by transformers and my understanding

01:10:30.360 --> 01:10:39.880
of that is like basically like a a graph neural net um hooked on to an lstm so like the graph neural

01:10:39.880 --> 01:10:46.040
net kind of decides where to look back in the sentence and things like that so um but I'm not

01:10:46.040 --> 01:10:51.800
directly aware of any recurrent technologies used directly with graph neural nets what another

01:10:51.800 --> 01:10:56.280
thing that graph neural nets are very useful for which is kind of counterintuitive is like predicting

01:10:56.280 --> 01:11:02.520
graph structure so you have data and you want to um and you want to predict which edges exist

01:11:03.080 --> 01:11:07.080
so like you might want to predict social relationships or something like that and um my

01:11:07.080 --> 01:11:11.880
understanding is that you you start off with a complete graph and run a graph neural net

01:11:11.880 --> 01:11:17.560
and your graph kind of learns a probability of discarding an edge and that's very very powerful

01:11:19.320 --> 01:11:23.880
so it it's intuitive and intuitive but graph neural nets can learn graphs which I think is

01:11:23.880 --> 01:11:28.920
awesome I think I think like learning graphs is like learning graphs is a really difficult

01:11:28.920 --> 01:11:37.240
problem in machine learning um and yeah of course so it's going to be yeah it's an intimidating

01:11:37.240 --> 01:11:43.960
problem that or n choose tuna yeah so that's a really good question so what kind of problems

01:11:43.960 --> 01:11:50.040
and why can be addressed with graph neural nets my understanding is that very recently there's been

01:11:50.040 --> 01:11:56.120
kind of benchmarking so there's been like a list of 50 problems and some some attempt to decide

01:11:56.120 --> 01:12:00.680
you know what what can various classes of graph neural net algorithms do and what they can't do

01:12:01.480 --> 01:12:11.320
um my very rough picture as as a graph neural net greenhorn is um that anything that you can kind

01:12:11.320 --> 01:12:17.800
of decide via a finite like a small amount of walking around your graph and if you if if you're

01:12:17.800 --> 01:12:21.320
extremely smart and you're allowed to walk around your graph a little bit and you can already make

01:12:21.320 --> 01:12:26.280
a decision then that's something that you could solve so something like detecting planarity or

01:12:26.280 --> 01:12:30.440
detecting a three cycle or something this is something that you can solve but if it involves

01:12:30.520 --> 01:12:35.960
exploring the whole graph particularly potentially in many many different iterations for example

01:12:35.960 --> 01:12:41.240
finding a Hamiltonian cycle or something um yeah it's not going to work

01:12:43.160 --> 01:12:46.760
she'll have said the piecewise linear representation can you say a bit more about it

01:12:47.720 --> 01:12:51.720
sure so how about I will ask answer Joel's question and then maybe I can say something

01:12:51.720 --> 01:12:55.640
about it but I'll give other people the chance to leave because it's somewhat specialised

01:12:56.360 --> 01:13:01.640
so a part of Adam salt Wagner's work was learning graphs learning good people

01:13:03.160 --> 01:13:07.000
to a particularly conjecture for instance so we might hear about learning graphs in a few weeks

01:13:07.000 --> 01:13:11.640
yes exactly so salt Wagner's work is using reinforcement learning to produce interesting

01:13:11.640 --> 01:13:15.880
graphs okay so now we'll declare it over and anybody that's interested in this piecewise

01:13:15.880 --> 01:13:23.960
linear business can stick around I think this is super beautiful and um I will just explain it

01:13:23.960 --> 01:13:29.720
briefly so let's just consider the following silly examples so we're looking at the

01:13:31.160 --> 01:13:43.080
sn acting on s3 acting on r3 or permutation representation now we know that r3 decomposes

01:13:43.160 --> 01:13:54.520
canonically as nat plus triv and that is the set of vectors in r3 such that the sum of the lambda

01:13:54.520 --> 01:14:11.160
i is zero and triv is the set of r times 111 and this decomposition is completely canonical

01:14:13.400 --> 01:14:21.080
so now you can ask the following like just kind of totally naive question if we go from nat

01:14:22.520 --> 01:14:26.680
into r3 and then apply value

01:14:29.640 --> 01:14:38.280
go to r3 back down to nat the composition is an s3 equivariant

01:14:43.720 --> 01:14:48.680
pl endomorphism

01:14:49.640 --> 01:14:55.800
i.e in the category of piecewise linear maps from this vector space to itself that a s3

01:14:55.800 --> 01:14:59.080
equivariant this is a pl endomorphism and what is it

01:15:02.120 --> 01:15:10.040
it's super beautiful so basically what you do is inside so here's nat so we divide up

01:15:10.040 --> 01:15:21.640
nat so just for people that don't do this every day nat is the um symmetries of the triangle

01:15:24.440 --> 01:15:29.880
embedded inside r3 r2 okay so we just take an equilateral triangle and we take the symmetries

01:15:29.880 --> 01:15:43.560
of the equilateral triangle so now um there's three regions here and what happens so there's six

01:15:43.560 --> 01:15:55.480
regions there's the blue regions and the red regions and the blue regions get squashed

01:16:00.840 --> 01:16:03.880
and the red regions get kind of expanded out

01:16:09.160 --> 01:16:10.200
so these get squashed

01:16:12.760 --> 01:16:13.800
and these get expanded

01:16:19.080 --> 01:16:25.960
and i don't know this is just like a very beautiful basic um like pl endomorphism of a

01:16:26.040 --> 01:16:32.360
representation that i've never encountered before in my life okay and you can start having fun like

01:16:32.360 --> 01:16:32.920
what is this

01:16:38.440 --> 01:16:44.760
or nat inside r in and you know this is a nice exercise

01:16:45.320 --> 01:16:56.680
and yeah so that one of the things that i find really interesting is that home

01:16:59.400 --> 01:17:05.320
pl from any representation to r um is interesting

01:17:05.800 --> 01:17:18.120
okay so example is that home from pl from the sign representation to r

01:17:19.160 --> 01:17:21.640
contains the absolute value map

01:17:25.960 --> 01:17:31.080
um like this is for this is the sign representation of s2 oh no sign representation in general

01:17:31.080 --> 01:17:40.680
in fact um but home from the trivial representation pl to any irrep

01:17:42.680 --> 01:17:46.840
not equal to the trivial zero

01:17:49.640 --> 01:17:55.160
and i kind of feel like this is telling us something remarkable that about kind of how

01:17:55.240 --> 01:18:00.840
equivalence kind of flows through a neural net so this is still very speculative but what

01:18:00.840 --> 01:18:08.680
i feel like is that you have some kind of measure of complexity so it like at the start you have all

01:18:08.680 --> 01:18:09.320
all irreps

01:18:14.120 --> 01:18:20.840
and then at the end you have the trivial and then you have the maps in the in the neural net so you

01:18:20.840 --> 01:18:30.920
have linear and then you have pl and then you have linear pl and these pl maps have a definite

01:18:30.920 --> 01:18:35.480
sense of direction like you know once we get through the trivial representation we can never

01:18:35.480 --> 01:18:42.760
get out of it again and like i don't know this seems to explain some some very interesting

01:18:42.760 --> 01:18:47.560
aspects of neural nets but it's just exciting stuff that i've been thinking about last week so

01:18:47.640 --> 01:18:52.920
very unbaked um so maybe when it's baked i can uh talk more about it

01:18:55.560 --> 01:18:59.640
so thanks are the blue rays there inside the reflecting hyperplanes

01:19:01.480 --> 01:19:03.240
this is alpha one and this is alpha two

01:19:05.880 --> 01:19:12.120
so no oh yeah the so the reflecting hyperplanes would be like this and this and this

01:19:13.880 --> 01:19:15.400
thanks everyone i think we'll stop now

01:19:17.560 --> 01:19:18.060
you

