1
00:00:00,000 --> 00:00:13,200
So, welcome to this week's lecture.

2
00:00:13,200 --> 00:00:16,800
So, seminar structure.

3
00:00:16,800 --> 00:00:22,240
Next week there'll be some linear combination of Gayog and Geordi, and the constant is yet

4
00:00:22,240 --> 00:00:24,040
to be determined.

5
00:00:24,040 --> 00:00:29,200
The week after, so after next week we move into the talks from experts.

6
00:00:29,200 --> 00:00:33,820
So, we've been trying to give you background and then the experts will tell us what's really

7
00:00:33,820 --> 00:00:34,820
going on.

8
00:00:34,820 --> 00:00:38,440
So, I'm really excited by the lineup.

9
00:00:38,440 --> 00:00:45,680
So, Adam Saltbaugner has done some amazing work using reinforcement learning to construct

10
00:00:45,680 --> 00:00:48,880
counter examples in graph theory.

11
00:00:48,880 --> 00:00:54,720
If you want to prepare for his talk, the paper is really beautifully written.

12
00:00:54,720 --> 00:00:57,640
Bandad Hosseini works on graph methods.

13
00:00:57,640 --> 00:01:03,120
So, we'll see a bit of graph neural nets today and we'll see more in his talk.

14
00:01:03,120 --> 00:01:10,040
Carlos Simpson is talking on a kind of archetypal garden of how to use machine learning and

15
00:01:10,040 --> 00:01:12,760
mathematical proof.

16
00:01:12,760 --> 00:01:16,360
And I think that that's extremely interesting.

17
00:01:16,360 --> 00:01:22,960
And then there's another three talks lined up after that, taking us to the end of semester.

18
00:01:22,960 --> 00:01:26,760
And the other thing I wanted to point out is Joel has been doing an amazing job with

19
00:01:26,760 --> 00:01:27,760
the toe labs.

20
00:01:27,760 --> 00:01:31,440
If you've been taking part in the tutorials, you'll be aware of this.

21
00:01:31,440 --> 00:01:35,800
But if you're watching online and you just want to muck around with something, they're

22
00:01:35,800 --> 00:01:38,360
really great and I'm hoping that they provide a good resource.

23
00:01:38,360 --> 00:01:43,160
So, we'll hope to keep these going for the expert talks so you can play with some of

24
00:01:43,160 --> 00:01:46,760
Adam's ideas, play with some of Bumdud's ideas, et cetera.

25
00:01:46,760 --> 00:01:51,200
So, again, the seminar principles.

26
00:01:51,200 --> 00:01:57,440
So, today we'll be about geometric deep learning and geometric deep learning is very much the

27
00:01:57,440 --> 00:02:04,520
question of how do you incorporate symmetry into a learning problem.

28
00:02:04,520 --> 00:02:12,560
But before we go into that, I just want to recall the notion of an inductive bias, which

29
00:02:12,640 --> 00:02:15,360
is a very important notion in machine learning.

30
00:02:15,360 --> 00:02:18,520
So what's going on here?

31
00:02:18,520 --> 00:02:25,040
So it's very common situation in maths that you have some problem and you know or suspect

32
00:02:25,040 --> 00:02:27,680
something about the solution.

33
00:02:27,680 --> 00:02:31,840
So a basic example would be something like we expect the solution to be smooth or we

34
00:02:31,840 --> 00:02:34,520
hope the solution is smooth.

35
00:02:34,520 --> 00:02:39,560
The solution should satisfy conservation of energy, for example, there'll be some differential

36
00:02:39,560 --> 00:02:42,160
equation that the solution should satisfy.

37
00:02:42,160 --> 00:02:47,320
The solution should be invariant under a group, so this occurs all the time in physics.

38
00:02:47,320 --> 00:02:52,800
The solution might be locally determined, the counter example, so also we might be looking

39
00:02:52,800 --> 00:02:57,480
for a counter example, we might suspect something about the counter example.

40
00:02:57,480 --> 00:03:03,520
So the fancy names for this are inductive bias or prior.

41
00:03:03,520 --> 00:03:07,520
So inductive bias just means something that you know about the problem a priori before

42
00:03:07,520 --> 00:03:08,840
starting to solve it.

43
00:03:08,840 --> 00:03:16,480
And it's very important to remember what inductive biases there are in a problem.

44
00:03:16,480 --> 00:03:20,800
So how does one imagine that one has some inductive bias?

45
00:03:20,800 --> 00:03:23,280
So for example, the solution might be smooth.

46
00:03:23,280 --> 00:03:30,880
So this is related very much to regularization.

47
00:03:30,880 --> 00:03:38,400
So in this was Gayog's talk last week.

48
00:03:38,400 --> 00:03:44,280
So for example, he talked about this Gaussian kernel, which very much encourages functions

49
00:03:44,280 --> 00:03:52,280
that have you know Fourier modes that aren't wiggling around too quickly, that are smooth

50
00:03:52,280 --> 00:03:54,880
in a very strong sense.

51
00:03:54,880 --> 00:03:58,840
Solution should satisfy conservation of energy, this might be another example of regularization.

52
00:03:58,840 --> 00:04:02,880
There's also these fascinating things, if you want to have a Google called physically

53
00:04:02,880 --> 00:04:10,360
informed neural nets.

54
00:04:10,360 --> 00:04:15,920
So here you don't require the solution to satisfy some differential equation, but you

55
00:04:15,920 --> 00:04:18,560
add the differential equation to the cost function.

56
00:04:18,560 --> 00:04:23,720
And so it encourages the solution to satisfy a differential equation.

57
00:04:23,720 --> 00:04:26,400
Problem should be the solution should be invariant under.

58
00:04:26,400 --> 00:04:29,640
So this is today.

59
00:04:29,640 --> 00:04:31,600
That's the subject of today.

60
00:04:31,600 --> 00:04:33,720
The solution might be locally determined.

61
00:04:33,720 --> 00:04:39,720
So this is like examples might be CNN's or LSTM.

62
00:04:39,720 --> 00:04:44,240
Okay we expect for example small part areas of small parts of an image to play an important

63
00:04:44,240 --> 00:04:47,000
role initially.

64
00:04:47,000 --> 00:04:49,240
And the counter example is probably highly connected.

65
00:04:49,240 --> 00:04:53,600
I included this example because this is an example where I've got no idea how to put

66
00:04:53,600 --> 00:04:55,600
this kind of information into a network.

67
00:04:55,600 --> 00:05:03,800
And this was definitely my experience with DeepMind in that often, so the movement from

68
00:05:03,800 --> 00:05:10,480
an inductive bias to the neural net design is art rather than science.

69
00:05:10,480 --> 00:05:16,800
So several times with the DeepMind team I said oh you know I know this about the solution

70
00:05:16,800 --> 00:05:22,040
and they said we have absolutely no idea how to incorporate this into the model.

71
00:05:22,040 --> 00:05:29,520
And I think that one should be aware that this is often a big issue.

72
00:05:29,520 --> 00:05:35,520
So yeah one should be just be aware that it's very important to have some kind of inductive

73
00:05:35,520 --> 00:05:43,760
bias and be aware of it but you might not know what to do with it.

74
00:05:43,760 --> 00:05:50,560
And this is just the same slide that very similar to things that Georg has been saying.

75
00:05:50,560 --> 00:05:57,360
You have the capacity of a model roughly speaking how many parameters it has and there is this

76
00:05:57,360 --> 00:06:00,240
playoff between simplicity and expressivity.

77
00:06:00,240 --> 00:06:07,960
So a lot of parameters means you can express for example any function but training may

78
00:06:07,960 --> 00:06:13,680
be infeasible you might have hundreds of billions of parameters and it's less interpretable

79
00:06:13,680 --> 00:06:21,280
and so there's often sweet spots but in this trade off.

80
00:06:21,280 --> 00:06:28,640
Okay so today what I'm talking about is symmetry in neural networks and I'm really extremely

81
00:06:28,640 --> 00:06:33,240
happy to give this talk because it's been kind of a revelation for me.

82
00:06:33,240 --> 00:06:39,040
So I began with this book Geometric Deep Learning by Bronstein Bruner Cohen and Petar

83
00:06:39,040 --> 00:06:45,160
Velichkovich who I worked with on the DeepMind project and Gayog pointed out this group here

84
00:06:45,160 --> 00:06:50,960
with very convolutional networks and Petar recently just pointed out this steerable CNNs

85
00:06:50,960 --> 00:06:57,600
and it's very interesting subject and I think it's a really remarkable example.

86
00:06:57,600 --> 00:07:03,720
So in physics we often see this phenomenon where just knowing some kind of symmetry is

87
00:07:03,720 --> 00:07:08,280
present has enormous effects on your ability to solve a problem or formulate a model or

88
00:07:08,280 --> 00:07:12,840
something like that so there's this extraordinary paper I think it's by gross called symmetry

89
00:07:12,840 --> 00:07:19,480
and physical laws which I found really inspiring and I find this

90
00:07:19,480 --> 00:07:24,080
equivariance in convolutional neural nets to be a kind of similar story it's like it

91
00:07:24,080 --> 00:07:28,720
seems so innocent to require some kind of invariance and yet it essentially determines

92
00:07:28,720 --> 00:07:32,800
your entire architecture it's really remarkable.

93
00:07:32,800 --> 00:07:38,160
So never underestimate symmetry I had this on my web page for about 10 years as the first

94
00:07:38,160 --> 00:07:40,920
thing that you read.

95
00:07:40,920 --> 00:07:50,120
So I want to review what kind of vanilla CNN is and then so I'll first just remember

96
00:07:50,120 --> 00:07:57,040
what a CNN is and then we'll view a CNN through the language of group theory and I want to

97
00:07:57,040 --> 00:08:06,440
try to convince you that three basic principles already basically determine CNN architecture.

98
00:08:06,440 --> 00:08:14,360
So here's my image in the top right so this is a grayscale image I'm assuming that it

99
00:08:14,360 --> 00:08:19,560
is on a square so I have a fixed width and a fixed height a fixed number of pixels wide

100
00:08:19,560 --> 00:08:27,840
a fixed number of pixels high and I have my pixel value is given by a real valued function

101
00:08:27,840 --> 00:08:35,120
so for example this pixel value might be between zero and 255 and what I'm looking for is some

102
00:08:35,240 --> 00:08:41,160
function from this is called a so I'm calling this a periodic image because I'm kind of

103
00:08:41,160 --> 00:08:49,640
wrapping the top you know the physicists would say I'm compactifying on the torus and you

104
00:08:49,640 --> 00:08:54,320
know I sound very fancy when I say that but we're just simplifying our situation by imagining

105
00:08:54,320 --> 00:09:00,160
that our image is on the torus and there's it just simplifies the group theoretic discussion

106
00:09:00,160 --> 00:09:03,400
in a second and there's no genuine need to do this.

107
00:09:03,400 --> 00:09:11,280
So we're seeking some function from periodic images i.e functions on this grid to the real

108
00:09:11,280 --> 00:09:18,760
numbers which are for example positive on tigers and negative on non-tigers is a classic

109
00:09:18,760 --> 00:09:22,680
and a machine learning problem and also the problem on which machine learning has been

110
00:09:22,680 --> 00:09:36,280
wildly successful and we do this in the following way so we have several layers and typically

111
00:09:36,280 --> 00:09:47,200
these layers will consist of other periodic images perhaps at lower scales so when in

112
00:09:47,200 --> 00:09:51,880
the first two layers of this neural net I'm assuming that my periodic image is the same

113
00:09:51,960 --> 00:09:58,360
scale and then in the third layer I'm assuming that the periodic image has dropped in scale

114
00:09:58,360 --> 00:10:03,400
somewhat so I'm this h is assumed to h prime is assumed to divide h.

115
00:10:09,080 --> 00:10:14,680
So what one should think about this problem so this problem this seek so we're seeking a

116
00:10:14,680 --> 00:10:20,120
function from here to here and this function is going to be highly non-linear

117
00:10:21,960 --> 00:10:28,520
so it's a non-linear function on this big vector space and I guess one of the points

118
00:10:28,520 --> 00:10:34,040
of machine learning is that learning a highly non-linear function on a high dimensional vector

119
00:10:34,040 --> 00:10:41,240
space is a very difficult task and we get enormous amount of mileage out of viewing it

120
00:10:41,240 --> 00:10:48,840
as composed of out of rather simple functions so the simple functions that we use are convolutions

121
00:10:48,840 --> 00:10:52,120
so this might be like we might have some kind of filter here

122
00:10:55,720 --> 00:11:06,920
the filter and this might be for example 8-1-1-1-1-1-1-1-1-1-1-1-1-1 and this particular filter

123
00:11:07,720 --> 00:11:14,840
we point wise multiply with our image and then sum up the result and so this particular filter

124
00:11:14,840 --> 00:11:20,920
would have the effect that on areas of blank color we would get a very low value but on

125
00:11:20,920 --> 00:11:25,880
out on edges we would get a high value so it would be have a kind of um outlining effect

126
00:11:27,160 --> 00:11:33,720
so this is an example of something that we might uh convolve with so that's applying this

127
00:11:33,720 --> 00:11:40,120
filter can be rephrased as a convolution and what we do so we convolve and then we apply

128
00:11:40,920 --> 00:11:49,720
apply a value and then we convolve and we apply a value and then we do an operation called pooling

129
00:11:49,720 --> 00:11:54,360
that I'll basically ignore here which might be you look at a grid of pixels and take the maximum

130
00:11:54,360 --> 00:12:00,600
element so this will take you down to a smaller image and then finally at the end you might do

131
00:12:00,600 --> 00:12:07,960
some fully connected layers and the point of all this is that we don't we specify this architecture

132
00:12:08,680 --> 00:12:14,680
uh at the beginning and we specify all the values at the beginning but we learn the convolutions

133
00:12:14,680 --> 00:12:17,240
that's the important point we learn these filters

134
00:12:19,560 --> 00:12:25,240
okay so that was meant to just be a review now I want to look at this through the language of group here

135
00:12:29,000 --> 00:12:32,600
so this is just a direct copy of the previous architecture

136
00:12:33,320 --> 00:12:44,440
so z mod h squared sorry z mod hz squared is a group so it's z mod hz times z mod hz

137
00:12:46,360 --> 00:12:47,960
very simple abelian group

138
00:12:53,720 --> 00:12:59,480
and as with any group it acts on functions on that group

139
00:13:00,360 --> 00:13:07,800
so if I have a function on my group what I can do is translate my group around and I get a new

140
00:13:07,800 --> 00:13:15,080
function okay and convolving by a single so when I translate around by my group this is the same

141
00:13:15,080 --> 00:13:23,720
thing as convolving with a delta function on my group so on my group I can consider the function

142
00:13:24,360 --> 00:13:32,600
that's just one at a particular group element and zero elsewhere and convolving with that element

143
00:13:32,600 --> 00:13:36,120
is the same thing as translating by that group element

144
00:13:38,920 --> 00:13:47,480
and uh so any any convolution is a linear combination of these convolution with a single

145
00:13:47,480 --> 00:13:54,280
with a delta function and so this is gamma equivalent so another way so in the abelian case

146
00:13:54,280 --> 00:14:01,960
kind of nothing nothing matters in terms of orders you know when I wrote g dot f of x equals f of

147
00:14:02,600 --> 00:14:08,680
g plus x it doesn't matter whether I whether I write x plus g or x minus g but in the non-abelian

148
00:14:08,680 --> 00:14:14,520
case I'd have to have an inverse and in the non-elabelian case I would kind of think about

149
00:14:14,760 --> 00:14:20,520
convolutions as maybe acting from the right or something like that but it's a general fact that

150
00:14:22,360 --> 00:14:30,520
the if we look at functions on a group then the equivalent maps from functions on a group to

151
00:14:30,520 --> 00:14:35,320
functions on a group are the same thing as functions on that group acting by a convolution

152
00:14:36,360 --> 00:14:42,040
and that's what I that's what I say here so the equivalent maps from functions on the group to

153
00:14:42,040 --> 00:14:49,240
functions on the group are simply functions on the group okay and this is true for any gamma

154
00:14:51,000 --> 00:14:54,440
this is very basic representation theory if you will

155
00:14:59,800 --> 00:15:07,960
so remember that I hate questions and any question will involve um a horror show

156
00:15:08,920 --> 00:15:10,280
so please don't ask any questions

157
00:15:15,160 --> 00:15:20,360
so what are the basic observations about this so we want often in these problems we want a

158
00:15:20,360 --> 00:15:26,680
gamma invariant answer so if we move our picture of a cat around if we translate around the answer

159
00:15:26,680 --> 00:15:31,160
should still be cat okay this is one of the reasons that I assumed a periodic

160
00:15:31,480 --> 00:15:40,920
periodic image that's very important another point is that so there's another classic machine

161
00:15:40,920 --> 00:15:47,960
learning task which is called image segmentation which asks us to say where are the two eyes in

162
00:15:47,960 --> 00:15:53,720
this picture or you know when your phone for example tells you where the head of the people

163
00:15:53,720 --> 00:15:59,160
this is an example of image segmentation now if you think about what this task is

164
00:15:59,160 --> 00:16:04,440
this is not invariant it's equivariant this means that if I move the image

165
00:16:05,560 --> 00:16:12,280
my my prediction should move in the same way so we often want a gamma invariant answer or a gamma

166
00:16:12,280 --> 00:16:23,400
equivariant answer

167
00:16:23,400 --> 00:16:45,320
okay convolution and value are gamma equivariant and for simplicity I'm ignoring pooling layers

168
00:16:45,320 --> 00:16:49,800
but we can definitely add them into the discussion but I feel like the the guts of this business

169
00:16:50,760 --> 00:16:53,800
really is exposed when we ignore pooling so I'm going to do that from now on

170
00:16:55,720 --> 00:17:06,200
so so convolution is equivariant and and value is equivariant if we so we have our image we

171
00:17:06,200 --> 00:17:11,480
have a whole bunch of real numbers if we translate and then set some of those numbers to zero that's

172
00:17:11,560 --> 00:17:13,880
the same thing as setting some of those numbers to zero and then translate

173
00:17:18,440 --> 00:17:25,320
locality yeah no any activation function would be equivariant but as we'll discuss in a second

174
00:17:25,320 --> 00:17:30,280
it's really essential that these are permutation representations for an active function activation

175
00:17:30,280 --> 00:17:37,960
function to be equivariant so so we have requirement one we want a gamma invariant answer

176
00:17:38,280 --> 00:17:45,640
requirement two is that everything going on in this network should be gamma equivariant

177
00:17:46,840 --> 00:17:54,360
requirement three is locality so what this says often in you know if you look apparently at the

178
00:17:54,360 --> 00:18:00,680
early layers of the brain in the visual cortex what happens is local and so and it's very natural

179
00:18:00,680 --> 00:18:08,440
to also impose this in a cnn so we our our filters are supported around the identity initially

180
00:18:08,440 --> 00:18:13,720
and then later on we let them grow out through pooling and potentially fully connected layers

181
00:18:15,000 --> 00:18:21,640
so an actual cnn we wouldn't require we wouldn't look at periodic images and what we would do

182
00:18:21,640 --> 00:18:29,080
is pat around the edge um to make all our images the same size for example yeah and we would only

183
00:18:29,080 --> 00:18:33,960
have we the same principles would be there but we wouldn't have a kind of full symmetry that only

184
00:18:33,960 --> 00:18:39,640
makes sense to shift pictures a little bit but what I find remarkable here and it's kind of a

185
00:18:39,640 --> 00:18:45,320
simple simple thing is that gamma invariance gamma equivariance and locality basically tell me what

186
00:18:45,320 --> 00:18:50,920
I what I have to do in my neural net so if you assume that you should compose it out of simple

187
00:18:50,920 --> 00:18:57,960
functions and if you fix relu then everything else is basically specified which is remark which seems

188
00:18:57,960 --> 00:19:05,400
to me to be remarkable and what I want to explain soon is that there's kind of nothing special about

189
00:19:05,400 --> 00:19:11,800
this particular group that this would actually tell us how to make predictions on any space on

190
00:19:11,800 --> 00:19:18,280
which a group acts transitively so let me just emphasize one extremely important point from a

191
00:19:19,240 --> 00:19:28,760
implementation point of view okay so just for completeness gaug asked here whether

192
00:19:30,120 --> 00:19:36,040
relu was specific for it being equivariant and the response was no any non-linear activation

193
00:19:36,040 --> 00:19:40,360
function would be fine at this point as long as our representation is a permutation representation

194
00:19:41,400 --> 00:19:48,040
and what Stefan asked was in this particular picture here um you know for completeness

195
00:19:48,040 --> 00:19:51,640
if we translated this little cat we'd have half the cat's head over here and half the cat's head

196
00:19:51,640 --> 00:19:56,440
over here and the question was you know how does an how does an actual cnn in real life

197
00:19:58,440 --> 00:20:04,600
do this and basically you know we don't we don't enforce that full equivariance we only

198
00:20:04,600 --> 00:20:09,480
allow kind of small translations within some bounded region so kind of partial symmetry

199
00:20:10,280 --> 00:20:11,480
thank you very much for the reminder

200
00:20:14,040 --> 00:20:22,280
so this is very important from a basic implementation point of view imagine on the left hand side we

201
00:20:22,280 --> 00:20:29,720
have two we this is a layer of our neural net and so we have l1 inputs and l2 outputs

202
00:20:30,840 --> 00:20:36,040
single layer so now if you think about the number of parameters it's l1 times l2

203
00:20:36,200 --> 00:20:45,320
so if it's easy to find a picture for example with 10 million uh 10 million pixels in it

204
00:20:45,880 --> 00:20:54,440
and if we do one layer then that's whatever 10 million squared is you know i'm not a physicist

205
00:20:54,440 --> 00:20:59,720
i don't know what 10 million squared is okay so some enormous number that i can probably never

206
00:20:59,720 --> 00:21:10,680
actually train on a computer and but if we're doing so here here i have a first layer of a

207
00:21:11,400 --> 00:21:17,400
convolutional neural net in one dimensional in one dimension so here are my inputs

208
00:21:19,720 --> 00:21:26,600
and locality says that my filter only affects neighboring points so that's why i have these

209
00:21:26,600 --> 00:21:35,880
three parameters x y and z and equivariance says that these x y and z are the same across the whole

210
00:21:35,880 --> 00:21:44,920
thing so no matter how big this layer is it just depends on three size um three parameters okay

211
00:21:44,920 --> 00:21:51,800
so i should emphasize this is one piece of the first layer so up here this would be one of these

212
00:21:51,800 --> 00:21:58,520
pieces so i could expect to have potentially 20 of these pieces or something but still some you

213
00:21:58,520 --> 00:22:06,520
know 20 times 3 is a lot smaller than 10 million squared i'm enough of a physicist to know that

214
00:22:06,520 --> 00:22:18,760
inequality okay so now i want to explain a blueprint for learning on a general homogeneous

215
00:22:18,760 --> 00:22:28,120
space for a group so we have our group and i'm typically thinking about a finite group or a

216
00:22:28,120 --> 00:22:36,040
league group like so three or something like that so gamma was z mod h z squared before

217
00:22:36,920 --> 00:22:44,360
and x is a transitive gamma set so this just means that um gamma acts transitively on x but

218
00:22:44,360 --> 00:22:48,680
in the category of leap in the category of differential manifold this would mean that

219
00:22:48,680 --> 00:22:56,680
i have a manifold with a continuous action of my or a smooth action of my league and in any

220
00:22:56,680 --> 00:23:05,240
situation in which um this makes sense we have that x is just the same thing as gamma mod a

221
00:23:05,240 --> 00:23:11,880
single stabilizer and what we want to do is learn an invariant so i'll stick to the invariant case

222
00:23:11,880 --> 00:23:18,520
but notice that we might also want to make an equivariant prediction function from functions on

223
00:23:18,520 --> 00:23:25,960
x to r now very basic representation theoretic observation or maybe so basic but it's not yet

224
00:23:25,960 --> 00:23:34,360
representation theory is that because our action is transitive there is only one linear

225
00:23:34,360 --> 00:23:44,360
uh or at most one linear map from functions on x to r that is invariant namely

226
00:23:45,800 --> 00:23:54,280
like summing over my finite set or integrating or so i found this kind of illustrative because

227
00:23:54,280 --> 00:23:59,080
this tells you for example in the image classification task you're definitely looking for a nonlinear

228
00:23:59,080 --> 00:24:05,800
function because a linear function would be like averaging over pixel values and this is the kind

229
00:24:05,800 --> 00:24:15,240
of silliest thing that one could imagine so we're looking for some um invariant function

230
00:24:17,000 --> 00:24:27,720
and here's the blueprint so we fix a transitive gamma set this is where we want to make the

231
00:24:27,800 --> 00:24:32,600
prediction and then we just our architecture consists of a whole lot of choices of transitive

232
00:24:32,600 --> 00:24:39,720
gamma sets and basically i think one way to think about these transitive gamma sets is

233
00:24:39,720 --> 00:24:46,600
so the the invariant prediction says that you want to take um so any gamma has an important

234
00:24:46,600 --> 00:24:52,840
transitive gamma set namely one point and this is where we want our prediction to end up and then

235
00:24:52,840 --> 00:24:59,720
if you look at um classical c and n's you want your sets to kind of slowly get smaller in some

236
00:24:59,720 --> 00:25:03,320
sense until you reach the prediction so you can think about these transitive gamma sets

237
00:25:03,320 --> 00:25:14,280
as slowly decreasing in size if that makes sense and this is all we do so we um consider some

238
00:25:14,360 --> 00:25:21,000
equivariant maps so convolution so this should be a gamma a gamma equivariant

239
00:25:24,520 --> 00:25:27,880
linear map

240
00:25:30,280 --> 00:25:33,080
and then we never do a relu and then we do another one and then we do a relu and then

241
00:25:33,080 --> 00:25:39,880
we do another one and we want to train across the parameters of gamma equivariant linear maps

242
00:25:40,600 --> 00:25:46,120
yeah and probably this is a point of it so this whole space is r

243
00:25:50,200 --> 00:25:54,280
so if gamma has a metric or similar we might want convolution supported near the identity

244
00:25:54,920 --> 00:25:58,680
now this is the most important point and i think that this has kind of been missed in the

245
00:25:59,400 --> 00:26:04,680
in the machine learning literature there's something very basic in representation theory

246
00:26:04,680 --> 00:26:09,160
which i call the double coset formula people might call it hecar algebras there's many

247
00:26:09,160 --> 00:26:20,760
different names for it so we're asking what is such a map so because any transitive set is

248
00:26:20,760 --> 00:26:26,360
simply gamma mod h or gamma mod h prime we want to know what this home space is

249
00:26:27,960 --> 00:26:34,840
and the formula says that homomorphisms from such a function space to such a function space

250
00:26:34,840 --> 00:26:37,000
are simply functions on double cosets

251
00:26:40,040 --> 00:26:44,840
now there's many different ways to understand this formula if you if you're in the world of

252
00:26:44,840 --> 00:26:49,560
finite groups this is a very nice exercise if you're in the world of compact league groups it's a

253
00:26:50,280 --> 00:26:57,160
significantly more difficult exercise but i just want you to accept this formula as a kind of

254
00:26:58,280 --> 00:27:04,360
beautiful thing in the world and we'll see it's very useful okay and if you want to know more

255
00:27:04,360 --> 00:27:11,080
about it i'm very happy to talk more about this formula okay but this is a kind of i don't know

256
00:27:11,080 --> 00:27:17,480
very useful formula in many different situations so this is telling us what the possible space of

257
00:27:17,480 --> 00:27:29,480
convolutions is so here's an example imagine that we're learning on a sphere so we have a nice sphere

258
00:27:29,480 --> 00:27:38,920
here and we have so three so this is um orthogonal three by three matrices of determinant one so

259
00:27:38,920 --> 00:27:47,400
these are these transformations so it acts on the sphere and s2 is a transitive space i can move

260
00:27:47,400 --> 00:27:51,480
any point on the sphere to another point on the sphere of iron orthogonal transformation

261
00:27:51,480 --> 00:27:59,080
what is the stabilizer of single point it's those rotations in the axis that point determines through

262
00:27:59,080 --> 00:28:09,800
the origin so s2 is s03 mod s1 now imagine that we want to learn on the sphere so we want to have

263
00:28:09,800 --> 00:28:15,400
some image on the sphere some function on the sphere and we want to say it's a cat or something

264
00:28:16,120 --> 00:28:21,960
you might ask i don't generally see pictures of cats on spheres this is my answer to that

265
00:28:22,840 --> 00:28:27,720
okay this is a beautiful article in quanta so this is the cosmic background radiation

266
00:28:28,680 --> 00:28:36,200
no absolutely extraordinary thing from around about 2003 where we see the early what the universe

267
00:28:36,200 --> 00:28:40,600
looked at early on and we do this by basically going around the world and looking out into space

268
00:28:40,600 --> 00:28:43,080
and so it's an archetypal example of an image on a sphere

269
00:28:47,800 --> 00:28:54,360
so sam has a question um we'll just i just want to admire this picture for 10 more seconds

270
00:28:54,440 --> 00:29:02,440
so i'm told that you can see the fluctuations of quantum field theory in this picture so

271
00:29:02,440 --> 00:29:06,760
this is a very early universe so it's when the universe was very small and you expect

272
00:29:08,200 --> 00:29:12,520
the behavior to be given by the laws of the very small and i'm told that you can see

273
00:29:13,080 --> 00:29:18,520
evidence of quantum field theory in this picture that totally blows my mind okay so sam's question

274
00:29:18,520 --> 00:29:23,160
for discrete finitely generated gamma could support near the identity be regarded as having

275
00:29:23,160 --> 00:29:28,680
sort some sort of choice of generator that's a really good question i was thinking about

276
00:29:28,680 --> 00:29:36,120
what the support nearly up near the identity kind of means so in the example of the cnn

277
00:29:37,240 --> 00:29:45,480
we have this discrete group which has no no really convincing metric on it but it is embedded in

278
00:29:46,040 --> 00:29:51,800
s1 times s1 that does have a good metric on it and so for groups that come with some embedding

279
00:29:53,160 --> 00:29:57,160
we can put a metric on them but i also think that that's a good suggestion if you have a

280
00:29:58,520 --> 00:30:02,440
some kind of uh what's what's that distance you're talking about it's um like kind of

281
00:30:02,440 --> 00:30:07,560
distance in the kaillie graph might also be a a decent measure of locality i also want to try

282
00:30:07,560 --> 00:30:14,280
to explain in a second that for a non-Abelian group locality is less important so so the building

283
00:30:14,280 --> 00:30:20,520
blocks so what are the homogeneous spaces for so three so this is in a league group so i can

284
00:30:20,520 --> 00:30:25,160
ask what are the dimensions of the subgroups of s03 so there's a whole lot of interesting finite

285
00:30:25,160 --> 00:30:30,040
subgroups of s03 for example the symmetries of the icosahedron form a very interesting

286
00:30:30,680 --> 00:30:40,920
subgroup of s03 and um and then you have the two sphere and rp2 and then you have a point

287
00:30:40,920 --> 00:30:46,760
and that's it okay so our building blocks are rather restricted which is interesting

288
00:30:46,760 --> 00:30:53,960
and also i would say that we if you're employing some kind of practicality in building your model

289
00:30:54,520 --> 00:31:00,200
you don't want complicated things like s03 modified subgroup so

290
00:31:02,200 --> 00:31:07,720
and rp2 and s2 are very very similar you know one is just a two-fold cover of the other

291
00:31:08,920 --> 00:31:14,520
and so i would advocate building a building a network which just involves functions on

292
00:31:14,600 --> 00:31:21,160
spheres and functions on a point so this is the proposal for a blueprint for learning on the sphere

293
00:31:21,160 --> 00:31:25,720
and also i am aware that it's very difficult for a computer to understand a function on s2

294
00:31:25,720 --> 00:31:30,600
okay but this is meant to be some kind of blueprint that you then try to interpret

295
00:31:31,880 --> 00:31:36,120
and sometimes you know to have the idea of what you're doing very clearly in your head is very

296
00:31:36,120 --> 00:31:41,560
useful when you come to implementing something here we have h equals h prime exactly so i'll

297
00:31:41,560 --> 00:31:47,800
go through the double coset formula in two examples now so the point the the problem here

298
00:31:47,800 --> 00:31:53,640
which geyog is pointing out so geyog was asking which which subgroup are we using the double

299
00:31:53,640 --> 00:31:57,960
coset formula in here i just want to first say why we're using the double coset formula

300
00:31:57,960 --> 00:32:04,600
we want to know what are these maps what are our possible so three every variant convolutions here

301
00:32:04,600 --> 00:32:11,960
so what are the so three every rank convolutions so this is the double coset formula again this is

302
00:32:11,960 --> 00:32:21,960
our friend okay i'm just specializing the double coset formula for so three so that made the formula

303
00:32:21,960 --> 00:32:26,440
much less easy to read so i'll delete it again okay so what does this say let's first do a silly

304
00:32:26,440 --> 00:32:35,240
example what are the homomorphisms from s03 mod s1 i.e s2 to s03 mod s03 namely a point

305
00:32:36,120 --> 00:32:40,840
so i said as an exercise in very great generality the only such function is given essentially by

306
00:32:40,840 --> 00:32:45,160
integrating over your space up to a scalar but let's see it pop out of the double coset formula

307
00:32:45,160 --> 00:32:51,160
so s03 mod s03 is a point yeah so we've got functions on a point what's more interesting

308
00:32:51,160 --> 00:32:57,000
so this is the kind of silly example what's more interesting is what are the rest of the layers

309
00:32:57,720 --> 00:33:02,040
you know so just to emphasize here this is telling us that even though this is an enormous vector

310
00:33:02,040 --> 00:33:10,680
space with this only one scalar possibility of of maps here so this belongs to r this belongs to r

311
00:33:11,400 --> 00:33:12,200
this belongs to r

312
00:33:13,160 --> 00:33:23,480
so now ignore the integral bit at the moment just look at this so what are the s03

313
00:33:23,480 --> 00:33:30,040
equivariant homomorphisms from functions on s2 to functions on s2 they're functions on by our double

314
00:33:30,040 --> 00:33:38,520
coset formula s1 mod s03 mod s1 so that's the same thing as s s1 mod s2 so i take s2 and i

315
00:33:38,680 --> 00:33:45,160
have s1 rotating it around and then i quotient that out and our representatives for that quotient

316
00:33:45,160 --> 00:33:51,720
space are just an interval stretching from one pole to the other so functions on that interval

317
00:33:53,560 --> 00:34:00,760
so what the hell are these intertwiners so i should say that such an element inside here

318
00:34:01,400 --> 00:34:02,600
is called an intertwiner

319
00:34:02,920 --> 00:34:13,880
so intertwiner is synonymous with s03 equivariant linear map

320
00:34:16,360 --> 00:34:22,600
so i can look at one way to understand these things is to try to look at delta function so a delta

321
00:34:22,600 --> 00:34:30,200
function at the identity at this base point is just the identity a delta function at this end

322
00:34:30,200 --> 00:34:38,040
is the antipode but what the hell is going on in the middle you know you're you're seeking a

323
00:34:38,040 --> 00:34:44,760
continuous family of operators which interpolates between the identity and the antipode okay looks

324
00:34:44,760 --> 00:34:50,520
like a tough ask but there's a really beautiful thing you can do which is you consider the

325
00:34:50,520 --> 00:34:57,400
following operator on function so i have a function on the on the two sphere so this is in

326
00:34:58,360 --> 00:35:08,040
on s2 and now and i have a gamma and i can consider a new function which at a point x is

327
00:35:08,040 --> 00:35:14,280
given by the integral around a loop of my original function at distance gamma

328
00:35:20,200 --> 00:35:26,280
from my point there's a way of producing a new function so i've told you how to take a function

329
00:35:26,600 --> 00:35:33,080
s2 get a new function on s2 and it's a beautiful thought exercise that this is invariant this

330
00:35:33,080 --> 00:35:38,920
this is equivariant okay so if i move my function and then do this operation that's the same thing

331
00:35:38,920 --> 00:35:43,480
is doing this operation and then moving my function okay so these are these

332
00:35:45,960 --> 00:35:47,400
intertwiners so

333
00:35:50,040 --> 00:35:55,800
so that's the answer for what all these maps are and of course like that's still a infinite

334
00:35:55,800 --> 00:36:05,080
dimensional vector space but compared to you know functions on like if you're just thinking

335
00:36:05,080 --> 00:36:10,840
about linear maps here that's something like functions on s2 times s2 so it's like a four

336
00:36:10,840 --> 00:36:17,800
dimensional roughly speaking and it's kind of remarkable that just in employing this

337
00:36:17,800 --> 00:36:23,720
equivariance massively cuts down the number of parameters and of course you can make this whole

338
00:36:23,720 --> 00:36:29,560
picture even richer using spherical harmonics and there's like incredibly nice functions to put in

339
00:36:29,560 --> 00:36:37,560
here projecting to the irreducible representations inside functions on s2 etc i should have said

340
00:36:37,560 --> 00:36:42,760
very very much earlier like for me fun is just firstly it's to remind us that we're having fun

341
00:36:43,800 --> 00:36:48,920
secondly um it's just some kind of class of function so when i'm talking about the sphere

342
00:36:48,920 --> 00:36:52,920
i'm probably talking about l2 functions when i'm talking about a um discrete set i'm just talking

343
00:36:52,920 --> 00:37:00,040
about any function etc so yeah the point is that when we have a non-Abelian gamma there's a big

344
00:37:00,040 --> 00:37:08,840
reduction in parameters so um in the cnn slide there was this equivariance plus locality drastic

345
00:37:08,840 --> 00:37:14,120
reduces the number of parameters and here i'm kind of saying that equivariance plus non-Abelian

346
00:37:14,120 --> 00:37:20,280
drastically reduces the number of parameters which i think is very interesting so my task for myself

347
00:37:20,280 --> 00:37:25,240
and if you have any ideas i'd love to hear it is find an interesting learning problem

348
00:37:25,240 --> 00:37:31,160
where the symmetries are an interesting non-Abelian group what's a learning problem where the

349
00:37:31,160 --> 00:37:37,960
symmetries are naturally sl2 fq or something like that or you know some interesting groups so a lot

350
00:37:37,960 --> 00:37:43,960
of the groups that show up in machine learning are very much related to um three-dimensional space

351
00:37:43,960 --> 00:37:52,120
or two-dimensional space or so like p4 which consists of um all translations and 90-degree

352
00:37:52,120 --> 00:37:56,440
rotation shows up a lot and stuff like that but it would be lovely to inject some really

353
00:37:56,440 --> 00:38:00,200
interesting groups into this oh gl2 yeah gl2 would be great

354
00:38:05,320 --> 00:38:10,040
yeah that's it so stefan just suggested learning on hyperbolic space and that's a great

355
00:38:10,120 --> 00:38:17,480
great suggestion yeah i don't know why i didn't think of that i had fl2 uh sorry yeah so learning on

356
00:38:24,120 --> 00:38:28,360
okay there's enormous um possibilities here that i think are very interesting

357
00:38:29,320 --> 00:38:34,360
so let let's just go back to cnn's so the the question is basically like what the hell's going on

358
00:38:34,920 --> 00:38:44,520
so let's imagine so i'll try to explain um what the hell's going on and then we can have a short break

359
00:38:45,160 --> 00:38:53,480
so let's say that we're trying to do image processing so we're z mod hz squared we have functions on this

360
00:38:53,640 --> 00:39:00,920
and then we we want to have a layer of our neural net

361
00:39:02,920 --> 00:39:04,280
so typically

362
00:39:07,240 --> 00:39:12,200
one layer of our neural net might be like this you know one piece of one layer of our

363
00:39:14,840 --> 00:39:20,040
neural net might look like this so now what's the dimension of this space

364
00:39:21,000 --> 00:39:26,840
the dimension is h squared namely the number of

365
00:39:28,840 --> 00:39:36,200
points in the set and what's the dimension of this space the dimension whoops the dimension is h square

366
00:39:38,680 --> 00:39:43,800
okay so now if i were doing a fully connected neural net

367
00:39:44,760 --> 00:39:52,760
i would have h squared basis vectors here h squared basis vectors here and then i would have an h squared times h squared matrix

368
00:39:53,800 --> 00:40:00,760
so i'd have an enormous matrix of size h to the four that's and each of those parameters i have to train

369
00:40:01,960 --> 00:40:08,600
okay what do i do in cnn's i say i want this matrix to be invariant

370
00:40:08,760 --> 00:40:15,960
that already cuts down the number of parameters from h to the four back down to h squared

371
00:40:17,240 --> 00:40:21,000
and then i say i want this matrix to satisfy locality

372
00:40:22,120 --> 00:40:25,560
and that cuts down my number of parameters from h squared to nine

373
00:40:26,680 --> 00:40:32,520
so harini is asking uh what basically like why do you restrict the parameters to this extent

374
00:40:33,400 --> 00:40:35,480
so i would say that there's two reasons for this

375
00:40:36,440 --> 00:40:42,920
so these are inductive priors so they're not there's something that i believe is true about the solution

376
00:40:43,480 --> 00:40:48,920
they're not something that like is definitely true about the solution there's some category of practicality

377
00:40:48,920 --> 00:40:53,880
i want to build a model that works i can't train a hundred billion parameters but i can train

378
00:40:54,680 --> 00:40:59,880
you know a hundred or a thousand fine yeah and the inductive priors in this are invariants

379
00:41:00,600 --> 00:41:07,320
namely i can see you i can still see you yeah that's you know like you're still there like that's

380
00:41:07,320 --> 00:41:14,600
invariance yeah um and the other thing is locality and i think locality makes a lot of sense like when

381
00:41:14,600 --> 00:41:20,360
i look at this room i don't think the first thing that happens in my brain is i think oh i'm in

382
00:41:20,360 --> 00:41:30,440
kaslo 273 what happens first is i go oh edge corner chair person stairs light looks like a lecture hall

383
00:41:31,320 --> 00:41:41,400
sydney probably kaslo 273 yeah and so that's invariant that's um locality and these are our

384
00:41:41,400 --> 00:41:46,600
inductive priors and these inductive priors massively cut down the parameters and then from

385
00:41:46,600 --> 00:41:51,800
then we're cooking on gas and we can get these models that actually work no but it's again like

386
00:41:51,800 --> 00:41:59,240
changing the group is a gain inductive priors so um for example like you know have you been upside

387
00:41:59,240 --> 00:42:06,520
down and you look and it's actually much harder to recognize stuff so the idea that we satisfy this

388
00:42:06,520 --> 00:42:11,720
invariance is much less well established than the idea that we satisfy this invariance and then we

389
00:42:11,720 --> 00:42:19,560
want to bake in that symmetry exactly yeah i don't know if you really want rotations so like a classic

390
00:42:19,560 --> 00:42:25,080
pre-training task in image recognition is to recognize whether your image is upside down or not

391
00:42:26,440 --> 00:42:31,560
so you know that's a classically non-invariant thing under rotation by 180

392
00:42:34,200 --> 00:42:38,040
but there's there's situations like you know in an MRI scan or something

393
00:42:38,040 --> 00:42:42,760
where it really you know you really want that invariance that's when you should bake it into the model

394
00:42:46,760 --> 00:42:52,280
if you go back to our friend l2 of s2 there's a great exercise so the laplacian

395
00:42:52,280 --> 00:42:57,160
okay so the casimir gives you the laplacian on the sphere and the eigenspaces for the laplacian

396
00:42:57,160 --> 00:43:02,360
are the spherical harmonics so the the casimir is acting everywhere in this whole big diagram

397
00:43:02,360 --> 00:43:07,240
commuting with everything and splitting it up into irreducible it's not so much locality it's the

398
00:43:07,240 --> 00:43:19,640
fact that so three is maybe i can write so you know l2 of s2 is a topological direct sum of l gamma

399
00:43:20,680 --> 00:43:22,920
where gamma is

400
00:43:28,680 --> 00:43:29,880
is a spherical harmonics

401
00:43:37,720 --> 00:43:43,240
and the the casimir is providing this decomposition so it's breaking

402
00:43:44,520 --> 00:43:48,120
breaking up this space so the casimir on each one of these acts by a different scalar

403
00:43:49,240 --> 00:43:55,880
and it's the the casimir aka the laplacian acts on each of these you know this is something like

404
00:43:56,680 --> 00:43:59,640
restriction of of degree

405
00:44:01,880 --> 00:44:04,920
gamma or gamma over two polynomial polynomials

406
00:44:07,800 --> 00:44:15,160
and so you have this totally canonical decomposition of this space of functions

407
00:44:15,880 --> 00:44:24,520
and everything wherever i had this picture everything here is respecting this decomposition

408
00:44:24,520 --> 00:44:26,680
all the linear maps respecting this decomposition

409
00:44:30,040 --> 00:44:35,400
and roughly speaking you can kind of think about like you know if you take your function here

410
00:44:36,040 --> 00:44:40,600
and do a Fourier expansion of it then you get a whole lot of quantities

411
00:44:41,720 --> 00:44:44,520
and those quantities are giving you the

412
00:44:48,600 --> 00:44:52,440
you get a whole lot of scalars and those scalars are basically telling you how it acts on each one of these

413
00:44:58,680 --> 00:45:04,040
okay so this is just super interesting for me but maybe more technical

414
00:45:04,920 --> 00:45:11,400
uh so i want to go in the second half i want to go over why permutation representations i want to go over

415
00:45:11,400 --> 00:45:17,880
something called deep sets and then i want to go over graph neural nets

416
00:45:19,240 --> 00:45:25,080
so one question that is very natural to ask as a representation theorist

417
00:45:25,400 --> 00:45:35,560
is why so if you take functions on a set on a gamma set this is called a permutation representation

418
00:45:36,360 --> 00:45:40,360
and it's called a permutation representation because if you look at the matrices that represent

419
00:45:40,360 --> 00:45:48,200
your elements they're permutation matrices and in our first class in representation theory the

420
00:45:48,200 --> 00:45:52,840
first representations we see a permutation but then we quickly convince our students that

421
00:45:53,800 --> 00:45:58,120
we should break up permutation representations into irreducible representations and that's

422
00:45:58,120 --> 00:46:06,440
really interesting so why am i insisting that we have permutation representations everywhere

423
00:46:08,760 --> 00:46:13,640
so i i found that this charming little lemma which i i didn't find in the literature

424
00:46:14,360 --> 00:46:19,880
which is if you have a representation of gamma which is assumed finite i'm not quite sure what

425
00:46:19,880 --> 00:46:26,760
the analog of this for a league group is then we so once we have v we can choose a basis for it

426
00:46:27,640 --> 00:46:34,600
and once we have a basis we can ask is relu gamma equivalent yep so remember relu takes our

427
00:46:35,480 --> 00:46:39,240
vector which now that we have a basis is just a sequence of real numbers and the ones that are

428
00:46:39,240 --> 00:46:43,400
negative it sets to zero and the ones that are positive it keeps so is this gamma equivalent

429
00:46:44,120 --> 00:46:49,400
if our representation is just permuting our coordinates around it's easier to see that

430
00:46:49,400 --> 00:46:57,880
it's gamma equivalent but uh that's if and only if so in order to have a gamma equivalent

431
00:46:57,880 --> 00:47:01,480
relu with respect to some basis you have to be permutation with respect to that basis

432
00:47:04,760 --> 00:47:10,120
now this is something that has been exciting me a lot over the last few days and is having

433
00:47:10,120 --> 00:47:15,400
a hell of a lot of fun with is basically like piecewise linear representation theory

434
00:47:16,200 --> 00:47:22,200
um so what you could imagine is you you have these layers and they all break up into irreducible

435
00:47:22,200 --> 00:47:29,720
representations and if you include an irreducible into a permutation representation do a relu and

436
00:47:29,720 --> 00:47:36,520
then project back you get a piecewise linear endomorphism equivariant endomorphism of your

437
00:47:36,520 --> 00:47:40,760
representation and so what you could imagine is networks in which you have irreducible

438
00:47:40,760 --> 00:47:50,280
representations together with equivariant non-linearities and my impression is that this

439
00:47:50,280 --> 00:47:55,800
is extremely interesting and I've already learned like basic things about representations that I

440
00:47:55,800 --> 00:48:01,080
didn't know from thinking about this so if you're interested in this ask me but it's kind of much

441
00:48:01,080 --> 00:48:07,560
more specialized so I'm not going to talk about it today so let's I want to do another example

442
00:48:07,560 --> 00:48:14,040
of our blueprint so one way of seeing this is just first think about this

443
00:48:15,720 --> 00:48:21,320
so imagine that we have a whole lot of points and they're unordered and they have a whole

444
00:48:21,320 --> 00:48:27,320
lot of information attached to them for example you could imagine all of the citizens of Sydney

445
00:48:27,320 --> 00:48:33,080
and they're labeled by their age and how much tax they paid last year yeah so I've got a two

446
00:48:33,080 --> 00:48:39,560
dimensional vector associated to every person in Sydney now one way of viewing this data set is

447
00:48:39,560 --> 00:48:46,360
as a point cloud so what I have is an enormous in this you know age and tax example I just have R2

448
00:48:46,360 --> 00:48:52,040
and I have an enormous number of points in R2 and I want to make some qualitative statement so a basic

449
00:48:52,040 --> 00:48:56,440
statement that I could make is some kind of like center of mass statement like the average age of

450
00:48:56,440 --> 00:49:01,320
people in Sydney is blah that would be a kind of boring measurement but a much more interesting

451
00:49:01,320 --> 00:49:07,320
measurement would be there's this kind of weird hole in this data for example you know a particular

452
00:49:07,320 --> 00:49:12,680
age in a particular tax is you know not paid in some region or something like that that would be

453
00:49:12,680 --> 00:49:16,840
a much more interesting statement you can make about this data analysis and there's about this

454
00:49:16,840 --> 00:49:23,160
data and this is one of the one of the this is related to this very interesting subject called

455
00:49:23,160 --> 00:49:31,800
persistent homology which I know next to nothing about uh okay but we have a point cloud so we

456
00:49:31,800 --> 00:49:37,160
want to make a prediction based on this point cloud and so this is an equivariant prediction task

457
00:49:37,880 --> 00:49:46,920
we have so uh so here we have rd to the n so here are our n different points

458
00:49:47,240 --> 00:49:54,520
uh and we want to learn some function you know like is there a big hole in this data or something

459
00:49:54,520 --> 00:50:03,320
like that um and it's it's convenient to swap the indices so if you think about how s n acts here

460
00:50:04,280 --> 00:50:11,480
it acts like I have a whole packet of numbers um and then it permutes them around but it's more

461
00:50:12,280 --> 00:50:18,120
useful to view this from the from the point of equivariance as one packet of numbers like age

462
00:50:18,120 --> 00:50:22,520
that's being permitted around and one packet of numbers like how much tax was paid being

463
00:50:22,520 --> 00:50:29,480
commuted around and so I'll do this innocent rewriting um but I'm just pointing this out so

464
00:50:29,480 --> 00:50:35,160
it doesn't confuse the hell out of you on the next slide uh so we want to make an s n invariant

465
00:50:35,160 --> 00:50:39,640
prediction so basically we have in the language of representation theory we have a whole lot of

466
00:50:39,640 --> 00:50:45,800
copies of the most basic permutation representation of s n namely s n acts acting on functions on

467
00:50:45,800 --> 00:50:53,000
an n set and we want to make an s n invariant prediction so let's do some basic representation

468
00:50:53,000 --> 00:50:59,960
theory which I almost certainly learned from andrew at some point in about 30 or something

469
00:51:00,680 --> 00:51:06,520
so we take functions from one up to n functions on the set one up to n so this is a permutation

470
00:51:06,520 --> 00:51:12,200
representation of s n and we take the trivial representation so this is where every permutation

471
00:51:12,200 --> 00:51:20,440
just acts by the identity so here's a whole lot of homomorphism formulas so this is before I was

472
00:51:20,440 --> 00:51:25,880
saying what are the arrows in my neural net here I'm working them out explicitly what's what parameters

473
00:51:25,880 --> 00:51:29,640
there are so home from the trivial representation of the trivial representation this is the same

474
00:51:29,640 --> 00:51:36,360
thing as home from r to r this is r times the identity home from n to one this is another

475
00:51:36,360 --> 00:51:42,040
instance of this statement that on a permutation representation the only invariant measurement

476
00:51:42,040 --> 00:51:48,520
you can make is essentially summing up your entries that's that home back the other way

477
00:51:49,400 --> 00:51:54,920
here if we look at the image of one we want some vector we want some function which is

478
00:51:54,920 --> 00:52:00,200
invariant under s n i.e. we want this function to take the same value everywhere which is alpha

479
00:52:00,840 --> 00:52:08,120
now a little bit trickier a little bit like you know this would be a second week exercise

480
00:52:08,120 --> 00:52:12,200
in representations of the symmetric group or something is that home from the trivial from

481
00:52:12,200 --> 00:52:17,960
this permutation representation to itself is two-dimensional and it's spanned by the identity

482
00:52:17,960 --> 00:52:23,560
and the map that sums up the coordinates and and takes that sum and multiplies it by

483
00:52:24,520 --> 00:52:34,760
the constant function now exercise deduce this from the double coset formula all of these formulas

484
00:52:34,760 --> 00:52:38,840
are very easy concept consequences of the double coset formula why is that the case

485
00:52:40,760 --> 00:52:46,040
do it if you're a student you should do this and if you're a student it's not already obvious

486
00:52:46,040 --> 00:52:51,960
to you you should do this so i this is deep set architecture so you can look at so here's

487
00:52:52,760 --> 00:52:59,160
here's our paper from 2017 and to me as a non-machine learning person it looks a bit

488
00:52:59,160 --> 00:53:08,120
mystifying but this is just another instance of our blueprint so here's our input so this would be

489
00:53:08,200 --> 00:53:15,640
our three three-dimensional point cloud input yep so we have um three parameters per point

490
00:53:18,280 --> 00:53:26,680
now we do s in equilibrium maps and we uh sprinkle around ends and ones these are our building blocks

491
00:53:28,440 --> 00:53:37,880
now note how crazily this reduces the number of parameters for a large n so here if we had

492
00:53:37,880 --> 00:53:44,040
no assumption of s in equilibrium this would be an n squared space of parameters you know and n

493
00:53:44,040 --> 00:53:50,440
could easily be a million or something but now because we want this to be s in equilibrium we've

494
00:53:50,440 --> 00:53:57,160
just got two two possibilities here we've got one possibility here we've got one possibility here

495
00:53:57,160 --> 00:54:05,240
we've got one possibility etc so this allows us to make enormous um networks involving you know

496
00:54:06,200 --> 00:54:11,720
hundreds of billion you know billion dimensional things um with few parameters

497
00:54:13,720 --> 00:54:21,720
okay and that is um deep set architecture and it's i think it's um state of the art in terms of

498
00:54:22,920 --> 00:54:29,960
point cloud prediction okay graph neural nets if there's no questions

499
00:54:30,040 --> 00:54:38,120
uh graphs are everywhere in mathematics they come in many forms and variants so

500
00:54:40,760 --> 00:54:45,560
i just want you to keep in mind that graph here is a very um loose term it might be a

501
00:54:45,560 --> 00:54:50,760
directed graph it might be a digraph a directed graph the edges might be colored the vertices

502
00:54:50,760 --> 00:54:55,720
might be colored the edges might be weighted the vertices might be weighted the vertices might have

503
00:54:55,720 --> 00:55:00,600
10 parameters associated to them we might be talking about hyper graphs so that's graphs where we

504
00:55:00,600 --> 00:55:07,320
have like an edge need can connect more than what more than two two vertices etc so it's

505
00:55:07,320 --> 00:55:13,880
whole plethora of things called graphs and just want to emphasize that there's many ways so

506
00:55:13,880 --> 00:55:17,880
graphs are everywhere in mathematics but once you start thinking about them they're even more

507
00:55:17,880 --> 00:55:22,280
everywhere because there's a whole lot of stuff that wasn't obviously a graph initially and then

508
00:55:22,280 --> 00:55:27,560
you can make it a graph so examples of this uh you might say well you know graph theory is one

509
00:55:27,560 --> 00:55:31,720
dimensional topology and i'm a sophisticated eight dimensional topologist and i only care about

510
00:55:31,720 --> 00:55:36,360
eight dimensional manifolds yeah but if you take a compact eight dimensional manifold you can choose

511
00:55:36,360 --> 00:55:41,320
a point cloud on it and you'll get a graph and that graph tells you enormous amounts about that

512
00:55:41,320 --> 00:55:47,000
eight manifold if you have a simplicial complex you know for me like graphs are just one dimensional

513
00:55:47,000 --> 00:55:52,440
simplicial complexes and so i said to petah oh we should be sophisticated and learn on

514
00:55:52,440 --> 00:55:58,200
simplicial complexes and he said well a simplicial complex is just a graph truly you know here's

515
00:55:58,200 --> 00:56:04,200
my triangle here are my edges and here are my vertices it's a colored a simplicial complex is

516
00:56:04,200 --> 00:56:11,640
a colored vertex colored graph okay of a special form so this is another example this is from

517
00:56:11,640 --> 00:56:18,440
gaorg so if we have a a data set and it's somehow embedded in a space then we can get a natural

518
00:56:18,440 --> 00:56:25,720
metric graph out of it by looking at distances between vertices we might include the coordinates here

519
00:56:25,720 --> 00:56:33,000
we might do some funny function applied to this these lengths etc okay so graphs are everywhere

520
00:56:33,800 --> 00:56:41,080
and graph neural nets seem to be an incredibly powerful flexible way of dealing with um data so

521
00:56:42,040 --> 00:56:46,680
i think the graph neural nets have kind of really genuine like c and n's the thing that's that we

522
00:56:46,680 --> 00:56:51,640
stare at as mathematicians and think how could we make something like this that would help us in

523
00:56:51,640 --> 00:56:55,880
mathematics but i think that graphs are actually a thing that will help us in mathematics all the

524
00:56:55,880 --> 00:57:05,000
time so that's very worthwhile thinking about so what the graph neural nets do an example we might

525
00:57:05,720 --> 00:57:11,000
want to learn a function on graphs so an example would be a function which is learn planarity

526
00:57:11,960 --> 00:57:17,320
okay so this output's a positive number if it's planar a negative number if it's not planar so

527
00:57:17,320 --> 00:57:22,040
that would be a prediction task on graphs um we also might want to know for example the

528
00:57:22,040 --> 00:57:28,840
Euler characteristic of the graph that would be another example of a prediction task another

529
00:57:28,840 --> 00:57:34,920
important example kind of more like image like generalized image recognition is producing

530
00:57:35,880 --> 00:57:42,280
um some learning some function from functions on graphs to r so you might think that so

531
00:57:43,400 --> 00:57:49,880
no you you might repackage c and n's as being a grid and then an image is the same thing as a

532
00:57:49,880 --> 00:57:57,560
function on the vertices of this grid another very important thing is that um like there's many

533
00:57:57,560 --> 00:58:02,440
many incredibly interesting for example embedding problems of graphs so you give me a graph and I

534
00:58:02,440 --> 00:58:08,440
want to put it in some space in an interesting way um and one way of doing that would be to provide

535
00:58:08,440 --> 00:58:13,800
coordinates of where I want to put the vertices of that graph and so that would be an example of

536
00:58:13,800 --> 00:58:23,000
learning a function from graphs to functions on the vertices of a graph uh so I guess the takeaway

537
00:58:23,000 --> 00:58:28,840
from this is that anything to do with graphs graph neural nets uh useful for as long as it's not

538
00:58:28,840 --> 00:58:33,160
like an NP hard problem on graphs of which there are plenty yeah so graph neural nets aren't going

539
00:58:33,160 --> 00:58:36,440
to help you solve something like is there a Hamiltonian circuit or something like that

540
00:58:38,200 --> 00:58:43,320
so what's the basic idea so imagine that I give you a graph and you want to learn on it

541
00:58:45,000 --> 00:58:49,560
it's enormously difficult as far as I can work out to work out the automorphism of a group of a

542
00:58:49,560 --> 00:58:54,600
graph so this is something that people spend many many years thinking about from an algorithmic

543
00:58:54,600 --> 00:59:00,120
point of view and so I might not know what global symmetries are present so what I was talking about

544
00:59:00,120 --> 00:59:09,080
before does not apply um or that just like most graphs have no symmetry whatsoever um or you might

545
00:59:13,320 --> 00:59:17,640
so Gaston is asking what do you mean by hard um so

546
00:59:18,200 --> 00:59:28,200
that I mean yeah maybe maybe like NP or something like that but I just want I like in my mind there's

547
00:59:28,200 --> 00:59:35,560
there's stuff on graphs which is useful and maybe not so crazily difficult like um like

548
00:59:35,560 --> 00:59:40,360
embedding your graph in a nice way or something like that and then there's a whole lot of like

549
00:59:40,360 --> 00:59:45,560
seemingly innocent problems on graphs that are extremely hard um like embedding a graph

550
00:59:45,560 --> 00:59:50,120
improvably the nice the best way or something like that or finding a Hamiltonian circuit or

551
00:59:50,120 --> 00:59:56,600
stuff like that okay so in graph land it's easy to wander into an intractable problem

552
00:59:58,200 --> 01:00:04,920
um but there's also a whole lot of useful stuff that can be done so so there's plenty of local

553
01:00:04,920 --> 01:00:12,120
symmetry in graphs so around every vertex we have a symmetric group of symmetry and also we have a

554
01:00:12,120 --> 01:00:18,600
metric so you can imagine processes which are symmetric and kind of diffuse on the graph and

555
01:00:18,600 --> 01:00:21,880
that's what a graph neural net is so I'll quickly go over the architecture

556
01:00:24,840 --> 01:00:30,040
so this is an important slide so here's my graph

557
01:00:30,840 --> 01:00:41,080
and as part of my architecture I fix n1

558
01:00:42,360 --> 01:00:49,880
n2 n3 and of course I'm just telling you one possible variant of like a thousand different

559
01:00:49,880 --> 01:00:55,880
possibilities of building graph neural net but once you've seen one of them then the other ones

560
01:00:55,960 --> 01:01:03,160
make a lot more sense so we fix these n1 and then what we do is each of our layers is a sum

561
01:01:03,160 --> 01:01:10,520
over the vertices of that particular rn1 okay so you know in a vanilla neural net we just fix

562
01:01:10,520 --> 01:01:19,480
dimensions here we fix dimensions at every vertex so that's this and so in this particular case my

563
01:01:19,800 --> 01:01:23,640
my neural net looks like this so I have

564
01:01:27,240 --> 01:01:31,960
three layers so here I have some linear map here I have a relu here I have a linear map a relu

565
01:01:33,880 --> 01:01:42,600
and then a fully connected layer okay so what do I do basically I train

566
01:01:43,560 --> 01:01:57,960
self and neighbor maps so here's the formula down here so I'm telling you what phi x of v so here's

567
01:01:57,960 --> 01:02:08,920
my my layer which is phi and I wanted to find you this map and in order to do that I can tell you

568
01:02:08,920 --> 01:02:16,520
this map evaluated at a particular vertex so that this might be vertex v and what I'm saying in

569
01:02:16,520 --> 01:02:25,000
order to get that answer what you do is you take this self map times whatever I've got here plus

570
01:02:25,000 --> 01:02:32,760
all of these neighbor maps so roughly speaking in my second layer something here has a term that

571
01:02:32,760 --> 01:02:39,720
comes from here together with terms that come from the three neighbors so it's a very natural

572
01:02:40,440 --> 01:02:53,400
it's called a message pass and as usual si is something like is affine linear

573
01:02:54,200 --> 01:03:03,400
okay so s1 is an n1 times n2 matrix

574
01:03:06,440 --> 01:03:07,720
plus an n2 vector

575
01:03:10,440 --> 01:03:16,600
okay so each of these so this is yes thank you Brian this should be an s2

576
01:03:16,600 --> 01:03:27,480
yeah so that's very important that so this is an another example of an inductive prior

577
01:03:28,120 --> 01:03:34,040
sorry inductive bias or a prior so what we're saying is that we want these n1

578
01:03:35,480 --> 01:03:42,600
these n1s sorry Stefan asked should all these n1s be the same and the answer is in general yes

579
01:03:43,480 --> 01:03:50,360
so we want for example the n1s that talk to this guy from here and from here

580
01:03:51,160 --> 01:03:58,280
to be the same n1s that talk to this guy from here here okay so the n1s are the same so

581
01:03:58,280 --> 01:04:00,040
if you imagine this matrix here

582
01:04:03,080 --> 01:04:09,720
it looks like something like s1 s1 s1 s1 down the diagonal and then n1s

583
01:04:09,960 --> 01:04:15,320
in off diagonal places given by the adjacency matrix

584
01:04:17,400 --> 01:04:21,480
you know something like this so inside this space of like

585
01:04:22,280 --> 01:04:29,400
n1 times vertices times n2 times vertices so this is what my big matrix would look like

586
01:04:29,400 --> 01:04:33,880
I'm saying like it should be block diagonal and a whole lot of blocks should be the same

587
01:04:33,880 --> 01:04:37,240
so it's a very strong inductive bias to

588
01:04:39,880 --> 01:04:48,920
assume now if I'm honest you know we might have seven like let's say two different colors on

589
01:04:48,920 --> 01:04:56,120
our vertices and then we would train um neighbor maps that preserve the color of vertices neighbor

590
01:04:56,120 --> 01:04:59,880
maps that change the color from red to blue neighbor maps that change the color from blue

591
01:04:59,880 --> 01:05:05,960
to red etc so there's a zillion variants but in the basic vanilla version of a neural net

592
01:05:05,960 --> 01:05:10,680
we assume all the n1s are the same and all the s1s are the same so this one's

593
01:05:11,320 --> 01:05:13,800
so I'll give the diagonal term of this s1

594
01:05:23,400 --> 01:05:26,360
so it's a very complicated slide but it's a simple idea I think

595
01:05:27,480 --> 01:05:34,200
so we do that for a number of times and then we evaluate or in the situation where we're

596
01:05:34,200 --> 01:05:37,880
trying to learn for example and embedding or something like that we wouldn't do the final

597
01:05:37,880 --> 01:05:44,440
layer we've got some coordinates on our vertices and we're happy another classic example of a task

598
01:05:44,440 --> 01:05:49,720
might be you want to divide your vertices into two classes and so then you would you'd do all your

599
01:05:49,720 --> 01:05:53,800
layers and then you would say at the end uh this is a real number and then you would softmax that

600
01:05:53,800 --> 01:06:00,120
and then that would be the probability that your vertex is in or out of this class okay and also

601
01:06:00,120 --> 01:06:05,720
there's you know a million variants often the neighbor term is weighted by one over the degree

602
01:06:05,720 --> 01:06:12,440
of the vertex okay that's what I set up there it's affine maps

603
01:06:17,240 --> 01:06:21,720
okay so that's the architecture and we'll have some fun playing with this architecture in the

604
01:06:21,720 --> 01:06:31,160
colab tomorrow so many variants are possible for digraphs digraphs you might think that you only

605
01:06:31,160 --> 01:06:35,480
train a forward map but generally you don't you train a forward map and a back with map

606
01:06:36,680 --> 01:06:43,160
so back forward and backwards neighbor maps if graph has edge coloring you can train message

607
01:06:43,160 --> 01:06:49,480
passing for every color of the edge vertex coloring similarly you might also have a couple

608
01:06:49,480 --> 01:06:54,760
of global parameters hanging around and in every step your global parameters speak to the parameters

609
01:06:54,760 --> 01:07:00,440
on the graph and are spoken to by parameters from the graph in some way which is probably nothing to

610
01:07:00,440 --> 01:07:08,440
do with the n ones on the s ones so these are some nice examples imagine that I consider this

611
01:07:08,440 --> 01:07:17,720
digraph here if you look at what a graph neural net does on this digraph it exactly mimics cnn's

612
01:07:18,280 --> 01:07:26,440
without falling layer so you know it's it's it's not it's not an exaggeration to say that

613
01:07:27,800 --> 01:07:30,280
cnn's are a subset of giant graph neural nets

614
01:07:36,920 --> 01:07:44,440
and if you kind of unpack this definition for uh deeps for a complete graph you

615
01:07:45,400 --> 01:07:47,320
uh basically get deepsets

616
01:07:49,800 --> 01:07:55,720
and that is all for today so thank you very much and yeah if there's any questions please ask

617
01:07:55,720 --> 01:07:59,480
so gay i'll ask a question i'll just ask uh answer the question of dr bouts you first

618
01:08:01,000 --> 01:08:04,680
I wonder if by adding linear maps between vertices of the graph and making the whole

619
01:08:04,680 --> 01:08:12,920
architecture commutative kind of quirl net has any ml interpretation yeah so I totally agree

620
01:08:12,920 --> 01:08:17,560
that when you look at neural net architectures it looks very much like river varieties and

621
01:08:17,560 --> 01:08:22,920
things that we study in representation theory however one really can't underestimate the

622
01:08:22,920 --> 01:08:31,400
effect of this relu of like so if you think about a like just a classic feed forward vanilla neural

623
01:08:31,400 --> 01:08:38,200
net um if you don't have the relu's we understand totally what happens by basically near algebra

624
01:08:39,160 --> 01:08:43,800
but when we add the relu's we can suddenly approximate any function on a compact set

625
01:08:44,440 --> 01:08:51,720
so and we we have absolutely no idea of what happens inside the the neural net so yeah I find

626
01:08:51,720 --> 01:08:56,840
the quiver language like very useful to think to think about but um one shouldn't underestimate

627
01:08:57,800 --> 01:09:04,840
relu's so gaug asked um what have graph neural nets been used for

628
01:09:06,200 --> 01:09:15,560
um and my understanding is that um like I don't know like let's say half of facebook and 75 percent

629
01:09:15,560 --> 01:09:21,880
of twitter is graph neural nets um because you've got all these like connection graphs and social

630
01:09:21,880 --> 01:09:30,120
networks and stuff like that um graph neural nets were like the absolute center thing that we used

631
01:09:30,120 --> 01:09:37,400
with deep mind too on this work on cash analysis polynomials um my understanding is that graph neural

632
01:09:37,400 --> 01:09:45,240
nets are kind of um taking over neural net world in terms of they're very flexible um they're very

633
01:09:45,240 --> 01:09:51,720
powerful um and a lot of tasks where for example c and n would involve like a drastic change of

634
01:09:51,720 --> 01:09:58,360
architecture um in graph neural nets you can just like add a vertex or something like that so

635
01:09:58,360 --> 01:10:03,080
they're very flexible powerful framework for machine learning as far as I can make out

636
01:10:04,520 --> 01:10:09,240
sam yates asked for discrete valued problem could we pick other group rings say with some

637
01:10:09,240 --> 01:10:17,000
choice of analogous relu like function perhaps uh that's an intimidating prospect for me because

638
01:10:17,000 --> 01:10:23,080
I wouldn't know how to train and things like that so my very vague understanding so there's this

639
01:10:23,080 --> 01:10:30,360
kind of revolution in the last kind of three four years given by transformers and my understanding

640
01:10:30,360 --> 01:10:39,880
of that is like basically like a a graph neural net um hooked on to an lstm so like the graph neural

641
01:10:39,880 --> 01:10:46,040
net kind of decides where to look back in the sentence and things like that so um but I'm not

642
01:10:46,040 --> 01:10:51,800
directly aware of any recurrent technologies used directly with graph neural nets what another

643
01:10:51,800 --> 01:10:56,280
thing that graph neural nets are very useful for which is kind of counterintuitive is like predicting

644
01:10:56,280 --> 01:11:02,520
graph structure so you have data and you want to um and you want to predict which edges exist

645
01:11:03,080 --> 01:11:07,080
so like you might want to predict social relationships or something like that and um my

646
01:11:07,080 --> 01:11:11,880
understanding is that you you start off with a complete graph and run a graph neural net

647
01:11:11,880 --> 01:11:17,560
and your graph kind of learns a probability of discarding an edge and that's very very powerful

648
01:11:19,320 --> 01:11:23,880
so it it's intuitive and intuitive but graph neural nets can learn graphs which I think is

649
01:11:23,880 --> 01:11:28,920
awesome I think I think like learning graphs is like learning graphs is a really difficult

650
01:11:28,920 --> 01:11:37,240
problem in machine learning um and yeah of course so it's going to be yeah it's an intimidating

651
01:11:37,240 --> 01:11:43,960
problem that or n choose tuna yeah so that's a really good question so what kind of problems

652
01:11:43,960 --> 01:11:50,040
and why can be addressed with graph neural nets my understanding is that very recently there's been

653
01:11:50,040 --> 01:11:56,120
kind of benchmarking so there's been like a list of 50 problems and some some attempt to decide

654
01:11:56,120 --> 01:12:00,680
you know what what can various classes of graph neural net algorithms do and what they can't do

655
01:12:01,480 --> 01:12:11,320
um my very rough picture as as a graph neural net greenhorn is um that anything that you can kind

656
01:12:11,320 --> 01:12:17,800
of decide via a finite like a small amount of walking around your graph and if you if if you're

657
01:12:17,800 --> 01:12:21,320
extremely smart and you're allowed to walk around your graph a little bit and you can already make

658
01:12:21,320 --> 01:12:26,280
a decision then that's something that you could solve so something like detecting planarity or

659
01:12:26,280 --> 01:12:30,440
detecting a three cycle or something this is something that you can solve but if it involves

660
01:12:30,520 --> 01:12:35,960
exploring the whole graph particularly potentially in many many different iterations for example

661
01:12:35,960 --> 01:12:41,240
finding a Hamiltonian cycle or something um yeah it's not going to work

662
01:12:43,160 --> 01:12:46,760
she'll have said the piecewise linear representation can you say a bit more about it

663
01:12:47,720 --> 01:12:51,720
sure so how about I will ask answer Joel's question and then maybe I can say something

664
01:12:51,720 --> 01:12:55,640
about it but I'll give other people the chance to leave because it's somewhat specialised

665
01:12:56,360 --> 01:13:01,640
so a part of Adam salt Wagner's work was learning graphs learning good people

666
01:13:03,160 --> 01:13:07,000
to a particularly conjecture for instance so we might hear about learning graphs in a few weeks

667
01:13:07,000 --> 01:13:11,640
yes exactly so salt Wagner's work is using reinforcement learning to produce interesting

668
01:13:11,640 --> 01:13:15,880
graphs okay so now we'll declare it over and anybody that's interested in this piecewise

669
01:13:15,880 --> 01:13:23,960
linear business can stick around I think this is super beautiful and um I will just explain it

670
01:13:23,960 --> 01:13:29,720
briefly so let's just consider the following silly examples so we're looking at the

671
01:13:31,160 --> 01:13:43,080
sn acting on s3 acting on r3 or permutation representation now we know that r3 decomposes

672
01:13:43,160 --> 01:13:54,520
canonically as nat plus triv and that is the set of vectors in r3 such that the sum of the lambda

673
01:13:54,520 --> 01:14:11,160
i is zero and triv is the set of r times 111 and this decomposition is completely canonical

674
01:14:13,400 --> 01:14:21,080
so now you can ask the following like just kind of totally naive question if we go from nat

675
01:14:22,520 --> 01:14:26,680
into r3 and then apply value

676
01:14:29,640 --> 01:14:38,280
go to r3 back down to nat the composition is an s3 equivariant

677
01:14:43,720 --> 01:14:48,680
pl endomorphism

678
01:14:49,640 --> 01:14:55,800
i.e in the category of piecewise linear maps from this vector space to itself that a s3

679
01:14:55,800 --> 01:14:59,080
equivariant this is a pl endomorphism and what is it

680
01:15:02,120 --> 01:15:10,040
it's super beautiful so basically what you do is inside so here's nat so we divide up

681
01:15:10,040 --> 01:15:21,640
nat so just for people that don't do this every day nat is the um symmetries of the triangle

682
01:15:24,440 --> 01:15:29,880
embedded inside r3 r2 okay so we just take an equilateral triangle and we take the symmetries

683
01:15:29,880 --> 01:15:43,560
of the equilateral triangle so now um there's three regions here and what happens so there's six

684
01:15:43,560 --> 01:15:55,480
regions there's the blue regions and the red regions and the blue regions get squashed

685
01:16:00,840 --> 01:16:03,880
and the red regions get kind of expanded out

686
01:16:09,160 --> 01:16:10,200
so these get squashed

687
01:16:12,760 --> 01:16:13,800
and these get expanded

688
01:16:19,080 --> 01:16:25,960
and i don't know this is just like a very beautiful basic um like pl endomorphism of a

689
01:16:26,040 --> 01:16:32,360
representation that i've never encountered before in my life okay and you can start having fun like

690
01:16:32,360 --> 01:16:32,920
what is this

691
01:16:38,440 --> 01:16:44,760
or nat inside r in and you know this is a nice exercise

692
01:16:45,320 --> 01:16:56,680
and yeah so that one of the things that i find really interesting is that home

693
01:16:59,400 --> 01:17:05,320
pl from any representation to r um is interesting

694
01:17:05,800 --> 01:17:18,120
okay so example is that home from pl from the sign representation to r

695
01:17:19,160 --> 01:17:21,640
contains the absolute value map

696
01:17:25,960 --> 01:17:31,080
um like this is for this is the sign representation of s2 oh no sign representation in general

697
01:17:31,080 --> 01:17:40,680
in fact um but home from the trivial representation pl to any irrep

698
01:17:42,680 --> 01:17:46,840
not equal to the trivial zero

699
01:17:49,640 --> 01:17:55,160
and i kind of feel like this is telling us something remarkable that about kind of how

700
01:17:55,240 --> 01:18:00,840
equivalence kind of flows through a neural net so this is still very speculative but what

701
01:18:00,840 --> 01:18:08,680
i feel like is that you have some kind of measure of complexity so it like at the start you have all

702
01:18:08,680 --> 01:18:09,320
all irreps

703
01:18:14,120 --> 01:18:20,840
and then at the end you have the trivial and then you have the maps in the in the neural net so you

704
01:18:20,840 --> 01:18:30,920
have linear and then you have pl and then you have linear pl and these pl maps have a definite

705
01:18:30,920 --> 01:18:35,480
sense of direction like you know once we get through the trivial representation we can never

706
01:18:35,480 --> 01:18:42,760
get out of it again and like i don't know this seems to explain some some very interesting

707
01:18:42,760 --> 01:18:47,560
aspects of neural nets but it's just exciting stuff that i've been thinking about last week so

708
01:18:47,640 --> 01:18:52,920
very unbaked um so maybe when it's baked i can uh talk more about it

709
01:18:55,560 --> 01:18:59,640
so thanks are the blue rays there inside the reflecting hyperplanes

710
01:19:01,480 --> 01:19:03,240
this is alpha one and this is alpha two

711
01:19:05,880 --> 01:19:12,120
so no oh yeah the so the reflecting hyperplanes would be like this and this and this

712
01:19:13,880 --> 01:19:15,400
thanks everyone i think we'll stop now

713
01:19:17,560 --> 01:19:18,060
you

