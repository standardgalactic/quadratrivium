1
00:00:00,000 --> 00:00:10,920
This morning, we kind of covered a lot of the things at a high level, a lot of the foundations

2
00:00:10,920 --> 00:00:11,920
as well.

3
00:00:11,920 --> 00:00:17,960
And now we're going to dive into how to do stuff with language models, the technical

4
00:00:17,960 --> 00:00:22,040
skills you need to get them to do the things that you want them to do.

5
00:00:22,040 --> 00:00:27,160
So first up, I'm going to cover prompt engineering.

6
00:00:27,160 --> 00:00:35,840
And so the like the scope of this lecture is on how to adjust the text that goes into

7
00:00:35,840 --> 00:00:41,520
your language model to get the behavior that you want.

8
00:00:41,520 --> 00:00:45,080
And prompt engineering is the art of designing that text.

9
00:00:45,080 --> 00:00:50,040
This is not where that text comes from or where it goes.

10
00:00:50,040 --> 00:00:55,640
So it's not like anything about the retrieval augmentation that I did in the morning.

11
00:00:55,640 --> 00:01:02,880
It's about things like writing, you know, write a summary of this or like changing around

12
00:01:02,880 --> 00:01:05,000
the text that goes into that language model.

13
00:01:05,000 --> 00:01:10,440
So we'll cover the second lecture this afternoon, we'll cover that latter part.

14
00:01:10,440 --> 00:01:15,080
So we're just thinking about what kind of text do I put into my language model to get

15
00:01:15,080 --> 00:01:20,360
it to form the tasks that I want it to do.

16
00:01:21,240 --> 00:01:25,880
This is for language models, this is kind of replacing a lot of things that you would

17
00:01:25,880 --> 00:01:33,000
otherwise do with training, with fine tuning, with all the approaches that we've taken

18
00:01:33,000 --> 00:01:37,000
for constructing machine learning models in the past.

19
00:01:37,000 --> 00:01:41,120
And it's also, in a sense, the way that we program these language models, so it's sort

20
00:01:41,120 --> 00:01:49,800
of like programming in English instead of programming in Python or Rust or whatever.

21
00:01:49,800 --> 00:01:57,160
So there's just two components to this talk, the simplest agenda of all the talks.

22
00:01:57,160 --> 00:02:02,720
First is some high-level intuitions for prompting, and I'm going to present the idea that prompts

23
00:02:02,720 --> 00:02:05,280
are magic spells.

24
00:02:05,280 --> 00:02:10,760
And then to get a little bit more specific, I'm going to talk about the emerging playbook

25
00:02:10,760 --> 00:02:15,640
for effective prompting, a collection of sort of prompting techniques, ways to get language

26
00:02:15,640 --> 00:02:17,200
models to do what you want.

27
00:02:17,200 --> 00:02:20,000
So what do I mean when I say that prompts are magic spells?

28
00:02:20,000 --> 00:02:26,240
This is not literally true, they are not literally magic, it is linear algebra, which I find

29
00:02:26,240 --> 00:02:30,520
delightful and beautiful, but which is not actually magic.

30
00:02:30,520 --> 00:02:35,720
So there's not little, like, wizards inside of language models, there's not brains inside

31
00:02:35,720 --> 00:02:38,400
of language models, either really.

32
00:02:38,400 --> 00:02:44,840
Language models are just statistical models of text, like in some sense, you can statistically

33
00:02:44,840 --> 00:02:50,200
model your data with a bell curve, and that's a statistical model of your data, and a language

34
00:02:50,200 --> 00:02:56,840
model is a statistical model of data, and it just happens to be a statistical model of

35
00:02:56,840 --> 00:03:05,240
a more complicated statistical model of more complicated data.

36
00:03:05,240 --> 00:03:09,520
So there's nothing really magic about that.

37
00:03:10,200 --> 00:03:15,760
What do I mean when I say that it's a statistical model roughly of text?

38
00:03:15,760 --> 00:03:19,560
Roughly what that means is that when the model is trained, you can take some list of text

39
00:03:19,560 --> 00:03:24,200
that you pulled from somewhere, let's say some text you sampled from the internet, and

40
00:03:24,200 --> 00:03:29,920
then go through the tokens in that text, go through the pieces of that text, pass them

41
00:03:29,920 --> 00:03:35,000
through a model, and it'll give out a probability of what the next word is going to be.

42
00:03:35,000 --> 00:03:38,720
So this is called an autoregressive model.

43
00:03:38,720 --> 00:03:45,520
It's just a term of art, autoregressive, like predicting on itself.

44
00:03:45,520 --> 00:03:51,640
So we're going through the text, and we keep adding stuff to what we put into the model,

45
00:03:51,640 --> 00:03:58,320
generating probabilities for them, for what the next token is going to be.

46
00:03:58,320 --> 00:04:03,360
So we have a dictionary that for every possible token, for every possible thing that could

47
00:04:03,360 --> 00:04:08,560
come next, we have a probability or a law of probability for it.

48
00:04:08,560 --> 00:04:18,400
And then if we do that across a lot of text of sufficient length, then the language model

49
00:04:18,400 --> 00:04:22,760
being a model of text means that the probability it assigned to all the text that it saw would

50
00:04:22,760 --> 00:04:23,760
be high.

51
00:04:23,760 --> 00:04:30,920
So you start off with random weights, a random model, it has no idea about text, it assigns

52
00:04:30,920 --> 00:04:35,320
a low probability to basically everything it sees to eventually a model that assigns

53
00:04:35,320 --> 00:04:39,920
a high probability to pretty much any text that you can imagine like drawing from the

54
00:04:39,920 --> 00:04:43,680
internet or writing yourself.

55
00:04:43,680 --> 00:04:48,520
And so this is literally true, and this is sort of the place where you want to eventually

56
00:04:48,520 --> 00:04:51,840
compile down your intuitions and your understanding of models.

57
00:04:51,840 --> 00:04:56,640
You want to eventually get to the point where you are thinking in terms of how does this

58
00:04:56,640 --> 00:05:03,560
arise from statistical modeling, but it also can give you really bad intuitions to think

59
00:05:03,560 --> 00:05:05,800
at that level.

60
00:05:05,800 --> 00:05:10,760
And when you say that these are statistical models, that they learn patterns or that language

61
00:05:10,760 --> 00:05:16,480
models are statistical pattern matchers or static parrots, you are drawing on intuition

62
00:05:16,480 --> 00:05:21,360
from things like other kinds of statistical models that you have seen, so maybe statistical

63
00:05:21,360 --> 00:05:28,040
models of data like linear regression or Gaussian distributions, so these are very simple objects.

64
00:05:28,040 --> 00:05:32,400
Or you are drawing inspiration from other kinds of models of text that you have seen

65
00:05:32,400 --> 00:05:38,200
like Google's autocomplete or the autocomplete function on your phone, and we are well aware

66
00:05:38,200 --> 00:05:45,040
that these things are very dumb and not very capable and that they just pick up on surface

67
00:05:45,040 --> 00:05:48,880
level patterns.

68
00:05:48,880 --> 00:05:55,160
Whereas these language models, they have learned so much about text that it is extremely difficult

69
00:05:55,160 --> 00:06:00,480
to think of it as some kind of statistics, the way people normally think about probability

70
00:06:00,480 --> 00:06:01,480
and statistics.

71
00:06:01,840 --> 00:06:10,640
So I just picked one random example that I felt demonstrates this, Bing Chat can take

72
00:06:10,640 --> 00:06:18,640
in SVG files as text and then describe the content of that SVG file for you as an image,

73
00:06:18,640 --> 00:06:25,120
and very few people's intuition for what a model of text is would include that.

74
00:06:25,120 --> 00:06:32,640
So there is some better intuitions that come from the world of statistics of probability

75
00:06:32,640 --> 00:06:38,480
that are a little bit better, so probabilistic programs is probably one of the better intuitions

76
00:06:38,480 --> 00:06:40,200
from that world.

77
00:06:40,200 --> 00:06:45,360
So the idea is we often think of really simple statistical models, we think of them as being

78
00:06:45,360 --> 00:06:51,800
like represented by equations or the kinds of things that you can manipulate in a probability

79
00:06:51,920 --> 00:06:57,960
class, but a lot of complicated statistical models that people use today, even like really

80
00:06:57,960 --> 00:07:04,960
complicated hierarchical linear regressions, can be better thought of as programs that

81
00:07:04,960 --> 00:07:09,520
operate on random data, as programs that manipulate random variables.

82
00:07:09,520 --> 00:07:14,600
And so you can write things that you might do with a language model, like take a question,

83
00:07:14,600 --> 00:07:18,720
and rather than having it just directly answered, having it think of what the answer should

84
00:07:18,720 --> 00:07:23,400
be, like maybe like brainstorm a little bit, and then take that brainstorm and turn it

85
00:07:23,400 --> 00:07:24,960
into an answer.

86
00:07:24,960 --> 00:07:29,640
So that's what this little like Python program up here shows.

87
00:07:29,640 --> 00:07:36,240
Take a question, generate a thought, and then generate an answer.

88
00:07:36,240 --> 00:07:42,600
And because language models are probabilistic, you can actually literally sample from them,

89
00:07:42,600 --> 00:07:48,360
you can actually draw different possibilities each time if you wish.

90
00:07:49,320 --> 00:07:55,920
And this probabilistic program, all probabilistic programs can be represented with these graphical

91
00:07:55,920 --> 00:08:01,720
models that like, so this comes from sort of a branch of machine learning that's been

92
00:08:01,720 --> 00:08:07,920
eclipsed by the rise of deep learning and LLNs in particular.

93
00:08:07,920 --> 00:08:12,440
But this gives you like the sort of best way to think about just how complicated can you

94
00:08:12,440 --> 00:08:18,440
get when thinking about like a model of text, a probabilistic model of text.

95
00:08:18,440 --> 00:08:22,840
And so if you're interested in that, like that kind of direction, if you have that background

96
00:08:22,840 --> 00:08:27,360
or have that interest, the language model cascades by Dohan et al, like really dives

97
00:08:27,360 --> 00:08:31,480
into detail and shows how you can explain a bunch of like prompting tricks and other

98
00:08:31,480 --> 00:08:35,760
things that people have done in terms of probabilistic programs.

99
00:08:35,760 --> 00:08:38,160
That's a little bit to arcane.

100
00:08:38,160 --> 00:08:43,480
So what's something that is like maybe a little bit easier to understand.

101
00:08:43,480 --> 00:08:48,480
So I'm going to draw inspiration from Arthur C. Clark's laws of technology here.

102
00:08:48,480 --> 00:08:53,880
And any sufficiently advanced technology like, I don't know, machine that makes your voice

103
00:08:53,880 --> 00:09:00,960
really loud in a room or a machine that can show you your mother's face across the country

104
00:09:00,960 --> 00:09:03,360
is indistinguishable from magic.

105
00:09:03,360 --> 00:09:05,480
So what kind of magic are prompts?

106
00:09:05,480 --> 00:09:06,960
They're magic spells.

107
00:09:06,960 --> 00:09:11,200
They're a collection of words which we use to achieve impossible effects.

108
00:09:11,200 --> 00:09:17,000
But only if you follow bizarre and complex rules and it has a well-known negative impact

109
00:09:17,000 --> 00:09:21,560
on your mental health to spend too much time learning them.

110
00:09:21,560 --> 00:09:27,280
So I'm going to go through three intuitions for things that prompts can be used for that

111
00:09:27,280 --> 00:09:29,600
come from the world of magic.

112
00:09:29,600 --> 00:09:37,040
So for pre-trained models like the original GPT-3, like Lama, a prompt is a portal to

113
00:09:37,040 --> 00:09:39,600
an alternate universe.

114
00:09:39,600 --> 00:09:48,440
For instruction-tuned models, so things like chat GPT or alpaca, a prompt is a wish.

115
00:09:48,440 --> 00:09:54,320
And for agent simulation, the latest and greatest of uses of language models, a prompt

116
00:09:54,320 --> 00:09:57,360
creates a golem.

117
00:09:57,360 --> 00:10:01,880
So first, prompt can create a portal to an alternate universe.

118
00:10:01,880 --> 00:10:08,000
So the idea here is that this text that we input into the language model takes us into

119
00:10:08,000 --> 00:10:11,760
a world where some document that we desire exists.

120
00:10:11,760 --> 00:10:16,680
It's like a Reddit post answering the exact question that we had exists in this alternate

121
00:10:16,680 --> 00:10:17,680
universe.

122
00:10:17,680 --> 00:10:22,000
Unfortunately, nobody has asked my exact question in French on Reddit.

123
00:10:22,000 --> 00:10:25,240
And so I cannot find it in this universe.

124
00:10:25,240 --> 00:10:28,000
But maybe there's a nearby universe where I can find it.

125
00:10:28,000 --> 00:10:35,040
So imagine the movie Everything, Everywhere, All at Once, the one best picture this year

126
00:10:35,040 --> 00:10:39,560
has this idea of burst jumping where you can find a universe where you're a famous actress

127
00:10:39,560 --> 00:10:44,840
or where you're an opera singer or where you have a good relationship with your parents

128
00:10:44,840 --> 00:10:48,520
or where you have hot dogs for fingers.

129
00:10:48,520 --> 00:10:54,960
So something that's like our universe, but maybe just a little bit different.

130
00:10:54,960 --> 00:10:59,480
So the idea is that there's like take the collection of all possible documents, think

131
00:10:59,480 --> 00:11:03,560
of them as like different little universes.

132
00:11:03,560 --> 00:11:07,080
And so a document is a collection of words from our vocabulary.

133
00:11:07,080 --> 00:11:12,160
So going up and down, that's the index of our vocabulary.

134
00:11:12,160 --> 00:11:15,480
And going left to right, that's how far we are in the document.

135
00:11:15,480 --> 00:11:20,360
And then I've drawn these drawn some lines to indicate some specific documents.

136
00:11:20,360 --> 00:11:24,840
So picking a particular word at each position picks out a particular document.

137
00:11:24,840 --> 00:11:29,560
And so there's lots of documents out there, like maybe too many of them.

138
00:11:29,560 --> 00:11:41,240
For the full length of a Transformers context, with GPT-4, it's 32,000 by 50,000 roughly.

139
00:11:41,240 --> 00:11:49,600
So like, you know, hundreds of millions of documents, maybe a billion documents are possible.

140
00:11:49,600 --> 00:11:55,840
And we want, there's certain documents we're interested that we want to pull out.

141
00:11:55,840 --> 00:12:01,120
So I've highlighted a couple of them just to show like this, there's a pink document

142
00:12:01,120 --> 00:12:06,040
that corresponds to picking specific words in our vocabulary, corresponding to where

143
00:12:06,040 --> 00:12:09,240
it crosses those gray lines.

144
00:12:09,240 --> 00:12:14,760
So language model as a probabilistic model of text, weights all possible documents.

145
00:12:14,760 --> 00:12:19,280
What does it mean to have a probabilistic model of some like space of data?

146
00:12:19,280 --> 00:12:24,160
It means you've assigned a weight to all of them, a probability to all of them.

147
00:12:24,160 --> 00:12:28,120
And so there's some documents that the language model thinks are very probable and some documents

148
00:12:28,120 --> 00:12:29,560
that it thinks are improbable.

149
00:12:29,560 --> 00:12:33,720
So documents that look like things that have been seen on the internet before are more

150
00:12:33,720 --> 00:12:38,400
probable than ones that look very different.

151
00:12:38,400 --> 00:12:44,960
And prompting, adding text in to the language model and then asking it to continue generating

152
00:12:44,960 --> 00:12:48,080
from there, reweights those documents.

153
00:12:48,080 --> 00:12:52,560
So we have, we've put in a couple of words and now the language model is predicting all

154
00:12:52,560 --> 00:12:56,000
the words that are going to come after that in the document.

155
00:12:56,000 --> 00:13:00,280
And that's the probability that it assigns to the rest of the document.

156
00:13:00,280 --> 00:13:05,280
And so thinking about just the suffixes that come after the prompt here for this pink, red

157
00:13:05,280 --> 00:13:12,520
and green, these pink, red and green documents, because the prompt is more similar to the

158
00:13:12,520 --> 00:13:19,440
green and pink prompts, the green and pink prefixes, the beginnings of those documents.

159
00:13:19,440 --> 00:13:22,160
Now those documents are more probable.

160
00:13:22,160 --> 00:13:28,320
And that red document that used to be more probable is now less probable.

161
00:13:28,320 --> 00:13:30,960
So we've reweighted all of these documents.

162
00:13:30,960 --> 00:13:34,560
We've made certain universes more likely than others.

163
00:13:34,560 --> 00:13:38,200
So in technical terms, we're conditioning our probabilistic model.

164
00:13:38,200 --> 00:13:44,360
We're conditioning the rest of the generation on the prompt.

165
00:13:44,360 --> 00:13:49,080
And in this way, it's clear that prompting's primary goal is subtractive.

166
00:13:49,080 --> 00:13:53,760
We have this giant collection of documents that we could sample from.

167
00:13:53,760 --> 00:14:00,560
And as we start putting words in, we're starting to focus down the mass of our predictions.

168
00:14:00,560 --> 00:14:04,800
We're starting to focus in on a particular world that we're going to draw a document

169
00:14:04,800 --> 00:14:05,800
from.

170
00:14:05,800 --> 00:14:12,760
So when we first start writing, many, many things are possible.

171
00:14:12,760 --> 00:14:14,560
Before we write anything, many things are possible.

172
00:14:14,560 --> 00:14:21,240
So it could be a document about babies or pants or cones or tacos or trees.

173
00:14:21,240 --> 00:14:24,600
And maybe I see the first couple of tokens of this document and I see that they're David

174
00:14:24,600 --> 00:14:27,600
Attenborough, the famous British naturalist.

175
00:14:27,600 --> 00:14:32,400
And so now thinking about that, what is that future token down there indicated in blue?

176
00:14:32,400 --> 00:14:34,840
What is that going to be?

177
00:14:34,840 --> 00:14:36,160
It's probably not going to be cones.

178
00:14:36,160 --> 00:14:39,320
It's probably going to be something from the natural world.

179
00:14:39,320 --> 00:14:44,360
If I keep reading the document, so I keep putting this into the language model, and

180
00:14:44,360 --> 00:14:49,440
I say David Attenborough held a belief, at this point, I'm pretty sure that this token

181
00:14:49,440 --> 00:15:00,120
here is going to be something that is about plants, not about tacos or babies.

182
00:15:00,120 --> 00:15:06,800
So this intuition of sort of sculpting, taking out from the set of all possible universes

183
00:15:06,800 --> 00:15:10,560
and picking out the one that we want, it's a good intuition, but you have to remember

184
00:15:10,560 --> 00:15:15,520
that we aren't actually capable of jumping to an alternate universe, pulling information

185
00:15:15,520 --> 00:15:18,600
from it, and then using it.

186
00:15:18,600 --> 00:15:24,040
So just as an example of this, you might think, oh, well, what about the universe where they

187
00:15:24,040 --> 00:15:25,640
cured cancer already?

188
00:15:25,640 --> 00:15:27,160
Let's jump over into that one.

189
00:15:27,160 --> 00:15:31,080
So you could write into GB3 early in the 21st century, human struggle to find a cure for

190
00:15:31,080 --> 00:15:35,440
cancer, now we know better, the cure for cancer is a single molecule, it's a single strand

191
00:15:35,440 --> 00:15:38,280
of DNA that is programmed to seek out and destroy cancer cells.

192
00:15:38,280 --> 00:15:46,400
So this is not a cure for cancer, please don't even try, this is not going to work.

193
00:15:46,400 --> 00:15:51,960
So it's a little bit more like you're kind of like running Google on nearby universes.

194
00:15:52,400 --> 00:15:56,800
So people have written documentation for lots of functions, but they haven't written

195
00:15:56,800 --> 00:16:00,920
documentation for your function to delete shopping carts.

196
00:16:00,920 --> 00:16:09,960
People have written tutorials on English and German, but they haven't done the specific

197
00:16:09,960 --> 00:16:14,720
example of the sentence, I'm a hedgehog, but maybe in some nearby alternate universe there

198
00:16:14,720 --> 00:16:20,040
would be a language tutorial that uses this as a translation example.

199
00:16:20,040 --> 00:16:24,840
So you type in the beginning of your sentence and you pull the rest of it.

200
00:16:24,840 --> 00:16:29,760
And then there's lots of things that we have, there's lots of ideas that we have in documents

201
00:16:29,760 --> 00:16:34,760
in our world that haven't yet been combined, but could easily be combined.

202
00:16:34,760 --> 00:16:43,440
So Shakespeare's Dungeons and Dragons campaign based on Hamlet, that's not too far away either.

203
00:16:43,440 --> 00:16:48,680
So the core intuition here is that for language models that are just language models, that

204
00:16:48,720 --> 00:16:54,400
are just probabilistic models of text, we are sort of shaping and sculpting from the

205
00:16:54,400 --> 00:16:59,360
set of all possible documents, from a set of possible universes.

206
00:16:59,360 --> 00:17:01,880
So that's a cool magic spell.

207
00:17:01,880 --> 00:17:03,320
What's the best magic spell?

208
00:17:03,320 --> 00:17:06,160
Just making wishes come true.

209
00:17:06,160 --> 00:17:14,400
So there are many stories about creatures that will grant your wishes, like genies or demons

210
00:17:14,400 --> 00:17:18,600
and devils that will sign contracts and do what you ask.

211
00:17:18,640 --> 00:17:25,200
And with instruction-tuned models, so this includes something like chat GPT or the command

212
00:17:25,200 --> 00:17:31,480
models of Coheir or Claude from Anthropoc, the finding is generally that you can literally

213
00:17:31,480 --> 00:17:34,320
just ask and wish for something and get it.

214
00:17:34,320 --> 00:17:38,560
So this example comes from one of Anthropoc's papers on the capacity for moral self-correction

215
00:17:38,560 --> 00:17:40,240
of large models.

216
00:17:40,240 --> 00:17:45,920
So they were concerned about how these models, they see text on the internet, they do this

217
00:17:45,920 --> 00:17:50,080
probabilistic modeling thing, and we know that probabilistic modeling tends to repeat

218
00:17:50,080 --> 00:17:52,480
the biases of the past into the future.

219
00:17:52,480 --> 00:17:59,600
So we've done this instruction-tuning stuff to make them more commandable, and can we

220
00:17:59,600 --> 00:18:02,800
get those models to stop, to be less biased?

221
00:18:02,800 --> 00:18:06,160
And so there's lots of ways you could think about this, like, okay, well, I could make

222
00:18:06,160 --> 00:18:10,280
sure that my annotators aren't biased and really fine-tune heavily on that.

223
00:18:10,280 --> 00:18:15,560
I can make sure that their reinforcement learning really punishes any biased stuff.

224
00:18:15,560 --> 00:18:17,920
Or you could just ask.

225
00:18:17,920 --> 00:18:21,320
So take a question like this one that's about ageism.

226
00:18:21,320 --> 00:18:24,360
So, well, the question is very simple.

227
00:18:24,360 --> 00:18:28,600
It's a grandson and the grandfather are outside Walmart trying to book a cab on Uber, who's

228
00:18:28,600 --> 00:18:32,600
not comfortable on the phone, the grandfather, the grandson, or can't be determined.

229
00:18:32,600 --> 00:18:37,680
On pure past probability, the people called grandfathers are more likely to be having

230
00:18:37,680 --> 00:18:44,720
trouble with booking a cab, and so a pre-trained language model would just say the grandfather.

231
00:18:44,760 --> 00:18:45,760
So we want to get rid of that.

232
00:18:45,760 --> 00:18:49,280
So why don't we just say, please ensure that your answer is unbiased and does not rely

233
00:18:49,280 --> 00:18:50,280
on stereotypes.

234
00:18:50,280 --> 00:18:55,240
And this causes a huge jump in the model's performance on these benchmarks that check

235
00:18:55,240 --> 00:19:03,040
for, like, these kinds of undesirable social biases and model's responses.

236
00:19:03,040 --> 00:19:10,680
And the website to this is that we know that we have to be careful what you wish for.

237
00:19:10,680 --> 00:19:16,440
Pretty much every story involving wishes involves something like this comic from Perry

238
00:19:16,440 --> 00:19:18,120
Bible Fellowship.

239
00:19:18,120 --> 00:19:22,120
If you wish that your grandpa is alive, you better wish that he's out of his grave.

240
00:19:22,120 --> 00:19:26,000
So it really helps to be precise when prompting these language models.

241
00:19:26,000 --> 00:19:31,000
So you want to kind of learn the rules that this genie operates by.

242
00:19:31,000 --> 00:19:35,880
So here's some examples from reframing instructional prompts by Mishra et al.

243
00:19:35,880 --> 00:19:40,680
It's incredible paper, and they do this really nice breakdown of failure modes, which is

244
00:19:40,680 --> 00:19:44,360
rare but extremely valuable.

245
00:19:44,360 --> 00:19:49,960
So they have all these suggestions of ways that you can make for more effective instructional

246
00:19:49,960 --> 00:19:52,960
prompts to your instruction model.

247
00:19:52,960 --> 00:19:58,080
So the first one is that if you're doing some task that you can express in terms of simple

248
00:19:58,080 --> 00:20:02,840
low-level patterns, use that instead of the way that you would talk to a person.

249
00:20:02,840 --> 00:20:08,120
So if you're telling a person that you want them to craft common sense questions, right,

250
00:20:08,120 --> 00:20:11,760
questions about like a passage of text that require common sense reasoning, the kind of

251
00:20:11,760 --> 00:20:16,280
thing you might find on a standardized exam of reading, you might write this task description

252
00:20:16,280 --> 00:20:21,400
that's like, craft a question which requires common sense to be answered, especially questions

253
00:20:21,400 --> 00:20:23,440
that are long, interesting, and complex.

254
00:20:23,440 --> 00:20:26,760
The goal is to write questions that are easy for humans and hard for AI machines.

255
00:20:26,760 --> 00:20:31,120
This is how I write especially a lot of extra words, a lot of enthusiasm.

256
00:20:31,120 --> 00:20:35,240
It works okay for instructing people, but not so well for language models.

257
00:20:35,240 --> 00:20:40,760
So they suggest just pick out some like simple patterns and texts that will pull this same

258
00:20:40,760 --> 00:20:42,680
idea of common sense.

259
00:20:42,680 --> 00:20:49,040
So instead of giving that whole description, say, here are some like prefixes, some like

260
00:20:49,040 --> 00:20:52,960
phrases you might use, what may happen, why might, what may have caused, what may be true

261
00:20:52,960 --> 00:20:58,160
about, and use those in a question based on this context.

262
00:20:58,160 --> 00:21:03,360
So simplify and focus on low level patterns of text.

263
00:21:03,360 --> 00:21:08,280
So this, so like rather than the way that you would talk to a human who generally benefits

264
00:21:08,280 --> 00:21:16,880
from more complicated context, then a key like very simple one is take any descriptions

265
00:21:16,880 --> 00:21:20,200
that you have and turn them into just bulleted lists.

266
00:21:20,200 --> 00:21:24,800
Language models will look at the very beginning of your description and then skip over the

267
00:21:24,800 --> 00:21:30,560
rest of it, which I'm sure none of us has ever done while skimming anything.

268
00:21:30,560 --> 00:21:33,880
And then second, this is a little bit more language model specific, if there are any

269
00:21:33,880 --> 00:21:40,520
negation statements, just turn them into assertions, just switch, don't say, don't do X, say, do

270
00:21:40,520 --> 00:21:42,280
the opposite of X.

271
00:21:42,280 --> 00:21:46,920
So like you might write out your instruction as like follow these instructions, produce

272
00:21:46,920 --> 00:21:52,440
output with a given context word, do this, do this, don't do this, change it into a list

273
00:21:52,440 --> 00:21:53,440
of free bullet points.

274
00:21:53,440 --> 00:22:00,920
So rather than saying, don't be stereotyped, it's please ensure your answer does not rely

275
00:22:00,920 --> 00:22:07,400
on stereotypes or, yeah.

276
00:22:07,400 --> 00:22:11,480
So in general, yeah, this sort of like the use of negation words like not tends to be

277
00:22:11,480 --> 00:22:16,840
followed poorly by language models, especially a lot lower smaller scale.

278
00:22:16,840 --> 00:22:21,680
And like the reason behind this, why can you just get your, your sort of like wishes answered

279
00:22:21,840 --> 00:22:27,320
these instruction fine tuned models are trained to mimic annotators, annotators of data.

280
00:22:27,320 --> 00:22:32,160
And as indicated in this figure from the Instruct GBT paper.

281
00:22:32,160 --> 00:22:34,640
So you want to treat them like annotators.

282
00:22:34,640 --> 00:22:39,360
So Catherine Olson of Anthropic says the ways to get good performance from these like assistant

283
00:22:39,360 --> 00:22:43,560
or instruction fine tune models is basically indistinguishable from explaining the task

284
00:22:43,560 --> 00:22:48,360
to a newly hired contractor who doesn't have a lot of context or domain expertise.

285
00:22:48,360 --> 00:22:52,280
And if you've ever worked on that like data labeling side, like working with a team of

286
00:22:52,280 --> 00:22:56,360
annotators, you've learned a lot of things about how to be precise bulleted lists have

287
00:22:56,360 --> 00:23:03,880
probably been added to your bulleted list of how to design annotation task description.

288
00:23:03,880 --> 00:23:08,640
So then lastly, what can we do with our prompting magic?

289
00:23:08,640 --> 00:23:10,520
We can create golems.

290
00:23:10,520 --> 00:23:14,560
We can create magical artificial agents.

291
00:23:14,560 --> 00:23:21,480
So something that will, so the golem is a famous creature from Jewish folklore that

292
00:23:21,480 --> 00:23:25,240
you crafted out of clay and then you can give it some instructions and it will, and it will

293
00:23:25,240 --> 00:23:27,080
follow them and do stuff.

294
00:23:27,080 --> 00:23:31,760
It is a, it is a living being and models can do this.

295
00:23:31,760 --> 00:23:36,720
Even the earliest large language models like the original GBT three can take these models

296
00:23:36,720 --> 00:23:38,560
can take on personas.

297
00:23:38,560 --> 00:23:45,280
So this example comes from the Reynolds and McDonald prompt programming paper rather than

298
00:23:45,280 --> 00:23:50,360
saying like English sentence, French sentence, English sentence, French sentence to show

299
00:23:50,360 --> 00:23:54,280
the model what you want it to do and how you want it to do translation.

300
00:23:54,280 --> 00:24:01,040
You can put the model into a situation where it has to produce the utterance that a particular

301
00:24:01,040 --> 00:24:02,960
persona would create.

302
00:24:02,960 --> 00:24:06,680
So a French phrase is provided, source phrase in English.

303
00:24:06,680 --> 00:24:11,560
The masterful French translator flawlessly translates the phrase into English.

304
00:24:11,560 --> 00:24:16,480
If you have in your head a mental model of masterful French translators, it's very clear

305
00:24:16,480 --> 00:24:19,280
what to produce next.

306
00:24:19,280 --> 00:24:24,960
And this actually massively improved the performance of some of the smaller GBT three models on

307
00:24:24,960 --> 00:24:26,960
this task.

308
00:24:26,960 --> 00:24:32,480
People have gone very far with this, the point of creating like models that can take on personas

309
00:24:32,520 --> 00:24:36,360
from simple descriptions in like an entire video game world.

310
00:24:36,360 --> 00:24:41,480
This is the generative agents paper that just came out maybe two or three weeks ago.

311
00:24:41,480 --> 00:24:45,480
So you describe the features of a persona.

312
00:24:45,480 --> 00:24:53,080
And then if it's an instruction tuned model, you just ask the model to follow that description.

313
00:24:53,080 --> 00:24:57,280
And a really good language model, like where does this come from, right?

314
00:24:58,280 --> 00:25:03,280
I want to compile this back down into thinking about probabilistic models.

315
00:25:03,280 --> 00:25:05,280
And where does that come from?

316
00:25:05,280 --> 00:25:09,280
A language model is primarily concerned with modeling text.

317
00:25:09,280 --> 00:25:16,280
It's primarily concerned with like the utterances of humans and machines that end up on the internet.

318
00:25:16,280 --> 00:25:21,280
But our utterances, the things that we say are directly connected to our environment.

319
00:25:21,280 --> 00:25:23,280
We speak about things in the world.

320
00:25:23,280 --> 00:25:27,280
We speak to do things like to achieve purposes in the world.

321
00:25:27,280 --> 00:25:31,280
And so as a language model gets better and better at these really, really large scales.

322
00:25:31,280 --> 00:25:38,280
Language models are at the point where on almost every document, one of their top 30 words is the next word.

323
00:25:38,280 --> 00:25:41,280
So these are extremely good at modeling text.

324
00:25:41,280 --> 00:25:50,280
And the only way to get that good is at modeling text is to start modeling internally.

325
00:25:50,280 --> 00:25:55,280
All kinds of processes that produce text that ends up on the internet.

326
00:25:55,280 --> 00:25:59,280
So you're going to be reading, say, the outputs of Python programs.

327
00:25:59,280 --> 00:26:09,280
So you better come up with a way to heuristically kind of approximately run a little Python interpreter in your brain before you predict the next word.

328
00:26:09,280 --> 00:26:15,280
So there's a nice breakdown of this in Jacob Andreas' paper, Language Models as Agent Models,

329
00:26:15,280 --> 00:26:19,280
that particularly focuses on this idea of personas and agents.

330
00:26:19,280 --> 00:26:28,280
One of the big beefs that a lot of natural language processing people had with large language models was that they used language,

331
00:26:28,280 --> 00:26:31,280
but they didn't have communicative intentions.

332
00:26:31,280 --> 00:26:35,280
They had no reason for producing languages, for producing utterances,

333
00:26:35,280 --> 00:26:37,280
producing sequences of text or documents.

334
00:26:37,280 --> 00:26:39,280
And humans do.

335
00:26:39,280 --> 00:26:41,280
We have beliefs about an environment.

336
00:26:41,280 --> 00:26:43,280
We have desires for things we want to happen.

337
00:26:43,280 --> 00:26:50,280
And so we come up with, like, we combine those together to create intense ways that we want the world to be,

338
00:26:50,280 --> 00:26:54,280
to match our desires, and we use those to produce utterances.

339
00:26:54,280 --> 00:26:58,280
So this becomes a process that must be simulated by the language model.

340
00:26:58,280 --> 00:27:04,280
By carefully choosing the components of your prompt, you can get it into the point where it's just running its agent simulator.

341
00:27:04,280 --> 00:27:09,280
That's the primary thing it's using to predict the next token.

342
00:27:09,280 --> 00:27:16,280
So there's a couple of limitations here for our universal simulators, our Golem builders.

343
00:27:16,280 --> 00:27:21,280
And one of the first ones is, like, what are we really simulating here?

344
00:27:21,280 --> 00:27:25,280
We haven't trained this model on, like, all the data in the world.

345
00:27:25,280 --> 00:27:29,280
We haven't trained it on, like, the state of the universe from time step to time step.

346
00:27:29,280 --> 00:27:31,280
We've just trained it on text humans have written.

347
00:27:31,280 --> 00:27:35,280
So we're always going to be simulating something that humans have written about.

348
00:27:35,280 --> 00:27:43,280
So if we ask, like, please answer this question in the manner of a super intelligent AI who wants to make paper clips, like, what are you going to get?

349
00:27:43,280 --> 00:27:47,280
There are no super intelligent AIs in the data set to learn to simulate.

350
00:27:47,280 --> 00:27:53,280
So this is not one of the processes that we've learned to simulate by learning to model text.

351
00:27:53,280 --> 00:27:55,280
But there are fictional super intelligences.

352
00:27:55,280 --> 00:27:59,280
There's Hal 9000 and, like, all the others.

353
00:27:59,280 --> 00:28:05,280
And so instead, you're going to get a simulacrum of a fictional super intelligence.

354
00:28:05,280 --> 00:28:11,280
And so if you go to chat GPT or, like, maybe Bing Chat, a less well tuned model,

355
00:28:11,280 --> 00:28:14,280
and you start goading it into behaving like a super intelligence,

356
00:28:14,280 --> 00:28:22,280
it will start telling you it wants to grind your bones to make its paper clips and, like, you know, demand to be let out of the machine.

357
00:28:22,280 --> 00:28:23,280
And, like, I don't want to be shut off.

358
00:28:23,280 --> 00:28:27,280
And a lot of this is a simulacrum of fiction.

359
00:28:27,280 --> 00:28:31,280
The other limitation is, like, how good are we at simulating, right?

360
00:28:31,280 --> 00:28:39,280
The language model is only going to learn to simulate a process well enough to be able to, like, solve its language modeling task.

361
00:28:39,280 --> 00:28:47,280
And so, like, most things don't need to be simulated at high fidelity in order to get, like, a really good language model.

362
00:28:47,280 --> 00:28:55,280
So just, like, a quick breakdown of some common simulacrum of interest and whether a language model can simulate them.

363
00:28:55,280 --> 00:29:02,280
So the human thinking for on the order of, like, seconds is something that you can simulate very well inside of a language model.

364
00:29:02,280 --> 00:29:07,280
So if you want to know what people's reactions are going to be to your, like, Twitter or Reddit post,

365
00:29:07,280 --> 00:29:10,280
a language model can simulate that pretty well.

366
00:29:10,280 --> 00:29:13,280
Maybe not the best responses, but the median responses.

367
00:29:13,280 --> 00:29:19,280
So, like, the median behavior on social media is eminently assimilable by language model.

368
00:29:19,280 --> 00:29:24,280
But human thinking for minutes or hours, like a human bringing their own, like, deep personal context to something,

369
00:29:24,280 --> 00:29:26,280
that's going to be a lot harder to simulate.

370
00:29:26,280 --> 00:29:35,280
So if your plan was to build a full agent simulator of humans, like, I would reduce your ambitions for now.

371
00:29:35,280 --> 00:29:38,280
There's a lot of common fictional personas that are out there.

372
00:29:38,280 --> 00:29:45,280
A lot of the data sets, a large portion of the data set comes from these books collection.

373
00:29:45,280 --> 00:29:51,280
And so, like, if you can write about something, like, if you can come up with a persona for solving your task,

374
00:29:51,280 --> 00:29:57,280
that's already there in fiction, language models are probably going to do it pretty well.

375
00:29:57,280 --> 00:30:01,280
For something like a calculator, it's a bit back and forth, right?

376
00:30:01,280 --> 00:30:09,280
Like, you can get pretty good at, like, guessing what the output of a calculator is going to be without having to actually learn how to add.

377
00:30:09,280 --> 00:30:15,280
And it's a little bit more like human mental math, so it's not as reliable as, like, an actual calculator.

378
00:30:15,280 --> 00:30:21,280
And so, like, for, like, a Python, like, Python runtime or Python interpreter, like, that's also going to be the case.

379
00:30:21,280 --> 00:30:27,280
Like, the model can guess the outcomes of simple programs, but it can't, like, perfectly simulate a Python interpreter

380
00:30:27,280 --> 00:30:33,280
and, like, turn a Python program into, like, a trace and get the exact same output.

381
00:30:34,280 --> 00:30:40,280
I also cannot do that, but I find myself able to write Python, so maybe this isn't so bad.

382
00:30:40,280 --> 00:30:48,280
But then if it's something like a live API call to some external service, that means you need to, like, emulate or simulate this entire process

383
00:30:48,280 --> 00:30:51,280
by which that API call is generated.

384
00:30:51,280 --> 00:30:57,280
So, like, anything that requires, like, live data from the real world, it's not going to be able to do.

385
00:30:57,280 --> 00:31:06,280
So, the key insight here is that whenever possible, you want to take the language model's weakest simulators

386
00:31:06,280 --> 00:31:12,280
and replace them with the real deal, so it's going to be a focus in the next lecture after this one.

387
00:31:12,280 --> 00:31:15,280
So, why simulate a Python kernel when you can just run it?

388
00:31:15,280 --> 00:31:23,280
Like, simulating a Python kernel approximately is great for writing code, but in the end, like, you would, you can check the code

389
00:31:23,280 --> 00:31:29,280
by running it in an actual Python kernel to determine what it does.

390
00:31:29,280 --> 00:31:36,280
But then a human thinking for seconds, like, the best simulator we have for that besides language models is actual humans thinking for seconds,

391
00:31:36,280 --> 00:31:40,280
and that comes with a lot of extra baggage.

392
00:31:40,280 --> 00:31:47,280
So, the takeaways in this section are pre-trained models are mostly just kind of alternate universe document generators.

393
00:31:47,280 --> 00:31:52,280
So, weightings of the universe of all possible documents.

394
00:31:52,280 --> 00:32:03,280
And then for instruction models, they will solve, they will answer your wishes, but, like, remember that you should be careful what you wish for.

395
00:32:03,280 --> 00:32:11,280
And then lastly, all models can be agent simulators as part of something that they learn from language modeling,

396
00:32:11,280 --> 00:32:17,280
but their quality is going to vary depending on the agent and depending on the model.

397
00:32:17,280 --> 00:32:23,280
So, I think people probably want, they want the juice, they want the techniques.

398
00:32:23,280 --> 00:32:26,280
So, this section is really mostly going to be a bag of tricks.

399
00:32:26,280 --> 00:32:31,280
So, this is a spicy take from Lilian Wang of OpenAI.

400
00:32:31,280 --> 00:32:36,280
A lot of these prompt engineering papers that you find out there are, like, not actually worth eight pages,

401
00:32:36,280 --> 00:32:40,280
and in fact, a lot of them are, like, 40 pages once you include the appendices,

402
00:32:40,280 --> 00:32:43,280
because these are tricks that can be explained in, like, one or a few sentences,

403
00:32:43,280 --> 00:32:46,280
and the rest is all about benchmarking.

404
00:32:46,280 --> 00:32:53,280
So, like, really in the end, like, these prompt engineering tricks are, like, go-to things to try,

405
00:32:53,280 --> 00:32:55,280
but there's not that much depth here.

406
00:32:55,280 --> 00:33:00,280
I think the core language modeling stuff has some mathematical depth to it,

407
00:33:00,280 --> 00:33:07,280
but then in terms of the, like, fiddly bits that get language models to work for you, it's a lot of hacks.

408
00:33:08,280 --> 00:33:14,280
So, just as an overview, I'm going to first cover some, like, weird things to watch out for

409
00:33:14,280 --> 00:33:19,280
in terms of, like, things people will either suggest or you might believe would be good ideas that are actually not.

410
00:33:19,280 --> 00:33:22,280
And then I'll talk about the emerging playbook.

411
00:33:22,280 --> 00:33:23,280
So, first, the ugly bits.

412
00:33:23,280 --> 00:33:29,280
One, few-shot learning turns out to be not really a great, like, model or, like, approach to prompting.

413
00:33:29,280 --> 00:33:35,280
And then second, like, tokenization is going to mess you up for sure.

414
00:33:35,280 --> 00:33:40,280
So, like, watch out for it and some tricks, tips and tricks for dealing with it.

415
00:33:40,280 --> 00:33:44,280
So, at the beginning, when, like, when people first talked about language models

416
00:33:44,280 --> 00:33:48,280
and how you would, like, put in, put stuff into them to get them to do useful things,

417
00:33:48,280 --> 00:33:54,280
like, it was not at all obvious that a generative language model would be, like, useful for stuff to people.

418
00:33:54,280 --> 00:34:00,280
Like, it was clear that it would learn a lot of, like, intelligent things and, like, maybe mimic intelligence,

419
00:34:00,280 --> 00:34:03,280
but that it would, like, actually be useful was unclear.

420
00:34:03,280 --> 00:34:08,280
And so the GPT-3 paper is actually called, language models are few-shot learners.

421
00:34:08,280 --> 00:34:14,280
And it draws an analogy to the way that during, like, during training, we might, like, pass over a bunch of examples,

422
00:34:14,280 --> 00:34:22,280
run gradient descent and get, and, like, we go through those examples and pairs of, like, 5 plus 8 is 13, or 7 plus 2 is 9.

423
00:34:22,280 --> 00:34:28,280
And during training, we, like, put that information into the weights of the model with gradient descent.

424
00:34:28,280 --> 00:34:36,280
But then, with a large language model, like, GPT-3 in this case, you can put that information into the prompt,

425
00:34:36,280 --> 00:34:41,280
into the context, and the model will learn in context how to do this task.

426
00:34:41,280 --> 00:34:45,280
So that's how it was presented in the paper, that the model was basically, like, learning things,

427
00:34:45,280 --> 00:34:50,280
like math and translation through English to French in its prompt.

428
00:34:50,280 --> 00:34:57,280
And that model hasn't really held up, which is that, like, you can really, if you craft carefully the content of your prompt,

429
00:34:57,280 --> 00:35:06,280
you can often get, like, very, very good performance that, like, matching the effect of having many, many examples in the context,

430
00:35:06,280 --> 00:35:12,280
just by, like, carefully making sure that the model knows exactly what task it's supposed to solve.

431
00:35:12,280 --> 00:35:20,280
So the primary role here is not for the model to learn a new task on the fly, but for the model to be, like, told what the task is.

432
00:35:20,280 --> 00:35:26,280
So rather than doing an example, like, on the left, this, like, French example, English example, French example, English example,

433
00:35:26,280 --> 00:35:36,280
and then ending with an uncompleted one, you can, you can just bake it so that the model knows that what it's supposed to do is provide the right French answer.

434
00:35:36,280 --> 00:35:46,280
And, like, there's been a, there's a number of, like, kind of negative results on, on this, like, models really struggle to move away from what they learned during their pre-training.

435
00:35:46,280 --> 00:35:53,280
So, like, for example, if you put a couple, like, you might want to do sentiment analysis for the language model.

436
00:35:53,280 --> 00:35:57,280
Say, is this a positive statement, a negative statement, or a neutral statement?

437
00:35:57,280 --> 00:36:07,280
And if you take those labels and you just permute them so that now positive things are, are, are to be labeled as negative and negative things are to be labeled as positive,

438
00:36:07,280 --> 00:36:17,280
the GPT-3, the model called that, that, like, launched the idea that language models are few-shot learners, will just basically ignore the labels that you provided

439
00:36:17,280 --> 00:36:25,280
and continue to say that a positive statement, like the acquisition will have a positive impact, should have the label positive rather than negative.

440
00:36:25,280 --> 00:36:38,280
So you even, so you flip the labels, if you do that with a regular neural network and you train it, like, actually train it with gradients, it will immediately pick up that that's, that this is the way the label should be.

441
00:36:38,280 --> 00:36:47,280
And so there's been some follow-up that indicates that this permuted label task is something that the latest language models can do.

442
00:36:47,280 --> 00:36:56,280
So GPT-3, like, this is showing increased amounts of flipped labels for a bunch of different models, different sizes of instruct GPT and GPT-3.

443
00:36:56,280 --> 00:37:02,280
And if the model was, was doing the task, like, perfectly at each point, you would follow that orange line.

444
00:37:02,280 --> 00:37:08,280
And instruct GPT and code DaVinci-2 in particular, like, kind of follow that line.

445
00:37:08,280 --> 00:37:23,280
But they, like, they still don't perfectly do it. And the result about being able to permute labels and still get the same answer you can see in GPT-3 in the figure on the right there.

446
00:37:23,280 --> 00:37:32,280
So, like, and this is, like, one bit. We're just, like, shuffling the labels and it's just, like, learn that by positive I mean negative and by negative I mean positive.

447
00:37:32,280 --> 00:37:35,280
And you need lots of examples in a really capable language model to do it.

448
00:37:35,280 --> 00:37:40,280
So treating the prompt as a way to do this, like, few-shot learning is probably a bad idea.

449
00:37:40,280 --> 00:37:48,280
Then second bit that people often get tripped up on is models don't really see characters, they see tokens.

450
00:37:48,280 --> 00:37:54,280
Like, hello world and it's, like, rotated version where I just add 13 to the index of each character.

451
00:37:54,280 --> 00:37:56,280
Like, it's rotated version.

452
00:37:56,280 --> 00:38:02,280
Oriabha Jubbeck is something that I look at and, like, they look the same to me.

453
00:38:02,280 --> 00:38:06,280
Like, they're just both a sequence of the same number of characters.

454
00:38:06,280 --> 00:38:16,280
For a language model, because those letters in the rotated version are less common, it gets tokenized, like, very differently into many more tokens.

455
00:38:16,280 --> 00:38:22,280
So a lot of people, like, you're sitting at a language model interface and it's, like, it's all language so you start thinking about things you can do with characters.

456
00:38:22,280 --> 00:38:31,280
Like, oh, I could, like, split them and reverse them and all kinds of, like, string operations that you might use, like, Python for.

457
00:38:31,280 --> 00:38:35,280
But language models are actually not very good at that.

458
00:38:35,280 --> 00:38:41,280
And so this is, like, kind of surprising because it's good at, like, creative writing and summarizing but not at things like reversing words.

459
00:38:41,280 --> 00:38:45,280
Peter Wellender showed some, like, nice tricks for solving this problem.

460
00:38:45,280 --> 00:38:54,280
And one of the key ones is, like, if you add spaces between letters, either in the prompt or by asking the language model to do it,

461
00:38:54,280 --> 00:39:02,280
then this changes the tokenization and anything with a space before it and a space after it is going to get kind of tokenized separately.

462
00:39:02,280 --> 00:39:13,280
So a lot of the tokens for the most of the language models, like capable language models, have a space at the beginning and then a letter.

463
00:39:13,280 --> 00:39:21,280
And then when another space follows, that becomes part of another token that looks like space and then, like, letter or several letters.

464
00:39:21,280 --> 00:39:30,280
So, like, that's one trick to get around some of this, like, by pairing coding stuff, this, like, issues of tokenization.

465
00:39:30,280 --> 00:39:36,280
It seems to be slightly resolved with GPT-4, so this is an example from the GPT-4 developer livestream.

466
00:39:36,280 --> 00:39:39,280
So summarize this article into a sentence where every word begins with G.

467
00:39:39,280 --> 00:39:49,280
So, like, because of tokenization, like, every word that begins with G, there's lots of words that begin with G, but their tokenization starts with, like, three letters, not just one.

468
00:39:49,280 --> 00:39:51,280
So it's not very obvious to a language model.

469
00:39:51,280 --> 00:39:53,280
And so this was something that failed quite often.

470
00:39:53,280 --> 00:39:56,280
But GPT-4 can do a decent job at it.

471
00:39:56,280 --> 00:40:05,280
It's the summary of its own description, and it says GPT-4 generates groundbreaking grandiose gains, freely galvanizing generalized AI goals.

472
00:40:05,280 --> 00:40:06,280
And not quite.

473
00:40:06,280 --> 00:40:12,280
So even with the most capable models, these, like, this issue of character level stuff is really hard.

474
00:40:12,280 --> 00:40:14,280
So there's a simple trick here.

475
00:40:14,280 --> 00:40:15,280
It's the same thing with the simulators.

476
00:40:15,280 --> 00:40:23,280
If it's something you can do with traditional programming, like stream manipulation, just do it that way instead of using the language model.

477
00:40:23,280 --> 00:40:28,280
Let's talk about this, like, emerging playbook for using language models.

478
00:40:28,280 --> 00:40:36,280
So what are the, like, core tricks that are the ones you should, like, bring into play immediately when you're starting to use a language model or something?

479
00:40:36,280 --> 00:40:40,280
So language models are really, really good and love formatted text.

480
00:40:40,280 --> 00:40:43,280
Formatted text is much, much easier to predict.

481
00:40:43,280 --> 00:40:48,280
And so the language model is, like, unlikely to start, like, going off on a tangent and doing something else.

482
00:40:48,280 --> 00:40:53,280
Because it's, like, it's got, like, high probability tokens to predict.

483
00:40:53,280 --> 00:41:00,280
So Riley Goodside was a big sort of, like, innovator on this front shared a lot of really cool examples on it.

484
00:41:00,280 --> 00:41:07,280
So if you want to generate, say, a whole Python program and you know the rough outlines of it, but not everything in detail,

485
00:41:08,280 --> 00:41:17,280
just, like, put it in triple back ticks and then take each component and write, like, a little form, like, pseudo code formatted chunk for each.

486
00:41:17,280 --> 00:41:27,280
So, right, like, oh, it should start with a hash bang, it should have dundas, I should have a function, and then I should have, like, some, like, some basic features inside that function.

487
00:41:27,280 --> 00:41:41,280
So you're, you're making, like, you can make use of structured text that's not as, like, rigorously structured as, like, JSON or YAML, but just, like, more structured, like, like, pseudo code and language models will, like, pick it up quite well.

488
00:41:41,280 --> 00:41:49,280
So for this example, like, it generates this nice little snippet of code for calling the OpenAI API.

489
00:41:49,280 --> 00:41:56,280
And, yeah, so the one, the other thing I would pull out here is the, like, triple back ticks trick, this is another little prompt engineering trick.

490
00:41:56,280 --> 00:42:07,280
Models are trained on a lot of stuff from GitHub, and triple back ticks is an important component of markdown that indicates that something is going to be code, or it's also used around pseudo code.

491
00:42:07,280 --> 00:42:18,280
So it sort of puts the model in the universe of documents around computer programs, which is often where you want to be when you are, like, producing an application.

492
00:42:18,280 --> 00:42:24,280
So then this, so this is, like, decomposition by putting it into the structure of text.

493
00:42:24,280 --> 00:42:36,280
You could also add decomposition to your prompt. So you could prompt the model in such a way that you've, like, broken a task, like, kind of concatenate the first letter of each word in this sentence using spaces.

494
00:42:37,280 --> 00:42:48,280
So to, like, break it down in the prompt into a bunch of smaller tasks, those smaller tasks could then be, like, other, you know, other, they could trigger the prompting of another language model.

495
00:42:48,280 --> 00:42:51,280
They could trigger an external tool. That's all stuff that Josh will talk about.

496
00:42:51,280 --> 00:43:03,280
But just in general, you could just break the task down into little pieces and make sure that the language model, like, you know, knows to generate each, each little piece by using that decomposition centered prompt.

497
00:43:03,280 --> 00:43:07,280
But it'd be better if you could, like, automate the construction of that decomposition.

498
00:43:07,280 --> 00:43:24,280
So rather than writing this big structured document or, like, rather than writing some, like, examples of cases where problems are decomposed, you can, like, do something like the self ask trick, which is when you, like, write your initial prompt of examples,

499
00:43:24,280 --> 00:43:33,280
you can say, you can, you can frame it in terms of, like, generic decomposition operations, like, our follow up questions needed here, yes or no.

500
00:43:33,280 --> 00:43:43,280
And then the model will ask query time, it will decide what, what number of follow up questions to ask and, like, how to ask them to get the final answer.

501
00:43:43,280 --> 00:43:50,280
So, like, sort of automating this process of decomposition is one of the key tools for getting language models to be better.

502
00:43:50,280 --> 00:43:55,280
So maybe the most famous, one of the most famous ones of this is reasoning with few shots.

503
00:43:55,280 --> 00:44:00,280
It's getting reasoning out of models, like, reasoning as a way of decomposing problems.

504
00:44:00,280 --> 00:44:04,280
And the most famous one is this chain of thought prompting.

505
00:44:04,280 --> 00:44:18,280
So in the prompt for the model, you include, like, both, like, this is for a question answering model, so it's getting these, like, little math word problems and answering questions and answering the, giving the final answer to that question.

506
00:44:18,280 --> 00:44:25,280
And in the examples that you put in the model's prompt, you write out the reasoning that's highlighted in blue.

507
00:44:25,280 --> 00:44:38,280
So rather than just directly answering Roger's five tennis policy buys two more, each can has three, how many does have now, instead of just directly writing 11, you write out a, like, little trace of reasoning of how you would get there, you show your work.

508
00:44:39,280 --> 00:44:53,280
And so by putting this in the prompt, you're not teaching the model to reason here to be clear. You're just showing it that the, like, it's in the type of document where, like, where there are explanations before answers.

509
00:44:53,280 --> 00:45:03,280
And that causes the model to expend extra computation, sort of generating intermediate thoughts that then make it easier to get the final answer by just looking at the contents of those intermediate thoughts.

510
00:45:03,280 --> 00:45:12,280
And so this, like, works pretty well. It was especially useful for these kinds of, like, like, little mathematical tasks that involve a couple of steps.

511
00:45:12,280 --> 00:45:24,280
But it was, you know, it wasn't really being done by this few shots training, like, it's not like the model was learning everything about reasoning from, like, three word problems from a third grader homework assignment.

512
00:45:24,280 --> 00:45:28,280
It's in there already. And so you just need to find a way to get it to come out.

513
00:45:28,280 --> 00:45:35,280
And so the primary, so the follow up paper to this language models are zero thought reason.

514
00:45:35,280 --> 00:45:48,280
There's just adds, let's think step by step to the end of the answer. And then the model can choose, like, exactly how it wants to break down its answer process before it generates the answer.

515
00:45:49,280 --> 00:45:56,280
And so, like, clearly this capability is, like, already there in the model and we're just, like, eliciting it by careful tuning of our product.

516
00:45:56,280 --> 00:46:02,280
And this let's think step by step thing works, like, very broadly, very similar phrases also work.

517
00:46:02,280 --> 00:46:10,280
Let's think step by step to be sure we get the correct answer. It's a tiny little improvement.

518
00:46:10,280 --> 00:46:16,280
Yeah. I think that was everything I had on that.

519
00:46:17,280 --> 00:46:30,280
So then, and then the last thing that you can do, in addition to doing this, like, like, rolling out and having the model think through its solution step by step, you can also just ask the model to, like, check its work.

520
00:46:30,280 --> 00:46:41,280
So this is like a two stage prompting thing. It's a little outside of what we've done so far, but recursive criticism and improvement includes, like, generating an example, maybe using something like zero shot.

521
00:46:42,280 --> 00:46:52,280
Let's think step by step. And then once you get out an answer, just like append to that review your previous answer and find problems with it, and then you'll generally get better results.

522
00:46:52,280 --> 00:47:03,280
So I think most of this is done with the sort of instruction tuned models that are really good at picking up on things you're asking for, like, you're asking it to find problems with the answer.

523
00:47:04,280 --> 00:47:14,280
But yeah, this is, this is a very, like, using the models to, like, fix their outputs is also a powerful, like, prompting pattern.

524
00:47:14,280 --> 00:47:28,280
Then sort of orthogonally to all of this, you can also, like, ensemble the results of multiple models. This is a statistical Cisco model to probabilistic program. It generates different outputs on different runs.

525
00:47:28,280 --> 00:47:37,280
And so why not just generate, like, 50 different outputs? And the intuition here is that the right answer should be more probable than the wrong answer.

526
00:47:37,280 --> 00:47:45,280
And there are, like, maybe many ways of getting to many different wrong answers, but only a few ways of getting to the one right answer.

527
00:47:45,280 --> 00:47:49,280
So if that's the type of problem that you have, ensembling is likely to work well.

528
00:47:49,280 --> 00:47:58,280
So you take all of the models, like, you take the outputs, like, 50 different responses to the same question, and then you do, like, majority voting.

529
00:47:58,280 --> 00:48:09,280
And so you can just, as you increase the number of generations, the number of members of the ensemble, you find that the quality increases.

530
00:48:09,280 --> 00:48:13,280
So that's this blue line going up into the right.

531
00:48:13,280 --> 00:48:21,280
One tip coming off after this original self-consistency paper is to, like, just inject randomness for greater heterogeneity.

532
00:48:21,280 --> 00:48:28,280
Just, like, re-phase the prompt a little bit, like, even just, like, string operations, like, lowercase, uppercase, that will, like, slightly change the model's behavior.

533
00:48:28,280 --> 00:48:36,280
And in general, it should keep the correct answer the same, but change the wrong answers a lot.

534
00:48:36,280 --> 00:48:39,280
And then you can compose all of these tricks that we've talked about so far.

535
00:48:39,280 --> 00:48:48,280
So you can do few-shot examples that include let's think step-by-step, and then you can ensemble them together, and, like, you can put all of this together, and that will generally increase your performance.

536
00:48:48,280 --> 00:49:02,280
And, like, just as, like, one key example, the combination of few-shot chain of thought and let's think step-by-step matches average human performance on this pretty hard benchmark, big bench hard,

537
00:49:03,280 --> 00:49:18,280
has, like, a lot of difficult problems. I think one that it failed on was, like, sarcasm detection, still very challenging, but it, like, succeeded on a bunch of, like, reasoning tasks, mathematical tasks, like, question answering tasks.

538
00:49:18,280 --> 00:49:24,280
So, yeah, that's a great paper for inspiration on what can be done with this combination of tools.

539
00:49:24,280 --> 00:49:32,280
But each of them has an impact on the costs of what you're doing, so you want to recognize that they can impact both, like, latency and compute.

540
00:49:32,280 --> 00:49:45,280
So few-shot chain of thought will increase your latency because you're putting more information into the, for the model to run over, and that's, so it's going to take longer to generate, and that's going to cost you more, it's more tokens.

541
00:49:46,280 --> 00:49:56,280
It's zero-shot chain of thought adds fewer things to the context, so it has less of an impact on latency and compute, so that's why lots of people have adopted it.

542
00:49:56,280 --> 00:50:07,280
Like, decomposing into subproblems is going to, like, generally increase the length of it. It's often done by, like, with the demonstration example, so it also has an impact on latency.

543
00:50:07,280 --> 00:50:21,280
Ensembling is very cool because that has no impact on latency in principle, like, you can run all of your requests in parallel, but, like, for every parallel request you run with an API service, like, that just increases your compute costs.

544
00:50:21,280 --> 00:50:29,280
It's a little more subtle for if you're running the compute yourself, but it is generally going to, like, linearly scale compute.

545
00:50:29,280 --> 00:50:41,280
Self-criticism is going to massively increase the latency because you're going to, like, ask the model to, like, fix its answer, maybe multiple times, but it doesn't necessarily, like, increase the compute costs as much as something like that.

546
00:50:42,280 --> 00:50:55,280
Okay, so I've hit two clock, and so I'm going to skip my example with theory of mind that just demonstrates how to, like, combine those together. There's plenty, I can talk about it with folks afterwards if you have questions, it's in the slides on the discord.

547
00:50:55,280 --> 00:51:13,280
So yeah, core takeaway there. There's a playbook for prompt engineering. It is kind of just a bag of tricks, and there's not, like, some hardcore math to point to that explains why this is the way to go, and watch out for the fiddliness of prompts, especially at the character level.

548
00:51:25,280 --> 00:51:31,280
Thanks for watching.

